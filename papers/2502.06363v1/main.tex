\documentclass[a4paper]{article} % 一般的なスタイルの書き方
\usepackage[margin=1in]{geometry}
\renewcommand{\baselinestretch}{1.3}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

\usepackage{amsmath,amssymb,amsthm}
%\usepackage{amsaddr}
\usepackage{bm}
\usepackage{natbib}
\usepackage{graphicx,here,comment}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{wrapfig}
% \usepackage{tablefootnote}
%\usepackage{fullpage}
%\usepackage{booktabs}
%\usepackage[textsize=tiny]{todonotes}
\usepackage{footnote}
\makesavenoteenv{table}  % table 環境で脚注を保存できるようにする

\usepackage{mathtools}

% \usepackage{url} 
\usepackage{hyperref}
% \usepackage[pagebackref]{hyperref}


\newcommand{\indep}{\perp \!\!\! \perp}
\def\ci{\perp\!\!\!\perp}
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\bignorm}[1]{\left\lVert#1\right\rVert}
\newcommand{\floor}[1]{\lfloor #1 \rfloor}
\newcommand{\ceil}[1]{\lceil #1 \rceil}
\newcommand{\rbr}[1]{\left(#1\right)}
\newcommand{\sbr}[1]{\left[#1\right]}
\newcommand{\cbr}[1]{\left\{#1\right\}}
\newcommand{\abr}[1]{\left\langle #1\right\rangle}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
% \newcommand{\bH}{\mathbb{H}}
\newcommand{\mF}{\mathcal{F}}
\newcommand{\mG}{\mathcal{G}}
\newcommand{\bG}{\mathbb{G}}
\newcommand{\mA}{\mathcal{A}}
\newcommand{\mB}{\mathcal{B}}
\newcommand{\mC}{\mathcal{C}}
\newcommand{\mD}{\mathcal{D}}
\newcommand{\mJ}{\mathcal{J}}
\newcommand{\mH}{\mathcal{H}}
\newcommand{\mK}{\mathcal{K}}
\newcommand{\mS}{\mathcal{S}}
\newcommand{\mT}{\mathcal{T}}
\newcommand{\mO}{\mathcal{O}}
\newcommand{\mP}{\mathcal{P}}
\newcommand{\mM}{\mathcal{M}}
\newcommand{\mN}{\mathcal{N}}
\newcommand{\mX}{\mathcal{X}}
\newcommand{\mY}{\mathcal{Y}}
\newcommand{\mR}{\mathcal{R}}
\newcommand{\fD}{\mathfrak{D}}
\newcommand{\fR}{\mathfrak{R}}
% \newcommand{\bX}{\mathbb{X}}
% \newcommand{\bY}{\mathbb{Y}}
\newcommand{\Ep}{\mathbb{E}}
\renewcommand{\Pr}{\mathbb{P}}
\newcommand{\Tr}{\mathrm{T}}
\newcommand{\Cr}{\mathrm{C}}
\newcommand{\Exp}{\mathrm{Exp}}
\newcommand{\Inj}{\mathrm{Inj}}
\newcommand{\Dr}{\mathrm{D}}
\newcommand{\Sr}{\mathrm{S}}
\newcommand{\mE}{\mathcal{E}}
\newcommand{\mW}{\mathcal{W}}
\newcommand{\mZ}{\mathcal{Z}}
\newcommand{\mfM}{\mathfrak{M}}
\newcommand{\mfE}{\mathfrak{E}}
\renewcommand{\hat}{\widehat}
\renewcommand{\tilde}{\widetilde}
\newcommand{\pr}{\mbox{Pr}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\renewcommand{\d}{\mathrm{d}}
\newcommand{\mone}{\textbf{1}}
\newcommand{\PL}{Polyak-\L ojasiewicz}
% \renewcommand{\span}{\mathrm{span}}
\def\bX{\mathbf{X}}
\def\bY{\mathbf{Y}}
\def\bK{\mathbf{K}}
\def\btheta{\boldsymbol{\theta}}
\def\bbeta{\boldsymbol{\beta}}
\def\bfeta{\boldsymbol{\eta}}
\def\ba{\bm{a}}
\def\bff{\bm{f}}
\def\bk{\bm{k}}
\def\br{\bm{r}}
\def\bs{\bm{s}}
\def\bt{\bm{t}}
\def\bu{\bm{u}}
\def\bI{\bm{I}}
\def\bv{\bm{v}}
\def\bw{\bm{w}}
\def\bx{\bm{x}}
\def\by{\bm{y}}
\def\bz{\bm{z}}
\def\bZ{\bm{Z}}

\newcommand{\Z}{\mathbb{Z}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Banach}{\mathbb{B}}
\newcommand{\Hilbert}{\mathbb{H}}
\newcommand{\trace}{\mathrm{tr}}
\newcommand{\prox}{\mathrm{prox}}
\newcommand{\diag}{\mathrm{diag}}

\newcommand{\sinc}{\operatorname{sinc}}
\DeclareMathOperator\E{\mathbb{E}}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\Corr}{Corr}
\DeclareMathOperator{\Err}{E} % sqrt{B + V}
\DeclareMathOperator{\hatErr}{\hat{E}}
\DeclareMathOperator{\hatErri}{\hat{E}^{(i)}}

\usepackage{color}
\newcommand{\rc}{\color{red}}
%\newcommand{\updated}[1]{\textcolor{red}{#1}}
\newcommand{\updated}[1]{#1}
\newcommand{\bc}{\color{blue}}
\newcommand{\oc}{\color{orange}}
\usepackage{newtxtext,newtxmath}

\newcommand{\secref}[1]{Section~\ref{#1}}
\newcommand{\appref}[1]{Appendix~\ref{#1}}
\newcommand{\algoref}[1]{Algorithm~\ref{#1}}
\newcommand{\asmpref}[1]{Assumption~\ref{#1}}
\newcommand{\thmref}[1]{Theorem~\ref{#1}}
\newcommand{\lemref}[1]{Lemma~\ref{#1}}
\newcommand{\corref}[1]{Corollary~\ref{#1}}
\newcommand{\exref}[1]{Example~\ref{#1}}
\newcommand{\remref}[1]{Remark~\ref{#1}}
\newcommand{\figref}[1]{Figure~\ref{#1}}
\newcommand{\tabref}[1]{Table~\ref{#1}}
\newcommand{\Ich}{I(t)}
\newcommand{\Iopt}{i^*(t)}
\newcommand{\Xit}{X^{(i)}(t)}
\newcommand{\Xarm}[1]{X^{(#1)}}
\newcommand{\Xarmt}[1]{X^{(#1)}(t)}
\newcommand{\Xarmnt}[2]{X^{(#1)}_{#2}(t)}
\newcommand{\Xt}{X^{(I(t))}(t)}
\newcommand{\Yt}{Y^{(I(t))}(t)}
\newcommand{\Zit}{Z^{(i)}(t)}
\newcommand{\Xin}[1]{X^{(i)}_{#1}}
\newcommand{\Xarmn}[2]{X^{(#1)}_{#2}}
\newcommand{\Yin}[1]{Y^{(i)}_{#1}}
\newcommand{\Yarmn}[2]{Y^{(#1)}_{#2}}
\newcommand{\bfXi}{\mathbf{X}^{(i)}}
\newcommand{\bfZi}{\mathbf{Z}^{(i)}}
\newcommand{\bA}{\mathbf{A}}
\newcommand{\bfXit}{\bfXi(t)}
\newcommand{\bfYi}{\mathbf{Y}^{(i)}}
\newcommand{\bfYit}{\bfYi(t)}
\newcommand{\thetai}{\theta^{(i)}}
\newcommand{\hattheta}{\hat{\theta}}
\newcommand{\hatthetai}{\hattheta^{(i)}}
\newcommand{\hatthetaarm}[1]{\hattheta^{(#1)}}
\newcommand{\thetaarm}[1]{\theta^{(#1)}}
\newcommand{\thetaarmn}[2]{\theta^{(#1)}_{#2}}
\newcommand{\Reg}{R}
%\newcommand{\Ni}{N^{(i)}}
\newcommand{\Ni}{N}
%\newcommand{\Nit}{N^{(i)}(t)}
%\newcommand{\Nitime}[1]{N^{(i)}(#1)}
\newcommand{\Narmt}[1]{N^{(#1)}(t)}
\newcommand{\Sigmai}{{\Sigma^{(i)}}}
\newcommand{\Sigmaarm}[1]{{\Sigma^{(#1)}}}
\newcommand{\Lambdai}{\Lambda^{(i)}}
\newcommand{\Vi}{V^{(i)}}
\newcommand{\lambdaij}{\lambda^{(i)}_j}
\newcommand{\lambdaik}{\lambda^{(i)}_k}
\newcommand{\lambdainum}[1]{\lambda^{(i)}_{#1}}
\newcommand{\lambdaarmnum}[2]{\lambda^{(#1)}_{#2}}
\newcommand{\uik}{u^{(i)}_k}
%\newcommand{\nui}{\nu^{(i)}}
%\newcommand{\nuiT}{\nu_T^{(i)}}
\newcommand{\zetai}{\zeta^{(i)}}
\newcommand{\zetaiNT}{\zetai_{N,T}}
\newcommand{\biasiNT}{B_{N,T}^{(i)}}
\newcommand{\variNT}{V_{N,T}^{(i)}}
\newcommand{\UpperiNT}{U^{(i)}_{N,T}} %旧C_b
\newcommand{\UpperiNumberT}[1]{U^{(i)}_{#1,T}} %旧C_b
\newcommand{\Upper}{U} %旧C_b
\newcommand{\LoweriNT}{L^{(i)}_{N,T}}
\newcommand{\LoweriNumberT}[1]{L^{(i)}_{#1,T}}
%\newcommand{\Lower}{L}
\newcommand{\nn}{\nonumber\\}
\newcommand{\ThetaScale}{\theta_{\mathrm{max}}} %Thetaの最大サイズ
\newcommand{\hatalphait}{\hat{\alpha}_t^{(i)}}
\newcommand{\Clambda}{C_{\lambda}}
\newcommand{\hatSigmai}{{\hat{\Sigma}^{(i)}}}
\newcommand{\Stop}{\mathrm{Stop}}
\newcommand{\Stopi}{\mathrm{Stop}^{(i)}}
\newcommand{\polylog}{\mathrm{polylog}}
\newcommand{\poly}{\mathrm{poly}}
\newcommand{\CUpper}{C_U}
\newcommand{\cUpper}{c_U}
\newcommand{\CLower}{C_L}
\newcommand{\cLower}{c_L}
\newcommand{\CPoly}{{C_{\mathrm{poly}}}}
%\newcommand{\CStop}{{C_{\mathrm{stop}}}}
\newcommand{\CStopTr}{{C_{T}}}
\newcommand{\myeps}{{\epsilon_T}}
\newcommand{\1}{\mbox{1}\hspace{-0.25em}\mbox{l}}

\newcommand{\sek}{k_{\mathrm{SE}}}
\newcommand{\matk}{k_{\mathrm{Mat\acute{e}rn}}}


\usepackage{authblk}

\title{Improved Regret Analysis in Gaussian Process Bandits: \\ Optimality for Noiseless Reward, RKHS norm, \\and Non-Stationary Variance}
\date{}
\author[1]{Shogo Iwazaki}
\author[2,3]{Shion Takeno}

\affil[1]{MI-6 Ltd.}
\affil[2]{Nagoya University}
\affil[3]{RIKEN AIP}

\affil[ ]{\texttt{{shogo.iwazaki@gmail.com}}}
\affil[ ]{\texttt{{takeno.s.mllab.nit@gmail.com}}}

\begin{document}
\maketitle



% \begin{document}

% \twocolumn[
% \icmltitle{Improved Regret Analysis in Gaussian Process Bandits: \\
% Optimality for Noiseless Reward, RKHS norm, and Non-Stationary Variance}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance, which is the preferred way.
% \icmlsetsymbol{equal}{*}

% \begin{icmlauthorlist}
% \icmlauthor{Firstname1 Lastname1}{equal,yyy}
% \icmlauthor{Firstname2 Lastname2}{equal,yyy,comp}
% \icmlauthor{Firstname3 Lastname3}{comp}
% \icmlauthor{Firstname4 Lastname4}{sch}
% \icmlauthor{Firstname5 Lastname5}{yyy}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
% %\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
% %\icmlauthor{}{sch}
% %\icmlauthor{}{sch}
% \end{icmlauthorlist}

% \icmlaffiliation{yyy}{Department of XXX, University of YYY, Location, Country}
% \icmlaffiliation{comp}{Company Name, Location, Country}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

% \icmlcorrespondingauthor{Firstname1 Lastname1}{first1.last1@xxx.edu}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
% \icmlkeywords{Machine Learning, ICML}

% \vskip 0.3in
% ]


\begin{abstract}
We study the Gaussian process (GP) bandit problem, whose goal is to minimize regret under an unknown reward function lying in some reproducing kernel Hilbert space (RKHS). 
% Maximum variance reduction (MVR) and phased elimination (PE) are near-optimal GP bandit algorithms for which the maximum posterior variance analysis plays a vital role. 
The maximum posterior variance analysis is vital in analyzing near-optimal GP bandit algorithms such as maximum variance reduction (MVR) and phased elimination (PE).
Therefore, we first show the new upper bound of the maximum posterior variance, which improves the dependence of the noise variance parameters of the GP. By leveraging this result, we refine the MVR and PE to obtain (i) a nearly optimal regret upper bound in the noiseless setting and (ii) regret upper bounds that are optimal with respect to the RKHS norm of the reward function. Furthermore, as another application of our proposed bound, we analyze the GP bandit under the time-varying noise variance setting, which is the kernelized extension of the linear bandit with heteroscedastic noise. For this problem, we show that MVR and PE-based algorithms achieve noise variance-dependent regret upper bounds, which matches our regret lower bound.
\end{abstract}

\section{Introduction}
\label{sec:intro}
The Gaussian process (GP) bandits~\citep{srinivas10gaussian} is a powerful framework for sequential decision-making tasks to minimize regret defined by a black-box reward function, which belongs to known reproducing kernel Hilbert space (RKHS). 
%
%
The applications include many fileds such as robotics~\cite{Berkenkamp2021bayesian}, experimental design~\cite{lei2021bayesian}, and hyperparameter tuning task~\cite{snoek2012practical}. 



Many existing studies have been conducted to obtain the theoretical guarantee for the regret.
%
Establised work by \citet{srinivas10gaussian} has shown the upper bounds of the cumulative regret for the GP upper confidence bound (GP-UCB) algorithm.
%
Furthermore, \citet{valko2013finite} have shown the tighter regret upper bound for the SupKernelUCB algorithm.
%
\citet{scarlett2017lower} have shown the lower bound of the regret, which implies that the regret upper bound from \citep{valko2013finite} is near-optimal; that is, the regret upper bound matches the lower bound except for the poly-logarithmic factor.
%
Then, several studies further tackled obtaining a near-optimal GP-bandit algorithm.
%
\citet{vakili2021optimal} have proposed maximum variance reduction (MVR), which is shown to be near-optimal for the simple regret incurred by the last recommended action.
%
Furthermore, \citet{li2022gaussian} have shown that phased elimination (PE) is near-optimal for the cumulative regret.
%
The regret analysis of MVR and PE heavily depends on the upper bound for the maximum posterior variance.
% Most existing analyses~\citep{srinivas10gaussian,chowdhury2017kernelized,vakili2021optimal,li2022gaussian} derive the regret upper bounds through the upper bound for the maximum posterior variance or sum of the posterior variances with respect to the selected actions.



We derive the upper bound of the maximum posterior variance in \secref{sec:uub_pv}, by which we tackle tightening the regret upper bound in the settings where room for improvement remains.
%
Our contributions are summarized as follows:
\begin{enumerate}
    \item In \secref{sec:uub_pv}, we obtain the upper bound of the maximum posterior variance (Lemma~\ref{lem:pvu_mvr} and Corollary~\ref{cor:pvu_mvr}). Our proposed bound is tighter than the existing bound when the noise variances approach zero. 
    % By this result, we derived the regret upper bound in the following three settings.
    %
    \item In \secref{sec:nls}, we analyze the GP bandit under the noiseless setting. We show a novel result that PE achieves the cumulative regret upper bound that matches the conjectured lower bound shown by \citet{vakili2022open} under common assumptions in the GP bandit literature. Furthermore, we prove that MVR achieves the exponentially converging and near-optimal simple regret upper bounds for squared exponential (SE) and Mat\'{e}rn kernels, respectively. These results are summarized in Tables~\ref{tab:nl_cr_compare}--\ref{tab:nl_sr_compare}.
    %
    \item In \secref{sec:rkhs_od}, we show that the modified PE- and MVR-style algorithms achieve the near-optimal cumulative and simple regret upper bounds with respect to the RKHS norm upper bound of the reward function under several conditions. These results are summarized in Tables~\ref{tab:cr_rkhs_compare}--\ref{tab:sr_eps_rkhs_compare}.
    %
    \item In \secref{sec:nsv}, we analyze the GP-bandit problem with the non-stationary noise variance, which is the kernelized extension of the linear bandit with heteroscedastic noise~\citep{zhou2021nearly}.
    We first study the regret lower bound.
    Then, we show that the modified PE- and MVR-style algorithms achieve the near-optimal cumulative and simple regret upper bounds, respectively. 
    To our knowledge, our analyses are the first for this setting, though the non-stationary noise is a frequently faced problem.
\end{enumerate}


\subsection{Related Works}

The theoretical assumption of the GP bandit is twofold: Bayesian setting~\citep{srinivas10gaussian,freitas2012exponential,Russo2014-learning,scarlett2018tight,takeno2023-randomized,takeno2024-posterior} where the reward function follows GPs, and the frequentist setting, where the reward function lies in known RKHS~\citep{srinivas10gaussian,chowdhury2017kernelized,vakili2021optimal,li2022gaussian}.
%
Although this paper concentrates on deriving the regret upper bound for the frequentist setting, our Lemma~\ref{lem:pvu_mvr} and Corollary~\ref{cor:pvu_mvr} are versatile and can be applied to the Bayesian setting.


% Usual GP bandit
Many GP bandit algorithms have been proposed in the frequentist setting~\citep[for example, ][]{srinivas10gaussian,valko2013finite,chowdhury2017kernelized,janz2020bandit,vakili2021optimal,li2022gaussian}.
%
Although several existing methods~\citep{valko2013finite,janz2020bandit,camilleri2021high,salgia2021domain,li2022gaussian} achieve near-optimal regret upper bounds for the ordinary GP bandit setting as summarized in \citep{li2022gaussian}, we develop PE- and MVR-style algorithms due to their simplicity.
%
On the other hand, although these existing methods are near-optimal regarding the time horizons, the optimality regarding the RKHS norm of the reward function has not been shown as summarized in Tables~\ref{tab:cr_rkhs_compare}--\ref{tab:sr_eps_rkhs_compare}.


% noiseless GP bandit
The regret analyses are also conducted on the noiseless setting~\citep{bull2011convergence,lyu2019efficient,vakili2022open,salgiarandom,kim2024bayesian,flynn2024tighter}.
%
Regarding the cumulative regret, we obtained a tighter upper bound for both SE and Mat\'ern kernels than existing results without the additional assumption for the reward function like Assumption~4.2 in \citep{salgia2021domain}.
%
Regarding the simple regret, \citet{kim2024bayesian} have shown that the random sampling-based algorithm achieves the known-best regret upper bound in terms of the expectation regarding the algorithm's randomness.
% 
Compared with this result, we show the regret upper bounds that always hold with the deterministic MVR-style algorithm.
%
In particular, the regret upper bound is tighter for the Mat\'ern kernel than that from \citep{kim2024bayesian}.
%
Tables~\ref{tab:nl_cr_compare}--\ref{tab:nl_sr_compare} summarize the comparison.


% lower bound of GP bandit
Compared with the regret upper bound, the analysis for the regret lower bound is limited~\citep{bull2011convergence,scarlett2017lower,cai2021on,vakili2022open}.
%
From these results, we will confirm the optimality of the GP bandit algorithms in Sections~\ref{sec:nls} and \ref{sec:rkhs_od}.
%
In \secref{sec:nsv}, our regret lower bound for the non-stationary noise variance setting is directly obtained from the proofs of~\citep{bull2011convergence,scarlett2017lower}.



% non-stationary noise variance (linear bandit)
The linear bandit with heteroscedastic noise, where the noise variance is non-stationary with respect to the time horizons, has been studied \citep{zhou2021nearly,zhang2021improved,kim2022improved,zhou2022computationally,zhao2023variance}.
%
These studies aim to obtain the noise variance-dependent regret upper bound, characterized by the sum of noise variances.
%
To our knowledge, the kernelized extension of this setting has not been investigated.
%
Furthermore, as discussed in \secref{sec:nsv}, the direct extension from the linear bandit methods is not near-optimal.



% \begin{table*}[tb]
%     \centering
%     \begin{tabular}{c|c|c|c|c|l}
%          & Regret (SE) & \multicolumn{3}{|c|}{Regret (Mat\'ern)} & Remark \\ \hline
%          &  &  &  \\ \hline
%         GP-UCB & $O\rbr{\sqrt{T \ln^{d+1} T}}$ & $\tilde{O}\rbr{T^{\frac{\nu + d}{2\nu + d}}}$ & \\
%         Explore-then-Commit & N/A & $O\rbr{T^{\frac{d}{\nu + d}}}$ & \\
%         REDS & N/A & $\tilde{O}\rbr{T^{\frac{\nu + d}{2\nu + d}}}$ & \\
%         Kernel-AMM-UCB & $O\rbr{\ln^{d+1} T}$ & $\tilde{O}\rbr{T^{\frac{\nu d + d^2}{2\nu^2 + 2\nu d + d^2}}}$ & \\
%         Phased Elimination & $O(\ln T)$ & $\begin{cases}
%             \tilde{O}\rbr{T^{\frac{d - \nu}{d}}} & \\
%             \tilde{O}\rbr{\ln^{\alpha} T} & \\
%             \tilde{O}\rbr{\ln T} & 
%         \end{cases}$
%         & \\
%         Conjectured Lower Bound & N/A & &
%     \end{tabular}
%     \caption{Caption}
%     \label{tab:my_label}
% \end{table*}

\begin{table*}[tb]
    \centering
    \caption{Comparison between existing noiseless algorithms' guarantees for cumulative regret and our result. 
    In all algorithms, the smoothness parameter of the Mat\'ern kernel is assumed to be $\nu > 1/2$.
    Furthermore, $d$, $\ell$, $\nu$, and $B$ are supposed to be $\Theta(1)$ here. ``Type'' column shows that the regret guarantee is  (D)eterministic or (P)robabilistic. Throughout this paper, the notation $\tilde{O}(\cdot)$ represents the order notation whose poly-logarithmic dependence is ignored.
    }
    \begin{tabular}{c|c|c|c|c|c|l}
    \multicolumn{1}{c|}{\multirow{2}{*}{Algorithm}} & \multicolumn{1}{|c|}{\multirow{2}{*}{Regret (SE)}} & \multicolumn{3}{|c|}{Regret (Mat\'ern)} & \multirow{2}{*}{Type} & \multirow{2}{*}{Remark} \\ \cline{3-5}
    \multicolumn{1}{c|}{}  & \multicolumn{1}{|c|}{}  & $\nu < d$  & $\nu = d$  & $\nu > d$ &  & \\ \hline \hline
     GP-UCB & \multirow{3}{*}{$O\rbr{\sqrt{T \ln^{d} T}}$} & \multicolumn{3}{|c|}{\multirow{3}{*}{$\tilde{O}\rbr{T^{\frac{\nu + d}{2\nu + d}}}$}} & \multirow{3}{*}{D} & \\ 
     \cite{lyu2019efficient} & & \multicolumn{3}{|c|}{} & & \\ 
     \cite{kim2024bayesian} & & \multicolumn{3}{|c|}{} & & \\ \hline
     Explore-then-Commit & \multirow{2}{*}{N/A} & \multicolumn{3}{|c|}{\multirow{2}{*}{$\tilde{O}\rbr{T^{\frac{d}{\nu + d}}}$}} & \multirow{2}{*}{P} & \\ 
     \cite{vakili2022open} &  & \multicolumn{3}{|c|}{} & & \\ \hline
         Kernel-AMM-UCB
      & \multirow{2}{*}{$O\rbr{\ln^{d+1} T}$} & \multicolumn{3}{|c|}{\multirow{2}{*}{$\tilde{O}\rbr{T^{\frac{\nu d + d^2}{2\nu^2 + 2\nu d + d^2}}}$}} & \multirow{2}{*}{D} & \\ 
      \cite{flynn2024tighter}
      &  & \multicolumn{3}{|c|}{} & & \\ \hline
     REDS & \multirow{2}{*}{N/A} & \multirow{2}{*}{$\tilde{O}\rbr{T^{\frac{d - \nu}{d}}}$} & \multirow{2}{*}{$O\rbr{\ln^{\frac{5}{2}} T}$} & \multirow{2}{*}{$O\rbr{\ln^{\frac{3}{2}} T}$} & \multirow{2}{*}{P} & Assumption for \\
     \cite{salgiarandom} &  &  &  &  & & level-set is required. \\ \hline
     \textbf{PE} & \multirow{2}{*}{$O\rbr{\ln T}$} & \multirow{2}{*}{$\tilde{O}\rbr{T^{\frac{d - \nu}{d}}}$} & \multirow{2}{*}{$O\rbr{\ln^{2 +\alpha} T}$} & \multirow{2}{*}{$O\rbr{\ln T}$} & \multirow{2}{*}{D} & $\alpha > 0$ is an arbitrarily  \\ 
     \textbf{(our analysis)} & & & & &  & fixed constant. \\ \hline
     Conjectured Lower Bound & \multirow{2}{*}{N/A} & \multirow{2}{*}{$\Omega\rbr{T^{\frac{d - \nu}{d}}}$} & \multirow{2}{*}{$\Omega(\ln T)$} & \multirow{2}{*}{$\Omega(1)$} & \multirow{2}{*}{N/A} & \\
     \cite{vakili2022open} & & & & & & \\
    \end{tabular}
    \label{tab:nl_cr_compare}
\end{table*}


\section{Preliminaries}
\label{sec:prelim}
\paragraph{Problem Setting.}
Let $f: \mX \rightarrow \R$ be an unknown reward function with compact input domain $\mX \subset \R^d$. At each step $t$, a learner chooses the query point $\bx_t \in \mX$; after that, the learner observes corresponding reward $y_t \coloneqq f(\bx_t) + \epsilon_t$, where $\epsilon_t$ is a mean-zero random variable. Under this setup, the learner's goal is to minimize the following cumulative regret $R_T$ or the simple regret $r_T$:
\begin{align}
    R_T &= \sum_{t \in [T]} f(\bx^{\ast}) - f(\bm{x}_t), \\
    r_T &= f(\bx^{\ast}) - f(\hat{\bm{x}}_T),
\end{align}
where $[T] = \{1, \ldots, T\}$ and $\bx^{\ast} \in \mathrm{arg~max}_{\bx \in \mX} f(\bx)$. 
Furthermore, $\hat{\bm{x}}_T \in \mX$ is the estimated maximizer, 
which is returned by the algorithm at the end of step $T$.
\paragraph{Regularity Assumptions.}
To construct an algorithm, we leverage the following assumptions.

\begin{assumption}[Smoothness of $f$]
\label{asmp:smoothness}
Assume that $f$ be an element of RKHS $\mH_k$ with bounded RKHS norm $\|f\|_{\mH_k} \leq B < \infty$. Here, $\mH_k$ and $\|f\|_{\mH_k}$ respectively denote RKHS and its norm endowed with known positive definite kernel $k: \mX \times \mX \rightarrow \R$. 
Furthermore, we assume $k(\bx, \bx) \leq 1$ holds for all $\bx \in \mX$.
\end{assumption}

\begin{assumption}[Assumption for noise]
\label{asmp:noise}
The noise sequence $(\epsilon_t)_{t \in \mathbb{N}_+}$ is mutually independent. 
Furthermore, assume that $\epsilon_t$ is a sub-Gaussian random variable with variance proxy 
$\rho_t \geq 0$; namely, $\Ep[\exp(\lambda \epsilon_t)] \leq \exp(\lambda^2 \rho_t^2 / 2)$ holds for all $\lambda \in \R$.
\end{assumption}

In existing works~\citep[e.g., ][]{srinivas10gaussian}, \asmpref{asmp:smoothness} is the standard assumption for encoding the smoothness of the underlying reward function depending on the kernel.
We focus on the following SE kernel $\sek$ and Mat\'ern kernel $\matk$ that are commonly used in the GP bandit:
\begin{align*}
    \sek(\bx, \tilde{\bx}) &= \exp\rbr{- \frac{\|\bx - \tilde{\bx}\|_2^2}{2\ell^2}}, \\
    \matk(\bx, \tilde{\bx}) &= \frac{2^{1-\nu}}{\Gamma(\nu)} \rbr{\frac{\sqrt{2\nu} \|\bx - \tilde{\bx}\|_2}{\ell}}^{\nu} J_{\nu}\rbr{\frac{\sqrt{2\nu} \|\bx - \tilde{\bx}\|_2}{\ell}},
\end{align*}
where $\ell > 0$ and $\nu > 0$ are the lengthscale and smoothness parameter, respectively.
Furthermore, $\Gamma(\cdot)$ and $J_{\nu}$ denote Gamma and modified Bessel function, respectively. \asmpref{asmp:noise} is also common in existing work~\citep[e.g., ][]{vakili2021optimal,li2022gaussian}. 
In Sections~\ref{sec:nls}--\ref{sec:rkhs_od}, we consider the stationary noise variance setting whose variance proxy $\rho_t$ is fixed over time, while the non-stationary noise variance setting that allows the time-dependent variance proxies $\rho_t$ in \secref{sec:nsv}.

\paragraph{Gaussian Process.}
GP is a fundamental kernel-based model that gives both the prediction 
and its uncertainty quantification of the underlying function.
Let $\mathcal{GP}(0, k)$ be a mean-zero GP whose covariance is characterized by the kernel function $k$. 
In addition, let $\bX \coloneqq (\bx_1, \ldots, \bx_t)$ and $\by \coloneqq (y_1, \ldots, y_t)^{\top}$ be training input and output data of GP, respectively. 
Then, under the Bayesian assumption that 
$f$ follows the GP prior $\mathcal{GP}(0, k)$, the posterior distribution of $f(\bx)$ given 
$\bX$ and $\by$
is defined as Gaussian distribution, whose mean $\mu_{\Sigma}(\bx; \bX, \by)$ and variance $\sigma_{\Sigma}^2(\bx; \bX)$ are
\begin{align*}
    \mu_{\Sigma}(\bx; \bX, \by) &= \bk(\bx, \bX)^{\top} (\bK(\bX, \bX) + \Sigma)^{-1} \by, \\
    \sigma_{\Sigma}^2(\bx; \bX) &= k(\bx, \bx) - \bk(\bx, \bX)^{\top} (\bK(\bX, \bX) + \Sigma)^{-1} \bk(\bx, \bX),
\end{align*}
where $\bK(\bX, \bX) \coloneqq [k(\bx, \tilde{\bx})]_{\bx, \tilde{\bx} \in \bX} \in \R^{t \times t}$ and $\bK(\bx, \bX) = [k(\bx, \tilde{\bx})]_{\tilde{\bx} \in \mX} \in \R^t$
respectively represent the kernel matrix and vector defined by $\bx$ and $\bX$. 
Furthermore, $\Sigma \in \R^{t \times t}$ is the positive definite variance parameter matrix that defines the noise structure of the observation of GP. 
Namely, given the input data $\bX$, the corresponding outputs $\by$ is assumed to be given as $\by = \bm{f}(\bX) + \bm{\epsilon}$ under the GP modeling, where $\bm{\epsilon} \sim \mN(\bm{0}, \Sigma)$ and $\bm{f}(\bX) \sim \mN(\bm{0}, K(\bX, \bX))$. We would like to emphasize that the above modeling assumptions ($f \sim \mathcal{GP}(0, k)$ and $\bm{\epsilon} \sim \mN(\bm{0}, \Sigma)$) are ``fictional'' assumptions that are used only for GP modeling, and are distinct from Assumptions~\ref{asmp:smoothness} and \ref{asmp:noise}. 

\paragraph{Maximum Information Gain.}
The maximum information gain (MIG) is the kernel-dependent complexity parameter used to characterize the regret and confidence bounds in the GP bandits.
Given the variance parameter matrix $\Sigma_T \coloneqq \mathrm{diag}(\lambda_1^2, \ldots, \lambda_T^2)$, 
the MIG $\gamma_T(\Sigma_T)$ is defined as
\begin{equation}
    \gamma_T(\Sigma_T) = \max_{\bX \subset \mX^T} I_{\Sigma_T}(\bm{f}(\bX), \by),
\end{equation}
where $I_{\Sigma_T}(\bm{f}(\bX), \by) \coloneqq \frac{1}{2} \ln \frac{\det(\Sigma_T + \bK(\bX, \bX))}{\det (\Sigma_T)}$ denote the mutual information between $\by$ and $\bm{f}(\bX)$, under the GP modeling assumpsions $\bm{f}(\bX) \sim \mN(\bm{0}, K(\bX, \bX))$, $\by = \bm{f}(\bX) + \bm{\epsilon}$, and $\bm{\epsilon} \sim \mN(\bm{0}, \Sigma_T)$.
When $\Sigma_T = \lambda^2 \bI_T$ with some fixed $\lambda > 0$ and the identity matrix $\bI_T \in \R^{T \times T}$, the upper bound of MIG $\overline{\gamma}(T, \lambda^2)$ is known in 
several commonly used kernels. For example, $\gamma_T(\lambda^2 \bI_T) \leq \overline{\gamma}(T, \lambda^2) = O(\ln^{d+1} (T/\lambda^2))$ and $\gamma_T(\lambda^2 \bI_T) \leq \overline{\gamma}(T, \lambda^2) = O((T/\lambda^2)^{\frac{d}{2\nu + d}} (\ln (T/\lambda^2))^{\frac{2\nu}{2\nu + d}})$ in SE and Mat\'ern kernels with $\nu > 1/2$, respectively. 
%
% Furthermore, for general $\Sigma_T$, several existing works show the relation $\gamma_T(\Sigma_T) \leq \gamma_T(\underline{\lambda}_T^2 \bI_T)$ with $\underline{\lambda}_T^2 = \min_{t \in [T]} \lambda_t^2$~\cite{}. %TODO: consider the maximum 
Furthermore, for general $\Sigma_T$, we can see $\gamma_T(\Sigma_T) \leq \gamma_T(\underline{\lambda}_T^2 \bI_T)$ with $\underline{\lambda}_T^2 = \min_{t \in [T]} \lambda_t^2$ from the data processing inequality~\citep[Theorem~2.8.1 of][]{cover2006-information}. 
%
We show the proof in Appendix~\ref{sec:MIG_monotone_proof} for completeness.

\paragraph{Maximum Variance Reduction and Phased Elimination.}
MVR is the algorithm that sequentially chooses the most uncertain action $\bm{x}_t = \argmax_{\bm{x} \in \mX} \sigma_{\Sigma}^2(\bx; \bX)$ as shown in Algorithm~\ref{alg:mvr} in \appref{sec:pseudo_code_pe_mvr}.
%
PE is the algorithm that combines MVR and candidate elimination as shown in Algorithm~\ref{alg:pe} in \appref{sec:pseudo_code_pe_mvr}.
%
PE divides the time horizons into batches with appropriately designed lengths and performs MVR in each batch.
%
In PE, after each batch, the inputs whose UCB is lower than the maximum of lower CB are eliminated from the candidates.
%
%
MVR and PE achieve near-optimal simple and cumulative regret upper bounds, respectively.
%
Due to their simplicity, we analyze MVR- and PE-style algorithms.


% \begin{algorithm}[t!]
%     \caption{Phased Elimination (PE)}
%     \label{alg:pe}
%     \begin{algorithmic}[1]
%         \REQUIRE Confidence width parameter $\beta^{1/2} > 0$, initial batch size $N_1$, noise variance parameter $\lambda^2 \geq 0$.
%         \STATE Initialize the potential maximizer $\mathcal{X}_{1} \leftarrow \mathcal{X}$.
%         \FOR {$i = 1, 2, \ldots$}
%             \STATE $\bX_0^{(i)} = \emptyset$.
%             \FOR {$j = 1, \ldots, N_i$}
%                 \STATE $\bm{x}_{j}^{(i)} \leftarrow \mathrm{arg~max}_{\bm{x} \in \mathcal{X}_{i}} \sigma_{\lambda^2\bI_{j-1}}(\bm{x}; \bX_{j-1}^{(i)})$.
%                 \STATE Observe $y_{j}^{(i)} = f\left(\bm{x}_{j}^{(i)}\right) + \epsilon_{j}^{(i)}$.
%                 \STATE $\bX_{j}^{(i)} \leftarrow [\bx_m^{(i)}]_{m \in [j]}$.
%             \ENDFOR
%             \STATE $\bm{y}^{(i)} \leftarrow \sbr{y_{j}^{(i)}}_{k \in [N_i]}$.
%             \STATE Calculate $\text{lcb}_{i}(\cdot)$ and $\text{ucb}_{i}(\cdot)$ as
%             \begin{align*}
%                 \text{lcb}_{i}(\bm{x}) &= \mu_{\lambda^2\bI_{N_i}}(\bm{x}; \bX_{N_i}^{(i)}, \by^{(i)}) -\beta^{1/2} \sigma_{\lambda^2\bI_{N_i}}(\bm{x}; \bX_{N_i}^{(i)}), \\
%                 \text{ucb}_{i}(\bm{x}) &= \mu_{\lambda^2\bI_{N_i}}(\bm{x}; \bX_{N_i}^{(i)}, \by^{(i)}) + \beta^{1/2} \sigma_{\lambda^2\bI_{N_i}}(\bm{x}; \bX_{N_i}^{(i)}).
%             \end{align*}
%             \STATE $\mathcal{X}_{i+1} \leftarrow \left\{ \bm{x} \in \mathcal{X}_{i} ~\middle|~ \text{ucb}_{i}(\bm{x}) \geq \max\limits_{\tilde{\bm{x}} \in \mathcal{X}_{i}} \text{lcb}_{i}(\tilde{\bm{x}}) \right\}$.
%             \STATE Update the batch size $N_{i+1} \leftarrow 2N_i$.
%         \ENDFOR
%     \end{algorithmic}
% \end{algorithm}

% \begin{algorithm}[t!]
%     \caption{Maximum Variance Reduction (MVR)}
%     \label{alg:mvr}
%     \begin{algorithmic}[1]
%         \REQUIRE Noise variance parameter $\lambda^2 \geq 0$.
%         \STATE $\bX_{0} = \emptyset$.
%         \FOR {$t = 1, 2, \ldots, T$}
%             \STATE $\bm{x}_{t} \leftarrow \mathrm{arg~max}_{\bm{x} \in \mathcal{X}} \sigma_{\lambda^2 \bI_{t-1}}(\bm{x}; \bX_{t-1})$.
%             \STATE Observe $y_t$ and construct data $\bX_{t} \coloneqq [\bx_j]_{j \in [t]}$.
%         \ENDFOR
%         \STATE Return the estimated solution $\hat{\bx}_T \coloneqq \mathrm{arg~max}_{\bm{x} \in \mathcal{X}} \mu_{\lambda^2 \bI_{T}}(\bm{x}; \bX_{T}; \by_T)$, where $\bm{y}_T = \sbr{y_{t}}_{t \in [T]}$.
%     \end{algorithmic}
% \end{algorithm}

\section{Uniform Upper Bound of Posterior Variance for Maximum Variance Reduction}
\label{sec:uub_pv}
In this section, we describe the theoretical core result that gives a new upper bound of the posterior variance for the MVR algorithm. Specifically, our result improves the existing upper bound of posterior variance when decreasing noise variance parameters.
\begin{lemma}[General posterior variance upper bound for MVR]
\label{lem:pvu_mvr}
Fix any compact subset $\tilde{\mX} \subset \mX$. Then, the following two statements hold:
\begin{enumerate}
    \item \textbf{Stationary var.:} Let $(\overline{\lambda}_T)_{T \in \N_+}$ be a non-negative sequence, and $(\tilde{\lambda}_T)_{T \in \N_+}$ be a strictly positive sequence such that $\overline{\lambda}_T \leq \tilde{\lambda}_T$. Furthermore, for any $T \in \N_+$, $t \in [T]$, define $\bx_{T, t} \in \tilde{\mX}$ as $\bx_{T, t} \in \mathrm{arg~max}_{\bx \in \tilde{\mX}} \sigma_{\overline{\lambda}_T^2 \bI_{t-1}}(\bx; \bX_{T, t-1})$, where 
    $\bX_{T, t-1} = (\bx_{T, 1}, \ldots, \bx_{T, t-1})$. Then, for any $T \in \{T \in \N_+ \mid T/2 \geq 3 \gamma_T(\tilde{\lambda}_T^2\bI_T)\}$, the following inequality holds:
    \begin{equation}
    \label{eq:stat_pvu}
        \max_{\bx \in \tilde{\mX}} \sigma_{\overline{\lambda}_T^2 \bI_T}(\bx; \bX_{T, T}) \leq \frac{4}{T} \sqrt{\tilde{\lambda}_T^2 T \gamma_T(\tilde{\lambda}_T^2 \bI_T)}.
    \end{equation}
    \item \textbf{Non-stationary var.:} Let $(\lambda_t)_{t \in \N_+}$ be a non-negative sequence, and $(\tilde{\lambda}_t)_{t \in \N_+}$ be a strictly positive sequence such that $\lambda_t \leq \tilde{\lambda}_t$. Furthermore, for any $t \in \N_+$, define $\bx_{t} \in \tilde{\mX}$ as $\bx_{t} \in \mathrm{arg~max}_{\bx \in \tilde{\mX}} \sigma_{\Sigma_{t-1}}(\bx; \bX_{t-1})$, where $\bX_{t-1} = (\bx_{1}, \ldots, \bx_{t-1})$ and $\Sigma_{t-1} = \mathrm{diag}(\lambda_1^2, \ldots, \lambda_{t-1}^2)$. Then, for any $T \in \{T \in \N_+ \mid T/2 \geq 4 \gamma_T(\tilde{\Sigma}_T)\}$, 
    \begin{equation}
    % \label{eq:non_stat_pvu}
        \max_{\bx \in \tilde{\mX}} \sigma_{\Sigma_T}(\bx; \bX_{T}) \leq \frac{4}{T} \sqrt{\rbr{\sum_{t=1}^T \tilde{\lambda}_t^2} \gamma_T(\tilde{\Sigma}_T)},
    \end{equation}
    where $\tilde{\Sigma}_t = \mathrm{diag}(\tilde{\lambda}_1^2, \ldots, \tilde{\lambda}_t^2)$.
\end{enumerate}
\end{lemma}

To make the above statements explicit, we give the following corollary for $k = \sek$ and $k = \matk$ with the stationary variance parameter as a special case of \lemref{lem:pvu_mvr}. The proof is in Appendix~\ref{sec:uub_pv_proof}.

\begin{corollary}
    \label{cor:pvu_mvr}
    Suppose the assumptions in statement 1 of \lemref{lem:pvu_mvr}. Then, the following four statements hold:
    \begin{enumerate}
        \item Suppose $k = \sek$ and fix any $\alpha > 0$. 
        If $\overline{\lambda}_T^2 = \Omega(\exp(-T^{\frac{1}{d+1}} \ln^{-\alpha} (1+T)))$, Eq.~\eqref{eq:stat_pvu} holds with $\tilde{\lambda}_T^2 = \overline{\lambda}_T^2$ for all $T \geq \overline{T}$, where $\overline{T} < \infty$ is the constant that depends on $\mX$, $\alpha$, $d$, and $\ell$.
        \item Suppose $k = \matk$ with $\nu > 1/2$ and fix any $\alpha > 0$. 
        If $\overline{\lambda}_T^2 = \Omega(T^{-\frac{2\nu}{d}} (\ln (1+T))^{\frac{2\nu (1+\alpha)}{d}})$, Eq.~\eqref{eq:stat_pvu} holds with $\tilde{\lambda}_T^2 = \overline{\lambda}_T^2$ for all $T \geq \overline{T}$, where $\overline{T} < \infty$ is the constant that depends on $\mX$, $\alpha$, $d$, $\ell$, and $\nu$.
        \item Suppose $k = \sek$ and fix any $\alpha > 0$ and $C > 0$. 
        If $\forall T \in \N_+, \overline{\lambda}_T^2 < C\exp\rbr{-T^{\frac{1}{d+1}}\ln^{-\alpha} (1+T)}$ (including $\overline{\lambda}_T = 0$), 
        the following inequality holds for all $T \geq \overline{T}$:
        \begin{equation*}
            \max_{\bx \in \tilde{\mX}} \sigma_{\overline{\lambda}_T^2 \bI_T}(\bx; \bX_{T, T}) \leq O\rbr{\sqrt{\exp\rbr{-T^{\frac{1}{d+1}} \ln^{-\alpha} T }}},
        \end{equation*}
        where $\overline{T} < \infty$ and the implied constant of the above inequality depend on $\mX$, $\alpha$, $d$, $C$, and $\ell$. 
        \item Suppose $k = \matk$ with $\nu > 1/2$ and fix any $\alpha > 0$ and $C > 0$. 
        If $\forall T \in \N_+, \overline{\lambda}_T^2 < C T^{-\frac{2\nu}{d}} (\ln (1+T))^{\frac{2\nu (1+\alpha)}{d}}$ (including $\overline{\lambda}_T = 0$), 
        the following inequality holds for all $T \geq \overline{T}$:
        \begin{equation}
            \max_{\bx \in \tilde{\mX}} \sigma_{\overline{\lambda}_T^2 \bI_T}(\bx; \bX_{T, T}) \leq O\rbr{T^{-\frac{\nu}{d}} (\ln T)^{\frac{\nu (1+\alpha)}{d}}},
        \end{equation}
        where $\overline{T} < \infty$ and the implied constant of the above inequality depend on $\mX$, $\alpha$, $d$, $C$, $\ell$, and $\nu$. 
    \end{enumerate}
    In all of the above statements, $\overline{T}$ increases as decreasing of $\alpha$, 
    and $\overline{T} \rightarrow \infty$ as $\alpha \rightarrow 0$.
\end{corollary}

\paragraph{Comparison with Existing Upper Bound.}
Here, we compare statement 1 in \lemref{lem:pvu_mvr} with the existing upper bound.
By considering the case $\tilde{\lambda}_T^2 = \overline{\lambda}_T^2$, our result shows $O\rbr{T^{-1}\sqrt{\overline{\lambda}_T^2 T \gamma_T(\overline{\lambda}_T^2 \bI_T})}$ upper bound of posterior standard deviation under the sublinear increasing condition of MIG so that $T/2 \geq 3\gamma_T(\overline{\lambda}_T^2\bI_T)$ holds.
On the other hand, the existing analysis of MVR\footnote{Eq.~\eqref{eq:std_pvu} also holds for any algorithm by replacing $\max_{\bx \in \tilde{\mX}} \sigma_{\overline{\lambda}_T^2 \bI_T}(\bx; \bX_{T, T})$ with $\frac{1}{T}\sum_{t=1}^T \sigma_{\overline{\lambda}_T^2 \bI_T}(\bx_{T,t}; \bX_{T, t-1})$. For example, GP-UCB~\cite{srinivas10gaussian,chowdhury2017kernelized} and GP-TS~\cite{chowdhury2017kernelized} use the upper bound of $\frac{1}{T}\sum_{t=1}^T \sigma_{\overline{\lambda}_T^2 \bI_T}(\bx_{T,t}; \bX_{T, t-1})$.} implies 
\begin{align}
\label{eq:std_pvu}
\begin{split}
    \max_{\bx \in \tilde{\mX}} \sigma_{\overline{\lambda}_T^2 \bI_T}(\bx; \bX_{T, T}) 
    \leq 
    \begin{cases}
        O\rbr{\frac{1}{T}\sqrt{\overline{\lambda}_T^{2} T \gamma_T(\overline{\lambda}_T^{2} \bI_T)}} ~~&\mathrm{if}~~ \overline{\lambda}_T^{2} = \Omega(1), \\
        O\rbr{\frac{1}{T}\sqrt{\frac{T \gamma_T(\overline{\lambda}_T^{2} \bI_T)}{\ln \rbr{1 + \overline{\lambda}_T^{-2}}}}} ~~&\mathrm{if}~~\overline{\lambda}_T^{2} = o(1).
    \end{cases}
\end{split}
\end{align}
Since $\overline{\lambda}_T^{2} \leq 1/ \ln(1 + \overline{\lambda}_T^{-2})$, our analysis improves the noise parameter dependence on the decreasing regime of $\overline{\lambda}_T^2$. 
Furthermore, several recent noiseless GP bandit works also derive the related result to \lemref{lem:pvu_mvr} or \corref{cor:pvu_mvr}.
\citet{flynn2024tighter} consider the noiseless setting by relying on elliptical potential count lemma (\lemref{lem:epcl} below), and the naive adaptation of their analysis leads\footnote{
In some settings of $\overline{\lambda}_T^2$, 
the first term of r.h.s. $\gamma_T(\overline{\lambda}_T^2 \bI_T) / T$ can be small by putting $\min\cbr{\overline{\gamma}\rbr{\gamma_T(\overline{\lambda}_T^2\bI_T), \overline{\lambda}_T^2}, \gamma_T(\overline{\lambda}_T^2 \bI_T)}$ in the first term, instead of $\gamma_T(\overline{\lambda}_T^2 \bI_T)$. See \lemref{lem:epcl}.}
\begin{equation*}
    \max_{\bx \in \tilde{\mX}} \sigma_{\overline{\lambda}_T^2 \bI_T}(\bx; \bX_{T, T}) \leq O\rbr{\frac{\gamma_T(\overline{\lambda}_T^2 \bI_T)}{T} + \frac{\sqrt{\overline{\lambda}_T^{2} T \gamma_T(\overline{\lambda}_T^{2} \bI_T)}}{T}}.
\end{equation*}
In the above equation, the decreasing speed of $\overline{\lambda}_T^2$ is more restricted than that of \corref{cor:pvu_mvr} to obtain the same order upper bound as Eq.~\eqref{eq:stat_pvu}. For example, if $k = \matk$ and $\overline{\lambda}_T^2 = \Omega(T^{-\frac{2\nu}{d}} (\ln (1+T))^{\frac{2\nu (1+\alpha)}{d}})$ as with the condition of statement 4 in \corref{cor:pvu_mvr}, 
the first term $\frac{\gamma_T(\overline{\lambda}_T^2 \bI_T)}{T}$ 
dominates the second term since the existing upper bound of MIG implies $\frac{\gamma_T(\overline{\lambda}_T^2 \bI_T)}{T} = \tilde{O}(1)$.
Furthermore, \citet{salgiarandom} derives a similar result to our statement 4 with a random sampling algorithm instead of MVR. The main theoretical advantage of our result is the constant $\overline{T}$ depends on entire input space $\mX$ instead of the subset $\tilde{\mX}$, while that of \cite{salgiarandom} has the dependence on $\tilde{\mX}$.
This dependence on $\tilde{\mX}$ raises the requirement of the additional level-set assumption to apply the PE-style algorithm in noiseless feedback (see Assumption~4.2 in \cite{salgiarandom}). 
Furthermore, as described in the proof sketch below, our proof mainly relies on the simple extension of the well-known information gain arguments from \cite{srinivas10gaussian}, not on the technique of \cite{salgiarandom} that involves the theoretical tools from function approximation literature.

\paragraph{Proof sketch of \lemref{lem:pvu_mvr}.}
Here, since statement 2 is derived as the extension of the proof of statement 1, we only describe the proof sketch of statement 1 for simplicity. We leave the full proof, including statement 2, in Appendix~\ref{sec:uub_pv_proof}. Our proof is based on the following two observations:
\begin{enumerate}
    \item For any index set $\mT \subset [T]$, $\max_{\bx \in \tilde{\mX}} \sigma_{\overline{\lambda}_T^2 \bI_T}(\bx; \bX_{T, T})$ can be bounded from above by the average observed posterior standard deviation on $\mT$ from the definition of MVR. Namely, $\max_{\bx \in \tilde{\mX}} \sigma_{\overline{\lambda}_T^2 \bI_T}(\bx; \bX_{T, T}) \leq \frac{1}{|\mT|} \sum_{t \in \mT} \sigma_{\overline{\lambda}_T^2 \bI_{t-1}}(\bx_{T,t}; \bX_{T, t-1})$ holds.
    \item If we set $\mT = \{t \in [T] \mid \tilde{\lambda}_T^{-1} \sigma_{\tilde{\lambda}_T^2 \bI_{t-1}}(\bx_{T,t}; \bX_{T, t-1}) \leq 1\}$, 
    \begin{align}
    \begin{split}
        \sigma_{\tilde{\lambda}_T^2 \bI_{t-1}}^2(\bx_{T,t}; \bX_{T, t-1}) 
        &= \tilde{\lambda}_T^2 \min\cbr{1, \tilde{\lambda}_T^{-2}\sigma_{\tilde{\lambda}_T^2 \bI_{t-1}}^2(\bx_{T,t}; \bX_{T, t-1})} \\
        &\leq 2 \tilde{\lambda}_T^2 \ln \rbr{1 + \tilde{\lambda}_T^{-2}\sigma_{\tilde{\lambda}_T^2 \bI_{t-1}}^2(\bx_{T,t}; \bX_{T, t-1})}.
    \end{split}
    \end{align}
    for all $t \in \mT$. We use $\forall a \geq 0, \min\{1, a\} \leq 2 \ln(1 + a)$ in the last line.
    By relying on the standard MIG-based analysis (e.g., Theorem 5.3, 5.4 in \cite{srinivas10gaussian}) and the assumption $\overline{\lambda}_T \leq \tilde{\lambda}_T$, the above inequality implies
    \begin{align}
        \sum_{t \in \mT} \sigma_{\overline{\lambda}_T^2 \bI_{t-1}}(\bx_{T,t}; \bX_{T, t-1}) \leq 2 \sqrt{\tilde{\lambda}_T^2 T \gamma_T(\tilde{\lambda}_T^2 \bI_T)}.
    \end{align}
\end{enumerate}
From the above two observations, the remaining interest is the increasing speed of $|\mT|$. We use the following lemma, which is 
called the \emph{elliptical potential count} lemma in \cite{flynn2024tighter}, as the analogy of elliptical potential arguments in linear bandits.

\begin{lemma}[Elliptical potential count lemma, Lemma D.9 in \cite{flynn2024tighter}]
    \label{lem:epcl}
    Fix any $T \in N_+$, any sequence $\bx_1, \ldots, \bx_T \in \mX$, and any $\overline{\lambda} > 0$.
    Set $\mT^c$ as $\mT^c = \{t \in [T] \mid \overline{\lambda}^{-1} \sigma_{\overline{\lambda}^2 \bI_{t-1}}(\bx_{t}; \bX_{t-1}) > 1\}$, where 
    $\bX_t = (\bx_1, \ldots, \bx_t)$.
    Then, the number of elements of $\mT^c$ satisfies $|\mT^c| \leq \min \cbr{3 \overline{\gamma}\rbr{3 \gamma_T(\overline{\lambda}^2 \bI_T), \overline{\lambda}^2}, 3 \gamma_T(\overline{\lambda}^2 \bI_T)}$.
    Furthermore, $\overline{\gamma}(\cdot, \cdot)$ is any monotonic upper bound of MIG defined on $\R_+ \times \R_+$, which satisfies $\forall T \in \N_+, \lambda > 0, \gamma_T(\lambda^2 \bI_T) \leq \overline{\gamma}(T, \lambda^2)$ and $\forall \lambda > 0, T \geq 1, \epsilon \geq 0, \overline{\gamma}(T, \lambda^2) \leq \overline{\gamma}(T + \epsilon, \lambda^2)$.
\end{lemma}
From the above lemma, we obtain the lower bound of $\mT$ as $|\mT| \geq T - 3\gamma_T(\tilde{\lambda}_T^2 \bI_T)$. 
Finally, we obtain the desired result by noting $|\mT| \geq T/2$ holds for any $T \in \{T \in \N_+ \mid T/2 \geq 3 \gamma_T(\tilde{\lambda}_T^2\bI_T)\}$.

\section{Noiseless Setting}
\label{sec:nls}
As a first application of our result, we study a noiseless setting; namely, we focus on the setting where $\rho_t = 0$ for all $t \in \N_+$ in \asmpref{asmp:noise}. 
The following results show our cumulative and simple regret guarantees for PE and MVR.
\begin{theorem}[Cumulative Regret Bound for PE.]
    \label{thm:cr_pe_nl}
    Suppose Assumptions \ref{asmp:smoothness} and \ref{asmp:noise} hold with $\rho_t = 0$ for all $t \in \N_+$. Furthermore, assume $B$, $d$, $\ell$, and $\nu$ are $\Theta(1)$.
    Then, when running \algoref{alg:pe} with $\beta^{1/2} = B$, $\lambda = 0$, and any fixed $N_1 \in \N_+$, the following statements hold:
    \begin{itemize}
        \item If $k = \sek$, $R_T = O(\ln T)$.
        \item If $k = \matk$ with $\nu > 1/2$,
        \begin{equation}
            R_T = \begin{cases}
                \tilde{O}(T^{\frac{d - \nu}{d}})~~&\mathrm{if}~~\nu < d, \\
                O((\ln T)^{2+\alpha})~~&\mathrm{if}~~\nu = d, \\
                O(\ln T)~~&\mathrm{if}~~\nu > d.
            \end{cases}
        \end{equation}
        Here, $\alpha > 0$ is an arbitrarily fixed constant.
    \end{itemize}
\end{theorem}

\begin{theorem}[Simple Regret Bound for MVR.]
    \label{thm:sr_mvr_nl}
    Suppose the same conditions as those of \thmref{thm:cr_pe_nl}. Then, when running \algoref{alg:mvr} with $\lambda = 0$, the following statements hold:
    \begin{itemize}
        \item If $k = \sek$, $r_T = O\rbr{\exp\rbr{-\frac{1}{2}T^{\frac{1}{d+1}} \ln^{-\alpha} T}}$.
        \item If $k = \matk$ with $\nu > 1/2$, $r_T = \tilde{O}\rbr{T^{-\frac{\nu}{d}}}$.
    \end{itemize}
\end{theorem}

\begin{remark}
    \label{rem:disc}
    The above theorems assume that the learner can exactly choose $\bx_t$ in the algorithms, which is unreasonable for continuous domain $\mX$. 
    However, the similar guarantee, which is worse by additional $\sqrt{\ln T}$ multiplicative factor than the above results, can be obtained by the existing analysis~\citep{li2022gaussian} under the additional Lipschitz assumption for $f$.
    % However, as with the existing PE literature (e.g., \cite{li2022gaussian}), with the additional Lipschitz assumption for $f$, 
    % we can obtain the same guarantee by the discretization of the input domain $\mX$ with $1/T$-net, while additional $\sqrt{\ln T}$ factor is multiplied.
    Note that such Lipschitz assumption for $f$ automatically holds under fixed $B$ when we set $k = \sek$ or $k = \matk$ with $\nu > 1$~\cite{lee2022multi}.
\end{remark}

The proof of Theorems~\ref{thm:cr_pe_nl} and \ref{thm:sr_mvr_nl} are respectively derived 
by directly following standard analysis of PE and MVR with statements 3 and 4 of \corref{cor:pvu_mvr}. 
We describe full proofs in Appendix~\ref{sec:nl_proof} for completeness.

\paragraph{Discussion.}
As summarized in Tables~\ref{tab:nl_cr_compare}--\ref{tab:nl_sr_compare}, our results are the same as or superior to the best-known upper bounds 
in almost all cases. The only exception is the simple regret with $k = \sek$, whose polynomial factor in exponential gets worse from $-(\alpha+1/d)$ into $-1/(d+1)$, compared to the algorithm of \cite{kim2024bayesian}. Roughly speaking, the numerator of the factor $-1/(d+1)$ in our analysis 
comes from the exponent of the upper bound of MIG $O(\ln^{d+1}(T/\lambda^2))$.  
We expect our simple regret has room of improvement from $\tilde{O}(\exp(-T^{\frac{1}{d+1}} \ln^{-\alpha} T))$ into $\tilde{O}(\exp(-T^{\frac{2}{d}} \ln^{-\alpha} T))$ in future work,
since the conjectured best upper bound of MIG is $O(\ln^{d/2}(T/\lambda^2))$ from the regret lower bound~\cite{scarlett2017lower}.

\section{Optimal Dependence of RKHS Norm Upper Bound}
\label{sec:rkhs_od}
As the second application of our result, we consider improving the existing dependence of RKHS norm upper bound $B$ in the regret upper bounds.
\subsection{Simple Regret}
The following theorem shows our results for simple regret.

\begin{theorem}[Simple Regret Bound for MVR.]
    \label{thm:sr_mvr_rkhs}
    Suppose Assumptions \ref{asmp:smoothness} and \ref{asmp:noise} hold with $\rho_t = \rho > 0$ for all $t \in \N_+$. 
    Furthermore, assume $\rho$, $d$, $\ell$, and $\nu$ are $\Theta(1)$, and $\mX$ is finite.
    Then, when running \algoref{alg:mvr} with $\lambda^2 = \Theta(B^{-2})$, the following statements hold for any fixed $\alpha > 0$ with probability at least $1-\delta$:
    \begin{itemize}
        \item If $k = \sek$, $B = O\rbr{\exp\rbr{T^{\frac{1}{d+1}}\ln^{-\alpha} (1+T)}}$, and $T \geq \overline{T}$, then, $r_T = O\rbr{\sqrt{\frac{\ln^{d+1} (TB^2)}{T}}}$.
        Here, $\overline{T}$ is the constant, defined in statement 1 of \corref{cor:pvu_mvr}.
        \item If $k = \matk$ with $\nu > 1/2$, $B = O\rbr{T^{\frac{\nu}{d}} \ln^{\frac{-\nu(1+\alpha)}{d}} (1+T)}$, and $T \geq \overline{T}$, then, $r_T = \tilde{O}\rbr{B^{\frac{d}{2\nu + d}}T^{-\frac{\nu}{2\nu + d}}}$.
        Here, $\overline{T}$ is the constant defined in statement 2 of \corref{cor:pvu_mvr}.
    \end{itemize}
\end{theorem}

The important point is that the setting of the noise parameter $\lambda^2 = \Theta(1/B^2)$ depends on $B$.
We describe the full proof of the above theorem in Appendix~\ref{sec:rkhs_od_proof}.


\begin{remark}
    If we allow additional logarithmatic factor, we can eliminate the finiteness assumption of $\mX$ in \thmref{thm:sr_mvr_rkhs} by relying on the discretization with $1/T$-net as with \remref{rem:disc}.
    The notable point we have to care about is that the Lipschitz constant is given as $B$ in the existing result~\cite{lee2022multi}.
    Therefore, the extension of \thmref{thm:sr_mvr_rkhs} for continuous domain 
    requires $1/(BT)$-net to maintain the order of $B$, and the resulting regret upper bound suffers from additional $\sqrt{\ln (TB)}$ factor, instead of $\sqrt{\ln T}$ factor derived from the standard discretizing argument that does not care the dependence of $B$~\cite{li2022gaussian}.
\end{remark}

\paragraph{Discussion.}
In both kernels, the polynomial dependence of $B$ matches the lower bound 
in \cite{scarlett2017lower}, while there exists room for improvement in the logarithmic factor.
On the other hand, there exist some exceptional cases that Theorem~\ref{thm:sr_mvr_rkhs} does not cover, 
even though its lower bound of the simple regret is guaranteed to converge to $0$.
For example, when $B = \Theta(T^{\frac{\nu}{d}} \ln^{-\frac{\nu}{d}} 
T)$, the lower bound of \cite{scarlett2017lower} 
suggest that some algorithm find any $\epsilon$-optimal point for sufficiently large $T$ (namely, simple regret converges to $0$), 
while violating our assumption. As with the discussion in \secref{sec:nls}, 
this limitation can be eliminated in the future if the upper bound of MIG matches the conjectured best upper bound.

\subsection{Cumulative Regret}

The following theorem also shows the PE algorithm can achieve optimal dependence of $B$ up to a poly-logarithmic factor.
\begin{theorem}[Cumulative Regret Bound for PE.]
    \label{thm:cr_pe_rkhs}
    Suppose Assumptions \ref{asmp:smoothness} and \ref{asmp:noise} hold with $\rho_t = \rho > 0$ for all $t \in \N_+$. 
    Furthermore, assume $\rho$, $d$, $\ell$, and $\nu$ are $\Theta(1)$, and $\mX$ is finite.
    Then, when running \algoref{alg:pe} with $\beta^{1/2} = (B + \rho \lambda^{-1})\sqrt{2 \ln \frac{2|\mX|(1 + \log_2 T)}{\delta}}$, $\lambda^2 = \Theta(B^{-2})$, and any fixed $N_1 \in \N_+$, the following statements hold with probability at least $1-\delta$:
    \begin{itemize}
        \item If $k = \sek$ and $B = O(\sqrt{T})$, then, $R_T = O\rbr{(\ln T) \sqrt{T \rbr{\ln^{d+1} (TB^2)} \rbr{\ln \frac{|\mX|}{\delta}}}}$.
        \item If $k = \matk$ with $\nu > 1/2$ and $B = O\rbr{T^{\frac{2 \nu^2 + 3 \nu d }{ 4 d^2 + 4\nu^2 + 6\nu d }}}$, then, $R_T = \tilde{O}\rbr{T^{\frac{\nu+d}{2\nu+d}} B^{\frac{d}{2\nu+d}}}$.
    \end{itemize}
\end{theorem}

See Appendix~\ref{sec:rkhs_od_proof} for the proof.
In contrast to the analysis of the MVR, the analysis of PE cannot 
leverage \corref{cor:pvu_mvr} by setting $\lambda^2 = \Theta(B^{-2})$.
Intuitively, this is because the existence of the common constant $\overline{T}$ over each batch is not guaranteed since $\lambda^2$ depends only on $T$, 
not the total step size of each batch. Due to this limitation, the above result is proved by leveraging \lemref{lem:epcl} as with the analysis of \cite{flynn2024tighter}, instead of using \corref{cor:pvu_mvr}. 
As a result, the conditions about $B$ are more restricted 
than those of \thmref{thm:sr_mvr_rkhs}, due to the fundamental limitation 
of the analysis of \cite{flynn2024tighter} as previously discussed in \secref{sec:uub_pv}. For example, if $k = \matk$, the increasing speed of $B = O\rbr{T^{\frac{2 \nu^2 + 3 \nu d }{ 4 d^2 + 4\nu^2 + 6\nu d }}}$ in 
\thmref{thm:cr_pe_rkhs} is more restricted than $B = \tilde{O}\rbr{T^{\frac{\nu}{d}}}$ in \thmref{thm:sr_mvr_rkhs}, regardless the fact that lower bound~\cite{scarlett2017lower} suggest the sublinear cumulative regret is achievable when $B = o(T^{\frac{\nu}{d}})$.
We leave future research to break this limitation.

\section{Non-Stationary Variance}
\label{sec:nsv}
As the third application of our result, we consider the non-stationary
variance setting, which falls between a noiseless and noisy regime. 
In this setting, our goal is to quantify the regret by the cumulative variance proxy $V_T = \sum_{t \in [T]} \rho_t^2$.
That is, we aim to construct an algorithm that achieves better performance than the one for the stationary noise setting if $V_T$ increases sublinearly.
While the non-stationary variance setting has already been studied and motivated in the linear bandits~\cite{zhou2021nearly}, to our knowledge, no existing GP-bandits literature exists for this problem.
Therefore, in \appref{sec:pot_app_nsv}, we describe some potential applications to motivate non-stationary variance setting in GP-bandits.

By following the existing works~\cite{zhou2021nearly,zhang2021improved,zhou2022computationally}, we suppose that the learner can access 
true variance proxy $\rho_t^2$ at the end of step $t$. 
We leave the unknown $\rho_t^2$ setting for future research.
Note that, as described later, the direct extension of the existing linear bandit algorithm with non-stationary variance does not lead to the near-optimal guarantee.


\paragraph{Lower bound.}
Since the stationary noise problem with $\rho^2 = V_T/T$ is 
subsumed in the non-stationary problem with the cumulative
variance proxy $V_T$, the following lower bounds are obtained as the corollary of the existing stationary variance lower bound~\cite{scarlett2017lower} 
and noiseless lower bound~\cite{bull2011convergence}.

\begin{corollary}[Lower bound for cumulative regret]
    \label{cor:cr_nsv_lower}
    Let $\mX$ be $\mX = [0, 1]^d$. Furthermore, assume $V_T = \Omega(1)$ and $V_T = O(T^2)$ with sufficiently small implied constant.
    Then, for any algorithm, there exists a GP bandit problem instance that satisfies Assumptions~\ref{asmp:smoothness} and \ref{asmp:noise} with $\sum_{t \in [T]} \rho_t^2 = V_T$ and the following two statements:
    \begin{itemize}
        \item If $k = \sek$, $\Ep[R_T] = \Omega\rbr{\sqrt{V_T \ln^{\frac{d}{2}} \frac{T^2}{V_T}}}$.
        \item If $k = \matk$, $\Ep[R_T] = \Omega\rbr{V_T^{\frac{\nu}{2\nu + d}} T^{\frac{d}{2\nu + d}}}$.
    \end{itemize}
    Here, we assume $B$, $d$, $\ell$, and $\nu$ are $\Theta(1)$.
\end{corollary}

\begin{corollary}[Lower bound for simple regret]
    \label{cor:sr_nsv_lower}
    Let $\mX$ be $\mX = [0, 1]^d$. Furthermore, assume $V_T = \Omega(1)$.
    If there exists an algorithm such that $\Ep[r_T] \leq \epsilon$ hold for all problem instances 
    that satisfy Assumptions~\ref{asmp:smoothness} and \ref{asmp:noise} with $\sum_{t \in [T]} \rho_t^2 = V_T$, then:
    \begin{itemize}
        \item For $k = \sek$, the total step size $T$ needs to satisfy $T \geq \Omega\rbr{\sqrt{\frac{V_T}{\epsilon^2} \ln^{\frac{d}{2}} \frac{1}{\epsilon}}}$.
        \item For $k = \matk$, the total step size $T$ needs to satisfy 
        \begin{equation*}
            T \geq \begin{cases}
        \Omega\rbr{\sqrt{\frac{V_T}{\epsilon^2} \rbr{\frac{1}{\epsilon}}^{\frac{d}{\nu}}}} ~&\mathrm{if}~~d \leq 2\nu ~\mathrm{or}~V_T = \Omega(T^{\frac{d-2\nu}{d}}), \\
        \Omega\rbr{\rbr{\frac{1}{\epsilon}}^{\frac{d}{\nu}}} ~&\mathrm{if}~~d > 2\nu ~\mathrm{and}~V_T = O(T^{\frac{d-2\nu}{d}}).
        \end{cases}
        \end{equation*}
    \end{itemize}
    Here, we assume $B$, $d$, $\ell$, and $\nu$ are $\Theta(1)$.
    Furthermore, $\epsilon > 0$ is a sufficiently small constant.
\end{corollary}
%
The lower bound $\Omega\rbr{\rbr{1 / \epsilon}^{\frac{d}{\nu}}}$ for $\matk$ if $d > 2\nu$ and $V_T = O(T^{\frac{d-2\nu}{d}})$ in Corollary~\ref{cor:sr_nsv_lower} come from \cite{bull2011convergence}, and others come from \cite{scarlett2017lower}.

Note that the noiseless lower bound for expected regret also holds for noisy settings since an expected regret in the noisy setting can always be reduced to one in the noiseless setting, whose algorithm randomness is induced by observation noise. 
Interestingly, the above simple regret lower bound indicates 
that if $d > 2\nu$ and $V_T = O(T^{\frac{d-2\nu}{d}})$,
the non-stationary variance setting may have the same level of difficulty as that of the noiseless problem. 
Our VA-MVR algorithm proposed below justifies this fact 
by providing simple regret upper bound matching the above lower bound.

\paragraph{Algorithm.}
Algorithms~\ref{alg:vape} and \ref{alg:vamvr} in Appendix~\ref{sec:pseudo_code} show PE and MVR-based algorithms for non-stationary variance problems, which we call variance-aware PE (VA-PE) and MVR (VA-MVR), respectively.
The algorithms themselves are the variant of the standard PE or MVR algorithms that directly set the true variance proxy $\rho_t^2$ to the noise variance parameter $\lambda_t^2$ for the
heteroscedastic GP model.

\paragraph{Theoretical analysis.}
The following theorems give the cumulative and simple regret guarantees for VA-PE and VA-MVR, respectively.

\begin{theorem}[Cumulative regret upper bound for VA-PE]
\label{thm:vape}
Suppose Assumptions~\ref{asmp:smoothness} and \ref{asmp:noise}, and $|\mX| < \infty$ holds. Furthermore, assume $V_T \coloneqq \sum_{t \in [T]} \rho_t^2 = \Omega(1)$. Then, when running Algorithm~\ref{alg:vape} with $\beta^{1/2} = \rbr{B + \sqrt{2 \ln \frac{2|\mX|(1 + \log_2 T)}{\delta}}}$, with probability at least $1-\delta$, $R_T = O\rbr{(\ln T)\sqrt{V_T \rbr{\ln^{d+1}\frac{T^2}{V_T}} \rbr{\ln \frac{|\mX|}{\delta}}}}$ if $k = \sek$. Furthermore, if $k = \matk$,
\begin{equation*}
R_T = \begin{cases}
        \tilde{O}\rbr{V_T^{\frac{\nu}{2\nu+d}} T^{\frac{d}{2\nu+d}}} ~~&\mathrm{if}~~d \leq 2\nu, \\
        \tilde{O}\rbr{V_T^{\frac{\nu}{2\nu+d}} T^{\frac{d}{2\nu+d}}} ~~&\mathrm{if}~~d > 2\nu, V_T = \Omega\rbr{T^{\frac{d - 2\nu}{d}}}, \\
        \tilde{O}\rbr{T^{\frac{d-\nu}{d}}} ~~&\mathrm{if}~~d > 2\nu, V_T = O\rbr{T^{\frac{d - 2\nu}{d}}}.
    \end{cases}    
\end{equation*}
    
\end{theorem}

\begin{theorem}[Simple regret upper bound for VA-MVR]
\label{thm:vamvr}
    Suppose Assumptions~\ref{asmp:smoothness} and \ref{asmp:noise}, and $|\mX| < \infty$ holds. Furthermore, assume $V_T  = \Omega(1)$. Then, when running Algorithm~\ref{alg:vamvr}, with probability at least $1-\delta$, $r_T = O\rbr{\sqrt{\frac{V_T}{T^2} \rbr{\ln^{d+1}\frac{T^2}{V_T}} \rbr{\ln \frac{|\mX|}{\delta}}}}$ if $k = \sek$. Furthermore, if $k = \matk$,
\begin{equation*}
r_T = \begin{cases}
        \tilde{O}\rbr{V_T^{\frac{\nu}{2\nu+d}} T^{-\frac{2\nu}{2\nu+d}}} ~~&\mathrm{if}~~d \leq 2\nu, \\
        \tilde{O}\rbr{V_T^{\frac{\nu}{2\nu+d}} T^{-\frac{2\nu}{2\nu+d}}} ~~&\mathrm{if}~~d > 2\nu~\mathrm{and}~V_T = \Omega\rbr{T^{\frac{d - 2\nu}{d}}}, \\
        \tilde{O}\rbr{T^{-\frac{\nu}{d}}} ~~&\mathrm{if}~~d > 2\nu~\mathrm{and}~V_T = O\rbr{T^{\frac{d - 2\nu}{d}}}.
    \end{cases}    
\end{equation*}
\end{theorem}

In both results, the regret upper bound matches the lower bound up to the logarithmic factor, except 
for the cumulative regret guarantee for $d > 2\nu$, $V_T = O\rbr{T^{\frac{d - 2\nu}{d}}}$ in $k = \matk$.
However, note that the resulting regret $\tilde{O}\rbr{T^{\frac{d-\nu}{d}}}$ in this exceptional case matches the conjectured lower bound~\cite{vakili2022open}; Therefore, as with our simple regret lower bound, $\tilde{O}\rbr{T^{\frac{d-\nu}{d}}}$ upper bound in our analysis has no room for improvement if the conjectured lower bound in \cite{vakili2022open} is true.

\paragraph{Comparison with the stationary setting.}
When $V_T = \Theta(T)$, our result matches the 
existing $\tilde{O}(T^{\frac{\nu+d}{2\nu+d}})$ upper bound for the stationary setting. If we consider the setting that $V_T$ increases sublinearly, our algorithm achieves a smaller regret than the existing stationary lower bounds. For example, 
if $V_T = \Theta(1)$ in $k = \sek$, the resulting regret becomes logarithmcally increasing regret $R_T = \tilde{O}(1)$, while the regret lower bound for stationary variance setting is $\tilde{\Omega}(\sqrt{T})$.

\paragraph{Comparison with the algorithm in heteroscedastic linear bandits.}
In heteroscedastic linear bandits, the weighted OFUL+ algorithm, which is known to achieve nearly optimal regret with UCB-based algorithm construction, is proposed in the known $\rho_t^2$ setting.
We can also consider the extension of weighted OFUL+ to the GP bandits by constructing a UCB-based score with a heteroscedastic GP model. We call this extension variance-aware GP-UCB (VA-GP-UCB), 
and give the details in Appendix~\ref{sec:va_gp_ucb_proof}.
However, the regret of VA-GP-UCB becomes strictly worse than VA-PE and VA-MVR due to the following two reasons: Firstly, as with the stationary adaptive confidence bound (e.g., Lemma~3.11 in \cite{abbasi2013online}), the existing adaptive confidence bound for heteroscedastic GP-model~\cite{kirschner18heteroscedastic} contains $O(\sqrt{\gamma_t(\Sigma_t)})$ factor 
in the confidence width parameter, which leads to the sub-optimal order of the regret. Secondly, our analysis of VA-GP-UCB relies on the extension of the elliptical potential count lemma (\lemref{lem:epcl_nst}) to heteroscedastic GP model, which could result in worse dependence of the noise variance parameters than that of \lemref{lem:pvu_mvr} (See the discussion in \secref{sec:uub_pv}). To our knowledge, the existing technical tools lead no direct way to avoid the above two issues. 

Finally, we give the summary of our results for the non-stationary variance setting in Tables~\ref{tab:cr_nsv_compare}--\ref{tab:sr_eps_nsv_compare} in \appref{sec:add_tab}.


\section{Conclusion}
We study the GP-bandit problem with the following three settings:
(i) noiseless observation, (ii) varying RKHS norm, and (iii) non-stationary variance setting. We first propose the new uniform upper bound of the posterior standard deviation of GP in the MVR algorithm. By leveraging this upper bound, we refine the regret guarantee of the existing PE and MVR algorithms. Our derived upper bound matches the lower bound up to the logarithmic factor in the aforementioned three settings.

% \section*{Impact Statement}

% This paper presents work whose goal is to advance the field of 
% Machine Learning. There are many potential societal consequences 
% of our work, none of which we feel must be specifically highlighted here.


\section*{Acknowledgements}
This work was supported by JST ACT-X Grant Number JPMJAX23CD, JST PRESTO Grant Number JPMJPR24J6, JSPS KAKENHI Grant Number (JP23K19967 and JP24K20847), and RIKEN Center for Advanced Intelligence Project.


\bibliographystyle{plainnat}
\bibliography{main}

\clearpage
\onecolumn
\appendix

\section{Additional Table Summary}
\label{sec:add_tab}
We describe the additional table to summarize the results of the existing analysis and ours. 
\tabref{tab:nl_sr_compare} summarizes the results of the simple regret upper bounds.
Furthermore, Tables~\ref{tab:cr_rkhs_compare}--\ref{tab:sr_eps_rkhs_compare} and Tables~\ref{tab:cr_nsv_compare}--\ref{tab:sr_eps_nsv_compare} show the results for the setting of \secref{sec:rkhs_od} and \secref{sec:nsv}, respectively.

\begin{table*}[tb]
    \centering
    \caption{Comparison between existing noiseless algorithms' guarantees for simple regret and our result. 
    In all algorithms except for GP-UCB+ and EXPLOIT+, the smoothness parameter of the Mat\'ern kernel is assumed to be $\nu > 1/2$.}
    \begin{tabular}{c|c|c|c|l}
    \multirow{2}{*}{Algorithm} & \multirow{2}{*}{Regret (SE)} & \multirow{2}{*}{Regret (Mat\'ern)} & \multirow{2}{*}{Type} & \multirow{2}{*}{Remark} \\
     &  & & &  \\ \hline \hline
    GP-EI & \multirow{2}{*}{N/A} & \multirow{2}{*}{$\tilde{O}\rbr{T^{-\frac{\min\{1, \nu\}}{d}}}$} & \multirow{2}{*}{D} & \\ 
    \cite{bull2011convergence} &  &  & & \\ \hline
    GP-EI with $\epsilon$-Greedy & \multirow{2}{*}{N/A} & \multirow{2}{*}{$\tilde{O}\rbr{T^{-\frac{\nu}{d}}}$} & \multirow{2}{*}{P} & \\ 
    \cite{bull2011convergence} &  &  & & \\ \hline
    GP-UCB & \multirow{3}{*}{$O\rbr{\sqrt{\frac{\ln^{d} T}{T}}}$} & \multirow{3}{*}{$\tilde{O}\rbr{T^{-\frac{\nu}{2\nu + d}}}$} & \multirow{3}{*}{D} & \\
    \cite{lyu2019efficient} &  &  & & \\
    \cite{kim2024bayesian} &  &  & & \\ \hline
    Kernel-AMM-UCB & \multirow{2}{*}{$O\rbr{\frac{\ln^{d+1} T}{T}}$} & \multirow{2}{*}{$\tilde{O}\rbr{T^{-\frac{\nu d + 2\nu^2}{2\nu^2 + 2\nu d + d^2}}}$} & \multirow{2}{*}{D} & \\
    \cite{flynn2024tighter} &  &  & & \\ \hline
    GP-UCB+,  & \multirow{3}{*}{$O\rbr{\exp\rbr{-CT^{\frac{1}{d}-\alpha}}}$} & \multirow{3}{*}{$O\rbr{T^{-\frac{\nu}{d}+\alpha}}$} & \multirow{3}{*}{P} & $\alpha > 0$ is an arbitrarily \\ 
    EXPLOIT+ & & & & fixed constant.  \\ 
    \cite{kim2024bayesian} & & & & $C > 0$ is some constant. \\ \hline
    \textbf{MVR} & \multirow{2}{*}{$O\rbr{\exp\rbr{-\frac{1}{2}T^{\frac{1}{d+1}}\ln^{-\alpha} T}} $} & \multirow{2}{*}{$\tilde{O}\rbr{T^{-\frac{\nu}{d}}}$} & \multirow{2}{*}{D} & $\alpha > 0$ is an arbitrarily fixed \\
    \textbf{(our analysis)} & & & & constant. \\ \hline
    Lower Bound & \multirow{2}{*}{N/A} & \multirow{2}{*}{$\Omega\rbr{T^{-\frac{\nu}{d}}}$} & \multirow{2}{*}{N/A} & \\
    \cite{bull2011convergence} & & & & \\
    \end{tabular}
    \label{tab:nl_sr_compare}
\end{table*}

\begin{table*}[tb]
    \centering
    \caption{Comparison of expected cumulative regret upper bound between existing noisy algorithms' guarantees and our result in the regime where the RKHS norm upper bound $B$ may change along with $T$. 
    Here, $d$, $\ell$, $\nu$, and the noise level $\rho^2$ are supposed to be $\Theta(1)$. 
    Note that the table below describes the expected regret by setting confidence level $\delta = 1/T$ and $\delta = 1/(TB)$ in existing PE and our PE, respectively. The resulting regrets for existing PE and our PE respectively suffer from additional $O(\sqrt{\ln T})$ and $O(\sqrt{\ln TB^2})$ factors in high-probability regret upper bound of PE.
    }
    \begin{tabular}{c|c|c}
        Algorithm & Cumulative Regret (SE) & Cumulative Regret (Mat\'ern) \\ \hline \hline
          GP-UCB & \multirow{2}{*}{$O\rbr{B \sqrt{T \ln^{d+1} T} + \sqrt{T}\ln^{d+1} T}$} & \multirow{2}{*}{$\tilde{O}\rbr{BT^{\frac{\nu + d}{2\nu + d}} +
          T^{\frac{2\nu + 3d}{4\nu + 2d}}}$}  \\ 
          \cite{srinivas10gaussian} & & \\ \hline
          Existing PE & \multirow{2}{*}{$O\rbr{\max\cbr{B, 1} \sqrt{T \ln^{d+4} T}}$} & \multirow{2}{*}{$\tilde{O}\rbr{\max\cbr{B, 1} T^{\frac{\nu + d}{2\nu + d}}}$}  \\ 
          \cite{li2022gaussian} & &  \\ \hline
          \textbf{PE} & \multirow{2}{*}{$O\rbr{\sqrt{T \ln^{d+2} (TB^2)}(\ln T)} $} & \multirow{2}{*}{$\tilde{O}\rbr{B^{\frac{d}{2\nu+d}} T^{\frac{\nu + d}{2\nu + d}}}$}  \\ 
          \textbf{(our analysis)} & & \\ \hline
          Lower bound & \multirow{2}{*}{$\Omega\rbr{\sqrt{T \ln^{\frac{d}{2}} (TB^2)}}$} & \multirow{2}{*}{$\Omega\rbr{B^{\frac{d}{2\nu+d}} T^{\frac{\nu+d}{2\nu+d}}}$}  \\ 
          \cite{scarlett2017lower} & & 
        \end{tabular}
    \label{tab:cr_rkhs_compare}
\end{table*}

\begin{table*}[tb]
    \centering
    \caption{Comparison between existing noisy algorithms' guarantees and our result in the regime where the RKHS norm upper bound $B$ may change along with $T$. 
    Here, $d$, $\ell$, $\nu$, and the noise level $\rho^2$ are supposed to be $\Theta(1)$. As with \tabref{tab:cr_rkhs_compare}, note that the resulting regret upper bounds for existing MVR and ours suffer from additional logarithmic factors in high-probability regret upper bound.
    }
    \begin{tabular}{c|c|c}
        Algorithm & Simple regret (SE) & Simple regret (Mat\'ern) \\ \hline \hline
          GP-UCB & \multirow{2}{*}{$O\rbr{B \sqrt{\frac{\ln^{d+1} T}{T}} + \frac{\ln^{d+1} T}{\sqrt{T}}}$} & \multirow{2}{*}{$\tilde{O}\rbr{BT^{-\frac{\nu}{2\nu + d}} +
          T^{-\frac{2\nu - d}{4\nu + 2d}}}$}  \\ 
          \cite{srinivas10gaussian} & & \\ \hline
          Existing MVR & \multirow{2}{*}{$O\rbr{\max\cbr{B, 1} \sqrt{\frac{\ln^{d+2} T}{T}}}$} & \multirow{2}{*}{$\tilde{O}\rbr{\max\cbr{B, 1} T^{-\frac{\nu}{2\nu+d}}}$}  \\ 
          \cite{vakili2021optimal} & &  \\ \hline
          \textbf{MVR} & \multirow{2}{*}{$O\rbr{\sqrt{\frac{\ln^{d+2}(TB^2)}{T}}} $} & \multirow{2}{*}{$\tilde{O}\rbr{B^{\frac{d}{2\nu+d}} T^{-\frac{\nu}{2\nu + d}}}$}  \\ 
          \textbf{(our analysis)} & & 
        \end{tabular}
    \label{tab:sr_rkhs_compare}
\end{table*}

\begin{table*}[tb]
    \centering
    \caption{Comparison of the total time step condition to find expected $\epsilon$-optimal solution in the regime where the RKHS norm upper bound $B$ may change along with $T$. 
    Here, $d$, $\ell$, $\nu$, and the noise level $\rho^2$ are supposed to be $\Theta(1)$.
    }
    \begin{tabular}{c|c|c}
        Algorithm & Time to simple regret $\epsilon$ (SE) & Time to simple regret $\epsilon$ (Mat\'ern) \\ \hline \hline
          GP-UCB & \multirow{2}{*}{$O\rbr{\frac{B^2}{\epsilon^2}\ln^{d+1} \frac{B}{\epsilon} + \frac{1}{\epsilon^2}\ln^{2(d+1)} \frac{1}{\epsilon}} $} & \multirow{2}{*}{$\tilde{O}\rbr{\rbr{\frac{B}{\epsilon}}^{2 + \frac{d}{\nu}} + \rbr{\frac{1}{\epsilon}}^{\frac{4\nu + 2d}{d-2\nu}}}$~~(\text{if}~~$2\nu > d$)}  \\ 
          \cite{srinivas10gaussian} & & \\ \hline
          Existing MVR & \multirow{2}{*}{$O\rbr{\frac{\max\cbr{B^2, 1}}{\epsilon^2} \ln^{d+2}\frac{\max\cbr{B, 1}}{\epsilon}}$} & \multirow{2}{*}{$\tilde{O}\rbr{\rbr{\frac{\max\cbr{B, 1}}{\epsilon}}^{2+\frac{d}{\nu}}}$}  \\ 
          \cite{vakili2021optimal} & &  \\ \hline
          \textbf{MVR} & \multirow{2}{*}{$O\rbr{\frac{1}{\epsilon^2} \ln^{d+2} \frac{B}{\epsilon}}$} & \multirow{2}{*}{$\tilde{O}\rbr{\frac{1}{\epsilon^2}\rbr{\frac{B}{\epsilon}}^{\frac{d}{\nu}}}$}  \\ 
          \textbf{(our analysis)} & & \\ \hline
          Lower bound & \multirow{2}{*}{$\Omega\rbr{\frac{1}{\epsilon^2} \ln^{\frac{d}{2}} \frac{B}{\epsilon}}$} & \multirow{2}{*}{$\Omega\rbr{\frac{1}{\epsilon^2}\rbr{\frac{B}{\epsilon}}^{\frac{d}{\nu}}}$}  \\ 
          \cite{scarlett2017lower} & & 
        \end{tabular}
    \label{tab:sr_eps_rkhs_compare}
\end{table*}

\begin{table*}[tb]
    \centering
    \caption{Summary of the cumulative regret upper bounds of the naive applications of existing results and our results in non-stationary variance setting. 
    Here, $d$, $\ell$, and $\nu$ are supposed to be $\Theta(1)$. In the table below, $\overline{\rho}_T^2 \coloneqq \max_{t \in [T]} \rho_t^2$ denotes the maximum variance proxy up to step $T$. Note that the table below describes the expected regret by setting confidence level $\delta = 1/T$ in PE and VA-PE, respectively. The resulting regrets suffer from additional $O(\sqrt{\ln T})$ factors in high-probability regret upper bound of these algorithms.
    }
    \begin{tabular}{c|c|c|c}
        \multirow{2}{*}{Algorithm} & \multirow{2}{*}{Cumulative Regret (SE)} & \multicolumn{2}{|c}{Cumulative Regret (Mat\'ern)} \\ \cline{3-4}
        & & $d\leq 2\nu$ or $V_T = \Omega\rbr{T^{\frac{d-2\nu}{d}}}$ & $d < 2\nu$ and $V_T = O\rbr{T^{\frac{d-2\nu}{d}}}$ \\ \hline \hline
          GP-UCB & \multirow{2}{*}{$O\rbr{\sqrt{\frac{T}{\ln(1 + \overline{\rho}_T^{-2})}} \ln^{d+1} \frac{T}{\overline{\rho}_T^{2}}} $} & \multicolumn{2}{|c}{\multirow{2}{*}{$\tilde{O}\rbr{\sqrt{\frac{1}{\ln(1 + \overline{\rho}_T^{-2})}} \overline{\rho}_T^{-\frac{2d}{2\nu+d}} T^{\frac{2\nu +3d}{4\nu+2d}}}$}}  \\ 
          \cite{srinivas10gaussian} & & \multicolumn{2}{|c}{} \\ \hline
          PE & \multirow{2}{*}{$O\rbr{(\ln T)^{3/2}\sqrt{\frac{T}{\ln(1 + \overline{\rho}_T^{-2})} \ln^{d+1} \frac{T}{\overline{\rho}_T^{2}}}} $} & \multicolumn{2}{|c}{\multirow{2}{*}{$\tilde{O}\rbr{\sqrt{\frac{1}{\ln(1 + \overline{\rho}_T^{-2})}} \overline{\rho}_T^{-\frac{d}{2\nu+d}} T^{\frac{\nu +d}{2\nu+d}}}$}}  \\ 
          \cite{li2022gaussian} & & \multicolumn{2}{|c}{} \\  \hline
          \textbf{VA-GP-UCB} & \multirow{2}{*}{$O\rbr{\sqrt{V_T} \ln^{d+1} T}$} & \multicolumn{2}{|c}{\multirow{2}{*}{$\tilde{O}\rbr{T^{\frac{2d}{2\nu+d}}\sqrt{V_T}}$}}  \\ 
          \textbf{(ours)} & & \multicolumn{2}{|c}{} \\ \hline
          \textbf{VA-PE} & \multirow{2}{*}{$O\rbr{(\ln T)^{3/2} \sqrt{V_T \rbr{\ln^{d+1} \frac{T^2}{V_T}}}}$} & \multirow{2}{*}{$\tilde{O}\rbr{V_T^{\frac{\nu}{2\nu+d}} T^{\frac{d}{2\nu+d}}}$} & \multirow{2}{*}{$\tilde{O} \rbr{T^{\frac{d-\nu}{d}}}$} \\ 
          \textbf{(ours)} & & & \\ \hline
          Lower bound & \multirow{2}{*}{$\Omega\rbr{\sqrt{V_T \ln^{\frac{d}{2}} \frac{T^2}{V_T}}}$} & \multicolumn{2}{|c}{\multirow{2}{*}{$\Omega\rbr{V_T^{\frac{\nu}{2\nu+d}} T^{\frac{d}{2\nu+d}}}$}}  \\ 
          (\corref{cor:cr_nsv_lower}) & & \multicolumn{2}{|c}{}
        \end{tabular}
    \label{tab:cr_nsv_compare}
\end{table*}

\begin{table*}[tb]
    \centering
    \caption{Summary of the simple regret upper bounds of the naive applications of existing results and our results in non-stationary variance setting. 
    Here, $d$, $\ell$, and $\nu$ are supposed to be $\Theta(1)$. In the table below, $\overline{\rho}_T^2 \coloneqq \max_{t \in [T]} \rho_t^2$ denotes the maximum variance proxy up to step $T$. As with \tabref{tab:cr_nsv_compare}, note that the resulting regrets for MVR and VA-MVR suffer from additional logarithmic factors in high-probability regret upper bound.
    }
    \begin{tabular}{c|c|c|c}
        \multirow{2}{*}{Algorithm} & \multirow{2}{*}{Simple Regret (SE)} & \multicolumn{2}{|c}{Simple Regret (Mat\'ern)} \\ \cline{3-4}
        & & $d\leq 2\nu$ or $V_T = \Omega\rbr{T^{\frac{d-2\nu}{d}}}$ & $d > 2\nu$ and $V_T = O\rbr{T^{\frac{d-2\nu}{d}}}$ \\ \hline \hline
        GP-UCB & \multirow{2}{*}{$O\rbr{\sqrt{\frac{1}{T\ln(1 + \overline{\rho}_T^{-2})}} \ln^{d+1} \frac{T}{\overline{\rho}_T^{2}}} $} & \multicolumn{2}{|c}{\multirow{2}{*}{$\tilde{O}\rbr{\sqrt{\frac{1}{\ln(1 + \overline{\rho}_T^{-2})}} \overline{\rho}_T^{-\frac{2d}{2\nu+d}} T^{-\frac{2\nu - d}{4\nu +2d}}}$}}  \\ 
        \cite{srinivas10gaussian} & & \multicolumn{2}{|c}{} \\ \hline
        MVR & \multirow{2}{*}{$O\rbr{\sqrt{\frac{\ln T}{T \ln(1 + \overline{\rho}_T^{-2})} \ln^{d+1} \frac{T}{\overline{\rho}_T^{2}}}} $} & \multicolumn{2}{|c}{\multirow{2}{*}{$\tilde{O}\rbr{\sqrt{\frac{1}{\ln(1 + \overline{\rho}_T^{-2})}} \overline{\rho}_T^{-\frac{d}{2\nu+d}} T^{-\frac{\nu}{2\nu+d}}}$}}  \\ 
        \cite{vakili2021optimal} & & \multicolumn{2}{|c}{}\\  \hline
        \textbf{VA-GP-UCB} & \multirow{2}{*}{$O\rbr{\frac{\sqrt{V_T}}{T} \ln^{d+1} T}$} & \multicolumn{2}{|c}{\multirow{2}{*}{$\tilde{O}\rbr{T^{-\frac{2\nu - d}{2\nu+d}}\sqrt{V_T}}$}}  \\ 
        \textbf{(ours)} & & \multicolumn{2}{|c}{} \\ \hline
        \textbf{VA-MVR} & \multirow{2}{*}{$O\rbr{\sqrt{\frac{V_T}{T^2} \rbr{\ln^{d+1} \frac{T^2}{V_T}} (\ln T)}}$} & \multirow{2}{*}{$\tilde{O}\rbr{V_T^{\frac{\nu}{2\nu+d}} T^{-\frac{2\nu}{2\nu+d}}}$} & \multirow{2}{*}{$\tilde{O}\rbr{T^{-\frac{\nu}{d}}}$} \\ 
        \textbf{(ours)} & & & \\
        \end{tabular}
    \label{tab:sr_nsv_compare}
\end{table*}

\begin{table*}[tb]
    \centering
    \caption{Summary of the total time step condition to find $\epsilon$-optimal solution in non-stationary variance setting. We only focus on our algorithms and a lower bound here for simplicity.
    }
    \begin{tabular}{c|c|c|c}
        \multirow{2}{*}{Algorithm} & \multirow{2}{*}{Time to Simple Regret $\epsilon$ (SE)} & \multicolumn{2}{|c}{Time to Simple Regret $\epsilon$ (Mat\'ern)} \\ \cline{3-4}
        & & $d\leq 2\nu$ or $V_T = \Omega\rbr{T^{\frac{d-2\nu}{d}}}$ & $d > 2\nu$ and $V_T = O\rbr{T^{\frac{d-2\nu}{d}}}$ \\ \hline \hline
        \textbf{VA-GP-UCB} & \multirow{2}{*}{$O\rbr{\sqrt{\frac{V_T}{\epsilon^2}} \ln^{d+1} \frac{V_T}{\epsilon^2}}$} & \multicolumn{2}{|c}{\multirow{2}{*}{$\tilde{O}\rbr{\rbr{\frac{V_T}{\epsilon^2}}^{\frac{2\nu+d}{2\nu-d}}}$~~(\text{if} $2\nu > d$)}} \\ 
        \textbf{(ours)} & & \multicolumn{2}{|c}{} \\ \hline
        \textbf{VA-MVR} & \multirow{2}{*}{$O\rbr{\sqrt{\frac{V_T}{\epsilon^2} \rbr{\ln^{d+1} \frac{1}{\epsilon}} \rbr{\ln \frac{V_T}{\epsilon^2}}}}$} & \multirow{2}{*}{$\tilde{O}\rbr{\rbr{\frac{V_T}{\epsilon^2}}^{\frac{1}{2}} \rbr{\frac{1}{\epsilon}}^{\frac{d}{2\nu}}}$} & \multirow{2}{*}{$\tilde{O}\rbr{\rbr{\frac{1}{\epsilon}}^{\frac{d}{\nu}}}$} \\ 
        \textbf{(ours)} & & & \\ \hline
        Lower bound & \multirow{2}{*}{$\Omega\rbr{\sqrt{\frac{V_T}{\epsilon^2} \ln^{\frac{d}{2}} \frac{1}{\epsilon}}}$} & \multirow{2}{*}{$\Omega\rbr{\rbr{\frac{V_T}{\epsilon^2}}^{\frac{1}{2}} \rbr{\frac{1}{\epsilon}}^{\frac{d}{2\nu}}}$} & \multirow{2}{*}{$\Omega\rbr{\rbr{\frac{1}{\epsilon}}^{\frac{d}{\nu}}}$} \\ 
        (\corref{cor:sr_nsv_lower}) & & & \\
        \end{tabular}
    \label{tab:sr_eps_nsv_compare}
\end{table*}

\section{Monotone Property of MIG}
\label{sec:MIG_monotone_proof}

Fix $\bX \subset \mX^T$.
%
Let $\by_1$ and $\by_2$ be $\by_1 = \bm{f}(\bX) + \bm{\epsilon}_1$ and $\by_2 = \bm{f}(\bX) + \bm{\epsilon}_1 + \bm{\epsilon}_2$, where $\bm{\epsilon}_1 \sim \mN(\bm{0}, \bm{S}_1)$, $\bm{\epsilon}_2 \sim \mN(\bm{0}, \bm{S}_2)$, and $\bm{S}_1$ and $\bm{S}_2$ are some positive semidefinite matrices.
%
Then, from the definition, $\bm{f}(\bX)$ and $\by_2$ are conditionally independent given $\by_1$.
%
Therefore, we can see that,
\begin{align*}
    I(\bm{f}(\bX) ; \by_1) 
    &= I(\bm{f}(\bX) ; \by_1) + I(\bm{f}(\bX) ; \by_2 \mid \by_1) \\
    &= I(\bm{f}(\bX) ; \by_2) + I(\bm{f}(\bX) ; \by_1 \mid \by_2) \\
    &\geq I(\bm{f}(\bX) ; \by_2),
\end{align*}
where $I(\cdot ; \cdot)$ and $I(\cdot ; \cdot \mid \cdot)$ are mutual information and conditional mutual information.
%
The above inequality is called the data processing inequality~\citep[Theorem~2.8.1 of][]{cover2006-information}.
%
Note that the first and second equalities and last inequality are obtained by $I(\bm{f}(\bX); \by_2 \mid \by_1) = 0$ from the conditional independent property, the chain rule, and the non-negativity of the mutual information, respectively.
%
Since this inequality holds for any $\bX \subset \mX^T$, by setting $\bm{S}_1 = \underline{\lambda}_T^2 \bI_T$ and $\bm{S}_2 = \Sigma_T - \underline{\lambda}_T^2 \bI_T$ we can obtain
\begin{align*}
    \max_{\bX \subset \mX^T} I_{\underline{\lambda}_T^2 \bI_T} (\bm{f}(\bX) ; \by) &\geq \max_{\bX \subset \mX^T} I_{\Sigma_T} (\bm{f}(\bX) ; \by).
\end{align*}


\section{Proof of \secref{sec:uub_pv}}
\label{sec:uub_pv_proof}
\subsection{Proof of \lemref{lem:pvu_mvr}}
\begin{proof}
    Below, we give the proofs for stationary and non-stationary variance parameters separately.
    \paragraph{Stationary variance case.}
    Fix any $T \in \{T \in \N_+ \mid T/2 \geq 3 \gamma_T(\tilde{\lambda}_T^2\bI_T)\}$ and define $\mT = \cbr{t \in [T] \mid \tilde{\lambda}_T^{-1} \sigma_{\tilde{\lambda}_T^2\bI_{t-1}}(\bx_{T,t}; \bX_{T, t-1}) \leq 1}$. Then, we have the following if $\mT \neq \emptyset$:
    \begin{align}
        \label{eq:st_mvr}
        \max_{\bx \in \tilde{\mX}} \sigma_{\overline{\lambda}_T^2\bI_{T}}(\bx; \bX_{T, T})
        &\leq \frac{1}{|\mT|} \sum_{t \in \mT} \sigma_{\overline{\lambda}_T^2\bI_{t-1}}(\bx_{T,t}; \bX_{T, t-1}) \\
        \label{eq:st_monot}
        &\leq \frac{1}{|\mT|} \sum_{t \in \mT} \sigma_{\tilde{\lambda}_T^2\bI_{t-1}}(\bx_{T,t}; \bX_{T, t-1}) \\
        \label{eq:st_mT}
        &= \frac{1}{|\mT|} \sum_{t \in \mT} \tilde{\lambda}_T \min\cbr{1, \tilde{\lambda}_T^{-1}\sigma_{\tilde{\lambda}_T^2\bI_{t-1}}(\bx_{T,t}; \bX_{T, t-1})} \\
        \label{eq:st_sch}
        &\leq \frac{1}{|\mT|} \sqrt{\tilde{\lambda}_T^2 |\mT|  \sum_{t \in \mT} \min\cbr{1, \tilde{\lambda}_T^{-2}\sigma_{\tilde{\lambda}_T^2\bI_{t-1}}^2(\bx_{T,t}; \bX_{T, t-1})}} \\
        \label{eq:st_min_ln}
        &\leq \frac{2}{|\mT|} \sqrt{\tilde{\lambda}_T^2 T \sum_{t \in \mT} \frac{1}{2} \ln\rbr{1 +  \tilde{\lambda}_T^{-2}\sigma_{\tilde{\lambda}_T^2\bI_{t-1}}^2(\bx_{T,t}; \bX_{T, t-1})}} \\
        &\leq \frac{2}{|\mT|} \sqrt{\tilde{\lambda}_T^2 T \sum_{t \in [T]} \frac{1}{2} \ln\rbr{1 +  \tilde{\lambda}_T^{-2}\sigma_{\tilde{\lambda}_T^2\bI_{t-1}}^2(\bx_{T,t}; \bX_{T, t-1})}} \\
        \label{eq:st_mig}
        &\leq \frac{2}{|\mT|} \sqrt{\tilde{\lambda}_T^2 T \gamma_T(\tilde{\lambda}_T^2\bI_{T})},
    \end{align}
    where:
    \begin{itemize}
        \item Eq.~\eqref{eq:st_mvr} follows from the fact that $\sigma_{\overline{\lambda}_T^2\bI_{T}}(\bx; \bX_{T, T}) \leq \sigma_{\overline{\lambda}_T^2\bI_{t-1}}(\bx_{T,t}; \bX_{T, t-1})$ holds for all $t \in [T]$, $\bx \in \tilde{\mX}$ from the MVR selection rule.
        \item Eq.~\eqref{eq:st_monot} follows from the monotonicity of the posterior variance on the noise parameter. Note that $\overline{\lambda}_T \leq \tilde{\lambda}_T$ holds from the assumption.
        \item Eq.~\eqref{eq:st_mT} follows from the definition of $\mT$.
        \item Eq.~\eqref{eq:st_sch} follows from Schwartz's inequality.
        \item Eq.~\eqref{eq:st_min_ln} follows from $|\mT| \leq T$ and the inequality $\forall a \geq 0, \min\{1, a\} \leq 2 \ln(1 + a)$.
        \item Eq.~\eqref{eq:st_mig} follows from 
        $\sum_{t \in [T]} \frac{1}{2} \ln\rbr{1 +  \tilde{\lambda}_T^{-2}\sigma_{\tilde{\lambda}_T^2\bI_{t-1}}^2(\bx_{T,t}; \bX_{T, t-1})} = I_{\tilde{\lambda}_{T}^2 \bI_T}(\bm{f}(\bX_{T,T}), \by) \leq \gamma_{T}(\tilde{\lambda}_{T}^2 \bI_T)$. See, e.g., Theorem~5.3 in \cite{srinivas10gaussian}.
    \end{itemize}
    Furthermore, from the condition of $T$, we have
    \begin{align}
        |\mT| 
        &= T - |\mT^c| \\
        &\geq T - 3\gamma_T(\tilde{\lambda}_T^2\bI_T) \\
        \label{eq:st_mt_lb}
        &\geq T/2,
    \end{align}
    where the first inequality follows from \lemref{lem:epcl}.
    Combining Eq.~\eqref{eq:st_mt_lb} with Eq.~\eqref{eq:st_mig}, we obtain the desired result.
    
    \paragraph{Non-stationary variance setting.}
    We start by extending the elliptical potential count lemma to the non-stationary setting.
    \begin{lemma}[Elliptical potential count lemma for non-stationary variance setting]
    \label{lem:epcl_nst}
    Fix any $T \in N_+$, any sequence $\bx_1, \ldots, \bx_T \in \mX$, and $\lambda_1, \ldots, \lambda_T > 0$.
    Define $\mT^c$ as $\mT^c = \{t \in [T] \mid \lambda_t^{-1} \sigma_{\Sigma_{t-1}}(\bx_{t}; \bX_{t-1}) > 1\}$, where $\bX_{t-1} = (\bx_1, \ldots, \bx_{t-1})$ and $\Sigma_{t-1} = \mathrm{diag}(\lambda_1^2, \ldots, \lambda_{t-1}^2)$.
    Then, the number of elements of $\mT^c$ satisfies $|\mT^c| \leq \min \cbr{4 \overline{\gamma}\rbr{4\gamma_T(\Sigma_T), \underline{\lambda}_T^2}, 4\gamma_T(\Sigma_T)}$, where $\underline{\lambda}_T^2 = \mathrm{min}_{t \in [T]} \lambda_t^2$. Furthermore, $\overline{\gamma}(\cdot, \cdot)$ is any monotonic upper bound of MIG defined on $\R_+ \times \R_+$, which satisfies $\forall T \in \N_+, \lambda > 0, \gamma_T(\lambda^2 \bI_T) \leq \overline{\gamma}(T, \lambda^2)$ and $\forall \lambda > 0, T \geq 1, \epsilon \geq 0, \overline{\gamma}(T, \lambda^2) \leq \overline{\gamma}(T + \epsilon, \lambda^2)$.
\end{lemma}
\begin{proof}[Proof of \lemref{lem:epcl_nst}]
    If $\mT^c = \emptyset$, the claimed inequality is trivial, so we focus on the case where $\mT^c \neq \emptyset$ hereafter.
    From the definition of $\mT^c$, we have
    \begin{align}
        \label{eq:epcl_cnt}
        |\mT^c| 
        &= \sum_{t \in \mT^c} \min\cbr{1, \lambda_t^{-1} \sigma_{\Sigma_{t-1}}(\bx_{t}; \bX_{t-1})} \\
        \label{eq:epcl_sq}
        &\leq \sum_{t \in \mT^c} \min \cbr{1, \lambda_t^{-2} \sigma_{\Sigma_{t-1}}^2(\bx_{t}; \bX_{t-1})} \\
        \label{eq:epcl_min_ln}
        &\leq 4 \sum_{t \in \mT^c} \frac{1}{2} \ln \rbr{1 + \lambda_t^{-2} \sigma_{\Sigma_{t-1}}^2(\bx_{t}; \bX_{t-1})} \\
        &\leq 4 \sum_{t \in [T]} \frac{1}{2} \ln \rbr{1 + \lambda_t^{-2} \sigma_{\Sigma_{t-1}}^2(\bx_{t}; \bX_{t-1})} \\
        \label{eq:epcl_mig}
        &\leq 4 \gamma_T(\Sigma_T).
    \end{align}
    In the above inequalities:
    \begin{itemize}
        \item Eqs.~\eqref{eq:epcl_cnt} and \eqref{eq:epcl_sq} follows from $ 1 = \min\cbr{1, \lambda_t^{-1} \sigma_{\Sigma_{t-1}}(\bx_{t}; \bX_{t-1})} \leq \min\cbr{1, \lambda_t^{-2} \sigma_{\Sigma_{t-1}}^2(\bx_{t}; \bX_{t-1})}$, which holds for all $t \in \mT^c$ from the definition of $\mT^c$.
        \item Eq.~\eqref{eq:epcl_min_ln} follows from the inequality $\forall a \geq 0, \min\{1, a\} \leq 2 \ln(1 + a)$.
        \item Eq.~\eqref{eq:epcl_mig} follows from 
        $\sum_{t \in [T]} \frac{1}{2} \ln \rbr{1 + \lambda_t^{-2} \sigma_{\Sigma_{t-1}}^2(\bx_{t}; \bX_{t-1})} = I_{\Sigma_{T}}(\bm{f}(\bX_{T}), \by) \leq \gamma_{T}(\Sigma_{T})$. This is a direct extension of Theorem~5.3 in \cite{srinivas10gaussian} and is proved explicitly in the proof of Proposition~1 in \cite{makarova2021riskaverse}.
    \end{itemize}
    Here, we set $t_1, \ldots, t_{|\mT^c|} \in [T]$ as the elements of $\mT^c$, which are indexed in the increasing order. Furthermore, for all $i \in [|\mT^c|]$,
    let us respectively define $\tilde{\bx}_i$, $\tilde{\bX}_i$, $\tilde{\lambda}_t$, and $\tilde{\Sigma}_i$ as $\tilde{\bx}_i = \bx_{t_i}$, $\tilde{\bX}_i = (\bx_{t_1}, \ldots, \bx_{t_i})$, $\tilde{\lambda}_i = \lambda_{t_i}$, and $\tilde{\Sigma}_i = \mathrm{diag}(\lambda_{t_1}^2, \ldots, \lambda_{t_i}^2)$.
    Then, from Eq.~\eqref{eq:epcl_min_ln}, we also have the following inequality:
    \begin{align}
        |\mT^c| 
        &\leq 4 \sum_{t \in \mT^c} \frac{1}{2} \ln \rbr{1 + \lambda_t^{-2} \sigma_{\Sigma_{t-1}}^2(\bx_{t}; \bX_{t-1})} \\
        &\leq 4 \sum_{t = 1}^{|\mT^c|} \frac{1}{2} \ln \rbr{1 + \tilde{\lambda}_t^{-2} \sigma_{\tilde{\Sigma}_{t-1}}^2(\tilde{\bx}_{t}; \tilde{\bX}_{t-1})} \\
        &\leq 4 \gamma_{|\mT^c|}\rbr{\tilde{\Sigma}_{|\mT^c|}} \\
        &\leq 4 \overline{\gamma}\rbr{|\mT^c|, \min_{t \in [|\mT^c|]} \tilde{\lambda}_t^2} \\
        &\leq 4 \overline{\gamma}\rbr{4\gamma_T(\Sigma_T), \underline{\lambda}_T^2}.
    \end{align}
    The second inequality follows from the fact that the posterior variance has the monotonicity on data; namely, $\sigma_{\Sigma_{t_i-1}}^2(\bx_{t_i}; \bX_{t_i-1}) \leq 
    \sigma_{\tilde{\Sigma}_{i-1}}^2(\tilde{\bx}_{i}; \tilde{\bX}_{i-1})$ holds since the input data $\tilde{\bX}_{i-1}$ is included in $\bX_{t_i-1}$.
\end{proof}

The remaining proof is given by following the proof strategy under the stationary variance setting using \lemref{lem:epcl_nst}.
Here, fix any $T \in \{T \in \N_+ \mid T/2 \geq 4\gamma_T(\tilde{\Sigma}_T)\}$ and define $\mT = \cbr{t \in [T] \mid \tilde{\lambda}_t^{-1} \sigma_{\tilde{\Sigma}_{t-1}}(\bx_t; \bX_{t-1}) \leq 1}$. Then, as with the proof in the stationary variance setting, we have:
    \begin{align}
        \label{eq:nst_mvr}
        \max_{\bx \in \tilde{\mX}} \sigma_{\Sigma_{T}}(\bx; \bX_{T})
        &\leq \frac{1}{|\mT|} \sum_{t \in \mT} \sigma_{\Sigma_{t-1}}(\bx_t; \bX_{t-1}) \\
        &\leq \frac{1}{|\mT|} \sum_{t \in \mT} \sigma_{\tilde{\Sigma}_{t-1}}(\bx_t; \bX_{t-1}) \\
        \label{eq:nst_mT}
        &= \frac{1}{|\mT|} \sum_{t \in \mT} \tilde{\lambda}_t \min\cbr{1, \tilde{\lambda}_t^{-1}\sigma_{\tilde{\Sigma}_{t-1}}(\bx_t; \bX_{t-1})} \\
        \label{eq:nst_sch}
        &\leq \frac{1}{|\mT|} \sqrt{\rbr{\sum_{t \in \mT} \tilde{\lambda}_t^2}  \sum_{t \in \mT} \min\cbr{1, \tilde{\lambda}_t^{-2}\sigma_{\tilde{\Sigma}_{t-1}}^2(\bx_t; \bX_{t-1})}} \\
        \label{eq:nst_min_ln}
        &\leq \frac{2}{|\mT|} \sqrt{\rbr{\sum_{t \in [T]} \tilde{\lambda}_t^2} \sum_{t \in \mT} \frac{1}{2} \ln\rbr{1 +  \tilde{\lambda}_t^{-2}\sigma_{\tilde{\Sigma}_{t-1}}^2(\bx_t; \bX_{t-1})}} \\
        &\leq \frac{2}{|\mT|} \sqrt{\rbr{\sum_{t \in [T]} \tilde{\lambda}_t^2} \sum_{t \in [T]} \frac{1}{2} \ln\rbr{1 +  \tilde{\lambda}_t^{-2}\sigma_{\tilde{\Sigma}_{t-1}}^2(\bx_t; \bX_{t-1})}} \\
        \label{eq:nst_mig}
        &\leq \frac{2}{|\mT|} \sqrt{\rbr{\sum_{t \in [T]} \tilde{\lambda}_t^2} \gamma_T(\tilde{\Sigma}_{T})}.
    \end{align}
    Furthermore, from \lemref{lem:epcl_nst} and the condition of $T$, we have
    \begin{align}
        |\mT| 
        &= T - |\mT^c| \\
        &\geq T - 4\gamma_T(\tilde{\Sigma}_T) \\
        \label{eq:nst_mt_lb}
        &\geq T/2.
    \end{align}
    Combining the above inequality with Eq.~\eqref{eq:nst_mig}, we obtain the desired result.
\end{proof}

\subsection{Proof of \corref{cor:pvu_mvr}}
\begin{proof}
    We describe the proof for each statement separately.
    \paragraph{Statement 1.}
    From the assumption, $\forall T \in \N_+, \overline{\lambda}_T^2 \geq C \exp\rbr{-T^{\frac{1}{d+1}} \ln^{-\alpha} (1 + T)}$ holds for some constant $C > 0$.
    Furthermore, since $k = \sek$, there exist constant $\overline{C} > 0$ such that $\gamma_T(\overline{\lambda}_T^2 \bI_T) \leq \overline{C} \ln^{d+1} (T/\overline{\lambda}_T^2)$. Here, $\overline{C}$ depends on $\ell$ and $d$. 
    Then,
    \begin{align}
        \gamma_T(\overline{\lambda}_T^2 \bI_T) 
        &\leq \overline{C} \ln^{d+1} \rbr{T C^{-1} \exp\rbr{T^{\frac{1}{d+1}} \ln^{-\alpha} (1+T)}} \\
        &\leq \overline{C} \ln^{d+1} \rbr{\exp\rbr{\tilde{C} T^{\frac{1}{d+1}} \ln^{-\alpha} (1+T)}} \\
        \label{eq:se_stat1_mig}
        &= T \overline{C}\tilde{C}^{d+1} \rbr{\ln (1+T)}^{-\alpha(d+1)},
    \end{align}
    where $\tilde{C} > 0$ is a constant that satisfies $T C^{-1} \exp\rbr{T^{\frac{1}{d+1}} \ln^{-\alpha} (1+T)} \leq \exp\rbr{\tilde{C} T^{\frac{1}{d+1}} \ln^{-\alpha} (1+T)}$ for all $T \in \N_+$.
    Here, note that $\tilde{C}$ may depend on $d$, $\alpha$, and $C$.
    Since $\overline{C}\tilde{C}^{d+1} \rbr{\ln (1+T)}^{-\alpha(d+1)} \rightarrow 0$ as $T \rightarrow 0$, 
    there exists a constant $\overline{T}$ such that $T/2 \geq 3T \overline{C}\tilde{C}^{d+1} \rbr{\ln (1+T)}^{-\alpha(d+1)}$ holds for all $T \geq \overline{T}$.
    Furthermore, such constant $\overline{T}$ satisfies $\forall T \geq \overline{T}, T/2 \geq 3 \gamma_T(\overline{\lambda}_T^2 \bI_T)$ from Eq.~\eqref{eq:se_stat1_mig}.
    Therefore, for any $T \geq \overline{T}$, Eq.~\eqref{eq:std_pvu} holds from \lemref{lem:pvu_mvr} with $\tilde{\lambda}_T^2 = \overline{\lambda}_T^2$.

    \paragraph{Statement 2.}
    From the assumption, $\forall T \in \N_+, \overline{\lambda}_T^2 \geq C T^{-\frac{2\nu}{d}} \ln^{\frac{2\nu(1+\alpha)}{d}}(1+T)$ holds for some constant $C > 0$.
    Furthermore, since $k = \matk$ with $\nu > 1/2$, there exist constant $\overline{C} > 0$ such that $\gamma_T(\overline{\lambda}_T^2 \bI_T) \leq \overline{C} (T/\overline{\lambda}_T^2)^{\frac{d}{2\nu + d}} \ln^{\frac{2\nu}{2\nu + d}} (1 + T/\overline{\lambda}_T^2)$. Here, $\overline{C}$ depends on $\ell$, $\nu$, and $d$. 
    Then,
    \begin{align}
        \gamma_T(\overline{\lambda}_T^2 \bI_T) 
        &\leq \overline{C} C^{-\frac{d}{2\nu+d}} T \rbr{\ln^{-\frac{2\nu}{2\nu +d}(1+\alpha)} (1+T)} \ln^{\frac{2\nu}{2\nu + d}} \rbr{1 + C^{-1}T^{\frac{2\nu + d}{d}} \ln^{-\frac{2\nu(1+\alpha)}{d}}(1+T)} \\
        &\leq \overline{C} C^{-\frac{d}{2\nu+d}} T \rbr{\ln^{-\frac{2\nu}{2\nu +d}(1+\alpha)} (1+T)} \ln^{\frac{2\nu}{2\nu + d}} \rbr{\hat{C}(1+T)^{\frac{2\nu + d}{d}}} \\
        \label{eq:se_stat2_mig}
        &\leq \overline{C} C^{-\frac{d}{2\nu+d}} \tilde{C} T \ln^{-\frac{2\nu\alpha}{2\nu +d}} (1+T),
    \end{align}
    where $\hat{C} > 0$ is a constant that satisfies $1 + C^{-1}T^{\frac{2\nu + d}{d}} \ln^{-\frac{2\nu(1+\alpha)}{d}}(1+T) \leq \hat{C}(1+T)^{\frac{2\nu + d}{d}}$ for all $T \in \N_+$.
    Furthermore, $\tilde{C} > 0$ is a constant that satisfies $\ln^{\frac{2\nu}{2\nu + d}} \rbr{\hat{C}(1+T)^{\frac{2\nu + d}{d}}} \leq \tilde{C} \ln^{\frac{2\nu}{2\nu + d}} \rbr{1+T} $ for all $T \in \N_+$.
    Here, note that $\hat{C}$ and $\tilde{C}$ may depend on $d$, $\alpha$, and $\nu$, but are the constants under the condition that $d$, $\alpha$, and $\nu$ are fixed.
    Since $\overline{C} C^{-\frac{d}{2\nu+d}} \tilde{C} \ln^{-\frac{2\nu\alpha}{2\nu +d}} (1+T) \rightarrow 0$ as $T \rightarrow 0$, 
    there exists a constant $\overline{T}$ such that $T/2 \geq 3\overline{C} C^{-\frac{d}{2\nu+d}} \tilde{C} T \ln^{-\frac{2\nu\alpha}{2\nu +d}} (1+T)$ holds for all $T \geq \overline{T}$.
    Furthermore, such constant $\overline{T}$ satisfies $\forall T \geq \overline{T}, T/2 \geq 3 \gamma_T(\overline{\lambda}_T^2 \bI_T)$ from Eq.~\eqref{eq:se_stat2_mig}.
    Therefore, for any $T \geq \overline{T}$, Eq.~\eqref{eq:std_pvu} holds from \lemref{lem:pvu_mvr} with $\tilde{\lambda}_T^2 = \overline{\lambda}_T^2$.

    \paragraph{Statements 3 and 4.}
    For statement 3, we set $\tilde{\lambda}_T^2 = C \exp\rbr{-T^{\frac{1}{d+1}} \ln^{-\alpha}(1+T)}$. 
    Then, following the same argument as the proof of statement 1 in \corref{cor:pvu_mvr}, 
    we confirm that $\gamma_T(\tilde{\lambda}_T^2 \bI_T) = o(T)$, and the quantity $\min\{T \in \N_+ \mid \forall t \geq T, t/2 \geq 3 \gamma_t(\tilde{\lambda}_t^2 \bI_t)\} $ is bounded from 
    above by some finite constant $\overline{T}$ that depends on $d$, $\alpha$, and $C$.
    Furthermore, from $\gamma_T(\tilde{\lambda}_T^2 \bI_T) = o(T)$ and the definition of $\tilde{\lambda}_T$, we can evaluate the order of the r.h.s. in Eq.~\eqref{eq:stat_pvu} as 
    \begin{equation}
        \frac{4}{T} \sqrt{\tilde{\lambda}_T^2 T \gamma_T(\tilde{\lambda}_T^2 \bI_T)} 
        \leq O\rbr{\sqrt{\exp\rbr{-T^{\frac{1}{d+1}} \ln^{-\alpha} T}}}. 
    \end{equation}
    We also obtain the result for statement 4 by setting $\tilde{\lambda}_T^2$ as $\tilde{\lambda}_T^2 = C T^{-\frac{2\nu}{d}} (\ln T)^{\frac{2\nu (1+\alpha)}{d}}$ and calculating the explicit value of r.h.s. in Eq.~\eqref{eq:stat_pvu}. 
\end{proof}

\section{Proof in \secref{sec:nls}}
\label{sec:nl_proof}
\subsection{Proof of \thmref{thm:cr_pe_nl}}

\begin{lemma}[Deterministic confidence bound for noiseless setting, Lemma~11 in \cite{lyu2019efficient} or Proposition~1 in \cite{vakili2021optimal}]
    \label{lem:detrm_cb}
    Suppose Assumptions~\ref{asmp:smoothness}, \ref{asmp:noise} with $\forall t \in \N_+, \rho_t = 0$ hold.
    Then, for any sequence\footnote{Strictly speaking, the input $(\bx_t)_{t \in \N_+}$ is required to have no duplication to guarantee the existence of the inverse gram matrix. 
    In all of our algorithms, such events only occur when the algorithm finds the maximizer $\bx^{\ast}$, which leads to subsequent instantaneous regrets of $0$, and our upper bounds trivially hold. Therefore, we suppose that such events do not occur in our proof.} $(\bx_t)_{t \in \N_+}$ on $\mX$, the following event holds:
    \begin{equation}
        \forall t \in \N_+,~\forall \bx \in \mX,~|f(\bx) - \mu_{\lambda^2 \bI_t}(\bx; \bX_t, \bm{f}_t)| \leq B \sigma_{\lambda^2\bI_t}(\bx; \bX_t),
    \end{equation}
    where $\lambda = 0$, $\bX_t = (\bx_1, \ldots, \bx_t)$, and $\bm{f}_t = (f(\bx_1), \ldots, f(\bx_t))^{\top}$.
\end{lemma}
The proof of \thmref{thm:cr_pe_nl} follows by using \lemref{lem:detrm_cb} and \lemref{cor:pvu_mvr} in the standard PE analysis.
\begin{proof}[Proof of \thmref{thm:cr_pe_nl}]
    Fix any $\alpha > 0$ and set $\overline{T}$ as the constant defined in statements 3 or 4 of Corollary~\ref{cor:pvu_mvr}. 
    Furthermore, let $\overline{i} \in \N_+$ be the first batch index 
    such that $N_{\overline{i}} \geq \overline{T}$ holds.
    In both kernels, the cumulative regret before the start of batch $\overline{i}+1$ is bounded from above by $\max\{8B\overline{T}, 2BN_1\}$ due to $\|f\|_{\infty} \leq \|f\|_{\mH_k} \leq B$, $N_{\overline{i}} < 2 \overline{T}$, and $\sum_{i=1}^{\overline{i}-1}N_i \leq N_{\overline{i}}$ if $N_1 < \overline{T}$.
    Next, for any $i$-th batch with $i \geq \overline{i} + 1$, we have
    \begin{align}
        \label{eq:ub_lb}
        \sum_{j=1}^{N_i} f(\bx^{\ast}) - f(\bx_j^{(i)})
        &\leq \sum_{j=1}^{N_i} \mathrm{ucb}_{i-1}(\bx^{\ast}) - \mathrm{lcb}_{i-1}(\bx_j^{(i)}) \\
        &= \sum_{j=1}^{N_i} \mathrm{lcb}_{i-1}(\bx^{\ast}) - \mathrm{ucb}_{i-1}(\bx_j^{(i)}) + 2 \beta^{1/2} \sigma_{\lambda^2\bI_{N_{i-1}}}(\bx_j^{(i)}; \bX_{N_{i-1}}^{(i-1)}) + 2 \beta^{1/2} \sigma_{\lambda^2\bI_{N_{i-1}}}(\bx^{\ast}; \bX_{N_{i-1}}^{(i-1)}) \\
        \label{eq:pm_bound}
        &\leq \sum_{j=1}^{N_i} \mathrm{lcb}_{i-1}(\bx^{\ast}) - \max_{\bx \in \mX_{i-1}} \mathrm{lcb}_{i-1}(\bx) + 4 B \max_{\bx \in \mX_{i-1}}\sigma_{\lambda^2\bI_{N_{i-1}}}(\bx; \bX_{N_{i-1}}^{(i-1)}) \\
        &\leq 4 B N_i \max_{\bx \in \mX_{i-1}}\sigma_{\lambda^2\bI_{N_{i-1}}}(\bx; \bX_{N_{i-1}}^{(i-1)}),
    \end{align}
    where Eq.~\eqref{eq:ub_lb} follows from the definition of $\beta^{1/2}$ and \lemref{lem:detrm_cb}. 
    Furthermore, Eq.~\eqref{eq:pm_bound} follows from $\bx_j^{(i)}, \bx^{\ast} \in \mX_{i-1}$.
    \paragraph{For SE kernel.}
    From statement 3 in \corref{cor:pvu_mvr}\footnote{The application of \lemref{lem:pvu_mvr} requires the compactness of the potential maximizers, which is verified by the continuity of $\text{ucb}_i(\cdot)$ under $\sek$ and $\matk$.}, we have
    \begin{align}
        4 B N_i \max_{\bx \in \mX_{i-1}}\sigma_{\lambda^2\bI_{N_{i-1}}}(\bx; \bX_{N_{i-1}}^{(i)})
        &\leq 8 B N_{i-1} \sqrt{C_1 \exp\rbr{-N_{i-1}^{\frac{1}{d+1}} \ln^{-\alpha} N_{i-1}}} \\
        &\leq 8 B C_2.
    \end{align}
    In the above inequalities, $C_1, C_2 \in (0, \infty)$ are the constant that may depend on $d$, $\ell$, and $\alpha$.
    The existence of $C_2$ is guaranteed by $t \sqrt{\exp\rbr{-t^{\frac{1}{d+1}} \ln^{-\alpha} t}} \rightarrow 0$.
    Since the total number of batches is bounded from above by $1 + \log_2 T$, we have
    \begin{equation}
        R_T \leq \max\{8B\overline{T}, 2BN_1\} + 8 B C_2 \log_2 T = O(\ln T).
    \end{equation}
    
    \paragraph{For Mat\'ern kernel.}
    From statement 4 in \corref{cor:pvu_mvr}, we have
    \begin{align}
        4 B N_i \max_{\bx \in \mX_{i-1}}\sigma_{\lambda^2\bI_{N_{i-1}}}(\bx; \bX_{N_{i-1}}^{(i)})
        &\leq 8 B C_1 N_{i-1}^{\frac{d - \nu}{d}} \ln^{\frac{\nu}{d}(1+\alpha)}N_{i-1}, 
    \end{align}
    with some constant $C_1 \in (0, \infty)$ that depends on $d$, $\nu$, $\ell$, and $\alpha$.
    When $d > \nu$, we have $8 B C_1 N_{i-1}^{\frac{d - \nu}{d}} \ln^{\frac{\nu}{d}(1+\alpha)}N_{i-1} \leq 8BC_1 T^{\frac{d-\nu}{d}} \ln^{\frac{\nu}{d}(1+\alpha)} T$; 
    therefore, 
    \begin{equation}
        R_T \leq \max\{8B\overline{T}, 2BN_1\} + 8BC_1 T^{\frac{d-\nu}{d}} \rbr{\ln^{\frac{\nu}{d}(1+\alpha)} T} \rbr{\log_2 T} = \tilde{O}\rbr{T^{\frac{d - \nu}{d}}}.
    \end{equation}
    When $d = \nu$, we have $8 B C_1 N_{i-1}^{\frac{d - \nu}{d}} \ln^{\frac{\nu}{d}(1+\alpha)}N_{i-1} \leq 8BC_1 \ln^{1+\alpha} T$; hence,
    \begin{equation}
        R_T \leq \max\{8B\overline{T}, 2BN_1\} + 8BC_1 \rbr{\ln^{1+\alpha} T} \rbr{\log_2 T} = O\rbr{\ln^{2+\alpha} T}.
    \end{equation}
    Finally, when $d < \nu$, we have $8 B C_1 N_{i-1}^{\frac{d - \nu}{d}} \ln^{\frac{\nu}{d}(1+\alpha)} N_{i-1} \leq 8BC_2$ for some constant $C_2 \in (0, \infty)$; therefore,
    \begin{equation}
        R_T \leq \max\{8B\overline{T}, 2BN_1\} + 8BC_2 \rbr{\log_2 T} = O\rbr{\ln T}.
    \end{equation}
\end{proof}

\subsection{Proof of \thmref{thm:sr_mvr_nl}}
\begin{proof}
    If $T \geq \overline{T}$, from \lemref{lem:detrm_cb}, we have
    \begin{align}
        f(\bx^{\ast}) - f(\hat{\bx}_T)
        &\leq \mu_{\lambda^2 \bI_T}(\bx^{\ast}; \bX_T, \bm{f}_T) + B \sigma_{\lambda^2\bI_T}(\bx^{\ast}; \bX_T) - \mu_{\lambda^2 \bI_T}(\hat{\bx}_T; \bX_T, \bm{f}_T) + B \sigma_{\lambda^2\bI_T}(\hat{\bx}_T; \bX_T) \\
        &\leq 2 B \max_{\bx \in \mX} \sigma_{\lambda^2\bI_T}(\bx; \bX_T),
    \end{align}
    where the last line follows from the definition of $\hat{\bx}_T$.
    \paragraph{For SE kernel.}
    From statement 3 in Corollary~\ref{cor:pvu_mvr} and $\|f\|_{\infty} \leq B$, we have
    \begin{equation}
        r_T \leq \begin{cases}
            2B~~&\mathrm{if}~~T < \overline{T}, \\
            2BC_1 \exp\rbr{-\frac{1}{2} T^{\frac{1}{d+1}} \ln^{-\alpha} T} ~~&\mathrm{if}~~T \geq \overline{T},
        \end{cases}
    \end{equation}
    Here, $C_1 \in (0, 1)$ is the implied constant in \corref{cor:pvu_mvr}.
    From the above inequality, $\forall T \in N_+\setminus \{1\}, r_T \leq B C_2 \exp\rbr{-\frac{1}{2} T^{\frac{1}{d+1}} \ln^{-\alpha} T}$ holds for sufficiently large constant $C_2 \in (0, \infty)$, which depends on $C_1$, $\overline{T}$, $\alpha$, and $d$.
    Note that $\overline{T}$ and $C_1$ are the constant that only depends $\alpha$, $d$, $\ell$, and $\nu$.
    This implies $r_T = O\rbr{\exp\rbr{-\frac{1}{2} T^{\frac{1}{d+1}} \ln^{-\alpha} T}}$.

    \paragraph{For Mat\'ern kernel.}
    From statement 4 in Corollary~\ref{cor:pvu_mvr}, we have
    \begin{equation}
        r_T \leq \begin{cases}
            2B~~&\mathrm{if}~~T < \overline{T}, \\
            2BC_1 T^{-\frac{\nu}{d}} \ln^{\frac{\nu}{d}(1+\alpha)} T ~~&\mathrm{if}~~T \geq \overline{T},
        \end{cases}
    \end{equation}
    Here, $C_1 \in (0, 1)$ is the implied constant in \corref{cor:pvu_mvr}.
    From the above inequality, $\forall T \in N_+\setminus \{1\}, r_T \leq B C_2 T^{-\frac{\nu}{d}} \ln^{\frac{\nu}{d}(1+\alpha)} T$ holds for sufficiently large constant $C_2 \in (0, \infty)$, which depends on $C_1$, $\overline{T}$, $\alpha$, $d$, and $\nu$.
    This implies $r_T = \tilde{O}\rbr{T^{-\frac{\nu}{d}}}$.
\end{proof}

\section{Proof in \secref{sec:rkhs_od}}
\label{sec:rkhs_od_proof}
\subsection{Proof of \thmref{thm:cr_pe_rkhs}}

\begin{lemma}[Non-adaptive confidence bound for noisy setting, Theorem~1 in \cite{vakili2021optimal}]
    \label{lem:noisy_cb}
    Fix any $T \in \N_+$, $\delta \in (0, 1)$, $\lambda^2 > 0$, and suppose Assumptions~\ref{asmp:smoothness}, \ref{asmp:noise} with $\forall t \in \N_+, \rho_t = \rho \geq 0$.
    Furthermore, assume $\mX$ is finite.
    Then, if the input sequence $(\bx_t)_{t \in [T]}$ is independent of the noise sequence $(\epsilon_t)_{t \in [T]}$, the following event holds with probability at least $1 - \delta$:
    \begin{equation}
        \forall \bx \in \mX,~|f(\bx) - \mu_{\lambda^2 \bI_t}(\bx; \bX_T, \by_T)| \leq \rbr{B + \frac{\rho}{\lambda}\sqrt{2 \ln \frac{2|\mX|}{\delta}}}\sigma_{\lambda^2\bI_T}(\bx; \bX_T),
    \end{equation}
    where $\bX_T = (\bx_1, \ldots, \bx_T)$ and $\by_T = (y_1, \ldots, y_T)^{\top}$.
\end{lemma}
\begin{proof}[Proof of \thmref{thm:cr_pe_rkhs}]
From \lemref{lem:noisy_cb} and the union bound, the following event holds with probability at least $1 - \delta$:
\begin{equation}
    \forall i \in [Q_T], \forall \bx \in \mX, \mathrm{lcb}_i(\bx) \leq f(\bx) \leq \mathrm{ucb}_i(\bx),
\end{equation}
where $Q_T \in \N_+$ denotes the total batch size of PE. Note that $Q_T \leq 1 + \log_2 T$ holds. Hereafter, we assume the above event holds.

First, the cumulative regret at the first batch is bounded from above by $2BN_1$.
Next, for any $i$-th batch with $i \geq 2$, we have
\begin{align}
        \label{eq:pe_regret_start}
        \sum_{j=1}^{N_i} f(\bx^{\ast}) - f(\bx_j^{(i)})
        &\leq \sum_{j=1}^{N_i} \mathrm{ucb}_{i-1}(\bx^{\ast}) - \mathrm{lcb}_{i-1}(\bx_j^{(i)}) \\
        &= \sum_{j=1}^{N_i} \mathrm{lcb}_{i-1}(\bx^{\ast}) - \mathrm{ucb}_{i-1}(\bx_j^{(i)}) + 2 \beta^{1/2} \sigma_{\lambda^2\bI_{N_{i-1}}}(\bx_j^{(i)}; \bX_{N_{i-1}}^{(i-1)}) + 2 \beta^{1/2} \sigma_{\lambda^2\bI_{N_{i-1}}}(\bx^{\ast}; \bX_{N_{i-1}}^{(i-1)}) \\
        &\leq \sum_{j=1}^{N_i} \mathrm{lcb}_{i-1}(\bx^{\ast}) - \max_{\bx \in \mX_{i-1}} \mathrm{lcb}_{i-1}(\bx) + 4 B \max_{\bx \in \mX_{i-1}}\sigma_{\lambda^2\bI_{N_{i-1}}}(\bx; \bX_{N_{i-1}}^{(i-1)}) \\
        \label{eq:pe_regret_end}
        &\leq 4 N_i \beta^{1/2} \max_{\bx \in \mX_{i-1}} \sigma_{\lambda^2\bI_{N_{i-1}}}(\bx; \bX_{N_{i-1}}^{(i-1)}).
    \end{align}
From the definition of $\beta^{1/2}$ and $\lambda^2 = C/B^2$ for some constant $C > 0$, the above inequality implies
\begin{align}
    \label{eq:rkhs_regret_batch}
    \sum_{j=1}^{N_i} f(\bx^{\ast}) - f(\bx_j^{(i)}) &\leq 
    4 N_i B \rbr{1 + C^{-1/2}\rho \sqrt{2 \ln \frac{2|\mX|(1 + \log_2 T)}{\delta}}} \max_{\bx \in \mX_{i-1}}\sigma_{\lambda^2\bI_{N_{i-1}}}(\bx; \bX_{N_{i-1}}^{(i-1)}).
\end{align}
Here, let us define $\mT$ and $\mT^c$ as $\mT = \{j \in [N_{i-1}] \mid \lambda^{-1} \sigma_{\lambda^2\bI_{j-1}}(\bx_j; \bX_{j-1}^{(i-1)}) \leq 1\}$ and $\mT^c = [N_{i-1}] \setminus \mT$, respectively. 
From elliptical potential count lemma (\lemref{lem:epcl}), we have
\begin{equation}
    |\mT^c| \leq \min \cbr{3 \overline{\gamma}\rbr{3 \gamma_{N_{i-1}}(\lambda^2 \bI_{N_{i-1}}), \lambda^2}, 3 \gamma_{N_{i-1}}(\lambda^2 \bI_{N_{i-1}})}.
\end{equation}
Furthermore, from the definition of $\mT$, we have the following inequality 
as with the Eqs.~\eqref{eq:st_monot}--\eqref{eq:st_mig}:
\begin{align}
    \sum_{j \in \mT} \sigma_{\lambda^2\bI_{j-1}}(\bx_j; \bX_{j-1}^{(i-1)})
    \leq \sqrt{\lambda^2 N_{i-1} \gamma_{N_{i-1}}(\lambda^2 \bI_{N_{i-1}})}.
\end{align}
Then, regarding the maximum of posterior standard deviation, 
we have the following from the above inequalities:
\begin{align}
    &\max_{\bx \in \mX_{i-1}}\sigma_{\lambda^2\bI_{N_{i-1}}}(\bx; \bX_{N_{i-1}}^{(i-1)}) \\
    &\leq \frac{1}{N_{i-1}}\sum_{j \in [N_{i-1}]} \sigma_{\lambda^2\bI_{j-1}}(\bx_j; \bX_{j-1}^{(i-1)}) \\
    &\leq \frac{1}{N_{i-1}} \sbr{|\mT^c| + \sum_{j \in \mT} \sigma_{\lambda^2\bI_{j-1}}(\bx_j; \bX_{j-1}^{(i-1)})} \\
    &\leq \frac{1}{N_{i-1}} \sbr{\min \cbr{3 \overline{\gamma}\rbr{3 \gamma_{N_{i-1}}(\lambda^2 \bI_{N_{i-1}}), \lambda^2}, 3 \gamma_{N_{i-1}}(\lambda^2 \bI_{N_{i-1}})} + 2\sqrt{\lambda^2 N_{i-1} \gamma_{N_{i-1}}(\lambda^2 \bI_{N_{i-1}})}} \\
    &\leq \frac{1}{N_{i-1}} \sbr{\min \cbr{3 \overline{\gamma}\rbr{3 \gamma_{T}(\lambda^2 \bI_{T}), \lambda^2}, 3 \gamma_{T}(\lambda^2 \bI_{T})} + 2\sqrt{\lambda^2 T \gamma_{T}(\lambda^2 \bI_{T})}},
\end{align}
where the first inequality follows from the definition of the MVR-selection rule, and the second inequality follows from 
$\sigma_{\lambda^2\bI_{j-1}}(\bx_j; \bX_{j-1}^{(i-1)}) \leq k(\bx_j, \bx_j) \leq 1$. 
Combining the above inequality with Eq.~\eqref{eq:pe_regret_end} and $Q_T \leq (1 + \log_2 T)$, we have
\begin{equation}
    \label{eq:general_ub_rkhs}
    R_T 
    \leq 2BN_1 + 8 (1 + \log_2 T) \beta^{1/2} \sbr{\min \cbr{3 \overline{\gamma}\rbr{3 \gamma_{T}(\lambda^2 \bI_{T}), \lambda^2}, 3 \gamma_{T}(\lambda^2 \bI_{T})} + 2\sqrt{\lambda^2 T \gamma_{T}(\lambda^2 \bI_{T})}}.
\end{equation}
% \paragraph{For SE kernel.}
% Note that $\lambda^2 = \Theta(1/B^2)$, $B = O(\sqrt{T})$, and $\overline{\gamma}(t, \lambda^2) = O(\ln^{d+1} (t/\lambda^2))$.
% Since $3\gamma_T(\lambda^2 \bI_T) = O(\ln^{d+1} (TB^2))$ and $3\overline{\gamma}\rbr{3\gamma_T(\lambda^2 \bI_T), \lambda^2} = O\rbr{\ln^{d+1} (B^2 \ln^{d+1} (TB^2))}$, $\min \cbr{3 \overline{\gamma}\rbr{3 \gamma_{T}(\lambda^2 \bI_{T}), \lambda^2}, 3 \gamma_{T}(\lambda^2 \bI_{T})} = O\rbr{\ln^{d+1} (B^2 \ln^{d+1} (TB^2))}$ under $B = O(\sqrt{T})$. Furthermore, from $\sqrt{\lambda^2 T \gamma_T(\lambda^2 \bI_T)} = O\rbr{\frac{\sqrt{T}}{B}\sqrt{\ln^{d+1} (TB^2)}}$, 
% $\min \cbr{3 \overline{\gamma}\rbr{3 \gamma_{T}(\lambda^2 \bI_{T}), \lambda^2}, 3 \gamma_{T}(\lambda^2 \bI_{T})} + 2\sqrt{\lambda^2 T \gamma_{T}(\lambda^2 \bI_{T})} = O\rbr{\frac{\sqrt{T}}{B}\sqrt{\ln^{d+1} (TB^2)}}$ when $B = O(\sqrt{T})$. 
% Hence, by noting $\beta^{1/2} = \Theta\rbr{B \sqrt{\ln \frac{|\mX|}{\delta}}}$ and $B = O(\sqrt{T})$, we have
% \begin{align}
%     R_T 
%     &\leq O\rbr{\max\cbr{B, (\ln T) \sqrt{T \rbr{\ln^{d+1} (TB^2)} \rbr{\ln \frac{|\mX|}{\delta}}}}} \\
%     &= O\rbr{(\ln T) \sqrt{T \rbr{\ln^{d+1} (TB^2)} \rbr{\ln \frac{|\mX|}{\delta}}}}.
% \end{align}

\paragraph{For SE kernel.}
Note that $\lambda^2 = \Theta(1/B^2)$, $B = O(\sqrt{T})$, and $\overline{\gamma}(t, \lambda^2) = O(\ln^{d+1} (t/\lambda^2))$.
Since $3\gamma_T(\lambda^2 \bI_T) = O(\ln^{d+1} (TB^2))$, we obtain $\min \cbr{3 \overline{\gamma}\rbr{3 \gamma_{T}(\lambda^2 \bI_{T}), \lambda^2}, 3 \gamma_{T}(\lambda^2 \bI_{T})} = O(\ln^{d+1} (TB^2))$. Furthermore, from $\sqrt{\lambda^2 T \gamma_T(\lambda^2 \bI_T)} = O\rbr{\frac{\sqrt{T}}{B}\sqrt{\ln^{d+1} (TB^2)}}$, 
$\min \cbr{3 \overline{\gamma}\rbr{3 \gamma_{T}(\lambda^2 \bI_{T}), \lambda^2}, 3 \gamma_{T}(\lambda^2 \bI_{T})} + 2\sqrt{\lambda^2 T \gamma_{T}(\lambda^2 \bI_{T})} = O\rbr{\frac{\sqrt{T}}{B}\sqrt{\ln^{d+1} (TB^2)}}$ when $B = O(\sqrt{T})$. 
Hence, by noting $\beta^{1/2} = \Theta\rbr{B \sqrt{\ln \frac{|\mX|}{\delta}}}$ and $B = O(\sqrt{T})$, we have
\begin{align}
    R_T 
    &\leq O\rbr{\max\cbr{B, (\ln T) \sqrt{T \rbr{\ln^{d+1} (TB^2)} \rbr{\ln \frac{|\mX|}{\delta}}}}} \\
    &= O\rbr{(\ln T) \sqrt{T \rbr{\ln^{d+1} (TB^2)} \rbr{\ln \frac{|\mX|}{\delta}}}}.
\end{align}

% \paragraph{For Mat\'ern kernel.}
% Note that $\lambda^2 = \Theta(1/B^2)$, $B = O\rbr{T^{\frac{2\nu^2 + 3 \nu d}{4\nu^2 + 4d^2 - 2\nu d}}}$, and $\overline{\gamma}(t, \lambda^2) = \tilde{O}\rbr{(t/\lambda^2)^{\frac{d}{2\nu + d}}}$.
% Here, $3\gamma_T(\lambda^2 \bI_T) = \tilde{O}\rbr{(TB^2)^{\frac{d}{2\nu +d}}}$ and $3\overline{\gamma}\rbr{3\gamma_T(\lambda^2 \bI_T), \lambda^2} = \tilde{O}\rbr{T^{\rbr{\frac{d}{2\nu +d}}^2} (B^2)^{\frac{d(2\nu+2d)}{(2\nu+d)^2}}}$. 
% From the easy elemental calculation, we confirm the following three facts: (i) $B = O\rbr{T^{-\frac{\nu}{d}}}$ implies $T^{\rbr{\frac{d}{2\nu +d}}^2} (B^2)^{\frac{d(2\nu+2d)}{(2\nu+d)^2}} = \Omega\rbr{(TB^2)^{\frac{d}{2\nu +d}}}$, and vise vasa, (ii) $T^{\rbr{\frac{d}{2\nu +d}}^2} (B^2)^{\frac{d(2\nu+2d)}{(2\nu+d)^2}} = O\rbr{\sqrt{\lambda^2 T \gamma_{T}(\lambda^2 \bI_{T})}} = 
% \tilde{O}\rbr{T^{\frac{\nu+d}{2\nu+d}} B^{-\frac{2\nu}{2\nu+d}}}$ under $B = O\rbr{T^{\frac{2\nu^2 + 3 \nu d}{4\nu^2 + 4d^2 - 2\nu d}}}$, and (iii) $(TB^2)^{\frac{d}{2\nu +d}} = O\rbr{\sqrt{\lambda^2 T \gamma_{T}(\lambda^2 \bI_{T})}}$ under $B = o\rbr{T^{-\frac{\nu}{d}}}$.
% Therefore, from the above three facts with Eq.~\eqref{eq:general_ub_rkhs}, we have
% \begin{equation}
%     R_T \leq \tilde{O}\rbr{B + B \sqrt{\lambda^2 T \gamma_{T}(\lambda^2 \bI_{T})}} \leq \tilde{O}\rbr{\max\cbr{B, T^{\frac{\nu+d}{2\nu+d}} B^{\frac{d}{2\nu+d}}}}.
% \end{equation}
% Finally, since $B = O\rbr{T^{\frac{\nu + d}{2\nu}}}$ implies $T^{\frac{\nu+d}{2\nu+d}} B^{\frac{d}{2\nu+d}} = \Omega(B)$, and $T^{\frac{2\nu^2 + 3 \nu d}{4\nu^2 + 4d^2 - 2\nu d}} = O\rbr{T^{\frac{\nu + d}{2\nu}}}$, we have
% \begin{equation}
%     R_T \leq \tilde{O}\rbr{T^{\frac{\nu+d}{2\nu+d}} B^{\frac{d}{2\nu+d}}}
% \end{equation}
% under $B = O\rbr{T^{\frac{2\nu^2 + 3 \nu d}{4\nu^2 + 4d^2 - 2\nu d}}}$.


\paragraph{For Mat\'ern kernel.}
Note that $\lambda^2 = \Theta(1/B^2)$, $B = O\rbr{T^{\frac{2 \nu^2 + 3 \nu d }{ 4 d^2 + 4\nu^2 + 6\nu d }}}$, and $\overline{\gamma}(t, \lambda^2) = \tilde{O}\rbr{(t/\lambda^2)^{\frac{d}{2\nu + d}}}$.
%
Furthermore, we can see that
\begin{align*}
    \frac{2 \nu^2 + 3 \nu d }{ 4 d^2 + 4\nu^2 + 6\nu d }
    \leq \frac{\nu}{d} 
    \Leftrightarrow& 2 \nu^2 d + 3\nu d^2 \leq 4 \nu d^2 + 4 \nu^3 + 6\nu^2 d \\
    \Leftrightarrow& 0 \leq \nu d^2 + 4\nu^3 + 4 \nu^2 d \\
    \Leftrightarrow& 0 \leq \nu (d + 2\nu)^2.
\end{align*}
%
Thus, from $B = O\rbr{T^{\frac{2 \nu^2 + 3 \nu d }{ 4 d^2 + 4\nu^2 + 6\nu d }}}$, we can obtain $B = O\rbr{T^{\frac{\nu}{d}}}$.
%
Then, 
$3\gamma_T(\lambda^2 \bI_T) = \tilde{O}\rbr{(TB^2)^{\frac{d}{2\nu +d}}}$, 
$\sqrt{\lambda^2 T \gamma_{T}(\lambda^2 \bI_{T})} = 
\tilde{O}\rbr{T^{\frac{\nu+d}{2\nu+d}} B^{-\frac{2\nu}{2\nu+d}}}$, and $3\overline{\gamma}\rbr{3\gamma_T(\lambda^2 \bI_T), \lambda^2} = \tilde{O}\rbr{T^{\rbr{\frac{d}{2\nu +d}}^2} (B^2)^{\frac{d(2\nu+2d)}{(2\nu+d)^2}}}$. 
%
Therefore, for the second term of Eq.~\eqref{eq:general_ub_rkhs}, we see that
\begin{align}
    &8 (1 + \log_2 T) \beta^{1/2} \sbr{\min \cbr{3 \overline{\gamma}\rbr{3 \gamma_{T}(\lambda^2 \bI_{T}), \lambda^2}, 3 \gamma_{T}(\lambda^2 \bI_{T})} + 2\sqrt{\lambda^2 T \gamma_{T}(\lambda^2 \bI_{T})}} \\
    &= \tilde{O} \left(\beta^{1/2} \sbr{\min \cbr{T^{\rbr{\frac{d}{2\nu +d}}^2} (B^2)^{\frac{d(2\nu+2d)}{(2\nu+d)^2}}, (TB^2)^{\frac{d}{2\nu +d}}} + T^{\frac{\nu+d}{2\nu+d}} B^{-\frac{2\nu}{2\nu+d}}} \right).
    \label{eq:cumulative_rkhs_matern_takeno}
\end{align}
%
Note that, if $B = \Theta\rbr{T^{\frac{\nu}{d}}}$, then $T^{\rbr{\frac{d}{2\nu +d}}^2} (B^2)^{\frac{d(2\nu+2d)}{(2\nu+d)^2}} = \Theta(T)$ and $(TB^2)^{\frac{d}{2\nu +d}} = \Theta(T)$.
%
Therefore, if $B = O\rbr{T^{\frac{\nu}{d}}}$, $T^{\rbr{\frac{d}{2\nu +d}}^2} (B^2)^{\frac{d(2\nu+2d)}{(2\nu+d)^2}} = O\left( (TB^2)^{\frac{d}{2\nu +d}} \right)$ since $\frac{d}{2\nu +d} < \frac{d(2\nu+2d)}{(2\nu+d)^2}$.
%
Thus, Eq.~\eqref{eq:cumulative_rkhs_matern_takeno} is $\tilde{O} \left(\beta^{1/2} \sbr{T^{\rbr{\frac{d}{2\nu +d}}^2} (B^2)^{\frac{d(2\nu+2d)}{(2\nu+d)^2}} + T^{\frac{\nu+d}{2\nu+d}} B^{-\frac{2\nu}{2\nu+d}}} \right)$.
%
Then, we can see that
\begin{align}
    \frac{
            T^{\rbr{\frac{d}{2\nu +d}}^2} (B^2)^{\frac{d(2\nu+2d)}{(2\nu+d)^2}}
        }{
            T^{\frac{\nu+d}{2\nu+d}} B^{-\frac{2\nu}{2\nu+d}}
        }
    &= T^{\frac{d^2 - 2 \nu^2 - 3 \nu d - d^2 }{(2\nu +d)^2}}
    B^{\frac{4d \nu + 4 d^2 + 4\nu^2 + 2\nu d}{(2\nu+d)^2}} \\
    &= T^{\frac{ - 2 \nu^2 - 3 \nu d}{(2\nu +d)^2}}
    B^{\frac{4 d^2 + 4\nu^2 + 6\nu d}{(2\nu+d)^2}}.
\end{align}
Therefore, if $B = O\left(T^{\frac{2 \nu^2 + 3 \nu d }{ 4 d^2 + 4\nu^2 + 6\nu d }}\right)$, then $T^{\rbr{\frac{d}{2\nu +d}}^2} (B^2)^{\frac{d(2\nu+2d)}{(2\nu+d)^2}} = O \left( T^{\frac{\nu+d}{2\nu+d}} B^{-\frac{2\nu}{2\nu+d}} \right)$.
%
Thus, from the assumption $B = O\rbr{T^{\frac{2 \nu^2 + 3 \nu d }{ 4 d^2 + 4\nu^2 + 6\nu d }}}$, $T^{\rbr{\frac{d}{2\nu +d}}^2} (B^2)^{\frac{d(2\nu+2d)}{(2\nu+d)^2}} = O \left( T^{\frac{\nu+d}{2\nu+d}} B^{-\frac{2\nu}{2\nu+d}} \right)$.
%
Hence, Eq.~\eqref{eq:cumulative_rkhs_matern_takeno} is $\tilde{O} \left(\beta^{1/2} T^{\frac{\nu+d}{2\nu+d}} B^{-\frac{2\nu}{2\nu+d}} \right) = \tilde{O} \left(T^{\frac{\nu+d}{2\nu+d}} B^{\frac{d}{2\nu+d}} \right)$ because of $\beta^{1/2} = \Theta\rbr{B \sqrt{\ln \frac{|\mX|}{\delta}}}$.
%
Therefore, we have
\begin{equation}
    R_T = \tilde{O}\rbr{\max\cbr{B, T^{\frac{\nu+d}{2\nu+d}} B^{\frac{d}{2\nu+d}}}}.
\end{equation}
%
Furthermore, noting that $B = \Theta\rbr{T^{\frac{\nu + d}{2 \nu}}} \Leftrightarrow B = \Theta\rbr{T^{\frac{\nu + d}{2 \nu + d}} B^{\frac{d}{2\nu + d}}}$, since $\frac{\nu + d}{2\nu} = \frac{1}{2} + \frac{d}{2\nu} > \frac{2 \nu^2 + 3 \nu d }{ 4 d^2 + 4\nu^2 + 6\nu d }$, we see $B = O\rbr{T^{\frac{\nu+d}{2\nu+d}} B^{\frac{d}{2\nu+d}}}$.
%
Consequently, we have
\begin{equation}
    R_T = \tilde{O}\rbr{ T^{\frac{\nu+d}{2\nu+d}} B^{\frac{d}{2\nu+d}}}.
\end{equation}
\end{proof}




\subsection{Proof of \thmref{thm:sr_mvr_rkhs}}
\begin{proof}
    From \lemref{lem:noisy_cb}, we have the following with probability at least $1 - \delta$:
    \begin{align}
        f(\bx^{\ast}) - f(\hat{\bx}_T)
        &\leq \mu_{\lambda^2 \bI_T}(\bx^{\ast}; \bX_T, \by_T) + \beta^{1/2} \sigma_{\lambda^2\bI_T}(\bx^{\ast}; \bX_T) - \mu_{\lambda^2 \bI_T}(\hat{\bx}_T; \bX_T, \by_T) + \beta^{1/2} \sigma_{\lambda^2\bI_T}(\hat{\bx}_T; \bX_T) \\
        &\leq 2 \beta^{1/2} \max_{\bx \in \mX} \sigma_{\lambda^2\bI_T}(\bx; \bX_T) \\
        &= 2 B \rbr{1 + \rho\sqrt{2 \ln \frac{2|\mX|}{\delta}}} \max_{\bx \in \mX} \sigma_{\lambda^2\bI_T}(\bx; \bX_T).
    \end{align}
    Here, when $k = \sek$, note that the condition $B = O(\exp(T^{\frac{1}{d+1}} \ln^{-\alpha}(1 + T)))$ implies $\lambda^2 = O(\exp(-\frac{1}{2}T^{\frac{1}{d+1}}\ln^{-\alpha}(1+T)))$. 
    Therefore, in the SE kernel, from statement 1 in \corref{cor:pvu_mvr}, we have the following inequality for any $T \geq \overline{T}$.
    \begin{equation}
        r_T \leq 
            8\rbr{1 + \rho\sqrt{2 \ln \frac{2|\mX|}{\delta}}}\sqrt{\frac{\overline{C} \ln^{d+1}(TB^2)}{T}} = O\rbr{\sqrt{\frac{\ln^{d+1}(TB^2)}{T}}},
    \end{equation}
    where $\overline{C} > 0$ is the implied constant of the upper bound of MIG.
    As for the Mat\'ern kernel, leveraging statement 2 in \corref{cor:pvu_mvr} by noting the condition of $B$, we have
    \begin{equation}
        r_T \leq 
            \frac{8}{T}\rbr{1 + \rho\sqrt{2 \ln \frac{2|\mX|}{\delta}}} \sqrt{\overline{C} B^{\frac{2d}{2\nu + d}}T^{\frac{2(\nu + d)}{2\nu + d}} \ln^{\frac{2\nu}{2\nu +d}} (TB^2)} = \tilde{O}\rbr{B^{\frac{d}{2\nu +d}} T^{-\frac{\nu}{2\nu+d}}},
    \end{equation}
    for any $T \geq \overline{T}$. 
\end{proof}

\section{Proof in \secref{sec:nsv}}
\label{sec:nsv_proof}
\subsection{Proof of \thmref{thm:vape}}
\begin{lemma}[Non-adaptive confidence bound for non-stationary variance setting, extension of Theorem~1 in \cite{vakili2021optimal}]
    \label{lem:nsv_noisy_cb}
    Fix any $T \in \N_+$, $\delta \in (0, 1)$, any non-negative sequence $(\lambda_t)_{t \in \N_+}$, and suppose Assumptions~\ref{asmp:smoothness} and \ref{asmp:noise} with $\rho_t \leq \lambda_t$.
    Furthermore, assume $\mX$ is finite.
    Then, if the input sequence $(\bx_t)_{t \in [T]}$ is independent of the noise sequence $(\epsilon_t)_{t \in [T]}$, the following event holds with probability at least $1 - \delta$:
    \begin{equation}
        \forall \bx \in \mX,~|f(\bx) - \mu_{\Sigma_T}(\bx; \bX_T, \by_T)| \leq \rbr{B + \sqrt{2 \ln \frac{2|\mX|}{\delta}}}\sigma_{\Sigma_T}(\bx; \bX_T),
    \end{equation}
    where $\bX_T = (\bx_1, \ldots, \bx_T)$, $\by_T = (y_1, \ldots, y_T)^{\top}$, and $\Sigma_T = \mathrm{diag}(\lambda_1^2, \ldots, \lambda_T^2)$.
\end{lemma}
\begin{proof}
    The proof almost directly follows by replacing the original Proposition~1 in \cite{vakili2021optimal}. Let $\bZ_T(\bx)$ be $\bZ_T(\bx)^{\top} = \bk(\bx, \bX_T)^{\top} (\bK(\bX_T, \bX_T) + \Sigma_T)^{-1}$. Then, following the proof of 
    Proposition~1 in \cite{vakili2021optimal}, we obtain its extension as 
    \begin{equation}
        \label{eq:ext_prop1}
        \sigma_{\Sigma_T}^2(\bx; \bX_T) = \sup_{f:\|f\|_{\mH_k} \leq 1} \rbr{f(\bx) - \bZ_T(\bx)^{\top} \bm{f}_T}^2 + \|\bZ_T(\bx)\|_{\Sigma_T}^2, 
    \end{equation}
    where $\bm{f}_T = (f(\bx_1), \ldots, f(\bx_T))^{\top}$ and $\|\bZ_T(\bx)\|_{\Sigma_T} = \bZ_T(\bx)^{\top}\Sigma_T \bZ_T(\bx)$.
    Next, we consider replacing Proposition~1 with Eq.~\eqref{eq:ext_prop1} in the original proof of Theorem~1 in \cite{vakili2021optimal}.
    As with the original proof, we decompose $|f(\bx) - \mu_{\Sigma_T}(\bx; \bX_T, \by_T)|$ into two terms:
    \begin{equation}
        \label{eq:decomp_error}
        |f(\bx) - \mu_{\Sigma_T}(\bx; \bX_T, \by_T)| \leq |f(\bx) - \bZ_T(\bx)^{\top} \bm{f}_T| + |\bZ_T(\bx)^{\top} \bm{\epsilon}_T|,
    \end{equation}
    where $\bm{\epsilon}_T = (\epsilon_1, \ldots, \epsilon_T)^{\top}$.
    The first term of r.h.s. is bounded from above as
    \begin{equation}
        \label{eq:first_rhs_ub}
        |f(\bx) - \bZ_T(\bx)^{\top} \bm{f}_T| = B\left|\frac{f(\bx)}{B} - \bZ_T(\bx)^{\top} \rbr{\frac{\bm{f}_T}{B}}\right| \leq B\sigma_{\Sigma_T}(\bx; \bX_T).
    \end{equation}
    The last inequality follows from Eq.~\eqref{eq:ext_prop1} since $\left\|f(\cdot)/B\right\|_{\mH_k} \leq 1$ holds by \asmpref{asmp:smoothness}.
    Regarding the second term, $\bZ_T(\bx)^{\top} \bm{\epsilon}_T$ is the sub-Gaussian random variable from the independence assumption between $(\bx_t)_{t \in [T]}$ and $(\epsilon_t)_{t \in [T]}$, and its concentration inequality is obtained by evaluating the upper bound of the moment generating function. By following the proof of Theorem~1 in \cite{vakili2021optimal}, we have
    \begin{align}
        \Ep\sbr{\exp(\bZ_T(\bx)^{\top} \bm{\epsilon}_T)} 
        &\leq \exp \rbr{\frac{\|\bZ_T(\bx)\|_{\mathrm{diag}(\rho_1^2, \ldots, \rho_t^2)}^2}{2}} \\
        &\leq \exp \rbr{\frac{\|\bZ_T(\bx)\|_{\Sigma_T}^2}{2}} \\
        &\leq \exp \rbr{\frac{\sigma_{\Sigma_T}^2 (\bx; \bX_T)}{2}},
    \end{align}
    where the second inequality follows from the assumption $\rho_t \leq \lambda_t$, and the third inequality follows from Eq.~\eqref{eq:ext_prop1}. 
    The above upper bound for the moment-generating function implies the following 
    concentration inequality for any $\bx\in \mX$ from Chernoff-Hoeffding inequality:
    \begin{equation}
        \label{eq:second_rhs_ub}
        \Pr\rbr{|\bZ_T(\bx)^{\top} \bm{\epsilon}_T| \leq \sqrt{2 \ln \frac{2}{\delta}}\sigma_{\Sigma_T}(\bx; \bX_T)
        } \geq 1 - \delta.
    \end{equation}
    Finally, we obtain the desired result by combining Eq.~\eqref{eq:decomp_error} with Eqs.~\eqref{eq:first_rhs_ub}, \eqref{eq:second_rhs_ub}, and the union bound.
\end{proof}

\begin{proof}[Proof of \thmref{thm:vape}]

From \lemref{lem:nsv_noisy_cb} and the union bound, the confidence bound is valid 
with probability at least $1-\delta$. Hereafter, 
From the assumption $V_T = \Omega(1)$, we have the constant $C > 0$ such that $V_T \geq C$. Below, we prove each statement of the theorem separately based on the setting of the kernel.

\paragraph{For $k = \sek$.}

Let us set $\overline{T}$ as $\overline{T} = \min\{T \in \N_+ \mid \forall t \geq T, t/2 \geq 4\gamma_t(Ct^{-1}\bI_t)\}$. 
Note that the constant $\overline{T}$ is well-defined, since $\gamma_t(Ct^{-1}\bI_t) = O(\ln^{d+1} t^2) = o(t)$. 
Furthermore, let $\overline{i} \in \N_+$ be the first batch index such that $N_{\overline{i}} \geq \overline{T}$ holds.
Then, the cumulative regret before the start of $(\overline{i}+1)$-th batch is bounded from above by $\max\{8B\overline{T}, 2BN_1\}$. 
Next, we consider the cumulative regret after the start of $(\overline{i}+1)$-th batch. We apply statement 2 of \lemref{lem:pvu_mvr} with the following arguments:
\begin{enumerate}
\item Set $\tilde{\lambda}_j^{(i)}$ as $\rbr{\tilde{\lambda}_j^{(i)}}^2 = \max\cbr{\rbr{\lambda_j^{(i)}}^2, \frac{V_T}{N_i}}$. Then, we have $\rbr{\tilde{\lambda}_j^{(i)}}^2 \geq V_T/N_i \geq C/N_i$. Furthermore, we set $\tilde{\Sigma}_{N_i}^{(i)} = \mathrm{diag}\rbr{\rbr{\tilde{\lambda}_1^{(i)}}^2, \ldots, \rbr{\tilde{\lambda}_{N_i}^{(i)}}^2}$.
 \item From the above lower bound of $\tilde{\lambda}_j^{(i)}$, we have $\gamma_{N_i}\rbr{\tilde{\Sigma}_{N_i}^{(i)}} \leq \gamma_{N_i}(CN_i^{-1}\bI_{N_i})$, which implies $N_i/2 \geq 4 \gamma_{N_i}(\tilde{\Sigma}_{N_i}^{(i)})$ if $N_i \geq \overline{T}$. 
 Therefore, applying statement 2 of \lemref{lem:pvu_mvr}, we have 
 \begin{equation}
     \label{eq:non_stat_pvu_pe}
        \max_{\bx \in \mX_{i}} \sigma_{\Sigma_{N_i}^{(i)}}(\bx; \bX_{N_i}^{(i)}) \leq \frac{4}{N_i} \sqrt{\rbr{\sum_{j=1}^{N_i} \rbr{\tilde{\lambda}_{j}^{(i)}}^2} \gamma_{N_i}\rbr{\tilde{\Sigma}_{N_i}^{(i)}}} ~~\mathrm{for~all~i \geq \overline{i}}.
 \end{equation}
\end{enumerate}
The equation inside the square root in r.h.s. of Eq.~\eqref{eq:non_stat_pvu_pe} can be bounded from above further as
\begin{align}
    \rbr{\sum_{j=1}^{N_i} \rbr{\tilde{\lambda}_{j}^{(i)}}^2} \gamma_{N_i}\rbr{\tilde{\Sigma}_{N_i}^{(i)}}
    &\leq \sbr{\sum_{j=1}^{N_i} \rbr{\rho_j^{(i)}}^2 + \sum_{j=1}^{N_i} \frac{V_T}{N_i}} \gamma_{N_i}\rbr{\tilde{\Sigma}_{N_i}^{(i)}} \\
    &\leq 2V_T \gamma_{N_i}\rbr{\frac{V_T}{N_i}\bI_{N_i}} \\
    &\leq 2V_T \gamma_{T}\rbr{\frac{V_T}{T}\bI_{T}} \\
    &\leq O\rbr{V_T \ln^{d+1} \frac{T^2}{V_T}}.
\end{align}
Since $\rho_j^{(i)} = \lambda_j^{(i)}$, the total cumulative regret is bounded from above with probability at least $1-\delta$ as
\begin{align}
    \label{eq:pe_bound_se_nsv}
    R_T &= \sum_{i \leq \overline{i}} \sum_{j=1}^{N_i} f(\bx^{\ast}) - f(\bx_j^{(i)})
    + \sum_{i > \overline{i}} \sum_{j=1}^{N_i} f(\bx^{\ast}) - f(\bx_j^{(i)}) \\
    &\leq \max\{8B\overline{T}, 2BN_1\}
    + 4 \beta^{1/2} \sum_{i > \overline{i}} N_i \max_{\bx \in \mX_{i-1}} \sigma_{\Sigma_{N_{i-1}}^{(i-1)}}(\bx; \bX_{N_{i-1}}^{(i-1)}) \\
    &\leq \max\{8B\overline{T}, 2BN_1\}
    + 32 \beta^{1/2} \sum_{i > \tilde{i}} \sqrt{\rbr{\sum_{j=1}^{N_{i-1}} \rbr{\tilde{\lambda}_{j}^{(i-1)}}^2} \gamma_{N_{i-1}}\rbr{\tilde{\Sigma}_{N_{i-1}}^{(i-1)}}} \\
    &\leq \max\{8B\overline{T}, 2BN_1\}
    + O\rbr{\beta^{1/2} (1 + \log_2 T) \sqrt{V_T \ln^{d+1}\frac{T^2}{V_T}}} \\
    &\leq O\rbr{(\ln T)\sqrt{V_T \rbr{\ln^{d+1}\frac{T^2}{V_T}} \rbr{\ln \frac{|\mX|}{\delta}}}},
\end{align}
where the first inequality follows from the standard analysis of the PE (e.g., Eqs.~\eqref{eq:pe_regret_start}--\eqref{eq:pe_regret_end}) with \lemref{lem:nsv_noisy_cb}. Furthermore, the second inequality uses the fact that the total number of batches is at most $1 + \log_2 T$.

\paragraph{For $k = \matk$.}
The proof is almost the same as that of the SE kernel, while 
we need to introduce the additional lower bound to set $\tilde{\lambda}_j^{(i)}$. 
Let us set $\overline{T}$ as $\overline{T} = \min\{T \in \N_+ \mid \forall t \geq T, t/2 \geq 4\gamma_t(\overline{\lambda}_t^2 \bI_t)\}$, where we set $\overline{\lambda}_t^2 = t^{-\frac{2\nu}{d}} \ln^{\frac{2\nu(1 + \alpha)}{d}} (1 + t)$ here, where $\alpha > 0$ is any fixed constant. 
Next, set $\tilde{\lambda}_j^{(i)}$ as $\rbr{\tilde{\lambda}_j^{(i)}}^2 = \max\cbr{\rbr{\lambda_j^{(i)}}^2, \frac{V_T}{N_i}, \overline{\lambda}_{N_i}^2}$.
Then, as with the proof for the case $k = \sek$, the following statement holds from statement 2 of \lemref{lem:pvu_mvr}:

\begin{equation}
     % \label{eq:non_stat_pvu_pe}
        \max_{\bx \in \mX_{i}} \sigma_{\Sigma_{N_i}^{(i)}}(\bx; \bX_{N_i}^{(i)}) \leq \frac{4}{N_i} \sqrt{\rbr{\sum_{j=1}^{N_i} \rbr{\tilde{\lambda}_{j}^{(i)}}^2} \gamma_{N_i}\rbr{\tilde{\Sigma}_{N_i}^{(i)}}} ~~\mathrm{for~all~i \geq \overline{i}},
 \end{equation}
 where $\overline{i} \in \N_+$ is the first batch index such that $N_{\overline{i}} \geq \overline{T}$ holds. Furthermore, 
\begin{align}
    \rbr{\sum_{j=1}^{N_i} \rbr{\tilde{\lambda}_{j}^{(i)}}^2} \gamma_{N_i} \rbr{\tilde{\Sigma}_{N_i}^{(i)}}
    &\leq \sbr{\sum_{j=1}^{N_i} \rbr{\rho_j^{(i)}}^2 + \sum_{j=1}^{N_i} \frac{V_T}{N_i} + \sum_{j=1}^{N_i} \overline{\lambda}_{N_i}^2} \gamma_{N_i}\rbr{\tilde{\Sigma}_{N_i}^{(i)}} \\
    &\leq \rbr{2V_T + N_i \overline{\lambda}_{N_i}^2} \gamma_{N_i}\rbr{\tilde{\Sigma}_{N_i}^{(i)}}.
\end{align}
In the above inequality, we can see that $\gamma_{N_i}\rbr{\tilde{\Sigma}_{N_i}^{(i)}} \leq \gamma_{N_i}\rbr{\frac{V_T}{N_i}\bI_{N_i}} \leq \gamma_{T}\rbr{\frac{V_T}{T} \bI_{T}}$.
Furthermore, if $V_T/N_i \geq \overline{\lambda}_{N_i}^2$, then, $2V_T + N_i\overline{\lambda}_{N_i}^2 \leq 3V_T$, which 
implies
\begin{equation}
    \sqrt{\rbr{\sum_{j=1}^{N_i} \rbr{\tilde{\lambda}_{j}^{(i)}}^2} \gamma_{N_i} \rbr{\tilde{\Sigma}_{N_i}^{(i)}}} \leq \sqrt{3V_T \gamma_{T}\rbr{\frac{V_T}{T} \bI_{T}}} = \tilde{O}\rbr{V_T^{\frac{\nu}{2\nu+d}} T^{\frac{d}{2\nu+d}}}.
\end{equation}
On the other hand, $\gamma_{N_i}\rbr{\tilde{\Sigma}_{N_i}^{(i)}} \leq \gamma_{N_i}\rbr{\overline{\lambda}_{N_i}^2 \bI_{N_i}}$ also holds.
%
If $V_T/N_i \leq \overline{\lambda}_{N_i}^2$, then, $2V_T + N_i\overline{\lambda}_{N_i}^2 \leq 3N_i\overline{\lambda}_{N_i}^2$, which 
implies
\begin{equation}
    \sqrt{\rbr{\sum_{j=1}^{N_i} \rbr{\tilde{\lambda}_{j}^{(i)}}^2} \gamma_{N_i} \rbr{\tilde{\Sigma}_{N_i}^{(i)}}} \leq \sqrt{3N_i\overline{\lambda}_{N_i}^2 \gamma_{N_i}\rbr{\overline{\lambda}_{N_i}^2 \bI_{N_i}}}
    = \tilde{O}\rbr{N_i^{\frac{d - \nu}{d}}} \leq \begin{cases}
        \tilde{O}(1) ~~&\mathrm{if}~~d \leq \nu, \\
        \tilde{O}\rbr{T^{\frac{d-\nu}{d}}} ~~&\mathrm{if}~~d > \nu.
    \end{cases} 
\end{equation}
Therefore, since $T^{\frac{d}{2\nu + d}} \geq T^{\frac{d - \nu}{d}}$ if $d \leq 2\nu$ and $V_T = \Omega(1)$, we have
\begin{equation}
       \sqrt{\rbr{\sum_{j=1}^{N_i} \rbr{\tilde{\lambda}_{j}^{(i)}}^2} \gamma_{N_i} \rbr{\tilde{\Sigma}_{N_i}^{(i)}}} = \begin{cases}
        \tilde{O}\rbr{V_T^{\frac{\nu}{2\nu+d}} T^{\frac{d}{2\nu+d}}} ~~&\mathrm{if}~~d \leq 2\nu, \\
        \tilde{O}\rbr{\max\cbr{V_T^{\frac{\nu}{2\nu+d}} T^{\frac{d}{2\nu+d}}, T^{\frac{d-\nu}{d}}}} ~~&\mathrm{if}~~d > 2\nu.
    \end{cases}
\end{equation}
Hence, as with Eq.~\eqref{eq:pe_bound_se_nsv} of the proof for $k = \sek$, 
with probability at least $1- \delta$, we have
\begin{align}
    R_T &\leq 8B\overline{T}
    + 16 \beta^{1/2} (1 + \log_2 T) \sqrt{\rbr{\sum_{j=1}^{N_i} \rbr{\tilde{\lambda}_{j}^{(i)}}^2} \gamma_{N_i}\rbr{\tilde{\Sigma}_{N_i}^{(i)}}} \\
    & = \begin{cases}
        \tilde{O}\rbr{V_T^{\frac{\nu}{2\nu+d}} T^{\frac{d}{2\nu+d}}} ~~&\mathrm{if}~~d \leq 2\nu, \\
        \tilde{O}\rbr{\max\cbr{V_T^{\frac{\nu}{2\nu+d}} T^{\frac{d}{2\nu+d}}, T^{\frac{d-\nu}{d}}}} ~~&\mathrm{if}~~d > 2\nu.
    \end{cases}
\end{align}
Here, $V_T = O(T^{\frac{d-2\nu}{d}})$ implies $V_T^{\frac{\nu}{2\nu+d}} T^{\frac{d}{2\nu+d}} = O(T^{\frac{d-\nu}{d}})$, and $V_T = \Omega(T^{\frac{d-2\nu}{d}})$ implies vise versa; therefore, by combining the condition $V_T = \Omega(1)$, 
we have
\begin{align}
    R_T = \begin{cases}
        \tilde{O}\rbr{V_T^{\frac{\nu}{2\nu+d}} T^{\frac{d}{2\nu+d}}} ~~&\mathrm{if}~~d \leq 2\nu, \\
        \tilde{O}\rbr{V_T^{\frac{\nu}{2\nu+d}} T^{\frac{d}{2\nu+d}}} ~~&\mathrm{if}~~d > 2\nu~\mathrm{and}~V_T = \Omega\rbr{T^{\frac{d - 2\nu}{d}}}, \\
        \tilde{O}\rbr{T^{\frac{d-\nu}{d}}} ~~&\mathrm{if}~~d > 2\nu~\mathrm{and}~V_T = O\rbr{T^{\frac{d - 2\nu}{d}}}.
    \end{cases}
\end{align}
\end{proof}


\subsection{Proof of \thmref{thm:vamvr}}
\begin{proof}
Set $\beta^{1/2} = \rbr{B + \sqrt{2 \ln \frac{2|\mX|}{\delta}}}$.
From \lemref{lem:nsv_noisy_cb}, with probability at least $1-\delta$, we have
\begin{align}
    f(\bx^{\ast}) - f(\hat{\bx}_T)
    &\leq \mu_{\Sigma_T}(\bx^{\ast}; \bX_T, \by_T) + \beta^{1/2} \sigma_{\Sigma_T}(\bx^{\ast}; \bX_T) - \mu_{\Sigma_T}(\hat{\bx}_T; \bX_T, \by_T) + \beta^{1/2} \sigma_{\Sigma_T}(\hat{\bx}_T; \bX_T) \\
    &\leq 2 \beta^{1/2} \max_{\bx \in \mX} \sigma_{\Sigma_T}(\bx; \bX_T).
\end{align}
We first consider the case where $k = \sek$.
Let us respectively define $\overline{T}$, $\tilde{\lambda}_t$, $\tilde{\Sigma}_T$ as $\overline{T} = \min\{T \in \N_+ \mid \forall t \geq T, t/2 \geq 4\gamma_t(Ct^{-1}\bI_t)\}$, $\tilde{\lambda}_t^2 = \max\{\lambda_t^2, V_T/T\}$, 
and $\tilde{\Sigma}_T = \mathrm{diag}(\tilde{\lambda}_1^2, \ldots, \tilde{\lambda}_T^2)$. Here, $C > 0$ is the constant such that $V_T \geq C$. Note that the existence of $C$ is guaranteed by the assumption $V_T = \Omega(1)$.
Then, as with the arguments of the proof of \thmref{thm:vape}, 
\begin{equation}
    \label{eq:nsv_mvr_pv}
        \max_{\bx \in \mX} \sigma_{\Sigma_{T}}(\bx; \bX_{T}) \leq \frac{4}{T} \sqrt{\rbr{\sum_{t=1}^{T} \tilde{\lambda}_t^2} \gamma_{T}\rbr{\tilde{\Sigma}_{T}}} ~~\mathrm{for~all~T \geq \overline{T}}.
\end{equation}
From the definition of $\tilde{\lambda}_t$, the above inequality implies, for all $T \geq \overline{T}$, 
\begin{align}
    f(\bx^{\ast}) - f(\hat{\bx}_T)
    &\leq \frac{8\beta^{1/2}}{T} \sqrt{\rbr{\sum_{t=1}^{T} \tilde{\lambda}_t^2} \gamma_{T}\rbr{\tilde{\Sigma}_{T}}} \\
    &\leq \frac{8\beta^{1/2}}{T} \sqrt{\rbr{\sum_{t=1}^{T} \rho_t^2 + \sum_{t=1}^{T} \frac{V_T}{T}} \gamma_{T}\rbr{\tilde{\Sigma}_{T}}} \\
    &\leq \frac{8\beta^{1/2}}{T} \sqrt{2V_T \gamma_{T}\rbr{\frac{V_T}{T}\bI_T}} \\
    &= O\rbr{ \sqrt{\frac{V_T}{T^2} \rbr{\ln^{d+1} \frac{T^2}{V_T}} \rbr{\ln \frac{|\mX|}{\delta}}}}.
\end{align}

Next, when $k = \matk$, we set 
$\overline{T}$, $\tilde{\lambda}_t$, $\tilde{\Sigma}_T$ as $\overline{T} = \min\{T \in \N_+ \mid \forall t \geq T, t/2 \geq 4\gamma_t(\overline{\lambda}_t^2\bI_t)\}$, $\tilde{\lambda}_t^2 = \max\{\lambda_t^2, V_T/T, \overline{\lambda}_T^2\}$, and $\tilde{\Sigma}_T = \mathrm{diag}(\tilde{\lambda}_1^2, \ldots, \tilde{\lambda}_T^2)$, respectively.
Here, $\overline{\lambda}_t^2 = t^{-\frac{2\nu}{d}} \ln^{\frac{2\nu(1 + \alpha)}{d}} (1 + t)$, where $\alpha > 0$ is any fixed constant. 
Then, as with the arguments of the proof of \thmref{thm:vape}, statement \eqref{eq:nsv_mvr_pv} also holds for $k = \matk$. 
Therefore, 
\begin{align}
    f(\bx^{\ast}) - f(\hat{\bx}_T)
    &\leq \frac{8\beta^{1/2}}{T} \sqrt{\rbr{\sum_{t=1}^{T} \tilde{\lambda}_t^2} \gamma_{T}\rbr{\tilde{\Sigma}_{T}}} \\
    &\leq \frac{8\beta^{1/2}}{T} \sqrt{\rbr{2V_T + T\overline{\lambda}_T^2}\gamma_{T}\rbr{\tilde{\Sigma}_{T}}}.
\end{align}
As with the proof of \thmref{thm:vape}, considering the two cases: $V_T/T \geq \overline{\lambda}_T^2$ or not, we obtain 
\begin{align}
    \sqrt{\rbr{2V_T + T\overline{\lambda}_T^2}\gamma_{T}\rbr{\tilde{\Sigma}_{T}}}
    = \begin{cases}
        \tilde{O}\rbr{V_T^{\frac{\nu}{2\nu+d}} T^{\frac{d}{2\nu+d}}} ~&\mathrm{if}~\frac{V_T}{T} \geq \overline{\lambda}_T^2, \\
        \tilde{O}\rbr{T^{\frac{d-\nu}{d}}} ~&\mathrm{if}~\frac{V_T}{T} < \overline{\lambda}_T^2.
    \end{cases}
\end{align}
Therefore, we have
\begin{equation}
    f(\bx^{\ast}) - f(\hat{\bx}_T)
    = \tilde{O}\rbr{\max\cbr{T^{-\frac{\nu}{d}}, V_T^{\frac{\nu}{2\nu+d}} T^{-\frac{2\nu}{2\nu+d}}}}.
\end{equation}
Finally, since $V_T = O(T^{\frac{d - 2\nu}{d}})$ implies $V_T^{\frac{\nu}{2\nu + d}} T^{-\frac{2\nu}{2\nu +d}} = O(T^{-\frac{\nu}{d}})$, and $V_T = \Omega(1)$, we have
\begin{equation}
    f(\bx^{\ast}) - f(\hat{\bx}_T)
    = 
    \begin{cases}
        \tilde{O}\rbr{V_T^{\frac{\nu}{2\nu+d}} T^{-\frac{2\nu}{2\nu+d}}} ~~&\mathrm{if}~~d \leq 2\nu, \\
        \tilde{O}\rbr{V_T^{\frac{\nu}{2\nu+d}} T^{-\frac{2\nu}{2\nu+d}}} ~~&\mathrm{if}~~d > 2\nu~\mathrm{and}~V_T = \Omega\rbr{T^{\frac{d - 2\nu}{d}}}, \\
        \tilde{O}\rbr{T^{-\frac{\nu}{d}}} ~~&\mathrm{if}~~d > 2\nu~\mathrm{and}~V_T = O\rbr{T^{\frac{d - 2\nu}{d}}}.
    \end{cases}
\end{equation}
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Auxiliary Lemmas}

% We here show the generalized version of Theorem~1 in \citep{vakili2021optimal}.
% %
% The difference is that (i) the assumption changes from the i.i.d. property of $(\epsilon_i)_{i \in [t]}$ to conditionally $\rho_t$-sub-Gaussian property of $(\epsilon_i)_{i \in [t]}$ and (ii) the noise variance can be non-stationary.
% %
% However, the proof is almost the same as \citep{vakili2021optimal}, except for using Azuma's inequality~\citep[Lemma~3.7 in][]{Handel-HDP} and changing how to use Proposition~1 in \cite{vakili2021optimal}.
% %
% \begin{lemma}[Non-adaptive confidence bound for non-stationary noise variance modified from Theorem~1 in \citep{vakili2021optimal}]
%     \label{lem:nsv_noisy_cb}
%     Fix any $T \in \N_+$, $\delta \in (0, 1)$, any non-negative sequence $(\lambda_t)_{t \in \N_+}$, and suppose Assumptions~\ref{asmp:smoothness} and \ref{asmp:noise} with $\rho_t \leq \lambda_t$.
%     Furthermore, assume $\mX$ is finite.
%     Then, if the input sequence $(\bx_t)_{t \in [T]}$ is independent of the noise sequence $(\epsilon_t)_{t \in [T]}$, the following event holds with probability at least $1 - \delta$:
%     \begin{equation}
%         \forall \bx \in \mX,~|f(\bx) - \mu_{\Sigma_T}(\bx; \bX_T, \by_T)| \leq \rbr{B + \sqrt{2 \ln \frac{2|\mX|}{\delta}}}\sigma_{\Sigma_T}(\bx; \bX_T),
%     \end{equation}
%     where $\bX_T = (\bx_1, \ldots, \bx_T)$, $\by_T = (y_1, \ldots, y_T)^{\top}$, and $\Sigma_T = \mathrm{diag}(\lambda_1^2, \ldots, \lambda_T^2)$.
% \end{lemma}
% \begin{proof}
%     We show the result for any fixed $(\bx_t)_{t \in [T]}$, that is,
%     \begin{align*}
%         \Pr \rbr{ \forall \bx \in \mX,~|f(\bx) - \mu_{\Sigma_T}(\bx; \bX_T, \by_T)| \leq \rbr{B + \sqrt{2 \ln \frac{2|\mX|}{\delta}}}\sigma_{\Sigma_T}(\bx; \bX_T) \biggm\vert (\bx_t)_{t \in [T]}} \geq 1 - \delta,
%     \end{align*}
%     %
%     since it deduces the desired result as follows:
%     \begin{align*}
%         &\Pr \rbr{ \forall \bx \in \mX,~|f(\bx) - \mu_{\Sigma_T}(\bx; \bX_T, \by_T)| \leq \rbr{B + \sqrt{2 \ln \frac{2|\mX|}{\delta}}}\sigma_{\Sigma_T}(\bx; \bX_T) } \\
%         &= \mathbb{E}_{(\bx_t)_{t \in [T]}} \left[ \Pr \rbr{ \forall \bx \in \mX,~|f(\bx) - \mu_{\Sigma_T}(\bx; \bX_T, \by_T)| \leq \rbr{B + \sqrt{2 \ln \frac{2|\mX|}{\delta}}}\sigma_{\Sigma_T}(\bx; \bX_T) \biggm\vert (\bx_t)_{t \in [T]}} \right] \geq 1 - \delta.
%     \end{align*}
%     %
%     Note that $(\bx_t)_{t \in [T]}$ is independent of the noise sequence $(\epsilon_t)_{t \in [T]}$ from the assumption.
    
    
%     Let $\bZ_T(\bx)$ be $\bZ_T(\bx)^{\top} = \bk(\bx, \bX_T)^{\top} (\bK(\bX_T, \bX_T) + \Sigma_T)^{-1}$. Then, using Lemma~, we obtain its extension as 
%     \begin{equation}
%         \label{eq:ext_prop1}
%         \sigma_{\Sigma_T}^2(\bx; \bX_T) = \sup_{f:\|f\|_{\mH_k} \leq 1} \rbr{f(\bx) - \bZ_T(\bx)^{\top} \bm{f}_T}^2 + \|\bZ_T(\bx)\|_{\Sigma_T}^2, 
%     \end{equation}
%     where $\bm{f}_T = (f(\bx_1), \ldots, f(\bx_T))^{\top}$ and $\|\bZ_T(\bx)\|_{\Sigma_T} = \bZ_T(\bx)^{\top}\Sigma_T \bZ_T(\bx)$.
%     Next, we consider replacing Proposition~1 with Eq.~\eqref{eq:ext_prop1} in the original proof of Theorem~1 in \cite{vakili2021optimal}.
%     As with the original proof, we decompose $|f(\bx) - \mu_{\Sigma_T}(\bx; \bX_T, \by_T)|$ into two terms:
%     \begin{equation}
%         \label{eq:decomp_error}
%         |f(\bx) - \mu_{\Sigma_T}(\bx; \bX_T, \by_T)| \leq |f(\bx) - \bZ_T(\bx)^{\top} \bm{f}_T| + |\bZ_T(\bx)^{\top} \bm{\epsilon}_T|,
%     \end{equation}
%     where $\bm{\epsilon}_T = (\epsilon_1, \ldots, \epsilon_T)^{\top}$.

    
%     The first term of r.h.s. is bounded from above as
%     \begin{equation}
%         \label{eq:first_rhs_ub}
%         |f(\bx) - \bZ_T(\bx)^{\top} \bm{f}_T| = B\left|\frac{f(\bx)}{B} - \bZ_T(\bx)^{\top} \rbr{\frac{\bm{f}_T}{B}}\right| \leq B\sigma_{\Sigma_T}(\bx; \bX_T).
%     \end{equation}
%     The last inequality follows from Eq.~\eqref{eq:ext_prop1} since $\left\|f(\cdot)/B\right\|_{\mH_k} \leq 1$ holds by \asmpref{asmp:smoothness}.

    
%     Regarding the second term, $\bZ_T(\bx)^{\top} \bm{\epsilon}_T = \sum_{i=1}^T Z_{T, i}(\bx) \epsilon_i$, where $Z_{T, i}(\bx)$ is $i$-th element of $\bZ_T(\bx)$.
%     %
%     Then, we can see that $Z_{T, t}(\bx) \epsilon_t \mid (\epsilon_i)_{i \in [t-1]}, (\*x_i)_{i \in [T]}$ is zero mean and $|Z_{T, t}(\bx)| \rho_t$ subgaussian from the conditionally $\rho_t$ subgaussian assumption of $(\epsilon_i)_{i \in [T]}$.
%     %
%     Therefore, using Lemma~3.7 in \citep{Handel-HDP}, we have
%     \begin{align}
%         \Ep\sbr{\exp(\bZ_T(\bx)^{\top} \bm{\epsilon}_T)} 
%         &= \Ep\sbr{\exp\rbr{\sum_{i=1}^T Z_{T, i}(\bx) \epsilon_i}} \\
%         &\leq \exp\rbr{\sum_{i=1}^T Z_{T, t}^2(\bx) \rho_t^2} \\
%         &\leq \exp\rbr{\sum_{i=1}^T Z_{T, t}^2(\bx) \lambda_t^2} \\
%         &= \exp \rbr{\frac{\|\bZ_T(\bx)\|_{\Sigma_T}^2}{2}} \\
%         &\leq \exp \rbr{\frac{\sigma_{\Sigma_T}^2 (\bx; \bX_T)}{2}},
%     \end{align}
%     where the second inequality follows from the assumption $\rho_t \leq \lambda_t$, and the third inequality follows from Eq.~\eqref{eq:ext_prop1}. 
%     The above upper bound for the moment-generating function implies the following 
%     concentration inequality for any $\bx\in \mX$ from Chernoff-Hoeffding inequality:
%     \begin{equation}
%         \label{eq:second_rhs_ub}
%         \Pr\rbr{|\bZ_T(\bx)^{\top} \bm{\epsilon}_T| \leq \sqrt{2 \ln \frac{2}{\delta}}\sigma_{\Sigma_T}(\bx; \bX_T)
%         } \geq 1 - \delta.
%     \end{equation}
%     Finally, we obtain the desired result by combining Eq.~\eqref{eq:decomp_error} with Eqs.~\eqref{eq:first_rhs_ub}, \eqref{eq:second_rhs_ub}, and the union bound.
% \end{proof}


% The following lemma is directly obtained from the same proof:
% \begin{lemma}[Non-adaptive confidence bound for stationary noise variance modified from Theorem~1 in \cite{vakili2021optimal}]
%     \label{lem:noisy_cb}
%     Fix any $T \in \N_+$, $\delta \in (0, 1)$, $\lambda^2 > 0$, and suppose Assumptions~\ref{asmp:smoothness}, \ref{asmp:noise} with $\forall t \in \N_+, \rho_t = \rho \geq 0$.
%     Furthermore, assume $\mX$ is finite.
%     Then, if the input sequence $(\bx_t)_{t \in [T]}$ is independent of the noise sequence $(\epsilon_t)_{t \in [T]}$, the following event holds with probability at least $1 - \delta$:
%     \begin{equation}
%         \forall \bx \in \mX,~|f(\bx) - \mu_{\lambda^2 \bI_t}(\bx; \bX_T, \by_T)| \leq \rbr{B + \frac{\rho}{\lambda}\sqrt{2 \ln \frac{2|\mX|}{\delta}}}\sigma_{\lambda^2\bI_T}(\bx; \bX_T),
%     \end{equation}
%     where $\bX_T = (\bx_1, \ldots, \bx_T)$ and $\by_T = (y_1, \ldots, y_T)^{\top}$.
% \end{lemma}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Pseudo Code of PE and MVR}
\label{sec:pseudo_code_pe_mvr}
Algorithms~\ref{alg:pe} and \ref{alg:mvr} show the pseudo-code of PE and MVR, respectively. In the PE algorithm, we denote $\bx_j^{(i)}$, $y_j^{(i)}$, and $\epsilon_j^{(i)}$ as the selected query point, observed output, and the observation noise at step $j$ on $i$-th batch, respectively.

\begin{algorithm}[t!]
    \caption{Phased Elimination (PE)}
    \label{alg:pe}
    \begin{algorithmic}[1]
        \REQUIRE Confidence width parameter $\beta^{1/2} > 0$, initial batch size $N_1$, noise variance parameter $\lambda^2 \geq 0$.
        \STATE Initialize the potential maximizer $\mathcal{X}_{1} \leftarrow \mathcal{X}$.
        \FOR {$i = 1, 2, \ldots$}
            \STATE $\bX_0^{(i)} = \emptyset$.
            \FOR {$j = 1, \ldots, N_i$}
                \STATE $\bm{x}_{j}^{(i)} \leftarrow \mathrm{arg~max}_{\bm{x} \in \mathcal{X}_{i}} \sigma_{\lambda^2\bI_{j-1}}(\bm{x}; \bX_{j-1}^{(i)})$.
                \STATE Observe $y_{j}^{(i)} = f\left(\bm{x}_{j}^{(i)}\right) + \epsilon_{j}^{(i)}$.
                \STATE $\bX_{j}^{(i)} \leftarrow [\bx_m^{(i)}]_{m \in [j]}$.
            \ENDFOR
            \STATE $\bm{y}^{(i)} \leftarrow \sbr{y_{j}^{(i)}}_{k \in [N_i]}$.
            \STATE Calculate $\text{lcb}_{i}(\cdot)$ and $\text{ucb}_{i}(\cdot)$ as
            \begin{align*}
                \text{lcb}_{i}(\bm{x}) &= \mu_{\lambda^2\bI_{N_i}}(\bm{x}; \bX_{N_i}^{(i)}, \by^{(i)}) -\beta^{1/2} \sigma_{\lambda^2\bI_{N_i}}(\bm{x}; \bX_{N_i}^{(i)}), \\
                \text{ucb}_{i}(\bm{x}) &= \mu_{\lambda^2\bI_{N_i}}(\bm{x}; \bX_{N_i}^{(i)}, \by^{(i)}) + \beta^{1/2} \sigma_{\lambda^2\bI_{N_i}}(\bm{x}; \bX_{N_i}^{(i)}).
            \end{align*}
            \STATE $\mathcal{X}_{i+1} \leftarrow \left\{ \bm{x} \in \mathcal{X}_{i} ~\middle|~ \text{ucb}_{i}(\bm{x}) \geq \max\limits_{\tilde{\bm{x}} \in \mathcal{X}_{i}} \text{lcb}_{i}(\tilde{\bm{x}}) \right\}$.
            \STATE Update the batch size $N_{i+1} \leftarrow 2N_i$.
        \ENDFOR
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}[t!]
    \caption{Maximum Variance Reduction (MVR)}
    \label{alg:mvr}
    \begin{algorithmic}[1]
        \REQUIRE Noise variance parameter $\lambda^2 \geq 0$.
        \STATE $\bX_{0} = \emptyset$.
        \FOR {$t = 1, 2, \ldots, T$}
            \STATE $\bm{x}_{t} \leftarrow \mathrm{arg~max}_{\bm{x} \in \mathcal{X}} \sigma_{\lambda^2 \bI_{t-1}}(\bm{x}; \bX_{t-1})$.
            \STATE Observe $y_t$ and construct data $\bX_{t} \coloneqq [\bx_j]_{j \in [t]}$.
        \ENDFOR
        \STATE Return the estimated solution $\hat{\bx}_T \coloneqq \mathrm{arg~max}_{\bm{x} \in \mathcal{X}} \mu_{\lambda^2 \bI_{T}}(\bm{x}; \bX_{T}; \by_T)$, where $\bm{y}_T = \sbr{y_{t}}_{t \in [T]}$.
    \end{algorithmic}
\end{algorithm}

\section{Pseudo Code of VA-PE and VA-MVR}
\label{sec:pseudo_code}
Algorithms~\ref{alg:vape} and \ref{alg:vamvr} show the 
pseudo-code of VA-PE and VA-MVR described in \secref{sec:nsv}, respectively.

\begin{algorithm}[t!]
    \caption{Variance-aware phased elimination (VA-PE)}
    \label{alg:vape}
    \begin{algorithmic}[1]
        \REQUIRE Confidence width parameter $\beta^{1/2} > 0$, finite input set $\mathcal{X}$, initial batch size $N_1 \in \N_+$.
            \STATE Initialize the potential maximizer $\mX_{1} \leftarrow \mX$.
            \FOR {$i = 1, 2, \ldots$}
            \STATE $\bX_0^{(i)} = \emptyset$, $\Sigma_0^{(i)} = \emptyset$.
            \FOR {$j = 1, \ldots, N_i$}
                \STATE $\bm{x}_{j}^{(i)} \leftarrow \mathrm{arg~max}_{\bm{x} \in \mathcal{X}_{i}} \sigma_{\Sigma_{j-1}^{(i)}}(\bm{x}; \bX_{j-1}^{(i)})$.
                \STATE Observe $y_{j}^{(i)} = f\left(\bm{x}_{j}^{(i)}\right) + \epsilon_{j}^{(i)}$.
                \STATE Obtain the variance proxy $\rbr{\rho_j^{(i)}}^2$.
                \STATE $\rbr{\lambda_j^{(i)}}^2 \leftarrow \rbr{\rho_j^{(i)}}^2$.
                \STATE $\bX_{j}^{(i)} \leftarrow [\bx_m^{(i)}]_{m \in [j]}$, $\Sigma_j^{(i)} \leftarrow \mathrm{diag}\rbr{\rbr{\lambda_1^{(i)}}^2, \ldots, \rbr{\lambda_j^{(i)}}^2}$
            \ENDFOR
            \STATE $\bm{y}^{(i)} \leftarrow \sbr{y_{j}^{(i)}}_{k \in [N_i]}$.
            \STATE Calculate $\text{lcb}_{i}(\cdot)$ and $\text{ucb}_{i}(\cdot)$ as
            \begin{align*}
                \text{lcb}_{i}(\bm{x}) &= \mu_{\Sigma_{N_i}^{(i)}}\rbr{\bm{x}; \bX_{N_i}^{(i)}, \by^{(i)}} -\beta^{1/2} \sigma_{\Sigma_{N_i}^{(i)}}\rbr{\bm{x}; \bX_{N_i}^{(i)}}, \\
                \text{ucb}_{i}(\bm{x}) &= \mu_{\Sigma_{N_i}^{(i)}}\rbr{\bm{x}; \bX_{N_i}^{(i)}, \by^{(i)}} + \beta^{1/2} \sigma_{\Sigma_{N_i}^{(i)}}\rbr{\bm{x}; \bX_{N_i}^{(i)}}.
            \end{align*}
            \STATE $\mathcal{X}_{i+1} \leftarrow \left\{ \bm{x} \in \mathcal{X}_{i} ~\middle|~ \text{ucb}_{i}(\bm{x}) \geq \max\limits_{\tilde{\bm{x}} \in \mathcal{X}_{i}} \text{lcb}_{i}(\tilde{\bm{x}}) \right\}$.
            \STATE Update the batch size $N_{i+1} \leftarrow 2N_i$.
        \ENDFOR
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}[t!]
    \caption{Variance-aware maximum variance reduction (VA-MVR)}
    \label{alg:vamvr}
    \begin{algorithmic}[1]
        \REQUIRE Finite input set $\mathcal{X}$.
        \STATE $\bX_{0} = \emptyset$, $\Sigma_0 = \emptyset$.
        \FOR {$t = 1, 2, \ldots, T$}
            \STATE $\bm{x}_{t} \leftarrow \mathrm{arg~max}_{\bm{x} \in \mathcal{X}} \sigma_{\Sigma_{t-1}}(\bm{x}; \bX_{t-1})$.
            \STATE Observe $y_t$, and obtain the variance proxy $\rho_t^2$.
            \STATE Construct data $\bX_{t} \coloneqq [\bx_j]_{j \in [t]}$.
            \STATE $\lambda_t^2 \leftarrow \rho_t^2$, $\Sigma_t \leftarrow \mathrm{diag}(\lambda_1^2, \ldots, \lambda_t^2)$.
        \ENDFOR
        \STATE Return the estimated solution $\hat{\bx}_T \coloneqq \mathrm{arg~max}_{\bm{x} \in \mathcal{X}} \mu_{\Sigma_T}(\bm{x}; \bX_{T}; \by_T)$, where $\bm{y}_T = \sbr{y_{t}}_{t \in [T]}$.
    \end{algorithmic}
\end{algorithm}

\section{VA-GP-UCB Algorithm}
\label{sec:va_gp_ucb_proof}

We consider the GP-UCB-based algorithm as the extension of the variance-aware UCB-style algorithms~\cite{zhou2021nearly,zhang2021improved,zhou2022computationally} in linear bandits.

\paragraph{Algorithm.}
\algoref{alg:vagpucb} shows the pseudo-code of VA-GP-UCB algorithm. 
Overall algorithm construction is almost the same as the GP-UCB algorithm with 
heteroscedastic GP-model. The only difference is the application of the lower threshold $\zeta > 0$ for the variance parameter (Line~7 in \algoref{alg:vagpucb}).
Intuitively, such a lower threshold has a role in preventing an explosion of the MIG in the first term of the elliptical potential count lemma~(\lemref{lem:epcl_nst}) in the analysis. Note that such a lower threshold is also leveraged in the existing variance-aware UCB-style algorithms~\cite{zhou2021nearly,zhang2021improved,zhou2022computationally}. 
We believe this algorithm construction is the most natural kernelized extension of the methods proposed in \cite{zhou2021nearly,zhang2021improved,zhou2022computationally}. 

\paragraph{Theoretical analysis for VA-GP-UCB.}

\begin{lemma}[Adaptive confidence bound in non-stationary variance, e.g., Lemma 7 in \cite{kirschner18heteroscedastic}]
\label{lem:hetero_ada_cb}
Fix any strictly positive sequence $(\lambda_t)_{t\in\N_+}$ and 
define $\Sigma_t$ as $\Sigma_t = \mathrm{diag}(\lambda_1^2, \ldots, \lambda_t^2)$.
Suppose Assumptions~\ref{asmp:smoothness} and \ref{asmp:noise} holds 
with $\rho_t \leq \lambda_t$. Then, for any algorithm, the following event holds with probability at least $1 - \delta$:
\begin{equation}
    \label{eq:ada_cb_ns}
    \forall t \in \N_+,~\forall \bx \in \mX,~|\mu_{\Sigma_{t-1}}(\bx; \bX_{t-1}, \by_{t-1}) - f(\bx)| \leq \rbr{B + \sqrt{2 \gamma_t(\Sigma_t) + 2 \ln \frac{1}{\delta}}} \sigma_{\Sigma_{t-1}}(\bx; \bX_{t-1}).
\end{equation}
\end{lemma}

\begin{lemma}[General regret bound of VA-GP-UCB]
    \label{lem:gen_reg_vagpucb}
    Fix any $\zeta > 0$ and $\delta \in (0, 1)$.
    Suppose Assumptions~\ref{asmp:smoothness} and \ref{asmp:noise} hold. Then, when running \algoref{alg:vagpucb} with $\beta_t^{1/2} = B + \sqrt{2 \gamma_t(\Sigma_t) + 2 \ln \frac{1}{\delta}}$, 
    the following two statements hold with probability at least $1-\delta$:
    \begin{itemize}
        \item The cumulative regret $R_T$ of VA-GP-UCB satisfies
        \begin{equation}
            R_T \leq 2B \min \cbr{4 \overline{\gamma}\rbr{4\gamma_T(\Sigma_T), \zeta^2}, 4\gamma_T(\Sigma_T)} + 4 \beta_T^{1/2} \sqrt{\rbr{V_T + \zeta^2 T} \gamma_T(\Sigma_T)}.
        \end{equation}
        \item The simple regret $r_T$ of VA-GP-UCB satisfies
        \begin{equation}
            r_T \leq \frac{2B}{T} \min \cbr{4 \overline{\gamma}\rbr{4\gamma_T(\Sigma_T), \zeta^2}, 4\gamma_T(\Sigma_T)} + \frac{4\beta_T^{1/2}}{T} \sqrt{\rbr{V_T + \zeta^2 T} \gamma_T(\Sigma_T)}
        \end{equation}
        In the above upper bound, the estimated solution $\hat{\bx}_T$ is defined as 
        $\hat{\bx}_T = \bx_{\tilde{t}}$ with $\tilde{t} \in \mathrm{arg~max}_{t \in [T]} \mathrm{lcb}_t(\bx_t)$. Here, $\mathrm{lcb}_t(\bx_t)$ is defined as $\mathrm{lcb}_t(\bx_t) = \mu_{\Sigma_{t-1}}(\bm{x}_t; \bX_{t-1}, \by_{t-1}) - \beta^{1/2}_t \sigma_{\Sigma_{t-1}}(\bm{x}_t; \bX_{t-1})$.
    \end{itemize}
\end{lemma}
\begin{proof}
    From the construction of $\lambda_t^2$ in \algoref{alg:vagpucb}
    and the definition of $\beta^{1/2}$, the event \eqref{eq:ada_cb_ns} holds with probability at least $1-\delta$.
    Hereafter, we suppose the event \eqref{eq:ada_cb_ns} holds.
    Furthermore, we set $\mT = \{t \in [T] \mid \lambda_t^{-1} \sigma_{\Sigma_{t-1}}(\bx_{t}; \bX_{t-1}) \leq 1\}$
    and $\mT^c = \{t \in [T] \mid \lambda_t^{-1} \sigma_{\Sigma_{t-1}}(\bx_{t}; \bX_{t-1}) > 1\}$. 
    \paragraph{Cumulative regret upper bound.}
    We decompose $R_T$ as $R_T = \sum_{t \in \mT} f(\bx^{\ast}) - f(\bx_t) + \sum_{t \in \mT^c} f(\bx^{\ast}) - f(\bx_t)$, 
    and consider the upper bound of each term separately.
    First, the first term satisfies
    \begin{align}
        \sum_{t \in \mT} f(\bx^{\ast}) - f(\bx_t)
        &\leq 2 \sum_{t \in \mT} \beta_t^{1/2} \sigma_{\Sigma_{t-1}}(\bx; \bX_{t-1}) \\
        &\leq 2 \beta_T^{1/2} \sum_{t \in \mT}  \sigma_{\Sigma_{t-1}}(\bx; \bX_{t-1}) \\
        \label{eq:ub_ft}
        &\leq 4 \beta_T^{1/2} \sqrt{\rbr{\sum_{t \in [T]} \lambda_t^2} \gamma_T(\Sigma_T)} \\
        &\leq 4 \beta_T^{1/2} \sqrt{\rbr{V_T + \zeta^2 T} \gamma_T(\Sigma_T)},
    \end{align}
    where the first inequality follows from the event \eqref{eq:ada_cb_ns} and the UCB-selection rule of $\bx_t$, 
    and the third inequality follows from the same arguments as Eqs.~\eqref{eq:nst_mT}--\eqref{eq:nst_mig}. 
    Regarding the second term, from the extension of elliptical potential count lemma (\lemref{lem:epcl_nst}), 
    we have
    \begin{align}
        \sum_{t \in \mT^c} f(\bx^{\ast}) - f(\bx_t)
        &\leq 2B |\mT^c| \\
        \label{eq:ub_st}
        &\leq 2B \min \cbr{4 \overline{\gamma}\rbr{4\gamma_T(\Sigma_T), \zeta^2}, 4\gamma_T(\Sigma_T)}.
    \end{align}
    From Eqs.~\eqref{eq:ub_ft} and \eqref{eq:ub_st}, 
    we obtain the desired result.

    \paragraph{Simple regret upper bound.}
    From the definition of $\hat{\bx}_T$, for any $t \in \mT$, 
    we have
    \begin{align}
        f(\bx^{\ast}) - f(\hat{\bx}_T)
        &\leq \mathrm{ucb}_t(\bx_t) - \mathrm{lcb}_{\tilde{t}}(\bx_{\tilde{t}}) \\
        &\leq \mathrm{ucb}_t(\bx_t) - \mathrm{lcb}_{t}(\bx_{t}) \\
        &\leq 2\beta_T^{1/2} \sigma_{\Sigma_{t-1}}(\bx; \bX_{t-1}).
    \end{align}
    Furthermore, for any $t \in \mT^c$, 
    we have $f(\bx^{\ast}) - f(\hat{\bx}_T) \leq 2B$.
    By taking the average of the above inequalities, 
    we have
    \begin{align}
        f(\bx^{\ast}) - f(\hat{\bx}_T) 
        &\leq \frac{1}{T} \sbr{\sum_{t \in \mT} 2\beta_T^{1/2} \sigma_{\Sigma_{t-1}}(\bx; \bX_{t-1}) + \sum_{t \in \mT^c} 2B }\\
        &\leq \frac{4\beta_T^{1/2}}{T} \sqrt{\rbr{V_T + \zeta^2 T} \gamma_T(\Sigma_T)} + \frac{2B}{T} \min \cbr{4 \overline{\gamma}\rbr{4\gamma_T(\Sigma_T), \zeta^2}, 4\gamma_T(\Sigma_T)}.
    \end{align}
\end{proof}


\begin{theorem}[Regret bound of VA-GP-UCB for $k=\sek$ and $k = \matk$.]
    \label{thm:cr_sr_vagpucb}
    Let us assume the same setting as that of \lemref{lem:gen_reg_vagpucb}. Furthermore, suppose $V_T = \Omega(1)$.
    Then, the following statements hold 
    with probability at least $1-\delta$, 
    \begin{itemize}
        \item If $k = \sek$, by setting $\zeta^2$ as $\zeta^2 = 1/T$, we have
        \begin{equation}
            R_T = O\rbr{\sqrt{V_T} \ln^{d+1} T}~~\mathrm{and}~~r_T = O\rbr{\sqrt{\frac{V_T}{T^2}} \ln^{d+1} T}.
        \end{equation}
        \item If $k = \matk$, by setting $\zeta^2$ as $\zeta^2 = 1/T$, we have
        \begin{equation}
            R_T = \tilde{O}\rbr{T^{\frac{2d}{2\nu +d}} \sqrt{V_T}}~~\mathrm{and}~~r_T = \tilde{O}\rbr{T^{-\frac{2\nu-d}{2\nu +d}} \sqrt{V_T}}.
        \end{equation}
    \end{itemize}
\end{theorem}
\begin{proof}
    When $k = \sek$, $\gamma_T(\Sigma_T) = O\rbr{\ln^{d+1}\frac{T}{\zeta^2}} = O\rbr{\ln^{d+1} T^2}$ and $4 \beta_T^{1/2} \sqrt{\rbr{V_T + \zeta^2 T} \gamma_T(\Sigma_T)} = O\rbr{\gamma_T(\Sigma_T) \sqrt{V_T}} = O\rbr{\sqrt{V_T} \ln^{d+1} T^2}$. By noting $V_T = \Omega(1)$, we obtain
    \begin{equation}
    R_T = O\rbr{\sqrt{V_T} \ln^{d+1} T}
    \end{equation}
    from \lemref{lem:gen_reg_vagpucb}.
    
    When $k = \matk$, $\gamma_T(\Sigma_T) \leq \gamma_T(\zeta^2 \bI_T) = \tilde{O}\rbr{\rbr{\frac{T}{\zeta^2}}^{\frac{d}{2\nu+d}}} = \tilde{O}\rbr{T^{\frac{2d}{2\nu+d}}}$, $\overline{\gamma}\rbr{4\gamma_T(\Sigma_T), \zeta^2} = \tilde{O}\rbr{T^{\rbr{\frac{d}{2\nu+d}}^2} (\zeta^2)^{-\frac{d}{2\nu+d} - \rbr{\frac{d}{2\nu+d}}^2}} = \tilde{O}\rbr{T^{\sbr{2\rbr{\frac{d}{2\nu+d}}^2 + \frac{d}{2\nu+d}}}}$, and $4 \beta_T^{1/2} \sqrt{\rbr{V_T + \zeta^2 T} \gamma_T(\Sigma_T)} = O\rbr{\gamma_T(\Sigma_T) \sqrt{V_T}} = \tilde{O}\rbr{ T^{\frac{2d}{2\nu+d}} \sqrt{V_T}}$. 
    By noting $V_T = \Omega(1)$, we can conclude the following from \lemref{lem:gen_reg_vagpucb}:
    \begin{equation}
    R_T = \tilde{O}\rbr{T^{\frac{2d}{2\nu +d}} \sqrt{V_T}}.
    \end{equation}
    Finally, by comparing the upper bound of $r_T$ and $R_T$ in \lemref{lem:gen_reg_vagpucb},
    we can also confirm that the simple regret bounds are followed by multiplying $1/T$ to the above cumulative regret upper bounds. 
\end{proof}

\begin{remark}
    In \thmref{thm:cr_sr_vagpucb}, we suppose that the learner 
    does not have prior knowledge about $V_T$ before running the algorithm.
    On the other hand, if we assume the learner knows $V_T$, 
    the regret upper bound in \thmref{thm:cr_sr_vagpucb} 
    can be smaller by setting the lower threshold $\zeta^2$ depending on $V_T$. Specifically, when $k = \matk$, the setting $\zeta^2 = V_T/T$ improves the polynomial dependence of $V_T$.
\end{remark}

\begin{algorithm}[t!]
    \caption{Variance-aware Gaussian process upper confidence bound (VA-GP-UCB)}
    \label{alg:vagpucb}
    \begin{algorithmic}[1]
        \REQUIRE Confidence width parameter $\beta_t^{1/2} > 0$, lower threshod $\zeta > 0$ of variance parameters.
        \STATE $\bX_0 = \emptyset$, $\by_0 = \emptyset$, $\Sigma_0 = \emptyset$.
        \FOR {$t = 1, \ldots, T$}
            \STATE Compute $\mathrm{ucb}_{t}(\bx) \coloneqq \mu_{\Sigma_{t-1}}(\bm{x}; \bX_{t-1}, \by_{t-1}) + \beta^{1/2} \sigma_{\Sigma_{t-1}}(\bm{x}; \bX_{t-1})$.
            \STATE $\bm{x}_t \leftarrow \mathrm{arg~max}_{\bm{x} \in \mathcal{X}} \mathrm{ucb}_{t}(\bx)$.
            \STATE Observe $y_t = f\left(\bm{x}_t\right) + \epsilon_t$.
            \STATE Obtain the variance proxy $\rho_t^2 \geq 0$.
            \STATE $\lambda_t^2 \leftarrow \max\cbr{\rho_t^2, \zeta^2}$.
            \STATE $\bX_{t} \leftarrow [\bx_m]_{m \in [t]}$, $\Sigma_t \leftarrow \mathrm{diag}\rbr{\lambda_1^2, \ldots, \lambda_t^2}$.
        \ENDFOR
    \end{algorithmic}
\end{algorithm}

\section{Potential Applications for Non-Stationary Variance Setting in GP-Bandits}
\label{sec:pot_app_nsv}
In this section, We discuss potential applications for the non-stationary variance setting in GP-bandits.

\begin{itemize}
    \item \textbf{Reinforcement Learning:} The existing variance dependent algorithms in linear bandits~\cite{zhou2021nearly,zhang2021improved,zhou2022computationally,zhao2023variance}
    motivates the non-stationary variance setting as one of the online reinforcement learning problems. 
    Specifically, \citet{zhou2021nearly} consider the regret minimization problem under a specific Markov decision process (MDP), which is called linear mixture MDP, and subsumes the linear bandit problem when the length of the horizon is $1$. In the decision-making under linear mixture MDP, \citet{zhao2023variance} show the algorithm with variance-dependent regret guarantees.
    If we assume the linear mixture MDP whose feature map of the transition probability is infinite-dimensional and induced by some kernel function, the kernelized extension of the setting of the existing works~\cite{zhou2021nearly,zhang2021improved,zhao2023variance} is naturally derived. Extensions in such a reinforcement learning setting are an interesting direction for our future research.
    \item \textbf{Experimental Design:} In scientific experiments, the observation noise level of the result of the experiment may vary over time. Specifically, the observation noise level can increase over extended periods due to factors such as environmental fluctuations, material degradation, or systematic drifts in measurement instruments. For example, such factors in measurement accuracy have been observed in studies of chemical analysis~\cite{hickstein2018rapid}.
    \item \textbf{Stationary Setting with Heteroscedastic Variance:} \citet{kirschner18heteroscedastic} consider the heteroscedastic variance setting where the variance proxy $\rho^2(\bx_t)$ may depend on the selected input $\bx_t$. If we apply our algorithm to a heteroscedastic setting, the resulting regret of our non-stationary variance algorithm is quantified by the cumulative variance proxy $V_T = \sum_{t=1}^T \rho^2(\bx_t)$ of the selected inputs\footnote{Our VA-GP-UCB algorithm can be applied in the same conditionally sub-Gaussian assumption as used in \cite{kirschner18heteroscedastic} (Eq.~(1) in \cite{kirschner18heteroscedastic}). On the other hand, VA-PE and VA-MVR algorithms require a more restricted independence assumption that the noise $(\epsilon_t)$ are conditionally independent given the MVR input sequence.}. The further precise quantification of the increasing speed of $V_T$ in this setting requires an additional structural assumption about $\rho^2(\cdot)$. For example, we expect $V_T$ is increasing sublinearly with our algorithm design of $\bx_t$ if there exists a unique maximizer $\bx^{\ast}$, and $\rho^2(\bx) \rightarrow \rho^2(\bx^{\ast})$ (as $\bx \rightarrow \bx^{\ast}$) and $\rho^2(\bx^{\ast}) = 0$ holds. We believe this research direction is an interesting application of our analysis to the heteroscedastic setting.
\end{itemize}

\end{document}