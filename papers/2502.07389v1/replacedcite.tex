\section{Related Work}
\label{related}

Supervised heart anomaly detection approaches have relied heavily on labeled datasets to train models for heartbeat classification. These methods, while effective, face limitations due to the labor intensive process of obtaining labeled data and the variability of ECG signals between different patients and conditions ____. Moreover, the lack of labeled data in real-life scenarios hinders the deployment of such supervised techniques. To address these challenges, recent research has increasingly focused on self-supervised and unsupervised DL for ECG analysis ____. Self-supervised learning, in particular, has gained attraction due to its ability to deal with unlabeled data and its effectiveness in leveraging different datasets to learn robust representations that improve downstream task performance. In general, this approach involves creating surrogate or pretext tasks that do not require manual annotations. Specifically, pretext methodologies can be divided into contrastive, predictive, and generative categories. In the context of ECG, different works have employed self-supervised learning frameworks for anomaly detection using such pretext task techniques ____. 

First, for the contrastive approach, although most of the recent works are targeting ECG-based arrhythmia recognition ____, there are also some studies applying such methodology and derivatives for general cardiac anomaly detection. For instance, in ____, a task-oriented self-supervised contrastive learning approach, initially devised for anomaly detection in electroencephalography, was proposed to tackle ECG anomaly detection. This method uses a 3-class convolutional neural network trained on artificially generated abnormal ECG data, simulating anomalies by varying waveform amplitude and frequency among individuals. This innovative approach was validated over an open dataset and reported a 75\% F1-score for a binary ECG anomaly detection task. However, depending on simulated anomalies brings up questions about the method's capacity to effectively represent the intricate and varied nature of real-world ECG irregularities, highlighting the need for a more suitable self-supervised learning approach. In a similar contrastive vein to the previous work, in ____, the authors adapted state-of-the-art self-supervised methods, precisely instance discrimination and latent forecasting, to train models on 12-lead ECG data without heavily relying on labeled datasets. Utilizing public datasets, they demonstrated that pretrained models achieve near-supervised performance. Although this study establishes a compelling case for self-supervised techniques in biosignal representation learning and delves into some of the advantages of fine-tuning a self-supervised pre-trained model in a contrastive context, they are still relying on augmentations to generate semantically equivalent views of the original ECG data. The latter can sometimes introduce synthetic artifacts that do not fully capture the complexity and variability of real-world ECG signals. Secondly, the self-supervised predictive learning pipeline can be spotted in different works dealing with cardiac anomaly classification ____. From these, ____ stands out as a recent research effort to propose a self-supervised predictive learning system for general ECG anomaly detection. Specifically, the authors employed a trend-assisted method that is fed from a random masked multi-scale restoration to obtain an ECG trend feature. This is done following both a global and a local ECG analysis to end up with global-local ECG pairs features. Note that for the local restoration they perform heartbeat segmentation. Finally, they employed the ECG reports as attributes or labels following a self-supervised predictive scheme to guide the model training using the features extracted. Their results using open datasets reported an F1-score of 88.33\%, outperforming the state-of-the-art. However, they rely on the same dataset for both training and testing, and they do not give details on how the partition between both sets was done %, raising concerns about potential overfitting and the robustness of such approach in real-world scenarios.
. Additionally, the model's complexity and the requirement for detailed patient-specific information may limit its applicability in settings where such comprehensive data is not readily available. A similar approach is presented in ____ where the authors proposed a novel model using transformer-based architecture for detecting anomalies in ECG signals. The main objective is to utilize the transformer's capability to capture long-distance dependencies in time series data, improving anomaly detection performance. The model, comprising an embedding layer and a transformer encoder, was tested on different datasets, achieving an 89.5\% accuracy and a 92.3\% F1 score for the MIT-BIH dataset. However, the need for substantial computational resources and careful configuration of the transformer's model parameters make such a system challenging to implement in practice. Finally, different self-supervised generative learning systems are also found pushing and going towards a fully unsupervised methodology ____ but require a final supervised approach to detect anomalies. %For instance, the authors in ____ presented a novel approach using transformer-based architecture for detecting anomalies in ECG signals. The main objective is to utilize the transformer's capability to capture long-distance dependencies in time series data, improving anomaly detection performance. The model, comprising an embedding layer and a transformer encoder, was tested on different datasets, achieving an 89.5\% accuracy and a 92.3\% F1 score for the MIT-BIH dataset. However, the need for substantial computational resources and careful configuration of the transformer's model parameters make such system challenging to implement in practice. 
Note that, unlike the contrastive and the predictive approaches, generative pipelines learn to model the entire data distribution. This enables them to capture a wide variety of features and patterns in the data, leading to richer and more diverse representations. Thus, these systems can provide a better holistic understanding of the data, which can be useful for downstream tasks.

Within this context, self-supervised generative learning offers the adequate tools to train models on large amounts of unlabeled data, leading to systems that are then fine-tuned with a smaller amount of data, enhancing their generalization capabilities. The latter fact is key when facing real-life scenarios where there is a scarcity of labels. In fact, more efforts are needed to generate pipelines that are applicable to real-life settings. For example, both the authors in ____ and in ____ presented systems that aim to provide domain adaptation features. The former does it in a user-specific manner, while the latter employs memory modules embedded into the proposed model that are dynamically updated to reflect changes in the data distribution. 


As a result of the limitations discussed previously found in the literature in the context of self-supervised anomaly detection of the ECG, this work presents a self-supervised generative learning framework. Specifically, this proposal emerges as a particularly innovative solution by bringing a forecasting pretext upstream task together with a domain adaption process using a completely different dataset, to later being able to perform general and fully unsupervised cardiac anomaly detection.


\begin{figure*}[tbh]
\centering
\includegraphics{figs/overview.drawio.pdf}
\caption{\textbf{Proposed System.} General functionality of our proposed system for ECG forecasting with domain adaptation and heart anomaly detection.}
\label{fig:system}
\end{figure*}