\section{Provably Efficient Criteria}\label{sec:app_efficient_critieria}

\subsection{Running Time Analysis for Inference Pipeline of Origin FlowAR Architecture }\label{sec:runtime_origin_flowar}

We proceed to compute the total running time for the inference pipeline of the origin FlowAR architecture.
\begin{lemma}[Inference Runtime of Original FlowAR Architecture, formal version of Lemma~\ref{lem:runtime_old_flowar_informal}]\label{lem:runtime_old_flowar_formal}
    Consider the original FlowAR inference pipeline with the following parameters:
    \begin{itemize}
        \item {\bf Input tensor:} $\X \in \R^{h \times w \times c}$. Assume $h=w=n$ and $c = O(\log n)$.
        \item {\bf Number of scales:} $K = O(1)$.
        \item {\bf Scale factor:} For $i \in [K]$, $r_i:= a^{K-i}$ where base factor $a \in \mathbb{N}^+$.
        \item {\bf Upsampling functions}  For $i \in [K]$, $\phi_{\mathrm{up},i}(\cdot,a)$ from Definition~\ref{def:bicubic_up_sample_function}.
        \item {\bf Attention layer:}  For $i \in [K]$, $\mathsf{Attn}_i(\cdot)$ which acts on flattened sequences of dimension defined in Definition~\ref{def:attn_layer}.
        \item {\bf Feed forward layer: } For $i \in [K]$, $\mathsf{FFN}_i(\cdot)$ which acts on flattened sequences of dimension defined in Definition~\ref{def:ffn}.
        \item {\bf Flow matching layer:} For $i \in [K]$, $\mathsf{NN}_i(\cdot,\cdot,\cdot)$ denote the flow-matching layer defined in Definition~\ref{def:flow_matching_architecture}.
    \end{itemize}
    Under these conditions, the total inference runtime of FlowAR is bounded by $O(n^{4+o(1)})$.
    
\end{lemma}
\begin{proof}
    {\bf Part 1: Running time of bicubic up-sample Layer.} The $i$-th layer pf FlowAR model contains $\phi_{\mathrm{up},1}(\cdot,2),\dots,\phi_{\mathrm{up},i-1}(\cdot,2)$. Considering $\phi_{\mathrm{up},i-1}(\cdot,2)$, this operation needs $O(n^{2}c/2^{2(K-i)})$ time. Then the total time required for upsampling in the i-th layer of the FlowAR architecture is $O( n^2 c \cdot \frac{1}{2^{2K}} \cdot (1 - \frac{1}{4^i}))$ which is due to simple algebra. Hence, the total runtime for all bicubic up sample functions is
    \begin{align*}
        \mathcal{T}_{\mathrm{up}} = &~ \sum_{i=1}^K O( n^2 c \cdot \frac{1}{2^{2K}} \cdot (1 - \frac{1}{4^i}))\\
        =&~ O(n^{2+o(1)})
    \end{align*}
    where the first equation is derived from summing up all the running time of the up sample functions, the second step is due to simple algebra and $K = O(1)$ and $c = O(\log n)$.

    {\bf Part 2: Running time of Attention Layer.} The input size of the $i$-th attention layer $\mathsf{Attn}_i$ is $\sum_{j=1}^i (n/2^{K-j}) \times \sum_{j=1}^i(n/2^{K-j}) \times c $. So the time needed to compute the $i$-th attention layer is $O(n^4c \cdot (2^i-1)^4/2^{4K-4})$. Hence, the total runtime for all attention layers is
    \begin{align*}
        \mathcal{T}_{\mathrm{Attn}} =&~ \sum_{i=1}^K O( n^4c \cdot (2^i-1)^4/2^{4K-4})\\
        =&~ O(n^{4+o(1)})
    \end{align*}
    The first equation is derived from summing up all the running time of the attention layer, the second step is due to simple algebra and $K = O(1)$ and $c = O(\log n)$.

    {\bf Part 3: Running time of FFN Layer.} The input size of the $i$-th FFN layer $\mathsf{FFN}_i$ is $\sum_{j=1}^i (n/2^{K-j}) \times \sum_{j=1}^i(n/2^{K-j}) \times c $. So by Definition~\ref{def:ffn}, we can easily derive that the time needed to compute the $i-$th FFN layer is $O(n^2c^2 (2^i-1)^2/2^{2K-2} )$. Hence, the total runtime for all FFN layers is
    \begin{align*}
        \mathcal{T}_{\mathrm{FFN}} = &~ \sum_{i=1}^K O(n^2c^2 (2^i-1)^2/2^{2K-2}) \\
        =&~ O(n^{2+o(1)})
    \end{align*}
    The first step is derived from summing up all the running time of the FFN layer,  and the second step is due to simple algebra and $K = O(1)$ and $c = O(\log n)$.

    {\bf Part 4: Running time of Flow Matching Layer.} The input size of the $i$-th flow-matching layer $\mathsf{NN}_i$ is $ (n/2^{K-i}) \times (n/2^{K-i}) \times c $. It's trivially that the running time of the flow-matching layer will be dominated by the running time of the attention layer, which is $O(n^{4}c/ 2^{4(K-i)})$ (see Part 2 of Definition~\ref{def:flow_matching_architecture}). Hence, the total runtime for all flow-matching layers is 
    \begin{align*}
        \mathcal{T}_{\mathsf{FM}} =&~ \sum_{i=1}^K O(n^4 c /2^{4(K-i)})\\
        =&~ O(n^{4+o(1)})
    \end{align*}
    The first step is derived from summing up all the running time of the origin flow-matching layer, and the second step is due to simple algebra and $K = O(1)$ and $c = O(\log n)$.

    Then, by summing up Part 1 to Part 4, we can get the total running time for FlowAR architecture, which is
    \begin{align*}
        \mathcal{T}_{\mathrm{ori}} =&~ \mathcal{T}_{\mathrm{up}} + \mathcal{T}_{\mathrm{Attn}} + \mathcal{T}_{\mathrm{FFN}} + \mathcal{T}_{\mathsf{FM}}\\
        =&~ O(n^{4+o(1)})
    \end{align*}
\end{proof}
Lemma~\ref{sec:runtime_origin_flowar} demonstrates the runtime required for the original FlowAR architecture, from which we can deduce that the dominant term in the runtime comes from the computation of the Attention Layer.
\subsection{Running Time Analysis for Inference Pipeline of Fast FlowAR Architecture }\label{sec:runtime_fast_flowar}
In this section, we apply the conclusions of \cite{as23} to the FlowAR architecture, where all Attention modules in FlowAR are computed using the Approximate Attention Computation defined in Definition~\ref{def:aattc}.
\begin{lemma}[Inference Runtime of Fast FlowAR Architecture, formal version of Lemma~\ref{lem:runtime_fast_flowar_informal}]\label{lem:runtime_fast_flowar_formal}
    Consider the original FlowAR inference pipeline with the following parameters:
    \begin{itemize}
        \item {\bf Input tensor:} $\X \in \R^{h \times w \times c}$. Assume $h=w=n$ and $c = O(\log n)$.
        \item {\bf Number of scales:} $K = O(1)$.
        \item {\bf Scale factor:} For $i \in [K]$, $r_i:= a^{K-i}$ where base factor $a \in \mathbb{N}^+$.
        \item {\bf Upsampling functions}  For $i \in [K]$, $\phi_{\mathrm{up},i}(\cdot,a)$ from Definition~\ref{def:bicubic_up_sample_function}.
        \item {\bf Approximate Attention layer:}  For $i \in [K]$, $\mathsf{AAttC}_i(\cdot)$ defined in Definition~\ref{def:aattc}.
        \item {\bf Feed forward layer: } For $i \in [K]$, $\mathsf{FFN}_i(\cdot)$ which acts on flattened sequences of dimension defined in Definition~\ref{def:ffn}.
        \item {\bf Fast flow-matching layer:} For $i \in [K]$, $\mathsf{FNN}_i(\cdot,\cdot,\cdot)$ denote the fast flow-matching layer defined in Definition~\ref{def:fast_flow_matching_architecture}.
    \end{itemize}
    Under these conditions, the total inference runtime of FlowAR is bounded by $O(n^{2+o(1)})$.
      
\end{lemma}
\begin{proof}
    {\bf Part 1: Running time of bicubic up-sample Layer.} The runtime of the upsample function in the fast FlowAR architecture is the same as that in the original FlowAR architecture, which is
    \begin{align*}
        \mathcal{T}_{\mathrm{up}} =  O(n^{2+o(1)})
    \end{align*}
    

    {\bf Part 2: Running time of Attention Layer.} The input size of the $i$-th approximate attention computation layer $\mathsf{AAttC}_i$ is $\sum_{j=1}^i (n/2^{K-j}) \times \sum_{j=1}^i(n/2^{K-j}) \times c $. So the time needed to compute the $i$-th attention layer is $O(n^{2+o(1)} \cdot (2^i-1)^4/2^{4K-4})$. Hence, the total runtime for all attention layers is
    \begin{align*}
        \mathcal{T}_{\mathrm{Attn}} =&~ \sum_{i=1}^K O( n^{2+o(1)} \cdot (2^i-1)^4/2^{4K-4})\\
        =&~ O(n^{2+o(1)})
    \end{align*}
    The first equation is derived from summing up all the running time of the approximate attention computation layer, and the second equation is due to basic algebra and $K = O(1)$.

    {\bf Part 3: Running time of FFN Layer.} The runtime of the FFN layer in the fast FlowAR architecture is the same as that in the original FlowAR architecture, which is
    \begin{align*}
        \mathcal{T}_{\mathrm{FFN}} =  O(n^{2+o(1)})
    \end{align*}

    {\bf Part 4: Running time of Flow Matching Layer.} For each $i \in [K]$, the input size of the $i$-th fast flow-matching layer $\mathsf{FNN}_i$ is $ (n/2^{K-i}) \times (n/2^{K-i}) \times c $. By Definition~\ref{def:mlp}, we can know that the total computational time for the MLP layer is $O(n^{2+o(1)})$, which is due to $c=O(\log n)$. Then by Lemma~\ref{lem:as23_attention}, we can speed up the attention computation from $O(n^{4+o(1)})$ to $O(n^{2+o(1)})$. Hence, the total runtime for all flow-matching layers is
     \begin{align*}
        \mathcal{T}_{\mathrm{Attn}} =&~ \sum_{i=1}^K O( n^{2+o(1)})\\
        =&~ O(n^{2+o(1)})
    \end{align*}
    The equation is due to $K = O(1)$.

    Then, by summing up Part 1 to Part 4, we can get the total running time for fast FlowAR architecture, which is
    \begin{align*}
        \mathcal{T}_{\mathrm{fast}} =&~ \mathcal{T}_{\mathrm{up}} + \mathcal{T}_{\mathrm{Attn}} + \mathcal{T}_{\mathrm{FFN}} + \mathcal{T}_{\mathsf{FM}}\\
        =&~ O(n^{2+o(1)})
    \end{align*}
    
    
\end{proof}

\subsection{Error Analysis of \texorpdfstring{$\mathsf{MLP}(\X')$}{} and \texorpdfstring{$\mathsf{MLP}(\X)$}{}}\label{sec:error_analysis_of_mlp_x_prime_mlp_x}
We conduct the error analysis between $\mathsf{MLP}(\X')$ and $\mathsf{MLP}(\X)$ where $\X'$ is the approximation version of $\X$.
\begin{lemma}[Error analysis of MLP Layer]\label{lem:error_analysis_mlp}
    If the following conditions hold:
    \begin{itemize}
        \item Let $\X \in \R^{h \times w \times c}$ denote the input tensor.
        \item Let $\X' \in \R^{h \times w \times c}$ denote the approximation version of input tensor $\X$.
        \item Let $\epsilon \in (0, 0.1)$ denote the approximation error. 
        \item Suppose we have $\| \X' - \X \|_\infty \leq \epsilon$.
        \item Let $R > 1$.
        \item Assume the value of each entry in matrices can be bounded by $R$.  
        \item Let $\mathsf{MLP}(\cdot,c,d)$ denote the MLP layer defined in Definition~\ref{def:mlp}.
    \end{itemize}
    We can demonstrate the following
    \begin{align*}
        \|\mathsf{MLP}(\X') - \mathsf{MLP}(\X)\|_\infty \leq cR\epsilon
    \end{align*}
    Here, we abuse the $\ell_\infty$ norm in its tensor form for clarity.
\end{lemma}
\begin{proof}
    We can show that for $i \in [h],j \in [w], l \in [c]$, we have
    \begin{align*}
        \|\mathsf{MLP}(\X',c,d)_{i,j,*} - \mathsf{MLP}(\X,c,d)_{i,j,*}\|_\infty =&~ \| \X'_{i,j,*}\cdot W - \X_{i,j,*} \cdot W \|_\infty\\
        \leq&~ \|\underbrace{(\X'_{i,j,*}-\X_{i,j,*})}_{1 \times c} \cdot \underbrace{W}_{c\times d} \|_\infty \\
        \leq &~ c \cdot \|\underbrace{(\X'_{i,j,*}-\X_{i,j,*})}_{1 \times c}\|_\infty \cdot \|\underbrace{W}_{c\times d} \|_\infty\\
        \leq&~ c \cdot R \cdot \epsilon
    \end{align*}
    The first equation is due to Definition~\ref{def:mlp}, the second inequality is derived from simple algebra, the third inequality is a consequence of basic matrix multiplication, and the last inequality comes from the conditions of this lemma.

    Then by the definition of $\ell_\infty$ norm, we can easily get the proof.
\end{proof}



\subsection{Error Analysis of \texorpdfstring{$\mathsf{AAttC}(\X')$}{} and \texorpdfstring{$\mathsf{Attn}(\X)$}{}}\label{sec:error_analysis_of_aattc_x_prime_attn_x}
We conduct the error analysis between $\mathsf{AAttC}(\X')$ and $\mathsf{Attn}(\X)$ where $\X'$ is the approximation version of $\X$.
\begin{lemma}[Error analysis of $\mathsf{AAttC}(X')$ and $\mathsf{Attn}(X)$, Lemma B.4 of \cite{kll+25}]\label{lem:error_analysis_aattc_attn}
    If the following conditions hold:
    \begin{itemize}
        \item Let $\X \in \R^{h \times w \times c}$ denote the input tensor.
        \item Let $\X' \in \R^{h \times w \times c}$ denote the approximation version of input tensor $\X$.
        \item Let $\epsilon \in (0, 0.1)$ denote the approximation error. 
        \item Suppose we have $\| \X' - \X \|_\infty \leq \epsilon$.
        \item Let $R > 1$.
        \item Assume the value of each entry in matrices can be bounded by $R$. 
        \item Let $\mathsf{Attn}$ denote the attention layer defined in Definition~\ref{def:attn_layer}.
        \item Let $\mathsf{AAttC}$ denote the approximated attention layer defined in Definition~\ref{def:aattc}.
        \item Let $U,V \in \R^{hw \times k}$ be low-rank matrices constructed for polynomial approximation of attention matrix $\mathsf{AAttC}(\X)$.
        \item Let $f$ be a polynomial with degree $g$.
    \end{itemize}
    We can demonstrate the following:
    \begin{align*}
        \| \mathsf{AAttC}(\X') - \mathsf{Attn}(\X) \|_\infty \leq O( k R^{g+1} c) \cdot \epsilon
    \end{align*}
    Here, we abuse the $\ell_\infty$ norm in its tensor form for clarity.
\end{lemma}

\subsection{Error Analysis of \texorpdfstring{$\mathsf{FFN}(\X')$}{} and \texorpdfstring{$\mathsf{FFN}(\X)$}{}}\label{sec:error_analysis_of_ffn_x_prime_ffn_x}
In this section, we conduct the error analysis between $\mathsf{FFN}(\X')$ and $\mathsf{FFN}(\X)$ where $\X'$ is the approximation version of $\X$.
\begin{lemma}[Error analysis of $\mathsf{FFN}(\X')$ and $\mathsf{FFN}(\X)$]\label{lem:error_analysis_ffn}
    If the following conditions hold:
    \begin{itemize}
        \item Let $\X \in \R^{h \times w \times c}$ denote the input tensor.
        \item Let $\X' \in \R^{h \times w \times c}$ denote the approximation version of input tensor $\X$.
        \item Let $\epsilon \in (0, 0.1)$ denote the approximation error. 
        \item Suppose we have $\| \X' - \X \|_\infty \leq \epsilon$.
        \item Let $R > 1$.
        \item Assume the value of each entry in matrices can be bounded by $R$. 
        \item Let $\mathsf{FFN}$ denote the FFN layer defined in Definition~\ref{def:ffn}.
        \item Let the activation function $\sigma(\cdot)$ in $\mathsf{FFN}$ be the ReLU activation function.
    \end{itemize}
    We can demonstrate the following:
    \begin{align*}
        \| \mathsf{FFN}(\X') - \mathsf{FFN}(\X) \|_\infty \leq O(c^2 R^2) \cdot \epsilon
    \end{align*}
    Here, we abuse the $\ell_\infty$ norm in its tensor form for clarity.
\end{lemma}
\begin{proof}
    Firstly we can bound that for $i \in [h], j \in [w]$
    \begin{align}\label{eq:linear_transformation_bound}
        \| (\X'_{i,j,*}\cdot W_1 +b_1) - (\X_{i,j,*}\cdot W_1 +b_1)\|_\infty =&~ \| \underbrace{(\X'_{i,j,*}-\X_{i,j,*})}_{1 \times c} \cdot \underbrace{W_1}_{c\times c}\|_\infty\notag\\
        \leq&~ c \cdot \|\X'_{i,j,*}-\X_{i,j,*}\|_\infty \|W_1 \|_\infty\notag\\
        \leq&~ c \cdot \epsilon \cdot R
    \end{align}
    The first equation comes from basic algebra, the second inequality is due to basic matrix multiplication, and the last inequality follows from the conditions of this lemma.
    
    We can show that for $i \in [h], j \in [w]$,
    \begin{align*}
        \| \mathsf{FFN}(\X')_{i,j,*} - \mathsf{FFN}(\X)_{i,j,*} \|_\infty=&~ \| \X'_{i,j,*}-\X_{i,j,*} +\underbrace{ (\sigma(\X_{i,j,*}\cdot W_1 + b_1)- \sigma(\X'_{i,j,*}\cdot W_1 + b_1))}_{1 \times c} \cdot \underbrace{W_2}_{c\times c}\|_\infty \\
        \leq&~ \| \X'_{i,j,*}-\X_{i,j,*}\|_\infty +c\cdot \|W_2\|_\infty \cdot \|\sigma(\X_{i,j,*}\cdot W_1 + b_1)- \sigma(\X'_{i,j,*}\cdot W_1 + b_1)\|_\infty\\
        \leq&~ \epsilon + c R \cdot \| (\X'_{i,j,*} W_1 +b_1) - (\X_{i,j,*} W_1 +b_1)\|_\infty\\
        \leq&~ \epsilon + c^2 R^2 \cdot \epsilon \\
        =&~ O(c^2 R^2) \cdot \epsilon
    \end{align*}
    The first equation is due to Definition~\ref{def:ffn}, the second step follows from triangle inequality and basic matrix multiplication, the third step follows from the property of ReLU activation function and basic algebra, the fourth step follows from Eq.~\eqref{eq:linear_transformation_bound}, and the last step follows from simple algebra.
\end{proof}

\subsection{Error Analysis of \texorpdfstring{$\phi_{\mathrm{up}}(\X')$}{} and \texorpdfstring{$\phi_{\mathrm{up}}(\X)$}{}}\label{sec:error_analysis_of_phi_x_prime_phi_x}
In this section, we conduct the error analysis between $\phi_{\mathrm{up}}(\X')$ and $\phi_{\mathrm{up}}(\X)$ where $\X'$ is the approximation version of $\X$.
\begin{lemma}[Error Analysis of Up Sample Layer, Lemma B.5 of \cite{kll+25}]\label{lem:error_analysis_up_layer}
If the following conditions hold:
\begin{itemize}
    \item Let $\X \in \R^{h \times w \times c}$ denote the input tensor.
    \item Let $\X' \in \R^{h \times w \times c}$ denote the approximation version of input tensor $\X$.
    \item Let $a = 2$ denote a positive integer.
    \item Let $\phi_{\mathrm{up}, i}(\cdot, a)$ be the bicubic up sample function defined in Definition~\ref{def:bicubic_up_sample_function}.
    \item Let $\epsilon \in (0,0.1)$ denote the approximation error.
    \item Let $\|X-X'\|_\infty\leq \epsilon$.
\end{itemize}
Then we have
\begin{align*}
    \| \phi_{\rm up}(\X',a) - \phi_{\rm up}(\X,a) \|_\infty \leq O(\epsilon)
\end{align*}
Here, we abuse the $\ell_\infty$ norm in its tensor form for clarity.
\end{lemma}

\subsection{Error Analysis of \texorpdfstring{$\mathsf{FNN}(\F'^t,\X',t)$}{} and \texorpdfstring{$\mathsf{NN}(\F^t,\X,t)$}{}}\label{sec:error_analysis_of_flow_matching_layer}
In this section, we conduct the error analysis between $\mathsf{FNN}(\F'^t,\X',t)$ and  $\mathsf{NN}(\F^t,\X,t)$ where $\X'$ is the approximation version of $\X$.
\begin{lemma}[Error Analysis of Flow Matching Layer]\label{lem:error_analysis_flow_matching_layer}
If the following conditions hold:
\begin{itemize}
    \item Let $\X \in \R^{h \times w \times c}$ denote the input tensor.
    \item Let $\X' \in \R^{h \times w \times c}$ denote the approximation version of input tensor $\X$.
    \item Let $\F^t,\mathsf{FF}^t \in \R^{h \times w \times c}$ be the interpolated input defined in Definition~\ref{def:flow}.
    \item Let $\mathsf{NN}(\cdot,\cdot,\cdot)$ denote flow-matching layer defined in Definition~\ref{def:flow_matching_architecture}.
    \item Let $\mathsf{FNN}(\cdot,\cdot,\cdot)$ denote fast flow-matching layer defined in Definition~\ref{def:fast_flow_matching_architecture}.
     \item Let $\mathsf{Attn}$ denote the attention layer defined in Definition~\ref{def:attn_layer}.
    \item Let $\mathsf{AAttC}$ denote the approximated attention layer defined in Definition~\ref{def:aattc}.
    \item Let $R > 1$.
    \item Assume the value of each entry in matrices can be bounded by $R$. 
    \item Let $U,V \in \R^{hw \times k}$ be low-rank matrices constructed for polynomial approximation of attention matrix $\mathsf{AAttC}(\X)$.
    \item Let $f$ be a polynomial with degree $g$.
    \item Let $\epsilon \in (0,0.1)$ denote the approximation error.
    \item Let $\|\X-\X'\|_\infty\leq \epsilon$.
    \item Let $t \in [0,1]$ denote a time step.
    \item Assume that Layer-wise Norm layer $\mathsf{LN}(\cdot)$ defined in Definition~\ref{def:ln} does not exacerbate the propagation of errors, i.e., if $\|X'-X\|_\infty \leq \epsilon$, then $\|\mathsf{LN}(X')-\mathsf{LN}(X)\|_\infty \leq \epsilon$.
\end{itemize}
Then we have
\begin{align*}
    \| \mathsf{FNN}(\mathsf{FF}^t,\X',t) - \mathsf{NN}(\F^t,\X,t) \|_\infty \leq O(kR^{g+6}c^3) \cdot \epsilon
\end{align*}
Here, we abuse the $\ell_\infty$ norm in its tensor form for clarity.
\end{lemma}
\begin{proof}
    Firstly, we can show that
    \begin{align*}
        \|\mathsf{FF}^t- \F^t\|_\infty = \| t (\X' - \X) \|_\infty \leq \epsilon
    \end{align*}
    The inequality comes from $t \in [0,1]$ and $\|\X'-\X\|_\infty \leq \epsilon$.

    By {\bf Step 1} of Definition~\ref{def:flow_matching_architecture} and Definition~\ref{def:fast_flow_matching_architecture}, we need to compute
    \begin{align*}
        \alpha_1, \alpha_2, \beta_1, \beta_2, \gamma_1, \gamma_2=&~  \mathsf{MLP}(\X + t \cdot {\bf 1}_{h \times w \times c},c,6c)\\
         \alpha'_1, \alpha'_2, \beta'_1, \beta'_2, \gamma'_1, \gamma'_2=&~  \mathsf{MLP}(\X' + t \cdot {\bf 1}_{h \times w \times c},c,6c)\\
    \end{align*}
    Then, we can show that
    \begin{align*}
        \|\alpha'_1 - \alpha_1\|_\infty \leq c R \epsilon
    \end{align*}
    where the step follows from Lemma~\ref{lem:error_analysis_mlp}. The same conclusion holds for the intermediate parameter $\alpha_2, \beta_1, \beta_2, \gamma_1, \gamma_2$.



    By {\bf Step 2} of Definition~\ref{def:flow_matching_architecture} and Definition~\ref{def:fast_flow_matching_architecture}, we need to compute
    \begin{align*}
        \F'^t =&~ \mathsf{Attn}(\gamma_1 \circ \mathsf{LN}(\F^t)+\beta_1) \circ \alpha_1\\
        \mathsf{FF}'^t =&~ \mathsf{AAttC}(\gamma'_1 \circ \mathsf{LN}(\mathsf{FF}^t)+\beta'_1) \circ \alpha'_1\\
    \end{align*}
    Then, we move forward to show that
    \begin{align}\label{eq:flow_matching_tmp1}
        &~\| \gamma'_1 \circ \mathsf{LN}(\mathsf{FF}^t) + \beta'_1 - \gamma_1 \circ \mathsf{LN}(\F^t) - \beta_1\|_\infty\notag\\
        \leq&~ \| \gamma'_1 \circ \mathsf{LN}(\mathsf{FF}^t) - \gamma_1 \circ \mathsf{LN}(\F^t)  \|_\infty + \|\beta'_1-\beta_1\|_\infty\notag\\
        \leq&~  \| \gamma'_1 \circ (\mathsf{LN}(\mathsf{FF}^t) - \mathsf{LN}(\F^t))\| + \|(\gamma'_1-\gamma_1) \circ \mathsf{LN}(\F^t)\|_\infty + cR\epsilon\notag\\
        \leq&~ R \cdot \epsilon + R\cdot \epsilon + cR\epsilon\notag\\
        =&~ O(cR)\cdot \epsilon
    \end{align}
    where the first and second step follows from triangle inequality, the third step follows from conditions of this Lemma, and the last step follows from simple algebra.

    Then we have
    \begin{align}\label{eq:flow_matching_tmp2}
        \| \mathsf{AAttC}(\gamma'_1 \circ \mathsf{LN}(\mathsf{FF}^t) + \beta'_1) -\mathsf{Attn}(\gamma_1 \circ \mathsf{LN}(\F^t) + \beta_1)\|_\infty \leq&~ O(k R^{g+1} c) \cdot O(cR) \cdot \epsilon \notag\\
        \leq&~ O(kR^{g+2} c^2) \epsilon
    \end{align}
    where the first step follows from Lemma~\ref{lem:error_analysis_aattc_attn} and Eq.~\eqref{eq:flow_matching_tmp1} and the second step follows from simple algebra.

    Now, we are able to show that
    \begin{align}\label{eq:error_analysis_tmp3}
        \| \mathsf{FF}'^t -  \F'^t\|_\infty =&~ \|\mathsf{AAttC}(\gamma'_1 \circ \mathsf{LN}(\mathsf{FF}^t)+\beta'_1) \circ \alpha'_1 -  \mathsf{Attn}(\gamma_1 \circ \mathsf{LN}(\F^t)+\beta_1) \circ \alpha_1\|_\infty\notag \\
        \leq&~ \| \mathsf{AAttC}(\gamma'_1 \circ \mathsf{LN}(\mathsf{FF}^t)+\beta'_1) \circ (\alpha'_1-\alpha_1)\|_\infty \notag \\+&~ \|\alpha_1 \cdot \mathsf{AAttC}(\gamma'_1 \circ \mathsf{LN}(\mathsf{FF}^t) + \beta'_1) -\mathsf{Attn}(\gamma_1 \circ \mathsf{LN}(\F^t) + \beta_1) \|_\infty\notag \\
        \leq&~ R \cdot cR\epsilon + R \cdot O(kR^{g+2}c^2) \epsilon\notag \\
        =&~   O(kR^{g+3}c^2) \epsilon
    \end{align}
    where the first step follows from the definition of $\wh{\F}'^t$ and $\wh{\F}^t$, the second step follows from triangle inequality, the third step follows from Eq.~\eqref{eq:flow_matching_tmp2} and the conditions of this lemma, and the last step follows from simple algebra.

    By {\bf Step 3} of Definition~\ref{def:flow_matching_architecture} and Definition~\ref{def:fast_flow_matching_architecture}, we need to compute
    \begin{align*}
        \F''^t =&~\mathsf{MLP}(\gamma_2 \circ \mathsf{LN}(\F'^t)+ \beta_2,c,c) \circ \alpha_2\\
        \mathsf{FF}''^t=&~\mathsf{MLP}(\gamma'_2 \circ \mathsf{LN}(\mathsf{FF}'^t)+ \beta'_2,c,c) \circ \alpha'_2
    \end{align*}
    Then, we move forward to show that
    \begin{align}\label{eq:error_analysis_tmp4}
        &~\|\gamma'_2 \circ \mathsf{LN}(\mathsf{FF}'^t)+ \beta'_2 - \gamma_2 \circ \mathsf{LN}(\F'^t) -\beta_2 \|_\infty\notag\\
        \leq&~ \| \gamma'_2 \circ \mathsf{LN}(\mathsf{FF}'^t)- \gamma_2 \circ \mathsf{LN}(\F'^t) \|_\infty + \|\beta'_1-\beta_1\|_\infty\notag\\
        \leq&~\|\gamma'_2\circ(\mathsf{LN}(\mathsf{FF}'^t) - \mathsf{LN}(\F'^t)) \|_\infty+ \|(\gamma'_2-\gamma_2)\circ \mathsf{LN}(\F'^t)\|_\infty + cR\epsilon\notag\\
        \leq&~ R \cdot O(kR^{g+3}c^2) \epsilon + cR\epsilon \cdot R + cR\epsilon\notag\\
        =&~ O(kR^{g+4}c^2)\cdot \epsilon
    \end{align}
    where the first and the second steps follow from triangle inequality, the third step follows from Eq.~\eqref{eq:error_analysis_tmp3} the conditions of this lemma, and the last step follows from simple algebra.

    Then, we can show
    \begin{align}\label{eq:error_analysis_tmp5}
        \|\mathsf{MLP}(\gamma'_2 \circ \mathsf{LN}(\mathsf{FF}'^t)+ \beta'_2)-\mathsf{MLP}(\gamma_2 \circ \mathsf{LN}(\F'^t) +\beta_2)\|_\infty \leq&~  c R \cdot O(kR^{g+4} c^2) \cdot \epsilon\notag\\
        =&~ O(kR^{g+5}c^3)\cdot \epsilon
    \end{align}
    where the first step follows from Lemma~\ref{lem:error_analysis_mlp} and Eq.~\eqref{eq:error_analysis_tmp4} and the second step follows from simple algebra.

    Finally, we are able to show that
    \begin{align*}
        &~\|\mathsf{FNN}(\mathsf{FF}^t,\X',t) - \mathsf{FN}(\F^t,\X,t)\|_\infty\\
        =&~ \|\mathsf{MLP}(\gamma'_2 \circ \mathsf{LN}(\mathsf{FF}'^t)+ \beta'_2,c,c) \circ \alpha'_2 - \mathsf{MLP}(\gamma_2 \circ \mathsf{LN}(\F'^t)+ \beta_2,c,c) \circ \alpha_2 \|_\infty\\
        \leq&~ \| (\mathsf{MLP}(\gamma'_2 \circ \mathsf{LN}(\mathsf{FF}'^t)+ \beta'_2,c,c) - \mathsf{MLP}(\gamma_2 \circ \mathsf{LN}(\F'^t)+ \beta_2,c,c))\circ \alpha'_2\|_\infty \\+ &~ \|\mathsf{MLP}(\gamma_2 \circ \mathsf{LN}(\F'^t)+ \beta_2,c,c)  \circ (\alpha'_2 - \alpha_2) \|_\infty\\
        \leq&~ R \cdot O(kR^{g+5}c^3) \cdot \epsilon + R \cdot cR\epsilon\\
        =&~  O(kR^{g+6}c^3) \cdot \epsilon
    \end{align*}
    where the step follows from the definition of output of $\mathsf{FFN}(\F'^t,\X',t)$ and $ \mathsf{FN}(\F^t,\X,t)$, the second step follows from triangle inequality, the third step follows from Eq.~\eqref{eq:error_analysis_tmp5} and conditions of this lemma, and the last step follows from simple algebra.

    Then, we complete the proof.
\end{proof}

\subsection{Error Analysis of Fast FlowAR Architecture}\label{sec:error_analysis_fast_flowar}
Here, we proceed to present the error analysis of fast FlowAR Architecture.
\begin{lemma}[Error Bound Between Fast FlowAR and FlowAR Outputs]\label{lem:error_analysis_fast_flowar}
    Given the following:
    \begin{itemize}
        \item {\bf Input tensor:} $\X \in \R^{h \times w \times c}$.
        \item {\bf Scales number:} $K = O(1)$.
        \item {\bf Dimensions:} Let $h=w=n$ and $c = O(\log n)$. Let $\wt{h}_i := \sum_{j=1}^i h/r_j$ and $\wt{w}_i := \sum_{j=1}^i w/r_j$.
        \item {\bf Bounded Entries:} All tensors and matrices have entries bounded by $R = O(\sqrt{\log n})$.
        \item {\bf Layers:}
        \begin{itemize}
            \item $\phi_{\mathrm{up},a}(\cdot)$ :  bicubic upsampling function (Definition~\ref{def:bicubic_up_sample_function}).
            \item $\mathsf{Attn}(\cdot)$: attention layer (Definition~\ref{def:attn_layer}).
            \item $\mathsf{AAttC(\cdot)}$: approximate attention layer (Definition~\ref{def:aattc})
            \item $\mathsf{NN}(\cdot,\cdot,\cdot)$: flow-matching layer (Definition~\ref{def:flow_matching_architecture})
            \item $\mathsf{FNN}(\cdot,\cdot,\cdot)$: fast flow-matching layer (Definition~\ref{def:fast_flow_matching_architecture})
        \end{itemize}
        \item {\bf Input and interpolations:}
        \begin{itemize}
            \item Initial inputs: $\Z_{\mathrm{init}} \in \R^{(h/r_1)\times(w/r_1) \times c}$.
            \item $\Z_i:$ Reshaped tensor of  $\Z_{\mathrm{init}}, \phi_{\mathrm{up},1}(\wt{\Y}_1), \dots, \phi_{\mathrm{up},i-1}(\wt{\Y}_{i-1})$ for FlowAR.
            \item $\Z'_i:$ Reshaped tensor of  $\Z_{\mathrm{init}}, \phi_{\mathrm{up},1}(\wt{\Y}'_1), \dots, \phi_{\mathrm{up},i-1}(\wt{\Y}'_{i-1})$ for  Fast FlowAR.
            \item $\mathsf{F}_i^{t_i} \in \R^{h/r_i \times w/r_i \times c}$ be the interpolated value of FlowAR (Definition~\ref{def:flow}).
            \item $\mathsf{FF}_i^{t_i} \in \R^{h/r_i \times w/r_i \times c}$ be the interpolated value of Fast FlowAR (Definition~\ref{def:flow}).
        \end{itemize} 
        \item {\bf Outputs:}
        \begin{itemize}
            \item $\wt{\Y}_i \in \R^{h/r_i \times w/r_i \times c}$: FlowAR output at layer $i$ (Definition~\ref{def:flow_architecture_inference})
            \item $\wt{\Y}'_i \in \R^{h/r_i \times w/r_i \times c}$: Fast FlowAR output at layer $i$ (Definition~\ref{def:fast_flow_architecture_inference})
        \end{itemize}
    \end{itemize}
    Under these conditions, the $\ell_\infty$ error between the final outputs is bounded by:
    \begin{align*}
        \|\wt{\Y}'_K - \wt{\Y}_K\|_\infty \leq 1/\poly(n)
    \end{align*}
    
\end{lemma}
\begin{proof}
    We can conduct math induction as the following.
    
    Consider the first layer of fast FlowAR Architecture. Firstly, we can show that
    \begin{align*}
        \| \mathsf{AAttC}_1(\Z_{1}) - \mathsf{Attn}_1(\Z_{1})\|_\infty \leq 1/\poly(n)
    \end{align*}
    The inequality is derived Lemma~\ref{lem:as23_attention}.
    
    Then, we have
    \begin{align*}
        \| \wh{\Y}'_1 - \wh{\Y}_1\|_\infty =&~ \|\mathsf{FFN}_1(\mathsf{AAttC}_1(\Z_{1})) - \mathsf{FFN}_1(\mathsf{Attn}_1(\Z_{1}))\|_\infty\\
        \leq&~ O(c^2 R^2) \cdot 1/\poly(n)\\
        = &~ 1/\poly(n)
    \end{align*}
    The first equation comes from the definition of $\wh{Y}'_1$ and $\wh{Y}_1$, the second inequality is due to Lemma~\ref{lem:error_analysis_ffn} and the last equation is due to $c = O(\log n)$ and $R = O(\sqrt{\log n})$.

    Then, we can show that
    \begin{align*}
        \| \wt{\Y}'_1 - \wt{Y}_1\|_\infty =&~ \| \mathsf{FNN}_1(\mathsf{FF}^{t_1}_1,\wh{Y}'_1,t_1)  - \mathsf{NN}_1(\F^{t_1}_1,\wh{Y}_1,t_1) \|_\infty\\
        \leq &~ O(kR^{g+6}c^3) \cdot 1/\poly(n)\\
        =&~ 1/\poly(n)
    \end{align*}
    The first equation is due to the definition of $\Y'_1$ and $\Y_1$, the second inequality comes from Lemma~\ref{lem:error_analysis_flow_matching_layer}, and the last step follows from $c = O(\log n)$ and $R = O(\sqrt{\log n})$.


    Assume that the following statement is true for $k$-th iteration (where $k < K$):
    \begin{align*}
        \|\wt{\Y}'_k - \wt{Y}_k\|_\infty \leq 1/\poly(n)
    \end{align*}
    Then, we can easily to bound
    \begin{align*}
        \| \Z'_{k+1} - \Z_{k+1}\|_\infty \leq 1/\poly(n)
    \end{align*}
    The inequality is due to Lemma~\ref{sec:error_analysis_of_phi_x_prime_phi_x} and Definition of $\Z'_{k+1}$ and $\Z_{k+1}$.

    Then, we can show that 
    \begin{align*}
        \|\mathsf{AAttC}_{k+1}(\Z'_{k+1}) -\mathsf{Attn}_{k+1}(\Z_{k+1}) \|_\infty \leq&~ O(k R^{g+1} c) \cdot 1/\poly(n)\\
        =&~ 1/\poly(n) 
    \end{align*}
    The first inequality comes from Lemma~\ref{lem:error_analysis_aattc_attn}, and the second equation is due to $c = O(\log n)$ and $R = O(\sqrt{\log n})$.

    Then we have
    \begin{align*}
        \| \wh{\Y}'_{k+1} - \wh{\Y}_{k+1}\|_\infty =&~ \|\mathsf{FFN}_{k+1}(\mathsf{AAttC}_{k+1}(\Z'_{k+1})) - \mathsf{FFN}_{k+1}(\mathsf{Attn}_{k+1}(\Z_{k+1}))\|_\infty\\
        \leq&~ O(c^2 R^2) \cdot 1/\poly(n)\\
        = &~ 1/\poly(n)
    \end{align*}
   The first equation comes from the definition of $\wh{Y}'_{k+1}$ and $\wh{Y}_{k+1}$, the second inequality is due to Lemma~\ref{lem:error_analysis_ffn} and the third equation is due to $c = O(\log n)$ and $R = O(\sqrt{\log n})$.

    Then, we can derive that
    \begin{align*}
        \| \wt{\Y}'_{k+1} - \wt{Y}_{k+1}\|_\infty =&~ \| \mathsf{FNN}_{k+1}(\mathsf{FF}^{t_{k+1}}_{k+1},\wh{Y}'_{k+1},t_{k+1})  - \mathsf{NN}_{k+1}(\F^{t_{k+1}}_{k+1},\wh{Y}_k+1,t_{k+1}) \|_\infty\\
        \leq &~ O(kR^{g+6}c^3) \cdot 1/\poly(n)\\
        =&~ 1/\poly(n)
    \end{align*}
    The first equation comes from the definition of $\Y'_{k+1} $ and $ \Y_{k+1}$, the second inequality is due to Lemma~\ref{lem:error_analysis_flow_matching_layer} and the third equation is due to $c = O(\log n)$ and $R = O(\sqrt{\log n})$.

    Then, by mathematical induction, we can get the proof.
\end{proof}