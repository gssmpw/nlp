\section{Provably Efficient Criteria}\label{sec:efficient_critieria}

\subsection{Approximate Attention Computation}\label{sec:fast_attn}
In this section, we introduce approximate attention computation, which can accelerate the computation of the attention layer.

\begin{definition}[Approximate Attention Computation $\mathsf{AAttC}(n, d, R, \delta)$, Definition 1.2 in \cite{as23}]\label{def:aattc}
    Given an input sequence $X \in \R^{n \times d}$ and an approximation tolerance $\delta > 0$. Let $Q,K,V \in \R^{n \times d}$ be weigh matrices bounded such that
    \begin{align*}
        \max\{\|Q\|_\infty,\|K\|_\infty,\|V\|_\infty\} \leq R
    \end{align*}
    The {\bf Approximate Attention Computation} $\mathsf{AAttC}(n, d, R, \delta)$ outputs a matrix $N \in \R^{n \times d }$ satisfying:
    \begin{align*}
        \| N - \mathsf{Attn}(X)\|_\infty \leq \delta
    \end{align*}
\end{definition}

Next, we present a lemma that demonstrates the computational time cost of the AATTC method.
\begin{lemma}[Fast Attention via Subquadratic Computation, Theorem 1.4 of \cite{as23}]\label{lem:as23_attention}
Let $\mathsf{AAttC}$ be formalized as in Definition~\ref{def:aattc}. 
For parameter configurations:
\begin{itemize}
    \item Embedding dimension $d = O(\log n)$,
    \item $R = \Theta(\sqrt{\log n})$,
    \item Approximation tolerance $\delta = 1/\poly(n)$,
\end{itemize}
the $\mathsf{AAttC}$ computation satisfies
\begin{align*}
    \mathcal{T}(n, n^{o(1)}, d) = n^{1 + o(1)},
\end{align*}
where $\mathcal{T}$ denotes the time complexity under these constraints.

\end{lemma}


\subsection{Fast FlowAR Architecture in the Inference Pipleline}\label{sec:fast_flowar}
Firstly, we define the fast flow-matching layer, where the $\mathsf{Attn}$ layers in the original flow-matching module are replaced with $\mathsf{AAttC}$ layers.


\begin{definition}[Fast Flow Matching Architecture]\label{def:fast_flow_matching_architecture}
Given the following:
\begin{itemize}
    \item {\bf Input tensor:} $\X \in \R^{h \times w \times c}$.
    \item {\bf Scales number:} $K \in \mathbb{N}$.
    \item {\bf Token maps:} For $i \in [K]$, $\wh{\Y}_i \in \R^{(h / r_i) \times (w/r_i) \times c}$ denote the token maps generated by autoregressive transformer defined in Definition~\ref{def:ar_transformer}.
    \item {\bf Interpolation Tokens:} For $i \in [K]$, $\F_i^t \in \R^{(h / r_i) \times (w/r_i) \times c}$ denote interpolated input defined in Definition~\ref{def:flow}.
    \item {\bf Time step:} For $i \in [K]$, $t_i \in [0,1]$ denotes timestep.
    \item {\bf Approximate Attention layer:}  For $i \in [K]$, $\mathsf{AAttC}_i(\cdot):\R^{h/r_i \times w/r_i \times c} \to \R^{h/r_i \times w/r_i \times c}$ is defined in Definition~\ref{def:attn_layer}.
    \item {\bf MLP layer:}  For $i \in [K]$, $\mathsf{MLP}_i(\cdot,c,d):\R^{h/r_i \times w/r_i \times c} \to \R^{h/r_i \times w/r_i \times c}$ is defined in Definition~\ref{def:mlp}.
    \item {\bf LN layer:} For $i \in [K]$, $\mathsf{LN}_i(\cdot):\R^{h/r_i \times w/r_i \times c} \to \R^{h/r_i \times w/r_i \times c}$ is defined in Definition~\ref{def:ln}.
\end{itemize}
The computation steps of flow-matching layers are as follows:
\begin{itemize}
    \item {\bf Time-conditioned parameter generation:}
    \begin{align*}
        \alpha_1, \alpha_2, \beta_1, \beta_2, \gamma_1, \gamma_2:=  \mathsf{MLP}_i(\wh{\Y}_i + t_i \cdot {\bf 1}_{(h / r_i) \times (w/r_i) \times c},c,6c)
    \end{align*}
    \item {\bf Intermediate variable computation:}
    \begin{align*}
        \F'^{t_i}_i:= \mathsf{AAttC}_i (\gamma_1 \circ \mathsf{LN}(\F_i^{t_i}) + \beta_1) \circ \alpha_1
    \end{align*}
    with $\circ$ denoting Hadamard (element-wise) product.
    \item {\bf Final projection:}
    \begin{align*}
        \F''^{t_i}_i := \mathsf{MLP}_i(\gamma_2 \circ \mathsf{LN}(\F'^{t_i}_i)+ \beta_2,c,c) \circ \alpha_2
    \end{align*}
\end{itemize}
The operation is denoted as $\F''^{t_i}_i := \mathsf{FNN}_i(\wh{\Y_i},\F_i^{t_i},t_i)$
\end{definition}


Next, we define the Fast FlowAR inference pipeline architecture, where all 
$\mathsf{Attn}$ layers in the original FlowAR architecture are replaced with $\mathsf{AAttC}$ layers.




\begin{definition}[Fast FlowAR Inference Architecture]\label{def:fast_flow_architecture_inference}
    Given the following:
    \begin{itemize}
        \item {\bf Scales number:} $K \in \mathbb{N}$.
        \item {\bf Scale factor:} For $i \in [K]$, $r_i:= a^{K-i}$ where base factor $a \in \mathbb{N}^+$.
        \item {\bf Upsampling functions:}  For $i \in [K]$, $\phi_{\mathrm{up},i}(\cdot,a): \R^{(h/r_i) \times (w/r_i) \times c}\to \R^{(h/r_{i+1}) \times (w/r_{i+1}) \times c}$ from Definition~\ref{def:bicubic_up_sample_function}.
        \item {\bf Approximate Attention layer:}  For $i \in [K]$, $\mathsf{AAttC}_i(\cdot):\R^{(\sum_{j=1}^i h/r_j \cdot w/r_{j})\times c} \to \R^{(\sum_{j=1}^i h/r_j \cdot w/r_{j})\times c}$ which acts on flattened sequences of dimension defined in Definition~\ref{def:aattc}.
        \item {\bf Feed forward layer: } For $i \in [K]$, $\mathsf{FFN}_i(\cdot): \R^{(\sum_{j=1}^i h/r_j \cdot w/r_{j})\times c} \to \R^{(\sum_{j=1}^i h/r_j \cdot w/r_{j})\times c}$ which acts on flattened sequences of dimension defined in Definition~\ref{def:ffn}.
        \item {\bf Fast flow-matching layer:} For $i \in [K]$, $\mathsf{FNN}_i(\cdot,\cdot,\cdot):\R^{h/r_i \times w/r_i \times c}\times \R^{h/r_i \times w/r_i \times c}\times \R \to \R^{h/r_i \times w/r_i \times c}$ denote the fast flow-matching layer defined in Definition~\ref{def:fast_flow_matching_architecture}.
        \item {\bf Initial condition:} $\Z_{\mathrm{init}} \in \R^{(h/r_1) \times (w/r_1) \times c}$ denotes the initial token maps which encodes class information.
        \item {\bf Time steps:} For $i \in [K]$, $t_i \in [0,1]$ denotes time steps.
        \item {\bf Interpolated inputs:} For $i \in [K]$, $\F_i^{t_i} \in \R^{h/r_i \times w/r_i \times c}$ defined in Definition~\ref{def:flow}.
        \item {\bf Cumulative dimensions:} We define $\wt{h}_i := \sum_{j=1}^i h/r_j$ and  $\wt{w}_i := \sum_{j=1}^i w/r_j$ for $i \in [K]$.
    \end{itemize}
    The FlowAR model conducts the following recursive construction:
    \begin{itemize}
        \item {\bf Base case $i=1$:}
        \begin{align*}
            &~\Z_1 = \Z_{\mathrm{init}}\\
            &~\wh{\Y}_1 = \mathsf{FFN}_1(\mathsf{AAttC}_1(\Z_1))\\
            &~\wt{\Y}_1 = \mathsf{FNN}_1(\wh{\Y}_1,\F_{1}^{t_1},t_1)
        \end{align*}
        \item {\bf Inductive step $i \geq 2$:}
        \begin{itemize}
            \item {\bf Spatial aggregation:}
            \begin{align*}
                \Z_i = \mathsf{Concat}(\Z_{\mathrm{init}},\phi_{\mathrm{up},1}(\wt{\Y}_{i-1}),\dots,\phi_{\mathrm{up},i-1}(\wt{\Y}_{i-1})) \in \R^{(\sum_{j=1}^i h/r_j \cdot w/r_j)\times c}
            \end{align*}
            \item {\bf Autoregressive transformer computation:}
            \begin{align*}
                \wh{\Y}_i = \mathsf{FFN}_i(\mathsf{AAttC}_i(\Z_1))_{\wt{h}_{i-1}:\wt{h}_{i-1},\wt{w}_{i}:\wt{w}_{i},0:c}
            \end{align*}
            \item {\bf Flow matching layer:}
            \begin{align*}
                \wt{\Y}_i = \mathsf{FNN}_i(\wh{\Y}_i,\F_{i}^{t_i},t_i)
            \end{align*}
        \end{itemize}
        The final output is $\wt{\Y}_K \in \R^{h \times w \times c}$.
    \end{itemize}
   
\end{definition}

\subsection{Running Time}\label{sec:running_time}
In this section, we analyzed the running time required by the original FlowAR architecture and the running time required by the Fast FlowAR architecture. The results indicate that by adopting the Approximate Attention computation module, we can accelerate the running time of FlowAR to almost quadratic time.

First, we present the results of the running time analysis for the original FlowAR model.
\begin{lemma}[Inference Runtime of Original FlowAR Architecture, informal version of Lemma~\ref{lem:runtime_old_flowar_formal}]\label{lem:runtime_old_flowar_informal}
    Consider the original FlowAR inference pipeline with the following parameters:
    \begin{itemize}
        \item {\bf Input tensor:} $\X \in \R^{h \times w \times c}$. Assume $h=w=n$ and $c = O(\log n)$.
        \item {\bf Number of scales:} $K = O(1)$.
        \item {\bf Scale factor:} For $i \in [K]$, $r_i:= a^{K-i}$ where base factor $a \in \mathbb{N}^+$.
        \item {\bf Upsampling functions}  For $i \in [K]$, $\phi_{\mathrm{up},i}(\cdot,a)$ from Definition~\ref{def:bicubic_up_sample_function}.
        \item {\bf Attention layer:}  For $i \in [K]$, $\mathsf{Attn}_i(\cdot)$ which acts on flattened sequences of dimension defined in Definition~\ref{def:attn_layer}.
        \item {\bf Feed forward layer: } For $i \in [K]$, $\mathsf{FFN}_i(\cdot)$ which acts on flattened sequences of dimension defined in Definition~\ref{def:ffn}.
        \item {\bf Flow matching layer:} For $i \in [K]$, $\mathsf{NN}_i(\cdot,\cdot,\cdot)$ denote the flow-matching layer defined in Definition~\ref{def:flow_matching_architecture}.
    \end{itemize}
    Under these conditions, the total inference runtime of FlowAR is bounded by $O(n^{4+o(1)})$.
\end{lemma}

Then, we present the results of the running time analysis for the fast FlowAR model.
\begin{lemma}[Inference Runtime of Fast FlowAR Architecture, informal version of Lemma~\ref{lem:runtime_fast_flowar_formal}]\label{lem:runtime_fast_flowar_informal}
    Consider the original FlowAR inference pipeline with the following parameters:
    \begin{itemize}
        \item {\bf Input tensor:} $\X \in \R^{h \times w \times c}$. Assume $h=w=n$ and $c = O(\log n)$.
        \item {\bf Number of scales:} $K = O(1)$.
        \item {\bf Scale factor:} For $i \in [K]$, $r_i:= a^{K-i}$ where base factor $a \in \mathbb{N}^+$.
        \item {\bf Upsampling functions}  For $i \in [K]$, $\phi_{\mathrm{up},i}(\cdot,a)$ from Definition~\ref{def:bicubic_up_sample_function}.
        \item {\bf Approximate Attention layer:}  For $i \in [K]$, $\mathsf{AAttC}_i(\cdot)$ defined in Definition~\ref{def:aattc}.
        \item {\bf Feed forward layer: } For $i \in [K]$, $\mathsf{FFN}_i(\cdot)$ which acts on flattened sequences of dimension defined in Definition~\ref{def:ffn}.
        \item {\bf Fast flow-matching layer:} For $i \in [K]$, $\mathsf{FNN}_i(\cdot,\cdot,\cdot)$ denote the fast flow-matching layer defined in Definition~\ref{def:fast_flow_matching_architecture}.
    \end{itemize}
    Under these conditions, the total inference runtime of FlowAR is bounded by $O(n^{2+o(1)})$.
\end{lemma}

\subsection{Error Propagation Analysis}\label{lem:error_propagation_analysis}
In this section, we present an error analysis introduced by the fast algorithm applied to the FlowAR model.
\begin{lemma}[Error Bound Between Fast FlowAR and FlowAR Outputs, informal version of Lemma~\ref{lem:error_analysis_fast_flowar}]\label{lem:error_analysis_fast_flowar_informal}
    Given the following:
    \begin{itemize}
        \item {\bf Input tensor:} $\X \in \R^{h \times w \times c}$.
        \item {\bf Scales number:} $K = O(1)$.
        \item {\bf Dimensions:} Let $h=w=n$ and $c = O(\log n)$. Let $\wt{h}_i := \sum_{j=1}^i h/r_j$ and $\wt{w}_i := \sum_{j=1}^i w/r_j$.
        \item {\bf Bounded Entries:} All tensors and matrices have entries bounded by $R = O(\sqrt{\log n})$.
        \item {\bf Layers:}
        \begin{itemize}
            \item $\phi_{\mathrm{up},a}(\cdot)$ :  bicubic upsampling function (Definition~\ref{def:bicubic_up_sample_function}).
            \item $\mathsf{Attn}(\cdot)$: attention layer (Definition~\ref{def:attn_layer}).
            \item $\mathsf{AAttC(\cdot)}$: approximate attention layer (Definition~\ref{def:aattc})
            \item $\mathsf{NN}(\cdot,\cdot,\cdot)$: flow-matching layer (Definition~\ref{def:flow_matching_architecture})
            \item $\mathsf{FNN}(\cdot,\cdot,\cdot)$: fast flow-matching layer (Definition~\ref{def:fast_flow_matching_architecture})
        \end{itemize}
        \item {\bf Input and interpolations:}
        \begin{itemize}
            \item Initial inputs: $\Z_{\mathrm{init}} \in \R^{(h/r_1)\times(w/r_1) \times c}$.
            \item $\Z_i:$ Reshaped tensor of  $\Z_{\mathrm{init}}, \phi_{\mathrm{up},1}(\wt{\Y}_1), \dots, \phi_{\mathrm{up},i-1}(\wt{\Y}_{i-1})$ for FlowAR.
            \item $\Z'_i:$ Reshaped tensor of  $\Z_{\mathrm{init}}, \phi_{\mathrm{up},1}(\wt{\Y}'_1), \dots, \phi_{\mathrm{up},i-1}(\wt{\Y}'_{i-1})$ for  Fast FlowAR.
            \item $\mathsf{F}_i^{t_i} \in \R^{h/r_i \times w/r_i \times c}$ be the interpolated value of FlowAR (Definition~\ref{def:flow}).
            \item $\mathsf{FF}_i^{t_i} \in \R^{h/r_i \times w/r_i \times c}$ be the interpolated value of Fast FlowAR (Definition~\ref{def:flow}).
        \end{itemize} 
        \item {\bf Outputs:}
        \begin{itemize}
            \item $\wt{\Y}_i \in \R^{h/r_i \times w/r_i \times c}$: FlowAR output at layer $i$ (Definition~\ref{def:flow_architecture_inference})
            \item $\wt{\Y}'_i \in \R^{h/r_i \times w/r_i \times c}$: Fast FlowAR output at layer $i$ (Definition~\ref{def:fast_flow_architecture_inference})
        \end{itemize}
    \end{itemize}

    Under these conditions, the $\ell_\infty$ error between the final outputs is bounded by:
    \begin{align*}
        \|\wt{\Y}'_K - \wt{\Y}_K\|_\infty \leq 1/\poly(n)
    \end{align*}
    
\end{lemma}

\subsection{Existence of Almost Quadratic Time Algorithm}\label{sec:almost_quadratic_time_algorithm}
This section presents a theorem proving the existence of a quadratic-time algorithm that speeds up the FlowAR architecture and guarantees a bounded additive error.
\begin{theorem}[Existence of Almost Quadratic Time Algorithm]
\label{thm:upper_bound:formal}
Suppose $d = O(\log n)$ and $R = o(\sqrt{\log n})$. There is an algorithm that approximates the  FlowAR architecture up to $1/\poly(n)$ additive error in $O(n^{2+o(1)})$ time.
\end{theorem}
\begin{proof}
    By combining the result of Lemma~\ref{lem:runtime_fast_flowar_informal} and Lemma~\ref{lem:error_analysis_fast_flowar_informal}, we can easily derive the proof.
\end{proof}

Our Theorem~\ref{thm:upper_bound:formal} shows that we can accelerate FlowAR while only introducing a small error. Using the low-rank approximation in the attention mechanism is also used in previous works \cite{kll+25,lls+24_conv,llss25,lss+25_relu,chl+24_rope,lss+24,lssz24_tat,as24_iclr,as24b,as24_rope,hsk+24}.