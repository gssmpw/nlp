\section{Introduction}\label{sec:introduction}

Visual generation has become a transformative force in artificial intelligence, reshaping capabilities in creative design, media synthesis, and digital content creation. Advances in deep generative models, such as Generative Adversarial Networks (GANs) \cite{gpm+20}, Variational Autoencoders (VAEs) \cite{doe16}, diffusion models \cite{hja20,ssk+20} and flow-based model \cite{kd18}, have enabled the synthesis of high-fidelity images, videos, and 3D assets with unprecedented diversity and realism. The introduction of the visual autoregressive model (VAR) \cite{tjy+24} represents a significant shift in the paradigm in the visual generation field. The VAR model adopts a coarse-to-fine Scale-wise prediction to replace the traditional autoregressive image generation techniques. This innovative technique enables the VAR model to effectively capture visual distributions while outperforming diffusion transformers in image generation benchmarks.

Recently, the introduction of FlowAR \cite{ryh+24} has further advanced the field of autoregressive visual generation. Specifically, FlowAR streamlines the scale design of VAR, improving generalization for predictions at the next scale and enabling seamless integration with the Flow Matching model \cite{lgl23} for high-quality image generation. It is worth noting that FlowAR has achieved cutting-edge results in multiple empirical studies of visual generation.


As the visual generation model architectures grow increasingly sophisticated to meet the demands of high-resolution and photorealistic generation, critical questions arise regarding their computational efficiency and intrinsic expressive power. While empirical improvements in generation quality dominate the current discourse, comprehending the theoretical foundations of these models continues to be a pivotal challenge. To tackle the challenge mentioned above, some prior researchers have made significant contributions. For example, \cite{ms24} show that $\mathsf{DLOGTIME}$-uniform $\TC^0$ circuits can simulate softmax-attention transformers; later, \cite{cll+24} show that the introduction of RoPE will not enhance the express power of transformer; \cite{kll+25_circuit_var} present the circuit complexity for the VAR model. Up to now, the expressiveness from a circuit complexity perspective of the FlowAR model remains unexplored. This gap raises an important question:
\begin{center}
   {\it Does the Flow Matching architecture enhance the expressive power of the VAR Model?} 
\end{center}
This study seeks to explore this question through the lens of circuit complexity. First, we provide a model formulation for each module of FlowAR. Our insight is that using circuit complexity theory, we prove that each module of FlowAR, including the Attention Layer, Flow-Matching Layer, and others, can be simulated by a constant-depth, polynomial-size $\TC^0$ circuit. Ultimately, the combined result shows that the entire FlowAR architecture can be simulated by a constant-depth, polynomial-size $\TC^0$ circuit. Therefore, our conclusion offers a negative response to the question: despite the inclusion of the flow-matching mechanism, the expressive power of FlowAR, in terms of circuit complexity, is on par with that of the VAR model.

In addition, we explored the runtime of the FlowAR model inference process and potential efficient algorithms. Specifically, we analyzed the runtime of each module in the FlowAR model and found that the bottleneck affecting the overall runtime originates from the computation of the attention mechanism. As a result, we accelerated the original attention computation using low-rank approximation, which makes the overall runtime of the FlowAR model almost quadratic.


The primary contributions of our work are summarized below:
\begin{itemize}
    \item {\bf Circuit Complexity Bound:} FlowAR model can be simulated by a $\mathsf{DLOGTIME}$-uniform $\mathsf{TC}^0$ family. (Theorem~\ref{thm:flowar_tc0})
    \item {\bf Provably Efficient Criteria:} Suppose the largest feature map produced by the FlowAR model has dimensions $n \times n \times c$ and $c = O(\log n)$. We prove that the time complexity of the FlowAR model architecture is $
    O(n^{4+o(1)})$. By applying low-rank approximation to the Attention module within FlowAR, we obtain a FlowAR model with an almost quadratic runtime. Explicitly, we demonstrate that the FlowAR model variant's time complexity in realistic settings is $O(n^{2+o(1)})$. (Theorem~\ref{thm:upper_bound:formal})
\end{itemize}

{\bf Roadmap.} 
The paper's organizational structure is outlined as follows: Section~\ref{sec:related_work} synthesizes key academic contributions in the domain. Section~\ref{sec:preliminary} then elucidates foundational circuit complexity principles essential for subsequent analysis. Subsequent sections progress systematically, with Section~\ref{sec:model_formulation_of_flowar} detailing mathematical formalizations for all FlowAR modules. Section~\ref{sec:main_result} outlines our principal findings. 
Section~\ref{sec:efficient_critieria} presents provably efficient criteria of the fast FlowAR model.
In Section~\ref{sec:conclusion}, we conclude our paper.