\section{Supplementary Proof for Section~\ref{sec:main_result}}\label{sec:app_missing_proof}
In this section, we present some missing proofs in Section~\ref{sec:main_result}. 

\subsection{Computing Multiple-layer Perceptron in \texorpdfstring{$\TC^0$}{} }\label{sec:app_mlp_tc0}
This section presents the detailed proof for Lemma~\ref{lem:mlp_tc0_informal}.

\begin{lemma}[MLP computation in $\mathsf{TC}^0$, formal version of Lemma~\ref{lem:mlp_tc0_informal}]\label{lem:mlp_tc0_formal}
    Given an input tensor $\X\in \R^{h \times w \times c}$. Let $\mathsf{MLP}(\X,c,d)$ be the MLP layer defined in Definition~\ref{def:mlp}. Under the following constraints:
    \begin{itemize}
        \item Satisfy $h = w = n$,
        \item Channel bounds: $c, d\leq n$,
        \item Precision: $p \leq \poly(n)$,
    \end{itemize}
    The $\mathsf{MLP}(\X,c,d)$ function can be computed by a uniform $\mathsf{TC}^0$ circuit with:
    \begin{itemize}
        \item Size: $\poly(n)$.
        \item Depth: $2d_\mathrm{std} + d_{\oplus}$.
    \end{itemize}
    with $d_{\mathrm{std}}$ and $d_{\oplus}$ defined in Definition~\ref{lem:float_operations_TC}.
\end{lemma}


\begin{proof}
    For each $j \in [hw]$, by Lemma~\ref{lem:matrix_multi}, 
    compute $X_{j,*} \cdot W$ requires depth $d_{\mathrm{std}} + d_{\oplus}$. By Part 1 of Lemma~\ref{lem:float_operations_TC}, compute $X_{j,*} \cdot W + b$ requires depth $d_{\mathrm{std}}$.  Since for all $j \in [hw]$, the computation $X_{j,*} \cdot W + b$ can be simulated in parallel. Hence the total depth remains $2d_\mathrm{std} + d_{\oplus}$ and size is $\poly(n)$.
\end{proof}
\subsection{Computing Feed Forward Layer in \texorpdfstring{$\TC^0$}{} }\label{sec:app_ffn_tc0}
This section presents the detailed proof for Lemma~\ref{lem:ffn_tc0_informal}.

\begin{lemma}[FFN computation in $\mathsf{TC}^0$, formal version of Lemma~\ref{lem:ffn_tc0_informal}]\label{lem:ffn_tc0_formal}
 Given an input tensor $\X\in \R^{h \times w \times c}$. Let $\mathsf{FFN}(X):\R^{h \times w \times c} \to \R^{h \times w \times c}$ as defined in Definition~\ref{def:ffn}. Under the following constraints:
 \begin{itemize}
     \item Satisfy $h = w = n$,
         \item Channel bound: $c \leq n$,
         \item Precision bound: $p \leq \poly(n)$.
 \end{itemize}
The $\mathsf{FFN}(\X)$ layer can be computed by a uniform $\mathsf{TC}^0$ circuit with:
\begin{itemize}
    \item Size: $\poly(n)$.
    \item Depth: $6d_\mathrm{std} + 2d_{\oplus}$.
\end{itemize}
with $d_{\mathrm{std}}$ and $d_{\oplus}$ defined in Definition~\ref{lem:float_operations_TC}.
\end{lemma}
\begin{proof}
    For each $j \in [hw]$, by the proof of Lemma~\ref{lem:mlp_tc0_formal}, compute $X_{j,*} \cdot W_1 + b_1$ requires depth $2d_{\mathrm{std}} + d_{\oplus}$. By Lemma~\ref{lem:float_operations_TC}, compute $A_1 = \sigma(X_{j,*} \cdot W + b)$ requires depth $d_{\mathrm{std}}$. By applying Lemma~\ref{lem:mlp_tc0_formal} again, compute $A_2 = A_1\cdot W_2 +b_2$ requires depth $2d_{\mathrm{std}} + d_{\oplus}$. Lastly, by Part 1 of Lemma~\ref{lem:float_operations_TC}, compute $X_{j,*} + A_2$ requires depth $d_{\mathrm{std}}$.
    
    Combing the result above, we can have that compute $Y_{j,*}=X_{j,*} + \sigma(X_{j,*} \cdot W_1 + b_1)\cdot W_2 +b_2$ requires depth $6d_{\mathrm{std}}+2d_{\oplus}$.

    Since for all $j \in [hw]$, the computation $Y_{j,*}$ can be simulated in parallel. Hence the total depth remains $6d_{\mathrm{std}}+2d_{\oplus}$ and size is $\poly(n)$.
\end{proof}

\subsection{Computing Attention Layer in \texorpdfstring{$\TC^0$}{} }\label{sec:app_attn_tc0}
This section presents the detailed proof for Lemma~\ref{lem:attn_tc0_informal}.

\begin{lemma}[Attention layer computation in $\mathsf{TC}^0$, formal version of Lemma~\ref{lem:attn_tc0_informal}]\label{lem:attn_tc0_formal}
     Given an input tensor $\X \in \R^{h \times w \times c}$. Let $\mathsf{Attn}(X):\R^{h \times w \times c} \to \R^{h \times w \times c}$ as defined in Definition~\ref{def:attn_layer}. Under the following constraints:
     \begin{itemize}
         \item Satisfy $h = w = n$,
         \item Channel bound: $c \leq n$,
         \item Precision bound: $p \leq \poly(n)$.
     \end{itemize}
     The $\mathsf{Attn}(\X)$ layer can be computed by a uniform $\mathsf{TC}^0$ circuit with:
     \begin{itemize}
        \item Size: $\poly(n)$.
        \item Depth: $6(d_{\mathrm{std}} + d_{\oplus}) + d_{\exp}$.
     \end{itemize}    
     with $d_{\mathrm{std}}$ and $d_{\oplus}$ defined in Definition~\ref{lem:float_operations_TC}, $d_{\exp}$ defined in Definition~\ref{lem:exp}. 
\end{lemma}
\begin{proof}
    We analyze the $\mathsf{TC}^0$ simulation of the attention layer through sequential computation phases:
    \begin{itemize}
        \item {\bf Key-Query Product}: Compute $W_QW_K^\top$ vial Lemma~\ref{lem:matrix_multi} requires depth $d_{\mathrm{std}} + d_{\oplus}$.
        \item {\bf Pairwise Score Computation}: Compute $s_{i,j} = X_{i,*}   W_Q   W_K^\top   X_{j,*}^\top$ requires depth $2(d_{\mathrm{std}} + d_{\oplus})$ by Lemma~\ref{lem:matrix_multi}. By  Lemma~\ref{lem:exp}, computing $A_{i,j} = \exp(s_{i,j})$ requires depth $d_{\exp}$. 
    \end{itemize}
    Since all entries $A_{i,j}$ for $i, j \in [n]$ can be computed in parallel, the attention matrix $A$ computation requires depth $3(d_{\mathrm{std}} + d_{\oplus}) + d_{\exp}$.

    Then keep on analyzing:
    \begin{itemize}
        \item {\bf Row Nomalization:} Computing $D:=\diag(A{\bf 1}_n)$ requires depth $d_{\oplus}$ by Lemma~\ref{lem:float_operations_TC}. Computing $D^{-1}$ requires depth $d_{\mathrm{std}}$ by Lemma~\ref{lem:float_operations_TC} .
        \item {\bf Value Projection} Computing $AXW_V$ requires depth $2(d_{\mathrm{std}} + d_{\oplus})$ by applying Lemma~\ref{lem:matrix_multi}. Compute $D^{-1} \cdot A X W_V$ requires $d_{\mathrm{std}}$.
    \end{itemize}

    Combing the result, we need a
    \begin{align*}
        d_{\mathrm{all}} = 6(d_{\mathrm{std}} + d_{\oplus}) + d_{\exp}
    \end{align*}
    depth and size $\poly(n)$ uniform $\mathsf{TC}^0$ circuit to compute the attention layer.
\end{proof}

\subsection{Computing Layer-wise Norm Layer in \texorpdfstring{$\TC^0$}{} }\label{sec:app_ln_tc0}
This section presents the detailed proof for Lemma~\ref{lem:ln_tc0_informal}.


\begin{lemma}[Layer-wise norm layer computation in $\TC^0$, formal version of Lemma~\ref{lem:ln_tc0_informal}]\label{lem:ln_tc0_formal}
    Given an input tensor $\X \in \R^{h \times w \times c}$. Let $\mathsf{LN}(X):\R^{h \times w \times c} \to \R^{h \times w \times c}$ as defined in Definition~\ref{def:ln}. Under the following constraints:
    \begin{itemize}
         \item Satisfy $h = w = n$,
         \item Channel bound: $c \leq n$,
         \item Precision bound: $p \leq \poly(n)$.
     \end{itemize}
     The $\mathsf{LN}(\X)$ layer can be computed by a uniform $\mathsf{TC}^0$ circuit with:
     \begin{itemize}
        \item Size: $\poly(n)$.
        \item Depth: $5d_\mathrm{std} + 2d_{\oplus} + d_\mathrm{sqrt}$.
     \end{itemize}    
     with $d_{\mathrm{std}}$ and $d_{\oplus}$ defined in Definition~\ref{lem:float_operations_TC}, $d_{\mathrm{sqrt}}$ defined in Definition~\ref{lem:sqrt}.
\end{lemma}

\begin{proof}
    By Part 1 and Part 3  of Lemma~\ref{lem:float_operations_TC}, 
    computing mean vector $\mu_j$ requires depth $d_{\mathrm{std}}+d_{\oplus}$. By Part 1 and Part 3  of Lemma~\ref{lem:float_operations_TC}, 
    computing mean vector $\sigma^2_i$ requires depth $2d_{\mathrm{std}}+d_{\oplus}$.      By Lemma~\ref{lem:float_operations_TC} and Lemma~\ref{lem:sqrt}, computing $Y_{j,*}$ requires depth  $2d_{\mathsf{std}}+d_{\oplus}$. So the process requires total depth $5d_\mathrm{std} + 2d_{\oplus} + d_\mathrm{sqrt}$ and $\poly(n)$ size. 
\end{proof}