
\section{Model Formulation for FlowAR Architecture}\label{sec:model_formulation_of_flowar}
In this section, we provide a mathematical definition for every module of FlowAR. 
Section~\ref{sec:sample_function} provides the definition of up-sample and down-sample functions.
In Section~\ref{sec:downsample_tokenizer}, we mathematically define the VAE tokenizer.  Section~\ref{sec:ar_transformer} presents a mathematical formulation for every module in the autoregressive transformer in FlowAR. Section~\ref{sec:flow_matching} provides some important definitions of the flow-matching architecture. In Section~\ref{sec:inference_of_flowar}, we also provide a rigorous mathematical definition for the overall architecture of the FlowAR Model during the inference process.



\subsection{Sample Function}\label{sec:sample_function}
We define the bicubic upsampling function.
\begin{definition}[Bicubic Upsampling Function]\label{def:bicubic_up_sample_function}
Given the following:
\begin{itemize}
    \item {\bf Input tensor:} $\X \in \R^{h \times w \times c}$ where $h,w,c$ represent height, width, and the number of channels, respectively.
    \item {\bf Scaling factor:} A positive integer $r \geq 1$.
    \item {\bf Bicubic kernel:} $W:\R \to [0,1]$
\end{itemize}
The bicubic upsampling function $\phi_{\mathrm{up}}(\X,r)$ computes an output tensor $\Y \in \R^{rh \times rw \times c}$. For every output position $i \in [rh], j \in [rw], l \in [c]$:
\begin{align*}
    \Y_{i,j,l} =  \sum_{s=-1}^2 \sum_{t=-1}^2 W(s) \cdot W(t) \cdot \X_{\lfloor \frac{i}{r}\rfloor+s, \lfloor \frac{j}{r}\rfloor+t,l}
\end{align*}

\end{definition}
Next, we define the downsampling function.
\begin{definition}[Linear Downsampling Function]\label{def:linear_down_sample_function}
Given the following:
\begin{itemize}
    \item {\bf Input tensor:} $\X \in \R^{h \times w \times c}$ where $h,w,c$ represent height, width, and the number of channels, respectively.
    \item {\bf Scaling factor:} A positive integer $r \geq 1$.
\end{itemize}
The linear downsampling function $\phi_{\mathrm{down}}(\X,r)$ computes an output tensor $\Y \in \R^{(h/r) \times (w/r) \times c}$. Let $\Phi_{\mathrm{down}} \in \R^{(h/r \cdot w/r) \times hw}$ denote a linear transformation matrix. Reshape $\X$ into the matrix $X \in \R^{hw \times c}$ by flattening its spatial dimensions.  The output matrix is defined via:
\begin{align*}
     Y = \Phi_{\mathrm{down}}X \in \R^{(h/r \cdot w/r) \times c},
\end{align*}
Then reshaped back to $\Y \in \R^{(h/r) \times(w/r) \times c}$.
\end{definition}


\subsection{Multi-Scale Downsampling Tokenizer}\label{sec:downsample_tokenizer}
Given an input image, the FlowAR model will utilize the VAE to generate latent representation $\X^{\R^{h \times w \times c}}$. To meet the requirements of Multi-Scale autoregressive image generation, FlowAR uses a Multi-Scale VAE Tokenizer to downsample $\X$ and generate Token Maps of different sizes.
\begin{definition}[Multi-Scale Downsampling Tokenizer]\label{def:downsample_tokenizer}
 Given the following:
 \begin{itemize}
     \item {\bf Latent representation tensor:} $\X\in \R^{h \times w \times c}$ generated by VAE.
     \item {\bf Number of scales:} $K \in \mathbb{N}$.
     \item {\bf Base scaling factor:}  positive integer $a \geq 1$
     \item {\bf Downsampling functions:} For $i \in [K]$, define scale-specific factors $r_i := a^{K-i}$ and use the linear downsampling function $\phi_{\mathrm{down}}(\X,r_i)$ from Definition~\ref{def:linear_down_sample_function}.
 \end{itemize}
 Then tokenizer outputs a sequence of token maps $\{\Y^2, \Y^2,\dots, \Y^K\}$, where the $i$-th token map is
 \begin{align*}
     \Y^i := \phi_{\mathrm{down},i}(\X,r_i) \in \R^{(h / r_i) \times (w/r_i) \times c},
 \end{align*}
 Formally, the tokenizer is defined as
 \begin{align*}
     \mathsf{TN}(\X) := \{\Y^{1}, \dots,\Y^{K}\}.
 \end{align*} 

\end{definition}
\begin{remark}
    In \cite{ryh+24}, the base factor is set to $a = 2$, resulting in exponentially increasing scales $r_i = 2^{K-i}$ for $i \in [K]$.
\end{remark}

\subsection{Autoregressive Transformer}\label{sec:ar_transformer}

The autoregressive transformer is a key module of the FlowAR model. We will introduce each layer of autoregressive transformer in this section.
\begin{definition}[Attention Layer]\label{def:attn_layer}
Given the following:
\begin{itemize}
    \item {\bf Input tensor:} $\X \in \R^{h \times w \times c}$ where $h,w,c$ represent height, width, and the number of channels, respectively.
    \item {\bf Weight matrices:} $W_Q,W_K,W_V \in \R^{c \times c}$ will be used in query, key, and value projection, respectively.
\end{itemize}
The attention layer $\mathsf{Attn}(\X)$ computes an output tensor $\Y \in \R^{h \times w \times c}$ as follows:
\begin{itemize}
    \item {\bf Reshape:} Flatten $\X$ into a matrix $X \in \R^{hw \times c}$ with spatial dimensions collapsed.
    \item {\bf Compute attention matrix:} For $i,j \in [hw]$, compute pairwise scores:
    \begin{align*}
        A_{i,j} := & ~\exp(  X_{i,*}   W_Q   W_K^\top   X_{j,*}^\top), \text{~~for~} i, j \in [hw].
    \end{align*}
    \item {\bf Normalization:} Compute diagnal matrix $D:=\diag(A {\bf 1}_n) \in \R^{hw \times hw}$, where ${\bf 1}_n$ is the all-ones vector. And compute:
    \begin{align*}
         Y := D^{-1}AXW_V \in \R^{hw \times c}.
    \end{align*}
    then reshape $Y$ to $\Y \in \R^{h \times w \times c}$.
\end{itemize}
\end{definition}


Then, we define the multiple-layer perception layer.
\begin{definition}[MLP layer]\label{def:mlp}
Given the following:
\begin{itemize}
    \item {\bf Input tensor:} $\X \in \R^{h \times w \times c}$ where $h,w,c$ represent height, width, and the number of channels, respectively.
    \item {\bf Weight matrices and bias vector:} $W \in \R^{c \times d}$ and $b \in \R^{1 \times d}$.
\end{itemize}
The MLP layer computes an output tensor $\Y \in \R^{h \times w \times d}$ as follows:
\begin{itemize}
    \item {\bf Reshape:} Flatten $\X$ into a matrix $X \in \R^{hw \times c}$ with spatial dimensions collapsed.
    \item {\bf Affine transformation:} For all $j \in [hw]$, compute
    \begin{align*}
        Y_{j,*} = \underbrace{X_{j,*}}_{1\times c} \cdot \underbrace{W}_{c \times d} + \underbrace{b}_{1 \times d}
    \end{align*}
    Then reshape $Y \in \R^{hw \times d}$ into $\Y \in \R^{h \times w \times d}$.
\end{itemize}
The operation is denoted as $\Y := \mathsf{MLP}(\X,c,d)$.
\end{definition}


Next, we introduce the definition of the feedforward layer.
\begin{definition}[Feed forward layer]\label{def:ffn}
Given the following:
\begin{itemize}
    \item {\bf Input tensor:} $\X \in \R^{h \times w \times c}$ where $h,w,c$ represent height, width, and the number of channels, respectively.
    \item {\bf Weight matrices and bias vector:} $W_1, W_2 \in \R^{c \times d}$ and $b_1, b_2 \in \R^{1 \times d}$.
    \item {\bf Activation:} $\sigma:\R \to \R$ denotes the $\mathsf{ReLU}$ activation function which is applied element-wise.
\end{itemize}
The feedforward layer computes an output tensor $\Y \in \R^{h \times w \times d}$ as follows:
\begin{itemize}
    \item {\bf Reshape:} Flatten $\X$ into a matrix $X \in \R^{hw \times c}$ with spatial dimensions collapsed.
    \item {\bf Transform:} For each $j \in [hw]$, compute 
    \begin{align*}
        Y_{j,*}=  \underbrace{X_{j,*}}_{1 \times c} +  \sigma (\underbrace{X_{j,*}}_{1\times c} \cdot \underbrace{W_1}_{c \times c} + \underbrace{b_1}_{1\times c}) \cdot \underbrace{W_2}_{c \times c} + \underbrace{b_2}_{1 \times c} \in \R^{1 \times c}
    \end{align*}
    where $\sigma$ acts element-wise on intermediate results. Then reshape $Y \in \R^{hw \times c}$ into $\Y \in \R^{h \times w \times c}$.
\end{itemize}
The operation is denoted as $\Y := \mathsf{FFN}(\X)$.

\end{definition}

To move on, we define the layer normalization layer.
\begin{definition}[Layer Normalization Layer]\label{def:ln}
    Given the following:
    \begin{itemize}
        \item {\bf Input tensor:} $\X \in \R^{h \times w \times c}$ where $h,w,c$ represent height, width, and the number of channels, respectively.
    \end{itemize}
    The layer normalization computes $\Y$ through
    \begin{itemize}
        \item {\bf Reshape:} Flatten $\X$ into a matrix $X \in \R^{hw \times c}$ with spatial dimensions collapsed.
        \item {\bf Normalize:} For each $j \in [hw]$, compute
        \begin{align*}
            Y_{j,*} =  \frac{X_{j,*}-\mu_j}{\sqrt{\sigma_j^2}}
        \end{align*}
        where
        \begin{align*}
            \mu_j := \sum_{k=1}^c X_{j,k}/c, ~~ \sigma_{j}^2 = \sum_{k=1}^c(X_{j,k}-\mu_j)^2/c
        \end{align*}
        Then reshape $Y \in \R^{hw \times c}$ into $\Y \in \R^{h \times w \times c}$.
    \end{itemize}
    The operation is denoted as $\Y := \mathsf{LN}(\X)$.
\end{definition}


Now, we can proceed to show the definition of the autoregressive transformer.
\begin{definition}[Autoregressive Transformer]\label{def:ar_transformer}
    Given the following:
    \begin{itemize}
        \item {\bf Input tensor:} $\X \in \R^{h \times w \times c}$ where $h,w,c$ represent height, width, and the number of channels, respectively.
        \item {\bf Scales number:} $K \in \mathbb{N}$ denote the number of total scales in FlowAR.
        \item {\bf Token maps:} For $i \in [K]$, $\Y_i \in \R^{(h/r_i) \times (w/r_i) \times c}$ generated by the Multi-Scale Downsampling Tokenizer defined in Definition~\ref{def:downsample_tokenizer} where $r_i = a^{K-i}$ with base $a \in \mathbb{N}^+$.
        \item {\bf Upsampling functions:}  For $i \in [K]$, $\phi_{\mathrm{up},i}(\cdot,a): \R^{(h/r_i) \times (w/r_i) \times c}\to \R^{(h/r_{i+1}) \times (w/r_{i+1}) \times c}$ from Definition~\ref{def:bicubic_up_sample_function}.
        \item {\bf Attention layer:}  For $i \in [K]$, $\mathsf{Attn}_i(\cdot):\R^{(\sum_{j=1}^i h/r_j \cdot w/r_{j})\times c} \to \R^{(\sum_{j=1}^i h/r_j \cdot w/r_{j})\times c}$ which acts on flattened sequences of dimension defined in Definition~\ref{def:attn_layer}.
        \item {\bf Feed forward layer: } For $i \in [K]$, $\mathsf{FFN}_i(\cdot): \R^{(\sum_{j=1}^i h/r_j \cdot w/r_{j})\times c} \to \R^{(\sum_{j=1}^i h/r_j \cdot w/r_{j})\times c}$ which acts on flattened sequences of dimension defined in Definition~\ref{def:ffn}.
        \item {\bf Initial condition:} $\Z_{\mathrm{init}} \in \R^{(h/r_1) \times (w/r_1) \times c}$ denotes the initial token maps which encodes class information.
    \end{itemize}
    Then, the autoregressive processing is:
    \begin{enumerate}
        \item {\bf Initialization: } Let $\Z_1:=\Z_{\mathrm{init}}$.
        \item {\bf Iterative sequence construction:} For $i \geq 2$.
        \begin{align*}
            \Z_i := \mathsf{Concat}(\mathsf{Z}_{\mathrm{init}}, \phi_{\mathrm{up}, 1}(\Y^1, a), \ldots, \phi_{\mathrm{up}, i-1}(\Y^{i-1}, a)) \in \R^{(\sum_{j=1}^i h/r_j \cdot w/r_{j})\times c}
        \end{align*}
        where $\mathsf{Concat}$ reshapes tokens into a unified spatial grid.
        \item {\bf Transformer block:} For $i \in [K]$,
        \begin{align*}
            \mathsf{TF}_i(\Z_i) := \mathsf{FFN_i}(\mathsf{Attn}_i(\Z_i)) \in \R^{(\sum_{j=1}^i h/r_j \cdot w/r_{j})\times c}
        \end{align*}
        \item {\bf Output decomposition:} Extract the last scale's dimension   from the reshaped $\mathsf{TF}_i(\Z_i)$ to generate $\wh{\Y}_i \in \R^{(h/r_i) \times (w/r_i) \times c}$.
    \end{enumerate}
\end{definition}


\subsection{Flow Matching}\label{sec:flow_matching}
We begin by outlining the concept of velocity flow in the flow-matching architecture.
\begin{definition}[Flow]\label{def:flow}
Given the following:
\begin{itemize}
    \item {\bf Input tensor:} $\X \in \R^{h \times w \times c}$ where $h,w,c$ represent height, width, and the number of channels, respectively.
    \item {\bf Scales number:} $K \in \mathbb{N}$.
    \item {\bf Noise tensor: } For $i \in [K]$, $\F_i^0 \in \R^{(h / r_i) \times (w/r_i) \times c}$ with every entry sampled from $\mathcal{N}(0,1)$.
    \item {\bf Token maps:} For $i \in [K]$, $\wh{\Y}_i \in \R^{(h / r_i) \times (w/r_i) \times c}$ denote the token maps generated by autoregressive transformer defined in Definition~\ref{def:ar_transformer}.
\end{itemize}
Then, the model does the following:
\begin{itemize}
    \item {\bf Interpolation:} For timestep $t \in [0,1]$ and scale $i$,
    \begin{align*}
        \F_i^t := t \wh{\Y}_i + (1-t) \F_i^0
    \end{align*}
    defining a linear trajectory between noise $\F_0^i$ and target tokens  $\wh{\Y}_i$.
    \item {\bf Velocity Field:} The time-derivative of the flow at scale $i$ is 
    \begin{align*}
        \V^t_i := \frac{\d \F^t_{i}}{\d t} = \wh{\Y_i} -\F^0_i.
    \end{align*}
    constant across $t$ due to linear interpolation.
\end{itemize}
\end{definition}

To move forward, we propose an approach to enhance the performance of the flow-matching layer by replacing linear interpolation with a Quadratic Bézier curve.

\begin{definition}[High Order Flow]\label{def:high_order_flow}
Given the following:
\begin{itemize}
    \item {\bf Input tensor:} $\X \in \R^{h \times w \times c}$ where $h,w,c$ represent height, width, and the number of channels, respectively.
    \item {\bf Scales number:} $K \in \mathbb{N}$.
    \item {\bf Noise tensor: } For $i \in [K]$, $\F_i^0 \in \R^{(h / r_i) \times (w/r_i) \times c}$ with every entry sampled from $\mathcal{N}(0,1)$.
    \item {\bf Token maps:} For $i \in [K]$, $\wh{\Y}_i \in \R^{(h / r_i) \times (w/r_i) \times c}$ denote the token maps generated by autoregressive transformer defined in Definition~\ref{def:ar_transformer}.
\end{itemize}
Then, the model does the following:
\begin{itemize}
    \item {\bf Interpolation:} For timestep $t \in [0,1]$ and scale $i$,
    \begin{align*}
        \F_i^t := (1-t)^2 \F_i^0 + 2t(1-t) \mathsf{C}_i + t^2 \wh{\Y}_i
    \end{align*}
    defining a quadratic Bézier curve as the interpolation path between the initial noise and the target data. To be noticed, we take $\mathsf{C} = \frac{\F_i^0+\wh{\Y}_i}{2}$ as a control point that governs the curvature of the trajectory. This formulation replaces the standard linear interpolation with a higher-order flow, enabling a smoother and more flexible transition from noise to data in the flow-matching framework.
    \item {\bf Velocity Field:} The time-derivative of the flow at scale $i$ is 
    \begin{align*}
        \V^t_i := &~\frac{\d \F^t_{i}}{\d t}\\ =&~ -2(1-t)\F_i^0 + 2(1-2t) \mathsf{C}_i + 2t \wh{\Y}_i 
    \end{align*}
    constant across $t$ due to linear interpolation.
\end{itemize}
\end{definition}

We are now able to define the flow-matching layer, which is integrated in the FlowAR model.
\begin{definition}[Flow Matching Architecture]\label{def:flow_matching_architecture}
Given the following:
\begin{itemize}
    \item {\bf Input tensor:} $\X \in \R^{h \times w \times c}$ where $h,w,c$ represent height, width, and the number of channels, respectively.
    \item {\bf Scales number:} $K \in \mathbb{N}$ denote the number of total scales in FlowAR.
    \item {\bf Token maps:} For $i \in [K]$, $\wh{\Y}_i \in \R^{(h / r_i) \times (w/r_i) \times c}$ denote the token maps generated by autoregressive transformer defined in Definition~\ref{def:ar_transformer}.
    \item {\bf Interpolation Tokens:} For $i \in [K]$, $\F_i^t \in \R^{(h / r_i) \times (w/r_i) \times c}$ denote interpolated input defined in Definition~\ref{def:flow}.
    \item {\bf Time step:} For $i \in [K]$, $t_i \in [0,1]$ denotes timestep.
    \item {\bf Attention layer:}  For $i \in [K]$, $\mathsf{Attn}_i(\cdot):\R^{h/r_i \times w/r_i \times c} \to \R^{h/r_i \times w/r_i \times c}$ is defined in Definition~\ref{def:attn_layer}.
    \item {\bf MLP layer:}  For $i \in [K]$, $\mathsf{MLP}_i(\cdot,c,d):\R^{h/r_i \times w/r_i \times c} \to \R^{h/r_i \times w/r_i \times c}$ is defined in Definition~\ref{def:mlp}.
    \item {\bf LN layer:} For $i \in [K]$, $\mathsf{LN}_i(\cdot):\R^{h/r_i \times w/r_i \times c} \to \R^{h/r_i \times w/r_i \times c}$ is defined in Definition~\ref{def:ln}.
\end{itemize}
The computation steps of flow-matching layers are as follows:
\begin{itemize}
    \item {\bf Time-conditioned parameter generation:}
    \begin{align*}
        \alpha_1, \alpha_2, \beta_1, \beta_2, \gamma_1, \gamma_2:=  \mathsf{MLP}_i(\wh{\Y}_i + t_i \cdot {\bf 1}_{(h / r_i) \times (w/r_i) \times c},c,6c)
    \end{align*}
    \item {\bf Intermediate variable computation:}
    \begin{align*}
        \F'^{t_i}_i:= \mathsf{Attn}_i (\gamma_1 \circ \mathsf{LN}(\F_i^{t_i}) + \beta_1) \circ \alpha_1
    \end{align*}
    with $\circ$ denoting Hadamard (element-wise) product.
    \item {\bf Final projection:}
    \begin{align*}
        \F''^{t_i}_i := \mathsf{MLP}_i(\gamma_2 \circ \mathsf{LN}(\F'^{t_i}_i)+ \beta_2,c,c) \circ \alpha_2
    \end{align*}
\end{itemize}
The operation is denoted as $\F''^{t_i}_i := \mathsf{NN}_i(\wh{\Y_i},\F_i^{t_i},t_i)$
\end{definition}


\subsection{Inference of FlowAR Architecture}\label{sec:inference_of_flowar}
The inference phase of the FlowAR model differs from the training phase. During inference, neither the VAE nor the Multi-Scale Downsampling layers are used. Instead, given an initial token map representing class embeddings, the model autoregressively generates token maps across scales.
\begin{definition}[FlowAR Inference Architecture]\label{def:flow_architecture_inference}
    Given the following:
    \begin{itemize}
        \item {\bf Scales number:} $K \in \mathbb{N}$ denote the number of total scales in FlowAR.
        \item {\bf Scale factor:} For $i \in [K]$, $r_i:= a^{K-i}$ where base factor $a \in \mathbb{N}^+$.
        \item {\bf Upsampling functions:}  For $i \in [K]$, $\phi_{\mathrm{up},i}(\cdot,a): \R^{(h/r_i) \times (w/r_i) \times c}\to \R^{(h/r_{i+1}) \times (w/r_{i+1}) \times c}$ from Definition~\ref{def:bicubic_up_sample_function}.
        \item {\bf Attention layer:}  For $i \in [K]$, $\mathsf{Attn}_i(\cdot):\R^{(\sum_{j=1}^i h/r_j \cdot w/r_{j})\times c} \to \R^{(\sum_{j=1}^i h/r_j \cdot w/r_{j})\times c}$ which acts on flattened sequences of dimension defined in Definition~\ref{def:attn_layer}.
        \item {\bf Feed forward layer: } For $i \in [K]$, $\mathsf{FFN}_i(\cdot): \R^{(\sum_{j=1}^i h/r_j \cdot w/r_{j})\times c} \to \R^{(\sum_{j=1}^i h/r_j \cdot w/r_{j})\times c}$ which acts on flattened sequences of dimension defined in Definition~\ref{def:ffn}.
        \item {\bf Flow matching layer:} For $i \in [K]$, $\mathsf{NN}_i(\cdot,\cdot,\cdot):\R^{h/r_i \times w/r_i \times c}\times \R^{h/r_i \times w/r_i \times c}\times \R \to \R^{h/r_i \times w/r_i \times c}$ denote the flow-matching layer defined in Definition~\ref{def:flow_matching_architecture}.
        \item {\bf Initial condition:} $\Z_{\mathrm{init}} \in \R^{(h/r_1) \times (w/r_1) \times c}$ denotes the initial token maps which encodes class information.
        \item {\bf Time steps:} For $i \in [K]$, $t_i \in [0,1]$ denotes time steps.
        \item {\bf Interpolated inputs:} For $i \in [K]$, $\F_i^{t_i} \in \R^{h/r_i \times w/r_i \times c}$ defined in Definition~\ref{def:flow}.
        \item {\bf Cumulative dimensions:} We define $\wt{h}_i := \sum_{j=1}^i h/r_j$ and  $\wt{w}_i := \sum_{j=1}^i w/r_j$ for $i \in [K]$.
    \end{itemize}
    The FlowAR model conducts the following recursive construction:
    \begin{itemize}
        \item {\bf Base case $i=1$:}
        \begin{align*}
            &~\Z_1 = \Z_{\mathrm{init}}\\
            &~\wh{\Y}_1 = \mathsf{FFN}_1(\mathsf{Attn}_1(\Z_1))\\
            &~\wt{\Y}_1 = \mathsf{NN}_1(\wh{\Y}_1,\F_{1}^{t_1},t_1)
        \end{align*}
        \item {\bf Inductive step $i \geq 2$:}
        \begin{itemize}
            \item {\bf Spatial aggregation:}
            \begin{align*}
                \Z_i = \mathsf{Concat}(\Z_{\mathrm{init}},\phi_{\mathrm{up},1}(\wt{\Y}_{i-1}),\dots,\phi_{\mathrm{up},i-1}(\wt{\Y}_{i-1})) \in \R^{(\sum_{j=1}^i h/r_j \cdot w/r_j)\times c}
            \end{align*}
            \item {\bf Autoregressive transformer computation:}
            \begin{align*}
                \wh{\Y}_i = \mathsf{FFN}_i(\mathsf{Attn}_i(\Z_1))_{\wt{h}_{i-1}:\wt{h}_{i-1},\wt{w}_{i}:\wt{w}_{i},0:c}
            \end{align*}
            \item {\bf Flow matching layer:}
            \begin{align*}
                \wt{\Y}_i = \mathsf{NN}_i(\wh{\Y}_i,\F_{i}^{t_i},t_i)
            \end{align*}
        \end{itemize}
        The final output is $\wt{\Y}_K \in \R^{h \times w \times c}$.
    \end{itemize}
   
\end{definition}

