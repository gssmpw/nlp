\section{Related Work}
\label{sec:related_work}



\paragraph{Flow-based and diffusion-based models.} Another line of work focuses on flow-based and diffusion-based models for image and video generation \cite{hja20,hhs23,ltl+24}. The latent diffusion model (LDM) \cite{rbl+22} transforms image generation from pixel space to latent space, reducing the computational cost of diffusion-based generative models. This transformation enables these models to scale to larger datasets and model parameters, contributing to the success of LDM. Subsequent works, such as  U-ViT \cite{bnx+23} and  DiT \cite{px23}, replace the U-Net architecture with Vision Transformers (ViT) \cite{d20}, leveraging the power of Transformer architectures for image generation. Later models like SiT \cite{aak21} incorporate flow-matching into the diffusion process, further enhancing image generation quality. Many later studies \cite{ekb+24,jsl+24,wsd+24,wcz+23,wxz+24} have pursued the approach of integrating the strengths of both flow-matching and diffusion models to develop more effective image generation techniques. More related works on flow models and diffusion models can be found in \cite{hst+22,swyy23,wsd+24,lssz24_gm,llss24,hwl+24,hwsl24,lzw+24,cgl+25_homo,ccl+25,csy25,cll+25_var,cgl+25_gen,ssz+25_dit,ssz+25_prune}.

\paragraph{Circuit complexity.} Circuit complexity is a key field in theoretical computer science that explores the computational power of Boolean circuit families. Different circuit complexity classes are used to study machine learning models, aiming to reveal their computational constraints. A significant result related to machine learning is the inclusion chain $\mathsf{AC}^0 \subset \mathsf{TC}^0 \subseteq \mathsf{NC}^1$, although it is still unresolved whether $\mathsf{TC}^0 = \mathsf{NC}^1$ \cite{v99,ab09}. 
The analysis of circuit complexity limitations has served as a valuable methodology for evaluating the computational capabilities of diverse neural network structures. Recent investigations have particularly focused on Transformers and their two principal derivatives: Average-Head Attention Transformers (AHATs) and SoftMax-Attention Transformers (SMATs). Research has established that non-uniform threshold circuits operating at constant depth (within $\mathsf{TC}^0$ complexity class) can effectively simulate AHAT implementations \cite{mss22}, with parallel studies demonstrating similar computational efficiency achieved through L-uniform simulations for SMAT architectures \cite{lag+22}. Subsequent theoretical developments have extended these investigations, confirming that both architectural variants can be effectively approximated using \textsf{DLOGTIME}-uniform $\mathsf{TC}^0$ circuit models \cite{ms24}.
In addition to standard Transformers, circuit complexity analysis has also been applied to various other frameworks \cite{cll+24_mamba_circut,kll+25_circuit_var}. Other works related to circuit complexity can be referenced in \cite{cll+24,cll+24_tensor_tc,lll+24_hopfield_tc,cll+25_mamba_tc,lls+25,lls+25_grok}.