\section{Complexity of FlowAR Architecture}\label{sec:main_result}
This section presents key results on the circuit complexity of fundamental modules in the FlowAR architecture. Section~\ref{lem:matrix_multi} analyzes matrix multiplication, while Section~\ref{sec:down_up_tc0} examines the up-sampling and down-sampling functions. In Sections~\ref{sec:mlp_tc0} and \ref{sec:ffn_tc0}, we compute the circuit complexity of the MLP and FFN layers, respectively. Sections~\ref{sec:attention_tc0} and \ref{sec:ln_tc0} focus on the single attention layer and layer normalization. Section~\ref{sec:flow_matching} addresses the flow-matching layer. Finally, Section~\ref{sec:main_result} presents our main result, establishing the circuit complexity bound for the complete FlowAR architecture.
\subsection{Computing Matrix Products in \texorpdfstring{$\TC^0$}{}}\label{sec:compute_matrix_product}
we demonstrate that matrix multiplication is computable in $\mathsf{TC}^0$, which will be used later.
\begin{lemma}[Matrix multiplication in $\TC^0$, \cite{cll+24}]\label{lem:matrix_multi}
    Let the precision $p \leq \poly(n)$. Let $X \in \mathsf{F}_p^{n_1 \times d}, Y \in \mathsf{F}_p^{d \times n_2}$ be matrices. Assume $n_1\leq\poly(n), n_2\leq\poly(n)$. The matrix product $XY$ can be computed by a uniform $\mathsf{TC}^0$ circuit with:
    \begin{itemize}
        \item Size: $\poly(n)$.
        \item Depth: $d_{\mathrm{std}}+d_{\oplus}$.
    \end{itemize}
    where $d_{\mathrm{std}}$ and $d_{\oplus}$ are defined in Definition~\ref{lem:float_operations_TC}.
\end{lemma}


\subsection{Computing Down-Sampling and Up-Sampling in in \texorpdfstring{$\TC^0$}{}}\label{sec:down_up_tc0}
In this section, we show that Up-Sampling can be efficiently computable by a uniform $\mathsf{TC}^0$ circuit.
\begin{lemma}[Up-Sampling computation in $\mathsf{TC}^0$]\label{lem:up_tc0}
     Let $\X \in \R^{h \times w \times c}$ be the input tensor. Let $\phi_{\mathrm{up}}(X,r):\R^{h \times w \times c} \to \R^{(hr) \times (wr) \times c}$ denote the bicubic up sample function defined in Definition~\ref{def:bicubic_up_sample_function}. Assume $n = h = w$. Assume $r \leq n$. Assume $c \leq n$. Assume $p \leq \poly(n)$.
    The linear up sample function can be computed by a uniform $\mathsf{TC}^0$ circuit with:
    \begin{itemize}
        \item Size: $\poly(n)$.
        \item Depth: $2d_\mathrm{std} + d_{\oplus}$.
    \end{itemize}
    where $d_{\mathrm{std}}$ and $d_{\oplus}$ are defined in Definition~\ref{lem:float_operations_TC}.
\end{lemma}
\begin{proof}
    For each $i \in [nr], j \in [nr], l \in [c]$, we need to compute $\phi_{\mathrm{up}}(\X,r)_{i,j,l} = \sum_{s=-1}^2 \sum_{t=-1}^2 W(s)\cdot W(t)\cdot \X_{\frac{i}{r}+s,\frac{j}{r}+s,l}$.  We need a $2d_{\mathrm{std}}$ depth and $\poly(n)$ size circuit to compute $W(s)\cdot W(t)\cdot \X_{\frac{i}{r}+s,\frac{j}{r}+s,l}$ by Part 1 of Lemma~\ref{lem:float_operations_TC} and for all $s,t \in \{-1,0,1,2\}$, these terms can be computed in parallel. Furthermore, by Part 3 of Lemma~\ref{lem:float_operations_TC}, we can need a $d_{\oplus}$ depth and $\poly(n)$ size circuit to compute $\sum_{s=-1}^2 \sum_{t=-1}^2 W(s)\cdot W(t)\cdot \X_{\frac{i}{r}+s,\frac{j}{r}+s,l}$. Since the computation of $\phi_{\mathrm{up}}(\X,r)_{i,j,l}$ needs a $2d_{\mathrm{std}}+d_{\oplus}$ depth and $\poly(n)$ size circuit.

    Since for all $i \in [nr], j \in [nr], l \in [c]$, we can compute $\phi_{\mathrm{up}}(\X,r)_{i,j,l}$ in parallel, then the total depth of the circuit is $2d_{\mathrm{std}} + d_{\oplus}$ and size remains $\poly(n)$.
\end{proof}

Then, we move forward to consider the down-sampling function.
\begin{lemma}[Down-Sampling computation in $\mathsf{TC}^0$]\label{lem:down_tc0}
    Let $\X \in \R^{h \times w \times c}$ be the input tensor. Let $\phi_{\mathrm{down}}(X,r)$ be the linear down sample function from Definition~\ref{def:linear_down_sample_function}. Assume $n = h = w$. Assume $r \leq n$. Assume $c \leq n$. Assume $p \leq \poly(n)$.

    The function $\phi_{\mathrm{down}}$ can be computed by a uniform $\mathsf{TC}^0$ circuit with

    \begin{itemize}
        \item Size: $\poly(n)$.
        \item Depth: $d_\mathrm{std} + d_{\oplus}$.
    \end{itemize}
    where $d_{\mathrm{std}}$ and $d_{\oplus}$ are defined in Definition~\ref{lem:float_operations_TC}.
\end{lemma}
\begin{proof}
    By Definition~\ref{def:linear_down_sample_function}, we know that down-sampling computation is essentially matrix multiplication. Then, by Lemma~\ref{lem:matrix_multi}, we can easily get the proof.
\end{proof}







\subsection{Computing Multiple-layer Perceptron in \texorpdfstring{$\TC^0$}{} }\label{sec:mlp_tc0}
 We prove that MLP computation can be efficiently simulated by a uniform $\mathsf{TC}^0$ circuit.
\begin{lemma}[MLP computation in $\mathsf{TC}^0$, informal version of Lemma~\ref{lem:mlp_tc0_formal}]\label{lem:mlp_tc0_informal}
    Given an input tensor $\X\in \R^{h \times w \times c}$. Let $\mathsf{MLP}(\X,c,d)$ be the MLP layer defined in Definition~\ref{def:mlp}. Under the following constraints:
    \begin{itemize}
        \item Satisfy $h = w = n$,
        \item Channel bounds: $c, d\leq n$,
        \item Precision: $p \leq \poly(n)$,
    \end{itemize}
    The $\mathsf{MLP}(\X,c,d)$ function can be computed by a uniform $\mathsf{TC}^0$ circuit with:
    \begin{itemize}
        \item Size: $\poly(n)$.
        \item Depth: $2d_\mathrm{std} + d_{\oplus}$.
    \end{itemize}
    with $d_{\mathrm{std}}$ and $d_{\oplus}$ defined in Definition~\ref{lem:float_operations_TC}.
\end{lemma}




\subsection{Computing Feed-Forward Layer in \texorpdfstring{$\TC^0$}{} }\label{sec:ffn_tc0}
We also prove that feed-forward network computation can be simulated by a uniform $\mathsf{TC}^0$ circuit.
\begin{lemma}[FFN computation in $\mathsf{TC}^0$, informal version of Lemma~\ref{lem:ffn_tc0_formal}]\label{lem:ffn_tc0_informal}
 Given an input tensor $\X\in \R^{h \times w \times c}$. Let $\mathsf{FFN}(X):\R^{h \times w \times c} \to \R^{h \times w \times c}$ as defined in Definition~\ref{def:ffn}. Under the following constraints:
 \begin{itemize}
     \item Satisfy $h = w = n$,
         \item Channel bound: $c \leq n$,
         \item Precision bound: $p \leq \poly(n)$.
 \end{itemize}
The $\mathsf{FFN}(\X)$ layer can be computed by a uniform $\mathsf{TC}^0$ circuit with:
\begin{itemize}
    \item Size: $\poly(n)$.
    \item Depth: $6d_\mathrm{std} + 2d_{\oplus}$.
\end{itemize}
with $d_{\mathrm{std}}$ and $d_{\oplus}$ defined in Definition~\ref{lem:float_operations_TC}.
\end{lemma}




\subsection{Computing Single Attention Layer in \texorpdfstring{$\TC^0$}{}.} \label{sec:attention_tc0}
 We prove the single attention layer can be efficiently simulated by a uniform $\mathsf{TC}^0$ circuit.
\begin{lemma}[Attention layer computation in $\mathsf{TC}^0$, informal version of Lemma~\ref{lem:attn_tc0_formal}]\label{lem:attn_tc0_informal}
     Given an input tensor $\X \in \R^{h \times w \times c}$. Let $\mathsf{Attn}(X):\R^{h \times w \times c} \to \R^{h \times w \times c}$ as defined in Definition~\ref{def:attn_layer}. Under the following constraints:
     \begin{itemize}
         \item Satisfy $h = w = n$,
         \item Channel bound: $c \leq n$,
         \item Precision bound: $p \leq \poly(n)$.
     \end{itemize}
     The $\mathsf{Attn}(\X)$ layer can be computed by a uniform $\mathsf{TC}^0$ circuit with:
     \begin{itemize}
        \item Size: $\poly(n)$.
        \item Depth: $6(d_{\mathrm{std}} + d_{\oplus}) + d_{\exp}$.
     \end{itemize}    
     with $d_{\mathrm{std}}$ and $d_{\oplus}$ defined in Definition~\ref{lem:float_operations_TC}, $d_{\exp}$ defined in Definition~\ref{lem:exp}.
\end{lemma}



\subsection{Computing Layer-wise Norm Layer in \texorpdfstring{$\TC^0$}{}.}\label{sec:ln_tc0}
We prove that the layer normalization layer can be efficiently simulated by a uniform $\mathsf{TC}^0$ circuit.
\begin{lemma}[Layer normalization layer computation in $\TC^0$, informal version of Lemma~\ref{lem:ln_tc0_formal}]\label{lem:ln_tc0_informal}
    Given an input tensor $\X \in \R^{h \times w \times c}$. Let $\mathsf{LN}(X):\R^{h \times w \times c} \to \R^{h \times w \times c}$ as defined in Definition~\ref{def:ln}. Under the following constraints:
    \begin{itemize}
         \item Satisfy $h = w = n$,
         \item Channel bound: $c \leq n$,
         \item Precision bound: $p \leq \poly(n)$.
     \end{itemize}
     The $\mathsf{LN}(\X)$ layer can be computed by a uniform $\mathsf{TC}^0$ circuit with:
     \begin{itemize}
        \item Size: $\poly(n)$.
        \item Depth: $5d_\mathrm{std} + 2d_{\oplus} + d_\mathrm{sqrt}$.
     \end{itemize}    
     with $d_{\mathrm{std}}$ and $d_{\oplus}$ defined in Definition~\ref{lem:float_operations_TC}, $d_{\mathrm{sqrt}}$ defined in Definition~\ref{lem:sqrt}.
\end{lemma}


\subsection{Computing Flow Matching Layer in \texorpdfstring{$\TC^0$}{}.}\label{sec:fl_tc0}
We prove that the flow-matching layer can be efficiently simulated by a uniform $\mathsf{TC}^0$ circuit.
\begin{lemma}[Flow matching layer computation in $\TC^0$]\label{lem:fm_tc0}
     Given an input tensor $\X \in \R^{h \times w \times c}$. Let $\mathsf{NN}(X)$ denote the flow-matching layer defined in Definition~\ref{def:flow_matching_architecture}. Under the following constraints:
     \begin{itemize}
         \item Satisfy $h = w = n$,
         \item Channel bound: $c \leq n$,
         \item Precision bound: $p \leq \poly(n)$.
     \end{itemize}
     The $\mathsf{NN}(\cdot, \cdot,\cdot)$ can be computed by a uniform $\mathsf{TC}^0$ circuit with
     \begin{itemize}
        \item Size: $\poly(n)$.
        \item Depth: $26d_{\mathrm{std}}+ 12 d_{\oplus} + 2d_{\mathrm{sqrt}} + d_{\exp}$.
     \end{itemize} 
    with $d_{\mathrm{std}}$ and $d_{\oplus}$ defined in Definition~\ref{lem:float_operations_TC}, $d_{\exp}$ defined in Definition~\ref{lem:exp} and $d_{\mathrm{sqrt}}$ defined in Definition~\ref{lem:sqrt}.
\end{lemma}

\begin{proof}
    {\bf Considering Step 1 in the flow-matching layer:}
    By Lemma~\ref{lem:mlp_tc0_formal}, parameters $\alpha_1,\alpha_2,\beta_1,\beta_2,\gamma_1,\gamma_2$ are computed via a circuit with 
    \begin{itemize}
        \item {\bf Depth}: $2d_{\mathrm{std}} + d_\oplus$.
        \item {\bf Size}: $\poly(n)$
    \end{itemize}


    
    {\bf Considering Step 2 in flow-matching layer:} 
     By Lemma~\ref{lem:ln_tc0_formal}, $\mathsf{LN}(\F_i^t)$ requires depth $5d_\mathrm{std} + 2d_{\oplus} + d_\mathrm{sqrt}$. By Lemma~\ref{lem:float_operations_TC}, $A_1 = \gamma_1 \circ \mathsf{LN}(\F_t)+\beta_1$ requires depth $2d_{\mathrm{std}}$. By Lemma~\ref{lem:attn_tc0_formal}, $A_2 = \mathsf{Attn}(A_1)$ requires depth $6(d_{\mathrm{std}}+d_\oplus)+d_{\exp}$. By Lemma~\ref{lem:float_operations_TC} again, scaling $A_2 \circ \alpha_1$ requires depth $d_{\mathrm{std}}$. The total depth requires $14d_{\mathrm{std}} + 8d_{\oplus}+d_{\mathrm{sqrt}}+d_{\exp}$ for step 2.
    

    {\bf Considering Step 3 in flow-matching layer:} By Lemma~\ref{lem:ln_tc0_formal}, $\mathsf{LN}(\F'^t_i)$ requires depth $5d_\mathrm{std} + 2d_{\oplus} + d_\mathrm{sqrt}$. By Lemma~\ref{lem:float_operations_TC}, $A_3 = \gamma_2 \circ \mathsf{LN}(\wh{\F}_t)+\beta_2$ requries depth  $2d_{\mathrm{std}}$. By Lemma~\ref{lem:mlp_tc0_formal}, $A_4 = \mathsf{MLP}(A_3,c,c)$ requires depth $2d_{\mathrm{std}} + d_\oplus$.
    By Lemma~\ref{lem:float_operations_TC} again, $A_4 \circ \alpha_2$ requires depth $d_{\mathrm{std}}$. The total depth requires $10d_{\mathrm{std}}+3d_{\oplus}+d_{\mathrm{sqrt}}$  for step 3..

    Finally, combining the result above, we need a circuit with depth $26d_{\mathrm{std}}+ 12 d_{\oplus} + 2d_{\mathrm{sqrt}} + d_{\exp}$ and size $\poly(n)$ to simulate the flow-matching layer.
    
\end{proof}


\subsection{Circuit Complexity Bound for FlowAR Architecture}\label{sec:main_result_flowar}
We present that the FlowAR Model can be efficiently simulated by a uniform $\mathsf{TC}^0$ circuit.
\begin{theorem}[FlowAR Model computation in $\TC^0$]\label{thm:flowar_tc0}
Given an input tensor $\X \in \R^{h \times w \times c}$. Under the following constraints:
\begin{itemize}
    \item Satisfy $h = w = n$,
    \item Channel bound: $c \leq n$,
    \item Precision bound: $p \leq \poly(n)$.
    \item Number of scales: $K = O(1)$,
    \item $d_{\mathrm{std}},d_\oplus,d_{\mathrm{sqrt}},d_{\exp} = O(1)$.
\end{itemize}
Then, the FlowAR Model can be simulated by a uniform $\TC^0$ circuit family.

\end{theorem}
\begin{proof}
    For every $i \in [K]$, by Lemma~\ref{lem:up_tc0}, Lemma~\ref{lem:down_tc0},  Lemma~\ref{lem:attn_tc0_informal}, Lemma~\ref{lem:ffn_tc0_informal} and Lemma~\ref{lem:fm_tc0}, we can simulate the $i$-th layer of FlowAR Model with a uniform $\TC^0$ circuit whose size is $\poly(n)$ and depth is $O(1)$. Since the total number of layers $K = O(1)$, the composition of all $K$ circuits yields a single uniform $\mathsf{TC}^0$ circuit with
    \begin{itemize}
        \item {\bf Size:} $\poly(n)$,
        \item {\bf Depth: }$O(1)$.
    \end{itemize}
\end{proof}

In Theorem~\ref{thm:flowar_tc0}, we establish that a FlowAR model with $\poly(n)$ precision, constant depth, and $\poly(n)$ size can be efficiently simulated by a $\mathsf{DLOGTIME}$-uniform $\TC^0$ circuit family. This indicates that while the flow-matching architecture enhances the capability of visual autoregressive models, the FlowAR architecture remains inherently limited in expressivity under circuit complexity theory.