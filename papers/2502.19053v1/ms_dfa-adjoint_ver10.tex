%#BIBTEX bibtex2pdf ms_dfa-adjoint_ver10
% ****** Start of file apssamp.tex ******
%
%   This file is part of the APS files in the REVTeX 4.2 distribution.
%   Version 4.2a of REVTeX, December 2014
%
%   Copyright (c) 2014 The American Physical Society.
%
%   See the REVTeX 4 README file for restrictions and more information.
%
% TeX'ing this file requires that you have AMS-LaTeX 2.0 installed
% as well as the rest of the prerequisites for REVTeX 4.2
%
% See the REVTeX 4 README file
%
% It also requires running BibTeX. The commands are as follows:
%  1)  latex apssamp.tex
%  2)  bibtex apssamp
%  3)  latex apssamp.tex
%  4)  latex apssamp.tex
%
\documentclass[%
  preprint,
%superscriptaddress,
%groupedaddress,
%unsortedaddress,
%runinaddress,
%frontmatterverbose, 
%preprint, 
%preprintnumbers,
%nofootinbib,
%nobibnotes,
%bibnotes,
 amsmath,amssymb,
% aps,
%pra,
%prb,
%rmp,
%prstab,
%prstper,
%floatfix,
]{revtex4-2}

%\usepackage{amsmath}
%\usepackage{amssymb}
%\usepackage[dvipdfmx]{graphicx}
\usepackage{dcolumn} % Align table columns on decimal point       
\usepackage{bm}      % bold math
\usepackage{setspace}
\usepackage{amsfonts}
\usepackage{braket}

\usepackage{graphicx}% Include figure files
%\usepackage{dcolumn}% Align table columns on decimal point
%\usepackage{bm}% bold math
%\usepackage{hyperref}% add hypertext capabilities
%\usepackage[mathlines]{lineno}% Enable numbering of text and display math
%\linenumbers\relax % Commence numbering lines

%\usepackage[showframe,%Uncomment any one of the following lines to test 
%%scale=0.7, marginratio={1:1, 2:3}, ignoreall,% default settings
%%text={7in,10in},centering,
%%margin=1.5in,
%%total={6.5in,8.75in}, top=1.2in, left=0.9in, includefoot,
%%height=10in,a5paper,hmargin={3cm,0.8in},
%]{geometry}

%
\newcommand{\vc}{\mathbf}
\newcommand{\gvc}[1]{\mbox{\boldmath $#1$}}
\newcommand{\fracd}[2]{\frac{\displaystyle #1}{\displaystyle #2}}
\newcommand{\ave}[1]{\left< #1 \right>}
\newcommand{\red}[1]{\textcolor[named]{Red}{#1}}
\newcommand{\blue}[1]{\textcolor[named]{Blue}{#1}}
\newcommand{\green}[1]{\textcolor[rgb]{0,0.6,0}{#1}}
\newcommand{\del}[3] {\frac{\partial^{#3} #1}{\partial #2^{#3}}}
\newcommand{\dev}[3]{\frac{\text{d}^{#3} #1}{\text{d}#2^{#3}}}
\newcommand{\pdev}[3]{{\text{d}^{#3} #1}/{\text{d}#2^{#3}}}
\newcommand{\pdel}[3]{{\partial^{#3} #1}/{\partial #2^{#3}}}
\newcommand{\intd}[1]{\text{d} {#1}}
\newcommand{\emf}[1]{{\gtfamily \bfseries #1}}


\newcommand{\subti}[1]{\begin{itemize} \item \emf{ #1} \end{itemize}}

\newcommand{\Real}{\operatorname{Re}}
\newcommand{\Imag}{\operatorname{Im}}

\newcommand{\am}{{\bm a}}
\newcommand{\bb}{{\bm b}}
\newcommand{\ee}{{\bm e}}
\newcommand{\ff}{{\bm f}}
\newcommand{\hh}{{\bm h}}
\newcommand{\mm}{{\bm m}}
\newcommand{\pp}{{\bm p}}
\newcommand{\rr}{{\bm r}}
\newcommand{\sm}{{\bm s}}
%\newcommand{\tm}{{\bm t}}
\newcommand{\tm}{{\yy_{tag}}}
\newcommand{\uu}{{\bm u}}
\newcommand{\vv}{{\bm v}}
\newcommand{\MM}{{\bm M}}
\newcommand{\ww}{{\bm w}}
\newcommand{\xx}{{\bm x}}
\newcommand{\yy}{{\bm y}}
\newcommand{\zz}{{\bm z}}
\newcommand{\ttheta}{{\bm \theta}}
\newcommand{\BB}{{\bm B}}
\newcommand{\tBB}{{\tilde{\bm B}}}
%
\newcommand{\Model}{{\mathbb M}}
\newcommand{\RR}{{\mathbb R}}
\newcommand{\NN}{{\mathbb N}}
%
\newcommand{\oomega}{\mbox{\boldmath $\omega$}}
\newcommand{\WW}{{\bm W}}
\newcommand{\Ab}{\mbox{\boldmath $A$}}
\newcommand{\FF}{\mbox{\boldmath $F$}}
\newcommand{\KK}{\mbox{\boldmath $K$}}
\newcommand{\GG}{\mbox{\boldmath $G$}}
%
\newcommand{\tr}{\mathrm{T}}
%
\newcommand{\CC}{\mbox{$\hat{C}$}}
\newcommand{\II}{\mbox{$\hat{I}$}}
\newcommand{\HH}{\mbox{$\hat{H}$}}

\newcommand{\Am}{\mbox{$\hat{A}$}}
\newcommand{\PP}{\mbox{$\hat{P}$}}
\newcommand{\QQ}{\mbox{$\hat{Q}$}}

\newcommand{\Te}{T_e}
\newcommand{\Tc}{T_c}
\newcommand{\ta}{\tau}
\newcommand{\dl}{\delta}

\usepackage{amsmath}	% required for `\align' (yatex added)
\begin{document}

\preprint{APS/123-QED}

\title{
%Noise-robust physical neural networks trained without backpropagation: 
Blending optimal control and biologically plausible learning for noise-robust physical neural networks
%
%Noise-robust physical neural networks driven by external control
}
%\thanks{A footnote to the article title}%


\author{
Satoshi Sunada$^{1}$,
%}
%\email{sunada@se.kanazawa-u.ac.jp}
%\author{
Tomoaki Niiyama$^{1}$,
Kazutaka Kanno$^{2}$, Rin Nogami$^{2}$, 
Andr\'e R\"ohm$^{3}$, Takato Awano$^{1}$, and Atsushi Uchida$^{2}$ 
}
%

\affiliation{%
%${}^1$Graduate School of Natural Science and Technology, Kanazawa University
%Kakuma, Kanazawa, Ishikawa, 920-1192, Japan\\
%
${}^1$Faculty of Mechanical Engineering, Institute of Science and
Engineering, Kanazawa University,
Kakuma-machi, Kanazawa, Ishikawa 920-1192, Japan \\
%
%
${}^2$Department of Information and Computer Sciences, Saitama
 University, 255 Shimo-Okubo, Sakura-ku, Saitama City, Saitama,
 338-8570, Japan\\
%
${}^3$Department of Information Physics and Computing, Graduate School of Information Science and Technology, The University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo 113-8656, Japan\\
}%

\date{\today}% It is always \today, today,
             %  but any date may be explicitly specified

\begin{abstract}
The rapidly increasing computational demands for artificial intelligence (AI) have spurred the exploration of computing principles beyond conventional digital computers. Physical neural networks (PNNs) offer efficient neuromorphic information processing by harnessing the innate computational power of physical processes; however, training their weight parameters is computationally expensive. We propose a training approach for substantially reducing this training cost. Our training approach merges an optimal control method for continuous-time dynamical systems with a biologically plausible training method---direct feedback alignment. In addition to the reduction of training time, this approach achieves robust processing even under measurement errors and noise without requiring detailed system information. The effectiveness was numerically and experimentally verified in an optoelectronic delay system. Our approach significantly extends the range of physical systems practically usable as PNNs.
\end{abstract}

%\keywords{Suggested keywords}%Use showkeys class option if keyword
                              %display desired
\maketitle

%\section{Introduction}
{\it Introduction.---}The availability of vast amounts of data and computing power has spurred rapid development of deep learning. However, the increasing power demands for deep learning have outpaced the advancements in digital electronics \cite{Mehonic:2022wi}. This situation has motivated the development of novel computing technologies that differ significantly from the existing von Neumann computers \cite{Furber_2016, Nahmias:18,RevModPhys.91.045002,Merolla668,Grollier:2020aa,Markovic:2020aa,9049105,Shastri:2021aa,Nakajima:2022aa,Science.abq8271_Delocalized,Sciadv.aay6946_wavephysics,PhysRevResearch.3.033243,Zhang:2020aa,TANAKA2019100,Wright:2022aa,doi:10.1126/science.adi8474}. 

Utilizing physical systems enables efficient neuromorphic information processing by leveraging the intrinsic properties of the systems themselves \cite{Wright:2022aa,doi:10.1126/science.adi8474,momeni2024trainingphysicalneuralnetworks}. One example that has garnered significant attention in physical sciences is reservoir computing (RC) \cite{TANAKA2019100,Jaeger78,Maass:2002,VERSTRAETEN2007391,Appeltant:2011ab,Liang:2024aa} or extreme learning machines (ELMs) \cite{HUANG2006489,Ortin:2015aa}, which can utilize physical dynamics for information processing, making it powerful for processing time-dependent or static data. However, RC and ELMs are limited in tunability as they are trained only at the readout weight parameters, which restricts their information processing capabilities.

Deep neural networks (DNNs) offer high expressivity owing to its layer-to-layer information propagation, enabling advanced processing for practical tasks. A straightforward approach to physically implementing DNNs is using a physical system with adjustable internal elements for weight control \cite{PhysRevApplied.18.014040,Shen:2017aa,Feldmann:2021aa}. (See Fig.~\ref{fig_schematic}(a)). However, designing such a physical system with numerous adjustable elements remains challenging for large-scale computations. 

This study considers an alternative approach that involves driving physical systems externally [Fig.~\ref{fig_schematic}(b)], eliminating the need for adjusting internal elements. It offers the following features:  
First, it enables advanced learning even in physical systems without tuning elements, using only a few external controllers \cite{PhysRevApplied.15.034092}. Second, the approach offers a high expressivity for information processing of continuous-time dynamical systems \cite{NEURIPS2018_69386f6b}, unlike DNNs that are associated with discrete-time dynamical systems. Third, concepts, such as neurons and synapses, do not appear explicitly, allowing for a direct correlation between the dynamical process and DNN-like information processing. This provides insights into the learning mechanism from the perspective of optimal control of dynamical systems.  

However, optimal control methods for dynamical systems encounter a common challenge shared with the backpropagation method utilized in DNNs.
The parameter updates are based on error backpropagation, necessitating a substantial amount of computational energy and time for training. 
Conversely, biological systems achieve efficient learning in noisy and dynamic environments without relying on error backpropagation \cite{Jaeger:2023uw}. 

This study proposes a training approach to significantly reduce the computational training cost by incorporating a biologically plausible learning strategy known as direct feedback alignment (DFA) \cite{nkland2016direct} into an optimal control method for dynamical systems. This approach is based on external driving and can be applied to various dynamical systems without requiring numerous adjustable internal parameters, unlike previous approaches \cite{Wright:2022aa,Nakajima:2022aa,doi:10.1126/science.adi8474}; therefore, it is versatile and useful for extracting the computational capabilities of physical systems. Moreover, the proposed approach enables highly robust processing amid noise and yields excellent performance. 
This approach can unlock the computational potential of numerous physical platforms.


\begin{figure}[htbp]
\centering\includegraphics[width=8.6cm]{Figure1.eps}
\caption{\label{fig_schematic}
(a) Schematic of physical neural networks (PNNs) with adjustable internal parameters for weight control. 
(b) Physical system driven by external control signals.
(c) Proposed approach based on external control. 
}
\end{figure}
 

%\section{Theory}
{\it Optimal control-based model.---}The primary objective is to identify a function that can map an input $\xx \in \RR^{N_x}$ onto its corresponding target $\tm \in \RR^{L}$ for a given training dataset, where $N_x$ and $L$ denote the degree of freedom of input data and target information, respectively. To express the input-target relationship, this study employs a continuous-time dynamical system with time delays, which is governed by 
%
\begin{align}
\dev{\rr(t)}{t}{}  = \FF\left[
\rr(t),\rr(t-\tau_D),\uu(t)%\rr(t-\tau_2),\cdots,\rr(t-\tau_M),\uu(t)
\right], \label{eq_system} 
\end{align}
where $\rr(t) \in \RR^{N_r}$ represents the state vector at time $t$ and $\uu(t) \in \RR^{N_u}$ represents $N_u$ control signals. $\tau_D$ denotes the delay time. The dynamical system is driven by external control signals $\uu(t)$, such that output $\yy$ corresponds to a target vector $\tm$ for a given input $\xx$. The schematic is shown in Fig.~\ref{fig_schematic}(c). Based on the correspondence between DNNs and dynamical systems \cite{PhysRevApplied.15.034092}, an input $\xx$ is encoded in an initial state as $\rr(t) = \GG_{in}(\xx)$ for $-\tau_D \le t \le 0$. The input information is transformed by the time evolution (forward propagation) of the dynamical system up to the end time $t = T_e$. Let $T_c$ be the time length of the output layer, respectively. $\yy$ is expressed as a function of $\rr(t;\xx)$ for $T_e - T_c < t \le T_e$ as $\yy = \GG_{out}\left(\zz\right)$, where $\zz = \int_{T_e-T_c}^{T_e}\ww(t)\rr(t) dt$ and $\ww(t) \in \RR^{L\times N_r}$ represents an output weight function. The learning objective is to identify optimal control $\uu^{*}(t)$ and $\ww^{*}(t)$ that minimize the loss function $L(\yy,\tm)$ for a given training dataset.

The output weight function $\ww(t)$ can easily be updated by utilizing the gradient of $L$ with respect to $\ww(t)$ as $\ww(t) \rightarrow \ww(t) - \eta_w \pdel{L}{\ww}{}$, where $\eta_w$ represents the learning rate. This does not incur significant computational costs. Meanwhile, $\uu(t)$ can be trained through the adjoint method \cite{kirk2004optimal} developed within the realm of optimal control theory, as shown in Supplementary Material (SM) \cite{Supplementary}. The update of $\uu(t)$ is described by the adjoint vector $\pp(t)$, corresponding to the back-propagated error, as $\uu(t) \rightarrow \uu(t) - \eta_u \pp^{\tr}\pdel{\FF}{\uu}{}$ \cite{Supplementary}, where $\eta_u$ denotes the learning rate.
%
$\pp(t)$ is determined by solving the adjoint equation backward in time from $t = T_e$ to $0$, with $\pp(T_e) = 0$,
\begin{align}
\dev{\pp^{\tr}}{t}{}
&= 
-\Theta_1(t)\del{L}{\zz}{}\ww
-\pp^{\tr}\del{\FF}{\rr}{} \nonumber \\
&-
\Theta_0(t)\pp^{\tr}(t+\tau_D)\del{\FF(t+\tau_D)}{\rr}{},
\label{eq_adeq_text}
\end{align}
where $\Theta_1$ and $\Theta_0$ represent step functions that are equal to 1 only for $T_e-T_c\le t < T_e$ and $0 \le t \le T_e-\tau_D$. 
% 
%Eq.~(\ref{eq_adeq_text}) corresponds to the continuous-time version of backpropagation \cite{Supplementary}.
%
The adjoint method corresponds to backpropagation-based training for a continuous-time dynamical system described by Eq.~(\ref{eq_system}). See SM for details \cite{Supplementary}.
%
However, solving Eq.~(\ref{eq_adeq_text}) requires significant computational resources for training. 
%Furthermore, for hardware implementation, detailed prior knowledge of $\FF$ in the physical systems is required for solving Eq.~(\ref{eq_adeq_text}), which can hinder efficient training. 

{\it DFA--adjoint.---}To address these challenges, we incorporate the DFA into the adjoint optimal control method. DFA utilizes fixed random matrices and the error at the last layer to train the weight parameters \cite{nkland2016direct,NEURIPS2020_69d1fc78}. This approach helps to overcome the vanishing gradient problem and can lead to a substantial reduction in the computation steps required for backpropagation. 
%Additionally, DFA can be applied to the training of large-scale models \cite{NEURIPS2020_69d1fc78}. 
Based on the correspondence between DNNs and dynamical systems \cite{PhysRevApplied.15.034092,DBLP:journals/corr/abs-1908-10920,NIPS2016_14851003}, we define the following adjoint vector \cite{Supplementary}:
\begin{align}
 \hat{\pp}^{\tr}(t) = \ee^{\tr}\BB(t), 
\end{align}
where $\ee = \yy-\tm$ represents the error at the last layer and $\BB(t) \in \RR^{L\times N_r}$ denotes a time-dependent random matrix. 
%
Instead of solving Eq. (\ref{eq_adeq_text}) directly, $\pp(t)$ is replaced with $\hat{\pp}(t)$.
%We can skip solving Eq.~(\ref{eq_adeq_text}). 
%The control signals, $\uu(t)$, can be updated without solving Eq.~(\ref{eq_adeq_text}). 
This approach is referred to as {\it DFA--adjoint} in this study. This enables learning of the control signal, $\uu(t)$, through the following steps:

\noindent (1) Forward propagation: The dynamical system evolves over time based on given input data $\xx$.\\
\noindent (2) $\hat{\pp}^{\tr}(t) = \ee^{\tr}\BB(t)$ is computed from the output $\yy = \GG_{out}(\zz)$. \\
\noindent (3) $\uu(t)$ and $\ww(t)$ are updated.
Steps (1)--(3) are repeated until the desirable output $\yy$ is obtained.
%
The DFA--adjoint method corresponds to a generalized version of the DFA method, which can be applied to continuous-time systems \cite{Supplementary}.
% 

In the DFA--adjoint method, the selection of $\BB(t)$ is crucial for achieving optimal computational performance. We determined that the characteristic timescale of the waveform of $\BB(t)$ should closely align with that of the system dynamics. 
We set the elements of $\BB(t)$ as follows:
\begin{align}
B_{li}(t)  = \sum_{m=1}^{M_B}
A_m^{(li)}\sin
\left(
\omega_m t
+
\phi_m^{(li)}
\right), \label{eq_bb}
\end{align} 
where $A_m^{(li)}$ and $\phi_m^{(li)}$ are determined by the values drawn from a uniform distribution. $\omega_m$ distributes around the characteristic angular frequency of the system dynamics. 
%
See SM for details \cite{Supplementary}. 

{\it Numerical simulations.---}To assess the training performance of the DFA--adjoint method, we considered a time-delay system, governed by the following equation:
\begin{align}
\tau_L \dev{r(t)}{t}{} 
&= -\left(1 + \dfrac{\tau_L}{\tau_H} \right)r(t)
- \dfrac{1}{\tau_H}\int r(t)dt \nonumber \\
&+ 
\beta\cos^2\left(
u_1(t)r(t-\tau_D) + u_2(t)
\right)
+\tau_L\sigma\xi(t), \label{eq_delay}
\end{align}
where $u_1(t)$ and $u_2(t)$ represent external driving forces to control the dynamical system. $\tau_D$, $\beta$, $\tau_L$, and $\tau_H$ represent delay time, feedback strength, and the time constants of low- and high-pass filters, respectively. 
$\xi(t)$ denotes the Gaussian noise with zero mean and unity standard deviation, whereas $\sigma$ denotes the noise strength. The equation corresponds to a model equation for an optoelectronic delay system \cite{murphy2010complex}. 

We encode input data $\xx$ as $r(t) = G_{in}(\xx) = \mm^{\tr}(t)\xx$ for $-\tau_D \le t \le 0$ using a mask pattern $\mm(t) \in \RR^{N_x}$.
%
From this initial state, the system evolves over time until $T_e = 3\tau_D$. The output $\yy$ is determined from the softmax functions of $\zz$ with $T_c = \tau_D$. 

To assess the performance, we utilized the MNIST handwritten digit dataset, which comprises a training set of 60,000 images, along with a test set of 10,000 images \cite{MNIST_LeCun_url}. 
The DFA--adjoint method was employed to train control signals $u_1(t)$ and $u_2(t)$, as well as $\mm(t)$ for the MNIST dataset \cite{Supplementary}. For training, a minibatch update was executed using the Adam optimizer \cite{kingma2014adam}. 

Figure~\ref{fig_result1}(a) shows $r(t)$, $u_1(t)$, and $u_2(t)$ before training and after 50 training epochs. The following parameters were utilized in this simulation:
$\tau_L = 15.9$ $\mu$s, $\tau_H = 1.59$ ms, $\tau_D = 0.92$ ms, $\beta = 3.0$, and $\sigma = 0$. 
$r(t)$ exhibited changes corresponding to variations in $u_1(t)$ and $u_2(t)$. After 50 training epochs, the classification accuracy increased to 0.973 [Fig.~\ref{fig_result1}(b)].
%
To gain insight into the training mechanism, we measured the cosine similarity between the update vectors in the DFA--adjoint and normal adjoint (gradient-based) methods. See SM for details \cite{Supplementary}.
Figure~\ref{fig_result1}(c) shows that the similarity becomes positive, indicating that the update direction in the DFA--adjoint method tends to roughly align with that computed in the adjoint method. This trend is similar to that observed in feedback alignment applied to standard neural networks \cite{Lillicrap:2016aa}.

%
For further performance evaluations, we utilized the Fashion-MNIST \cite{arXiv.1708.07747} and CIFAR-10 datasets \cite{Krizhevsky09learningmultiple}
and compared the classification accuracy achieved through training with the DFA--adjoint method against results using the adjoint method and without optimal control. 
In the absence of control, the system functions as an ELM.
% with only readout weights being trained. 
The results are listed in Table~\ref{tab:table1}.
The DFA--adjoint method exhibited superior classification performance compared with the control-free scenario, albeit showing a slight degradation compared with the adjoint method and previous backpropagation approach \cite{JMLR:v16:hermans}.


\begin{figure}[htbp]
\centering\includegraphics[width=8.6cm]{Figure2.eps}
\caption{\label{fig_result1}
(a) $r(t)$, $u_1(t)$, and $u_2(t)$ before and after training. $u_1(t)$ and $u_2(t)$ before training were represented by black solid and black dashed lines, respectively. 
(b) A typical learning curve obtained by a single run (black solid line with circles) and mean learning curve (blue solid line). 
(c) Cosine similarity in the update vectors between the DFA--adjoint and adjoint methods \cite{Supplementary}. The shadow represents the standard deviation. (d) Average computational training time per image.}
\end{figure}

\begin{table}[b]%The best place to locate the table environment is directly after its first reference in text
\caption{\label{tab:table1}%
Classification accuracy for MNIST, Fashion-MNIST, and CIFAR-10 datasets. Each accuracy was obtained by five runs \cite{Supplementary}.}
\begin{ruledtabular}
\begin{tabular}{lcdr}
%\textrm{Left\footnote{Note a.}}&
\textrm{Methods }&
\textrm{MNIST}&
\multicolumn{1}{c}{\textrm{Fashion-MNIST}}&
\textrm{CIFAR-10}\\
\colrule
DFA--adjoint & 0.973 & 0.866 & 0.466 \\
%Adjoint control & 0.977 & 0.881 & 0.507\\

Adjoint (Backprop.) & 0.977 & 0.881 & 0.507\\
ELM (w/o control)  & 0.869 & 0.792 & 0.275\\
\end{tabular}
\end{ruledtabular}
\end{table}

The DFA--adjoint method offered a notable advantage by eliminating the need to solve Eq.~(\ref{eq_adeq_text}) for backpropagation, significantly reducing the computational training time. Training time reduction is particularly crucial for large-scale physical systems, e.g., those with a long delay time. Figure~\ref{fig_result1}(d) shows the average computational training times per one input image for the MNIST dataset as a function of delay time $\tau_D$. The rate of the computational training time to $\tau_D$ is less than one-fifth of that for the adjoint method, making a substantial impact on training efficiency for a larger-scale system.
%
Further acceleration is possible when a parallel computation scheme can be used.

In terms of hardware implementation, robustness against external noise is a critical requirement.
%
To assess robustness to noise, Gaussian noise was incorporated into Eq.~(\ref{eq_delay}), and $r(t)$ was measured starting from an initial state with various noise instances. The noise robustness was evaluated through the following cross-correlation: 
\begin{align}
C = \dfrac{1}{K}\sum_{k>k'}\dfrac{
\langle
r_k(t)r_{k'}(t)
\rangle_t
}
{\sigma_k\sigma_{k'}},
\end{align}
where $r_k(t)$ and $\sigma_k$ represent the state variable at time $t$ and the standard deviation of the $k$-th instance, respectively. $\langle \cdot \rangle_t$ and $K$ denote the time average and number of summations, respectively. 
%
Figures~\ref{fig_result2}(a) and \ref{fig_result2}(b) show $C$ and classification accuracy for the MNIST dataset as functions of noise strength $\sigma$, respectively. 
%
For this demonstration, we set $\beta = 3.0$, resulting in a chaotic behavior before training. Hence, the state variable was sensitive to noise, leading to varying outputs even for identical input data. 
%
This sensitivity resulted in a significant reduction in $C$. In practice, the time evolution after training in the control-free scenario led to decreasing $C$ and classification accuracy with increasing $\sigma$. 
In contrast, the DFA--adjoint method effectively mitigated the decline in $C$, thereby maintaining consistently high accuracy.
%
The noise robustness observed in the DFA--adjoint method can be attributed to its external driving. The system is driven by the applied control signals and compelled to adhere to these signals, rather than noise, thereby enhancing the robustness of the system dynamics.

%
Noise robustness is retained even when the noise strength $\sigma^*$ during the training phase differs from the noise strength $\sigma$ in the test phase. As shown in Fig.~\ref{fig_result2}(c), classification accuracy for test data can be maintained when $\sigma < \sigma^*$. This also suggests that training under strong noise perturbation makes the system more robust.
%

In addition to its noise robustness, the DFA--adjoint method demonstrated robustness against mismatches in system parameters. This insensitivity is crucial when utilizing an actual physical system, as the parameters may be challenging to measure accurately. 
To illustrate this point, we considered a scenario in which measurement errors existed in $\beta$, $\tau_L$, and $\tau_H$. Figure~\ref{fig_result2}(d) depicts the classification accuracy as a function of each mismatch error. 
High accuracy can be maintained even with 200 $\%$ mismatch errors. 

{\it Toward a model-free training approach.---}A drawback of the proposed training approach is its reliance on a model of $\FF$ (e.g., $\pdel{\FF}{\uu}{}$ is used for the control update). However, we found that the approach can still function effectively without an accurate model of $\FF$. High accuracy is maintained even when $\FF$ is replaced by typical activation functions, such as Gaussian or $\tanh$ functions. Additionally, we found that an appropriate design of external control can eliminate the need for prior knowledge of the physical systems in this approach. See SM for further details \cite{Supplementary}.

\begin{figure}[htbp]
\centering\includegraphics[width=8.6cm]{Figure3.eps}
\caption{\label{fig_result2}
(a) $C$ and (b) classification accuracy as a function of $\sigma$. 
For comparison, the results for ELM (without control) are presented.
(c) Accuracy as a function of $\sigma$ in the system trained with a different noise strength $\sigma^*$.
(d) Robustness to parameter mismatch error normalized using a true value, $(p - p_0)/p_0$, where $p$ and $p_0$ represent the parameter value used for training and a true value for $\beta$, $\tau_L$, or $\tau_H$. 
}
\end{figure}

{\it Experiment.---}We validated the effectiveness of the DFA--adjoint method through experiments. The experimental setup is shown in Fig.~\ref{fig_exp}(a). The system was composed of a laser diode (LD), optical fiber for delayed feedback, photodetector (PD1), and two Mach-Zehnder modulators (MZM1 and MZM2) that introduce nonlinearity of $\cos^2(\cdot)$. MZM1 was utilized for signal input and control. For $-\tau_D \le t \le 0$, the laser light was modulated by an input signal encoding data $\xx$, which was generated using an arbitrary waveform generator (AWG). For $t>0$, MZM1 was employed to control the laser light with signal $u(t)$. The optical output from MZM2 was fed back into the MZM2 after traversing a 200 m long optical fiber, with a delay time of 1.022 $\mu$s in the feedback loop. PD1 converted the feedback light to an electrical signal. The feedback gain was adjusted using an optical attenuator (ATT1) and an electrical amplifier (AMP). PD1 and AMP function as a low-pass filter with $\tau_L = 1.59$ ns and a high-pass filter with $\tau_H =  530$ ns, respectively. The bias points for MZM1 and MZM2 were both set as $-\pi/4$. 


\begin{figure}[htbp]
\centering\includegraphics[width=8.6cm]{Figure4.eps}
\caption{\label{fig_exp}
(a) Experimental setup for an optoelectronic delay system. LD: semiconductor laser diode, ISO: optical isolator, MZM: Mach-Zehnder modulator, AMP: electrical amplifier, ATT: optical attenuator, PD: photodetector, OSC: digital oscilloscope, AWG: arbitrary waveform generator. (b) Learning curve. 
}
\end{figure}

To assess the classification performance, we used the MNIST handwritten digit image dataset. 3,000 images were used for training and 500 images were used for testing. Figure~\ref{fig_exp}(b) depicts the learning curve for the system trained with the DFA--adjoint method. The classification accuracy increased to 0.964 at the 20th training epoch, which was comparable to the numerical result shown in Fig.~\ref{fig_result1}(b). 
%
This result shows that the proposed method can turn a real physical system with external disturbances and unknown precise parameters into a computing system.

{\it Summary.---}We proposed the DFA--adjoint method and have experimentally and numerically demonstrated its classification performance for an optoelectronic delay system. The DFA--adjoint method enables robust operation against noise and parameter mismatches. The computational training time can be significantly reduced compared to the conventional backpropagation (adjoint method). The DFA--adjoint method leverages the external control of dynamical systems; thus, it can be applied to any physical system, device, or material without adjustable elements and can unlock their computational potentials, enabling energy-efficient learning suitable for edge devices with limited resources.

%{\it Acknowledgement.---}
This work was supported in part by JSPS KAKENHI (Grant
No. JP20H04255, No. JP22H05198, No. JP23H03467, No. JP19H00868, No. JP20K15185, No. JP22H05195), and JST, CREST Grant No. JPMJCR24R2.
%\begin{acknowledgements}
%

%
%\end{acknowledgements}
\begin{thebibliography}{44}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi
\expandafter\ifx\csname bibnamefont\endcsname\relax
  \def\bibnamefont#1{#1}\fi
\expandafter\ifx\csname bibfnamefont\endcsname\relax
  \def\bibfnamefont#1{#1}\fi
\expandafter\ifx\csname citenamefont\endcsname\relax
  \def\citenamefont#1{#1}\fi
\expandafter\ifx\csname url\endcsname\relax
  \def\url#1{\texttt{#1}}\fi
\expandafter\ifx\csname urlprefix\endcsname\relax\def\urlprefix{URL }\fi
\providecommand{\bibinfo}[2]{#2}
\providecommand{\eprint}[2][]{\url{#2}}

\bibitem[{\citenamefont{Mehonic and Kenyon}(2022)}]{Mehonic:2022wi}
\bibinfo{author}{\bibfnamefont{A.}~\bibnamefont{Mehonic}} \bibnamefont{and}
  \bibinfo{author}{\bibfnamefont{A.~J.} \bibnamefont{Kenyon}},
  \emph{\bibinfo{title}{Brain-inspired computing needs a master plan}},
  \bibinfo{journal}{Nature} \textbf{\bibinfo{volume}{604}},
  \bibinfo{pages}{255} (\bibinfo{year}{2022}).

\bibitem[{\citenamefont{Furber}(2016)}]{Furber_2016}
\bibinfo{author}{\bibfnamefont{S.}~\bibnamefont{Furber}},
  \emph{\bibinfo{title}{Large-scale neuromorphic computing systems}},
  \bibinfo{journal}{Journal of Neural Engineering}
  \textbf{\bibinfo{volume}{13}}, \bibinfo{pages}{051001}
  (\bibinfo{year}{2016}).

\bibitem[{\citenamefont{Nahmias et~al.}(2018)\citenamefont{Nahmias, Shastri,
  Tait, de~Lima, and Prucnal}}]{Nahmias:18}
\bibinfo{author}{\bibfnamefont{M.~A.} \bibnamefont{Nahmias}},
  \bibinfo{author}{\bibfnamefont{B.~J.} \bibnamefont{Shastri}},
  \bibinfo{author}{\bibfnamefont{A.~N.} \bibnamefont{Tait}},
  \bibinfo{author}{\bibfnamefont{T.~F.} \bibnamefont{de~Lima}},
  \bibnamefont{and} \bibinfo{author}{\bibfnamefont{P.~R.}
  \bibnamefont{Prucnal}}, \emph{\bibinfo{title}{Neuromorphic photonics}},
  \bibinfo{journal}{Opt. Photon. News} \textbf{\bibinfo{volume}{29}},
  \bibinfo{pages}{34} (\bibinfo{year}{2018}).

\bibitem[{\citenamefont{Carleo et~al.}(2019)\citenamefont{Carleo, Cirac,
  Cranmer, Daudet, Schuld, Tishby, Vogt-Maranto, and
  Zdeborov\'a}}]{RevModPhys.91.045002}
\bibinfo{author}{\bibfnamefont{G.}~\bibnamefont{Carleo}},
  \bibinfo{author}{\bibfnamefont{I.}~\bibnamefont{Cirac}},
  \bibinfo{author}{\bibfnamefont{K.}~\bibnamefont{Cranmer}},
  \bibinfo{author}{\bibfnamefont{L.}~\bibnamefont{Daudet}},
  \bibinfo{author}{\bibfnamefont{M.}~\bibnamefont{Schuld}},
  \bibinfo{author}{\bibfnamefont{N.}~\bibnamefont{Tishby}},
  \bibinfo{author}{\bibfnamefont{L.}~\bibnamefont{Vogt-Maranto}},
  \bibnamefont{and}
  \bibinfo{author}{\bibfnamefont{L.}~\bibnamefont{Zdeborov\'a}},
  \emph{\bibinfo{title}{Machine learning and the physical sciences}},
  \bibinfo{journal}{Rev. Mod. Phys.} \textbf{\bibinfo{volume}{91}},
  \bibinfo{pages}{045002} (\bibinfo{year}{2019}).

\bibitem[{\citenamefont{Merolla et~al.}(2014)\citenamefont{Merolla, Arthur,
  Alvarez-Icaza, Cassidy, Sawada, Akopyan, Jackson, Imam, Guo, Nakamura
  et~al.}}]{Merolla668}
\bibinfo{author}{\bibfnamefont{P.~A.} \bibnamefont{Merolla}},
  \bibinfo{author}{\bibfnamefont{J.~V.} \bibnamefont{Arthur}},
  \bibinfo{author}{\bibfnamefont{R.}~\bibnamefont{Alvarez-Icaza}},
  \bibinfo{author}{\bibfnamefont{A.~S.} \bibnamefont{Cassidy}},
  \bibinfo{author}{\bibfnamefont{J.}~\bibnamefont{Sawada}},
  \bibinfo{author}{\bibfnamefont{F.}~\bibnamefont{Akopyan}},
  \bibinfo{author}{\bibfnamefont{B.~L.} \bibnamefont{Jackson}},
  \bibinfo{author}{\bibfnamefont{N.}~\bibnamefont{Imam}},
  \bibinfo{author}{\bibfnamefont{C.}~\bibnamefont{Guo}},
  \bibinfo{author}{\bibfnamefont{Y.}~\bibnamefont{Nakamura}},
  \bibnamefont{et~al.}, \emph{\bibinfo{title}{A million spiking-neuron
  integrated circuit with a scalable communication network and interface}},
  \bibinfo{journal}{Science} \textbf{\bibinfo{volume}{345}},
  \bibinfo{pages}{668} (\bibinfo{year}{2014}).

\bibitem[{\citenamefont{Grollier et~al.}(2020)\citenamefont{Grollier, Querlioz,
  Camsari, Everschor-Sitte, Fukami, and Stiles}}]{Grollier:2020aa}
\bibinfo{author}{\bibfnamefont{J.}~\bibnamefont{Grollier}},
  \bibinfo{author}{\bibfnamefont{D.}~\bibnamefont{Querlioz}},
  \bibinfo{author}{\bibfnamefont{K.~Y.} \bibnamefont{Camsari}},
  \bibinfo{author}{\bibfnamefont{K.}~\bibnamefont{Everschor-Sitte}},
  \bibinfo{author}{\bibfnamefont{S.}~\bibnamefont{Fukami}}, \bibnamefont{and}
  \bibinfo{author}{\bibfnamefont{M.~D.} \bibnamefont{Stiles}},
  \emph{\bibinfo{title}{Neuromorphic spintronics}}, \bibinfo{journal}{Nature
  Electronics} \textbf{\bibinfo{volume}{3}}, \bibinfo{pages}{360}
  (\bibinfo{year}{2020}).

\bibitem[{\citenamefont{Markovi{\'c} et~al.}(2020)\citenamefont{Markovi{\'c},
  Mizrahi, Querlioz, and Grollier}}]{Markovic:2020aa}
\bibinfo{author}{\bibfnamefont{D.}~\bibnamefont{Markovi{\'c}}},
  \bibinfo{author}{\bibfnamefont{A.}~\bibnamefont{Mizrahi}},
  \bibinfo{author}{\bibfnamefont{D.}~\bibnamefont{Querlioz}}, \bibnamefont{and}
  \bibinfo{author}{\bibfnamefont{J.}~\bibnamefont{Grollier}},
  \emph{\bibinfo{title}{Physics for neuromorphic computing}},
  \bibinfo{journal}{Nature Reviews Physics} \textbf{\bibinfo{volume}{2}},
  \bibinfo{pages}{499} (\bibinfo{year}{2020}).

\bibitem[{\citenamefont{Bogaerts and Rahim}(2020)}]{9049105}
\bibinfo{author}{\bibfnamefont{W.}~\bibnamefont{Bogaerts}} \bibnamefont{and}
  \bibinfo{author}{\bibfnamefont{A.}~\bibnamefont{Rahim}},
  \emph{\bibinfo{title}{Programmable photonics: An opportunity for an
  accessible large-volume pic ecosystem}}, \bibinfo{journal}{IEEE Journal of
  Selected Topics in Quantum Electronics} \textbf{\bibinfo{volume}{26}},
  \bibinfo{pages}{1} (\bibinfo{year}{2020}).

\bibitem[{\citenamefont{Shastri et~al.}(2021)\citenamefont{Shastri, Tait,
  Ferreira~de Lima, Pernice, Bhaskaran, Wright, and Prucnal}}]{Shastri:2021aa}
\bibinfo{author}{\bibfnamefont{B.~J.} \bibnamefont{Shastri}},
  \bibinfo{author}{\bibfnamefont{A.~N.} \bibnamefont{Tait}},
  \bibinfo{author}{\bibfnamefont{T.}~\bibnamefont{Ferreira~de Lima}},
  \bibinfo{author}{\bibfnamefont{W.~H.~P.} \bibnamefont{Pernice}},
  \bibinfo{author}{\bibfnamefont{H.}~\bibnamefont{Bhaskaran}},
  \bibinfo{author}{\bibfnamefont{C.~D.} \bibnamefont{Wright}},
  \bibnamefont{and} \bibinfo{author}{\bibfnamefont{P.~R.}
  \bibnamefont{Prucnal}}, \emph{\bibinfo{title}{Photonics for artificial
  intelligence and neuromorphic computing}}, \bibinfo{journal}{Nature
  Photonics} \textbf{\bibinfo{volume}{15}}, \bibinfo{pages}{102}
  (\bibinfo{year}{2021}).

\bibitem[{\citenamefont{Nakajima et~al.}(2022)\citenamefont{Nakajima, Inoue,
  Tanaka, Kuniyoshi, Hashimoto, and Nakajima}}]{Nakajima:2022aa}
\bibinfo{author}{\bibfnamefont{M.}~\bibnamefont{Nakajima}},
  \bibinfo{author}{\bibfnamefont{K.}~\bibnamefont{Inoue}},
  \bibinfo{author}{\bibfnamefont{K.}~\bibnamefont{Tanaka}},
  \bibinfo{author}{\bibfnamefont{Y.}~\bibnamefont{Kuniyoshi}},
  \bibinfo{author}{\bibfnamefont{T.}~\bibnamefont{Hashimoto}},
  \bibnamefont{and} \bibinfo{author}{\bibfnamefont{K.}~\bibnamefont{Nakajima}},
  \emph{\bibinfo{title}{Physical deep learning with biologically inspired
  training method: gradient-free approach for physical hardware}},
  \bibinfo{journal}{Nature Communications} \textbf{\bibinfo{volume}{13}},
  \bibinfo{pages}{7847} (\bibinfo{year}{2022}).

\bibitem[{\citenamefont{Sludds et~al.}(2022)\citenamefont{Sludds,
  Bandyopadhyay, Chen, Zhong, Cochrane, Bernstein, Bunandar, Dixon, Hamilton,
  Streshinsky et~al.}}]{Science.abq8271_Delocalized}
\bibinfo{author}{\bibfnamefont{A.}~\bibnamefont{Sludds}},
  \bibinfo{author}{\bibfnamefont{S.}~\bibnamefont{Bandyopadhyay}},
  \bibinfo{author}{\bibfnamefont{Z.}~\bibnamefont{Chen}},
  \bibinfo{author}{\bibfnamefont{Z.}~\bibnamefont{Zhong}},
  \bibinfo{author}{\bibfnamefont{J.}~\bibnamefont{Cochrane}},
  \bibinfo{author}{\bibfnamefont{L.}~\bibnamefont{Bernstein}},
  \bibinfo{author}{\bibfnamefont{D.}~\bibnamefont{Bunandar}},
  \bibinfo{author}{\bibfnamefont{P.~B.} \bibnamefont{Dixon}},
  \bibinfo{author}{\bibfnamefont{S.~A.} \bibnamefont{Hamilton}},
  \bibinfo{author}{\bibfnamefont{M.}~\bibnamefont{Streshinsky}},
  \bibnamefont{et~al.}, \emph{\bibinfo{title}{Delocalized photonic deep
  learning on the internet's edge}}, \bibinfo{journal}{Science}
  \textbf{\bibinfo{volume}{378}}, \bibinfo{pages}{270} (\bibinfo{year}{2022}).

\bibitem[{\citenamefont{Hughes et~al.}(2019)\citenamefont{Hughes, Williamson,
  Minkov, and Fan}}]{Sciadv.aay6946_wavephysics}
\bibinfo{author}{\bibfnamefont{T.~W.} \bibnamefont{Hughes}},
  \bibinfo{author}{\bibfnamefont{I.~A.~D.} \bibnamefont{Williamson}},
  \bibinfo{author}{\bibfnamefont{M.}~\bibnamefont{Minkov}}, \bibnamefont{and}
  \bibinfo{author}{\bibfnamefont{S.}~\bibnamefont{Fan}},
  \emph{\bibinfo{title}{Wave physics as an analog recurrent neural network}},
  \bibinfo{journal}{Science Advances} \textbf{\bibinfo{volume}{5}},
  \bibinfo{pages}{eaay6946} (\bibinfo{year}{2019}).

\bibitem[{\citenamefont{Nakane et~al.}(2021)\citenamefont{Nakane, Hirose, and
  Tanaka}}]{PhysRevResearch.3.033243}
\bibinfo{author}{\bibfnamefont{R.}~\bibnamefont{Nakane}},
  \bibinfo{author}{\bibfnamefont{A.}~\bibnamefont{Hirose}}, \bibnamefont{and}
  \bibinfo{author}{\bibfnamefont{G.}~\bibnamefont{Tanaka}},
  \emph{\bibinfo{title}{Spin waves propagating through a stripe magnetic domain
  structure and their applications to reservoir computing}},
  \bibinfo{journal}{Phys. Rev. Res.} \textbf{\bibinfo{volume}{3}},
  \bibinfo{pages}{033243} (\bibinfo{year}{2021}).

\bibitem[{\citenamefont{Zhang et~al.}(2020)\citenamefont{Zhang, Gao, Tang, Yao,
  Yu, Chang, Yoo, Qian, and Wu}}]{Zhang:2020aa}
\bibinfo{author}{\bibfnamefont{W.}~\bibnamefont{Zhang}},
  \bibinfo{author}{\bibfnamefont{B.}~\bibnamefont{Gao}},
  \bibinfo{author}{\bibfnamefont{J.}~\bibnamefont{Tang}},
  \bibinfo{author}{\bibfnamefont{P.}~\bibnamefont{Yao}},
  \bibinfo{author}{\bibfnamefont{S.}~\bibnamefont{Yu}},
  \bibinfo{author}{\bibfnamefont{M.-F.} \bibnamefont{Chang}},
  \bibinfo{author}{\bibfnamefont{H.-J.} \bibnamefont{Yoo}},
  \bibinfo{author}{\bibfnamefont{H.}~\bibnamefont{Qian}}, \bibnamefont{and}
  \bibinfo{author}{\bibfnamefont{H.}~\bibnamefont{Wu}},
  \emph{\bibinfo{title}{Neuro-inspired computing chips}},
  \bibinfo{journal}{Nature Electronics} \textbf{\bibinfo{volume}{3}},
  \bibinfo{pages}{371} (\bibinfo{year}{2020}).

\bibitem[{\citenamefont{Tanaka et~al.}(2019)\citenamefont{Tanaka, Yamane,
  H{\'e}roux, Nakane, Kanazawa, Takeda, Numata, Nakano, and
  Hirose}}]{TANAKA2019100}
\bibinfo{author}{\bibfnamefont{G.}~\bibnamefont{Tanaka}},
  \bibinfo{author}{\bibfnamefont{T.}~\bibnamefont{Yamane}},
  \bibinfo{author}{\bibfnamefont{J.~B.} \bibnamefont{H{\'e}roux}},
  \bibinfo{author}{\bibfnamefont{R.}~\bibnamefont{Nakane}},
  \bibinfo{author}{\bibfnamefont{N.}~\bibnamefont{Kanazawa}},
  \bibinfo{author}{\bibfnamefont{S.}~\bibnamefont{Takeda}},
  \bibinfo{author}{\bibfnamefont{H.}~\bibnamefont{Numata}},
  \bibinfo{author}{\bibfnamefont{D.}~\bibnamefont{Nakano}}, \bibnamefont{and}
  \bibinfo{author}{\bibfnamefont{A.}~\bibnamefont{Hirose}},
  \emph{\bibinfo{title}{Recent advances in physical reservoir computing: A
  review}}, \bibinfo{journal}{Neural Networks} \textbf{\bibinfo{volume}{115}},
  \bibinfo{pages}{100} (\bibinfo{year}{2019}).

\bibitem[{\citenamefont{Wright et~al.}(2022)\citenamefont{Wright, Onodera,
  Stein, Wang, Schachter, Hu, and McMahon}}]{Wright:2022aa}
\bibinfo{author}{\bibfnamefont{L.~G.} \bibnamefont{Wright}},
  \bibinfo{author}{\bibfnamefont{T.}~\bibnamefont{Onodera}},
  \bibinfo{author}{\bibfnamefont{M.~M.} \bibnamefont{Stein}},
  \bibinfo{author}{\bibfnamefont{T.}~\bibnamefont{Wang}},
  \bibinfo{author}{\bibfnamefont{D.~T.} \bibnamefont{Schachter}},
  \bibinfo{author}{\bibfnamefont{Z.}~\bibnamefont{Hu}}, \bibnamefont{and}
  \bibinfo{author}{\bibfnamefont{P.~L.} \bibnamefont{McMahon}},
  \emph{\bibinfo{title}{Deep physical neural networks trained with
  backpropagation}}, \bibinfo{journal}{Nature} \textbf{\bibinfo{volume}{601}},
  \bibinfo{pages}{549} (\bibinfo{year}{2022}).

\bibitem[{\citenamefont{Momeni et~al.}(2023)\citenamefont{Momeni, Rahmani,
  Mall{\'e}jac, del Hougne, and Fleury}}]{doi:10.1126/science.adi8474}
\bibinfo{author}{\bibfnamefont{A.}~\bibnamefont{Momeni}},
  \bibinfo{author}{\bibfnamefont{B.}~\bibnamefont{Rahmani}},
  \bibinfo{author}{\bibfnamefont{M.}~\bibnamefont{Mall{\'e}jac}},
  \bibinfo{author}{\bibfnamefont{P.}~\bibnamefont{del Hougne}},
  \bibnamefont{and} \bibinfo{author}{\bibfnamefont{R.}~\bibnamefont{Fleury}},
  \emph{\bibinfo{title}{Backpropagation-free training of deep physical neural
  networks}}, \bibinfo{journal}{Science} \textbf{\bibinfo{volume}{382}},
  \bibinfo{pages}{1297} (\bibinfo{year}{2023}).

\bibitem[{\citenamefont{Momeni et~al.}(2024)\citenamefont{Momeni, Rahmani,
  Scellier, Wright, McMahon, Wanjura, Li, Skalli, Berloff, Onodera
  et~al.}}]{momeni2024trainingphysicalneuralnetworks}
\bibinfo{author}{\bibfnamefont{A.}~\bibnamefont{Momeni}},
  \bibinfo{author}{\bibfnamefont{B.}~\bibnamefont{Rahmani}},
  \bibinfo{author}{\bibfnamefont{B.}~\bibnamefont{Scellier}},
  \bibinfo{author}{\bibfnamefont{L.~G.} \bibnamefont{Wright}},
  \bibinfo{author}{\bibfnamefont{P.~L.} \bibnamefont{McMahon}},
  \bibinfo{author}{\bibfnamefont{C.~C.} \bibnamefont{Wanjura}},
  \bibinfo{author}{\bibfnamefont{Y.}~\bibnamefont{Li}},
  \bibinfo{author}{\bibfnamefont{A.}~\bibnamefont{Skalli}},
  \bibinfo{author}{\bibfnamefont{N.~G.} \bibnamefont{Berloff}},
  \bibinfo{author}{\bibfnamefont{T.}~\bibnamefont{Onodera}},
  \bibnamefont{et~al.}, \emph{\bibinfo{title}{Training of physical neural
  networks}}, \bibinfo{journal}{arXiv.2406.03372}  (\bibinfo{year}{2024}).

\bibitem[{\citenamefont{Jaeger and Haas}(2004)}]{Jaeger78}
\bibinfo{author}{\bibfnamefont{H.}~\bibnamefont{Jaeger}} \bibnamefont{and}
  \bibinfo{author}{\bibfnamefont{H.}~\bibnamefont{Haas}},
  \emph{\bibinfo{title}{Harnessing nonlinearity: Predicting chaotic systems and
  saving energy in wireless communication}}, \bibinfo{journal}{Science}
  \textbf{\bibinfo{volume}{304}}, \bibinfo{pages}{78} (\bibinfo{year}{2004}).

\bibitem[{\citenamefont{Maass et~al.}(2002)\citenamefont{Maass,
  Natschl{\"a}ger, and Markram}}]{Maass:2002}
\bibinfo{author}{\bibfnamefont{W.}~\bibnamefont{Maass}},
  \bibinfo{author}{\bibfnamefont{T.}~\bibnamefont{Natschl{\"a}ger}},
  \bibnamefont{and} \bibinfo{author}{\bibfnamefont{H.}~\bibnamefont{Markram}},
  \emph{\bibinfo{title}{{Real-Time Computing Without Stable States: A New
  Framework for Neural Computation Based on Perturbations}}},
  \bibinfo{journal}{Neural Computation} \textbf{\bibinfo{volume}{14}},
  \bibinfo{pages}{2531} (\bibinfo{year}{2002}).

\bibitem[{\citenamefont{Verstraeten et~al.}(2007)\citenamefont{Verstraeten,
  Schrauwen, D'Haene, and Stroobandt}}]{VERSTRAETEN2007391}
\bibinfo{author}{\bibfnamefont{D.}~\bibnamefont{Verstraeten}},
  \bibinfo{author}{\bibfnamefont{B.}~\bibnamefont{Schrauwen}},
  \bibinfo{author}{\bibfnamefont{M.}~\bibnamefont{D'Haene}}, \bibnamefont{and}
  \bibinfo{author}{\bibfnamefont{D.}~\bibnamefont{Stroobandt}},
  \emph{\bibinfo{title}{An experimental unification of reservoir computing
  methods}}, \bibinfo{journal}{Neural Networks} \textbf{\bibinfo{volume}{20}},
  \bibinfo{pages}{391} (\bibinfo{year}{2007}).

\bibitem[{\citenamefont{Appeltant et~al.}(2011)\citenamefont{Appeltant,
  Soriano, Van~der Sande, Danckaert, Massar, Dambre, Schrauwen, Mirasso, and
  Fischer}}]{Appeltant:2011ab}
\bibinfo{author}{\bibfnamefont{L.}~\bibnamefont{Appeltant}},
  \bibinfo{author}{\bibfnamefont{M.~C.} \bibnamefont{Soriano}},
  \bibinfo{author}{\bibfnamefont{G.}~\bibnamefont{Van~der Sande}},
  \bibinfo{author}{\bibfnamefont{J.}~\bibnamefont{Danckaert}},
  \bibinfo{author}{\bibfnamefont{S.}~\bibnamefont{Massar}},
  \bibinfo{author}{\bibfnamefont{J.}~\bibnamefont{Dambre}},
  \bibinfo{author}{\bibfnamefont{B.}~\bibnamefont{Schrauwen}},
  \bibinfo{author}{\bibfnamefont{C.~R.} \bibnamefont{Mirasso}},
  \bibnamefont{and} \bibinfo{author}{\bibfnamefont{I.}~\bibnamefont{Fischer}},
  \emph{\bibinfo{title}{Information processing using a single dynamical node as
  complex system}}, \bibinfo{journal}{Nature Communications}
  \textbf{\bibinfo{volume}{2}}, \bibinfo{pages}{468} (\bibinfo{year}{2011}).

\bibitem[{\citenamefont{Liang et~al.}(2024)\citenamefont{Liang, Tang, Zhong,
  Gao, Qian, and Wu}}]{Liang:2024aa}
\bibinfo{author}{\bibfnamefont{X.}~\bibnamefont{Liang}},
  \bibinfo{author}{\bibfnamefont{J.}~\bibnamefont{Tang}},
  \bibinfo{author}{\bibfnamefont{Y.}~\bibnamefont{Zhong}},
  \bibinfo{author}{\bibfnamefont{B.}~\bibnamefont{Gao}},
  \bibinfo{author}{\bibfnamefont{H.}~\bibnamefont{Qian}}, \bibnamefont{and}
  \bibinfo{author}{\bibfnamefont{H.}~\bibnamefont{Wu}},
  \emph{\bibinfo{title}{Physical reservoir computing with emerging
  electronics}}, \bibinfo{journal}{Nature Electronics}
  \textbf{\bibinfo{volume}{7}}, \bibinfo{pages}{193} (\bibinfo{year}{2024}).

\bibitem[{\citenamefont{Huang et~al.}(2006)\citenamefont{Huang, Zhu, and
  Siew}}]{HUANG2006489}
\bibinfo{author}{\bibfnamefont{G.-B.} \bibnamefont{Huang}},
  \bibinfo{author}{\bibfnamefont{Q.-Y.} \bibnamefont{Zhu}}, \bibnamefont{and}
  \bibinfo{author}{\bibfnamefont{C.-K.} \bibnamefont{Siew}},
  \emph{\bibinfo{title}{Extreme learning machine: Theory and applications}},
  \bibinfo{journal}{Neurocomputing} \textbf{\bibinfo{volume}{70}},
  \bibinfo{pages}{489} (\bibinfo{year}{2006}), \bibinfo{note}{neural Networks}.

\bibitem[{\citenamefont{Ort{\'\i}n et~al.}(2015)\citenamefont{Ort{\'\i}n,
  Soriano, Pesquera, Brunner, San-Mart{\'\i}n, Fischer, Mirasso, and
  Guti{\'e}rrez}}]{Ortin:2015aa}
\bibinfo{author}{\bibfnamefont{S.}~\bibnamefont{Ort{\'\i}n}},
  \bibinfo{author}{\bibfnamefont{M.~C.} \bibnamefont{Soriano}},
  \bibinfo{author}{\bibfnamefont{L.}~\bibnamefont{Pesquera}},
  \bibinfo{author}{\bibfnamefont{D.}~\bibnamefont{Brunner}},
  \bibinfo{author}{\bibfnamefont{D.}~\bibnamefont{San-Mart{\'\i}n}},
  \bibinfo{author}{\bibfnamefont{I.}~\bibnamefont{Fischer}},
  \bibinfo{author}{\bibfnamefont{C.~R.} \bibnamefont{Mirasso}},
  \bibnamefont{and} \bibinfo{author}{\bibfnamefont{J.~M.}
  \bibnamefont{Guti{\'e}rrez}}, \emph{\bibinfo{title}{A unified framework for
  reservoir computing and extreme learning machines based on a single
  time-delayed neuron}}, \bibinfo{journal}{Scientific Reports}
  \textbf{\bibinfo{volume}{5}}, \bibinfo{pages}{14945} (\bibinfo{year}{2015}).

\bibitem[{\citenamefont{Dillavou et~al.}(2022)\citenamefont{Dillavou, Stern,
  Liu, and Durian}}]{PhysRevApplied.18.014040}
\bibinfo{author}{\bibfnamefont{S.}~\bibnamefont{Dillavou}},
  \bibinfo{author}{\bibfnamefont{M.}~\bibnamefont{Stern}},
  \bibinfo{author}{\bibfnamefont{A.~J.} \bibnamefont{Liu}}, \bibnamefont{and}
  \bibinfo{author}{\bibfnamefont{D.~J.} \bibnamefont{Durian}},
  \emph{\bibinfo{title}{Demonstration of decentralized physics-driven
  learning}}, \bibinfo{journal}{Phys. Rev. Appl.}
  \textbf{\bibinfo{volume}{18}}, \bibinfo{pages}{014040}
  (\bibinfo{year}{2022}).

\bibitem[{\citenamefont{Shen et~al.}(2017)\citenamefont{Shen, Harris, Skirlo,
  Prabhu, Baehr-Jones, Hochberg, Sun, Zhao, Larochelle, Englund
  et~al.}}]{Shen:2017aa}
\bibinfo{author}{\bibfnamefont{Y.}~\bibnamefont{Shen}},
  \bibinfo{author}{\bibfnamefont{N.~C.} \bibnamefont{Harris}},
  \bibinfo{author}{\bibfnamefont{S.}~\bibnamefont{Skirlo}},
  \bibinfo{author}{\bibfnamefont{M.}~\bibnamefont{Prabhu}},
  \bibinfo{author}{\bibfnamefont{T.}~\bibnamefont{Baehr-Jones}},
  \bibinfo{author}{\bibfnamefont{M.}~\bibnamefont{Hochberg}},
  \bibinfo{author}{\bibfnamefont{X.}~\bibnamefont{Sun}},
  \bibinfo{author}{\bibfnamefont{S.}~\bibnamefont{Zhao}},
  \bibinfo{author}{\bibfnamefont{H.}~\bibnamefont{Larochelle}},
  \bibinfo{author}{\bibfnamefont{D.}~\bibnamefont{Englund}},
  \bibnamefont{et~al.}, \emph{\bibinfo{title}{Deep learning with coherent
  nanophotonic circuits}}, \bibinfo{journal}{Nature Photonics}
  \textbf{\bibinfo{volume}{11}}, \bibinfo{pages}{441} (\bibinfo{year}{2017}).

\bibitem[{\citenamefont{Feldmann et~al.}(2021)\citenamefont{Feldmann,
  Youngblood, Karpov, Gehring, Li, Stappers, Le~Gallo, Fu, Lukashchuk, Raja
  et~al.}}]{Feldmann:2021aa}
\bibinfo{author}{\bibfnamefont{J.}~\bibnamefont{Feldmann}},
  \bibinfo{author}{\bibfnamefont{N.}~\bibnamefont{Youngblood}},
  \bibinfo{author}{\bibfnamefont{M.}~\bibnamefont{Karpov}},
  \bibinfo{author}{\bibfnamefont{H.}~\bibnamefont{Gehring}},
  \bibinfo{author}{\bibfnamefont{X.}~\bibnamefont{Li}},
  \bibinfo{author}{\bibfnamefont{M.}~\bibnamefont{Stappers}},
  \bibinfo{author}{\bibfnamefont{M.}~\bibnamefont{Le~Gallo}},
  \bibinfo{author}{\bibfnamefont{X.}~\bibnamefont{Fu}},
  \bibinfo{author}{\bibfnamefont{A.}~\bibnamefont{Lukashchuk}},
  \bibinfo{author}{\bibfnamefont{A.~S.} \bibnamefont{Raja}},
  \bibnamefont{et~al.}, \emph{\bibinfo{title}{Parallel convolutional processing
  using an integrated photonic tensor core}}, \bibinfo{journal}{Nature}
  \textbf{\bibinfo{volume}{589}}, \bibinfo{pages}{52} (\bibinfo{year}{2021}).

\bibitem[{\citenamefont{Furuhata et~al.}(2021)\citenamefont{Furuhata, Niiyama,
  and Sunada}}]{PhysRevApplied.15.034092}
\bibinfo{author}{\bibfnamefont{G.}~\bibnamefont{Furuhata}},
  \bibinfo{author}{\bibfnamefont{T.}~\bibnamefont{Niiyama}}, \bibnamefont{and}
  \bibinfo{author}{\bibfnamefont{S.}~\bibnamefont{Sunada}},
  \emph{\bibinfo{title}{Physical deep learning based on optimal control of
  dynamical systems}}, \bibinfo{journal}{Phys. Rev. Applied}
  \textbf{\bibinfo{volume}{15}}, \bibinfo{pages}{034092}
  (\bibinfo{year}{2021}).

\bibitem[{\citenamefont{Chen et~al.}(2018)\citenamefont{Chen, Rubanova,
  Bettencourt, and Duvenaud}}]{NEURIPS2018_69386f6b}
\bibinfo{author}{\bibfnamefont{R.~T.~Q.} \bibnamefont{Chen}},
  \bibinfo{author}{\bibfnamefont{Y.}~\bibnamefont{Rubanova}},
  \bibinfo{author}{\bibfnamefont{J.}~\bibnamefont{Bettencourt}},
  \bibnamefont{and} \bibinfo{author}{\bibfnamefont{D.~K.}
  \bibnamefont{Duvenaud}}, in \emph{\bibinfo{booktitle}{Advances in Neural
  Information Processing Systems}}, edited by
  \bibinfo{editor}{\bibfnamefont{S.}~\bibnamefont{Bengio}},
  \bibinfo{editor}{\bibfnamefont{H.}~\bibnamefont{Wallach}},
  \bibinfo{editor}{\bibfnamefont{H.}~\bibnamefont{Larochelle}},
  \bibinfo{editor}{\bibfnamefont{K.}~\bibnamefont{Grauman}},
  \bibinfo{editor}{\bibfnamefont{N.}~\bibnamefont{Cesa-Bianchi}},
  \bibnamefont{and} \bibinfo{editor}{\bibfnamefont{R.}~\bibnamefont{Garnett}}
  (\bibinfo{publisher}{Curran Associates, Inc.}, \bibinfo{year}{2018}),
  vol.~\bibinfo{volume}{31}, pp. \bibinfo{pages}{\bibinfo{publisher}{Curran
  Associates, Inc.}}

\bibitem[{\citenamefont{Jaeger et~al.}(2023)\citenamefont{Jaeger, Noheda, and
  van~der Wiel}}]{Jaeger:2023uw}
\bibinfo{author}{\bibfnamefont{H.}~\bibnamefont{Jaeger}},
  \bibinfo{author}{\bibfnamefont{B.}~\bibnamefont{Noheda}}, \bibnamefont{and}
  \bibinfo{author}{\bibfnamefont{W.~G.} \bibnamefont{van~der Wiel}},
  \emph{\bibinfo{title}{Toward a formal theory for computing machines made out
  of whatever physics offers}}, \bibinfo{journal}{Nature Communications}
  \textbf{\bibinfo{volume}{14}}, \bibinfo{pages}{4911} (\bibinfo{year}{2023}).

\bibitem[{\citenamefont{N{\o}kland}(2016)}]{nkland2016direct}
\bibinfo{author}{\bibfnamefont{A.}~\bibnamefont{N{\o}kland}},
  \emph{\bibinfo{title}{Direct feedback alignment provides learning in deep
  neural networks}}, \bibinfo{journal}{Advances in Neural Information
  Processing Systems 29 (NIPS 2016)}  (\bibinfo{year}{2016}).

\bibitem[{\citenamefont{Kirk}(2004)}]{kirk2004optimal}
\bibinfo{author}{\bibfnamefont{D.~E.} \bibnamefont{Kirk}},
  \emph{\bibinfo{title}{Optimal control theory: an introduction}}
  (\bibinfo{publisher}{Dover Publications, Inc., Mineola, NY}, \bibinfo{year}{2004}).

\bibitem[{Sup()}]{Supplementary}
\bibinfo{journal}{See Supplemental Material for the details on the derivation
  of Eq. (2), the DFA-Adjoint method, the time-dependent random matrix, the model-free approach, numerical simulations, and experimental setup.}

\bibitem[{\citenamefont{Launay et~al.}(2020)\citenamefont{Launay, Poli,
  Boniface, and Krzakala}}]{NEURIPS2020_69d1fc78}
\bibinfo{author}{\bibfnamefont{J.}~\bibnamefont{Launay}},
  \bibinfo{author}{\bibfnamefont{I.}~\bibnamefont{Poli}},
  \bibinfo{author}{\bibfnamefont{F.}~\bibnamefont{Boniface}}, \bibnamefont{and}
  \bibinfo{author}{\bibfnamefont{F.}~\bibnamefont{Krzakala}}, in
  \emph{\bibinfo{booktitle}{Advances in Neural Information Processing
  Systems}}, edited by
  \bibinfo{editor}{\bibfnamefont{H.}~\bibnamefont{Larochelle}},
  \bibinfo{editor}{\bibfnamefont{M.}~\bibnamefont{Ranzato}},
  \bibinfo{editor}{\bibfnamefont{R.}~\bibnamefont{Hadsell}},
  \bibinfo{editor}{\bibfnamefont{M.}~\bibnamefont{Balcan}}, \bibnamefont{and}
  \bibinfo{editor}{\bibfnamefont{H.}~\bibnamefont{Lin}}
  (\bibinfo{publisher}{Curran Associates, Inc.}, \bibinfo{year}{2020}),
  vol.~\bibinfo{volume}{33}, p. \bibinfo{pages}{9346}.

\bibitem[{\citenamefont{Liu and
  Theodorou}(2019)}]{DBLP:journals/corr/abs-1908-10920}
\bibinfo{author}{\bibfnamefont{G.}~\bibnamefont{Liu}} \bibnamefont{and}
  \bibinfo{author}{\bibfnamefont{E.~A.} \bibnamefont{Theodorou}},
  \emph{\bibinfo{title}{Deep learning theory review: An optimal control and
  dynamical systems perspective}}, \bibinfo{journal}{CoRR}
  \textbf{\bibinfo{volume}{abs/1908.10920}} (\bibinfo{year}{2019}).

\bibitem[{\citenamefont{Poole et~al.}(2016)\citenamefont{Poole, Lahiri, Raghu,
  Sohl-Dickstein, and Ganguli}}]{NIPS2016_14851003}
\bibinfo{author}{\bibfnamefont{B.}~\bibnamefont{Poole}},
  \bibinfo{author}{\bibfnamefont{S.}~\bibnamefont{Lahiri}},
  \bibinfo{author}{\bibfnamefont{M.}~\bibnamefont{Raghu}},
  \bibinfo{author}{\bibfnamefont{J.}~\bibnamefont{Sohl-Dickstein}},
  \bibnamefont{and} \bibinfo{author}{\bibfnamefont{S.}~\bibnamefont{Ganguli}},
  in \emph{\bibinfo{booktitle}{Advances in Neural Information Processing
  Systems}}, edited by \bibinfo{editor}{\bibfnamefont{D.}~\bibnamefont{Lee}},
  \bibinfo{editor}{\bibfnamefont{M.}~\bibnamefont{Sugiyama}},
  \bibinfo{editor}{\bibfnamefont{U.}~\bibnamefont{Luxburg}},
  \bibinfo{editor}{\bibfnamefont{I.}~\bibnamefont{Guyon}}, \bibnamefont{and}
  \bibinfo{editor}{\bibfnamefont{R.}~\bibnamefont{Garnett}}
  (\bibinfo{publisher}{Curran Associates, Inc.}, \bibinfo{year}{2016}),
  vol.~\bibinfo{volume}{29}, pp. \bibinfo{pages}{\bibinfo{publisher}{Curran
  Associates, Inc.}}

\bibitem[{\citenamefont{Murphy et~al.}(2010)\citenamefont{Murphy, Cohen,
  Ravoori, Schmitt, Setty, Sorrentino, Williams, Ott, and
  Roy}}]{murphy2010complex}
\bibinfo{author}{\bibfnamefont{T.~E.} \bibnamefont{Murphy}},
  \bibinfo{author}{\bibfnamefont{A.~B.} \bibnamefont{Cohen}},
  \bibinfo{author}{\bibfnamefont{B.}~\bibnamefont{Ravoori}},
  \bibinfo{author}{\bibfnamefont{K.~R.} \bibnamefont{Schmitt}},
  \bibinfo{author}{\bibfnamefont{A.~V.} \bibnamefont{Setty}},
  \bibinfo{author}{\bibfnamefont{F.}~\bibnamefont{Sorrentino}},
  \bibinfo{author}{\bibfnamefont{C.~R.} \bibnamefont{Williams}},
  \bibinfo{author}{\bibfnamefont{E.}~\bibnamefont{Ott}}, \bibnamefont{and}
  \bibinfo{author}{\bibfnamefont{R.}~\bibnamefont{Roy}},
  \emph{\bibinfo{title}{Complex dynamics and synchronization of
  delayed-feedback nonlinear oscillators}}, \bibinfo{journal}{Philosophical
  Transactions of the Royal Society A: Mathematical, Physical and Engineering
  Sciences} \textbf{\bibinfo{volume}{368}}, \bibinfo{pages}{343}
  (\bibinfo{year}{2010}).

\bibitem[{\citenamefont{LeCun et~al.}()\citenamefont{LeCun, Cortes, and
  Burges}}]{MNIST_LeCun_url}
\bibinfo{author}{\bibfnamefont{Y.}~\bibnamefont{LeCun}},
  \bibinfo{author}{\bibfnamefont{C.}~\bibnamefont{Cortes}}, \bibnamefont{and}
  \bibinfo{author}{\bibfnamefont{C.~J.~C.} \bibnamefont{Burges}},
  \emph{\bibinfo{title}{The mnist database of handwritten digits}},
 \bibinfo{type}{http://yann.lecun.com/exdb/mnist/}.

\bibitem[{\citenamefont{Kingma and Ba}(2014)}]{kingma2014adam}
\bibinfo{author}{\bibfnamefont{D.~P.} \bibnamefont{Kingma}} \bibnamefont{and}
  \bibinfo{author}{\bibfnamefont{J.}~\bibnamefont{Ba}},
  \emph{\bibinfo{title}{Adam: A method for stochastic optimization}},
  \bibinfo{journal}{arXiv preprint arXiv:1412.6980}  (\bibinfo{year}{2014}).

\bibitem[{\citenamefont{Lillicrap et~al.}(2016)\citenamefont{Lillicrap,
  Cownden, Tweed, and Akerman}}]{Lillicrap:2016aa}
\bibinfo{author}{\bibfnamefont{T.~P.} \bibnamefont{Lillicrap}},
  \bibinfo{author}{\bibfnamefont{D.}~\bibnamefont{Cownden}},
  \bibinfo{author}{\bibfnamefont{D.~B.} \bibnamefont{Tweed}}, \bibnamefont{and}
  \bibinfo{author}{\bibfnamefont{C.~J.} \bibnamefont{Akerman}},
  \emph{\bibinfo{title}{Random synaptic feedback weights support error
  backpropagation for deep learning}}, \bibinfo{journal}{Nature Communications}
  \textbf{\bibinfo{volume}{7}}, \bibinfo{pages}{13276} (\bibinfo{year}{2016}).

\bibitem[{\citenamefont{Xiao et~al.}(2017)\citenamefont{Xiao, Rasul, and
  Vollgraf}}]{arXiv.1708.07747}
\bibinfo{author}{\bibfnamefont{H.}~\bibnamefont{Xiao}},
  \bibinfo{author}{\bibfnamefont{K.}~\bibnamefont{Rasul}}, \bibnamefont{and}
  \bibinfo{author}{\bibfnamefont{R.}~\bibnamefont{Vollgraf}},
  \emph{\bibinfo{title}{Fashion-mnist: a novel image dataset for benchmarking
  machine learning algorithms}}, \bibinfo{howpublished}{arxiv.1708.07747}
  (\bibinfo{year}{2017}).

\bibitem[{\citenamefont{Krizhevsky}(2009)}]{Krizhevsky09learningmultiple}
\bibinfo{author}{\bibfnamefont{A.}~\bibnamefont{Krizhevsky}},
  \bibinfo{type}{https://www.cs.toronto.edu/~kriz/cifar.html}
  (\bibinfo{year}{2009}).

\bibitem[{\citenamefont{Hermans et~al.}(2015)\citenamefont{Hermans, Soriano,
  Dambre, Bienstman, and Fischer}}]{JMLR:v16:hermans}
\bibinfo{author}{\bibfnamefont{M.}~\bibnamefont{Hermans}},
  \bibinfo{author}{\bibfnamefont{M.~C.} \bibnamefont{Soriano}},
  \bibinfo{author}{\bibfnamefont{J.}~\bibnamefont{Dambre}},
  \bibinfo{author}{\bibfnamefont{P.}~\bibnamefont{Bienstman}},
  \bibnamefont{and} \bibinfo{author}{\bibfnamefont{I.}~\bibnamefont{Fischer}},
  \emph{\bibinfo{title}{Photonic delay systems as machine learning
  implementations}}, \bibinfo{journal}{Journal of Machine Learning Research}
  \textbf{\bibinfo{volume}{16}}, \bibinfo{pages}{2081} (\bibinfo{year}{2015}).

\end{thebibliography}


%\bibliographystyle{apsrev}
%\bibliography{apsrev.bst}
%\bibliography{/Users/sunada/Dropbox/My_Refs_db}
%\bibliography{ms_dfa-adjoint_ver9.bbl}
%\begin{thebibliography}{99}
%
%\bibitem{Markovic2020}
%D. Markovi$\acute{c}$, A. Mizrahi, D. Querlioz, and J. Grollier, 
%Physics for neuromorphic computing,
%Nature Reviews Physics {\bf 2}, 499 (2020).
%
%\bibitem{Rabinovich2006}
%M. I. Rabinovich, P. Varona, A. I. Selverston, and H. D. I. Abarbanel, 
%Dynamical principles in neuroscience,
%Rev. Mod. Phys. {\bf 78}, 1213 (2006).
%\end{thebibliography}

\end{document}
