\input{figures/tail_size}

\input{figures/tail_gain}

\input{figures/mean_size_temp}
\section{Results and Discussion}
Figure \ref{fig:tail_size} illustrates how the tail of the conformal set size distribution (quantiles at 0.9, 0.95, and 0.975) varies with the inverse temperature $1/\tau$ across the three datasets introduced in Section \ref{sec:datasets} for different $\alpha$ values. Each curve includes a red dot marking $1/\tau_*$, our estimated temperature that minimizes the tail size. To validate this estimation method, a green dot indicates the true optimal temperature, $1/\tau_{opt}$, whenever it significantly differs from our estimate. Across all selected quantiles, $\alpha$ values, and datasets, the tail size obtained with $\tau_*$ was at most $0.775$ larger than the one achieved with $\tau_{opt}$, underscoring the robustness of our method. The figure also reveals that the sensitivity of tail set size to deviations from $1/\tau_{opt}$ increases as $\alpha$ decreases. Specifically, for lower $\alpha$ values, deviations from the optimal temperature lead to more pronounced tail size increases. This highlights the importance of precise temperature tuning for applications demanding higher coverage guarantees.

\input{figures/mean_loss}

Figure \ref{fig:tail_gain} demonstrates the impact of $\alpha$ on tail size reduction, comparing it against the typical untuned value of $1/\tau=100$ commonly used in VLMs. Notably, the reduction in tail size is most significant for smaller $\alpha$, aligning with the findings in Figure~\ref{fig:tail_size}. For Kinetics400, this reduction reaches as much as 77 classes at the highest quantile and lowest $\alpha$. As $\alpha$ increases, the tail size reduction diminishes, reflecting a distribution shift toward smaller set sizes.
However, this reduction in tail size comes at the cost of an increased average set size. Figure \ref{fig:mean_size} shows that average set sizes decrease as $1/\tau$ increases, eventually plateauing. Unfortunately, the temperatures $\tau_*$ that minimize the tail size (marked in red in Figure \ref{fig:mean_size}) occur before this plateau, leading to a higher average set size compared to the typical $1/\tau=100$ setting. This trade-off is visualized in Figure \ref{fig:mean_loss}, following a similar trend as the tail size reduction shown in Figure \ref{fig:tail_gain}.

The bottom portion of Figure \ref{fig:intro_set_sizes} illustrates how the conformal set size distribution changes when transitioning from $1/\tau=100$ to $1/\tau=1/\tau_*$. Adjusting $1/\tau$ influences the distribution of conformal set sizes in two ways:
\begin{enumerate}
    \item \textbf{Tail Shift} The distribution tail shifts, with the optimal tail size depending on $\alpha$ and the dataset, lying at the lower end of the $1/\tau$ range. Deviations from $1/\tau_{opt}$ increase the tail size more rapidly for $1/\tau > 1/\tau_{opt}$ than for $1/\tau < 1/\tau_{opt}$.
    \newpage
    \item \textbf{Average Shift} The average set size decreases sharply with increasing $1/\tau$ until reaching a plateau or minimum, beyond which it may increase slightly at a slower rate.
\end{enumerate}


As these two effects have different optimal $1/\tau$ values for a given $\alpha$, it is impossible to simultaneously optimize both. A compromise temperature can be chosen based on task-specific priorities to balance the trade-offs effectively.

To ensure the generality of our findings, we repeated the experiments with CLIP models with alternative visual encoders, including ViT-L/14, ViT-B/32, ResNet101, and ResNet50. Figure \ref{fig:other_archs} shows the results for $\alpha=0.03$, demonstrating that our temperature tuning method significantly reduces tail set sizes across both ViT-based and CNN-based architectures.

\input{figures/other_archs}