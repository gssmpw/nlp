\section{Related Work}

\subsection{Conformal Predictions}
Providing reliable confidence estimates for predictions made by deep learning models is essential in many applications. CP offers a solution by producing sets of potential output classes for a given input, with guarantees—under mild distributional assumptions—that the true class will be included in the set with a specified probability. 

The Least Ambiguous set-valued Classifier~(LAC)~\cite{vovk2005algorithmic,sadinle2019least,lei2015conformal} computes a non conformity score for each sample $i$ inside a calibration set---which differs from the training, validation, and test sets---as: 
\begin{equation}\label{eq:nonconf}
s_i = 1 - y_{i,k^*}
\end{equation} where $k^*$ is the ground-truth class of sample $i$ and $y_{i,k}$ the $k^{\text{th}}$ component of the soft label $y_i$ within the $K-$simplex $\Delta_K$, as predicted by the model. For a specific error tolerance $\alpha$, the $1 - \alpha$ quantile $\hat{q}$ is derived from the distribution of the non-conformity scores of the calibration samples. The prediction sets for a test sample $i$ is obtained by including all classes $k$ for which $y_{i,k} \geq 1 - \hat{q}$. 

This approach provides theoretical guarantees, specifically that the conformal prediction set for a test sample will, on average, include the true class with a probability of at least $1-\alpha$. This property, known as \textit{coverage}, holds for the dataset as a whole but does not ensure class-specific coverage. For a detailed discussion on the theoretical guarantees and assumptions, refer to~\cite{vovk2005algorithmic,sadinle2019least}. %Adaptive prediction set (APS) \cite{romano2020classification} introduces a different method for computing the conformity scores and shows its generalizability to class specific coverage.  %More recently, regularized APS (RAPS) \cite{angelopoulos2020uncertainty} has been proposed in the specific setting of image classification using deep convolutional neural networks, with a regularization term promoting smaller set sizes. 

The idea of using conformal predictors in the context of HITL systems is not new \cite{zhan2020electronic, cresswell2024conformal}, however, to the best of our knowledge, this work is the first to employ it in conjunction with VLMs for HAR.

\subsection{Human action recognition}
In this work, HAR denotes the problem of classifying video clips or images of a person or group of person performing a well defined action into a set of classes. To address this challenge, early methods focused on the use of handcrafted features \cite{dollar2005behavior, wang2013dense}. Later on, thanks to the emergence of large scale datasets such as Kinetics \cite{carreira2017quo} and advances in machine learning, approaches transitioned to the design of carefully crafted deep-learning architectures trained in a supervised manner. Those architectures include networks based on convolutional building blocks \cite{wang2016temporal, feichtenhofer2019slowfast, jiang2019stm} and more recently transformers~\cite{arnab2021vivit}. 

However, these models are designed to predict a fixed set of classes and do not generalize to settings not seen during training. Current state-of-the-art approaches avoid this problem by relying on VLMs, which can gracefully handle novel classes at test-time due to their extensive pretraining. This paradigm was introduced by \cite{wang2021actionclip}, and later refined by several works such as \cite{ni2022expanding} and \cite{rasheed2023fine}.

\subsection{Vision-Language models}
Vision-language alignment has become a highly influential paradigm for pretraining models that can tackle a broad range of downstream tasks with minimal or no reliance on labeled data. Contrastive approaches such as CLIP~\cite{radford_learning_2021} learn a common representation for textual descriptions and images by concurrently optimizing a textual and visual encoders to retrieve positive pairs of captions and images. Therefore, encoding texts describing the classes of the problem at hand, which are called \textit{textual prompts}, allows for the creation of an ad-hoc classifier by comparing the similarities of input images to the encoded prompts. 

Formally, the textual prompts for each of the class $k\in [1,K]$ are processed by the textual encoder to yield the corresponding $\ell_2$ normalized embedding $t_k \in \mathbb{R}^d$, where $d$ is the dimension of the shared latent space for images and texts. The query image $x_i$ is then mapped to its $\ell_2$ normalized embedding $f_i \in \mathbb{R}^d$ through the visual encoder, and similarities scores 
\begin{equation}
l_{i,k} = f_i^\top t_k
\end{equation}
are computed for each class. These logits can be transformed into probabilistic predictions $y_i \in \Delta_K$ with the softmax function as follows:
\begin{equation}
y_{i,k}= \frac{\exp (l_{i,k}/\tau)}{\sum_{j=1}^K \exp (l_{i,j}/\tau)}.
\label{eq:softmax_labels}
\end{equation}

In Equation \ref{eq:softmax_labels}, the temperature parameter $\tau$ controls the sharpness of the resulting probabilistic prediction. When $\tau$ goes to infinity, the soft labels are uniformly equal to $1/K$ while the softmax becomes the $arg max $ operator when $\tau~=~0$. Note that the temperature parameter is \textit{not} an hyper-parameter of the pre-training procedure, as it is also optimized during pre-training \cite{radford_learning_2021}.