@article{mnih2015humanlevel,
  added-at = {2023-05-11T14:23:41.000+0200},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  biburl = {https://www.bibsonomy.org/bibtex/2fb15f4471c81dc2b9edf2304cb2f7083/csl_uth},
  description = {Human-level control through deep reinforcement learning - nature14236.pdf},
  interhash = {eac59980357d99db87b341b61ef6645f},
  intrahash = {fb15f4471c81dc2b9edf2304cb2f7083},
  issn = {00280836},
  journal = {Nature},
  keywords = {ML Reinforcement_Learning},
  month = feb,
  number = 7540,
  pages = {529--533},
  publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  timestamp = {2023-05-17T09:30:10.000+0200},
  title = {Human-level control through deep reinforcement learning},
  url = {http://dx.doi.org/10.1038/nature14236},
  volume = 518,
  year = 2015
}
@book{sutton1998rl,
  added-at = {2019-07-13T10:11:53.000+0200},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  biburl = {https://www.bibsonomy.org/bibtex/2f46601cf8b13d39d1378af0d79438b12/lanteunis},
  edition = {Second},
  interhash = {ac6b144aaec1819919a2fba9f705c852},
  intrahash = {f46601cf8b13d39d1378af0d79438b12},
  keywords = {},
  publisher = {The MIT Press},
  timestamp = {2019-07-13T10:11:53.000+0200},
  title = {Reinforcement Learning: An Introduction},
  url = {http://incompleteideas.net/book/the-book-2nd.html},
  year = {2018 }
}
@misc{bellemare2016exploration,
      title={Unifying Count-Based Exploration and Intrinsic Motivation}, 
      author={Marc G. Bellemare and Sriram Srinivasan and Georg Ostrovski and Tom Schaul and David Saxton and Remi Munos},
      year={2016},
      eprint={1606.01868},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/1606.01868}, 
}
@misc{rengarajan2022rlsparse,
      title={Reinforcement Learning with Sparse Rewards using Guidance from Offline Demonstration}, 
      author={Desik Rengarajan and Gargi Vaidya and Akshay Sarvesh and Dileep Kalathil and Srinivas Shakkottai},
      year={2022},
      eprint={2202.04628},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2202.04628}, 
}
@inproceedings{ng1999rewshaping,
  author       = {Andrew Y. Ng and
                  Daishi Harada and
                  Stuart Russell},
  editor       = {Ivan Bratko and
                  Saso Dzeroski},
  title        = {Policy Invariance Under Reward Transformations: Theory and Application
                  to Reward Shaping},
  booktitle    = {Proceedings of the Sixteenth International Conference on Machine Learning
                  {(ICML} 1999), Bled, Slovenia, June 27 - 30, 1999},
  pages        = {278--287},
  publisher    = {Morgan Kaufmann},
  year         = {1999},
  timestamp    = {Wed, 20 Apr 2022 13:29:53 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/NgHR99.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{laud2003shaping,
author = {Laud, Adam and DeJong, Gerald},
title = {The influence of reward on the speed of reinforcement learning: an analysis of shaping},
year = {2003},
isbn = {1577351894},
publisher = {AAAI Press},
abstract = {Shaping can be an effective method for improving the learning rate in reinforcement systems. Previously, shaping has been heuristically motivated and implemented. We provide a formal structure with which to interpret the improvement afforded by shaping rewards. Central to our model is the idea of a reward horizon, which focuses exploration on an MDP's critical region, a subset of states with the property that any policy that performs well on the critical region also performs well on the MDP. We provide a simple algorithm and prove that its learning time is polynomial in the size of the critical region and, crucially, independent of the size of the MDP. This identifies low reward horizons with easy-to-learn MDPs. Shaping rewards, which encode our prior knowledge about the relative merits of decisions, can be seen as artificially reducing the MDP's natural reward horizon. We demonstrate empirically the effects of using shaping to reduce the reward horizon.},
booktitle = {Proceedings of the Twentieth International Conference on International Conference on Machine Learning},
pages = {440–447},
numpages = {8},
location = {Washington, DC, USA},
series = {ICML'03}
}
@inproceedings{asmuth2008potential,
author = {Asmuth, John and Littman, Michael L. and Zinkov, Robert},
title = {Potential-based shaping in model-based reinforcement learning},
year = {2008},
isbn = {9781577353683},
publisher = {AAAI Press},
abstract = {Potential-based shaping was designed as a way of introducing background knowledge into model-free reinforcement-learning algorithms. By identifying states that are likely to have high value, this approach can decrease experience complexity--the number of trials needed to find near-optimal behavior. An orthogonal way of decreasing experience complexity is to use a model-based learning approach, building and exploiting an explicit transition model. In this paper, we show how potential-based shaping can be redefined to work in the model-based setting to produce an algorithm that shares the benefits of both ideas.},
booktitle = {Proceedings of the 23rd National Conference on Artificial Intelligence - Volume 2},
pages = {604–609},
numpages = {6},
location = {Chicago, Illinois},
series = {AAAI'08}
}
@inproceedings{griffith2008shaping,
 author = {Griffith, Shane and Subramanian, Kaushik and Scholz, Jonathan and Isbell, Charles L and Thomaz, Andrea L},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Policy Shaping: Integrating Human Feedback with Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/file/e034fb6b66aacc1d48f445ddfb08da98-Paper.pdf},
 volume = {26},
 year = {2013}
}
@misc{lillicrap2019ddpg,
      title={Continuous control with deep reinforcement learning}, 
      author={Timothy P. Lillicrap and Jonathan J. Hunt and Alexander Pritzel and Nicolas Heess and Tom Erez and Yuval Tassa and David Silver and Daan Wierstra},
      year={2019},
      eprint={1509.02971},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1509.02971}, 
}
@misc{schaul2016prioritizeder,
      title={Prioritized Experience Replay}, 
      author={Tom Schaul and John Quan and Ioannis Antonoglou and David Silver},
      year={2016},
      eprint={1511.05952},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1511.05952}, 
}
@misc{andrychowicz2018hindsighter,
      title={Hindsight Experience Replay}, 
      author={Marcin Andrychowicz and Filip Wolski and Alex Ray and Jonas Schneider and Rachel Fong and Peter Welinder and Bob McGrew and Josh Tobin and Pieter Abbeel and Wojciech Zaremba},
      year={2018},
      eprint={1707.01495},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1707.01495}, 
}
@misc{vanhasselt2015doubledqn,
      title={Deep Reinforcement Learning with Double Q-learning}, 
      author={Hado van Hasselt and Arthur Guez and David Silver},
      year={2015},
      eprint={1509.06461},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1509.06461}, 
}
@misc{wang2016duelingdqn,
      title={Dueling Network Architectures for Deep Reinforcement Learning}, 
      author={Ziyu Wang and Tom Schaul and Matteo Hessel and Hado van Hasselt and Marc Lanctot and Nando de Freitas},
      year={2016},
      eprint={1511.06581},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1511.06581}, 
}
@misc{fujimoto2018clippeddqn,
      title={Addressing Function Approximation Error in Actor-Critic Methods}, 
      author={Scott Fujimoto and Herke van Hoof and David Meger},
      year={2018},
      eprint={1802.09477},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/1802.09477}, 
}
@misc{hessel2017rainbow,
      title={Rainbow: Combining Improvements in Deep Reinforcement Learning}, 
      author={Matteo Hessel and Joseph Modayil and Hado van Hasselt and Tom Schaul and Georg Ostrovski and Will Dabney and Dan Horgan and Bilal Piot and Mohammad Azar and David Silver},
      year={2017},
      eprint={1710.02298},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/1710.02298}, 
}
@article{shakya2023surveyrl,
title = {Reinforcement learning algorithms: A brief survey},
journal = {Expert Systems with Applications},
volume = {231},
pages = {120495},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.120495},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423009971},
author = {Ashish Kumar Shakya and Gopinatha Pillai and Sohom Chakrabarty},
keywords = {Reinforcement learning, Stochastic optimal control, Function approximation, Deep Reinforcement Learning (DRL)},
abstract = {Reinforcement Learning (RL) is a machine learning (ML) technique to learn sequential decision-making in complex problems. RL is inspired by trial-and-error based human/animal learning. It can learn an optimal policy autonomously with knowledge obtained by continuous interaction with a stochastic dynamical environment. Problems considered virtually impossible to solve, such as learning to play video games just from pixel information, are now successfully solved using deep reinforcement learning. Without human intervention, RL agents can surpass human performance in challenging tasks. This review gives a broad overview of RL, covering its fundamental principles, essential methods, and illustrative applications. The authors aim to develop an initial reference point for researchers commencing their research work in RL. In this review, the authors cover some fundamental model-free RL algorithms and pathbreaking function approximation-based deep RL (DRL) algorithms for complex uncertain tasks with continuous action and state spaces, making RL useful in various interdisciplinary fields. This article also provides a brief review of model-based and multi-agent RL approaches. Finally, some promising research directions for RL are briefly presented.}
}
@misc{zare2023surveyil,
      title={A Survey of Imitation Learning: Algorithms, Recent Developments, and Challenges}, 
      author={Maryam Zare and Parham M. Kebria and Abbas Khosravi and Saeid Nahavandi},
      year={2023},
      eprint={2309.02473},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2309.02473}, 
}
@misc{hester2017dqndemonstrations,
      title={Deep Q-learning from Demonstrations}, 
      author={Todd Hester and Matej Vecerik and Olivier Pietquin and Marc Lanctot and Tom Schaul and Bilal Piot and Dan Horgan and John Quan and Andrew Sendonaris and Gabriel Dulac-Arnold and Ian Osband and John Agapiou and Joel Z. Leibo and Audrunas Gruslys},
      year={2017},
      eprint={1704.03732},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/1704.03732}, 
}
@misc{vecerik2018lddpgfd,
      title={Leveraging Demonstrations for Deep Reinforcement Learning on Robotics Problems with Sparse Rewards}, 
      author={Mel Vecerik and Todd Hester and Jonathan Scholz and Fumin Wang and Olivier Pietquin and Bilal Piot and Nicolas Heess and Thomas Rothörl and Thomas Lampe and Martin Riedmiller},
      year={2018},
      eprint={1707.08817},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/1707.08817}, 
}
@misc{schmitt2018kickstartrl,
      title={Kickstarting Deep Reinforcement Learning}, 
      author={Simon Schmitt and Jonathan J. Hudson and Augustin Zidek and Simon Osindero and Carl Doersch and Wojciech M. Czarnecki and Joel Z. Leibo and Heinrich Kuttler and Andrew Zisserman and Karen Simonyan and S. M. Ali Eslami},
      year={2018},
      eprint={1803.03835},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1803.03835}, 
}
@misc{agarwal2022reincarnatingrl,
      title={Reincarnating Reinforcement Learning: Reusing Prior Computation to Accelerate Progress}, 
      author={Rishabh Agarwal and Max Schwarzer and Pablo Samuel Castro and Aaron Courville and Marc G. Bellemare},
      year={2022},
      eprint={2206.01626},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2206.01626}, 
}
@misc{nair2021awac,
      title={AWAC: Accelerating Online Reinforcement Learning with Offline Datasets}, 
      author={Ashvin Nair and Abhishek Gupta and Murtaza Dalal and Sergey Levine},
      year={2021},
      eprint={2006.09359},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2006.09359}, 
}
@INPROCEEDINGS{malato2024boa,

  author={Malato, Federico and Hautamäki, Ville},

  booktitle={2024 IEEE Conference on Games (CoG)}, 

  title={Online Adaptation for Enhancing Imitation Learning Policies}, 

  year={2024},

  volume={},

  number={},

  pages={1-8},

  keywords={Training;Adaptation models;Machine learning algorithms;Imitation learning;Games;Real-time systems;Autonomous agents;imitation learning;behavioral cloning;inverse reinforcement learning;online adaptation},

  doi={10.1109/CoG60054.2024.10645565}}
@misc{haarnoja2018sac,
      title={Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor}, 
      author={Tuomas Haarnoja and Aurick Zhou and Pieter Abbeel and Sergey Levine},
      year={2018},
      eprint={1801.01290},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1801.01290}, 
}
@misc{schulman2017trpo,
      title={Trust Region Policy Optimization}, 
      author={John Schulman and Sergey Levine and Philipp Moritz and Michael I. Jordan and Pieter Abbeel},
      year={2017},
      eprint={1502.05477},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1502.05477}, 
}
@misc{schulman2017ppo,
      title={Proximal Policy Optimization Algorithms}, 
      author={John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
      year={2017},
      eprint={1707.06347},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1707.06347}, 
}
@misc{ross2011dagger,
      title={A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning}, 
      author={Stephane Ross and Geoffrey J. Gordon and J. Andrew Bagnell},
      year={2011},
      eprint={1011.0686},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1011.0686}, 
}
@misc{kelly2019hgdagger,
      title={HG-DAgger: Interactive Imitation Learning with Human Experts}, 
      author={Michael Kelly and Chelsea Sidrane and Katherine Driggs-Campbell and Mykel J. Kochenderfer},
      year={2019},
      eprint={1810.02890},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/1810.02890}, 
}
@misc{malato2022hddaugment,
      title={Improving Behavioural Cloning with Human-Driven Dynamic Dataset Augmentation}, 
      author={Federico Malato and Joona Jehkonen and Ville Hautamäki},
      year={2022},
      eprint={2201.07719},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2201.07719}, 
}
@misc{chevalierboisvert2023miniworld,
      title={Minigrid \& Miniworld: Modular \& Customizable Reinforcement Learning Environments for Goal-Oriented Tasks}, 
      author={Maxime Chevalier-Boisvert and Bolun Dai and Mark Towers and Rodrigo de Lazcano and Lucas Willems and Salem Lahlou and Suman Pal and Pablo Samuel Castro and Jordan Terry},
      year={2023},
      eprint={2306.13831},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2306.13831}, 
}
@INPROCEEDINGS{todorov2012mujoco,
  author={Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  booktitle={2012 IEEE/RSJ International Conference on Intelligent Robots and Systems}, 
  title={MuJoCo: A physics engine for model-based control}, 
  year={2012},
  volume={},
  number={},
  pages={5026-5033},
  keywords={Engines;Optimization;Computational modeling;Heuristic algorithms;Dynamics;Mathematical model},
  doi={10.1109/IROS.2012.6386109}}

@misc{kingma2022vae,
      title={Auto-Encoding Variational Bayes}, 
      author={Diederik P Kingma and Max Welling},
      year={2022},
      eprint={1312.6114},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1312.6114}, 
}
@inproceedings{
higgins2017betavae,
title={beta-{VAE}: Learning Basic Visual Concepts with a Constrained Variational Framework},
author={Irina Higgins and Loic Matthey and Arka Pal and Christopher Burgess and Xavier Glorot and Matthew Botvinick and Shakir Mohamed and Alexander Lerchner},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=Sy2fzU9gl}
}
@misc{wandb,
      title={Weight \& Biases homepage},
      author={Lukas Biewald and Chris Van Pelt and Shawn Lewis},
      url={https://wandb.ai/site/papers/}
}
@misc{huang2021cleanrl,
      title={CleanRL: High-quality Single-file Implementations of Deep Reinforcement Learning Algorithms}, 
      author={Shengyi Huang and Rousslan Fernand Julien Dossa and Chang Ye and Jeff Braga},
      year={2021},
      eprint={2111.08819},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2111.08819}, 
}
@misc{torabi2018bc,
      title={Behavioral Cloning from Observation}, 
      author={Faraz Torabi and Garrett Warnell and Peter Stone},
      year={2018},
      eprint={1805.01954},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/1805.01954}, 
}