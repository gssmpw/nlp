\section{Related Work}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/suboptimal_convergence.pdf}
    \caption{An example of policy converging to a local minimum when the expert optimality assumption does not hold. With respect to the baseline (blue), the kickstarted policy $\pi_{\theta}$ (orange) learns much faster at the beginning of training. Still, performance saturates due to the joint effect of expert not being optimal and the distribution-based constraint.}
    \label{fig:local_minimum}
\end{figure}
A summary of the information discussed in this Section is reported in Table~\ref{tab:previous_research}. The Table reports a list of approaches that are closely related to our proposal. For each source, we report a brief description of its main points, a list of compatible DRL algorithms, whether the optimality of expert is assumed, and a short overview of its main features.

In detail, {\em Deep-Q-Learning from Demonstrations} (DQfD)~\cite{hester2017dqndemonstrations} and {\em DDPG from Demonstrations} (DDPGfD)~\cite{vecerik2018lddpgfd} leverage human-collected trajectories and add a supervised margin loss term, a 1-step and an n-step double Q-Learning terms, and an L2 regularization term to the loss. Each term addresses and corrects a specific failure case, such as a non-representative dataset for the margin loss and stability of training for the double Q-learning terms.

In \cite{schmitt2018kickstartrl}, authors collect experience from previously trained agents to train new ones. This approach assumes that an optimal policy exists and can be trained for the task at hand. As such, we point out that such method would be best applicable to e.g. transfer learning from simulations to autonomous agents such as robots acting in the real world. 

For {\em QDagger}~\cite{agarwal2022reincarnatingrl}, authors suggest that suboptimal data coming from either pre-trained policies or human experts could be used to train a student network. Their method is based on adding an entropy-based penalty term to the loss function. Additionally, QDagger requires an offline pre-training phase, which we avoid in our proposal.

In \cite{rengarajan2022rlsparse}, the authors propose a new framework, {\em Learning Online from Guidance Offline} (LOGO), and derive a penalty term based on KL-divergence for {\em Trusted Region Policy Optimization} (TRPO)~\cite{schulman2017trpo}. More specifically, authors propose to alternate between a policy improvement and a policy guidance step. During policy improvement, the agent is trained using TRPO. Then, an additional step to guide the policy towards the expert {\em while remaining in the truster region} is performed. We point out that the algorithm is specifically tailored on one learning algorithm, TRPO, whereas we propose a generic method that is widely applicable; additionally, LOGO performs two gradient steps for each update, effectively increasing training time. Finally, in the case of partial observability authors propose to train an additional critic for the expert and a discriminator to estimate the current policy performance w.r.t. the expert. We highlight that training additional models might introduce instabilities and propagate errors that lead to catastrophic results.

Similarly, {\em Advantage Weighted Actor Critic} (AWAC)~\cite{nair2021awac} derives a closed-form update to train a policy on a mixture of offline and online data, starting from a constrained optimization problem based on KL-divergence between a policy and an expert. Although being tested only on SAC, the method is applicable to virtually any off-policy algorithm. AWAC features an offline pre-training step and initializes the replay buffer with human expert trajectories at the beginning of the online training phase, similar to DQfD and DDPGfD.