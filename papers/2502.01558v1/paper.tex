%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{makecell}

\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Search-Based Adversarial Estimates for Improving Sample Efficiency in Off-Policy Reinforcement Learning}

\begin{document}

\twocolumn[
\icmltitle{Search-Based Adversarial Estimates for Improving \\ Sample Efficiency in Off-Policy Reinforcement Learning}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Federico Malato}{uef}
\icmlauthor{Ville Hautam\"aki}{uef}
\end{icmlauthorlist}

\icmlaffiliation{uef}{School of Computing, University of Eastern Finland, Joensuu, Finland}

\icmlcorrespondingauthor{Federico Malato}{federico.malato@uef.fi}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Reinforcement Learning, Sample Efficiency, Similarity Search}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%5\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Sample inefficiency is a long-lasting challenge in deep reinforcement learning (DRL). Despite dramatic improvements have been made, the problem is far from being solved and is especially challenging in environments with sparse or delayed rewards. In our work, we propose to use Adversarial Estimates as a new, simple and efficient approach to mitigate this problem for a class of feedback-based DRL algorithms. Our approach leverages latent similarity search from a small set of human-collected trajectories to boost learning, using only five minutes of human-recorded experience. The results of our study show algorithms trained with Adversarial Estimates converge faster than their original version. Moreover, we discuss how our approach could enable learning in feedback-based algorithms in extreme scenarios with very sparse rewards.
\end{abstract}

\section{Introduction}
\label{sec:intro}

Sample inefficiency is a long-lasting challenge in {\em deep reinforcement learning} (DRL) affecting all well-known algorithms, such as Deep-Q-Network (DQN)~\cite{mnih2015humanlevel}, Deep Deterministic Policy Gradient (DDPG)~\cite{lillicrap2019ddpg} and Soft Actor Critic (SAC)~\cite{haarnoja2018sac}. The problem relates to the high number of transitions needed to learn an optimal policy~\cite{sutton1998rl}. Sample inefficiency has several causes, ranging from a high-dimensional observation or action spaces~\cite{mnih2015humanlevel}, to an ineffective exploration strategy~\cite{bellemare2016exploration}, or even an ill-designed reward signal~\cite{rengarajan2022rlsparse}. In general, a good reward signal should be mathematically well-defined and encapsulate the learning objective perfectly~\cite{sutton1998rl}. Unfortunately, specifying a reward signal with these properties is generally a non-trivial task, and in some cases it might not be possible at all~\cite{shakya2023surveyrl,zare2023surveyil,rengarajan2022rlsparse}. It is possible to address this shortcoming by defining a {\em sparse} reward, that is, assigning a positive constant value only to certain transitions. As an example, consider the case of a robot moving in a room, whose objective is to move to a certain goal, given an initial position. If the distance between the robot and the goal position can be computed, then the inverse of that distance could be used as a good, dense reward signal; if this is not possible, though, a good solution would be to assign a value of $1$ upon reaching the goal, and $0$ otherwise. The latter case represents an example of sparse reward. Additionally, such reward is also {\em delayed}, as the robot will not observe it until the end of an episode.

\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{figures/graphical_abstract.pdf}
    \caption{A schematic illustration of our proposed method, Adversarial Estimates. The agent sees the expert as an adversary. At each timestep, the reward obtained by the agent is compared to the estimated reward obtained by the expert in similar situations, retrieved from a set of expert trajectories. This feedback is then used to guide training.}
    \label{fig:boa}
\end{figure}

\begin{table*}[t]
    \centering
    \caption{A summary of the most relevant previous research used as inspiration for our work.}
    \begin{tabular}{ccccc}
        \toprule
        \thead{Paper}  & \thead{Description} & \thead{Algorithms} & \thead{Assumes expert \\ optimality} & \thead{Main features} \\
        \midrule
        \makecell{DQN/DDPG from \\ Demonstrations \\ (DQfD)/(DDPGfD) \\ \cite{hester2017dqndemonstrations}/ \\ \cite{vecerik2018lddpgfd}} & \makecell{Loads human-collected experience \\ in replay buffer and adapts \\ DQN/DDPG objective}      & \makecell{DQN/ \\ DDPG}         & Yes & \makecell{Pre-training; \\ Mixture random sampling; \\ Distribution-based update} \\
         & & \\
        \makecell{Reincarnating RL \\ \cite{agarwal2022reincarnatingrl}} & \makecell{Uses previous experience \\ from policy or human expert \\ as teacher in a DAgger-like \\ approach}      & \makecell{Off-policy \\ algorithms} & \textbf{No} & \makecell{Pre-training; \\ Mixture random sampling; \\ Distribution-based update}\\
         & & \\
         \makecell{Learning Online from \\ Guidance Offline \\ (LOGO) \\ \cite{rengarajan2022rlsparse}} & \makecell{Entropy-based loss \\ regularization to learn from \\ human-collected trajectories}      & TRPO & \textbf{No} & \makecell{Mixture random sampling; \\ Distribution-based update} \\
         & & \\
        \makecell{Advantage Weighted \\ Actor-Critic \\ (AWAC) \\ \cite{nair2021awac}} & \makecell{Defines closed-form solution \\ for KL-bounded update \\ based on expert policy}    & \makecell{Off-policy \\ algorithms}         & Yes       & \makecell{Pre-training; \\ Mixture random sampling; \\ Distribution-based update} \\
         & & \\
        \makecell{\textbf{Adversarial} \\ \textbf{Estimates} \\ \textbf{(Ours)}} & \makecell{Uses search on human-collected \\ experience to boost Q-values \\ convergence}       & \makecell{Off-policy \\ algorithms}          & \textbf{No}       & \makecell{Search-based sampling} \\
        \bottomrule
    \end{tabular}
    %\caption{A summary of the most relevant previous research used as inspiration for our work. For each source we report a brief description, applicability, reliance on expert optimality, and an overview of relevant features.}
    \label{tab:previous_research}
\end{table*}

Using sparse and delayed reward simplifies the task of specifying the goal of a task; unfortunately, the sporadic feedback turns the task into an harder one to learn. In particular, a sparse reward signal could prevent an agent from collecting meaningful experience, thus delaying or preventing learning~\cite{lillicrap2019ddpg,rengarajan2022rlsparse}. Similarly, an agent acting to solve a task with delayed rewards might not experience positive reinforcement very often, increasing the number of transitions sampled to converge~\cite{andrychowicz2018hindsighter}. Despite vast efforts in this field and many proposed solutions, the problem of mitigating sample inefficiency for sparse or delayed reward tasks is still an open problem~\cite{shakya2023surveyrl}.

Previous research can be divided into three categories, depending on the approach used to address the problem of sample efficiency. The first category proposes to include off\-line data in the replay buffer of an off-policy algorithm at the beginning of training, and to adapt the learning objective accordingly. This line of research is used in {\em Deep-Q-Learning from Demonstrations} (DQfD)~\cite{hester2017dqndemonstrations} and {\em Deep Deterministic Policy Gradients from Demonstrations} (DDPGfD)~\cite{vecerik2018lddpgfd}. The second category proposes to add a pre-training phase using only human expert data, and subsequently fine-tuning the policy online using a distribution-based penalty term to the loss. Examples of this approach are \cite{schmitt2018kickstartrl}, QDagger~\cite{agarwal2022reincarnatingrl} and {\em Learning Online from Guidance Offline} (LOGO)~\cite{rengarajan2022rlsparse}. Finally, a third category combines ideas from both groups, such as {\em Advantage Weighted Actor Critic} (AWAC)~\cite{nair2021awac}. For their approach, the authors preload an offline dataset in the replay buffer and use it for pre-training their policy. Then, they fine-tune their policy using a loss derived from a KL-constrained optimization problem.


In our work, we propose to detach from three key aspects of previous methods: the need for pre-training, the distribution-based regularization, and the narrow application to algorithms. Firstly, although pre-training perfectly serves its purpose of transferring knowledge between policies~\cite{agarwal2022reincarnatingrl}, we deem this step unnecessarily long for standard policy training. Additionally, such phase might rely on high-quality data from an optimal or near-optimal expert, which might be impractical for complex tasks. In second instance, relying on a distribution-based regularizer biases the policy to follow the expert. If the expert is considered optimal~\cite{hester2017dqndemonstrations,vecerik2018lddpgfd,nair2021awac}, the trained policy will at most match those performance. This is a problem when the expert is suboptimal, hence binding the learned policy to be suboptimal as well (see Figure~\ref{fig:local_minimum}). A solution to this is gradually detaching from the expert policy using a decaying hyperparameter~\cite{rengarajan2022rlsparse}. While this allows for an improvement over the expert, it usually comes with a degradation in performance, as the sudden drop of the red line in Figure~\ref{fig:local_minimum} shows. Finally, previous approaches such as AWAC show that it is possible to find a broadly applicable update rule. As such, although previous work~\cite{hester2017dqndemonstrations,vecerik2018lddpgfd,rengarajan2022rlsparse} shows that tweaking a specific algorithm leads to an improvement, we consider these results to have limited application.

Mainly inspired by QDagger and AWAC, and leveraging {\em Bayesian online adaptation} (BOA)~\cite{malato2024boa}, we propose to improve sample efficiency in delayed reward environments by combining standard DRL and similarity search (in the form of {\em online adaptation}) on a small pool of human-collected trajectories. \textbf{Our contributions are three-fold}: first, we derive a general solution for the loss penalization approach, which allows us to overcome limitations arising from the expert optimality assumption~\cite{torabi2018bc}; second, we remove the need for pre-training, while keeping the broad applicability of such method to any off-policy RL algorithm; third, we reduce the amount of data needed, and remove the need for a pre-trained policy, by using only five minutes of collected experience from a human expert. Additionally, in Section~\ref{sec:conclusions} we discuss the possibility of further extending our idea to on-policy algorithms, highlighting the broad applicability of our approach.

\section{Related Work}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/suboptimal_convergence.pdf}
    \caption{An example of policy converging to a local minimum when the expert optimality assumption does not hold. With respect to the baseline (blue), the kickstarted policy $\pi_{\theta}$ (orange) learns much faster at the beginning of training. Still, performance saturates due to the joint effect of expert not being optimal and the distribution-based constraint.}
    \label{fig:local_minimum}
\end{figure}
A summary of the information discussed in this Section is reported in Table~\ref{tab:previous_research}. The Table reports a list of approaches that are closely related to our proposal. For each source, we report a brief description of its main points, a list of compatible DRL algorithms, whether the optimality of expert is assumed, and a short overview of its main features.

In detail, {\em Deep-Q-Learning from Demonstrations} (DQfD)~\cite{hester2017dqndemonstrations} and {\em DDPG from Demonstrations} (DDPGfD)~\cite{vecerik2018lddpgfd} leverage human-collected trajectories and add a supervised margin loss term, a 1-step and an n-step double Q-Learning terms, and an L2 regularization term to the loss. Each term addresses and corrects a specific failure case, such as a non-representative dataset for the margin loss and stability of training for the double Q-learning terms.

In \cite{schmitt2018kickstartrl}, authors collect experience from previously trained agents to train new ones. This approach assumes that an optimal policy exists and can be trained for the task at hand. As such, we point out that such method would be best applicable to e.g. transfer learning from simulations to autonomous agents such as robots acting in the real world. 

For {\em QDagger}~\cite{agarwal2022reincarnatingrl}, authors suggest that suboptimal data coming from either pre-trained policies or human experts could be used to train a student network. Their method is based on adding an entropy-based penalty term to the loss function. Additionally, QDagger requires an offline pre-training phase, which we avoid in our proposal.

In \cite{rengarajan2022rlsparse}, the authors propose a new framework, {\em Learning Online from Guidance Offline} (LOGO), and derive a penalty term based on KL-divergence for {\em Trusted Region Policy Optimization} (TRPO)~\cite{schulman2017trpo}. More specifically, authors propose to alternate between a policy improvement and a policy guidance step. During policy improvement, the agent is trained using TRPO. Then, an additional step to guide the policy towards the expert {\em while remaining in the truster region} is performed. We point out that the algorithm is specifically tailored on one learning algorithm, TRPO, whereas we propose a generic method that is widely applicable; additionally, LOGO performs two gradient steps for each update, effectively increasing training time. Finally, in the case of partial observability authors propose to train an additional critic for the expert and a discriminator to estimate the current policy performance w.r.t. the expert. We highlight that training additional models might introduce instabilities and propagate errors that lead to catastrophic results.

Similarly, {\em Advantage Weighted Actor Critic} (AWAC)~\cite{nair2021awac} derives a closed-form update to train a policy on a mixture of offline and online data, starting from a constrained optimization problem based on KL-divergence between a policy and an expert. Although being tested only on SAC, the method is applicable to virtually any off-policy algorithm. AWAC features an offline pre-training step and initializes the replay buffer with human expert trajectories at the beginning of the online training phase, similar to DQfD and DDPGfD.

\section{Preliminaries}
In RL and DRL, our aim is to train a policy $\pi_{\theta}: S \rightarrow A$ parameterized by a set of parameters $\theta$ to select actions that, at any timestep $t$, maximize the expected sum of discounted future rewards 
\begin{equation}
    \mathbb{E}[\sum_{\tau=t}^{\infty}\gamma^{\tau - t}r_{\tau}] . 
\end{equation}

\subsection{Markov decision process}
We model our problem as a {\em Markov decision process} (MDP). Formally, the problem is defined as a $5$-tuple $(S, A, T, R, \gamma)$, where we denote $S \subseteq \mathbb{R}^d$ as the $d$-dimensional state space, $A$ as a continuous or discrete action space, $T: S \times A \rightarrow S$ the (unknown) transition dynamics, $R:S \times A \rightarrow \mathbb{R}$ a reward function, and $\gamma \in [0, 1)$ as the discount factor.

Additionally, our image-based experiments are modeled on a {\em partially observable Markov decision process} (POMDP). Such problems represent a generalization of MDPs where the full state of the environment is not observable by the agent. Formally, the problem is defined as a $7$-tuple $(S, A, T, R, \Omega, O, \gamma)$ where, other than the previously defined $S$, $A$, $T$, and $R$, we introduce a set of (partial) observations $\Omega \subseteq \mathbb{R}^{d}$, and a function $O: S \times A \rightarrow \Omega$ mapping the probability of observing $o \in \Omega$, given a certain state $s \in S$ and an action $a \in A$.

\subsection{Clipped Double-Q-Learning}
{\em Clipped Double-Q-Learning} (cDQL)~\cite{fujimoto2018clippeddqn} is an off-policy value function algorithm for DRL based on the concept of temporal difference (TD) error. The algorithm is a tweaked version of DQN that improves stability during training. At each timestep $t$, a policy $\pi_{\theta}$ greedily selects an action $a_t$ following
\begin{equation}
    a_t = \argmax_{a \in A}Q^{t}(s_t, a)
\end{equation}
where 
\begin{equation}
    Q^{t}(s_t, a) = \mathbb{E}[R_t | s_t, a, \pi_{\theta}], R_t = \sum_{\tau=t}^{\infty}\gamma^{\tau - t}r_{\tau}.
\end{equation}
Subsequently, the algorithm updates its Q-values using
\begin{equation}
    Q^{t+1}(s_{t+1}, a) = \mathbb{E}[r_t + \gamma \max_{a' \in A}Q^{t}(s_t, a) | s_{t+1}, a].
\end{equation}
Differently from the standard DQN, cDQL uses two twin neural networks to estimate the policy: one is updated with gradient methods; the second one, called the {\em target network} is updated with lower frequency by soft-updating the weights of the policy network. This is done to avoid overshooting. In particular, cDQL bounds the Q-values by computing the current estimate $y_{t}$ as 
\begin{equation}
    y_{t} = r_{t} + \gamma\min(Q_{\theta}^{t}(s_t, a_t), Q_{\phi}^{t}(s_t, a_t)),
\end{equation}
where $\phi$ are the parameters of the target network. cDQL minimizes
\begin{equation}
\label{eq:loss_dqn}
    \mathcal{L}(\theta) = \mathbb{E}_{t}[(y_{t} - Q_{\theta}^{t+1}(s_{t+1}, a_{t+1}))^{2}].
\end{equation}

\subsection{Advantage Weighted Actor Critic}
Given a guidance policy $\pi_{\beta}$, AWAC aims at efficiently finding an optimal policy $\pi^{*}$ by leveraging a dataset $\mathcal{D}$ collected from $\pi_{\beta}$. In particular, a parameterized policy $\pi_{\theta}$ is trained following
\begin{equation}
    \theta_{k + 1} = \argmax_{\theta} \mathbb{E}_{s,a\sim\mathcal{D}}[\log{\pi_{\theta}(a|s)\exp{\frac{1}{\lambda}A^{\pi_k}(s, a)}}]
\end{equation}
which is equivalent to adding a weighted regularization term $D_{\mathrm{KL}}(\pi_{\theta} || \pi_{\beta})$ to the off-policy loss (Equations~\ref{eq:loss_dqn} and~\ref{eq:loss_sac}).

\subsection{Bayesian online adaptation}
BOA uses Bayesian statistics to update the belief of a DRL agent, given a parameterized policy $\pi_{\theta}$, a prior distribution $\pi_{\theta}(a_{t}^{(\theta)} | s_{t})$ and an expert policy $\pi_{E}$. By modeling the prior as
\begin{equation}
    \pi_{\theta}(a_{t}^{(\theta)} | s_{t}) \sim \mathrm{Dirichlet}(K, \bold{\alpha}_{\mathrm{prior}})
\end{equation}
with $K = |A|$, $\bold{\alpha}_{\mathrm{prior}, i} = \pi_{\theta}(a_{t}^{(\theta)} = i | s_{t})$ and we leverage Bayesian statistics to compute the posterior
\begin{equation}
    \pi_{\theta}(a_{t}^{(\theta)} | a_{t}^{(E)}, s_{t}) \propto \pi_{E}(a_{t}^{(E)} | a_{t}^{(\theta)}, s_{t})\pi_{\theta}(a_{t}^{(\theta)} | s_{t})
\end{equation}
which is still a Dirichlet with $K$ components and $\bold{\alpha}_{\mathrm{posterior}} = \bold{\alpha}_{\mathrm{prior}} + c_{t}$, where $c_{t}$ is a vector storing the number of occurrences of each action sampled from $\pi_{E}$. In substance, we can update the belief of a network following the "suggestions" of an arbitrary expert policy. In \cite{malato2024boa}, $\pi_{E}$ is chosen to be a simple search-based policy that, at each timestep $t$, retrieves the $k$-most similar stored latents to the current state $s_{t}$ from a pre-encoded, small demonstration dataset.

\section{Adversarial Estimates}
As highlighted in Table \ref{tab:previous_research}, previous research incorporates off\-line experience, either sampled from a previously trained policy or collected by human contractors, to improve the quality of experience collected by the agent during training. Notably, all methods explicitly use an offline dataset along with the experience in the replay buffer, either mixing the two sets (\cite{hester2017dqndemonstrations,vecerik2018lddpgfd}), or sampling a mixture of experiences (\cite{nair2021awac,agarwal2022reincarnatingrl,rengarajan2022rlsparse}). Motivated by these attempts, we propose an alternative view to the problem of improving sample efficiency.

Differently from previous approaches, we focus on leveraging human-collected experience to boost the convergence rate of Q-values using {\em Adversarial Estimates}. Loosely inspired by sealed bid auction games, we suppose a competitive scenario in which two players, $A$ and $B$, are bidding without knowing the opponent strategy or utility function. Intuitively, if player $A$ knows the utility function of player $B$, they might adjust their beliefs and, as a consequence, their bidding {\em before} the bidding itself.

Similarly, we propose an adversarial scenario for single agent RL, in which the agent $\pi_{\theta}$ plays to improve over an expert $\pi_{E}$. $\pi_{\theta}$ does not know the rewards obtained by $\pi_{E}$. Still, it can {\em estimate} such returns, by assuming that both players are following the same objective. 

Intuitively, we want $\pi_{\theta}$ to pursue close or winning scenarios over $\pi_{E}$; on the contrary, we want to refute situations where the performance gap is unfavorable for the agent. As such, we propose an adversarial estimate of the form
\begin{equation}
\label{eq:zeta}
    Z_{t}(s, a) = E_{(s', a') \sim \mathcal{D}}[Q_{\phi}^{t}(s', a')] - R_{t}(s, a),
\end{equation}
where $Q_{\phi}$ denotes the target network, $(s', a') \sim \mathcal{D}$ are state-action pairs extracted from the expert dataset $\mathcal{D}$ using similarity search, and $R_{t}(s, a)$ is the {\em actual} reward observed at timestep $t$.

In our formulation, we over-estimate the rewards obtained by the opponent $\pi_{E}$ using search~\cite{malato2024boa}, and compare them to the actual rewards obtained by the policy $\pi_{\theta}$. The effect of this choice is two-fold. First, we match the penalty terms proposed in previous work, since $Z_{t}(s, a) \rightarrow 0$ as $R_{t}(s, a) \rightarrow E_{(s', a') \sim \mathcal{D}}[Q_{\phi}^{t}(s', a')]$, that is, when the agent matches the {\em expected, over-estimated performance} of the expert. Notably, our update does not bound $\pi_{\theta}$ to lie in the vicinity of $\pi_{E}$. Secondly, we ensure faster convergence of the Q-values by sampling multiple {\em similar} transitions from $\mathcal{D}$ and averaging their estimated returns. In practice, this creates a "more realistic expectation" on state $s$, which is then implicitly propagated in the loss through $Z_{t}(s, a)$.

\subsection{Mathematical derivation}
\label{sec:derivation}
The first part of our derivation follows \cite{nair2021awac}. We aim at solving a constrained optimization problem of the form
\begin{align}
    \pi_{k+1} = \argmax_{\pi \in \Pi} \mathbb{E}_{a \sim \pi(\cdot | s)}[R^{\pi_{k}}(s, a)] \\
    \mathrm{s.t.} \quad D_{\mathrm{KL}}(\pi(\cdot | s) || \pi_{\beta}(\cdot | s)) \leq \epsilon,
\end{align}
where $\pi_{\beta}(\cdot | s)$ is an expert policy. Note that, differently from AWAC, we formulate the problem in term of maximization of a reward function $R^{\pi_{k}}(s, a)$ following a policy $\pi_{k}$. As such, our formulation, in principle, covers any algorithm that aims at maximizing a reward.

By enforcing the KKT conditions, we derive a lagrangian solution for our problem
\begin{align}
    \mathcal{L}(\pi, \lambda) = \mathbb{E}_{a \sim \pi(\cdot | s)}[R^{\pi_{k}}(s, a)] + \nonumber \\ 
    + \lambda(\epsilon - D_{\mathrm{KL}}(\pi(\cdot | s) || \pi_{\beta}(\cdot | s))),
\end{align}
which is equivalent to the dual problem
\begin{align}
    \min_{\lambda \geq 0}\max_{\pi \in \Pi} \mathcal{L(\pi, \lambda)}.
\end{align}

In our formulation, $\lambda$ regulates the interaction between the two terms in $\mathcal{L}(\pi, \lambda)$. More specifically, it expresses the importance of being close to the expert policy $\pi_{\beta}$. As such, we relax our problem and consider $\lambda$ a hyperparameter. After dropping the constant term $\lambda\epsilon$, we are therefore left with
\begin{align}
    \max_{\pi \in \Pi} \mathbb{E}_{a \sim \pi(\cdot | s)}[R^{\pi_{k}}(s, a)] - \lambda D_{\mathrm{KL}}(\pi(\cdot | s) || \pi_{\beta}(\cdot | s))),
\end{align}
which is equivalent to minimizing
\begin{align}
\label{eq:kl_loss}
    \mathcal{L}(\theta) = \mathbb{E}_{a \sim \pi_{\theta}(\cdot | s)}[R^{\pi_{k}}(s, a)] + \lambda D_{KL}(\pi_{\theta}(\cdot | s) || \pi_{\beta}(\cdot | s))).
\end{align}

Following~\cite{nair2021awac}, we notice that $\pi_{\theta}$ can be projected in a parameterized policy space using either direction of the KL divergence. Specifically, by choosing the reverse formulation we can estimate it by simply sampling transitions from $\pi_{\beta}$. In our specific scenario, we choose $\pi_{\beta} = \pi_{E}$, that is, the expert policy used in \cite{malato2024boa}. That is, at each timestep we can retrieve the $k$-most relevant transitions from our dataset $\mathcal{D}$ and use them as minibatch to enforce the KL constraint.

For the second part of our derivation, we aim at demonstrating the validity of our update in place of the KL constraint. In particular, we want to prove that using $Z_{t}(s, a)$ (Equation~\ref{eq:zeta}) instead of the KL term leads to a relaxation of the optimization problem posed in Equation \ref{eq:kl_loss}.

First, we need to prove that the two updates share the same objective. To this end, we remove the optimality assumption of $\pi_{E}$. That is, we consider that there will exist at least one policy $\pi$ in the space of policies $\Pi$ such that
\begin{equation}
    \mathbb{E}_{(s', a') \sim \pi}[R(s', a')] \geq \mathbb{E}_{(s', a') \sim \pi_{E}}[R(s', a')].
\end{equation}
In this case, we say that $\pi \in \Pi_{\geq E}$ by defining
\begin{equation}
    \Pi_{\geq E} := \{\pi | \mathbb{E}_{(s', a') \sim \pi}[R(s', a')] \geq \mathbb{E}_{(s', a') \sim \pi_{E}}[R(s', a')]\}.
\end{equation}
By definition, $\pi_{E} \in \Pi_{\geq E}$. We now consider that
\begin{equation}
    D_{\mathrm{KL}}(\pi_{\theta}||\pi_{E}) \xrightarrow{\pi_{\theta} \rightarrow \pi_{E}} 0,
\end{equation}
that is, the KL divergence will reach its minimum as the policy converges towards the expert. Similarly,
\begin{gather}
    Z_{t}(s, a) \xrightarrow{\pi_{\theta} \rightarrow \pi} 0, \pi \in \Pi_{\geq E}.
\end{gather}
By imposing $\pi = \pi_{E}$, we find that $Z_{t}(s, a)$ behaves numerically as $D_{\mathrm{KL}}(\pi_{\theta}||\pi_{E})$. We can therefore re-write our loss as 
\begin{align}
    \mathcal{L}_{\mathrm{AE}}(\theta) = \mathbb{E}_{a \sim \pi_{\theta}(\cdot | s)}[R^{\pi_{k}}(s, a)] + \lambda Z_{t}(s, a).
\end{align}
This objective admits all the solutions of Equation \ref{eq:kl_loss}, with the (possible) addition of new, {\em at least equally performing} ones. That is, $Z_{t}(s, a)$ relaxes this minimization problem.

Intuitively, the KL constraint limits $\pi_{\theta}$ to be close to $\pi_{E}$. We argue that such limitation is desirable only on the assumption that $\pi_{E}$ is effectively an optimal policy, or that it is represented as such from the data in $\mathcal{D}$~\cite{zare2023surveyil}. Consistently with previous research, we argue that such assumption might be hazardous, especially for complex tasks and small demonstrations datasets~\cite{zare2023surveyil,ross2011dagger,kelly2019hgdagger,malato2022hddaugment}. Whenever the optimality assumption is not satisfied, $\pi_{\theta}$ might learn to follow a suboptimal policy and, as such, get stuck in a local minimum. In Figure~\ref{fig:local_minimum} we show an example of this behaviour from our preliminary experiments for the present study.

\section{Experiments}
\label{sec:experiments}
We use vanilla cDQL as main comparison for our experiments, and compare them to our improved agents. Additionally, we include HER, QDagger, LOGO and AWAC as additional baselines. As some of the agent we tested require pre-training, we train a simple {\em Behavioral Cloning} (BC)~\cite{torabi2018bc} baseline and use it as expert policy. Each BC agent is trained on the same dataset that we leverage with our method. To select the best possible expert policy, we extensively tested each BC policy during training and retained only the one with the highest mean reward. Our source code for the experiment is available at [hidden information]\footnote{The link is temporarily removed to preserve anonimity of the submission}.

We demonstrate our approach a total of five environments from the Miniworld~\cite{chevalierboisvert2023miniworld} benchmark. Specifically, we test the agents on three navigation environments, {\em OneRoom}, {\em FourRooms}, and {\em MazeS3}; additionally, we include a constrained navigation environment, {\em Sidewalk}, and survival-based task named {\em CollectHealth} from the same suite. These environments are partially observable and feature $80 \times 60$ RGB images as input, which we resize to $64 \times 64$ for convenience; additionally, the reward is sparse and delayed, being observed at the end of an episode {\em only if the agent is successful}. The action space is discrete.

\paragraph{Data collection}
For each environment we manually collected approximately 5 minutes of experience, amounting to 10 trajectories for more complex tasks (e.g. {\em MazeS3}) and 20 trajectories for easier ones, such as {\em OneRoom}. On average, each trajectory lasts around $150$ timesteps for complex tasks and $75$ for quicker ones, hence an agent has approximately $1500$ transitions per task at its disposal.

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=0.97\linewidth]{figures/miniworld_results.pdf}
    \caption{Comparison between our agent (red) compared to five baselines, cDQL (blue), QDagger (orange), HER (green), LOGO (purple), and AWAC (brown). Agents are trained for $2.5$M timesteps. Each training is repeated five times on different seeds.}
    \label{fig:training_res}
\end{figure*}

\paragraph{Network architectures}
Our DQN implementation is taken from CleanRL~\cite{huang2021cleanrl}. All other models are built on top of the same code base, preserving the original as much as possible and implementing only the algorithm-specific changes to it. For LOGO, we re-implemented a basic version of TRPO and edited it, following the source code provided by the authors of the original paper. All our DQN-derived policies are MLPs with two hidden layers, of $256$ units each. For TRPO, we use MLPs with two hidden layers, equipped with $512$ units each.

To process visual inputs, we train a beta-variational autoencoder ($\beta$-VAE)~\cite{kingma2022vae,higgins2017betavae} neural network with four convolutional layers, followed by a flattening layer and a dense layer to project the flattened features into the desired latent space. The network takes a single $64\times64$ RGB image as input and outputs a latent of size $128$. Each $\beta$-VAE is trained to simply reconstruct the input image, with no conditioning on action selection. We pre-train an encoder for each environment using a dataset of $400$ trajectories collected by a random policy. After training, we detach the decoder and only use the encoder as features extractor. All tested agents share the same features extractor. A list of hyperparameters is available in Appendix~\ref{app:A}.

\section{Results}
\label{sec:results}
\begin{table*}[t]
    \centering
    \caption{Numerical results for our training runs on the two most representative environments, {\em OneRoom} and {\em Sidewalk}. All agents have been trained for five times on five different seeds. For each agent, we report the mean reward and standard deviation obtained during training every $500$k timesteps. Full results are available in Appendix~\ref{app:B}.}
    \begin{tabular}{ccccccc}
        \toprule
        \thead{Environment} & \thead{Algorithm}  & \thead{Reward@500k} & \thead{Reward@1M} & \thead{Reward@1.5M} & \thead{Reward@2M} & \thead{Reward@2.5M}\\
        \midrule
         & cDQL & $0.349 \pm 0.044$ & $0.796 \pm 0.061$ & $0.845 \pm 0.022$ & $0.867 \pm 0.028$ & $0.851 \pm 0.022$ \\
         & HER & $0.067 \pm 0.024$ & $0.045 \pm 0.010$ & $0.046 \pm 0.019$ & $0.020 \pm 0.010$ & $0.029 \pm 0.014$ \\
         & QDagger & $\mathbf{0.679 \pm 0.032}$ & $0.662 \pm 0.014$ & $0.608 \pm 0.027$ & $0.606 \pm 0.033$ & $0.591 \pm 0.026$ \\
        {\em OneRoom} & LOGO & $0.209 \pm 0.049$ & $0.137 \pm 0.032$ & $0.106 \pm 0.016$ & $0.078 \pm 0.040$ & $0.087 \pm 0.029$ \\
         & AWAC & $0.129 \pm 0.032$ & $0.135 \pm 0.066$ & $0.149 \pm 0.021$ & $0.137 \pm 0.032$ & $0.109 \pm 0.027$ \\
         & \makecell{\textbf{AE} \\ \textbf{(Ours)}} & $0.669 \pm 0.062$ & $\mathbf{0.817 \pm 0.031}$ & $\mathbf{0.873 \pm 0.029}$ & $\mathbf{0.853 \pm 0.029}$ & $\mathbf{0.876 \pm 0.018}$ \\
        \midrule
         & cDQL & $0.025 \pm 0.047$ & $0.301 \pm 0.204$ & $0.520 \pm 0.074$ & $0.601 \pm 0.063$ & $0.673 \pm 0.055$ \\
         & HER & $0.000 \pm 0.000$ & $0.000 \pm 0.000$ & $0.000 \pm 0.000$ & $0.000 \pm 0.000$ & $0.000 \pm 0.000$ \\
         & QDagger & $\mathbf{0.283 \pm 0.099}$ & $0.393 \pm 0.104$ & $0.408 \pm 0.088$ & $0.438 \pm 0.047$ & $0.373 \pm 0.104$ \\
        {\em Sidewalk} & LOGO & $0.003 \pm 0.004$ & $0.000 \pm 0.000$ & $0.004 \pm 0.005$ & $0.002 \pm 0.004$ & $0.002 \pm 0.003$ \\
         & AWAC & $0.000 \pm 0.000$ & $0.000 \pm 0.000$ & $0.000 \pm 0.000$ & $0.000 \pm 0.000$ & $0.000 \pm 0.000$ \\
         & \makecell{\textbf{AE} \\ \textbf{(Ours)}} & $0.148 \pm 0.040$ & $\mathbf{0.501 \pm 0.038}$ & $\mathbf{0.646 \pm 0.066}$ & $\mathbf{0.727 \pm 0.046}$ & $\mathbf{0.769 \pm 0.060}$ \\
        \bottomrule
    \end{tabular}
    \label{tab:partial_res}
\end{table*}

The results of our experiments are shown in Figure~\ref{fig:training_res}. For each agent, we report the value of the reward on the y-axis and the number of timesteps on the x-axis. Our results are also publicly available on Weight\&Biases~\cite{wandb} at [hidden information]\footnote{The link is temporarily removed to preserve anonimity of the submission}, along with additional relevant metrics and media. Additionally, we report a subset of numerical results in Table~\ref{tab:partial_res}, highlighting the change in mean reward as train progresses. The full version of these results is available in Appendix~\ref{app:B}.

Our method improves sample efficiency of cDQL in all tasks. This is especially visible in {\em OneRoom}, {\em Sidewalk} and {\em CollectHealth}. Notably, vanilla cDQL completely fails in the latter task, while our agent can in some cases solve it. Similarly, agent trained with QDagger and AWAC learns to successfully collect some medkits. We point out that the environment is quite difficult for all tested agents, as it requires to approach medkits and then actively gathering them by pressing a key, while the medkit is not in sight. This is a design feature of the environment itself.

Numerically, in {\em OneRoom} our agent reaches an average reward of $0.8$ after around $750$k timesteps, while the cDQL baseline only reaches the same performance after roughly $1.2$M steps. That is, speeding up convergence of $37.5$\%. No other baseline method converges to that value of the reward. Similarly, in {\em Sidewalk} our agent reaches $0.6$ average reward after around $1.2$M, while vanilla cDQL reaches the same performance at $1.8$M, that is, using $33.3$\% more transitions. Notably, cDQL with Adversarial Estimates converges at an average reward of $0.738 \pm 0.062$, while vanilla cDQL only reaches $0.663 \pm 0.215$. This fact follows from our claim that our method does not bound a policy to follow the expert, rather encourages to improve over it. We highlight that HER, LOGO and AWAC completely fail the task, while QDagger converges to a mean average of $0.373 \pm 0.018$. Notably, in both tasks QDagger obtains a higher mean reward than our method in the early stages of training as a result of pre-training. Then, QDagger clearly remains trapped in a local minimum. Contrary to the findings of the authors~\cite{agarwal2022reincarnatingrl}, in our experiments using a suboptimal BC agent to pre-train QDagger had an impact on its performance. We attribute this mismatch to the difficulty of solving an image-based, partially observable environment: as the information available to an agent is very limited, QDagger might require a near-optimal expert.

AWAC and HER completely fail {\em Sidewalk}. Despite showing some promise at the beginning of training, also LOGO is unable to leverage the expert data for this task. Additionally, all three methods achieve only faint results in {\em OneRoom} (mean reward around $0.1$). This last result is unexpected, especially after the promising starts that all agents showed.

Despite our best efforts, our results differ significantly from the ones reported in the original papers. We believe that this gap is to be attributed to the inherent difficulty of the tasks in Miniworld. In particular, the observation carries a small amount of information. Additionally, the reward is observed only on the last timestep of an episode, and only in case of success. As such, it is quite common to observe no reward in the early stages of training, hence preventing learning or severely limiting the capabilities of an agent.

We highlight the smaller improvement of our method on {\em MazeS3} and in {\em FourRooms} over the standard cDQL. In the first case, using Adversarial Estimates allows cDQL to converge to a higher reward, $0.381 \pm 0.051$, in contrast with the baseline that tops at $0.357 \pm 0.044$. Notably, our method reaches the same performance as the baseline in $860$k timesteps, hence using $65.6$\% less frames than regular cDQL. In {\em FourRooms}, using Adversarial Estimates improves over the baseline (cDQL $0.308 \pm 0.246$, AE $0.377 \pm 0.048$), and matches the baseline best reward after only $975$k timesteps, just above a third of the time. Like other tasks, QDagger is the only direct competitor of our approach. Still, while it shows better behavior in the early stages of training, also on this task QDagger converges to a local minimum. Similarly to the other tasks, AWAC, LOGO and HER are unable to complete {\em FourRooms} reliably, reaching a maximum performance of $0.068 \pm 0.012$, $0.044 \pm 0.013$ and $0.018 \pm 0.005$ respectively. However, we notice that all compared agents converge to a low reward. We explain this shortcoming with the difficulty of the environments: in particular, learning to navigate in complex environments with very sparse rewards represents a considerable challenge, especially with a relatively tight time budget. As such, we believe that relaxing these constraints might lead to an improvement in performance.

\section{Conclusions}
\label{sec:conclusions}
We have presented {\em Adversarial Estimates}, a new approach to improve the sample efficiency of a DRL agent based on game theory and search. Our approach improves over the other approaches, as it is more easily and widely applicable. Moreover, its requirements are very loose, it requires no pre-training, and its effects are visible with as low as five minutes of collected experience.

\paragraph{Limitations}
Adversarial Estimates is far from being perfect. While being useful, it can be applied only on tasks where previous experience can be collected. Also, while not explicitly requiring optimal data, using very low quality data, such as a dataset gathered from a random policy, would likely result in a significant loss in performance. Finally, our approach is based on over-estimating the Q-values of the expert. As such, whenever such estimation fails, our approach will fail as well.

\paragraph{Future work}
In our work, we have studied Adversarial Estimates as a way to kickstart training in off-policy DRL algorithms. However, we hypothesize that our formulation could be extended to on-policy methods as well. In particular, actor-critic on-policy algorithms such as TRPO~\cite{schulman2017trpo} and {\em Proximal policy Optimization} (PPO)~\cite{schulman2017ppo} rely on a value function critic providing a feedback $V(s)$, rather than an action-value estimate $Q(s, a)$. We believe that introducing an action-value critic could be key to make our approach applicable to such algorithms. 

Another interesting research direction would include combining our approach with other approaches. For instance, it would be interesting to combine our approach with QDagger, perhaps leveraging our search-based policy as teacher, hence improving the pre-training phase.

\section*{Impact Statement}
This paper presents a new approach for improving sample efficiency in Reinforcement Learning (RL). We believe that our contributions could pose another step towards making RL more applicable in real world scenarios. In particular, improving sample efficiency with the use of minimal data leads to better and faster convergence, hence lowering the costs of such applications and increasing their appeal. Simultaneously, using small datasets reduces the cost of gathering examples, leading to more contained time and monetary budgets.

\bibliography{biblio}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Hyperparameters}
\label{app:A}
\begin{table}[H]
    \centering
    \caption{Hyperparameters used for training the $\beta$-VAE.}
    \begin{tabular}{c|c}
        \thead{Name} & \thead{Value} \\
        \midrule
         latent size & $128$ \\
         channels & $[3, 32, 64, 128]$ \\
         kernel size & $[4, 4, 4, 4]$ \\
         stride & $[2, 2, 2, 2]$ \\
         \midrule
         learning rate & $3 \times 10^{-4}$ \\
         batch size & $128$ \\
         epochs & $30$ \\
         temperature & $0.0 \rightarrow 5 \times 10^{-8}$ \\
         temp. increase & $0.1 * \mathrm{epochs} \rightarrow 0.9 * \mathrm{epochs}$ \\
    \bottomrule
    \end{tabular}
    \label{tab:encoder_params} 
\end{table}
In this section, we list the hyperparameters we have used to train our models, to ensure reproducibility. More specifically, in Tables~\ref{tab:encoder_params} and ~\ref{tab:cdql_params} we report the hyperparameters we used to train the VAE encoders and the common hyperparameters of cDQL, respectively. These parameters are shared across all algorithms. Additionally, in each subsection we disclose the algorithm-specific parameters that we have used.
\begin{table}[H]
    \centering
    \caption{Hyperparameters used for training all instances of cDQL.}
    \begin{tabular}{c|c}
        \thead{Name} & \thead{Value} \\
        \midrule
         learning rate & $1 \times 10^{-4}$ \\
         $\gamma$ & $0.99$ \\
         $\tau$ & $1.0$ \\
         buffer size & $250000$ \\
         batch size & $32$ \\
         $\epsilon$ & $1.0 \rightarrow 0.05$ \\
         exploration fraction & $0.1$ \\
         target train frequency & $1000$ \\
         train frequency & $4$ \\
    \bottomrule
    \end{tabular}
    \label{tab:cdql_params} 
\end{table}
For cDQL, we retain most of the values used in the CleanRL code. Due to memory constraints, we resize the buffer size to a quarter of the original size. Consistently, we reduce the timesteps budget for our agents down to a quarter of the original size. We justify this choice by highlighting that all the agents have converged after $2.5$M steps. Additionally, by equally scaling down these two parameters, we ensure that the buffer size is completely filled by the time the end of exploration occurs, hence ensuring plenty of time for an agent to explore the state space extensively.
\subsection{HER}
HER does not have specific hyperparameters. However, we tuned the number of subgoals to be imposed for each batch, and found the best results for $n = 16$. As such, after the regular sampling from the replay buffer, we sample an additional $16$ transitions and impose a reward of $1$.
\subsection{LOGO}
\begin{table}[H]
    \centering
    \caption{A list of hyperparameters used for LOGO.}
    \begin{tabular}{c|c}
        \thead{Name} & \thead{Value} \\
        \midrule
         \# rollouts & $10$ \\
         max KL & $0.01$ \\
         \makecell{batch size \\ (discriminator)} & $256$ \\
         learning rate & $3 \times 10^{-4}$ \\
         \bottomrule
    \end{tabular}
    \label{tab:logo_params}
\end{table}
In the partial observability case, LOGO uses an additional discriminator network to discern between the state-actions pairs $(s, a)$ generated by the policy and the ones generated by the expert. In our experiments, we train the discriminator using the same hyperparameters as the original code base, that is, a batch size of $256$ and a learning rate of $3 \times 10^{-4}$. The other hyperparameters listed in Table~\ref{tab:logo_params}, \# rollouts and max KL, refer to the number of rollouts to gather before an update and the size of the trust region, respectively.
\subsection{QDagger}
\begin{table}[H]
    \centering
    \caption{List of hyperparameters used in our QDagger implementation and their relative values.}
    \begin{tabular}{c|c}
        \thead{Name} & \thead{Value} \\
        \midrule
         teacher steps & $125000$ \\
         offline steps & $125000$ \\
         teacher evaluation episodes & $10$ \\
         $\tau$ & $1.0$ \\
         $\lambda$ & $1.0$ \\
         \bottomrule
    \end{tabular}
    \label{tab:qdagger_params}
\end{table}
QDagger uses an additional pre-training phase on a replay buffer of transitions sampled by the expert policy. In our experiments, we train a simple BC policy using our dataset and use it to gather $125$k transitions. Then, we pre-train the agent on the BC-collected dataset for an additional $125$k steps. Finally, the agent is trained online with an additional distillation term balanced with $\lambda$. The hyperparameters are summarized in Table~\ref{tab:qdagger_params}.
\subsection{AWAC}
\begin{table}[H]
    \centering
    \caption{Hyperparameters used for training cDQL using the AWAC update.}
    \begin{tabular}{c|c}
        \thead{Name} & \thead{Value} \\
        \midrule
         offline steps & $100000$ \\
         offline eval episodes & $10$ \\
         $\lambda$ & $0.3$ \\
    \end{tabular}
    \label{tab:awac_params}
\end{table}
AWAC features a pre-training phase. We report the specific hyperparameters in Table~\ref{tab:awac_params}. In our experiments, we pre-train the agent for $100$k steps and keep track of its performance by testing it on $10$ evaluation episodes every $1000$ forward passes. The offline and online phases use the same closed-form update derived by the authors. Additionally, a scaling term $\lambda=0.3$ balances the effect of the update.
\subsection{Adversarial Estimates}
Our method does not use additional hyperparameters w.r.t. to the cDQL baseline. One exception is an optional scaling factor $\lambda \in [0, 1]$ to balance the effect of the adversarial estimates on the total loss. In our experiments, we leave $\lambda = 1.0$.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\section{Full training results}
\label{app:B}
\begin{table*}[t]
    \centering
    \caption{Full version of Table~\ref{tab:partial_res}, shown in Section~\ref{sec:results}.}
    \begin{tabular}{ccccccc}
        \toprule
        \thead{Environment} & \thead{Algorithm}  & \thead{Reward@500k} & \thead{Reward@1M} & \thead{Reward@1.5M} & \thead{Reward@2M} & \thead{Reward@2.5M}\\
        \midrule
         & cDQL & $0.349 \pm 0.044$ & $0.796 \pm 0.061$ & $0.845 \pm 0.022$ & $0.867 \pm 0.028$ & $0.851 \pm 0.022$ \\
         & HER & $0.067 \pm 0.024$ & $0.045 \pm 0.010$ & $0.046 \pm 0.019$ & $0.020 \pm 0.010$ & $0.029 \pm 0.014$ \\
         & QDagger & $\mathbf{0.679 \pm 0.032}$ & $0.662 \pm 0.014$ & $0.608 \pm 0.027$ & $0.606 \pm 0.033$ & $0.591 \pm 0.026$ \\
        {\em OneRoom} & LOGO & $0.209 \pm 0.049$ & $0.137 \pm 0.032$ & $0.106 \pm 0.016$ & $0.078 \pm 0.040$ & $0.087 \pm 0.029$ \\
         & AWAC & $0.129 \pm 0.032$ & $0.135 \pm 0.066$ & $0.149 \pm 0.021$ & $0.137 \pm 0.032$ & $0.109 \pm 0.027$ \\
         & \makecell{\textbf{AE} \\ \textbf{(Ours)}} & $0.669 \pm 0.062$ & $\mathbf{0.817 \pm 0.031}$ & $\mathbf{0.873 \pm 0.029}$ & $\mathbf{0.853 \pm 0.029}$ & $\mathbf{0.876 \pm 0.018}$ \\
        \midrule
         & cDQL & $0.193 \pm 0.042$ & $0.219 \pm 0.040$ & $0.250 \pm 0.042$ & $0.282 \pm 0.046$ & $0.308 \pm 0.031$ \\
         & HER & $0.061 \pm 0.024$ & $0.017 \pm 0.011$ & $0.011 \pm 0.010$ & $0.013 \pm 0.013$ & $0.018 \pm 0.005$ \\
         & QDagger & $\mathbf{0.221 \pm 0.059}$ & $0.218 \pm 0.029$ & $0.171 \pm 0.030$ & $0.169 \pm 0.023$ & $0.156 \pm 0.021$ \\
        {\em FourRooms} & LOGO & $0.080 \pm 0.014$ & $0.051 \pm 0.027$ & $0.041 \pm 0.013$ & $0.069 \pm 0.014$ & $0.044 \pm 0.013$ \\
         & AWAC & $0.077 \pm 0.022$ & $0.084 \pm 0.024$ & $0.061 \pm 0.018$ & $0.082 \pm 0.021$ & $0.067 \pm 0.012$ \\
         & \makecell{\textbf{AE} \\ \textbf{(Ours)}} & $0.150 \pm 0.056$ & $\mathbf{0.271 \pm 0.052}$ & $\mathbf{0.324 \pm 0.038}$ & $\mathbf{0.360 \pm 0.035}$ & $\mathbf{0.377 \pm 0.048}$ \\
        \midrule
         & cDQL & $0.025 \pm 0.047$ & $0.301 \pm 0.204$ & $0.520 \pm 0.074$ & $0.601 \pm 0.063$ & $0.673 \pm 0.055$ \\
         & HER & $0.000 \pm 0.000$ & $0.000 \pm 0.000$ & $0.000 \pm 0.000$ & $0.000 \pm 0.000$ & $0.000 \pm 0.000$ \\
         & QDagger & $\mathbf{0.283 \pm 0.099}$ & $0.393 \pm 0.104$ & $0.408 \pm 0.088$ & $0.438 \pm 0.047$ & $0.373 \pm 0.104$ \\
        {\em Sidewalk} & LOGO & $0.003 \pm 0.004$ & $0.000 \pm 0.000$ & $0.004 \pm 0.005$ & $0.002 \pm 0.004$ & $0.002 \pm 0.003$ \\
         & AWAC & $0.000 \pm 0.000$ & $0.000 \pm 0.000$ & $0.000 \pm 0.000$ & $0.000 \pm 0.000$ & $0.000 \pm 0.000$ \\
         & \makecell{\textbf{AE} \\ \textbf{(Ours)}} & $0.148 \pm 0.040$ & $\mathbf{0.501 \pm 0.038}$ & $\mathbf{0.646 \pm 0.066}$ & $\mathbf{0.727 \pm 0.046}$ & $\mathbf{0.769 \pm 0.060}$ \\
        \midrule
         & cDQL & $-1.676 \pm 0.618$ & $-1.749 \pm 0.392$ & $-1.784 \pm 0.362$ & $-1.954 \pm 0.075$ & $-1.612 \pm 0.683$ \\
         & HER & $-1.624 \pm 0.330$ & $-1.688 \pm 0.441$ & $-1.985 \pm 0.017$ & $-1.333 \pm 0.492$ & $-1.580 \pm 0.553$ \\
         & QDagger & $\mathbf{9.466 \pm 1.373}$ & $\mathbf{10.025 \pm 2.637}$ & $\mathbf{10.827 \pm 2.358}$ & $\mathbf{9.464 \pm 2.023}$ & $\mathbf{10.616 \pm 4.139}$ \\
        {\em CollectHealth} & LOGO & $-2.000 \pm 0.000$ & $-2.000 \pm 0.000$ & $-2.000 \pm 0.000$ & $-2.000 \pm 0.000$ & $-2.000 \pm 0.000$ \\
         & AWAC & $1.133 \pm 0.530$ & $0.952 \pm 0.809$ & $1.063 \pm 0.695$ & $1.226 \pm 1.156$ & $0.801 \pm 0.698$ \\
         & \makecell{\textbf{AE} \\ \textbf{(Ours)}} & $2.400 \pm 1.583$ & $3.839 \pm 3.780$ & $1.708 \pm 1.912$ & $4.530 \pm 2.540$ & $3.309 \pm 2.786$ \\
        \midrule
         & cDQL & $0.253 \pm 0.059$ & $0.357 \pm 0.058$ & $0.364 \pm 0.027$ & $0.351 \pm 0.044$ & $0.357 \pm 0.044$ \\
         & HER & $0.040 \pm 0.022$ & $0.035 \pm 0.015$ & $0.029 \pm 0.008$ & $0.017 \pm 0.015$ & $0.015 \pm 0.013$ \\
         & QDagger & $\mathbf{0.388 \pm 0.044}$ & $\mathbf{0.452 \pm 0.035}$ & $\mathbf{0.435 \pm 0.029}$ & $0.394 \pm 0.026$ & $0.359 \pm 0.034$ \\
        {\em MazeS3} & LOGO & $0.120 \pm 0.034$ & $0.093 \pm 0.040$ & $0.112 \pm 0.025$ & $0.112 \pm 0.028$ & $0.108 \pm 0.027$ \\
         & AWAC & $0.082 \pm 0.038$ & $0.098 \pm 0.022$ & $0.104 \pm 0.016$ & $0.134 \pm 0.033$ & $0.137 \pm 0.031$ \\
         & \makecell{\textbf{AE} \\ \textbf{(Ours)}} & $0.301 \pm 0.058$ & $0.363 \pm 0.044$ & $0.397 \pm 0.057$ & $\mathbf{0.406 \pm 0.055}$ & $\mathbf{0.381 \pm 0.051}$ \\
        \bottomrule
    \end{tabular}
    %\caption{A summary of the most relevant previous research used as inspiration for our work. For each source we report a brief description, applicability, reliance on expert optimality, and an overview of relevant features.}
    \label{tab:full_results}
\end{table*}
Table~\ref{tab:full_results} summarizes the results of our experiments as reported in Section~\ref{sec:results} (Figure~\ref{fig:training_res}). As discussed in the main paper, adversarial estimates improves sample efficiency \textbf{and} performance on every task, with respect to the cDQL baseline. Notably, QDagger improves over our method in {\em CollectHealth}. Additionally, its pre-training phase allows it to reach higher mean reward at $500$k steps on every task. Despite this, Adversarial Estimates improves in the later stages, possibly due to the expert optimality assumption of the BC agent. By following it during pre-training, QDagger converges to a local minimum in three tasks ({\em OneRoom}, {\em FourRooms} and {\em Sidewalk}), hence demonstrating the partial disadvantage of relying on previously trained agents for policy training.
\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
