\section{Results}

\begin{figure*}
	\centering
    \includegraphics[width=\linewidth]{figures/likert.pdf}	
	\caption{Significant effects of embodiment on attributing contributions to the AI, perceived communication and partnership, and attention paid toward the AI. The plots show the percentages of participants who gave a response to a question, grouped by whether embodiment was enabled in the corresponding modeling session or not.}
 \Description{Likert scale plots for the questionnaires}
	\label{fig:embodiment-plots}
\end{figure*}

In this section, we report our empirical study's quantitative and qualitative results.

\subsection{Quantitative Results}

We analyzed the influence of our independent variables on the dependent variables using inferential statistics. 

\subsubsection{Number of Engagements}

We analyzed the number of engagements to measure the participant's interest in contributing to the model (see \autoref{fig:results:nr_of_engagements}). We defined the number of engagements as the total number of selections, modifications, and undo actions that a user performed. We found results ranging from \val{16.0}{6.0} (with \ivHigh{} and \ivInc{} without \ivEmb{}) to \val{21.3}{8.4} (without any ai contribution visualization).

For statistical analysis, we fitted Poisson
regression models and applied Type III Wald chi-square tests for significance testing. The analysis indicated a significant (\chisq{1}{6.29}{<.05}) main effect of \ivHigh{} on the number of engagements that was, however, not supported by post-hoc tests.

Further, we found a significant (\chisq{1}{6.68}{<.01}) interaction effect between \ivHigh{} and \ivInc{}. Without \ivHigh{}, the addition of an \ivInc{} significantly ($<.05$) reduced the number of engagements. With \ivHigh{}, however, the addition of \ivInc{} seemingly increased the number of engagements ($n.s.$)

Finally, we found a significant (\chisq{1}{4.86}{<.05}) interaction effect between \ivHigh{} and \ivEmb{}. Here, we found the same trend where without \ivHigh{}, \ivEmb{} reduced the number of engagements while increasing it with \ivHigh{}. However, these results were not significant.

\subsubsection{Painting Time}

As another measure for the engagement of the participants, we measured the time participants spent painting parts of the model (see \autoref{fig:results:nr_of_engagements}). We found painting times ranging from \valS{67.2 }{37.1} (with all ai contribution visualizations) to \valS{108.0}{47.8} (without any ai contribution visualization).

We analyzed the data using 3-way repeated measures ANOVA. We tested the data for violations of normality and sphericity assumptions using Shapiro-Wilk's and Mauchly's tests and found no violations.

The analysis indicated a significant (\anova{1}{15}{4.926}{<.05}) main effect of the \ivInc{} on the painting time. Post-hoc tests confirmed significantly ($p<.05$) higher painting times without the \ivInc{}.

Further, the analysis indicated a significant (\anova{1}{15}{9.349}{<.01}) main effect of the \ivEmb{} on the painting time. Again, post-hoc tests confirmed significantly ($p<.01$) higher painting times without the \ivEmb{}.

\subsection{Questionnaire Results}
We analyzed the responses of our participants using the Aligned-Rank Transform (ART) procedure as proposed by \citet{wobbrock_aligned_2011} followed by ART-C~\cite{elkin2021} corrected using Tukey's method for post-hoc comparisons where appropriate. 
\autoref{fig:embodiment-plots} depicts some of the most interesting effects found in our analysis. 



\subsubsection{Perception of AI}
We assess the perception of AI in general by analyzing the questions on predictability, support, efficiency, and competence.
None of the visualization aspects significantly affected predictability, efficiency, or competence. However, in conditions with embodiment, participants rated the system as significantly more supportive ($F(1, 15)=5.74$, $p<.05$).

\subsubsection{Perception of AI output}
We assess the perception of AI output by analyzing the questions on unexpectedness, unusualness, value, and creativity.
There were no significant differences in ratings for unusualness, value, or creativity for any visualization aspects. However, in conditions with incremental visualization, participants rated the finished 3D model as significantly more unexpected ($F(1, 15)=7.52$, $p<.05$).

\subsubsection{Collaborative experience}
We assess the collaborative experience by analyzing the partnership, communication, alignment, and agency questions.
There were no significant differences in ratings for alignment or agency. However, with embodiment, participants rated communication significantly higher ($F(1, 15)=26.19$, $p<.001$). Additionally, participants rated partnership lower when highlighting was enabled ($F(1, 15)=6.18$, $p<.05$) and higher when embodiment was enabled ($F(1, 15)=23.5$, $p<.001$). We also found a significant interaction of highlighting and embodiment concerning this measure ($F(1, 15)=11.02$, $p<.01$). Post-hoc tests do not show significant differences in ratings between the individual variable combinations.

\subsubsection{Appeal}
We assess the overall system appeal by analyzing the questions on excitement, motivation, affinity, and enjoyment.
There were no significant differences in ratings for motivation and affinity. However, in conditions with highlighting, participants enjoyed using the system significantly less ($F(1, 15)=7.89$, $p<.05$). Additionally, we found a significant interaction of highlighting and embodiment concerning the measure of excitement ($F(1, 15)=5.83$, $p<.05$). Post-hoc tests do not show significant differences in ratings between the individual variable combinations.

\subsubsection{Behavioral engagement}
We assess behavioral engagement by analyzing the questions from the Behavioral Engagement Scale.
Only two questions showed significant differences between the enablement of any visualization aspects: The extent to which the AI affected what the participant did was larger when incremental visualization was enabled ($F(1, 15)=6.54$, $p<.05$), and the amount of attention participants paid to the AI was more significant with embodiment ($F(1, 15)=9.43$, $p<.01$). We also found a three-way interaction of all visualization aspects concerning the latter variable ($F(1, 15)=7.87$, $p<.05$); however, post-hoc tests do not show significant differences in ratings between the individual variable combinations.

\subsubsection{Closeness, satisfaction, and contribution}
We assess these factors by analyzing satisfaction, contribution, and closeness questions.
There were no significant differences in closeness to the AI as measured by the IOS Scale. However, in conditions with highlighting, participants were significantly less satisfied with the finished 3D model ($F(1, 15)=5.06$, $p<.05$). In conditions with embodiment, participants attributed significantly more of the contribution to the finished 3D model to the AI ($F(1, 15)=9.19$, $p<.01$).

\subsection{Qualitative Results}
\subsubsection{Data  Analysis}
We collected qualitative data in a final semi-structured interview.
Our data analysis procedure is based on Braun and Clarke's \cite{braun_using_2006} step-by-step approach of thematic analysis.
While two authors conducted the analysis, the results and intermediate steps were discussed among all authors.
The analysis was performed based on the transcripts of the audio-recorded interviews. The interview responses were a mix of English and German. All quotes were translated into English by the authors, who are fluent in English and German.
The two authors first familiarized themselves with the data by reading the transcripts. 
We used Delvetool~\footnote{Delvetool for qualitative data analysis \url{https://app.delvetool.com/}, last accessed: August 30, 2024} for coding the data.
In the first analysis step, both authors read the same subset of transcripts (8 transcripts/50\%) separately and created initial codes inductively. They then discussed the codes and formed themes by grouping them using an online mind map tool~\footnote{Miro tool for online mind maps: \url{https://miro.com/}, last accessed: August 30, 2024}. After the initial theme generation, all authors discussed the themes before one of the authors coded the remaining transcripts using the updated codes. Some new codes were merged with the existing themes during the second coding stage. Finally, the two authors met again to discuss the final codes and update and name the themes.


\begin{figure*}
	\centering
		\subfloat[\centering Dinosaur]{{\includegraphics[width=3.6cm]{figures/models-dinosaur.jpg} }}\vspace{.07cm}
		\subfloat[\centering Pickup]{{\includegraphics[width=3.6cm]{figures/models-pickup.jpg} }}\vspace{.07cm}
		\subfloat[\centering Chair]{{\includegraphics[width=3.6cm]{figures/models-chair.jpg} }}\vspace{.07cm}
		\subfloat[\centering House]{{\includegraphics[width=3.6cm]{figures/models-house.jpg} }}\vspace{.3cm}
	\caption{The four modifiable objects in their default form before any modifications.}%
 \Description{Four 3D objects: A dinosaur, a pickup, a chair, a house}
	\label{fig:modmodels}%
\end{figure*}

\begin{figure*}
	\centering
	\includegraphics[width=.85\linewidth]{figures/finished-models.pdf}		
	\caption{A selection of finished models designed by participants, each influenced differently by the AI or its contributions.}
 \Description{Four modified objects: A purple dinosaur, a colorful pickup, a colored chair with a pair of antlers, and a colorful house.}
	\label{fig:inflmodels}
\end{figure*}

\subsubsection{Codes and Themes}
Coding the first half of the transcripts produced 103 codes.
They were then grouped into 21 initial summary codes.
Before coding the second half of the transcripts, we discussed and summarised the codes into 73 unique codes.
Next, the second half of the transcript was coded, during which we added 15 codes to the coding set and merged them with the existing ones into 20 summary codes. We then formed the following four themes based on the summary codes\footnote{See the mind map of the codes in the supplementary materials}.

\subsubsection{Task Division and Control}
\textbf{Participants experienced varying levels of control over the AI, ranging from dependency to independence and different hierarchical levels.}
One key aspect discussed by participants was how to divide tasks between themselves and the AI, particularly concerning the level of control each would have (or desire to have) over the task.
Their comments revealed a relationship axis involving a degree of dependence/independence and, in the case of dependence, a hierarchy of interaction. The participants' comments were quite diverse, showing no standard agreement on the type of control they felt over the AI.
Some participants (P13, P15) mentioned they did not feel they were working with the AI. Others noted that they felt like they were co-working with the AI but not collaborating (P8): \textit{"It felt more like ... when you're [..] doing exercise sheets, and you have a partner who never talks to you and just hands something in without you even noticing." }. Some participants also stated that they ignored the input from the AI (P15).
Although the AI's input was indeed predetermined, some users felt in control of its actions and described a hierarchical relation (P1). They viewed the AI as a tool rather than a collaborator or co-worker. In this context, participants felt that the AI should support the user.
The third relationship we observed was that of a partner, where participants felt they were pursuing a common goal with the AI.
While many participants did not note any particular influences of embodiment on the relationship, P5 explained that they felt that collaboration was more relevant to them than working with an embodied AI.
Finally, some participants described feeling that the AI was in control and that their decisions were overruled by it (P2, P8).

Participants also wished for different types of relationships when asked for improvements or desired changes in the interaction.
Some participants suggested a form of task sharing with the AI, where both would choose from a set of options (P4).
Other participants desired a more hierarchical way of control, where they wanted to give commands to the AI (P1, P2), while others were okay with the AI working on its own as long as the user could make the final decision about accepting or rejecting its decisions (P2): \textit{So, [the AI] should be able to try out things, but in any case, it should not override my decisions.}
These wishes are closely related to the desire for better communication between AI and the user (P3, P6). For example, the AI should indicate whether they understood the user's request and whether the requested action was possible (P3). P2 also indicated they would like to receive feedback from the AI about its perception of the task.

In addition to spoken or verbal feedback, participants indicated that they would like to interact with the AI through different modalities and meta-level instructions, for example, by pointing at things or by making general style requests that are then performed by the AI (P2: \textit{"for example I can write: I want to make a goth house, and then the AI automatically suggests goth windows and so on.}).


\subsubsection{AI as Enabler of New Experiences}
\textbf{Interacting with the AI sparked creativity among participants and enabled new experiences.}
We observed that some participants felt the AI enabled them to have more creative experiences. For example, participants liked the AI's suggested modifications (P1). P8 liked that these suggestions guided them towards a satisfying result, and P7 appreciated that the AI suggestions led to new inputs: \textit{[...] so when they [the AI] conjured something new, then, of course, one got new input.} P2 also mentioned that they felt inspired by the AI: \textit{[...] the things from the AI also inspired me during the process.} Some participants even mentioned that they felt the AI helped them to be more creative and open (P7, P8), stating that they were happy with the result, although they, for example, chose a color scheme that they would not have gone for themselves. %

\subsubsection{Emotional Experience with AI} %
\textbf{Participants had both positive and negative experiences with the AI.}
Participants generally found the experience interesting (P2, P6, P7).
Their opinions and experiences with the AI were both positive and negative. On the positive side, participants felt that the AI created funny results and a funny experience in general (P3, P4). On the negative side, participants mentioned that they were confused about the AI and its created outcome (P2). They also did not like the slow response time or waiting for the AI to finish their modifications (P4, P5). Finally, some participants felt surprised (P3, P4) by the suggestions or distracted by the AI output (P1, P3).

\subsubsection{Experiences with Embodied AI}
\textbf{The embodied AI evoked a variety of emotions.}
The participants experienced quite diverse emotions when interacting with the embodied AI. Some found it annoying because they had to wait for the embodied AI to finish the animation (P5) and disliked it being slow (P3, P4, P5, P7). While some participants liked to watch the AI, others felt it was distracting (P3). However, participants also felt curiosity when working with the embodied AI (P8). 

Despite these negative feelings, participants also expressed positive emotions towards the AI. For example, they perceived it to be helpful (P1), cute (P2), or funny (P2). Participants also made comments about the appearance of the embodied AI. Some participants found it uncanny and weird (P3). Some participants specifically criticized the avatar's permanent smiling expression, explaining that \textit{"smiling for no reason is creepy"} (P11) and mentioned that they did not prefer the AI to be human-like (P7). Indeed, several participants noted that they would like to be able to customize the appearance of the embodied AI (P3). P5 found it even scary when seeing the AI for the first time: \textit{"Yes, I would say that during the iterations, I got used to it, but all in all, I was quite scared when I saw it the first time"}. However, we also observed positive interaction effects. P1 stated that \textit{"changes felt more tangible when done by the embodied AI"}. P4 felt that social conventions applied to the interaction with the AI: \textit{"it was nicer to wait when at least there was this person who made something"}, and P7 stated that the embodied AI helped not to feel alone. Overall, some participants felt a connection to the embodied AI. P7 even said they missed the embodied AI when it disappeared for some of the iterations. Finally, while some participants did not like the slow version of the continuous interaction mode, they felt it was more acceptable when the AI was embodied versus when it was not.












\subsection{Participants' Approach and Task Limitations}
This section summarizes how participants generally approached the task. It also describes the limitations introduced by the study task unrelated to the human-AI interaction.

Almost all participants divided their modeling sessions into two stages: First, they searched for AI modifications and let the AI perform all the options they could find, undoing any modifications they were not happy with. The second stage was typically spent entirely on painting. In a few cases, participants were satisfied with the model before the five minutes had elapsed and spent the remaining time mostly just looking at their creations. Most participants did not discover all possible modifications that could be performed on the four models.

Participants often criticized the fact that only limited modifications were possible for the given models and that potential changes did not vary when a model appeared for a second time. Some participants hoped to be able to influence the AI's modifications through their behavior. P11 voiced the name of the object that he was hoping to appear in a modification spot. At the same time, some participants tried to elicit specific modifications using the colors they painted with, and P8 even wrote a modification prompt on the object's surface itself.

Participants were also bothered that it could not be re-instantiated once they had undone a modification. They moreover criticized several aspects of the painting mechanism, such as the limited amount of colors, the lack of an eraser, and the issue that if a model had repeating textures, painting on one part of a model could make a copy of the painted stroke appear at another location on the model that used the same texture.



Some exemplary images of finished models created by participants during the modeling sessions are depicted in \autoref{fig:inflmodels}.
















