\section{Related Work}
In the following, we discuss related work in 3D modeling in VR, AI for 3D object generation, co-creative systems in general, embodied AI, and AI contribution visualization.


\subsection{3D Modeling in VR}
Three-dimensional modeling has been a topic of interest in computer research since as early as the 1970s \cite{clark_designing_1976}, with HoloSketch \cite{deering_holosketch_1995} being among the earliest contributions to 3D modeling software for immersive environments. 
In general, there are several ways to approach 3D design in VR. For instance, sketching, though intuitive and quick, suffers from a lack of haptic feedback mid-air. Some systems mitigate inaccuracies by smoothing out hand-drawn lines in post-processing \cite{wesche_freedrawer_2001}, featuring the use of real physical objects in AR as guides \cite{reipschlager_designar_2019}, using haptic feedback of the controller as assistance \cite{elsayed_vrsketchpen_2020} or letting the user sketch on 2D surfaces and then transform the lines in 3D space \cite{arora_symbiosissketch_2018, jackson_lift-off_2016, wesche_freedrawer_2001, dorta_hyve-3d_2016}. Similar problems arise with digital sculpting, in which the user shapes desired forms out of primitives and pre-included models using virtual sculpting tools \cite{noauthor_transform_2020}. Other approaches include mesh modeling \cite{reipschlager_designar_2019, butterworth_3dm_1992} and assembly from primitives \cite{deering_holosketch_1995, reipschlager_designar_2019, do_3darmodeler_2010}.
Additionally, there also exist several commercially available tools for 3D design in VR that allow users to paint 3D artworks~\cite{noauthor_tilt_nodate}, design objects by free-hand drawing~\cite{noauthor_transform_2020}, and modify the mesh directly~\cite{toff_blocks_2017}.
Closely related to this paper, GetWild \cite{wong_getwild_2022} is a VR environment-editing system incorporating AI-generated models into the creation process to speed up the design pipeline and increase accessibility for users with little modeling experience. 

While allowing for expressive 3D modeling, direct modeling approaches can be complex for users with little modeling experience. 
In contrast, this paper investigates systems that co-creatively help users create 3D objects in VR.


\subsection{AI for 3D Object Generation and Manipulation}


While AI research for 2D image synthesis has made notable progress in the past few years, the field of 3D object generation has faced more difficulties due to limitations of the different representation modes of 3D data \cite{shi_deep_2023,li_generative_2023} as well as a lack of text-to-3D training data for deep learning models \cite{li_generative_2023}. Neural Radiance Fields (NeRF) \cite{mildenhall_nerf_2020} is a recently developed approach to representing 3D data, initially serving the purpose of 3D image reconstruction. Due to their ability to bridge the gap between 2D and 3D data, NeRFs can be combined with powerful text-to-image generative models to construct advanced text-to-3D generative models \cite{li_generative_2023}. This has led to a recent upsurge in AI tools for 3D object generation, such as DreamFusion \cite{poole_dreamfusion_2022} and Magic3D \cite{lin_magic3d_2023}. 
However, outputs of NeRF-based 3D generation are difficult to use in downstream 3D applications since a lot of 3D graphics software requires standard data representations like meshes \cite{li_generative_2023}. Shap-E \cite{jun_shap-e_2023} is a recent work demonstrating object generation based on a text prompt, with a mesh output created within less than two minutes.

Though generative models like the ones mentioned above are already helpful for exploring 3D design ideas, it can be difficult for users to formulate a textual prompt that will produce an output meeting all of the user's envisioned criteria. 
This one-step approach often forces the user to make the AI generate an entirely new object based on a rephrased prompt. 
For this reason, efforts have been made to develop AI tools that can modify a given 3D object according to a user's instructions.

SKED \cite{mikaeili_sked_2023} uses sketch-based instructions to enable interactive editing of 3D objects. 
In contrast, ShapeCrafter \cite{fu_shapecrafter_2023} generates a 3D object from a text-based description and lets the user refine object characteristics by recursively adding descriptive phrases.
Similarly, Text2Mesh \cite{michel_text2mesh_2021} can change the style of 3D objects: Given an input mesh and a text prompt, it modifies color and geometry details that adhere to the user-specified style. Additionally, SPAGHETTI \cite{hertz_spaghetti_2022} allows users to select parts of a 3D object and perform rough local transformations on them, based on which the generative model creates an edited version of the same object.

Although previous works include interactive elements similar to co-creative systems, none leverage the intuitive and expressive potential of spatial 3D interaction and presentation offered by VR. Instead, these systems rely on indirect instructions via text or 2D interface interactions. In contrast, this work focuses explicitly on VR environments.

\subsection{Co-creative Systems}

Human-computer co-creativity is a process in which a human and a computer contribute as equals to the same creative process \cite{davis_human-computer_2013}. 
Such co-creative systems have been commonly developed for various domains~\cite{rezwana_designing_2022}. Examples are game design~\cite{liapis_sentient_2013,lucas_stay_2017} and visual art~\cite{davis_empirically_2016,oh_i_2018,lin_it_2020}. 
 


A stream of research has also started to investigate co-creative systems for editing 3D objects. For instance, Liu et al. \cite{liu_interactive_2018} have proposed a system where users can take turns creating and editing 3D objects with an AI.
Closely related, Calliope \cite{urban_davis_designing_2021} has explored interaction possibilities of using generative adversarial networks as an active collaborator in VR. However, it did not investigate the specific influences of different representation modes of such a collaborator on user perception.
Other than Calliope, co-creative systems are typically not studied in VR environments but in physical or digital 2D ones. We want to explore whether findings from those areas also apply to co-creative systems in VR.


\subsection{Embodied AI}
Embodiment can be defined in several ways, though Ziemke~\cite{ziemke_whats_2003} has conceptualized the most basic form of an embodied system as a system that can act on and be acted on by its environment. Other notions include physical embodiment, which necessitates a physical body, and organismoid embodiment, which further constrains the physical body to resemble a living being. Guckelsberger et al. \cite{guckelsberger_embodiment_2021} extend Ziemke's typology with the concept of virtual embodiment, which requires a virtual body that can act on and be acted on by a virtual environment.


Research has shown that the virtual embodiment of agents can improve motivation, positive attitude, and collaborative experiences in creative systems \cite{baylor_promoting_2009, rezwana_understanding_2022, davis_empirically_2016}. Additionally, organismoid embodiment enhances identification, empathy, and perceived creativity and is considered essential for co-creativity due to the unique perspective it provides \cite{guckelsberger_embodiment_2021}. Also, Kim et al. \cite{kim_does_2018} investigated the effect of embodiment on the perception of a digital AR assistant.
Moruzzi \cite{moruzzi_artificial_2022} found that AI embodiment increased perceived creativity in a collaborative artistic process, while Rezwana and Maher \cite{rezwana_user_2023} showed that AI personification influenced users' perceptions of the AI as an independent collaborator and affected their ethical views on ownership of co-created artifacts.



Studies on the effects of embodiment in VR have mainly examined AI agents without co-creativity. In contrast, studies on the embodiment of co-creative AI have mainly been carried out in non-VR environments.
We combine both promising directions of previous works by studying embodiment in the context of a co-creative object-generating agent embedded in VR.

\subsection{Non-player Characters in Games} %
In the domain of game development, the design of autonomous human-like agents, so-called Non-Player Characters (NPCs), was addressed in the context of believability \cite{10.1145/176789.176803}. Recent research explores the potential of NPCs in virtual reality~\cite{10.1145/3677098} and of LLM-based agents with more human-like reasoning, planning, and execution abilities~\cite{10.1145/3586183.3606763}. Moreover, in a systematic literature review,~\citet{wittmann2022} discuss NPC design patterns and how to transfer those to the design of AI systems for human-AI collaboration. The authors point out six relevant focus fields: NPC responsiveness, appearance, communication patterns, emotional aspects, behavioral characteristics, and player-NPC and NPC-NPC team structures.

Unlike NPCs, which are inherently embodied as avatars, co-creative tools for object building do not inherently require such embodiment. This property raises important questions about the role of embodiment in such tools and its interaction with factors like the highlighting and visualization of changes, which are the focus of this paper.




\input{figures/ModelingFlow5}


\subsection{AI Contribution Visualization}
Computational creativity can be assessed through the creator, the product, the process, or the environment, but humans often interpret a system's creativity based on appearance, behavior, and output \cite{rhodes_analysis_1961, lee_what_2020, parise_cooperating_1999, el-zanfaly_sand_2023, pearce_towards_2001}. Observing the creative process may lead to higher evaluations due to empathy and perceived effort, known as the "effort heuristic" \cite{colton_painting_2012, kruger_effort_2004}, and delays in AI output generation can increase user engagement and perceived control \cite{liu_how_2023}. However, empirical studies on incremental visualization of AI creativity are lacking \cite{linkola_how_2022}, prompting the need for research on both AI embodiment and perceptual evidence in co-creative VR systems.

Thus far, no empirical studies have investigated the effects of such an intentionally incremental visualization of AI contributions, especially not separately from the impact of AI embodiment. Therefore, we address this lack of research by contributing a study in which both embodiments and perceptual evidence in the form of incremental contribution visualization are examined in the context of a co-creative VR system.
