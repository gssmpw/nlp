\section{Methodology}

Our study investigates the collaboration with an object-generating AI, focusing on the often overlooked crucial factor of AI-to-human communication~\cite{rezwana_designing_2022}.
We conducted a controlled experiment to examine how three representation modes (highlighting, incremental visualization, and embodiment) of the AI's generative contributions impact the co-creative process.
Our study is guided by the following research questions (RQs), each targeting one of the three representation modes:


\begin{description}
\item[RQ1] \emph{How does \textbf{highlighting} affect the co-creative collaboration experience, in particular regarding the predictability of AI?}   
Based on decades of HCI research and design practices that emphasize the importance of highlighting focus or activity, we hypothesize that highlighting will positively affect perceived communication quality, as it is a well-known mechanism to guide users' attention. 
For the same reason, we also expect that with highlighting, users will pay more attention to the AI, and the clarity of the AI intentions will be perceived better.
	
\item[RQ2] \emph{How does \textbf{incremental visualization} of changes affect the co-creative collaboration experience, in particular regarding a user's perception of the AI's efficiency and competence compared to a discrete visualization of the AI's contributions?}
Inspired by human collaboration, where it takes time for a modification to take shape, we expect that incremental visualization will affect measures concerning those same qualities as above, but also measures on the perceived system efficiency and competence, measures on the perception of AI outputs, perceived alignment of AI outputs with the users' visions, perceived agency, and the proportion to which contributions to the final artifact are attributed to the AI.
 
\item[RQ3] \emph{How does the \textbf{embodiment} of an AI affect the co-creative collaboration experience, in particular regarding the perceived supportiveness, efficiency, and competence of the AI?} 
Research on NPCs and games highlights embodiment as a critical factor in interaction, making embodiment a potentially promising factor for co-creative tools in VR. We thus expect that embodiment will affect the perceived supportiveness, efficiency, and competence of the AI, the perceived value and creativity of AI outputs, measures concerning collaborative experience, appeal, and behavioral engagement, as well as the previously mentioned contribution attribution and users' perceived closeness to the AI.
\end{description}


\begin{figure*}%
	\includegraphics[width=.9\linewidth]{figures/IVTable.pdf}\vspace{.3cm} 
	\caption{We vary three different independent variables: (1) whether the AI has an embodied representation or is disembodied, (2) whether the AI's contributions are shown immediately or build up incrementally, and (3) if the area that the AI is going to modify is highlighted or not.}
 \Description{Showing the different IVs with an example of a chair. Left are four images illustrating the difference between disembodied AI. Right are four images showing the VR scene with an embodied AI}
	\label{fig:IVs}
\end{figure*}



\subsection{Task}
For our study, we placed participants in a virtual room, which was empty, besides a podium in the center (see \autoref{fig:results:ModelingFlow_a_Empty}) and a timer on the wall. In the embodied AI conditions, the AI avatar is shown additionally (see \autoref{fig:results:ModelingFlow_b_AI_Creation}). During the task, the user and the AI can collaboratively and iteratively modify a given 3D model. 

At the beginning of a modeling session, a 3D object appears on the podium, and the timer starts running. As long as the timer was running, the user could modify the object by either painting its texture (see \autoref{fig:results:ModelingFlow_c_Painting}), adding elements to the existing object (see \autoref{fig:results:ModelingFlow_d_AI_Addition}), or instructing the AI to modify (see \autoref{fig:results:ModelingFlow_e_AI_Modification}).
We chose painting as the users' primary design responsibility as it allowed for a consistent engagement with the 3D object during the AI-driven modifications and ensured comparable levels of object alteration and creative expression while avoiding interference.
For the modifications, users were always controlling if and when any changes to the object should be executed (including being able to revert the AI's modification).
We simulated the AI agent in a Wizard-of-Oz approach, in the sense that its contributions to the 3D objects were not the results of an actual generative process but instead object additions and replacements created by human 3D artists in advance. 

Further, the creative contributions of the AI agent are not designed to align with the contributions of the human partner but instead emerge completely independently of the human's behavior. This contribution style is typical for what Rezwana and Maher \cite{rezwana_designing_2022} refer to as \textit{provoking agents} (as opposed to \textit{pleasing agents}), which evoke creativity in their human partners by providing divergent ideas and alternative explorations.
While the AI was active, the user could not give the AI another modification instruction. %

We provided participants with four starting objects available for modification: A dinosaur, a chair, a pickup truck, and a house (see \autoref{fig:inflmodels}). %
Each object allowed for seven modifications randomly chosen by the system.
The AI modifications either consisted of an object part being replaced by another one (e.g., switching out the legs of a chair with legs of a different type) or an object part being added to the object (e.g., adding a dormer to the roof of the house).

While most modifications matched the object's type, a few unexpected changes were intentionally introduced (e.g., bunny ears on a dinosaur or cheese in a pickup truck). 
This design allowed us to explore how users perceive AI creativity, novelty, and predictability across different visualization settings.

\subsection{Design}

\subsubsection{Independent Variables}
In our study, we vary three representation modes that change how the AI carries out a modification instruction (see \autoref{fig:IVs}):

\begin{description}

\item[\ivHigh] 
If highlighting is enabled, instructing the AI to modify a part of the object will cause an outline to appear around the object part in question. If none of the other visualization aspects are enabled, this outline will be displayed for three seconds and then disappear at the same time that the object part is replaced with its modified version. If incremental visualization is enabled, the highlight will be displayed on the object part for exactly as long as the incremental process takes. If embodiment is enabled, the highlight will be displayed on the object part from the moment the AI avatar starts walking toward the object part until it has finished stepping back from the object. We base this concept of highlighting active elements on traditional 2D interface design to guide user attention.%


\item[\ivInc]
If incremental visualization is enabled, instructing the AI to modify an object part will cause this part to disappear and its modified version to slowly appear slice by slice as if a 3D printer was printing it. If the embodiment is enabled, a "printing layer" will be placed on top of the incrementally growing object part, vaguely resembling the additive manufacturing technique of sheet lamination. The AI avatar will appear to move this layer along the printing direction through a line connecting its right hand to one of the layer's corners.
	
\item[\ivEmb] 
If embodiment is enabled, an AI avatar will sit on a chair in the corner of the room at the beginning of a modeling session. Once instructed to modify a part of the 3D object, it will get up and walk toward the object part to perform the modification. If incremental visualization is enabled, it will stand beside the object holding the printing layer until the incremental process is finished. If incremental visualization is not disabled, the avatar will carry out a waving hand gesture reminiscent of a person performing a magic trick, after which the modified version of the object part replaces its previous version. Once the object part is finished being modified, the avatar will walk away a few steps from the object and wait in this location until it is instructed for another modification. After the AI has finished, it walks back to its chair autonomously.

\end{description}

 \begin{table*}[t!]
    \caption{The list of questions participants were asked in the semi-structured interview.}
	\resizebox{\textwidth}{!}{%
		\begin{tabular}{@{}ll@{}}
			\toprule
			Overall experience & How was your experience carrying out the given task? \\
			& Which of the runs did you prefer? \\
			& Which one did you like the least? \\ \midrule
			Perception of and satisfaction with the results & What do you think of your results? \\ 
			& Were you aiming for these kinds of results? \\ \midrule
			Perception of AI & How would you describe the relationship between you and the AI in the context of these tasks? \\
			& Would you have preferred the Ai to do more/less/other things? \\
			& In an optimal future version of this system, how would the AI behave? \\ \midrule
			Suggestions & Do you have any other comments or suggestions? \\ \bottomrule
		\end{tabular}%
	}
	\label{tab:interviewqs}
\end{table*}

When none of the representation modes were enabled, the modification process solely consisted of the object part being replaced by its modified version instantaneously without highlighting or any additional embodiment.

We varied all three independent variables in a repeated measures design, resulting in a total of $2 \times 2 \times 2 = 8$ conditions. %
We counterbalanced the order of all factors in a Balanced Latin Square to prevent learning effects.
Each participant conducted eight modeling trials across eight different conditions. In each session, participants modified one of the four 3D objects (see \autoref{fig:modmodels}). Each object was presented to each participant twice, with the order randomized for each individual. 
We chose four objects as a suitable compromise, striking a balance between variety and familiarity. 
This approach allowed participants to engage their creativity by attempting various design approaches with the same model while also providing a second chance to revisit and revise previous modifications – an opportunity to learn from failure that we argue is valuable in our research context.



\subsubsection{Dependent Variables}
Data collected during the study consisted of the following:

\newcommand{\paragraphB}[1]{{\textbf{#1}}}
\paragraphB{Before the experiment:} Demographic information was collected through a Google Forms survey.

\paragraphB{During modeling sessions:} A log of user interactions with the system was recorded. This consisted of the type of each interaction with the object and the AI, as well as the time at which it was performed. Additionally, a screencast of the participant's perspective in VR was recorded in each modeling session.

\paragraphB{Between modeling sessions:} After each modeling session, the user was asked to fill out a questionnaire.
We base our questionnaire on several other questionnaires from the HCI and Psychology:
\begin{itemize}
	\item The Mixed-Initiative Creativity Support Index (MICSI) \cite{lawton_drawing_2023} measures the degree to which a co-creative system supports a user in creative tasks. For the purposes of our questionnaire, we adopted eight items, some of them as slightly modified versions: the two items on \textit{Enjoyment} and one item each on \textit{Alignment} and \textit{Agency} – all measured on 7-point Likert scales – and one item each on \textit{Contribution}, \textit{Satisfaction}, \textit{Surprise}, and \textit{novelty} – all measured on 7-point non-Likert scales.
	\item Rezwana and Maher \cite{rezwana_understanding_2022} modified the Creativity Support Index (CSI) \cite{cherry_quantifying_2014}, which the MICSI was based on, to include two items evaluating \textit{partnership} and \emph{communication} in human-AI collaboration. We adopted both of those items in our questionnaire.
	\item The User Experience Questionnaire (UEQ) \cite{laugwitz_construction_2008} measures general user experience for interactive systems and consists of items on seven-point adjective scales. We adopted five of those items: One from the \textit{efficiency} scale, two from the \textit{Dependability} scale, and two from the \textit{Stimulation} scale.
	\item The Inclusion of Other in the Self (IOS) Scale \cite{aron_inclusion_1992} is a pictorial single-item scale measuring \textit{closeness} between the respondent and another person. Though the AI in our co-creative system is not a person, we are nevertheless interested in whether closeness, when applied to a co-creative AI partner, is affected by our visualization aspects. We, therefore, included this item in our questionnaire.
	\item The Social Presence in Gaming Questionnaire (SPGQ) \cite{de_kort_digital_2007} was developed for digital games and measures players' awareness of and involvement with their co-players. Though our co-creative system is not a game, we are interested in whether our visualization aspects influence the degree to which users perceive a co-creative AI as a separate presence that influences and can be influenced by the user. This is why we adopted all eight items from the \textit{Behavioral Engagement Scale} of this questionnaire, all of which are answered with 5-point intensity scales.
\end{itemize}
In addition to the items sourced from the above-mentioned questionnaires, we included three Likert items on AI competence, contribution value, and AI creativity, amounting to 27 questions in total\footnote{See the supplementary materials for a list of all items}.
 
\paragraphB{At the end of the experiment:} In a semi-structured interview, participants answered questions about their overall experience with the system and their perception of the finished 3D objects and the AI (see \autoref{tab:interviewqs}).




\subsection{Apparatus}
The study was carried out using a Windows 11 computer with an NVIDIA GeForce GTX 1080 graphics card and a Meta Quest Pro head-mounted display (HMD) connected to the computer via Quest Link. The software was run directly from its Unity environment. Participants were given both of the Meta Quest Pro controllers and were free to walk around in a room-scale tracked space. Questionnaire replies were filled out on a separate computer.

\input{figures/quant_data_plot}

\subsection{Procedure}
After welcoming participants and before starting the study, participants filled out an informed consent form and completed a demographic information survey. We then provided an overview of the study procedure and system usage guidelines.
Next, we instructed the participants on using the HMD and controllers and let them familiarize themselves with the VR interactions in a virtual room by testing selection and painting operations on a sphere. 
Once ready, participants began their first modeling session, tasked to \textit{"be creative and work together to create a new version of this model"}, emphasizing a focus on making the given object look different from its default state but letting participants choose their own design goals.
We timed a 5-minute session (visualized by a virtual clock) and automatically disabled all interactions once time ran out. 
Participants were invited to voice any thoughts they had during the sessions.
Participants then filled out questionnaires outside the VR environment before repeating this process for a total of eight conditions. After completing all sessions, we conducted a semi-structured interview with each participant. 
In total, the study lasted around 90 minutes. All participants were given cake as compensation for their time.





\subsection{Participants}
We recruited 16 participants (7 identified as female, 9 as male, and 0 as non-binary). Ages ranged from 20 to 30, with a mean age of 24 ($SD=3$). 
Out of all the participants, five had no prior experience with VR, while nine had tried it before, and two had used it more often. 
Nine participants had no experience with 3D modeling, three had tried it before, and four had done it more often. 
Most participants had interacted with AI in the form of Large Language Models before. In contrast, half of the participants had used AI for image generation before, and only three participants had experience with AI for 3D object generation. 
