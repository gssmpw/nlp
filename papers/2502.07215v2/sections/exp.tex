\begin{figure*}[!h]
    \centering
    \begin{subfigure}[b]{0.8\linewidth}
        \centering
        \includegraphics[width=0.45\linewidth]{images/residual/text/CIReVL_Recall5.png}
        \hfil
        \includegraphics[width=0.45\linewidth]{images/residual/text/pic2word_recall5.png}
        \caption{\textbf{PDV-T}: Impact of $\alpha$ scaling on composed text embeddings}
        \label{fig:residual_text_sub}
    \end{subfigure}
    
    \begin{subfigure}[b]{0.8\linewidth}
        \centering
        \includegraphics[width=0.45\linewidth]{images/residual/image/CIReVL_Recall5.png}
        \hfil
        \includegraphics[width=0.45\linewidth]{images/residual/image/pic2word_recall5.png}
        \caption{\textbf{PDV-I}: Impact of $\alpha$ scaling on composed image embeddings}
        \label{fig:residual_image_sub}
    \end{subfigure}
    
    \begin{subfigure}[b]{0.8\linewidth}
        \centering
        \includegraphics[width=0.45\linewidth]{images/residual/fusion/CIReVL_Recall5.png}
        \hfil
        \includegraphics[width=0.45\linewidth]{images/residual/fusion/pic2word_recall5.png}
        \caption{\textbf{PDV-F}: Impact of varying $\beta$ with on composed fused embeddings}
        \label{fig:residual_fusion_sub}
    \end{subfigure}
    \caption{Impact of changing $\alpha$/$\beta$ on Recall@5 performance across different PDV applications. For each row, results are shown for the CIReVL (left) and Pic2Word (right) baseline methods.}
    \label{fig:residual_all}
\end{figure*}

\section{Experiments} 
\label{sec:exp}
\noindent\textbf{Implementation Details.} We utilize the official implementations of four ZS-CIR baseline methods: CIReVL\footnote{https://github.com/ExplainableML/Vision\_by\_Language} and LDRE \footnote{https://github.com/yzy-bupt/LDRE} as representative caption-based feature extraction approaches and Pic2Word\footnote{https://github.com/google-research/composed\_image\_retrieval} and SEARLE\footnote{https://github.com/miccunifi/SEARLE} as representative pseudo tokenization-based methods. All feature extraction processes follow the original implementations provided by these baseline methods. However, to calculate $\Delta_{PDV}$, we need text embeddings without prompts, which are not provided in the original implementations. For CIReVL and LDRE, we obtain these embeddings by passing the generated image captions directly to CLIP. For Pic2Word and SEARL, we construct the base text embedding by passing the phrase ``a photo of $\langle$token$\rangle$" to CLIP, where $\langle$token$\rangle$ represents the extracted image token obtained via text inversion.

\noindent\textbf{Datasets and Base Vision-Language Models.} Following previous work, we evaluated our method on a suite of datasets including Fashion-IQ \cite{wu2021fashion}, CIRR \cite{liu2021image} and CIRCO \cite{baldrati2023zero}. Our proposed method is a plug-and-play approach requiring no additional training, leveraging only pre-trained models. For feature extraction, we use three CLIP variants: ViT-B/32, ViT-L/14, and ViT-G/14, and used the same pre-trained weights as used by the baseline methods. For image tokenization, we employ the pre-trained Pic2Word model. 

\subsection{Effect of Using the PDV}
We now explore the impact of the three proposed uses of the PDV: Using the PDV to augment text queries (PDV-T, see Sec. \ref{sec:exp1}), using the PDV to augment image queries (PDV-I, see Sec. \ref{sec:exp2}), and using the PDV in queries that fuse image and text data (PDV-F, see Sec. \ref{sec:exp3}).

\begin{table*}
	\footnotesize
	\centering
	\begin{tabular}{l|l|c|c|c|cccccccc}
		\hline
		\textbf{Fashion-IQ} & & & & & \multicolumn{2}{c}{\textbf{Shirt}} & \multicolumn{2}{c}{\textbf{Dress}} & \multicolumn{2}{c}{\textbf{Toptee}} & \multicolumn{2}{c}{\textbf{Average}} \\ \hline
		Backbone & Method& $\beta$ & $\alpha_{I}$& $\alpha_{T}$ & R@10 & R@50 & R@10 & R@50 & R@10 & R@50 & R@10 & R@50 \\
		\hline
		\multirow{6}{*}{ViT-B/32} %
		& SEARLE & - & - & - & 24.14 & 41.81 & 18.39 & 38.08 & 25.91 & 47.02 & 22.81 & 42.30 \\
		& SEARLE + \textbf{PDV-F} & 0.9 & 1.1 & 0.9 & \hli{24.83} & 41.71 & \hli{20.13} & \hli{41.40} & \hli{25.96} & \hli{47.17}  & \hli{23.64} & \hli{43.43} \\
		& CIReVL \textdagger &- & -& -& 28.36 & 47.84 & 25.29 & 46.36 & 31.21 & 53.85 & 28.29 & 49.35 \\
		& CIReVL + \textbf{PDV-F} & 0.75 & 1.4 & 1.4 & \hlb{32.88} & \hlb{52.80} & \hlb{32.67} & \hlb{54.49} & \hlb{38.91} & \hlb{61.81} & \hlb{34.82} & \hlb{56.37} \\
		& LDRE \textdagger & - & - & - & 27.38 & 46.27 & 19.97 & 41.84 & 27.07 & 48.78 & 24.81 & 45.63 \\
		& SEIZE \textdagger & - & - & - & \underline{29.38} & \underline{47.97} & \underline{25.37} & \underline{46.84} & \underline{32.07} & \underline{54.78} & \underline{28.94} & \underline{49.86} \\
		\hline
		\multirow{8}{*}{ViT-L/14} & Pic2Word & & & & 25.96 & 43.52 & 19.63 & 40.90 & 27.28 & 47.83 & 24.29 & 44.08 \\
		& Pic2Word + \textbf{PV-F} & 0.8 & 1.0 & 1.0 & \hli{28.21} & \hli{44.55} & \hli{20.92} & \hli{42.24} & \hli{29.02} & \hli{48.90}& \hli{26.05} & \hli{45.23}\\
		& SEARLE & - & - & - & 26.84 & 45.19 & 20.08 & 42.19 & 28.40 & 49.62 & 25.11 & 45.67 \\
		& SEARLE +\textbf{PDV-F} & 0.8 & 1.2 & 1.0 & \hli{28.66} & \hli{46.76} & \hli{23.60} & \hli{46.41} & \hli{31.00} & \hli{52.32} & \hli{27.75} & \hli{48.50} \\
		& CIReVL \textdagger & & & & 29.49 & 47.40 & 24.79 & 44.76 & 31.36 & 53.65 & 28.55 & 48.57 \\
		
		& CIReVL + \textbf{PDV-F} & 0.55 & 1 & 1.3 & \hlb{37.78} & \hlb{54.22} & \hlb{33.61} & \hlb{56.07} & \hlb{41.61} & \hlb{62.16} & \hlb{37.67} & \hlb{57.48} \\
		& LinCIR & - & - & - & 29.10 & 46.81 & 20.92 & 42.44 & 28.81 & 50.18 & 26.82 & 46.49 \\
        & SEIZE & -& -& -& \underline{33.04} & \underline{53.22} & \underline{30.93} & \underline{50.76} & \underline{35.57} & \underline{58.64} & \underline{33.18} & \underline{54.21} \\
		\hline
        \multirow{6}{*}{ViT-G/14} & Pic2Word  & - & - & - & 33.17 & 50.39 & 25.43 & 47.65 & 35.24 & 57.62 & 31.28 & 51.89\\
         & SEARLE  & - & - & - & 36.46 & 55.35 & 28.16 & 50.32 & 39.83 & 61.45 & 34.81 & 55.71\\
		  & CIReVL \textdagger & -& -& -& 33.71 & 51.42 & 27.07 & 49.53 & 35.80 & 56.14 & 32.19 & 52.36 \\
		& CIReVL + \textbf{PV-F} & 0.6 & 1.4 & 1.4 & \hli{41.90} & \hli{58.19} & \hlb{40.70} & \hlb{62.82} & \underline{\hli{48.09}}& \hli{67.77}& \underline{\hli{43.56}}& \hli{62.93}\\
        & LinCIR & - & - & - & \textbf{46.76} & \underline{65.11} & 38.08& 60.88& \textbf{50.48}& \underline{71.09}& \textbf{45.11} & \underline{65.69}\\
        & SEIZE & - & - & - & \underline{43.60} & \textbf{65.42}& \underline{39.61} & \underline{61.02} & 45.94& \textbf{71.12}& 43.05& \textbf{65.85}\\
		\hline
	\end{tabular}
	\caption{Average recall for different methods on Fashion-IQ validation dataset. \textdagger~denotes that numbers are taken from the original paper.}
	\label{tab:fashion_iq_results}
\end{table*}


\begin{table*}
	\centering
	\footnotesize
	\setlength{\tabcolsep}{4pt}
	\begin{tabular}{ll|c|c|c|cccc|cccc|ccc}
		\hline
		\multicolumn{2}{c|}{\textbf{Dataset}} & & & &  \multicolumn{4}{c|}{\textbf{CIRCO}} & \multicolumn{7}{c}{\textbf{CIRR}} \\
		\hline
		\multicolumn{2}{c|}{Metric} & & & & \multicolumn{4}{c|}{mAP@k} & \multicolumn{4}{c|}{Recall@k} &\multicolumn{3}{c}{$R_s$@k} \\
		\cline{3-16}
		Arch & Method & $\beta$ & $\alpha_I$ & $\alpha_T$ & k=5 & k=10 & k=25 & k=50 & k=1 & k=5 & k=10 & k=50 & k=1 & k=2 & k=3 \\
		\hline
		\multirow{8}{*}{ViT-B/32} 
		& PALAVRA\cite{cohen2022my} \textdagger & -& -& -& 4.61 & 5.32 & 6.33 & 6.80 & 16.62 & 43.49 & 58.51 & 83.95 & 41.61 & 65.30 & 80.94 \\
		& SEARLE \textdagger & -& -&- & 9.35 & 9.94 & 11.13 & 11.84 & 24.00 & 53.42 & 66.82 
		& 89.78 & 54.89 & 76.60 & 88.19 \\
		& SEARLE + \textbf{PDV-F} & 0.9 & 1.4 & 1.2 & \hli{9.99} & \hli{10.50}  & \hli{11.70} & \hli{12.40} & \hli{24.53} & \hli{53.71} & \hli{67.33} & \hli{89.81} & \hli{56.94} & \hli{78.05} & \hli{88.99} \\
		&CIReVL \textdagger & - & - & -& 14.94 & 15.42 & 17.00 & 17.82 & 23.94 & 52.51 & 66.00 & 86.95 & 60.17 & 80.05 & 90.19 \\
		& CIReVL + \textbf{PDV-F} & 0.75 & 1.4 & 1.2 & \hlb{19.90} & \hlb{20.61} & \hlb{22.64} & \hlb{23.52} & \hlb{33.25} & \hlb{64.15} & \hlb{75.23} & \hlb{92.43} & \hlb{65.81} &\underline{\hli{83.76}} &\underline{\hli{92.10}} \\
		& LDRE & -& -& -& 17.81 & 18.04 & 19.73 & 20.67 & 25.69 & 55.52 & 68.77 & 89.86 & 60.10 & 80.58 & 91.04 \\
		& LDRE + \textbf{PDV-F} & 0.75 & 1.4 & 1.4 & \hli{17.80} & \hli{18.78} & \hli{20.61} & \hli{21.56} & \underline{\hli{29.30}} & \underline{\hli{60.39}} & \underline{\hli{72.51}} & \underline{\hli{91.42}} & \hli{63.06} & \hli{82.36} & \hli{91.54} \\
        & SEIZE & -&- &- & \underline{19.04} & \underline{19.64} & \underline{21.55}& \underline{22.49}& 27.47 & 57.42& 70.17 & - & \underline{65.59} & \textbf{84.48}& \textbf{92.77} \\
 		\hline
		\multirow{10}{*}{ViT-L/14}
		& Pic2Word & -& -& -& 6.81 & 7.49 & 8.51 & 9.07 & 23.69 & 51.32 & 63.66 & 86.21 & 53.61 & 74.34 & 87.28 \\
		& Pic2Word + \textbf{PDV-F} & 0.85 & 1.2 & 1.0 & \hli{7.74} &  \hli{8.67} & \hli{9.77} & \hli{10.37} & \hli{23.90} & \hli{51.95} & \hli{64.63} & \hli{87.04} & \hli{53.16}  & \hli{74.07} & \hli{87.08}\\
		& SEARLE \textdagger & - & - & - & 11.68 & 12.73 & 14.33 & 15.12 & 24.24 & 52.48 & 66.29 & 88.84 & 53.76 & 75.01 & 88.19 \\
		& SEARLE + \textbf{PDV-F} & 0.85 & 1.4 & 1.2 & \hli{12.58} & \hli{13.57} & \hli{15.30} & \hli{16.07} & \hli{25.64} & \hli{53.61} & \hli{66.58} & \hli{88.55} & \hli{55.83} & \hli{76.48} & \hli{88.53} \\
		& CIReVL \textdagger & -& -& -& 18.57 & 19.01 & 20.89 & 21.80 & 24.55 & 52.31 & 64.92 & 86.34 & 59.54 & 79.88 & 89.69 \\
		& CIReVL + \textbf{PDV-F} & 0.75 & 1.4 & 1.2 & \hlb{25.67} & \hlb{26.61} & \underline{\hli{28.81}} & \hlb{29.95} & \hlb{36.24} & \hlb{66.17} & \hlb{76.96} & \hlb{92.29} & \hlb{68.07} & \hlb{85.35} & \hlb{93.47} \\
		& LDRE & -& -& -& 22.32 & 23.75 & 25.97 & 27.03 & 26.68 &55.45  & 67.49 & 88.65 & 60.39 & 80.53 & 90.15 \\
		& LDRE + \textbf{PDV-F} & 0.75 & 1.4 & 1.4 & \hli{25.23} & \hli{26.52} & \hlb{28.94} & \hlb{29.95} & \underline{\hli{30.16}} & \underline{\hli{59.98}} & \underline{\hli{71.90}} & \underline{\hli{90.87}} & \hli{63.66} & \hli{82.87} & \hli{91.57} \\

        & LinCIR & - & - & - &12.59 &13.58 &15.00 &15.85 &25.04 &53.25 &66.68 & - &57.11 &77.37 &88.89\\
        & SEIZE & -& -& -& 24.98 & 25.82 &28.24 &\underline{29.35}& 28.65 &57.16& 69.23& - &\underline{66.22} &\underline{84.05} &\underline{92.34} \\
        

        
		\hline
		\multirow{7}{*}{ViT-G/14} & CIReVL \textdagger & -& -& -& 26.77 & 27.59 & 29.96 & 31.03 & 34.65 & 64.29 & 75.06 & 91.66 & 67.95 & 84.87 & 93.21 \\

		& CIReVL + \textbf{PDV-F} & 0.75 & 1.4 & 1.2 & \hli{30.02} & \hli{31.46} & \hli{34.01} & \hli{35.08} & \hli{38.15} &\hli{67.93} & \hli{77.90} & \hli{92.77} & \hli{69.37} & \hli{85.37} & \hli{93.45}  \\
		
		& LDRE & -& -& -& \underline{33.30} & \underline{34.32} & \underline{37.17} & \underline{38.27} & 37.40 & 66.96 & 78.17 & 93.66 & 68.84 & 85.64 & 93.90 \\
		& LDRE + \textbf{PDV-F} & 0.75 & 1.4 & 1.4 & \hlb{34.88} & \hlb{36.41} & \hlb{39.12} & \hlb{40.23} & \hlb{42.51} & \hlb{72.22} & \hlb{81.71} & \hlb{94.94} & \underline{\hli{72.39}} & \underline{\hli{88.34}} & \underline{\hli{94.80}} \\
        & SEARLE & - & - & - & 13.20 &13.85 &15.32 &16.04 & 34.80 & 64.07 & 75.11 &-&68.72 &84.70 &93.23 \\
        & LinCIR & - & - & - & 19.71 &21.01 &23.13 &24.18 &35.25 &64.72 &76.05 & - &63.35 &82.22 &91.98 \\
        & SEIZE & -& -& -& 32.46 & 33.77 &36.46 &37.55 &\underline{38.87} & \underline{69.42} & \underline{79.42} & -&\textbf{74.15} & \textbf{89.23} & \textbf{95.71} \\
		\hline
	\end{tabular}
	\caption{Performance comparison on CIRCO and CIRR test datasets. As in previous works, for CIRCO, mAP@k is reported, while for CIRR both Recall@k and $R_s$@k metrics are used. \textdagger~denotes that numbers are taken from the original paper.}
	\label{tab:circo_cirr_results}
\end{table*}

\noindent{\textbf{Analysing the PDV for Text (PDV-T)}}
\label{sec:exp1}
To investigate how scaling the prompt vector, $\Delta_{PDV}$, affects retrieval performance with composed text embeddings, we conducted experiments using two zero-shot approaches (CIReVL and Pic2Word) with different backbone networks across three datasets. We evaluated the performance by varying the scaling parameter, $\alpha$ (Eq. \ref{eqn:text_embedding}), from -0.5 to 3 by an interval of 0.1.

The results are presented in Figure \ref{fig:residual_text_sub}. To account for scale variations across different experiments, we report relative recall values, where a baseline of zero is established at $\alpha=1$. As shown in Figure \ref{fig:residual_text_sub}, varying $\alpha$ leads to significant changes in relative recall performance\footnote{See supplementary material for Recall@10 and Recall@50 figures}. Our analysis reveals method-specific patterns across datasets. With CIReVL, increasing $\alpha$ improves relative recall on both FashionIQ and CIRCO datasets. In contrast, Pic2Word shows no significant improvement on FashionIQ and CIRR when varying $\alpha$, while CIRCO's performance improves when $\alpha$ is reduced to 0.8-1.0. This divergent behavior is fundamentally linked to each method's ability to generate an accurate $\Delta_{PDV}$. As demonstrated in Tables \ref{tab:fashion_iq_results} and \ref{tab:circo_cirr_results}, CIReVL consistently outperforms Pic2Word across various benchmarks, indicating its superior ability to generate a more accuraute composed query, and thus a more accurate $\Delta_{PDV}$. Consequently, increasing $\alpha$ yields greater benefits for CIReVL compared to Pic2Word.

We visualize the top-5 retrieval results using CIReVL with a ViT-B-32 backbone across three datasets (one reference image from each) under varying $\alpha$ values, as shown in Figure \ref{fig:residual_qual}\red{a}. As $\alpha$ increases, the retrieved results show stronger alignment with the prompt. Conversely, when $\alpha$ exceeds 1, the results include semantically related but unseen variations, while $\alpha$ values below 0.5 yields results opposite to the prompt's intent. For instance, ``brighter blue and sleeveless" retrieves ``dark blue with sleeves," ``plain background" yields ``natural/dark background," and ``young boy" returns ``adult" images.





\noindent{\textbf{Analysing the PDV for Image (PDV-I)}}
\label{sec:exp2}
To evaluate whether $\Delta_{PDV}$ enhances the retrieval performance of image embeddings, we conducted experiments following the protocol described in Section~\ref{sec:exp1}. We modified image embeddings by adding $\Delta_{PDV}$ scaled with $\alpha$ values ranging from -0.5 to 2.0, where $\alpha=0$ represents the original image-only embeddings. As shown in Figure \ref{fig:residual_image_sub}, Recall@K exhibits a positive correlation with $\alpha$ for values below 1. This upward trend continues until $\alpha=2.0$ for CIReVL, while Pic2Word's performance peaks when $\alpha$ reaches 1.4.  The performance of PDV-I was evaluated on the CIRR and CIRCO datasets by comparing it with other visual embedding-based methods, as detailed in Table \ref{tab:circo_cirr_results_pdv-I}. The results reveal that PDV-I achieved marginal improvements over existing approaches.

Following the methodology in Section~\ref{sec:exp1}, we conduct similar visualizations, with results shown in Figure \ref{fig:residual_qual}\red{b}. As with PDV-T, increasing $\alpha$ leads to stronger alignment between retrieved results and the prompt. When $\alpha$ exceeds 0.5, the results exhibit semantic relationships to the query, while $\alpha$ values below 0.5 yield results opposing the prompt's intent.
Notably, PDV-I's top retrievals demonstrate higher visual similarity to reference images compared to PDV-F, as evidenced by the preserved design elements in the clothing item (left) and laptop (middle). This characteristic is particularly valuable for applications include fashion search \cite{wu2021fashion} and logo retrieval \cite{tursun2019component}, where visual similarity plays a crucial role.



\begin{figure*}[!tbh]
	\centering
	\includegraphics[width=0.825\linewidth]{images/qualitative/PV_qual_all_mini.pdf}
	\caption{Visualisation of the impact of $\alpha$/$\beta$ scaling on top-5 retrieval results. CIReVL with ViT-B-32 Clip model is the baseline method used. Representative examples with prompts from three datasets: FashionIQ (left), CIRR (middle), and CIRCO (right) are shown at the top. \textbf{\textcolor{boxgreen}{Green}} and \textbf{\textcolor{boxblue}{blue}} bounding boxes indicate true positives and near-true positives, respectively.}
	\label{fig:residual_qual}
	
\end{figure*}

\noindent{\textbf{Analysing PDV Fusion (PDV-F)}}
\label{sec:exp3}
Finally, we evaluate the effectiveness of fusing image and text-composed embeddings by varying the fusion parameter, $\beta$, from 0 to 1 while maintaining $\alpha=1$
for both PDV-I and PDV-F. At $\beta=0$, the model relies solely on composed image embeddings, while at $\beta=1$, it uses only composed text embeddings. As shown in Figure \ref{fig:residual_fusion_sub}, the fusion of both embeddings consistently outperforms using either embedding type alone. Optimal retrieval performance is typically achieved when $\beta$ is between 0.4 and 0.8.

We similarly visualize the top-5 retrieved results across different $\beta$ values. As shown in Figure \ref{fig:residual_qual}\red{c}, when $\beta$ is small, the retrieved results maintain high visual similarity to the reference image. Conversely, as $\beta$ exceeds 0.5, the results demonstrate stronger semantic alignment with the prompt.



\subsection{ZS-CIR Benchmark Comparison}






\begin{table*}
	\centering
	\footnotesize
	\setlength{\tabcolsep}{4pt}
	\begin{tabular}{l|l|c|cccc|cccc|ccc}
		\hline
		\multicolumn{2}{c|}{\textbf{Dataset}} & & \multicolumn{4}{c|}{\textbf{CIRCO}} & \multicolumn{7}{c}{\textbf{CIRR}} \\
		\hline
		& Metric & & \multicolumn{4}{c|}{mAP@k} & \multicolumn{4}{c|}{Recall@k} & \multicolumn{3}{c}{$R_s$@k} \\
		\cline{2-14}
		Arch & Method & $\alpha_I$ & k=5 & k=10 & k=25 & k=50 & k=1 & k=5 & k=10 & k=50 & k=1 & k=2 & k=3 \\
		\hline
		\multirow{6}{*}{ViT-B/32} 
		& Image-only \textdagger & - & 1.34 & 1.60 & 2.12 & 2.41 & 6.89 & 22.99 & 33.68 & 59.23 & 21.04 & 41.04 & 60.31 \\
		& Text-only \textdagger & - & 2.56 & 2.67 & 2.98 & 3.18 & 21.81 & 45.22 & 57.42 & 81.01 & 62.24 & 81.13 & 90.70 \\
		& Image + Text \textdagger & - & 2.65 & 3.25 & 4.14 & 4.54 & 11.71 & 35.06 & 48.94 & 77.49 & 32.77 & 56.89 & 74.96 \\
		& SEARLE + \textbf{PDV-I} & 1.5 & 4.77 & 5.23  & 6.31 & 6.82 & 16.65 & 42.53 & 55.16 & 81.42 & 44.68 & 67.78 & 82.94\\
		& CIReVL + \textbf{PDV-I} & 2.0 & \textbf{10.29 }& \textbf{10.80} & \textbf{12.23} & \textbf{12.93} & \textbf{27.18} & \textbf{56.53} & \textbf{67.76} & \textbf{87.64} & \textbf{59.81} & \textbf{79.59} & \textbf{90.15}\\
		& LDRE + \textbf{PDV-I} & 2.0 & 8.00 & 8.88 & 10.06 & 10.72 & 23.37 & 51.21 & 63.69 & 85.57 & 55.57 & 76.63 & 88.15\\
		\hline
	\end{tabular}
	\caption{PDV-I performance on CIRCO and CIRR test datasets. Note that the image-only approach utilizes the visual embedding of the reference image, whereas the text-only approach employs the text embedding of the prompt.}
	\label{tab:circo_cirr_results_pdv-I}
\end{table*}

We evaluated PDV-F alongside four baseline approaches (CIReVL, LDRE, Pic2Word, and SEARLE) across three benchmarks. Notably, CIReVL was tested with three different backbones on three datasets, as its models and intermediate results are publicly available. However, for the remaining methods, we conducted partial evaluations due to limited open-source availability or restricted support.

The numerical results are presented in Tables \ref{tab:fashion_iq_results} and \ref{tab:circo_cirr_results}.
On the FashionIQ benchmark, PDV-F yields substantial improvements for all baseline approaches, with CIReVL showing particularly strong gains that scale with backbone size. Similarly, all methods demonstrate significant performance improvements on CIRCO and CIRR datasets. Notably, CIReVL achieves larger improvements compared to other methods, with the most substantial gains observed when using small and medium backbone architectures. Our PDV-F implementation within the CIReVL framework consistently outperformed other state-of-the-art methods, including LinCIR and SEIZE, across most evaluation metrics. Similar to SEIZE, PDV-F offers the advantage of being entirely training-free; however, unlike SEIZE, it does not significantly increase feature extraction computational costs. While LinCIR demonstrates exceptional inference speed, it lacks the training-free nature of our approach, requiring dedicated model training before deployment.  




