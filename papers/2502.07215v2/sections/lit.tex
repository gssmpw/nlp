\section{Related Work}
\label{sec:lit}
\textbf{Vision-Language (VL) models} have revolutionized computer vision by effectively bridging visual and textual modalities. The emergence of powerful models such as CLIP \cite{radford2021learning}, ALIGN \cite{jia2021scaling}, and Florence \cite{yuan2021florence} has enabled remarkable advances in multi-modal understanding. Trained on large-scale image-text pairs through contrastive learning, these models learn rich visual-semantic representations that generalize across domains and tasks.
Building upon these advances, Composed Image Retrieval (CIR) has shown significant progress \cite{baldrati2022effective,saito2023pic2word,karthik2024visionbylanguage}. Early approaches leveraged VL models and either trained a combiner network to compose text and image features \cite{baldrati2022effective}, or fine-tuned a text encoder \cite{baldrati2022conditioned} to extract task-specific text features. However, these methods required expensive domain-specific triplets (reference image, modified image, and text description) that must be manually verified. Recent work has explored alternative approaches to reduce the data collection burden, such as using synthetic triplets \cite{gu2023compodiff} or mining triplets from large-scale image-text datasets \cite{liu2023zeroshot}. However, these methods still incur significant computational costs during training.

\noindent{{\bf Zero-shot CIR with Text Inversion}} Recent research has focused on zero-shot approaches to address these challenges. Many methods adopt \textit{text inversion}, a technique initially proposed for personalized image generation \cite{gal2022textual,ruiz2023dreambooth}, which maps images to pseudo-tokens or words. Pic2Word \cite{saito2023pic2word} introduced a self-supervised text inversion network trained with cyclic contrastive loss, though it requires a large-scale image dataset. SEARLE \cite{baldrati2023zero} reduces the cost of training Pic2Word and improves the efficiency of the text inversion network. KEDs \cite{suo2024knowledge} implicitly models the attributes of the reference images by incorporating a database; thus, tokens obtained through inversion include attributes such as color, object number and layout. To further improve scalability, LinCIR \cite{gu2024lincir} proposed a language-only approach that reduces training costs and increases scalability. Most recently, CIReVL \cite{karthik2024visionbylanguage} introduced a more direct approach that leverages image captioning models to generate natural language descriptions of reference images, which are then combined with text that specifies desired modifications to form queries. Subsequently proposed methods, such as LDRE \cite{yang2024ldre} and SEIZE \cite{yang2024semantic}, leverage multiple captions over a single caption to increase the diversity and also take the semantic increment during the composition into consideration.

\noindent{{\bf Composition with a Residual}} 
In contrast to ZS-CIR, early supervised CIR approaches learned prompt-induced modifications by training on labelled triplet data (reference image, prompt, and target image). Vo et al. \cite{vo2019composing} pioneered this approach by introducing a residual learning module based on an LSTM network. Subsequently, several methods \cite{chen2020learning,wu2021fashion,yu2020curlingnet} adopted similar residual learning strategies for text-image composition. Baldrati et al. \cite{baldrati2022conditioned} further advanced this approach by fine-tuning CLIP's text encoder to learn residual embeddings.
While these prior works explored residual-based approaches, they all relied on supervised training. In contrast, our proposed PDV achieves similar capabilities by directly leveraging pre-trained VL models, eliminating the need for task-specific training.







