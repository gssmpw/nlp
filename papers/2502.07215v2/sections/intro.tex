\section{Introduction}
\label{sec:intro}



Composed Image Retrieval (CIR) involves searching for images using a combination of a reference image and a prompt that describes how the target image should differ from the reference \cite{ vo2019composing, baldrati2022effective,baldrati2023zero,saito2023pic2word}. Compared to traditional content-based image retrieval (CBIR) systems, CIR offers increased flexibility and precision by allowing users to articulate complex, multi-modal queries that combine visual and semantic information \cite{saito2023pic2word,karthik2024visionbylanguage,cohen2022my}. %

\begin{figure*}[!ht]
	\centering
	\includegraphics[width=0.7\linewidth]{images/CIR_mot_2x2_img.drawio-SK.pdf}
	\caption{Overview of Prompt Directional Vector (PDV) for Zero-Shot Composed Image Retrieval (ZS-CIR). (a) Standard ZS-CIR pipeline. (b) PDV calculation process. (c) Dynamic text embedding composition using PDV. (d) Fusion of composed embeddings: PDV-modified image embedding combined with composed text embedding.}
	\label{fig:motiv}
\end{figure*}

The core challenge in CIR lies in effectively integrating information from two distinct modalities: image and text. With the rapid progress in vision and language models (VLMs), CIR has attracted significant attention in the computer vision community \cite{liu2021image,baldrati2022effective,karthik2024visionbylanguage,saito2023pic2word,baldrati2023zero}. Early approaches to CIR were primarily supervised in nature \cite{kim2021dual,anwaar2021compositional,vo2019composing,chen2020learning,wu2021fashion,lee2021cosmo}. However, as highlighted by Saito et al. \cite{saito2023pic2word}, the labeling cost for supervised datasets in this domain is prohibitively high, prompting researchers to explore more efficient alternatives, namely zero-shot composed image retrieval (ZS-CIR). In this work, we provide a simple and training-free approach to improve the controllability and accuracy of existing ZS-CIR approaches.

ZS-CIR leverages VLMs, denoted by $\Psi$, which operate through a dual-pathway architecture. The first pathway consists of a vision branch, $\Psi_I$, that extracts feature representations from target images, $I_{target}$. The second pathway employs a language branch, $\Psi_T$, that processes a textual composition of reference images, $I_{ref}$, and user-provided text prompts, $P$. This composition, represented by $\mathcal{F}(I_{ref}, P)$, can be achieved through two primary methods: (1) \textit{Caption Generation}, where a caption is generated for the reference image using a VLM, and this caption is merged with the text-prompt using Large Language Models (LLMs), as demonstrated in CIReVL \cite{karthik2024visionbylanguage}; or (2) \textit{Pseudo Tokenization}, which uses CLIP's \cite{radford2021learning} visual branch to process $I_{ref}$ and a mapping network (consisting of a lightweight multi-layer perceptron) to tokenise the image, as demonstrated in Pic2Word \cite{saito2023pic2word}. The resulting $\mathcal{F}(I_{ref},P)$ is a textual query representation that encompasses both the provided visual and text information, and facilitates zero-shot retrieval.
The aforementioned pipeline is illustrated in Figure \ref{fig:motiv}\red{a}. 

We identify three major gaps in the literature, despite the promising results \cite{saito2023pic2word,karthik2024visionbylanguage,gu2024lincir,baldrati2023zero}:

\noindent{\bf{Gap 1: Staticly Composed Text Embedding}}. Target images, $I_{target}$, may not appear in the Top-K retrieved results when other gallery images produce embeddings closer to the composed text embedding $\Psi_{T}(\mathcal{F}(I_{ref},P))$ than the target image embedding $\Psi_{I}(I_{target})$, as shown in Figure \ref{fig:motiv}\red{a}. In such cases, users must iteratively refine their prompts and regenerate composed text embeddings, incurring additional manual effort and computational overhead.
    
\noindent{\bf{Gap 2: Underutilisation of Reference Image Embedding}}. Current methods generally do not utilize the embedding $\Psi_{I}(I_{ref})$ of the reference image directly for retrieval; instead, $\Psi_{I}(I_{ref})$ is used solely for composition. This omission stems from consistently poor retrieval performance when incorporating these embeddings, as documented in multiple studies \cite{saito2023pic2word,karthik2024visionbylanguage,baldrati2023zero}.

\noindent{\bf{Gap 3: Suboptimal Performance of Image-Text Embedding Fusion}}. While the fusion of image and text embeddings outperforms single-modality approaches (image-only or text-only) \cite{saito2023pic2word,karthik2024visionbylanguage,baldrati2023zero}, it still underperforms compared to composed text embeddings.

\noindent{\bf{Promp Directional Vector (PDV): a Plug-and-Play Solution}}. We propose the \emph{Prompt Directional Vector} (PDV) as a straightforward, training-free approach to address the aforementioned gaps. Denoted by $\Delta_{PDV}$, the PDV represents the residual vector between two text embeddings: the composed text embedding $\Psi_{T}(\mathcal{F}(I_{ref},P))$ and the reference image text embedding $\Psi_{T}(\mathcal{F}(I_{ref}))$. The latter is equivalent to $\Psi_{T}(\mathcal{F}(I_{ref}, P_{Empty}))$, where $P_{Empty}$ represents an empty input string, corresponding to the unprompted baseline. As illustrated in Figure \ref{fig:motiv}\red{b} and shown via a red arrow, this PDV captures the semantic modification induced by the prompt. In the following, we summarize how the PDV effectively addresses the three aforementioned challenges.

\noindent{\bf{\method: Addressing Gap 1}}. To change the static nature of the composed text embedding and increase flexibility and utility for users, we generalise the synthesis of the composed text embeddings $\Psi_{T}(\mathcal{F}(I_{ref}, P))$. We interpret $\Psi_{T}(\mathcal{F}(I_{ref}, P)$ as a shift from the reference image text embedding without the prompt, $\Psi_{T}(\mathcal{F}(I))$, by a vector $\Delta_{PDV}$. Under this formulation, the baseline ZS-CIR approach can be viewed as a special case where $\Psi_{T}(\mathcal{F}(I_{ref}, P)) = \Psi_{T}(\mathcal{F}(I_{ref}, P)) + \alpha\Delta_{PDV}$ with $\alpha=1$. We hypothesize that when $\Delta_{PDV}$ captures the desired modifications, but not their precise magnitude (particularly with less descriptive prompts), adjusting $\alpha$ can enhance retrieval performance and controllability. As demonstrated in Figure \ref{fig:motiv}\red{c}, increasing $\alpha$ to 1.3 produces results more closely aligned with the target compared to the default $\alpha=1$.

    
\noindent{\bf{\method: Addressing Gap 2}}. Although image embeddings $\Psi_{I}(I_{ref})$ contain valuable visual content regarding the reference image, they lack prompt-specific semantic information, leading to poor performance when used in ZS-CIR. By leveraging the shared semantic space learned by Vision-Language models, we can transfer prompt semantics to the image embedding by adding the Prompt Vector $\Delta$, obtaining $\Psi_{I}(I_{ref}) + \alpha\Delta_{PDV}$, as illustrated in Figure \ref{fig:motiv}\red{d}. We denote this augmented representation as the \textit{composed image embedding}. Similar to the dynamic composed text embedding, this representation can be adjusted through a scaling factor, $\alpha$ to offer controllability to enhance retrieval.


\noindent{\bf{\method: Addressing Gap 3}}. 
Lastly, several studies demonstrate that the direct fusion of image and text embeddings outperforms using either input feautre (image or text-prompt) alone. \cite{saito2023pic2word,karthik2024visionbylanguage,baldrati2023zero}. However, this fusion approach still underperforms compared to using the composed text embeddings. This performance gap exists because prompt embeddings are significantly changed by incorporating context from the reference image. Specifically, $\Delta$ is not equivalent to $\Psi_{T}(P)$. To address this, we propose fusing the composed text and composed image embeddings, as illustrated in Figure \ref{fig:motiv}\red{d}. Through varying the fusion weight factor $\beta$, we can dynamically control the balance between visual similarity to the reference image and semantic alignment with the prompt without needing to craft new prompts, or modify reference images. Lower $\beta$ values prioritize visual fidelity, while higher values emphasize semantic modifications specified in the prompt.


PDV serves as a plug-and-play enhancement for most ZS-CIR approaches, offering a simple and training-free solution. The computational overhead is minimal, requiring only the calculation of text and image embeddings from the reference image. We evaluate PDV by integrating it with four distinct ZS-CIR methods across various CIR benchmarks. Our experimental results demonstrate that all three use cases of PDV consistently improve upon the baseline approaches, particularly when the baseline method already generates accurate compositional embeddings.

\noindent{\textbf{Contributions}}. Our main contributions are as follows:
\begin{itemize}
   \item We introduce the Prompt Directional Vector (PDV), a simple and training-free enhancement that overcomes limitations of current Zero-Shot CIR methods.
   
   \item We propose three novel applications of PDV: (1) dynamic composed text embedding synthesis through PDV scaling, which offers enhanced control over retrieval results without tedious prompt modification; (2) composed image embedding synthesis via semantic transfer of prompts to visual features through PDV addition, which prioritizes visual similarity; and (3) effective fusion of composed text and image embeddings, which improves overall performance and enables controllable balancing of visual and semantic similarity.
   
   \item Through extensive experiments on multiple benchmarks with four ZS-CIR methods, we demonstrate that PDV consistently improves retrieval performance with minimal computational overhead.
\end{itemize}

