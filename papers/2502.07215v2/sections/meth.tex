\section{Methodology}
\label{sec:meth}

\subsection{Baseline ZS-CIR Framework}
Composed Image Retrieval (CIR) enables users to search for target images $I_{target}$ by providing a reference image, $I_{ref}$, and a text prompt, $P$, describing desired modifications. Zero-shot composed image retrieval (ZS-CIR) leverages Vision-Language (VL) models, $\Psi$, such as CLIP \cite{radford2021learning}, whose vision branch, $\Psi_{I}$, and text branch, $\Psi_{T}$, are trained to learn a shared embedding space where semantically similar image and text pairs are mapped close to each other. In this framework, as show in Figure \ref{fig:motiv}\red{a}, target images are encoded using the vision branch, $\Phi_{I}$, while the query is composed by processing both $I_{ref}$ and $P$ through the text branch $\Psi_{T}$, as composition operations are more naturally handled in the text modality. 

Recent ZS-CIR approaches generate the composed text embedding from $I_{ref}$ and $P$ using one of two methods: direct image captioning (CIReVL, LDRE and SEIZE) or pseudo tokenization (Pic2Word, LinCIR, SEARLE and KEDs). We denote this composition process as $\mathcal{F}$, resulting in a composed text embedding $\Psi_T(\mathcal{F}(I_{ref}, P))$.

In an ideal ZS-CIR scenario, the target image $I_{target}$ should appear within the top-k results retrieved from the gallery $\mathcal{D}$. This retrieval is formalized as:
\begin{equation}
\label{eq:zscir}
\mathbb{I}_{top-k} = \argmax_{I \in \mathcal{D}} \frac{\Psi_T(\mathcal{F}(I_{ref}, P))^T \cdot \Psi_I(I)}{\lVert\Psi_T(\mathcal{F}(I_{ref}, P))\rVert \cdot \lVert\Psi_I(I)\rVert}.
\end{equation}
If $I_{target} \notin \mathbb{I}_{top-k}$, the user must reformulate the prompt and repeat the feature extraction process to obtain alternative retrieval results, incurring time and computational resource costs. Notably, as shown in Eq. \ref{eq:zscir}, only the composed feature embedding $\Psi_T(\mathcal{F}(I_{ref}, P))$ directly influences the computation of $\mathbb{I}_{top-k}$ results. Although the gallery images are represented by their image embeddings, the image embedding of the reference image $\Psi_I(I_{ref})$ does not contribute to the retrieval process.



\subsection{Our Approach: Prompt Directional Vector}

Rather than simply employing the composed embedding alone, $\Psi_T(\mathcal{F}(I_{ref}, P))$, as depicted in Figure \ref{fig:motiv}\red{b},
we propose a generalized formulation of composed text embeddings by considering the embedding modification direction, $\Delta_{PDV}$, which is derived from the difference between the provided prompt, $P$, and the reference image, $I_{ref}$. Formally, we define $\Delta_{PDV}$ as,
\begin{equation}
\Delta_{PDV} = \Psi_T(\mathcal{F}(I_{ref}, P)) - \Psi_T(\mathcal{F}(I_{ref})).
\label{eq:pdv}
\end{equation}
We then form the composed text embedding as follows,
\begin{equation}
	\Psi_T(\mathcal{F}(I_{ref}, P)) = \Psi_T(\mathcal{F}(I_{ref})) + \alpha_T\Delta_{PDV}, \label{eqn:text_embedding}
\end{equation}
{\noindent}where $\alpha$ controls the movement along the prompt vector $\Delta_{PDV}$ and $\Psi_T(\mathcal{F}(I_{ref}))$ is the original text embedding.

\subsection{Strategies for Using PDV}

We explore three strategies for using $\Delta_{PDV}$:

\textbf{(1) Prompt Directional Vector for Text (PDV-T)}, which enhances controllability in ZS-CIR. While baseline ZS-CIR approaches represent a special case where $\alpha=1$, varying $\alpha$ provides users with additional control over the retrieval process (refer to Figure \ref{fig:motiv}\red{c}). Setting $\alpha>1$ amplifies the modification specified by the prompt, while $\alpha<1$ reduces its effect. This approach offers a more efficient alternative to modifying the prompt directly, as it requires neither new feature extraction nor prompt reformulation. Note that we use the notation $\Phi_{PDV-T}$ to represent the composed text embedding.

\begin{figure}[!tbh]
\vspace*{-0.4cm}
\centerline{
\includegraphics[width=1.1\linewidth]{images/CIR_vs_I+T.drawio.pdf}}
	\caption{Comparison of Image + Text (a) vs PDV (b).}
	\label{fig:vtvspv}
\end{figure}

\textbf{(2) Prompt  Directional Vector for Image (PDV-I)}, which extends the modification principle to visual embeddings. While previous approaches primarily relied on composed text embeddings, experimental results show that direct fusion of image and text features yields inferior performance compared to composed features. We hypothesize that this performance gap arises because the direct text embedding, $\Phi_{T}(P)$, differs significantly from the prompt vector $\Delta_{PDV}$, as illustrated in Figure \ref{fig:vtvspv}. This difference occurs because the semantic meaning of natural language is context-sensitive, where in our case the context is provided by the reference image embedding $\Psi_T(\mathcal{F}(I_{ref}))$. To address this limitation, we propose combining $\Delta_{PDV}$ with visual embeddings. Specifically, we compute the composed visual embedding $\Phi_{PDV-I}$
as $\Psi_{I}(I_{ref}) + \alpha_I\Delta_{PDV}$, where $\Psi_{I}(I_{ref})$ represents the original visual embedding obtained from the reference image, and the same prompt vector obtained via Eq. \ref{eq:pdv} is used to modify this visual representation.

\textbf{(3) Prompt  Directional Vector Fusion (PDV-F)}, which calculates the final similarity score between a query and target image which combines both composed embeddings. This fusion embedding, $\Phi_{PDV-F}$, can be defined as,
\begin{equation}
	\Phi_{PDV-F} = (1-\beta)\Phi_{PDV-I} +\beta\Phi_{PDV-T},
\end{equation}
where $\beta$ is a weighting parameter balancing the contribution of the composed visual and textual embeddings.







