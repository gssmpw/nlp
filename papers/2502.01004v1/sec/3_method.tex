\section{Method}
\subsection{Preliminaries}
\noindent\textbf{Problem formulation.}
Zero-shot 6D pose estimation in bin-picking aims to detect the target \note{objects} in the input RGB-D image $I\! \in\! \mathbb{R}^{H \times W \times 4}$ of the scene, based on the given CAD model $\mathcal{O}$, and to predict their relative 6D pose transformations $R \in SO(3)$ and $t \in \mathbb{R}^3$ \emph{w.r.t} the CAD model.



\vspace{1mm}
\noindent\textbf{Zero-shot 6D pose estimation pipeline.} Given that directly estimating the pose of randomly stacked \note{workpieces} is quite challenging, we adopt the popular two-stage pipeline~\cite{chen2023zeropose, sam6d, FoundationPose, nguyen2024gigapose}. It simplifies pose estimation into two sub-tasks: 1) object detection and 2) point registration via RGB-D feature matching. Next, we briefly describe this pipeline.

The pipeline usually employs a CAD-prompt segmentation model~\cite{chen2023zeropose, sam6d} to detect each instance of the target object from $I$, obtaining the 2D mask of each instance. With the mask and depth map, it extracts the point cloud $\mathcal{P}_s = \{p_i \in \mathbb{R}^3\}_{i=1}^{N_s}$ for each instance of the target object in the camera coordinate system. Herein $N_s$ is the valid pixels in the mask. 

With the point cloud of each instance, the pose estimation is formulated as a point registration problem. Defining an object coordinate system in CAD and denoting the point cloud uniformly sampled from CAD as $\mathcal{P}_o = \{q_i \in \mathbb{R}^3\}_{i=1}^{N_o}$, where $N_o$ is the sampled point number. The pipeline first estimates the point-to-point correspondence $\mathcal{C}$ between $\mathcal{P}_s$ and $\mathcal{P}_o$ through \noteb{feature matching of the color point clouds}. Then, it calculates the pose parameter $R$ and $t$ by solving: 
\begin{equation}
\label{equ:solve_pose}
\min_{R, t} \sum\nolimits_{({p}_{x_i}, {q}_{y_i}) \in \mathcal{C}} \lVert R {p}_{x_i} + t - {q}_{y_i} \rVert^2_2.
% \vspace{-3pt}
\end{equation}

The \noteb{color point feature matching method} highly relies on discriminative textures to model the correspondence. It struggles to deal with the workpieces lacking sufficient texture and containing ambiguous local regions with similar shapes and appearances.


\subsection{Overview of ZeroBP}
\noteb{We first provide an overview of ZeroBP, a zero-shot 6D pose estimation method customized for bin-picking. It performs zero-shot 6D pose estimation via two steps}: 1) picking-box-aware workpiece detection, and 2) robust point registration with position-aware correspondence.

\vspace{1mm}
\noindent\textbf{Picking-box-aware workpiece detection.}
ZeroBP exploits the prior information of the bin-picking task to improve the detection accuracy. Specifically, it employs a zero-shot detector~\cite{groundingdino} with the tailored text description prompt to pre-locate the picking box, and then detect the workpieces within the picking box following~\cite{chen2023zeropose}, avoiding the interference on workpiece detection from cluttered backgrounds.



\vspace{1mm}
\noindent\textbf{Point registration with position-aware correspondence.} With the point clouds of the instance and CAD model, $\mathcal{P}_s$ and $\mathcal{P}_o$, ZeroBP calculates the correspondence $\mathcal{C}$ between them and accordingly solve Eq.~\eqref{equ:solve_pose} to obtain $R$ and $t$. 

To address the challenge posed by workpieces with insufficient texture and ambiguous local regions, we propose to learn Position-Aware Correspondence (PAC) based on both local features and global positions of the points, as shown in Figure~\ref{fig:overall_framwork}. \noteb{The key to PAC is encoding the global position of the heterogeneous point clouds, $\mathcal{P}_s$ and $\mathcal{P}_o$ in a comparable manner so that they can be used to guide the model to discriminate the local regions with similar shapes and appearances. Comparability means that we first need to project $\mathcal{P}_s$ and $\mathcal{P}_o$ to a shared coordinate system, which however requires knowing the relative pose between $\mathcal{P}_s$ and $\mathcal{P}_o$.} This raises an intriguing cyclical dependency issue: accurate global position is necessary for robust pose estimation, yet acquiring the global position itself depends on the pose. \noteb{To handle this problem, we introduce an initial pose and alternately refine the global position and pose step-by-step. During step-wise refining, we use position-aware cross-attention to fully integrate the global positions and local features to establish robust correspondence.}

We adopt a coarse-to-fine correspondence modeling strategy, following~\cite{qin2023geotransformer}. As shown in Fig.~\ref{fig:overall_framwork}, we first use KPConv-FPN~\cite{thomas2019kpconv} to extract the hierarchical features of $\mathcal{P}_s$ and $\mathcal{P}_o$, obtaining the coarse-level superpoints $\mathcal{P}^c_s$, $\mathcal{P}^c_o$ and their features $\mathcal{F}^c_s \in \mathbb{R}^{\lvert {\mathcal{P}^c_s} \vert \times d^c}$, $\mathcal{F}^c_o \in \mathbb{R}^{\lvert {\mathcal{P}^c_o} \vert \times d^c}$ and the fine-level points $\mathcal{P}^f_s$, $\mathcal{P}^f_o$ and their features $\mathcal{F}^f_s \in \mathbb{R}^{\lvert {\mathcal{P}^f_s} \vert \times d^f}$, $\mathcal{F}^f_o \in \mathbb{R}^{\lvert \mathcal{P}^f_o \vert \times d^f}$, where $d^c$ and $d^f$ are feature dimensions. 
\note{Note that we technically feed color point clouds to KPConv-FPN.}
\noteb{With the features, ZeroBP first models the coarse-level position-aware correspondence on representative superpoints, and then accordingly models the fine-level position-aware correspondence on the dense points. Subsequently, it solves Eq.~\ref{equ:solve_pose} to obtain the final predicted pose parameter $\hat{R}$ and $\hat{t}$.}

%After extracting the points within the local region of each superpoint, ZeroBP further models the fine-level position-aware correspondence on these dense points 

\subsection{Global positional encoding of heterogeneous point clouds}
The global position aims to aid in distinguishing between two points having similar shape and appearance but located far apart. Different from existing positional encoding~\cite{transformer,qin2023geotransformer} used for homogeneous sources, our positional encoding is designed for processing heterogeneous point clouds, \emph{i.e.}, $\mathcal{P}_s$ and $\mathcal{P}_o$. To address the above-mentioned cyclical dependency between the pose and global position, we first estimate an approximate initial pose and then alternately refine the global position and pose step-by-step. For the positional encoding itself, we propose a multiplicative positional encoding, defined as the directional vector from the object centroid to the surface point. It can be seamlessly integrated into correspondence modeling.

\vspace{1mm}
\noindent\textbf{Initial pose estimation.}
To obtain an initial pose, we estimate the pose of the target workpieces based on only the local features. Specifically, we perform feature matching between the coarse-level features of the scene instance $\mathcal{F}_s^c$ and the CAD model $\mathcal{F}_o^c$ by calculating their cosine similarity matrix. Then, we select the superpoint pairs with top $K$ similarities to obtain the correspondence $\hat{\mathcal{C}}^c$. 
With the correspondence $\hat{\mathcal{C}}^c$, we minimize Eq.~\eqref{equ:solve_pose} to generate the initial pose $\hat{R}'$, $\hat{t}'$ by the weighted singular value decomposition (WSVD)~\cite{svd} algorithm.

\vspace{1mm}
\noindent\textbf{Position encodings via directional vector.}
With the initial pose, we separately transform the heterogeneous superpoints, $\mathcal{P}^{c}_s$ and $\mathcal{P}^{c}_o$, to obtain the directional vectors within a shared coordinate system. Without loss of generality, we set the above-mentioned centroid defining the direction as the origin of the object coordinate system in the CAD model.
In this way, we translate the superpoints of the scene instance $\mathcal{P}_s^c$ by $\hat{t}'$ and rotate the superpoints of the CAD model $\mathcal{P}_o^c$ for alignment. Then we can obtain the positional encodings $\hat{\mathcal{V}}_s^c \in \mathbb{R}^{\lvert {\mathcal{P}^c_s}\rvert \times 3}$ and $\hat{\mathcal{V}}_o^c \in \mathbb{R}^{\lvert {\mathcal{P}^c_o}\rvert \times 3}$ after normalization, \emph{i.e.,} the directional vectors of the superpoints:
\note{\begin{equation}
\label{eq:coe}
    \hat{\mathcal{V}}_s^c = \frac{\mathcal{P}_s^c - \hat{t}'}{\lVert \mathcal{P}_s^c - \hat{t}'\rVert^2_2}, \hat{\mathcal{V}}_o^c = \frac{\hat{R}' \mathcal{P}_o^c}{\lVert \hat{R}'  \mathcal{P}_o^c\rVert^2_2}.              
\end{equation}}
Eq.~\eqref{eq:coe} formulates the positional encodings for superpoints, and the positional encoding for dense points can be done in the same way. Fig.~\ref{fig:overall_framwork} depicts the positional encoding process.

The directional vectors are multiplicative positional encodings. The inner product between two vectors represents the angle between the corresponding directions, measuring the position consistency between the superpoints from $\mathcal{P}_{s}^{c}$ and $\mathcal{P}_{o}^{c}$. This multiplicative nature means that the positional encodings can be directly integrated into the cross-attention between the features of $\mathcal{P}^{c}_s$ and $\mathcal{P}^{c}_o$, allowing learning correspondence based on both local features and global positions.

\vspace{1mm}
\noindent\textbf{Alternate refinement between pose and global position.} With the above positional encoding, ZeroBP models the position-aware correspondence between $\mathcal{P}_{s}^{c}$ and $\mathcal{P}_{o}^{c}$ and uses WSVD to solve Eq.~\eqref{equ:solve_pose} to obtain a new estimated pose, as shown in Fig.~\ref{fig:overall_framwork}. Then ZeroBP uses the new pose to calculate the positional encoding following Eq.~\eqref{eq:coe}. Overall, ZeroBP repeats the above alternate refinement to obtain accurate correspondence between the coarse-level superpoints $\mathcal{P}_{s}^{c}$ and $\mathcal{P}_{o}^{c}$. A similar alternate refinement mechanism is also employed for modeling the correspondence between the fine-level points $\mathcal{P}_{s}^{f}$ and $\mathcal{P}_{o}^{f}$.

\input{tables/pose_results}
\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.9 \textwidth]{figures/vis_robi.jpg}
    \caption{Visualizations of the 6D pose estimation results on the real-world bin-picking dataset ROBI~\cite{robi}.
}
    \label{fig:vis_robi}
    \vspace{-5mm}
\end{figure*}


\subsection{Position-aware cross-attention}
To effectively exploit the global position to improve correspondence modeling, we propose a bidirectional Position-Aware Cross-Attention (PACA) to perform feature interaction between $\mathcal{P}_s^c$ and $\mathcal{P}_o^c$ assisted by the global position information.
As shown in Fig.~\ref{fig:overall_framwork}, PACA leverages the global position information in two manners: 1) it uses an embedding layer to map the positional encodings (\emph{i.e.}, the directional vectors $\hat{\mathcal{V}}_s^c$ and $\hat{\mathcal{V}}_o^c$) into the embedding space and adds the resulting positional embeddings ($\hat{\mathcal{E}}_s^c$ and $\hat{\mathcal{E}}_o^c$) to the superpoint features ($\hat{\mathcal{F}}_s^c$ and $\hat{\mathcal{F}}_o^c$) for attention calculation; and 2) it employs the cosine similarity matrix between the positional encodings $\hat{\mathcal{V}}_s^c$ and $\hat{\mathcal{V}}_o^c$ to reweight the original attention map.

Herein we take the scene-to-CAD cross-attention as an example to formulate the position-aware cross-attention. With the positional embeddings, the query, key, and value are calculated as $Q = (\hat{\mathcal{E}}_s^c + \hat{\mathcal{F}}_s^c)W^q$, $K = (\hat{\mathcal{E}}_o^c + \hat{\mathcal{F}}_o^c)W^k$, $V = \hat{\mathcal{F}}_o^cW^v$, where $W^{q/k/v} \in \mathbb{R}^{d^c \times d^c}$ are projection weights. Then, we use the above-mentioned cosine similarity matrix to reweight the attention map calculated by $K$ and $V$. Thus the reweighted attention map $\mathcal{A}$ can be formulated as:
\begin{align}
    &\mathcal{A} = \sigma(\frac{Q K^T}{\sqrt{d^c}} \cdot \mathcal{A}^c_{\cos}),\\
    &\mathcal{A}_{\cos}^c = (\hat{\mathcal{V}}_s^c (\hat{\mathcal{V}}_o^{c})^{T} + 1) / 2.
\end{align}
Herein $\sigma$ denotes the Softmax function~\cite{lecun1989backpropagation}, $\mathcal{A}_{\cos}^c$ is the cosine similarity matrix normalized to $[0,1]$, and $\cdot$ is element-wise multiplication.
Finally, we multiply the attention matrix $\mathcal{A}$ with the value matrix to obtain the enhanced features $\hat{\mathcal{F}}_s^c$ of the scene superpoints, as follows:
\begin{equation}
    \hat{\mathcal{F}}_s^c = \phi(Q + \mathcal{A}V).
\end{equation}
Herein $\phi$ refers to the feed-forward network~\cite{lecun2015deep}. CAD-to-scene cross-attention can be done in a similar way to obtain the enhanced features $\hat{\mathcal{F}}_o^c$ of the CAD superpoints.


\subsection{Coarse-to-fine position-aware correspondence modeling}
\noindent\textbf{Modeling position-aware correspondence on superpoints.}
To estimate the coarse-level position-aware correspondence in superpoints, we stack the position-aware attention modules \noteb{for} $N$ layers and perform alternate refinement for $N$ steps. 
Given the initial pose $\hat{R}'$ and $\hat{t}'$, and superpoint points $\mathcal{P}_s^c$ and $\mathcal{P}_o^c$, we calculate their position encodings $\hat{\mathcal{V}}_s^c$ and $\hat{\mathcal{V}}_o^c$. Then we combine these positional encodings with their features and perform bidirectional position-aware cross-attention to enhance the superpoint features. After that, we perform matching with the enhanced superpoint features to obtain the correspondence and update the initial pose for the next refinement step. After $N$-step refinement, we obtain a robust coarse-level correspondence.

\vspace{1mm}
\noindent\textbf{Modeling position-aware correspondence on points.}
To estimate the fine-level position-aware correspondence in points, we first find the points in the reception field of superpoints, obtaining the dense points and their fine-level features from the backbone.
Similar to the processing on superpoints, we stack the module for $N$ layers and perform alternate refinement for $N$ steps on dense points. Notably, there are two main differences with the superpoint stage. The initial pose $\hat{R}'$ and $\hat{t}'$ in the first layer is estimated based on the coarse-level correspondence. Following \cite{qin2023geotransformer, chen2023zeropose}, the cross-attention module is removed for efficiency. Herein we update the point feature from the embedding layers. After obtaining the fine-level correspondence, we estimate the final pose $\hat{R}$, $\hat{t}$ by solving Eq.~\eqref{equ:solve_pose}.
