\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/rag_diana.drawio.pdf}
    \caption{The \ac{RAG} workflow.}
    \label{fig:rag}
\end{figure}


\section{Background and problem description}
\label{sec:prelims}
We first explain the \ac{RAG} workflow and how \ac{RAG} uses vector databases.
We then introduce the concept of federated search and its associated challenges.

\subsection{\Acf{RAG}}
\ac{RAG} enhances the reliability of \ac{LLM} responses by integrating retrieved information as part of the input (or \emph{prompt})~\cite{lewis2020retrieval}.
Unlike traditional models that rely solely on pre-trained knowledge stored in their weights, \ac{RAG} retrieves relevant external documents or data during inference, improving accuracy and reducing hallucinations.
This approach is particularly helpful for tasks requiring up-to-date information or factual knowledge.

We illustrate the \ac{RAG} workflow in \Cref{fig:rag} which consists of eight steps.
At first, documents, videos, or other data are split into chunks. Each chunk is then converted into a high-dimensional vector using an embedding model~(step 1).
These embeddings are then stored in a vector database~(2).
When a user submits a query~(3), it is transformed into an embedding and passed to the retriever~(4), which searches for the most relevant stored embeddings~(5) and retrieves the corresponding data chunks~(6).
The retrieved context and the original query are then fed into the \ac{LLM}, which generates a response grounded in the retrieved information~(7).
This enriched query ultimately returns a more accurate and context-aware answer to the user than when not using \ac{RAG}~(8).

The vector database is a specialized database designed for similarity search, where queries are matched based on their vector representations rather than exact keyword matches.
In the context of \ac{RAG}, each document in the database is first converted into a high-dimensional embedding using a pre-trained \ac{ML} model.
By leveraging embeddings, vector databases can capture the semantic similarity between queries and documents, allowing for more flexible and context-aware retrieval compared to traditional keyword-based search methods~\cite{han2023comprehensive}.
When a query is issued, it is similarly transformed into an embedding, and the database retrieves the most relevant documents by performing a nearest-neighbor search in the embedding space.
This search is typically accelerated using indexing techniques such as \ac{ANN}~\cite{li2019approximate}, enabling efficient retrieval even with large data sources.


\begin{figure}[t]
	\centering
	\inputplot{plots/routing}{0}
	\caption{The relevance of different corpora in \ac{RAG} when answering questions, using question sets from \mirage.}
	\label{fig:routing}
\end{figure}

\subsection{Towards federated \ac{RAG} search}
Federated search is a retrieval paradigm where a query is executed across multiple independent data sources, aggregating relevant results without requiring data to be present in a single database~\cite{shokouhi2011federated}.
In the context of \ac{RAG}, federated search can enhance response accuracy by dynamically selecting appropriate knowledge bases for a given query.

A key challenge in federated \ac{RAG} search is determining which data sources are relevant to a given query and retrieving information only from those sources.
Not all data sources contribute equally to the queries.
We empirically show this by analyzing chunk relevance using corpora and questions from the \textsc{MIRAGE} benchmark.
\Cref{fig:routing} shows the relevance of different corpora, highlighting the variability in corpus relevance depending on the query.
For example, the bar corresponding to the \textsc{MedQA} question set and the \textsc{StatPearls} corpus shows a relevance score of 53.6\%, meaning that for 53.6\% of queries in \textsc{MedQA}, at least one chunk in the retrieved relevant chunks originates from \textsc{StatPearls}.
While some corpora, such as \textsc{PubMed}, consistently provide valuable information for all question sets, relying on a single corpus is often insufficient.
Indeed, results from~\cite{xiong-etal-2024-benchmarking} demonstrate that combining multiple corpora improves retrieval performance.
Corpora, such as \textsc{StatPearls} or \textsc{Wikipedia}, are useful in specific cases.
The differences in corpus relevance underscore the importance of adequate resource selection for a given query.


One must strike a balance in the number of data sources being queried.
Over-selecting data sources can dilute relevance by introducing potentially irrelevant data chunks in the \ac{LLM} prompt, making it harder for the \ac{LLM} to extract useful knowledge.
Under-selecting data sources risks missing critical information, particularly in domains where information is distributed sparsely across multiple repositories.
At the same time, we aim for the overall retrieval latency to be as low as possible, ensuring timely \ac{LLM} responses.
Achieving a good trade-off between retrieval efficiency and response quality remains an open problem.
Therefore, this work answers the following question: \emph{how can we design an efficient routing mechanism for federated \ac{RAG} search that minimize retrieval overhead while selecting relevant data sources?}

