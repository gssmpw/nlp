\begin{figure*}[th!]
    \centering
    \includegraphics[width=.9\linewidth]{figures/workflow.drawio.pdf}
    \caption{The workflow of \sys. The components specific to \sys are indicated in the box with the dashed border. In contrast to existing \ac{RAG} workflows that rely on a single data store, \sys enables efficient federated search by using a lightweight router to determine relevant data sources during an inference request.
    }
    \label{fig:workflow}
\end{figure*}

\section{Design of \sys}
\label{sec:design}

We now introduce the design of \sys.
In the following, we assume that there are $ n $ data sources that can contain information relevant to a user query.
First, we explain the high-level workflow of \sys and then provide details on how \sys selects which data sources to query.

\subsection{\sys workflow}
We visualize the \sys workflow in~\Cref{fig:workflow}, allowing the querying of $ n $ distinct data sources.
These data sources can, for example, be distributed across different organizations.
We show the components specific to \sys in the dashed box.
In line with existing \ac{RAG} systems, the query by a user (step 1) is converted into an embedding using an embedding model (2).
However, this query embedding is then forwarded to a \emph{router}, whose purpose is to decide which of the $ n $ data sources are relevant.
We detail the design of our router in \Cref{s:query-routing}.

After determining the relevant data sources, we send a query to these data sources with the vector embedding.
Using top-$k$ querying with $ n $ data sources results in $ m \times k $ retrieved embeddings, where $ m $ is the number of contacted data sources ($ m \leq n $) (4).
We refine the results by selecting the $ k $ embeddings closest to the query embedding (5).
Using these embeddings, we retrieve the associated data chunks (6).
Finally, the original user query and relevant data chunks are fed to the \ac{LLM} (7), and a response is generated (8).

\subsection{Lightweight query routing with shallow neural networks}
\label{s:query-routing}
To enable efficient retrieval across multiple data sources, \sys uses a lightweight query router, implemented as a shallow \ac{NN} with several fully connected layers.
This suffices to determine the relevance of each data source before retrieval.
Using a shallow \ac{NN} is inspired by practices in \ac{MoE} models and ensembles.
\ac{MoE} models leverage a small router function to decide which subset of experts to activate~\cite{zhou2022mixture}.
Similarly, shallow neural networks are used for decision-making in one-shot federated ensembles~\cite{allouah2024effectiveness}.
This work applies similar ideas to selecting relevant data sources in \ac{RAG}.

\subsubsection{Training phase}
The \sys router learns to make routing decisions by looking at query-data source relevance, essentially replaying step 3-5 in \Cref{fig:workflow} for model training.
Specifically, we send a set of training query embeddings to all $n$ data sources to obtain relevant embeddings and reduce the obtained $ n \times k $ embeddings to the top-$k$ most relevant ones, \eg, based on their distance to the query embedding.
$k$ is a parameter chosen by the user that we use for training and inference.
\sys assigns a binary relevance indicator to each query-data source pair based on whether or not the merged top-$k$ results contain embeddings from a given data source.
The model takes the following five features as input:
\begin{enumerate*}[label=\emph{(\roman*)}]
\item the query embedding,
\item the centroid of the queried data source,
\item the distance between the query embedding and the centroid,
\item the number of items in the queried data source, and
\item the density of the queried data source.
\end{enumerate*}
The centroid, computed as the average vector representation of all document embeddings in a data source, summarizes its overall semantic content.
The density of the data source quantifies how tightly packed the document embeddings are around the centroid, indicating whether the data source is highly specialized (high density) or more diverse (low density).
A single routing model is then trained using this data and the labels, allowing it to predict whether a given data source is relevant for future queries.

\subsubsection{Inference phase}
Once trained, \sys uses this model to efficiently route incoming user queries to relevant data sources.
We do an inference for each of the available data sources.
The inference request for each data source completes quickly (with sub-millisecond latency, see \Cref{sec:exp_efficiency}) and can be done in parallel or in batches since they are independent.


