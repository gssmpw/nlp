\section{Evaluation}
\label{sec:evaluation}


We implement \sys and evaluate its effectiveness and efficiency. 
Specifically, we measure 
\begin{enumerate*}[label=\emph{(\roman*)}]
\item the effectiveness of our routing mechanism in selecting relevant data sources (\Cref{sec:exp_routing_effectiveness}), 
\item the reduction in the total number of queries and communication volume (\Cref{sec:exp_efficiency}), and
\item the impact on end-to-end \ac{RAG} accuracy compared to querying all available sources (\Cref{sec:exp_accuracy}).
\end{enumerate*}

\subsection{Experimental setup}
We next outline the details of the router model and datasets used in our evaluation.

\subsubsection{Router model}
We use a three-layer fully connected \ac{NN} for routing.
The network consists of two hidden layers: the first hidden layer has 256 neurons, followed by Layer Normalization, ReLU activation, and Dropout, while the second hidden layer has 128 neurons, also followed by Layer Normalization, ReLU activation, and Dropout. The output layer consists of a single neuron that produces a raw logit score, predicting whether the corpus is relevant to the given query. 
The model is trained using Binary Cross-Entropy with Logits Loss (\textsc{BCEWithLogitsLoss}) with a positional weight to address class imbalance.
We use a cyclic scheduler for the learning rate $ \gamma $, oscillating $\gamma$ between 0.001 and 0.005.
Model performance is evaluated on the validation set after each epoch, and the best model is selected based on validation accuracy.
To train the model, we split training data at the question level, with a train-validation-test split ratio of 30\%-10\%-60\%.
Features are standardized using \textsc{StandardScaler} to normalize input distributions.





\subsubsection{Datasets.}
We evaluate \sys with two commonly used benchmarks: \mirage~\cite{xiong-etal-2024-benchmarking} and \mmlu~\cite{hendrycks2020measuring}.

\textbf{\mirage} is a benchmark designed to systematically evaluate \ac{RAG} systems for medical question answering~\cite{xiong-etal-2024-benchmarking}.
It consists of \num{7663} questions drawn from five widely used medical QA datasets.
We use \medrag as knowledge source, which includes five corpora with data chunks related to healthcare~\cite{xiong-etal-2024-benchmarking}.
For generating embeddings, we use \textsc{MedCPT}~\cite{jin2023medcpt}, a domain-specific model designed for biomedical contexts.
For retrieval, we use the \textsc{IndexFlatL2} index structure, provided by the \textsc{FAISS} library~\cite{douze2024faiss}, ensuring exact search and eliminating sources of approximation in our experiments.
We treat each corpus as a separate data source.
To emulate the setting with a single data source (conventional \ac{RAG}) in some experiments, all corpora are combined into a single dataset, referred to as \medcorp.
To run \sys with a \ac{RAG} pipeline, we adopt and extend the \medrag toolkit.
As \ac{LLM}, we use the open-source LLaMA 3.1 Instruct model~\cite{dubey2024llama}, and use the \textsc{Ollama} framework for inference~\cite{ollama}.

\textbf{\mmlu} is a benchmark that evaluates \ac{LLM} systems across tasks ranging from elementary mathematics to legal reasoning~\cite{hendrycks2020measuring}.
We use ten subject-specific subsets of \mmlu for our experiments, with a total of \num{2313} questions.
As a knowledge source, we use a subset of the Wikimedia/Wikipedia dataset, sourced from \textsc{Wikipedia} and embedded using the Cohere Embed V3 model~\cite{cohere_wikipedia_2023}.
From this dataset, we extract one million snippets and cluster them into ten groups using the $k$-means algorithm to simulate different data sources. 
After clustering, we observe significant variance in the cluster size, ranging from \num{49397} to \num{148341} snippets per cluster.
We use the same \ac{LLM} as for the \mirage benchmark.
To run \mmlu, we use the \textsc{RQABench} framework~\cite{retrieval_qa_benchmark}. 

\subsection{Hardware}
We run our experiments on our university cluster\footnote{See \url{https://www.epfl.ch/research/facilities/rcp/}.}.
Each node has a NVIDIA A100 GPU and contains 500 GB of main memory.

\begin{figure*}[t]
    \centering
    \inputplot{plots/mean-recall}{1}
    \caption{The mean recall for both benchmarks and for different data sources. We also show the mean recall for \sys.}
    \label{fig:recall}
\end{figure*}


\subsection{\sys routing effectiveness}
\label{sec:exp_routing_effectiveness}
We first explore the effectiveness of \sys routing, regarding retrieval recall and classification performance.

\textbf{Retrieval recall.}
\Cref{fig:recall} shows the mean recall for \mirage (top 10 and top 32) and \mmlu.
For \mirage, we show recall for different corpora and question sets; for \mmlu, we show recall for each data source.
Recall is computed by comparing the retrieved snippets for a question against the set of relevant snippets across all data sources, to establish a ground truth.
The recall value indicates the proportion of relevant chunks retrieved by each corpus when queried independently. We also show the recall of \sys.

\Cref{fig:recall} shows that for \mirage, \textsc{PubMed} achieves high recall across all benchmarks, while other corpora show variable recall performance.
No single corpus can reliably cover all queries.
For \mmlu, the eighth cluster achieves a mean recall of 49.4\%, whereas all other clusters have a recall below 15.3\%.
\sys consistently achieves high recall across all benchmarks: for \mirage, this is between 95.3\% and 99.0\% for top-10 retrieval and between 96.7\% and 98.5\% for top-32 retrieval.
For \mmlu, we observe 90\% recall overall.
Thus, our router is effective at retrieving relevant data chunks.

\begin{table}[t]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{lccccc}
\toprule
\textbf{Experiment} & \textbf{Accuracy (\%)} & \textbf{Recall (\%)} & \textbf{F1-Score (\%)} & \textbf{AUC (\%)} \\
\midrule
\mirage (Top 32) & 85.63 ± 3.92 & 85.47 ± 3.61 & 85.79 ± 2.45 & 92.6 ± 2.33 \\
\mirage (Top 10) & 87.3 ± 6.1 & 88.32 ± 3.96 & 85.43 ± 4.18 & 93.67 ± 3.33 \\
\mmlu (Top 10) & 90.06 ± 5.04 & 76.23 ± 6.64 & 78.29 ± 7.59 & 92.88 ± 3.29 \\
\bottomrule
\end{tabular}
}
\caption{Classification metrics (averages and standard deviations) for our router and for different experiments.}
\label{tab:summary_classification_results}
\end{table}

\textbf{Classification performance.}
To further evaluate the effectiveness of our router, we show its classification performance in predicting corpora relevance for a given query.
\Cref{tab:summary_classification_results} presents various classification metrics, \ie, accuracy, precision, recall, F1-score, and AUC, for our three experiments.
In this context, recall refers to the performance of the routing model in identifying relevant corpora, rather than the recall of retrieved snippets within the retrieved context.
Similarly, accuracy refers to the classification accuracy of the routing model, not the end-to-end \ac{LLM} accuracy in generating final responses.

We achieve relatively high accuracy for all experiments, ranging from 85.6\% for \mirage (Top 32) to 90.1\% for \mmlu.
The accuracy and recall for top-10 retrieval compared to top-32 retrieval are slightly lower, possibly because this is a more difficult classification problem.
Finally, we observe that recall and F1-score for \mmlu are lower than those for \mirage, likely because relevant data sources in \mmlu cover a broader range of topics, making them less distinct compared to the more structured and domain-specific nature of \mirage's medical dataset.
Nevertheless, \Cref{tab:summary_classification_results} shows that our router is effective at determining relevant data sources.

\begin{figure}[t]
	\centering
	\inputplot{plots/communication}{2}
	\caption{The number of queries for both benchmarks and for different routing strategies.}
	\label{fig:communication}
\end{figure}

\subsection{\sys efficiency gains}
\label{sec:exp_efficiency}
Next, we quantify the reduction by \sys in the number of queries and communication volume.

\textbf{Number of queries.}
\Cref{fig:communication} shows the number of queries for both benchmarks when querying all data sources (naive), when querying relevant data sources (assuming ground truth knowledge), and when using \sys (predicted).
We show these results for both benchmarks and for top-10 and top-32 retrieval for \mirage.
\Cref{fig:communication} demonstrates that the number of queries sent by \sys is significantly lower compared to querying all data sources.
In the top-32 retrieval setting and for \mirage, the reduction in queries sent to data sources ranges from 28.93\% for \textsc{MedQA} to 69.5\% for \textsc{PubMedQA}.
Routing achieves even greater efficiency gains in top-10 retrieval settings, with reductions for \mirage ranging from 39.9\% for \textsc{MedQA} to 71.3\% for \textsc{PubMedQA}.
For \mmlu, the reduction of \sys compared to naive routing is 77.5\%, reducing the number of queries from \num{13890} to \num{3126}.
We further observe that the number of queries by \sys approximates the optimal routing scenario, highlighting the effectiveness of our solution.



\textbf{Communication volume.}
\sys also decreases communication volume.
For top-32 retrieval and \mirage, this reduction ranges from 22.3\% for \textsc{MedQA} (200.0 MB → 155.4 MB) to 53.70\% for \textsc{PubMedQA} (622.18 MB → 341.04 MB).
In this setting, aggregating across all question banks, routing reduces total data transfer by 41.13\%, decreasing communication volume from 1156.34 MB under the naive setting to 680.72 MB. The gains in efficiency for the \mmlu benchmark are more pronounced due to the increased number of data sources, leading to a 76.2\% reduction in communication volume from 73.3 MB to merely 17.42 MB. 

\textbf{Inference time.}
The routing inference time is minimal in terms of latency.
Inference with a batch size of 32 completes within 0.3 milliseconds with an NVIDIA A100 GPU and 0.8 milliseconds with a AMD EPYC 7543 32-Core CPU.
As such, the overhead of routing has a negligible impact on the end-to-end latency of queries.
Because our router is lightweight, it also suitable for usage on low-resource devices.



\begin{table}[t]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lcc}
\toprule
\textbf{Corpus} & \textbf{Top 32 Accuracy (\%)} & \textbf{Top 10 Accuracy (\%)} \\
\midrule
No \ac{RAG} & 67.04 ± 7.66 & 67.04 ± 7.66 \\
\ac{RAG} (all corpora) & 72.22 ± 9.86 & 72.21 ± 10.33 \\
\textbf{\sys (our work)} & \textbf{72.24 ± 9.36} & \textbf{72.00 ± 10.57} \\
\bottomrule
\end{tabular}%
}
\caption{Mean and standard deviation of end-to-end \ac{RAG} accuracy for configurations with the \mirage benchmark.}
\label{tab:medcpt_avg}
\vspace{-0.6cm}
\end{table}

\subsection{End-to-end \ac{RAG} accuracy}
\label{sec:exp_accuracy}
Finally, we compute the average end-to-end accuracy of \mirage across top-10 and top-32 retrieval settings for different corpora and for \mmlu.
The results for \mirage are shown in \Cref{tab:medcpt_avg}.
Without \ac{RAG}, we achieve a 67.0\% accuracy.
When using traditional \ac{RAG} with a single database containing all corpora, this accuracy increases to 72.22\% and 72.21\% for top-10 and top-32 retrieval, respectively, surpassing all individual corpora.
We observe that using individual corpora, such as \textsc{StatPearls}, can lead to a decrease in accuracy compared to not using \ac{RAG} (not shown in~\Cref{tab:medcpt_avg}). 
This occurs because the information from a single corpus is not always beneficial for \ac{RAG}, as also shown in \Cref{fig:routing}.
When using \sys, we achieve 72.24\% and 72.0\% accuracy.
For the \mmlu benchmark, we get 43.59\% accuracy using a single database with all the data and 43.29\% accuracy using \sys.
Thus, \sys only has a marginal impact on achieved \ac{RAG} accuracy.
These results further reinforce that querying all corpora is not necessary for achieving high accuracy.
















