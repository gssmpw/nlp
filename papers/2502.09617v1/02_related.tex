\section{Related Work}
Rendering of human avatars can be broadly categorized into two main areas: `per-scene optimized human rendering' and `generalizable human rendering'. We review both areas next before discussing prior work on dual shape representations, which combine Gaussians and meshes, and iterative networks. 

\textbf{Per-scene optimized human rendering.} Human rendering from multiview or monocular videos has achieved great results in recent years, benefitting from  progress in neural rendering, e.g., neural radiance fields (NeRF)~\citep{Mildenhall2020NeRFRS} and Gaussian splatting~\citep{kerbl3Dgaussians}.

NeuralBody~\citep{peng2021neural} is one of the earlier works that explores NeRFs for human rendering. It regresses the colors and opacities based on the latent codes associated with the vertices of a deformable mesh. HumanNeRF~\citep{Weng2022HumanNeRFFR}  learns subject-specific representations from a monocular video and improves over prior works by introducing non-rigid transformations.
Followup NeRF-based works further improve the rendering quality~\citep{yu2023monohuman}, training speed~\citep{geng2023learning, jiang2023instantavatar}, and rendering speed~\citep{jiang2023instantavatar}. %
Later, Gaussian splatting was adopted by human rendering techniques due to its superior rendering speed~\citep{lei2024gart, wen2024gomavatar, hu2024gaussianavatar, kocabas2024hugs, li2023human101, paudel2024ihuman}. 
Human101~\citep{li2023human101} advances the training speed to $\sim$100s on ZJU-MoCap and MonoCap. iHuman~\citep{paudel2024ihuman} further improves the training speed to 12s on PeopleSnapshot and can be trained on as few as 6 frames.
Even though the training speed improves significantly when using Gaussian splatting, real-world applications often prefer sub-second training times. Moreover, without learned priors from large-scale datasets,  per-scene optimization approaches suffer from overfitting when the training views are sparse.

Differently, in this work, we adopt the dual shape representation introduced by GoMAvatar~\citep{wen2024gomavatar} and adapt it to generalizable human rendering. This permits to  reconstruct the 3D representation in less than one second and further excels even if only sparse inputs are available. %

\textbf{Generalizable human rendering.} Generalizable human rendering operates on sparse source views and benefits from learned priors and inductive biases extracted during a training phase from large-scale datasets. In addition, it has a greater potential to attain a faster speed when recovering a 3D representation from the source views. ActorsNeRF~\citep{mu2023actorsnerf} combines per-scene optimization with priors learned from large-scale datasets using a two-stage training. Diffusion-based approaches and large-reconstruction model-based methods ~\citep{weng2024single,  chen2024generalizable, xue2024human, kolotouros2024avatarpopup, pan2024humansplat} denoise the multiview images or other properties. %
Since it requires multiple steps for each denoising process, diffusion-based approaches usually take 2-10s to reconstruct the human avatar from images. Another line of works~\citep{remelli2022drivable, hu2023sherf, kwon2021neural, kwon2023neural, li2024ghunerf, pan2023transhuman, zheng2024gpsgaussian} build a single feed-forward approach to recover a 3D representation. They operate on source views and output a  3D representation for novel view rendering. Without evaluating the network  several times, feed-forward methods are much faster  compared to diffusion-based methods.

Our approach falls in the feed-forward category. However, differently, we devise an end-to-end trainable iterative feedback module to improve performance.  As we show quantitatively and qualitatively in \cref{sec:exp}, our approach achieves better rendering quality compared to prior feed-forward methods, while not being significantly slower.

\textbf{Gaussians-on-Mesh dual shape representation.} 
Though Gaussian splatting alone achieves superior rendering quality and speed, it suffers from overfitting when a good position initialization is not available~\citep{wen2024gomavatar} and its underlying geometry is less accurate~\citep{paudel2024ihuman, qian2023gaussianavatars}. 
Prior work~\citep{wen2024gomavatar, paudel2024ihuman} regularizes the Gaussians and enables animation using parametric models such as FLAME~\citep{FLAME:SiggraphAsia2017} and SMPL~\citep{loper2015smpl}.
We also combine Gaussian splatting with a mesh. Different from the use of one Gaussian per face by \cite{wen2024gomavatar} and \cite{paudel2024ihuman}, we adopt a coupled-multi-resolution representation: a low-resolution mesh is deformed and Gaussians are linked to a high-resolution mesh. Different from~\cite{qian2023gaussianavatars}, who split the Gaussians based on gradient signals, we subdivide the mesh and bind the Gaussians on the subdivided mesh since  gradients are unavailable in our generalized human rendering setting which uses only a feed-forward pass. SuGaR~\citep{guedon2024sugar} works on general \textit{static} scenes and attaches multiple Gaussians to each triangle based on predefined barycentric coordinates. However, the Gaussians' scales are learned in the world coordinates, while we define Gaussian parameters in a triangle's \textit{local} coordinates. %
This modification is important for modeling dynamic scenes. 

\textbf{Iterative network.} Our approach falls into the category of iterative feedback networks~\citep{adler2017solving, manhardt2018deep, carreira2016human, li2018deepim, ma2020deep}. 
The core idea is to learn to iteratively update the output through a forward process. This method works particularly well when feedback signals can be incorporated at each step to improve the estimation. Previous works either unrolled standard optimizers into differentiable feedforward networks~\citep{wang2016proximal, belanger2016structured, schwing2015fully, zuo2025ogni}, explicitly optimizing an energy function, or trained a generic iterative network with supervised learning without an explicit energy formulation~\citep{andrychowicz2016learning, wichrowska2017learned, flynn2019deepview, teed2020raft}. In computer vision, these methods have been used for pose estimation~\citep{li2018deepim, carreira2016human}, inverse problems~\citep{ma2020deep}, dense reconstruction~\citep{flynn2019deepview}, optical flow~\citep{teed2020raft}, and depth estimation~\citep{zuo2025ogni}. 
Our work presents a novel use of this iterative framework for generalizing human avatars.

