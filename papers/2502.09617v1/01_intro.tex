\section{Introduction}
\label{sec:intro}
Generalizable rendering of an animatable human avatar from sparse inputs, i.e., images showing a human in the same clothing and environment but not necessarily the same pose, is an important problem for augmented and virtual reality applications. Envision generation of an animatable avatar from a few quickly taken pictures in an unconstrained environment and efficient yet high-quality pose-conditioned rendering in a virtual world.

To address this application, recent approaches  \citep{kwon2024ghg, zheng2024gpsgaussian, li2024ghunerf, hu2023sherf, pan2023transhuman} resort to generalizable reconstruction methods. Generalizable methods avoid scene-specific optimization at inference time but instead use \textit{just} a single deep net forward pass, making reconstruction efficient. During an offline training phase the deep net extracts data priors and inductive biases from a reasonably large dataset. Due to the learned priors, it can be applied to sparser inputs compared to scene-specific training.

For rendering, recent methods  \citep{wen2024gomavatar, paudel2024ihuman, guedon2024sugar} introduce a dual shape representation, combining the advantages of a mesh, i.e., regularization via the manifold neighborhood connectivity induced by the triangle mesh, with those of Gaussian splats, i.e., fast and flexible rendering.

However, the use of \textit{just} a single deep net forward pass during reconstruction prevents present-day methods from refining their prediction. This is a concern because apparent errors that can be detected by comparing available inputs to a corresponding rendering of the reconstruction are not utilized. 
Moreover, w.r.t.\ the dual shape representation for human rendering, GoMAvatar~\citep{wen2024gomavatar} and iHuman~\citep{paudel2024ihuman} employ identical resolutions for the underlying mesh and Gaussians, i.e., one Gaussian for each triangle face in the mesh. This is a concern because a reasonably low-dimensional mesh representation is desirable for efficient reconstruction, while a high-dimensional Gaussian splat representation is desirable for high-quality rendering. GaussianAvatar~\citep{qian2023gaussianavatars} uses an adaptive density control based on gradients to densify Gaussians on the mesh. However, generalizable human rendering reconstructs and renders subjects in a feed-forward pass and therefore gradients are unavailable to guide the densification.

To address the first concern of not leveraging apparent errors, we propose a novel iterative feedback-based reconstruction network. The iterative update mechanism augments generalized methods via a feedback mechanism to improve results by fusing information from inputs, the current 3D reconstruction, and current rendering from input views. Importantly, the designed iterative update mechanism is end-to-end trainable, i.e., the feedback is taken into account when training the generalized reconstruction. Note that the iterative update mechanism makes reconstruction slightly slower,
yet our un-optimized version still performs the task in less than one second. Since reconstruction is a one-off task, independent from pose-conditioned rendering, we think it makes sense to spend a bit more effort than a simple deep network. %

To address the second concern, we study a coupled-multi-resolution Gaussians-on-Mesh representation. More specifically, reconstruction is performed with a low-resolution mesh while we increase the number of Gaussians by attaching multiple ones to a single triangle face. This is achieved via a sub-division-like procedure. Beneficially, reconstruction remains efficient while rendering can achieve high-quality and high-resolution results.

We illustrate our method in \cref{fig:teaser} and observe compelling rendering quality and  speed. We assess the efficacy of the proposed method on the challenging THuman2.0, XHuman and AIST++ data. As mentioned, reconstruction needs less than one second and rendering runs at 95.1 FPS on one NVIDIA A100 GPU. The rendering quality of the designed method outperforms the state-of-the-art, improving PSNR/LPIPS*/FID to 24.65/110.82/51.27 from 21.90/133.41/61.67 for GHG~\citep{kwon2024ghg}.
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth,trim={0 0.7cm 0 0},clip]{figs/teaser.pdf} \vspace{-0.5cm}
    \caption{\textbf{Overview.} We tackle the problem of generalizable human rendering. Given sparse source images (multiview images or multi-frame images), we reconstruct the 3D human representation in canonical  T-pose space. The canonical representation can be animated  and rendered in novel views.} \vspace{-0.6cm}
    \label{fig:teaser}
\end{figure}
