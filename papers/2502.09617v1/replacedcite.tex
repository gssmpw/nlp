\section{Related Work}
Rendering of human avatars can be broadly categorized into two main areas: `per-scene optimized human rendering' and `generalizable human rendering'. We review both areas next before discussing prior work on dual shape representations, which combine Gaussians and meshes, and iterative networks. 

\textbf{Per-scene optimized human rendering.} Human rendering from multiview or monocular videos has achieved great results in recent years, benefitting from  progress in neural rendering, e.g., neural radiance fields (NeRF)____ and Gaussian splatting____.

NeuralBody____ is one of the earlier works that explores NeRFs for human rendering. It regresses the colors and opacities based on the latent codes associated with the vertices of a deformable mesh. HumanNeRF____  learns subject-specific representations from a monocular video and improves over prior works by introducing non-rigid transformations.
Followup NeRF-based works further improve the rendering quality____, training speed____, and rendering speed____. %
Later, Gaussian splatting was adopted by human rendering techniques due to its superior rendering speed____. 
Human101____ advances the training speed to $\sim$100s on ZJU-MoCap and MonoCap. iHuman____ further improves the training speed to 12s on PeopleSnapshot and can be trained on as few as 6 frames.
Even though the training speed improves significantly when using Gaussian splatting, real-world applications often prefer sub-second training times. Moreover, without learned priors from large-scale datasets,  per-scene optimization approaches suffer from overfitting when the training views are sparse.

Differently, in this work, we adopt the dual shape representation introduced by GoMAvatar____ and adapt it to generalizable human rendering. This permits to  reconstruct the 3D representation in less than one second and further excels even if only sparse inputs are available. %

\textbf{Generalizable human rendering.} Generalizable human rendering operates on sparse source views and benefits from learned priors and inductive biases extracted during a training phase from large-scale datasets. In addition, it has a greater potential to attain a faster speed when recovering a 3D representation from the source views. ActorsNeRF____ combines per-scene optimization with priors learned from large-scale datasets using a two-stage training. Diffusion-based approaches and large-reconstruction model-based methods ____ denoise the multiview images or other properties. %
Since it requires multiple steps for each denoising process, diffusion-based approaches usually take 2-10s to reconstruct the human avatar from images. Another line of works____ build a single feed-forward approach to recover a 3D representation. They operate on source views and output a  3D representation for novel view rendering. Without evaluating the network  several times, feed-forward methods are much faster  compared to diffusion-based methods.

Our approach falls in the feed-forward category. However, differently, we devise an end-to-end trainable iterative feedback module to improve performance.  As we show quantitatively and qualitatively in \cref{sec:exp}, our approach achieves better rendering quality compared to prior feed-forward methods, while not being significantly slower.

\textbf{Gaussians-on-Mesh dual shape representation.} 
Though Gaussian splatting alone achieves superior rendering quality and speed, it suffers from overfitting when a good position initialization is not available____ and its underlying geometry is less accurate____. 
Prior work____ regularizes the Gaussians and enables animation using parametric models such as FLAME____ and SMPL____.
We also combine Gaussian splatting with a mesh. Different from the use of one Gaussian per face by ____ and ____, we adopt a coupled-multi-resolution representation: a low-resolution mesh is deformed and Gaussians are linked to a high-resolution mesh. Different from____, who split the Gaussians based on gradient signals, we subdivide the mesh and bind the Gaussians on the subdivided mesh since  gradients are unavailable in our generalized human rendering setting which uses only a feed-forward pass. SuGaR____ works on general \textit{static} scenes and attaches multiple Gaussians to each triangle based on predefined barycentric coordinates. However, the Gaussians' scales are learned in the world coordinates, while we define Gaussian parameters in a triangle's \textit{local} coordinates. %
This modification is important for modeling dynamic scenes. 

\textbf{Iterative network.} Our approach falls into the category of iterative feedback networks____. 
The core idea is to learn to iteratively update the output through a forward process. This method works particularly well when feedback signals can be incorporated at each step to improve the estimation. Previous works either unrolled standard optimizers into differentiable feedforward networks____, explicitly optimizing an energy function, or trained a generic iterative network with supervised learning without an explicit energy formulation____. In computer vision, these methods have been used for pose estimation____, inverse problems____, dense reconstruction____, optical flow____, and depth estimation____. 
Our work presents a novel use of this iterative framework for generalizing human avatars.