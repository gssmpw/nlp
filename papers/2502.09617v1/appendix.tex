\section*{Appendix --- LIFe-GoM: Generalizable Human Rendering with Learned Iterative Feedback Over Multi-Resolution Gaussians-on-Mesh}

This appendix is structured as follows:
\begin{itemize}
    \item Sec.~\ref{sec: appendix_related} summarizes mesh representations in human modeling;
    \item Sec.~\ref{sec: appendix_arch} provides the detailed architecture of the iterative feedback module;
    \item Sec.~\ref{sec: appendix_dataset} details the datasets;
    \item Sec.~\ref{sec: appendix_baseline} shows baseline details on the presented datasets;
    \item Sec.~\ref{sec: appendix_result} provides additional results and analysis. Please visit the project webpage\footnote{\url{https://wenj.github.io/LIFe-GoM/}} for more qualitative results;
    \item Sec.~\ref{sec: appendix_limitation} showcases failure cases in our approach.
\end{itemize}

\section{Addtional Related Works}
\label{sec: appendix_related}
\textbf{Mesh representations in human modeling.} Meshes as an explicit representation are easy to animate and can be rendered at a fast speed. Further, meshes can be easily integrated into the classic graphics pipeline. Therefore, meshes are widely used in human modeling~\citep{liao2024tada,zhang2023getavatar,liao2023high}. However, as mentioned in GoMAvatar~\citep{wen2024gomavatar}, it is difficult to learn to deform the mesh using photometric losses and mesh rasterization. Hence, methods using meshes as the underlying representation either extract them from other types of representations such as a signed distance function (SDF)~\citep{zhang2023getavatar,liao2023high}, or apply explicit supervision on the geometry, e.g., supervising surface normals~\citep{liao2024tada,zhang2023getavatar,liao2023high}. In contrast, we opt to use the Gaussians-on-Mesh representation that binds Gaussians on the mesh and uses Gaussian splatting for rendering. This enables us to overcome the difficulty in optimization. Consequently, our entire model is learned via photometric losses only. Further, Gaussians-on-Mesh leverages the flexibility of Gaussian Splatting, enabling more photorealistic rendering than textured meshes.


\section{Details for the Iterative Feedback Module}
\label{sec: appendix_arch}

The detailed architecture of the iterative feedback module is provided in Fig.~\ref{fig: feature_matching_feedback_module}. Given  source images and rendered images, we first extract  image features via an image encoder. Then we apply multi-source fusion which samples  aligned image features for each of the low-resolution vertices $\{v_{t-1, i}^{c\downarrow}\}_{i=1}^{V^\downarrow}$ and mixes the features from $N$ sources. After that, a Point Transformer is adopted to encode all vertices. Note that the iterative feedback module operates on the low-resolution mesh.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/feature_matching_feedback.pdf}
    \caption{\textbf{Iterative feedback  module.} The iterative feedback module takes as input the representation $\text{GoM}_{t-1}^c$ obtained from the previous iteration, the source images and images rendered with $\text{GoM}_{t-1}^c$. The module is designed to compare the rendered images and source images, and to summarize the result in a feature vector of dimension $C^\prime$ for each vertex in the low-resolution mesh. Here, $B$ denotes the batch size, $N$ refers to the number of source images, and $H$ and $W$ are the height and weight of the images respectively. Further, $V^\downarrow$ is the number of vertices in the \textit{low}-resolution mesh, $C$ refers to the dimension of the feature vector from the image encoder, and $C^\prime$ denotes the dimension of the output feature from the Point Transformer. The entire module operates on the low-resolution mesh.}
    \label{fig: feature_matching_feedback_module}
\end{figure}

\textbf{Image encoder.} We use ResNet-18~\citep{he2016deep} with ImageNet pretrained weights as the image encoder. The image feature is the concatenation of features from 5 intermediate layers and therefore has a dimension of 1192, i.e., $C=1192$ in Fig.~\ref{fig: feature_matching_feedback_module}. Concatenating multi-level features ensures a large receptive field and is essential for iterative updates.

\textbf{Multi-source fusion.}
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/multisource_fusion.pdf}
    \caption{\textbf{Multi-source fusion.} Multi-source fusion first samples the vertex-aligned image features from the encoded images. Then we use two Transformer encoder layers to fuse the information from each of the $N$ source images. In the Transformer encoder layers, the three input arrows from top to bottom represent the query matrix $Q$, the key matrix $K$, and the value matrix $V$ of the attention layer respectively. We additionally associate a learnable vertex embedding with each vertex. Please check Appendix~\ref{sec: appendix_arch}  for details.}
    \label{fig: multisource_fusion}
\end{figure}
Multi-source fusion first samples  image features for all vertices in the low-resolution mesh. Concretely, the $i$-th vertex $v_i^{c\downarrow}, i\in\{1, \dots, V^\downarrow\}$ is first articulated via the available source human poses $\{P_n\}_{n=1}^N$ and then projected  onto  images via the available camera intrinsics $\{K_n\}_{n=1}^N$ and extrinsics $\{E_n\}_{n=1}^N$. The aligned features are sampled at the projected points from each of the $N$ source images. Subsequently we mix the sampled features from the $N$ source images using two Transformer encoder layers. The query matrix $Q$, key matrix $K$ and value matrix $V$ for each Transformer encoder are illustrated in Fig.~\ref{fig: multisource_fusion}. 
The input, intermediate and output dimensions are $C=1192$. We use 6 heads in the attention layers.
Note that in the second Transformer encoder layer, we use a learnable vertex embedding $\{e_{t-1, i}^\downarrow\}_{i=1}^{V\downarrow}$ as the query. The learnable vertex embedding is updated in iterative updates together with the low-resolution vertices. %

\textbf{Point Transformer.} The Point Transformer~\citep{zhao2021point} is used to encode the vertices and to produce high-level features for all low-resolution vertices. The output dimension of each vertex is 32, i.e., $C^\prime=32$ in Fig.~\ref{fig: feature_matching_feedback_module}.

\section{Dataset Details}
\label{sec: appendix_dataset}
\textbf{THuman2.0~\citep{tao2021function4d}}. We use THuman2.0 to evaluate our approach in the setting of multiview source images. THuman2.0 has 526 high-quality 3D human scans, texture maps and corresponding SMPL-X parameters. We follow the experimental setup of GHG~\citep{kwon2024ghg} and split the dataset into 426 subjects for training and 100 subjects for evaluation. We render multiview images from the 3D scans. 3 or 5 images are used as source images and the remaining ones are used for evaluation. 

\textbf{XHuman~\citep{shen2023xavatar}}. We use XHuman to validate our approach to cross-domain generalization quantitatively. The dataset provides 20 subjects with high-quality scans and SMPL-X parameters. We sample three scans (f00001, f00051, f00101) for each subject. We prepare the dataset in the same way as THuman2.0. To validate the ability of cross-domain generalization, we only evaluate in this dataset without any finetuning.

\textbf{AIST++~\citep{li2021learn}}.  The AIST++ dataset is used to evaluate the setting of multi-frame source images. The AIST++ dataset consists of multiview dancing videos, camera calibration parameters, and human motions represented in SMPL poses. We adopt the training and evaluation protocol of ActorsNeRF~\citep{mu2023actorsnerf}. Specifically, we use subjects 1-15 and 21-30 for training and leave out subjects 16-20 for evaluation. We choose one motion sequence for each subject. We only use camera 1 for training. During evaluation, we sample 5 source images from Camera 1 and use Camera 2-7 to evaluate generalizable novel view and pose synthesis.

\section{Baseline Details}
\label{sec: appendix_baseline}
We compare with per-scene optimized approaches including GoMAvatar~\citep{wen2024gomavatar}, 3DGS-Avatar~\citep{qian20243dgs} and iHuman~\citep{paudel2024ihuman}, and other generalizable human rendering approaches including NHP~\citep{kwon2021neural}, NIA~\citep{kwon2023neural}, GHG~\citep{kwon2024ghg} and GPS-Gaussian~\citep{zheng2024gpsgaussian} on THuman2.0. We use 3 source images when comparing with GoMAvatar, 3DGS-Avatar, iHuman, NHP, NIA and GHG. 
For the comparison with GPS-Gaussian, we adopt 5 source images following the setting of GHG~\citep{kwon2024ghg}, since GPS-Gaussian requires the source views to overlap with each other and thus does not work well with very sparse views. We compared with the pretrained GHG~\citep{kwon2024ghg} on XHuman. On AIST++, we compare with HumanNeRF~\citep{Weng2022HumanNeRFFR}, GoMAvatar~\citep{wen2024gomavatar}, 3DGS-Avatar~\citep{qian20243dgs}, iHuman~\citep{paudel2024ihuman} and ActorsNeRF~\citep{mu2023actorsnerf}. HumanNeRF, GoMAvatar, 3DGS-Avatar and iHuman need to be trained per scene. ActorsNeRF adopts a two-stage training: In the first stage, it learns a categorical prior from large-scale datasets. In the second stage, it adopts per-scene optimization given the source images. Now we detail the training setup of each baseline.

\textbf{NHP~\citep{kwon2021neural}, NIA~\citep{kwon2023neural} and GHG~\citep{kwon2024ghg}.} We follow the same setting as reported in GHG~\citep{kwon2024ghg} for training and evaluation.

\textbf{GPS-Gaussian~\citep{zheng2024gpsgaussian}.} As described in GHG~\citep{kwon2024ghg}, GPS-Gaussian can work on as few as 5 input views. We render the THuman2.0 dataset to accommodate this setting. We use the default parameters provided in GPS-Gaussian to train the model.

\textbf{GoMAvatar~\citep{wen2024gomavatar}.} GoMAvatar originally takes SMPL parameters as inputs. On THuman2.0, we adjust it to work with SMPL-X parameters. On AIST++, we keep the original setting and use SMPL. We train the model for 100K iterations on both datasets instead of 200K iterations as stated in the paper, to avoid overfitting sparse inputs. 

\textbf{3DGS-Avatar~\citep{qian20243dgs}.} Similar to GoMAvatar, we adapt 3DGS-Avatar to take SMPL-X parameters as inputs on THuman2.0 and keep its original setting on AIST++. We train it for 2K iterations to avoid overfitting the very sparse inputs.

\textbf{iHuman~\citep{paudel2024ihuman}.} We adapt iHuman to work with SMPL-X parameters on the THuman2.0 dataset. It requires subdivided SMPL or SMPL-X templates as inputs. For a fair comparison, we adopt the same subdivision strategy for the SMPL-X template as ours, in which we subdivide all faces twice. We find that the model cannot converge after 15 epochs, the number of epochs specified in the original iHuman paper. Instead, we use 150 epochs which takes a longer time for training but provides better rendering quality. On AIST++, we use the default hyperparameters for training.

\textbf{HumanNeRF~\citep{Weng2022HumanNeRFFR} and ActorsNeRF~\citep{mu2023actorsnerf}.} We follow the same setting as reported in ActorsNeRF~\citep{mu2023actorsnerf} for training and evaluation.

\section{Additional Analysis}
\label{sec: appendix_result}

For additional qualitative results we refer the reader to the project webpage. It contains videos for freeview rendering, cross-domain generalization and novel pose synthesis.

\subsection{Additional comparisons on THuman2.0}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth,trim=4cm 0 0 0,clip]{results/thuman_3view.pdf}
    \vspace{-8mm}
    \caption{\textbf{Comparison to baselines in the setting of 3 source images on THuman2.0.} 
    Our method produces less noise than iHuman and GoMAvatar, and more accurate geometry and sharper details than GHG.}
    \label{fig: appendix_thuman2.0_3view}
\end{figure*}

We compare our approach against GoMAvatar~\citep{wen2024gomavatar}, iHuman~\citep{paudel2024ihuman}, and GHG~\citep{kwon2024ghg} in the setting of 3 source images on THuman2.0. Note that GoMAvatar and iHuman are scene-specific methods while GHG is a generalizable approach, the same as ours. In Fig.~\ref{fig: appendix_thuman2.0_3view}, we showcase additional qualitative comparisons to the baselines.

In the setting of very sparse inputs, e.g., 3 views, scene-specific methods suffer from overfitting and struggle to render uncorrupted novel views. Generalizable approaches, in contrast, constrain the output space with the data priors learned from large-scale datasets, which leads to more plausible rendering quality. Compared to other generalizable approaches, ours outputs more accurate geometry and sharper details.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth,trim=4cm 0 0 0,clip]{results/thuman_5view.pdf}
    \vspace{-8mm}
    \caption{\textbf{Comparison to GPS-Gaussian in the setting of 5 source images on THuman2.0.} 
    Our method produces more complete shape and sharper details.}
    \label{fig: appendix_thuman2.0_5views}
\end{figure*}

We show the qualitative comparison between ours and GPS-Gaussian in the setting of 5 views on THuman in Fig.~\ref{fig: appendix_thuman2.0_5views}. GPS-Gaussian~\citep{zheng2024gpsgaussian} relies on stereo depth estimation to locate the Gaussians. During inference time, it takes as inputs two adjacent views and interpolates the novel views in between. Therefore, it requires the adjacent views to overlap with each other. As mentioned in GHG~\citep{kwon2024ghg}, 5 views are the minimal number of input views that GPS-Gaussian can work on. Even with 5 views as inputs, we still find that it fails in the non-overlapped regions, leaving incomplete silhouettes in rendering. In contrast, ours outperforms GPS-Gaussian qualitatively and quantitatively in the setting of 5 views and can work on as few as 3 views. Another key difference between GPS-Gaussian and our approach is that we reconstruct the human subject in the canonical T-pose while the representation of GPS-Gaussian is in the same pose as the source images. Therefore, ours can take images in unaligned poses as inputs and render novel poses without extra effort, as demonstrated in Fig.~\ref{fig: quant_novel_pose}.

\subsection{Additional comparisons on AIST++}
\begin{table}[t]
\caption{\textbf{Per-scene breakdown on AIST++.} We use lighter gray for scene-specific methods, while the others are generalizable methods.}
\label{tab: appendix_aistpp_breakdown}
\centering
\footnotesize
\begin{adjustbox}{width=1.0\linewidth}
\begin{tabular}{l|rrr|rrr|rrr}
\toprule
                                 & \multicolumn{1}{l}{\textbf{PSNR $\uparrow$}} & \multicolumn{1}{l}{\textbf{SSIM $\uparrow$}} & \multicolumn{1}{l|}{\textbf{LPIPS* $\downarrow$}} & \multicolumn{1}{l}{\textbf{PSNR $\uparrow$}}  & \multicolumn{1}{l}{\textbf{SSIM $\uparrow$}} & \multicolumn{1}{l|}{\textbf{LPIPS* $\downarrow$}} & \multicolumn{1}{l}{\textbf{PSNR $\uparrow$}} & \multicolumn{1}{l}{\textbf{SSIM $\uparrow$}} & \multicolumn{1}{l}{\textbf{LPIPS* $\downarrow$}}                        \\ \midrule
\rowcolor[HTML]{EFEFEF} 
                                 & \multicolumn{3}{c|}{\cellcolor[HTML]{EFEFEF}d16}                                             & \multicolumn{3}{c|}{\cellcolor[HTML]{EFEFEF}d17}                                             & \multicolumn{3}{c}{\cellcolor[HTML]{EFEFEF}d18}                                             \\ 
{\color[HTML]{9B9B9B} HumanNeRF} & {\color[HTML]{9B9B9B} 24.37} & {\color[HTML]{9B9B9B} 0.9752} & {\color[HTML]{9B9B9B} 29.59} & {\color[HTML]{9B9B9B} 24.86} & {\color[HTML]{9B9B9B} 0.9762} & {\color[HTML]{9B9B9B} 29.39} & {\color[HTML]{9B9B9B} 22.77} & {\color[HTML]{9B9B9B} 0.9738} & {\color[HTML]{9B9B9B} 33.02} \\
{\color[HTML]{9B9B9B} GoMAvatar} & {\color[HTML]{9B9B9B} 24.35} & {\color[HTML]{9B9B9B} 0.9769} & {\color[HTML]{9B9B9B} 24.80} & {\color[HTML]{9B9B9B} 25.12} & {\color[HTML]{9B9B9B} 0.9780} & {\color[HTML]{9B9B9B} 25.17} & {\color[HTML]{9B9B9B} 23.18} & {\color[HTML]{9B9B9B} 0.9771} & {\color[HTML]{9B9B9B} 27.57} \\
{\color[HTML]{9B9B9B} 3DGS-Avatar}    & {\color[HTML]{9B9B9B} 25.22} & {\color[HTML]{9B9B9B} 0.9776} & {\color[HTML]{9B9B9B} 26.01} & {\color[HTML]{9B9B9B} 25.71} & {\color[HTML]{9B9B9B} 0.9787} & {\color[HTML]{9B9B9B} 27.70} & {\color[HTML]{9B9B9B} 23.75} & {\color[HTML]{9B9B9B} 0.9757} & {\color[HTML]{9B9B9B} 29.98} \\
{\color[HTML]{9B9B9B} iHuman}    & {\color[HTML]{9B9B9B} 25.41} & {\color[HTML]{9B9B9B} 0.9804} & {\color[HTML]{9B9B9B} 21.79} & {\color[HTML]{9B9B9B} 25.59} & {\color[HTML]{9B9B9B} 0.9805} & {\color[HTML]{9B9B9B} 23.69} & {\color[HTML]{9B9B9B} 24.25} & {\color[HTML]{9B9B9B} 0.9786} & {\color[HTML]{9B9B9B} 24.37} \\
ActorsNeRF                        & 25.22                        & 0.9796                        & 22.03                        & 25.88                        & 0.9808                        & 22.85                        & 24.50                        & 0.9811                        & 22.38                        \\
Ours                             & 25.43                        & 0.9801                        & 21.48                        & 25.73                        & 0.9812                        & 21.94                        & 24.46                        & 0.9810                        & 22.21                        \\ \midrule
\rowcolor[HTML]{EFEFEF} 
                                 & \multicolumn{3}{c|}{\cellcolor[HTML]{EFEFEF}d19}                                             & \multicolumn{3}{c|}{\cellcolor[HTML]{EFEFEF}d20}                                             & \multicolumn{3}{c}{\cellcolor[HTML]{EFEFEF}Average}                                         \\
{\color[HTML]{9B9B9B} HumanNeRF} & {\color[HTML]{9B9B9B} 24.51} & {\color[HTML]{9B9B9B} 0.9759} & {\color[HTML]{9B9B9B} 28.68} & {\color[HTML]{9B9B9B} 24.55} & {\color[HTML]{9B9B9B} 0.9791} & {\color[HTML]{9B9B9B} 27.63} & {\color[HTML]{9B9B9B} 24.21} & {\color[HTML]{9B9B9B} 0.9760} & {\color[HTML]{9B9B9B} 29.66} \\
{\color[HTML]{9B9B9B} GoMAvatar} & {\color[HTML]{9B9B9B} 24.36} & {\color[HTML]{9B9B9B} 0.9773} & {\color[HTML]{9B9B9B} 25.24} & {\color[HTML]{9B9B9B} 24.68} & {\color[HTML]{9B9B9B} 0.9806} & {\color[HTML]{9B9B9B} 23.95} & {\color[HTML]{9B9B9B} 24.34} & {\color[HTML]{9B9B9B} 0.9780} & {\color[HTML]{9B9B9B} 25.34} \\
{\color[HTML]{9B9B9B} 3DGS-Avatar}    & {\color[HTML]{9B9B9B} 25.32} & {\color[HTML]{9B9B9B} 0.9783} & {\color[HTML]{9B9B9B} 27.70} & {\color[HTML]{9B9B9B} 25.70} & {\color[HTML]{9B9B9B} 0.9819} & {\color[HTML]{9B9B9B} 24.44} & {\color[HTML]{9B9B9B} 25.14} & {\color[HTML]{9B9B9B} 0.9784} & {\color[HTML]{9B9B9B} 27.17} \\
{\color[HTML]{9B9B9B} iHuman}    & {\color[HTML]{9B9B9B} 25.11} & {\color[HTML]{9B9B9B} 0.9800} & {\color[HTML]{9B9B9B} 23.29} & {\color[HTML]{9B9B9B} 25.48} & {\color[HTML]{9B9B9B} 0.9829} & {\color[HTML]{9B9B9B} 21.37} & {\color[HTML]{9B9B9B} 25.17} & {\color[HTML]{9B9B9B} 0.9805} & {\color[HTML]{9B9B9B} 22.90} \\
ActorsNeRF                        & 25.24                        & 0.9801                        & 22.87                        & 25.30                        & 0.9827                        & 21.34                        & 25.23                        & 0.9809                        & 22.29                        \\
Ours                             & 25.19                        & 0.9805                        & 21.42                        & 25.43                        & 0.9829                        & 20.98                        & 25.25                        & 0.9812                        & 21.61           \\ \bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

Following ActorsNeRF, we also list the per-scene breakdown on 5 evaluation scenes on AIST++ in \cref{tab: appendix_aistpp_breakdown}.

Next, we present the qualitative comparison to GoMAvatar, iHuman and ActorsNeRF in Fig.~\ref{fig: appendix_aistpp}. AIST++ is challenging for two reasons: 1) The subjects perform challenging and diverse poses in the videos. 2) The poses provided by the dataset are less accurate and the masks are predicted from off-the-shelf tools which are also less accurate. Due to the explicit nature of the Gaussians-on-Mesh representation, our method produces fewer floaters than NeRF-based ActorNeRF. Meanwhile, we capture better silhouettes and produce less noise compared to iHuman and GoMAvatar, the two scene-specific methods.

\begin{figure*}
    \includegraphics[width=\linewidth,trim=1cm 0 0 0,clip]{results/aistpp.pdf}
    \caption{\textbf{Comparisons on baselines on AIST++.} Our method has fewer floaters compared to ActorsNeRF and produces more complete shape than GoMAvatar and iHuman. Meanwhile, ours is 11$\times$ faster than iHuman in reconstruction.}
    \label{fig: appendix_aistpp}
\end{figure*}

\subsection{Input pose sensitivity}

We quantitatively compare the sensitivity to input pose accuracy for our approach and GHG~\citep{kwon2024ghg}. In this experiment, we add Gaussian noise of increasing standard deviation ($0.1, 0.3, 0.5$) to the  poses provided by THuman2.0. The results are summarized in \cref{tab: pose_sensitivity}. Both methods are affected by the accuracy of the input poses. However, our approach improves upon GHG in all noise levels.

To make our approach less sensitive to the accuracy of input poses, we can explore a pose refinement network that is jointly trained with the iterative feedback. We leave it for future work.

\begin{table}[t!]
\caption{\textbf{Comparison regarding inaccurate input poses.} We add random Gaussian noise of different standard deviations to the poses provided by THuman2.0. Our method outperforms GHG for all noise levels.}
\label{tab: pose_sensitivity}
\centering
\footnotesize
\begin{tabular}{l|rrr|rrr|rrr}
\toprule
\thead{Noise}  & \multicolumn{3}{c|}{\thead{std=0.1}} & \multicolumn{3}{c|}{\thead{std=0.3}} & \multicolumn{3}{c}{\thead{std=0.5}} \\
\thead{Method}                        & \thead{PSNR$\uparrow$} & \thead{LPIPS*$\downarrow$} & \thead{FID$\downarrow$}  & \thead{PSNR$\uparrow$} & \thead{LPIPS*$\downarrow$} & \thead{FID$\downarrow$}  & \thead{PSNR$\uparrow$} & \thead{LPIPS*$\downarrow$} & \thead{FID$\downarrow$} \\
\midrule
GHG    & 21.25   & 136.87   & 62.03  & 19.66   & 149.73   & 64.15  & 18.53   & 163.48   & 68.57  \\
Ours   & 23.96   & 113.80   & 53.15  & 22.02   & 123.15   & 57.22  & 20.43   & 134.86   & 62.84 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Input shape sensitivity}

We quantitatively evaluate the sensitivity to input SMPL-X shape accuracy for our approach in \cref{tab: shape_sensitivity}. In this experiment, we initialize the canonical mesh with the average SMPL shape by setting the beta parameter to a tensor of all zeros for all subjects. Neither ground-truth nor predicted SMPL shapes are used for any subject. We call this setting ``Ours w/o SMPL-X shape'' in the table.
\begin{table}[t!]
\caption{\textbf{Importance of SMPL-X  shape input.} We compare our method  w/ and w/o SMPL-X shape input for mesh initialization. We only observe a slight drop in performance if the SMPL-X shape is not used.}
\label{tab: shape_sensitivity}
\centering
\footnotesize
\begin{tabular}{l|rrr}
\toprule
\thead{Method}                        & \thead{PSNR$\uparrow$} & \thead{LPIPS*$\downarrow$} & \thead{FID$\downarrow$}  \\
\midrule
Ours w/o SMPL-X shape    & 24.15  & 112.88 & 52.01  \\
Ours w/ SMPL-X shape    & 24.65  &  110.82 & 51.27  \\
\bottomrule
\end{tabular}
\end{table}

Our method attains PSNR/LPIPS*/FID of 24.15/112.88/52.01 w/o SMPL shapes as input. Compared to 24.65/110.82/51.27 with SMPL shapes as input, we only observe a small drop, which shows the robustness of our method to the accuracy of SMPL-X shapes. Even \textit{without} SMPL-X shapes, our method still significantly outperforms GHG's 21.90/133.41/61.67 \textit{with} SMPL-X shapes as input. We further demonstrate the robustness in \cref{fig: shape_sensitivity}. Although the average shape is smaller than the ground-truth shape, our method still captures the correct shape.

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figs/ablation_shape.pdf}
    \caption{\textbf{Robustness to SMPL-X shape accuracy.} We use the ground-truth SMPL-X shape and the average shape as initialization of the canonical mesh. Although the average shape is smaller than the ground-truth shape, our method still captures the correct shape.}
    \label{fig: shape_sensitivity}
\end{figure}

\section{Limitations}
\label{sec: appendix_limitation}
We present three types of failure cases in our method and discuss the possible next steps to resolve the issues.

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{results/failure_v3.pdf}
    \caption{\textbf{Examples of failure cases.}}
    \label{fig: limit}
\end{figure}

\textbf{Failure in hallucination large regions.} Without an explicit hallucination module, our method is unable to inpaint large invisible regions in source images, as is shown in \cref{fig: limit}(a). A possible solution is to render the invisible parts and update our canonical representation using priors from image inpainting models.

\textbf{Wrong underlying topology.} Our coupled-multi-resolution Gaussians-on-Mesh representation associates the Gaussians with the underlying mesh. Analogously to the original Gaussians-on-Mesh representation, since the underlying mesh is deformed from human parametric models such as SMPL and SMPL-X, it cannot change vertex connectivities to fit the topology of clothes such as dresses and coats. Although the wrong topology will not affect the rendering, it is a future direction to correct the underlying mesh for use in other downstream tasks.

\textbf{Failures for unseen clothing types.} We observe failures for unseen clothing such as dresses, as shown in \cref{fig: limit}(b). As a generalizable method, a more comprehensive training set containing different clothings and more diverse subjects is needed. We leave it for future work.

