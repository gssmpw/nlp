\section{Experiments}
\label{sec:exp}

\subsection{Implementation details}
\textbf{Representation details.} We initilize the low-resolution mesh $\left\{\{v_{0, i}^{c\downarrow}\}_{i=1}^{V^\downarrow}, \{f_{j}^\downarrow\}_{j=1}^{F^\downarrow}\right\}$ in $\text{GoM}_0^c$ with SMPL or SMPL-X, %
depending on the human pose representation used in the dataset. The high-resolution mesh is obtained by subdividing the low-resolution mesh. %

\textbf{Architecture details.} We provide the detailed architecture in Appendix~\ref{sec: appendix_arch}. %

\textbf{Training details.} We set $\lambda_\text{per}=1.0$, $\lambda_M=5.0$ and $\lambda_\text{lap}=100$ in \cref{eq: loss} on THuman2.0 and $\lambda_\text{per}=1.0$, $\lambda_M=0$ and $\lambda_\text{lap}=100$ in \cref{eq: loss} on AIST++. We use the SSIM loss in THuman2.0 and the LPIPS loss in AIST++ following the baselines. We use Adam as the optimizer. On THuman2.0, the learning rates of the image encoder and the rest of the model are $1\mathrm{e}{-4}$ and $5\mathrm{e}{-5}$ respectively. On AIST++, we set the learning rate of all parameters to $5\mathrm{e}{-5}$. We optimize the model for 200K iterations on THuman2.0 and 100K iterations on AIST++.

\subsection{Experimental setup}
We evaluate our approach in two settings: 1) \textbf{Multiview source images.} Our approach can take multiview images as input to produce a canonical representation;  2) \textbf{Multi-frame source images.} Since our approach directly learns a 3D representation in the canonical space instead of a posed space, our method can also operate on images showing various human poses, e.g., frames sampled from a monocular video. 
Our approach can synthesize both novel views and novel poses.

\textbf{Datasets.} We validate our approach on THuman2.0~\citep{tao2021function4d}, XHuman~\citep{shen2023xavatar} and AIST++~\citep{li2021learn} quantitatively. We use THuman2.0 to evaluate our approach in the setting of multiview source images.  XHuman  is used to validate the cross-domain generalization of our approach. In other words, we train our model on THuman2.0 and test on XHuman without fine-tuning. The AIST++ dataset is used to evaluate the  multi-frame source image setting. Please see Appendix~\ref{sec: appendix_dataset} for detailed dataset setup.



\begin{table}[t!]
\vspace{-4mm}
\caption{\textbf{Comparison on THuman2.0.} The proposed method improves state-of-the-art in PSNR, LPIPS$^*$ and FID. We highlight the best result in bold font. Methods marked in gray are per-scene optimized methods.}
\label{tab: thuman2.0}
\centering
\footnotesize
\vspace{-3mm}
\begin{tabular}{c|l|rrr}
\toprule
\thead{Number of \\ source views}                  & \thead{Method}                        & \thead{PSNR$\uparrow$} & \thead{LPIPS*$\downarrow$} & \thead{FID$\downarrow$} \\
\midrule
\multirow{6}{*}{3} & {\color[HTML]{9B9B9B} GoMAvatar~\citep{wen2024gomavatar}} & {\color[HTML]{9B9B9B} 23.05} & {\color[HTML]{9B9B9B} 133.98}& {\color[HTML]{9B9B9B} 87.51}\\
                & {\color[HTML]{9B9B9B} 3DGS-Avatar~\citep{qian20243dgs}} & {\color[HTML]{9B9B9B} 21.25}& {\color[HTML]{9B9B9B} 160.48}& {\color[HTML]{9B9B9B} 157.21}\\
                & {\color[HTML]{9B9B9B} iHuman~\citep{paudel2024ihuman}} & {\color[HTML]{9B9B9B} 22.77}& {\color[HTML]{9B9B9B} 131.67}& {\color[HTML]{9B9B9B} 101.70}\\
                   & NHP~\citep{kwon2021neural}                           &    23.32  &  184.69      &  136.56              \\
                   & NIA~\citep{kwon2023neural}                           &    23.20   &     181.82    &  127.30           \\
                   & GHG~\citep{kwon2024ghg}                           &    21.90  &    133.41    &  61.67                  \\
                   & LIFe-GoM (Ours) &  \textbf{24.65}    &  \textbf{110.82}      &  \textbf{51.27}              \\
\midrule
\multirow{2}{*}{5} & GPS-Gaussian~\citep{zheng2024gpsgaussian}                  &   20.39   &    152.34    &  65.90       \\
                   & LIFe-GoM (Ours) &  \textbf{25.57}   &    \textbf{105.39}    &  \textbf{38.57}    \\
\bottomrule
\end{tabular}\vspace{-4mm}
\end{table}

\begin{table}[t!]
\caption{\textbf{Comparison on XHuman.} We evaluate on XHuman to prove the ability of cross-domain generalization. The proposed method improves state-of-the-art in PSNR, LPIPS$^*$ and FID. We highlight the best result in bold font.}\vspace{-3mm}
\label{tab: xhuman}
\centering
\footnotesize
\begin{tabular}{l|rrr}
\toprule
\thead{Method}                        & \thead{PSNR$\uparrow$} & \thead{LPIPS*$\downarrow$} & \thead{FID$\downarrow$} \\
\midrule
GHG~\citep{kwon2024ghg}                           &    23.52 & 112.91 & 50.51     \\
LIFe-GoM (Ours) &  \textbf{25.32} & \textbf{99.32} & \textbf{42.90}    \\ 
\bottomrule
\end{tabular}\vspace{-6mm}
\end{table}

\textbf{Baselines.} We compare with GoMAvatar~\citep{wen2024gomavatar}, 3DGS-Avatar~\citep{qian20243dgs}, iHuman~\citep{paudel2024ihuman}, NHP~\citep{kwon2021neural}, NIA~\citep{kwon2023neural}, GHG~\citep{kwon2024ghg} and GPS-Gaussian~\citep{zheng2024gpsgaussian} on THuman2.0. On AIST++, we compare with HumanNeRF~\citep{Weng2022HumanNeRFFR}, GoMAvatar~\citep{wen2024gomavatar}, 3DGS-Avatar~\citep{qian20243dgs}, iHuman~\citep{paudel2024ihuman} and ActorsNeRF~\citep{mu2023actorsnerf}. Please refer to Appendix~\ref{sec: appendix_baseline} for details.


\textbf{Evaluation metrics.} We report  PSNR, $\text{LPIPS}^*(=\text{LPIPS} \times 1000)$ and FID on THuman2.0 following GHG~\citep{kwon2024ghg}. 
We report  PSNR, SSIM and LPIPS$^*$ on AIST++ following ActorsNeRF~\citep{mu2023actorsnerf}.

\subsection{Quantitative results}
\textbf{THuman2.0.} We summarize our results in \cref{tab: thuman2.0} for both the three-view and the five-view setting. 

In the three-view setting, our method significantly outperforms per-scene optimized methods including GoMAvatar, 3DGS-Avatar and iHuman, and generalizable approaches including NHP, NIA, and GHG in PSNR, LPIPS$^\ast$, and FID. 
Our approach achieves 24.65/110.82/51.27 in PSNR/LPIPS$^\ast$/FID, compared to GHG's 21.90/133.41/61.67. 
Importantly, we use 330K Gaussians for splatting, $7.5\times$ fewer than GHG's 2.8M, resulting in faster rendering (10.52ms vs.\ GHG's 20.30ms) at $1024\times1024$ resolution on a NVIDIA A100 GPU. 
Our method takes 907.92ms to reconstruct the coupled-multi-resolution Gaussians-on-Mesh in canonical space, significantly faster than scene-specific methods but slower than GHG. That said, reconstruction only needs to be done once per input subject, as the reconstructed avatar will be cached and reused for articulation and rendering, which runs at 95 FPS. 

We compare our approach to GPS-Gaussian using five images. As GPS-Gaussian relies on depth prediction between adjacent views, five images are the minimum it needs. Despite that, it still fails in non-overlapping regions. Our approach significantly improves upon GPS-Gaussian in this setting.





\textbf{XHuman.} We summarize the cross-dataset generalization results in \cref{tab: xhuman}. We directly apply GHG and our approach trained on THuman2.0 in the setting of 3 source views to the XHuman dataset without any finetuning. Our approach achieves PSNR/LPIPS*/FID of 25.32/99.32/42.90, significantly outperforming GHG's 23.52/112.91/50.51.

{\bf AIST++.} 
\cref{tab: aist} summarizes quantitative results on AIST++. Our method achieves 25.25/0.9812/21.61 in PSNR/SSIM/LPIPS*, matching ActorsNeRF's 25.23/0.9809/22.11 and surpassing per-scene optimized methods. Importantly, our method needs only 589 ms for 3D reconstruction, whereas iHuman, the fastest scene-specific method, requires 6.61s and other baselines take minutes to hours.



\begin{table}[t]
\vspace{-4mm}
\centering
\caption{\textbf{Comparison on AIST++.} We achieve comparable  quality as ActorsNeRF while requiring much less time in reconstruction or optimization. We highlight the best result in bold font. Methods marked in gray are per-scene optimized methods.}\vspace{-3mm}
\label{tab: aist}
\footnotesize
\begin{tabular}{l|rrr|rr}
\toprule
\thead{Method}                        & \thead{PSNR$\uparrow$} & \thead{SSIM$\uparrow$} & \thead{LPIPS*$\downarrow$} & \thead{\shortstack{Reconstruction or \\ optimization time$\downarrow$}} \\
\midrule
{\color[HTML]{9B9B9B} HumanNeRF~\citep{Weng2022HumanNeRFFR}}                           &   {\color[HTML]{9B9B9B}24.21}   &  {\color[HTML]{9B9B9B}0.9760}      &   {\color[HTML]{9B9B9B}29.66}  &           {\color[HTML]{9B9B9B}$\sim$2h}                           \\
{\color[HTML]{9B9B9B}GoMAvatar~\citep{wen2024gomavatar}} & {\color[HTML]{9B9B9B}24.34}& {\color[HTML]{9B9B9B}0.9780}& {\color[HTML]{9B9B9B}25.34}& {\color[HTML]{9B9B9B}$\sim$10h}\\
{\color[HTML]{9B9B9B}3DGS-Avatar~\citep{qian20243dgs}}	&{\color[HTML]{9B9B9B}25.14}	&{\color[HTML]{9B9B9B}0.9784}	&{\color[HTML]{9B9B9B}27.17}	&{\color[HTML]{9B9B9B}$\sim$2min} \\
{\color[HTML]{9B9B9B}iHuman~\citep{paudel2024ihuman}}	&{\color[HTML]{9B9B9B}25.17}	&{\color[HTML]{9B9B9B}0.9805}	&{\color[HTML]{9B9B9B}22.90}	&{\color[HTML]{9B9B9B}6.61s} \\
ActorsNeRF~\citep{mu2023actorsnerf}                           &   25.23   &   0.9809   &   22.45  &                  $\sim$4h                            \\
LIFe-GoM (Ours) &  \textbf{25.25}                        & \textbf{0.9812}                        & \textbf{21.61}   &      \textbf{589.27ms}             \\
\bottomrule
\end{tabular}\vspace{-2mm}
\end{table}

\begin{figure}[t]
\vspace{-2mm}
    \centering
    \begin{minipage}{.45\textwidth}
        \includegraphics[width=\linewidth]{results/crossdomain.pdf}
    \vspace{-8mm}
        \caption{\textbf{Cross-domain generalization} on DNA-Rendering dataset w/o finetuning.
        }
        \label{fig: cross_domain}
    \end{minipage}\hspace{.5cm}
    \begin{minipage}{.45\textwidth}
        \includegraphics[width=\linewidth]{results/novelpose.pdf}
        \vspace{-8mm}
    \caption{\textbf{Novel pose synthesis.} Poses are from BEDLAM dataset.}
    \label{fig: quant_novel_pose}
    \end{minipage}
\vspace{-6.5mm}
\end{figure}
\subsection{Qualitative results}


Please refer to Appendix~\ref{sec: appendix_result} for more qualitative results, including a comparison to baselines.

\textbf{Cross-domain generalization.} We demo our approach on cross-domain generalization in \cref{fig: cross_domain}, using the DNA-Rendering data~\citep{cheng2023dna}. Without fine-tuning, our approach can generalize to challenging subjects, e.g., loose clothes.

\textbf{Novel pose synthesis.} Instead of directly reconstructing human avatars in the pose of the source images, our approach outputs the canonical representation in T-pose via the  \texttt{Reconstructor}. %
Benefitting from this choice, we can synthesize novel poses without postprocessing such as binding the skeletons. In \cref{fig: quant_novel_pose}, we retarget the avatar to challenging new pose sequences from the BEDLAM dataset~\citep{Black_CVPR_2023}. The avatar is reconstructed using the model which was used to report results in the 3 source view setting of \cref{tab: thuman2.0}.\vspace{-0.2cm}

\subsection{Ablation studies}

\textbf{Analysis of iterative step choice.} 
We study how the number of iterations ($T$) influences the reconstruction time and rendering quality. Results are summarized in \cref{tab: iterative_update} and \cref{fig: iterative_update}(a). Note that $T=1$ means a single feed-forward pass, i.e., iterative updates are disabled. Using more iterations improves the rendering quality at the expense of more reconstruction time ($\sim$290ms per iteration). The PSNR improves by $+0.78$ and $+0.91$ when $T=2$ and $T=3$ respectively compared to $T=1$. Starting with $T=4$, the benefit of more iterations diminishes. We choose $T=3$ in our final model to balance  rendering quality and reconstruction time.

\begin{table}[t!]
\vspace{-4mm}
\centering
\caption{\textbf{Iterative step choice.} More iterations lead to better rendering at the expense of longer reconstruction. We use 3 iterations for the best quality-speed tradeoff, as highlighted in gray.}
\vspace{-3mm}
\label{tab: iterative_update}
\footnotesize
\begin{tabular}{c|rrr|r}
\toprule
\thead{\# iterations}                        & \thead{PSNR$\uparrow$} & \thead{LPIPS*$\downarrow$} & \thead{FID$\downarrow$} & \thead{\shortstack{Reconstruction \\ time (ms)$\downarrow$}} \\
\midrule
1 & 23.74 & 124.58 & 64.59 & 328.79 \\
2 & 24.52 & 112.47 & 52.16 & 618.67 \\
\cellcolor[HTML]{D3D3D3}3 & \cellcolor[HTML]{D3D3D3}24.65 & \cellcolor[HTML]{D3D3D3}110.82 & \cellcolor[HTML]{D3D3D3}51.27 & \cellcolor[HTML]{D3D3D3}907.92 \\
4 & 24.69 & 110.46 & 51.25 & 1198.14\\
5 & 24.70 & 110.38 & 51.02 & 1563.92 \\
\bottomrule
\end{tabular}\vspace{-5mm}
\end{table}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{results/ablation_v2.pdf}\vspace{-3mm}
    \caption{\textbf{Ablation studies.} We study the effect of iterative feedback (left). The geometry improves as the number of iterations increases. We show the importance of linking Gaussians to the high-resolution mesh (right). The high-resolution mesh is subdivided from the low-resolution counterpart. A higher resolution yields better texture details.}
    \label{fig: iterative_update}\vspace{-3mm}
\end{figure}

\textbf{Coupled-multi-resolution Gaussians-on-Mesh.} As mentioned in \cref{sec: canonical_rep} and \cref{sec: reconstruction}, we update the vertices of the low-resolution mesh, while the Gaussians are associated with the high-resolution mesh. Both are updated jointly. This choice is necessary for two reasons: 1) simply updating the vertices of the high-resolution mesh increases the reconstruction time from 907.92ms to 12.45s, making it too slow for both training and inference; 2) learning Gaussians in the high-resolution mesh guarantees good rendering quality. Note that the high-resolution mesh is obtained by subdividing the low-resolution mesh. In \cref{tab: multires}, we show that the rendering improves to 118.64/58.45 and 110.82/51.27 in LPIPS$^*$/FID when subdividing once and twice respectively from 140.60/93.44 without subdivision. The improvement can also be observed in \cref{fig: iterative_update}(b). Note that we do not observe consistent improvement in PSNR. This is because PSNR sometimes prefers blurry results. %
The resolution of the high-resolution mesh affects both the reconstruction speed and the rendering speed since we render the source images during the reconstruction stage. As the reconstruction time is still less than 1s, we choose to subdivide twice for better rendering quality.

\begin{table}[t!]
\centering
\caption{\textbf{Coupled-multi-resolution Gaussians-on-Mesh.} Increasing the number of subdivisions improves rendering quality at the cost of longer reconstruction and rendering times. We subdivide twice in our final model to ensure quality while maintaining real-time, as highlighted in gray.
}
\label{tab: multires}
\vspace{-2mm}
\footnotesize
\begin{tabular}{c|rrr|rr}
\toprule
\thead{\# subdivision}                        & \thead{PSNR$\uparrow$} & \thead{LPIPS*$\downarrow$} & \thead{FID$\downarrow$} & \thead{\shortstack{Reconstruction \\ time (ms)$\downarrow$}}  & \thead{\shortstack{Rendering \\ time (ms)$\downarrow$}} \\
\midrule
0 & 24.76 & 140.60 & 93.44 & 538.02 & 3.20 \\
1 & 24.88 & 118.64 & 58.45 & 607.49 & 3.93 \\
\cellcolor[HTML]{D3D3D3}2 & \cellcolor[HTML]{D3D3D3}24.65 & \cellcolor[HTML]{D3D3D3}110.82 & \cellcolor[HTML]{D3D3D3}51.27 & \cellcolor[HTML]{D3D3D3}907.92 & \cellcolor[HTML]{D3D3D3}10.52 \\
\bottomrule
\end{tabular}
\vspace{-5mm}
\end{table}
