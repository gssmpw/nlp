\section{Method}
In the following we first provide an overview of the proposed approach in \cref{sec:method:overview}. We then detail our two contributions: first the coupled-multi-resolution Gaussians-on-Mesh representation in \cref{sec: rendering} and then our reconstruction approach with iterative feedback in \cref{sec: reconstruction}. Finally we provide some information on training of the proposed method in \cref{sec:method:training}.

\subsection{Overview}
\label{sec:method:overview}
\textbf{Input.} The proposed method operates on a set of source images $\{I_n\}_{n=1}^N$, corresponding binary source masks $\{M_n\}_{n=1}^N$ identifying the human, source camera extrinsics $\{E_n\}_{n=1}^N$, source camera intrinsics $\{K_n\}_{n=1}^N$, and human poses $\{P_n\}_{n=1}^N$. Here, $N$ is the number of source images. The human pose $P_n=(R_n^j, T_n^j)_{j=1}^J$ is represented by a collection of $J$ rotations $R_n^j$ and translations $T_n^j$. %

\textbf{Output.} Given this input, our goal is to render the target image $I_\text{tg}^\text{pred}$ and its corresponding binary mask $M_\text{tg}^\text{pred}$ given as additional input the target camera extrinsics $E_\text{tg}$, intrinsics $K_\text{tg}$, and the target human pose $P_\text{tg}$, again specified via a collection of $J$ rotation matrices and translation vectors.

\textbf{Method overview.}
We render $I_\text{tg}^\text{pred}$ and $M_\text{tg}^\text{pred}$ by transforming a learned canonical Gaussian-on-Mesh representation $\text{GoM}^c$ %
specified in a  T-pose space. For this, Gaussians and  mesh (i.e., $\text{GoM}^c$) are first articulated using the target pose $P_\text{tg}$ and subsequently transformed to target image space via the target camera parameters. We provide details in \cref{sec: rendering} and formally write this as
    \begin{equation}
        I_\text{tg}^\text{pred}, M_\text{tg}^\text{pred} = \texttt{Renderer}(\text{GoM}^c, P_\text{tg}, E_\text{tg}, K_\text{tg}).
        \label{eq:render}
    \end{equation}

The canonical 3D representation $\text{GoM}^c$ is extracted from the $N$ source images. We abstract this via
    \begin{equation}
        \text{GoM}^c = \texttt{Reconstructor}(\{I_n\}_{n=1}^N, \{M_n\}_{n=1}^N, \{P_n\}_{n=1}^N, \{E_n\}_{n=1}^N, \{K_n\}_{n=1}^N), 
        \label{eq:reconstructor}
    \end{equation}
and provide details in \cref{sec: reconstruction}. Unlike GPS-Gaussian~\citep{zheng2024gpsgaussian}, we choose to reconstruct the subject in the canonical T-pose instead of the poses provided as an input. Benefitting from this choice, our representation can be retargeted to novel poses without any post-processing, such as skeleton binding. Further, our model can operate on images showing different poses. Notably, our $\text{GoM}^c$ representation uses different resolutions for the Gaussians and the mesh, and the $\texttt{Reconstructor}$ benefits from an iterative feedback update. %

\subsection{Coupled-multi-resolution Gaussians-on-Mesh representation}
\label{sec: rendering}
In this section, we describe the details of the \texttt{Renderer} used in  \cref{eq:render}. We first define the coupled-multi-resolution Gaussians-on-Mesh representation in \cref{sec: canonical_rep}, which refers to our canonical T-pose shape. Next, we detail  articulation and rendering in \cref{sec: articulation} and \cref{sec: gaussian_splatting}.

\subsubsection{Canonical representation}
\label{sec: canonical_rep}


The classic Gaussians-on-Mesh (GoM) representation associates one Gaussian with one triangle face of a mesh, i.e., the number of Gaussians is identical to the number of triangle faces. Further note, in GoMAvatar~\citep{wen2024gomavatar}, the vertices of the mesh and the Gaussians' parameters in the triangle's \textit{local} coordinates are optimized per scene. 
\begin{wrapfigure}{r}{0.5\textwidth}
        \includegraphics[width=\linewidth]{figs/rep_v2.pdf}
    \vspace{-5mm}
    \caption{\textbf{Multi-resolution Gaussians-on-Mesh representation.} We use a low-res mesh for faster animation and simpler geometry and attach Gaussians on a high-res mesh for better rendering.}
    \label{fig: gom_representation}
    \vspace{-8mm}
\end{wrapfigure}
To achieve high-quality rendering, GoMAvatar subdivides the mesh to increase the number of Gaussians. However, in the generalizable human rendering setting,  naively subdividing the mesh significantly increases the reconstruction time from less than 1s to $\sim$13s since the network operates on a larger set of points. We therefore study the coupled-multi-resolution Gaussians-on-Mesh representation. It reduces the computational cost while simultaneously improving the rendering quality. Concretely, we achieve this by  deforming the vertices of a low-resolution mesh and attaching the Gaussians to a coupled high-resolution mesh.

Formally, we define the coupled-multi-resolution Gaussians-on-Mesh representation in the canonical space as follows:
\begin{equation}
    \text{GoM}^c \triangleq \left\{\{v_{i}^{c\downarrow}\}_{i=1}^{V^\downarrow}, \{w_{i}^\downarrow\}_{i=1}^{V^\downarrow}, \{f_{j}^\downarrow\}_{j=1}^{F^\downarrow}, \{v_{i}^c\}_{i=1}^V, \{f_{j}\}_{j=1}^{F}\right\}. \label{eq: gom_c}
\end{equation}
Here, $\{v_{i}^{c\downarrow}\}_{i=1}^{V^\downarrow}$ and $\{f_{j}^\downarrow\}_{j=1}^{F^\downarrow}$ define the $V^\downarrow$ vertices and $F^\downarrow$ faces of the low-resolution mesh respectively. Note, $f_j^\downarrow \triangleq (\{\Delta_{j,k}^\downarrow\}_{k=1}^3)$, where $\Delta_{j,k}^\downarrow \in \{1, \dots, V^\downarrow\}$ is the $k$-th vertex index of the $j$-th triangle in the low-resolution mesh. To articulate it to any given human pose, we utilize  linear blend skinning weights $w_{i}^\downarrow \in \mathbb{R}^J$ corresponding to the $i$-th vertex $v_{i}^\downarrow$ in the low-resolution mesh. 

The high-resolution mesh is specified via $\{v_{i}^c\}_{i=1}^{V}$ and $\{f_{j}\}_{j=1}^{F}$, which subsume the $V$ vertices and $F$ faces. %
These are obtained by subdividing the low-resolution mesh. Different from the low-resolution mesh representation, we attach  Gaussians to the high-resolution face $f_j$, i.e.,
\begin{equation}
    f_j \triangleq (r_j, s_j, c_j, o_j, \{\Delta_{j,k}\}_{k=1}^3), \label{eq: face}
\end{equation}
with $j\in\{1, \dots, F\}$. Here, $r_j \in so(3)$ and $s_j \in \mathbb{R}^3$ are the rotation and scale in the faces's \textit{local} coordinate system. Moreover, $c_j \in \mathbb{R}^3$ is the RGB color, $o_j$ is the offset defined in the faces's \textit{local} coordinate system, and $\{\Delta_{j,k}\}_{k=1}^3$ are the three vertex indices belonging to the $j$-th triangle, i.e., $\Delta_{j,k} \in \{1, \dots, V\}$. We illustrate the representation in \cref{fig: gom_representation}.


\subsubsection{Articulation}
\label{sec: articulation}
It remains to answer 1) how we transform the defined coupled-multi-resolution Gaussians-on-Mesh representation to the target pose; and 2) how we perform rendering. To answer the first question, 
given a target pose $P_\text{tg}$, we articulate the canonical coupled-multi-resolution Gaussians-on-Mesh representation $\text{GoM}^c$ to a  Gaussians-on-Mesh representation $\text{GoM}^o \triangleq \left\{\{v_{i}^{o\downarrow}\}_{i=1}^{V^\downarrow}, \{f_{j}^\downarrow\}_{j=1}^{F^\downarrow}, \{v_{i}^o\}_{i=1}^V, \{f_{j}\}_{j=1}^{F}\right\}$ in the pose space utilizing linear blend skinning. Note that this representation is still multi-resolution because linear blend skinning is performed in the low-resolution space for efficiency reasons while high-quality rendering requires high-resolution Gaussian information. Concretely, we transform the canonical low-resolution 3D vertex coordinates $v_i^{c\downarrow}$ to posed low-resolution 3D vertex coordinates 
\begin{equation}
    v_i^{o\downarrow}=\texttt{LBS} \left( v_{i}^{c\downarrow}, w_{i}^\downarrow, P_\text{tg} \right) = \frac{\sum_{j=1}^J w_{i}^{j\downarrow} (R_j^p v_{i}^{c\downarrow} + t_j^p)}{\sum_{k=1}^{J} w_{i}^{k\downarrow}}. \label{eq: lbs}
\end{equation}
Here, $\texttt{LBS}$ refers to classic linear blend skinning. %
Since the high-resolution canonical space mesh $\left\{\{v_{i}^c\}_{i=1}^{V}, \{f_{j}\}_{j=1}^{F}, \right\}$ is obtained from the low-resolution canonical space mesh via subdivision, it is straightforward to transfer the vertex transformations between the posed low-resolution 3D vertex coordinates $v_i^{o\downarrow}$ and its canonical counterpart $v_i^{c\downarrow}$ to the high-resolution mesh and obtain $\{v_{i}^o\}_{i=1}^{V}$.

\subsubsection{Rendering with Gaussian splatting}
\label{sec: gaussian_splatting}
Given the pose space Gaussians-on-Mesh representation $\text{GoM}^o$ and the target camera parameters $E_\text{tg}$ and $K_\text{tg}$, we render the target image $I_\text{tg}^\text{pred}$ and the mask $M_\text{tg}^\text{pred}$ with Gaussian splatting.

Our Gaussian parameters defined in \cref{eq: face} are located in the triangle's local coordinates. To render the images, we first transform the local Gaussian parameters to the world coordinates. Following GoMAvatar~\citep{wen2024gomavatar}, we denote the local-to-world transformation of the $j$-th high-resolution face as $A_j$. The mean of the Gaussian and its covariance are computed via
\begin{equation}
\label{eq: gaussian}
    \mu_j = \frac{1}{3}\sum_{k=1}^3 v_{ \Delta_{j,k}}^o + A_j \cdot o_j 
    \quad\quad\text{and}\quad\quad
    \Sigma_{j} = A_{j} (R_{j} S_{j} S_{j}^T R_{j}^T) A_{j}^T,
\end{equation}
where $R_{j}$ and $S_{j}$ are the matrices encoding rotation $r_{j}$ and scale $s_{j}$. The color of the Gaussian is $c_j$.

\subsection{Reconstruction with iterative feedback}
\label{sec: reconstruction}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/iterative_module_v2.pdf} \vspace{-1cm}
    \caption{\textbf{Iterative feedback.} We iteratively update in a feed-forward way the vertices of the low-resolution mesh and the Gaussian parameters attached to the high-resolution mesh. We repeat the update for $T$ steps. Each step $t$ operates on the source images, camera parameters and human poses, as well as the last iteration's results including the canonical representation $\text{GoM}_{t-1}^c$ and the predicted source images rendered by $\text{GoM}_{t-1}^c$ (the brown arrows).} \vspace{-0.5cm}
    \label{fig:iterative_update}
\end{figure}

It remains to answer how to reconstruct the canonical space coupled-multi-resolution Gaussians-on-Mesh representation $\text{GoM}^c$. %
For this, our $\texttt{Reconstructor}$ defined in \cref{eq:reconstructor} uses sparse source images $\{I_n\}_{n=1}^N$ and masks $\{M_n\}_{n=1}^N$. Note that the sparse inputs can be multiview images or multi-frame images sampled from a monocular video, where human poses are not necessarily identical across frames. We also assume that human poses $\{P_n\}_{n=1}^N$ and camera parameters $\{E_n\}_{n=1}^N, \{K_n\}_{n=1}^N$ are given which can be human-annotated or predicted from off-the-shelf tools. %


We reconstruct the canonical representation in T-pose rather than the input poses, enabling animation without any post-processing and allowing the model to handle images in unaligned poses. %
The added difficulty due to this choice:  the gap between the canonical pose and the input poses. While scene-specific methods refine the canonical representation with gradient-based optimization, generalizable approaches must predict it in a feed-forward pass which leads to undesired reconstruction quality. To address this challenge, we propose iterative feedback updates that successively `refine' the canonical representation in a feed-forward manner, as illustrated in Fig.~\ref{fig:iterative_update}.

To compute 
$\text{GoM}^c$, we perform a $T$ step  iterative feedback update.
We use $\text{GoM}_t^c$ to denote the output representation from the $t$-th step, i.e., $t\in\{0, \dots, T\}$ and let 
\begin{equation}
    \text{GoM}_t^c \triangleq \left\{\{v_{t, i}^{c\downarrow}\}_{i=1}^{V^\downarrow}, \{w_{i}^\downarrow\}_{i=1}^{V^\downarrow}, \{f_{j}^\downarrow\}_{j=1}^{F^\downarrow}, \{v_{t, i}^c\}_{i=1}^V, \{f_{t, j}\}_{j=1}^{F}\right\}.
\end{equation}
Here, the step-dependent face information is given by 
\begin{equation}
    f_{t, j} \triangleq (r_{t, j}, s_{t, j}, c_{t, j}, o_{t, j}, \{\Delta_{j,k}\}_{k=1}^3), \quad\text{with}\quad j\in\{1, \dots, F\}.
\end{equation}
Note, $\text{GoM}_0^c$, the canonical representation at $t=0$, is the initialization and $\text{GoM}^c = \text{GoM}^c_T$. 

We emphasize that our iterative feedback updates  the low-resolution mesh vertices $\{v_i^{c\downarrow}\}_{i=1}^{V^\downarrow}$, and the Gaussian parameters $\{r_j, s_j, c_j, o_j\}_{j=1}^F$ associated with the high-resolution faces. The vertices in the high-resolution mesh $\{v_{t, i}^c\}_{i=1}^V$ follow the low-resolution update, analogously to the articulation update discussed in \cref{sec: articulation}. %

At each step $t$, we update the low-resolution mesh vertices and high-resolution Gaussian parameters using the following equations:
\begin{align}
    v_{t, i}^{c\downarrow} &= v_{t-1, i}^{c\downarrow} + \text{MLP}(\Tilde{F}_{t, i}^\downarrow), \label{eq: v_update} \\
    r_{t, j}, s_{t, j}, c_{t, j}, o_{t, j} &= \text{MLP}(\text{cat}(\Tilde{F}_{t, j} , \{F_{n, t, j}\}_{n=1}^N)). \label{eq: gaussian_update}
\end{align}
Here, $\Tilde{F}_{t, i}^\downarrow, i\in\{1, \dots, V^\downarrow\}$ is our `feedback' feature for the $i$-th vertex in the low-resolution mesh. Further, $\Tilde{F}_{t, j}, j\in\{1, \dots, F\}$ in \cref{eq: gaussian_update} is a `feedback' feature for the $j$-th face in the high-resolution mesh. It is acquired by first interpolating $\Tilde{F}_{t, i}^\downarrow, i\in\{1, \dots, V^\downarrow\}$ to get vertex features in the high-resolution mesh and then concatenating the 3 vertices' features belonging to the $j$-th face.
To preserve  details, we also concatenate source image features  $\{F_{n, t, j}\}_{n=1}^N$ which are obtained by projecting the mean of the $j$-th Gaussian at step $t$ to the $n$-th view.

To compute the `feedback' feature $\Tilde{F}_{t, i}^\downarrow, i\in\{1, \dots, V^\downarrow\}$, 
we first render the source views using the canonical representation from the last iteration via
\begin{equation}
    I_{n, t-1}^\text{pred}, M_{n, t-1}^\text{pred} = \texttt{Renderer}(\text{GoM}_{t-1}^c, P_n, E_n, K_n), \quad n\in\{1, \dots, N\}.
\end{equation}
Then we extract  image features from $\{I_{n, t-1}^\text{pred}\}_{n=1}^N$. %
For each vertex $v_{t, i-1}^{c\downarrow}$ in the low-resolution mesh, we extract  pixel-aligned 
source image features and predicted image features. We concatenate both and
feed them into an iterative feedback  network. The iterative feedback  network consists of a multi-source fusion block that mixes the information from $N$ sources, and a Point Transformer that encodes all the vertices. Its output feature is $\{\Tilde{F}_{t, i}^\downarrow\}_{i=1}^{V^\downarrow}$. Please refer to Appendix~\ref{sec: appendix_arch} for more details.


\subsection{Training}
\label{sec:method:training}
Both rendering and reconstruction using our iterative feedback network and coupled representation are end-to-end differentiable. To learn the network parameters, we use a training loss composed of L1 and perceptual losses, comparing predicted and ground-truth RGB images, L1 loss for masks as well as a Laplacian loss for regularization. The loss is averaged over all source and target images, as well as all $T$ iterative feedback steps. Formally, we minimize the average of $\text{Loss}_{n,t}, n\in\{1, \dots, N, \text{tg}\}, t\in\{1, \dots, T\}$ and 
\begin{equation}
   \text{Loss}_{n, t} = L_1(I_n, I_{n, t}^\text{pred}) + \lambda_\text{per} \text{Perceptual}(I_n, I_{n, t}^\text{pred}) + \lambda_M L_1(M_n, M_{n, t}^\text{pred}) + \lambda_\text{lap} \text{Lap}(\text{GoM}_t^c).
\label{eq: loss}
\end{equation}
Here, $L_1(\cdot, \cdot)$ is the L1 loss. $\text{Perceptual}(\cdot, \cdot)$ is the perceptual loss between predictions and ground-truths, e.g., SSIM or LPIPS. $\text{Lap}(\cdot)$ is the Laplacian loss applied on the low-resolution mesh of the canonical GoM representation. $\lambda_\text{per}$,  $\lambda_M$ and $\lambda_\text{lap}$ are user-specified hyperparameters. 
