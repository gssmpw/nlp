\section{Related Work}
\paragraph{Time-Sensitive Datasets}
In recent years, time-sensitive datasets have predominantly been designed for question answering tasks and primarily consisting of textual data~\cite{zhang2021situatedqa,chen2021dataset,tan2023towards,liska2022streamingqa,wei2023menatqa,yang2024continual}. Among these, the SituatedQA dataset~\cite{zhang2021situatedqa} represents a significant contribution by emphasizing open-domain, time-sensitive question answering. It reannotates questions from the Natural Questions (NQ)~\cite{kwiatkowski2019natural} and Wikidata~\cite{vrandevcic2014wikidata} to capture contextual dependencies and temporal variations in answers. Similarly, TimeQA~\cite{chen2021dataset} comprises 20,000 questions, including a challenging variant that requires models to infer answers from implicit temporal cues in text passages. Additionally, the TempReason dataset~\cite{tan2023towards} offers a comprehensive framework for evaluating various facets of temporal understanding. In these Open Book Question Answering (OBQA) settings, models leverage external textual resources to deduce correct answers~\cite{izacard2020leveraging,zaheer2020big,wei2021finetuned,ouyang2022training,yang2024enhancing}.

Time-sensitive datasets have also been developed for Closed Book Question Answering (CBQA), where models must generate answers relying solely on the information contained in the question~\cite{fevry2020entities,roberts2020much,dhingra2022time}. Furthermore, datasets built on knowledge graphs—such as TEQUILA~\cite{jia2018tequila}, TimeQuestions~\cite{jia2021complex}, and CronQuestions~\cite{saxena2021question}—pose more complex natural language queries, requiring models to rank entities according to their temporal relevance.

%In recent years, numerous contemporary time-sensitive datasets have been introduced, predominantly composed in the format of question answering and exclusively in textual form~\cite{zhang2021situatedqa,chen2021dataset,tan2023towards,liska2022streamingqa,wei2023menatqa}. A significant contribution to this field is the SituatedQA dataset~\cite{zhang2021situatedqa}, which emphasizes open-domain time-sensitive QA. It uniquely reannotates questions from the Natural Questions (NQ)~\cite{kwiatkowski2019natural} and Wikidata~\cite{vrandevcic2014wikidata} to reflect context dependency and variability in answers across different times and locations. Another notable dataset, TimeQA~\cite{chen2021dataset} comprises 20,000 questions and its hard version requiring models to infer from implicit temporal cues within text passages.In addition, the TempReason dataset~\cite{tan2023towards} introduced by Tan presents a comprehensive framework for evaluating various aspects of temporal understanding. These datasets with the Open Book Question Answering (OBQA) setting, relying on external text to help language models~\cite{izacard2020leveraging,zaheer2020big,wei2021finetuned,ouyang2022training} in deducing correct answers.

%There are also time-sensitive datasets structured around Closed Book Question Answering (CBQA), where the models must rely solely on the information within the question, without external text~\cite{fevry2020entities,roberts2020much,dhingra2022time}.Moreover, there are time-sensitive datasets based on knowledge graphs, such as TEQUILA~\cite{jia2018tequila}, TimeQuestions~\cite{jia2021complex}, and CronQuestions~\cite{saxena2021question}. These datasets feature more complex questions in natural language and require models to rank entities from a knowledge graph based on their temporal relevance.

\paragraph{Multimodal Dialogue Datasets}
Multimodal dialogue research has gained traction with the emergence of datasets that integrate images with multi-turn textual dialogues. Such datasets aim to jointly model visual and linguistic information to either answer questions~\cite{antol2015vqa,das2017visual,seo2017visual,kottur2019clevr,li2023stablellava} or generate coherent responses~\cite{meng2020openvidial,zheng2021mmchat,wang2021openvidial,zang2021photochat,feng2022mmdialog}. For example, Mostafazadeh et al.~\cite{mostafazadeh2017image} introduced the IGC dataset, which comprises 4,000 dialogues centered around an image accompanied by a textual description and related questions and responses. Building on this, Shuster et al.~\cite{shuster2018image} released the ImageChat dataset, a substantially larger collection that captures more diverse conversational scenarios.
Recent efforts have incorporated persona information to foster more personalized interactions. Datasets such as FoCusd~\cite{jang2022call}, MPChat~\cite{ahn2023mpchat}, DuLeMon~\cite{xu2022long}, and MSPD~\cite{kwon2023and} augment dialogues with persona details—ranging from purely textual to multimodal attributes—enabling models to extract relevant personal context and enhance the naturalness of generated responses.

%Recently, several multimodal dialogue datasets have emerged, incorporating one or more images alongside multi-turn textual dialogues. Research in multimodal dialogue primarily aims to comprehend images and utterances within a context to either answer questions~\cite{antol2015vqa,das2017visual,seo2017visual,kottur2019clevr, li2023stablellava} or generate natural responses~\cite{meng2020openvidial,zheng2021mmchat,wang2021openvidial,zang2021photochat,feng2022mmdialog}. ~\cite{mostafazadeh2017image} introduced the IGC dataset, which consists of 4,000 dialogues, each featuring an image with a textual description as well as accompanying questions and responses centered around the image. ~\cite{shuster2018image} released the ImageChat dataset, which significantly larger than IGC. As research into multimodal dialogue has deepened, datasets incorporating persona information have become increasingly prevalent. Datasets such as FoCusd~\cite{jang2022call}, MPChat~\cite{ahn2023mpchat}, DuLeMon~\cite{xu2022long}, and MSPD~\cite{kwon2023and} include dialogues paired with persona information, ranging from purely textual to multimodal personas. Correspondingly, models are designed to extract relevant personal information, which can significantly enhance the generation of dialogue responses.