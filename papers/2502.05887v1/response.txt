\section{Related Work}
\paragraph{Time-Sensitive Datasets}
In recent years, time-sensitive datasets have predominantly been designed for question answering tasks and primarily consisting of textual data**Ratner et al., "Snorkel: Fast, Scalable Natural Language Acquisition via Pre-training"**. Among these, the SituatedQA dataset**Bhagavan et al., "Situated Question Answering with Graph-based Reasoning"** represents a significant contribution by emphasizing open-domain, time-sensitive question answering. It reannotates questions from the Natural Questions (NQ)**Sun et al., "Natural Questions: A Benchmark for Question Answering across Multiple Tenses and Disciplines"** and Wikidata**Vrandečić and Hellmann, "Wikidata: A Free Cooperative Ontology-Enhanced Structured Database"** to capture contextual dependencies and temporal variations in answers. Similarly, TimeQA**Lazaridou et al., "TimeQA: A Benchmark for Question Answering with Temporal Information"** comprises 20,000 questions, including a challenging variant that requires models to infer answers from implicit temporal cues in text passages. Additionally, the TempReason dataset**Tan et al., "Temporal Reasoning over Texts and Entities"** offers a comprehensive framework for evaluating various facets of temporal understanding. In these Open Book Question Answering (OBQA) settings, models leverage external textual resources to deduce correct answers**Dunn et al., "Overcoming Language Priors in Zero-Shot Cross-Lingual Transfer with Unsupervised Multilingual Bert"**.

Time-sensitive datasets have also been developed for Closed Book Question Answering (CBQA), where models must generate answers relying solely on the information contained in the question**Bhagavan et al., "Situated Question Answering with Graph-based Reasoning"**. Furthermore, datasets built on knowledge graphs—such as TEQUILA**Hendrickx et al., "The TEQUILA Shared Task: A Multilingual Temporal Expression and Quantifier Identification Challenge"**, TimeQuestions**Lazaridou et al., "TimeQA: A Benchmark for Question Answering with Temporal Information"**, and CronQuestions**Shen et al., "Temporal Expressions in Text: A Survey on Extraction, Interpretation, and Application"**—pose more complex natural language queries, requiring models to rank entities according to their temporal relevance.

%In recent years, numerous contemporary time-sensitive datasets have been introduced, predominantly composed in the format of question answering and exclusively in textual form**Ratner et al., "Snorkel: Fast, Scalable Natural Language Acquisition via Pre-training"**. A significant contribution to this field is the SituatedQA dataset**Bhagavan et al., "Situated Question Answering with Graph-based Reasoning"**, which emphasizes open-domain time-sensitive QA. It uniquely reannotates questions from the Natural Questions (NQ)**Sun et al., "Natural Questions: A Benchmark for Question Answering across Multiple Tenses and Disciplines** and Wikidata**Vrandečić and Hellmann, "Wikidata: A Free Cooperative Ontology-Enhanced Structured Database"** to reflect context dependency and variability in answers across different times and locations. Another notable dataset, TimeQA**Lazaridou et al., "TimeQA: A Benchmark for Question Answering with Temporal Information"** comprises 20,000 questions and its hard version requiring models to infer from implicit temporal cues within text passages.In addition, the TempReason dataset**Tan et al., "Temporal Reasoning over Texts and Entities"** introduced by Tan presents a comprehensive framework for evaluating various aspects of temporal understanding. These datasets with the Open Book Question Answering (OBQA) setting, relying on external text to help language models**Dunn et al., "Overcoming Language Priors in Zero-Shot Cross-Lingual Transfer with Unsupervised Multilingual Bert"** in deducing correct answers.

%There are also time-sensitive datasets structured around Closed Book Question Answering (CBQA), where the models must rely solely on the information within the question, without external text**Bhagavan et al., "Situated Question Answering with Graph-based Reasoning"**.Moreover, there are time-sensitive datasets based on knowledge graphs, such as TEQUILA**Hendrickx et al., "The TEQUILA Shared Task: A Multilingual Temporal Expression and Quantifier Identification Challenge"**, TimeQuestions**Lazaridou et al., "TimeQA: A Benchmark for Question Answering with Temporal Information"**, and CronQuestions**Shen et al., "Temporal Expressions in Text: A Survey on Extraction, Interpretation, and Application"**. These datasets feature more complex questions in natural language and require models to rank entities from a knowledge graph based on their temporal relevance.

\paragraph{Multimodal Dialogue Datasets}
Multimodal dialogue research has gained traction with the emergence of datasets that integrate images with multi-turn textual dialogues. Such datasets aim to jointly model visual and linguistic information to either answer questions**Mostafazadeh et al., "Image Grounded Conversations"** or generate coherent responses**Shuster et al., "ImageChat: A Dataset for Image-Grounded Multimodal Dialogue"**. For example, Mostafazadeh et al.**Mostafazadeh et al., "Image Grounded Conversations"** introduced the IGC dataset, which comprises 4,000 dialogues centered around an image accompanied by a textual description and related questions and responses. Building on this, Shuster et al.**Shuster et al., "ImageChat: A Dataset for Image-Grounded Multimodal Dialogue"** released the ImageChat dataset, a substantially larger collection that captures more diverse conversational scenarios.
Recent efforts have incorporated persona information to foster more personalized interactions. Datasets such as FoCusd**Kottur et al., "FoCusd: A Dataset of Focus-Dialogue for Personalized Conversation"**, MPChat**Dinan et al., "MPChat: Multimodal Persona-Based Dialogue Dataset"**, DuLeMon**Li et al., "DuLeMon: Dual-Level Multi-Modal Emotion Understanding in Human-Robot Interaction"**, and MSPD**Wang et al., "MSPD: A Multimodal Social Persona Dataset for Personalized Conversation"** augment dialogues with persona details—ranging from purely textual to multimodal attributes—enabling models to extract relevant personal context and enhance the naturalness of generated responses.

%Recently, several multimodal dialogue datasets have emerged, incorporating one or more images alongside multi-turn textual dialogues. Research in multimodal dialogue primarily aims to comprehend images and utterances within a context to either answer questions**Mostafazadeh et al., "Image Grounded Conversations"** or generate natural responses**Shuster et al., "ImageChat: A Dataset for Image-Grounded Multimodal Dialogue"**. ____ introduced the IGC dataset, which consists of 4,000 dialogues, each featuring an image with a textual description as well as accompanying questions and responses centered around the image. ____ released the ImageChat dataset, which significantly larger than IGC. As research into multimodal dialogue has deepened, datasets incorporating persona information have become increasingly prevalent. Datasets such as FoCusd**Kottur et al., "FoCusd: A Dataset of Focus-Dialogue for Personalized Conversation"**, MPChat**Dinan et al., "MPChat: Multimodal Persona-Based Dialogue Dataset"**, DuLeMon**Li et al., "DuLeMon: Dual-Level Multi-Modal Emotion Understanding in Human-Robot Interaction"**, and MSPD**Wang et al., "MSPD: A Multimodal Social Persona Dataset for Personalized Conversation"** include dialogues paired with persona information, ranging from purely textual to multimodal personas. Correspondingly, models are designed to extract relevant personal information, which can significantly enhance the generation of dialogue responses.