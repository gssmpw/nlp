\section{Experiments}
\subsection{Experimental Setup}
\paragraph{Baselines} We consider the following baselines:

\begin{itemize}
\vspace{-2mm}
    \item SBERT+CLIP: We adopt a Transformer~\cite{vaswani2017attention} initialized weights of SBERT~\cite{reimers2019sentence} and CLIP-ViT-B/32 vision model~\cite{radford2021learning} as text encoder and vision encoder to represent text and image respectively. SBERT enhances the original BERT model~\cite{devlin2018bert} to better handle similarity comparisons of  dialogue and memory textual information. CLIP-ViT-B/32 vision model utilizes a Vision Transformer (ViT)~\cite{dosovitskiy2020image} with 32 attention heads, which enables it to capture more visual features.
    
\vspace{-2mm}
    \item CLIP+CLIP: We utilize the CLIP-ViT-B/32 model~\cite{radford2021learning} as text encoder (CLIP-ViT-B/32 text model) and vision encoder (CLIP-ViT-B/32 vision model). CLIP-ViT-B/32 text model employs a Transformer similar to GPT~\cite{radford2018improving}, designed specifically for processing textual input, making it ideally suited for textual analysis requirements.

\end{itemize}

\paragraph{Training}

We train both baselines and our framework for 5 epochs with a batch size of 8 on a NVIDIA Tesla V100 GPU. The model is optimized using Adam~\cite{kingma2014adam} with a learning rate of $3e^{-6}$. For our framework, we incorporated the Adaptive Temporal Module (ATM) into two baselines to validate the effectiveness of framework. We set the number of speaker's memories is $m=20$ and the number of candidates is $C=100$.

\paragraph{Evaluation Metrics}
We assess the performance of the model on two tasks using Recall@1 and Mean Reciprocal Rank (MRR), which is the standard evaluation metrics on dialogue task~\cite{lee2021constructing,feng2022mmdialog, ahn2023mpchat}. Recall@1 quantifies the model's accuracy in retrieving the most relevant result as the top result for each query, effectively capturing the model's ability to return the most relevant result as the first item. MRR evaluates the average inverse ranking of the first relevant result across queries, providing insight into the model's overall retrieval quality.

\subsection{Experimental Results}

We conduct experiments of two baselines with and without our framework on time-sensitive tasks in MTPChat. Besides, we define two input settings: one limited to dialogue, and the other encompassing both dialogue and speaker's memories. The findings, as depicted in Table~\ref{results1}, reveal several insights: 1) MTPChat poses challenges in terms of the multimodal temporal awareness capabilities of models. Despite TNRP and TGMP being retrieval tasks, both baselines exhibited inadequate performance on these time-sensitive challenges, achieving Recall@1 scores not surpassing 70. 2) Our framework is model-agnostic and effective, enhancing performance over both baselines. Note that in our TNRP task, where labels contain only the response text, the ATM module—which is tailored for multimodal fusion balance—yields a less pronounced improvement. 
%Specifically, it outperformed the SBERT+CLIP model by 10\% and 2.2\% in Recall@1 for the TNRP and TGMP tasks, respectively. Similarly, it surpassed the CLIP+CLIP model, recording improvements of 10\% and 6.8\% in Recall@1 for the same tasks. 
3) The temporal ordering of dialogue and memories plays a pivotal role in MTPChat. In previous works with multimodal persona-grounded dialogue datasets~\cite{zhong2020towards,wen2021automatically}, the persona information serves as supplementary data to improve the accuracy of predicted dialogue responses. However, in MTPChat, both persona memory and dialogue are essential components. They not only enhance the model's temporal awareness but also significantly influence performance. For instance, for CLIP+CLIP+ATM model on TGMP task, when the input lacked memory data, performance significantly dropped by 20.1\% in Recall@1 and 15.1\% in MRR.


\begin{table}[t]

\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{l|c|c|c|c|c}
\toprule
\multirow{2}*{Model} & \multirow{2}*{Input Setting} &\multicolumn{2}{c|}{TNRP} & \multicolumn{2}{c}{TGMP}\\ 
\cline{3-6}
%\cline{6-17}
 \multirow{2}{*}{}&\multirow{2}{*}{} &R@1&MRR&R@1&MRR\\
\midrule
\multirow{2}*{SBERT+CLIP} &{$d$} & 58.26 & 69.90 & 49.17 & 63.38\\
\cline{2-6}
%\multirow{3}*{} &{$d$, $\mathcal{M}$}(zero-shot) &  & & & \\
\cline{2-6}
\multirow{2}*{} &{$d$, $\mathcal{M}$} & 61.32 & 72.55 & 58.90 & 73.53 \\
\midrule
\multirow{2}*{SBERT+CLIP+ATM} &{$d$} & 58.70 & 70.26 & 52.04 & 65.35 \\
\cline{2-6}
%\multirow{3}*{} &{$d$, $\mathcal{M}$}(zero-shot) &  & & & \\
\cline{2-6}
\multirow{2}*{} &{$d$, $\mathcal{M}$} &  61.55 & 72.78 & 60.22 & 74.26  \\
\midrule
\multirow{2}*{CLIP+CLIP} &{$d$} & 66.20 & 76.34 & 56.91 & 70.64\\
\cline{2-6}
%\multirow{3}*{} &{$d$, $\mathcal{M}$}(zero-shot) &  & & & \\
\cline{2-6}
\multirow{2}*{} &{$d$, $\mathcal{M}$} & 68.75 & 78.66 & 67.25& 80.50\\
\midrule

\multirow{2}*{CLIP+CLIP+ATM} &{$d$} & 66.97 & 76.96 &  57.35 & 71.04 \\
\cline{2-6}
%\multirow{3}*{} &{$d$, $\mathcal{M}$}(zero-shot) &  & &  & \\
\cline{2-6}
\multirow{2}*{} &{$d$, $\mathcal{M}$} & \textbf{69.26} & \textbf{78.92} & \textbf{71.82} & \textbf{83.68} \\
\bottomrule
\end{tabular} 
}
\vspace{-2mm}
\caption{\label{results1}
Results of the Temporal Next Response Prediction (TNRP) and Temporal Grounding Memory Prediction (TGMP) tasks. Symbols means: dialogue $d=(c^{d}, {i}^{d}, t^{d})$ contains a context, an image and time information. A speaker's memory set $\mathcal{M}=\{M_{1},\ldots,M_{m}\}$, where each memory $M = (c^M, i^M, t^M)$ characterized by a context, an image and time information.
}
\end{table}

%exp2 learnable linear weights
%exp3 mean weights

\begin{table}[t]

\resizebox{\linewidth}{!}{
\begin{tabular}{p{4.5cm}|p{3cm}<{\centering}|p{3cm}<{\centering}}
\toprule
\multirow{2}*{Method} & \multicolumn{2}{c}{Temporal Grounding Memory Prediction}\\ 
\cline{2-3}
%\cline{6-17}
 \multirow{2}{*}{} &R@1&MRR\\
\midrule
Attention Fusion& 63.65 & 76.72\\
\midrule
Linear Fusion& 66.41 & 79.59 \\
\midrule
Mean-Pool Fusion& 67.25 & 80.50 \\
\midrule
ATM (ours)& \textbf{71.82} & \textbf{83.68} \\
\bottomrule
\end{tabular} 
}
\vspace{-2mm}
\caption{\label{results2}
Comparison of Adaptive Temporal Module (ATM) with other methods of feature integration on Temporal Grounding Memory Prediction task.
}
\vspace{-3mm}
\end{table}


In addition, to evaluate the performance of the Adaptive Temporal Module within our proposed system, we conducted a comparative analysis against other feature fusion methods:
\begin{itemize}
 \vspace{-2mm}
    \item Attention Fusion: This method adeptly combines textual and temporal data with image features, employing an attention-based module to learn weights. This enhances the model's sensitivity to contextually significant features.
 \vspace{-2mm}   
    \item Linear Fusion: Incorporates two linear layers optimized during training, enabling the model to directly learn the weights that most effectively combine textual and visual information.
 \vspace{-6mm} 
    \item Mean-Pool Fusion: This approach computes the mean of the combined features, aggregating them from different modalities by simple averaging.

\end{itemize}

These methods were assessed using the CLIP+CLIP model on the Temporal Grounding Memory Prediction (TGMP) task. The findings in Table~\ref{results2} indicate that the Adaptive Temporal Module surpassed other techniques, achieving improvements of 12.8\%, 8.1\%, and 6.4\% in Recall@1, respectively. These results substantiate the superior capability of our framework to effectively enhance multimodal integration with temporal awareness.


\subsection{Ablation Study}

\paragraph{Zero-Shot Setting}

\begin{table}[h]

\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{l|c|c|c|c|c}
\toprule
\multirow{2}*{Model} & \multirow{2}*{Input Setting} &\multicolumn{2}{c|}{TNRP} & \multicolumn{2}{c}{TGMP}\\ 
\cline{3-6}
%\cline{6-17}
 \multirow{2}{*}{}&\multirow{2}{*}{} &R@1&MRR&R@1&MRR\\
\midrule
\multirow{2}*{CLIP+CLIP} &{$d$, $\mathcal{M}$(zero-shot)} & 39.49 & 52.07 & 54.59 & 61.27 \\
\cline{2-6}
%\multirow{3}*{} &{$d$, $\mathcal{M}$}(zero-shot) &  & & & \\
\cline{2-6}
\multirow{2}*{} &{$d$, $\mathcal{M}$} & 68.75 & 78.66 & 67.25 & 80.50  \\

\bottomrule
\end{tabular} 
}
 \vspace{-2mm} 
\caption{\label{results3}
Ablation study of baseline CLIP+CLIP with zero-shot setting.
}
\vspace{-2mm} 
\end{table}

We explore the performance of the CLIP+CLIP model with a zero-shot setting on time-sensitive tasks. As shown in Table~\ref{results3}, the model demonstrates poor performance on MTPChat time-sensitive tasks, showing the challenges inherent in MTPChat and highlighting the urgent need for research into multimodal temporal awareness.

\paragraph{The Importance of Temporal Awareness}

\begin{table}[h]

\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{l|c|c|c}
\toprule
\multirow{2}*{Model} & \multirow{2}*{Input Setting}  & \multicolumn{2}{c}{TGMP}\\ 
\cline{3-4}
%\cline{6-17}
 \multirow{2}{*}{}&\multirow{2}{*}{} &R@1&MRR\\
\midrule
\multirow{2}*{CLIP+CLIP} &{$d$, $\mathcal{M}$(without time)} & 60.99 & 65.09  \\
\cline{2-4}
%\multirow{3}*{} &{$d$, $\mathcal{M}$}(zero-shot) &  & & & \\
\cline{2-4}
\multirow{2}*{} &{$d$, $\mathcal{M}$} & 68.75 & 78.66   \\

\bottomrule
\end{tabular} 
}
\vspace{-2mm} 
\caption{\label{results4}
Ablation study of baseline CLIP+CLIP without time information.
}
\vspace{-2mm} 
\end{table}


This study highlights the critical role of temporal awareness in models. Utilizing the CLIP+CLIP model, we trained on datasets both with and without temporal data of dialogue and memories. These models were then evaluated on the Temporal Grounding Memory Prediction (TGMP) task. Our findings (see Table~\ref{results4}) reveal a marked difference in performance: models without temporal awareness demonstrated substantial difficulties in time-sensitive tasks. Conversely, models incorporating temporal awareness significantly excelled, achieving a 12.7\% increase in recall@1 and a 20.8\% improvement in MRR.


%sbert+clip的结果
%添加少量的时间数据，可以帮助GPP任务涨点
