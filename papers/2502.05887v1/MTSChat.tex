%\subsection{Comparison with Other Datasets}
\section{Comparison with Existing Datasets}
We start with a brief comparison of existing datasets, emphasizing multi-modal and time-aware strategies (see Table~\ref{comparison} for an overview).

\paragraph{Time-Sensitive QA Datasets}

Time-Sensitive Question Answering (TSQA) involves interpreting and responding to questions that are dependent on specific time points or intervals. We analyse a set of TSQA datasets~\cite{dhingra2022time,chen2021dataset,liska2022streamingqa,tan2023towards}, as shown in the upper part of Table~\ref{comparison}. 
%These datasets challenge a model's ability to handle time by modifying the temporal information within the questions. 
Currently, TSQA datasets typically use free-text form or knowledge graphs (KGs) and are structured as QA tasks. However, our work introduces the first multimodal time-aware dataset based on conversation. Similar to TSQA, we modify the time of dialogues, which affects the responses and the related grounding memory, thereby testing the model's ability to understand time.

\paragraph{MultiModal Dialogue Datasets}

Multimodal dialogue datasets generally comprise one or more images and multi-turn textual dialogues. As depicted in the lower half of Table~\ref{comparison}, we analyse two representative datasets~\cite{zang2021photochat,feng2022mmdialog}. These datasets are designed for models to interpret images and utterances within a dialogue framework and generate coherent responses. Our MTPChat dataset, although drawing on the conversational structure and task, distinctively emphasizes the annotation and manipulation of time information. MTPChat allows the model to acknowledge the influence of temporal dynamics on dialogue interaction and memory processes, demonstrating temporal awareness. 
%This feature marks a significant distinction from other current multimodal dialogue datasets.

\paragraph{Time-Sensitive Video-Centric Dataset}

TimeIT~\cite{ren2023timechat} is a novel dataset focused on video-based instructions, encompassing a collection of long-video datasets annotated with timestamps. It requires models to describe video content across specified time intervals. The description follows a structured format, such as ``<timestamp\_start> to <timestamp\_end> seconds: <event\_description>''. 
%TimeIT aims to enable models to intuitively describe content corresponding to timestamps in videos, testing their temporal sensitivity. 
Ingeniously, our dataset integrates time of dialogues and memories, making model awareness of the time order of dialogue and memory significant influence on dialogue responses and memory recall. In contrast to TimeIT's tasks that directly answer timestamp and associated content, MTPChat offers a more complex challenge with implicit time factor, pushing the boundaries of temporal understanding in multimodal dialogue models.

\section{MTPChat Dataset}

Our dataset is built on the basis of MPChat~\cite{ahn2023mpchat}, a comprehensive multimodal persona-grounded dialogue dataset that includes both linguistic and visual components derived from episodic-memory-based personas. MPChat gathered from the social media platform Reddit, consists of memory image-sentence pairs and dialogue instances grounded on the speakers' multimodal memories. 

A significant challenge is the ingenious integration of time information and multimodal dialogue, aiming to establish a multimodal time-aware dataset. Based on MPChat dataset, we develop a novel methodology that involves three primary steps: 1) Time annotations, 2) Constructing time-aware conversations, and 3) Memory annotations. These efforts achieve the creation of a pioneering multimodal time-aware dialogue dataset. MTPChat breaks away from the limitations of current time-sensitive datasets confined to QA tasks, free-text formats and relying on explicit time information. We believe that our work fosters the development of more diverse time-sensitive datasets and advancing research toward achieving human-level temporal understanding in models.

\subsection{Time Annotations}
%\subsection{Constructing Time Information}%构建时间信息
\label{section:2.1}

We converted the UTC strings in MPChat dataset into date format ``yyyy/mm/dd'' and incorporated this feature into both the dialogue and memory components. The dialogue in our dataset is structured as a triplet consisting of (dialogue context, dialogue image, dialogue time), while each memory of the speaker is similarly organized as a triplet (memory description context, memory image, memory time).

\subsection{Time-Aware Conversations}
%\subsection{Constructing Dialogues Before Grounding Memory Takes Place} 
\label{section:2.2}

In real-world scenarios, conversations can vary significantly based on the time they occur, even with similar contexts. For instance, as a high school student asked, ``What is machine learning?'', you might respond with no knowledge on the subject. However, after three years of studying machine learning at university, your response to the same conversation would be more detailed, potentially including discussions about deep learning and related topics.

Inspired by how the temporal order of conversation and memories influences human responses, we constructed conversational data with temporal orders:
\begin{itemize}
\vspace{-2mm}
\item Later Stage Conversations: We used the original memories and conversations from the MPChat dataset, adding time annotations as described in Section~\ref{section:2.1}. For instance, if you are a university student with three years of study in machine learning and are asked, ``What is machine learning?'', your response might include topics like deep learning.
\vspace{-1mm}
    \item Early Stage Conversations: To simulate conversations from earlier times, we assumed there was no prior memory of the discussion topic. We used the context of the original conversations but removed the original responses. We then add new, earlier time annotations and responses. The newly created responses differ from the original ones and contain minimal information about the discussion topic due to the lack of relevant memory. For example, if you are a high school student asked, ``What is machine learning?'', you might respond with little to no knowledge on the subject.

    Specifically, we utilized GPT-4 \cite{achiam2023gpt} to process a combination of inputs: the dialogue context, dialogue image, newly modified dialogue time, and speaker memories predating this new dialogue time. GPT-4 generated responses under the following guidelines: 1) responses could not exceed 40 words; 2) if the provided memories' topics significantly differed from the conversation, the response should indicate the speaker’s lack of familiarity with the conversations topic; 3) if the provided memories and conversation topics were only slightly different, the response should reflect the speaker's intention to engage with and explore the conversation topic.
\end{itemize}


\subsection{Memory Annotations}

%\subsection{Constructing Grounding Memory After Dialogue Takes Place}
\vspace{-1mm}
To gain a more precise understanding of the model's capabilities in temporal awareness, we align conversations with memory. For the memory component, we add time annotations as outlined in Section~\ref{section:2.1}. Since the memories of the speakers are sourced from real users on Reddit, we avoid creating fabricated memories to preserve data authenticity. Additionally, we incorporate a ``No Memory'' category into the speaker's memory set. Structured similarly to existing memory triplets (memory description context, memory image, memory time), the ``No Memory'' category is assigned as the description context, indicating that there is no memory to align with the response. \footnote{We also correlate ``No Memory'' with a plain white image as the memory image.} This memory category is used to align early-stage conversations. We then synchronize the memory time with the conversation's time information.

\subsection{Dataset Statistics}


MTPChat comprises 18,973 conversations and 25,877 users. We divided MTPChat into training, validation, and test sets with 15,056, 1,994, and 1,923 conversations respectively. We analyzed the proportion of later stage conversations and early stage conversations, finding a ratio of 3:1. As well as later stage conversations with grounding memories (some later stage conversations lack grounding memory) and early stage conversations with ``No Memory'', resulting in a ratio of 2:1. Furthermore, to gain deeper insight into the time information within MTPChat, we charted the distribution of times across conversations and memories in Fig~\ref{data}.


\begin{figure}[t]
\centering
\includegraphics[width=0.85\columnwidth]{data.png}
\vspace{-2mm}
\caption{Distribution of times across conversations and memories in training, validation, and test set.
}
\vspace{-2mm}
\label{data}
\end{figure}





