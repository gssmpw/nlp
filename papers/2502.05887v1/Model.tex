\begin{figure}[t]
\centering
\includegraphics[width=0.9\columnwidth]{model.png}
\vspace{-2mm}
\caption{Architecture of our framework with Adaptive Temporal Module (ATM).
}
\label{model}
\vspace{-4mm}
\end{figure}
\vspace{-2mm}
\section{Framework}
In this section, we present a framework to perform above retrieval tasks based on dialogue and memory. The inputs include dialogue ${d}_n$, the speaker's response ${r}_n$ to the dialogue and a memory set $\mathcal{M}_n$. We define various encoders to process different modalities of data, fuse the extracted features, and achieve both the temporal next response prediction task and the temporal grounding memory prediction task. The architecture of our framework is shown in Fig~\ref{model}.

\textbf{Text Encoder}
In this study, we employ the text encoder to process textual components within tasks, specifically extracting representations of text and date information from dialogues, memories, and responses. For both dialogue and speaker memories, which may contain multiple entries, we first concatenate the text and date information for each entry. These concatenated strings are then further combined using a delimiter, forming unified representations. This method ensures comprehensive feature extraction by the text encoder, facilitating a more robust analysis of the textual data involved.

\textbf{Vision Encoder}
%\vspace{-1mm}
Similarly, our vision encoder to extract features from images embedded in dialogues and memories. In datasets featuring speaker memories with multiple images, each image is processed by this vision encoder. The extracted features are then aggregated via mean-pool operation to create a consolidated visual representation. This methodology ensures a coherent integration of visual data, significantly enhancing the model's capacity to process multi-image features effectively.

\textbf{Adaptive Temporal Module}
Following the extraction of textual and visual representations, it is essential to effectively integrate these features. As the inclusion of date information into textual representations can impact the correspondence between the text and vision features extracted by text encoder and vision encoder, we propose a method to dynamically balance these modalities to maintain the alignment of text and visual information within the same set of memories and dialogues. We introduce a module called the Adaptive Temporal Module (ATM), which is designed to be both simple and effective.

First, we concatenate the corresponding text and vision features and map them through a linear layer. Subsequently, a sigmoid layer is used to derive the weights for both text and vision features. These weights are then employed to merge the features based on their relevance, ensuring better alignment and integration. This approach allows for a more coherent and contextually appropriate fusion of multimodal features, enhancing the overall interpretative capability of the model.
