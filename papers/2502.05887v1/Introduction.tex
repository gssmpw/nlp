\section{Introduction}
Temporal awareness has garnered significant attention in AI research, particularly following Min et al.’s work \cite{min2020ambigqa}, which highlighted the inherent temporal dynamics in question-answering systems. Understanding time-sensitive information is crucial across various domains, including financial decision-making, event prediction, multimedia content analysis, and conversational AI. To explore temporal reasoning in large language models (LLMs), multiple time-sensitive datasets have been developed. TimeQA \cite{chen2021dataset} and SituatedQA \cite{zhang2021situatedqa} provide temporally grounded questions with free-text contexts extracted from WikiData \cite{vrandevcic2014wikidata}. Similarly, TEMPLAMA \cite{dhingra2022time} builds on temporal knowledge bases, while StreamingQA \cite{liska2022streamingqa} compiles time-sensitive question-answering (QA) data from English news articles spanning 2007 to 2020.
%Research on temporal awareness has attracted considerable interest subsequent to \cite{min2020ambigqa} seminal work, which illuminated the temporal dynamics inherent in answers to questions. This temporal dimension is critical across various domains, such as financial decision-making, event outcomes, multimedia content analysis and perceptions of topics. To explore the temporal awareness of large language models (LLMs), several time-sensitive datasets have been developed for research purposes. Among these, the TimeQA~\cite{chen2021dataset} and SituatedQA~\cite{zhang2021situatedqa} datasets offer time-sensitive questions accompanied by free-text contexts extracted from WikiData~\cite{vrandevcic2014wikidata}. Additionally, the TEMPLAMA dataset~\cite{dhingra2022time} was constructed based on the temporal knowledge base. StreamingQA~\cite{liska2022streamingqa} was compiled from collections of news articles in the English WMT challenges spanning 2007 to 2020.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{intro.png}
\caption{An example of a multimodal, time-sensitive, persona-grounded scenario, showcasing how the user's dialogue responses evolve over time based on the temporal dynamics of dialogue and episodic memories.
}
\vspace{-3mm}
\label{intro}
\end{figure}

\begin{table*}[ht]
\centering
\resizebox{0.95\linewidth}{!}
{
\begin{tabular}{l|ccccc}
\toprule
\textbf{Dataset} & \textbf{Knowledge Corpus} & \textbf{\# Samples} & \textbf{Time-Sensitive} & \textbf{Task} & \textbf{has Images}\\
\midrule
TempLama~\cite{dhingra2022time} & CustomNews & 50.0k & YES & Question Answering & NO\\
TimeQA~\cite{chen2021dataset} & Wikipedia & 41.2k & YES & Question Answering & NO\\
StreamingQA~\cite{liska2022streamingqa} & WMT07-20 & 138.0k & YES & Question Answering & NO\\
TempReason-L2L3~\cite{tan2023towards} & Wikipedia & 49.0k & YES & Question Answering & NO\\

\midrule
PhotoChat~\cite{zang2021photochat} & OpenImage V4& 12.3k & NO & Dialogue & YES \\
MMDialog~\cite{feng2022mmdialog} & SocialMedia& 1.1M & NO & Dialogue & YES\\
\midrule
\textbf{MTPChat} &Reddit & 28.7k & YES & Dialogue & YES \\
\bottomrule
\end{tabular}
}
\vspace{-2mm}
\caption{\label{comparison}
Related datasets overview, including free-text time-sensitive datasets and multimodal dialogue datasets.}
\vspace{-4mm}
\end{table*}

%To explore the temporal awareness of large language models (LLMs), several time-sensitive datasets have been developed for research purposes. Among these, the TimeQA~\cite{chen2021dataset} and SituatedQA~\cite{zhang2021situatedqa} datasets offer time-sensitive questions accompanied by free-text contexts extracted from WikiData~\cite{vrandevcic2014wikidata}. Additionally, the TEMPLAMA dataset~\cite{dhingra2022time} was constructed based on the temporal knowledge base. StreamingQA~\cite{liska2022streamingqa} was compiled from collections of news articles in the English WMT challenges spanning 2007 to 2020.

%Considering temporal aspects in a multimodal dialogue dataset, common in real-world applications, is challenging. However, there is limited work addressing this problem.For previous datasets, firstly, they are confined to the task setting: QA tasks, and secondly, both the questions and contexts being free-text (only linguistic information). A recently proposed time-sensitive multimodal dataset for long video understanding, termed TimeIT~\cite{ren2023timechat}. This dataset, while innovative, presents three primary limitations: 1) its concentration on QA tasks restricts broader application scope; 2) the explicit temporal markers in the videos fail to fully challenge the model's capabilities in temporal sensitivity to implicit temporal cues; and 3) the fixed response format ``<timestamp\_start> to <timestamp\_end> seconds: <event\_description>'' simplifies the task by reducing the requirement for complex temporal reasoning.
However, these datasets primarily focus on text-based QA tasks, limiting their applicability to real-world conversational AI. They lack the multimodal components (e.g., images) that are essential for capturing rich temporal contexts and do not account for persona-grounded dialogues, where responses evolve based on a user's dynamic memory and past interactions. Although TimeIT \cite{ren2023timechat} introduces time-sensitive multimodal tasks for long-video understanding, it has several limitations: (1) its focus on QA tasks restricts broader conversational applications, (2) the use of explicit temporal markers in videos reduces the challenge of reasoning over implicit temporal cues, and (3) its rigid response format  (e.g., ``<timestamp\_start> to <timestamp\_end> seconds: <event\_description>'') simplifies the task, minimizing complex temporal reasoning. Similarly, MPChat~\cite{ahn2023mpchat} provides persona-grounded dialogues with multimodal memory, but it lacks an explicit temporal dimension.

%Addressing the limitations found in current time-related datasets, we introduce MTPChat, an innovative multimodal time-aware persona dialogue dataset. Firstly, This dataset features a comprehensive data structure that integrates linguistic, visual, and temporal elements within its dialogues and persona memories, which directly addresses the limitations of the free-text time-sensitive data formats currently available.  Secondly, MTPChat offers various time-sensitive tasks: Temporal Next Response Prediction (TNRP) and Temporal Grounding Memory Prediction (TGMP). These tasks with temporal dynamic aspect are designed to make models aware of the impact of time and predict varying responses and grounding memories evolve significantly over time. The variety of task settings broadens the scope of research in time-sensitive domains. Thirdly, MTPChat increases the complexity of the dataset by utilizing time as implicit cues. It skillfully employs the time order of dialogues and memories to demonstrate the influence of time on human cognition processes. Fig~\ref{intro} depicts an example in multimodal time-sensitive scenarios.

To overcome these limitations, we introduce MTPChat, a multimodal, time-aware persona dialogue dataset built upon MPChat~\cite{ahn2023mpchat}, a comprehensive multimodal persona-grounded dialogue dataset. Rather than relying on explicit timestamps, MTPChat leverages the natural progression of dialogues and memories to simulate real-world temporal shifts in human cognition. Figure~\ref{intro} illustrates an example of a multimodal time-sensitive scenario. Our dataset integrates linguistic, visual, and temporal elements, making it the first of its kind to model persona-driven temporal changes in dialogues and memory. Unlike existing time-sensitive datasets, MTPChat incorporates dialogue, persona memory, and visual elements, enhancing its realism and complexity. In addition, we propose two novel tasks—Temporal Next Response Prediction (TNRP) and Temporal Grounding Memory Prediction (TGMP)—that challenge models to infer implicit temporal cues and track evolving responses over time. 



%rather than merely describing timestamps and corresponding content. 

%Moreover, based on the tasks presented in MTPChat, we propose a pioneering framework featuring an adaptive temporal module. This framework is designed to augment the model's capacity for integrating linguistic, visual, and temporal elements, thereby facilitating more coherent interconnections among them. Specifically, this adaptive temporal module is used to dynamically merge features based on their relevance, enhancing the coherence and efficacy of the integration.

Beyond dataset creation, we introduce an adaptive temporal module designed to enhance the temporal reasoning capabilities of multimodal models. This framework dynamically integrates linguistic, visual, and temporal streams, allowing for more effective reasoning over time-sensitive interactions. Specifically, the module dynamically merges features based on their temporal relevance, improving coherence in multimodal integration.


%Specifically, this adaptive temporal module dynamically adjusts its parameters to the evolving temporal dynamics of multimodal data streams, ensuring a cohesive integration that enhances overall processing efficacy.



%Finally, we conducted experiments on MTPChat using SBERT~\cite{reimers2019sentence} and CLIP~\cite{radford2021learning} models, which demonstrated that MTPChat poses novel challenges to the model in multimodal time-sensitive scenarios. Furthermore, we compared our framework with other methods of feature integration, proving that our framework can effectively and markedly enhance the model's capabilities in integrating multimodal streams with temporal awareness.

To evaluate MTPChat, we conducted experiments using SBERT \cite{reimers2019sentence} and CLIP \cite{radford2021learning}. Our results demonstrate that MTPChat introduces new challenges in multimodal, time-sensitive scenarios, requiring models to track temporal changes effectively.
Our adaptive temporal module outperforms other feature integration methods, significantly enhancing a model’s ability to reason over multimodal time-aware dialogue.


The main contributions of this work are as follows:
 \begin{itemize}
 
    \item We introduce the first multimodal, time-aware persona dialogue dataset, which contains numerous instances where both dialogue responses and grounding memories evolve significantly over time.
 
    \item We define Temporal Next Response Prediction (TNRP) and Temporal Grounding Memory Prediction (TGMP) to advance research in time-aware conversational AI.

    \item We propose a framework with an adaptive temporal module that enhances a model’s ability to integrate multimodal streams while maintaining temporal awareness.
  
    \item Experimental results validate the novel challenges posed by MTPChat and demonstrate that our framework outperforms existing methods in multimodal temporal reasoning.

\end{itemize}

