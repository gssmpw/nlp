
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{pifont} % For checkmark and xmark
\definecolor{deepgreen}{RGB}{0,200,0}
\definecolor{deepred}{RGB}{130,0,0}
\newcommand{\cmark}{\textcolor{deepgreen}{\ding{51}}} % Define command for checkmark
\newcommand{\xmark}{\textcolor{deepred}{\ding{55}}} % Define command for xmark


\title{AdaManip: Adaptive Articulated Object Manipulation Environments and Policy Learning}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Yuanfei Wang\textsuperscript{* \rm 1}, Xiaojie Zhang\textsuperscript{* \rm 2}, Ruihai Wu\textsuperscript{* \rm 1}, Yu Li\textsuperscript{\rm 2}, Yan Shen\textsuperscript{\rm 1}, Mingdong Wu\textsuperscript{\rm 1},\\
\textbf{Zhaofeng He\textsuperscript{\rm 2}, Yizhou Wang\textsuperscript{\rm 1 3 4 5}, Hao Dong\textsuperscript{\rm 1}} \\
\textsuperscript{1} Center on Frontiers of Computing Studies,
School of  Computer Science, Peking University\\
\textsuperscript{2} Beijing University of Posts and Telecommunications\\
\textsuperscript{3} Inst. for Artificial Intelligence, Peking University \\
\textsuperscript{4} Nat'l Eng. Research Center of Visual Technology, Peking University \\
\textsuperscript{5} State Key Laboratory of General Artificial Intelligence, Peking University
}

% \author{Antiquus S.~Hippocampus, Natalia Cerebro \& Amelie P. Amygdale \thanks{ Use footnote for providing further information
% about author (webpage, alternative address)---\emph{not} for acknowledging
% funding agencies.  Funding acknowledgements go at the end of the paper.} \\
% Department of Computer Science\\
% Cranberry-Lemon University\\
% Pittsburgh, PA 15213, USA \\
% \texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
% \And
% Ji Q. Ren \& Yevgeny LeNet \\
% Department of Computational Neuroscience \\
% University of the Witwatersrand \\
% Joburg, South Africa \\
% \texttt{\{robot,net\}@wits.ac.za} \\
% \AND
% Coauthor \\
% Affiliation \\
% Address \\
% \texttt{email}
% }

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}

\maketitle

\vspace{-0.3cm}

\begin{abstract}

\label{sec:abstract}
Articulated object manipulation is a critical capability for robots to perform various tasks in real-world scenarios.
Composed of multiple parts connected by joints, articulated objects are endowed with diverse functional mechanisms through complex relative motions. 
For example, a safe consists of a door, a handle, and a lock, where the door can only be opened when the latch is unlocked.
The internal structure, such as the state of a lock or joint angle constraints, cannot be directly observed from visual observation. Consequently, successful manipulation of these objects requires adaptive adjustment based on trial and error rather than a one-time visual inference.
However, previous datasets and simulation environments for articulated objects have primarily focused on simple manipulation mechanisms where the complete manipulation process can be inferred from the object's appearance. 
To enhance the diversity and complexity of adaptive manipulation mechanisms, we build a novel articulated object manipulation environment and equip it with 9 categories of objects. 
Based on the environment and objects, we further propose an adaptive demonstration collection and 3D visual diffusion-based imitation learning pipeline that learns the adaptive manipulation policy.
The effectiveness of our designs and proposed method is validated through both simulation and real-world experiments.
Our project page is available at: \href{https://adamanip.github.io}{\texttt{https://adamanip.github.io}}
\let\thefootnote\relax\footnotetext{* indicates equal contribution}
%\let\thefootnote\relax\footnotetext{Correspondence to \texttt{yuanfei\_wang@pku.edu.cn} and \texttt{hao.dong@pku.edu.cn}}

\end{abstract}

\vspace{-0.4cm}
\section{Introduction}
\label{sec:intro}
Among the various categories of objects in our daily life,
articulated objects are highly significant as they are common in our surroundings (such as cabinets, doors, and laptops) and their components are complex, featuring rich and diverse geometries, semantics, articulations, and functions.
Therefore, learning 
articulated object representation~\citep{du2023learning, wei2022self, heppert2023carto, lei2024nap} and manipulation~\citep{xu2022umpnet, wu2022vatmart} are essential while challenging for future robots in home-assistant tasks.

Among articulated object manipulation tasks, door manipulation~\citep{urakami2019doorgym} is first and most thoroughly studied, as doors are most common and useful in our daily lives.
Afterwards,
with the release of diverse articulated object manipulation datasets and environments~\citep{Mo_2019_CVPR, xiang2020sapien, liu2022akb, geng2023gapartnet},
various manipulation tasks (like opening, sliding, rotating, and further language-guided manipulation) on many categories of articulated objects (such as pots, lamps, and cabinets) have been studied.


While previous covered various aspects of articulated object manipulation,
one of the most essential features of articulated objects,
the mechanisms of different parts and articulations for accomplishing the final manipulation goal,
has yet to be explored.
For example,
a safe can be directly opened by pulling the door in previous environments,
while in the real world,
the robot may have to first turn the key to unlock the latch,
and then pull open the door.
While UniDoorManip~\citep{li2024unidoormanip} proposes an environment with the corresponding dataset that can simulate the mechanisms of doors,
the mechanisms of various types of articulated objects could be much more diverse and complicated.
Therefore,
we build an environment that can simulate the above-described complex mechanisms of articulated object manipulation,
equipping this environment with 9 categories of different objects covering 5 types of adaptive mechanisms (details described in Section~\ref{sec:env}).

The different mechanisms of articulated objects call for two core capabilities of the policy: (1) multi-modal action proposal and (2) adaptive manipulation from history actions.
For an observed object, 
the manipulation policy may contain multiple modes, including different manipulation trajectories.
For example,
when observing the safe with its door closed (Bottom-Left in Figure~\ref{fig:fig1}),
to achieve the goal of opening the door,
the policy could be either directly pulling the door (when the door is unlocked) or turning the key and then pulling the door (when the door is locked),
and the method should be able to model these modalities from the same visual observation.

Furthermore,
to identify and execute the accurate action from the multi-modal manipulation action candidates, it is necessary to adapt the manipulation policy based on previous actions and their corresponding outcomes.
For example,
when pulling the door results in no movement, the Adaptive policy should adapt from proposing multi-modal actions (either pulling the door or turning the key) to single-modal action (turning the key) (Bottom-Right in Figure~\ref{fig:fig1}). %In contrast, the Static policy trained on optimal trajectories (Bottom-Middle in Figure~\ref{fig:fig1}) is not able to adapt.

% Therefore,
% successful manipulations with such trials during the manipulation procedures are introduced as the input of the adaptive manipulation policy,
% and the history actions with corresponding results are taken as input to mitigate the multi-modalities of the manipulation policy and adapt it to the accurate one.

\input{fig/fig1}
To support multi-modal action proposals,
we take advantage of the designs of diffusion policy~\citep{Chi2023DiffusionPV} and its following studies~\citep{Ze20243DDP, yan2024dnact, tie2025et-seed}, which have demonstrated modeling multi-modal distributions from only a few successful demonstrations.
To empower the policy with adaptive manipulation abilities,
while previous studies only collect optimal success trajectories (without any failures during the manipulation) for training,
we introduce trajectories including failure actions and the recovery and adaptation actions from failures for training,
as the failure actions help in revealing the accurate mechanisms and manipulation policy.
Figure~\ref{fig:fig1} showcases the superiority of our proposed adaptive policy learning method.
The pure visual observation could not tell whether the door was locked or not.
The static policy that only takes the passive one-frame visual input will randomly propose one of the multi-modal trajectory candidates.
On the contrary,
the adaptive policy will first try pulling the door, and then adapt the policy distribution from multi-modal to single-model accordingly, as its training data include the failure of a direct pulling trial on the locked safe door, and then adaptively turning the key to open the safe after the failure successfully.


Based on our proposed novel environment,
we have conducted extensive experiments on 9 categories of 277 different objects, covering 5 types of mechanisms,
showcasing the necessity of the proposed environment and dataset,
and the effectiveness of the proposed policy learning framework in efficiently and intelligently adapting the manipulation.

In summary,
our contributions include:

\begin{itemize}
    \item We study the novel problem of adaptively manipulating articulated objects with diverse mechanisms and build an environment with various categories of objects and mechanisms.
    \item We propose a novel framework that learns the adaptive manipulation policy for various mechanisms from diverse demonstrations.
    \item Extensive experiments have demonstrated the significance of our proposed environment, and the effectiveness of the proposed adaptive policy learning framework.
\end{itemize}

\vspace{-0.3cm}

\section{Related Work}
\vspace{-0.1cm}

\subsection{Articulated Object Environments and Datasets}
\vspace{-0.2cm}


To facilitate the study of representation and manipulation of diverse and complex articulated objects,
DoorGym~\citep{urakami2019doorgym}, a door manipulation environment is first introduced with diverse doors.
UniDoorManip~\citep{li2024unidoormanip} further empowers door environments with different mechanisms.
PartNet-Mobility dataset first introduces multiple categories of articulated objects from PartNet~\citep{Mo_2019_CVPR, Chang2015ShapeNetAI}, integrated with the sapien environment~\citep{xiang2020sapien, mu2021maniskill, gu2023maniskill2} to support various articulated object manipulation tasks. 
Further, GAPartNet~\citep{geng2023gapartnet} provides fine-grained part annotations, AKB-48~\citep{liu2022akb} provides real-world articulated object models, and Arnold~\citep{gong2023arnold} provides the environment for language-guided manipulation.

\vspace{-0.2cm}
\subsection{Articulated Object Manipulation}
\vspace{-0.2cm}

There have been a series of studies studying articulated object manipulation.
Where2Act~\citep{Mo_2021_ICCV} first studies the point-level affordance for short-term manipulation,
with affordance-based~\citep{wu2022vatmart}, flow-based~\citep{eisner2022flowbot3d, zhang2023flowbot++}, part-based~\citep{geng2023partmanip} and rl-based~\citep{geng2022end} methods study the long-horizon manipulation.
Environment-Aware Affordance~\citep{wu2023learningenv, li2024mobileafford} further studies the manipulation with environment constraints.
Where2Explore~\citep{ning2023where2explore} and AdaAfford~\citep{wang2021adaafford} study converting passive visual priors to manipulation posteriors using the few-shot interactions, which tackle the problem of exploring novel articulated object categories with novel geometries and parts, and manipulation on ambiguous kinematics and dynamics. 
Besides, coarse-to-fine method ~\citep{ling2024articulated} studies the sim2real framework for real-world manipulation, and language-guided methods~\citep{xu2024naturalvlm, gong2023arnold} explore the manipulation with language guidance.
While these works mainly investigated the manipulation with simple mechanisms (such as directly opening a door or safe), in our work,
we further study the policy for manipulating articulated objects with diverse and complex mechanisms, with a novel proposed environment supporting such objects.

\vspace{-0.2cm}

\section{Adaptive Manipulation Environment}
\label{sec:env}
\vspace{-0.2cm}


Previous datasets and simulation environments for articulated objects often lack diversity and realistic manipulation mechanisms~\citep{urakami2019doorgym, li2024unidoormanip, geng2023gapartnet, xiang2020sapien, geng2023partmanip}. 
To address this issue, we developed a new environment to better explore complex mechanisms in articulated object manipulation and learn adaptive manipulation policies. 
Based on IsaacGym~\citep{makoviychuk2021isaac}, this environment simulates these mechanisms and includes 9 categories of objects (Section~\ref{sec:dataset}) with 5 types of adaptive mechanisms (Section~\ref{sec:mechanisms}).

\vspace{-0.1cm}

\subsection{Articulated Object Dataset}
\vspace{-0.1cm}

\label{sec:dataset}


In recent years, several works have proposed datasets for articulated object manipulation. PartNet-Mobility~\citep{xiang2020sapien} and AKB-48~\citep{liu2022akb} offer diverse datasets for articulated objects but focus on cross-category geometry diversity, neglecting the mechanisms of different parts and articulations needed to achieve the final manipulation goal. 
For instance, PartNet-Mobility includes the safe category, but the door can be directly opened without rotating the knob to unlock the latch. GAPartNet~\citep{geng2023gapartnet} provides fine-grained part annotations but still fails to model complex manipulation mechanisms. DoorGym~\citep{urakami2019doorgym} claims a large-scale, scalable dataset specifically for door manipulation, considering the latch mechanism of doors. 
UniDoorManip~\citep{li2024unidoormanip} enriches the diversity of door geometry by composing instances. However, both DoorGym and UniDoorManip are limited to door manipulation and do not cover more diverse and long-term mechanisms.

To investigate real-world articulated object manipulation, we introduce a new dataset that encompasses more realistic adaptive manipulation mechanisms.
Our dataset includes 9 categories of 277 objects: \textbf{Bottle}, \textbf{Pen}, \textbf{Coffee Maker}, \textbf{Window}, \textbf{Pressure Cooker}, \textbf{Lamp}, \textbf{Door}, \textbf{Safe}, and \textbf{Microwave}. Table~\ref{tab:datasets} provides detailed statistics of our dataset, and Figure~\ref{fig:fig2} visualizes instances of each category. The object assets in our dataset are handcrafted from materials primarily obtained from 3D Warehouse~\citep{3dwarehouse}. More details can be found in Appendix~\ref{data_construct}.

\input{tabs/datasets}
\input{tabs/comparison}
\vspace{-0.1cm}
\subsection{Adaptive Manipulation Mechanism}
\vspace{-0.1cm}

\label{sec:mechanisms}

Most existing articulated object environments focus primarily on geometric diversity across different categories of objects. While these objects may contain multiple parts, manipulating one part typically does not impact other parts' state or joint limits, resulting in simplified manipulation mechanisms. Common actions in these environments include pushing or pulling a part, such as opening a drawer or pressing a button, which can be deduced purely from visual observation. However, real-world manipulation often depends on internal joint states that are not visible externally, necessitating adaptive manipulation policies based on feedback.

% To better simulate real-world articulated objects and their manipulation mechanisms, we have identified five types of adaptive mechanisms that enhance the fidelity of our adaptive manipulation environment:

To better simulate real-world articulated object manipulation as well as corresponding mechanisms, we have identified five adaptive mechanisms that enhance the fidelity of our environment:

\textbf{Lock Mechanism}: Common in everyday objects like doors or safes, the lock mechanism requires an initial action such as rotating a key or knob or pressing a button to unlock the object before it can be opened. This mechanism tracks the key part's joint state during manipulation and updates the lock state accordingly. If the lock state transitions to "unlock", the door joint limit is lifted to allow opening; otherwise, the door remains locked. Since the lock state cannot be inferred visually, the robot must interact with the object to determine the lock state and adapt its policy accordingly.

\textbf{Random Rotation Direction}: When rotating a knob, cap, or handle, the direction (\emph{i.e.}, clockwise or counterclockwise) is determined by the internal revolute joint limit. Our environment randomly assigns the rotation direction upon initialization, preventing visual inference of the direction. The robot must attempt one direction and switch if unsuccessful.

\begin{figure}
% \centering
\includegraphics[width=1.0\textwidth]{fig/Figure2.pdf}
\caption{
Adaptive manipulation dataset and environments. Bottle and Pressure Cooker feature the \textbf{Rotate \& Slide} mechanism, requiring continued rotation after a failed lift. The window includes the \textbf{Lock} and \textbf{Random Rotation Direction} mechanisms, necessitating exploration of the correct rotation direction to unlock the latch. Microwave incorporates the \textbf{Lock} and \textbf{Switch Contact} mechanisms, where the robot must first pull the handle to check the lock state and press the button if locked.
}
\vspace{-0.5cm}
\label{fig:fig2}
\end{figure}


\textbf{Rotate \& Slide Mechanism}: This mechanism requires a part to be rotated to a specific angle before it can be lifted or pulled out, such as lifting the lid of a pressure cooker. The required rotation angle is not visually discernible, necessitating the robot to rotate the part incrementally and attempt to slide it to determine if the correct angle is reached. We randomize the revolute joint limit and initially set the prismatic joint limit to zero, lifting it once the correct angle is achieved.

\textbf{Push/Rotate Mechanism}: Due to the similar appearance of buttons and knobs, it is unclear whether the part should be pushed or rotated. For example, a lamp might be turned on by pushing a button in some instances and rotating a knob in others. Our environment includes both revolute and prismatic joints for the same part in the URDF file and randomly determines whether the part should be pushed or rotated, adjusting the joint limit accordingly.

\textbf{Switch Contact Mechanism}: Based on the lock mechanism, this requires the robot to manipulate different key parts, such as a handle and a knob, if they are separate. For instance, in a safe, the robot must switch contact points during manipulation due to the lock state ambiguity, preventing it from determining the sequence of contact points at the outset.

Table~\ref{tab:comparison} compares the richness and diversity of mechanisms between our environment and others. We visualize the adaptive manipulation mechanisms in Figure~\ref{fig:fig2} and Figure~\ref{fig:app_mechanism}. More details can be found in Appendix~\ref{adaseq} and \ref{app_mech}.



\vspace{-0.2cm}
\section{Method}
\vspace{-0.3cm}

As illustrated in Figure~\ref{fig:fig3}, we propose a novel framework that learns an adaptive manipulation policy for various mechanisms from collected adaptive demonstrations. 
% To achieve this, we leverage the annotations of part poses in our dataset to generate expert manipulation trajectories within the proposed environments, considering invisible internal states to ensure the trajectories are adaptive.
To achieve this, we leverage the annotated part poses in our dataset to generate expert manipulation trajectories in the environments, considering invisible internal states to ensure the trajectories are adaptive.
Next, to model the expert trajectory distribution with high multi-modality, we employ 3D visual diffusion-based imitation learning~\citep{Ze20243DDP, Chi2023DiffusionPV}, which learns the gradient of the action score function to generate actions.




\vspace{-0.2cm}
\subsection{Adaptive Demonstration Collection}
\vspace{-0.2cm}

Our goal is to generate adaptive demonstrations that are optimal under partial observation. For instance, when opening a microwave, the expert adaptive policy initially pulls the handle to check the lock state. If the latch is locked, the policy will push the button before opening the door. If the latch is not locked, the policy continues pulling the handle to open the door. In contrast, a static policy with full observation would know the lock state in advance and could directly open the door or push the button without first trying to pull the handle.


We design rule-based expert adaptive policies to gather adaptive demonstrations in our simulation environments. We start by computing the bounding box of the part mesh and annotating the part pose through an interactive script. Using these annotations, we label the sequences of parts and manipulation actions for each category, ensuring optimal trajectories under partial observation. For example, the manipulation sequence for a locked safe is grasping the handle, pulling the door, grasping the knob, rotating the knob, grasping the handle, and opening the door. Note that the recorded trajectories are \textbf{end effector poses} instead of the high-level action labels. More details are provided in Appendix~\ref{demo_collect}. With these policies, we collect adaptive demonstrations of robot motion trajectories for all 9 categories of objects.


\input{fig/fig3}

\vspace{-0.2cm}
\subsection{3D Diffusion-Based Adaptive Policy Learning}
\vspace{-0.2cm}

Given the collected demonstration dataset $D=\{(o_t,a_t)\}$, we aim to learn a policy that models the conditional distribution $P(A_t|O_t,\hat{A}_t)$. Here $A_t$ refers to the predicted action sequence $A_t=(a_t,...,a_{t+T_a})$, where $T_a$ is the action horizon. $O_t$ refers to the observation history, including 3D point clouds and proprioception states, $O_t=(o_{t-T_o},...,o_t)$, where $T_o$ is the history horizon. $\hat{A}_t$ refers to action history, $\hat{A}_t=(a_{t-T_o-1},...,a_{t-1})$.

However, conducting imitation learning on $D$ is challenging due to its multi-modal nature: The ambiguity of the internal states of articulated objects results in multiple successful manipulation trajectories under the same visual observation. 
Thanks to recent progress in diffusion-based methods~\citep{Chi2023DiffusionPV,Ze20243DDP,ke20243d}, we can better fit the multi-modal distribution by learning the action score function.

Following the Diffusion Policy~\citep{Chi2023DiffusionPV}, we utilize DDPM~\citep{ho2020denoising} to estimate the conditional distribution \( P(A_t|O_t,\hat{A}_t) \). The DDPM scheduler performs \( K \) iterations of denoising steps to transform Gaussian noise \( A_t^K \) into a noise-free action \( A_t^0 \). This process adheres to Eq.~\ref{reverse}, where \( \alpha_k \), \( \gamma_k \), and \( \sigma_k \) are parameters determined by the scheduler. This reverse process conditions the action prediction on both observations and previous actions, differing from the vanilla implementation of the diffusion policy.

\begin{equation}
\vspace{-0.2cm}
\label{reverse}
% \label{reverse}
A_t^{k-1} = \alpha_{k}\left(A_t^{k} - \gamma_{k}\epsilon_{\theta}(A_t^{k}, O_t, \hat{A}_t, k)\right) + \mathcal{N}(0, \sigma_{k}^2 I)
\vspace{-0.1cm}
\end{equation}

The noise prediction network \( \epsilon_\theta \) is trained by minimizing the loss function in Eq.~\ref{loss}, which effectively minimizes the variational lower bound of the KL-divergence between the data distribution and the sample distribution drawn from DDPM:

\begin{equation}
\vspace{-0.1cm}
\label{loss}
\mathcal{L}(\theta) = \mathbb{E}_{
    k \sim \mathcal{U}(0, 100), 
   \atop 
   \epsilon^{k} \sim \mathcal{N}(0, \sigma_{k}^2 I)
   }
   \left[ \left\Vert \epsilon^{k} - \epsilon_{\theta}(A_t^0+\epsilon^k, O_t, \hat{A}_t, k) \right\Vert_2^2  \right] 
   \vspace{-0.1cm}
\end{equation}

In practice, the observation \( o_t \) consists of an observed third-view partial point cloud and the robot's proprioception state (\emph{e.g.}, end-effector pose, robot joint angles, and velocities). 
The point cloud is first cropped and downsampled by Farthest Point Sampling (FPS) and then encoded by PointNet++~\citep{qi2017pointnet++}. The action $a_t$ is the next robot end-effector goal pose. 
Notably, we find that employing a 6D rotation representation~\citep{ke20243d} for the end-effector pose action stabilizes the training process. 
During execution, the policy only executes a sub-sequence of the predicted actions, as the adaptive manipulation process requires high-frequency adjustments.

\input{fig/appendix_mechanism}

\begin{figure}
  % \centering
  % \includegraphics{fig/trails.png}
  \includegraphics[width=1.0\textwidth]{fig/result.pdf}
  \caption{\textbf{Manipulation Trajectories Proposed by Our Method and Others.} Our method can sequentially propose stable and accurate adaptive actions, while others have their respective drawbacks.}
  \vspace{-0.2cm}
  \label{fig:sim_result}
\end{figure}

\vspace{-0.2cm}
\section{Experiments}
\vspace{-0.1cm}
\label{sec:exp}

\subsection{Settings and Metric}

We conduct experiments in the category level covering all the 9 object categories, and collect 20 adaptive manipulation demonstrations for each object as the training data.
For evaluation metric, we use success rate of manipulations.
To evaluate what kind of adaptive demonstrations is the most beneficial to adaptive policy learning,
we train adaptive policies on adaptive demonstrations with different numbers of adaptive trials.

% \input{fig/trails}
\subsection{Baselines and Ablation}

To demonstrate the superiority of our proposed method and its components in manipulating articulated objects with different mechanisms,
we compare it with state-of-the-art affordance-based methods, offline imitation learning methods, a sampling-based method, and an ablation of our method.

% current state-of-the-art methods for articulated object manipulation, policy adaption for articulated object manipulation, and an ablated version.



\begin{itemize}
    \item \textbf{VAT-Mart}~\citep{wu2022vatmart}, affordance-based~\citep{zhao2022dualafford, wu2023learningforesightful} method that predicts the open-loop trajectories from the one-frame passive observation.
    \item \textbf{AdaAfford}~\citep{wang2021adaafford}, adaptive method that adjusts the manipulation policy based on history interactions using CVAE~\citep{sohn2015learning}.
    \item \textbf{Sampling}, a planning-based method that samples a macro action from a discrete set at each high-level time step and then plans to the sub-goal pose associated with the selected macro action computed based on the object part pose annotation.
    \item \textbf{ACT}~\citep{zhao2023learning}, an imitation learning method that uses Action Chunking with Transformers and CVAE.
    \item \textbf{DP3}~\citep{Ze20243DDP}, Diffusion Policy extended with 3D visual representations.
    \item \textbf{Ours w/o adaptive}, replace the adaptive demonstration data with static demonstration data that is optimal under full observation, maintaining the diffusion-based imitation learning.

\end{itemize}

\vspace{-0.5cm}

\input{tabs/task_std}

\input{tabs/error_trails}

\subsection{Simulation Results}

Table~\ref{tab:task} and Figure~\ref{fig:sim_result} respectively show the success rates and manipulation trajectories of different methods.
Figure~\ref{fig:app_mechanism} further shows more manipulation trajectories proposed by our method.
Our method outperforms all other methods and demonstrates stable and accurate action trajectories for adaptive manipulation. 
From the visualizations, we can observe that, for \textbf{VAT-Mart}, as an open-loop method,
it is difficult to fit the whole trajectory space and directly predict a manipulation trajectory at a time, and the open-loop method does not support adapting the policy from previously executed actions.
Using the diffusion-based imitation method,
our method can more accurately model the poses of actions using the limited number of data (such advantage is also demonstrated in other diffusion-based imitation studies~\citep{Chi2023DiffusionPV, Ze20243DDP}). 
In contrast,
% the actions proposed by \textbf{AdaAfford} is not so stable due to the capability limit of cVAE.
 \textbf{AdaAfford} suffers from the inferior multi-modal distribution modeling capability of CVAE and the increased training data requirements associated with point-level affordance learning.
The \textbf{Sampling}-based method performs worse than AdaManip and other baselines because its sampling process because its sampling process cannot efficiently leverage priors or posteriors learned from demonstrations. While \textbf{ACT} and \textbf{DP3} outperform affordance-based methods, they fall short of AdaManip due to the absence of an adaptive demonstration collection pipeline. Additionally, \textbf{ACT} is further limited by its lack of the robust multi-modality modeling capability provided by diffusion models.


\textbf{Ours w/o adaptive} is not trained from demonstrations with recovery actions from failure trials, so
the learned policy could not adapt from failure actions to finally achieve the goal. As illustrated in Figure~\ref{fig:sim_result}, when the safe is locked and \textbf{Ours w/o adaptive} attempts to open the door, it fails to switch to rotating the knob and instead continues on the opening trajectory. 

Table~\ref{tab:tails} shows the effects of repeated adaptive trials in adaptive demonstrations for training.
When only using optimal successful actions (\emph{i.e.}, 0 adaptive trials) under full observation,
the model is the same with \textbf{Ours w/o adaptive} and could not have the adaptation capability.
When using more than one adaptive trial at the same object state,
these adaptive trials are redundant and increase the complexity of distributions to model,
while not increasing the scenarios the policy can handle.
Therefore,
the performance decreases when the number of repeated adaptive trials increases from 1. These results validate our demonstration collection design, which limits the manipulation sequence to only one adaptive trial.

\input{tabs/real_table}

\vspace{-0.1cm}
\subsection{Real-World Experiments}
To validate the generalization of our adaptive diffusion policy to real-world scenarios, we conduct experiments on various real-world objects like Pressure Cooker, Microwave, Bottle, and Safe.
For the real-world settings, we employ a Franka Emika Panda Robot Arm as our agent. To capture 3d visual observation, we position an Azure Kinect DK camera adjacent to the robot arm.  Our policy takes the real-time point clouds from the depth camera, the robot state from the robot arm, and previous actions as the input, and generates the corresponding end-effector action in a close-loop fashion. We collected 35 adaptive expert demonstrations for each object by human teleoperation to train the policies, and conducted 10 evaluation trials per object.

Table~\ref{tab:real_success} presents the number of successful executions across four real-world tasks. The results in Figure~\ref{fig:real_result} demonstrate that our adaptive policy can be effectively applied to real-world scenarios. To better illustrate this adaptive behavior, we visualize different trajectories under different object states in Figure~\ref{fig:real_micro}. The red trajectory shows that the robot finds out that the microwave is locked after it failed to pull the door and turned to push the button to unlock the door. Conversely, if the microwave is initially unlocked, the robot will continue to open the door after it grasps the handle, as depicted by the blue trajectory in sub-figure 2 of Figure~\ref{fig:real_micro}. For more visualizations, please see Appendix~\ref{app_mech}.


\input{fig/real_result}
\input{fig/real_micro_new}

\vspace{-0.1cm}
\section{Conclusion}
\label{sec:conclusion}
\vspace{-0.1cm}

We study the problem of adaptive manipulation policy for manipulating articulated objects with diverse and complex mechanisms, build environments with different categories of such objects that support the various manipulation mechanisms, and propose a novel framework that learns the adaptive manipulation policy for various mechanisms from diverse adaptive demonstrations based on diffusion policy.
The significance of our proposed environment and the effectiveness of the proposed adaptive policy learning framework have been demonstrated by our experiments.

Our paper represents the initial study into the environment suitable for adaptive manipulation policy learning. Currently, our dataset encompasses 9 categories with a total of 277 objects, which we plan to expand by introducing more categories and instances to cover increasingly complex and realistic mechanisms. Additionally, incorporating deformable object manipulation into adaptive tasks represents a significant direction for future research. Moving forward, the AdaManip environment will progressively include a broader spectrum of real adaptive manipulation tasks, making it a comprehensive platform for both training and testing adaptive manipulation policies.
% An adaptive policy that 

% \textbf{Broader Impact.}
% This work shows great potential to robots in various indoor manipulation scenarios. No negative potential impacts have emerged.

\section*{Acknowledgements}
This project is supported by the National Science and Technology Major Project (2022ZD0114904), the National Natural Science Foundation of China (No. 62376006), the
National Youth Talent Support Program (8200800081), the National Natural Science Foundation of China (No. 62136001) and NSFC-6247070125.

\bibliography{reference}
\bibliographystyle{iclr2025_conference}

\newpage
\appendix

\section{Hyper-parameters for experiments}
\label{hyperparam}
Table~\ref{tab:hyper} summarizes the hyperparameters for our diffusion policy model and training details.
\input{tabs/hyperparams}
% \input{fig/appendix_mechanism}


\section{Dataset Construction}
\label{data_construct}
Apart from object assets collected from existing datasets~\citep{xiang2020sapien, li2024unidoormanip}, most of the object assets in our dataset are obtained from 3Dwarehouse~\citep{3dwarehouse}. We dedicate significant time and effort to carefully selecting the available object meshes, segmenting them into distinct parts, re-aligning the object mesh coordinate systems, and subsequently developing Python scripts to facilitate the efficient synthesis of operational dataset instances. Similar to GAPartNet~\citep{geng2023gapartnet}, we annotate the object assets with rich and comprehensive labels.


\section{Detailed Expert Policy Design for Adaptive Demonstration Collection}
\label{demo_collect}
\subsection{Pose Annotation}
To achieve precise part operation, we employ a pose labeling approach for the parts. Initially, we roughly estimate the part pose through the bounding box of the part mesh calculated using the Python library trimesh. Subsequently, we observe the pose annotation results in real-time through an online interactive script to complete the precise annotation of the pose.

\subsection{Adaptive Manipulation Sequence}
\label{adaseq}
\textbf{Bottle:} Grasp the Cap. Rotate Cap after a failed Lift Up. Randomly sample Rotate/Lift if the previous action is Rotate.

\textbf{Pen:} Same as Bottle.

\textbf{Pressure Cooker:} Grasp the Handle. Rotate Handle after a failed Lift Up. Randomly sample Rotate/Lift if the previous action is Rotate.

\textbf{Coffee Maker:} Grasp Portafilter. Rotate Portafilter after a failed Pull Down. Randomly sample Rotate/Pull if the previous action is Rotate.

\textbf{Window:} Grasp the Handle. Randomly choose a direction to Rotate Handle. If failed, choose the other direction. Rotate Handle after a failed open trial. Randomly sample Rotate/Open if the previous action is Rotate.

\textbf{Door:} Same as Window.

\textbf{Lamp:} Randomly choose to Push/Clockwise Rotate/Counter Clockwise Rotate. Never choose a failed action.

\textbf{Safe:} Pull Door. If succeed, then continue opening the door. If failed, Rotate Knob. Randomly choose a direction to Rotate and choose the other one if failed. Then Pull Door again to open it.

\textbf{Microwave:} Pull Door. If succeed, then continue opening the door. If failed, Push Button. Then Pull Door again to open it.

\subsection{Trajectory Sparsification}

If the history consists of a dense trajectory of the robot's end effector poses, the policy would require a long history context length to capture previous failures. However, training a policy with a long history context is challenging, as it requires more computing resources and is less robust. To mitigate this issue, we chose to sparsify the trajectory. Only key frames of the demonstration trajectories are saved for imitation learning, but the recorded actions are still 6D end effector poses instead of high-level macro actions.

For example, in the open-safe task, the recorded history includes grasping poses and several manipulation poses while omitting most intermediate steps. The history condition for the policy is as follows: [grasp the handle, pull the door and fail to open]. Other intermediate poses during execution are excluded from the history. Based on this context, the robot predicts the next goal pose [unlock the key]. Once the goal pose is predicted, we apply inverse kinematics (IK) to plan the path for execution.



\input{fig/real_other_new}


\section{Mechanism Visualization and Experimental Results}
\label{app_mech}
Figure~\ref{fig:app_mechanism} illustrates the operating mechanism of the remaining categories and the experimental results. The mechanism of the lamp in the upper left corner is \textbf{"Push/Rotate"}. which can be operated by pressing/rotating the button/knob, but only one way is correct. The pen and coffee machine in the upper right corner fall under the \textbf{"Rotate/Slide"}. To operate them, you need to rotate a specific part to a certain angle to open the pen cap or the coffee machine handle. The door at the bottom belongs to the \textbf{"Random Rotation Direction"} and \textbf{"Lock"}, where the handle must be rotated either clockwise or counterclockwise to open the door.

Figure~\ref{fig:real_other} shows the visualizations of the other 3 experiments in real world apart from Open Microwave shown in Figure~\ref{fig:real_micro}. In these visualizations, the red trajectories represent the paths actually executed by the robot, while the blue trajectories indicate potential paths under different object states. For instance, in Sub-figure 3 of the Open Bottle experiment, the blue trajectory shows that if the cap is rotated sufficiently, the robot can then lift the cap. This visualization helps illustrate the adaptability of the robot's actions based on the observed state of the objects involved.

\end{document}
