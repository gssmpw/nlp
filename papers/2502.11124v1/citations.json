[
  {
    "index": 0,
    "papers": [
      {
        "key": "urakami2019doorgym",
        "author": "Urakami, Yusuke and Hodgkinson, Alec and Carlin, Casey and Leu, Randall and Rigazio, Luca and Abbeel, Pieter",
        "title": "Doorgym: A scalable door opening environment and baseline agent"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "li2024unidoormanip",
        "author": "Li, Yu and Zhang, Xiaojie and Wu, Ruihai and Zhang, Zilong and Geng, Yiran and Dong, Hao and He, Zhaofeng",
        "title": "UniDoorManip: Learning Universal Door Manipulation Policy Over Large-scale and Diverse Door Manipulation Environments"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "Mo_2019_CVPR",
        "author": "Mo, Kaichun and Zhu, Shilin and Chang, Angel X. and Yi, Li and Tripathi, Subarna and Guibas, Leonidas J. and Su, Hao",
        "title": "{PartNet}: A Large-Scale Benchmark for Fine-Grained and Hierarchical Part-Level {3D} Object Understanding"
      },
      {
        "key": "Chang2015ShapeNetAI",
        "author": "Angel X. Chang and Thomas A. Funkhouser and Leonidas J. Guibas and Pat Hanrahan and Qi-Xing Huang and Zimo Li and Silvio Savarese and Manolis Savva and Shuran Song and Hao Su and Jianxiong Xiao and L. Yi and Fisher Yu",
        "title": "ShapeNet: An Information-Rich 3D Model Repository"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "xiang2020sapien",
        "author": "Xiang, Fanbo and Qin, Yuzhe and Mo, Kaichun and Xia, Yikuan and Zhu, Hao and Liu, Fangchen and Liu, Minghua and Jiang, Hanxiao and Yuan, Yifu and Wang, He and others",
        "title": "Sapien: A simulated part-based interactive environment"
      },
      {
        "key": "mu2021maniskill",
        "author": "Mu, Tongzhou and Ling, Zhan and Xiang, Fanbo and Yang, Derek Cathera and Li, Xuanlin and Tao, Stone and Huang, Zhiao and Jia, Zhiwei and Su, Hao",
        "title": "ManiSkill: Generalizable Manipulation Skill Benchmark with Large-Scale Demonstrations"
      },
      {
        "key": "gu2023maniskill2",
        "author": "Gu, Jiayuan and Xiang, Fanbo and Li, Xuanlin and Ling, Zhan and Liu, Xiqiang and Mu, Tongzhou and Tang, Yihe and Tao, Stone and Wei, Xinyue and Yao, Yunchao and others",
        "title": "Maniskill2: A unified benchmark for generalizable manipulation skills"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "geng2023gapartnet",
        "author": "Geng, Haoran and Xu, Helin and Zhao, Chengyang and Xu, Chao and Yi, Li and Huang, Siyuan and Wang, He",
        "title": "Gapartnet: Cross-category domain-generalizable object perception and manipulation via generalizable and actionable parts"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "liu2022akb",
        "author": "Liu, Liu and Xu, Wenqiang and Fu, Haoyuan and Qian, Sucheng and Yu, Qiaojun and Han, Yang and Lu, Cewu",
        "title": "Akb-48: A real-world articulated object knowledge base"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "gong2023arnold",
        "author": "Gong, Ran and Huang, Jiangyong and Zhao, Yizhou and Geng, Haoran and Gao, Xiaofeng and Wu, Qingyang and Ai, Wensi and Zhou, Ziheng and Terzopoulos, Demetri and Zhu, Song-Chun and others",
        "title": "ARNOLD: A Benchmark for Language-Grounded Task Learning With Continuous States in Realistic 3D Scenes"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "Mo_2021_ICCV",
        "author": "Mo, Kaichun and Guibas, Leonidas J. and Mukadam, Mustafa and Gupta, Abhinav and Tulsiani, Shubham",
        "title": "Where2Act: From Pixels to Actions for Articulated 3D Objects"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "wu2022vatmart",
        "author": "Ruihai Wu and Yan Zhao and Kaichun Mo and Zizheng Guo and Yian Wang and Tianhao Wu and Qingnan Fan and Xuelin Chen and Leonidas Guibas and Hao Dong",
        "title": "{VAT}-Mart: Learning Visual Action Trajectory Proposals for Manipulating 3D {ART}iculated Objects"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "eisner2022flowbot3d",
        "author": "Eisner, Ben and Zhang, Harry and Held, David",
        "title": "Flowbot3d: Learning 3d articulation flow to manipulate articulated objects"
      },
      {
        "key": "zhang2023flowbot++",
        "author": "Zhang, Harry and Eisner, Ben and Held, David",
        "title": "Flowbot++: Learning generalized articulated objects manipulation via articulation projection"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "geng2023partmanip",
        "author": "Geng, Haoran and Li, Ziming and Geng, Yiran and Chen, Jiayi and Dong, Hao and Wang, He",
        "title": "Partmanip: Learning cross-category generalizable part manipulation policy from point cloud observations"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "geng2022end",
        "author": "Geng, Yiran and An, Boshi and Geng, Haoran and Chen, Yuanpei and Yang, Yaodong and Dong, Hao",
        "title": "End-to-End Affordance Learning for Robotic Manipulation"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "wu2023learningenv",
        "author": "Ruihai Wu and Kai Cheng and Yan Zhao and Chuanruo Ning and Guanqi Zhan and Hao Dong",
        "title": "Learning Environment-Aware Affordance for 3D Articulated Object Manipulation under Occlusions"
      },
      {
        "key": "li2024mobileafford",
        "author": "Yu Li and Kai Cheng and Ruihai Wu and Yan Shen and Kaichen Zhou and Hao Dong",
        "title": "MobileAfford: Mobile Robotic Manipulation through Differentiable Affordance Learning"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "ning2023where2explore",
        "author": "Ning, Chuanruo and Wu, Ruihai and Lu, Haoran and Mo, Kaichun and Dong, Hao",
        "title": "Where2Explore: Few-shot Affordance Learning for Unseen Novel Categories of Articulated Objects"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "wang2021adaafford",
        "author": "Wang, Yian and Wu, Ruihai and Mo, Kaichun and Ke, Jiaqi and Fan, Qingnan and Guibas, Leonidas and Dong, Hao",
        "title": "AdaAfford: Learning to Adapt Manipulation Affordance for 3D Articulated Objects via Few-shot Interactions"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "ling2024articulated",
        "author": "Ling, Suhan and Wang, Yian and Wu, Shiguang and Zhuang, Yuzheng and Xu, Tianyi and Li, Yu and Liu, Chang and Dong, Hao",
        "title": "Articulated Object Manipulation with Coarse-to-fine Affordance for Mitigating the Effect of Point Cloud Noise"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "xu2024naturalvlm",
        "author": "Xu, Ran and Shen, Yan and Li, Xiaoqi and Wu, Ruihai and Dong, Hao",
        "title": "NaturalVLM: Leveraging Fine-grained Natural Language for Affordance-Guided Visual Manipulation"
      },
      {
        "key": "gong2023arnold",
        "author": "Gong, Ran and Huang, Jiangyong and Zhao, Yizhou and Geng, Haoran and Gao, Xiaofeng and Wu, Qingyang and Ai, Wensi and Zhou, Ziheng and Terzopoulos, Demetri and Zhu, Song-Chun and others",
        "title": "ARNOLD: A Benchmark for Language-Grounded Task Learning With Continuous States in Realistic 3D Scenes"
      }
    ]
  }
]