
\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} 
\usepackage{hyperref}

\newcommand{\theHalgorithm}{\arabic{algorithm}}


\usepackage[arxiv]{icml2025}


\input{preamble}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage[capitalize,noabbrev]{cleveref}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage[textsize=tiny]{todonotes}


\icmltitlerunning{Mean-Shift Distillation for Diffusion Mode Seeking}

\begin{document}

\twocolumn[
\icmltitle{Mean-Shift Distillation for Diffusion Mode Seeking}


\begin{icmlauthorlist}
\icmlauthor{Vikas Thamizharasan}{umass,adobe}
\icmlauthor{Nikitas Chatzis}{ntua}
\icmlauthor{Iliyan Georgiev}{adobe}
\icmlauthor{Matthew Fisher}{adobe}
\icmlauthor{Difan Liu}{adobe}
\icmlauthor{Nanxuan Zhao}{adobe}
\icmlauthor{Evangelos Kalogerakis}{umass,crete}
\icmlauthor{Michal Lukáč}{adobe}

\end{icmlauthorlist}

\icmlaffiliation{umass}{University of Massachusetts, Amherst}
\icmlaffiliation{adobe}{Adobe Research
}
\icmlaffiliation{ntua}{National Technical University of Athens}
\icmlaffiliation{crete}{TU Crete
}

\icmlcorrespondingauthor{Vikas Thamizharasan}{vthamizharas@umass.edu}

\icmlkeywords{Diffusion Models, Score-distillation sampling, Mode Seeking, Mean-Shift, Score-based Generative Models}

\vskip 0.3in


    
]

\printAffiliationsAndNotice{}  

\begin{abstract}
We present \emph{mean-shift distillation}, a novel diffusion distillation technique that provides a provably good proxy for the gradient of the diffusion output distribution. This is derived directly from mean-shift mode seeking on the distribution, and we show that its extrema are aligned with the modes. We further derive an efficient product distribution sampling procedure to evaluate the gradient.

Our method is formulated as a drop-in replacement for score distillation sampling (SDS), requiring neither model retraining nor extensive modification of the sampling procedure. We show that it exhibits superior mode alignment as well as improved convergence in both synthetic and practical setups, yielding higher-fidelity results when applied to both text-to-image and text-to-3D applications with Stable Diffusion.
\end{abstract}

\section{Introduction}
\label{sec:intro}

Soon after image diffusion~\cite{dhariwal2021diffusion} models exploded in popularity, \citet{poole2022dreamfusion} introduced the idea of using them for image optimization. Intuitively, this can be expressed as the notion that images more likely to be generated by a diffusion model are ``better'' in the sense of being more faithful to the data distribution the diffusion model was trained on.

Formally, diffusion models provide a mechanism to sample images $x \in \mathcal{I}$ from some learned distribution $p(x)$. We then have a parameter vector $\vartheta \in \mathcal{P}$, along with an image-generating model $g: \mathcal{P} \rightarrow \mathcal{I}$. Given an initialization $\vartheta^0$, we seek to optimize a $\vartheta^k$ such that $p(g(\vartheta^k)) > p(g(\vartheta^0))$. We expect this to yield an image $g(\vartheta^k)$ of higher quality, under the metric the diffusion model is trained for.

\vspace{2mm}

We could imagine optimizing $\vartheta$ by determining the gradient $\nabla p(g(\vartheta))$ and ascending along it. However, while we may use our diffusion model to sample from $p(x)$, we can neither easily evaluate $p(x)$ nor determine its gradient $\nabla_x p(x)$. Even though we can formally express $p(x)$ in terms of the score function $\epsilon(x,t)$ through the instantaneous change of variable formula~\cite{grathwohl2018scalable}, evaluating this formula requires calculating the divergence of the score function along the entire ODE path, making this of only theoretical interest. Evaluating the gradient of this quantity is even less practical.

Score distillation sampling (SDS) \cite{poole2022dreamfusion} attempts to address this problem by offering proxies for the density gradient that are easier to estimate. However, their theoretical properties are not rigorously established, and SDS suffers from significant bias as well as variance, yielding inaccurate gradients. Examining the loss landscape of SDS in \cref{fig:fractal_2D_part1}, we indeed see that not only are the maxima of this function not collocated with the modes of $p(x)$, but even in the simplest cases the loss creates ``phantom modes'' that are well out of distribution.

Our method offers both better alignment with the distribution and lower variance of the gradient estimate.

\paragraph{Contributions.}

In this paper, we propose \emph{mean-shift distillation}, a distribution-gradient proxy based on a well-known mode-seeking technique. Furthermore, we show that:
\begin{itemize}
    \item This proxy can be implemented easily, with minimal changes to the diffusion sampling procedure;
    \item It evaluates with less variance than SDS with improved mode alignment;
    \item It has superior behavior, converging to modes of the trained distribution with a clear termination criterion.
\end{itemize}

\section{Related Work}
\label{sec:related}

\paragraph{Denoising diffusion.}

In our work we rely most directly on the mean-shift method of mode seeking~\cite{Cheng98,ComaniciuM02Meanshift}, but our ability to apply it to diffusion rests on a body of theoretical analysis of this process.

Mathematically, denoising diffusion consists of solving an initial value problem (IVP) on a random variable from a simple, typically standard normal distribution, where the time-dependent gradient is learned by reversing the process of adding noise to the distribution being modeled~\cite{song2021scorebased,song2021denoising}. Already in these works authors suggest ways in which the output distribution may be manipulated by adding terms to the differential equation underlying the initial value problem, a property we will rely on to manipulate the output to our method's advantage.

A surprising connection between mean shift and diffusion emerged from the analysis of the optimal denoising model~\cite{Karras2022edm,chen2024a}. Since the forward (noising) process can be expressed as successive convolutions with a Gaussian kernel, the intermediate distributions are in fact Gaussian-kernel density estimates of the data distributions, with kernel bandwidth proportional to the time parameter. Therefore in the ODE of the reverse (inference) process, the gradient of the denoiser is theoretically equal to the mean-shift vector with appropriate kernel and bandwidth. Mean-shifting on the IVP time domain does not in fact seek modes of the output distribution, but we take advantage of this knowledge to implement mean shift on that domain.

Further related to the analysis of modes in particular, \citet{karras2024guiding, bradley2024classifierfreeguidancepredictorcorrector} suggest that applying classifier-free guidance (CFG)~\cite{ho2021classifierfree} to diffusion has the effect of sharpening the modes of the output distribution. This guidance does not explicitly \emph{seek} modes, but we have found that using CFG synergizes well with both SDS and our methodß.

\paragraph{Distilling diffusion priors.}

Score distillation sampling (SDS) \cite{poole2022dreamfusion,sjc} has emerged as a useful technique for leveraging the priors learned by large-scale image models beyond 2D raster images. SDS provides an optimization procedure to estimate the parameters of a differentiable image generator, such that the rendered image is pushed towards a higher-probability region of a pre-trained prompt-conditioned image diffusion model. Originally proposed to optimize volumetric representations like NeRFs, it has been extended to other non-pixel-based representations \cite{jain2023vectorfusion,yi2023gaussiandreamer,bah20244dfy,Thamizharasan_2024_CVPR}.

The tendency of SDS to produce over-smoothened results due to high variance is well documented. A plethora of works have been proposed to mitigate this behavior, e.g.\ to factorize the gradient to reduce the bias \cite{hertz2023delta,yu2024texttod,katzir2024noisefree,alldieck2024scoredistillationsamplinglearned}, or
to replace the uniform noise sampling in SDS with noise obtained by running DDIM inversion \citet{EnVision2023luciddreamer,lukoianov2024score}. \citet{wang2023steindreamer} propose a control variate for SDS, \citet{wang2023prolificdreamer, xu2024diversescoredistillation,yan2025consistentflowdistillation} improve diversity of generations, and \citet{wang2024esd} alleviate the multi-view inconsistency problem.


\input{figures/fractal_2D_part1}

\section{Mean-Shift Distillation}
\label{sec:method}

In this section we derive the mean-shift vector for the diffusion output distribution, and show how it approximates the gradient thereof. We further show how an efficient estimate of this vector may be obtained with a minimal modification of diffusion sampling. We begin with a motivation of our development by illustrating the pitfalls of SDS.

\subsection{Motivation}
\label{sec:motivation}

Given a pre-trained diffusion model $\epsilon_\phi$, the SDS loss penalizes the KL-divergence of a unimodal Gaussian distribution centered around $x$ and the data distribution $p_\phi(z_t; y, t)$ captured by the frozen diffusion model conditioned on text embeddings $y$. With $x=g(\vartheta)$, an image rendered by $\vartheta$ via a differentiable renderer $g$, \citet{poole2022dreamfusion} derive the gradient of the loss $\mathcal{L}_\mathrm{SDS}$ with respect to $\vartheta$:
\begin{multline}
    \label{eq:vanilla_sds}
        \nabla_{\vartheta} \mathcal{L}_\mathrm{SDS} = \mathbb{E}_{
        t, \epsilon
        } \Big[
        \alpha(t)
        \left(
         \epsilon_{\phi}(\alpha(t)x+\epsilon;t) - \epsilon 
         \right)
         \Big] \frac{\partial x}{\partial \vartheta},\\[2mm]
         \mathrm{with}\;t\sim U(0,T),\epsilon \sim \mathcal{N}(0,\sigma(t)I).
\end{multline}
To illustrate the pitfalls of SDS, we simulate it in 2D using a small denoising diffusion network (\cref{fig:fractal_2D_part1}). This allows us to set $\vartheta = x \in \sR^2$ (where $g$ becomes an identity map). 
We construct a fractal-like dataset as shown by \citet{karras2024guiding}, with analytic ground-truth probability density and score. 
This data distribution is a mixture of highly anisotropic Gaussians, where most of the probability mass resides in narrow regions, emulating the low intrinsic dimensionality of natural images \cite{Roweis2000, MikhailLaplace}. 
For a baseline, we compare it with DDIM \cite{song2021denoising}, a popular first-order sampling algorithm, with classifier-free guidance (CFG) \cite{ho2021classifierfree}. 
More details can be found in~\cref{sec:toydist_subsec}.

It is immediately apparent how even in this simple setting, the optima to which SDS converges do not model the output distribution well. Furthermore, the convergence itself is problematic due to very high variance of SDS, which we will address later.

\subsection{Mean-shift Gradient Approximation}
\label{sec:mean_shift}

\newcommand{\bandwidth}{\lambda}
\newcommand{\kernel}{{K_{\bandwidth}}}
\newcommand{\gaussian}{{G_{\bandwidth}}}
\renewcommand{\ps}{p^{\ast}_{\bandwidth}}
\newcommand{\psd}{\dot{p}_{\bandwidth}}

We start by convolving the data density $p$ with a radial Gaussian kernel $\gaussian(x)=c_\bandwidth e^{-x^2/\bandwidth^2}$ with bandwidth $\bandwidth$, normalized by a constant $c_\bandwidth$. This convolution yields a smoothed density $\ps(x)$:
\begin{equation}
\label{eq:gradient_smoothed_density}
    \ps(x) = p \ast \gaussian (x) = \int \gaussian(x-y) p(y) dy.
\end{equation}
We now take the gradient $\nabla_x \ps(x)$ of the smoothed density and substitute the Gaussian kernel's gradient:
\begin{align}
    \label{eq:distro_convolution}
    \nabla_x \ps(x)
        &= \int \nabla_x\gaussian(x-y) p(y) dy \\
        &= \int c_\bandwidth (x - y) \gaussian(x-y) p(y) dy.
\end{align}
We then take the stationary-point equation and reorganize it as a fixed-point iteration:
\begin{equation}
    \label{eq:fixed_point_iteration}
    \!\!\!\nabla_x \ps\!(x) = 0
    \;\;\Longrightarrow\;\;
    x' \!= \frac{\int y \gaussian(x-y) p(y) dy}{\int \gaussian(x-y) p(y) dy},\!
\end{equation}
where the constant $c_\bandwidth$ cancels out. We will discuss the practical estimation of the integrals in \cref{sec:product_sampling} below.

The iterative process in \cref{eq:fixed_point_iteration} is a continuous version of mean shift~\cite{ComaniciuM02Meanshift}. 
We may turn this into gradient proxy with several desirable properties. Defining the mean-shift vector $\vec{m}(x) = x' - x$, it follows from \cref{eq:distro_convolution} that
\begin{equation}
    \vec{m}(x) \, \propto \, \ps (x) \nabla_x \ps (x).
\end{equation}
Since the smoothed density $\ps(x)$ is always non-negative, $\vec{m}(x)$ is always aligned with its gradient $\nabla \ps (x)$. It is also aligned with the gradient of the true density $p$ as $\bandwidth \rightarrow 0$ (when such gradient exists). This means that a differential step along the vector $\vec{m}(x)$ will improve the likelihood of $x$, making this a good proxy for the kernel density estimation gradient. Furthermore, it implies that $\vec{m}(x)$ will be zero at the modes of $\ps (x)$, giving us a convergence criterion.

\subsection{Gradient Estimation via Product Sampling}
\label{sec:product_sampling}

The integrals in \cref{eq:fixed_point_iteration} can be both estimated using samples $y$ from the density $p$; such estimation yields the classical mean-shift expression
\begin{equation}
    \label{eq:mean_shift}
    x' = \frac{\sum_{y_i \sim p} K_\lambda (x - y_i) y_i}{\sum_{y_i \sim p} K_\lambda (x - y_i)}.
\end{equation}
In our case, we do not have such samples readily available. We could in theory use images from the training dataset as these samples, or else use the diffusion model to generate them -- either as a pre-process or on-the-fly during iteration. Unfortunately, that would be prohibitively costly as the datasets are typically quite large and accurate estimation would require a very large number of samples for practical (i.e.\ small) kernel bandwidths $\bandwidth$.

Our key insight is that the right-hand side of \cref{eq:fixed_point_iteration} can be viewed as an expectation with respect to a density $\psd$ that is the product of $p$ and the kernel $\gaussian$ centered at $x$:
\begin{equation}
    \label{eq:fixed_point_iteration_expectation}
    x' = \int y \, \psd(y|x) dy \,=\, \mathbb{E}_{y\sim \psd(y|x)}[y].
\end{equation}
To generate samples $y$ from this product density, we exploit the fact that diffusion models employ score-based sampling \cite{song2021scorebased, dhariwal2021diffusion}. Instead of using the score $\nabla\!\log p$ of the density $p$ in DDIM sampling, we use the score of our product density:
\begin{align}
            \nonumber
        \nabla_{\!z_t}\!\log(\psd\!(y|x))
            &= \nabla_{\!z_t}\!\log p(z_t) + \nabla_{\!z_t}\!\log \gaussian(x - z_t) \\
            &= \nabla_{\!z_t}\!\log p(z_t) - \frac{x - z_t}{\bandwidth^2},
        \label{eq:ms_guidance}
\end{align}
which is the sum of the density score (provided by the diffusion model) and the score of our Gaussian kernel.

Having the ability to generate samples $y_i$ from the product density, we can estimate the mean-shift iterate (\ref{eq:fixed_point_iteration_expectation}) as
\begin{equation}
    \label{eq:mc_step_estimate}
    x' \approx \frac{1}{N} \sum_{y_i \sim \psd(y|x)} y_i.
\end{equation}
In practice we use a single sample $y$, which simplifies our mean-shift vector to
\begin{equation}
\label{eq:mean_shift_vector}
    \vec{m}(x) = y - x.
\end{equation}
We can thus step along $\vec{m}$ to seek the modes of the data density~$p$. Substituting a learned score model into \cref{eq:ms_guidance} gives us
\begin{equation}
\label{eq:kernel_guided_score}
   \hat\epsilon_t = \epsilon_\theta(z_t;t) - \frac{x-z_t}{\lambda^2}.
\end{equation}

\input{figures/fractal_2D_part2}

\subsection{Practical Considerations}

\paragraph{Impact of guidance.}

Conditional score estimates from diffusion models, $\epsilon_\theta (z_t, c) \approx - \sigma_t \nabla_{z_t} \log p(z_t | c)$, are improved in practice with classifier-free guidance (CFG) \cite{ho2021classifierfree}, which sharpens the distribution around the modes:
\begin{equation}
    \label{eq:cfg_update}
    \tilde{\epsilon}_\theta (z_t, c) = (1 + w) \epsilon_\theta (z_t, c) - w \epsilon_\theta (z_t).
\end{equation}
We may directly substitute this for the denoiser term in \cref{eq:kernel_guided_score}. 
Despite its practical success, the denoising direction induced by CFG does not provide theoretical guarantees in producing samples from $p_{0,w}(z_t|c)$ \cite{bradley2024classifierfreeguidancepredictorcorrector}. 
Even in simple settings, as observed in \cref{fig:fractal_2D_part1}(b), CFG can lead to mode drops. While alternative guidance strategy exists \cite{karras2024guiding}, we stick with the dominant practice of using CFG (\cref{eq:cfg_update}).
We have found that this synergizes well with mode-seeking by mean-shift, and show the effects of this in evaluation below. 

\paragraph{Integrating kernel score.}
\label{sec:integrating}

Because the magnitude of the kernel term in \cref{eq:kernel_guided_score} can be quite high when \mbox{$|y-z_t|$} is high relative to $\lambda$, directly implementing this can result in instability while denoising particularly with explicit integrators. Higher-order integrators are generally capable of dealing with this instability, but require many more score function evaluations. 

To address this, we note that in isolation the kernel term has the form of a negative exponential centered on $y$, or explicitly:
\begin{equation}
    z_{t+\Delta t}=y + (z_t - y) e^{\frac{\Delta t}{\lambda^2}},
\end{equation}
where $\Delta t$ is negative. We take advantage of this to formulate a stable approximation that avoids the stability issues with a minimal change to the integration process. Instead of feeding the full composite score function to the integrator, in each time step we first integrate only the score function with the existing integrator to get ${z'}_{t+\Delta t}$. Immediately after, we separately account for the kernel term by computing the final output as
\begin{equation}
    \label{eq:integratingkernel}
    z_{t+\Delta t}=y+({z'}_{t+\Delta t} - y) e^{\frac{\Delta t}{\lambda^2}}.
\end{equation}
We note such numerically instability has been observed when using high CFG values. A remedy is to apply guidance in a limited interval \cite{kynkaanniemi2024applying}. We leverage similar ad-hoc tricks by applying the kernel term in limited interval through the sampling chain.











\section{Practical Implementation and Evaluation}

In this section, we construct synthetic examples on which we demonstrate that our proposed method behaves as theory predicts, alleviating the issues SDS exhibits even in these simple scenarios. We further explain the issues encountered when translating this theory into practice, and describe adaptations we designed to make our method work with real-world diffusion models. Finally, we evaluate that in these real-world scenarios, we retain the desirable properties and performance of our method. 
When applicable, we make comparisons with SDI \cite{lukoianov2024score}, a method that improves upon SDS by proposing a better noise term, yet, retaining the same gradient computation. 

\subsection{Idealized Setting}
\label{sec:ideal}

In order to manage large data dimensionality as well as massive training datasets, diffusion in practice employs a trained neural network to represent the denoiser $D$. However, \citet{Karras2022edm} have identified an analytical solution to minimizing the denoiser error, the \emph{ideal denoiser}
$D^*(x;t)$:
\begin{equation}
\label{eq:idealdenoiser}
    D^*(x;t)=
    \frac{\sum_i y_i \mathcal{N}(x;y_i,\sigma(t))}
    {\sum_i \mathcal{N}(x;y_i,\sigma(t))},
\end{equation}
where $y_0 \dots y_n$ are samples in our training set. 
Attentive readers will notice that this is in fact the discrete mean shift formula \cite{ComaniciuM02Meanshift}, with training samples taking the place of data samples and noise magnitude $\sigma(t)$ taking place of the kernel bandwidth $\lambda$. This expression is feasible to compute in practice for small datasets, and we may substitute it into the SDS formula (\ref{eq:vanilla_sds}) to get an explicit solution for the SDS gradient:
\begin{multline*}
    \!\!\!\!\nabla_{x} \mathcal{L}_\mathrm{SDS} = \mathbb{E}_{t, z_t \sim \mathcal{N}(\alpha_tx,\sigma_t^2\mathbf{I})} \Bigg[ w(t)  \frac{z_t - D^*(z_t;\sigma_t)}{\sigma_t} \frac{\partial x}{\partial \theta} \Bigg].
\end{multline*}
This gives us an integral we can numerically evaluate with reasonable guarantees of its behavior. This allows us to compare both methods on synthetic datasets, eliminating any error introduced by training and evaluating a neural model to show that the theoretical properties hold.

\subsection{Toy Distributions in $\sR^2$}
\label{sec:toydist_subsec}

In addition to the fractal dataset (\cref{fig:fractal_2D_part1}), we extend our analysis to other 2D datasets, with ${y_i}^m_{i=1} \subset \gM \in \sR^2$ sampled from various challenging toy 2D densities  \cite{scikit-image, rozen2021moserflowdivergencebasedgenerative}. 
Given $10^4$ samples from the data distribution and points densely initialized across a grid $[-1.5, 1.5]^2$, we evaluate SDS (\cref{alg:sds}) and our \methodname (\cref{alg:msg}) with $10^3$ Monte Carlo samples. 
We perform distillation with a learned denoiser (\ref{eq:cfg_update}) and an ideal denoiser (\ref{eq:idealdenoiser}). 
For the learned denoiser, we use the architecture and training setup used by \citet{karras2024guiding} and similarly represent the densities as mixtures of Gaussians. 
We use the Adam optimizer \cite{kingmaB14Adam} and run the optimization procedure for 150 steps with a learning rate of $0.08$.
For our \methodname, we set an initial bandwidth of $0.316 \sim \sqrt{0.1}$ which is linearly decayed over the course of the optimization. 
For the ideal denoiser, due to it's high cost requirements in time and memory, we instead opt to use a few steps of gradient descent with high learning rate.

We visualize the generated samples produced by our method and SDS after the optimization in \cref{fig:toydensity}. In addition to optimizing samples, we evaluate both SDS and MSD gradients across the domain and then numerically integrate them to reconstruct the loss functions they represent. This makes the bias in SDS particularly obvious, as the peaks of this reconstructed function may be well out of distribution.

We suspect that this bias persists in SDS in higher dimensional settings and is what causes SDS optimized results to be blurry and exhibit other artifacts (top row of \cref{fig:fractal_2D_part2}).

In addition to bias, we are interested in evaluating the variance of the gradient estimate. This is an important factor for convergence, since ascending a stochastic estimate of the gradient is essentially a random walk. In such, high variance of the estimate may make the walk take longer to converge -- indeed, with sufficiently high variance we may find the iteration often taking \emph{backwards} steps with respect to the true gradient. Furthermore, a walk with high variance may not stay converged at an optima, and instead randomly oscillate around them.

To quantify the variance of an estimate $\hat{g}(x)$ of the gradient $g(x)$, we employ a slight variation of the Monte Carlo estimator efficiency formula
\begin{equation}
    \varepsilon (\hat{g}(x))=\frac{
    |g(x)|^2
    } {\mathrm{MSE}(\hat{g}(x)) \: \mathrm{cost}(\hat{g}(x))}.
\end{equation}
We measure cost as number of invocations of the score model, since that is the typical bottleneck in diffusion. 
Normalization by the squared norm of $g$ is included to account for the fact that due to bias and scaling, different estimators may converge to gradients of different magnitude, and the normalized MSE then roughly describes the probability of the estimated gradient pointing the ``right'' way. 

We accumulate both MSE and cost over many independent estimations, and average over many values of $x$.\input{tables/efficiency}
\noindent{}The result of these efficiency comparisons between our method and SDS is in \cref{tab:efficiency} (in log-scale). Although getting a single estimate with our method requires more score model invocations, the efficiency of our method is significantly higher. We observe similar behavior in complex settings (\cref{fig:fid_vs_optimization}).
In \cref{tab:idealdenooiser_metrics_1}, we evaluate the quality of the generated samples. Our method outperforms SDS, in both the ideal denoiser and learned denoiser setting.








\input{tables/2D_density_metrics}
\input{figures/toy_2d_densities_plots}

\subsection{Practical Setting}
\label{sec:practical_setting}

For large-scale image datasets, idealized denoiser is no longer tractable and we have to contend with a learned denoising function, as well as all the associated machinery.

This introduces numerical issues. Namely, as already mentioned in \cref{sec:integrating}, the magnitude of the kernel term may grow to where the standard first or second order integrators can no longer manage it; but conversely, so does the magnitude of the learned score term when $z_t$ is far out of distribution; why this is the case becomes apparent when we consider that the ideal denoiser (\cref{sec:ideal}) uses the same equation as mean shift. This is a problem because at the start of the optimization, it is likely in a high-dimensional space that $x$ will be out of distribution and we have to choose between the integration failing because the denoiser term has a high magnitude, or because the kernel term has a high magnitude.

To alleviate this, we use two heuristic approximations: applying guidance in limited interval (\cref{sec:integrating}) and scaling our sample in \cref{eq:integratingkernel} by noise corresponding to time step $t$. In practice, we apply inversion to get the latter.
These are designed to keep the iterate in a region with reasonable score magnitude and still sample a distribution that is an approximation of the product distribution.


    

\subsection{Pre-trained Stable Diffusion}
\label{sec:stablediffusion_exps}

We use the latent-space diffusion model, Stable Diffusion, as the diffusion prior for text-conditioned optimization of parameters of differentiable image generators. Specifically, we optimize parameters $\vartheta$ of generator $g$, a rendering function that maps $\vartheta$ to an image $\mathcal{I}$. The rendered image $\mathcal{I}$ is fed to the image encoder to get $x^k$, our latent at optimization step $k$, over which the gradient is computed. We define two settings, (1) where $\vartheta$ represents an RGB image, and (2) where $\vartheta$ represents a 3D volume. Specifically:
\vspace{-2mm}
\begin{enumerate}
    \item \textbf{Text-to-2D.} We represent 2D images via a coordinated-based MLP $f$ with learnable parameters $\vartheta$ that takes as input a 2D point $p$ in the unit square $p=(x,y) \in [0,1]^2$ and outputs RGB $ \in [0,1]^3$; $f(p;\vartheta):\sR^2 \rightarrow $ RGB. We use this non pixel-based representation of an image for two reasons, (1) to prevent our method and the baselines from taking the exact gradient step i.e. running diffusion sampling and setting $x^k$ to the denoised latent $z_0$, and (2) we can directly compare with images sampled via DDIM, an unconstrained image generation setting.
    \item \textbf{Text-to-3D.} We represent 3D volumes as NeRFs, following \cite{poole2022dreamfusion}. The NeRF is parameterized by two MLPs, one for foreground and one for background. The former has 64 hidden nodes and 2 layers, with input $(x,y,z)$ coordinates encoded via HashGrid \cite{mueller2022instant}.
\end{enumerate}

\paragraph{Implementation details.}

We implement all our code in PyTorch, on a single NVIDIA A100 gpu. We use the Threestudio \cite{threestudio2023} framework for experiments involving pre-trained Stable Diffusion. All the experiments use Adam optimizer with lr$=10^{-2}$. We set optimization steps to $400$ for text-to-2D and $7k$ for text-to-3D. We use a monotonically decreasing schedule for the bandwidth $\lambda$.

\input{tables/text_to_2D}
\input{figures/fid_vs_iter}

\subsection{Evaluation}

\paragraph{Dataset.}

We use a subset of the prompts curated by \citet{poole2022dreamfusion,hertz2023delta}. We include all prompts in \cref{sec:prompts}.

\paragraph{Metrics.}

For toy density dataset (\cref{sec:toydist_subsec}), we compute negative log-likelihood scores (NLL), generative precision and recall~\cite{Kynkaanniemi2019}, and maximum mean discrepancy (MMD).
For text-to-2D, we use images produced by DDIM to represent the ground truth distribution. To evaluate fidelity of the images, FID~\cite{heusel2017fid} is computed for each baseline (SDS, SDI) and ours against this ground truth image set. We also compute CLIP scores~\cite{clipscore} to measure prompt-generation alignment.

\paragraph{Quantitative comparisons.}

\Cref{tab:image_fusion} reports results for FID and CLIP-based similarity, comparing our method with SDS and SDI. We outperform both baselines in image fidelity and are our generations are more faithful to the input prompt.

\paragraph{Qualitative comparisons.}

\Cref{fig:fractal_2D_part2} (top row) and \cref{fig:appendix_results} compares our method with SDS and SDI on text-to-2D generation, qualitatively. In the latter, we show the importance of the two heuristics (\cref{sec:practical_setting}) to resolve numerical instabilities, absence of which can result in visual artifacts. SDS, as discussed, produces low-fidelity results while SDI's inversion accumulates numerical errors during early stages of optimization.
In \cref{fig:textto3d}, we qualitatively compare results for text-to-3D optimization. We restrict to qualitative comparison for this task as quantitative metrics have high variance due to the absence of a ground truth dataset.

\paragraph{Impact of bandwidth.}

\Cref{fig:impact_of_lambda} shows the impact of the bandwidth ($\lambda$) term on the denoising process. First, we sample three parameters $\{\vartheta^k\}$ from our text-to-2D optimization pipeline at iterations $k=\{100, 200, 400\}$ and also sample three discrete $\lambda$ values $\{\lambda_1 \ll \lambda_2 \ll \lambda_3 \}$. Then, we run our forward pass once for each $\lambda_i$, independently. We visualize four decoded denoised latents $z_0$ (with different random seeds). The highlighted images show the optimal choices of $\lambda$ for each $x^k$ (the encoded latent for $\vartheta^k$). At high bandwidth value $\lambda_3$, the influence of the kernel term in the product sampling is negligible. This degenerates to vanilla denoising and we observe high variance in the output, irrespective of our current $x^k$. This is ideal at early stages of optimization. As bandwidth is annealed, we observe reduction in variance. Yet, the quality of the outputs can degrade if the kernel term dominates while $x^k$ is not \textit{``in-distribution''}. As $x^k$ approaches the mode of the distribution corresponding to the input text-prompt at final stages of optimization (when $k=400$), with a low bandwidth $\lambda_1$, our denoised latent $z_0 \approx x^k$. This provides us with a convergence criteria and we terminate when $\lambda$ is below the threshold $\lambda_1$.


\input{figures/text_to_3D}
\input{figures/impact_of_lambda}

\section{Conclusion}

In this paper, we have reframed diffusion distillation in terms of explicitly ascending the gradient of the data distribution. We have derived mean-shift distillation as a proxy that provably aligns with this gradient, and in the limit its maxima are collocated with the modes of the data distribution.

We have demonstrated that compared to SDS, this method achieves better mode alignment as well as lower gradient variance, which in practice translates to more realistic optimization results as well as improved convergence rate.

Since this method simply provides optimization gradient much like SDS does, it may be used as a one-to-one replacement without retraining of the underlying model, or indeed substantial code modification.

While the basic algorithm works as the theory predicts in synthetic scenarios, with real-world models we have to contend with integrator error due to large score magnitudes. We have designed heuristics to alleviate this and achieve improvements on SDS in practice, but we hope future work will be able to improve the integration and/or sampling procedure, obviating the need for heuristics.


\section*{Impact Statement}

As a more or less straightforward substitute of an existing method (SDS), our method inherits ethical concerns of the diffusion models it is being applied to, and the applications it is being put towards. It remains important to take care with sourcing training data to avoid copyright issues, bias issues, and training harmful content into the model. On the output side, generative models improve accessibility to creative expression, which however also makes it easier to produce harmful content including, but not limited to, misinformation, defamatory and obscene images. Ultimately these issues are impossible to fully solve on the tooling side and we must rely on other methods to analyse content and establish authenticity thereof to compensate.

That said, improved convergence properties of our method mean that less computation is required to achieve the same result, alleviating some of the environmental impacts associated with these generative methods.


\bibliography{main}
\bibliographystyle{icml2025}


\newpage
\appendix
\onecolumn

\section{Implementation details}

\input{figures/pseudocode}

\section{List of prompts}
\label{sec:prompts}

\begin{enumerate}
    \renewcommand{\labelenumi}{}
    \item \textit{``A DSLR photo of a hamburger''}
    \item \textit{``A blue jay standing on a large basket of rainbow macarons''}
    \item \textit{``A DSLR photo of a squirrel dressed as a samurai weighing a katana''}
    \item \textit{``A DSLR photo of a knight in silver armor''}
    \item \textit{``Line drawing of a Lizard dressed up like a victorian woman, lineal color''}
    \item \textit{``A photo of a car made out of sushi''}
    \item \textit{``A DSLR photo of a tulip''}
    \item \textit{``A DSLR photo of a Pumpkin head zombie, skinny, highly detailed, photorealistic''}
    \item \textit{``A watercolor painting of a sparrow, trending on artstation''}
    \item \textit{``Michelangelo style statue of man sitting on a chair''}
\end{enumerate}

\section{Ablations and more results}

\input{figures/ablations}


\end{document}
