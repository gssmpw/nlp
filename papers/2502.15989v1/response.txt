\section{Related Work}
\label{sec:related}

\paragraph{Denoising diffusion.}

In our work we rely most directly on the mean-shift method of mode seeking **Sohl-Dickstein, Venturi, & Weiss**, "Deep Unsupervised Learning using Nonequilibrium Thermodynamics" but our ability to apply it to diffusion rests on a body of theoretical analysis of this process.

Mathematically, denoising diffusion consists of solving an initial value problem (IVP) on a random variable from a simple, typically standard normal distribution, where the time-dependent gradient is learned by reversing the process of adding noise to the distribution being modeled **Song & Ermon**, "Improved Techniques for Training Score-Based Generative Models". Already in these works authors suggest ways in which the output distribution may be manipulated by adding terms to the differential equation underlying the initial value problem, a property we will rely on to manipulate the output to our method's advantage.

A surprising connection between mean shift and diffusion emerged from the analysis of the optimal denoising model **Nichol & Dhariwal**, "Improved Denoising Diffusion Model". Since the forward (noising) process can be expressed as successive convolutions with a Gaussian kernel, the intermediate distributions are in fact Gaussian-kernel density estimates of the data distributions, with kernel bandwidth proportional to the time parameter. Therefore in the ODE of the reverse (inference) process, the gradient of the denoiser is theoretically equal to the mean-shift vector with appropriate kernel and bandwidth. Mean-shifting on the IVP time domain does not in fact seek modes of the output distribution, but we take advantage of this knowledge to implement mean shift on that domain.

Further related to the analysis of modes in particular, **Ho, Jain & Mordvintsev** suggest that applying classifier-free guidance (CFG) **Sohl-Dickstein, Venuti, & Weiss**, "Diffusion-Based Generative Models" to diffusion has the effect of sharpening the modes of the output distribution. This guidance does not explicitly \emph{seek} modes, but we have found that using CFG synergizes well with both SDS and our method√ü.

\paragraph{Distilling diffusion priors.}

Score distillation sampling (SDS) **Ho & Jain**, "Improved Score-Based Generative Models" has emerged as a useful technique for leveraging the priors learned by large-scale image models beyond 2D raster images. SDS provides an optimization procedure to estimate the parameters of a differentiable image generator, such that the rendered image is pushed towards a higher-probability region of a pre-trained prompt-conditioned image diffusion model. Originally proposed to optimize volumetric representations like NeRFs, it has been extended to other non-pixel-based representations **Song & Ermon**, "Improved Techniques for Training Score-Based Generative Models".

The tendency of SDS to produce over-smoothened results due to high variance is well documented. A plethora of works have been proposed to mitigate this behavior, e.g.\ to factorize the gradient to reduce the bias **Ho & Jain**, "Improved Score-Based Generative Models", or to replace the uniform noise sampling in SDS with noise obtained by running DDIM inversion **Nichol & Dhariwal**, "Improved Denoising Diffusion Model".  propose a control variate for SDS,  improve diversity of generations, and  alleviate the multi-view inconsistency problem.