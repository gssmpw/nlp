
%% bare_jrnl.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% see http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



\documentclass[journal]{IEEEtran}
\usepackage{stackengine}
\usepackage{blindtext}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage{graphics} % for pdf, bitmapped graphics files
\usepackage{epsfig} % for postscript graphics files
\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
\usepackage{amsmath} % assumes amsmath package installed
\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{cite}
\usepackage{multicol}
\usepackage{mathtools, cuted}
\usepackage[cmintegrals]{newtxmath}
\usepackage{epstopdf}
\usepackage{graphics} % for pdf, bitmapped graphics files
\usepackage{epsfig} 
\newtheorem{prop}{Proposition}
\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
\usepackage{amsmath} % assumes amsmath package installed
\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{cite}
\usepackage{multicol}
\usepackage{mathtools, cuted}
\usepackage[cmintegrals]{newtxmath}
\usepackage{mathtools}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\usepackage{mathptmx}
\usepackage{helvet}
\usepackage{courier}
\usepackage{subfigure}
\usepackage{type1cm}
\usepackage{titlesec}
\setcounter{MaxMatrixCols}{25}
\setcounter{secnumdepth}{4}
% \newtheorem{lemma}{Lemma}
% \newtheorem{sublemma}{Lemma}[lemma]
% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
\usepackage{epsfig} % for postscript graphics files
\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
\usepackage{amsmath} % assumes amsmath package installed
\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{cite}
\usepackage{multicol}
\usepackage{mathtools, cuted}
\usepackage[cmintegrals]{newtxmath}
% \usepackage{algpseudocode}
\usepackage{makeidx}         % allows index generation
% \usepackage[ruled,vlined]{algorithm2e}
% \newtheorem{lemma}[theorem]{Lemma}
% \newtheorem{corollary}{Corollary}[theorem]
%\usepackage{caption}
%\renewcommand\labelenumi{(\theenumi)}
%\usepackage{subcaption}
% \usepackage{algorithmic}
\usepackage{algorithm}% http://ctan.org/pkg/algorithm
\usepackage{algpseudocode}% http://ctan.org/pkg/algorithmicx
\DeclareMathOperator*{\argmin}{\arg\!\min}
\DeclareMathOperator*{\argmax}{\arg\!\max}
% \newtheorem{theorem}{Theorem}
                             % when including figure files
\usepackage{multicol}        % used for the two-column index
\usepackage[bottom]{footmisc}% places footnotes at page bottom
\usepackage{caption}
\usepackage{nomencl}
\usepackage[usenames, dvipsnames]{color}

\let\proof\relax
\let\endproof\relax
\let\openbox\relax
\let\endopenbox\relax

%\usepackage{caption}
%\renewcommand\labelenumi{(\theenumi)}
%\usepackage{subcaption}
% \usepackage{algorithmic}
% \usepackage{arevmath}     % For math symbols
% \usepackage[noend]{algpseudocode}
% \DeclareMathOperator*{\argmin}{\arg\!\min}
% \DeclareMathOperator*{\argmax}{\arg\!\max}
% \newtheorem{theorem}{Theorem}
                             % when including figure files
\usepackage{multicol}        % used for the two-column index
\usepackage[bottom]{footmisc}% places footnotes at page bottom
\usepackage{caption}
\usepackage{titlesec}
\usepackage{nomencl}
\usepackage[usenames, dvipsnames]{color}
\usepackage{multirow}

\usepackage{amsthm}
 
\theoremstyle{definition}
\newtheorem{definition}{Definition}

\theoremstyle{theorem}
\newtheorem{theorem}{Theorem}

\theoremstyle{remark}
\newtheorem{remark}{Remark}
\theoremstyle{proposition}
\newtheorem{proposition}{Proposition}
% \theoremstyle{lemma}
% \newtheorem{lemma}{Lemma}
\theoremstyle{corollary}
\newtheorem{corollary}{Corollary}
\theoremstyle{proof}

\newtheorem{assumption}{Assumption}
\theoremstyle{assumption}

\newtheorem{property}{Property}
\theoremstyle{property}

\newtheorem{lemma}{Lemma}
\theoremstyle{lemma}

% \theoremstyle{proof}




% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.




% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Containment Control Approach for Steering Opinion in a Social Network}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%

\author{Hossein Rastgoftar% <-this % stops a space
\thanks{{\color{black}H. Rastgoftar is with the Department
of Aerospace and Mechanical Engineering, University of Arizona, Tucson,
AZ, 85721 USA e-mail: hrastgoftar@arizona.edu.}}
% <-this % stops a space
% \thanks{J. Doe and J. Doe are with Anonymous University.}% <-this % stops a space
% \thanks{Manuscript received April 19, 2005; revised August 26, 2015.}}
}
% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
% \markboth{
% IEEE Transactions on Control of Network Systems
% }

% {Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for IEEE Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.




% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
The paper studies the problem of steering multi-dimensional opinion in a social network. Assuming the society of desire consists of stubborn and regular agents, stubborn agents are considered as leaders who specify the desired opinion distribution as a distributed reward or utility function. In this context, each regular agent is seen as a follower, updating its bias on the initial opinion and influence weights by averaging their observations of the rewards their influencers have received. Assuming random graphs with reducible and irreducible topology specify the influences on regular agents, opinion evolution is represented as a containment control problem in which stability and convergence to the final opinion are proven. 

% The paper considers the problem of the safe operation of multiple agents with different capabilities and access authorities  to effectively and safely accomplish a complex mission. This problem is decomposed into two main sub-problems. The first sub-problem is to obtain the desired configuration of the agent team so that the best coverage of a distributed target is achieved while distinct inaccessible regions are avoided. To achieve this, we first apply the principles of computational fluid dynamics to establish a nonsingular mapping between the motion space and a planning space that excludes all inaccessible regions. We then develop a novel deep neural network forward learning (DNNFL) to abstractly represent the target by a finite number of points specifying the desired configuration of the agent team. The second sub-problem is the mission planning that is defined as the event-triggered Markov Decision Process (ET-MDP) with constrained actions and components that are updated by a non-deterministic finite automaton.



\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
The evolution of opinions in social systems has received a lot of attention from the control community in recent years. To understand how ideas, views, and attitudes can spread in a group, researchers have employed a number of methods to mathematically describe onion evolution. These mathematical models have the potential to forecast the results of public debate, reduce the dissemination of false information, and offer recommendation systems. This paper studies the problem of opinion evolution in a social system when we adapt the Fredrick Johnson to  analyze multi-dimensional opinion dynamics and formulate it as a containment control problem. 


\subsection{Related Work}
In the literature, Fredrick Johnson (FJ) \cite{zhou2024friedkin, frasca2024opinion} and DeGroot \cite{wu2022mixed, zhou2020two, liu2022probabilistic} models have been used to analyze the evolution of beliefs in social networks. Adapting the Fredrick Johnson (FJ) model, the authors of \cite{kang2022coevolution, 10591448, lin2018opinion} study the evolution of opinion in signed networks, which can be used to assess collaborative and competitive behaviors in a society. The multidimensional opinion dynamics under the FJ model is studied in \cite{parsegov2016novel, zhou2022multidimensional} to analyze the evolution of multiple topics in a social network. The stability and convergence of the dynamics of evolution of opinion in random networks are studied in \cite{wang2024final, xing2024transient, xing2024concentration}. The authors investigate the impact of stubborn agents' opinions on polarization and disagreement within a social network in \cite{shirzadi2024stubborn}. Co-evolution of opinion and action has been studied in \cite{9303954, mo2022coevolution, wang2024co, 10168221}. Opinion evolution was formulated as a mean-field game in Ref. \cite{6760259}. In \cite{debuse2024study}, the authors study the evolution of opinions in a time-varying network, assuming that agents do not follow the most divergent points of view. The authors in \cite{sprenger2024control} apply the FJ model to learn about users' opinions and build a recommendation system for social networks.  

This paper applies containment control models to steer opinions in a social network. Containment control is  a well-received  leader-follower method in which group coordination is guided by a finite number of leaders and acquired by followers through local communication. Refs. \cite{cao2012distributed, ji2008containment} provide necessary and sufficient conditions for stability and convergence in the multi-agent containment coordination problem. Containment under fixed and switching inter-agent communications is investigated in Refs. \cite{cao2012distributed, notarstefano2011containment, li2015containment} Also,  multi-agent containment control in the presence of time-varying delays is analyzed in \cite{shen2016containment, liu2014containment}. Refs. \cite{wang2013distributed, liu2015distributed}  have studied finite-time containment control of a multi-agent system. Containment control has been defined as a decentralized affine transformation in \cite{rastgoftar2021scalable, rastgoftar2021safe, rastgoftar2022spatio}.







\subsection{Contribution}
We consider the evolution of multidimensional opinion in a community of agents under the FJ model \cite{zhou2024friedkin} where we innovatively present evolution as a containment control problem to steer opinions in a society. By classifying agents as regular and stubborn agents, we consider scenarios at which the influences among the regular agents are propagated through stochastic communications structured by reducible and irreducible networks and time-varying communication weights.

By considering stubborn agents as leaders specifying a reward distribution for a community of desire, the paper develops a framework for regular agents to achieve a desired distribution in a decentralized manner by maximizing a locally and ad hoc observable reward. More precisely, the regular agents are not aware of the reward distribution, specified by the leaders (stubborn agents) over the opinion space, but they are aware of the reward of their influencer agents, which enables them to   update their biased initial opinion and influence weights through maximization of average reward. 

% update their bias on their own initial opinion 

Compared to the existing literature, the paper offers the following novel contributions:
\begin{itemize}
    \item While containment control models have been previously used for multi-agent coordination, this paper applied the containment control model to steer opinions in a social network. in this context, the paper provides the proof of convergence and final containment of multidimensional opinions under both irreducible and reducible communication strategies.
    \item The paper provides a framework for decentralized acquisition of a desired distribution of the community, determined by the leaders of the community, by the regular agents. 
    \item The paper formally specifies conditions for reducibility of opinion network dynamics. By using these conditions, the paper provides a proof for the minimum convergence time of the community's opinion towards the final opinion.
\end{itemize}


This paper is organized as follows: The objective of the paper  is overviewed in Section \ref{Problem Statement}. The opinion evolution dynamics under reducible and irreducible networks are modeled as a containment control problem in Section \ref{Opinion Evolution Dynamics}. Section \ref{Decentralized Acquisition of Biases and Influences} develops a strategy for regular agents to assign their influence weights and biases, on their initial opinions, in a decentralized manner. Simulation results are presented in Section \ref{Simulation Results} and followed by the concluding remarks in Section \ref{Conclusion}.


 \section{Problem Statement} \label{Problem Statement}
We consider a system of $N$ agents defined by set $\mathcal{V}=\left\{1,\cdots,N\right\}$ evolving over a multi-dimensional opinion space $\mathcal{O}=\left[0,1\right]^n$ where $n$ denoting the dimension of the space defines $n$ possible subjects defined by set $\mathcal{S}=\left\{1,\cdots,n\right\}$. We use $\mathbf{o}_i(k)=\begin{bmatrix}o_{i,1}(k)&\cdots& o_{i,n}(k)\end{bmatrix}^T$ to aggregate the opinion of agent  $i\in\mathcal{V}$,  where  
$
o_{i,j}(k)\in \left[0,1\right]$ denotes the opinion of agent $i\in \mathcal{V}$ about subject $j\in \mathcal{S}$ at discrete time $k\in \mathbb{Z}$.

We use graph $\mathcal{G}\left(\mathcal{V},\mathcal{E}_k\right)$ to specify interactions among the agents, where $\mathcal{E}_k\subset \mathcal{V}\times \mathcal{V}$ the edges of the graph $\mathcal{G}$. To be more precise, agent $i\in \mathcal{V}$ influences on agent $j\in \mathcal{V}$, if $(i,j)\in \mathcal{E}$. Given $\mathcal{E}_k$, set
\begin{equation}
    \mathcal{N}_i\left(k\right)=\left\{j\in \mathcal{V}:\left(j,i\right)\in \mathcal{E}_k\right\},\qquad \forall i\in \mathcal{V},~\forall k\in \mathbb{Z},
\end{equation}
defines all agents influencing on $i\in \mathcal{V}$.


By using the Friedkin-Johnsen model \cite{zhou2024friedkin}, the opinion of the agent $i\in \mathcal{V}$ is updated  by
\begin{equation}\label{opinionevolutionindividual}
\mathbf{o}_i\left(k+1\right)=\left(1-\lambda_i(k)\right)\sum_{j\in \mathcal{N}_i}w_{i,j}(k)\mathbf{o}_j(k)+\lambda_i(k)\mathbf{o}_i(0),
\end{equation}
for every $k\in \mathbb{Z}$, where $\lambda_i(k)\in \left[0,1\right]$ is the bias of agent $i\in \mathcal{V}$ on her/his opinion; $w_{i,j}\geq 0$ is the weight of influence of agent $j\in \mathcal{N}_i$ on agent $i\in \mathcal{V}$ at discrete time $k$; and
\begin{equation}
    \sum_{j\in \mathcal{N}_i}w_{i,j}\left(k\right)=1.
\end{equation}
We say the agent $i\in \mathcal{V}$ is \textit{stubborn} if $\lambda_i=1$. Otherwise, the agent $i\in \mathcal{V}$ is called \textit{regular}. Therefore, set $\mathcal{V}$ can be expressed as 
\begin{equation}
    \mathcal{V}=\mathcal{V}_S\cup \mathcal{V}_R
\end{equation}
where $\mathcal{V}_S=\left\{i\in \mathcal{V}:\lambda_i(k)=1,\forall k\in \mathbb{Z}\right\}$ and $\mathcal{V}_R=\mathcal{V}\setminus \mathcal{V}_S$.



The main objective of the paper is that the regular agents achive a desired distribution assigned by the stubbron agents through local communication with their in-neighbors. To this end, we define $\mathcal{U}:\mathcal{O}\rightarrow \mathbb{R}_+$ as the utility function and  make the following assumptions:
\begin{assumption}\label{unknownutility}
    Regular agent $i\in \mathcal{V}$ does not know about the distribution of $\mathcal{U}$ over the opinion space $\mathcal{O}$.
\end{assumption}
\begin{assumption}\label{knownutility}
    Regular agent $i\in \mathcal{V}$ can be informed about the $u_i(k)=\mathcal{U}(\mathbf{o}_i(k))$ and $u_j(k)=\mathcal{U}(\mathbf{o}_j(k))$ for every $j\in \mathcal{N}_i(k)$ at every discrete time $k$.
\end{assumption}

Given the above problem setting, the paer aims to provide solutions for the following two problems:

\textbf{Problem 1:} It is desired  to present the network opinion dynamics as a containment control problem where we provide proofs for the stability and convergence  of the network opinion evolution under time-varying biases, influences, and network structure. 

\textbf{Problem 2:} It is desired that every regular agent $i\in \mathcal{V}_R$ determine influene $w_{i,j}(k)$ and bias $\lambda_i(k)$ in a decentralized fashion while meeting Assumptions \ref{unknownutility} and \ref{knownutility},  so that the ultimate opinion of the regular agents maximizes the utility function $\mathcal{U}$. 




% the average  opinion of $\mathcal{V}$'s agents converges to 
% % \begin{equation}
% % \mathbf{o}^*=\sum_{i\in \mathcal{S}}\alpha_i\mathbf{o}_i(0)
% % \end{equation}
% % where $\alpha_i\geq0$ is constant and 
% % \begin{equation}
% %     \sum_{i\in \mathcal{S}}\alpha_i=1.
% % \end{equation}

% % To achieve this objective, we solve the following two problems:

% % \textbf{Problem 1: Openness Optimization} We aim to assign  $\lambda_i^*(k)$ for every agent $i\in \mathcal{V}$ such that the average opinion of $\mathcal{V}$'s agents converges to $\mathcal{O}^*$.

% % \textbf{Problem 2: Stability Analysis and Convergence Guarantee:} By obtaining the network opinion dynamics, we provide a proof for the convergence of the network opinion while ensuring all feasibility conditions.

\section{Opinion Evolution Dynamics}\label{Opinion Evolution Dynamics}
Let $\mathcal{V}_R=\left\{1,\cdots,N_R\right\}$ and  $\mathcal{V}_S=\left\{N_R+1,\cdots,N\right\}$ identify the regular and stubborn agents, respectively. Then,
\begin{equation}
    \mathbf{x}(k)=\mathrm{vec}\left(\begin{bmatrix}\mathbf{o}_1(k)&\cdots&\mathbf{o}_{N_R}(k)\end{bmatrix}^T\right)\in \mathbb{R}^{nN_R\times 1}
\end{equation}
as the state vector, aggregating components of opinions of the regular agents. Note that ``vec'' is the matrix vectorization operator that converts a matrix $P$ by $Q$ into a vector $PQ$ by $1$. Also,
\begin{equation}
    \mathbf{u}=\mathrm{vec}\left(\begin{bmatrix}
        \mathbf{o}_{N_R+1}&\cdots&\mathbf{o}_{N}&\mathbf{o}_1(1)&\cdots&\mathbf{o}_{N_R}(1)
    \end{bmatrix}^T\right)\in \mathbb{R}^{Nn\times 1}
\end{equation}
is a constant vector aggregating opinion components of the stubborn agents and the initial opinion of regular agents. 
\begin{definition}
    The paper defines the influence matrix
$\mathbf{W}=\left[W_{i,j}\right]\in \mathbb{R}^{N_R\times N}$ as the influence matrix aggregating the influences of all agents on regular agents, where
\begin{equation}
    W_{i,j}=\begin{cases}
        w_{i,j}&i\in \mathcal{V}_R,~j\in \mathcal{N}_i\\
        0&\mathrm{otherwise}
    \end{cases}
    .
\end{equation}
\end{definition}
\begin{definition}
    The paper defines the bias matrix
\begin{equation}
    \mathbf{\Lambda}(k)=\mathrm{diag}\left(\lambda_1(k),\cdots,\lambda_{N_R}(k)\right)\in \mathbb{R}^{N_R\times N_R}.
\end{equation}
to quantify the bias of every regular agent on its initial opinion.
\end{definition}



We analyze stability and convergence of the opinion evolution dynamics under irreducible and reducible network in Sections \ref{Opinion Evolution under Irreducible Network} and \ref{Opinion Evolution under Reducible Network}, respectively.

\subsection{Opinion Evolution under Irreducible Network}\label{Opinion Evolution under Irreducible Network}
Given $\mathbf{W}$ and $\mathbf{\Lambda}$, we define
\begin{equation}
    \mathbf{L}=\begin{bmatrix}
        \left(\mathbf{I}_{N_R}-\mathbf{\Lambda}\right)\mathbf{W}&\mathbf{\Lambda}
    \end{bmatrix}
    \in \mathbb{R}^{N_R\times\left(N+N_R\right)}
\end{equation}
Note that the $(i,j)$ entry of $\mathbf{L}=\left[L_{i,j}\right]$, denote by $L_{i,j}$, is obtained as follows:
\begin{equation}
    L_{i,j}=\begin{cases}
        \left(1-\lambda_i\right)w_{i,j}&j\leq N,~i\in \mathcal{V}_R,~j\in \mathcal{N}_i\\
        \lambda_i&j>N,j=i\\
        0&\mathrm{otherwise}
    \end{cases}
    .
\end{equation}
Let  $\mathbf{L}$ be partitioned as follows:
\begin{equation}\label{dynamics}
    \mathbf{L}=\begin{bmatrix}
        \mathbf{A}&\mathbf{B}
    \end{bmatrix}
\end{equation}
where $\mathbf{A}\in \mathbb{R}^{N_R\times N_R}$ and $\mathbf{B}\in\mathbb{R}^{N_R\times N}$. Then, the network opinion dynamics is obtained by
\begin{equation}\label{networkopiniondynamics}
    \mathbf{x}\left(k+1\right)=\left(\mathbf{I}_n\otimes \mathbf{A}\right)\mathbf{x}\left(k\right)+\left(\mathbf{I}_n\otimes \mathbf{B}\right)\mathbf{u}
\end{equation}

\begin{theorem}
    If graph $\mathcal{G}$ is defined such that there exists at least one path from agent, then, $i\in \mathcal{V}_R$,  then,
     dynamics \eqref{networkopiniondynamics} is stable. 
\end{theorem}
\begin{proof}
    See the proof in \cite{proskurnikov2017tutorial}.
\end{proof}


\begin{theorem}
    If graph $\mathcal{G}$ is defined such that there exists at least one path from every stubborn agent to every agent, $i\in \mathcal{V}_R$,  then
    \begin{equation}
        \mathbf{D}=-\mathbf{I}_n+\mathbf{A}
    \end{equation}
    is Hurwitz,
    \begin{equation}
        \mathbf{C}=-\mathbf{D}^{-1}\mathbf{B}
    \end{equation}
    is one-sum row and non-negative.
\end{theorem}

\begin{proof}
    The matrix $\mathbf{A}\in \mathbb{R}^{N_R\times N_R}$ is nonnegative where the sum of the entries $\mathbf{A}(k)$ is less than $1$. Therefore, the spectral radius of $\mathbf{A}(k)$ is indicated by $r$ and is less than $1$ at every discrete time $k$. As a result, the eigenvalues of $\mathbf{D}\in \mathbb{R}^{N_R\times N_R}$ are located inside a disk of radius, $r<1$ which is centered at $-1+0\mathbf{j}$. This implies that matrix $\mathbf{D}$ is Hurwitz at every discrete time $k$.

    To prove that $\mathbf{C}$ is a one-sum row, we define the matrix $\mathbf{Y}=\begin{bmatrix}         \mathbf{D}&\mathbf{B}     \end{bmatrix}$ where every row of $\mathbf{Y}$ sums up to $0$. Applying the Gaussian Jordan elimination approach, we can convert $\mathbf{D}$ to $\mathbf{I}_{Nn}$, since $\mathbf{D}$ is invertible. Therefore, applying the Gaussian Jordan elimination approach converts $\mathbf{Y}$ to $\mathbf{Y}'=\begin{bmatrix}
        \mathbf{I}&-\mathbf{D}^{-1}\mathbf{B}
    \end{bmatrix}$ by using row algebraic operations. Applying the row algebraic operations does not change sum of the rows of $\mathbf{Y}$, therefore,   every row of $\mathbf{Y}$ and $\mathbf{Y}'$ sums up to $0$. This implies that every row of the matrix $\mathbf{D}$ sums up to $1$.

    The diagonal elements of $\mathbf{D}$ are all $-1$ and its off-diagonal elements are all non-zero. Therefore, $\mathbf{Y}$ can be converted to $\mathbf{Y}''=\begin{bmatrix}
        -\mathbf{I}&\mathbf{Z}
    \end{bmatrix}$ by applying row-algebraic operations, where $\mathbf{Z}$ is non-positive. Because $\mathbf{Y}'=-\mathbf{Y}''$, we conclude that $-\mathbf{Z}=-\mathbf{D}^{-1}B$ is non-negative. 
    % Because $w_{i,j}>0$ and $\lambda_i>0$ and the
     
    
    % To prove that matrix $\mathbf{D}$ is positive, we note that the r

    
\end{proof}

The equilibrium of dynamics is achieved when $\mathbf{x}(k)$ converges to a constant vector $\mathbf{x}^*$. Under this situation $\mathbf{x}(k)$ and $\mathbf{x}(k+1)$ can be substituted by $\mathbf{x}^*$, and as the results, $\mathbf{x}^*$ is obtained by
\begin{equation}
    \mathbf{x}^*=\mathbf{C}\mathbf{u}.
\end{equation}

\subsection{Opinion Evolution under Reducible Network}\label{Opinion Evolution under Reducible Network}
In this section, we model opinion evolution in situations where agents can be divided into $M$ distinct groups and influences between the regular agent groups are arranged in a horizontal order. Under these assumptions, the group identification numbers are defined by the set $\mathcal{M}=\left\{1,\cdots,M\right\}$. Formally speaking, set $\mathcal{V}_R$ can be expressed as 
\begin{equation}
    \mathcal{V}_R=\bigcup_{l\in \mathcal{M}}\mathcal{V}_l,
\end{equation}
where $\mathcal{V}_l$ is a disjoint subset of $\mathcal{V}$ for every $l\in \mathcal{M}$. 
% \begin{definition}
Given $\mathcal{V}_0$ through $\mathcal{V}_M$, we define
\begin{equation}\label{Wl}
    \mathcal{W}_l=\begin{cases}
        \mathcal{V}_S\cup\mathcal{V}_l&l=1\\
        \mathcal{W}_{l-1}\cup\mathcal{V}_l&l\in \mathcal{M}\setminus \left\{1\right\}
    \end{cases}
    ,\qquad \forall l\in \mathcal{M}.
\end{equation}
% Opinion propagation under reducible network is achieved if $i\in \mathcal{W}_l$
% \end{definition}



\begin{assumption}\label{assumcom}
    We assume that 
    \begin{equation}
        \left(\bigwedge_{l\in \mathcal{M}}\bigwedge_{i\in \mathcal{V}_{l}}\left(\mathcal{N}_i\subset \mathcal{W}_{l}\right)\right)\vee\left(\bigwedge_{l\in \mathcal{M}\setminus \left\{1,M\right\}}\bigwedge_{i\in \mathcal{V}_{l}}\left(\mathcal{N}_i\subset \mathcal{W}_{l-1}\right)\right)
    \end{equation}
\end{assumption}
Assumption \ref{assumcom} implies that the opinion of the agent $i\in \mathcal{V}_l$ can only be influenced by the agents of $\mathcal{W}_r$' if $r\leq l$, when $l\in \mathcal{M}$, or $r<l$, depending on $l\in \mathcal{M}\setminus \left\{1,M\right\}$. The latter condition holds when 

\begin{definition}\label{OrderNumberSet}
Assuming $N_l=\left|\mathcal{V}_l\right|$, for every $l\in \mathcal{M}$, {\color{black}$\mathcal{O}_l$} assigns a unique order number to every $\mathcal{V}_l$'s agent. The order number of the agent $i\in \mathcal{V}_l$ is denoted by $o_i$, where $o_i=\mathcal{O}_l(i)$, for every $l\in \mathcal{M}$.
\end{definition}
\vspace{-.25cm}
\begin{definition}
    We define 
    $
    \mathbf{Q}_l=\left[Q_{i,j}^l\right]\in \mathbb{R}^{N_l\times N_R}
    $ as a transformation matrix 
    with $(i,j)$ entry 
    \vspace{-0.2cm}
    \begin{equation}\label{Ql}
       Q_{i,j}^l=\begin{cases}
            1&j=\mathcal{O}_l(i),~i\in \mathcal{V}_l\\
            0&\mathrm{else}
        \end{cases}
        ,\qquad l\in \mathcal{M}.
    \end{equation}
% as 
\end{definition}
We note that
\vspace{-0.25cm}
\begin{equation}\label{identityidentity}
    \mathbf{Q}_l\mathbf{Q}_l^T=\mathbf{I}_{N_l},\qquad \forall l\in \mathcal{M},
\end{equation}
where $\mathbf{I}_{N_l}\in \mathbb{R}^{N_l\times N_l}$ is the identity matrix.
% \vspace{-.25cm}
% \begin{definition}
%     We define 
%     \vspace{-0.2cm}
%     \begin{equation}
%     \bar{\mathbf{Z}}_l=\left(\mathbf{I}_3\otimes \mathbf{Q}_l\right)\mathbf{Z}\in \mathbb{R}^{3N_l\times1},\qquad l\in \mathcal{M},
% \end{equation}
% as the vector aggregating desired  position compblack when all agents defined by $\mathcal{V}_l$  when agents' order numbers are defined by $\mathcal{O}_l$, for every $l\in \mathcal{M}$.
% \end{definition}

\begin{definition}
    We define
\begin{equation}
    \mathbf{Q}=\begin{bmatrix}
                  \mathbf{Q}_1\\\vdots\\\mathbf{Q}_M
    \end{bmatrix}
    \in \mathbb{R}^{N_R\times N_R}
\end{equation}
as a transformation matrix.
\end{definition}
Note that 
\begin{equation}
    \mathbf{Q}\mathbf{Q}^T= \mathbf{Q}^T\mathbf{Q}=\mathbf{I}_{N_R}.
\end{equation}
\begin{definition}
    We define 
    \vspace{-0.2cm}
\begin{equation}
    \bar{\mathbf{x}}_l(k)=\left(\mathbf{I}_n\otimes \mathbf{Q}_l\right)\mathbf{x}(k)\in \mathbb{R}^{nN_l\times1},\qquad l\in \mathcal{M},
\end{equation}
as the vector aggregating opinions of all agents defined by $\mathcal{V}_l$.
% when agents' order numbers are defined by $\mathcal{O}_l$, for every $l\in \mathcal{M}$.
\end{definition}
We can partition $\mathbf{L}\in \mathbb{R}^{N_R\times \left(N+N_R\right)}$ as follows:
\begin{equation}
    \mathbf{L}=\begin{bmatrix}
        \mathbf{A}&\mathbf{S}&\mathbf{\Lambda}
    \end{bmatrix}
\end{equation}
where $\mathbf{A}\in \mathbb{R}^{N_R\times N_R}$ defines influences among the regular agents and $\mathbf{S}\in \mathbb{R}^{N_R\times \left(N-N_R\right)}$ aggregates influences of the stubborn agents on regular agents. 

\begin{definition}
    The paper defines
    \begin{equation}
        \bar{\mathbf{A}}_{pq}=\mathbf{Q}_p\mathbf{A}\mathbf{Q}_q^T\in \mathbb{R}^{N_p\times N_q},\qquad p,q\in \mathcal{M}
    \end{equation}
\end{definition}


\begin{definition}
    The paper defines
    \begin{equation}
        \bar{\mathbf{\Lambda}}_{pq}=\mathbf{Q}_p\mathbf{\Lambda}\mathbf{Q}_q^T\in \mathbb{R}^{N_p\times N_q},\qquad p,q\in \mathcal{M}.
    \end{equation}
\end{definition}
Notice that $\bar{\mathbf{\Lambda}}_{pq}$ is a positive definite and diagonal matrix, if $p=q$ and $p\in \mathcal{M}$. Also, $\bar{\mathbf{\Lambda}}_{pq}\in\mathbf{0}_{N_p\times N_q} \mathbb{R}^{N_p\times N_q}$, if $p\neq q$ and $p,q\in \mathcal{M}$.
% Because $\mathbf{}$

% \begin{definition}
%     The paper defines 
%     \begin{equation}\label{sb}
%     \bar{\mathbf{A}}=\mathbf{Q}\mathbf{A}\mathbf{Q}^T.
% \end{equation}
% \end{definition}
% % \begin{definition}
% %     The paper defines 
% %     \begin{equation}\label{rb}
% %     \bar{\mathbf{S}}=\mathbf{Q}\mathbf{S}
% % \end{equation}
% % \end{definition}
% \begin{definition}
%     The paper defines 
% \begin{equation}\label{lb}
%     \bar{\mathbf{\Lambda}}=\mathbf{Q}\mathbf{\Lambda}\mathbf{Q}^T.
% \end{equation}
% \end{definition}
\begin{definition}
    The paper defines 
\begin{equation}
\begin{split}
     \bar{\mathbf{u}}=&\left(\mathbf{I}_n\otimes\mathbf{H}\right)\begin{bmatrix}\mathbf{u}_A^T&\mathbf{u}_S^T\end{bmatrix}^T\in \mathbb{R}^{Nn\times 1}
\end{split}   
\end{equation}
where
\begin{equation}
    \mathbf{u}_S=\mathrm{vec} \left(\begin{bmatrix}
        \mathbf{o}_{N_R+1}(0)&\cdots&\mathbf{o}_N(0)
    \end{bmatrix}^T\right)\in \mathbb{R}^{n\left(N-N_R\right)\times 1},
\end{equation}
\begin{equation}
    \mathbf{u}_A=\mathrm{vec}\left(
    \begin{bmatrix}
        \mathbf{o}_{1}(0)&\cdots&\mathbf{o}_{N_R}(0)
    \end{bmatrix}^T\right),
\end{equation}
\begin{equation}
\mathbf{H}=
    \begin{bmatrix}
         \mathbf{I}_{N-N_R}&\mathbf{0}_{\left(N-N_R\right)\times N_R}\\\mathbf{0}_{ N_R\times \left(N-N_R\right)}&\mathbf{Q}
     \end{bmatrix}
     .
\end{equation}
\end{definition}
    % The paper defines 
Note that $\mathbf{H}^T\mathbf{H}=\mathbf{H}\mathbf{H}^T=\mathbf{I}_N$.

\begin{theorem}
If the regular agent $i\in \mathcal{V}_l$ is solely influenced by $\mathcal{W}_l$, for every $l\in \mathcal{M}$, then the communication matrix $\mathbf{W}$ is reducible and network opinion dynamics is obtained by
\begin{equation}\label{convertednetwork}
    \bar{\mathbf{x}}(k+1)=\left(\mathbf{I}_n\otimes \bar{\mathbf{A}}\right)\bar{\mathbf{x}}(k)+\left(\mathbf{I}_n\otimes \begin{bmatrix}
        \bar{\mathbf{S}}&\mathbf{\bar{\Lambda}}
    \end{bmatrix}\right)\bar{\mathbf{u}}.
\end{equation}
where 
\begin{equation}\label{sb}
    \bar{\mathbf{A}}=\mathbf{Q}\mathbf{A}\mathbf{Q}^T,
\end{equation}
\begin{equation}\label{rb}
    \bar{\mathbf{S}}=\mathbf{Q}\mathbf{S},
\end{equation}
\begin{equation}\label{lb}
    \bar{\mathbf{\Lambda}}=\mathbf{Q}\mathbf{\Lambda}\mathbf{Q}^T.
\end{equation}



\end{theorem}
\begin{proof}
    When every regular agent updates its opinion by Eq. \eqref{opinionevolutionindividual}, the network opinion dynamics is obtained by
    \begin{equation}\label{proof1}
        \mathbf{x}\left(k+1\right)=\left(\mathbf{I}_n\otimes \mathbf{A}\right)\mathbf{x}\left(k\right)+\left(\mathbf{I}_n\otimes\begin{bmatrix}
            \mathbf{S}&\mathbf{\mathbf{\Lambda}}
        \end{bmatrix} \right)\mathbf{u}
    \end{equation}
    Let both sides of  Eq. \eqref{proof1} be pre-multiplied by $\mathbf{I}_n\otimes \mathbf{Q}$, and $\mathbf{x}\left(k\right)$ and $\mathbf{u}$ be substituted by the following terms:
    \[    \mathbf{x}\left(k\right)=\left(\mathbf{I}_n\otimes\mathbf{Q}^T\right)\left(\mathbf{I}_n\otimes\mathbf{Q}\right)\mathbf{x}\left(k\right)=\left(\mathbf{I}_n\otimes\mathbf{Q}^T\right)\bar{\mathbf{x}}\left(k\right)
    \]
    \[    \mathbf{u}=\left(\mathbf{I}_n\otimes\mathbf{H}^T\right)\left(\mathbf{I}_n\otimes\mathbf{H}\right)\mathbf{u}=\left(\mathbf{I}_n\otimes\mathbf{H}^T\right)\bar{\mathbf{u}}
    \]
    Then, Eq. \eqref{proof1} is converted to
    \begin{equation}\label{proof10}
        \bar{\mathbf{x}}\left(k+1\right)=\left(\mathbf{I}_n\otimes \left(\mathbf{Q}\mathbf{A}\mathbf{Q}^T\right)\right)\bar{\mathbf{x}}\left(k\right)+\left(\mathbf{I}_n\otimes \left(\mathbf{Q}\begin{bmatrix}
            \mathbf{S}&\mathbf{\Lambda}
        \end{bmatrix}\mathbf{H}^T\right)\right)\bar{\mathbf{u}}
        .
    \end{equation}
    Per Eqs. \eqref{sb}, \eqref{rb}, and \eqref{lb}, we can replace $\bar{\mathbf{A}}=\mathbf{Q}\mathbf{A}\mathbf{Q}^T$ and $\begin{bmatrix}
            \bar{\mathbf{S}}&\bar{\mathbf{\Lambda}}
        \end{bmatrix}=\mathbf{Q}\begin{bmatrix}
            \mathbf{S}&\mathbf{\Lambda}
        \end{bmatrix}\mathbf{H}^T$ into Eq. \eqref{proof10}. Thus, we obtain the network opinion dynamics in the form of Eq. \eqref{convertednetwork}.
\end{proof}
\begin{theorem}\label{thm4}
   Assume agents who influence of every $i\in \mathcal{V}_R$ are  defined by $\mathcal{N}_i$ and assigned such that 
   \begin{equation}\label{assum39}
        \left(\bigwedge_{l\in \mathcal{M}}\bigwedge_{i\in \mathcal{V}_{l}}\left(\mathcal{N}_i\subset \mathcal{W}_{l}\right)\right).
    \end{equation}
   Then, matrix  $\bar{\mathbf{A}}\in \mathbb{R}^{N_R\times N_R}$ is reducible and obtained by
    \begin{equation}
        \bar{\mathbf{A}}=\begin{bmatrix}
            \bar{\mathbf{A}}_{11}&\cdots&\mathbf{0}\\
            \vdots&\ddots&\vdots\\
            \bar{\mathbf{A}}_{M1}&\cdots&\bar{\mathbf{A}}_{MM}\\
        \end{bmatrix}
        ,
    \end{equation}
    \begin{equation}\label{barS}
        \bar{\mathbf{S}}=\begin{bmatrix}
            \mathbf{Q}_1\mathbf{S}\\
            \mathbf{0}_{\left(N_R-N_1\right)\times \left(N-N_R\right)}
        \end{bmatrix}
        \in\mathbb{R}^{N_R\times \left(N-N_R\right)}.
    \end{equation}
\end{theorem}
% \begin{proof}
    
% \end{proof}
% \begin{theorem}\label{thm4}
%    Assume influencers of every $i\in \mathcal{V}_R$, defined by $\mathcal{N}_i$, are assigned such that 
%    \begin{equation}
%         \left(\bigwedge_{l\in \mathcal{M}}\bigwedge_{i\in \mathcal{V}_{l}}\left(\mathcal{N}_i\subset \mathcal{W}_{l}\right)\right).
%     \end{equation}
%    Then, matrix  $\bar{\mathbf{A}}\in \mathbb{R}^{N_R\times N_R}$ is reducible and obtained by
%     \begin{equation}
%         \bar{\mathbf{A}}=\begin{bmatrix}
%             \bar{\mathbf{A}}_{11}&\cdots&\mathbf{0}\\
%             \vdots&\ddots&\vdots\\
%             \bar{\mathbf{A}}_{M1}&\cdots&\bar{\mathbf{A}}_{MM}\\
%         \end{bmatrix}
%         .
%     \end{equation}
% \end{theorem}
\begin{proof}
     The agent $i\in \mathcal{V}_p\subset \mathcal{V}_R$ is not influenced by any agent $j\in \mathcal{V}_q\subset \mathcal{V}_R$ if $q>p$. This implies that $(i,j)$ entries of matrices $\mathbf{A}$ and $\mathbf{W}$, denoted by $A_{i,j}$ and $W_{i,j}$, respectively, are both zero, for every $i\in \mathcal{V}_p$ and every $j\in \mathcal{V}_q$. Therefore, the entries of every row of the matrix $\bar{A}_{pq}=\mathbf{Q}_p\mathbf{A}\mathbf{Q}_q^T$ are zero, which in turn implies that $\bar{\mathbf{A}}_{pq}=\mathbf{0}_{N_p\times N_q}$. Also, $\mathbf{Q}_l\mathbf{S}=\mathbf{0}_{N_l\times \left(N-N_R\right)}$ if $l\in \mathcal{M}\setminus \left\{1\right\}$ because only $V_1$'s agents access the opinions of the stubborn agents that are defined by $\mathcal{V}_S$. Therefore, $\bar{\mathbf{S}}=\mathbf{Q}\mathbf{S}$ simplifies to the expression in Eq. \eqref{barS}.
\end{proof}
Per Theorem \ref{thm4}, if condition \eqref{assum39} is satisfied, the opinion evolution dynamics simplify to
\begin{equation}
    \bar{\mathbf{x}}_l(k+1)=\begin{cases}        \bar{\mathbf{A}}_{11}\bar{\mathbf{x}}_1(k)+\bar{\mathbf{\Lambda}}_{11}\bar{\mathbf{x}}_1(0)+\bar{\mathbf{S}}_1\bar{\mathbf{u}}_S&l=1\in \mathcal{M}\\
        \sum_{h=1}^l\bar{\mathbf{A}}_{lh}\bar{\mathbf{x}}_h(k)+\bar{\mathbf{\Lambda}}_{ll}\bar{\mathbf{x}}_l(0)&l=\mathcal{M}\setminus \left\{1\right\}\\
    \end{cases}
    ,
\end{equation}
where 
\begin{equation}
    \bar{\mathbf{S}}_1=\mathbf{Q}_1\mathbf{S}.
\end{equation}
\begin{theorem}\label{DNNthm}
    Assume inter-agent influences are such that the in-neighbor set $\mathcal{N}_i$ satisfy the following condition:
    \begin{equation}\label{DNN}
        \left(\bigwedge_{i\in \mathcal{V}_1}\left(\mathcal{N}_i\in \mathcal{V}_S\right)\right)\wedge\left(\bigwedge_{l\in \mathcal{M}\setminus \left\{1\right\}}\bigwedge_{i\in \mathcal{V}_l}\left(\mathcal{N}_i\subset \mathcal{W}_{l-1}\right)\right),
    \end{equation}
    where $\mathcal{W}_l$ is defined by Eq. \eqref{Wl} for every $l\in \mathcal{M}$. 
    Then, the opinion of every regular agent $i$ converges to its final values in $M$ time steps, where $M$ is the number of layers of the DNN.
    % \begin{enumerate}
    % \item inter-agent influences, defined by graph $\mathcal{G}$, can be represented by a deep neural network with $M$ layers, and
    % \item the opinion of every regular agent $i$ converges to its final values in $M$ time steps, where $M$ is the number of layers of the DNN.
    % \end{enumerate}    
\end{theorem}
\begin{proof}
If the theorem's assumption is satisfied, then the diagonal blocks of matrix $\bar{\mathbf{A}}\in \mathbb{R}^{N_R\times N_R}$ are all zero which in turn implies that eigenvalues of $\bar{\mathbf{A}}$ are  all zero, and as the result,
% . By definingg
% \begin{equation}
%     \bar{d}_l=\mathbf{I}_\otimes \left(\mathbf{Q}_l\begin{bmatrix}
%         \mathbf{S}&\mathbf{\Lambda}
%     \end{bmatrix}\right)\mathbf{u},
% \end{equation}
the opinion of the layer $l\in \mathcal{M}$ is updated by
\begin{equation}\label{bad}
    \bar{\mathbf{x}}_l(k+1)=\begin{cases}        \bar{\mathbf{\Lambda}}_{11}\bar{\mathbf{x}}_1(0)+\bar{\mathbf{S}}_1\bar{\mathbf{u}}_S&l=1\in \mathcal{M}\\
        \sum_{h=1}^{l-1}\bar{\mathbf{A}}_{lh}\bar{\mathbf{x}}_h(k)+\bar{\mathbf{\Lambda}}_{ll}\bar{\mathbf{x}}_l(0)&l=\mathcal{M}\setminus \left\{1\right\}\\
    \end{cases}
    .
\end{equation}
% \begin{equation}\label{bad}
%     \bar{\mathbf{x}}_l(k+1)=\begin{cases}
%         \bar{\mathbf{d}}_l&l=1\\
%         \sum_{h=1}^{l-1}\left(\mathbf{I}_n\otimes\bar{\mathbf{A}}_{l,h}\right)\bar{\mathbf{x}}_h(k)&l\in \mathcal{M}\setminus\left\{1\right\}
%     \end{cases}
%     .
% \end{equation}
In Eq. \eqref{bad}, $\bar{\mathbf{x}}_l$ converges to its final value after $\bar{\mathbf{x}}_{l-1}$ converging to the  final opinion vector, where every $l\in \mathcal{M}\setminus\left\{1\right\}$, where $\bar{\mathbf{x}}_1$ converges to the final opinion vector at $k=1$. Therefore, agents of layer $l\in \mathcal{M}$ converge in $l$ time steps.
\end{proof}
Note that the influence graph $\mathcal{G}$ can be converted to a deep neural network when the assumption of Theorem \ref{DNNthm}, given by Eq. \eqref{DNN}, is satisfied. This is because agents of are classified into $M$ district groups, specified by $\mathcal{V}_1$ through $\mathcal{V}_M$, where agents $\mathcal{V}_l$ do not influence each other for every $l\in \mathcal{M}$. The input layer of the DNN consists of $N-N_R$ neurons where each neuron uniquely represents a stubborn agent.   


For better clarification, Figure \ref{DNNCommunicaty} illustrates a schematic of a community consisting of $13$ people where they are identified by set $\mathcal{V}=\left\{1,\cdots,13\right\}$. We can express this $\mathcal{V}$ as $\mathcal{V}=\mathcal{V}_S\cup \mathcal{V}_R$ where disjoint subsets  $\mathcal{V}_S=\left\{1,2,3,4,14\right\}\subset \mathcal{V}$ and  $\mathcal{V}_R=\mathcal{V}\setminus \mathcal{V}_S$ define stubborn and regular agents, respectively. We can express $\mathcal{V}_R$ as $\mathcal{V}_R=\mathcal{V}_1\cup\mathcal{V}_2\cup\mathcal{V}_3$ where $\mathcal{V}_1=\left\{5,6,10,11\right\}$, $\mathcal{V}_2=\left\{8,9,13\right\}$, and $\mathcal{V}_3=\left\{7\right\}$. The inter-agent influences shown in Fig. \ref{DNNCommunicaty}(a) can be converted to the DNN shown in Fig. \ref{DNNCommunicaty}(b). The DNN' input layer consists of five neurons representing the stubborn agents. The DNN has three interior layers; therefore, $\mathcal{M}=\left\{1,2,3\right\}$. Note that the neurons of layers $1$ through $3$ are defined by Eq. \eqref{Wl} as follows: $\mathcal{W}_1=\mathcal{V}_S\cup\mathcal{V}_1$, $\mathcal{W}_2=\mathcal{W}_1\cup\mathcal{V}_2$, $\mathcal{W}_3=\mathcal{W}_2\cup\mathcal{V}_3$.


\section{Decentralized Acquisition of Biases and Influences}\label{Decentralized Acquisition of Biases and Influences}
Every agent $i\in \mathcal{V}$ that satisfies Assumption \ref{knownutility} is aware of its initial reward, $u_i(0)=\mathcal{U}\left(\mathbf{o}_i(0)\right)$ and $u_j(k)=\mathcal{U}\left(\mathbf{o}_j(k)\right)$ which is the current reward received by every in-neighbor $j\in \mathcal{N}_i$. Therefore, agent $i\in \mathcal{V}_R$ can obtain
\begin{equation}
    \bar{u}_i(k)=u_i(0)+\sum_{j\in \mathcal{N}_i}u_j(k)
\end{equation}
and set up matrix $\mathbf{L}(k)$ by defining $L_{ij}$ as follows:
\begin{equation}\label{Lij}
    L_{ij}(k)=
    \begin{cases}
        {u_j(k)\over\bar{u}_i(k)}&j\in \mathcal{N}_i\\
        {u_i(0)\over\bar{u}_i(k)}&j=N+i\\
        0&\mathrm{otherwise}
    \end{cases}
    ,\qquad \forall i\in \mathcal{V}_R.
\end{equation}
By applying Eq. \eqref{Lij}, to define matrix $\mathbf{L}$, every row of matrix $\mathbf{L}$ sums up to $1$. Agent $i\in \mathcal{V}_R$ can also obtain its own bias $\lambda_i(k)$ and its own reliance of agent $j\in \mathcal{N}_i$ by
\begin{equation}
    \lambda_{i}(k)=L_{i,N+i}(k),\qquad \forall i\in \mathcal{V}_R,
\end{equation}
\begin{equation}
    w_{i,j}(k)={L_{i,j}(k)\over 1-L_{i,N+i}(k)},\qquad \forall i\in \mathcal{V}_R,~j\in \mathcal{N}_i.
\end{equation}

\begin{figure}[h]
\centering
\subfigure[]{\includegraphics[width=0.98\linewidth]{HumanNetwork-V2.png}}
\subfigure[]{\includegraphics[width=0.98\linewidth]{DNNUpdated.png}}
% \subfigure[]{\includegraphics[width=0.32\linewidth]{w-37-4-V2.eps}}
% \subfigure[]{\includegraphics[width=0.32\linewidth]{w-37-28-V2.eps}}
% \subfigure[]{\includegraphics[width=0.32\linewidth]{x37-V2.eps}}
% \subfigure[]{\includegraphics[width=0.32\linewidth]{y37-V2.eps}}
 % \vspace{-0.5cm}
\caption{\label{DNNCommunicaty} The example inte-agent influences shown in sub-figure (a)  is converted into a DNN shown in sub-figure (b).
% (\textit{\textbf{c}}) Functionality of the approach for abstract representation of targets.
}
% \label{Scomweight}
\end{figure}
Note that 
\begin{equation}
    \sum_{j\in \mathcal{N}_i}w_{i,j}(k)=1,\qquad \forall k\in \mathbb{Z},~\forall i\in \mathcal{V}_R.
\end{equation}
\section{Simulation Results}\label{Simulation Results}
In this section, we present the simulation results for two dimensional opinion evolution under DNN-based reducible and irreducible networks. To generate the results, we consider opinion evolution over two-dimensional space $o_1-o_2$ and  define utility function as two-dimensional Gaussian distribution with mean vector $\mu=\begin{bmatrix}0.25&0.6\end{bmatrix}^T$ and covariance matrix $\mathbf{\Sigma}=0.1\mathbf{I}_2$.
  
\begin{figure}[ht]
\centering
\includegraphics[width=0.48 \textwidth]{InitialDistribution-V3.png}
\caption{Initial distribution of of $\mathcal{V}$ in the opinion space $o_1-o_2$. The red and black  nodes represent ``stubborn'' and ``regular'' agents, respectively.}
\label{initialdistribution} 
\end{figure}
\subsection{Opinion Evolution under DNN-Based Reducible Network}
We consider a community of $57$ agents define by set $\mathcal{V}=\left\{1,\cdots,57\right\}$. Set $\mathcal{V}$ can be expressed as  $\mathcal{V}=\mathcal{V}_S\cup\mathcal{V}_R$, where $\mathcal{V}_S=\left\{1,\cdots,5\right\}$ and $\mathcal{V}_R=\left\{6,\cdots,57\right\}$ identify the stubborn and regular agents, respectively. We divide regular agents into three groups; therefore, $\mathcal{V}_R$ can be expressed as $\mathcal{V}_R=\mathcal{V}_1\cup\mathcal{V}_2\cup \mathcal{V}_3$, where $\mathcal{V}_1=\left\{6,19,32,45\right\}$, $\mathcal{V}_2=\left\{7,8,9,20,21,22,33,34,35,46,47,48\right\}$, and $\mathcal{V}_3=\mathcal{V}\setminus\left(\mathcal{V}_1\cup\mathcal{V}_2\right)$. The initial configuration of the agent team in the opinion space $o_1-o_2$ is shown in Fig. \ref{initialdistribution}. The communication among the  regular agents is defined by a deep neural network with $M=3$ internal layers as shown in Fig. \ref{DNN}, where the input layer consists of five neurons representing the stubborn agents.




\begin{figure*}[ht]
\centering
\includegraphics[width=0.98 \textwidth]{DNNCommunication.png}
\caption{Inter-agent communication is defined by a deep neural network with $M=3$ internal layers. }
\label{DNN} 
\end{figure*}



\begin{figure}[ht]
\centering
\includegraphics[width=0.45 \textwidth]{FinalConf.eps-Comunication.png}
\caption{The reward distribution over the opinion space and the final opinion of the community.}
\label{Reward} 
\end{figure}


\begin{figure}[ht]
\centering
\includegraphics[width=0.45 \textwidth]{O1.png}
\caption{The $o_1$ (first) component of opinion of all regular agents versus discrete time $k$.It is seen that the first component of all regular agents converges to its final value in $M=3$ time steps.}
\label{O11} 
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=0.45 \textwidth]{O2.png}
\caption{The $o_2$ (second) component of opinion of all regular agents versus discrete time $k$. It is seen that the second component of all regular agents converges to its final value in $M=3$ time steps.}
\label{O22} 
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=0.45 \textwidth]{InitialDistributionSteering.png}
\caption{Initial distribution of $\mathcal{V}$ in the opinion space $o_1-o_2$. The red and black  nodes represent ``stubborn'' and ``regular'' agents, respectively.}
\label{InitialDistributionSteering} 
\end{figure}


\begin{figure}[ht]
\centering
\includegraphics[width=0.45 \textwidth]{Convergence.png}
\caption{Steering opinion through containment control.}
\label{Convergence} 
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=0.45 \textwidth]{InitialDistribution.png}
\caption{Initial distribution of of $\mathcal{V}$ in the opinion space $o_1-o_2$. The red and black  nodes represent ``stubborn'' and ``regular'' agents, respectively.}
\label{initialdistributionIrreducible} 
\end{figure}
\begin{figure}[ht]
\centering
\includegraphics[width=0.45 \textwidth]{O1CompRegular.png}
\caption{The $o_1$ (first) component of opinion of all regular agents versus discrete time $k$.}
\label{O11-V2} 
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=0.45 \textwidth]{O2CompRegular.png}
\caption{The $o_2$ (second) component of the opinions of all regular agents versus discrete time $k$. }
\label{O22-V2} 
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=0.45 \textwidth]{FinalConfReward.png}
\caption{The reward distribution over the opinion space and the final opinion of the community.}
\label{RewardV2} 
\end{figure}


Figure \ref{Reward} shows the reward distribution defined over the opinion space $o_1-o_2$ and the final opinion of the regular agents (black dots) and stubborn agents (red dots), where the regular agents achieve the final opinion in a truly decentralized fashion by updating their biases and influences using the formula given in Section \ref{Decentralized Acquisition of Biases and Influences}. Figurs \ref{O11} and \ref{O22} plot $o_1$ and $o_2$ components of all regular agents versus discrete time $k$ for $k=0,\cdots,7$. It is observed that operations of regular agents converge to their final vlues in $M=3$ time steps, as proven by Theorem \ref{DNNthm}. 



\subsection{Opinion Evolution under Irreducible Network}
We consider a community consisting of $N=100$ agents defined by set $\mathcal{V}=\left\{1,\cdots,100\right\}$. Supposing there exist four stubborn agents, we express $\mathcal{V}$ as $\mathcal{V}=\mathcal{V}_R\cup\mathcal{V}_S$, where $\mathcal{V}_R=\left\{1,\cdots,96\right\}$ and $\mathcal{V}_S=\left\{97,\cdots,100\right\}$ identify regular and stubborn agents, respectively.  

\subsubsection{Convergence and Opinion Steering under Irreducible Network}
We consider an opinion evolution scenario in which the stubborn agents' opinions do not lie at the boundary of the agents' initial opinion configuration. Let agents be initially distributed in the opinion space as shown in Fig. \ref{InitialDistributionSteering} with stubborn agents forming a rectangular convex hull that does not contain all the regular agents. For simulation,  without loss of generality, we assume  that the edge set $\mathcal{E}_k=\mathcal{E}$ is time-invariant. However, the communication weights and biases of regular agents are time-varying and updated through the reward maximization strategy presented in Section \ref{Decentralized Acquisition of Biases and Influences}. Figure \ref{Convergence} shows that the final opinions of all regular agents are steered from an arbitrary initial distribution towards  the rectangular convex hull that  is obtained based on the opinions of the stubborn agents.


% \label{initialdistributionIrreducible} 

\subsubsection{Opinion Evolution under Time-Varying and Random Network}
We consider the same reward distribution shown in Fig. \ref{Reward} (i.e. reward distributions in Figs. \ref{Reward} and \ref{RewardV2} are the same). The initial distribution of the agents in the opinion space $o_1-o_2$ is shown in Fig. \ref{initialdistributionIrreducible}. We suppose that regular agents are randomly influenced by some other regular or stubborn agents. Therefore, the edge set $\mathcal{E}_k\subset \mathcal{V}\times \mathcal{V}$ is stochastic and time-varying. The $o_1$ and $o_2$ components of regular agents are plotted versus discrete time $k$ in Figs. \ref{O11-V2} and \ref{O22-V2}, respectively. Figures \ref{O11-V2} and \ref{O22-V2} illustrate that the regular agents tend to their final opinions at every discrete time $k>4$. Figure \ref{RewardV2}    the configuration of regular agents at discrete time $k=20$ in the opinion space $o_1-o_2$.

\section{Conclusion}\label{Conclusion}
This paper models the FJ opinion evolution dynamics as a containment control problem; stubborn agents are considered as leaders specifying a desired opinion distribution of the society. It was proven and demonstrated that regular agents can achieve a desired opinion distribution in a truly decentralized fashion through random communication with some other (regular or stubborn) agents. The paper assumed that every regular agent is completely open to following the reward maximization rule suggested in Section \ref{Decentralized Acquisition of Biases and Influences} to assign its communication weight  based on the rewards acquired by its own in-neighbors (influences). For the future work, we will relax this assumption and develop a game-theoretic framework to develop a game-theoretic framework to maximize strategies for both regular and obstinate agents. This will allow the stubborn agents to modify the reward distribution in response to the regular agents' responses. 
\section{Acknowledgement}
 This work was supported by the
National Science Foundation under Award 2133690 and Award 1914581.
\bibliographystyle{IEEEtran}
\bibliography{reference}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Rastgoftar.jpg}}]
{\textbf{Hossein Rastgoftar}} an Assistant Professor at the University of Arizona. Prior to this, he was an adjunct Assistant Professor at the University of Michigan from 2020 to 2021. He was also an Assistant Research Scientist (2017 to 2020) and a Postdoctoral Researcher (2015 to 2017) in the Aerospace Engineering Department at the University of Michigan Ann Arbor. He received the B.Sc. degree in mechanical engineering-thermo-fluids from Shiraz University, Shiraz, Iran, the M.S. degrees in mechanical systems and solid mechanics from Shiraz University and the University of Central Florida, Orlando, FL, USA, and the Ph.D. degree in mechanical engineering from Drexel University, Philadelphia, in 2015. 
% His current research interests include dynamics and control, multiagent systems, cyber-physical systems, and optimization and Markov decision processes.
\end{IEEEbiography}

\end{document}
