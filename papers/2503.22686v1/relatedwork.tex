\section{Related Work}
\label{sec:related-work}
The proliferation of cyber information influence across digital platforms poses significant challenges in various domains, including public health, politics, and finance [37, 55]. Further, the notion that online discourse or shared information might be of dubious value distorts social norms. This is because cyber information influence, while once exclusive to nation state soft power doctrine, has filtered down to a variety of technology augmented social strata. Certainly, large campaigns require state level resources and coordination. However, individual social media users as well as small ideologically aligned groups can exert cyber information influence across a plethora of knowledge domains. 

\subsection{Misinformation, Disinformation, and Fake News}
Cyber information influence is normally understood to consist of three categories. While misinformation refers to the unintentional spread of false or misleading information, disinformation is the deliberate dissemination of deceptive content with the intent to mislead audiences. Fake news, then, is disinformation masquerading as legitimate news. The increasing accessibility of social media and the ease of content generation have exacerbated the rapid spread of disinformation in specific, overwhelming traditional fact-checking efforts that rely on manual verification by human experts [40]. As a result, researchers have explored automated solutions to enhance the detection and mitigation of cyber information influence [15, 42, 43].

\subsection{Evolution of Detection Techniques}
Broadly speaking, there are four groups of cyber information influence detection techniques (Table \ref{table:1}). Each represents a step in the overarching chronology of technological advancement. 

\begin{table*}[!htbp]
  \centering
  \begin{threeparttable} \caption{Summary of Cyber Information Influence Detection Approaches}
      \begin{tabular}{lccc}
          \toprule
          {\textbf{Approach}} & \textbf{Strengths} & \textbf{Weaknesses} & \textbf{Best Use Case} \\
          \hline
          \hline
          Rule-based & Simple & Rigidity & Structured \\
          & Interpretable & High false-positive rate & Well-defined \\ \hline 
          Traditional ML & Scalable & Context limited & News \\
          & Data-driven & Requires feature engineering & Structured text \\ 
          & Accurate & & \\ \hline 
          Deep learning & Context-aware & Opaque & Social media \\
          & Feature automation  & Compute expensive & Large datasets \\ \hline 
          Hybrid & Context-rich detection & Resource intense & Multi-platform or media \\
          & Complex integration & & \\
          \bottomrule
      \end{tabular}
      \label{table:1}

  \end{threeparttable}
\end{table*}

\subsubsection{Rule-Based}
Early cyber information influence detection efforts primarily employed rule-based approaches [48], which relied on handcrafted linguistic features, keyword matching, and syntactic patterns to flag content [46, 65]. Such approaches utilized predefined dictionaries, syntactic patterns, and stylistic features to target characteristics such as sensational language, clickbait phrases, and linguistic anomalies [28, 44]. However, rule-based methods suffer from three significant limitations. Foremost, such methods are rigid. In other words, rules-based approaches lack adaptability to evolving misinformation tactics [30, 36]. Consequently, rules-based techniques tend to exhibit high false-positive rates. These methods often flag legitimate content incorrectly, especially when evaluating data even moderately outside the initial configuration or rule parameters. Thus, a third limitation can be contextualized as domain dependency. That is, as with any rule-drive system, the rules must be constantly maintained if the system is to retain effectiveness in its task. As a result, rule-based approaches were gradually replaced by more flexible ML techniques that could learn patterns directly from data [34, 45, 47].

\subsubsection{Traditional ML}
To overcome the rigidity of rule-based systems, researchers adopted ML approaches. ML introduced statistical methods to automatically learn patterns from data. Early attempts employed ML models such as SVM, DT, and RF leveraged handcrafted features such as n-grams, readability scores, and sentiment polarity to differentiate between legitimate and misleading content [12]. As a general method, ML addresses the rigidity found in rules-based approaches thereby obviating domain dependency to various degrees of success. ML also scales better without the need for hands-on maintenance requirements. Yet, for all the advantages compared to rule-based methods, ML brought along a new set of limitations. Effectiveness and performance depend on careful feature engineering by human operators. Therefore, appropriate domain knowledge is necessary to engineer a robust and reliable implementation. Moreover, a nontrivial volume of problem-space specific data is required to properly train and tune ML detection techniques. This renders the ML model tightly coupled to both the selected features and problem definitions. Such is not conceptually different from the rules-based limitations insofar as the result is a nontrivial false positive rate. Further, ML has contextual understanding limitations because models cannot capture semantic relationships within text. These shortcomings led to the adoption of deep learning models, which offered automatic feature learning capabilities [44, 45].

\subsubsection{Deep Learning}
Enter deep learning models. These include Convolutional Neural Networks (CNNs), Long Short-Term Memory (LSTM) networks, and Transformer-based architectures [32] (e.g., BERT, RoBERTa). These more modern approaches mark a significant advancement in cyber information influence detection [41]. To that end, deep learning models have demonstrated superior performance largely due to contextual embeddings and attention mechanisms capturing the subtle patterns that traditional ML approaches fail to handle. Deep learning also eliminates the need for manual feature engineering. However, challenges persist, such as requiring large, labeled datasets to achieve high performance [65]. Furthermore, deep learning has high computational complexity and high resource requirements. Both limit deployment feasibility. Meanwhile, deep learning models are black box in nature making it difficult to understand model decisions [35, 63, 64]. One can reasonably deduce then that despite the strengths of deep learning, the method alone is not sufficient for robust misinformation detection. Hence, the exploration of hybrid approaches incorporating additional contextual signals [40].

\subsubsection{Hybrid}
To address the limitations of content-based approaches, recent research has shifted towards hybrid models, which combine textual analysis with auxiliary features such as social network structures, temporal patterns, and user credibility scores [41, 49]. The motivation is to assess the reliability of content sources in addition to tracing user engagement patterns through interaction graphs. Overall, hybrid models have demonstrated improved robustness by leveraging multimodal data. Indeed, hybrid approaches have improved detection accuracy compared to traditional and deep learning techniques. However, hybrid methods introduce additional complexity and require significant computational resources [12, 20, 33].

\subsection{Need for Meta-Analysis}
Despite significant advancements in cyber information influence detection techniques over time, one can observe three persistent challenges across all approaches. The literature demonstrates variability in datasets, feature selection, and evaluation metrics across studies. The variability limits the ability for one to determine the most appropriate technique for a given context. High volumes of data can be a challenge for real-time detection schemes as well. Moreover, even when a technique exists capable of such processing, the computational demands scale in lockstep which makes experimentation, let alone production implementations, costly.  Like the constraints associated with rigidity or tight coupling to training datasets observed in earlier techniques, deep learning models often perform well on benchmark datasets but struggle in applied, practical scenarios. Further, existing literature focuses on qualitative assessments of detection technique performance with broad variance in quantitative result reporting. Often, the literature presents conflicting results due to variations in study design and datasets used. To that end, a meta-analysis is crucial to quantitatively assess the effectiveness of different AI-based misinformation detection techniques, identify consistent trends, and provide evidence-based recommendations for future research [20, 28].