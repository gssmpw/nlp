\section{Related Work}
\noindent{\textbf{Language Model Applications in Meetings.}} Considerable research has been dedicated to the summarization of meetings**Vaswani, "Attention Is All You Need"** and other real-life dialogues**Brown et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**. In the context of meetings, key tasks include meeting transcript summarization and action item identification**Chen et al., "Deep Learning for Natural Language Processing (NLP) Tasks"**. MeetingQA**Chen et al., "MeetingQA: A Dataset for Meeting Question Answering"** investigated Q\&A tasks based on meeting transcripts, highlighting the challenges faced by models such as RoBERTa in handling real-world meeting data. Recent advancements in LLMs have opened new avenues for enhancing these tasks. For instance, an LLM-based meeting recap system**Yang et al., "Leveraging Pre-trained Models for Meeting Recap"** has demonstrated effectiveness in generating accurate and coherent summaries and action items.

% Meeting recap including summary and actions has been a popular topic to explore in Natural Language Processing (NLP) domain \xt{refs}. MeetingQA \xt{ref} explored Q\&A tasks based on meeting transcripts, i.e. identifying answers to questions asked during discussion among meeting participants, and found it is a challenging real-world task for models such as RoBERTa and DeBERTa \xt{refs? better description?}. Recent advancements in LLMs have brought new opportunities. ____ present the design and implementation and evaluation in-context a meeting recap system. \xt{comment on effectiveness in one sentence.}

%A lot of great research works have focused on summarization of meetings**Yang et al., "Deep Learning for Meeting Summarization"** and other real-life dialogues**Brown et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**. Specifically in meeting scenario, the tasks are meeting transcript summarization and action item identification**Chen et al., "Deep Learning for Natural Language Processing (NLP) Tasks"**. MeetingQA**Chen et al., "MeetingQA: A Dataset for Meeting Question Answering"** investigated Q\&A tasks based on meeting transcripts, aiming to identify answers to questions posed during discussions, highlighting the challenges faced by models like RoBERTa in handling real-world meeting data. Recent advancements in LLMs have opened new avenues for improving these tasks. For instance, a LLM-based meeting recap system**Yang et al., "Leveraging Pre-trained Models for Meeting Recap"** demonstrates its effectiveness in generating accurate and coherent summaries and action items.

\noindent{\textbf{Facilitator in Multi-Participant Chat.}} MUCA **Zhang et al., "Multimodal User-Centered Architecture for Meetings"** presents a framework that leverages LLMs to facilitate group chats by simulating users, demonstrating notable effectiveness in goal-oriented conversations. Similarly, approaches like GPT-4o demo for meetings**Brown et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"** are designed to serve as facilitators in group discussions. While these studies underscore LLMsâ€™ capabilities in managing group chats, they primarily focus on LLMs guiding the meeting process rather than representing individuals with different roles.

%designed to serve as facilitators in group discussions, but they lack the specific goal of knowledge-enabled participation. While these works highlight the capabilities of LLMs in managing group chats, they do not address LLMs' ability as a participant. Additionally, they focus on general facilitation skills rather than targeted engagement within the specific context of meetings.

\noindent{\textbf{Role-Playing with LLMs: Characters and Digital Twins.}}
% Digital twin has always been a hot topic. Recently Reid Hoffman released an interview between himself and his digital version by fine-tuning GPT4 (\xt{double check}). Though impressive and shed a preview of digital representation, the capability demonstration in this interview is only limited to the 1:1 scenario and the group discussion scenario remain unexplored. In addition, the attempt of simulating a famous person have also intrigued works such as \xt{refs}. But these works focus on try to see whether LLM can stay within character or study social activity by agent group chat social environment.
%The concept of digital twins has been a prominent topic in recent research. 
Role-play prompting**Zhang et al., "Role-Play Prompting for Chain-of-Thought Reasoning"** has been shown to be a more effective trigger for the chain-of-thought process in LLMs. Additionally, efforts to simulate famous personalities**Brown et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"** have garnered interest, leading to research on maintaining character consistency and studying social interactions within agent-based group chat environments. Recently, Reid Hoffman**Hoffman, "An interview between himself and his digital twin built on GPT-4"** showcased an interview between himself and his digital twin built on GPT-4. Although this demonstration highlighted the potential of digital representations, it was confined to one-on-one interactions, leaving the complexities of group discussions unexplored. Unlike previous work, our work focus on LLMs as meeting participant delegates, delivering targeted engagement tailored to multi-participant, meeting-specific objectives. Our comprehensive evaluation and real-world deployment further demonstrate the system's potential to significantly reduce the burden of meetings on individuals, thereby advancing the application of LLMs in professional environments.
%significantly differs from these previous efforts by focusing on the deployment of LLMs as meeting delegates. Unlike role-play and digital twin applications that primarily aim to replicate individual personalities or enhance chain-of-thought processes, our approach aims to enable LLMs to participate in and facilitate real-time meeting scenarios. This involves not only maintaining character consistency but also addressing specific meeting objectives, improving content accuracy, and ensuring timely responses. This focus on practical, real-world application in a dynamic, multi-participant setting makes our work a valuable contribution to the field, providing insights into the challenges and opportunities of using LLMs to alleviate the burden of meetings on individuals.

%\xt{need more and differentiation, emnlp/acl refs!}
%\xt{meeting related}
%\xt{simulated: better not to touch on this topic as we don't focus on that}
%\jz{mention the difference between group discussion vs. 1:1 as  Reidhoffman }

%