
\documentclass{article} % For LaTeX2e
\usepackage[preprint]{colm2025_conference}

\usepackage{microtype}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{graphicx} 
\usepackage{lineno}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{amsmath}
\definecolor{darkblue}{rgb}{0, 0, 0.5}
\hypersetup{colorlinks=true, citecolor=darkblue, linkcolor=darkblue, urlcolor=darkblue}


\title{Fast or Better? Balancing Accuracy and Cost in Retrieval-Augmented Generation with Flexible User Control}

%\title{Faster or Better: Enabling User Controlled Step Optimization of Retrieval-Augmented Generation}

%\title{Faster or Better: A Tuning Parameter that Enables User Control of Retrieval-Augmented Generation Optimization}

%\title{Faster or Better: A Tuning Parameter for Adaptive RAG that Enables User Control}

%\title{Faster or Better: An Adaptive Step Tuning Parameter for \\Retrieval Augmented Generation}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \colmfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.
\author{Jinyan Su$^1$, Jennifer Healey$^2$,
  Preslav Nakov$^3$,  Claire Cardie$^1$\\
  $^1$ Cornell University, 
  $^2$ Adobe Research,
  $^3$ Mohamed bin Zayed University of Artificial Intelligence\\
  \texttt{\{js3673, ctc9\}@cornell.edu, jehealey@adobe.com, preslav.nakov@mbzuai.ac.ae}
  }


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
%\newcommandnotesJen[1]{textbf{textcolor{blue}{Jen: #1}}}

\begin{document}

\ifcolmsubmission
\linenumbers
\fi

\maketitle

\begin{abstract}
Retrieval-Augmented Generation (RAG) has emerged as a powerful approach to mitigate large language model (LLM) hallucinations by incorporating external knowledge retrieval. However, existing RAG frameworks often apply retrieval indiscriminately,leading to inefficiencies—over-retrieving when unnecessary or failing to retrieve iteratively when required for complex reasoning. Recent adaptive retrieval strategies, though adaptively navigates these retrieval strategies, predict only based on query complexity and lacks user-driven flexibility, making them infeasible for diverse user application needs. In this paper, we introduce a novel user-controllable RAG framework that enables dynamic adjustment of the accuracy-cost trade-off. Our approach leverages two classifiers: one trained to prioritize accuracy and another to prioritize retrieval efficiency. Via an interpretable control parameter $\alpha$, users can seamlessly navigate between minimal-cost retrieval and high-accuracy retrieval based on their specific requirements. We empirically demonstrate that our approach effectively balances accuracy, retrieval cost, and user controllability \footnote{Code can be found at \url{https://github.com/JinyanSu1/Flare-Aug}.}, making it a practical and adaptable solution for real-world applications.
\end{abstract}

\section{Introduction}
Retrieval-Augmented Generation (RAG) 
\cite{lewis2020retrieval, khandelwal2019generalization, izacard2023atlas} has emerged as a promising approach to address large language models (LLM) hallucinations, outputs that appear accurate but are actually factually incorrect. Hallucinations more often occur when the facts necessary to answer a query are outside the scope of the model's training either because the facts are too recent, too obscure or answered by proprietary data \cite{trivedi2022interleaving, yao2022react}. By integrating retrieval modules, RAG enables LLMs to access and incorporate relevant external information, helping them stay updated with evolving world knowledge and reduce hallucination.  


Early work on retrieval-augmented LLMs primarily focused on single-hop queries \cite{lazaridou2022internet, ram2023context}, where a single retrieval step retrieves relevant information based solely on the query. However, many complex queries require multi-step reasoning to arrive at the correct answer. For instance, a question such as, "When did the people who captured Malakoff come to the region where Philipsburg is located?", cannot be answered with a single retrieval step. Instead, such queries necessitate an iterative retrieval process, where the model retrieves partial knowledge, reasons over it, and conducts additional retrievals based on intermediate conclusions \cite{trivedi2022interleaving, press2022measuring, jeong2024adaptive}. While multi-step retrieval enables deeper reasoning, it also incurs significant computational overhead. Conversely, for simple queries, LLMs may already encode sufficient knowledge to generate an accurate response without retrieval. However, many existing RAG frameworks apply retrieval indiscriminately, either over-relying on external knowledge when parametric memory suffices or failing to retrieve iteratively when deeper reasoning is required. This lack of adaptability results in inefficiencies: unnecessary retrieval increases latency and computational cost, while insufficient retrieval leads to incomplete or incorrect answers. Given these challenges, there is a growing need for adaptive RAG systems that dynamically adjust retrieval strategies based on query complexity \cite{mallen2023not}.



Adaptive-RAG \cite{jeong2024adaptive} approach have attempted to balance accuracy and retrieval cost by inferring question complexity with a classifier to categorize queries into the most suitable retrieval strategies (e.g., no retrieval, single-step retrieval, or multi-step retrieval). it remains impractical due to its inability to dynamically adjust the accuracy-cost trade-off. Specifically, it lacks user-driven flexibility, preventing fine-grained control over retrieval strategies to suit diverse application needs. In real-world scenarios, retrieval preferences vary depending on the task domain, computational constraints, and user priorities—some applications may prioritize minimal cost, while others require higher accuracy despite increased retrieval overhead.



To overcome these limitations, we propose
\textbf{Flare-Aug} (\textbf{FL}exible, \textbf{A}daptive \textbf{RE}trieval-\textbf{Aug}mented Generation), a user-controllable RAG framework that enables fine-grained adjustment of the cost-accuracy trade-off.
Flare-Agu introduces two classifiers: one trained to prioritize accuracy and another trained to prioritize cost. To seamlessly balance these two objectives, Flare-Aug incorporates an interpretable and easy-to-tune parameter $\alpha$, which controls the weighting between the two classifiers. This simple yet effective approach allows users to control their own accuracy and cost trade-off that best align with their specific requirements and user applications.




\section{Relative Work}
Recent research on adaptive retrieval has explored various strategies to determine when and how retrieval should be performed, often leveraging internal model states or external classifiers to optimize retrieval decisions.

Several works focus on leveraging LLM internal states to adapt retrieval dynamically. \cite{baek2024probing}  utilize hidden state representations from intermediate layers of language models to determine whether additional retrieval is necessary for a given query. Similarly, \cite{yao2024seakr} extract self-aware uncertainty from LLMs’ internal states to decide when retrieval should be performed. \cite{jiang2023active} propose an iterative retrieval approach, where the model predicts the upcoming sentence and uses it as a retrieval query if the sentence contains low-confidence tokens.
However, these methods require access to the LLM's internal states, making them impractical for many real-world applications, especially given the increasing reliance on proprietary, closed-source LLMs.


Another line of work trains external classifiers to predict retrieval necessity.
\cite{wang2023self} train a classifier for LLM self-knowledge, allowing models to switch adaptively between retrieval and direct response generation.
\cite{jeong2024adaptive} introduce an adaptive retrieval framework that dynamically selects among no retrieval, single-step retrieval, or multi-step retrieval based on query complexity. \cite{tang2024mba} propose a multi-arm bandit-based approach, where the model explores different retrieval strategies and optimizes retrieval choices based on feedback. \cite{wang2024adaptive} develop an adaptive retrieval framework for conversational settings, determining whether retrieval is necessary based on dialogue context.
\cite{mallen2023not} take an entity-centric approach, retrieving only when the entity popularity in a query falls below a certain threshold.

Despite these advancements, none of these approaches enable direct user control over retrieval strategies. 
\section{Problem Setting}
\subsection{Diverse User Query Complexities and Retrieval Strategies}
We begin by introducing the varying complexity of user queries and their corresponding retrieval strategies. As illustrated in Figure \ref{fig: diverse user query}, different levels of query complexity necessitate distinct retrieval approaches. Specifically, we consider three primary retrieval strategies: no retrieval, single-step retrieval, and multi-step retrieval, which correspond to zero, one, and multiple retrieval steps, respectively. These strategies align with the availability of relevant knowledge within the model’s parameters and the extent to which retrieval and reasoning must interact to generate an answer. \begin{figure*}[h]
    \centering
\includegraphics[width=1\linewidth]{Figures/diverse_user_query.png}
    \caption{Illustrations of diverse user queries, their corresponding query complexity and most suitable retrieval strategy.}
    \label{fig: diverse user query}
\end{figure*}

\paragraph{Parametric Knowledge Queries \& No Retrieval}
Some user queries are considered simple by LLMs because they can be directly answered using their parametric knowledge—information stored within the model’s parameters \citep{izacard2023atlas}. Since LLMs memorize widely known factual knowledge during pretraining, they can respond to such queries without relying on external retrieval. Scaling further enhances this memorization capability, improving the accuracy of responses to popular factual questions \citep{mallen2023not}. For example, queries such as \textit{"Who wrote Romeo and Juliet?"} or \textit{"What is the capital of France?"} can be answered accurately using only the model’s internal representations. For these retrieval-free queries, retrieval augmentation is unnecessary because the model already encodes the relevant knowledge. Introducing retrieval in such cases may be redundant or even counterproductive—it can increase latency, add computational cost, or introduce noise from irrelevant retrieved documents \citep{su2024towards}. Consequently, for these parametric knowledge queries that are answerable by memorized knowledge, such as well-known facts or frequently encountered knowledge, it is more efficient to use a no-retrieval strategy, allowing the model to generate responses and answer directly.
\paragraph{Knowledge-Deficient Queries \& Single-Step Retrieval} Some queries fall outside an LLM’s parametric knowledge, such as less popular, long-tail factual knowledge that was not sufficiently represented in pretraining data\citep{mallen2023not}; time-sensitive information such as recent events and  evolving regulations, or highly specialized domain knowledge that is uncommon in general corpora.
For instance, a query like \textit{"Who won the Nobel Prize in Physics in 2023?"} necessitates retrieval for models with a training cut-off date of 2022, as the event occurred afterward and is unlikely to be encoded in the model’s parameters. Similarly, domain-specific questions such as \textit{"What are the latest FDA guidelines for AI-based medical devices?"} may not be well covered in publicly available pretraining datasets, requiring retrieval from authoritative sources to provide an accurate and up-to-date response.
We refer to these queries as Knowledge Deficient Queries, where retrieval is essential to supplement missing knowledge, but complex reasoning is not required. In these cases, retrieval serves as a direct knowledge lookup, providing factual information that the model can integrate into its response without needing complex reasoning or inference. Once the relevant information is retrieved, the model can directly generate an accurate answer.

\paragraph{Intertwined Retrieval-Reasoning Queries \& Multi-Step Retrieval} Some complex queries require both retrieval and reasoning in an interdependent manner, where the model must retrieve information iteratively while reasoning at each step to determine the next retrieval.  Unlike knowledge Deficient queries, where passively consuming content from the single retrieval step is sufficient to reach the correct answer, Intertwined Retrieval-Reasoning Queries demand both factual knowledge and the model’s ability to reason, plan, infer, and integrate retrieved facts to iteratively refine its understanding and construct the final correct answer. 
For example, in response to the query \textit{"Who was the US president when the inventor of the telephone was born?"}, the model must first recognize that additional information is needed about the inventor of the telephone. This reasoning step directs the retrieval process, leading the model to retrieve the name \textit{Alexander Graham Bell} and his birth year. Once retrieved, this new information updates the model's understanding, allowing it to reason further and determine that it must now retrieve the name of the U.S. president during the retrieved year. 
This retrieval feeds reasoning, and reasoning directs retrieval, forming a continuous cycle that progresses until the final answer is constructed. A single retrieval step is likely to fail in such cases because the initially retrieved information alone is insufficient to answer the query. The sequential nature of retrieval in these queries requires the model to dynamically decides what to retrieve at each step, ensuring that each retrieval builds upon prior reasoning to progressively arrive at the correct answer. However, compared to no retrieval and single step retrieval, this multi-step retrieval process incurs higher computational cost, as each additional retrieval step increases latency and resource consumption.

\subsection{Adaptive Retrieval}
\paragraph{The Need for Adaptive Retrieval in Real-World Applications} In real-world applications, user queries vary significantly. A fixed retrieval strategy can lead to unnecessary computational costs, increased latency, or suboptimal accuracy, depending on the characteristics of the query and no particular retrieval strategy is optimal for all the query types. For example, forcing retrieval for simple queries that can be answered directly from an LLM's parametric knowledge (e.g., \textit{"Who wrote Pride and Prejudice?"}) wastes computational resources and increases latency without improving accuracy. Conversely, for time-sensitive queries (e.g. \textit{"what is the current inflation rate in the U.S.?"}), relying solely on parametric knowledge leads to outdated or hallucinated responses, while employing multi-step retrieval unnecessarily increases cost and latency. Similarly, for complex queries that require multi-step reasoning, some level of latency is unavoidable. For these queries, using no retrieval or only a single-step retrieval would often result in incomplete or incorrect responses, reducing the utility and overall effectiveness of the system. Thus, it is crucial to develop an adaptive retrieval system that can dynamically balances accuracy, efficiency, cost.

\paragraph{Limitations of Prior Adaptive Retrieval Approaches}
While previous works, such as Adaptive-RAG \cite{jeong2024adaptive}, have attempted to balance efficiency and cost by training classifiers using data collected from model predictions and the inductive dataset biases, they are adaptive only to static query complexities but not to user preferences, making them insufficient for handling the diverse needs of real-world applications.
A truly adaptive retrieval system should incorporate both query complexity and user-level controllability. In addition to diverse query complexities, different users and application settings may prioritize accuracy over cost or vice versa, depending on their specific requirements. 
For example, a medical researcher using an LLM to summarize clinical trial results may prefer higher accuracy and completeness, even at the cost of increased retrieval latency and computational expenses.
In contrast, a customer service chatbot handling frequent, simple user queries (e.g., \textit{"What is the return policy?"}) might prioritize fast, cost-effective responses, as occasional inaccuracies pose only negligible risk.
Similarly, a stock market analyst requiring precise, up-to-date financial information might opt for a retrieval-intensive approach, despite longer response times, whereas a real-time virtual assistant answering everyday factual queries should minimize retrieval overhead, emphasizing speed over exhaustive accuracy.
Given these diverse application domains and varying user demands, a one-size-fits-all adaptive retrieval approach is still suboptimal. Instead, a truly adaptive RAG system should be both efficient and customizable, enabling dynamically optimize the trade-off between accuracy and cost while allowing flexible user control.

\section{Flare-Aug: Flexible Adaptive Retrieval Augmented Generation}
In this section, we present the main component of our framework, which contains two classifier, Cost-Optimized Classifier, which is dynamic and LLM-dependent, and Reliability-Optimized Classifier, which is static and dataset-dependent. The overview of the Flare-Aug framework is illustrated in Figure \ref{fig: flare overview}.
\begin{figure*}[h]
    \centering
\includegraphics[width=1\linewidth]{Figures/flare.png}
    \caption{An overview of Flexible Adaptive Retrieval Augmented Generation (Flare-Aug) framework.}
    \label{fig: flare overview}
\end{figure*}
\subsection{Cost-Optimized Classifier}
The Cost-Optimized Classifier selects the cheapest retrieval strategy that still ensures a correct answer. This classifier is LLM-dependent, since the training data is collected specifically from the LLM used for retrieval, and the prediction is sensitive and tailored to the LLM. 
The query in the training data is created by using the LLM to answer with all the three strategies, for each query, among the correctly answered strategies, using the strategy with minimal cost as the gold label. Thus, the labeling process itself is biased towards maximally eliminating unnecessary retrieval. The trained classifier, as a result, minimizes retrieval cost, latency, and computational complexity while maintaining an acceptable level of accuracy. However, a key concern of this classifier is that LLM may arrive at the correct answer by chance or by exploiting shortcuts in reasoning, leading to unverified accuracy. While this classifier is highly beneficial for cost-sensitive and efficiency-driven applications, it is unsuitable for high-risk domains where retrieval must be systematic and reliably grounded in external knowledge.

\paragraph{Training Data for Cost-Optimized Classifier} The labeling process for the cost-optimized classifier is straightforward: we aim to select the cheapest retrieval strategy that still ensures a correct answer. Formally, let $S = \{s_{no}, s_{sg}, s_{multi}\}$ denote the set of available retrieval strategies: no retrieval ($s_{no}$), single-step retrieval ($s_{sg}$) and multi-step retrieval $(s_{multi})$. For each query $q$ in the training data, we evaluate whether it can be answered correctly under each retrieval strategy by assessing the LLM's correctness with these strategies. Denote $LLM(q, s)$ as the correctness function, where:
\begin{equation*}
    LLM(q,s) = 
    \begin{cases}
        1, & \text{if the LLM answers correctly with retrieval strategy } s, \\
        0, & \text{otherwise}.
    \end{cases}
\end{equation*}

Let $S_q\subseteq S$ be the subset of retrieval strategies that lead to correct answers for $q$:  $S_q = \{s\in S|LLM(q,s)=1\}$. Since the retrieval strategies incur different costs, where $c(s_{no})<c(s_{sg})<c(s_{multi})$, we assign the lowest-cost correct strategy as the label for $q$:
\begin{equation*}
s^{*}(q) = \arg\min_{s\in S_q} c(s)
\end{equation*}


For example, if $S_q = \{s_{no}, s_{sg}\}$, we choose $s_{no}$ as the label for $q$ since it has a lower cost than $s_{sg}$. Thus, for each query, we first determine $S_q$, the set of all retrieval strategies that yield a correct answer. We then assign $s^{*}(q)$ as the strategy with the lowest retrieval cost. If $S_q=\emptyset$ (i.e., the LLM fails to answer the query correctly under any strategy), we exclude the query from the dataset.


 
\subsection{Reliability-Optimized Classifier} 
The Reliability-Optimized Classifier assigns retrieval strategies based on intrinsic bias of the question answering, ensuring consistency, stability and reliability across different LLMs. This classifier are trained with only binary label: single-step retrieval and multi-step retrieval, with data from both single-hop QA dataset and multi-hop QA dataset. The accurate prediction of these dataset relies on the assumption that, single hop QA dataset and multi-hop QA dataset are intrinsically biases towards single-step retrieval and multi-step retrieval, respectively, and thus, may provide signals indicating which strategy to use during retrieval. Since there are only two labels, this classifier guarantees to have at least one retrieval, it may induce unnecessary computational overhead when using more powerful LLMs or handling straightforward queries that do not require retrieval. Despite this, since it is not LLM specific, it is more stable and reliable, encouraging higher accuracy and more reliable response. This classifier is effective in applications where accuracy is paramount, albeit at a higher cost.

\paragraph{Training Data for Reliability-Optimized Classifier } The Reliability-Optimized Classifier is trained using binary labels—single-step retrieval and multi-step retrieval—derived from question-answering datasets. Unlike the Cost-Optimized Classifier, which tailors retrieval strategy to a specific LLM, this classifier relies on dataset-level biases to infer retrieval needs, making it more stable across different LLMs. For each query $q$, we assign a label $s^{*}(q)$ based on the dataset it belongs to:
\begin{equation*}
s^{*}(q) =  \begin{cases}
        s_{sg}, & \text{if $q$ originates from a single-hop QA dataset } \\
        s_{multi}, & \text{if $q$ originates from a multi-hop QA dataset}.
    \end{cases}
\end{equation*}
This labeling approach assumes that queries from single-hop QA datasets are best handled with single-step retrieval, while multi-hop QA datasets require multi-step retrieval. Since the classifier does not verify correctness based on a specific LLM, it ensures LLM-agnostic stability though it does not minimize unnecessary retrieval. 



\subsection{User-Controllable Adaptive Classifier}
To fully realize user-controllability, we introduce a simple and interpretable parameter $\alpha$ that allows users to define their preferred trade-offs, providing flexibility across diverse use cases while dynamically optimizing the retrieval strategy. Specifically, after training the Cost-Optimized Classifier (denoted by $W_{coc}$) and the Reliability-Optimized Classifier (denoted as $W_{roc}$), we construct a User-Controllable Adaptive Classifier by interpolating between their parameters based on the user-defined control parameter $\alpha$.  The resulting classifier is defined as: $W_{\alpha} = (1-\alpha) W_{coc}+ \alpha W_{roc}$, where $\alpha \in [0, 1]$ controls the trade-off between cost and accuracy. Setting $\alpha=0$ results in a purely cost-optimized classifier, while $\alpha=1$ yields a fully reliability-optimized classifier. This formulation provides a continuous and customizable retrieval strategy, allowing users to flexibly adjust the balance between efficiency and accuracy based on their specific needs.
\section{Experiments}
\subsection{Experimental Setups}
We describe the datasets, baselines, evaluation metrics, and experimental details in our study. 

\paragraph{Dataset} To simulate a realistic retrieval scenario, where queries naturally vary in complexity and diversity, we evaluate on a mixed dataset with queries from three single-hop question answer datasets—SQuAD \citep{rajpurkar2016squad}, Natural Questions \citep{kwiatkowski2019natural}, and TriviaQA \citep{joshi2017triviaqa}—and three multi-hop QA datasets—MuSiQue \citep{trivedi2022musique}, HotpotQA \citep{yang2018hotpotqa}, and 2WikiMultiHopQA \citep{ho2020constructing}. This combination reflects the heterogeneous nature of real-world queries, encompassing both fact-based retrieval tasks and multi-hop reasoning tasks that require linking information across multiple sources. For training, we randomly sample 500 queries from each dataset, resulting in a total of 3,000 training examples. Similarly, for evaluation, we construct a test set by randomly sampling 500 queries from each dataset, yielding a total of 3,000 test queries. 
\paragraph{Baselines} To assess the effectiveness of our approach in balancing accuracy, cost, and user control flexibility, we compare it against both adaptive and static retrieval strategies. Specifically, we evaluate our method against the adaptive retrieval approach \textbf{Adaptive-RAG} \cite{jeong2024adaptive}, as well as three static retrieval strategies: always using \textbf{No Retrieval}; always using \textbf{Single-Step Retrieval} and always using \textbf{Multi-Step Retrieval} \citep{trivedi2022interleaving}.

\paragraph{Evaluation Metrics} We evaluate the trade-off between accuracy and retrieval cost using two key metrics. Accuracy measures whether the predicted answer contains the ground-truth answer. Retrieval cost is quantified as the number of retrieval steps required to generate an answer, where No Retrieval has a cost of 0, Single-Step Retrieval has a cost of 1, and Multi-Step Retrieval incurs a variable cost depending on the number of retrieval steps per query. By jointly considering these two metrics, we assess how different retrieval strategies balancing the trade-off between the  response quality and computational efficiency. 

\paragraph{Implementation Details} 
Following \cite{jeong2024adaptive} and \cite{trivedi2022interleaving}, we use BM25, a term-based sparse retrieval model, as the retriever. For queries from multi-step datasets, we utilize the pre-processed corpus from \cite{trivedi2022interleaving} as the external document corpus, while for queries from single-hop datasets, we use Wikipedia, preprocessed by \cite{karpukhin2020dense}, as the external document corpus. For answer generation, we employ two sizes of Flan-T5 models \citep{chung2024scaling} (Flan-T5 XL and Flan-T5 XXL) and two sizes of GPT-4o models \cite{hurst2024gpt} (GPT-4o and GPT-4o-mini). For the classifier, we train a T5-Large model \cite{raffel2020exploring} using 500 samples from each dataset, resulting in a total of 3,000 queries for the labeling process. The classifier is trained with a learning rate of 
3e-5
  and a batch size of 64 on a single 80GB A100 GPU.  We train the model for 20 epochs, as the loss has converged stably without indications of overfitting. We demonstrate the robustness of the classifier in Appendix \ref{app: classifier}, with experiments on different training epochs and different sizes of classifiers. 
\subsection{Main Results}
Figure \ref{fig: Acc} and Figure \ref{fig: step} illustrate the accuracy and retrieval cost across different values of $\alpha$. We observe that both accuracy and cost \textbf{monotonically} increase with the user-controllable parameter $\alpha$, allowing users to easily adjust the retrieval strategy based on their preferred trade-offs between accuracy and efficiency. Due to the space limit, the exact quantitative results can be find in Table \ref{tab: acc-step valid} in the Appendix \ref{app: exp}. 
\paragraph{Accuracy Trends Across Different $\alpha$} As shown in Figure \ref{fig: Acc}, the No Retrieval strategy yields the lowest accuracy, with values below 0.2 for the Flan-T5 models, while achieving relatively higher performance GPT4o series models (approximately 0.43 for GPT-4o Mini and 0.53 for GPT-4o). Applying Single-Step Retrieval significantly improves accuracy, particularly for Flan-T5 models, increasing their accuracy to around 0.39. However, the performance gains for GPT-4o models are relatively smaller, as these models already exhibit strong parametric knowledge storage and advanced reasoning abilities. In contrast, Flan-T5 models benefit more from retrieval, as they are less capable of answering queries without external information.
The accuracy of Adaptive-RAG falls between Single-Step Retrieval and Multi-Step Retrieval. Meanwhile, our user-controllable approach achieves accuracy levels ranging between those of Single-Step Retrieval and Multi-Step Retrieval, allowing users to tailor the retrieval strategy to different application needs. Notably, our approach can even outperform Multi-Step Retrieval for Flan-T5-XL, Flan-T5-XXL, and GPT-4o Mini, demonstrating its effectiveness in optimizing retrieval decisions based on query complexity.
\begin{figure*}[h]
    \centering
\includegraphics[width=1\linewidth]{Figures/accuracy_plot.png}
    \caption{Average accuracy of different approaches on validation set.}
    \label{fig: Acc}
\end{figure*}
\paragraph{Cost Trends Across Different $\alpha$}
Figure \ref{fig: step} illustrates the retrieval cost, measured in retrieval steps, across different values of the user-controllable parameter 
 $\alpha$. Similar to the accuracy trends observed in Figure \ref{fig: Acc}, retrieval cost monotonically increases with $\alpha$, enabling users to adjust retrieval expenditure according to their cost constraints. For Flan-T5 models, the minimum retrieval cost (i.e., setting $\alpha=0$) is close to Single-Step Retrieval, indicating that these models require certain amount of retrieval to answer queries correctly. In contrast, for GPT-4o models, the minimum cost is closer to No Retrieval, as these models possess stronger parametric knowledge and reasoning abilities, allowing them to correctly answer a larger proportion of queries without retrieval. This behavior highlights the adaptability of the Cost-Optimized Classifier, which is LLM-specific and automatically adjusts to the retrieval needs of different models—retrieving more for Flan-T5 models while retrieving less for GPT-4o models.Furthermore, while Figure \ref{fig: Acc} shows that our approach achieves accuracy comparable to or exceeding Multi-Step Retrieval as $\alpha$ increases, Figure \ref{fig: step} demonstrates that its retrieval cost remains consistently lower than that of Multi-Step Retrieval. Thus, compared to Multi-Step Retrieval, our  approach incurs significantly lower retrieval overhead while without sacrificing accuracy. 
\begin{figure*}[h]
    \centering
\includegraphics[width=1\linewidth]{Figures/step_plot.png}
    \caption{Total number of  steps (cost) of different approaches on validation set.}
    \label{fig: step}
\end{figure*}
\paragraph{Accuracy-Cost trade-off} In Figure \ref{fig: Acc-cost}, accuracy-cost trade-off of our approach across different values of 
 $\alpha$. For Flan-T5 XL, Flan-T5 XXL, and GPT-4o Mini, Adaptive-RAG is positioned on the lower right side of our trade-off curves, indicating that our approach consistently outperforms Adaptive-RAG in both accuracy and cost efficiency. Another way to interpret the accuracy-cost plot is by fixing either accuracy or cost and comparing the other metric. For example, when fixing accuracy to the level achieved by Multi-Step Retrieval, our approach requires significantly fewer retrieval steps, demonstrating its ability to achieve comparable or superior accuracy with lower retrieval overhead. 
\begin{figure*}[h]
    \centering
\includegraphics[width=1\linewidth]{Figures/acc_cost.png}
    \caption{Accuracy-Cost plot of different approaches on validation set.}
    \label{fig: Acc-cost}
\end{figure*}




\subsection{Practicality of Setting 
$\alpha$.} 
To further demonstrate the practicality of our approach, we provide two simple and intuitive strategies that can be used for setting $\alpha$:
\begin{itemize}
\item \textbf{Incremental Adjustment}: Since $\alpha$ increases monotonically with both cost and accuracy, users can start with an initial $\alpha$ and refine it based on observed retrieval cost and response quality. If the retrieval cost exceeds their budget, they can lower $\alpha$; if the response quality is unsatisfactory, they can increase $\alpha$. 
\item \textbf{Validation-Based Estimation}: Users can estimate a suitable $\alpha$ directly by examining the accuracy-cost trade-off on the validation set, allowing them to select a value that aligns with their desired balance between retrieval cost and accuracy. In Appendix \ref{app: set alpha based on validation set}, we conduct additional experiments on the test set to illustrate that $\alpha$ can be easily estimated based on results on validation set. 
\end{itemize}


\section{Conclusion}
In this work, we present Flare-Aug, a flexible and adaptive retrieval-augmented generation framework that incorporate user controllability into RAG framework besides accuracy and cost trade-offs. Unlike existing adaptive RAG methods that provide only static adaptation to query complexity, Flare-Aug enables fine-grained and dynamic control over retrieval behavior, allowing users to balance accuracy and cost based on their specific needs. By enabling truly adaptive retrieval tailored to user preferences, Flare-Aug represents a step toward more efficient, customizable, and scalable retrieval-augmented generation, paving the way for future advancements in personalized RAG system.
\section{Limitation and Future Work}
Future research could extend our approach to more advanced adaptive retrieval-augmented generation systems that dynamically adjust not only retrieval strategies but also LLM selection based on task complexity and resource constraints. Additionally, while our classifier is computationally lightweight compared to retrieval costs, an important direction for future work is to develop a more unified retrieval adaptation framework, enabling LLMs to self-regulate retrieval decisions without relying on an external classifier. 


% \section*{Ethics Statement}



\bibliography{colm2025_conference}
\bibliographystyle{colm2025_conference}

\appendix
\section{Ablation on classifier}\label{app: classifier}
\paragraph{Different sizes of classifiers}
Figure \ref{fig: diff-classifier} shows accuracy and total steps predicted by different sizes of classifier and $\alpha$ on validation set, and Figure \ref{fig: Acc-cost-diff-classifier} shows a more clear comparison on the trade-offs of accuracy and cost by different sizes of classifiers. Though the performance of different classifiers varies, we find that there is no dominating advantage in using larger classifiers. 

\begin{figure*}[t!]
    \centering
    \begin{subfigure}[t]{0.8\textwidth}
        \centering
        \includegraphics[height=2in]{Figures/accuracy_different_classifier.png}
        \caption{Accuracy using different parameter $\alpha$ and different sizes of classifiers.}
    \end{subfigure}%
    \\
    \begin{subfigure}[t]{0.8\textwidth}
        \centering
        \includegraphics[height=2in]{Figures/step_different_classifier.png}
        \caption{Total retrieval steps (cost) using different parameter $\alpha$ and different sizes of classifiers.}
    \end{subfigure}
    \caption{Accuracy and total steps predicted by different sizes of classifier and $\alpha$ on validation set. }
     \label{fig: diff-classifier}
\end{figure*}
\begin{figure*}[h]
    \centering
\includegraphics[width=0.8\linewidth]{Figures/different_classifier_acc_step.png}
    \caption{Accuracy-Cost trade-off plot of different sizes of classifiers. }
    \label{fig: Acc-cost-diff-classifier}
\end{figure*}
\paragraph{Different training epochs} Figure \ref{fig: diff-classifier-epoches} shows accuracy and total steps predicted by t5-3b and t5-base training for 20 and 40 epochs on validation set, and Figure \ref{fig: Acc-cost-diff-classifier-epoches} shows a more clear comparison on the trade-offs of accuracy and cost by t5-3b and t5-base training for 20 and 40 epochs on validation set. 

\begin{figure*}[t!]
    \centering
    \begin{subfigure}[t]{0.8\textwidth}
        \centering
        \includegraphics[height=2in]{Figures/accuracy_different_classifier_epochs.png}
        \caption{Accuracy of using t5-3b and t5-base as a classifier and different training epochs. }
    \end{subfigure}%
    \\
    \begin{subfigure}[t]{0.8\textwidth}
        \centering
        \includegraphics[height=2in]{Figures/step_different_classifier_epochs.png}
        \caption{Total retrieval steps (cost) of using t5-3b and t5-base as a classifier and different training epochs.}
    \end{subfigure}
    \caption{Accuracy and total steps of  t5-3b and t5-base as a classifier and different training epochs. }
     \label{fig: diff-classifier-epoches}
\end{figure*}
\begin{figure*}[h]
    \centering
\includegraphics[width=0.8\linewidth]{Figures/different_classifier_epochs.png}
    \caption{Accuracy-Cost trade-off plot of using t5-3b and t5-base as a classifier with different training epochs. }
    \label{fig: Acc-cost-diff-classifier-epoches}
\end{figure*}

\paragraph{Training Loss} In Figure \ref{fig: training-loss}, we show average training loss v.s. training epochs for cost optimized classifier for different models and reliability optimized classifier. 
\begin{figure*}[h]
    \centering
\includegraphics[width=0.8\linewidth]{Figures/training_loss_comparison.png}
    \caption{Average training loss v.s. training epochs for cost optimized classifier for different models and reliability optimized classifier. }
    \label{fig: training-loss}
\end{figure*}


\section{More Experimental Results} \label{app: exp}
\subsection{Quantitative Results on Validation set}
In Table \ref{tab: acc-step valid}, we detail the step and accuracy of different retrieval strategies on validation dataset. 
\begin{table}[h]
    \centering
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{l|cc|cc|cc|cc}
        \hline
       \multirow{2}{*}{Models} & \multicolumn{2}{c|}{\textbf{flan-t5-xl}} & \multicolumn{2}{c|}{\textbf{flan-t5-xxl}} & \multicolumn{2}{c|}{\textbf{gpt4o-mini}} & \multicolumn{2}{c}{\textbf{gpt4o}} \\\cline{2-9}
        & Acc & Steps & Acc & Steps & Acc & Steps & Acc & Steps \\
        \hline
        \multicolumn{9}{c}{\textbf{Static Retrieval}} \\
        \hline
        No Retrieval & 0.160 & 0.0 & 0.193 & 0.0 & 0.408 & 0.0 & 0.527 & 0.0 \\
        Single-step Retrieval & 0.389 & 1.0 & 0.419 & 1.0 & 0.477 & 1.0 & 0.539 & 1.0 \\
        Multi-step Retrieval & 0.437 & 4.7 & 0.452 & 2.1 & 0.533 & 5.6 & 0.595 & 4.2 \\

        \hline
        \multicolumn{9}{c}{\textbf{Adaptive-RAG}} \\
        \hline
        AdaptiveRAG
 & 0.427 & 2.3 & 0.439 & 1.3 & 0.510 & 2.6 & 0.585 & 2.0 \\\hline
  \multicolumn{9}{c}{\textbf{Flare-Aug(Ours)}} \\\hline
        $\alpha=0.0$ & 0.388 & 1.3 & 0.385 & 0.9 & 0.445 & 0.5 & 0.533 & 0.2 \\
       $\alpha=0.2$ & 0.421 & 1.6 & \textbf{0.444} & \textbf{1.2} & 0.490 & 1.7 & 0.557 & 0.7 \\
        $\alpha=0.4$ & \textbf{0.439} & \textbf{2.2} & 0.461 & 1.4 & 0.524 & 2.8 & 0.575 & 2.2 \\
        $\alpha=0.6$ & 0.441 & 2.6 & 0.465 & 1.5 & 0.532 & 3.2 & 0.584 & 2.6 \\
        $\alpha=0.8$ & 0.439 & 2.8 & 0.463 & 1.6 & 0.533 & 3.4 & 0.589 & 2.8 \\
        $\alpha=1.0$ & 0.441 & 3.0 & 0.463 & 1.6 & 0.534 & 3.5 & 0.592 & 2.9 \\

        \hline
    \end{tabular}
    \caption{Accuracy and average steps of different retrieval strategies on validation dataset. }
\label{tab: acc-step valid}
\end{table}
\subsection{Feasibility of estimating $\alpha$ based on Validation Set.}\label{app: set alpha based on validation set}
In Figure \ref{fig: acc-test500}, Figure \ref{fig: step-test500} and Figure \ref{fig: Acc-cost_test500}, we show the accuracy and cost with various $\alpha$, as well as the accuracy-cost plot on test set. (The quantitative result in Table  \ref{tab: acc-step test}). Combines with the experimental results for validation dataset, we empirically verify that users can estimate the accuracy-cost trade-off from validation results. As shown in Figure \ref{fig: acc-test500}, Figure \ref{fig: step-test500} and Figure \ref{fig: Acc-cost_test500}, the test set follows the same trend as the validation set. For example, based on Figure \ref{fig: Acc}, setting $\alpha=0.4$ or larger for flan-t5-xl achieves an accuracy level comparable to Multi-Step Retrieval, and using the same $\alpha$ value on test set, as shown in Figure \ref{fig: acc-test500}, yields a similar accuracy-cost balance. Moreover, if the goal is to achieve an accuracy higher than Adaptive-RAG, the validation results suggest setting $\alpha=0.2, 0.2, 0.4, 0.6$ for flan-t5-xl, flan-t5-xxl, gpt-4o-mini and gpt-4o respectively, which is consistent with that on test set (except for gpt4o model, where on test set, setting $\alpha=0.4$ is enough to achieve a comparable accuracy to Adaptive-RAG).
Thus, users can rely on validation-based tuning of $\alpha$ for effective deployment.




\begin{table}[h]
    \centering
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{l|cc|cc|cc|cc}
        \hline
        \multirow{2}{*}{Models} & \multicolumn{2}{c|}{\textbf{flan-t5-xl}} & \multicolumn{2}{c|}{\textbf{flan-t5-xxl}} & \multicolumn{2}{c|}{\textbf{gpt4o-mini}} & \multicolumn{2}{c}{\textbf{gpt4o}} \\ \cline{2-9}
        & Acc & Steps & Acc & Steps & Acc & Steps & Acc & Steps \\
        \hline
        \multicolumn{9}{c}{\textbf{Static Retrieval}} \\
        \hline
        No Retrieval & 0.154 & 0.0 & 0.184 & 0.0 & 0.402 & 0.0 & 0.512 & 0.0 \\
        Single-step Retrieval & 0.386 & 1.0 & 0.418 & 1.0 & 0.491 & 1.0 & 0.553 & 1.0 \\
        Multi-step Retrieval & 0.423 & 4.6 & 0.443 & 2.1 & 0.543 & 5.6 & 0.599 & 4.1 \\

        \hline
        \multicolumn{9}{c}{\textbf{Adaptive-RAG}} \\
        \hline
        AdaptiveRAG
 & 0.414 & 2.3 & 0.437 & 1.3 & 0.528 & 2.6 & 0.591 & 2.0 \\

        \hline
        \multicolumn{9}{c}{\textbf{Flare-Aug (Ours)}} \\
        \hline
        $\alpha=0.0$ & 0.381 & 1.3 & 0.388 & 0.8 & 0.445 & 0.5 & 0.520 & 0.2 \\
        $\alpha=0.2$ & 0.408 & 1.7 & 0.443 & 1.2 & 0.498 & 1.6 & 0.549 & 0.6 \\
        $\alpha=0.4$ & 0.425 & 2.2 & 0.455 & 1.4 & 0.541 & 2.8 & 0.593 & 2.2 \\
        $\alpha=0.6$ & 0.427 & 2.6 & 0.452 & 1.5 & 0.547 & 3.2 & 0.598 & 2.5 \\
        $\alpha=0.8$ & 0.428 & 2.8 & 0.451 & 1.6 & 0.550 & 3.4 & 0.600 & 2.7 \\
        $\alpha=1.0$ & 0.430 & 2.9 & 0.450 & 1.6 & 0.550 & 3.5 & 0.601 & 2.8 \\

        \hline
    \end{tabular}
    \caption{Accuracy and average steps of different retrieval strategies on test dataset.}
    \label{tab: acc-step test}
\end{table}

\begin{figure*}[h]
    \centering
\includegraphics[width=1\linewidth]{Figures/accuracy_plot_test500.png}
    \caption{Retrieval accuracy of different approaches on test set.}
    \label{fig: acc-test500}
\end{figure*}
\begin{figure*}[h]
    \centering
\includegraphics[width=1\linewidth]{Figures/step_plot_test500.png}
    \caption{Total number of  steps (cost) of different approaches on test set.}
    \label{fig: step-test500}
\end{figure*}

\begin{figure*}[h]
    \centering
\includegraphics[width=1\linewidth]{Figures/acc_cost_test500.png}
    \caption{Accuracy-Cost plot of different approaches on test set.}
    \label{fig: Acc-cost_test500}
\end{figure*}

\end{document}
