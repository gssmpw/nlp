% Talking about Spatial Recognition/Reasoning
Spatial reasoning plays a significant role in human cognition and daily activities. 
It is also a crucial aspect in many AI problems, including language grounding~\citep{zhang2022lovis, 10610443}, navigation~\citep{, yamada2024evaluatingspatialunderstandinglarge}, computer vision~\citep{liu2023visualspatialreasoning, chen2024spatialvlmendowingvisionlanguagemodels}, medical domain~\citep{gong20233dsamadapter}, and image generation~\citep{gokhale2023benchmarkingspatialrelationshipstexttoimage}.
One key concept in spatial reasoning is the Frame of Reference (FoR), which identifies the perspective of spatial expressions. FoR has been studied extensively in cognitive linguistics~\citep{Edmonds-Wathen852956, VUKOVIC2015110}.
\citet{Levinson_2003} initially defines three FoR classes: \textit{relative}, based on the observer’s perspective; \textit{intrinsic}, based on an inherent feature of the reference object; and \textit{absolute}, using environmental cues like cardinal directions -See  Figure~\ref{fig:FoR_classes}.
This framework was expanded by ~\citet{TENBRINK2011704} to create a more comprehensive framework, serving as the basis of our work.
Understanding FoR is important for many applications, especially in embodied AI.
In such applications, an agent must simultaneously comprehend multiple perspectives, including the one from the instruction giver and from the instruction follower, to communicate and perform tasks effectively.
% Other areas, such as video narrative generation and 3D scene construction, \pk{face similar challenges of handling multiple FoRs from textual instructions.: need to align multiple FoRs used in textual instructions. [YOU DID NOT TALK ABOUT SPECIFIC CHALLENGE THAT IS WHY I WOULD REMOVE] }
However, recent spatial evaluation benchmarks have largely overlooked FoR.
For example, the text-based benchmarks~\citet{shi2022stepgamenewbenchmarkrobust, mirzaee2022transferlearningsyntheticcorpora, rizvi2024sparcsparpspatialreasoning} and text-to-images benchmarks~\citep{{gokhale2023benchmarkingspatialrelationshipstexttoimage, huang2023t2icompbenchcomprehensivebenchmarkopenworld, cho2023dallevalprobingreasoningskills, cho2023visualprogrammingtexttoimagegeneration}}  assume a fixed perspective for all spatial expressions. 
This inherent bias limits situated spatial reasoning, restricting adaptability in interactive environments where perspectives can change.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\columnwidth]{Figures/FoR.png}
    \caption{Illustration of FoR classes. The cat is the locatum, the car is the relatum, and the arrow indicates the perspective.}
  \label{fig:FoR_classes}
\end{figure}



% Additionally, understanding this concept is significant for several AI applications.
% An important application is embodied AI. Particularly in a real environment, an instruction-giver and instruction-follower have different perspectives, and there are potential variations in their usage of FoRs. 
% In such a setting, the model must comprehend the dynamic changes in the FoR (perspective changes) in the instruction to perform the task effectively. 
% FoR comprehension can benefit other applications, such as video narrative generation and 3D scene construction based on text.
% The recent spatial evaluation benchmarks have paid less attention to the importance of FoRs.
% For instance, the textual-only benchmarks concentrate on the complex reasoning task; however, they limit the evaluation to intrinsic FoR, using one object as the center of coordinates. 
% Similarly, text-to-image benchmarksoften assume a camera perspective for spatial expressions. 
% However, only limited studies investigate how AI models understand FoR.

% Purpose benchmark and experiment
To systematically investigate the role of FoR in spatial understanding and create a new resource, that is, \textbf{F}rame \textbf{o}f \textbf{R}eference \textbf{E}valuation in \textbf{S}patial Reasoning \textbf{T}asks (FoREST),
FoREST is designed to evaluate models’ ability to comprehend FoR from textual descriptions and extend this evaluation to grounding and visualization.
Our benchmark includes spatial expressions with FoR ambiguity—where multiple FoRs may apply to the described situation—and spatial expressions with only a single valid FoR.
This design allows evaluation of the models' understanding of FoR in both scenarios.
We evaluate several LLMs in a QA setting that require FoR understanding and apply the FoR concept in text-to-image models.
Our findings reveal performance differences across FoR classes and show that LLMs exhibit bias toward specific FoRs when handling ambiguous cases.
This bias extends to layout-diffusion models, which rely on LLM-generated layouts in the image generation pipeline.
To enhance FoR comprehension in LLMs, we propose Spatial-Guided prompting, which enables models to analyze and extract additional spatial information, including directional, topological, and distance relations. 
We demonstrate that incorporating spatial information improves question-answering and layout generation, ultimately enhancing text-to-image generation performance.

Our contribution\footnote{code and dataset available at \href{https://github.com/HLR/FoREST.git}{GitHub repository}. } are summarized as follows, 
1. We introduce the FoREST benchmark to systematically evaluate LLMs’ FoR comprehension in a QA setting.
2. We analyze the impact of FoR information on text-to-image generation using multiple diffusion models.
3. We propose a prompting approach that generates spatial information, which can be incorporated into QA and layout diffusion to enhance performance.