\noindent\textbf{Frame of Reference in Cognitive Studies.}
The concept of the frame of reference in cognitive studies was introduced by \citealt{Levinson_2003} and later expanded with more diverse spatial relations \citep{TENBRINK2011704}.
Subsequent research investigated the human preferences for specific FoR classes~\citep{Edmonds-Wathen852956, VUKOVIC2015110, SHUSTERMAN2016115, Ruotolo2016}. For instance, \citealt{Ruotolo2016} examined how FoR influences scene memorization and description under time constraints. Their study found that participants performed better when spatial relations were based on their position rather than external objects, highlighting a distinction between relative and intrinsic FoR.

\noindent\textbf{Frame of Reference in AI.}
Several benchmarks have been developed to evaluate the spatial understanding of AI models in multiple modalities; for instance, %navigation~\citep{yamada2024evaluatingspatialunderstandinglarge}, 
textual QA~\citep{shi2022stepgamenewbenchmarkrobust, mirzaee2022transferlearningsyntheticcorpora, rizvi2024sparcsparpspatialreasoning}, and text-to-image (T2I) benchmarks~\citep{gokhale2023benchmarkingspatialrelationshipstexttoimage, huang2023t2icompbenchcomprehensivebenchmarkopenworld, cho2023dallevalprobingreasoningskills, cho2023visualprogrammingtexttoimagegeneration}.
However, most of these benchmarks overlook the frame of reference (FoR), assuming a single FoR for all instances despite its significance in cognitive studies.
Recent works in vision-language research are beginning to address this problem. 
For instance, \citealt{liu2023visualspatialreasoning} examines FoR’s impact on visual question-answering but focuses only on limited FoR categories. Our work covers more diverse FoRs.
\citealt{comfortFoR} examine FoR ambiguity and understanding in vision-language models by evaluating spatial relations derived from visual input under different FoR questions. 
Their approach relies on images from camera perspectives, with FoR indicated in the question.
In contrast, our work focuses on the reasoning of spatial relations when dealing with multiple FoRs and when there are changes in perspective in explaining the context beyond the camera’s viewpoint. 
Additionally, we show that explicitly identifying the FoR helps improve spatial reasoning in both question-answering and text-to-image generation, particularly when involving multiple perspectives.
% \tp{
% \citealt{comfortFoR} examine FoR ambiguity and understanding in vision-language models by evaluating spatial relations derived solely from visual input under different FoR questions. Their approach relies on images from camera perspectives, with FoR introduced through the question.
% In contrast, our work focuses on the role of FoR in spatial relations, analyzing how LLMs interpret different FoR classes by incorporating various perspectives beyond the camera’s viewpoint.
% Additionally, we assess the broader impact of FoR in spatial reasoning through spatial expression beyond question-answering tasks, including text-to-image generation. 
% Lastly, we demonstrate that explicitly identifying FoR can enhance spatial reasoning tasks, particularly when involving multiple perspectives.
% }
