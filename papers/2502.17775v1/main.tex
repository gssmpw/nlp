% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{adjustbox}
\usepackage{subcaption}
\usepackage{amssymb}% http://ctan.org/pkg/amssymb
\usepackage{pifont}% http://ctan.org/pkg/pifont

% Set up listings for Python
\lstset{
    language=Python,                 % Language of the code
    basicstyle=\ttfamily\footnotesize, % Font style and size
    keywordstyle=\color{blue},        % Style for keywords
    commentstyle=\color{gray},        % Style for comments
    stringstyle=\color{red},          % Style for strings
    showstringspaces=false,           % Don't display spaces in strings
    numberstyle=\tiny\color{gray},    % Style for line numbers
    frame=single,                     % Adds a frame around the code
    breaklines=true,                  % Automatic line breaking
    captionpos=b,                     % Caption position (b for bottom)
    tabsize=4                         % Size of tabs
}

\newcommand{\pk}[1]{{\textcolor{red}{~(PK: #1)}}}
\newcommand{\tp}[1]{{\textcolor{orange}{~(TP: #1)}}}
\newcommand{\xmark}{\ding{55}}%
\newcommand{\improve}[1]{($\textcolor{green}{\uparrow #1}$)}
\newcommand{\worse}[1]{($\textcolor{red}{\downarrow #1}$)}
\newcommand{\iclr}[1]{{\textcolor{blue}{#1}}}


% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}
\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{FoREST: \textbf{F}rame \textbf{o}f \textbf{R}eference \textbf{E}valuation in \textbf{S}patial Reasoning \textbf{T}asks}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Tanawan Premsri \\
  Department of Computer Science \\
  Michigan State University\\
  \texttt{premsrit@msu.edu} \\\And
  Parisa Kordjamshidi \\
Department of Computer Science \\
  Michigan State University\\
  \texttt{kordjams@msu.edu} \\}

\begin{document}
\maketitle
\begin{abstract}
Spatial reasoning is a fundamental aspect of human intelligence. 
One key concept in spatial cognition is the Frame of Reference (FoR), which identifies the perspective of spatial expressions. 
Despite its significance, FoR has received limited attention in AI models that need spatial intelligence. There is a lack of dedicated benchmarks and in-depth evaluation of large language models (LLMs) in this area.
To address this issue, we introduce the \textbf{F}rame \textbf{o}f \textbf{R}eference \textbf{E}valuation in \textbf{S}patial Reasoning \textbf{T}asks (FoREST) benchmark, designed to assess FoR comprehension in LLMs.
We evaluate LLMs on answering questions that require FoR comprehension and layout generation in text-to-image models using FoREST.
Our results reveal a notable performance gap across different FoR classes in various LLMs, affecting their ability to generate accurate layouts for text-to-image generation. 
This highlights critical shortcomings in FoR comprehension.
To improve FoR understanding, we propose Spatial-Guided prompting, which improves LLMsâ€™ ability to extract essential spatial concepts. 
Our proposed method improves overall performance across spatial reasoning tasks.
\end{abstract}

\section{Introduction}
\input{01_Introduction}

%by guiding the model to focus on three key spatial relations. 
\section{Spatial Primitives}\label{sec:primitives}
\input{02_Primitive}



\section{FoREST Dataset Construction}\label{sec:DatasetCreation}

\input{03_DatasetCreation}


\section{Models and Tasks} 
\input{04_ModelTask}


\section{Experimental Results}
\input{05_Experimental}

%%%%%%%%%%%%%%%%*******Related Works

\section{Related Work}
\input{06_Relatedwork}

\section{Conclusion}
Given the significance of spatial reasoning in AI applications, 
we introduce \textbf{F}rame \textbf{o}f \textbf{R}eference \textbf{E}valuation in \textbf{S}patial Reasoning \textbf{T}asks (FoREST) benchmark to evaluate Frame of Reference comprehension in textual spatial expressions via question-answering and grounding in visual modality by diffusion models.
Based on this benchmark, our results reveal notable differences in FoR comprehension across LLMs and their struggle with questions that require adaptation between multiple FoRs.
Moreover, the bias in FoR interpretations impacts the layout generation with LLMs for text-to-image models. 
To improve FoR comprehension, we propose Spatial-Guided prompting, which first generates spatial relation's topological, distal, and directional type information in addition to FoR and includes this information in downstream task prompting.
Employing SG improves the overall performance in QA tasks requiring FoR understanding and text-to-image generation.

\section{Limitations}
While we analyze LLMs' shortcomings, our benchmark only highlights areas for improvement, not harming the model.
The trustworthiness and reliability of the LLMs are still a research challenge.
Our analysis is confined to the spatial reasoning domain and does not account for biases related to gender or race. However, we acknowledge that linguistic and cultural variations in spatial expression are not considered, as our study focuses solely on English. Extending this work to multiple languages could reveal important differences in FoR adaptation. 
Our analysis is still limited to the synthetic environment. Future research should consider the broader implications of the frame of reference of spatial reasoning in real-world applications.
Additionally, our experiments require substantial GPU resources, limiting the selection of LLMs and constraining the feasibility of testing larger models. The computational demands also pose accessibility challenges for researchers with limited resources.
We find no ethical concerns in our methodology or results, as our study does not involve human subjects or sensitive data. 



% Entries for the entire Anthology, followed by custom entries
\bibliography{ref}
% \bibliographystyle{acl_natbib}

\appendix
\input{99_Appendix}

\end{document}
