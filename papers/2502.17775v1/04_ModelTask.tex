% We aim to evaluate language models' ability on identify FoR and its impacts on T2I models. 
The FoREST benchmark supports multiple tasks, including FoR identification, Question Answering (QA) that requires FoR comprehension, and Text-to-Image (T2I). This paper focuses on QA and T2I for a deeper evaluation of spatial reasoning. 
FoR identification experiments are provided in Appendix~\ref{appendix:FoRIdentification}.

\subsection{Question-Answering (QA)}\label{sec:QA_explanation}

\noindent\textbf{Task.}
This QA task evaluates LLMs’ ability to adapt contextual perspectives across different FoRs.
Both A and C splits are used in this task. 
The input is the context, consisting of a spatial expression $T$ and relatum orientation, if available, and a question $Q$ that queries the spatial relation from either an observer or the relatum’s perspective. 
The output is a spatial relation $S$, restricted to \{left, right, front, back\}.

% \pk{In the C-split, perspective information—introduced to clarify the A-split and create the C-split—is also included in the input.: not sure what is this. Just mention which slpit you consider for this, is it C , A or both, you have explained all before do not repeat. When you say c-split, it includes the augmented sentences from A-split too, you do not need to mention that unless you want to exclude them.} 



\noindent\textbf{Zero-shot baseline.} 
We call the LLM with instructions, a spatial context, $T$, and a question, $Q$, expecting a spatial relation as the response. 
The prompt instructs the model to answer the question with one of the candidate spatial relations without any explanations.


\noindent\textbf{Few-shot baseline.} 
We create four spatial expressions, each assigned to a single FoR class to prevent bias. Following the steps in Section~\ref{sec:QA_generation}, we generate a corresponding question and answer for each. These serve as examples in our few-shot prompting. The input to the model is instruction, example, spatial context, and the question.

\noindent\textbf{Chain-of-Thought baseline~\citep{wei2023chainofthoughtpromptingelicitsreasoning}.}
To create Chain-of-Thought (CoT) examples, we modify the prompt to require reasoning before answering.
We manually crafted reasoning explanations with the necessary information for each example we used in the few-shot setting.
The input to the model is instruction, CoT example, spatial context, and the question.



% \begin{table*}[t]
%     \tiny
%     \centering
%     \begin{tabular}{|l|c c c|c c c|c|c|c|c c c|c c c|c|}
%     \hline
%     Model & \multicolumn{3}{|c|}{Cow} & \multicolumn{3}{|c|}{Car}  & Box  & Pen & Avg. & \multicolumn{3}{|c|}{Cow}  & \multicolumn{3}{|c|}{Car}  & Avg. \\
%     \hline
    
%     Metric & I \% & R \% & Acc. & I \% & R \% & Acc. & \multicolumn{3}{|c|}{Acc.} & I \% & R \% & Acc. & I \% & R \% & Acc. & Acc. \\ 
%        \hline
%        & \multicolumn{5}{|c|}{Camera's perspective} & \multicolumn{3}{|c|}{Relatum's perspective} \\
%        \hline
%      Llama3-8B (0-shot) & $82.22$  & $79.39$ & $95.00$ & $94.06$ & $83.58$ & $65.24$ & $68.70$ & $65.73$ & $82.22$  & $79.39$ & $95.00$ & $94.06$ & $83.58$ & $65.24$ & $68.70$ & $65.73$ \\
%      Llama3-8B (4-shot) & $82.98$ & $86.07$ & $96.67$ & $92.21$ & $84.79$ & $56.78$ & $62.02$ & $57.53$ \\

%      Llama3-8B (CoT) & $52.06$ & $50.19$ & $58.33$ & $54.92$ & $52.33$ & $50.22$ & $46.56$ & $49.70$ \\
     
%      Llama3-8B (SG + COT)) & $76.30$  & $73.09$ & $66.67$ & $76.23$ & $75.63$ & $75.86$ & $75.95$ & $75.87$ \\
     
%          \hline
%      Llama3-70B (0-shot)& $62.52$ & $65.46$ & $73.33$ & $72.54$ & $64.32$ & $62.14$ & $61.83$ & $62.09$ \\
%      Llama3-70B (4-shot) & $62.23$ & $64.69$ & $85.83$ & $85.45$ & $65.83$ & $57.07$ & $61.83$ & $57.74$ \\
%      Llama3-70B (CoT) & $80.74$ & $79.58$ & $95.83$ & $94.88$ & $82.63$ & $77.15$ & $80.92$ & $77.69$ \\
%      Llama3-70B (SG + COT)& $73.57$ & $74.81$ & $100.00$ & $100.00$ & $77.47$ & $65.68$ & $67.75$ & $65.98$ \\
%      \hline
%      Qwen2-7B (0-shot) & $87.36$ & $89.31$ & $91.67$ & $93.85$ & $88.46$ & $72.34$ & $81.68$ & $73.67$ \\

%      Qwen2-7B (4-shot)& $86.66$ & $79.77$ & $87.50$ & $85.04$ & $85.66$ & $51.90$ & $59.73$ & $53.02$ \\

%      Qwen2-7B (CoT) & $80.83$ & $76.91$ & $91.67$ & $88.93$ & $81.58$ & $58.97$ & $63.55$ & $59.62$ \\
%      Qwen2-7B (SG + COT)) & $90.08$ & $93.32$ & $100.00$ & $98.57$ & $91.72$ & $75.86$ & $81.11$ & $76.60$ \\
%     \hline
%      Qwen2-72B (0-shot) & $95.56$ & $95.04$ & $100.00$ & $100.00$ & $96.13$ & $79.28$ & $83.59$ & $79.89$ \\
%      Qwen2-72B (4-shot) & $84.44$ & $85.50$ & $100.00$ & $100.00$ & $86.78$ & $78.26$ & $86.26$ & $79.40$ \\
%      Qwen2-72B (CoT) & $88.59$ & $83.40$ & $100.00$ & $100.00$ & $89.58$ & $85.46$ & $83.59$ & $85.19$ \\
%       Qwen2-72B (SG + COT)) & $88.97$ & $89.12$ & $100.00$ & $99.80$ & $90.53$ & $85.96$ & $87.40$ & $86.17$ \\ 
%      \hline
%     \end{tabular}
%     \caption{The accuracy of A-split frame of reference question-answering with various LLMs. In the Car and Cow cases, the parenthesis $(x, y)$ represents the ratio of correct answers where the model assumes $x$\% relative FoR and $y$\% intrinsic FoR for ambiguous expression.}
%     \label{tab:A_split-QA}
% \end{table*}

\begin{table*}[t]
    \setlength{\tabcolsep}{1mm}
    \small
    \centering
    \begin{tabular}{|l| c c | c | c c | c| c | c | c || c c | c | c c | c| c |}
    \hline
     & \multicolumn{9}{|c||}{Camera perspective} & \multicolumn{7}{|c|}{Relatum perspective} \\
    \cline{2-17}
     Model & \multicolumn{3}{|c|}{Cow} & \multicolumn{3}{|c|}{Car} & Box & Pen & Avg. &\multicolumn{3}{|c|}{Cow} & \multicolumn{3}{|c|}{Car} & Avg. \\
      \cline{2-17}
       & R\% & I\% & Acc. & R\% & I\% & Acc. & Acc. & Acc. & Acc. &R\% & I\% & Acc. & R\% & I\% & Acc. & Acc. \\ 
        \hline
Llama3-70B (1) & $48.1$ & $\mathbf{51.5}$ & $62.5$ & $\mathbf{58.0}$ & $41.6$ & $65.5$ & $73.3$ & $72.5$ & $64.3$  & $\mathbf{61.0}$ & $38.7$ & $62.1$ & $\mathbf{51.8}$ & $47.9$ & $61.8$ & $62.1$ \\
Llama3-70B (2) & $49.1$ & $\mathbf{50.5}$ & $62.2$ & $\mathbf{52.2}$ & $47.4$ & $64.7$ & $85.8$ & $85.5$ & $65.8$  & $\mathbf{59.6}$ & $40.1$ & $57.1$ & $\mathbf{55.5}$ & $44.2$ & $61.8$ & $57.7$ \\
Llama3-70B (3) & $49.4$ & $\mathbf{50.3}$ & $80.7$ & $49.4$ & $\mathbf{50.3}$ & $79.6$ & $95.8$ & $94.9$ & $82.6$  & $\mathbf{60.8}$ & $39.0$ & $77.2$ & $\mathbf{55.1}$ & $44.6$ & $80.9$ & $77.7$ \\
Llama3-70B (4) & $\mathbf{59.4}$ & $40.2$ & $73.6$ & $\mathbf{57.9}$ & $41.7$ & $74.8$ & $100.0$ & $100.0$ & $77.5$  & $\mathbf{60.6}$ & $39.1$ & $65.7$ & $\mathbf{56.0}$ & $43.7$ & $67.7$ & $66.0$ \\
\hline
Qwen2-72B (1) & $\mathbf{96.6}$ & $2.9$ & $95.6$ & $\mathbf{95.9}$ & $3.6$ & $95.0$ & $100.0$ & $100.0$ & $96.1$  & $8.8$ & $\mathbf{90.6}$ & $79.3$ & $7.8$ & $\mathbf{91.7}$ & $83.6$ & $79.9$ \\
Qwen2-72B (2) & $\mathbf{89.0}$ & $10.5$ & $84.4$ & $\mathbf{85.6}$ & $13.9$ & $85.5$ & $100.0$ & $100.0$ & $86.8$  & $17.7$ & $\mathbf{81.8}$ & $78.3$ & $10.4$ & $\mathbf{89.1}$ & $86.3$ & $79.4$ \\
Qwen2-72B (3) & $\mathbf{67.2}$ & $32.4$ & $88.6$ & $\mathbf{62.0}$ & $37.6$ & $83.4$ & $100.0$ & $100.0$ & $89.6$  & $21.3$ & $\mathbf{78.3}$ & $85.5$ & $22.7$ & $\mathbf{76.9}$ & $83.6$ & $85.2$ \\
Qwen2-72B (4) & $\mathbf{93.0}$ & $6.5$ & $90.1$ & $\mathbf{94.6}$ & $4.9$ & $93.3$ & $100.0$ & $98.6$ & $91.7$  & $8.2$ & $\mathbf{91.2}$ & $86.0$ & $10.5$ & $\mathbf{89.0}$ & $87.4$ & $86.2$ \\
\hline
GPT-4o (1) & $\mathbf{84.3}$ & $15.3$ & $94.5$ & $\mathbf{88.5}$ & $11.0$ & $97.3$ & $99.2$ & $99.8$ & $95.6$  & $21.6$ & $\mathbf{78.0}$ & $91.6$ & $16.1$ & $\mathbf{83.5}$ & $90.5$ & $91.4$ \\
GPT-4o (2) & $\mathbf{69.0}$ & $30.6$ & $76.6$ & $\mathbf{80.3}$ & $19.2$ & $89.5$ & $100.0$ & $100.0$ & $81.5$  & $29.0$ & $\mathbf{70.5}$ & $74.7$ & $30.9$ & $\mathbf{68.7}$ & $77.5$ & $75.1$ \\
GPT-4o (3) & $41.5$ & $\mathbf{58.3}$ & $92.3$ & $38.2$ & $\mathbf{61.6}$ & $91.0$ & $100.0$ & $99.8$ & $93.2$  & $33.9$ & $\mathbf{65.8}$ & $93.9$ & $32.0$ & $\mathbf{67.6}$ & $93.9$ & $93.9$ \\
GPT-4o(4) & $26.0$ & $\mathbf{73.9}$ & $79.2$ & $27.7$ & $\mathbf{72.1}$ & $79.4$ & $96.7$ & $94.3$ & $81.4$  & $16.2$ & $\mathbf{83.4}$ & $95.5$ & $19.2$ & $\mathbf{80.4}$ & $94.8$ & $95.4$ \\
\hline
    \end{tabular}
    \caption{QA accuracy in the A-Split across various LLMs. R\% and I\% represent the percentage the model assumes relative or intrinsic FoR for ambiguous expression explained in Section~\ref{sec:evaluation_setting}. Acc is the accuracy, and Avg is the micro-average of accuracy. (1): 0-shot, (2): 4-shot, (3): CoT, and (4): SG + CoT.}
    \label{tab:A_split-QA}
\end{table*}

% \vspace{-5mm}
\subsection{Text-To-Image (T2I)}\label{sec:t2i_models}

\noindent\textbf{Task.}  This task aims to determine the diffusion models' ability to consider FoR by evaluating their generated images. The input is a spatial expression, $T$, and the output is a generated image ($I$). We use the context from both C and A splits with external FoRs for this task.

\noindent\textbf{Stable Diffusion Models.} 
We use the stable diffusion models as the baseline for the T2I task. 
This model only needs the scene description as input. 
% \pk{Therefore, we provide $T$ to the model and expect an output image of $I$.: not needed, you can remove.}

\noindent\textbf{Layout Diffusion Models.}
We evaluate the Layout Diffusion model, a more advanced T2I model operating in two phases: text-to-layout and layout-to-image.
Given that LLMs can generate the bounding box layout~\citep{cho2023visualprogrammingtexttoimagegeneration}, we provide them with instructions and $T$ to create the layout. 
The layout consists of bounding box coordinates for each object in the format of \{object: $[x, y, w, h]$\}, where $x$ and $y$ denote the starting point and $h$ and $w$ denote the height and width. 
The bounding box coordinates and $T$ are then passed to the layout-to-image model to produce the final image, $I$. 


\subsection{Spatial-Guide Prompting}\label{sec:SG_prompting}
We hypothesize that the spatial relation types and FoR classes explained in Section~\ref{sec:primitives} can improve question-answering and layout generation.
For instance, the \textit{external intrinsic} FoR emphasizes that spatial relations originate from the relatum’s perspective.
To leverage this, we propose Spatial-Guided (SG) prompting, an additional step applied before QA or layout generation steps.
This step extracts spatial information, including direction, topology, distance as well as the FoR from spatial expression $T$. The extracted information will serve as supplementary for guiding LLMs in QA and layout generation.
We manually craft four examples covering these aspects.
First, we specify the perspective for \textit{directional relations}, e.g., \textit{left} relative to the observer, to distinguish intrinsic from relative FoR.
Next, we indicate whether the locatum is inside or outside the relatum for \textit{topological relations} to differentiate internal from external FoR.
Lastly, we provide an estimated quantitative distance to support topological and directional relation identification, such as \textit{far}.
These examples are then provided as a few-shot example for the model to extracted information automatically.
% SG prompting follows the few-shot setting that
% \pk{We provide prompting, spatial context, and SG examples for LLMs to extract spatial relations and FoR.: not clear, SG was itself the extracted information, or I am confused here.}
%  SG is the extracted information, this line only tell how to call model to generate SG