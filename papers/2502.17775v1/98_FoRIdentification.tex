

% \tp{Combine this with improvement with SG prompting}
% \begin{wraptable}[11]{r}{0.4\textwidth}
%     \tiny
%     \centering
%     \begin{tabular}{|l|c|c|}
%          \hline
%          Model & EI & ER  \\
%          \hline
%           SD-1.5 & $ 51.11$  & $ 21.61$ \\
%           SD-2.1 & $ 57.97$  & $ 21.49$ \\
%           \hline
%           Llama3-8B + GLIGEN & $ 53.67$  & $ 25.78$ \\
%           Llama3-70B + GLIGEN & $ 54.49$  & $ 29.45$ \\
%            Llama3-8B + SG + GLIGEN (Our)& $ 57.46$  & $ 27.96$ \\
%           Llama3-70B + SG + GLIGEN (Our) & $ 56.54$  & $ 30.59$ \\
%           \hline
%     \end{tabular}
%     \caption{The separate accuracy visualized Cow and Car Case in I-A-Split for external relative (ER) and external intrinsic interpretations (EI) of FoR.}
%     \label{tab:IA_cow_spilt}
% \end{wraptable}


% \subsubsection{FoR impact on Image Generation}
% We evaluate SG layout diffusion to assess the impact of using FoR on image generation.
% We focus on VISOR$_{cond}$ as it better reflects the model's spatial understanding than the overall performance measured by VISOR$_{uncond}$.
% Due to space limitations, VISOR$_{uncond}$ results are reported in Appendix~\ref{appendix:Visor_uncond}.
% Table~\ref{tab: I_split} shows that adding FoR information (Llama3 + SG + GLIGEN) improves performance across all splits compared to the baseline models (Llama3 + GLIGEN).
% The most significant gains occur when the relatum lacks intrinsic direction, making external relative FoR the only valid option. 
% However, the results show a significant bias towards the relative FoR of our model.
% This bias becomes more evident when comparing SD-2.1 with the baseline of our model (Llama3 + GLIGEN). 
% This illustrates that the GLIGEN only significantly improves spatial comprehension on relative FoR.
% In contrast, SD-2.1 surpasses all GLIGEN-based models, including ours, when FoR is intrinsic, as seen in the cond($I$) of the I-C split in Table~\ref{tab:I_split}.
% This limitation likely arises from the reliance on bounding boxes for generating spatial configurations, which makes it challenging to handle intrinsic FoR due to the lack of object properties and orientation.
% This challenge is further highlighted \iclr{in the separate corrected interpretations for I-A split}. \iclr{From these results }, GLIGEN only shows higher correct interpretation in external relative compared to SD-2.1.
% This confirms again that the main improvement in layout diffusion is in the relative FoR, which utilizes the camera perspective as coordinates for spatial relations.
% Regardless of GLIGEN's bias, incorporating FoR information from SG-prompting still improves all FoR classes. 
% \iclr{We provide further analysis of the improvement when employing SG in the layout generation in the Appendix~\ref{appedix:anaylize_SG_improment_t2i}.
% Our experimental observations also show that Llama's bias when generating layouts aligns with the identified FoR, which prefers external intrinsic in A-Split and external relative in C-Split.
% }


% Table~\ref{tab:I_split} shows that our proposed SG prompting enhances downstream tasks, i.e., text-to-image generation. 
% Our method improves the VISOR$_{cond}$ score by at least $2.32\%$ in I-A split and $0.57\%$ in I-C-Split.
% A significant improvement is observed in the I-A split, where our method boosts Llama3-8B + GLIGEN by around 5\% % for both the Cow-and-Car Case (I) and Pen-and-Box Case (R).


% However, there is only a tiny improvement when relatum has intrinsic directions. 
% To better comprehend the situation, we investigate the proportion of correct predictions in Llama3-8B + GLIGEN and SD-2.1 in Cow Case and Car Case of I-A spilled where the relatum can possess intrinsic directions. 
% We observed that SD-2.1 tends to favor intrinsic interpretation from the spatial expression, as shown in Figure~\ref{fig:Correct-I-A-Split}. 
% This shows a significant bias towards intrinsic FoR by the SD-2.1 model. 
% This result aligns with the findings from the textual experiments, where LLMs favored intrinsic FoR classes when relatum has intrinsic directions; see Cow and Car cases in Figure~\ref{fig:cow_car_case}. 
% Nevertheless, the proportion of relative interpretations of FoR in the layout diffusion models is higher than SD-2.1. 
% This also illustrates that the layout diffusion is biased towards the relative FoR when the relatum has intrinsic directions. 
% This can be due to generating the final spatial configuration based on the bounding boxes, 
% Consequently,  we can observe that the models' performance on I-C-split is slightly lower than normal stable diffusion in the Cow Case and Car Case. 

% \begin{table}[]
%     \centering
%     \begin{subtable}{c|c}
%          &  \\
%          & 
%     \end{subtable}
%     \caption{Caption}
%     \label{tab:my_label}
% \end{table}

% \noindent\textbf{Improvement by SG Layout.}
% Table~\ref{tab:I_split} shows that our proposed SG prompting enhances downstream tasks, i.e., text-to-image generation. 
% Our method improves the VISOR$_{cond}$ score by at least $2.32\%$ in I-A split and $0.57\%$ in I-C-Split.
% A significant improvement is observed in the I-A split, where our method boosts Llama3-8B + GLIGEN by around 5\% % for both the Cow-and-Car Case (I) and Pen-and-Box Case (R).
% To further explain these improvements, we evaluate the generated bounding boxes. We assess the spatial relations of bounding boxes generated in the I-C split, where spatial relations are interpreted as left or right, as shown in Table~\ref{tab:layout_results}. 
% We found that our SG prompting improved the bounding boxes generated by Llama3-70B by $3.48\% $. 
% However, Llama3-8B slightly decreased $0.22\%$ for left and right spatial relations. 
% This evaluation is done on all generated layouts from the I-C split, which is not the same portion evaluated by VISOR$_{cond}$. Therefore, we include layout$_{cond}$ score in the same table for comparison. In the same subset, Llama3-8B with our method still shows improvement, leading to a better VISOR$_{cond}$ score.
% Thus, by providing FoR information using SG prompting, Llama3 can generate better spatial configuration bounding boxes, ultimately improving the quality of image generation.



% \begin{table}[ht!]
%     \begin{adjustbox}{width=\columnwidth -0mm, center}
%     \begin{tabular}{|l | c c | c | c c | c |}
%     \hline
%          & \multicolumn{6}{c|}{VISOR(\%)} \\ \cline{2-7}
%         Model & unsound (I) & second (R) & second (avg) & cond (I) & cond (R) & cond (avg) \\
%         \hline
%         SD-1.5 & $ 35.06$  & $ 35.68$ & $ 35.40$ & $ 53.92$ & $ 53.77$ & $ 53.83$ \\
%         SD-2.1 & $\mathbf{45.98}$  & $ 46.59$ & $\mathbf{46.31}$ & $\mathbf{60.06}$ & $ 59.64$ & $ 59.83$ \\
%         \hline
%         Llama3-8B + GLIGEN & $ 33.98$  & $ 39.36$ & $ 36.89$ & $ 57.51$ & $ 65.98$ & $ 62.12$ \\
%         Llama3-70B + GLIGEN & $ 38.04$  & $ 46.04$ & $ 42.37$ & $ 56.47$ & $ 69.53$ & $ 63.49$ \\
%         Llama3-8B + SG + GLIGEN (Our) & $ 36.28$  & $ 44.43$ & $ 40.70$ & $ 58.84$ & $\mathbf{70.36}$ & $ \mathbf{65.15}$ \\
%         Llama3-70B + SG + GLIGEN (Our) & $ 38.23$  & $\mathbf{48.62}$ & $ 43.86$ & $ 56.77$ & ${70.04}$ & ${64.06}$ \\
%         \hline
%     \end{tabular}
%     \end{adjustbox}
%     \caption{VISOR score on the I-C-Split where $I$ refer to counterparts of the Cow Case and Car Case where locatum has intrinsic direction, and $R$ refer to counterparts of the Box Case and Pen case where locatum lacks intrinsic direction, $avg$ is mirco-average of $I$ and $R$. cond and unsound are explained in Section~\ref{sec:evaluation_setting}}
%     \label{tab:I-C-Split}
% \end{table}


% \begin{table}[ht!]
%     \centering
%     \begin{tabular}{|l | c |c c c c c c|}
%     \hline
%      & \textbf{OA} (\%) &  \multicolumn{6}{c |}{\textbf{VISOR} (\%)} \\\cline{3-8}
%      Model & & uncond & cond & 1 & 2 & 3 & 4 \\ 
%      \hline
% SD-1.5 &  47.71 &  43.51 &  68.72 &  79.69 &  54.62 &  29.53 &  10.20 \\
% SD-2.1 &  62.48 &  59.89 &  75.39 &  90.60 &  73.97 &  50.34 &  24.66 \\
% Llama3-8B + GLIGEN &  0.00 &  0.00 &  0.00 &  0.00 &  0.00 &  0.00 &  0.00 \\
% Llama3-8B + SG + GLIGEN &  55.93 &  50.48 &  83.17 &  79.41 &  59.72 &  40.97 &  21.81 \\
% Llama3-70B + GLIGEN &  57.88 &  53.17 &  81.43 &  81.14 &  63.01 &  44.21 &  24.31 \\
% Llama3-70B + SG + GLIGEN &  61.56 &  57.12 &  83.75 &  84.90 &  66.84 &  48.60 &  28.12 \\
% \hline
%     \end{tabular}
%     \caption{Ambiguous case}
%     \label{tab:my_label}
% \end{table}

% \begin{table}[ht!]
%     \centering
%     \begin{tabular}{|l | c |c c c c c c|}
%     \hline
%      & \textbf{OA} (\%) &  \multicolumn{6}{c |}{\textbf{VISOR} (\%)} \\\cline{3-8}
%      Model & & uncond & cond & 1 & 2 & 3 & 4 \\ 
%      \hline
% SD-1.5 &  38.47 &  35.40 &  53.83 &  73.56 &  43.73 &  19.12 &  5.18 \\
% SD-2.1 &  48.57 &  46.31 &  59.83 &  83.83 &  59.10 &  31.50 &  10.82 \\
% Llama3-8B + GLIGEN &  0.00 &  0.00 &  0.00 &  0.00 &  0.00 &  0.00 &  0.00 \\
% Llama3-8B + SG + GLIGEN &  44.77 &  40.70 &  65.15 &  72.20 &  47.98 &  28.72 &  13.89 \\
% Llama3-70B + GLIGEN &  45.89 &  42.37 &  63.49 &  72.52 &  50.04 &  31.25 &  15.69 \\
% Llama3-70B + SG + GLIGEN &  47.23 &  43.86 &  64.06 &  73.36 &  51.07 &  33.45 &  17.57 \\
% \hline
%     \end{tabular}
%     \caption{Clear case}
%     \label{tab:my_label}
% \end{table}




 % show that the pure-language LLMs do not have the same preference as VLMs when interpreting the perspective as highlighted by that paper.

% Several benchmarks created for testing spatial reasoning of multiple area in AI (textual)
% Most of them largely overlook the concept of FoR and using only one FoR across all instance. 
% The study of FoR in liu2023
% Another dedicated dataset that is built to evaluate VLM systematically.
% The GPT cannot adapt from the camera to another perspective in the scene, which aligns with our findings. However, we also show that even though GPT cannot convert spatial relation to another perspective from camera, it can convert another perspective into the camera.
% Several benchmarks have been developed across various domains to evaluate the spatial understanding of computation models. 
% In the text-based domain, recent benchmarks focus on navigating with spatial instructions~\citep{yamada2024evaluatingspatialunderstandinglarge} or question-answering tasks~\citep{shi2022stepgamenewbenchmarkrobust, mirzaee2022transferlearningsyntheticcorpora, rizvi2024sparcsparpspatialreasoning}.
% These benchmarks are developed to assess the spatial reasoning capability without paying attention to FoR.
% Existing research often lacks explicit consideration of FoR, and the benchmarks do not include FoR annotations. Consequently, evaluating FoR understanding remains a research gap in spatial reasoning-related work.
% Similarly, text-to-image (T2I) benchmarks~\citep{gokhale2023benchmarkingspatialrelationshipstexttoimage, huang2023t2icompbenchcomprehensivebenchmarkopenworld, cho2023dallevalprobingreasoningskills, cho2023visualprogrammingtexttoimagegeneration} face the same issue.
% They usually focus on correctly placing two objects based on spatial relation from the camera perspective and relative FoR. 
% Nevertheless, few works in vision-text domains are starting to recognize the importance of a FoR~\citep{chen2024spatialvlmendowingvisionlanguagemodels, liu2023visualspatialreasoning}.
% One notable study is provided by~\citealt{liu2023visualspatialreasoning}. 
% They provide a case study on the FoR and results showing that making the model capable of understanding the FoR affects downstream performance on visual question answering. However, their study is limited in terms of FoR categories. 
% In our work, we extend the coverage of benchmarks into more diverse frames of reference for the FoR recognition tasks. 
% Moreover, we are the first to study the impact of FoR identification on text-to-image generation as a downstream task.
