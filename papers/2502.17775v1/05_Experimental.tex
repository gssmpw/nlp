\begin{table*}[ht!]
    \small
    \setlength{\tabcolsep}{1mm}
    \centering
    \begin{tabular}{|l|c|c|c|c|c||c|c|c|c|c|}
    \hline
     & \multicolumn{5}{|c||}{Camera perspective} & \multicolumn{5}{|c|}{Relatum perspective} \\
    \cline{2 - 11}
    Model & ER (CP) & EI (RP) & II (RP) & IR (CP) & Avg. & ER (CP) & EI (RP) & II (RP) & IR (CP) & Avg. \\
    \hline
     Llama3-70B (0-shot) & $44.8$ & $38.4$ & $39.7$ & $54.4$ & $42.6$ &$42.2$ & $47.1$ & $62.5$ & $34.4$ & $45.1$ \\
     Llama3-70B (4-shot) & $43.0$ & $40.0$ & $39.1$ & $47.3$ & $41.9$ & $41.8$ & $60.9$ & $77.7$ & $35.2$ & $52.0$ \\
     Llama3-70B (CoT) & $57.8$ & $46.1$ & $44.7$ & $46.0$ & $51.5$ & $\mathbf{55.5}$ & $56.8$ & $71.5$ & $49.0$ & $56.6$ \\
     Llama3-70B (SG + CoT) & $47.6$ & $42.9$ & $50.0$ & $35.6$ & $45.0$ &$55.4$ & $64.5$ & $75.0$ & $47.1$ & $60.1$ \\
     \hline
     Qwen2-72B (0-shot) & $94.5$ & $35.2$ & $31.8$ & $93.2$ & $66.9$ & $28.7$ & $89.3$ & $93.6$ & $23.8$ & $59.0$ \\
     Qwen2-72B (4-shot) & $90.2$ & $39.5$ & $39.1$ & $68.5$ & $65.3$ & $33.5$ & $92.1$ & $94.0$ & $29.5$ & $62.7$ \\
     Qwen2-72B (CoT) & $81.4$ & $57.4$ & $58.6$ & $62.5$ & $69.1$ & $39.5$ & $83.7$ & $85.2$ & $37.7$ & $61.6$ \\
     Qwen2-72B (SG + CoT) & $97.6$ & $42.5$ & $31.3$ & $93.8$ & $71.4$ & $42.8$ & $86.6$ & $92.0$ & $34.0$ & $64.5$ \\
     \hline
    GPT-4o (0-shot)  & $79.7$ & $45.1$ & $39.5$ & $90.2$ & $64.2$  & $46.9$ & $88.5$ & $98.2$ & $34.8$ & $67.5$ \\
    GPT-4o (4-shot) & $68.0$ & $52.6$ & $60.7$ & $74.1$ & $61.8$  & $44.9$ & $\mathbf{98.2}$ & $\mathbf{100.0}$ & $37.5$ & $71.2$ \\
    GPT-4o (CoT) & $81.7$ & $\mathbf{76.1}$ & $\mathbf{82.4}$ & $71.5$ & $78.8$  & $53.0$ & $91.1$ & $90.6$ & $\mathbf{50.8}$ & $71.9$ \\
    GPT-4o (SG + CoT)  & $\mathbf{97.9}$ & $72.2$ & $72.7$ & $\mathbf{93.4}$ & $\mathbf{85.8}$  & $48.9$ & $96.3$ & $95.9$ & $36.1$ & $\mathbf{71.8}$ \\
\hline
    \end{tabular}
    \caption{QA accuracy in the C-Split across various LLMs. ER, EI, II, and IR  denote external relative, external intrinsic, internal intrinsic, and internal relative FoRs. Avg represents the micro-average accuracy. CP refers to context with camera perspective, while RP denotes context with relatum perspective.}
    \label{tab:QA_c_split}
\end{table*}

\subsection{Evaluation Metrics}\label{sec:evaluation_setting}
\noindent\textbf{QA.}  We report an accuracy measure defined as follows. Since the questions can have multiple correct answers, specifically in A-split, as explained in Section~\ref{sec:DatasetCreation}, the prediction is correct if it matches any valid answer.
Additionally, we report the model’s bias distribution when FoR ambiguity exists.
$I$\% is the percentage of correct answers when assuming an intrinsic FoR, while $R$\% is this percentage with a relative FoR assumption. 
Note that cases where both FoR assumptions lead to the same answer are excluded from these calculations.

\noindent\textbf{T2I.} 
We adopt  \textit{spatialEval}~\citep{cho2023visualprogrammingtexttoimagegeneration} approach for evaluating T2I spatial ability. However, we modify it to account for FoR.
We convert all relations to a camera perspective before passing them to spatialEval, which assumes this viewpoint.
Accuracy is determined by comparing the bounding box and depth map of the relatum and locatum. 
For FoR ambiguity, a generated image is correct if it aligns with at least one valid FoR interpretation.
We report results using VISOR$_{cond}$ and VISOR$_{uncond}$~\citep{gokhale2023benchmarkingspatialrelationshipstexttoimage}, metrics for assessing T2I spatial understanding.
VISOR$_{cond}$ evaluates spatial relations only when both objects appear correctly, aligning with our focus on spatial reasoning rather than object creation. In contrast, VISOR$_{uncond}$ evaluates the overall performance, including object creation errors.

\subsection{Experimental Setting}
\noindent\textbf{QA.} We use Llama3-70B~\citep{dubey2024Llama3herdmodels}, Qwen2-72B~\citep{qwen2model}, and GPT-4o (\textit{gpt-4o-2024-11-20})~\citep{openai2024gpt4technicalreport} as the backbones for prompt engineering. 
To ensure reproducibility, we set the temperature of all models to 0.
For all models, we apply \textit{zero-shot}, \textit{few-shot}, \textit{CoT}, and our proposed prompting with CoT (SG+CoT).
%The detail of SG prompting is in Section~\ref{sec:SG_prompting}. 
%The creation of examples used for ICL is detailed in Section~\ref{sec:QA_explanation}.

\noindent\textbf{T2I.}
We select Stable Diffusion SD-1.5 and SD-2.1~\citep{rombach2021highresolution} as our stable diffusion models and GLIGEN\citep{li2023gligenopensetgroundedtexttoimage} as the layout-to-image backbone.
For translating spatial descriptions into textual bounding box information, we use Llama3-8B and Llama3-70B, as detailed in Section~\ref{sec:t2i_models}. 
The same LLMs are used to generate spatial information for SG prompting. 
We generate four images to compute the VISOR score following~\cite{gokhale2023benchmarkingspatialrelationshipstexttoimage}
Inference steps for all T2I models are set to 50.
For the evaluation modules, we select grounding DINO~\citep{liu2024groundingdinomarryingdino} for object detection and DPT~\citep{ranftl2021visiontransformersdenseprediction} for depth mapping, following VPEval~\cite{cho2023visualprogrammingtexttoimagegeneration}. The experiments were conducted on an $A6000$ GPU, totaling approximately $300$ GPU hours.

\subsection{Results}

\noindent\textbf{RQ1. What is the bias of the LLMs for the ambiguous FoR? }
Table~\ref{tab:A_split-QA} presents the QA results for the A-split. 
Ideally, a model that correctly extracts the spatial relation without considering perspective should achieve 100\% accuracy, as the context lacks a fixed perspective. 
However, this ideal model is not the focus of our work. We aim to assess model bias by measuring how often LLMs adopt a specific perspective when answering.
In the Cow and Pen case, relatum properties do not introduce FoR ambiguity in directional relations, making the task pure extraction rather than reasoning.
Thus, we focus on the $I$\% and $R$\% of the Cow and Car cases, which best reflect LLMs’ bias.
Qwen2 achieves around 80\% accuracy across all experiments by selecting spatial expressions directly from the context, suggesting it may disregard the question’s perspective.
GPT-4o shows similar bias in 0-shot and 4-shot settings but shifts toward intrinsic interpretation with CoT. This bias reduces accuracy in camera-perspective questions from 93.2\% to 81.4\%, where FoR adaptation is more challenging than relation extraction. 
Llama3-70B lacks a strong preference, balancing assumptions but slightly favoring relative FoR. This uncertainty lowers performance, requiring more reasoning to reach the correct answer.
In summary, Qwen2 achieves higher accuracy by focusing on relation extraction without considering FoR reasoning, while other models attempt reasoning but struggle to reach correct conclusions, leading to lower performance.


\begin{table*}[ht!]
    \centering
    \setlength{\tabcolsep}{1mm}
    \small
    \begin{tabular}{|l | c c c | c | c | c | c | c | c |}
    \hline
         & \multicolumn{8}{c|}{VISOR(\%)} \\ \cline{2-9}
          & \multicolumn{5}{|c|}{ A-Split } &  \multicolumn{3}{|c|}{ C-Split }\\ \hline
        Model & \multicolumn{3}{|c|}{cond (I)} & cond (R) & cond (avg) & cond (I) & cond (R) & cond (avg) \\ \cline{2-4}
        & EI FoR & ER FoR & all & & & & & \\ \hline
        SD-1.5   & $ 51.11$  & $ 21.61$  &  $ 72.72$ & $ 48.95$ & $ 68.72$ & $ 53.92$ & $ 53.77$ & $ 53.83$ \\
        SD-2.1  & $ 57.97$  & $ 21.49$ &  $ 79.46$ & $ 54.10$ & $ 75.39$ & $\mathbf{60.06}$ & $ 59.64$ & $ 59.83$ \\
        \hline
        Llama3-8B + GLIGEN& $ 53.67$  & $ 25.78$ & $ 79.45$ & $ 66.08$ & $ 77.38$ & $ 57.51$ & $ 65.98$ & $ 62.12$ \\
        Llama3-70B + GLIGEN & $ 54.49$  & $ 29.45$ & $ 83.94$ & $ 68.68$ & $ 81.43$ & $ 56.47$ & $ 69.53$ & $ 63.49$ \\
        Llama3-8B + SG + GLIGEN (Our) & $ 57.46$  & $ 27.96$ & $ 85.42$ & $\mathbf{71.14}$ & $ 83.17$ & $ 58.84$ & $\mathbf{70.36}$ & $ \mathbf{65.15}$ \\
        Llama3-70B + SG + GLIGEN (Our)  & $ 56.54$  & $ 30.59$ & $ \mathbf{87.13}$ & $ 66.56$ & $\mathbf{83.75}$ & $ 56.77$ & ${70.04}$ & ${64.06}$ \\
        \hline
    \end{tabular}
    \caption{VISOR$_{cond}$ score on the A and C splits where $I$ refer to the Cow case and Car case where relatum has intrinsic directions, and $R$ refer to the Box case and Pen case where relatum lacks intrinsic directions, $avg$ is mirco-average of $I$ and $R$. cond are explained in Section~\ref{sec:evaluation_setting}. EI and ER FoR represent the generated image considered corrected by EI or ER FoR }
    \label{tab:I_split}
\end{table*}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{Figures/cf_matrix_change_perspective3.png}
    \caption{Confusion matrices of spatial relation answers when Llama3 and GPT-4o are required to adapt FoR in the 0-shot and (SG+CoT) settings.}
    \label{fig:cf_conversion}
\end{figure}

\noindent\textbf{RQ2. Can the model adapt FoR when answering the questions?}
To address this research question, we analyze QA that required FoR comprehension results in C-Split from Table~\ref{tab:QA_c_split}.
Note that the context and question in these tasks explicitly indicate a perspective.
The results indicate that LLMs struggle with FoR conversion, particularly when the question has relatum and the context has camera perspectives, achieving only up to 55.5\% accuracy.
We further demonstrate how Llama3 and GPT-4o adapt FoR using the confusion matrix in Figure~\ref{fig:cf_conversion}. 
Our findings reveal that pure-text LLM (Llama3) has confusion between left and right.
Humans typically reverse front and back while preserving left and right when describing the spatial relation from perspective.
However, Llama3 incorrectly reverses left and right, leading to poor adaptation to the camera perspective.
In contrast, very large multimodal-language models like GPT-4o follow the expected pattern, as observed by~\citealt{comfortFoR}.
While our GPT-4o results suggest some ability to convert the relatum’s perspective into the camera’s with in-context learning (72\% accuracy), the reverse transformation in the textual domain remains challenging (53\% accuracy). 
This difficulty persists when converting spatial relations from the camera perspective from images to the relatum’s perspective as observed in~\citealt{comfortFoR}.

\noindent\textbf{RQ3. How can an explicit FoR identification help spatial reasoning in QA?}
We compare CoT and CoT+SG results to evaluate how explicit FoR identification affects LLMs’ spatial reasoning in QA.
Based on C-Split results (Table~\ref{tab:I_split}), incorporating SG encourages the model to identify the perspective from input expression ranging from 2.9\% to 30\% in cases where the context and question share the same perspective. 
These cases are easier as the models do not need FoR adaptation. 
%The only exception is Llama3 for questions with the camera’s perspective, where explicit FoR identification with SG prompting negatively impacts performance.
With one exception, as can be seen, the performance level of the Llama3 baseline, for questions with camera perspective, is so poor that FoR identification in SG cannot help boost its performance compared to the improvements made in other language models.
We should note that among our selected LLMs, Llama3 is the only one not trained with visual information; we speculate this can be a factor in LLMs' understanding of FoR.
SG is less effective in reasoning when the context and question have different perspectives despite aiding in identifying the correct FoR of the spatial description in the context. 
% but does not improve reasoning for converting spatial relations across perspectives.
This limitation is evident in A-Split results (Table~\ref{tab:A_split-QA}), where models only show significant improvement when SG aligns their preference with the question’s perspective, as seen in Qwen2-72B and GPT-4o.
SG identification results are reported in the Appendix~\ref{appendix:FoRIdentification}.
Still, incorporating FoR identification improves overall spatial reasoning (see the Avg column for SG+CoT in Table~\ref{tab:I_split}).

\noindent\textbf{RQ4. How can explicit FoR identification help spatial reasoning in visualization?}
We evaluate SG layout diffusion to assess the impact of incorporating FoR in image generation. 
We focus on VISOR$_{cond}$ metric, as it better reflects the model’s spatial understanding than the overall performance measured by VISOR$_{uncond}$, which is reported in Appendix~\ref{appendix:Visor_uncond} due to space limitation.
Table~\ref{tab:I_split} shows that adding spatial information and FoR classes (SG+GLIGEN) improves performance across all splits compared to the baseline models (GLIGEN).
SG improved the model's performance when expressions can be interpreted as relative FoR.
These results align with the QA results shown in Table~\ref{tab:A_split-QA} indicating that \textit{Llama3  prefers relative FoR if dealing with the camera's perspective}.
In contrast, baseline diffusion models (SD-1.5 and SD-2.1) perform better for intrinsic FoR even though GLIGEN is based on SD-2.1.
This outcome might be due to GLIGEN's reliance on bounding boxes for generating spatial configurations, which makes it struggle with intrinsic FoR due to the absence of object properties and orientation. Nevertheless, incorporating FoR information via SG-prompting improves performance across all FoR classes despite this specific bias.
We provide further analysis on SG for the layout generation in Appendix~\ref{appedix:anaylize_SG_improment_t2i}.