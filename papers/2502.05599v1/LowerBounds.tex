%We begin this section by lower bounding the regret of the most natural algorithm 
%that bids $b_t=v_t$. This choice automatically ensures that the ROSC is satisfied since
%$p_t(b_t) \le v_t$ from \eqref{eq:myersontruthfulcond} for $b_t=v_t$.
{\bf For deriving our lower bounds} we will consider the following special class of the allocation and payment functions $x_t(b),p_t(b)$ that are of threshold type, denoted by $x_t^\theta, p_t^\theta$, defined as follows.
\begin{align} \label{def:thresholdfunc}
x_t(b) =
&\left\{
\begin{aligned}
1 & \ \ \text{for}\ \ \ b \ge \theta,\\
0 &\ \  \text{otherwise,}
\end{aligned}\right.
& 
p_t^\theta(b) = 
\left\{\begin{aligned}
 \theta &\ \  \text{for}\ b \ge \theta,\\
 0 &\ \  \text{otherwise,}
\end{aligned}\right. 
%&\text{, pentru legea I, ?i}
%&\text{pentru legea a II-a}
\end{align}
%One special class of the allocation functions $x_t(b)$, that will be useful for deriving bounds on the regret of any online algorithm is of threshold type, defined as follows.
%\begin{equation}\label{def:thresholdfunc}
%x_t(b) = \begin{cases} 1 & \text{for}\ b \ge \theta, \\
% 0 & \text{otherwise}. \end{cases}
%\end{equation}
where $\theta\le 1$ without loss of generality (WLOG) since $v_t\le 1$.
%We let  to denote an allocation function of type \eqref{def:thresholdfunc}. 
%For allocation functions satisfying  \eqref{def:thresholdfunc}, the payment function \eqref{eq:myersontruthfulcond} simplifies to 
%\begin{equation}\label{eq:myersontruthfulcondsimpler}
%p_t^\theta(b) = \begin{cases} \theta & \text{for}\ b \ge \theta, \\
% 0 & \text{otherwise}. \end{cases}
%\end{equation}
When we consider allocation function of type \eqref{def:thresholdfunc}, succinctly we write the input as $(v_t, x_t^\theta)$, where $(v_t, \theta_t)$ is i.i.d. generated from an unknown distribution, and the payment function is \eqref{def:thresholdfunc}.

Note that the threshold allocation and payment functions \eqref{def:thresholdfunc} satisfy the Myerson's
condition \cite{myerson1981optimal}, %\vspace{-0.1in}
\begin{equation}\label{eq:myersontruthfulcond}
p_t(b) = b x_t(b) - \int_0^b x_t(u) du,
\end{equation}
that is satisfied by large classes of auction, e.g. second price auction. Thus, our derived lower bounds are valid also for structured inputs and for not inputs chosen absolutely adversarially.

 \begin{rem} Myerson's
condition is known to evoke truthful bids in a usual auction where the utility is simply $v_t-p_t(b_t)$ without any constraints. 
\end{rem}



\begin{definition}\label{defn:slotwin}
For allocation functions of type $x_t^{\theta_t}$ \eqref{def:thresholdfunc}, 
an algorithm is defined to {\bf win} a slot if its bid $b_t$ is at least as much as $\theta_t$. A slot that is not won is called {\bf lost}. Let $\b1_{\cA}(t)=1$ if algorithm $\cA$ wins the slot $t$, and is zero otherwise.
\end{definition}
With allocation functions of type \eqref{def:thresholdfunc}, the contribution to ROSC-S comes only from slots that are {\it won}, i.e., when  $b_t\ge \theta_t$, and is equal to $v_t - \theta_t$, which can be both positive and negative. To satisfy the ROSC, the expected positive contribution has to be used judiciously to win slots where the expected contribution is negative.
%Thus,  the total slack available for bidding available with any online algorithm while satisfying the ROSC is 
%$\sum_{t, v_t >\theta_t} (v_t - \theta_t)$, which can be used to win slots with $v_t < \theta_t$. 
To illustrate this concretely, consider the following example.
\begin{example}\label{exm:1}
Let $v_t=v=1/2$ for all $t$. Moreover, let the allocation function be of type  \eqref{def:thresholdfunc}, where  $\theta_t \in \{0.4, 0.6, 0.7\}$ with equal probability in any slot $t$. The positive contribution ROSC-S comes from slots where $\theta_t=0.4$ of amount $v-\theta_t=0.1$.
In expectation, there are $T/3$ slots with  $\theta_t=0.4$, thus the total expected positive contribution to ROSC is $0.1 T/3$ which can be 
used by any algorithm to bid for slots when $\theta_t\in \{0.6, 0.7\} > v$, where it can either get a negative per-slot contribution of $0.1$ or $0.2$ towards ROSC-S. To satisfy the ROSC, any algorithm has to win slots such that the sum of the expected negative contribution to ROSC is at least $-0.1 T/3$.
\end{example}
%\vspace{-0.1in}
\section{WarmUp}\label{sec:warmup}
To better understand the problem, we first consider the simplest algorithm $\cA_s$ that always bids $b_t=v_t$. From \eqref{eq:paymentcondition}, it follows that $\cA_s$ satisfies the ROSC. Thus, the 
only question is what is the regret of $\cA_s$?

%Next, we show that $\cA_s$ that satisfies the ROSC has at least $\Omega(T)$ regret, even when $v_t=v$ for all $t$. Since achieving $O(T)$ regret is trivial, thus, essentially we get that 
\begin{lemma} $\cR_{\cA_s}(T) =\Omega(T)$ regret even when $v_t=v\  \forall  \ t$.
\end{lemma}
%Thus $\cA_s$ that satisfies the ROSC trivially, is a `weak' algorithm, and more intelligent algorithms are needed for optimally solving Problem \eqref{eq:prob}.
%\vspace{-0.1in}
\begin{proof}
Let $v_t = 1/2$ for any $t$. For any slot $t$, let  the allocation function be $x_t^{\theta_t}$ and $\theta_t = v_t \pm \epsilon$ with equal probability with $0 < \epsilon < 1/2$. Then $\cA_s$ that bids $b_t=v_t$ gets a valuation of $v_t=1/2$ with probability $1/2$ whenever $\theta_t=v_t-\epsilon$ and a valuation of $0$ with probability $1/2$ whenever $\theta_t=v_t+\epsilon$. Thus the expected accrued valuation of $\cA_s$ is $\frac{T}{2} \cdot \frac{1}{2}$.

$\opt$ on the other hand can choose to bid $b_t^\opt=\theta_t, \ \forall \ t$,  and accrues a valuation of $1/2$ on each slot, while satisfying the ROSC, since $p_t^{\theta_t}(b^\opt_t) - v_t x_t^{\theta_t}(b^\opt_t) = \pm \epsilon$ with equal probability.
%Only thing we need to check is whether choosing $b_t=\theta_t, \ \forall \ t$ . 
%Whenever $\theta_t=v_t-\epsilon$, $p_t^{\theta_t}(b^\opt_t) - v_t x_t^{\theta_t}(b^\opt_t) = -\epsilon$ and 
%whenever $\theta_t=v_t+\epsilon$, $p_t^{\theta_t}(b^\opt_t) - v_t x_t^{\theta_t}(b^\opt_t) = \epsilon$. Since  $\theta_t = v_t \pm \epsilon$ with equal probability, therefore
%$$\bbE\left\{\sum_{t=1}^Tp_t^{\theta_t}(b^\opt_t)\right\} =   \bbE\left\{\ \sum_{t=1}^T v_t x_t^{\theta_t}(b^\opt_t)\right\}$$ 
%and hence $\opt$ satisfies the constraint. 
Thus, $\cR_{\cA_s}(T) =  \frac{T}{2} - \frac{T}{4} \ge T/2$.
\end{proof}
We next consider the question of whether a more intelligent algorithm can do better than $\cA_s$ and answer it in the negative.
%Clearly, since $v_t\le 1$, the regret of $\cA_s= O(T)$.
%We next show surprisingly that $\cA_s$ is an order-wise optimal algorithm by showing that any algorithm that satisfies the ROSC exactly has to suffer a regret of $\Omega(T)$.

%After showing that algorithm $\cA_s$ is weak in terms of regret, we next turn our attention to the currently best known algorithm \cite{Feng} that is based on primal-dual technique, and derive a lower bound on its regret. 
%that has been shown to have $O(\sqrt{T})$ regret for Problem \eqref{eq:prob} but in the process violates the ROSC by an amount of at most $O(\sqrt{T})$. We next show that the regret bound for \cite{Feng}'s algorithm is tight.

%Next, we show a negative result that the regret of any algorithm for solving Problem \eqref{eq:prob} that satisfies the ROSC is $\Omega(\sqrt{T})$ even when $v_t=v \ \forall \ t$. Thus, learning the allocation functions itself $x_t$ even when $v_t$ is known for any online algorithm is challenging.

%Two important questions that arise from Theorem \ref{thm:lbuniv} are i) does there exist an $\cA$ with $\cR_\cA(T)= O(\sqrt{T})$ satisfying the ROSC  when $v_t=v \ \forall \ t$, and ii) what is the regret lower bound when $v_t\ne v \ \forall \ t$. We answer the first question next, and defer the second question to Section \ref{sec:gen}.
%, where we show that the regret of 
%any online algorithm is at least $\Omega(T)$ when  $v_t\ne v \ \forall \ t$.
% for solving Problem \eqref{eq:prob} satisfying the ROSC.



%We will need the following basic fact to proceed further for deriving the regret lower bound for any online algorithm. 
%
%{\bf Prediction Problem :} Consider two Bernoulli distributions $\cP$ and $\cQ$ with means $x$ and $x+\beta$, respectively, where $x>0, \beta >0$. 
%A coin is tossed repeatedly with probability of heads distributed according to either $\cP$ or $\cQ$, where repeated tosses are independent. 
%From the observed samples, the problem is to predict the correct 
%distribution $\cP$ or $\cQ$ such that the success probability of the prediction is at least $1-\gamma$. 
%
%
%\begin{proposition}\label{lem:prediction} %\cite{Theorem 14.2, lattimore2020bandit} 
%  Let $n$ be fixed. Consider any algorithm $\cA_n$ that obtains $n$ samples and solves the prediction problem with success probability  at least $1-\gamma$. Then $n >\left( \frac{\ln 2}{ 8\beta^2} \log \left(\frac{1}{4\gamma}\right)\right)$ for $\beta<1/4$.
%\end{proposition}
%
%
%
%Consider any online algorithm $\cA$ for solving \eqref{eq:prob} where input is either Input 1 or Input 2.
%Proposition \ref{lem:prediction} implies that $\cA$ cannot distinguish whether the input is Input 1 or Input 2 with probability $>1/2$ unless it obtains $\Omega(1/\delta^2)$ samples. Let $\delta=1/T^{1/4}$, thus, by time less than $c \sqrt{T}$ (for a constant $c$), any algorithm $\cA$ for solving \eqref{eq:prob} cannot distinguish whether input is either Input 1 or Input 2 with probability more than $1/2$. 
%
%Compared to $\opt$'s decision choice that knows whether the input is Input 1 or 2, $\cA$'s job is hard in choosing to bid either $b_t = C$ or $D$. 
%If $\cA$ always chooses $b_t=C$ until time slot $c \sqrt{T}$, then with probability at least $1/2$ the input is Input 1 and $\cA$ will lose $c \sqrt{T}/4$ slots in expectation, since whenever $\theta_t=D$ (which happens with probability $1/4$) $\cA$ loses slot $t$. Thus, the regret for $\cA$ if it always chooses $b_t=C$ until time $c \sqrt{T}$ is $c \sqrt{T}/8$. Similar conclusion follows even if $\cA$ always chooses $b_t=C$ for $c_1 \sqrt{T}$ slot among the first  $c \sqrt{T}$ slots, where $c_1$ is a constant with $c_1<c$.
%
%Thus, to avoid $\Omega(\sqrt{T})$ regret, $\cA$ has to choose $b_t=D$ for some $c_2\sqrt{T}$ slots until time slot $c \sqrt{T}$ where $c_2$ is a constant with $c_2<c$. In this case, however, we will exploit the constraint 
%satisfaction condition of \eqref{eq:prob}. Under this choice of $\cA$, with probability at least $1/2$ the input is Input 2.  Thus, for all slots where $\cA$ chooses to 
%bid $b_t =D$ and $\theta_t=D$ (which happens with constant probability in any slot until the first $c\sqrt{T}$ slots because of the choice made by $\cA$), the $\text{Margin}(t) = v_t-\theta_t = \frac{1}{2}-D <0$. However, with Input $2$, $\bbE\{\theta_t| \theta_t\in\{A,B\}\} + C =0$. Let $T'$ be the set of slots where $\cA$ chooses to 
%bid $b_t =D$ and $\theta_t=D$, and suppose after $c\sqrt{T}$ $\cA $ correctly identifies the input to be Input 2, and always bids $b_t=C$. In this case, the overall constraint can be written as 
%\begin{align}
%\bbE\left\{\sum_{t=1}^Tv_t -p_t \right\}&  = \sum_{t\in T'} \frac{1}{2}-D+ \sum_{t \in T\backslash T'}\bbE\{v_t-\theta_t\}, \\
%&\stackrel{(a)}= \sum_{t\in T'} \frac{1}{2}-D + 0,\\
%& < 0,
%\end{align} 
%where $(a)$ follows $\bbE\{\theta_t| \theta_t\in\{A,B\}\} + C =0$.
%Thus, in case $\cA$ chooses $b_t=D$ for some $c_2\sqrt{T}$ slots until time slot $c \sqrt{T}$ where $c_2$ is a constant with $c_2<c$, it violates the constraint. 
%

%\end{proof}

