%\vspace{-0.1in}
\section{Lower Bound on Regret of Any Online Algorithm that strictly satisfies the ROSC}\label{sec:gen}
%The main result of this section is stated next, that shows the limitation of any online algorithm when $v_t\ne v, \ \forall \ t$ in obtaining a better than linear regret for solving Problem \eqref{eq:prob}. 
\begin{theorem}\label{thm:lbunivGenV}
$\cR_\cA  = \Omega(T)$ for any $\cA$ that solves \eqref{eq:prob} while strictly satisfying ROSC.
\end{theorem}
Theorem \ref{thm:lbunivGenV} is a {\bf major negative result} and precludes the existence of any online algorithm with sub-linear regret while 
strictly satisfying the ROSC.
The proof of Theorem \ref{thm:lbunivGenV} is provided in Appendix \ref{app:lbnotequal}, 
where the main idea is to construct two inputs 
with small K-L divergence (making them difficult to distinguish for any $\cA$) but for which the behaviour of $\opt$ is very different to ensure that the 
ROSC is satisfied. Interestingly, the lower bound is derived when $v_t$ takes only two possible values $\sfv_1, \sfv_2$ such that that $\sfv_2/\sfv_1<2$. Then $\cA$'s limitation in not knowing the input beforehand is utilized to derive the result. 

Moreover, Theorem \ref{thm:lbunivGenV} is derived by using allocation functions of the type \eqref{def:thresholdfunc} that also satisfy the Myerson's condition. Thus, Theorem \ref{thm:lbunivGenV} is also surprising since the ROSC is a constraint in expectation and there is a lot of structure to the problem, e.g., the input is i.i.d., and the allocation and the payment functions are structured. 

Trivially, Algorithm $\cA_s$ that always bids $b_t=v_t$ has regret $O(T)$. Thus, we get the following result.
\begin{corollary} Algorithm $\cA_s$ that always bids $b_t=v_t$ and exactly satisfies the ROSC is order-wise optimal.
\end{corollary}

\begin{rem} Note that Theorem \ref{thm:lbunivGenV} sheds no light on the refined question: what is the smallest regret possible when ROSC is allowed to be violated by $O(T^\alpha)$ for $0< \alpha<1$.
\end{rem}

%where the main idea is similar to that of Theorem \ref{thm:lbuniv}, but now the analysis is more delicate where we have the freedom in choosing multiple values for $v_t$. In particular, we construct two inputs where $v_t$ takes only two possible values $\sfv_1, \sfv_2$ such that that $\sfv_2/\sfv_1<2$ (that ensures that the behaviour of $\opt$ is very different with the two inputs) and is used 
%to `confuse' any $\cA$ that is unaware of the particular input. The two inputs are chosen to have
% small K-L divergence which makes them difficult to distinguish for any $\cA$. 
 %The proof is provided in Appendix \ref{app:proof:thm:lbunivGenV}.
 
 In light of Theorem \ref{thm:lbunivGenV}'s negative result,  instead of regret, the more meaningful performance metric for studying Problem \eqref{eq:prob} is the competitive ratio \cite{vaze2023online} defined as follows. 
%the ratio of the accrued valuation of an algorithm and 
%that of the $\opt$ minimized over all possible inputs (in this case distributions). Formally, the competitive ratio $\mu_\cA$ for $\cA$ is defined as follows.
\begin{definition}\label{defn:compratio} For any $\cA$, its competitive ratio
  $\mu_{\cA} = \min_{\cD} \frac{ \bbE\left\{\ \sum_{t=1}^T v_t x_t(b^\cA_t)\right\} }{ \bbE\left\{\ \sum_{t=1}^T v_t x_t(b^\opt_t)\right\} },$
  where  $\cA$ and $\opt$ satisfy ROSC, and $\cD$ is the unknown distribution.
\end{definition} The objective is to derive an algorithm with largest competitive ratio as possible, which we do in Section \ref{sec:competitive}. 


Next, we continue our lower bound expedition and consider Problem \ref{eq:prob} in the repeated identical auction setting where $v_t=v$ for all $t$, where the goal is only to maximize the number of slots won by an algorithm while strictly satisfying the ROSC.