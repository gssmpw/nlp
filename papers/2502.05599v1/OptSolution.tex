



\section{$1/2$-Competitive Algorithm for Problem \ref{eq:prob}}\label{sec:competitive}
In this section, for notational simplicity, we consider the discrete version of Problem \ref{eq:prob}, where in any slot $t$, $v_t$ takes $K$ (arbitrary) possible values $\sfv_1 > \dots > \sfv_K, \sfv_1\le 1$ with probabilities $p_1, \dots, p_K$ that are independent of $T$. Note that $K$, $\sfv_1 > \dots > \sfv_K$ and $p_1, \dots, p_K$ are unknown to begin with. We consider allocation and payment functions 
of type \eqref{def:thresholdfunc}. The results generalize in a straightforward manner for general allocation and payment functions unlike Theorem \ref{thm:ubequalcase} which is specific to allocation and payment functions 
of type \eqref{def:thresholdfunc}.
%Let $q_{k,\gamma} = \bbP(\theta_t \in (v_k, \gamma))$, while $r_{k,\gamma} = \bbE\{ \theta_t | \theta_t \in (v_k, \gamma)\}$.

Let the expected per-slot positive and negative  contribution to ROSC when $v_t=\sfv_k$ be 
$\ell_k = \bbE\{\sfv_k - \theta_t|v_t=\sfv_k,  \theta_t\le \sfv_k\}$ and $r_k = \bbE\{ \theta_t-\sfv_k |v_t=\sfv_k,  \theta_t> \sfv_k\}$, respectively. Moreover, let  
$\overleftarrow{w}_k = \bbP( \theta_t\le v_t|v_t= \sfv_k)$ and $\overrightarrow{w}_k = \bbP( \theta_t > v_t|v_t= \sfv_k)$. Thus, $\sum_{i=1}^k p_i\overleftarrow{w}_i \ell_i$ is the total expected positive contribution to ROSC which can be used to win slots where $\theta_t > v_t$.

%Using these definitions we can restate the problem \eqref{eq:prob} as an optimization over $\gamma_k$'s.
%\begin{align}\label{eq:optprobfulldist}
%\max_{\gamma_k, 0\le \gamma_k \le 1, k=1, \dots, K} & \quad \sum_{k=1}^K p_k\overleftarrow{w}_k  \sfv_k +  \sum_{k=1}^K  p_k\overrightarrow{w}_k \sfv_k q_{k,\gamma_k} \\ \nn
%\text{subj. to} & \quad  \sum_{k=1}^K  p_k\overrightarrow{w}_k  r_{k,\gamma_k} \le \sum_{k=1}^K p_k\overleftarrow{w}_k \ell_k, \ \forall \ k.
%\end{align}

%Even though \eqref{eq:optprobfulldist} is an exact reformulation of \eqref{eq:prob}, it is unwieldy to solve since it involves distribution functions $r_{k,\gamma_k},q_{k,\gamma_k}$ and there is no obvious structure to the optimal solution. 
%Note that the ROSC is only via the expectation thus, 
%an alternate way to reformulate \eqref{eq:prob} instead of \eqref{eq:optprobfulldist} is as follows.



Since $\sfv_1\le 1$ and $\theta_t \le 1$ for all $t$, let an algorithm bid $b_t = 1$ when $v_t=\sfv_k$ with probability $q_k$. 
Doing so, if $\theta_t \le v_t$, then bidding $b_t = 1$ is same as bidding $b_t = v_t$. In case $\theta_t > v_t$, then by bidding $b_t=1$, 
the expected per-slot negative contribution to the ROSC is simply $r_k$. % = \bbE\{ \theta_t-\sfv_k |v=\sfv_k,  \theta_t> \sfv_k\}$.
  Then problem \eqref{eq:prob} can be recast as the following LP, 
\begin{align}\nonumber
\max_{  \  0\le q_k \le 1} & \quad \sum_{k=1}^K p_k\overleftarrow{w}_k \sfv_k + \sum_{k=1}^K p_k\overrightarrow{w}_k \sfv_k  q_k \\ \label{eq:optprob}
\text{subj. to} & \quad  \sum_{k=1}^K p_k\overrightarrow{w}_k r_k q_k + \sum_{k=1}^K p_k \overleftarrow{w}_k \ell_k \ge 0, \ \forall \ k,
%&\quad  .
\end{align}
where the constraint in \eqref{eq:optprob} exactly corresponds to the ROSC. 
Let $\cP_{\text{light}} =\{K, p_k, \overleftarrow{w}_k, \overrightarrow{w}_k, r_k, \ell_k\}$.
 Assuming the knowledge of $\cP_{\text{light}}$, \eqref{eq:optprob} is similar to the fractional knapsack problem \cite{vaze2023online} and the optimal solution has the following structure. 

\begin{lemma}\label{lem:order}
For \eqref{eq:optprob},  $\exists$ a threshold $\sigma^\star$, such that for indices $k$ such that for $\frac{ \sfv_k}{r_k} > \sigma^\star$, $q_k^\star = 1$, and for $\frac{ \sfv_k}{r_k} < \sigma^\star$, $q_k^\star = 0$. The only fractional solution $q_k^\star$ is for index $k$ such that $\frac{ \sfv_k}{r_k}=\sigma^\star$.
\end{lemma}
%Proof is standard and can be found in textbook \cite{vaze2023online}.
 Assuming the knowledge of $\cP_{\text{light}}$
Lemma \ref{lem:order} implies an optimal algorithm to find the exact solution of \eqref{eq:optprob}. Solve \eqref{eq:optprob} to find $\sigma^\star$. 
Bid $b_t=1$ with probability $1$ for as many $k$'s in order for which $\frac{\sfv_k}{r_k} > \sigma^\star$, bid $b_t=1$ with probability $q<1$ (so as to make the constraint tight in \eqref{eq:optprob}) for index $k$ for which
 $\frac{\sfv_k}{r_k} = \sigma^\star$. For all other $k$'s,  bid $b_t=\sfv_k$, i.e. win such a slot only if $\theta_t \le \sfv_k$ and do not spend any positive accrued contribution to the ROSC-S for winning them.
%index items in decreasing value of $\frac{\sfv_k}{r_k}$, and bid $b_t=1$ with probability $1$ for as many $k$'s in order for which $\frac{\sfv_k}{r_k} > \sigma^\star$ that satisfies the constraint \eqref{eq:optprob}, and for at most one item with 
%$\frac{\sfv_k}{r_k} = \sigma^\star$ $b_t=1$ with probability $q<1$ so as to make the constraint tight in \eqref{eq:optprob}.  For all other $k$'s, bid $b_t=\sfv_k$, i.e. win it only if $\theta_t \le \sfv_k$ and do not spend any positive accrued margin in \eqref{eq:prob} for winning them.


%In light of Lemma \ref{lem:order} an obvious algorithm to solve \eqref{eq:prob} is to first learn all parameters of $\cP_{\text{light}}$ and then arrange items decreasing value of $\frac{\sfv_k}{r_k}$. and bid $b_t=1$ with probability $1$ for as many $k$'s in order that satisfies the constraint \eqref{eq:optprob}, and for at most one item $b_t=1$ with probability $q<1$ so as to make the constraint tight in \eqref{eq:optprob}.  
The quantities involved in \eqref{eq:optprob},
$\cP_{\text{light}} =\{K, p_k, \overleftarrow{w}_k, \overrightarrow{w}_k, r_k, \ell_k\}$, except $K$, are all just expected values and can be easily estimated.
Since $\cP_{\text{light}}$ is unknown, algorithm \textsf{Learn-Alg} (with pseudo-code in Algorithm \ref{alg:light}) first learns the parameters of $\cP_{\text{light}}$ by dedicating the first half of the time horizon $T$, and then applies the solution implied by Lemma \ref{lem:order}.   
%Not only Algorithm \textsf{Learn-Alg} is lighter with respect to Algorithm Learn-Alg in terms of its learning requirements, it also gives a simple structural answer to the optimal allocation. 

%Given that we are interested in lower bounding the competitive ratio of any algorithm, we can devote a constant fraction of the total 
%time horizon $T$ for learning and then employ the solution of  \eqref{eq:optprobfulldist} for the remaining constant fraction to get a constant competitive ratio.  Thus, our first algorithm (\textsf{Learn-Alg}) is as follows.
%\vspace{-0.1in}
\begin{algorithm}
\caption{\textsf{Learn-Alg}}\label{alg:light}
\begin{algorithmic}[1]
%\Procedure{Euclid}{$a,b$}\Comment{The g.c.d. of a and b}
\State Dedicate slots $t=1, \dots, T/2$ for learning parameters of $\cP_{\text{light}}$ by bidding $b_t=v_t$ and using sample mean estimator ${\hat X}=\frac{1}{n}\sum_{i=1}^n X_i$ for each quantity, ${\hat K} = \#$ distinct values of $v_t$'s observed in time period $[1,T/2]$.
\State Arrange all items in decreasing order of $\frac{ \sfv_k}{r_k} $ for $k=1,\dots, {\hat K}$, and  find $\sigma^\star$ following Lemma \ref{lem:order} with $K={\hat K}$
\State For any slot $t\ge T/2$
\State $\ \ $ If $\frac{ \sfv_k}{r_k} > \sigma^\star \quad\quad\  $ Bid $b_t = 1$ 
%\State 
\State $\quad$Else If $\frac{ \sfv_k}{r_k} = \sigma^\star$ Bid $b_t = 1$ with probability $q_k$:  $q_k$ makes the equality in \eqref{eq:optprob} tight
\State $\quad$Else If $\frac{ \sfv_k}{r_k} < \sigma^\star \quad$ Bid $b_t = v_t$ 
\State End
\end{algorithmic}
\end{algorithm}
\vspace{-0.2in}
\begin{rem}
As defined in Algorithm \ref{alg:light}, ${\hat K}$ is the estimated number of distinct values 
that $v_t$ takes.
Since $p_1, \dots, p_K$ do not depend on $T$, the probability that ${\hat K}\ne K$ is $o(1/T)$. %Thus, we can proceed next assuming ${\hat K}\ne K$ without affecting our results.
\end{rem}

\begin{theorem}\label{proof:learn1}
The competitive ratio (Definition \ref{defn:compratio})
  $\mu_{\textsf{Learn-Alg}} \ge 1/2-o(K/T)$ and \textsf{Learn-Alg} satisfies the ROSC with high probability. 
\end{theorem}
\vspace{-0.2in}


%Assuming the knowledge of all the quantities used in \eqref{eq:optprobfulldist}, $\cP =\{ \sfv_k, p_k, \overleftarrow{w}_k, \overrightarrow{w}_k, r_{k,\gamma_k},q_{k,\gamma_k}, \ell_k\}$, one can solve \eqref{eq:optprobfulldist} exactly. Thus, an obvious algorithm to solve \eqref{eq:prob} is to first learn all parameters of $\cP$ and then solve \eqref{eq:optprobfulldist}. Given that we are interested in lower bounding the competitive ratio of any algorithm, we can devote a constant fraction of the total 
%time horizon $T$ for learning and then employ the solution of  \eqref{eq:optprobfulldist} for the remaining constant fraction to get a constant competitive ratio.  Thus, our first algorithm (\textsf{Learn-Alg}) is as follows.
%
%\begin{algorithm}
%\caption{\textsf{Learn-Alg}}\label{alg:learning1}
%\begin{algorithmic}[1]
%%\Procedure{Euclid}{$a,b$}\Comment{The g.c.d. of a and b}
%\State Dedicate $t=1, \dots, T/2$ for learning parameters of $\cP$ by bidding $b_t=v_t$
%\State Solve \eqref{eq:optprobfulldist} to get optimal $\gamma_k$ for $k=1,\dots, K$.
%\State For any slot $t\ge T/2$
%\State Bid $b_t = \sfv_k + \gamma_k$ if $v_t=\sfv_k$.
%\end{algorithmic}
%\end{algorithm}
%


%\newpage
%\begin{comment}
%Even though \textsf{Learn-Alg} achieves a half-fraction of the optimal accrued valuation, it is heavy on learning since it needs to learn all the parameters of $\cP_{\text{light}}$. An obvious question is: can algorithms that do not need to learn anything other than $\sfv_k, p_k$ perform reasonably well. We answer that next in the negative and expose their inherent limitations.
%%show next, that 
%% However, note that the ROSC is only via the expectation thus, an alternate way to reformulate \eqref{eq:prob} instead of \eqref{eq:optprobfulldist} is as follows.
%%
%%
%%
%%Since $\sfv_1\le 1$ and $\theta_t \le 1$ for all $t$, let an algorithm bid $b_t = 1$ when $v_t=\sfv_k$ with probability $q_k$. 
%%Doing so, if $\theta_t \le v_t$, then there is no change compared to bidding $b_t = v_t$. In case $\theta_t > v_t$, then by bidding $b_t=1$, 
%%the `negative' contribution to the ROSC is simply $r_k = \bbE\{ \theta_t-\sfv_k |v=\sfv_k,  \theta_t> \sfv_k\}$.
%%Then the problem \eqref{eq:prob} can be recast as 
%%\begin{equation}\label{eq:optprob}
%%\begin{align}
%%\max_{q_k,  \  0\le q_k \le 1, k=1, \dots, K} & \quad \sum_{k=1}^K p_k\overleftarrow{w}_k \sfv_k + \sum_{k=1}^K p_k\overrightarrow{w}_k \sfv_k  q_k \\
%%\text{subj. to} & \quad  \sum_{k=1}^K p_k\overrightarrow{w}_k r_k q_k \le \sum_{k=1}^K p_k \overleftarrow{w}_k \ell_k, \ \forall \ k=1,\dots, K.
%%%&\quad  .
%%\end{align}
%%\end{equation}
%%In comparison to $\cP$ \eqref{eq:optprobfulldist}, the quantities involved in \eqref{eq:optprob},
%%$\cP_{\text{light}} =\{ \sfv_k, p_k, \overleftarrow{w}_k, \overrightarrow{w}_k, r_k, \ell_k\}$, are easier to obtain since all of them are just expectations and not distribution 
%%functions as in $\cP$. Assuming the knowledge of all the quantities of $\cP_{\text{light}}$, one can solve \eqref{eq:optprob} and the optimal solution has the following 
%%structure. 
%%
%%\begin{lemma}\label{lem:order}
%%For \eqref{eq:optprob},  there exists a threshold $\sigma^\star$, such that for indices $k$ such that for $\frac{ \sfv_k}{r_k} > \sigma^\star$, $q_k^\star = 1$, and for $\frac{ \sfv_k}{r_k} < \sigma^\star$, $q_k^\star = 0$. The only fractional solution $q_k^\star$ is for index $k$ such that $\frac{ \sfv_k}{r_k}=\sigma^\star$.
%%\end{lemma}
%%\begin{proof}
%%Without loss of generality, let all $\frac{ \sfv_k}{r_k}$ be distinct.
%%  Let the statement be false, and in particular in the optimal solution $\bq$ there exists a $k'$ (in order) for which $q_{k'} < 1$  but $q_{k'+1} > 0$. We will contradict to the optimality of $\bq$ by creating a new solution with larger objective function while satisfying the constraint. Consider a new solution ${\hat \bq}$ where we keep all 
%%  $q_k$'s the same other than $q_{k'}$ and $q_{k'+1}$ which are changed to ${\hat q}_{k'} = q_{k'}+q_{k'+1} \frac{r_{k'+1}}{r_{k'}}$ and ${\hat q}_{k'+1}=0$. Clearly, ${\hat \bq}$ satisfies the constraint. However, the change in objective function value (going from $\bq$ to $\hat \bq$) is 
%%  $$q_{k'+1}r_{k'+1} \left( \frac{\sfv_{k'}}{r_{k'}} - \frac{\sfv_{k'+1}}{r_{k'+1}}\right) >0,$$ since $\frac{\sfv_{k'}}{r_{k'}} > \frac{\sfv_{k'+1}}{r_{k'+1}}$ by definition.
%%  Thus, $\bq$ cannot be optimal.
%%\end{proof}
%%Lemma \ref{} implies an optimal algorithm to find the exact solution of \eqref{eq:optprob}, which is to index items in decreasing value of $\frac{\sfv_k}{r_k}$, and bid $b_t=1$ with probability $1$ for as many $k$'s in order that satisfies the constraint \eqref{eq:optprob}, and for at most one item $b_t=1$ with probability $q<1$ so as to make the constraint tight in \eqref{eq:optprob}.  For all other $k$'s, bid $b_t=\sfv_k$, i.e. win it only if $\theta_t \le \sfv_k$ and do not spend any positive margin on winning the slots where $\theta_t>\sfv_k$.
%%
%%
%%%In light of Lemma \ref{lem:order} an obvious algorithm to solve \eqref{eq:prob} is to first learn all parameters of $\cP_{\text{light}}$ and then arrange items decreasing value of $\frac{\sfv_k}{r_k}$. and bid $b_t=1$ with probability $1$ for as many $k$'s in order that satisfies the constraint \eqref{eq:optprob}, and for at most one item $b_t=1$ with probability $q<1$ so as to make the constraint tight in \eqref{eq:optprob}.  
%%This is summarized in Algorithm \textsf{Learn-Alg}. Not only Algorithm \textsf{Learn-Alg} is lighter with respect to Algorithm Learn-Alg in terms of its learning requirements, it also gives a simple structural answer to the optimal allocation. 
%
%%Given that we are interested in lower bounding the competitive ratio of any algorithm, we can devote a constant fraction of the total 
%%time horizon $T$ for learning and then employ the solution of  \eqref{eq:optprobfulldist} for the remaining constant fraction to get a constant competitive ratio.  Thus, our first algorithm (\textsf{Learn-Alg}) is as follows.
%
%%\begin{algorithm}
%%\caption{\textsf{Learn-Alg}}\label{alg:learning2}
%%\begin{algorithmic}[1]
%%%\Procedure{Euclid}{$a,b$}\Comment{The g.c.d. of a and b}
%%\State Dedicate $t=1, \dots, T/2$ for learning parameters of $\cP_{\text{light}}$ by bidding $b_t=v_t$
%%\State Arrange all items in decreasing order of $\frac{ \sfv_k}{r_k} $, and  find $\sigma^\star$ following Lemma \ref{lem:order}
%%\State For any slot $t\ge T/2$
%%\State If $\frac{ \sfv_k}{r_k} > \sigma^\star$
%%\State \quad Bid $b_t = 1$ 
%%\State Else If $\frac{ \sfv_k}{r_k} = \sigma^\star$
%%\State \quad Bid $b_t = 1$ with probability $q_k$ where $q_k$ makes the equality in \eqref{eq:optprob} tight
%%\State Else If $\frac{ \sfv_k}{r_k} < \sigma^\star$
%%\State \quad Bid $b_t = v_t$ 
%%\State End
%%\end{algorithmic}
%%\end{algorithm}
%%
%%\begin{theorem}\label{proof:learn2}
%%  The competitive ratio of \textsf{Learn-Alg} is at least $1/2-O(K/T)$. 
%%\end{theorem}
%%\begin{proof}
%%Learning the parameters of $\cP_{\text{light}}$ is easier than learning the parameters of $\cP$. Thus, the 
%%rest of the proof is identical to the Proof of Lemma \ref{proof:learn1}.
%%%same as learning expected valued from i.i.d. samples of a distribution with finite variance . Hence, from Chernoff bound, we have 
%%%$$\bbP\left(|\frac{1}{n}\sum_{i=1}^n X_i - \mu| > \epsilon\right) \le \frac{\sigma^2}{n \epsilon^2},$$ where $\sigma^2$ is the variance. 
%%%Taking a union bound over $K$ such quantities, we get te
%%\end{proof}
%%
%
%
%
%\subsection{Cheaper Algorithms}
%%Towards that end, we next propose an algorithm that remains always feasible and bids 
%%according to the relative value revealed compared to other known values of $v_t$.
%Let an 
%algorithm only know $\sfv_k$,  $p_k$, and $\overleftarrow{w}_k$ for $k=1,\dots, K$, by possibly learning them in first $T/2$ slots.
%Let $\mu = \sum_{k=1}^K p_k\overleftarrow{w}_k v_k$.
%%Let all these values be known (possibly by observing them till time $T/2$).
%%Let $\text{Margin}(t) = \sum_{t=1}^T v_t x_t(b_t) - \sum_{t=1}^Tp_t(b_t)$ be the current margin with respect to constraint in  \eqref{eq:prob}. Essentially, this means that in the text time slot if bid $b_t \le v_t+\text{Margin}(t)$ it will not violate the constraint in  \eqref{eq:prob}.
%Given that no information about $r_k$ is learnt, an obvious choice of an algorithm for solving \eqref{eq:optprob} is to bid $b_t$ in proportion to the revealed value $v_t=\sfv_k$ with respect to other values of $\sfv_k's$ while ensuring that the algorithm always satisfies
%the ROSC. 
%{\bf Algorithm $\cA_m$:}
%Learn $\sfv_k$ and $p_k,\overleftarrow{w}_k$ for $k=1,\dots, K$ until slot $T/2$ by bidding $b_t = \sfv_k,$
%for slot $t$ where $v_t=\sfv_k$ and for $t\le T/2$, $\text{Margin}(t) = \text{Margin}(t-1) + (v_t-\theta_t)$ with $\text{Margin}(0)=0$.
%For time slots $t\ge T/2+1$ if $v_t=v_k$, then algorithm $\cA_m$ bids  
%\begin{equation}\label{eq:bidprocessgenAm}
%b_t = v_k + \frac{p_k\overleftarrow{w}_k v_k}{\mu}\text{Margin}(t-1),
%\end{equation}
% where the Margin process updates as in \eqref{defn:marginprocess}.
%%  \begin{equation}\label{defn:marginprocessgenAm}
%%\text{Margin}(t) = \begin{cases} \text{Margin}(t-1) + (v_t-\theta_t) & \text{if} \ b_t \ge \theta_t, 
%%\\ 
%% \text{Margin}(t-1), & \text{otherwise}.
%% \end{cases}
%%\end{equation} Algorithm $\cA_m$ wins slot $t$ whenever $b_t \ge \theta_t$. 
%
%We next show that algorithm $\cA_m$ has a poor competitive ratio that decreases as $1/\sqrt{T}$.
%\begin{lemma}\label{lem:Am}
%  $\mu_{\cA_m} \le O(1/\sqrt{T}).$
%\end{lemma}
%\begin{proof}
%%Let the learning about $\sfv_k$ and $p_k,\overleftarrow{w}_k$ for $k=1,\dots, K$ be complete at slot $T/2$.
%  To prove the lemma, consider the following input. 
%  Let $v$ take only three values $\sfv_1=1/2$ $\sfv_2=1/4$ and $\sfv_3 = 3/8$ with equal probability in any slot. Moreover, when $v_t=\sfv_1=1/2$ then  
%  $\theta_t=\theta_1=1$, when $v_t=\sfv_2=1/4$  then  
%  $\theta_t=\theta_21/4+ 1/\sqrt{T}$ and when $v_t=\sfv_3=3/8$ then 
%  $\theta_t=\theta_3=3/8- 1/\sqrt{T}$. 
%  Let the learning about $\sfv_k$ and $p_k,\overleftarrow{w}_k$ for $k=1,\dots, K$ be complete at time slot $T/2$. For the given input, at the end of slot $T/2$, the 
%  $\text{Margin}(T/2)$ will be $\Theta(\sqrt{T})$. 
%  Since in any slot $t > T/2$, whenever $\sfv_3=3/8$, $\theta_3=3/8- 1/\sqrt{T}$, the total positive Margin across all $T$ slots will be $\Theta(\sqrt{T})$. Thus, without loss of generality, we assume that  for $t > T/2$, either $\sfv_1=1/2$ $\sfv_2=1/4$ with equal probability.
%  
%  For this input, consider an algorithm $\cB$ that bids $b_t= \sfv_2+ 1/\sqrt{T}$, whenever, $v_t=\sfv_2$ and bids $0$ when $v_t=\sfv_1$. Clearly, $\cB$ wins all slots when $v_t=\sfv_2$, and its expected accrued valuation is $\frac{1}{4}\cdot \frac{T}{2}$.
%  
%  In comparison to $\cB$, $\cA_m$ does win some slots when $v_t=\sfv_1$. Because of this,  the expected time slot by which $\cA_m$ exhausts its total margin ($\text{Margin}(T/2)=\Theta(\sqrt{T})$) is $O(\sqrt{T})$ which means it can win at most $\Theta(\sqrt{T})$ slots and hence its expected accrued valuation is $O(\sqrt{T})$. Since $\cB$ is as good as $\opt$,  the competitive ratio of $\cA_m$ is at most $O(1/\sqrt{T})$.
%\end{proof}
%
%It might appear that $\cA_m$ has a bad competitive ratio since it ends up winning some slots with large value of $\theta$ by dedicating a large portion of its margin. A course correction could be to limit the bid by the ratio of the total amount of margin left and the number of slots left, as follows.
%
%{\bf Algorithm $\cA'_m$:} 
%$\cA'_m$ has everything else the same as $\cA_m$ except \eqref{eq:bidprocessgenAm}, which is replaced by  
%\begin{equation}\label{eq:bidprocessgenAmprime}
%b_t = v_k + \min\left\{\frac{p_k v_k}{\mu}\text{Margin}(t-1), \frac{\text{Margin}(t-1)}{T-t}\right\}.
%\end{equation}
%Algorithm $\cA'_m$ wins slot $t$ whenever $b_t \ge \theta_t$.
%We next show that $\cA'_m$ has even poorer competitive ratio performance than $\cA_m$. 
%\begin{lemma}\label{}
%  $\mu_{\cA'_m} \le O(1/T).$
%\end{lemma}
%\begin{proof}
%  Compared to Lemma \ref{lem:Am}, the only change in the input is  that $\sfv_1=1/2$ and $\sfv_2=1/T^2$ with probability $c/T$ and $1-c/T$, respectively for some constant $c$. Moreover, when $\sfv_1=1/2$ then $\theta_1=1$ and $\sfv_2=1/T^2$ then 
%  $\theta_2=1/T^2+ 1/\sqrt{T}$. Rest of the input is the same as considered in Lemma \ref{lem:Am}
%  Consider an algorithm $\cB$ that bids $b_t= 1$, whenever, $v_t=\sfv_1$ and bids $0$ otherwise. Clearly, $\cB$ can win 
%  $\Omega(\sqrt{T})$ slots  when $v_t=\sfv_1$. But since $v_t=\sfv_1$ with probability $c/T$, $\cB$'s expected accrued valuation is $T \cdot \frac{c}{T}. 1/2 =c/2$.
%  
%  In comparison to $\cB$, $\cA'_m$ wins far less  slots when $v_t=\sfv_1$. In particular, initially because of $b_t = v_t + \min\{., \frac{\text{Margin}(t-1)}{T-t}\}$, 
%   $\cA'_m$ wins slots when $v_t=\sfv_2$. $\cA'_m$ can win a slot when $v_t=\sfv_1$ only if $\frac{\text{Margin}(t-1)}{T-t}>1/2$. But since $\text{Margin}(T/2)=\Theta(\sqrt{T})$, and all slots for which $v_t=\sfv_2$ (which happens with probability $1-1/T$) are being won by $\cA'_m$, $\frac{\text{Margin}(t-1)}{T-t}>1/2$ is never true. Thus, 
%   $\cA'_m$ can possibly win all slots when  $v_t=\sfv_2$ and its accrued valuation can be at most $1/T^2 \cdot T = \frac{1}{T}$.
%   
%
%   
%    Since $\cB$ is as good as $\opt$,  the competitive ratio of $\cA'_m$ is at most $O(1/T)$.
%
%\end{proof}
%\end{comment}
%
%\begin{comment}
%
%Let the left margin mean for $\sfv_k$ be 
%$\ell_k = \bbE\{\sfv_k - \theta_t|v=\sfv_k,  \theta_t\le \sfv_k\}$ and the total left margin be $L = \sum_{k=1}^K p_k \ell_k$.
%Clearly, $L \le \sum_{k=1}^K p_k v_k$. 
%
%To ensure that the ROSC is satisfied
%$L$ is essentially the total allowance available to win slots when $\theta_t > v_t$. Thus, it readily follows that 
%\begin{equation}\label{eq:ubgen}
%\sum_{k=1}^K v_k (1-p_k) + p_1 v_1 \cdot L
%\end{equation} is an upper bound on the total value accrued by any algorithm that satisfies the ROSC.
%
%
%
%Depending on the values of $L, \mu, p_k, v_k$,  it is easy to see that either 
%$\text{Margin}(t)$ converges to a positive number or diverges to infinity. In the latter case, Algorithm $\cA_m$ wins all slots and there is nothing to prove. 
%So let $\text{Margin}(t)$ converge to $m^\star > 0$. 
%
%Taking an averaged out approach, let $b_t  = v_k + \frac{p_k v_k}{\mu} m^\star$ whenever $v_t=v_k$. 
%Let $\eta_k = \bbE\{\theta_k-v_k| \theta_k \le  v_k + \frac{p_k v_k}{\mu} m^\star\}$.
%Using this, we get the following relation
%\begin{equation}\label{eq:balance}
%\sum_{k=1}^K \eta_k p_k = L,
%\end{equation}
%since otherwise $\text{Margin}(t)$ would not have converged. 
%If the distributions of $\theta_k$ were known, then from \eqref{eq:balance} we can find $m^\star$.
%
%Note that since $\theta_t \in [0,1]$, we get that $\eta_k \le \bbP\left(\theta_k \le v_k + \frac{p_k v_k}{\mu} m^\star\right)$.
%Therefore \eqref{eq:balance} implies 
%\begin{equation}\label{eq:mostcritical}
%\sum_{k=1}^K p_k \bbP\left(\theta_k \le v_k + \frac{p_k v_k}{\mu} m^\star\right) \ge L
%\end{equation}
%The corresponding payoff obtained by $\cA_m$ is just 
%\begin{equation}\label{eq:lbgen}
%\sum_{k=1}^K v_k (1-p_k) + \sum_{k=1}^K v_k p_k \bbP\left(\theta_k \le v_k + \frac{p_k v_k}{\mu} m^\star\right).
%\end{equation}
%
%
%To be done: Show that \eqref{eq:lbgen} is at least a constant fraction of \eqref{eq:ubgen}.
%
%
%
%
%
%
%
%
%\section{$\opt$ Solution}
%Let $v_t$ take $K$ possible values $\sfv_1 < \dots <\sfv_K$ with probabilities $p_1, \dots, p_K$. 
%Let the left margin mean and right margin mean for $\sfv_k$ be 
%$\ell_k = \bbE\{\sfv_k - \theta_t|v=\sfv_k,  \theta_t\le \sfv_k\}$ and $r_k = \bbE\{ \theta_t-\sfv_k |v=\sfv_k,  \theta_t> \sfv_k\}$. Let $\overleftarrow{w}_k = \bbP( \theta_t\le \sfv_k)$ and $\overrightarrow{w}_k = \bbP( \theta_t > \sfv_k)$.
%Let $q_k$ be the probability with which $\opt$ bids $b_t = \theta_t$ when $v_t=\sfv_k$ and $\theta_t > \sfv_k$. Then the problem \eqref{eq:prob} can be recast as 
%\begin{equation}\label{eq:optprob}
%\begin{align}
%\max_{q_k, 0\le q_k \le 1, k=1, \dots, K} & \quad \sum_{k=1}^K (\overleftarrow{w}_k+ \overrightarrow{w}_kq_k) p_k \sfv_k \\
%\text{subj. to} & \quad  \sum_{i=1}^k q_ip_i\overrightarrow{w}_i r_i \le \sum_{i=1}^k p_i\overleftarrow{w}_i \ell_i, \ \forall \ k=1,\dots, K.
%%&\quad  .
%\end{align}
%\end{equation}
%Since $\overleftarrow{w}_kp_k \sfv_k$ is a constant,  problem \eqref{eq:optprob} is equal to 
%\begin{equation}\label{eq:optprob1}
%\begin{align}
%\max_{q_k, 0\le q_k \le 1, k=1, \dots, K} & \quad \sum_{k=1}^K  \overrightarrow{w}_kq_k p_k \sfv_k \\
%\text{subj. to} & \quad  \sum_{i=1}^k q_ip_i\overrightarrow{w}_i r_i \le \sum_{i=1}^k p_i\overleftarrow{w}_i \ell_i, \ \forall \ k=1,\dots, K.
%%&\quad  .
%\end{align}
%\end{equation}
%Compared to Problem \eqref{eq:optprob1}, where the maximization if over $q_k$ that can be different for $k=1,\dots, K$, consider 
%\begin{equation}\label{eq:optprob2}
%\begin{align}
%\max_{q, 0\le q \le 1} & \quad q \sum_{k=1}^K  \overrightarrow{w}_k p_k \sfv_k \\
%\text{subj. to} & \quad  q\sum_{i=1}^K p_i\overrightarrow{w}_i r_i \le \sum_{i=1}^K p_i\overleftarrow{w}_i \ell_i .%&\quad  .
%\end{align}
%\end{equation}
%where for each $k$, bid $b_t = \theta_t$ with the same probability $q$. Clearly, the optimal value of \eqref{eq:optprob2} is less than that of the optimal value of \eqref{eq:optprob1}, however, as we show next, it is at least $1/2$ of the value of the optimal value of \eqref{eq:optprob1}.
%
%\begin{lemma}\label{}
%The value achieved by \eqref{eq:optprob1} is at least $1/2$ of the optimal value of \eqref{eq:optprob1}
%\end{lemma}
%\begin{proof}
%Clearly, for any $k$, the optimal value of $q_k^\star$ in \eqref{eq:optprob1} is at most $\min\left\{1, \frac{ \sum_{i=1}^K p_i\overleftarrow{w}_i \ell_i}{p_k\overrightarrow{w}_k r_k}\right\}$.
%
%Thus, the optimal valuation for the $k^{th}$ term in \eqref{eq:optprob1} is at most 
%\begin{equation}\label{eq:opt1val}
%\sfv_k \overrightarrow{w}_k p_k \min\left\{1, \frac{ \sum_{i=1}^K p_i\overleftarrow{w}_i \ell_i}{p_k\overrightarrow{w}_k r_k}\right\}.
%\end{equation}
%
%
%In comparison the $q^\star$ for \eqref{eq:optprob2} is simply 
%$q^\star = \min\left\{1, \frac{\sum_{i=1}^K p_i\overleftarrow{w}_i\ell_i}{\sum_{i=1}^K p_i\overrightarrow{w}_i r_i}\right\}$, and 
%the corresponding optimal valuation for the $k^{th}$ term in \eqref{eq:optprob2} is 
%\begin{equation}\label{eq:opt2val}
%\sfv_k  \overrightarrow{w}_k p_k  \min\left\{1, \frac{\sum_{i=1}^K p_i\overleftarrow{w}_i\ell_i}{\sum_{i=1}^K p_i\overrightarrow{w}_i r_i}\right\}.
%\end{equation}
%\end{proof}
%
%
%
%Letting $\bq = [q_1 \dots q_k]$, $c_k = \overrightarrow{w}_kq_kp_k$, $b_k= \sum_{i=1}^k p_i\overleftarrow{w}_i \ell_i$ and 
%\begin{equation}\label{}
%\bA = \left[\begin{array}{cccccc}
%p_1\overrightarrow{w}_1 r_1 & p_2\overrightarrow{w}_2 r_2 & p_3\overrightarrow{w}_3 r_3& \dots& p_{K-1}\overrightarrow{w}_{K-1} r_{K-1} & p_K\overrightarrow{w}_K r_K\\
%0 & p_2\overrightarrow{w}_2 r_2 & p_3\overrightarrow{w}_3 r_3& \dots& p_{K-1}\overrightarrow{w}_{K-1} r_{K-1} & p_K\overrightarrow{w}_K r_K\\
%0 & 0 & p_3\overrightarrow{w}_3 r_3& \dots& p_{K-1}\overrightarrow{w}_{K-1} r_{K-1} & p_K\overrightarrow{w}_K r_K\\
%\vdots& \vdots& \ddots& \dots&p_{K-1}\overrightarrow{w}_{K-1} r_{K-1} & p_K\overrightarrow{w}_K r_K\\
%0 & 0 & \dots& 0&0 & p_K\overrightarrow{w}_K r_K
%\end{array}\right]
%\end{equation}
%problem \eqref{eq:optprob} is a linear program (LP) $\max_{\bA \bx \ge \bb} \bc^T \bx$. Since $\cA$ has a special structure, its inverse is given by 
%
%\begin{equation}\label{}
%\bA^{-1} = \left[\begin{array}{cccccc}
%(p_1\overrightarrow{w}_1 r_1)^{-1} & -(p_1\overrightarrow{w}_1 r_1)^{-1} & 0& \dots& 0 & 0\\
%0 & (p_2\overrightarrow{w}_2 r_2)^{-1} & -(p_2\overrightarrow{w}_2 r_2)^{-1}& 0& \dots & 0\\
%%0 & 0 & (p_3\overrightarrow{w}_3 r_3)^{-1}& -(p_3\overrightarrow{w}_3 r_3)^{-1}& 0 & 0\\
%\vdots& \vdots& \ddots& \dots&(p_{K-1}\overrightarrow{w}_{K-1} r_{K-1})^{-1} & -(p_{K-1}\overrightarrow{w}_{K-1} r_{K-1})^{-1}\\
%0 & 0 & \dots& 0&0 & (p_K\overrightarrow{w}_K r_K)^{-1}
%\end{array}\right]
%\end{equation}
% \end{comment}