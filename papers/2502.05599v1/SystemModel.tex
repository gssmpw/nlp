%\section{Notation}
%For any process, its {\bf drift} is defined as the difference between its expected increase and expected decrease in any time slot.
\section{Problem Formulation}
We consider the following online learning model, where time is discrete and in time slot $t$: an ad query is generated that has three attributes, its value $v_t \in [0,1]$,  the allocation function $x_t: \bbR^+ \cup \{0\} \rightarrow [0,1]$, and the payment function $q_t: \bbR^+ \cup \{0\}\rightarrow [0,1]$. 
The allocation function $x_t(b_t)$ is effectively the probability of `winning' the slot $t$ by bidding $b_t$, and the expected valuation accrued from slot $t$ is $v_t x_t(b_t)$. The allocation function $x_t$ is required to be a non-decreasing function of the bid $b_t$ with $x_t(0) =0$. The payment function $q_t$   is also non-decreasing, is zero when the allocation is zero, and is at most the bid. Thus, the expected payment function is $p_t(b_t)= q_t(b_t)x_t(b_t)$ which satisfies
 \begin{equation}\label{eq:paymentcondition}
p_t(v_t) \le v_t x_t(v_t).
\end{equation}


Instead of working with $q_t$, we work with $p_t$ from here onwards.
%4\vspace{-0.02in}
%Because of constraint \eqref{eq:myersontruthfulcond}, the input at slot $t$ is effectively only a two-tuple $(v_t,x_t)$ where $v_t$ ($x_t$) is revealed before (after) the bid $b_t$ is made. 
The value $v_t$ is visible to the online learner before making a decision at time slot $t$ about the bid $b_t$ it wants to make. Once $b_t$ is chosen, both $x_t(b_t)$ and $p_t(b_t)$ are revealed. 
We consider the stochastic model, where the input $(v_t,x_t,p_t)$ is generated i.i.d. from an unknown distribution $\cD$. 
%It is easy to construct input examples, where $(v_t,x_t,p_t)$ if not i.i.d., then the performance of any online learning policy is arbitrarily bad.



The problem for the online learner is to choose $b_t$ in slot $t$ knowing the past information $$\{(v_\tau, x_\tau(b_\tau), p_\tau(b_\tau))_{\tau=1, \dots, t-1}\}$$ and the present value $ v_t$ so as to maximize the total expected accrued valuation subject to ROSC, {\it that requires that the 
total expected payment should be at most the total accrued valuation}. Formally, the problem is 
%\vspace{-0.2in}
\begin{align}\nn
\max_{b(t)} & \quad V(T)=  \bbE\left\{\ \sum_{t=1}^T v_t x_t(b_t)\right\},\\\label{eq:prob}
\text{subj. to} &  \quad \bbE\left\{\sum_{t=1}^Tp_t(b_t)\right\} \le   \bbE\left\{\ \sum_{t=1}^T v_t x_t(b_t)\right\}.
\end{align}
where constraint in \eqref{eq:prob} is the ROSC.
To characterize the performance of any online learning algorithm $\cA$, we define its  regret as 
\begin{equation}\label{defn:regret}
\cR_\cA(T) = \max_{\cD} \left(V_{\opt}(T) - V_{\cA}(T)\right) %\bbE\left\{\ \sum_{t=1}^T v_t x_t(b^\opt_t)\right\} - \bbE\left\{\ \sum_{t=1}^T v_t x_t(b^\cA_t)\right\}
\end{equation}
where $\cD$ is the distribution for $(v_t,x_t,p_t)$, and  $\opt$ is the optimal offline algorithm that knows all of $v_t, x_t(b), p_t(b), t=1,\dots, T$ and all $b$ at time $1$ itself) and makes its bids $b^\opt_t$ while satisfying the ROSC (which is a constraint enforced in expectation and not on sample path).

We have defined ROSC as a constraint in expectation. With abuse of notation, its {\bf sample path} counterpart is defined as {\bf ROSC-S}.
\begin{definition}\label{defn:ccv}
Let $\text{CV}(t ) = p_t(b_t) - v_t x_t(b_t)$ be the contribution to ROSC-S  at slot $t$.
Let $\text{CCV}(\tau) = \sum_{t=1}^\tau \text{CV}(t)$ denote the cumulative contribution to ROSC-S till slot $\tau$. 
Let $\text{Margin}(\tau) = \max\{0, - \text{CCV}(\tau)\}$. For ROSC to be satisfied, $\bbE\{\text{CCV}(T)\}\le 0$.
%, the allowance available to bid without violating the ROSC at time $\tau+1$.
\end{definition}
