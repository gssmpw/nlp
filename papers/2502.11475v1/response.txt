\section{Related Work}
% \subsection{Preference Optimization for Code Generation}
Large language models (LLMs) have made significant progress in generating code from natural language descriptions, showing great potential for automating software development tasks. Models**Vaswani et al., "Attention Is All You Need"**, **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"** have demonstrated strong performance, thanks to extensive training on diverse datasets. To further enhance their capabilities, posting training methods like Supervised Fine-Tuning (SFT) **Joulin et al., "FastText at Scale"** and Direct Preference Optimization **Li et al., "Direct Preference Optimization for Code Generation"** are commonly applied. Preference optimization approaches focus on aligning model outputs with desired outcomes by prioritizing more favorable responses over less favorable ones. 
% Direct Preference Optimization (DPO) is one such method that has shown effectiveness in domains like mathematics, where it improves model performance through preference-based distinctions between outputs. DPO **Li et al., "Direct Preference Optimization for Code Generation"** have achieves stable improvement in the code generation tasks .
However, existing DPO approaches fail to address one important issue: they do not directly target the most error-prone points in generated code. Errors in these high-impact parts can lead to significant quality and reliability issues in the final output. 
We aim to address this issue by focusing the preference optimization learning on these error-prone points in the generated code. 
% \lijia{Please consider citing our aiXcoder-7B paper, haha.}

% \subsection{Fine-grained Preference Optimization}

Some fine-grained preference optimization methods **Li et al., "Step-DPO: Step-Wise Preference Optimization for Code Generation"** have shown strong potential in domains like mathematics, which rely heavily on natural language reasoning. Step-DPO **Li et al., "Step-DPO: Step-Wise Preference Optimization for Code Generation"** and Step-Controlled DPO **Jain et al., "Step-Controlled Direct Preference Optimization for Code Generation"** propose generating step-wise preference datasets to enable optimization learning based on the standard DPO loss. TDPO **Kim et al., "Token-Level Fine-Grained Preference Optimization via Forward KL Divergence"** enhances the DPO loss by incorporating forward KL divergence constraints at the token level, achieving fine-grained alignment for each token. cDPO **Chen et al., "Critical Token-based Direct Preference Optimization for Code Generation"** proposes a tricky method to find the critical token in the thought chain that affects overall accuracy. However, the identified tokens are typical in natural language and the method does not apply to code, which features similar overall patterns but relies on specific key elements in long reasoning processes.
However, in the context of code generation, where a small error-prone point can lead to major functional errors, these exisiting methods often struggle to construct adequate datasets or fail to achieve ideal improvements due to weak fine-grained reward signals.
To address this, we propose Focused-DPO, a framework that improves code generation by focusing on optimizing these high-impact parts. 
Our dataset construction method employs a self-generation and validation process to construct datasets that explicitly identify error-prone points, ensuring the optimization learning process directly enhances the parts of the code that matter most for overall correctness.