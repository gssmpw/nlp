\section{Related Work}
% \subsection{Preference Optimization for Code Generation}
Large language models (LLMs) have made significant progress in generating code from natural language descriptions, showing great potential for automating software development tasks. Models\citep{GPT-4,li2023starcoder,qwencoder, guo2024deepseek, aixcoder} have demonstrated strong performance, thanks to extensive training on diverse datasets. To further enhance their capabilities, posting training methods like Supervised Fine-Tuning (SFT) \cite{luo2023wizardcoder, wei2023magicoder} and Direct Preference Optimization \cite{qwencoder, codedpo, stepcoder, codeoptimise, plum} are commonly applied. Preference optimization approaches focus on aligning model outputs with desired outcomes by prioritizing more favorable responses over less favorable ones. 
% Direct Preference Optimization (DPO) is one such method that has shown effectiveness in domains like mathematics, where it improves model performance through preference-based distinctions between outputs. DPO have achieves stable improvement in the code generation tasks .
However, existing DPO approaches fail to address one important issue: they do not directly target the most error-prone points in generated code. Errors in these high-impact parts can lead to significant quality and reliability issues in the final output. 
We aim to address this issue by focusing the preference optimization learning on these error-prone points in the generated code. 
% \lijia{Please consider citing our aiXcoder-7B paper, haha.}

% \subsection{Fine-grained Preference Optimization}

Some fine-grained preference optimization methods \cite{rafailov2024direct, lai2024step, stepctrldpo,tdpo, cdpo} have shown strong potential in domains like mathematics, which rely heavily on natural language reasoning. Step-DPO \cite{lai2024step} and Step-Controlled DPO \cite{stepctrldpo} propose generating step-wise preference datasets to enable optimization learning based on the standard DPO loss. TDPO \cite{tdpo} enhances the DPO loss by incorporating forward KL divergence constraints at the token level, achieving fine-grained alignment for each token. cDPO \cite{cdpo} proposes a tricky method to find the critical token in the thought chain that affects overall accuracy. However, the identified tokens are typical in natural language and the method does not apply to code, which features similar overall patterns but relies on specific key elements in long reasoning processes.
However, in the context of code generation, where a small error-prone point can lead to major functional errors, these exisiting methods often struggle to construct adequate datasets or fail to achieve ideal improvements due to weak fine-grained reward signals.
To address this, we propose Focused-DPO, a framework that improves code generation by focusing on optimizing these high-impact parts. 
Our dataset construction method employs a self-generation and validation process to construct datasets that explicitly identify error-prone points, ensuring the optimization learning process directly enhances the parts of the code that matter most for overall correctness.