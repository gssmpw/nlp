\section*{Limitation}

Despite the contributions of our work, there are several limitations that we aim to address in future research:

\paragraph{Comparison with Advanced RL Techniques}
While our study demonstrates the effectiveness of Focused-DPO, we do not extensively compare it with other advanced reinforcement learning (RL) alignment techniques, such as DeepSeek-R1 \cite{guo2025deepseekr1}. These online RL alignment techniques typically require substantial training resources, high-quality datasets, and complex reward environments, making their application highly resource-intensive.
In contrast, offline alignment methods such as Focused-DPO approximate similar optimization objectives while introducing necessary simplifications and derivations. 
This allows Focused-DPO to achieve comparable or even equivalent optimization results with significantly lower resource requirements. 
Moreover, we leverage prior knowledge discovered in this work: the insight that only a small part of the generated code—specifically, the Error-Prone Points—plays a critical role in determining the overall correctness of the output. By incorporating this insight into the training loss design, we further enhance training efficiency and effectiveness.
Focused-DPO’s low resource requirements and reliable performance make it applicable to a wide range of code generation scenarios. Further exploration of how Focused-DPO compares to these advanced RL techniques in performance and efficiency remains an area for future investigation.


\paragraph{Dataset Construction Strategy}
In Focused-DPO, we introduce a dataset construction technique named \textbf{Error-Prone Identification}, which automatically identifies error-prone points in generated code. 
The primary focus of this paper is on error-prone points associated with correctness in the final output code. However, other factors in source code, such as efficiency, readability, and security, are equally important for optimization.
Exploring whether these factors also reveal "Error-Prone Points" in source code is an intriguing direction for future work. For example, techniques like static code analysis, code smells detection, and identification of common vulnerabilities could help identify and penalize insecure patterns during data construction, leading to safer and more robust code generation.

Additionally, our dataset construction pipeline includes specific design choices, such as the use of a page-rank mechanism and the identification of error-prone points based on common prefixes and suffixes. 
Our preliminary experiments suggest that these settings effectively support the performance of Focused-DPO. Detailed discussions on these designs are provided in Appendix \ref{sec:discussion}.



% \paragraph{Error-Prone Points Identification}

% In our Focused-DPO method, we introduce a dataset construction technique called \textbf{Error Prone Identification} to automatically identify error-prone points in generated code. To assess the correctness of the code, we employ a self-generation-and-validation mechanism based on PageRank, which captures the relative quality of different code snippets \cite{codedpo}. We are not like approaches such as Magicoder \cite{wei2023magicoder}, which directly use all test cases as ground truth. In our experiments we use the policy model to generate datasets. Since the policy model's generation quality is not as robust as that of more powerful models like GPT-4 (used in Magicoder), the PageRank-based method allows us to automatically filter out lower-quality test cases (those with lower scores after iteration), thereby ensuring higher overall dataset quality.

% We find that different models exhibit varying levels of accuracy across different problems. Therefore, for each model's training dataset, we performed necessary filtering by removing code problems with excessively high or low accuracy rates, ensuring a consistent number of code problems in the final dataset. Moreover, we observe that models tend to exhibit similarities in error-prone points when solving the same problems. For example, when comparing the error-prone points identified by \textit{DeepSeekCoder-6.7B-instruct} and \textit{Qwen2.5-Coder-Instruct-7B} models on the same set of programming problems, we found a 32\% overlap. This indicates that there are commonalities in the error-prone points across different models.

% In our ablation studies, we compare error-prone points constructed using the \textit{diff} method and the \textit{common\_prefix} method, noting slight differences in the final results. 
% Balancing effectiveness and efficiency, we use the method based on \textit{prefix} and \textit{suffix}, which allows us to identify error-prone points in generated code in a simple yet effective manner. We plan to further explore more identification strategies in future work.

% \paragraph{Data Scaling For Focused-DPO}

% In our experiments, we use the policy model to sample the dataset for training, with the dataset statistics provided in Table \ref{tab:dataset-statistics}. We also explore how scaling the training data affects the final performance of Focused-DPO. Specifically, we investigate two additional settings: doubling the original training dataset to 10k samples and halving the dataset to 2.5k samples, to observe how these changes impact the effectiveness of the model after Focused-DPO training. The results indicate that fine-grained preference optimization converges efficiently within our given data range, and increasing the dataset size does not significantly improve the results.

% % 限制 acl 2024要求
% % 作者必须在专门的“局限性”部分讨论其工作的局限性。这部分应该包含在论文的末尾，在参考文献之前，它不会计入页数限制。这包括长论文和短论文。注意，在2023年12月之前，这是可选的。
% %对于ARR提交，参考文献和附录应包含在论文的pdf中，但不计入页数限制。
% %正文8页


