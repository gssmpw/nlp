%% 
%% Copyright 2007-2024 Elsevier Ltd
%% 
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%% 
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.3 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.3 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%% 
%% Template article for Elsevier's document class `elsarticle'
%% with harvard style bibliographic references

% \documentclass[preprint,12pt,authoryear]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[authoryear,preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
% \documentclass[final,1p,times,authoryear]{elsarticle}
% \documentclass[final,1p,times,twocolumn,authoryear]{elsarticle}
%% \documentclass[final,3p,times,authoryear]{elsarticle}
\documentclass[final,3p,times,twocolumn,authoryear]{elsarticle}
%% \documentclass[final,5p,times,authoryear]{elsarticle}
% \documentclass[final,5p,times,twocolumn,authoryear]{elsarticle}

%% For including figures, graphicx.sty has been loaded in
%% elsarticle.cls. If you prefer to use the old commands
%% please give \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
%% The amsmath package provides various useful equation environments.
\usepackage{amsmath}
%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers.
%% \usepackage{lineno}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{adjustbox}
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

% \journal{}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for theassociated footnote;
%% use the fnref command within \author or \affiliation for footnotes;
%% use the fntext command for theassociated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for theassociated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \affiliation{organization={},
%%            addressline={}, 
%%            city={},
%%            postcode={}, 
%%            state={},
%%            country={}}
%% \fntext[label3]{}

\title{Deep Learning-Based Facial Expression Recognition for the Elderly: A Systematic Review}

\author[1]{F. Xavier Gaya-Morey\corref{cor1}}
\ead{francesc-xavier.gaya@uib.es}
% \ead[url]{https://orcid.org/0000-0003-1231-7235}
% \credit{Conceptualization, Methodology, Validation, Investigation, Writing - Original Draft, Writing - Review \& Editing Preparation, Visualization}

\affiliation[1]{%
    organization={Universitat de les Illes Balears},
    addressline={Carretera de Valldemossa, km 7.5}, 
    city={Palma, Illes Balears},
    postcode={07122}, 
    country={Spain}}

% Second author

% Third author
\author[1]{Jose M. Buades-Rubio}
\ead{josemaria.buades@uib.es}
% \ead[url]{https://orcid.org/0000-0002-6137-9558}
% \credit{Conceptualization, Methodology, Writing - Review \& Editing Preparation, Supervision, Project administration, Funding acquisition}

\author[2]{Philippe Palanque}
\ead{philippe.palanque@irit.fr}

\affiliation[2]{%
    organization={ICS-IRIT, University Toulouse 3, Paul Sabatier},
    addressline={118 Rte de Narbonne}, 
    city={Toulouse},
    postcode={31062}, 
    country={France}}

\author[3]{Raquel Lacuesta}
\ead{lacuesta@unizar.es}

\affiliation[3]{%
    organization={Universidad de Zaragoza},
    addressline={C. de Pedro Cerbuna, 12}, 
    city={Zaragoza, Aragón},
    postcode={50009}, 
    country={Spain}}
    
\author[1]{Cristina Manresa-Yee}
\ead{cristina.manresa@uib.es}
% \ead[url]{https://orcid.org/0000-0002-8482-7552}
% \credit{Conceptualization, Methodology, Writing - Review \& Editing Preparation, Supervision, Project administration, Funding acquisition}
    
% Corresponding author text
\cortext[cor1]{Corresponding author}

%% Abstract
\begin{abstract}
The rapid aging of the global population has highlighted the need for technologies to support elderly, particularly in healthcare and emotional well-being. Facial expression recognition (FER) systems offer a non-invasive means of monitoring emotional states, with applications in assisted living, mental health support, and personalized care. This study presents a systematic review of deep learning-based FER systems, focusing on their applications for the elderly population. Following a rigorous methodology, we analyzed 31 studies published over the last decade, addressing challenges such as the scarcity of elderly-specific datasets, class imbalances, and the impact of age-related facial expression differences. Our findings show that convolutional neural networks remain dominant in FER, and especially lightweight versions for resource-constrained environments. However, existing datasets often lack diversity in age representation, and real-world deployment remains limited. Additionally, privacy concerns and the need for explainable artificial intelligence emerged as key barriers to adoption. This review underscores the importance of developing age-inclusive datasets, integrating multimodal solutions, and adopting XAI techniques to enhance system usability, reliability, and trustworthiness. We conclude by offering recommendations for future research to bridge the gap between academic progress and real-world implementation in elderly care.
\end{abstract}

% %%Graphical abstract
% \begin{graphicalabstract}
% %\includegraphics{grabs}
% \end{graphicalabstract}

%%Research highlights
% \begin{highlights}
%     \item Systematic review of deep learning facial expression recognition systems in elderly
%     \item Most datasets suffer from elderly underrepresentation and class imbalance
%     \item Few studies include privacy protection methods
%     \item Most studies disregard the use of explainable artificial intelligence tools
%     \item Real-world deployment remains insufficiently explored
% \end{highlights}

%% Keywords
\begin{keyword}
%% keywords here, in the form: keyword \sep keyword
facial expression recognition \sep
deep learning \sep 
computer vision \sep 
elderly

%% PACS codes here, in the form: \PACS code \sep code

%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)

\end{keyword}

\end{frontmatter}

%% Add \usepackage{lineno} before \begin{document} and uncomment 
%% following line to enable line numbers
%% \linenumbers

%% main text
%%

\section{Introduction}

    The global population is aging at an unprecedented rate, with significant implications for multiple aspects of society, including healthcare, social services, and the economy \citep{bloom2016chapter}. According to the World Population Aging report \citep{WHO2024}, there will be 265 million persons aged 80 years or older by the mid-2030s, more than the number of infants (1 year of age or less), and by the late 2070s, the number of persons at ages 65 years and higher is projected to reach 2.2 billion, surpassing the number of children (under age 18). This shift in demographics, driven by increasing life expectancy and declining fertility rates, particularly in developed countries, is creating a significant demand for innovations in elderly care. As older adults are more susceptible to physical and cognitive health conditions, including dementia, Alzheimer’s disease, and other age-related illnesses \citep{langa2018cognitive}, they often require specialized care. Providing such care on a large scale poses unique challenges, calling for the development of technologies that can assist with monitoring, diagnosing, and enhancing the well-being of this rapidly growing segment of the population with very specific needs.

    One area where technology can have a significant impact is in the real-time recognition of emotional states in elderly individuals. Facial expression recognition (FER) systems offer a non-invasive means to monitor emotional well-being and identify potential mental health issues early on \citep{fei2019survey}. FER, in particular, has become an important tool in healthcare, enabling caregivers and medical professionals to better understand the emotional states of their patients and respond accordingly. Social robots, for example, have been shown to increase social engagement among the elderly and can help reduce loneliness and depression by providing emotional support \citep{joan2015what, wang2014towards}. By incorporating FER into these systems, they can interact more naturally with older adults, helping to recognize emotions and respond in ways that are empathetic and supportive. However, despite the growing need for such technology, research in FER focused on the elderly population remains underrepresented in the literature \citep{ma2019elderreact}. Most existing studies on FER target adults and younger adults, leaving a significant gap in understanding how to optimize these systems for older adults, whose facial features may change due to age-related factors \citep{ko2021changes}, making emotion recognition more challenging \citep{mary2016review}.
    
    The foundation of FER systems largely stems from the work of psychologist Paul Ekman, who identified six basic facial expressions—happiness, sadness, fear, anger, surprise, and disgust—that are universally recognized across cultures \citep{ekman1992argument}. These six expressions have since formed the basis of most emotion recognition systems, despite ongoing debates about their universality and the ability of facial expressions to fully capture the complexity of human emotions \citep{barrett2019emotional}. For older adults, FER systems have the potential to greatly improve quality of life by facilitating better communication between caregivers and patients, supporting emotional monitoring, and enabling more personalized care. FER can assist across an array of domains, including medical diagnosis and treatment \citep{grabowski2019emotional}, enabling timely intervention, which is particularly important for individuals suffering from cognitive decline, where emotional expression may be the primary means of communication.
    
    In recent years, Deep Learning (DL) has revolutionized the field of computer vision, including FER \citep{omahony2020deep}. DL techniques, especially Convolutional Neural Networks (CNNs), have demonstrated remarkable performance in recognizing facial expressions by automatically learning features from large datasets without the need for extensive manual feature engineering \citep{li2022deep}. The success of DL in tasks such as object detection, speech recognition, and natural language processing has motivated its application to FER, where it has achieved state-of-the-art performance. One of the key advantages of DL models is their ability to generalize well to complex data, such as facial images, which often involve variations in lighting, pose, and expression. For FER in elderly populations, DL offers the possibility of overcoming the challenges posed by age-related changes in facial structure, allowing for more accurate recognition across diverse populations and over a long period of time for the same population.
    
    However, while DL-based FER systems hold significant promise, they also come with challenges. A key limitation of DL models is their "black-box" nature, meaning that the decision-making process of these models is often opaque to users. In high-stakes applications, such as healthcare, understanding how a model arrives at a particular prediction is crucial for building trust in the system and ensuring its reliability \citep{barredoarrieta2020explainable, adadi2018peeking}. For instance, a medical professional may need to know why a system predicts that a patient is exhibiting signs of depression or distress before making treatment decisions based on that prediction. This lack of transparency in DL models has led to the rise of explainable artificial intelligence (XAI), a field that seeks to make AI systems more interpretable and understandable to humans \citep{barredoarrieta2020explainable}. XAI techniques aim to shed light on the internal workings of complex models by providing explanations for their predictions \citep{gunning2019xai—explainable}. In the context of FER for the elderly, XAI methods can offer caregivers insights into why a system interprets certain facial expressions in specific ways, allowing them to better assess the emotional well-being of their patients. Moreover, the integration of XAI can enhance the development of FER systems by allowing researchers to identify and mitigate biases in the data or model, leading to more robust and reliable systems \citep{burkart2021survey}. Thus, the adoption of XAI techniques is essential for making DL-based FER systems not only effective but also trustworthy and accountable. Furthermore, these techniques can provide insights into expression differences influenced by age.

    This paper presents a systematic review of the current state of DL-based facial expression recognition systems, with a specific focus on their applications for elderly people. The review follows the guidelines for conducting a systematic literature review (SLR) in software engineering, as outlined by \cite{kitchenham2007guidelines}, which provide a comprehensive framework for ensuring rigor in methodology, structure, and best practices. The structure of the paper is as follows: first, we summarize previous reviews on FER, alongside background information emphasizing the effects of aging in FER, highlighting the need for specialized solutions for this population. Next, we introduce the review questions. Following that, the methodology of the review is detailed, including data sources, the search strategy, criteria for study selection, quality assessment procedures, and the process of data extraction. Subsequently, the list of selected studies is thoroughly analyzed to provide insights that address the review questions. In the discussion section, we reflect on the key findings, outline the identified strengths and weaknesses of the analyzed studies, and offer recommendations for future works on the subject. Finally, last section of the paper presents the conclusions drawn from the SLR.

\section{Related Work}

    In this section, we examine the literature to identify studies that assess the impact of age on FER tasks, as well as prior reviews on FER. Our objective is twofold: first, to underscore the lack of FER reviews focused on specific age groups, particularly the elderly, and second, to provide evidence of the age bias often introduced by age-agnostic approaches. This analysis aims to highlight the need for age-sensitive methodologies in FER and to emphasize the importance of addressing this gap in existing reviews.

    \subsection{Effects of Aging on Facial Expression Recognition}

        Numerous psychological studies have explored the effects of aging on facial expression recognition, demonstrating that observers are influenced by their own age. For example, older individuals tend to exhibit deficits in decoding specific emotions \citep{isaacowitz2011bringing, ruffman2008meta}. However, aging impacts not only the observers but also the individuals displaying the target expressions. In this regard, \cite{fölster2014facial} examined how age-related changes in the face, such as wrinkles and folds, influence the decoding process of emotional expressions, concluding that the age of the face is a critical factor. This finding is supported by subsequent studies, such as \cite{ko2021changes}, which identified variations in facial expression intensities and muscle usage across different age groups. For instance, elderly individuals tend to display more negative emotions and engage more muscles in the lower face compared to younger people. Similarly, \cite{ngrondhuis2021having} used generative adversarial networks (GAN) to investigate the increased difficulty in identifying expressions of older adults and attributed this challenge to the decline in facial muscle function with age. \cite{battinisonmez2019computational} explored the effect of training FER models with data from different age groups and found that recognizing expressions in elderly faces posed the greatest difficulty. A review by \cite{raghebatallah2019review} on the effect of facial aging on FER identified several open challenges, including the scarcity of images capturing the same individuals across different ages (which hinders the learning of aging patterns), the tendency of models to overlook important facial features, and the considerable variation in aging effects across subjects. Moreover, the training of a Siamese CNN on the ElderReact and EmoReact datasets by \cite{rahatuljannat2021expression} highlighted differences in the expressions of elderly individuals and children. \cite{park2022facial} demonstrated the importance of addressing age bias in FER, showing that age-specific training yielded a 22\% improvement in accuracy compared to using a non-age-specific dataset.

        Automatic FER systems are heavily influenced by the age of the faces in the dataset, making the choice of dataset critical. Datasets such as FACES \citep{ebner2010faces} and LifeSpan \citep{minear2004lifespan}, are frequently employed \citep{guo2013facial, mary2016review, wu2015enhanced, wang2015facial, al_garaawi2016study, lopes2018facial, battinisonmez2019computational, caroppo2017facial, caroppo2019facial, caroppo2020comparison, al_garaawi2022fully}, largely due to their inclusion of subjects from a broad age range, which facilitates the development of age-invariant models. To address the effects of aging, several strategies have been proposed. One approach involves removing aging features through facial smoothing techniques that eliminate age-related details without compromising essential structural information \citep{guo2013facial, mary2016review}. Another method incorporates age information during training using Bayesian networks, while marginalizing over age during testing \citep{wu2015enhanced, wang2015facial}. \cite{al_garaawi2016study} first analyzed age-related differences in facial characteristics, then later incorporated age as a key feature in their model \citep{al_garaawi2022fully}, using a weighted combination of age group estimators and age-specific expression recognizers. 
        
        Other studies have focused on specific age groups rather than addressing aging effects comprehensively. Datasets utilized in such approaches include ElderReact \citep{ma2019elderreact} and Tsinghua \citep{yang2020tsinghua} for elderly subjects, CK+ \citep{lucey2010extended}, JAFFE \citep{lyons2020coding}, AFEW \citep{dhall2007collecting}, and FER-2013 \citep{goodfellow2013challenges} for adults, and LIRIS \citep{khan2019novel}, CAFE \citep{lobue2014child}, DEFSS \citep{meuwissen2017creation}, and EmoReact \citep{nojavanasghari2016emoreact} for children, among others.
        
    
    \subsection{Summary of Previous Reviews}
        % Background 
        
        Mining the literature, we identified several existing reviews addressing facial expression recognition. Although most of these do not focus specifically on elderly populations and therefore do not consider the effects of human aging discussed in the previous section, they do provide valuable insights into the datasets and techniques commonly employed in FER. In this section, we analyze relevant reviews and surveys from the past decade, highlighting that many critical aspects addressed in our current study have not been thoroughly examined in prior reviews.
    
        \begin{table*}
            \centering
            % \begin{adjustbox}{width=.65\textwidth}
            \begin{tabular*}{.71\textwidth}{l|ll|ll|lllll}
                 & \multicolumn{2}{c|}{\textbf{Process}} & \multicolumn{2}{c|}{\textbf{Focus}} &\multicolumn{5}{c}{\textbf{Researched data}} \\
                 \multicolumn{1}{c|}{} & \rotatebox{90}{Systematic} & \rotatebox{90}{Prev. reviews} & \rotatebox{90}{Elderly} & \multicolumn{1}{c|}{\rotatebox{90}{DL}} & \rotatebox{90}{Datasets} & \rotatebox{90}{Multimodality} & \rotatebox{90}{Deployment} & \rotatebox{90}{Privacy} & \rotatebox{90}{XAI}\\ \midrule
                \cite{ghayoumi2017quick} &  &  &  & \cmark &  &  &  &  \\
                \cite{asad2017recent} &  &  &  &  &  &  &  &  &  \\
                \cite{chulko2018brief} &  & \cmark &  &  & \cmark &  &  &  &  \\
                \cite{rajeswari2018literature} &  &  &  &  &  &  &  &  &  \\
                \cite{yantililiana2018review} &  &  &  &  &  &  &  &  &  \\
                \cite{fei2019survey} &  &  & \cmark &  &  &  &  &  &  \\
                \cite{bhattacharya2019survey} &  &  &  &  &  &  &  &  &  \\
                \cite{martinez2019automatic} &  & \cmark &  &  & \cmark &  &  &  &  \\
                \cite{canedo2019facial} & \cmark &  &  &  & \cmark & \cmark &  &  &  \\
                \cite{achinchanikar2019facial} &  &  &  & \cmark & \cmark &  &  &  &  \\
                \cite{svyas2019survey} &  &  &  &  & \cmark &  &  &  &  \\
                \cite{sari2020automated} &  &  &  & \cmark & \cmark &  &  &  &  \\
                \cite{li2022deep} &  &  &  & \cmark & \cmark & \cmark &  &  &  \\
                \cite{patel2020facial} &  & \cmark &  &  & \cmark &  &  &  &  \\
                \cite{ribeiroalexandre2020systematic} & \cmark & \cmark &  &  & \cmark & \cmark &  &  &  \\
                \cite{pranathi2021review} &  &  &  &  & \cmark &  &  &  &  \\
                \cite{dalvi2021survey} & \cmark & \cmark &  &  & \cmark & \cmark &  &  &  \\
                \cite{revina2021survey} &  &  &  &  & \cmark &  &  &  &  \\
                \cite{moolchandani2021survey} &  &  &  &  & \cmark &  &  &  &  \\
                \cite{ullah2021systematic} & \cmark &  &  &  & \cmark &  &  &  &  \\
                \cite{muazu2021systematic} &  &  &  &  & \cmark &  &  &  &  \\
                \cite{msaleemabdullah2021facial} &  &  &  & \cmark &  &  &  &  &  \\
                \cite{moehtay2021feature} &  &  &  &  & \cmark &  &  &  &  \\
                \cite{raijain2021recent} &  &  &  &  & \cmark & \cmark &  &  &  \\
                \cite{modi2022state-of-the-art} &  &  &  &  &  &  &  &  &  \\
                \cite{zagocanal2022survey} & \cmark & \cmark &  &  & \cmark &  &  &  &  \\
                \cite{azlinaabaziz2022systematic} & \cmark &  &  &  & \cmark &  & \cmark &  &  \\
                \cite{maithri2022automated} & \cmark &  &  &  & \cmark & \cmark &  &  &  \\
                \cite{rehmankhan2022facial} &  & \cmark &  &  & \cmark &  &  &  &  \\
                \cite{guerdelli2022macro} &  &  &  &  & \cmark & \cmark &  &  &  \\
                \cite{rashmiadyapady2023comprehensive} &  &  &  &  &  &  &  & \cmark &  \\
                \cite{liang2023survey} &  &  &  & \cmark & \cmark & \cmark &  &  &  \\
                \cite{labzour2023survey} &  &  & \cmark &  & \cmark &  &  &  &  \\
                \cite{vinicioslopespinto2023systematic} & \cmark &  &  & \cmark & \cmark &  &  &  &  \\
                \cite{boughanem2023facial} &  &  &  & \cmark & \cmark & \cmark &  &  &  \\
                \cite{chitleong2023facial} & \cmark & \cmark &  &  &  &  & \cmark &  &  \\
                \cite{almasoudi2023facial} &  &  &  &  & \cmark &  &  &  &  \\
                \cite{kumari2024emotion} &  &  &  & \cmark & \cmark &  &  &  &  \\
                \cite{kaur2024facial} & \cmark & \cmark &  &  & \cmark & \cmark &  &  &  \\
                \cite{mohana2024facial} & \cmark & \cmark &  &  & \cmark &  &  &  &  \\
                This study & \cmark & \cmark & \cmark & \cmark & \cmark & \cmark & \cmark & \cmark & \cmark \\
                \bottomrule
            \end{tabular*}
            % \end{adjustbox}
            \caption{Comparison of previous reviews with our work, considering the key aspects of the current systematic review. The columns represent the following criteria, from left to right: whether the review is systematic; it includes an exploration of prior reviews; it focuses on the elderly population; it focuses on DL-based approaches; it provides a list of datasets; it investigates the use of multimodal data; it examines the deployment of FER systems in real-world environments; it considers privacy a critical issue; and it discusses the application of XAI techniques in FER systems.}
            \label{tab:review-comparison}
        \end{table*}
        
        To begin with, it is important to consider the methodological rigor of prior review processes. A systematic review methodology promotes a thorough and unbiased selection and analysis of studies by precisely defining information sources, search strategies, study selection criteria, and data extraction procedures. Additionally, quality assessment steps are essential to ensure that only high-quality, relevant studies are included. However, only 12 of the 41 reviews listed in Table \ref{tab:review-comparison} followed a systematic review process. \cite{kitchenham2007guidelines} also emphasize the importance of analyzing existing reviews in the field to identify gaps in the literature and avoid duplicate efforts. Yet, only 11 studies accounted for previous reviews, as illustrated in the table.
        
        The main distinction between the current study and earlier reviews lies in our focus on FER applications for the elderly and deep learning tools. Only two previous reviews have concentrated specifically on elderly populations. \cite{labzour2023survey} discussed 11 studies on elderly-focused FER, covering both traditional and deep learning methods and their associated datasets. \cite{fei2019survey} also reviewed FER in elderly populations, primarily as a tool for early diagnosis of mild cognitive impairment. However, their work lacks coverage of recent studies from the last five years and focuses primarily on traditional computer vision approaches rather than DL-based methods. Consequently, there is a gap in the literature regarding recent DL-based FER research specific to elderly individuals.
        
        Ten reviews were identified that discuss DL in FER \citep{ghayoumi2017quick, achinchanikar2019facial, sari2020automated, li2022deep, msaleemabdullah2021facial, liang2023survey, vinicioslopespinto2023systematic, boughanem2023facial, kumari2024emotion}. Of these, only the work by Pinto et al. employed a systematic review process, and none of these reviews examined previous FER reviews, which may have contributed to overlapping lists of studies. While many of these reviews included extensive lists of commonly used datasets--an important consideration in DL-based approaches--none were focused on elderly populations, nor did they address critical aspects for the current work such as deployment in real-world environments, privacy preservation, or explainable AI techniques.
        
        Three critical aspects—deployment in real--world settings, privacy preservation, and XAI techniques--remain largely unexplored in prior reviews. Deployment in real-world scenarios was examined in only two reviews, which described applications involving mobile apps and robots \citep{chitleong2023facial} and smart home integration \citep{azlinaabaziz2022systematic}. Although ethical concerns regarding privacy were noted in two reviews \citep{kaur2024facial, almasoudi2023facial}, only one review explored this issue in depth \citep{rashmiadyapady2023comprehensive}. Furthermore, despite the growing importance of XAI, this area was not thoroughly investigated in any review, even though some listed it as a future challenge \citep{mohana2024facial, kaur2024facial, patel2020facial}.
        
        Given the evident lack of reviews focused on elderly populations using DL-based techniques for FER, the present study aims to address this gap. Additionally, as few reviews have examined factors critical for the practical deployment of FER systems in real-world environments, and none have explored XAI in detail, our work may offer valuable insights for the development of practical, reliable applications.

\section{Review Questions}
    % Identify primary and secondary review questions. Note this section may be included in the background section.

    We identified two primary review questions, each associated with multiple secondary questions, as shown in Table \ref{tab:questions}.

    \begin{table*}[h]
        \label{tab:questions}
        % \begin{adjustbox}{width=\textwidth}
        \centering
        \begin{tabular*}{.81\textwidth}{ll}
            \toprule
            \textbf{ID} & \textbf{Research Question} \\
            \midrule
            \textbf{RQ1} & 
            \textbf{What DL techniques are applied for FER in elderly populations?}\\
            
            RQ1.1 & 
            Which architectures are most commonly used?\\
    
            RQ1.2 & 
            Which datasets are most frequently employed?\\
    
            RQ1.3 & 
            What are the primary characteristics of the data utilized?\\
    
            RQ1.4 & 
            To what extent are facial landmarks and action units used?\\
    
            RQ1.5 & 
            What other tasks are commonly computed alongside FER?\\
            
            \midrule
    
            \textbf{RQ2} & 
            \textbf{How can these techniques be effectively deployed in real-world environments?}\\

            RQ2.1 &
            Are proposed solutions being deployed?\\
    
            RQ2.2 &
            Are aging biases addressed?\\
    
            RQ2.3 & 
            Is privacy a primary concern?\\
    
            RQ2.4 & 
            Is economic cost a primary consideration?\\
    
            RQ2.5 & 
            Are XAI techniques employed?\\
            
            \bottomrule
        \end{tabular*}
        % \end{adjustbox}
        \caption{Primary and secondary research questions used for this SLR.}
    \end{table*}
    
    RQ1 aims to find which deep learning techniques are applied specifically to facial expression recognition in elderly populations, and understand how are they used. The associated secondary questions delve into particular facets of these techniques, such as the prevalent architectures and datasets. Additionally, the questions examine data characteristics (e.g., image or video format, and multimodal integration) and assess the role of facial landmarks and action units, which are common features in FER research.

    In contrast, RQ2 focuses on practical considerations for deploying FER systems for elderly users in real-world settings. We investigate factors crucial to successful deployment, including the potential impact of age-related biases in training data that could lead to suboptimal performance in real-life scenarios. We also assess privacy and economic considerations. Finally, given the critical need for transparency and trust in DL applications within healthcare settings \citep{adadi2018peeking}, we explore the extent to which explainable AI techniques are integrated into these systems.

\section{Review Methods}

    In this section, we provide a detailed description of the systematic review process, adhering to the guidelines outlined by \cite{kitchenham2007guidelines}. First, we present the data sources and search strategy employed to compile the initial list of studies. Next, we explain the quality assessment and study selection procedures used to exclude low-quality and irrelevant works. Lastly, we outline the information extracted from each selected study.

    \subsection{Data sources}

        To ensure a comprehensive collection of relevant studies, we utilized five distinct digital databases. SCOPUS and Web of Science (WOS) were selected for their broad interdisciplinary coverage, while the ACM Digital Library, IEEE Xplore Digital Library, and PubMed were chosen for their focus on computer science, technology, and biomedical research, respectively. Figure \ref{fig:sources} illustrates the distribution of studies retrieved from each source. Notably, the ACM Digital Library yielded the highest number of studies among the five databases.
        
        \begin{figure}[h]
             \centering
             \includegraphics[width=\columnwidth]{sources.png}
             \caption{Distribution of publications retrieved across the five databases.}
            \label{fig:sources}
        \end{figure}
        
    \subsection{Search strategy}

        To ensure a more precise search in the SCOPUS database, we limited our query to the title, abstract, and keywords, as searching the full text yielded a significant number of irrelevant results. In contrast, full-text searches were performed in the other databases. Although the query strings were adjusted according to the specific requirements of each search engine, they consistently included four key concepts: facial expression recognition, computer vision, elderly population, and deep learning. Each concept was paired with multiple synonyms and, where applicable, the "*" wildcard was used to capture various word terminations.

        The search was constrained to publications from the past ten years, specifically from 2015 through September 2024, inclusive. Figure \ref{fig:years} presents the number of related studies identified by year, demonstrating a clear upward trend, particularly over the last three years.
        
        \begin{figure}[h]
             \centering
             \includegraphics[width=\columnwidth]{years.png}
             \caption{Number of publications found from 2015 to September 2024 (both included).}
            \label{fig:years}
        \end{figure}
    
    \subsection{Quality assessment} \label{section:quality}

        To assess the quality of the included studies, we followed the guidelines proposed by \cite{kitchenham2007guidelines}. The evaluation was based on four primary criteria:
        
        \begin{itemize}
            \item \textbf{Type of publication}. Since conference proceedings may not always undergo a rigorous peer-review process, unlike journal articles, the type of publication was considered as a quality indicator.
            
            \item \textbf{Reproducibility}. This criterion assesses whether the work can be replicated, either through detailed explanations of the methods used, by providing access to the programming code, or by making the collected dataset publicly available.
            
            \item \textbf{Benchmarking with the state-of-the-art}. Whether the proposed methods are compared with prior works under similar conditions, such as using the same performance metrics, dataset splits, and methodologies.
            
            \item \textbf{Performance on public datasets}. The use of publicly available datasets is important, as it allows other researchers to compare their results with those of prior studies in a standardized manner.
        \end{itemize}
        
        By applying these criteria, we were able to filter out studies that exhibited potential bias. This quality assessment was integrated into the study selection process, further detailed in Section \ref{section:selection}, and the list of extracted data from each study, outlined in Section \ref{section:extraction}.

    \subsection{Study selection} \label{section:selection}

        After gathering the search results from multiple databases, totaling 285 studies, we identified and removed 16 duplicate entries. The remaining studies were then screened individually to exclude publications that were not directly relevant to the research topics. Additionally, we eliminated works that did not meet our inclusion criteria, including those not written in English or Spanish, those without full-text access, or those that did not meet the quality standards outlined in Section \ref{section:quality}. 

        Figure \ref{fig:selection} illustrates the progression of the selection process, from the initial collection of studies to the final screening. Ultimately, 31 studies were selected for inclusion in this review, of which 15 were journal articles, and 16 were conference proceedings. As stated by \cite{kitchenham2007guidelines}, "publication bias can lead to systematic bias in systematic reviews unless special efforts are made to address this issue". Since scanning conference proceedings is a standard search strategy to address publication bias, they were retained for analysis among the relevant studies.
        
        \begin{figure*}[h]
            \centering
            \includegraphics[width=.8\textwidth]{selection.png}
            \caption{Summary of the systematic review process: collection of publications from the five databases, duplicate removal, and final study selection.}
            \label{fig:selection}
        \end{figure*}
    
    \subsection{Data extraction and synthesis} 
    \label{section:extraction}

        To synthesize the relevant studies, different information was extracted from each one. The gathered fields are shown in Figure \ref{fig:extraction}.
        
        \begin{figure*}[h]
            \centering
            \includegraphics[width=.9\textwidth]{extracted-fields.png}
            \caption{Information extracted from each relevant study.}
            \label{fig:extraction}
        \end{figure*}
        
\section{Results} 
\label{section:results}
%     % Non-quantitative summaries should be provided to summarize each of the studies and presented in tabular form.
%     % Quantitative summary results should be presented in tables and graphs. 
%     % - Findings
%     %     Description of primary studies. Results of any quantitative summaries. Details of any meta-analysis.
%     % - Sensitivity analysis

    The complete list of relevant studies is provided in Table \ref{tab:studies}. In this section, we present a detailed analysis of these studies to address the research questions posed. For improved readability, the results have been organized into distinct subsections, each addressing a review question.

    \begin{table*}[h]
        \centering
        \begin{adjustbox}{width=\textwidth}
        \begin{tabular*}{1.05\textwidth}{l|cccccccc}
            \toprule
            \multirow{2}{*}{\textbf{Study}} & \multirow{2}{*}{\textbf{Elderly}} & \multirow{2}{3.2em}{\textbf{Aging effects}} & \multirow{2}{*}{\textbf{Video}} & \multirow{2}{4.5em}{\textbf{Economic cost}} & \multirow{2}{*}{\textbf{Deployed}} & \multirow{2}{*}{\textbf{XAI}} & \multirow{2}{3em}{\textbf{Multi-modal}} & \multirow{2}{*}{\textbf{Privacy}} \\
            \\
            \midrule
            \cite{gaya-morey2024ai-powered} &  &  &  & \cmark & \cmark & \cmark &  &  \\
            \cite{huang2024auto} & \cmark &  &  &  &  &  &  & \cmark \\
            \cite{huang2024facial} & \cmark & \cmark &  &  &  &  &  &  \\
            \cite{jayanthi2024enhanced} &  &  &  &  &  &  &  &  \\
            \cite{nadjadecarolis2024exploring} & \cmark &  & \cmark & \cmark & \cmark &  & \cmark & \cmark \\
            \cite{zhu2024defining} & \cmark &  &  &  &  &  &  & \cmark \\
            \cite{anand2023multi-label} & \cmark &  & \cmark &  &  & \cmark & \cmark &  \\
            \cite{chen2023emotion-reading} &  &  &  &  & \cmark &  &  & \cmark \\
            \cite{khajontantichaikun2023facial} & \cmark &  &  &  &  &  &  &  \\
            \cite{petrou2023lightweight} &  &  &  & \cmark &  &  &  &  \\
            \cite{bi2022dynamic} &  &  & \cmark &  &  &  &  &  \\
            \cite{carretopicón2022do} & \cmark &  &  &  &  &  &  &  \\
            \cite{ekosantoso2022facial} &  &  &  &  &  &  &  &  \\
            \cite{fahn2022image} &  &  &  & \cmark & \cmark &  &  &  \\
            \cite{fan2022fer-pcvt} & \cmark &  &  &  &  & \cmark &  &  \\
            \cite{fei2022novel} & \cmark &  &  & \cmark &  &  &  &  \\
            \cite{jiang2022automated} &  &  &  &  &  &  &  &  \\
            \cite{khajontantichaikun2022emotion} & \cmark &  &  &  &  &  &  &  \\
            \cite{sreevidya2022elder} & \cmark &  & \cmark &  &  &  & \cmark &  \\
            \cite{kim2021age} & \cmark & \cmark &  &  &  &  &  &  \\
            \cite{rahatuljannat2021expression} & \cmark & \cmark &  &  &  &  &  &  \\
            \cite{caroppo2020comparison} & \cmark & \cmark &  & \cmark &  &  &  & \cmark \\
            \cite{sharma2020audio-visual} & \cmark &  & \cmark &  &  &  & \cmark & \cmark \\
            \cite{yan2020deep} &  &  &  & \cmark & \cmark &  &  &  \\
            \cite{zhang2020facial} &  &  & \cmark &  &  &  &  &  \\
            \cite{skibelirokkones2019facial} &  &  & \cmark &  &  & \cmark &  &  \\
            \cite{caroppo2018facial} & \cmark &  &  & \cmark &  &  &  &  \\
            \cite{yang2018joint} & \cmark & \cmark &  &  &  &  &  &  \\
            \cite{ziauddin2017facial1} & \cmark &  &  &  &  &  &  & \cmark \\
            \cite{ziauddin2017facial2} &  &  &  &  &  & \cmark &  & \cmark \\
            \cite{wu2015enhanced} & \cmark & \cmark &  &  &  &  &  & \cmark \\
            \bottomrule
            \end{tabular*}
            \end{adjustbox}
        \caption{Relevant studies identified in the systematic review. The columns, from left to right, indicate whether the study includes experiments with elderly users, examines the effects of age on FER, utilizes the temporal dimension, considers economic cost, integrates the solution into a framework or deployment, employs XAI tools, incorporates additional modalities, and implements privacy protection methods.}
        \label{tab:studies}
    \end{table*}

    \subsection{RQ1. DL techniques for FER in elderly populations}
    
        Next, we present the findings related to the first research question, focusing on common DL architectures, datasets, data characteristics, the use of facial landmarks and action units, and related tasks in performing FER for elderly populations.

    \subsubsection{RQ1.1. Deep learning architectures}
    \label{sec:models}
    
        % \begin{itemize}
        %     \item Table with list of architectures.
        %     \item Taxonomy: CNNs, RNNs, Transformers
        %     \item Smaller architectures: MobileNet, EfficientNet, simple CNNs, mini-Xception...
        %     \item Unsupervised: Bayesian Network, AE...
        %     \item Software: Face++, Sighthound, Amazon Rekognition, Microsoft Face, OpenFace 2.0, FaceReader
        %     \item Classes used
        % \end{itemize}

        Table \ref{tab:models} provides an overview of the deep learning-based methods employed in the relevant studies. The table highlights the diversity in model architectures and their applications to various tasks within the facial expression recognition pipeline.

        \begin{table*}
            \centering
            % \begin{adjustbox}{width=\textwidth}
            \begin{tabular*}{\textwidth}{ll|llllll}
                \toprule
                \textbf{Reference} & \textbf{Model/Software} & \textbf{Task/s} & \textbf{Studies} \\
                \midrule
                \cite{VGG} & VGG & FER, feat. ext. & 6 \\
                \cite{GoogLeNet} & Inception & FER, feat. ext. & 5 \\
                \cite{lecun1999object} & CNN & FER, feat. ext. & 5 \\
                \cite{EfficientNet} & EfficientNet & FER, feat. ext. & 3 \\
                \cite{Xception} & Xception & FER & 3 \\
                \cite{MobileNet} & MobileNet & FER, FR, feat. ext.,   landmarks & 3 \\
                \cite{ren2017faster} & Faster R-CNN & FER, OD & 3 \\
                \cite{ResNet} & ResNet & FER, FR, feat. ext. & 3 \\
                \cite{YOLO} & YOLO & FER, OD & 3 \\
                \cite{zhang2016joint} & MTCNN & OD, landmarks & 3 \\
                \cite{noldus} & FaceReader & FER, landmarks, AUs & 2 \\
                \cite{liu2016ssd} & SSD & FER, OD & 2 \\
                \cite{GAN} & GAN & dataset gen., privacy prot. & 2 \\
                \cite{AlexNet} & AlexNet & FER & 2 \\
                \cite{hinton2006fast} & DBN & FER & 2 \\
                \cite{LSTM} & LSTM & FER & 2 \\
                \cite{kuprashevich2024mivolo} & MiVOLO & age, gender & 1 \\
                \cite{ruan2024scatnet} & SCaTNet & feat. ext. & 1 \\
                \cite{openai2024gpt-4} & GPT-4 & text emotion recognition, chatbot & 1 \\
                \cite{kabir2023spinalnet} & SpinalNet & FER & 1 \\
                \cite{SilNet2022} & SilNet & FER & 1 \\
                \cite{liu2022video} & Swin-Transformer & FER & 1 \\
                \cite{fan2022fer-pcvt} & FER-PCVT & FER & 1 \\
                \cite{prados-torreblanca2022shape} & SPIGA & landmarks & 1 \\
                \cite{dosovitskiy2021image} & ViT & feat. ext. & 1 \\
                \cite{wang2021pyramid} & PvT & FER & 1 \\
                \cite{wu2021cvt} & CvT & FER & 1 \\
                \cite{guo2021sample} & SCRFD & OD & 1 \\
                \cite{cao2021openpose} & OpenPose & pose estimation & 1 \\
                \cite{qin2020u2-net} & U2-Net & OS & 1 \\
                \cite{hernandez-ortega2019faceqnet} & FaceQnet & facial image quality & 1 \\
                \cite{baltrusaitis2018openface} & OpenFace 2.0 & landmarks, AUs & 1 \\
                \cite{MicrosoftFace} & Microsoft Face & FER & 1 \\
                \cite{Transformer} & Transformer & FER & 1 \\
                \cite{AmazonRekognition} & Amazon Rekognition & FER & 1 \\
                \cite{niandola2016squeezenet} & SqueezeNet & OD & 1 \\
                \cite{Sighthound} & Sighthound & FER & 1 \\
                \cite{WeiNet2015} & WeiNet & FER & 1 \\
                \cite{Siamese_CNNs} & Siamese NN & FER & 1 \\
                \cite{Song2014} & SongNet & FER & 1 \\
                \cite{GRU} & GRU & FER & 1 \\
                \cite{Face++} & Face++ & FER & 1 \\
                \cite{vincent2010stacked} & SDAE & feat. ext. & 1 \\
                \cite{pearl1988probabilistic} & Bayesian Network & FER & 1 \\
                \cite{jhopfield1982neural} & RNN & FER & 1 \\
                \bottomrule
                \end{tabular*}
                % \end{adjustbox}
            \caption{Deep learning methods and software, along with the tasks they were applied to and the number of studies in which they were utilized. The following acronyms and abbreviations where used for conciseness: facial expression recognition (FER), feature extraction (feat. ext.), object detection (OD), facial recognition (FR), object segmentation (OS).}
            \label{tab:models}
        \end{table*}

        The methods listed in the table were used for a wide range of tasks. The most common was FER itself, where models included a final classification layer to predict facial expressions. Convolutional models such as VGG, Inception, and ResNet were often employed to extract complex features, which were later used for classification tasks \citep{zhang2020facial,bi2022dynamic,huang2024facial}. Object detectors, including Faster R-CNN, YOLO, SSD, MTCNN, and SCRFD, were used in pre-processing steps to localize and crop faces before performing FER \citep{jiang2022automated,gaya-morey2024ai-powered}. Notably, Faster R-CNN, YOLO, and SSD were also used by Khajontantichaikun et al. \citeyearpar{khajontantichaikun2022emotion,khajontantichaikun2023facial} to simultaneously perform object detection and FER. In addition to FER, some studies focused on auxiliary tasks such as facial landmark estimation, for which MobileNet, SPIGA, MTCNN, FaceReader, and OpenFace 2.0 were employed \citep{zhu2024defining,chen2023emotion-reading,gaya-morey2024ai-powered}. Among these, FaceReader and OpenFace 2.0 also supported action unit estimation. Other applications included emotion recognition from text, as demonstrated by \cite{nadjadecarolis2024exploring}, who used GPT-4 for emotion classification and text generation in a social robot. \cite{gaya-morey2024ai-powered} also explored the integration of multiple tasks in a social robot, among which U2-Net and MiVOLO were used for object segmentation and for age and gender estimation, respectively. To address privacy concerns, \cite{huang2024auto} employed GAN to transform images and FaceQnet to estimate image quality. GAN were also used by \cite{zhu2024defining} to generate a FER dataset comprising multiple age groups.

        Three primary families of architectures were frequently employed: CNNs, recurrent neural networks (RNNs), and Transformers. Convolutional neural networks were the most widely used, appearing in 27 models across 22 of the reviewed studies. These feed-forward architectures are highly effective for computer vision tasks and were often used as either end-to-end FER solutions \citep{petrou2023lightweight,gaya-morey2024ai-powered} or feature extractors \citep{caroppo2020comparison,sreevidya2022elder}. On the other hand, RNNs such as GRU and LSTM were used in only four studies, where they excelled in handling temporal information for video-based FER \citep{zhang2020facial} or sequential non-visual data \citep{sharma2020audio-visual}. Transformers, known for their attention mechanisms, have gained traction in recent years. Nine models from the table incorporated attention-based methods, appearing in six studies. Transformers were used for tasks ranging from FER in images \citep{ekosantoso2022facial,huang2024auto} to multimodal analysis \citep{fan2022fer-pcvt} and hybrid convolutional-transformer models like the Convolutional Vision Transformer (CvT).

        A few studies incorporated unsupervised learning phases in their FER pipelines. Uddin et al. \citeyearpar{ziauddin2017facial2,ziauddin2017facial1} used deep belief networks (DBNs) with an initial unsupervised training phase for feature extraction learning, followed by supervised fine-tuning. Similarly, \cite{caroppo2018facial} utilized a Stacked Denoising Auto-Encoder (SDAE), which first learned to reconstruct input data in an unsupervised phase before being fine-tuned for FER.

        To enable deployment on low-resource devices or ensure real-time processing, some studies adopted lightweight architectures. MobileNet \citep{fei2022novel,gaya-morey2024ai-powered} was designed specifically for mobile applications, while EfficientNet \citep{nadjadecarolis2024exploring,gaya-morey2024ai-powered,ekosantoso2022facial} offered scalable performance with optimized resource usage. Similarly, the SqueezeNet \cite{niandola2016squeezenet} achieved the same accuracy than AlexNet with 50 times less parameters, thanks to their proposed \textit{Fire modules}. The mini-Xception network \citep{yan2020deep,petrou2023lightweight}, a compact version of  Xception, drastically reduced the number of trainable parameters, making it ideal for lightweight FER solutions.

        Commercial software also played a role in some studies. \cite{kim2021age} investigated the age bias in FER tools like Amazon Rekognition, Face++, Microsoft Face, and Sighthound, revealing significant performance drops when analyzing elderly faces. OpenFace 2.0 was employed by \cite{sharma2020audio-visual} for facial landmarks and action unit detection, which were subsequently used by DL models for FER. FaceReader, capable of estimating facial landmarks, action units, and expressions, was examined by \cite{zhu2024defining}, who defined expected outputs for smiling images. \cite{chen2023emotion-reading} further integrated FaceReader into an emotion-reading care environment.

        The number and type of output classes in FER models varied across studies. As shown in Figure \ref{fig:classes}, most studies focused on the six basic facial expressions--happiness, sadness, anger, surprise, fear, and disgust--along with the neutral expression. Additional expressions such as contempt \citep{zhang2020facial,skibelirokkones2019facial,fan2022fer-pcvt,jayanthi2024enhanced}, sleep, calmness \citep{chen2023emotion-reading}, strain, tiredness, pain \citep{fan2022fer-pcvt}, excitement, and frustration \citep{anand2023multi-label} were also explored in a smaller number of studies.
        
        \begin{figure}[h]
             \centering
             \includegraphics[width=\columnwidth]{classes.png}
             \caption{Distribution of FER classes observed in the analyzed studies.}
            \label{fig:classes}
        \end{figure}

        Lastly, several studies employed traditional computer vision approaches coupled with deep learning for facial expression recognition. Support vector machines (SVMs) were frequently utilized for facial expression classification after feature extraction through other methods \citep{fei2022novel,caroppo2020comparison}. Principal component analysis was applied in some studies to reduce data dimensionality prior to classification \citep{ziauddin2017facial2,bi2022dynamic}. For face detection and cropping during pre-processing, the Viola-Jones algorithm was employed in multiple works \citep{fei2022novel,gaya-morey2024ai-powered}. Additionally, a variety of feature descriptors were implemented across studies, including local binary patterns \citep{caroppo2020comparison,zhang2020facial,ziauddin2017facial2}, local directional strength patterns \citep{skibelirokkones2019facial}, and the Geneva minimalistic acoustic parameter set \citep{sharma2020audio-visual}.
        
    \subsubsection{RQ1.2. Datasets}
        % \begin{itemize}
        %     \item Table with list of datasets.
        %     \item Automatic vs. controlled
        %     \item Image vs. Video
        %     \item Classes
        %     \item External vs. custom datasets.
        % \end{itemize}
        
        Table \ref{tab:datasets} provides an overview of the datasets used for training in the FER tasks identified in the reviewed studies. As highlighted, these datasets vary significantly in terms of annotations, data type, classes, and sample sizes.

        \begin{table*}
            \centering
            \begin{adjustbox}{width=\textwidth}
            \begin{tabular*}{1.075\textwidth}{ll|llllll}
                \toprule
                \textbf{Reference} & \textbf{Dataset} & \textbf{Users} & \textbf{Ages} & \textbf{Type} & \textbf{Samples} & \textbf{Classes} & \textbf{Studies}\\
                \midrule
                \cite{goodfellow2013challenges} & FER-2013 & N/A & N/A & Image & 35,887 & 7 & 7 \\
                \cite{cebner2010faces-a} & FACES & 171 & 19-80 & Image & 2,052 & 6 & 6 \\
                \cite{lucey2010extended} & CK+ & 123 & 18-50 & Video & 593 & 7 & 5 \\
                \cite{minear2004lifespan} & LifeSpan & 576 & 18-93 & Image & 1,354 & 8 & 4 \\
                \cite{ma2019elderreact} & ElderReact & 46 & N/A & Video & 1,323 & 6 & 3 \\
                \cite{mollahosseini2017affectnet} & AffectNet & N/A & N/A & Image & 440,000 & 7 & 3 \\
                \cite{nojavanasghari2016emoreact} & EmoReact & 63 & 4-14 & Video & 1,102 & 8 & 3 \\
                \cite{zhao2011facial} & Oulu-CASIA & 80 & 23-58 & Video & 480 & 6 & 2 \\
                \cite{langner2010presentation} & RaFD & 49 & N/A & Image & 5,880 & 8 & 2 \\
                \cite{fei2022novel} & CAD & N/A & N/A & Image & 165 & 5 & 1 \\
                \cite{fei2022novel} & CEPD & N/A & N/A & Image & 13,692 & 6 & 1 \\
                \cite{hussain2021human} & Jafar   Hussain & N/A & N/A & Image & 679 & 5 & 1 \\
                \cite{yang2020tsinghua} & TFED & 110 & 19-76 & Image & 1128 & 8 & 1 \\
                \cite{oliver2020uibvfed} & UIBVFED & 20 & 20-80 & Image & 640 & 6 & 1 \\
                \cite{jlyons2020coding} & JAFFE & 10 & N/A & Image & 219 & 7 & 1 \\
                \cite{li2018recursive} & CIFE & N/A & N/A & Image & 14,756 & 7 & 1 \\
                \cite{livingstone2018ryerson} & RAVDESS & 24 & 21-33 & Video & 7,356 & 8 & 1 \\
                \cite{bagherzadeh2018multimodal} & CMU-MOSEI & 1,000 & N/A & Video & 3,228 & 6 & 1 \\
                \cite{zhalehpour2017baum-1} & BAUM-1s & 31 & 19-65 & Video & 1,184 & 8 & 1 \\
                \cite{pantic2005web-based} & MMI & 75 & 19-62 & Video+Image & 848+740 & 6 & 1 \\
                \cite{lundqvist1998karolinska} & KDEF & 70 & 20-30 & Image & 4,900 & 7 & 1 \\
                \bottomrule
                \end{tabular*}
                \end{adjustbox}
            \caption{FER datasets used in the studies included in this review. The columns, from left to right, display the number of users, their age range, whether the dataset contains image or video data, the total number of samples, the number of facial expression classes, and the number of relevant studies that used each dataset.}
            \label{tab:datasets}
        \end{table*}

        Several datasets were automatically collected from the Internet, including FER-2013, ElderReact, AffectNet, EmoReact, Jafar Hussain, CIFE, and CMU-MOSEI. This collection approach enables the creation of large datasets, such as FER-2013 (35,887 samples) and AffectNet (440,000 samples). However, these datasets often suffer from issues such as poor expression annotations \citep{mejia-escobar2023towards}, the presence of duplicate samples \citep{ijimai}, and the absence of demographic information about users. Conversely, datasets collected under controlled conditions offer advantages such as user selection based on demographic criteria, multiple samples from the same individual, and controlled lighting conditions. For example, FACES and LifeSpan emphasize age diversity, supporting cross-age group studies, while ElderReact and EmoReact target elderly individuals and children, respectively. Additionally, UIBVFED stands out as the only synthetically generated dataset, allowing for precise control over the facial expressions of avatars, as well as their demographic characteristics, including age, gender, and ethnicity.

        Although the majority of datasets are in image format, several are video-based \citep{lucey2010extended, ma2019elderreact, nojavanasghari2016emoreact, zhao2011facial, livingstone2018ryerson, bagherzadeh2018multimodal, zhalehpour2017baum-1}. Notably, MMI \citep{pantic2005web-based} offers samples in both image and video formats. Video-based datasets enable the incorporation of temporal information into FER tasks and are particularly valuable for systems requiring real-time processing.

        Most datasets include the six basic facial expressions (happiness, sadness, anger, fear, disgust, and surprise). Additionally, the neutral expression is included in twelve datasets, while contempt is present in three. Other, less common expressions are found in only one dataset each, such as boredom \citep{zhalehpour2017baum-1}, content \citep{yang2020tsinghua}, calmness \citep{livingstone2018ryerson}, curiosity, uncertainty, excitement, frustration \citep{nojavanasghari2016emoreact}, annoyance, and grumpiness \citep{minear2004lifespan}.

        Instead of relying on the datasets listed in Table \ref{tab:datasets}, several of the analyzed studies chose to create custom datasets tailored to their specific research needs. These efforts provided unique opportunities to address specialized populations or expressions that were not well-represented in existing datasets. For instance, Uddin et al. \citeyearpar{ziauddin2017facial2,ziauddin2017facial1} developed a dataset containing 40 close-up RGB-D videos for each of the six basic facial expressions. This approach leveraged depth information to improve facial expression recognition while maintaining user privacy. Similarly, Khajontantichaikun et al. \citeyearpar{khajontantichaikun2022emotion,khajontantichaikun2023facial} curated a dataset of 900 images of Thai elderly individuals, with 150 samples per basic expression. These samples were collected from online media and annotated by five human evaluators. \cite{zhu2024defining}, on the other hand, employed a generative adversarial network to synthesize 20 images for each age group: children, young adults, middle-aged adults, and elderly individuals.
        
        Several studies focused on populations with specific medical or psychological conditions. For instance, \cite{huang2024auto} introduced the Parkinson's disease facial expression dataset, which comprises RGB images of the six basic expressions from 95 patients with Parkinson’s disease. \cite{fan2022fer-pcvt} constructed a dataset of 1,302 videos from stroke patients aged 18-85, capturing not only basic expressions (happiness, sadness, surprise, and anger) but also four condition-specific expressions: painful, strained, tired, and neutral. Additionally, this dataset included facial action coding system (FACS) labels, making it valuable for detailed facial action unit analysis. \cite{sharma2020audio-visual} gathered 140 videos from 70 elderly participants aged 65 and above, 28 of whom were identified as apathetic, enabling the study of apathy recognition in older adults.

        To address the issue of imbalanced data, several techniques are available, such as rebalancing through undersampling or oversampling and employing weighted loss functions. However, despite many studies using imbalanced datasets, such as AffectNet and LifeSpan, the majority either did not apply these methods or did not mention their use. Among the studies that explicitly addressed the imbalance, four utilized undersampling techniques \citep{sreevidya2022elder,wu2015enhanced,huang2024facial,fahn2022image}, while three opted to remove minority classes altogether \citep{yang2018joint,petrou2023lightweight,anand2023multi-label}.
    
    \subsubsection{RQ1.3. Data characteristics}        

        Seven studies leveraged the temporal dimension of visual data to perform facial expression recognition, while the remaining studies were limited to using static images. Most video-based solutions adopted a common approach: extracting features from individual frames and employing an RNN to capture temporal information. For instance, \cite{zhang2020facial} utilized the VGG16 network for feature extraction, followed by a long short-term memory (LSTM) network to process sequential data. Similarly, \cite{skibelirokkones2019facial} employed local directional strength patterns as frame descriptors, which were then fed into an RNN. \cite{bi2022dynamic} adopted a comparable methodology, using GoogleNet for feature extraction, applying principal component analysis to reduce data dimensionality, and processing the resulting data with a Bayesian probabilistic framework. In the same line, \cite{sreevidya2022elder} extracted features from multiple video frames using a convolutional neural network and utilizing an LSTM to handle the temporal aspects. 
        
        Other studies explored variations of this approach to optimize performance. For example, \cite{sharma2020audio-visual} combined visual features extracted by VGG16 with action units and facial landmarks obtained via OpenPose 2.0, processing the sequential data with a gated recurrent unit (GRU). While most studies relied on RNNs to model sequential information, \cite{anand2023multi-label} took a different route by employing a multimodal Transformer architecture \citep{Transformer} that integrated visual data, audio, and text. \cite{nadjadecarolis2024exploring}, despite performing FER on still images, incorporated temporal information by calculating the frequency of different expressions over a defined time period. They determined the final expression by selecting the most frequently occurring one, giving greater weight to the most recent frames.

        Only four studies opted for multimodal solutions, integrating non-visual data into the facial expression recognition pipeline. These approaches demonstrated the potential of combining modalities to enhance predictive performance. 
        
        \cite{sharma2020audio-visual} investigated apathy detection from a multimodal perspective. For the audio modality, they extracted features using the Geneva minimalistic acoustic parameter set, which includes 18 low-level descriptors based on frequency, energy, and spectral parameters. These features were processed through fully connected layers to learn meaningful representations. The audio features were then concatenated with visual features, action units, and facial landmarks before being passed through additional fully connected layers. Comparing their multimodal model with uni-modal approaches, they observed significant improvements in prediction accuracy when leveraging multiple modalities.
        
        \cite{sreevidya2022elder} proposed two distinct methods for processing audio signals. The first approach used a one-dimensional representation, extracting features such as prosody, spectral coefficients, and voice quality characteristics (e.g., tenseness and creakiness) and feeding them into a 1-D convolutional neural network. The second approach converted the audio signals into 2-D spectrograms, which were processed using the Inception-V2 \citep{szegedy2016rethinking} network. For the visual modality, a separate CNN was employed to compute FER. The final multimodal model fused intermediate layers from both modalities, with the fusion layers determined through a grid-based search. Results showed that combining features from both audio and visual data significantly improved overall performance.
        
        \cite{anand2023multi-label} introduced text as a third modality in their FER pipeline, proposing a multimodal Transformer network. The architecture consisted of modality-specific peer networks that independently learned discriminative features from each modality. These features were then fused by minimizing the Kullback-Leibler divergence between peer networks. This design enabled the network to learn mode-specific patterns while effectively leveraging multiple modalities simultaneously, leading to remarkable improvements in performance, particularly in cross-dataset settings.
        
        \cite{nadjadecarolis2024exploring} focused on integrating vision and text modalities. For the visual modality, they trained an EfficientNet model \citep{tan2020efficientnet} to process RGB images. For the text modality, they utilized GPT-4.0 \citep{openai2024gpt-4} to predict emotions from textual inputs. A late-fusion strategy was employed for the final prediction, using the confidence scores from both modalities. 
        
    \subsubsection{RQ1.4. Facial landmarks and action units for FER}
    \label{section:landmarks}
        % \begin{itemize}
        %     \item Only three use them for FER.
        %     \item List studies using landmarks and AUs and how.
        % \end{itemize}
        
        Facial landmarks and action units (AUs) are two fundamental tools frequently employed for facial expression recognition. Facial landmarks are visually identifiable points on the face, such as specific regions around the eyes, nose, and mouth, which help characterize facial geometry. AUs, on the other hand, were introduced in the facial action coding system \citep{ekman1978facial} to describe 46 basic muscle movements, providing a more granular understanding of facial expressions. These features serve as inputs for various computational approaches to facial expression recognition.  
        
        One notable application of landmarks and AUs is in the multimodal apathy detection solution by \cite{sharma2020audio-visual}. They integrated landmarks and AUs as features into a multimodal model alongside visual and audio data. They computed these features using the OpenFace 2.0 toolkit \citep{baltrusaitis2018openface}, and demonstrated that combining multiple modalities significantly enhanced system performance. In contrast, \cite{caroppo2020comparison} extracted geometrical features--linear, elliptical, and polygonal--from facial landmarks estimated using an improved version of active shape models \citep{milborrow2014active}. These features were then used to train various machine learning models. However, their results showed that the performance of geometrical landmark-based features lagged behind that of DL-based features, such as those computed by VGG16 \citep{simonyan2015very}. A different approach was adopted by \cite{wu2015enhanced}, who utilized manually annotated fiducial points provided by \cite{guo2013facial} to train a Bayesian network. By incorporating age labels during the training phase, Wu et al. demonstrated significant performance improvements, highlighting the potential of integrating demographic features like age into the FER task.
        
        On the other hand, some studies computed facial landmarks and AUs but did not use them directly for facial expression recognition. For instance, \cite{gaya-morey2024ai-powered}, \cite{zhu2024defining}, and \cite{chen2023emotion-reading} incorporated these features for other purposes, such as system development and integration, but refrained from leveraging them for FER tasks specifically.

    \subsubsection{RQ1.5. Other tasks}
        % \begin{itemize}
        %     \item List tasks that are computed alongside FER in the studies.
        % \end{itemize}
        
        Apart from the facial expression recognition task, which is the central focus of this review, several other tasks were computed alongside it in multiple studies.  
        
        Face detection emerged as the most common preprocessing step in the studies reviewed. This step involves cropping the image to focus on the face and removing irrelevant data for the FER task. Several approaches were used for face detection, with the Viola-Jones technique \citep{viola2001rapid} being the most prevalent due to its simplicity, high speed, and low computational requirements. Another common preprocessing task was facial landmark estimation, which is further addressed in Section \ref{section:landmarks}.  
         
        Age estimation was frequently employed to address facial expression differences among age groups. For example, \cite{caroppo2020comparison} automatically labeled images from the CIFE and FER-2013 datasets using landmark-based methods \citep{wu2012age} to study FER accuracy across different age groups. Similarly, \cite{zhu2024defining} utilized FaceReader \citep{noldus} to classify images into age groups and analyze performance variations. \cite{huang2024facial} not only used age estimation with ResNet18 \citep{he2016deep} for evaluation purposes but also integrated it into their FER system to compute age group-specific features. In contrast, \cite{yang2018joint} adopted a multi-tasking approach, simultaneously predicting both age and facial expressions to improve accuracy for both tasks. Because age estimation is included as a means to studying aging effects on facial expression recognition, these works are analyzed in more depth in Section \ref{section:aging-effect}
        
        FER was also employed as a tool for detecting cognitive impairment (CI), which is often characterized by abnormal emotional patterns. \cite{fei2022novel} used the occurrence of facial expressions in consecutive frames as input to an SVM, enabling CI detection in elderly individuals. \cite{jiang2022automated} adopted a similar approach, utilizing the VGG19 network \citep{simonyan2015very} to predict facial expressions. These predictions were then fed into a linear regression model and an SVM for CI detection. They found that CI participants displayed significantly fewer positive emotions, more negative emotions, and higher facial expressiveness. However, attempts to identify CI subtypes using the same features were unsuccessful. Apathy detection, closely related to FER, was also explored by \cite{sharma2020audio-visual}, who proposed a multimodal solution that combined RGB data, action units, and audio. Their results demonstrated that using multiple modalities improved performance compared to relying on a single modality.  
        
        Several studies integrated FER with additional tasks to create multifunctional systems. \cite{yan2020deep} combined FER with facial recognition and fall detection. For facial recognition, they used MobileNetV2 \citep{sandler2018mobilenetv2}, while fall detection was achieved by estimating body pose using OpenPose \citep{cao2021openpose} and analyzing shoulder and chest coordinates. This integration aimed to detect and respond to different warning situations, like falls and negative expressions prolonged in time. Similarly, \cite{fahn2022image} performed fall detection using body pose estimation and incorporated gaze, facial expression, and speech recognition into a social robot. \cite{nadjadecarolis2024exploring} extended FER by including emotion recognition from text and dialogue generation using GPT-4.0. These tasks, paired with FER, endowed a social robot with empathy capabilities. A user study involving 30 elderly participants revealed that the robot's empathic features improved usability and provided more positive user experiences. \cite{gaya-morey2024ai-powered} took this further by including eight tasks in their system: FER, face and person detection, age and gender estimation, facial recognition, facial landmark estimation, and background subtraction. They also provided multiple options for each task to accommodate different use cases. Their objective was to create a versatile, AI-powered module for any socially interactive agent.
    
    \subsection{RQ2. Successful deployment in real-world environments}

        We now present the findings related to the second research question, emphasizing framework integration, deployment, the impact of age on FER, privacy considerations, economic costs, and the use of explainable AI techniques.

    \subsubsection{RQ2.1. Deployment}

        % \begin{itemize}
        %     \item List studies deploying the system in a real-case scenario or implementing a framework.
        % \end{itemize}
    
        The deployment of FER systems in real-world scenarios, such as care systems, presents multiple challenges, including privacy concerns and the need for seamless integration with other tools. Several studies have addressed these issues, offering innovative approaches to improve the functionality and user experience of FER systems.
    
        \cite{chen2023emotion-reading} proposed an adaptive nursing care environment that adjusts environmental factors--such as music, lighting, temperature, and scent--based on the detected emotions of users. This system aims to transform negative emotions into positive ones, for example, guiding anger and fear toward calmness, and disgust and sadness toward happiness. By personalizing environmental settings, they highlighted how FER systems can actively enhance users' well-being and emotional state.
    
        Building on the theme of user interaction, \cite{nadjadecarolis2024exploring} focused on the role of empathy in human-robot interactions for elderly care. Their system recognized emotions using both visual data and text and enabled the robot to express emotions through speech and facial expressions displayed on a screen. GPT 4.0 \citep{openai2024gpt-4} was employed for emotional state analysis and dialogue generation. A user study involving 30 elderly participants found that the empathic version of the robot was more user-friendly and provided a better overall experience compared to a non-empathic version.
        
        Similarly, \cite{yan2020deep} developed a multi-functional system for clinical monitoring, combining facial expression, gesture, and face recognition to aid in patient treatment. This system can alert caregivers of critical situations, such as falls, the presence of unknown individuals, or prolonged negative emotions (e.g., sadness, anger, fear, or disgust).
        
        \cite{gaya-morey2024ai-powered} expanded the scope of FER applications by integrating multiple computer vision tasks into a single module designed for socially interactive agents. Their system supports a wide range of tasks, including facial expression recognition, facial recognition, facial landmark estimation, face and person detection, age and gender estimation, and background subtraction. Additionally, they incorporated explainable AI tools to enhance transparency and user trust.
        
        Lastly, \cite{fahn2022image} explored the integration of FER with complementary modalities, including gaze, speech, and pose recognition, within a social robot. While their work provided detailed descriptions of each task, it lacked specifics regarding their integration into the robot and did not include a user study.
    
    \subsubsection{RQ2.2. Effects of age on facial expression recognition}
    \label{section:aging-effect}
        % \begin{itemize}
        %     \item List studies directly facing (mentioning) the aging effect.
        %     \item List studies including elderly users in the training. Relate with dataset list.
        % \end{itemize}
        
        The age of users' faces significantly influences both human and automated methods for decoding facial expressions, as highlighted in numerous studies \citep{fölster2014facial, ko2021changes, battinisonmez2019computational}. Consequently, various works have introduced specific attention mechanisms to address the challenges posed by age differences in facial expression recognition.
        
        One of the earliest efforts to incorporate age into FER models was undertaken by \cite{wu2015enhanced}, who proposed a three-node Bayesian network that integrated age information during training. Utilizing the FACES and LifeSpan datasets, which include both expression and age labels, they evaluated the model's performance through within-group and between-group assessments. Their findings revealed a substantial bias in all between-group cases, underscoring the importance of age-specific modeling. Notably, incorporating age information during training improved recognition accuracy across most expressions, except for happiness.  
        
        Building on this concept, \cite{yang2018joint} adopted a multi-task learning framework to simultaneously predict age and expressions. Their approach, which leveraged two feature-extracting sub-networks (a convolutional neural network and a scatter network), allowed for shared feature representations across tasks. The model demonstrated superior performance on the FACES and LifeSpan datasets, outperforming earlier single-task solutions in both age and expression predictions.  
        
        Another innovative approach came from \cite{huang2024facial}, who integrated an age group classifier directly into the recognition pipeline. This allowed their model to specialize in age-specific features for distinct groups: babies (0–3 years), adolescents (4–19), young adults (20–39), middle-aged adults (40–69), and elderly adults (70+). Tested on the RAF-DB, AffectNet, and FACES datasets, their method achieved significant performance gains, particularly in recognizing facial expressions in the elderly population.  
        
        Some works have focused on evaluating and addressing age-related biases in commercial FER systems. For example, \cite{kim2021age} benchmarked four commercial systems using the FACES dataset across three age groups: young, middle-aged, and older adults. They found that these systems consistently performed best on younger individuals and worst on older users. Moreover, specific expression classes, such as anger and neutrality, exhibited significantly lower positive predictive values for older individuals, highlighting critical shortcomings in current FER technologies. \cite{zhu2024defining} also explored the performance of a commercial system, i.e., FaceReader \citep{noldus}. They used a GAN to generate 80 images distributed across four age groups: children, young adults, middle-aged adults, and elderly individuals. To validate the system, they conducted a survey with 496 participants via Amazon Mechanical Turk. While the system achieved strong agreement with survey responses for young, middle-aged, and elderly images (Cohen's kappa $>0.8$), it performed poorly for children's images (kappa = 0.4), suggesting unique challenges in recognizing expressions in younger faces.  
        
        A more targeted exploration of age-specific datasets was conducted by \cite{rahatuljannat2021expression}, who focused on children and elderly populations using the EmoReact and ElderReact datasets. By employing a siamese neural network trained on pairs of images (positive pairs with the same expression and negative pairs with different expressions), they achieved strong within-dataset performance. However, cross-dataset testing revealed interesting dynamics: training on ElderReact and testing on EmoReact led to decreased performance, while training on EmoReact and testing on ElderReact resulted in an unexpected improvement.  
        
        Finally, \cite{caroppo2020comparison} offered a broader perspective by studying traditional machine learning and deep learning methods across four datasets: FACES, LifeSpan, FER-2013, and CIFE. They analyzed performance across four age groups: young (18–29 years), middle-aged (30–49 years), old (50–69 years), and very old (70–93 years). Since FER-2013 and CIFE lacked age labels, they computed them automatically through landmark-based methods \citep{wu2012age} to conduct their evaluations. Among the models tested, a combination of the VGG16 deep architecture with random forest yielded the best results, particularly for aging adults. However, cross-dataset evaluations revealed notable performance drops, especially for younger individuals, with per-class results varying significantly by dataset. 
    
    % \subsubsection{Data capture devices}
        % \begin{itemize}
        %     \item It was not specified in any study. Maybe this question can be removed. 
        % \end{itemize}
    
    \subsubsection{RQ2.3. User privacy protection}
        % \begin{itemize}
        %     \item Figure showing proportion of studies with and without privacy concerns.
        %     \item Describe the three main strategies: depth-only, RGB with privacy protection, and RGB with landmarks.
        %     \item List and explain all studies in the text.
        % \end{itemize}
        
        Privacy preservation is a critical concern in multiple fields, especially in healthcare applications, and particularly when visual data is involved. Multiple reviewed studies addressed this issue through diverse strategies aimed at protecting user identities while maintaining the effectiveness of facial expression recognition systems.
        
        \cite{huang2024auto} tackled privacy concerns by employing a Star Generative Adversarial Network (StarGAN) to generate synthetic facial expression images of Parkinson’s disease patients. This technique preserved the patients’ identities while retaining essential expression data, ensuring privacy without compromising system performance. Another effective strategy involved the use of depth images instead of RGB data. Uddin et al. \citeyearpar{ziauddin2017facial2,ziauddin2017facial1} demonstrated the feasibility of FER with depth images, which inherently obfuscate user identities. They employed various feature descriptors, such as local directional position patterns and modified local directional patterns, to achieve robust FER performance.
        
        Several studies leveraged facial landmarks or action units to enhance FER while reducing reliance on raw RGB data. For instance, \cite{sharma2020audio-visual}, \cite{caroppo2020comparison}, and \cite{wu2015enhanced} utilized these intermediate features, which allowed anonymization of the data after the initial RGB image processing stage. This approach offers a balance between accuracy and privacy by removing identifiable information in later stages of processing.
        
        \cite{nadjadecarolis2024exploring} adopted a different approach, embedding their FER system into a small robot. Their privacy-preservation efforts focused on local processing, ensuring that all computations occurred within the robot itself, thereby eliminating the need for transmitting user data over the Internet.
    
    \subsubsection{RQ2.4. Economic cost}
        % \begin{itemize}
        %     \item Say how many studies included economic concerns, tried to lower hardware requirements, etc.
        % \end{itemize}
    
        The economic cost is a common concern when deploying a system, and it can be addressed in various ways, such as reducing hardware requirements or utilizing low-cost equipment. We identified eight studies that mentioned economic cost in some capacity.
        
        \cite{nadjadecarolis2024exploring} conducted their experiment using the Ubtech Alpha Mini robot, chosen for its affordability. To further reduce costs, they selected the EfficientNet model \citep{tan2020efficientnet} for the FER task, which helped lower hardware requirements while increasing inference speed. Similarly, \cite{petrou2023lightweight} used the mini-Xception model for FER, notable for requiring only 58,000 parameters and having a size smaller than 1 MB, making it ideal for low-specification environments. \cite{yan2020deep} also leveraged cost-effective models, utilizing mini-Xception for FER, SqueezeNet \citep{niandola2016squeezenet} for face detection, and MobileNetV2 \citep{sandler2018mobilenetv2} for face recognition. These models were deployed on a Raspberry Pi with a Pi Camera, both highly affordable components. \cite{fei2022novel} and \cite{fahn2022image} adopted similar strategies to reduce hardware needs by employing compact models. Fei et al. utilized MobileNetV2, while Fahn et al. developed a small custom CNN for FER, showcasing how smaller models can effectively address economic constraints.
        
        \cite{gaya-morey2024ai-powered} approached economic cost differently, offering a framework with multiple options for each task, accommodating a wide range of hardware requirements. This flexibility allows their framework to adapt to various environments and budgets. Additionally, their client-server scheme significantly reduces the hardware demands on the robot, as computationally intensive tasks are handled by an external computer.
        
        Lastly, Caroppo et al. \citeyearpar{caroppo2018facial,caroppo2020comparison} highlighted in their future work plans to integrate their system into a cost-effective application, further emphasizing the importance of affordability in FER system design.
    
    \subsubsection{RQ2.5. Explainable AI techniques}
        % \begin{itemize}
        %     \item List the XAI methods used: LIME, Grad-CAM, and clustering visualization.
        %     \item Explain the purpose of their utilization: find system faults, increase user trust, etc.
        % \end{itemize}
    
        As noted in the work by \cite{barredoarrieta2020explainable}, the search for explainable AI models is driven by goals such as trustworthiness, causality among data variables, transferability, informativeness for decision-making, confidence, and fairness, among others. Among relevant studies, we identified six that address these goals.
    
        \cite{anand2023multi-label} developed a multimodal solution to FER, integrating text, audio, and visual data, and providing explanations separately for each modality. They employed local interpretable model-agnostic explanations (LIME) \citep{ribeiro2016why} for the text and vision modalities, and its audio counterpart, audioLIME \citep{haunschmid2020audiolime}, for audio. LIME explains the predictions of any classifier by learning an interpretable model locally around a prediction. The explanations consisted of visual plots (video frames and spectrograms) where the most relevant features for correct predictions were highlighted in green. In the visual modality, these features primarily corresponded to the face and arms. \cite{gaya-morey2024ai-powered} also adopted model-agnostic XAI methods for integration into their general-purpose computer vision module designed for socially interactive agents. They incorporated three approaches: LIME (previously mentioned), randomized input sampling for explanation of black-box models (RISE) \citep{petsiuk2018rise}, and Shapley additive explanations (SHAP) \citep{lundberg2017unified}. Being model-agnostic, these methods are versatile and can be applied to any technique used for any task. The explanations they generate share a common format: visual plots that highlight the importance of different image regions using color gradients.
    
        In contrast to model-agnostic methods, \cite{fan2022fer-pcvt} adopted model-specific XAI techniques tailored to the architectures they evaluated. Their study introduced a patch-convolutional vision transformer (FER-PCVT) for FER, which they compared against ResNet18 \citep{he2016deep} and Vision Transformer (ViT) \citep{dosovitskiy2021image}. To explain ResNet18, they used Gradient-weighted Class Activation Mapping (Grad-CAM) \citep{selvaraju2017grad}, a method specifically designed for convolutional neural networks commonly used in computer vision tasks. Grad-CAM uses the gradients flowing into the final convolutional layer to produce a coarse localization map that highlights important regions in the image for a target prediction. For transformer-based models, such as ViT and FER-PCVT, they aggregated attention weights across all layers. All explanations were visualized as heat maps, with hot regions corresponding to the most relevant areas for a specific prediction. FER-PCVT demonstrated the best results, extracting more specific facial features, while ViT attributed importance to data on the periphery of the image for certain classes, and ResNet18 focused on more extensive facial regions.
    
        Some studies also analyzed the clustering ability of the computed features in the last layer before classification. Although different from the aforementioned XAI methods, these approaches provide insights into the models' inner workings by examining how well the models separate classes and display the perceived similarities between them. In this context, \cite{fan2022fer-pcvt} used t-distributed stochastic neighbor embedding to visualize the last layer of the model by mapping each data point to a two-dimensional space. Similarly, Uddin et al. explored various visual descriptors such as local directional strength patterns, local directional position patterns, and modified local directional patterns in several works \citeyearpar{ziauddin2017facial2,ziauddin2017facial1,skibelirokkones2019facial}, employing either local discriminant analysis or generalized discriminant analysis to get 3D plots where classes were clearly grouped into distinct clusters.

\section{Discussion}

    % Strengths, weaknesses, recommendations for future studies...
    In this section, we discuss the major findings of the systematic literature review, and analyze strengths and weaknesses of the found approaches for facial expression recognition on the elderly, as well as their main characteristics, attending to common aspects to take into account for deployment in real scenarios. Additionally, we give recommendations for future works, based on the findings.

    % Architectures, classes
    Convolutional architectures remain the most frequently utilized for vision-based data, as evidenced by the reviewed studies. Popular models such as VGG, Inception, and ResNet have proven to be robust feature extractors \citep{ekosantoso2022facial,bi2022dynamic}. Recurrent architectures, on the other hand, are predominantly employed when temporal information is crucial for recognition tasks \citep{skibelirokkones2019facial,zhang2020facial}. Meanwhile, Transformer architectures are gaining traction for both visual and sequential data \citep{fan2022fer-pcvt,anand2023multi-label}. For resource-constrained environments, lightweight models such as MobileNet, EfficientNet, and mini-Xception, designed for efficient deployment on low-power devices, have emerged as leading solutions \citep{fei2022novel,nadjadecarolis2024exploring,yan2020deep}.

    % Datasets and problems with imbalanced datasets
    Deep learning models thrive on data-driven learning, but they are inherently susceptible to biases in the training datasets. Three widely used datasets--FER-2013, CK+, and AffectNet--lack adequate representation of elderly individuals and do not include age annotations. This is a notable limitation given the demonstrated differences in facial expressions across age groups (see Section \ref{section:aging-effect}). In contrast, datasets such as FACES and LifeSpan provide age annotations, making them valuable for improving prediction accuracy for elderly populations \citep{caroppo2020comparison,wu2015enhanced}. Similarly, ElderReact, while lacking explicit age annotations, contains a large volume of videos featuring elderly individuals. Class imbalance is another significant challenge in FER datasets, exemplified by AffectNet (with over a 100,000-sample difference between the happiness and disgust classes) and LifeSpan (580 samples for the neutral class versus only 7 for disgust). However, many studies lack information on how they addressed this imbalance \citep{jiang2022automated,nadjadecarolis2024exploring,caroppo2018facial,caroppo2020comparison}. When tackled, class imbalance was typically addressed by undersampling \citep{sreevidya2022elder,wu2015enhanced,huang2024facial,fahn2022image} or by removing minority classes entirely \citep{anand2023multi-label,petrou2023lightweight,yang2018joint}. While these methods can help in solving the imbalance problem, they reduce dataset diversity. Alternative approaches, such as oversampling techniques \citep{bach2017study,mohammed2020machine} or class-weighted loss functions \citep{cui2019class-balanced}, are preferable as they maintain dataset size and variability. Additionally, metrics like accuracy should be avoided for evaluating imbalanced data, as they fail to reflect true model performance across classes. Instead, metrics like the F1 score, which balances precision and recall for each class, are more appropriate \citep{branco2016survey}.

    % Data characteristics: image vs. video and multiple modalities
    The literature reveals no consensus on whether static images suffice for FER or whether motion information is essential. For instance, while \cite{alves2013recognition} argued that dynamic expressions elicit stronger facial and physiological responses in observers, Gold et al. \citeyearpar{gold2013efficiency} found no additional benefit in using dynamic data for emotion recognition beyond what a single static snapshot provides. Nonetheless, static images dominate the analyzed studies, likely due to the scarcity of large-scale video datasets. Developing video datasets for FER could not only provide new insights into the importance of dynamic expressions but also allow direct comparisons with static datasets. Similarly, multimodal solutions combining visual data with other modalities, such as audio, have demonstrated greater robustness to errors in a single modality. For instance, studies leveraging the ElderReact dataset showed that combining visual and audio data outperformed single-modality solutions \citep{sreevidya2022elder,anand2023multi-label}.

    % Other tasks and facial landmarks
    Facial expression recognition can be combined with other tasks, as demonstrated in various studies, such as facial recognition \citep{yan2020deep}, fall detection \citep{fahn2022image}, and speech recognition \citep{nadjadecarolis2024exploring}. These combinations can be particularly valuable in healthcare environments, enabling systems to automatically identify users, detect potentially hazardous situations, and provide tools to mitigate loneliness. Integrating these tasks is advantageous when developing smart care systems, as they can operate using the same input data (e.g., RGB video from a camera) and share similar resource requirements. Additionally, tasks such as face detection and age estimation have been primarily utilized as preprocessing steps to enhance the performance of expression recognition systems \citep{huang2024facial,sreevidya2022elder}.

    % Aging effects, deployment
    Given the impact of aging on FER, as discussed in Section \ref{section:aging-effect}, these studies incorporate training with datasets that include elderly individuals. Some go further by improving results through approaches such as incorporating user age into training \citep{huang2024facial,wu2015enhanced} or adopting multi-tasking frameworks \citep{yang2018joint}. However, we believe these approaches primarily address the scarcity of data representing diverse age groups, which often results in underrepresentation of elderly users. This highlights the critical need for more comprehensive datasets tailored to this demographic or the integration of multiple datasets that include elderly people. Moreover, only a few studies were found to integrate the developed methods into real-world applications, such as mobile applications, assisted living devices, and healthcare centers \citep{nadjadecarolis2024exploring,chen2023emotion-reading}. Since the ultimate goal of these studies is societal adoption and practical deployment, we argue that further research is required to understand elder users’ acceptance and trust in these technologies. Additionally, investigations into their potential benefits, risks of failure, and other real-world considerations are essential for ensuring successful implementation.

    % Privacy and economic cost
    As highlighted by \cite{little2009pervasive}, the proliferation of technology raises critical concerns about usability, trust, and privacy for elderly users. Notably, most reviewed studies (22 in total) relied on RGB data with identifiable user information, yet very few incorporated privacy-preserving measures. Techniques such as anonymization through facial landmark estimation or action units \citep{sharma2020audio-visual} and privacy-preserving transformations \citep{huang2024auto} should be more widely adopted. Local processing is another recommended strategy to reduce privacy risks associated with external data exchange \citep{nadjadecarolis2024exploring}. Economic considerations were also prevalent, with many studies focusing on reducing hardware requirements \citep{yan2020deep,fahn2022image,nadjadecarolis2024exploring}. Combining privacy-preserving techniques with lightweight, local solutions could simultaneously address privacy and cost concerns, making the systems more accessible.

    % XAI
    A notable gap in the reviewed studies is the limited use of explainable AI techniques. XAI is crucial for justifying model predictions and fostering user trust, particularly with complex and opaque models \citep{barredoarrieta2020explainable}. While some studies analyzed the clustering ability of features \citep{skibelirokkones2019facial,ziauddin2017facial2}, this approach only measures classification effectiveness and neglects input feature contributions. Advanced XAI methods, such as LIME \citep{ribeiro2016why} or Grad-CAM \citep{selvaraju2017grad}, were employed in a few studies \citep{fan2022fer-pcvt,gaya-morey2024ai-powered,anand2023multi-label} and should be more widely adopted for future FER research.

\section{Conclusion}

    This systematic review has provided a comprehensive analysis of deep learning-based facial expression recognition systems with a focus on elderly populations. Our findings reveal significant progress in leveraging deep learning techniques, particularly convolutional neural networks, for vision-based FER tasks. Lightweight architectures, such as MobileNet and mini-Xception, have demonstrated potential for deployment in resource-constrained environments, while emerging approaches like Transformer-based models and multimodal frameworks offer promising avenues for enhancing robustness and accuracy. Despite these advances, several critical challenges remain.

    A notable limitation across studies is the underrepresentation of elderly individuals in FER datasets. The most widely used data sets, such as FER-2013 and AffectNet, lack sufficient diversity in age representation and fail to account for age-related facial changes that impact expression recognition. This highlights the urgent need to develop comprehensive and age-inclusive datasets or to merge existing ones into larger and more diverse datasets to improve FER performance across diverse age groups. Addressing class imbalance, another prevalent issue, will require broader adoption of more advanced rebalancing techniques, than simple undersampling and minority class removal, as found.
    
    The real-world deployment of FER systems for the elderly remains insufficiently explored. Privacy concerns, usability challenges, and the lack of explainable AI adoption hinder the integration of these technologies into healthcare and assisted living applications, which are of high stake for elderly users. Incorporating privacy-preserving measures, such as anonymized facial data or local processing, alongside lightweight models, could address privacy concerns and improve accessibility (as age typically increases disabilities). Furthermore, XAI techniques must be implemented more widely to foster user trust and transparency, allowing caregivers and medical professionals to better understand what is detected by the system and the predictions that are forecasted.
    
    Finally, while FER systems have immense potential to improve elderly care, additional research is needed to evaluate their societal acceptance, trustworthiness, and their impact on elder users when they are deployed. Future work should prioritize interdisciplinary efforts that combine technical innovation with human-centered design to ensure that these technologies are effective, ethical, and accessible. By addressing these challenges, FER systems can play a transformative role in enhancing the emotional well-being and quality of life of the aging population.

\section*{CRediT authorship contribution statement}
    \textbf{F. Xavier Gaya-Morey:} Conceptualization, Methodology, Validation, Investigation, Writing - Original Draft, Writing - Review \& Editing Preparation, Visualization. \textbf{Jose M. Buades-Rubio:} Conceptualization, Methodology, Writing - Review \& Editing Preparation, Supervision, Project administration, Funding acquisition. \textbf{Philippe Palanque:} Writing - Review \& Editing Preparation, Supervision. \textbf{Raquel Lacuesta:} Writing - Review \& Editing Preparation, Supervision. \textbf{Cristina Manresa-Yee:} Conceptualization, Methodology, Writing - Review \& Editing Preparation, Supervision, Project administration, Funding acquisition.

\section*{Funding}
   This work is part of the Project PID2023-149079OB-I00 (EXPLAINME) funded by MICIU/AEI/10.13039/ 501100011033/ and FEDER, EU and of Project PID2022-136779OB-C32 (PLEISAR) funded by MICIU/ AEI /10.13039/501100011033/ and FEDER, EU. F. X. Gaya-Morey was supported by an FPU scholarship from the Ministry of European Funds, University and Culture of the Government of the Balearic Islands.
    
\section*{Declaration of Competing Interest}
    The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.
   
\section*{Data availability}
    All data relevant to the study are included in this document. No additional data were generated.

\section*{Declaration of generative AI and AI-assisted technologies in the writing process}
    During the preparation of this work the authors used ChatGPT in order to improve the readability and language of the manuscript. After using this tool, the authors reviewed and edited the content as needed and take full responsibility for the content of the published article.
    
%% If you have bib database file and want bibtex to generate the
%% bibitems, please use
%%
 \bibliographystyle{elsarticle-harv} 
 \bibliography{bibliography}

%% else use the following coding to input the bibitems directly in the
%% TeX file.

%% Refer following link for more details about bibliography and citations.
%% https://en.wikibooks.org/wiki/LaTeX/Bibliography_Management

% \begin{thebibliography}{00}

%% For authoryear reference style
% \bibitem[Author(year)]{label}
%% Text of bibliographic item

% \bibitem[Lamport(1994)]{lamport94}
%   Leslie Lamport,
%   \textit{\LaTeX: a document preparation system},
%   Addison Wesley, Massachusetts,
%   2nd edition,
%   1994.

% \end{thebibliography}
\end{document}

\endinput
%%
%% End of file `elsarticle-template-harv.tex'.


