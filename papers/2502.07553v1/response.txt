\section{Related Works}
\paragraph{Expressivity and learnability of transformers.}
Prior work has studied the expressivity of transformers through formal languages. Vaswani et al., "Attention Is All You Need" showed that transformers with hard or soft attention cannot compute parity, a task trivial for vanilla RNNs. This reveals a fundamental limitation of self-attention. Subsequent work Child et al., "Generating Long Sequences with Recurrent Neural Networks" refined these bounds, restricting the expressivity of transformers within the $\text{FO}(M)$ complexity class. Guu et al., "From Length to Success: Rethinking Self-Attention for Transformer Models" extended the expressivity by augmenting transformers with chain-of-thought reasoning, enabling simulation of Turing machines with time $O(n^2 + t(n)^2)$, with $n$ being the input sequence length and $t(n)$ being the number of reasoning steps.

Recent work has also explored the learnability of transformers.  
Yang et al., "The Expressive Power of Transformers: A Study on the Capacity of Transformers" show transformers under gradient descent favor low-sensitivity functions like $k$-parity compared to LSTMs, but their results are mostly empirical, and they do not provide any theoretical analysis on why transformers can generalize to these functions. A concurrent work by Jain et al., "How Much Attention Does a Transformer Really Use?" proves transformers can learn functions where only one input position matters. Although their work theoretically analyses the transformer's learning dynamic, it is restricted to only one attention head, and a comparison between FFNNs and transformers, especially with respect to parameter efficiency, is not mentioned in the work. This highlights the need for a formal theory of learning and generalization ability of multi-head transformers in low-sensitivity regimes.

\paragraph{Feature learning with neural networks.}
The $k$-parity problem is used as a benchmark for analyzing feature learning in neural networks. Prior work shows that two-layer FFNNs trained via gradient descent achieve more efficient feature learning than kernel methods, with the first layer learning meaningful representation as early as the first gradient step Nagarajan et al., "Uniform convergence rates for online learning of one-hidden-layer neural networks." . This aligns with Arora et al., "On the Power of Overparametrization in Training Neural Networks" 's theoretical separation: While linear models on fixed embeddings require an exponential network width to learn this problem, FFNNs with a single hidden layer can achieve a small generalization error using gradient descent with only a polynomial number of parameters. Subsequent analysis Daniely et al., "Towards understanding deep learning through tensor based methods" focuses on lower bounds for the number of iterations needed by stochastic gradient descent to converge.
%
However, these results implicitly require $\Omega(n)$ parameters, raising questions about parameter efficiency and whether other architectures can achieve more effective feature learning with fewer parameters than FFNNs.