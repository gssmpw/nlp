[
  {
    "index": 0,
    "papers": [
      {
        "key": "self_attention_limit",
        "author": "Michael Hahn",
        "title": "Theoretical Limitations of Self-Attention in Neural Sequence Models"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "circuit",
        "author": "Yiding Hao and\nDana Angluin and\nRobert Frank",
        "title": "Formal Language Recognition by Hard Attention Transformers: Perspectives\nfrom Circuit Complexity"
      },
      {
        "key": "log-precision_trans",
        "author": "William Merrill and\nAshish Sabharwal",
        "title": "A Logic for Expressing Log-Precision Transformers"
      },
      {
        "key": "power_hard_attn",
        "author": "Pascal Bergstr{\\\"{a}}{\\ss}er and\nChris K{\\\"{o}}cher and\nAnthony Widjaja Lin and\nGeorg Zetzsche",
        "title": "The Power of Hard Attention Transformers on Data Sequences: {A} Formal\nLanguage Theoretic Perspective"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "cot_express",
        "author": "William Merrill and\nAshish Sabharwal",
        "title": "The Expressive Power of Transformers with Chain of Thought"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "simplicity_bias",
        "author": "Satwik Bhattamishra and\nArkil Patel and\nVarun Kanade and\nPhil Blunsom",
        "title": "Simplicity Bias in Transformers and their Ability to Learn Sparse\nBoolean Functions"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "single-location-attention",
        "author": "Pierre Marion and\nRapha{\\\"{e}}l Berthier and\nG{\\'{e}}rard Biau and\nClaire Boyer",
        "title": "Attention layers provably solve single-location regression"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "feature_learning_2",
        "author": "Jimmy Ba and\nMurat A. Erdogdu and\nTaiji Suzuki and\nZhichao Wang and\nDenny Wu and\nGreg Yang",
        "title": "High-dimensional Asymptotics of Feature Learning: How One Gradient\nStep Improves the Representation"
      },
      {
        "key": "feature_learning_1",
        "author": "Zhenmei Shi and\nJunyi Wei and\nYingyu Liang",
        "title": "Provable Guarantees for Neural Networks via Gradient Feature Learning"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "LPNN",
        "author": "Amit Daniely and\nEran Malach",
        "title": "Learning Parities with Neural Networks"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "SQlower",
        "author": "Yiwen Kou and\nZixiang Chen and\nQuanquan Gu and\nSham M. Kakade",
        "title": "Matching the Statistical Query Lower Bound for k-sparse Parity Problems\nwith Stochastic Gradient Descent"
      }
    ]
  }
]