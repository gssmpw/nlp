\section{Related Works}
\paragraph{Expressivity and learnability of transformers.}
Prior work has studied the expressivity of transformers through formal languages. \citet{self_attention_limit} showed that transformers with hard or soft attention cannot compute parity, a task trivial for vanilla RNNs. This reveals a fundamental limitation of self-attention. Subsequent work~\citep{circuit, log-precision_trans, power_hard_attn} refined these bounds, restricting the expressivity of transformers within the $\text{FO}(M)$ complexity class. \citet{cot_express} extended the expressivity by augmenting transformers with chain-of-thought reasoning, enabling simulation of Turing machines with time $O(n^2 + t(n)^2)$, with $n$ being the input sequence length and $t(n)$ being the number of reasoning steps.

Recent work has also explored the learnability of transformers. 
\citet{simplicity_bias} show transformers under gradient descent favor low-sensitivity functions like $k$-parity compared to LSTMs, but their results are mostly empirical, and they do not provide any theoretical analysis on why transformers can generalize to these functions. A concurrent work by~\citet{single-location-attention} proves transformers can learn functions where only one input position matters. Although their work theoretically analyses the transformer's learning dynamic, it is restricted to only one attention head, and a comparison between FFNNs and transformers, especially with respect to parameter efficiency, is not mentioned in the work. This highlights the need for a formal theory of learning and generalization ability of multi-head transformers in low-sensitivity regimes.

\paragraph{Feature learning with neural networks.}
The $k$-parity problem is used as a benchmark for analyzing feature learning in neural networks. Prior work shows that two-layer FFNNs trained via gradient descent achieve more efficient feature learning than kernel methods, with the first layer learning meaningful representation as early as the first gradient step~\citep{feature_learning_2, feature_learning_1}. This aligns with \citet{LPNN}'s theoretical separation: While linear models on fixed embeddings require an exponential network width to learn this problem, FFNNs with a single hidden layer can achieve a small generalization error using gradient descent with only a polynomial number of parameters. Subsequent analysis~\citep{SQlower} focuses on lower bounds for the number of iterations needed by stochastic gradient descent to converge.
%
However, these results implicitly require $\Omega(n)$ parameters, raising questions about parameter efficiency and whether other architectures can achieve more effective feature learning with fewer parameters than FFNNs.