\section{Related work}
%\hp{Need to squeeze related work. Also, add a caption to each paragraph.}

%In this section, we briefly review existing related work.
% \hp{Would be better to add a couple of more generic references for each category. Currently we only have 34 refs. Aiming at 50+.}

\textbf{RL for CO}\quad
% RL has been widely used to solve CO problems.  ____ proposes a generic reinforcement learning framework for solving the vehicle routing problem, which is shown to outperform the traditional heuristic algorithms. ____ proposes a framework called Neural Combinatorial Optimization, which combines neural networks and reinforcement learning. The authors demonstrate the how to use this framework to solve the TSP problem and achieve excellent performance. ____ proposes a model based on the attention mechanism and uses the REINFORCE algorithm to achieve better results on a number of combinatorial optimization problems such as TSP, VRP, Orienteering Problem (OP), etc., as compared to a series of baseline algorithms. ____, on the other hand, proposes a combination of reinforcement learning and graph embedding to solve combinatorial optimization problems by constructing solutions step by step. Although these algorithms show promising results, the combinatorial optimization problems they solve are almost always deterministic and do not require sequential decision making, which may not be effective for more complex combinatorial optimization problems.
%\hp{need to mention search algorithms in methodology / experiment.}
RL has been widely used to solve CO problems ____. ____ propose a generic RL framework for solving the vehicle routing problem (VRP), outperforming traditional heuristic algorithms. ____ introduce neural combinatorial optimization, combining neural networks and RL to solve the TSP problem. ____ develop an attention-based model using the REINFORCE algorithm, achieving superior results on various CO problems like TSP, VRP, and the Orienteering Problem (OP). ____ combine RL with graph embedding to construct solutions for CO problems step by step. ____ present a method where neural networks iteratively improve combinatorial solutions through learned local modifications. ____ introduce a method where policy gradient techniques are used to train neural networks to develop heuristics for solving the TSP. ____ propose Sinkhorn policy gradient methods to learn permutation matrices for CO problems. ____ combine decision diagrams with deep RL to enhance optimization bounds for combinatorial problems. ____ introduce an iterative approach that leverages REINFORCE to enhance solutions for VRPs. Recent methods have further integrated search algorithms, such as active search ____, Monte Carlo tree search ____, and beam search ____, to enhance the solution qualities of RL algorithms during inference time. While these algorithms show promising results, they typically tackle one-shot deterministic CO problems without sequential decision-making, limiting their effectiveness for more complex scenarios.

\textbf{Graph embedding}\quad
% Since most combinatorial optimization problems are problems on graphs, graph embedding techniques are particularly important, aiming to represent graph structural data in a continuous vector space, thus preserving the structural properties of the graph and facilitating downstream reinforcement learning tasks. ____ and ____ are seminal works in this area. ____ explores neighborhoods more efficiently through a biased random walk approach, which aims to lean a mapping of nodes to a low-dimensional vector space and preserve the neighbor structure of nodes as much as possible by maximizing the likelihood. ____, on the other hand, is inspired by language models, and uses truncated random walks to obtain local information of nodes, and then learns potential representations of these nodes. In addition, significant progress has been made in the research of graph neural networks. Graph Convolutional Networks (GCN) ____, Graph Attention Networks (GAT) ____ and Graph Transformer Networks ____, and other models have been are effective means of dealing with graph embeddings. Nonetheless, which graph embedding technique to use is most beneficial for reinforcement learning tasks remains a worthy research question.
Graph embedding techniques are crucial for RL-based solutions to CO problems, representing graph structures in continuous vector spaces to preserve structural properties, facilitate downstream RL tasks, and generalize to unseen (potentially larger) graphs. %Seminal works by ____ and ____ are influential. 
Node2Vec ____ uses biased random walks to explore neighborhoods efficiently, which maps nodes to low-dimensional vector spaces while preserving the neighbor structures.
Deepwalk ____, which is inspired by language models, uses truncated random walks to obtain local node information and learn potential representations. Significant progress has also been made with graph neural networks (GNN) ____, such as GCN ____, Graph Attention Networks (GAT) ____ and Graph Transformer Networks ____. ____ evaluate the expressiveness of GNNs, comparing them to the Weisfeiler-Lehman test in capturing graph structures. ____ analyze the effectiveness and mechanisms of GCNs in semi-supervised learning. ____ introduce a method to interpret GNNs by identifying the crucial subgraphs and features that influence their predictions.
Nonetheless, determining the most beneficial graph embedding technique for RL tasks remains an important research question.

\textbf{Hierarchical reinforcement learning}\quad
HRL introduces a hierarchical structure to RL, decomposing complex tasks into simpler subtasks to improve learning efficiency. 
The option framework ____ defines HRL with options as temporally extended actions, which enables agents to learn at different levels of temporal abstraction. 
____ introduce a multi-level RL approach based on subgoals, where high-level managers set tasks for sub-managers to achieve efficiently. 
____ propose a data-efficient HRL method using off-line experiences to train higher-level and lower-level layers, and the results demonstrate its ability to learn complex behaviors. 
____ develop a method to automatically learn policy, termination function, and intra-option policy in HRL using intra-option policy gradient and termination gradient, without extra rewards or subgoals. ____ divide agent behavior into two levels: the Manager, which sets goals in a latent space, and the Worker, which executes these goals with primitive actions.
%, facilitating the learning of more abstract and temporally extended behaviors. 
____ combine hierarchical structures and intrinsic motivation to improve learning in complex tasks. ____ introduce a technique called HER that enhances RL by learning from failures as if they were successes with different goals, and ____ integrate HER with HRL to enhance learning at multiple levels.
Despite their success, these methods often do not address the specific challenges of SSCO problems, especially the need to handle sequential, combinatorial state-action spaces, and dynamic environments. Furthermore, they typically do not integrate graph-based representations, which could significantly enhance their effectiveness when dealing with graph-based CO problems.