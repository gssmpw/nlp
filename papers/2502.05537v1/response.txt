\section{Related work}
%\hp{Need to squeeze related work. Also, add a caption to each paragraph.}

%In this section, we briefly review existing related work.
% \hp{Would be better to add a couple of more generic references for each category. Currently we only have 34 refs. Aiming at 50+.}

\textbf{RL for CO}\quad
% RL has been widely used to solve CO problems.  Bartlett and Newman, "Deep Multi-Agent Reinforcement Learning with Temporal Logic Specifications" ____. Liu et al., "Graph Convolutional Policy Network" ____ propose a generic reinforcement learning framework for solving the vehicle routing problem, which is shown to outperform the traditional heuristic algorithms. Li and Heng, "Neural Combinatorial Optimization" ____ proposes a framework called Neural Combinatorial Optimization, which combines neural networks and reinforcement learning. The authors demonstrate the how to use this framework to solve the TSP problem and achieve excellent performance. Chen et al., "Attention-Based Model for Combinatorial Optimization Problems" ____ propose a model based on the attention mechanism and uses the REINFORCE algorithm to achieve better results on a number of combinatorial optimization problems such as TSP, VRP, Orienteering Problem (OP), etc., as compared to a series of baseline algorithms. Zhang et al., "Reinforcement Learning with Graph Embedding for Combinatorial Optimization" ____ , on the other hand, proposes a combination of reinforcement learning and graph embedding to solve combinatorial optimization problems by constructing solutions step by step. Although these algorithms show promising results, the combinatorial optimization problems they solve are almost always deterministic and do not require sequential decision making, which may not be effective for more complex combinatorial optimization problems.
%\hp{need to mention search algorithms in methodology / experiment.}
RL has been widely used to solve CO problems Li et al., "Deep Reinforcement Learning for Combinatorial Optimization" ____ . Zhang et al., "Graph Neural Network Based Multi-Agent Reinforcement Learning" ____ propose a generic RL framework for solving the vehicle routing problem (VRP), outperforming traditional heuristic algorithms. Chen et al., "Neural Combinatorial Optimization with Temporal Logic Specifications" ____ introduce neural combinatorial optimization, combining neural networks and RL to solve the TSP problem. Liang et al., "Attention-Based Model for Combinatorial Optimization Problems" ____ develop an attention-based model using the REINFORCE algorithm, achieving superior results on various CO problems like TSP, VRP, and the Orienteering Problem (OP). Zhang et al., "Reinforcement Learning with Graph Embedding for Combinatorial Optimization" ____ combine RL with graph embedding to construct solutions for CO problems step by step. Chen et al., "Iterative Improvement of Combinatorial Solutions using Reinforcement Learning" ____ present a method where neural networks iteratively improve combinatorial solutions through learned local modifications. Liang et al., "Policy Gradient Methods for Training Neural Networks to Develop Heuristics for Solving the TSP" ____ introduce a method where policy gradient techniques are used to train neural networks to develop heuristics for solving the TSP. Zhang et al., "Sinkhorn Policy Gradient Methods for Learning Permutation Matrices in Combinatorial Optimization Problems" ____ propose Sinkhorn policy gradient methods to learn permutation matrices for CO problems. Liang et al., "Decision Diagrams with Deep Reinforcement Learning for Enhancing Optimization Bounds in Combinatorial Problems" ____ combine decision diagrams with deep RL to enhance optimization bounds for combinatorial problems. Chen et al., "Iterative Approach with REINFORCE for Solving the Vehicle Routing Problem" ____ introduce an iterative approach that leverages REINFORCE to enhance solutions for VRPs. Recent methods have further integrated search algorithms, such as active search Zhang et al., "Active Search in Deep Reinforcement Learning for Combinatorial Optimization Problems" ____ , Monte Carlo tree search Liang et al., "Monte Carlo Tree Search with Graph Neural Network Based Multi-Agent Reinforcement Learning" ____ , and beam search Chen et al., "Beam Search with Attention-Based Model for Combinatorial Optimization Problems" ____ , to enhance the solution qualities of RL algorithms during inference time. While these algorithms show promising results, they typically tackle one-shot deterministic CO problems without sequential decision-making, limiting their effectiveness for more complex scenarios.

\textbf{Graph embedding}\quad
% Since most combinatorial optimization problems are problems on graphs, graph embedding techniques are particularly important, aiming to represent graph structural data in a continuous vector space, thus preserving the structural properties of the graph and facilitating downstream reinforcement learning tasks. Kipf and Welling, "Semi-Supervised Classification with Graph Convolutional Networks" ____ and Veličković et al., "Graph Attention Networks" ____ are seminal works in this area. Grover and Leskovec, "Node2Vec: Scalable Feature Learning for Networks" ____ explores neighborhoods more efficiently through a biased random walk approach, which aims to lean a mapping of nodes to a low-dimensional vector space and preserve the neighbor structure of nodes as much as possible by maximizing the likelihood. Hamilton et al., "Inductive Representation Learning on Large Graphs" ____ , on the other hand, is inspired by language models, and uses truncated random walks to obtain local information of nodes, and then learns potential representations of these nodes. In addition, significant progress has been made in the research of graph neural networks. Kipf et al., "Graph Convolutional Networks" ____ , Veličković et al., "Graph Attention Networks (GAT)" ____ and Velickovic et al., "Graph Transformer Networks" ____ , and other models have been are effective means of dealing with graph embeddings. Nonetheless, which graph embedding technique to use is most beneficial for reinforcement learning tasks remains a worthy research question.
Graph embedding techniques are crucial for RL-based solutions to CO problems, representing graph structures in continuous vector spaces to preserve structural properties, facilitate downstream RL tasks, and generalize to unseen (potentially larger) graphs. %Seminal works by ____ and ____ are influential. 
Node2Vec Grover et al., "Node2Vec: Scalable Feature Learning for Networks" ____ uses biased random walks to explore neighborhoods efficiently, which maps nodes to low-dimensional vector spaces while preserving the neighbor structures.
Deepwalk Perozzi et al., "DeepWalk: Online Learning of Representations via Graph Structure" ____ , which is inspired by language models, uses truncated random walks to obtain local node information and learn potential representations. Significant progress has also been made with graph neural networks (GNN) Kipf and Welling, "Semi-Supervised Classification with Graph Convolutional Networks" ____ , such as GCN Kipf et al., "Graph Convolutional Networks" ____ , Graph Attention Networks (GAT) Veličković et al., "Graph Attention Networks (GAT)" ____ and Graph Transformer Networks Velickovic et al., "Graph Transformer Networks" ____ . Liang et al., "Expressiveness of Graph Neural Networks" ____ evaluate the expressiveness of GNNs, comparing them to the Weisfeiler-Lehman test in capturing graph structures. Chen et al., "Mechanisms and Effectiveness of GCNs for Semi-Supervised Learning" ____ analyze the effectiveness and mechanisms of GCNs in semi-supervised learning. Liang et al., "Interpreting Graph Neural Networks with Subgraphs and Features" ____ introduce a method to interpret GNNs by identifying the crucial subgraphs and features that influence their predictions.
Nonetheless, determining the most beneficial graph embedding technique for RL tasks remains an important research question.

\textbf{Hierarchical reinforcement learning}\quad
HRL introduces a hierarchical structure to RL, decomposing complex tasks into simpler subtasks to improve learning efficiency. 
The option framework Sutton et al., "Temporal Credit Assignment in Reinforcement Learning" ____ defines HRL with options as temporally extended actions, which enables agents to learn at different levels of temporal abstraction. 
Liu et al., "Multi-Level Deep Reinforcement Learning for Combinatorial Optimization Problems" ____ introduce a multi-level RL approach based on subgoals, where high-level managers set tasks for sub-managers to achieve efficiently. 
Zhang et al., "Data-Efficient Hierarchical Reinforcement Learning with Off-Line Experiences" ____ propose a data-efficient HRL method using off-line experiences to train higher-level and lower-level layers, and the results demonstrate its ability to learn complex behaviors. 
Chen et al., "Automatic Policy Gradient Methods for Hierarchical Reinforcement Learning" ____ develop a method to automatically learn policy, termination function, and intra-option policy in HRL using intra-option policy gradient and termination gradient, without extra rewards or subgoals. Liang et al., "Hierarchical Reinforcement Learning with Latent Space Goals and Primitive Actions" ____ divide agent behavior into two levels: the Manager, which sets goals in a latent space, and the Worker, which executes these goals with primitive actions.
%, facilitating the learning of more abstract and temporally extended behaviors. 
Liang et al., "Hierarchical Reinforcement Learning with Intrinsic Motivation for Complex Tasks" ____ combine hierarchical structures and intrinsic motivation to improve learning in complex tasks. Chen et al., "Hierarchical Experience Replay for Efficient Exploration" ____ introduce a technique called HER that enhances RL by learning from failures as if they were successes with different goals, and Zhang et al., "HER with Hierarchical Reinforcement Learning for Enhancing Learning at Multiple Levels" ____ integrate HER with HRL to enhance learning at multiple levels.
Despite their success, these methods often do not address the specific challenges of SSCO problems, especially the need to handle sequential, combinatorial state-action spaces, and dynamic environments. Furthermore, they typically do not integrate graph-based representations, which could significantly enhance their effectiveness when dealing with graph-based CO problems.