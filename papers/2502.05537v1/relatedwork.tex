\section{Related work}
%\hp{Need to squeeze related work. Also, add a caption to each paragraph.}

%In this section, we briefly review existing related work.
% \hp{Would be better to add a couple of more generic references for each category. Currently we only have 34 refs. Aiming at 50+.}

\textbf{RL for CO}\quad
% RL has been widely used to solve CO problems.  \citet{nazari2018reinforcement} proposes a generic reinforcement learning framework for solving the vehicle routing problem, which is shown to outperform the traditional heuristic algorithms. \citet{bello2016neural} proposes a framework called Neural Combinatorial Optimization, which combines neural networks and reinforcement learning. The authors demonstrate the how to use this framework to solve the TSP problem and achieve excellent performance. \citet{koolAttentionLearnSolve2019} proposes a model based on the attention mechanism and uses the REINFORCE algorithm to achieve better results on a number of combinatorial optimization problems such as TSP, VRP, Orienteering Problem (OP), etc., as compared to a series of baseline algorithms. \citet{khalil2017learning}, on the other hand, proposes a combination of reinforcement learning and graph embedding to solve combinatorial optimization problems by constructing solutions step by step. Although these algorithms show promising results, the combinatorial optimization problems they solve are almost always deterministic and do not require sequential decision making, which may not be effective for more complex combinatorial optimization problems.
%\hp{need to mention search algorithms in methodology / experiment.}
RL has been widely used to solve CO problems \cite{mazyavkina2021reinforcement}. \citet{nazari2018reinforcement} propose a generic RL framework for solving the vehicle routing problem (VRP), outperforming traditional heuristic algorithms. \citet{bello2016neural} introduce neural combinatorial optimization, combining neural networks and RL to solve the TSP problem. \citet{kool2018attention} develop an attention-based model using the REINFORCE algorithm, achieving superior results on various CO problems like TSP, VRP, and the Orienteering Problem (OP). \citet{khalil2017learning} combine RL with graph embedding to construct solutions for CO problems step by step. \citet{chen2019learning} present a method where neural networks iteratively improve combinatorial solutions through learned local modifications. \citet{deudon2018learning} introduce a method where policy gradient techniques are used to train neural networks to develop heuristics for solving the TSP. \citet{emami2018learning} propose Sinkhorn policy gradient methods to learn permutation matrices for CO problems. \citet{cappart2019improving} combine decision diagrams with deep RL to enhance optimization bounds for combinatorial problems. \citet{lu2019learning} introduce an iterative approach that leverages REINFORCE to enhance solutions for VRPs. Recent methods have further integrated search algorithms, such as active search \citep{hottung2021efficient}, Monte Carlo tree search \citep{fu2021generalize}, and beam search \citep{kwon2020pomo}, to enhance the solution qualities of RL algorithms during inference time. While these algorithms show promising results, they typically tackle one-shot deterministic CO problems without sequential decision-making, limiting their effectiveness for more complex scenarios.

\textbf{Graph embedding}\quad
% Since most combinatorial optimization problems are problems on graphs, graph embedding techniques are particularly important, aiming to represent graph structural data in a continuous vector space, thus preserving the structural properties of the graph and facilitating downstream reinforcement learning tasks. \citet{grover2016node2vec} and \citet{perozzi2014deepwalk} are seminal works in this area. \citet{grover2016node2vec} explores neighborhoods more efficiently through a biased random walk approach, which aims to lean a mapping of nodes to a low-dimensional vector space and preserve the neighbor structure of nodes as much as possible by maximizing the likelihood. \citet{perozzi2014deepwalk}, on the other hand, is inspired by language models, and uses truncated random walks to obtain local information of nodes, and then learns potential representations of these nodes. In addition, significant progress has been made in the research of graph neural networks. Graph Convolutional Networks (GCN) \cite{kipf2016semi}, Graph Attention Networks (GAT) \cite{velickovic2017graph} and Graph Transformer Networks \cite{yun2019graph}, and other models have been are effective means of dealing with graph embeddings. Nonetheless, which graph embedding technique to use is most beneficial for reinforcement learning tasks remains a worthy research question.
Graph embedding techniques are crucial for RL-based solutions to CO problems, representing graph structures in continuous vector spaces to preserve structural properties, facilitate downstream RL tasks, and generalize to unseen (potentially larger) graphs. %Seminal works by \citet{grover2016node2vec} and \citet{perozzi2014deepwalk} are influential. 
Node2Vec \citep{grover2016node2vec} uses biased random walks to explore neighborhoods efficiently, which maps nodes to low-dimensional vector spaces while preserving the neighbor structures.
Deepwalk \citep{perozzi2014deepwalk}, which is inspired by language models, uses truncated random walks to obtain local node information and learn potential representations. Significant progress has also been made with graph neural networks (GNN) \cite{scarselli2009graph, zhou2020graph, wu2021comprehensive}, such as GCN \cite{kipf2016semi}, Graph Attention Networks (GAT) \cite{velivckovic2018graph} and Graph Transformer Networks \cite{yun2019graph}. \citet{xu2018powerful} evaluate the expressiveness of GNNs, comparing them to the Weisfeiler-Lehman test in capturing graph structures. \citet{li2018deeper} analyze the effectiveness and mechanisms of GCNs in semi-supervised learning. \citet{ying2019gnnexplainer} introduce a method to interpret GNNs by identifying the crucial subgraphs and features that influence their predictions.
Nonetheless, determining the most beneficial graph embedding technique for RL tasks remains an important research question.

\textbf{Hierarchical reinforcement learning}\quad
HRL introduces a hierarchical structure to RL, decomposing complex tasks into simpler subtasks to improve learning efficiency. 
The option framework \cite{sutton1999between} defines HRL with options as temporally extended actions, which enables agents to learn at different levels of temporal abstraction. 
\citet{dayan1992feudal} introduce a multi-level RL approach based on subgoals, where high-level managers set tasks for sub-managers to achieve efficiently. 
\citet{nachum2018data} propose a data-efficient HRL method using off-line experiences to train higher-level and lower-level layers, and the results demonstrate its ability to learn complex behaviors. 
\citet{bacon2017option} develop a method to automatically learn policy, termination function, and intra-option policy in HRL using intra-option policy gradient and termination gradient, without extra rewards or subgoals. \citet{vezhnevets2017feudal} divide agent behavior into two levels: the Manager, which sets goals in a latent space, and the Worker, which executes these goals with primitive actions.
%, facilitating the learning of more abstract and temporally extended behaviors. 
\citet{kulkarni2016hierarchical} combine hierarchical structures and intrinsic motivation to improve learning in complex tasks. \citet{andrychowicz2017hindsight} introduce a technique called HER that enhances RL by learning from failures as if they were successes with different goals, and \citet{levy2018learning} integrate HER with HRL to enhance learning at multiple levels.
Despite their success, these methods often do not address the specific challenges of SSCO problems, especially the need to handle sequential, combinatorial state-action spaces, and dynamic environments. Furthermore, they typically do not integrate graph-based representations, which could significantly enhance their effectiveness when dealing with graph-based CO problems.