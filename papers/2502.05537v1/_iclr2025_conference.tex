\pdfoutput=1
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsthm}
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{amsmath}
\usepackage[defaultcolor=red]{changes}
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{wrapfig}
\usepackage{subfigure}
\newtheorem{theorem}{Theorem}
\newtheorem*{theorem*}{Theorem}
\setcitestyle{square,comma,numbers}



\title{Sequential Stochastic Combinatorial Optimization Using Hierarchical Reinforcement Learning}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\iclrfinalcopy

\author{Xinsong Feng$^{1}$, Zihan Yu$^{2}$, Yanhai Xiong$^{3}$, Haipeng Chen$^{3}$\\
$^{1}$UCLA, $^{2}$The University of Hong Kong, $^{3}$William \& Mary \\
\texttt{xsfeng@ucla.edu, u3634664@connect.hku.hk, \{yxiong05, hchen23\}@wm.edu} \\
}

% \author{Xinsong Feng, Zihan Yu, Yanhai Xiong, Haipeng Chen \\
% UCLA, The University of Hong Kong, William \& Mary \\
% \texttt{xsfeng@ucla.edu, } \\
% \And
% Zihan Yu \\
%  \\
% \texttt{u3634664@connect.hku.hk} \\
% \And
% Yanhai Xiong, Haipng Chen \\
%  \\
% \texttt{\{yxiong05, hchen23\}@wm.edu}
% }

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\xs}[1]{{\color{blue}#1}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\setlength {\marginparwidth}{2cm} 
\begin{document}


\maketitle

\begin{abstract}


%You may encounter a combinatorial optimization (CO) problem at any time in your daily life, such as planning a travel route or allocating medical resources. 
% In recent years, with extensive research on reinforcement learning (RL), it has been regarded as a promising tool for solving combinatorial optimization (CO) problems.
Reinforcement learning (RL) has emerged as a promising tool for combinatorial optimization (CO) problems due to its ability to learn fast, effective, and generalizable solutions. 
Nonetheless, existing works mostly focus on one-shot deterministic CO, while sequential stochastic CO (SSCO) has rarely been studied despite its broad applications such as adaptive influence maximization (IM) and infectious disease intervention. 
In this paper, we study the SSCO problem where we first decide the budget (e.g., number of seed nodes in adaptive IM) allocation for all time steps, and then select a set of nodes for each time step. The few existing studies on SSCO simplify the problems by assuming a uniformly distributed budget allocation over the time horizon, yielding suboptimal solutions. We propose a generic hierarchical RL (HRL) framework called wake-sleep option (WS-option), a two-layer option-based framework that simultaneously decides adaptive budget allocation on the higher layer and node selection on the lower layer. 
WS-option starts with a coherent formulation of the two-layer Markov decision processes (MDPs), capturing the interdependencies between the two layers of decisions. Building on this, WS-option employs several innovative designs to balance the model's training stability and computational efficiency, preventing the vicious cyclic interference issue between the two layers. Empirical results show that WS-option exhibits significantly improved effectiveness and generalizability compared to traditional methods. Moreover, the learned model can be generalized to larger graphs, which significantly reduces the overhead of computational resources.
%WS-option employs two innovative designs: 1) We use a wake-sleep training procedure which initially fixes but pre-trains the higher layer while training the lower layer, followed by simultaneously training both layers to enhance overall system performance. 2) We use Monte Carlo (MC) methods for budget allocation in the higher layer and temporal-difference (TD) learning for node selection in the lower layer.
%Specifically, we design a two-layer structure based on the option framework and formulate Markov decision processes (MDP) for both layers, and then use Monte-Carlo methods and Deep Q-Network (DQN) to train them, respectively. \hp{Still does not seem exciting. Let's come back at this later.}
%\replaced{Specifically, we design a two-layer structure based on the option framework and formulate Markov decision processes (MDP) for both layers, and then use Monte-Carlo methods and Deep Q-Network (DQN) to train them, respectively.}{Specifically, we use the Monte-Carlo method to enable the first layer to learn budget allocation, while the second layer learns node selection via Deep Q-Network (DQN). }%\hp{Need to rewrite this to highlight the novelties.}



\end{abstract}
\section{Introduction}
% \hp{refer to the group resources Google doc for how to write an abstract. Introduction is an extended version of the abstract.  Write in a way as if you are telling a story. I suggest to first have the outline/flow as bulletins, and then start writing.}

% \begin{itemize}
%     \item Motivation: 1) context and importance of CO
%     \item Gap from prior work: 1) traditional approaches, shortcomings. 2) RL, limitations. 
%     \item Proposed Work: 1) introduction to HRL 2) Our appraoch 3) Key contributions
%     \item Findings: 
%     \item Conclusion: 1) limitations 2) future work
    
    
% \end{itemize}

% \begin{itemize}
%     \item Motivation: 1) CO is important. There are extensive studies on RL for CO. 
%     \item Gap from prior work: most focus on one-shot deterministic CO. But sequential stochastic CO (SSCO) has a lot of real-world applications, such as ... Only \cite{chenContingencyawareInfluenceMaximization} and \cite{ouActiveScreeningRecurrent2021} study SSCO. But they assume that the budget is evenly allocated, which is not optimal. For example, under these three scenarios: ... We provide a better solution by jointly considering budget allocation and seed set selection. 
%     \item Proposed Work: 1) introduction to HRL 2) Our approach with some description 3) Key contributions
%     \item Findings \& Conclusion: summarize main results, no need to talk about limitation / future work here as we only have 9 pages. We can briefly talk about them at the end of the paper (the conclusion section).
    
    
% \end{itemize}


%CO problems are everywhere and are almost embedded in our lives. 
Combinatorial optimization (CO) problems cover a broad spectrum of application domains, such as social networks \cite{schuetz2022combinatorial, angelini2023modern}, public health \cite{gireesan2024deep, lee2021mind}, transportation \cite{cacchiani2009models}, telecommunications \cite{cao1999applying}, and scheduling \cite{oren2021solo, yin2023accelerating}. %\hp{@Zihan: can you find 1-2 references for each application} 
Hence, much attention has been drawn from both the theory and application communities toward solving CO problems. 
%Imagine that you want to plan the most economical and efficient route for travel. Or considering the challenges faced by hospitals during an epidemic, they need to allocate limited medical resources to treat as many patients as possible. Nowadays researchers are becoming more aware of the significance of solving CO problems. 
Recently, there have been studies using reinforcement learning (RL) to learn generalizable heuristic policies for CO problems on graphs~\citep{khalil2017learning,nazari2018reinforcement,deudon2018learning,bengio2020machine,kool2018attention,berto2023rl4co}. 
%The key idea is to decompose the selection of nodes into a sequence, and learn a heuristic policy that selects nodes sequentially. 
The RL policy is trained on a set of previously seen training graphs and generalizes to unseen test graphs of similar structure.
This policy is usually represented using graph embedding methods such as structure2vec (S2V)~\citep{dai2016discriminative} and graph convolutional networks (GCNs)~\citep{kipf2016semi}. 
%RL emerging as a promising solution for dynamically solving complex CO problems has been widely studied \cite{daiLearningCombinatorialOptimization2018, berto2023rl4co, koolAttentionLearnSolve2019}. 
However, most existing studies focus on one-shot deterministic CO, while research on another broad class of CO problems, which contain \textit{stochasticity} and must be solved \textit{sequentially} over multiple time steps, remains limited. 

%\hp{Add 1-2 references for each.}
%To illustrate this, we give the main problem to solve as follows:

% \textbf{Problem statement}\quad In this paper, we study the SSCO problem where we first decide the budget allocation for all the time steps, and then select a combination of nodes for each time step. Given a graph $G=(V, E)$, where $V$ and $E$ represent the set of vertices and edges on the graph, respectively, the number of time steps $T$, and the total budget $K$ for selecting nodes. The task is to decide 1) an allocation $K_1,K_2,\ldots,K_T$ of budget over the time horizon, together with 2) a sequence of node sets $S_{1},S_{2},\dots,S_{T} \subseteq V$, such that it maximizes the cumulative reward $\sum_{t=1}^T r_t$, while satisfying the per-time step (e.g., a week or month) budget constraint $|S_{t}|\leq K_t, \ \forall t$, and the overall budget constraint $\sum_{t}^TK_t\leq K$. $r_t$ is the reward related to the problem instance and the two layers of decisions. %\hp{illustrate it.}
% SSCO has a wide range of real-world applications such as the resource allocation of wireless communication networks \cite{liang2019deep}, route planning \cite{joe2020deep}, and adaptive IM \cite{peng2019adaptive, tong2020time}.

\textbf{Problem Statement}\quad
In this paper, we study the sequential stochastic CO (SSCO) problem where we first decide the budget allocation for all the time steps, and then select a combination of nodes for each time step. Given a graph $G=(V, E)$, where $V$ and $E$ represent the set of vertices and edges on the graph, respectively, the number of time steps $T$, and the total budget $K$ for selecting nodes, the task is to decide 1) an allocation $K_1,K_2,\ldots,K_T$ of budget over the time horizon, together with 2) a sequence of node sets $S_{1},S_{2},\dots,S_{T} \subseteq V$, such that
\begin{subequations} \label{eq:formulation}
\begin{align}
& \underset{\begin{subarray}{c} 
    K_1, \ldots, K_T \\ 
    S_1, \ldots, S_T 
\end{subarray}}{\text{maximize}} 
\quad && \sum\nolimits_{t=1}^{T} r_t(S_t) \\
& \text{subject to} 
\quad && \sum\nolimits_{t=1}^{T} K_t \leq K, \label{eq:higher-layer constraint} \\
& 
&& |S_t| \leq K_t, \quad \forall t = 1, \ldots, T, \label{eq:lower-layer constraint} \\
& 
&& K_t \in \mathbb{N}, \quad \forall t = 1, \ldots, T, \\
& 
&& S_t \subseteq V, \quad \forall t = 1, \ldots, T.
\end{align}
\end{subequations}
Here, $r_t(S_t)$ denotes the reward obtained from the nodes in set $S_t$ at the time step $t$. 
SSCO has a wide range of real-world applications such as resource allocation in wireless communication networks \cite{liang2019deep}, route planning \cite{joe2020deep}, and adaptive influence maximization (IM) \cite{peng2019adaptive, tong2020time}.




% As far as we know, there are only two works that address SSCO using RL, under the context of adaptive IM \cite{chenContingencyawareInfluenceMaximization} and medical resource allocation for epidemic control \cite{ouActiveScreeningRecurrent2021}. However, they assume that the budget in the case is evenly allocated across all the time steps (i.e., $K_t=K/T, \ \forall t$), which may not be optimal under these three scenarios: 1) Stochastic environment -- there are randomness in the network or node state. 2) Access to information and feedback. Additional information or feedback can be obtained later in the time horizon which can be used to design better solutions. %during the selection process, then selecting nodes over multiple time steps can use the extra information to optimize subsequent selections. 
% 3) Risk management. Multi-shot choices mitigate the risk of making a wrong decision at a certain time point. In these scenarios, evenly allocated budgets fail to adapt to changing conditions, leverage real-time information and feedback, or mitigate risks effectively, leading to sub-optimal decisions.
%\hp{I feel this is a bit over-complicated. These are actually all just randomness + more information revealed in the future. So can be summarized as adaptive sequential decision making. Using the AIM as a running example here maybe a better idea.}
%\hp{Need to explain why.}


% As far as we know, there are only two works that address SSCO using RL, in the context of adaptive IM \cite{chen2021contingency} and medical resource allocation for epidemic control \cite{ou2021active}. 
As far as we know, there are not many works that address SSCO using RL; two notable examples are adaptive IM in \cite{chen2021contingency} and medical resource allocation for epidemic control in \cite{ou2021active}. 
However, they assume that the budget is evenly allocated across all the time steps (i.e., $K_t=K/T, \ \forall t$), which may not be optimal in stochastic environments or when additional information or feedback becomes available over time. 
These scenarios need adaptive sequential decision-making. 
For example, in the adaptive IM problem, an even allocation policy cannot adapt to sudden changes and uncertainties, leading to resource shortages at critical times or redundancy at less important times. 
Additionally, it lacks the flexibility to adjust strategy when more important information becomes available.

Despite its criticality, solving SSCO is extremely challenging due to the following factors: 1) \textit{Combinatorial complexity}. The search space grows exponentially as the graph grows, which makes it difficult to find the optimal solution efficiently. 2) \textit{Stochasticity}. Future states and rewards are uncertain, so the ability to adaptively update the solutions based on the real-time state and stochasticity in the future is required.
 % cope with future changes and robustness are required, as well as the ability to adaptively update the solutions based on the real-time state. 
 3) \textit{Sequential decision making}. Decisions are interdependent and require sophisticated planning that considers both current and future consequences. 4) \textit{Scalability}. The methods need to have the ability to be generalized to large-scale problems in the real world.

%\hp{merge the following two paras. Explain what is our key idea, why we use it, and what are the technical novelties.}
%\hp{need to mention RL for CO. Need to re-check this para later.}

While traditional RL struggles with these challenges, hierarchical RL (HRL) presents a viable solution by decomposing complex tasks into simpler sub-tasks to improve learning efficiency. Several works have contributed to the development of HRL. The option framework models temporally extended actions, allowing agents to learn at different levels of abstraction \cite{sutton1999between}. Multi-level approaches use higher-level managers to set tasks for sub-managers \cite{dayan1992feudal}. Data-efficient methods leverage offline experiences for training \cite{nachum2018data}. Combining HRL with intrinsic motivation \cite{kulkarni2016hierarchical} and hindsight experience replay (HER) \cite{andrychowicz2017hindsight} further enhances learning.

%In this context, HRL introduces a hierarchical structure to address the limitations of traditional RL in solving complex CO problems. HRL can decompose complex tasks into smaller and simpler sub-tasks, which not only improves learning efficiency and performance but also follows the intuition of human thinking patterns. Consequently, HRL is an excellent solution for solving complex CO problems. 

In this paper, we propose an HRL framework, WS-option, for solving SSCO by jointly considering budget allocation and node selection, with a coherent definition of MDPs for both layers. The lower layer is particularly challenging due to the need for explicit definitions of states and rewards.
%The lower layer must refine and process decisions passed down from the higher layer, and its states and rewards often need explicit definition due to their indirect or non-exist nature.
%we demonstrate how to define an MDP, particularly for the lower layer. Defining the MDP for the lower layer is especially challenging because it must refine and process decisions passed down from the higher layer, and because the states and rewards of the lower layer are often not directly accessible, may not actually exist, and need to be defined by us. 
The MDP formulation directly affects the overall performance of the model. Maintaining model stability is crucial due to the interdependence and mutual influence of the two layers. To improve model stability, especially in the higher layer, we adopt MC methods to learn budget allocation, known as the option policy. To ensure faster convergence and stable operation of the lower layer, we use TD methods to learn node selection, known as the intra-option policy \cite{bacon2017option}. These methods ensure that both layers can run stably and efficiently, avoiding the issue of potential vicious mutual interference.

Our main contributions are as follows.
\begin{itemize}[leftmargin=*]
    \item We are the first to formally summarize and define the generic class of SSCO problems. 
    \item We design a novel HRL algorithm, WS-option, to solve the formulated SSCO, with major novelties in terms of the two-layer MDP formulation, and new wake-sleep training procedures to balance the training stability and efficiency and avoid the potential vicious mutual interference. 
    Our algorithm is a generic framework that applies to a wide range of SSCO problems of the same nature. 
    For completeness, we also provide a brief theoretical analysis of the algorithm's convergence.
    %\item We formulate the underlying MDPs for two layers, and in particular, for the second layer, we propose explicit methods for defining states and rewards to cope with the challenge of ambiguous states and rewards in complex environments.
    %\item We show how to train both layers: for the higher layer, MC method is used; for the lower layer, TD-learning is used.
    \item We conduct experiments on two distinct SSCO problem instances -- adaptive IM and route planning. 
    The results show that compared to traditional methods, our algorithm exhibits superior performance in solving both SSCO problems. 
    In particular, the algorithm is able to generalize to larger unseen graphs, thus significantly reducing computational overheads for larger-scale SSCO problems.
    % and enable the models trained to generalize to larger graphs. The results show that compared with traditional methods, our proposed algorithm exhibits strong generalization and superior performance in solving various types of SSCO problems. In particular, the algorithm is able to be generalized to larger graphs, which means that we can train only on small and simple graphs and later apply the model to larger graphs, thus significantly reducing computational and resource overheads. %This is an important advantage that is difficult to achieve with traditional methods.
\end{itemize}




\section{Related work}
%\hp{Need to squeeze related work. Also, add a caption to each paragraph.}

%In this section, we briefly review existing related work.
% \hp{Would be better to add a couple of more generic references for each category. Currently we only have 34 refs. Aiming at 50+.}

\textbf{RL for CO}\quad
% RL has been widely used to solve CO problems.  \citet{nazari2018reinforcement} proposes a generic reinforcement learning framework for solving the vehicle routing problem, which is shown to outperform the traditional heuristic algorithms. \citet{bello2016neural} proposes a framework called Neural Combinatorial Optimization, which combines neural networks and reinforcement learning. The authors demonstrate the how to use this framework to solve the TSP problem and achieve excellent performance. \citet{koolAttentionLearnSolve2019} proposes a model based on the attention mechanism and uses the REINFORCE algorithm to achieve better results on a number of combinatorial optimization problems such as TSP, VRP, Orienteering Problem (OP), etc., as compared to a series of baseline algorithms. \citet{khalil2017learning}, on the other hand, proposes a combination of reinforcement learning and graph embedding to solve combinatorial optimization problems by constructing solutions step by step. Although these algorithms show promising results, the combinatorial optimization problems they solve are almost always deterministic and do not require sequential decision making, which may not be effective for more complex combinatorial optimization problems.
%\hp{need to mention search algorithms in methodology / experiment.}
RL has been widely used to solve CO problems \cite{mazyavkina2021reinforcement}. \citet{nazari2018reinforcement} propose a generic RL framework for solving the vehicle routing problem (VRP), outperforming traditional heuristic algorithms. \citet{bello2016neural} introduce neural combinatorial optimization, combining neural networks and RL to solve the TSP problem. \citet{kool2018attention} develop an attention-based model using the REINFORCE algorithm, achieving superior results on various CO problems like TSP, VRP, and the Orienteering Problem (OP). \citet{khalil2017learning} combine RL with graph embedding to construct solutions for CO problems step by step. \citet{chen2019learning} present a method where neural networks iteratively improve combinatorial solutions through learned local modifications. \citet{deudon2018learning} introduce a method where policy gradient techniques are used to train neural networks to develop heuristics for solving the TSP. \citet{emami2018learning} propose Sinkhorn policy gradient methods to learn permutation matrices for CO problems. \citet{cappart2019improving} combine decision diagrams with deep RL to enhance optimization bounds for combinatorial problems. \citet{lu2019learning} introduce an iterative approach that leverages REINFORCE to enhance solutions for VRPs. Recent methods have further integrated search algorithms, such as active search \citep{hottung2021efficient}, Monte Carlo tree search \citep{fu2021generalize}, and beam search \citep{kwon2020pomo}, to enhance the solution qualities of RL algorithms during inference time. While these algorithms show promising results, they typically tackle one-shot deterministic CO problems without sequential decision-making, limiting their effectiveness for more complex scenarios.

\textbf{Graph embedding}\quad
% Since most combinatorial optimization problems are problems on graphs, graph embedding techniques are particularly important, aiming to represent graph structural data in a continuous vector space, thus preserving the structural properties of the graph and facilitating downstream reinforcement learning tasks. \citet{grover2016node2vec} and \citet{perozzi2014deepwalk} are seminal works in this area. \citet{grover2016node2vec} explores neighborhoods more efficiently through a biased random walk approach, which aims to lean a mapping of nodes to a low-dimensional vector space and preserve the neighbor structure of nodes as much as possible by maximizing the likelihood. \citet{perozzi2014deepwalk}, on the other hand, is inspired by language models, and uses truncated random walks to obtain local information of nodes, and then learns potential representations of these nodes. In addition, significant progress has been made in the research of graph neural networks. Graph Convolutional Networks (GCN) \cite{kipf2016semi}, Graph Attention Networks (GAT) \cite{velickovic2017graph} and Graph Transformer Networks \cite{yun2019graph}, and other models have been are effective means of dealing with graph embeddings. Nonetheless, which graph embedding technique to use is most beneficial for reinforcement learning tasks remains a worthy research question.
Graph embedding techniques are crucial for RL-based solutions to CO problems, representing graph structures in continuous vector spaces to preserve structural properties, facilitate downstream RL tasks, and generalize to unseen (potentially larger) graphs. %Seminal works by \citet{grover2016node2vec} and \citet{perozzi2014deepwalk} are influential. 
Node2Vec \citep{grover2016node2vec} uses biased random walks to explore neighborhoods efficiently, which maps nodes to low-dimensional vector spaces while preserving the neighbor structures.
Deepwalk \citep{perozzi2014deepwalk}, which is inspired by language models, uses truncated random walks to obtain local node information and learn potential representations. Significant progress has also been made with graph neural networks (GNN) \cite{scarselli2009graph, zhou2020graph, wu2021comprehensive}, such as GCN \cite{kipf2016semi}, Graph Attention Networks (GAT) \cite{velivckovic2018graph} and Graph Transformer Networks \cite{yun2019graph}. \citet{xu2018powerful} evaluate the expressiveness of GNNs, comparing them to the Weisfeiler-Lehman test in capturing graph structures. \citet{li2018deeper} analyze the effectiveness and mechanisms of GCNs in semi-supervised learning. \citet{ying2019gnnexplainer} introduce a method to interpret GNNs by identifying the crucial subgraphs and features that influence their predictions.
Nonetheless, determining the most beneficial graph embedding technique for RL tasks remains an important research question.

\textbf{Hierarchical reinforcement learning}\quad
HRL introduces a hierarchical structure to RL, decomposing complex tasks into simpler subtasks to improve learning efficiency. 
The option framework \cite{sutton1999between} defines HRL with options as temporally extended actions, which enables agents to learn at different levels of temporal abstraction. 
\citet{dayan1992feudal} introduce a multi-level RL approach based on subgoals, where high-level managers set tasks for sub-managers to achieve efficiently. 
\citet{nachum2018data} propose a data-efficient HRL method using off-line experiences to train higher-level and lower-level layers, and the results demonstrate its ability to learn complex behaviors. 
\citet{bacon2017option} develop a method to automatically learn policy, termination function, and intra-option policy in HRL using intra-option policy gradient and termination gradient, without extra rewards or subgoals. \citet{vezhnevets2017feudal} divide agent behavior into two levels: the Manager, which sets goals in a latent space, and the Worker, which executes these goals with primitive actions.
%, facilitating the learning of more abstract and temporally extended behaviors. 
\citet{kulkarni2016hierarchical} combine hierarchical structures and intrinsic motivation to improve learning in complex tasks. \citet{andrychowicz2017hindsight} introduce a technique called HER that enhances RL by learning from failures as if they were successes with different goals, and \citet{levy2018learning} integrate HER with HRL to enhance learning at multiple levels.
Despite their success, these methods often do not address the specific challenges of SSCO problems, especially the need to handle sequential, combinatorial state-action spaces, and dynamic environments. Furthermore, they typically do not integrate graph-based representations, which could significantly enhance their effectiveness when dealing with graph-based CO problems.
\section{The WS-option framework}
%\hp{some of the notations are from the next subsection (MDP)}
This section will show how to learn budget allocation and node selection using our proposed WS-option framework. 
We begin with a brief introduction to the option framework, followed by the architecture used in our approach. 
Subsequently, we will formulate the MDPs for both hierarchical layers and adopt value-based methods as the backbone RL approach. 
In particular, to balance training stability and computational efficiency, we employ MC methods to learn the Q-function for the higher layer and TD-learning to learn the Q-function for the lower layer. 
%The policy decisions are based on these two functions.
Additionally, it is important to note that our model is trained on different graphs that follow a certain distribution to enhance its generalization ability to unseen, similarly-structured graphs from the same distribution.
The network architecture is shown in Appendix \ref{appendix:model}.

%\hp{Section/subsection letter case should be consistent throughout}

%\hp{Use a diagram to better illustrate the entire framework.}
\subsection{Preliminary: the option framework} \label{sec:option framework}
The option framework, proposed by \cite{sutton1999between}, is an implementation of temporal abstraction in RL. 
It provides a straightforward way to describe temporally extended actions, known as options, which are higher-level actions composed of several primitive actions. 
Formally, an option $o$ is defined by the tuple $(I, \pi^{II}, \beta)$, where 
\begin{itemize}[leftmargin=*]
    \item Initiation set $I$: a set of states where the option can be initialized. We will assume that any state satisfies $s \in I$ in our problems. 
    \item Intra-option policy $\pi^{II}$:  a policy that maps states to actions under a given option, controlling the lower-level actions taken under that option. We use $\pi^{II}$ as the lower-level node selection policy.%It is typically used to refer to the lower-level policy.
    \item Termination function $\beta$: a mapping $\beta:S\to[0,1]$ that specifies the probability of terminating the option in a given state, where $S$ is the state space.
\end{itemize}
When an agent operates within the option framework, it follows the process: 1) Option selection -- the agent selects an option based on its option policy given a state. 2) Option execution -- once the option is determined, the agent follows the intra-option policy $\pi^{II}$ until the option terminates. 3) Option termination -- when the termination condition is met, the agent terminates the current option and proceeds to the next option selection. 

In our study, we adopt a slightly different approach to defining options. 
While at the higher layer, the option $o^{I}_t$ we select represents the budget $K_{t}$ allocated at the current time step $t$, we simplify the information representation when passing the option to the lower layer. 
Specifically, we use option $o^{II}_t=1$ to indicate that the lower layer should continue selecting nodes, and option $o^{II}_t=0$ to indicate that it should stop selecting nodes and interact with the environment. 
It is important to note that to handle cases where the budget allocated by the higher layer is 0 (i.e., $o^{I}_t=o^{II}_t=0$), we introduce a \textit{null action} $a_{t,\emptyset}$ for the lower layer. The null action $a_{t,\emptyset}$ represents directly interacting with the environment and moving to the next time step. 
In fact, we achieved very good results by introducing this null action.

%\hp{We need a subsection to briefly talk about the network architecture, though this is not the focus.}
%\hp{I agree with Yanhai that this should go before the option framework. In this case let's just call it WS-option instead of Preliminary: The option framework. Let me go over the entire paper more closely and work on this later.}
\subsection{Hierarchical MDP formulation}
%\hp{This needs to be written in paragraphs. Bulletins are good for outlines but eventually we need paragraphs.}
Here, we define the MDPs for the two layers of our hierarchical model (c.f.  Figure \ref{fig:hrl-ssco}). 
For ease of understanding, we will take the adaptive IM problem as a running example. 
%Figure \ref{fig:hrl-ssco} illustrates the structure of the hierarchical MDP.
More details of both problems are shown in Appendix \ref{description:aim} and \ref{description:rp}, respectively.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.85\textwidth]{./HRL-SSCO1.pdf}
    \caption{Hierarchical MDPs for SSCO}
    \label{fig:hrl-ssco}
\end{figure}
%\hp{need explicit definitions of reward and transition function, can be generic}
\subsubsection{Higher layer MDP}
%\hp{Check letter case for all sections/subsections/paragraph captions.}
% States: the state $S$ is features of all nodes and also includes global information such as time steps $T_{r}$ and  remaining budget $K_{r}$.

% Actions:  given the current state $S$, the action is the budget $0\leq k\leq K_{r}$ allocated to the current time step.

% Rewards: Instantaneous reward for taking action $a$ in state $s$. Details will be discussed in the next section.

% Transitions: the state transition is dynamic, based on the current state and action taken.  

% Policy: a greedy policy is used, $\pi(s)=\arg\underset{o}{\max}Q_{\Omega}(s, o)$ 

%\hp{notations for a and o are mixed.}

% The state $s^I\in S$ includes the features of all nodes and global information such as remaining time steps $T_{r}$ and the remaining budget $K_{r}$. Given the current state, the option $o^{I}$ represents the budget $0\leq K_t\leq K_{r}$ allocated to the current time step. The reward is an instantaneous reward for taking option $o$ in state $s^I$, with specific details discussed in the next section. State transitions are determined by the environment, such as an activation process in IM problem. The policy is $\epsilon$-greedy, balancing exploration, with the probability of $\epsilon$ to choose a random option, and exploitation, with the probability of $1-\epsilon$ to choose $\pi(s^I)=\arg\underset{o}{\max}\ Q^I(s^I, o)$.

\textbf{State}\quad
% take AIM for example:
% The state $s^I=(X, g)\in S$ includes two types of information: node features $X$ (e.g., inactive or active in adpative IM problem) and global information $g$ (e.g., the remaining time steps $T_{r}$ and the remaining budget $K_{r}$). For instance, in the adaptive IM problems, we denote the state of nodes as $X_t\in \{0,1\}^{|V|\times 3}$. The state of each node is represented as a one-hot vector, indicating that the node is in one of three possible states: inactive, active (able to activate other nodes), or removed (active but unable to activate other nodes).
The state $s^I=(X, g)\in S$ includes two types of information: node features $X$ (e.g., inactive or active in adaptive IM problem) and global information $g$ (e.g., the remaining time steps $T_{r}$ and the remaining budget $K_{r}$). 


\textbf{Option}\quad
The option $o^{I}$ represents the budget $0\leq K_t\leq K_{r}$ allocated to the current time step $t$. 

\textbf{State transition}\quad
The state transition in this layer is stochastic, which is a fundamental characteristic of SSCO problems.
% Take AIM problem for example:
% For example, given the current state $s_t^I$ and action $a_t^{II}$ (selected nodes) from the lower layer, we first change the state of selected nodes $a_t^{II}$. For each node $v \in a_t^{II}$, 
% \begin{equation}\label{eq:activate}
%     X_t^{v} = [0, 1, 0].
% \end{equation}
% Now we consider the interaction with environment. Assume the set of current active nodes is $C_{\text{a}, t}$ and the set of their inactive neighbors is $C_{\text{ina}, t}$. Then, for each node $v \in C_{\text{ina}, t}$, the probability of being activated is 
% \begin{equation}
%     P(X_t^v = [0,1,0])=1-\prod\nolimits_{u \in \mathcal{N}(v) \cap C_{\text{a}, t}}(1-p(u,v)).
% \end{equation}
% where $\mathcal{N}(v)$ is the set of neighbors of node $v$, and $p(u,v)$ is the probability that node $v$ is activated by node $u$. Then, we update the state of newly activated nodes $\mathcal{A}$ as in equation \ref{eq:activate}. 
Given the current state $s_t^I$ and the action $a_t^{II}$ from the lower layer, the state $s^I_{t+1}$ is updated according to the following function:
\begin{equation}
    s_{t+1}^I = f(s_t^I, a_t^{II}) + \eta(s_t^I, a_t^{II}),
\end{equation}
where $f(s_t^I, a_t^{II})$ is a deterministic function that updates the state $s_t^I$ based on the current action $a_t^{II}$, while $\eta(s_t^I, a_t^{II})$ represents the stochastic component, which introduces randomness into the state transition, thereby capturing the stochastic nature of the environment.
For example, in the adaptive IM problem, there is no deterministic component.
Instead, the entire state transition is governed by the stochastic process $\eta(s_t^I, a_t^{II})$, which can be interpreted as the "activation process" within this problem.

\textbf{Reward}\quad The total reward depends on the problem of interest.
%We define the total reward first. 
In adaptive IM, it is the total number of influenced nodes (e.g., active and removed nodes). Then, the immediate reward $r^{I}$ is the increase in the total reward.

\subsubsection{Lower layer MDP} 
% \begin{itemize}[leftmargin=*]
%     \item States: the state $S'$ can be a transitional state, which does not actually exist. In our problem, it can represent transitional states where just one node is selected so its state changes but has not yet interacted with the environment. 
%     \item Options: the option $o$ is $0$ or $1$, where "1" represents continuing to select nodes. "0" represents not selecting any nodes and interacting with the environment.
%     \item Actions: one selected node or taking a null action.
%     \item Rewards: to define the rewards, we first denote the marginal reward of selecting node $v$ as $m_{v}$. Then, the reward for selecting node $v$ is $\frac{m_{v}}{\sum_{u\in A_{t}}m_{u}}(R_{t}-R_{interact})$, where $A_{t}$ is the set of nodes selected at time $t$, $R_{t}$ is the reward of the first layer at time $t$, $R_{interact}$ is the reward from interacting with the environment without selecting any nodes.
%     \item Transitions: there are two types of transitions: 1) option $o$ is 1, the state of the selected node changes. 2) option $o$ is 0, the agent interacts with the environment, and the state changes dynamically.
%     \item Policy: also a greedy policy is used, $\pi(s)=\arg\underset{a}{\max}Q_{\omega}(s, a)$ 
% \end{itemize}
%$\hp{Notations of the two layers are a bit mixed. E.g., state, action/option, time, ... maybe use S for higher, s for lower; t for higher, $\tau$ for lower, R for higher, r for lower... }
\textbf{State}\quad
The state $s^{II}$  is also defined as $s^{II}=\{X, g\}$.

\textbf{Option}\quad
The option $o^{II}$ can be either 0 or 1, where "1" represents continuing to select nodes, and "0" represents not selecting any node and interacting with the environment. 

\textbf{Action}\quad
Two kinds of actions are defined. Given the current time step $t$, the action $a^{II}_{t, j}$ is the node selected for a sub time step $j$ (e.g., $o_{t,j}^{II}=1$) or just the null action $a_{t, \emptyset}$ (e.g., $o_{t,j}^{II}=0$).
And the $a_t^{II}$ is all the nodes selected in the current time step or the null action $a_{t, \emptyset}$ if no nodes are selected.

\textbf{State transition}\quad
The state transition can occur either in a deterministic or stochastic way. 
% If option $o^{II}_t$ is 1 and the action is node $v$, then we update the state by setting $X_{t,j+1}=X_{t, j}$ and $X_{t,j+1}^v = [0, 1, 0]$. 
If option $o^{II}_t$ is 1 and the action is node $v$, we update the state of node $v$ deterministically (e.g., activating node $v$). 
If option $o$ is 0, the agent interacts with the environment as described in the higher layer MDP.

\textbf{Reward}\quad
The reward in the lower layer significantly affects the convergence of the model. 
We first define the reward $r^{II}_{t,\emptyset}$ for the null action $a_{t, \emptyset}$, which means directly interacting with the environment at time step $t$. 
$r^{II}_{t,\emptyset}$ depends on the specific problem, and the intuition is to determine the expected reward when moving to the next day if we do not select any node in the current state.  
For propagation problems, we can choose $r^{II}_{t,\emptyset}=r_t^I - k_{\text{eff}}$, where $k_{\text{eff}}$ represents the effective number of nodes in action $a_t^{II}$ (e.g., number of inactive nodes in $a_t^{II}$) that can yield actual gain. 
For problems without a propagation process (e.g., TSP, RP), we can simply set this reward to 0. 
Next, we define the marginal reward $m_{t,v}$ for node $v$. 
We denote the selected nodes in the current sub time step $j$ as $A_t=\{a_{t,v_1},...,a_{t,v_{j-1}} \}$. The marginal reward is obtained as follows 
\begin{equation}
    m_{t,v} = R_t(s^I_t, A_t\cup\{a_{t,v}\}) - R_t(s^I_t, A_t),
\end{equation}
where $R_t(S,A)$ is the immediate reward from taking action $A$ in state $S$. 
In our experiments, this reward is estimated by averaging the results of multiple (typically 10) simulations.
As a result, the reward for selecting node $v$ is given by
\begin{equation} \label{eq:reward}
    r^{II}_{t,v}=\frac{m_{t,v}}{\sum_{u\in a^{II}_{t}}m_{t,u}}(r^I_{t}-r^{II}_{t,\emptyset}).
\end{equation}

In this manner, the total reward at each time step for the lower layer will remain consistent with the higher layer, enhancing the model's stability.


%The key to designing a unified RL framework that addresses the joint task is to come up with a way that handles the interdependency of the two decision problems. To do this, we propose a sequence-of-sequence decomposition of the multi-shot SCOPs, with a hierarchical MDP formulation of the problem (c.f. Figure \ref{fig:hrl-ssco}). On the high-level sequence, the state $\mathbf{s}^I_h=\langle \mathbf{X}_h, B_h \rangle$ at a main time step $h$ (e.g., a day/week) contains the status information $\mathbf{X}_h$ of the network, as well as the remaining budget $B_h=B-\sum_{h'=1}^h b_{h'}$. The action is to decide the budget $b_h$ at $h$. On the low-level sequence, the initial sub-state $\mathbf{s}^I_{h,1} = \mathbf{s}^{II}_h$ is the same as the high-level state (or with the remaining budget info removed), the time horizon $T=b_h$ equals the decided budget, and the sub-action is to decide which node to select. 

% \begin{wrapfigure}{r}{0.65\textwidth}
%     \centering
%     \includegraphics[width=\linewidth]{./HRL-SSCO1.pdf}
%     \vspace{-6mm}
%     \caption{HRL for SSCO}
%     \label{fig:hrl-ssco}
%     \vspace{-5mm}
% \end{wrapfigure}


%The two levels of decision-making are interdependent, and training the two levels of policies $\pi^I$ and $\pi^{II}$ is done interactively. The dependency works in four-fold. First, when $\pi^I$ decides the high-level action $b_h$, it is used to decide the time horizon $T=b_h$ of the low-level decision, since the low-level decision is constrained by the budget set on the high-level. Second, the sub-state $\mathbf{s}^I_{h,1} = \mathbf{s}^{II}_h$ is initialized as the main state. Third, the immediate reward of the main time step $h$ is obtained by interacting with the environment using the accumulated sub-action $\mathbf{a}_h^{II}=\sum_{t=1}^{T=b_h}\mathbf{a}_{h,t}^{II}$, since the diffusion model needs input as to which nodes are selected, not just how many. This reward signal will be used to train $\pi^{II}$. Last, in the Bellman backup for the low-level policy $\pi^{II}$, we will use the value function $v_{\pi^{II}}$ of the high-level policy, as this yields a smaller variance compared to directly using the value function of the low-level policy.

\subsection{WS-option}
%\hp{Use the Algorithm acronym here. HERO4SSCO is too general. We should be more specific, e.g., two-stage-option, curriculum-option (curriculum learning in the sense of wake-sleep), etc.}

%To learn the budget allocation and node selection policies, we consider having both layers learn the corresponding $Q$-functions. We present the details of the algorithm step by step.

% First, show the equation.

Due to the high interdependencies between the learned Q-values across the two policy layers, along with advantages such as reduced sample complexity, we employ a value-based RL for both layers.
In our framework, the policies of both layers are $\epsilon$-greedy.



\begin{algorithm}[htbp]
\caption{WS-option: Overall training}
\label{algo:training}
\begin{algorithmic}[1]
\STATE Initialize experience buffer $\mathcal{M}_1$ and $\mathcal{M}_2$ for the higher and lower layer, respectively.
\STATE Initialize Q-networks for both layers.
\FOR{epoch $i=1,..,N$}
    \STATE Sample a Graph $G$ from the distribution $\mathbb{D}$.
    \FOR{episode $j=1$ \TO $M$}
        \IF{$i \leq N/2$}
            \STATE Set the flag \texttt{use\_fixed\_higher\_layer} to \texttt{true}.
        \ELSE
            \STATE Set flag \texttt{use\_fixed\_higher\_layer} to \texttt{false}.
        \ENDIF
        \STATE Run an episode with the flag \texttt{use\_fixed\_higher\_layer} (see Algorithm \ref{algo:run}).
        \STATE Store transitions (see Algorithm \ref{algo:store-trans}).
    \ENDFOR
    \STATE Sample a sequential minibatch of the most recent transitions from the replay buffer $\mathcal{M}_1$.
    \STATE Train the higher layer using MC methods with sampled transitions.
    \STATE Sample a random minibatch of transitions from the replay buffer $\mathcal{M}_2$.
    \STATE Train the lower layer using TD-learning with sampled transitions.
\ENDFOR
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[htbp]
\caption{WS-option: Run episode}
\label{algo:run}
\begin{algorithmic}[1]
\STATE Initialize the state $s^I_1$ and temporary storage $\mathcal{T}$ for episode transitions.
\FOR{step $t=1$ \TO $T$}
    \IF{\texttt{use\_fixed\_higher\_layer} is \texttt{True}}
        \STATE $o^I_t \gets \text{average budget allocation}$.
    \ELSE
        \STATE $o^I_t \gets \begin{cases} \text{randomly selected in the remaining budget}, \text{w.p.}\ \epsilon_1, \\ \arg\max_o q^I(s^I_t, o), \text{otherwise}. \end{cases}$
    \ENDIF
    \STATE Initialize the set of selected nodes $a^{II}_t = \{\}$.
    \FOR{$k=1$ \TO $o^I_t$}
        \STATE $s^{II}_{t,k}, o^{II}_{t,k}\gets \mathrm{state\ transition}(s^I_t,o_t^{I},  a^{II}_t)$
        \STATE $a^{II}_{t,k} \gets \begin{cases} \text{randomly select a node from } V \setminus a^{II}_t, \text{w.p.},\ \epsilon_2 \\ \arg\max_{a} q^{II}(s^{II}_{t,k}, o^{II}_{t,k}, a) \text{ subject to } a \notin a^{II}_t, \text{otherwise}. \end{cases}$
        \STATE $a^{II}_t \gets a^{II}_t \cup \{ a^{II}_{t,k}\}$
    \ENDFOR
    \STATE Execute action $a^{II}_t$ and obtain reward $r^I_t$ and next state $s^I_{t+1}$.
    % \STATE Store transition $(s^I_t, o^I_t, a^{II}_t, r^I_t, s^{I}_{t+1})$ in temporary storage $\mathcal{T}$ (using Algorithm \ref{algo:store-trans}).
    \STATE Store the transition $(s^I_t, o^I_t, a^{II}_t, r^I_t, s^I_{t+1})$ in the temporary storage $\mathcal{T}$.
    \STATE Update the current state: $s^I_t \gets s^{I}_{t+1}$.
\ENDFOR
\end{algorithmic}
\end{algorithm}

Moreover, as shown from the formulation of the MDPs for both layers, we can see that the Q-functions of the two layers are highly interdependent. 
On one hand, the higher-layer option selection will determine feasible action spaces for the lower-layer node selection; 
on the other hand, the specific lower-layer node selection will affect how good the higher-layer's selected option is. 
Hence, we observe that, if we simply adopt the commonly used (double) Q-learning framework \cite{watkins1992q,mnih2013playing,mnih2015human,hasselt2010double}, or more essentially off-policy TD learning \cite{sutton1988learning} for both layers, then the Q-values for both layers, especially the first layer will tend to diverge. 
This is because of the estimation bias derived from the deep models (see Appendix \ref{appendix:model}) and bootstrapping of TD learning. 
%\hp{can we show an image of this?} 
We can see the result in Figure \ref{fig:q-value2}.
To stabilize the training of both layers, while maintaining computational efficiency, we propose the following two designs for our training algorithm, i.e., wake-sleep training and layer-wise learning method selection. 
The pseudocode of the training algorithm is shown in Algorithm \ref{algo:training}. 
During each epoch, a graph is obtained from a certain distribution $\mathbb{D}$ (line 4). 
We divide all epochs into two stages (lines 6-10), which correspond to the wake-sleep training procedure described in this section. 
After we run algorithm \ref{algo:run} and \ref{algo:store-trans}, we can use the transitions to train both layers (lines 14-16). 
Note that during the training of the higher layer, transistions are sampled sequentially (line 14).
Although the higher-layer policy, based on MC method, is inherently on-policy and does not require stored transitions, we maintain this to ensure consistency across both layers.





% \begin{algorithm}
% \caption{HRL for SSCO: Store Transitions}
% \label{algo:store-trans}
% \begin{algorithmic}[1]
% \FOR{each transition $(s_t, o_t, a_t, r_t, s_{t+1})$ in $\mathcal{T}$}
%     \STATE Add $g$ to the transition.
%     \STATE Store transition $(s_t, o_t,g_t)$ in replay buffer $\mathcal{M}_1$ for the first layer.
%     \FOR{each node $u$ in $a_t$}
%         \STATE Calculate new state $s_{t,1}$ and $o_{t,1}$ based on $s_t$, $o_t$, and $a_t$.
%         \STATE Obtain marginal reward $r_{t,1}$ through simulation.
%         \STATE Scale the reward $r_{t,1}$.
%         \STATE Store transition $(s_{t,1}, o_{t,1}, u, r_{t,1}, s_{t+1,1})$ in replay buffer $\mathcal{M}_2$ for the second layer.
%     \ENDFOR
% \ENDFOR
% \end{algorithmic}
% \end{algorithm}

% \begin{algorithm}
% \caption{HRL for SSCO: Store Transitions}
% \label{algo:store-trans}
% \begin{algorithmic}[1]
% \FOR{each transition $(s_t, o_t, a_t, r_t, s_{t+1})$ in $\mathcal{T}$}
%     \STATE Add $g$ to the transition.
%     \STATE Store transition $(s_t, o_t, g)$ in replay buffer $\mathcal{M}_1$ for the first layer.
%     \STATE Initialize $s_\tau = s_t$ and $o_\tau = o_t$.
%     \FOR{each node $u$ in $a_t$}
%         \STATE Update state $s_{\tau+1}$ and $o_{\tau+1}$ based on $s_{inter}$, $o_{inter}$, and $u$.
%         \STATE Obtain marginal reward $r_{\tau}$ through simulation.
%         \STATE Scale the reward $r_\tau$.
%         \STATE Store transition $(s_{\tau}, o_{\tau}, u, r_{\tau}, s_{\tau+1}, o_{\tau+1})$ in replay buffer $\mathcal{M}_2$ for the second layer.
%         \STATE Set $s_{inter} = s_{inter+1}$ and $o_{inter} = o_{inter+1}$.
%     \ENDFOR
% \ENDFOR
% \end{algorithmic}
% \end{algorithm}



Algorithm \ref{algo:run} shows the process of running episodes. 
We first get the option (lines 3-6), then choose node one by one (lines 8-13). 
Subsequently, we run Algorithm \ref{algo:store-trans} (see Appendix \ref{appendix:algo3}) to store transitions for training.
Algorithm \ref{algo:store-trans} shows how to store transitions, especially for the lower layer. 
Since the second layer does not interact with the environment after selecting a single node and suffers from sparse rewards, we first obtain the marginal rewards for each transition state through simulation (lines 5-10). 
To keep the total reward of the lower layer consistent with the higher layer, we scale all the marginal rewards (lines 12-15), which does not affect the relative $Q$ value of the actions.


% \begin{algorithm}[tp]
% \caption{WS-option: Store transitions}
% \label{algo:store-trans}
% \begin{algorithmic}[1]
% \FOR{each transition $(s^I_t, o^I_t, a^{II}_t, r^I_t, s^{I}_{t+1})$ in $\mathcal{T}$}
%     \STATE Compute the gain $g_t=\sum_{i=t}^T \gamma^{i-1}r^I_i$.
%     \STATE Store the transition $(s^I_t, o^I_t, g_t)$ in the replay buffer $\mathcal{M}_1$.
%     \STATE Set $s^{II}_{\tau} \gets s^I_t$ and $o^{II}_{\tau} \gets o^{I}_t$.
%     \FOR{each node $u$ in the $a^{II}_t$}
%         \STATE $s^{II}_{\tau+1}, o^{II}_{\tau+1} \gets \text{state transition}(s^{II}_{\tau}, o^{II}_{\tau}, u)$  
%         \STATE Estimate the marginal reward $m^{II}_{\tau}$ by running 10 simulations.
%         \STATE Store the intermediate transition $(s^{II}_{\tau}, o^{II}_{\tau}, u, m^{II}_{\tau}, s^{II}_{\tau+1}, o^{II}_{\tau+1})$ for later scaling.
%         \STATE Update $s^{II}_{\tau} \gets s^{II}_{\tau+1}$ , $o^{II}_{\tau} \gets o^{II}_{\tau+1}$.
%     \ENDFOR
%     \FOR{each stored intermediate transition $(s^{II}_{\tau}, o^{II}_{\tau}, u, m^{II}_{\tau}, s^{II}_{\tau+1}, o^{II}_{\tau+1})$}
%         \STATE Scale the reward $r^{II}_{\tau}= \frac{m^{II}_\tau}{\sum m^{II}_k} (r^I_t - r^{II}_{t,\emptyset})$.
%         \STATE Store the transition $(s^{II}_{\tau}, o^{II}_{\tau}, u, r^{II}_{\tau}, s^{II}_{\tau+1}, o^{II}_{\tau+1})$ in the replay buffer $\mathcal{M}_2$.
%     \ENDFOR
% \ENDFOR
% \end{algorithmic}
% \end{algorithm}

\subsubsection{Wake-sleep training}
The primary challenge in HRL lies in ensuring stable training and model convergence. 
Notably, training both layers together can lead to instability, as the policies of each layer are interdependent and can interfere with one another.
To address this issue, we devise a wake-sleep approach to enhance training stability. 
As we will show in Section \ref{sec:convergence}, this proposed wake-sleep training procedure is well-aligned with our theoretical analysis.
In Figure \ref{fig:q-value1}, we present the Q value learned by traditional HRL, where both layers are trained simultaneously from the beginning, and our proposed WS-option.
Then, we will provide the details of this wake-sleep training procedure.
% Specifically, we show that when the higher layer is fixed, the lower layer will converge. 
% Similarly, if the lower layer is fixed and has already converged, the higher layer will also converge.
% However, simultaneous training of both layers poses significant challenges to convergence.
% Therefore, we first fix the higher layer during the \textit{sleep} stage.
% Then, we proceed to train both layers together, which ultimately shows convergence.


In the \textit{sleep} stage of this training paradigm, the Q-function of the higher layer is initially frozen and an average budget allocation strategy is used instead. 
While more advanced strategies could be considered, we choose the simplest one to allow the lower layer to approach convergence under the current high-layer policy.
The lower layer's Q-function is sufficiently trained along with the average budget allocation strategy in this stage. 
Meanwhile, the high layer's Q-function will be pre-trained offline using the trajectories of the average strategy. 
In the subsequent \textit{wake} stage, both layers are trained online simultaneously, allowing for the fine-tuning and optimization of their interdependent policies, which ultimately lead to convergence.
% Then, we enter the \textit{wake} stage where both layers are trained online at the same time. 

% This wake-sleep approach enhances training stability and convergence of the model. 
% We can stabilize the lower layer's policy in the sleep stage and prevent interference from an untrained higher layer. 
% Pre-training the higher layer simultaneously allows it to learn a good preliminary initialization of the model parameters that can guide the lower layer more effectively. 
% Training both layers together in the wake stage allows for fine-tuning and optimizing the interactions between the layers, leading to an overall more cohesive and effective policy. 
% %The training process is shown in Algorithm \ref{algo:training}.
% In fact, the intuition behind is based on the convergence analysis for our framework, which is shown at the end of this subsection.
% Specifically, we show that when the higher layer is fixed, the lower layer will converge. 
% Similarly, if the lower layer is fixed and has already converged, the higher layer will also converge.
% However, simultaneous training of both layers poses significant challenges to convergence.
% Therefore, we first fix the higher layer during the \textit{sleep} stage.
% Then, we proceed to train both layers together, which ultimately shows convergence.



\subsubsection{Layer-wise learning method selection} 
%\hp{We should use this property to name our algorithm}
%\hp{point to lines in ALg 1}
% \hp{Should also mention overestimation of Q by TD.}

% \hp{We should present an ablation study result for this (reviewers will probably ask.)}
% Because MC methods directly use the complete trajectory for Q-value update, they reduce intermediate errors of TD methods and yield a more stabilized training process. 
MC methods provide unbiased estimates by calculating the expected value via a complete trajectory, offering reliable and accurate results when the sample size is sufficient. In contrast, TD methods may reduce the accuracy of the Q estimation if errors occur during the intermediate process, known as error propagation. Additionally, in practice, TD methods often result in the overestimation of Q values. Hence, using MC methods to provide more reliable Q value estimates for the higher layer can enhance the model stability.
Our experimental results show that this trick, combined with wake-sleep training, is one of the key components that makes the HRL training successful. 
Figure \ref{fig:q-value2} shows the Q values learned using MC and TD methods. 
The Q values from TD methods are monotonic, while those from MC methods are concave.  




% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.48\textwidth]{./WS-ablation.pdf}
%     \includegraphics[width=0.44\textwidth]{./Q-value.pdf}
%     \caption{Q-values Learned by MC and TD Methods for AIM problem $(T=10, K=20)$}
%     \label{fig:q-value}
% \end{figure}

\begin{figure}[htbp]
    \subfigure[WS-option vs. traditional HRL]{
    \centering
    \includegraphics[width=0.48\textwidth]{./WS-ablation.pdf}
    \label{fig:q-value1}
    }\subfigure[MC methods vs. TD methods]{
    \centering
    \includegraphics[width=0.44\textwidth]{./Q-value.pdf}
    \label{fig:q-value2}
    }
    \caption{Q-values learned for the AIM problem $(T=10, K=20)$.}
\end{figure}



%the MC method shows excellent stability and convergence.

On the downside, MC methods are usually more data-hungry because they have to wait until the end of a trajectory to get a training sample. Given that the lower-layer Q-learning (which is essentially a subtask that uses RL to solve one-shot CO problems) has been demonstrated to be reliable ~\citep{khalil2017learning,nazari2018reinforcement,deudon2018learning,bengio2020machine,kool2018attention,berto2023rl4co}, we choose the off policy TD-learning (Q-learning) framework for the lower-layer Q-function update. 
TD-learning has a faster convergence speed compared to MC methods. When training in the sleep stage, the rapid feedback of the TD method can help the lower layer converge faster, so that the whole model will be better initialized with more optimized parameters. This in turn will accelerate the co-training in the wake stage.

%During training, MC method is used for the higher layer, mainly because MC method directly uses the complete returns at the end of each episode for updating, this approach reduces intermediate errors and ensures a stable training process. In hierarchical reinforcement learning, due to the existence of dependencies between the higher and lower layers, the whole system will be affected if one of the layers is unstable. In fact, in our experiments, the MC method shows excellent stability and convergence.

%\hp{Replace DQN with TD learning as TD is more fundamental.}
% For the lower layer, we use TD-learning for training. TD-learning has a faster convergence speed compared to MC methods. When training in the first stage, the rapid feedback of the TD method can help the lower layer converge faster, so that the whole model will be more stable when co-training in the second stage.

% \hp{Go over the algorithms. E.g., Lines xx-xx are doing this; Lines yy-yy are doing that. Our novelty lies in Lines zz-zz, where ... }\\
% \hp{Alg 2 should call Alg 3?}\\
% Algorithm \ref{algo:run} shows the process of running episodes, and Algorithm \ref{algo:store-trans} shows how to store transitions, especially for the second layer. Since the second layer does not interact with the environment after selecting a single node and suffers from sparse rewards, we first obtain the marginal rewards for each transition state through simulation. To keep the total reward of the lower layer consistent with the higher layer, we scale all the marginal rewards, which does not affect the relative $Q$ value of the actions.

% \begin{algorithm}
% \caption{HRL for SSCO}
% \begin{algorithmic}[1]
% \STATE Initialize experience buffer $\mathcal{M}_1$ for the first layer.
% \STATE Initialize experience buffer $\mathcal{M}_2$ for the second layer.
% \STATE Initialize Q-networks for both layers.
% \FOR{epoch $i=1,..,N$}
%     \STATE Obtain Graph $G$ from distribution $\mathbb{D}$.
%     \FOR{episode $j=1,...,M$}
%         \STATE Initialize state $s_0$.
%         \STATE Initialize temporary storage for episode transitions $\mathcal{T}$.
%         \FOR{step $t=1,...,T$}
%             \IF{$i \leq N/2$}
%                 \STATE $o_t \gets \text{average budget allocation}$
%             \ELSE
%                 \STATE $o_t \gets \begin{cases} \text{randomly selected in the remaining budget}, \text{w.p.}\ \epsilon \\ \arg\max_o Q_\Omega(s_t, o), \text{otherwise} \end{cases}$
%             \ENDIF
%             \STATE $a_t \gets \begin{cases} \text{random nodes $u \in V$ }, \text{w.p.}\ \epsilon \\ \arg\max_a Q_\Omega(s_t, a), \text{otherwise} \end{cases}$  
%             \STATE Execute action $a_t$ and observe reward $r_t$ and next state $s_{t+1}$.
%             \STATE Store transition $(s_t, o_t, a_t, r_t, s_{t+1})$ in temporary storage $\mathcal{T}$.
%             \STATE Update state $s_t \gets s_{t+1}$.
%         \ENDFOR
%         \STATE Calculate $g_t$ for all transitions in $\mathcal{T}$.
%         \FOR{each transition $(s_t, o_t, a_t, r_t, s_{t+1})$ in $\mathcal{T}$}
%             \STATE Add $g_t$ to the transition.
%             \STATE Store transition $(s_t, o_t, r_t, g_t, s_{t+1})$ in replay buffer $\mathcal{M}_1$ for the first layer.
%             \STATE Store transition $(s_t, o_t, a_t, r_t, s_{t+1})$ in replay buffer $\mathcal{M}_2$ for the second layer.
%         \ENDFOR
%     \ENDFOR
%     \STATE Sample random minibatch of transitions from replay buffer $\mathcal{M}_1$.
%     \STATE Train the first layer using MC methods with sampled transitions.
%     \STATE Sample random minibatch of transitions from replay buffer $\mathcal{M}_2$.
%     \STATE Train the second layer using TD-learning with sampled transitions.
% \ENDFOR
% \end{algorithmic}
% \end{algorithm}

\subsubsection{Convergence analysis}\label{sec:convergence}
For completeness of our study and to offer intuition for our framework, we present a concise theoretical analysis of the algorithm's convergence.
We first discuss the convergence of the intra-option policy and give Theorem \ref{theorem:1}.
Then, we show the convergence of the option policy and give the Theorem \ref{theorem:2}. 
The proofs are provided in Appendix \ref{proof:theorem1} and Appendix \ref{proof:theorem2}, respectively.
\begin{theorem}\label{theorem:1}
\textnormal{(Intra-option policy convergence)}. In our WS-option framework, given any Markov transition $(s_\tau, o_\tau, a_\tau, r_\tau, s_{\tau+1}, o_{\tau+1})$, the Q-value function $q^{II}(s_\tau, o_\tau, a_\tau)$ converges to the optimal Q-value function $q^{II}_*(s_\tau, o_\tau, a_\tau)$ with probability 1, assuming that the higher-layer policy is fixed.
\end{theorem}
% \begin{proof}
% See Appendix \ref{proof:theorem1}.
% \end{proof}

\begin{theorem}\label{theorem:2}
\textnormal{(Option policy convergence)}. In our WS-option framework, given any state-option pair $(s_t, o_t)$, the Q-value function $q^{I}(s_t, o_t)$ converges to the optimal Q-value function $q^{I}_*(s_t, o_t)$ with probability 1, assuming that for any given higher-layer policy, the lower-layer policy always provides the corresponding conditionally optimal response.
\end{theorem}
% \begin{proof}
% See Appendix \ref{proof:theorem2}.
% \end{proof}
As a result, the convergence of the hierarchical framework is guaranteed by the convergence of both layers. 
Although the interaction between layers will have some effects, we use the wake-sleep procedure instead. 
As illustrated in Figure \ref{fig:training}, after the sleep stage, we can get an optimal policy $\pi^{II}$ conditioned on the higher layer's policy $\pi^{I}$.
Notably, even when the higher layer policies differ, the nature of the node selection task leads to lower layer optimal policy $\pi^{II}$ being similar to any conditional optimal policy $\pi'^{II}$.
% Due to the nature of node selection task, optimal policy $\pi^II$ closely approximates any conditional optimal policy $\pi^{'II}$.
This is analogous to the Stackelberg game setting, where the follower can always give best response.
In this case, the convergence of the total framework can be achieved.
Notably, our analysis is just limited to the tabular case.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{./Training_process.pdf}
    \caption{Wake-sleep training procedure}
    \label{fig:training}
\end{figure}
\section{Experimental results}\label{sec:experiment}
\subsection{SSCO problem instances}

In this article, we examine two broad classes of problems: the propagation problem (high stochasticity) and the route planning problem (low stochasticity). 
For the propagation problem, we use an adaptive version of the classic IM problem. 
For the route planning (RP) problem, we consider a model tailored for travel route planning. 
% We provide a brief description of the adaptive IM (AIM) problem in the next. 
% The description of RP and the MDP formulations for AIM and RP are in the appendix.
% More details of both problens are in the appendix.
% The details of both problems are shown in Appendix \ref{description:aim} and Appendix \ref{description:rp}.
Notably, instead of showing the standard deviation of test results, we perform two-sample t-tests to show the effectiveness of our framework in SSCO scenarios.


\subsection{Comparison with baselines} \label{sec:baseline}
% \subsubsection*{Objective}
% Evaluate the performance and robustness of the proposed hierarchical reinforcement learning algorithm.
% \subsubsection*{Tasks}
% Conduct experiments in different settings
% \begin{itemize}
%     \item different sizes of graphs
%     \item different total budget and time horizon
%     \item different network dynamics (propagation parameters)
% \end{itemize}

% \subsubsection*{Baselines}
% \begin{itemize}
%     \item static seeding policy
%     \item heuristic seeding policy
%     \item score policy
%     \item degree policy 
% \end{itemize}

% \subsubsection*{Evaluation Metrics}
% \begin{itemize}
%     \item cumulative rewards (mean+std)
% \end{itemize}
% \hp{We need to use some case studies to show how the budget is allocated and how seeds are selected, like what we see in our meetings.} 

% \hp{We may also add training curves of the Q/loss to results/appendix.}

% \hp{What other results can be included? -- check past results.}

% \hp{Use bolded font for best results.}

\textbf{Baselines} \quad 
To evaluate the effectiveness of our approach, we compare it against appropriate baselines for each problem, measuring the cumulative reward obtained by various methods. 
Each baseline is named in the form of A-B, where A is the higher-layer algorithm, and B is the lower-layer algorithm.
For the AIM problem, we use the baselines mentioned in \citet{tong2020time}: average policy, static policy, and normal policy at the high level. 
The static policy divides the time period $T$ into $d$ cycles, allocating the same resources only on the first day of each cycle. 
The normal policy allocates all resources at the beginning. 
At the lower level, we use a degree-based greedy strategy and a well-designed score-based strategy. 
The score for each node $v$ is defined as $s_v = \sum_{u \in \mathcal{N}_{\mathrm{in}}(v)} p(u,v)$, where $\mathcal{N}_{\mathrm{in}}(v)$ denotes the set of inactive nodes among the neighbors of $v$, and $p(u,v)$ is the probability that $u$ can successfully activate $v$.
This method takes into account the characteristics of the problem and uses expected values to score each node, showing strong performance in our experiments. 
For the RP problem, we use genetic algorithms (GA) and greedy algorithms. 
We train and test on graphs of the same size here.
The results are shown in Tables \ref{tab:result_aim}-\ref{tab:result_rp}, where $T$ and $K$ are chosen according to \citet{tong2020time}.

% \begin{table}[h!]
%     \centering
%     \caption{Experimental results for both problems}
%     \label{tab:result1}
%     \resizebox{\textwidth}{!}{
%     \begin{tabular}{l|lccccccc|ccc}
%         \toprule
%         & & \multicolumn{6}{c}{AIM, $n=200$} & & \multicolumn{3}{c}{RP, $n=100$} \\
%         Setting & & \textbf{WS-option} & average/degree & average/score & K/degree & K/score & static/degree & static/score & \textbf{WS-option} & greedy & GA  \\
%         \midrule
%         $T,K=10,10$ & & 76.794±12.212 & 62.55±14.428 & 69.149±12.914 & 67.984±14.281 & 73.690±12.908 & 69.065±14.868 & 73.381±12.703 & 7.458±0.438 & 6.291±0.283 & 6.791±0.516 \\
%         $T,K=10,20$ & & 127.507±9.082 & 118.754±10.851 & 125.022±9.785 & 108.523±11.341 & 117.284±9.196 & 114.552±10.933 & 126.548±9.9452 & 12.858±0.602 & 12.024±0.872 & 11.647±1.636 \\
%         $T,K=10,30$ & & 129.058±8.449 & 122.502±9.408 & 128.287±7.996 & 109.387±9.052 & 118.776±6.294 & 122.369±9.940 & 128.006± 8.192 & 18.571±0.645 & 15.675±1.438 & 15.696±1.692 \\
%         $T,K=20,10$ & & 80.945±12.780 & 72.180±14.831 & 80.314±13.974 & 63.469±13.433 & 70.6754±13.931 & 70.567±15.812 & 71.677±14.376 & 6.970±0.359 & 6.733±0.478 & 6.925±0.542 \\
%         \bottomrule
%     \end{tabular}}
% \end{table}

% \begin{table}[h!]
%     \centering
%     \caption{Experimental results for AIM, $n=200$}
%     \label{tab:result_aim}
%     \resizebox{\textwidth}{!}{
%     \LARGE
%     \begin{tabular}{l|lccccccc}
%         \toprule
%         & & \multicolumn{6}{c}{AIM, $n=200$} \\
%         Setting & & \textbf{WS-option} & average/degree & average/score & K/degree & K/score & static/degree & static/score \\
%         \midrule
%         $T,K=10,10$ & & 76.79±12.21 & 62.55±14.43 & 69.15±12.91 & 67.98±14.28 & 73.69±12.91 & 69.07±14.87 & 73.38±12.70 \\
%         $T,K=10,20$ & & 127.51±9.08 & 118.75±10.85 & 125.02±9.79 & 108.52±11.34 & 117.28±9.20 & 114.55±10.93 & 126.55±9.95 \\
%         $T,K=10,30$ & & 129.06±8.45 & 122.50±9.41 & 128.29±8.00 & 109.39±9.05 & 118.78±6.29 & 122.37±9.94 & 128.01±8.19 \\
%         $T,K=20,10$ & & 80.95±12.78 & 72.18±14.83 & 80.31±13.97 & 63.47±13.43 & 70.68±13.93 & 70.57±15.81 & 71.68±14.38 \\
%         \bottomrule
%     \end{tabular}}
% \end{table}
% \hp{/ should be - }
\begin{table*}[ht!]
    \centering
    \caption{Experimental results for AIM, $n=200$. All cases have p-values $\le 0.05$.}
    \label{tab:result_aim}
    \begin{tabular}{l|cccccccc}
        \toprule
        Method & $T,K=10,10$ & $T,K=10,20$ & $T,K=10,30$ & $T,K=20,10$ \\
        \midrule
        \textbf{WS-option} & \textbf{76.00} & \textbf{118.56} & \textbf{129.06} & \textbf{80.95} \\
        average-degree & 67.92 & 104.54 & 122.50 & 72.18 \\
        average-score & 74.36 & 116.10 & 128.29 & 80.31 \\
        normal-degree & 69.28 & 101.50.34 & 109.39 & 63.47 \\
        normal-score & 75.05 & 111.89 & 118.78 & 70.68 \\
        static-degree & 70.02 & 105.25 & 122.37 & 70.57 \\
        static-score & 74.81 & 118.13 & 128.01 & 71.68 \\
        \bottomrule
    \end{tabular}
\end{table*}



% \begin{table}[ht!]
%     \centering
%     \caption{Experimental results for RP, $n=100$}
%     \label{tab:result_rp}
%     \begin{tabular}{l|lccc}
%         \toprule
%         & & \multicolumn{3}{c}{RP, $n=100$} \\
%         Setting & & \textbf{WS-option} & greedy & GA \\
%         \midrule
%         $T,K=10,10$ & & 7.46±0.44 & 6.29±0.28 & 6.79±0.52 \\
%         $T,K=10,20$ & & 12.86±0.60 & 12.02±0.87 & 11.65±1.64 \\
%         $T,K=10,30$ & & 18.57±0.65 & 15.68±1.44 & 15.70±1.69 \\
%         $T,K=20,10$ & & 6.97±0.36 & 6.73±0.48 & 6.93±0.54 \\
%         \bottomrule
%     \end{tabular}
% \end{table}

\begin{table*}[ht!]
    \centering
    \caption{Experimental results for RP, $n=100$. All cases have p-values $\le 0.05$.}
    \label{tab:result_rp}
    \begin{tabular}{l|cccc}
        \toprule
        Method & $T,K=10,10$ & $T,K=10,20$ & $T,K=10,30$ & $T,K=20,10$ \\
        \midrule
        \textbf{WS-option} & \textbf{7.46} & \textbf{12.86} & \textbf{18.57} & \textbf{7.52} \\
        greedy & 6.29 & 12.02 & 15.68 & 6.73 \\
        GA & 6.79 & 11.65 & 15.70 & 6.93 \\
        \bottomrule
    \end{tabular}
\end{table*}



As shown in Tables \ref{tab:result_aim}-\ref{tab:result_rp}, our method performs consistently well across various settings.
As expected, our method outperforms the baseline methods in the AIM problem, which emphasizes structural information about the graph as well as feedback processing of stochasticity (whether the node is successfully activated or not). 
In our experiments, we use simple graph neural networks. In fact, employing more advanced graph neural networks could further improve the performance of our algorithm, but graph embedding is not the focus of our study. 
For the RP, our algorithm also shows advantages. 
The graph of RP is essentially a fully connected graph, which does not emphasize the structural information of the graph but focuses more on the distance between nodes. 
Nonetheless, the primary advantage of our algorithm is its ability to be applied to larger graphs without the need for retraining after a single training, which significantly reduces computational overhead compared to traditional algorithms. 
Additionally, cumulative reward during training for AIM problem is provided in Figure \ref{fig:cumu-reward} of Appendix \ref{appendix:cumu reward}.

\subsection{Assessing the learned policies}
% \subsubsection*{Objective}
% Validate the appropriateness of the budget allocation policy learned by the algorithm.
% \subsubsection*{Tasks}
% \begin{itemize}
%     \item Conduct controlled experiments on small and simple graphs where the optimal policy is easily known.
%     \item Compare the learned policy against theoretical optimums to evaluate accuracy.
    
% \end{itemize}

We now assess the learned budget allocation and node selection policies using the AIM problem. 
By fixing one layer's policy, we compare the cumulative rewards obtained by only the other layer, enabling us to independently assess the policy of each layer.
%On the one hand, we will fix one layer and compare the cumulative rewards in the case of using only the other layer. In addition to this, we will also test on simple and smaller graphs to evaluate how good our strategies are by analyzing the specific choices made.

% \begin{table}[h!]
%     \centering
%     \caption{Evaluation result}
%     \label{tab:result2}
%     \resizebox{\textwidth}{!}{
%     \LARGE
%     \begin{tabular}{l|lcccc|ccc}
%         \toprule
%         & & \multicolumn{3}{c}{lower layer fixed (using the learned policy)} & & \multicolumn{3}{c}{higher layer fixed (using average policy)} \\
%         Setting &&  \textbf{WS-option} & average & K & static & \textbf{WS-option} & degree & score \\
%         \midrule
%         $T,K=10,10$ &&  76.79±12.21 & 71.45±12.76 & 75.27±15.22 & 74.85±12.57 & 71.45±12.76 & 62.55±14.43 & 69.15±12.91  \\
%         $T,K=10,20$ &&  127.51±9.08 & 126.35±8.84 & 120.26±8.53 & 125.46±9.47 & 126.35±8.84 & 118.75±10.85 & 125.02±9.79   \\
%         \bottomrule
%     \end{tabular}}
% \end{table}

\begin{table*}[ht!]
    \centering
    \caption{Cumulative rewards when varying one layer’s policy while the other layer remains fixed. All cases have p-values $\le 0.05$.}
    \label{tab:result2}
    \begin{tabular}{l|lcccc}
        \toprule
        & & \multicolumn{4}{c}{lower layer fixed (using the learned policy)} \\
        Setting && \textbf{WS-option} & average & normal & static \\
        \midrule
        $T,K=10,10$ && \textbf{76.79} & 71.45 & 75.27 & 74.85 \\
        $T,K=10,20$ && \textbf{127.51} & 126.35 & 120.26 & 125.46 \\
        \midrule
        & & \multicolumn{4}{c}{higher layer fixed (using the average policy)} \\
        Setting && \textbf{WS-option} & degree & score \\
        \midrule
        $T,K=10,10$ && \textbf{71.45} & 62.55 & 69.15 \\
        $T,K=10,20$ && \textbf{126.35} & 118.75 & 125.02 \\
        \bottomrule
    \end{tabular}
\end{table*}


To evaluate the higher layer, we fix the lower layer using its learned policy and vary the budget allocation policy. 
Conversely, to evaluate the performance of the lower layer, we fix the higher layer using the average budget allocation policy and vary the node selection policy. 
As shown in Table \ref{tab:result2}, our algorithm performs well even when applied to a single layer. 
Both the node selection and the learned budget allocation strategies yield better results compared to the baseline approach. The learned budget allocation policy is showed in Appendix \ref{appendix:allocation}, Figures \ref{fig:budget-allocation1} to \ref{fig:budget-allocation3}.


% \subsubsection*{Evaluation Metrics}
% \begin{itemize}
%     \item Seeding Policy
%     \item Cumulative Rewards
% \end{itemize}

\subsection{Generalization to larger graphs}
% \subsubsection*{Objective}
% Demonstrate the algorithm’s ability to generalize across different models and to larger graphs.
% \subsubsection*{Tasks}
% \begin{itemize}
%     \item Evaluate performance on different graph-based models.
%     \item Train the algorithm on small-scale graphs and test it on larger, more complex networks.
    
% \end{itemize}

% \subsubsection*{Baselines}
% \begin{itemize}
%     \item static seeding policy
%     \item heuristic seeding policy
%     \item score policy
%     \item degree policy 
% \end{itemize}

% \subsubsection*{Evaluation Metrics}
% \begin{itemize}
%     \item Cumulative Rewards
% \end{itemize}

Most excitingly, our algorithm generalizes well to larger graphs. 
This means that models trained on small graphs can be effectively applied to larger ones. 
The reason for this generalization ability lies in the framework we use, which does not include any parameters related to graph size or number of nodes. 
This allows us to use the same parameters for graphs of different sizes. 

% \begin{table}[h]
% \centering
% \caption{AIM: Generalization to larger graphs}
% \label{tab:result3}
% \resizebox{\textwidth}{!}{
% \LARGE
% \begin{tabular}{c|ccccccc}
% \toprule
% test size & \textbf{WS-option} & average/degree & average/score & K/degree & K/score & static/degree & static/score \\ \midrule
%    200  &  120.88±10.25 & 110.18±12.22 & 119.80±10.33 & 103.22±11.51 & 106.47±10.56 & 109.62±11.22 & 118.86±10.43 \\ 
%    500  &  194.68±28.25 & 162.69±31.34 & 177.44±29.66 & 183.41±32.54 & 192.25±29.12 & 162.73±30.22 & 186.10±28.37 \\ 
%    600  &  186.20±24.19 & 156.93±26.20 & 177.76±25.32 & 172.84±29.88 & 181.40±25.01 & 164.39±26.79 & 177.01±26.38 \\ \bottomrule
% \end{tabular}}
% \end{table}

% Backup for N=200, 500, 600
% \begin{table*}[h]
% \centering
% \caption{AIM: Generalization to larger graphs}
% \label{tab:result3}
% \begin{tabular}{l|ccc}
% \toprule
% & 200 & 500 & 600 \\ \midrule
% \textbf{WS-option} & \textbf{120.88±10.25} & \textbf{194.68±28.25} & \textbf{186.20±24.19} \\ 
% average-degree & 110.18±12.22 & 162.69±31.34 & 156.93±26.20 \\ 
% average-score & 119.80±10.33 & 177.44±29.66 & 177.76±25.32 \\ 
% normal-degree & 103.22±11.51 & 183.41±32.54 & 172.84±29.88 \\ 
% normal-score & 106.47±10.56 & 192.25±29.12 & 181.40±25.01 \\ 
% static-degree & 109.62±11.22 & 162.73±30.22 & 164.39±26.79 \\ 
% static-score & 118.86±10.43 & 186.10±28.37 & 177.01±26.38 \\ 
% \bottomrule
% \end{tabular}
% \end{table*}


% \begin{table*}[h]
% \centering
% \caption{AIM: Generalization to larger graphs}
% \label{tab:result3}
% \begin{tabular}{l|cccc}
% \toprule
% & 200 & 400 & 700 & 1000 \\ \midrule
% \textbf{WS-option} & \textbf{120.88±10.25} & \textbf{176.13±21.64} & \textbf{202.41±38.39} & \textbf{225.38±38.20} \\ 
% average-degree & 110.18±12.22 & 144.93±24.50 & 161.22±33.55 & 168.80±36.82 \\ 
% average-score & 119.80±10.33 & 158.98±22.93 & 168.59±35.13 & 177.34±36.51 \\ 
% normal-degree & 103.22±11.51 & 152.46±24.60 & 191.65±35.96 & 213.96±50.05 \\ 
% normal-score & 106.47±10.56 & 166.45±21.84 & 192.42±37.89 & 216.71±43.98 \\ 
% static-degree & 109.62±11.22 & 155.08±23.59 & 180.32±34.35 & 194.38±49.59 \\ 
% static-score & 118.86±10.43 & 171.91±21.22 & 190.00±33.82 & 199.76±41.42\\ 
% \bottomrule
% \end{tabular}
% \end{table*}

\begin{table*}[h]
\centering
\caption{AIM: Generalization to larger graphs. All cases have p-values $\le 0.05$.}
\label{tab:result3}
\begin{tabular}{l|cccc}
\toprule
& 200 & 400 & 700 & 1000 \\ \midrule
\textbf{WS-option} & \textbf{120.88} & \textbf{176.13} & \textbf{204.87} & \textbf{221.16} \\ 
average-degree & 110.18 & 144.93 & 155.85 & 167.58 \\ 
average-score & 119.80 & 158.98 & 165.24 & 176.31 \\ 
normal-degree & 103.22 & 152.46 & 186.01 & 210.57 \\ 
normal-score & 106.47 & 166.45 & 199.41 & 220.81 \\ 
static-degree & 109.62 & 155.08 & 178.66 & 198.44 \\ 
static-score & 118.86 & 171.91 & 188.44 & 204.16\\ 
\bottomrule
\end{tabular}
\end{table*}

We train the model on graphs with 50-100 nodes with $T=10, K=20$. Table \ref{tab:result3} and Table \ref{tab:result4} (see Appendix \ref{appendix:generalization}) present the results of our tests on different graph sizes. 
As shown, even though the model was trained on small graphs, it still exhibits excellent performance on larger graphs. 
This result demonstrates that our algorithm has good scalability and generalization ability to effectively deal with large-scale graph structure problems.

\section{Conclusion}
We propose an option-based HRL framework for solving complex SSCO problems. 
The core of our approach is to use the wake-sleep training procedure and layer-wise learning method selection to make the system more stable without interfering with each other, as well as to facilitate our learning process by giving a clear definition of MDPs of two layers. 
Through extensive experimental evaluations, we demonstrate the strong performance of our algorithm in solving the SSCO problem, its ability to learn effective budget allocation and node selection policies, and its strong generalization to larger graphs. 
However, a notable limitation of our algorithm is that each new problem needs a new graph embedding technique, which adds complexity to the framework's implementation across different scenarios.
We leave the investigation of model-agnostic graph embedding techniques for RL-based solutions to SSCO as future work, potentially borrowing ideas of recent advancements on graph foundation models \cite{liu2023towards}.



% \subsubsection*{Author Contributions}
% If you'd like to, you may include  a section for author contributions as is done
% in many journals. This is optional and at the discretion of the authors.

% \subsubsection*{Acknowledgments}
% Use unnumbered third level headings for the acknowledgments. All
% acknowledgments, including those to funding agencies, go at the end of the paper.


% \bibliography{iclr2025_conference}
% \bibliographystyle{iclr2025_conference}

% \appendix
% \section{Appendix}
% You may include other additional sections here.

\newpage

\section*{Acknowledgements}

This research was partially supported by the National Science Foundation under IIS-2348405. The authors acknowledge William \& Mary Research Computing for providing computational resources and/or technical support that have contributed to the results reported within this paper.

\bibliographystyle{iclr2025_conference}
\bibliography{HRL4CO,rl} 

\newpage
\appendix
\tableofcontents


\newpage

\section{Theoretical Insights}\label{appendix:insights}

\subsection{Hierarchical Learning for Bi-Level Optimization}
Based on the problem formulation in \eqref{eq:formulation}, we provide a high-level insight into the nature of this problem, which motivates the use of HRL. 
Specifically, the SSCO problem can be reformulated as a bi-level optimization problem:
\begin{align} 
&\max_{\pi^h} && J(\pi^I, \pi^{II*}(\pi^I)) \label{eq:higher_level_objective} \\
&\text{subject to} && \pi^{II*}(\pi^I) \in \arg\max_{\pi^I} J(\pi^I, \pi^{II}) \label{eq:lower_level_objective}
\end{align}
where $J=\sum\nolimits_{t=1}^{T} r_t(S_t)$, $\pi^I=\{K_t\}$, and $\pi^{II}=\{S_t\}$.
Here, $\pi^{I}$ is constrained by \eqref{eq:higher-layer constraint}, and $\pi^{II}$ satisfies the constraints in \eqref{eq:lower-layer constraint}. 

The formulation highlights that the SSCO problem inherently exhibits a bi-level optimization structure.
The higher-level policy $\pi^{I}$  determines the budget allocation strategy (e.g., $\{K_t\}$), while the lower-level policy $\pi^{II}$ selects the corresponding subsets (e.g., $\{S_t\}$) based on the higher-level decisions. 

Given this hierarchical dependency, it is natural and intuitive to employ a hierarchical learning algorithm such as HRL to address such problems effectively.

In contrast, traditional single-agent RL methods struggle with this bi-level optimization problem due to the following limitations:
\begin{itemize}
    \item \textbf{Hierarchical Dependency}: Single-agent RL lacks the structure to handle the interdependent objective, as the lower-level policy $\pi^{II}$ directly depends on higher-level decisions $\pi^I$.
    \item \textbf{Exploration Efficiency}: HRL separates exploration across two tractable levels, improving learning effiency compared to single-agent RL's flat exploration approach, which is one of the HRL's greatest advantages \citep{wen2020efficiency}.
    \item \textbf{Scalability and Interpretability}: Optimizing the joint action space of both levels in single-agent RL is computationally intractable. HRL, by decoupling the problem into manageable subproblems, offers a scalable, interpretable, and modular framework for solving complex hierarchical tasks.
\end{itemize}





\subsection{Intuition behind Wake-Sleep Option Framework} \label{sec:algo intuition}
The core idea for solving this bi-level optimization problem is to decompose it into two separate single-level optimization problems.

When the lower layer provides a conditionally optimal solution, denoted as $\pi^{II*}(\pi^I)$, the bi-level problem reduces to a single-layer problem, which corresponds to the objective defined in \eqref{eq:higher_level_objective}. 
In this context, the task is just to find the optimal policy for the higher layer.
Conversely, when the higher-layer policy is fixed, the problem becomes solving the sub-optimization objective in \eqref{eq:lower_level_objective}, which is to find the corresponding conditionally optimal lower-layer policy.
Building on this decomposition, the framework iteratively refines the higher-layer policy by alternating between the two optimization processes. 
Specifically, a higher-layer policy is first provided, and the corresponding conditionally optimal lower-layer policy is determined. 
This iterative procedure continues until the global optimal solution is achieved.

However, in the context of the SSCO problem, the role of lower layer is to perform node selection, while the higher layer only adjusts the termination condition. 
Due to the nature of this task, we claim that the conditionally optimal lower-layer policy remains similar across different higher-layer policies $\pi^I$.
Given this, the wake-sleep procedure is employed. 
During the sleep stage, the lower layer determines a near-optimal policy. 
Subsequently, the two layers are trained jointly during the wake stage. 
Since the lower layer has already converged to a close-to-optimal policy, the joint training further refines both the higher-layer policy and the lower-layer policy, ensuring that the global optimal solution is achieved.




\subsection{Simplification of Options in the Lower Layer}

In this section, we justify our simplification of options and provide a deeper understanding of the option framework within our HRL approach. 
First, we revisit the definition of an option $o$ in \ref{sec:option framework}. 
In the traditional option framework, an option $o$ is defined as a tuple $(I, \pi^{II}, \beta)$, where:


\begin{itemize}
    \item $I$ is the initiation set, which specify the states where the option can be initialized. 
    \item $\pi^{II}$ is the lower-policy that the agent follows while the option is being executed.
    \item $\beta$ is the termination condition that determines when the option should terminate.
\end{itemize}
Options allow the agent to execute temporally extended actions, thereby enabling more efficient exploration and decision-making by abstracting lower-level actions into higher-level policies.

\subsubsection{Why Simplify Options?}
In our specific bi-level optimization problem, the primary role of the option in the lower layer is to determine when the lower-level policy $\pi^{II}$ should terminate its node selection process. 
Specifically, the higher-level budget allocation $K_t$ implicitly decides the number of nodes the lower layer should select before terminating. 
Given this context, we simplify options due to the following reasons.

\paragraph{Redundant Information and Enhanced Interpretability}
The numerical value of the budget $K_t$ does not provide additional useful information beyond determining the number of node selections. Therefore, conveying the exact budget value to the lower layer is unnecessary and does not contribute to the termination decision. By representing options as binary indicators, we clarify whether the lower layer should continue or terminate, enhancing the interpretability of the policy.

\paragraph{Improved Learning Efficiency}
Reducing the option to a binary indicator allows the lower layer to focus solely on a straightforward termination condition, without needing to interpret specific budget values. This simplification minimizes the complexity of the learning task, leading to faster convergence and more stable learning processes.

Therefore, we simplify the option passed to the lower layer to a binary indicator:
\begin{itemize}
    \item \( o^{II}_t = 1 \): Indicates that the lower layer should continue selecting nodes.
    \item \( o^{II}_t = 0 \): Indicates that the lower layer should stop selecting nodes and interact with the environment.
\end{itemize}

\subsubsection{Justification of Budget Irrelevance}
It is important to note that the specific numerical value of the budget $K_t$    does not inherently affect the termination condition beyond determining the number of node selections. 
The budget serves as a constraint that limits the range of the lower layer's actions but does not need to be explicitly taken as part of the option. 
By abstracting the budget into a binary termination signal, we retain the essential functionality required for effective decision-making without introducing additional complexity.

Overall, this design choice strikes a balance between simplicity and functionality, leveraging the hierarchical structure of our problem to enhance learning efficiency and policy interpretability.









\subsection{Lower-Layer Reward Design }

The reward structure in the lower layer is a critical component of the WS-option framework, as it directly influence both the stability and convergence of the model.
Proper design of the reward ensures that the hierarchical optimization aligns seamlessly across layers, mitigating potential instability caused by sparse or inconsistent feedback signals.

\subsubsection{Challenges in Lower-Layer Reward Design}

The task of the lower layer is to select nodes based on the budget allocated by the higher layer. 
However, if the lower layer were to select all $K_t$ nodes in a single step, the size of the action space would grow combinatorially as $\binom{N}{K_t}$, where $N$ is the total number of (available) nodes.
This combinatorial explosion makes the optimization process computationally infeasible for large $N$ and $K_t$.
Therefore, we adopt a sequential selection approach where the lower layer selects one node at a time and repreat this process for $K_t$ times. 
This strategy effectively reduces the action space size from $\binom{N}{K_t}$ to $N$ per selection step, significantly enhancing the tractability of the optimization problem. 

However, this sequential approach introduces a new challenge: sparse rewards. 
Specifically, the environment only provides feedback after the final node in the sequence has been selected, resulting in delayed and aggregate reward signals. 
Such delayed feedback make it difficult for the model to identify the contribution of individual node selections to the overall reward.


\subsubsection{Normalization of Marginal Reward}

We normalize marginal rewards to ensure that the total reward of the lower layer for each time step $t$ is the same as the reward of the higher layer, which is
\begin{equation}
    \sum_{u \in a_t^{II}} r^{II}_{t,u} + r^{II}_{t, \emptyset} = r_t^I = R(s^I_t, o^I_t, a_t^{II}).
\end{equation}

At each time step, the reward $R(s^I_t, o^I_t, a_t^{II})$ is determined jointly by both the higher-layer option $o^I_t$ and the lower-layer action $a_t^{II}$. 
Since both layers share the same reward and objective, it is essential that the rewards remain aligned.

When the lower-layer actions are decomposed into sequential sub-actions (e.g., selecting one node at a time), the individual rewards for each sub-action must sum up to the total reward $R(s^I_t, o^I_t, a_t^{II})$.
Without normalization, the reward distribution across these sub-actions could deviate, leading to inconsistencies between the lower layer's objective and the higher layer's global reward signal.
As a result, by normalizing the marginal reward, we ensure: 1) reward consistency. The total reward for the lower layer matches the original reward defined by the hierarchical framework, maintaining coherence between the two layers; and then 2) objective alignment. Both the higher layer and lower layer optimize the same objective, preserving the shared purpose of achieving the overall framework's goals. 











\subsection{Convergence Analysis}\label{proof:convergence}
\subsubsection{Proof of Theorem 1} \label{proof:theorem1}
\begin{theorem*}
\textnormal{(Intra-option policy convergence)}. In our WS-option framework, given any Markov transition $(s_\tau, o_\tau, a_\tau, r_\tau, s_{\tau+1}, o_{\tau+1})$, the Q-value function $q^{II}(s_\tau, o_\tau, a_\tau)$ converges to the optimal Q-value function $q^{II}_*(s_\tau, o_\tau, a_\tau)$ with probability 1, assuming that the higher-layer policy is fixed.
\end{theorem*}
\begin{proof}
The update formula of intra-option Q-value in tabular form is as follows:
\begin{equation}
q^{II}_{n+1}(s_{t,\tau}, o_{t,\tau}, a_{t,\tau})=
\begin{cases}
r_{t,\tau} + \gamma\underset{a}{\max} q^{III}_{n}(s_{t,\tau+1}, o_{t,\tau+1}, a), \quad &\text{If $o_{t,\tau}>0$ and $a_{t,\tau}\ne a_{t,\phi}$}, \\
r_{t,\tau} + \gamma  q^{I}(s_{t+1}, o_{t+1}), &\text{otherwise.}
\end{cases}
\end{equation}
According to Theorem 1 from \cite{jaakkola1993convergence}, we need to show the update operator is a contraction mapping.
For $o_{t,\tau}>0$ and $a_{t,\tau}\ne a_{t,\phi}$,
\begin{equation}
    \begin{aligned}
&|q^{II}_{n+1}(s_{t,\tau},o_{t,\tau},a_{t,\tau}) - q_{*}^{II}(s_{t,\tau},o_{t,\tau},a_{t,\tau})|  \\
=& |r_{t,\tau} + \gamma\underset{a}{\max} q^{II}_{n}(s_{t,\tau+1}, o_{\tau+1}, a) - r_{t,\tau} - \gamma\underset{a}{\max} q^{II}_{*}(s_{t,\tau+1}, o_{t,\tau+1}, a)| \\
=&\gamma|\underset{a}{\max} q^{II}_{n}(s_{t,\tau+1}, o_{t,\tau+1}, a)-\underset{a}{\max} q^{II}_{*}(s_{t,\tau+1}, o_{t,\tau+1}, a)| \\
\leq& \gamma \underset{a}{\max} |q^{II}_{n}(s_{t,\tau+1}, o_{t,\tau+1}, a)-q^{II}_{*}(s_{t,\tau+1}, o_{t,\tau+1}, a)|
\end{aligned}
\end{equation}
For the other case, 
\begin{equation}
    \begin{aligned}
|q^{II}_{n+1}(s_{t,\tau},o_{t,\tau},a_{t,\tau}) - q_{*}^{II}(s_{t,\tau},o_{t,\tau},a_{t,\tau})| 
= |r_{t,\tau} + \gamma  q^{I}(s_{t+1}, o_{t+1}) - r_{t,\tau} - \gamma  q^{I}_{*}(s_{t+1}, o_{t+1})| \\
\end{aligned}
\end{equation}
Note that $q^{I}(s_{t+1},o_{t+1})=\underset{a}{\max}\ q^{II}(s_{t+1},o_{t+1},a)$. Therefore, we have
\begin{equation}
    \begin{aligned}
&|q^{II}_{n+1}(s_{t,\tau},o_{t,\tau},a_{t,\tau}) - q_{*}^{II}(s_{t,\tau},o_{t,\tau},a_{t,\tau})| \\=&\gamma|\underset{a}{\max} q^{II}_{n}(s_{t+1}, o_{t+1}, a)-\underset{a}{\max} q^{II}_{*}(s_{t+1}, o_{t+1}, a)| \\
\leq& \gamma \underset{a}{\max} |q^{II}_{n}(s_{t+1}, o_{t+1}, a)-q^{II}_{*}(s_{t+1}, o_{t+1}, a)| \\
\end{aligned}
\end{equation}
So we prove the convergence of the intra-option policy.
\end{proof}

\subsubsection{Proof of Theorem 2}\label{proof:theorem2}
\begin{theorem*}
\textnormal{(Option policy convergence)}. In our WS-option framework, given any state-option pair $(s_t, o_t)$, the Q-value function $q^{I}(s_t, o_t)$ converges to the optimal Q-value function $q^{I}_*(s_t, o_t)$ with probability 1, assuming that for any given higher-layer policy, the lower-layer policy always provides the corresponding conditionally optimal response.
\end{theorem*}
\begin{proof}
The Q-value of the higher layer is updated by:
\begin{align}
q^{I}_{t+1}(s_{t},o_{t})=q^{I}_{t}(s_{t},o_{t})-\alpha_{t}(s_{t},o_{t})(q^{I}_{t}(s_{t},o_{t})-g_{t}),
\end{align}
where $g_{t}$ is the return estimated by MC methods. 
Again, according to the Theorem 1 from \cite{jaakkola1993convergence}, to establish the convergence, we need to show that $\mathbb{E}[q^{I}_{t}(s_{t},a_{t})-g_{t}]=0$.
Under the assumption that for any given higher-layer policy, the lower-layer always provides the best-response policy, the lower-layer policy remains fixed and conditionally optimal at each time step $t$.
at each time step $t$, the lower policy is fixed and the conditionally optimal. 
Consequently, the expected value of the observed return $g_t$ matches the Q-value estimate $q^{I}_{t}(s_{t},o_{t})$. 
Therefore, the condition $\mathbb{E}[q^{I}_{t}(s_{t},o_{t}) - g_{t}] = 0$ is satisfied.
As a result, the Q-value function $q^{I}$ converges to the optimal option Q-value $q^{I}_{*}$ with probability 1.
\end{proof}


\subsubsection{Interaction and Convergence of Hierarchical Layers}

The theorems presented above establish the convergence of two separate single-level optimization problems, as discussed in Section \ref{sec:algo intuition}. 
However, ensuring the convergence of the entire framework for bi-level optimization remains challenging due to the inherent interdependence between the two layers.

To address this, we alternate between the two optimization processes and further use our WS option framework, which effectively decouples their dependencies during training.
Specifically, the higher-layer policy provides guidance to the lower-layer policy, which then adapts to the given conditions and converges to its conditionally optimal response. 
Once the lower layer converges, the higher-layer policy is refined based on the updated performance metrics.

This iterative approach leverages the individual convergence of each layer, as proven in Theorems in \ref{proof:theorem1} and \ref{proof:theorem2}, to ensure the overall framework converges. 
By combining the independent convergence of the two layers, the framework achieves global optimization for the bi-level problem.


\newpage
\section{Algorithm Details}\label{appendix:algorithm details}

\subsection{Algorithm 3} \label{appendix:algo3}
\begin{algorithm}[htp]
\caption{WS-option: Store transitions}
\label{algo:store-trans}
\begin{algorithmic}[1]
\FOR{each transition $(s^I_t, o^I_t, a^{II}_t, r^I_t, s^{I}_{t+1})$ in $\mathcal{T}$}
    \STATE Compute the gain $g_t=\sum_{i=t}^T \gamma^{i-1}r^I_i$.
    \STATE Store the transition $(s^I_t, o^I_t, g_t)$ in the replay buffer $\mathcal{M}_1$.
    \STATE Set $s^{II}_{\tau} \gets s^I_t$ and $o^{II}_{\tau} \gets o^{I}_t$.
    \FOR{each node $u$ in the $a^{II}_t$}
        \STATE $s^{II}_{\tau+1}, o^{II}_{\tau+1} \gets \text{state transition}(s^{II}_{\tau}, o^{II}_{\tau}, u)$  
        \STATE Estimate the marginal reward $m^{II}_{\tau}$ by running 10 simulations.
        \STATE Store the intermediate transition $(s^{II}_{\tau}, o^{II}_{\tau}, u, m^{II}_{\tau}, s^{II}_{\tau+1}, o^{II}_{\tau+1})$ for later scaling.
        \STATE Update $s^{II}_{\tau} \gets s^{II}_{\tau+1}$ , $o^{II}_{\tau} \gets o^{II}_{\tau+1}$.
    \ENDFOR
    \FOR{each stored intermediate transition $(s^{II}_{\tau}, o^{II}_{\tau}, u, m^{II}_{\tau}, s^{II}_{\tau+1}, o^{II}_{\tau+1})$}
        \STATE Scale the reward $r^{II}_{\tau}= \frac{m^{II}_\tau}{\sum m^{II}_k} (r^I_t - r^{II}_{t,\emptyset})$.
        \STATE Store the transition $(s^{II}_{\tau}, o^{II}_{\tau}, u, r^{II}_{\tau}, s^{II}_{\tau+1}, o^{II}_{\tau+1})$ in the replay buffer $\mathcal{M}_2$.
    \ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm}


\section{Implementation Details}\label{appendix:implementation details}

\subsection{Network architecture}\label{appendix:model}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{./Architecture1.pdf}
    \caption{Network architecture}
    \label{fig:architecture}
\end{figure}
% The network architecture consists of two primary components: the higher-level policy network and the lower-level policy network. 
% Overall, we use an encoder-decoder architecture. 
% Specifically, we use a GNN as the encoder to obtain node embeddings. 
% Subsequently, a multi-layer perceptron (MLP) is used to output the $Q$-values based on these embeddings and the context embedding, such as the current location node in the route planning (RP) problem. 
% Details of the model architecture are provided in the Appendix \ref{Model architecture}.
The network architecture consists of two primary components: the higher layer network and the lower layer network, as illustrated in Figure \ref{fig:architecture}. 
There are two inputs: the node feature $X$ and the context $c$. 
We use context $c$ to represent non-node feature information, such as global information $g$ and the option $o^I$, which is a one-dimensional vector. 
The basic idea is to use graph network layers to get node embeddings. 
Then, we concatenate the node embeddings with the context embedding, which are used as the input of linear layers.
% More details are provided in the Appendix \ref{Model architecture}.

%\hp{Talk about the different categories of info and how we design a ``simple'' model to capture them?}
% \hp{Briefly talk about the simple model architecture.}

% \textbf{Higher-level policy network with action-in}\quad 
\textbf{Higher layer network with action-in}\quad 
This network is used for estimating $q^I(s^I,o^I)$. 
It is worth noting that we adopt an action-in network structure, which differs from the traditional action-out structure in typical Q-learning frameworks like DQN \cite{mnih2013playing,mnih2015human}. 
In the action-in structure, actions are also taken as inputs. 
In our study, this means the option (budget) is an input. 
We choose the action-in structure for two main reasons: first, it can capture the properties of a particular state-action pair more accurately because of the ability to embed the action into potential features. 
More importantly, since the size of the budget is not used as a parameter of the network (i.e., the number of output nodes), this approach allows for more powerful generalization ability of the model, which is not possible with the action-out structure.



For the network architecture, we use graph network layers to obtain node embeddings $h$. 
An average readout function is then applied to get the graph embedding $\overline{h}$. 
After concatenating this with the context embedding, we use linear layers to compute $q^I(s^I,o^I)$.



% \textbf{Lower-level policy network with action-out}\quad 
\textbf{Lower layer network with action-out}\quad 
% The lower-layer network adopts a traditional "action-out" structure to estimate $Q^{II}(s^{II},a)$. After obtaining the node embeddings $h$ through the graph layers, we concatenate each node embedding $h_{i}$ with the context embedding $c$. Subsequently, a MLP is used to compute the Q-value for each node.
The lower layer network adopts a traditional "action-out" structure to estimate $q^{II}(s^{II}, o^{II}, a^{II})$. 
After obtaining the node embeddings $h$ through graph layers, we concatenate each node embedding $h_{i}$ with the context embedding $c$ (so we need to expand it first). 
Subsequently, we average the node embeddings to get the graph embedding, which is used to denote the null action.



\textbf{Details of the network}\quad
In our study, we use different architectures for two problems. 
For the AIM problem, we employ the S2V structure in \cite{khalil2017learning} with $T=3$ to obtain node embedding and graph embedding (64-dimensional) for the higher layer. 
Subsequently, we use a linear layer to get the embedding of the option (64-dimensional), which is also called context embedding. 
We then concatenate the graph embedding with the context embedding and apply three linear layers to get the Q-value. 
For the lower layer, we also use S2V structure but with $T=1$ to get both node embedding and graph embedding (64-dimensional). 
And using two linear layers for node embedding and graph embedding, respectively, to estimate the Q-value of all action ($n$ nodes and a null action). 
To tackle the RP problem, we use attention layers due to their effectiveness in capturing the features of distances and benefits associated with nodes.
For the higher layer, three attention layers with an embedding size of 64 are used to get the node and graph embeddings.
Additionally, for this problem, the context is the option, the current node and the starting node. 
We use a linear layer to get the option embedding and combine it with node embeddings of the current node and starting node to get the context embedding and concatenate them with the graph embedding.
The combined embedding is then passed through three linear layers to estimate the Q-value.
For the lower layer, a single attention layer is used to get the node embedding (64 dimensional). 
The context embedding is the same as the higher layer.
Notably, the embedding of the current node is employed as the representation for the null action instead of the graph embedding.
Finally, three linear layers are applied to get the Q-value.







\subsection{Adaptive influence maximization (AIM)}\label{description:aim}
AIM is an extension of IM \cite{kempe2003maximizing}, which aims to progressively select seed nodes in dynamic and uncertain environments to maximize the spread of information within a social network. 
AIM has diverse applications in fields such as viral marketing, information dissemination, and public health. 
In this paper, we use the Independent Cascade (IC) model as the information propagation model.


The problem is modeled on a directed graph $G=(V,E)$, where $V$ and $E$ are nodes and edges, respectively. Each node can be either active or inactive. Once a node $u$ becomes active (either by being selected as a seed or being activated by other nodes), it will attempt to activate each of its inactive neighbors $v$ with a probability $p(u,v)$ on the next day and cannot activate nodes afterwards. The probability $p(u,v)$ is constant and is set to $\frac{1}{d^{-}(v)}$ in our model, where $d^{-}(v)$ denotes the indegree of node $v$. Initially, all nodes are inactive, and before each day starts, we can select some seed nodes to activate. We assume a total budget $K$ (i.e., the total number of nodes that can be selected) and study how to allocate the seed nodes to maximize the influenced nodes within the time constraint of $T$ days \cite{tong2020time}.  
%The MDPs for AIM and RP can be found in the appendix. 

The MDP formulation of AIM is described in the following. The datasets used in the experiments are randomly generated using the \texttt{erdos\_renyi\_graph()} function from Python's NetworkX library, with a probability of $0.01$.

% \section{MDP formulation of AIM}
% For the higher level, the state represents whether each node is activated or not; the action is selecting the budget $k_{t}$ allocated for the day $t$; the reward is the number of newly activated nodes; and the transition is the change in state caused by the propagation process. In the IC model, activated nodes have a certain probability of activating their adjacent inactive nodes; for the lower level, the state still indicates whether each node is activated, but also includes the option chosen by the higher level. The action involves selecting a node, with an additional null action introduced to handle cases where the budget chosen by the higher level is zero, corresponding to not activating any node and only allowing the propagation process to occur. Specifically, to calculate the benefit of a particular node, we first determine its marginal reward $m_{v}$ through multiple simulations. Additionally, we need to account for the reward corresponding to the null action, which in this problem is $R_{t}-k_{eff}$, where $R_{t}$ is the total reward from the previous level and $k_{eff}$ is the number of nodes selected for activation on that day. We can then use $k_{eff}$ to scale the marginal reward $m_{v}$ to obtain the second level's reward.

% \section{MDP formulation of AIM}\label{mdp:aim}
% For example, given the current state $s_t^I$ and action $a_t^{II}$ (selected nodes) from the lower layer, we first change the state of selected nodes $a_t^{II}$. For each node $v \in a_t^{II}$, 
% \begin{equation}\label{eq:activate}
%     X_t^{v} = [0, 1, 0].
% \end{equation}
% Now we consider the interaction with environment. Assume the set of current active nodes is $C_{\text{a}, t}$ and the set of their inactive neighbors is $C_{\text{ina}, t}$. Then, for each node $v \in C_{\text{ina}, t}$, the probability of being activated is 
% \begin{equation}
%     P(X_t^v = [0,1,0])=1-\prod\nolimits_{u \in \mathcal{N}(v) \cap C_{\text{a}, t}}(1-p(u,v)).
% \end{equation}
% where $\mathcal{N}(v)$ is the set of neighbors of node $v$, and $p(u,v)$ is the probability that node $v$ is activated by node $u$. Then, we update the state of newly activated nodes $\mathcal{A}$ as in equation \ref{eq:activate}.
\textbf{MDP formulation} \quad
For the higher level, we denote the state of nodes as $X_t\in \{0,1\}^{|V|\times 3}$. 
The state of each node is represented as a one-hot vector, indicating that the node is in one of three possible states: inactive, active (able to activate other nodes), or removed (active but unable to activate other nodes).
What's more, the global information here is the remaining budget.
The option is the budget allocated to the current time step. 
The reward is the increase in the number of influenced nodes. 
For the state transition, for example, given the current state $s_t^I$ and action $a_t^{II}$ (selected nodes) from the lower layer, we first change the state of selected nodes $a_t^{II}$. For each node $v \in a_t^{II}$, 
\begin{equation}\label{eq:activate}
    X_t^{v} = [0, 1, 0].
\end{equation}
Now we consider the interaction with environment. Assume the set of current active nodes is $C_{\text{a}, t}$ and the set of their inactive neighbors is $C_{\text{ina}, t}$. Then, for each node $v \in C_{\text{ina}, t}$, the probability of being activated is 
\begin{equation}
    P(X_t^v = [0,1,0])=1-\prod\nolimits_{u \in \mathcal{N}(v) \cap C_{\text{a}, t}}(1-p(u,v)).
\end{equation}
where $\mathcal{N}(v)$ is the set of neighbors of node $v$, and $p(u,v)$ is the probability that node $v$ is activated by node $u$. Then, we update the state of newly activated nodes $\mathcal{A}$ as in equation \ref{eq:activate}.

For the lower level, the state remains consistent with the higher level but includes the option chosen by the higher level. 
The action is to select a node. 
The immediate reward is defined in Equation \ref{eq:reward}. 
The state transition occurs by simply setting the selected node to an active state.




\subsection{Route planning (RP)}\label{description:rp}
RP includes a broad class of optimization problems that seeks to find an optimal route to satisfy specific constraints and objectives. 
This problem is applicable to fields such as logistics, travel planning, and vehicle routing. 
In this paper, we focus on a travel planning scenario with \( N \) cities. 
The traveler must select \( K \) cities to visit, starting from a specific city. 
Each city \( c \) provides a profit \( p_c \), but the combined profit for visiting multiple cities in one day is a submodular function of these individual profits. 
The profit of each city changes dynamically over time and resets to 0 once visited. 
There is a maximum daily travel distance \( d_{\max} \), with penalties for exceeding it. 
The goal is to visit the selected cities within \( T \) days and return to the starting point, maximizing overall travel profits.

Specifically, the initial profit of each city follows from a uniform distribution $U(0.5,1)$ and updates as follows:
\begin{equation}\label{eq:RP update}
    p_{c, t+1} = p_{c, t} + \xi_{c, t+1}, \quad \text{for } t \ge 0,
\end{equation}
where $\xi_{c,t+1} \sim U(-0.05,0.05)$. For a set of cities $C$, the combined profit in one day is defined as 
\begin{equation}
    f(C) = \sum_{c \in C} p_c - \eta (\max(|C|-1, 0))^2,
\end{equation}
where $\eta$ is a coefficient set to 0.1 in our experiments. 
Exceeding the maximum daily travel distance $d_{\mathrm{max}}$ results in a penalty proportional to the excess distance (we choose 5). 
The $d_{\mathrm{max}}$ we set is $1.5 \max_{i,j} d_{ij}$, where $d_{ij}$ is the distance of city $i$ and $j$.
The datasets used in this problem (coordinates of the city) are also randomly generated using the \texttt{random.rand()} function from Python's Numpy library with a scale of 100.

% \section{MDP formulation of RP}{\label{description:3}}
\textbf{MDP formulation}\quad
For the higher level, the state includes: the coordinates of each city, their profits, the current city, and the starting city. 
The option is the same as in the AIM problem.
The reward is the combined profits of the selected cities at the current time step. 
The state transition involves the update of profits as described in Equation \ref{eq:RP update} and the change of the current city. 

For the lower level, the state remains consistent with the higher level but also includes the option chosen by the higher level. 
The action is to select a city. 
The marginal reward is simply the profit of the selected city, and the immediate reward can be obtained as described in Equation \ref{eq:reward}. Note that there is no propagation process, and the expected profit of each city does not change. 
Therefore, we can set $r^{II}_{t,\emptyset}$ to 0.
The state transition only involves the change in the current node and is deterministic.



The definitions of MDPs for these two problems are also shown in Table \ref{mdp}, and we will use the framework proposed in this paper to address these problems.

%\hp{Move this table to appendix. Use normal size.}
\begin{table*}[ht]
\centering
\caption{MDPs for AIM and RP problems}
\label{mdp}
\resizebox{\textwidth}{!}{
\LARGE
\begin{tabular}{c|c|c|c|c}
\hline
\textbf{Problem} &\textbf{Layer} &\textbf{State} &\textbf{Option/Action} &\textbf{Reward}  \\ 
\hline
\multirow{2}{*}{AIM} & Higher &active and inactive &choose a budget $k$  & number of new active nodes \\ 
\cline{2-5}
~ & Lower      &active and inactive, option &select a node    &scaled marginal reward of choosing a node \\ \hline
\multirow{2}{*}{RP} & Higher &coordinates, profits &choose a budget $k$  & combined profit of $k$ cities \\ 
\cline{2-5}
~ & Lower      &coordinates, profits, option &select a city &scaled profit \\ \hline
\end{tabular}
}
\end{table*}

% \begin{table}[ht]
% \centering
% \caption{RP: generalization to larger graphs}
% \label{tab:result4}
% \begin{tabular}{c|ccc}
% \toprule
% test size & \textbf{WS-option} & greedy & GA  \\ \midrule
%    200  &  14.446±0.626& 12.410±1.107&12.622±1.047\\ 
%    400  &13.483±0.653&10.741±1.222&10.963±1.224\\ 
%    700  &14.486±0.675& 11.372±1.230&11.367±1.288\\ 
%    1000  &  14.225±0.611
%  & 12.043±1.643
% &11.767±1.709      \\ \bottomrule
% \end{tabular}
% \end{table}

\newpage
\subsection{Experiment hyper-parameter}\label{hyper-parameter}
To ensure the stability of the learning process in both problems, we employ Double Deep Q-Networks with an update frequency of 10 episodes.
The $\epsilon$-greedy strategy we use is initialized with $\epsilon=0.9$ and decays by a factor of 0.98 after each single training, with a minimum threshold of 0.1. 
We train each model for $20$ epochs and $10$ episodes.

% \section{Compute resource}
All experiments on two COPs are implemented in Python and conducted on two machines. One is NVIDIA GeForce RTX 4090 GPU and Intel 24GB 24-Core i9-13900K. The other is NVIDIA V100 GPU and Inter 256 GB 32-Core Xeon E5-2683 v4.

\begin{table}[h!]
\centering
\caption{Experiment hyper-parameters for both problems}
\label{tab:hyper-parameters}
\begin{tabular}{|>{\centering\arraybackslash}p{4cm}|>{\centering\arraybackslash}p{4cm}|>{\centering\arraybackslash}p{4cm}|}
\hline
\textbf{Parameter} & \textbf{AIM} & \textbf{RP} \\ \hline
\text{Discount factor} ($\gamma$) & 0.99 & 0.99 \\ \hline
\text{Learning rate (higher-layer)} & $10^{-3}$ & $10^{-2}$ \\ \hline
\text{Learning rate (lower-layer)} & $10^{-4}$ & $10^{-4}$ \\ \hline
\text{Batch size} & 32 & 32 \\ \hline
\text{DDQN update frequency} & 10 episodes & 10 episodes \\ \hline
\text{Initial $\epsilon$ in $\epsilon$-greedy} & 0.9 & 0.9 \\ \hline
\text{Decay factor for $\epsilon$} & 0.98 & 0.98 \\ \hline
\text{Minimum $\epsilon$ threshold} & 0.1 & 0.1 \\ \hline
\text{Epochs} & 20 & 20 \\ \hline
\text{Episodes} & 10 & 10 \\ \hline
\text{Hardware} & \multicolumn{2}{c|}{\begin{tabular}[c]{@{}c@{}}NVIDIA GeForce RTX 4090, Intel 24GB 24-Core i9-13900K,\\ NVIDIA V100, Intel 256GB 32-Core Xeon E5-2683 v4\end{tabular}} \\ \hline
\end{tabular}
\end{table}


\subsection{Using T-Test for Evaluation}

To evaluate the performance of our model, we use a t-test rather than standard deviation. 
This approach is particularly suitable given the presence of both aleatoric and epistemic uncertainties.
Typically, we conduct multiple experiments to calculate the mean and standard deviation. 
Through this process, the epistemic uncertainty—arising from limited data—can be mitigated according to the law of large numbers. 
However, aleatoric uncertainty, which stems from inherent stochasticity in the environment, is irreducible and contributes to natural variability.
Consequently, the standard deviation in such scenarios primarily reflects aleatoric uncertainty. 
In our stochastic environment, this metric alone provides limited insight into the true performance of the model.


\section{Additional Results}


\subsection{Cumulative reward} \label{appendix:cumu reward}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=3.3in]{./reward.pdf}
    \caption{Cumulative reward during training for AIM $T=10, K=20$}
    \label{fig:cumu-reward}
\end{figure}

\subsection{Resource allocation} \label{appendix:allocation}
Figure \ref{fig:budget-allocation1}-- \ref{fig:budget-allocation3} shows the budget allocation for AIM problem. The last three days have been ignored due to none resource allocation.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=3.3in]{./budget_allocation1.pdf}
    \caption{Budget allocation for $T=10,K=10$}
    \label{fig:budget-allocation1}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=3.3in]{./budget_allocation2.pdf}
    \caption{Budget allocation for $T=10,K=20$}
    \label{fig:budget-allocation2}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=3.3in]{./budget_allocation3.pdf}
    \caption{Budget allocation for $T=10,K=30$}
    \label{fig:budget-allocation3}
\end{figure}



\subsection{Generalization to larger graphs} \label{appendix:generalization}

% \begin{table*}[ht]
% \centering
% \caption{RP: Generalization to larger graphs}
% \label{tab:result4}
% \begin{tabular}{l|cccc}
% \toprule
% & 200 & 400 & 700 & 1000 \\ \midrule
% \textbf{WS-option} & \textbf{14.45±0.63} & \textbf{13.48±0.65} & \textbf{14.49±0.68} & \textbf{14.23±0.61} \\ 
% greedy & 12.41±1.11 & 10.74±1.22 & 11.37±1.23 & 12.04±1.64 \\ 
% GA & 12.62±1.05 & 10.96±1.22 & 11.37±1.29 & 11.77±1.71 \\ 
% \bottomrule
% \end{tabular}
% \end{table*}

\begin{table*}[htbp]
\centering
\caption{RP: Generalization to larger graphs. All cases have p-values $\le 0.05$.}
\label{tab:result4}
\begin{tabular}{l|cccc}
\toprule
& 200 & 400 & 700 & 1000 \\ \midrule
\textbf{WS-option} & \textbf{14.45} & \textbf{13.48} & \textbf{14.49} & \textbf{14.23} \\ 
greedy & 12.41 & 10.74 & 11.37 & 12.04 \\ 
GA & 12.62 & 10.96 & 11.37 & 11.77 \\ 
\bottomrule
\end{tabular}
\end{table*}

\subsection{Evaluation on the Real-World Data}

Real-world datasets often exhibit certain patterns, such as clustering characteristics. 
These patterns are easier for our RL-based algorithm to recognize, which can, in turn, achieve even better results.
To demonstrate this, we evaluate our model on \textit{power2500} dataset, which has 2500 nodes.

\begin{table}[htbp]
\centering
\begin{tabular}{lrr}
\toprule
Method         & T=10, K=20 &  \\
\midrule
\textbf{WS-option}      &     \textbf{496.37} &  \\
average-degree &     314.49 &  \\
average-score  &     317.16 &  \\
normal-degree  &     449.30 &  \\
normal-score   &     462.54 &  \\
static-degree  &     350.84 &  \\
static-score   &     393.10 &  \\
\bottomrule
\end{tabular}
\caption{Performance comparison of methods on the \textit{power2500} dataset (T=10, K=20).}
\label{tab:method_comparison}
\end{table}

The results in Table~\ref{tab:method_comparison} show that our RL-based algorithm (WS-option) significantly outperforms baseline methods, especially under scenarios characterized by clustering patterns.
This demonstrates the model's capability to exploit structural properties in real-world datasets.

\subsection{Ablation Study for Sleep Stage with Different Training Time}

\begin{table}[htbp]
\centering
\begin{tabular}{lrr}
\toprule
Method                       & T=10, K=20 &  \\
\midrule
WS-option (sleep with 1/2 $N$) &     118.56 &  \\
WS-option (sleep with 1/3 $N$) &     104.84 &  \\
WS-option (sleep with 2/3 $N$) &     118.59 &  \\
average-degree               &     104.54 &  \\
average-score                &     116.10 &  \\
normal-degree                &     101.50 &  \\
normal-score                 &     111.89 &  \\
static-degree                &     105.25 &  \\
static-score                 &     118.13 &  \\
\bottomrule
\end{tabular}
\caption{Performance comparison under different training time for the sleep stage (T=10, K=20).}
\label{tab:sleep_stage_results}
\end{table}

Note that we use the same random seed as in \ref{sec:baseline}.
From the results, we observe that ensuring the lower layer converges during the sleep stage is crucial. 
Specifically, when the model trains with $\frac{1}{2}N$ or $\frac{2}{3}N$, the performance is nearly identical, indicating sufficient convergence. 
In contrast, training with $\frac{1}{3}N$ leads to significantly degraded performance, demonstrating that insufficient convergence in the lower layer undermines the overall results.

\subsection{Ablation Study for Simplified Option in the Lower Layer}

The experimental results for evaluating the effect of simplifying the option in the lower layer are presented in Table~\ref{tab:ablation_study}. 
We observe that using a simplified option achieves slightly better performance compared to the non-simplified version (118.56 vs. 116.37). 
This shows that while the non-simplified option can still yield good results with sufficient iterations, it requires more training for agents to accurately identify termination conditions. 
Specifically, termination occurs only when the budget equals 0; when the budget is greater than 0, the behavior remains the same across methods.

\begin{table}[htbp]
\centering
\begin{tabular}{lrr}
\toprule
Method                            & T=10, K=20 &  \\
\midrule
\textbf{WS-option}                         &     \textbf{118.56} &  \\
WS-option (non-simplified option) &     116.37 &  \\
average-degree                    &     104.54 &  \\
average-score                     &     116.10 &  \\
normal-degree                     &     101.50 &  \\
normal-score                      &     111.89 &  \\
static-degree                     &     105.25 &  \\
static-score                      &     118.13 &  \\
\bottomrule
\end{tabular}
\caption{Ablation study results for simplified options in the lower layer (T=10, K=20).}
\label{tab:ablation_study}
\end{table}

















\end{document}
