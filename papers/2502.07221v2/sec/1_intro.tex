\section{Introduction}
\label{sec:intro}
\begin{figure}
  \centering
    \includegraphics[width=0.5\textwidth]{fig1.pdf}
  \caption{Performance comparison across benchmark datasets, showing that our proposed MLLM4PUE achieves baseline methods across most of tasks. Weight F1 (wF1) is reported for classification task. For the retrieval and composed retrieval task, we report recall@5 scores. This highlights the advanced multimodal fusion capabilities of MLLM4PUE.}
  \label{fig1}
  \vspace{-2ex}
\end{figure}

Pathology remains the gold standard for diagnosing a wide range of diseases~\cite{xu2024multimodal,ma2024towards}. With advancements in artificial intelligence (AI), digital pathology has emerged as a promising field, using AI to address challenges across various tasks such as cancer diagnosis~\cite{lu2021data,sun2024context}, metastasis detection~\cite{zhang2023multi,khaliliboroujeni2022end}, mutation prediction~\cite{coudray2018classification,qu2021genetic}, and survival analysis~\cite{xiong2024mome,wang2024dual,zhou2024cohort}. These achievements, however, rely on models trained on large, well-labeled datasets. Given the hundreds of tumor types cataloged in the WHO classification\footnote{tumourclassification.iarc.who.int/} and the labor-intensive nature of data collection and annotation~\cite{erickson2022overview}, developing separate models for each pathology task is impractical, especially for rare diseases with limited data~\cite{kundra2021oncotree,lu2023visual}. Furthermore, while many efforts focus on image, pathology often involves text, including diagnostic reports and research papers~\cite{Che_WsiCaption_MICCAI2024,chen2025wsi,lu2024visual,guo2024histgen}. Thus, there is a growing need to shift toward task-agnostic models that generate universal multimodal embeddings capable of supporting different pathology tasks.

Due to the success of CLIP~\cite{radford2021learning} model, several studies~\cite{lu2023visual, huang2023visual, sun2024pathasst, ikezogwo2024quilt, lu2024visual} have adapted CLIP-based models for pathology, training on extensive image-text pairs to extract embeddings that enable zero-shot evaluation across various pathology tasks. However, these methods encounter two significant challenges. First, CLIP~\cite{radford2021learning} processes images and text independently, limiting its ability to capture complex interactions between modalities~\cite{jiang2024e5}.  Additionally, CLIP~\cite{radford2021learning} can only handle text inputs with a maximum length of 74 tokens, which restricts its ability to leverage longer and more detailed textual descriptions. Second, evaluations of these models have mostly centered on isolated tasks, such as classification or retrieval~\cite{lu2023visual, sun2024pathasst, ikezogwo2024quilt}, which separate image and text components and do not fully assess multimodal integration capabilities. Moreover, inconsistencies in datasets and sample selection criteria across studies lead to non-reproducible and incomparable results. These challenges emphasize the need for a more integrated model and a standardized benchmarking framework to advance multimodal embeddings in pathology.

To tackle the first challenge, we propose MLLM4PUE, a framework that uses a MLLM to generate universal multimodal embeddings for pathology. Unlike CLIP~\cite{radford2021learning}, which processes images and text separately, MLLM employs a transformer-based architecture to fully integrate these modalities, allowing it to learn from complex image-text relationships~\cite{liu2024visual, liu2024improved}. Moreover, MLLM can handle longer textual inputs compared to CLIP, overcoming the 74-token limitation and enabling richer contextual understanding, which is important in pathology area. To harness the capability of MLLM in generating embeddings, we take the following steps. First, we employ summarization prompts to guide the MLLM to output embeddings. Next, we fine-tune MLLM with a LoRA module using pathology image-text pairs, adapting it specifically for the pathology domain.

To address the second challenge, we introduce the Pathology Multimodal Embedding Benchmark (PMEB) to establish a standardized evaluation platform for multimodal embeddings in pathology. This benchmark encompasses three meta-task categories: retrieval, composed retrieval, and classification. To ensure the tasks effectively assess multimodal embeddings, we have reformulated them to better suit this purpose. Each task provides the model with prompts to process a query and identify the correct target from a set of candidates. These queries and targets can consist of images, text, or a combination of both, enabling a flexible and comprehensive evaluation of multimodal capabilities. PMEB offers a standardized and reproducible approach for assessing multimodal embeddings in pathology. Overall, our contributions are summarized as follows:
\begin{itemize}
\item We introduce MLLM4PUE, a framework converting pathology images and texts into robust embeddings within a unified model. This is the first time to explore the possibility of re-purposing MLLM for extracting embeddings in pathology area.
% This framework integrates multimodal information within a unified model, converting any combination of images and text into robust embeddings.

\item We provide PMEB, a comprehensive benchmark for evaluating multimodal embeddings in pathology, encompassing a broad range of tasks to support robust and generalizable assessment of these embeddings.

\item MLLM4PUE effectively represents multimodal information, consistently surpassing the performance of baseline models across amounts of evaluated tasks, as demonstrated by the quantitative results in Fig.~\ref{fig1}.

\item Our approach offers new insights into digital pathology by adopting MLLM for universal embeddings. Future research can build on this work to unify the development of models for diverse pathology tasks.
\end{itemize}

\begin{table*}[h!]
\small
\centering
\begin{tabular}{cccccccc}
\hline
Meta-Task&Data source&Query&Target&Original Tasks&Original samples&Selected samples\\
\hline
\multirow{4}{*}{Retrieval}&Arch-book~\cite{gamper2021multiple}& I & T & Image-text retrieval &1,306&1,306 \\ 
&Arch-book~\cite{gamper2021multiple}& T & I & Text-image retrieval&1,306&1,306 \\ 
&Arch-pubmed~\cite{gamper2021multiple}& I & T & Image-text retrieval&1,923&1,923 \\ 
&Arch-pubmed~\cite{gamper2021multiple}& T & I & Text-image retrieval&1,923&1,923 \\ 
\hline
Composed& Quilt-VQA~\cite{seyfioglu2024quilt} & I+T & T & Histopathology VQA&985&724\\ 
retrieval& Quilt-VQA-RED~\cite{seyfioglu2024quilt} & I+T & T & Histopathology VQA&335&252\\ 
\hline
\multirow{12}{*}{Classification}& Bach~\cite{aresta2019bach}& I & T & Breast tissue&399&399\\ 
& CRC-100k~\cite{kather2018100} & I & T & Colorectal cancer&100,000&1,000\\
& Databiox~\cite{kather2018100} & I & T & Colorectal cancer&922&922\\
& Digestpath~\cite{da2022digestpath} & I & T & Signet ring cell&455&455\\ 
& Digestpath~\cite{da2022digestpath} & I & T & Colonoscopy tissue &660&660\\ 
& Kathercolon~\cite{kather2019predicting} & I & T & Colorectal cancer tissue&7,180&1,000\\
& LC25000~\cite{borkowski2019lung} & I & T & Colon adenocarcinoma&10,000&1,000\\ 
& LC25000~\cite{borkowski2019lung} & I & T & Lung adenocarcinoma&15,000&1,000\\ 
& Osteo~\cite{arunachalam2019viable} & I & T & Osteosarcoma&1,144&1,144\\ 
& Renalcell~\cite{brummer2022integrative} & I & T & Renal tissue&36,687&1,000\\
& Sicap~\cite{silva2020going} & I & T & Gleason pattern &12,081&1,000\\ 
& Skincancer~\cite{kriegsmann2022deep} & I & T & Skin cancer tissue&129,369&1,000\\ 
& Wsss4luad~\cite{han2022wsss4luad} & I & T & LUAD tissue&10,091&1,000\\ 
\hline
\end{tabular}
% \vspace{-1ex}
\caption{The statistics of our PMEB benchmark: 15 original tasks from 14 datasets. I and T denote image and text, respectively.}
\vspace{-2ex}
\label{t1}
\end{table*}
\section{Related Work}
\textbf{Vision-language Models in Digital Pathology.}
In digital pathology, recent studies have adopted the CLIP~\cite{radford2021learning} model, leveraging paired image-text data within a contrastive learning framework to align similar image-text embeddings while separating dissimilar ones~\cite{lu2023visual,huang2023visual,sun2024pathasst,ikezogwo2024quilt,javed2024cplip}. For instance, PLIP~\cite{huang2023visual} fine-tunes CLIP on large collections of image-text pairs sourced from platforms like Twitter and other public datasets. Similarly, PathCLIP~\cite{sun2024pathasst} and QuiltNet~\cite{ikezogwo2024quilt} scale up pathology-specific data for training. CONCH~\cite{lu2024visual} employs over 1.17 million image-caption pairs to fine-tune CoCa~\cite{yu2022coca} model. Despite these advancements, these methods fine-tune either CLIP~\cite{radford2021learning} or CoCa~\cite{yu2022coca} using pathology image-caption pairs and rely on separate encoders for processing images and text. In contrast, we propose an MLLM-based framework to capture universal multimodal embeddings for pathology by integrating image and text data within a unified model. This integrated framework enhances performance across a wide range of digital pathology tasks, offering a more holistic and effective solution.

\noindent
\textbf{Multimodal Embedding Learning.} Creating effective multimodal embeddings remains a challenging area of research. CLIP ~\cite{radford2021learning} addresses this by employing separate encoders for images and text, aligning them in a common space through contrastive learning. This approach has influenced many subsequent models, such as BLIP~\cite{li2022blip} and CoCa~\cite{yu2022coca}. While these models excel in a variety of tasks, the use of distinct encoders restricts their capacity to fully integrate visual and textual data, which is a key requirement for tasks that involve combined visual-language inputs, such as search queries involving both images and text.

With the rise of large language models (LLMs), MLLMs have extended LLMs to process multiple data modalities, achieving notable progress in understanding and reasoning across diverse input types~\cite{liu2024visual, liu2024improved, li2024llava}. Although MLLMs exhibit strong capabilities in interpreting multimodal content and following complex instructions, there is still limited research on their application in creating embeddings. The recent work E5-V~\cite{jiang2024e5} fine-tunes an LLM with summarization prompts and text-only data to extract embeddings, then integrates a vision module to obtain multimodal embeddings for zero-shot multimodal retrieval tasks. However, for pathological adaptation, E5-V~\cite{jiang2024e5} requires a domain-specific LLM pretrained on pathology instruction data, which demands detailed and standardized annotations. In contrast, our framework, MLLM4PUE can fine-tune MLLM with a LoRA module using existing pathology image-text pairs, bypassing these prohibitive requirements.

