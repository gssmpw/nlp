\begin{table*}[h!]
\small
\centering
\begin{tabular}{ccccccccc}
\hline
\multirow{2}{*}{Task} & \multirow{2}{*}{Model} & \multicolumn{3}{c}{Arch-PubMed~\cite{gamper2021multiple}} & \multicolumn{3}{c}{Arch-book~\cite{gamper2021multiple}} \\ 
&& Recall@5 &Recall@10 & Recall@50 & Recall@5 & Recall@10 & Recall@50\\
\hline
\multirow{7}{*}{i2t} 
& E5-V~\cite{jiang2024e5} & 0.006 & 0.011 & 0.052 & 0.009 & 0.018 & 0.067\\ 
& \cellcolor{trainA} PLIP~\cite{huang2023visual} & \cellcolor{trainA} 0.037 & \cellcolor{trainA} 0.067 & \cellcolor{trainA} 0.185 & \cellcolor{trainA} 0.096 & \cellcolor{trainA} 0.152 & \cellcolor{trainA} 0.393\\ 
& \cellcolor{trainA} MLLM4PUE-O & \cellcolor{trainA} 0.105 & \cellcolor{trainA} 0.160 & \cellcolor{trainA} 0.366 & \cellcolor{trainA} 0.185 & \cellcolor{trainA} 0.264 & \cellcolor{trainA} 0.575\\ 
& \cellcolor{trainB} PathCLIP~\cite{ikezogwo2024quilt} & \cellcolor{trainB} 0.275 & \cellcolor{trainB} 0.388 & \cellcolor{trainB} 0.680 & \cellcolor{trainB} 0.152 & \cellcolor{trainB} 0.234 & \cellcolor{trainB} 0.482\\ 
& \cellcolor{trainB} MLLM4PUE-P & \cellcolor{trainB} \textbf{0.372} & \cellcolor{trainB} \textbf{0.495} & \cellcolor{trainB} \textbf{0.782} & \cellcolor{trainB} \textbf{0.192} & \cellcolor{trainB} \textbf{0.283} & \cellcolor{trainB} \textbf{0.603}\\ 
& \cellcolor{trainC} QuiltNet~\cite{ikezogwo2024quilt} & \cellcolor{trainC} 0.069 & \cellcolor{trainC} 0.111 & \cellcolor{trainC} 0.273 & \cellcolor{trainC} 0.116 & \cellcolor{trainC} 0.168 & \cellcolor{trainC} 0.384\\ 
& \cellcolor{trainC} MLLM4PUE-Q & \cellcolor{trainC} 0.122 & \cellcolor{trainC} 0.177 & \cellcolor{trainC} 0.407 & \cellcolor{trainC} 0.182 & \cellcolor{trainC} 0.248 & \cellcolor{trainC} 0.502\\ 
\hline
\multirow{7}{*}{t2i} 
& E5-V~\cite{jiang2024e5} & 0.006 & 0.011 & 0.052 & 0.009 & 0.018 & 0.067\\ 
& \cellcolor{trainA} PLIP~\cite{huang2023visual} & \cellcolor{trainA} 0.037 & \cellcolor{trainA} 0.067 & \cellcolor{trainA} 0.181 & \cellcolor{trainA} 0.112 & \cellcolor{trainA} 0.164 & \cellcolor{trainA} 0.419\\ 
& \cellcolor{trainA} MLLM4PUE-O & \cellcolor{trainA} 0.114 & \cellcolor{trainA} 0.173 & \cellcolor{trainA} 0.385 & \cellcolor{trainA} 0.193 & \cellcolor{trainA} 0.280 & \cellcolor{trainA} \textbf{0.600}\\ 
& \cellcolor{trainB} PathCLIP~\cite{ikezogwo2024quilt} & \cellcolor{trainB} 0.236 & \cellcolor{trainB} 0.348 & \cellcolor{trainB} 0.630 & \cellcolor{trainB} 0.137 & \cellcolor{trainB} 0.196 & \cellcolor{trainB} 0.445\\ 
& \cellcolor{trainB} MLLM4PUE-P & \cellcolor{trainB} \textbf{0.297} & \cellcolor{trainB} \textbf{0.399} & \cellcolor{trainB} \textbf{0.688} & \cellcolor{trainB} 0.185 & \cellcolor{trainB} 0.277 & \cellcolor{trainB} 0.555\\ 
& \cellcolor{trainC} QuiltNet~\cite{ikezogwo2024quilt} & \cellcolor{trainC} 0.056 & \cellcolor{trainC} 0.092 & \cellcolor{trainC} 0.237 & \cellcolor{trainC} 0.100 & \cellcolor{trainC} 0.152 & \cellcolor{trainC} 0.389\\ 
& \cellcolor{trainC} MLLM4PUE-Q & \cellcolor{trainC} 0.135 & \cellcolor{trainC} 0.193 & \cellcolor{trainC} 0.433 & \cellcolor{trainC} \textbf{0.210} & \cellcolor{trainC} \textbf{0.302} & \cellcolor{trainC} 0.584\\ 
\hline
\end{tabular}
  % \vspace{-1ex}
\caption{Performance comparison with baseline methods on two retrieval datasets, reporting recall metrics at different thresholds. Task i2t and t2i denote image-to-text and text-to-image retrieval, respectively. MLLM4PUE-O, MLLM4PUE-P, and MLLM4PUE-Q denote our model pre-trained on Openpath~\cite{huang2023visual}, PathCap~\cite{sun2024pathasst},  Quilt1M~\cite{ikezogwo2024quilt}, respectively. Bold values indicate the best performance. Same color represents models trained on the same dataset.}
\label{t2}
  % \vspace{-1ex}
\end{table*}
\section{Experiments}
\textbf{Training datasets.} 
We leverage three public datasets, Openpath~\cite{huang2023visual}, PathCap~\cite{sun2024pathasst} and Quilt1M~\cite{ikezogwo2024quilt}, to construct a robust training foundation for pathology image-text pairs. The Openpath dataset~\cite{huang2023visual} is assembled by gathering data from Twitter and LAION using popular pathology hashtags. Since PLIP\cite{huang2023visual} provided only Twitter IDs, we retrieve the corresponding data from Twitter and LAION using these IDs, resulting in a dataset of 138,874 image-text pairs after denoising. The PatchCap dataset~\cite{sun2024pathasst} comprises 207K high-quality samples from authoritative sources, and we use all provided image-text samples. Similarly, the Quilt1M dataset~\cite{ikezogwo2024quilt} aggregates pathology image-text pairs from several public sources. As many images in Quilt1M are associated with multiple captions, we concatenate the captions for each image to create comprehensive entries. Through these careful preparations, we compile an extensive training dataset consisting of 593,838 image-text pairs.

\noindent
\textbf{Baseline Methods and Metrics.}
We evaluate the performance of our proposed MLLM4PUE against several recent baseline models across various downstream tasks. Specifically, we compare it with three pathology CLIP-based models: PLIP~\cite{huang2023visual}, PathCLIP~\cite{sun2024pathasst}, and QuiltNet~\cite{ikezogwo2024quilt}, all of which are trained on the same dataset as our method. Additionally, we compare our approach with E5-V~\cite{jiang2024e5}, a method that fine-tunes an LLM with summarization prompts and text-only data to extract embeddings, then integrates a vision module to obtain multimodal embeddings for zero-shot multimodal retrieval tasks. For zero-shot classification tasks, we also compare our method with CONCH~\cite{lu2024visual}, a model trained on over 1.17 million private image-caption pairs, whereas our method is trained on fewer than 0.6 million pairs.

To evaluate the performance of retrieval and composed retrieval tasks, we use the Recall@K metric, which quantifies the proportion of relevant items contained within the top K retrieved results. For classification tasks, due to the imbalanced distribution of classes, we use the weighted F1 (wF1) score to assess performance. The weighted F1 is calculated by averaging the F1 scores for each class, with each class's score weighted by its frequency of occurrence.

\noindent
\textbf{Implementation Details.} Our framework is implemented in Pytorch and we use LLaVA-NeXT-8B~\cite{li2024llava} as our backbone MLLM. To save the GPU memory, we use QLoRA~\cite{dettmers2024qlora} and gradient checkpointing with DeepSpeed ZeRO-2 to fine-tune the MLLM, and all the images are resized to 336 Ã— 336 pixels. The temperature is set to 0.02. All experiments are run on six H100 GPUs with a gradient accumulation batch size of 144 and a learning rate of 4e-4.

\begin{table*}
\small
\centering
\resizebox{\textwidth}{!}{
% \begin{tabular}{c|l|cccccccccccccc}
%\begin{tabular}{c*{13}{>{\centering\arraybackslash}p{0.7cm}}}
\begin{tabular}{ccccccccccccccc}
\hline
Model & Bach & \begin{tabular}{c}Data\\biox\end{tabular} & \begin{tabular}{c}Digest\\(Colon)\end{tabular} & \begin{tabular}{c}Digest\\(Signet)\end{tabular} & \begin{tabular}{c}CRC\\100k\end{tabular} & \begin{tabular}{c}Kather\\7k\end{tabular} & \begin{tabular}{c}LC\\colon\end{tabular} & \begin{tabular}{c}LC\\lung\end{tabular} & Osteo & \begin{tabular}{c}Renal\\cell\end{tabular} & Sicap & \begin{tabular}{c}Skin\\cancer\end{tabular} & \begin{tabular}{c}Wsss\\4luad\end{tabular}\\
\hline
E5-V~\cite{jiang2024e5} & 0.100 & 0.162 & 0.197 & 0.049 & 0.224 & 0.206 & 0.333 & 0.209 & 0.300 & 0.051 & 0.230 & 0.017 & 0.132 \\
CONCH~\cite{lu2024visual} & 0.511 & 0.413 & 0.835 & 0.824 & 0.680 & 0.625 & 0.890 & 0.650 & 0.382 & 0.393 & 0.359 & \textbf{0.593} & \textbf{0.772}\\
\rowcolor{trainA} PLIP~\cite{huang2023visual} & 0.124 & 0.194 & 0.696 & 0.252 & 0.503 & 0.504 & 0.697 & 0.743 & 0.554 & 0.413 & 0.080 & 0.311 & 0.394 \\
\rowcolor{trainA} MLLM4PUE-O & 0.561 & \textbf{0.437} & 0.784 & 0.480 & 0.642 & 0.509 & \textbf{0.958} & 0.744 & 0.581 & 0.507 & 0.284 & 0.411 & 0.636\\
\rowcolor{trainB} PathCLIP~\cite{sun2024pathasst} & 0.528 & 0.282 & 0.785 & 0.707 & 0.611 & 0.492 & 0.937 & \textbf{0.887} & 0.668 & 0.359 & 0.267 & 0.332 & 0.648 \\
\rowcolor{trainB} MLLM4PUE-P & \textbf{0.577} & 0.388 & 0.799 & \textbf{0.900} & \textbf{0.763} & \textbf{0.783} & 0.937 & 0.886 & \textbf{0.751} & \textbf{0.543} & \textbf{0.502} & 0.504 & 0.754 \\
\rowcolor{trainC} QuiltNet~\cite{ikezogwo2024quilt} & 0.403 & 0.318 & 0.491 & 0.564 & 0.564 & 0.591 & 0.884 & 0.801 & 0.578 & 0.435 & 0.322 & 0.467 & 0.364 \\
\rowcolor{trainC} MLLM4PUE-Q & 0.437 & 0.434 & \textbf{0.868} & 0.851 & 0.686 & 0.668 & 0.886 & 0.866 & 0.613 & 0.516 & 0.440 & 0.507 & 0.758\\
\hline
\hline
E5-V~\cite{jiang2024e5} & 0.251 & 0.324 & 0.367 & 0.169 & 0.299 & 0.306 & 0.500 & 0.348 & 0.385 & 0.166 & 0.337 & 0.053 & 0.267 \\
CONCH~\cite{lu2024visual} & 0.559 & 0.444 & 0.832 & 0.802 & 0.703 & 0.644 & 0.890 & 0.665 & 0.439 & 0.447 & 0.405 & \textbf{0.607} & \textbf{0.783} \\
\rowcolor{trainA} PLIP~\cite{huang2023visual} & 0.251 & 0.306 & 0.708 & 0.253 & 0.532 & 0.541 & 0.721 & 0.749 & 0.551 & 0.488 & 0.179 & 0.290 & 0.450 \\
\rowcolor{trainA} MLLM4PUE-O & 0.569 & \textbf{0.448} & 0.793 & 0.429 & 0.666 & 0.546 & \textbf{0.958} & 0.775 & 0.624 & 0.518 & 0.426 & 0.453 & 0.671 \\
\rowcolor{trainB} PathCLIP~\cite{sun2024pathasst} & 0.556 & 0.386 & 0.784 & 0.666 & 0.610 & 0.489 & 0.937 & \textbf{0.888} & 0.698 & 0.350 & 0.362 & 0.378 & 0.688 \\
\rowcolor{trainB} MLLM4PUE-P & \textbf{0.599} & 0.386 & 0.812 & \textbf{0.892} & \textbf{0.767} & \textbf{0.784} & 0.937 & 0.886 & \textbf{0.744} & \textbf{0.576} & \textbf{0.509} & 0.496 & 0.760 \\
\rowcolor{trainC} QuiltNet~\cite{ikezogwo2024quilt} & 0.441 & 0.413 & 0.628 & 0.503 & 0.607 & 0.586 & 0.904 & 0.802 & 0.588 & 0.434 & 0.338 & 0.458 & 0.449 \\
\rowcolor{trainC} MLLM4PUE-Q & 0.479 & 0.441 & \textbf{0.866} & 0.849 & 0.695 & 0.661 & 0.887 & 0.872 & 0.609 & 0.494 & 0.444 & 0.520 & 0.761\\
\hline
\end{tabular}
}
  % \vspace{-1ex}
\caption{Performance comparison with baseline methods on zero-shot classification tasks, reporting weighted F1 (wF1) (top) and accuracy (bottom) metrics. MLLM4PUE-O, MLLM4PUE-P, and MLLM4PUE-Q denote our model pre-trained on Openpath~\cite{huang2023visual}, PathCap~\cite{sun2024pathasst},  Quilt1M~\cite{ikezogwo2024quilt}, respectively. Bold values indicate the best performance. Same color represents models trained on the same dataset.}
\label{t3}
  \vspace{-2ex}
\end{table*}
\section{Results}
\subsection{Zero-shot Cross-modal Retrieval}
We first evaluate our methods on the retrieval task on Arch-PubMed~\cite{gamper2021multiple} and Arch-book datasets~\cite{gamper2021multiple}, where the query is an image or text, with the target being the corresponding paired text or image. Recall@k metric is employed to assess the model's ability to retrieve relevant pathology image-text pairs among top k results. As shown in Table \ref{t2}, our models consistently outperform the baseline models, especially at higher recall values. Notably, in the image-to-text (i2t) task on Arch-book dataset~\cite{gamper2021multiple}, our MLLM4PUE trained on Openpath dataset~\cite{huang2023visual} achieves a Recall@5 of 0.185, indicating an approximate 9\% improvement over PLIP~\cite{huang2023visual}. Moreover, our PathCap-pretrained model surpasses the performance of PathCLIP~\cite{sun2024pathasst} on both benchmark datasets. A similar enhancement is observed in the text-to-image (t2i) retrieval task. For instance, on the Arch-PubMed dataset~\cite{gamper2021multiple}, our model achieves a Recall@10 of 0.193, marking a 10\% increase in performance. These results collectively demonstrate that our framework excels at capturing multimodal embeddings more effectively than CLIP-based models with the same training data. Furthermore, it is important to note that the E5-V model~\cite{jiang2024e5} yields significantly lower recall scores across all tasks and datasets due to the lack of fine-tuning with pathology-specific data.

\begin{table*}[h!]
\small
\centering
\begin{tabular}{cccccccc}
\hline
\multirow{2}{*}{Model} & \multicolumn{3}{c}{Quilt-VQA~\cite{seyfioglu2024quilt}} & \multicolumn{3}{c}{Quilt-VQA-RED~\cite{seyfioglu2024quilt}} \\ 
& Recall@5 &Recall@10 & Recall@50 & Recall@5 & Recall@10 & Recall@50\\
\hline
E5-V~\cite{jiang2024e5} & 0.214 & 0.323 & 0.495 & 0.351 & 0.441 & 0.668\\ 
\rowcolor{trainA}PLIP~\cite{huang2023visual} & 0.191 & 0.224 & 0.349 & 0.250 & 0.286 & 0.500\\ 
\rowcolor{trainA}MLLM4PUE-O & 0.239 & 0.334 & 0.583 & 0.373 & 0.460 & 0.790\\ 
\rowcolor{trainB}PathCLIP~\cite{ikezogwo2024quilt} & 0.192 & 0.242 & 0.413 & 0.262 & 0.337 & 0.603\\ 
\rowcolor{trainB}MLLM4PUE-P & 0.218 & 0.329 & 0.541 & 0.363 & 0.468 & 0.790\\ 
\rowcolor{trainC}QuiltNet~\cite{ikezogwo2024quilt} & 0.180 & 0.209 & 0.336 & 0.222 & 0.274 & 0.460\\ 
\rowcolor{trainC}MLLM4PUE-Q & \textbf{0.475} & \textbf{0.598} & \textbf{0.823} & \textbf{0.675} & \textbf{0.754} & \textbf{0.948}\\ 
\hline
\end{tabular}
\caption{Zero-shot composed retrieval performance on the Quilt-VQA~\cite{seyfioglu2024quilt} and Quilt-VQA-RED~\cite{seyfioglu2024quilt} datasets, with Recall metrics reported at various thresholds. MLLM4PUE-O, MLLM4PUE-P, and MLLM4PUE-Q denote our model pre-trained on Openpath~\cite{huang2023visual}, PathCap~\cite{sun2024pathasst},  Quilt1M~\cite{ikezogwo2024quilt}, respectively. Bold values indicate the best performance. Same color represents models trained on the same dataset.}
\label{t4}
\vspace{-2ex}
\end{table*}

\subsection{Zero-shot Patch Classification}
We then evaluate the performance of zero-shot classification on patch-level pathology images across 13 classification tasks from our PMEB. In this downstream task, each query is an image, and the target is an extended sentence of the class name as described in Section~\ref{2.4}. Table~\ref{t3} illustrates that our MLLM4PUE consistently surpasses baseline models on most datasets. Notably, with the same training data, MLLM4PUE-O outperforms PLIP~\cite{huang2023visual} across all datasets. Similarly, MLLM4PUE-Q achieves the best performance compared to QuiltNet~\cite{ikezogwo2024quilt}, demonstrating the effectiveness of our pretraining strategy on the same training dataset. While CONCH~\cite{lu2024visual} attains the highest score on SkinCancer~\cite{kriegsmann2022deep} and Wsss4luad~\cite{han2022wsss4luad}, its superior performance stems from utilizing a much larger private dataset (1.16M). In contrast, our model, MLLM4PUE-P, outperforms all baseline models across seven datasets with only 0.2 million training samples, and is only 0.02 less than CONCH~\cite{lu2024visual} on the Wsss4luad dataset~\cite{han2022wsss4luad}. This underscores the advantages of leveraging vision-language alignment in pathology representation learning. These findings collectively highlight the effectiveness of our approach, especially considering that our models were trained on fewer samples, further demonstrating their data efficiency in pathology-specific zero-shot classification. The accuracy metric shows a similar trend, where MLLM4PUE-P outperforms the baselines on most datasets.
% , such as achieving 0.599 accuracy on the Digest Colon task compared to 0.559 for CONCH.

% We then evaluate the performance of zero-shot classification on patch-level pathology images on 13 classification tasks from our PMEB. In this downstream task, each query is an image, and the target is an extended sentence of the class name as described in Section~\ref{2.4}. Table~\ref{t3} illustrates that our MLLM4PUE consistently surpasses baseline models at most dataset. Notably, with the same training data, MLLM4PUE-O outperforms PLIP~\cite{huang2023visual} all compared methods on all the dataset. Similarly, MLLM4PUE-Q achieves the best performance compared with QuiltNet~\cite{ikezogwo2024quilt}. While CONCH~\cite{lu2024visual} attains the highest score on Skincancer~\cite{kriegsmann2022deep} and Wsss4luad~\cite{han2022wsss4luad}, its superior performance likely stems from utilizing a larger private dataset (1.16M). Our model, MLLM4PUE-P surpass all baseline across seven benchmarks with only 0.2 million dataset, underscoring the benefits of leveraging vision-language alignment in pathology representation learning. These findings collectively highlight the effectiveness of our approach, especially given that our models were trained on less samples, further demonstrating their data efficiency in pathology-specific zero-shot classification. The accuracy metric also shows the same trend.
\subsection{Zero-shot Composed Retrieval}
We further evaluate the performance of composed retrieval on two datasets, Quilt-VQA~\cite{seyfioglu2024quilt} and Quilt-VQA-RED~\cite{seyfioglu2024quilt}, where the task involves using an image and a question as the query to retrieve the correct answer. For our model and E5-V~\cite{jiang2024e5}, we apply the prompt \texttt{<image>\textbackslash n <question>\textbackslash n Summarize above H\&E image and question in one word:} to integrate the image and the question. For the other three CLIP-based baseline models, we add the image and question embedding directly since these models handle image and text separately. The Recall@k metric is employed to assess how well the model can identify the correct answer within the top k candidates.
As shown in Table \ref{t3}, our proposed MLLM4PUE achieves better results compared to baseline methods. Specifically, on the Quilt-VQA dataset~\cite{seyfioglu2024quilt}, MLLM4PUE achieves an increase in Recall@5 by approximately 4\%, 3\%, and 30\% over PLIP~\cite{huang2023visual}, PathCLIP~\cite{sun2024pathasst}, and QuiltNet~\cite{ikezogwo2024quilt}, respectively. Similarly, on the Quilt-RED-VQA dataset~\cite{seyfioglu2024quilt}, MLLM4PUE pre-trained on Quilt1M~\cite{ikezogwo2024quilt}, has achieved a Recall@5 of 0.675, which represents the best performance. Interestingly, the E5-V model~\cite{jiang2024e5} exhibits superior performance compared to all CLIP-based models, despite lacking fine-tuning on pathology-specific data. This demonstrates the robust capacity of our method in multimodal fusion, highlighting its proficiency in handling diverse data inputs effectively.

\subsection{Visualization of Modality Gap}
To illustrate the modality gap, we employed t-SNE visualization to compare the embeddings of our MLLM4PUE-P model with those of PathCLIP~\cite{sun2024pathasst}, both trained on the Patchcap dataset~\cite{sun2024pathasst}. As shown in Fig 4, the qualitative analysis clearly shows that the embeddings produced by MLLM4PUE-P form tighter clusters in the low-dimensional space, whereas those from PathCLIP~\cite{sun2024pathasst} are more dispersed. This difference reflects a significant improvement in cross-modal alignment of our method.

\begin{figure}[!h]
  \centering
\includegraphics[width=0.9\linewidth]{r1.pdf}
  % \vspace{-2ex}
   \caption{Distribution of image and text embeddings on Arch-PubMed~\cite{seyfioglu2024quilt} and Arch-Book~\cite{seyfioglu2024quilt} dataset.}
   \label{fig:onecol}
\vspace{-2ex}
\end{figure}
To further quantify the modality gap, we adopted the metric $||\Delta||_{gap}$ 
as proposed in Liang et al. ~\cite{liang2022mind}. Our experiments reveal that, on the Arch-PubMed dataset~\cite{seyfioglu2024quilt}, our method reduces the gap from 0.9284 to 0.2587, and on the Arch-Book dataset~\cite{seyfioglu2024quilt}, from 1.0408 to 0.3739. These results provide strong evidence that our model can significantly narrow the gap between image and text embeddings.
\subsection{Ablation Studies}

\begin{table*}[h!]
\small
\centering
\begin{tabular}{cccccccc}
\hline
\multirow{2}{*}{Model}& & \multicolumn{3}{c}{Quilt-VQA~\cite{gamper2021multiple}} & \multicolumn{3}{c}{Quilt-VQA-RED~\cite{gamper2021multiple}} \\ 
&& Recall@5 &Recall@10 & Recall@50 & Recall@5 & Recall@10 & Recall@50\\
\hline
\multirow{2}{*}{QuiltNet~\cite{ikezogwo2024quilt}}& only question & 0.170 & 0.189 & 0.275 & 0.187 & 0.234 & 0.440\\ 
& add embedding & 0.180 & 0.209 & 0.336 & 0.222 & 0.274 & 0.460\\ 
\hline
\multirow{3}{*}{MLLM4PUE}&only question & 0.291 & 0.362 & 0.539 & 0.429 & 0.488 & 0.694\\ 
& add embedding& 0.394 & 0.500 & 0.746 & 0.520 & 0.619 & 0.909\\ 
&prompt & \textbf{0.475} & \textbf{0.598} & \textbf{0.823} & \textbf{0.675} & \textbf{0.754} & \textbf{0.948}\\
\hline
\end{tabular}
\caption{Ablation study of image and question embedding fusion methods on composed retrieval task. All models are trained on the Quilt1M~\cite{ikezogwo2024quilt} dataset. Recall metrics are reported. Bold values indicate the best performance.}
\label{t6}
\vspace{-2ex}
\end{table*}
\textbf{Robustness of PMEB.} Since some datasets in our PMEB are super large, we choose a random sample of 1,000. To assess the robustness of our benchmark, we conduct an ablation study by computing the wf1 scores using 1,000, 3,000, 5,000, and all available samples for each dataset. As shown in Figure~\ref{fig5}, the performance change between 1,000 and the full dataset remains within a small margin across all datasets, confirming that using only 1,000 samples provides a reliable approximation of the complete dataset's performance. Given that the performance difference is typically within 0.01 to 0.04 across different sample sizes, using 1,000 samples significantly reduces computational costs while maintaining reliable performance. This makes PMEB highly efficient for large-scale experiments without sacrificing meaningful evaluation. We also report the result of accuracy metric and the results in CONCH~\cite{lu2024visual} and PathCLIP~\cite{sun2024pathasst} model in the Supplementary Materials, which shows the same trend.
\begin{figure}[!h]
  \centering
\includegraphics[width=\linewidth]{chart.pdf}
\vspace{-4ex}
   \caption{Ablation study on the sample numbers of our benchmark on our MLLM4PUE-P model. We computed the wF1 scores using 1,000, 3,000, 5,000, and all available samples for each dataset.}
   \label{fig5}
   \vspace{-2ex}
\end{figure}

\noindent
\textbf{Embedding Fusion Methods.} This experiment is designed to validate the quality of our benchmark and highlight the effectiveness of our MLLM4PUE in modality fusion compared to CLIP-based model in composed retrieval tasks. As shown in Table~\ref{t6}, MLLM4PUE outperforms QuiltNet~\cite{ikezogwo2024quilt} in every setting. First, comparing ``only question'' results, our MLLM4PUE shows significant improvement compared with QuiltNet~\cite{ikezogwo2024quilt}, while both models demonstrate substantial performance gains when incorporating visual information via ``add embedding''. This consistent pattern across both models confirms that our benchmark has been properly curated, requiring true integration of visual and textual information rather than allowing models to infer answers solely from questions. 

Second, the results reveal that QuiltNet~\cite{ikezogwo2024quilt}, a CLIP-based model, shows little difference between using only the question text and integrating image and text embeddings. This indicates that CLIP's method lacks robust modality integration, as the inclusion of visual information fails to significantly enhance its effectiveness. Conversely, our model MLLM4PUE demonstrates substantial improvements by effectively fusing image and text information.  The true strength of our approach becomes evident when employing a prompt-based method, leading to significantly higher recall scores. Specifically, it achieves Recall@5 values of 0.475 on Quilt-VQA and 0.675 on Quilt-VQA-RED, representing approximately an 8\% and 12\% improvement over the simple addition method, respectively, and an 18\% and 25\% improvement over using the question text alone. These findings clearly indicate that our model's ability to integrate both visual and textual modalities results in significant enhancements in retrieval accuracy. This surpasses both mere modality addition and the question-only configuration, highlighting the effectiveness of our modality fusion technique for composed retrieval tasks. Our approach provides advantages that cannot be achieved through separate handling or limited integration of text and image data.

\section{Conclusion}
In this paper, we have explored a novel paradigm for applying MLMMs to generate multimodal universal embeddings for a broad range of pathology downstream tasks. By applying summarization prompts and inserting lightweight LoRA modules, our proposed framework, MLLM4PUE, can effectively integrates image and text data, and addresses the limitations of previous CLIP-based methods. We also introduce PMEB, which provides a standardized and comprehensive benchmark for evaluating multimodal capabilities, promoting reproducibility and comparability in the field. Comprehensive experiment results across 15 diverse zero-shot retrieval and classification tasks underscore the versatility and effectiveness of our method. We anticipate that our framework will provide valuable insights into the pathology vision-language foundation models and inspire further research in this area.