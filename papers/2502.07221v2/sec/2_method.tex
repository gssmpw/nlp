\begin{figure*}
  \centering
\includegraphics[width=0.94\textwidth]{fig3.pdf}
  % \vspace{-1ex}
  \caption{Overview of the MLLM4PUE framework. (a) Illustration of pathology image-text contrastive learning with our proposed method. With differents prompts, MLLM can generate embeddings for multimodal inputs. Image and text embeddings are aligned using a contrastive loss. (b) Zero transfer for classification. (c) Zero transfer for composed retrieval.}
  \label{fig:short}
  \vspace{-2ex}
\end{figure*}
\section{Methodology}
\label{sec:formatting}
In this section, we first introduce the Pathology Multimodal Embedding Benchmark (PMEB), a comprehensive benchmark for evaluating multimodal embeddings in diverse pathology tasks. Then, we present the MLLM4PUE framework, which integrates multimodal embeddings, contrastive learning, and zero-shot transfer methods to align and adapt pathology image-text data effectively, as illustrated in Fig.~\ref{fig:short}.
\subsection{Pathology Multimodal Embedding Benchmark}
We propose PMEB, a comprehensive benchmark designed to evaluate pathology multimodal embeddings. PMEB include cancer tissue classification~\cite{aresta2019bach, borkowski2019lung, kriegsmann2022deep}, Gleason pattern grading~\cite{silva2020going}, and cell identification~\cite{da2022digestpath}, among other tasks. As shown in Table~\ref{t1}, PMEB comprises 15 original tasks from 14 datasets, organized into three meta-tasks: retrieval, classification, and composed retrieval. All tasks are reformulated to assess multimodal embeddings, where the model receives an instructional prompt and a query, which may consist of text, images, or both, and selects the correct target from a set of options.

In the retrieval task, the query can be either an image or text, with the target being the corresponding paired text or image. For the classification task, queries are images with targets as text labels representing various classes. The composed retrieval task involves queries that combine an image with text formatted as a question, with the target being the expected answer. This composed retrieval task, specifically designed for multimodal embedding evaluation, incorporates integrated visual and language inputs and has not been applied in previous methods. 

Due to the large size of some datasets, we standardize their sizes. For datasets containing more than 1,000 samples (except Osteo~\cite{arunachalam2019viable}), we choose a random sample of 1,000, ensuring the original category distribution is preserved. For the composed retrieval task, we select only open-ended questions from the data source, where answers are more complex than a simple ``yes'' or ``no''. Additionally, since there is an overlap between questions and answers in the VQA data source, we utilize ChatGPT~\cite{achiam2023gpt} for data cleaning to ensure clarity and accuracy in task evaluation. Further details of each dataset within PMEB are provided in Supplementary Materials.

\subsection{Multimodal Embeddings with MLLM}\label{2.2}
We first introduce how our proposed MLLM4PUE leverages MLLMs for universal multimodal embedding learning.
Unlike CLIP, which directly generates embeddings for multimodal inputs, MLLMs are inherently generative models and require a different approach to produce embeddings. We uses a prompt-based strategy specifically designed to guide MLLMs in embedding multimodal data.

Given an MLLM $f_{\varphi}$ and a query $q$, we use the prompt, $\texttt{<$q$> \textbackslash n Summarize the above <> in one word:}$, to generate $q_{input}$. In this setup, \texttt{<$q$>} serves as a placeholder for the query, while \texttt{<>} indicates the type of the query's content. For instance, if query $q$ is an image, the prompt would be \texttt{<image>\textbackslash n Summarize the above H\&E image in one word:}; if query $q$ is a sentence, then the prompt would be \texttt{<text>\textbackslash n Summarize the above sentence in one word:}. 

The prompt design incorporates two components: first, the word \texttt{Summarize} directs the MLLM to distill the information of the multimodal input. The second part, \texttt{in one word:}, instructs the MLLM to compress this distilled information into a single token, thereby facilitating a unified multimodal embedding. Once the extended input $q_{input}$ is constructed, it is processed by the MLLM $f_{\varphi}$ to generate embeddings: $h = f_{\varphi}(q_{input})$, where $h$ is the embedding of last token obtained from the MLLM output, as illustrated in Fig.~\ref{f3}. This approach tailors the generation capabilities of MLLMs to effectively produce and utilize multimodal embeddings, expanding their application potential beyond traditional generative outputs.

\subsection{Pathology Contrastive Learning}\label{2.3}
Next, we describe the approach for projecting pathological data into a latent embedding space using conservative learning.
As illustrated in Fig.~\ref{fig:short}, given a batch of $N$ paired image and text samples $\{(v_n, t_n)\}_{n=1,\dots,N}$ and MLLM $f_{\varphi}$, for each sample $(v_i, t_i)$, we feed them into $f_{\varphi}$ to get the image and text embeddings $(h_{vi}, h_{ti})$ as described in Sec. \ref{2.2}.

The objective is to build a latent space for pathology image-text embeddings that maximizes the similarity between embeddings of the paired samples in the batch, while minimizing the similarity between embeddings of the $N-1$ incorrect pairs. We optimize an infoNCE contrastive Loss $\mathcal {L}$ to train our model.
\begin{equation}
\mathcal {L}=-(\log \frac {e^{cos(h_{vi}, h_{ti})/ \tau }}{\sum _{j=1}^{n}e^{cos(h_{vi}, h_{tj})/ \tau }} + \log \frac {e^{cos(h_{ti}, h_{vi})/ \tau }}{\sum _{j=1}^{n}e^{cos(h_{ti}, h_{vj})/ \tau}}),
\end{equation}
where $\tau$ is a temperature parameter and
$cos(h_{vi}, h_{ti})$ and $cos(h_{ti}, h_{vi})$ represent the cosine similarity in both directions in contrastive learning.

\begin{figure}
   \centering  
  \includegraphics[width=0.4\textwidth]{fig4.pdf}  
% \vspace{-1ex}
  \caption{Illustration of the MLLM4PUE framework for generating multimodal embeddings using an MLLM. The image is processed through an Image Encoder and projection layer, with a prompt guiding the MLLM to distill the image content into a single-word embedding. The final embedding $h$ is extracted from the last token as the unified multimodal representation.}  
  \label{f3} 
  \vspace{-1ex}
\end{figure}
\subsection{Zero-shot Transfer Evaluation}\label{2.4}
Our model, trained primarily on retrieval tasks focused on image-text pairs, requires adaptation to handle zero-shot classification and composed retrieval tasks. The details of how we modify the model for these different zero-shot downstream tasks are introduced as follows. 

For zero-shot classification, we adopt a prompt-based method inspired by the CLIP model~\cite{radford2021learning}. 
In this approach, each class name is expanded into a sentence using a specific template. For instance, the class name ``Colon adenocarcinoma'' is converted into the sentence ``An H\&E image of Colon adenocarcinoma'' using the template ``An H\&E image of \{ \}.'' We apply this method to create sentences for all class names. Our model then computes embeddings for these sentences and the test images, extracting the last token of the MLLM output as described in Section~\ref{2.2}. The similarity between these embeddings is calculated as outlined in Section \ref{2.3}, and test image labels are assigned based on the highest similarity scores.

For the composed retrieval task, where the query consists of both an image and a question, we need to convert this multimodal information into a unified embedding. Thus, we use the prompt \texttt{<image>\textbackslash n <question>\textbackslash n Summarize the above H\&E image and question in one word:} to integrate the image and the question. Our model then generates embeddings for this combined input, as explained in Section~\ref{2.2}, which are used to retrieve the embeddings of the corresponding answers.
