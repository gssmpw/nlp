% ICCV 2025 Paper Template

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage{iccv}              % To produce the CAMERA-READY version
% \usepackage[review]{iccv}      % To produce the REVIEW version
\usepackage[pagenumbers]{iccv} % To force page numbers, e.g. for an arXiv version

% Import additional packages in the preamble file, before hyperref
\input{preamble}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, 
% e.g. with the file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete *.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you should be clear).
\definecolor{iccvblue}{rgb}{0.21,0.49,0.74}
\usepackage[pagebackref,breaklinks,colorlinks,allcolors=iccvblue]{hyperref}

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\paperID{****} % 7521 Enter the Paper ID here
\def\confName{ICCV}
\def\confYear{2025}

\newcommand{\methodname}{UniGaze\xspace}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{\methodname: Towards Universal Gaze Estimation via Large-scale Pre-Training}

%%%%%%%%% AUTHORS - PLEASE UPDATE
\author{Jiawei Qin$^{1}$, Xucong Zhang$^{2}$, Yusuke Sugano$^{1}$ \\
\normalsize $^1$ Institute of Industrial Science, The University of Tokyo, Komaba 4-6-1, Tokyo, Japan \\
\normalsize $^2$ Computer Vision Lab, Delft University of Technology, Mekelweg 5, Delft, Netherlands \\
{
\tt\small \{jqin, sugano\}@iis.u-tokyo.ac.jp}\\
\tt\small xucong.zhang@tudelft.nl
}


\begin{document}
% \maketitle


\twocolumn[{%
\renewcommand\twocolumn[1][]{#1}%
\maketitle
\vspace{-25pt}
\begin{center}
  % \captionsetup{type=figure}
  \includegraphics[width=0.9\linewidth]{figs/teaser_v6.pdf}
  % \fbox{\rule{0pt}{2.5in} \rule{0.95\linewidth}{0pt}}
  \captionof{figure}{
  Leveraging self-supervised pre-training on large-scale facial data, the proposed \methodname demonstrates strong generalization to unseen in-the-wild face images under diverse conditions, including facial appearance, lighting conditions, variant head poses, and face resolutions. We draw the estimated gaze direction with green arrows. More examples are shown in the supplementary materials.
  }
  \label{fig:teaser}
\end{center}%
}]


\begin{abstract}
Despite decades of research on data collection and model architectures, current gaze estimation models encounter significant challenges in generalizing across diverse data domains. 
Recent advances in self-supervised pre-training have shown remarkable performances in generalization across various vision tasks.
However, their effectiveness in gaze estimation remains unexplored.
We propose \methodname, for the first time, leveraging large-scale in-the-wild facial datasets for gaze estimation through self-supervised pre-training.
Through systematic investigation, we clarify critical factors that are essential for effective pre-training in gaze estimation. 
Our experiments reveal that self-supervised approaches designed for semantic tasks fail when applied to gaze estimation, while our carefully designed pre-training pipeline consistently improves cross-domain performance. 
Through comprehensive experiments of challenging cross-dataset evaluation and novel protocols including leave-one-dataset-out and joint-dataset settings, we demonstrate that \methodname significantly improves generalization across multiple data domains while minimizing reliance on costly labeled data. 
Source code and model are available at \url{https://github.com/ut-vision/UniGaze}.
\end{abstract}

  
\input{sec/1_intro}
\input{sec/2_related}
\input{sec/3_method}
\input{sec/4_exp}
\input{sec/5_conclusion}


{
    \small
    \bibliographystyle{ieeenat_fullname}
    \bibliography{main}
}



\input{supp_arxiv}
\end{document}
