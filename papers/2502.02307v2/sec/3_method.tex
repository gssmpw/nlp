

\section{\methodname}\label{sec:method}

Our \methodname model consists of a large-scale pre-training stage followed by task-specific training. 
The pre-training stage utilizes a carefully curated dataset of \SI{1.6}{\mega{}} facial images that combines real-world and synthetic data to ensure broad coverage of head poses and facial appearances. 
We adopt the MAE framework to learn robust facial representations from this diverse dataset and then train it with labeled gaze data for the downstream gaze estimation task. 

\subsection{Pre-Training Datasets}\label{sec:pretrain_data_prepare}

\begin{table}[t]
    \begin{center}
        \resizebox{0.94\linewidth}{!}{ %< auto-adjusts font size to fill line
        \setlength{\tabcolsep}{6pt}
            \begin{tabular}{l|cc|c}
            \toprule
            \textbf{Dataset}  & \textbf{Type} & \textbf{\# Identities} & \textbf{\# Samples} \\
            \midrule
            CelebV-Text~\cite{yu2023celebv} & Real & 13,179 & 666,967 \\
            VFHQ~\cite{xie2022vfhq} & Real & 10,382 & 231,809 \\
            VGGFace2~\cite{cao2018vggface2} & Real & 9,131 & 182,603 \\
            \midrule
            FaceSynthetics~\cite{wood2021fake} & Syn. & 86,878$^{\dagger}$ & 86,878 \\
            SFHQ-T2I~\cite{david_beniaguev_2024_SFHQ_T2I} & Syn. & 120,241$^{\dagger}$ & 120,241 \\
            FFHQ-NV~\cite{Karras2019stylegan2, qin2022learning} & Syn. & 25,000 & 100,000 \\
            XGaze-Dense~\cite{zhang2020eth,agisoft_metashape,qin2023domain} & Syn. & 60 & 267,160 \\
            \midrule
            Total  & - & 264,871 & 1,655,668\\
            \bottomrule
            \end{tabular}
        }
    \end{center}
    \caption{
        Statistics of face datasets used to pre-train \methodname in terms of data type to be real or synthetic (Syn.), number of identities, and number of samples.
        The $^{\dagger}$ indicates that we assume there are no duplicated identities during the synthesis image generation.
    }\label{table:face_data_info}
\end{table}



Learning robust gaze representations requires extensive head pose and facial appearance variations. 
To achieve this, we combine two complementary data sources: real data that captures natural face appearances and synthetic data that allows us to cover extreme pose variations and diverse appearances systematically. 


\Cref{table:face_data_info} summarizes our pre-training dataset composition, which comprises approximately \SI{1.6}{\mega{}} samples.
This combined dataset includes over \SI{260}{\kilo{}} unique identities, vastly exceeding the 1,474 subjects in the existing GazeCapture dataset~\cite{krafka2016eye}. 
Although GazeCapture~\cite{krafka2016eye} and ETH-XGaze~\cite{zhang2020eth} offer over \SI{2.4}{\mega{}} and \SI{1}{\mega{}} samples, respectively, our combined dataset provides a significantly higher level of diversity.
In this manner, our pre-training data offers a significantly broader representation regarding facial appearances, head poses, and environmental conditions, surpassing existing gaze estimation datasets.

\begin{figure*}[t]
  \begin{center}
      \includegraphics[width=0.9\linewidth]{figs/norm_samples_distribution_v1.pdf}
      \caption{Example of the normalized facial images from different datasets in the pre-training stage. We also draw their head pose distributions where the vertical axis is the pitch rotation angle and the horizontal axis is the yaw rotation angle in degrees.
      }\label{fig:face_samples_distribution}
  \end{center}
\end{figure*}



\noindent\textbf{Real Datasets.} 
We use VGGFace2~\cite{cao2018vggface2} for its large number of identities with diverse conditions. 
Additionally, we incorporate two high-quality video datasets, VFHQ~\cite{xie2022vfhq} and CelebV-Text~\cite{yu2023celebv}, leveraging their natural pose and gaze variations provided by video sequences. 
To reduce redundancy while maintaining diversity, we sub-sample every 15 frames from VFHQ, 45 frames from CelebV-Text, and select 20 images per identity from VGGFace2. 



\noindent\textbf{Synthetic Datasets.} 
To ensure diverse facial appearances, we use two synthetic datasets: FaceSynthetics~\cite{wood2021fake}, generated via computer graphics, and SFHQ-T2I~\cite{david_beniaguev_2024_SFHQ_T2I}, created with diffusion models. 
We further employ novel-view synthesis techniques to extend the range in head poses.
We reconstruct 3D facial shapes from FFHQ~\cite{Karras2019stylegan2} via the single-view method~\cite{qin2022learning}, synthesizing FFHQ-NV. 
Following~\cite{qin2023domain}, we use Metashape~\cite{agisoft_metashape} for multi-view 3D face reconstruction and apply novel-view rendering to get XGaze-Dense. 
Since XGaze-Dense is used without gaze labels, its role is equivalent to that of a generic facial dataset in our pre-training. 


\noindent\textbf{Data Pre-processing}
We perform facial landmark detection~\cite{bulat2017far} to estimate the 3D head pose by perspective-n-point (PnP) algorithm~\cite{fischler1981random}.
We then apply data normalization~\cite{zhang2018revisiting} to crop face images, ensuring alignment of the input space between MAE pre-training and gaze estimation. 
We filter out samples with extreme head poses for VFHQ, CelebV-Text, SFHQ-T2I, and FaceSynthetics to eliminate extreme cases where the face is invisible. 
Precisely, we discard samples with an $L_2$ norm of pitch and yaw angles exceeding 80 degrees.
\Cref{fig:face_samples_distribution} shows example face images and head pose distributions for each dataset.


\subsection{Training Procedure}

We follow the MAE~\cite{he2022masked} to pre-train the ViT model without any gaze labels. 
Briefly, it randomly masks patches of the input image and the model is trained to predict the masked contents.
Given an input image $\bm{X}$, it is divided into $N$ patches $\{x_i\}_{i=1}^N$.
A subset of these patches is masked out, denoted by $\{x_i\}_{i \in M}$ with $M \subset \{1, \ldots, N\}$.
The encoder processes the visible patches $\{x_i\}_{i \in V}$, where $V = \{1, \ldots, N\} \setminus M$, and generates a latent representation $z$.
An extra decoder then takes $z$ and reconstructs the masked patches $\{\hat{x}_i\}_{i \in M}$ with the loss $\mathcal{L}_{\text{MAE}} = \frac{1}{|M|} \sum_{i \in M} \| x_i - \hat{x}_i \|^2$.
This process encourages the model to capture meaningful structures and features in $z$, making it well-suited for downstream tasks~\cite{khirodkar2025sapiens}.
During pre-training, we randomly apply flip, central crop, and color jitter, and the mask ratio is 75\%.
The loss is computed with the pixel value normalized within each patch, which is suggested to have better representation~\cite{he2022masked}. 


\noindent\textbf{Gaze Estimation Training}
With labeled gaze datasets, we train the pre-trained model for the gaze estimation downstream task. 
We replace the decoder with a fully connected layer to predict gaze direction from the latent representation $z$.
The gaze direction is represented by a 2D vector in the polar angle coordinate system.
We use the loss $\mathcal{L}_{1} = \| \bm{g} - \hat{\bm{g}} \|_1$, where $\bm{g}$ and $\hat{\bm{g}}$ denote the ground-truth gaze label and the prediction, respectively.


\subsection{Implementation Details}
We use the Adam optimizer~\cite{kingma2014adam} with a base learning rate of \num{1.5e-4} and a weight decay of \num{0.05}.
We set a batch size of 4,096 and pre-train the ViT-Huge model for 300 epochs, which requires approximately 120 hours on four NVIDIA H100 GPUs.
During gaze estimation training, we do not apply any image augmentation.
We use the Adam optimizer~\cite{kingma2014adam} with a learning rate of \num{1e-4} and a weight decay of \num{1e-6}, and the one-cycle learning rate~\cite{smith2019super}.
More details and examples can be found in supplementary materials.

