\section{Related Works}
\label{sec:related}

\subsection{Appearance-based Gaze Estimation}
Appearance-based methods for gaze estimation have gained prominence, leveraging the ability of deep learning to learn gaze representations from gaze-labeled datasets~\cite{zhang2015appearance,mpii_zhang19_pami,zhang2020eth,kellnhofer2019gaze360,krafka2016eye,funes2014eyediap,fischer2018rt}.
However, these approaches are limited by the scarcity of comprehensive, well-labeled gaze datasets, which hinders performance in unconstrained settings.
One solution to data scarcity has been synthetic data generation, where gaze images are synthesized with controllable variables such as lighting, head pose, and redirected gaze~\cite{qin2022learning,zheng2020self,jin2023redirtrans,ruzzi2023gazenerf,wang2023high_nerf,yin2022nerfgaze}. 
Despite its utility, synthetic data often lacks realism, leading to domain adaptation challenges~\cite{wood2015rendering,kim2019nvgaze}. 
Furthermore, subtle inaccuracies in gaze labels can compromise model performance when transferred to real-world scenarios.


In addition to data-driven approaches, method-driven solutions have been explored to improve robustness and generalizability~\cite{yin2024clip,liu2024gaze,bao2024feature,cheng2022puregaze,zhao2024improving,xu2023learning}.
For instance, gaze frontalization~\cite{xu2024gaze}, multi-view consistency~\cite{hisadome2024rotation,bao2024unsupervised}, and contrastive learning~\cite{he2020momentum,jindal2023contrastive} have been used to learn generalized gaze representations.
Clip-Gaze~\cite{yin2024clip} and LG-Gaze~\cite{yin2024lggaze} use the linguistic features extracted from the vision-language model to regularize the gaze feature learning.
Alternatively, Kothari~\etal~\cite{kothari2021weakly} utilizes in-the-wild face datasets with \textit{look-at-each-other} labels as weak supervision.
Furthermore, domain adaptation methods~\cite{park2019few,wang2022contrastive} incorporate target domain samples with limited or no labels.
For example, PnP-GA+ uses the plug-and-play method to adapt the gaze estimation model to new domains with assembling model variants~\cite{liu2024pnp}.


Network architectures in gaze estimation remain predominantly CNN-based, particularly relying on ResNet~\cite{he2016deep}.
Recent work by Cheng~\etal~\cite{cheng2022gaze} explored ViTs~\cite{dosovitskiy2020vit} for gaze estimation, finding that although ViTs hold promise, they require extensive pre-training data beyond standard ImageNet~\cite{deng2009imagenet} to perform effectively.
This motivates the exploration of ViT models specifically tailored to learn diverse gaze representations through pre-training approaches.


\subsection{Large-scale Pre-Training in Vision Models}\label{sec:related_pretrain}
Large-scale pre-training has become fundamental for foundational model development in computer vision. 
Recent studies show that pre-trained generative models enhance representation learning across various applications. 
For example, diffusion models~\cite{ho2020denoising,rombach2022high} have been successfully applied to image classification~\cite{li2023your} and segmentation~\cite{li2023dreamteacher,xiang2023denoising,zhao2023unleashing}.
Another effective unsupervised pre-training approach is masked autoencoding~\cite{he2022masked,srivastava2024omnivec,singh2023effectiveness,fang2023eva}, which demonstrates strong capabilities in image recognition and robust feature extraction.
It is also shown that masked image modeling benefits more for ViT than CNN family~\cite{fang2022corrupted,kraus2024masked}.




MAE pre-training has been adapted to several domains such as audio~\cite{huang2022masked}, video~\cite{tong2022videomae}, and microscopy~\cite{kraus2024masked}.
Recently, Sapiens~\cite{khirodkar2025sapiens} leverages human-centric data for MAE pre-training, resulting in strong generalization across multiple human-related tasks.
These adaptations to specific domains have made a crucial step in advancing research.
Similarly, multiple self-supervised pre-training works have been proposed to achieve notable improvement in face-centric tasks, such as expression recognition, facial attribute recognition, and face alignment~\cite{zheng2022general,cai2023marlin,gao2024self,wang2023toward,sun2024face}.
These studies underscore the effectiveness of pre-training masked autoencoders on large, diverse datasets for enhancing model robustness and generalizability.
In contrast, gaze estimation has received less attention in this context.
A concurrent work~\cite{jiang2024learning} even highlighted a limitation in applying MAE to gaze estimation with ViT, noting that random masking tends to focus on global semantics while neglecting critical gaze-related information. 

