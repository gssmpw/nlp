\section{Introduction}
\label{sec:intro}

Gaze estimation is a key task in computer vision with broad applications in human-computer interaction~\cite{majaranta2014eye,belardinelli2024gaze}, virtual reality~\cite{lystbaek2022gaze}, and behavioral analysis~\cite{harezlak2018application,ke2024using,sharma2024review}. 
The methodology in unconstrained environments has been actively researched in past decades~\cite{baluja1993non,tan2002appearance,lu2012head,lu2014learning,zhang2015appearance,cheng2024appearance,ghosh2023automatic,wang2019generalizing}.
However, achieving robust and accurate gaze estimation in unseen test environments remains a fundamental challenge.
Although current models can achieve high accuracy when tailored to specific datasets or individuals~\cite{park2019few}, they consistently show significant performance degradation in novel environments. 
This challenge is evidenced by the persistent difficulty in training one single model to achieve high accuracy across various gaze estimation datasets~\cite{funes2014eyediap,mpii_zhang19_pami,krafka2016eye,kellnhofer2019gaze360,zhang2020eth}.
Despite decades of research on data collection and model architecture, the problem of generalization across variations in head pose, identity, and lighting conditions remains largely unsolved.




Recent advances in self-supervised pre-training have shown remarkable potential for improving model generalization to arbitrary data domains.
Following scaling laws~\cite{kaplan2020scaling}, larger models trained with extensive data and compute resources can be more sample-efficient, which has been observed with significant performance improvements across numerous tasks such as image classification~\cite{srivastava2024omnivec,singh2023effectiveness,fang2023eva}, segmentation~\cite{li2023dreamteacher,xiang2023denoising,zhao2023unleashing,kirillov2023segment}, human-centric tasks~\cite{khirodkar2025sapiens}, gaze following~\cite{lin2024gazehta}, and face-focused applications~\cite{zheng2022general,cai2023marlin,gao2024self,wang2023toward,sun2024face}.  
However, their effectiveness in gaze estimation remains unexplored.
Unlike these typical tasks, gaze estimation is a geometric regression task to map facial appearance to precise 3D vectors.
Our experiments show that existing pre-trained models~\cite{zheng2022general,caron2021emerging,chen2021empirical} fail to improve gaze estimation, as they are optimized for semantic understanding rather than the fine-grained facial structure crucial for gaze direction.
This raises an essential question: Can large-scale pre-training benefit the fine-grained geometric nature of gaze estimation? 
Our research offers an answer: Yes, but \textit{only when the pre-training is specifically tailored to the unique constraints of gaze estimation}.



In this work, we present a novel approach toward \textbf{Uni}versal \textbf{Gaze} estimation, dubbed \methodname, exploring the potential of large-scale self-supervised pre-training for appearance-based gaze estimation. 
We employ Masked Autoencoder (MAE)~\cite{he2022masked} on the Vision Transformer (ViT) architecture~\cite{dosovitskiy2020vit}, using diverse in-the-wild face image datasets.
Through systematic experimentation, we discovered that pre-training for gaze estimation requires three essential components that differ from typical pre-training approaches: 
(1) pre-training on normalized facial images maintaining the spatial configuration required by downstream gaze models~\cite{zhang2018revisiting}, 
(2) ensuring diverse yet balanced head pose distributions to learn robust facial representations across viewing angles, and 
(3) incorporating sufficient identity diversity to generalize across different facial appearances.
This strategy allows the model to learn appropriate feature representations within the specific input space required by gaze estimation models, enabling effective transfer to the downstream gaze estimation task.



Through extensive experiments, we demonstrate that training our pre-trained \methodname on gaze-specific datasets yields substantial generalization performance improvements across multiple data domains~\cite{zhang2020eth,mpii_zhang19_pami,funes2014eyediap,krafka2016eye,kellnhofer2019gaze360}, surpassing state-of-the-art domain generalization methods~\cite{cheng2022puregaze,xu2023learning,bao2024feature,yin2024clip,yin2024lggaze,zhao2024improving}.
Pre-training with general semantics~\cite{zhou2024deformable,caron2021emerging,chen2021empirical}, face-specific semantics~\cite{zheng2022general}, and vanilla MAE trained on small-scale gaze data~\cite{jiang2024learning} all fail to transfer effectively to gaze estimation, sometimes even performing worse than simple CNNs.
In contrast, our carefully designed pre-training approach consistently improves accuracy across diverse environments.
This highlights our crucial discovery that suitable datasets, normalized facial images, and varied yet balanced head pose distributions are vital for developing transferable representations for gaze estimation, offering practical guidelines for future research in this field.


Additionally, in gaze estimation, the widely used cross-dataset evaluation usually only trains the model on a single dataset, which cannot fully represent the appearance diversity and pose range in real-world environments.
There also remains the possibility that some adaptation methods merely overfit some specific training/testing dataset combinations.
To address this limitation and reflect the practical requirements of real-world applications, we propose two novel evaluation protocols utilizing multiple datasets for training: a \textit{leave-one-dataset-out} setting that assesses generalization to unseen datasets and a \textit{joint-dataset} setting that evaluates the achievable performance across multiple datasets.
Our comprehensive evaluation demonstrates that \methodname consistently achieves superior performance across diverse environments under these protocols, suggesting the effectiveness of large-scale pre-training. 




In summary, our contributions are threefold: 
(i) We present \methodname, a novel gaze estimation model that addresses the fundamental challenge of cross-domain generalization in appearance-based gaze estimation via large-scale pre-training.
(ii) We provide empirical evidence that MAE pre-training on normalized face images can learn meaningful representations for gaze estimation tasks.
(iii) We propose leave-one-dataset-out and joint-dataset evaluation protocols that offer practical benchmarks for assessing gaze estimation performance in real-world scenarios.


