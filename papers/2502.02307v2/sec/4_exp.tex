

       
\begin{table*}[t]
\begin{center}
    \begin{subtable}[t]{0.33\textwidth}
    \begin{center}
    \resizebox{0.99\linewidth}{!}{ %< auto-adjusts font size to fill line
    \setlength{\tabcolsep}{4.5pt}
        \begin{tabular}{l|cccc}
        \toprule
        \textbf{Models \textbackslash~Test}  & \textbf{M} & \textbf{GC} & \textbf{E} & \textbf{G360} \\
        \midrule
        ResNet-50        & 6.75 & 10.08 & 10.28 & 19.80 \\
        GazeTR-50 & 7.09 & 10.95 & 9.47 & 21.10 \\   %% (3 cams)
        \midrule
        DINO-B~\cite{caron2021emerging} & 7.73 & 9.12 & 11.98 & 19.44 \\
        MoCo-v3-B~\cite{chen2021empirical} & 7.19 & 8.88 & 9.50 & 19.09 \\
        FaRL-B~\cite{zheng2022general} & 7.18& 8.56& 9.53&17.28 \\
        \rowcolor{LightGray}
        \methodname-B & 6.21 & 7.35 & 6.64 & 12.18 \\
       \midrule
        ViT-H   & 7.68 & 9.36 & 10.40& 20.38 \\
       \rowcolor{LightGray}
      \methodname-H  & \textbf{5.57} & \textbf{6.56}& \textbf{6.53} & \textbf{11.19} \\
        \bottomrule
        \end{tabular}
    }
    \caption{ Results when trained on XGaze. }
    \label{table:cross_x}
    \end{center}
    \end{subtable}
    \vspace{8pt} % Adjust space as needed
    \begin{subtable}[t]{0.33\textwidth}
    \begin{center}
    \resizebox{0.99\linewidth}{!}{ %< auto-adjusts font size to fill line
    \setlength{\tabcolsep}{4.5pt}
       \begin{tabular}{l|cccc}
        \toprule
        \textbf{Models \textbackslash~Test} & \textbf{X\textsubscript{Test}} & \textbf{GC} & \textbf{E} & \textbf{G360} \\
        \midrule
        ResNet-50        & 32.80  & 6.75 & 17.11 & 27.92 \\
        GazeTR-50 & 29.00 & 7.06 & 19.38 & 28.22 \\
        \midrule
        DINO-B~\cite{caron2021emerging} & 33.41 & 8.13 & 20.07 & 30.39 \\
        MoCo-v3-B~\cite{chen2021empirical} & 30.99&6.53&17.56&27.14\\
        FaRL-B~\cite{zheng2022general} &30.43 &6.14 &16.55 &25.97 \\
        \rowcolor{LightGray}
         \methodname-B & 30.56 & 5.68 & 17.54 & \textbf{20.85} \\
        \midrule
        ViT-H   & \textbf{28.25} & 6.87 & 16.02 & 25.30 \\
        \rowcolor{LightGray}
        \methodname-H      & 33.11 &\textbf{4.87}&\textbf{12.66}&21.28 \\
        \bottomrule
        \end{tabular}
    }
    \caption{ Results when trained on MPIIFaceGaze.}
    \label{table:cross_m}
    \end{center}
    \end{subtable}
    \begin{subtable}[t]{0.33\textwidth}
    \begin{center}
    \resizebox{0.99\linewidth}{!}{ %< auto-adjusts font size to fill line
    \setlength{\tabcolsep}{4.5pt}
       \begin{tabular}{l|cccc}
        \toprule
        \textbf{Models \textbackslash~Test} & \textbf{X\textsubscript{Test}} & \textbf{M} & \textbf{E} & \textbf{G360} \\
        \midrule
        ResNet-50        & 26.56 & 5.84 & 13.39 & 25.33 \\
        GazeTR-50 & 23.57&5.49 &14.25 &25.48 \\
        \midrule
        DINO-B~\cite{caron2021emerging} & 27.56 & 6.97 & 16.75 & 26.49 \\
        MoCo-v3-B~\cite{chen2021empirical} &27.26&5.29&14.75&25.94\\
        FaRL-B~\cite{zheng2022general} & 26.55 &5.74&15.18&23.49 \\
        \rowcolor{LightGray}
         \methodname-B & \textbf{20.63} & 5.01 & 10.91 & 18.91 \\
        \midrule
        ViT-H   & 23.49 & 5.54 & 14.60 & 23.98\\
        \rowcolor{LightGray}
        \methodname-H  & 22.67&\textbf{4.89}&\textbf{10.61}&\textbf{17.77} \\
        \bottomrule
        \end{tabular}
    }
    \caption{ Results when trained on GazeCapture.}
    \label{table:cross_gc}
    \end{center}
    \end{subtable}
    \\
    \begin{subtable}[t]{0.49\textwidth}
    \begin{center}
    \resizebox{0.75\linewidth}{!}{ %< auto-adjusts font size to fill line
    \setlength{\tabcolsep}{6pt}
       \begin{tabular}{l|cccc}
        \toprule
        \textbf{Models \textbackslash~Test} & \textbf{X\textsubscript{Test}} & \textbf{M} & \textbf{GC} & \textbf{G360} \\
        \midrule
        ResNet-50        & 37.50 & 16.88 & 16.37 & 30.03 \\
        GazeTR-50 & 35.82 & 16.37 &16.63& 25.69 \\
        \midrule
        DINO-B~\cite{caron2021emerging} & 38.21 & 17.84 & 18.45 & 33.49 \\
        MoCo-v3-B~\cite{chen2021empirical} &31.98&14.29&14.15&24.90\\
        FaRL-B~\cite{zheng2022general} & 34.74 &12.96&12.31&26.88 \\
        \rowcolor{LightGray}
         \methodname-B &25.37 & \textbf{8.91} & \textbf{8.89} & 18.27 \\
        \midrule
        ViT-H   & 30.90 & 15.51& 13.51 &26.12 \\
        \rowcolor{LightGray}
        \methodname-H   &\textbf{23.79}&8.93&9.97&\textbf{16.00} \\
        \bottomrule
        \end{tabular}
    }
    \caption{ Results when trained on EYEDIAP.}
    \label{table:cross_ed}
    \end{center}
    \end{subtable}
    \begin{subtable}[t]{0.49\textwidth}
    \begin{center}
    \resizebox{0.75\linewidth}{!}{ %< auto-adjusts font size to fill line
    \setlength{\tabcolsep}{6pt}
       \begin{tabular}{l|cccc}
        \toprule
        \textbf{Models \textbackslash~Test} & \textbf{X\textsubscript{Test}} & \textbf{M} & \textbf{GC} & \textbf{E}  \\
        \midrule
        ResNet-50   & 18.83 & 10.25 & 9.90 & 12.06 \\
        GazeTR-50 & 18.04 & 11.28 & 10.82 & 13.69\\
        \midrule
        DINO-B~\cite{caron2021emerging} & 22.25 & 11.02 & 11.17 & 17.53 \\
        MoCo-v3-B~\cite{chen2021empirical} &17.59&7.50&8.72&11.91\\
        FaRL-B~\cite{zheng2022general} & 20.55 &7.62&7.75&12.43 \\
        \rowcolor{LightGray}
         \methodname-B & \textbf{12.22} & 6.00 & 8.11 & 8.74 \\
        \midrule
        ViT-H   & 18.63 & 8.24 & 9.43 & 11.50 \\
        \rowcolor{LightGray}
        \methodname-H   & 16.39 & \textbf{5.43} & \textbf{6.48}& \textbf{6.97}\\
        \bottomrule
        \end{tabular}
    }
    \caption{ Results when trained on Gaze360.}
    \label{table:cross_g360}
    \end{center}
    \end{subtable}
    \caption{
        Cross-dataset evaluation of different models trained on one dataset and tested on multiple unseen datasets. 
        Each subtable corresponds to a specific training dataset, with columns representing the testing datasets (\textbf{X\textsubscript{Test}}: XGaze Test, \textbf{M}: MPIIFaceGaze, \textbf{GC}: GazeCapture, \textbf{E}: EYEDIAP, \textbf{G360}: Gaze360). 
        Results demonstrate the generalization ability of each model, with \methodname consistently outperforming other baselines in most settings, showcasing the effectiveness of our pre-training approach.
        }
\label{table:cross_dataset}
\end{center}
\end{table*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Experiments}\label{sec:experiments}


\subsection{Experimental Settings}\label{sec:exp_setting}

\paragraph{Gaze Datasets}
We conduct experiments on multiple gaze estimation datasets following the common way of utilization in recent works.
\textbf{MPIIFaceGaze}~\cite{zhang2017s} contains 15 subjects with nearly frontal head poses. 
In experiments requiring splitting, the first ten subjects are used for training and the remaining five for testing.
\textbf{ETH-XGaze}~\cite{zhang2020eth} comprises over \SI{750}{\kilo{}} publicly available gaze-labeled images of 80 subjects. We refer to the ``XGaze'' as the 80 subjects, while ``XGaze Train/Test'' indicates 60/20-subject split.
When training, we randomly select three out of the 18 cameras to reduce redundancy without losing effectiveness, and we utilize all cameras for testing. 
\textbf{EYEDIAP}~\cite{funes2014eyediap} includes 16 subjects and two sessions with screen (CS) and 3D floating object (FT) targets. 
We use both sessions and split the data into training and test sets by subjects, with an 8/8 split.
The data is pre-processed using the pipeline by Park~\etal~\cite{park2019few}. 
\textbf{Gaze360}~\cite{kellnhofer2019gaze360} consists of indoor and outdoor images of 238 subjects with wide ranges of head poses and gaze directions. We use the training and test split defined in the original paper.
\textbf{GazeCapture}~\cite{krafka2016eye} contains around 1400 subjects collected through crowd-sourcing. We use the training and test split defined in the original GazeCapture paper and pre-process with the pipeline by Park~\etal~\cite{park2019few}.
When training, we sample every 15 frames to reduce redundancy, and we use all samples for testing.


\paragraph{Baseline Architectures}
We draw upon existing gaze estimation research and consider baselines of convolutional neural networks, ViTs, and hybrid models.
\textbf{ResNet}~\cite{he2016deep} models are lightweight yet powerful CNNs, which dominate the backbone and baseline in most of the gaze estimation works \cite{zhang2020eth,cheng2022puregaze,xu2023learning,zhao2024improving}. 
We use ResNet-50 in our experiments.
\textbf{GazeTR-50 (Hybrid)}~\cite{cheng2022gaze} is a hybrid network where the image features extracted from the ResNet-50 are fed into the transformer.
We include \textbf{DINO-B}~\cite{caron2021emerging}, \textbf{MoCo-v3-B}~\cite{chen2021empirical}, and \textbf{FaRL-B}~\cite{zheng2022general} as representative pre-trained ViT-Base models: DINO and MoCo-v3 have rich general semantic representations, while FaRL is specialized for face analysis tasks and pre-trained on \SI{20}{\mega{}} LAION-Face samples.
We also include the ImageNet~\cite{deng2009imagenet} pre-trained \textbf{ViTs}~\cite{dosovitskiy2020vit} for comparison.
We use the Base (B), Large (L), and Huge (H) variants in our experiments.

\begin{table}[t]
\begin{center}
    \resizebox{0.99\linewidth}{!}{ %< auto-adjusts font size to fill line
    \setlength{\tabcolsep}{4pt}
        \begin{tabular}{l|cccc}
        \toprule
        \textbf{Models}  & \textbf{X}$\rightarrow$\textbf{M} & \textbf{X}$\rightarrow$\textbf{E\textsubscript{CS}}& \textbf{G360}$\rightarrow$\textbf{M} & \textbf{G360}$\rightarrow$\textbf{E\textsubscript{CS}} \\
        \midrule
        \rowcolor{LightGray}
        ResNet-18$^{\dagger}$~\cite{zhao2024improving}  & 8.02 & 9.11 & 8.04 & 9.20 \\
        \rowcolor{LightGray}
        PureGaze$^{\dagger}$~\cite{cheng2022puregaze}  & 7.08 & 7.48 & 9.28 & 9.32 \\
        \rowcolor{LightGray}
        Gaze-Consistent$^{\dagger}$~\cite{xu2023learning} & 6.50 & 7.44 & 7.55 & 9.03 \\
        \rowcolor{LightGray}
        AGG$^{\dagger}$~\cite{bao2024feature}     & \underline{5.91} & 6.75 & 7.87  & 7.93 \\
        \rowcolor{LightGray}
        CLIP-Gaze$^{\dagger}$~\cite{yin2024clip}     & 6.41 & 7.51 & 6.89 & 7.06 \\
        \rowcolor{LightGray}
        LG-Gaze$^{\dagger}$~\cite{yin2024lggaze}     & 6.45 & 7.22 & 6.83 & 6.86 \\
        \rowcolor{LightGray}
        Gaze-BAR$^{\dagger}$~\cite{zhao2024improving}     & 6.35 & 6.72 & 6.96 & 8.79 \\
        ViT-H & 7.68 & 8.58 & 8.24 & 7.79 \\ 
        \midrule
        \methodname-B & 6.21 & \underline{5.08} & \underline{6.00} & \underline{6.63} \\
        \methodname-H & \textbf{5.57} &  \textbf{4.65} & \textbf{5.43} & \textbf{5.35} \\
        \bottomrule
        \end{tabular}

}
\end{center}
\caption{
    Domain generalization compared with SOTA methods in the cross-dataset setting. 
    Since most of the methods in the table are based on ResNet-18, we also show the ViT-H baseline that only pre-trained on ImageNet for fair comparison with \methodname-H.
}
\label{table:sota}
\end{table}


\begin{table}[t]
    \centering
    \resizebox{0.88\linewidth}{!}{ %< auto-adjusts font size to fill line
    \setlength{\tabcolsep}{4.5pt}
         \begin{tabular}{l|ccccc}
        \toprule
        \textbf{Methods \textbackslash~Dataset} & \textbf{X} & \textbf{M} & \textbf{GC} & \textbf{E} & \textbf{G360} \\
        \midrule
        Abdelrahman~\etal~\cite{abdelrahman2023l2cs}  & - & 3.92 & - & - & - \\
        Guan~\etal~\cite{guan2023end} & - & - & - & - & 9.81 \\  
        Shi~\etal~\cite{shi2024agent} & - & \textbf{3.61} & - & 4.78 & - \\
        3DGazeNet~\cite{ververas20253dgazenet} & 4.2 & 4.0 & 3.1 & - & 9.6 \\
        \midrule
        \methodname-H & \textbf{3.96} & 4.07 & \textbf{3.01} & \textbf{4.34} & \textbf{9.44} \\
        \bottomrule
        \end{tabular}
        }
    \caption{
    Within-dataset evaluation compared with SOTA methods. Note we here follow the train-test split from individual previous works.}
    \label{table:within}
\end{table}



\subsection{Cross-Dataset Evaluation}\label{sec:cross_dataset}

We first assess the generalization of our \methodname with the commonly used cross-dataset evaluation setting, where models are trained on one dataset and tested on an unseen dataset. 
Unlike previous studies that primarily report results for models trained on ETH-XGaze and/or Gaze360~\cite{cheng2022puregaze,xu2023learning,zhao2024improving,liu2021generalizing,liu2022jitter}, we evaluate models trained individually on five different gaze datasets and measure their cross-dataset performance on the remaining datasets. 
In \cref{table:cross_dataset}, each subtable corresponds to a training dataset, with each column representing a different test dataset. 
We compare several baseline models with our \methodname.
Note the 60 subjects from XGaze-Dense used during pre-training (\cref{sec:pretrain_data_prepare}) are excluded from the XGaze Test set.



Although, commonly, larger models tend to achieve better performance in typical computer vision tasks, the large ViT-based models (ViT-H, DINO-B, MoCo-v3-B) do not consistently surpass the ResNet-50 on gaze estimation as revealed in \cref{table:cross_dataset}.
This discrepancy indicates that simply increasing the model size without curated pre-training data is not effective for the downstream gaze estimation task.
Especially, the FaRL-B pre-trained on large face-centric data still cannot effectively handle gaze information across diverse datasets. 


By contrast, the proposed \methodname-H, pre-trained on diverse facial datasets, demonstrates superior performance across nearly all settings. 
For completeness of fairness, we also show the \methodname-B based on the ViT-B backbone, which still outperforms similar model size models DINO-B and FaRL-B.
Note \methodname-B outperforms \methodname-H in some cases, which suggests again that our combination of large-model with our curated large-scale pre-training data effectively enhances ViT's ability to learn gaze-specific representations, rather than only increasing the size of the model. 
This answers our question that MAE pre-training with diverse facial data can strengthen the model's ability to capture fine-grained gaze geometry.

Besides, the results emphasize the importance of label range in training data.
While pre-training improves generalization across domains, it alone is insufficient when the training gaze data has a narrow label range.
For example, the model trained on MPIIFaceGaze (\cref{table:cross_m}) exhibits high errors on datasets such as XGaze Test (33.11 degrees) and Gaze360 (21.28 degrees), which have larger ranges of head poses and gaze directions than MPIIFaceGaze.
Thus, while MAE pre-training improves ViT's capacity to learn robust representations, diverse label coverage in the gaze estimation training stage remains crucial for achieving consistent performance in gaze estimation across domains.


\subsection{Comparison with Domain Generalization}\label{sec:domain_generalization}

\begin{table}[t]
    \centering
        \resizebox{0.88\linewidth}{!}{ %< auto-adjusts font size to fill line
        \setlength{\tabcolsep}{5.5pt}
            \begin{tabular}{l|ccccc}
            \toprule
            \textbf{Models \textbackslash~Test}  & \textbf{X\textsubscript{Test}} & \textbf{M} & \textbf{GC} & \textbf{E} & \textbf{G360} \\
            \midrule
            ResNet-50       & 16.31 & 4.94 & 6.71 & 7.89 & 18.69 \\
            GazeTR-50     & 15.47 & 4.94 & 7.22& 8.26 & 19.75\\
            ViT-L           & 19.32 & 5.09 & 6.33 & 8.98 & 22.68 \\
            \midrule
            FaRL-B          & 19.08 & 5.09 & 6.08 & 8.18 & 18.28 \\
            \methodname-B   & 11.78 & 4.73 & 5.86 & 6.31 & 12.41 \\
            \methodname-L   &\textbf{10.93}& 4.64 & 5.79 & 6.56 & 12.44  \\
            \methodname-H   & 11.29 &\textbf{4.51}& \textbf{5.47}&\textbf{5.88}&\textbf{12.37} \\
            \bottomrule
            \end{tabular}
        }
    \caption{
        Results of leave-one-dataset-out evaluation. 
        By taking the five gaze estimation datasets, we train the model on four datasets and test on the remaining one respectively. We show the results of each test dataset in the column. 
        Except for baselines, we evaluate three versions of \methodname with backbones of ViT-Base (\methodname-B), ViT-Large (\methodname-L), and ViT-Huge (\methodname-H).
    }
    \label{table:leave_out}
\end{table}



We further assemble the experiment results from our \methodname and compare them with the current SOTA domain generalization methods~\cite{cheng2022puregaze,xu2023learning,zhao2024improving,bao2024feature,yin2024clip}.
In \cref{table:sota}, we pick the typical cross-dataset setting, where the training dataset is either ETH-XGaze or Gaze360, and test on MPIIFaceGaze and screen targets (CS) subset of EYEDIAP.



Across all datasets, our \methodname-H model consistently achieves the lowest error, surpassing SOTA methods by significant margins.
Notably, while the ViT-H model pre-trained on ImageNet alone does not outperform domain generalization methods, our \methodname-H model pre-trained on large-scale data demonstrates significant improvements.
Again, this highlights that MAE pre-training on large-scale data greatly enhances ViT's ability for domain generalization, validating the effectiveness of our approach in improving gaze estimation across diverse domains.
Moreover, even \methodname-B, despite having a model size comparable to other SOTA methods, still achieves substantially better performance.



\begin{table}[t]
    \begin{center}
        \resizebox{0.95\linewidth}{!}{ %< auto-adjusts font size to fill line
        \setlength{\tabcolsep}{5.5pt}
            \begin{tabular}{l|ccccc}
            \toprule
            \textbf{Models \textbackslash~Test}  & \textbf{X\textsubscript{test}} & \textbf{M\textsubscript{test}} & \textbf{GC\textsubscript{test}} & \textbf{E\textsubscript{test}} & \textbf{G360\textsubscript{test}} \\
            \midrule
            ResNet-50   & 5.04 & 5.88 & 3.59 & 6.04 & 10.55 \\
            GazeTR-50 & 4.63 & 5.82 & 3.52 & 6.06 & 10.39 \\
            ViT-L & 5.24 & 5.42 & 3.66 & 6.30 & 10.54 \\
            \midrule
            \methodname-B & 4.75 & 5.40 & 3.37 & 5.52 & 9.64 \\
            \methodname-L & 4.67 & 5.12 & \textbf{3.17} & 5.33 & 9.29  \\
            \methodname-H & \textbf{4.46} & \textbf{5.08} & 3.20 & \textbf{5.16} & \textbf{9.07}  \\
            \bottomrule
            \end{tabular}
        }
    \end{center}
    \caption{
        Results of the joint-dataset evaluation protocol. We train the model on the gathered training splits of all five gaze estimation datasets.
        We test the model on the test set of each dataset individually, shown in each column.
    }
    \label{table:aggregate}
\end{table}





\begin{figure*}[t]
  \centering
  \begin{subfigure}{0.49\linewidth}
    \centering
    \includegraphics[width=0.88\linewidth]{figs/data_size/cross_X_perc_change.pdf}
    \caption{ The results when trained on ETH-XGaze.}
    \label{fig:plot_err_vs_data_size_xgaze}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.49\linewidth}
    \centering
    \includegraphics[width=0.9\linewidth]{figs/data_size/leave_one_dataset_out_perc_change.pdf}
    \caption{ The results of the leave-one-dataset-out evaluation.}
    \label{fig:plot_err_vs_data_size_leave_out}
  \end{subfigure}
  \caption{Effect of MAE pre-training data size on gaze estimation performance.
      The horizontal axis is the percentage of the pre-training data and the vertical axis is the percentage of the error reduction from the 0\% baseline.
    }
  \label{fig:plot_err_vs_data_size}
\end{figure*}




\subsection{Within-Dataset Evaluation}\label{sec:within}
To align with other SOTA methods, the splits for MPIIFaceGaze, ETH-XGaze, and EYEDIAP used in this section differ from the subject splits used elsewhere in the paper.
For GazeCapture and Gaze360, it is the same as defined in \cref{sec:exp_setting}.
For MPIIFaceGaze, we use the leave-one-subject-out protocol~\cite{abdelrahman2023l2cs,shi2024agent,ververas20253dgazenet}. 
For ETH-XGaze, we follow the train-test split provided in~\cite{ververas20253dgazenet}.
For EYEDIAP, we apply the four-fold validation scheme from~\cite{cheng2022gaze,cheng2024appearance}. 
The results in \cref{table:within} demonstrate that \methodname-H outperforms SOTA on most datasets, except MPIIFaceGaze, indicating that the proposed pre-training enhances generalizability even within the same dataset.





\subsection{Leave-One-Dataset-Out Evaluation}\label{sec:leave_one_out}


We conduct a \textit{leave-one-dataset-out} evaluation to further assess the generalizability of \methodname.
Using five gaze estimation datasets, we train the model on a combination of four datasets and test it on the held-out dataset. 
In this manner, we assess the upper-bound performance that can be achieved in each domain by maximizing the use of existing datasets.
We compare the baseline models of ResNet-50, GazeTR-50, and ViT-H that are pre-trained on ImageNet.
We present the three versions of \methodname with different backbones of ViT-Base, ViT-Large, and ViT-Huge.


We show the results in \cref{table:leave_out} with each column showing the results on each test dataset. 
Across test datasets, the proposed \methodname with three backbone sizes consistently surpasses all baselines, achieving substantial error reductions. 
By comparing the \methodname with three sizes of backbones, we can see that the trend clearly shows the larger model achieves better performances in general.
Notably, large-margin improvements from \methodname happen in the XGaze and Gaze360 test sets, highlighting that \methodname significantly enhances ViT’s generalization ability across diverse domains by fostering robust representations of facial appearance and image quality.



\subsection{Joint-Dataset Evaluation}\label{sec:joint}



Building upon the leave-one-dataset-out evaluation, we conduct a new evaluation protocol \textit{joint-dataset} evaluation.
In this setting, we gather the training sets of all five datasets, including XGaze Train, MPIIFaceGaze Train, GazeCapture Train, EYEDIAP Train, and Gaze360 Train to form a large and comprehensive training set.
The model trained on this large training set is expected to be the most powerful gaze estimator that we can acquire.
We test the trained model on the test sets of each dataset.
It has another meaning of generalization, in which one model performs gaze estimation across multiple data domains.


\Cref{table:aggregate} shows the evaluation results where each column represents a specific test dataset.
As expected, our \methodname-H model improves over other methods, achieving the lowest error on most test datasets compared to all baselines and smaller backbone versions.
Since the model is trained on data that covers the domains of all test datasets, the resulting errors are generally lower and saturated compared to other settings. 
It aligns with our purpose of training one model for all domains with the lowest error.


\subsection{Ablation Studies}\label{sec:ablation}

\subsubsection{Effect of Pre-Training Data Size}
The large-scale pre-training with MAE is the most critical factor for our \methodname. 
To analyze the impact of MAE pre-training data size, we vary the amount of data used for the pre-training stage. 
With the full pre-training data of around \SI{1.6}{\mega{}} images, we randomly sample subsets from each component dataset (\cref{sec:pretrain_data_prepare}) to create 25\%, 50\%, and 75\% subsets of the full data.
We use the \methodname-L as the backbone, and the 0\% refers to the ImageNet pre-trained ViT-L.


We present two experiment settings in \cref{fig:plot_err_vs_data_size}, the XGaze training (left) and the leave-one-dataset-out setting (right).
Beginning with the 0\% baseline, we illustrate the percentage reduction in error (vertical axis) as the pre-training dataset size (horizontal axis) increases, allowing us to capture and compare relative performance improvements across various pre-training levels.


Overall, the results indicate that the pre-trained model on a larger subset consistently achieves lower error across all test domains.
There are notable performance improvements with 25\% and 50\% of the data, and the improvement gap becomes smaller when reaching the 75\% subset to the full data, which is expected due to a sufficient amount of data diversity for the training.
These results confirm that increasing the amount of data in MAE pre-training strengthens ViT’s representation learning, leading to improved accuracy and generalization across diverse gaze estimation tasks.



\begin{table}[t]
    \centering
    \resizebox{1.0\linewidth}{!}{ %< auto-adjusts font size to fill line
    \setlength{\tabcolsep}{4.5pt}
         \begin{tabular}{l|ccccc}
        \toprule
        \textbf{Models \textbackslash~Test} & \textbf{X\textsubscript{test}} & \textbf{M} & \textbf{GC} & \textbf{E} & \textbf{G360} \\
        \midrule
        FaRL-B~\cite{zheng2022general}  & 19.08 & 5.09 & 6.08 & 8.18 & 18.28 \\
        \midrule
        \methodname-B (\textit{Real, {\small limited poses}}) & 16.70 & 5.29 & 6.87 & 6.57 & 13.78 \\
        \methodname-B (\textit{Real, {\small w/o norm.}}) & 14.56 & 4.95 & 6.27 & 6.93 & 14.48 \\
        \methodname-B (\textit{Real}) & 11.95 & 4.86 & 6.14 & \textbf{6.26} & 12.71 \\
        \methodname-B & \textbf{11.78} & \textbf{4.73} & \textbf{5.86} & 6.31 & \textbf{12.41} \\
        \bottomrule
        \end{tabular}
        }
    \caption{
    Comparison of different formats of the input data in the leave-one-dataset-out evaluation setting. Each column shows the results on each test dataset. For fair compassion with FaRL-B~\cite{zheng2022general}, we use the ViT-B backbone (\methodname-B). 
    \textit{Real} stands for the combination of our real datasets CelebV-Text, VFHQ, and VGGFace2.
    }
\label{table:ablation_data_format}
\end{table}



\subsubsection{Effect of Pre-Training Data Attributes}
To better understand the performance gap from FaRL-B~\cite{zheng2022general}, we examine the impact of different data in \cref{table:ablation_data_format}.
To assess the effect of the synthetic data, we first limit the pre-training dataset to the real datasets: CelebV-Text, VFHQ, and VGGFace2 (\textit{Real}).
To evaluate the impact of pre-processing specifically for gaze estimation, we also assess the performance when pre-training is conducted using the loose face bounding boxes directly, without applying any data normalization~\cite{zhang2018revisiting} (\textit{Real, w/o norm.}).
To evaluate the effect of wide head pose range, we limit head pose variability by filtering out samples with a pitch-yaw $L_2$ norm exceeding 10 degrees, reducing the dataset size to about 20\% (\textit{Real, limited poses}). 
Note that the different amounts of data could also affect the model's performance.
We use the leave-one-dataset-out evaluation protocol, given the trade-off between the task difficulty and simplification.


The results demonstrate that the \methodname-B trained on full data performs best compared to other baselines in almost all settings.
Models without normalization (\textit{Real, w/o norm.}) show substantially degraded performance despite using identical data.
Models with limited pose ranges (\textit{Real, limited poses}) also perform significantly worse than those trained on a more diverse range.
Integrating real and synthetic data yields the best results, which can be attributed to combining naturalistic appearance variations from real data with controlled pose variations from synthetic data.



