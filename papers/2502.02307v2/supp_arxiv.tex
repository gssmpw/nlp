\clearpage
\setcounter{page}{1}
\setcounter{figure}{0}
\setcounter{table}{0}
\setcounter{equation}{0}

\appendix


\begin{center}
    {\Large \bfseries Supplementary Materials}\\[1em]
\end{center}




\begin{table*}[t]
    \centering
    \resizebox{1.0\linewidth}{!}{ %< auto-adjusts font size to fill line
    \setlength{\tabcolsep}{4.5pt}
         \begin{tabular}{l|ccccc}
        \toprule
        \textbf{Training Data \textbackslash~Test}  & \textbf{X\textsubscript{test}} & \textbf{M\textsubscript{test}} & \textbf{GC\textsubscript{test}} & \textbf{E\textsubscript{test}} & \textbf{G360\textsubscript{test}} \\
        \midrule
        \rowcolor{LightGray} \multicolumn{6}{l}{ResNet-50}  \\
         \textit{same-domain}       & 5.25  & 5.11   & 3.49  &  8.51 & 11.87\\
         \textit{leave-one-dataset-out}    & 16.31 (\increase{210.7}) & 6.23 (\increase{21.9}) & 6.35 (\increase{82.0}) & 8.25 (\reduce{3.1}) & 20.38 (\increase{71.7}) \\
         \textit{joint-dataset} & \textbf{5.04} (\reduce{4.0}) & 5.88 (\increase{15.1}) & 3.59 (\increase{2.9}) & \textbf{6.04} (\reduce{29.0}) & \textbf{10.55} (\reduce{11.1}) \\
         \midrule
        \rowcolor{LightGray}
        \multicolumn{6}{l}{\methodname-H} \\
        \textit{same-domain}   & 4.62 & 5.19 & 3.01 & 6.11 & 9.44  \\
         \textit{leave-one-dataset-out} & 11.29 (\increase{144.4}) & 5.22 (\increase{0.6}) & 5.13 (\increase{70.4}) & 6.14 (\increase{0.5}) & 13.12 (\increase{39.0}) \\
          \textit{joint-dataset} & \textbf{4.46} (\reduce{3.5}) & \textbf{5.08} (\reduce{2.1}) & 3.20 (\increase{6.3}) & \textbf{5.16} (\reduce{15.6}) & \textbf{9.07} (\reduce{3.9})  \\
        \bottomrule
        \end{tabular}
        }
    \caption{
        Comparison of different training data configurations for gaze estimation.
        Each column represents a specific test dataset: XGaze Test, MPIIFaceGaze Test, GazeCapture Test, EYEDIAP Test, and Gaze360 Test.
        Each row corresponds to a training configuration:
        \textit{Same-domain} means training on the same domain as the test set,
        \textit{leave-one-dataset-out} means training on the remaining four datasets other than the test set, and \textit{joint-dataset} means training on the aggregated Train split of all five datasets.
        The percentages in parentheses indicate the reduction or increment compared to the \textit{same-domain} results, where lower errors indicate better performance.
        For the \textit{leave-one-dataset-out} configuration, the errors reported here are on the Test splits, while the main paper reports errors on the entire dataset.
    }
\label{table:comprehensive}
\end{table*}



\noindent In this supplementary material, we first provide an analysis of the effect of combining multiple domains.
Then, we include additional ablations to investigate the effects of color-jitter augmentation and pixel normalization during the pre-training.
Finally, we present qualitative results, highlighting images captured under diverse and challenging conditions.



\section{Analysis on Combining Multiple Domains}\label{sec:supp_data}

We analyze the effect of different training data configurations on gaze estimation performance.
Specifically, we compare three configurations: training on the same domain (\textit{same-domain}), training on multiple domains excluding the testing domain (\textit{leave-one-dataset-out}), and training on multiple domains including the testing domain (\textit{joint-dataset}).

\Cref{table:comprehensive} shows the comparison of gaze errors for these configurations.
Each column corresponds to a specific test dataset: XGaze Test, MPIIFaceGaze Test, GazeCapture Test, EYEDIAP Test, and Gaze360 Test, while each row represents a training configuration.
This \textit{same-domain} setting is different from the \textit{within-dataset} in the main paper. 
We use the splits defined in Sec.~4.1 of the main paper.
Especially, please note that for MPIIFaceGaze dataset, we train the model on the first 10 subjects and test on the remaining five subjects, different from the typical leave-one-subject-out protocol~\cite{abdelrahman2023l2cs,shi2024agent,ververas20253dgazenet}.

The percentages in parentheses indicate the reduction or increment compared to the \textit{same-domain} results, where lower errors indicate better performance.
Note that, for the \textit{leave-one-dataset-out} configuration, errors on the entire left-out dataset are reported in the main paper, but here we present errors on the Test split to align with the other configurations that require dataset splits.



\paragraph{\textit{Same-domain}}
In general, training and testing on the same domain (\textit{same-domain}) yields the best results, even though datasets combined from multiple domains have the potential to be more diverse.
This emphasizes the persistent challenge of achieving optimal performance when using data from different domains.
The exception observed for E\textsubscript{test} with the ResNet-50 backbone may be attributed to the limited number of samples in the EYEDIAP Train split.

\paragraph{\textit{Leave-one-dataset-out}}
In the \textit{leave-one-dataset-out} configuration, we observe varying tendencies across different test datasets. 
Some datasets achieve errors comparable to the \textit{same-domain} results, while others remain challenging. 
For instance, for M\textsubscript{test} and E\textsubscript{test}, which are relatively less complex, the remaining four datasets provide sufficient information to achieve good performance.
In contrast, for X\textsubscript{test}, GC\textsubscript{test}, and G360\textsubscript{test}, the remaining four datasets fail to fully capture the critical factors required for optimal performance.
This variation highlights the strong dependence of performance on the attributes of the training data.

Importantly, our \methodname-H demonstrates a smaller performance gap compared to ResNet-50 in most cases, with the only exception being EYEDIAP, where the difference is marginal. 
This suggests that \methodname-H is better equipped to learn gaze representations from out-of-domain data with less overfitting, underscoring its enhanced generalization capability.


\paragraph{\textit{Joint-dataset}}

Overall, the \textit{joint-dataset} configuration demonstrates significant promise, creating a single model robust across multiple test domains. 
For \methodname-H, the only exception is GC\textsubscript{test}, where the \textit{joint-dataset} configuration produces a slightly higher error (3.01$\rightarrow$3.20). 
Although this suggests some negative effects from the other four datasets, the effects remain marginal.
While the improvement percentages for \methodname-H are smaller compared to ResNet-50, the absolute errors are consistently lower.


\begin{table}[t]
    \centering
    \resizebox{0.8\linewidth}{!}{ %< auto-adjusts font size to fill line
    \centering
    \setlength{\tabcolsep}{6pt}
        \begin{tabular}{ccc|cccc}
        \toprule
        \textbf{\textit{Real}} & \textbf{\textit{Syn.}} & \textbf{\textit{NV.}}  & \textbf{M} & \textbf{GC} & \textbf{E} & \textbf{G360} \\
        \midrule
        \checkmark &            &            & 6.79 & 7.81 & 6.86 & 12.93 \\
        \checkmark & \checkmark &            & 6.57 & 7.37 & \textbf{6.51} & 13.23 \\
        \checkmark & \checkmark & \checkmark & \textbf{6.21} & \textbf{7.35} & 6.64 & \textbf{12.18} \\
        \bottomrule
        \end{tabular}
    }
    \caption{
    We ablate the pre-training facial datasets by comparing real, synthetic, and novel-rendered images.
    The comparison is performed on the \methodname-B network, followed by training on XGaze.
    The last row represents the full-dataset setting.
    }
\label{table:ablation_data_type}
\end{table}


\begin{figure*}[t]
  \centering
  \begin{subfigure}{0.99\linewidth}
    \centering
    \includegraphics[width=0.99\linewidth]{figs/supp/MAE_without_pixel_norm.pdf}
    \caption{ MAE reconstruction examples without pixel normalization.}
    \label{fig:no_pixel_norm}
  \end{subfigure}
  % \hfill
  \begin{subfigure}{0.99\linewidth}
    \centering
    \includegraphics[width=0.99\linewidth]{figs/supp/MAE_with_pixel_norm.pdf}
    \caption{ MAE reconstruction examples with pixel normalization (Proposed).}
    \label{fig:w_pixel_norm}
  \end{subfigure}
  \caption{Examples comparison of the pixel normalization during the MAE pre-training.
  The left, middle, and right columns show the original image, masked input, and the reconstructed image, respectively.
    }
  \label{fig:pixel_norm_samples}
\end{figure*}


\begin{figure}[t]
  \centering
    \includegraphics[width=0.9\linewidth]{figs/data_diversity/leave_perc_change.pdf}
  \caption{
        Effect of MAE pre-training dataset composition on downstream gaze estimation performance.  
        The horizontal axis represents the incremental accumulation of datasets, while the vertical axis shows the percentage reduction in error relative to the first CelebV-Text dataset~\cite{yu2023celebv}.
    }
  \label{fig:plot_err_vs_data_composition}
\end{figure}


\section{Additional Ablation Studies on Pre-Training}


\paragraph{Effect of Pre-Training Dataset Composition}


Beyond the overall pre-training dataset size, the composition of the dataset also plays a critical role in learning effective gaze representations.
To investigate the impact of different facial dataset components, we conduct an experiment where we incrementally accumulate datasets during the MAE pre-training stage and analyze their effect on the downstream gaze estimation performance.
Starting with CelebV-Text~\cite{yu2023celebv}, we progressively add datasets for pre-training and evaluate model separately on gaze estimation.
Each pre-trained model is subsequently trained on gaze datasets using the same \textit{leave-one-dataset-out} protocol.
\Cref{fig:plot_err_vs_data_composition} illustrates the error change across different test sets as more datasets are included in pre-training.



Overall, the results indicate that adding more diverse data during pre-training generally enhances gaze generalization.
However, there are exceptions that adding a model can result in increased error for specific test sets.
For example, adding VFHQ increases the error on the XGaze Test set from 12.65 to 13.32, while including SFHQ-T2I causes performance fluctuations across different benchmarks.
This suggests that certain dataset attributes may not align well with particular test distributions, leading to suboptimal transferability.
On the other hand, datasets such as VGGFace2 and XGaze-Dense provide performance improvements on most test sets.
Additionally, performance gains becomes marginal as the dataset number increases, aligning with the analysis of pre-training data size in main paper.


In conclusion, dataset diversity plays a crucial role in improving MAE pre-training for gaze estimation.
A more detailed analysis of dataset attributes and their impact on gaze estimation remains an open research question, which we leave for future work.
Nonetheless, our empirical results suggest that increasing data diversity in pre-training tends to improve model performance across various test domains.



\paragraph{Effect of Novel-View Synthesis Data in Pre-Training}\label{sec:ablation_novel_view_data}
To examine the effect of novel-view synthesis in pre-training data, we conduct further experiments separating these two elements.
In \cref{table:ablation_data_type}, we conduct an ablation study by varying data subsets during the pre-training: real datasets (CelebV-Text, VGGFace2, and VFHQ), synthetic datasets (FaceSynthetics and SFHQ-T2I), and novel-view-rendered datasets (FFHQ-NV and XGaze-Dense).
We use the \methodname-B to conduct the experiment due to its time efficiency.
After pre-training, we train on XGaze and test on the rest of the four datasets. 

The results further clarify the effect of different data types on the model's generalizability.
Adding synthetic data (\textit{Real + Syn.}) reduces errors in several test domains compared to using only real data, suggesting the variability of the synthetic data contributes to generalization.
Further incorporating novel-view data (\textit{Real + Syn. + NV}) provides additional performance gains, especially in head-pose generalization, likely due to the expanded range of facial orientations.
This finding supports the idea that a mix of real, synthetic, and novel-view data in MAE pre-training strengthens ViT’s representation learning.



\paragraph{Effect of Pixel Normalization}\label{sec:ablation_pixel_norm}
The patch normalization technique is applied during the MAE pre-training as suggested in~\cite{he2022masked} which is different from reconstructing the natural image, as shown in \cref{fig:pixel_norm_samples}.
We compare models pre-trained with and without patch normalization to investigate its impact.

\paragraph{Effect of Color-Jitter Augmentation}\label{sec:ablation_augmentation}
Color jittering introduces randomness in brightness, contrast, saturation, and hue to simulate diverse lighting conditions, enhancing the robustness of learned features.
We compare models pre-trained with and without color-jitter augmentation to investigate its impact.

\paragraph{Results}
We use the \methodname-B model as the backbone and compare different pre-training settings, followed by training on XGaze and testing on the remaining four datasets.
\Cref{table:ablation_colorjitter_pixelnorm} demonstrates that both color-jitter augmentation and pixel normalization contribute to improved gaze estimation performance, highlighting their benefits for the generalization of the pre-trained model.
Notably, pixel normalization consistently improves performance across all test datasets, aligning with the observations in the original MAE paper~\cite{he2022masked}, which showed that pixel normalization enhances representation learning.



\begin{table}[t]
    \centering
    \resizebox{0.97\linewidth}{!}{ %< auto-adjusts font size to fill line
    \centering
    \setlength{\tabcolsep}{6pt}
        \begin{tabular}{cc|cccc}
        \toprule
        \textbf{Color-Jitter} & \textbf{Pixel Norm.} & \textbf{M} & \textbf{GC} & \textbf{E} & \textbf{G360} \\
        \midrule
         \XSolidBrush & \XSolidBrush     & 7.52 & 8.01 &  8.56 & 14.14 \\
        \checkmark & \XSolidBrush   &  7.17 & 8.23 & 8.03 & 14.03 \\
        \XSolidBrush   & \checkmark &  7.18 & 7.94 & 8.05 & 13.66 \\
        \checkmark & \checkmark & \textbf{6.21} & \textbf{7.35} & \textbf{6.64} & \textbf{12.18} \\
        \bottomrule
        \end{tabular}
    }
    \caption{
        Ablation studies on the pre-training, comparing the effect of the color-jitter augmentation and the pixel normalization.
        During the gaze estimation training, we train the model using XGaze and test on the other four datasets to evaluate the generalizability.
    }
\label{table:ablation_colorjitter_pixelnorm}
\end{table}




\section{Comparison with the SOTAs}
3DGazeNet~\cite{ververas20253dgazenet} collects in-the-wild face images with pseudo gaze labels and applies multi-view synthesis to obtain an augmented dataset ITWG-MV.
To account for the difference in test data settings, we compare 3DGazeNet with \methodname-H separately in \cref{table:supp_sota_3dgn}.
The results demonstrate that \methodname-H outperforms 3DGazeNet in all domain generalization settings.

\paragraph{Re-implementation}
In the main paper, we compared our \methodname-H model with state-of-the-art (SOTA) methods using their reported results. 
It is important to note that minor discrepancies may arise due to differences in our data pre-processing compared to prior work~\cite{cheng2022puregaze,xu2023learning,zhao2024improving}. 
To ensure a fair comparison, we re-implemented ResNet-18 and PureGaze~\cite{cheng2022puregaze} using our pre-processed datasets, aligning them with the reported results~\cite{cheng2022puregaze,zhao2024improving}. 
The re-implementation results, alongside the reported values, are summarized in \cref{table:supp_sota}.

While minor differences exist between our re-implementation and the reported values, the improvements achieved by our \methodname-H model remain significant, demonstrating its superior performance across all domain generalization tasks. 



\begin{table}[t]
\begin{center}
    \resizebox{0.99\linewidth}{!}{ %< auto-adjusts font size to fill line
    \setlength{\tabcolsep}{4pt}
        \begin{tabular}{l|cccc}
        \toprule
        \textbf{Models}  & \textbf{X}$\rightarrow$\textbf{M} & \textbf{X}$\rightarrow$\textbf{GC}& \textbf{G360}$\rightarrow$\textbf{M} & \textbf{G360}$\rightarrow$\textbf{GC} \\
        \midrule
        \rowcolor{LightGray}
        3DGazeNet$^{\dagger}$~\cite{ververas20253dgazenet} & 6.0 & 7.8 & 6.3 & 8.0  \\
        \midrule
        \methodname-H & \textbf{5.57} &  \textbf{6.56} & \textbf{5.43} & \textbf{6.48} \\
        \bottomrule
        \end{tabular}
}
\end{center}
\caption{
    Domain generalization compared with SOTA methods. 
    The results marked with $^{\dagger}$ are directly cited from previous studies~\cite{ververas20253dgazenet}.
}
\label{table:supp_sota_3dgn}
\end{table}


\begin{table}[t]
\begin{center}
    \resizebox{0.99\linewidth}{!}{ %< auto-adjusts font size to fill line
    \setlength{\tabcolsep}{4pt}
        \begin{tabular}{l|cccc}
        \toprule
        \textbf{Models}  & \textbf{X}$\rightarrow$\textbf{M} & \textbf{X}$\rightarrow$\textbf{E\textsubscript{CS}}& \textbf{G360}$\rightarrow$\textbf{M} & \textbf{G360}$\rightarrow$\textbf{E\textsubscript{CS}} \\
        \midrule
        ResNet-18   & 7.57 & 9.54 & 9.24  & 8.07  \\
        \rowcolor{LightGray}
        ResNet-18$^{\dagger}$~\cite{zhao2024improving}  & 8.02 & 9.11 & 8.04 & 9.20 \\
        PureGaze & 6.68 & 7.62 & 8.87 & 10.53 \\ %% the paper used pure50 for XGaze, pure18 for Gaze360
        \rowcolor{LightGray}
        PureGaze$^{\dagger}$~\cite{cheng2022puregaze}  & 7.08 & 7.48 & 9.28 & 9.32 \\
        \midrule
        \methodname-H & \textbf{5.57} &  \textbf{4.65} & \textbf{5.43} & \textbf{5.35} \\
        \bottomrule
        \end{tabular}

}
\end{center}
\caption{
    Domain generalization compared with SOTA methods and their re-implementations. 
    The results marked with $^{\dagger}$ are cited from previous studies~\cite{cheng2022puregaze,zhao2024improving}, and the rest of the results are based on our implementation.
}
\label{table:supp_sota}
\end{table}




\section{Implementation Details}

\paragraph{Novel-Rendered Data Preparation}

To render images from novel views, we follow the rendering approach described in~\cite{qin2022learning}.
To control the head pose, we randomly generate target head poses and compute the corresponding rotation matrices to apply to the 3D face models. 
During the rendering process, 40\% of the images are assigned a random background color, while the remaining 60\% use random scene images from the Places365 dataset~\cite{zhou2017places} as background. 
Additionally, to simulate varied lighting conditions, half of the rendered images are adjusted to have lower ambient light intensity, ranging from $0.2$ to $0.75$.

All face images in our method are in the size of \num{224} $\times$ \num{224} after the data normalization process~\cite{zhang2018revisiting}.
When the camera parameters are unknown, we use a camera matrix with focal length $f$ set to the image width and principal point $(c_x, c_y)$ set to half the image height and width.

\paragraph{Pre-Training}
We apply random color jitter augmentation with a probability of 0.5 and the following parameters: hue in the range $[-0.15, 0.15]$, saturation in $[0.8, 1.2]$, contrast in $[0.4, 1.8]$, and brightness in $[0.7, 1.3]$.
We apply random grayscale with a probability of 0.05 on all images.

\paragraph{Gaze Estimation Training}

We use the Adam optimizer~\cite{kingma2014adam} with a learning rate of \num{1e-4} and a weight decay of \num{1e-6} for all experiments.
For experiments with ResNet-50 and GazeTR-50, we set the batch size to 128 and decay the learning rate by 0.1 every five epochs, with a total of 12 epochs.
For cross-dataset evaluation with \methodname-H, we use a batch size of 128 and train the model for eight epochs with the one-cycle learning rate schedule~\cite{smith2019super}.
For \textit{leave-one-dataset-out} and \textit{joint-dataset} evaluations, we set the batch size to 160 with 12 epochs.

\section{Qualitative Results}
In this section, we present additional qualitative results using the \methodname-H model trained on the aggregated datasets under the \textit{joint-dataset} setting. 
We employ an off-the-shelf facial landmark detector~\cite{bulat2017far} to extract landmarks and perform data normalization.
Gaze estimation is conducted on the normalized images, and the results are de-normalized back to the original image for visualization.
For reference, we also include the normalized faces alongside the original images.

\Cref{fig:results_qual_wild_one_person} and \cref{fig:results_qual_wild} showcase examples from various in-the-wild videos captured under challenging conditions, including large head poses and diverse lighting environments.
Notably, we also include a synthetic example from URAvatar~\cite{li2024uravatar} (bottom row in \cref{fig:results_qual_wild}), which generates faces with controlled viewpoints and lighting.
Furthermore, \cref{fig:results_qual_VAT} presents examples from the gaze-following dataset VideoAttentionTarget~\cite{chong2020detecting}, a collection of diverse samples extracted from movies. 
This dataset provides annotated gaze targets, which are visualized when annotated within the image frame, as some targets may be out of frame.

These examples highlight the model's ability to predict gaze direction accurately in unseen environments, even under extreme head poses, challenging lighting conditions, and synthetic appearances.



\input{sec/qualitative}

\section{Ethical Considerations}

Our research involves the use of existing facial and gaze datasets.
In accordance with ethical guidelines, we rely on the fact that these datasets were originally collected and published following relevant ethical and data protection standards, including obtaining consent, and we do not generate or collect additional new data.
Our experimental protocols involve only image content, with no identifiable personal information or links to other personal data.



