\section{Related Work}
\label{sec:relwork}
We survey closely related work on floating-point formalization and testing-based specification discovery, followed by some non-floating-point formalization efforts.

An SMT theory for floating point reasoning was proposed by \citet{rummer2010smt}, which also included formalizations for rounding modes.  However, SMT-based floating point reasoning has historically been found to have poor scalability~\cite{darulova2014sound,schkufza2014stochastic}, but has been successfully used for error analysis~\cite{solovyev2018rigorous}. \citet{leeser2014make} demonstrated success in using SMT for floating-point reasoning, albeit using the theory of Reals.  \citet{martin2015automatable} redefined the floating point theory, substantially improving SMT's capabilities.  \citet{darulova2018daisy} used SMT to statically analyze floating point programs, for instance to compare roundoff errors between fixed-point and floating-point arithmetic. Floating point capabilities have similarly been implemented in other theorem provers, such as Coq~\cite{2011flcoq} which has also been used for error analysis~\cite{appel2024vcfloat2}. Each of these formalizations follows the IEEE standard~\cite{ieee-754-2008} and hence do not contain support for the non-standard accumulator which our work provides.
Titolo et al.~\cite{laura-titolo-munoz-vmcai} present an abstract interpretation framework for floating-point program roundoff error analysis.

Using tests to identify the implementation peculiarities of floating point units dates as far back as~\citet{paranoia}.  
In the case of GPU tensor cores, there has been considerable interest in understanding their functional as well as performance characteristics. \citet{sun2022dissecting} studied the tensor core implementations across various NVIDIA architectures.  While they primarily focused on the throughput and latency, they briefly investigated the numerical behavior of tensor cores by studying the relative error for different floating point formats. \citet{blanchard2020mixed} devised a framework to perform an error analysis of block fused multiply-add units.  Their method incorporates the supported precision of the uni in its formulation, allowing it to support future units that may offer a different precision. \citet{Hickmann2019ExperimentalAO} and \citet{fasi2021numerical} studied tensor cores by using carefully constructed experiments to determine the hardware's behavior such as its rounding mode, precision, and support for subnormals. Xinyi et al.~\cite{xinyili24} employed similar techniques while further exploring the block-FMA feature and additional bits for tensor cores and AMD's matrix cores.  \citet{yan2020demystifying} also studied the instruction-level details of the tensor cores, providing insights into how the matrix operation is performed on Turing, showing how the threads in a warp cooperate to compute the mma operation.

Formal descriptions of architectural components have been used to detect subtle correctness and security properties unrelated to floating-point arithmetic.
The Check tools (TriCheck~\cite{trippel2017tricheck}, CoatCheck~\cite{lustig_coatcheck_2016}, 
CCICheck~\cite{manerkar_ccicheck_2015}, 
PipeCheck~\cite{lustig_pipecheck_2014}), focus on memory consistency models and highlight the pitfalls resulting from under-specified ISA details.
The CheckMate tool~\cite{trippel_checkmate_2018} uses model checking to automatically create exploits for cache side channels.
Manual formalization of specifications is costly and this has led to work that seeks to automate the creation of formal ISA semantics. SAIL~\cite{armstrong_isa_2019} and K~\cite{dasgupta_complete_2019} have been explicitly built for ISA specifications. For x86, \citet{godefroid-taly} leveraged SMT to find input examples, while ~\citet{heule_stratified_2016} explored stratified synthesis. Using program synthesis has been explored to automatically formalize hardware specifications for memory consistency models~\cite{hsiao2021synthesizing,norman2023pipesynth}.




\section{PTX and SASS Background}
\label{sec:ptx-sass}

The NVIDIA GPUs we use in this work are commonly programmed in the CUDA programming language, a C++ dialect that supports explicit data parallelism and the ability to specify which functions run on the CPU and which run on the GPU.
To use the tensor cores, CUDA provides library functions that are internally implemented using inline assembly in the \textit{virtual} PTX instruction set architecture (ISA)~\cite{nvidia-ptx}.
PTX is a GPU independent ISA which resembles a compiler intermediate representation with features such as types, infinite registers, scoping, and so on. 
The physical ISA, commonly referred to as SASS~\cite{BinaryUtilities}, resembles a more traditional machine ISA and, unlike PTX, changes across GPU architectures.
PTX is compiled to SASS using an architecture-specific assembler called \texttt{ptxas}.
PTX provides forward compatibility with newer GPU architectures. 
If the GPU driver does not find the SASS for the current architecture in the executable, it will recompile the PTX in the executable at runtime to the architecture-appropriate SASS.
While NVIDIA provides a PTX specification, it does not provide any information about SASS, prompting reverse engineering efforts~\cite{sun2022dissecting, yan2020demystifying, fang2022towards,hayes_decoding_2019}.

\subsection{Tensor Cores and the \texttt{HMMA} Instruction}
\label{sec:tcbg}





\texttt{HMMA} is the primary SASS instruction that interacts with the tensor cores~\cite{jia2018dissecting}.
Programmers usually use CUDA's Warp Matrix or \texttt{wmma} Functions~\cite[\S7.24]{cuda-c-programming} to use the tensor cores.
However, the cores can also be accessed directly using inline assembly by using the PTX \texttt{mma.m8n8k4} instruction. 
Examining with \texttt{cuobjdump}~\cite{BinaryUtilities} the disassembly of SASS programs that use either of these methods confirms variants of the \texttt{HMMA} SASS instruction are used.


 



Across different architectures, the behavior of the tensor cores and its corresponding \texttt{HMMA} instruction changes. On Volta and Turing, the tensor cores are invoked via the \texttt{HMMA.884} instruction, while the Ampere architecture replaced this with \texttt{HMMA.16816}. Both instructions multiply two matrices, $A$ and $B$, and add a third matrix, $C$, though the \texttt{884} variant operates on $4\times4$ matrices, while the \texttt{16816} variant operates on $8\times8$ matrices. In fact, this change highlights another portability concern: the \texttt{mma.m8n8k4} PTX instruction that previously invoked tensor cores no longer does so on the Ampere architecture. Instead, it produces a sequence of Fused Multiply-Add (\texttt{FMA}) instructions that use the device's slower floating point cores whose numerical properties are quite different from tensor cores.

For both the \texttt{884} and \texttt{16816} variants, the \texttt{HMMA} operation consists of three steps: 1) multiplying matrix A and B, 2) accumulating the products of A, B along with matrix C, and 3) rounding the final result.  Each element in the resulting $N\times N$ matrix $D$ is computed via the following equation:

\begin{equation}\label{eq:matrix-mul-formula}
\begin{split}
    D_{i,j} = A_{i,1}\cdot B_{1,j} + A_{i+1,1}\cdot &B_{1,j+1} + \ldots + A_{N, 1}\cdot B_{1, N}
    \end{split}
\end{equation}


Unlike most GPU instructions, where each thread's calculations are independent of other threads, the \texttt{HMMA} instruction requires all threads within the warp to cooperate to compute the result, and only one matrix multiplication is performed per warp per instruction. Prior work by \citet{yan2020demystifying,fang2022towards} has described how matrix elements are mapped to each participating threads' registers.
The \texttt{HMMA} instruction supports both F16 and F32 types for elements of $C$ and $D$ which can be individually specified. $A$ and $B$ are always F16. Since Volta, tensor cores introduced have support for more formats: INT4 and INT8 in Turing, followed by Ampere's support for double-precision (FP64) and a custom format, Tensor Float 32 (TF32). This work focuses on the FP16 and FP32 formats that are supported on all tensor cores, though the properties we establish can be adapted to study tensor core behavior for different formats.