\documentclass[runningheads]{llncs}

\usepackage[T1]{fontenc}
\usepackage{graphicx} \usepackage{amsmath}
\usepackage{bm}
\usepackage{listings}
\usepackage{orcidlink}
\usepackage{hyperref}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{float}
\usepackage[numbers]{natbib}
\usepackage{color, colortbl}
\usepackage{algorithm,algpseudocode}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}
\PassOptionsToPackage{hyphens}{url}
\usepackage[hyphenbreaks]{breakurl}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{svg}
\usetikzlibrary{chains,automata,positioning, shapes, arrows.meta,matrix,calc}


\usepackage{color,xspace,listings,comment,tabulary} \newcommand{\asplossubmissionnumber}{411}
\input{macros}


\usepackage[normalem]{ulem}

\tikzset{
  treenode/.style = {shape=rectangle, rounded corners,
                     draw, anchor=center,
                     text width=5em, align=center,
                     inner sep=1ex},
  root/.style     = {treenode, font=\ttfamily},
  env/.style      = {treenode, font=\ttfamily\normalsize},
  finish/.style   = {root},
}

\begin{document}
\newtheorem{myprop}{Property}

\title{An SMT Formalization of Mixed-Precision Matrix Multiplication}
\subtitle{Modeling Three Generations of Tensor Cores}
\author{Benjamin Valpey\inst{1} \and
Xinyi Li\inst{2} \and
Sreepathi Pai\inst{1} \and Ganesh Gopalakrishnan \inst{2}}
\authorrunning{B. Valpey et al.}
\institute{University of Rochester, Rochester, NY 14627, USA \email{\{bvalpey,sree\}@cs.rochester.edu}\\ \and University of Utah, Salt Lake City, UT 84112, USA \email\{xin\_yi.li@utah.edu, ganesh@cs.utah.edu\}
}

\date{}
\maketitle

\begin{abstract}
Many recent computational accelerators provide non-standard (e.g.,
reduced precision) arithmetic operations to enhance
performance for floating-point matrix multiplication.  Unfortunately, the properties of these accelerators are
not widely understood and lack sufficient descriptions of their behavior. This makes it difficult for tool builders beyond the original vendor to target or simulate the hardware correctly, or for algorithm
designers to be confident in their code. To address these gaps, prior studies have probed the behavior of these units with manually crafted tests. Such tests are cumbersome to design, and adapting them as the accelerators evolve requires repeated manual effort.

We present a formal model for the tensor cores of
Nvidia's Volta, Turing, and Ampere GPUs.  We identify specific
properties---rounding mode, precision, and accumulation order---that
drive these cores' behavior.  We formalize these properties and then use
the formalization to automatically generate discriminating inputs that
illustrate differences among machines.  Our results confirm many of the
findings of previous tensor core studies, but also identify subtle
disagreements.  In particular, Nvidia's machines do not, as previously
reported, use round-to-zero for accumulation, and their 5-term
accumulator requires 3 extra carry-out bits for full accuracy.  Using
our formal model, we analyze two existing algorithms that use
half-precision tensor cores to accelerate single-precision
multiplication with error correction.  Our analysis reveals that the
newer algorithm, designed to be more accurate than the first, is
actually less accurate for certain inputs.
\end{abstract}

\section{Introduction}\label{sec:intro}
\input{intro.tex}

\label{sec:bg}
\input{bg.tex}

\input{fasi-case-study}
\input{ootomo-case-study}


\section{Conclusions and Future Work}
\input{conc}

\section*{Acknowledgments}
This work is supported in part by NSF Awards 2403379, 2346394, 2217154, and 2124100.

\bibliographystyle{plainnat}
\bibliography{valpey-bibliography,ganesh,sree}

\clearpage
\onecolumn

\end{document}
