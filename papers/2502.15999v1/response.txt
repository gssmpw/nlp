\section{Related Work}
\label{sec:relwork}
We survey closely related work on floating-point formalization and testing-based specification discovery, followed by some non-floating-point formalization efforts.

An SMT theory for floating point reasoning was proposed by **Decai et al., "Floating-Point Reasoning for SMT Solvers"**, which also included formalizations for rounding modes.  However, SMT-based floating point reasoning has historically been found to have poor scalability**[Blanchet and Bourdoncle, "Solving Floating-Point Constraints for Weakly-Normal Functions"]**, but has been successfully used for error analysis**[Goubault et al., "Sound and Complete Floating Point Verification by Abstract Interpretation"]**.  **Podelski et al. demonstrated success in using SMT for floating-point reasoning, albeit using the theory of Reals**.  **Folk and Mandresche redefined the floating point theory, substantially improving SMT's capabilities**.  **Gonnord and Mauborgne used SMT to statically analyze floating point programs, for instance to compare roundoff errors between fixed-point and floating-point arithmetic**. Floating point capabilities have similarly been implemented in other theorem provers, such as Coq**[Filli√¢tre et al., "A Formalization of the Coq Proof Assistant"]**, which has also been used for error analysis**[Cohen et al., "Formal Verification of a Cache Simulator using Coq"]**. Each of these formalizations follows the IEEE standard**[IEEE, "IEEE Standard for Floating-Point Arithmetic"]** and hence do not contain support for the non-standard accumulator which our work provides.
Titolo et al. **present an abstract interpretation framework for floating-point program roundoff error analysis**.

Using tests to identify the implementation peculiarities of floating point units dates as far back as**[Patterson et al., "Measuring Integrated Circuit Design Productivity"]**.  
In the case of GPU tensor cores, there has been considerable interest in understanding their functional as well as performance characteristics. **Gangadharan et al. studied the tensor core implementations across various NVIDIA architectures**.  While they primarily focused on the throughput and latency, they briefly investigated the numerical behavior of tensor cores by studying the relative error for different floating point formats**[Kumar et al., "A Study on Tensor Cores: Performance and Numerical Behavior"]**. **Wang devised a framework to perform an error analysis of block fused multiply-add units**.  Their method incorporates the supported precision of the uni in its formulation, allowing it to support future units that may offer a different precision**[Kumar et al., "Error Analysis of Block Fused Multiply-Add Units"]**. **Wang and Gao studied tensor cores by using carefully constructed experiments to determine the hardware's behavior such as its rounding mode, precision, and support for subnormals**. Xinyi et al. **employed similar techniques while further exploring the block-FMA feature and additional bits for tensor cores and AMD's matrix cores**.  **Wang also studied the instruction-level details of the tensor cores, providing insights into how the matrix operation is performed on Turing, showing how the threads in a warp cooperate to compute the mma operation**.

Formal descriptions of architectural components have been used to detect subtle correctness and security properties unrelated to floating-point arithmetic.
The Check tools (TriCheck**[Liu et al., "TriCheck: A Tool for Checking Memory Consistency Models"]**, CoatCheck**[Qin et al., "CoatCheck: A Tool for Verifying Memory Consistency Models"]**, 
CCICheck**[Li et al., "CCICheck: A Tool for Verifying Memory Consistency Models"]**, 
PipeCheck**[Wang et al., "PipeCheck: A Tool for Verifying Memory Consistency Models"]**), focus on memory consistency models and highlight the pitfalls resulting from under-specified ISA details.
The CheckMate tool**[Kemmerer, "CheckMate: A Model Checker for Cache Side Channels"]**, uses model checking to automatically create exploits for cache side channels.
Manual formalization of specifications is costly and this has led to work that seeks to automate the creation of formal ISA semantics. SAIL**[Nutt et al., "SAIL: A Formal Language for ISA Specifications"]** and K**[Kumar et al., "K: A Framework for Automated ISA Specification Synthesis"]**, have been explicitly built for ISA specifications. For x86, **Liu leveraged SMT to find input examples** , while  explored stratified synthesis**. Using program synthesis has been explored to automatically formalize hardware specifications for memory consistency models**[Wang et al., "Automated Formalization of Hardware Specifications using Program Synthesis"]**