Using SMT, we formalized the properties of tensor cores and modeled their behavior across three generations.
We showed how the in-progress specification and an automated theorem prover could be used together to resolve contradictory observations obtained using solely test-based methods.
While most of our findings align with those of \citet{fasi2021numerical}, our model provided evidence that the rounding mode used for accumulation was simply truncation and that there 3 extra bits used for carry out for Volta and Turing's 5-term accumulator.
Once the model was built, we were able to use it and an automated theorem prover to investigate two algorithms that utilize tensor cores and examine claims about their relative accuracies, thus demonstrating the usefulness of our model to algorithm designers.

The framework we established is fully parametric and future work can reuse it to study the properties of tensor cores as they evolve with new generations. Preliminary experiments on Hopper GPUs (for which we lacked sufficient access to thoroughly study), for instance, indicate that even more bits may be preserved during significand alignment. Our model can also be adjusted to study different floating-point formats such as NVIDIA's 8-bit exponent, 10-bit mantissa TF32 format, or the two FP8 formats supported on Hopper. 

Given that future HPC hardware will likely be supported by non-standard hardware developed primarily for AI (including especially Tensor Cores)~\cite{reed2023hpc}, formalizations such as ours can play a central role in supporting reliable scientific computing in the future.
We plan to develop formal support to analyze such algorithms using techniques presented in this paper.
