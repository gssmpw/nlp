\section{Related Work}
\label{sec:relwork}
We survey closely related work on floating-point formalization and testing-based specification discovery, followed by some non-floating-point formalization efforts.

An SMT theory for floating point reasoning was proposed by \citet{rummer2010smt}, which also included formalizations for rounding modes.  However, SMT-based floating point reasoning has historically been found to have poor scalability~\cite{darulova2014sound,schkufza2014stochastic}, but has been successfully used for error analysis~\cite{solovyev2018rigorous}. \citet{leeser2014make} demonstrated success in using SMT for floating-point reasoning, albeit using the theory of Reals.  \citet{martin2015automatable} redefined the floating point theory, substantially improving SMT's capabilities.  \citet{darulova2018daisy} used SMT to statically analyze floating point programs, for instance to compare roundoff errors between fixed-point and floating-point arithmetic. Floating point capabilities have similarly been implemented in other theorem provers, such as Coq~\cite{2011flcoq} which has also been used for error analysis~\cite{appel2024vcfloat2}. Each of these formalizations follows the IEEE standard~\cite{ieee-754-2008} and hence do not contain support for the non-standard accumulator which our work provides.
Titolo et al.~\cite{laura-titolo-munoz-vmcai} present an abstract interpretation framework for floating-point program roundoff error analysis.

Using tests to identify the implementation peculiarities of floating point units dates as far back as~\citet{paranoia}.  
In the case of GPU tensor cores, there has been considerable interest in understanding their functional as well as performance characteristics. \citet{sun2022dissecting} studied the tensor core implementations across various NVIDIA architectures.  While they primarily focused on the throughput and latency, they briefly investigated the numerical behavior of tensor cores by studying the relative error for different floating point formats. \citet{blanchard2020mixed} devised a framework to perform an error analysis of block fused multiply-add units.  Their method incorporates the supported precision of the uni in its formulation, allowing it to support future units that may offer a different precision. \citet{Hickmann2019ExperimentalAO} and \citet{fasi2021numerical} studied tensor cores by using carefully constructed experiments to determine the hardware's behavior such as its rounding mode, precision, and support for subnormals. Xinyi et al.~\cite{xinyili24} employed similar techniques while further exploring the block-FMA feature and additional bits for tensor cores and AMD's matrix cores.  \citet{yan2020demystifying} also studied the instruction-level details of the tensor cores, providing insights into how the matrix operation is performed on Turing, showing how the threads in a warp cooperate to compute the mma operation.

Formal descriptions of architectural components have been used to detect subtle correctness and security properties unrelated to floating-point arithmetic.
The Check tools (TriCheck~\cite{trippel2017tricheck}, CoatCheck~\cite{lustig_coatcheck_2016}, 
CCICheck~\cite{manerkar_ccicheck_2015}, 
PipeCheck~\cite{lustig_pipecheck_2014}), focus on memory consistency models and highlight the pitfalls resulting from under-specified ISA details.
The CheckMate tool~\cite{trippel_checkmate_2018} uses model checking to automatically create exploits for cache side channels.
Manual formalization of specifications is costly and this has led to work that seeks to automate the creation of formal ISA semantics. SAIL~\cite{armstrong_isa_2019} and K~\cite{dasgupta_complete_2019} have been explicitly built for ISA specifications. For x86, \citet{godefroid-taly} leveraged SMT to find input examples, while ~\citet{heule_stratified_2016} explored stratified synthesis. Using program synthesis has been explored to automatically formalize hardware specifications for memory consistency models~\cite{hsiao2021synthesizing,norman2023pipesynth}.