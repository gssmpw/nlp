As applications strive for greater performance in today's post Moore's Law era, hardware designers have turned to specialized hardware units and non-standard ISA extensions to satisfy this need. In 2016, Google announced its Tensor Processing Unit, a hardware unit specializing in matrix multiplication \cite{google-tpu}. The next year, NVIDIA announced its new Volta architecture would feature Tensor Cores \cite{volta-whitepaper} which have since evolved with each new architecture.
Today, all major CPU and GPU vendors incorporate ISA extensions for matrix multiplication that operate on reduced-precision floating point formats.
Notably, the functionality of these extensions is not standardized yet, and is poorly documented as well.
While non-standard designs might be successfully employed within closed-source vendor libraries and for low-precision AI applications, they can prove to be difficult and error-prone for others who seek to build innovative linear algebra methods that are precision-sensitive~\cite{haidar2020mixed}.
Tensor cores have demonstrated numerical inconsistency across architectures that has an impact on the portability of algorithms. For example, a tensor core implementation of Fast-Fourier transform~\cite{tcfft2024} saw its mean relative error drop by as much as 34\\footnote{An accuracy test provided that accompanies the code reports an error of $1.5e^{-2}$ on Volta and $9.92e^{-3}$ on Ampere} when moving from Volta's tensor cores to Ampere's. This shifting behavior, coupled with the lack of a specification, presents a challenge for safety-critical applications that are sensitive to an implementation's numerical behavior. Furthermore, without a behavioral specification, efforts such as \citet{Goodloe_Mu√±oz_Kirchner_Correnson_2013} that verify numerical programs cannot be utilized.
Thankfully, work done by researchers to understand tensor core functionality~\cite{fasi2021numerical,Hickmann2019ExperimentalAO,xinyili24} has led to novel algorithms that can use these cores to speed up even single-precision computations for HPC~\cite{markidis,ootomo2022recovering}. As new cores are released, though, these same efforts must be repeated.

In this paper, we provide a formal description for the tensor cores across three generations of graphics cards. These formal descriptions can not only provide accurate and reliable component-level specifications enabling automated reasoning about their functionality but also facilitate the the creation of novel, hitherto unimagined, uses, while also improving debuggability, correctness checking, and security analyses.

Our models of tensor cores support two key properties: i)~they are executable, ii)~they can be used in automated reasoning. Our models are also parametric, enabling them to be quickly adapted for new architectures.

Our novel contributions include:

\begin{itemize}
\item A formalization of the numerical properties of mixed-precision block FMA, collected from prior literature, that can be used to identify the properties of different matrix multiplication units
\item A formal, executable model of the matrix multiplication units across three generations of GPUs - Volta, Turing, and Ampere
\item A revision of a mischaracterization in prior work that had concluded the rounding mode of tensor cores is round-to-zero. The actual rounding behavior, truncation, is subtly different. 
\item An analysis of two error-correcting matrix-multiplication algorithms that shows, due to the properties of the tensor cores, the method which trades speed for accuracy can actually produce less accurate results than its faster counterpart.
\end{itemize}

The functional and performance aspects of tensor core behavior have received significant scrutiny through testing~\cite{sun2022dissecting,fasi2021numerical,yan2020demystifying}.
Nevertheless, our SMT formalization unearths subtle discrepancies between test-based reverse engineered descriptions and the actual hardware.

In addition to enabling program analyses, formal models enable the construction of hardware simulators. These simulators in turn allow developers to evaluate the numerical behavior of their algorithms on multiple different architectures, all from the same machine and without requiring access to many different and potentially expensive devices. 

The rest of this paper is structured as follows. 
Section~\ref{sec:relwork}
provides a list of closely related work.
Section~\ref{sec:ptx-sass} provides
background on PTX and SASS, two instruction sets pertinent to
NVIDIA GPUs and necessary to understand Tensor Cores. It also details the HMMA instruction that carries out the matrix
operation $D=A\times B+C$.
Then, in section \ref{sec:fasi-case-study}, we formalize the numerical properties of tensor cores, compare our findings with previous works, and describe our resulting formal model. In section \ref{sec:ootomo-case-study}, we study two methods used to perform single-precision multiplications using the half-precision tensor cores discussed in \citet{ootomo2022recovering}.  We then analyze these algorithms, using SMT to try to prove that the error of Ootomo and Yokota's method is always better than \citet{markidis}.



 
  
