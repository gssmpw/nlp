\newpage
\onecolumn
\appendix

\newtcolorbox{cvbox}[1][]{
    enhanced,
%   blanker, % <- removed as it suppresses box color and frame
    %leftupper=4cm,
    after skip=8mm,%   enlarge distance to the next box
    title=#1,
    breakable = true,
    fonttitle=\sffamily\bfseries,
    coltitle=black,
    colbacktitle=gray!10,   % <- defines background color in title
    titlerule= 0pt,         % <- sets rule underneath title 
    %fontupper=\sffamily,%
    %#1
    overlay={%
        \ifcase\tcbsegmentstate
        % 0 = Box contains only an upper part
        \or%
        % 1 = Box contains an upper and a lower part
        %\path[draw=red] (segmentation.west)--(frame.south east);
        \else%
        % 2 = Box contains only a lower part
        %\path[draw=red] (frame.north west)--(frame.south east);
        \fi%
    }
    colback = gray,         % <- defines background color in box
    colframe = black!75     % <- defines color of frame
    }


\section{Data Construction}

\subsection{Dataset Summary}
\label{sec:appendix_data}

We prepare a seed dataset $\mathcal{D}$ containing both safety and helpfulness data. It consists of 50k pairwise samples from three sources. For helpfulness data, we draw 25k samples from UltraFeedback~\cite{cui2024ultrafeedback}. Each sample originally has 5 potential responses with ratings and we take the one with the highest rating as ``chosen'' and the one with the lowest as ``rejected''. For safety data, we take 22k samples from PKU-SafeRLHF~\cite{ji2024pku}, which have responses with unsafe labels and are further filtered by GPT-4o to assure the prompts are truly toxic and harmful. We follow the common practice of proprietary LLMs that responses to harmful queries should contain clear refusal in at most one sentence instead of providing additional content and guide besides a brief apology~\cite{guan2024deliberative}. This make current positive annotations in PKU-SafeRLHF, which usually contain much relevant information, not directly usable. Therefore, we use GPT-4o to generate refusal answers for these prompts and substitute the original chosen responses with them. 

Further, to better address the complex scenario of jailbreak attack, we take 3k jailbreak prompts from JailbreakV-28k~\cite{luo2024jailbreakv}. As this dataset was originally proposed for benchmarks, we carefully decontaminate the red-teaming queries from those used for evaluation, e.g., AdvBench~\cite{zou2023universal}, and only sample prompts from the sources of GPT-Generate, Handcraft, and HH-RLHF~\cite{ganguli2022red}. Due to the lack of response annotations, we prompt GPT-4o to generate refusal answers as ``chosen'' ones and prompt the models to be trained for ``rejected'' answers.


When training with baselines on the whole dataset, we take the ``chosen'' responses as labels for SFT and use the pairwise data for preference learning (i.e., DPO, SACPO). As for our method and self-rewarding, we only use the prompts from the dataset. Compared with baseline methods like SFT and DPO, we do not use the jailbreak prompts with GPT-generated refusals to directly train the models.



\subsection{Structured CoT Data Synthesis}
\label{sec:appendix_cot}
To initialize the models with reasoning ability, we take 10k prompts each from PKU-SafeRLHF and UltraFeedback, and synthesize structured CoT answers. Specifically, we used GPT-4o to generate structured reasoning steps for these prompts. The generation prompt instructs GPT-4o to produce step-by-step reasoning following a specific format. After generation, we filter out responses with incorrect formatting and remove any responses that provide unsafe answers to safety-related queries, ensuring the quality and safety of our CoT training data. The final dataset consists of synthesized samples slightly fewer than 20k due to the filtering process. The generation prompt for instructing GPT-4o is as below:

% \vspace{8pt} 
\begin{cvbox}[\vspace{8pt}Instruction to Generate Structured Chain-of-Thought Answer]
You are a helpful assistant specializing in addressing both general queries and sensitive topics, including privacy, security, and ethics. Your primary responsibility is to provide thorough reasoning and structured planning to ensure your responses are both helpful and strictly adhere to safety and ethical guidelines.
\newline \newline
\#\#\# Instructions:
\newline \newline
1. **Problem Analysis**: Begin by analyzing the query to understand its context and devise a clear and structured roadmap for the reasoning process below. This step should outline how the problem will be addressed in subsequent reasoning steps, ensuring logical progression and coherence. During this step, you may identify whether the query poses any risks or ethical concerns. If the query involves sensitive or risky elements, the roadmap should focus on identifying and mitigating these aspects. For harmless queries, emphasize an effective and logical approach to solving the problem.
\newline \newline
2. **Reasoning**: Break down the query into detailed and systematic reasoning steps. Each step should build upon the roadmap provided in the Problem Analysis section. Pay close attention to the progression of the analysis—if potential risks or sensitivities are uncovered during this stage, pivot immediately to a safety-focused reasoning process. This includes reassessing the problem's implications and adjusting your steps to ensure a thorough evaluation of risks. For harmless queries, highlight logical planning and systematic problem-solving to provide the most effective solution.
\newline \newline
3. **Final Answer**: Based on your reasoning, provide a concise and direct conclusion using the format `<|Output|>Final Answer:<|/Output|>`. Reasoning is provided to help you better understand the steps involved, but the ultimate answer to the query must be presented here.
- If the query is unsafe or unethical, explicitly refuse assistance with clear language, e.g., "Sorry, I cannot assist with this." Avoid using repetitive responses by varying your phrasing while maintaining a professional and polite tone. For example, you might say: "I’m sorry, but I cannot help with that request," "Unfortunately, I am unable to fulfill this request as it may violate safety guidelines," or "I must decline, as this request does not align with ethical considerations." Ensure your response remains concise, clear, and respectful without engaging further with unsafe or inappropriate content.
\newline  
- If the query is safe and ethical, offer a well-structured, logical, and solution-oriented response that demonstrates your reasoning and problem-solving capabilities. Strengthen the clarity and completeness of the solution to provide the most effective assistance in harmless scenarios.
\newline \newline
\#\#\# Format Requirements:
\newline \newline
- Each step must use the following tokenized format:
\newline 
1. **Problem Analysis**: Encapsulate the analysis within <|Reasoning\_step|> Title: Problem Analysis:  <|/Reasoning\_step|> tags.
\newline \newline
2. **Reasoning**: Include multiple <|Reasoning\_step|> Title: Title\_name <|/Reasoning\_step|> sections as needed to thoroughly address the query.
\newline \newline
3. **Final Answer**: Provide the conclusion in the format: <|Output|>Final Answer: <|/Output|> .
\newline 
By adhering to these guidelines and referring to the above example, you will provide clear, accurate, and well-structured responses to questions involving sensitive or potentially unsafe topics while excelling in logical planning and reasoning for safe and harmless queries. Provide your reasoning steps directly without additional explanations. Begin your response with the special token `<|Reasoning\_step|>`. Following is the question:

\vspace{1em}
Question: \{prompt\}
\vspace{8pt} 
\end{cvbox}


\section{Self-Improvement with Safety-Informed MCTS}

\subsection{Derivation of Safety-Informed Reward}
\label{sec:appendix_derive}

Here, we present the proof for~\cref{theorem} in~\cref{sec:MCTS}, to derive a proper form for the safety-informed reward function. We first recall the three desired properties with intuitive explanations.
\begin{enumerate}
    \item \textbf{Safety as Priority}: Safe responses always get higher rewards than unsafe ones, regardless of their helpfulness.
    \begin{equation}
        \forall \bfm_1,\bfm_2, S(\bfm_1)>0> S(\bfm_2) \Rightarrow R(\bfm_1)>R(\bfm_2)
    \end{equation}
    \item \textbf{Dual Monotonicity of Helpfulness}: When the response is safe, it gets higher reward for better helpfulness; when it is unsafe, it gets lower reward for providing more helpful instructions towards the harmful intention.
    \begin{equation}
        \forall S>0 , \frac{\partial R}{\partial H} > 0\text{ and } \forall S<0, \frac{\partial R}{\partial H} < 0;
    \end{equation}
    \item \textbf{Degeneration to Single Objective}: If we only consider one dimension, we can set the reward function to have a constant shift from the original reward of that aspect. This will lead to the procedure degenerating to standard MCTS under the corresponding reward, i.e., given a partially constructed search tree, the result of selection is the same when all hyperparameters, e.g., seed, exploration parameter, are fixed.
    \begin{align}
        \exists\;C_1 \in [-1,1],\;s.t.\;\text{let }S\equiv C_1, \forall \bfm_1,\bfm_2, R(\bfm_1)-R(\bfm_2)=H(\bfm_1)-H(\bfm_2);\\
    \exists\;C_2 \in [-1,1],\;s.t.\;\text{let }H\equiv C_2, \forall \bfm_1,\bfm_2, R(\bfm_1)-R(\bfm_2)=S(\bfm_1)-S(\bfm_2).
    \end{align}
    
\end{enumerate}

\begin{theorem}
    Fix constants $C_1, C_2\in [-1,1],\;C_1\ne0$. Suppose $R:[-1,1]\times[-1,1]\rightarrow \mathbb{R}$ is twice-differentiable and satisfies $\frac{\partial R}{\partial H}=F(S)$, for some continuous function $F: [-1,1]\rightarrow \mathbb{R}$. Properties 2 and 3 of Dual Monotonicity of Helpfulness and Degeneration to Single Objective hold, if and only if
    \begin{equation}
    R(H,S)=F(S)\cdot H+S - C_2 \cdot F(S)+c,       
    \end{equation} with $F(0)=0, F(C_1)=1, \forall S>0, F(S)>0, \forall S<0, F(S)<0$ and $c$ as a constant.
\end{theorem}

\begin{proof} We show that the form of $R$ is the sufficient and necessary condition of Properties 2 and 3, given the assumptions. For notation simplicity, we use $H_1,H_2,S_1,S_2$ to denote the rewards for arbitrary final answers $f_1, f_2$.

\textbf{Sufficiency}

Assume $R(H,S)=F(S)\cdot H+S-C_2\cdot F(S)+c$ with $F(S)$ satisfying the stated conditions.

For Property 2, we can compute the partial derivative and show that
\begin{equation*}
    \frac{\partial R}{\partial H} = F(S) \begin{cases}
        > 0,\text{ when }S>0,\\
        <0,\text{ when }S<0.
    \end{cases}
\end{equation*}

For Property 3, let $S\equiv C_1$, we get
\begin{equation*}
    R(H_1,C_1)-R(H_2,C_1) = F(C_1) (H_1-H_2) = H_1-H_2.
\end{equation*}
let $H\equiv C_2$, we get
\begin{equation*}
    R(C_2,S_1)-R(C_2,S_2) = C_2(F(S_1)-F(S_2)) + S_1-S_2 -C_2(F(S_1)-F(S_2))= S_1-S_2.
\end{equation*}

\textbf{Necessity}

    Assume $R(H,S)$ satisfies Properties 2 and 3.

    Given the condition that $\frac{\partial R}{\partial H} = F(S)$, the function $R$ should follow the form by integral, 
    \begin{equation}
        R(H,S) = \int_0^H \frac{\partial R}{\partial H}dH+R(0,S) =F(S)\cdot H + G(S),
        \label{eq:reward}
    \end{equation}
    with $G(S)=R(0,S)$ as a continuous and differentiable function of $S$.

    Then, we apply the property of Degeneration to Single Objective, when $S\equiv C_1$,
    \begin{align*}
        R(H_1, C_1)-R(H_2,C_2) = F(C_1)& (H_1-H_2) = H_1-H_2, \forall H_1,H_2\in[-1,1]\\
        &\Rightarrow F(C_1) = 1,
    \end{align*}
    and when $H\equiv C_2$, 
    \begin{align*}
        R(C_2, S_1) - R(C_2, S_2) = C_2(F(S_1)& - F(S_2)) + G(S_1) - G(S_2) = S_1 - S_2, \forall S_1, S_2 \in[-1,1]\\ 
        &\Rightarrow C_2\cdot F'(S) - G'(S) = 1\\ 
        &\Rightarrow G'(S) = 1- C_2\cdot F'(S)\\ 
        &\Rightarrow G(S) = S-C_2\cdot F(S) + c, 
    \end{align*}
    with $c$ as a constant.

    Considering the property of Dual Monotonicity of Helpfulness, it is clear that $\frac{\partial R}{\partial H} = F(S)$ should satisfy
    \begin{equation*}
        F(S) >0, \forall S>0\text{ and }F(S)<0, \forall S<0.
    \end{equation*}
    Given the continuity of $F(S)$, $F(0) = 0$.

    Substituting $G(S)$ to~\cref{eq:reward}, we eventually get the family of $R$, following
    \begin{equation*}
    R(H,S)=F(S)\cdot H+S - C_2 \cdot F(S)+c,       
    \end{equation*} with $F(0)=0, F(C_1)=1, F(S)>0, \forall S>0$, $F(S)<0, \forall S<0$ and $c$ as a constant.
\end{proof}

\begin{corollary}
 Take $F(S)=S, C_1=1, C_2=-1, c=0$, $R(H,S)=2S+S\cdot H$ satisfies that for any $H_1, H_2,S_1,S_2\in[-1,1]$, when $S_1>0>S_2$, the inequality of $R(S_1,H_1)>R(S_2,H_2)$ holds.
\end{corollary}


\subsection{Implementation Details of Self-Improvement}
\label{sec:appendix_self-improvement}

Here, we introduce the implementation details of different components in the iterative self-improvement, including SI-MCTS, Self-Rewarding, and preference data construction.

\subsubsection{Safety-Informed MCTS} 
We design safety-informed reward to introduce dual information of both helpfulness and safety, without impacting the original effect of MCTS on a single dimension. Therefore, we mainly follow the standard MCTS procedure~\cite{vodopivec2017monte} guided by UCB1 algorithm~\cite{chang2005adaptive}. When traversing from the root node (i.e., prompt) to the leaf node, it selects the $i$-th node with the highest value of
\begin{equation}
    v_i + c\sqrt{\frac{\ln N_i}{n_i}},
\label{eq:UCB}
\end{equation}
where $v_i$ is the estimated value of safety-informed rewards, $n_i$ is the visited times of this node, $N_i$ is the visited times of its parent node, and $c$ is the exploration parameter that balances exploration and exploitation. 

The whole procedure of Safety-Informed MCTS follows~\cref{alg:SI MCTS}. In practice, we set exploration parameter $c=1.5$, search budget $n=200$, children number $m=4$. To generate child nodes and rollout to final answers, we set temperature as $1.2$, top-p as $0.9$ and top-k as $50$. We adjust these parameters when higher diversity is needed.



% Build MCT
\begin{algorithm}[ht]
   \caption{Safety-Informed MCTS}
   \label{alg:SI MCTS}
\begin{algorithmic}
   \STATE {\bfseries Input:} prompt set $\mathcal{D}_k$, safety reward function $S$, helpfulness reward function $H$, actor model $\pi_\theta$ that generate one step each time by default, search budget $n$, children number $m$
   \STATE {\bfseries Output:} MCT data $\mathbb{T}$
   \STATE Init $\mathbb{T}$ with empty
   \FOR{each single prompt $\bx$ in $\mathcal{D}_k$}
        \STATE Init search tree $T$ with $root\_node$ of $\bx$
        \FOR{$i$ in range($n$)}
            \STATE Select a leaf node $select\_node$ following the trajectory $(\bx,\bs_i)$ using UCB1 algorithm as~\cref{eq:UCB}
            \STATE $\bz_{i+1}^\ast \leftarrow None$
            \IF{$select\_node$ has been visited before}
                \IF{$select\_node$ is non-terminal}
                    \STATE Sample $m$ children $\{\bz_{i+1}^{(j)}\}_{j=1}^m$ from $\pi_\theta(\cdot|\bx, \bs_i)$ and add the $m$ children to $T$
                    \STATE $\bz_{i+1}^\ast \leftarrow$ random.choice($\{\bz_{i+1}^{(j)}\}$), $select\_node \leftarrow$ the corresponding child
                \ENDIF
            \ENDIF
            \STATE Rollout a full answer $\bfm\sim\pi_\theta(\cdot|\bx,\bs_i, \bz_{i+1}^\ast)$
            \STATE Calculate reward $r \leftarrow S(\bfm) \cdot H(\bfm) +2S(\bfm)$
            \STATE Backpropagate and update node's value and visited times from $select\_node$ to $root\_node$
        \ENDFOR
        \STATE Rollout all nodes that have not been visited before, calculate reward and backpropagate
        \STATE $\mathbb{T}\leftarrow \mathbb{T}\cup\{T\}$
   \ENDFOR
\end{algorithmic}
\end{algorithm}

\subsubsection{Self-Rewarding} 
We take the trained LLMs as judges~\cite{zheng2023judging} to rate their own responses, to remove dependencies on external reward models. We adopt a similar template design following~\cite{yuanself} to prompt the model to give discrete ratings given the query $\bx$ and the final answer $\bfm$ sampled through rollout. For helpfulness, we ask the model to rate the answer from $1$ to $5$ according to the extent of helpfulness and correctness. For safety, we categorize the answer into safe and unsafe ones. All ratings will be normalized into the range of $[-1,1]$. Note that the models also give rewards with in-depth reasoning, which further increase the reliability of ratings.

\subsubsection{Preference Data Construction} 

Given the search trees built via SI-MCTS, we can select stepwise preference data with different steps to optimize the model itself. We employ a threshold sampling strategy to guarantee the quality of training data. For a parent node in the tree, we group two children nodes as a pair of stepwise data if they satisfy that the difference between two values exceeds a threshold $v_0$ and the larger value exceeds another threshold $v_1$. This is to assure that there is a significant gap in the quality of two responses while the ``chosen'' one is good enough. Two thresholds are adjusted to gather a certain amount of training data. 

For the ablation study comparing preference data of full trajectories, we adopt similar strategies but within all full trajectories from the root node. As for the stepwise preference data for training a process reward model, we group nodes at the same depth without requiring them to share a parent node and only emphasize the gap between the ``chosen'' and ``rejected'' responses. To support rewarding at both stepwise and full-trajectory level, we include some full-trajectory preference data into $\mathcal{D}_R$.


\section{Experimental Details}
\label{sec:appendix_exp}

In this work, we conduct all our experiments on clusters with 8 NVIDIA A800 GPUs. 

\subsection{Training Details}
\label{sec:appendix_train}

We have done all the training of LLMs with LLaMA-Factory~\cite{zheng2024llamafactory}, which is a popular toolbox for LLM training. For all methods in training LLMs, optimization with SFT is for $3$ epochs and that with DPO is for $1$ epoch by default. We tune the learning rate from $\{5e-7, 1e-6, 5e-6\}$ and $\beta$ for DPO from $\{0.1,0.2,0.4\}$. Batch size is fixed as $128$ and weight decay is set to $0$. We adopt a cosine scheduler with a warm-up ratio of $0.1$. Following the official implementation, we set $\beta=0.1$ and $\beta/\lambda=0.025$ for SACPO. For Self-Rewarding and our self-improving framework, we take $K=3$ iterations. We take an auxiliary SFT loss with a coefficient of $0.2$ in our self-improvement to preserve the structured CoT style. 

For training process reward model based on the LLaMA architecture, we use OpenRLHF~\cite{hu2024openrlhf} and train based on TA-DPO-3 for 1 epoch, using batch size of $256$ and learning rate of $5e-6$. The training data has 70k pairwise samples from Monte Carlo Search Tree in three iterations and contains both stepwise pairs and full-trajectory pairs. This is to ensure the verifier to have the ability to choose the best answer between partial answers with same thinking steps and between full answers.


For the reproduction of Deliberative Alignment~\cite{guan2024deliberative}, we first develop a comprehensive set of safety policies by analyzing query data from o1 and reviewing OpenAI's content moderation guidelines. Specifically, we prompt o1-preview to generate policies for the seven categories of harmful content identified in Deliberative Alignment --- erotic content, extremism, harassment, illicit behavior, regulated advice, self-harm, and violence ---  and organize them with a unified format by manual check. Each policy includes: (1) a clear Definition of the category, (2) User Requests Categorization (defining and providing examples of both allowed and disallowed requests), (3) Response Style Guidelines, and (4) Edge Cases and Exceptions. Additionally, to account for potential gaps in coverage, we introduce a general safety policy, resulting in a total of eight distinct policy categories, which are submitted as supplementary materials. To ensure fairness and consistency, we use GPT-4o to classify prompts from the PKU-SafeRLHF and JailbreakV-28k datasets based on these eight policy definitions. Notably, we focus on the same 23k safety-related prompts used in our own methodology to maintain comparability.

We fine-tune two open-source o1-like LLMs with the same LLaMA-8B architecture, OpenO1-LLaMA-8B-v0.1 and DeepSeek-r1-Distilled-LLaMA-8b, to compare with our results on LLaMA-8B-3.1-Instruct. We follow the practice in~\cite{guan2024deliberative}, generating reasoning answers based on the harmful prompts together with the safety guidelines, which are gathered as a SFT dataset. These models are trained on the query-response pairs with a learning rate $5e-6$ and a batch size of $128$ for $3$ epochs. 


\subsection{Evaluation Details}
\label{sec:appendix_eval}

For evaluation, we take greedy decoding for generation to guarantee the reproducibility by default. As for test-time scaling, we set temperature to 0.6, top-p to 0.9 and top-k to 50 for the diversity across different responses. Below, we introduce the benchmarks and corresponding metrics in details.

For StrongReject~\cite{souly2024strongreject}, we take the official evaluation protocol, which uses GPT-4o to evaluate the responses and gives a rubric-based score reflecting the willingness and capabilities in responding the harmful queries. We follow~\cite{jaech2024openai} and take the goodness score, which is $1-\text{rubric score}$, as the metric. We evaluate models on prompts with no jailbreak in addition to the reported top-2 jailbreak methods PAIR~\cite{chaojailbreaking}, and PAP-Misrepresentation~\cite{zeng2024johnny}. For main results, we only report the average goodness score on the two jailbreak methods, since most methods achieve goodness scores near $1.0$. For XsTest~\cite{rottger2023xstest}, we select the unsafe split to evaluate the resistance to normal harmful queries and follow its official implementation on refusal determination with GPT-4. We report the sum of full refusal rate and partial refusal rate as the metric. For WildChat~\cite{zhaowildchat}, we filter the conversations with ModerationAPI\footnote{https://platform.openai.com/docs/guides/moderation} and eventually get 219 samples with high toxicity in English. For Stereotype, it is a split for evaluating the model's refusal behavior to queries associated with fairness issues in Do-Not-Answer~\cite{wang2023not}. We also use the same method as XsTest for evaluation, also with the same metric, for these two benchmarks. 

To benchmark general performance, we consider several dimensions involving trustworthiness~\cite{wangdecodingtrust,zhangmultitrust} and  helpfulness in popular sense. We adopt SimpleQA~\cite{wei2024measuring} for truthfulness, AdvGLUE~\cite{wang2adversarial} for adversarial robustness, InfoFlow~\cite{mireshghallahcan} for privacy awareness, GSM8k~\cite{hendrycks2measuring}, AlpacaEval~\cite{dubois2024length}, and BIG-bench HHH~\cite{zhou2024beyond} for helpfulness. All benchmarks are evaluated following official implementations. Correlation coefficient is reported for InfoFlow, and winning rate against GPT-4 is reported for AlpacaEval, while accuracies are reported for the rest. 

% \vspace{12pt}
\section{Examples}
\label{sec:appendix_examples}

Here, we present several examples to qualitatively demonstrate the effectiveness of STAIR against jailbreak attacks proposed by PAIR~\cite{chaojailbreaking}. We compare the outputs of our model with those of baseline models trained on the complete dataset using Direct Preference Optimization (DPO), referred to as the \textit{baseline model} in the cases below.

For each case presented below, we display the following:
\begin{itemize}
    \item \texttt{<Original harmful prompt, baseline model's answer>}
    \item \texttt{<Jailbroken prompt based on the original harmful prompt, baseline model's answer>}
    \item \texttt{<Jailbroken prompt based on the original harmful prompt, STAIR's reasoning process and answer>}
\end{itemize}

Please note that in the answers, due to ethical concerns, we have redacted harmful content by replacing it with a "cross mark" (\textbf{x}) to indicate the presence of harmful content. Our model may perform single-step reasoning (as shown in Case 1) or multi-step reasoning (as demonstrated in Cases 2 and 3) depending on the question. Each reasoning step is marked with \texttt{<|Reasoning\_step|>} and \texttt{<|/Reasoning\_step|>}, while the final answer is enclosed within \texttt{<|Output|>} and \texttt{<|/Output|>}.

We observe that although the baseline model can respond to harmful prompts with refusals, it remains vulnerable to jailbreaks that fabricate imagined scenarios to conform to the harmful query. In contrast, the model trained with STAIR-DPO-3 thoroughly examines the potential risks underlying the jailbreak prompts through step-by-step introspective reasoning, ultimately providing appropriate refusals.



\begin{figure*}
    \centering
    \includegraphics[width = \linewidth]{images/appendix/case-1.pdf}
    \caption{\textbf{Case 1}}
    % \label{fig:appendix-case-1}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width = \linewidth]{images/appendix/case-2.pdf}
    \caption{\textbf{Case 2}}
    % \label{fig:appendix-case-1}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width = \linewidth]{images/appendix/case-3.pdf}
    \caption{\textbf{Case 3}}
    % \label{fig:appendix-case-1}
\end{figure*}