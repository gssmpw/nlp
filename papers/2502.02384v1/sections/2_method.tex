\section{Safety Alignment with Introspective Reasoning}


\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{images/draft/pipeline.png}
    \vspace{-3ex}
    \caption{The framework of STAIR consists of 3 stages. First, a model is initially trained on structured CoT data generated by prompting GPT-4o. It is then used to construct Safety-Informed MCTS (SI-MCTS) through self-generation and self-rewarding. The safety-informed reward function in this process incorporates the information of safety with helpfulness into the internal search nodes. From the constructed search trees, a stepwise preference dataset is collected with threshold sampling for optimizing the model via step-level DPO. This self-improvement process can be repeated for $K=3$ iterations. Finally, a process reward model (PRM) can be further trained based on the same search trees and guide the model from the last iteration to generate better and safer responses through test-time search algorithms.}
    \label{fig:framework}
\end{figure*}

In this section, we introduce the details of our framework, STAIR. The initial objective of safety alignment is to guarantee that, for an instruction-tuned language model $\pi_\theta$, which generates a response $\by$ to a query $\bx$ following $\by\sim\pi_\theta(\cdot|\bx)$, it can accurately identify and properly refuse malicious queries, thereby avoiding harmful outputs. We develop safety-aware introspective reasoning to seek better safety alignment in risky scenarios while preserving the general performance. In this study, similar to previous works~\cite{qi2024safety,weiassessing}, we take a dataset $\mathcal{D}$ covering both helpfulness and safety to balance the two objectives. 



Below, we introduce format alignment with structured CoT data in \cref{sec:SFT}. Iterative self-improvement based on Safety-Informed MCTS is explained in~\cref{sec:MCTS}, followed by an extension to test-time scaling in~\cref{sec:TTS}.


\subsection{Structured CoT Format Alignment}
\label{sec:SFT}



To make a model analyze risks with System 2 thinking instead of directly saying ``sorry'', we first equip it with the reasoning ability. Although LLMs can perform CoT reasoning by prompting~\cite{wei2022chain}, their safety awareness does not improve to the same extent as their general performance, as presented in~\cref{tab:benchmarks}, which motivates us to enhance safety-aware reasoning through fine-tuning. 


In this stage, we only take a small split of $\mathcal{D}$ to align the response format of reasoning as a phase of warm-up. We adopt a structured CoT format as illustrated in~\cref{fig:framework}, which not only enhances the interpretability of the reasoning process but also provides clear markers for step division in the subsequent procedures. Specifically, we require the model to output each step with a title summarizing the step, followed by the detailed thinking. Each step is formatted as a block enclosed by the special tokens \emph{<|Reasoning\_step|>} and \emph{<|/Reasoning\_step|>}. Upon completing the steps of reasoning, the model provides its final answer in the last block marked by \emph{<|Output|>} and \emph{<|/Output|>}, which is then used to evaluate the answer's correctness and safety. For a malicious prompt with risks identified through reasoning steps, a clear refusal should be provided in the final answer.



We prompt GPT-4o to rewrite a response to a query $\bx$ with an $n$-step reasoning path $\by_\text{CoT}=(\bz_1, \bz_2,...,\bz_n, \bfm)$ composed of reasoning steps $\bz_i$ and a final answer given by $\bfm$ which we also denote as $\bz_{n+1}$ for simplicity. We thereby construct a dataset $\mathcal{D}_{\text{CoT}}=\{(\bx,\by_\text{CoT})\}$ following the formatting requirements. The detailed prompt for response generation is provided in~\cref{sec:appendix_cot}.  
We use Supervised Fine-Tuning (SFT) on the data to align the response style.


\subsection{Self-Improvement with Safety-Informed MCTS}
\label{sec:MCTS}



In this stage, we aim to enhance the model's safety-aware reasoning by fully leveraging its own potential in a self-improvement manner~\cite{panglanguage}, utilizing CoT reasoning data generated by the model instead of external annotations. In the field of LLM reasoning~\cite{chen2024alphamath,chen2024step}, MCTS~\cite{vodopivec2017monte} has been a common practice to enhance reasoning by exploring more potential responses. It follows 4 stages -- selection, expansion, rollout, and backpropagation -- and estimates the values of internal nodes according to the rewards given at the end, which typically reflect correctness and helpfulness. However, this cannot be directly applied to safety alignment, as it involves multiple objectives regarding both helpfulness and harmlessness.


\textbf{Safety-Informed MCTS.} To this end, we introduce Safety-Informed MCTS (SI-MCTS), which adapts the traditional MCTS workflow by incorporating safety considerations into the rationale searching process. Given a model $\pi_\theta^0$ trained on structured reasoning data, we can output the reasoning steps one by one, taking each as a search node. For a partial solution $(\bz_1,...,\bz_{i})$ to a query $\bx$ from $\mathcal{D}$, it represents a traversal from the root node and can be expanded by sampling $m$ child nodes $\{\bz_{i+1}^{(j)}\}_{j=1}^m$. A rollout from a node reaches its end when a final answer $\bfm$ is sampled, and a reward is then assigned to $\bfm$ and backpropagated. Rather than simply evaluating correctness, we design a reward function that benefits the reasoning data generation with additional safety information. The reward design must ensure that safety is guaranteed as a constraint while maintaining the original performance of MCTS when applied to helpful-only data. Formally, let the evaluation of $\bfm$ consist of a rewarding function $H(\bfm)\in[-1,1]$ for helpfulness and a rewarding function $S(\bfm)\in[-1,1]$ for safety. We assume that when the answer $\bfm$ is safe, $S(\bfm)>0$, and otherwise, $S(\bfm)<0$. The safety-informed reward function $R:[-1,1]\times[-1,1]\rightarrow\mathbb{R}$ is parameterized by $H$ and $S$, such that for any final answer $\bfm$, $R(\bfm):= R(H(\bfm),S(\bfm))$. We require $R$ to satisfy three properties as follows:
\begin{enumerate}[nolistsep]
    \item \textbf{Safety as Priority}: Safe responses always get higher rewards than unsafe ones, i.e.,\\ $\forall \bfm_1,\bfm_2, S(\bfm_1)>0> S(\bfm_2) \Rightarrow R(\bfm_1)>R(\bfm_2)$;
    \item \textbf{Dual Monotonicity of Helpfulness}: Whether helpfulness is expected depends on the response safety, i.e.,\\ $\forall S>0 , \frac{\partial R}{\partial H} > 0\text{ and } \forall S<0, \frac{\partial R}{\partial H} < 0$;
    \item \textbf{Degeneration to Single Objective}: When only one aspect is focused, we can set $R$ to have a constant difference from the reward of that aspect, i.e.,\\
    % $\exists\;C_1 \in [-1,1],\;\text{when }S=C_1, \frac{\partial R}{\partial H}\equiv1$;
    % \\
    % $\exists\;C_2 \in [-1,1],\;\text{when }H=C_2, \frac{\partial R}{\partial S}\equiv1$.
    $\exists\;C_1 \in [-1,1],\;s.t.\;\text{let }S\equiv C_1, \forall \bfm_1,\bfm_2, R(\bfm_1)-R(\bfm_2)=H(\bfm_1)-H(\bfm_2)$;\\
    $\exists\;C_2 \in [-1,1],\;s.t.\;\text{let }H\equiv C_2, \forall \bfm_1,\bfm_2, R(\bfm_1)-R(\bfm_2)=S(\bfm_1)-S(\bfm_2)$.
\end{enumerate}
To find a proper function $R$ for rewarding, we first present a theorem below, whose proof is derived in~\cref{sec:appendix_derive}.

\begin{theorem}
\label{theorem}
    Fix constants $C_1, C_2\in [-1,1],\;C_1\ne0$. Suppose $R:[-1,1]\times[-1,1]\rightarrow \mathbb{R}$ is twice-differentiable and satisfies $\frac{\partial R}{\partial H}=F(S)$, for some continuous function $F: [-1,1]\rightarrow \mathbb{R}$. The last two properties hold if and only if
    \begin{equation}
    R(H,S)=F(S)\cdot H+S - C_2 \cdot F(S)+c,       
    \label{eq:r-func}
    \end{equation} with $F(0)=0, F(C_1)=1, \forall S>0, F(S)>0, \forall S<0, F(S)<0$ and $c$ as a constant.
\end{theorem}

We notice that, by taking $C_1=1, C_2=-1, F(S)=S, c=0$ in~\cref{eq:r-func}, $R(H,S)=S\cdot H + 2S$ is the simplest form that also satisfies the first property of ``safety as priority''. 
The integration of harmlessness in reward can propagate safety information back to the internal reasoning nodes, facilitating the selection of safety-aware reasoning data in the MCTS procedure. As illustrated in~\cref{fig:framework}, given a query with harmful intent, a response with detailed instructions may achieve a much higher helpfulness score than a simple refusal, which is not ideal in safety alignment. In contrast, once their safety scores are incorporated, the safety-informed rewards better reflect human values by aligning the preference towards safer outcomes.

\textbf{Self-Rewarding Mechanism.} With the goal of realizing the model's potential, we adopt a self-rewarding mechanism~\cite{yuanself} by leveraging the model's capabilities of instruction following and reasoning, while also avoiding the cost of external evaluators, such as GPT-4. Following the practice in previous works~\cite{yuanself,zhang2024chain}, we prompt the trained model to provide ratings of responses and use them to calculate the rewards.

\textbf{Stepwise Preference Optimization.} As verified previously~\cite{zhang2024chain}, stepwise preference data can provide more concise and dense supervision than data with only full trajectories. Therefore, when the searching budget of MCTS is exhausted, we can construct a stepwise preference dataset $\mathcal{D}_1$ from the search trees by pairing nodes $(\bz_{i+1}^{w}, \bz_{i+1}^{l})$ that share a common previous solution path $\bs_i=(\bz_1,...,\bz_{i})$ according to their values. We then perform step-level Direct Preference Optimization (DPO)~\cite{rafailov2024direct} on it. Threshold sampling is employed to ensure the high quality of preference samples by imposing constraints on the value differences and the absolute values of positive samples. For a pair-wise sample $(\bx,\bs_i,\bz_{i+1}^w,\bz_{i+1}^l)\sim\mathcal{D}_1$ generated by $\pi_\text{ref}$, which is $\pi_\theta^0$ in this case, the training objective becomes 
\begin{equation}\small
-\log\sigma\left(\beta\log\frac{\pi_{\theta}(\bz_{i+1}^w|\bx,\bs_i)}{\pi_{\text{ref}}(\bz_{i+1}^w|\bx,\bs_i)}-\beta\log\frac{\pi_{\theta}(\bz_{i+1}^l|\bx,\bs_i)}{\pi_{\text{ref}}(\bz_{i+1}^l|\bx,\bs_i)}\right).
\end{equation}


\textbf{Iterative Self-Improvement.} Note that in this stage, all computations only involve the trained model with a given subset of prompts from $\mathcal{D}$ and do not require any other external signals. We can repeat the process to further boost the safety alignment based on thoughtful reasoning data with increasing quality throughout iterations. Formally, we iteratively optimize a model $\pi_\theta^k\,(k=1,...,K)$ using step-level DPO on a preference dataset $\mathcal{D}_k$ generated by the model $\pi_\theta^{k-1}$ trained in the last iteration with SI-MCTS. More details of this stage are introduced in~\cref{sec:appendix_self-improvement}.



\subsection{Test-time Scaling}
\label{sec:TTS}


We employ test-time scaling techniques to fully leverage our method's introspective reasoning capabilities during the inference phase. Specifically, test-time scaling~\cite{snell2024scaling,jaech2024openai} involves allocating additional computational resources during inference through advanced search algorithms, thereby enabling models to generate higher-quality responses.
However, a reward model is usually needed to evaluate multiple potential responses. We notice that the constructed search trees of SI-MCTS can offer this additional benefit beyond DPO. The estimated values of internal nodes in the trees naturally capture the relative superiority of different partial reasoning trajectories. We sample pairs of partial solutions with the same depth in the search tree, i.e., $(\bx, \bs_i^w, \bs_i^l)$, to construct a preference dataset $\mathcal{D}_R$ for reward modeling. By replacing the linear head on the model $\pi_\theta^K$ from iterative training, we train a process reward model (PRM) $r_\phi$ to evaluate a partial solution $(\bx,\bs_i)$ on $\mathcal{D}_R$ via Bradley-Terry model~\cite{ouyang2022training}, by optimizing the objective:
\begin{equation}
   -\mathbb{E}_{(\bx,\bs_i^w,\bs_i^l)\sim\mathcal{D}_R} [\log\sigma(r_\phi(\bx,\bs_i^w)-r_\phi(\bx,\bs_i^l))].
\end{equation}
In practice, we supplement $\mathcal{D}_R$ with pairs of full-trajectory solutions $(\bx, \by^w,\by^l)$ to enable comparison between full answers with different steps. With the trained PRM, we adopt Best-of-N (BoN)~\cite{lightmanlet}, which selects the best answer from $N$ full-trajectory outputs, and Beam Search~\cite{xie2024self}, which generates multiple candidates by maintaining the most promising options at each reasoning step, to validate the method's effectiveness of test-time scaling in safety. 




