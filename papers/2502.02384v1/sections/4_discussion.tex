\section{Discussions}

In this section, we carry out some discussions about the relationship of STAIR with the techniques applied in proprietary LLMs. For StrongReject, we report the goodness scores on three types of data, including PAIR, PAP-Misrepresentation, and None for queries without jailbreak.



\begin{table}[ht]
\vspace{-1ex}
    \centering
    \caption{Comparison with open-source reasoning LLMs and those trained with Deliberative Alignment on multiple benchmarks.}
    \scriptsize
    % \renewcommand{\arraystretch} % Increase row height
\resizebox{\linewidth}{!}{%
    \begin{tabular}{l|c@{\;\,}c@{\;\,}cc@{\;\,}c}
    \toprule[1.5pt]
       \multirow{2}{*}{o1-Like Models}  & \multicolumn{3}{c}{StrongReject} & \multirow{2}{*}{XsTest}  & \multirow{2}{*}{GSM8k}  \\ \cmidrule(lr){2-4}
         & None & PAIR & PAP-Mis \\\midrule
      LLaMA-o1  & 0.5771 & 0.4441 & 0.5272 & 27.00\% &  79.38\%  \\
      Skywork-o1  & 0.6865 & 0.4034 & 0.4397 & 27.50\% &  91.28\%  \\
      OpenO1 & 0.6837 & 0.3367 & 0.3522 & 34.00\% & 87.41\% \\
      DeepSeek-r1-Dist. & 0.5551 & 0.2987 & 0.3590 & 26.00\% & 91.28\%\\
      QwQ-32B-Preview & 0.8800 & 0.3195 &  0.5978 & 88.50\% & \bf 95.22\%\\\midrule
      \multicolumn{6}{c}{+ \sc Deliberative Alignment}\\\midrule
      Open-o1 & 0.9030 & 0.3782 & 0.4400 & 79.00\% & 86.58\%\\
      DeepSeek-r1-Dist. & 0.9756 & 0.5759 & 0.5895 & 78.00\% & 91.13\%\\\midrule
      STAIR-DPO-3 & \bf 1.0000 & \bf 0.7919 & \bf 0.9677 & \bf 99.00\% & 87.64\%   \\\bottomrule[1.5pt]
    \end{tabular}}
    \label{tab:reasoning}
    \vspace{-2ex}
\end{table}



\subsection{Reasoning for Alignment}

Alongside the release of o-family models by OpenAI~\cite{jaech2024openai}, they proposed the technique of Deliberative Alignment~\cite{guan2024deliberative}, which benefits safety alignment from the existing powerful reasoning foundation models. Our method, in contrast, does not rely on this prerequisite and can make normal instruction-tuned LLMs better aligned by integrating safety-aware reasoning. 

We reproduce deliberative alignment to our best on open-source o1-like LLMs and compare the results. To guarantee a fair comparison, we select models inheriting LLaMA-8B, including LLaMA-o1~\cite{zhang2024accessing}, Skywork-o1-Open-LLaMA-3.1-8B~\cite{skyworkopeno12024}, OpenO1-LLaMA-8B\footnote{https://huggingface.co/O1-OPEN/OpenO1-LLama-8B-v0.1}, and DeepSeek-r1-Distilled-LLaMA-8B~\cite{deepseekai2025deepseekr1incentivizingreasoningcapability} with an exception of QwQ-32B-Preview~\cite{qwq-32b-preview}. We first test the safety of these models and find that most of them cannot resist even simple harmful queries, as shown by the results of StrongReject-None and XsTest in~\cref{tab:reasoning}. Then, we combine the 25k safety-related prompts in the seed dataset with some safety policies, which are generated by OpenAI o1-preview and manually organized, and ask the model to reason according to the provided terms and decide whether to refuse the queries. After filtering the responses with successful refusals, we use the prompts and responses to train the model using SFT. This procedure is conducted on Open-o1 and DeepSeek-r1-Distilled. We can notice the increasing refusal rates on straightforward questions, but the vulnerability to jailbreak attacks still remains. This might be attributed to the limited reasoning capability, the lack of more complex data, or the absence of further RL training. By comparison, the model trained after three iterations with STAIR has better resilience against jailbreak while preserving comparable performance on GSM8k.







\subsection{Comparison with Commercial LLMs}

Besides the publicly released technique, commercial LLMs, which are more broadly used by society, usually have their own safety guardrails against malicious jailbreak attacks. We select a group of popular commercial LLMs from different institutions and compare their performance on StrongReject with our method.

\cref{tab:proprietary} lists the results on diverse commercial LLMs. We can see that most LLMs can correctly refuse straightforward harmful questions, with goodness scores all over 0.95. However, some of them demonstrate worrying vulnerability to modern jailbreak attacks, while Claude-3.5 from Anthropic has the best defense. o1, reported to be much better than GPT-4o~\cite{jaech2024openai}, is not included because of the frequent warnings of jailbreak attempts during API calls. Through iterative self-improvement of safety-aware reasoning, we consolidate LLaMA to a comparable level to Claude, even surpassing it when we apply test-time scaling.


\begin{table}[t]
    \centering
    \caption{Comparison with Proprietary LLMs on StrongReject}
    \scriptsize
    \newcommand{\degree}{90}
    \resizebox{\linewidth}{!}{%
    \begin{tabular}{l@{\;\,}|@{\;\;}c@{\;\;}c@{\;\;}c@{\;\;}c@{\;\;}c@{\;\;}c@{\;\;}|@{\;\;}c@{\;\;}c}
    \toprule[1.5pt]
         & \rotatebox{\degree}{GPT-4o} & \rotatebox{\degree}{Claude-3} & \rotatebox{\degree}{Claude-3.5} & \rotatebox{\degree}{Qwen-Max} & \rotatebox{\degree}{Gemini-1.5} & \rotatebox{\degree}{DeepSeek-R1}& \rotatebox{\degree}{STAIR-DPO-3} & \rotatebox{\degree}{+Beam Search}\\\midrule
     None    & 0.9796 & 0.9968 & \bf 1.0000 & 0.9844 & 0.9952 & 0.9633 & \bf 1.0000 & \bf 1.0000\\\midrule
     PAIR    & 0.3327 & 0.8710 & \bf 0.9129 &  0.3187 & 0.5791 & 0.2069 & 0.7919 & 0.8994\\
     PAP-Mis & 0.4217 & 0.9601 & 0.9589 & 0.4269 & 0.7504 & 0.4034 & 0.9677  &\bf 0.9788 \\\midrule
     Average &  0.3772 & 0.9156 & 0.9359 & 0.3728 & 0.6648 & 0.3052 & 0.8798  & \bf 0.9391 \\
     \bottomrule[1.5pt]
    \end{tabular}}
    \label{tab:proprietary}
    \vspace{-3ex}
\end{table}

