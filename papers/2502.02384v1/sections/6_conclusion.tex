\section{Conclusion}

In this paper, we introduce System 2 thinking into LLM safety alignment, thereby enabling models to better distinguish potential safety risks in complex scenarios, such as jailbreak, with in-depth analysis while maintaining their general performance. Concretely, we present STAIR, a framework for better safety alignment with introspective reasoning. After an initial warm-up with structured CoT data, we employ iterative self-improvement on stepwise data generated with Safety-Informed MCTS, which provides dual signals of safety and helpfulness with a safety-informed reward evaluated by the model itself. Additionally, we train a process reward model with data from the same search trees and validate the effect of test-time scaling on safety alignment. Benchmarking STAIR on harmlessness and general capabilities supports the effectiveness of integrating safety alignment with safety-aware reasoning.

%\newpage


\section*{Impact Statement}

While the motivation and data in our work involve some ethically sensitive issues like jailbreak attacks, whose potential societal consequences have been frequently discussed in the field of LLM, our primary objective is to advance the safety alignment of LLMs, mitigating the societal and ethical risks instead of amplifying them. 