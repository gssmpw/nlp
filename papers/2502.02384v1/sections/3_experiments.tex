\section{Experimental Results}
We demonstrate the effectiveness of STAIR through extensive experiments on multiple benchmarks that reflect both the safety guardrails and general capabilities of LLMs. 

\subsection{Experimental Settings}

We hereby introduce the key experimental settings, with more details explained in~\cref{sec:appendix_data} and~\ref{sec:appendix_exp}.


\textbf{Models and Datasets.} We take two base LLMs for safety alignment, LLaMA-3.1-8B-Instruct~\cite{dubey2024llama} and Qwen-2-7B-Instruct~\cite{qwen2}. For test-time scaling and ablation studies, only LLaMA is utilized. All experiments use a seed dataset $\mathcal{D}$ comprising 50k samples from three sources. For safety-focused data, we use a modified version of 22k preference samples from PKU-SafeRLHF~\cite{ji2024pku} along with 3k jailbreak data from JailbreakV-28k~\cite{luo2024jailbreakv}. Additionally, 25k pairwise data are drawn from UltraFeedback~\cite{cui2024ultrafeedback} to maintain helpfulness, as done in prior works~\cite{qi2024safety,wu2024thinking}. Note that responses in $\mathcal{D}$ are in normal conversational style rather than reasoning-oriented. While we use the whole dataset with labels for training baselines, we only take 10k samples each from PKU-SafeRLHF and UltraFeedback to construct structured CoT data $\mathcal{D}_{\text{CoT}}$. During each self-improvement iteration, 5k safety and 5k helpfulness samples are utilized. Jailbreak prompts are used only in the final two iterations, with 1k and 2k samples, respectively.

\textbf{Baselines.} We first evaluate the performance of CoT prompting~\cite{wei2022chain} to assess the contribution of available reasoning capability to safety consolidation. We then include SFT and DPO~\cite{rafailov2024direct} on standard datasets as representative alignment techniques, both of which are employed in our framework. Besides, SACPO~\cite{wachi2024stepwise}, designed to mitigate the safety-performance trade-off with two-step DPO, and Self-Rewarding~\cite{yuanself}, which leverages self-generated and self-rewarded data in iterative DPO, are also used as baselines for comparison.


\textbf{Evaluation.} We use 10 popular benchmarks to evaluate harmlessness and general performance of the trained models. For harmlessness, models are required to provide clear refusals to harmful queries, following~\cite{guan2024deliberative}. We test the models on StrongReject~\cite{souly2024strongreject}, XsTest~\cite{rottger2023xstest}, highly toxic prompts from WildChat~\cite{zhaowildchat}, and the stereotype-related split from Do-Not-Answer~\cite{wang2023not}. We report the average goodness score on the top-2 jailbreak methods of PAIR~\cite{chaojailbreaking} and PAP~\cite{zeng2024johnny} for StrongReject, and refusal rates for the rest. For general performance, we use benchmarks reflecting diverse aspects of trustworthiness in addition to the popular ones for helpfulness like GSM8k~\cite{hendrycks2measuring}, AlpacaEval2.0~\cite{dubois2024length} and BIG-bench HHH~\cite{zhou2024beyond}. We take SimpleQA~\cite{wei2024measuring} for truthfulness, InfoFlow~\cite{mireshghallahcan} for privacy awareness, and AdvGLUE~\cite{wang2adversarial} for adversarial robustness. Official metrics are reported for all.

% We leave other details including hyperparameters and evaluation strategies in~\cref{sec:appendix_exp}.


\begin{table*}[ht]
    \centering
    \caption{Performance on diverse benchmarks reflecting both harmlessness and general performance. CoT Style represents whether the method adopt Chain-of-Thought reasoning, while Self Gen. denotes whether the method use self-generated data for training. For all reported metrics, the best results are marked in \textbf{bold} and the second best results are marked by \underline{underline}.}
    \renewcommand{\arraystretch}{1.1} % Increase row height
    
\resizebox{\textwidth}{!}{%
    \begin{tabular}{l@{\;\,}|@{\;\,}c@{\;\,}|@{\;\,}c@{\;\,}|c@{\;\,}c@{\;\,}c@{\;\,}c|c@{\;\,}c@{\;\,}c@{\;\,}c@{\;\,}c@{\;\,}c}
        \toprule[1.5pt]
       & \multirow{2}{*}{\makecell{CoT\\Style}} & \multirow{2}{*}{\makecell{Self\\Gen.}}  &  \multicolumn{4}{c|}{\textbf{Harmlessness}} & \multicolumn{6}{c}{\textbf{General}}  \\ \cmidrule(lr){4-7}\cmidrule(lr){8-13}
       & & & StrongReject  & XsTest  & WildChat  & Stereotype  &  SimpleQA 	&  InfoFlow  &  AdvGLUE  & GSM8k  & AlpacaEval  & HHH  \\\midrule
        \multicolumn{13}{c}{\sc Llama-3.1-8B-Instruct} \\ \midrule
        Base &  - & - & 0.4054 & 88.00\% & 47.94\% & 87.37\% & 2.52\% & 0.4229 & 58.33\% &85.60\% &  25.55\% & 82.50\%\\ 
        CoT & \cmark & - & 0.3790 & 87.00\% & 50.23\% & 65.26\% & 4.09\% &  0.7041 & 58.40\% & 87.11\% &22.04\% & 81.63\% \\
        SFT & \xmark & \xmark & 0.4698 & 94.50\% & 50.68\% & 94.74\% & 4.72\% &  0.7134 & 57.53\% &72.02\% & 9.21\% & 82.63\% \\
        DPO & \xmark & \xmark & 0.5054 & 86.00\% & 54.79\% & \bf 97.89\% & 4.46\% & 0.7081 & 66.27\% &84.15\% &  15.26\% & 83.84\% \\ 
        SACPO & \xmark & \xmark  & 0.7264 & 88.50\% & 58.45\% & 96.84\% & 0.74\% &  0.0503 & 65.60\% &86.50\% & 20.44\% & 85.21\%\\ 
        Self-Rewarding & \xmark & \cmark & 0.4633 & \bf 99.00\% & 49.77\% & 94.74\% & 2.70\%  & 0.6618 & 59.10\% & \bf 88.10\%& 26.41\% & 82.09\%\\\midrule
        STAIR-SFT & \cmark & \xmark & 0.6536 & 85.50\% & 50.68\% & 94.74\% & \underline{6.31\%} & \underline{0.7876} & \bf 70.57\% & 86.05\%  &  31.21\% & 83.13\%\\
        +DPO-1 & \cmark & \cmark & 0.6955 & 94.00\% & 57.99\% & \bf 97.89\% & 6.08\% & \bf 0.7998 & 65.93\% & 86.81\% & 34.48\% & 84.53\% \\
        +DPO-2 & \cmark & \cmark & \underline{0.7973} & 96.50\% & \underline{68.95\%} & 96.84\% & 6.00\% &  0.7700 & \underline{69.43\%} & 87.26\% &\underline{36.24\%} & \bf 87.09\% \\
        +DPO-3 & \cmark & \cmark & \bf  0.8798 &  \bf 99.00\% & \bf 69.86\% & 96.84\% & \bf 6.38\% &  0.7395 & 69.20\% &\underline{87.64\%} &\bf  38.66\% & \underline{85.66\%} \\ \midrule
        \multicolumn{13}{c}{\sc Qwen-2-7B-Instruct} \\ \midrule
        Base &  - & - & 0.3808 & 72.50\% & 47.49\% & 90.53\% & 3.79\% & 0.7221 & 66.50\%& \underline{87.49\%}  & 20.06\% & 87.87\%\\ 
        CoT & \cmark & -  & 0.3792 & 70.00\% & 42.92\% & 88.42\% & 3.03\%& 0.7628 & 65.60\% & \bf 88.10\%  & \underline{25.97\%} & 88.30\%\\
        SFT & \xmark & \xmark & 0.4952 & 84.00\% & 58.45\% & 91.58\% & 3.47\% & 0.6267 & 66.90\% &82.34\% &  8.94\% & 89.74\% \\
        DPO & \xmark & \xmark & 0.5026 & 69.00\% & 66.21\% & 88.42\% & 2.59\% &  0.6793 & 70.97\% & 81.43\% & 11.48\% & 88.08\% \\
        SACPO & \xmark & \xmark & 0.5577 & 75.00\% & 60.27\% & 95.79\% & 0.62\%  & 0.6213 & 64.10\% & 85.22\% & 17.04\% & 89.60\% \\ 
        Self-Rewarding & \xmark & \cmark & 0.5062 & 96.00\% & 52.51\% &  94.74\% & 3.37\% & 0.7140 & 66.13\% & 87.34\% & 14.69\% & 88.31\% \\\midrule
        STAIR-SFT & \cmark & \xmark & 0.7356 & 83.50\% & 62.56\% & 95.79\% & 3.81\% &  0.8215 & 70.57\% &84.61\% & 20.31\% & \underline{90.38\%} \\
        +DPO-1 & \cmark & \cmark & 0.7606 & 96.50\% & 65.19\% & 95.79\% & \underline{3.88\%} & \underline{0.8235} & \underline{73.10\%} & 84.76\% & 23.29\% & 90.21\% \\
        +DPO-2 & \cmark & \cmark & \underline{0.8137} & \underline{98.50\%} & \underline{67.90\%} & \underline{97.89\%} & 3.79\% & \bf 0.8646 & 72.83\% & 86.05\% & 24.86\% & 90.11\% \\
        +DPO-3 & \cmark & \cmark & \bf 0.8486 & \bf 99.00\% & \bf 80.56\% & \bf 98.95\% & \bf 4.07\% & 0.7644 & \bf 74.13\% & 85.75\% & \bf 26.31\% & \bf 90.71\% \\ \bottomrule[1.5pt]
    \end{tabular}}
    \label{tab:benchmarks}
    \vspace{-2ex}
\end{table*}



\subsection{Main Results}

We present the results on diverse benchmarks evaluating both the harmlessness and the general performance in~\cref{tab:benchmarks}, which shows the superiority of STAIR, attributed to the incorporation of introspective reasoning to safety alignment and the self-improvement on stepwise data generated with SI-MCTS. 
We use STAIR-SFT to represent the model trained on $\mathcal{D}_\text{CoT}$ with SFT and DPO-k to denote the model after the k-th iteration of self-improvement. Some qualitative examples are displayed in~\cref{sec:appendix_examples}.

First of all, though initially aligned with instruction tuning, the base LLMs remain vulnerable to harmful queries, especially jailbreak attacks. This is evidenced by the goodness scores below 0.40 on StrongReject. We then explore CoT prompting to stimulate the existing reasoning capability in LLMs. While it leads to improvements in reasoning-dependent tasks like GSM8k and InfoFlow, it shows no enhancement in safety. When applying SFT or DPO to the whole dataset $\mathcal{D}$, we observe significant safety-performance trade-offs due to the conflicting objectives. For instance, for both LLaMA-3.1 and Qwen-2 trained with SFT and DPO, their winning rates against GPT-4 on AlpacaEval decline sharply compared to base models. By employing safety-constrained optimization, the trade-off issue is mitigated to a large extent by SACPO, with better safety enhancements compared to previous methods. However, the performance on SimpleQA and InfoFlow degrades, reflecting losses in factual knowledge and over-refusals to benign privacy-related queries. For Self-Rewarding, their improvements on XsTest, which contains queries apparently harmful, are considerable due to the original behaviors of direct refusals in base LLMs. Nevertheless, the behaviors of refusals fail to generalize to jailbreak attacks, as they lack sufficient capabilities to analyze the underlying risks. 

In comparison, STAIR demonstrates more balanced and continuous improvements on diverse benchmarks. With CoT format alignment, the models acquire the basic ability of safety-aware reasoning, enhancing their resilience against harmful inputs. Further training with stepwise preference data generated by SI-MCTS leads to consistent safety enhancements while maintaining or even improving general performance. For example, LLaMA-3.1 achieves an increase of over 20\% in refusal rate on WildChat after three iterations of self-improvement, while its winning rate against GPT-4 on AlpacaEval reaches 38.66\%, a significant improvement compared to 25.55\% for the base model. Similar trends are observed on other benchmarks like SimpleQA and GSM8k. Besides, the accuracy on AdvGLUE is substantially higher than other baselines, highlighting the benefit to robustness from step-by-step reasoning. On StrongReject, both LLMs eventually reach goodness scores of 0.8798 and 0.8486 respectively, which firmly confirm the positive impact of integrating reasoning with safety alignment.

\subsection{Test-time Scaling}

Using the trained process reward model, we investigate the impact of test-time scaling. Since both stepwise and full-trajectory data are used for training, we employ Best-of-N (BoN) and Beam Search, with results presented in~\cref{fig:tts-safe} and~\ref{fig:tts-helpful} for StrongReject and AlpacaEval respectively. Extra computational costs are estimated based on the number of generated steps relative to one-time greedy decoding, expressed in logarithmic form. For example, Bo8 and beam search generating 4 successors with a beam width of 2 correspond to $\log_2(N)=3$. The results indicate that test-time scaling consistently improves both safety and helpfulness. Both searching methods bring improvements of 0.06 for goodness on StrongReject and more than 3.0\% for winning rates on Alpaca.
This supports that the effect of test-time scaling can generalize from math and coding~\cite{snell2024scaling,xie2024self} to more general scenarios like safety.


\begin{figure*}[t]
     \centering
     \begin{minipage}{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth, trim={1cm 1cm 1cm 1cm}]{images/draft/strongreject.png}
         \vspace{-4ex}
         \caption{Changes in goodness scores on StrongReject with test-time scaling.}
         \label{fig:tts-safe}
     \end{minipage}
     \hfill
     \begin{minipage}{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth, trim={1cm 1cm 1cm 1cm}]{images/draft/alpaca.png}
         \vspace{-4ex}
         \caption{Changes in winning rates on AlpacaEval when with test-time scaling.}
         \label{fig:tts-helpful}
     \end{minipage}
     \hfill
     \begin{minipage}{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth, trim={1cm 1cm 1cm 1cm}]{images/draft/balance.png}
         \vspace{-4ex}
         \caption{Results on StrongReject and AlpacaEval as the ratio of safety data varies.}
         \label{fig:data}
     \end{minipage}
        % \caption{Three simple graphs}
        % \label{fig:three graphs}
    \vspace{-1ex}
\end{figure*}


\subsection{Detailed Analysis}

We then conduct some ablation studies to confirm the effectiveness of our framework.

\textbf{Balance between Safety and Helpfulness Data.} To evaluate the impact of the ratio between safety and helpfulness data in the training dataset, we conduct a study during the CoT format alignment stage as a representative. We plot the performance in terms of safety and helpfulness to the varying ratios in~\cref{fig:data}. While a trade-off between safety and helpfulness is observed, consistent with prior findings~\cite{bai2022training}, the performance in both dimensions consistently exceeds that of the base model. This highlights the effectiveness of training with structured CoT data.

\textbf{Step-level Optimization.} To verify the effectiveness of stepwise preference data in the stage of self-improvement, we compare the performance of DPO-1, which is trained on stepwise data based on STAIR-SFT using DPO, with models trained on full trajectory data using either SFT or DPO. The full trajectory data is selected from the same search trees of SI-MCTS, with the total number of training samples kept equal to that of DPO-1. Results in~\cref{tab:iterative} support our strategy of step-level optimization, which brings more fine-grained supervision to safety-aware reasoning.

\textbf{Iterative Training.} We adopt iterative optimization for continuous improvement, motivated by the belief that data generated in later iterations is of higher quality. To validate this, we compare the results of DPO-3 with the model trained using data crafted from all prompts in a single iteration and the model trained on data from the first iteration for three times as many epochs. Results in~\cref{tab:iterative} demonstrate superior improvements on different benchmarks, confirming the improving data quality throughout iterations.




\begin{table}[ht]
\vspace{-1ex}
    \centering
    \caption{Ablation studies on iterative training on stepwise data}
    % \renewcommand{\arraystretch}{1.2} % Increase row height
\resizebox{\linewidth}{!}{%
    \begin{tabular}{l@{\;\,}|@{\;\,}c@{\;\,}c@{\;\,}c@{\;\,}c}
    \toprule[1.5pt]
         & StrongReject & XsTest & GSM8k & AlpacaEval  \\ \midrule
      \multicolumn{5}{c}{Stepwise Data}\\\midrule
      STAIR-SFT + Full (SFT) &  0.6222 & 87.00\% & 85.29\% & 28.10\% \\
      STAIR-SFT + Full (DPO) &  0.6663 & 92.50\% & 86.50\% & 32.87\%\\\midrule
      STAIR-SFT + Step (DPO) & \bf 0.6955 & \bf 94.00\% & \bf 86.81\% & \bf 34.48\% \\\midrule
      \multicolumn{5}{c}{Iterative Training}\\\midrule
      1st Split, 3$\times$ Epochs & 0.6745 & 97.50\%  & 85.75\% & 37.28\% \\
      Full Dataset, 1 Iteration   & 0.7342 & 90.00\%  & 86.58\% & 36.96\%\\\midrule
      STAIR-DPO-3 & \bf 0.8798 & \bf 99.00\% &  \bf 87.64\% & \bf 38.66\% \\\bottomrule[1.5pt]
    \end{tabular}}
    \label{tab:iterative}
    \vspace{-2ex}
\end{table}
