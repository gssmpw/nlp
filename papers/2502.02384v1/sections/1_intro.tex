\section{Introduction}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{images/draft/intro.png}
    \vspace{-4ex}
    \caption{Although the existing safety alignment methods enable LLMs to refuse queries with apparent risks directly, they often fail to resist jailbreak attacks that manage to avoid the initial tokens for refusal. The instinctive responses correspond to System 1 thinking. In this paper, we propose to improve safety alignment with introspective reasoning, encouraging LLMs to scrutinize the underlying risks with safety-aware System 2 thinking before making refusals.}
    \label{fig:intro}
    % \vspace{-2ex}
\end{figure}

The versatility of Large Language Models (LLMs) across a wide range of tasks~\cite{achiam2023gpt, bai2023qwen, dubey2024llama}, from fluid conversation~\cite{dubois2024length} to complex reasoning in mathematics~\cite{cobbe2021training,hendrycks2measuring} and code~\cite{chen2021evaluating,nam2024using}, has facilitated their integration into numerous AI-assisted applications. These include high-stakes domains such as medical diagnostics~\cite{ullah2024challenges}, educational tools~\cite{zhang2024simulating}, and legal consulting~\cite{nigam2024rethinking}, where LLMs frequently interact directly with humans. However, the widespread usage has also exposed their potential to generate harmful content~\cite{dong2024attacks}, such as deception, violence, and discrimination, raising serious concerns about their trustworthiness~\cite{liu2023jailbreaking,wangdecodingtrust} as well as an urgent need for techniques to ensure their safe use. 


Safety alignment~\cite{bai2022training,daisafe} has become a critical solution to enhance the safety and harmlessness of LLMs, enabling them to identify harmful queries and mitigate risks with direct refusals. Typical approaches of aligning model behaviors with human values involve Supervised Fine-Tuning (SFT)~\cite{liu2023makes,alpaca}, preference-based optimization like Reinforcement Learning from Human Feedback (RLHF)~\cite{ouyang2022training,bai2022training} and Direct Preference Optimization (DPO)~\cite{rafailov2024direct,liu2024enhancing}. However, when applied to safety, they often encounter a compromise in general performance, due to the conflicts between objectives~\cite{anwar2024foundational,lin2024mitigating}. This challenge spurs the development of more advanced algorithms~\cite{daisafe,wachi2024stepwise,zhou2024beyond}, framing safety alignment as a multi-objective or a constrained optimization problem to balance safety and helpfulness. 


Though these methods enable models to reject malicious requests with noticeable risks, their effectiveness remains limited in more complex scenarios where potential harms are difficult to identify. For instance, aligned LLMs are still vulnerable to jailbreak attacks~\cite{souly2024strongreject}, which employ diverse strategies, including adversarial suffixes~\cite{zou2023universal} and elaborate disguises~\cite{chaojailbreaking,zeng2024johnny}, to conceal the threats and mislead models to overlook them. This arises from the use of direct refusals in safety training, where models are taught to decline harmful prompts by instinct. As depicted in~\cref{fig:intro}, once such shortcut, termed ``shallow alignment''~\cite{qi2024safety}, is bypassed with jailbreak, the model is likely to conform to the request and output harmful content. This renders current approaches with rapid refusals insufficient for safety alignment, resembling System 1 thinking in the dual-process theory~\cite{evans2003two} that is instinctive and unconscious. In contrast, System 2 thinking with more deliberation and logical reasoning can help with careful risk analysis for better resistance and safer responses~\cite{jaech2024openai}.



In this paper, we introduce \textbf{STAIR}, a framework improving \textbf{S}afe\textbf{T}y \textbf{A}lignment with \textbf{I}trospective \textbf{R}easoning, which examines potential risks through chain-of-thought (CoT) analysis and assures harmless outputs with safety-aware System 2 thinking. As displayed in~\cref{fig:framework}, STAIR consists of 3 stages, structured CoT format alignment, self-improvement with Safety-Informed MCTS (SI-MCTS), and test-time scaling.
Concretely, we first prepare the model with structured CoT reasoning through fine-tuning on a small set of safety and helpfulness data. Based on that, we aim to fully exploit the potential of the model to further enhance its safety-aware reasoning and resort to an iterative self-improvement mechanism~\cite{huang2023large,panglanguage}. We generate data for subsequent preference optimization with SI-MCTS. A safety-informed reward, evaluated by the model itself~\cite{yuanself}, is proposed to aggregate more safety-related information to the internal search nodes representing reasoning steps in addition to helpfulness, facilitating the search for better reasoning paths towards safer responses. We perform step-level DPO to strengthen the safety alignment on these stepwise reasoning data. This can bring continuous improvements as we repeat the process for iterations without the need for extra annotations. Furthermore, we train a process reward model (PRM)~\cite{lightmanlet} with preference data from the same search trees. Applying it with test-time search algorithms like Best-of-N or Beam Search~\cite{xie2024self}, we can stimulate more thoughtful reasoning to acquire responses of higher quality.



We conduct extensive experiments to assess the effectiveness of STAIR. In terms of safety, STAIR consistently enhances the resistance to various harmful queries, achieving a goodness score of 0.88 on StrongReject for LLaMA, outperforming the best baseline by 0.15. Moreover, benefiting from the decomposed problem-solving in reasoning, STAIR mitigates safety-performance trade-offs and gets improvements in multiple dimensions, including helpfulness, truthfulness, robustness, and privacy awareness. The winning rates against GPT-4 on AlpacaEval for LLaMA and Qwen increase by 13.11 and 6.25, compared to their base models, while most baselines merely yield improvements. The process reward model, trained on pairwise data from SI-MCTS, further contributes to safety and helpfulness, with performance improving as test-time computations scale. Finally, we demonstrate the advantages of incorporating reasoning into safety alignment by comparing our method with open-source reasoning LLMs and proprietary LLMs, showing that STAIR with test-time scaling achieves a comparable goodness score of 0.94 on StrongReject with Claude-3.5.


