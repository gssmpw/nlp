\section{Related Work}

\textbf{Safety of LLMs.} LLMsâ€™ tendency to generate harmful responses to malicious queries requires safety alignment. Techniques like SFT~\cite{liu2023makes,alpaca}, DPO~\cite{rafailov2024direct,liu2024enhancing}, and RLHF~\cite{ouyang2022training,bai2022training} often result in trade-offs between safety and performance~\cite{anwar2024foundational}, as harmlessness and helpfulness objectives can conflict. This may weaken general capabilities~\cite{lin2024mitigating} and reduce response diversity~\cite{kirkunderstanding}. Some approaches mitigate these trade-offs through multi-objective~\cite{zhou2024beyond,guo2024controllable} or constrained preference optimization~\cite{daisafe,wachi2024stepwise}.  While such methods enable LLMs to refuse overtly risky queries, they remain susceptible to jailbreak attacks~\cite{zou2023universal,liuautodan, souly2024strongreject}, where risks are obscured through diverse strategies. Defensive techniques like representation engineering~\cite{zou2024improving}, machine unlearning~\cite{liu2024rethinking}, and safeguarding~\cite{ji2024aligner,wang2024self} improve robustness to jailbreak attacks but often rely on external designs, limiting their applications. Our work aims to incorporate reasoning into safety alignment via fine-tuning, enabling models to think more about the potential risks. A concurrent work, Deliberative Alignment~\cite{guan2024deliberative}, also highlights the benefit of reasoning for safety, but assumes access to a large reasoning model, while our study does not rely on that, more applicable to models with limited reasoning capabilities.


\textbf{LLM Reasoning and Self-Improvement.} Inspired by the dual-process theory~\cite{evans2003two}, where System 1 is instinctive and System 2 is deliberate, recent LLM advancements have demonstrated success in abstract reasoning tasks like math~\cite{chen2024alphamath,chen2024step} and coding~\cite{liu2024codemind}. The potential of reasoning in LLMs was first explored through prompting-based techniques such as chain-of-thought (CoT)~\cite{wei2022chain} and tree-of-thought (ToT)~\cite{yao2023tree}. Subsequent research has focused on learning to reason~\cite{jaech2024openai}, with a key challenge being the scarcity of high-quality reasoning data. To address this, synthetic data generation methods have emerged, using search algorithms like Monte Carlo Search Tree~\cite{vodopivec2017monte}, with the correctness evaluated by verifiers or golden answers~\cite{luo2024improve,wan2024alphazero,jiao2024learning,zhang2024rest}. Self-rewarding mechanisms~\cite{yuanself} reduce needs of external supervision~\cite{zhang2024chain,chen2024language} and fit within self-improvement frameworks that use self-generated data~\cite{gulcehre2023reinforced,zhang2024chain,lee2024llm2llm}. Process Reward Models (PRMs) further enhance this field by evaluating reasoning trajectories~\cite{zhang2024rest,lightmanlet}, guiding LLMs to produce deliberate, well-reasoned answers during inference. This aligns with the emerging test-time scaling law~\cite{snell2024scaling}. In this work, we pioneer the integration of safety alignment with LLM reasoning, demonstrating the effectiveness of enhanced safety-aware introspective reasoning. 
