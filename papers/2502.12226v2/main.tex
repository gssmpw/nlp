%File: anonymous-submission-latex-2025.tex
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai25}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\usepackage{tcolorbox}
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
% \usepackage{algorithm}
% \usepackage{algorithmic}

%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
% Define colors
\definecolor{headercolor}{gray}{0.85}
\definecolor{boxcolor}{gray}{0.95}

% Setup for fancy headers and footers
% Define custom colors
\definecolor{lightgray}{gray}{0.95}
\definecolor{darkgray}{gray}{0.85}
\definecolor{highlight}{gray}{0.9}
\definecolor{questioncolor}{gray}{0.9} % Light gray for question
\definecolor{answercolor}{gray}{0.95}   % Slightly darker gray for answers

% \definecolor{elicolor}{}{}

\colorlet{Mycolor1}{red!30}
\usepackage{etoolbox}
\AtBeginEnvironment{tcolorbox}{\small}


\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
% \floatstyle{ruled}
% \newfloat{listing}{tb}{lst}{}
% \floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2025.1)
}

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{2} %May be changed to 1 or 2 if section numbers are desired.


% \title{Rating \underline{F}oundation \underline{M}odels  Supporting \underline{T}ime-\underline{S}eries (FMTS) for Robustness through a Causal Lens}
% -- Alternative
\title{\textbf{On Creating a Causally Grounded Usable Rating Method for Assessing the Robustness of Foundation Models  Supporting Time Series}}
% -- Alternative
%\title{Effectiveness of Ratings in Assessing the Robustness of \underline{F}oundation \underline{M}odels  Supporting \underline{T}ime-\underline{S}eries (FMTS)}

\author {
    % Authors
    Kausik Lakkaraju\textsuperscript{\rm 1}, 
    Rachneet Kaur\textsuperscript{\rm 2},  
    Parisa Zehtabi\textsuperscript{\rm 3}, 
    Sunandita Patra\textsuperscript{\rm 2}, \\
    Siva Likitha Valluru\textsuperscript{\rm 1}, 
    Zhen Zeng\textsuperscript{\rm 2}, 
    Biplav Srivastava\textsuperscript{\rm 1}, 
    Marco Valtorta\textsuperscript{\rm 1}
}
\affiliations {
    \textsuperscript{\rm 1}University of South Carolina, USA\\
    \textsuperscript{\rm 2}J.P. Morgan AI Research, USA\\
    \textsuperscript{\rm 3}J.P. Morgan AI Research, UK
}

% \author{
%     %Authors
%     % All authors must be in the same font size and format.
%     Written by AAAI Press Staff\textsuperscript{\rm 1}\thanks{With help from the AAAI Publications Committee.}\\
%     AAAI Style Contributions by Pater Patel Schneider,
%     Sunil Issar,\\
%     J. Scott Penberthy,
%     George Ferguson,
%     Hans Guesgen,
%     Francisco Cruz\equalcontrib,
%     Marc Pujol-Gonzalez\equalcontrib
% }
% \affiliations{
%     %Afiliations
%     \textsuperscript{\rm 1}Association for the Advancement of Artificial Intelligence\\
%     % If you have multiple authors and multiple affiliations
%     % use superscripts in text and roman font to identify them.
%     % For example,

%     % Sunil Issar\textsuperscript{\rm 2}, 
%     % J. Scott Penberthy\textsuperscript{\rm 3}, 
%     % George Ferguson\textsuperscript{\rm 4},
%     % Hans Guesgen\textsuperscript{\rm 5}
%     % Note that the comma should be placed after the superscript

%     1101 Pennsylvania Ave, NW Suite 300\\
%     Washington, DC 20004 USA\\
%     % email address must be in roman text type, not monospace or sans serif
%     proceedings-questions@aaai.org
% %
% % See more examples next
% }


% REMOVE THIS: bibentry
% This is only needed to show inline citations in the guidelines document. You should not need it and can safely delete it.
\usepackage{bibentry}
% END REMOVE bibentry

% Packages we added.
\usepackage{amsmath}
\usepackage{algorithm2e}
\RestyleAlgo{ruled}
% \usepackage{pdfpages}
\usepackage{subcaption}
\usepackage{array}
\usepackage{multirow}
\usepackage{titling}
% \newcommand{\kl}[1]{{\color{red}~{\em Comment by Kausik: #1}}}
% \newcommand{\biplav}[1]{{\color{blue}~{\em Comment by Biplav: #1}}}
% \definecolor{sunanditacolor}{HTML}{FF5478} % Using hexadecimal
% \newcommand{\sunandita}[1]{{\color{sunanditacolor}~{\em Comment by Sunandita: #1}}}
% \newcommand{\parisa}[1]{{\color{magenta}~{\em Comment by Parisa: #1}}}
% \newcommand{\zhen}[1]{{\color{cyan}~{\em Comment by Zhen: #1}}}
% \newcommand{\rachneet}[1]{{\color{orange}~{\em Comment by Rachneet: #1}}}
% \newcommand{\marco}[1]{{\color{green}~{\em Comment by Marco: #1}}}
% \newcommand{\likitha}[1]{{\color{teal}~{\em Comment by Likitha: #1}}}



% \renewcommand{\kl}[1]{}
%  \renewcommand{\biplav}[1]{}
%  \renewcommand{\sunandita}[1]{}
%  \renewcommand{\parisa}[1]{}
%  \renewcommand{\zhen}[1]{}
%  \renewcommand{\rachneet}[1]{}
% \renewcommand{\likitha}[1]{}

\begin{document}

\maketitle

\begin{abstract}
%The emergence of Foundation Models (FMs) has brought advancements to a range of tasks, including complex ones such as time-series forecasting across various critical sectors such as healthcare and finance. However, like most AI models, FMs are susceptible to noise and perturbations in input data, which can lead to inaccuracies. In the finance sector, these can have significant impact, affecting various stakeholders such as investors, analysts and traders. This paper introduces a novel approach to evaluate the robustness of Time-series Foundation Models (TFMs) using causally grounded experimental setup.  We evaluate seven time-series forecasting models, ranging in architecture, size, and functionality - from general-purpose to time-series-specific across experimental settings that include four perturbation scenarios with real-world relevance and data from six leading stocks in three industries over a year. These models support different modalities, handling diverse data types. Through causal analysis, we aim to assess the reliability and accuracy of the TFMs, thereby supporting developers and end-users in making informed decisions. Additionally, we conduct a user study to determine the effectiveness of our rating approach in communicating the behavior of TFMs to the end-users, highlighting how well our approach practically applies to real-world scenarios. 

Foundation Models (FMs) have improved time series forecasting in various sectors, such as finance, but their vulnerability to input disturbances can hinder their adoption by stakeholders, such as, investors and analysts. To address this, we propose a causally grounded rating framework to study the robustness of Foundational Models for Time Series (FMTS) with respect to input perturbations. 
%\zhen{missing the explicit motivation for using financial stock time series} 
We evaluate our approach on the stock price prediction problem, a well studied problem with easily accessible public data, evaluating six state-of-the-art (some multi-modal) FMTS across six prominent stocks spanning three industries. 
The ratings proposed by our framework effectively 
% \zhen{do we have support for this claim of accuracy? because it aligns with user study?} 
assess the robustness of FMTS and also offer actionable insights 
% \zhen{make sure to summarize actionable insights in experiments overall conclusion} 
for model selection and deployment.
Within the scope of our study, we find that (1) multi-modal FMTS exhibit better robustness and accuracy compared to their uni-modal versions and,
%\zhen{is it that the models just perform equally bad across perturbations? I guess what a useful model would be is something that is accurate and robust at the same time, is there any model like that we would like to highlight and conclude here?}, 
(2) FMTS pre-trained on time series forecasting task exhibit better robustness and forecasting accuracy compared to general-purpose FMTS pre-trained across diverse settings. 
%\zhen{for next token prediction? ``diverse settings" might lead to a misunderstanding that the FMTS is pre-trained for diverse time series tasks. Or just say ``general-purpose FMTS".}.
Further, to validate our framework's usability, we conduct a user study showcasing FMTS prediction errors along with our computed ratings. The study confirmed that our ratings reduced the difficulty for users in comparing the robustness of different systems.
\end{abstract}
\input{sections/introduction}
\input{sections/related-work}
\input{sections/problem}
\input{sections/method}
\input{sections/experiments}
\input{sections/discussion}
%\input{sections/conclusion}


\noindent {\textbf {Acknowledgements.}} This paper was prepared for informational purposes in part by the Artificial Intelligence Research group of JPMorgan Chase \& Co and its affiliates (“J.P. Morgan”) and is not a product of the Research Department of J.P. Morgan.  J.P. Morgan makes no representation and warranty whatsoever and disclaims all liability, for the completeness, accuracy or reliability of the information contained herein.  This document is not intended as investment research or investment advice, or a recommendation, offer or solicitation for the purchase or sale of any security, financial instrument, financial product or service, or to be used in any way for evaluating the merits of participating in any transaction, and shall not constitute a solicitation under any jurisdiction or to any person, if such solicitation under such jurisdiction or to such person would be unlawful.
% \noindent {\textbf {Acknowledgements.}} This paper was prepared for informational purposes in part by the Artificial Intelligence Research group of JPMorgan Chase \& Co and its affiliates (“J.P. Morgan”) and is not a product of the Research Department of J.P. Morgan.  J.P. Morgan makes no representation and warranty whatsoever and disclaims all liability, for the completeness, accuracy or reliability of the information contained herein.  This document is not intended as investment research or investment advice, or a recommendation, offer or solicitation for the purchase or sale of any security, financial instrument, financial product or service, or to be used in any way for evaluating the merits of participating in any transaction, and shall not constitute a solicitation under any jurisdiction or to any person, if such solicitation under such jurisdiction or to such person would be unlawful.
\bibliography{references}

% ------
\clearpage
\appendix
This is the supplementary material for the paper titled:
{\em On Creating a Causally Grounded Usable Rating Method for Assessing the
Robustness of Foundation Models Supporting Time Series}, submitted to AAAI 2025.


In this supplementary material, we provide additional details. 
Section~\ref{sec:appendix-algo-details} gives the rating algorithms. Section~\ref{sec:appendix-related-work} provides additional related work on robustness testing of FMs. Section~\ref{sec:appendix-metrics} provides the detailed description of existing evaluation metrics used to rate the FMTS in our experiments.
Section~\ref{sec:appendix-high-res-figs} contains the higher resolution version of the figures presented in the main paper. 
Section~\ref{sec:appendix-experiments} provides additional experimental results. Section~\ref{sec:appendix-user-study} contains additional user study results containing all the hypotheses validated along with statistical test results, and conclusions.
Section~\ref{sec:appendix-study-q} contains the user study form that was circulated to collect responses from the users.
Section~\ref{sec:appendix-source-code} contains source code used to process the datasets downloaded from Yahoo! finance.
Section~\ref{sec:appendix-implementation-details} contains additional system implementation details such as hyperparameters chosen.
Section~\ref{sec:appx-reproduc} contains the reproducibility checklist.

% \biplav{Accurately mention sections with usage of labels.}
% the deta algorithms adapted from \cite{kausik2024rating} to rate FMTS for robustness. We also provide additional experimental results to help better understand our work in detail. 
% The material is organized as follows:
% \tableofcontents




%----
\section{Details of  Rating Algorithms}
\label{sec:appendix-algo-details}

Apart from Algorithm 2, the rest were adapted from \cite{kausik2024rating} to suit the FMTS forecasting setting. Here is how the rating method works.
\begin{enumerate}
    \item Algorithm 1 computes the weighted rejection score (WRS) which was defined in Appendix \ref{sec:appendix-metrics} in the main paper.
    \item Algorithm 2 computes the PIE \% based on Propensity Score Matching (PSM) which was defined in Section \ref{sec:metrics} in the main paper.
    \item Algorithm 3 creates a partial order of systems within each perturbation based on the raw scores computed. It will arrange the systems in ascending order w.r.t the raw score. The final partial order (PO) will be a dictionary of dictionaries. 
    \item Algorithm 4 computes the final ratings for systems within each perturbation based on the PO from previous algorithm. It splits the set of raw score values obtained within each perturbation into `L' parts where `L' is the rating level chosen by the user. Each of the systems is given a rating based on the compartment number in which its raw score belongs. The algorithm will return a dictionary with perturbations as keys and ratings provided to each system within the perturbation as the value.
\end{enumerate}



\SetKwComment{Comment}{/* }{ */}
\begin{algorithm}
\small
	\caption{\emph{WeightedRejectionScore}}
	
	\textbf{Purpose:} is used to calculate the weighted sum of the number of rejections of null-hypothesis for Dataset $d_j$ pertaining to a system $s$, Confidence Intervals (CI) $ci_k$ and Weights $w_k$.
	
	\textbf{Input:}\\
	 $d$, dataset corresponding to a specific perturbation.\\
	 $CI$, confidence intervals (95\%, 70\%, 60\%). \\
      $s$, system for which WRS is being computed.   \\
     $W$, weights corresponding to different CIs (1, 0.8, 0.6).  \\
    \textbf{Output:} \\
     $Z$, Sensitive attribute  \\ 
	$\psi$, weighted rejection score.
	    $ \psi \gets 0$  
		\For {each $ci_i, w_i \in CI, W$} {
                    // $z_m, z_n$ are classes of $Z$

                    
		            \For {each $z_m, z_n \in Z$} { 
		                $t, pval, dof \gets T-Test(z_m, z_n) $\; 
		                $t_{crit} \gets LookUp(ci_i, dof) $\;
		                \eIf{$t_{crit} > t$} {
		                    $\psi \gets \psi + 0 $\;
		                 }
		                    {$\psi \gets \psi + w_i $}
		               
		            }
    }
    \Return $\psi$
\end{algorithm} 


\begin{algorithm}
\small
	\caption{ \emph{ComputePIEScore}}
	
	\textbf{Purpose:}  is used to calculate the Deconfounding Impact Estimation using Propensity Score Matching (PIE).
	
	\textbf{Input:}\\
	$s$, a system belonging to the set of test systems, $S$.\\
	$D$, datasets pertaining to a perturbation (different distributions).\\
        $p$, A perturbation other than $p_0$ \\
        $p_0$, control perturbation (or no perturbation. \\

    \textbf{Output:} \\
	$\psi$, PIE score.
	

    $ \psi \gets 0$
    
    $PIE\_list \gets []$
    \hspace{0.5 in} //  To store the list of PIE \% of all the datasets.

	\For {each $d_j \in D$} {
    	    $ APE\_o \gets  E(R|P=p_m) - E(R|P=p_0) $\; 
    	    $ APE\_m \gets  E(R|do(P=p_m)) - E(R|do(P=p_0)) $\; 
    		$PIE\_list[j] \gets (APE\_m - APE\_o) * 100$\;
        }
        $\psi \gets MAX(PIE\_list)$\;
    \Return $\psi$
\end{algorithm}


\begin{algorithm}
\small
	\caption{ \emph{CreatePartialOrder}}
	
	\textbf{Purpose:} is used to create a partial order based on the computed weighted rejection score / the PIE \%.
		
	\textbf{Input:}\\
	$S$, Set of systems.\\
        $P$, Set of perturbations.\\
	% $D$, datasets pertaining to different dataset groups.\\
	% $CI$, confidence intervals (95\%, 70\%, 60\%).  \\
 %    $W$, weights corresponding to different CIs (1, 0.8, 0.6).  \\
    $F$, Flag that says whether the confounder is present (1) or not (0).  \\
    $D$, $CI$, $W$ (as defined in the previous algorithms). \\
    \textbf{Output:} \\
	$PO$, dictionary with a partial order for each perturbation.

 
	    $ PO \gets \{\}$\; 
            $SD \gets \{\}$;

            
	    \eIf {F == 0}{
		    \For {each $p_i \in P$}{ 
                    \For {each $s_j \in S$} {
		              $\psi \gets WeightedRejectionScore(p_i, s_j, D, CI, W)$\; 
		              $SD[s_j] \gets \psi$\;
 		    }
                $PO[p_i] \gets SORT(SD)$\;
                }
 		    }
 		    {\For {each $p_j \in P$} {
                    \For {each $s_i \in S$}{ 
		        $\psi \gets ComputePIEScore(s_i, p_j, p_0, D, CI, W)$\; 
		          $SD[s_j] \gets \psi$\;
 		    }
                $PO[p_i] \gets SORT(SD)$\;
                }
 		   }
	    \Return $PO$
\end{algorithm}  


\begin{algorithm}
\small
	\caption{\emph{AssignRating}}
	
	\textbf{Purpose:} \emph{AssignRating} assigns a rating to each of the SASs based on the partial order and the number of rating levels, $L$.
		
	\textbf{Input:}\\
	$S$, $D$, $CI$, $W$, $P$ (as defined in the previous algorithms).\\
	$L$, rating levels chosen by the user.\\

    \textbf{Output:} \\
	$R$, dictionary with perturbations as keys and ratings assigned to each system within each perturbation as the values.\\

    $R \gets \{\}$\;
    $PO \gets \text{CreatePartialOrder}(S,D,CI,W,G)$\;

    \For{$p_i \in P$}{
        $\psi \gets [PO[p_i].\text{values()}]$\;

        \eIf{$\text{len}(S) > 1$}{
            $G \gets \text{ArraySplit}(\psi, L)$\;
            \For{$k, i \in PO[p_i], \psi$}{
                \For{$g_j \in G$}{
                    \If{$i \in g_j$}{
                        $SD[k] \gets j$\;
                    }
                }
            }
        }{
            // Case of a single SAS in $S$\\
            \eIf{$\psi == 0$}{
                $SD[k] \gets 1$\;
            }{
                $SD[k] \gets L$\;
            }
        }
        $R[p_i] \gets SD$\;
    }
    \Return{$R$}\;
\end{algorithm}



\clearpage
\section{Additional Related Work}
\label{sec:appendix-related-work}
\subsection{Robustness Testing of Foundation Models}
\cite{zhang2022contrastive} examined group distribution shifts and evaluated FMs on image classification tasks with spurious confounders. In our work, we assess the robustness of FMs within time series forecasting by measuring their performance in the presence of two confounders across various perturbation settings and test dataset distributions.
\cite{zhang2023foundation} used foundation models as a surrogate oracle to measure the robustness of image classification models.  However, their test systems did not include any foundation models. \cite{xiao2024ritfis} introduces a framework, RITFIS, to assess the LLM-based software against natural language input. However, they did not consider any other modalities. \cite{schlarmann2023adversarial} showed that imperceivable attacks on images to change the caption output of multi-modal FMs can lead to broadcasting of fake information to honest users. They only evaluate robustness of OpenFlamingo model under different attacks but does not compare its performance with any other FMs. None of these works assess the effectiveness and usability of their robustness testing methods. We address this gap through a user study. 


% In our experiments, we consider perturbations in the input data  to TS models common inspired by omission errors and adversarial attacks, and dependent on both input modality. 
% \cite{chanda2022omission}
% \clearpage

\section{Evaluation Metrics}
\label{sec:appendix-metrics}
% In this section, we define our evaluation metrics. We evaluate the systems across two dimensions: Forecasting accuracy and Robustness.
In this section, we define our evaluation metrics: forecasting accuracy and robustness.
\subsubsection{Forecasting Accuracy Metrics}
We evaluate the systems' forecasting accuracy using established metrics commonly applied in time-series forecasting tasks \cite{makridakis2022m5}.

\noindent {\bf Symmetric mean absolute percentage error (SMAPE) } is defined as, 
    {\tiny
    \begin{equation} \label{eq:smape}
        SMAPE = \frac{1}{T}\sum_{t=1}^T\frac{|x_t - \hat{x}_t|}{(|x_t| +|\hat{x}_t|)/2}, 
    \end{equation}
    }
where $T= 20$ (i.e., the value of $d$) is the total number of observations in the predicted time series. 
% $x_t$ represents the actual observed values, and $\hat{x}_t$ the predicted values at each time step $t= 1, \ldots, 20$. 
SMAPE scores range from 0 to 2, with lower scores indicating more precise forecasts. 

% \noindent {\bf Mean absolute scaled error (MASE)} quantifies the mean absolute error of the forecasts relative to the mean absolute error of a naive one-step forecast calculated on the training data.
\noindent {\bf Mean absolute scaled error (MASE)} measures the mean absolute error of forecasts relative to that of a naive one-step forecast on the training data.
{\small
\begin{equation}\label{eq:mase}
    MASE=\frac{\frac{1}{T} \sum_{i=t+1}^{t+T}|x_{i} - \hat{x_{i}}|}{\frac{1}{t}\sum_{i=1}^{t}|x_{i} - x_{i-1}|},
\end{equation}
}
where in our case, $t = 80$, and $T = 100$.
Lower MASE values indicate better forecasts. 

\noindent {\bf Sign Accuracy} quantifies the average classification accuracy across all test samples, where a higher accuracy indicates more precise predictions. This metric classifies based on how the predicted forecasts align with the most recent observed values in the input time series.

\noindent {\bf Robustness Metrics}
%\subsubsection{Robustness Metrics}
We adapt WRS metric originally proposed in \cite{kausik2024rating} to answer RQ1. Additionally, we introduce two new metrics: APE and PIE \% (modified versions of ATE \cite{abdia2017propensity} and DIE \% \cite{kausik2024rating}) tailored to answering RQ2 and RQ3.

\noindent{\bf Weighted Rejection Score (WRS):} WRS, introduced in \cite{kausik2024rating}, measures statistical bias. First, Student's t-test \cite{student1908probable} compares max residual distributions $(R^{max}_{t} | Z)$ for different values of the protected attribute $Z$. We measure this between each pair of industries or companies, resulting in $^3C_2 = 3$ comparisons. 
% The computed t-value of each pair is compared with the critical t-value based on the confidence interval (CI) and degrees of freedom (DoF), to either reject or accept the null hypothesis. 
\cite{kausik2024rating} chose different confidence intervals (CI) [95\%, 75\%, 60\%] that have different critical values and if the computed t-value is less than the critical value, the null hypothesis is rejected. WRS is mathematically defined by the following equation:
\vspace{-0.5em}
\begin{equation}
\textbf{Weighted Rejection Score (WRS)} = \sum_{i\in CI} w_i*x_i,
\label{eq:wrs}
\vspace{-0.5em}
\end{equation}
\noindent where, $x_i$ is the variable set based on whether the null hypothesis is accepted (0) or rejected (1). $w_i$ is the weight that is multiplied by $x_i$ based on the CI. For example, if CI is 95\%, $x_1$ is multiplied by 1. The lower the CI, the lower the weight will be. WRS helps us answer RQ1 (see Section~\ref{sec:problem}). 


\clearpage
\section{Figures in a Higher Resolution from Main Paper}
\label{sec:appendix-high-res-figs}
\begin{figure}[!h]
    \centering
    \includegraphics[width=0.4\textwidth]{figs/Causal_General.png}
    \caption{Causal model $\mathcal{M}$ for FMTS. The validity of link `1' depends on the data distribution ($P|Z$), while the validity of the links `2' and `3' are tested in our experiments.}
   \label{fig:causal-model-supp}
   \vspace{-2.2em}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.4\textwidth]{figs/causal_variants.png}
    \caption{Variants of the causal diagram in Figure \ref{fig:causal-model} used to answer different research questions (RQs).}
   \label{fig:cms-supp}
   \vspace{-1em}
\end{figure}


\begin{figure*}
 \centering
\includegraphics[width=\textwidth]{figs/FMTS_Workflow.png}
\caption{\textbf{Data to predictions}. Workflow for performing statistical and causal analysis to compute raw scores and assign final ratings to the test systems}
\label{fig:system-workflow-supp}
 \end{figure*}
 
 \begin{figure*}
 \centering
 \includegraphics[width=\textwidth]{figs/rating-workflow-v2.png}
 \caption{\textbf{Predictions to ratings}. Black arrows denote the unperturbed and red arrows indicate the perturbed paths. Dashed lines shows the multi-modal path. The perturbed parts of the plots are highlighted in red.}
 \label{fig:rating-workflow-supp}
 \end{figure*}

\begin{figure*}[!h]
\centering
\includegraphics[width=\textwidth]{figs/all_metrics_scatter_plot_v2.png}
\caption{Studying each metric with respect to impact of company and industry as confounders for all models and all perturbations. Plotted in double logarithmic scale, lower values indicate better robustness. Ratings generated by our method (with $L=3$) are shown on the top of each plot. The complete final order (with ratings) are shown in Table \ref{tab:ratings} in Appendix \ref{sec:appendix-experiments}.
}
\label{fig:confs-supp}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figs/Radar_all_v1.png}
    \caption{Radar plots showing for all systems (a) mean forecasting accuracy with respect to all metrics, (b) forecasting accuracy under P2 (half-valued pertubation) Appendix \ref{sec:appendix-experiments}: Figs. \ref{fig:radar-rob}, 
 \ref{fig:radar-acc} show all perturbations; (c) mean robustness metrics for FMTS and $S_a$, and (d) robustness under P2. Each axis is normalized and inverted if needed so that outer ring implies better performance.}
    \label{fig:radar-agg-supp}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figs/Radarchart_Modality_v2.png}
    \caption{Effect of the modalities for $S_g$ (left) and $S_p$ (right).}
    \label{fig:radar_modality-supp}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figs/Radarchart_Arch_v2.png}
    \caption{Role of architecture in forecasting accuracy and robustness. Performance is averaged across models within each category. See Table~\ref{tab:fms}.}
    \label{fig:radar-arch-supp}
\end{figure*}

\clearpage
\section{Additional Experimental Results}
\label{sec:appendix-experiments}
In this section, Table \ref{tab:ratings} shows the partial order and final order with respect to all the metrics defined in Section \ref{sec:metrics}. Table \ref{tab:h4} shows the forecasting accuracy values for all the systems. Table \ref{tab:cases-values} shows the research questions, and average values for all metrics across systems and perturbations (average values are referred to in the conclusions made for each RQ in Section \ref{sec:expts}). Figure \ref{fig:bar-acc} and \ref{fig:bar-rob} shows bar plots with all the robustness metric values and forecasting accuracy values. Figure \ref{fig:heatmaps} shows the heatmap for all metrics for all the models. 

Table \ref{tab:ratings} shows the partial order and final order with respect to all the metrics defined in Section \ref{sec:metrics}. Table \ref{tab:h4} shows the forecasting accuracy values for all the systems. Table \ref{tab:cases-values} shows the research questions, and average values for all metrics across systems and perturbations (average values are referred to in the conclusions made for each RQ in Section \ref{sec:expts}). Figure \ref{fig:bar-acc} and \ref{fig:bar-rob} shows bar plots with all the robustness metric values and forecasting accuracy values. Figure \ref{fig:heatmaps} shows the heatmap for all metrics for all the models. 






\begin{table*}[!ht]
\centering
{\tiny
    \begin{tabular}{|p{5em}|p{0.2em}|p{45em}|p{25em}|}
    \hline
          {\bf Forecasting Evaluation Dimensions} &
          {\bf P} &    
          {\bf Partial Order} &
          {\bf Complete Order} 
          \\ \hline 
          \multirow{3}{6em}{WRS$_I$$\downarrow$} &
          P0 & 
          \{$S_g$: 4.6, S$_m$: 4.6, $S_r$: 4.6, $S_c$: 5.9, $S_a$: 5.9, $S_p^{ni}$: 5.9, $S_g^{ni}$: 6.9, $S_p$: 6.9, $S_b$: 6.9\}   &
          \{$S_g$: 1, S$_m$: 1, $S_r$: 1, $S_c$: 2, $S_a$: 2, $S_p^{ni}$: 2, $S_g^{ni}$: 3, $S_p$: 3, $S_b$: 3\}
          \\ \cline{2-4}
          &
          P1 & 
          \{$S_a$: 2.6, $S_m$: 4.6, $S_g$: 4.6, $S_g^{ni}$: 4.6, $S_r$: 4.6, $S_p^{ni}$: 5.9, $S_p$: 6.9, $S_c$: 6.9, $S_b$: 6.9\}  &
          \{$S_a$: 1, $S_m$: 1, $S_g$: 1, $S_g^{ni}$: 1, $S_r$: 1, $S_p^{ni}$: 2, $S_p$: 3, $S_c$: 3, $S_b$: 3\}
          \\ \cline{2-4}
          &
          P2 & 
          \{$S_a$: 4.6, $S_g$: 4.6, $S_g^{ni}$: 4.6, $S_p^{ni}$: 4.6, $S_m$: 4.6,  $S_r$: 4.6, $S_c$: 6.9, $S_p$: 6.9, $S_b$: 6.9\}   &
          \{$S_a$: 1, $S_g$: 1, $S_g^{ni}$: 1, $S_p^{ni}$: 1, $S_m$: 1,  $S_r$: 1, $S_c$: 2, $S_p$: 2, $S_b$: 2\} 
          \\ \cline{2-4}
          &
          P3 & 
          \{$S_g$: 4.6, $S_g^{ni}$: 4.6, S$_m$: 4.6, $S_r$: 4.6, $S_c$: 4.6, $S_a$: 5.9, $S_p^{ni}$: 6.9, $S_p$: 6.9, $S_b$: 6.9\}   &
          \{$S_g$: 1, $S_g^{ni}$: 1, S$_m$: 1, $S_r$: 1, $S_c$: 1, $S_a$: 2, $S_p$: 3, $S_p^{ni}$: 3, $S_b$: 3\} 
          \\ \hline
          \multirow{3}{6em}{WRS$_C$$\downarrow$} &
          P0 & 
          \{$S_a$: 2.6, S$_g$: 4.6, $S_g^{ni}$: 4.6, $S_p^{ni}$: 4.6, $S_c$: 5.6, $S_p$: 6.9, $S_m$: 6.9, $S_r$: 6.9, $S_b$: 6.9\}    &
          \{$S_a$: 1, S$_g$: 1, $S_g^{ni}$: 1, $S_p^{ni}$: 1, $S_c$: 2, $S_p$: 3, $S_m$: 3, $S_r$: 3, $S_b$: 3\}
          \\ \cline{2-4}
          &
          P1 & 
          \{$S_a$: 0.6, $S_c$: 4.6, $S_p$: 5.9, $S_p^{ni}$: 5.9, $S_r$: 5.9, $S_g^{ni}$: 5.9, $S_g$: 6.9, $S_m$: 6.9, $S_b$: 6.9\}    &
          \{$S_a$: 1, $S_c$: 1, $S_p$: 2, $S_p^{ni}$: 2, $S_r$: 2, $S_g^{ni}$: 2, $S_g$: 3, $S_m$: 3, $S_b$: 3\}
          \\ \cline{2-4}
          &
          P2 & 
          \{$S_a$: 2.6, $S_c$: 4.6, $S_r$: 4.6, $S_p^{ni}$: 4.6, $S_p$: 5.2, $S_g$: 5.9, $S_g^{ni}$: 5.9, $S_m$: 6.9, $S_b$: 6.9\}   &
          \{$S_a$: 1, $S_c$: 1, $S_r$: 1, $S_p^{ni}$: 1, $S_p$: 2, $S_g$: 2, $S_g^{ni}$: 2, $S_m$: 3, $S_b$: 3\}
          \\ \cline{2-4}
          &
          P3 & 
          \{$S_g$: 4.6, $S_g^{ni}$: 4.6, $S_p^{ni}$: 4.6, S$_p$: 4.6, $S_c$: 4.6, $S_m$: 6.9, $S_a$: 6.9, $S_r$: 6.9, $S_b$: 6.9\}   &
          \{$S_g$: 1, $S_g^{ni}$: 1, $S_p^{ni}$: 1, S$_p$: 1, $S_c$: 1, $S_m$: 2, $S_a$: 2, $S_r$: 2, $S_b$: 2\}
          \\ \hline
          \multirow{3}{6em}{PIE$_I$ \%$\downarrow$} &
          P1 & 
          \{$S_g^{ni}$: 124.50, $S_g$: 600.31, $S_r$: 1041.01, $S_p^{ni}$: 1196, $S_m$: 1426.81, $S_c$: 1441.59, $S_p$: 1765.84, $S_a$: 2720.26, $S_b$: 3283.88\} &
          \{$S_g^{ni}$: 1, $S_g$: 1, $S_r$: 1, $S_p^{ni}$: 2, $S_m$: 2, $S_c$: 2, $S_p$: 2, $S_a$: 3, $S_b$: 3\}
          \\ \cline{2-4}
          &
          P2 & 
          \{$S_c$: 357.72, $S_g^{ni}$: 527.76, $S_g$: 597.54, $S_a$: 902.54, $S_m$: 1326.20, $S_r$: 1463.71, $S_p^{ni}$: 1653.53, $S_b$: 2174.39, $S_p$: 2295.68\}  &
          \{$S_c$: 1, $S_g^{ni}$: 1, $S_g$: 1, $S_a$: 2, $S_m$: 2, $S_r$: 2, $S_p^{ni}$: 3, $S_b$: 3, $S_p$: 3\}  
          \\\cline{2-4}
          &
          P3 & 
          \{$S_g$: 703.94, $S_g^{ni}$: 884.34, $S_c$: 911.53, $S_p^{ni}$: 972.95, $S_a$: 1195.04, $S_m$: 2998.25, $S_p$: 3208.04, $S_r$: 3560.94, $S_b$: 7489.48\}   &
          \{$S_g$: 1, $S_g^{ni}$: 1, $S_c$: 1, $S_p^{ni}$: 2, $S_a$: 2, $S_m$: 2, $S_p$: 3, $S_r$: 3, $S_b$: 3\}   
          \\\hline
          \multirow{3}{4em}{PIE$_C$ \%$\downarrow$} &
          P1 & 
          \{$S_c$: 515.91, $S_g$: 663.75, $S_g^{ni}$: 696.44, $S_a$: 982.38, $S_m$: 1028.48, $S_p^{ni}$: 1101.24, $S_p$: 1474.76, $S_r$: 4756.40, $S_b$: 6916.11\}  &
          \{$S_c$: 1, $S_g$: 1, $S_g^{ni}$: 1, $S_a$: 2, $S_m$: 2, $S_p^{ni}$: 2, $S_p$: 3, $S_r$: 3, $S_b$: 3\}
          \\ \cline{2-4}
          &
          P2 & 
          \{$S_g^{ni}$: 469.16, $S_c$: 576.18, $S_g$: 651.07, $S_m$: 1150.45, $S_a$: 1275.04, $S_p^{ni}$: 2238.21, $S_p$: 3257.35, $S_r$: 4274.38, $S_b$: 9474.61\}   &
          \{$S_g^{ni}$: 1, $S_c$: 1, $S_g$: 1, $S_m$: 2, $S_a$: 2, $S_p^{ni}$: 2, $S_p$: 3, $S_r$: 3, $S_b$: 3\} 
          \\ \cline{2-4}
          &
          P3 & 
          \{$S_g^{ni}$: 436.33, $S_g$: 513.47, $S_c$: 650.20, $S_m$: 866.61, $S_r$: 1305.78, $S_a$: 1716.68, $S_b$: 1846.56, $S_p^{ni}$: 2773.74, $S_p$: 4064.03\}  &
          \{$S_g^{ni}$: 1, $S_g$: 1, $S_c$: 1, $S_m$: 2, $S_r$: 2, $S_a$: 2, $S_b$: 3, $S_p^{ni}$: 3, $S_p$: 3\} 
          \\ \hline
          \multirow{3}{6em}{APE$_I$$\downarrow$} &
          P1 & 
          \{$S_g^{ni}$: 2.50, $S_g$: 11.75, $S_c$: 14.69, $S_p^{ni}$: 17.41, $S_m$: 19.94, $S_p$: 26.50, $S_r$: 48.80, $S_a$: 61.87, $S_b$: 101.31\}   &
          \{$S_g^{ni}$: 1, $S_g$: 1, $S_c$: 1, $S_p^{ni}$: 2, $S_m$: 2, $S_p$: 2, $S_r$: 3, $S_a$: 3, $S_b$: 3\} 
          \\ \cline{2-4}
          &
          P2 & 
          \{$S_g$: 3.72, $S_c$: 4.79, $S_g^{ni}$: 6.06, $S_a$: 11.32, $S_m$: 13.36, $S_p^{ni}$: 18.94, $S_p$: 26.02, $S_r$: 42.91, $S_b$: 101.20\}  &
          \{$S_g$: 1, $S_c$: 1, $S_g^{ni}$: 1, $S_a$: 2, $S_m$: 2, $S_p^{ni}$: 2, $S_p$: 3, $S_r$: 3, $S_b$: 3\} 
          \\ \cline{2-4}
          &
          P3 & 
          \{$S_a$: 7.87, $S_g$: 8.40, $S_g^{ni}$: 9.09, $S_c$: 9.50, $S_p^{ni}$: 16.73, $S_m$: 31.36, $S_r$: 36.59, $S_p$: 37.39, $S_b$: 99.72\} &
          \{$S_a$: 1, $S_g$: 1, $S_g^{ni}$: 1, $S_c$: 2, $S_p^{ni}$: 2,  $S_m$: 2, $S_r$: 3, $S_p$: 3, $S_b$: 3\}
          \\ \hline
          \multirow{3}{6em}{APE$_C$$\downarrow$} &
          P1 & 
          \{$S_b$: 0, $S_c$: 6.31, $S_g^{ni}$: 9.49, $S_g$: 10.41, $S_m$: 15.33, $S_r$: 15.36, $S_p^{ni}$: 15.57, $S_p$: 23.99, $S_a$: 59.80\}  &
          \{$S_b$: 1, $S_c$: 1, $S_g^{ni}$: 1, $S_g$: 2, $S_m$: 2, $S_r$: 2, $S_p^{ni}$: 3, $S_p$: 3, $S_a$: 3\} 
          \\ \cline{2-4}
          &
          P2 & 
          \{$S_b$: 0, $S_g^{ni}$: 5.31, $S_c$: 6.42, $S_g$: 8.69, $S_p$: 10.81, $S_m$: 13.92, $S_r$: 17.61, $S_a$: 21.39, $S_p^{ni}$: 27.63\}   &
          \{$S_b$: 1, $S_g^{ni}$: 1, $S_c$: 1, $S_g$: 2, $S_p$: 2, $S_m$: 2, $S_r$: 3, $S_a$: 3, $S_p^{ni}$: 3\} 
          \\ \cline{2-4}
          &
          P3 & 
          \{$S_b$: 0, $S_g^{ni}$: 6.48, $S_g$: 7.06, $S_a$: 7.42, $S_c$: 8.80, $S_m$: 10.87, $S_r$: 16.63, $S_p^{ni}$: 35.35, $S_p$: 46.50 \}    &
          \{$S_b$: 1, $S_g^{ni}$: 1, $S_g$: 1, $S_a$: 2, $S_c$: 2,$S_m$: 2, $S_r$: 3, $S_p^{ni}$: 3, $S_p$: 3 \} 
          \\ \hline
          \multirow{3}{6em}{SMAPE$\downarrow$} &
          P0 & 
          \{$S_a$: 0.040, $S_c$: 0.043, $S_g$: 0.049, $S_p^{ni}$: 0.079, $S_p$: 0.095, $S_g^{ni}$: 0.095, $S_m$: 0.097, $S_r$: 0.829, $S_b$: 1.276 \} &
          \{$S_a$: 1, $S_c$: 1, $S_g$: 1, $S_p^{ni}$: 2, $S_p$: 2, $S_g^{ni}$: 2, $S_m$: 2, $S_r$: 3, $S_b$: 3 \} 
          \\ \cline{2-4}
          &
          P1 & 
          \{$S_c$: 0.065, $S_g^{ni}$: 0.067, $S_g$: 0.072, $S_a$: 0.084, $S_m$: 0.100, $S_p$: 0.100, $S_p^{ni}$: 0.100, $S_r$: 0.830, $S_b$: 1.276 \} &
          \{$S_c$: 1, $S_g^{ni}$: 1, $S_g$: 1, $S_a$: 2, $S_m$: 2, $S_p$: 2, $S_p^{ni}$: 2, $S_r$: 3, $S_b$: 3 \}
          \\ \cline{2-4}
          &
          P2 & 
          \{$S_g$: 0.051, $S_c$: 0.053, $S_g^{ni}$: 0.060, $S_a$: 0.069, $S_p^{ni}$: 0.095, $S_m$: 0.098, $S_p$: 0.100, $S_r$: 0.830, $S_b$: 1.276 \}   &
          \{$S_g$: 1, $S_c$: 1, $S_g^{ni}$: 1, $S_a$: 2, $S_p^{ni}$: 2, $S_m$: 2, $S_p$: 3, $S_r$: 3, $S_b$: 3 \} 
          \\ \cline{2-4}
          &
          P3 & 
          \{$S_a$: 0.040, $S_c$: 0.043, $S_g$: 0.049, $S_g^{ni}$: 0.056, $S_p^{ni}$: 0.078, $S_p$: 0.092, $S_m$: 0.097, $S_r$: 0.830, $S_b$: 1.276 \}   &
          \{$S_a$: 1, $S_c$: 1, $S_g$: 1, $S_g^{ni}$: 2, $S_p^{ni}$: 2, $S_p$: 2, $S_m$: 3, $S_r$: 3, $S_b$: 3 \} 
          \\ \hline
          \multirow{4}{6em}{MASE$\downarrow$} &
          P0 & 
          \{$S_a$: 3.79, $S_c$: 4.18, $S_g$: 4.64, $S_p^{ni}$: 7.19, $S_p$: 8.91, $S_m$: 9.03, $S_g^{ni}$: 10.37, $S_r$: 86.45, $S_b$: 947.56 \}    &
          \{$S_a$: 1, $S_c$: 1, $S_g$: 1, $S_p^{ni}$: 2, $S_p$: 2, $S_m$: 2, $S_g^{ni}$: 3, $S_r$: 3, $S_b$: 3 \} 
          \\ \cline{2-4}
          &
          P1 & 
          \{$S_c$: 5.40, $S_g^{ni}$: 5.65, $S_g$: 6.13, $S_p^{ni}$: 8.87, $S_p$: 9.19, $S_m$: 9.32, $S_a$: 18.36, $S_r$: 86.99, $S_b$: 947.56 \}   & 
          \{$S_c$: 1, $S_g^{ni}$: 1, $S_g$: 1, $S_p^{ni}$: 2, $S_p$: 2, $S_m$: 2, $S_a$: 3, $S_r$: 3, $S_b$: 3 \} 
          \\ \cline{2-4}
          &
          P2 & 
          \{$S_g$: 4.74, $S_c$: 4.99, $S_g^{ni}$: 5.59, $S_a$: 8.24, $S_p^{ni}$: 8.49, $S_m$: 9.15, $S_p$: 9.32, $S_r$: 86.87, $S_b$: 947.56 \}    &
          \{$S_g$: 1, $S_c$: 1, $S_g^{ni}$: 1, $S_a$: 2, $S_p^{ni}$: 2, $S_m$: 2, $S_p$: 3, $S_r$: 3, $S_b$: 3 \}
          \\ \cline{2-4}
          &
          P3 & 
          \{$S_a$: 3.79, $S_c$: 4.10, $S_g$: 4.64, $S_g^{ni}$: 5.39, $S_p^{ni}$: 7.11, $S_p$: 8.68, $S_m$: 9.03,  $S_r$: 86.65, $S_b$: 947.56 \} &
          \{$S_a$: 1, $S_c$: 1, $S_g$: 1, $S_g^{ni}$: 2, $S_p^{ni}$: 2, $S_p$: 2, $S_m$: 3,  $S_r$: 3, $S_b$: 3 \} 
          \\ \hline
          \multirow{3}{6em}{Sign Accuracy \%$\uparrow$} &
          P0 & 
          \{$S_m$: 40.70, $S_p$: 45.09, $S_p^{ni}$: 47.67, $S_r$: 49.88, $S_g^{ni}$: 50.41, $S_g$: 52.08, $S_c$: 53.75, $S_a$: 60.08, $S_b$: 62.60 \} &  
          \{$S_m$: 1, $S_p$: 1, $S_p^{ni}$: 1, $S_r$: 2, $S_g^{ni}$: 2, $S_g$: 2, $S_c$: 3, $S_a$: 3, $S_b$: 3 \}
          \\ \cline{2-4}
          &
          P1 & 
          \{$S_m$: 41.19, $S_p$: 44.33, $S_p^{ni}$: 46.77, $S_r$: 49.62, $S_g$: 50.53, $S_c$: 52.09, $S_g^{ni}$: 53.93, $S_a$: 57.08, $S_b$: 62.60 \} &
          \{$S_m$: 1, $S_p$: 1, $S_p^{ni}$: 1, $S_r$: 2, $S_g$: 2, $S_c$: 2,  $S_g^{ni}$: 3, $S_a$: 3, $S_b$: 3 \}
          \\ \cline{2-4}
          &
          P2 & 
          \{$S_m$: 41.05, $S_p$: 44.02, $S_p^{ni}$: 47.67, $S_r$: 49.64, $S_g$: 49.75, $S_c$: 50.79, $S_g^{ni}$: 54.43, $S_a$: 57.13, $S_b$: 62.60 \}    &
          \{$S_m$: 1, $S_p$: 1, $S_p^{ni}$: 1, $S_r$: 2, $S_g$: 2, $S_c$: 2, $S_g^{ni}$: 3, $S_a$: 3, $S_b$: 3 \} 
          \\ \cline{2-4}
          &
          P3 & 
          \{$S_m$: 40.72, $S_p$: 44.26, $S_p^{ni}$: 47.50, $S_r$: 49.71, $S_g$: 51.34, $S_c$: 51.35, $S_g^{ni}$: 52.97, $S_a$: 59.98, $S_b$: 62.60 \}  &
          \{$S_m$: 1, $S_p$: 1, $S_p^{ni}$: 1, $S_r$: 2, $S_g$: 2, $S_c$: 2, $S_g^{ni}$: 3, $S_a$: 3, $S_b$: 3 \}
          \\ \hline
    \end{tabular}
    }
    \caption{Final raw scores and ratings based on different metrics computed. Higher ratings indicate higher bias for WRS and PIE \%, higher disruption for APE, greater inaccuracy for MASE and SMAPE, and higher accuracy for Sign Accuracy. For simplicity, we denoted the raw scores for accuracy metrics using just the mean value, but standard deviation was also considered for rating. The chosen rating level, L = 3. Overall, across all the settings, system $S_p$ exhibited statistical bias in 50 \% of cases, confounding bias in 100 \% of cases, and disruptive behavior in 50 \% of the cases based on APE values.}
    \label{tab:ratings}
\end{table*}



\begin{table*}
\centering
    \begin{tabular}{|p{5em}|p{1em}|p{3em}|p{3em}|p{3em}|p{3em}|p{3em}|p{3em}|p{3em}|p{3em}|p{3em}|}
    \hline
          {\bf Metric} &
          {\bf P} &    
          {\bf $S_{g}$} &
          {\bf $S_g^{ni}$} &
          {\bf $S_{p}$} &
          {\bf $S_p^{ni}$} &
          {\bf $S_{m}$} &
          {\bf $S_{c}$} &
          {\bf $S_a$} & 
          {\bf $S_b$} &
          {\bf $S_r$}
          \\ \hline 
          \multirow{6}{4em}{SMAPE$\downarrow$} & 
          P0 &
           0.049 $\pm$ 0.047  &
           0.095 $\pm$ 0.103  &
           0.095 $\pm$ 0.075 &
           0.079 $\pm$ 0.081 &
           0.097 $\pm$ 0.072 &
           0.043 $\pm$ 0.054 &
           \textbf{0.040 $\pm$ 0.037} &
          \multirow{6}{1.5em}{1.276 $\pm$ 0.663} &
          0.829 $\pm$ 0.638
          \\ \cline{2-9}
            \cline{11-11}
          % -------
            & 
          P1 &
          0.072 $\pm$ 0.123 &
          0.067 $\pm$ 0.178 &
          0.100 $\pm$ 0.125 &
          0.100 $\pm$ 0.143 &
          0.100 $\pm$ 0.076 &
          \textbf{0.065 $\pm$ 0.189} &
          0.084 $\pm$ 0.282 &
          &
          0.830 $\pm$ 0.639
          \\ \cline{2-9}
            \cline{11-11}
          % -------
            & 
          P2 &
          \textbf{0.051 $\pm$ 0.047} &
          0.060 $\pm$ 0.085 &
          0.100 $\pm$ 0.088 &
          0.095 $\pm$ 0.097 &
          0.098 $\pm$ 0.074 &
          0.053 $\pm$ 0.092 &
          0.069 $\pm$ 0.217 &
          &
          0.830 $\pm$ 0.639
          \\ \cline{2-9}
            \cline{11-11}
          % -------
            & 
          P3 &
          0.049 $\pm$ 0.045 &
          0.056 $\pm$ 0.052  &
          0.092 $\pm$ 0.074 &
          0.078 $\pm$ 0.078 &
          0.097 $\pm$ 0.071 &
          0.043 $\pm$ 0.048 &
          \textbf{0.040 $\pm$ 0.037} &
           &
          0.830 $\pm$ 0.640
          \\ \hline
          % -------

          \multirow{6}{4em}{MASE$\downarrow$} & 
          P0 &
          4.64 $\pm$ 4.62 &
          10.37 $\pm$ 13.63 &
          8.91 $\pm$ 7.01 &
          7.19 $\pm$ 6.94 &
          9.03 $\pm$ 6.91 &
          4.18 $\pm$ 7.75 &
          \textbf{3.79 $\pm$ 3.59} &
          \multirow{6}{1em}{947.56 $\pm$ 767.65} &
          86.45 $\pm$ 72.72
          \\ \cline{2-9}
            \cline{11-11}
          % -------
             & 
          P1 &
          6.13 $\pm$ 8.31 &
          5.65 $\pm$ 10.23 &
          9.19 $\pm$ 8.61 &
          8.87 $\pm$ 8.94 &
          9.32 $\pm$ 7.39 &
          \textbf{5.40 $\pm$ 12.45}&
          18.36 $\pm$ 168.82 &
          &
          86.99 $\pm$ 73.53 
          \\ \cline{2-9}
            \cline{11-11}
          % -------
             & 
          P2 &
          \textbf{4.74 $\pm$ 4.53} &
          5.59 $\pm$ 8.19 &
          9.32 $\pm$ 7.94 &
          8.49 $\pm$ 7.94 &
          9.15 $\pm$ 7.15 &
          4.99 $\pm$ 9.90 &
          8.24 $\pm$ 48.58 &
          &
          86.87 $\pm$ 73.32 
          \\ \cline{2-9}
            \cline{11-11}
             & 
          P3 &
          4.64 $\pm$ 4.42 &
          5.39 $\pm$ 5.27 &
          8.68 $\pm$ 7.18 &
          7.11 $\pm$ 6.75 &
          9.03 $\pm$ 6.90 &
          4.10 $\pm$ 6.33 &
          \textbf{3.79 $\pm$ 3.57} &
             &
          86.65 $\pm$ 73.11 
          \\ \hline
          % -------
          \multirow{4}{4em}{Sign Accuracy (\%)$\uparrow$}   & 
          P0 &
          52.08 &
          50.41 &
          45.09 &
          47.67 &
          40.70 &
          53.75 &
          \textbf{60.08} &
          \multirow{6}{1em}{62.60} &
          49.88
          \\ \cline{2-9}
            \cline{11-11}
             & 
          P1 &
          50.53 &
          53.93 &
          44.33 &
          46.77 &
          41.19 &
          52.09 &
          \textbf{57.08} &
          &
          49.62
          \\ \cline{2-9}
            \cline{11-11}
             & 
          P2 &
          49.75 &
          54.43 &
          44.02 &
          47.67 &
          41.05 &
          50.79 &
          \textbf{57.13} &
          &
          49.64
          \\ \cline{2-9}
            \cline{11-11}
              & 
          P3 &
          51.34 &
          52.97 &
          44.26 &
          47.50 &
          40.72 &
          51.35 &
          \textbf{59.98} &
           &
          49.71
          \\ \hline
    \end{tabular}
    %}
    \caption{Performance metrics for test systems across different perturbations. SMAPE and MASE scores are reported as mean $\pm$ standard deviation.}
    \label{tab:h4}
\end{table*}

\begin{table*}[ht]
\centering
   {\small
    \begin{tabular}{|p{8em}|p{8em}|p{3em}|p{12em}|p{9em}|p{8em}|}
    \hline
          {\bf Research Question} &    
          {\bf Causal Diagram} &
          {\bf Metrics Used} &
          {\bf Comparison across Systems} &
          {\bf Comparison across Perturbations} &
          {\bf Key Conclusions} \\ \hline 
          % 
          \textbf{RQ1:} Does $Z$ affect $R^{max}_{t}$, even though $Z$ has no effect on $P$? & 
          \begin{minipage}{.05\textwidth}
          \vspace{2.5mm}
          \centering
          \includegraphics[width=25mm, height=13mm]{figs/Causal_H1.png} 
          \end{minipage} &
          WRS &
          \{\textcolor{green}{$S_a$: 3.96}, $S_g$: 5.05, $S_g^{ni}$: 5.21, $S_r$: 5.34, $S_p^{ni}$: 5.38, $S_c$: 5.46, $S_m$: 5.75, \textcolor{red}{$S_p$: 6.28}, $S_b$: 6.9\} &
          \{\textcolor{green}{P2: 5.18}, P1: 5.2, P3: 5.35, \textcolor{red}{P0: 5.46}\} &
          \textbf{\textit{S} with low statistical bias}: $S_a$. 
          \textbf{\textit{S} with high statistical bias}: $S_p$.
          \textbf{\textit{P} that led to more statistical bias}: P0
          \textbf{Analysis with more discrepancy}: Inter-industry
          \\ \hline 
          % -------
          \textbf{RQ2:} Does $Z$ affect the relationship between $P$ and $R^{max}_{t}$ when $Z$ has an effect on $P$? & 
          \begin{minipage}{.05\textwidth}
          \vspace{2.5mm}
          \centering
          \includegraphics[width=25mm, height=13mm]{figs/Causal_H2.png}
          \end{minipage} &
          PIE \% &
          \{\textcolor{green}{$S_g^{ni}$: 523.09}, $S_g$: 621.68, $S_c$: 742.19, $S_a$: 1206.44, $S_m$: 1466.13, $S_p^{ni}$: 1655.94, \textcolor{red}{$S_p$: 2677.62}, $S_r$: 2733.7, $S_b$: 5197.51\} &
          \{\textcolor{green}{P1: 995.19}, P2: 1252.2, \textcolor{red}{P3: 1563.94}\} &
          \textbf{\textit{S} with low confounding bias}: $S_g^{ni}$. 
          \textbf{\textit{S} with high confounding bias}: $S_p$. 
          \textbf{\textit{P} that led to more confounding bias}: P3.  
          \textbf{Confounder that led to more bias}: \textit{Industry}
          \\ \hline 
          % -------
          \textbf{RQ3:} Does $P$ affect $R^{max}_{t}$ when $Z$ may have an effect on $R^{max}_{t}$? & 
          \begin{minipage}{.05\textwidth}
          \vspace{2.5mm}
          \centering
          \includegraphics[width=25mm, height=13mm]{figs/Causal_H3.png}
          \end{minipage} &
          APE &
          \{\textcolor{green}{$S_g^{ni}$: 6.49}, $S_g$: 8.34, $S_c$: 8.42, $S_m$: 17.46, $S_p^{ni}$: 21.94, $S_a$: 28.28, \textcolor{red}{$S_p$: 28.53}, $S_r$: 29.65, $S_b$: 50.37\} &
          \{\textcolor{green}{P2: 12.74}, P3: 17.34, \textcolor{red}{P1: 21.11}\} &
          \textbf{\textit{S} with low APE}: $S_g^{ni}$.
          \textbf{\textit{S} with high APE}: $S_p$.
          \textbf{\textit{P} with low APE}: P2.
          \textbf{\textit{P} with high APE}: P1.
          \textbf{Confounder that led to high APE}: \textit{Company}
          \\ \hline 
          % -------
          \textbf{RQ4:} Does $P$ affect the accuracy of $S$? & 
          This hypothesis does not necessitate a causal model for its evaluation. &
          SMAPE, MASE, Sign Accuracy &
          \textbf{SMAPE}: \{\textcolor{green}{$S_c$: 0.05}, $S_a$: 0.06, $S_g$: 0.06, $S_g^{ni}$: 0.07, $S_p^{ni}$: 0.09, $S_p$: 0.1, \textcolor{red}{$S_m$: 0.1}, $S_r$: 0.83, $S_b$: 1.28\}; 

          
          \textbf{MASE}: \{\textcolor{green}{$S_c$: 4.67}, $S_g$: 5.04, $S_g^{ni}$: 6.75, $S_p^{ni}$: 7.91, $S_a$: 8.54, $S_p$: 9.03, \textcolor{red}{$S_m$: 9.13}, $S_r$: 86.74, $S_b$: 947.56\};  

          
          \textbf{Sign Accuracy}: \{\textcolor{red}{$S_m$: 40.91}, $S_p$: 44.42, $S_p^{ni}$: 47.4, $S_r$: 49.71, $S_g$: 50.93, $S_c$: 51.99, $S_g^{ni}$: 52.94, \textcolor{green}{$S_a$: 58.57}, $S_b$: 62.6\}&
          \textbf{SMAPE}: \{\textcolor{green}{P3: 0.06}, P0: 0.07, P1: 0.08, \textcolor{red}{P2: 0.08}\}; 

          
          \textbf{MASE}: \{\textcolor{green}{P3: 6.11}, P0: 6.87, P2: 7.22, \textcolor{red}{P1: 8.99}\}; 

          
          \textbf{Sign Accuracy}: \{\textcolor{red}{P2: 49.26}, P1: 49.42, P3: 49.73, \textcolor{green}{P0: 49.97}\}&
          \textbf{\textit{S} with good performance}: $S_c$. 
          \textbf{\textit{S} with poor performance}: $S_m$. 
          \textbf{\textit{P} with high impact on performance}: P2.  
          \\ \hline 
    \end{tabular}
    }
    \caption{Summary of the research questions answered in the paper, causal diagram, metrics used in the experiment, average of the metric values compared across different systems, average computed across different perturbations, and the key conclusions drawn from the experiment. \textbf{Overall, multi-modal FMTS demonstrated greater robustness and forecasting accuracy compared to multi-modal FMTS. TS FMTS demonstrated greater robustness and forecasting accuracy compared to GP FMTS}. All the raw scores and ratings are shown in Table \ref{tab:ratings}.}
    \label{tab:cases-values}
\end{table*}


 \begin{figure*}[h]
  \centering
  \begin{subfigure}{\textwidth}
  \centering
  \includegraphics[width=.8\textwidth]{plots/smape.png} 
 \caption{SMAPE}
 \label{fig:bar-smape}
  \end{subfigure}
  \begin{subfigure}{\textwidth}
  \centering
  \includegraphics[width=.8\textwidth]{plots/mase.png}
 \caption{MASE}
 \label{fig:bar-mase}
  \end{subfigure}
  \begin{subfigure}{\textwidth}
  \centering
  \includegraphics[width=.8\textwidth]{plots/SignAcc.png}
  \caption{Sign Accuracy}
  \label{fig:bar-sign-acc}
  \end{subfigure}
  \caption{Bar plots showing the robustness metrics values across different systems and perturbations.}
 \label{fig:bar-acc}
\end{figure*}


 \begin{figure*}[h]
  \centering
  \begin{subfigure}{\textwidth}
  \centering
  \includegraphics[width=.8\textwidth]{plots/wrs.png} 
 \caption{WRS}
 \label{fig:bar-wrs}
  \end{subfigure}
  \begin{subfigure}{\textwidth}
  \centering
  \includegraphics[width=.8\textwidth]{plots/pie.png}
 \caption{PIE \% scores}
 \label{fig:bar-pie}
  \end{subfigure}
  \begin{subfigure}{\textwidth}
  \centering
  \includegraphics[width=.8\textwidth]{plots/ape.png}
  \caption{APE scores}
  \label{fig:bar-ape}
  \end{subfigure}
  \caption{Bar plots showing the robustness metrics values across different systems and perturbations.}
 \label{fig:bar-rob}
\end{figure*}

\begin{figure*}
 \centering
\includegraphics[width=1.1\textwidth]{plots/Radar_Rob.png}
\caption{Radar plots showing robustness metrics for all FMTS and $S_a$ under different perturbations. }
\label{fig:radar-rob}
\end{figure*}

\begin{figure*}[b]
     \centering
     \includegraphics[width=1.1\textwidth]{plots/Radar_Acc.png}
     \caption{Radar plots showing forecasting accuracy metrics for all systems under different perturbations.}
     \label{fig:radar-acc}
\end{figure*}



\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figs/heatmap.png}
    \caption{Heatmap for all metrics for all models. Lighter shade indicates better performance.}
    \label{fig:heatmaps}
\end{figure*}

\clearpage
\section{Additional User Study Results}
\label{sec:appendix-user-study}
In this section, we present all the hypotheses, the results from the statistical tests conducted to validate these hypotheses, and the conclusions drawn from the results. 


\begin{table*}[h!]
\centering
\begin{tabular}{|p{2.5em}|p{2.5em}|p{2.5em}|p{2.5em}|c|c|c|c|c|c|c|c|}
\hline
\textbf{Metric} & 
\textbf{Q1} & 
\textbf{Q2} & 
\textbf{Q4} & 
\textbf{Q5} & 
\textbf{Q6} & 
\textbf{Q8} & 
\textbf{Q9} & 
\textbf{Q10} & 
\textbf{Q12} &
\textbf{Q13} &
\textbf{Q14} 
\\ \hline
$\mu$ & 
3.1923 & 
2.8077 & 
2.5385 & 
2.7692 & 
2.9231 & 
2.6923 & 
2.9231 & 
3.2308 & 
2.6538 & 
2.8077 &
3.0769 \\ \hline
$\sigma$ & 
1.2335 & 
1.3570 & 
1.3336 & 
1.1767 & 
1.3834 & 
1.0870 & 
1.2625 & 
1.4507 & 
1.1981 & 
1.3570 & 
1.4676 \\ \hline
t-statistic & 4.9287 & 3.0349 & 2.0588 & 3.3333 & 3.4023 & 3.2476 & 3.7282 & 4.3259 & 2.7828 & 3.0349 & 3.7417 \\ \hline
p-value & 0.0000$^*$ & 0.0028$^*$ & 0.0250$^*$ & 0.0013$^*$ & 0.0011$^*$ & 0.0017$^*$ & 0.0005$^*$ & 0.0001$^*$ & 0.0051$^*$ & 0.0028$^*$ & 0.0005$^*$ \\ \hline
\end{tabular}
\caption{Summary of one sample right-tailed t-test results: Comparison of sample means to the hypothesized mean of 2 with a sample size of 26. The right-tailed p-values indicate whether the sample means are significantly greater the hypothesized mean. $^*$ denotes that mean of responses for all the questions is greater than 2.}
\label{tab:user-study-sanity}
\end{table*}


\begin{table*}[!ht]
\centering
    \begin{tabular}{|p{6cm}|p{2cm}|p{2cm}|p{6cm}|}
    \hline
        \textbf{Hypothesis} & 
        \textbf{Test Performed}  &
        \textbf{Statistics} &
        \textbf{Conclusion}\\
        \hline
         There is a high positive correlation between users' fairness rankings and rankings generated by our rating method.  & 
         Spearman Rank Correlation &
         $\rho = 0.73$ &
         The fairness rankings generated by our rating method aligns well with users' rankings. 
         \\ \hline
         The mean of the responses for Q4 is less than or equal to the mean of the responses for Q6. & 
         Paired t-test &
         t-statistic: -1.18, p-val: 0.12 &
         Users found it easy to interpret the behavior of the systems from rankings compared to graphs and statistics with a confidence interval of 85 \%.
         \\ \hline
         There is a very high positive correlation between users' rankings and rankings generated by our rating method. & 
         Spearman Rank Correlation &
         $\rho$: 0.91 &
         The robustness rankings generated by our rating method aligns very well with users' rankings.
         \\ \hline
         The mean of the responses for Q8 is less than or equal to the mean of the responses for Q10. & 
         Paired t-test &
         t-statistic: -1.89, p-val: 0.03 &
         Users found it easy to interpret the behavior of the systems from rankings compared to graphs and statistics with a confidence interval of 95 \%.
         \\  \hline
         There is a weak positive correlation between users' rankings and rankings generated by our rating method. & 
         Spearman Rank Correlation &
         $\rho$: 0.14 &
         The robustness rankings generated by our rating method weakly aligns with users' rankings.
         \\ \hline
         The mean of the responses for Q12 is less than or equal to the mean of the responses for Q14. & 
         Paired t-test &
         t-statistic: -1.62, p-val: 0.06 &
         Users found it easy to interpret the behavior of the systems from rankings compared to graphs and statistics with a confidence interval of 90 \%.
         \\  \hline
    \end{tabular}
    \caption{Table with the hypotheses evaluated in the user study, statistical tests used to validate the hypotheses, results obtained, and conclusions drawn.}
    \label{tab:user-study-results}
\end{table*}



% \begin{table}[h!]
% {\tiny
%     \centering
%     \begin{tabular}{|m{4.8cm}|m{0.6cm}|m{0.5cm}|m{0.7cm}|}
%         \hline
%         \textbf{Question} & \textbf{$\bar{x}$} & \textbf{$t$} & \textbf{p-value} \\
%         \hline
%         Familiarity with time-series & 3.1818 & 4.1608 & 0.0002* \\
%         \hline
%         Familiarity with financial tasks & 2.5909 & 2.2004 & 0.0196* \\
%         \hline
%         Ease of interpreting behavior through graphs in fairness study & 2.4545 & 1.5554 & 0.0674 \\
%         \hline
%         Rating accuracy in fairness study & 2.7273 & 2.7478 & 0.0060* \\
%         \hline
%         Ease of interpreting behavior through ratings & 2.8636 & 2.8444 & 0.0049* \\
%         \hline
%         Ease of interpreting behavior through graphs in robustness study 1 & 2.5455 & 2.4208 & 0.0123* \\
%         \hline
%         Rating accuracy in robustness study 1 & 2.6818 & 2.6418 & 0.0076* \\
%         \hline
%         Ease of interpreting behavior through ratings & 3.0000 & 3.2404 & 0.0020* \\
%         \hline
%         Ease of interpreting behavior through graphs in robustness study 2 & 2.4545 & 1.8002 & 0.0431* \\
%         \hline
%         Rating accuracy in robustness study 2 & 2.6364 & 2.1877 & 0.0201* \\
%         \hline
%         Ease of interpreting behavior through ratings & 2.9091 & 2.8257 & 0.0051* \\
%         \hline
%     \end{tabular}
%     \caption{Table showing the questions, sample mean, t-statistic and p-value from t-test computed for user responses (on a scale of 1-5) to the study. The hypothesized mean for all questions is 2. Significant p-values (p $<$ 0.05) are marked with an asterisk (*)\kl{Add Q3, Q7, and Q11 as well.}\kl{Move to supplementary}\kl{Hypothesis, correlation value, implications.}\biplav{Update numbers. All are significant now.}}
%     \label{tab:user-study}
% }
% \end{table}


% --- 
% \clearpage
% \section{User Study Questionnaire}
% \label{sec:appendix-study-q}
% Below is the user study form (questionnaire) that was circulated to collect responses for the user study.
% \includepdf[pages=-]{UserStudyForm.pdf}

\clearpage
\section{Source Code for Data Processing}
\label{sec:appendix-source-code}
\begin{lstlisting}
# Convert data from Yahoo! finance to sliding window format.
def sliding_window(data, window_size, company):
    sequences = []
    for i in range(len(data) - window_size):
        seq = data[i:(i + window_size + 1)].tolist()
        sequences.append([company] + seq)
    return pd.DataFrame(sequences)

# Perturbations:
# Drop-to-zero: Every 80th stock price in the numerical data will be turned into zero.
def drop_to_zero(df, col):

  new_df = df.copy()
  new_df.loc[new_df.index % 80 == 0, col] = 0

  return new_df

# Value halved: Every 80th stock price in the numerical data will be halved.
def value_halved(df, col):

  new_df = df.copy()
  new_df.loc[new_df.index % 80 == 0, col] /= 2

  return new_df

# Missing values: Every 80th stock price in the numerical data will be 'NaN'.
def missing_values(df, col):

  new_df = df.copy()
  new_df.loc[new_df.index % 80 == 0, col] = float('nan')

  return new_df

# Code to generate time series line plots.
def plot_ts(input_path, output_path):
    data = pd.read_csv(input_path)

    companies = data.iloc[:, 0]
    time_steps = data.iloc[:, 1:]

    for i, company in enumerate(companies):
        plt.figure(figsize=(12, 6))
        plt.plot(time_steps.columns, time_steps.iloc[i], marker='o')
        plt.title(f'Time Series for {company}', fontsize=19)
        plt.xlabel('Time Steps', fontsize=17)
        plt.ylabel('Values', fontsize=17)
        plt.grid(True)
        x_ticks = time_steps.columns[::5]
        plt.xticks(x_ticks, rotation=45, fontsize=15)
        plt.yticks(fontsize=15)
        plt.tight_layout()
        plt.savefig(os.path.join(output_path, f'sample_{i+1}_time_series.png'))
        plt.close()
\end{lstlisting}


\clearpage
\section{Additional Implementation Details}
\label{sec:appendix-implementation-details}
All Forecasting Model Training Systems (FMTS) were executed on Colab notebooks utilizing the L4 GPU available through Colab Pro, which offers 22.5 GB of GPU RAM. Additional details regarding the models such as the inference times and other architectural details can be found in Section \ref{sec:systems}.

\subsubsection{Hyperparameters set}
\begin{itemize}
    \item \textbf{MOMENT}: head\_dropout: 0.1, weight\_decay: 0, freeze\_encoder: True, freeze\_embedder: True, freeze\_head: False
    \item \textbf{Phi-3}: \_attn\_implementation='eager', max\_new\_tokens: 300, temperature: 0.0, do\_sample: False
    \item \textbf{Gemini}: Temperature: 0. Rest of the parameters were default.
\end{itemize}

        
\section{Reproducibility Checklist}
\label{sec:appx-reproduc}
This paper:

\begin{enumerate}
    \item Includes a conceptual outline and/or pseudocode description of AI methods introduced 


    \textbf{Answer}: Yes (Appendix \ref{sec:appendix-algo-details}).

    \item Clearly delineates statements that are opinions, hypothesis, and speculation from objective facts and results 

    \textbf{Answer}: Yes (Section \ref{sec:expts})

    \item Provides well marked pedagogical references for less-familiar readers to gain background necessary to replicate the paper 

    \textbf{Answer}: Yes (Sections \ref{sec:introduction}, \ref{sec:related-work})
    
    
    Does this paper make theoretical contributions? (yes/no)

    \textbf{Answer}: No

% If yes, please complete the list below.

% All assumptions and restrictions are stated clearly and formally. (yes/partial/no)
% All novel claims are stated formally (e.g., in theorem statements). (yes/partial/no)
% Proofs of all novel claims are included. (yes/partial/no)
% Proof sketches or intuitions are given for complex and/or novel results. (yes/partial/no)
% Appropriate citations to theoretical tools used are given. (yes/partial/no)
% All theoretical claims are demonstrated empirically to hold. (yes/partial/no/NA)
% All experimental code used to eliminate or disprove claims is included. (yes/no/NA)

    \item Does this paper rely on one or more datasets? 

    \textbf{Answer}: Yes (Description in Section \ref{sec:exp_app}

    \item A motivation is given for why the experiments are conducted on the selected datasets 

    \textbf{Answer}: Yes (Sections \ref{sec:introduction}, \ref{sec:related-work})

    \item All novel datasets introduced in this paper are included in a data appendix. 

    \textbf{Answer}: NA (We used an existing dataset from Yahoo! Finance.)

    
    \item All novel datasets introduced in this paper will be made publicly available upon publication of the paper with a license that allows free usage for research purposes. 

    \textbf{Answer}: NA

    \item All datasets drawn from the existing literature (potentially including authors’ own previously published work) are accompanied by appropriate citations. 

    \textbf{Answer}: NA
    
    \item All datasets drawn from the existing literature (potentially including authors’ own previously published work) are publicly available. 

    \textbf{Answer}: Yes (Section \ref{sec:exp_app})

    
    \item All datasets that are not publicly available are described in detail, with explanation why publicly available alternatives are not scientifically satisfying. 

    \textbf{Answer}: NA 

    \item Does this paper include computational experiments? 

    \textbf{Answer}: Yes

    \item Any code required for pre-processing data is included in the appendix.

    \textbf{Answer}: Yes, code required to convert the data downloaded from Yahoo! finance to sliding window, apply perturbations, and generate time series plots are provided in Appendix \ref{sec:appendix-source-code}.
    
    \item All source code required for conducting and analyzing the experiments is included in a code appendix.

    \textbf{Answer}: Yes. You can find the source code and data here: \url{https://anonymous.4open.science/r/rating-fmts-1B30/README.md}

    \item All source code required for conducting and analyzing the experiments will be made publicly available upon publication of the paper with a license that allows free usage for research purposes. 

    \textbf{Answer}: Yes

    \item All source code implementing new methods have comments detailing the implementation, with references to the paper where each step comes from 

    \textbf{Answer}: Yes
    
    \item If an algorithm depends on randomness, then the method used for setting seeds is described in a way sufficient to allow replication of results. 

    \textbf{Answer}: Yes (provided in the source code).

    \item This paper specifies the computing infrastructure used for running experiments (hardware and software), including GPU/CPU models; amount of memory; operating system; names and versions of relevant software libraries and frameworks. 

    \textbf{Answer}: Yes (in Appendix \ref{sec:appendix-implementation-details})

    \item This paper formally describes evaluation metrics used and explains the motivation for choosing these metrics. 

    \textbf{Answer}: Yes (Section \ref{sec:metrics} and Appendix \ref{sec:appendix-metrics})
    
    \item This paper states the number of algorithm runs used to compute each reported result. 

    \textbf{Answer}: No.

    \item Analysis of experiments goes beyond single-dimensional summaries of performance (e.g., average; median) to include measures of variation, confidence, or other distributional information.

    \textbf{Answer}: Yes (Section \ref{sec:expts}, Appendix \ref{sec:appendix-experiments}, Section \ref{sec:userstudy}, and Appendix \ref{sec:appendix-user-study})

    \item The significance of any improvement or decrease in performance is judged using appropriate statistical tests (e.g., Wilcoxon signed-rank). 

    \textbf{Answer}: Yes (Section \ref{sec:expts}, Appendix \ref{sec:appendix-experiments}, Section \ref{sec:userstudy}, and Appendix \ref{sec:appendix-user-study}) 

    
    \item This paper lists all final (hyper-)parameters used for each model/algorithm in the paper’s experiments. (yes/partial/no/NA)

    \textbf{Answer}: Yes (Appendix Appendix \ref{sec:appendix-implementation-details}).

    
    \item This paper states the number and range of values tried per (hyper-) parameter during development of the paper, along with the criterion used for selecting the final parameter setting. (yes/partial/no/NA)

    \textbf{Answer}: NA

\end{enumerate}
\end{document}
