\section{Solution Approach}
% \zhen{Rating Framework instead? and update all the other mentions accordingly if you agree}\kl{As we are also talking about perturbations and model predictions, it wouldn't make sense.}
Our solution approach consists of the following 
% \zhen{I'm not sure how much we can claim novelty on these components, the overall rating framework provides a new capability, but I might not try to claim novelty on every component within it}\kl{Agreed. Changed a couple of things.} 
components: (1) Three perturbations that are applied to both numerical time series data and line plots.  (2) Metrics, APE and PIE \% 
% \zhen{all metrics?}\kl{Other metrics are from previous works.}
that assess the performance of FMTS from accuracy and robustness perspectives and aid in assigning ratings. (3) A `data to predictions' and `predictions to ratings' workflow for rating FMTS using both uni-modal (numerical) and multi-modal (line plots (image)) data.
% \zhen{I'd recommend to move the metrics here and keep them concise to keep the overall rating framework in one place. And it will be much direct and clear here to associate different metrics to each of the RQs.}
%\vspace{-1.1em}

\noindent \textbf{Perturbations:}
\label{sec:perturbations}
We introduce one syntactic (STP) and two semantic perturbations (SMP) (Figure \ref{fig:system-workflow}) inspired by real-world applications in unintended scenarios to assess the accuracy and robustness of FMTS. SMPs alter data meaning while preserving context, e.g., a stock's value might change due to data entry errors or market-specific catalysts. STPs modify the structure of the data without altering the content. %Figure \ref{fig:system-workflow} shows unperturbed and perturbed line plots along with the workflow. %We introduce the following perturbations:
% SPs are alterations made to data that change its meaning while preserving the overall context. For example, in time series forecasting, a stock's value might change drastically due to errors in data entry, or it might fluctuate due to some market-specific catalyst that affects certain companies. Figure \ref{fig:system-workflow} shows the unperturbed image and perturbed line plots along with the workflow. We introduce the following perturbations:
% \zhen{a general comment, there are details that are implementation or experiment detail, but not part of the the methodology formulation itself, be careful to not include those in the methodology section (came across several instances that need attention), and only mention those details later in implementation or experiments, or briefly mention them in methodology by saying In the experiments, we used ...}\kl{Noted.}
%\noindent {\bf Drop-to-zero (P1)} is inspired by the common data entry errors \cite{ley2019analysis}. We set every $n^{th}$ value in the original stock price data collected from the source to zero. Once we sample the time series with a sliding window of size $n$, each sample will have a zero. 
\noindent {\bf Drop-to-zero (P1)} is an SMP inspired by common data entry errors \cite{ley2019analysis}. Every $n^{th}$ value in the original stock price data is set to zero. Sampling the time series with a sliding window of size $n$ ensures each sample contains a zero.
\noindent In {\bf Value halved perturbation (P2)}, an SMP, we reduce every $n^{th}$ number in the original stock price data to half of its value. This perturbation simulates periodic adjustments, possibly reflecting events like stock splits or dividend payments. {\bf Missing values perturbation (P3)}, an STP, converts every $n^{th}$ number in the original stock price data to a null value, simulating real-world missing data points in financial datasets due to system incomplete transmissions.

\subsection{Evaluation Metrics}
\label{sec:metrics}

In this section, we describe our evaluation metrics for measuring forecasting accuracy and robustness.\footnote{Full formulas are provided in the Appendix \ref{sec:appendix-metrics}.\label{footnote:formula}}

\noindent \textbf{Forecasting Accuracy Metrics} We evaluate forecasting accuracy using three metrics \cite{makridakis2022m5}: Symmetric mean absolute percentage error (SMAPE) measures average percentage error between actual and predicted values. Mean absolute scaled error (MASE) compares the mean absolute error of forecasts to a naive one-step forecast. Sign Accuracy quantifies how well predicted forecasts align with recent observed values. For SMAPE and MASE, lower values indicate better performance, while higher values are better for Sign Accuracy.

\noindent \textbf{Robustness Metrics} We adapt the Weighted Rejection Score (WRS)$^\text{\ref{footnote:formula}}$ originally proposed in \cite{kausik2024rating} to measure statistical bias by comparing max residual distributions for different values of the sensitive attributes using Student's t-test \cite{student1908probable} under different confidence intervals (CI). It helps us answer RQ1. Additionally, we introduce two new metrics: APE and PIE \% (modified versions of ATE \cite{abdia2017propensity} and DIE \% \cite{kausik2024rating}) tailored to answering our research questions:



\noindent{\bf Average Perturbation Effect (APE)} Average Treatment Effect provides the average difference in outcomes between between treated and untreated units \cite{wang2017g}. In our context, it computes the difference between perturbed data residuals (P1 through P3) and the unperturbed data residuals (P0), thereby measuring the impact of the perturbation on the outcome. Hence, we refer to this metric as APE. This metric helps us answer RQ3. It is formally defined using the following equation: 
\vspace{-0.2em}
{\small
    \begin{equation}
    [|E[R^{max}_{t} = j| do(P = i)] - E[R^{max}_{t} = j| do(P = 0)]| ]
    \label{eq:ape}
    \end{equation}
} 
\noindent \textbf{Propensity Score Matching - Deconfounding Impact Estimation \% (PSM-DIE \% or PIE \%)} In \cite{kausik2024rating}, a linear regression model was used to estimate causal effects, assuming a linear relationship between variables. This method, however, doesn't capture non-linear relationships or fully eliminate confounding biases. They proposed the DIE \% metric for binary treatment values. Our work uses six treatment (perturbation) values, applying Propensity Score Matching (PSM) \cite{rosenbaum1983central} to target confounding effects by matching treatment and control units based on treatment probability, similar to RCTs and independent of outcome variables \cite{baser2007choosing}. We modify DIE \% to introduce PIE \% (PSM-DIE\%) to answer RQ2. It is defined as:
{\small
\begin{equation}
\left[ ||APE_{o}| - |APE_{m}|| \right] * 100
\label{eq:pie}
\end{equation}
}
%\vspace{-2em}

\noindent $APE_{o}$ and $APE_{m}$ represent APE computed before and after applying PSM, respectively. $PIE \%$ measures the true impact of $Z$ on the relationship between $P$ and $R^{max}_{t}$.



% \subsection{Workflow}
% Our proposed workflow is composed of two parts: (a) Data to Predictions, which Figure \ref{fig:system-workflow} depicts this workflow where FMTS process the input and predicts the next `d' timesteps (with $d = 20$). FMTS and baseline models used are described in Section \ref{sec:systems}; (b) Predictions to Ratings, which Figure \ref{fig:rating-workflow} describes the process, where a method from \cite{kausik2024rating} has been adoprted. In \cite{kausik2024rating}, the authors introduced a method to rate text-based sentiment analysis systems for bias. We adapted their approach to accommodate our more complex multi-modal data with multiple perturbations, going beyond the textual data and binary treatments used in the original work. The modified metrics, APE and PIE \%, as defined in Section \ref{sec:metrics}, enable us to effectively handle this complexity. The modified rating algorithms are detailed in the appendix. The metrics, referred to as \textit{raw scores}, establish a partial order that assists in determining the final system ratings. The range of final ratings depends on the rating level, L.
\subsection{Workflow}
\label{sec:workflow}
Our proposed workflow consists of two parts: \textbf{Data to Predictions} and \textbf{Predictions to Ratings}. In the first part, as shown in Figure \ref{fig:system-workflow}\footnote{All the figures in the paper can be found in a higher resolution in the Appendix \ref{sec:appendix-high-res-figs}.}, FMTS process the input and predict the next `d' timesteps. The FMTS and baseline models are described in Section \ref{sec:systems}. In the second part, illustrated in Figure \ref{fig:rating-workflow}, we adapt the method from \cite{kausik2024rating} to rate text-based sentiment analysis systems for bias. We extended their approach to handle our more complex multi-modal data with multiple perturbations, beyond the original textual data and binary treatments. The modified metrics, APE and PIE\%, defined in Section \ref{sec:metrics}, help manage this complexity. The rating algorithms are detailed in Appendix \ref{sec:appendix-algo-details}. These metrics, referred to as \textit{raw scores}, establish a partial order for determining final system ratings, which vary by rating level, L.


\begin{figure*}
     \centering
     \begin{subfigure}[b]{0.55\textwidth}
         \centering
    \includegraphics[width=\textwidth]{figs/FMTS_Workflow.png}
         \caption{\textbf{Data to predictions}}
         \label{fig:system-workflow}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.38\textwidth}
     \centering
     \includegraphics[width=\textwidth]{figs/rating-workflow-v2.png}
     \caption{\textbf{Predictions to ratings}}
     \label{fig:rating-workflow}
     \end{subfigure}
     \vspace{-1em}
    
    
    % \sunandita{Figure can be optimized further. Lots of empty space here}
    % \zhen{it's better that the perturbation is shown on the same underlying time series. Does the image represent the actual image sent to the multimodal FMTS? with or without the grid?}\kl{yes.}
    
    \label{fig:workflow}
    \caption{(a) Black arrows denote the unperturbed and red arrows indicate the perturbed paths. Dashed lines shows the multi-modal path. The perturbed parts of the plots are highlighted in red.
    (b) Workflow for performing statistical and causal analysis to compute raw scores and assign final ratings to the test systems}
    \vspace{-1em}
\end{figure*}


% \noindent \textbf{From Data to Predictions:(Figure \ref{fig:system-workflow})} 
% % \zhen{is it necessary to list data to prediction, then prediction to ratings? I think the flow is simple enough even without enumerating them.}
%  Red arrows denote the workflow for perturbed data, while black arrows represent the workflow for unperturbed data. Dashed arrows denote the multi-modal path. FMTS process the input and predicts the next `d' timesteps (with $d = 20$). FMTS and baseline models are described in Section \ref{sec:systems}.
% \sunandita{This section feels strange. We need a better description about the data->perturbations step. The arrows are already explained in the caption.}

% \noindent \textbf{From Predictions to Ratings: (Figure \ref{fig:rating-workflow})}
% In \cite{kausik2024rating}, the authors introduced a method to rate text-based sentiment analysis systems for bias. We adapted their approach to accommodate our more complex multi-modal data with multiple perturbations, going beyond the textual data and binary treatments used in the original work. The modified metrics, APE and PIE \%, as defined in Section \ref{sec:metrics}, enable us to effectively handle this complexity. The modified rating algorithms are detailed in the appendix. The metrics, referred to as \textit{raw scores}, establish a partial order that assists in determining the final system ratings. The range of final ratings depends on the rating level, L. 
% \zhen{maybe a high-level explanation of what was improved to accomondate multi-modal data? Then for the sake of space, the detailed algorithm is in appendix.} 

% After data preprocessing, the first step is to identify sensitive attributes that may act as confounders. 
% The process begins with data preprocessing, followed by the identification of sensitive attributes that may act as confounders.
% If no confounders are detected, statistical methods are used to answer RQ1 from Section \ref{sec:problem}, using the bias metric described in Section \ref{sec:metrics}. In the presence of confounders, causal analysis is necessary. We utilize two metrics: one to assess the impact of each perturbation and another to compute confounding bias, addressing RQ2 and RQ3. 

% \zhen{it's a bit confusing when trying to connect this with the RQs earlier, but could  just be due to my lack of background in causal analysis.}
% The final ratings from our experiments are shown in Table \ref{tab:ratings}.


% \subsection{User Study}
% \label{sec:user-study}
% We conducted a user study to evaluate and validate the scores and ratings generated by our approach for comparing the performance of various time-series forecasting models (TSFMs) based on a given metric. Our evaluation focused on two key metrics: robustness and fairness. For each metric, participants were asked to rank the systems given different representation of systems' performance, i.e., only plots or generated scores, and rankings.
% %
% The primary objectives of this user study were to assess: (a) hthe difficulty users experienced in evaluating the performance of TSFMs using different data representations (plots, scores, and rankings), and (b)  the degree of alignment between users' rankings and the rankings produced by our approach.

% In Section \ref{sec:userstudy}, we provide a detailed discussion of the user study setup, the hypotheses being tested, the results obtained, and our interpretations and conclusions.


% The primary aim of our user study is to assess how effectively users can evaluate and rank various time-series forecasting models (TSFMs), whose behavior is modeled through various representations, i.e., plots, scores, and rankings. This study is IRB-approved by our institution\footnote{Details anonymized for reviewing.} and involves users interacting with the FMTS models to predict future stock prices for companies across different industries (e.g., Technology and Pharmaceuticals). We introduce participants to definitions such as robustness, fairness, and error metrics (maximum residual, mean, and standard deviation of errors) to ensure they could make informed evaluations, regardless of prior knowledge.

% Our study consists of three panels: one on fairness, two on robustness (under perturbations P1 and P2). In the fairness study, participants are presented with fairness graphs of six different systems and an ideal system using stock price data from two industries: Technology and Pharmaceuticals. They are provided with mean and standard deviation of errors, and asked to rank the systems from least to most fair. They are then asked to compare their rankings to those generated by our automated procedure, with respect to accuracy and  ease of comparison. Similar questions are asked for the robustness study, where participants evaluate systems based on their resistance to changes or disruptions (we call these as perturbations). The goal is to understand how the users perceive the fairness and robustness of different TSFMs. The summarized version of the questions along with the results are shown in \ref{tab:user-study}. The complete set of questions and visualizations used in the study can be found in User Study section of the supplementary.



