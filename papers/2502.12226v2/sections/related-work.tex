\section{Related Work}
\label{sec:related-work}
%We now contextualize our work with related literature so that our contributions are highlighted. We cover FMTS, perturbations in time-series, 
% robustness testing of FMs, 
%and rating of AI systems. 

\noindent \textbf{Foundation Models Supporting Time Series} 
The use of FMs for time series forecasting has advanced significantly. 
% \cite{lu2022frozen} first demonstrated that transformers pre-trained on text data (LLMs) can effectively solve sequence modeling tasks in other modalities, paving the way for leveraging language pre-trained transformers for time series analysis. Recent studies have focused on reprogramming LLMs for time series tasks through parameter-efficient fine-tuning and suitable tokenization strategies \cite{zhou2023one, gruver2024large, jin2023time, cao2023tempo, ekambaram2024tiny}. These methods have successfully adapted transformers to the unique challenges of time series forecasting. \cite{zhou2023one} and \cite{jin2023time} further illustrate the versatility and robustness of fine-tuned language pre-trained transformers for diverse time series tasks.
\cite{lu2022frozen} showed that transformers pre-trained on text data can solve sequence modeling tasks in other modalities, enabling their application to time series analysis. Recent studies have reprogrammed LLMs for time series tasks through parameter-efficient fine-tuning and tokenization strategies \cite{zhou2023one, gruver2024large, jin2023time, cao2023tempo, ekambaram2024tiny}. 
% These methods have successfully adapted transformers to the unique challenges of time series forecasting. 
\cite{zhou2023one} and \cite{jin2023time} further illustrate the versatility and robustness of fine-tuned language pre-trained transformers for diverse time series tasks.
% Several models have contributed to the advancement of time series forecasting. \cite{ansari2024chronos} and \cite{woo2024unified} have improved forecasting accuracy and model generalization.  
% % \cite{ansari2024chronos} and \cite{woo2024unified} have pushed the boundaries of forecasting accuracy and model generalization. 
% \cite{rasul2023lag} and \cite{das2023decoder} have explored new tokenization strategies and fine-tuning methods to improve model performance. Additionally, \cite{garza2023timegpt} and \cite{ekambaram2024tiny} have focused on creating lightweight and efficient models for real-time applications. \cite{talukder2024totem} stands out with its unique approach to integrating multiple temporal patterns, enhancing forecasting precision.
% FMs trained from scratch have achieved SOTA on time series tasks. Zero-shot forecasting, exemplified by \cite{gruver2024large}, showcases the ability of these models to make accurate predictions without domain-specific training. \cite{cao2023tempo} and \cite{goswami2024moment} have introduced approaches to enhance the performance and efficiency of time series models, leveraging transformer architectures to capture temporal dependencies more effectively. In our experiments, we select Gemini-V and Phi-3 as the GP models and Chronos and MOMENT as TS models due to their SOTA performance in their respective categories.
Several models have advanced time series forecasting. \cite{ansari2024chronos} and \cite{woo2024unified} have improved forecasting accuracy and model generalization, while
% \cite{ansari2024chronos} and \cite{woo2024unified} have pushed the boundaries of forecasting accuracy and model generalization. 
\cite{rasul2023lag} and \cite{das2023decoder} have explored new tokenization strategies and fine-tuning methods. \cite{garza2023timegpt} and \cite{ekambaram2024tiny} developed lightweight models for real-time applications, and \cite{talukder2024totem} integrated multiple temporal patterns to improve precision. FMs trained from scratch, like \cite{gruver2024large}, achieved SOTA in zero-shot forecasting, with \cite{cao2023tempo} and \cite{goswami2024moment} further improving model performance. 
%In our experiments, we select Gemini-V and Phi-3 as the GP models and Chronos and MOMENT as TS models due to their SOTA performance in their respective categories.
Please see Section~\ref{sec:exp_app} for the FMTS we selected due to their SOTA performance in their respective categories.

%The use of FMs for time series forecasting has seen significant advancements in recent years. \cite{lu2022frozen} first demonstrated that transformers pre-trained on text data (LLMs) can effectively solve sequence modeling tasks in other modalities. This work opened the door to leveraging language pre-trained transformers for time series analysis. Recent studies have built on this foundation, focusing on reprogramming LLMs for time series tasks through parameter-efficient fine-tuning and suitable tokenization strategies \cite{zhou2023one, gruver2024large, jin2023time, cao2023tempo, ekambaram2024tiny}. These methods have proven successful in adapting the powerful capabilities of transformers to the unique challenges of time series forecasting. OneFitsAll \cite{zhou2023one} and Time-LLM \cite{jin2023time} further illustrate how language pre-trained transformers can be fine-tuned for diverse time series tasks, demonstrating their versatility and robustness. 
% \zhen{reason why we didn't include these models in our study, weights not available? or other justification, to prevent that naturally raised question from readers.}\kl{Good point. We need to discuss. I added 2 sentences at the bottom but they are probably not very convincing.}
%Several other models have contributed to the advancement of time series forecasting. Chronos \cite{ansari2024chronos} and Moirai \cite{woo2024unified} have pushed the boundaries of forecasting accuracy and model generalization. Lag-llama \cite{rasul2023lag} and TimesFM \cite{das2023decoder} have explored new tokenization strategies and fine-tuning methods to improve model performance. Additionally, Time-GPT1 \cite{garza2023timegpt} and Tiny-Time Mixers \cite{ekambaram2024tiny} have focused on creating lightweight and efficient models suitable for real-time applications. TOTEM \cite{talukder2024totem} stands out with its unique approach to integrating multiple temporal patterns, further enhancing forecasting precision.
%Aside from reprogramming LLMs for time series, FMs trained from scratch have achieved SOTA on times series tasks. 
%Zero-shot forecasting, exemplified by \cite{gruver2024large}, showcases the ability of these models to make accurate predictions without domain-specific training.  TEMPO \cite{cao2023tempo} and MOMENT \cite{goswami2024moment} have introduced approaches to enhance the performance and efficiency of time series models, leveraging transformer architectures to capture temporal dependencies more effectively.
% \zhen{and these are on various time series tasks including time series forecasting?}
% \zhen{These are models that are specifically trained for time series forecasting, I'd suggest mentioning them first after the LLM reprogramming, and then expanding to the models that are trained across time series tasks instead. The flow of this subsection feels a bit odd as of now.} \kl{Done.}
%In our experiments, we select Gemini-V and Phi-3 as the GP models and Chronos and MOMENT as TS models due to their SOTA performance in their respective categories. 

%\vspace{-0.3em}
\noindent \textbf{Perturbations in Time Series Data} TS data is commonly stored in spreadsheets and databases, which are prone to changes due to acts of omission (e.g., negligence, data-entry errors) or commission (e.g., adversarial attacks, sabotage). Omission errors are most common \cite{spreadsheets-errors-risks-survey}. Tools like Microsoft Excel and Google Sheets are widely used for data collection and analysis, allowing end-user programming \cite{spreadsheets-future-workshop}. However, over 90\% of spreadsheets contain errors due to issues like incorrect formulae, leading to multi-billion dollar losses \cite{spreadsheet-qa-survey}.
%\cite{spreadsheet-qa-survey,spreadsheets-errors-risks-survey}.
Adversarial attacks are also increasing in data stores and AI models for tasks like forecasting.
% \cite{papernot2016transferability} introduced a black-box attack method using a substitute model to generate adversarial examples, demonstrating transferability across tasks. \cite{baluja2017adversarial} focused on white-box attacks using gradient information. 
\cite{karim2019adversarial} adapted these concepts to time series, exploring both black-box and white-box attacks. \cite{oregi2018adversarial} revealed the vulnerability of distance-based classifiers. \cite{rathore2020untargeted} examined various adversarial attacks on time series classifiers. TSFool \cite{li2022tsfool} introduced a multi-objective black-box attack to craft imperceptible adversarial time series to fool RNN classifiers.
%Time series (TS) data is widely stored and manipulated in spreadsheets and databases. These are also the tools which see considerable changes or perturbations due to acts of omission that are unintended (e.g., negligence, data-entry errors) or commission which are deliberate (e.g., adversarial attacks, sabotage). 
%Among these, changes due to omission are most common \cite{spreadsheets-errors-risks-survey}.
%For example, a spreadsheet, implemented in tools like Microsoft Excel and Google Sheets, is a common data collection and analysis environment that also allows end-user programming \cite{spreadsheets-future-workshop}. They are used widely at the workplace and are often a door opener to more advanced scientific tools. But gaining expertise in them needs practice since a large proportion of spreadsheets ($\succ$ 90\%) are known to have errors due to issues like incorrect formulae caused by improper understanding of behavior during routine operations like copy-paste and end-user programming, which have caused losses of multi-billion dollars \cite{spreadsheet-qa-survey,spreadsheets-errors-risks-survey}.
% \zhen{do we need to relate our perturbations to these attacks? otherwise, we must manage the readers' expectations on what types of perturbations we focus on other than adversarial attacks, and motivate it properly}
%\zhen{Play down this a bit, and emphasize and justify why we focus on the type of perturbations we consider in the paper, to mimic operational errors in practice apart from adversarial attacks, citing the 2024 and 1996 papers Biplav added.} 
%Furthermore, adversarial attacks are also increasing both in data stores and in AI models created to solve tasks like forecasting.
%Foundational work by ~\cite{papernot2016transferability} introduced a black-box attack method that involved training a substitute model to generate adversarial examples capable of misleading the target model, demonstrating the transferability property across similar tasks. In contrast, research by ~\cite{baluja2017adversarial} focused on white-box attacks, using gradient information and probabilistic outputs to craft adversarial examples. Researchers~\cite{karim2019adversarial} have adapted these concepts to the time series domain, exploring both black-box and white-box attacks on time series classification models. In addition, ~\cite{oregi2018adversarial} revealed the susceptibility of distance-based time-series classifiers to adversarial examples. ~\cite{rathore2020untargeted} examined untargeted, targeted, and universal adversarial attacks on time series classifiers, demonstrating the effectiveness of these attacks across various datasets. TSFool~\cite{li2022tsfool} introduced a multi-objective black-box attack to craft highly imperceptible adversarial time series to fool RNN classifiers.
%Adversarial attacks on time-series data are initially focused on time-series classification tasks, leveraging concepts adapted from adversarial attacks in other domains.
%explored adversarial sample crafting for time series classification using elastic similarity measures,  %These works collectively underscore the ongoing efforts to understand and mitigate the risks posed by adversarial attacks on time series classification models.
% More recently, research into adversarial attacks on time series forecasting models has revealed distinct challenges and novel attack strategies. One primary challenge is targeted attacks. While targeted adversarial attacks on time series classification aim to misclassify specific instances, achieving similar precision in time series forecasting is more complex due to the sequential nature of the data. Perturbations must be designed to influence specific aspects of the forecast (e.g., directional shifts or amplitude changes) without disrupting the overall temporal dependencies, making precise control more challenging~\cite{govindarajulu2023targeted}. Another challenge is attacks on multivariate forecasting. Adversarial attacks could exploit the inter-dependences between variables. ~\cite{liu2022robust} introduced sparse and indirect cross-time-series attacks in multivariate settings, which are more effective and realistic than direct attacks in univariate cases.
% \zhen{Biplav, could we make a quick comment here as well that we focus more on data error side in practice, other than attacks? and cite the paper that you mentioned on data errors? Otherwise this section of adversarial attacks feel a bit standalone to other sections}
%These challenges underscore the need for ongoing research to develop effective adversarial attack strategies and robust defense mechanisms tailored to the unique characteristics of time series forecasting models.
% -----


\noindent \textbf{Rating AI Systems} Several works have assessed and rated AI systems for trustworthiness from a third-party perspective without access to training data. \cite{srivastava2020rating} proposed a method to rate AI systems for bias, specifically targeting gender bias in machine translators \cite{srivastava2018towards}, and used visualizations to communicate these ratings \cite{bernagozzi2021vega}. They conducted user studies on trust perception through visualizations \cite{vega-userstudy-translatorbias}, but these lacked causal interpretation. \cite{kausik2024rating} introduced a causal analysis approach to rate bias in sentiment analysis systems, extending it to assess their impact when used with translators \cite{kausik2023the}. We extend their method to rate MM-TSFM for robustness against perturbations. Causal analysis offers advantages over statistical analysis by determining accountability, aligning with humanistic values, and quantifying the direct influence of various attributes on forecasting accuracy.

