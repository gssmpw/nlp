\section{Discussion and Conclusion}

Our paper aimed to measure the impact of perturbations and confounders on the outcome of FMTS using stock prices across leading companies and industries. We studied \textit{Industry} and \textit{Company} as confounders, motivated by the intuition that stakeholders rely on learning-based systems for stock purchase decisions and would be interested in knowing if model errors depend on stock price ranges. For example, does a model commit more errors predicting META's stock prices compared to MRK's? To minimize volatility effects, we performed both intra-industry and inter-industry analyses. In future, we plan to study confounders like seasonal trends and financial news.
%Our chosen metrics effectively answered the research questions in Section \ref{sec:problem}.
As demonstrated, we believe metrics should be selected based on the questions one wants to answer, rather than relying solely on statistical accuracy. The hypothesis testing approach from \cite{kausik2024rating}, adapted for our work, helped quantify biases and perturbation impacts on test systems. 
% Traditional explainability approaches often limit human decision-making involvement \cite{miller2023explainable}, whereas hypothesis-driven decision support, despite increasing cognitive load, can improve AI-assisted decision support. We built a causal model based on intuition and tested each causal link's validity through hypothesis formation and metric computation.
The perturbations used in our analysis have real-world impacts, applicable to both numeric and multi-modal data. While methods like differential evaluation can find the most impactful perturbation variations, we focused on assessing whether simple, subtle perturbations affect FMTS.

\noindent \textbf{Conclusion} We proposed a causally grounded empirical framework to study FMTS robustness against three input perturbations, evaluating seven state-of-the-art FMTS across six prominent stocks in three industries. Our framework's ratings accurately assessed FMTS robustness and provided actionable insights for model selection and deployment. Experiments showed multi-modal FMTS exhibited greater robustness, while uni-modal FMTS had higher forecasting accuracy. FMTS trained on time series tasks showed better robustness and accuracy compared to general-purpose FMTS. A user study confirmed our ratings effectively convey FMTS robustness to end-users, demonstrating the framework's real-world applicability.

% \kl{Assumptions made throughout the paper and current scope along with future work: on confounder, perturbations, systems, causal model variations, user study (participants number)}
%Our paper aimed to measure the impact of perturbations and confounders on the outcome of FMTS.  We used the stock prices of six leading companies across three industries for our evaluation. 


%However, we would like to collect historical data for more than two companies in each industry as future work.
%The analyses we did in the paper were with respect to \textit{Industry} and \textit{Company}. The motivation behind choosing these specific confounders is based on our intuition that, as stakeholders are relying more on learning-based systems to decide whether to purchase stocks from a company, they would be interested to know if the errors committed by these learning-based models depend on the range of stock prices (which could serve as a proxy for the industry or company). For example, is a model committing more errors when predicting the stock prices for META compared to MRK? Errors can be committed by other factors such as the volatility of stock prices within an industry. To minimize the effect of volatility, we also performed intra-industry along with inter-industry analysis. As a part of future work, we would also like to do causal analysis considering other confounders such as seasonal trends and additional external factors also be done using additional sources such as financial news. 

%The metrics we have chosen perfectly answered the research questions stated in Section \ref{sec:problem}. Instead of relying on just statistical accuracy metrics as the only performance metrics, we believe that one should select the metrics based on the questions one wants to answer. The hypothesis testing approach introduced in \cite{kausik2024rating}, which we have adapted for our work not only helped us answer the research questions but also quantified the biases and perturbation impacts on the outcome of the test systems. Traditional explainability approaches or other statistical approaches take control from human decision-makers, limiting their involvement \cite{miller2023explainable}. \cite{miller2023explainable} argues that hypothesis-driven decision support, though increases human cognitive load, can truly improve AI-assisted decision support. These approaches usually follow the process of abductive reasoning which involves forming hypotheses and judging their likelihood for explaining the systems' behavior to the end-users. Similarly, in our work, though we initially built a causal model based on our intuition, we tested the validity of each causal link by forming hypotheses and verifying their plausibility through the computation of different metrics. 

%Each of the perturbations we used for our analysis has a real-world impact. Some of these could be applied to just numeric data and some can be applied to multi-modal data.  Methods like differential evaluation can be used to find the most impactful variation of a perturbation. But we only aimed to assess whether simple, and sometimes subtle perturbations would have an impact on the outcome of a time-series forecasting model. 

%We proposed a causally grounded empirical framework to study the robustness of FMTS with respect to three input perturbations. We evaluated seven state-of-the-art (some multimodal) FMTS across six prominent stocks spanning three industries. The ratings proposed by our framework accurately assessed the robustness of FMTS and also offer actionable insights for model selection and deployment. Through our experiments, we showed that multi-modal FMTS exhibited greater robustness, while uni-modal FMTS exhibited greater forecasting accuracy. Further, FMTS trained on time series forecasting tasks showed better robustness and forecasting accuracy compared to general-purpose FMTS across diverse settings. The user study we conducted confirmed that our ratings effectively convey the robustness of FMTS to end-users, demonstrating the framework's real-world adoptability.
