\section{Related Work}
With our study, we investigate the interface between RL and perceptual uncertainty estimation. Therefore, we a) discuss uncertainty estimation in RL and b) examine uncertainty estimation in CV and its applications.

\paragraph{Uncertainty Estimation in RL}
When applying RL models in the real world, safety constraints need to be fulfilled \cite{10675394}. Uncertainty estimation or uncertainty aware-models \cite{DBLP:journals/corr/KahnVPAL17,8793611,zhang2022safereinforcementlearningcontrastive,thomas2021safe} can be used to assess safety guarantees. 
This can help to avoid unsafe trajectories \cite{thomas2021safe}, states 
\cite{pmlr-v37-sui15,wachi2018safeexploration} or actions 
\cite{zhang2022safereinforcementlearningcontrastive}. Gaussian processes are used to model unknown functions like the reward function to restrict the exploration to only safe states \cite{pmlr-v37-sui15,wachi2018safeexploration}. 
Additionally, the estimated uncertainty information can be included into the reward \cite{zhang2022safereinforcementlearningcontrastive} or cost function \cite{DBLP:journals/corr/KahnVPAL17}. 
Zhang and Guo propose a risk preventive training method which allows to choose trajectories with a low risk based on an uncertainty estimate for a state-action pair leading to unsafe states \cite{zhang2022safereinforcementlearningcontrastive}. In the approach proposed by Kahn et al.\ the cost function depends on the estimated collision probability which leads to a defensive behavior in unknown environments \cite{DBLP:journals/corr/KahnVPAL17}.
Furthermore, the uncertainty of decisions can be modeled by Bayesian RL techniques like ensembles networks \cite{hoel2020tactical,Zhang2024SafeReinforcmentLearning}.
These approaches aim at finding a safe policy function or value function by performing uncertainty-aware policy optimization, however most of the approaches assume an accurate observation of the environment.
Nevertheless, perception and raw sensor data may be uncertain or error-prone \cite{kendall2017uncertaintiesneedbayesiandeep}.
In contrast to the aforementioned studies, we investigate how the agent's behavior is influenced when it is informed about the presence or absence of uncertainty in the perception.

\paragraph{Uncertainty Estimation in CV}
For scene understanding based on semantic segmentation, several uncertainty estimation (UE) methods were proposed for perception networks \cite{gal2016dropout,efron1987better}. 
Monte-Carlo dropout \cite{gal2016dropout,kendall2017uncertaintiesneedbayesiandeep}, approximating Bayesian inference, is one of the simplest but effective methods for UE, however inefficient due to multiple inferences per image. 
Shen et al.~\cite{shen2021real} propose a distillation method to learn the conditional predictive distribution of a dropout model, thus establishing real time capability. 
The knowledge about the uncertainty can  be used e.g., to detect OOD objects \cite{maag2024pixelwisegradientuncertaintyconvolutional}, detect false positive predictions \cite{9116288} or calibrating confidences \cite{naeini_mahdi_pakdaman_obtaining_2015}. However, UE methods are often limited to one specific application. 
In the study of Kahl et al.~\cite{kahl2024valuesframeworksystematicvalidation}, the authors point out a gap between theory and practice of UE methods. They propose that uncertainty methods should be evaluated on multiple relevant downstream tasks as varied as OOD-detection \cite{Oberdiek_2020_CVPR_Workshops,Cen_2021_ICCV,ancha2024deep}, active learning \cite{colling2020metaboxnewregionbased,tejaswi2019regionbasedal,pmlr-v235-franco24a} and failure detection \cite{9116288,yingda2020synthesizethencompare,chan2019metafusioncontrolledfalsenegativereduction}.
However, all approaches have in common that the evaluation of the UE methods is limited to vision tasks. Its application to RL is sparsely researched.
To our knowledge, the effect of informing the agent about uncertainty in the perception module 
has yet to be addressed.
With our work we address this interface between vision uncertainty and its influence on subsequent actions of an RL agent.