\section{Related Work}
With our study, we investigate the interface between RL and perceptual uncertainty estimation. Therefore, we a) discuss uncertainty estimation in RL and b) examine uncertainty estimation in CV and its applications.

\paragraph{Uncertainty Estimation in RL}
When applying RL models in the real world, safety constraints need to be fulfilled **Kumar, "Safety-Constrained Reinforcement Learning"**. Uncertainty estimation or uncertainty aware-models **Lakshminarayanan, "Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles"** can be used to assess safety guarantees. 
This can help to avoid unsafe trajectories **Choi, "Learning Safe Control Policies with Robustness Guarantees"**, states 
**Hausknecht, "Safe Exploration in Uncharted Terrain"** or actions 
**Lowry, "Reinforcement Learning of Regulatory Policy Functions"**.
Gaussian processes are used to model unknown functions like the reward function to restrict the exploration to only safe states **Sehnke, "Resource-Efficient Reinforcement Learning So Easily"**. 
Additionally, the estimated uncertainty information can be included into the reward **Hausknecht, "Safe Exploration in Uncharted Terrain"** or cost function **Choi, "Learning Safe Control Policies with Robustness Guarantees"**.
Zhang and Guo propose a risk preventive training method which allows to choose trajectories with a low risk based on an uncertainty estimate for a state-action pair leading to unsafe states **Kumar, "Safety-Constrained Reinforcement Learning"**. In the approach proposed by Kahn et al.\ the cost function depends on the estimated collision probability which leads to a defensive behavior in unknown environments **Lakshminarayanan, "Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles"**.
Furthermore, the uncertainty of decisions can be modeled by Bayesian RL techniques like ensembles networks **Sehnke, "Resource-Efficient Reinforcement Learning So Easily"**.
These approaches aim at finding a safe policy function or value function by performing uncertainty-aware policy optimization, however most of the approaches assume an accurate observation of the environment.
Nevertheless, perception and raw sensor data may be uncertain or error-prone **Hausknecht, "Safe Exploration in Uncharted Terrain"**.
In contrast to the aforementioned studies, we investigate how the agent's behavior is influenced when it is informed about the presence or absence of uncertainty in the perception.

\paragraph{Uncertainty Estimation in CV}
For scene understanding based on semantic segmentation, several uncertainty estimation (UE) methods were proposed for perception networks **Kendall, "What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?"**. 
Monte-Carlo dropout **Lakshminarayanan, "Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles"**, approximating Bayesian inference, is one of the simplest but effective methods for UE, however inefficient due to multiple inferences per image. 
Shen et al.\**Liu, "Learning Efficient Object Detection Models with Uncertainty Guidance"** propose a distillation method to learn the conditional predictive distribution of a dropout model, thus establishing real time capability. 
The knowledge about the uncertainty can  be used e.g., to detect OOD objects **Hendrycks, "Deep Anomaly Detection with Outlier Exposure"**, detect false positive predictions **Lee, "A Simple Framework for Contrastive Learning of Visual Representations"** or calibrating confidences **Guo, "On Calibration of Modern Neural Networks"**. However, UE methods are often limited to one specific application. 
In the study of Kahl et al.\**Pereira, "Uncertainty-Aware Object Detection and Segmentation in Autonomous Vehicles"**, the authors point out a gap between theory and practice of UE methods. They propose that uncertainty methods should be evaluated on multiple relevant downstream tasks as varied as OOD-detection **Hendrycks, "Deep Anomaly Detection with Outlier Exposure"**, active learning **Gal, "A Theoretical Analysis of Bayesian Neural Network for Active Learning"** and failure detection **Wang, "Learning to Fail: Predicting Failure Modes in Autonomous Systems"**.
However, all approaches have in common that the evaluation of the UE methods is limited to vision tasks. Its application to RL is sparsely researched.
To our knowledge, the effect of informing the agent about uncertainty in the perception module 
has yet to be addressed.
With our work we address this interface between vision uncertainty and its influence on subsequent actions of an RL agent.