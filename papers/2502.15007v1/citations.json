[
  {
    "index": 0,
    "papers": [
      {
        "key": "ettinger-etal-2016-probing",
        "author": "Ettinger, Allyson  and\nElgohary, Ahmed  and\nResnik, Philip",
        "title": "Probing for semantic evidence of composition by means of simple classification tasks"
      },
      {
        "key": "belinkov-etal-2017-neural",
        "author": "Belinkov, Yonatan  and\nDurrani, Nadir  and\nDalvi, Fahim  and\nSajjad, Hassan  and\nGlass, James",
        "title": "What do Neural Machine Translation Models Learn about Morphology?"
      },
      {
        "key": "conneau-etal-2018-cram",
        "author": "Conneau, Alexis  and\nKruszewski, German  and\nLample, Guillaume  and\nBarrault, Lo{\\\"\\i}c  and\nBaroni, Marco",
        "title": "What you can cram into a single {\\$}{\\&}!{\\#}* vector: Probing sentence embeddings for linguistic properties"
      },
      {
        "key": "belinkov2022probing",
        "author": "Belinkov, Yonatan",
        "title": "Probing classifiers: Promises, shortcomings, and advances"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "calderon2024behalf",
        "author": "Calderon, Nitay and Reichart, Roi",
        "title": "On Behalf of the Stakeholders: Trends in NLP Model Interpretability in the Era of LLMs"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "dai-etal-2022-knowledge",
        "author": "Dai, Damai  and\nDong, Li  and\nHao, Yaru  and\nSui, Zhifang  and\nChang, Baobao  and\nWei, Furu",
        "title": "Knowledge Neurons in Pretrained Transformers"
      },
      {
        "key": "meng2022locating",
        "author": "Kevin Meng and David Bau and Alex Andonian and Yonatan Belinkov",
        "title": "Locating and Editing Factual Associations in {GPT}"
      },
      {
        "key": "olsson2022context",
        "author": "Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Johnston, Scott and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris",
        "title": "In-context Learning and Induction Heads"
      },
      {
        "key": "wang2022interpretabilitywildcircuitindirect",
        "author": "Kevin Wang and Alexandre Variengien and Arthur Conmy and Buck Shlegeris and Jacob Steinhardt",
        "title": "Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small"
      },
      {
        "key": "conmy2023",
        "author": "Conmy, Arthur and Mavor-Parker, Augustine and Lynch, Aengus and Heimersheim, Stefan and Garriga-Alonso, Adri\\`{a}",
        "title": "Towards Automated Circuit Discovery for Mechanistic Interpretability"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "cunningham2023sparseautoencodershighlyinterpretable",
        "author": "Hoagy Cunningham and Aidan Ewart and Logan Riggs and Robert Huben and Lee Sharkey",
        "title": "Sparse Autoencoders Find Highly Interpretable Features in Language Models"
      },
      {
        "key": "yu-etal-2023-characterizing",
        "author": "Yu, Qinan  and\nMerullo, Jack  and\nPavlick, Ellie",
        "title": "Characterizing Mechanisms for Factual Recall in Language Models"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "yao2024knowledgecircuitspretrainedtransformers",
        "author": "Yunzhi Yao and Ningyu Zhang and Zekun Xi and Mengru Wang and Ziwen Xu and Shumin Deng and Huajun Chen",
        "title": "Knowledge Circuits in Pretrained Transformers"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "radford2019language",
        "author": "Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya",
        "title": "Language Models are Unsupervised Multitask Learners"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "zhang2024tinyllama",
        "author": "Peiyuan Zhang and Guangtao Zeng and Tianduo Wang and Wei Lu",
        "title": "TinyLlama: An Open-Source Small Language Model"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "gao2024",
        "author": "Leo Gao and\nTom Dupr{\\'{e}} la Tour and\nHenk Tillman and\nGabriel Goh and\nRajan Troll and\nAlec Radford and\nIlya Sutskever and\nJan Leike and\nJeffrey Wu",
        "title": "Scaling and evaluating sparse autoencoders"
      },
      {
        "key": "templeton2024scaling",
        "author": "Templeton, Adly and Conerly, Tom and Marcus, Jonathan and Lindsey, Jack and Bricken, Trenton and Chen, Brian and Pearce, Adam and Citro, Craig and Ameisen, Emmanuel and Jones, Andy and Cunningham, Hoagy and Turner, Nicholas L and McDougall, Callum and MacDiarmid, Monte and Freeman, C. Daniel and Sumers, Theodore R. and Rees, Edward and Batson, Joshua and Jermyn, Adam and Carter, Shan and Olah, Chris and Henighan, Tom",
        "title": "Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "nostalgebraist2020",
        "author": "Nostalgebraist",
        "title": "{interpreting GPT: the logit lens}"
      },
      {
        "key": "xu-etal-2021-probing",
        "author": "Xu, Hongfei  and\nvan Genabith, Josef  and\nLiu, Qiuhui  and\nXiong, Deyi",
        "title": "Probing Word Translations in the Transformer and Trading Decoder for Encoder Layers"
      },
      {
        "key": "belrose2023eliciting",
        "author": "Nora Belrose and Zach Furman and Logan Smith and Danny Halawi and Igor Ostrovsky and Lev McKinney and Stella Biderman and Jacob Steinhardt",
        "title": "Eliciting Latent Predictions from Transformers with the Tuned Lens"
      },
      {
        "key": "din2023jump",
        "author": "Alexander Yom Din and Taelin Karidi and Leshem Choshen and Mor Geva",
        "title": "Jump to Conclusions: Short-Cutting Transformers With Linear Transformations"
      },
      {
        "key": "razzhigaev-etal-2024-shape",
        "author": "Razzhigaev, Anton  and\nMikhalchuk, Matvey  and\nGoncharova, Elizaveta  and\nOseledets, Ivan  and\nDimitrov, Denis  and\nKuznetsov, Andrey",
        "title": "The Shape of Learning: Anisotropy and Intrinsic Dimensions in Transformer-Based Models"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "nostalgebraist2020",
        "author": "Nostalgebraist",
        "title": "{interpreting GPT: the logit lens}"
      },
      {
        "key": "belrose2023eliciting",
        "author": "Nora Belrose and Zach Furman and Logan Smith and Danny Halawi and Igor Ostrovsky and Lev McKinney and Stella Biderman and Jacob Steinhardt",
        "title": "Eliciting Latent Predictions from Transformers with the Tuned Lens"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "anthropic2021",
        "author": "Nelson Elhage and Neel Nanda and Catherine Olsson and Tom Henighan\u2020 and Nicholas Joseph\u2020 and Ben Mann\u2020 and Amanda Askell and Yuntao Bai and Anna Chen and Tom Conerly and Nova DasSarma and Dawn Drain and Deep Ganguli and Zac Hatfield-Dodds and Danny Hernandez and Andy Jones and Jackson Kernion and Liane Lovitt and Kamal Ndousse and Dario Amodei and Tom Brown and Jack Clark and Jared Kaplan and Sam McCandlish and Chris Olah\u2021",
        "title": "A Mathematical Framework for Transformer Circuits"
      },
      {
        "key": "razzhigaev2024transformersecretlylinear",
        "author": "Anton Razzhigaev and Matvey Mikhalchuk and Elizaveta Goncharova and Nikolai Gerasimenko and Ivan Oseledets and Denis Dimitrov and Andrey Kuznetsov",
        "title": "Your Transformer is Secretly Linear"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "li-etal-2023-sentence",
        "author": "Li, Haoran  and\nXu, Mingshi  and\nSong, Yangqiu",
        "title": "Sentence Embedding Leaks More Information than You Expect: Generative Embedding Inversion Attack to Recover the Whole Sentence"
      },
      {
        "key": "wan2024informationleakageembeddinglarge",
        "author": "Zhipeng Wan and Anda Cheng and Yinggui Wang and Lei Wang",
        "title": "Information Leakage from Embedding in Large Language Models"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "wan2024informationleakageembeddinglarge",
        "author": "Zhipeng Wan and Anda Cheng and Yinggui Wang and Lei Wang",
        "title": "Information Leakage from Embedding in Large Language Models"
      }
    ]
  }
]