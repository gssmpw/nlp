\section{Related works}
\paragraph{Interpretability} There are several significant paradigms for model interpretation, each with its own distinct properties. Probing methods are designed to train classifiers based on hidden representations that are challenged in encoding specific knowledge \cite{ettinger-etal-2016-probing,belinkov-etal-2017-neural,conneau-etal-2018-cram,belinkov2022probing}. While these approaches show whether specific language features are incorporated into LLMs, they do not analyze internal representations during knowledge activation, leaving the model's behavior largely a black box.

In contrast, mechanistic interpretability introduces approaches to explore the inner behavior of models.  \citet{calderon2024behalf} mention that mechanistic interpretability aims to explore the internal representations of deep learning models through the activations of specific neurons and layer connections. A significant branch of research dedicated to examining model responses involves probing changes in behavior resulting from perturbations, noise in embeddings, or masking of network weights \cite{dai-etal-2022-knowledge,meng2022locating,olsson2022context,wang2022interpretabilitywildcircuitindirect,conmy2023}. 

Discovering interpretable features through training sparse autoencoders (SAEs) has become a promising direction in the LLM interpretation \cite{cunningham2023sparseautoencodershighlyinterpretable,yu-etal-2023-characterizing}. Typically, SAEs focus on activations of specific LLM components, such as attention heads or multilayer perceptrons (MLPs). By decomposing model computations into understandable circuits, we can see how information heads, relation heads, and MLPs encode knowledge\cite{yao2024knowledgecircuitspretrainedtransformers}.


While most research has concentrated on the analysis of Small Language Models, such as GPT-2 \cite{radford2019language} and TinyLLAMA \cite{zhang2024tinyllama}, recent work has advanced this area by proposing modifications to improve the scalability and sparsity of autoencoders for larger LLMs, such as GPT-4 or Claude 3 Sonet \cite{gao2024,templeton2024scaling}.

\paragraph{Linearity of LLM hidden states}

The study of the internal structure of transformer-based models has been of great interest among researchers \cite{nostalgebraist2020,xu-etal-2021-probing,belrose2023eliciting,din2023jump,razzhigaev-etal-2024-shape}. Several studies, such as ``Logit Lens''\footnote{\url{https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens}}, have explored projecting representations from the intermediate layers into the vocabulary space by observing their evolution across different layers \cite{nostalgebraist2020,belrose2023eliciting}. Relying on this research, the authors also investigate the complex structure of hidden representations through linearization \cite{anthropic2021,razzhigaev2024transformersecretlylinear}.

\paragraph{Contextualization of LLM hidden states}

One of the areas of research into the internal representations of Transformers is the embeddings contextualization analysis. Recent studies have demonstrated that sentence representations provided by Transformer decoders can contain information about the entire previous context \cite{li-etal-2023-sentence,wan2024informationleakageembeddinglarge}. \citet{wan2024informationleakageembeddinglarge} proposed two initial methods for reconstructing original texts from modelâ€™s hidden states, finding these methods effective for the embeddings from shallow layers but less effective for deeper layers, known as ``Embed Parrot.''


Our work proposes a unified framework for LLM interpretability by exploring properties such as linearity, anisotropy, and intrinsic dimension of hidden representations. We introduce new approaches to assess contextual memory in token representations and analyze intermediate layer contributions to token prediction.