@misc{anthropic2021,
      title={A Mathematical Framework for Transformer Circuits}, 
      author={Nelson Elhage and Neel Nanda and Catherine Olsson and Tom Henighan† and Nicholas Joseph† and Ben Mann† and Amanda Askell and Yuntao Bai and Anna Chen and Tom Conerly and Nova DasSarma and Dawn Drain and Deep Ganguli and Zac Hatfield-Dodds and Danny Hernandez and Andy Jones and Jackson Kernion and Liane Lovitt and Kamal Ndousse and Dario Amodei and Tom Brown and Jack Clark and Jared Kaplan and Sam McCandlish and Chris Olah‡},
      year={2021},
      eprint={https://transformer-circuits.pub/2021/framework/index.html},
}

@inproceedings{belinkov-etal-2017-neural,
    title = "What do Neural Machine Translation Models Learn about Morphology?",
    author = "Belinkov, Yonatan  and
      Durrani, Nadir  and
      Dalvi, Fahim  and
      Sajjad, Hassan  and
      Glass, James",
    editor = "Barzilay, Regina  and
      Kan, Min-Yen",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-1080",
    doi = "10.18653/v1/P17-1080",
    pages = "861--872",
}

@article{belinkov2022probing,
  title={Probing classifiers: Promises, shortcomings, and advances},
  author={Belinkov, Yonatan},
  journal={Computational Linguistics},
  volume={48},
  number={1},
  pages={207--219},
  year={2022},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@misc{belrose2023eliciting,
      title={Eliciting Latent Predictions from Transformers with the Tuned Lens}, 
      author={Nora Belrose and Zach Furman and Logan Smith and Danny Halawi and Igor Ostrovsky and Lev McKinney and Stella Biderman and Jacob Steinhardt},
      year={2023},
      eprint={2303.08112},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{calderon2024behalf,
  title={On Behalf of the Stakeholders: Trends in NLP Model Interpretability in the Era of LLMs},
  author={Calderon, Nitay and Reichart, Roi},
  journal={arXiv preprint arXiv:2407.19200},
  year={2024}
}

@inproceedings{conmy2023,
 author = {Conmy, Arthur and Mavor-Parker, Augustine and Lynch, Aengus and Heimersheim, Stefan and Garriga-Alonso, Adri\`{a}},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {16318--16352},
 publisher = {Curran Associates, Inc.},
 title = {Towards Automated Circuit Discovery for Mechanistic Interpretability},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/34e1dbe95d34d7ebaf99b9bcaeb5b2be-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}

@inproceedings{conneau-etal-2018-cram,
    title = "What you can cram into a single {\$}{\&}!{\#}* vector: Probing sentence embeddings for linguistic properties",
    author = {Conneau, Alexis  and
      Kruszewski, German  and
      Lample, Guillaume  and
      Barrault, Lo{\"\i}c  and
      Baroni, Marco},
    editor = "Gurevych, Iryna  and
      Miyao, Yusuke",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1198",
    doi = "10.18653/v1/P18-1198",
    pages = "2126--2136",
}

@misc{cunningham2023sparseautoencodershighlyinterpretable,
      title={Sparse Autoencoders Find Highly Interpretable Features in Language Models}, 
      author={Hoagy Cunningham and Aidan Ewart and Logan Riggs and Robert Huben and Lee Sharkey},
      year={2023},
      eprint={2309.08600},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2309.08600}, 
}

@inproceedings{dai-etal-2022-knowledge,
    title = "Knowledge Neurons in Pretrained Transformers",
    author = "Dai, Damai  and
      Dong, Li  and
      Hao, Yaru  and
      Sui, Zhifang  and
      Chang, Baobao  and
      Wei, Furu",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.581",
    doi = "10.18653/v1/2022.acl-long.581",
    pages = "8493--8502",
}

@misc{din2023jump,
      title={Jump to Conclusions: Short-Cutting Transformers With Linear Transformations}, 
      author={Alexander Yom Din and Taelin Karidi and Leshem Choshen and Mor Geva},
      year={2023},
      eprint={2303.09435},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{ettinger-etal-2016-probing,
    title = "Probing for semantic evidence of composition by means of simple classification tasks",
    author = "Ettinger, Allyson  and
      Elgohary, Ahmed  and
      Resnik, Philip",
    booktitle = "Proceedings of the 1st Workshop on Evaluating Vector-Space Representations for {NLP}",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W16-2524",
    doi = "10.18653/v1/W16-2524",
    pages = "134--139",
}

@article{gao2024,
  author       = {Leo Gao and
                  Tom Dupr{\'{e}} la Tour and
                  Henk Tillman and
                  Gabriel Goh and
                  Rajan Troll and
                  Alec Radford and
                  Ilya Sutskever and
                  Jan Leike and
                  Jeffrey Wu},
  title        = {Scaling and evaluating sparse autoencoders},
  journal      = {CoRR},
  volume       = {abs/2406.04093},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2406.04093},
  doi          = {10.48550/ARXIV.2406.04093},
  eprinttype    = {arXiv},
  eprint       = {2406.04093},
  timestamp    = {Fri, 05 Jul 2024 16:54:14 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2406-04093.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{li-etal-2023-sentence,
    title = "Sentence Embedding Leaks More Information than You Expect: Generative Embedding Inversion Attack to Recover the Whole Sentence",
    author = "Li, Haoran  and
      Xu, Mingshi  and
      Song, Yangqiu",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.881",
    doi = "10.18653/v1/2023.findings-acl.881",
    pages = "14022--14040",
}

@article{meng2022locating,
  title={Locating and Editing Factual Associations in {GPT}},
  author={Kevin Meng and David Bau and Alex Andonian and Yonatan Belinkov},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2022},
  note={arXiv:2202.05262}
}

@misc{nostalgebraist2020,
  author = {Nostalgebraist},
  title = {{interpreting GPT: the logit lens}},
  year = {2020},
  howpublished = {\url{https://www.alignmentforum.org/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens}}
}

@article{olsson2022context,
   title={In-context Learning and Induction Heads},
   author={Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Johnston, Scott and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
   year={2022},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html}
}

@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}

@inproceedings{razzhigaev-etal-2024-shape,
    title = "The Shape of Learning: Anisotropy and Intrinsic Dimensions in Transformer-Based Models",
    author = "Razzhigaev, Anton  and
      Mikhalchuk, Matvey  and
      Goncharova, Elizaveta  and
      Oseledets, Ivan  and
      Dimitrov, Denis  and
      Kuznetsov, Andrey",
    editor = "Graham, Yvette  and
      Purver, Matthew",
    booktitle = "Findings of the Association for Computational Linguistics: EACL 2024",
    month = mar,
    year = "2024",
    address = "St. Julian{'}s, Malta",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-eacl.58",
    pages = "868--874",
}

@misc{razzhigaev2024transformersecretlylinear,
      title={Your Transformer is Secretly Linear}, 
      author={Anton Razzhigaev and Matvey Mikhalchuk and Elizaveta Goncharova and Nikolai Gerasimenko and Ivan Oseledets and Denis Dimitrov and Andrey Kuznetsov},
      year={2024},
      eprint={2405.12250},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.12250}, 
}

@article{templeton2024scaling,
       title={Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet},
       author={Templeton, Adly and Conerly, Tom and Marcus, Jonathan and Lindsey, Jack and Bricken, Trenton and Chen, Brian and Pearce, Adam and Citro, Craig and Ameisen, Emmanuel and Jones, Andy and Cunningham, Hoagy and Turner, Nicholas L and McDougall, Callum and MacDiarmid, Monte and Freeman, C. Daniel and Sumers, Theodore R. and Rees, Edward and Batson, Joshua and Jermyn, Adam and Carter, Shan and Olah, Chris and Henighan, Tom},
       year={2024},
       journal={Transformer Circuits Thread},
       url={https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html}
    }

@misc{wan2024informationleakageembeddinglarge,
      title={Information Leakage from Embedding in Large Language Models}, 
      author={Zhipeng Wan and Anda Cheng and Yinggui Wang and Lei Wang},
      year={2024},
      eprint={2405.11916},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.11916}, 
}

@misc{wang2022interpretabilitywildcircuitindirect,
      title={Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small}, 
      author={Kevin Wang and Alexandre Variengien and Arthur Conmy and Buck Shlegeris and Jacob Steinhardt},
      year={2022},
      eprint={2211.00593},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2211.00593}, 
}

@inproceedings{xu-etal-2021-probing,
    title = "Probing Word Translations in the Transformer and Trading Decoder for Encoder Layers",
    author = "Xu, Hongfei  and
      van Genabith, Josef  and
      Liu, Qiuhui  and
      Xiong, Deyi",
    editor = "Toutanova, Kristina  and
      Rumshisky, Anna  and
      Zettlemoyer, Luke  and
      Hakkani-Tur, Dilek  and
      Beltagy, Iz  and
      Bethard, Steven  and
      Cotterell, Ryan  and
      Chakraborty, Tanmoy  and
      Zhou, Yichao",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.7",
    doi = "10.18653/v1/2021.naacl-main.7",
    pages = "74--85",
}

@misc{yao2024knowledgecircuitspretrainedtransformers,
      title={Knowledge Circuits in Pretrained Transformers}, 
      author={Yunzhi Yao and Ningyu Zhang and Zekun Xi and Mengru Wang and Ziwen Xu and Shumin Deng and Huajun Chen},
      year={2024},
      eprint={2405.17969},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.17969}, 
}

@inproceedings{yu-etal-2023-characterizing,
    title = "Characterizing Mechanisms for Factual Recall in Language Models",
    author = "Yu, Qinan  and
      Merullo, Jack  and
      Pavlick, Ellie",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.615",
    doi = "10.18653/v1/2023.emnlp-main.615",
    pages = "9924--9959",
}

@misc{zhang2024tinyllama,
      title={TinyLlama: An Open-Source Small Language Model}, 
      author={Peiyuan Zhang and Guangtao Zeng and Tianduo Wang and Wei Lu},
      year={2024},
      eprint={2401.02385},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2401.02385}, 
}

