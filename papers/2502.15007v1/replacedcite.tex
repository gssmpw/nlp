\section{Related works}
\paragraph{Interpretability} There are several significant paradigms for model interpretation, each with its own distinct properties. Probing methods are designed to train classifiers based on hidden representations that are challenged in encoding specific knowledge ____. While these approaches show whether specific language features are incorporated into LLMs, they do not analyze internal representations during knowledge activation, leaving the model's behavior largely a black box.

In contrast, mechanistic interpretability introduces approaches to explore the inner behavior of models.  ____ mention that mechanistic interpretability aims to explore the internal representations of deep learning models through the activations of specific neurons and layer connections. A significant branch of research dedicated to examining model responses involves probing changes in behavior resulting from perturbations, noise in embeddings, or masking of network weights ____. 

Discovering interpretable features through training sparse autoencoders (SAEs) has become a promising direction in the LLM interpretation ____. Typically, SAEs focus on activations of specific LLM components, such as attention heads or multilayer perceptrons (MLPs). By decomposing model computations into understandable circuits, we can see how information heads, relation heads, and MLPs encode knowledge____.


While most research has concentrated on the analysis of Small Language Models, such as GPT-2 ____ and TinyLLAMA ____, recent work has advanced this area by proposing modifications to improve the scalability and sparsity of autoencoders for larger LLMs, such as GPT-4 or Claude 3 Sonet ____.

\paragraph{Linearity of LLM hidden states}

The study of the internal structure of transformer-based models has been of great interest among researchers ____. Several studies, such as ``Logit Lens''\footnote{\url{https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens}}, have explored projecting representations from the intermediate layers into the vocabulary space by observing their evolution across different layers ____. Relying on this research, the authors also investigate the complex structure of hidden representations through linearization ____.

\paragraph{Contextualization of LLM hidden states}

One of the areas of research into the internal representations of Transformers is the embeddings contextualization analysis. Recent studies have demonstrated that sentence representations provided by Transformer decoders can contain information about the entire previous context ____. ____ proposed two initial methods for reconstructing original texts from modelâ€™s hidden states, finding these methods effective for the embeddings from shallow layers but less effective for deeper layers, known as ``Embed Parrot.''


Our work proposes a unified framework for LLM interpretability by exploring properties such as linearity, anisotropy, and intrinsic dimension of hidden representations. We introduce new approaches to assess contextual memory in token representations and analyze intermediate layer contributions to token prediction.