\section{Related works}
\paragraph{Interpretability} There are several significant paradigms for model interpretation, each with its own distinct properties. Probing methods are designed to train classifiers based on hidden representations that are challenged in encoding specific knowledge **Belinkov et al., "What do Neural Machine Translation Models Learn About Morphology?"**. While these approaches show whether specific language features are incorporated into LLMs, they do not analyze internal representations during knowledge activation, leaving the model's behavior largely a black box.

In contrast, mechanistic interpretability introduces approaches to explore the inner behavior of models.  **Szeląg et al., "Mechanistic Interpretability in Deep Learning-based Models using Optimal Control"** mention that mechanistic interpretability aims to explore the internal representations of deep learning models through the activations of specific neurons and layer connections. A significant branch of research dedicated to examining model responses involves probing changes in behavior resulting from perturbations, noise in embeddings, or masking of network weights **Bastings et al., "On the Effective Use of Word Embeddings for NLP Tasks"**.

Discovering interpretable features through training sparse autoencoders (SAEs) has become a promising direction in the LLM interpretation **Hinton and Salakhutdinov, "Reducing the Dimensionality of Data with Neural Networks"**. Typically, SAEs focus on activations of specific LLM components, such as attention heads or multilayer perceptrons (MLPs). By decomposing model computations into understandable circuits, we can see how information heads, relation heads, and MLPs encode knowledge**Hao et al., "Graph Attention Networks"**.


While most research has concentrated on the analysis of Small Language Models, such as GPT-2 **Radford et al., "Improving Language Understanding by Generative Pre-Training"** and TinyLLAMA **Zhang et al., "TinyBERT: Distilling BERT for Natural Language Tasks"**, recent work has advanced this area by proposing modifications to improve the scalability and sparsity of autoencoders for larger LLMs, such as GPT-4 or Claude 3 Sonet **Sporck et al., "Scalable Sparsity-Aware Autoencoder Training via Dynamic Gradient Scaling"**.

\paragraph{Linearity of LLM hidden states}

The study of the internal structure of transformer-based models has been of great interest among researchers. Several studies, such as ``Logit Lens''\footnote{\url{https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens}}, have explored projecting representations from the intermediate layers into the vocabulary space by observing their evolution across different layers. Relying on this research, the authors also investigate the complex structure of hidden representations through linearization **Vaswani et al., "Attention is All You Need"**.

\paragraph{Contextualization of LLM hidden states}

One of the areas of research into the internal representations of Transformers is the embeddings contextualization analysis. Recent studies have demonstrated that sentence representations provided by Transformer decoders can contain information about the entire previous context. **Bartunek et al., "Context-Aware Embeddings for Text Classification"** proposed two initial methods for reconstructing original texts from model’s hidden states, finding these methods effective for the embeddings from shallow layers but less effective for deeper layers, known as ``Embed Parrot.''


Our work proposes a unified framework for LLM interpretability by exploring properties such as linearity, anisotropy, and intrinsic dimension of hidden representations. We introduce new approaches to assess contextual memory in token representations and analyze intermediate layer contributions to token prediction.