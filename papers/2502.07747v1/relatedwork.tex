\section{Related Works}
% Recent research in large language models (LLMs) has focused on improving understanding, reasoning, and deductive capabilities. 
Foundational LLMs, such as GPT-2 and GPT-3, demonstrated strong performance across various text-based tasks, though they initially struggled with complex, multi-step reasoning \citep{radford2019language, brown2020language}. 


% \subsection{LLM Reasoning and Deductive Capabilities}

% Advancements in reasoning techniques, such as Chain-of-Thought (CoT) prompting, have significantly improved LLM performance on complex reasoning tasks. 
CoT prompting, which encourages models to break down problems into logical steps, has been shown to enhance accuracy and coherence in deductive tasks \citep{wei2022chain}. 
Additional methods, like Self-Reflection prompting, further improve reliability by having models verify and refine their responses, leading to more thoughtful answers \citep{shinn2024reflexion, madaan2024self}.

% \subsection{Narrative Understanding and Story Comprehension}

LLMs' abilities to handle narrative reasoning—tracking characters, plot progression, and thematic elements—have also been a focal area of AI research. 
Studies have shown that while models can generate coherent stories, they often struggle with consistency over long narratives \citep{ammanabrolu2021automated, rashkin2020plotmachines}. 
Enhanced approaches have aimed to improve narrative coherence, though challenges remain, particularly in maintaining character roles and logical plot flow.

% \subsection{Benchmarks for Deductive Reasoning in LLMs}

Several benchmarks assess LLMs’ reasoning and comprehension, including MMLU, HELM, and Big-Bench (BBH), which evaluate performance across diverse tasks \citep{hendrycks2020measuring, liang2022holistic, srivastava2022beyond}. 
These benchmarks incorporate tasks requiring reasoning and narrative comprehension, though few focus specifically on deductive reasoning within mystery narratives.

% This paper builds on these foundations by introducing a dataset uniquely tailored to assess LLMs’ narrative reasoning and deductive abilities within mystery contexts, providing a focused benchmark for evaluating inference in complex narrative scenarios.


%