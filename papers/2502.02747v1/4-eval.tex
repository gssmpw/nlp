\section{Evaluation}
\label{sec:Evaluation}

We evaluate \sys from the following aspects:
First, we perform a large-scale comparison of \sys with both SOTA open-source and closed-source methods on the SWE-Bench-Lite and SWE-Bench-Verified patching benchmark~\cite{jimenez2023swe}, showcasing \sys's ability to balance patching accuracy and cost-efficiency.
Second, we conduct a stability analysis on \sys and \openhands, demonstrating~\sys{}'s human-based planning is more stable than the SOTA agent-based planning. 
Third, we conduct an ablation study to quantify the contribution of each component to \sys's overall performance.
Finally, we show \sys's compatibility and performance on different models, including \gpt~\cite{GPT-4o}, \claude~\cite{anthropic_claude}, and a reasoning model \oo~\cite{GPT-o3}. 
We failed to integrate \deepseek~\cite{DeepSeek-r1} due to the problems with their APIs (See~\cref{appx:exp4}).
% In the following, we specify the setup and design of each experiment and discuss their results.

\subsection{\sys vs. Baselines on SWE-Bench}
\label{exp:comparison}

\noindent\textbf{Setup and design.}
We utilize the \textit{SWE-Bench}~\cite{jimenez2023swe} benchmark, where each instance corresponds to an issue in a GitHub repository written in \texttt{Python}.
Specifically, we consider two subsets: \textit{SWE-Bench-Lite}~\cite{SWE-Bench-Lite}, consisting of 300 instances, and \textit{SWE-Bench-Verified}~\cite{SWE-Bench-Verified}, comprising 500 instances that have been verified by humans to be resolvable.

We mainly compare \sys with three SOTA open-source methods: two agent-based planning methods \openhands~\cite{wang2024openhands} and \autocode~\cite{zhang2024autocoderover}, and a human-based planning method \agentless~\cite{xia2024agentless}. 
We also compare it with two closed-source methods that have cost reported: Globant Code Fixer Agent~\cite{Globant_Code_Fixer_Agent} (\globant for short) and CodeStory Midwit Agent~\cite{CodeStory_Midwit_Agent} (\codestory for short).
In~\cref{appx:exp1}, we include a more comprehensive comparison of \sys against 29 other tools, showing our positions on the SWE-Bench leaderboard.
Given our goal of addressing stability and cost together with the resolved rate, comparing closed-source methods that have a higher resolved rate but without cost is not our focus.
Most of these methods follow agent-based planning that may cost way more than ours.
For example, \codestory mentions that it costs them \$10,000 to achieve 62.2\% on the SWE-Bench-Verified benchmark~\cite{SWE-Bench-Verified}, whereas \sys achieves a 53.60\% with less than \$500 (20$\times$ cheaper).
In addition, as shown in~\cref{exp:Stability}, agent-based planning is less stable than human-based planning.

To align with most methods, we use the \claude model as the LLM in \sys.
~\cref{appx:implement} shows our implementation details.
We report two metrics \textit{Resolved Rate} (\%): the percentage of resolved instances in the benchmark,\footnote{An instance/issue is resolved means the patch fixes the issue while passing all hidden functionality tests.} and \textit{Average Cost} (\$): the average model API cost of running the tool on each instance. 
For the baselines, we retrieve their performance from their submission logs on the SWE-Bench and their papers and official blogs. 

\noindent\textbf{Results.}
\cref{tab:swe_bench_leaderboards} shows the performance of \sys and selected baselines on two subsets of the SWE-Bench benchmark.
Although, on both benchmarks, the closed-source methods achieve the highest performance, their internal design and methodology are not publicly available and we cannot assess their stability.
Notably, the cost of \codestory is 20$\times$ higher than \sys.
The cost of \globant is more comparable to \sys on SWE-Bench-Lite, but we cannot assess their performance and cost on SWE-Bench-Verified. 
Among open-source methods, \openhands achieves higher resolved rates than the human-based planning tool, \agentless, on both benchmarks. 
However, \openhands has a higher cost than \agentless, i.e., around $91.07\%$ more expensive on SWE-Bench Lite when using the same \claude. 
This result validates our discussion in Section~\ref{subsec:tech_overview}, human-based planning is more cost-efficient than agent-based planning, and agent-based planning has the potential to achieve higher optimal resolved rates.

In comparison, \sys demonstrates a clear advantage in balancing resolved rate and cost. 
On SWE-Bench Lite, it resolves 45.33\% (136/300) of the issues, outperforming all open-source methods with a low cost of \$0.97 per instance. 
Similarly, on SWE-Bench Verified, \sys achieves a resolved rate of 53.60\% (268/500), surpassing all open-source methods while maintaining the same cost efficiency of \$0.99 per instance.
These results highlight the efficacy and cost-efficiency of \sys.
% ~\cref{appx:case_study} provides a case study on the instances in which we succeed and fail. 


\subsection{\sys vs \openhands in Stability}
\label{exp:Stability}

\begin{figure}
    \centering
    \includegraphics[width=85mm]{Figures/evaluation/exp2_bar.pdf}
    \vspace{-4mm}
    \caption{\sys vs. \openhands in the resolved rate (bars) and the total cost (lines) on 45 instances from SWE-Bench-Lite.}
    \label{fig:stability_bar}
    \vspace{-4mm}
\end{figure}

\noindent\textbf{Setup and design.}
We compare the stability of \sys and \openhands, the SOTA open-source agent-based planning tool.
We find $102$ common instances resolved by \sys and \openhands in the SWE-Bench-Lite benchmark and randomly select a subset of $45$.
We run \sys and \openhands on these instances three times with \gpt model and different \texttt{Python} random seeds.
We report and compare their resolved rate and total cost in each run. 

\noindent\textbf{Results.}
\cref{fig:stability_bar} shows the resolved rate and costs of \sys and \openhands across three runs.
As shown in the figure, \sys consistently resolved more instances, achieving 30, 32, and 35 resolved instances in the three runs, with a standard deviation of 2.52.
In comparison, \openhands resolved only 15, 20, and 21 instances, with a higher standard deviation of 3.21.
The lower standard deviation of \sys demonstrates its stability, which further validates our discussion about human-based planning vs. agent-based planning in~\cref{subsec:tech_overview}.
Additionally, \sys demonstrated a clear advantage in terms of cost efficiency, with costs of \$8.72, \$14.81, and \$14.42 for the three runs, resulting in an average of \$12.65 per run. 
This is substantially lower than \openhands, which incurred costs of \$32.78, \$33.31, and \$34.97, with an average of \$33.69 per run. 
These results further highlight \sys's ability to achieve higher resolved rates with greater stability and at a lower cost.

\subsection{Ablation Studies}
\label{exp:ablation}

\begin{figure}
    \centering
    \includegraphics[width=85mm]{Figures/evaluation/exp3_bar.pdf}
    \vspace{-2mm}
    \caption{Ablation study results on the SWE-Bench-Lite benchmark. \ding{182}$\sim$\ding{185} refers to \textit{Base Local+Gen}, \textit{Our Local+Gen}, \textit{Our Local+Gen+PoC}, and \textit{Our Local+Gen+Val}, respectively.}
    \label{fig:abliation_bar}
    \vspace{-4mm}
\end{figure}

\noindent\textbf{Setup and design.}
We conduct a detailed ablation study to investigate the efficacy of key designs in \sys.
We use the full SWE-Bench-Lite benchmark and the \claude model for all variations of our method.
Specifically, we consider the following four variations:
\noindent\ding{182}\textit{Base Local+Gen}: We combine simple localization without providing the LLM with tools or a review step, along with simple generation without the two-phase design (\cref{fig:overview}).
We choose the final patch by majority voting.
\noindent\ding{183}\textit{Our Local+Gen}: We combine \sys's localization and generation components together with majority voting for final patch selection. 
Comparing \ding{182} with \ding{183} can assess the effectiveness of our proposed techniques for localization and generation. 
\noindent\ding{184}\textit{Our Local+Gen+PoC}: We further add our validation component to \ding{183} but with only the PoC tests (the validation strategy employed by most existing tools).
Comparing \ding{183} with \ding{184} can assess the effectiveness of having PoC validation instead of simple majority voting. 
\noindent\ding{185}\textit{Our Local+Gen+Val}: We add the full validation component, comparing \ding{184} with \ding{185} can assess the efficacy of having functionality tests in validation.
Finally, comparing \ding{185} with \sys can assess the importance of having an additional refinement component. 

\noindent\textbf{Results.}
\cref{fig:abliation_bar} shows the resolved rates across different variations and our final method. 
By incrementally building upon the core functionalities of \sys, we evaluate the contributions of individual components to the overall patching performance.

\underline{Localization and generation.}
First, we can observe that \ding{182} with the simple localization and generation only get a resolved rate of 32.7\% (98/300). 
In contrast, \ding{183} with our improved localization and generation increases the resolved rate to 38.7\% (116/300).
This result first confirms the challenges of simple localization and generation designs discussed in~\cref{subsec:localization} and~\cref{subsec:generation}, as they prevent \ding{182} from achieving a better performance.
More importantly, it validates the effectiveness of our designs in adding tools and a review step in localization and the two-step procedure (i.e., planning and generation) in the generation. 

\underline{PoC and functionality validation.}
\ding{184} with our localization and generation as well as PoC validation unexpectedly lowers the resolved rate to 37.00\% (111/300). 
This result suggests that relying solely on PoC validation may resolve the targeted issue while introducing new functional issues. 
As such, when functionality tests are added, \ding{185} significantly improves the resolved rate to 41.67\% (125/300). 
This result shows that functionality tests play a crucial role in identifying and filtering out the patches that fix the target issues but break the original functionalities of the codebase.
As mentioned above, a patch must pass all hidden functionality tests to be marked as a success; having functionality tests is important to filter out false positives. 

\underline{Refinement.}
Finally, adding our refinement component on top of \ding{185} improves the resolved rate from 41.67\% to 45.33\%.
The result demonstrates the effectiveness of our refinement design. 
It also justifies our claim in~\cref{subsec:refinement} that generating new patches from scratch when the current trial fails is less effective than refining the partially correct patches based on the validation feedback. 

\subsection{\sys on Different Models}
\label{subsec:Model_test}

% \begin{table}[t]
% \centering
% \caption{\sys with different choices of LLMs on 100 cases from SWE-Bench-Lite.}
% \label{tab:model_comparison}
% %\resizebox{\textwidth/2}{!}{
% \begin{tabular}{rc}
% \toprule
% \textbf{LLM Model} & \textbf{Resolved Rate (\%)}  \\
% \midrule
% \gpt & xxxx (x.00\%)  \\
% \oo & 43 (43.00\%) \\
% \claude & 39 (39.00\%)  \\
% \deepseek & xxxx (x.00\%)  \\
% \bottomrule
% \end{tabular}
% %}
% \end{table}

\noindent\textbf{Setup and design.}
To demonstrate the compatibility of \sys to different LLMs, we conduct an experiment that integrates \sys with three SOTA LLMs: two general models \gpt and \claude, and one reasoning model: \oo. 
We select a subset of 100 instances from the SWE-Bench-Lite benchmark; all these 100 instances have been successfully resolved by at least one method ranked Top-10 on the SWE-Bench leaderboard. 
We run \sys with the selected models on these instances and report the final resolved rate. 
We keep all other components the same and only change the model to show the impacts of the different models.

% We record the resolution rate (percentage of successfully resolved cases) for each LLM, comparing their performance in terms of patch accuracy and consistency. 
% By isolating the model variable, this setup allows us to quantify the contribution of each LLM to \sysâ€™s overall efficacy and assess their suitability for automated code patching tasks.

% verified result: root@8bb690ee5f0b:/opt/PatchingAgent# ls results_final_verified/
% lite result: root@4178337f2502:/opt/PatchingAgent/results_final_lite 

\noindent\textbf{Results.}
The resolved rate of \sys with different models are: \gpt: 19.00\%; \claude: 39.00\%, and \oo: 43.00\%.
\oo achieves the highest resolved rate, indicating having inference-phase reasoning capabilities is helpful not only for general math and coding tasks but also for the specialized patching task.
Note that although we cannot directly compare with the results reported from official reports~\cite{Claude_SWE_report,o1_SWE_report,o3_SWE_report}, as they conduct their testing on the SWE-Bench-Verified benchmark. 
However, they follow the same trend: \oo > \claude > \gpt.
It is also worth noting that \sys with \claude on the SWE-Bench-Verified benchmark reports a higher resolved rate than the official report from \claude and OpenAI-O1 model.
Although the full o3 reports a resolved rate of 71.7\%, it do not disclose any details about the system design, cost, and stability. 
Overall, this experiment demonstrates the compatibility of \sys to different models as well as the efficacy of having a reasoning model in \sys.


