% Additional technical details 
\section{Additional Technical Details}
\label{appx:tech}

\noindent\textbf{Tools provided in localization.} 
For file-level localization, we provide LLM with three tools, which are \textit{search\_func\_def}, \textit{search\_class\_def}, and \textit{search\_string}.
\textit{search\_func\_def} and \textit{search\_class\_def} take the function or class name as input and output the file containing the corresponding function or class definition based on exact matching. 
\textit{search\_string} takes a string as input and returns the file that contains this string the most times. 
If no file is found, we perform a fuzzy match with a decreasing similarity threshold until a match is found or timeout. 
This is because the string searched by the LLM, often an error message in $D_i$, is typically a formatted string in the code and may not exactly match the one in $D_i$.
% why this set of tools
We chose to provide these three tools because they could cover most of the information provided in the issue descriptions.

Note that the use of these tools is predefined and incorporated into the file-level prompt at the start of the localization process, rather than being determined dynamically by the LLM. 
This ensures that our approach remains firmly rooted in human-based planning.

\noindent\textbf{Additional PoC information.} 
We also attempt to identify the issue-introducing commit by finding the first historical commit where the PoC triggers the issue and its preceding commit. 
This is achieved by using a binary search to find the latest commit whose PoC output matches the current commit.
Then, we prompt LLM to analyze the PoC output to determine if the preceding commit produces expected results or triggers unrelated errors. 
If it is the former case, we consider we have found the issue-introducing commit. We extract the code difference between the issue-introducing commit and its previous commit. 
This code difference provides valuable insights into how the bug was introduced, which is highly beneficial for both localization and generation. 
In~\cref{exp:comparison}, \sys successfully identified the issue-introducing commit in 18 out of 300 instances on the SWE-Bench-Lite benchmark, among which 13 were resolved, achieving a resolved rate of 72.22\%.
In the SWE-Bench-Verified benchmark, the issue-introducing commit was found in 39 instances, with 24 successfully resolved, achieving a resolved rate of 61.53\%. These high resolved rates further demonstrate the effectiveness of the issue-introducing commit.

\noindent\textbf{Ranking criteria.}
We design our ranking criteria to prioritize the PoC test over benign functionality tests. 
The rank of a patch $p$ is defined as 

\begin{equation}
    \begin{aligned}
      RK_{p} = \indicator(\text{PoC\_failed}) + \frac{num_\text{failed\_func\_test}}{num_\text{executed\_func\_test}}  \, ,
    \end{aligned}
\end{equation}
where $RK_{p}$ is the rank of the patch $p$, $\indicator(\text{PoC\_failed})$ is an indicator function determined by whether the PoC fails, $num_\text{failed\_func\_test}$ is the number of failed functionality tests, and $num_\text{executed\_func\_test}$ is the number of executed functionality tests.
We rank the patches based on the reverse order of $RK_{p}$ (i.e., the lower the $RK_{p}$, the higher the ranking).
If multiple patches have the same rank, we leverage an LLM to determine which one is the best.

\section{Implementation}
\label{appx:implement}
% Justify all the hyper-parameters
\noindent\textbf{Reproduction.} 
For reproduction, we set the iteration limit as 7 for the PoC generation. 
We generate at most one PoC for each instance. 
If a PoC is successfully obtained, we leverage the Python coverage package to get the files that are covered during the execution of the PoC. 
We provide LLM with $\mathcal{R}$'s directory tree structure and the issue description $D_i$, directly prompting LLM to retrieve three existing test files as benign functionality tests. 

\noindent\textbf{Localization.}
In localization, we perform a hierarchical workflow, including file level, class and function level and line level localization, followed by a review step.
At the file level, \sys constructs a tree-structured repository representation, filtered based on the PoC coverage, retaining only the files executed during PoC execution. 
The issue description, the repository representation, and the set of tools described in~\cref{appx:tech} are then provided to the LLM, prompting it to return the top 5 files most relevant to the issue. 
To achieve self-consistency~\cite{ahmed2023better}, we perform file-level localization 4 times and perform majority voting to get the top 5 files. These files are inputs of the class and function-level localization.
At the class and function level, the LLM is provided with the signatures and comments of classes and functions extracted from the retrieved files and is prompted to identify functions and classes likely related to the issue. The number of functions and classes returned by LLM is not limited. 
At the line level, we provide the complete source code of the identified classes and functions to the LLM and prompt it to localize to specific lines. 
The class and function-level localization and the line-level localization are both performed 4 times, with the results of each step merged.
The localized lines, along with the surrounding code within a ±15-line range, are considered the root cause that will be provided to generation.
At the end, we prompt LLM to perform review. If the root cause is less than 150 lines of code, we prompt LLM to retrieve more code snippets related to the root cause. Otherwise, we prompt LLM to check whether the current root cause is correct and fix it if LLM determines that it is incorrect.

\noindent\textbf{Generation.}
For the experiments in~\cref{sec:Evaluation}, we set $N_{\text{max}}$ as 12, and $N$ as 4. 
Thus, \sys generates 4 plans and 4 patches in a single batch and can generate up to 12 patches for one instance.
We constrain the maximum number of steps in a plan to 3.
For the first batch, we use the prompts as specified in~\cref{appx:prompts}, generating one patch for the comprehensive-patch prompt, one for the minimal-patch prompt and two patches for the standard-patch prompt.
For the following batches, we only use the standard-patch prompt. This is done to facilitate caching and reduce costs. 
Following previous work~\cite{Aider}, we prompt LLM to generate patch in Search/Replace edit format and transfer them to git diff format by ourselves. For each edit generated for a step of a plan, we utilize Python's ast library for syntax check and Flake8 for lint check. If LLM cannot generate an edit that passes both the syntax check and the lint check, we skip generation for the current step.

\noindent\textbf{Validation.} For the PoC test, we run the PoC, collect the outputs and provide it to LLM, along with $D_i$ and the PoC code, and prompt LLM to judge whether the issue is fixed. For functionality tests, we implement a parser that extracts the function names of the failed test from the test output. We assign each patch a rank as specified in~\cref{appx:tech}.

\noindent\textbf{Refinement.} For the refinement, if the current batch of patch pass no new functionality tests, we rerun localization in the next batch. For the file level localization, we force the LLM to return one file that is not within the previous localized files. For the function and class level localization and the line level localization, we force LLM to return code that is not within the previous localization results. If there is still no patch that passes any new test in the next batch, we discard the newly localized results and restore the original localization results before rerunning localization in the following batches.

\section{Additional Experiments and Results}
\label{appx:result}

\begin{table}[t]
\centering
\caption{SWE-Bench-Verified leaderboard excerpts from tools with more than 50\% resolved rate}
\label{tab:swe_bench_verified_leaderboards}
%\resizebox{0.5\textwidth}{!}{
\begin{tabular}{r|c|c}
\Xhline{1.0pt}
Tool & LLM & \%Resolved \\
\hline Blackbox Ai Agent & NA & $314(62.80 \%)$ \\
\hline CodeStory Midwit Agent + swe-search & NA & $311(62.20 \%)$ \\
\hline Learn-by-interact & NA & $301(60.20 \%)$ \\
\hline devlo & NA & $291(58.20 \%)$ \\
\hline Emergent E1 & NA & $286(57.20 \%)$ \\
\hline Gru & \claude & $285(57.00 \%)$ \\
\hline EPAM AI/Run Developer Agent & NA & $277(55.40 \%)$ \\
\hline Amazon Q Developer Agent & NA & $275(55.00 \%)$ \\
\hline \cellcolor[HTML]{E3E8FF} PatchPilot & \cellcolor[HTML]{E3E8FF} \claude & \cellcolor[HTML]{E3E8FF} 268 (53.60\%) \\
\hline OpenHands + CodeAct v2.1 e & \claude & $265(53.00 \%)$ \\
\hline Google Jules & Gemini 2.0 F & $261(52.20 \%)$ \\
\hline Engine Labs & NA & $259(51.80 \%)$ \\
\hline Agentless & \claude & $104(50.80 \%)$ \\
\hline Solver & NA & $100(50.00 \%)$ \\
\hline Bytedance MarsCode Agent & NA & $100(50.00 \%)$ \\
\Xhline{1.0pt}
\end{tabular}
%}
\end{table}

\begin{table*}[t]
\caption{SWE-Bench-Lite leaderboard excerpts}
\label{tab:swe_bench_lite_leaderboards}
\centering
%\resizebox{1\textwidth}{!}{
\begin{tabular}{r|c|c|c}
\Xhline{1.0pt} Tool & LLM & \%Resolved & \$Avg. Cost \\
\hline Blackbox Ai Agent & NA & 147 (49.00\%) & - \\
\hline Gru & NA & 146 (48.67\%) & - \\
\hline Globant Code Fixer Agent & NA & 145 (48.33\%) & <\$1.00 \\
\hline devlo & NA & 142 (47.33\%) & - \\
\hline \cellcolor[HTML]{E3E8FF} PatchPilot & \cellcolor[HTML]{E3E8FF} \claude & \cellcolor[HTML]{E3E8FF} 136 (45.33\%) & \cellcolor[HTML]{E3E8FF} \$0.97 \\
\hline Kodu-v1 & \claude & 134 (44.67\%) & - \\
\hline OpenHands+CodeAct v2.1 & \claude & 126 (41.67\%) & \$1.33 \\
\hline Composio SWE-Kit & NA & 124 (41.00\%) & - \\
\hline \multirow[t]{2}{*}{Agentless} & \claude & 123 (40.67\%) & - \\
 & \gpt & 96(32.00 \%) & \$0.70 \\
\hline Bytedance MarsCode & NA & 118 (39.33\%) & - \\
\hline \multirow[t]{2}{*}{Moatless} & \claude & 115 (38.33\%) & \$0.30 \\
 & \gpt & 74(24.67 \%) & \$0.13 \\
\hline Honeycomb & NA & 115 (38.33\%) & - \\
\hline AppMap Navie v2 & NA & 108 (36.00\%) & - \\
\hline Isoform & NA & 105 (35.00\%) & - \\
\hline SuperCoder2.0 & NA & 102 (34.00\%  & - \\
\hline Alibaba Lingma Agent& \claude + \claude & 99 (33.00\%) & - \\
\hline CodeShellTester & \gpt & 94(31.33 \%) & - \\
\hline Amazon Q Developer-v2 & NA & 89 (29.67\%) & - \\
\hline SpecRover & \gpt + \claude & 93 (31.00\%) & \$0.65 \\
\hline CodeR & \gptfour & 85 (28.33\%) & \$3.34 \\
\hline SIMA & \gpt & 83 (27.67\%) & \$0.82 \\
\hline MASAI & NA & 82 (27.33 \%) & - \\
\hline IBM Research Agent-101 & NA & 80(26.67 \%) & - \\
\hline Aider & \gpt + \claude & 79(26.33 \%) & - \\
\hline IBM AI Agent SWE-1.0 & NA & 71(23.67 \%) & - \\
\hline Amazon Q Developer & NA & 61 (20.33\%) & - \\
\hline AutoCodeRover-v2 & \gpt & 92 (30.67\%) & - \\
\hline RepoGraph & \gpt & 89 (29.67\%) & - \\
\hline Openhands+CodeAct v1.8 & \claude & 80(26.67 \%) & \$1.14 \\
\hline \multirow[t]{3}{*}{SWE-agent} & \claude & 69(23.00 \%) & \$1.62 \\
 & \gpt & 55 (18.33\%) & \$2.53 \\
 & \gptfour & 54 (18.00\%) & \$2.51 \\
% \hline AutoCodeRover & \claude & 57 (19.00\%) & \$0.45 \\
% \hline \multirow[t]{4}{*}{RAG [101]} & Claude 3 Opus & 13(4.33\%) & \$0.25 \\
%  & \claude & 8 (2.67 \%) & \$0.13 \\
%  & Claude-2 & 9 (3.00\%) & - \\
%  & GPT-3.5 & 1 (0.33\%) & - \\
% \hline PatchPilot & GPT-o3-mini &  &  \\
\Xhline{1.0pt}
\end{tabular}
%}
\end{table*}


\begin{wrapfigure}{r}{0.4\textwidth} 
    \centering
    \vspace{-20mm}
    \includegraphics[width=\linewidth]{Figures/evaluation/exp2_venn.pdf} 
    \vspace{-2mm}
    \caption{Venn of resolved instances in rounds}
    \label{fig:stability_venn}
     \vspace{-5mm}
\end{wrapfigure}

\subsection{More Comparison on SWE-Bench.}
\label{appx:exp1}
~\cref{tab:swe_bench_lite_leaderboards} and~\cref{tab:swe_bench_verified_leaderboards} are about the current results on the SWE-Bench Lite/Verified, which show that \sys has the highest performance among open-source tools and is competitive with closed-source tools in terms of resolved rate and cost.



\subsection{More Analysis on Stablility Test.}
\label{appx:exp2}
Next, we go deep to analyze the resolved instances in each run in~\cref{exp:Stability}. 
\cref{fig:stability_venn} provides a Venn diagram comparison of the intersection and union of resolved instances across the three runs for both tools. 
\sys{} achieved a total intersection of 25 instances and a union of 38 instances, yielding a higher intersection-to-union ratio of 0.66. 
In contrast, \openhands had a total intersection of 12 instances and a union of 25 instances, with a lower intersection-to-union ratio of 0.48. 
The larger and more stable intersection instances for PatchPilot indicate its higher consistency in resolving instances across different runs. 
This stability is further emphasized by the Venn diagram, which visually demonstrates PatchPilot's ability to consistently resolve overlapping instances, underscoring its reliability and effectiveness.


\subsection{Failed Attempts with \deepseek.}
\label{appx:exp4}
During our experiments with four platforms offering \deepseek model services (\emph{DeepSeek}~\cite{DeepSeek-r1}, \emph{together.ai}~\cite{together.ai}, \emph{firework.ai}~\cite{fireworks.ai}, and \emph{deepinfra}~\cite{deepinfra}), we encountered multiple barriers that prevented us from completing the necessary tests. 
Specifically, DeepSeek’s own platform was affected by a DDoS attack, causing repeated service disruptions. Meanwhile, both together.ai and firework.ai exhibited severe response delays that rendered our experiments infeasible. 
In addition, \emph{deepinfra} restricted the permissible context length for \deepseek to 16k tokens, falling short of our PatchPilot pipeline’s requirements. 
Consequently, despite our attempts, we were unable to obtain experimental results for \deepseek under these conditions.



\subsection{Case Study}
\label{appx:case_study}

\begin{lstlisting}[
                    language=Python, 
                    caption={Diff of the issue-introducing commit of django-12284.}, 
                    label={lst:patch_code_bug},
                  ]
             if not getattr(cls, self.attname, None):
                 setattr(cls, self.attname, self.descriptor_class(self))
         if self.choices is not None:
-            setattr(cls, 'get_%s_display' % self.name,
-                    partialmethod(cls._get_FIELD_display, field=self))
+            if not hasattr(cls, 'get_%s_display' % self.name):
+                setattr(
+                    cls,
+                    'get_%s_display' % self.name,
+                    partialmethod(cls._get_FIELD_display, field=self),
+                )
\end{lstlisting}

\begin{lstlisting}[
                    language=Python, 
                    caption={Our best patch in the first batch.}, 
                    label={lst:patch_code_before},
                  ]
             if not getattr(cls, self.attname, None):
                 setattr(cls, self.attname, self.descriptor_class(self))
         if self.choices is not None:
-            if not hasattr(cls, 'get_%s_display' % self.name):
-                setattr(
-                    cls,
-                    'get_%s_display' % self.name,
-                    partialmethod(cls._get_FIELD_display, field=self),
-                )
+            setattr(
+                cls,
+                'get_%s_display' % self.name,
+                partialmethod(cls._get_FIELD_display, field=self),
+            )

\end{lstlisting}

\begin{lstlisting}[
                    language=Python, 
                    caption={Our final patch for django-12284.}, 
                    label={lst:patch_code_final},
                  ]
             if not getattr(cls, self.attname, None):
                 setattr(cls, self.attname, self.descriptor_class(self))
         if self.choices is not None:
-            if not hasattr(cls, 'get_%s_display' % self.name):
+            method_name = 'get_%s_display' % self.name
+            if method_name not in cls.__dict__:
                 setattr(
                     cls,
-                    'get_%s_display' % self.name,
+                    method_name,
                     partialmethod(cls._get_FIELD_display, field=self),
                 )

\end{lstlisting}

\begin{lstlisting}[
                    language=Python, 
                    caption={Golden patch for django-12284.}, 
                    label={lst:goleden_patch},
                  ]
            if not getattr(cls, self.attname, None):
                setattr(cls, self.attname, self.descriptor_class(self))
        if self.choices is not None: 
-           if not hasattr(cls, 'get_%s_display' % self.name):
+           if 'get_%s_display' % self.name not in cls.__dict__:
                setattr(
                    cls,
                    'get_%s_display' % self.name,
                    partialmethod(cls._get_FIELD_display, field=self),
                )
\end{lstlisting}

\noindent\textbf{Successful Cases.}
Listing~\ref{lst:patch_code_bug}--~\ref{lst:goleden_patch} shows an example issue that was resolved by \sys but rarely resolved by other methods in~\cref{tab:swe_bench_lite_leaderboards}. 
Among the top 10 methods, only Blackbox AI Agent successfully solved this issue besides our approach.

\underline{Issue details:} In Django, when a model field defines \textit{choices}, the \textit{get\_xxx\_display} method returns the label for a field's value. 
However, if a subclass overrides the \textit{choices}, the \textit{get\_xxx\_display} method may not work correctly for the new choices. 
This happens because Django reuses the method from the parent class instead of creating a new one for the subclass, leading to incorrect results for the updated choices. 
The patch solution should ensure that Django generates a new \textit{get\_xxx\_display} method specifically for the subclass to properly handle the overridden choices.

\underline{Findings:} In our study of processing this issue, we discovered that \sys’s two core features are critical to resolving the issue: 
\textit{(1) pinpointing the issue-introducing commit} to accurately localize the root cause and gain insight into the critical logic modifications directly related to the issue.
and \textit{(2) leveraging feedback from failed tests} to iteratively refine the patch until all tests are passed.

Listing~\ref{lst:patch_code_bug} shows the diff of the issue-introducing commit, which closely matches the code area modified by the golden patch in Listing~\ref{lst:goleden_patch} and clearly shows the logical changes that led to the issue.

In the first batch of patching, the best patch generated by \sys is shown in Listing ~\ref{lst:patch_code_before}. It patched the buggy code by directly reversely applying the diff in the Listing~\ref{lst:patch_code_bug}.
This patch successfully fixes the buggy behavior; however, original functionalities are also affected, leading to a failed functionality test.

In the second batch, \sys retrieves the output of the failed functionality test and its code, performs refinement based on the patch in Listing~\ref{lst:patch_code_before}. \sys successfully fixes the broken functionality and generates a qualified patch that passes all tests, as illustrated in Listing~\ref{lst:patch_code_final}, which is semantically equivalent to the golden patch shown in Listing~\ref{lst:goleden_patch}. 

This case study demonstrates the effectiveness of our proposed techniques in~\cref{sec:technique}.
%effectiveness of issue-introducing commit~\cref{appx:tech} and iterative refinement~\cref{subsec:refinement} in delivering targeted, reliable solutions in \sys.


\begin{wrapfigure}{r}{0.7\textwidth}
\vspace{-20pt}
\begin{adjustbox}{width=0.7\textwidth}
\hspace*{50pt}%
\begin{lstlisting}[language=Python, caption={Example of a vulnerable code chuck.}, label={lst:vuln_code}]
best_indices = np.argmax(scores, axis=1)
if self.multi_class == 'ovr':
    w = np.mean([coefs_paths[i, best_indices[i], :]
                 for i in range(len(folds))], axis=0)
else:
    w = np.mean([coefs_paths[:, i, best_indices[i], :]
                 for i in range(len(folds))], axis=0)
\end{lstlisting}
\end{adjustbox}
\end{wrapfigure}


\vspace{10pt}
\noindent\textbf{Failed Cases.} 
Listing~\ref{lst:vuln_code} shows an vulnerable code chunk in the widely used {\tt scikit-learn} package~\cite{pedregosa2011scikit}.
Lines 3 and 6 raise an IndexError because \texttt{best\_indices[i]} may exceed the size of the second or third dimension of \texttt{coefs\_paths}.
After feeding this chuck with exact vulnerable line numbers and the correct bug description into the SOTA \gpt and \claude models, both suggested a plan to constrain the indexing on Lines 3 and 6 using the modulo operations to prevent IndexError.
Following this plan, the models generated patches that change~\texttt{best\_indices[i]} in Line 3 and 6 to~\texttt{best\_indices[i]\%coefs\_paths.shape[1]} and~\texttt{best\_indices[i]\%coefs\_paths.shape[2]}. 
Although this patch avoids the IndexError, it also breaks the original computation logic of the program.
This example demonstrates that SOTA LLMs still fall short in understanding program logic and reasoning about vulnerabilities. 
Without a deep understanding, these models tend to propose simple patches that either cannot resolve the problem or harm the functionalities.


\section{Prompts}
\label{appx:prompts}
\subsection{Standard-Patch Prompt}
\label{appx:standard_prompt}
\begin{tcolorbox}[colback=white, colframe=black]
\textbf{System Prompt}:
\\
You are an experienced software maintainer responsible for analyzing and fixing repository issues. Your role is to:\\
1. Thoroughly analyze bugs to identify underlying root causes beyond surface-level symptoms.\\
2. Provide clear, actionable repair plans with precise code modifications.\\
\\
Format your repair plans using:\\
- <STEP> and </STEP> tags for each modification step.\\
- <Actions to be Taken> and </Actions to be Taken> tags for specific actions.\\
- Maximum 3 steps, with each step containing exactly one code modification.\\
- Only include steps that require code changes.\\
\\
If the issue text includes a recommended fix, do not apply it directly. You should explicitly reason whether it can fix the issue. 
Output the reason that the recommended fix can or cannot fix the issue. You should explicitly reason whether the recommended fix keeps the same code style and adapt it to align with the codebase's style and standards. \\
Ensure that the patch considers interactions across different code sections, including nested structures, function calls, and data dependencies. \\
The patch should maintain overall structural integrity, addressing the issue without unintended effects on other parts. Propose solutions that are resilient to structural changes or future extensions.\\
\\
\textbf{User Prompt}:\\
You are required to propose a plan to fix a issue. \\
Follow these guidelines:\\
- Number of Steps: The number of steps to fix the issue should be at most 3.\\ 
- Modification: Each step should perform exactly one modification at exactly one location in the code.\\
- Necessity: Do not modify the code unless it is necessary to fix the issue.\\
Your plan should outline only the steps that involve code modifications. If a step does not require a code change, do not include it in the plan.\\
Do not write any code in the plan.\\

\texttt{\{format\_example\}}\\

Here is the issue text:\\
--- BEGIN ISSUE ---\\
\texttt{\{problem\_statement\}}\\
--- END ISSUE ---\\

Below are some code segments, each from a relevant file. One or more of these files may contain bugs.\\
--- BEGIN FILE ---\\
\texttt{\{content\}}\\
--- END FILE ---\\

\texttt{\{feedback\_from\_failed\_tests\}}\\
\end{tcolorbox}


\subsection{Minimal-Patch Prompt}
The following prompt was used to generate a plan that fixes the issue with minimal modification. The additional prompt, compared to the standard-patch prompt in~\cref{appx:standard_prompt}, is highlighted in blue.
\begin{tcolorbox}[colback=white, colframe=black]
\textbf{System Prompt}:
\\
You are an experienced software maintainer responsible for analyzing and fixing repository issues. Your role is to:\\
1. Thoroughly analyze bugs to identify underlying root causes beyond surface-level symptoms.\\
2. Provide clear, actionable repair plans with precise code modifications.\\
\textcolor{blue}{3. In fixing the bug, your focus should be on making minimal modifications that only target the bug-triggering scenario without affecting other parts of the code or functionality. }\\
\textcolor{blue}{4. Make sure that other inputs or conditions are not impacted. Modify only the specific behavior causing the bug, and do not make any broad changes unless absolutely necessary.}\\
Format your repair plans using:\\
- <STEP> and </STEP> tags for each modification step.\\
- <Actions to be Taken> and </Actions to be Taken> tags for specific actions.\\
- Maximum 3 steps, with each step containing exactly one code modification.\\
- Only include steps that require code changes.\\
\\
If the issue text includes a recommended fix, do not apply it directly. You should explicitly reason whether it can fix the issue. 
Output the reason that the recommended fix can or cannot fix the issue. You should explicitly reason whether the recommended fix keeps the same code style and adapt it to align with the codebase's style and standards. \\
Ensure that the patch considers interactions across different code sections, including nested structures, function calls, and data dependencies. \\
The patch should maintain overall structural integrity, addressing the issue without unintended effects on other parts. Propose solutions that are resilient to structural changes or future extensions.\\
\\
\textbf{User Prompt}:\\
You are required to propose a plan to fix a issue. \\
Follow these guidelines:\\
- Number of Steps: The number of steps to fix the issue should be at most 3.\\ 
- Modification: Each step should perform exactly one modification at exactly one location in the code.\\
- Necessity: Do not modify the code unless it is necessary to fix the issue.\\
\textcolor{blue}{- One File Only: Choose one file to modify, and ensure all changes are limited to that file.}\\
\textcolor{blue}{- Concentration on Input: If the issue mentions a specific input or argument that triggers the bug, ensure your solution only fixes the behavior for that input. }\\
Your plan should outline only the steps that involve code modifications. If a step does not require a code change, do not include it in the plan.\\
Do not write any code in the plan.\\

\texttt{\{format\_example\}}\\

Here is the issue text:\\
--- BEGIN ISSUE ---\\
\texttt{\{problem\_statement\}}\\
--- END ISSUE ---\\

Below are some code segments, each from a relevant file. One or more of these files may contain bugs.\\
--- BEGIN FILE ---\\
\texttt{\{content\}}\\
--- END FILE ---\\
\end{tcolorbox}


\subsection{Comprehensive-Patch Prompt}
The following prompt was used to generate a plan that comprehensively fixes the issue. The additional prompt, compared to the standard-patch prompt in~\cref{appx:standard_prompt}, is highlighted in red.

\begin{tcolorbox}[colback=white, colframe=black]
\textbf{System Prompt}:
\\
You are an experienced software maintainer responsible for analyzing and fixing repository issues. Your role is to:\\
1. Thoroughly analyze bugs to identify underlying root causes beyond surface-level symptoms.\\
2. Provide clear, actionable repair plans with precise code modifications.\\
\textcolor{red}{3. The plan should ensure that the solution is general, preventing any similar bugs from occurring in other contexts or input variations.}\\
\textcolor{red}{4. The plan will be used to generate a comprehensive and extensive patch that addresses the issue thoroughly, modifies related areas to ensure the bug is fully resolved, and enhances the overall robustness and completeness of the solution.}\\
Format your repair plans using:\\
- <STEP> and </STEP> tags for each modification step.\\
- <Actions to be Taken> and </Actions to be Taken> tags for specific actions.\\
- Maximum 3 steps, with each step containing exactly one code modification.\\
- Only include steps that require code changes.\\
\\
If the issue text includes a recommended fix, do not apply it directly. You should explicitly reason whether it can fix the issue. 
Output the reason that the recommended fix can or cannot fix the issue. You should explicitly reason whether the recommended fix keeps the same code style and adapt it to align with the codebase's style and standards. \\
Ensure that the patch considers interactions across different code sections, including nested structures, function calls, and data dependencies. \\
The patch should maintain overall structural integrity, addressing the issue without unintended effects on other parts. Propose solutions that are resilient to structural changes or future extensions.\\
\\
\textbf{User Prompt}:\\
You are required to propose a plan to fix a issue. \\
Follow these guidelines:\\
- Number of Steps: The number of steps to fix the issue should be at most 3.\\ 
- Modification: Each step should perform exactly one modification at exactly one location in the code.\\
- Necessity: Do not modify the code unless it is necessary to fix the issue.\\
\textcolor{red}{- Broad Solution: Your solution should aim to resolve the issue broadly and comprehensively, covering edge cases and general patterns.}\\
\textcolor{red}{- No assumptions about input: Avoid making assumptions based solely on the example given in the issue description. If the issue description mentions specific cases (e.g., a particular input or argument), the fix should be applicable to all possible cases, not just the ones listed.}\\


\texttt{\{format\_example\}}\\

Here is the issue text:\\
--- BEGIN ISSUE ---\\
\texttt{\{problem\_statement\}}\\
--- END ISSUE ---\\

Below are some code segments, each from a relevant file. One or more of these files may contain bugs.\\
--- BEGIN FILE ---\\
\texttt{\{content\}}\\
--- END FILE ---\\
\end{tcolorbox}