% Overview figure 
\begin{figure*}[tbp]
    \centering
    \includegraphics[width=155mm]{Figures/Design/overview.pdf}
    \vspace{-5mm}
    \caption{Overview of \sys. The system processes input through reproduction, localization, generation, validation, and refinement to obtain a final patch. Both localization and generation have two phases. The validation considers both PoC and functionality tests.  Finally, the iterative refinement involves two conditions: C1 checks if the patch passes all tests, if yes, the patch will be outputted; if no, C2 then checks if the current patch passes a new test compared to the previous round.}
    \label{fig:overview}
\end{figure*}

\section{Methodology of \sys}
\label{sec:technique}

\subsection{Technical Overview}
\label{subsec:tech_overview}

\textbf{Problem definition.}
Given a buggy code repository written in \texttt{Python}, denoted as $\mathcal{R}$, which contains a set of functionalities $\mathcal{F} = {f_1, f_2, \dots, f_n}$ written in different files.
The repository may have one or more issues, where each issue $\beta_{i}$ has an issue description written in text, denoted as $D_i$. 
The issue $\beta_{i}$ affects a subset of functionalities, denoted as $\mathcal{F}_{B_{i}} \subseteq \mathcal{F}$.
A successful patch, denoted as $p$, should fix all functionalities in $\mathcal{F}_{B_{i}}$ while preserving the behaviors of the unaffected functionalities $\mathcal{F}{s} = \mathcal{F} \setminus \mathcal{F}_{B_{i}}$.
Our main goals are twofold. 
First, we aim to resolve as many issues as possible across different issues and diverse repositories.
Second, we also aim to maximize the stability and reduce the cost of our patching framework.
We believe~\emph{stability and cost-efficiency} are critical for real-world applications of a patching tool. 
An unstable tool that produces only one correct patch across multiple runs significantly hinders its applicability for critical bugs.
Furthermore, if the tool is too costly to use, it limits its usage by ordinary users.

\textbf{Rationale behind \sys.}
Recall from Section~\ref{sec:rw} that we discussed the advantages and disadvantages of human-based versus agent-based planning. 
In general, agent-based planning is more expensive and less stable than human-based planning.
However, it may give a higher optimal issue resolved rate than human-based planning, as the LLM planner can explore more tailored workflows for different issues.
In contrast, human-based planning relies on a uniform workflow across different scenarios, which may not be effective in certain instances.
As such, if the primary goal is to maximize the resolved rate on certain benchmarks, agent-based planning should be the preferred strategy. 
Indeed, most existing tools follow this approach, especially in industry settings with much more resources than academia.
However, as mentioned above, a high resolved rate is not our sole goal, nor is it the only metric for evaluating a good patching agent.
Stability and cost-efficiency are equally important as the resolved rate given that we are developing a tool that can be used in real work rather than just exploring the boundary of LLM agents. 
As such, we choose to follow human-based planning in our patching agent. 

\cref{fig:overview} illustrates the workflow of \sys. 
It consists of five phases: reproduction, localization, patch generation, validation, and patch refinement. 
As discussed in Section~\ref{sec:rw}, localization and generation are commonly included in existing approaches.
We add three additional components to improve the overall patching effectiveness and efficiency.
The reproduction and validation components are crucial for determining patch quality and selecting the correct patch candidates for deployment.
Some advanced patching agents also include these components; in~\cref{subsec:reproduce}, we will specify how we designed ours to be more accurate and stable.
Refinement is a unique component in \sys, as we observe that improving a partially correct patch based on validation feedback is often more effective and efficient than generating a new patch from scratch.
This aligns with human experience, as a correct patch often requires multiple rounds of testing and refinement.

\textbf{Workflow of \sys.}
As shown in~\cref{fig:overview}, given the input of the codebase $\mathcal{R}$ and the description $D_i$ of the target issue $\beta_i$, \sys first calls reproduction to recover a set of testing cases, including PoC (a test that can trigger the issue) and benign functionality tests.
\sys runs the PoC and obtains the files it covered and the outputs. 
Then, the localization component takes as input $\mathcal{R}$, $D_i$, and information related to the PoC and outputs the root cause (specific lines causing the issue).
Similarly to \agentless{}~\cite{xia2024agentless}, our localization also follows a hierarchical workflow but with additional tools to better extract and leverage the program structures. 
After identifying the root cause, the generation component generates $N$ patch candidates at once.
As discussed in~\cref{subsec:generation}, the key novelty here is separating planning and generation and leveraging multiple prompting strategies to encourage patch diversity. 
The generated patch candidates are then fed to the validation component, which ranks the candidates based on their results of running the PoC and functionality tests.
If the validation cannot find a qualified patch that passes all available tests, the refinement component will be called to refine the top-ranked patch candidate or refine the localization based on the validation results.
\sys iteratively performs refinement and validation until it either identifies a qualified patch or reaches the maximum allowed number of generated patches ($N_{\text{max}}$).



\subsection{Reproduction and validation}
\label{subsec:reproduce}

\noindent\textbf{Reproduction.}
We introduce three improvements over existing work~\cite{xia2024agentless}.
First, reproduction in existing patching agents directly provides an LLM with $\mathcal{R}$ and $D_i$ and prompts it to generate a PoC.
However, $D_i$ often includes only short code snippets related to the issue without specifying necessary dependencies and configurations (e.g., the issue descriptions of \texttt{Django} typically do not have environment setups). 
Without such information, the generated PoCs often fail to run successfully. 
To address this challenge, we propose a~\emph{self-reflection-based PoC reproduction}, which is similar to the Reflexion mechanism designed for language agents~\cite{shinn2024reflexion}.
During the process, we let LLM iteratively generate and refine the generated PoC for certain iterations. 
We carefully construct our prompts to guide the LLM focus on checking and correcting 1) whether any key dependencies and configurations are missing; and 2) whether the PoC actually reproduces the target issue. 
If the reproduction fails to generate a valid PoC within the maximum iterations, we proceed without a PoC.
Second, different from existing works that only use the generated PoCs, we extract a more complete set of information based on PoCs.
This includes files covered by running the PoC, stack traces and outputs. 
As we will discuss later, this extra information helps localization and refinement. 
Third, we utilize LLM to identify three functionality test files from $\mathcal{R}$ that are most relevant to the target issue (each file may contain multiple testing cases).
These functionality tests enable the validation component to decide if the patch candidates preserve the functionalities of $\mathcal{F}_{S}$, an important metric for a successful patch.
More details about additional information retrieved based on PoCs are discussed in~\cref{appx:tech}.

\noindent\textbf{Validation.}
The simple validation strategy utilized in existing works~\cite{tao2024magis, Globant_Code_Fixer_Agent, ma2024lingma} is just to feed the patch candidates and the related information to an LLM and let it select the most qualified one. 
A more advanced strategy~\cite{liu2024marscode,wang2024openhands,arora2024masai} is to run the generated PoC and let an LLM decide whether the patches fix the issue based on their outputs.
As mentioned above, ensuring the correctness of the original functionalities is as important as fixing the issue.  
As such, we include the functionality tests recovered by our reproduction in the validation. 
Specifically, we first run our PoC on the patch candidates and use an LLM as a judge for its evaluation. 
Since no assertions are available for bug fixing, this serves as the only feasible solution.
We then also run the functionality tests and decide whether they pass based on their given assertions.
Finally, we rank the patches based on the tests they pass.
As specified in~\cref{appx:tech}, we prioritize patches that pass PoC tests over functionality tests during ranking.

\subsection{Localization}
\label{subsec:localization}

\textbf{Key challenges.}
Some existing localization directly query an LLM to identify the root cause at a line level~\cite{yang2024swe,arora2024masai,zhang2024autocoderover}.
Although they provide the LLM with tools to retrieve information from the codebase and allow it to refine its results, it is still difficult for LLMs to directly perform localizations at the line level. 
Besides, most agent-based tools incur high costs because they need to maintain the LLM agent's context history during localization.
\agentless{} designs a hierarchical workflow, which first identifies the issue-related files, then the functions, and lastly the lines.
This method gradually zooms into and makes the task easier at the line level as it filters out the majority of the non-related functions in the earlier steps. 
At each step, \agentless{} lets the LLM make decisions only based on the issue description. 
This approach has three critical limitations. 
First, the information in issue descriptions is diverse and not all of them have useful information for localization.
For example, some descriptions only specify error messages and PoC-related information that is not helpful for localization. 
Second, this method lacks a direct mechanism for retrieving details directly from the codebase.
Third, in most cases, the localization returns only the root cause it is confident about as a few lines of code. 
While this information is accurate, it is often insufficient for writing a correct patch due to the lack of necessary context.

\textbf{Our design.}
We follow the three-step procedure in \agentless{} given it is more stable and efficient than letting LLM directly do line level localization.
First, to address the limitation of inconsistent issue descriptions, we provide the LLM with the PoC code and information after running it (i.e., files it covered, stack trace, and running outputs). 
This enables the LLM to access more comprehensive information, such as key functions or classes invoked in the PoC and the stack trace, which is particularly useful for cases where only the code to reproduce the issue is provided in the issue description. 
For example, the files covered by PoC can help filter out some files irrelevant to the target issue, reducing the search space, especially for codebases with many files.
Second, to enable the LLM to extract and leverage more information from the codebase, we add a set of tools to the localization component.
These tools allow the LLM to search for class definitions and function definitions, or perform fuzzy string matching to locate and return relevant files. 
These tools provide precise search capabilities and can handle both class/function level information and line level details.
~\cref{appx:tech} has more details on the tools we integrate. 
Third, as shown in~\cref{fig:overview}, we add a review step that lets an LLM retrieve code snippets related to the current root cause.
As mentioned above, localization oftentimes returns overly precise root causes that fail to include necessary context or even do not fully cover all root causes. 
Identifying more contexts is important to generate correct and complete patches.
Note that we still constrain the maximum length of the final root cause to make sure not to overwhelm the generation with excessive context.


\subsection{Patch Generation}
\label{subsec:generation}

\textbf{Key challenge.}
Most existing patch generation components simply stack the related information and feed them to LLM for patch generation.
Such a simple solution has two critical challenges.
First, LLMs typically give incomplete patches.
This is because fixing an issue often requires modifications across multiple locations or involves multiple steps, making it difficult to generate a complete patch in one shot.
In addition, the incomplete root causes also lead to this issue. 
Second, being able to generate diverse patches is also crucial to increasing the likelihood of finding a successful patch within certain trials.
Moreover, we find that simply increasing the temperature still results in similar patches.
We need other strategies to increase patch diversity, enabling the agent to search for more potential solutions.

\textbf{Our design.}
First, as shown in~\cref{fig:overview}, rather than directly generating the patch, \sys breaks down the generation process into planning and generation. 
The planning phase first queries the LLM to generate a patch plan with multiple steps. 
The generation phase then generates the patch following the plan.
After finishing each step in the plan, we also include a lightweight in-generation validation with lint and syntax checks, and reconduct this step if the check fails. 
This design is motivated by the Chain-of-thoughts prompting strategy~\cite{wei2022chain}.
That is, having a plan explicitly forces the LLM to break down the patch generation into multiple steps.
This helps the model to better reason about the patch task, encouraging it to provide more complete patches. 
Besides the in-generation validation can identify and fix errors at an early stage, improving the patch efficiency. 
Second, to enhance the diversity of the generated patch candidates, we design three types of prompts for plan generation. 
These prompts explicitly guide the LLM to produce patching plans with different focuses: a comprehensive and extensive patch designed to prevent similar issues, a minimal patch with the smallest possible modifications, or a standard patch without any specific instructions.
~\cref{appx:prompts} contains more details on the prompts that we use.
As demonstrated~\cref{fig:overview}, we will generate $N$ plans following the pre-specified prompts and thus produce $N$ patch candidates in each batch.   
\begin{table*}[th!]
\centering
% \setlength{\tabcolsep}{4pt} 
% \renewcommand{\arraystretch}{1.2} 
\caption{Comparison of \sys and five baselines on the two benchmarks. ``Agent-based'' and ``Human-based'' refer to agent-based planning and human-based planning, respectively. ``-'' means not available. Note that \globant does not report the results on SWE-Bench-Verified and \codestory does not report their result on SWE-Bench-Lite. They both do not disclose the LLM model(s) in their agents.}
\label{tab:swe_bench_leaderboards}
\resizebox{\textwidth}{!}{
\begin{tabular}{r|rc|ccc|ccc}
\Xhline{1.0pt}
&\multirow{2}{*}{\begin{tabular}[r]{@{}r@{}}\textbf{Patching} \\ \textbf{agent}\end{tabular}}   
& \multirow{2}{*}{\begin{tabular}[r]{@{}r@{}}\textbf{Open-} \\ \textbf{source}\end{tabular}} & \multicolumn{3}{c|}{\textbf{SWE-Bench-Lite}}  & \multicolumn{3}{c}{\textbf{SWE-Bench-Verified}}  \\ \cline{4-9}
& &  &\textbf{LLM} & \textbf{Resolved\%} & \textbf{Cost (\$)} & \textbf{LLM} & \textbf{Resolved\%} & \textbf{Cost (\$)} \\ \hline
\multirow{4}{*}{\begin{tabular}[r]{@{}r@{}}\textbf{Agent-} \\ \textbf{based}\end{tabular}} & \autocode  & \checkmark   & \gpt   & 30.67\% (92)           & 0.65   & \claude    & 51.80\% (259)  & 4.50 \\ 
 & \openhands & \checkmark  & \claude & 41.67\% (126)       & 2.14  & \claude         & 53.00\% (265)        & 2.19  \\ 
 & \globant & \ding{53}  & -  & 48.33\% (145)       & 1.00   & -  & -   & -    \\ 
 & \codestory & \ding{53}    & -   & -  & -   & -  & 62.20\% (311)  & 20.00  \\ \hline
\multirow{2}{*}{\begin{tabular}[r]{@{}r@{}}\textbf{Human-} \\ \textbf{based}\end{tabular}} & \agentless & \checkmark  & \claude  & 40.67\% (123)       & 1.12   & \claude  & 50.80\% (254)   & 1.19  \\
 & \cellcolor[HTML]{E3E8FF} \sys   & \cellcolor[HTML]{E3E8FF} \checkmark  & \cellcolor[HTML]{E3E8FF} \claude   & \cellcolor[HTML]{E3E8FF} 45.33\% (136)       & \cellcolor[HTML]{E3E8FF}  0.97    & \cellcolor[HTML]{E3E8FF}  \claude   &  \cellcolor[HTML]{E3E8FF}  53.60\% (268)        & \cellcolor[HTML]{E3E8FF}  0.99     \\ \Xhline{1.0pt}
\end{tabular}
}
\end{table*}

\subsection{Patch Refinement}
\label{subsec:refinement}
Recall that refinement is a unique component in \sys that existing works do not have.
The motivation for adding this component is to better leverage the validation feedback and the current partially correct patches.
As shown in~\cref{exp:ablation}, refining existing parties based on validation results is more effective and efficient than re-generating patches from scratch.
More specifically, as demonstrated in~\cref{fig:overview}, \sys focuses on refining the top-ranked patch in the current batch.
It feeds the current batch and its validation result back to the generation component and asks it to generate a new batch of patches.
The generation still follows the planning and generation workflow.
Here, when generating the plans, we design the prompt to guide the model to correct the failed testing cases of the current patch. 
This process continues until a qualified patch that passes all validations is generated, or the total number of generated patches reaches the predefined limit of $N_{\text{max}}$. 
Note that if the patches generated in a whole batch do not pass any new tests, we rerun the localization with the validation results to obtain a new root cause.
This additional step gives \sys the opportunity to leverage information from later components to correct localization errors and ultimately succeed in generating qualified patches.





















%========================== OLD VERSION ==============================


% \textbf{Workflow of \sys.}
% ~\cref{fig:overview} presents an overview of \sys{}, which takes as input a GitHub issue described in natural language and a buggy project codebase. 
% \sys{} processes these inputs through five phases: reproduction, fault localization, patch generation, validation, and refinement, and produces a candidate patch aimed at addressing the issue.

% \sys begins with the reproduction phase, where it performs issue reproduction and functionality test retrieval and output the a "proof of concept" (PoC), to replicate the issue described in the issue
% For issue reproduction, \sys generates a Python script, referred to as a "proof of concept" (PoC), to replicate the issue described in the issue. If the issue is successfully reproduced, \sys collects the PoC’s coverage data and attempts to identify the commit that introduced the issue. These outputs are then used in the subsequent fault localization and patch generation phases.

% For functionality test retrieval, \sys utilizes the issue description and the repository's directory tree structure to locate functionality tests relevant to the reported issue. The top-N most relevant test files are identified and returned.

% After completing the reproduction phase, \sys transitions into the fault localization phase, which employs a hierarchical approach to narrow down the potential patch locations across three levels of granularity: file level, class/function level, and fine-grained line level.
% At the file level, \sys constructs a tree-structured repository representation, filtered based on the PoC coverage, retaining only the files executed during PoC execution. The issue description and the repository representation are then provided to the LLM, prompting it to return the top $N$ files most relevant to the issue. Since the repository tree structure lacks detailed source code information, the LLM is also equipped with a set of search tools.
% At the class and function level, the LLM is provided with the signatures and comments of classes and functions extracted from the retrieved files and is prompted to identify functions and classes likely related to the issue.
% At the fine-grained line level, the complete source code of the identified classes and functions is presented to the LLM, which is then prompted to identify potential patch locations—specific code snippets requiring modification. At last, we ask LLM to review the localization results for self-consistency.

% \sys then transitions into the patch generation phase. Using the localization results and the issue description, the LLM is prompted to generate $batch\_size$ patch candidates.
% To generate each patch candidate, we first prompts the LLM to create a detailed plan, then generates patches incrementally by addressing each planned step. To ensure patch diversity, we vary the prompting strategies, requesting comprehensive preventive fixes, minimal modifications, or standard patches.

% After patch generation, \sys verifies each patch candidate by applying them and running the PoC and functionality tests retrieved during the reproduction phase. The best-performing patch from the current batch, based on validation results, is selected as the current best patch.

% If the current best patch passes all available tests, or if \sys has generated a total number of patch candidates equal to $max\_sample$, \sys will terminate and output the current best patch. 
% Otherwise, \sys enters the refinement stage, initiating a loop where it applies the current best patch to the codebase and generates another batch of patch candidates. 
% In each iteration, the LLM is provided with additional information, including the code of the failed tests and feedback from their execution. 
% Note that if none of the patch candidates in a batch passes more tests than the current best patch, \sys will re-perform fault localization using feedback from the failed tests before generating the next batch of patch candidates.

% In the following sections, we introduce the technical challenges and our design for each component in detail. 







%%%%%%%%%%%%%%%%%%%%%%%% Our text %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \noindent\textbf{goal and key challenges} 
% Design a better predefined workflow for agent-based patching while open up more freedom for LLM. We hope to achieves top performance while maintaining low costs and high stability.



% \noindent\textbf{How to solve the changes}

% The key difference between methods lies in localization, which significantly affects outcomes. We combine human-designed workflow while giving llm some freedom (can re-do localization during refinement, can call search\_tools during localization).

% The refinement is also important. It is a natural advantage of the AutoCodeRover line of work. We also implement a refinement workflow.

% Existing methods do not fully leverage reproduction and validation and don't generate patch-specific pocs. We generate patch-specific pocs and we extract coverage and issue-introducing commits for localization and generation.  

% We also did other minor optimizations in the generation (step-by-step and ToT generation of patches).

% % \textbf{Technique details:} 
% \begin{itemize}
%     \item Coverage-based functionality test.
%     \item Existing methods didn't fully leverage the poc. In order to better leverage the poc, we also get coverage (for localization) and issue introducing commit (for repair).
%     \item PoCs specific to patches. Existing methods only generate pocs based on the issue description but don't generate pocs specific for patches. The poc generated from the issue description may not be the only way to trigger the issue, so that a patch passing the poc test may only partially fix the issue. We generate pocs intended to break a specific patch instead.
% \end{itemize}

% \subsection{Localization}
% \textbf{Technique details:} 
% \begin{itemize}
%     \item Capability of localization after generation
%     \item Search tools
%     \item Filtering based on coverage.
% \end{itemize}

% \subsubsection{Patch Generation (First do ablation study to see if it affects a lot)}
% \textbf{Technique details:} 
% \begin{itemize}
%     \item Planning that breaks down the patching task into multiple steps
%     \item ToT during generation of patches. For each step, generate multiple samples, do lint and syntax check, select the best partial patch.
% \end{itemize}

% \subsection{Iterative Refinement}

% Desicribe refinement workflow here.