\section{PatchingKitty Outline}

General, say do 7atch

Go to intro

Automatically generate patches for github issues, given issue text description and a complete codebase. We assume that we have an environment in which the github project is installed, as all of the existing agents who have reproduction/verification on the SWE-bench leader board.

\subsection{Existing Work and limitations}

At a high level
There are currently two lines of work. 

LLM planning - workflow
The AutoCodeRover line of work let LLM decide what to do in each step, they can natrually refine based on previous generation results. But their performance is unstable, and they cost more since they need to maintain history. 

Human planning - workflow
For the Agentless line of work, the current works follow a human-designed linear workflow, each patch is generated from scratch instead of based on a previous patch, and there are no feedbacks from previously failed patches. Also, their linear workflow is too restricted, e.g., llm doesn't have the freedom to do localiaztion after generation. If the localization is wrong, all following steps are useless. 


\noindent\textbf{LLM planning.}
Introduce all the methods and discuss limitations


\noindent\textbf{Human planning.}
Introduce all the methods and discuss limitations

% The existing approaches consist of four primary steps:
% \begin{itemize}
%     \item Reproduction: Writing a reproducer test based on the issue statement.
%     \item Localization: Providing tools and allowing the LLM to decide the localization workflow.
%     \item Generation: Generating patches.
%     \item Verification and Selection: Reviewing patches and reproducer tests to ensure correctness.
% \end{itemize}

% \textbf{Existing Agent-based Patching Systems}

% There are two lines of work.
% The AutoCodeRover line of work does not have a predefined workflow or only includes high-level steps (Reproduction, Localization, Generation, Verification and Selection). They rely on the LLM as a planner, allowing the LLM to generate the specific workflow for fixing each issue. What tasks to perform within each stage is also determined by the LLM. On the other hand, the Agentless line of work follows a predefined and restricted workflow.

% \subsubsection*{AutoCodeRover Line of Work}
% \begin{itemize}
%     \item Includes systems like AutoCodeRover, Openhands-codeact, Moatless Tools, Globant Code Fixer Agent, and Composio SWE-Kit.
%     \item AutoCodeRover follows a localization and generation workflow but does not enforce a fixed localization workflow, instead relying on tools and the LLM to decide what to do during localization.
%     \item SpecRover (New AutoCodeRover):
%     \begin{itemize}
%         \item Added Reproduction and Verification.
%         \item Updated workflow: Reproduction $\to$ Localization $\to$ [Generation $\to$ Verification loop].
%     \end{itemize}
%     \item Openhands-codeact introduces an additional step to consider edge cases and verify that patches handle them effectively.
%     \item Globant Code Fixer Agent provides additional tools to assist with localization, like get\_related\_files, detect\_language\_and\_get\_dependencies, and search\_codebase.
%     \item Moatless Tools focuses on improving workflow and tools. Has better editing and search tools. And could do localization after generation.
%     \item Composio SWE-Kit focuses on better tools for localization and generation.
% \end{itemize}

% \subsubsection*{Agentless Line of Work}
% \begin{itemize}
%     \item Agentless V1.0 follows a localization and generation workflow with a fixed localization workflow.
%     \item Agentless Variants:
%     \begin{itemize}
%         \item Agentless+Repograph: Uses Repograph for localization.
%         \item Agentless V1.5 (+Reproduction+Verification): Adds reproduction and verification steps, doesn't have loop.
%     \end{itemize}
% \end{itemize}


\subsection{Key Techniques}

\subsection{Overview}

\noindent\textbf{goal and key challenges} 
Design a better predefined workflow for agent-based patching while open up more freedom for LLM. We hope to achieves top performance while maintaining low costs and high stability.



\noindent\textbf{How to solve the changes}

The key difference between methods lies in localization, which significantly affects outcomes. We combine human-designed workflow while giving llm some freedom (can re-do localization during refinement, can call search\_tools during localization).

The refinement is also important. It is a natural advantage of the AutoCodeRover line of work. We also implement a refinement workflow.

Existing methods do not fully leverage reproduction and verification and don't generate patch-specific pocs. We generate patch-specific pocs and we extract coverage and bug-introducing commits for localization and generation.  

We also did other minor optimizations in the generation (step-by-step and ToT generation of patches).

\noindent\textbf{Overview}
a FIGURE OF COMpONENTS


\subsection{Reproduction and Localization}

% what's key challenge for building this component 

% what's your solution

\subsection{Generation}

\subsection{Verification}

\subsection{Refinement}



% % \textbf{Technique details:} 
% \begin{itemize}
%     \item Coverage-based functionality test.
%     \item Existing methods didn't fully leverage the poc. In order to better leverage the poc, we also get coverage (for localization) and bug introducing commit (for repair).
%     \item PoCs specific to patches. Existing methods only generate pocs based on the issue description but don't generate pocs specific for patches. The poc generated from the issue description may not be the only way to trigger the issue, so that a patch passing the poc test may only partially fix the issue. We generate pocs intended to break a specific patch instead.
% \end{itemize}

% \subsection{Localization}
% \textbf{Technique details:} 
% \begin{itemize}
%     \item Capability of localization after generation
%     \item Search tools
%     \item Filtering based on coverage.
% \end{itemize}

% \subsubsection{Patch Generation (First do ablation study to see if it affects a lot)}
% \textbf{Technique details:} 
% \begin{itemize}
%     \item Planning that breaks down the patching task into multiple steps
%     \item ToT during generation of patches. For each step, generate multiple samples, do lint and syntax check, select the best partial patch.
% \end{itemize}

% \subsection{Iterative Refinement}

% Desicribe refinement workflow here.



\subsection{Evaluation}

swe-bench and you choose lite and verified 

\subsubsection{Exp 1: Compare with baselines on SWE-bench Lite/verified} [Done]
Model: Claude 3.5/o3

\textbf{setup:} 
\textbf{Top open-sources tools:}
Openhands-codeact/Composio SWE-Kit 
Agentless

\textbf{Top commercial tools:} Globant Code Fixer Agent, devlo

metric accuracy and cost, any other???

what you did: procedure
Sample number: We choose a number of samples (target max\_samples=10) so that we are cheaper than open-hands (openhands v2.1 which solved 41.67\% total cost is \$401.2864, each case cost \$1.33). Our previous average cost with max\_samples=4 is \$2.6 and solved 41.67\%.

% We need to first optimize the cost by caching the prompts and disabling ToT if it does not help a lot.

% A table of agents with resolved issue numbers, model, and average cost. Analyze the performance of other agents based on their submission on the SWE-bench leaderboard (no need to rerun).

\textbf{results:} 

\subsubsection{Comparison of individual components on lite/verified}

% agentless
% openhands


\textbf{Comparison of the Localization performance:} 

File level, func level and line level.

% agentless [done]

\textbf{Comparison of Reproduction and Verification:} 

\textbf{Reproduction}

How many instances can be reproduced by each agent, the resolving rate of the set of instances that can be reproduced and can not be reproduced.

Compare the selected functionality files by us with those by moatless tools (as baseline). 
Moatless tools appends test\_ to patched file and does pattern match for finding func tests.
How to compare fairly?

\textbf{Verification}

% ask LLM (o3/claude)


\subsubsection{Comparative Experiment of Stability}

Randomly choose 30 instances, run openhands (with their default setup) and PatchingKitty for three times each, show the solved instance numbers, the average cost in each run and the std.


\subsubsection{Ablation studies (keep the same amount of sample)}

Only run on SWE-bench Lite.

Model: o3-mini/Gemini-flash 2.0 since it is faster and cheaper.

Base method: localization + generation + how to select (let LLM to select)

Reproduction + localization + generation + verification (10)

Your method (10)



% \textbf{Reproduction and Verification}

% We disable the functionality test and the poc test, compare the number of solved instances and cost.

% One problem here is that the stop criteria of refinement depend on the poc and functionality tests. One naive method is not to stop until max samples.

% Conclusion: reproduction and verification helped us solve more instances and save money by deciding which instances to refine.

% \textbf{Generation}

% Our step-by-step ToT method may cost too much relative to the improvements gained.

% We compare the two settings:

% Naive method: generate a plan and then generate patch at one time.

% Our method: generate a plan and then generate patch step by step using ToT, apply each step, and do lint and syntax check.

% \textbf{Localization}

% We disable the coverage, the searching tool and the capability of localization after generation, and rerun the whole experiment on SWE-bench Lite. Compare the file level, func level and line level accuracy and the resolve rate.

% Conclusion: Our optimizations on localization helped to achieve better results. 

% \textbf{Refinement}

% The naive way of generating multiple patches is to generate each sample from scratch, based on the original repo. We compare this naive solution with refinement (generate patches based on the previous best patch).

% For the baseline method, we fix max\_samples=10, fix the model as claude 3.5, and set the same stop generation criteria as refinement. We fix everything else other than whether to provide the previous best patch and the feedback related to this patch (failed poc tests and functionality tests. etc.).

% Conclusion: refinement can achieve better results than sampling from scratch with lower cost (since we will stop refinement for some instances before generating max\_sample patches).

\subsection{Discussion}

\subsection{Conclusions}


