@inproceedings{dai2022knowledge,
  title={Knowledge Distillation for Small-Footprint Efficient Transformers},
  author={Dai, Damai and Li, Can and Peng, Baoxin},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL)},
  year={2022}
}

@article{gao2021making,
  title={Making Pre-Trained Language Models Better Few-Shot Learners},
  author={Gao, Tianyu and Fisch, Adam and Chen, Danqi},
  journal={arXiv preprint arXiv:2103.03511},
  year={2021}
}

@article{lee2022deduplicating, 
  title={Deduplicating Training Data Makes Language Models Better},
  author={Lee, Kenton and Ippolito, Demi and Nouri, Ethan and Bras, Romal Le and Callison-Burch, Chris and Sedoc, Jo√£o},
  journal={arXiv preprint arXiv:2203.06642},
  year={2022}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={arXiv preprint arXiv:2203.02155},
  year={2022}
}

@article{qiu2022edgeformer,
  title={EdgeFormer: Transformer-based Natural Language Processing on Edge Devices},
  author={Qiu, Hang and Qiu, Jincheng and Li, Yuming and Li, Qiang and Li, Jie},
  journal={arXiv preprint arXiv:2205.12487},
  year={2022}
}

@article{stiennon2020learning,
  title={Learning to summarize from human feedback},
  author={Stiennon, Natan and Ouyang, Long and Ziegler, Jeff and Byrne, Ryan and Radford, Alec and Paull, Lauren and Bachand, Alexander and Kim, Damian and Moore, Pablo Mostafazadeh Davila and others},
  journal={arXiv preprint arXiv:2009.01325},
  year={2020}
}

@inproceedings{su2022globalpipeline,
  title={GlobalPipeline: An Efficient Model Parallelism via Simple Data-Parallelism Principles},
  author={Su, Zhiqing and Tan, Yi Tay and Liang, Yaqian and Choi, Hyunji and Park, Haejun and Chen, Sangdoo and Lin, Xi and others},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year={2022}
}

@article{zhang2022deflating,
  title={Deflating Pre-trained Language Models for Efficient Deployment on Embedded Systems},
  author={Zhang, Jiahui and Li, Zhengjie and Xue, Ying and Zhang, Chenghua and Li, Dongsheng},
  journal={arXiv preprint arXiv:2212.08015},
  year={2022}
}

