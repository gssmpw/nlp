\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage[numbers]{natbib}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\newcommand{\zz}[1]{{\color{blue}[Zhenghao: #1]}}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

% \title{AUTHOR GUIDELINES FOR ICASSP 2024 PROCEEDINGS MANUSCRIPTS}
% TransVDM: Image-to-Video Generation with Transparency Using Diffusion Models
\title{TransVDM: Motion-Constrained Video Diffusion Model for Transparent Video Synthesis}

\author{
    \IEEEauthorblockN{Menghao Li, Zhenghao Zhang, Junchao Liao, Long Qin, Weizhi Wang}
    \IEEEauthorblockA{Alibaba Group}
    % \IEEEauthorblockA{\{limenghao.lmh,zhangzhenghao.zzh,liaojunchao.ljc,ql362507,wangweizhi.wwz\}@alibaba-inc.com}
    % {zhangzhenghao.zzh,liaojunchao.ljc,limenghao.lmh,ql362507,wangweizhi.wwz\}@alibaba-inc.com}
}

\maketitle

\begin{abstract}
% In this paper, we introduce TransVDM, the first diffusion based model capable of generating transparent videos by integrating image and text. Traditional video generation models typically handle RGB channels, lacking the ability to capture transparency information. TransVDM incorporates a Transparency Variational Autoencoder (TVAE) that encodes alpha channel transparency into the latent manifold of the pretrained latent diffusion models, enabling seamless support for transparency within video diffusion models. Additionally, we propose an Alpha Motion Constraint Module (AMCM) that helps the network recognize vacuous regions as transparent areas, preventing the generation of unreasonable artifacts in these regions, thereby significantly enhancing the quality and realism of the generated videos. To train the model, we build a dataset comprising 100K transparent images and videos sourced from the Internet and meticulously curated to ensure high quality and diversity. Experimental results demonstrate the effectiveness of our method across multiple benchmarks. Our research paves the way for advancements in transparent video generation, providing new solutions for applications in design industry, digital content creation, and beyond.
Recent developments in Video Diffusion Models (VDMs) have demonstrated remarkable capability to generate high-quality video content. Nonetheless, the potential of VDMs for creating transparent videos remains largely uncharted. In this paper, we introduce TransVDM, the first diffusion-based model specifically designed for transparent video generation. TransVDM integrates a Transparent Variational Autoencoder (TVAE) and a pretrained UNet-based VDM, along with a novel Alpha Motion Constraint Module (AMCM). The TVAE captures the alpha channel transparency of video frames and encodes it into the latent space of the VDMs, facilitating a seamless transition to transparent video diffusion models. To improve the detection of transparent areas, the AMCM integrates motion constraints from the foreground within the VDM, helping to reduce undesirable artifacts. Moreover, we curate a dataset containing 250K transparent frames for training. Experimental results demonstrate the effectiveness of our approach across various benchmarks.
\end{abstract}

\begin{IEEEkeywords}
Transparent Video Generation, Diffusion Models, Image Animation
\end{IEEEkeywords}

\section{Introduction}
% In recent years, the field of video generation has witnessed remarkable advancements, while little attention has been given to the specific challenge of transparent video generation. Many visual creative workflows such as  
% video editing software and animated emoji production leverage transparency as a vital component, underscoring the growing demand for effective solutions in this area.






% Video Diffusion Models (VDMs) have made substantial strides in recent years, showcasing their ability to synthesize realistic and high-resolution videos. However, the application of VDMs to the synthesis of transparent videos presents new challenges and opportunities, as it combines two critical subtasks: the synthesis of RGB video content and the precise segmentation of video elements. Transparent video generation involves not only creating visually compelling content but also accurately representing transparency or alpha channels, which indicates the degree of opacity for each pixel. This additional complexity is crucial for compositing generated video elements seamlessly into existing scenes, fulfilling an important requirement in various domains such as film production, augmented reality (AR), and computer-generated imagery (CGI).

% Existing VDMs primarily focus on text-to-video or image-to-video synthesis. Notable methods such as Imagen Video \cite{ho2022imagen} and Make-A-Video \cite{singer2022make} leverage modified 3D U-Net architectures to generate high-resolution videos from text prompts, effectively combining image and video data. Additionally, fine-grained approaches like DragNUWA \cite{yin2023dragnuwa} and Animate-Anything \cite{dai2023fine} utilize trajectories or motion masks to drive image animation, enhancing flexibility and precision in generating dynamic content.

% Due to the aforementioned methods' lack of support for transparency, we propose the TransVDM framework, to facilitate transparent video generation. Our work is inspired by LayerDiffusion \cite{zhang2024transparent}, which effectively explores layered image generation, and concludes that encoding alpha channel information separately as a small perturbation within the RGB latent space allows traditional VAEs to support transparency. Building upon this, we train a Transparent Variational Autoencoder (TVAE) to enable the pre-trained VDM to seamlessly support transparent video generation from RGBA images. However, we find that the generation quality is subpar, with artifacts commonly occurring in transparent regions. We speculate this is due to the model's inability to distinguish between dynamic foregrounds and static transparent areas, causing it to drive motion equally across both.To address this issue, we introduce a lightweight Alpha Motion Constraint Module (AMCM), which integrates coordinates of foreground bounding box as motion constraints. This significantly reduces artifacts and enhances the quality of the generated transparent videos. 
Transparent video generation involves not only creating visually compelling content but also accurately representing the alpha channels for each pixel, which indicates the degree of transparency.
This technology has broad applications across various fields, including film production and augmented reality (AR), where transparent elements are crucial for content composition and creation. However, existing video generation methods, such as Video Diffusion Models (VDMs) \cite{nichol2021glide,ho2022video,he2022latent,wang2023modelscope}, are limited to supporting only RGB channels and cannot generate transparent videos in an end-to-end manner.

% Recently, Video Diffusion Models (VDMs) have made notable progress in synthesizing realistic, high-resolution videos. For example, Imagen Video \cite{ho2022imagen} and Make-A-Video \cite{singer2022make} employ modified 3D U-Net architectures to generate high-resolution videos from textual descriptions, effectively merging image and video capabilities. Furthermore, precise methods like DragNUWA \cite{yin2023dragnuwa} and Animate-Anything \cite{dai2023fine} utilize motion trajectories or masks to facilitate image animation, enhancing both flexibility and accuracy in producing dynamic content. Nonetheless, the capability of VDMs to effectively generate transparent videos continues to be an underexplored area.

In this paper, we present TransVDM, the first approach specifically designed to tackle the challenge of transparent video generation using VDMs. Unlike conventional post-processing techniques that depend on video segmentation, matting or color palette modifications, TransVDM emphasizes the direct generation of transparent videos from the outset. Inspired by LayerDiffuse~\cite{zhang2024transparent}, we initially utilized a Transparent Variational Autoencoder (TVAE) to separately encode alpha channel information as a minor perturbation within the latent space of the pretrained VDM. This process enables the VDM to incorporate alpha information within its input image, providing it the basic ability to generate transparent videos directly.
While this approach successfully facilitates transparent video generation within the existing VDM framework, it also introduces challenges, specifically that VDMs are prone to generating artifacts in transparent areas. We speculate this arises because any region in the image can be affected by the VDM, leading to pixel value changes. Consequently, this sometimes results in undesirable motion within the RGB channels of transparent areas while the corresponding alpha values transform into non-transparent values, which ultimately forms artifacts. To address this issue, we propose a lightweight Alpha Motion Constraint Module (AMCM), which incorporates the coordinates of foreground bounding boxes as motion constraints into the VDM. This integration serves to explicitly reduce the model's tendency to generate undesirable motion in transparent regions, significantly minimizing artifacts and enhancing the overall quality of the generated transparent videos.

To enable transparent video generation, we have developed a dataset consisting of 250k transparent frames for training purposes, which includes 100k high-quality transparent images and 150k transparent video frames. Our training process follows a two-stage approach: initially, we train the TVAE using the transparent images, and subsequently, we utilize the transparent videos to train the AMCM module.

Our contributions are summarized as follows: (1) We introduce TransVDM, the first diffusion model designed for transparent video generation. (2) We utilize a Transparent Variational Autoencoder to enhance the existing VDM framework and introduce a motion constraint to reduce artifacts in transparent regions. (3) We have curated a dataset consisting of 250k transparent frames for training, which can serve as a robust baseline for future research endeavors.

% Faced with the challenge of dataset scarcity for transparent video generation, we construct a dataset comprising 250K transparent frames, including 100K high-quality transparent images and 150K transparent video frames. The training of the TVAE relies on these 2D images, which we collect from the Internet. We process them by filtering out images like low-resolution and non-transparent ones. For the AMCM, we utilize open-source video segmentation datasets like Youtube VOS \cite{xu2018youtube} to split multi-object scenes into individual transparent segments. We also generate captions using CoCa \cite{yu2022coca}, resulting in 150K frames for training.

% Our contributions are summarized as follows: (1) TransVDM is the first diffusion model to achieve transparent video generation. (2) We curate a dataset comprising 250K transparent frames. (3) We train a Transparent Variational Autoencoder (TVAE) to encode alpha information. (4) We introduce the Alpha Motion Constraint Module (AMCM) to effectively constrain motion in transparent regions.

% Our contributions are summarized as follows. (1) We are the first to propose transparent image-to-video generation utilizing diffusion models. (2) We present a dataset containing 100K transparent images and videos. (3) We introduce the Alpha Motion Constraint Module (AMCM) to effectively regulate motion in transparent regions.





% As the need for high-quality video content increases, various methods have emerged to facilitate video generation. The majority of recent developments focus on text-to-video synthesis, where Video Diffusion Model (VDM) \cite{ho2022video}, LVDM \cite{he2022latent} and ModelScopeT2V \cite{wang2023modelscope} pioneer this domain by transforming the 2D U-Net architecture \cite{ronneberger2015u} into a 3D U-Net framework, enabling joint training on both image and video data. Imagen Video \cite{ho2022imagen} utilizes a group of video diffusion models to synthesize high-resolution videos from text prompts, while Make-A-Video \cite{singer2022make} combines off-the-shelf text-to-image generation model with decoupled spacetime attention mechanism to produce high-quality videos without using paired video-text dataset. In addition, plenty of recent works based on VDMs \cite{guo2023animatediff, wang2023lavie} have been proven progress. Even though their success in video generation, they mainly rely on coarse-grained text prompts for semantic control, which limits their ability to manipulate finer details in the generated content. This lack of granularity often results in outputs that do not align with user's intentions, particularly in complex scenes where precise adjustments are needed. 

% Building upon the limitations of text-to-video generation, recent advances in image-to-video synthesis offer a more refined approach by utilizing static images as conditions. The LFDM \cite{ni2023conditional} realizes human portrait animation adopting a two-stage flow-based diffusion model.  Stable Video Diffusion \cite{blattmann2023stable} fuses image embedding into the backbone via cross attention, making results semantically coherent with the given image. Moreover, a range of image animation techniques have been developed that incorporate certain control mechanisms. For instance, DragNuwa \cite{yin2023dragnuwa} utilizes trajectories to guide objects within the image along specified paths, allowing for greater flexibility in animation. Similarly, Animate-Anything \cite{dai2023fine} employs motion masks to facilitate regional motion control. Furthermore, Follow Your Click \cite{ma2024follow} combines image segmentation with animation to allow users to configure movements for designated objects. The latest advancement, Tora \cite{zhang2024tora}, explores the effects of integrating trajectories within a diffusion transformer architecture.

% While the aforementioned image-to-video methods primarily focus on RGB channels, they lack support for alpha channels. We propose TransVDM for generating transparent videos, effectively bridging the gap within the realm of image conditioned video generation. To the best of our knowledge, our approach is the first diffusion model capable of generating videos with transparency. TransVDM offers guidance and facilitates deeper investigations in this emerging area. Inspired by the work of LayerDiffusion \cite{zhang2024transparent}, which thoroughly explores layered image generation, concluding that encoding alpha channel information separately within the latent space of diffusion models allows for seamless support of transparency. Following LayerDiffusion, we collect a high-quality dataset of over 100,000 transparent images, all cleared for commercial and research purposes. With this dataset, we train a Transparent Variational Autoencoder (TVAE) from scratch, designed to encode the transparency information into the RGB latent space. We discover this architecture enables existing image-to-video diffusion models to acquire the capability to generate transparent videos in a zero-shot manner. However, the model tends to equally drive motion in both transparent and opaque regions, resulting in unreasonable artifacts in the transparent areas. To address this issue, we propose a lightweight Alpha Motion Constraint Module (AMCM), which is designed to regulate the motion of transparent regions. Due to the lack of available data, we additionally collect 10K transparent videos by processing open-source video segmentation datasets.

\section{Related Work}
The goal of current video generation is primarily to create RGB videos from a given text prompt or a combination of text and image. Early works, such as those using GANs for video generation \cite{saito2017temporal,tulyakov2018mocogan,wang2018video,li2018video} produce low-quality results. Subsequently, diffusion model based methods become popular, such as LVDM \cite{he2022latent} and ModelScopeT2V \cite{wang2023modelscope}, which transform the 2D U-Net architecture \cite{ronneberger2015u} into a 3D framework for joint training on video content. Additionally, Imagen Video \cite{ho2022imagen} synthesizes high-resolution videos from text prompts by leveraging multiple video diffusion models, while Make-A-Video \cite{singer2022make} integrates text-to-image generation with a decoupled spacetime attention mechanism. Furthermore, methods aimed at achieving fine-grained control have also gained traction, with approaches such as DragNUWA \cite{yin2023dragnuwa} and Animate-Anything \cite{dai2023fine}, which utilize motion trajectories or masks to enhance image animation. However, these methods lack support for transparency, limiting their effectiveness in certain scenarios. We address this by building the TransVDM framework, which incorporates transparency handling to improve the versatility of video generation.


% Despite significant progress in these diffusion model approaches \cite{guo2023animatediff, wang2023lavie}, they still primarily rely on coarse-grained text prompts for semantic control, which limits their ability to manipulate finer details in the generated content, particularly in complex scenes where user intentions may be difficult to achieve.

\begin{figure*}[htbp]
    \centering
\includegraphics[width=0.95\textwidth]{images/framework3.pdf}
    \caption{
    TransVDM Framework. The TVAE is trained during Stage 1, which is utilized in Stage 2 to produce adjusted noisy latents with alpha information. In Stage 1, the RGB channels receive a smoothing operation to mitigate abrupt visual artifacts that may occur when alpha edges are inaccurately predicted, allowing the transparent areas of the original image to blend more seamlessly. The Stage 2 diagram illustrates a U-Net based VDM, the concatenation of the conditioned frame and adjusted noisy latents, along with a CLIP text encoder and a simplified processing flow of the AMCM module. The AMCM module actually comprises fully connected layers and a temporal attention layer. The input features from the temporal transformer are combined with $B$, where B represents the normalized coordinates of the bounding boxes for each input frame, and processed through the AMCM module before being passed back to the temporal transformer.
    % The Stage 2 diagram illustrates a simplified processing flow of the AMCM module, which actually comprises fully connected layers and a temporal attention layer. The input features from the temporal transformer are combined with POS and processed through the AMCM module before being passed back to the temporal transformer.
    }
    \label{fig:framework}   
\end{figure*}

\section{Method}
Starting with a transparent image $I_t\in \mathbb{R}^{H \times W \times 4}$, we denote the first 3 RGB channels as $I_c \in \mathbb{R}^{H \times W \times 3}$ and the alpha channel as $I_\alpha \in \mathbb{R}^{H \times W \times 1}$. We first integrate the alpha channel $I_\alpha$ into the RGB latent space by leveraging a Transparent Variational Autoencoder (TVAE). Subsequently, we combine a pretrained image-text-to-video model which is based on AnimateAnything \cite{dai2023fine}, with our proposed Alpha Motion Constraint Module (AMCM) to finalize the TransVDM framework, as shown in Fig.\ref{fig:framework}. In section A, we provide a brief preliminary of video diffusion models, followed by an overview of TVAE in section B, a detailed explanation of AMCM in section C, and a description of our dataset in section D. 
\subsection{Preliminary}
In this section, we introduce the preliminary concepts of latent video diffusion model (LVDM) \cite{he2022latent}. Given an image $ x_0=I_c \in \mathbb{R}^{H \times W \times 3}$, the process begins by utilizing a pretrained 
vanilla Variational Autoencoder (VAE) to encode the raw image $x_0$ into the latent space $ z_0 \in \mathbb{R}^{c \times h \times w}$, which could be reconstructed by the VAE decoder. The diffusion model is optimized to predict the added noise to $z_0$ via the following objective function:
\begin{equation}
    \mathcal{L}_\epsilon = \left\|\epsilon - \epsilon_\theta(z_t, t, c)\right\|_2^2,
\end{equation}
where $c$ represents the image and text prompt,  $\epsilon_\theta$ is the noise prediction function and $z_t$ is obtained by t-step addition of Gaussian noise to $z_0$:
\begin{equation}
       z_t = \sqrt{\bar{\alpha}_t} z_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon, \quad \epsilon \sim \mathcal{N}(0, I),
\end{equation}
where $t\in[0, T]$ is the time step, $\bar{\alpha}_t = \prod_{i=1}^{t} (1 - \beta_i)$ and $\beta_t$ is a coefficient with respect to t.

\subsection{Transparent Variational Autoencoder}
As shown in Fig.\ref{fig:framework} stage 1, our Transparent Variational Autoencoder (TVAE) integrates the alpha channel by separately encoding it and adding it into the RGB latent. This design ensures the alpha embedding introduces a minimal perturbation to the latent space of the vanilla VAE, avoiding changes to the overall distribution of the latent space. This enables the VAE process to transform from supporting RGB-only images to accommodating transparency information.

The TVAE consists of an encoder and a decoder. Specifically, the TVAE encoder is a simple multilayer Convolutional Neural Network (CNN) \cite{lecun1998gradient} that downscales the image by a factor of 8 in both width and height, resulting in the same dimensionality of the transparent latent $z_\alpha$ with the RGB latent $z$. The decoder mirrors that of the U-Net architecture. The TVAE is trained with two reconstruction objectives: the identity reconstruction loss and the transparency reconstruction loss. The identity loss only affects the weights of the TVAE encoder, ensuring the adjusted latent distribution remains intact, thereby enabling the vanilla VAE to accurately reconstruct RGB images. In addition, the transparency reconstruction loss is employed to jointly optimize the TVAE encoder and decoder. This yields the core TVAE procedure, which operates through the following steps:
\begin{itemize}
\item Latent Encoding: the TVAE encoder generates the latent $z_\alpha$ given an RGB image $I_c$ and the corresponding alpha channel $I_\alpha$:
\begin{equation}
    z_\alpha = \mathcal{E}_\text{{TVAE}}(I_c, I_\alpha),
\end{equation}

\item Latent Decoding: The adjusted latent $z_\text{{adj}} = z + z_\alpha$ paired with the original RGB reconstruction $\hat{I}$ is decoded back to the reconstructed transparent image:
\begin{equation}
       [\hat{I}_c, \hat{I}_\alpha] = \mathcal{D}_\text{{TVAE}}(\hat{I}, z_\text{{adj}}),
\end{equation}
where $\hat{I}_c$ and $\hat{I}_\alpha$ are the reconstructed RGB and alpha channels.

The identity loss is expressed as:
\begin{equation}
    \mathcal{L}_\text{{identity}} = ||I - \hat{I}||_2^2 = ||I - \mathcal{D}_\text{{VAE}}^*(\mathcal{E}_\text{{VAE}}^*(I)+z_{\alpha})||_2^2,
\end{equation}
where * denotes frozen models. Similarly, the transparency reconstruction loss is represented as: 
\begin{equation}
      \mathcal{L}_\text{{recon}} = ||I_c - \hat{I}_c||_2^2 + ||I_\alpha - \hat{I}_\alpha||_2^2,
\end{equation}
The final training objective is the weighted sum of two loss functions:
\begin{equation}
    \mathcal{L}_\text{{TVAE}} = \mathcal{L}_\text{{recon}} + \lambda \mathcal{L}_\text{{identity}},
\end{equation}
where $\lambda$ is a hyper-parameter.
\end{itemize}

\subsection{Alpha Motion Constraint Module}
As illustrated in Fig.\ref{fig:framework} stage 2, the backbone network is a frozen VDM. The Alpha Motion Constraint Module (AMCM) module functions as a fusion mechanism within each block of the U-Net architecture. It integrates the normalized coordinates of the bounding boxes that enclose the foreground of each alpha frame, treating them as constraint parameters that restrict the regions where motion occurs. Note that the AMCM module is comprised of several fully connected layers and a temporal attention mechanism.

Specifically, during training, the normalized coordinates of the top-left and bottom-right corners of the bounding boxes are utilized as the constraint parameters. During inference, the constraint parameters default to the values derived from the given image, meaning that the these parameters are the same for each frame. The AMCM is designed as a lightweight yet effective component. It accepts a sequence of normalized coordinates  $B \in \mathbb{R}^{N \times 4}$, where $N$ is the number of frames in the input video. The normalized coordinates are defined as:
\begin{equation}
    B = \{(x_{\text{min}}^i, y_{\text{min}}^i, x_{\text{max}}^i, y_{\text{max}}^i)\}_{i=1}^{N},
\end{equation}
where $(x_\text{{min}}^i,y_\text{{min}}^i,x_\text{{max}}^i,y_\text{{max}}^i)$ represents the coordinates of the bounding box corners for the $i$-th frame. Within the AMCM module, $B$ is repeated and concatenated with the feature maps $F$ prior to the temporal transformer. The concatenated features in the $l$-th block are represented as:
\begin{equation}
    {F}_\text{{cat}}^{(l)} = \text{concat}({F}^{(l)}, B),
\end{equation}
Subsequently, the concatenated features undergo processing through the fully connected layers, followed by the temporal attention mechanism inside the AMCM. The output features are then fed into the temporal transformer. Thus, the AMCM serves as a plugin that directly connects the spatial transformer and the temporal transformer.

\subsection{Dataset}
Our dataset for transparent video generation consists of transparent images and videos. The former data is primarily collected from the Internet, comprising images mostly handcrafted by designers, all of which are licensed for research purpose. These high-quality images feature fine edges and cover a diverse range of categories, including animals, humans, and objects. We employ a thorough cleaning process, which involves removing images with entirely white alpha channels, filtering out images with resolutions below 100x100, eliminating text-only images, and manually excluding non-compliant images. Through this process, we obtain 100K transparent images.

In contrast, acquiring high-quality transparent video data presents greater challenges. Therefore, we leverage the open-source video segmentation dataset (Youtube VOS \cite{xu2018youtube}) utilizing approximately 3K videos. These are then segmented by object ID, resulting in around 10K transparent video clips, totaling approximately 150K frames. For each video, we extract the middle frame and use an image captioning model (CoCa \cite{yu2022coca}) to generate captions, creating video-text pairs for training the AMCM module.
% \subsection{Implementation Details}
% We utilize the AdamW optimizer with a learning rate of 3e-5 for both the TVAE and the AMCM training. The TVAE is trained on a single A10 GPU for 100K iterations with a batch size of 4, while the AMCM is trained on 4 A10 GPUs with a batch size of 16 for 3K iterations. All training data is resized to a resolution of 384x384, and videos are sampled at 8 frames. The loss scaling factor $\lambda$ is set to 1.


\section{Experiments}
\subsection{Experimental Setup}
We utilize the AdamW optimizer with a learning rate of 3e-5 for both the TVAE and the AMCM training. The TVAE is trained on a single A10 GPU for 100K iterations with a batch size of 4, while the AMCM is trained on 4 A10 GPUs with a batch size of 16 for 3K iterations. All training data is resized to a resolution of 384x384, and videos are sampled at 16 frames. The loss scaling factor $\lambda$ is set to 1.

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.47\textwidth]{images/qua_exp_4.pdf}
    \caption{
        Qualitative Results. The input includes two image-text pairs, with the corresponding texts being ``a man smiling" and ``the candle is lit up".
    }
    \label{fig:qua_exp}   
\end{figure}

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.47\textwidth]{images/morecases_2.pdf}
    \caption{
        More cases generated by TransVDM, with the first column showing the conditioned images and the following columns displaying the generated outputs. The prompts are ``a cat turns head", ``Barbie blinking her eyes", and ``the apple is swaying in the wind".
    }
    \label{fig:morecase}   
\end{figure}

\subsection{Qualititive Results}
We present the results of our proposed TransVDM model compared to traditional post-processing methods (VDM+SAM2) in Fig.\ref{fig:qua_exp}. The first column showcases the conditioned images, while the subsequent columns illustrate three generated transparent frames. Our approach directly generates transparent videos from the given transparent image and text prompt. In contrast, the post-processing method requires two steps: first, it replaces the transparent regions of the input image with a specific background to produce an RGB image, which is then processed using the vanilla VDM model to generate a video, followed by removing the background. For the post-processing method, we employ a green screen technique akin to that used in the film industry, filling the transparent images with a green background to convert them into RGB format. To eliminate the background from the generated frames, common methods include segmentation (SAM2 \cite{ravi2024sam}), matting (PPMatting \cite{chen2022pp}), and chroma key. Segmentation and matting are automated algorithms, whereas chroma key necessitates manual threshold adjustments to match the green pixel values within a specified range. The results from these three methods are similar, often leaving green backgrounds around the edges of the foreground. As shown in Fig.\ref{fig:qua_exp}, we display the results of using SAM2. We observe that our method produces cleaner results, confirming the advantages of TransVDM over conventional post-processing techniques. Lastly, we show more results generated by TransVDM in Fig.\ref{fig:morecase}.

\begin{table}[!t]
% \setlength{\tabcolsep}{10pt}
\renewcommand{\arraystretch}{1.3}
\centering
\caption{Ablation Study of AMCM}
\begin{tabular}{ccc}
\hline
Method   & FVD~($\downarrow$) & LPIPS~($\downarrow$) \\ \hline
w/o AMCM & 680 & 0.222 \\
w/ AMCM   & \textbf{614} & \textbf{0.219} \\ \hline
\end{tabular}
\label{tab:study}
\end{table}

\subsection{Ablation Study}
In this section, we conduct an ablation study on a self-constructed test dataset, which consists of 100 manually collected transparent video-text pairs. We adopt two commonly used metrics in video generation: FVD and LPIPS \cite{yang2023video}. FVD evaluates the temporal quality of generated videos while LPIPS assesses the spatial visual quality of the generated results. Notably, we convert the transparent videos to RGB format for metric calculation.

As shown in Tab.\ref{tab:study} , our method demonstrates a significant improvement in terms of FVD compared to the vanilla VDM without the AMCM module, while the LPIPS remain comparable. This indicates training the AMCM module substantially enhances the performance of the generated videos, thereby confirming the effectiveness of our method.


% \section{Future Work}
% In this paper, we utilize the AMCM framework, where the motion constraint for each frame is implicitly derived from the first frame. In the future, we plan to explore the possibility of manually inputting motion constraints to enable more controllable video generation. Additionally, it is crucial to enhance our dataset, both in terms of quality and quantity, to improve the robustness of our methods. We also intend to investigate more powerful video diffusion model (VDM) foundations, such as SVD \cite{blattmann2023stable} and Diffusion Transformer based model \cite{hong2022cogvideo}, to further elevate the performance of our framework.

\section{conclusion and future work}
We propose the first diffusion model for transparent video generation, which incorporates a Transparent Variational Autoencoder (TVAE), a vanilla Variational Diffusion Model (VDM), and an Alpha Motion Constraint Module (AMCM). Additionally, we have built a dataset of 250K samples to train the TVAE and AMCM. Extensive experiments demonstrate the superiority of our method. In future work, we plan to explore the possibility of manually inputting motion constraints during inference for more controllable video generation, while also enhancing our dataset regarding quality and quantity to improve the robustness of our method. Moreover, we intend to investigate more powerful VDM foundations, such as SVD \cite{blattmann2023stable} and Diffusion Transformer based models \cite{peebles2023scalable,hong2022cogvideo,zhang2024tora}, to further elevate the performance of our framework.

% We propose the first diffusion model for transparent video generation, which incorporates a TVAE, a vanilla VDM, and an AMCM module. Additionally, we build a dataset of 250K samples for training the TVAE and AMCM.  Extensive experiments demonstrate
% the superiority of our method. Furthermore, our approach paves the way for advancements in this field and serves as a crucial basis for subsequent research efforts. 



% In the future, we aim to investigate the potential of the AMCM for generating controllable videos, enhance our dataset with higher quality and greater quantity, and explore methods for more flexible motion constraints.







% The IEEEtran class file is used to format your paper and style the text. All margins, 
% column widths, line spaces, and text fonts are prescribed; please do not 
% alter them. You may note peculiarities. For example, the head margin
% measures proportionately more than is customary. This measurement 
% and others are deliberate, using specifications that anticipate your paper 
% as one part of the entire proceedings, and not as an independent document. 
% Please do not revise any of the current designations.

% \section{Prepare Your Paper Before Styling}
% Before you begin to format your paper, first write and save the content as a 
% separate text file. Complete all content and organizational editing before 
% formatting. Please note sections \ref{AA}--\ref{SCM} below for more information on 
% proofreading, spelling and grammar.

% Keep your text and graphic files separate until after the text has been 
% formatted and styled. Do not number text heads---{\LaTeX} will do that 
% for you.

% \subsection{Abbreviations and Acronyms}\label{AA}
% Define abbreviations and acronyms the first time they are used in the text, 
% even after they have been defined in the abstract. Abbreviations such as 
% IEEE, SI, MKS, CGS, ac, dc, and rms do not have to be defined. Do not use 
% abbreviations in the title or heads unless they are unavoidable.

% \subsection{Units}
% \begin{itemize}
% \item Use either SI (MKS) or CGS as primary units. (SI units are encouraged.) English units may be used as secondary units (in parentheses). An exception would be the use of English units as identifiers in trade, such as ``3.5-inch disk drive''.
% \item Avoid combining SI and CGS units, such as current in amperes and magnetic field in oersteds. This often leads to confusion because equations do not balance dimensionally. If you must use mixed units, clearly state the units for each quantity that you use in an equation.
% \item Do not mix complete spellings and abbreviations of units: ``Wb/m\textsuperscript{2}'' or ``webers per square meter'', not ``webers/m\textsuperscript{2}''. Spell out units when they appear in text: ``. . . a few henries'', not ``. . . a few H''.
% \item Use a zero before decimal points: ``0.25'', not ``.25''. Use ``cm\textsuperscript{3}'', not ``cc''.)
% \end{itemize}

% \subsection{Equations}
% Number equations consecutively. To make your 
% equations more compact, you may use the solidus (~/~), the exp function, or 
% appropriate exponents. Italicize Roman symbols for quantities and variables, 
% but not Greek symbols. Use a long dash rather than a hyphen for a minus 
% sign. Punctuate equations with commas or periods when they are part of a 
% sentence, as in:
% \begin{equation}
% a+b=\gamma\label{eq}
% \end{equation}

% Be sure that the 
% symbols in your equation have been defined before or immediately following 
% the equation. Use ``\eqref{eq}'', not ``Eq.~\eqref{eq}'' or ``equation \eqref{eq}'', except at 
% the beginning of a sentence: ``Equation \eqref{eq} is . . .''

% \subsection{\LaTeX-Specific Advice}

% Please use ``soft'' (e.g., \verb|\eqref{Eq}|) cross references instead
% of ``hard'' references (e.g., \verb|(1)|). That will make it possible
% to combine sections, add equations, or change the order of figures or
% citations without having to go through the file line by line.

% Please don't use the \verb|{eqnarray}| equation environment. Use
% \verb|{align}| or \verb|{IEEEeqnarray}| instead. The \verb|{eqnarray}|
% environment leaves unsightly spaces around relation symbols.

% Please note that the \verb|{subequations}| environment in {\LaTeX}
% will increment the main equation counter even when there are no
% equation numbers displayed. If you forget that, you might write an
% article in which the equation numbers skip from (17) to (20), causing
% the copy editors to wonder if you've discovered a new method of
% counting.

% {\BibTeX} does not work by magic. It doesn't get the bibliographic
% data from thin air but from .bib files. If you use {\BibTeX} to produce a
% bibliography you must send the .bib files. 

% {\LaTeX} can't read your mind. If you assign the same label to a
% subsubsection and a table, you might find that Table I has been cross
% referenced as Table IV-B3. 

% {\LaTeX} does not have precognitive abilities. If you put a
% \verb|\label| command before the command that updates the counter it's
% supposed to be using, the label will pick up the last counter to be
% cross referenced instead. In particular, a \verb|\label| command
% should not go before the caption of a figure or a table.

% Do not use \verb|\nonumber| inside the \verb|{array}| environment. It
% will not stop equation numbers inside \verb|{array}| (there won't be
% any anyway) and it might stop a wanted equation number in the
% surrounding equation.

% \subsection{Some Common Mistakes}\label{SCM}
% \begin{itemize}
% \item The word ``data'' is plural, not singular.
% \item The subscript for the permeability of vacuum $\mu_{0}$, and other common scientific constants, is zero with subscript formatting, not a lowercase letter ``o''.
% \item In American English, commas, semicolons, periods, question and exclamation marks are located within quotation marks only when a complete thought or name is cited, such as a title or full quotation. When quotation marks are used, instead of a bold or italic typeface, to highlight a word or phrase, punctuation should appear outside of the quotation marks. A parenthetical phrase or statement at the end of a sentence is punctuated outside of the closing parenthesis (like this). (A parenthetical sentence is punctuated within the parentheses.)
% \item A graph within a graph is an ``inset'', not an ``insert''. The word alternatively is preferred to the word ``alternately'' (unless you really mean something that alternates).
% \item Do not use the word ``essentially'' to mean ``approximately'' or ``effectively''.
% \item In your paper title, if the words ``that uses'' can accurately replace the word ``using'', capitalize the ``u''; if not, keep using lower-cased.
% \item Be aware of the different meanings of the homophones ``affect'' and ``effect'', ``complement'' and ``compliment'', ``discreet'' and ``discrete'', ``principal'' and ``principle''.
% \item Do not confuse ``imply'' and ``infer''.
% \item The prefix ``non'' is not a word; it should be joined to the word it modifies, usually without a hyphen.
% \item There is no period after the ``et'' in the Latin abbreviation ``et al.''.
% \item The abbreviation ``i.e.'' means ``that is'', and the abbreviation ``e.g.'' means ``for example''.
% \end{itemize}
% An excellent style manual for science writers is \cite{b7}.

% \subsection{Authors and Affiliations}
% \textbf{The class file is designed for, but not limited to, six authors.} A 
% minimum of one author is required for all conference articles. Author names 
% should be listed starting from left to right and then moving down to the 
% next line. This is the author sequence that will be used in future citations 
% and by indexing services. Names should not be listed in columns nor group by 
% affiliation. Please keep your affiliations as succinct as possible (for 
% example, do not differentiate among departments of the same organization).

% \subsection{Identify the Headings}
% Headings, or heads, are organizational devices that guide the reader through 
% your paper. There are two types: component heads and text heads.

% Component heads identify the different components of your paper and are not 
% topically subordinate to each other. Examples include Acknowledgments and 
% References and, for these, the correct style to use is ``Heading 5''. Use 
% ``figure caption'' for your Figure captions, and ``table head'' for your 
% table title. Run-in heads, such as ``Abstract'', will require you to apply a 
% style (in this case, italic) in addition to the style provided by the drop 
% down menu to differentiate the head from the text.

% Text heads organize the topics on a relational, hierarchical basis. For 
% example, the paper title is the primary text head because all subsequent 
% material relates and elaborates on this one topic. If there are two or more 
% sub-topics, the next level head (uppercase Roman numerals) should be used 
% and, conversely, if there are not at least two sub-topics, then no subheads 
% should be introduced.

% \subsection{Figures and Tables}
% \paragraph{Positioning Figures and Tables} Place figures and tables at the top and 
% bottom of columns. Avoid placing them in the middle of columns. Large 
% figures and tables may span across both columns. Figure captions should be 
% below the figures; table heads should appear above the tables. Insert 
% figures and tables after they are cited in the text. Use the abbreviation 
% ``Fig.~\ref{fig}'', even at the beginning of a sentence.

% \begin{table}[htbp]
% \caption{Table Type Styles}
% \begin{center}
% \begin{tabular}{|c|c|c|c|}
% \hline
% \textbf{Table}&\multicolumn{3}{|c|}{\textbf{Table Column Head}} \\
% \cline{2-4} 
% \textbf{Head} & \textbf{\textit{Table column subhead}}& \textbf{\textit{Subhead}}& \textbf{\textit{Subhead}} \\
% \hline
% copy& More table copy$^{\mathrm{a}}$& &  \\
% \hline
% \multicolumn{4}{l}{$^{\mathrm{a}}$Sample of a Table footnote.}
% \end{tabular}
% \label{tab1}
% \end{center}
% \end{table}

% \begin{figure}[htbp]
% \centerline{\includegraphics{fig1.png}}
% \caption{Example of a figure caption.}
% \label{fig}
% \end{figure}

% Figure Labels: Use 8 point Times New Roman for Figure labels. Use words 
% rather than symbols or abbreviations when writing Figure axis labels to 
% avoid confusing the reader. As an example, write the quantity 
% ``Magnetization'', or ``Magnetization, M'', not just ``M''. If including 
% units in the label, present them within parentheses. Do not label axes only 
% with units. In the example, write ``Magnetization (A/m)'' or ``Magnetization 
% \{A[m(1)]\}'', not just ``A/m''. Do not label axes with a ratio of 
% quantities and units. For example, write ``Temperature (K)'', not 
% ``Temperature/K''.

% \section*{Acknowledgment}

% The preferred spelling of the word ``acknowledgment'' in America is without 
% an ``e'' after the ``g''. Avoid the stilted expression ``one of us (R. B. 
% G.) thanks $\ldots$''. Instead, try ``R. B. G. thanks$\ldots$''. Put sponsor 
% acknowledgments in the unnumbered footnote on the first page.

% \section*{References}

% Please number citations consecutively within brackets \cite{b1}. The 
% sentence punctuation follows the bracket \cite{b2}. Refer simply to the reference 
% number, as in \cite{b3}---do not use ``Ref. \cite{b3}'' or ``reference \cite{b3}'' except at 
% the beginning of a sentence: ``Reference \cite{b3} was the first $\ldots$''

% Number footnotes separately in superscripts. Place the actual footnote at 
% the bottom of the column in which it was cited. Do not put footnotes in the 
% abstract or reference list. Use letters for table footnotes.

% Unless there are six authors or more give all authors' names; do not use 
% ``et al.''. Papers that have not been published, even if they have been 
% submitted for publication, should be cited as ``unpublished'' \cite{b4}. Papers 
% that have been accepted for publication should be cited as ``in press'' \cite{b5}. 
% Capitalize only the first word in a paper title, except for proper nouns and 
% element symbols.

% For papers published in translation journals, please give the English 
% citation first, followed by the original foreign-language citation \cite{b6}.



% \begin{thebibliography}{00}
% \bibitem{b1} G. Eason, B. Noble, and I. N. Sneddon, ``On certain integrals of Lipschitz-Hankel type involving products of Bessel functions,'' Phil. Trans. Roy. Soc. London, vol. A247, pp. 529--551, April 1955.
% \bibitem{b2} J. Clerk Maxwell, A Treatise on Electricity and Magnetism, 3rd ed., vol. 2. Oxford: Clarendon, 1892, pp.68--73.
% \bibitem{b3} I. S. Jacobs and C. P. Bean, ``Fine particles, thin films and exchange anisotropy,'' in Magnetism, vol. III, G. T. Rado and H. Suhl, Eds. New York: Academic, 1963, pp. 271--350.
% \bibitem{b4} K. Elissa, ``Title of paper if known,'' unpublished.
% \bibitem{b5} R. Nicole, ``Title of paper with only first word capitalized,'' J. Name Stand. Abbrev., in press.
% \bibitem{b6} Y. Yorozu, M. Hirano, K. Oka, and Y. Tagawa, ``Electron spectroscopy studies on magneto-optical media and plastic substrate interface,'' IEEE Transl. J. Magn. Japan, vol. 2, pp. 740--741, August 1987 [Digests 9th Annual Conf. Magnetics Japan, p. 301, 1982].
% \bibitem{b7} M. Young, The Technical Writer's Handbook. Mill Valley, CA: University Science, 1989.
% \end{thebibliography}

\clearpage
\bibliographystyle{abbrv}
\bibliography{references}
\end{document}
