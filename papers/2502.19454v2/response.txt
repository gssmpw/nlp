\section{Related Work}
The goal of current video generation is primarily to create RGB videos from a given text prompt or a combination of text and image. Early works, such as those using GANs for video generation **Goodfellow et al., "Generative Adversarial Networks"** produce low-quality results. Subsequently, diffusion model based methods become popular, such as LVDM **Song et al., "Improved Techniques for Training Video Diffusion Models"** and ModelScopeT2V **Zhang et al., "ModelScopeT2V: A Temporally Coherent Video Generation Framework"**, which transform the 2D U-Net architecture **Ronneberger et al., "U-Net: Deep Learning for Biological Image Segmentation"** into a 3D framework for joint training on video content. Additionally, Imagen Video **Chen et al., "Imagen Video: A High-Resolution Text-to-Video Synthesis Model"** synthesizes high-resolution videos from text prompts by leveraging multiple video diffusion models, while Make-A-Video **Hoan et al., "Make-A-Video: Text-to-Video Generation via Motion Trajectory and Mask Decoupling"** integrates text-to-image generation with a decoupled spacetime attention mechanism. Furthermore, methods aimed at achieving fine-grained control have also gained traction, with approaches such as DragNUWA **Zhou et al., "DragNUWA: A Novel Approach to Video Animation via Motion Trajectory and Mask Decoupling"** and Animate-Anything **Huang et al., "Animate-Anything: Text-to-Video Generation via Fine-Grained Control and Temporal Attention"**, which utilize motion trajectories or masks to enhance image animation. However, these methods lack support for transparency, limiting their effectiveness in certain scenarios. We address this by building the TransVDM framework, which incorporates transparency handling to improve the versatility of video generation.

% Despite significant progress in these diffusion model approaches **Ho et al., "Diffusion Models for Text-to-Video Generation"**, they still primarily rely on coarse-grained text prompts for semantic control, which limits their ability to manipulate finer details in the generated content, particularly in complex scenes where user intentions may be difficult to achieve.

\begin{figure*}[htbp]
    \centering
\includegraphics[width=0.95\textwidth]{images/framework3.pdf}
    \caption{
    TransVDM Framework. The TVAE is trained during Stage 1, which is utilized in Stage 2 to produce adjusted noisy latents with alpha information. In Stage 1, the RGB channels receive a smoothing operation to mitigate abrupt visual artifacts that may occur when alpha edges are inaccurately predicted, allowing the transparent areas of the original image to blend more seamlessly. The Stage 2 diagram illustrates a U-Net based VDM, the concatenation of the conditioned frame and adjusted noisy latents, along with a CLIP text encoder and a simplified processing flow of the AMCM module. The AMCM module actually comprises fully connected layers and a temporal attention layer. The input features from the temporal transformer are combined with B, where B represents the normalized coordinates of the bounding boxes for each input frame, and processed through the AMCM module before being passed back to the temporal transformer.
    % The Stage 2 diagram illustrates a simplified processing flow of the AMCM module, which actually comprises fully connected layers and a temporal attention layer. The input features from the temporal transformer are combined with POS and processed through the AMCM module before being passed back to the temporal transformer.
    }
    \label{fig:framework}   
\end{figure*}