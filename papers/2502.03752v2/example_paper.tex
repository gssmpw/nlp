%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}
\usepackage{multirow}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\def\tcr{\textcolor{red}}
\def\tcb{\textcolor{blue}}
\def\tcg{\textcolor{green}}
\def\tcp{\textcolor{violet}}
\def\tb{\textbf}
\def\ra{\rightarrow}
\def\la{\leftarrow}
\def\Cbb{\mathbb{C}}
\def\Fbb{{\mathbb{F}}}
\def\Pbb{{\mathbb{P}}}

\def\mc{\mathcal}
\def\mb{\mathbb}
% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:

\icmltitlerunning{PRISM: A Robust Framework for Skill-based Meta-Reinforcement Learning with Noisy Demonstrations}

\begin{document}

\twocolumn[
\icmltitle{PRISM: A Robust Framework for Skill-based Meta-Reinforcement Learning with Noisy Demonstrations}
% \icmltitle{PRISM: Robust Skill-based Meta Reinforcement Learning with Prioritized Refinement under Noisy Demonstrations}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Sanghyeon Lee}{unist}
\icmlauthor{Sangjun Bae}{unist}
\icmlauthor{Yisak Park}{unist}
\icmlauthor{Seungyul Han}{unist} \hspace{-0.12in} $^*$
\end{icmlauthorlist}

\icmlaffiliation{unist}{Graduate School of Artificial Intelligence, UNIST, Ulsan, South Korea}
\icmlcorrespondingauthor{Seungyul Han}{syhan@unist.ac.kr}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]
% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.
\begin{abstract}
Meta-reinforcement learning (Meta-RL) facilitates rapid adaptation to unseen tasks but faces challenges in long-horizon environments. Skill-based approaches tackle this by decomposing state-action sequences into reusable skills and employing hierarchical decision-making. However, these methods are highly susceptible to noisy offline demonstrations, resulting in unstable skill learning and degraded performance. To overcome this, we propose Prioritized Refinement for Skill-Based Meta-RL (PRISM), a robust framework that integrates exploration near noisy data to generate online trajectories and combines them with offline data. Through prioritization, PRISM extracts high-quality data to learn task-relevant skills effectively. By addressing the impact of noise, our method ensures stable skill learning and achieves superior performance in long-horizon tasks, even with noisy and sub-optimal data.
\end{abstract}

\vspace{-2em}
\section{Introduction}
\label{sec:intro}
\begin{figure*}[ht]
    \centerline{\includegraphics[width=0.9\textwidth]{./images/1_motivation.pdf}}
    \caption{Sample trajectories in the Maze2D environment: (a) Noisy demonstrations from the offline dataset, (b) Trajectories explored by the exploration policy near the noisy dataset to uncover useful skills, and (c) Trajectories utilizing refined skills to solve unseen test tasks}
    \label{fig:motiv}
    \vskip -0.2in
\end{figure*}



Reinforcement Learning (RL) has achieved significant success in domains such as game environments and robotic control \cite{mnih2015human, andrychowicz2020learning}. However, it struggles to adapt quickly to new tasks. Meta-RL addresses this limitation by enabling rapid adaptation to unseen tasks through meta-learning how policies solve problems \cite{duan2016rl, finn2017model}. Among various approaches, context-based meta-RL stands out for its ability to represent similar tasks with analogous contexts and leverage this information in the policy, facilitating quick adaptation to new tasks \cite{rakelly2019efficient, zintgraf2019varibad}. Notably, PEARL \cite{rakelly2019efficient} has been widely studied for its high sample efficiency, achieved through off-policy learning, which allows for the reuse of previous samples. Despite these strengths, existing meta-RL methods face challenges in long-horizon environments, where extracting meaningful context information becomes difficult, hindering effective learning.

Skill-based approaches address these challenges by breaking down long state-action sequences into reusable skills, facilitating hierarchical decision-making and enhancing efficiency in complex tasks \cite{pertsch2021accelerating, pertsch2022guided, shi2023skill}. Among these, SPiRL \cite{pertsch2021accelerating} defines skills as temporal abstractions of actions, employing them as low-level policies within a hierarchical framework to achieve success in long-horizon tasks. SiMPL \cite{nam2022simpl} builds on this by extending skill learning to meta-RL, using offline expert data to train skills and a context-based high-level policy for task-specific skill selection. Despite these advancements, such methods are highly susceptible to noisy offline demonstrations, which can destabilize skill learning and reduce reliability. In real-world settings, noise often stems from factors like infrastructure aging or environmental perturbations, underscoring the importance of addressing this challenge \cite{brys2015reinforcement, chae2022robust, yu2024usn}.

While other RL domains have explored methods to handle noisy demonstrations \cite{sasaki2020behavioral, mandlekar2022matters}, skill-based learning has largely remained underexplored in this context. To tackle this gap, we propose Prioritized Refinement for Skill-Based Meta-RL (PRISM), a robust framework that incorporates two key contributions: (1) A prioritized skill refinement framework that leverages an exploration policy to discover useful trajectories near noisy offline data, extracting high-quality data through prioritization and employing disentangled skill learning to integrate skills from both online and offline datasets. (2) Maximum return relabeling, a novel technique for evaluating noisy offline trajectories by relabeling their returns based on task relevance, ensuring the selection of high-quality data. By dynamically balancing and prioritizing data from online and offline datasets, PRISM ensures stable and effective skill learning even in noisy environments. These innovations significantly enhance the robustness and generalizability of skill-based meta-RL, delivering reliable performance in real-world noisy scenarios.


Fig. \ref{fig:motiv} illustrates how the proposed algorithm learns effective skills from noisy demonstrations in the Maze2D environment, where the agent starts at a designated point and must reach an endpoint for each task. Fig. \ref{fig:motiv}(a) shows noisy offline trajectories, which fail to produce effective skills when used directly. In contrast, Fig. \ref{fig:motiv}(b) demonstrates how the prioritized refinement framework uses the exploration policy to navigate near noisy trajectories, identifying paths critical for solving long-horizon tasks and refining useful skills through prioritization. Finally, Fig. \ref{fig:motiv}(c) shows how the high-level policy applies these refined skills to successfully solve unseen tasks. These results highlight the method’s ability to refine and prioritize skills from noisy datasets, ensuring stable learning and enabling the resolution of long-horizon tasks in unseen environments. This paper is organized as follows: Section \ref{sec:background} provides an overview of meta-RL and skill learning, Section \ref{sec:method} details the proposed framework, and Section \ref{sec:exp} presents experimental results showcasing the framework’s robustness and effectiveness, along with an ablation study of key components.

\section{Related Works}
\label{sec:related}
\textbf{Skill-based Reinforcement Learning: }Skill-based RL has gained traction for tackling complex tasks by leveraging temporally extended actions. Researchers have proposed information-theoretic approaches to discover diverse and predictable skills \cite{gregor2016variational, eysenbachdiversity, achiam2018variational, sharmadynamics}, with recent work improving skill quality through additional constraints and objectives \cite{strouselearning, park2022lipschitz, park2023controllability, hudisentangled}. In offline scenarios, approaches focus on learning transferable behavior priors and hierarchical skills from demonstration data \cite{pertsch2021accelerating, pertsch2022guided, xu2022aspire, shi2023skill}. Building upon these foundations, various skill-based meta-RL approaches have been developed, from hierarchical and embedding-based methods \cite{nam2022simpl, chien2023variational, cho2024hierarchical} to task decomposition strategies \cite{yoo2022skills, he2024decoupling} and unsupervised learning frameworks \cite{gupta2018unsupervised, jabri2019unsupervised, shin2024semtra}.


\textbf{Hierarchical Frameworks: }Hierarchical approaches in RL have been pivotal for solving long-horizon tasks, where various methods have been proposed including goal-conditioned learning \cite{levy2019learning, li2019hierarchical, gehring2021hierarchical}, and option-based frameworks \cite{bacon2017option, riemer2018learning, barreto2019option, araki2021logical}. Recent advances in goal-conditioned RL have focused on improving sample efficiency \cite{robert2024sample}, offline learning \cite{park2024hiql}, and robust state representations \cite{yin2024representation}. The integration of hierarchical frameworks with meta-RL has shown significant potential for rapid task adaptation and complexity handling \cite{frans2018meta, fu2020mghrl, fu2023meta}. Recent work has demonstrated that hierarchical architectures in meta-RL can provide theoretical guarantees for learning optimal policies \cite{chua2023provable} and achieve efficient learning through transformer-based architectures \cite{shala2024hierarchical}.


\textbf{Relabeling Techniques for Meta-RL: }Recent developments in meta-RL have introduced various relabeling techniques to enhance sample efficiency and task generalization \cite{pong2022offline, jiang2023doubly}. Goal relabeling approaches have extended hindsight experience replay to meta-learning contexts \cite{packer2021hindsight, wanhindsight}, enabling agents to learn from failed attempts. For reward relabeling, model-based approaches have been proposed to relabel experiences across different tasks \cite{mendonca2020meta}, improving adaptation to out-of-distribution scenarios. Beyond these categories, some methods have introduced innovative relabeling strategies using contrastive learning \cite{yuan2022robust, zhou2024generalizable} and metric-based approaches \cite{li2020multi} to create robust task representations in offline settings.


\section{Background}
\label{sec:background}


\vspace{-.5em}
\subsection{Meta-Reinforcement Learning Setup}
\label{subsec:metarl}

In meta-RL, each task $\mathcal{T}$ follows a task distribution $p(\mathcal{T})$ and is defined as a Markov Decision Process (MDP) environment $\mathcal{M}^\mathcal{T}=\bigl(\mathcal{S}, \mathcal{A}, R^\mathcal{T},P^\mathcal{T},\gamma\bigr)$, where $\mathcal{S}\times\mathcal{A}$ represents the shared state-action space, $R^\mathcal{T}$ denotes the reward function, $P^\mathcal{T}$ is the state transition probability, and $\gamma$ is the discount factor. At each time $t$, the agent selects an action $a_t$ using the policy $\pi$, receives a reward $r_t=R^\mathcal{T}(s_t,a_t)$, and transitions to the next state $s_{t+1}~\sim P^\mathcal{T}(\cdot|s_t,a_t)$. The objective of meta-RL is to train a policy $\pi$ to maximize the return $G=\sum_t \gamma^t r_t$ for the training task set $\mathcal{M}_{\mathrm{train}}$ while enabling the policy to rapidly adapt to unseen test task set $\mathcal{M}_{\mathrm{test}}$, where $\mathcal{M}_{\mathrm{train}} \cap \mathcal{M}_{\mathrm{test}} = \emptyset$.

\vspace{-.5em}

\subsection{Offline Dataset and Skill Learning}
\label{subsec:offksill}

To address long-horizon tasks, skill learning from an offline dataset $\mathcal{B}_\text{off}:=\{\tilde{\tau}_{0:H}\}$ is considered, which comprises sample trajectories $\tilde{\tau}_{t:t+k}:=(s_t,a_t,\cdots,s_{t+k})$ without reward information, where $H$ is the episode length. The dataset $\mathcal{B}_\text{off}$ is typically collected through human interactions or pretrained policies. Among various skill learning methods, SPiRL \cite{pertsch2021accelerating} focuses on learning a reusable low-level policy $\pi_l$, using $q(\cdot|\tilde{\tau}_{t:t+H_s})$ as a skill encoder to extract the skill latent $z$ by minimizing the following loss function:
\begin{equation}
\vspace{-.5em}
\mathbb{E}_{\substack{\tilde{\tau}_{t:t+H_s}\sim\mathcal{B}_\text{off},\\z\sim q(\cdot|\tilde{\tau}_{t:t+H_s})}}\left[\mathcal{L}(\pi_l,q,p,z)\right]
\label{eq:spirl},
\end{equation}
\ where $\mathcal{L}(\pi_l,q,p,z):=-\sum^{t+H_s-1}_{k=t} \log \pi_l(a_k|s_k,z)+ \lambda^\text{kld}_l\mathcal{D}_\text{KL}\left(q||\mathcal{N}(\mathbf{0}, \mathbf{I})\right) + \mathcal{D}_\text{KL}(\lfloor q\rfloor||p)$, $H_s$ is the skill length, $\lambda^\text{kld}_l$ is the coefficient for KL divergence (KLD) $\mathcal{D}_{\text{KL}}$, $\lfloor\cdot\rfloor$ is the stop gradient operator, and $\mathcal{N}(\boldsymbol{\mu},\mathbf{\Sigma})$ represents a Normal distribution with mean $\boldsymbol{\mu}$ and covariance matrix $\mathbf{I}$. Here, $p(z|s_t)$ is the skill prior to obtain the skill distribution $z$ for a given state $s_t$ directly. Using the learned skill policy $\pi_l$, the high-level policy $\pi_h$ is trained within a hierarchical framework using standard RL methods.


\vspace{-.5em}

\subsection{Skill-based Meta-Reinforcement Learning}
\label{subsec:skillmetarl}


SiMPL \cite{nam2022simpl} integrates skill learning into meta-RL by utilizing an offline dataset of expert demonstrations across various tasks. The skill policy $\pi_l$ is trained via SPiRL, while a task encoder $q_e$ extracts the task latent $e^\mathcal{T} \sim q_e$ using the PEARL \cite{rakelly2019efficient} framework, a widely-used meta-RL method. During meta-training, the high-level policy $\pi_h(z|s,e^\mathcal{T})$ selects a skill latent $z$ and executes the skill policy $\pi_l(a|s,z)$ for $H_s$ time steps, optimizing $\pi_h$ to maximize the return for each task $\mathcal{T}$ as:
\begin{equation}
    \min_{\pi_h}\mathbb{E}_{\tau_h^\mathcal{T}\sim\mathcal{B}_h^\mathcal{T},e^\mathcal{T}\sim q_e(\cdot|c^\mathcal{T})}\Bigl[
    \mathcal{L}^{\mathrm{RL}}_h(\pi_h) + \lambda^\text{kld}_h \mathcal{D}_\text{KL}\bigl(
    \pi_h\,\vert\vert\,p
    \bigr)
    \Bigr],
\label{eq:simpl}
\end{equation}
where $\lambda^\text{kld}_h$ is the KL divergence coefficient, $c^\mathcal{T}$ represents the contexts of high-level trajectories $\tau_h^\mathcal{T}:=(s_0,z_0,\sum_{t=0}^{H_s-1} r_t, s_{H_s},z_{H_s},\sum_{t=H_s}^{2H_s-1} r_t,\cdots)$ for task $\mathcal{T}$, $\mathcal{L}^{\mathrm{RL}}_h$ denotes the RL loss for $\pi_h$, and $\mathcal{B}_h^\mathcal{T}=\{\tau_h^\mathcal{T}\}$ is the high-level buffer that stores $\tau_h^\mathcal{T}$ for each $\mathcal{T}\in\mathcal{M}_{\mathrm{train}}$. Here, the reward sums $\sum_{t=kH_s}^{(k+1)H_s-1} r_t$ are obtained via environment interactions of $a_t\sim\pi_l(\cdot|s_t,z_{kH_s})$ for $t=kH_s,\cdots,(k+1)H_s-1$ with $k=0,\cdots$.  During meta-test, the high-level policy is adapted using a limited number of samples, showing good performance on long-horizon tasks.



\section{Methodology}
\label{sec:method}
\subsection{Motivation: Challenges in Direct Skill Learning with Noisy Demonstrations}
\label{subsec:noisydemon}
\begin{figure}[ht]
    \centering
    \subfigure[]{\includegraphics[width=0.5\columnwidth]{images/4-1_kitchen-simulation.pdf}}%
    \subfigure[]{\includegraphics[width=0.5\columnwidth]{images/4-1_kitchen-performance.pdf}}%
    \vskip -0.1in
    \caption{Comparison of skill learning in the microwave-opening task using expert and noisy demonstrations: (a) Learned skills, (b) Meta-RL performance utilizing the learned skills}
    \label{fig:noisy}
    % \vskip -0.2in
\end{figure}


Existing skill-based meta-RL methods, as discussed in Section \ref{sec:background}, rely on offline datasets assumed to contain expert demonstrations. However, real-world datasets often suffer from action noise due to aging robots, environmental perturbations, or sensor errors. Unlike online setups that adapt through re-training, static offline datasets are vulnerable to noise. This significantly hinders skill learning, particularly in long-horizon tasks with cumulative errors and manipulation tasks requiring precision. Fig. \ref{fig:noisy}(a) compares skills learned from expert and noisy datasets for a microwave-opening task in the Kitchen environment. Skills from the expert dataset successfully complete the task, while those from noisy demonstrations fail to grasp or open the microwave. Fig. \ref{fig:noisy}(b) shows that SiMPL, a skill-based meta-RL method, performs poorly on test tasks when relying on skills learned from noisy data compared to expert data. This stems from existing methods learning indiscriminately from offline datasets, regardless of trajectory quality. To tackle the challenges posed by noisy offline datasets, we propose prioritized skill refinement, integrating online exploration and maximum return relabeling to dynamically identify and refine task-relevant skills. The following sections detail how the proposed methods leverage these components to ensure robust skill learning for meta-RL under noisy conditions.


\begin{figure}[t]
    % \vskip 0.2in
    \centerline{\includegraphics[width=\columnwidth]{images/4-2_structure.pdf}}
    \vspace{-1em}
    \caption{Structure of the proposed PRISM framework}
    % \end{center}
    \label{fig:structure}
    \vskip -0.2in
    % \vspace{-2em}
\end{figure}

\subsection{Prioritized Skill Refinement with Exploration}
\label{subsec:skillimprove}

In this section, we introduce the prioritized refinement for skill-based meta-RL (PRISM) framework to address the limitations of traditional skill learning, which relies heavily on offline datasets often containing noisy and irrelevant trajectories, as shown in Fig. \ref{fig:noisy}(a). To tackle this inefficiency, PRISM incorporates an exploration policy $\pi_{\mathrm{exp}}(a_t|s_t, i)$ for each training task $\mathcal{T}_i$, where $i = 1, \cdots, N_{\mathcal{T}, \mathrm{train}}$, and $N_{\mathcal{T}, \mathrm{train}}$ denotes the number of training tasks in $\mathcal{M}_{\mathrm{train}}$. 
% The exploration policy initially performs RL with intrinsic motivation techniques \cite{burda2018exploration} near the noisy offline dataset to identify task-relevant trajectories. 
While research on improving exploration capabilities has been conducted across various domains \cite{ermolov2020latent, han2021diversity, chu2024meta, jo2024fox}, we utilize intrinsic motivation techniques \cite{burda2018exploration} to identify task-relevant trajectories by performing exploration near the noisy offline dataset initially. However, the abundance of irrelevant trajectories in the noisy offline dataset can hinder efficient learning.


To address this, PRISM introduces two key buffers: the exploration buffer $\mathcal{B}_\text{exp}^i$ and the prioritized online buffer $\mathcal{B}_\text{on}^i$. The exploration buffer $\mathcal{B}_\text{exp}^i$ stores all trajectories collected by the exploration policy, providing comprehensive samples necessary for RL training, defined as $\mathcal{B}_\text{exp}^i := \{\tau_{\text{exp}}^i\}$, where $\tau_{\text{exp}}^i = (s_0^i, a_0^i, r_0^i, \cdots, s_H^i)$ represents an exploration trajectory, and $a_t^i \sim \pi_\text{exp}(\cdot|s_t, i)$ for $t = 0, \cdots, H$. 
The prioritized online buffer $\mathcal{B}_\text{on}^i$, initialized with the offline dataset $\mathcal{B}_\text{off}$, selectively retains high-quality trajectories based on their returns, defined as $\mathcal{B}_\text{on}^i := \{\tau_\text{high}^i\}$, where $\tau_\text{high}^i$ are trajectories prioritized by their highest returns $G = \sum_{t=0}^{H-1} \gamma^t r_t^i$. These high-return trajectories include both exploration trajectories $\tau_{\text{exp}}^i$ collected by the exploration policy and low-level trajectories $\tau_l^i$ generated by the low-level policy $\pi_l(a_t|s_t, z)$ during the execution of the high-level policy $\pi_h$, where $z \sim \pi_h(\cdot|s_t, e^{\mathcal{T}_i})$ and $e^{\mathcal{T}_i} \sim q_e$.

By filtering and retaining only the most useful trajectories, $\mathcal{B}_\text{on}^i$ effectively guides the exploration policy, surpassing the noisy offline buffer in efficiency and utility, as inspired by self-supervised learning approaches \cite{xin2020self, kim2024symmetric}. The exploration policy $\pi_\text{exp}$ is subsequently updated to minimize the following loss function, enabling it to focus on high-quality samples stored in $\mathcal{B}_\text{on}^i$ and discover additional task-relevant action sequences:
\vspace{-2em}

{\small
\begin{equation}
\sum_i \mathbb{E}_{\tau^i \sim \mathcal{B}_\text{exp}^i\cup\mathcal{B}_\text{on}^i}\left[\mathcal{L}^{\text{RL}}_{\text{exp}}(\pi_{\text{exp}})\right]+\lambda^\text{kld}_\text{exp}\mathbb{E}_{\tau^i\sim\mathcal{B}_\text{on}^i}\mathcal{D}_\text{KL}(\hat{\pi}_d^i||\pi_{\text{exp}}),
\label{eq:exploss}
\vspace{-.5em}
\end{equation}}
where $\lambda^\text{kld}_\text{exp}$ is the KLD coefficient for $\pi_{\text{exp}}$, and $\hat{\pi}_d^i$ denotes the action data distribution derived from $\mathcal{B}_\text{on}^i$. This setup ensures that the exploration policy focuses on task-relevant trajectories, starting near the offline dataset since the prioritized online buffer is initialized with it. Over time, as high-quality trajectories accumulate in the online buffer, the exploration policy progressively improves its ability to discover better paths and task-relevant actions.

Furthermore, PRISM utilizes both the prioritized online buffer and the offline dataset to train the skill encoder $q$, skill prior $p$, and low-level policy $\pi_l$, following the SPiRL framework \cite{pertsch2021accelerating}. The refined low-level policy $\pi_l$ is shared across tasks, while the high-level policy $\pi_h$ leverages these refined skills to perform skill-based meta-RL within the hierarchical structure of SiMPL \cite{nam2022simpl}. The overall structure of PRISM is illustrated in Fig. \ref{fig:structure}. In this process, skills are iteratively refined using high-quality samples from the prioritized online buffer, enhancing the high-level policy's ability to improve task-solving performance on both training and test tasks.


\vspace{-.5em}

\subsection{Maximum Return Relabeling for Skill Prioritization}
\label{subsec:relabeling}
\begin{figure}[!t]
    % \vskip 0.2in
    \vspace{-.5em}
    \centerline{\includegraphics[width=0.95\columnwidth]{./images/4-3_relabeling.pdf}}
    \vspace{-.5em}
    \caption{An illustration of maximum return relabeling}
    % \end{center}
    \label{fig:maxrelabel}
    \vskip -0.15in
\end{figure}

The proposed PRISM framework trains the low-level policy $\pi_l$ by leveraging both the offline dataset $\mathcal{B}_\text{off}$ and the prioritized online buffers $\mathcal{B}_\text{on}^i$ for each training task $\mathcal{T}_i$. While $\mathcal{B}_\text{on}^i$ provides high-quality trajectories tailored to training tasks, relying solely on it can lead to overfitting and poor generalization. Conversely, $\mathcal{B}_\text{off}$ includes essential trajectories for generalization but also contains noisy and irrelevant data, as illustrated in Fig. \ref{fig:motiv}. To balance these, PRISM combines both $\mathcal{B}_\text{off}$ and $\mathcal{B}_\text{on}^i$ to effectively train $\pi_l$.

However, as $\mathcal{B}_\text{off}$ often includes noisy and irrelevant trajectories, uniform sampling from it can unnecessarily expand the skill space and hinder the high-level policy $\pi_h$ from efficiently identifying task-relevant skills. To address this, PRISM employs maximum return relabelling, a method that evaluates the importance of trajectories in $\mathcal{B}_\text{off}$ by assigning hypothetical returns based on their task relevance. This prioritization ensures only the most useful trajectories contribute to skill learning. To compute the hypothetical return for each trajectory, PRISM trains a reward model $\hat{R}(s_t, a_t, i)$ for each training task $\mathcal{T}_i$. The reward model is optimized using the following loss function:
\begin{equation}
    \vspace{-.5em}
    \mathbb{E}_{(s_t^i,a_t^i,r_t^i)\sim \mathcal{B}_{\mathrm{exp}}^i\cup\mathcal{B}_{\mathrm{on}}^i}[(\hat{R}(s^i_t,a^i_t,i) - r_t^i)^2]
    \label{eq:reward},
\end{equation}
where the ground truth rewards $r_t^i$ are derived from the exploration buffers $\mathcal{B}_{\text{exp}}^i$ and prioritized online buffers $\mathcal{B}_{\text{on}}^i$ for all tasks. The trained reward model is used to assign hypothetical returns $\sum_t\gamma^t \hat{R}(s_t,a_t,i)$, and the maximum return $\hat{G}$ for each trajectory $\tilde{\tau} \in \mathcal{B}_\text{off}$ is then calculated as:
\begin{equation}
\hat{G}(\tilde{\tau}):=\max_i \left\{\sum_t\gamma^t \hat{R}(s_t,a_t,i)\right\}.
\label{eq:offlinepriority}
    \vspace{-.5em}
\end{equation}
Here, $\hat{G}$ captures the highest return among all training tasks for a given trajectory $\tilde{\tau}$, ensuring that only trajectories with significant potential for solving training tasks are prioritized. Using the estimated $\hat{G}$, we employ a prioritization method to select the most important trajectories for training the skill policy $\pi_l$, skill encoder $q$, and skill prior $p$. The loss function $\mathcal{L}_{\mathrm{skill}}(\pi_l, q,p)$, modified from Eq. \eqref{eq:spirl} as:
\begin{align}
\mathcal{L}_{\mathrm{skill}}(\pi_l,q,p):=&(1-\beta)\mathbb{E}_{\substack{\tilde{\tau}\sim P_{\mathcal{B}_\text{off}},\\z\sim q(\cdot|\tilde{\tau})}}\left[
\mathcal{L}(\pi_l,q,p,z)\right]
\label{eq:skillours}
\\&+\frac{\beta}{N_{\mathcal{T},\mathrm{train}}}\sum_i\mathbb{E}_{\substack{\tau^i\sim \mathcal{B}_\text{on}^i,\\z\sim q(\cdot|\tau^i)}}\left[\mathcal{L}(\pi_l,q,p,z)
\right],
\nonumber
\end{align}
where $\mathcal{L}(\pi_l,q,p,z):=-\sum^{t+H_s-1}_{k=t} \log \pi_l(a_k|s_k,z)+ \lambda^\text{kld}_l\mathcal{D}_\text{KL}\left(q||\mathcal{N}(\mathbf{0}, \mathbf{I})\right) + \mathcal{D}_\text{KL}(\lfloor q\rfloor||p)$ is the skill  loss defined in Eq. \eqref{eq:spirl}, and $P_{\mathcal{B}_\text{off}}(\tilde{\tau}) = \mathrm{Softmax}(\hat{G}(\tilde{\tau})/T)$ denotes the sampling probability for prioritization, computed by applying a Softmax function over all trajectories in $\mathcal{B}_\text{off}$, with $T > 0$ as the temperature parameter. For $\mathcal{B}_\text{on}$, which already contains high-return trajectories, trajectories are sampled uniformly. The control factor $\beta$ dynamically adjusts the balance between the offline and online datasets during training based on their average returns, given by
\vspace{-0.5em}
\begin{equation}
    \beta = \frac{\exp(\bar{G}_{\text{on}}/T)}{\exp(\bar{G}_{\text{on}}/T) + \exp(\bar{G}_{\text{off}}/T)}
    \label{eq:beta},
    \vspace{-.5em}
\end{equation}
where $\bar{G}_{\text{off}}$ represents the average $\hat{G}$ across all trajectories in $\mathcal{B}_\text{off}$, and $\bar{G}_{\text{on}}$ is the average actual return in $\mathcal{B}_\text{on}^i$ across all tasks. Fig. \ref{fig:maxrelabel} illustrates the prioritization process, showing how $\beta$ dynamically balances contributions from offline and online datasets. This mechanism ensures the selection of task-relevant trajectories from both datasets, facilitating efficient training of the low-level policy. In Section \ref{sec:exp}, we evaluate the effectiveness of this approach in improving $\pi_h$ training compared to using $\mathcal{B}_\text{off}$ without prioritization. We also analyze the evolution of $\beta$ during training and the characteristics of trajectories selected for skill learning.



For implementation, PRISM refines skills every $K_{\text{iter}}$ iterations and reinitializes $\pi_h$ to avoid non-stationary issues. Before training, the low-level policy $\pi_l$, skill encoder $q$, and skill prior $p$ are pre-trained on the offline dataset $\mathcal{B}_\text{off}$ using Eq. \eqref{eq:spirl}. Exploration policy is trained using the soft actor-critic (SAC) algorithm \cite{haarnoja2018soft}, which incorporates entropy to encourage exploration. Intrinsic rewards for exploring novel trajectories are generated via random network distillation (RND) \cite{burda2018exploration}, leveraging a randomly initialized target network for exploration. The meta-training process of PRISM is outlined in Algorithm \ref{algo:metatrain}, with additional details on the meta-test phase, meta-RL losses with value functions provided in Appendix \ref{appsec:impdetail}.
\vspace{-0.5em}

\begin{algorithm}[!ht]
   \caption{PRISM: Meta-Train Phase}
   \label{algo:metatrain}
\begin{algorithmic}[1]
\REQUIRE Training tasks $\mathcal{M}_{\text{train}}$, offline dataset $\mathcal{B}_\text{off}$, and low-level policy $\pi_l$, skill encoder $q$, and skill prior $p$.
\ENSURE High-level policy $\pi_h$, exploration policy $\pi_\text{exp}$, task encoder $q_e$, and reward model $\hat{R}$.
\STATE Fix $\bar{\pi}_l \gets \pi_l$, $\bar{q} \gets q$, and $\bar p \gets p$.
\FOR{iteration $k = 1,2,~\cdots$}
    \FOR{task $i = 1$ {\bfseries to} $N_{\mathcal{T},\text{train}}$}
        \STATE Collect high-level trajectories $\tau_h^i$ and low-level trajectories $\tau_l^i$ using $\pi_h$ with $\bar{\pi}_l, q_e$.
        \STATE Collect exploration trajectories $\tau_\text{exp}^i$ using $\pi_\text{exp}$.
        \STATE Filter high-return trajectories $\tau_\text{high}^i$ from $\tau_l^i$ and $\tau_\text{exp}^i$ s.t. $G > \min_{\tau' \in \mathcal{B}_\text{on}^i} G(\tau')$.
        \STATE Store $\tau_h^i$, $\tau_\text{exp}^i$, and $\tau_\text{high}^i$ into $\mathcal{B}_h^i$, $\mathcal{B}_\text{exp}^i$, and $\mathcal{B}_\text{on}^i$.
    \ENDFOR
    \STATE Compute prioritization factors: $P_{\mathcal{B}_\text{off}}$ and $\beta$.
    \FOR{gradient step}
        \STATE Update $\pi_h$, $q_e$ using Eq. \eqref{eq:simpl} (Meta-RL).
        \STATE Update $\pi_\text{exp}$ using Eq. \eqref{eq:exploss} (Skill exploration).
        \STATE Update $\pi_l$, $q$, $p$ using Eq. \eqref{eq:skillours} (Skill refinement).
        \STATE Update reward model $\hat{R}$ using Eq. \eqref{eq:reward}.
    \ENDFOR
    \IF{$k \bmod K_\text{iter} = 0$}
        \STATE Update $\bar{\pi}_l \gets \pi_l$, $\bar{q} \gets q$, and $\bar{p} \gets p$.
        \STATE Reinitialize $\pi_h$.
    \ENDIF
\ENDFOR
\end{algorithmic}
\end{algorithm}

\vspace{-1.5em}

\section{Experiment}
\label{sec:exp}
In this section, we evaluate the robustness of proposed PRISM framework to noisy demonstrations in long-horizon environments and analyze how skill discovery and refinement during training enhance performance.


\subsection{Experimental Setup}
\label{subsec:expsetup}

We compare the proposed PRISM with 3 non-meta RL baselines: {\bf SAC}, which trains test tasks directly without using the offline dataset; {\bf SAC+RND}, which incorporates RND-based intrinsic noise for enhanced exploration; and {\bf SPiRL}, which learns skills from the offline dataset using Eq. \eqref{eq:spirl} and trains high-level policies for individual tasks. Also, we include 4 meta-RL baselines: {\bf PEARL}, a widely used context-based meta-RL algorithm without skill learning; {\bf PEARL+RND}, which integrates RND-based exploration into PEARL; {\bf SiMPL}, which applies skill-based meta-RL using Eq. \eqref{eq:simpl}; and our {\bf PRISM}. PRISM's hyperparameters primarily follow \citet{nam2022simpl}, with additional parameters (e.g., temperature $T$) tuned via hyperparameter search, while other baselines use author-provided code. Results are averaged over 5 random seeds, with standard deviations represented as shaded areas in graphs and $\pm$ values in tables.

\vspace{-.5em}

\subsection{Environmental Setup}
\label{subsec:envsetup}
\begin{figure}[ht]
    \centering
    \vspace{-1em}
    \subfigure[Kitchen]{\includegraphics[width=0.35\columnwidth]{images/5-2_kitchen.png}}
    \subfigure[Office]{\includegraphics[width=0.35\columnwidth]{images/5-2_office.png}}
    % \vskip -0.1in
    \subfigure[Maze2D]{\includegraphics[width=0.35\columnwidth]{images/5-2_maze.png}}
    \subfigure[AntMaze]{\includegraphics[width=0.35\columnwidth]{images/5-2_antmaze.png}}
    \vspace{-1em}
    \caption{Considered long-horizon environments}
    \label{fig:envs}
    \vspace{-1em}
\end{figure}

\begin{table*}[t]
    \vspace{-1em}
    \caption{Performance comparison: Final test average return for all considered environments}
    \label{table:perf}
    % \vskip 0.15in
    \begin{center}
    \begin{small}
    
    % \begin{tabular}{ccrrrrrrr}
    \begin{tabular}{rccccccc}
    \toprule
    \textbf{Environment(Noise)} & \textbf{SAC} & \textbf{SAC+RND} & \textbf{PEARL} & \textbf{PEARL+RND} & \textbf{SPiRL} & \textbf{SiMPL} & \textbf{PRISM} \\
    \midrule
    Kitchen(Expert) & 
        \multirow{4}{*}{$0.01$\tiny{$\pm$}$0.01$} &     % SAC
        \multirow{4}{*}{$0.02$ \tiny{$\pm$}$0.05$} &    % SAC+RND
        \multirow{4}{*}{$0.23$ \tiny{$\pm$}$0.14$} &    % PEARL
        \multirow{4}{*}{$0.42$\tiny{$\pm$}$0.16$} &     % PEARL+RND
        $3.11$\tiny{$\pm$}$0.33$ &                      % SPiRL
        $3.40$\tiny{$\pm$}$0.18$ &                      % SiMPL
        $\mathbf{3.97}$\tiny{$\pm$}$0.09$\\             % Ours
    Kitchen($\sigma=0.1$) & & & & & 
        $3.37$\tiny{$\pm$}$0.31$ &                      % SPiRL
        $3.76$\tiny{$\pm$}$0.14$ &                      % SiMPL
        $\mathbf{3.91}$\tiny{$\pm$}$0.12$ \\            % Ours
    Kitchen($\sigma=0.2$) & & & & & 
        $2.06$\tiny{$\pm$}$0.43$ &                      % SPiRL
        $2.18$\tiny{$\pm$}$0.33$ &                      % SiMPL
        $\mathbf{3.73}$\tiny{$\pm$}$0.16$ \\            % Ours
    Kitchen($\sigma=0.3$) & & & & & 
        $0.83$\tiny{$\pm$}$0.17$ &                      % SPiRL
        $0.81$\tiny{$\pm$}$0.25$ &                      % SiMPL
        $\mathbf{3.48}$\tiny{$\pm$}$0.07$\\             % Ours
    \midrule
    Office(Expert) & 
        \multirow{4}{*}{$0.00$\tiny{$\pm$}$0.00$} &     % SAC
        \multirow{4}{*}{$0.00$\tiny{$\pm$}$0.00$} &     % SAC+RND
        \multirow{4}{*}{$0.01$\tiny{$\pm$}$0.01$} &     % PEARL
        \multirow{4}{*}{$0.01$\tiny{$\pm$}$0.01$} &     % PEARL+RND
        $0.65$\tiny{$\pm$}$0.24$ &                      % SPiRL
        $2.50$\tiny{$\pm$}$0.26$ &                      % SiMPL
        $\mathbf{2.86}$\tiny{$\pm$}$0.35$ \\            % Ours
    Office($\sigma=0.1$) & & & & & 
        $0.91$\tiny{$\pm$}$0.31$ &                      % SPiRL
        $3.33$\tiny{$\pm$}$0.39$ &                      % SiMPL
        $\mathbf{3.40}$\tiny{$\pm$}$0.38$ \\            % Ours
    Office($\sigma=0.2$) & & & & & 
        $0.49$\tiny{$\pm$}$0.22$ &                      % SPiRL
        $1.20$\tiny{$\pm$}$0.24$ &                      % SiMPL
        $\mathbf{2.01}$\tiny{$\pm$}$0.24$ \\            % Ours
    Office($\sigma=0.3$) & & & & &
        $0.42$\tiny{$\pm$}$0.14$ &                      % SPiRL
        $0.11$\tiny{$\pm$}$0.04$ &                      % SiMPL
        $\mathbf{1.68}$\tiny{$\pm$}$0.15$ \\            % Ours
    \midrule
    Maze2D(Expert) & 
        \multirow{4}{*}{$0.20$\tiny{$\pm$}$0.06$} &     % SAC
        \multirow{4}{*}{$0.35$\tiny{$\pm$}$0.07$} &     % SAC+RND
        \multirow{4}{*}{$0.10$\tiny{$\pm$}$0.01$} &     % PEARL
        \multirow{4}{*}{$0.11$\tiny{$\pm$}$0.08$} &     % PEARL+RND
        $0.77$\tiny{$\pm$}$0.06$ &                      % SPiRL
        $0.80$\tiny{$\pm$}$0.04$ &                      % SiMPL
        $\mathbf{0.87}$\tiny{$\pm$}$0.05$ \\            % Ours
    Maze2D($\sigma=0.5$) & & & & & 
        $\mathbf{0.89}$\tiny{$\pm$}$0.03$ &             % SPiRL
        $0.87$\tiny{$\pm$}$0.05$ &                      % SiMPL
        $\mathbf{0.89}$\tiny{$\pm$}$0.03$ \\            % Ours
    Maze2D($\sigma=1.0$) & & & & & 
        $0.80$\tiny{$\pm$}$0.01$ &                      % SPiRL
        $0.87$\tiny{$\pm$}$0.05$ &                      % SiMPL
        $\mathbf{0.93}$\tiny{$\pm$}$0.05$ \\            % Ours
    Maze2D($\sigma=1.5$) & & & & & 
        $0.81$\tiny{$\pm$}$0.05$ &                      % SPiRL
        $0.68$\tiny{$\pm$}$0.06$ &                      % SiMPL
        $\mathbf{0.99}$\tiny{$\pm$}$0.02$ \\            % Ours
    \midrule
    AntMaze(Expert) & 
        \multirow{4}{*}{$0.00$\tiny{$\pm$}$0.00$} &     % SAC
        \multirow{4}{*}{$0.00$\tiny{$\pm$}$0.00$} &     % SAC+RND
        \multirow{4}{*}{$0.00$\tiny{$\pm$}$0.00$} &     % PEARL
        \multirow{4}{*}{$0.00$\tiny{$\pm$}$0.00$} &     % PEARL+RND
        $0.64$\tiny{$\pm$}$0.09$ &                      % SPiRL
        $0.67$\tiny{$\pm$}$0.07$ &             % SiMPL
        $\mathbf{0.81}$\tiny{$\pm$}$0.08$ \\            % Ours
    AntMaze($\sigma=0.5$) & & & & & 
        $0.76$\tiny{$\pm$}$0.10$ &                      % SPiRL
        $0.77$\tiny{$\pm$}$0.05$ &                      % SiMPL
        $\mathbf{0.82}$\tiny{$\pm$}$0.05$ \\            % Ours
    AntMaze($\sigma=1.0$) & & & & & 
        $0.50$\tiny{$\pm$}$0.06$ &                      % SPiRL
        $0.33$\tiny{$\pm$}$0.09$ &                      % SiMPL
        $\mathbf{0.60}$\tiny{$\pm$}$0.02$ \\               % Ours
    AntMaze($\sigma=1.5$) & & & & & 
        $0.30$\tiny{$\pm$}$0.01$ &                      % SPiRL
        $0.27$\tiny{$\pm$}$0.05$ &                      % SiMPL
        $\mathbf{0.41}$\tiny{$\pm$}$0.01$ \\            % Ours
    \bottomrule
    \end{tabular}
        
    \end{small}
    \end{center}
    
    \vspace{-2em}
\end{table*}



We evaluate algorithms across 4 long-horizon, multi-task environments: Kitchen and Maze2D from \citet{nam2022simpl}, and Office and AntMaze, newly introduced in this work, as illustrated in Fig. \ref{fig:envs}. Offline datasets $\mathcal{B}_{\text{off}}$ are generated by policies perturbed from the expert policy using Gaussian action noise at varying levels $\sigma$, tailored to each environment. Detailed descriptions are as follows:
 

{\bf Kitchen:} The Franka Kitchen environment in D4RL benchmark \cite{fu2020d4rl}, proposed by \citet{gupta2020relay}, features a robotic arm completing 4 sequential subtasks selected from 7 available subtasks, receiving a reward of 1 for each successful subtask. The environment includes 25 meta-train and 10 meta-test tasks, with noise levels ranging from expert demonstrations (noise-free) to Gaussian action noise at $\sigma=0.1$, $0.2$, and $0.3$.

{\bf Office:} The Office environment, adapted from \citet{pertsch2022guided} for meta-RL. Each task requires moving 3 randomly selected office objects into one of 3 containers on a desk, receiving a reward of 1 for each successful pick or place action. The environment includes 7 objects, 25 meta-train tasks, and 10 meta-test tasks, with noise levels ranging from expert to Gaussian noise with $\sigma=0.1$, $0.2$, and $0.3$.

{\bf Maze2D:} Based on D4RL \cite{fu2020d4rl}, this navigation task requires a 2-DoF point mass agent to reach a goal point in a larger 20x20 maze than the default D4RL Maze2D, receiving a reward of 1 upon reaching the goal. The environment includes 40 meta-train tasks and 10 meta-test tasks, with noise levels ranging from expert to Gaussian action noise at $\sigma=0.5$, $1.0$, and $1.5$.

{\bf AntMaze:} Similar to Maze2D from D4RL \cite{fu2020d4rl}, this environment uses a more complex ant agent in a smaller 10x10 maze to account for the increased difficulty, receiving a reward of 1 upong reaching the goal. The environment includes 20 meta-train tasks and 10 meta-test tasks, with noise levels ranging from expert to Gaussian action noise at $\sigma=0.5$, $1.0$, and $1.5$.


More detailed experimental setup, including explanations of other baselines, descriptions of the considered environments and data collection processes, as well as our hyperparameter configurations, can be found in Appendix \ref{appsec:exp}.

\subsection{Performance Comparison}
\label{subsec:perfcomp}


We compare the proposed PRISM with various baseline algorithms. Non-meta RL algorithms are trained directly on each test task for 0.5K iterations due to the absence of a meta-train phase. Meta-RL algorithms undergo meta-training for 10K iterations in Kitchen and Office, and 4K in Maze2D and AntMaze, followed by fine-tuning on test tasks for an additional 0.5K iterations. For PRISM, the skill update interval $K_{\text{iter}}$ is set to 2K for Kitchen, Office, and AntMaze; 1K for Maze2D. To ensure a fair comparison, PRISM counts each update process from its exploration and high-level policies as one iteration. Table \ref{table:perf} presents the final average return across  test tasks after the test iterations. From the result, SAC and PEARL baselines, which do not utilize skills or offline datasets, perform poorly on long-horizon tasks, yielding a single result across all noise levels. In contrast, SPiRL, SiMPL, and PRISM, which leverage skills, achieve significantly better performance.


\begin{figure}[ht]
    % \vskip 0.2in
    \centerline{\includegraphics[width=\columnwidth]{images/5-3_performance.pdf}}
    
    \vspace{-1em}
    \caption{Learning curves of the meta-train and meta-test phases on Kitchen ($\sigma=0.3$) and Maze2D ($\sigma=1.5$). PRISM ($\pi_\text{exp}$) and PRISM ($\pi_h$) denote the performance of the exploration policy $\pi_\text{exp}$ and high-level policy $\pi_h$ during meta-training. }
    % \end{center}
    \label{fig:performance}
    \vspace{-2em}
\end{figure}

SPiRL and SiMPL, however, show sharp performance declines as dataset noise increases. While both perform well with expert data, SiMPL struggles under noisy conditions due to instability in its task encoder $q_e$, sometimes performing worse than SPiRL. Here, the baseline results for Maze2D (Expert) are somewhat lower than those reported in the SiMPL paper. This discrepancy likely arises because, in constructing the offline dataset, we considered fewer tasks compared to SiMPL, resulting in trajectories that do not fully cover the map. Interestingly, minor noise occasionally boosts performance by introducing diverse trajectories that improve skill learning. In contrast, PRISM demonstrates superior robustness across all evaluated environments, consistently outperforming baselines at varying noise levels. For example, in the Kitchen environment, PRISM maintains strong performance under significant noise by effectively refining useful skills, while in Maze2D, higher noise levels lead to the discovery of diverse skills, achieving perfect task completion when $\sigma=1.5$. These results highlight PRISM's ability to refine and discover robust skills, significantly enhancing meta-RL performance. Moreover, PRISM excels with both noisy and expert data, achieving superior test performance by learning more effective skills.

Fig. \ref{fig:performance} shows the learning progress during meta-train and meta-test phases for Kitchen ($\sigma=0.3$) and Maze2D ($\sigma=1.5$), emphasizing the significant performance gap between PRISM and other methods. The periodic drop in PRISM's high-level performance corresponds to the reinitialization of $\pi_h$ every $K_\text{iter}$. Non-meta RL algorithms, even those with RND-based exploration, struggle with the considered long-horizon tasks. Similarly, SPiRL and SiMPL, constrained by noisy offline datasets, exhibit limited improvement due to reliance on fixed offline data. In contrast, PRISM periodically refines skills through prioritized skill refinement, leading to superior meta-test performance. Additional results for other noise levels and the evolution of control factor $\beta$ are provided in Appendix \ref{appsec:addcomp}.


\begin{figure*}[!t]
    \vspace{-.5em}
    \centerline{\includegraphics[width=\textwidth]{images/5-4_refinement.pdf}}
    \vskip -0.1in
    \caption{Visualization of buffer control factor $\beta$ dynamics and refined skill evolution in Kitchen ($\sigma=0.3$) and Maze2D ($\sigma=1.5$). In Kitchen, the refined skills at $t=15$ and $t=30$ during the microwave-opening task are depicted, while in Maze2D, trajectories using refined skills illustrate the process of progressively expanding to broader areas to solve tasks.}
    % \end{center}
    \label{fig:refinement}
    \vskip -0.2in
\end{figure*}


    \vspace{-.5em}
\subsection{Visualization of Skill Refinement Process}

\begin{figure}[!t]
    % \vskip 0.2in
    \centerline{\includegraphics[width=0.9\columnwidth]{images/5-5_ablation-exp-relabeling.pdf}}
    \vspace{-1em}
    \caption{Ablation study: Component evaluation}
    % \end{center}
    \label{fig:ableval}
    \vspace{.5em}
    \centerline{\includegraphics[width=0.9\columnwidth]{images/5-5_ablation-returntemp.pdf}}
    \vspace{-1em}
    \caption{Ablation study: Prioritization temperature $T$}
    % \end{center}
    \label{fig:ablrettemp}
    % \vskip -0.27in
    \vskip -0.36in
    % \vspace{-3em}
\end{figure}

To analyze skill learning and refinement, Fig. \ref{fig:refinement} shows the evolution of the buffer control factor $\beta$ and skill refinement in Kitchen ($\sigma=0.3$) and Maze2D ($\sigma=1.5$). For Kitchen, the microwave-opening subtask is examined, while for Maze2D, skill improvements are observed in navigating the maze. In the early stages (1K iterations for Kitchen, 0.5K for Maze2D), pretrained skills learned solely from the offline dataset, are used as no skill updates occur until $K_{\text{iter}}$. This results in poor performance: in Kitchen, the agent fails to grasp the microwave handle, and in Maze2D, noisy trajectories lead to task failures. As training progresses, $\beta$ initially relies on offline data but gradually increases as high-quality samples accumulate in the online buffer. This shift enables task-relevant skill refinement and improved task-solving.

As a result, by iteration 5K in Kitchen, the agent learns to open the microwave, refining this skill to complete the task more efficiently by iteration 10K. In Maze2D, the agent explores more diverse trajectories over iterations, ultimately solving all training tasks by iteration 4K. These results highlight how PRISM refines skills iteratively by leveraging prioritized data from offline and online buffers. As shown in Table \ref{table:perf}, the learned skills generalize effectively to unseen test tasks, demonstrating PRISM's robustness and efficacy. In addition, improvement in task representation through skill refinement, along with additional visualizations, including skill representations, are provided in Appendix \ref{appsec:visual}.
\vspace{-.5em}

\subsection{Ablation Studies}
\label{subsec:abl}

We evaluate the impact of PRISM's components and key hyperparameters in Kitchen ($\sigma=0.3$) and Maze2D ($\sigma=1.5$), focusing on the effect of the prioritization temperature $T$. Additional analyses, including component evaluation and KLD coefficient $\lambda^\text{kld}_\text{exp}$ for all noise levels, are in Appendix \ref{appsec:addabl}.

{\bf Component Evaluation:} To evaluate the importance of PRISM's components, we compare the meta-test performance of PRISM with all components included against the following variations: (1) Without $\mathcal{B}_\text{off}$, relying solely on $\mathcal{B}_\text{on}^i$; (2) Without $P_{\mathcal{B}_\text{off}}$, applying uniform sampling in $\mathcal{B}_\text{off}$ instead of maximum return relabeling; (3) Without $\mathcal{B}_\text{on}$, using only $\mathcal{B}_\text{off}$; and (4) Without $\pi_\text{exp}$, removing the exploration policy. As illustrated in Fig. \ref{fig:ableval}, performance drops significantly when either buffer is removed, showing that both are crucial for effective skill discovery. Uniform sampling in $\mathcal{B}_\text{off}$ also reduces performance, underlining the importance of maximum return relabeling. Lastly, excluding $\pi_\text{exp}$ notably degrades results, emphasizing the critical role of exploration in skill refinement and trajectory discovery.

{\bf Prioritization Temperature $T$:} The prioritization temperature $T$ adjusts the prioritization between online and offline buffers. Specifically, lower $T$ biases sampling toward high-return buffers, while higher $T$ results in uniform sampling. As shown in Fig. \ref{fig:ablrettemp}, meta-test performance varies with $T$. When $T=0.1$, performance drops due to an excessive focus on one buffer, as the trends observed in the component evaluation. Conversely, high $T=2.0$ also degrades performance by eliminating prioritization. These results highlight the importance of proper tuning: $T=1.0$ for Kitchen and $T=0.5$ for Maze2D achieve the best performance.

\vspace{-.5em}
\section{Conclusion}


In this paper, we propose PRISM, a robust skill-based meta-RL framework designed to address noisy offline demonstrations in long-horizon tasks. Through prioritized skill refinement and maximum return relabeling, PRISM effectively prioritizes task-relevant trajectories for skill learning and discovery. Experimental results highlight its robustness to noise and superior performance, demonstrating its potential for efficient meta-RL in real-world applications.

\section*{Impact Statement}
This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\counterwithin{table}{section}
\counterwithin{figure}{section} 
\renewcommand{\theequation}{\thesection.\arabic{equation}}

\setcounter{equation}{0}
\setcounter{algorithm}{0}



\section{Implementation Details on PRISM}
\label{appsec:impdetail}
This section provides a detailed implementation of the proposed PRISM framework. As outlined in Section \ref{sec:method}, PRISM begins with an initial skill learning phase (pre-train) to train the low-level skill policy, skill encoder, and skill prior. It then progresses to the meta-train phase, where skill model refinement is performed through skill exploration using the exploration policy, and skill-based meta-RL is executed using the high-level policy and task encoder. Finally, in the meta-test phase, rapid adaptation to the target task is achieved via fine-tuning based on the trained high-level policy and task encoder. Section \ref{appsec:initphase} details the initial skill learning phase, Section \ref{appsec:trainphase} elaborates on the meta-train phase, and Section \ref{appsec:testphase} explains the meta-test phase. All loss functions in PRISM are redefined in terms of the neural network parameters of its policies and models. Additionally, the overall structure for the meta-train and meta-test phases is provided in Algorithms \ref{algo:metatrain-detail} and \ref{algo:metatest-detail}.


\subsection{Initial Skill Learning Phase}
\label{appsec:initphase}

Following SPiRL \cite{pertsch2021accelerating}, introduced in Section \ref{sec:background}, we train initial skills using the offline dataset $\mathcal{B}_\text{off}$. The low-level skill policy $\pi_{l,\phi}$, skill encoder $q_\phi$, and skill prior $p_\phi$ are parameterized by $\phi$ and trained using the following loss function (modified from Eq. \eqref{eq:spirl}):
\begin{equation}
\label{eq:spirl-detail}
\begin{aligned}
    \mathcal{L}_\text{spirl}(\phi) &:= \mathbb{E}_{\substack{(s_{t:t+H_s}, a_{t:t+H_s})\sim\mathcal{B}_\text{off}\\z\sim q_\phi(\cdot|s_{t:t+H_s}, a_{t:t+H_s})}}\Bigl[\mathcal{L}(\pi_{l,\phi},q_\phi,p_\phi, z)\Bigr]\\
    &\ \ \begin{aligned}
        =\mathbb{E}_{\substack{(s_{t:t+H_s}, a_{t:t+H_s})\sim\mathcal{B}_\text{off}\\z\sim q_\phi(\cdot|s_{t:t+H_s}, a_{t:t+H_s})}}\Biggl[
        -\sum^{t+H_s-1}_{k=t} \log \pi_{l,\phi}(a_k|s_k,z) 
        &+\lambda^\text{kld}_l\mathcal{D}_\text{KL}\Bigl(q_\phi(\cdot | s_{t:t+H_s}, a_{t:t+H_s}) \,\Big\vert\Big\vert\, \mathcal{N}(\mathbf{0}, \mathbf{I})\Bigr)\\
        &+\mathcal{D}_\text{KL}\Bigl( \lfloor q_\phi(\cdot| s_{t:t+H_s}, a_{t:t+H_s})\rfloor \,\Big\vert\Big\vert\, p_\phi(\cdot | s_t) \Bigr)
        \Biggr],
    \end{aligned}
\end{aligned}
\end{equation}
where $\lfloor \cdot \rfloor$ represents the stop gradient operator, which prevents the KL term for skill prior learning from influencing the skill encoder. Using the pre-trained $\pi_{l,\phi}$, $q_\phi$, and $p_\phi$, PRISM refines skills during the meta-train phase to further enhance task-solving capabilities.

\subsection{Meta-Train Phase}
\label{appsec:trainphase}
As described in Section \ref{sec:method}, PRISM comprises three main processes: Skill Exploration, which explores diverse skills near the prioritized on-policy buffer $\mathcal{B}_\text{on}$; Prioritized Skill Refinement, which improves skills using $\mathcal{B}_\text{on}$; and Skill-based Meta-RL, which trains the high-level policy and task encoder to effectively utilize learned skills for solving tasks. Detailed explanations for each process are provided below.

{\bf Skill Exploration}\\

As described in Section \ref{subsec:skillimprove}, the exploration policy $\pi_{\text{exp},\psi}$, parameterized by $\psi$, is designed to expand the skill distribution and discover task-relevant behaviors near trajectories stored in the prioritized on-policy buffer $\mathcal{B}_{\text{on}}^i$ for each training task $\mathcal{T}^i$. This buffer prioritizes trajectories that best solve the tasks. Additionally, the state-action value function $Q_{\text{exp},\psi}$, also parameterized by $\psi$, is defined to train the exploration policy using soft actor-critic (SAC). To enhance exploration, both extrinsic reward $r_t^i$ and intrinsic reward $r_{\text{int},t}^i$ are employed. The intrinsic reward, based on random network distillation (RND), is computed as the L2 loss between a randomly initialized target network $\hat{f}_{\bar{\eta}}^i$ and a prediction network $f_\eta^i$, parameterized by $\eta$ and $\bar{\eta}$, respectively, and is expressed as:

\begin{equation}
    r_{\text{int},t}^i := \left\| 
    f_\eta^i(s_{t+1})-\hat{f}_{\bar{\eta}}^i(s_{t+1})
     \right\|^2_2,
\label{eq:rnd}
\end{equation}

where $i$ is the task index, and $f_\eta$ is updated to minimize this loss. A dropout layer is applied to $f_\eta$ to prevent over-sensitivity to state $s$. The RL loss functions of SAC for training the exploration policy $\pi_{\text{exp},\psi}$ and the state-action value function $Q_{\text{exp},\psi}$ using the intrinsic reward $r_{\text{int},t}$ are defined as follows:
\begin{align}
\label{eq:exp-detail}
\begin{aligned}
&\begin{aligned}
    \mathcal{L}_\text{exp}^\text{critic}(\psi) := \sum_i \mathbb{E}_{\substack{(s_t,a_t,r_t^i,s_{t+1})\sim \mathcal{B}^i_\text{exp}\cup\mathcal{B}^i_\text{on}\\a_{t+1}\sim\pi_{\text{exp},\psi}(\cdot | s_t, i)}}\biggl[\frac{1}{2}\biggl(
    Q_{\text{exp},\psi}(s_t,a_t,i) - \Bigl(\delta_\text{ext}r_t^i + &\delta_\text{int}r_\text{int,t}^i + \gamma_\text{exp}\bigl(Q_{\text{exp},\psi}(s_{t+1},a_{t+1},i) \\
    &+ \lambda^\text{ent}_\text{exp}\log\pi_{\text{exp},\psi}(a_{t+1}|s_{t+1},i)
    \bigr)
    \Bigr)\biggr)^2\biggr]
\end{aligned}\\
&\begin{aligned}
    \mathcal{L}_\text{exp}^\text{actor}(\psi) := &\sum_i \mathbb{E}_{\substack{s_t\sim\mathcal{B}^i_\text{exp}\cup\mathcal{B}^i_\text{on}\\a_t\sim\pi_{\text{exp},\psi}(\cdot|s_t, i)}}\biggl[
    \lambda^\text{ent}_\text{exp}\log\pi_{\text{exp},\psi}(a_t|s_t,i) - Q_{\text{exp},\psi}(s_t,a_t,i) \biggr]
    -\lambda^\text{kld}_\text{exp} \sum_i\mathbb{E}_{(s_t,a_t)\sim\mathcal{B}^i_\text{on}}\biggl[
    \log\pi_{\text{exp},\psi}(a_t|s_t,i)
    \biggr].
\end{aligned}
\end{aligned}
\end{align}
Here, $\delta_\text{ext}$ and $\delta_\text{int}$ are extrinsic and intrinsic reward ratios, $\gamma_\text{exp}$ is the discount factor, $\lambda^\text{ent}_\text{exp}$ is the exploration entropy coefficient adjusted automatically by SAC, and $\lambda^\text{kld}_\text{exp}$ is the KLD coefficient. 
Also, note that Eq. \eqref{eq:exp-detail} provides a parameterized and detailed reformulation of Eq. \eqref{eq:exploss} from Section \ref{subsec:skillimprove}, explicitly incorporating parameterization and loss scaling details. 



{\bf Prioritized Skill Refinement with Maximum Return Relabeling}\\

To extract better trajectories and learn skills that effectively solve tasks, the online buffer $\mathcal{B}^i_\text{on}$ selectively stores high-return trajectories collected during the meta-training phase through the execution of the low-level policy $\pi_{l,\phi}$ and the exploration policy $\pi_{\text{exp},\psi}$. A trajectory $\tau^i$ is added to $\mathcal{B}^i_\text{on}$ if its return $G(\tau^i)$ exceeds the minimum return in the buffer, $\min_{\tau'\in\mathcal{B}^i_\text{on}}G(\tau')$. To refine skills, maximum return relabeling is applied using the parameterized reward model $\hat{R}_\zeta$ with parameter $\zeta$. The reward model is trained by minimizing the following MSE loss:
\begin{equation}
    \mathcal{L}_\text{reward}(\zeta) := \mathbb{E}_{(s^i_t,a^i_t,r^i_t)\sim\mathcal{B}^i_\text{exp}\cup\mathcal{B}^i_\text{on}}\Bigl[\Bigl(\hat R_\zeta(s^i_t,a^i_t,i) - r^i_t\Bigr)^2\Bigr]
    \label{eq:reward-detail}.
\end{equation}
This assigns priorities to offline trajectories $\tilde{\tau} \in \mathcal{B}_\text{off}$ (Eq. \eqref{eq:offlinepriority}), updated for $N_\text{priority}$ samples per iteration.

For skill refinement, the low-level skill policy $\pi_{l,\phi}$, skill encoder $q_\phi$, and skill prior $p_\phi$ are optimized using the following loss function. This incorporates both high-return trajectories from the online buffer $\mathcal{B}^i_\text{on}$ and trajectories from the offline buffer $\mathcal{B}_\text{off}$, weighted by their importance:
\begin{equation}
    \mathcal{L}_\text{skill}(\phi) := (1-\beta)\mathbb{E}_{\substack{(s_{t:t+H_s},a_{t:t+H_s})\sim P_{\mathcal{B}_\text{off}}\\z\sim q_\phi(\cdot|s_{t:t+H_s},a_{t:t+H_s})}}
    \Bigl[\mathcal{L}(\pi_{l,\phi},q_\phi,p_\phi,z)\Bigr]
    +\frac{\beta}{N_{\mathcal{T},\text{train}}}\sum_i \mathbb{E}_{\substack{(s_{t:t+H_s},a_{t:t+H_s})\sim\mathcal{B}^i_\text{on}\\z\sim q_\phi(\cdot|s_{t:t+H_s},a_{t:t+H_s})}}
    \Bigl[\mathcal{L}(\pi_{l,\phi},q_\phi,p_\phi,z)\Bigr],
    \label{eq:skillours-detail}
\end{equation}
where $\beta$ is the buffer control factor defined in Eq. \eqref{eq:beta}, and $\mathcal{L}(\pi_{l,\phi}, q_\phi, p_\phi, z)$ is the skill learning objective defined in Eq. \ref{eq:spirl-detail} for optimizing $\pi_{l,\phi}$, $q_\phi$, and $p_\phi$. During training, we update the low-level policy $\bar{\pi}_{l,\phi}$, skill encoder $\bar{q}_\phi$, and skill prior $\bar{p}_\phi$ used for the skill-based meta-RL every $K_\text{iter}$ iterations. Specifically, the updates are performed as follows: $\bar{\pi}_{l,\phi}\leftarrow \pi_{l,\phi}$, $\bar{q}_\phi\leftarrow q_\phi$, and $\bar{p}_\phi\leftarrow p_\phi$.

{\bf Skill-based Meta-RL}\\


To improve skills through prioritized refinement, the updated and fixed low-level skill policy $\bar{\pi}_{l,\phi}$ and skill prior $\bar{q}_\phi$ are utilized to train the high-level policy following the SiMPL framework introduced in Section \ref{sec:background}. The objective is to select skill representations $z$ that maximize task returns while ensuring the high-level policy remains close to the skill prior for stable and efficient learning. The high-level policy $\pi_{h,\theta}$ and value function $Q_{h,\theta}$ are parameterized by $\theta$ and trained using the soft actor-critic (SAC) framework, with the RL loss functions defined as:
\begin{align}
\label{eq:simpl-detail}
\begin{aligned}
&\begin{aligned}
    \mathcal{L}_h^\text{critic}(\theta) := \mathbb{E}_{\substack{(s_t,z_t,r^h_t,s_{t+H_s-1})\sim\mathcal{B}^\mathcal{T}_h,e^\mathcal{T}\sim q_{e,\theta}(\cdot | c^\mathcal{T})\\z_{t+1}\sim \pi_{h,\theta}(\cdot|s_{t+H_s-1}, e^\mathcal{T})}}\biggl[\frac{1}{2}\biggl(
    &Q_{h,\theta}(s_t, z_t, e^\mathcal{T}) - \Bigl(
        r^h_t + \gamma_h\bigl( Q_{h,\theta}(s_{t+H_s-1}, z_{t+1}, e^\mathcal{T}) \\
        &- \lambda^\text{kld}_h\mathcal{D}_\text{KL}\bigl( \pi_{h,\theta}(\cdot | s_{t+H_s-1}, e^\mathcal{T})\,\big\vert\big\vert\, \bar{p}_\phi(\cdot | s_{t+H_s-1}) \bigr) \bigr)
    \Bigr)
    \biggr)^2\biggr]
\end{aligned}\\
&\mathcal{L}_h^\text{actor}(\theta) := \mathbb{E}_{\substack{s_t\sim\mathcal{B}^\mathcal{T}_h, e^\mathcal{T}\sim q_{e,\theta}(\cdot|c^\mathcal{T})\\ z_t\sim\pi_{h,\theta}(\cdot|s_t,e^\mathcal{T})}}\biggl[
    \lambda^\text{kld}_h \mathcal{D}_\text{KL}\Bigl( \pi_{h,\theta}(\cdot|s_t, e^\mathcal{T}) \,\Big\vert\Big\vert\, \bar{p}_\phi(\cdot | s_t) \Bigr)
    - Q_{h,\theta}(s_t,z_t,e^\mathcal{T})
    \biggr],
\end{aligned}
\end{align}
where $q_{e,\theta}$ is the parameterized task encoder with parameter $\theta$, $\gamma_h$ is the high-level discount factor, and $\lambda^\text{kld}_h$ is the high-level KLD coefficient. The term $r^h_t=\sum^{t+H_s-1}_{k=t}r_k$ represents the cumulative rewards, with states and rewards obtained by executing the low-level policy $\bar{\pi}_{l,\phi}$ using $z_t\sim \pi_{h,\theta}(\cdot|s_t)$ over $H_s$ timesteps. The context $c^\mathcal{T}={(s_k,z_k,r_k^h,s_{k+H_s-1})}^{N_\text{prior}}_{k=1}$, where $N_\text{prior}$ is the number of context transitions, denotes the high-level transition set of task $\mathcal{T}$. This context is used to select the task representation $e^\mathcal{T}$ from the task encoder $q_{e,\theta}$. Also, note that Eq. \eqref{eq:simpl-detail} is a parameterized modification of Eq. \eqref{eq:simpl} from Section \ref{sec:background}.
\vspace{-0.5em}

\subsection{Meta-Test Phase}
\label{appsec:testphase}

After completing the meta-train phase of PRISM, the meta-test phase is performed on the test task set $\mathcal{M}_{\mathrm{test}}$. In this phase, previously learned components, including the task encoder $q_{e,\theta}$, low-level skill policy $\bar{\pi}_{l,\phi}$, and skill prior $\bar{p}_\phi$, are kept fixed and are no longer updated. Only the high-level policy $\pi_{h,\theta}$ and high-level value function $Q_{h,\theta}$ are trained for each test task using the soft actor-critic (SAC) framework.

During meta-testing, for each test task $\mathcal{T}$, the task representation $e^\mathcal{T}$ is inferred from the fixed task encoder $q_{e,\theta}$. The SAC algorithm is then applied to optimize the high-level policy and value function for the specific test task, following the same loss functions as defined in Eq. \eqref{eq:simpl-detail} from the meta-training phase. This approach ensures efficient adaptation to unseen tasks by leveraging the fixed, pre-trained low-level skills and task representations.
\vspace{-0.7em}
\begin{algorithm}[!ht]
   \caption{PRISM: Meta-Train Phase}
   \label{algo:metatrain-detail}
\begin{algorithmic}[1]
\REQUIRE Training tasks $\mathcal{M}_{\text{train}}$, offline dataset $\mathcal{B}_\text{off}$, low-level policy $\pi_{l,\phi}$, skill encoder $q_\phi$, and skill prior $p_\phi$.
\ENSURE {High-level policy $\pi_{h,\theta}$, exploration policy $\pi_{\text{exp},\psi}$, task encoder $q_{e,\theta}$, reward model $\hat{R}_\zeta$, \\ and value functions $Q_{h,\theta}$, $Q_{\text{exp},\psi}$.}
\STATE Fix $\bar{\pi}_{l,\phi} \gets \pi_{l,\phi}$, $\bar{q}_\phi \gets q_\phi$, and $\bar{p}_\phi\gets p_\phi$.
\FOR{iteration $k = 1,2,~\cdots$}
    \FOR{task $i = 1$ {\bfseries to} $N_{\mathcal{T},\text{train}}$}
        \STATE Collect high-level trajectories $\tau_h^i$ and low-level trajectories $\tau_l^i$ using $\pi_{h,\theta}$ with $\bar{\pi}_{l,\phi}$, $q_{e,\theta}$.
        \STATE Collect exploration trajectories $\tau_\text{exp}^i$ using $\pi_{\text{exp},\psi}$.
        \STATE Filter high-return trajectories $\tau_\text{high}^i$ from $\tau_l^i$ and $\tau_\text{exp}^i$ s.t. $G > \min_{\tau' \in \mathcal{B}_\text{on}^i} G(\tau')$.
        \STATE Store $\tau_h^i$, $\tau_\text{exp}^i$, and $\tau_\text{high}^i$ into $\mathcal{B}_h^i$, $\mathcal{B}_\text{exp}^i$, and $\mathcal{B}_\text{on}^i$.
    \ENDFOR
    \STATE Compute prioritization factors: $P_{\mathcal{B}_\text{off}}$ and $\beta$.
    \FOR{gradient step}
        \STATE Update $\pi_{h,\theta}$, $Q_{h,\theta}$, $q_{e,\theta}$ using Eq. \eqref{eq:simpl-detail} with $\theta\leftarrow\theta-\lambda^\text{lr}_h\cdot\nabla_\theta(\mathcal{L}^\text{critic}_h(\theta)+\mathcal{L}^\text{actor}_h(\theta))$ (Meta-RL).
        \STATE Update $\pi_{\text{exp},\psi}$, $Q_{\text{exp},\psi}$ using Eq. \eqref{eq:exp-detail} with $\psi\leftarrow\psi-\lambda^\text{lr}_\text{exp}\cdot\nabla_\psi(\mathcal{L}^\text{critic}_\text{exp}(\psi)+\mathcal{L}^\text{actor}_\text{exp}(\psi))$ (Skill exploration).
        \STATE Update $\pi_{l,\phi}$, $q_\phi$, $p_\phi$ using Eq. \eqref{eq:skillours-detail} with $\phi\leftarrow\phi-\lambda^\text{lr}_l\cdot\nabla_\phi\mathcal{L}_\text{skill}(\phi)$ (Skill refinement).
        \STATE Update reward model $\hat{R}_\zeta$ using Eq. \eqref{eq:reward-detail} with $\zeta\leftarrow\zeta-\lambda^\text{lr}_\text{reward}\cdot\nabla_\zeta\mathcal{L}_\text{reward}(\zeta)$.
    \ENDFOR
    \IF{$k \bmod K_\text{iter} = 0$}
        \STATE Update $\bar{\pi}_{l,\phi} \gets \pi_{l,\phi}$, $\bar{q}_\phi \gets q_\phi$, and $\bar{p}_\phi\gets p_\phi$.
        \STATE Reinitialize $\pi_{h,\theta}$.
    \ENDIF
\ENDFOR
\end{algorithmic}
\end{algorithm}
\vspace{-2em}
\begin{algorithm}[ht!]
   \caption{PRISM: Meta-test phase}
   \label{algo:metatest-detail}
\begin{algorithmic}[1]
% \small
\REQUIRE {Target task $\mathcal{T}$, high-level policy $\pi_{h,\theta}$, value function $Q_{h,\theta}$, task encoder $q_{e,\theta}$, and low-level policy $\pi_{l,\phi}$.}
% low-level policy $\pi_{l,\phi}$, skill prior $p_\phi$, task encoder $q_{e,\theta}$, high-level policy $\pi_{h,\theta}$, high-level Q-function $Q_{h,\theta}$.
% \ENSURE High-level buffer $\mathcal{B}^{\mathcal{T}^*}_h$.
% for context
\STATE Collect context $c^\mathcal{T}$ using $\pi_{h,\theta}$ with $\pi_{l,\phi}$, $e\sim\mathcal{N}(\mathrm{0},\mathrm{I})$.
\STATE Compute task representation $e^\mathcal{T}\sim q_{e,\theta}(\cdot|c^\mathcal{T})$.
% \FOR{$k=1$ {\bfseries to} $N_\text{prior}$}
%     \STATE Collect high-level trajectory $\tau^{\mathcal{T}^*}_h$ using $\pi_{h,\theta}, e\sim\mathcal{N}(\mathbf{0},\mathbf{I})$.
% \ENDFOR
\FOR{iteration $k=1,2,\dots$}
    \STATE Collect high-level trajectory $\tau^\mathcal{T}_h$ using $\pi_{h,\theta}$ with $\pi_{l,\phi}$, $e^\mathcal{T}$.
    \STATE Store $\tau^\mathcal{T}_h$ into $\mathcal{B}^\mathcal{T}_h$.
    \FOR{gradient step}
        \STATE Update $\pi_{h,\theta}$, $Q_{h,\theta}$ using Eq. \eqref{eq:simpl-detail} with $\theta\leftarrow\theta-\lambda^\text{lr}_h\cdot\nabla_\theta(\mathcal{L}^\text{critic}_h(\theta)+\mathcal{L}^\text{actor}_h(\theta))$
        % \STATE Compute $\mathcal{L}^{h^*}_\text{critic}(\theta), \mathcal{L}^{h^*}_\text{actor}(\theta)$ loss using $p_\phi$, Eq. \eqref{eq:metatest}.
        % \STATE Update the $\pi_{h,\theta},Q_{h,\theta}$ parameter $\theta$ by $\theta\leftarrow\theta-\lambda^\text{lr}_h\cdot\nabla_\theta(\mathcal{L}^{h^*}_\text{critic}(\theta)+\mathcal{L}^{h^*}_\text{actor}(\theta))$.
    \ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm}
\clearpage


\newpage
\section{Detailed Experimental Setup}
\label{appsec:exp}


In this section, we provide a detailed description of our experimental setup. The implementation is built on PyTorch with CUDA 11.7, running on an AMD EPYC 7313 CPU with an NVIDIA GeForce RTX 3090 GPU. PRISM is implemented based on the official open-source code of SiMPL, available at \href{https://github.com/namsan96/SiMPL}{https://github.com/namsan96/SiMPL}. For the environment implementations, we used SiMPL's code for the Kitchen and Maze2D environments, SkiLD's open-source code for the Office environment at \href{https://github.com/clvrai/skild}{https://github.com/clvrai/skild}, and D4RL's open-source code for AntMaze at \href{https://github.com/Farama-Foundation/D4RL/tree/master}{https://github.com/Farama-Foundation/D4RL/tree/master}.

The hyperparameters for low-level policy training were referenced from SPiRL \cite{pertsch2021accelerating}. Additional details about the baseline algorithms are provided in Section \ref{appsec:basedetail}, while Section \ref{appsubsec:envdetail} elaborates on the environments used for evaluation. Section \ref{appsubsec:dataset} explains the construction of offline datasets for varying noise levels, and Section \ref{appsubsec:hyper} details the network architectures and hyperparameter configurations for policies, value functions, and other models. 










\subsection{Other Baselines}
\label{appsec:basedetail}
Here are the detailed descriptions and implementation details of the algorithms used for performance comparison:

{\bf SAC}\\
SAC \cite{haarnoja2018soft} is a reinforcement learning algorithm that incorporates entropy to improve exploration. Instead of a standard value function, SAC uses a soft value function that combines entropy, with the entropy coefficient adjusted automatically to maintain the target entropy. To enhance value function estimation, SAC employs double $Q$ learning, using two independent value functions. SAC learns tasks from scratch without utilizing meta-train tasks or offline datasets. For the Kitchen and Office environments, the discount factor $\gamma$ is set to $0.95$, while $\gamma=0.99$ is used for Maze2D and AntMaze environments.

{\bf SAC+RND}\\
SAC+RND combines SAC with random network distillation (RND) \cite{burda2018exploration}, an intrinsic motivation technique, to enhance exploration. Like SAC, it learns tasks from scratch without meta-train tasks or offline datasets. RL hyperparameters are shared with SAC, and RND-specific hyperparameters are set to match those in PRISM. Additionally, for fair comparison, the ratio of extrinsic to intrinsic rewards is aligned with PRISM.

{\bf PEARL}\\
PEARL \cite{rakelly2019efficient} is a context-based meta-RL algorithm that leverages a task encoder $q_e$ to derive task representations, which are then used to train a meta-policy. PEARL adapts its learned policy quickly to unseen tasks without utilizing skills or offline datasets. Unlike the original PEARL, which does not fine-tune during the meta-test phase, we modified it to include fine-tuning on target tasks for a fair comparison.

{\bf PEARL+RND}\\
PEARL+RND extends PEARL by incorporating RND to enhance exploration. Like SAC+RND, the ratio of extrinsic to intrinsic rewards is set to match PRISM for fair comparison.

{\bf SPiRL}\\
SPiRL \cite{pertsch2021accelerating} is a skill-based RL algorithm that first learns a fixed low-level policy from an offline dataset and then trains a high-level policy for specific tasks. SPiRL's loss function is detailed in Section \ref{sec:background}, and for fair comparison, loss scaling is aligned with PRISM.

{\bf SiMPL}\\
SiMPL \cite{nam2022simpl} is a skill-based meta-RL algorithm that uses both offline datasets and meta-train tasks. While it shares PRISM's approach of extracting reusable skills and performing meta-train and meta-test phases, SiMPL fixes the skill model without further updates during meta-training.  SiMPL's loss function is also detailed in Section \ref{sec:background}, and SiMPL's implementation uses the same hyperparameters as PRISM to ensure a fair comparison.

\newpage
\subsection{Environmental Details}
\label{appsubsec:envdetail}
{\bf Kitchen}
\begin{figure}[ht]
    \centerline{\includegraphics[width=0.45\columnwidth]{images/B-2_kitchen-detail.jpg}}
    \caption{Kitchen environment: An example of performing a task (microwave$\rightarrow$kettle$\rightarrow$bottom burner$\rightarrow$slide cabinet)}
    \label{fig:kitchen-detail}
    \vskip -0.2in
\end{figure}


The Franka Kitchen environment is a robotic manipulation setup based on the 7-DoF Franka robot. It is introduced by \citet{gupta2020relay} and later adapted by \citet{nam2022simpl} to exclude task information from the observation space, making it more suitable for meta-learning. The environment features seven manipulatable objects: bottom burner, top burner, light switch, slide cabinet, hinge cabinet, microwave, and kettle.

Each subtask involves manipulating one object to its target state, while a full task requires sequentially completing four subtasks. The agent earns a reward of 1 for each completed subtask, with a maximum score of 4 achievable within a 280-timestep horizon. For instance, an example task, illustrated in Fig. \ref{fig:kitchen-detail}, requires the agent to complete the sequence: microwave $\rightarrow$ kettle $\rightarrow$ bottom burner $\rightarrow$ slide cabinet. The observation space is a 60-dimensional continuous vector representing object positions and robot state information, while the action space is a 9-dimensional continuous vector. Based on the task setup from \citet{nam2022simpl}, we expanded the meta-train task set by adding two additional tasks, resulting in a total of 25 meta-train tasks and 10 meta-test tasks. Detailed task configurations are provided in Table \ref{table:kitchen-tasks}.
\begin{table}[ht!]
    \scriptsize
    \centering
    \begin{tabular}{|c|cccc|c|cccc|}
    \hline
        \multicolumn{5}{|c|}{\small Meta-train task} & \multicolumn{5}{c|}{\small Meta-test task} \\
        \cline{1-5} \cline{6-10}
        \# & Subtask1 & Subtask2 & Subtask3 & Subtask4 & \# & Subtask1 & Subtask2 & Subtask3 & Subtask4 \\
    \hline
        1 & microwave & kettle & bottom burner & slide cabinet & 1 & microwave & bottom burner & light switch & top burner \\
        2 & microwave & bottom burner & top burner & slide cabinet & 2 & microwave & bottom burner & top burner & light switch \\
        3 & microwave & top burner & light switch & hinge cabinet & 3 & kettle & bottom burner & light switch & slide cabinet \\
        4 & kettle & bottom burner & light switch & hinge cabinet & 4 & microwave & kettle & top burner & hinge cabinet \\
        5 & microwave & bottom burner & hinge cabinet & top burner & 5 & kettle & bottom burner & slide cabinet & top burner \\
        6 & kettle & top burner & light switch & slide cabinet & 6 & kettle & light switch & slide cabinet & hinge cabinet \\
        7 & microwave & kettle & slide cabinet & bottom burner & 7 & kettle & bottom burner & top burner & slide cabinet \\
        8 & kettle & light switch & slide cabinet & bottom burner & 8 & microwave & bottom burner & slide cabinet & hinge cabinet \\
        9 & microwave & kettle & bottom burner & top burner & 9 & bottom burner & top burner & slide cabinet & hinge cabinet \\
        10 & microwave & kettle & slide cabinet & hinge cabinet & 10 & microwave & kettle & bottom burner & hinge cabinet \\
        11 & microwave & bottom burner & slide cabinet & top burner &&&&&\\
        12 & kettle & bottom burner & light switch & top burner &&&&&\\
        13 & microwave & kettle & top burner & light switch &&&&&\\
        14 & microwave & kettle & light switch & hinge cabinet &&&&&\\
        15 & microwave & bottom burner & light switch & slide cabinet &&&&&\\
        16 & kettle & bottom burner & top burner & light switch &&&&&\\
        17 & microwave & light switch & slide cabinet & hinge cabinet &&&&&\\
        18 & microwave & bottom burner & top burner & hinge cabinet &&&&&\\
        19 & kettle & bottom burner & slide cabinet & hinge cabinet &&&&&\\
        20 & bottom burner & top burner & slide cabinet & light switch &&&&&\\
        21 & microwave & kettle & light switch & slide cabinet &&&&&\\
        22 & kettle & bottom burner & top burner & hinge cabinet &&&&&\\
        23 & bottom burner & top burner & light switch & slide cabinet &&&&&\\
        24 & top burner & hinge cabinet & microwave & slide cabinet &&&&&\\
        25 & bottom burner & hinge cabinet & light switch & kettle &&&&&\\
    \hline
    \end{tabular}
    \caption{List of meta-train tasks and meta-test tasks in Kitchen environment}
    \label{table:kitchen-tasks}
\end{table}


\newpage
{\bf Office}\\
\begin{figure}[ht!]
    \centerline{\includegraphics[width=0.45\columnwidth]{images/B-2_office-detail.jpg}}
    \caption{Office environment: An example of performing a task ((shed2, drawer)$\rightarrow$(eraser1, container)$\rightarrow$(pepsi2, tray))}
    \label{fig:office-detail}
    \vskip -0.2in
\end{figure}


The Office environment is a robotic manipulation setup featuring a 5-DoF robotic arm. Originally proposed by \citet{pertsch2022guided}, it has been modified to accommodate meta-learning tasks. The environment simulates an office cleaning scenario with seven objects (eraser1, shed1, pepsi1, gatorade, eraser2, shed2, pepsi2) and three organizers (tray, container, drawer).

The goal is to move objects to their designated organizers, with each object-to-organizer transfer constituting a subtask. A full task involves completing three sequential subtasks, where a subtask is defined as an (object, organizer) pair. For tasks involving a tray or container, the agent earns a reward of 1 for both picking and placing the object. For tasks involving the drawer, the agent receives 1 reward point for each of the following actions: opening the drawer, picking, placing, and closing the drawer. This scoring setup allows for a maximum score of 8 within a 300-timestep horizon.

An example task, depicted in Fig. \ref{fig:office-detail}, requires the agent to sequentially complete: (shed2 $\rightarrow$ drawer), (eraser1 $\rightarrow$ container), and (pepsi2 $\rightarrow$ tray). The observation space is a 76-dimensional continuous vector, including object positions and robot state information, while the action space is an 8-dimensional continuous vector. The meta-train and meta-test sets include 25 and 10 tasks, respectively, similar to the configuration in the Kitchen environment. A detailed task list is provided in Table \ref{table:office-tasks}.

\begin{table}[ht!]
    \scriptsize
    \centering
    \begin{tabular}{|c|ccc|c|ccc|}
    \hline
        \multicolumn{4}{|c|}{\small Meta-train task} & \multicolumn{4}{c|}{\small Meta-test task} \\
        \cline{1-4} \cline{5-8}
        \# & Subtask1 & Subtask2 & Subtask3 & \# & Subtask1 & Subtask2 & Subtask3 \\
    \hline
        1 & (shed2, drawer) & (eraser1, container) & (pepsi2, tray) & 1 & (gatorade, drawer) & (eraser1, tray) & (pepsi2, container) \\
        2 & (shed2, container) & (eraser1, drawer) & (pepsi1, tray) & 2 & (eraser1, drawer) & (eraser2, container) & (pepsi1, tray) \\
        3 & (eraser1, tray) & (shed2, drawer) & (gatorade, container) & 3 & (eraser2, drawer) & (pepsi1, tray) & (gatorade, container) \\
        4 & (pepsi1, tray) & (eraser1, container) & (eraser2, drawer) & 4 & (shed2, drawer) & (pepsi2, tray) & (pepsi1, container) \\
        5 & (shed1, tray) & (shed2, drawer) & (pepsi2, container) & 5 & (shed2, container) & (gatorade, tray) & (eraser1, drawer) \\
        6 & (pepsi1, container) & (shed1, tray) & (eraser2, drawer) & 6 & (gatorade, container) & (eraser2, drawer) & (pepsi2, tray) \\
        7 & (gatorade, tray) & (eraser2, container) & (eraser1, drawer) & 7 & (gatorade, tray) & (shed1, container) & (eraser1, drawer) \\
        8 & (pepsi2, container) & (shed2, drawer) & (eraser1, tray) & 8 & (pepsi2, drawer) & (shed1, tray) & (pepsi1, container) \\
        9 & (shed2, drawer) & (gatorade, container) & (pepsi2, tray) & 9 & (pepsi1, tray) & (pepsi2, container) & (shed2, drawer) \\
        10 & (eraser1, container) & (pepsi2, drawer) & (shed1, tray) & 10 & (gatorade, drawer) & (pepsi1, container) & (eraser2, tray) \\
        11 & (eraser2, drawer) & (shed2, tray) & (pepsi2, container) &&&& \\
        12 & (pepsi2, container) & (shed2, drawer) & (shed1, tray) &&&& \\
        13 & (shed2, tray) & (pepsi1, container) & (eraser1, drawer) &&&& \\
        14 & (gatorade, tray) & (eraser1, drawer) & (pepsi1, container) &&&& \\
        15 & (eraser1, tray) & (shed1, drawer) & (gatorade, container) &&&& \\
        16 & (eraser2, drawer) & (gatorade, container) & (shed2, tray) &&&& \\
        17 & (shed2, tray) & (pepsi2, drawer) & (shed1, container) &&&& \\
        18 & (pepsi1, container) & (pepsi2, tray) & (eraser1, drawer) &&&& \\
        19 & (shed2, tray) & (gatorade, drawer) & (shed1, container) &&&& \\
        20 & (gatorade, tray) & (pepsi1, container) & (pepsi2, drawer) &&&& \\
        21 & (eraser1, tray) & (shed2, drawer) & (pepsi2, container) &&&& \\
        22 & (eraser1, tray) & (gatorade, drawer) & (shed2, container) &&&& \\
        23 & (pepsi1, container) & (shed2, drawer) & (eraser2, tray) &&&& \\
        24 & (gatorade, drawer) & (shed1, tray) & (pepsi2, container) &&&& \\
        25 & (eraser2, container) & (pepsi1, drawer) & (eraser1, tray) &&&& \\
    \hline
    \end{tabular}
    \caption{List of meta-train tasks and meta-test tasks in Office environment}
    \label{table:office-tasks}
\end{table}


\newpage
{\bf Maze2D}
\begin{figure}[ht!]
    \centering
    \subfigure[Simulation]{\includegraphics[width=0.41\columnwidth]{images/5-2_maze.png}}%
    \subfigure[Training and test tasks]{\raisebox{-0.1cm}{\includegraphics[width=0.28\columnwidth]{images/B-2_maze.png}}}%
    \caption{Maze2D environment: Visualization of simulation and meta-training/task tasks in Maze2D}
    \label{fig:maze-detail}
    \vskip -0.2in
\end{figure}

The Maze2D environment is a navigation setup where a 2-DoF ball agent moves toward a goal point. Initially introduced by \citet{fu2020d4rl} and later adapted by \citet{nam2022simpl} for meta-learning tasks, the environment is defined on a 20x20 grid. The agent receives a reward of 1 upon reaching the goal point within a horizon of 2000 timesteps.

Fig. \ref{fig:maze-detail} (a) provides a visualization of the Maze2D environment, while Fig. \ref{fig:maze-detail} (b) illustrates the meta-train and meta-test tasks. In Fig. \ref{fig:maze-detail} (b), green squares indicate the goal points for 40 meta-train tasks, and red squares represent the goal points for 10 meta-test tasks. All tasks share the same starting point at (10, 10), marked by a blue cross. The observation space is a 4-dimensional continuous vector containing the ball's position and velocity, while the action space is a 2-dimensional continuous vector.


{\bf AntMaze}
\begin{figure}[ht!]
    \centering
    \subfigure[Simulation]{\includegraphics[width=0.41\columnwidth]{images/5-2_antmaze.png}}%
    \subfigure[Training and test tasks]{\raisebox{-0.1cm}{\includegraphics[width=0.28\columnwidth]{images/B-2_antmaze.png}}}%
    \caption{AntMaze environment: Visualization of simulation and meta-training/task tasks in AntMaze}
    \label{fig:antmaze-detail}
    \vskip -0.2in
\end{figure}

The AntMaze environment combines navigation and locomotion, replacing the 2-DoF ball from the Maze2D environment with a more complex 8-DoF quadruped Ant robot. Initially proposed by \citet{fu2020d4rl} and later adapted for meta-learning setups, the environment is defined on a 10x10 grid. The agent receives a reward of 1 upon reaching the goal point within a horizon of 1000 timesteps.

Fig. \ref{fig:antmaze-detail} (a) shows a simulation image of the AntMaze environment, and Fig. \ref{fig:antmaze-detail} (b) depicts the meta-train and meta-test tasks. In Fig. \ref{fig:antmaze-detail} (b), green squares mark the goal points for 20 meta-train tasks, while red squares denote the goal points for 10 meta-test tasks. All tasks share a common starting point at (5, 5), indicated by a blue cross. The observation space is a 29-dimensional continuous vector that includes the Ant's state and its $(x, y)$ coordinates, while the action space is an 8-dimensional continuous vector.


\newpage
\subsection{Construction of Offline Dataset}
\label{appsubsec:dataset}
In this section, we detail the offline datasets used in our experiments. For the Office, Maze2D, and AntMaze environments, we employ rule-based oracle controllers provided by each environment. The Office oracle controller is available at \href{https://github.com/clvrai/skild}{https://github.com/clvrai/skild}, while the Maze2D and AntMaze oracle controllers can be found in \href{https://github.com/Farama-Foundation/D4RL/tree/master}{https://github.com/Farama-Foundation/D4RL/tree/master}. For the Kitchen environment, which only provides human demonstrations, we train a policy using behavior cloning to serve as the oracle controller.

For the Kitchen environment, 1M transitions are collected using 25 tasks that are not part of the training or test task sets $\mathcal{M}_\text{train}\cup\mathcal{M}_\text{test}$. Similarly, the Office environment collects 1M transitions using 80 tasks. The Maze2D and AntMaze environments follow the same approach, collecting 0.5M transitions using 40 and 50 tasks respectively, with randomly sampled initial and goal points. Unlike SiMPL, which randomly samples initial and goal points for each trajectory in the Maze2D environment, we limit our data collection to 40 distinct tasks, resulting in trajectories that do not fully cover the map. To introduce noise in the demonstrations, Gaussian noise with various standard deviations $\sigma$ is added to the oracle controller's actions. For the Kitchen and Office environments, noise levels of $\sigma=0.1, 0.2,$ and $0.3$ are used, while for Maze2D and AntMaze, $\sigma=0.5, 1.0,$ and $1.5$ are applied.

\newpage
\subsection{Hyperparameter Setup}
\label{appsubsec:hyper}




In this section, we outline the hyperparameter setup for the proposed PRISM framework. For high-level policy training, we adopt the hyperparameters from SiMPL for the Kitchen and Maze2D environments. For the Office and AntMaze environments, we conduct hyperparameter sweeps using the Kitchen and Maze2D configurations as baselines.

Most of PRISM's hyperparameters are inherited from SiMPL, given its multiple loss functions. However, parameter sweeps are specifically conducted for the proposed prioritized skill refinement and maximum return relabeling components. We explore prioritization temperature values $T\in[0.1,0.5,1.0,2.0]$ and KLD coefficients $\lambda^\text{kld}_\text{exp}\in[0,0.001,0.002,0.005]$ for skill exploration, selecting the best-performing configurations as defaults. Additionally, the ratio of intrinsic to extrinsic rewards is fixed at levels that show optimal performance in single-task SAC experiments.

For implementing the skill models ($\pi_l,q,p$), we follow SPiRL by utilizing LSTM \cite{graves2012long} for the skill encoder and MLP structures for the low-level policy and skill decoder. For implementing the high-level models ($\pi_h,Q_h,q_e$), we follow SiMPL by utilizing Set Transformer \cite{lee2019set} for the task encoder and MLP structures for the high-level policy and value function. Additionally, for implementing the PRISM framework, we utilize MLP structures for $\pi_\text{exp}$, $Q_\text{exp}$, and $\hat R$. The detailed hidden network sizes are presented in Table \ref{table:hyper-shared} and Table \ref{table:hyper-env}.
Table \ref{table:hyper-shared} presents the network architectures (the number of nodes in fully connected layers) and the hyperparameters shared across all environments, while Table \ref{table:hyper-env} details the environment-specific hyperparameter setups.
\vspace{-1em}
\begin{table}[!ht]
\centering
\caption{Network Architecture and Shared Hyperparameters}
\label{table:hyper-shared}
\renewcommand{\arraystretch}{1.2}
\resizebox{0.65 \textwidth}{!}{%
    \begin{tabular}{|c|c|c|cccc|}
    \hline
    \multirow{21}{*}{\begin{tabular}[c]{@{}c@{}}Shared\\Hyperparameters\end{tabular}} & \multirow{2}{*}{Group} & \multirow{2}{*}{Name} & \multicolumn{4}{c|}{Environments} \\
    \cline{4-7}
    & & & Kitchen & Office & Maze2D & AntMaze \\
% --- high-level
    \cline{2-7}
    % & \multirow{7}{*}{\begin{tabular}[c]{@{}c@{}}High-level\end{tabular}} & Buffer size $\mathcal{B}^i_h$ & \multicolumn{2}{c}{3000} & \multicolumn{2}{c|}{20000} \\
    & \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}High-level\end{tabular}} & Discount Factor $\gamma_h$ & \multicolumn{4}{c|}{0.99} \\
    & & Learning rate $\lambda^\text{lr}_h$ & \multicolumn{4}{c|}{0.0003} \\
    % & & Buffer size $\mathcal{B}^i_h$ & \multicolumn{2}{c}{3000} & \multicolumn{2}{c|}{20000} \\
    % & & Discount factor $\gamma_h$ & \multicolumn{2}{c}{0.99} & \multicolumn{2}{c|}{0.99} \\
    % & & Task latent dimension & \multicolumn{2}{c}{6} & \multicolumn{2}{c|}{5} \\
    % & & Initial KLD coefficient $\alpha_h$ & \multicolumn{2}{c}{0.03} & 0.001 & 0.0003 \\
    % & & RL batch size (per task) & \multicolumn{2}{c}{256} & 1024 & 512 \\
    % & & Context batch size (per task) $N_\text{prior}$ & \multicolumn{2}{c}{1024} & 8192 & 4096 \\
    & & Network size $\pi_h,Q_h$ & \multicolumn{2}{c}{[128]$\times$6} & [256]$\times$4 & [128]$\times$6 \\
% --- low-level
    \cline{2-7}
    & \multirow{11}{*}{\begin{tabular}[c]{@{}c@{}}Low-level\end{tabular}} & Buffer size $\mathcal{B}^i_\text{on}$ & \multicolumn{4}{c|}{10K} \\
    & & KLD coefficient $\lambda^\text{kld}_l$ & \multicolumn{4}{c|}{0.0005} \\
    % & & Prioritization temperature $T$ & \multicolumn{2}{c}{1.0} & \multicolumn{2}{c|}{0.5} \\
    & & Skill length $H_s$ & \multicolumn{4}{c|}{10} \\
    & & Skill dimension $\text{dim}(z)$ & \multicolumn{4}{c|}{10} \\
    & & \# of priority update trajectory $N_\text{priority}$ & \multicolumn{4}{c|}{200} \\
    & & Learning rate $\lambda^\text{lr}_\text{skill}$ & \multicolumn{4}{c|}{0.001} \\
    & & Learning rate $\lambda^\text{lr}_\text{reward}$ & \multicolumn{4}{c|}{0.0003} \\
    & & Network size $\hat R$ & \multicolumn{4}{c|}{[128]$\times$3} \\
    & & Network size $\pi_l$ & \multicolumn{4}{c|}{[128]$\times$6} \\
    & & Network size $p$ & \multicolumn{4}{c|}{[128]$\times$7} \\
    & & Network size $q$ & \multicolumn{4}{c|}{LSTM[128]} \\
    % & & Skill refinement $K_\text{iter}$ & \multicolumn{2}{c}{1000} & 500 & 1000 \\
% --- exploration
    \cline{2-7}
    % & \multirow{5}{*}{\begin{tabular}[c]{@{}c@{}}Exploration\end{tabular}} & Buffer size $\mathcal{B}^i_\text{exp}$ & 100K & 200K & 100K & 300K \\
    & \multirow{5}{*}{\begin{tabular}[c]{@{}c@{}}Exploration\end{tabular}} & RND state dropout ratio & \multicolumn{4}{c|}{0.7} \\
    % & & Discount factor $\gamma_\text{exp}$ & \multicolumn{2}{c}{0.95} & \multicolumn{2}{c|}{0.99} \\
    % & & Initial entropy coefficient $\beta_\text{exp}$ & \multicolumn{2}{c}{0.2} & \multicolumn{2}{c|}{0.1} \\
    % & & KLD coefficient $\alpha_\text{exp}$ & \multicolumn{2}{c}{0.001} & \multicolumn{2}{c|}{0.001} \\
    % & & RND extrinsic ratio $\delta_\text{ext}$ & 5 & 2 & 10 & 10 \\
    % & & RND intrinsic ratio $\delta_\text{int}$ & \multicolumn{2}{c}{0.1} & \multicolumn{2}{c|}{0.01} \\
    % & & RND state dropout ratio & \multicolumn{4}{c|}{0.7} \\
    & & RND output dimension & \multicolumn{4}{c|}{10} \\
    & & Learning rate $\lambda^\text{lr}_\text{exp}$ & \multicolumn{4}{c|}{0.0003} \\
    & & Network size $\pi_\text{exp},Q_\text{exp}$ & \multicolumn{4}{c|}{[256]$\times$4} \\
    & & Network size $f,\hat f$ & \multicolumn{4}{c|}{[128]$\times$4} \\
     \hline
    \end{tabular}}
\end{table}

\vspace{-2em}

\begin{table}[!ht]
\centering
\caption{Environmental Hyperparameters}
\label{table:hyper-env}
\renewcommand{\arraystretch}{1.2}
\resizebox{0.65 \textwidth}{!}{%
    \begin{tabular}{|c|c|c|cccc|}
    \hline
    \multirow{19}{*}{\begin{tabular}[c]{@{}c@{}}Environmental\\Hyperparameters\end{tabular}} & \multirow{2}{*}{Group} & \multirow{2}{*}{Name} & \multicolumn{4}{c|}{Environments} \\
    \cline{4-7}
    & & & Kitchen & Office & Maze2D & AntMaze \\
    \cline{2-7}
    & \multirow{5}{*}{\begin{tabular}[c]{@{}c@{}}High-level\end{tabular}} & Buffer size $\mathcal{B}^i_h$ & 3000 & 3000 & 20000 & 20000\\
    & & KLD coefficient $\lambda^\text{kld}_h$ & 0.03 & 0.03 & 0.001 & 0.0003\\
    & & Task latent dimension $\text{dim}(e)$ & 5 & 5 & 6 & 6\\
    & & Batch size (RL, per task) & 256 & 256 & 1024 & 512 \\
    & & Batch size (context, per task) & 1024 & 1024 & 8192 & 4096 \\
    
    \cline{2-7}
    & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Low-level\end{tabular}} & Skill refinement $K_\text{iter}$ & 2000 & 2000 & 1000 & 2000\\
    & & Prioritization temperature $T$ & 1.0 & 1.0 & 0.5 & 0.5\\

    \cline{2-7}
    & \multirow{9}{*}{\begin{tabular}[c]{@{}c@{}}Exploration\end{tabular}} & Buffer size $\mathcal{B}^i_\text{exp}$ & 100K & 200K & 100K & 300K\\
    & & Discount factor $\gamma_\text{exp}$ & 0.95 & 0.95 & 0.99 & 0.99 \\
    & & RND extrinsic ratio $\delta_\text{ext}$ & 5 & 2 & 10 & 10 \\
    & & RND intrinsic ratio $\delta_\text{int}$ & 0.1 & 0.1 & 0.01 & 0.01 \\
    & & Entropy coefficient $\lambda^\text{ent}_\text{exp}$ & 0.2 & 0.2 & 0.1 & 0.1\\
    & & \multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}KLD coefficient $\lambda^\text{kld}_\text{exp}$\end{tabular}} & 0.005 (Expert) & \multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}0.001\end{tabular}} & \multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}0.001\end{tabular}} & \multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}0.001\end{tabular}}\\
    & & & 0.005 ($\sigma=0.1$) & & & \\
    & & & 0.002 ($\sigma=0.2$) & & & \\
    & & & 0.001 ($\sigma=0.3$) & & & \\
     \hline
    \end{tabular}}
\end{table}


\clearpage
\newpage
\section{Additional Comparison Results}
\label{appsec:addcomp}
\subsection{Performance Comparison}



Fig. \ref{fig:metatest} presents the learning curves of average returns for the algorithms on test tasks, corresponding to the experiments summarized in Table \ref{table:perf}. Rows represent evaluation environments, and columns denote noise levels. PRISM consistently demonstrated superior robustness, outperforming all baselines across various environments and noise levels.

At higher noise levels such as Noise($\sigma=0.2$), Noise($\sigma=0.3$) for Kitchen and Office, and Noise($\sigma=1.0$), Noise($\sigma=1.5$) for Maze2D and AntMaze, significant performance improvements highlight the effectiveness of skill refinement in addressing noisy demonstrations. Even with high-quality offline datasets like Expert and Noise($\sigma=0.1$) for Kitchen and Office, and Noise($\sigma=0.5$) for Maze2D and AntMaze, PRISM further improved performance by learning task-relevant skills. These learning curves align with the trends observed in the main experiments, confirming that skill refinement enhances performance across various dataset qualities and effectively adapts to the task distribution.

\begin{figure}[!ht]
    % \vskip 0.2in
    \centerline{\includegraphics[width=0.9\columnwidth]{images/C-1_metatest.pdf}}
    \vskip -0.2in    
\caption{Performance comparison: Learning curves across considered environments and noise levels}
    \label{fig:metatest}
    \vskip -0.1in
\end{figure}


\subsection{Evolution of The Control Factor $\beta$}



Fig. \ref{fig:control-factor} illustrates the evolution of the control factor $\beta$ during the meta-train phase across all environments and noise levels. Initially, low $\beta$ values reflect reliance on offline datasets for skill learning, particularly in high-return environments like Kitchen and Office, where training starts with $\beta$ values close to zero. This approach prevents performance degradation by avoiding early dependence on lower-quality online samples, while ensuring gradual and stable changes in the skill distribution. As training progresses, the quality of online samples improves, leading to a gradual increase in $\beta$, which facilitates greater utilization of online data for skill refinement. For offline datasets with higher noise levels, $\beta$ converges to higher values. Consequently, PRISM learns increasingly effective skills as training proceeds, achieving superior performance on unseen tasks, underscoring the importance of PRISM's ability to dynamically balance the use of offline and online data based on dataset quality.


\begin{figure}[!ht]
    % \vskip 0.2in
    \centerline{\includegraphics[width=0.9\columnwidth]{images/C-2_batchratio.pdf}}
    \caption{The evolution of the control factor $\beta$ during the meta-train phase}
    \label{fig:control-factor}
    % \vskip -0.2in
\end{figure}



\section{Additional Visualization Results for PRISM}
\label{appsec:visual}


\subsection{Improvement in Task Representation through Skill Refinement}
\label{appsec:taskenc}

Fig. \ref{fig:appendix-task-representation} illustrates the effect of skill refinement on task representation through t-SNE visualizations of task embeddings $e^\mathcal{T} \sim q_e$ in the Kitchen environment ($\sigma=0.3$), with different tasks represented by distinct colors. In Fig. \ref{fig:appendix-task-representation} (a), the task encoder is trained using fixed skills directly derived from noisy demonstrations. The noisy skills obstruct the encoder’s ability to form clear task representations, making task differentiation challenging. This limitation highlights why, in SiMPL, relying on skills learned from noisy datasets can sometimes result in poorer performance compared to SPiRL, which focuses on task-specific skill learning. 

Conversely, Fig. \ref{fig:appendix-task-representation} (b) presents the t-SNE visualization when the task encoder is trained during the meta-train phase with refined skills. The improved skills enable the encoder to form more distinct and task-specific representations, facilitating better task discrimination. This improvement allows the high-level policy to differentiate tasks more effectively and select optimal skills for each, thereby enhancing meta-RL performance. These findings demonstrate that the proposed skill refinement not only improves the low-level policy but also significantly enhances the task encoder’s ability to represent and distinguish tasks, contributing to overall performance improvements.


\begin{figure}[!ht]
    % \vskip 0.2in
    \centerline{\includegraphics[width=0.8\columnwidth]{images/D-1_task-embedding.pdf}}
    \caption{t-SNE visualization of task representations in the Kitchen environment ($\sigma=0.3$): (a) Using only the noisy offline dataset, (b) Proposed PRISM trained with refined skills}
    \label{fig:appendix-task-representation}
\end{figure}
\newpage
\subsection{Additional Visualizations of Refined Skills}
\label{appsec:skillrep}

Here, we present additional visualization results for PRISM in the Kitchen and Maze2D environments. Fig. \ref{fig:appendix-kitchen} illustrates the results in the Kitchen environment ($\sigma=0.3$) after training is completed. On the left, the t-SNE visualization shows the skill representation $z \sim \pi_{h,\theta}$, while the right side highlights the distribution of skills corresponding to each subtask in the t-SNE map and how these skills solve subtasks over time in the Kitchen environment. In the t-SNE map, clusters of markers with the same shape but different colors indicate that identical subtasks share skills across different tasks. From the results, it is evident that the skills learned using the proposed PRISM framework are well-structured, with representations accurately divided according to subtasks. This enables the high-level policy to select appropriate skills for each subtask, effectively solving the tasks. Furthermore, while SiMPL trained on noisy data often succeeds in only one or two subtasks, PRISM progressively refines skills even in noisy environments, successfully solving most given subtasks. 


Fig. \ref{fig:appendix-maze-test} illustrates how the high-level policy utilizes refined skills obtained at different meta-train iterations (0.5K, 2K, and 4K) during the meta-test phase to solve a task in Maze2D ($\sigma=1.5$). When using skills trained solely on the offline dataset (meta-train iteration 0.5K), the agent failed to perform adequate exploration at meta-test iteration 0K. Even at meta-test iteration 0.5K, the noise within the skills hindered the agent's ability to converge to the target task. 
In contrast, after refining the skills at meta-train iteration 2K, the agent successfully explored most of the maze during exploration, except for certain tasks in the upper-left region, and achieved all meta-test tasks by iteration 0.5K. Finally, using skills refined at meta-train iteration 4K, the agent not only explored almost the entire maze at meta-test iteration 0K but also completed all meta-test tasks by iteration 0.5K. Additionally, trajectories generated with refined skills showed significantly reduced deviations and shorter paths compared to those using noisy skills. Overall, the results in Fig. \ref{fig:appendix-maze-test} highlight the importance of PRISM's skill refinement process in ensuring robust and efficient performance.

\clearpage
\begin{figure}[!ht]
    % \vskip -0.4in
    \centerline{\includegraphics[width=0.65\columnwidth]{images/D-2_skill_kitchen.pdf}}
    % \vskip -0.1in
    \caption{t-SNE visualization of skill representations (left) and refined skill trajectories for various subtasks (right) in Kitchen ($\sigma=0.3$). In the skill representation, marker colors denote tasks, while marker shapes indicate subtasks.}
    % \vskip -0.1in
    \label{fig:appendix-kitchen}
\end{figure}


\begin{figure}[!ht]
    % \vskip -0.4in
    \centerline{\includegraphics[width=0.7\columnwidth]{images/D-2_metatest_maze.pdf}}
    \vskip -0.1in
    \caption{Illustration of trajectories of refined skills during the meta-test phase in Maze2D ($\sigma=1.5$), across various training and test iterations.}
    \vskip -0.1in
    \label{fig:appendix-maze-test}
\end{figure}

\newpage
\section{Additional Ablation Studies}
\label{appsec:addabl}

In this section, we conduct additional ablation studies for Kitchen and Maze2D environments across all noise levels. These studies include component evaluation and PRISM's skill refinement-related hyperparameters discussed in Section \ref{appsubsec:comp}, prioritization temperature $T$ in Section \ref{appsubsec:temp}, and the KLD coefficient $\lambda^\text{kld}_\text{exp}$ for the exploration policy in Section \ref{appsubsec:kld}.


\subsection{Component Evaluation}
\label{appsubsec:comp}

Fig. \ref{fig:appendix-component} presents comprehensive results across all noise levels from the component evaluation in Section \ref{subsec:abl}. While improvements are modest under conditions with high-quality offline datasets, such as Expert and Noise ($\sigma=0.1$) for Kitchen, and Expert and Noise ($\sigma=0.5$) for Maze2D, notable performance gains are still observed. The significant degradation observed in the absence of the exploration policy $\pi_\text{exp}$ highlights its crucial role in mitigating minor noise and discovering improved paths. For higher noise levels, such as Noise ($\sigma=0.2$), Noise ($\sigma=0.3$) for Kitchen, and Noise ($\sigma=1.0$), Noise ($\sigma=1.5$) for Maze2D, excluding the online buffer or prioritized skill refinement ($\mathcal{B}_\text{on}$ or $P_{\mathcal{B}_\text{off}}$) caused significant performance drops, emphasizing the importance of maximum return relabeling in PRISM. Additionally, relying solely on the online buffer without utilizing the offline dataset led to performance deterioration, demonstrating the offline dataset's value in addressing meta-test tasks involving behaviors not available during meta-train. Beyond these specific findings, most trends align with the results discussed in the main text, further validating the effectiveness of PRISM's components across different noise levels.


\begin{figure}[!ht]
    \vspace{-1em}
    % \vskip -0.4in
    \centerline{\includegraphics[width=0.9\columnwidth]{images/E-1_component.pdf}}
    \caption{Component evaluation across all noise levels in Kitchen and Maze2D}
    \label{fig:appendix-component}
    \vskip -0.2in
\end{figure}

\subsection{Prioritization Temperature $T$}
\label{appsubsec:temp}

The prioritization temperature $T$ regulates the balance between sampling from online and offline buffers. Fig. \ref{fig:appendix-temp} shows performance across different noise levels in Kitchen and Maze2D environments as $T$ varies. Low $T$ values lead to excessive sampling from high-return buffers, while high $T$ approximates uniform sampling, diminishing the prioritization effect. Both environments experienced degraded performance at $T=0.1$ and $T=2.0$, highlighting the importance of proper tuning. In the Kitchen environment (maximum return = 4), $T=1.0$ achieved the best performance across all noise levels, whereas in the Maze2D environment (maximum return = 1), $T=0.5$ was optimal. This difference occurs because environments with lower max returns exhibit smaller gaps between low- and high-return buffers, reducing the effect of prioritization. Based on these findings, we set the best-performing hyperparameter values as defaults for each environment.



\subsection{KLD Coefficient $\lambda^\text{kld}_\text{exp}$}
\label{appsubsec:kld}


The KLD coefficient $\lambda^\text{kld}_\text{exp}$ regulates the strength of the KLD term between the exploration policy and the action distribution induced by the prioritized online buffer $\mathcal{B}_{\text{on}}^i$ for each task $\mathcal{T}^i$. Fig. \ref{fig:appendix-kld-coeff} illustrates performance variations with $\lambda^\text{kld}_\text{exp}\in[0, 0.001,0.002,0.005]$.

In the Kitchen environment, $\lambda^\text{kld}_\text{exp}=0.005$ performed best for Expert and Noise ($\sigma=0.1$), while $\lambda^\text{kld}_\text{exp}=0.002$ and $\lambda^\text{kld}_\text{exp}=0.001$ were optimal for Noise ($\sigma=0.2$) and Noise ($\sigma=0.3$), respectively. At lower noise levels, the high-level policy benefits from quickly following high-return samples, whereas at higher noise levels, focusing on exploration to discover shorter paths becomes more advantageous. For the Maze2D environment, performance was consistent across $\lambda^\text{kld}_\text{exp}=0.001, 0.002,$ and $0.005$, with only minor variations observed. However, when $\lambda^\text{kld}_\text{exp}=0$, removing the KLD term resulted in significant performance degradation across all noise levels in both Kitchen and Maze2D environments. This highlights the necessity of guidance from high-return samples for effectively solving long-horizon tasks.  Based on these results, we selected the best-performing hyperparameter values as defaults for each environment.

\begin{figure}[!ht]
    % \vskip 0.2in
    \centerline{\includegraphics[width=0.9\columnwidth]{images/E-2_returntemp.pdf}}
    \vskip -0.1in
    \caption{Ablation study for the prioritization temperature $T$ across all noise levels in Kitchen and Maze2D}
    \label{fig:appendix-temp}
\end{figure}

\begin{figure}[!ht]
    \centerline{\includegraphics[width=0.9\columnwidth]{images/E-3_klcoeff.pdf}}
    \vskip -0.1in
    \caption{Ablation study for the KLD coefficient $\lambda^\text{kld}_\text{exp}$ across all noise levels in Kitchen and Maze2D}
    \label{fig:appendix-kld-coeff}
\end{figure}

\section{Limitations}

The PRISM algorithm introduces a robust skill-based meta-RL method through a novel skill refinement approach. By incorporating skill exploration, it discovers useful behaviors around demonstrations, and through maximum return relabeling, it enables prioritized skill refinement that is resilient to demonstration quality. However, there are some potential areas for improvement. First, simultaneously training both low- and high-level policies in PRISM can increase computational costs and memory usage. Nevertheless, as mentioned, the iteration count is calculated separately for exploration and high-level policy updates, keeping the computational load per iteration comparable. Additionally, memory issues can be mitigated by adjusting buffer sizes. Importantly, PRISM demonstrates superior performance even when baseline algorithms are trained for extended periods, indicating that the benefits of PRISM outweigh its drawbacks. Another limitation is that fine-tuning is required during the meta-test phase. However, this is an inherent aspect of skill-based learning. Future work could focus on enabling zero-shot skill adaptation to further enhance PRISM's utility and applicability. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}
