\section{Related Works}
\label{sec:related}
\textbf{Skill-based Reinforcement Learning: }Skill-based RL has gained traction for tackling complex tasks by leveraging temporally extended actions. Researchers have proposed information-theoretic approaches to discover diverse and predictable skills \cite{gregor2016variational, eysenbachdiversity, achiam2018variational, sharmadynamics}, with recent work improving skill quality through additional constraints and objectives \cite{strouselearning, park2022lipschitz, park2023controllability, hudisentangled}. In offline scenarios, approaches focus on learning transferable behavior priors and hierarchical skills from demonstration data \cite{pertsch2021accelerating, pertsch2022guided, xu2022aspire, shi2023skill}. Building upon these foundations, various skill-based meta-RL approaches have been developed, from hierarchical and embedding-based methods \cite{nam2022simpl, chien2023variational, cho2024hierarchical} to task decomposition strategies \cite{yoo2022skills, he2024decoupling} and unsupervised learning frameworks \cite{gupta2018unsupervised, jabri2019unsupervised, shin2024semtra}.


\textbf{Hierarchical Frameworks: }Hierarchical approaches in RL have been pivotal for solving long-horizon tasks, where various methods have been proposed including goal-conditioned learning \cite{levy2019learning, li2019hierarchical, gehring2021hierarchical}, and option-based frameworks \cite{bacon2017option, riemer2018learning, barreto2019option, araki2021logical}. Recent advances in goal-conditioned RL have focused on improving sample efficiency \cite{robert2024sample}, offline learning \cite{park2024hiql}, and robust state representations \cite{yin2024representation}. The integration of hierarchical frameworks with meta-RL has shown significant potential for rapid task adaptation and complexity handling \cite{frans2018meta, fu2020mghrl, fu2023meta}. Recent work has demonstrated that hierarchical architectures in meta-RL can provide theoretical guarantees for learning optimal policies \cite{chua2023provable} and achieve efficient learning through transformer-based architectures \cite{shala2024hierarchical}.


\textbf{Relabeling Techniques for Meta-RL: }Recent developments in meta-RL have introduced various relabeling techniques to enhance sample efficiency and task generalization \cite{pong2022offline, jiang2023doubly}. Goal relabeling approaches have extended hindsight experience replay to meta-learning contexts \cite{packer2021hindsight, wanhindsight}, enabling agents to learn from failed attempts. For reward relabeling, model-based approaches have been proposed to relabel experiences across different tasks \cite{mendonca2020meta}, improving adaptation to out-of-distribution scenarios. Beyond these categories, some methods have introduced innovative relabeling strategies using contrastive learning \cite{yuan2022robust, zhou2024generalizable} and metric-based approaches \cite{li2020multi} to create robust task representations in offline settings.