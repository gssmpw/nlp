\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{booktabs} 
\usepackage{subfigure}
\usepackage{amsmath}
\usepackage{tabularx} 
\usepackage[margin=1in]{geometry} 
\setlength{\parindent}{2em}
\title{Dynamic Classification: Leveraging Self-Supervised Classification to Enhance Prediction Performance}
\author{ZHONG Ziyuan, ZHOU Junyang}

\author{
	ZHONG Ziyuan, ZHOU Junyang \\
	Shenzhen BAK Battery Co., Ltd. \\
	Intelligent Technology Department \\
	ShenZhen, Guangdong Province, China \\
	\texttt{zzy13534959496@163.com} \\
}


\begin{document}
	
	\maketitle
	\newpage
	\begin{abstract}

In this paper, we propose an innovative dynamic classification algorithm designed to achieve the objective of zero missed detections and minimal false positives. The algorithm partitions the data into \(N\) equivalent training subsets and \(N\) prediction subsets using a supervised model, followed by independent predictions from \(N\) separate predictive models. This enables each predictive model to operate within a smaller data range, thereby improving overall accuracy. Additionally, the algorithm leverages data generated through supervised learning to further refine prediction results, filtering out predictions that do not meet accuracy requirements without the need to introduce additional models. Experimental results demonstrate that, when data partitioning errors are minimal, the dynamic classification algorithm achieves exceptional performance with zero missed detections and minimal false positives, significantly outperforming existing model ensembles. Even in cases where classification errors are larger, the algorithm remains comparable to state-of-the-art models.  

The key innovations of this study include self-supervised classification learning, the use of small-range subset predictions, and the direct rejection of substandard predictions. While the current algorithm still has room for improvement in terms of automatic parameter tuning and classification model efficiency, it has demonstrated outstanding performance across multiple datasets. Future research will focus on optimizing the classification component to further enhance the algorithm’s robustness and adaptability.


	\end{abstract}
		
\noindent
\textbf{Key words:} Dynamic Classification Algorithm, Prediction Accuracy Improvement, Self-supervised learning, Data Distribution, Subset Prediction, Industrial forecasts, Result Filtering

	\newpage
	\tableofcontents
	\newpage
	\section{Background and Introduction}
The rapid increase in global energy demand has not only exacerbated environmental pollution but also intensified climate change and deepened the energy crisis. As a result, the development of renewable energy sources has become a crucial direction for achieving global sustainable development \cite{miao2020}. Currently, renewable energy sources such as solar, wind, and biomass are gradually becoming key alternatives to fossil fuels. However, as of 2017, fossil energy still accounted for 73.5\% of the global electricity supply, while renewable energy made up only 26.5\% \cite{qazi2019}. Promoting the research, development, and application of new energy technologies has been recognized as a core strategy to address future energy shortages and environmental challenges.

Energy storage technology, especially battery technology, plays a critical role in new energy systems. It not only promotes the efficient use of renewable energy but also plays a vital role in transforming energy structures \cite{miao2020}. Lithium-ion batteries, liquid current batteries, sodium-sulfur batteries, and other energy storage technologies have been rapidly developed in recent years, showing significant improvements in energy density, safety, and cost. However, there remains a gap in realizing the goal of "high safety, low cost, long lifespan, and environmental friendliness" \cite{johansson2024}. Therefore, future technology development in energy storage is expected to feature the coexistence of various technological approaches.

In recent years, with the in-depth study of battery materials, new battery technologies have emerged. Research has shown that optimizing material composition and battery design can significantly enhance battery performance, such as increasing energy density and cycle life \cite{miao2020}. Furthermore, the introduction of the circular economy concept has provided new directions for the sustainable development of batteries, such as efficient material recycling and reuse \cite{johansson2024}. Nonetheless, battery technology still faces many challenges in achieving the goals of a fully sustainable lifecycle and environmental protection. Future research should focus on discovering new materials and the synergistic application of multiple technological approaches \cite{miao2020, johansson2024}.

The application of artificial intelligence (AI) in the field of energy storage shows great potential. Using machine learning and big data technologies, AI accelerates the discovery, performance prediction, and optimal design of new materials. For instance, the materials genome program developed using AI has significantly improved the efficiency of battery materials development \cite{luo2020}. Additionally, AI has demonstrated strong capabilities in battery condition monitoring, fault diagnosis, and performance prediction, providing robust support for advancing battery technology \cite{luo2020}. In the future, AI is expected to further drive innovation and widespread adoption of battery technologies, injecting new vitality into energy storage development \cite{luo2020}.

The rapid advancement of AI technology offers a new opportunity for battery technology innovation. In recent years, machine learning algorithms have been widely applied in battery performance prediction and state-of-health (SOH) estimation, primarily focusing on non-invasive methods to obtain microstate information inside batteries \cite{jones2022}. For example, using data-driven machine learning models, charging and discharging curve characteristics can be analyzed to predict the capacity degradation and remaining lifetime of a battery \cite{durmus2024}. Moreover, AI has accelerated the development of novel energy storage materials through high-throughput computing and material genome programs, significantly improving research efficiency and material screening accuracy \cite{luo2020}.

Regarding AI's application in lithium-ion batteries, current research mainly focuses on model-based and data-based approaches \cite{durmus2024}. Model-based methods analyze the chemical reactions and degradation mechanisms inside the battery. Though theoretically reflective of battery operations, these models often suffer from large errors due to complex internal chemical reactions and environmental factors \cite{jones2022}. Data-based methods, such as convolutional neural networks (CNN), recurrent neural networks (RNN), and support vector machines (SVM), have attracted attention for their flexibility and accuracy in nonlinear problems \cite{durmus2024}. For instance, CNN and RNN models optimized by genetic algorithms (GA) have significantly improved battery capacity prediction, achieving a root mean square error (RMSE) of only 0.1176\%, representing a considerable advancement over previous models.

Despite these advances, challenges remain. The predictive performance of data-driven approaches relies heavily on high-quality data, which is costly and limited in quantity\cite{jones2022}. In addition, due to the different internal structures and operating conditions of different batteries, the generalization ability of the models is weak, making it difficult to adapt to complex and changing real-world situations\cite{miao2020}. Meanwhile, traditional methods usually rely on trial and error in parameter setting, a time-consuming and labor-intensive process that limits the practical application of these models\cite{durmus2024}. Finally, there is a need in industry for a reliable method of picking out substandard products, not just a need to make predictions about a particular batch. In other words, there is an industrial need for 0 misses and fewer false positives. The current practice is basically to overlay the prediction with a classification model to pick out the substandard products, an approach that is difficult to strike a balance between misses and false kills, and is also unreliable.

To address these challenges, this paper proposes a dynamic classification algorithm (DCA). The algorithm automatically divides the data into subsets by self-supervised learning and enables each subset to predict a small range of data based on the distributional features of the training set. Finally, the information from previous self-supervised learning is skillfully used to filter the predictions, which in turn reliably achieves the conditions of 0 misses and low false positives, thus eliminating the need for additional models. This approach significantly improves performance when the classification error is small, and rivals current state-of-the-art models when the classification error is large. Dynamic classification algorithms are highly adaptable and are expected to be widely used in various fields and industries.
	
	
	\section{Related Work}
	
Numerous studies have explored methods to improve model prediction accuracy, with a focus on optimization algorithms, machine learning, and hybrid approaches. This section reviews key advancements in the field, highlighting their methodologies, strengths, and limitations, and provides a comparative analysis with the proposed Dynamic Classification Algorithm (DCA).

Azevedo et al. \cite{azevedo2023} conducted a systematic review of hybrid optimization and machine learning methods for clustering and classification. Their approach integrated various techniques to overcome the limitations of single methods. However, their framework required significant computational resources and faced challenges in adapting to rapidly evolving data environments. Kotary et al. \cite{kotary2023} proposed a joint prediction and optimization learning framework that directly learns optimal solutions from observable features. Despite its theoretical innovation, the approach demands end-to-end training for optimization problems, which can be computationally impractical, especially for nonconvex or discrete scenarios.

Khan et al. \cite{khan2023} combined neural networks with traditional filters (e.g., Kalman and $\alpha$-$\beta$ filters) to enhance the prediction accuracy of dynamic systems under noisy conditions. While this method demonstrated significant improvements in dealing with uncertainty, it relied on complex parameter tuning, limiting its applicability in resource-constrained environments. Ippolito et al. \cite{ippolito2023} combined supervised and unsupervised learning to improve stratigraphic classification accuracy, offering probability distributions rather than discrete classifications. However, their method required extensive data preprocessing and feature engineering, which increased computational costs.

Son et al. \cite{son2023} applied both supervised and unsupervised learning to predict social media user engagement in the airline industry. Although innovative in its use of sentiment analysis for classification, the model required intensive data cleaning and customization for different business contexts. Shah and Satyanarayana \cite{shah2023} introduced a predictive range corrector model to enhance reliability by accounting for deviations between observed and predicted outputs. While this approach improved prediction accuracy, its reliance on control sequence optimization made it less scalable for broader applications.

Dalal et al. \cite{dalal2023} developed a supply chain optimization model using Convolutional Neural Networks (CNNs) and Bidirectional Long Short-Term Memory Networks (BiLSTMs). Their method improved sustainability by leveraging CNNs for resource allocation and BiLSTMs for temporal demand forecasting. Similarly, Guillén et al. \cite{guillen2023} introduced the LSB-MAFS technique, combining Least Squares Boosting (LSB) and Multivariate Adaptive Regression Spline (MARS) to enhance production frontier forecasting. While effective, the method primarily addressed specific regression tasks, limiting its adaptability.

Currently, data prediction results are often filtered through additional classification models. However, these models typically struggle to establish a reliable balance between missed detections and false positives in practical applications. This approach not only increases computational overhead but also risks over-filtering or omitting crucial data, thereby compromising the overall reliability of the prediction system. In contrast, the proposed Dynamic Classification Algorithm (DCA) employs self-supervised learning to automatically partition data subsets and, by predicting within small classification errors, directly filters results based on the distributional characteristics of the training set—without the need for additional models. The dynamic classification method ensures that the range for each subset is accurately defined, which allows the algorithm to achieve zero missed detections while effectively minimizing false positives, thus improving overall prediction accuracy. Furthermore, experimental results show that when the classification errors are minimal in the self-supervised subset partitioning phase, DCA significantly outperforms existing methods in terms of prediction stability and accuracy, and it demonstrates strong adaptability across various datasets and application scenarios.



	\section{Abbreviations and Keywords List}
		\begin{table}[htbp]
		\centering
		\caption{Abbreviations and Keywords List}
		\label{tab:Abbreviations}
		\scalebox{1}{
		\begin{tabular}{@{}>{\raggedright}m{3cm} m{13cm}@{}}
			\toprule
			\textbf{Abbreviation} & \textbf{Definition} \\ 
			\midrule
			DC Error & Error rates during \textbf{Dynamic Classification Process} inside DC algorithm \\ 
			DC-E & Dynamic classification algorithm with result excluded \\ 
			Excluded Rate & Percentage of excluded data in the total volume \\ 
			$N$ & Number of classifications \\ 
			$Train_p$ & Data other than $Train_t$ in the training set \\ 
			$Train_t$ & Training-training set, the dataset obtained by re-sampling from the training set \\ 
			\hline
			$BP$ & Back Propagation Neural Network \\ 
			$DC$ & Dynamic classification algorithm \\ 
			$GC$ & Gaussian classification \\ 
			$GMM$ & Gaussian Mixture Model \\ 
			$KC$ & Kmeans classification \\ 
			$LGBM$ & Light Gradient Boosting Machine Model \\ 
			$N$ & Number of classifications \\ 
			$RF$ & Random Forest Model \\ 
			$SVM$ & Support Vector Machine Model \\ 
			$XGBoost$ & eXtreme Gradient Boosting \\ 
			\bottomrule
		\end{tabular}
		}
	\end{table}





\section{Algorithm Structure Overview}
The Dynamic Classification Algorithm (DCA) is designed to enhance prediction accuracy by partitioning the data into smaller, well-defined subsets. The algorithm consists of four core components, each contributing significantly to the overall effectiveness:

\begin{itemize}
	\item \textbf{Routine Operations} – Standard preprocessing tasks such as data cleaning, normalization, and feature selection, which are essential in machine learning workflows.
	\item \textbf{Dynamic Classification Process} – The core innovation of DCA, where self-supervised learning iteratively refines data segmentation, improving classification boundaries.
	\item \textbf{Redundant Training and Prediction} – A mechanism that enhances stability by incorporating data from neighboring subsets, improving prediction accuracy.
	\item \textbf{Excluding Predicted Results} – A filtering strategy that removes predictions failing to meet accuracy requirements, ensuring retained results achieve a balanced zero missed detections and minimal false positives.
\end{itemize}

This section provides a detailed breakdown of the algorithm’s structure, classification principles, redundant training mechanism, and the filtering of predicted results. The overall structure is illustrated in \textbf{Fig.\ref{fig:figure1}}, which divides the framework into four main components.

The **routine processing module (white section)** consists of fundamental preprocessing tasks such as data cleaning, normalization, and feature selection. These steps ensure data quality before classification and prediction.

The **dynamic classification module (red section)** is at the core of the DCA. The dataset is split into $Train_t$ and $Train_p$. Through iterative optimization, the algorithm adjusts classification boundaries based on the numerical distribution of training data, ensuring that each subset operates within a smaller, controlled data range instead of relying on broad, generalized classification.

The **redundant training and prediction module (blue section)** further optimizes classification by incorporating data from adjacent classification intervals. This module enhances model robustness by supplementing predictions with contextual data, reducing boundary inconsistencies.

The **excluding predicted results module (yellow section)** utilizes supervised learning insights to filter out unreliable predictions. By leveraging pre-learned segmentation boundaries, this module ensures that only high-confidence predictions are retained, effectively achieving a well-balanced zero missed detections and minimal false positives.

These four components collectively optimize classification, improve prediction accuracy, and enhance model adaptability across various datasets.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.95\textwidth]{Figure/Figure1.png}
	\caption{Structure of Dynamic Classification Algorithm}
	\label{fig:figure1}
\end{figure}
	
	\subsection{Principle of Dynamic Classification Processes}
	The **Dynamic Classification Process** is a fundamental component of the Dynamic Classification Algorithm (DCA). In traditional classification models, the learning process typically begins by extracting feature representations, followed by clustering or classification. However, this approach can lead to the coexistence of high and low values within the same classification, a phenomenon that diminishes the predictive model's ability to generalize effectively. Specifically, when target values span a wide numerical range, this inconsistency complicates the model’s learning process, as the features may be similar while the predicted values diverge significantly.
	
	To address this issue, the DCA leverages **self-supervised learning** to learn from the distribution of predicted target values. The algorithm dynamically partitions the dataset into smaller, more homogenous subsets, iteratively adjusting classification boundaries until an optimal segmentation is found. This ensures that each subset contains target values within a narrower range, thereby improving prediction accuracy. The **Dynamic Classification Process** consists of six key modules: **Division Module**, **Classification Model Module**, **Convergence Judgment Module**, **Penalty Module**, **Correction Module**, and **Evaluation Module**, as illustrated in \textbf{Fig.\ref{fig:figure2}}.
	
	\begin{figure}[htbp]
		\centering
		\includegraphics[width=0.9\textwidth]{Figure/Figure2.png}
		\caption{Structure of the Dynamic Classification Algorithm}
		\label{fig:figure2}
	\end{figure}

	During the initialization phase, the training set is divided into $Train_t$ and $Train_p$ using a **1:1 ratio**, ensuring a balanced training and validation setup.
	
	\textbf{Division Module}: The first step involves manually defining the number of planned classifications, **N**. The initial classifier applies a **Gaussian kernel function** to measure the distribution of values in the target column of $Train_t$, and the data is then divided based on the degree of fluctuation in the sorted values. Larger fluctuations indicate finer divisions, ensuring more precise segmentation (see \textbf{Fig.\ref{fig:figure3}}). This results in **N intervals**, defined by **N-1 cutoff points**, forming the initial segmentation list. This list is subsequently used in the Classification Model Module.
	
	However, in cases where a specific accuracy requirement must be met, manual intervention can yield better results. For example, if only predictions achieving \textbf{98\% or higher accuracy} are considered valid, the predefined division ranges should align with this requirement. Considering a confidence interval of \textbf{95\%}, the division module can be manually adjusted so that the preset range aligns with \textbf{$2\% \times 95\%$}. This fine-tuning process enhances the precision of the classification boundaries, which in turn optimizes the performance of the \textbf{Excluding Module}.
	
	\begin{figure}[htbp]
		\centering
		\subfigure[N = 5]{\label{fig:figure3_1}\includegraphics[width=0.325\textwidth]{Figure/Figure3_1.png}}
		\subfigure[N = 9]{\label{fig:figure3_2}\includegraphics[width=0.325\textwidth]{Figure/Figure3_2.png}}
		\subfigure[N = 13]{\label{fig:figure3_3}\includegraphics[width=0.325\textwidth]{Figure/Figure3_3.png}}
		\caption{Classifications $N$ of N = 5, 9, 13}
		\label{fig:figure3}
	\end{figure}

	\textbf{Classification Model Module}: This module trains a set of classification models using the $Train_t$ and $Train_p$ data. A variety of models—such as **Decision Trees**, **Random Forests**, and **LGBM** are employed, utilizing multi-threading to optimize performance. Each model is trained independently, with its performance evaluated through a confusion matrix on  $Train_p$. The best-performing model is selected for the next iteration based on its ability to minimize classification errors.
	
	\textbf{Convergence Judgment Module}: The confusion matrix output from the Classification Model Module is converted into percentages, with off-diagonal values representing the classification loss. The total loss for each iteration is recorded in **LossList**, and the model with the lowest loss (**BestLoss**) is identified as the **BestModel**. If the loss values do not change over **15 iterations**, or if the last **10 iterations** show significant instability, a warning is issued to prevent convergence to a local minimum.
	
	\textbf{Evaluation Module}: To evaluate the model's overall performance, a score is computed using \textbf{Equation~\ref{eq:score}}, which balances the best-case loss with the regular performance over the last 10 iterations. This scoring method ensures that models with occasional excellent performance but inconsistent overall results are not selected.
	
	\[
	\text{Score} = 0.5 \times \text{BestLoss} + 0.5 \times \frac{\sum_{i=n-10}^{n} a_i}{10}, \quad a_i \in \text{LossList}
	\label{eq:score}
	\]
	
	Once all iterations are complete, the $OptimalClassificationModel$ and $OptimalSegmentationList$ are finalized.
	
	In the event of non-convergence, the **Penalty Module** applies penalties based on two core factors: 
	\begin{itemize}
		\item \textbf{Degree of Misclassification}: The greater the misclassification degree (i.e., how far the misclassified sample is from its correct classification), the higher the penalty. This is calculated by measuring the distance between the misclassified value and its correct interval, with larger deviations resulting in greater penalties.
		\item \textbf{Number of Misclassified Samples}: The more misclassified samples in a given class, the stronger the penalty. This is determined by counting the number of samples that do not fit within the target classification interval.
	\end{itemize}
	
	The penalty is calculated based on the \textbf{misclassification degree}. Larger deviations from the correct classification result in larger penalties. The penalty matrix is updated based on the following formula:
	
	\[
	Y = \left(\frac{X}{N}\right)^2 + 1
	\]
	
	where \( X \) represents the misclassification degree, and \( N \) is the number of classification intervals. This formula ensures that misclassifications with large deviations incur greater penalties.
	
	After calculating the initial penalty, a \textbf{secondary penalty} is applied based on the \textbf{number of misclassified samples}. If a higher number of samples is misclassified in a particular class, the penalty increases. This penalty is calculated using the following formula:
	
	\[
	Y = X^2
	\]
	
	This further increases the penalty for large misclassification errors, ensuring that significant discrepancies are addressed.
	
	Once the penalties are applied, the \textbf{misclassified samples} are reassessed. If the classification error exceeds an acceptable threshold, the model returns to the \textbf{Classification Model Module} for further iteration and refinement. This iterative process ensures that the model continues to improve until the penalty stabilizes at an acceptable level.

	
	\subsection{Principle of Redundant Prediction}
The best model identified in the Evaluation Module will classify both the training and prediction sets. The corresponding subsets within their respective intervals are then obtained, as shown in \textbf{Table \ref{tab:classification}}.
	
	\begin{table}[htbp]
		\centering
		\caption{Classification correspondence table}
		\label{tab:classification}
		\begin{tabular}{cccc}
			\toprule
			Set classification & Training set & Prediction model & Prediction set \\
			\midrule
			1 & Training set 1 & Model 1 & Prediction set 1 \\
			2 & Training set 2 & Model 2 & Prediction set 2 \\
			\multicolumn{1}{c}{\centering\dots} & \multicolumn{1}{c}{\centering\dots} & \multicolumn{1}{c}{\centering\dots} & \multicolumn{1}{c}{\centering\dots} \\
			N & Training set N & Model N & Prediction set N \\
			\bottomrule
		\end{tabular}
	\end{table}
	
In this process, slight perturbations in the classification of the prediction set are inevitable. These may manifest as minor overlaps between prediction intervals, caused by fluctuations in prediction data or minor data entry errors. To resolve this, redundant training is introduced. This technique combines the training data from a target classification interval with data from adjacent intervals. Data from neighboring intervals is selected according to two criteria:

	\begin{itemize}
		\item The data must be sorted by predicted target values, and the data closest to the target classification is selected.
		\item The amount of data selected from neighboring intervals cannot exceed $K/4$, where $K$ is the total number of target training set, $4$ is the empirical value obtained from multiple experiments.
	\end{itemize}

The introduction of redundant training enhances the model's robustness by expanding the training subset’s data range, incorporating relevant data from neighboring intervals. This helps balance the prediction range of the prediction subset, ensuring that the predicted values within each subset fall within the expected target range, as derived from the $Optimal Segmentation List$ in the Dynamic Classification section. Samples falling outside this range are filtered out, based on the required accuracy, leaving only those predictions that meet the accuracy threshold.

\subsection{Excluding Predicted Results}
The boundaries of each subset are initially determined by the OptimalSegmentationList, which is obtained through supervised learning from the training set. However, these boundaries can be manually fine-tuned according to accuracy requirements to achieve stable performance and higher precision. This ensures that predictions falling within the valid range are retained, while those deemed inaccurate are removed.

The exclusion process operates as follows: once the predictions are generated, they are compared against the predefined segmentation boundaries. Predictions that fall outside their corresponding subset’s range are flagged as unreliable and subsequently discarded. This ensures that only the most accurate predictions are retained, enhancing the overall reliability of the classification results.

Critically, the exclusion mechanism is designed to guarantee \textbf{zero missed detections}, meaning that all valid predictions are preserved. Within this constraint, adjustments to the segmentation boundaries focus on reducing false positives as much as possible. By dynamically refining these parameters, the exclusion module contributes to a more precise and robust prediction system, allowing for high accuracy with minimal unnecessary exclusions.


\subsection{Algorithm Advantages and Effectiveness}

In practical applications, prediction-based models often face two main challenges. First, balancing the impact of samples with lower and higher target values is difficult. When the training set is sampled with a normal distribution, the predictions tend to concentrate around the middle of the value range, and using uniform sampling only slightly mitigates this issue. The second challenge arises when trying to identify inaccurate predictions. After predictions are made, it is necessary to categorize and re-assess them against an accuracy threshold, which often leads to either missed detections or over-exclusion of data.

The Dynamic Classification Algorithm (DCA) effectively addresses these issues by partitioning the data into smaller, well-defined subsets. This segmentation reduces the range within which each model operates, allowing it to focus on narrower, more homogeneous data intervals. As a result, the complexity of predictions is minimized, improving the accuracy of the model. Importantly, this approach ensures that each subset operates independently, reducing errors that might arise from overlapping data distributions or perturbations in predictions.

Without dynamic classification, unsupervised models may fail to properly distinguish between features with similar values, leading to inaccurate predictions, particularly when the feature distributions are broad. Moreover, in traditional prediction structures, re-learning and selection of new models after predictions often exacerbate these issues, making them computationally expensive and inefficient.

The Dynamic Classification Algorithm (DCA) effectively reduces the complexity of predictions by limiting each subset's data range. By segmenting the data into quantifiable intervals, DCA can precisely filter prediction results, ensuring that only predictions that meet the required accuracy are retained. This method is particularly significant in industrial applications, as it eliminates the need for additional models or complex retraining steps, simplifying the prediction process. 

Furthermore, DCA ensures that the model achieves the goal of **zero missed detections** while minimizing **false positives** as much as possible. By carefully adjusting the segmentation boundaries, the algorithm allows for stable, high-accuracy predictions with minimal exclusion. This balancing act is crucial in real-world scenarios, where the precision of the classification process is paramount, and the model must maintain both accuracy and robustness under various operational conditions.

In practical terms, this algorithm has demonstrated its effectiveness by achieving high accuracy with minimal exclusion. Through the use of self-supervised learning and optimal segmentation, the DCA reliably filters predictions, achieving zero missed detections while minimizing false positives. This ability to discard substandard predictions directly based on data characteristics ensures that the model remains robust and accurate, even in real-world, dynamic environments.

The Dynamic Classification Algorithm has shown superior performance in various datasets, offering a promising solution for applications that require high precision, such as industrial forecasting. While the algorithm still presents opportunities for improvement, particularly in automatic parameter tuning and classification efficiency, it has already proven to be a valuable tool for improving prediction accuracy in both small and large-scale classification tasks.
	
\section{Data Testing Situation}
\subsection{Non-Open Source Data}

In battery production, consistency is crucial. In production processes involving chemical and sorting stages, cells must undergo discharge testing to verify that their capacity meets the required standards. The test data for this study comes from actual factory production, consisting of three different production batches, and is used to predict cell capacity. The necessary information about these batches has been anonymized and is provided in \textbf{Table \ref{tab:batch_data_info}}.

The data for these three batches follows a normal distribution. Feature extraction primarily relies on characteristics from the middle production stages, such as weight, time, density, and voltage features from the formation and capacity sorting stages. Data preprocessing steps include filtering, temperature compensation, and removing outliers using the interquartile range method. The dynamic classification algorithm can be applied to both uniform and normal distributions, although parameter adjustments may vary. 

\begin{table}[h]
	\centering
	\caption{Non-Open Source Data Information}
	\begin{tabular}{|c|c|c|c|}
		\hline
		Batch/Dataset & 22LA & 27LA & 13LA \\ \hline
		Production Time & May (15 days) & June (26 days) & July (6 days) \\ \hline
		System Information & System 1 & System 2 & System 3 \\ \hline
		Total Data & Approximately 70,000 & Approximately 140,000 & Approximately 85,000 \\ \hline
	\end{tabular}
	\label{tab:batch_data_info}

\end{table}


\subsubsection{Comparison Plan and Indicator Settings}

Due to confidentiality, the specific numerical values can only be presented approximately. The total number of features is approximately 100, of which about 20\% originate from intrinsic battery cell characteristics (such as weight, coating density, etc.), while the remaining 80\% are derived from data generated during the production process, including voltage, temperature, and capacity data. In the data preprocessing stage, outliers were handled using the interquartile range method. Specifically, 17\% of the outliers were filtered from the 22LA dataset, 19\% from the 27LA dataset, and 14\% from the 13LA dataset. The remaining data underwent normalization and standardization.

Since average accuracy alone is not sufficient to effectively differentiate between models, this study selected more detailed indicators to assess model performance. These include the proportion of samples with prediction accuracy between 99\% and 100\% relative to the total, and the proportion of samples with prediction accuracy between 99.5\% and 100\%. The higher the percentage of these two indicators, the greater the reliability and precision of the prediction.

The experiment compared three methods: KMeans model + prediction, GMM model + prediction, and dynamic classification model. All of these models use linear regression as the underlying prediction method, while the direct prediction results (DP) of the linear model are also recorded as comparison data. The training and prediction sets were divided in a 3:7 ratio, using the same features and outlier detection methods for both sets. To ensure reliability, each experiment was verified with two random seeds, generating two sets of random test results.

For subsequent tests, all prediction models will use the same features and parameters, with linear regression serving as the comparison method to further assess performance differences. These detailed indicators allow for a more accurate evaluation of each model's performance.

\subsubsection{Comparison Analysis}
	\textbf{Table \ref{tab:best_prediction_results}} records all test results with the best prediction effect.
\begin{table}[htbp]
	\centering
	\caption{Best Prediction Results for Non-Open Source Data}
	\label{tab:best_prediction_results}
	\scalebox{0.9}{
	\begin{tabular}{cccccccccc}
		\toprule
		\textbf{Dataset} & \textbf{Item} & & \textbf{DP} & & \textbf{GC} & & \textbf{KC} & & \textbf{DC}\\
		\midrule
		\multirow{2}{*}{22LA} & 1\% Error Ratio & & 96.69\% & & 98.40\% & & 97.68\% & &99.56\% \\
		& 0.5\% Error Ratio & & 78.94\% & &85.7\% & &82.89\% & &98.15\% \\
		\hline
		\multirow{2}{*}{27LA} & 1\% Error Ratio & &92.52\% & &94.37\% & &94.03\% & &99.56\% \\
		& 0.5\% Error Ratio & &65.87\% & &70.98\% & &70.58\% & &96.73\% \\
		\hline
		\multirow{2}{*}{13LA} & 1\% Error Ratio & &97.72\% & &98.03\% & &97.79\% & &99.75\% \\
		& 0.5\% Error Ratio & &86.37\% & &87.19\% & &86.29\% & &98.94\%\\
		\bottomrule
	\end{tabular}
	}
\end{table}

After completing these steps, in order to achieve the goal of zero missed detections and minimal false positives, we must exclude predictions that are inaccurate. To evaluate this step, we introduce comparisons with advanced models like XGBoost, Gaussian classification models, and Random Forest. These models, as advanced methods, are more capable of capturing detailed information, which typically results in better performance.

Since introducing a new model requires a certain amount of data for training, let's assume we know that some data is predicted inaccurately, and this data will serve as the training set. This assumption is made to better illustrate the advantages of the dynamic classification algorithm in its mechanism. Assume that the data with prediction accuracy below 99\% is $J$, and we divide $
J$ into 10 parts, supplementing them with randomly selected data that meets the accuracy threshold, ensuring the training set size remains constant at $J$, with consistent features. Each model will use 9 different training sets, where the ratio of inaccurate to accurate data starts at 1:9 and gradually increases until it reaches 9:1. Finally, we will record the missed detection rate, false positive rate, missed detections count, and false positives count. The results from the three models are shown in \textbf{Fig.\ref{fig:figure4}}.


Among the experimental results from these three models, all models achieved a good balance with a 4:6 training-to-test data ratio. The corresponding metrics for false positives and false negatives are depicted in the graph. The bar sections represent the miss rate (blue) and overkill rate (red), while the line sections indicate the absolute number of false positives and false negatives. We can observe that as the proportion of non-compliant data in the training set increases, the miss rate significantly decreases, but the overkill rate gradually increases. This result is the combined effect of changes in both the proportion of the training set and the non-compliant data in the prediction set.

In terms of achieving the goal of 'zero missed detections and minimal false positives,' XGBoost performed the best. When the proportion of non-compliant to compliant data was set at 8:2, XGBoost successfully excluded all non-compliant data, as seen in the low miss rate. However, the false positive rate remained significant, with the number of misclassified samples also notable. These results illustrate the trade-off in balancing the two metrics: even though misclassified instances were minimized, almost half of the remaining data was misidentified. This discrepancy highlights that, in real-world scenarios, it is hard for model to fully achieve the ideal of zero missed detections and minimal false positives, as production data often contain noise and errors that are difficult to discern. The graph visually represents the model's performance under different training-prediction ratios, showing the progression from a high miss rate to an acceptable level as the proportion of non-compliant data decreases.
 
\begin{figure}[h!]
	\centering
	\subfigure[Excluding results of XGBoost]{\label{fig:figure4_1}\includegraphics[width=0.95\textwidth]{Figure/Figure4_1.png}}
	\subfigure[Excluding results of Gaussian SVM]{\label{fig:figure4_2}\includegraphics[width=0.475\textwidth]{Figure/Figure4_2.png}}
	\subfigure[Excluding results of Rnadom Forest]{\label{fig:figure4_3}\includegraphics[width=0.475\textwidth]{Figure/Figure4_3.png}}
	\caption{Excluding Results of XGBoost with Different Training and Prediction R	atios}
	\label{fig:figure4}
\end{figure}

Under the Dynamic Classification Algorithm (DCA) framework, the model can leverage information derived from effective self-supervised learning to filter out predictions with significant errors. After independent prediction, each predicted result should fall within its designated range. If a sample exceeds this range by more than ±5\%, it can be considered inaccurate. If there are specific accuracy requirements, the range can be fine-tuned by adjusting the segmentation values, thus achieving zero missed detections and minimal false positives. This method ensures that the boundaries of each subset are confined within the accuracy limits, ensuring that each prediction meets the desired precision standard.

The range for each prediction is derived from the OptimalSegmentationList, which is initially defined by the initial segmentation values. While the division module automatically generates initial segmentation values based on the distribution of training set samples, manual adjustments of these initial values can provide results that better meet specific accuracy requirements.

In the **13LA dataset**, to directly filter the data with an error of less than 1\%, the predictions are excluded using the exclusion module of the dynamic classification to obtain the final prediction. First, the initial division ratio of dynamic classification is manually set as:
[0.02, 0.03, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.03, 0.02]
Finally, the **LGBT classification model** achieves the best result, and the optimized division ratio, combined with the numerical distribution of the targets in the training set, produces the $Optimal Segmentation List$:[108873.9, 109546.5, 110193.2, 110485.0, 110707.1, 110873.8, 111040.5, 111193.3, 111359.85, 111540.5, 111860.0, 112123.8]
The classification error is only **3.39\%**. The prediction results are collected and matched back to the division list for exclusion. The exclusion is set to discard the first and last categorization intervals entirely, while the 11 intermediate intervals are expanded by **1.0025 times** each. The non-excluded and excluded results are shown in \textbf{Table \ref{tab:Culled Result}}, and the number of samples per interval is shown in \textbf{Table \ref{tab:Culled Information}}. The obtained image results are presented in \textbf{Fig.\ref{fig:figure5}}.

Since the error of the dynamic classification is very small, this method allows us to discard **5\%** of the data and still achieve **99\%** or higher accuracy for all remaining data.



	\begin{table}[htbp]
		\centering
		\caption{Comparison Table of Non-excluded and Excluded Information for 13LA}
		\label{tab:Culled Result}
		\scalebox{0.825}{
			\begin{tabular}{cccc}
				\toprule
				\textbf{Item} & \textbf{Average Accuracy} & \textbf{Percentage of errors within 1\%} & \textbf{Percentage of errors within 0.5\%} \\
				\midrule
				Non-excluded Result & 99.9142\% & 99.9879\% & 99.7495\% \\
				Excluded Result & 99.9148\% (0.0006\%↑) & 100.0000\% (0.0121\%↑)& 99.7941\% (0.0446\%↑) \\
				\bottomrule
			\end{tabular}
		}
	\end{table}


\begin{figure}[htbp]
	\centering
	\subfigure[13LA Non-excluded ]{\label{fig:figure5_1}\includegraphics[width=0.325\textwidth]{Figure/Figure5_1.png}}
	\subfigure[13LA Excluded]{\label{fig:figure5_2}\includegraphics[width=0.325\textwidth]{Figure/Figure5_2.png}}
	\subfigure[13LA Comparison]{\label{fig:figure5_3}\includegraphics[width=0.325\textwidth]{Figure/Figure5_3.png}}
	\caption{Average precision results}
	\label{fig:figure5}
\end{figure}


\begin{table}[htbp]
	\centering
	\caption{Samples Number Table of Non-excluded and Excluded Information for 13LA}
	\label{tab:Culled Information}
	\scalebox{0.65}{
		\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c}
		\toprule
		\textbf{Item} & \multicolumn{13}{c|}{\textbf{Number of samples per interval}} & \textbf{Total number}& \textbf{Proportions}\\
		\midrule
		Non-excluded Result &818&2750&8293&8200&8918&8287&8618&9853&9141&9027&7991&2329&1002&85227&100.0000\%\\
		Excluded Result &0&2708&8170&8023&8677&8077&8383&9585&8934&8875&7863&2288&0&81583&95.7244\%\\
		Excluded Samples Number &818&42&123&177&241&210&235&268&207&152&128&41&1002&3644&4.2756\%\\
		\bottomrule
	\end{tabular}
	}
\end{table}

From the above results, it is evident that the dynamic classification algorithm (DCA) has several advantages:

\begin{itemize}
\item High Prediction Accuracy: DCA significantly improves prediction accuracy by dividing the data into smaller subsets and making predictions within smaller, controlled ranges, reducing prediction errors.
\item Effective Exclusion Mechanism: The exclusion mechanism ensures that predictions outside the expected range are filtered out, achieving zero missed detections and minimal false positives, as demonstrated in the 13LA dataset.
\item No Need for Additional Models: Unlike traditional methods, DCA does not require additional models or retraining steps, reducing computational overhead while maintaining high prediction accuracy.
\item Flexible and Adaptable: DCA's ability to dynamically adjust the classification ranges and fine-tune the model makes it highly adaptable to various datasets and prediction tasks, ensuring robust performance even in complex scenarios.
\end{itemize}

It is important to note that for DCA to achieve such excellent results, classification errors must remain below around 5\%. If classification errors exceed this threshold, unexpected samples may appear in the classification results, distorting the range of classification intervals. This disruption in the classification range would lead to a decline in overall prediction accuracy and make it difficult to achieve the goal of zero missed detections and minimal false positives. Therefore, maintaining classification error below 5\% is crucial to the algorithm’s performance.

	\subsection{Open Source Data}
Open-source datasets are sourced entirely from Kaggle. Four non-sequential datasets, representing various fields, were selected to validate the performance of the proposed model. Comparisons were made with the Random Forest model, SVM (with a Gaussian kernel), Linear Regression model, LGBM model, BP Neural Network model, XGBoost model, and the DC algorithm.Four datasets from different domains were chosen for the regression prediction task:
\begin{enumerate}
	
\item California Housing Prices dataset: A widely used dataset for regression tasks. It contains over 20,000 entries but suffers from a data availability rate of only 85.3\%. This dataset is well-suited for verifying the prediction accuracy of the models.

\item Appliances Energy Prediction: This dataset contains data related to energy usage and environmental conditions within a residential building. It includes measurements of temperature, humidity, and energy consumption of appliances and lights in various rooms, as well as outdoor weather data from a nearby weather station..

\item Estimation of Obesity: This dataset includes data for estimating obesity levels in individuals from Mexico, Peru, and Colombia based on their eating habits and physical condition. It contains both real and synthetically generated data.

\item Student Performance Dataset: This dataset contains information about student performance in Math and Portuguese language courses, including demographic, family, and school-related features.
\end{enumerate}
	
Due to the relatively clean data, these datasets were processed within a uniform framework, employing outlier detection through a quartile-based approach and normalization. The training and prediction sets were divided in a 1:1 ratio. The details of the data, including the effects of classification errors and culling features, are presented in Table \ref{tab:open_source_data_results}, while Fig.\ref{fig:figure8} visualizes the evaluation results.
	
The DC algorithm performs admirably across all four datasets, achieving top-tier results, particularly when the classification error is below 5\%. However, as the DC algorithm relies on a classification method for subclass prediction, its performance is impacted when the feature correlation is weak, leading to higher classification errors. In cases like this, even after some data is excluded, the results only approach the performance of Random Forest and XGBoost. When analyzing the Estimation Obesity dataset, which benefits from stronger feature support, the classification error is very low. This indicates that the samples in each subset are nearly always accurate, allowing the Optimal Segmentation List to effectively support the culling function. This enables data reorganization without requiring additional models, significantly improving prediction results. Conversely, datasets like Appliances Energy Prediction and Student Performance Prediction, which have higher classification errors, lack strong feature support, making it difficult to surpass the performance of Random Forest and XGBoost. Nonetheless, the DC algorithm shows great potential, particularly when classification errors are low, allowing it to filter out low-accuracy data through hyperparameter design and the culling function, enhancing the final prediction results.
	
	\begin{table}[htbp]
		\centering
		\caption{Prediction Results for Open Source Data}
		\label{tab:open_source_data_results}
		\scalebox{0.65}{
		\begin{tabular}{c|ccccccccc|ccc}
			\toprule
			\textbf{Dataset} & \textbf{Item} & \textbf{RF} & \textbf{SVM} & \textbf{Linear} & \textbf{LGBM} & \textbf{BP} & \textbf{XGBoost} & \textbf{DC} & \textbf{DC-E} & \textbf{N} & \textbf{DC Error} &\textbf{Excluded Rate}\\
			\midrule
			\multirow{2}{*}{California Housing Prices} & MSE & 0.0088 & 0.0181 & 0.0219 & 0.0109 & 0.0137 & 0.0087 & 0.0102 & 0.0093 & \multirow{2}{*}{N=3} & \multirow{2}{*}{14.96\%} & \multirow{2}{*}{5.31\%} \\
			& R2 & 0.7756 & 0.5377 & 0.6149 & 0.7210 & 0.6514 & 0.7772 & 0.7411 & 0.7448& & &\\
			\hline
			\multirow{2}{*}{Appliances Energy Prediction} & MSE & 0.0043 & 0.0097 & 0.0092 & 0.0058 & 0.0055 & 0.0046 & 0.0052 & 0.0042 & 
			\multirow{2}{*}{N=4} & \multirow{2}{*}{34.59\%} & \multirow{2}{*}{ 28.11\%} \\
			& R2 & 0.6003 & 0.1103 & 0.1569 & 0.4651 & 0.4894 & 0.5793 & 0.5244 & 0.5771 & & &  \\
			\hline
			\multirow{2}{*}{Estimation Obesity} & MSE & 0.0001 & 0.0036 & 0.0061 & 0.0001 & 0.0001 & 0.0001 & 0.0002 & 0.0001 & 
			\multirow{2}{*}{N=4} & \multirow{2}{*}{1.40\%} & \multirow{2}{*}{16.20\%} \\
			& R2 & 0.9936 & 0.8036 & 0.6610 & 0.9950 & 0.9960 & 0.9932 & 0.9917 & 0.9961 & & &  \\
			\hline
			\multirow{2}{*}{Student Performance Prediction} & MSE & 0.0094 & 0.0205 & 0.0141 & 0.0095 & 0.0253 & 0.0097 & 0.0125 & 0.0114 & 
			\multirow{2}{*}{N=7} & \multirow{2}{*}{45.93\%} & \multirow{2}{*}{27.68\%} \\
			& R2 & 0.8555 & 0.6865 & 0.7842 & 0.8544 & 0.6128 & 0.8508 & 0.8085 & 0.8488 & & & \\
			\bottomrule
		\end{tabular}
	}
	\end{table}

\begin{figure}[htbp]
	\centering
	\subfigure[California Housing Prices]{\label{fig:figure8_1}\includegraphics[width=0.475\textwidth]{Figure/Figure8_1.png}}
	\subfigure[predicting energy usage]{\label{fig:figure8_2}\includegraphics[width=0.475\textwidth]{Figure/Figure8_2.png}}
	\subfigure[Estimation Obesity]{\label{fig:figure8_3}\includegraphics[width=0.475\textwidth]{Figure/Figure8_3.png}}
	\subfigure[predicting student scores]{\label{fig:figure8_4}\includegraphics[width=0.475\textwidth]{Figure/Figure8_4.png}}
	\caption{Prediction results under two different public datasets}
	\label{fig:figure8}
\end{figure}



\newpage

\section{Summary and Next Steps}
This paper introduces a dynamic classification algorithm (DCA) designed to address the limitations of existing prediction models. By leveraging self-supervised learning based on the distribution of sample data in the training set, the algorithm effectively narrows down the range of data within each subset, leading to improved prediction accuracy. The algorithm consists of three key components: a regular part, a classification part, and a prediction part. Each categorized subclass, containing both predicted and training data, allows the algorithm to predict smaller ranges independently, thus enhancing overall prediction results. As a consequence, DCA is highly sensitive to classification errors, and when these errors are minimized, the algorithm demonstrates excellent performance.

In the case of non-public datasets, particularly factory production data, the DCA achieves a classification error rate of less than 1\%, significantly enhancing prediction accuracy. For public datasets, while the classification errors are slightly higher, the performance of DCA remains comparable to other advanced models, including Random Forest and XGBoost. Notably, in the Estimation Obesity dataset, where the classification error rate is only 1.4\%, DCA delivers exceptional results, confirming its ability to utilize the first part of supervised classification to effectively support data filtering. This, in turn, allows the algorithm to outperform existing regression models.

Despite its promising results, the DCA algorithm still requires optimization, particularly in the classification phase. Future research will focus on three primary optimization areas:
\begin{enumerate}
\item Automated Parameter Tuning: Developing an automated system to dynamically adjust the number of classifications and sensitivity based on the data distribution, ensuring more precise partitioning for improved classification performance.
\item Classification Model Dependency: Enhancing the algorithm's robustness by reducing its dependence on the classification model's effectiveness. A stronger classification model will lead to better results, particularly when the feature correlation is weak.
\item Avoiding Local Optima: Introducing mechanisms to prevent the algorithm from falling into local optima, ensuring that the model converges to the most optimal solution.
\end{enumerate}
These optimizations will further improve the robustness and applicability of the DCA algorithm, enabling it to handle a wider range of data characteristics and real-world scenarios, and provide even more reliable and accurate predictions.

	\section{References}
	
	\begin{enumerate}
	\bibitem{miao2020} Miao, P., et al. (2020). Research progress and prospect of battery energy storage technology. Energy Storage Science and Technology, 9(3), 670-679. https://doi.org/10.19799/j.cnki.2095-4239.2020.0059.
	
	\bibitem{johansson2024} Johansson, B., et al. (2024). Challenges and opportunities to advance manufacturing research for sustainable battery life cycles. Frontiers in Manufacturing Technology, 4, 1360076. https://doi.org/10.3389/fmtec.2024.1360076.
	
	\bibitem{qazi2019} Qazi, A., et al. (2019). Towards Sustainable Energy: A Systematic Review of Renewable Energy Sources, Technologies, and Public Opinions. IEEE Access, 7, 63837-63851. https://doi.org/10.1109/ACCESS.2019.2906402.
	
	\bibitem{luo2020} Luo, Z., et al. (2020). A Survey of Artificial Intelligence Techniques Applied in Energy Storage Materials R$\&$D. Frontiers in Energy Research, 8, Article 116. https://doi.org/10.3389/fenrg.2020.00116.
	
	\bibitem{durmus2024} Durmus, F., $\&$ Karagol, S. (2024). Lithium-Ion Battery Capacity Prediction with GA-Optimized CNN, RNN, and BP. Applied Sciences, 14(5662). https://doi.org/10.3390/app14135662.
	
	\bibitem{jones2022} Jones, P. K., et al. (2022). Impedance-based forecasting of lithium-ion battery performance amid uneven usage. Nature Communications, 13(4806). https://doi.org/10.1038/s41467-022-32422-w.
	
	\bibitem{azevedo2023} Azevedo, A., et al. (2023). A systematic literature review on hybrid optimization and machine learning approaches in clustering and classification problems.
	
	\bibitem{kotary2023} Kotary, K., et al. (2023). A joint prediction and optimization learning framework for decision quality improvement.
	
	\bibitem{khan2023} Khan, M., et al. (2023). Enhancing dynamic system prediction accuracy with neural networks and conventional filters.
	
	\bibitem{ippolito2023} Ippolito, L., et al. (2023). Combining supervised and unsupervised learning for stratigraphic classification accuracy.
	
	\bibitem{son2023} Son, D., et al. (2023). Predicting airline user engagement on social media using unsupervised and supervised learning processes.
	
	\bibitem{shah2023} Shah, K., and Satyanarayana, N. (2023). Improved model using prediction horizon based corrector for predictive control.
	
	\bibitem{dalal2023} Dalal, S., et al. (2023). Improving efficiency and sustainability via supply chain optimization through CNNs and BiLSTM.
	
	\bibitem{guillen2023} Guillén, M.D., et al. (2023). Improving the predictive accuracy of production frontier models for efficiency measurement using machine learning: The LSB-MAFS method.
	\end{enumerate}
\end{document}