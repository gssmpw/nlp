\section{Related Work}
Numerous studies have explored methods to improve model prediction accuracy, with a focus on optimization algorithms, machine learning, and hybrid approaches. This section reviews key advancements in the field, highlighting their methodologies, strengths, and limitations, and provides a comparative analysis with the proposed Dynamic Classification Algorithm (DCA).

Azevedo et al. \cite{azevedo2023} conducted a systematic review of hybrid optimization and machine learning methods for clustering and classification. Their approach integrated various techniques to overcome the limitations of single methods. However, their framework required significant computational resources and faced challenges in adapting to rapidly evolving data environments. Kotary et al. \cite{kotary2023} proposed a joint prediction and optimization learning framework that directly learns optimal solutions from observable features. Despite its theoretical innovation, the approach demands end-to-end training for optimization problems, which can be computationally impractical, especially for nonconvex or discrete scenarios.

Khan et al. \cite{khan2023} combined neural networks with traditional filters (e.g., Kalman and $\alpha$-$\beta$ filters) to enhance the prediction accuracy of dynamic systems under noisy conditions. While this method demonstrated significant improvements in dealing with uncertainty, it relied on complex parameter tuning, limiting its applicability in resource-constrained environments. Ippolito et al. \cite{ippolito2023} combined supervised and unsupervised learning to improve stratigraphic classification accuracy, offering probability distributions rather than discrete classifications. However, their method required extensive data preprocessing and feature engineering, which increased computational costs.

Son et al. \cite{son2023} applied both supervised and unsupervised learning to predict social media user engagement in the airline industry. Although innovative in its use of sentiment analysis for classification, the model required intensive data cleaning and customization for different business contexts. Shah and Satyanarayana \cite{shah2023} introduced a predictive range corrector model to enhance reliability by accounting for deviations between observed and predicted outputs. While this approach improved prediction accuracy, its reliance on control sequence optimization made it less scalable for broader applications.

Dalal et al. \cite{dalal2023} developed a supply chain optimization model using Convolutional Neural Networks (CNNs) and Bidirectional Long Short-Term Memory Networks (BiLSTMs). Their method improved sustainability by leveraging CNNs for resource allocation and BiLSTMs for temporal demand forecasting. Similarly, Guillén et al. \cite{guillen2023} introduced the LSB-MAFS technique, combining Least Squares Boosting (LSB) and Multivariate Adaptive Regression Spline (MARS) to enhance production frontier forecasting. While effective, the method primarily addressed specific regression tasks, limiting its adaptability.

Currently, data prediction results are often filtered through additional classification models. However, these models typically struggle to establish a reliable balance between missed detections and false positives in practical applications. This approach not only increases computational overhead but also risks over-filtering or omitting crucial data, thereby compromising the overall reliability of the prediction system. In contrast, the proposed Dynamic Classification Algorithm (DCA) employs self-supervised learning to automatically partition data subsets and, by predicting within small classification errors, directly filters results based on the distributional characteristics of the training set—without the need for additional models. The dynamic classification method ensures that the range for each subset is accurately defined, which allows the algorithm to achieve zero missed detections while effectively minimizing false positives, thus improving overall prediction accuracy. Furthermore, experimental results show that when the classification errors are minimal in the self-supervised subset partitioning phase, DCA significantly outperforms existing methods in terms of prediction stability and accuracy, and it demonstrates strong adaptability across various datasets and application scenarios.