\section{Experimental Results}
\label{sec:results}

\inparagraph{Datasets.} We validate our method on a number of data from different shape categories. We considered registered human shapes from \textbf{FAUST}~\cite{bogo2014faust}, non-isometric four-legged animals from \textbf{SMAL}~\cite{zuffi2017smal}, and partial shapes from \textbf{SHREC16}~\cite{shrec16}. The input shapes are roughly aligned and we train our correspondence block on each one of them individually~\cite{cao2023unsup}. These datasets do not include temporal sequences, so we train on all possible pair combinations. We also evaluate our method on real-world scans, using motion sequences scans of clothed humans from \textbf{4D-DRESS}~\cite{wang20244ddress}, and noisy Kinect acquisitions of human-object interactions from \textbf{BeHave}~\cite{bhatnagar22behave}. For both cases, correspondences are obtained by template shape registration. Notably, the obtained correspondences are rough estimations, often imprecise, and thus do not guarantee continuous bijective maps between shapes, \eg., due to garments, occlusions, or noise in the acquisitions.

%To validate our method and show possible applications, we evaluate our method on several datasets including synthetic datasets: \textbf{Faust}~\cite{bogo2014faust}, \textbf{SMAL}~\cite{zuffi2017smal} and \textbf{SHREC16}~\cite{shrec16}. For these datasets that do not incorporate temporal structure, we train them for all possible combinations. We trained our correspondence block on these datasets to obtain correspondence between each shape. We also try our method on real-world datasets: \textbf{BeHave}~\cite{bhatnagar22behave} and \textbf{4D-DRESS}~\cite{wang20244ddress}. For these datasets, we follow the instructions to fit the SMPL~\cite{smpl} model to each real-world shape with registration. 

% \begin{itemize}
%     \item Behave \checkg
%     \item 4D-DRESS ? \checkg
%     \item Animals (smal, shrec20/16) \checkg
% \end{itemize}
\inparagraph{Baseline methods.} We compare our method against recent NIR-based methods that solve similar problems. LipMLP~\cite{liu2022learning} encourages smoothness in the pair-wise interpolation by relying on Lipschitz constraints; NISE~\cite{Novello2023neural}, similar to us, relies on the level-set equation and uses pre-defined paths to interpolate between neural implicit surfaces. NFGP~\cite{yang2021geometry} relies on a user-defined set of points as handles, together with rotation and translation parameters. We also consider~\cite{anonymous2024implicit} as the most relevant baseline method. Nevertheless, all these methods are tailored for shape pairs. To this end, to evaluate the performance of our method in the context of shape sequences, we compare our method to LIMP~\cite{Cosmo2020}, a mesh-based approach that constructs a latent space and preserves geometric metrics during interpolations.

% \inparagraph{Implementation and Training.} We implement our method using JAX~\cite{jax2018github} and utilize 6 layers MLP with 256 nodes for both implicit and Velocity Nets. We choose latent vector size equals 64.

\inparagraph{Training time.} 
We train all methods on a commodity GeForce GTX TITAN X GPU. Our method needs approximately \emph{8 to 10 minutes per pair}, \ie, for a sequence with 10 pairs, our method takes roughly 1.5 hours.
% \fb{is that the name?}
The work~\cite{anonymous2024implicit} and LipMLP~\cite{liu2022learning} require similar runtimes as our method when training one pair.  NISE~\cite{Novello2023neural} takes around 2 hours for each pair. LIMP~\cite{Cosmo2020} first needs to downsample each mesh to 2,500 vertices and training time takes around 30 minutes per pair. NFGP~\cite{yang2021geometry} requires training separately for each time step and each step
% \fb{rewrote; check that it is correct} 
takes 8 to 10 hours, which makes the training time go up to 40 hours for recovering 5 intermediate shapes.

\inparagraph{Evaluation Protocol.} We compare three main settings. First, we train on a single registered shape pair (S) and test the interpolation quality (\textbf{Pairs S/S}). Second, we consider the case of training on registered shape sequences and test the interpolation quality (\textbf{Seq. S/S}), for which we rely on similar metrics. Finally, we consider training on registered shape pairs but \emph{testing on Real point clouds (R)} (\textbf{Pairs S/R}). As metrics, we consider the standard deviation of surface area ($\text{SA}\sigma = \sqrt{\sum_{t=0}^N (A_t-\bar{A})^2 /N}$, where $A_t$ is the surface area for mesh at time $t$ and $\bar{A}$ is the average surface area over the interpolated meshes), which is expected to be close to $0$ for the isometric cases. When we have access to ground truth for the intermediate frames, we also report the Chamfer Distance (CD) and the Hausdorff Distance (HD) errors of the predictions. For the Pairs S/R setting, we also report the pointwise root-mean-square error (P-RMSE), which indicates the Euclidean distance of deformed mesh vertices to the ground truth mesh vertices. In the case a method is not applicable to a certain setting,
% When it is not possible to evaluate a method on a setting\fb{can we say `In the case a method is not applicable to a certain setting'? that'd be stronger! Or what do we mean here?}
we note that with a cross (\xmark).



% \inparagraph{Metrics.}

% Our method needs approximately \emph{8 to 10 minutes per pair} on a GeForce GTX TITAN X GPU with CUDA, \ie, for a sequence with 10 pairs, our method takes roughly 1.5 hours. Per-pairwise~\cite{anonymous2024implicit} and LipMLP~\cite{liu2022learning} take similar time as our method.  NISE~\cite{Novello2023neural} either uses a pre-defined path for each point or interpolates the SDF value instead when the path is not given. It takes around 2 hours for training one pair. LIMP~\cite{Cosmo2020} first needs to downsample each mesh to 2,500 vertices and training costs $\sim$ 30 minutes per pair. NFGP~\cite{yang2021geometry} needs users to first define a set of points as handle points, together with the necessary rotation and translation parameters. Notably, NFGP only outputs starting and ending shapes, \ie, we have to train for each time step separately to get an interpolation sequence. Each time step needs to train 8 to 10 hours. That means to get 5 intermediate shapes it takes \emph{40 hours}, regardless of time spent on finding rotations, translations, and pre-train SDF net. For more training details please refer to our supplementary material.
\subsection{Isometric Shape Interpolation}
\inparagraph{Quantitative comparison.} We show a quantitative comparison of isometric human shapes from 4D-Dress for the three settings in Table~\ref {tab:comp_4d}. We chose this dataset since the frequency of the scans lets us have ground truth for the intermediate frames, as well as access to real scans. Despite competitors being tailored for the Pairs S/S setting, our method performs on par, with better area preservation. Our method also supports sequences, contrary to the majority of previous methods. LIMP fails to generate intermediate shapes that are faithful
% \fb{bad grammar; i dont understand what we mean here}
to the ground truth. 
We believe that LIMP's poor performance is caused by its strong data demand that is not fulfilled here.
To prove our generalization,  we also show results on the interpolation of SMAL isometric animals in Table~\ref{tab:comp_animal}. The ground-truth evaluation frames are obtained by interpolation of SMAL pose parameters. Our method outperforms the competitors on all the metrics. We report all qualitative results in supplementary materials.

\inparagraph{Large Deformations.} 
% Inspecting our results, we found that 
A major advantage of our method is that we support large deformations. We show an example of this between two FAUST shapes in Figure~\ref{fig:large_def}. Although isometric, their drastic change of limbs constitutes a challenge for purely extrinsic methods, causing evident artifacts. Our method preserves areas an order of magnitude better than mesh-based methods (LIMP).

% In this section, we perform 3 evaluations on the isometric deformation case: 
% \begin{enumerate*}[label=(\roman*), itemjoin=~]
% \item quantitative evaluation and comparison on human dataset \textbf{4D-DRESS}.
% \item qualitative large deformation comparison on ~\textbf{FAUST}.
% \item quantitative evaluation and comparison on non-human dataset \textbf{SMAL}.
% \end{enumerate*}
% \textbf{4D-DRESS} contains high frame rate mesh temporal sequences that record the real movement of a human. We choose every 5th mesh as a keyframe and generate the intermediate meshes. To evaluate on non-human object dataset, we use the parameter from the \textbf{SMAL} dataset to deform one animal category mesh to generate 5 intermediate meshes between two meshes as ground truth to evaluate the results. We compute the Chafer Distance (CD), Hausdorff Distance (HD), and the standard deviation of surface area ($\text{SA}\sigma$) to evaluate the quality of interpolated meshes. \SAstd reports surface area changes, it indicates if the surface is stretched or diminished. We train our method using both single pair input and temporal sequence inputs to compare with previous methods. We show the quantitative evaluation on~\cref{tab:comp_4d} for the human dataset and~\cref{tab:comp_animal} for non-human object datasets. Due to the page limits, the visualization of these two tables we kindly refer readers to our supplementary material. We show the visual result for large deformation case in~\cref{fig:comp_4d}. 


% Because some of the comparison methods only take single-pair as input, we train these methods on the same pair and report the average errors over 5 interpolated meshes.
% In this section, we evaluate our method on \textbf{4D-DRESS} and compare it with the previous methods. The dataset contains high frame rate mesh temporal sequences that record the real movement of a human. We choose every 5th mesh as a keyframe and generate the intermediate meshes. We compute the Chafer Distance (CD), Hausdorff Distance (HD), and the standard deviation of surface area ($\text{SA}\sigma$) to evaluate the quality of interpolated meshes. \SAstd reports surface area changes, it indicates if the surface is stretched or diminished. Because some of the comparison methods only take single-pair as input, we train these methods on the same pair and report the average errors over 5 interpolated meshes. We additionally train our method on the sequence of mesh pairs and report the numbers to indicate our method performance similarly in both single-pair and multi-pair inputs. The quantitative comparison is in~\cref{tab:comp_4d} and due to the space limits the visual results please refer to the supplementary. 
 % the visualization result is shown in~\cref{fig:comp_4d}. 
% To show our method's performance in non-human objects, we evaluate on the \textbf{SMAL} dataset~\cite{zuffi2017smal}. To create the ground truth for quantitative measurement, we use the parameter from the SMAL dataset to deform one animal category mesh to generate 5 intermediate meshes between two meshes as ground truth to evaluate the results. We report the same metrics that were mentioned before in~\cref{tab:comp_animal}. For the visualization of the~\cref{tab:comp_animal}, we refer the readers to our Supplementary.
% Moreover, to evaluate our Velocity Net, we also report the RMSE (Root Mean Square Error) per-vertex Euclidean distance (P-RMSE). 
\input{tex_figure/table_ricc}
\input{tex_figure/table_smal}

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.9\linewidth]{images/faust2.pdf}
	\caption{\textbf{Large Deformations.} 4Ddeform handles large deformations better than previous works, providing one order of magnitude less area distortion, even compared to mesh-based ones (LIMP~\cite{Cosmo2020}). In the top row, we visualize the error in the estimated input correspondence.
	\label{fig:large_def}}
 	\vspace{-0.3cm}
\end{figure}



\subsection{Non-isometry and Partiality}
\inparagraph{Non-isometry.} A significant challenge is modeling interpolation when the shape metric is drastically changing between frames. This significantly hampers the chances of obtaining reliable correspondence and control over the full-shape geometry. In~\cref{fig:comp_animal}, we show an interpolation between a cougar and a cow from SMAL. As can be seen on top of the image, the correspondence error is quite noisy. As a consequence, our method is the only one that shows consistency also in the thin geometry (e.g., legs). We argue this is a direct contribution of our losses Eq.~\eqref{eq:distortion}  and Eq.~\eqref{eq:stretching_loss}. Due to the lack of ground truth intermediate shapes, we only report \SAstd for each method's results.  

\inparagraph{Partiality.} Finally, an extremely challenging case are partially observed shapes.
% , \ie, when in one of the frames, a consistent portion of the shape is missing, and it is not possible to predict its position. 
Here, an ideal interpolation would provide a smooth interpolation of the overlapping part, while keeping the non-overlapping part as consistent and rigid as possible. We provide an example of a cat from SHREC16 in~\cref{fig:partial_shrec}. Despite the highly imprecise correspondence, we see that our method is the one with the best preservation of the absent area and, overall, the smallest area distortion. For both non-isometric and partial shapes, our method provides more realistic deformations than~\cite{anonymous2024implicit}.


% In this section, we show the capability of our method which can interpolate even when the target shape is incomplete or highly non-isometric. We use \textbf{SHREC16} dataset to show incomplete shape deformation and use \textbf{SMAL} dataset to show the non-isometric deformation. The non-isometric visual comparison results are shown in~\cref{fig:comp_animal} and incomplete shape deformation results are shown in~\cref{fig:ablation_shrec} together with our ablation study. Due to the lack of ground truth intermediate shapes, for these two cases, we compute the surface area stand deviation (\SAstd) to show that the interpolated meshes are volume-preserving and do not have over-stretch on the surface. Our method can recover high-quality intermediate shape even in small details, especially compared to~\cite{anonymous2024implicit}. We contribute this to our distortion loss~\eqref{eq:distortion} and~\eqref{eq:stretching}. These two physical constraints are typically helpful when encountering even more challenging cases, as shown in~\cref{fig:ablation_shrec}. NFGP~\cite{yang2021geometry} cannot handle partial shapes and non-isometric shapes. All methods that need to fit the second shape first fail~\cite {Novello2023neural, liu2022learning}, as the second point cloud $\vt{P}_1$ is incomplete. Only~\cite{anonymous2024implicit} and our method can handle both situations. However, for challenging cases, our method provides better realistic deformation than~\cite{anonymous2024implicit}.
\begin{figure}[ht]
	\centering
 \begin{tikzpicture}[spy using outlines={circle, magnification=2, size=0.55cm, connect spies}]
    \node[anchor=south west,inner sep=0] (image) at (-2, 0){
	\includegraphics[width=\linewidth]{images/smal.pdf}};
 \begin{scope}[x={(image.south west)},y={(image.north east)}]
        % \draw[red,thick] (0.5,0.5) rectangle (0.6,0.6); % Adjust coordinates as necessary
        \spy[red] on (2.47, 0.68) in node (image) [left] at (3.65,0.59); % Adjust coordinates as 
        \spy[red] on (2.44, 1.78) in node (image) [left] at (3.70,1.76);
    \end{scope}
\end{tikzpicture}
	\caption{\textbf{Non-isometric deformation.} We deform two different animals from SMAL, relying on a noisy correspondence (top row). Compared to the previous methods, our method results in plausible deformations, while preserving thin geometric details (e.g., legs).}
	\label{fig:comp_animal}
 	\vspace{-0.3cm}
\end{figure}
% \input{tex_figure/table_sa}

\subsection{Ablation Study}
To demonstrate the effectiveness of our distortion loss $\L_{d}$ and stretching loss $\L_{st}$, we perform a quantitative comparison on the 4D-Dress dataset. We report quantitative evaluation in~\cref{tab:ablation_4d}. We highlight that although the losses have a minor influence in the Pair S/S case, they are useful in providing consistency when the network has to capture relations on a wider set of shapes (Seq. S/S); further, they show robustness in the presence of real noise (Pairs S/R). This follows our intuition that such losses serve as regularization, especially in the more challenging cases. This is further highlighted by the qualitative results of partial shapes of~\cref{fig:partial_shrec}.  
% In this section, we show the effects of our distortion loss $\L_{d}$ and stretching loss $\L_{st}$. We show that these two losses are important, especially for challenging cases. Distortion constrains the movement of points in locally bending or distorting in the same way. Stretching loss helps to prevent unwanted stretch of the surface, especially for partial target shapes. We show visual comparison with and without distortion and stretching loss on \textbf{SHREC16} and also report the \SAstd ($\times 10)$ numbers (see~\cref{fig:ablation_shrec}). Visually in~\cref{fig:ablation_shrec}, the method creates artifacts around the tail of the cat without distortion loss. The cat's front legs are over-stretched without stretching loss.
\input{tex_figure/table_ablation}
\begin{figure}[t]
	\centering
 \begin{tikzpicture}
  \node [inner sep=0pt] (image) at (0,0) {
	\includegraphics[width=.9\linewidth]{images/shrec16.pdf}};
    \draw[red, thick] (3.2, -0.2) circle [radius=0.35cm];
    \draw[red, thick] (3.2, -1.45) circle [radius=0.35cm];
    \draw[red, thick] (2.63, -2.4) circle [radius=0.25cm];
    \draw[red, thick] (3.2, -3.9) circle [radius=0.3cm];
    \end{tikzpicture}
	\caption{\textbf{Partial shape deformation.} We consider the case in which one of the input shapes is only partially available while having noisy correspondences (correspondence error visualized in the top row). Other methods often collapse the unseen part or create unreasonable stretches. Similar effects are observed when we remove some of our novel losses. Our method provides plausible interpolations, both for the visible and missing parts.}
	\label{fig:partial_shrec}
 	\vspace{-0.3cm}
\end{figure}

\begin{figure*}[th]
	\centering
	\includegraphics[width=.85\textwidth]{images/behave3.pdf}
	\caption{\textbf{Upsampling and extrapolation.} The top shows an example of the BeHave sequence. Starting from a sparse set of keyframes (1fps, colored point clouds), our method lets us interpolate the registration (first row), as well as the real Kinect point clouds (second row) between keyframes at an arbitrary continuous resolution. On the bottom, we show extrapolation on a 4D-Dress sequence. With just a few key frames, we can deform the real point cloud even beyond the final frame, obtaining an estimation of the plausible continuation of the action.}
	\label{fig:app_upsample}
 	\vspace{-0.3cm}
\end{figure*}

\subsection{Applications}\label{subsec:application}
Our method enables a series of new applications, such as the upsampling of real captures and the handling of noisy point clouds. We also show that the learned networks are capable of generalizing and extrapolating in the time domain.

\inparagraph{Temporal Upsampling.}
In many real-world datasets, human movements are recorded by sensors such as RGBD cameras, which provide real-world point clouds of humans and possible  object interactions. However, due to technical constraints or device setup differences, human motion datasets have different frame rates for recording the movement~\cite{bhatnagar22behave, wang20244ddress, GRAB}. 
% This complicates their merge and often results in synchronizing all the sequences to the lowest frame rate for consistency. 
This is the case for BeHave~\cite{bhatnagar22behave}, where the input Kinect sequences are carefully annotated with significant manual intervention to align SMPL and an object template to the input, resulting in a frame rate of only 1 FPS. We show that our interpolation method can efficiently upsample not only the annotated SMPL data, but also the noisy real-world Kinect point clouds without additional effort. We highlight that our Velocity Net can easily be generalized to untrained real-world point clouds to obtain upsampling sequences. We show the upsampling results in~\cref{fig:app_upsample}
% We show in~\cref{fig:app_upsample} that our method trained on the frames of the sequence (semantic colored point clouds) can interpolate \emph{both} the ground truth and also the real capturing (RGB colored point cloud), providing dense and high-quality augmentation. 


\inparagraph{Real-World Data Deformation.}  Here, we show another application scenario in high-quality meshes.
% \fb{grammar; i dont understand what we mean here}. 
High-quality data with tens of thousands of vertices, defining a dense, precise point-to-point correspondence is demanding and often unfeasible to be processed. Therefore, it is a common practice to fit a template to such input high-quality data and use it as rough guidance. In~\cref{fig:app_upsample}, we show how, from just a few frames equipped with a rough SMPL alignment,
% \fb{to we mean we fit a SMPL model to a few frames?}, 
we can manipulate a 40k vertices real scan, maintaining its structure along all the sequences. We refer to~\cref{tab:comp_4d} as an evaluation of our method's performance both on the real-world and SMPL interpolation. 

% However, how to combine this template to control the input point cloud still remains a challenge, especially in the case of non-rigid and topological changes. As mentioned before, our method is not restricted only to the point cloud seen at training time but also generalizes to point clouds with a completely different structure. In~\cref{fig:app_4d}, we show how, from just a few frames of an SMPL estimation, we can manipulate a 40k vertices real scan, maintaining its structure along all the sequences. We refer to~\cref{tab:comp_4d} as an evaluation of our method's performance both on the real-world and SMPL interpolation. 

%We report the CD, HD, and SAstd w.r.t. to the real-world mesh as well. Moreover, as each vertex of the SMPL mesh is traceable, we also report the RMSE (Root Mean Square Error) per-vertex Euclidean distance (P-RMSE) on the SMPL mesh (see~\cref{tab:comp_4d} last three columns).



% \begin{figure*}[h]
% 	\centering
% 	\includegraphics[width=\textwidth]{images/dress4d_app.pdf}
% 	\caption{\textbf{Deform real-world meshes using Velocity Net}: The colored point cloud in the upper row is the input training pairs and the color indicates the correspondences. The interpolated and extrapolated point clouds are shown in gray color. The lower row shows we deform the real-world mesh vertices using our Velocity Net. Even though the Velocity Net is never trained on real-world data, it still provides deformation w.r.t to the trained point cloud pair. The extrapolated deformation still gives physically plausible movements on both point clouds and real-world meshes.}
% 	\label{fig:app_4d}
%  	%\vspace{-0.5cm}
% \end{figure*}


% \inparagraph{Temporal Upsampling and Real-World Data Deformation}
% In many real-world datasets, human movements are recorded by sensors such as RGBD cameras, which provide real-world point clouds of humans and possible interactive objects. However, due to technical constraints or device setup differences, human motion datasets have different frame rates for recording the movement~\cite{bhatnagar22behave, wang20244ddress, GRAB}. This complicates their merge and often results in synchronizing all the sequences to the lowest frame rate for consistency. This is the case for BEHAVE~\cite{bhatnagar22behave}, where the input Kinect sequences are carefully annotated with a significant manual intervention to align SMPL and an object template to the input, resulting in just a 1 FPS frame rate.

% Real-world data also presents noise and lacks precise point-to-point correspondences. Therefore, it is a common practice to fit a template to the data such that one can get clean and simpler guidance. This strategy is also used in another situation where real-world datasets such as 4D-Dress contain high-quality data with tens of thousands of vertices. Therefore, defining a dense, precise point-to-point correspondence is demanding and often simply unfeasible. In both cases, either the real-world data upsampling or high-quality meshes manipulating, how to combine this template to control the input point cloud remains a challenge, especially in the case of non-rigid and topological changes.

% We show how, from just a few frames of an SMPL estimation, our Velocity Net can easily be generalized to untrained real-world point clouds to obtain upsampling sequences, and we treat a large number of vertices as point clouds and by manipulating the point cloud we can also manipulating the high-quality meshes.
% We show in~\cref{fig:app_upsample} that our method trained on the frames of the sequence (semantic colored point clouds) can interpolate \emph{both} the ground truth and also the real capturing (RGB colored point cloud), providing dense and high-quality augmentation. In~\cref{fig:app_upsample}, we show we can manipulate a 40k vertices real scan, maintaining its structure along all the sequences. We refer to~\cref{tab:comp_4d} as an evaluation of our method's performance both on the real-world and SMPL interpolation. 


\inparagraph{Extrapolation for Movement Generation.} Our method lets us obtain dense intermediate frames at arbitrary temporal resolution and allows us to deform the data that is a bit far from the sparse input correspondence. Moreover, the learned physical deformation allows us to generate movements even beyond the considered sequence. Our velocity field can extrapolate outside of the training time domain ($0$ to $1$), while remaining physically plausible, as shown in~\cref{fig:app_upsample}. 
% This lets us synthesize new samples while retaining consistency.

% \inparagraph{Movements Recombination} \RM{preliminary; ignore} Finally, our collection-based learning lets us also explore and control the intermediate results in a way that was not possible with previous optimization-based methods \cite{anonymous2024implicit}. In\cref{fig:behave_gen}, we mix feature combinations from different sequences to generate new ones. 


% We perform the following experiments \begin{enumerate*}[label=(\roman*), itemjoin=~]
% \item We extrapolate our Velocity Net outside of the training time domain to generate new movement sequences. 
% \item We infer our velocity net on untrained real-world point cloud (on \textbf{BeHave}) or meshes vertices (on \textbf{4D-DRESS}) to generate new movement sequences.
% \item We mix feature combinations to generate new sequences.
% \end{enumerate*}

% \begin{figure*}[h]
% 	\centering
% 	\includegraphics[width=\linewidth]{images/behave_gen.pdf}
% 	\caption{\textbf{Generate new movements}: We show that our method can generate new movements using untrained feature combinations. We chose two trained pairs in a sequence, indicated by colored point clouds. The yellow arrow direction is the trained pair and the interpolation direction. The green arrows show that we combine the features of the input pair on both sides of the arrow to generate new movements.}
% 	\label{fig:behave_gen}
%  	%\vspace{-0.5cm}
% \end{figure*}

% \textcolor{red}{
% \section{To Our Dear Reviewer(s)}
% First of all, thank you for your time. 
% We particularly would like to know:
% \begin{itemize}
%     \item Do you think the method description is clear? Like if the pipeline figure, method part is misleading or clear?
%     \item Do you like the applications we presented, do you find it appealing?
%     \item any more suggestions on figures? writing? 
% \end{itemize}
% }

