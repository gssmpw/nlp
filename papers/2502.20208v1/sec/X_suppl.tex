\clearpage
\setcounter{page}{1}
\setcounter{section}{-1}
\maketitlesupplementary

\renewcommand \thesection{S\arabic{section}}
\renewcommand{\thefigure}{S.\arabic{figure}}
\renewcommand{\thetable}{S.\arabic{table}}
\renewcommand{\theequation}{s.\arabic{equation}}


\section{Contents of Supplementary Materials}
The supplementary zip folder contains the following:
\begin{itemize}
    \item A PDF document providing additional details on our method, including mathematical formulations, training specifics, and extended visualizations of our results.
    \item A video folder showcasing animations of our results. 
    \item A link to our project page \url{https://4deform.github.io/}.
\end{itemize}


\section{Losses Derivations}
\label{sec:math}
\inparagraph{Distortion Loss.} If one breaks down the rate of deformation tensor in~\cref{eq:deformation_tensor}, $\vt{D}$ it is the symmetric part of the velocity gradient $\nabla \mathcal{V}$ plus its transpose. It is called the rate of deformation tensor which gives the rate of stretching of elements. Since $\mathcal{V}:\mathbb{R}^3\to\mathbb{R}^3$, $\vt{D}$ is a $3\times3$ matrix, it is also related to stress tensor in continuum mechanics. We adopt the second invariants of the deviatoric stress tensor~\cite{Irgens2008}
\eq{
J_2 &= \frac{1}{3}\tr(\vt{D})^2 - \frac{1}{2}\big(\tr(\vt{D})^2 - \tr(\vt{D}\cdot \vt{D})\big) \\
 &= \frac{1}{6}\tr(\vt{D}\cdot \vt{D}) - \frac{1}{2}\tr(\vt{D})^2 \;.
}
The second invariant equal to zero implies that there is no shape-changing (distortional) component in the deformation or stress. In this case, all principal stresses or strains are equal, leading to a purely hydrostatic (isotropic) stress or strain state~\cite{timoshenko1956strength, dieter1976mechanical}.

\inparagraph{Stretching Loss} In fact, the term is related to the (right) Cauchy strain tensor and also related to distortion loss. As in~\cref{eq:stretching}, the deformation term $\vt{F}^\top\vt{F} := \vt{C}$ is called the Cauchy strain tensor~\cite{kaye1998definition}. The term $\vt{F}^\top\vt{F} - \vt{I} := \vt{E}$ is called Green-Lagrange strain tensor and used to evaluate how much a given displacement differs locally from a rigid body displacement. Write it in gradient tensor, \ie, $\nabla \mathcal{V}$, we have 
\eq{
\vt{E} = \frac{1}{2}\big( (\nabla\mathcal{V})^\top + \nabla \mathcal{V} + (\nabla\mathcal{V})^\top(\nabla\mathcal{V}) \big)\;.
}
Therefore,~\cref{eq:stretching_loss} can be seen as projecting the rigid displacement to the tangent space of point $\vt{x}$. Even from a different perspective, our formulation coincides with the stretching loss in NFGP~\cite{yang2021geometry}. Different is we have an explicit formulation of deformation operator $\vt{F}$.

\inparagraph{Normal Deformation} Even though our method does not require an oriented point cloud as input. If normal information is available from the given point clouds, one could utilize the natural property of implicit representation to add normal deformation constraints.  We follow the projection from our stretching loss, for any vector $\vt{t}_1$ and $\vt{t}_2$ in the tangent space of point $\vt{x}$ with normal $\vt{n}$, then we have $\vt{n}(\vt{x})\cdot \vt{t}_1 = 0 $ and $\vt{n}(\vt{x}) \cdot \vt{t}_2 = 0 $. The deformation transform $\vt{t}_1$ to $\vt{t}_1' = \vt{F}\vt{t}$, and $\vt{t}_2' = \vt{F}\vt{t}_2$ the $\vt{F}$ is the same as in~\cref{eq:stretching}. Therefore, $\vt{t}_1'$ and $\vt{t}_2'$ lie in the tangent space of the deformed surface point $\vt{x}'$, thus, the normal in $\vt{x}'$, denoted as $\vt{n}'$ should be perpendicular to $\vt{t}_1'$ and $\vt{t}_2'$, that is,
\eq{
\vt{n}'\cdot\vt{t}_1' = 0, ~~\vt{n}'\cdot\vt{t}_2' = 0\;.
}
Then we have 
\eq{
\vt{n}'\cdot \vt{F}\vt{t}_1 = 0, ~~\vt{n}'\cdot \vt{F}\vt{t}_2 = 0\;.
}
This implies 
\eq{
\vt{F}^\top\vt{n}' = \lambda\vt{n}\;. 
}
We normalized it and get the Normal Deformation Loss as  
\eq{
\L_n = \int_{\dom} \norm{\vt{n}_t - \frac{\vt{F}^\top \vt{n}_{t+1}}{\norm{\vt{F}^\top \vt{n}_{t+1}}} }_{l^2} \dd \vt{x}\;.
}




% \subsection{Streching loss explain in figure}

% \subsection{Normal deformation}

\section{Training Details}
In this section, we summarize the training efficiency of our method and the comparison methods. We plot the average training time (per pair) in~\cref{fig:training_time}. LipMLP~\cite{liu2022learning} trains the fastest as they do not have discrete time steps during training. Our method trains as fast as~\cite{anonymous2024implicit} per pair. However, our method can directly train on temporal sequences without manually switching training pairs. In addition to that, NFGP~\cite{yang2021geometry} requires more than 75 hours to train a 5-step interpolation and LIMP~\cite{Cosmo2020} trains only on meshes with $2,500$ vertices and takes longer than our methods.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{images/training_time.pdf}
    \caption{\textbf{Training time visualization.} We plot the rough training time with comparison methods to show the efficiency of our methods.}
    \label{fig:training_time}
\end{figure}

\inparagraph{LIMP Training Protocol.} LIMP \cite{Cosmo2020} learns a latent space of meshes and constructs an interpolation constrained under geometric properties. This method supports both isometric and non-isometric deformations. However, the input meshes are required to be in \emph{pointwise correspondence} and \emph{labeled based on stylistic classes}. Additionally, a pre-processing step is needed on the input meshes to reduce the number of vertices to $2500$ and this step is done using iterative edge collapse~\cite{garland1997surface}. The model supports sequence training and training for 20,000 epochs takes about
% 40-50 hours for sequence and 
30-40 minutes for pair training. 

\inparagraph{NISE Training Protocol.} NISE~\cite{Novello2023neural} is a method that learns both isometric and non-isometric deformations between two input meshes. 
% Unlike other approaches~\cite{Cosmo2020, yang2021geometry}, it does not require ground truth labeled pointwise correspondences or user-defined handle points. However, 
It relies on a pair of pre-trained SDF networks to linearly interpolate neural implicit surfaces, which form the foundation for modeling the deformation. In the paper of NISE~\cite{Novello2023neural}, the author mentioned that the method can interpolate along a pre-defined linear path as well. However, this path needs to be defined per point and it can only interpolate linearly according to the Euclidean coordinates of the points. The method can only be trained on mesh pairs, and training each pair, including pre-training the SDF network to fit the input, requires approximately 4 hours for 20,000 epochs. Excluding the pre-training time is approximately 2 hours per pair.

\inparagraph{NFGP Training Protocol.} Training NFGP~\cite{yang2021geometry} requires first training an SDF network that fits the implicit field on the input shapes, which takes about 2 hours for 100 epochs. After that, a set of points is defined per deformation step as handles, along with the necessary rotation and translation parameters to transform these handle points into target points. To be able to use NFGP~\cite{yang2021geometry} as a time-dependent interpolation network that generates $t$ intermediate shapes, one needs to train the network $t$ times and decide how the gradual deformation at each time step should appear. Therefore, the process of defining handle points requires a thorough understanding of how to set rotations and translations to obtain physically plausible interpolation. Moreover, visualization is essential for selecting handles and targets from the vertices of the meshes reconstructed from their SDF network. The training for 500 epochs per deformation time step takes 8 hours. Thus, 50 hours — including the training for the implicit network — are required for deformation with 5 time steps.


\section{Visualizations of Quantitative Evaluated Sequences}
In this section, we show the visual results of~\cref{tab:comp_4d} on \textbf{4D-Dress}~\cite{wang20244ddress} in~\cref{fig:4d_dress}. And the visual results of~\cref{tab:comp_animal} on~\textbf{SMAL}~\cite{zuffi2017smal} dataset in~\cref{fig:lion}, to show the deformation of non-human objects. 
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{images/dress4d_supp.pdf}
    \caption{\textbf{Visual results on human isometric deformation.} We show the visualization of our interpolated meshes on~\textbf{4D-Dress}~\cite{wang20244ddress}. LIMP~\cite{Cosmo2020} can recover reasonable movement, however, it turns the leg in the wrong direction.}
    \label{fig:4d_dress}
\end{figure}


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{images/lion_supp.pdf}
    \caption{\textbf{Visual results on non-human object deformation.} We show the visualization results for an animal data in~\textbf{SMAL}~\cite{zuffi2017smal}. LIMP~\cite{Cosmo2020} can only handle $2,500$ vertices, thus the interpolated mesh is low-quality.}
    \label{fig:lion}
\end{figure}


\section{More Visualization}
We show more visualization results of our method on real-world datasets. We show more sequences from \textbf{BeHave}~\cite{bhatnagar22behave} in~\cref{fig:chairblack} and \cref{fig:backpack}. We also show more visualization of high-resolution real-world mesh interpolation on~\textbf{4D-Dress}~\cite{wang20244ddress} in~\cref{fig:dress4d_supp}.

\begin{figure*}[h]
    \centering
    \includegraphics[width=\linewidth]{images/chairblack.pdf}
    \caption{\textbf{Upsampling on real-world data.} We show examples of the \textbf{BeHave}~\cite{bhatnagar22behave} sequence. Starting from a sparse set of keyframes (1fps, colored point clouds), our method lets us interpolate the registration (first row), as well as the real Kinect point clouds (second row) between keyframes at an arbitrary continuous resolution.}
    \label{fig:chairblack}
\end{figure*}

\begin{figure*}[h]
    \centering
    \includegraphics[width=\linewidth]{images/backpack.pdf}
    \caption{\textbf{Upsampling on real-world data.} We show examples of the \textbf{BeHave}~\cite{bhatnagar22behave} sequence. Starting from a sparse set of keyframes (1fps, colored point clouds), our method lets us interpolate the registration (first row), as well as the real Kinect point clouds (second row) between keyframes at an arbitrary continuous resolution.}
    \label{fig:backpack}
\end{figure*}

\begin{figure*}[h]
    \centering
    \includegraphics[width=\linewidth]{images/dress4d_supp_seq.pdf}
    \caption{\textbf{Deformation on real-world mesh.} We examples of the \textbf{4D-Dress}~\cite{wang20244ddress} sequence. Starting from a sparse set of approximated registration of SMPL model~\cite{smpl}, our method lets us interpolate the real-world, high-resolution meshes (second row, around $40,000$ vertices) between keyframes.}
    \label{fig:dress4d_supp}
\end{figure*}


\section{Upsampling Video}
We use our method to upsample sequences in~\textbf{BeHave}~\cite{bhatnagar22behave} to $30$FPS and render video for it. Please visit our anonymous project page \url{https://4deform.github.io/}. 
% 







% Having the supplementary compiled together with the main paper means that:
% % 
% \begin{itemize}
% \item The supplementary can back-reference sections of the main paper, for example, we can refer to \cref{sec:intro};
% \item The main paper can forward reference sub-sections within the supplementary explicitly (e.g. referring to a particular experiment); 
% \item When submitted to arXiv, the supplementary will already included at the end of the paper.
% \end{itemize}
% % 
% To split the supplementary pages from the main paper, you can use \href{https://support.apple.com/en-ca/guide/preview/prvw11793/mac#:~:text=Delete%20a%20page%20from%20a,or%20choose%20Edit%20%3E%20Delete).}{Preview (on macOS)}, \href{https://www.adobe.com/acrobat/how-to/delete-pages-from-pdf.html#:~:text=Choose%20%E2%80%9CTools%E2%80%9D%20%3E%20%E2%80%9COrganize,or%20pages%20from%20the%20file.}{Adobe Acrobat} (on all OSs), as well as \href{https://superuser.com/questions/517986/is-it-possible-to-delete-some-pages-of-a-pdf-document}{command line tools}.
% \clearpage

% {
%     \small
%     \bibliographystyle{ieeenat_fullname}
%     \bibliography{subbib}
% }

