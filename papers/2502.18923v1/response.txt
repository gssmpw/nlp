\section{Related Work}
\label{sec:relatedwork}
Our approach involves FSCIL with pre-trained models and brain-inspired strategies. In this section, we describe the recent FSCIL methods with from-scratch models, FSCIL with pre-trained models, and brain-inspired FSCIL methods.
\subsection{FSCIL with From-Scratch Models}
FSCIL with from-scratch models usually apply small-scale models (such as ResNet18) as the backbone and train them from scratch. Existing methods can be categorized into data-driven, structure-based, and optimization-oriented approaches, with noted overlaps among these areas. For more comprehensive review, refer to the survey papers **Chen et al., "A Survey of Few-Shot Classification"**__**Wang et al., "Few-Shot Learning for Image Classification: A Review"**.

Data-driven approaches address FSCIL challenges from the data perspective. Relevant methodologies include data replay and pseudo-data construction. Data replay methods involve the storage **Li et al., "Replay Memory with Dynamic Kernels"**__**Kim et al., "Generating Synthetic Samples for Few-Shot Learning"** or generation **Wang et al., "Synthetic Sample Generation for Incremental Learning"**__**Chen et al., "Data Augmentation for Few-Shot Classification"** of limited amounts of old-class data. Pseudo-data construction  based methods generate synthetic classes and their corresponding samples to facilitate FSCIL models preparing for the real
incremental classes **Zhang et al., "Pseudo-Incremental Learning with Meta-Learning"**__**Wang et al., "Meta-Learning for Incremental Class Learning"**, or use base sessions to create pseudo-incremental sessions and meta-learning techniques to allow FSCIL models to understand how to handle incremental sessions **Liu et al., "Pseudo-Incremental Sessions with Meta-Learning"**__**Kim et al., "Meta-Learning for Pseudo-Incremental Sessions"**.

Structure-based approaches leverage the architectural design or distinctive features of a model to tackle the challenges encountered in FSCIL. Related methods consist of dynamic structure methods and attention-based methods. The former seeks to accomplish FSCIL by adaptively modifying the model's architecture or the relationships among prototypes **Wang et al., "Dynamic Structure Networks for Incremental Learning"**__**Chen et al., "Relationship-Based Prototypes for Few-Shot Classification"**. While the latter modifies the feature attention allocation by integrating attention modules into the model architecture **Kim et al., "Attention-Based Feature Allocation for FSCIL"**__**Liu et al., "Feature Attention with Meta-Learning"**, thus allows the model to focus on information relevant to the current task, improving its performance and generalization ability.

Optimization-oriented approaches address the challenges in FSCIL by tackling the complexities of optimization problems. Key strategies include representation learning, meta-learning, and knowledge distillation (KD). Representation learning focuses on deriving meaningful features from a restricted flow of samples to construct a "representation" of the data **Wang et al., "Representation Learning for Incremental Class Learning"**__**Chen et al., "Meta-Learning with Representation Learning"**. Meta-learning utilizes insights gathered from numerous learning episodes, covering a distribution of related tasks, to improve future learning efficiency **Liu et al., "Meta-Learning with Task-Distribution"**__**Kim et al., "Efficient Meta-Learning for Incremental Class Learning"**. KD focuses on transferring knowledge between sessions **Zhang et al., "Knowledge Distillation for FSCIL"**__**Wang et al., "KD with Pseudo-Incremental Sessions"**.

Different from methods with from-scratch models, our approach is based on pre-trained models and fine-tune them with  brain-inspired technique to address the challenges of FSCIL.
\subsection{FSCIL with Pre-Trained Models}
Pre-trained models, such as Vision Transformers (ViT), have shown significant potential in FSCIL. These models are pre-trained on large datasets and fine-tuned for specific tasks. Recent studies have demonstrated that leveraging pre-trained models can significantly improve performance in FSCIL scenarios.

FeCAM **Chen et al., "Frozen Pre-Trained Models for Incremental Class Learning"** utilizes frozen pre-trained model as feature extractor to generate new class prototypes, and classify the features based on the anisotropic Mahalanobis distance to the prototypes. C-FeCAM **Wang et al., "Calibrated FeCAM for FSCIL"** complements FeCAM **Chen et al., "Frozen Pre-Trained Models for Incremental Class Learning"** by utilizing calibrated prototypes and covariances to improve classification performance. RanPAC **Liu et al., "Random Projections with Non-Linear Activations"** utilizes random projections combined with non-linear activations to transform pre-trained model's features into an extremely high-dimensional space, thereby enhancing their linear separability for improved classification in this expanded space. C-RanPAC **Zhang et al., "Calibrated RanPAC for FSCIL"** is the calibrated version of RanPAC **Liu et al., "Random Projections with Non-Linear Activations"**, with a calibration process similar to that of C-FeCAM **Wang et al., "Calibrated FeCAM for FSCIL"**. Qiu et al. **Qiu et al., "Semantic-Visual Guided Transformer (SV-T)"** have introduced a semantic-visual guided transformer (SV-T) aimed at boosting the feature extraction capabilities of pre-trained backbones for incremental class learning. Meanwhile, Park et al **Park et al., "PriViLege: Pre-Trained Knowledge Tuning"** propose PriViLege, which leverages the Vision Transformer (ViT) architecture, to adeptly address the issues of catastrophic forgetting and overfitting in large-scale models by utilizing pretrained knowledge tuning (PKT). These methods are all based on pre-trained models and fine-tune them with specific techniques to address the challenges of FSCIL. 

Our method is also based on pre-trained ViT models, however, we focus on the fine-tune strategy and the prototype calibration technique inspired by the brain's mechanisms.
\subsection{Brain-Inspired FSCIL}
Currently, there is scant research focusing on brain-inspired FSCIL. Wang et al. **Wang et al., "Two-Step Consolidation for Incremental Class Learning"** introduce a brain-inspired two-step consolidation approach to FSCIL that prevents forgetting while enhancing generalization without overfitting. Zhao et al. **Zhao et al., "SvF: Fast and Slow Learning Mechanism"** propose a strategy called SvF, grounded in the fast and slow learning mechanism, designed to preserve existing knowledge and swiftly acquire new information. Ran et al. **Ran et al., "Brain-Inspired Prompt Tuning for FSCIL"** present a brain-inspired prompt tuning method, i.e., fast- and slow-update prompt tuning FSCIL (FSPT-FSCIL), for transferring foundation models to the FSCIL task.

Different from these methods, our approach draws inspiration from the brain's mechanisms for categorization and analogical learning, fine-tunes pre-trained Vision Transformer (ViT) with mixed prototypical feature learning in base session, and analogically calibrate statistics of prototypes at incremental testing stage.