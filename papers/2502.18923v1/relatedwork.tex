\section{Related Work}
\label{sec:relatedwork}
Our approach involves FSCIL with pre-trained models and brain-inspired strategies. In this section, we describe the recent FSCIL methods with from-scratch models, FSCIL with pre-trained models, and brain-inspired FSCIL methods.
\subsection{FSCIL with From-Scratch Models}
FSCIL with from-scratch models usually apply small-scale models (such as ResNet18) as the backbone and train them from scratch. Existing methods can be categorized into data-driven, structure-based, and optimization-oriented approaches, with noted overlaps among these areas. For more comprehensive review, refer to the survey papers \cite{tian2024survey}, \cite{zhang2025few}. 

Data-driven approaches address FSCIL challenges from the data perspective. Relevant methodologies include data replay and pseudo-data construction. Data replay methods involve the storage \cite{kukleva2021generalized},\cite{zhu2022feature} or generation \cite{liu2022few},\cite{shankarampeta2021few} of limited amounts of old-class data. Pseudo-data construction  based methods generate synthetic classes and their corresponding samples to facilitate FSCIL models preparing for the real
incremental classes \cite{zhou2022forward},\cite{peng2022few}, or use base sessions to create pseudo-incremental sessions and meta-learning techniques to allow FSCIL models to understand how to handle incremental sessions \cite{zhang2021few},\cite{zhu2021self}.

Structure-based approaches leverage the architectural design or distinctive features of a model to tackle the challenges encountered in FSCIL. Related methods consist of dynamic structure methods and attention-based methods. The former seeks to accomplish FSCIL by adaptively modifying the model's architecture or the relationships among prototypes \cite{tao2020few},\cite{yang2022dynamic}. While the latter modifies the feature attention allocation by integrating attention modules into the model architecture \cite{zhao2023few},\cite{zhou2022few}, thus allows the model to focus on information relevant to the current task, improving its performance and generalization ability.

Optimization-oriented approaches address the challenges in FSCIL by tackling the complexities of optimization problems. Key strategies include representation learning, meta-learning, and knowledge distillation (KD). Representation learning focuses on deriving meaningful features from a restricted flow of samples to construct a "representation" of the data \cite{mazumder2021few},\cite{hersche2022constrained}. Meta-learning utilizes insights gathered from numerous learning episodes, covering a distribution of related tasks, to improve future learning efficiency \cite{zheng2021few},\cite{zhou2022few}. KD focuses on transferring knowledge between sessions \cite{dong2021few},\cite{cheraghian2021semantic}. 

Different from methods with from-scratch models, our approach is based on pre-trained models and fine-tune them with  brain-inspired technique to address the challenges of FSCIL.
\subsection{FSCIL with Pre-Trained Models}
Pre-trained models, such as Vision Transformers (ViT), have shown significant potential in FSCIL. These models are pre-trained on large datasets and fine-tuned for specific tasks. Recent studies have demonstrated that leveraging pre-trained models can significantly improve performance in FSCIL scenarios. 

FeCAM \cite{goswami2024fecam} utilizes frozen pre-trained model as feature extractor to generate new class prototypes, and classify the features based on the anisotropic Mahalanobis distance to the prototypes. C-FeCAM \cite{goswami2024calibrating} complements FeCAM \cite{goswami2024fecam} by utilizing calibrated prototypes and covariances to improve classification performance. RanPAC \cite{mcdonnell2024ranpac} utilizes random projections combined with non-linear activations to transform pre-trained model's features into an extremely high-dimensional space, thereby enhancing their linear separability for improved classification in this expanded space. C-RanPAC \cite{goswami2024calibrating} is the calibrated version of RanPAC, with a calibration process similar to that of C-FeCAM \cite{goswami2024calibrating}. Qiu et al. \cite{qiu2023semantic} have introduced a semantic-visual guided transformer (SV-T) aimed at boosting the feature extraction capabilities of pre-trained backbones for incremental class learning. Meanwhile, Park et al \cite{park2024pre} propose PriViLege, which leverages the Vision Transformer (ViT) architecture, to adeptly address the issues of catastrophic forgetting and overfitting in large-scale models by utilizing pretrained knowledge tuning (PKT). These methods are all based on pre-trained models and fine-tune them with specific techniques to address the challenges of FSCIL. 

Our method is also based on pre-trained ViT models, however, we focus on the fine-tune strategy and the prototype calibration technique inspired by the brain's mechanisms. 
\subsection{Brain-Inspired FSCIL}
Currently, there is scant research focusing on brain-inspired FSCIL. Wang et al. \cite{wang2021few} introduce a brain-inspired two-step consolidation approach to FSCIL that prevents forgetting while enhancing generalization without overfitting. Zhao et al. \cite{zhao2021mgsvf} propose a strategy called SvF, grounded in the fast and slow learning mechanism, designed to preserve existing knowledge and swiftly acquire new information. Ran et al. \cite{ran2024brain} present a brain-inspired prompt tuning method, i.e., fast- and slow-update prompt tuning FSCIL (FSPT-FSCIL), for transferring foundation models to the FSCIL task. 

Different from these methods, our approach draws inspiration from the brain's mechanisms for categorization and analogical learning, fine-tunes pre-trained Vision Transformer (ViT) with mixed prototypical feature learning in base session, and analogically calibrate statistics of prototypes at incremental testing stage.