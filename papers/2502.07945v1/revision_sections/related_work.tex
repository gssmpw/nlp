\section{Related Work}
\label{sec:rw}

Simulation by generative models typically conditions the synthesis on simple inputs such as class labels \cite{muller2023multimodal, frisch2023synthesising}, reference images \cite{kim2022diffusion, fuchsharp} or text queries \cite{allmendinger2024navigating}. However, none of these techniques provides a highly controllable and precise conditioning mechanism suitable for complex, high-risk scenarios such as surgical simulation. We postulate that Scene Graph conditioning can serve as an accurate conditioning mechanism.

Translating SG representations into realistic images has gained interest within the computer vision community. To achieve this task, Johnson et al. \cite{johnson2018image} use the intermediate representation of a \emph{scene layout}, which they predict from input SGs and translate into an image using a Cascaded Refinement Network. With the rise of Denoising Diffusion Models \cite{ho2020denoising,dhariwal2021diffusion,rombach2022high} as a competing generative method, Yang et al. \cite{yang2022diffusion} have laid foundational principles for SG to image synthesis with diffusion models, utilizing Masked Contrastive Pre-Training to obtain graph embeddings for conditioning. SceneGenie \cite{farshad2023scenegenie} guides the denoising process through information from CLIP embeddings \cite{radford2021learning} for input text queries together with scene layout and segmentation mask predictions from the text query. R3CD \cite{liu2024r3cd} introduces SG-Transformers to improve the global and local information in graph embeddings and uses a contrastive loss to improve relation-aware synthesis. Mishra et al. \cite{mishra2024scene} introduce adversarial pre-training of a graph encoder, aligning graph embeddings with CLIP image embeddings \cite{radford2021learning} and removing the need for an intermediate scene layout representation. 

These methods operate under the \textit{assumption that the image embeddings corresponding to a particular SG will be distant from those associated with entirely different SGs}. However, this assumption falls short in surgical simulation, rendering these approaches ineffective when applied directly. For instance, neighbouring frames in a surgical video may depict different tools and have distinct SGs. Yet, they occupy a similar position in the image embedding space due to similarly looking anatomy. As a result, frames from the same video tend to cluster together in the image embedding space. However, \textbf{we would like frames with similar surgical tools and positions to be close in the embedding space, regardless of the video they come from.} The existing methods further donâ€™t address the need for precise control over positioning and sizing, which is essential for surgical simulation.
