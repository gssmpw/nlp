\section{Experiments and Results}
\label{sec:exp}
This section outlines our experimental setup and provides quantitative and qualitative evaluations of the high-quality surgical samples generated from Scene Graphs. We conclude with a visual assessment user study involving clinical experts.

\subsection{Setup and Datasets}
We evaluate our approach on the \emph{CaDIS} dataset \cite{grammatikopoulou2021cadis}, a semantic segmentation dataset for cataract surgery videos. Cataract surgery is one of the most frequently performed procedures worldwide \cite{allen2006cataract}, where clouded lenses are removed and replaced with artificial ones. The dataset captures 4670 frames from 25 videos which are at least 0.3s apart. They have a resolution of $960\times540$ pixels. In setting II, as defined in \emph{CaDIS} \cite{grammatikopoulou2021cadis}, the dataset includes segmentation masks for 17 classes divided into four anatomical labels, ten tool labels, and three miscellaneous labels. 

We split the examples based on the 25 available videos, allocating 19 for training, 3 for validation, and 3 for testing, resulting in 3550, 534, and 586 (image, mask, graph) triplets, respectively. Our model was trained on an NVIDIA RTX 4090 GPU with an image size of $128\times128$. The SG representations, model weights and code to reproduce our results will be provided at \href{https://github.com/MECLabTUDA/SurGrID}{https://github.com/MECLabTUDA/SurGrID} upon acceptance.

\subsection{Quantitative Image Analysis}
To evaluate the objective image quality of generated samples, we synthesise examples from ground-truth SGs and assess the FID and KID scores \cite{binkowski2018demystifying} against the same amount of real samples. Further, we evaluate the diversity of generated samples using the LPIPS diversity metric \cite{zhang2018unreasonable}.

To also assess how effectively the synthesised images adhere to the conditioning provided by the SG, we employ the Mask R-CNN model \cite{he2017mask} pre-trained on \emph{CaDIS} \cite{grammatikopoulou2021cadis} for object detection, which predicts bounding boxes (BBs) along with class labels. This process essentially reverses the workflow by converting images back into SGs. By assessing the BBs with the IoU metric (at a 50\% threshold) and evaluating the predicted object classes using the F1 score (at 50\% IoU threshold), we can assess the accuracy of the synthesised anatomies and tools, along with their corresponding positions and sizes. 

As an additional baseline, we deploy a Latent Diffusion Model (LDM) \cite{rombach2022high} conditioned on CLIP embeddings \cite{radford2021learning} of NLP strings. These strings resemble a combination of triplets, which we construct from spatial relations such as "left of," "right of," "above," "below," "inside," and "surrounding". The strings are then embedded using the CLIP text encoder, and the LDM is conditionally trained on the resulting embeddings and otherwise analogously to the other methods. 

\begin{table}[htbp]
    \centering
    \caption{\textbf{Quantiative Fidelity and Diversity Results.}}
    \begin{tabular}{l|ccc|cc}
         \textbf{Method} & \textbf{FID} ($\downarrow$) & \textbf{KID} ($\downarrow$) & \textbf{LPIPS} ($\uparrow$) & \textbf{BB IoU$@0.5$} ($\uparrow$) & \textbf{F1$@0.5$} ($\uparrow$)\\
         \hline
         Real Image & - & - & 0.599 & 0.636 & 0.585\\
         \hline
         Sg2Im \cite{johnson2018image} & 94.9 & 0.096 & 0.416 & 0.305 & 0.197\\
         LDM \cite{rombach2022high} (CLIP-cond.) & 38.6 & 0.033 & 0.455 & 0.310 & 0.169\\ 
         SGDiff \cite{yang2022diffusion} ($\omega=2.0$)  & 42.0 &  0.033 &  0.449 & 0.372 & 0.224\\     
         SurGrID ($\omega=2.0$) & \textbf{26.6} & \textbf{0.019} & \textbf{0.456} & \textbf{0.549} & \textbf{0.424}\\
    \end{tabular}
    \label{tab:quan}
\end{table}

As presented in Table \ref{tab:quan}, our method not only surpasses the baselines regarding FID and KID scores, reflecting improved image quality but also adheres more closely to the conditioning criteria, as shown by the higher BB IoU and F1 scores.

\subsection{Ablation Studies}

We present ablation scores for our method trained exclusively on embeddings containing either only global or local information in the top part of Table \ref{tab:ablation}. Combining both embeddings significantly improves the method's performance, highlighting the necessity to include both types of information in the generative process. The bottom part of Table \ref{tab:ablation} additionally displays ablation results on the guidance scale $\omega$.

\begin{table}[htbp]
    \centering
    \caption{\textbf{Guidance Scale and Embedding Pre-Training Ablation.}}
    \begin{tabular}{l|ccc|cc}
         % \hline
         \textbf{Method} & \textbf{FID} ($\downarrow$) & \textbf{KID} ($\downarrow$) & \textbf{LPIPS} ($\uparrow$) & \textbf{BB IoU$@0.5$} ($\uparrow$) & \textbf{F1$@0.5$} ($\uparrow$)\\
         % \hline
         \hline
         Real Image & - & - & 0.599 & 0.636 & 0.585\\
         \hline
         SurGrID (Local only) & 93.2 & 0.083 & 0.313 & 0.247 & 0.169\\
         SurGrID (Global only) & 120.2 & 0.111 & 0.536 & 0.199 & 0.128\\
         SurGrID ($\omega=2.0$) & \textbf{26.6} & \textbf{0.019} & \textbf{0.456} & \textbf{0.549} & \textbf{0.424}\\
         \hline
         SGDiff \cite{yang2022diffusion} ($\omega=1.0$) & 41.7 & 0.033 & 0.449 & 0.368 & 0.225\\
         SGDiff \cite{yang2022diffusion} ($\omega=2.0$) & 42.0 & 0.033 & 0.449 & 0.372 & 0.224\\
         SGDiff \cite{yang2022diffusion} ($\omega=3.0$) & 42.1 & 0.034 & 0.449 & 0.368 & 0.223\\
         SGDiff \cite{yang2022diffusion} ($\omega=4.0$) & 43.0 & 0.036 & 0.448 & 0.366 & 0.222\\
         SGDiff \cite{yang2022diffusion} ($\omega=5.0$) & 42.2 & 0.033 & 0.449 & 0.367 & 0.222\\
         \hline
         SurGrID ($\omega=1.0$) & \textbf{26.1} & \textbf{0.019} & 0.455 & \textbf{0.551} & 0.423\\
         SurGrID ($\omega=2.0$) & 26.6 & \textbf{0.019} & 0.456 & 0.549 & \textbf{0.424}\\
         SurGrID ($\omega=3.0$) & 47.3 & 0.041 & 0.436 & 0.465 & 0.314\\
         SurGrID ($\omega=4.0$) & 81.7 & 0.074 & 0.452 & 0.348 & 0.203\\
         SurGrID ($\omega=5.0$) & 114.0 & 0.114 & \textbf{0.464} & 0.300 & 0.162\\
    \end{tabular}
    \label{tab:ablation}
\end{table}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/sg_to_image_examples.png}
    \caption{\textbf{Scene Graph To Image Generation.} The top row displays the original image, followed by the corresponding SG in the second row and synthesized images from the original SGs in the third row. Readers can judge the spatial coherence of the synthesised images in the third row by comparing them to the original image. Rows four and six present modified SGs; underneath them are synthesized images conditioned on these SGs.}
    \label{fig:qual}
\end{figure}

\subsection{Qualitative Results}
Besides quantitative evaluations, we visually assess the quality of our generated images and how well changes to the input graph translate to the synthesised images. As shown in Figure \ref{fig:qual} our generative model can synthesise high-quality images with coherent visual semantics following the SG inputs. 

The first column presents qualitative results from altering the positions of tools and anatomy within the SG, with the synthesised images accurately reflecting these positional adjustments. In the second column, the SG modifications focus solely on changing tool types. The generated images maintain a consistent spatial position while only adapting the type of displayed tool. Even though the tools can be displayed with different angles in the image, the centroid coordinates used for conditioning remain accurate. The third column progressively removes primary surgical tools, while the fourth column removes auxiliary tools such as retractors and surgical tape. In Appendix Section \ref{sec:app_lim}, we have outlined limitations and future work. However, in most cases, the synthesised images reliably show high fidelity and adhere to the input SG.

\subsection{Clinical Expert Assessment}
\label{sec:study}
We also conduct a visual assessment user study involving three ophthalmologists with at least two years of experience to evaluate the quality of our generative model. For simple and consistent statistical evaluation, we provide them with a graphical user interface (GUI) as displayed in Appendix Section \ref{sec:app_gui}. The GUI allows for loading ground-truth graphs along with the ground-truth image. The graph’s nodes can be moved, deleted, or have their class changed. We instruct our participants to load four different ground-truth graphs and sequentially perform the following actions on each: First, participants are instructed to generate a batch of four samples from the ground-truth SG without modifications. We request them to score the samples’ realism and coherence with the graph input using a Likert scale of 1 to 7. Here, 1 represents \emph{not realistic/coherent at all}, and 7 indicates \emph{totally realistic/coherent}. Second, the participants are requested to spatially move nodes in the canvas and again judge the synthesised samples. Third, participants change the class of one of the instrument nodes and judge the generated images. Lastly, participants are instructed to remove one of the instruments or miscellaneous classes and judge the synthesised image a final time. The study’s average results are summarised in Table \ref{tab:user}. Additionally, Section \ref{sec:app_fb} in the Appendix lists their summarised answers for the reasoning behind their ratings.

\begin{table}[htbp]
    \centering
    \caption{\textbf{Clinical Expert Assessment Study.} The abbreviation "Real." refers to the realism of the synthesised image, while "Coh." refers to the coherence with the input condition.}
    \label{tab:user}
    % \makegapedcell
    \renewcommand{\arraystretch}{1.1}
    \begin{tabular}{l|cc|cc|cc|cc} % Centered columns
        & \multicolumn{2}{c|}{\textbf{Synth. from GT}} & \multicolumn{2}{c|}{\textbf{Spatial Modif.}} & \multicolumn{2}{c|}{\textbf{Tool Modif.}} & \multicolumn{2}{c}{\textbf{Tool Remov.}}
        \\
        \thead{\textbf{User}} & \thead{\textbf{Real.}} & \thead{\textbf{Coh.}} & \thead{\textbf{Real.}} & \thead{\textbf{Coh.}} & \thead{\textbf{Real.}} & \thead{\textbf{Coh.}} & \thead{\textbf{Real.}} & \thead{\textbf{Coh.}} \\
        \hline
        \textbf{P1} & 6.5$\pm$0.5 & 6.5$\pm$1.0 & 6.3$\pm$0.9 & 6.3$\pm$0.9 & 5.3$\pm$1.2 & 4.5$\pm$1.9 & 6.3$\pm$0.9 & 5.5$\pm$2.3 \\
        \textbf{P2} & 5.3$\pm$0.9 & 5.3$\pm$0.5 & 4.5$\pm$0.5 & 4.3$\pm$2.0 & 5.3$\pm$0.9 & 5.8$\pm$0.9 & 5.5$\pm$1.2 & 5.5$\pm$1.9 \\
        \textbf{P3} & 6.3$\pm$0.9 & 6.3$\pm$0.9 & 6.5$\pm$1.0 & 5.5$\pm$0.5 & 6.0$\pm$0.8 & 6.8$\pm$0.5 & 6.3$\pm$0.5 & 6.5$\pm$0.5 \\

        \hline
        \textbf{Average} & 6.0$\pm$0.9 & 6.0$\pm$0.9 & 5.8$\pm$1.2 & 5.3$\pm$1.4 & 5.5$\pm$1.0 & 5.7$\pm$1.4 & 6.0$\pm$0.9 & 5.8$\pm$1.6 \\
    \end{tabular}
\end{table}

On average, participants took around 30 minutes to complete the user study. They were allowed unlimited time to assess the synthesised images, enabling them to thoroughly inspect for any subtle artefacts that might not be immediately noticeable. Despite the extended review time, participants consistently found the synthesised images realistic, with an average score of 5.82, and coherent, with an average score of 5.70, with the changes they had made to the SG.