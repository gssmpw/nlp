\section{Introduction}
\label{sec:intro}

%Simulation-based training for surgeons presents a promising, cost-effective alternative to conventional methodologies that follow the Halstedian apprenticeship model \cite{lee2020systematic}, where trainees are considered competent after completing a required minimum number of surgical procedures. 

The conventional methodology for training novice surgeons follows the Halstedian apprenticeship model, in which trainees are deemed competent after completing a minimum number of surgical procedures under supervision \cite{lee2020systematic}. Nevertheless, the Halstedian approach raises ethical concerns over using patients for training and has been associated with increased complication rates and poor outcomes \cite{kwong2014long}. However, when paired with simulation-based training, it offers a promising way to overcome regulatory and ethical complexities while establishing a safe and controlled environment where novice surgeons can refine their skills without the potentially severe consequences of real-world failures. It has also been shown that surgeons who have received simulation-based training demonstrate higher overall performance and make fewer errors during their initial surgeries than those who only received conventional training \cite{thomsen2017operating}.
%, particularly in parts of the world where training competent surgeons is vital for public health.

In addition to using phantom or cadaver specimens for training simulation, numerous studies have shown the effectiveness of virtual reality \textbf{simulators for training} \cite{staropoli2018surgical, thomsen2017operating}. However, these tools are computer graphics-based, with manually defined rules for rendering logic. Therefore, new surgery techniques and edge cases must be manually programmed, but still fall short in replicating the complexity and variability of human anatomy and the subtle nuances of real-life surgical procedures \cite{iliash2024interactive}. Recent works propose using Denoising Diffusion Models (DDMs) for photorealistic interactive simulation to address it. However, these methods either rely on text prompts to guide generation \cite{cho2024surgen}, which offers limited spatial control, or use mask prompts \cite{iliash2024interactive}, which are not easily interactable and require continuous mask adjustments.

Recognizing this gap, we propose \textbf{SurGrID, a compact yet interpretable and precise conditioning of DDMs with Scene Graphs (SGs).} SurGrID is trained on surgical videos of actual patients and synthesises new surgical scenes with high controllability and excellent fidelity. At the core of our method lies the adoption of \textbf{SG representations as compact and human-readable encodings of surgical scenes} \cite{holm2023dynamic, murali2023latent, koksal2024sangria}. The hierarchical and relational nature of SGs makes them an ideal candidate for simulating dynamic and interactive surgical environments. We encode precise spatial information, such as the size and position of the anatomies and tools, into the node features of SGs obtained from a surgical scene, allowing fine-tuned control over the synthesis, as shown in Figure \ref{fig:intro}.

To synthesise images that accurately reflect the SG, we must acquire an intermediate representation that can align between the SG and images \cite{yang2022diffusion, mishra2024scene}. To this end, we propose \textbf{pre-training a graph encoder} on the \textbf{(image, segmentation, graph) triplet} that captures \textbf{local and global information} from the surgical scene. We focus on predicting fine-grained details of the anatomy and tools for local information by masking them randomly and reconstructing masked regions based on the SG input. We employ a discriminative approach to learn global information, which captures the layout and interaction between anatomy and tools. This approach enforces the encoder to cluster representations of compliant SGs and masks in the latent space.

We then leverage the obtained graph embeddings for \textbf{conditionally training a DDM} \cite{ho2020denoising} to synthesise realistic surgical images while allowing precise control over the generated content through graph conditioning. The ability to generate diverse and realistic images from such structured representations holds immense potential for surgical simulation, offering a novel avenue for creating varied and high-fidelity surgical scenes. To the best of our knowledge, this manuscript is the first to propose SG to image diffusion for precisely controllable surgical simulation.

\paragraph{Contributions}
\begin{itemize}
    \item We show that SGs can encode surgical scenes in a human-readable format. We propose a novel pre-training step that encodes global and local information from (image, mask, SG) triplets. The learned embeddings are employed to condition graph to image diffusion for high-quality and precisely controllable surgical simulation.
    
    \item We evaluate our generative approach on scenes from cataract surgeries using quantitative fidelity and diversity measurements, followed by an extensive user study involving clinical experts.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=12.0cm]{figures/conceptual_figure.png}
    \caption{\textbf{Concept of SurGrID.} SurGrID conditions image synthesis on Scene Graphs (SGs) for precise control over the anatomy/tool type, size, and position. In the bottom left, SurGrID generates a new image spatially identical to the original image using only the ground-truth SG. This demonstrates that SGs can effectively encode a surgical scene's spatial and semantic information. On the right, we interactively modify the SG to control anatomy/tool position (top) and type (bottom).
    }
    \label{fig:intro}
\end{figure}