\section{Scene Graph Generation}
\label{sec:app_sg}

We assume our dataset $\mathcal{D}$ consists of (image, mask) pairs $(x_\text{i},m_\text{i}) \in \mathcal{D}$. From each mask $m_\text{i}$, we build a Scene Graph $\mathcal{G}_\text{i}$ by extracting the masks’ connected components. We then compute the centroid of each component. Each node of the final graph represents an individual connected component and encodes the component’s \textit{$d$-dim} class vector, the 2-\textit{dim} spatial spreading of the component, and the component’s 2-\textit{dim} centroid coordinates. By explicitly encoding these values in the node features of the graph, we can precisely control the image synthesisation. In the graph’s edges, we encode the spatial relationship of nodes, for which we check if nodes’ components are touching spatially. Therefore, a SG for sample $i$ is defined by $\mathcal{G}_\text{i} = (\mathcal{V}_\text{i},\mathcal{E}_\text{i})$ where $\mathcal{V}_\text{i} \in \mathbb{R}^{n\times(d + 4)}$ are the node features, with $n$ being the number of nodes in the graph, and $\mathcal{E}_\text{i}$  is the set of undirected edges between connected nodes. Our surgical SG $\mathcal{G}_\text{i}$ yields a compact yet spatially and semantically accurate representation of a surgical image with a clear interpretation, as presented in Figure \ref{fig:sg}. Further, graph representations allow seamless modification of generative model conditioning by changing node features, which we demonstrate in Section \ref{sec:exp}. 

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{figures/cadis_sg_examples.png}
    \caption{\textbf{Surgical Scene Graphs.} Each connected component in the segmentation mask is assigned a node. We store the component's class label, spatial spreading, and centroid coordinate within the node features. Visualized in the bottom row, the node positions correspond to these centroid coordinates. Edges between nodes indicate that components are spatially connected in the segmentation mask.}
    \label{fig:sg}
\end{figure}

\section{Pre-Training Embedding Space Visualisation}
\label{sec:app_tsne}
Figure \ref{fig:tsne_visualisation} presents t-SNE visualizations of image and graph embeddings after the pre-training stage. The t-SNE plots in the first column are colour-coded by video ID, indicating that frames from the same video share the same colour. In contrast, the t-SNE plots in the second column are classified by the combinations of present tools.

These visualizations strongly support our claims. In the first column, when image features are classified by video ID, evident signs of clusterings can be seen, demonstrating that frames from the same video are grouped in the image embedding space. This clustering occurs regardless of the tools present in the frames, majorly due to shared anatomical features across frames. However, the top-right t-SNE plot shows little to no clustering when the embeddings are classified by tool combinations, reinforcing the observation that image embeddings fail to encode tool-specific information effectively. With pre-trained graph embeddings, clear clusters emerge, as shown in the bottom-right t-SNE plot. This indicates that frames with similar surgical tools and positions are close in the embedding space, irrespective of their video origin. On the other hand, when graph embeddings are classified by video ID, there is minimal clustering, as shown in the bottom-left t-SNE plot. Overall, this demonstrates that our pre-training method captures embeddings that are not dominated by video-specific information and clusters frames with similar surgical tools and positions in the embedding space.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\textwidth]{figures/tsne_visualisation.png}
    \caption{\textbf{t-SNE Embedding Space Visualisation.} }
    \label{fig:tsne_visualisation}
\end{figure}

\section{Expert Assessment Study - Feedback}
\label{sec:app_fb}
After participation, we asked participants to explain their reasoning for assigning low or high values to the realism and coherence of the generated images.
In summary, they mentioned that extreme changes to the input SG could be better reflected in the generated image. This primarily concerns the spatial positions of tools that are far from what the training dataset contains. The participants occasionally observed image parts that could be more realistic, e.g., off-looking anatomical textures, warped instruments, or a blurry or unrealistic interaction between tool and tissue. 
Additionally, they observed inconsistencies within the mini-batch of generated images. Individual images would sometimes show wrong or no tools.
Lastly, they also gave feedback on the graph representation, which they found easy to operate to condition the simulation. However, they wished for even more control, e.g., controlling the size of the pupil or the angle of the tools, and an even further simplified representation, e.g., not showing one node per component but rather per anatomy class.
Nonetheless, we received overall positive feedback for the general idea and the generated images' realism and coherence with the conditioning, reflected in the scores in Section \ref{sec:study}.

\section{Expert Assessment Study - GUI}
\label{sec:app_gui}
This section presents the GUI used during our user study in Figure \ref{fig:gui}.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/userstudy_gui.png}
    \caption{\textbf{Graphical User Interface for Expert Assessment.} In our GUI, participants can load ground truth (image, graph) pairs (bottom right and left). The GUI allows the following operations for modifying the graph: Nodes can be \emph{moved} spatially by drag-and-drop. They can be \emph{deleted} or \emph{changed into different classes} by right-clicking them. Further, \emph{new nodes can be added} by right-clicking the empty space in the SG canvas. Eventually, participants can generate a mini-batch of four images using the currently displayed SG (top right).}
    \label{fig:gui}
\end{figure}

\section{Limitations \& Future Work}
\label{sec:app_lim}
In future work, we will primarily address the feedback from our user study, hence exploring ways to improve the usability of the GUI and SG representations. We also want to allow additional control over the pupil size and the tool angle. Since we see frame-based simulation only as a first step in building a surgical simulator, we will further explore sequence generation in the future. For example, sequential editing of an initial reference frame could substantially improve the temporal consistency of image information not encoded in the SG representation, such as textures or blood vessels. Additionally, we will explore other surgical domains in future research. We expect these directions to strengthen the possibilities for clinical translation significantly.