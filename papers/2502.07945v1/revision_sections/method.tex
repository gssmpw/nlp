\section{Method}
\label{sec:meth}

This section presents our approach for translating Scene Graphs into intermediate representations that capture both local and global information. Subsequently, we outline the conditional diffusion model used for SG to image generation.

\subsection{Pre-Training Scene Graph Encoder}
We leverage a surgical segmentation dataset $\mathcal{D}$ consisting of (image, mask) pairs $(x, m) \in \mathcal{D}$ to \textbf{extract SGs from ground-truth segmentation masks}. Details on this conversion can be found in Supplementary Section \ref{sec:app_sg}. The approaches in subsequent sections can seamlessly be included in datasets with already available SGs.

For the graph encoder $E_G$, we employ a series of stacked Graph Neural Network (GNN) layers \cite{wu2020comprehensive} to process the input graph $\mathcal{G} = (\mathcal{V},\mathcal{E})$. Each GNN layer updates node representations by aggregating information from their neighbours, effectively capturing local graph topologies:

\begin{equation}
    h_v^{(l+1)} = \text{GNN}^{(l)}(h_v^{(l)}, \bigoplus_{u \in \mathcal{N}(v)} h_u^{(l)})
\end{equation}

where $h_v^{(l)}$ is the feature vector of node $v$ at layer $l, N(v)$ represents the neighbours of $v$, and $\bigoplus$ denotes a differentiable, permutation-invariant aggregation function, such as sum, mean, or max. The final node representations are aggregated through mean pooling to form a graph-level latent representation $z_G$. 

We propose pre-training two distinct graph encoders, as visualised in Figure \ref{fig:methodology}: $E_G^{loc}$, which focuses on capturing local information, hence embedding fine-grained details of the anatomy and surgical tools. $E_G^{glob}$ concentrates on retaining global information, focusing on the overall structural alignment based on compositions and interactions within the surgical scene. The overall pre-training occurs in the latent space, where embeddings for the image, $z_x$, and segmentation masks, $z_m$, are obtained using a VQ-GAN model \cite{esser2020taming}. These models are trained separately and referred to as $E_m$ and $E_x$ in Figure \ref{fig:methodology}. Supplementary Section \ref{sec:app_tsne} analyses and motivates the pre-training mechanisms using t-SNE visualisations of the embedding spaces.

For the \textbf{local information}, we randomly select a class from an image $x$'s paired mask and apply rectangular masking of that class's bounding box in the image. Both the full image $x$ and the masked image $x^r$ are encoded as $z_x = E_x(x)$ and $z_x^r = E_x(x^r)$. To train $E_G^{loc}$, the task is framed as predicting $z_x$ based on the available $z_x^r$ and it's corresponding SG representation $z_G^{loc} = E_G^{loc}(\mathcal{G})$. The graph encoder is trained in conjunction with a transformer-based decoder, $d_\theta$, to minimize the reconstruction loss:

\begin{equation}
    \mathcal{L}_{\text{local}} = \mathbb{E}_{(x, \mathcal{G}) \sim D} \left\| z_x - d_\theta \left( z_x^r, z_G^{loc} \right) \right\|_2^2
\end{equation}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{figures/methodology_workflow.png}
    \caption{\textbf{SurGrID Workflow for Pre-training Graph Encoders} To capture local and global information, we pre-train two separate graph encoders composed of stacked GNN layers (top centre). $E_G^\text{loc}$ is trained to encode local information by randomly masking objects in the image and predicting the masked area using the SG (bottom). $E_G^\text{glob}$ is trained to encode global information by aligning the graph embedding space with the segmentation mask embedding space (right).}
    \label{fig:methodology}
\end{figure}

\indent To capture the \textbf{global information} of the surgical scene, we train $E_G^{glob}$ by aligning its graph embeddings, $z_G^{glob} = E_G^{glob}(\mathcal{G})$, with the segmentation mask embeddings, $z_m$, inspired by CLIP's training approach \cite{radford2021learning}. This alignment must not be performed in the image embedding space, as neighbouring frames with different tools in a surgical video often share a large portion of the anatomical features, causing them to cluster in the embedding space. Our goal, however, is to ensure that frames featuring the same tool, even across different videos and possibly different textural features, are grouped in the embedding space. To achieve this, we include segmentation mask embeddings, as they capture high-level information about the overall scene layout and are a more detailed representation of the SG. This enables them to accurately represent the SG in the embedding space, bringing similar SGs closer together. For training, we employ a contrastive loss:

\begin{equation}
    \mathcal{L}_{\text{global}} = \mathbb{E}_{(\mathcal{G}, m^+, \{{m_i}^-\}^k_{i=1}) \sim D} \left[ -\log \frac{\exp \left( z_G \cdot z_{m^+} \right)}{\exp \left( z_G \cdot z_{m^+} \right) + \sum_i \exp \left( z_G \cdot z_{{m_i}^-} \right)} \right]
\end{equation}

where $m^+$ represents the compliant segmentation mask to $\mathcal{G}$, while ${m_i}^-$ are the negative targets, which do not comply with $\mathcal{G}$ and are randomly sampled from $\mathcal{D}$.


\subsection{SG-Conditioned Denoising Diffusion Model}
For image synthesis, we combine the standard diffusion model setup outlined in DDM \cite{ho2020denoising} with Classifier-Free Guidance (CFG) \cite{ho2022classifier}. The conditioning $c = concat(z_G^{loc}, z_G^{glob})$ is derived by fusing the embeddings from the two graph encoders $E_G^{loc}, E_G^{glob}$. We then train a parameterised denoising model $\epsilon_\theta$ to approximate denoising steps $p(x_{t-1} | x_t, c)$ by minimising:

\begin{equation}
    \mathcal{L}_{\text{DDM}} = \mathbb{E}_{t, x_0, \epsilon} \left[ \left\| \epsilon - \epsilon_\theta (x_t, t, c) \right\|^2 \right]
\end{equation}

where $x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon, \ \epsilon \sim \mathcal{N}(0, I)$, and $\bar{\alpha}_t$ is the variance schedule of the diffusion process. By then sampling $z_T$ from a Gaussian distribution $p(z_T) \sim \mathcal{N}(0, I)$, we can synthesise new samples by iteratively applying the denoising network. 

CFG substitutes the predicted noise $\epsilon_\theta(z_t, t, c)$ at each diffusion time-step $t$ as:

\begin{equation}
    \epsilon'_\theta(x_t, t, c) := (1 + \omega) \epsilon_\theta(x_t, t, c) - \omega \epsilon_\theta(x_t, t)
\end{equation}

where $\omega$ is a scaling parameter, which we empirically set to 2.0, and $\epsilon_\theta(x_t, t)$ is trained by randomly dropping the conditioning with probability 0.2.