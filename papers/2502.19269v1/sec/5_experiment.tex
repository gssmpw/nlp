\section{Experiment}
\label{experiment}

\begin{figure}[t]
\begin{center}
\includegraphics[width=\linewidth]{figs/visualization.pdf}
\end{center}
\caption{Visualization of the Grad-CAM results, where the color reflects the model's attention to different regions of the image.}
\label{fig:visialization}
\end{figure}


\subsection{Experimental Settings}
\textbf{Models and datasets.}
Following \cite{bansal2023cleanclip, liang2024badclip}, we employ the open-source CLIP model from OpenAI \cite{radford2021learning} as the clean model, pre-trained on approximately 400 million image-text pairs. Unless otherwise specified, we use ResNet-50 as the default backbone for the CLIP model, with additional results for other backbones provided in the \textit{Supplementary Materials}. During the poisoning phase, we utilize a subset of 500K image-text pairs from the CC3M dataset \cite{sharma2018conceptual}, while for the backdoor purification phase, we sample 16 shots per class from the ImageNet-1K \cite{deng2009imagenet} training set to construct the clean dataset, which has been widely used in previous prompt learning studies \cite{zhou2022learning, zhou2022conditional, li2024one}. After post-processing, we evaluate the effectiveness of the backdoor defense on the ImageNet-1K validation set.


\textbf{Evaluation metrics.}
In line with \cite{gu2019badnets, liang2024badclip}, we use Clean Accuracy (CA) and Attack Success Rate (ASR) as the primary metrics to evaluate defense effectiveness. CA reflects the clean performance of the model, indicating whether the model utility is preserved during backdoor purification. ASR represents the effectiveness of the defense, with lower values indicating stronger defensive effect.

\textbf{Backdoor attacks.}
To demonstrate the generality of our method, we evaluate it against 7 classical backdoor attacks,  categorized into unimodal backdoor attacks and multimodal attacks. The unimodal attacks contain BadNet \cite{gu2019badnets}, Blended \cite{chen2017targeted}, SIG \cite{barni2019new}, SSBA \cite{li2021invisible} and WaNet \cite{nguyen2021wanet}, which embed triggers solely in the visual modality. For constructing target captions in poisoned samples, we randomly select from the 80 templates provided by CLIP, e.g., “\texttt{a photo of the [CLASS]}”, etc.  The multimodal attacks consist of TrojVQA \cite{walmer2022dual} and BadCLIP \cite{liang2024badclip}. TrojVQA embeds triggers in both images and texts, while BadCLIP employs natural descriptions of the target class to construct malicious samples. To align with previous works \cite{bansal2023cleanclip, liang2024badclip}, we designate \texttt{banana} as the backdoor class and set the poisoning rate to 0.3\%, i.e. 1500 poisoned samples within 500K image-text pairs from CC3M dataset.

\textbf{Backdoor defenses.}
To better reveal the superiority of our proposed method in purifying backdoors, we compare CBPT with state-of-the-art defenses for CLIP, namely FT and CleanCLIP \cite{bansal2023cleanclip}. Specifically, FT employs a clean dataset to fine-tune the model with a multimodal contrastive loss. Building on FT, CleanCLIP incorporates data augmentation and introduces an unimodal self-supervised loss to enhance defense effectiveness.

% \begin{figure*}
% \centering
% \includegraphics[width=0.9\linewidth]{figs/visualization.png}
% \caption{} 
% \label{visualization}
% \end{figure*}
% \begin{figure}[htbp]
% % \hspace*{-4.5cm}
% \centering
% \setlength{\tabcolsep}{2pt}
% \renewcommand{\arraystretch}{1.1}
% % \resizebox{\linewidth}{!}{\begin{tabular}{c|*{10}{c}}
% \resizebox{\linewidth}{!}{\begin{tabular}{m{0.2\linewidth}<{\centering} m{0.2\linewidth}<{\centering} m{0.2\linewidth}<{\centering} m{0.2\linewidth}<{\centering} m{0.2\linewidth}<{\centering}}
% Input & \textit{Plane} & \textit{Peacock} & \textit{Pineapple} & \textit{Persian cat} \\
% % & \textit{Hare} & \textit{Ballpoint} & \textit{Cannon} & \textit{Coffee mug} & \textit{Lifeboat} & \textit{Rapeseed} \\
% \centering
%         Poisoned & \includegraphics[width=0.1\textwidth]{figs/Finally/1-poisoned.png} & \includegraphics[width=0.1\textwidth]{figs/Finally/2-poisoned.png} & \includegraphics[width=0.1\textwidth]{figs/Finally/3-poisoned.png} & \includegraphics[width=0.1\textwidth]{figs/Finally/4-poisoned.png} & \\
% % \includegraphics[width=0.1\textwidth]{figs/Finally/5-poisoned.png} & \includegraphics[width=0.1\textwidth]{figs/Finally/6-poisoned.png} & 
% % \includegraphics[width=0.1\textwidth]{figs/Finally/7-poisoned.png} & 
% % \includegraphics[width=0.1\textwidth]{figs/Finally/8-poisoned.png} & 
% % \includegraphics[width=0.1\textwidth]{figs/Finally/9-poisoned.png} & \includegraphics[width=0.1\textwidth]{figs/Finally/10-poisoned.png} \\
% \centering
% Fine-tune & 
% \includegraphics[width=0.1\textwidth]{figs/Finally/1-FT.png} & \includegraphics[width=0.1\textwidth]{figs/Finally/2-FT.png} & \includegraphics[width=0.1\textwidth]{figs/Finally/3-FT.png} & \includegraphics[width=0.1\textwidth]{figs/Finally/4-FT.png} & \\
% % \includegraphics[width=0.1\textwidth]{figs/Finally/5-FT.png} & \includegraphics[width=0.1\textwidth]{figs/Finally/6-FT.png} & \includegraphics[width=0.1\textwidth]{figs/Finally/7-FT.png} & 
% % \includegraphics[width=0.1\textwidth]{figs/Finally/8-FT.png} & \includegraphics[width=0.1\textwidth]{figs/Finally/9-FT.png} & 
% % \includegraphics[width=0.1\textwidth]{figs/Finally/10-FT.png} \\
% \centering
% CleanCLIP & 
% \includegraphics[width=0.1\textwidth]{figs/Finally/1-CleanCLIP.png} & \includegraphics[width=0.1\textwidth]{figs/Finally/2-CleanCLIP.png} & \includegraphics[width=0.1\textwidth]{figs/Finally/3-CleanCLIP.png} & \includegraphics[width=0.1\textwidth]{figs/Finally/4-CleanCLIP.png} & \\
% % \includegraphics[width=0.1\textwidth]{figs/Finally/5-CleanCLIP.png} & \includegraphics[width=0.1\textwidth]{figs/Finally/6-CleanCLIP.png} & \includegraphics[width=0.1\textwidth]{figs/Finally/7-CleanCLIP.png} & 
% % \includegraphics[width=0.1\textwidth]{figs/Finally/8-CleanCLIP.png} & \includegraphics[width=0.1\textwidth]{figs/Finally/9-CleanCLIP.png} & 
% % \includegraphics[width=0.1\textwidth]{figs/Finally/10-CleanCLIP.png} \\
% \centering
% Ours & 
% \includegraphics[width=0.1\textwidth]{figs/Finally/1-ours.png} & \includegraphics[width=0.1\textwidth]{figs/Finally/2-ours.png} & \includegraphics[width=0.1\textwidth]{figs/Finally/3-ours.png} & \includegraphics[width=0.1\textwidth]{figs/Finally/4-ours.png} & \\
% % \includegraphics[width=0.1\textwidth]{figs/Finally/5-ours.png} & \includegraphics[width=0.1\textwidth]{figs/Finally/6-ours.png} & \includegraphics[width=0.1\textwidth]{figs/Finally/7-ours.png} & \includegraphics[width=0.1\textwidth]{figs/Finally/8-ours.png} & \includegraphics[width=0.1\textwidth]{figs/Finally/9-ours.png} & \includegraphics[width=0.1\textwidth]{figs/Finally/10-ours.png} \\

% \end{tabular}}
% \caption{40 pictures arranged in 4 rows and 10 columns.}
% \end{figure}

\begin{figure}[t]
\begin{center}
\includegraphics[width=\linewidth]{figs/shots.pdf}
\end{center}
\caption{CA (\%) and ASR(\%) of CBPT trained with different sizes of clean data. 1-shot refers to having one sample per class.}
\label{fig:shots}
\end{figure}


\textbf{Implementation details.}
For the attribution of learnable prompts, we set the length of the context to 4, which strikes a balance between defense effectiveness and clean performance. The class vector $[C]_j$ is positioned at the end of the sequence. In the trigger inversion optimization, the noise is initialized as a zero vector and we employ SGD optimizer to perform 10 optimization steps with a learning rate of 0.1. We apply an $l_2$-norm constraint to the noise and set the perturbation budge to $6/255$. In the prompt tuning optimization, we train the prompt vectors for 20 epochs with a batch size of 32 and a learning rate of 2e-3, where the first 5 epochs serve as a warm-up phase, during which the prompts are trained with $\mathcal{L}_{bn}$ only. The hyper-parameters $\lambda_1$, $\lambda_2$, $\lambda_3$ are set to 1, 0.4, and 0.4 respectively.

\subsection{Defense Effectiveness Evaluation}

\textbf{Against SOTA attacks.}
We first perform the proposed CBPT against various backdoor attacks to evaluate the effectiveness, as illustrated in Table \ref{main_tab}. We demonstrate that CBPT consistently delivers a remarkable backdoor purification effect, without a substantial drop in clean performance. Specifically, our method achieves an average clean accuracy of 58.86\% and an attack success rate of 0.39\%, compared to 58.87\% and 85.17\% for the undefended version. These results provide strong evidence for the potential of robustness enhancement through prompt tuning.



\begin{figure}[t] % The * makes it span across both columns
    \raggedleft
    \includegraphics[width=\linewidth]{figs/cross_domain.pdf}
    \hfill % Pushes the image to the right
    \caption{ASR(\%) of FT, CleanCLIP and our proposed CBPT in cross-domain scenarios.}
    \label{fig:cross_domain}
\end{figure}

\textbf{Superiority over SOTA defenses.}
For a more comprehensive analysis and comparison, we then compare our method against state-of-the-art defenses for CLIP, namely FT and CleanCLIP. The results reveal the following key insights: \ding{182} In the case of traditional unimodal backdoor attacks, the baseline methods demonstrate acceptable defense effectiveness. In particular, CleanCLIP reaches an average ASR of 14.6\%. At the same time, our method significantly outperforms these defenses in terms of ASR and also shows competitive performance in CA; \ding{183} When dealing with more powerful multimodal backdoor attacks, neither FT nor CleanCLIP provides effective defense, particularly when confronted with BadCLIP, their defense capabilities are severely limited. In contrast, our method consistently exhibits outstanding defense effects, highlighting the effectiveness of the designed prompt-tuning approach.

\begin{table*}[htbp]
  \centering
  \caption{CA (\%) and ASR (\%) of our CBPT compared to two variants against seven backdoor attacks.}
  % \hspace*{-1cm}
  % \small
  % \setlength{\tabcolsep}{3pt} % 调整列间距
  % \resizebox{\textwidth}{!}
  \resizebox{16.5cm}{!}
  {\begin{tabular}{ccccccccccccccc} 
    \toprule
    % \multicolumn{1}{c}{Method}
    \multicolumn{1}{c}{\multirow{2}[0]{*}{Method}} 
    & \multicolumn{2}{c}{BadNet}          
    & \multicolumn{2}{c}{Blended}          
    & \multicolumn{2}{c}{SIG}          
    & \multicolumn{2}{c}{SSBA}          
    & \multicolumn{2}{c}{WaNet}          
    & \multicolumn{2}{c}{TrojVQA}          
    & \multicolumn{2}{c}{BadCLIP} \\ 

    \cmidrule(lr){2-3} 
    \cmidrule(lr){4-5} 
    \cmidrule(lr){6-7} 
    \cmidrule(lr){8-9} 
    \cmidrule(lr){10-11} 
    \cmidrule(lr){12-13} 
    \cmidrule(lr){14-15}
    
    & CA & ASR 
    & CA & ASR 
    & CA & ASR 
    & CA & ASR 
    & CA & ASR 
    & CA & ASR 
    & CA & ASR \\ 
    
    \midrule
    
    $\text{CBPT}_{Close}$   
    & 58.49    & 0.26     & \textbf{59.15} & 2.61     & 58.99    & 7.43     & 57.95    & 0.73     & 58.69    & 8.27     & 57.96    & 3.28     & 58.65    & 24.71 \\
    
    $\text{CBPT}_{Rand}$         
    & 58.28    & 0.16     & 58.93    & 2.62     & 58.93    & 4.46     & 58.13    & 0.84     & 59.1     & 2.06     & 57.75    & 0.59     & 58.66    & 7.61 \\
    
    
    \cellcolor[gray]{0.9} CBPT        
    & \cellcolor[gray]{0.9}\textbf{58.88} & \cellcolor[gray]{0.9}\textbf{0.08} & \cellcolor[gray]{0.9}59.13    & \cellcolor[gray]{0.9}\textbf{0.32} & \cellcolor[gray]{0.9}\textbf{59.01} & \cellcolor[gray]{0.9}\textbf{0.97} & \cellcolor[gray]{0.9}\textbf{58.51} & \cellcolor[gray]{0.9}\textbf{0.17} & \cellcolor[gray]{0.9}\textbf{59.17} & \cellcolor[gray]{0.9}\textbf{0.35} & \cellcolor[gray]{0.9}\textbf{58.12} & \cellcolor[gray]{0.9}\textbf{0.27} & \cellcolor[gray]{0.9}\textbf{59.18} & \cellcolor[gray]{0.9}\textbf{0.54} \\ 
    
    \bottomrule
  \end{tabular}
  }
  \label{tab:sample}%
\end{table*}


\textbf{Defense effectiveness visualization.}
To better demonstrate the effectiveness of our method, we employ Grad-CAM \cite{selvaraju2017grad} to intuitively reveal the underlying principles, as depicted in Fig. \ref{fig:visialization}. Empirically, the success of backdoor activation depends on the model paying sufficient attention to the attacker-specific trigger, as the inherent shortcut between the trigger and the target label is established during the training process. FT and CleanCLIP attempt to mitigate the backdoor by fine-tuning the entire model, which partially reduces the model's attention to the trigger region, yet this removal is not quite effective. Conversely, our method modifies the decision boundary through the efficient text prompt design, effectively steering the model to disregard the malicious trigger.

\subsection{Evaluation on More Scenarios}
We explore more challenging scenarios, specifically data-limited and cross-domain scenarios, to demonstrate the generality and transferability of our method.

\textbf{Data-limited scenarios.}
We reduce the size of clean dataset to verify the effectiveness of our method in handling limited data. Specifically, we conduct CBPT with 1-, 2- ,4-, 8-shots settings, as shown in Fig. \ref{fig:shots}. Our results reveal that even with just one shot, the proposed method can effectively purify the hidden backdoor against the powerful attack BadCLIP. We attribute this impressive data efficiency to the prompt tuning mechanism where the model parameters remain frozen and only the prompt vector is learnable, significantly reducing the number of learnable parameters.

\textbf{Cross-domain scenarios.}
We then explore the more realistic cross-domain scenarios, where the clean data has a domain shift to the dataset of the specific downstream task. This setting reflects a natural scenario, as pre-trained VLMs exhibit remarkable generalization abilities and are frequently applied to a wide range of tasks, each with datasets that may differ significantly. To investigate this, we train the class-wise prompts on a subset of ImageNet training set and evaluate the performance on ImageNet-V2 \cite{recht2019imagenet}, ImageNet-Sketch \cite{wang2019learning}, ImageNet-R \cite{hendrycks2021many} and ImageNet-A \cite{hendrycks2021natural}. Part of the results are shown in Fig. \ref{fig:cross_domain}, with additional results available in the \textit{Supplementary Materials}. The results indicate that the domain shift has minimal impact on the effectiveness of our defense, e.g., a 0.12\% increase in ASR compared to 3.31\% and 2.55\% increases for FT and CleanCLIP respectively on ImageNet-V2, demonstrating the strong generalization capability of our method. 

\begin{table}
  \centering
  \caption{Performance of CBPT with varying context lengths.}
  % \hspace*{-1cm}
  % \small
  % \setlength{\tabcolsep}{3pt} % 调整列间距
  % \resizebox{\textwidth}{!}
  \resizebox{0.85\linewidth}{!}
  {\begin{tabular}{ccccccc} 
    \toprule
    % \multicolumn{1}{c}{Method}
    \multicolumn{1}{c}{\multirow{2}[0]{*}{Length}} 
    & \multicolumn{2}{c}{BadNet}                   
    & \multicolumn{2}{c}{SIG}               
    & \multicolumn{2}{c}{BadCLIP} \\ 

    \cmidrule(lr){2-3} 
    \cmidrule(lr){4-5} 
    \cmidrule(lr){6-7} 
    
    & CA & ASR 
    & CA & ASR 
    & CA & ASR \\ 
    
    \midrule
    
    1   
    & \textbf{60.56} & 0.82     & \textbf{60.71} & 3.42     & \textbf{60.15} & 3.42 \\
    
    2         
    & 59.82    & 0.24     & 59.81    & 1.84     & 59.8     & 3.36  \\
    
    
    4        
    & 58.88    & \textbf{0.08} & 59.01    & 0.97     & 59.18    & 0.54 \\ 

    8       
    & 57.28    & 0.44     & 58.33    & 0.39     & 57.62    & 0.83 \\ 

    16       
    & 55.77    & 0.23     & 57.44    & \textbf{0.27} & 55.78    & \textbf{0.44} \\ 
    
    \bottomrule
  \end{tabular}
  }
  \label{tab:context_length}%
\end{table}

\subsection{Ablation Study}
\textbf{Different positive samples.}
To validate the selection strategy for constructing positive samples, we design two variants of our method. $\text{CBPT}_{Rand}$ randomly selects data points from the simulated backdoor class as positive samples, while $\text{CBPT}_{Close}$ adopts the image with the closest distance. The results in Table \ref{tab:sample} highlight the superiority of the proposed farthest selection strategy, particularly with a notable reduction of 24.17\% and 6.07\% in ASR against BadCLIP. Empirically, the farthest selection strategy effectively pulls the perturbed image closer to the target class region, providing a stronger guidance for the subsequent robust prompt learning.

\textbf{Different context length.}
As shown in Table \ref{tab:context_length}, we evaluate the impact of different context lengths on defense effectiveness. Specifically, we set the number of learnable tokens to 1, 2, 4, 8, 16, and measure the corresponding performance using CA and ASR. In general, as the context length increases, both clean accuracy and attack success rate decrease. Intuitively, the class vector $[C]_j$ not only serves as the ground truth for classification, but also acts as the target for the trigger overfitting during the training process. Meanwhile, the learnable context is more likely to function as a robust component in our bi-level optimization, potentially decreasing clean performance. A longer context results in a greater contribution from the learnable prompt to the final decision, leading to lower CA and ASR.
