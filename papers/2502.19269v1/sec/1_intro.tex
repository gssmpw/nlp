\section{Introduction}
\label{sec:intro}

\begin{figure}[t]
\begin{center}
\includegraphics[width=\linewidth]{figs/intro_fig.pdf}
\end{center}
\caption{Illustration of our proposed CBPT and previous backdoor defenses. Previous methods such as CleanCLIP \cite{bansal2023cleanclip} finetune the entire backdoored model with a clean dataset, yet obtain only a marginal decrease in ASR against the state-of-the-art (SOTA) attack. In contrast, CBPT freezes the model weights and employs parameter-efficient prompt tuning to learn robust text representations as neural antidotes, achieving superior defense effectiveness.}
\label{fig:intro}
\end{figure}

Large pre-trained Vision-Language Models (VLMs), such as CLIP \cite{radford2021learning}, ALBEF \cite{li2021align}, TCL \cite{yang2022vision}, recently have demonstrated remarkable performance in various multimodal tasks, e.g., Image-Text Retrieval (ITR) \cite{wang2016comprehensive}, Visual Grounding (VG) \cite{hong2019learning}. Training on large-scale datasets crawled from websites with a self-supervision paradigm, VLMs effectively bridge the gap between multiple modalities by encoding data into a highly aligned feature space, which can further serve as high-quality representations in various downstream tasks. Meanwhile, with the prevalence of the pretrain-and-finetune paradigm, users with limited computational resources can download the open-source pre-trained VLMs and finetune them on smaller, private datasets, balancing efficiency and performance.


Despite the significant success, they have been shown to be vulnerable to threatening backdoor attacks \cite{liang2024badclip, bai2024badclip}. By injecting specific triggers into a small subset of training data, attackers can poison the training process to implant convert backdoors into the victim model. The poisoned model performs normally on benign inputs but exhibits attacker-specific behaviors once the input is embedded with the trigger. When VLMs are deployed in real-world applications, especially in security-related scenarios such as autonomous driving \cite{eykholt2018robust, zhang2022towards}, financial systems \cite{sarkar2018robust} and facial recognition \cite{xue2021backdoors, le2024comprehensive}, the presence of backdoors becomes a critical concern due to their potential risks and high invisibility. Worse still, the pretrain-and-finetune paradigm may accelerate the proliferation of backdoors: once a pre-trained, open-source VLM is poisoned, the fine-tuned models are likely to inherit the backdoor due to its stubbornness.

To address the severe backdoor threat in VLMs, several defenses have been proposed to purify the malicious injected backdoor. The mainstream approaches, such as FT and CleanCLIP \cite{bansal2023cleanclip}, typically fine-tune the whole suspicious model with an auxiliary clean dataset. Unfortunately, these methods provide only limited resistance to state-of-the-art attacks, \eg, TrojVQA \cite{walmer2022dual} and BadCLIP \cite{liang2024badclip}. A possible reason for their failure is that the massive parameters in VLMs facilitate the establishment of strong connections between the triggers and the pre-defined outputs, which brings great challenges to unlearning these robust backdoors. Also, directly fine-tuning the entire VLM exhibits less effectiveness since the substantial model parameters increase the difficulty of reaching a consistent and stable optimization direction.
Another disadvantage is that these methods simply utilize a benign loss to fine-tune the poisoned model, totally overlooking the incorporation of information from the attacker-adopted triggers, which further leads to an indirect and less effective purifying direction.
Building on these insights, we propose Class-wise Backdoor Prompt Tuning (CBPT), an efficient prompt tuning method based on bi-level optimization to learn class-wise, backdoor-resistant text prompts for CLIP while keeping the model weights frozen, as illustrated in Fig. \ref{fig:intro}. Inspired by the observation that adversarial noise often shares strong similarities with backdoor triggers due to the existence of backdoor shortcuts \cite{wang2020practical, zhu2024neural}, we optimize adversarial perturbations towards the simulated backdoor class and then leverage the learned adversarial perturbations as dummy triggers with similar functionality to those adopted by attackers.   
Additionally, we employ the powerful contrastive learning technique to achieve effective trigger inversion, i.e., generate perturbations that push the adversarial embeddings away from the original ones while drawing them closer to embeddings of carefully crafted positive samples from the target class. Guided by the simulated triggers, we then optimize the class-wise text prompts using clean samples and poisoned samples injected with the dummy trigger, to strengthen VLM's backdoor robustness and preserve the benign utility. 

We conduct extensive experiments to benchmark CBPT against seven backdoor attacks and two defenses. The significant reduction in attack success rates (ASR) combined with the preservation of clean accuracy (CA), demonstrates the superiority of our proposed defense over baseline methods. To assess the generality of our approach, we also consider cross-domain scenarios where the fine-tuning dataset has a domain shift with the test data.
Additionally, we perform prompt tuning across 1-, 2-, 4-, 8-, 16-shot learning and 1-, 2-, 4-, 8-, and 16-token learning, highlighting the data efficiency as well as parameter efficiency of our method. We further investigate three context positions to assess their impact on defense effectiveness.

In summary, our main \textbf{contributions} are as follows:
\begin{itemize}
    \item We propose CBPT, a novel class-wise prompt tuning method that inverts potential backdoor triggers and optimizes class-wise prompts to boost backdoor robustness. 
    \item We incorporate contrastive learning with meticulously constructed positive and negative visual samples to achieve effective and efficient trigger inversion.
    \item Extensive experiments demonstrate that our method effectively purifies SOTA backdoor attacks, including both unimodal and multimodal attacks on CLIP models, outperforming SOTA defenses by a substantial margin.
\end{itemize}