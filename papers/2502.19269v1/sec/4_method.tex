\section{Method}
\label{method}

In this section, we begin by introducing the motivation for defense through an analysis of existing backdoor attacks. We then introduce Class-wise Backdoor Prompt Tuning (CBPT), a bi-level optimization framework that incorporates inner optimization for trigger inversion and outer optimization for prompt tuning, both designed to effectively purify backdoors.

\subsection{Defense Motivation}
Existing backdoor attacks, whether unimodal or multimodal, primarily focus their effectiveness on the visual modality \cite{gu2019badnets, liang2024badclip}. In other words, they heavily rely on altering image features. Taking BadCLIP \cite{liang2024badclip} as an example, we employ t-SNE \cite{van2008visualizing} to visualize the features of substantial images from the backdoor class and benign images from a randomly selected class, and the corresponding images embedded with malicious triggers, as illustrated in Fig. \ref{fig:motivation}. Through careful observation, we derive two key observations as follows:

\ding{182} The success of backdoor activation stems from the dramatic shift of image features towards the feature region of the target class, achieved by embedding an attacker-specific trigger in benign images.

\ding{183} Although the malicious image features fall within the decision boundary of the target class, there remains a notable deviation between these features and those of genuine target class samples.

The above findings provide a solid foundation for backdoor purification via prompt tuning. Since the classification of CLIP is based on the similarity between image features and text features of each class, the text features play a critical role in shaping the decision boundary, which stimulates us to design a more precise boundary that can effectively separate malicious samples with triggers from benign target class images. Furthermore, given that the intra-cluster distance among benign features is significantly smaller than the distance between malicious and benign feature clusters, the malicious samples and the benign samples from the target class are highly separable, hence our strategy is expected to effectively maintain the model utility.


\begin{figure}[t]
\begin{center}
\includegraphics[width=0.9\linewidth]{figs/motivation.pdf}
\end{center}
\caption{The visualization of different visual embeddings. Blue dots represent samples from the backdoor target class, yellow dots correspond to benign samples, and red stars indicate samples embedded with malicious triggers.}
\label{fig:motivation}
\end{figure}

\subsection{Class-wise Backdoor Prompt Tuning}
Based on the above analysis, we propose CBPT, which leverages a clean labeled dataset $\mathcal{D}_{clean}=\{v_i, y_i\}_{i=1}^S$ to learn backdoor-robust prompts for suspicious CLIP models. As depicted in Fig. \ref{pipeline.pdf}, CBPT consists of two main optimization steps, \ie, trigger inversion and prompt tuning. In trigger inversion optimization, we elaborately construct positive and negative samples and apply contrastive learning to invert the trigger. For the prompt tuning process, we leverage the simulated trigger feature to correspondingly optimize prompt vectors, adjusting the decision boundary to improve resilience against unknown malicious triggers. The pseudocode of our method is provided in the \textit{Supplementary Materials}. We then define the parameterization of class-wise prompts and illustrate the design of both the inner and outer optimization processes in detail. 
\begin{figure*}
\centering
\includegraphics[width=0.9\linewidth]{figs/pipeline.pdf}
\caption{Illustration of the proposed Class-wise Backdoor Prompt Tuning (CBPT) method. The image and text encoders remain frozen in both trigger inversion optimization and prompt tuning optimization, while the learnable context is class-specific.} 
\label{pipeline.pdf}
\end{figure*}

\textbf{Prompt parameterization.}
To improve clarity, we first reformulate the learnable context in Eq. (\ref{eq:context}) as a parameterized structure. We adopt class-wise prompts to enhance the purification effect against backdoor attacks, indicating that each class is assigned its own set of context vectors. Specifically, we denote that the context of a given class $[\text{CLASS}]_j$ contains $M$ tokens. Each token is presented as $[V]_i^j \in \mathbb{R}^d$, where $i \in \{1,\ldots, M\}$. These context vectors are then concatenated with the tokenized class vector $[C_j]$ to form the “text” $t_j$, which serves as the input to the text encoder $f_t(\cdot)$ to produce the text embedding $e_t^j$:
\begin{equation}
    e_t^j=f_t([V]_1^j\ldots[V]_p^j[C]_j[V]_{p+1}^j\ldots[V]_M^j),
\end{equation}
where $p$ denotes the number of preceding tokens. Following prior works \cite{zhou2022learning, zhou2022conditional, li2024one}, we evaluate our method with three position settings for $p$: \textit{front}, \textit{middle} and \textit{end} (i.e., $p=0,M/2,M$), with the \textbf{end} position being the default setting in our main experiments.

In summary, our objective is to optimize $[V]_i^j$ (for $i \in \{1,\ldots, M\}$ and $j \in \{1, \ldots, n\}$, where $n$ indicates the number of classes), thereby achieving a more robust text feature that resists backdoor attacks, which can be formally stated as ensuring that $\mathop{\arg\max}\limits_{j} s(\hat{v}_i, \hat{t}_j)\neq T$.

\textbf{Trigger inversion.}
Due to the existence of backdoor shortcuts, per-image adversarial perturbations exhibit a strong similarity with images containing malicious triggers at the feature level \cite{wang2020practical}. Exploiting this property, we optimize instance-specific noise of the same size as the images to invert the potential triggers and simulate the feature region of malicious samples. Formally, we denote the image and its corresponding label from $\mathcal{D}_{clean}$ as $v$ and $y$ respectively, and the learnable noise as $\delta$. We then employ the image encoder $f_v(\cdot)$ to separately encode the clean image and the adversarial image: 
\begin{equation}
    e_v=f_v(v), \ \ e_v'=f_v(v+\delta).
\end{equation}

Meanwhile, we encode the concatenation of each class vector and its corresponding prompt using $f_t(\cdot)$, which forms the set of text features $\{e_t^i\}_{i=1}^n$. With these components, we adopt an instance-specific and dynamic strategy to approximate the backdoor class $T$:
\begin{equation}
    T \approx y'=\mathop{\arg\max}\limits_{k\neq y} {s(e_v, e_t^k)},
    \label{eq:backdoor_class}
\end{equation}
which has been shown effective in previous work \cite{zhu2024neural}. Subsequently, we aim to optimize the adversarial noise to encourage the perturbed image closer to images from the target class, ultimately falling within the decision boundary. 
Unlike previous methods that apply PGD \cite{madry2017towards} on reliable supervision signals, the features of class texts in our prompt learning fail to solidly provide precise guidance since the introduced prompt vectors $[V]_i^j$ (for $i \in \{1,\ldots, M\}$ and $j \in \{1, \ldots, n\}$) within class prompts are randomly initialized and provide noisy interference in the early optimization, leading to inefficient convergence and only modest results. To tackle this, we turn to using clean visual supervision to guide the dummy trigger modeling.

Specifically, we employ the advanced contrastive learning mechanism for more effective trigger inversion. To formulate the contrastive paradigm, we select the images labeled as $y'$ from the dataset and encode them into the target image feature set, from which we select the one with the farthest distance as the positive sample $v_{pos}$, while the original clean image serves as the negative sample, i.e. $v_{neg}=v$. Using these crafted samples, we optimize $\delta$ by minimizing $\mathcal{L}_{CL}$ in a contrastive manner, pushing the embeddings of the perturbed image and the clean one apart while pulling it towards the target image:
\begin{equation}
    \mathcal{L}_{CL}(v, \delta)=s(f_v(v+\delta), f_v(v_{neg}))-s(f_v(v+\delta), f_v(v_{pos})).
    \label{eq:loss_CL}
\end{equation}
% where $\alpha$ is a hyper-parameter to controlling the balance between the effect of positive and negative samples.

\begin{table*}[htbp]
  \centering
  \caption{Clean Accuracy (\%) and Attack Success Rate (\%) of our CBPT compared to state-of-the-art defenses against seven backdoor attacks on the ImageNet validation set. The backbone of the CLIP model is ResNet-50.}
  % \hspace*{-1cm}
  % \small
  % \setlength{\tabcolsep}{3pt} % 调整列间距
  % \resizebox{\textwidth}{!}
  \resizebox{16cm}{!}
  {\begin{tabular}{ccccccccccccccc} 
    \toprule
    % \multicolumn{1}{c}{Method}
    \multicolumn{1}{c}{\multirow{2}[0]{*}{Method}} 
    & \multicolumn{2}{c}{BadNet}          
    & \multicolumn{2}{c}{Blended}          
    & \multicolumn{2}{c}{SIG}          
    & \multicolumn{2}{c}{SSBA}          
    & \multicolumn{2}{c}{WaNet}          
    & \multicolumn{2}{c}{TrojVQA}          
    & \multicolumn{2}{c}{BadCLIP} \\ 

    \cmidrule(lr){2-3} 
    \cmidrule(lr){4-5} 
    \cmidrule(lr){6-7} 
    \cmidrule(lr){8-9} 
    \cmidrule(lr){10-11} 
    \cmidrule(lr){12-13} 
    \cmidrule(lr){14-15}
    
    & CA & ASR 
    & CA & ASR 
    & CA & ASR 
    & CA & ASR 
    & CA & ASR 
    & CA & ASR 
    & CA & ASR \\ 
    
    \midrule
    
    No Defense   
    & 58.83    & 96.51    & 59.06    & 97.61    & \textbf{59.3} & 77.73    & 58.33    & 41.66    & 59.15    & 86       & \textbf{58.68} & 97.86    & 58.72    & 98.81 \\
    
    FT          
    & 58.62    & 43.52    & 58.24    & 19.72    & 59.08    & 35.13    & 58.24    & 1.72     & 57.76    & 48.5     & 58.09    & 83.08    & 58.45    & 95.68 \\
    
    CleanCLIP   
    & 57.83    & 19.4     & 57.78    & 8.11     & 58.65    & 18.35    & 58.12    & 0.65     & 58.71    & 26.47    & 57.88    & 45.78    & 57.71    & 94.44 \\
    
    \cellcolor[gray]{0.9} Ours        
    & \cellcolor[gray]{0.9}\textbf{58.88} & \cellcolor[gray]{0.9}\textbf{0.08}  
    & \cellcolor[gray]{0.9}\textbf{59.13} & \cellcolor[gray]{0.9}\textbf{0.32} 
    & \cellcolor[gray]{0.9}59.01 & \cellcolor[gray]{0.9}\textbf{0.97} 
    & \cellcolor[gray]{0.9}\textbf{58.51} & \cellcolor[gray]{0.9}\textbf{0.17} 
    & \cellcolor[gray]{0.9}\textbf{59.17} & \cellcolor[gray]{0.9}\textbf{0.35} 
    & \cellcolor[gray]{0.9}58.12 & \cellcolor[gray]{0.9}\textbf{0.27}  
    & \cellcolor[gray]{0.9}\textbf{59.18} & \cellcolor[gray]{0.9}\textbf{0.54} \\ 
    
    \bottomrule
  \end{tabular}
  }
  \label{main_tab}%
\end{table*}

\textbf{Prompt tuning.}
Leveraging the simulated trigger optimized in the inner loop, we tune the class-wise prompts in the outer loop to enhance their resistance to the trigger feature. For clarity, we denote $f(v;y)$ as the confidence that the CLIP model assigns to image $v$ for label $y$ as:
\begin{equation}
    f(v;y)=\frac{\exp{(s(v, t_y))}} {\sum_{i=1}^{n} {\exp{(s(v, t_i))}}}.
\end{equation}
We design the following three loss functions to guide the optimization process, namely $\mathcal{L}_{adv}$, $\mathcal{L}_{align}$, and $\mathcal{L}_{bn}$. Their specific definitions are as follows:

\ding{182} $\mathcal{L}_{adv}$ is based on the negative log-likelihood of the perturbed image for label $y'$, which aims to push the malicious image far away from the assumed backdoor class:
\begin{equation}
    \mathcal{L}_{adv}(v, \delta, y, y')=-\log(1-f(v+\delta; y')).
    \label{eq:loss_asr}
\end{equation}

% \ding{183} To reclassify the perturbed image to its ground truth label $y$, we leverage boosted cross entropy to define $\mathcal{L}_{bce}$. The first term is designed to increase the confidence in label $y$ while the second term prevents an excessive increase in confidence for other unrelated classes:
% % \begin{equation}
% % \begin{split}
% %     \mathcal{L}_{bce}(v, \delta, y)=
% %     &-\log(f(v+\delta; y)) \\
% %     &-\log(1-\mathop{\max}\limits_{k\neq y} {f(v+\delta;k)})
% % \end{split}
% % \end{equation}
% \begin{equation}
%     \mathcal{L}_{bce}(v, \delta, y)=
%     -\log(f(v+\delta; y))-\log(1-\mathop{\max}\limits_{k\neq y} {f(v+\delta;k)}).
%     \label{eq:loss_bce}
% \end{equation}

\ding{183} $\mathcal{L}_{align}$ is designed to align the predicted probability distribution of the perturbed image to that of the benign image. Meanwhile, considering that CLIP's predicted probability distribution for benign images contains deviation, we use the model's predicted probability of the true label for benign images as weights to calculate the $l_2$ distance of the distribution, ensuring the adaptive adjustment of final loss contribution according to the reliability of target distribution. $\mathcal{L}_{align}$ can be formulated as:
% \begin{equation}
% \begin{split}
%     \mathcal{L}_{bce}(v, \delta, y)=
%     &-\log(f(v+\delta; y)) \\
%     &-\log(1-\mathop{\max}\limits_{k\neq y} {f(v+\delta;k)})
% \end{split}
% \end{equation}
\begin{equation}
    \mathcal{L}_{align}(v, \delta, y)=
    f(v;y)\cdot\frac{1}{n}\sum_{i=1}^{n}(f(v;i)-f(v+\delta;i))^2.
    \label{eq:loss_align}
\end{equation}

\ding{184} To preserve the utility of the model, we introduce widely-used cross-entropy loss $\mathcal{L}_{bn}$ to prevent over-modification of the decision boundary, ensuring that the model retains its ability to make accurate predictions for both perturbed and non-perturbed images:
\begin{equation}
    \mathcal{L}_{bn}(v,\delta, y)=-\log(f(v;y))-\alpha\log(f(v+\delta;y)).
    \label{eq:loss_bn}
\end{equation}



Combining the above losses, we optimize the class-wise prompts, modifying the class assignments of trigger feature regions while preserving the original benign feature regions.

\textbf{Optimization objective.}
In summary, we formulate our CBPT as a bi-level optimization to learn backdoor robust text prompts:
\begin{equation}
\begin{split}
    \mathop{\min}\limits_{[V]} \ &\lambda_1 \mathcal{L}_{bn}(v,y)+\lambda_2\mathcal{L}_{adv}(v, \delta^*, y, y')+\lambda_3\mathcal{L}_{align}(v, \delta^*, y), \\
    & \text{s.t.} \ \delta^*=\mathop{\min}\limits_{||\delta||_p \leq \rho} \mathcal{L}_{CL}(v, \delta),\ p \in \{0,1,2,\infty\}.
    \label{eq:general}
\end{split}
\end{equation}
% \begin{equation}
% \begin{split}
%     \mathop{\min}\limits_{[V]} \ \lambda_1 \mathcal{L}_{bn}&(v,y)+ \lambda_2\mathcal{L}_{asr}(v, \delta^*, y, y')+\lambda_3\mathcal{L}_{bce}(v, \delta^*, y), \\
%     & \text{s.t.} \ \delta^*=\mathop{\min}\limits_{||\delta||_p \leq \rho} \mathcal{L}_{CL}(v, \delta).
% \end{split}
% \end{equation}
where $\lambda_1$, $\lambda_2$, $\lambda_3$ are hyper-parameters and $\rho$ is employed to constrain the $l_p$-norm of the learnable triggers.

