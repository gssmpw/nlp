\section{Related Work}
\label{sec:related work}

\subsection{Backdoor Attacks and Defences}
\textbf{Backdoor attacks} occur during the training phase when a small subset of training samples is poisoned, thereby implanting a concealed backdoor into the model. While the model maintains normal performance on benign inputs, it exhibits attacker-specific behaviors once the backdoor is activated by a specific trigger. Since the pioneer work \cite{gu2019badnets} highlights the backdoor vulnerability of CNNs, numerous works \cite{chen2017targeted, barni2019new, li2021invisible, nguyen2021wanet} have attempted to improve the attack success rate or increase trigger stealth. For instance, Blended \cite{chen2017targeted} employes weighted addition to fuse the image and the trigger, while SIG \cite{barni2019new} corrupts samples of the target class and achieves the attack without label poisoning, significantly enhancing imperceptibility. In the domain of VLMs, TrojVQA \cite{walmer2022dual} introduces the trigger in each of the input modalities, activating the attack only when both triggers are presented. BadCLIP \cite{liang2024badclip} utilizes dual-embedding-guided trigger optimization to align the poisoned samples with target class features, achieving state-of-the-art attack performance. The success of these attacks underscores the urgent need for a robust and comprehensive defense.

\textbf{Backdoor defenses} aim to mitigate the potential backdoor threats, which can be divided into in-training defenses and post-processing defenses based on their operational phase. In-training defenses, leveraging the access to the training dataset,  detect suspicious samples via transformation sensitivity \cite{chen2022effective} or feature clustering \cite{huang2022backdoor}. Post-processing defenses primarily focus on techniques such as trigger reversion \cite{wang2019neural, chen2019deepinspect}, model pruning \cite{liu2018fine, wu2021adversarial, li2023reconstructive} and model fine-tuning \cite{li2021neural, zeng2021adversarial}. To protect VLMs, CleanCLIP \cite{bansal2023cleanclip} utilizes a clean dataset to fine-tune the model with a combination of multimodal contrastive loss and unimodal self-supervised loss, while RoCLIP \cite{yang2024robust} constructs a large caption pool to match images with the most similar captions. However, current methods exhibit limited resistance to advanced attacks and often degrade performance on clean data. 
% To address these limitations, we propose to use prompt tuning, a powerful tool in VLMs, to effectively purify backdoors while largely preserving the model utility.  
To address these limitations, we propose leveraging prompt tuning, a highly effective technique within VLMs, to efficiently purify backdoors while preserving the model utility to a significant extent.
\subsection{Prompt Tuning}
Prompt tuning originated in Natural Language Processing, where prompts serve as model-familiar input formats. Instead of modifying internal weights, prompt tuning adapts the model to specific downstream tasks through prompt optimization, significantly reducing computational costs. CoOp \cite{zhou2022learning} pioneers the application of prompt tuning in VLMs, especially adapting CLIP for image recognition. Building on this, CoCoOp \cite{zhou2022conditional} introduces input-specific prompts, improving generation for unseen classes through a lightweight, learnable neural network. Moreover, prompt tuning has also been extended to text-to-image synthesis \cite{tao2023galip}, semantic segmentation \cite{zhou2023zegclip}, and adversarial robustness \cite{li2024one}. However, its application for backdoor purification remains unexplored. In this paper, we fill this gap by investigating how prompt tuning can improve robustness against backdoor attacks.
