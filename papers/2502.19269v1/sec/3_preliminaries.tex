\section{Preliminaries}
\label{preliminariy}

\subsection{Review of CLIP}
CLIP-like VLMs aim to map multimodal inputs into a highly aligned feature space and consist of an image encoder $f_v(\cdot)$ and a text encoder $f_t(\cdot)$ with parameters denoted as $\boldsymbol{\theta}_v$ and $\boldsymbol{\theta}_t$ respectively. For an input image-text pair $(v, t)$, the model encodes the input into the embeddings as:
\begin{equation}
    e_v=f_v(v;\boldsymbol{\theta}_v),\ \ e_t=f_t(t;\boldsymbol{\theta}_t).
\end{equation}
The correlation between an image-text pair can be quantified by the cosine similarity of their respective embeddings:
\begin{equation}
    s(v,t)=\cos(e_v, e_t).
\end{equation}

During training, CLIP bridges different modalities through contrastive learning. Given a batch of image-text pairs $\{v_i, t_i\}_{i=1}^{N}$, CLIP maximizes the similarity between matching pairs while minimizing that for unrelated pairs:
\begin{equation}
\begin{split}
\mathop{\arg\min}\limits_{\boldsymbol{\theta}_v, \boldsymbol{\theta}_t} &-\frac{1}{2N}\sum_{i=1}^{N} \log \left( \frac{\exp(s(v_i, t_i)/\tau)}{\sum_{j=1}^{N} \exp(s(v_i, t_j)/\tau)} \right) \\
&-\frac{1}{2N}\sum_{i=1}^{N} \log \left( \frac{\exp(s(v_i, t_i)/\tau)}{\sum_{j=1}^{N} \exp(s(v_j, t_i)/\tau)} \right). 
\end{split}
\end{equation}
Due to its pre-training on large-scale datasets crawled from the web, CLIP demonstrates remarkable generalization capabilities and can be effectively applied to various downstream tasks. For instance, in zero-shot image classification, CLIP can calculate the similarity between the image embedding and the text embeddings of all target labels, predicting the label with the highest similarity as the output:
\begin{equation}
\mathop{\arg\max}\limits_{j} s(v, t_j).
\end{equation}
In practical applications, handcrafted templates such as “\texttt{a photo of a [CLASS]}” often serve as model-friendly formats to enhance adaptability and stability \cite{huang2023sentence, radford2021learning}. Here, “\texttt{a photo of a}” functions as contextual information, providing supportive information for decision-making. However, handcrafted prompts typically require expert experience and are restricted to discrete words. As an emerging technique, prompt tuning overcomes these limitations by optimizing prompt vectors in the continuous space, leading to significantly improved performance \cite{zhou2022learning}. Formally, assuming the context contains $p$ preceding tokens and $M-p$ following tokens, the text feature encoded by CLIP can be represented as:
\begin{equation}
    e_t=f_t(concat([V_{front}, tokenize([\operatorname{CLASS}]), V_{end}])),
    \label{eq:context}
\end{equation}
where $V_{front} \in \mathbb{R}^{p \times d}$, $V_{end} \in \mathbb{R}^{(M-p) \times d}$ are learnable parameters, and $d$ denotes the dimension of token vector.



\subsection{Threat Model}
To implant backdoors in VLMs, the attacker specifies a backdoor class, denoted as $T$, and samples a subset $\mathcal{D}_{sub}=\{v_i, t_i\}_{i=1}^{m}$ from the training set $\mathcal{D}_{train}$, which is subsequently embedded with attacker-specific triggers:
\begin{equation}
    \hat{v}_i=v_i \oplus \Delta_v, \ \ \hat{t}_i=t_i \oplus \Delta_t,
\end{equation}
where $\Delta_v$ and $\Delta_t$ denote the visual and text triggers respectively, $\oplus$ represents the fusion operation. The poisoned samples $\mathcal{D}_{poi}=\{\hat{v}_i, \hat{t}_i\}_{i=1}^{m}$, combined with the remaining benign samples, form the modified training set: $\mathcal{D}_{train}'=\mathcal{D}_{poi}\cup(\mathcal{D}_{train}-\mathcal{D}_{sub})$. Trained on this malicious dataset, the model behaves normally with benign inputs, but exhibits harmful behavior once the triggers appear:
\begin{equation}
    \mathop{\arg\max}\limits_{j} s(v_i, t_j)=i, \ \ \mathop{\arg\max}\limits_{j} s(\hat{v}_i, \hat{t}_j)=T.
\end{equation}

