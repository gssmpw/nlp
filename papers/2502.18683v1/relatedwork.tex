\section{Related work}
In this section, we detail mind perception and moral attribution theories and how each has been applied to HCI, and we formulate our study hypotheses.

\subsection{Mind perception of AI}

\citet{gray07} popularized the psychological theory of mind perception with a survey that showed participants images and brief text descriptions of 13 entities such as a baby, a robot, and God, and asked about a range of mental faculties (e.g., “capable of feeling afraid or fearful”). They found that people perceive minds along two dimensions: agency, the capacity to plan and act, and experience, the capacity to sense and feel. While other frameworks for mind perception have been proposed, such as one-dimensional \cite{tzelios22}, three-dimensional \cite{weisman17}, and five-dimensional \cite{malle19a} models, this two-dimensional structure has been replicated in the context of robots \cite{kamide13}, and its dimensions have been shown to affect important HCI outcomes, such as the willingness to interact with a robot \cite{stafford14}, the perceived helpfulness of a chatbot \cite{lee24}, and the “uncanny valley,” the feeling of unease when robots seem too human-like \cite{gray12a}.

In their initial work, \citet{gray07} found that a social robot was perceived as having a moderate degree of agency (between a young chimpanzee and a five-year old child) and a very low degree of experience (approximately that of a dead person). \citet{jacobs22} showed participants images and brief text descriptions of eight real-world AIs (Alexa, Atlas, Beam, Maslo, Roomba, Siri, Sophia, and Sphero) and a range of non-AI entities such as bacteria and chimpanzees and tested perceptions of their agency and experience. In their study, the vacuum cleaner Roomba and the video telepresence robot Beam were perceived as having the least experience and agency, respectively, while the human-like Sophia and Atlas were perceived as having the most experience and agency. Each AI was perceived as having more agency, but less experience, than a human fetus, and as having less agency than a frog. \citet{hwang22} found similar results in a study of 36 U.S. university students who also drew pictures of Alexa, “AI,” a computer, Google Assistant, a robot, “self,” and Siri. Each AI was rated as having more experience and more agency than the computer but less than “self.”

As technological developments in the field of AI have accelerated, particularly with the advent of LLMs \cite{vaswani17}, researchers have begun to test mind perception in more advanced AI. \citet{scott23} showed a sample of online crowdworkers short videos of GPT-3, Alexa, and a robot vacuum cleaner, and they found people perceived each system to have some degree of consciousness. In a U.S. nationally representative survey, \citet{anthis24a} found that 10\% of people believed ChatGPT is sentient (which they defined to participants as “the capacity to have positive and negative experiences”) and 20\% believed that at least some currently existing AIs are sentient. Finally, \citet{jacobs23} found that people rated ChatGPT as higher in agency than experience and that ratings for both dimensions increased after being shown three brief prompts and ChatGPT’s responses.

Overall, the literature suggests that people perceive some degree of mind in AIs—but less experience than agency—and that there is more variation in perceived agency than experience. However, these studies tested relatively few types of AIs with limited comparison to non-AI entities, and many studies were carried out before LLMs and other highly capable modern AIs.

\subsection{Moral attributions to AI}

Mental agency and experience are correlated with two analogous moral faculties: moral agency, the capacity to do right and wrong, and moral patiency, the capacity to be treated rightly or wrongly \cite{gray07}. \citet{gray12} and \citet{gray12b} argued that perceptions of mind and morality are closely related: Perceived agency is the primary determinant of attributions of moral agency, and perceived experience is the primary determinant of attributions of moral patiency. Studies have used a variety of measures to operationalize moral agency and patiency \cite{ladak23a}, including specific scales developed in the context of robotics \cite{banks19, banks23}. Attributions of moral agency and patiency are associated with a range of outcomes relevant to HCI, such as trust \cite{wester24}, willingness to engage \cite{banks19}, and cooperation \cite{bonnefon24}.

Reflecting the moderate degree of perceived mental agency in AI, moral agency is also perceived to a moderate degree. \citet{shank18} found that people attributed moral wrongness and moral responsibility to AIs for committing moral violations such as racist parole decisions. \citet{stuart21} tested attributions of blame to a human, corporation, and robot for each deciding to use a fertilizer that causes groundwater pollution; the robot was attributed some degree of blame, though less than the human or corporation making the same decision. \citet{monroe14} tested how much people blamed five entities for a range of norm violations. They found that a “normal human” and “cyborg” received the most blame, followed by an “AI in a human body,” “akratic human,” and “advanced robot.” The advanced robot was still attributed a degree of blame close to the mid-point on their scale.

Reflecting low attributions of experience, people attribute AIs low moral patiency. \citet{pauketat22} found that people place “artificial intelligence” and “robots” at the fringes of their “moral circle” (i.e., the boundary around moral patients), further out from the middle than apple trees and murderers. Similarly, \citet{rottman21} found that people placed each of a “supercomputer,” an “intelligent robot,” and a “self-driving car” at the fringes of their moral circles. \citet{lima20} tested support for 11 rights for AIs and found that most people only weakly supported granting them the right to protection against cruelty and did not support any of the other rights. \citet{anthis24a} tested moral concern for 11 different types of AIs: digital copies of human and animal brains; human-, animal-, and machine-like robots; and various virtual AIs without physical bodies. They found that the most moral concern was granted to the digital human brains and human-like robots, followed by digital animal brains and animal-like robots, and the least concern was given to AI video game characters. While their study compared a range of generically defined entities on one measure of moral patiency, there may be important differences across specific real-world entities (e.g., ChatGPT, Alexa), differences across measures of moral patiency, and associations with other factors (e.g., mind perception) that remain untested.

\subsection{The relationship between mind and morality in AI}

Several studies have analyzed the relationship between perceived mind and morality in the context of AI. In terms of moral agency, people attribute more moral responsibility to robots for their decisions in moral dilemmas when they are perceived as having more agency or more experience \cite{nijssen22}; they hold more autonomous AI crime prediction systems more responsible \cite{hong19}; and they perceive more anthropomorphic robots to have more agentic mental faculties (e.g., language, thought), which results in a higher willingness to punish the robots \cite{yam22}. In terms of moral patiency, social robots are perceived to have more experience than economic robots and are in turn attributed more moral patiency \cite{stojilovic24, zhang21a, wang18}; robots with the capacity for experience are less likely to be sacrificed in moral dilemmas \cite{nijssen19} and are more likely to be granted rights \cite{tanibe17}; and in a study testing the effects of 11 design features, emotion expression, an external indicator of experience, was one of the strongest predictors of attributing moral patiency \cite{ladak24}. These studies show that the relationship between mind and morality predicted by mind perception theory \cite{gray12, gray12b} holds for generically described robots or “AI” in the abstract, but it is unclear how they translate to the context of real-world AIs, particularly to the latest generation of advanced AI.

\subsection{Hypotheses}

The preceding studies motivate several hypotheses. First, given the increasing diversity of AI systems along dimensions that are known to affect perceptions of mind and morality, such as anthropomorphism, autonomy, and emotion expression, we expect there will be significant differences between AIs. For example, it is plausible that an emotionally expressive chatbot would be attributed more experience and moral patiency than one that is not emotionally expressive. Second, we expect perceptions of AIs to be varied enough such that they will be rated comparably to different non-AIs, such as some AIs perceived as having a degree of agency close to inanimate objects (e.g., a rock) and others comparable to some nonhuman animals (e.g., an ant). For example, it is plausible that an autonomous vehicle designed to make complex decisions in real-time would be attributed a degree of mental and moral agency comparable to a nonhuman animal, whereas a less autonomous robot vacuum cleaner would be rated more similarly to inanimate objects. Third, we expect that perceived mind will be associated with morality in AIs, and we will test how this relationship manifests in the context of the diverse range of real-world AIs included in our study. Formally, we test the following three hypotheses:

\begin{itemize}
    \item \textbf{H1}: There will be differences between AIs in their degree of perceived (a) agency, (b) experience, (c) moral agency, and (d) moral patiency.
    \item \textbf{H2}: There will be differences between AIs and non-AI entities in their degree of perceived (a) agency, (b) experience, (c) moral agency, and (d) moral patiency.
    \item \textbf{H3}: Perceived mind and morality will be related within AIs such that (a) agency will be positively associated with moral agency, and (b) experience will be positively associated with moral patiency.
\end{itemize}