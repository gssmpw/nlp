\section{Algorithm}\label{sec:algo}
In this section, we begin by briefly reviewing the self-play algorithm with online mirror descent (OMD) updates, which is used in previous general preference alignment algorithm~\citep{zhang2024iterative}. Next, we present our proposed algorithm, which leverages the faster convergence properties of optimistic OMD, inspired by advancements in game theory~\citep{rakhlin2013optimization,syrgkanis2015fast}. Through theoretical analysis, we show that our approach achieves an improved bound on the duality gap. Finally, we describe the implementation of our algorithm. Following \citet{azar2024general,zhang2024iterative}, we omit the context $x$ throughout the rest of the paper since each context is independent.

\subsection{Self-play Algorithm with OMD Update}\label{sec:inpo}
Self-play algorithms are widely used in approximating the Nash policy~\citep{bai2020near,liu2021sharp}. The key idea is to let the policy play against itself, enabling iterative self-improvement. The algorithm is performed in an online manner, with each iteration using online mirror descent (OMD) to update the policy. Specifically, at iteration $t$, we find the policy that maximizes the following objective:
\begin{align}\label{eq:single_update}
\pi_{t+1}=\argmax_{\pi} \inner{\pi,r_t}-\frac{1}{\eta}\KL(\pi \Vert \pi_t),
\end{align}
where $r_t(y)=\mathbb{P}(y \succ \pi_t)=\E_{y' \sim \pi_t}[\mathbb{P}(y \succ y')]$ is the expected win rate of response $y$ against the current policy $\pi_t$, and $\eta>0$ is the learning rate. This objective ensures that $\pi_{t+1}$ not only aims to maximize the win rate over $\pi_t$ but also remains close to $\pi_t$, as measured by the KL divergence term. The stability introduced by the KL regularization is critical for achieving a sublinear regret bound. Without this regularization, one can construct examples where the algorithm suffers from linear regret, which is undesirable~\citep{lattimore2020bandit}.

% We next consider the multi-turn scenario where the horizon $H > 1$. In this setting, preferences are only provided for the final states, and there is no direct feedback for intermediate states. To address this, we use Q-value functions, which capture the long-term expected outcomes, in the optimization objective. For each state $s_h$, the update rule at iteration $t$ is as follows:\footnote{The objectives in \citet{shani2024multi,zhang2024iterative} include a log density ratio between $\pi_t$ and a reference policy $\pi_{\mathrm{ref}}$. This is because their game objective incorporates a KL divergence term between the player's policy and $\pi_{\mathrm{ref}}$, whereas we directly use the win rate between the two players as the objective. Detailed discussion is deferred to Section~\ref{sec:discuss}.}
% \begin{align}
% &~\pi_{t+1}(\cdot \mid s_h) \nonumber \\
% =&~\argmax_{\pi} \inner{\pi,Q^{\pi_t,\pi_t}}(s_h) - \frac{1}{\eta}\KL(\pi(\cdot \mid s_h) \Vert \pi_t(\cdot \mid s_h)), \label{eq:omd_multi_update}
% \end{align}
% where $\inner{\pi,Q}(s_h)$ represents $\inner{\pi(\cdot \mid s_h),Q(s_h,\cdot)}$. Here $Q^{\pi_t,\pi_t}$ serves the similar role as $r_t$ in Eq.~\eqref{eq:single_update} and $\inner{\pi,Q^{\pi_t,\pi_t}}(s_h)$ measures the probability of $\pi$ outperforming $\pi_t$ under state $s_h$.

Similar to the analysis in \citet{zhang2024iterative}, we can show that the uniform mixture of $\pi_{1:T}$ achieves an $\Ocal(T^{-1/2})$ duality gap, as stated in the following theorem. The proof is deferred to Appendix~\ref{sec:proof_omd}.
\begin{theorem}\label{thm:omd_guarantee}
Let $D=\max_{\pi} \KL(\pi \Vert \pi_1)$ and $\bar \pi=\frac{1}{T}\sum_{t=1}^T \pi_t$. Self-play algorithm in Eq.~\eqref{eq:single_update} with $\eta=\sqrt{\frac{D}{T}}$ satisfies:
\begin{align*}
\mathrm{DualGap}(\bar \pi) \le \frac{4\sqrt{D}}{\sqrt{T}}.
\end{align*}
\end{theorem}
\citet{zhang2024iterative} also demonstrate that self-play with OMD achieves last-iterate convergence. This result is attributed to the strong convexity induced by the KL regularization terms in their game objectives. However, since our objective does not include these KL terms, the last-iterate convergence may not hold in our game formulation.

\subsection{Optimistic Nash Policy Optimization}
While self-play with OMD update already achieves an $\Ocal(\sqrt{T})$ regret bound, which is near-optimal in many online learning scenarios, there is still room for improvement by better leveraging the self-play structure. Recent advancements in learning in games~\citep{rakhlin2013optimization, syrgkanis2015fast} demonstrate that a faster convergence rate of $\Ocal(T^{-1})$ can be achieved when both players adopt optimistic OMD update. In this subsection, we introduce how to integrate optimistic OMD into the self-play algorithm, resulting in an algorithm called Optimistic Nash Policy Optimization (ONPO).

The key idea of optimistic OMD is to incorporate a reward or loss predictor at each iteration. Recall that in OMD update, we use the expected win rate over the current policy $\pi_t$ as the reward vector $r_t$ to compute $\pi_{t+1}$. While in optimistic OMD, the learner utilizes a reward predictor $m_t$ and adopts a two-step update strategy:
\begin{align*}
\pi_t&=\argmax_{\pi} \inner{\pi,m_t}-\frac{1}{\eta}\KL(\pi \Vert \pi'_t)  \\
\pi'_{t+1}&=\argmax_{\pi} \inner{\pi,r_t}-\frac{1}{\eta}\KL(\pi \Vert \pi'_t).
\end{align*}
Here $\pi_t$ aims to maximize the reward predictor $m_t$ and the auxiliary policy $\pi'_{t+1}$ is updated after observing the actual reward $r_t$. The word ``optimistic'' comes from that the learner believes that the predictor $m_t$ provides a good approximation of the true reward $r_t$.

Next, we describe how to apply optimistic OMD in our self-play algorithm. In both OMD and optimistic OMD, the KL regularization term is consistently used to ensure that the next policy remains close to the previous policies. This regularization provides stability, making it reasonable to assume that the change from $\pi_t$ to $\pi_{t+1}$ is small. Based on this observation, we directly use the reward information from the previous iteration as the predictor, i.e., let $m_t=r_{t-1}=\E_{y
' \sim \pi_{t-1}}\bra{\mathbb{P}(y \succ y')}$.

In the following theorem, we demonstrate that ONPO achieves an $\Ocal(1/T)$ duality gap, improving over the previous $\Ocal(1/\sqrt{T})$ result. 
\begin{theorem}\label{thm:onpo_regret}
Let $D=\max_{\pi} \KL(\pi \Vert \pi'_1)$ and $\bar \pi=\frac{1}{T}\sum_{t=1}^T \pi_t$, ONPO algorithm with $\eta = \min\{\frac{1}{2},\sqrt{D}\}$ satisfies:
\begin{align*}
\mathrm{DualGap}(\bar \pi) \le \frac{4\sqrt{D}}{T}.
\end{align*}
\end{theorem}
Here, $\pi'_1 = \pi_1$ is the initialization policy. Theoretically, $\pi'_1$ can be set as a uniform policy, in which case $D$ is bounded by $\log |\mathcal{Y}|$. In RLHF practice, $\pi'_1$ is typically a supervised fine-tuned policy.

The proof is provided in Appendix~\ref{sec:proof_onpo}. The key to achieving the $\Ocal(1/T)$ rate lies in the regret bounded by variation in utilities (RVU) property of optimistic OMD. Specifically, the stability terms $\|r_t - r_{t-1}\|_{\infty}^2$ are canceled out by the negative term $-\|\pi_t - \pi_{t-1}\|_1^2$, which arises from the self-play mechanism where $r_t$ represents the win rate over $\pi_t$.  

Notably, the duality gap bound in \citet{zhang2024iterative} also depends on the maximum log density ratio between $\pi_t$ and a reference policy $\pi_{\mathrm{ref}}$, due to the KL-regularized game formulation. When optimistic OMD is applied in such a regularized game, the stability terms transform into  
$$
\max_y \left|\mathbb{P}(y \succ \pi_t) - \mathbb{P}(y \succ \pi_{t-1}) + \log \frac{\pi_t(y)}{\pi_{t-1}(y)}\right|,
$$ 
which cannot be canceled by the negative terms. However, the motivation behind regularizing the game is to keep the learner's policy close to the reference policy $\pi_{\mathrm{ref}}$, which aligns with the stability introduced in our update rule. Therefore, explicit regularization in our game objective is not necessary.

\subsection{Implementation of ONPO}
In this subsection, we describe the implementation of ONPO with query access to the preference oracle $\mathbb{P}$. The primary challenge in implementing ONPO lies in computing $r_t(y)$, which involves taking an expectation over the entire policy $\pi_t$. Fortunately, this challenge can be addressed by avoiding the direct estimation of $r_t(y)$ and instead relying on binary preference feedback between responses.

To achieves this, our goal is to design a loss function that does not involve $\mathbb{P}(y \succ \pi_t)$ for policy optimization. We focus on obtaining the loss objective for $\pi_t$ here and the derivation for $\pi'_t$ is similar. The key observation is that, $\pi_t$ has a closed-form solution which satisfies $\forall y,y' \in \Ycal$,
$$
\log \frac{\pi_t(y)}{\pi_t(y')}-\log \frac{\pi'_t(y)}{\pi'_t(y')}=\eta \pa{\mathbb{P}(y \succ \pi_{t-1}}-\mathbb{P}(y' \succ \pi_{t-1}).
$$
Therefore, similar to the techniques used in~\citet{azar2024general,zhang2024iterative}, solving $\pi_t$ is equivalent to finding the minimizer of the following loss function:
{\small
\begin{align*}
\E_{y,y' \sim \pi_{t-1}}\bra{\pa{g_t(\pi,y,y')-\eta\pa{\mathbb{P}(y \succ \pi_{t-1})-\mathbb{P}(y' \succ \pi_{t-1})}}^2}.
\end{align*}}
\noindent where $g_t(\pi,y,y')=\log \frac{\pi(y)}{\pi(y')}-\log \frac{\pi'_t(y)}{\pi'_t(y')}$. Since the inside win rate term is with respect to $\pi_{t-1}$ and we also have an expectation over $\pi_{t-1}$ outside, the loss function can be further written as
\begin{align*}
\E_{y,y' \sim \pi_{t-1},y_w,y_l \sim \lambda_p(y,y')}\bra{\pa{g_t(\pi,y_w,y_l)-\frac{\eta}{2}}^2},
\end{align*}
where $\lambda_p$ is the preference distribution~\citep{calandriello2024human}:
\begin{align*}
\lambda_p(y,y')=\begin{cases} (y,y') \quad \textrm{with probability $\P(y \succ y')$} \\
(y',y) \quad \textrm{with probability $1- \P(y \succ y')$.}
\end{cases}
\end{align*}
To calculate the loss function, we only need the access to sample from the current policy, which is standard and easy to implement in practice. Putting everything together, the implementation of ONPO is summarized in Algorithm~\ref{alg:onpo_prac}.

In the beginning, we initialize $\pi'_1$ and $\pi_1$ with the supervised fine-tuned policy $\pi_{\textrm{SFT}}$. At each iteration $t$, we sample responses from the current policy $\pi_t$ and use the preference feedback from the oracle $\mathbb{P}$ to construct the dataset $D_t$. Then we can directly minimize the corresponding loss functions on $D_t$ to find $\pi'_{t+1}$ and $\pi_{t+1}$ respectively. We use the last iteration policy $\pi_T$ as the output policy, which is consistent with online RLHF practice~\citep{dong2024rlhf,wu2024self,zhang2024iterative}.

\begin{algorithm}[t]
\caption{Implementation of ONPO
\label{alg:onpo_prac}
}
\begin{algorithmic}[1]
\STATE {\bfseries Input:} Number of iterations $T$, learning rate $\eta$, preference oracle $\mathbb{P}$, supervised fine-tuned policy $\pi_{\textrm{SFT}}$.
\STATE Initialize $\pi'_1 \leftarrow \pi_{\textrm{SFT}}$, $\pi_1 \leftarrow \pi_{\textrm{SFT}}$.
\FOR{iteration $t = 1,2,\dotsc,T-1$}
\STATE Sample response pairs from the current policy $\pi_t$: $\{y_1^{(i)},y_2^{(i)}\}_{i=1}^n \sim \pi_t$.
\STATE Construct preference dataset $D_t=\{y_w^{(i)},y_l^{(i)}\}_{i=1}^n$ with feedback from the oracle $\mathbb{P}$.
\STATE Calculate $\pi'_{t+1}$ as:
\begin{align*}
\pi'_{t+1}=\argmin_{\pi} \E_{y_w,y_l \sim D_t}\bra{\pa{g_{t}(\pi,y_w,y_l)-\frac{\eta}{2}}^2}.
\end{align*}
\STATE Calculate $\pi_{t+1}$ as:
\begin{align*}
\pi_{t+1}=\argmin_{\pi} \E_{y_w,y_l \sim D_t}\bra{\pa{g_{t+1}(\pi,y_w,y_l)-\frac{\eta}{2}}^2}.
\end{align*}
\ENDFOR
\STATE Output $\pi_T$.
\end{algorithmic}
\end{algorithm}







