\section{Preliminary}\label{sec:prelim}
\paragraph{Problem Setup.} We study the contextual formulation which is extensively used in previous RLHF literature~\citep{rafailov2024direct,xiong2024iterative}. The prompt $x \in \Xcal$ is sampled from an unknown prompt distribution $d_1$. $\Ycal$ is the response space and an LLM is characterized by a policy $\pi:\Xcal \rightarrow \Delta(\Ycal)$ which outputs the response probability given the context. For any policy $\pi$, we use $\E_{\pi}$ to denote the expectations under $\pi$. 
% \paragraph{Contextual Markov Decision Process.} We study RLHF in a multi-turn reinforcement learning setting, modeled as a finite-horizon contextual Markov Decision Process (CMDP)~\citep{shani2024multi}. A CMDP is specified by the tuple $\inner{H,\Xcal, \Scal=\bigcup_{h=1}^{H+1} \Scal_h,\Acal,P,d_1}$, where $H$ is the horizon, $\Xcal$ is the context space, $\Scal_h$ is the state space at step $h$, $\Acal$ is the action space, $P:\Xcal \times \Scal \times \Acal \rightarrow \Delta(\Scal)$ is the transition dynamics with $P(\cdot \mid x,s_h,a_h)$ supported on $\Scal_{h+1}$, and $d_1 \in \Delta(\Xcal)$ is the context distribution. An LLM is characterized by a policy $\pi:\Scal \times \Xcal \rightarrow \Delta(\Acal)$ which outputs the action probability given the state and the context. The interaction process between the LLM and the environment proceeds as follows: at the start, the context $x$ is sampled from $d_1$ and the LLM begins at the fixed initial state $s_1$ and takes action $a_1 \sim \pi(\cdot \mid s_1,x)$, then the environment transits to the next state $s_2 \sim P(\cdot \mid x,s_1,a_1)$ and $a_2 \sim \pi(\cdot \mid s_2,x)$. This process repeats for $H$ steps and terminates after arriving at the final state $s_{H+1}$. For any policy $\pi$, we use $\E_{\pi}$ to denote the expectations under episodes generated by $\pi$.

% This CMDP framework captures a variety of LLM applications. For instance, consider a chatbot interacting with a user: the user begins by posing a question, represented as the context $x$. The chatbot generates a response $a_1$, and the user reacts by providing further clarification, prompting the chatbot to refine its response. Here, $s_h$ corresponds to the dialogue history up to the $h$-th turn, and the userâ€™s behavior determines the transition dynamics. Another example is the token-level MDP~\citep{rafailov2024r}, where $x$ is the prompt, each action $a_h$ represents a token from the vocabulary $\mathcal{A}$, and $s_h = (a_1, \dots, a_{h-1})$ represents the sequence of tokens generated so far. In this case, the transition $P(\cdot \mid s_{h-1}, a_h)$ is deterministic, appending $a_h$ to $s_{h-1}$.

\paragraph{General Preferences.} In this work, we drop the BT model assumption~\citep{bradley1952rank} and focus on directly aligning LLMs with general preferences. To this end, we define a general preference oracle as follows:
\begin{definition}[General Preference Oracle] \label{def:general_oracle} There exists a preference oracle $\P: \Xcal \times \Ycal \to \Ycal \rightarrow [0,1]$, which can be queried to obtain the binary preference signal:
$$
z \sim \mathrm{Ber}\big(\P(y^1 \succ y^2 \mid x)),
$$
where $z=1$ indicates $y^1$ is preferred to $y^2$, and $z = 0$ indicates the opposite.
\end{definition}
Unlike the BT model assumption, which assumes the existence of a reward function $R^*$ for each $x$ and $y$, the general preference oracle always compares $y^1$ to another $y^2$. This setup aligns with practical scenarios, where it is often easier for users to compare two responses than to assign an absolute score to a single response. Since the preference signal always involves two responses, potentially come from two different policies, we formulate the LLM alignment problem as a two-player zero-sum game. The objective of this game is the expected win rate between the two players:
\begin{align*}
J(\pi_1,\pi_2):=\E_{x \sim d_1}\E_{y^1 \sim \pi_1,y^2 \sim \pi_2}\bra{\mathbb{P}(y^1 \succ y^2 \mid x)}.
\end{align*}
Here $\pi_1$ is the policy of the max-player, aiming to maximize the objective, while $\pi_2$ is the policy of the min-player, aiming to minimize it.

% \paragraph{Single-turn setting.} The single-turn setting, which is similar to the contextual bandit formulation, has been extensively studied in previous RLHF literature~\citep{rafailov2024direct,xiong2024iterative}. When the horizon $H=1$, the CMDP reduces to the single-turn setting. With the initial state $s_1$ fixed, the action is sampled directly as $a_1 \sim \pi(\cdot \mid x)$. Since only a single action is taken, the preference oracle can directly compare two actions under the context $x$. The game objective in the single-turn setting is then defined as:
% \begin{align*}
% J(\pi_1,\pi_2)=\E_{x \sim d_1}\E_{a^1 \sim \pi_1,a^2 \sim \pi_2}\bra{\mathbb{P}(a^1 \succ a^2 \mid x)}.
% \end{align*}


\paragraph{Nash Policies and Duality Gap.} Our learning goal is to find the Nash equilibrium of the game, which is defined as:
\begin{align*}
\pi_1^*,\pi_2^*:=\argmax_{\pi_1} \argmin_{\pi_2} J(\pi_1,\pi_2).
\end{align*}
Due to the symmetric nature of the game, the Nash policies for both players are identical, i.e., $\pi_1^*=\pi_2^*=\pi^*$, and the game value is $J(\pi^*,\pi^*)=0.5$. Since Nash policies are the best responses to each other, for any policy $\pi$, we have $J(\pi^*,\pi) \ge 0.5$, indicating that the Nash policy will not lose to any other policy. To quantify how well a policy $\pi$ approximates $\pi^*$, we define the duality gap as:
\begin{align*}
\mathrm{DualGap}(\pi):=\max_{\pi_1} J(\pi_1,\pi)-\min_{\pi_2} J(\pi, \pi_2).
\end{align*}
The duality gap is non-negative and $\mathrm{DualGap}(\pi) = 0$ if and only if $\pi = \pi^*$. Hence, our goal is to find a policy that minimizes the duality gap. Once we achieve $\mathrm{DualGap}(\pi) \leq \epsilon$, we say that $\pi$ is an $\epsilon$-approximate Nash policy.

% \paragraph{Preference-based Value functions.} In reinforcement learning, value functions are typically defined with respect to the reward functions. However, in the preference learning setting, feedback involves comparisons between two policies. Therefore, we define preference-based value functions and Q-functions~\citep{shani2024multi} for a policy pair $(\pi_1, \pi_2)$ as follows\footnote{Following \citet{azar2024general,shani2024multi}, we omit the context $x$ throughout the rest of the paper since each context is independent.}:
% \begin{align*}
% V^{\pi_1,\pi_2}(s_h)&=\E_{\pi_1}\bra{\Pcal(s_{H+1} \succ \pi_2) \mid s_h}, \\
% Q^{\pi_1,\pi_2}(s_h,a_h)&=\E_{\pi_1}\bra{\Pcal(s_{H+1} \sim \pi_2 \mid s_h,a_h}.
% \end{align*}
% Unlike value functions in Markov games~\citep{bai2020near, zhang2023offline}, $\pi_2$ always starts from the initial state $s_1$, rather than the current state $s_h$. This distinction arises because transitions are independent for each policy, and the policies interact only when their final states are compared.
