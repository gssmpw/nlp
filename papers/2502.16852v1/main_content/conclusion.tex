\section{Conclusion and Future Work}\label{sec:conclusion}
We propose Optimistic Nash Policy Optimization (ONPO), a novel approach for aligning LLMs with general preferences via self-play. By integrating optimistic online mirror descent, ONPO achieves an improved duality gap bound for approximating the Nash policy of the game. Our experimental results demonstrate that ONPO consistently outperforms or matches state-of-the-art general preference alignment methods across multiple benchmarks. For future work, we aim to explore the implementation of ONPO under the multi-turn setting. In addition, we plan to design different strategies for actively selecting preference data to further enhance alignment performance.