\section{Related Work}\label{sec:related}
\paragraph{Reward-Based RLHF.} Since the first RLHF framework proposed by \citet{christiano2017deep}, RLHF has achieved tremendous success in aligning large language models (LLMs), powering models such as Instruct-GPT~\citep{ouyang2022training}, Llama 2~\citep{touvron2023llama}, and Claude~\citep{bai2022training}. The RLHF pipeline typically involves training a reward model followed by applying policy gradient methods such as PPO~\citep{schulman2017proximal} to optimize a KL-regularized objective~\citep{korbak2022rl,li2023remax}. Nevertheless, the use of PPO in RLHF introduces challenges, including instability during training~\citep{choshen2019weaknesses} and high computational costs~\citep{yuan2023rrhf}. To address these limitations, \citet{rafailov2024direct} proposed the DPO algorithm, which directly optimizes preferences by minimizing a loss objective on offline datasets. Additionally, other direct preference learning algorithms have been developed, including offline methods~\citep{ethayarajh2024kto} and online (iterative) methods~\citep{xie2024exploratory,xiong2024iterative,yuan2024self}. However, all these algorithms are reward-based and rely on the Bradley-Terry (BT) model assumption. In this paper, we remove the BT model assumption and consider general preference alignment.


\paragraph{RLHF with General Preferences.}\citet{azar2024general} is the first to consider the general preference without BT model assumption. They propose the offline IPO algorithm to learn the optimal policy when the comparator policy is fixed.~\citet{munos2023nash} formulate the alignment problem as a two-player zero-sum game and propose the iterative Nash-MD algorithm to find the Nash policy of the game. Subsequently, there has been a line of work~\citep{ye2024theoretical,calandriello2024human,rosset2024direct,wu2024self} developing online algorithms for learning the Nash policy. The closest work related to ours is~\citet{zhang2024iterative}, which also employs a no-regret learning algorithm for self-play. However, our algorithm incorporates an optimistic predictor into the policy update, achieving improved theoretical guarantees and better empirical performance. A detailed comparison between our algorithm and other general preference alignment algorithms is provided in Section~\ref{sec:discuss}.

\paragraph{Learning in Games.} Online learning and self-play algorithms are widely used in approximating the equilibrium of games, including normal-form games~\citep{freund1999adaptive,daskalakis2011near,mai2018cycles,roy2019online,chen2020hedging,wei2020linear,daskalakis2021near}, extensive-form games~\citep{zinkevich2007regret,kroer2020faster,kozuno2021model,lee2021last,bai2022near} and Markov games~\citep{wei2017online,jin2021v,liu2021sharp,mao2023provably}. Our work is inspired by the faster convergence properties of optimistic online mirror descent in equilibrium learning~\citep{rakhlin2013optimization,syrgkanis2015fast}.

