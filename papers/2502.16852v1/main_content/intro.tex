\section{Introduction}\label{sec:intro}
Reinforcement learning from human feedback (RLHF) has played a pivotal role in aligning large language models (LLMs) with human preferences. The goal of RLHF is to fine-tune LLMs to generate responses that are preferred by humans. It has been successfully deployed in state-of-the-art models, including Instruct-GPT~\citep{ouyang2022training} and Claude~\citep{bai2022training}. The first RLHF framework for LLMs was developed by~\citet{ouyang2022training}, where after the pre-training stage, the LLM is fine-tuned to maximize the reward signal from a reward model using the proximal policy optimization (PPO) algorithm~\citep{schulman2017proximal}. This pipeline requires training both the reward model and the policy model. In addition, policy gradient approaches such as PPO often exhibit high variance and instability during training~\citep{peng2023stabilizing}, leading to increased computational costs.

To develop a more stable and computationally lightweight alignment approach,~\citet{rafailov2024direct} propose the Direct Preference Optimization (DPO) algorithm, which directly trains the LLM on a preference dataset and bypasses the need for a reward model. DPO uses an offline preference dataset, and since its development, a line of research has explored different exploration strategies and proposed online direct preference alignment algorithms~\citep{xiong2024iterative,xie2024exploratory,dong2024rlhf,yuan2024self}. All these methods assume that human preferences can be modeled using the Bradley-Terry (BT) model, where a reward function $R^*$ exists such that, for any prompt $x$ and response pair $(y^1, y^2)$, the preference between $y^1$ and $y^2$ satisfies:  
\[
\mathbb{P}(y^1 \succ y^2 \mid x) = \sigma(R^*(x,y^1) - R^*(x,y^2)),
\]
where $\sigma(z) = \frac{1}{1+\exp(-z)}$ is the sigmoid function.

However, the existence of a reward function and the BT model are strong assumptions that can be overly restrictive when modeling complex human preferences. For example, the preference signals in the BT model are always transitive: if $A$ is preferred to $B$ and $B$ is preferred to $C$, then $A$ must always be preferred to $C$. This transitive property contradicts evidence from human decision-making~\citep{may1954intransitivity,tversky1969intransitivity}, especially when preferences are at the population level and aggregated from different human groups~\citep{may1954intransitivity,ye2024theoretical}.  Furthermore, the limitations of the BT model have also been observed in RLHF practice. ~\citet{jiang2023llm} show that a preference model with 0.4B parameters achieves performance comparable to Llama-2-13B-based reward models. ~\citet{ye2024theoretical} train a BT reward model and a preference model separately using the same base model and preference dataset, and their results demonstrate that the preference model consistently outperforms the reward model on Reward-Bench~\citep{lambert2024rewardbench} under both base models. These findings motivate us to drop the BT model assumption and instead consider general preferences.

In this work, we study the problem of aligning LLMs with general preferences and formulate it as a two-player zero-sum game. Our objective is to approximate the Nash policy of the game, which ensures a win rate of at least 50\% against any other policy. As established in the game theory literature~\citep{bai2020near,liu2021sharp}, self-play algorithms have proven to be highly effective in approximating Nash policies. Building on this, we aim to propose a novel online RLHF algorithm that further leverages the self-play strcture to enhance general preference alignment for LLMs. Our contributions are summarized as follows.

\paragraph{Contributions.} We propose a novel online general preference alignment algorithm, Optimistic Nash Policy Optimization (ONPO). Inspired by recent advancements in game theory, our algorithm integrates optimistic online mirror descent~\citep{rakhlin2013optimization,syrgkanis2015fast} into the self-play framework. By utilizing a reward predictor in a two-step update strategy, ONPO more effectively leverages the self-play mechanism and achieves a faster convergence rate of $\mathcal{O}(T^{-1})$, improving upon the previous $\mathcal{O}(T^{-1/2})$ result.  

ONPO can be efficiently implemented by directly minimizing a loss objective on a preference dataset, making it computationally lightweight in practice. We evaluate ONPO on several representative benchmarks, comparing it with state-of-the-art general preference alignment algorithms. Experimental results demonstrate that ONPO consistently outperforms or achieves performance comparable to the baselines across different base models and benchmarks. Notably, on the AlpacaEval 2.0 benchmark~\citep{li2023alpacaeval}, ONPO achieves a 21.2\% and 9.9\% relative improvement over the strongest baseline when using Mistral-Instruct and Llama-3-8B as the base models, respectively.

\paragraph{Organization.} Section~\ref{sec:related} presents related work on RLHF and learning in games. The problem formulation and preliminaries are provided in Section~\ref{sec:prelim}. Our algorithm and its theoretical guarantees are detailed in Section~\ref{sec:algo}. In Section~\ref{sec:discuss}, we compare our approach with other general preference alignment algorithms and explore its extension to the multi-turn setting. Experimental results are presented in Section~\ref{sec:exp}. Finally, we conclude the paper and discuss future directions in Section~\ref{sec:conclusion}.