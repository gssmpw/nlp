\section{Discussion}\label{sec:discuss}
In this section, we first discuss the differences between ONPO and other general preference alignment methods. Then we introduce how to extend ONPO to the multi-turn setting.
\subsection{Comparison between ONPO and Other General Preference Alignment Methods}
\paragraph{IPO.}\citet{azar2024general} is the first to address general preference alignment in LLMs. The optimization objective of IPO is:
\begin{align*}
\max_{\pi} \E_{y \sim \pi, y' \sim \mu}\bra{\mathbb{P}(y \succ y')}-\tau \KL(\pi \Vert \pi_{\textrm{ref}}),
\end{align*}
where $\mu$ is a fixed policy. From a game-theoretic perspective, the goal of IPO is to find the best response to $\mu$. However, this approach only ensures that the learned policy outperforms $\mu$, which leaves the possibility that another policy could outperform the learned policy. In contrast, our approach focuses on learning the Nash policy in a two-player game. This provides stronger theoretical guarantees, as the Nash policy will not lose to any other policy.

\paragraph{Nash-MD.}\citet{munos2023nash} is the first to formulate the alignment problem as a two-player zero-sum game. Their game objective includes KL regularization terms, which ensure that the player's policy remains close to the reference policy $\pi_{\textrm{ref}}$. The KL terms are weighted by a parameter $\tau$. They proposed an iterative algorithm, Nash-MD, to learn the Nash policy of the game. At each iteration $t$, the policy is updated as:
\begin{align*}
\pi_{t+1}=\argmax_{\pi} \mathbb{P}(\pi \succ \pi'_t)-\frac{1}{\eta_t} \KL(\pi,\pi'_t),
\end{align*}
where $\pi'_t$ is a geometric mixture policy of the current policy $\pi_t$ and the reference policy $\pi_{\textrm{ref}}$:
\begin{align*}
\pi'_t(y)=\frac{\pi_t(y)^{1-\eta_t \tau}\pi_{\textrm{ref}}(y)^{\eta_t \tau}}{\sum_{y'}{\pi_t(y')^{1-\eta_t \tau}\pi_{\textrm{ref}}(y')^{\eta_t \tau}}}.
\end{align*}
Nash-MD requires sampling from the mixture policy $\pi'_t$. However, the response space $\mathcal{Y}$ is often exponentially large, making the exact computation of $\pi'_t$ intractable. To address this, \citet{munos2023nash} propose sampling from an approximate policy. The theoretical guarantees of this approximation remain unclear. In contrast, our approach only requires sampling from the current policy $\pi_t$, which is straightforward to implement in practice.

\paragraph{Online IPO.}\citet{calandriello2024human} propose the online IPO population loss:
\begin{align*}
\mathop{\E}_{\substack{y, y' \sim {\mathrm{SG}[\pi]} \\ y_w, y_l \sim \lambda_p(y, y')}}\bra{\pa{\log{\frac{\pi(y_w)\pi_{\textrm{ref}}(y_l)}{\pi(y_l)\pi_{\textrm{ref}}(y_w)}}-\frac{1}{2 \tau}}^2},
\end{align*}
where $\mathrm{SG}$ is the stop-gradient operator, which prevents gradients from propagating through the data-generation process. Unlike the offline IPO approach, which always samples from a fixed policy $\mu$, online IPO leverages responses generated by the current policy $\pi$. 

Since the policy $\pi$ is updated throughout training, policy gradient methods are used to minimize the objective. However, as discussed earlier, policy gradient methods in RLHF have limitations, including being resource-intensive and unstable to train. In contrast, ONPO avoids these challenges by directly minimizing a loss function over a preference dataset, offering a more stable and efficient implementation.

\paragraph{DNO.} The theoretical version of DNO (Algorithm 1 in \citet{rosset2024direct}) relies on computing $r_t(y)=\E_{y' \sim \pi_t}\bra{\mathbb{P}(y \succ y')}$, which requires taking an expectation over the current policy $\pi_t$. This computation is challenging to implement in practice, so \citet{rosset2024direct} propose a practical version, DNO-Prct (Algorithm 2), where $\pi_{t+1}$ is updated as follows:
\begin{align*}
\argmax_{\pi} \E_{y_w,y_l \sim D_t} \log \bra{\sigma\pa{\eta \log\frac{\pi(y_w)\pi_t(y_l)}{\pi_t(y_w)\pi(y_l)}}}.
\end{align*}
When constructing the dataset $D_t$, only response pairs with large margins are selected. This selection is motivated by the fact that, to approximate DNO, the ideal condition is $\sigma(r_t(y_w) - r_t(y_l)) \approx 1$. However, this cannot be fully achieved since $r_t(y) \in [0, 1]$. Notably, the objective of DNO-Prct is identical to the DPO objective~\citep{rafailov2024direct}. Therefore, DNO-Prct can be viewed as an iterative version of DPO.

\paragraph{SPPO.}\citet{wu2024self} propose a self-play algorithm SPPO. The policy update in SPPO is:
{\small
\begin{align*}
\pi_{t+1}=\argmin_{\pi} \E_{y \sim \pi_t}\pa{\log \frac{\pi(y)}{\pi_t(y)} - \eta \pa{\widehat P(y \succ \pi_t) - \frac{1}{2}}}^2,
\end{align*}}
where $\wh{P}$ is a heuristic approximation of $\mathbb{P}(y \succ \pi_t)$. However, obtaining an accurate estimation of $\mathbb{P}(y \succ \pi_t)$ is challenging in practice. For example, Hoeffdingâ€™s inequality suggests that more than 100 queries are needed to ensure $\left|\mathbb{P}(y \succ \pi_t) - \widehat{P}(y \succ \pi_t)\right| \leq 0.1$. This requirement results in high annotation and computation costs, as 100 oracle queries are needed for a single response $y$. In contrast, ONPO bypasses the need to estimate $\mathbb{P}(y \succ \pi_t)$ and instead relies on binary preference signals between two responses.

\paragraph{INPO.}\citet{zhang2024iterative} propose a self-play algorithm, INPO, which employs OMD to iteratively update the policy, as described in Section~\ref{sec:inpo}. Leveraging the faster convergence properties of optimistic OMD, ONPO achieves an improved duality gap bound of $\Ocal(T^{-1})$, compared to the $\Ocal(T^{-1/2})$ bound of INPO.

\subsection{Extension to the Multi-Turn Setting}  
In this subsection, we describe how ONPO can be extended to the multi-turn setting, which is formulated as a contextual Markov decision process (CMDP)~\citep{shani2024multi}. The interaction between the LLM and the environment unfolds as follows: the LLM starts at a fixed initial state $s_1 \in \Scal$ and takes an action $y_1 \sim \pi(\cdot \mid s_1)$. The environment then transitions to the next state $s_2 \sim P(\cdot \mid s_1, y_1)$ according to the transition dynamics $P$, and the LLM subsequently takes action $y_2 \sim \pi(\cdot \mid s_2)$. This process repeats for $H$ steps, ultimately reaching the final state $s_{H+1}$. At the end of the interaction, the preference oracle compares two final states and provides a preference signal:  
$
z \sim \mathrm{Ber}\big(\P(s_{H+1}^1 \succ s_{H+1}^2 )\big).
$
This CMDP formulation effectively captures various LLM applications, including chatbot interactions and token-level MDPs~\citep{rafailov2024r}.


In the multi-turn setting, the challenge is that preferences are only provided for the final states, and there is no direct feedback for intermediate states. To address this, we use Q-value functions, which capture the long-term expected outcomes, in the optimization objective. For each state $s_h$, the update rule for $\pi_{t+1}(\cdot \mid s_h)$ is:
\begin{align*}
\argmax_{\pi} \inner{\pi,Q^{\pi_t,\pi_t}(s_h,\cdot)} - \frac{1}{\eta}\KL(\pi(\cdot \mid s_h) \Vert \pi_t(\cdot \mid s_h)),
\end{align*}
where $Q^{\pi_t,\pi_t}(s_h,y_h)=\E_{\pi_t}\bra{\Pcal(s_{H+1} \succ \pi_t) \mid s_h,y_h}$ and  $\Pcal(s \succ \pi_t)$ represents $\E_{\pi_t}\bra{\mathbb{P}(s \succ s_{H+1})}$. Here $\inner{\pi,Q^{\pi_t,\pi_t}(s_h,\cdot)}$ measures the probability of $\pi$ outperforming $\pi_t$ at state $s_h$. The update rule for $\pi'_{t+1}$ is similar, except that the KL divergence is computed between $\pi$ and $\pi'_t$.


The primary challenge in implementing ONPO in the multi-turn setting lies in the efficient estimation of $Q^{\pi_t,\pi_t}$.~\citet{shani2024multi} propose to use an actor-critic framework that employs policy-gradient methods such as PPO~\citep{schulman2017proximal} for policy optimization. However, policy-gradient methods are known to exhibit high variance and sensitivity to implementation details, leading to increased computational costs. In this paper, we focus on implementing ONPO in the single-turn setting and leave the implementation under the multi-turn setting for future work.
