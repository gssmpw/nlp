\section{Experiments}\label{sec:exp}
\begin{table*}[ht]
    \centering 
    \caption{Results on three benchmarks. ``ONPO+Mistral-It" refers to tuning the Mistral-Instruct model with ONPO, while ``ONPO+Llama-3-SFT" refers to tuning the Llama-3-SFT model with ONPO. Results where the baseline outperforms ONPO are underlined.}\label{tab:res_main}
    \vspace{5pt}
    \begin{tabular}{c|c|cccc}
    \toprule
    \textbf{Model}  &\textbf{Size} & \textbf{AlpacaEval 2.0} & \textbf{Arena-Hard} & \textbf{MT-Bench}  \\ \midrule
    Iterative DPO + Mistral-It & 7B& 32.0 & 22.2  & 7.35 \\
    SPPO + Mistral-It & 7B & 33.1 & 24.5 & 7.51 \\
    INPO + Mistral-It & 7B & 35.3 & 25.3 & 7.46 \\
    ONPO + Mistral-It & 7B & \textbf{42.8} & \textbf{29.7} & \textbf{7.68} \\
    \midrule
    \midrule
    Iterative DPO + Llama-3-SFT & 8B& 28.3 & 31.9  & 8.34 \\
    SPPO + Llama-3-SFT & 8B& 38.5 & 32.9 & 8.23 \\
    INPO + Llama-3-SFT & 8B & 44.2 & \underline{37.0} & 8.28 \\
    ONPO + Llama-3-SFT & 8B & \textbf{48.6} & \textbf{36.4} & \textbf{8.40} \\
    \midrule
    \midrule
    % Vicuna-33b-v1.3 & 33B & 17.6 & 8.6 & 7.12 \\
    Llama-3-8B-it & 8B & 24.8 & 21.2 & 7.97\\
    Tulu-2-DPO-70B & 70B & 21.2 & 15.0 & 7.89 \\
    Llama-3-70B-it & 70B & 34.4  &41.1 & 8.95\\
    Mixtral-8x22B-it & 141B & 30.9 & 36.4 & 8.66  \\
    \midrule
    \midrule
    GPT-3.5-turbo-0613 & - & 22.7  & 24.8 & 8.39 \\
    GPT-4-0613 & - & 30.2 & 37.9 & 9.18 \\
    Claude-3-Opus & - & 40.5 & 60.4 & 9.00  \\
    GPT-4 Turbo (04/09) & - & 55.0 & 82.6 & -\\
    \bottomrule
    \end{tabular}
    \end{table*}
\subsection{Main Results} \paragraph{Experiment Setup.} We implement ONPO following the online RLHF workflow described in \citet{dong2024rlhf}. Two base models are used as the initial policy $\pi_1$: Llama-3-SFT\footnote{\url{https://huggingface.co/RLHFlow/LLaMA3-SFT}}, based on Llama-3-8B~\citep{dubey2024llama}, and Mistral-Instruct-v0.3\footnote{\url{https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3}}, an instruct fine-tuned version of the Mistral-7B-v0.3. For the general preference oracle, we use a pairwise preference model\footnote{\url{https://huggingface.co/RLHFlow/pair-preference-model-LLaMA3-8B}}, which demonstrates better performance compared to the BT reward model~\citep{zhang2024iterative}. Training details for the preference model are available in \citet{dong2024rlhf}.

At each iteration, the current policy generates $K = 8$ responses using a set of prompts\footnote{\url{https://huggingface.co/datasets/RLHFlow/prompt-collection-v0.1}}. To select $y_w$ (winner) and $y_l$ (loser), we follow the tournament approach in \citet{zhang2024iterative}, where the eight responses are compared pairwise to identify the winning and losing responses.

Since online or iterative alignment methods have been shown to outperform offline counterparts, we focus on comparing ONPO with other online methods for a fair evaluation. These include iterative DPO~\citep{dong2024rlhf}, SPPO~\citep{wu2024self} and INPO~\citep{zhang2024iterative}, where the latter two are general preference alignment approaches.

We evaluate the models on three representative benchmarks: AlpacaEval 2.0~\citep{li2023alpacaeval}, Arena-Hard~\citep{li2024live} and MT-Bench~\citep{zheng2024judging}. AlpacaEval 2.0 has 805 instructions from five datasets, including self-instruct test set~\citep{wang2022self}, Open Assistant test set, Anthropic's helpful test set~\citep{bai2022training}, Vicuna test set~\citep{zheng2024judging} and Koala test set~\citep{koala_blogpost_2023}. Arena-Hard includes 500 challenging user queries from Chatbot Arena. Both AlpacaEval 2.0 and Arena-Hard compare model-generated answers against reference answers from a baseline model, using GPT-4 Preview-1106 as the judge model. We report the win rate for Arena-Hard and the length-controlled (LC) win rate~\citep{dubois2024length} for AlpacaEval 2.0. MT-Bench consists of 80 multi-turn questions, where responses are rated by GPT-4 on a 1-10 scale, with the average rating reported.

\paragraph{Results.} The model performance is summarized in Table~\ref{tab:res_main}. Our results show that ONPO consistently outperforms or achieves comparable performance to the baselines across both base models. Among the three benchmarks, the length-controlled (LC) win rate in AlpacaEval 2.0 exhibits the highest 0.98 Spearman correlation with Chatbot Arena rankings~\citep{dubois2024length}. In this benchmark, ONPO outperforms the strongest baseline by a clear margin—achieving a 9.9\% improvement on Llama-3-SFT and a 21.2\% improvement on Mistral-It. These results align with our theoretical findings, demonstrating that ONPO benefits from an improved bound on the duality gap. We also compare ONPO with other LLMs that have significantly larger parameters, such as Llama-3-70B-it, Mixtral-8x22B-it and GPT-4-Turbo. Remarkably, our ONPO even outperforms models with at least nine times more parameters.

\subsection{More Results on Academic Tasks}
\begin{table*}[ht]
    \centering
    \caption{Model performance on more academic benchmarks (AVG: average).}\label{tab:academic}
    \vspace{5pt}
    \begin{tabular}{c|c|c|c|c|c|c|c}
    \toprule
    \textbf{Model} & \textbf{GPQA} & \textbf{Hellaswag} & \textbf{MMLU-Pro} & \textbf{Winogrande} & \textbf{TruthfulQA} & \textbf{GSM8K} &\textbf{AVG} \\ \midrule
    Mistral-It & 30.1 & 83.5 & 30.4 & 74.2 & 59.7 & 49.5 & 54.6 \\ 
    Iterative DPO & 29.6 & 83.3 & 28.0 & 75.1 & 64.0 & 45.7 & 54.3 \\ 
    SPPO & 28.7 & 83.5 & 28.1 & 73.9 & 66.4 & 49.9 & 55.1 \\
    INPO & 28.8 & 82.9 & 28.9 & 74.9 & 64.7 & 46.3 & 54.4  \\
    ONPO & 30.4 & 83.7 & 29.9 & 75.1 & 65.5 & 47.8 & \textbf{55.4}  \\
    \bottomrule
    \end{tabular}
\end{table*}
In this subsection, we evaluate the model’s reasoning and calibration abilities across six academic benchmarks: GPQA~\citep{rein2023gpqa} for graduate-level science question answering, MMLU-Pro~\citep{wang2024mmlu} for multitask language understanding, Hellaswag~\citep{zellers2019hellaswag} for commonsense inference, Winogrande~\citep{sakaguchi2021winogrande} for difficult commonsense reasoning, TruthfulQA~\citep{lin2021truthfulqa} to assess the model’s tendency to reproduce falsehoods, and GSM8K~\citep{cobbe2021training} for mathematical reasoning.

It is important to note that these benchmarks primarily evaluate a model’s intrinsic knowledge and capabilities, which are developed during the pre-training stage rather than the alignment stage. However, as observed in prior work~\citep{ouyang2022training,openai2023gpt}, alignment can sometimes have a negative impact on these abilities—a phenomenon known as the ``alignment tax". Therefore, our purpose in presenting these results is to verify that our alignment method preserves the model’s abilities rather than demonstrating performance improvements.

We show the results using Mistral-Instruct-v0.3 as the base model and compare ONPO with three baselines as well as the base model itself. The results in Table~\ref{tab:academic} show that ONPO achieves a slightly higher average performance than both the base model and the baselines, demonstrating that ONPO does not over-align the model and effectively preserves its intrinsic knowledge and abilities.

\subsection{Hyperparameter Sensitivity Analysis}
\begin{figure}[t]
\centering
\includegraphics[width=0.45\textwidth]{main_content/eta_res.png}
\caption{
Performance of ONPO with different values of $\eta$ on Arena-Hard and AlpacaEval 2.0. ONPO consistently outperforms the best baseline, which achieves a win rate of 25.3 on Arena-Hard and 35.3 on AlpacaEval, respectively.}
\label{fig:res_eta}
\end{figure}
In this subsection, we analyze the sensitivity of ONPO to the hyperparameter $\eta$, which serves as the learning rate in the update rule. We conduct experiments using Mistral-Instruct-v0.3 as the base model and vary $\eta$ from $200/3$ to $200$. The results, presented in Figure~\ref{fig:res_eta}, indicate that ONPO consistently achieves strong performance across different values of $\eta$ and outperforms the baselines, demonstrating its robustness to hyperparamter variations.


