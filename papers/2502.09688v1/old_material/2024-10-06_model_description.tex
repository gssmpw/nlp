The image autoencoder is a vector quantized variational auto-encoder (VQ-VAE) that operates on 3D image patches. To enable a high compression rate without sacrificing reconstruction quality,\cite{esser2020taming} we propose a stacked encoder-decoder design with low compression at each level but a high overall compression rate, as in Fig.~\ref{fig:architecture}a. Given a full-body CT image $\mathbf{X} \in \R^{H \times W \times D}$, let
\begin{equation}
  \label{eq:7}
  \mathbf{Z}_{\rm img} = \mathcal{E}_{\rm img}(\mathbf{X}) \text{~~~and~~~}
  \hat{\mathbf{X}} = \mathcal{D}_{\rm img}(\mathbf{Z}_{\mathrm{img}})
\end{equation} denote the patch-wise embedding and reconstruction, respectively, of the full-body image. The embedding $\mathbf{Z}_{\rm img}\in \R^{\frac{H}{16} \times \frac{W}{16} \times \frac{D}{16} \times E}$ consists of discrete codes downsampled by the cumulated compression rate, 16, with embedding dimension $E$.
The encoder $\mathcal{E}_{\rm img}$ and decoder $\mathcal{D}_{\rm img}$ use the architecture described in van den Oord \etal\cite{van2017neural} to perform vector quantization with $K$ codes, albeit with 3D convolutions instead of 2D. The final decoder uses the perceptual reconstruction loss $\mathcal{L}_{\rm rec}$ and adversarial discriminator loss $\mathcal{L}_{GAN}$ in Esser.\etal\cite{esser2020taming} Additionally, a multi-window L1 loss is introduced to re-scale the image output so that the soft tissue reconstruction contributes more equally to the loss gradient than it would otherwise. For two scalar voxel values $x, \hat{x} \in \R$, let
\begin{equation}
  \label{eq:8}
  \mathcal{L}_{\rm mw}(x, \hat{x}) =
  \begin{cases}
    \lambda_{\rm soft} |x - \hat{x}| & \text{if } x < \text{HU}_{\rm soft}\\
    \lambda_{\rm hard} |x - \hat{x}| & \text{else},
  \end{cases}
\end{equation}
where $\text{HU}_{\rm soft}$ is chosen as the upper boundary of soft tissue density values. We find that using stacked autoencoders and $\mathcal{L}_{\rm mw}$ in place of the L1 loss improves the peak signal to noise ratio (PSNR) of the reconstruction from 29.08 to 31.06.
The multi-window reconstruction loss also improves the anatomical consistency of soft tissue structures during both reconstruction and sampling.

The segmentation autoencoder is trained in a similar manner to the image autoencoder, using a single VQ-VAE with the same overall compression rate.
Given the full-body segmentation $\mathbf{Y} \in \R^{H \times W \times D}$, let
\begin{equation}
  \label{eq:9}
  \mathbf{Z}_{\rm seg} = \mathcal{E}_{\rm seg}(\mathbf{X}) \text{~~~and~~~}
  \hat{\mathbf{Y}} = \mathcal{D}_{\rm seg}(\mathbf{Z}_{\mathrm{seg}})
\end{equation} denote the patch-wise embedding and reconstruction, respectively, of the full-body segmentation.

encoder $\mathcal{E}_{\rm seg}$ and decoder $\mathcal{D}_{\rm seg}$ are trained to convergence using the DICE loss as the primary reconstruction loss, with perceptual and adversarial discriminator losses as well.\cite{esser2020taming} After convergence, the embeddings $\mathbf{Z}_{\mathrm{seg}}$ are stored to disk.

$\mathbf{x}_i \in \R^{h \times w \times d}$ denote the $i$\textsuperscript{th} 3D image patch. We use an overall compression rate of 16, so the final patch embedding and reconstruction are
\begin{equation}
    \label{eq:6}
    \mathbf{z}_{\mathrm{img},i}^{(1)} = \mathcal{E}_{\rm img}^{(1)}(\mathbf{x})
  \text{~~~and~~~} \hat{\mathbf{x}}_i^{(1)} = \mathcal{D}_{\rm img}^{(1)}(\mathbf{z}_{\mathrm{img},i}^{(1)}).
\end{equation}

The image autoencoder is a vector quantized variational auto-encoder (VQVAE)\cite{van2017neural} that operates on 3D image patches.
To enable a high compression rate without sacrificing reconstruction quality,\cite{esser2020taming} we propose a stacked encoder-decoder design with low compression rates at each level but a high overall compression rate, as in Fig~\ref{fig:architecture}a.

Given a full-body CT image $\mathbf{X}$ in the training set, let $\mathbf{x}_i \in \R^{h \times w \times d}$ denote the $i$\textsuperscript{th} 3D image patch.
The first image patch embedding $\mathbf{z}_{\mathrm{img},i}^{(1)}$ and reconstruction $\hat{\mathbf{x}}_i^{(1)}$ are given by
\begin{align}
  \label{eq:2}
  \mathbf{z}_{\mathrm{img},i}^{(1)} = \mathcal{E}_{\rm img}^{(1)}(\mathbf{x})
  \text{~~~and~~~} \hat{\mathbf{x}}_i^{(1)} = \mathcal{D}_{\rm img}^{(1)}(\mathbf{z}_{\mathrm{img},i}^{(1)}).
\end{align}
The encoder $\mathcal{E}_{\rm img}^{(1)}$ and decoder $\mathcal{D}_{\rm img}^{(l)}$ use the architecture described in van den Oord \etal\cite{van2017neural} with 3D instead of 2D convolutions, as well as the perceptual reconstruction loss $\mathcal{L}_{\rm rec}$ and adversarial discriminator loss $\mathcal{L}_{GAN}$ in Esser \etal.\cite{esser2020taming}
Additionally, a multi-window L1 loss is introduced. This simple modification re-scales the image output so that the soft tissue reconstruction contributes more equally to the loss gradient---compared to hard tissue reconstruction---than it would otherwise. For two scalar voxel values $x, \hat{x} \in \R$, let
\begin{align}
  \label{eq:3}
  \mathcal{L}_{\rm mw}(x, \hat{x}) =
  \begin{cases}
    \lambda_{\rm soft} |x - \hat{x}| & \text{if } x < \text{HU}_{\rm soft}\\
    \lambda_{\rm hard} |x - \hat{x}| & \text{else}
  \end{cases},
\end{align} where $\text{HU}_{\rm soft}$ is chosen as the upper boundary of soft tissue density values. Then $\mathcal{L}_{\rm mw}(\mathbf{x}_i, \hat{\mathbf{x}}_i)$ is the average over the patch. These losses are summed and weighted as described in Section~\todo{sec}.
After training $\mathcal{E}_{\rm img}^{(1)}$ and $\mathcal{D}_{\rm img}^{(l)}$ to convergence, a second VQVAE $\left(\mathcal{E}_{\rm img}^{(2)}, \mathcal{D}_{\rm img}^{(2)}\right)$, is trained to reconstruct highly compressed feature embeddings $\mathbf{z}_{\mathrm{img},i}^{(2)}$. These have the same architecture but are trained using only mean-squared error (MSE) for the loss function. Additional VQVAE layers might be used for further compression, allowing for higher resolution images to be represented in our framework. After convergence, we denote the final composition of embeddings $\mathbf{z}_{\mathrm{img},i} = \mathcal{E}_{\rm img}(\mathbf{x}_i)$ and reconstructions $\hat{\mathbf{x}}_{i} = \mathcal{D}_{\rm img}(\mathbf{z}_{\mathrm{img},i})$.

To encourage anatomical consistency, we separately encode the full-body image segmentation $\mathbf{Y}$ in a patch-wise manner.
Similar to above, we denote the segmentation patch as $\mathbf{y}_i \in \R^{h \times w \times d \times C}$, where $C$ is the number of classes, and the encoder and decoder as
\begin{equation}
  \label{eq:5}
  \mathbf{z}_{\mathrm{seg}, i} = \mathcal{E}_{\rm seg}(\mathbf{x}_i) \text{~~~and~~~} \mathbf{y}_i = \mathcal{D}_{\rm seg}(\mathbf{z}_{\mathrm{seg}, i}).
\end{equation}
These are trained to convergence using DICE loss as the primary reconstruction loss, with perceptual and adversarial discriminator losses as well.\cite{esser2020taming} After convergence, the embeddings $\mathbf{z}_{\mathrm{seg},i}$ are stored to disk.
