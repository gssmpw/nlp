\section{Related Work}
\label{sec:related}

Large-scale digitisation efforts such as Chronicling America ____ have made vast collections of historical U.S. newspaper scans available, prompting the development of methods to transform raw page images into structured text corpora. Early pipeline designs emphasized layout segmentation and OCR; for example, Tesseract has been widely used for text extraction from scanned pages, while more specialized Transformer-based OCR approaches ____ have recently emerged to tackle noisier archives. These processes typically produce article-level content by detecting bounding boxes for headlines and paragraphs.

Once article texts are extracted, another important challenge is reproduced-text detection, essential for identifying syndicated material like wire stories. Earlier approaches, such as the Viral Texts project ____, relied on n-gram overlaps to uncover reprinted content within historical collections. More recent methods leverage neural encoders to map near-duplicate articles into similar vector representations, improving robustness to noise or minor edits. Contrastive training of bi-encoder models has proven especially effective ____, as illustrated by Dell and colleaguesâ€™ wire-article clustering pipeline. Furthermore, large language models (LLMs) now offer a potent means of post-OCR text cleanup, reducing character-level error rates and enhancing downstream tasks like entity recognition ____. Combined, these advances in layout segmentation, deduplication, and correction underpin the construction of high-quality historical text datasets, including wire-focused resources such as \texttt{Headlines} ____, \texttt{Newswire} ____, and the present \texttt{Southern Newswire Corpus}, which applies these methods to an expanded set of newspapers and services while adding a fully corrected text option.