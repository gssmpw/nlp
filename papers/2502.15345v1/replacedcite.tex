\section{Related Work}
% \textbf{Reinforcement Learning}

Dynamic Programming and Reinforcement Learning have been well studied for decades with numerous applications in industry and finance. For further details, see textbooks ____ and surveys ____.
% \textbf{Sample Complexity for Solving DMDPs}

Solving MDPs and finding optimal policies efficiently is one of the most classic problems in dynamic programming and reinforcement learning. 
% Traditional deterministic methods, such as value iteration, policy iteration, and linear programming ____ can compute (approximately) optimal policies when the entire MDP is given. 
% To relax the requirement of knowing the whole MDP, 
Recent research focuses on this specific problem with access only to a generative model for sampling state transitions. Both DMDP and AMDP have been well-studied under this assumption. For DMDP, ____ provide a sample complexity lower bound of $\tilde{\Omega}((1-\gamma)^{-3} N \epsilon^{-2})$. There are several works that match this bound, including value iteration ____, Q-learning ____, and Empirical MDP with Blackbox ____.
However, no primal-dual algorithms have yet achieved the optimal bound. ____ proposes a randomized primal-dual method with sample complexity $\tilde{\Omega}(\min\{(1-\gamma)^{-6} |\mathcal{S}|^2N \epsilon^{-2},\tau^{4}(1-\gamma)^{-4} N \epsilon^{-2}\})$, where $\tau$ is an ergodic condition parameter. 
____ achieve $\tilde{O}((1-\gamma)^{-4} N \epsilon^{-2})$ using mirror descent type algorithms, respectively, representing the best results for primal-dual methods. Our work is closely related to ____, and we provide more discussions in Appendix \ref{sec:app-com-sidford-efficient}.
Whether convex optimization methods can achieve the optimal sample complexity remains an open problem.
Our work takes a different approach: We explore whether incorporating predictions of the transition matrix can enhance primal-dual methods and improve sample complexity bounds. To the best of our knowledge, this is the first work to introduce the notion of prediction into solving MDPs with access only to a generative model.

% ____ designs a stochastic mirror descent algorithm for both DMDP and AMDP and achieves $\tilde{O}((1-\gamma)^{-4} N \epsilon^{-2})$ , currently the best achievable sample complexity bound by primal-dual type methods. ____ achieves the same result with online mirror descent. 
% It is still an open problem that whether convex optimization type method can achieve the optimal sample complexity bound. Our paper goes in an orthogonal direction. We are interested in whether additional information in terms of prediction on transition matrix can enhance primal-dual type method and improve sample complexity bound. To the best of our knowledge, our work is the first one that introduce the notion of prediction into solving MDPs with only a generative model.
% There are several works that match the sample complexity lower bound, such as variance-reduced Q-value iteration ____, variance-reduced Q-learning ____, and empirical MDP with black-box ____. However, their methods are based on value iteration or Q-learning, and there are no primal-dual type algorithms achieve the optimal sample complexity bound.

% \textbf{Prediction on Bandits and RL}

Our work closely relates to reinforcement learning with prediction. ____ consider tabular MDPs with advice in the form of prediction of the Q-value and shows that this information can reduce regret. ____ study single-trajectory time-varying MDPs with untrusted machine-learned prediction. ____ leverages hint in the form of guessed optimal actions under a stochastic linear bandit setting. ____ explore how dynamic predictions can enhance algorithm performance regarding regret in a non-stationary bandit with knapsack problem. However, all these works use regret as the primary performance metric. In contrast, our problem is the first to focus on utilizing black-box predictions to reduce unnecessary exploration and improve sample complexity bound in solving MDPs and reinforcement learning. Our work aligns with the research stream of Algorithm with Advice ____ in general, and we provide more discussions in Appendix \ref{sec:app-com-alg-advice}.

% \textbf{Online Algorithm with Advice / Online Learning with Prediction}

% ____ on convex optimization

% Regarding the prediction model, our work aligns with the research stream of (Online) Algorithm with Black-box Advice ____. Various specific online problems have been explored, including caching ____, online resource allocation ____, online matching ____, online secretary ____, and convex optimization ____. Another related line of research considers online optimization with predictions, aiming to decrease regret with methods like optimistic mirror descent, where the predictions are provided sequentially. Full feedback model are studied in ____ and contextual bandit is studied in ____. For more reference, see \url{https://algorithms-with-predictions.github.io/}.
% Our work is the first to investigate leveraging predictions to enhance learning MDPs. Furthermore, we draw inspiration from algorithmic techniques in online optimization with sequential predictions, such as optimistic mirror descent, to effectively incorporate black-box predictions.

% \textbf{Parameter-free Algorithm}

Our algorithmic design is inspired by the parameter-free methods and adaptive learning rates from deterministic optimization. ____ develop a parameter-free stochastic gradient descent algorithm that achieves a double-logarithmic factor overhead compared to the optimal rate in the known-parameter setting. ____ introduces adaptive learning rates for online gradient descent. Interested readers can consult ____ for more references.

% \textbf{Offline RL}

Finally, our work is related to the line of Offline Reinforcement Learning ____. In this setting, the learning algorithm cannot interact with the environment and collect additional information about the model. Instead, it is provided by a static dataset of transitions and must learn the best policy based on this dataset. Various approaches have been developed for this problem, such as ____. While such datasets could potentially be used to construct a prediction transition matrix (e.g., via sample mean), these methods cannot apply to our setting, as our model allows for interaction with the environment through sampling state transitions. Furthermore, these approaches typically leverage datasets in ways far from estimating the transition matrix, limiting their applicability to our problem, where no dataset is available in advance.

% \textbf{MDP with external feedback}

% \textbf{Transfer Learning in RL}