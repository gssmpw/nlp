\section{Related Work}
% \textbf{Reinforcement Learning}

Dynamic Programming and Reinforcement Learning have been well studied for decades with numerous applications in industry and finance. For further details, see textbooks \cite{bellman1966dynamic,bertsekas1996neuro,bertsekas2012dynamic,bertsekas2022abstract,sutton2018reinforcement,szepesvari2022algorithms} and surveys \cite{kaelbling1996reinforcement,arulkumaran2017deep}.
% \textbf{Sample Complexity for Solving DMDPs}

Solving MDPs and finding optimal policies efficiently is one of the most classic problems in dynamic programming and reinforcement learning. 
% Traditional deterministic methods, such as value iteration, policy iteration, and linear programming \cite{puterman2014markov,bertsekas2012dynamic} can compute (approximately) optimal policies when the entire MDP is given. 
% To relax the requirement of knowing the whole MDP, 
Recent research focuses on this specific problem with access only to a generative model for sampling state transitions. Both DMDP and AMDP have been well-studied under this assumption. For DMDP, \cite{gheshlaghi2013minimax} provide a sample complexity lower bound of $\tilde{\Omega}((1-\gamma)^{-3} N \epsilon^{-2})$. There are several works that match this bound, including value iteration \cite{sidford2018near}, Q-learning \cite{wainwright2019variance}, and Empirical MDP with Blackbox \cite{agarwal2020model}.
However, no primal-dual algorithms have yet achieved the optimal bound. \cite{wang2020randomized} proposes a randomized primal-dual method with sample complexity $\tilde{\Omega}(\min\{(1-\gamma)^{-6} |\mathcal{S}|^2N \epsilon^{-2},\tau^{4}(1-\gamma)^{-4} N \epsilon^{-2}\})$, where $\tau$ is an ergodic condition parameter. 
\cite{jin2020efficiently,cheng2020reduction} achieve $\tilde{O}((1-\gamma)^{-4} N \epsilon^{-2})$ using mirror descent type algorithms, respectively, representing the best results for primal-dual methods. Our work is closely related to \cite{jin2020efficiently}, and we provide more discussions in Appendix \ref{sec:app-com-sidford-efficient}.
Whether convex optimization methods can achieve the optimal sample complexity remains an open problem.
Our work takes a different approach: We explore whether incorporating predictions of the transition matrix can enhance primal-dual methods and improve sample complexity bounds. To the best of our knowledge, this is the first work to introduce the notion of prediction into solving MDPs with access only to a generative model.

% \cite{jin2020efficiently} designs a stochastic mirror descent algorithm for both DMDP and AMDP and achieves $\tilde{O}((1-\gamma)^{-4} N \epsilon^{-2})$ , currently the best achievable sample complexity bound by primal-dual type methods. \cite{cheng2020reduction} achieves the same result with online mirror descent. 
% It is still an open problem that whether convex optimization type method can achieve the optimal sample complexity bound. Our paper goes in an orthogonal direction. We are interested in whether additional information in terms of prediction on transition matrix can enhance primal-dual type method and improve sample complexity bound. To the best of our knowledge, our work is the first one that introduce the notion of prediction into solving MDPs with only a generative model.
% There are several works that match the sample complexity lower bound, such as variance-reduced Q-value iteration \cite{sidford2018near}, variance-reduced Q-learning \cite{wainwright2019variance}, and empirical MDP with black-box \cite{agarwal2020model}. However, their methods are based on value iteration or Q-learning, and there are no primal-dual type algorithms achieve the optimal sample complexity bound.

% \textbf{Prediction on Bandits and RL}

Our work closely relates to reinforcement learning with prediction. \cite{golowich2022can} consider tabular MDPs with advice in the form of prediction of the Q-value and shows that this information can reduce regret. \cite{li2024beyond} study single-trajectory time-varying MDPs with untrusted machine-learned prediction. \cite{cutkosky2022leveraging} leverages hint in the form of guessed optimal actions under a stochastic linear bandit setting. \cite{lyu2023bandits} explore how dynamic predictions can enhance algorithm performance regarding regret in a non-stationary bandit with knapsack problem. However, all these works use regret as the primary performance metric. In contrast, our problem is the first to focus on utilizing black-box predictions to reduce unnecessary exploration and improve sample complexity bound in solving MDPs and reinforcement learning. Our work aligns with the research stream of Algorithm with Advice \cite{mitzenmacher2022algorithms} in general, and we provide more discussions in Appendix \ref{sec:app-com-alg-advice}.

% \textbf{Online Algorithm with Advice / Online Learning with Prediction}

% \cite{christianson2022chasing} on convex optimization

% Regarding the prediction model, our work aligns with the research stream of (Online) Algorithm with Black-box Advice \cite{mitzenmacher2022algorithms,purohit2018improving}. Various specific online problems have been explored, including caching \cite{lykouris2021competitive,rohatgi2020near}, online resource allocation \cite{balseiro2022single,golrezaei2023online}, online matching \cite{jin2022online}, online secretary \cite{antoniadis2020secretary,dutting2021secretaries}, and convex optimization \cite{christianson2022chasing}. Another related line of research considers online optimization with predictions, aiming to decrease regret with methods like optimistic mirror descent, where the predictions are provided sequentially. Full feedback model are studied in \cite{rakhlin2013online,rakhlin2013optimization,jadbabaie2015online} and contextual bandit is studied in \cite{wei2020taking}. For more reference, see \url{https://algorithms-with-predictions.github.io/}.
% Our work is the first to investigate leveraging predictions to enhance learning MDPs. Furthermore, we draw inspiration from algorithmic techniques in online optimization with sequential predictions, such as optimistic mirror descent, to effectively incorporate black-box predictions.

% \textbf{Parameter-free Algorithm}

Our algorithmic design is inspired by the parameter-free methods and adaptive learning rates from deterministic optimization. \cite{carmon2022making} develop a parameter-free stochastic gradient descent algorithm that achieves a double-logarithmic factor overhead compared to the optimal rate in the known-parameter setting. \cite{streeter2010less} introduces adaptive learning rates for online gradient descent. Interested readers can consult \cite{auer2002adaptive,orabona2014simultaneous,cutkosky2018black,orabona2019modern} for more references.

% \textbf{Offline RL}

Finally, our work is related to the line of Offline Reinforcement Learning \cite{levine2020offline}. In this setting, the learning algorithm cannot interact with the environment and collect additional information about the model. Instead, it is provided by a static dataset of transitions and must learn the best policy based on this dataset. Various approaches have been developed for this problem, such as \cite{fujimoto2019off,kumar2019stabilizing,kumar2020conservative,agarwal2020optimistic,wu2019behavior}. While such datasets could potentially be used to construct a prediction transition matrix (e.g., via sample mean), these methods cannot apply to our setting, as our model allows for interaction with the environment through sampling state transitions. Furthermore, these approaches typically leverage datasets in ways far from estimating the transition matrix, limiting their applicability to our problem, where no dataset is available in advance.

% \textbf{MDP with external feedback}

% \textbf{Transfer Learning in RL}