@inproceedings{carmon2022making,
  title={Making sgd parameter-free},
  author={Carmon, Yair and Hinder, Oliver},
  booktitle={Conference on Learning Theory},
  pages={2360--2389},
  year={2022},
  organization={PMLR}
}

@inproceedings{jin2020efficiently,
  title={Efficiently solving MDPs with stochastic mirror descent},
  author={Jin, Yujia and Sidford, Aaron},
  booktitle={International Conference on Machine Learning},
  pages={4890--4900},
  year={2020},
  organization={PMLR}
}

@inproceedings{jin2021towards,
  title={Towards tight bounds on the sample complexity of average-reward MDPs},
  author={Jin, Yujia and Sidford, Aaron},
  booktitle={International Conference on Machine Learning},
  pages={5055--5064},
  year={2021},
  organization={PMLR}
}

@article{li2024breaking,
  title={Breaking the sample size barrier in model-based reinforcement learning with a generative model},
  author={Li, Gen and Wei, Yuting and Chi, Yuejie and Chen, Yuxin},
  journal={Operations Research},
  volume={72},
  number={1},
  pages={203--221},
  year={2024},
  publisher={INFORMS}
}

@article{sidford2018near,
  title={Near-optimal time and sample complexities for solving Markov decision processes with a generative model},
  author={Sidford, Aaron and Wang, Mengdi and Wu, Xian and Yang, Lin and Ye, Yinyu},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@article{wang2020randomized,
  title={Randomized linear programming solves the markov decision problem in nearly linear (sometimes sublinear) time},
  author={Wang, Mengdi},
  journal={Mathematics of Operations Research},
  volume={45},
  number={2},
  pages={517--546},
  year={2020},
  publisher={INFORMS}
}

@article{wang2017primal,
  title={Primal-Dual pi Learning: Sample Complexity and Sublinear Run Time for Ergodic Markov Decision Problems},
  author={Wang, Mengdi},
  journal={arXiv preprint arXiv:1710.06100},
  year={2017}
}

@book{puterman2014markov,
  title={Markov decision processes: discrete stochastic dynamic programming},
  author={Puterman, Martin L},
  year={2014},
  publisher={John Wiley \& Sons}
}

@book{bertsekas2012dynamic,
  title={Dynamic programming and optimal control: Volume I},
  author={Bertsekas, Dimitri},
  volume={4},
  year={2012},
  publisher={Athena scientific}
}

@inproceedings{cheng2020reduction,
  title={A reduction from reinforcement learning to no-regret online learning},
  author={Cheng, Ching-An and Combes, Remi Tachet and Boots, Byron and Gordon, Geoff},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3514--3524},
  year={2020},
  organization={PMLR}
}

@article{sidford2023variance,
  title={Variance reduced value iteration and faster algorithms for solving Markov decision processes},
  author={Sidford, Aaron and Wang, Mengdi and Wu, Xian and Ye, Yinyu},
  journal={Naval Research Logistics (NRL)},
  volume={70},
  number={5},
  pages={423--442},
  year={2023},
  publisher={Wiley Online Library}
}

@article{jiang2020online,
  title={Online stochastic optimization with wasserstein based non-stationarity},
  author={Jiang, Jiashuo and Li, Xiaocheng and Zhang, Jiawei},
  journal={arXiv preprint arXiv:2012.06961},
  year={2020}
}

@inproceedings{agarwal2020model,
  title={Model-based reinforcement learning with a generative model is minimax optimal},
  author={Agarwal, Alekh and Kakade, Sham and Yang, Lin F},
  booktitle={Conference on Learning Theory},
  pages={67--83},
  year={2020},
  organization={PMLR}
}

@article{wainwright2019variance,
  title={Variance-reduced $ Q $-learning is minimax optimal},
  author={Wainwright, Martin J},
  journal={arXiv preprint arXiv:1906.04697},
  year={2019}
}

@article{orabona2019modern,
  title={A modern introduction to online learning},
  author={Orabona, Francesco},
  journal={arXiv preprint arXiv:1912.13213},
  year={2019}
}

@book{lattimore2020bandit,
  title={Bandit algorithms},
  author={Lattimore, Tor and Szepesv{\'a}ri, Csaba},
  year={2020},
  publisher={Cambridge University Press}
}

@book{bertsekas2022abstract,
  title={Abstract dynamic programming},
  author={Bertsekas, Dimitri},
  year={2022},
  publisher={Athena Scientific}
}

@article{bertsekas1996neuro,
  title={Neuro-dynamic programming},
  author={Bertsekas, DP},
  journal={Athena Scientific},
  year={1996}
}

@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

@article{gheshlaghi2013minimax,
  title={Minimax PAC bounds on the sample complexity of reinforcement learning with a generative model},
  author={Gheshlaghi Azar, Mohammad and Munos, R{\'e}mi and Kappen, Hilbert J},
  journal={Machine learning},
  volume={91},
  pages={325--349},
  year={2013},
  publisher={Springer}
}

@article{mitzenmacher2022algorithms,
  title={Algorithms with predictions},
  author={Mitzenmacher, Michael and Vassilvitskii, Sergei},
  journal={Communications of the ACM},
  volume={65},
  number={7},
  pages={33--35},
  year={2022},
  publisher={ACM New York, NY, USA}
}

@inproceedings{golowich2022can,
  title={Can Q-learning be improved with advice?},
  author={Golowich, Noah and Moitra, Ankur},
  booktitle={Conference on Learning Theory},
  pages={4548--4619},
  year={2022},
  organization={PMLR}
}

@inproceedings{rakhlin2013online,
  title={Online learning with predictable sequences},
  author={Rakhlin, Alexander and Sridharan, Karthik},
  booktitle={Conference on Learning Theory},
  pages={993--1019},
  year={2013},
  organization={PMLR}
}

@inproceedings{joulani2017modular,
  title={A modular analysis of adaptive (non-) convex optimization: Optimism, composite objectives, and variational bounds},
  author={Joulani, Pooria and Gy{\"o}rgy, Andr{\'a}s and Szepesv{\'a}ri, Csaba},
  booktitle={International Conference on Algorithmic Learning Theory},
  pages={681--720},
  year={2017},
  organization={PMLR}
}

@article{osband2013more,
  title={(More) efficient reinforcement learning via posterior sampling},
  author={Osband, Ian and Russo, Daniel and Van Roy, Benjamin},
  journal={Advances in Neural Information Processing Systems},
  volume={26},
  year={2013}
}

@article{strehl2008analysis,
  title={An analysis of model-based interval estimation for Markov decision processes},
  author={Strehl, Alexander L and Littman, Michael L},
  journal={Journal of Computer and System Sciences},
  volume={74},
  number={8},
  pages={1309--1331},
  year={2008},
  publisher={Elsevier}
}

@inproceedings{strehl2004empirical,
  title={An empirical evaluation of interval estimation for markov decision processes},
  author={Strehl, Alexander L and Littman, Michael L},
  booktitle={16th IEEE International Conference on Tools with Artificial Intelligence},
  pages={128--135},
  year={2004},
  organization={IEEE}
}

@inproceedings{todorov2012mujoco,
  title={Mujoco: A physics engine for model-based control},
  author={Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  booktitle={2012 IEEE/RSJ international conference on intelligent robots and systems},
  pages={5026--5033},
  year={2012},
  organization={IEEE}
}

@article{lazaridis2020deep,
  title={Deep reinforcement learning: A state-of-the-art walkthrough},
  author={Lazaridis, Aristotelis and Fachantidis, Anestis and Vlahavas, Ioannis},
  journal={Journal of Artificial Intelligence Research},
  volume={69},
  pages={1421--1471},
  year={2020}
}

@inproceedings{tessler2019action,
  title={Action robust reinforcement learning and applications in continuous control},
  author={Tessler, Chen and Efroni, Yonathan and Mannor, Shie},
  booktitle={International Conference on Machine Learning},
  pages={6215--6224},
  year={2019},
  organization={PMLR}
}

@article{weng2022tianshou,
  title={Tianshou: A highly modularized deep reinforcement learning library},
  author={Weng, Jiayi and Chen, Huayu and Yan, Dong and You, Kaichao and Duburcq, Alexis and Zhang, Minghao and Su, Yi and Su, Hang and Zhu, Jun},
  journal={Journal of Machine Learning Research},
  volume={23},
  number={267},
  pages={1--6},
  year={2022}
}

@book{szepesvari2022algorithms,
  title={Algorithms for reinforcement learning},
  author={Szepesv{\'a}ri, Csaba},
  year={2022},
  publisher={Springer nature}
}

@article{kaelbling1996reinforcement,
  title={Reinforcement learning: A survey},
  author={Kaelbling, Leslie Pack and Littman, Michael L and Moore, Andrew W},
  journal={Journal of artificial intelligence research},
  volume={4},
  pages={237--285},
  year={1996}
}

@article{arulkumaran2017deep,
  title={Deep reinforcement learning: A brief survey},
  author={Arulkumaran, Kai and Deisenroth, Marc Peter and Brundage, Miles and Bharath, Anil Anthony},
  journal={IEEE Signal Processing Magazine},
  volume={34},
  number={6},
  pages={26--38},
  year={2017},
  publisher={IEEE}
}

@article{bellman1966dynamic,
  title={Dynamic programming},
  author={Bellman, Richard},
  journal={science},
  volume={153},
  number={3731},
  pages={34--37},
  year={1966},
  publisher={American Association for the Advancement of Science}
}

@book{cormen2022introduction,
  title={Introduction to algorithms},
  author={Cormen, Thomas H and Leiserson, Charles E and Rivest, Ronald L and Stein, Clifford},
  year={2022},
  publisher={MIT press}
}

@inproceedings{christianson2022chasing,
  title={Chasing convex bodies and functions with black-box advice},
  author={Christianson, Nicolas and Handina, Tinashe and Wierman, Adam},
  booktitle={Conference on Learning Theory},
  pages={867--908},
  year={2022},
  organization={PMLR}
}

@article{li2024beyond,
  title={Beyond black-box advice: learning-augmented algorithms for MDPs with Q-value predictions},
  author={Li, Tongxin and Lin, Yiheng and Ren, Shaolei and Wierman, Adam},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{purohit2018improving,
  title={Improving online algorithms via ML predictions},
  author={Purohit, Manish and Svitkina, Zoya and Kumar, Ravi},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@inproceedings{cutkosky2022leveraging,
  title={Leveraging initial hints for free in stochastic linear bandits},
  author={Cutkosky, Ashok and Dann, Chris and Das, Abhimanyu and Zhang, Qiuyi},
  booktitle={International Conference on Algorithmic Learning Theory},
  pages={282--318},
  year={2022},
  organization={PMLR}
}

@inproceedings{lyu2023bandits,
  title={Bandits with knapsacks: advice on time-varying demands},
  author={Lyu, Lixing and Cheung, Wang Chi},
  booktitle={International Conference on Machine Learning},
  pages={23212--23238},
  year={2023},
  organization={PMLR}
}

@article{lykouris2021competitive,
  title={Competitive caching with machine learned advice},
  author={Lykouris, Thodoris and Vassilvitskii, Sergei},
  journal={Journal of the ACM (JACM)},
  volume={68},
  number={4},
  pages={1--25},
  year={2021},
  publisher={ACM New York, NY}
}

@inproceedings{rohatgi2020near,
  title={Near-optimal bounds for online caching with machine learned advice},
  author={Rohatgi, Dhruv},
  booktitle={Proceedings of the Fourteenth Annual ACM-SIAM Symposium on Discrete Algorithms},
  pages={1834--1845},
  year={2020},
  organization={SIAM}
}

@article{balseiro2022single,
  title={Single-leg revenue management with advice},
  author={Balseiro, Santiago and Kroer, Christian and Kumar, Rachitesh},
  journal={arXiv preprint arXiv:2202.10939},
  year={2022}
}

@article{golrezaei2023online,
  title={Online resource allocation with convex-set machine-learned advice},
  author={Golrezaei, Negin and Jaillet, Patrick and Zhou, Zijie},
  journal={arXiv preprint arXiv:2306.12282},
  year={2023}
}

@article{jin2022online,
  title={Online bipartite matching with advice: Tight robustness-consistency tradeoffs for the two-stage model},
  author={Jin, Billy and Ma, Will},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={14555--14567},
  year={2022}
}

@article{antoniadis2020secretary,
  title={Secretary and online matching problems with machine learned advice},
  author={Antoniadis, Antonios and Gouleakis, Themis and Kleer, Pieter and Kolev, Pavel},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={7933--7944},
  year={2020}
}

@inproceedings{dutting2021secretaries,
  title={Secretaries with advice},
  author={D{\"u}tting, Paul and Lattanzi, Silvio and Paes Leme, Renato and Vassilvitskii, Sergei},
  booktitle={Proceedings of the 22nd ACM Conference on Economics and Computation},
  pages={409--429},
  year={2021}
}

@article{rakhlin2013optimization,
  title={Optimization, learning, and games with predictable sequences},
  author={Rakhlin, Sasha and Sridharan, Karthik},
  journal={Advances in Neural Information Processing Systems},
  volume={26},
  year={2013}
}

@inproceedings{steinhardt2014adaptivity,
  title={Adaptivity and optimism: An improved exponentiated gradient algorithm},
  author={Steinhardt, Jacob and Liang, Percy},
  booktitle={International conference on machine learning},
  pages={1593--1601},
  year={2014},
  organization={PMLR}
}

@inproceedings{jadbabaie2015online,
  title={Online optimization: Competing with dynamic comparators},
  author={Jadbabaie, Ali and Rakhlin, Alexander and Shahrampour, Shahin and Sridharan, Karthik},
  booktitle={Artificial Intelligence and Statistics},
  pages={398--406},
  year={2015},
  organization={PMLR}
}

@inproceedings{wei2020taking,
  title={Taking a hint: How to leverage loss predictors in contextual bandits?},
  author={Wei, Chen-Yu and Luo, Haipeng and Agarwal, Alekh},
  booktitle={Conference on Learning Theory},
  pages={3583--3634},
  year={2020},
  organization={PMLR}
}

@article{streeter2010less,
  title={Less regret via online conditioning},
  author={Streeter, Matthew and McMahan, H Brendan},
  journal={arXiv preprint arXiv:1002.4862},
  year={2010}
}

@article{auer2002adaptive,
  title={Adaptive and self-confident on-line learning algorithms},
  author={Auer, Peter and Cesa-Bianchi, Nicolo and Gentile, Claudio},
  journal={Journal of Computer and System Sciences},
  volume={64},
  number={1},
  pages={48--75},
  year={2002},
  publisher={Elsevier}
}

@article{orabona2014simultaneous,
  title={Simultaneous model selection and optimization through parameter-free stochastic learning},
  author={Orabona, Francesco},
  journal={Advances in Neural Information Processing Systems},
  volume={27},
  year={2014}
}

@inproceedings{cutkosky2018black,
  title={Black-box reductions for parameter-free online learning in banach spaces},
  author={Cutkosky, Ashok and Orabona, Francesco},
  booktitle={Conference On Learning Theory},
  pages={1493--1529},
  year={2018},
  organization={PMLR}
}

@article{levine2020offline,
  title={Offline reinforcement learning: Tutorial, review, and perspectives on open problems},
  author={Levine, Sergey and Kumar, Aviral and Tucker, George and Fu, Justin},
  journal={arXiv preprint arXiv:2005.01643},
  year={2020}
}

@inproceedings{fujimoto2019off,
  title={Off-policy deep reinforcement learning without exploration},
  author={Fujimoto, Scott and Meger, David and Precup, Doina},
  booktitle={International conference on machine learning},
  pages={2052--2062},
  year={2019},
  organization={PMLR}
}

@article{kumar2020conservative,
  title={Conservative q-learning for offline reinforcement learning},
  author={Kumar, Aviral and Zhou, Aurick and Tucker, George and Levine, Sergey},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1179--1191},
  year={2020}
}

@article{kumar2019stabilizing,
  title={Stabilizing off-policy q-learning via bootstrapping error reduction},
  author={Kumar, Aviral and Fu, Justin and Soh, Matthew and Tucker, George and Levine, Sergey},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{agarwal2020optimistic,
  title={An optimistic perspective on offline reinforcement learning},
  author={Agarwal, Rishabh and Schuurmans, Dale and Norouzi, Mohammad},
  booktitle={International conference on machine learning},
  pages={104--114},
  year={2020},
  organization={PMLR}
}

@article{wu2019behavior,
  title={Behavior regularized offline reinforcement learning},
  author={Wu, Yifan and Tucker, George and Nachum, Ofir},
  journal={arXiv preprint arXiv:1911.11361},
  year={2019}
}

@inproceedings{wei2018more,
  title={More adaptive algorithms for adversarial bandits},
  author={Wei, Chen-Yu and Luo, Haipeng},
  booktitle={Conference On Learning Theory},
  pages={1263--1291},
  year={2018},
  organization={PMLR}
}

@article{lattimore2015pareto,
  title={The pareto regret frontier for bandits},
  author={Lattimore, Tor},
  journal={Advances in Neural Information Processing Systems},
  volume={28},
  year={2015}
}

@inproceedings{zhao2020sim,
  title={Sim-to-real transfer in deep reinforcement learning for robotics: a survey},
  author={Zhao, Wenshuai and Queralta, Jorge Pe{\~n}a and Westerlund, Tomi},
  booktitle={2020 IEEE symposium series on computational intelligence (SSCI)},
  pages={737--744},
  year={2020},
  organization={IEEE}
}

@inproceedings{peng2018sim,
  title={Sim-to-real transfer of robotic control with dynamics randomization},
  author={Peng, Xue Bin and Andrychowicz, Marcin and Zaremba, Wojciech and Abbeel, Pieter},
  booktitle={2018 IEEE international conference on robotics and automation (ICRA)},
  pages={3803--3810},
  year={2018},
  organization={IEEE}
}

@article{collins2021review,
  title={A review of physics simulators for robotic applications},
  author={Collins, Jack and Chand, Shelvin and Vanderkop, Anthony and Howard, David},
  journal={IEEE Access},
  volume={9},
  pages={51416--51431},
  year={2021},
  publisher={IEEE}
}

@article{yu2021reinforcement,
  title={Reinforcement learning in healthcare: A survey},
  author={Yu, Chao and Liu, Jiming and Nemati, Shamim and Yin, Guosheng},
  journal={ACM Computing Surveys (CSUR)},
  volume={55},
  number={1},
  pages={1--36},
  year={2021},
  publisher={ACM New York, NY}
}

@article{chen2022data,
  title={Data-pooling reinforcement learning for personalized healthcare intervention},
  author={Chen, Xinyun and Shi, Pengyi and Pu, Shanwen},
  journal={arXiv preprint arXiv:2211.08998},
  year={2022}
}

@article{liao2020personalized,
  title={Personalized heartsteps: A reinforcement learning algorithm for optimizing physical activity},
  author={Liao, Peng and Greenewald, Kristjan and Klasnja, Predrag and Murphy, Susan},
  journal={Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
  volume={4},
  number={1},
  pages={1--22},
  year={2020},
  publisher={ACM New York, NY, USA}
}

@inproceedings{mate2022field,
  title={Field study in deploying restless multi-armed bandits: Assisting non-profits in improving maternal and child health},
  author={Mate, Aditya and Madaan, Lovish and Taneja, Aparna and Madhiwalla, Neha and Verma, Shresth and Singh, Gargi and Hegde, Aparna and Varakantham, Pradeep and Tambe, Milind},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  number={11},
  pages={12017--12025},
  year={2022}
}

@article{taylor2009transfer,
  title={Transfer learning for reinforcement learning domains: A survey.},
  author={Taylor, Matthew E and Stone, Peter},
  journal={Journal of Machine Learning Research},
  volume={10},
  number={7},
  year={2009}
}

@article{zhu2023transfer,
  title={Transfer learning in deep reinforcement learning: A survey},
  author={Zhu, Zhuangdi and Lin, Kaixiang and Jain, Anil K and Zhou, Jiayu},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2023},
  publisher={IEEE}
}

@article{hua2021learning,
  title={Learning for a robot: Deep reinforcement learning, imitation learning, transfer learning},
  author={Hua, Jiang and Zeng, Liangcai and Li, Gongfa and Ju, Zhaojie},
  journal={Sensors},
  volume={21},
  number={4},
  pages={1278},
  year={2021},
  publisher={MDPI}
}

@inproceedings{gamrian2019transfer,
  title={Transfer learning for related reinforcement learning tasks via image-to-image translation},
  author={Gamrian, Shani and Goldberg, Yoav},
  booktitle={International conference on machine learning},
  pages={2063--2072},
  year={2019},
  organization={PMLR}
}