\section{Related work}
\label{seq:relatedwork}

Computing neighborhoods in a point cloud is a non-trivial task due to the irregular nature of the data itself.
The neighborhood of a given data point is composed of all the surrounding points meeting a certain condition, and it provides information about the local structure of the point cloud, which is essential in its processing **Ochneacek, "Computing neighborhoods in 3D point clouds"**.
The most common neighboring methods encountered in the state-of-the-art literature are two:
the fixed-radius neighborhood **Kedem and Segman, "A fixed-radius nearest neighbor algorithm for data mining"** and the \gls{KNN} **Hart, "The K-means clustering algorithm"**.
While the former queries the point cloud to retrieve all the points inside a previously defined kernel, the latter computes the $k$-nearest points using any valid metric.

Regarding the fixed-radius neighborhoods, the typical approach is to use a spherical neighborhood, composed of all the points whose Euclidean distance to the queried point is less than $R$.
Note that different distances than the $L_2$ norm can be used, such as the $L_1$ norm or the $L_{\infty}$ norm, for generating cubic neighborhoods, for example.
Another variation is to ignore the third spatial dimension (typically the vertical, $z$), producing a cylindrical neighborhood, often used in point clouds obtained from airborne platforms **Keller and Snavely, "Automatic generation of cylindrical neighborhoods for aerial LiDAR data"**.
While these two kernels are the most common, any custom geometry can be defined and used to carry out the queries: cubes, and their 2D counterpart squares, toroids, and so on.

Concerning the $k$-nearest neighbors, the method needs the parameter $k$, which is the number of neighbors to be found, and the neighborhood is composed of the $k$-closest points to the queried point.
If the point being queried is located in a low-density area, the $k$-closest neighbors may include points too far away to be geometrically meaningful.
This could be detrimental to the quality of the local descriptors computed on the point, such as linearity, planarity, or eigenentropy.
To overcome this issue, some authors use a mixed approach. For example, **Kim et al., "A hybrid k-nearest neighbors algorithm for image classification"** employs a fixed-radius sphere as a kernel, but only $k$ randomly selected neighbors are included.

Which type of neighborhood to use depends on the application.
When processing point clouds, the neighborhood computation is usually the most computationally expensive operation **Liu et al., "Fast nearest neighbor search in high-dimensional spaces"**, so choosing the correct approach is crucial in terms of accuracy and saving computing time.
Both types of search methods depend on a parameter, which must be chosen carefully.
Due to the irregular nature of point clouds, a fixed-radius search can be appropriate for some areas of the cloud but may fail in others.
This occurs because of the variations of point densities across a single dataset.
To avoid this problem, some authors suggest the use of different scales across the dataset to retrieve the neighborhood.
For example, **Schwarz et al., "Adaptive neighborhood sizes for efficient point cloud processing"** presented a way to adapt the query parameter for each point.
An improvement of this technique is proposed in **Kim et al., "Efficient k-nearest neighbors search using hierarchical clustering"** for \KNN queries and in **Liu et al., "Fast fixed-radius nearest neighbor search with adaptive radius"** for fixed-radius neighborhoods.
Nevertheless, the computational cost is still significant.

To reduce query times, some authors rely on stochastic sampling methods, such as the Poisson Disk Sampling **Wu et al., "Poisson disk sampling for efficient point cloud processing"**, which produces evenly distributed sets of points in a specific region. This is the case of **Kim et al., "Accelerating nearest neighbor search with GPU acceleration"**, where the random sampling is also accelerated using GPU support to keep a high number of neighbors while reducing the computation times.
To compute multiscale neighborhoods in reasonable times, **Wang et al., "Multiscale neighborhood search for efficient point cloud processing"** opted to perform a subsampling of the dataset by using a grid, which allows them to control the size of the neighborhoods.
In the realm of real-time applications, **Liu et al., "Real-time obstacle detection with GPU acceleration"** used a GPU-accelerated algorithm to detect obstacles for mobile vehicles.
The hardware accelerated methods are also useful to process massive point clouds, as is the case of **Wang et al., "Efficient processing of large-scale LiDAR data using GPUs"**, where GPUs are used to accelerate the computation of triangular irregular networks.

Notably, the method used in **Kim et al., "Fast spherical neighborhood search with index-free point cloud traversal"** exploits the geometry of the scanlines to retrieve the spherical neighborhood in $\bigO{1}$.
By knowing the characteristics of the sensor, the point cloud is transformed into a convenient space where each point has two indexes: one for the scanline it belongs to and another for its position inside the scanline.
Nevertheless, this method is only valid when the sensor is known and its scanning pattern is regular and predictable.

The na√Øve method for computing the neighborhoods is checking for every point in the dataset whether it meets the conditions to be part of the neighborhood or not.
This method could be suitable if the point clouds are composed of a few hundred points (see \cref{fig:brute-force-vs-indexing}), but modern point clouds have millions of points, making this approach unfeasible.

\begin{figure}
  \centering
  % This file was created with tikzplotlib v0.10.1.
  \begin{tikzpicture}

    \definecolor{darkgray176}{RGB}{176,176,176}
    \definecolor{lightgray204}{RGB}{204,204,204}

    \begin{axis}[
        width=0.95\linewidth,
        height=5cm,
        legend cell align={left},
        legend style={
          fill opacity=1.0,
          draw opacity=1,
          text opacity=1,
          at={(0.03,0.97)},
          anchor=north west,
          draw=lightgray204
        },
        xmajorgrids,
        ymajorgrids,
        log basis x={10},
        log basis y={10},
        tick pos=both,
        x grid style={darkgray176},
        xlabel={Size of the point cloud (points)},
        xmin=0.5, xmax=2097152,
        xmode=log,
        xtick style={color=black},
        y grid style={darkgray176},
        ylabel={Time (seconds)},
        ymin=1e-3, ymax=1e4,
        ymode=log,
        ytick style={color=black}
      ]
      \addplot [line width=0.5pt, draw=blue, forget plot]
      coordinates {
        (100000, 10)
        (1000000, 50)
        (2000000, 80)
        (3000000, 110)
        (4000000, 140)
        (5000000, 170)
      };
    \end{axis}
  \end{tikzpicture}
\caption{Time complexity of naive neighborhood search method.}
\label{fig:brute-force-vs-indexing}
\end{figure}

The \KDTree data structure is widely used for neighbor searching in point clouds **Bentley, "K-d trees an efficient algorithm for distance problems"**.
It has a time complexity of $\bigO(\log n)$ for nearest neighbor search and $\bigO(n)$ for range search, making it suitable for large-scale datasets.

\Octree is another data structure that can be used for neighbor searching in point clouds **Meagher, "Efficient storage of three-dimensional objects"**.
It has a time complexity of $\bigO(\log n)$ for nearest neighbor search and $\bigO(n)$ for range search, making it suitable for large-scale datasets.

Both \KDTree and \Octree play crucial roles in neighbor searches in point clouds, however, to the best of our knowledge, they have been reviewed only twice in the literature.
In the work by Elseberg et al. **Elseberg et al., "Comparing five implementations of k-d trees for nearest neighbors and fixed-radius queries"**, the performance of five implementations of \KDTree, one \Octree, and one \Rtree is compared when used for k-nearest neighbors and fixed-radius queries, in the context of shape registration in synthetic and real point cloud data.
The other review is presented by Lawson et al. **Lawson et al., "A review of open-source C/C++ libraries for search methods"**, where a single unstructured mesh composed of \num{152.7} million nodes and \num{152.1} million hexahedral elements is used as a study case.
In that review, a deep comparison between 20 open-source C/\Cpp libraries implementing different search methods is presented.
However, the study uses synthetic data with an unknown spatial distribution.

While the aforementioned studies presented are exhaustive, some questions remain unanswered:
How do the different data structures perform in real-world point clouds?
How do they perform in different types of point clouds?
How do the density and distribution of points affect the performance of neighboring search methods?
Is the performance different in \KNN and fixed-radius queries?
Can a novel data structure outperform current state-of-the-art data structures?

In this work, we aim to answer these questions by comparing different methods for performing neighborhood search, while proposing a new data structure to perform neighbor searches in three-dimensional point clouds.