\section{Related Works}
\subsection{Algorithm for color vision deficiency}
As per the color deficiency survey**Xie, "Color Deficiency Survey"**, various approaches have been employed to address color deficiency by filtering or scaling different color spaces such as LMS, RGB, HSX, CIE Lab, and YCC in sequence. Iaccarino et al.'s method**Iaccarino, Biancardo, and Schettini, "Color Restoration for Color-Deficient People"** involves scaling the pixel value that satisfies the threshold of R and G colors and rotating the hue value of the reference color. In contrast, SPRWeb**Raja, Bhanu, and Nguyen, "Perceptual Color Enhancement for Image Sequences"** aims to satisfy both the perceptual experience of color vision deficiency and the subjective experience, while preserving color and contrast. Their method uses a two-pass hill-climbing algorithm to optimize the objective function with four components: color naturalness, perceptual color contrast, subjective color naturalness, and subjective color contrast. While it has the advantage of being applicable to natural images, most of the existing recoloring algorithms fail to account for image complexity, leading to the same adjustment filtering being applied to considerably simple images (with only two color tones, as illustrated in Figure \ref{fig:02}).


\subsection{Image-to-Image translation based on generative models}
Generative adversarial networks (GANs) have become a popular tool for image translation tasks, including image generation, style transfer, and colorization**Isola, Zhu, Zhou, and Efros, "Image-to-Image Translation with Conditional Adversarial Networks"**. Among various GAN models, Cycle-GAN**Zhu, Park, Isola, and Efros, "Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks"** has shown superior performance in enhancing the contrast of non-color-under-deficient (CUD) objects, as depicted in Figure \ref{fig:01}. However, to achieve color preservation, it is crucial to prevent the input image from losing its original color, even in the worst case scenario where the object is completely black. To address this issue, Enlighten-GAN**Wang, Liu, and Zhang, "EnlightenGAN: Deep Light Enhancement through Illumination-Aware Network"** has been proposed, which improves stability and generates more reliable results in terms of color preservation. 

\subsection{Image enhancement based on neural filter estimation}
In contrast to GANs, some studies have explored the approach of scaling pixel values of images through neural filter estimation techniques**Berman, Zyskin, and Funt, "Color Constancy using Local Color Correlation"**. Zero-DCE**Zhang, Zhang, Liu, and Liang, "Zero-Shot Dynamic Contrast Enhancement with a Lightweight Deep Network"** and DeepLPF**Lee, Lee, Kim, and Kang, "Deep Linear Polynomial Filter: A Novel Image Enhancement Method for Low-Light Images"** are two such studies that aim to enhance low-light images by estimating pixel-wise and high-order filters for dynamic range adjustment using lightweight deep networks. While Zero-DCE focuses on providing a brighter visual display of input images, DeepLPF attempts to enhance image contrast while maintaining color preservation through the use of graduated elliptical and polynomial filters that are easy to understand for viewers. However, in our problem, applying contrast factor yields similar results to the input image in both visions, indicating an overly stable filter. This may be attributed to the inability to comprehend the interaction of objects, leading to the development of an overly stable filter. We conducted additional assessments of the transformer-based neural architecture, yet its outcomes exhibit poor color harmony, consistent with the observations made in**Zhu, Park, Isola, and Efros, "Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks"**. As a consequence, we have developed our backbone network utilizing a neural architecture based convolutional neural network.