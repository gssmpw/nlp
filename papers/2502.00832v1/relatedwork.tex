\section{Related Work}
\subsection{Large Language Models}

Large language models (LLMs) have become a cornerstone in natural language processing (NLP), demonstrating exceptional capabilities across a wide range of tasks. The scaling of model size and training data has enabled state-of-the-art performance in tasks such as text generation, machine translation, and reasoning \cite{zhou2023thread,zhou2021triple}. Notable examples include autoregressive models and transformer-based architectures, which underpin some of the most influential models in the field \cite{nicholas2023lost,mueller2022cedille}.

Multilingual LLMs have further broadened the applicability of these models by extending their capabilities beyond English to a wide array of languages. These models address the data scarcity in low-resource languages, although significant challenges remain in ensuring their effectiveness across diverse linguistic contexts \cite{ojo2023african,chang2024goldfish}. Specialized models, such as those trained exclusively for a single language or task, have also been proposed to improve efficiency and safety, as demonstrated in studies on domain-specific LLMs \cite{ma2024llamareg,liu2024bioinformatics}.

In addition to their direct applications, LLMs have been explored as tools for understanding event phenomena \cite{zhou2022claret,zhou2022eventbert}. Studies have shown that LLMs can serve as scientific models for language, aiding in psycholinguistic research and providing insights into the relationship between language and thought \cite{grindrod2024modelling,houghton2023psycholinguistics}. However, limitations persist, including challenges in generalizing to unseen data, ethical concerns, and resource constraints \cite{veres2022limitations,ali2024survey}.

The growing interest in applying LLMs to specific domains, such as healthcare and bioinformatics, highlights their versatility. For example, LLMs have been successfully applied to medical text processing and image registration tasks, demonstrating their potential to enhance domain-specific applications \cite{ma2024llamareg,liu2024bioinformatics}. As the field advances, continued efforts are required to address issues of fairness, safety, and efficiency in the development and deployment of LLMs.


\subsection{Medical Large Language Models}

Medical Large Language Models (MedLLMs) have become a critical research focus in applying artificial intelligence to healthcare. These models, derived from general-purpose LLMs, are specifically adapted to process medical knowledge, assisting in clinical decision-making, medical question answering, and multi-modal data interpretation \cite{Buhnila2024,He2024}. By fine-tuning on domain-specific datasets such as electronic health records, biomedical literature, and clinical notes, MedLLMs aim to enhance diagnostic accuracy, automate documentation, and facilitate real-time medical consultations.

Recent advancements have explored various methodologies to improve the MedLLMs' effectiveness. Some works focus on parameter-efficient fine-tuning, such as retrieval-augmented generation and adapter-based tuning, to reduce computational overhead while maintaining strong medical reasoning capabilities \cite{Ma2024,Zhang2024}. Additionally, the integration of multi-modal information, including medical imaging, has demonstrated significant potential in enhancing diagnostic interpretations and visual-text alignment in clinical applications \cite{Bai2024}. Furthermore, knowledge-enhanced MedLLMs have been proposed to incorporate structured medical ontologies and external databases to reduce hallucinations and improve factual reliability in medical dialogue systems \cite{Wu2024,Liao2024}.

Benchmarking and evaluation frameworks have been developed to measure MedLLMs' performance across diverse medical tasks. Large-scale evaluation benchmarks, such as PromptCBLUE and MedExQA, assess models' ability to perform named entity recognition, medical inference, and clinical response generation \cite{Zhu2023,Kim2024}. These studies have highlighted the strengths of MedLLMs in understanding domain-specific concepts while also identifying persistent challenges such as limited generalization across specialties, potential biases in training data, and ethical concerns regarding model reliability.

Despite these advancements, challenges remain in adapting MedLLMs to real-world clinical environments. One of the key issues is ensuring generalization across multiple healthcare domains, including radiology, cardiology, and psychiatry, where specialized knowledge is required \cite{Panagoulias2024}. Moreover, the need for transparent and interpretable decision-making remains crucial, as MedLLMs are increasingly integrated into critical applications such as clinical diagnostics and patient risk assessment. Future research directions include improving model interpretability, integrating real-time patient feedback mechanisms, and refining evaluation metrics to better capture the complexities of medical language processing.