% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}
\usepackage{booktabs}
\usepackage{tabulary}
\usepackage{xcolor}
\usepackage{colortbl}
\usepackage{ulem}
\usepackage{array}

\usepackage{amsmath}        % 数学公式
\usepackage{amssymb}        % 数学符号
\usepackage{graphicx}       % 插入图片
\usepackage{algorithm}      % 算法环境
\usepackage{algorithmicx}      % 算法环境
\usepackage{algpseudocode}  % 伪代码语法
\usepackage{hyperref}       % 超链接
\usepackage{cleveref}       % 智能引用
\usepackage{amsthm}
% 可选包
\usepackage{siunitx}
\usepackage{multirow}
\usepackage{makecell}



% 定义颜色
\definecolor{lightblue}{RGB}{235,245,255}
\definecolor{highlight}{RGB}{230,230,230}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\newcommand{\argmax}{\mathop{\mathrm{argmax}}}
\newcommand{\argmin}{\mathop{\mathrm{argmin}}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\I}{\mathbb{I}}
% 算法配置
\algnewcommand{\algorithmicinput}{\textbf{Input:}}
\algnewcommand{\algorithmicoutput}{\textbf{Output:}}
\algnewcommand{\INPUT}{\item[\algorithmicinput]}
\algnewcommand{\OUTPUT}{\item[\algorithmicoutput]}



\title{Low-Confidence Gold: Refining Low-Confidence Samples for Efficient Instruction Tuning}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{Hongyi Cai \\
%   Universiti Malaya \\
%   \texttt{xcloudfance@gmail.com} \\\And
%   Jie Li \\
%   University of Science and Technology Beijing \\
%   \texttt{email@domain} \\\And
%   Wenzhen Dong \\
%   The Chinese University of Hong Kong \\
%   \texttt{email@domain} \\}





% \author{
% % Anonymous ACL submission
%  \textbf{Hongyi Cai\textsuperscript{1}},
%  \textbf{Jie Li\textsuperscript{2}},
%  \textbf{Wenzhen Dong\textsuperscript{3}}
% \\
%  \textsuperscript{1}Universiti Malaya,\\
%  \textsuperscript{2}University of Science and Technology Beijing,
%  \textsuperscript{3}The Chinese University of Hong Kong
% \\
% }

% \author{
%  \textbf{Hongyi Cai\textsuperscript{1}$^*$},
%  \textbf{Jie Li\textsuperscript{2}$^\dagger$},
%  \textbf{Wenzhen Dong\textsuperscript{3}}
%  \\
%  \textsuperscript{1}Universiti Malaya, \texttt{xcloudfance@gmail.com}
%  \\
%  \textsuperscript{2}University of Science and Technology Beijing, \texttt{email@domain}
%  \\
%  \textsuperscript{3}The Chinese University of Hong Kong, \texttt{email@domain}
%  \\
%  \\
%  $^*$Equal contribution \\
%  $^\dagger$Equal contribution
% }

% \author{
%  \textbf{Hongyi Cai\textsuperscript{1}\texttt{(xcloudfance@gmail.com)}},
%  \textbf{Jie Li\textsuperscript{2}\texttt{(email@domain)}},
%  \textbf{Wenzhen Dong\textsuperscript{3}\texttt{(email@domain)}}
%  \\
%  \textsuperscript{1}Universiti Malaya,
%  \textsuperscript{2}University of Science and Technology Beijing,
%  \textsuperscript{3}The Chinese University of Hong Kong
% }

\author{
 \textbf{Hongyi Cai\textsuperscript{1}},
 \textbf{Jie Li\textsuperscript{2}\footnotemark[1]},
 \textbf{Wenzhen Dong\textsuperscript{3}}
 \\
 \textsuperscript{1}Universiti Malaya, \texttt{xcloudfance@gmail.com}
 \\
 \textsuperscript{2}University of Science and Technology Beijing, \texttt{lj2085727892@163.com}
 \\
 \textsuperscript{3}The Chinese University of Hong Kong, \texttt{dongwz@link.cuhk.edu.hk}
}



\begin{document}
\maketitle

\footnotetext[1]{Equal contribution}
\begin{abstract}
The effectiveness of instruction fine-tuning for Large Language Models is fundamentally constrained by the quality and efficiency of training datasets. This work introduces Low-Confidence Gold (LCG), a novel filtering framework that employs centroid-based clustering and confidence-guided selection for identifying valuable instruction pairs. Through a semi-supervised approach using a lightweight classifier trained on representative samples, LCG curates high-quality subsets while preserving data diversity. Experimental evaluation demonstrates that models fine-tuned on LCG-filtered subsets of 6K samples achieve superior performance compared to existing methods, with substantial improvements on MT-bench and consistent gains across comprehensive evaluation metrics. The framework's efficacy while maintaining model performance establishes a promising direction for efficient instruction tuning.
\end{abstract}

\section{Introduction}

Large Language Models (LLMs) has been trained to follow instructions by specific supervised response data after pre-training stage. Many instruction finetuning (IFT) \cite{alpaca} datasets emerge to realize various downstream tasks, for example: mathematic calculation, sentence analysis, haiku writing and etc, aiming to strengthen the ability of LLMs in instruction following. To save vast human costs for data annotation, most of studies introduce other teacher LLMs (e.g. \texttt{text-davinci-003} \cite{gpt}) to align the best instructions with corresponding responses. 

However, IFT datasets (e.g. \texttt{Alpaca\_52k} \cite{alpaca}, \texttt{magpie} \cite{xu2024magpie}) suffer from misleading content and poor quality, resulting in the bottleneck of post-training performance, even though teacher models replenish the missing parts of context and instruction pairs. This highlights the need for effective data filtering methods that identify high-quality instruction subsets while reducing fine-tuning time and computational costs.




\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{latex/teaser.pdf}
    \caption{We target to select complex and quality samples confidence ranking for benefiting LLM training.}
    \vspace{-5mm}
    \label{fig:teaser}
\end{figure}


\texttt{Alpagasus} \cite{chen2023alpagasus} proposed a model-based approach that introduces proprietary LLMs to score data quality in multiple facets, replacing human annotation by taking advantage of the automated pipeline. However, this leads to datasets that are likely biased by the preference for redundant and limited responses \cite{panickssery2024llm}, which potentially deteriorates the diversity of the original data. \citealp{ge2024clustering} emphasizes the necessity of diversity and therefore proposed clustering and ranking to select subsets of data. Further, Superfiltering \cite{li2024superfiltering} gains more insights in small open-source LLM that scores the instruction following ability of \texttt{Alpaca\_52k}. Although the instruction score provides an efficient and simple criterion for data selection, it does not consistently correlate with both the quality and diversity of data. Consequently, improvements in performance may not always be guaranteed.


To address these challenges, we propose a novel data filtering framework, \textbf{Low-Confidence Gold (LCG)} for efficient instruction tuning that significantly reduces computational costs while maintaining model performance. Our approach, shown in \ref{fig:teaser}, innovatively seek to identify high-value instruction data through classification tasks. Specifically, we develop a lightweight classification model trained on centroid subsets that effectively categorizes instruction-response pairs, and leverage low-confidence predictions to curate challenging examples most beneficial for instruction tuning. Another perspective is, since the common instruction tuning data are lack of annotations and labels, we adopt the manner of semi-supervised learning, to construct pseudo-labels as our training groundtruth, as well as getting inspired quality data from affordable yet effective models.

Through extensive experiments on the \texttt{Alpaca\_52K dataset}, we demonstrate that our filtered subsets achieve comparable or better performance when fine-tuning various open-source language models, while requiring only a fraction of the original data. Our main contributions are threefold: 
\begin{enumerate}
    \item A novel and efficient data filtering paradigm for instruction tuning that combines nearest neighbor classification with confidence-based selection.
    \item We train a small classifier model that enables selection for the whole set of instruction finetuning data.
    \item Experiments and evaluations are conducted demonstrating the outstanding effectiveness of our filtered datasets working on multiple open-source LLMs. We reach \textbf{states-of-the-arts performance} in MT-Bench and HuggingFace OpenLLM Leaderboard benchmarks.
\end{enumerate}


% Our works:
% - Proposed a new paradigm that extracts a very small subset which utilizes pseudo-labels created from Nearest Neighbors as training data of classification model.
% - Using centroid subsets to train the classifier for only 3 epochs, when it manages to identify the approximate categories of text pairs and their corresponding tasks.
% - Many studies highlight the trick of effectively increasing LLMs performance is to use difficult, complex context as data. Inspired from semi-supervised learning, we selectively curate the new IFT subsets when our model proposes low confidence score in classifying pseudo-labels.
% - This method takes advantages of the initial learning of smaller models as a classifier, helping identify the data that remains challenging and perplexing for instruction tuning process.



\section{Preliminaries}


\subsection{K-means Clustering}


Given the Alpaca\_52k dataset $\mathcal{D} = \{(x_i, y_i)\}_{i=1}^{N}$ where $N=52,000$, we first cluster instructions into $K$ semantic groups using K-means. Let $\phi(x_i) \in \mathbb{R}^d$ denote the embedding vector of instruction $x_i$. The clustering objective minimizes:

\begin{equation}
\min_{\{C_k\}_{k=1}^K} \sum_{k=1}^K \sum_{x_i \in C_k} \|\phi(x_i) - \mu_k\|^2
\end{equation}

where $\mu_k = \frac{1}{|C_k|}\sum_{x_i \in C_k} \phi(x_i)$ is the centroid of cluster $C_k$. This partitions $\mathcal{D}$ into $K$ disjoint subsets $\{C_1, ..., C_K\}$ based on instruction similarity.

\subsection{Problem Setting}
Our filtering framework, LCG aims to select a subset $\mathcal{D}_{\text{filtered}} \subseteq \mathcal{D}$ that satisfies:

\begin{equation}
\mathcal{D}_{\text{filtered}} = \bigcup_{k=1}^K \left\{ (x_j, y_j) \in C_k \mid \mathcal{F}(x_j, y_j) < \tau_k \right\}
\end{equation}

where $\mathcal{F}: \mathcal{D} \rightarrow [0,1]$ is a discriminative confidence scorer and $\tau_k$ is an adaptive threshold for cluster $C_k$. The scorer $\mathcal{F}$ evaluates how "hard" a sample is to be trivially categorized, with higher values indicating the simplicity of data which is easily determined and differentiated. The training efficiency therefore increases since only a small subset of instructions are curated.

\begin{figure*}[t]
  \includegraphics[width=1.0\linewidth]{latex/2.vector.pdf}
  \caption{The overall pipeline of Low-Confidence Gold. We split our pipeline into two main steps: 1) Clustering to get pseudolabels and centroid data to collect initial diversity of data. 2) We feed annotated data into a tiny yet effective classifier to rank the confidences for the rest of distant data to implement subset selection. }
\end{figure*}



\section{Methodology}



\label{sec:method}

\subsection{Motivation}
\textbf{Instruction filtering demands a dual-focus mechanism that intrinsically balances data quality and diversity.} Traditional supervised methods face inherent scalability limitations as manual annotation becomes prohibitively expensive for large-scale instruction datasets \cite{liu-etal-2022-makes, longpre2023flan, deita}. Meanwhile, it is difficult to identify suitable and challenging data for LLMs training without introducing proprietary LLMs or labors. Our semi-supervised framework addresses these limitations through pseudo-label refinement and early-stopped confidence detection, creating dynamic selection boundaries aligned with language model learning dynamics.

\textbf{Cluster-centric pseudo-labeling addresses data distribution challenges in instruction tuning.} Traditional sampling methods often struggle to balance between common and rare instruction patterns, leading to either over-representation of frequent cases or loss of valuable rare examples. We create semantic clustering anchors that naturally preserve the diversity of instruction patterns. By sampling 3\% of data points nearest to cluster centroids, we ensure each semantic category contributes meaningful examples while maintaining the inherent data distribution characteristics.

\textbf{Early-stopped classifier training induces uncertainty to identify high-quality samples.} Limiting the classifier to 3 epochs creates deliberate underfitting - the model develops basic pattern recognition without over-specializing to pseudo-labels. When applied to non-centroid samples, this partially-trained classifier's low-confidence predictions signal instructions containing non-trivial semantic constructs. These samples challenge the classifier's emerging decision boundaries precisely because they contain valuable complexity that language models should master, not avoid.
            \begin{table*}[h!]
    \centering
    \scalebox{0.92}{
    \definecolor{lightblue}{RGB}{235,245,255}
    \definecolor{highlight}{RGB}{230,230,230}
    \begin{tabulary}{\textwidth}{L|C|CCCCC}
        \toprule
        \rowcolor{lightblue} 
        \textbf{Model} & \textbf{MT-bench} & \multicolumn{5}{c}{\textbf{Huggingface Open LLM Leaderboard Scores (\%)}} \\
        \cmidrule{3-7}
        \rowcolor{lightblue} 
        & \textbf{Score} & \textbf{Hellaswag} & \textbf{MMLU} & \textbf{GSM8k} & \textbf{ARC} & \textbf{Avg} \\
        \midrule
        
        \multicolumn{7}{l}{\textit{First Group - Base Model}} \\
        Mistral-7b-v0.3 & 3.639 & 60.94 & 58.96 & 36.62 & 48.81 & 51.33 \\
        \midrule
        
        \multicolumn{7}{l}{\textit{First Group - Methods}} \\
        Alpaca-52k & 4.018 & 61.18 & 57.73 & 31.61 & 53.07 & 50.90 \\
        SuperFiltering-10\% & 3.963 & 60.98 & 59.34 & 35.71 & 49.83 & 51.47 \\
        Random-6k & 4.314 & 60.83 & 58.75 & 35.03 & 53.07 & 51.92 \\
        Perplexity-6k & 4.352 & 61.64 & 58.48 & 37.00 & 51.88 & 52.25 \\
        Kmeans-6k & 4.283 & 60.86 & 58.45 & 35.10 & 52.05 & 51.62 \\
        LIMA-6k & 4.440 & 60.58 & 59.34 & 37.31 & 51.11 & 52.09 \\
        \rowcolor{highlight}
        \textbf{LCG-MultinomialNB-6k (Ours)} & \textbf{5.086} & \textbf{62.00} & \textbf{59.51} & \textbf{40.51} & \textbf{52.90} & \textbf{53.73} \\
        \rowcolor{highlight}
        \textbf{LCG-DistilBERT-6k (Ours)} & \uwave{4.894} & \uwave{61.99} & \textbf{59.51} & \uwave{40.33} & \uwave{52.22} & \uwave{53.51} \\
        \rowcolor{highlight}
        \textbf{LCG-DistilBERT-1k (Ours)} & 4.869 & 61.94 & \uwave{59.24} & 38.29 & 51.62 & 52.77 \\
        \midrule
        
        \multicolumn{7}{l}{\textit{Second Group - Base Model}} \\
        LLaMa3-8b & 3.418 & 60.17 & 62.13 & 50.42 & 50.26 & 49.98 \\
        \midrule
        
        \multicolumn{7}{l}{\textit{Second Group - Methods}} \\
        Alpaca-52k & 3.718 & 60.57 & 61.36 & 46.10 & 52.41 & 55.74 \\
        SuperFiltering-10\% & 3.968 & 60.38 & 61.95 & \uwave{50.34} & 51.54 & 55.36 \\
        Random-6k & 3.912 & 60.83 & 58.75 & 35.03 & 53.07 & 51.92 \\
        Perplexity-6k & 4.120 & \uwave{61.14} & 61.09 & 50.87 & \uwave{53.50} & 56.65 \\
        Kmeans-6k & 3.731 & 60.86 & 58.45 & 35.10 & 53.07 & 51.87 \\
        LIMA-6k & 4.450 & 60.58 & \uwave{62.13} & 50.34 & 51.11 & 55.82 \\
        \rowcolor{highlight}
        \textbf{LCG-DistilBERT-6k (Ours)} & \textbf{4.963} & \textbf{61.43} & \textbf{62.67} & \textbf{54.28} & \textbf{54.78} & \textbf{58.29} \\
                \rowcolor{highlight}

        \textbf{LCG-DistilBERT-1k (Ours)} & \uwave{4.776} & 60.95 & 62.26 & 52.92 & 52.82 & \uwave{57.23} \\
        \bottomrule
    \end{tabulary}
    }
    \caption{Performance comparison on standard benchmarks. Results in \textbf{bold} indicate best performance within each group, while \uwave{underlined} values represent second-best performance within each group. The table is divided into two groups, each with its base model and various fine-tuning methods.}
    \label{tab:model_comparison}
\end{table*}
\subsection{Centroid Coreset Selection for Pseudolabels}
In the initial step of our approach, we select a coreset from the whole corpus to identify pseudolabels by the K-means algorithm, which effectively determine each semantic clusters. Given a dataset of instruction pairs $\mathcal{D} = \{(\mathbf{x}_i, \mathbf{y}_i)\}_{i=1}^N$, we first encode each instruction $\mathbf{x}_i$ into a dense vector representation using MiniLM \cite{wang2022minilmv2}:

\begin{equation}
\mathbf{h}_i = \text{AvgPool}(\text{MiniLM}(\mathbf{x}_i)) \in \mathbb{R}^{384}
\end{equation}


This geometric progression ensures proportional coverage of both frequent and rare instruction patterns. Cluster centroids $\{\mathbf{c}_j\}_{j=1}^k$ are computed via:

\begin{equation}
\mathbf{c}_j = \frac{1}{|\mathcal{C}_j|} \sum_{\mathbf{x}_i \in \mathcal{C}_j} \tilde{\mathbf{h}}_i
\end{equation}
where $\mathcal{C}_j$ denotes the set of samples assigned to cluster $j$. Centroid-proximal samples are selected as high-confidence candidates:

\begin{equation}
\mathcal{D}_{\text{core}} = \{\mathbf{x}_i | \|\tilde{\mathbf{h}}_i - \mathbf{c}_{j(i)}\|_2 < \gamma\}
\end{equation}
where $\gamma$ is the 90th percentile distance within each cluster.



\subsection{Low-Confidence Gold: Calibrating with Low-confidence samples to select data}

After determining pseudolabels based on clusters, those annotations can be served for classification training. Specifically, we train a multi-class classifier on the core samples $\mathcal{D}_{\text{core}}$. The model architecture consists of:

\begin{equation}
f_\theta(\mathbf{x}) = \text{Softmax}(\mathbf{W}_2 \cdot \text{GELU}(\mathbf{W}_1 \mathbf{h}_i + \mathbf{b}_1) + \mathbf{b}_2)
\end{equation}





where $\mathbf{W}_1 \in \mathbb{R}^{384 \times 768}$, $\mathbf{W}_2 \in \mathbb{R}^{768}$ are learnable parameters, and GELU denotes the Gaussian Error Linear Unit activation. The model optimizes cross-entropy loss:

\begin{equation}
\begin{aligned}
\mathcal{L}(\theta) = -\frac{1}{|\mathcal{D}_{\text{core}}|} 
\sum_{(\mathbf{x}_i,y_i)} & \sum_{j=1}^k \mathbb{I}(y_i=j) \\
& \cdot \log p_\theta(y=j|\mathbf{x}_i)
\end{aligned}
\end{equation}

Training terminates at epoch $T=3$ since we aim to keep the model in an early-stopped stage so that they would not overfit to the centroid subset data. After training, we rank the confidence distribution calculated from \textit{softmax} function and select the top $K$ most uncertained data in each cluster.




\section{Experiments}
In this section, we utilize LCG to filter \texttt{Alpaca\_52k} dataset into 6k and evaluate the subset by fine-tuning in 2 open-source LLMs: 1) \texttt{Mistral-7b-v0.3} \cite{jiang2023mistral} and 2) \texttt{LLaMa3-8b} \cite{dubey2024llama}. Additionally, we adopt two different classifier as our semi-supervised training model, to examine difficult samples. Both results demonstrate the effectiveness of our method, as shown in Tab. \ref{tab:model_comparison}.

\textbf{Settings. } Two classifers are Multinomial Naive Bayes \cite{scikit-learn} and DistilBERT \cite{sanh2019distilbert} respectively. We set the training learning rate as 1e-5 and also 3 epochs to train as mentioned before. After training finished, we select curated datasets by confidences < 0.7 to fine-tune open-source LLMs by LoRA \cite{hu2021lora} with learning rate 2e-5 and 3 epochs.


Our proposed Low-confidence Gold (LCG) method consistently outperforms existing instruction data filtering approaches across multiple base models and evaluation benchmarks. When applied to Mistral-7b, LCG with MultinomialNB achieves the highest MT-bench score of 5.086, surpassing the previous best (LIMA-6k \cite{LIMA}) by 14.5\%. Similarly, LCG with DistilBERT demonstrates superior performance on LLaMA3-8b, improving the MT-bench score by 11.5\% over LIMA-6k. Notably, our method maintains strong performance even with only 1k examples, highlighting its effectiveness in identifying high-quality instruction data. The consistent improvements across diverse metrics (Hellaswag \cite{hellaswag}, MMLU \cite{mmlu}, GSM8k \cite{gsm8k}, and ARC \cite{arc}) further validate the robustness of our approach.

\section{Conclusion}
In this paper, we proposed Low-Confidence Gold (LCG), a novel data filtering framework that combines cluster-centric pseudo-labeling with early-stopped classifier training for efficient instruction tuning. Through extensive experiments, we demonstrated the strong performance across multiple benchmarks and base models validates the effectiveness of our semi-supervised learning paradigm in maintaining both data quality and diversity for instruction tuning.


\section{Limitation}
Our work introduces a semi-supervised training paradigm to curate a subset of data for instruction tuning based on confidence score. However there still exists several challenges: 1) Even though classifiers are tiny and spending low computational resources to train, it still takes time and efforts to initially select data with annotated pseudolabels. 2) It is likely to be hindered by the original biases and tasks of the dataset, which might still cause inefficiency after selection.

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\newpage

\bibliography{custom}

\newpage
\appendix

\section{Extended Analysis of Semi-Supervised Model Configurations}
\label{sec:appendix}




\subsection{MultinomialNB Implementation}
The confidence distribution patterns of our MultinomialNB baseline, as visualized in Fig. \ref{fig:mul}, reveal fundamentally different characteristics compared to deep learning architectures. The histogram demonstrates remarkable uniformity across confidence intervals (0.0-1.0 with 0.1 increments), showing no significant concentration in specific confidence ranges. This equilibrium phenomenon stems from the model's inherent probabilistic nature and linear decision boundaries, Zwhich produce well-calibrated confidence estimates despite its simplicity. 



\subsection{DistilBERT comparative experiment on learning rate}
Our DistilBERT implementation employed a systematic exploration of learning rate hyperparameters {1e-4, 1e-5, 1e-6} within the following experimental framework:
\begin{enumerate}
    \item Architecture: DistilBERT-base-uncased (66M parameters) with custom classification head.
    \item Optimization: Adam optimizer.
    \item Training regime: 3-epoch constraint to prevent overfitting in low-data scenarios.
    \item Data alignment: Identical train/test splits (stratified sampling) as MultinomialNB for direct comparability.
\end{enumerate}

The empirical results (shown in Fig. \ref{fig:dis}) demonstrate non-monotonic performance relationships with learning rate scaling. Peak accuracy (62\%) emerged at 1e-5, while extreme values at both ends (1e-4: 36\%, 1e-6: 28\%) showed substantial performance degradation. This U-shaped accuracy curve suggests existence of optimal learning rate basins in semi-supervised BERT fine-tuning.

The model exhibited distinct confidence distribution characteristics at the 1e-6 learning rate, with predictions predominantly clustered in the low-confidence range (0-0.2). However, as revealed in Figure 2, comparative analysis across learning rates demonstrated minimal performance variation, showing only marginal improvements that correlated with accuracy increments.
\begin{figure*}[t]
    \centering
    \includegraphics[width=0.48\linewidth]{latex/mul.pdf}
    \caption{The data distribution of MultinomialNB across different confidence intervals.}
    \label{fig:mul}
    \vspace{5mm}  % 调整两图之间的间距
    
    \includegraphics[width=0.9\linewidth]{latex/dis.pdf}
    \caption{The data distribution of DistilBERT across different confidence intervals under various learning rates.}
    \label{fig:dis}
    \vspace{-5mm}
\end{figure*}
\end{document}
