\section{Related Work}
\subsection{AI generated Science}

% AI generated scientific content 
Advancements in instructable generative models \cite{gpt4,stable_diffusion_3} have sparked interest in their use for scientific content generation \cite{scimon,sakana,NLP100,chemistry_tools,writing_assistant1}. Large Language Models (LLMs) have been explored for tasks such as  research hypothesis generation \cite{scimon,research_agent,NLP100}, paper drafting \cite{writing_assistant1,writing_assistant2}, and even experimental design \cite{chemistry_tools, machine_learning_research_agent}. Most studies focus on specific subproblems. Notable exceptions include \citet{sakana}, who apply LLMs to the entire research process, and \citet{machine_learning_research_agent}, who focus on machine learning research but exclude report writing.

% Evaluation Techniques used in these works 
A key challenge in this domain is evaluating generated outputs, such as the novelty or feasibility of a research hypothesis, which often requires time-consuming, costly reviews by domain experts \cite{NLP100}. To address this, some researchers have used LLM-based evaluations as substitutes for human judgment \cite{research_agent,hypothesis_generation,hypothesis_generation2}, with some even replacing human evaluators in certain cases entirely \cite{sakana}. However, recent studies question the reliability of LLMs as evaluators compared to human reviewers \cite{NLP100}. We propose citation count and review score prediction as alternative evaluation metrics and compare them to LLM-based systems.


\subsection{Automated Peer Reviewing}

% Overview of automating the Review Process
With the increase in scientific paper submissions, interest in automating aspects of the peer review process has grown \cite{automated_review_process}, spanning tasks like reviewer assignment \cite{malicious_bidding} and review generation \cite{review_generation1,review_generation2,review_generation3}. An overview of the tasks and related datasets is provided in \citet{review_dataset_overview}.

% Review Score Prediction // Paper Decision Prediction
While, automatically generated textual reviews  \cite{review_generation2} can provide valuable feedback to authors, they are unsuitable as standalone review metrics, as it is difficult to determine which method generates more valuable research papers. To develop automatic evaluation metrics, we focus on the task of \textit{Review Score Prediction} (RSP) \cite{peer_read}. Another related task, \textit{Paper Decision Prediction} (DSP) \cite{peer_assist,automated_review_process}, could also contribute to evaluation metrics but offers less granularity. While \citet{peer_read} explore review score prediction, their work is limited to a small dataset (PeerRead,<1000 submissions) and does not address temporal generalization. Follow up work focuses on improving prediction accuracy by dealing with the limited data by learning from unlabelled data \cite{review_score_prediction_unlabeled} or adding intermediate tasks for finetuning \cite{review_score_prediction_intermediate}.


% Relevance of a unified dataset  
As noted by \citet{review_dataset_overview}, most studies create their own datasets, many of which are derived from OpenReview. Alternative sources of reviews, such as F1000\footnote{\url{https://f1000research.com/}} and PeerJ\footnote{\url{https://peerj.com/}} cover a more diverse range of topics but lack an anonymous review process \cite{other_dataset}. The lack of a standardized, updated dataset with rich metadata limits method comparisons and progress in automating reviews. Our work addresses this by providing data models for scientific papers and reviews to unify all OpenReview submissions. 


\subsection{Citation Count Prediction}

Citation count prediction approaches differ based on the information used \cite{citation_prediction_sciencometrics}. Most models combine citation history \cite{citation_history1}, metadata (e.g., authors, h-index) \cite{citation_metadata1}, and paper content \cite{schubert}. For newly generated content, without citation history or metadata, only content-based models are applicable to build evaluation metrics. Early studies used n-gram and term-frequency features from titles and abstracts, with small datasets (~3,000 papers) and short time spans \cite{early_work_citation_prediction1, early_work_citation_prediction2}. Recent efforts scaled dataset sizes (~30,000â€“40,000 papers) and incorporated embeddings from large language models to improve prediction performance \cite{schubert, embeddings_citation_count_prediction}. Some studies show significant benefits from using full papers, while others report only marginal improvements \cite{embeddings_citation_count_prediction}.

These conflicting findings may stem from differences in datasets and target metrics employed by each study. Such variations complicate direct comparisons across works, hindering the overall progress of the field. Furthermore, few approaches explicitly account for generalization over time. Notably, many studies use validation sets with publication dates that are only minimally separated from those in the training set, with a maximum gap of one month \cite{newly_published_citation_prediction, cimate}.


Few studies have explored the relationship between citation counts and peer-review scores. While prior work shows that reviews can improve citation prediction models \cite{citation_prediction_review_text1, citation_prediction_review_text2}, our focus is on their correlation. A strong correlation would suggest both metrics capture similar aspects of a paper's quality. \citet{findings_openreview} found a weak positive correlation between citation counts and review scores for ICLR submissions from 2017 to 2019, which we extend to the entire OpenReview dataset.