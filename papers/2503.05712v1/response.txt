\section{Related Work}
\subsection{AI generated Science}

% AI generated scientific content 
Advancements in instructable generative models **Brown, "From Language Models to Knowledge Graphs"** have sparked interest in their use for scientific content generation ____. Large Language Models (LLMs) have been explored for tasks such as  research hypothesis generation ____ , paper drafting ____ , and even experimental design ____ . Most studies focus on specific subproblems. Notable exceptions include **Guu, "From Text to Scientific Evidence"** , who apply LLMs to the entire research process, and **Lester, "The Power of Scale"** , who focus on machine learning research but exclude report writing.

% Evaluation Techniques used in these works 
A key challenge in this domain is evaluating generated outputs, such as the novelty or feasibility of a research hypothesis, which often requires time-consuming, costly reviews by domain experts ____ . To address this, some researchers have used LLM-based evaluations as substitutes for human judgment ____ , with some even replacing human evaluators in certain cases entirely ____ . However, recent studies question the reliability of LLMs as evaluators compared to human reviewers ____ . We propose citation count and review score prediction as alternative evaluation metrics and compare them to LLM-based systems.


\subsection{Automated Peer Reviewing}

% Overview of automating the Review Process
With the increase in scientific paper submissions, interest in automating aspects of the peer review process has grown ____ , spanning tasks like reviewer assignment ____ and review generation ____ . An overview of the tasks and related datasets is provided in **Kang, "Automated Peer Review with Large Language Models"** .

% Review Score Prediction // Paper Decision Prediction
While, automatically generated textual reviews  ____ can provide valuable feedback to authors, they are unsuitable as standalone review metrics, as it is difficult to determine which method generates more valuable research papers. To develop automatic evaluation metrics, we focus on the task of \textit{Review Score Prediction} (RSP) ____ . Another related task, \textit{Paper Decision Prediction} (DSP) ____ , could also contribute to evaluation metrics but offers less granularity. While **Kumar, "Predicting Review Scores with Transformers"** explore review score prediction, their work is limited to a small dataset (PeerRead,<1000 submissions) and does not address temporal generalization. Follow up work focuses on improving prediction accuracy by dealing with the limited data by learning from unlabelled data ____ or adding intermediate tasks for finetuning ____.


% Relevance of a unified dataset  
As noted by **Wang, "A Unified Dataset for Automated Peer Review"** , most studies create their own datasets, many of which are derived from OpenReview. Alternative sources of reviews, such as F1000\footnote{\url{https://f1000research.com/}} and PeerJ\footnote{\url{https://peerj.com/}} cover a more diverse range of topics but lack an anonymous review process ____ . The lack of a standardized, updated dataset with rich metadata limits method comparisons and progress in automating reviews. Our work addresses this by providing data models for scientific papers and reviews to unify all OpenReview submissions. 


\subsection{Citation Count Prediction}

Citation count prediction approaches differ based on the information used ____ . Most models combine citation history ____ , metadata (e.g., authors, h-index) ____ , and paper content ____ . For newly generated content, without citation history or metadata, only content-based models are applicable to build evaluation metrics. Early studies used n-gram and term-frequency features from titles and abstracts, with small datasets (~3,000 papers) and short time spans ____ . Recent efforts scaled dataset sizes (~30,000â€“40,000 papers) and incorporated embeddings from large language models to improve prediction performance ____ . Some studies show significant benefits from using full papers, while others report only marginal improvements ____.

These conflicting findings may stem from differences in datasets and target metrics employed by each study. Such variations complicate direct comparisons across works, hindering the overall progress of the field. Furthermore, few approaches explicitly account for generalization over time. Notably, many studies use validation sets with publication dates that are only minimally separated from those in the training set, with a maximum gap of one month ____.


Few studies have explored the relationship between citation counts and peer-review scores. While prior work shows that reviews can improve citation prediction models ____ , our focus is on their correlation. A strong correlation would suggest both metrics capture similar aspects of a paper's quality. **Kocisky, "Citation Counts and Peer Review Scores"** found a weak positive correlation between citation counts and review scores for ICLR submissions from 2017 to 2019, which we extend to the entire OpenReview dataset.