%%%%%%%% ICML 2023 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{lipsum}
\usepackage{booktabs} % for professional tables
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2023} with \usepackage[nohyperref]{icml2023} above.
\usepackage{hyperref}
\usepackage{float}
% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2024}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2024}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{bm}
% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}
\usepackage{thmtools}
\usepackage{thm-restate}
\usepackage{enumitem}
\usepackage{soul}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\setlength{\belowcaptionskip}{0pt}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newcommand{\Var}{\mathrm{Var}}
\DeclareMathOperator*{\argmax}{argmax} % thin space, limits underneath in displays
\DeclareMathOperator*{\argmin}{argmin} % thin space, limits underneath in displays
\PassOptionsToPackage{dvipsnames}{xcolor}
% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}
\def\thickhline{\noalign{\hrule height1pt}}
\newcommand{\squishlisttwo}{
 \begin{list}{$\bullet$}
  { \setlength{\itemsep}{1pt}
     \setlength{\parsep}{0pt}
    \setlength{\topsep}{0pt}
    \setlength{\partopsep}{0pt}
    \setlength{\leftmargin}{1em}
    \setlength{\labelwidth}{1.5em}
    \setlength{\labelsep}{0.5em} } 
}
\newcommand{\squishend}{
  \end{list}  }
\setlength{\textfloatsep}{1.0pt plus 2.0pt minus 2.0pt}

\newcommand{\Xs}{\mathcal{X}}
\newcommand{\Leval}{\mathcal{L}_{\textrm{eval}}}
\newcommand{\Ltrain}{\mathcal{L}_{\textrm{train}}}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{DUET: Optimizing Training Data Mixtures via Feedback from Unseen Evaluation Tasks}

\begin{document}

\twocolumn[
\icmltitle{DUET: Optimizing Training Data Mixtures via\\ Feedback from Unseen Evaluation Tasks}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2023
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Zhiliang Chen}{nus,i2r}
\icmlauthor{Gregory Kang Ruey Lau}{nus}
\icmlauthor{Chuan Sheng Foo}{i2r,cfar}
\icmlauthor{Bryan Kian Hsiang Low}{nus}
\end{icmlauthorlist}

\icmlaffiliation{nus}{Department of Computer Science, National University of Singapore, Singapore}
\icmlaffiliation{i2r}{Institute for Infocomm Research, A*STAR,
Singapore}
\icmlaffiliation{cfar}{Centre for Frontier AI Research, A*STAR, Singapore}

\icmlcorrespondingauthor{Zhiliang Chen}{chenzhiliang@u.nus.edu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{} % otherwise use the standard text.

% \begin{abstract}
% \emph{Machine learning} (ML) models in the real world typically do not exist in isolation. They are usually part of a complex system (e.g., healthcare systems, self-driving cars) containing multiple ML and \textit{black-box} components. Unfortunately, optimizing the system performance, which requires us to \textit{jointly} train all ML components, presents a significant challenge because the number of system parameters is extremely high and the system has no analytical form. To circumvent this, we introduce a novel algorithm called A-BAD-BO which uses each ML component's local loss as an auxiliary indicator for system performance. A-BAD-BO uses \emph{Bayesian optimization} (BO) to optimize the local loss configuration of a system in a smaller dimensional space and exploits the differentiable structure of ML components to recover optimal system parameters from the optimized configuration. We show A-BAD-BO converges to optimal system parameters by showing that it is \emph{asymptotically no regret}. We use A-BAD-BO to optimize several synthetic and real-world complex systems, including a prompt engineering pipeline for \emph{large language models} containing millions of system parameters. Our results demonstrate that A-BAD-BO yields better system optimality than gradient-driven baselines and is more sample-efficient than pure BO algorithms.
% \end{abstract}

\begin{abstract}
The performance of a machine learning (ML) model depends heavily on the relevance of its training data to the domain of the downstream evaluation task. However, in practice, the data involved in an unseen evaluation task is often not known to us (e.g., conversations between an LLM and a user are end-to-end encrypted). So, it is not obvious what data  would be relevant for training/fine-tuning the ML model to maximize its task performance. 
Instead, one can only deploy the ML model in the unseen evaluation task to gather multiple rounds of coarse feedback on how well the model has performed.
This paper presents a novel global-to-local algorithm called DUET that can exploit the feedback loop by interleaving a \emph{data selection} method with \emph{Bayesian optimization}.
As a result, DUET can efficiently refine the training data mixture from a pool of data domains to maximize the model's performance on the unseen evaluation task and its convergence to the optimal data mixture can be theoretically guaranteed by analyzing its \textit{cumulative regret}. Empirical evaluation on image and LLM evaluation tasks shows that DUET finds better training data mixtures than conventional baselines.
%However, in many practical settings, we do not have knowledge of the data involved in an unseen evaluation task (e.g., conversations between an LLM and a user are end-to-end encrypted). Hence, it is not immediately clear which data points we should include to train or fine-tune the ML model to maximize its performance on the task. In such settings, we can only deploy our ML model on the unseen evaluation task to collect multiple rounds of coarse feedback that indicate how well our model has performed. Our paper addresses such realistic settings with DUET, a novel global-to-local algorithm that interleaves \emph{data selection methods} with \emph{Bayesian Optimization} to exploit the feedback loop from the unseen evaluation task. As a result, DUET efficiently refines the training data mixture from a pool of data domains to maximize a model's performance on the unseen evaluation task. Furthermore, we provide theoretical guarantees for DUET's convergence to the optimal data mixture by analyzing its \textit{cumulative regret}. Finally, we empirically demonstrate that DUET finds better training data mixtures than conventional baselines in a variety of image and LLM evaluation tasks.
% \lipsum[1-1] 
\end{abstract}
\vspace{-5mm}
\section{Introduction}
\label{sec:intro}

The performance of an ML model depends heavily on the composition of training data domains \cite{data-mixing-framework-optimize,xie2023doremi} and the downstream evaluation task \cite{hoffmann2022an, Domain-adapt-transfer-learning}. For instance, if users of an LLM are interested in asking layman science questions, then fine-tuning the LLM with more Wikipedia data allows it to converse better with the users. Hence, knowing the evaluation task is important as it informs us on the relevant training data to be selected from an existing pool of data domains to produce a better-performing ML model.

However, in practice, the data (e.g., its domain, distribution, or labels) involved in a downstream \emph{unseen evaluation task} is often not known to us. So, it is not obvious what data would be relevant for training or fine-tuning the ML model. Instead, one can only deploy the ML model a few times in the unseen evaluation task to gather multiple rounds of feedback on how well our ML model has performed, thereby creating a feedback loop. Furthermore, each round of feedback incurs significant time or monetary costs. Hence, the key challenge lies in how to achieve efficiency in the number of feedback rounds to refine the training data and improve the task performance of the ML model. This problem setting has become increasingly important recently: Any LLM owner would be interested in fine-tuning its LLM to converse better with the users but due to privacy concerns \cite{li2024humancenteredprivacyresearchage}, conversations between their deployed LLM and users are end-to-end encrypted  (\url{openai.com/enterprise-privacy}). So, the LLM owner does not know the conversation domain or data seen by the deployed LLM. Rather, the LLM owner can only receive coarse feedback on how well its LLM has performed in the conversation (e.g., ratings from human users indicating their satisfaction with the LLM) and gather multiple rounds of feedback from the users.

\begin{figure}[b]
    \centering
\includegraphics[scale=0.35]{pictures/overview_final_cropped.pdf}\vspace{-4mm}
    \caption{Overview of DUET algorithm that exploits a feedback loop to optimize the data mixture for the unseen evaluation task.}
    \label{fig:overview_approach}
\end{figure}

This paper presents a novel algorithm called \textbf{DUET} (Fig.~\ref{fig:overview_approach}) that can exploit the feedback loop to optimize the training \underline{\textbf{D}}ata mixture for the \underline{\textbf{U}}nseen \underline{\textbf{E}}valuation \underline{\textbf{T}}ask. DUET is a \textit{global-to-local} algorithm that interleaves \emph{influence function} (IF) \cite{koh2017influence} as a \emph{data selection} method \cite{albalak2024surveydataselectionlanguage, ting2017optimalsubsamplinginfluencefunctions} with \emph{Bayesian optimization} (BO) \cite{bo-practical, bo-gp-ucb-10} to optimize the training data mixture for the unseen evaluation task. At the global level, BO in DUET uses feedback from the unseen evaluation task to automatically reconfigure the mixing ratio of data domains in the training data mixture iteratively. At the local level, DUET uses IF to retrieve high-quality data points from each data domain until the proposed mixing ratio is reached, improving the quality of data mixture every iteration. By doing so, DUET efficiently refines the training data mixture and improve the ML model's performance without needing to know the data involved in the unseen evaluation task.
%any data-specific information from the task.

In our problem setting, (a) 
%we do not have access to direct data information (e.g., data distribution, labels) of the unseen evaluation task 
there is no direct access to the data (e.g., its domain, distribution, or labels)
involved in the unseen evaluation task but (b) multiple rounds of feedback (details covered in Sec.~\ref{section:problem setting}) can be gathered from the task using a trained ML model. App.~\ref{app:real-world-examples} provides other practical examples of such a setting. This setting is distinctively different from those considered in conventional domain adaptation (DA) and domain generalization (DG) works. In particular, prior DA work assumes knowledge of fine-grained data (e.g., a pool of labeled/unlabeled data \cite{zhang2022fewshotadaptationpretrainednetworks} or data distribution \cite{Domain-adapt-need-unlabeled,zhang-etal-2021-matching-distribution}) from the evaluation task for selecting relevant training data that match the evaluation data. 
%In our setting, we do not know any data information from the unseen evaluation task. 
On the other hand, DG considers a rigid setting with no knowledge (not even feedback) of the evaluation task \cite{invariant_feature_DG, DG_2024_iclr, DG_survey}. Recently, works such as DoReMi \cite{xie2023doremi} have also used distributionally robust optimization (DRO) \cite{data-mixing-framework-optimize, fan2024doge_domain_reweighting} to reweigh training data domains so that a trained LLM performs well for \textit{any} distribution of downstream language tasks. However, they do not exploit feedback from the actual downstream evaluation task to improve the training data mixture. Hence, they do not work well in our setting, as shown in Sec.~\ref{sec:experiments}. Lastly, some works \cite{ruder2017learningselectdatatransfer} have used feedback to select training data for transfer learning but rely heavily on hand-crafted data features and still require knowledge of the data from the downstream evaluation task. 

Other straightforward approaches do not work well in our problem setting. A naive approach is to train an ML model on the union of data taken from every data domain. However, our work here (Sec.~\ref{subsec:main-results}) and some others \cite{xia2024lessselectinginfluentialdata} show that the trained ML model does not perform as well as a model trained using strategically selected data relevant to the evaluation task. Another brute-force approach is to iterate through all possible data mixtures (of different mixing ratios) and select one that yields the best evaluation task performance, which is not feasible due to the need to evaluate an excessive number of ML models. Lastly, App.~\ref{app:related-work}  discusses related work on data selection which, in isolation,  do not exploit feedback from the evaluation task.

% [\textbf{Talk about our setting briefly and explain why it is highly relevant now}] Our paper considers the practical setting where we need to select data from a set of fixed data domain to train an ML model for an unseen downstream task.

% Introduce the setting where the evaluation task is unknown and give examples. Need to select data from different data domains to train a model which performs well over an unseen evaluation task, where only a coarse feedback signal is available. Give the encrypted chat example.

% \begin{figure}[h]
%     \centering
% \includegraphics[scale=0.35]{pictures/overview-cropped.pdf}\vspace{0mm}
%     \caption{Overview of our algorithm DUET.}
%     \label{fig:overview-approach}
% \end{figure}

To the best of our knowledge, DUET is the first work to exploit coarse feedback from an unseen evaluation task and interleaves data selection with BO to reweigh the data domains \textit{adaptively}. After several iterations, DUET automatically assigns a higher proportion of data mixture to more relevant training data domains, consequently producing a better data mixture. The specific contributions of our work here are as follows:
\squishlisttwo
    \item We introduce a novel and realistic problem setting where 
    %we do not know the data information of an unseen evaluation task 
    the data involved in an unseen evaluation task is not known to us
    but our ML model can be deployed to gather multiple rounds of feedback from the task. Then, we introduce a novel algorithm called \textbf{DUET} that can exploit the feedback loop to optimize the training \underline{\textbf{D}}ata mixture for the \underline{\textbf{U}}nseen \underline{\textbf{E}}valuation \underline{\textbf{T}}ask. To achieve this, DUET interleaves influence function as a data selection method (Sec.~\ref{sec: solve inner}) with Bayesian optimization (Sec.~\ref{sec: solve outer}).
    \item We provide a theoretical analysis of DUET's convergence to the optimal unseen evaluation task performance by analyzing DUET's \emph{attained cumulative regret} \cite{chen2024towardsautoai,bo-kernelized-bandits} under the BO framework (Sec.~\ref{sec:theory regret}).
    \item We demonstrate the effectiveness of DUET on a variety of image classification and LLM language tasks comprising both in-domain and out-of-domain unseen evaluation tasks. Compared to conventional approaches (e.g., DoReMi or uniform weights), DUET finds better data mixtures for training ML models that perform better on the downstream unseen evaluation task (Sec.~\ref{subsec:main-results}).
    
%\end{enumerate}
\squishend
\vspace{0mm}
\section{Preliminaries}
\vspace{0mm}\subsection{Bayesian optimization}
\label{sec:bo-preliminary}

We first provide an outline of how BO can be used to optimize a generic black-box objective function. We will provide details later on how BO is used in DUET (Sec.~\ref{sec: solve outer}). We consider a black-box objective function $f : \mathbb{R}^n \mapsto \mathbb{R}$ over the space of inputs $r \in \mathbb{R}^n$. The goal is to find $r^* \triangleq \argmin_{r} f(r)$ which minimizes the objective function. BO is an \textit{active algorithm} that strategically selects input points to query the black-box objective function, conditioned on previous function observations.
At each iteration $t=1,2,\dots,T$ of BO, we query the black-box function with a selected input $r_t$ to obtain a \textit{noisy} observation $\tilde{y}_t \triangleq f(r_t) + \epsilon_t$ with a sub-Gaussian noise $\epsilon_t$ (e.g., Gaussian or bounded noise) to form the sample $(r_t,\tilde{y}_t)$. Consistent with the work of~\citet{bo-kernelized-bandits}, we model the unknown function $f$ as a realization of a 
%surrogate 
\emph{Gaussian process} (GP) \cite{gp-for-ml} that is fully specified by its \emph{prior} mean $\mu(r)$ and covariance $\kappa(r,r')$ for all $r,r' \in \mathbb{R}^n$ 
%So, two nearby inputs should have correlated observations. 
where $\kappa$ is a \textit{kernel} function chosen to characterize the correlation of the observations between any two inputs $r$ and $r'$; a common choice is the \textit{squared exponential} (SE) kernel $\kappa(r,r')\triangleq \exp(-\lVert r-r'\rVert_2^2/(2m^2))$ with a  \textit{length-scale} hyperparameter $m$ that can be learned via maximum likelihood estimation from observations.
%~\cite{gp-for-ml}. 
Given a column vector $\bm{y}_t \triangleq [\tilde{y}_{\tau}]^{\top}_{\tau=1,\dots,t}$ of noisy observations at previous inputs $r_1,\dots,r_t$, the posterior belief of $f$ at any new input $r'$ is a Gaussian distribution with the following \emph{posterior} mean and variance:
\begin{equation}
\label{gp:posterior}
\begin{split}
     & \mu_t(r') \triangleq \kappa_t^{\top}(r')(K_t + \zeta I)^{-1}\bm{y}_t \\
     & \sigma_t(r') \triangleq \kappa(r',r')-\kappa_t^{\top}(r')(K_t + \zeta I)^{-1}\kappa_t(r')
\end{split}
\end{equation}
where $\kappa_t(r') \triangleq [\kappa(r', r_{\tau})]^{\top}_{\tau=1,\dots,t}$ is a column vector, $K_t \triangleq [\kappa(r_\tau,r_{\tau'})]_{\tau,\tau' \in 1,\ldots,t}$ is a $t \times t$ covariance matrix, and $\zeta > 0$ is viewed as a free hyperparameter that depends on the problem setting \cite{bo-kernelized-bandits}. 
Using~\eqref{gp:posterior}, 
the BO algorithm selects the next input query $r_{t+1}$ by optimizing an \textit{acquisition function}, such as %uses~\eqref{gp:posterior} to  (e.g., 
minimizing the
\emph{lower confidence bound} (LCB) acquisition function \cite{bo-gp-ucb-10}: $r_{t+1} = \argmin_r\mu_t(r) - \beta_{t+1}\sigma_t(r)$ with an exploration parameter $\beta_{t+1}$. In addition, BO can also handle constraints on inputs $r$ \cite{boconstraints}. 
%as a parameter which balances exploring and exploiting the function landscape). 
The cumulative regret (for $T$ BO iterations w.r.t.~a minimization problem) $R_T \triangleq \sum_{t=1}^T [f(r_t)-f(r^*)]$ is used to assess the performance of a BO algorithm \cite{bo-kernelized-bandits, tay2023bayesian_cost} where $f(r^*)$ is the true function minimum. A lower cumulative regret indicates a faster  convergence rate of the BO algorithm. We provide a theoretical analysis of DUET's cumulative regret in Sec.~\ref{sec:theory regret}.

\vspace{0mm}\subsection{Problem setting: Optimizing data mixtures}\label{section:problem setting}
In this subsection, we formally describe our problem setting. Suppose that we have $N$ training datasets $\mathcal{D}\triangleq\{D_1,D_2,\dots,D_N\}$ from $N$ different domains (e.g., Wikipedia, ArXiv for language tasks). Hence, $\mathcal{D}$ is the union of training datasets from each domain. Let $\mathcal{L}_{\textrm{eval}}(\theta)$ be the unseen evaluation task loss w.r.t.~an ML model parameterized by $\theta$. This loss can only be observed as a coarse feedback from the unseen evaluation task and does not have a closed, mathematical form. Our goal is to find an optimal data mixture $\Xs^* \in \mathcal{D}$ (a set of training data points) and learn model parameters $\theta_{\Xs^*}$ such that the unseen evaluation task loss $\mathcal{L}_{\textrm{eval}}$ is minimized:

\begin{equation}
\label{eq:original}
\begin{aligned}
\min_{\Xs \in \mathcal{D}} \quad & \mathcal{L}_{\textrm{eval}}(\theta_{\Xs})\\
\textrm{s.t.} \quad & |\Xs|=M,
\end{aligned}
\end{equation}

where $\theta_{\Xs} \triangleq \argmin_{\theta}\mathcal{L}_{\textrm{train}}(\Xs,\theta)$ is the model parameters learnt in a standard supervised learning manner (e.g., gradient descent) from a chosen data mixture $\Xs$ and $\mathcal{L}_{\textrm{train}}$ is a standard model training loss (e.g., cross-entropy loss for LLM prediction). $M$ is a practical constraint that can be decided beforehand \cite{mirzasoleiman2020coresetsdataefficienttrainingmachine} and is used to ensure the selected data mixture is not too large. In practice, evaluation task loss $\mathcal{L}_{\textrm{eval}}$ can also be interchanged with other measures to be maximized (e.g., accuracy, user ratings).

% Using the same prior LLM example, the inversed numerical rating given by a LLM's human user serves as an example of this evaluation loss $\mathcal{L}_{\textrm{eval}}$.

% On the other hand, local optimization approaches would instead attempt to minimize $r(\theta)$ (each entry can be minimized independently) but does not guarantee global objective ($\ref{system opt}$) is minimized.
\vspace{0mm}
\section{Optimizing Training Data Mixtures using DUET}
\label{sec:introducing-abollo}

Unfortunately, solving problem~($\ref{eq:original}$) is challenging because the unseen evaluation task loss $\Leval$ does not have a closed, mathematical form and finding the optimal data mixture $\Xs^*$ directly is a high-dimensional discrete optimization problem if the size of each dataset in $\mathcal{D}$ large. 
% Prior works \cite{data-mixing-framework-optimize, xie2023doremi} have shown that the training data mixing ratio affects the performance of the trained model significantly.
To alleviate this, DUET adopts a global-to-local approach to optimize the training data mixture. At a global level, DUET exploits feedback $\Leval$ from the unseen evaluation task to iteratively refine the mixing ratio of training data domains in $\mathcal{D}$. At a local level, DUET uses IF as a data selection method to remove low-quality data points from the data mixture at each iteration.

\subsection{Reparameterization of the optimization problem} \label{sec:reparameterization}
To perform DUET effectively, we first reparameterize the objective function of problem ($\ref{eq:original}$) into a bilevel optimization problem that, at the outer level, depends on the mixing ratio $r \in \mathbb{R}^N$ of training data domains (entries in $r$ sum to 1). This reparameterized problem has a unique structure that can be solved by interleaving data selection methods with BO, which we cover in Sec.~\ref{sec: solve inner} \& \ref{sec: solve outer}.

\begin{restatable}{theorem}{reparameterization}
\label{thm:bilevel}
    $\Xs^*$, the optimal set of data points from $\mathcal{D}$, is the solution of the original problem~($\ref{eq:original}$) iff $r^*=\textrm{ratio}(\Xs^*)$ is the optimal mixing ratio solution of the reparameterized problem:
\begin{equation}
\begin{aligned}
\label{eq:reparameterized}
\min_{r \in \mathbb{R}^N} \min_{\Xs \in S_r}\quad & \mathcal{L}_{\textrm{eval}}(\theta_{\Xs}),
\end{aligned}
\end{equation}
where $S_r \triangleq \{ \Xs : \Xs \in \mathcal{D},  \text{ratio}(\Xs) = r, |\Xs|=M\}$ and $\text{ratio}(\Xs)=r$ means that the data points in $\Xs$ satisfies the given ratio $r \in \mathbb{R}^{N}$ from $N$ data domains and $\lVert r \rVert_2 = 1$.
\end{restatable}

The proof can be found in App.~\ref{app:reparameterization}, where we show that $\Xs^*$, the solution data mixture of original problem ($\ref{eq:original}$), satisfies a mixing ratio $r^*$ that is also the solution of the reparameterized problem (\ref{eq:reparameterized}). Notice that this reparameterized problem consists of an outer and inner optimization problem, and the outer problem requires us to find the optimal mixing ratio $r^*$. DUET aims to solve problem~($\ref{eq:reparameterized}$) in an iterative manner. At the outer optimization level (global), DUET uses BO to exploit feedback from the evaluation task to propose a promising mixing ratio $r_t$ at each iteration $t$. At the inner optimization level (local), we introduce a sampling strategy that uses the IF values of each data point w.r.t. its local domain to retrieve a high-quality subset of data points that satisfies the proposed mixing ratio $r_t$ and approximately solves the inner problem. By repeating the process iteratively, our approach theoretically converges (theorem.~\ref{regret-abollo}) to the optimal data mixture and outperforms other baselines in our empirical experiments (Sec.~\ref{subsec:main-results}).

To illustrate DUET qualitatively, consider an unseen evaluation task consisting of an LLM being deployed to converse with users that frequently ask layman scientific questions. At first, an LLM fine-tuned on data from different domains with uniform ratio cannot perform optimally on the evaluation task, since most of the fine-tuning data are irrelevant. In the outer optimization problem, BO in DUET uses the feedback from the task to automatically place more weight w.r.t.~mixing ratio $r$ on the Wikipedia domain (better for layman scientific questions). In the inner optimization problem, DUET uses IF to remove low-quality data points (e.g., stub articles) from Wikipedia data \cite{quality-wiki} and allows us to estimate the solution of the inner problem more accurately (Sec.~\ref{sec: solve inner}). In the next few sections, we provide details and theoretical insights involved in solving both the inner (using IF as a data selection method in Sec.~\ref{sec: solve inner}) and outer problem (using BO in Sec.~\ref{sec: solve outer}).




% \vspace{0mm}\subsection{Interweaving data selection methods with Bayesian Optimization}\label{sec:brief-description}
% Our paper proposes to solve Eq.~$\ref{eq:reparameterized}$ by interweaving data selection methods and BO in an iterative manner. Briefly, we use BO to propose promising mixing ratio $r_t$ at each BO iteration $t$ for the outer problem. At the inner level, we develop a subroutine based on existing data selection methods to retrieve high quality data points ... By doing so, we show theoretically ... our empirical results ...
% \vspace{0.5mm}
% \begin{figure}[h]
%     \centering
%     \includegraphics[scale=0.6]{pictures/random/illustrative.png}
%     \vspace{-6mm}
%     \caption{Local and system loss landscape in a simple sequential system. A-BAD-BO uses BO to find the optimal local loss configuration $\bm{r}^*$. The \textcolor{brown}{brown} region is the set $S_{\bm{r}^*}$ of $\theta$ that attains $\bm{r}^*$. Our subroutine recovers the optimal system parameters from $S_{\bm{r}^*}$.}
%     \label{fig:illustrative}\vspace{0.5mm}
% \end{figure}

\vspace{0mm}\subsection{Using data selection methods for inner problem}\label{sec: solve inner}
\vspace{0mm}

The inner optimization problem seeks to find the best-performing data mixture that satisfies the given mixing ratio $r$ from the outer level. In this section, we propose an IF-driven estimator that relies on sampling to approximately solve the inner problem given a data ratio $r$:

\begin{equation}
\begin{aligned}
\label{eq:inner}
\Xs_r^* \triangleq \argmin_{\Xs \in S_r}\quad & \mathcal{L}_{\textrm{eval}}(\theta_{\Xs}),
\end{aligned}
\end{equation}

where $S_r \triangleq \{ \Xs : \text{ratio}(\Xs) = r, |\Xs|=M\}$. To solve the inner problem, we need to find a subset of data $\Xs_r^*$ that yields the lowest evaluation task loss $y_r^* = \mathcal{L}_{\textrm{eval}}(\theta_{\Xs^*_r})$ while still constrained to the proposed mixing ratio $r$. A simple approach, based on prior works on estimating distribution extrema \cite{order_statistics_estimate_function, german_tank}, is to \emph{randomly} sample $k$ different data mixtures from $S_r$. This yields $k$ samples of training data mixtures $\{\Xs_1,\dots,\Xs_k\}$ (each satisfying the mixing ratio $r$), in which a \textbf{uniform random estimator} for $y_r^*$ can be obtained by checking the evaluation task loss of the ML model trained on each data mixture sample and taking the minimum: $\widetilde{y_r^*} = \min_{\Xs_i} \{\Leval(\theta_{\Xs_1}),\dots,\Leval(\theta_{\Xs_k})\}$ % \begin{equation}
% \label{eq:uniform-random-sample-estimator}
%     \widetilde{y_r^*} = \min_{\Xs_i} \{\Leval(\theta_{\Xs_1}),\dots,\Leval(\theta_{\Xs_k})\},
% \end{equation}
and $\widetilde{\Xs}_r^*=\argmin_{\Xs_i} \{\Leval(\theta_{\Xs_1}),\dots,\Leval(\theta_{\Xs_k})\}$ as the solution estimate of inner problem (\ref{eq:inner}). The estimator $ \tilde{y}_r^*$ is the 1st-order statistic \cite{order-statistics} and a random variable. While consistent (i.e., as we increase the sampling size $k$, we can estimate the solution of Eq.~\ref{eq:inner} more accurately), the uniform random estimator $\tilde{y}_r^*$ has high variance (we provide empirical evidence in Fig.~\ref{fig:empirical-distribution}) because from $k$ uniformly random data mixture samples, it is unlikely we can select the optimal data mixture.

We aim to improve the quality of estimator $\widetilde{y_r^*}$ by using data selection methods \cite{ijcai2022data-strategy-valuation,wangJTShapley2024} in our sampling process to improve the chance of selecting a data mixture that results in a smaller evaluation task loss. Specifically, we want to reduce the estimator's variance or bias (w.r.t.~a fixed sampling size $k$) by increasing the chance of sampling high-quality data points (conversely, reduce the chance of sampling low-quality data points) from each data domain, before using it to train an ML model. To do so, we incorporate Influence function \cite{koh2017influence} (IF), a popular data selection method that identifies high-quality data points \cite{saunshi2023understanding} into our estimator $\widetilde{y_r^*}$, and show empirically that doing so improves our estimation of the inner problem solution by reducing our estimator's bias and variance. In App.~\ref{app:other-data-selection}, we also explore and discuss the use of other data selection methods, such as coresets \cite{mirzasoleiman2020coresetsdataefficienttrainingmachine} and diversity-driven subset selection \cite{wang2024diversitymeasurementsubsetselection}. In general, we found the use of IF the most practical due to its ease of implementation and efficiency.

\textbf{IF-driven estimator}. We construct the IF-driven estimator in the following manner: first, for each dataset $D_i \in \mathcal{D}$ from the training domains, we train or fine-tune a local model on that dataset (e.g., train a model from Wikipedia data, a model from ArXiv etc.). This produces $N$ different ML models. Second, we derive the IF value of every training data point w.r.t.~the trained ML model for its respective domain (this can be computed and stored beforehand; more details in App.~\ref{app:IF-details}). Lastly, given a mixing ratio $r$ proposed at each iteration, we perform weighted sampling from each domain based on each data point's IF value within the domain dataset (instead 
of uniform sampling as mentioned previously) until we satisfy the mixing ratio $r$. From hereon, we refer to this sampling process as \emph{IF-weighted sampling}. Hence, for each data domain, there is a higher chance to sample a data point with a higher IF value. This yields a single sample of data mixture $\Xs^{IF}$. By performing IF-weighted sampling $k$ times, we obtain $k$ samples of IF-weighted data mixtures $\{\Xs_1^{IF},\dots,\Xs_k^{IF}\}$, in which we obtain a new \textbf{IF-driven estimator}:
\begin{equation}
\label{eq:IF-sample-estimator}
    \widetilde{y_{r}^*} = \min_{\Xs_i} \{\Leval(\theta_{\Xs_1^{IF}}),\dots,\Leval(\theta_{\Xs_k^{IF}})\},
\end{equation}
which we use to estimate the solution of inner optimization problem (\ref{eq:inner}). The key difference between the IF-driven estimator and the uniform random estimator is that the IF-driven estimator places higher emphasis on selecting data with high IF values, and prior works \cite{saunshi2023understanding} have regarded data points with higher IF values as of higher quality. Next, we provide empirical evidence into why the IF-driven estimator performs better than the uniform random estimator in finding better data mixtures.

\begin{figure}[h]
    \centering
\includegraphics[scale=0.5]{pictures/non-exp/obs_noise.png}
    \caption{Empirical distribution of the uniform random and IF-driven estimator $\widetilde{y_r^*}$. \textcolor{red}{Red line} is the true inner problem solution that we are estimating.}
    \label{fig:estimator-noise}
\end{figure}

In Fig.~\ref{fig:estimator-noise}, we have a simple setting of mixing data from two domains to train an ML model to maximize an evaluation task accuracy (while Eq.~\ref{eq:inner} \& \ref{eq:IF-sample-estimator} consider the minimization case, we can use $\max$ instead of $\min$ for the maximization case). Here, we use a fixed mixing ratio $r$ of 1:1. The optimal data mixture satisfying this ratio attains the evaluation task accuracy indicated by the \textcolor{red}{red line} and is also the solution of the inner optimization problem (in this example, we obtain this by iterating through all possible data mixtures in a brute-force manner). Ideally, we want our estimator to be as close to the red line as possible. Next, we plot the empirical distribution of the \textbf{uniform random estimator} and \textbf{IF-driven estimator}. Empirically, the IF-driven estimator (\textcolor{teal}{green histogram}) has a lower variance and bias than the uniform random estimator (\textcolor{gray}{gray histogram}), producing a closer estimate to the true solution (\textcolor{red}{red line}). Therefore, the IF-driven estimator $\widetilde{y_{r}^*}$ estimates the solution of Eq.~\ref{eq:inner} more accurately with lower bias and variance.

% In App.~\ref{app:other-data-selection} and our experiments, we also explore and discuss the use of other data selection methods, such as coresets and driversity-driven subset selection. In general, we find IF the most practical and effective out of various data selection methods under our setting. 

Next, we would like to characterize how close the evaluation task loss of data mixture obtained from our IF-driven estimator $ \widetilde{y_{r}^*}$ is to the optimal evaluation task loss $y_r^*$ w.r.t.~a given data ratio $r$. To do so, we theoretically analyze the estimator's empirical distribution. From our experiments, the sampling distribution of the evaluation task loss of each data mixture sample $\Leval(\theta_{\Xs^{IF}})$ is similar to a truncated exponential distribution (we provide more evidence in App.~\ref{app:empirical-sampling}). Based on this, the following theorem characterizes how well the IF-driven estimator $ \widetilde{y_{r}^*}$ estimates $y_r^*$.

\begin{restatable}{theorem}{innererror}
\label{Theorem:inner error distribution}
    Let $\{\Xs_1^{IF},\dots,\Xs_k^{IF}\}$ be $k$ samples of data mixtures drawn from $S_r$ using IF-weighted sampling. Furthermore, assume each independent sample  $\Leval(\theta_{\Xs_i^{IF}})$ follows the shifted truncated exponential distribution $y_{r}^* + \exp_t(\lambda,c)$, for  $i=1,2,\dots,k$ where $\exp_t(\lambda, c)$ is a truncated exponential distribution governed by rate parameter $\lambda$ and truncated at $c > 0$. Then, the IF-driven estimator $\widetilde{y_{r}^*}$ defined in Eq.~\ref{eq:IF-sample-estimator} is a random variable: $y_r^* + \epsilon$, where $y_r^*$ is the true inner problem solution of Eq.~\ref{eq:inner} and $\epsilon$ is a random noise variable with probability density function: \vspace{0mm} $$\textrm{pdf}_\epsilon(u)=\frac{\lambda ke^{-\lambda u}}{1-e^{-\lambda c}} \left(\frac{e^{-\lambda u} - e^{-\lambda c}}{1-e^{-\lambda c }}\right)^{\hspace{-1mm}k-1} \text{on}\ u\in[0,c]\ .$$
    
    % Then, $\Leval(\theta_{\Xs_1^{IF}}),\dots,\Leval(\theta_{\Xs_k^{IF}})$ are $k$ samples from a random sampling distribution that is lower bounded by $y_{r}^*$ (i.e., the true solution of problem \ref{eq:inner}) and upper bounded by $y_{r}^* + c$ for some positive constant $c$. It follows that, the IF-driven estimator $\widetilde{y_r^*} = \min_{\Xs_i} \{\Leval(\theta_{\Xs_1}),\dots,\Leval(\theta_{\Xs_k})\}$ follows a distribution of $y_{r}^* + \epsilon$, where $\epsilon$ is a non-negative random variable and the following holds:
    % \squishlisttwo
    %     \item If each sample $L(\theta_i) \sim U(y_{\bm{r}},y_{\bm{r}}+c)$ for  $i=1,2,\ldots,k$, then $\epsilon = c \epsilon'$ with  $\epsilon' \sim \textrm{B}(1,k)$  where B is the Beta distribution.
        
    %     \item If each sample $L(\theta_i) \sim y_{\bm{r}} + \exp_t(\lambda,c)$ for  $i=1,2,\dots,k$ where $\exp_t(\lambda, c)$ is a truncated exponential distribution governed by rate parameter $\lambda$ and truncated at $c > 0$, then $\epsilon$ is a random variable governed by the probability density function\vspace{0mm} $$\textrm{pdf}_\epsilon(u)=\frac{\lambda ke^{-\lambda u}}{1-e^{-\lambda c}} \left(\frac{e^{-\lambda u} - e^{-\lambda c}}{1-e^{-\lambda c }}\right)^{\hspace{-1mm}k-1} \text{on}\ u\in[0,c]\ .$$
    % \squishend
\end{restatable}

The proof is shown in App.~\ref{app:proof-inner-error-distribution} and computes the probability distribution of the 1st order statistic (in which our estimator uses) of a truncated exponential distribution. In App.~\ref{app:extension-other-distributions}, we also provide details to help readers extend our analysis to other empirical sampling distributions as long as they are sub-Gaussian \cite{bo-kernelized-bandits}. This theorem indicates that the support of our IF-driven estimator's distribution is on $[y_r^*,y_r^*+c]$ and so this estimator is positively biased. Furthermore, the pdf indicates that the IF-driven estimator is \textit{consistent}, since the estimation error $\epsilon$ reduces asymptotically to 0 as the sampling size $k$ increases. Surprisingly, our experiments (Sec.~\ref{sec:experiments}) show that using $k=1$ is enough to select good data mixtures, underscoring the effectiveness of the IF-driven estimator in finding high-quality data mixtures. Theorem \ref{Theorem:inner error distribution} will be used in our theoretical analysis of DUET's convergence in Sec.~\ref{sec:theory regret}.

% In Sec.~\ref{sec:theory regret}, we theoretically analyze how this error distribution in Theorem~\ref{Theorem:inner error distribution} affects our algorithm's convergence rate.

\vspace{0mm}\subsection{Using Bayesian optimization for outer problem} \label{sec: solve outer}

With the IF-driven estimator introduced to estimate the inner optimization problem solution, we shift our focus to solving the outer optimization problem of problem (\ref{eq:reparameterized}), which aims to find the optimal data mixing ratio $r^*$ for the unseen evaluation task. Since the solution of the inner problem
$y_r^* = \min_{\Xs \in S_r} \mathcal{L}_{\textrm{eval}}(\theta_{\Xs})$  depends only on the mixing ratio $r$, we can succinctly define a function $f(r) \triangleq y_r^* = \min_{\Xs \in S_r} \mathcal{L}_{\textrm{eval}}(\theta_{\Xs})$, where for a given mixing ratio $r$, we use the IF-driven estimator to estimate a solution for the inner problem, producing $f(r)$. As such, the outer optimization problem of problem~(\ref{eq:reparameterized}) can be rewritten into:
\begin{equation}
\label{eq:outer-problem}
  \textstyle  \min_{r} f(r).
\end{equation}
where $r \in \mathbb{R}^{N}$ is the mixing ratio over the $N$ training domains and the sum of entries in $r$ is constrained to 1 (since it is a ratio). DUET uses BO with constraints of $\lVert r\rVert _ 2 = 1$ (Sec.~\ref{sec:bo-preliminary}) to find the optimal data mixture ratio $r^*$ to solve outer problem~(\ref{eq:outer-problem}). BO is suitable for solving this problem for a few reasons. First, evaluating $f$ requires us to use the IF-driven estimator to estimate the inner optimization problem solution and thus $f$ is a black-box function with no closed, mathematical form; BO is a principled and popular framework to optimize such black-box functions \cite{Book_garnett2023bayesian,BO-drug}. 
% Second, unlike original data selection problem~\eqref{eq:original}, which is combinatorial in nature over all possible data points, objective function $f$ has continuous inputs (the mixing ratio $r$) of a much lower dimension (equals to the number of data domains $N$).% Second, $f$ here can be shown to be continuous \cite{folkman1967continuity} and so it is appropriate for us to model $f$ as a realization of a GP, allowing us to perform BO over it. 
Second, we can only estimate the inner problem solution (Theorem~\ref{Theorem:inner error distribution}) using our IF-driven estimator introduced in the previous section. Hence, this implies we can only obtain \textit{noisy observations} $f(r) + \epsilon$, where $\epsilon$ is a random noise variable with the same distribution as that in theorem \ref{Theorem:inner error distribution}; fortunately, BO handles noisy function observations gracefully \cite{bo-gp-ucb-10, bo-kernelized-bandits} during the optimization process, allowing us to find the optimal mixing ratio eventually (theoretical results shown in Sec.~\ref{sec:theory regret}).


% Show that the outer problem can be tackled with BO directly, once we have developed a subroutine to estimate the inner problem. Talk about observation noise.


\subsection{Interleaving the IF-driven estimator and BO}
DUET uses BO at the outer level and IF-driven estimator at the inner level to iteratively optimize the data mixture, solving problem (\ref{eq:reparameterized}). We describe DUET in Algorithm.~\ref{alg:ABOLLO}.
\begin{algorithm}[h]
   \caption{\textbf{DUET}: Optimizing \textbf{\underline{D}}ata Mixtures for \textbf{\underline{U}}nseen \textbf{\underline{E}}valuation \textbf{\underline{T}}ask}
   \label{alg:ABOLLO}
\begin{algorithmic}[1]
   \STATE {\bfseries Input:} $N$ training datasets from $N$ domains $\{D_1,\dots,D_N\}$. Computed $IF$ values of each data point (App.~\ref{app:IF-details}) w.r.t.~its domain dataset and locally trained model. Initial observation of data mixture ratio and evaluation task performance: $\mathcal{D}_{0}\triangleq\{(r_0, \tilde{y}_0)\}$, SE kernel $\kappa$, sampling size $k$, parameter $\beta_t$ for acquisition step and total number of BO iterations $T$.
   \FOR{$t=1,\dots,T$}
   
   \STATE $r_{t} =  \argmin_{r} \mu_t(r) - \beta_t \sigma_t(r)$ (BO acquisition step)
   
   \STATE
    IF-weighted sampling to obtain $k$ samples of data mixtures $\{\Xs_1^{IF},\dots,\Xs_k^{IF}\}$ (Sec.~\ref{sec: solve inner}).

    \STATE
    \textbf{IF-driven estimator} at iteration $t$: \\ $\widetilde{y_t^*} = \min_{\Xs_i} \{\Leval(\theta_{\Xs_1^{IF}}),\dots,\Leval(\theta_{\Xs_k^{IF}})\}$.

    \STATE
    Keep track of best performing data mixture $\Xs_t^* = \argmin_{\Xs_i} \{\Leval(\theta_{\Xs_1^{IF}}),\dots,\Leval(\theta_{\Xs_k^{IF}})\}$.

    \STATE
    $\mathcal{D}_{t} = \mathcal{D}_{t-1} \cup \left\{\left(r_{t}, \widetilde{y_{t}^*}\right)\right\}$

    \STATE
    Update the GP posterior and $\kappa$ with updated observations $\mathcal{D}_{t+1}$ (Sec.~\ref{sec:bo-preliminary}).

   
   \ENDFOR
   \STATE $\Xs^* = \argmin_{\Xs_i^* \in \{ \Xs_1^*,\dots,\Xs_T^*\}} \Leval(\theta_{\Xs_i^*})$
   
\end{algorithmic}
\end{algorithm}
\vspace{-2mm}

At iteration $t$, DUET uses the LCB acquisition function \cite{bo-gp-ucb-10} on the GP posterior to propose a candidate mixing ratio $r_t$ for our data domains (Line 3). Using the proposed mixing ratio $r_t$, we use IF values of each data point to compute the IF-driven estimator $\widetilde{y_r^*}$ and keep track of the best performing data mixture $\Xs_t^*$ at current iteration $t$ (Line 4, 5 and 6). Note that the data mixture $\Xs_t^*$ at each iteration $t$ satisfies the proposed mixing ratio $r_t$. Next, we include $(r_{t+1}, \widetilde{y_{t}^*})$ into our historical observations $\mathcal{D}_{t+1}$ (Line 7) and update our GP posterior (Line 8). After which, we repeat the entire process, until the budget of $T$ BO iterations is exhausted. In the end, we recover the best performing data mixture $\Xs^*$ (Line 10).

DUET can be implemented easily by LLM practitioners. Once a data mixture is sampled using the IF-driven estimator to fine-tune the LLM at each BO iteration, the trained LLM can be deployed for a small period of time (e.g., one day on a small subset of users) to gather feedback (e.g., user rating) from conversations with human users. Then, DUET proposes a new data mixing ratio to refine the training data mixture. As seen from our experiments (Sec.~\ref{sec:experiments}), the model performance on the unseen evaluation task improves as DUET progressively optimizes the data mixture to be more relevant to the task.



\section{Theoretical Analysis}\label{sec:theory regret}

\subsection{Convergence analysis using cumulative regret}
We analyze the convergence rate of DUET using the growth of \textit{attained cumulative regret} \cite{chen2024towardsautoai} $\tilde{R}_T = \sum_{t=1}^T |\widetilde{y^*_{r_t}}-f(r_t)| = \sum_{t=1}^T |f(r^*) + \epsilon_t - f(r_t)|$ for $T$ BO iterations. The attained cumulative regret consists of two terms, where $|f(r^*)-f(r_{r_t})|$ indicates the quality of mixing ratio $r_t$ proposed at each iteration while $\epsilon_t$ indicates how well we can estimate the inner problem solution at every iteration. By analyzing the attained \textit{average} regret $\tilde{R}_T/T$ with $T \rightarrow \infty$, the following theorem helps us understand how close our algorithm converges \cite{BO-unknown-param-regret-explanation} to the optimal evaluation task loss with increasing number of BO iterations $T$.

% A lower cumulative regret indicates a faster algorithm convergence rate \cite{seb2023nash, bo-kernelized-bandits}.

\begin{restatable}{theorem}{regret}
\label{regret-abollo}
Let $f$ be the outer problem objective defined in Eq.~\ref{eq:outer-problem} with bounded RKHS norm: $||f||_{\kappa}=\sqrt{	\langle f,f \rangle_\kappa}$. Also, let our IF-driven estimator for the inner problem solution be governed by the error distribution introduced in Theorem~\ref{Theorem:inner error distribution} with constant $c$ and $\lambda=1$. Let $A_{c,k} = \frac{c^2(1-e^{-c}-\frac{c}{2})^{k-1}}{(1-e^{-c})^k}$, where $k$ is a fixed predecided sampling size. Then, running DUET over $f$ using the LCB acquisition function found in \citep{bo-kernelized-bandits} at each BO iteration $t=1,\dots,T$ yields the following \textbf{attained average regret} \cite{chen2024towardsautoai} upper bound with probability at least $1-\delta$:
% \begin{multline}
%     \displaystyle \lim_{T\xrightarrow{}\infty}\frac{\tilde{R}_T}{T} \leq \frac{6}{k} + \frac{2((1-e^{-c})-\frac{c}{2})^{k-1}}{(1-e^{- c})^k} \\ + \sqrt{\frac{6}{\sqrt{\delta} k} +  \frac{2((1-e^{-c})-\frac{c}{2})^{k-1}}{\sqrt{\delta}(1-e^{- c})^k}}
% \end{multline}
% \end{restatable}

$$
    \displaystyle \lim_{T\xrightarrow{}\infty}\frac{\tilde{R}_T}{T} \leq \frac{6(\sqrt[4]{\delta}+\sqrt{k})}{\sqrt[4]{\delta}k} + 2A_{c,k} + \frac{\sqrt{2A_{c,k}}}{\sqrt[4]{\delta}}.
$$
\end{restatable}
The proof is provided in App.~\ref{proof-regret} and bounds $|f(r^*)-f(r_{r_t})|$  and $\epsilon_t$ independently using  BO regret analysis \cite{chen2024towardsautoai, bo-kernelized-bandits} and the error distribution defined in Theorem.~\ref{Theorem:inner error distribution}. Our theorem's average regret indicates how close our algorithm converges to the optimal evaluation task loss with increasing BO iteration $T$ and different choices of sampling size $k$. Notice that because $c$ characterizes the error of our estimator in Theorem.~\ref{Theorem:inner error distribution}, a larger $c$ would decrease $A_{c,k}$ and our average regret. In addition, a larger sampling size $k$ reduces the estimation error of the inner problem (Theorem.~\ref{Theorem:inner error distribution}), decreasing $A_{c,k}$ and also reduces our regret bound, allowing us to achieve a better-performing data mixture.

In practice, using a large $k$ is computationally expensive because we need to use our IF-driven estimator to sample data mixtures and train our ML models $k$ times at each iteration (selecting one that attains the smallest $\Leval$). Fortunately, our experiments (Sec.~\ref{subsec:main-results}) show that setting $k=1$ is sufficient to achieve better results than other baselines. If computational resource is not an issue, we can also consider setting an adaptive sampling size \cite{chen2024towardsautoai} that increases w.r.t.~each iteration $t$.

% We provide additional analysis on our algorithm's convergence rate w.r.t.~adaptive sampling sizes in App.XXX.

% \subsection{Practical considerations}\label{sec:practical_con}
% There are several ways to improve y good results in our experiments (Sec.~\ref{sec:optimality_results}).


\section{Experiments and Discussion}\label{sec:experiments}
In this section, we conduct extensive experiments to showcase the effectiveness of DUET compared to other baselines. Our experimental evaluation pipeline is constructed as follows: \textit{first}, we select data mixtures from different data domains with DUET or other baselines. \textit{Second}, we train or fine-tune an ML model according to the selected data mixture. \textit{Third}, we deploy the ML model on the unseen evaluation task to evaluate how well the model has performed. In the next subsection, we provide more details next on how our experiment is setup with varying training and evaluation data domains to showcase DUET's effectiveness even in traditionally difficult out-of-domain scenarios. \textbf{Our code is in the supplementary material folder}.


\subsection{Experimental setup}\label{setup}
Our experiments are carried out on two broad classes of evaluation tasks. The first consists of image classification tasks by a VGG-16 model \cite{vgg} over different object domains \cite{imagenet,fashion-mnist-original}. The second consists of LLM evaluation tasks by a Llama-8b-Instruct model \cite{llama} across different knowledge domains. The image training data consist of binary classification of 4 different clothing types (\textbf{Shirt}, \textbf{Boots}, \textbf{Sandals}, \textbf{Bags}) from the FashionMNIST dataset \cite{fashion-mnist-original} and cat/dog classification from the \textbf{Dog} \& \textbf{Cat} dataset \cite{catsdogsclassification} (abbreviated as \textbf{Dog} in our plots). The training data domains for LLM evaluation consists of 9 topics: \textbf{Wikitext} \cite{wikitext-data}, \textbf{gsm8k} \cite{gsm8k}, \textbf{PubmedQA} \cite{pubmedqa}, \textbf{HeadQA} \cite{headqa} , \textbf{SciQ} \cite{sciq}, \textbf{TriviaQA} \cite{triviaQA}, \textbf{TruthfulQA} \cite{truthfulQA}, \textbf{Hellaswag} \cite{hellaswag}, and \textbf{CommonsenseQA} \cite{talmor2019commonsenseqaquestionansweringchallenge}. These domains are chosen specifically for their diversity to mimic topics seen by user-facing LLMs. In our experiments, we vary the difficulty of the unseen evaluation task by adjusting the training and evaluation data domains (see captions of Fig.~\ref{experiment:main-images} \& \ref{experiment:main-llm} for more information).

We compare our algorithm with several other baselines:
\textbf{DoReMi} is a DRO-driven approach which optimizes the data mixture so that the trained ML model performs well for any evaluation task domain distributions (the original algorithm is used for pre-training, but in our LLM setting we fine-tune our LLM instead).
The \textbf{Uniform weights} baseline samples from the training data domains uniformly to produce a data mixture of uniform ratio across different domains. We use DUET with a few different data selection methods: \textbf{DUET-IF} is our main method that uses our IF-driven estimator (Eq.~\ref{eq:IF-sample-estimator}) to select data mixtures at each BO iteration; \textbf{DUET-UR}, introduced in Sec.~\ref{sec: solve inner}, uses the uniform random estimator and randomly selects data mixtures that satisfy the proposed mixing ratio; \textbf{DUET-RH} (\textbf{R}emove \textbf{H}armful) removes the 20\% of data points with the lowest IF values from each data domain, before random sampling from the leftover data points. Other data selection baselines are discussed and shown in App.~\ref{app:additional-results-log-det}. We use a sampling size of $k=1$ and BO iterations $T=30$ for image classification and $T=10$ for language tasks. We also constrained the total number of selected data points to $M=10000$. 

\subsection{Main result}\label{subsec:main-results}
\begin{figure*}[h]
\centering
\subfloat[\textbf{Bag \& Boots}]
{\includegraphics[width=0.25\textwidth]
{pictures/exp/images/WITH_DOREMI_bag_boots_in_dist.png}\label{fig:sub1}
}
\subfloat[\textbf{Sneaker \& Shirt}]
{\includegraphics[width=0.25\textwidth]
{pictures/exp/images/WITH_DOREMI_sneaker_shirt_in_dist.png}\label{fig:sub1}
}
\subfloat[\textbf{\underline{Bag}}]
{\includegraphics[width=0.25\textwidth]
{pictures/exp/images/WITH_DOREMI_bag_ood.png}\label{fig:sub1}
}
\subfloat[\textbf{\underline{Shirt, Dog}}]
{\includegraphics[width=0.25\textwidth]
{pictures/exp/images/WITH_DOREMI_shirt_CD_ood.png}\label{fig:sub1}
}
\vspace{0mm}
\caption{Comparison of \textbf{DUET}'s convergence with other baselines for unseen image classification task domains (higher is better) over 30 iterations. The subcaptions denote the evaluation task domains. \underline{\textbf{Underlined evaluation tasks}} are more difficult because the evaluation task domains are removed from the training data (i.e., they are out-of-distribution).}
\label{experiment:main-images}
\subfloat[\textbf{TruthfulQA}]
{\includegraphics[width=0.241\textwidth]
{pictures/exp/llm/LLM_WITH_DOREMI_TRUTHFUL_ood.png}\label{fig:sub1}
}
\subfloat[\textbf{\underline{gsm8k}}]
{\includegraphics[width=0.241\textwidth]
{pictures/exp/llm/LLM_WITH_DOREMI_GSM8K_ood.png}\label{fig:sub1}
}
\subfloat[\textbf{\underline{PubMedQA, HeadQA}}]
{\includegraphics[width=0.241\textwidth]
{pictures/exp/llm/LLM_WITH_DOREMI_headQA_pubmedQA_ood.png}\label{fig:sub1}
}
\subfloat[\textbf{\underline{Commonsense, TriviaQA}}]
{\includegraphics[width=0.241\textwidth]
{pictures/exp/llm/LLM_WITH_DOREMI_commonsense_trivial_in_dist.png}\label{fig:sub1}
}
\vspace{0mm}
\caption{Results on unseen LLM evaluation task domains over 10 iterations based on the same setting as that in Fig.~\ref{experiment:main-images} (higher is better). The subcaptions denote the evaluation task domains. \underline{\textbf{Underlined evaluation tasks}} are more difficult because the evaluation task domains are removed from the training data (i.e., they are out-of-distribution). All results are done in a 0-shot setting with no special prompts.}
\vspace{-2mm}
\label{experiment:main-llm}
\end{figure*}
\textbf{DUET finds better data mixtures.} Our result (Fig.~\ref{experiment:main-images} \& \ref{experiment:main-llm}) shows that in different evaluation tasks, DUET finds data mixtures that produce better-performing ML models within a few iterations of feedback loops. The first column in Fig.~\ref{experiment:main-images} and \ref{experiment:main-llm} (for both image classification and LLM) consists of a relatively easier task where the evaluation task domain is found in the training task domains. In this case, DUET (\textcolor{teal}{green plot}) uses feedback from the evaluation task to find the optimal data mixture with more emphasis on the relevant training data domain. On the other hand, DoReMi (\textcolor{orange}{orange dotted line}) cannot adapt to the evaluation task and hence produces worse data mixtures. In the 2nd, 3rd and 4th columns, we increased the difficulty of our evaluation task by removing the evaluation task domain from our training domains (so, the task is out-of-domain). Surprisingly, even for these cases, DUET can still use the unseen evaluation task feedback to automatically improve the quality of the data mixture, achieving better model performance. This is because data from certain training domains could still be useful for the out-of-domain evaluation task (e.g., \textbf{Wikitext} data can still be helpful for mathematical questions in \textbf{gsm8k}). Hence, DUET uses feedback from the unseen evaluation task to place higher weights on more relevant training data domains. In App.~\ref{app:additional-results}, we provide more experimental results for different combinations of evaluation tasks to showcase the effectiveness of DUET.

\textbf{IF is an effective data selection method}. Our result also shows that DUET-IF, which uses the IF-driven estimator (Eq.~\ref{eq:IF-sample-estimator}) to place more sampling emphasis on data points with high IF values, performs better than DUET-UR and DUET-RH. This showcases the effectiveness of using IF values for DUET to work effectively, as compared to other data selection methods.

% we noticed that even if a portion of the evaluation task comes from data domains not found in the training data, our approach can still automatically find data mixtures that improve an ML model performance over that task [\textbf{better than other baselines}. This occurs because some domains, such as Wikipedia and PubmedQA, are distinct but still contain overlapping information. Hence, even when the evaluation task contains questions from PubmedQA data, which is not found in our training domains, feedback from prior iterations indicates that Wikipedia data is helpful for the evaluation task, and so our algorithm automatically assigns more data from the Wikitext domain.

% \begin{figure*}[h]
% \centering
% \subfloat[\textbf{Bag \& Boots}]
% {\includegraphics[width=0.25\textwidth]
% {pictures/exp/images/WITH_DOREMI_bag_boots_in_dist.png}\label{fig:sub1}
% }
% \subfloat[\textbf{Sneaker \& Shirt}]
% {\includegraphics[width=0.25\textwidth]
% {pictures/exp/images/WITH_DOREMI_sneaker_shirt_in_dist.png}\label{fig:sub1}
% }
% \subfloat[\textbf{Bag (OOD)}]
% {\includegraphics[width=0.25\textwidth]
% {pictures/exp/images/WITH_DOREMI_bag_ood.png}\label{fig:sub1}
% }
% \subfloat[\textbf{Shirt \& Dog (OOD)}]
% {\includegraphics[width=0.25\textwidth]
% {pictures/exp/images/WITH_DOREMI_shirt_CD_ood.png}\label{fig:sub1}
% }
% \vspace{0mm}
% \caption{Comparison of \textbf{DUET}'s convergence with other baselines in finding optimal data mixture for the downstream evaluation task (Higher is better) over 30 iterations. The subcaptions denote the evaluation task domains. Subfigures (c) and (d) are more difficult evaluation tasks because the evaluation task domains are removed from the training data (i.e., out-of-distribution).}
% \label{experiment:main-images}
% \subfloat[\textbf{TruthfulQA (OOD)}]
% {\includegraphics[width=0.25\textwidth]
% {pictures/exp/llm/LLM_WITH_DOREMI_TRUTHFUL_ood.png}\label{fig:sub1}
% }
% \subfloat[\textbf{TruthfulQA (OOD)}]
% {\includegraphics[width=0.25\textwidth]
% {pictures/dummy.jpg}\label{fig:sub1}
% }
% \subfloat[\textbf{gsm8k (OOD)}]
% {\includegraphics[width=0.25\textwidth]
% {pictures/dummy.jpg}\label{fig:sub1}
% }
% \subfloat[\textbf{CommonsenseQA \& TriviaQA}]
% {\includegraphics[width=0.25\textwidth]
% {pictures/dummy.jpg}\label{fig:sub1}
% }
% \vspace{0mm}
% \caption{Results on different LLM evaluation task domains over 10 iterations, with the same setting in Figure 3 (Higher is better).}
% \label{experiment:main-llm}
% \end{figure*}

% % \vspace{0mm}\subsection{Ablation study on sampling size $k$}\label{sec:ablation}
% % In this section, we investigate how varying sampling size $k$ influences our algorithm's performance.

\vspace{-2mm}
\subsection{Ablation study on different components of DUET}
Next, we perform ablation studies to qualitatively analyze the influence of BO and different data selection methods on DUET's convergence to the optimal evaluation task performance. For clarity purpose, we only use the results for one evaluation task for our analysis. \begin{figure}[h]
    \centering
\includegraphics[scale=0.37]{pictures/ablation_cropped.pdf}
\vspace{-1mm}
    \caption{Performance gains attained by different components of our algorithm. BO and data selection methods both increase the performance of the ML model on an unseen evaluation task.}
    \label{fig:ablation-detailed-effect}
\end{figure}
Fig.~\ref{fig:ablation-detailed-effect} shows that by naively using a uniform data mixture and training an ML model, we can only achieve an evaluation task performance given by the \textcolor{red}{red dotted line}. With only BO, DUET automatically reconfigures the mixing ratio and attains performance gain (\textbf{A}) over the uniform training data mixture. Next, by incorporating data selection methods, such as using IF values in DUET-IF, we attain even more performance gains (\textbf{B}) indicated by the \textcolor{teal}{green plot}. This is because using IF values helps to retrieve higher-quality data points at each iteration and reduces the estimation error of our inner problem (Sec.~\ref{sec: solve inner}), yielding higher-quality data mixtures. Lastly, different data selection methods have varying effectiveness and yield different performance gains for DUET (\textbf{C}). Here, we see that the IF-driven estimator attains the best performing data mixture in DUET-IF as compared to other data selection methods (e.g. DUET-RH). We also show more ablation studies w.r.t. the use of larger sampling size $k$ and other diversity-driven data selection methods in App.~\ref{app:additional-results-log-det}. In general, our results show that increasing $k$ improves the convergence of DUET. We also found that diversity-driven data selection methods \cite{wang2024diversitymeasurementsubsetselection} are too computationally expensive to be practical in our setting even with a greedy implementation \cite{log_det_fast_compute}.

% \begin{figure}[h]
%     \centering
% \includegraphics[scale=0.4]{pictures/exp/ablation in depth annotated-cropped.pdf}
%     \caption{Performance gains attained by different components of our algorithm.}
%     \label{fig:ablation-detailed-effect}
% \end{figure}


% \begin{figure*}[h]
% \centering
% \subfloat[\textbf{Bag \& Boots}]
% {\includegraphics[width=0.24\textwidth]
% {pictures/exp/images/in_dist/bag_boots.png}\label{fig:sub1}
% }
% \subfloat[\textbf{Bag, Boots \& Sandals}]
% {\includegraphics[width=0.24\textwidth]
% {pictures/exp/images/in_dist/bag_boots_sandals.png}\label{fig:sub1}
% }
% \subfloat[\textbf{Dog \& Shirt}]
% {\includegraphics[width=0.24\textwidth]
% {pictures/exp/main_result_images.png}\label{fig:sub1}
% }
% \subfloat[\textbf{Mixed domains}]
% {\includegraphics[width=0.24\textwidth]
% {pictures/exp/main_result_images.png}\label{fig:sub1}
% }
% \vspace{0mm}
% \caption{Comparison of \textbf{XXX}'s convergence with other baselines in finding optimal data mixture for the downstream unseen task (Higher is better). The subcaption (a), (b), (c) \& (d) denotes the evaluaton task of varying difficulty (more details in App. XXX). \textcolor{red}{Red} captions denote that the evaluation domain is removed from the training domain, to make the task more difficult.}
% \label{experiment:optimality}
% \vspace{0mm}
% \subfloat[\textbf{TruthfulQA}]
% {\includegraphics[width=0.24\textwidth]
% {pictures/dummy.jpg}\label{fig:sub1}
% }
% \subfloat[\textbf{Wikitext \& TriviaQA}]
% {\includegraphics[width=0.24\textwidth]
% {pictures/dummy.jpg}\label{fig:sub1}
% }
% \subfloat[\textbf{Math GSM8K}]
% {\includegraphics[width=0.24\textwidth]
% {pictures/dummy.jpg}\label{fig:sub1}
% }
% \subfloat[\textbf{Mixed domains}]
% {\includegraphics[width=0.24\textwidth]
% {pictures/dummy.jpg}\label{fig:sub1}
% }
% \vspace{0mm}
% \caption{Results on different LLM evaluation tasks with the same setting as Figure 3 (Higher is better).}
% \label{experiment:ablation}
% \end{figure*}

% \vspace{0mm}\subsection{Ablation study on sampling size $k$}\label{sec:ablation}
% In this section, we investigate how varying sampling size $k$ influences our algorithm's performance.

% \begin{figure}[h]
%     \centering
% \includegraphics[scale=0.4]{pictures/exp/ablation in depth annotated-cropped.pdf}
%     \caption{Performance gains attained by different components of our algorithm.}
%     \label{fig:ablation-sample-size}
% \end{figure}

% Fig.~\ref{fig:ablation-sample-size} shows that our algorithm attains faster convergence to better performance on the evaluation task with increasing sampling size $k$. This corroborates our findings in Theorem.~\ref{Theorem:inner error distribution}, where a larger sampling size $k$ helps to reduce the bias and variance of our estimator for the inner problem. Consequentially, this results in a smaller regret bound in Theorem.~\ref{regret-abollo}. Despite this finding, it is important to note using a large sample size $k$ might not be feasible in practice - it is resource-intensive to repeatedly train an ML model from every data mixture sample at each BO iteration. Our main results also show that setting sample size $k=1$ is sufficient for our algorithm to find better-performing data mixtures as compared to other baselines.
% \begin{figure} [t]
%     \centering
%   \subfloat[\textbf{Synthetic}\label{1a}]{%
%        \includegraphics[width=0.5\linewidth]{pictures/experiments/color_resize/synthetic_ablation_markers_blue.png}}
%     \hfill
%   \subfloat[\textbf{MNIST}\label{1b}]{%
%         \includegraphics[width=0.5\linewidth]{pictures/experiments/color_resize/mnist_large_ablation_markers_blue.png}}
%     \\[-2ex]
%   \subfloat[\textbf{Healthcare}\label{1c}]{%
%         \includegraphics[width=0.5\linewidth]{pictures/experiments/color_resize/Healthcare_Ablation_markers_blue.png}}
%     \hfill
%   \subfloat[\textbf{LLM prompt engineering}\label{1d}]{%
%         \includegraphics[width=0.5\linewidth]{pictures/experiments/LLM_ablation_markers.png}}
%   \caption{Ablation study on how sampling size $k$ affects \textbf{ABOLLO}'s convergence rate to  (Lower is better). Increasing sampling size $k$ generally improves \textbf{ABOLLO}'s convergence rate.}
% \label{experiment:ablation}
% \end{figure}


% \begin{figure} [t]
%     \centering
%   \subfloat[\textbf{Synthetic}\label{synthetic combined}]{%
%        \includegraphics[width=0.5\linewidth]{pictures/dummy.jpg}}
%     %
%   \subfloat[\textbf{MNIST}\label{mnist combined}]{%
%         \includegraphics[width=0.5\linewidth]{pictures/dummy.jpg}}
%     \\[-0.5ex]
%   \subfloat[\textbf{Healthcare}\label{healthcare combined}]{%
%         \includegraphics[width=0.5\linewidth]{pictures/dummy.jpg}}
%     %
%   \subfloat[\textbf{LLM}\label{llm combined}]{%
%         \includegraphics[width=0.5\linewidth]{pictures/dummy.jpg}}
%     \caption{Increasing $k$ and number of BO iterations \emph{both} improves the performance of A-BAD-BO. Moreover, for a fixed system query budget $kT$, it is better to choose moderate $k$ and $T$ values.}\vspace{2mm}
    
% \label{combined-effect}
% \end{figure}
\vspace{-2mm}
\section{Conclusion}
Our paper proposes DUET, a novel algorithm that exploits multiple rounds of feedback from a downstream unseen evaluation task to automatically optimize training data mixture. We provide theoretical guarantees of DUET and show that it finds better data mixtures in a variety of image and LLM evaluation tasks as compared to other conventional baselines. In light of the growing importance of our problem setting where we do not know the data in an unseen evaluation task is not known, we hope our work inspires future research to use coarse feedback from the evaluation task to refine the training data mixture for ML models.


\newpage

% \section*{Acknowledgements}
% This research is supported by the National Research Foundation (NRF), Prime Minister's Office, Singapore under its Campus for Research Excellence and Technological Enterprise (CREATE) programme. The Mens, Manus, and Machina (M3S) is an interdisciplinary research group (IRG) of the Singapore MIT Alliance for Research and Technology (SMART) centre.
% This research/project is supported by the National Research Foundation Singapore and DSO National Laboratories under the AI Singapore Programme (AISG Award No: AISG$2$-RP-$2020$-$018$).
% Zhiliang Chen is supported by the Institute for Infocomm Research of Agency for Science, Technology and
% Research (A$^\ast$STAR).

% \section*{Impact Statement}
% Our work presents an algorithm that iteratively optimizes data mixtures to improve the performance of an ML model's based on feedback from an unseen evaluation task. We find our work highly relevant and impactful in many real-world settings where practitioners can only gather feedback from unseen evaluation tasks.  We do not foresee any obvious negative ethical impacts from our work.
\bibliography{example_paper}
\bibliographystyle{icml2024}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
% \section{Notations}
% Here, we provide a list of important mathematical notations used frequently in our paper. Some of these notations could contain subscripts in our paper to denote the context where they are used (e.g., in BO iterations). We have defined them accordingly in places where these notations are introduced throughout the paper and this table can be used for ease of reference.

% \begin{table}[h]
% \caption{Important mathematical notations used in this paper.}
% \label{sample-table}
% \vskip 0.15in
% \begin{center}
% \begin{small}

% \begin{tabular}{ll}
% \toprule
% Notation & Definition\\
% \midrule
% $\theta^{(i)}$    & Trainable parameters in ML component $i$.  \\
% $\theta$ & Trainable parameters across \textit{all} ML components (also referred to as system parameters).\\
% $l_i(\theta^{(i)})$    & Local loss function of ML component $i$. \\
% $r(\theta)$    & Local loss vector function representing local loss function of \textit{every} ML component.\\
% $L(\theta)$     & System loss w.r.t.~$\theta$. \\
% $\bm{r}$      & Local loss configuration (a realization of function vector $r(\theta)$). \\
% $S_{\bm{r}}$      & The set $\{\theta\mid r(\theta) = \bm{r}\}$. \\
% $S'_{\bm{r}}$      & The set $\{L(\theta)\mid r(\theta) = \bm{r}\}$.
% \\
% $y_{\bm{r}}$   & minimum for the inner problem. \\
% $\tilde{y}$   & Our subroutine's estimated minimum for the inner problem. \\
% $k$   &  Sampling size used by our subroutine. \\
% $\epsilon$   &  Estimation error of our subroutine. Also treated as \textit{observation noise} in A-BAD-BO under the BO framework.\\
% $f(\bm{r})$   & Function representation (with no analytical form) of outer problem. Modeled as a realization of a GP. \\
% $\Tilde{R}_T$   & Attained cumulative regret after $T$ BO iterations (exact definition found in Sec.~\ref{sec:theory regret}). \\
% \bottomrule
% \end{tabular}

% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table}

\section{Additional Discussions}


\subsection{Real-world examples of our problem setting} \label{app:real-world-examples}
In our problem setting, (a) there is no direct access to the data  (e.g., its domain, distribution, or labels) involved in the unseen evaluation task but (b) multiple rounds of coarse feedback (details covered in Sec.~\ref{section:problem setting}) can be gathered from the task using a trained ML model. Here, we provide several real-world examples in which such a setting occurs.

\textbf{End-to-end encrypted conversations between LLM and users.}
This setting is specific to the conversational setting between a trained LLM and human users. LLM owners are interested in fine-tuning an LLM to converse well
with some human-user demographics but due to real-world
privacy concerns \cite{li2024humancenteredprivacyresearchage}, conversations between a
deployed LLM and users are end-to-end encrypted during
test-time (\url{openai.com/enterprise-privacy}). So,
an LLM owner does not have any knowledge of the conversation domain or the (unlabeled or labeled) data seen during
test-time. Instead, they only receive a feedback on
how well the LLM has performed in the conversation (e.g.,
ratings from the human user, how long each user stays on the applicaton). The LLM owner can collect multiple
rounds of feedback over a period of time. Hence, they can exploit this feedback to iteratively refine the training data for the ML model. Many chat-driven applications (e.g., whatsapp, telegram) nowadays use end-to-end encrypted chats, so our problem setting is relevant here.

\textbf{Model marketplace.} In addition, there are other scenarios in which a model owner needs to improve an ML model without having access to the data involved in the unseen evaluation task. For instance, an ML model owner might rent or sell an image classification model in a model marketplace (e.g., \url{https://aws.amazon.com/marketplace/solutions/machine-learning}). However, the consumer might give feedback (e.g., how often the model makes mistakes) to the ML model owner in hope that the ML model owner can improve the model's performance on its own evaluation task. Furthermore, the images used by the consumer in its evaluation task are considered sensitive data, so the ML model owner does not know any data involved in the unseen evaluation task. Hence, the ML owner can only rely on feedback from the consumer to improve the model's performance.

\subsection{More related works}\label{app:related-work}
Recently, a large class of data selection methods utilizing coresets, diversity or influence functions \cite{zhang2024speculativecoresetselectiontaskspecific, xia2024lessselectinginfluentialdata,koh2017influence} have been introduced to retrieve a smaller subset of data from an existing dataset. These data selection methods have become popular because they reduce training dataset size (which is an attractive feature when traning LLMs) and prior work \cite{xia2024lessselectinginfluentialdata} showed that training a model with strategically selected data points allows it to perform better. However, these works, when used in isolation, do not work well in our setting because they do not exploit feedback from an unseen evaluation task. For example, even if we can retrieve a high-quality data subset from an original dataset of a training domain, that domain might not even be relevant to the unseen evaluation task. Hence, data selection methods on their own \textbf{are not applicable to our setting}. Instead, our paper's algorithm interleaves BO and data selection method together to exploit feedback from the unseen evaluation task to optimize our training data mixture.

\subsection{Influence function and its calculations} \label{app:IF-details}
Influence function (IF) \cite{koh2017influence} has been developed to study the influence of a single data point on an ML model's predictions. In this section we provide a summary of IF and its derivation. The influence of
a data point $z$ on the loss of a test data point (or a set of test data points) $z_{\textrm{test}}$ for an ML model parameterized by $\theta$ is given by the
closed-form expression:

\begin{equation}
    \textrm{IF}_{z,z_{\text{test}}} = -\nabla_{\theta}L(z_{\text{test}}, \theta)^T H_{\theta}^{-1} \nabla_{\theta}L(z,\theta),
\end{equation}
where $L$ is the loss function of the ML model and $H$ is the hessian of the ML model w.r.t.~parameters $\theta$. In short, a data point is deemed more "influential" in reducing the model loss on a test data point if it has a higher IF value. As such, IF values have also become a popular method in selecting data points which are more helpful in training an ML model.

In our work, we segregated a validation dataset from each data domain's dataset, in which we use to derive the IF value of every training data point in that domain w.r.t. the validation dataset (after training a ML model over the training data). Then, we normalize these IF values (for data points in each data domain), allowing us to perform weighted random sampling at every BO iteration of our algorithm, obtaining a data subset of size $n$ for a given data domain. This IF-weighted sampling is repeated for every data domain until we sample a dataset fulfilling the proposed mixing ratio at every BO iteration. Hence, the resulting data mixture contains more proportion of high-quality data points (based on IF values). A summary of the IF-weighted sampling process for one data domain is given in Alg.~\ref{alg:IF-weighted sampling}. In our algorithm, we repeat this procedure for every data domain.

\begin{algorithm}[h]
   \caption{IF-weighted sampling for \textbf{one data domain containing} dataset $D$}
   \label{alg:IF-weighted sampling}
\begin{algorithmic}[1]
   \STATE {\bfseries Input:} number of data points $n$ required for the given data domain (taken from the mixing ratio proposed at current iteration). Dataset $D = \{x_1,x_2,...,x_{|D|}\}$, Influence value of each data point in data domain dataset $D$: $\mathcal{I} \triangleq [I_1,I_2,\dots,I_{|D|}]$, small constant $\epsilon$ to avoid degenerate-case normalization.
   
   \STATE Normalize the IF values into probabilities: $\mathcal{I}_{\text{normalized}} \triangleq [\frac{I_1 + \min{(\mathcal{I})} + \epsilon}{\sum \mathcal{I}}, \frac{I_2 + \min{(\mathcal{I})} + \epsilon}{\sum \mathcal{I}}, \dots, \frac{I_{|D|} + \min{(\mathcal{I})} + \epsilon}{\sum \mathcal{I}}]$

   \STATE Perform weighed sampling from dataset $D$ according to weights given by $\mathcal{I}_{\text{normalized}}$ $n$ times.
   
   

\end{algorithmic}
\end{algorithm}

\textbf{Precomputing IF values}. In addition, we just need to precompute the IF values of every data point once before reusing them repeatedly at every BO iteration. This greatly improves our algorithm's efficiency and runtime, as compared to other methods (see next section).


\subsection{Using other data selection methods to solve inner optimization problem} \label{app:other-data-selection}
Data selection methods \cite{albalak2024surveydataselectionlanguage, guo2024losslessdatasetdistillationdifficultyaligned} have been used to retrieve a representative subset of data from larger datasets. We note that in our work different data selection methods can be interchanged to produce different estimators for the inner problem solution in line 4 and 5 of Algorithm \ref{alg:ABOLLO}. For example, instead of using the IF-driven estimator which performs weighted sampling based on each data point's IF values, one could simply remove data points from each data domain whose IF value falls below a certain threshold because they have a higher chance of being low-quality \citep{koh2017influence}. However, our experiments (Sec.~\ref{subsec:main-results}) have shown that this (labeled as DUET-RH in Fig.~\ref{experiment:main-images} \& \ref{experiment:main-llm}) does not work as well as the IF-driven estimator.

Other data selection methods can be considered as well. For example, coresets \cite{zhang2024speculativecoresetselectiontaskspecific} have been used to distill a larger dataset into a smaller one while retaining some data properties (e.g., training loss gradients, the final performance of the trained ML model). This can also be used as an estimator of the inner problem solution: when a mixing ratio $r$ is proposed by BO, we can derive the number of data points needed for each data domain (e.g., $n$ number of data points for data domain $A$ etc.). Then, we can retrieve a size $n$ coreset from data domain $A$ (and also do this similarly for the other data domains), before combining each coreset into the final dataset used to train the ML model. Despite being conceptually simple, coresets typically require much more computational resources because we need to account for the interaction between every data point. Furthermore, we need to recompute the coreset for every BO iteration because it  depends on the mixing ratio, which changes every iteration. In contrast, IF values do not depend on the mixing ratio and can be precomputed and stored beforehand.

Lastly, uncertainty or diversity-driven \cite{wang2024diversitymeasurementsubsetselection} data selection methods can be used to select subsets of data that satisfy the proposed mixing ratio at every BO iteration. However, they also demand large amount of computational resources and require recomputation at every iteration. In App.~\ref{app:additional-results-log-det}, we provided additional experimental results using the log-determinant \cite{wang2024diversitymeasurementsubsetselection, log_det_fast_compute}, which captures the diversity of a sampled dataset, as a method to select data mixtures when estimating the solution of the inner problem. However, our results show that such methods do not work better than IF in DUET and are computationally expensive, making them unsuitable for our problem setting.

\subsection{Empirical distributions of estimators from different data selection methods}\label{app:empirical-sampling}

\begin{figure} [h]
    \centering
  \subfloat[\textbf{Empirical sampling distribution of }\label{fig-app:sampling_dist_plot}]{%
       \includegraphics[width=0.37\linewidth]{pictures/sampling_dist_new_color.png}}
  \subfloat[\textbf{Empirical estimator distribution}\label{fig-app:estimator_dist_plot}]{%
        \includegraphics[width=0.36\linewidth]{pictures/estimator_dist_new_color.png}}
    
  \caption{(a): Empirical distribution of evaluation task accuracy $\Leval(\theta_{\Xs})$ from each data mixture sample $\Xs$ (b): empirical distribution of the estimators introduced in Sec.~\ref{sec: solve inner}. The \textcolor{teal}{green histogram} is our method of performing IF-weighted sampling to obtain data mixtures. The \textcolor{gray}{gray histogram} is simply randomly sampling data mixtures with no data selection methods. The \textcolor{purple}{purple histogram} is the method of removing 20\% of the data points with the lowest IF values.} 
  \label{fig:empirical-distribution}
\end{figure}

We have introduced the IF-driven estimator in Sec.~\ref{sec: solve inner} as a method for us to estimate the solution of the inner problem. The IF-driven estimator performs IF-weighted sampling on data points from each data domain to produce data mixture samples (Eq.~\ref{eq:IF-sample-estimator}) constrained to a data mixing ratio $r$. Each data mixture sample is then used to train an ML model before obtaining a feedback on how well it has performed on the unseen evaluation task. Hence, this feedback on each data mixture sample is also a sampling distribution that we can empirically observe. Fig.~\ref{fig-app:sampling_dist_plot} shows the sampling distribution of the evaluation task performance obtained from each data mixture. Empirically, we see that the negative of this distribution is similar to a truncate exponential distribution mentioned in Theorem \ref{Theorem:inner error distribution} (We consider the negative of this random variable because our paper considers the evaluation task loss, but empirically we consider maximizing the evaluation task accuracy instead). In addition, the truncated exponential distribution is appropriate because it implies the unseen evaluation task loss is upper bounded at $y_r^* + c$ for a non-negative constant $c$; this is reasonable for many real-world settings (e.g., user rating is bounded).

Next, we plot the empirical distribution of the IF-driven estimator introduced in Eq.~\ref{eq:IF-sample-estimator} in Fig.~\ref{fig-app:estimator_dist_plot}. The distribution coincides with the estimator's distribution (formally, $y^*_r + \epsilon$) introduced in Theorem \ref{Theorem:inner error distribution}. From the estimator's distribution, we see that the IF-driven estimator (\textcolor{teal}{green histogram}) has the lower bias and variance as compared to other estimators.

% \subsection{Alternative estimator of $y_{\bm{r}}$}\label{appendix-alternative}
% In Sec.~\ref{sec: solve inner}, we introduced an estimator for the minimum $y_{\bm{r}}$ of inner problem \ref{inner opt} via $\tilde{y} = \min \{L(\theta_1),L(\theta_2),\dots,L(\theta_k)\}$. This estimator is positively biased but it is effectiveness even without knowledge of the sampling distribution. Here, we introduce an alternative \textit{unbiased} estimator for $y_{\bm{r}}$ assuming the underlying sampling distribution is a uniform distribution with \textit{unknown} width. Like previous works on estimating distribution extrema \cite{german_tank}, such alternative estimators will produce an estimate slightly smaller than the observed minimum.

% \begin{corollary}
%     Let $L(\theta_1),L(\theta_2),\dots,L(\theta_k)$ be $k$ samples drawn randomly from the set $S'_{\bm{r}} = \{L(\theta)\mid r(\theta)=\bm{r} \}$. Let $y_{\textrm{min}}$ be the minimum of these samples and $y_{\textrm{max}}$ be the maximum of these samples. If sample $L(\theta_i) \sim U(y_{\bm{r}}, y_{\bm{r}} + c)$ for $i=1,2,\dots,k$, then $\tilde{y} = y_{\textrm{min}} - \frac{c}{k+1}$ is an \textit{unbiased estimator} of $y_{\bm{r}}$. Furthermore, if $c$ is unknown, it can be estimated as $\hat{c} = \frac{k+1}{k-1}(y_{\textrm{max}}-y_{\textrm{min}})$ and $y_{\textrm{min}} - \frac{\hat{c}}{k+1}$ is still an unbiased estimator of $y_{\bm{r}}$.
% \end{corollary}

% \begin{proof}
% Since sample $L(\theta_i) \sim U(y_{\bm{r}}, y_{\bm{r}} + c)$ for $i=1,2,\dots,k$, we have shown in Theorem~\ref{Theorem:inner error distribution} that $y_{\textrm{min}} \sim y_{\bm{r}} + c \textrm{B}(1,k)$. Hence, we can see that

% \begin{equation}
% \label{equation_alternative}
% \begin{split}
%     \mathbb{E}\left(y_{\textrm{min}} -  \frac{c}{k+1}\right) & = y_{\bm{r}} + c \mathbb{E}(\textrm{B}(1,k)) - \frac{c}{k+1} \\
%     & = y_{\bm{r}} + c \left(\frac{1}{k+1}\right) - \frac{c}{k+1} \\
%     & = y_{\bm{r}}
% \end{split}
% \end{equation}
% where we have used the fact that $\mathbb{E}(\textrm{B}(1,k))=\frac{1}{1+k}$. Hence, $\tilde{y} =y_{\textrm{min}} -  \frac{c}{k+1}$ is an unbiased estimator of $y_{\bm{r}}$.
% \\ \\
% Next, it is sufficient to prove that $\hat{c}$ is an unbiased estimator of $c$. First, notice that $\mathbb{E}(y_{\textrm{max}}) = y_{\bm{r}} + c \mathbb{E}(\textrm{B}(k,1)) = y_{\bm{r}} + \frac{kc}{k+1}$. Then, we have that

% \begin{equation}
%     \begin{split}
%         \mathbb{E}(\hat{c}) & = \frac{k+1}{k-1}(\mathbb{E}(y_{\textrm{max}})-\mathbb{E}(y_{\textrm{min}})) \\
%         & = \frac{k+1}{k-1}(y_{\bm{r}} + \frac{kc}{k+1} - y_{\bm{r}} - \frac{c}{k+1})\\
%         & = c.
%     \end{split}
% \end{equation}
% Hence, $\tilde{c}$ is an unbiased estimator of $c$.
% By linearity of expectation and~\eqref{equation_alternative}, it follows that  $\mathbb{E}(y_{\textrm{min}} - \frac{\hat{c}}{k+1})$ is also an unbiased estimator of $y_{\bm{r}}$. 
% \end{proof}

% Although this estimator is unbiased, it requires us to know the underlying distribution (in this case, a uniform distribution). For general distribution families, an unbiased estimator may not have a closed form solution and is difficult to construct (in fact, these estimators may result in nonsensical minimum loss estimates such as negative accuracies in our problem setting). Last but not least, such alternative estimators of $y_{\bm{r}}$ are not that useful because they do not recover the system parameters $\theta^*$ which practitioners can actually use for their systems. For example, for our alternative estimator we do not know which system parameters actually correspond to our estimate $\tilde{y} = y_{\textrm{min}} - \frac{c}{k+1}$. Eventually, we can only use the sample which had the smallest observed system loss to retrieve usable system parameters (which is actually just the 1st order statistic estimator used in A-BAD-BO). Hence, we argue that relying on the simple 1st order statistic estimator in Theorem~\ref{Theorem:inner error distribution} as an estimator is a more practical choice.

% \vspace{0mm}\subsection{Using Gaussian process to model the outer problem} \label{app:gp model}
% In Sec.~\ref{sec: solve outer} we have modeled the outer problem $\min_{\bm{r}}f(\bm{r})$ as a realization of a Gaussian Process (GP). While GPs governed by SE kernel have already been shown to work effectively even if the true underlying function is not continuous \cite{bo-non-smooth-terrain}, we can show that $f$ is in fact differentiable under some assumptions, hence satisfying some smoothness properties. This provides a strong motivation for us to use GP to model our underlying function $f$. In general, it is difficult to capture the smoothness of $f$ over the entire local loss space without any prior assumptions, but we refer interested readers to the topic of \textit{Parametric Optimization} \cite{still2018lectures} for more information.

% \begin{proposition}
%     Let $\bm{r} \in \mathbb{R}^n$ and $\theta \in \mathbb{R}^d$. Also, assume $r(\theta): \mathbb{R}^d \mapsto \mathbb{R}^n$ is a (element-wise) differentiable and biijective function vector and $L(\theta):\mathbb{R}^n \mapsto \mathbb{R}$ is differentiable. Then $\displaystyle f(\bm{r}) = \min_{r(\theta)=\bm{r}}L(\theta)$ is also differentiable w.r.t.~$\bm{r}$.
% \end{proposition}
% \begin{proof} For simplicity, we will prove the proposition by assuming $n=d=1$. We aim to show that the limit \begin{equation*}
%     \lim_{h \rightarrow 0} \frac{f(\bm{r} + h) - f(\bm{r})}{h}
% \end{equation*}
% exists for any $\bm{r}$. To do so, let $L(\theta_{\bm{r}}^*)=f(\bm{r})$ for a given $\bm{r}$. This also implies that $r(\theta_{\bm{r}}^*)=\bm{r}$. Since $r(\theta)$ is biijective and differentiable, it implies that for any $h \in \mathbb{R}$, there exists $\delta \in \mathbb{R}$ (independent of $h$) such that
% \begin{equation}
%     r(\theta_{\bm{r}}^* + \delta h)=\bm{r} + h.
% \end{equation}
% Thus, we have that \begin{equation}
% \label{eq:limit}
%     \begin{aligned}
%             \lim_{h \rightarrow 0} \frac{f(\bm{r} + h) - f(\bm{r})}{h} &=      \lim_{h \rightarrow 0} \frac{L(\theta_{\bm{r}}^* + \delta h) - L(\theta_{\bm{r}}^*)}{h} 
%     \end{aligned}
% \end{equation}
% Since $L$ is differentiable, limit~\eqref{eq:limit} exists and we have shown that $f(\bm{r})$ is differentiable.
% \end{proof}
\section{Proofs}

\vspace{0mm}\subsection{Proof of Theorem~\ref{thm:bilevel}}\label{app:reparameterization}
\reparameterization*
\begin{proof}
Theorem~\ref{thm:bilevel} can be proven in two steps. First, we restate the theoretical results from \citep{chen2024towardsautoai} in Lemma \ref{lemma:reparameterize}. This Lemma reparameterizes any optimization problem $\min_x f(x)$ (while retaining the solution set \textit{exactly}) under some regular assumptions:
\begin{restatable}{lemma}{bilevel}
\label{lemma:reparameterize}
    Let $x \in \mathbb{R}^d$ and $y \in \mathbb{R}^n$. Also, consider well-defined functions $f$ over $\mathbb{R}^d \xrightarrow{} \mathbb{R}$ and $g$ over $\mathbb{R}^d \xrightarrow{} \mathbb{R}^n$. Then $x^*$ is a solution of $\argmin_x f(x)$ if and only if $y^* = g(x^*)$ is a solution of the second optimization problem over domain $\{y \mid \exists x, g(x)=y\}:$
    \begin{equation*}
    \begin{aligned}
    \min_{y} \min_{x} \quad & f(x)  \\
    \textrm{s.t.} \quad & g(x) = y    \\
    \end{aligned}
    \end{equation*}
\end{restatable}

% \begin{proof}
% We prove Lemma \ref{lemma:reparameterize} by showing that $\iff$ (if and only if implication) holds true w.r.t.~both statements. To begin, let $x,y \in \mathbb{R}^n$. Let $f$ and $g$ be well-defined functions over $ \mathbb{R}^n \xrightarrow{} \mathbb{R}$ and  $ \mathbb{R}^n \xrightarrow{} \mathbb{R}^n$ respectively (the term "well-defined" here implies that $\forall a \in \mathbb{R}^n$, $f(a)$ and $g(a)$ both produce unique function values).

% $(\Rightarrow)$ If $x^*$ is the solution of $\min_{x}f(x)$, then because $g$ is well-defined, there exists a corresponding $g(x^*) \in \mathbb{R}^n$. We want to show that $y^*=g(x^*)$ is the solution to the following reparameterized optimization problem:
% \begin{equation*}
% \begin{aligned}
% \min_{y} \min_{x} \quad & f(x)  \\
% \textrm{s.t.} \quad & g(x) = y,    \\
% \end{aligned}
% \end{equation*}
% To show this, consider any $y' \neq g(x^*)$ such that $S_{y'} = \{x';g(x')=y'\} \neq \varnothing$. It is clear that $\forall x' \in S_{y'}, f(x') \geq f(x^*)$ because $x^*$ is the global minimizer of $f$. Thus, $g(x^*)$ must be a solution for the reparameterized optimization problem.

% $(\Leftarrow)$ Conversely, if $y^*$ is the solution for the reparameterized optimization problem. Then consider the set $S_{y^*} = \{x;g(x)=y^*\}$ (This set is non-empty by definition, since $y^*$ is the solution and hence there exists $x$ such that $g(x)=y^*$). Let $x^* = \argmin_{x \in S_{y^*}} f(x)$. 
% % Then $\forall x' \neq x^*$, there exists $y'$ such that  $g(x')=y'$.
% By definition that $y^*$ is the solution of the reparameterized problem,
% $\min_{x \in S_{y^*}} f(x) \leq \min_{x \in S_{y'}} f(x)$ for all $y' \neq y^*$. Therefore, $x^*$ must be a minimizer of $f$ and a solution of $\min_{x}f(x)$.
% \end{proof}
The proof of Lemma \ref{lemma:reparameterize} can be found in Lemma C.1 of \citep{chen2024towardsautoai}.
Next, we show that the objective function of problem ~\ref{eq:reparameterized} introduced in our optimization problem satisfies these assumptions, allowing us to apply the Lemma \ref{lemma:reparameterize} directly.

In our setting, we set $x \triangleq \Xs$, $f(x) \triangleq \Leval(\theta_{\Xs})$ and $g(x) \triangleq \textrm{ratio}(\Xs)$. We can see that both functions are well-defined, where for any chosen input $\Xs$, there certainly exists an observed evaluation task loss $\Leval(\theta_{\Xs})$ and mixing ratio $\textrm{ratio}(\Xs)$. Lastly, by setting $y \triangleq r$, our optimization problem in problem (\ref{eq:reparameterized}) is of the identical form of the optimization problem shown in Lemma~\ref{lemma:reparameterize}. Therefore, our reparameterization process is valid.

\end{proof}

\vspace{0mm}\subsection{Proof of Theorem~\ref{Theorem:inner error distribution}}
\label{app:proof-inner-error-distribution}

\innererror*

\begin{proof}
Let $X_1,X_2,\dots,X_k$ be $k$ samples randomly drawn from a sampling distribution and $X_{\textrm{min}}=\min\{X_1,X_2,\dots,X_k\}$. This scenario mirrors the setting in Theorem~\ref{Theorem:inner error distribution}. Our goal is to derive the distribution of $X_{\textrm{min}}$ and show that it is exactly the same as the distribution of $\widetilde{y_r^*}$ shown in the Theorem~\ref{Theorem:inner error distribution}.

% \textbf{(a)} If $X_i \sim U(y_{\bm{r}}, y_{\bm{r}} + c)$, then the \emph{cumulative density function} (CDF) of $X_{\textrm{min}}$ is
% \begin{equation*}
% \begin{split}
% \displaystyle \textrm{cdf}_{(X_{\textrm{min}})}(u) & = 1-\mathbb{P}(X_{\textrm{min}} \geq u)\\
% & = 1 - \mathbb{P}(X_1 \geq u, X_2 \geq u, \dots, X_k \geq u) \\
%  & = 1 - \left(1-\frac{u-y_{\bm{r}}}{c}\right)^k, \quad y_{\bm{r}} \leq u \leq y_{\bm{r}}+c.
% \end{split}
% \end{equation*}
% and the \emph{probability density function} (PDF) can be computed as
% \begin{equation*}
% \begin{split}
% \displaystyle \textrm{pdf}_{(X_{\textrm{min}})}(u) & = \frac{\partial}{\partial u} \left(1 - \left(1-\frac{u-y_{\bm{r}}}{c}\right)^k \right) \\
%  & = \frac{k}{c} \left(1-\frac{u-y_{\bm{r}}}{c}\right)^{k-1}
% \end{split}
% \end{equation*}
% which is exactly equals to the PDF of a random variable $y_{\bm{r}} + c\epsilon'$ with $\epsilon'\sim\textrm{B}(1,k)$.

If each random sample $X_i \sim \exp_t(\lambda, c)$, we first use a commonly known result \cite{chen2024towardsautoai} that the CDF of any truncated distribution on $[0,c]$ is $\frac{F(u)-F(0)}{F(c)-F(0)}$  where $F$ is the CDF of the original distribution. Also, we note that for the untruncated exponential distribution, $F(u)=1-e^{-\lambda u}$. Hence, The CDF of $X_{\textrm{min}}$ is

\begin{equation*}
\begin{split}
\displaystyle \textrm{cdf}_{(X_{\textrm{min}})}(u) & = 1-\mathbb{P}(X_{\textrm{min}} \geq u)\\
& = 1 - \mathbb{P}(X_1 \geq u, X_2 \geq u, \dots, X_k \geq u) \\
 & = 1 - \left(1-\frac{1-e^{-\lambda u}}{1-e^{-\lambda c}}\right)^k, \quad  0 \leq u \leq c.
\end{split}
\end{equation*}
and so the PDF of $X_{\textrm{min}}$ can be computed as
\begin{equation*}
\begin{split}
\displaystyle \textrm{pdf}_{(X_{\textrm{min}})}(u) & = \frac{\partial}{\partial u} F_{(X_{\textrm{min}})}(u) \\
 & = \frac{\lambda ke^{-\lambda u}}{1-e^{-\lambda c}} \left(\frac{e^{-\lambda u} - e^{-\lambda c}}{1-e^{-\lambda c }}\right)^{k-1}, \quad 0 \leq u \leq c. 
\end{split}
\end{equation*}

In the original theorem, each sample $X_i$ follows the shifted truncated exponential distribution $y_r^* + \text{exp}_t(\lambda,c)$ where $y_r^*$ is a constant. Hence, we can see that our estimator has the distribution of $y_r^* + X_{\text{min}}$ where $X_{\text{min}}$ has the PDF above. Hence, the Theorem is proven by setting the random variable $\epsilon = X_{\text{min}}$.

\end{proof}

\subsection{Proof of Theorem~\ref{regret-abollo}}\label{proof-regret}
\regret*
\begin{proof}
We provide the proof of the sub-linear $\tilde{R}_T$ growth of DUET in Theorem~\ref{regret-abollo} by establishing upper bounds of $|\mu_t(x)-f(x)|$ and $\epsilon_t$ separately at each BO iteration $t$ and use the independence rule to bound their sum. To do so, we introduce the following two Lemmas.

Our first Lemma is taken from from known literature on Kernelized Bandits \cite{bo-kernelized-bandits} and provides the upper bound on difference between $f(x_t)$ and $\mu_t(x)$ at each BO iteration $t$.

\begin{lemma}
\label{lemma:concentration}
    Let $||f||_{\kappa}=\sqrt{	\langle f,f \rangle_\kappa} \leq B$. Also, assume that the observation noise associated with each BO iteration is $R$-sub-Gaussian with $R>0$. Then with probability at least $1-\delta$, the following holds for BO iteration $t \leq T$:
    \begin{equation}
        |\mu_t(x)-f(x)| \leq \left( B + R \sqrt{2(\gamma_t + 1 + \ln(1/\delta)}\right)\sigma_t(x)
    \end{equation}
    where $\gamma_{t}$ is the maximum information gain after $t$ observations and $\mu_t(x), \sigma_t^2(x)$ are mean and variance of posteror distribution of GP defined in Equation \ref{gp:posterior}, with $\lambda=1+2/T$.
\end{lemma}

Our second Lemma attempts to bound the expectation and variance of $\epsilon_t$, the non-negative observation noise (in our case, it corresponds to the estimation error involved in solving the inner problem) at each BO iteration $t$. These expectation and variance will be used later to bound our cumulative regret.

\begin{lemma}
\label{lemma:noise expectation variance}
    Let each observation noise $\epsilon_t$ of BO iteration $t$ follow the same probability distribution as $\epsilon$ defined in Theorem~\ref{Theorem:inner error distribution} with sampling size $k$ probability density function $f_{\epsilon_t}(u)=\frac{\lambda ke^{-\lambda u}}{1-e^{-\lambda c}} \left(\frac{e^{-\lambda u} - e^{-\lambda c}}{1-e^{-\lambda c }}\right)^{k-1}$ with $0<c\leq1$, $\lambda=1$ and $u \in [0,c]$, then $\mathbb{E}(\epsilon_t) \leq \frac{6}{k} +  \frac{2c^2((1-e^{-c})-\frac{c}{2})^{k-1}}{(1-e^{- c})^k}$ and $\Var(\epsilon_t) \leq \mathbb{E}(\epsilon_t)$.
\end{lemma}
\begin{proof}

For $\lambda=1$, we have that $f_{\epsilon_t}(u)=\frac{ ke^{- u}}{1-e^{- c}} \left(\frac{e^{- u} - e^{- c}}{1-e^{- c }}\right)^{k-1}$ with $0<c<1$ and $u \in [0,c]$. Then, the expectation:

\begin{equation}
\label{eq:expectation-bound}
    \begin{split}
        \mathbb{E}(\epsilon_t) &= \int_{0}^{c}u f_{\epsilon_t}(u)\,du
        \\ 
        &= \int_{0}^{c}  \frac{u  k e^{- u}}{1-e^{-  c}} \left(\frac{e^{-  u} - e^{-  c}}{1-e^{-  c }}\right)^{k-1} \,du
        \\
        &= \frac{k }{(1-e^{- c})^k}\int_{0}^{c}  ue^{-  u} \left(e^{-  u} - e^{-  c}\right)^{k-1} \,du
        \\
        & \stackrel{(1)}{\leq} \frac{k }{(1-e^{- c})^k}\int_{0}^{c}  u \left(e^{-  u} - e^{-  c}\right)^{k-1} \,du
        \\
        & \stackrel{(2)}{\leq} \frac{k }{(1-e^{- c})^k}\int_{0}^{c}  u \left( \left(1-\frac{u}{2} \right) - e^{-  c}\right)^{k-1} \,du
        \\
        & \stackrel{(3)}{\leq} \frac{k }{(1-e^{- c})^k} \left( 
        \frac{(u-2(1-e^{-c}))((1-e^{-c})-\frac{u}{2})^{k-1}(2(1-e^{-c}) + (k-1)u + u)}{k(k+1)} 
        \right) \Bigg| ^{u=c}_{u=0}
        \\
        & \stackrel{(4)}{=} \frac{1 }{(1-e^{- c})^k}\left( \frac{(c-2(1-e^{-c}))((1-e^{-c})-\frac{c}{2})^{k-1}(2(1-e^{-c}) + kc) + 4(1-e^{-c})^{k+1}}{k+1}\right)
        \\
        & \stackrel{(5)}{\leq} \frac{4(1-e^{-c})^{k+1}}{(k+1)(1-e^{- c})^k} +  \frac{2kc^2((1-e^{-c})-\frac{c}{2})^{k-1}}{(k+1)(1-e^{- c})^k} + \frac{2((1-e^{-c})-\frac{c}{2})^{k-1}(1-e^{-c})}{(k+1)(1-e^{- c})^k}
        \\
        & \stackrel{(6)}{\leq} \frac{6}{k} +  \frac{2c^2((1-e^{-c})-\frac{c}{2})^{k-1}}{(1-e^{- c})^k}
    \end{split}
\end{equation}

% \begin{equation}
%     \begin{split}
%         \mathbb{E}(\epsilon_t) &= \int_{0}^{c}u f_{\epsilon_t}(u)\,du
%         \\ 
%         &= \int_{0}^{c}  \frac{u \lambda k e^{-\lambda u}}{1-e^{- \lambda c}} \left(\frac{e^{- \lambda u} - e^{- \lambda c}}{1-e^{- \lambda c }}\right)^{k-1} \,du
%         \\
%         &= \frac{k \lambda}{(1-e^{-\lambda c})^k}\int_{0}^{c}  ue^{- \lambda u} \left(e^{- \lambda u} - e^{- \lambda c}\right)^{k-1} \,du
%         \\
%         & \stackrel{(1)}{\leq} \frac{k \lambda}{(1-e^{-\lambda c})^k}\int_{0}^{c}  u \left(e^{- \lambda u} - e^{- \lambda c}\right)^{k-1} \,du
%         \\
%         & \stackrel{(2)}{\leq} \frac{k \lambda}{(1-e^{-\lambda c})^k}\int_{0}^{c}  u \left( \left(1-\frac{u}{2} \right)^\lambda - e^{- \lambda c}\right)^{k-1} \,du
%         \\
%         & \stackrel{(3)}{\leq} \frac{k \lambda}{(1-e^{-\lambda c})^k}\int_{0}^{c}  u \left( 1-\frac{u}{2}  \right)^{\lambda(k-1)} \,du
%         \\
%         & \stackrel{(4)}{=} \frac{k \lambda}{(1-e^{-\lambda c})^k}\left( \frac{(c-2)(c(\lambda(k-1)+1) + 2)(1-\frac{c}{2})^{\lambda(k-1)} + 4}{(\lambda(k-1)+1)(\lambda(k-1)+2)}\right)
%         \\
%         & \stackrel{(5)}{\leq} \frac{k }{(1-e^{-\lambda c})^k}\left( \frac{4}{(k+1/\lambda -1)^2}\right)
%         \\
%         & \leq \frac{4}{(1-e^{-\lambda c})^k (2/\lambda -2+k)}
%     \end{split}
% \end{equation}

% \Bigg| ^{u=c}_{u=0}
where $\stackrel{(1)}{\leq}$ makes use of the fact that $e^{-\lambda u} \leq 1$ for $u \in [0,c]$ with $c>0$, $\stackrel{(2)}{\leq}$ uses the inequality $e^{-u} \leq 1-\frac{u}{2}$ for $u \in [0,c]$, and $c \leq 1$, $\stackrel{(3)}{=}$ uses the fact that $e^{-\lambda c} < 1$, $\stackrel{(4)}{=}$ is derived by solving the definite integral by parts and substitution and $\stackrel{(4)}{=}$ simplifies the upper bound with algebraic manipulation.

Next, the upper bound of the variance of $\epsilon_t$ can be derived by

\begin{equation}
\label{eq:variance-bound}
    \begin{split}
        \Var(\epsilon_t) &= \int_{0}^{c}u^2 f_{\epsilon_t}(u)\,du \\
        &\stackrel{(1)}{\leq} c \int_{0}^{c}u f_{\epsilon_t}(u)\,du \\
        &\stackrel{(2)}{\leq}  \int_{0}^{c}u f_{\epsilon_t}(u)\,du \\
        &= \mathbb{E}(\epsilon_t)
    \end{split}
\end{equation}
where $\stackrel{(1)}{\leq}$ makes use of the fact that $\epsilon_t$ lies in $[0,c]$ and $\stackrel{(2)}{\leq}$ makes use of the fact that $0 < c \leq 1$. This completes the proof on the bounds on $\mathbb{E}(\epsilon_t)$ and $\Var(\epsilon_t)$.
\end{proof}
Next, we observe that $x_t$ at each BO iteration $t$ is chosen via the IGP-LCB acquisition function (i.e., $x_t = \argmin_{x} \mu_{t-1}(x) - \beta_t \sigma_{t-1}(x)$ and $\beta_{t} = B + R \sqrt{2(\gamma_{t-1}+1+\ln(1/\delta_1))}$ where the observation noise associated with each BO iteration is $R$-sub Gaussian). Thus, we can see that at each iteration $t \geq 1$, we have $-\mu_{t-1}(x_t) + \beta_t \sigma_{t-1}(x_t) \geq -\mu_{t-1}(x^*) + \beta_t \sigma_{t-1}(x^*)$. It then follows that for all $t \geq 1$ and with probability at least $1-\delta_1$,
\begin{equation}
\label{eq:concentration}
    \begin{split}
        |f(x_t) - f(x^*)| &\stackrel{(1)}{\leq} f(x_t) - \mu_{t-1}(x^*) - \beta_t \sigma_{t-1}(x^*) \\
        &\stackrel{(2)}{\leq} f(x_t) - \mu_{t-1}(x_t) + \beta_t \sigma_{t-1}(x_t) \\
        &\leq \beta_t \sigma_{t-1}(x_t) + |\mu_{t-1}(x_t) - f(x_t)| \\
        &\leq 2\beta_t \sigma_{t-1}(x_t)
    \end{split}
\end{equation}

Therefore, by setting $\delta_1 = \delta_2 = \sqrt{\delta}$, it follows that with probability $1-\delta$ (this follows by rule of independence applied to the upper bound of events $\sum_{t=1}^T |f(x_t)-f(x^*)|$ and $\sum_{t=1}^T \epsilon_t$) that our \textbf{attained cumulative regret} can be bounded as
\begin{equation}
\label{eq:cumulative_regret_general}
    \begin{split}
        \tilde{R}_T &= \sum_{t=1}^T |\tilde{y}_t-f(x^*)| \\
        &= \sum_{t=1}^T |f(x_t)-f(x^*) + \epsilon_t| \\
        &\stackrel{(1)}{=} \sum_{t=1}^T |f(x_t)-f(x^*)| + \sum_{t=1}^T \epsilon_t \\
        &\stackrel{(2)}{\leq} 2\beta_T\sum_{t=1}^T \sigma_{t-1}(x_t) + \sum_{t=1}^T \epsilon_t \\
        &\stackrel{(3)}{=} 2\left(B + R \sqrt{2(\gamma_{T}+1+\ln(1/\sqrt{\delta}))}\right)\sum_{t=1}^T \sigma_{t-1}(x_t) + \sum_{t=1}^T \epsilon_t \\
        &\stackrel{(4)}{\leq}2\left(B + R \sqrt{2(\gamma_{T}+1+\ln(1/\sqrt{\delta}))}\right)\sum_{t=1}^T \sigma_{t-1}(x_t) + \sum_{t=1}^T\mathbb{E}(\epsilon_t) + \sum_{t=1}^T\sqrt{\frac{\Var(\epsilon_t)}{\delta_2}} \\
        &\stackrel{(5)}{=}2\left(B + R \sqrt{2(\gamma_{T}+1+\ln(1/\sqrt{\delta}))}\right)O(\sqrt{T\gamma_T}) + \sum_{t=1}^T\mathbb{E}(\epsilon_t) + \sum_{t=1}^T\sqrt{\frac{\Var(\epsilon_t)}{\delta_2}} \\
        &=O\left(\sqrt{T}(B\sqrt{\gamma_T}+R\gamma_T)\right) + \sum_{t=1}^T\mathbb{E}(\epsilon_t) + \sum_{t=1}^T\sqrt{\frac{\Var(\epsilon_t)}{\delta_2}} \\
        &\stackrel{(6)}{=} O\left(\sqrt{T}(B\sqrt{\gamma_T}+\frac{c^2\gamma_T}{4})\right) + \sum_{t=1}^T\mathbb{E}(\epsilon_t) + \sum_{t=1}^T\sqrt{\frac{\Var(\epsilon_t)}{\delta_2}}
    \end{split}
\end{equation}
where we have followed the attained cumulative regret proof in \cite{chen2024towardsautoai} closely and used the following facts: 
\begin{itemize}
    \item $\stackrel{(1)}{=}$ uses the fact that $\epsilon_t$ is non-negative in our problem setting (Theorem~\ref{Theorem:inner error distribution}).
    \item
    $\stackrel{(2)}{\leq}$ is derived from Eq.~\eqref{eq:concentration}.
    \item 
    $\stackrel{(3)}{=}$ uses the definition of $\beta_T$ in IGP-LCB acquisition function \cite{bo-kernelized-bandits} w.r.t.~$\delta_1 = \sqrt{\delta}$
    \item 
    $\stackrel{(4)}{\leq}$ uses Chebyshev's inequality over $\epsilon_t$ with probability at least $1-\delta_2$. 
    \item
    $\stackrel{(5)}{=}$ uses $\sum_{t=1}^T \sigma_{t-1}(x_t) \leq O(\sqrt{T \gamma_T})$ as shown in \textbf{Lemma 4} by Chowdhury \& Gopalan \yrcite{bo-kernelized-bandits}.
    \item $\stackrel{(6)}{=}$ uses the fact that $\epsilon_t$ is bounded on $[0,c]$ and all bounded random variables are R-sub-Gaussian with  $R=\frac{c^2}{4}$ \cite{subgaussian-bounded}. 
\end{itemize}
Next, we need to derive the upper bound of $\sum_{t=1}^T\mathbb{E}(\epsilon_t) + \sum_{t=1}^T\sqrt{\frac{\Var(\epsilon_t)}{\delta_2}}$ w.r.t.~$T$. This can be done by using the upper bound of the expectation and variance of $\epsilon_t$ proven in Lemma \ref{lemma:noise expectation variance}:
\begin{equation}
\label{eq:sum-expectation-variance}
    \begin{split}
         \sum_{t=1}^T\mathbb{E}(\epsilon_t) + \sum_{t=1}^T\sqrt{\frac{\Var(\epsilon_t)}
         {\delta_2}}& \stackrel{(1)}{\leq} \sum_{t=1}^T \left(\frac{6}{k} +\frac{2c^2((1-e^{-c})-\frac{c}{2})^{k-1}}{(1-e^{- c})^k} \right) + \sum_{t=1}^T \sqrt{\frac{6}{\delta_2 k} +  \frac{2c^2((1-e^{-c})-\frac{c}{2})^{k-1}}{\delta_2(1-e^{- c})^k}}
         \\
         &= \frac{6T}{k} + \frac{2Tc^2((1-e^{-c})-\frac{c}{2})^{k-1}}{(1-e^{- c})^k} + T \sqrt{\frac{6}{\delta_2 k} +  \frac{2c^2((1-e^{-c})-\frac{c}{2})^{k-1}}{\delta_2(1-e^{- c})^k}}
    \end{split}
\end{equation}
where $\stackrel{(1)}{\leq}$ uses Lemma \ref{lemma:noise expectation variance} directly.

Then, it follows from Eq.~\ref{eq:cumulative_regret_general} and \ref{eq:sum-expectation-variance} that with probability $1-\delta$ and $\delta_2=\sqrt{\delta}$, the \textbf{attained cumulative regret} $\tilde{R}_T$ at iteration $T$ is upper bounded by:

\begin{equation}
\label{eq:attained-cumulative-regret}
    \tilde{R}_T \leq O\left(\sqrt{T}(B\sqrt{\gamma_T}+\frac{c^2\gamma_T}{4})\right) + \frac{6T}{k} + \frac{2Tc^2((1-e^{-c})-\frac{c}{2})^{k-1}}{(1-e^{- c})^k} + T \sqrt{\frac{6}{\delta_2 k} +  \frac{2c^2((1-e^{-c})-\frac{c}{2})^{k-1}}{\delta_2(1-e^{- c})^k}}
\end{equation}

Finally we set $A_{c,k} = \frac{c^2(1-e^{-c}-\frac{c}{2})^{k-1}}{(1-e^{-c})^k}$. As  $T \to \infty$, with probability $1-\delta$ and $\delta_2=\sqrt{\delta}$, the attained \textit{average} regret converges to:
\begin{equation}
\label{eq:attained-average-regret}
\begin{split}
    \displaystyle \lim_{T \to \infty} \frac{\tilde{R}_T}{T} & \stackrel{(1)}{\leq} 
    \frac{6}{k} + \frac{2((1-e^{-c})-\frac{c}{2})^{k-1}}{(1-e^{- c})^k} + \sqrt{\frac{6}{\delta_2 k} +  \frac{2((1-e^{-c})-\frac{c}{2})^{k-1}}{\delta_2(1-e^{- c})^k}} \\
    & \stackrel{(2)}{\leq}
    \frac{6}{k} + \sqrt{\frac{6}{\delta_2 k}} +  2A_{c,k} + \sqrt{\frac{2A_{c,k}}{\delta_2}} \\
    & \leq \frac{6(\sqrt[4]{\delta}+\sqrt{k})}{\sqrt[4]{\delta}k} + 2A_{c,k} + \sqrt{\frac{2A_{c,k}}{\delta_2}}
\end{split}
\end{equation}
where $\stackrel{(1)}{\leq}$ divides Eq.~\ref{eq:attained-cumulative-regret} by $T$ throughout, eliminating the $O$ expression and $\stackrel{(2)}{\leq}$ uses the subsitition of $A_{c,k}$ and triangle inequality. This completes our proof for the attained average regret in Theorem \ref{regret-abollo}.
\end{proof}

% \subsection{Effect of other sampling schemes on regret bounds} \label{app-regret-bound-fixed-sampling}
% In real life settings, it may be computationally expensive to adopt a linearly increasing sampling scheme (e.g., $k=t$) for large BO iterations. In the extreme case, every system query may be computationally expensive and hence, a fixed sampling size (or a sub-linear sampling scheme) can be chosen prior to the algorithm to reduce computational cost. We provide two additional analyses with a sampling scheme of fixed $k$ and $k=\sqrt{t}$ (for each iteration $t$). Corollary \ref{cor:fixedkregret} and \ref{cor:sqrttregret} tell us that (a) with a fixed sampling size, A-BAD-BO eventually converges to a certain range of the minimum system loss and (b) with a sampling scheme of $k=\sqrt{t}$, A-BAD-BO still achieves sub-linear regret growth.
% \begin{restatable}{corollary}{fixkregret}
% \label{cor:fixedkregret}
%     Following the conditions laid out in Theorem~\ref{regret-abollo} but with fixed sampling size $k$ at each BO iteration $t=1,\dots,T$, A-BAD-BO yields with probability at least $1-\delta$:
%     \begin{itemize}
%         \item $\displaystyle \lim_{T\xrightarrow{}\infty}\frac{\tilde{R}_T}{T} \leq \frac{c}{1+k} + c\sqrt{\frac{k}{\sqrt{\delta}(k+1)^2(k+2)}}$ if $S'_{\bm{r}}$ is uniformly distributed with width $c > 0$.
%         \item $\displaystyle \lim_{T\xrightarrow{}\infty}\frac{\tilde{R}_T}{T} \leq \frac{4(1-e^{-c})}{k+1} + 2\sqrt{\frac{1-e^{-c}}{\sqrt{\delta}(k+1)}}$ if $S'_{\bm{r}}$ is exponentially distributed and truncated at $0< c \leq 1$ and rate $\lambda = 1$.
%     \end{itemize}
% \end{restatable}

% \begin{proof} First, ~\eqref{eq:cumulative_regret_general} has shown that $\tilde{R}_T \leq O\left(B\sqrt{T\gamma_T} + \sqrt{T\gamma_T(\gamma_{T}+1+\ln(1/\sqrt{\delta}))}\right) + \sum_{t=1}^T\mathbb{E}(\epsilon_t) + \sum_{t=1}^T\sqrt{\frac{\Var(\epsilon_t)}{\sqrt{\delta}}}$. Hence, $ \displaystyle \lim_{T \xrightarrow{} \infty}\frac{\tilde{R}_T}{T} = \lim_{T \xrightarrow{} \infty}\frac{1}{T} \left(\sum_{t=1}^T\mathbb{E}(\epsilon_t) + \sum_{t=1}^T\sqrt{\frac{\Var(\epsilon_t)}{\sqrt{\delta}}}\right)$ due to the sub-linear growth of $O\left(B\sqrt{T\gamma_T} + \sqrt{T\gamma_T(\gamma_{T}+1+\ln(1/\sqrt{\delta}))}\right)$. Furthermore, since sampling size $k$ is fixed, it follows from Lemma \ref{lemma:noise expectation variance} that $\epsilon_t = \epsilon_k$ for every BO iteration $t$ (i.e., the error term is identical at each iteration). Hence, we have
% \begin{equation}
% \begin{split}
%     \displaystyle \lim_{T \xrightarrow{} \infty}\frac{\tilde{R}_T}{T} &\leq \lim_{T \xrightarrow{} \infty}\frac{1}{T} \left(\sum_{t=1}^T\mathbb{E}(\epsilon_t) + \sum_{t=1}^T\sqrt{\frac{\Var(\epsilon_t)}{\sqrt{\delta}}}\right) \\
%     & = \lim_{T \xrightarrow{} \infty}\frac{1}{T} \left(T\mathbb{E}(\epsilon_k) + T\sqrt{\frac{\Var(\epsilon_k)}{\sqrt{\delta}}}\right) \\
%     &= \mathbb{E}(\epsilon_k) + \sqrt{\frac{\Var(\epsilon_k)}{\sqrt{\delta} }}.
% \end{split}
% \end{equation}
% Finally, we again use Lemma \ref{lemma:noise expectation variance} to see that if $S'_{\bm{r}}$ is uniformly distributed with width $c > 0$, then $\mathbb{E}(\epsilon_k) + \sqrt{\frac{\Var(\epsilon_k)}{\sqrt{\delta}}} = \frac{c}{1+k} + c\sqrt{\frac{k}{\sqrt{\delta}(k+1)^2(k+2)}}$. If $S'_{\bm{r}}$ is truncated exponentially distributed with $0 < c \leq 1$ and rate $\lambda=1$, $\mathbb{E}(\epsilon_k) + \sqrt{\frac{\Var(\epsilon_k)}{\sqrt{\delta}}} = \frac{4(1-e^{-c})}{k+1} + 2\sqrt{\frac{1-e^{-c}}{\sqrt{\delta}(k+1)}}$. This concludes the proof of the regret-growth of A-BAD-BO with fixed sampling size $k$.
% \end{proof}
% Therefore, if a practitioner decides to use a fixed sampling size, they would make a decision on what range they want to be within the optimal system loss and pick an appropriate $k$ which satisfies this range using Corollary~\ref{cor:fixedkregret} (e.g., for the uniform distribution case with $c=1$, we can pick $k\approx10$ to get within a 0.1 range of the minimum system loss).

% Our next Corollary shows that the attained cumulative regret of A-BAD-BO is still sub-linear w.r.t.~a sampling scheme of $k=\sqrt{t}$ at each BO iteration $t$:
% \begin{corollary}
% \label{cor:sqrttregret}
%     Following the conditions laid out in Theorem~\ref{regret-abollo} but with sub-linear sampling scheme of $k=\sqrt{t}$ at each BO iteration $t=1,\dots,T$, A-BAD-BO yields with probability at least $1-\delta$:
%     \begin{itemize}
%         \item $\Tilde{R}_T = O\left(\sqrt{T}(B\sqrt{\gamma_T}+\frac{c^2\gamma_T}{4}) + \frac{c \sqrt{T}}{\sqrt{\delta}}\right)$ if $S_{\bm{r}}'$ is uniformly distributed with width $c > 0$.
        
%         \item $\Tilde{R}_T = O\left(\sqrt{T}(B\sqrt{\gamma_T}+\frac{c^2\gamma_T}{4}) +
%         4(1-e^{-c}) (2\sqrt{T}) + \sqrt{\frac{4(1-e^{-c})}{\sqrt{\delta}}} H_T^{0.25}\right)$ if $S'_{\bm{r}}$ is exponentially distributed and truncated at $0< c \leq 1$ and rate $\lambda = 1$. $H_T^{0.25}$ refers to the generalized Harmonic number \cite{choi2011some}.
%     \end{itemize}
% \end{corollary}

% \begin{proof} We again use the same results from~\eqref{eq:cumulative_regret_general} that $\tilde{R}_T \leq O\left(B\sqrt{T\gamma_T} + \sqrt{T\gamma_T(\gamma_{T}+1+\ln(1/\sqrt{\delta}))}\right) + \sum_{t=1}^T\mathbb{E}(\epsilon_t) + \sum_{t=1}^T\sqrt{\frac{\Var(\epsilon_t)}{\sqrt{\delta}}}$ and prove the bounds for sum of the expectation and variance of $\epsilon_t$ until BO iteration $T$.

% \textbf{(A)} If $S'_{\bm{r}}$ is uniformly distributed with width $c$ and we have a sampling scheme of $k=\sqrt{t}$, then we have with probability $1-\sqrt{\delta}$ that 
% \begin{equation}
%     \begin{split}
%          \sum_{t=1}^T\mathbb{E}(\epsilon_t) + \sum_{t=1}^T\sqrt{\frac{\Var(\epsilon_t)}{\sqrt{\delta}}} &\stackrel{(1)}{=} \sum_{t=1}^T\frac{c}{1+\sqrt{t}} + \sum_{t=1}^T\sqrt{\frac{c^2 \sqrt{t}}{\sqrt{\delta} (1+\sqrt{t})^2(2+\sqrt{t})}} \\
%          &\leq
%         \sum_{t=1}^T\frac{c}{1+\sqrt{t}} + \sum_{t=1}^T\frac{c}{{\delta}^{0.25} \sqrt{t}} \\
%         &\stackrel{(2)}{\leq} 2c\sqrt{T} + \frac{2c \sqrt{T}}{{\delta}^{0.25}} \\
%         &= O\left(\frac{c\sqrt{T}}{\delta^{0.25}}\right)
%     \end{split}
% \end{equation}
% $\stackrel{(1)}{=}$  makes use of Lemma \ref{lemma:noise expectation variance} (for uniform case) directly and $\stackrel{(2)}{\leq}$ uses the fact that $\sum_{t=1}^T\frac{1}{\sqrt{t}} \leq 2\sqrt{T}$.


% \textbf{(B)} If $S'_{\bm{r}}$ is exponentially distributed and truncated at $c>0$ with rate $\lambda=1$, then with probability $1-\sqrt{\delta}$ we have
% \begin{equation}
%     \begin{split}
%          \sum_{t=1}^T\mathbb{E}(\epsilon_t) + \sum_{t=1}^T\sqrt{\frac{\Var(\epsilon_t)}
%          {\sqrt{\delta}}}&\stackrel{(1)}{\leq} \sum_{t=1}^T \frac{4(1-e^{-c})}{\sqrt{t}+1} + \sum_{t=1}^T\sqrt{\frac{4(1-e^{-c})}{\sqrt{\delta}(\sqrt{t}+1)}} \\ 
%          &\stackrel{(2)}{\leq} 4(1-e^{-c}) (2\sqrt{T}) +  \sum_{t=1}^T\sqrt{\frac{4(1-e^{-c})}{\sqrt{\delta}(\sqrt{t}+1)}}\\
%         &\stackrel{(3)}{\leq}
%         4(1-e^{-c}) (2\sqrt{T}) + \sqrt{\frac{4(1-e^{-c})}{\sqrt{\delta}}} H_T^{0.25}
%     \end{split}
% \end{equation}
% where $H_T^{0.25}$ is the \textit{generalized Harmonic number} \cite{choi2011some} and is sub-linear w.r.t.~T.  $\stackrel{(1)}{\leq}$ makes use of Lemma \ref{lemma:noise expectation variance} (for truncated exponential case) directly, $\stackrel{(2)}{\leq}$ again uses the fact that $\sum_{t=1}^T\frac{1}{\sqrt{t}} \leq 2\sqrt{T}$ and $\stackrel{(3)}{\leq}$ uses the definition of the generalized Harmonic number \cite{choi2011some}.
% \end{proof}

% Corollary~\ref{cor:sqrttregret} tells us that if we use a sampling scheme of $k=\sqrt{t}$, our attained regret bounds is higher than that for a sampling scheme of $k=t$ shown in Theorem~\ref{regret-abollo} (with a smaller sampling size, our subroutine would incur more noise, thus introducing larger \textit{observation noise} in the BO process, increasing cumulative regret growth). However, the advantage is that we may not need as much computational budget since we do not need to perform as many rounds of gradient descent (in our subroutine) per iteration. A practitioner would choose the suitable sampling scheme depending on the problem setting.

\subsection{Extending theoretical analysis based on different data selection methods}\label{app:extension-other-distributions}

Readers might be interested in how different data selection methods used to create different estimators affect our theoretical analysis. Here, we provide details on how one could replicate our paper's theoretical analysis to different estimators.

\textbf{Step 1. Establish the sampling distribution of $\Leval(\theta_{\Xs})$}. Using a particular data selection method, one obtains $k$ data mixture samples $\{\Xs_1,\dots,\Xs_k\}$ (in our paper, these samples are obtained via weighted sampling based on each data point's IF values). Then, one trains an ML model for each data mixture and obtain the evaluation task loss for each resulting ML model, yielding $\{\Leval(\theta_{\Xs_1}),\dots,\Leval{\theta_{\Xs_k}}\}$. From this set, one can empirically derive the sampling distribution of each sample $\Leval(\theta_{\Xs_i})$. In Theorem.~\ref{Theorem:inner error distribution}, we assumed that each sample $\Leval(\theta_{\Xs_i})$ follows the truncated exponential distribution. However, different data selection methods would certainly lead to different empirical sampling distributions.

\textbf{Step 2. Derive an estimator's empirical distribution}.
Next, we need to theoretically derive the 1st-order statistic \cite{order-statistics} of the empirical sampling distribution from Step 1, since we use the 1st-order statistic as our estimator. The procedure to do so is shown in App.~\ref{app:proof-inner-error-distribution} and uses a fairly standard procedure to derive the distribution of order statistics. For subsequent analysis to be tractable, the PDF of the 1st-order statistic should have a closed form (hence, a simpler sampling distribution in Step 1 is preferred). More importantly, the estimator's empirical distribution \textbf{should be R-sub-gaussian} for a fixed $R>0$. This is because for the regret-analysis proof in Eq.~\ref{eq:cumulative_regret_general} to hold true, the observation noise in the BO process should be R-sub-Gaussian. Fortunately, a large family of random distributions, including our IF-driven estimator introduced in this paper, are all R-sub-Gaussian (e.g., exponential family, all bounded random variables).

\textbf{Step 3. Derive the upper bound of estimator's expectation and variance}. Next, we derive the upper bound of the 1st-order statistic's expectation and variance
as shown in Lemma.~\ref{lemma:noise expectation variance}.


\textbf{Step 4. Derive attainable cumulative regret}. Lastly, we analyze the convergence rate of our algorithm using the growth of \textit{attained cumulative regret} \cite{chen2024towardsautoai} $\tilde{R}_T = \sum_{t=1}^T |\widetilde{y^*_{r_t}}-f(r_t)| = \sum_{t=1}^T |f(r^*) + \epsilon_t - f(r_t)|$ for $T$ BO iterations. Since the error term $\epsilon_t$ has the same expectation and variance of our estimator, we can use the results from Step 3 to derive our regret bound (as shown in Eq.~\ref{eq:cumulative_regret_general}).  




\newpage
\section{Additional Experimental Results and Discussions}
\vspace{0mm}\subsection{Additional details on experimental setup}\label{experiments_detail_extra}
In this section, we provide additional details in our experiments for ease of reproduceability. Throughout our experiments, we used the SE kernel with lengthscale parameters learnt from historical observations via maximum-likelihood \cite{gp-for-ml}. In our LCB acquisition function \cite{BO-experimental}, we set $\beta_t = 0.1$ (see Alg.~\ref{alg:ABOLLO}) throughout our experiments. Furthermore, we need to perform constrained BO \cite{boconstraints} in our experiments because the inputs to our optimization problem is a data mixing ratio $r$ whose sum of entries is constrained to 1. BoTorch allows us to implement such constraints (\url{botorch.org/docs/constraints}) easily. We used an exploration parameter of $\beta_t = 0.5$ in our BO acquisition step. For the image classification task, we purposely flip 10 percent of the training image labels to make our datasets noiser. Lastly, all evaluation for language tasks is done on \textbf{llm-harness} \cite{eval-harness} with default 0-shot settings. Hence, it is possible some of our paper's results differ from those reported in other papers (due to different prompting and inference settings). However, our paper's emphasis is on improving the ML model's performance with a few rounds refinement on the training data mixture. Hence, we expect DUET to work well even in other inference settings.

\subsection{Additional experimental results on different combination of evaluation task domains}\label{app:additional-results}
We also conducted experiments with different combinations of evaluation task domains for the image classification task. From the results, we can see DUET-IF with the IF-driven estimator (\textcolor{teal}{green plot}) consistently outperforms other baselines that use different data selection methods (introduced in Sec.~\ref{subsec:main-results}).
\begin{figure*}[h]
\centering

\subfloat[\textbf{bag, boots}]
{\includegraphics[width=0.2\textwidth]
{pictures/exp/images/in_dist/1.png}\label{fig:sub1}
}
\subfloat[\textbf{bag, dog}]
{\includegraphics[width=0.2\textwidth]
{pictures/exp/images/in_dist/2.png}\label{fig:sub1}
}
\subfloat[\textbf{bag, sandals}]
{\includegraphics[width=0.2\textwidth]
{pictures/exp/images/in_dist/3.png}\label{fig:sub1}
}
\subfloat[\textbf{bag, shirt}]
{\includegraphics[width=0.2\textwidth]
{pictures/exp/images/in_dist/4.png}\label{fig:sub1}
}
\subfloat[\textbf{bag, sneakers}]
{\includegraphics[width=0.2\textwidth]
{pictures/exp/images/in_dist/5.png}\label{fig:sub1}
}
\\
\subfloat[\textbf{boots, dogs}]
{\includegraphics[width=0.2\textwidth]
{pictures/exp/images/in_dist/6.png}\label{fig:sub1}
}
\subfloat[\textbf{boots, sandals}]
{\includegraphics[width=0.2\textwidth]
{pictures/exp/images/in_dist/7.png}\label{fig:sub1}
}
\subfloat[\textbf{boots, shirt}]
{\includegraphics[width=0.2\textwidth]
{pictures/exp/images/in_dist/8.png}\label{fig:sub1}
}
\subfloat[\textbf{boots, sneakers}]
{\includegraphics[width=0.2\textwidth]
{pictures/exp/images/in_dist/9.png}\label{fig:sub1}
}
\subfloat[\textbf{dog, sandals}]
{\includegraphics[width=0.2\textwidth]
{pictures/exp/images/in_dist/10.png}\label{fig:sub1}
}
\\
\subfloat[\textbf{dog, shirt}]
{\includegraphics[width=0.2\textwidth]
{pictures/exp/images/in_dist/11.png}\label{fig:sub1}
}
\subfloat[\textbf{dog, sneakers}]
{\includegraphics[width=0.2\textwidth]
{pictures/exp/images/in_dist/12.png}\label{fig:sub1}
}
\subfloat[\textbf{sandals, shirt}]
{\includegraphics[width=0.2\textwidth]
{pictures/exp/images/in_dist/13.png}\label{fig:sub1}
}
\subfloat[\textbf{sandal, sneakers}]
{\includegraphics[width=0.2\textwidth]
{pictures/exp/images/in_dist/14.png}\label{fig:sub1}
}
\subfloat[\textbf{shirt, sneakers}]
{\includegraphics[width=0.2\textwidth]
{pictures/exp/images/in_dist/14.png}\label{fig:sub1}
}
\vspace{0mm}
\caption{Results on different combination of image classification evaluation tasks to demonstrate the performance of DUET to refine the training data mixture as compared to other estimators and uniform weights. To reduce plot clutter, we have removed DoReMi \cite{xie2023doremi} because we found that it does not perform better than DUET across different combinations.}
\label{experiment:app-additional}
\end{figure*}

\newpage
\subsection{Additional ablation studies with diversity-driven data selection and different sampling size $k$}\label{app:additional-results-log-det}
\textbf{Comparison with diversity-driven data selection methods} While our paper introduced IF values as a data selection method to solve the inner problem (see Alg.~\ref{alg:IF-weighted sampling} in App.~\ref{app:IF-details}), other data selection methods can be used to approximately solve the inner problem (\ref{eq:inner}) as well. One class of work is diversity-driven subset selection \cite{wang2024diversitymeasurementsubsetselection} that selects a subset of data that is the most diverse and representative of the original dataset. This is done by
finding a data mixture with the largest log-determinant for its data-feature kernel. We use this method as an estimator to estimate the solution of our inner problem (\ref{eq:inner}) and compare its performance with our IF-driven estimator in Fig.~\ref{fig:app:ablation}(a), under the same out-of-domain gsm8k setting as that in Fig.~\ref{subsec:main-results}. Due to the large computational complexity of computing matrix determinants, we restricted the total number of data points to $M=1000$ (instead of $10000$ used in our main results). This large complexity arises because practical methods to calculate the determinant of a $n \times n$ matrix typically have a $\mathcal{O}(n^3)$ runtime complexity. On top of this, the greedy implementation \cite{log_det_fast_compute} to find the data mixture with highest log-determinant data features has a runtime of $\mathcal{O}(mn)$, where $m$ is the number of data points we need to retrieve at each BO iteration. Hence, this results in a runtime complexity of $\mathcal{O}(mn^4)$, which is too slow for larger datasets. Some implementation tricks (such as caching) can be used to speed up the computation of, but we still find diversity-driven data selection methods too slow to be practical in DUET. In fact, computing a single round of inner problem approximation took around 14 hours when computing the log-determinant (since we need to iterate through all data points and recompute the determinant of data feature matrix repeatedly). On the other hand, computing the IF-driven estimator only took less than 1 hour.

\textbf{Ablation study with varying sampling size $k$} Theorem \ref{Theorem:inner error distribution} \& \ref{regret-abollo} have highlighted how sampling size $k$ could theoretically affect the performance of DUET. In our main result, we showed that using a sampling size $k=1$ is sufficient for us to achieve better data mixtures than other baselines. In Fig.~\ref{fig:app:ablation}(b), we evaluated DUET with increasing number of sampling size $k$ when using the IF-estimator. Our results show that DUET with larger sampling size $k$ (\textcolor{teal}{green plot}) leads to an ML model with better performance than that with a smaller sampling size. This agrees with our theory that larger $k$ can reduce the estimation error of our estimator for the inner optimization problem (\ref{eq:inner}) and also leads to a smaller attained cumulative regret (Theorem~\ref{regret-abollo}).

\begin{figure} [h]
    \centering
  \subfloat[\textbf{DUET vs. log-det \cite{wang2024diversitymeasurementsubsetselection}}]
  {%
       \includegraphics[width=0.37\linewidth]{pictures/exp/Ablation_log_det.png}}
  \subfloat[\textbf{Ablation study w.r.t. sampling size $k$}]
  {%
        \includegraphics[width=0.36\linewidth]{pictures/exp/Ablation_k.png}}
    
  \caption{(a): Comparison of DUET paired with diversity-driven data selection methods \cite{wang2024diversitymeasurementsubsetselection} (marked as \textbf{log-det} in our plots) and DUET paired with IF-estimator (DUET-IF). (b): Ablation study of DUET-IF on sampling size $k$.} 
  \label{fig:app:ablation}
\end{figure}




% \subsection{Additional experimental results on combined effect of sampling size $k$ and BO 
% iterations}\label{experiments_results_extra}
% Given a sampling size of $k$ at each BO iteration (up till $T$ iterations), A-BAD-BO uses a total of $kT$ system queries. In real world setting, there may be a budget placed on the total number of system queries \cite{bo-practical}. Hence, we want to investigate the combined effect of chosen $k$ and $T$ on the convergence rate of A-BAD-BO. This allows us to decide for a given total number $N$ of system query budget, what $k$ and $T$ should we choose such that $kT=N$ and A-BAD-BO produces the best system performance.

% We perform A-BAD-BO w.r.t.~different sampling size $k$ and BO iterations $T$ to optimize each experimental system. We run 5 trials for each system and show the average lowest system loss achieved for each configuration of $k$ and $T$ chosen in Fig.~\ref{combined-effect}. From the heatmap, we clearly see that across all 4 experimental systems, the performance of A-BAD-BO improves w.r.t.~increasing $k$ \textit{and} $T$ (bottom right corner yields the best system performance). This agrees with our theoretical findings - higher $k$ yields lower estimation noise (which directly implies lower observation noise and better algorithm convergence) and higher BO iterations also give our algorithm more chances to explore the system loss landscape.

% Furthermore, we see that given a fixed system query budget (i.e., $kT$ equals a fixed number), it is better to choose a moderate $k$ and $T$ as compared to more extreme values. For example, in \textbf{MNIST} system, if we have a system query budget of 60, choosing $k=4$ and $T=15$ (which uses 60 system queries in total) in A-BAD-BO achieves better system loss than more extreme choices (e.g., $k=2,T=30$ or $k=30,T=2$).

% \begin{figure} [H]
%     \centering
%   \subfloat[\textbf{Synthetic}\label{synthetic combined}]{%
%        \includegraphics[width=0.3\linewidth]{pictures/experiments/synthetic_k_iterations.png}}
%     %
%   \subfloat[\textbf{MNIST}\label{mnist combined}]{%
%         \includegraphics[width=0.3\linewidth]{pictures/experiments/mnist_large_k_iterations.png}}
%     \\
%   \subfloat[\textbf{Healthcare}\label{healthcare combined}]{%
%         \includegraphics[width=0.3\linewidth]{pictures/experiments/healthcare_k_iterations.png}}
%     %
%   \subfloat[\textbf{LLM Prompt engineering}\label{llm combined}]{%
%         \includegraphics[width=0.3\linewidth]{pictures/experiments/LLM_k_iterations.png}}
%     \caption{We adjust the sampling size $k$ and number of BO iterations used in A-BAD-BO and plot the average log system loss achieved for each system. We see that increasing $k$ and number of iterations both improves the performance of A-BAD-BO. Moreover, for a fixed system query budget $kT$, it is better to choose moderate $k$ and $T$ values.}
% \end{figure}

% \clearpage
% \subsection{Additional experimental results with other acquisition functions}\label{experiments_results_acq_fn}
% Our main results in Fig.\ref{experiment:optimality} showed that A-BAD-BO achieves better system performance (e.g. loss, accuracy) across all systems with the Lower Confidence Bound (LCB) \cite{bo-gp-ucb-10} acquisition function. In this section, we also investigated whether using a different acquisition function i.e., Expected Improvement (EI) \cite{BO_original}, affected the effectiveness of our algorithm.

% As shown in Table \ref{table-acq}, EI yields similar (at times worse) results as compared to LCB. We believe this occurs because LCB encourages A-BAD-BO to explore different local loss configurations for the system, therefore producing better results. Regardless of the acquisition function, A-BAD-BO is still able to outperform other baselines, further showcasing its effectiveness.

% \begin{table}[h]
% \caption{Comparison of best system performance achieved with Lower Confidence Bound (LCB) \cite{bo-gp-ucb-10} acquisition function used in A-BAD-BO with the Expected Improvement (EI) acquisition function \cite{BO_original}. 200 system queries were used across all approaches. The best performing approach is \textbf{bolded}.}
% \label{table-acq}
% \vskip 0.15in
% \begin{center}
% \begin{small}
% \begin{sc}
% \begin{tabular}{lcccr}
% \toprule
% System & A-BAD-BO (LCB) & A-BAD-BO (EI) & Vanilla BO (EI) & TuRBO (EI) \\
% \midrule
% Synthetic (loss)  & 0.053$\pm$0.081 & \textbf{0.051$\pm$0.033} & 0.112$\pm$0.043&0.078$\pm$0.058  \\
% MNIST (loss)& \textbf{0.143$\pm$0.019} &0.158$\pm$ 0.007&0.4874$\pm$0.099&0.423$\pm$0.06\\
% Healthcare (loss)  & \textbf{3.45$\pm$0.333} & 5.35$\pm$1.14 & 7.10$\pm$0.64 & 9.09$\pm$0.44 \\
% LLM (accuracy) & \textbf{62.0$\pm$5.0} & 60.5$\pm$6.0& 51.3$\pm$3.3 & 	53.0$\pm$3.0
% \\

% \bottomrule
% \end{tabular}
% \end{sc}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table}

% \subsection{Additional experimental results with early local optimization termination}\label{experiments_results_regularizer}


% \begin{table}[h]
% \caption{Comparison of best system performance achieved by A-BAD-BO with \textbf{Early-stopping} after 200 system queries. The best performing approach is \textbf{bolded}.}
% \label{table-early-stopping}
% \vskip 0.15in
% \begin{center}
% \begin{small}
% \begin{sc}
% \begin{tabular}{lcccr}
% \toprule
% System & A-BAD-BO ($k=10$) & Early-stopping ($k=1$) & Early-stopping ($k=10$) \\
% \midrule
% Synthetic (loss)  & \textbf{0.053$\pm$0.081} & 0.185$\pm$0.090 & 0.0855 $\pm$ 0.090  \\
% MNIST (loss)& \textbf{0.143$\pm$0.019} &0.33$\pm$0.017&0.23$\pm$0.020\\
% Healthcare (loss)  & \textbf{3.45$\pm$0.333} & 8.45$\pm$2.14 & 6.55$\pm$1.88 \\
% LLM (accuracy) & \textbf{62.0$\pm$5.0} & 52.4 $\pm$2.0&55.0$\pm$1.5

% \\

% \bottomrule
% \end{tabular}
% \end{sc}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table}

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
