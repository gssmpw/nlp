\section{Proposed Method}
\label{sec:method}

In this section, we introduce our homotopy-based multi-objective framework for face parsing and its integration with both \textbf{GAN-based} and \textbf{diffusion-based} face editing models. We outline the problem formulation, dataset preparation, model architecture, training strategy, and evaluation pipeline, emphasizing \textbf{fairness}, \textbf{robustness}, and \textbf{semantic alignment}.

\subsection{Problem Formulation}
\label{subsec:problem_formulation}

We define the dataset \(\mathbf{X} = \{x_i\}\), where each face image is paired with a segmentation mask \( y_i \in \mathbf{Y} \), mapping to 19 facial components (e.g., hair, eyes, mouth). Demographic attributes are denoted as \(\mathbf{a}\) (e.g., \texttt{Male}, \texttt{Young}, \texttt{Wearing Hat}). Our objective is to train a segmentation function \( f_\theta(\cdot) \) that predicts \(\hat{y}_i\) while optimizing for accuracy, fairness, and robustness. Accuracy is maximized by aligning \(\hat{y}_i\) with \(y_i\) using Dice loss~\cite{sudre2017generalised}. Fairness is enforced by minimizing variance \(\mathrm{Var}(\mathrm{mIoU}_g)\) across demographic groups, ensuring equitable segmentation quality. Robustness is maintained by penalizing performance degradation (\(\mathrm{mIoU}\) drop) under input perturbations such as noise and occlusion.

\begin{algorithm}[h][t]
\caption{Multi-Objective Face Parsing (Pseudo-code)}
\label{alg:multi_objective_pseudocode}
\begin{algorithmic}[1]
\Require Homotopy function \(h(t)\) providing \((\alpha, \beta, \gamma)\) for epoch \(t\)
\For{epoch \(t = 1 \dots T\)}
    \State \((\alpha, \beta, \gamma) = h(t)\)
    \For{each batch in DataLoader}
        \State \textbf{Load} images \(\{x\}\), masks \(\{m\}\), attributes \(\{a\}\)
        \State outputs \(= f_{\theta}(x)\) \Comment{U-Net forward pass}
        \State \(\mathcal{L}_{\mathrm{acc}} = \mathrm{DiceLoss}(outputs, m)\)
        \State outputs\(_{\mathrm{noisy}} = outputs + \text{random\_noise}()\)
        \State \(\mathcal{L}_{\mathrm{rob}} = -\mathrm{mIoU}(\mathrm{softmax}(outputs_{\mathrm{noisy}}), m)\)
        \State \(\mathcal{L}_{\mathrm{fair}} = \mathrm{Var}\left[\mathrm{mIoU}_g\right]\)
        \State \(\mathcal{L}_{\text{total}} = \alpha\,\mathcal{L}_{\mathrm{acc}} + \beta\,\mathcal{L}_{\mathrm{rob}} + \gamma\,\mathcal{L}_{\mathrm{fair}}\)
        \State \textbf{Backward} and \textbf{update} \(\theta\)
    \EndFor
\EndFor
\end{algorithmic}
\end{algorithm}

\subsection{Dataset Preparation}
\label{subsec:dataset_preparation}

We employ the CelebAMask-HQ dataset \cite{CelebAMask-HQ}, divided into training, validation, and test sets. Each image and mask are resized to \(256 \times 256\) for compatibility with our U-Net architecture. Demographic attributes are extracted from annotations to compute fairness metrics.

\subsection{Model Architecture}
\label{subsec:model_architecture}

Our segmentation model utilizes a U-Net architecture with a ResNet-34 encoder pre-trained on ImageNet. It outputs 19 channels corresponding to distinct facial regions, balancing computational efficiency with high segmentation accuracy.

\subsection{Multi-Objective Training}
\label{subsec:multi_objective}

We train the U-Net segmentation models by optimizing a weighted sum of accuracy, fairness, and robustness losses, dynamically adjusted using homotopy-based scheduling. The training process is outlined in Algorithm~\ref{alg:multi_objective_pseudocode}.

\paragraph{Loss Components}
\begin{itemize}
    \item \textbf{Accuracy Loss (\(\mathcal{L}_{\mathrm{acc}}\)):} Dice loss measures the overlap between predicted and ground truth masks.
    \item \textbf{Robustness Loss (\(\mathcal{L}_{\mathrm{rob}}\)):} Negative \(\mathrm{mIoU}\) under perturbed predictions to ensure stability.
    \item \textbf{Fairness Loss (\(\mathcal{L}_{\mathrm{fair}}\)):} Variance of \(\mathrm{mIoU}\) across demographic groups to promote equitable performance.
\end{itemize}

\textbf{Alternative Fairness Computation:} We also compute per-group \(\mathrm{mIoU}\) for each demographic attribute, enabling detailed analysis of performance disparities (see Section~\ref{subsec:fairness_comparison}).

\subsection{Homotopy-Based Loss Scheduling}
\label{subsec:homotopy}

We dynamically balance the three loss components using epoch-dependent weights \(\alpha(t)\), \(\beta(t)\), and \(\gamma(t)\), ensuring \(\alpha(t) + \beta(t) + \gamma(t) = 1\). Initially, accuracy is prioritized, with weights shifting towards robustness and fairness over time. We explore three scheduling strategies:

\begin{itemize}
    \item \textbf{Linear:} \(\alpha(t)\) decreases linearly, while \(\beta(t)\) and \(\gamma(t)\) increase proportionally.
    \item \textbf{Sigmoid:} Smooth logistic transitions for gradual emphasis shifts.
    \item \textbf{Piecewise:} Abrupt changes in weight distribution at predefined training stages.
\end{itemize}

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/parameters_by_homotopy-crop.pdf}
    \caption{Comparison of \(\alpha\), \(\beta\), and \(\gamma\) schedules across three homotopy methods (Linear, Sigmoid, and Piecewise) over 30 epochs. Each subplot illustrates the evolution of a parameter (\(\alpha\), \(\beta\), or \(\gamma\)) as it adapts during training, highlighting the differences in transition dynamics across homotopy strategies. The legend below the figure identifies the homotopy method for each curve.}
    \label{fig:homotopy-schedules}
\end{figure}


Figure~\ref{fig:homotopy-schedules} illustrates the evolution of these weights across training epochs for each homotopy method.

\subsection{Integration with Generative Models}

\subsubsection{GAN-Based Face Editing}
\label{subsec:gan_integration}

We utilize the trained U-Nets to generate segmentation maps for the training and validation sets, which are then used to train a Pix2PixHD GAN. The GAN architecture comprises:

\begin{itemize}
    \item \textbf{Generator} \(G\): Transforms segmentation maps into RGB images.
    \item \textbf{Discriminator} \(D\): Distinguishes real images from generated ones.
\end{itemize}

The GAN training involves a combination of adversarial loss and pixel-level \(L_1\) reconstruction loss:
\[
\mathcal{L}_{\mathrm{GAN}} = \mathcal{L}_{\mathrm{adv}}(G, D) + \lambda \, \|\hat{x} - x\|_1,
\]
where \(\hat{x} = G(\text{segmentation\_map})\) and \(x\) is the real image.

During testing, the GAN generates images using segmentation maps from the test set produced by both single-objective and multi-objective U-Nets, enabling evaluation of how segmentation quality impacts generative performance.

\subsubsection{ControlNet-Based Face Editing}
\label{subsec:controlnet_integration}

In addition to GANs, we integrate \textbf{ControlNet} \cite{zhang2023adding} for diffusion-based face editing. ControlNet leverages segmentation maps to guide the diffusion process, enhancing image fidelity and semantic alignment. Our setup includes:

\begin{itemize}
    \item \textbf{ControlNet Model:} Pre-trained on Stable Diffusion, fine-tuned on our segmentation maps.
    \item \textbf{Diffusion Pipeline:} Combines ControlNet with a text encoder and U-Net backbone to generate photorealistic faces conditioned on segmentation maps.
\end{itemize}

\textbf{Training Procedure:} ControlNet is fine-tuned for a single epoch using segmentation maps from the training set. In diffusion-based experiments, we compare only the single-objective model with the multi-objective linear homotopy model to manage computational resources effectively. The training minimizes the standard denoising loss:
\[
\mathcal{L}_{\mathrm{ControlNet}} = \mathcal{L}_{\mathrm{denoise}},
\]
where \(\mathcal{L}_{\mathrm{denoise}}\) is the Mean Squared Error between predicted and actual noise. During testing, ControlNet generates images using test set segmentation maps from both U-Net models, allowing assessment of segmentation quality's effect on diffusion-based generation.

\subsection{Evaluation Metrics and Setup}
\label{subsec:evaluation}

\paragraph{Segmentation Metrics}  
We evaluate segmentation performance using the mean Intersection-over-Union (\(\mathrm{mIoU}\)) across 19 facial classes. Fairness is quantified by the variance \(\mathrm{Var}(\mathrm{mIoU}_g)\) across demographic groups, and robustness is assessed through performance under Gaussian noise, occlusions, and blur.

\paragraph{Generative Metrics}  
For GAN outputs, we evaluate image quality using \textbf{Fr√©chet Inception Distance (FID)}, which quantifies realism by comparing feature distributions between generated and real images. Additionally, \textbf{Learned Perceptual Image Patch Similarity (LPIPS)} measures perceptual similarity, where lower scores indicate greater visual resemblance to real images.

\paragraph{Implementation Details}  
All experiments are implemented in PyTorch and trained on four NVIDIA A10 GPUs using the Adam optimizer with a learning rate of \(10^{-4}\). For ControlNet, we fine-tune the pre-trained \texttt{control\_v11p\_sd15\_seg} model based on Stable Diffusion v1.5. Our pipeline supports gradient accumulation and mixed precision (FP16) for computational efficiency. Homotopy-based loss scheduling is configurable (\texttt{linear}, \texttt{sigmoid}, \texttt{piecewise}). Detailed training configurations will be released alongside our code and models to ensure reproducibility.

\paragraph{Workflow Summary}  
\begin{enumerate}
    \item \textbf{Train U-Nets:} Train single-objective and multi-objective U-Nets on the training set, validate on the validation set.
    \item \textbf{Generate Segmentation Maps:} Use trained U-Nets to produce segmentation maps for training, validation, and test sets.
    \item \textbf{Train GAN:} Train the Pix2PixHD GAN using segmentation maps from the training and validation sets.
    \item \textbf{Fine-Tune ControlNet:} Fine-tune ControlNet on training set segmentation maps for one epoch.
    \item \textbf{Generate and Evaluate Images:} Generate images using GAN and ControlNet with test set segmentation maps from both U-Net models; evaluate using FID and LPIPS.
\end{enumerate}

In the following section, we present quantitative and qualitative results demonstrating the effectiveness of our approach across various conditions and demographic groups.
