\section{Results \& Discussion}
\label{sec:results}
\begin{figure}[htbp]

        \includegraphics[width=\linewidth]{figures/perturbed.drawio.png}
        \label{fig:single_objective_blur}
    \caption{\textbf{Qualitative comparison of Single-Objective and Multi-Objective models under perturbations.}  
Blur (\(\text{severity} = 0.3\)), Gaussian Noise (\(\text{severity} = 0.1\)), and Occlusion (\(\text{severity} = 0.5\)) are applied to input images (first column). The Single-Objective model produces fragmented and inaccurate segmentations, especially in occluded and blurred regions. In contrast, the Multi-Objective model exhibits greater robustness, preserving facial structure despite degradations, with improved stability under occlusion.}
\label{fig:qualitative_results_comparison}
\end{figure}

\begin{figure*}[htb]
    \centering
    \includegraphics[width=\textwidth]{figures/fairness.drawio.pdf}
    \caption{
        \textbf{Comparison of Fairness Loss Strategies on High-Disparity Demographics.} 
        The left plot represents the fairness variance-based approach, which minimizes the variance of per-group mIoU scores, indirectly reducing fairness gaps across demographic attributes. The right plot represents the per-group mIoU fairness loss, which explicitly tracks and optimizes fairness at a finer granularity. While the variance-based approach smooths out overall disparities, the per-group fairness loss provides better control over specific demographic attributes, ensuring higher consistency across subpopulations. Multi-objective models (Linear, Sigmoid, Piecewise) tend to provide more equitable segmentation across demographics compared to the Single-Objective baseline, though certain attributes still show variability in performance.
    }
    \label{fig:fairness_comparison}
\end{figure*}


This section presents a comprehensive evaluation of our segmentation models and their impact on both \textbf{face parsing} and \textbf{generative synthesis} (GAN and diffusion-based). We compare single-objective and multi-objective training strategies across robustness, fairness, and perceptual quality.


\subsection{Segmentation Performance}
\label{subsec:segmentation_performance}

\begin{table}[ht]
    \centering
    \caption{
        \textbf{Comparison of Segmentation Objectives on U-Net.} 
        Quantitative results comparing single-objective and multi-objective training strategies (Linear, Sigmoid, and Piecewise Homotopy) based on mean Intersection over Union (mIoU) and Dice coefficient.
    }
    \label{tab:segmentation_results_all}
    \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{@{}lcc@{}}
    \toprule
    \textbf{Objective} & \textbf{mIoU (\%)} & \textbf{Dice (\%)} \\ 
    \midrule
    Single Objective          & 73.87            & 94.46  \\
    Multi-Objective (Linear)  & \textbf{74.21}   & 94.28  \\
    Multi-Objective (Sigmoid) & 73.50            & 94.35  \\
    Multi-Objective (Piecewise) & 73.80         & 94.47  \\
    Multi-Objective (Alt. Fairness) & 73.81    & \textbf{94.49}  \\ 
    \bottomrule
    \end{tabular}
    \end{adjustbox}
\end{table}


Despite dedicating training capacity to multiple competing objectives (fairness and robustness) rather than solely optimizing for accuracy, the multi-objective models achieve segmentation performance that remains on par with or even surpasses the single-objective baseline (Table \ref{tab:segmentation_results_all}). This suggests that our homotopy-based optimization effectively balances competing goals without significantly compromising segmentation accuracy, demonstrating the feasibility of integrating fairness and robustness without sacrificing core performance.

\subsection{Robustness Analysis: Performance Under Perturbations}
\label{subsec:robustness_analysis}

To evaluate the robustness of our segmentation models, we introduce perturbations including \textbf{Gaussian noise, blur, occlusion, and salt-and-pepper noise}. We then measure mIoU degradation under increasing severity.

\begin{table*}[ht]
    \centering
    \caption{
        \textbf{Robustness of U-Net Variants Under Different Perturbations (GAN Results).}
        Each cell shows FID $\downarrow$ / LPIPS $\downarrow$. Lower FID indicates more realistic outputs, whereas LPIPS reflects perceptual distance (which can imply diversity or artifacts). 
    }
    \label{tab:robustness_results}
    \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{lccccccccl}
    \toprule
    \multirow{2}{*}{\textbf{Model}} & 
    \multicolumn{2}{c}{\textbf{Gaussian Noise}} & 
    \multicolumn{2}{c}{\textbf{Blur}} &
    \multicolumn{2}{c}{\textbf{Brightness}} &
    \multicolumn{2}{c}{\textbf{Darkness}} &
    \multirow{2}{*}{\textbf{Notes}} \\ 
    \cmidrule(r){2-9}
    & \textbf{FID $\downarrow$} & \textbf{LPIPS $\downarrow$} & \textbf{FID $\downarrow$} & \textbf{LPIPS $\downarrow$} & \textbf{FID $\downarrow$} & \textbf{LPIPS $\downarrow$} & \textbf{FID $\downarrow$} & \textbf{LPIPS $\downarrow$} & \\
    \midrule

    \textbf{U-Net (Single)} &
    363.06 & 0.435 &
    259.12 & 0.403 &
    319.57 & 0.407 &
    367.75 & 0.431 &
    Baseline segmentation \\

    \textbf{U-Net (Linear + Alt. Fairness)} &
    373.83 & 0.419 &
    211.52 & 0.390 &
    298.46 & 0.384 &
    293.71 & 0.417 &
    Linear homotopy w/ alternate fairness \\

    \textbf{U-Net (Linear)} &
    322.23 & 0.434 &
    236.44 & 0.386 &
    313.02 & 0.433 &
    285.24 & 0.425 &
    Linear homotopy approach \\

    \textbf{U-Net (Sigmoid)} &
    349.30 & 0.437 &
    208.30 & 0.412 &
    286.38 & 0.444 &
    331.36 & 0.456 &
    Sigmoid multi-objective \\

    \textbf{U-Net (Piecewise)} &
    307.16 & 0.435 &
    216.98 & 0.384 &
    330.10 & 0.439 &
    326.82 & 0.430 &
    Piecewise multi-objective \\

    \bottomrule
    \end{tabular}
    \end{adjustbox}
\end{table*}

Table~\ref{tab:robustness_results} reports the robustness of each U-Net variant to common perturbations (Gaussian noise, blur, brightness increase, darkness) within the semantic-to-image GAN framework. While the single-objective baseline exhibits high FID scores across most perturbations, it maintains moderate LPIPS values, indicating that at least part of its generated diversity may stem from artifacts or less coherent facial/gestural details. In contrast, the multi-objective piecewise and linear homotopy models generally achieve lower FID scores—particularly under noise and brightness shifts—suggesting more robust and realistic outputs. Their LPIPS values remain in a comparable range, indicating that these methods retain perceptual diversity without succumbing to as many mode distortions. Interestingly, the sigmoid variant shows strong performance against blur in terms of FID (208.30) but the highest LPIPS under darkness (0.456), possibly reflecting that it generates more varied (yet not always consistently realistic) outputs under certain perturbations. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=\columnwidth]{figures/miou_perturb-crop.pdf}
    \caption{\textbf{Performance comparison of mIoU across methods and perturbation types under varying severities}. The plot illustrates the sensitivity of Single Objective and Multi-Objective methods (Linear, Sigmoid, and Piecewise) to perturbations, categorized by Gaussian noise, blur, occlusion, and salt-and-pepper noise. Each method is distinguished using different line styles and markers, while colors indicate the perturbation types.}
    \label{fig:miou_perturb_chart}
\end{figure}

Figure~\ref{fig:qualitative_results_comparison} illustrates segmentation performance under perturbations. The first column presents perturbed inputs, followed by predictions from the Single-Objective U-Net and the Multi-Objective U-Net (Linear). Under blur (top row), the Single-Objective model loses fine facial details, especially around the eyes and nose, leading to misalignment, whereas the Multi-Objective model maintains more cohesive structures. Gaussian noise (middle row) introduces artifacts and noisy edges in the Single-Objective model, while the Multi-Objective approach yields smoother and more stable segmentations. Occlusion (bottom row) severely disrupts Single-Objective predictions, often causing key facial regions to disappear, whereas the Multi-Objective model preserves identifiable structures, mitigating segmentation failures. These results confirm that Multi-Objective training improves segmentation resilience against real-world distortions.


In Figure \ref{fig:miou_perturb_chart} the Multi-Objective (Linear) method marginally outperforms Single Objective at mild and moderate severities. Under severe occlusion (0.5), both methods experience significant performance drops, but Multi-Objective retains a slight advantage. Salt and pepper noise sees marginal gains for Multi-Objective (Linear) at 0.1 severity (mIoU 0.03 vs. 0.02 for Single Objective), with all methods converging to very low mIoU ($\sim$0.00) at higher severities (0.3, 0.5). While Single Objective excels in specific mild perturbations, Multi-Objective methods, particularly the Linear variant, exhibit better robustness under moderate conditions, showcasing their adaptability to more challenging scenarios.

\subsection{Fairness Evaluation}
\label{subsec:fairness_eval}

\begin{table*}[ht]
    \centering
    \caption{
        \textbf{Class-wise Mean mIoU Comparison for U-Net Models.} 
        The table compares the segmentation performance of the single-objective and multi-objective trained U-Net models for each class in the CelebAMask-HQ dataset. Metrics are reported as mean Intersection-over-Union (mIoU) for 19 facial components. Higher values for each class are \textbf{bolded}.
    }
    \label{tab:unet_classwise_comparison}
    \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{@{}lccccccccccccccccccc@{}}
    \toprule
    \textbf{Model} &
    \textbf{0} &
    \textbf{1} &
    \textbf{2} &
    \textbf{3} &
    \textbf{4} &
    \textbf{5} &
    \textbf{6} &
    \textbf{7} &
    \textbf{8} &
    \textbf{9} &
    \textbf{10} &
    \textbf{11} &
    \textbf{12} &
    \textbf{13} &
    \textbf{14} &
    \textbf{15} &
    \textbf{16} &
    \textbf{17} &
    \textbf{18} \\ \midrule
    \textbf{Single Objective}      & 73.87 & 73.87 & 73.87 & 73.87 & 73.87 & 73.87 & 75.02 & 73.89 & 73.88 & 73.71 & 73.87 & 73.87 & 73.87 & 73.87 & 73.87 & 73.21 & 73.89 & 73.87 & 75.17 \\
    \textbf{Multi-Objective (Alt. Fairness)} & \textbf{74.21} & \textbf{74.21} & \textbf{74.21} & \textbf{74.21} & \textbf{74.21} & \textbf{74.21} & \textbf{75.06} & \textbf{74.23} & \textbf{74.22} & \textbf{74.09} & \textbf{74.21} & \textbf{74.18} & \textbf{74.21} & \textbf{74.21} & \textbf{74.21} & \textbf{73.49} & \textbf{74.23} & \textbf{74.21} & \textbf{75.24} \\ \bottomrule
    \end{tabular}
    \end{adjustbox}
    \vspace{1em}
    % \small
    % \textbf{Note:} Each column represents the class ID (0 to 18), and the values are mean mIoU (\%) for that specific class. 
\end{table*}


We measure fairness by computing performance disparities across demographic attributes. The class-wise mIoU results presented in Table~\ref{tab:unet_classwise_comparison} highlight the consistent advantages of incorporating fairness-based multi-objective training into U-Net models for face parsing. Across all 19 facial components, the multi-objective model outperforms the single-objective counterpart. Slight improvements are observed for all regions. Even for less distinctive or ambiguous classes like Class 7 (Eyebrows) and Class 15 (Accessories/Background), the multi-objective model achieves modest higher scores, with Class 15 improving from 73.21\% to 73.49\%.

\subsubsection{Comparison of Fairness Approaches}
\label{subsec:fairness_comparison}

In addition to our original fairness variance objective, we tested a refined approach that calculates per-group mIoU more explicitly (see Section~\ref{subsec:multi_objective}). Figure \ref{fig:fairness_comparison} show side-by-side comparisons of Single-Objective vs.\ Multi-Objective models across high-disparity demographic attributes. In both figures, our Multi-Objective (Linear) method generally achieves higher performance on underrepresented attributes (e.g., \texttt{Wearing\_Lipstick}, \texttt{Chubby}, \texttt{Big\_Lips}) compared to the Single-Objective baseline, though the exact mIoU values vary slightly under the new per-group logging scheme. Under this updated fairness measurement, we observe clearer demographic separations, particularly for attributes like \texttt{Eyeglasses} and \texttt{Smiling}, which highlights the subpopulations that benefit most from the Linear Homotopy weighting. Interestingly, while some categories (\texttt{Big\_Nose}, \texttt{Bags\_Under\_Eyes}) display marginally lower absolute mIoU scores, the gap between Single-Objective and Multi-Objective models becomes smaller. Finally, despite refining group-wise performance tracking, the Multi-Objective (Linear) model maintains comparable overall accuracy, indicating that this focus on fairness variance does not unduly compromise mean IoU on standard benchmarks.


\subsection{Impact on Generative Face Synthesis: GAN-Based Evaluation}
\label{subsec:gan_results}

\begin{figure}[t]
        \includegraphics[width=\linewidth]{figures/teaser_page_3.png}
\caption{\textbf{Impact of Segmentation Maps on GAN-Based Face Synthesis.}  
Segmentation maps from a \textbf{Single-Objective U-Net} and \textbf{Multi-Objective U-Nets} (Linear, Sigmoid, Piecewise) serve as inputs to a \textbf{Pix2Pix GAN}. Single-objective segmentation introduces inconsistencies, distorting facial details. In contrast, multi-objective segmentation improves structural coherence, yielding more natural and perceptually accurate face synthesis.}


    \label{fig:compare_composites}
\end{figure}

To analyze the downstream impact of segmentation quality, we use the generated segmentation maps as inputs to a Pix2PixHD GAN. Although we employed the same Pix2Pix-like GAN architecture for all experiments, the segmentation maps fed into the GAN were derived from U-Nets trained under distinct objectives. In the \emph{Single-Objective} case, which optimizes for raw segmentation accuracy alone, the parser occasionally misaligned facial contours—particularly around the eyes and mouth—producing vague or blurred regions in the final GAN-synthesized faces (Figure \ref{fig:qualitative_results_comparison}). By contrast, our \emph{Multi-Objective} U-Nets (Linear, Sigmoid, Piecewise) integrated fairness and robustness considerations, leading to segmentation maps with sharper boundaries and more consistent labeling of challenging facial attributes (e.g., hairlines or eyeglasses).

When these cleaner, more robust segmentation maps were passed to the same GAN, the generator more reliably reconstructed key features, yielding fewer artifacts and better overall realism. For instance, faces derived from the \emph{Multi-Objective (Sigmoid)} model exhibited reduced color bleed around the hair boundary, while the \emph{Piecewise} schedule often improved the jawline and cheek areas. Despite occasional local artifacts (e.g., minor tonal shifts), the multi-objective parses generally offered stronger geometric and semantic cues, enabling the GAN to produce final images that more faithfully mirrored the real reference. This underscores how upstream segmentation quality can impact downstream generative performance.

\begin{table}[ht]
    \centering
    \caption{\textbf{Comparison of Segmentation Models on GAN-Based and Diffusion-Based Face Synthesis.}}
    \label{tab:synthesis_results}
    \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{lcc}
    \toprule
    \multicolumn{3}{c}{\textbf{GAN-Based Face Synthesis}} \\
    \midrule
    \textbf{Segmentation Source} & \textbf{FID ↓} & \textbf{LPIPS ↓} \\ 
    \midrule
    Single-Objective U-Net       & 117.93  & 0.4419  \\
    Multi-Objective (Linear)      & 99.93   & 0.4269  \\
    Multi-Objective (Linear + Alt. Fairness)  & 107.92  & 0.4198  \\
    Multi-Objective (Sigmoid)     & 117.57  & 0.4378  \\
    Multi-Objective (Piecewise)   & \textbf{98.87}  & \textbf{0.4222}  \\
    \midrule
    \multicolumn{3}{c}{\textbf{Diffusion-Based Face Synthesis}} \\
    \midrule
    \textbf{Segmentation Source} & \textbf{FID ↓} & \textbf{LPIPS ↓} \\ 
    \midrule
    Single-Objective U-Net      & 261.01  & 0.7867  \\
    Multi-Objective U-Net (Linear)      & \textbf{257.18}  & \textbf{0.7848}  \\
    \bottomrule
    \end{tabular}
    \end{adjustbox}
\end{table}



% \begin{table}[ht]
%     \centering
%     \caption{
%         \textbf{Quantitative Results (Semantic-to-Image).}
%         The table summarizes the performance in terms of FID and LPIPS
%         for each segmentation method.
%     }
%     \label{tab:gan_results}
%     \begin{adjustbox}{max width=0.5\textwidth}
%     \begin{tabular}{@{}lccc@{}}
%     \toprule
%     \textbf{Segmentation Source} & \textbf{FID ($\downarrow$)} & \textbf{LPIPS ($\downarrow$)} & \textbf{Notes} \\ 
%     \midrule
%     U-Net (Baseline)                   & 117.93 & 0.4419 & Baseline segmentation \\
%     U-Net (Linear Homotopy)           & 99.93  & 0.4269 & Linear homotopy approach \\
%     U-Net (Linear + Alt. Fairness)    & 107.92 & 0.4198 & Linear w/ alternate fairness \\
%     U-Net (Sigmoid)                   & 117.57 & 0.4378 & Sigmoid approach \\
%     U-Net (Piecewise)                 & 98.87  & 0.4222 & Piecewise function approach \\
%     \bottomrule
%     \end{tabular}
%     \end{adjustbox}
% \end{table}

Table~\ref{tab:synthesis_results} reports FID and LPIPS values for our U-Net-based segmentation models applied to face and gesture synthesis. While the baseline model exhibits the highest LPIPS (0.4419), a closer inspection reveals that much of this increased perceptual distance stems from undesirable artifacts and inconsistencies in facial structure, rather than meaningful diversity. This aligns with its lower overall fidelity (FID = 117.93), suggesting that higher LPIPS in this case correlates with noisy, less coherent generations rather than enhanced variation.

In contrast, the Piecewise (FID = 98.87; LPIPS = 0.4222) and Linear Homotopy (FID = 99.93; LPIPS = 0.4269) models achieve lower LPIPS while maintaining strong realism, producing more structurally accurate and perceptually consistent outputs. These models strike a crucial balance—maintaining sufficient diversity in synthesis while avoiding excessive distortions.

\subsection{Impact on Diffusion-Based Face Synthesis}
\label{subsec:diffusion_results}

To assess the role of segmentation quality in image synthesis, we integrate our segmentation maps into \textbf{ControlNet} and evaluate their impact on diffusion-based face generation. Unlike GAN-based synthesis, diffusion models rely on iterative denoising, making them particularly sensitive to structured conditioning inputs such as segmentation maps.

We conducted an initial experiment using Stable Diffusion with ControlNet, conditioned on segmentation maps from both \textbf{Single-Objective} and \textbf{Multi-Objective} U-Nets. Due to computational constraints, training was limited to \textbf{one epoch}, making this an exploratory assessment. Results in Table~\ref{tab:synthesis_results} suggest that segmentation maps from Multi-Objective models yield slight but consistent improvements in \textbf{FID} and \textbf{LPIPS}, indicating higher perceptual realism and stability. While the performance gains are modest, they demonstrate that fairness- and robustness-aware segmentation models provide more reliable conditioning for diffusion-based face generation. As this study was limited to a single training epoch, further research is needed to refine these insights. 

% \begin{table}[ht]
%     \centering
%     \caption{\textbf{Preliminary Diffusion-Based Face Synthesis Results.} Lower FID and LPIPS values indicate better perceptual quality.}
%     \label{tab:diffusion_results}
%     \begin{adjustbox}{max width=0.48\textwidth}
%     \begin{tabular}{lcc}
%     \toprule
%     \textbf{Segmentation Model} & \textbf{FID ↓} & \textbf{LPIPS ↓} \\ 
%     \midrule
%     Single-Objective U-Net      & 261.01  & 0.7867  \\
%     Multi-Objective U-Net       & \textbf{257.18}  & \textbf{0.7848}  \\
%     \bottomrule
%     \end{tabular}
%     \end{adjustbox}
% \end{table}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{figures/teaser-Page-5.drawio.png}
    \caption{\textbf{Effect of Segmentation Quality on Diffusion-Based Synthesis After One Epoch of Fine-Tuning.}  
    The top row shows images generated using segmentation maps from the Single-Objective U-Net, while the bottom row corresponds to the Multi-Objective (Linear) U-Net. Despite being fine-tuned for just one epoch, the Multi-Objective model produces structurally coherent and visually consistent images, reducing artifacts and distortions in facial features. In contrast, the Single-Objective model exhibits irregular textures and geometric inconsistencies.}
    \label{fig:qualitative_results_comparison_diffusion}
\end{figure}

 
% \begin{table*}[ht]
%     \centering
%     \caption{
%         \textbf{Quantitative Comparison of Segmentation Models and Objectives.} 
%         The table presents results for various configurations of U-Net and DeepLabV3+, comparing single-objective and multi-objective training (linear, sigmoid, and piecewise homotopy). Metrics include mIoU, Dice, robustness score, fairness variance, and perturbed mIoU. Results from ablation studies and cross-domain generalization experiments are also included.
%     }
%     \label{tab:segmentation_results_all}
%     \begin{adjustbox}{max width=\textwidth}
%     \begin{tabular}{@{}lccccccc@{}}
%     \toprule
%     \textbf{Model} &
%     \textbf{Objective} &
%     \textbf{mIoU (\%)} &
%     \textbf{Dice (\%)} &
%     \textbf{Robustness Score (\%)} &
%     \textbf{Fairness Variance} &
%     \textbf{Perturbed mIoU (\%)} &
%     \textbf{Notes} \\ \midrule
    
%     \multicolumn{8}{c}{\textbf{U-Net}} \\ \midrule
%     U-Net (Baseline)                 & Single Objective            & 73.87            & 94.46           & --       & --       & --       & Baseline accuracy only \\
%     U-Net (Linear Homotopy)          & Multi-Objective (Linear)    & \textbf{74.21}   & 94.28           & --       & --       & --       & Balances all objectives \\
%     U-Net (Sigmoid Homotopy)         & Multi-Objective (Sigmoid)   & 73.50            & 94.35           & --       & --       & --       & Balances all objectives \\
%     U-Net (Piecewise Homotopy)       & Multi-Objective (Piecewise) & 73.80            & 94.47           & --       & --       & --       & Balances all objectives \\
%     U-Net (Alternative Fairness)     & Multi-Objective (Alt. Fairness) & 73.81       & \textbf{94.49}  & --       & --       & --       & Alternative fairness loss \\ 
%     U-Net (Ablation: Accuracy)  & Multi    & XX.XX & XX.XX & XX.XX & XX.XX & XX.XX & Removes robustness/fairness \\
%     U-Net (Ablation: Fairness)  & Multi    & XX.XX & XX.XX & XX.XX & XX.XX & XX.XX & Removes robustness/accuracy \\
%     U-Net (Ablation: Robustness)& Multi    & XX.XX & XX.XX & XX.XX & XX.XX & XX.XX & Removes fairness/accuracy \\
%     U-Net (Cross-Domain: Helen) & Multi    & XX.XX & XX.XX & XX.XX & XX.XX & XX.XX & Trained on CelebAMask-HQ \\ \midrule

%     % \multicolumn{8}{c}{\textbf{DeepLabV3+}} \\ \midrule
%     % DeepLabV3+ (Baseline)            & Single   & XX.XX & XX.XX & XX.XX & XX.XX & XX.XX & Baseline accuracy only \\
%     % DeepLabV3+ (Linear Homotopy)     & Multi    & XX.XX & XX.XX & XX.XX & XX.XX & XX.XX & Balances all objectives \\
%     % DeepLabV3+ (Sigmoid Homotopy)    & Multi    & XX.XX & XX.XX & XX.XX & XX.XX & XX.XX & Balances all objectives \\
%     % DeepLabV3+ (Piecewise Homotopy)  & Multi    & XX.XX & XX.XX & XX.XX & XX.XX & XX.XX & Balances all objectives \\
%     % DeepLabV3+ (Ablation: Accuracy)  & Multi    & XX.XX & XX.XX & XX.XX & XX.XX & XX.XX & Removes robustness/fairness \\
%     % DeepLabV3+ (Ablation: Fairness)  & Multi    & XX.XX & XX.XX & XX.XX & XX.XX & XX.XX & Removes robustness/accuracy \\
%     % DeepLabV3+ (Ablation: Robustness)& Multi    & XX.XX & XX.XX & XX.XX & XX.XX & XX.XX & Removes fairness/accuracy \\
%     % DeepLabV3+ (Cross-Domain: Helen) & Multi    & XX.XX & XX.XX & XX.XX & XX.XX & XX.XX & Trained on CelebAMask-HQ \\ \midrule

%     \end{tabular}
%     \end{adjustbox}
% \end{table*}

