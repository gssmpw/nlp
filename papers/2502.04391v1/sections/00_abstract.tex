Face parsing is a fundamental task in computer vision, enabling applications such as identity verification, facial editing, and controllable image synthesis. However, existing face parsing models often lack fairness and robustness, leading to biased segmentation across demographic groups and errors under occlusions, noise, and domain shifts. These limitations affect downstream face synthesis, where segmentation biases can degrade generative model outputs.
We propose a multi-objective learning framework that optimizes accuracy, fairness, and robustness in face parsing. Our approach introduces a homotopy-based loss function that dynamically adjusts the importance of these objectives during training. To evaluate its impact, we compare multi-objective and single-objective U-Net models in a GAN-based face synthesis pipeline (Pix2PixHD). Our results show that fairness-aware and robust segmentation improves photorealism and consistency in face generation. Additionally, we conduct preliminary experiments using ControlNet, a structured conditioning model for diffusion-based synthesis, to explore how segmentation quality influences guided image generation. Our findings demonstrate that multi-objective face parsing improves demographic consistency and robustness, leading to higher-quality GAN-based synthesis.\footnote{For anonymity during the review process, source codes and trained model weights are omitted but will be made publicly available upon publication.}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/teaser.png}  
    \caption{
        \textbf{Overview of Our Multi-Objective Face Parsing and Synthesis Framework.} 
        Our proposed \textit{homotopy-based multi-objective learning framework} optimizes \textbf{accuracy} ($L_{\text{acc}}$), \textbf{robustness} ($L_{\text{rob}}$), and \textbf{fairness} ($L_{\text{fair}}$). 
        This framework produces \textbf{fairness-aware and robust segmentation maps}, which are used to train two generative pipelines: 
        (1) a \textbf{GAN-based synthesis model (Pix2PixHD)}, where improved segmentation enhances \textit{photorealism and demographic consistency}, and 
        (2) a \textbf{diffusion-based synthesis model (ControlNet)}, where structured parsing maps guide \textit{semantic alignment and editability}. The improved segmentation quality enhances photorealism, fairness, and robustness in generative models. Key improvements include reduced bias in GAN-generated faces and more stable semantic conditioning in diffusion synthesis.}
    \label{fig:teaser}
\end{figure*}
