\section{Related Work}
\label{sec:related_work}
Automatic Program Repair (APR) has been a longstanding research area in the field of machine learning~\citep{devlin2017semcode, bhatia2016synfix, chen2019sequencer, feng-etal-2020-codebert, berabi2021tfix, codit, circle}. Most methodologies rely on supervised finetuning to adapt LLMs to the task of code generation using labeled pairs of broken / fixed code pairs, which is costly to obtain and often task- and problem-specific~\citep{Hu2022nsedit,cure,xia2022alpharepair, dinella2020hoppity}. On the other hand, unsupervised APR is challenging since it requires syntactic and semantic understanding of code, and most automatic code breaking approaches tend to be out-of distribution with real samples. \citet{yasunaga2021pmlr} train both a breaker and a fixer in order to learn to propose new code fixes that are realistic, and use a compiler to verify its correctness. Our work uses partial fixes generated by the model as the initial broken code to be fixed iteratively.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.99\linewidth]{plots/codeforces_category_distribution}
    \caption{\textbf{Category-wise analysis:} analysing the distribution of \aupairs/ across different categories and comparing it with the distribution of problems in the dataset.}
    \label{fig:category_analysis}
\end{figure}


More recently, a few unsupervised approaches have been proposed based on the capability of LLMs to generate code~\citep{chen2021evaluating,nijkamp2023codegen, chowdhery2024palmcoder, li2022alphacode, fried2023incoder, li2023starcoder}. The APR task still remains challenging, even though models are better at generating code~\citep{olausson2024self,chen2023teaching}. \citet{zhao2024repair}~use a step-by-step method to repair code using a reward model as a critic, providing feedback to finetune an LLM. \citet{shypula2024codeedits} propose a retrieval based few-shot prompting approach with Chain-of-Thought (CoT) reasoning traces, and use supervised fine-tuning (SFT) to finetune a model using self-play.

The main disadvantage of using SFT approaches comes from the need to finetune the model to the task, which becomes much more costly with ever-growing model sizes. In recent years the in-context learning (ICL) paradigm~\citep{brown2020icl} has been shown to be a flexible and compute-efficient adaptation approach to new tasks~\citep{oswald2022iclgrad, akyurek2023ICLlinear}. \citet{le2022coderl} use an LLM to generate code and a critic network to predict functional correctness of the the generated program, with zero-shot transfer to new tasks. Our work focuses on tasks in which the correctness is measured by the number of test cases the generated code passes. \citet{Gou2024critic} combine the use of LLMs with tools to provide feedback for the LLM to self-correct via additional calls to evaluate its own output in a validation setting. \citet{wang2024intervenor} also make use of external tools and use an LLM in a learner / teacher role to provide a chain of repairs to fix the code.

\citet{xin2024thinkrepair} propose an automated self-repair approach with few-shot prompting but using CoT and execution feedback information. \citet{agarwal2024manyshot} also use CoT rationales but remove them from context when few-shot-prompting the model. \citet{olausson2024self} show that using an LLM as a feedback source for self repair has its limitations when compared with the same number of independent model calls for the same problem since the ability to generated better code may be interconnected with the ability to identify its faulty behaviour. \citet{welleck2023selfcorrect} decouple the generation and correction phases by independently training a corrector with scalar and natural language feedback to correct intermediate imperfect generations. We use self-corrections, since we use the same model for generating the fixes and the broken code pairs, but the improvement is grounded on the number of passing tests, avoiding degenerate behaviours.


\citet{yuan2017arja} propose a multi-objective evolutionary algorithm to search over possible correct code patches; \citet{paredes2023funsearch} use an island-based evolutionary method to encourage exploration of diverse programs, and perform iterative best-shot-prompting to improve the quality of the generated code. We use a generative approach; closer to the work of~\citet{shirafuji2023fewshot}, we make use of ICL abilities of LLMs to generate improved code repairs, but we provide an extra submodular process to select the samples, that encourages diversity.