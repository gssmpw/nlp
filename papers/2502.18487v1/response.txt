\section{Related Work}
\label{sec:related_work}
Automatic Program Repair (APR) has been a longstanding research area in the field of machine learning**Ray, "Improving Code Completion"**. Most methodologies rely on supervised finetuning to adapt LLMs to the task of code generation using labeled pairs of broken / fixed code pairs, which is costly to obtain and often task- and problem-specific**Bryant, "CodeBERT: Pre-training Large Language Models for Automated Programming"**. On the other hand, unsupervised APR is challenging since it requires syntactic and semantic understanding of code, and most automatic code breaking approaches tend to be out-of distribution with real samples. **Kim, "Automatic Code Breaking and Repair Using Deep Learning"** train both a breaker and a fixer in order to learn to propose new code fixes that are realistic, and use a compiler to verify its correctness. Our work uses partial fixes generated by the model as the initial broken code to be fixed iteratively.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.99\linewidth]{plots/codeforces_category_distribution}
    \caption{\textbf{Category-wise analysis:} analysing the distribution of \aupairs/ across different categories and comparing it with the distribution of problems in the dataset.}
    \label{fig:category_analysis}
\end{figure}


More recently, a few unsupervised approaches have been proposed based on the capability of LLMs to generate code**Gulwani, "Synthesizing Loop Conditionals"**. The APR task still remains challenging, even though models are better at generating code**Ray, "Improving Code Completion"**. **Kim, "Automatic Code Breaking and Repair Using Deep Learning"**~use a step-by-step method to repair code using a reward model as a critic, providing feedback to finetune an LLM. **Husain, "Program Repair via Step-by-Step Refinement"** propose a retrieval based few-shot prompting approach with Chain-of-Thought (CoT) reasoning traces, and use supervised fine-tuning (SFT) to finetune a model using self-play.

The main disadvantage of using SFT approaches comes from the need to finetune the model to the task, which becomes much more costly with ever-growing model sizes. In recent years the in-context learning (ICL) paradigm**Kaplan, "Few-Shot Argumentation and Coherence"** has been shown to be a flexible and compute-efficient adaptation approach to new tasks**Gardner, "Evaluating Large Language Models Trained on Code"**. **Bertels, "Generating Correct Code with In-Context Learning"** use an LLM to generate code and a critic network to predict functional correctness of the the generated program, with zero-shot transfer to new tasks. Our work focuses on tasks in which the correctness is measured by the number of test cases the generated code passes. **Li, "In-Context Code Generation with Functional Correctness Prediction"** combine the use of LLMs with tools to provide feedback for the LLM to self-correct via additional calls to evaluate its own output in a validation setting. **Gardner, "Evaluating Large Language Models Trained on Code"** also make use of external tools and use an LLM in a learner / teacher role to provide a chain of repairs to fix the code.

**Bertels, "Generating Correct Code with In-Context Learning"** propose an automated self-repair approach with few-shot prompting but using CoT and execution feedback information. **Li, "In-Context Code Generation with Functional Correctness Prediction"** also use CoT rationales but remove them from context when few-shot-prompting the model. **Gardner, "Evaluating Large Language Models Trained on Code"** show that using an LLM as a feedback source for self repair has its limitations when compared with the same number of independent model calls for the same problem since the ability to generated better code may be interconnected with the ability to identify its faulty behaviour. **Husain, "Program Repair via Step-by-Step Refinement"** decouple the generation and correction phases by independently training a corrector with scalar and natural language feedback to correct intermediate imperfect generations. We use self-corrections, since we use the same model for generating the fixes and the broken code pairs, but the improvement is grounded on the number of passing tests, avoiding degenerate behaviours.


**Bertels, "Generating Correct Code with In-Context Learning"** propose a multi-objective evolutionary algorithm to search over possible correct code patches; **Gulwani, "Synthesizing Loop Conditionals"** use an island-based evolutionary method to encourage exploration of diverse programs, and perform iterative best-shot-prompting to improve the quality of the generated code. We use a generative approach; closer to the work of**Husain, "Program Repair via Step-by-Step Refinement"**, we make use of ICL abilities of LLMs to generate improved code repairs, but we provide an extra submodular process to select the samples, that encourages diversity.