\section{Related work}
Clustering techniques are central to natural language processing and machine learning. Hierarchical methods **Lee, "Hierarchical Document Clustering for High-Dimensional Data"** build tree-structured representations of nested document relationships. Density-based approaches like DBSCAN **Ester, Kriegel, Sander, and Xu, "A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases"** and graph-based methods detect clusters of arbitrary shapes, while spectral clustering **Shi and Malik, "Normalized Cuts and Image Segmentation"** leverages eigen-decomposition to uncover complex structures. Model-based techniques—including Gaussian mixture models **McLachlan and Peel, "Robust Cluster Analysis and Principal Component Analysis"** and recent neural network frameworks **Kingma and Ba, "Adam: A Method for Stochastic Optimization"**—provide probabilistic clustering formulations. Additionally, topic modeling methods, from probabilistic latent semantic analysis **Hofmann, "Unsupervised Text Classification Using LSI"** to latent Dirichlet allocation **Blei, Ng, and Jordan, "Latent Dirichlet Allocation"**, capture word co-occurrence patterns and latent topics. However, these approaches diverge from our objective of enhancing \texttt{k-means} with LLMs, and are therefore not directly comparable.

Recent studies have integrated LLMs into clustering pipelines to reduce expert supervision and enhance performance. For instance, **Vaswani et al., "Attention Is All You Need"** employ LLMs to augment document representations, generate pseudo pairwise constraints, and post-correct low-confidence assignments for query-efficient, few-shot semi-supervised clustering. Similarly, **Lewis et al., "Pre-training Extractive Summarizers in the absence of relevant annotated data"**, which uses instruction-tuned LLMs via interactive triplet and pairwise feedback to cost-effectively refine clustering granularity. Complementary approaches **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"** show that context-derived representations capture subtle semantic nuances beyond traditional embeddings. Additionally, **Rajpurkar et al., "SQuAD: 100,000+ Questions for Machine Learning of Short-Answers"** introduce a goal-driven, explainable clustering method that employs natural language descriptions to clarify cluster boundaries, while **Zhang et al., "Exploring Pre-training Techniques for Natural Language Processing Tasks"** present \texttt{IDAS} for intent discovery using abstractive summarization. Moreover, **Lee et al., "Adversarial Training Methods for Semi-Supervised Text Classification"** propose an iterative refinement mechanism that forms super-points to mitigate outliers and reassign ambiguous edge points, resulting in clusters with higher coherence and robustness. In contrast, our approach directly enhances the core \texttt{k-means} algorithm in an LLM-scalable manner.
\RestyleAlgo{ruled}
\begin{algorithm}[hbt!]
\caption{\texttt{k-LLMmeans}}\label{alg:kllmmeans}
\SetKwInOut{Data}{Data}
\SetKwInOut{Input}{input}
\Input{$\ D = \{d_1,\dots, d_n\}, k, I, m, l, T$}
%\tcp{Compute embeddings}
\For{$i \gets 1$ \KwTo $n$}{ 
    $\mathbf{x}_i = \text{Embedding}(d_i)$\;
}
%\tcp{Update centroids for $T$ iterations}
\For{$t \gets 1$ \KwTo $T$}{
    \If{$t = 1$}{
        \tcp{Initialize centroids using \texttt{k-means++}}
        \tcp{This step can be omitted if initial centroids are provided}
        $\{\boldsymbol{\mu}_1, \dots, \boldsymbol{\mu}_k\} \gets \texttt{k-means++}(\{d_1, \dots, d_n\}, k)$\;
    }
    \ElseIf{$t \bmod l = 0$}{
        \tcp{Update centroids with LLM every $l$ iterations}
        \For{$j \gets 1$ \KwTo $k$}{
            $m_j \gets \min(m, |C_j|)$\;
            
            %\tcp{Select $m_j$ representative documents from cluster using \texttt{k-means++}}
            $\{d_{z_1}, \dots, d_{z_{m_j}}\} \gets \texttt{k-means++}(\{d_i \mid i \in [C_j]\}, m_j)$\;
    
            %\tcp{Construct prompt for LLM-based summarization}
            $p_j \gets \text{Prompt}(I, \{d_{z_1}, \dots, d_{z_{m_j}}\})$\;
            $s_j \gets f_{\text{LLM}}(p_j)$\;
            %\tcp{Update centroid using the embedding of the LLM-generated summary}
            $\boldsymbol{\mu}_j \gets \text{Embedding}(s_j)$\;
        }
    }
    \Else{
        \tcp{Update centroids using standard averaging}
        \For{$j \gets 1$ \KwTo $k$}{
            $\boldsymbol{\mu}_j \gets \frac{1}{|C_j|} \sum_{i \in [C_j]} \mathbf{x}_i$\;
        }
    }
    %\tcp{Reset clusters}
    \For{$j \gets 1$ \KwTo $k$}{
        $C_j = \{\}$\;
    }
    %\tcp{Assign each point $\mathbf{x}_i$ to the cluster with the nearest centroid}
    \For{$i \gets 1$ \KwTo $n$}{
        $j^* \gets \arg\min_{j \in \{1, \dots, k\}} d(x_i, \boldsymbol{\mu}_j)$\;
        \tcp{Assign $x_i$ to cluster $C_{j^*}$}
        $C_{j^*} \gets C_{j^*} \cup \{x_i\}$\;
    }
}
\Return $\{\boldsymbol{\mu}_1, \dots, \boldsymbol{\mu}_k\}, \{s_1, \dots, s_k\}$
\end{algorithm}