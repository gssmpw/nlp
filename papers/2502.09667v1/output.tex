% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{natbib}
\usepackage{algorithm2e}
% \usepackage[margin=1in]{geometry}
\usepackage{multirow}
\usepackage{graphicx} % For \rotatebox
\usepackage{subcaption}
\usepackage{float}
\usepackage{bbm}
\usepackage[title]{appendix}
\usepackage{mathtools}    
% \usepackage{enumitem}
% \usepackage[square, comma, numbers]{natbib}
% \usepackage{setspace}
\usepackage{amsmath, amssymb, enumerate, amsthm}
\usepackage{mathrsfs}
\usepackage{booktabs}
\usepackage{enumerate}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{multirow}
\usepackage{makecell}


\usepackage{mathtools}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}
\usepackage{algorithm2e}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{\texttt{k-LLMmeans}: Summaries as Centroids for Interpretable and Scalable LLM-Based Text Clustering}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Jairo Diaz-Rodriguez \\
  Department of Mathematics and Statistics \\
  York University \\
  Toronto, Ontario M3J 1P3 \\
  \texttt{jdiazrod@yorku.ca}}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}
We introduce \texttt{k-LLMmeans}, a novel modification of the \texttt{k-means} clustering algorithm that utilizes LLMs to generate textual summaries as cluster centroids, thereby capturing contextual and semantic nuances often lost when relying on purely numerical means of document embeddings. This modification preserves the properties of \texttt{k-means} while offering greater interpretability: the cluster centroid is represented by an LLM-generated summary, whose embedding guides cluster assignments. 
We also propose a mini-batch variant, enabling efficient online clustering for streaming text data and providing real-time interpretability of evolving cluster centroids. Through extensive simulations, we show that our methods outperform vanilla \texttt{k-means} on multiple metrics while incurring only modest LLM usage that does not scale with dataset size. Finally, We present a case study showcasing the interpretability of evolving cluster centroids in sequential text streams. As part of our evaluation, we compile a new dataset from StackExchange, offering a benchmark for text-stream clustering. %Our results highlight that \texttt{k-LLMmeans} successfully integrates the transparency of LLM-derived summaries with the efficiency and robustness of \texttt{k-means}, providing a practical and principled enhancement to traditional text clustering methods.
\end{abstract}

\section{Introduction}
Text clustering is a fundamental task in natural language processing (NLP), widely applied in document organization, topic modeling, and information retrieval \citep{schutze2008introduction, steinbach2000comparison}. A common approach involves generating text embeddings \citep{devlin2018bert, sanh2019distilbert, mikolov2013efficient, pennington2014glove, brown2020language} for each document, which are then clustered using traditional algorithms \citep{PETUKHOVA2025100}. Among these, \texttt{k-means} \citep{macqueen1967some} is one of the most widely used, iteratively refining cluster centroids based on the mean of assigned points. Several alternative methods exist for computing centroids \citep{jain1988algorithms, bradley1996clustering, kaufman}, yet they often rely on purely numerical calculations. While effective, such approaches risk losing crucial contextual and semantic information, as averaging high-dimensional embeddings can obscure nuanced textual relationships \citep{reimers-gurevych-2019-sentence}.

In this work, we propose a modification to the \texttt{k-means} algorithm by leveraging large language models (LLMs) to dynamically generate cluster centroids. We call this the \texttt{k-LLMmeans}. Instead of always computing centroids as the mean of embedding vectors, in spaced iterations we generate centroids based on textual summaries of the cluster contents. At each of these iterations, the centroid is represented as the embedding of the LLM-generated summary of the texts belonging to the cluster. Building on \citet{jia2025dynamics}, the LLM-based centroids preserve key contextual and semantic aspects of the main documents in the cluster while filtering out secondary or lower-priority documents. This summary-based centroid provides a more interpretable and contextually relevant representation, mitigating the loss of meaning inherent in numerical averaging. 

One key advantage of our approach is the interpretability of the clustering process. Traditional \texttt{k-means} lacks explicit explanations for how clusters evolve over iterations, whereas our method provides textual summaries that offer insights into the semantic shifts of cluster centroids. This can be practically beneficial for debugging and validating the expected behavior of the algorithm. Moreover, this transparency in current status of the clustering becomes a key advantage on sequential clustering where clusters might evolve on time and interpretability becomes important. In this area we provide a modified version of the mini-batch \texttt{k-Means} \citep{sculley2010web} by utilizing our \texttt{k-LLMmeans} sequentially.

Several studies have explored unsupervised text clustering using LLMs \citep{zhang-etal-2023-clusterllm, feng2024llmedgerefine, de2023idas, viswanathan2024large, shi2023self, tarekegn2024large, nakshatri2023using}, demonstrating state-of-the-art performance across various datasets and benchmarks. These approaches consistently outperform traditional clustering algorithms such as \texttt{k-means}. However, a key limitation is their complexity and their reliance on fine-tuning or iterative querying of LLMs that scales with the dataset. While this improves clustering quality, it introduces instability, might requires extensive parameter/prompt tuning, and limits scalability for big data.

\textbf{Our approach is not designed to surpass complex state-of-the-art LLM-based text clustering methods but rather to provide a scalable and transparent LLM-enhanced modification to the well-established \texttt{k-means} algorithm, maintaining its key behavior}. Fundamentally, a centroid is a numerical abstraction that represents a cluster, and we posit that an LLM-generated textual summary can serve an analogous role. Crucially, our approach preserves most of the mathematical properties of \texttt{k-means}, ensuring that core theoretical guarantees—such as convergence behavior, cluster compactness, and complexity— remain intact. Unlike alternative strategies that introduce complex pre-processing or post-processing steps, often altering the underlying mathematical framework, our method seamlessly integrates LLMs while maintaining \texttt{k-means}' well-defined optimization landscape. This balance allows for enhanced interpretability and adaptability while preserving the efficiency that have made \texttt{k-means} a cornerstone of clustering algorithms. %By leveraging LLMs to refine centroid representations without disrupting the mathematical foundation of k-means, our approach provides a principled yet practical enhancement to text clustering.

In summary our contributions are as follows:
\begin{itemize}
    \item We introduce \texttt{k-LLMmeans} (Section~\ref{sec:kllmmeans}), that leverages LLMs to enhance centroid estimation in \texttt{k-means} for text clustering.
    \item We propose mini-batch \texttt{k-LLMmeans} (Section~\ref{sec:minibatchkllmmeans}), designed for sequential, scalable, and interpretable text-stream clustering.
    \item Through extensive simulations (Section~\ref{sec:experiments}), we demonstrate that both methods outperform \texttt{k-means} while maintaining low LLM usage that does not scale with the dataset size.
    \item We present a case study (Section~\ref{sec:case}) demonstrating the interpretability of evolving cluster centroids in sequential text streams.
\end{itemize}
As a by-product of our experiments, we compile a dataset from \citet{StackExchangeData} suitable for benchmarking text-streaming analysis methods (Section~\ref{sec:stack}).

\section{Related work}
Clustering techniques are central to natural language processing and machine learning. Hierarchical methods \citep{johnson1967hierarchical, blashfield1978literature} build tree-structured representations of nested document relationships. Density-based approaches like DBSCAN \citep{ester1996density} and graph-based methods detect clusters of arbitrary shapes, while spectral clustering \citep{ng2001spectral} leverages eigen-decomposition to uncover complex structures. Model-based techniques—including Gaussian mixture models \citep{dempster1977maximum} and recent neural network frameworks \citep{zhou2019end, huang2014deep, yang2016joint, zhang2021supporting, xie2016unsupervised}—provide probabilistic clustering formulations. Additionally, topic modeling methods, from probabilistic latent semantic analysis \citep{hofmann2001unsupervised} to latent Dirichlet allocation \citep{blei2003latent}, capture word co-occurrence patterns and latent topics. However, these approaches diverge from our objective of enhancing \texttt{k-means} with LLMs, and are therefore not directly comparable.

Recent studies have integrated LLMs into clustering pipelines to reduce expert supervision and enhance performance. For instance, \citet{viswanathan2024large} employ LLMs to augment document representations, generate pseudo pairwise constraints, and post-correct low-confidence assignments for query-efficient, few-shot semi-supervised clustering. Similarly, \citet{zhang-etal-2023-clusterllm} propose \texttt{ClusterLLM}, which uses instruction-tuned LLMs via interactive triplet and pairwise feedback to cost-effectively refine clustering granularity. Complementary approaches \citep{tipirneni2024context, PETUKHOVA2025100} show that context-derived representations capture subtle semantic nuances beyond traditional embeddings. Additionally, \citet{wang2023goal} introduce a goal-driven, explainable clustering method that employs natural language descriptions to clarify cluster boundaries, while \citet{de2023idas} present \texttt{IDAS} for intent discovery using abstractive summarization. Moreover, \citet{feng2024llmedgerefine} propose an iterative refinement mechanism that forms super-points to mitigate outliers and reassign ambiguous edge points, resulting in clusters with higher coherence and robustness. In contrast, our approach directly enhances the core \texttt{k-means} algorithm in an LLM-scalable manner.
\RestyleAlgo{ruled}
\begin{algorithm}[hbt!]
\caption{\texttt{k-LLMmeans}}\label{alg:kllmmeans}
\SetKwInOut{Data}{Data}
\SetKwInOut{Input}{input}
\Input{$\ D = \{d_1,\dots, d_n\}, k, I, m, l, T$}
%\tcp{Compute embeddings}
\For{$i \gets 1$ \KwTo $n$}{ 
    $\mathbf{x}_i = \text{Embedding}(d_i)$\;
}
%\tcp{Update centroids for $T$ iterations}
\For{$t \gets 1$ \KwTo $T$}{
    \If{$t = 1$}{
        \tcp{Initialize centroids using \texttt{k-means++}}
        \tcp{This step can be omitted if initial centroids are provided}
        $\{\boldsymbol{\mu}_1, \dots, \boldsymbol{\mu}_k\} \gets \texttt{k-means++}(\{d_1, \dots, d_n\}, k)$\;
    }
    \ElseIf{$t \bmod l = 0$}{
        \tcp{Update centroids with LLM every $l$ iterations}
        \For{$j \gets 1$ \KwTo $k$}{
            $m_j \gets \min(m, |C_j|)$\;
            
            %\tcp{Select $m_j$ representative documents from cluster using \texttt{k-means++}}
            $\{d_{z_1}, \dots, d_{z_{m_j}}\} \gets \texttt{k-means++}(\{d_i \mid i \in [C_j]\}, m_j)$\;
    
            %\tcp{Construct prompt for LLM-based summarization}
            $p_j \gets \text{Prompt}(I, \{d_{z_1}, \dots, d_{z_{m_j}}\})$\;
            $s_j \gets f_{\text{LLM}}(p_j)$\;
            %\tcp{Update centroid using the embedding of the LLM-generated summary}
            $\boldsymbol{\mu}_j \gets \text{Embedding}(s_j)$\;
        }
    }
    \Else{
        \tcp{Update centroids using standard averaging}
        \For{$j \gets 1$ \KwTo $k$}{
            $\boldsymbol{\mu}_j \gets \frac{1}{|C_j|} \sum_{i \in [C_j]} \mathbf{x}_i$\;
        }
    }
    %\tcp{Reset clusters}
    \For{$j \gets 1$ \KwTo $k$}{
        $C_j = \{\}$\;
    }
    %\tcp{Assign each point $\mathbf{x}_i$ to the cluster with the nearest centroid}
    \For{$i \gets 1$ \KwTo $n$}{
        $j^* \gets \arg\min_{j \in \{1, \dots, k\}} d(x_i, \boldsymbol{\mu}_j)$\;
        \tcp{Assign $x_i$ to cluster $C_{j^*}$}
        $C_{j^*} \gets C_{j^*} \cup \{x_i\}$\;
    }
}
\Return $\{\boldsymbol{\mu}_1, \dots, \boldsymbol{\mu}_k\}, \{s_1, \dots, s_k\}$
\end{algorithm}

\section{Preliminaries: \texttt{k-Means} for text clustering}

We can start formalizing the text clustering problem. Given a corpus of $n$ text documents $D = \{d_1,\cdots d_n\}$. Each document $d_i$ is represented as a $d$-dimensional embedding vector $\mathbf{x}_i \in \mathbb{R}^d$ such that:
$$\mathbf{x}_i  = \text{Embedding}(d_i).$$
We assume that all embeddings are normalized such that $\|\mathbf{x}_i\| = 1$, equivalently using the euclidian distance or cosine similarity as the distance metric between embeddings. The goal of K-Means clustering is to partition these $n$ document embeddings into $k$ clusters, minimizing the intra-cluster variance. Formally, we define the clustering objective as:
\begin{equation}
    \arg\min_{C_1, C_2, \dots, C_k} \sum_{j=1}^{k} \sum_{i \in [C_j]} \| \mathbf{x}_i - \boldsymbol{\mu}_j \|^2,
\end{equation}
where $C_j$ denotes the set of embeddings assigned to cluster $j$, $[C_j] = \{i|\mathbf{x}_i \in C_j\}$ denotes the set of embedding indices assigned to cluster $j$ and $\boldsymbol{\mu}_j$ is the cluster centroid, computed as the mean of the assigned embeddings:
\begin{equation}\label{eq:centroidupdate}
    \boldsymbol{\mu}_j = \frac{1}{|C_j|} \sum_{i \in [C_j]} \mathbf{x}_i.
\end{equation}
The \texttt{k-means} algorithm assigns each document embedding $\mathbf{x}_i$ to the closest centroid based on the smallest distance and updates centroids accordingly until convergence after $T$ iterations. 
The primary objective of the \texttt{k-means} algorithm is to iteratively adjust cluster centroids to minimize the within-cluster variance, effectively guiding the algorithm toward an optimal set of centroids. However, due to its sensitivity to initialization and the non-convex nature of its objective function, \texttt{k-means} does not guarantee convergence to the global optimum and can instead become trapped in local optima \citep{macqueen1967some, lloyd1982least}. Various strategies, such as \texttt{k-means++} initialization and multiple restarts, have been proposed to mitigate these issues and improve the likelihood of achieving better clustering results \citep{arthur2006k}.


\section{\texttt{k-LLMmeans}}\label{sec:kllmmeans}
To enhance \texttt{k-means} for text clustering, we introduce \texttt{k-LLMmeans}, a novel variant that integrates LLM-based centroids at intermediate stages of the clustering process. The formal procedure is outlined in Algorithm \ref{alg:kllmmeans}. The key distinction between \texttt{k-LLMmeans} and standard \texttt{k-means} lies in the centroid update mechanism: every $l$ iterations, the traditional update step in Equation \eqref{eq:centroidupdate} is replaced by 
\begin{equation}
    \boldsymbol{\mu}_j = \text{Embedding}(f_\text{LLM}(p_j))\\
\end{equation}
where $p_j = \text{Prompt}\left(I, \{ d_{z_i}| z_i \sim [C_j] \}_{i=1}^{m_j}\right)$, and $m_j = \min(m,|C_j|)$.
Here, $z_i \sim [C_j]$ denotes a sampled index of the embeddings assigned to cluster $C_j$ (without repetitions) and $m$ is a parameter that represents the maximum number of sampled indices used to compute the cluster centroid $\boldsymbol{\mu}_j$. In simple terms, we update a cluster's centroid by using the embedding of the response generated by an LLM when queried with a prompt containing a summarization instruction 
$I$ and a representative sample of documents from the cluster. Rather than providing all documents within the cluster as input, the LLM processes a representative sample as a context prompt. While incorporating the entire cluster is theoretically possible, it poses practical challenges due to prompt length limitations. Therefore, we propose selecting the sample cluster documents using a \texttt{k-means++} sampling of the cluster embeddings.  Our experiments demonstrate that this sampling process facilitates a more effective synthesis of the cluster’s content, leading to improved summaries and, consequently, more refined centroid updates. The instruction $I$ varies depending on the clustering task, but standard summarization prompts are generally sufficient.

This novel approach can help mitigate the tendency of \texttt{k-means} to get stuck in local optima by dynamically adjusting centroids using semantic insights from the data. Unlike standard \texttt{k-means}, which relies solely on Euclidean updates, an LLM can refine centroids based on contextual meaning and high-dimensional representations, allowing for better adaptation to complex structures in text data. By periodically re-centering clusters with LLM-informed embeddings, the algorithm can escape poor local minima and achieve more coherent and semantically meaningful clusters, even when the initial \texttt{k-means++} seeding is suboptimal.  Apart from this modification, our algorithm adheres to the core principles of \texttt{k-means}, preserving its well-established properties and ensuring practical robustness. We demonstrate in Section~\ref{sec:results} that \texttt{k-LLMmeans} consistently outperforms \texttt{k-means} across extensive simulations.

\begin{figure*}[ht]
\begin{center}
\centerline{\includegraphics[width=\linewidth]{examples_summary.jpg}}
\caption{An illustration of the dynamic evolution of an LLM-generated centroid during the \texttt{k-LLMmeans} clustering process.}
\label{fig:examples}
\end{center}

\end{figure*}

\subsection{Scalability and transparency}

%One of the key advantages of our approach over more complex LLM-based clustering algorithms is its scalability. As noted by \citet{feng2024llmedgerefine}, the LLM usage complexity of most state-of-the-art LLM-based clustering methods grows with the sample size. In contrast, our method decouples LLM usage from the dataset size, making it depend only on the number of clusters and $l$. Moreover, as demonstrated in Section XX, even small values of $l$ suffice to achieve performance improvements over \texttt{k-means}.

%Another major advantage of our approach is its transparency, a quality that sets it apart from both traditional and more complex LLM-based clustering methods. The centroids updated by the LLM are interpretable, providing meaningful descriptions of the clusters they represent. This interpretability enables practitioners to track patterns and monitor the algorithm’s evolution across iterations. While post-processing techniques can be applied to any clustering method to extract similar insights, our algorithm inherently incorporates this information during the clustering process, enhancing its practical utility.

Our approach offers two key advantages over more complex LLM-based clustering methods: scalability and transparency. Unlike most state-of-the-art methods, whose LLM usage complexity grows with sample size \citep{feng2024llmedgerefine}, our algorithm depends only on $k$ and $l$, with even small $l$ values yielding performance gains over \texttt{k-means}. Additionally, our method enhances interpretability by producing LLM-updated centroids that meaningfully represent clusters, allowing practitioners to track and validate the algorithm evolution without requiring post-processing. While similar insights can be extracted indirectly in other clustering algorithms, our approach integrates this interpretability directly into the clustering process, improving both usability and practical applicability (See Figure~\ref{fig:examples} for an example of such evolution of summaries).

 
\section{Mini-batch \texttt{k-LLMmeans}}\label{sec:minibatchkllmmeans}

%minibatchkllmmeans
\RestyleAlgo{ruled}
\begin{algorithm}[hbt!]
\caption{Mini-batch \texttt{k-LLMmeans}}\label{alg:minibatchkllmmeans}
\SetKwInOut{Data}{Data}
\SetKwInOut{Input}{input}
\Input{$\ \{D_1,\cdots,D_b\}, k, I, m, l, T \ 
$ }
\tcp{$b$ batches of documents}
\For{$j \gets 1$ \KwTo $k$}{
    $C_j = \{\}$\;
}
%\tcp{Initialize centroids in $\bf 0$}
$\{\boldsymbol{\mu}_1, \dots, \boldsymbol{\mu}_k\} \gets \{{\bf 0}, \dots, {\bf 0}\}$\;
\For{$i \gets 1$ \KwTo $b$}{ 
    \tcp{Compute \texttt{k-LLMmeans} with documents in batch\footnotemark}
    $\{\boldsymbol{\mu}^*_1, \dots, \boldsymbol{\mu}^*_k\}, \{C^*_1, \dots, C^*_k\}, S_b \gets \texttt{k-LLMmeans}(D_i,k,I,m,l,T)$\;
    \tcp{Update centroids proportional to current cluster sizes and batch cluster sizes}
    \For{$j \gets 1$ \KwTo $k$}{
        $\eta \gets \frac{|C^*_j|}{|C_j|+|C^*_j|}$\;
        $\boldsymbol{\mu}_j \gets \boldsymbol{\mu}_j (1-\eta) + \eta \boldsymbol{\mu}^*_j$\;
    }
}
\Return $\{\boldsymbol{\mu}_1, \dots, \boldsymbol{\mu}_k\}, \{S_1, \dots, S_b\}$
\end{algorithm}
\footnotetext{Here \texttt{k-LLMmeans} is initialized with the final centroids of the previous batch}

Mini-batch \texttt{k-means} \citep{sculley2010web} is an efficient strategy for large-scale text clustering that processes small, randomly sampled mini-batches instead of the full dataset. This approach substantially reduces memory usage and computational cost, making it well suited for continuously generated text streams—such as those from social media, news, or customer feedback—where data must be clustered incrementally without full dataset access. Mini-batch \texttt{k-means} exhibits convergence properties comparable to standard \texttt{k-means} while offering superior scalability. 

Although numerous streaming clustering methods that do not rely on LLMs have been studied \citep{silva2013data,aggarwal2018survey,ribeiro2017unsupervised,aggarwal2003framework,ackermann2012streamkm++,ordonez2003clustering}, only a few have incorporated LLMs \citep{tarekegn2024large,nakshatri2023using}. Moreover, existing offline LLM-based clustering approaches face scalability issues, highlighting the need for scalable LLM-driven clustering in an online setting. To address this gap, we introduce mini-batch \texttt{k-LLMmeans}, which directly extends mini-batch \texttt{k-means} by incorporating minimal LLM usage during centroid updates. Algorithm~\ref{alg:minibatchkllmmeans} details how mini-batch \texttt{k-LLMmeans} sequentially receives $b$ batches of documents $D_1,\dots D_b$ where each batch contains a set of documents (these batches can either be random samples from a large corpus or represent sequential data).  It processes each batch sequentially with \texttt{k-LLMmeans} and updates centroids incrementally using a weighted rule like mini-batch \texttt{k-means}. Our mini-batch \texttt{k-LLMmeans} algorithm preserves the desirable properties of mini-batch \texttt{k-means}, with low memory and LLM usage. Section~ \ref{sec:experiments} shows that it also outperforms in simulations. 

\section{Experiments}\label{sec:experiments}


\begin{table*}
  \centering
  \small
%\begin{table}[!htbp]
\centering
\begin{tabular}{clcccccccccccccc}
  \hline
 &\multirow{2}{*}{Dataset/Method}& \multicolumn{2}{c}{Bank77}&& \multicolumn{2}{c}{CLINC} && \multicolumn{2}{c}{GoEmo} && \multicolumn{2}{c}{Massive (I)} && \multicolumn{2}{c}{Massive (D)}\\
\cline{3-4} \cline{6-7} \cline{9-10} \cline{12-13} \cline{15-16}
&&NMI&dist&&NMI&dist&&NMI&dist&&NMI&dist&&NMI&dist\\
\hline
\multirow{6}{*}{\rotatebox{90}{distilbert}}&k-means&64.4&{\bf 0.335}&&77.2&{\bf 0.34}&&18.3&0.364&&58.1&0.366&&45.1&0.309\\
&k-medoids&56.0&0.591&&66.8&0.69&&14.0&0.769&&42.6&0.847&&27.2&0.81\\
&k-LLMmeans-1&64.8&0.351&&77.4&0.352&&18.3&0.358&&58.6&{\bf 0.362}&&46.0&{\bf 0.294}\\
&k-LLMmeans-5&64.8&0.358&&77.7&0.356&&18.3&0.354&&{\bf 59.0}&0.363&&45.6&0.306\\
&k-LLMmeans-FS1&64.8&0.352&&78.1&0.346&&{\bf 18.9}&0.357&&58.8&0.365&&45.7&0.299\\
&k-LLMmeans-FS5&{\bf 64.9}&0.363&&{\bf 78.7}&0.343&&18.8&{\bf 0.351}&&{\bf 59.0}&{\bf 0.362}&&{\bf 46.4}&0.295\\
\hline
\multirow{6}{*}{\rotatebox{90}{openai}}&k-means&83.0&0.225&&92.0&0.2&&20.5&0.287&&72.4&0.32&&67.9&0.246\\
&k-medoids&69.5&0.639&&77.7&0.694&&15.9&0.852&&52.4&0.86&&38.8&0.797\\
&k-LLMmeans-1&83.6&0.221&&92.5&0.194&&{\bf 22.3}&0.278&&73.0&0.314&&69.6&0.232\\
&k-LLMmeans-5&83.6&0.226&&92.8&0.186&&22.1&{\bf 0.275}&&73.5&0.307&&69.5&0.238\\
&k-LLMmeans-FS1&83.8&{\bf 0.219}&&92.8&0.186&&21.9&0.283&&73.6&0.314&&69.4&0.233\\
&k-LLMmeans-FS5&{\bf 84.1}&0.22&&{\bf 93.1}&{\bf 0.179}&&{\bf 22.3}&0.278&&{\bf 73.9}&{\bf 0.302}&&{\bf 70.6}&{\bf 0.227}\\
\hline
\multirow{6}{*}{\rotatebox{90}{e5-large}}&k-means&76.7&0.142&&90.8&0.131&&22.8&0.176&&70.9&0.175&&63.7&0.138\\
&k-medoids&64.5&0.331&&73.4&0.371&&15.7&0.447&&50.3&0.437&&36.3&0.405\\
&k-LLMmeans-1&78.6&0.138&&91.6&0.127&&23.4&0.176&&72.0&0.17&&64.8&0.134\\
&k-LLMmeans-5&79.0&0.136&&92.0&0.127&&23.7&0.174&&72.1&{\bf 0.166}&&{\bf 66.0}&{\bf 0.131}\\
&k-LLMmeans-FS1&78.8&0.136&&91.7&0.127&&23.7&0.174&&71.6&0.173&&64.9&0.136\\
&k-LLMmeans-FS5&{\bf 79.5}&{\bf 0.134}&&{\bf 92.5}&{\bf 0.119}&&{\bf 24.3}&{\bf 0.168}&&{\bf 72.4}&{\bf 0.166}&&65.9&0.133\\
\hline
\multirow{6}{*}{\rotatebox{90}{sbert}}&k-means&80.9&0.255&&91.0&0.215&&13.3&0.355&&70.7&0.344&&64.6&0.271\\
&k-medoids&69.0&0.64&&78.5&0.68&&12.0&0.899&&54.6&0.817&&44.6&0.8\\
&k-LLMmeans-1&81.7&0.253&&91.6&0.215&&13.6&0.351&&71.2&0.334&&65.6&0.248\\
&k-LLMmeans-5&81.8&0.253&&91.9&0.21&&13.7&0.348&&71.5&{\bf 0.325}&&65.2&0.25\\
&k-LLMmeans-FS1&82.0&{\bf 0.246}&&91.8&0.208&&{\bf 13.9}&0.348&&71.4&0.333&&{\bf 66.2}&{\bf 0.243}\\
&k-LLMmeans-FS5&{\bf 82.2}&0.247&&{\bf 92.5}&{\bf 0.198}&&{\bf 13.9}&{\bf 0.346}&&{\bf 71.9}&0.337&&65.6&0.254\\
\hline
 \end{tabular}
   \caption{\label{offlinetable} Average Normalized Mutual Information (NMI) and distance between final and true centroids (dist) for \texttt{k-means}, \texttt{k-medoids}, and four \texttt{k-LLMmeans} variants across 10 random seeds on four benchmark datasets (including both domain and intent from MASSIVE), using four different embedding models.
  }
 \end{table*}
 
\subsection{Datasets} 
We evaluate our clustering approach on four benchmark datasets:

\begin{itemize} \item \textbf{Bank77} \citep{casanueva-etal-2020-efficient}: Consists of 3,080 customer queries related to banking services, categorized into 77 distinct intents. 
\item \textbf{CLINC} \citep{larson-etal-2019-evaluation}: A diverse set of 4,500 queries spanning 150 intent classes across multiple domains, designed for open-domain intent classification. 
\item \textbf{GoEmo} \citep{demszky-etal-2020-goemotions}: Contains 2,984 social media posts annotated with 27 fine-grained emotion categories. We removed the neutral expressions to address data imbalance and retained only entries with a single, unique emotion. 
\item \textbf{MASSIVE} \citep{fitzgerald-etal-2023-massive}: Comprises 2,974 English-language virtual assistant utterances grouped into 18 domains and 59 intent categories. 
\end{itemize}
These datasets provide a robust evaluation setting for text clustering across different domains and classification granularities. 

\subsection{New compiled dataset for testing text-Streaming Clustering Algorithms}\label{sec:stack}
We extract and unify a challenging data stream comprising unique archive posts collected from 84 Stack Exchange sites \citep{StackExchangeData}. Each post is accompanied by the site label (domain) and timestamp, making this dataset well-suited for evaluating online or sequential clustering methods. 
Our raw dataset spans 84 domains, each containing at least 20 posts per year from 2018 to 2023 (with post lengths ranging from 20 to 1000 characters), totaling 499,359 posts. For our experiments, we focus on posts from 2020 to 2023 and further filter out labels that do not exceed 500 posts in 2023. The resulting subset comprises 35 distinct groups and 69,147 posts. Both the raw and clean data are provided with this paper.

\subsection{Methods}

\begin{table*}
  \centering
  \small
%\begin{table}[!htbp]
\centering
\begin{tabular}{lccccccccccc}
  \hline
 \multirow{3}{*}{Year/Method}& \multicolumn{2}{c}{2020}&& \multicolumn{2}{c}{2021} && \multicolumn{2}{c}{2022} && \multicolumn{2}{c}{2023}\\
 
 & \multicolumn{2}{c}{(69147 posts)}&& \multicolumn{2}{c}{(54322 posts)} && \multicolumn{2}{c}{(43521 posts)} && \multicolumn{2}{c}{(38953 posts)}\\
 
\cline{2-3} \cline{5-6} \cline{8-9} \cline{11-12} 
&NMI&dist&&NMI&dist&&NMI&dist&&NMI&dist\\
\hline
k-means&80.6&0.14&&79.0&0.167&&79.0&0.154&&79.6&0.138\\
mini-batch k-means&78.2&0.181&&77.4&0.166&&77.6&0.173&&77.0&0.175\\
seq. mini-batch k-means&76.6&0.187&&75.2&0.185&&75.6&0.187&&74.8&0.184\\
mini-batch k-LLMmeans-1&81.2&0.141&&80.3&0.138&&79.4&0.145&&80.0&0.136\\
mini-batch k-LLMmeans-5&80.9&0.144&&{\bf 80.5}&0.127&&{\bf 80.5}&{\bf 0.125}&&{\bf 80.3}&{\bf 0.129}\\
mini-batch k-LLMmeans-FS1&81.1&0.136&&79.8&0.141&&79.3&0.147&&79.8&0.138\\
mini-batch k-LLMmeans-FS5&{\bf 81.6}&{\bf 0.126}&&80.2&{\bf 0.126}&&80.1&0.129&&80.1&0.133\\
\hline
\end{tabular}
\caption{\label{seqtable} Average Normalized Mutual Information (NMI) and distance from final and true centroids (dist) for \texttt{k-means}, mini-batch 
 \texttt{k-means}, sequential mini-batch \texttt{k-means} and four sequential mini-batch \texttt{k-LLMmeans} variants with  across 5 random seeds on StackExchange data. 
  }
 \end{table*}

We evaluate our \texttt{k-LLMmeans} algorithm on each of the four static (non-streaming) dataset using the known number of clusters and performing 120 centroid-update iterations and 10 different seeds. To demonstrate the robustness of our approach, we compute embeddings with four different pretrained models: \texttt{distilbert} \citep{sanh2019distilbert}, \texttt{e5-large} \citep{wang2022text}, \texttt{s-bert} \citep{reimers-gurevych-2019-sentence}, and OpenAI’s \texttt{text-embedding-3-small} \citep{OpenAI2023TextEmbedding}. For the LLM component, we only use OpenAI’s \texttt{gpt-4o} to ensure any observed differences in performance arise from the effectiveness of our clustering method rather than the inherent strengths or weaknesses of different LLMs.

For the instruction task $I$, we employ a simple summarization prompt that adapted to each dataset. For example, for Bank77 we use the prompt: \emph{“The following is a cluster of online banking questions. Write a single question that represents the cluster concisely.”} We examine four variations of \texttt{k-LLMmeans} based on different numbers of summarization steps and the size of the prompts:

\begin{itemize} \item \textbf{\texttt{k-LLMmeans-1}}: A single summarization step ($l=60$) using all documents in the cluster as input. \item \textbf{\texttt{k-LLMmeans-5}}: Five summarization steps ($l=20$) with all documents in the cluster as input at each step. \item \textbf{\texttt{k-LLMmeans-FS1}}: A few-shot variant with 10 randomly selected documents in a single summarization step. \item \textbf{\texttt{k-LLMmeans-FS5}}: A few-shot variant with 10 randomly selected documents in five summarization steps. \end{itemize}
Our baselines are the standard centroid-based algorithms \texttt{k-means} and \texttt{k-medoids}. While alternative clustering methods may achieve stronger performance, our primary goal is to demonstrate improvement specifically over widely used centroid-based approaches.

\vspace{0.2cm}
\textbf{Methods for streaming dataset.} We partition the StackExchange data into four subsets, each corresponding to a single year from 2020 to 2023 (See Table~\ref{seqtable} for yearly dataset sizes). For each yearly subset, we split the data into $b = \lceil \tfrac{n}{10000} \rceil$ equal-sized batches ${D_1,\dots,D_b}$ in chronological order, where $n$ is the number of documents for that year. We then use the ground-truth clusters and run the mini-batch \texttt{k-LLMmeans} algorithm in its four previously described variants (for practical reasons for the full cluster method we use 50 samples). For comparison, we evaluate three baselines: mini-batch \texttt{k-means} with standard random sampling across the entire year (until convergence), sequential mini-batch \texttt{k-means} with $b$ chronological batches, and standard \texttt{k-means} on the full dataset. In all experiments, we use OpenAI’s models for both embeddings and the LLM.

\subsection{Results}\label{sec:results}

Table~\ref{offlinetable} reports the average Normalized Mutual Information (NMI) across all method--dataset--embedding combinations, as well as the average distance (dist) between the centroids produced by each method and the true centroids calculated with the ground truth clusters. A lower distance indicates greater proximity to the optimal solution. This distance aligns well with the objectives of our approach. We also measure Accuracy (not shown here due to space constraints), observing results consistent with the reported metrics. Overall, all \texttt{k-LLMmeans} variants outperform \texttt{k-means} except when using the \texttt{distilbert} embedding. However, the performance of every method using this embedding is clearly inferior compared to those employing any other embedding. Meanwhile, \texttt{k-medoids} emerges as the worst performer. Comparing the four \texttt{k-LLMmeans} configurations reveals only minor differences, although running five summarization steps ($l=20$) tends to offer better performance. Interestingly, using few-shot summarization generally leads to better results than prompting on the entire cluster. These observations suggest \texttt{k-LLMmeans} improves over \texttt{k-means} while remaining efficient, as few-shot summarization appears sufficient without needing extensive LLM usage.

We present the same evaluation metrics for sequential data in Table~\ref{seqtable}. The mini-batch \texttt{k-LLMmeans} methods consistently outperform all three baselines across all scenarios. Notably, they even surpass standard \texttt{k-means}, which operates on the full dataset rather than mini-batches. This highlights the scalability and efficiency of our approach for sequential data, achieving superior performance despite processing data sequentially. As observed previously, few-shot summarization proves effective, mitigating the need for extensive LLM usage.

\section{Case study}\label{sec:case}

\begin{figure*}[ht]
\begin{center}
\centerline{\includegraphics[width=\linewidth]{pictures-cropped.jpg}}
\caption{Sequential evolution of the LLM-generated centroids for four primary clusters during the three batches of the sequential mini-batch \texttt{k-LLMmeans} process applied to 2021 posts from the AI Stack Exchange site \citep{StackExchangeData}. Main aspects are manually highlighted in different color on each cluster.}
\label{fig:sequential}
\end{center}
\vskip -0.2in
\end{figure*}

To demonstrate the interpretability of our method in capturing the evolution of clusters within sequential data, we present a case study using posts from the AI site in the 2021 Stack Exchange dataset \citep{StackExchangeData}. We apply our mini-batch \texttt{k-LLMmeans} algorithm with three equal-length batches and a total of ten clusters. We use the instruction \emph{``The following is a cluster of questions from the AI community. Write a single question that represents the cluster''}.

\vspace{0.2cm}
\textbf{Clustering results. }
The resulting LLM-based centroids span key areas in AI and ML, including neural network training optimization, computer vision tasks, and broader topics such as small datasets, class imbalance, and interpretability. Other clusters cover advanced themes like symbolic-neural integration for AGI, deep reinforcement learning, theoretical debates (e.g., Bayesian vs. frequentist methods), as well as NLP and various architectural choices.

\vspace{0.2cm}
\textbf{Interpretation. }
The key insight from our algorithm is the evolution of clusters over batches, reflecting their dynamic nature over time. Figure~\ref{fig:sequential} illustrates this progression for four major themes: Image Model Optimization, AI Evolution and Challenges, AI Mathematics, and Advanced NLP Techniques.
The \emph{Image Model Optimization} cluster refines its focus from broad improvements in image classification and object detection to specific deep learning optimizations, such as facial expression recognition and anomaly detection. Over time, it emphasizes practical challenges, including handling imbalanced datasets, model uncertainty, and performance enhancement through transfer learning and specialized loss functions.
The \emph{AI Evolution and Challenges} cluster transitions from early rule-based AI and symbolic systems to advanced deep learning and generative models, highlighting both progress and limitations. The shift toward hybrid and neuro-symbolic AI reflects the growing need to integrate diverse techniques for AGI, balancing efficiency, adaptability, and safety.
The \emph{AI Mathematics} cluster starts with unconventional symbol usage and parameter definitions in machine learning, later expanding to matrix rank analysis, Bayesian and frequentist methods, adversarial attacks, and the mathematical foundations necessary for advanced AI concepts.
The \emph{Advanced NLP Techniques} cluster progresses from foundational models (e.g., Word2Vec, BERT) and basic tasks (e.g., sentiment analysis, text classification) to more complex challenges, including model evaluation, domain-specific vocabulary, and multilingual alignment. The shift highlights an increasing focus on contextual understanding, transfer learning, and alternative evaluation metrics for robust NLP.

The result of this case study could enhance post categorization, searchability, answer relevance, and trend detection, making AI discussions more efficient and insightful. More broadly, this demonstrates how our interpretable mini-batch \texttt{k-LLMmeans} clustering algorithm, applied to sequential text streaming, can help practitioners track topic evolution, improve decision-making, and enhance transparency in dynamic information flows.

\section{Conclusions}
We introduced \texttt{k-LLMmeans} and mini-batch \texttt{k-LLMmeans}, unsupervised clustering algorithms that refine \texttt{k-means} by integrating a lightweight LLM-based summarization into centroid updates. This modification enriches the contextual representation while preserving \texttt{k-means}' efficiency and scalability, yielding interpretable clusters for both static corpus of documents and streaming text data. 

\section*{Limitations}
Our method relies on both text embeddings and LLM queries, making it sensitive to the quality and biases of the underlying language models. Any inaccuracies in these models may propagate into the clustering results. While we demonstrate that a simple instruction-based approach is effective, the design of these instructions can still influence outcomes. Additionally, the few-shot variant of our algorithm assumes that a small number of samples can sufficiently capture the overall structure of the clusters. While this is generally practical, it may become a limitation when dealing with highly complex or heterogeneous clusters. Finally, similar to \texttt{k-means}, our \texttt{k-LLMmeans} method requires specifying the number of clusters in advance, which may not always align with the true structure of the data.

\bibliography{ref}

%\appendix

%\section{Example Appendix}
%\label{sec:appendix}

%This is an appendix.

\end{document}
