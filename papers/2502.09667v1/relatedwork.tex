\section{Related work}
Clustering techniques are central to natural language processing and machine learning. Hierarchical methods \citep{johnson1967hierarchical, blashfield1978literature} build tree-structured representations of nested document relationships. Density-based approaches like DBSCAN \citep{ester1996density} and graph-based methods detect clusters of arbitrary shapes, while spectral clustering \citep{ng2001spectral} leverages eigen-decomposition to uncover complex structures. Model-based techniques—including Gaussian mixture models \citep{dempster1977maximum} and recent neural network frameworks \citep{zhou2019end, huang2014deep, yang2016joint, zhang2021supporting, xie2016unsupervised}—provide probabilistic clustering formulations. Additionally, topic modeling methods, from probabilistic latent semantic analysis \citep{hofmann2001unsupervised} to latent Dirichlet allocation \citep{blei2003latent}, capture word co-occurrence patterns and latent topics. However, these approaches diverge from our objective of enhancing \texttt{k-means} with LLMs, and are therefore not directly comparable.

Recent studies have integrated LLMs into clustering pipelines to reduce expert supervision and enhance performance. For instance, \citet{viswanathan2024large} employ LLMs to augment document representations, generate pseudo pairwise constraints, and post-correct low-confidence assignments for query-efficient, few-shot semi-supervised clustering. Similarly, \citet{zhang-etal-2023-clusterllm} propose \texttt{ClusterLLM}, which uses instruction-tuned LLMs via interactive triplet and pairwise feedback to cost-effectively refine clustering granularity. Complementary approaches \citep{tipirneni2024context, PETUKHOVA2025100} show that context-derived representations capture subtle semantic nuances beyond traditional embeddings. Additionally, \citet{wang2023goal} introduce a goal-driven, explainable clustering method that employs natural language descriptions to clarify cluster boundaries, while \citet{de2023idas} present \texttt{IDAS} for intent discovery using abstractive summarization. Moreover, \citet{feng2024llmedgerefine} propose an iterative refinement mechanism that forms super-points to mitigate outliers and reassign ambiguous edge points, resulting in clusters with higher coherence and robustness. In contrast, our approach directly enhances the core \texttt{k-means} algorithm in an LLM-scalable manner.
\RestyleAlgo{ruled}
\begin{algorithm}[hbt!]
\caption{\texttt{k-LLMmeans}}\label{alg:kllmmeans}
\SetKwInOut{Data}{Data}
\SetKwInOut{Input}{input}
\Input{$\ D = \{d_1,\dots, d_n\}, k, I, m, l, T$}
%\tcp{Compute embeddings}
\For{$i \gets 1$ \KwTo $n$}{ 
    $\mathbf{x}_i = \text{Embedding}(d_i)$\;
}
%\tcp{Update centroids for $T$ iterations}
\For{$t \gets 1$ \KwTo $T$}{
    \If{$t = 1$}{
        \tcp{Initialize centroids using \texttt{k-means++}}
        \tcp{This step can be omitted if initial centroids are provided}
        $\{\boldsymbol{\mu}_1, \dots, \boldsymbol{\mu}_k\} \gets \texttt{k-means++}(\{d_1, \dots, d_n\}, k)$\;
    }
    \ElseIf{$t \bmod l = 0$}{
        \tcp{Update centroids with LLM every $l$ iterations}
        \For{$j \gets 1$ \KwTo $k$}{
            $m_j \gets \min(m, |C_j|)$\;
            
            %\tcp{Select $m_j$ representative documents from cluster using \texttt{k-means++}}
            $\{d_{z_1}, \dots, d_{z_{m_j}}\} \gets \texttt{k-means++}(\{d_i \mid i \in [C_j]\}, m_j)$\;
    
            %\tcp{Construct prompt for LLM-based summarization}
            $p_j \gets \text{Prompt}(I, \{d_{z_1}, \dots, d_{z_{m_j}}\})$\;
            $s_j \gets f_{\text{LLM}}(p_j)$\;
            %\tcp{Update centroid using the embedding of the LLM-generated summary}
            $\boldsymbol{\mu}_j \gets \text{Embedding}(s_j)$\;
        }
    }
    \Else{
        \tcp{Update centroids using standard averaging}
        \For{$j \gets 1$ \KwTo $k$}{
            $\boldsymbol{\mu}_j \gets \frac{1}{|C_j|} \sum_{i \in [C_j]} \mathbf{x}_i$\;
        }
    }
    %\tcp{Reset clusters}
    \For{$j \gets 1$ \KwTo $k$}{
        $C_j = \{\}$\;
    }
    %\tcp{Assign each point $\mathbf{x}_i$ to the cluster with the nearest centroid}
    \For{$i \gets 1$ \KwTo $n$}{
        $j^* \gets \arg\min_{j \in \{1, \dots, k\}} d(x_i, \boldsymbol{\mu}_j)$\;
        \tcp{Assign $x_i$ to cluster $C_{j^*}$}
        $C_{j^*} \gets C_{j^*} \cup \{x_i\}$\;
    }
}
\Return $\{\boldsymbol{\mu}_1, \dots, \boldsymbol{\mu}_k\}, \{s_1, \dots, s_k\}$
\end{algorithm}