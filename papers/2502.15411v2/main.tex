% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
%\usepackage[review]{acl}
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{listings}
\usepackage{xcolor}
\usepackage[most]{tcolorbox}
\usepackage[ruled,vlined]{algorithm2e}

\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
            \node[shape=circle,draw,inner sep=.6pt] (char) {#1};}}

\lstdefinelanguage{json}{
    basicstyle=\small\ttfamily,
    numbers=left,
    numberstyle=\tiny,
    stepnumber=1,
    numbersep=8pt,
    xleftmargin=15pt,
    showstringspaces=false,
    breaklines=true,
    frame=lines,
    backgroundcolor=\color{gray!10},
    literate=
     *{0}{{{\color{blue}0}}}{1}
      {1}{{{\color{blue}1}}}{1}
      {2}{{{\color{blue}2}}}{1}
      {3}{{{\color{blue}3}}}{1}
      {4}{{{\color{blue}4}}}{1}
      {5}{{{\color{blue}5}}}{1}
      {6}{{{\color{blue}6}}}{1}
      {7}{{{\color{blue}7}}}{1}
      {8}{{{\color{blue}8}}}{1}
      {9}{{{\color{blue}9}}}{1}
      {:}{{{\color{red}{:}}}}{1}
      {,}{{{\color{red}{,}}}}{1}
}

\tcbset{
    promptstyle/.style={
        enhanced,
        width=\linewidth,
        colback=white,
        colframe=black,
        colbacktitle=gray!20,
        coltitle=black,
        rounded corners,
        boxrule=0.5pt,
        drop shadow=black!50!white,
        attach boxed title to top left={
            xshift=-2mm,
            yshift=-2mm
        },
        boxed title style={
            rounded corners,
            size=small,
            colback=gray!20
        }
    },
    replystyleg/.style={
        enhanced,
        width=\linewidth,
        colback=green!15,
        colframe=black,
        colbacktitle=green!30,
        coltitle=black,
        boxrule=0.5pt,
        drop shadow=black!50!white,
        rounded corners,
        sharp corners=north,
        attach boxed title to top right={
            xshift=-2mm,
            yshift=-2mm
        },
        boxed title style={
            rounded corners,
            size=small,
            colback=green!40
        }
    },
    replystyler/.style={
        enhanced,
        width=\linewidth,
        colback=red!15,
        colframe=black,
        colbacktitle=red!40,
        coltitle=black,
        boxrule=0.5pt,
        drop shadow=black!50!white,
        rounded corners,
        sharp corners=north,
        attach boxed title to top right={
            xshift=-2mm,
            yshift=-2mm
        },
        boxed title style={
            rounded corners,
            size=small,
            colback=red!40
        }
    }
}

\newtcolorbox{promptbox}[1][]{
    promptstyle,
    title=Prompt,
    #1
}

\usepackage{graphicx}
\usepackage{todonotes}
\usepackage{booktabs}
\usepackage{comment}
\usepackage{amsmath}
\usepackage{array}
\usepackage{subcaption}
\usepackage{colortbl}
\usepackage{xcolor}

\usepackage{todonotes}
\definecolor{mike-color}{rgb}{0.858, 0.188, 0.478}
\newcommand{\mz}[1]{\textcolor{mike-color}{$_{Mike}$[#1]}}
\newcommand{\mztodo}[1]{\todo[color=pink]{$_{mike}$ {\footnotesize #1}}}
\newcommand{\hifi}{\textsc{HiFi-KPI}}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{\textsc{HiFi-KPI}:\\
A Dataset for Hierarchical KPI Extraction from Earnings Filings}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

%\author{First Author \\
%  Affiliation / Address line 1 \\
%  Affiliation / Address line 2 \\
%  Affiliation / Address line 3 \\
%  \texttt{email@domain} \\\And
%  Second Author \\
%  Affiliation / Address line 1 \\
%  Affiliation / Address line 2 \\
%  Affiliation / Address line 3 \\
%  \texttt{email@domain} \\}

\author{
  \textbf{Rasmus Aavang\textsuperscript{1,2}},
  \textbf{Giovanni Rizzi\textsuperscript{2}},
  \textbf{Rasmus BÃ¸ggild\textsuperscript{2}},
  \textbf{Alexandre Iolov\textsuperscript{2}},
\\
  \textbf{Mike Zhang\textsuperscript{1}},
  \textbf{Johannes Bjerva\textsuperscript{1}}
  %\textbf{Seventh Author\textsuperscript{1}},
  %\textbf{Eighth Author \textsuperscript{1,2,3,4}}
\\
\\
  \textsuperscript{1}Department of Computer Science, Aalborg University, Denmark \\
  \textsuperscript{2}ALIPES ApS, Denmark
%  \textsuperscript{3}Affiliation 3,
\\
  \small{
      \textbf{Correspondence:} \href{mailto:rtaj@cs.aau.dk}{rtaj@cs.aau.dk}
  }
}

\usepackage{float}


\begin{document}
\maketitle
\begin{abstract}
\looseness=-1
The U.S.\ Securities and Exchange Commission (SEC) requires that public companies file financial reports tagging numbers with the machine readable inline eXtensible Business Reporting Language (iXBRL) standard.
However, the highly complex and highly granular taxonomy defined by iXBRL limits label transferability across domains.
In this paper, we introduce the \textbf{Hi}erarchical \textbf{Fi}nancial \textbf{K}ey \textbf{P}erformance \textbf{I}ndicator (\hifi{}) dataset, designed to facilitate numerical KPI extraction at specified levels of granularity from unstructured financial text.
Our approach organizes a 218,126-label hierarchy using a taxonomy-based grouping method, investigating which taxonomy layer provides the most meaningful structure.
\textbf{\hifi{}} comprises $\sim$1.8M paragraphs and $\sim$5M entities, each linked to a label in the iXBRL-specific calculation and presentation taxonomies. 
%We explore these taxonomies as proxies for conceptual relations. 
We provide baselines using encoder-based approaches and structured extraction using Large Language Models (LLMs). 
To simplify LLM inference and evaluation, we additionally release \textbf{\hifi{} Lite}, a manually curated subset with four expert-mapped labels. 
We publicly release all artifacts.\footnote{Code and data are available at \texttt{\url{https://github.com/aaunlp/HiFi-KPI}}}
\end{abstract}

\section{Introduction}
Key accounting metrics explain over 70\% of a public company's share price~\cite{sadka2007understanding}.
Hence, the ability to accurately assess a company's financial health, can lead to investors earning staggering returns over short time periods~\citep{ke2005institutional}.
While several Natural Language Processing (NLP) datasets have been created for the financial domain~\cite{chen2022convfinqa, jorgensen-etal-2023-multifin} and from SEC filings~\cite{loukas2021edgar, loukas-etal-2022-finer, sharma-etal-2023-financial, lai2024sec}, the potential of parsing the information-rich iXBRL for financial downstream tasks and applications, however, remains unexplored.
Our parsing of the iXBRL format enables context by preserving the relationship between tags and their associated time periods, numerical values and currencies.
This enables more precise context for financial data extraction. 
%While several datasets have been created for natural language processing (NLP) in the financial domain~\cite{chen2022convfinqa, jorgensen-etal-2023-multifin}, and some for SEC filings~\cite{loukas2021edgar, loukas-etal-2022-finer, sharma-etal-2023-financial, lai2024sec}, to the best of our knowledge, none has explored the potential of parsing the information-rich iXBRL.
Mandated by the SEC, XBRL and later iXBRL enable financial reporting, iXBRL adopts the HTML format. %of financial documents.
%Our approach preserves the relationships between XBRL tags and their associated periods, numerical values and currencies by parsing the iXBRL format.
Additionally, our approach enables prediction by different levels of granularity by introducing a recursive approach to ascending the presentation and calculation taxonomies (Figure~\ref{fig:figure1}).


%By preserving the relations between XBRL tags and the period they describe, their numerical values, currencies, and lastly hierarchical relationships defined by the taxonomies, our approach enables a more detailed extraction of financial data.
%Our approach allows predicting different granularities (Figure~\ref{fig:figure1}) and the parsing enables investors to make much more informed decisions by linking financial metrics to precise temporal and monetary contexts. 
% This paper presents a novel hierarchical dataset from parsing iXBRL documents and traversing their taxonomies~\cite{SEC.gov_inline_xbrl}.
% In this paper, we present a novel hierarchical dataset generated by parsing iXBRL documents and traversing their taxonomies~\cite{SEC.gov_inline_xbrl}.


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{FinalFigure.png}
    \caption{\textbf{Granularity Selection.} Example snippet where arrows mark corresponding positions in a tree-map of the two taxonomies.
    Top left figure illustrate our recursive approach to ascending the hierarchy where
    leaves inherit their parents labels.
    }
    \label{fig:figure1}
\end{figure}

\paragraph{Contributions.} We contribute the following: 
\circled{1} \hifi{}: A large iXBRL-based dataset of $\sim$1.8M paragraphs and $\sim$5M entities, plus a smaller \hifi{} Lite with four expert-defined label clusters; 
\circled{2} A taxonomy-based granularity selection method that streamlines the calculation and presentation taxonomies; 
\circled{3} Baselines for text classification, sequence labeling, and LLM-based structured extraction over \hifi{}; 
\circled{4} Merged and company-specific versions of the taxonomies, alongside insights for future research on conceptual representations in finance.


\section{Related Work}
\paragraph{Financial NLP Datasets.}
Recent financial NLP datasets cover sentiment analysis~\citep{gupta2020comprehensive}, named entity recognition~\citep{alvarado2015domain, shah2022fluemeetsflangbenchmarks}, and numerical reasoning~\citep{chen2022finqadatasetnumericalreasoning}.
While early approaches used rule-based methods~\citep{extraction2007, sheikh2012rule, hutto2014vader}, modern efforts leverage large corpora e.g. from SEC fillings ~\citep{loukas2021edgar}. 
Unlike FiNER-139~\citep{loukas-etal-2022-finer}, where 80.42\% of entries do not contain any tags at all, our dataset maximizes informative content with 2.77 tags/sentence.

% Numerous financial NLP datasets focus on tasks such as sentiment analysis~\citep{gupta2020comprehensive}, named entity recognition~\citep{alvarado2015domain, shah2022fluemeetsflangbenchmarks}, and numerical reasoning~\citep{chen2022finqadatasetnumericalreasoning}.
% Early work often used rule-based or template-based approaches~\citep{extraction2007, sheikh2012rule, hutto2014vader} for extraction, whereas more recent efforts train advanced models on larger corpora of e.g., SEC filings~\citep{loukas2021edgar}.
% \citet{alvarado2015domain} showed the value of tailored annotation, significantly improving model performance on specialized financial text. 
% \citet{loukas-etal-2022-finer} and \citet{sharma-etal-2023-financial} introduced KPI-based datasets for 10-K/10-Q filings,  but did not leverage the context present in the information-rich iXBRL format or its hierarchical structure. In contrast to FiNER-139~\citep{loukas-etal-2022-finer}---that contains a substantial proportion (80.42\%) of text snippets with no entities--our dataset is structured to maximize informative content, with nearly all sentences containing relevant entity tags, averaging 2.77 tags per sentence.

\paragraph{Language Models in Finance.}
Transformer-based models~\citep{vaswani2017attention} led to specialized models like FinBERT~\citep{yang2020finbert}. 
Recent LLMs including BloombergGPT~\cite{wu2023bloomberggpt} as well as more general models like NuExtract~\citep{cripwell2024nuextract}, Qwen2.5~\citep{qwen2025qwen25technicalreport}, and Deepseek~\citep{deepseekai2024deepseekv3technicalreport} excel at structured extraction.
Our \hifi{} directly ties labels to the XBRL taxonomy, enabling downstream tasks beyond text labeling.
% Modern transformer-based approaches~\citep{vaswani2017attention} have yielded finance-specific approaches such as 
% FinBERT~\citep{yang2020finbert} and others~\citep{araci2019finbertfinancialsentimentanalysis}.
% LLMs like BloombergGPT~\cite{wu2023bloomberggpt} combine general and specialized data to tackle tasks ranging from sentiment analysis to question answering. New open-source LLMs, such as nuExtract~\citep{cripwell2024nuextract}, Qwen2.5~\citep{qwen2025qwen25technicalreport}, and Deepseek~\citep{deepseekai2024deepseekv3technicalreport} excel at structured extraction. In our \hifi{}, each label is directly tied to XBRL taxonomy labels, enabling more robust, concept-oriented tasks than simple text labeling.

\subsection{SEC Filings (10-Q \& 10-K)}
U.S. public companies must file quarterly (10-Q) and annual (10-K) reports~\citep{SECFinalRule2000,sec_exchange_2024}, which contain standardized financial statements.
% U.S.\ public companies must comply with SEC rules, including ``Regulation Fair Disclosure'' and the filing of 10-Q (quarterly) and 10-K (annual) reports~\citep{SECFinalRule2000,sec_exchange_2024}. 
% These contain detailed financial statements that often follow standard templates.

\paragraph{XBRL Taxonomy.}
iXBRL standardizes financial reporting~\citep{intrinio_xbrl}, using the \texttt{.cal} (calculation) and \texttt{.pre} (presentation) files for arithmetic and organizational structures~\citep{XBRLTaxonomies,XBRL_Presentation,PreparersGuide}. 
XBRL requires annotators to choose the most specific tag, creating high granularity, limiting standardization across companies~\citep{what_intrinio_xbrl}.
% aims to standardize how financial data are reported~\citep{intrinio_xbrl}, though actual practice varies by company. 
% The standard subset of XBRL labels includes thousands of granular tags, which hinders straightforward cross-company comparisons. 
% The taxonomy defines several relationship files; among these, the \texttt{.cal} (calculation) and \texttt{.pre} (presentation) files provide arithmetic and organizational structures, respectively~\citep{XBRLTaxonomies,XBRL_Presentation,PreparersGuide}. 
% In principle, filers must select the most specific suitable tag, creating high granularity but also limiting standardization. 
% Our dataset incorporates a taxonomy-based granularity selection method, selecting each entity's precise XBRL label, enabling analysis across filings.
% In contrast to FiNER-139---that contains a substantial proportion (80.42\%) of text snippets with only O-tags---our dataset is structured to maximize informative content, with nearly all sentences containing relevant entity tags---averaging 2.77 tags per sentence.
The presentation relationships in the XBRL US GAAP Taxonomy structure elements hierarchically to aid user navigation \cite{PreparersGuide}.
%These relationships are intended to be conceptually coherent, ensuring that, at least for expert reviewers, the organization of elements remains interpretable.
In the XBRL framework, taggers are expected to tag numerical values to the most specific applicable label within the taxonomy \cite{PreparersGuide}.
XBRL tags are long e.g., us-gaap:CumulativeEffectOf-NewAccountingPrincipleInPeriodOfAdoption, which, while detailed, poses challenges for analysis. 
Thus, our dataset exhibits a high degree of specificity, this granularity complicates cross-company generalization, as the open nature of XBRL permits diverse taxonomy implementations~\citep{what_intrinio_xbrl}.
% Our dataset incorporates taxonomy-based granularity selection, enabling cross-filing analysis while remaining tag specificity.
% \subsection{Financial Downstream Tasks} 
As unstructured data in finance grows~\cite{doi:10.1080/00014788.2019.1611730}, there is a strong interest in developing NLP benchmarks and methods for tasks like financial sentiment analysis, risk assessment, and finance-related question answering. 
By coupling iXBRL-derived tags with text data, \hifi{} provides a rich resource for these applications.

\section{\hifi{}: Taxonomy-based Granularity Selection}
\hifi{} supports multiple downstream tasks such as text classification, sequence labeling, structured information extraction, multi-label classification, and financial question answering.

\paragraph{Dataset Creation.}
Table~\ref{tab:merged_dataset_statistics} summarizes key statistics for both the full dataset and a smaller ``Lite'' subset for LLM inference.
We collected all 10-K and 10-Q filings published between 2017-01-01 and 2024-06-01, yielding 41,211 quarterly (10-Q) and 14,188 annual (10-K) reports. 
While iXBRL became mandatory on June 15, 2020 \citep{sec2018}, our start date also captures early voluntary adopters. 
We parsed each iXBRL document using \texttt{beautifulsoup} \citep{richardson2007beautiful} and regular expressions to extract text snippets along with every embedded XBRL tag, its date range, and numeric value. 
Any misparsed snippets were discarded, and we further filtered text for minimal formatting quality (e.g., removing leading punctuation or non-capitalized starts).
From these steps, we obtained \(\sim\)1.8M paragraphs (\(\sim\)1.07M from 10-Qs; \(\sim\)0.70M from 10-Ks) and \(\sim\)5M tagged entities. 
An example dataset entry appears in Listing~\ref{lst:dataset-example} (Appendix~\ref{app:data-example}), illustrating the iXBRL-derived fields (\texttt{Label}, \texttt{Currency}, \texttt{Value}, and date range). 

\begin{table}[t]
\centering
\scriptsize
\resizebox{\columnwidth}{!}{
\begin{tabular}{l|ll|ll|ll}
\toprule
 & \multicolumn{2}{c|}{\textbf{Train (Lite)}} & \multicolumn{2}{c|}{\textbf{Dev.\ (Lite)}} & \multicolumn{2}{c}{\textbf{Test (Lite)}} \\
\midrule
\textbf{Avg. words}          & 88.57 &      (80.72) & 88.71 & (83.13)   & 87.53 & (77.35) \\
\textbf{Avg. tags}           & 2.82  &      (3.11)  & 2.90  & (3.39)    & 2.75  & (2.85)   \\
\textbf{Word/tags}           & 42.87 &      (34.12) & 42.30 & (32.51)   & 43.26 & (34.10) \\
\textbf{Cutoff Date}         & \multicolumn{2}{c|}{2023.10.31}  & \multicolumn{2}{c|}{2024.05.31} & \multicolumn{2}{c}{2024.06.01} \\
\textbf{\# Paragraphs}       & 1.43M &     (6,359)  & 162K &(768)   & 179K & (856) \\
\textbf{\# Entities}         & 4.04M &     (19,749) & 468K &(2,601) & 491K & (2,437) \\
\bottomrule
\end{tabular}
}
\caption{\textbf{Dataset Statistics.} We show the full dataset and the lite version statistics in brackets.}
\label{tab:merged_dataset_statistics}
\end{table}

\paragraph{Hierarchy.}

Each filing has an associated XBRL attachment describing parent--child relationships in two key files: \texttt{.cal} (calculation) and \texttt{.pre} (presentation). 
We used a combination of \textit{edgartools} \citep{gunning_edgartools} and a custom scraper to download these attachments, followed by Arelle \citep{Arelle_GitHub} to parse them into JSON. 
We then aggregated the per-document hierarchies to build two ``master'' taxonomies ($P_{\text{master}}$): 
\[
P_{\text{master}}(t) = \arg\max_{p \in \mathcal{P}(t)} \text{count}(p, t),
\]
where \(t\) is a tag, \(\mathcal{P}(t)\) its possible parents, and \(\text{count}(p, t)\) the frequency of each parent--child relation. 
We release both presentation and calculation taxonomies, as well as each document's taxonomy, to facilitate work on conceptual representation.

\paragraph{Recursively Ascending the Hierarchy.}
XBRL tags (e.g., \texttt{us-gaap:LineOfCreditFacility- CurrentBorrowingCapacity} ) are highly specific, complicating cross-company comparisons. 
Starting with over 200K ultra-fine-grained labels we iteratively ascend the taxonomy from leaf to parent to reduce specificity but heighten representation. 
At each step, a leaf node inherits its parent label, merging many uncommon tags into broader buckets (e.g., \texttt{us-gaap:LineOfCreditFacility- CurrentBorrowingCapacity} becomes \texttt{us-gaap:DebtInstrumentLineItems}). 
This bottom-up approach preserves conceptual similarity while enabling more robust classification and clustering. In Algorithm~\ref{alg:taxonomy_collapse} (Appendix~\ref{app:algo}), we show a pseudo-algorithm of the method we employ.

\paragraph{\hifi{} Lite.}
To evaluate how well our methods generalize to the broader financial domain and facilitate further research on a more focused subset, we introduce \hifi{} Lite. 
This dataset was developed in collaboration with an industry expert, who assisted in mapping selected XBRL terms to their corresponding general finance concepts.
By bridging the gap between highly specific XBRL terminology and more commonly used financial language, \hifi{} Lite provides a valuable resource for assessing the applicability of our approach at clustering and classification beyond the specific XBRL tags.
We then process the larger dataset, retaining only snippets where more than 50\% of the entities align with the expert-curated mapping, ensuring relevance and consistency.

\section{Experiments}
We demonstrate the usefulness of our granularity selection method and establish three baselines using different methods, namely text classification, sequence labeling, and structured data extraction with LLMs. 
We report the aggregated macro F1 over the cumulative support.
Cumulative support defined as the total count of included ground truth tags. 
This is to showcase how good the model is at encapsulating a given amount of the dataset, at a given granularity.

\paragraph{Text Classification.}
We define a simple classification task: Predict the first entity's label from the paragraph. 
To enable rapid experimentation with different levels of granularity we use the \texttt{all-MiniLM-L6-v2} model from \texttt{sentence-transformers}~\citep{reimers2019sentencebertsentenceembeddingsusing}
to embed each paragraph and then fine-tune a single classification head for classification
%We train only the classification head 
with Adam~\citep{kingma2017adammethodstochasticoptimization} (\(\text{lr}=1\mathrm{e}{-5}\)) for 20 epochs on~\hifi{}.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.32\linewidth]{pretty_pre_01.png}\hspace{0.01\linewidth}
    \includegraphics[width=0.32\linewidth]{pretty_cal_01.png}\hspace{0.01\linewidth}
    \includegraphics[width=0.32\linewidth]{sequence_labelling_pretty.png}
    \caption{\textbf{Results \hifi{}.} We compare the aggregate average for macro F$_1$ for the presentation layer (left), the calculation layer (middle), and the sequence labeling experiment (right).}
    \label{fig:layer-comparison}
\end{figure*}

\paragraph{Sequence Labeling.}
Next, we consider a token-level prediction task: identify and classify each entity within a paragraph. We use \texttt{bert-base-uncased}~\citep{devlin2019bertpretrainingdeepbidirectional} with a standard token classification head, using the Adam optimizer~\citep{kingma2017adammethodstochasticoptimization}, learning rate of \(\text{lr}=1\mathrm{e}{-5}\), and max.\ 50 epochs on the 1,000 most frequent tags of~\hifi{} with early stopping. 
% We also test a single-step collapsation to reduce label fragmentation. 
Tags outside this set are mapped to a \textit{OOS} (out-of-scope) label.

\begin{comment}
\begin{table}[t]
    \centering
    \begin{tabular}{lcc}
        \toprule
        \textbf{Dataset} & \textbf{Accuracy} & \textbf{Weighted F1} \\
        \midrule
        Pre (Top 1000) & 97.43 & 97.48 \\
        Cal (Top 1000) & 98.94 & 99.05 \\
        %(Top 500) & 99.53 & 87.17 \\
        (Top 1000) & 99.26 & 99.25 \\
        \bottomrule
    \end{tabular}
    \caption{Performance metrics comparing taxonomy configurations: collapsed Presentation (Pre), collapsed Calculation (Cal), and uncollapsed datasets, using the top 500 and 1,000 most common labels. \textcolor{red}{These results are not true}}
    \label{tab:my_label}
\end{table}   
\end{comment}


\paragraph{LLM Based Structured Data Extraction.}
Finally, we use HiFi-KPI Lite to evaluate structured extraction, including tags, dates, currency, and numeric values. We compare three LLMs: NuExtract, Qwen-2.5-14B, and Deepseek-v3.
We provide identical system prompts (see Figure~\ref{fig:prompt-llm}; Appendix \ref{app:system}) and a small number of examples (i.e., 1-shot). 
NuExtract requires a specific JSON template format, so we tailored prompts accordingly.

% To benchmark state of the art LLMs we make use of \hifi{} Lite dataset.
% We test the data extraction task with three models: NuExtract-1.5B, Qwen 2.5-14B-Instruct,and DeepSeek-V3. We use the same system prompt for all models.
% NuExtact is a bit of a special case as it expects a certain format for its setup with a template for the json you want to produce \cite{constantin2024nuextract} which meant we followed the descriped receipe.
% We give the models x few shot examples and gives the model the task of extracting valid json.


\begin{table*}[ht]
    \centering
    \scriptsize
    % \setlength{\tabcolsep}{4pt} % reduce column spacing
    \begin{tabular}{l|rrr|rrr|rrr|rrr|rrr}
        \toprule
                            & \multicolumn{3}{c|}{\textbf{all-MiniLM-L6-v2 (TC)}} &\multicolumn{3}{c|}{\textbf{BERT (SL)}} & \multicolumn{3}{c|}{\textbf{NuExtract-1.5B}} & \multicolumn{3}{c|}{\textbf{Qwen2.5-14B}} & \multicolumn{3}{c}{\textbf{DeepSeek-V3}} \\
        \midrule
                            &P&R&$\mu$F$_1$ & P & R & $\mu$F$_1$ & P & R & $\mu$F$_1$ & P & R & $\mu$F$_1$& P & R & $\mu$F$_1$ \\
        \midrule
        \textbf{Start Date} & \multicolumn{3}{c|}{\cellcolor{gray!25}} & \multicolumn{3}{c|}{\cellcolor{gray!25}} & 0.0 & 0.0 & 0.0   & 55.6 & 52.6 &  54.1    & 68.6 & 66.6 & \textbf{67.6}       \\
        \textbf{End Date}   & \multicolumn{3}{c|}{\cellcolor{gray!25}} & \multicolumn{3}{c|}{\cellcolor{gray!25}}& 0.0 & 0.0 & 0.0   & 66.3 & 62.7 &  64.5   & 77.5 & 75.3 & \textbf{76.4}       \\
        \textbf{Currency}   &  \multicolumn{3}{c|}{\cellcolor{gray!25}} & \multicolumn{3}{c|}{\cellcolor{gray!25}} & 0.0 & 0.0 & 0.0   & 88.7 & 84.0 &  86.3   & 90.4 & 87.8 & \textbf{89.1}       \\
        \textbf{Value}      &  \multicolumn{3}{c|}{\cellcolor{gray!25}} & \multicolumn{3}{c|}{\cellcolor{gray!25}} & 7.1 & 9.2 & 8.0   & 56.5 & 53.4 &  54.9   & 66.5 & 64.5 & \textbf{65.5}       \\
        \textbf{Label}$^{*}$   & 44.2 &25.1 & 56.1$^{*}$   &89.21& 91.8 & \textbf{89.1}$^{*}$    & 0.4 & 0.6 & 0.5$^{*}$   & 63.7 & 60.2 &  49.5$^{*}$   & 67.8 & 65.9 & 46.4$^{*}$       \\
        \midrule
        \textbf{Entity EM} & \multicolumn{3}{c|}{\cellcolor{gray!25}} & \multicolumn{3}{c|}{\cellcolor{gray!25}} & \multicolumn{3}{c|}{0\%} & \multicolumn{3}{c|}{29\%} & \multicolumn{3}{c}{38\%} \\
        \bottomrule
    \end{tabular}
    \caption{\textbf{Results \hifi{} Lite.} We show performance of Text Classification (TC), Sequence Labeling (SL), NuExtract-1.5B, Qwen2.5-14B, and DeepSeek-V3 on each entity label in terms of Precision (P), Recall (R), and micro ($\mu$) F$_1$. For the \texttt{Label} entity (*), we report the macro F$_1$. Additionally, we show accuracy when all entity labels are correct (exact match; EM).}
    % comparison of models: precision/recall and micro F1 (Label, macro F1 over ground truth).}
    \label{tab:comparison}
\end{table*}

% \begin{table*}[ht]
%     \centering
%     \footnotesize
%     \setlength{\tabcolsep}{4pt} % reduce column spacing
%     \begin{tabular}{lcccc}
%         \toprule
%          & \textbf{all-MiniLM-L6-v2} & \textbf{DeepSeek V3} & \textbf{Qwen 2.5} & \textbf{NuExtract} \\
%         \midrule
%         \textbf{Start Date} & --- & 69\%/67\% 0.68 & 56\%/53\% 0.54 & 0\%/0\% 0.00 \\
%         \textbf{End Date}   & --- & 78\%/75\% 0.76 & 66\%/63\% 0.64 & 0\%/0\% 0.00 \\
%         \textbf{Currency}   & --- & 90\%/88\% 0.89 & 89\%/84\% 0.86 & 0\%/0\% 0.00 \\
%         \textbf{Value}      & --- & 66\%/65\% 0.66 & 56\%/53\% 0.55 & 7\%/9\% 0.08 \\
%         \textbf{Label}$^{*}$& 44\%/25\% (0.23) & 68\%/66\% (0.46) & 64\%/60\% (0.50) & 0\%/1\% (0.01) \\
%         \midrule
%         \textbf{Entity level Acc.} & --- & 38\% & 29\% & 0\% \\
%         \bottomrule
%     \end{tabular}
%     \caption{\hifi{} Lite comparison of models: precision/recall and micro F1 (Label, macro F1 over ground truth).}
%     \label{tab:comparison}
% \end{table*}


\begin{comment}
    \begin{table*}[ht]
    \centering
    \small
    \begin{tabular}{lllll}
        \toprule
        &\textbf{all-MiniLM-L6-v2}& \textbf{DeepSeek V3} & \textbf{Qwen 2.5} & \textbf{NuExtract} \\
        \midrule
        \textbf{Start Date} &N/A & 69\% / 67\% 0.68(0.34) & 56\% / 53\% 0.54(0.20) & 0\% / 0\% 0.00 (0.00)\\
        \textbf{End Date} &N/A & 78\% / 75\% 0.76(0.39) & 66\% / 63\% 0.64(0.26) & 0\% / 0\% 0.00 (0.00) \\
        \textbf{Currency} &N/A & 90\% / 88\% 0.89(0.12) & 89\% / 84\% 0.86(0.11) & 0\% / 0\% 0.00 (0.00) \\
        \textbf{Value} &N/A & 66\% / 65\% 0.66(0.64) & 56\% / 53\% 0.55(0.55) & 7\% / 9\% 0.08 (0.08) \\
        \textbf{Label} & 44\% / 25\% 0.56(0.23)& 68\% / 66\% 0.67(0.46) & 64\% / 60\% 0.62(0.50) & 0\% / 1\% 0.01 (0.01) \\
        \midrule
        \textbf{Entity correctly extracted}  &N/A & 38\% & 29\% & 0\% \\
        \bottomrule
    \end{tabular}
    \caption{Comparison of models on the structured data extraction task Precision / recall, micro f1 (Macro F1 over ground truth labels) }
    \label{tab:comparison}
\end{table*}
\end{comment}



\section{Results}
%Our main results are in Table~\ref{tab:comparison}.
%and shows us that the bigger models also corelates with better performance we espeically see that the NuExtract at least with our prompting doesnt seem to grasp the task.

\paragraph{Text Classification.}
Figure~\ref{fig:layer-comparison} shows that more coarse-grained labels of the \texttt{.pre} taxonomy generally boosts macro-F1, as the bottom-up approach reduces label sparsity. 
For \texttt{.cal}, performance saturates quickly, possibly because its hierarchy is less deep. 
Beyond $n =$ 3--4, improvements decrease. 
The model struggles on infrequent tags, showing opportunities to develop stronger methods.

\paragraph{Sequence Labeling.}
Sequence labeling shows significantly higher performance than text classification. %due to the richer context available at the token level. 
We observe the effectiveness of the \texttt{.pre} taxonomy, as the most common special \textit{OOS} (out-of-scope) label has significantly lower support than others, ensuring accurate broader-category classification.
The most common ungrouped tags achieves the highest macro F1, followed by \texttt{.cal}, and then \texttt{.pre}, before all representations converge to the same macro F1 for long-tail labels.
%However, long-tail tags remain problematic. 
%While going up the hierarchy helps, it also reduces granularity.
%Improvements thus appear more gradual than in the classification setting.

\paragraph{LLM-based Extraction.}
Table~\ref{tab:comparison} shows large performance variations. 
Qwen and Deepseek often extract dates, currency and numeric values correctly but struggle more with labels. 
NuExtract generally struggles across the board.
Overall, performance demonstrates the difficulty of extracting full, well-structured records from financial text.
Details about metric calculations in Appendix~\ref{app:calculation}.
%, especially at scale.
%To turn this dataset into a proper benchmark I test the performance of various existing models on the sequence labeling task of doing IOB-tagging of labels. 


%\subsection{A Descriptive Title for What our Methodological Innovation Is}
%\subsection{Hierarchical Modeling of KPI }


%If we look at the actual distribution of the depth levels of 1000 most common tags in figure , we see a very high peak at the 7th depth level both in terms of the label present in the top 100 and the occurrence rate across the dataset. Due to the idea that the parent relationship should be descriptive of the child it would be interesting to have the model first try to classify these more general parent tags and then the model is only able to chose from this parents child concepts.  \todo{Does this actually make sense considering how good the performance already is?}

%\todo{Here are multiple options to go for I like the idea doing a custom loss function the most}

%Therefore I propose a custom loss function that better explains to the model how much of a mistake it makes by punishing it less depending on how far away the node it proposes instead is in the XBRL tree. \todo{I like this idea more and more, but do you think it will fit inside this paper ?}

%Therefore I propose models that first try to classify the XBRL tag at depth of \{2,3,4,5\}. \todo{Is this overkill or just nice to try different stuff? try 6,7 as well?}


%\todo{Potential for better models ?}
%To further advance the field of named entity recognition I present models that outperform the current state of the art on both my dataset and similar datasets furthermore I take inspiration from that of deberta-v3-large as it is one of the best performing models on general language task compared to the model parameter count. 
%In connection with this new dataset I introduce a newly pretrained model on the same dataset, that expands upon the work of \cite{loukas-etal-2022-finer} and pretrains a mdoel on the dataset

%\subsection{Experimental Setup}
%I provide various novel models, I attempt to utilize the taxonomy of XBRL data to with my hierachical model as we can also see that a lot of the times that the model makes mistake it is actually close in taxonomy. 


%\subsection{Entity standardization}
%experiments with pretraining with different kind of number embedding was investigated on sec fillings in \cite{loukas-etal-2022-finer}.
%However one method that isnt explored that has previously showed promising results is the one presented in \cite{zhangetal2020languageembedding}, Therefore I apply the same data transformation to my data and try to finetune the Numbert model that closely matches the setup and architecture of the BERT model except for the pretraining on augmented data. 


%\section{Results}
%\todo{Meta: Ca. 0.5-1 pages of results and tables}

%To see how these results generalises to the finance domain I have performed some qualative testing on conference call transcripts as provided by finacial modelling API \cite{financialmodelingprep}. \textcolor{red}{update with proper citation practice for websites} 

%\textcolor{red}{find some nice examples that shows how much more my model picks up on conference calls}

%\section{Out-of-Domain Experiments and Qualitative Analysis}
%I.e. on a handful of earnings transcripts

\section{Discussion}
\paragraph{\hifi{} Dataset.}
The results suggest that the presentation layer provides greater flexibility in its ability to present finer and more coarse-grained labels, while also being an easier target to predict. 
%However, a caveat is the inherent complexity of financial reporting and SEC filings are primarily designed to meet regulatory requirements for public companies \cite{deloitte_xbrl_2023}.
The results also suggest that higher support correlates with higher performance.
Lastly performance of the text classification task is significantly lower than for sequence labeling, indicating that fine-tuning the BERT model for sequence labeling improves extractions, over training only the classification head for text classification. %meaning that the latter approach is a more suitable task for classifying the entity label. 

%Besides that fine-tuning a BERT model allows greater adaption to the finance domain than using the frozen embeddings and only training the classification head.
%Besides that it would seem that fine-tuning a
% meaning that Fine-tuning a BERT model outperform only training the classification head. 
% This suggest a complex task and showcase the need for a larger network that can specifically tune itself for the finance domain.

%One improvement that could have potentially significantly improved the model would have been accounting for what the taxonomies describe, in particular the calculation layer, applying the already defined underlying mathematical relationships should improve model interpretability as the number would then better match its actual context.
\paragraph{\hifi{} Lite.}
Results on \hifi{} Lite clearly shows that bigger models are better at the structured JSON generation task, highlighting the need for reasoning and understanding of the domain.
NuExtract, though designed for JSON extraction, often failed in our setup, likely due to its templated approach that seems to prioritize exact text extraction over following our label set.
However, the NuExtract model always uses the date format it has been fine-tuned on and not specified by the system prompt or dataset.
Finally, our fine-tuned smaller embedding models, especially sequence labeling, clearly outperform the Few-shot LLM-based models in label extraction, emphasizing the value of domain specific data and tuning.
%This paper highlights the challenge of structurally normalizing XBRL tags, despite XBRL's promise of standardization and machine readability \citep{SEC_InlineXBRL}.
%We make significant progress toward a more normalized representation, and establish benchmarks for future research.
%This paper makes clear that even with the use of advanced modeling and assumptions in practice the open standard still makes data analysis extremely hard and open for interpretation and different methodologies.
\section{Conclusion}

%Automated financial analysis and KPI extraction of SEC filings is challenging especially due to the ultra-fine-grained labels and the inconsistent use of (i)XBRL. 
Our \hifi{} dataset takes a significant step towards structuring the iXBRL label set, making possible different degrees of granularity in predictions. 
Our \hifi{} dataset demonstrates the potential of parsing iXBRL to provide contextual details for labels, including temporal, currency, and numeric value relations. 
Experiments on the expert-labeled \hifi{} Lite subset suggest that few-shot prompted generative LLMs can extract this context and generate structured JSON, though fine-tuned encoder-based models achieve better performance. 
Our results highlight a promising approach to structuring the taxonomy, with \hifi{} Lite highlighting the potential for even better aggregation algorithms.


%Our \hifi{} dataset showcase the potential in parsing the iXBRL format providing important context-details for labels with temporal, currency, numeric value relation. 
%Our experiments on the expert labeled subset \hifi{} Lite shows potential for few-shot prompted auto generative LLMs to extract this context and generate JSON-formated out of the unstructured text, however it seems that better performance can be achived with finetuned encoder-based models.
%Our results show a promising method for structuring the taxonomy, while the \hifi{} Lite highlights how there is still potential for making an even nicer more complex aggregate algorithm.
% maybe needs something more concluding.
\section*{Limitations}
A limitation of our experiments is that the annotation quality may vary throughout our dataset, evidenced by the fact that the SEC regularly publishes Data Quality Reminders. 
For instance, the SEC has noted that some filers use different labels for the same element on income statements across periods \cite{sec_changing_labels_2023} or don't report the most fundamental key figure earnings per share correctly \cite{sec_eps_tagging_2024}.
Lastly there is a bias in the dataset created as the text snippets in the dataset consists only of the spans in these documents, and we only include snippets that match our simple parser methodology.
\section*{Ethical Considerations}
Our work adheres to the ACL Code of Ethics. 
We do not anticipate any ethical concerns arising from the development and release of our dataset.

%Our work lays a foundation for refining taxonomies and improving structured extraction models, advancing scalable and context-aware financial data extraction.



%This means that trying to simplify this label space to a smaller, more meaningful general set comes with issues, as the use case for some of these XBRL tax is not financial analysis and definitely not to extract them with machine learning.
%\paragraph{Limitations of Label Collapsing}
%What makes the task even harder for an NLP model is that when we collapse the labelset into their parents we risk losing a lot of information. 
%Our methodology for clustering the labelspace does not acount for what the calculation layer really describes.
%We simply convert the same number that has a mathematical relation to other number into the other number without doing the math specified. 
% The same issue is present with the presentation taxonomy assuming that because a child belong to a certain kind of representation then the child simply is that presentation has major limitations.

%The original intent being to read it as part of the whole regulatory document.
%\paragraph{Snippet length and simplicity of parser}
%Another reminder is about companies not reporting probably the most fundamental and important key figure earnings per share \cite{sec_eps_tagging_2024}.  
%A notable challenge with iXBRL as a data format in the financial domain is the variability in annotation quality.
%\paragraph{Lite set}
%A similar issue arises in our Lite set simplification. In some cases, there is a clear reason why "us-gaap:Revenues" was not used directly, while in others, no strong justification exists. This variability reflects the complexities of financial reporting and the nuances in how financial terms are applied.




\section*{Acknowledgments}


% Innovation Fund Denmark
% Reviewer comments
% Anyone in AAU-NLP in particular
Rasmus Aavang is supported by the Industrial Ph.D. programme from Innovation Fund Denmark (grant code 4297-00016B).
We want to also thank Alipes ApS for their support in facilitating and funding this research and the useful discussions with their NLP team.
MZ and JB are supported by the research grant (VIL57392) from VILLUM FONDEN.
We further thank the AAU-NLP group for useful discussions and feedback on an earlier version of this work.



% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{custom}
\clearpage
\appendix

\section{Granularity Selection Algorithm}\label{app:algo}
\begin{algorithm}[h!]
\caption{Taxonomy-Based Grouping via Bottom-Up Selection}
\label{alg:taxonomy_collapse}
\DontPrintSemicolon
\KwIn{
  \begin{itemize}
    \item A complete hierarchical taxonomy $T$, potentially with up to 218{,}126 labels.
    \item Number of collapse steps $n$ (granularity level).
  \end{itemize}
}
\KwOut{
  \begin{itemize}
    \item A collapsed taxonomy $T'$ with fewer (or higher-level) nodes.
  \end{itemize}
}

\textbf{Step 1: Visualize and Analyze the Hierarchy.}\\
    Generate a treemap of the top 10,000 most common labels (and their ancestors) for both the presentation and calculation taxonomies. 
    Observe that the labels are often deeply nested (especially in the presentation taxonomy), with minimal sibling nodes.

\textbf{Step 2: Define the Collapse Strategy.}\\
    Given the depth-first structure, a bottom-up approach is most meaningful:
    \begin{enumerate}
      \item Identify all leaf nodes in $T$.
      \item Replace each leaf node with its parent node representation.
      \item Repeat for $n$ iterations (or until root node is reached).
    \end{enumerate}

\textbf{Step 3: Iterative Collapsing.}\\
    \For{$i = 1$ to $n$}{
        \begin{enumerate}
          \item Let $L$ be the set of all leaf nodes in $T$. 
          \item \ForEach{leaf $l \in L$}{
            Let $p$ be the parent of $l$.\\
            Replace $l$ with $p$ in $T$.
          }
        \end{enumerate}
    }

\textbf{Step 4: Output the Collapsed Taxonomy.}\\
    The resulting taxonomy $T'$ is now grouped at a higher level of granularity. 
    For example, \texttt{us-gaap:Revenues} is collapsed into \texttt{us-gaap:RevenuesAbstract} (along with potentially 20 other tags sharing the same location in the hierarchy).

\Return{$T'$}

\end{algorithm}


\section{Compute}
The compute we inference the models on are AMD Radeon Instinct MI250X GPUs and it took a total of 8 GPU hours in total for NuExtract-1.5B and Qwen2.5-14B. For DeepSeek-V3, we made use of the Together AI\footnote{See \url{https://www.together.ai/}.} inference API and it costed 2 USD on \hifi{} Lite.
For the text classification task we use a Nvidia GTX 1080 TI for 10 hours in total to run all the different layers labels.
For the sequence labeling we use a Nvidia L40s for around 288 hours total.

\section{Full result collapsing}
More detailed results from running the text classification task on \hifi{}
\begin{table}[h]
    \centering
    \resizebox{\columnwidth}{!}{%
    \begin{tabular}{c|c|cc|cc}
        \toprule
        Times & Unique  & \multicolumn{2}{c|}{Validation} & \multicolumn{2}{c}{Test} \\
        Collapsed & Train Labels & Accuracy & F1 (macro) & Accuracy & F1 (macro) \\
        \midrule
        1  & 8245  & 0.4849 & 0.0082 & 0.4603 & 0.0075 \\
        2  & 6624  & 0.5234 & 0.0110 & 0.4984 & 0.0097 \\
        3  & 2241  & 0.5467 & 0.0232 & 0.5156 & 0.0207 \\
        4  & 1571  & 0.5645 & 0.0368 & 0.5296 & 0.0315 \\
        5  & 1402  & 0.5688 & 0.0465 & 0.5341 & 0.0384 \\
        6  & 1326  & 0.5956 & 0.0494 & 0.5575 & 0.0400 \\
        7  & 1290  & 0.6161 & 0.0404 & 0.5740 & 0.0315 \\
        8  & 1275  & 0.6120 & 0.0317 & 0.5713 & 0.0245 \\
        9  & 1270  & 0.6253 & 0.0282 & 0.5882 & 0.0218 \\
        10 & 1266  & 0.6321 & 0.0249 & 0.5984 & 0.0194 \\
        \bottomrule
    \end{tabular}%
    }
    \caption{Performance metrics across different levels of hierarchical collapsing of the presentation layer}
    \label{tab:collapsed_performance}
\end{table}

\begin{table}[h]
    \centering
    \resizebox{\columnwidth}{!}{%
    \begin{tabular}{c|c|cc|cc}
        \toprule
        Times & Unique  & \multicolumn{2}{c|}{Validation} & \multicolumn{2}{c}{Test} \\
        Collapsed & Train Labels & Accuracy & F1 (macro) & Accuracy & F1 (macro) \\
        \midrule
        1  & 93,175  & 0.2471 &  0.0009 & 0.2466 & 0.0009 \\
        2  & 91,779  & 0.3002 & 0.0009 & 0.2979 & 0.0009 \\
        3  & 91,387  & 0.3253 & 0.0008 & 0.3238 & 0.0008 \\
        4  & 91,303  & 0.3469 & 0.0008 & 0.3500 & 0.0007 \\
        5  & 91,267  & 0.3558 & 0.0007 & 0.3595 & 0.0007 \\
        6  & 91,253  & 0.3668 & 0.0007 &  0.3734 & 0.0007 \\
        7  & 91,247  & 0.3764 & 0.0007 & 0.3841 & 0.0006 \\
        8  & 91,242  & 0.3772 & 0.0006 & 0.3859 &  0.0006 \\
        9  & 91,237  & 0.3800 &  0.0006 & 0.3880 & 0.0006 \\
        10 & 91,235  & 0.4101 & 0.0006 & 0.4159 & 0.0005 \\
        \bottomrule
    \end{tabular}%
    }
    \caption{Performance metrics across different levels of hierarchical collapsing of the calculation layer}
    \label{tab:collapsed_performance}
\end{table}

\section{Example of part of the treemap}
\begin{figure}[H]
    \centering
    \rotatebox{90}{ % Rotates the figure by 90 degrees
        \includegraphics[width=1\linewidth]{treemap.png} % Adjust width if needed
    }
    \caption{Tree map from us-gaap:RevnuesAbstract and down}
    \label{fig:treemap}
\end{figure}


\begin{comment}
 \begin{table*}sla

    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{c|c|c|c}
    model & f1 & f1 on \cite{loukas-etal-2022-finer} & f1 on  \cite{sharma-etal-2023-financial}  \\ \hline
        BERT-Cased & 0.7963 & 0.6879 & dummy \\
        SEC-BERT-base & 0.8033 & 0.6976 & dummy \\
        FinBERT & 0.7933 &  0.6864 & dummy  \\
        FinBERT \cite{yang2020finbert} & 0.7968 & 0.6864 & dummy\\
    \end{tabular}
    }
    \caption{
    \textcolor{red}{Needs to be redone or changed?}
    results finetuned on max 50 epochs with early stopping set with a patience of 2 these results are correct but sadly on an older version fo the dataset
    \cite{seqeval}}
    \label{tab:baseline}
\end{table*}   
\end{comment}


\newpage
\section{Data Example}
\label{app:data-example}

\begin{lstlisting}[language=json, caption={Example entry from the dataset. The start date marks the beginning of the period to which the value corresponds, and the end date marks its conclusion.}, label={lst:dataset-example}]
"form_type": "10-K",
"accession_number":"0001018840-24-000019",
"filing_date": 1711991312000,
"quarter_ending": "20240203",
"company_name": "ABERCROMBIE & FITCH CO /DE/",
"text": "Includes the U.S., Canada, and Latin America. Net sales in the U.S. were $3.3 billion, $2.8 billion, and $2.7 billion in Fiscal 2023, Fiscal 2022, and Fiscal 2021, respectively.",
"entities": [
{
  "Start character": 74,
  "End character": 77,
  "Label":"us-gaap:Revenues", 
  "Start date for period":"2023-01-29",
  "End date for period":"2024-02-03",
  "Currency / Unit": "USD",
  "Value": 3300000000.0
},
{
  "Start character": 88,
  "End character": 91,
  "Label": "us-gaap:Revenues",
  "Start date for period": "2022-01-30",
  "End date for period": "2023-01-28",
  "Currency / Unit": "USD",
  "Value": 2800000000.0
},
{
  "Start character": 106,
  "End character": 109,
  "Label":"us-gaap:Revenues",
  "Start date for period":"2021-01-31",
  "End date for period":"2022-01-29",
  "Currency / Unit":"USD",
  "Value": 2700000000.0
}]
\end{lstlisting}
\section{Elaboration on Metric Calculation, Defining Precision, Recall, Micro F1 and Macro F1}
\label{app:calculation}
For the \hifi{} Lite set, we define precision, recall, micro F1, and macro F1 using an adapted approach, as generative LLM predictions are unrestricted. 
A misclassified prediction is counted as a false negative for the true label and a false positive for the predicted label. 
A correct prediction is counted as a true positive. 
Using these definitions, we compute micro F1 as in any standard classification task.
For macro F1, we take the average F1 score of only the ground truth labels, excluding labels that appear solely in the predicted set
For the \hifi{} dataset, we compute the cumulative sum by iterating over the label distribution from the most frequent to the least frequent label in the test set. We then calculate the macro-average F1 score for the top x included labels.

\section{Data Split Expanded}
Since the cutoff dates for the dataset has a short window for test and validation a lot of companies only reports a 10-K in either the validation split or test split. 
We make the validation more representative by using company-specific cutoff dates, still following a temporal split striving to have as close as possible to 50\% in each set, if only datapoints is present, the choice is random. 
This approach leads to 89.88\% of companies in the validation set also appearing in the test set.
Finally, companies with their first filing after 2023-10-31 are assigned to the test set to better evaluate generalization to previously completely unseen domains.

\clearpage
\section{System prompt}
\begin{figure*}[h!]
\centering
\begin{tcolorbox}[title=System Prompt, promptstyle]
\lstset{
    basicstyle=\normalfont\sffamily\tiny,
    breaklines=true,
    frame=none,
    columns=fullflexible,
}
\begin{lstlisting}[linewidth=\linewidth]
###################
### System Prompt ###
###################

You are an expert data extraction assistant. Your task is to read a given text and extract financial or entity-related information. For each entity found in the text, extract:

        - **value**: numerical representation;
        - **currency_/_unit**: currency most often USD, shares, EUR, CAD, etc.;
        - **label**: revenues, earnings, eps, ebit, or XBRL-OOS (if it is none of the others);
        - **start_date_for_period**: (if available)
        - **end_date_for_period**: (if available)


        If no relevant data is found, return an empty list. Otherwise, return a json line of one or more dictionaries after the "entities" key, each containing these fields exactly:

        [
            {
                "entities": [
                    {
                    "label": "<extracted label>",
                    "start_date_for_period": "<YYYY-MM-DD>",
                    "end_date_for_period": "<YYYY-MM-DD>",
                    "currency_/_unit": "<unit or currency>",
                    "value": <numeric value>
                    }
                ]
            }
        ]

        No additional commentary or text should be included, only valid JSON.

        Example:

        Text: The Company has incurred losses since 2008 resulting from a combination of: declining net interest income, as our loan portfolio decreased from $109.8 million at December 31, 2008 to $62.3 million at December 31, 2016; increased provisions for loan losses between 2009 and 2012; and increasing non-interest expense related to professional fees and repossessed asset write-downs and costs. The Company recently incurred net losses of $866 for the nine months ended September 30, 2017 and $1,260 during the year ended December 31, 2016.  Our interest income for the nine months ended September 30, 2017 has increased with the increase in the balance of our loan portfolio, however, this growth has also resulted in an increase to our provision for loan losses.  Our non-interest expense has also increased for compensation and occupancy cost and includes costs for problem asset resolution at the beginning of the year. The loss for 2016 was largely a result of our net interest income reflecting the low balance of our loan portfolio, increasing professional fees for problem asset resolution and additional costs associated with operating as a public company. Non-interest expense for 2016 was also impacted by an operational loss not reimbursable from our insurance.
        
        [
            {
                "entities": [
                    {
                        "label": "XBRL-OOS",
                        "start_date_for_period": "2008-12-31",
                        "end_date_for_period": "2008-12-31",
                        "currency_/_unit": "USD",
                        "value": 109800000.0
                    },
                    {
                        "label": "XBRL-OOS",
                        "start_date_for_period": "2016-12-31",
                        "end_date_for_period": "2016-12-31",
                        "currency_/_unit": "USD",
                        "value": 62300000.0
                    },
                    {
                        "label": "earnings",
                        "start_date_for_period": "2017-01-01",
                        "end_date_for_period": "2017-09-30",
                        "currency_/_unit": "USD",
                        "value": -866000.0
                    },
                    {
                        "label": "earnings",
                        "start_date_for_period": "2016-01-01",
                        "end_date_for_period": "2016-12-31",
                        "currency_/_unit": "USD",
                        "value": -1260000.0
                    }
                ]
            }
        ]


\end{lstlisting}
\end{tcolorbox}
    \caption{\textbf{System Prompt}}
    \label{fig:prompt-llm}
\end{figure*}
\label{app:system}
%\newpage
\twocolumn[
\section{Finance Expert Handpicked Labels and Their Meaning}
\vspace{1em} % Add slight spacing to separate title from table
]
\begin{table}[]
    \centering
    \small
    \resizebox{\textwidth}{!}{ % Scale to fit page width
    \begin{tabular}{ll}
        \toprule
        \textbf{Label} & \textbf{Category} \\
        \midrule
        us-gaap:IncomeLossAttributableToParent & Earnings \\
        us-gaap:IncomeLossFromContinuingOperations & Earnings \\
        us-gaap:IncomeLossFromContinuingOperationsBeforeIncomeTaxesExtraordinaryItemsNoncontrollingInterest & Earnings \\
        us-gaap:IncomeLossFromContinuingOperationsBeforeIncomeTaxesMinorityInterestAndIncomeLossFromEquityMethodInvestments & Earnings \\
        us-gaap:NetIncomeLoss & Earnings \\
        us-gaap:NetIncomeLossAvailableToCommonStockholdersBasic & Earnings \\
        us-gaap:OperatingIncomeLoss & EBIT \\
        bw:IncrementalCommonSharesAttributableToDilutiveEffectOfNetIncome & EPS \\
        cmtl:WeightedAveragePerformanceSharesOutstandingDuringThePeriodThatAreExcludedfromEPSCalculation & EPS \\
        enb:WeightedAverageInterestInOwnCommonShares & EPS \\
        fcx:DilutiveSecuritiesExcludedfromComputationofEPSAmount & EPS \\
        gpmt:AntidilutiveSecuritiesExcludedfromComputationofEarningsPerShareInterestExpense & EPS \\
        gs:ImpactOfUnvestedShareBasedPaymentAwardsAsSeparateClassOfSecuritiesOnEarningsPerShareBasic & EPS \\
        land:WeightedAverageNumberOfOperatingPartnershipUnitsHeldByNoncontrollingInterest & EPS \\
        pcg:PlanOfReorganizationBackstopCommitmentPremiumCommonStockShares & EPS \\
        us-gaap:DistributedEarnings & EPS \\
        us-gaap:DividendsAndInterestPaid & EPS \\
        us-gaap:EarningsPerShareBasic & EPS \\
        us-gaap:EarningsPerShareBasicAndDiluted & EPS \\
        us-gaap:IncrementalCommonSharesAttributableToConversionOfDebtSecurities & EPS \\
        us-gaap:IncrementalCommonSharesAttributableToParticipatingNonvestedSharesWithNonForfeitableDividendRights & EPS \\
        us-gaap:IncrementalCommonSharesAttributableToShareBasedPaymentArrangements & EPS \\
        us-gaap:ParticipatingSecuritiesDistributedAndUndistributedEarningsLossBasic & EPS \\
        us-gaap:UndistributedEarnings & EPS \\
        us-gaap:WeightedAverageNumberOfSharesContingentlyIssuable & EPS \\
        us-gaap:WeightedAverageNumberOfSharesRestrictedStock & EPS \\
        us-gaap:DirectFinancingLeaseRevenue & Revenues \\
        us-gaap:FeeIncome & Revenues \\
        us-gaap:InsuranceCommissionsAndFees & Revenues \\
        us-gaap:OperatingLeaseLeaseIncome & Revenues \\
        us-gaap:PremiumsEarnedNet & Revenues \\
        us-gaap:Revenues & Revenues \\
        us-gaap:UnregulatedOperatingRevenue & Revenues \\
        us-gaap-supplement:FeeIncome & Revenues \\
        us-gaap-supplement:InterestIncomeOperatingPaidInKind & Revenues \\
        \bottomrule
    \end{tabular}
    }
    \caption{Mapping of XBRL labels to expert labels.}
    \label{tab:label_mapping}
\end{table}


%\begin{figure*}[ht]
%    \centering
%    \includegraphics[width=\linewidth]%{1000_most_common_present.png}
%    \caption{Could be interesting for appendix? as reason for the number of labels chosen 
%    \textcolor{red}{I think I want to remove this TBH the more I think about it}}

 %   \label{fig:most_common_present}
%\end{figure*}
%
%\section{Example Appendix}
%\label{sec:appendix}
%
%This is an appendix.

\end{document}
