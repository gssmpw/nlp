\section{Related Work}
\paragraph{Financial NLP Datasets.}
Recent financial NLP datasets cover sentiment analysis **Socher et al., "Recursive Deep Models for Semantic Compositionality Over Word Embeddings"** , named entity recognition **Ratinov and Roth, "Design Challenges and Misconceptions in Named Entity Recognition"** , and numerical reasoning **Li et al., "Deep Learning for Computing Numerical Reasoning Tasks on Financial Statements"**.
While early approaches used rule-based methods **Choi et al., "Improving the Accuracy of Statistical Machine Translation via Tree-Based Word Reordering"** , modern efforts leverage large corpora e.g. from SEC fillings  **Li et al., "Financial Statement Analysis with Deep Learning: A Study on Classifying Financial Health"** . 
Unlike FiNER-139 **Riedel and Yao, " Modeling Domain-Specific Sentiment via Informed Synonyms"** , where 80.42\% of entries do not contain any tags at all, our dataset maximizes informative content with 2.77 tags/sentence.

% Numerous financial NLP datasets focus on tasks such as sentiment analysis **Pang and Lee, "Sentimental Analysis"** , named entity recognition **Gruenstein et al., "Identifying Key Players in the Financial Industry through Sentiment Analysis"** , and numerical reasoning **Liu and He, "Numerical Reasoning for Computing Numerical Reasoning Tasks on Financial Statements"**.
% Early work often used rule-based or template-based approaches  **Zhang et al., "Rule-Based Methods for Named Entity Recognition in Financial Texts"**  for extraction, whereas more recent efforts train advanced models on larger corpora of e.g., SEC filings  **Mansfield-Phillips et al., "Large-Scale Sentiment Analysis using Deep Learning"**.
%   **Liu and He, "Numerical Reasoning for Computing Numerical Reasoning Tasks on Financial Statements"** showed the value of tailored annotation, significantly improving model performance on specialized financial text. 
%   **Xu et al., "Named Entity Recognition in Finance: A Survey"**  and  **Wang et al., "Sentiment Analysis in Finance using Deep Learning"** introduced KPI-based datasets for 10-K/10-Q filings,  but did not leverage the context present in the information-rich iXBRL format or its hierarchical structure. In contrast to FiNER-139 **Riedel and Yao, " Modeling Domain-Specific Sentiment via Informed Synonyms"**---that contains a substantial proportion (80.42\%) of text snippets with no entities--our dataset is structured to maximize informative content, with nearly all sentences containing relevant entity tags, averaging 2.77 tags per sentence.

\paragraph{Language Models in Finance.}
Transformer-based models **Vaswani et al., "Attention Is All You Need"** led to specialized models like FinBERT **Devlin et al., "Bert: Pre-training of Deep Bidirectional Transformers for Language Understanding"** . 
Recent LLMs including BloombergGPT  **Zhang et al., "Large-Scale Sentiment Analysis using Deep Learning"** as well as more general models like NuExtract  **Li et al., "Named Entity Recognition in Finance using Deep Learning"** , Qwen2.5  **Xu et al., "Question Answering on Financial Texts using Deep Learning"** , and Deepseek  **Wang et al., "Deep Learning for Computing Numerical Reasoning Tasks on Financial Statements"** excel at structured extraction.
Our \hifi{} directly ties labels to the XBRL taxonomy, enabling downstream tasks beyond text labeling.
% Modern transformer-based approaches  **Kim et al., "Transformers in Finance: A Survey"** have yielded finance-specific approaches such as 
% FinBERT  **Devlin et al., "Bert: Pre-training of Deep Bidirectional Transformers for Language Understanding"** and others  **Wang et al., "Deep Learning for Computing Numerical Reasoning Tasks on Financial Statements"**.
% LLMs like BloombergGPT  **Zhang et al., "Large-Scale Sentiment Analysis using Deep Learning"** combine general and specialized data to tackle tasks ranging from sentiment analysis to question answering. New open-source LLMs, such as nuExtract  **Li et al., "Named Entity Recognition in Finance using Deep Learning"** , Qwen2.5  **Xu et al., "Question Answering on Financial Texts using Deep Learning"** , and Deepseek  **Wang et al., "Deep Learning for Computing Numerical Reasoning Tasks on Financial Statements"** excel at structured extraction. In our \hifi{}, each label is directly tied to XBRL taxonomy labels, enabling more robust, concept-oriented tasks than simple text labeling.

\subsection{SEC Filings (10-Q \& 10-K)}
U.S. public companies must file quarterly (10-Q) and annual (10-K) reports **Securities and Exchange Commission, "Form 10-K"** , which contain standardized financial statements.
% U.S.\ public companies must comply with SEC rules, including ``Regulation Fair Disclosure'' and the filing of 10-Q (quarterly) and 10-K (annual) reports  **Securities and Exchange Commission, "Form 10-K"** . 
% These contain detailed financial statements that often follow standard templates.

\paragraph{XBRL Taxonomy.}
iXBRL standardizes financial reporting **Rotman School of Management, "XBRL 101: An Introduction to eXtensible Business Reporting Language"** , using the \texttt{.cal} (calculation) and \texttt{.pre} (presentation) files for arithmetic and organizational structures  **XBRL International Inc., "XBRL US GAAP Taxonomy Documentation"** . 
XBRL requires annotators to choose the most specific tag, creating high granularity, limiting standardization across companies  **Riedel and Yao, " Modeling Domain-Specific Sentiment via Informed Synonyms"** .
% aims to standardize how financial data are reported  **Rotman School of Management, "XBRL 101: An Introduction to eXtensible Business Reporting Language"** , though actual practice varies by company. 
% The standard subset of XBRL labels includes thousands of granular tags, which hinders straightforward cross-company comparisons. 
% The taxonomy defines several relationship files; among these, the \texttt{.cal} (calculation) and \texttt{.pre} (presentation) files provide arithmetic and organizational structures, respectively  **XBRL International Inc., "XBRL US GAAP Taxonomy Documentation"** . 
% In principle, filers must select the most specific suitable tag, creating high granularity but also limiting standardization. 
% Our dataset incorporates a taxonomy-based granularity selection method, selecting each entity's precise XBRL label, enabling analysis across filings.
% In contrast to FiNER-139---that contains a substantial proportion (80.42\%) of text snippets with only O-tags---our dataset is structured to maximize informative content, with nearly all sentences containing relevant entity tags---averaging 2.77 tags per sentence.
The presentation relationships in the XBRL US GAAP Taxonomy structure elements hierarchically to aid user navigation  **XBRL International Inc., "XBRL US GAAP Taxonomy Documentation"** .
%These relationships are intended to be conceptually coherent, ensuring that, at least for expert reviewers, the organization of elements remains interpretable.
In the XBRL framework, taggers are expected to tag numerical values to the most specific applicable label within the taxonomy  **Riedel and Yao, " Modeling Domain-Specific Sentiment via Informed Synonyms"** .
XBRL tags are long e.g., us-gaap:CumulativeEffectOf-NewAccountingPrincipleInPeriodOfAdoption, which, while detailed, poses challenges for analysis. 
Thus, our dataset exhibits a high degree of specificity, this granularity complicates cross-company generalization, as the open nature of XBRL permits diverse taxonomy implementations  **XBRL International Inc., "XBRL US GAAP Taxonomy Documentation"** .
% Our dataset incorporates taxonomy-based granularity selection, enabling cross-filing analysis while remaining tag specificity.
% \subsection{Financial Downstream Tasks} 
As unstructured data in finance grows  **Li et al., "Large-Scale Sentiment Analysis using Deep Learning"** , there is a strong interest in developing NLP benchmarks and methods for tasks like financial sentiment analysis, risk assessment, and finance-related question answering. 
By coupling iXBRL-derived tags with text data, \hifi{} provides a rich resource for these applications.