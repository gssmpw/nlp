\section{Related Work}
\paragraph{Financial NLP Datasets.}
Recent financial NLP datasets cover sentiment analysis~\citep{gupta2020comprehensive}, named entity recognition~\citep{alvarado2015domain, shah2022fluemeetsflangbenchmarks}, and numerical reasoning~\citep{chen2022finqadatasetnumericalreasoning}.
While early approaches used rule-based methods~\citep{extraction2007, sheikh2012rule, hutto2014vader}, modern efforts leverage large corpora e.g. from SEC fillings ~\citep{loukas2021edgar}. 
Unlike FiNER-139~\citep{loukas-etal-2022-finer}, where 80.42\% of entries do not contain any tags at all, our dataset maximizes informative content with 2.77 tags/sentence.

% Numerous financial NLP datasets focus on tasks such as sentiment analysis~\citep{gupta2020comprehensive}, named entity recognition~\citep{alvarado2015domain, shah2022fluemeetsflangbenchmarks}, and numerical reasoning~\citep{chen2022finqadatasetnumericalreasoning}.
% Early work often used rule-based or template-based approaches~\citep{extraction2007, sheikh2012rule, hutto2014vader} for extraction, whereas more recent efforts train advanced models on larger corpora of e.g., SEC filings~\citep{loukas2021edgar}.
% \citet{alvarado2015domain} showed the value of tailored annotation, significantly improving model performance on specialized financial text. 
% \citet{loukas-etal-2022-finer} and \citet{sharma-etal-2023-financial} introduced KPI-based datasets for 10-K/10-Q filings,  but did not leverage the context present in the information-rich iXBRL format or its hierarchical structure. In contrast to FiNER-139~\citep{loukas-etal-2022-finer}---that contains a substantial proportion (80.42\%) of text snippets with no entities--our dataset is structured to maximize informative content, with nearly all sentences containing relevant entity tags, averaging 2.77 tags per sentence.

\paragraph{Language Models in Finance.}
Transformer-based models~\citep{vaswani2017attention} led to specialized models like FinBERT~\citep{yang2020finbert}. 
Recent LLMs including BloombergGPT~\cite{wu2023bloomberggpt} as well as more general models like NuExtract~\citep{cripwell2024nuextract}, Qwen2.5~\citep{qwen2025qwen25technicalreport}, and Deepseek~\citep{deepseekai2024deepseekv3technicalreport} excel at structured extraction.
Our \hifi{} directly ties labels to the XBRL taxonomy, enabling downstream tasks beyond text labeling.
% Modern transformer-based approaches~\citep{vaswani2017attention} have yielded finance-specific approaches such as 
% FinBERT~\citep{yang2020finbert} and others~\citep{araci2019finbertfinancialsentimentanalysis}.
% LLMs like BloombergGPT~\cite{wu2023bloomberggpt} combine general and specialized data to tackle tasks ranging from sentiment analysis to question answering. New open-source LLMs, such as nuExtract~\citep{cripwell2024nuextract}, Qwen2.5~\citep{qwen2025qwen25technicalreport}, and Deepseek~\citep{deepseekai2024deepseekv3technicalreport} excel at structured extraction. In our \hifi{}, each label is directly tied to XBRL taxonomy labels, enabling more robust, concept-oriented tasks than simple text labeling.

\subsection{SEC Filings (10-Q \& 10-K)}
U.S. public companies must file quarterly (10-Q) and annual (10-K) reports~\citep{SECFinalRule2000,sec_exchange_2024}, which contain standardized financial statements.
% U.S.\ public companies must comply with SEC rules, including ``Regulation Fair Disclosure'' and the filing of 10-Q (quarterly) and 10-K (annual) reports~\citep{SECFinalRule2000,sec_exchange_2024}. 
% These contain detailed financial statements that often follow standard templates.

\paragraph{XBRL Taxonomy.}
iXBRL standardizes financial reporting~\citep{intrinio_xbrl}, using the \texttt{.cal} (calculation) and \texttt{.pre} (presentation) files for arithmetic and organizational structures~\citep{XBRLTaxonomies,XBRL_Presentation,PreparersGuide}. 
XBRL requires annotators to choose the most specific tag, creating high granularity, limiting standardization across companies~\citep{what_intrinio_xbrl}.
% aims to standardize how financial data are reported~\citep{intrinio_xbrl}, though actual practice varies by company. 
% The standard subset of XBRL labels includes thousands of granular tags, which hinders straightforward cross-company comparisons. 
% The taxonomy defines several relationship files; among these, the \texttt{.cal} (calculation) and \texttt{.pre} (presentation) files provide arithmetic and organizational structures, respectively~\citep{XBRLTaxonomies,XBRL_Presentation,PreparersGuide}. 
% In principle, filers must select the most specific suitable tag, creating high granularity but also limiting standardization. 
% Our dataset incorporates a taxonomy-based granularity selection method, selecting each entity's precise XBRL label, enabling analysis across filings.
% In contrast to FiNER-139---that contains a substantial proportion (80.42\%) of text snippets with only O-tags---our dataset is structured to maximize informative content, with nearly all sentences containing relevant entity tags---averaging 2.77 tags per sentence.
The presentation relationships in the XBRL US GAAP Taxonomy structure elements hierarchically to aid user navigation \cite{PreparersGuide}.
%These relationships are intended to be conceptually coherent, ensuring that, at least for expert reviewers, the organization of elements remains interpretable.
In the XBRL framework, taggers are expected to tag numerical values to the most specific applicable label within the taxonomy \cite{PreparersGuide}.
XBRL tags are long e.g., us-gaap:CumulativeEffectOf-NewAccountingPrincipleInPeriodOfAdoption, which, while detailed, poses challenges for analysis. 
Thus, our dataset exhibits a high degree of specificity, this granularity complicates cross-company generalization, as the open nature of XBRL permits diverse taxonomy implementations~\citep{what_intrinio_xbrl}.
% Our dataset incorporates taxonomy-based granularity selection, enabling cross-filing analysis while remaining tag specificity.
% \subsection{Financial Downstream Tasks} 
As unstructured data in finance grows~\cite{doi:10.1080/00014788.2019.1611730}, there is a strong interest in developing NLP benchmarks and methods for tasks like financial sentiment analysis, risk assessment, and finance-related question answering. 
By coupling iXBRL-derived tags with text data, \hifi{} provides a rich resource for these applications.