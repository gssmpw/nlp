\begin{table}[t]
    \centering\footnotesize
    \caption{A summary of the contextual information used during prompting, ordered from the least informative (top) to the most (bottom). Each prompting method provides more contextual information than the one above it. Keywords and summaries were generated using Gemini 1.0 Pro as discussed in Section~\ref{sect:setup}. See Appendix~\ref{sect:app:prompting} for prompt templates.
    }
    \label{tab:context}
\begin{tabularx}{\linewidth}{X|X}
  \toprule
  \bf Abbreviation & \bf Description\\ \midrule
\ttfamily continue (cont)&Simple continuation based on a short prefix (no prompting).\\[0.1cm]
\ttfamily chain-of-thought (cot)&Ask the model to generate an outline before generating the article, using a short prefix.\\[0.1cm]
\ttfamily short keywords (kw)&A few, unordered keywords.\\[0.1cm]
\ttfamily keywords (kw+)&Many unordered keywords.\\[0.1cm]
\ttfamily summary (su)&A summary of the article.\\[0.1cm] 
\ttfamily summary + keywords (su+)&Both a summary and many keywords.\\[0.1cm] 
\ttfamily excerpt (exc)&Ordered list of long excerpts from the original article.\\
  \bottomrule
  \end{tabularx}
\end{table}

Our goal is to investigate when and how LLM-generated texts can vary substantially from natural language in their fractal structure.   For that we need to generate synthetic texts. In order to correctly identify the impact of each factor we consider in our study (e.g. temperature setting, prompting, model size), we generate the data ourselves from scratch. We follow a similar setup to the one used in~\cite{verma-etal-2024-ghostbuster}, in which we restrict analysis to long documents or paragraphs, as opposed to short answers to questions. Similar to~\cite{verma-etal-2024-ghostbuster}, we query a capable LLM via its API, which is always Gemini 1.0 Pro~\citep{geminiteam2024geminifamilyhighlycapable} in our experiments, to generate some contextual information about an article (such as keywords or a summary) before asking another model to write an article based on those contextual information. Since each article is matched with a corresponding human-generated text (ground truth), we name this dataset ``Generated And Grounded Language Examples'' (GAGLE). It contains over 240,000 articles\footnote{The GAGLE dataset is available at: \url{https://huggingface.co/datasets/ibomohsin/gagle}}. 

\begin{figure*}[t]
    \centering
    \includegraphics[width=1.99\columnwidth]{figures/quality_fit.pdf}
    \caption{Quality of power law fit for fractal parameters in both human- and LLM-generated documents, for the same setting as in Appendix~\ref{sect:app:rho_doc_sample} where samples of documents are provided.}
    \label{fig:quality_fit}
\end{figure*}

The contextual information we use were chosen such that they can be \emph{ordered} from the least informative to the most, as shown in Table~\ref{tab:context}. The datasets we use are from five domains: (1) WIKIPEDIA~\citep{wikidump}, (2) BIGPATENT, consisting of over one million records of U.S. patents~\citep{sharma2019bigpatent}, (3) NEWSROOM, containing over one million news articles~\citep{Grusky_2018}, (4) SCIENTIFIC, a collection of research papers obtained from ArXiv and PubMed repositories~\citep{Cohan_2018}, and (5) BILLSUM, containing US Congressional and California state bills~\citep{kornilova2019billsum}. We only use, at most, 1,000 articles from each domain.


In our experiments, we use pretrained models (with simple continuation only) and instruction-tuned models with various prompting strategies as discussed earlier. During text generation, we experiment with three decoding temperatures: $\beta=0$ (greedy decoding), $\beta=0.5$, and $\beta=1$ (pretraining temperature). The three models we use are Gemini 1.0 Pro~\citep{geminiteam2024geminifamilyhighlycapable}, Mistral-7B~\citep{jiang2023mistral7b}, and Gemma-2B~\citep{gemmateam2024gemmaopenmodelsbased}.

Once the texts are generated, we score them using pretrained models. One goal is to identify if our findings remain robust across different scoring models. As mentioned earlier, some previous works suggest that smaller models might be better for scoring and detecting LLM-generated texts~\citep{mireshghallah-etal-2024-smaller} while other works suggest that using the same model for both generation and scoring yields better results~\citep{fagni2021tweepfake,10.5555/3618408.3619446}. 

Finally, once all the log-perplexity scores are calculated, we compute fractal parameters. Because  $\SSS$ and $\HHH$ are exponents of power laws, sufficiently long documents are required. Hence, we  encourage the model to generate long documents in the prompt (see prompting templates in Appendix~\ref{sect:app:prompting}). We drop the first 64 tokens to remove any warm-up effects, and ignore documents that are less than 400 tokens in length. When a document is ignored, we also ignore the corresponding ground-truth document, to remove this confounding effect. Then, we clip all documents (both human- and LLM-generated) to 400 tokens to have equal lengths. We estimate fractal parameters using the scales (time gaps) $\tau\in\{8, 16, 32, 48, 64, 96, 128, 160, 192, 256, 320\}$ with $\epsilon=10^{-2}$; see~\cite{alabdulmohsin2024fractals} for details on how to calculate them. We also use bootstrapping~\citep{efron1994introduction} to estimate confidence intervals by subsampling with replacement 10 independent samples. Figure~\ref{fig:quality_fit} illustrates quality of fit for the setting in Appendix~\ref{sect:app:rho_doc_sample} where samples of documents are provided.

\textbf{Disclaimer.} Our estimates of $\SSS$ and $\HHH$ differ from those in~\cite{alabdulmohsin2024fractals} for two reasons. Because LLM-generated texts are short (typically of about 500 tokens), we restrict the range of the scale term $\tau$ to at most $320$ tokens. Also, we use a slightly larger value of $\epsilon$ because we found that smaller values in short documents lead to high variance. Both imply that our estimates are less accurate. Nonetheless, our goal is to use $\SSS$ and $\HHH$ as statistical \emph{probes} to compare natural texts from LLM-generated, so we use the same hyperparameters for both types of documents.
