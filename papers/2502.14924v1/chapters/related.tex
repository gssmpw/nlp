Several studies have looked into the statistical properties of LLM-generated texts. For instance, \cite{guo2023closechatgpthumanexperts} analyzed ChatGPT's responses in comparison to human-generated text and found that ChatGPT-produced outputs tend to have lower log-perplexity scores. Based on these findings, the authors developed detection systems for identifying LLM-generated content, concluding that short texts are more challenging to detect than longer documents.

The observation that LLM-generated texts exhibit lower log-perplexity scores has been utilized by several other works for detecting such content, including~\cite{solaiman2019releasestrategiessocialimpacts,gehrmann-etal-2019-gltr,ippolito-etal-2020-automatic,vasilatos2023howkgptinvestigatingdetectionchatgptgenerated} and \cite{yang2023dnagptdivergentngramanalysis}, among others. This raises the question of which models are most suitable for evaluating text perplexity. In~\cite{mireshghallah-etal-2024-smaller}, the authors study this issue and suggest that smaller models are more effective! Additionally, \cite{hans2024spottingllmsbinocularszeroshot} proposed normalizing the average log-perplexity score using a cross-PPL metric calculated across two different models before applying a global detection threshold.

However, such approaches, which are based on 1\textsuperscript{st}-order statistics of log-perplexity scores, are not always effective~\citep{hans2024spottingllmsbinocularszeroshot}. For instance, methods, such as DetectGPT, which incorporate 2\textsuperscript{nd}-order information by estimating the curvature of the loss~\citep{10.5555/3618408.3619446}, have been found to yield better results.

Our work contributes to this line of research by focusing primarily on fractal parameters. As argued in~\citep{meister-cotterell-2021-language}, the evaluation of LLMs should go beyond log-perplexity and also consider how well LLMs capture other ``statistical tendencies'' observed in natural language. Our study has a similar goal. We conduct a comprehensive quantitative analysis involving a range of model architectures, decoding temperatures, and prompting methods, and we explore the differences between pretrained and instruction-tuned models as well, among other considerations.

Although our study may have implications for detecting LLM-generated content, detection is \emph{not} the primary focus of this work. It is important to acknowledge, however, that detecting LLM-generated content is critical and has been the subject of several noteworthy studies. These include the perplexity-based detection methods mentioned above, as well as supervised classification approaches~\citep{verma-etal-2024-ghostbuster,pu2022deepfaketextdetectionlimitations,jawahar-etal-2020-automatic,ghosal2023possibilitiesimpossibilitiesaigenerated,10.1145/3624725,dhaini-etal-2023-detecting,guo2023closechatgpthumanexperts}, with fine-tuning of pretrained models being especially effective~\citep{zellers2020defendingneuralfakenews,solaiman2019releasestrategiessocialimpacts,fagni2021tweepfake}. While detecting such content has inherent limitations~\citep{varshney2020limitsdetectingtextgenerated,sadasivan2024aigeneratedtextreliablydetected}---as LLMs are trained to model the full joint distribution of human language---continued progress in this area is essential for mitigating societal risks. These include, but are not limited to, the spread of misinformation~\citep{zellers2020defendingneuralfakenews}, fake online reviews~\citep{10.1145/3133956.3133990}, potentially harmful medical advice~\citep{guo2023closechatgpthumanexperts}, academic dishonesty~\citep{susnjak2022chatgptendonlineexam}, and extremist propaganda~\citep{mcguffie2020radicalizationrisksgpt3advanced}. The importance of these efforts is underscored by incidents where LLM-generated news articles containing factual inaccuracies were published with minimal human oversight~\citep{christian2023cnet}. We hope our work will help along these directions. 

In this work, we examine the impact of various factors, including the model size. Prior literature has consistently shown that larger models tend to perform better, with the benefits of scaling being predictable empirically~\citep{hestness2017deep,kaplan2020scaling,alabdulmohsin2022revisiting,zhai2106scaling}. These improvements are not limited to perplexity scores. For example, \cite{dou-etal-2022-gpt} found that larger models produce texts with fewer factual and coherence-related issues. Not surprisingly, we also find that bigger pretrained models yield fractal parameters that are closer to those of natural language (see Figure~\ref{fig:fractal_param_pt}). 

Additionally, we investigate other factors, such as temperature settings. Previous research has demonstrated that while improved decoding methods may deceive humans, they may also introduce detectable statistical anomalies~\citep{ippolito-etal-2020-automatic}. Our work explores these effects in detail from the lens of fractals, contributing to the broader understanding of how model parameters influence LLM output.

