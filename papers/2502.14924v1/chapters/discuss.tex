% Limitations: fractal parameters are exponents and can only be reliably estimated on a corpus of long documents. We used various models but they are all transformer-based. Our focus is on studying the fractal parameters, not on detection per se. Detection has potential societal implications, e.g. mistaking human texts for machine-generated texts can be quite harmful to the individual. For these reasons, we refrain from making claims about detection ability but note that the success of PPL-based methods might benefit from the incorporation of second-order statistics, such as H and S. 

In this work, we investigate whether LLMs are capable of replicating the fractal structure of language and examine the impact of various parameters such as the model size, decoding temperature, and prompting method. Our findings reveal that for pretrained models, larger architectures are more effective at capturing such fractal properties. In addition, with instruction-tuned models, the similarity to human language does not improve monotonically as the amount of contextual information in the prompt increases. Notably, the Hurst parameter emerged as a strong predictor of quality in generated texts, among other significant findings. To facilitate further research in this area, we release our GAGLE dataset, which comprises over 240,000 LLM-generated articles.

In terms of limitations, estimating fractal parameters requires analyzing large corpora of lengthy documents because these parameters describe properties of the underlying stochastic processes. Therefore, they may not be reliable at making conclusions about \emph{individual} documents or short texts. This limitation prevents us from making  claims about the ability to detect AI-generated content using these metrics alone. However, we note that perplexity-based detection methods might be enhanced by incorporating second-order statistics such as the H\"older and  Hurst exponents, since the range of those parameters in LLM-generated articles varies widely compared to human-generated texts. We leave the exploration of detection strategies that leverage these fractal characteristics for future work.

\section*{Impact Statement}
There are many potential societal consequences of advancing the field of artificial intelligence (AI), both positive, such as improved accessibility to high-quality healthcare and education, and negative, if such systems are misused. The goal in this work is to contribute to the ongoing effort to improve understanding of language models and the mechanisms behind their success. While we recognize the general ethical considerations that accompany the advancement of language models and AI, we do not feel that this particular work raises unique or unaddressed ethical concerns beyond the established considerations within the field.

\section*{Acknowledgement}
We thank Vinh Tran and Jeremiah Harmsen for their insightful reviews and suggestions, Mostafa Dehghani and Mike Mozer for early discussions, 
and Google DeepMind at large for providing a supportive research environment.