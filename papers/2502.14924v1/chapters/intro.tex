The information-theoretic complexity of language (i.e., its bits or "surprise") has been shown to exhibit both self-similarity and long-range dependence (LRD)~\citep{alabdulmohsin2024fractals}. In simple terms, a stochastic process is considered self-similar if its statistical properties remain consistent across different scales, regardless of the level of magnification applied. A well-known example of such behavior is found in Ethernet traffic~\citep{crovella1995explaining,leland1994self,paxson1995wide,willinger1997self}, where self-similarity manifests as burstiness across all time scales, thereby impacting the design of network device buffers~\citep{wilson2004high}. In language, such self-similarity is attributed to its recursive structure~\citep{altmann2012origin,willinger1995self}. On the other hand, a stochastic process is called long-range dependent (LRD) if its future is influenced by the \emph{distant} past, with no particular characteristic context length. 

Self-similarity and long-range dependence (LRD) can be quantified using the H\"older and Hurst exponents, respectively. The H\"older exponent, which we denote by $\SSS$ for self-similarity, characterizes the rate of decay in the autocorrelation function, with \emph{smaller} values of $\SSS$ indicating a \emph{more} significant self-similar structure (heavier tail)~\citep{watkins2019mandelbrot}. By contrast, larger values of the Hurst exponent $\HHH\gg0.5$ indicate more dependence across time~\citep{hurst1951long}. We refer the reader to~\cite{alabdulmohsin2024fractals} for the exact definitions of these quantities.
%In natural language, the H\"older exponent is estimated to be $\SSS = 0.59\pm0.08$, while the Hurst exponent is estimated to be $\HHH=0.70\pm0.09$~\citep{alabdulmohsin2024fractals}. 
A natural question that arises, next, is: How different are $\SSS$ and $\HHH$ in LLM-generated texts from natural language? In this work, we investigate this question in depth, aiming to identify conditions---such as temperature settings, instruction-tuning, prompting and model size---that influence an LLMâ€™s ability to replicate such fractal characteristics.

Before doing that, however, let us consider some arguments for why LLMs may or may not be capable of replicating the fractal structure of language. One argument for why they should be capable of doing so lies in the chain rule of probability. LLMs are reasonably calibrated at the token level~\citep{kadavath2022language}, implying that auto-regressive decoding should theoretically capture the structure of natural language as long as token-level probability scores remain well-calibrated. By the chain rule, each subsequent token's probability is conditioned on the prior tokens, and this process should \emph{ideally} reflect the self-similarity and long-range dependence inherent in language.

However, this idealized view may not hold in practice. A critical issue can arise, for example, from the mismatch between how LLMs are trained and how they are used at inference, as pointed out by~\cite{bachmann2024pitfallsnexttokenprediction}. During training, LLMs use teacher-forcing, where the correct previous tokens are always provided. This ensures that errors do not accumulate during training, but during inference, models must rely on their own predictions, leading potentially to compounding errors that can distort the fractal structure. Formally speaking, whereas LLMs can be well-calibrated at the \emph{next token} level, their ability to accurately predict the distribution over longer sequences of tokens might degrade during inference.

Another potential challenge lies in how language is generated by humans. Humans typically generate texts by first conceptualizing an underlying ``\emph{context}'' and then constructing sentences based on it, rather than improvising one token at a time with no regard to the overall intent. This is captured by the causal model in Figure~\ref{fig:main_a}, where a latent ``context'' generates a ``prefix'' (beginning of text), and, in conjunction with this prefix, both produce a ``suffix'' (continuation).

\begin{figure}[t]
\centering
    \includegraphics[width=0.6\columnwidth]{figures/main_figure_causal.pdf}
    \caption{A causal model we consider, in which a latent ``context'' generates a prefix and both produce a suffix.}\label{fig:main_a}
\end{figure}
% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.9\columnwidth]{figures/main_figure_S_v2.pdf}
%     \caption{The $y$-axis is $\log \tilde \SSS/\SSS$, where $\tilde\SSS$ is the H\"older exponent for LLM-generated texts and $\SSS$ is the H\"older exponent of natural texts. Here, the instruction-tuned Gemini 1.0 Pro checkpoint~\citep{geminiteam2024geminifamilyhighlycapable} with temperature $0.5$  is used to generate a text conditioned on some contextual information (e.g. keywords or summary). As we go to the right in the $x$-axis, strictly \emph{more} contextual information is provided in the prompt (see Table~\ref{tab:context} for meaning of abbreviations). We observe a double descent behavior.}
%     \label{fig:main_b} 
% \end{figure}
% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.9\columnwidth]{figures/main_dist.pdf}
%     \caption{The distribution of the H\"older exponent $\SSS$ across all real and LLM-generated texts with temperature $\beta=0.5$, covering 315 different combinations of generating model, scoring model, data, and prompting strategy. See Figures~\ref{fig:info_density_main} \& ~\ref{fig:s_over_h_full} for full results.}
%     \label{fig:main_c} 
% \end{figure}

Formally, under this hypothetical causal model, prompting corresponds to an \emph{interventional} (or ``\textbf{do}'') query. Let $\mathcal{V}$ be a finite vocabulary of tokens, where texts correspond to finite sequences $x\in\mathcal{V}^N$. Now, divide $x$ into a prefix $x_{0:n-1}$ and a suffix $x_{n:N}$. Assuming for simplicity that the set of possible latent contexts is finite, one classical result in causal inference states that the ``interventional'' distribution is given by (see Equation 4.5 in~\cite{neal2020introduction}): 
\begin{equation}\label{eq:do_query}
\footnotesize
    p\left(\mathrm{suf}\;|\;\textbf{do}(\mathrm{pre})\right) = \sum_{c\in\mathcal{C}} p\left(\textbf{c}=c\right)\cdot p\left(\mathrm{suf}\;|\;\mathrm{pre}, \,\textbf{c}=c\right).
\end{equation}
A language model, by contrast, learns the ``conditional'' distribution, which by marginalization is:
\begin{equation}\label{eq:cond}
\footnotesize
    p\left(\mathrm{suf}\;|\;\mathrm{pre}\right) = \sum_{c\in\mathcal{C}} p\left(\textbf{c}=c\;|\;\mathrm{pre}\right)\cdot p\left(\mathrm{suf}\;|\;\mathrm{pre}, \,\textbf{c}=c\right).
\end{equation}
We provide an example that illustrates such differences in Appendix~\ref{sect:app:doquery_example}. Under this hypothesis, LLMs may struggle to fully replicate the fractal nature of human language because the conditional distribution uses $p\left(\textbf{c}=c\;|\;\mathrm{prefix}\right)$ instead of the marginal $p\left(\textbf{c}=c\right)$, which should be used if prompting corresponds to an intervention. To account for this, we investigate the impact of availing various amounts of contextual information in the prompt. These prompting strategies range from minimal cues (e.g. few keywords) to detailed prompts (e.g. summaries or ordered  excerpts). Interestingly, increasing the information density in the prompt does not always improve fractal characteristics, as shown in Figure~\ref{fig:info_density_main}. In fact, the relationship for self-similarity seems to exhibit a ``double descent'', where providing a summary in the prompt generates texts that are \emph{less} similar to natural language than providing either an unordered set of keywords (less information) or an ordered set of excerpts (more).


Do fractal parameters in LLM-generated texts differ substantially from natural language?  Our findings suggest that they partially do.  Specifically, the range of fractal parameters in natural language is contained with a \emph{narrow} range, whereas those of LLMs' output vary widely, as demonstrated for the H\"older exponent $\SSS$ in Figure~\ref{fig:s_over_h_full}. Large values of $\SSS$ indicate less self-similar structure (i.e. lack of rich details) so we expect LLMs to fail occasionally to produce texts with small values of $\SSS$. This is indeed what we observe.  %Interestingly, we find that, across all combinations of scoring models and datasets, natural texts keep the H\"older exponent $\SSS$ in a narrow range while $\SSS$ in LLM-generated texts can vary substantially, as shown in Figure~\ref{fig:main}(right). %This implies that while many LLM outputs are indistinguishable from natural language, there exists a subset of LLM-generated texts that can be reliably identified based on their fractal parameters alone.

To conduct our study, we build a dataset comprising of over 240,000 LLM-generated articles. These  differ by the model that generated them, the contextual information provided in the prompt, the decoding temperature, and the data domain (e.g.  science, news, etc). To facilitate research in domains, such as detecting LLM-generated contents, we make this dataset public. The data card is provided in Appendix~\ref{sect:app:data_card} and samples from the data are shown in Appendix~\ref{sect:app:rho_doc_sample}.
\paragraph{Statement of Contribution.}In summary, we:
\begin{enumerate}
    \item provide a comprehensive analysis of how various factors---such as decoding temperatures, instruction-tuning and model size---affect the ability of LLMs to replicate the fractal properties of natural language.
    \item investigate how prompting affects the fractal structure of texts, showing evidence for a non-monotone behavior. We demonstrate that the range of fractal parameters in natural language is much narrower than in LLM's. This shows that fractal parameters might prove useful in identifying some (but not all) synthetic texts. We also connect the differences in fractal parameters to the quality of the texts.
    \item Our results hold across a variety of model architectures, demonstrating the generality of our findings and making them relevant for diverse LLM families.
    \item We release a dataset containing over 240,000 articles generated by various LLMs (both pretrained and instruction-tuned) across various settings (e.g. data domain and temperature).
\end{enumerate}
