\paragraph{Q1. How do log-perplexity scores in LLM-generated documents differ from those in language?}
To answer this question, results are shown in Figure~\ref{fig:ppl_gemini}. In agreement with prior works, we observe that LLM-generated texts have a lower log-perplexity scores (negative values in the $y$ axis) but {not for large pretrained models when prompted using their pretraining temperature $\beta=1$.} In the latter setting, LLM-generated texts have a similar average log-perplexity score to natural language. We illustrate this using the GLTR tool~\citep{gehrmann-etal-2019-gltr} in Figure~\ref{fig:gltr}. Gemma-2B is an exception, probably because it is much smaller than the rest of the models. Instruction-tuning, by contrast, lowers the log-perplexity of generated texts compared to natural language even at temperature $\beta=1$, although no prompting instructions are used in Figure~\ref{fig:ppl_gemini}. Overall, this suggests that detection methods relying solely on log-perplexity scores, such as GLTR, may not be adequate for identifying contents generated by pretrained models as they become increasingly more capable in the future.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.99\columnwidth]{figures/2/point2_1_gemini_m.pdf}
    \includegraphics[width=0.99\columnwidth]{figures/2/point2_1_mistral_7b_pt.pdf}
    \includegraphics[width=0.99\columnwidth]{figures/2/point2_1_gemma_2b_pt.pdf}
    \caption{The $y$-axis is either $\log\tilde\SSS/\SSS$ (left column) or $\log\tilde\HHH/\HHH$ (right column), where $\tilde \SSS$ is the H\"older exponent of LLM-generated texts while $\SSS$ is of natural language, and the same holds for $\tilde\HHH$ and  $\HHH$. The $x$-axis are the generating models: Gemini 1.0 Pro, Mistral-7B, and Gemma-2B (PT models with temperature $\beta=1$).  Subtitles indicate the model used for scoring the texts. As expected, we observe that larger models tend to replicate the fractal properties of natural language better than smaller models. In addition, LLM-generated texts has higher values of both $\SSS$ (less self-similarity) and $\HHH$ (more dependence).}
    \label{fig:fractal_param_pt}
\end{figure}
\begin{figure*}[t]
    \centering
    \includegraphics[width=0.68\columnwidth]{figures/1/point1_1_gemini_m_v2.pdf}
    \includegraphics[width=0.68\columnwidth]{figures/1/point1_1_mistral_7b_pt_v2.pdf}
    \includegraphics[width=0.68\columnwidth]{figures/1/point1_1_gemma_2b_pt_v2.pdf}
    \caption{$y$-axis is the log-ratio of log-PPL scores for both pretrained and instruction-tuned models, when Gemini 1.0 Pro (top), Mistral-7B (middle), and Gemma-2B (bottom) is used to score texts. Texts generated by large pretrained models do \emph{not} have a lower log-PPL than natural texts; instruction tuning and the use of small temperatures lead to that effect.}
    \label{fig:ppl_gemini}
\end{figure*}
\begin{figure}[t]
    \centering{\ttfamily\hspace{0.4cm}
    (HUMAN) \hspace{1.4cm} (PT) \hspace{1.4cm} (IT)\hspace{0.5cm}\phantom{0}} \\
    \includegraphics[width=0.31\columnwidth,valign=t]{figures/real.png}
    \includegraphics[width=0.31\columnwidth,valign=t]{figures/pt.png}
    \includegraphics[width=0.31\columnwidth,valign=t]{figures/it.png}
    \caption{Output of the GLTR tool~\citep{gehrmann-etal-2019-gltr} on texts generated by humans (left), Mistral-7B pretrained (middle) and Mistral-7B instruction-tuned (right) at temperature $\beta=1.0$. Both the pretrained and instruction-tuned models are provided with a short prefix. The output of the pretrained model looks similar to the human-generated text in terms of log-PPL scores (more orange, red, and purple tokens indicating surprise), in agreement with Figure~\ref{fig:ppl_gemini}.}
    \label{fig:gltr}
\end{figure}

\paragraph{Q2. If large pretrained LLMs at their pretraining temperature $\beta=1$ can replicate the 1st-order statistics of log-perplexity scores in language, do they also replicate its fractal parameters?}
 Figure~\ref{fig:fractal_param_pt} summarizes the results. We observe that larger pretrained models at temperature $\beta=1$ replicate the fractal properties of natural language better, regardless of which model is used for scoring. In addition, LLM-generated texts are systematically biased towards higher values of both $\SSS$ (less self-similarity) and $\HHH$ (more dependence) than in natural language. As we will discuss later in Q6, this means they are biased towards generating texts with \emph{lower} quality than in natural language.

\begin{figure*}[t]
    \centering
    \includegraphics[width=1.99\columnwidth]{figures/4/point4_1_S_v2.pdf}
    \includegraphics[width=1.99\columnwidth]{figures/4/point4_1_H_v2.pdf}
    \caption{Distribution of the log-ratio of H\"older exponent $\SSS$ (top) and Hurst exponent $\HHH$ (bottom) in pretrained (PT) and instruction-tuned (IT) models compared to natural language. Instruction-tuned models at low temperatures $\beta<1$ have higher values of $\HHH$; i.e. more dependence over time. See Section~\ref{sect:results}/Q3 for further discussion.}
    \label{fig:pt_vs_it_s_h}
\end{figure*}

\paragraph{Q3. What about fractal parameters in instruction-tuned models?}
We examine the impact of instruction tuning on fractal parameters when all texts are generated in both pretrained and instruction-tuned models using simple continuation (no prompting). We focus on simple continuation here to isolate the impact of instruction-tuning alone. To recall, Figure~\ref{fig:ppl_gemini} shows that instruction tuning generates texts with lower log-perplexity scores than natural language. Figure~\ref{fig:pt_vs_it_s_h} shows that  texts generated by instruction-tuned models have higher values of the Hurst exponent at low temperatures $\beta<1$, indicating more dependence over time. Self-similarity is not impacted, however.

\paragraph{Q4. What if contextual information is provided in the prompt to instruction-tuned models?}
As mentioned in Section~\ref{sect:intro}, we also consider the causal graph shown in Figure~\ref{fig:main_a}, and examine the impact of adding various contextual cues in the prompt (see Table~\ref{tab:context}). Figure~\ref{fig:info_density_main} summarizes the results across all combinations of generating models, scoring models, and datasets. For self-similarity, we observe a double descent. While the second descent is expected, given that LLMs should be eventually capable of replicating the original article if its entire content is provided in the prompt, the fact that providing a sample of unordered keywords is better than a summary is surprising! We also observe that asking the model to generate an outline first, similar to chain-of-thought (CoT) prompting~\citep{wei2023chainofthoughtpromptingelicitsreasoning}, yields fractal parameters that are closer to those of natural language than simple continuation. In addition, as shown in Appendix~\ref{sect:app:info_density_full_figures}, generating patent and science articles seem to be more sensitive to prompting than in other domains.

\begin{figure*}[t]
    \centering
    \includegraphics[width=1.95\columnwidth]{figures/3/point3_1_S_supermain_v2.pdf}
    \includegraphics[width=1.95\columnwidth]{figures/3/point3_1_H_supermain_v2.pdf}
    \caption{$y$-axis is the log of the ratio of the fractal parameters between LLM-generated texts and natural language, similar to Figure~\ref{fig:fractal_param_pt}. $x$-axis is from left to right the 7 prompting strategies in Table~\ref{tab:context} (top to bottom). Detailed results are in Appendix~\ref{sect:app:info_density_full_figures}.}
    \label{fig:info_density_main}
\end{figure*}

\paragraph{Q5. How sensitive are fractal parameters of LLM-generated articles to the contextual information provided in the prompt?} To answer this question, we first calculate for each of the three architectures Gemini 1.0 Pro, Mistral-7B, and Gemma-2B the average H\"older and Hurst exponents disaggregated by prompting method, where averages are calculated over all remaining variables (e.g. decoding temperature, scoring algorithm, and dataset). Then, we plot the standard deviation calculated across the prompting methods. The results are displayed in Figure~\ref{fig:prompt_sensitivity}. As expected, larger models are less sensitive to the choice of the prompting method than smaller models.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\columnwidth]{figures/prompt_sensitivity_s.pdf}\hfill
    \includegraphics[width=0.5\columnwidth]{figures/prompt_sensitivity_h.pdf}
    \caption{Standard deviation of $\SSS$ (left) and $\HHH$ (right) calculated across prompting methods, for each of the three models: Gemini 1.0 Pro (G-P), Mistral-7B (M-7) and Gemma-2B (G-2).}
    \label{fig:prompt_sensitivity}
\end{figure}

\paragraph{Q6. How do fractal parameters relate to the quality of output?}
To answer this question, we first recall that the H\"older exponent $\SSS$ quantifies the level of self-similarity in a stochastic process, with \emph{smaller} values indicating a \emph{more} self-similar structure (heavier tail); i.e. with complex, rich details at all levels of granularity. Hence, lower values of $\SSS$ are desirable. By contrast, the Hurst exponent $\HHH$ quantifies dependence over time. Values close to $\HHH\approx 0.5$ indicate no dependence (i.e. the process is random) while values close to $\HHH\approx 1.0$ indicate strong predictability (e.g. when the same text is repeated over and over again). Natural language has values close to $\HHH\approx 0.65$. In practice, LLMs do not generate words entirely at random, so the correlation between $\HHH$ and model quality is negative. For this reason, $\HHH$ is also strongly and positively correlated with $\SSS$, with a Pearson coefficient of 0.68 and a p-value $<10^{-15}$.  

In Figure~\ref{fig:quality}, we plot the average quality of documents as judged by Gemini 1.0 Pro against their average log-perplexity score and fractal parameters. We observe, as expected,  that both $\SSS$ and $\HHH$ are negatively correlated with quality. Interestingly, $\HHH$ is a much stronger predictor of average quality than the other metrics. The reason log-perplexity is not a good predictor of quality can be illustrated with a simple example. Suppose that the entire document comprises of a single word repeated over and over again. Then, the log-perplexity of each subsequent token gets progressively closer to zero, since the next token can be reliably predicted. Obviously, this does not imply that an article made of a single repeating word has a good quality. The Hurst exponent, on the other hand, will be quite large in the latter case, indicating poor quality. In Appendix~\ref{sect:app:rho_doc_sample}, we provide a sample of documents from hyperparameter settings that yield large and small values of $\HHH$ for comparison.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.58\columnwidth]{figures/quality_bits.pdf}\hfill
    \includegraphics[width=0.58\columnwidth]{figures/quality_s.pdf}\hfill
    \includegraphics[width=0.58\columnwidth]{figures/quality_h.pdf}
    \caption{Average quality of LLM-generated documents, as judged by Gemini 1.0 Pro, vs. log-PPL (left), H\"older exponent (middle), and Hurst exponent (right). The Hurst parameter is a much better predictor of quality than the other metrics. See Section~\ref{sect:results}/Q6 for discussion, Appendix~\ref{sect:app:prompting} for the exact prompt used in Gemini 1.0 Pro, and Appendix~\ref{sect:app:rho_doc_sample} for examples. }
    \label{fig:quality}
\end{figure*}

%Since small values of $\SSS$ are desirable, because they convey rich details at all scales, and \emph{intermediate} values of $\HHH$ are desirable---too small values imply lack of dependence while too large values mean that the text is predictable---we compare the ratio $\rho=\SSS/\HHH$ between natural language and LLM-generated texts.

\paragraph{Q7. How well does the range of fractal parameters overlap between natural language and LLM-generated texts?} 
Figure~\ref{fig:s_over_h_full} shows that the range of fractal parameters in LLM-generated texts is mostly a \emph{narrow} subset of those observed in natural language. Clearly, while there is an overlap, natural language maintains $\SSS$ and $\HHH$ in a narrow range whereas they vary widely in LLM-generated texts. This is consistent with the earlier observation about the relation between fractal parameters and quality of texts. Among the different factors considered, we find that \emph{prompting} has the biggest impact on $\SSS$ and $\HHH$ as shown in Table~\ref{tab:mutual_info}, which calculates the Shannon mutual information between fractal parameters and other variables. 

Nevertheless, it is important to keep in mind that in Figure~\ref{fig:s_over_h_full}, each value of the fractal parameter is calculated over a \emph{corpus} of texts, not individual documents. A corpus of texts corresponds to a particular combination of generating model, decoding temperature, prompting method, scoring model, and dataset. The reason fractal parameters are calculated over multiple documents is because they describe properties about the \emph{underlying stochastic process}, such as its autocorrelation function $\rho_n=\mathbb{E}[x_{t+n}x_t]$, not properties about a single individual realization of it. Hence, multiple independent measurements are needed. 


%However, we can ignore such technical details and simply ask ourselves how the distribution of the ratios $\SSS/\HHH$ would look like when estimating fractals parameters on each individual document. Figure~\ref{fig:s_over_h_full_per_doc} summarizes the result. 


\begin{figure*}[t]
    \centering
    \includegraphics[width=1.99\columnwidth]{figures/6/point6_3_S_fake.pdf}
    \includegraphics[width=1.99\columnwidth]{figures/6/point6_3_H_fake.pdf}
    \caption{Distribution of $\SSS$ and $\HHH$ for collections containing both LLM-generated and human-generated texts, where the values in legend indicates the proportion of LLM-generated texts. See Section~\ref{sect:results}/Q7 for details. Cases when LLMs repeat the same text mostly occur with greedy decoding (left) but we still see different distributions of $\SSS$ and $\HHH$ in higher temperatures.}
    \label{fig:s_over_h_full}
\end{figure*}
\begin{table}[t]
    \centering\scriptsize
    \caption{Shannon mutual information~\citep{shannon1948mathematical} between $\SSS$ or $\HHH$ and other variables, normalized by the Shannon entropy of the fractal parameter. We bin  values into intervals of length $0.1$. Prompting has the biggest impact on fractal parameters, followed by the data domain. 
    }
    \label{tab:mutual_info}
\begin{tabularx}{\linewidth}{l|X|X|X|X|X}
  \toprule
  &\bf Scoring & \bf Generating & \bf Temp &\bf Dataset &\bf Prompt \\
  &\bf Model & \bf Model & & & \\
  \midrule
  $\SSS$ &  0.2\% & 1.0\% & 1.0\% & 7.3\% & 8.4\%\\
   $\HHH$ &  1.7\% & 4.8\% & 4.3\% & 7.2\% & 19.7\%\\
  \bottomrule
  \end{tabularx}
\end{table}



\paragraph{Q8. Are there notable differences when LLMs are used to score their own outputs?}
Inspired by prior observations, which suggest that using \emph{similar} architectures for both scoring and generation might work better for detecting LLM-generated texts~\citep{10.5555/3618408.3619446,fagni2021tweepfake}, we explore if there are differences in fractal parameters when LLMs score their own outputs (i.e. using Mistral-7B to score its output, as opposed to the output of other models). 

One way to examine this is to look into the reduction in uncertainty:
\begin{equation}
    \mathcal{J}(X; Y) \doteq U(X\,|\,Z) - U(X\,|\,Z, Y),
\end{equation} 
where $U(X|Z)$ is a measure of the uncertainty in $X$ when conditioning on $Z$, such as the conditional Shannon entropy or the error rate of predicting $X$ given $Z$. In our setting, $X$ and $Y$ are the scoring and generating models while $Z$ are the remaining variables, such as dataset, decoding temperature, and fractal parameters. Intuitively, because fractal parameters are included in the set of predictors, if the error rate of predicting the generating model does not depend on the scoring model even when fractal parameters and other variables are included in the set of predictors, then fractal parameters remain relatively unchanged whether or not the same model is used for generating and scoring texts, which is indeed what we observe. Specifically, a random forest classifier, in its default Sciki-Learn implementation~\citep{scikit-learn}, can predict the scoring model with a high accuracy of 97.0\% without having to include the generating model in the set of predictors and this accuracy remains unchanged when including the generating model. Similarly, the accuracy of predicting the generating model is quite high at 97.8\% without using the scoring model as a predictor. Including the scoring model improves accuracy only slightly.

\paragraph{Q9. Are some domains easier to synthesize?} 
Table~\ref{tab:domains} shows that LLM-generated encyclopedic articles and legal documents are closer to those of natural language in terms of average log-perplexity scores (1st order statistics) and fractal parameters (2nd order). However, this may be a reflection of the weight of those domains during (pre)training, rather than anything fundamental about the domains themselves.



\begin{table}[t]
    \centering\scriptsize
    \caption{Log-ratio of fractal parameters and log-perplexity scores between LLM-generated texts and natural language using instruction-tuned models. Results are averaged across all settings (e.g. decoding temperatures,  models and prompts). LLM-generated encyclopedic and legal documents are closer to natural language, than other domains.
    }
    \label{tab:domains}
\begin{tabularx}{\linewidth}{l|c|c|c}
  \toprule
  \bf Dataset& $\SSS$ & $\HHH$ &\bf log-perplexity \\ \midrule
  NEWSROOM &$0.190\pm0.137$ & $\phantom{+}0.020\pm0.050$& $-0.575\pm0.290$\\
  SCIENTIFIC &$0.168\pm0.149$ &$\phantom{+}0.065\pm0.055$ &$-0.607\pm0.271$ \\
  BIGPATENT &$0.154\pm0.145$ &$\phantom{+}0.040\pm0.057$ &$	-0.525\pm0.262$ \\
  BILLSUM &$0.057\pm0.098$ &$-0.044\pm0.046$ &$-0.119\pm0.275$ \\
  WIKIPEDIA &$0.060\pm0.120$ &$-0.010\pm0.040$ &$-0.463\pm0.288$ \\
  \bottomrule
  \end{tabularx}
\end{table}
