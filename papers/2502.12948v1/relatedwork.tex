\section{Related Work}
\subsubsection{Vision-Language Training in the medical domain.}
Many studies have explored building specialized models in the medical domain \cite{convirt, wang2022medclip, zhang2024mediclip}. BiomedCLIP \cite{zhang2023biomedclip}, a vision-language model (VLM) was trained on 15M image-text pairs from the biomedical domain. Adapting these models to downstream tasks has been explored in various ways: a) End to end fine-tuning \cite{convirt, ikezogwo2024quilt},  b) Prompt tuning \cite{coop2022, cocoop}, c) Linear probing \cite{radford2021learning, coop2022,shakeri2024few}.  These methods still require additional training stages on a labeled "support" dataset for the downstream task, and can underperform in very low data cases, especially with complex tasks.  
Recently, multi-modal language models (MLLMs) have emerged as generalist models for various tasks \cite{medflamingo,medpalm,llavamed}. These are generative models capable of generating and processing long-form text. For instance, Med-Flamingo \cite{medflamingo} is an open-source, multimodal few-shot learner adapted to the medical domain. These models demonstrate impressive performance across a wide variety of tasks, such as visual question answering, classification, report generation, etc., on a wide range of modalities.  


\subsubsection{Synthetic Data Generation.}

Augmentation with synthetic data has the potential to mitigate the issue of limited domain data in medical imaging. However, generating high-quality synthetic medical images remains challenging. Clip-Medfake \cite{medfake} leverages Stable Diffusion \cite{stablediffusion} to generate synthetic images, which are then used to pretrain a CLIP model before fine-tuning on actual medical data. Latte-CLIP \cite{cao2024latteclip} uses MLLMs to generate descriptive text for domain-specific images, which are then used for CLIP model fine-tuning. Data synthesis in these methods is done by pre-trained models or models trained from a small portion of the original training data.  They lack fine grained control over the synthesis process. CtrlSynth \cite{cao2024ctrlsynth} utilizes pre-trained models to identify concepts within an image, systematically alter these attributes, and generate synthetic text with LLMs. This synthetic text is then converted into images through Stable Diffusion, creating a comprehensive synthetic dataset that supports model training on controlled variations. 

\subsubsection{LGE Detection.}

There is no consensus regarding the optimal method for LGE analyses. Commonly used manual and semi-automatic methods clinically include manual planimetry, the Full Width Half Maximum (FWHM) approach \cite{amado2004accurate, hsu2006quantitative}, and n-std from remote myocardium. Comparative studies examining these methods \cite{flett2011evaluation, heiberg2022infarct} reveal significant variability in quantification results, highlighting issues with both reliability and reproducibility.

Recently, there have been many DL-based studies focused on automated scar detection from cardiac LGE images \cite{zhang2021cascaded, kim2024deep, girum2021automatic, yang2021hybrid}. For instance, Kim et al. \cite{kim2024deep} leveraged segmental information to train models that identify the presence or absence of LGE by transforming the images into polar coordinates based on the left ventricular (LV) center and the right ventricular (RV) insertion points. Additionally, several studies have explored infarct segmentation on the publicly available EMIDEC dataset \cite{lalande2020emidec} containing 150 patients (100 for training), achieving classification accuracies as high as 0.92 \cite{lalande2022deep} and a Dice coefficient of up to 0.71 for infarct segmentation \cite{zhang2021cascaded}. These models are trained on dense pixel-wise annotations of myocardial and scar tissue provided by clinical experts, thereby enhancing their ability to accurately segment scar regions.