%File: anonymous-submission-latex-2025.tex
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai25}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath} 
\usepackage{enumitem}
\usepackage{amsfonts}


%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2025.1)
}

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.



% The file aaai25.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash

% \iffalse

% \author{
%     %Authors
%     % All authors must be in the same font size and format.
%     Written by AAAI Press Staff\textsuperscript{\rm 1}\thanks{With help from the AAAI Publications Committee.}\\
%     AAAI Style Contributions by Pater Patel Schneider,
%     Sunil Issar,\\
%     J. Scott Penberthy,
%     George Ferguson,
%     Hans Guesgen,
%     Francisco Cruz\equalcontrib,
%     Marc Pujol-Gonzalez\equalcontrib
% }
% \affiliations{
%     %Afiliations
%     \textsuperscript{\rm 1}Association for the Advancement of Artificial Intelligence\\
%     % If you have multiple authors and multiple affiliations
%     % use superscripts in text and roman font to identify them.
%     % For example,

%     % Sunil Issar\textsuperscript{\rm 2},
%     % J. Scott Penberthy\textsuperscript{\rm 3},
%     % George Ferguson\textsuperscript{\rm 4},
%     % Hans Guesgen\textsuperscript{\rm 5}
%     % Note that the comma should be placed after the superscript

%     1101 Pennsylvania Ave, NW Suite 300\\
%     Washington, DC 20004 USA\\
%     % email address must be in roman text type, not monospace or sans serif
%     proceedings-questions@aaai.org
% %
% % See more examples next
% }
% \fi








\title{Fake It Till You Make It: Using Synthetic Data and  Domain Knowledge\\ for Improved Text-Based Learning for LGE Detection}


\author{
    % Authors
    Athira J Jacob \textsuperscript{\rm 1,2},
    Puneet Sharma \textsuperscript{\rm 1},
    Daniel Rueckert \textsuperscript{\rm 2,3}
}


\affiliations{
    % Affiliations
    \textsuperscript{\rm 1}Digital Technology and Innovation, Siemens Healthineers, Princeton, NJ, USA\\
    \textsuperscript{\rm 2} AI in Healthcare and Medicine, Klinikum rechts der Isar, Technical University of Munich, Germany\\
    \textsuperscript{\rm 3} Department of Computing, Imperial College London, UK
}

\begin{document}

\maketitle

\begin{abstract}
Detection of hyperenhancement from cardiac LGE MRI images is a complex task requiring significant clinical expertise. Although deep learning-based models have shown promising results for the task, they require large amounts of data with fine-grained annotations. Clinical reports generated for cardiac MR studies contain rich, clinically relevant information, including the location, extent and etiology of any scars present. Although recently developed CLIP-based training enables pretraining models with image-text pairs, it requires large amounts of data and further finetuning strategies on downstream tasks. In this study, we use various strategies rooted in domain knowledge to train a model for LGE detection solely using text from clinical reports, on a relatively small clinical cohort of 965 patients.  We improve performance through the use of synthetic data augmentation, by systematically creating scar images and associated text. In addition, we standardize the orientation of the images in an anatomy-informed way to enable better alignment of spatial and text features. We also use a captioning loss to enable fine-grained supervision and explore the effect of pretraining of the vision encoder on performance. Finally, ablation studies are carried out to elucidate the contributions of each design component to the overall performance of the model.     
\end{abstract}

% Uncomment the following to link to your code, datasets, an extended version or similar.
%
% \begin{links}
%     \link{Code}{https://aaai.org/example/code}
%     \link{Datasets}{https://aaai.org/example/datasets}
%     \link{Extended version}{https://aaai.org/example/extended-version}
% \end{links}


\section{Introduction}

Late Gadolinium Enhancement (LGE) imaging—also referred to as Delayed Enhancement Imaging—plays a critical role in assessing myocardial viability. By highlighting affected areas with increased contrast uptake, it allows non-invasive detection and assessment of myocardial infarction, as well as ischemic and non-ischemic cardiomyopathies and other cardiac pathologies \cite{jenista2023revisiting}. However, detecting areas of hyperenhancement from LGE images (henceforth referred to as LGE or scar detection) is a challenging task. Enhancement can be subtle and is often influenced by spatial and temporal variations across different scanners, sequences, and study protocols. Moreover, image noise, artifacts, and varying LGE patterns add to the difficulty. These complexities make developing robust, generalizable automated solutions for LGE detection particularly challenging. Although deep learning (DL) approaches have achieved impressive performance in a range of medical imaging applications, they are heavily dependent on access to large, good quality, annotated datasets for increased accuracy and generalization. The annotation of LGE data, which requires precise delineation of both myocardial and enhancement regions, is especially complex due to its variability and requires substantial clinical expertise. These demands pose a limitation on large-scale DL training efforts in this area.

Meanwhile, clinical reports present a source of valuable information about LGE assessment. The reports are created from the cardiac magnetic resonance (CMR) study and contain clinician-written notes about the LGE, including location, extent, etiology, and other impressions. While these could be manually converted to binary labels to train a DL model, that is impractical for large amounts of data. Moreover, translating some of the information into discrete labels could potentially be a non-intuitive task. In such a situation, training directly with text offers an attractive alternative. 

Recently, Contrastive Language Image Pre-training (CLIP) \cite{radford2021learning} has achieved notable success in incorporating text supervision into vision models, for a wide range of downstream tasks in natural image processing, such as classification \cite{zhou2022learning},  object detection\cite{lin2023gridclip}, and segmentation \cite{luo2023segclip}. CLIP's approach aligns image and corresponding textual description embeddings within a shared latent space, facilitating a unified representation. This training paradigm has also been applied within medical imaging \cite{zhao2023clip}. However, CLIP models often require large datasets—often millions of image-text pairs—to learn robust associations.  Collecting such data, especially in specialized fields like medical imaging, is challenging due to limited availability and high annotation costs. These data demands can limit accessibility. In addition, despite the impressive zero-shot or few-shot performance, fine-tuning with task-specific data is often required to reach state-of-the-art (SoTA) performance. Many studies explore adapting CLIP models to downstream tasks through prompt tuning and linear probing. However, these require further training stages for the model, and is dependent on the size of the fine-tuning dataset. 

In this study, we train a model for LGE detection on a clinical cohort of 965 patients. We use text supervision from their corresponding clinical reports, without any further fine-tuning with discrete labels. We use domain knowledge to systematically augment the limited dataset with synthetic scar images and text. Moreover, we normalize the orientation of the images in an anatomy-informed way to facilitate correspondence between image and text features. Additionally, we add captioning loss to provide more granular supervision from the text.  Furthermore, the model’s encoder is pre-trained on a related task involving LGE images.  We obtain an average balanced accuracy of 0.83 on the test set. Though the model is supervised with patient-level descriptions, it can produce slice-level predictions due to the training strategy, which aids in interpretability. We also conduct ablation studies to assess the impact of each design choice and explore the effect of alternative encoder initialization strategies on model performance.


\section{Related Work}

\subsubsection{Vision-Language Training in the medical domain.}
Many studies have explored building specialized models in the medical domain \cite{convirt, wang2022medclip, zhang2024mediclip}. BiomedCLIP \cite{zhang2023biomedclip}, a vision-language model (VLM) was trained on 15M image-text pairs from the biomedical domain. Adapting these models to downstream tasks has been explored in various ways: a) End to end fine-tuning \cite{convirt, ikezogwo2024quilt},  b) Prompt tuning \cite{coop2022, cocoop}, c) Linear probing \cite{radford2021learning, coop2022,shakeri2024few}.  These methods still require additional training stages on a labeled "support" dataset for the downstream task, and can underperform in very low data cases, especially with complex tasks.  
Recently, multi-modal language models (MLLMs) have emerged as generalist models for various tasks \cite{medflamingo,medpalm,llavamed}. These are generative models capable of generating and processing long-form text. For instance, Med-Flamingo \cite{medflamingo} is an open-source, multimodal few-shot learner adapted to the medical domain. These models demonstrate impressive performance across a wide variety of tasks, such as visual question answering, classification, report generation, etc., on a wide range of modalities.  


\subsubsection{Synthetic Data Generation.}

Augmentation with synthetic data has the potential to mitigate the issue of limited domain data in medical imaging. However, generating high-quality synthetic medical images remains challenging. Clip-Medfake \cite{medfake} leverages Stable Diffusion \cite{stablediffusion} to generate synthetic images, which are then used to pretrain a CLIP model before fine-tuning on actual medical data. Latte-CLIP \cite{cao2024latteclip} uses MLLMs to generate descriptive text for domain-specific images, which are then used for CLIP model fine-tuning. Data synthesis in these methods is done by pre-trained models or models trained from a small portion of the original training data.  They lack fine grained control over the synthesis process. CtrlSynth \cite{cao2024ctrlsynth} utilizes pre-trained models to identify concepts within an image, systematically alter these attributes, and generate synthetic text with LLMs. This synthetic text is then converted into images through Stable Diffusion, creating a comprehensive synthetic dataset that supports model training on controlled variations. 

\subsubsection{LGE Detection.}

There is no consensus regarding the optimal method for LGE analyses. Commonly used manual and semi-automatic methods clinically include manual planimetry, the Full Width Half Maximum (FWHM) approach \cite{amado2004accurate, hsu2006quantitative}, and n-std from remote myocardium. Comparative studies examining these methods \cite{flett2011evaluation, heiberg2022infarct} reveal significant variability in quantification results, highlighting issues with both reliability and reproducibility.

Recently, there have been many DL-based studies focused on automated scar detection from cardiac LGE images \cite{zhang2021cascaded, kim2024deep, girum2021automatic, yang2021hybrid}. For instance, Kim et al. \cite{kim2024deep} leveraged segmental information to train models that identify the presence or absence of LGE by transforming the images into polar coordinates based on the left ventricular (LV) center and the right ventricular (RV) insertion points. Additionally, several studies have explored infarct segmentation on the publicly available EMIDEC dataset \cite{lalande2020emidec} containing 150 patients (100 for training), achieving classification accuracies as high as 0.92 \cite{lalande2022deep} and a Dice coefficient of up to 0.71 for infarct segmentation \cite{zhang2021cascaded}. These models are trained on dense pixel-wise annotations of myocardial and scar tissue provided by clinical experts, thereby enhancing their ability to accurately segment scar regions.

\section{Methodology}

\subsubsection{Preliminaries: CLIP-based training}

CLIP training framework consists of a vision encoder and a text encoder to extract features from pairs of images and text, respectively. Each encoder is followed by a set of projection layers to project the features into a common embedding space. More specifically, the input image $x_{img}$ is encoded by the encoder $E_{img}$ into feature vector $f_{img} \in \mathbb{R}^n$. A projection module $P_{img}$   maps the features into the embedding $v \in \mathbb{R}^p$.

\[ v = P_{img}(E_{img}(x_{img}))\]
 
Similarly, the text encoder $E_{txt}$ encodes the input text into feature vector $f_{txt} \in \mathbb{R}^m$. A projection module $P_{txt}$   maps the features into the embedding $t \in \mathbb{R}^p$ .
\[ t = P_{txt}(E_{txt}(x_{txt}))\]

The similarity score is calculated using dot product as \[s = v_n \cdot t_n \] where $v_n, t_n$ represent L2 normalized vectors.
Then, cross-entropy loss (CE) is used to maximize the similarity score within the same pair, and minimize the same across pairs \cite{radford2021learning}. 


\subsubsection{Preliminaries: Related tasks on LGE images.}

Prior to this study, we trained DL networks to segment the myocardium and detect anterior and posterior RV Insertion Points on LGE MRI Images. Ground truth (GT) annotations for these tasks are relatively easy to create. The myocardium segmentation network consisted of a UNet architecture, with DenseNet121 \cite{huang2017densely} encoder. It was trained on 7401 manually annotated images from 786 patients, from 3 centers. The model achieved an average Dice score of 0.88 on the test set of 131 patients (1197 images) from the 3 centers. The landmark detection model consisted of a UNet architecture, with ResNet18 \cite{resnet} encoder. It was trained on 3478 manually annotated images from 377 patients, from a single center to predict heatmaps centered around the landmark points. It achieved an average error 3.1 and 3.3 mm for the anterior and inferior RV Insertion Points, respectively. 

\subsection{Proposed Method}

\begin{figure}[t]
\centering
\includegraphics[width=0.95\columnwidth]{Figures/model.png} 
\caption{Overview of the proposed model}
\label{fig_model}
\end{figure}


We train a model to detect myocardial hyperenhancement from LGE images using relevant text from the clinical reports (Figure \ref{fig_model}). Due to the limited dataset size and the long-tailed distribution of LGE etiologies, we use domain knowledge to systematically augment the training data with synthetic image-text pairs. During training, the image-text pairs are aligned using global CLIP loss and a local caption loss. The image encoder is initialized with the weights from the myocardial segmentation network described in the previous section. Each of these parts is explained in detail in the following sections. During inference, we query the model using the following text: \texttt{there is hyperenhancement in the myocardium} and \texttt{there is no hyperenhancement in the myocardium}, denoting the positive and negative LGE classes respectively.

\subsection{Synthetic Data Generation}

\begin{figure}[t]
\centering
\includegraphics[width=0.99\columnwidth]{Figures/scar_pipeline.png} 
\caption{Synthetic Data Generation: a) Generation pipeline, b) Examples of synthetically generated images and corresponding captions. Text may refer to a region other than the one of the paired image (image 3). }
\label{fig_scar_pipeline}
\end{figure}

We add synthetic scars to real LGE images and create associated text descriptions (Figure \ref{fig_scar_pipeline}a). This allows us to augment the limited data, and systematically cover a wide variety of scar distributions. The scar is applied only to images with no prior LGE (as determined from the clinical report). A Controller module randomly chooses from a set of scar parameters. Then, a synthetic scar is added to the image, and text is created with these parameters. This is done at every training iteration, with a probability of $\lambda$. 

\subsubsection{Controller.} Wall location is randomly chosen as one of ["anterior", "inferior", "posterior"], or ["lateral", "septal"], or a combination of the words chosen from the two sets (Eg: "inferoseptal"). Scar extent is randomly chosen from ["sub-endocardial", "mid-myocardial", "epicardial", "transmural"]. "Transmural" signifies the scar spanning over more than 50\% of the myocardial thickness. 
\subsubsection{Image Generation.} For every negative LGE image (Figure 2a.i) at a given slice location, synthetic scars are added as follows:
\begin{enumerate}
\item Myocardial mask and anterior RVIP are determined using the previously trained DL networks (Figure 2a.ii).
\item Anterior RVIP is used to divide the myocardium into AHA segments: four if apical layer, or six segments if basal or mid (Figure 2a.iii).
\item The myocardial mask is divided equally into 3 concentric sections to represent endocardial, mid-myocardial and epicardial layers (Figure 2a.iv).
\item Wall location (chosen by the controller), along with slice location, is translated into AHA segments through hard-coded values. For eg, inferoseptal on basal level denotes segment 3.
\item A scar candidate region mask is created using an intersection between the identified AHA segments (Step 4) and chosen scar extent (by the controller). This is the "allowed" region for scar creation (Figure 2a.v).
\item Synthetic scar is created in the candidate region mask as a randomly placed, oriented and sized ellipse (Figure 2a.vi). A random pixel is chosen from the candidate region as the center of the synthetic scar. The radii of the ellipse are chosen randomly between set minimum and maximum values. The minimum and maximum values are determined as a fraction of the myocardial thickness at that point, depending on whether the scar is chosen to be transmural or within specific myocardial layers. The created ellipse is smoothed with a Gaussian filter for a more natural and continuous appearance.
\[
r_{min} = max(0.01, th * \rho_{min}) 
\]
\[
r_{max} = th * \rho_{max}  
\]
\[
\mathbf{r} = rand(r_{min}, r_{max}, 2)
\]
\[
\alpha = rand(0,\pi) 
\]
\[
\sigma = rand(0,1) * s_1 + s_2
\]

where, $th$ represents myocardial thickness at that point, $0 < \rho_{min}, \rho_{max} <= 1$ are hyperparameters representing ratios relative to myocardial thickness, $\mathbf{r} \in \mathbb{R}^2 $ are the radii of the major and minor axes of the ellipse, $\alpha$ represents the orientation of the major axis of the ellipse relative to the positive x-axis, and $\sigma$ represents the standard deviation of the Gaussian kernel used for smoothing. The created scar $M$ is min-max normalized to the range of $[0,1]$.
\item  The scar image $M$ is then blended with the image $I$ as, 
\[I_{synth} = I * (1-M) + \gamma * max(I) * M \]
\[ \gamma  = rand(b_1, b_2)\]
where $\gamma$ controls the brightness of the scar, and is randomly chosen between preset minimum and maximum values $b_1, b_2$ (Figure 2a.vii).
\end{enumerate}

\subsubsection{Text Generation.}
Given the chosen scar parameters (slice location, wall location, wall extent) from the controller module, the associated text description is synthesized using preset templates such as the following:\\
"\texttt{there is <extent>  delayed enhancement in <location> wall. This image is from <slice location> level.}"\\
or variations of this, where the words within \texttt{<>} are replaced with chosen scar parameters. The slice location of the input image is appended to stay consistent with real clinical text and contextualize the spatial location of the slice for the model (further explained in the Section  "Implementation Details: Text Encoder").  To add further variation to the text, the words "delayed enhancement", "delayed hyperenhancement", "late enhancement", "scar", "infarct" are used interchangeably. 
    
Examples of images with synthetically generated scar and corresponding text are shown in Figure \ref{fig_scar_pipeline}b.  

\subsection{Normalization of the LV orientation}

\begin{figure}[t]
\centering
\includegraphics[width=0.70\columnwidth]{Figures/lv_norm.png} 
\caption{Anatomically informed normalization of LV orientation }
\label{fig_lv_norm}
\end{figure}


Clinical descriptions of LGE use words that describe location relative to the orientation of the LV, defined by the RV insertion point. The orientation of the LV can vary across patients, and even across images within the same patient. While this could potentially be learnt implicitly from a large cohort of image-caption pairs, this could prove to be a difficult challenge in a dataset of limited size. To help the model better associate position descriptors with image features, we standardize the orientation of the LV using the anterior and inferior RVIPs (Figure \ref{fig_lv_norm}). The RVIPs for each image are obtained from the landmark detection model described previously. Using the two insertion points, each image is rotated to position the line connecting them along the vertical axis of the image, with anterior RVIP being on the top.

\subsection{Caption loss}

CLIP loss aligns image and text embeddings on a global level. However, LGE descriptions from clinical reports are information-dense, with multiple words providing critical information about the location, extent and etiology of the scar. To encourage granular supervision on the level of the individual text tokens, we use a captioning loss similar to contrastive captioner models \cite{yu2022coca}. A multi-modal decoder is applied to the text tokens, consisting of layers of multi-headed, self-attention layers, followed by cross-attention layers attending to features from the vision encoder. The final layer is the classification layer that predicts the distribution of the next token over the supported vocabulary set.     


\subsection{Task specific encoder}

We initialize the vision encoder with the weights of LGE myocardium segmentation model trained as described previously. Segmenting LGE myocardium is a closely related task to LGE detection, and is hypothesized to aid convergence. We later study the effect of this design choice by conducting ablation studies with various other image encoders. 

\section{Experiments}

\subsection{Data}

The data consists of 965 patients with cardiac MRI studies and clinical reports from a single center, of which 404 patients have reported LGE. The scans were performed on 1.5 T magnets (MAGNETOM Avanto, Siemens Healthcare, Erlangen, Germany) using a T1-weighted, phase sensitive inversion-recovery (PSIR), gradient-echo sequence, and were acquired 10 min after injection of a gadolinium-based contrast agent. The acquisition parameters are as follows, TR/TE: 2.4 /1.1 ms; flip angle: 50, slice thickness: 8 mm; in plane resolution between 1.5 × 1.5 6 $mm^2$ and 2.6 × 2.6 $mm^2$. The patients were divided in train, validation and testing splits of 772, 91 and 102 patients, respectively, while maintaining class distribution (LGE presence/absence).  

\subsection{Implementation Details}



\subsubsection{Vision Encoder.} The vision encoder consists of a Densenet121 \cite{huang2017densely} encoder with UNet decoder, with 5 downsampling layers. A MaxPool layer is added after the last layer of the DenseNet encoder to get a feature vector size of n = 1024.  The projection module consists of two Linear layers, separated by GelU non-linearity\cite{hendrycks2016gaussian} and followed by Dropout and LayerNorm \cite{ba2016layer}. 

For every patient, we select the segmented PSIR DICOM series  \cite{MUEHLBERG201813} using information in their DICOM tags, as this sequence theoretically has the higher spatial resolution required for LGE detection. In this dataset, these typically consist of 3 slices, covering apical, mid and basal regions of the heart. Each image is preprocessed according to the following steps: a) resizing to $1 mm \times 1 mm$ resolution, b) cropping to $112 \times 112$ dimension, centered around the LV. LV mask is obtained from the LGE segmentation network described previously, d) Upsampling $2\times$ to $224 \times 224$ e) capping intensities at the 98 percentile and f) normalizing to the range of [0,1].


\subsubsection{Text Encoder.} \label{section_text_encoding} We use the publicly available BiomedBERT \cite{zhang2023large}. Feature vector has size $m = 768$. The text encoder is held frozen for all experiments in this study. The projection module has the same architecture as described in the previous section and is optimized end-to-end during training. 

For each patient, text relevant to LGE imaging is extracted from the respective clinical report, from both the "Findings" and the "Impressions" sections, using a simple keyword search. The extracted text is split into individual sentences (henceforth also referred to as captions), from which one is sampled randomly for every iteration of training. 
However, this approach introduces an issue: the clinical text annotations are provided at the patient level, whereas the corresponding images represent specific heart sub-regions. To address this, we implement a straightforward solution by appending the phrase "\texttt{This image is from <slice location> level}" to each input text, where \texttt{<>} is replaced with basal, mid or apical, as per the image location. This is done consistently during both training and inference to help the model contextualize the image within the anatomical structure. While we limit our method here to three pre-selected slices for simplicity, this framework is adaptable to any number of images by adjusting the corresponding section tag.

\subsubsection{Scar augmentation parameters.}
In all experiments, we use $\lambda = 0.7$, $[\rho_{min}, \rho_{max}] = [0.1,0.4], [0.3,0.6], [0.7,0.1]$ for single-layer, two-layer, and transmural extents respectively, $s_1 = s_2 = 2 $, $b_1 = 0.8, b_2 = 1$. These were selected empirically to ensure image fidelity, anatomical relevance, and diversity across generated outputs.



\subsection{Baselines and Metrics}

We compare the proposed method against the following: 
\begin{enumerate} [label=\alph*)]
    \item BiomedCLIP \cite{zhang2023biomedclip}: We test the model on the same test set, using the same query text: \texttt{there is hyperenhancement in the myocardium} and \texttt{there is no hyperenhancement in the myocardium}. 
    \item MedFlamingo \cite{medflamingo}: The inference query is constructed as a prompt for few-shot, visual question-answering using examples: \texttt{<image 1> Does this cardiac LGE MR image show hyperenhancement in the LV myocardium? Answer: No. <image 2> Does this cardiac LGE MR image show hyperenhancement in the LV myocardium? Answer: Yes. <image 3>  Does this cardiac LGE MR image show hyperenhancement in the LV myocardium? Answer: }, where \texttt{image 1,image 2} are example images from the training set, without and with LGE respectively, and \texttt{image 3} is the query image. 
    \item Image-only classifier: A model is constructed to be identical to the CLIP based model, but without the text part, i.e the image encoder and a projection module followed by a classification head. The image encoder is initialized with the same pretrained weights. The classification head consists of two Linear layers separated by a GeLU non-linearity and a Dropout layer. The baseline model is trained with the GT binary labels extracted from the clinical reports, using binary cross entropy loss. 
\\\\
To reduce the stochastic variation in the results, multiple (n = 3) models were trained in every experiment and metrics were averaged. Balanced accuracy is adopted as the main metric to account for class imbalance in the dataset. 
Adam \cite{kingma2014adam}  optimizer is used with a learning rate of $1e^{-4} $. All models are trained across 4 x NVIDIA A100-SXM4-40GB GPUs using a batch size of 128, with fully distributed parallel processing.  Memory consumption (in GB) and throughput (in FPS) are measured on a single GPU of the same specifications. 
    
\end{enumerate}

\section{Results}



\begin{table}[]
\caption{Balanced accuracies, throughput and memory consumption of the compared methods (n = 102 patients)}
\label{main_results}
\begin{tabular}{l|c|c|c}
\hline
Method                     & FPS $\uparrow$  & Mem (GB) $\downarrow$ & Accuracy      \\ \hline \hline
\begin{tabular}[c]{@{}l@{}}BiomedCLIP\\ \cite{zhang2023biomedclip}\end{tabular}  & 74.4 & 0.76     & 0.58          \\
\begin{tabular}[c]{@{}l@{}}MedFlamingo\\ \cite{medflamingo}\end{tabular} & 1.4  & 31.09    & 0.50          \\
Image-only Classifier                                     & 84.5 & 0.04     & 0.77          \\ \hline
Proposed method                                           & 53.0 & 0.88     & \textbf{0.83} \\ \hline
\end{tabular}
\end{table}



Table 1 shows the quantitative results. The proposed method obtains a balanced accuracy of 0.83, outperforming the baselines. Both the publicly available medical VLMs (BiomedCLIP and MedFlamingo) encounter limitations on this task. These challenges likely stem from two key factors: (a) the VLMs were trained on a diverse array of medical imaging data, with cardiac MR constituting only a small subset, and LGE sequences representing an even smaller fraction of this subset; (b) LGE detection is an inherently challenging task that necessitates specialized clinical domain knowledge and the ability to analyze subtle, fine-grained features within highly localized regions of the images. The image-only classifier trained on this dataset demonstrates higher performance but lags behind the proposed method by 6 pp. 



\begin{table}[]
\caption{Ablation studies: Model is retrained using leave-one-out strategy, keeping everything else the same. pp stands for percentage points.}
\label{tab:table2}
\begin{tabular}{p{5.3cm}|p{2.2cm}}
\hline
Method                           & Accuracy      \\ \hline \hline
Proposed                         & 0.83          \\ 
(-) Synthetic scar augmentation  & 0.73 (-10 pp) \\
(-) LV orientation normalization & 0.77 (-6 pp)  \\ 
(-) Caption loss                 & 0.80 (-3 pp)  \\ \hline
\end{tabular}
\end{table}

\subsubsection{Ablation Studies}

Table 2 illustrates the impact of omitting different components of the proposed method. Among the components, synthetic scar augmentation has the most significant impact on performance, followed by the normalization of LV orientation, and lastly, the caption loss.

Table 3 presents the impact of different initialization strategies for the vision encoder. As previously described, the proposed model uses a vision encoder pre-trained on the related task of myocardium segmentation in LGE images. Here, we explore two alternative initializations:
\begin{enumerate} [label=\alph*)]
\item Task agnostic encoder: This is a unimodal foundation model pre-trained on 36 million CMR images \cite{jacob2024towards}. It was trained in a self-supervised manner, without any labelled data, hence is agnostic to any specific task. The model consisted of a ViT-S architecture, and was pretrained across many diverse sequences of CMR, such as cine, LGE, and mapping.   
\item Imagenet pretrained encoder: This uses the same DenseNet-UNet architecture of the proposed model, but with the publicly available ImageNet trained weights.   
\end{enumerate}

Both initialization choices provide practical alternatives for when training data and/or labels for a directly related task is unavailable. Our results indicate that the task-agnostic CMR foundation model (FM), pre-trained in a self-supervised manner, outperforms the ImageNet pre-trained model; however, it still falls short compared to the model trained on a closely related task. 




\begin{table}[]
\caption{Ablation study: Effect of different encoders on performance. }
\label{table3}
\begin{tabular}{p{5.5cm}|p{1.9cm}}
\hline
Method                           & Accuracy \\ \hline \hline
Proposed - Task specific encoder & \textbf{0.83}     \\
Task agnostic encoder - CMR FM \cite{jacob2024towards}   & 0.80     \\
Imagenet pretrained encoder      & 0.75     \\ \hline
\end{tabular}
\end{table}




\subsection{Qualitative Results}

\begin{figure}[t]
\centering
\includegraphics[width=0.95\columnwidth]{Figures/Figure_examples.png} 
\caption{Qualitative results on real clinical data. Model predictions for each image, with the patient-level GT text}
\label{fig_examples}
\end{figure}


Fig \ref{fig_examples} visualizes the results for 3 patients. Patient 1 shows a true positive detection. Note that though the GT text describes LGE presence across the whole heart, the model is able to produce predictions for individual slices, which aids interpretability. Patient 2 presents a false positive, with the network predicting the LGE presence in apical and mid-ventricular slices. This could be because of the presence of streak artifacts in these two images, degrading image quality and possibly confounding the model. Patient 3 presents a false negative case, with no LGE detected by the model, despite the GT indicating LGE in basal slices. Visual inspection did not reveal LGE in these slices; however, further review of other LGE images in the study confirmed the presence of basal scarring at the level of the LV outflow tract. This highlights a method limitation, as selecting only three input images may omit critical details, reducing the model's efficacy. This is also observed in Patient 1, where the selected images do not reflect all of the described scar.

\section{Conclusions}

Text-based training using clinical reports offers an alternative to obtaining hard labels in the clinical domain, which might be expensive and challenging. However, typically used methods such as CLIP, require large amounts of pretraining data, and further finetuning stages for downstream tasks. We present a method that incorporates domain knowledge to enable CLIP based training for small datasets. We create synthetic image-text pairs to augment the training set, use anatomical information to normalize the orientation of the image, use additional caption loss to enable fine-grained supervision and use related-task pretraining to improve the accuracy for the task. We demonstrate the feasibility of text-based training for specific tasks on small datasets, followed by zero-shot inference without any further finetuning stages. 

\textbf{Disclaimer.} The concepts and information presented in this paper/presentation are based on research results that are not commercially available. Future commercial availability cannot be guaranteed.






% \subsection{Overlength Papers}
% If your paper is too long and you resort to formatting tricks to make it fit, it is quite likely that it will be returned to you. The best way to retain readability if the paper is overlength is to cut text, figures, or tables. There are a few acceptable ways to reduce paper size that don't affect readability. First, turn on \textbackslash frenchspacing, which will reduce the space after periods. Next, move all your figures and tables to the top of the page. Consider removing less important portions of a figure. If you use \textbackslash centering instead of \textbackslash begin\{center\} in your figure environment, you can also buy some space. For mathematical environments, you may reduce fontsize {\bf but not below 6.5 point}.



\bibliography{aaai25}

\end{document}
