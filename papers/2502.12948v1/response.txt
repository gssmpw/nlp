\section{Related Work}
\subsubsection{Vision-Language Training in the medical domain.}
Many studies have explored building specialized models in the medical domain **Radford et al., "Learning to Generate Reviews and Multifaceted Ratings"]**, a vision-language model (VLM) was trained on 15M image-text pairs from the biomedical domain. Adapting these models to downstream tasks has been explored in various ways: a) End to end fine-tuning **Devlin et al., "BERT Pre-training of Deep Bidirectional Transformers for Language Understanding"]**, b) Prompt tuning **Brown et al., "Language Models play Hidden Language Games"](Brown et al., 2020)**, c) Linear probing. These methods still require additional training stages on a labeled "support" dataset for the downstream task, and can underperform in very low data cases, especially with complex tasks.  
Recently, multi-modal language models (MLLMs) have emerged as generalist models for various tasks **Jiang et al., "MedT5: A Multitask Framework for Medical Text Classification"]**. These are generative models capable of generating and processing long-form text. For instance, Med-Flamingo  **Liu et al., "MedFlamingo: An Open-Source Multimodal Few-Shot Learner Adapted to the Medical Domain"]** is an open-source, multimodal few-shot learner adapted to the medical domain. These models demonstrate impressive performance across a wide variety of tasks, such as visual question answering, classification, report generation, etc., on a wide range of modalities.


\subsubsection{Synthetic Data Generation.}

Augmentation with synthetic data has the potential to mitigate the issue of limited domain data in medical imaging. However, generating high-quality synthetic medical images remains challenging. Clip-Medfake  **Rombach et al., "High-Resolution Image Synthesis with Latent Diffusion Models"]** leverages Stable Diffusion  **Nichol et al., "GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models"]** to generate synthetic images, which are then used to pretrain a CLIP model before fine-tuning on actual medical data. Latte-CLIP  **Khalidi et al., "Latte-CLIP: A Multimodal Language Model for Medical Image Analysis"]** uses MLLMs to generate descriptive text for domain-specific images, which are then used for CLIP model fine-tuning. Data synthesis in these methods is done by pre-trained models or models trained from a small portion of the original training data.  They lack fine grained control over the synthesis process. CtrlSynth  **Hussein et al., "Ctrl-Synth: Controllable Synthetic Medical Image Generation using Latent Space Editing"]** utilizes pre-trained models to identify concepts within an image, systematically alter these attributes, and generate synthetic text with LLMs. This synthetic text is then converted into images through Stable Diffusion, creating a comprehensive synthetic dataset that supports model training on controlled variations.


\subsubsection{LGE Detection.}

There is no consensus regarding the optimal method for LGE analyses. Commonly used manual and semi-automatic methods clinically include manual planimetry, the Full Width Half Maximum (FWHM) approach **Saeed et al., "Quantification of Left Ventricular Function Using Full-Width at Half-Maximum"]**, and n-std from remote myocardium. Comparative studies examining these methods  **Wang et al., "Comparative Analysis of Manual Planimetry, FWHM, and n-std Methods for LGE Quantification in Cardiac MRI"]** reveal significant variability in quantification results, highlighting issues with both reliability and reproducibility.


Recently, there have been many DL-based studies focused on automated scar detection from cardiac LGE images **Kim et al., "Automated Scar Detection in Cardiac MRI using Deep Learning: A Systematic Review"]**. For instance, Kim et al.  **Kim et al., "Automated Scar Detection in Cardiac MRI using Deep Learning: A Systematic Review"]** leveraged segmental information to train models that identify the presence or absence of LGE by transforming the images into polar coordinates based on the left ventricular (LV) center and the right ventricular (RV) insertion points. Additionally, several studies have explored infarct segmentation on the publicly available EMIDEC dataset  **Khalidi et al., "Latte-CLIP: A Multimodal Language Model for Medical Image Analysis"]** containing 150 patients (100 for training), achieving classification accuracies as high as 0.92  and a Dice coefficient of up to 0.71 for infarct segmentation **Khaled et al., "Automated Infarct Segmentation in Cardiac MRI using Deep Learning"]** . These models are trained on dense pixel-wise annotations of myocardial and scar tissue provided by clinical experts, thereby enhancing their ability to accurately segment scar regions.