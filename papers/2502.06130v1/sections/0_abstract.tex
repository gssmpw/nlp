\begin{abstract}
  % \yaqi{Better not to use ``although'' as the first word.} Although recent Large Vision-Language Models (LVLMs) have shown remarkable performance in multi-modal tasks, they are prone to generating hallucinatory text responses that do not align with the given visual input, which restricts their practical applicability in real-world scenarios. In this work, we explore the potential of leveraging powerful text-to-image generative models to assist mitigating hallucinations in LVLMs.\yaqi{Why? Briefly explain the intuition /motivation before ``in this work''.} We present a pilot study that demonstrates generative models can function as zero-shot hallucination detectors, offering feedback at both the response and token levels. \yaqi{This pilot study sentence is redundant, especially in the abstract. Maybe combine with the previous sentence. We want to explain why we use generative model as feedback (e.g., inspired by ...), no need to explicitly state the pilot study.} Building on this insight, we introduce the self-correcting Decoding with Generative Feedback (DeGF), a novel training-free decoding algorithm that  mitigates hallucinations by incorporating generative feedback. Specifically, DeGF generates an image from the initial response produced by LVLMs, which acts as an auxiliary visual reference to correct (\textit{confirm} or \textit{revise}) the initial response. Extensive experimental results validate the effectiveness of our approach in mitigating diverse types of hallucinations in LVLMs, consistently surpassing state-of-the-art methods across two evaluated LVLMs and five benchmarks.

  % \yaqi{The overall flow of the abstract needs improvement. The pilot study is not our inspiration; we had an intuition first, and then conducted the study to validate itâ€”not the other way around. Before stating "In this work," we should include some reasoning behind why we pursued this study, then introduce the method. We don't need to mention the study in the abstract; instead, we could frame our findings as a discovery if that improves the flow.}

\looseness=-1
\vspace{-4pt}
While recent Large Vision-Language Models (LVLMs) have shown remarkable performance in multi-modal tasks, they are prone to generating hallucinatory text responses that do not align with the given visual input, which restricts their practical applicability in real-world scenarios. In this work, inspired by the observation that the text-to-image generation process is the inverse of image-conditioned response generation in LVLMs, we explore the potential of leveraging text-to-image generative models to assist in mitigating hallucinations in LVLMs. We discover that generative models can offer valuable self-feedback for mitigating hallucinations at both the response and token levels. Building on this insight, we introduce self-correcting Decoding with Generative Feedback (DeGF), a novel training-free algorithm that incorporates feedback from text-to-image generative models into the decoding process to effectively mitigate hallucinations in LVLMs. Specifically, DeGF generates an image from the initial response produced by LVLMs, which acts as an auxiliary visual reference and provides self-feedback to verify and correct the initial response through complementary or contrastive decoding. Extensive experimental results validate the effectiveness of our approach in mitigating diverse types of hallucinations, consistently surpassing state-of-the-art methods across six benchmarks. Code is available at \url{https://github.com/zhangce01/DeGF}.
\end{abstract}
