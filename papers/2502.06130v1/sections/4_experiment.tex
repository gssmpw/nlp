% \clearpage

\section{Experiments}
\label{sec:experiment}
In this section, we evaluate the effectiveness of our method in mitigating hallucinations in LVLMs across a range of benchmarking scenarios, comparing it with existing state-of-the-art approaches. 


\subsection{Experimental Settings}
\looseness=-1
\textbf{Evaluated LVLMs}. \RebuttalRevision{We evaluate the effectiveness of our method on three state-of-the-art open-source LVLMs: LLaVA-1.5~\citep{liu2024improved}, InstructBLIP~\citep{dai2024instructblip}, and Qwen-VL~\citep{bai2023qwen}}. Both LLaVA-1.5 and InstructBLIP utilize Vicuna-7B~\citep{chiang2023vicuna} as the language encoder, which is instruction-tuned from LLaMA~\citep{touvron2023llama}. In contrast, Qwen-VL~\citep{bai2023qwen} is based on the Qwen 7B backbone. Specifically, we implement our approach using weights of the Qwen-VL-Chat model.

% LLaVA-1.5~\citep{liu2023visual} employs a pre-trained CLIP ViT-L/14~\citep{radford2021learning} as the vision encoder, and trains a linear mapping layer to connect the vision and language modalities. In contrast, InstructBLIP~\citep{dai2024instructblip} builds on a pre-trained BLIP-2~\citep{li2023blip} and incorporates an instruction-aware Q-Former module to bridge the modalities.


\textbf{Benchmarks}. We conduct extensive experiments on six benchmarks: (1) \textbf{POPE~\citep{li2023evaluating}} is a widely used benchmark for assessing object hallucinations in LVLMs, which tests the models with yes-or-no questions regarding the presence of specific objects, such as, ``\texttt{Is there a \{object\} in the image?}'' (2) \textbf{CHAIR~\citep{rohrbach2018object}} evaluates object hallucinations in open-ended captioning tasks. It prompts the LVLMs to describe specific images selected from a random sample of 500 images from the MSCOCO validation set; (3) \textbf{MME-Hallucination~\citep{fu2023mme}} is a comprehensive benchmark for LVLMs consisting of four subsets: \textit{existence} and \textit{count} for object-level hallucinations, and \textit{position} and \textit{color} for attribute-level hallucinations; (4) \RebuttalRevision{\textbf{MMBench~\citep{liu2025mmbench}} serves as a comprehensive benchmark designed to assess the multi-modal understanding capabilities of LVLMs across 20 dimensions}; (5) \textbf{MMVP~\citep{tong2024eyes}} collects CLIP-blind pairs and evaluates the fine-grained visual recognition capabilities of LVLMs. It consists of 150 image pairs, each accompanied by a binary-option question; (6) \textbf{LLaVA-Bench} provides 24 images featuring complex scenes, memes, paintings, and sketches, along with 60 challenging questions.

\input{tabs/pope}

\textbf{Baselines}. \RebuttalRevision{As a simple baseline, we include results from regular decoding, where the next token is sampled directly from the post-softmax probability distribution.} Additionally, we compare the performance of our method with three state-of-the-art decoding approaches: VCD~\citep{leng2024mitigating}, M3ID~\citep{favero2024multi}, and RITUAL~\citep{woo2024ritual}. \RebuttalRevision{For evaluations on the CHAIR~\citep{rohrbach2018object} and MME-Hallucination~\citep{fu2023mme} benchmark, we further include comparisons with Woodpecker~\citep{chen2024halc}, HALC~\citep{chen2024halc}, DoLa~\citep{chuang2024dola} and OPERA~\citep{huang2024opera}.} We report the performance of these baselines based on our re-implementation using their released code bases.

\looseness=-1
\textbf{Implementation Details}. In our experiments, we adhere to the default query format for the input data used in both LLaVA-1.5~\citep{liu2024improved} and InstructBLIP~\citep{dai2024instructblip}. Additionally, we set $\alpha_1 = 3$, $\alpha_2 = 1$, and $\gamma = 0.1$ by default in our decoding process. We follow VCD~\citep{leng2024mitigating} to implement adaptive plausibility constraints~\citep{li2023contrastive}, where we set $\beta=0.1$ in open-ended CHAIR benchmark and $\beta=0.25$ for other tasks.
To ensure the reliability of our results, we conduct MME experiments three times with different initialization seeds and report the mean accuracy along with the standard deviation. All experiments are conducted on a single 48GB NVIDIA RTX 6000 Ada GPU. More implementation details are provided in Section~\ref{sec:detail} of the Appendix.




\subsection{Results and Discussions}
\looseness=-1
\textbf{Results on POPE}. In Table~\ref{tab:POPE}, we compare the performance of our method against other baselines on the POPE benchmark under three different negative sampling settings, across three datasets. 
As shown in the table, our method consistently outperforms other decoding methods on both LVLMs, achieving state-of-the-art accuracies across all 18 settings, with improvements of up to 5.24\% in accuracy, 6.33\% in precision, and 2.79\% in F1 score compared to the second-best approach. This suggests that incorporating a generative reference enables the LVLMs to perceive more fine-grained visual details, thereby effectively addressing object hallucinations. Moreover, while most decoding methods tend to be overconfident in their responses, the self-correcting decoding mechanism in our method makes it more conservative in responding \texttt{Yes}, as evidenced by significantly higher precision across all settings. This highlights its enhanced performance in filtering out false positives and suppressing misinformation.

Another notable finding is that our method shows significantly improved performance in the \textit{popular} and \textit{adversarial} settings, which are more challenging than the \textit{random} setting. In the \textit{popular} and \textit{adversarial} settings, non-existent negative objects frequently appear and co-occur with other objects~\citep{li2023evaluating}, making them more susceptible to hallucination by LVLMs, as evidenced by the varying degrees of performance degradation across all baselines. However, our method exhibits a lower performance drop compared to other baselines, demonstrating its effectiveness in addressing hallucinations arising from object co-occurrence.

\input{tabs/chair}
\input{tabs/mme}


\textbf{Results on CHAIR}. We also compare the performance of our methods and other state-of-the-art methods in the open-ended captioning task and report the CHAIR scores, recall, and the average length of responses in Table~\ref{tab:CHAIR}, \RebuttalRevision{Table~\ref{tab:CHAIR-128}, and Table~\ref{tab:CHAIR-256}}. The results, evaluated across two different LVLMs, consistently demonstrate performance improvements achieved by our method over the compared approaches. Specifically, our method outperforms the second-best approach by 3.0\% and 2.6\% on the CHAIR$_S$ metric, while also enhancing the detailedness of generated responses compared to regular decoding, as indicated by the higher recall and increased response length.
These results demonstrate that by incorporating generative feedback into the decoding process of LVLMs, our method effectively mitigates object hallucinations in open-ended captioning tasks.



% \clearpage
\begin{figure}[t]
\begin{minipage}[t]{0.49\linewidth}
\includegraphics[width=\linewidth]{figs/mmvp.png}
\vspace{-21pt}
\caption{\looseness=-1\textbf{Results on MMVP~\citep{tong2024eyes}}. We apply our approach to LLaVA-1.5~\citep{liu2024improved} and compare its performance against other hallucination mitigation methods.}
\label{fig:mmvp}
\end{minipage}
\hspace{2pt}
\begin{minipage}[t]{0.49\linewidth}
\makeatletter\def\@captype{table}
\vspace{-128pt}
\small
\begin{center}
\resizebox{\linewidth}{!}{
\renewcommand{\arraystretch}{1.05}
\begin{tabular}{lcccc}
    \toprule
      \multirow{2}{*}[-0.5ex]{\textbf{Method}}  &  \multicolumn{2}{c}{\textbf{LLaVA-1.5}} & \multicolumn{2}{c}{\textbf{InstructBLIP}} \\
     \cmidrule(lr){2-3}\cmidrule(lr){4-5}
         & Acc. $\uparrow$ & Det. $\uparrow$ & Acc. $\uparrow$ & Det. $\uparrow$ \\
     \midrule
     Regular & 2.88 & 3.29 & 3.42 & 3.96 \\
     \cc \textbf{Ours} & \cc \textbf{4.29} & \cc \textbf{4.54}  & \cc \textbf{4.38} & \cc \textbf{4.79} \\
     \midrule
      VCD & 3.62 & 3.83  & 3.71 & 4.21\\
      \cc \textbf{Ours} & \cc \textbf{4.04} & \cc \textbf{4.38}  & \cc \textbf{4.17} & \cc \textbf{4.58} \\
      \midrule
      M3ID & 3.88 & 4.08 & 4.00 & 4.33 \\
      \cc \textbf{Ours} & \cc \textbf{4.04} & \cc \textbf{4.29}  & \cc \textbf{4.08} & \cc \textbf{4.50} \\
    \bottomrule
\end{tabular}
}
\vspace{2pt}
\caption{\RebuttalRevision{\textbf{GPT-4V-aided evaluation on LLaVA-Bench}. Higher accuracy and detailedness ($\uparrow$) indicate better performance. The evaluation is performed on LLaVA-1.5~\citep{liu2024improved}.}}
\label{table:gpt4v}
\end{center}
\end{minipage}
\vspace{-3pt}
\end{figure}

\textbf{Results on MME-Hallucination and MMBench}. Beyond object hallucinations, we further compare the performance of our method with other approaches using the more comprehensive MME-Hallucination benchmark, which includes both object-level and attribute-level hallucinations. The results in Table~\ref{tab:MME} and Table~\ref{tab:MME-full}
demonstrate that our method significantly outperforms the compared methods, with substantial margins in the total score metric (\eg, +18.19 on LLaVA-1.5 and +21.11 on InstructBLIP) and consistently superior performance across various evaluation settings, achieving the best results in 6 out of 8 settings. Moreover, our method shows notable improvements on the attribute-level \textit{color} subset,  which is particularly challenging as it requires models to accurately capture subtle attribute information. This further illustrates the effectiveness of our approach in addressing a wide range of hallucinations, both at the object existence level and in finer-grained attribute recognition. \RebuttalRevision{Additionally, our proposed DeGF enhances the general multi-modal understanding capabilities of LVLMs, as evidenced by its superior performance on the MMBench benchmark.}


% \begin{wrapfigure}[18]{r}{0.5\textwidth}
% \centering
% \vspace{-15pt}
% \includegraphics[width=\linewidth]{figs/mmvp.png}
% \vspace{-18pt}
% \caption{\textbf{Results on MMVP~\citep{tong2024eyes} benchmark}. We apply our approach to LLaVA-1.5~\citep{liu2023visual} and compare its performance against other hallucination mitigation methods (VCD, M3ID, and RITUAL). For reference, we also report the performance of other LVLMs.}
% \label{fig:mmvp}
% \end{wrapfigure}
\textbf{Results on MMVP}. We conduct experiments on the MMVP benchmark to assess the fine-grained visual recognition capabilities of LVLMs. As shown in Figure~\ref{fig:mmvp}, applying our self-correcting decoding approach to LLaVA-1.5 significantly improves performance from 22.67\% to 27.33\%.
Our approach also demonstrates notable advantages over other hallucination mitigation baselines, further showcasing its superiority in handling nuanced visual recognition tasks.
These results suggest that our approach significantly enhances the model’s capacity to discern and correctly interpret fine-grained distinctions between images with similar appearances but different contents. By integrating generative feedback, our approach effectively reduces misinterpretations and improves the precision of visual recognition tasks, contributing to more reliable and accurate performance in complex scenarios.



% \begin{figure}[t]
% \centering
% \includegraphics[width=0.6\linewidth]{figs/mmvp.png}
% \caption{\textbf{Results on MMVP~\citep{tong2024eyes} benchmark}. We apply our approach to LLaVA-1.5~\citep{liu2023visual} and compare its performance against other hallucination mitigation methods. For reference, we also report the performance of other LVLMs: MiniGPT-4~\citep{zhu2023minigpt}, InstructBLIP~\citep{dai2024instructblip}, and Bard.}
% \label{fig:mmvp}
% \end{figure}


% \begin{wraptable}[11]{r}{0.5\textwidth}
% \small
% \begin{center}
% \vspace{-24pt}
% \caption{\textbf{GPT-4V-aided evaluation on LLaVA-Bench}. Higher scores ($\uparrow$) indicate better performance. }
% \vspace{-5pt}
% \label{table:gpt4v}
% \resizebox{\linewidth}{!}{
% \begin{tabular}{lcccc}
%     \toprule
%       \multirow{2}{*}[-0.5ex]{\textbf{Method}}  &  \multicolumn{2}{c}{\textbf{LLaVA-1.5}} & \multicolumn{2}{c}{\textbf{InstructBLIP}} \\
%      \cmidrule(lr){2-3}\cmidrule(lr){4-5}
%          & Acc. $\uparrow$ & Det. $\uparrow$ & Acc. $\uparrow$ & Det. $\uparrow$ \\
%      \midrule
%      Regular & 2.88 & 3.29 & 3.42 & 3.96 \\
%      \cc \textbf{Ours} & \cc \textbf{4.29} & \cc \textbf{4.54}  & \cc \textbf{4.38} & \cc \textbf{4.79} \\
%      \midrule
%       VCD & 24.4 & 7.9  & 61.8 & 54.2\\
%       \cc \textbf{Ours} & \cc \textbf{4.29} & \cc \textbf{4.54}  & \cc \textbf{4.38} & \cc \textbf{4.79} \\
%       \midrule
%       M3ID & 53.5 & 30.8 & 10.4 & 62.6 \\
%       \cc \textbf{Ours} & \cc \textbf{18.4} & \cc \textbf{6.1}  & \cc {62.7} & \cc {62.7} \\
%     \bottomrule
% \end{tabular}
% }
% \end{center}
% \end{wraptable}
\looseness=-1
\textbf{Results on LLaVA-Bench}. In Figure~\ref{fig:llavabench}, we present a case study on LLaVA-Bench comparing our method's response with the response generated by regular decoding using the LLaVA-1.5 model. Specifically, regular decoding often leads to hallucinated or inaccurate content, such as describing ``\texttt{the island below the mountain}''. 
Besides, the response generated by regular decoding tends to focus on elements like the ``\texttt{cloudy sky}'' and ``\texttt{cohesive and captivating island landscape}'' without providing specific information about the central features of the image. In contrast, our response is more detailed, mentioning the volcano, the road, the surrounding greenery, and the inhabited areas, which gives a clearer understanding of the image's content. \RebuttalRevision{The GPT-4V-aided evaluation shown in Table~\ref{table:gpt4v} further confirms that our method enhances both the accuracy and detailedness of the generated response, outperforming other hallucination mitigation approaches such as VCD and M3ID.}
Due to the page limit, please refer to Section~\ref{sec:case} of the Appendix for more case studies.



\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figs/llavabench.png}
\vspace{-18pt}
\caption{\textbf{Case study on the LLaVA-Bench benchmark}. We compare the responses generated by regular decoding and our method using LLaVA-1.5. GPT-4V-aided evaluation results are also provided alongside the responses. Hallucinated and accurate content is highlighted in \textcolor{darkred}{red} and \textcolor{darkgreen}{green}.} 
% \martin{Green-code correct parts corresponding to the red parts, \eg, island is located near the ocean?}
\vspace{-12pt}
\label{fig:llavabench}
\end{figure}

\input{tabs/ablation}

% \vspace{-5pt}
\subsection{Ablation Studies}
% \vspace{-3pt}
\textbf{Analysis of Distance Threshold $\gamma$}. In Section~\ref{sec:token}, we introduce a distance threshold $\gamma$ to determine the appropriate decoding algorithm for each generated token. Table~\ref{tab:gamma} presents an analysis of our method's performance with various values of $\gamma$ across three benchmarks. For simplicity, we report the performance on the MS-COCO dataset with \textit{random} setting for all POPE results in the ablation studies.
Notably, when $\gamma$ is set to either 0 or 1—corresponding to the exclusive use of contrastive or complementary decoding for all tokens—the performance exhibits a significant decline, by 0.6\% and 1.1\% in POPE accuracy, respectively. Moreover, our default setting of $\gamma=0.1$ achieves the optimal performance in 3 out of 4 evaluated metrics. Additional sensitivity analyses for other hyperparameters are provided in Section~\ref{sec:moreablation} of the Appendix.

\textbf{Effects of Different Generative Models}. Table~\ref{tab:generative} presents the performance of various variants of our method that incorporate different generative models (\ie, different versions of Stable Diffusion) while using the same LLaVA-1.5 backbone. The results indicate that the effectiveness of our DeGF is robust to the choice of generative models, as performance remains largely unaffected by the specific model used, and all variants demonstrate consistent improvements over the original regular decoding approach.
Although utilizing SD-XL-v1.0~\citep{podell2024sdxl} yields slightly better performance, we opt for SD-v1.5 as the default due to its faster image generation speed (3.8 s/image \vs 11.3 s/image).

\subsection{Efficiency Comparison}
\begin{wraptable}[11]{r}{0.5\textwidth}
\small
\begin{center}
\vspace{-16pt}
\caption{\RebuttalRevision{\textbf{Efficiency comparison}. For each method, we present the average inference latency per instance and peak GPU memory. Experiments are conducted on a single RTX A6000 Ada GPU.}}
\label{tab:efficiency}
\vspace{-5pt}
\resizebox{\linewidth}{!}{
\begin{tabular}{lcccc}
\toprule
Method  & Avg. Latency $\downarrow$ & GPU Memory $\downarrow$ & CHAIR$_S$ $\downarrow$  \\ 
\midrule
Regular  &  3.44 s {\tiny ($\times$1.00)} &  15778 MB  {\tiny ($\times$1.00)} & 55.0\\
VCD &  6.91 s {\tiny ($\times$2.01)} &  16634 MB  {\tiny ($\times$1.05)}  & 54.4 \\
OPERA & 24.70 s  {\tiny ($\times$7.18)} &  22706 MB  {\tiny ($\times$1.44)}& 52.6 \\
Woodpecker & 10.68 s {\tiny ($\times$3.10)} & 22199 MB  {\tiny ($\times$1.41)} & 57.6 \\
HALC & 22.61 s {\tiny ($\times$6.51)} &  23084 MB {\tiny ($\times$1.46)}& 51.0 \\
\rowcolor{gray!20}
\textbf{Ours} &  13.89 s {\tiny ($\times$4.04)}  & 19119 MB  {\tiny ($\times$1.21)}& 48.8\\
\bottomrule
\end{tabular}
}
\end{center}
\end{wraptable}

\RebuttalRevision{
In Table~\ref{tab:efficiency}, we compare the efficiency of our approach with other methods on the CHAIR benchmark using the LLaVA-1.5 model, with the maximum token length set to 128. Our approach involves two queries and incorporates a text-to-image generative model to mitigate hallucinations, resulting in a 4.04$\times$ increase in latency and a 1.21$\times$ increase in GPU memory usage. Specifically, our method consists of three stages: initial response generation, image generation, and response self-correction, which take an average of 3.4 seconds, 3.8 seconds, and 6.6 seconds per instance, respectively. Compared to other approaches, while our method is slower than regular decoding and contrastive decoding-based methods, it demonstrates efficiency advantages over OPERA and HALC. Note that our approach also achieves the lowest hallucination rates among all compared methods. In Appendix~\ref{subsec:speedup}, we discuss several strategies to accelerate our approach, such as limiting the length of the initial response and reducing the number of inference steps in the diffusion process.
}