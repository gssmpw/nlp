% \clearpage
\vspace{-3pt}
\section{Related Work}
\vspace{-2pt}
\looseness=-1
\textbf{Hallucination in LVLMs}.
% With the recent advances in LLMs' linguistic capabilities~\citep{touvron2023llama,chowdhery2023palm,brown2020language, chiang2023vicuna} and the growing potential of VLMs in multi-modal understanding~\citep{li2019visualbert,li2022blip,radford2021learning}, LVLMs have emerged as a powerful fusion of the two~\citep{chen2023shikra,liu2023visual,zhu2023minigpt,driess2023palm,alayrac2022flamingo}. 
% By training a modality connection module to align visual tokens with the textual embedding space of the LLM, LVLMs demonstrate unified decoding of visual and textual tokens, enabling their broad application in multimodal tasks such as visual question answering~\citep{liu2024survey}.
With advances of autoregressive LLMs~\citep{touvron2023llama,chowdhery2023palm,chiang2023vicuna}, researchers have extended these powerful models to process visual inputs, leading to the development of LVLMs~\citep{liu2023visual,dai2024instructblip,bai2023qwen,ye2024mplug}. These models typically train a modality alignment module to project visual tokens into the textual embedding space of the LLM, demonstrating impressive performance in various multi-modal tasks such as visual question answering and image captioning~\citep{liu2024survey,bai2024hallucination}.
However, LVLMs are prone to hallucinations, where contradictions arise between the visual content and the generated textual response~\citep{li2023evaluating, liu2024survey, bai2024hallucination}.

\looseness=-1
To mitigate hallucinations in LVLMs, early works have introduced various approaches, including reinforcement learning from human feedback (RLHF)~\citep{gunjal2024detecting, sun2023aligning}, applying auxiliary supervision~\citep{jiang2024hallucination, chen2023mitigating}, incorporating negative~\citep{liu2023mitigating} or noisy data~\citep{yue-etal-2024-less}, and training post-hoc revisors for correction~\citep{zhou2024analyzing, yin2023woodpecker}. Despite promising results, these methods often lack practicality due to their reliance on additional data and costly training processes. To address this, another line of work focuses on training-free methods that can be seamlessly integrated into existing LVLMs.
Such methods encompass contrastive decoding~\citep{leng2024mitigating, favero2024multi} and guided decoding with auxiliary information~\citep{chen2024halc,deng2024seeing,woo2024ritual}. 
In this work, we present a novel training-free approach that recursively enhances the accuracy of the LVLM response by incorporating text-to-image generative feedback. 
To the best of our knowledge, we are the first work to effectively utilize feedback from text-to-image generative models to mitigate hallucinations in LVLMs.

% With the recent advances in LLMs' linguistic capabilities~\citep{touvron2023llama,chowdhery2023palm,brown2020language, chiang2023vicuna} and the growing potential of VLMs in multi-modal understanding~\citep{li2019visualbert,li2022blip,radford2021learning}, LVLMs have emerged as a powerful fusion of the two~\citep{chen2023shikra,liu2023visual,zhu2023minigpt,driess2023palm,alayrac2022flamingo}. 
% By training a modality connection module to align visual tokens with the textual embedding space of the LLM, LVLMs demonstrate unified decoding of visual and textual tokens, enabling their broad application in multi-modal tasks such as visual question answering~\citep{liu2024survey}.

% However, the aforementioned methods fail to consider both the inclusion of reasonable visual information and the adaptation of token decoding. In contrast, \Ours{} leverages generated images as effective visual references, explicitly assisting LVLMs in mitigating hallucinations.

% \todo{Mention previous work: Img-Diff: Contrastive Data Synthesis for Multimodal Large Language Models}

\textbf{Text-to-Image Synthesis}.
Text-to-image synthesis aims to create realistic images from textual descriptions~\citep{zhu2019dm, ge2023expressive}.
In recent years, significant progress has been achieved in this area, largely due to the advent of deep generative models~\citep{zhan2023multimodal, goodfellow2014generative}. 
These advances include Generative Adversarial Networks (GAN)~\citep{sauer2023stylegan, kang2023scaling}, autoregressive models~\citep{chang2023muse, yu2022scaling}, and diffusion models~\citep{ho2020denoising,karras2022elucidating,nichol2022glide,saharia2022photorealistic, rombach2021highresolution}.
Among these, diffusion-based methods have been particularly distinguished due to their ability to generate high-quality, detailed images with fine-grained control over the synthesis process~\citep{yang2023diffusion,croitoru2023diffusion}.
Pre-trained on large-scale text-image datasets such as LAION~\citep{schuhmann2022laion}, diffusion-based methods have demonstrated strong vision-language alignment, making them valuable for downstream tasks such as classification~\citep{li2023your} and semantic segmentation~\citep{amit2021segdiff, wolleb2022diffusion}. 
% \zifu{More recently, diffusion models have been used to generate contrastive data for fine-tuning LVLMs, resulting in comprehensive improvements~\citep{jiao2024img}. However, these improvements stem from the data itself rather than advancements in LVLMs.} \martin{Previous sentence needs clarification.}

More recently, \citet{jiao2024img} incorporate text-to-image generative models to enhance fine-grained image recognition in LVLMs by introducing the Img-Diff dataset, which generates pairs of similar images using Stable Diffusion XL~\citep{podell2024sdxl}. Their results demonstrate that fine-tuning LVLMs with this additional data leads to improved performance on several VQA tasks.
In contrast, in this work, we directly leverage a pre-trained diffusion model to provide valuable self-feedback for refining the generated responses of LVLMs in the decoding process, dynamically improving the accuracy and consistency of the modelâ€™s response without modifying the underlying LVLMs. 
% Through empirical evaluations, we demonstrate that with generative feedback, our proposed training-free DeGF method effectively mitigates various types of hallucinations in LVLMs.