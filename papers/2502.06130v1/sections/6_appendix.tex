\clearpage
\appendix
\renewcommand{\thesection}{\Alph{section}}
\renewcommand\thefigure{\Alph{section}\arabic{figure}} 
\renewcommand\thetable{\Alph{section}\arabic{table}}  
\setcounter{section}{0}
\setcounter{figure}{0} 
\setcounter{table}{0} 

{\LARGE\sc Self-Correcting Decoding with Generative Feedback for Mitigating Hallucinations in Large Vision-Language Models\par}

{\LARGE\sc Appendix\par} \vspace{10pt}

In this supplementary document, we provide additional details and experimental results to enhance understanding and insights into our method.
This supplementary document is organized as follows:
\begin{itemize}[leftmargin=0.5cm, itemindent=0cm, itemsep=4pt,topsep=4pt,parsep=0pt]
    \item The limitations and broader impacts of this work are discussed in Section~\ref{sec:limitation}.
    \item Additional experimental details, including further implementation details, descriptions of other implemented baselines, and license information for the utilized code and datasets, are provided in Section~\ref{sec:detail}.
    \item Additional experimental results are presented in Section~\ref{sec:moreablation}.
    \item More case studies and GPT-4V-aided evaluations are provided in Section~\ref{sec:case}.
    \item Potential directions for future work are discussed in Section~\ref{sec:future}.
\end{itemize}

\section{Limitations and Broader Impacts}
\label{sec:limitation}
\textbf{Limitations}. Although our method effectively mitigates hallucinations in LVLMs, it relies on pre-trained text-to-image generative models, which introduces additional computational complexity. The process of generating images also adds time, potentially slowing down LVLM response generation and making it less suitable for real-time applications. However, our method is training-free, reducing the overhead typically associated with fine-tuning large models, and offering broader applicability across various tasks. Moreover, the use of generative feedback improves the model's ability to verify and correct responses, particularly in complex scenarios. Thus, while the computational trade-offs may limit real-time performance, our method excels in settings where accuracy and reliability are prioritized over speed. We also hope that advances in efficient diffusion-based models will improve the feasibility of our approach in real-world applications in the future.

\textbf{Broader Impacts}. In this work, our goal is to develop more reliable large vision-language models (LVLMs) by incorporating feedback from generative models. By using this feedback mechanism, we aim to address a critical issue faced by current multi-modal models: hallucinations, where models produce responses that are inconsistent with the visual input. Hallucinations not only degrade model performance but also pose risks in real-world applications by generating inaccurate or misleading information. Our approach leverages the strengths of generative models to detect and mitigate these hallucinations, improving the overall accuracy and reliability of LVLMs. In doing so, we contribute to enhancing trustworthiness and reducing the spread of misinformation in systems that rely on multi-modal AI, making them safer and more effective for a wide range of applications.

\section{More Experimental Details}
\label{sec:detail}
\subsection{Benchmarks and Metrics}
We conduct extensive experiments on the following benchmarks:
\begin{itemize}
    \item \looseness=-1 \textbf{POPE~\citep{li2023evaluating}} is a widely used benchmark for assessing object hallucinations in LVLMs. It tests the models with yes-or-no questions regarding the presence of specific objects, such as, ``\texttt{Is there a \{object\} in the image?}'' The benchmark draws data from three existing datasets: MSCOCO~\citep{lin2014microsoft}, A-OKVQA~\citep{schwenk2022okvqa}, and GQA~\citep{hudson2019gqa}, and comprises three distinct subsets—\textit{random}, \textit{popular}, and \textit{adversarial}—based on how the negative samples are generated. For each dataset setting, the benchmark provides 6 questions per image, resulting in 3,000 test instances. We evaluate the performance of different methods using four metrics: accuracy, precision, recall, and F1 score.
    \item \textbf{CHAIR~\citep{rohrbach2018object}} evaluates object hallucinations in open-ended captioning tasks. It prompts the LVLMs to describe specific images selected from a random sample of 500 images from the MSCOCO validation set and assesses performance based on two metrics:
    \begin{equation}
    \label{eq:chair metrics}
        \text{CHAIR}_I =  \frac{\text{\# hallucinated objects}}{\text{\# all objects mentioned}}, \quad\text{CHAIR}_S =  \frac{\text{\# sentences with hallucinated object}}{\text{\# all sentences}}.
    \end{equation}
    Additionally, we assess the recall and the average length of the generated responses.
    \item \textbf{MME-Hallucination~\citep{fu2023mme}} is a comprehensive benchmark for LVLMs consisting of four subsets: \textit{existence} and \textit{count} for object-level hallucinations, and \textit{position} and \textit{color} for attribute-level hallucinations. Each subset includes 30 images and 60 questions, with two questions per image. Similar to POPE~\citep{li2023evaluating}, these questions are structured as yes-or-no queries, and performance is assessed based on binary accuracy. Following the official implementation, the reported score is calculated by combining accuracy and accuracy+, where accuracy is based on individual questions, and accuracy+ is based on images where both questions are answered correctly.
    \item \textbf{MMBench~\citep{liu2025mmbench}} is a comprehensive evaluation benchmark designed to assess the multimodal understanding and reasoning capabilities of AI models. It focuses on tasks requiring the integration of visual and textual information, testing a model's ability to handle diverse, real-world scenarios. In particular, MMBench employs a hierarchical ability taxonomy, designating Perception and Reasoning as Level-1 (L-1) abilities. It further refines the taxonomy by incorporating more detailed ability dimensions, organizing them into six Level-2 (L-2) and twenty Level-3 (L-3) dimensions.
    \item \textbf{MMVP~\citep{tong2024eyes}} collects CLIP-blind pairs and evaluates the fine-grained visual recognition capabilities of LVLMs. It consists of 150 image pairs, each accompanied by a binary-option question. Each image is queried independently, and for a given pair, the LVLM's response is considered correct only if both associated questions are answered accurately.
    \item \textbf{LLaVA-Bench\footnote{\url{https://huggingface.co/datasets/liuhaotian/llava-bench-in-the-wild}.}} provides 24 images featuring complex scenes, memes, paintings, and sketches, along with 60 challenging questions. We select examples from this dataset to provide qualitative comparisons between the responses generated by different decoding methods. We also follow \citet{yin2023woodpecker} to evaluate the accuracy and detailedness of generated responses of different methods using the advanced LVLM, GPT-4V\footnote{\url{https://openai.com/index/gpt-4v-system-card}.}.
\end{itemize}

\subsection{More Implementation Details}
In our experiments, we adhere to the default query format for the input data used in both LLaVA-1.5~\citep{liu2023visual} and InstructBLIP~\citep{dai2024instructblip}. Additionally, we set $\alpha_1 = 3$, $\alpha_2 = 1$, and $\gamma = 0.1$ by default in our decoding process. We follow VCD~\citep{leng2024mitigating} to implement adaptive plausibility constraints~\citep{li2023contrastive}:
\begin{gather}
p_{\theta}(y_t) = 0, \quad \text{if} \,\,\, y_t \notin \mathcal{V}(y_{<t})\nonumber\\
    \text{where}\,\,\,\mathcal{V}(y_{<t}) = \{y_t \in \mathcal{V}: p_{\theta}(y_t | v, \mathbf{x}, \mathbf{y}_{<t}) \geq \beta \max_w  p_{\theta}(w | v, \mathbf{x}, \mathbf{y}_{<t})\}
    \label{eq:adaptive}
\end{gather}
Here, $\mathcal{V}$ is the whole vocabulary of LVLM, and hyperparameter $\beta \in [0, 1]$ controls the truncation of the next token distribution. A larger $\beta$ indicates more aggressive truncation, keeping only the high-probability tokens. In our implementation, we set the logits for $y_t \notin \mathcal{V}(y_{<t})$ to $-\infty$. By default, we set $\beta=0.1$ in the open-ended CHAIR benchmark and $\beta=0.25$ for other tasks. All experiments are conducted on a single 48GB NVIDIA RTX 6000 Ada GPU. 

Recall that in our method, we use a text-to-image generative model to reverse the image-to-text response generation process by producing a new image from the initial response. To ensure the new image is both high-quality and relevant, we aim to generate specific descriptions for the given visual content. Thus, we slightly modify the initial query prompt for each evaluated benchmark: 
\begin{itemize}
    \item \textbf{POPE~\citep{li2023evaluating}, MME-Hallucination~\citep{fu2023mme}, and MMVP~\citep{tong2024eyes}}. In POPE, MME-Hallucination, and MMVP benchmarks, models are tested with yes-or-no/binary selection questions, such as, ``\texttt{Is there a \{object\} in the image?}'' To obtain more detailed explanations and descriptions of the original image, we modify the prompt by adding, ``\texttt{Briefly describe relevant details.}'' This encourages the model to provide not only a yes-or-no answer but also additional visual information.
    \item \textbf{CHAIR~\citep{rohrbach2018object}}. For the CHAIR benchmark, we retain the original prompt, ``\texttt{Please describe this image in detail.}'' as it effectively prompts the model to provide comprehensive visual details from the original image.
\end{itemize}
Note that for the second query, where both the original and generated images are used as input, we apply the original prompt to ensure a fair comparison.
% \clearpage


\subsection{Details of Other Baselines}
In this work, we mainly compare the performance of our DeGF with three state-of-the-art approaches: VCD~\citep{leng2024mitigating}, M3ID~\citep{favero2024multi}, and RITUAL~\citep{woo2024ritual}. The method and implementation details for these approaches are provided below:
\begin{itemize}
\item \textbf{VCD~\citep{leng2024mitigating}} contrasts output distributions derived from original and distorted visual inputs. Specifically, given a textual query ${x}$ and a visual input ${v}$, the model generates two distinct output distributions: one conditioned on the original ${v}$ and the other on the distorted visual input ${v'}$, which is derived by applying pre-defined distortions (i.e., Gaussian noise mask) to ${v}$. 
Then, a new contrastive probability distribution is computed by:
\begin{gather}
p_{vcd}\left(y_t\right) =\mathsf{Softmax}\left[ (1+\alpha) 
f_\theta\left(y | v, \mathbf{x}, \mathbf{y}_{<t}\right) -\alpha f_\theta\left(y | v', \mathbf{x}, \mathbf{y}_{<t}\right)\right].
\end{gather}
In our implementation, we follow the default setting in VCD~\citep{leng2024mitigating} and set $\alpha=1$ for reproduction. To generate $v'$, we use a total of 500 noise steps.

\item \textbf{M3ID~\citep{favero2024multi}} contrasts output distributions derived from original visual inputs and pure text inputs without visual information. The final probability distribution is
\begin{equation}
    p_{m3id}\left(y_t\right) = \mathsf{Softmax}\left[f_\theta\left(y | v, \mathbf{x}, \mathbf{y}_{<t}\right) + \frac{1-e^{-\lambda t}}{e^{-\lambda t}} \left( f_\theta\left(y | v, \mathbf{x}, \mathbf{y}_{<t}\right) - f_\theta\left(y |  \mathbf{x}, \mathbf{y}_{<t}\right)\right) \right].
\end{equation}
Similarly, we follow their recommended best practice and set the hyperparameter $\lambda$, which balances the conditioned model and unconditioned model, to $0.02$.


\item \textbf{RITUAL~\citep{woo2024ritual}} applies common image transformations (\eg, crop, flip, color jitter, \etc) to the original visual input $v$,
This results in a transformed version of the visual input, $v^{(T)}$. Then,  RITUAL utilizes both the original and transformed images to generate the response and this dual-input approach significantly reduces the likelihood of hallucinatory outputs.
The probability distribution is calculated as follows:
\begin{equation}
p_{ritual}\left(y_t\right) =\mathsf{Softmax}\left[f_\theta\left(y | v, \mathbf{x}, \mathbf{y}_{<t}\right) +\kappa f_\theta\left(y | v^{(T)}, \mathbf{x}, \mathbf{y}_{<t}\right)\right].
\label{eq:ritual_sampling}
\end{equation}
Here, $\kappa$ is a balancing hyperparameter, adjusting the contribution of the transformed input relative to the original. We follow their official implementation to set $\kappa=3$ in default.

\end{itemize}

\subsection{Dataset and Code Licensing}
\textbf{Datasets}. We list the known license information for the datasets below: POPE~\citep{li2023evaluating} and MMVP~\citep{tong2024eyes} benchmarks are licensed under MIT License.
CHAIR~\citep{rohrbach2018object} is made available under the BSD 2-Clause License.
LLaVA-Bench is available under Apache-2.0 License.
MME-Hallucination~\citep{fu2023mme} benchmark dataset is collected by Xiamen University for academic research only.

\textbf{Code}. In this work, we also use some code implementations from the existing codebase: LLaVA~\citep{liu2023visual} and VCD~\citep{leng2024mitigating} are licensed under the Apache-2.0 License.
InstructBLIP~\citep{dai2024instructblip} is under BSD-3-Clause License.  RITUAL~\citep{woo2024ritual} is licensed under MIT License.

\section{More Experimental Results and Analysis}
\label{sec:moreablation}




\begin{table}[t]
    % \begin{minipage}[t!]{0.5\textwidth}
        \begin{center}
        \begin{small}
        
        \setlength{\tabcolsep}{5pt} % base value: 6pt
        \caption{\RebuttalRevision{\textbf{Results on CHAIR~\citep{rohrbach2018object} benchmark.} We limit the maximum number of new tokens to 128. Lower ($\downarrow$) CHAIR$_S$, CHAIR$_I$ and higher ($\uparrow$) recall and length indicate better performance. The best results in each setting are \textbf{bolded}, and the second-best are \underline{underlined}.}}
        \vspace{-6pt}
        \label{tab:CHAIR-128}
        \resizebox{\textwidth}{!}{
        \begin{tabular}{lcccccccc}
            \toprule
              \multirow{2}{*}[-0.5ex]{\textbf{Method}}  &  \multicolumn{4}{c}{\textbf{LLaVA-1.5}} & \multicolumn{4}{c}{\textbf{InstructBLIP}} \\
             \cmidrule(lr){2-5}\cmidrule(lr){6-9}
                 & CHAIR$_S$ $\downarrow$ & CHAIR$_I$ $\downarrow$ & Recall $\uparrow$ & Length $\uparrow$   & CHAIR$_S$ $\downarrow$ & CHAIR$_I$ $\downarrow$ & Recall $\uparrow$ & Length $\uparrow$ \\
             \midrule
             Regular & 55.0 & 16.3 & 71.9 & \textbf{97.3} &  57.0 & 17.6  & 68.3 & \textbf{100.4}\\
              VCD & 54.4 & 16.6 & 75.1 & \underline{97.0} &  60.4 & 17.8  & \textbf{72.5} & 99.9\\
              M3ID & 56.6 & 15.7 & \textbf{76.8} & 94.5 & 62.2 & 18.1 & 71.9 & 99.8\\
              RITUAL & \underline{49.6} & \underline{14.8}  & 74.7 & 96.2 &  \textbf{48.4} & \underline{14.5} & \underline{72.2} & \underline{100.0}\\
              Woodpecker & 57.6 & 16.7  & 70.3 &93.2 & 60.8 & 17.6 & 69.7 & 97.6 \\
              HALC & 51.0 &  \underline{14.8} & 75.3& 95.8 & 53.8  & 15.7 & 71.9 & 99.1 \\
              \cc \textbf{Ours} & \cc \textbf{48.8} & \cc \textbf{14.6}  & \cc \underline{76.0} & \cc 96.4 & \cc  \underline{49.2} & \cc \textbf{14.4}  & \cc \underline{72.2}  & \cc 98.9 \\
            \bottomrule
        \end{tabular}
        
        }
        \end{small}
        \end{center}
        \vspace{-10pt}
\end{table}

\begin{table}[t]
    % \begin{minipage}[t!]{0.5\textwidth}
        \begin{center}
        \begin{small}
        
        \setlength{\tabcolsep}{5pt} % base value: 6pt
        \caption{\RebuttalRevision{\textbf{Results on CHAIR~\citep{rohrbach2018object} benchmark.} We limit the maximum number of new tokens to 256. Lower ($\downarrow$) CHAIR$_S$, CHAIR$_I$ and higher ($\uparrow$) recall and length indicate better performance. The best results in each setting are \textbf{bolded}, and the second-best are \underline{underlined}.}}
        \vspace{-6pt}
        \label{tab:CHAIR-256}
        \resizebox{\textwidth}{!}{
        \begin{tabular}{lcccccccc}
            \toprule
              \multirow{2}{*}[-0.5ex]{\textbf{Method}}  &  \multicolumn{4}{c}{\textbf{LLaVA-1.5}} & \multicolumn{4}{c}{\textbf{InstructBLIP}} \\
             \cmidrule(lr){2-5}\cmidrule(lr){6-9}
                 & CHAIR$_S$ $\downarrow$ & CHAIR$_I$ $\downarrow$ & Recall $\uparrow$ & Length $\uparrow$   & CHAIR$_S$ $\downarrow$ & CHAIR$_I$ $\downarrow$ & Recall $\uparrow$ & Length $\uparrow$ \\
             \midrule
             Regular & 58.0 & 17.7 & 74.1 & \textbf{106.3} & 61.0 &  18.2 & 68.9 & \textbf{112.0}\\
              VCD & 58.2 & 16.7 & \underline{78.0}& \underline{103.5} & 63.0& 18.6  & \textbf{72.9}  & \underline{106.3} \\
              M3ID & 56.8 & 16.1& \textbf{80.7} & 98.2 &65.8  & 19.9& \underline{72.4} & 102.7\\
              RITUAL & \underline{51.0} & \underline{15.1}  & 76.0 & 100.9 & \underline{50.4} & \underline{15.3} & 72.0 & 102.0\\
              \cc \textbf{Ours} & \cc \textbf{49.8} & \cc \textbf{14.7}  & \cc 77.2 & \cc 103.3 & \cc \textbf{49.8}   & \cc \textbf{15.1}  & \cc 72.3  & \cc 103.3  \\
            \bottomrule
        \end{tabular}
        
        }
        \end{small}
        \end{center}
        \vspace{-10pt}
\end{table}


\begin{table*}[t!]
    % \begin{minipage}[t!]{0.5\textwidth}
        \begin{center}
        \begin{small}
        \setlength{\tabcolsep}{6pt} % base value: 6pt
        \caption{\looseness=-1\RebuttalRevision{\textbf{Results on MME-Hallucination~\citep{fu2023mme} benchmark.} We report the average MME scores along with the standard deviation across three random seeds for each subset. We also report the total scores achieved by the different methods across all four subsets in the final column. Higher scores ($\uparrow$) indicate better performance. The best results  are \textbf{bolded}, and the second-best are \underline{underlined}.}}
        % \martin{consistent with confidence intervals; either include error bars in all tables, or include none in main text and add in Appendix?}}
        \label{tab:MME-full}
        \vspace{-5pt}
        \resizebox{\textwidth}{!}{
        \begin{tabular}{llccccc}
            \toprule
             \multirow{2}{*}[-0.5ex]{\textbf{Model}} & \multirow{2}{*}[-0.5ex]{\textbf{Method}}  &  \multicolumn{2}{c}{\textbf{Object-level}} & \multicolumn{2}{c}{\textbf{Attribute-level}} & \multirow{2}{*}[-0.5ex]{\textbf{Total Score $\uparrow$}}\\
             \cmidrule(lr){3-4}\cmidrule(lr){5-6}
              &   & Existence $\uparrow$ & Count $\uparrow$ & Position $\uparrow$ & Color $\uparrow$   & \\
             \midrule
            \multirow{7}{*}{{\textbf{LLaVA-1.5}}} & Regular &  173.75 {\tiny ($\pm4.79$)} & 121.67 {\tiny ($\pm12.47$)} & 117.92 {\tiny ($\pm3.69$)\phantom{0}} & 149.17 {\tiny ($\pm7.51$)\phantom{0}} &  562.50 {\tiny ($\pm3.96$)\phantom{0}} \\
             & DoLa  & 176.67 {\tiny ($\pm2.89$)} & 113.33 {\tiny ($\pm10.41$)} & 90.55 {\tiny ($\pm8.22$)} & 141.67 {\tiny ($\pm7.64$)\phantom{0}} &  522.22 {\tiny ($\pm16.78$)}  \\
             & OPERA  & 183.33 {\tiny ($\pm6.45$)} & 137.22 {\tiny ($\pm6.31$)\phantom{0}} & 122.78 {\tiny ($\pm2.55$)\phantom{0}} &155.00 {\tiny ($\pm5.00$)\phantom{0}}  &  598.33 {\tiny ($\pm10.41$)}  \\
             & VCD  & 186.67 {\tiny ($\pm5.77$)} & 125.56 {\tiny ($\pm3.47$)\phantom{0}} & 128.89 {\tiny ($\pm6.73$)\phantom{0}} & 139.45 {\tiny ($\pm12.51$)} &  580.56 {\tiny ($\pm15.13$)}  \\
             & M3ID  & 186.67 {\tiny ($\pm5.77$)} &  128.33 {\tiny ($\pm10.41$)} & \underline{131.67} {\tiny ($\pm5.00$)\phantom{0}} & 151.67 {\tiny ($\pm20.88$)} &  598.11 {\tiny ($\pm20.35$)}  \\
             & RITUAL  & \underline{187.50} {\tiny ($\pm2.89$)} & \underline{139.58} {\tiny ($\pm7.64$)\phantom{0}}  & 125.00 {\tiny ($\pm10.27$)} & \underline{164.17} {\tiny ($\pm6.87$)\phantom{0}} & \underline{616.25} {\tiny ($\pm20.38$)}     \\
             & \cc \textbf{Ours} & \cc \textbf{188.33} {\tiny ($\pm2.89$)} & \cc \textbf{150.00} {\tiny ($\pm7.64$)\phantom{0}}  & \cc \textbf{133.89} {\tiny ($\pm3.85$)\phantom{0}} & \cc \textbf{172.22} {\tiny ($\pm3.47$)\phantom{0}} & \cc \textbf{644.44} {\tiny ($\pm9.18$)\phantom{0}} \\
            %  \arrayrulecolor{gray!50}\cmidrule(lr){2-4}
            %  & \textbf{\Ours}+VCD & 20.0 & 6.8 \\
            %  & \textbf{\Ours}+M3ID & 18.0 & 5.7 \\
            \midrule
            \multirow{7}{*}{{\textbf{InstructBLIP}}} & Regular & 160.42 {\tiny ($\pm5.16$)} & 79.17 {\tiny ($\pm8.22$)\phantom{0}} & \textbf{79.58} {\tiny ($\pm8.54$)\phantom{0}} & 130.42 {\tiny ($\pm17.34$)}  &  449.58 {\tiny ($\pm24.09$)}    \\
             & DoLa  & 175.00 {\tiny ($\pm5.00$)} & 55.00 {\tiny ($\pm5.00$)\phantom{0}} & 48.89 {\tiny ($\pm3.47$)\phantom{0}} & 113.33 {\tiny ($\pm6.67$)\phantom{0}}  &  392.22 {\tiny ($\pm7.88$)\phantom{0}}   \\
             & OPERA  & 175.00 {\tiny ($\pm3.33$)} & 61.11 {\tiny ($\pm3.47$)\phantom{0}} & 53.89 {\tiny ($\pm1.92$)\phantom{0}} &120.55 {\tiny ($\pm2.55$)\phantom{0}}  &  410.56 {\tiny ($\pm9.07$)\phantom{0}}     \\
             & VCD  & 158.89 {\tiny ($\pm5.85$)} &  \textbf{91.67} {\tiny ($\pm18.34$)} & 66.11 {\tiny ($\pm9.76$)\phantom{0}} & 121.67 {\tiny ($\pm12.58$)} & 438.33 {\tiny ($\pm16.07$)}  \\
             & M3ID  & 160.00 {\tiny ($\pm5.00$)} & 87.22 {\tiny ($\pm22.63$)} & \underline{69.44} {\tiny ($\pm9.18$)\phantom{0}} & 125.00 {\tiny ($\pm7.64$)\phantom{0}} & 441.67 {\tiny ($\pm17.32$)}    \\
             & RITUAL  & \underline{182.50} {\tiny ($\pm6.45$)} & 74.58 {\tiny ($\pm5.99$)\phantom{0}}  & 67.08 {\tiny ($\pm10.31$)} & \underline{139.17} {\tiny ($\pm0.96$)\phantom{0}} & \underline{463.33} {\tiny ($\pm12.40$)}   \\
             & \cc \textbf{Ours} & \cc \textbf{186.67} {\tiny ($\pm2.89$)} & \cc \underline{89.44} {\tiny ($\pm8.22$)\phantom{0}}  & \cc 58.33 {\tiny ($\pm4.41$)\phantom{0}} & \cc \textbf{150.00} {\tiny ($\pm1.89$)\phantom{0}} & \cc \textbf{484.44} {\tiny ($\pm11.34$)} \\
             \midrule
            \multirow{5}{*}{{\textbf{Qwen-VL}}} & Regular & 155.00 {\tiny ($\pm3.54$)} & 127.67 {\tiny ($\pm13.36$)} & {131.67} {\tiny ($\pm7.73$)\phantom{0}} & 173.00 {\tiny ($\pm9.75$)\phantom{0}}  &  587.33 {\tiny ($\pm31.06$)}    \\
             % & DoLa  & 175.00 {\tiny ($\pm5.00$)} & 55.00 {\tiny ($\pm5.00$)\phantom{0}} & 48.89 {\tiny ($\pm3.47$)\phantom{0}} & 113.33 {\tiny ($\pm6.67$)\phantom{0}}  &  392.22 {\tiny ($\pm7.88$)\phantom{0}}   \\
             % & OPERA  & 175.00 {\tiny ($\pm3.33$)} & 61.11 {\tiny ($\pm3.47$)\phantom{0}} & 53.89 {\tiny ($\pm1.92$)\phantom{0}} &120.55 {\tiny ($\pm2.55$)\phantom{0}}  &  410.56 {\tiny ($\pm9.07$)\phantom{0}}     \\
             & VCD  & 156.00 {\tiny ($\pm6.52$)} &  131.00 {\tiny ($\pm6.19$)\phantom{0}} & 128.00 {\tiny ($\pm3.61$)\phantom{0}} & 181.67 {\tiny ($\pm5.14$)\phantom{0}} & 596.67 {\tiny ($\pm11.61$)}  \\
             & M3ID  & \underline{178.33} {\tiny ($\pm2.89$)} & 143.33 {\tiny ($\pm2.89$)\phantom{0}} & {150.00} {\tiny ($\pm2.89$)\phantom{0}} & 175.00 {\tiny ($\pm5.00$)\phantom{0}} & 646.66 {\tiny ($\pm8.50$)\phantom{0}}    \\
             & RITUAL  & \underline{178.33} {\tiny ($\pm2.89$)} & \underline{142.22} {\tiny ($\pm16.19$)}  & \textbf{156.66} {\tiny ($\pm2.89$)\phantom{0}} & \textbf{178.33} {\tiny ($\pm2.89$)\phantom{0}} & \underline{655.55} {\tiny ($\pm14.99$)}   \\
             & \cc \textbf{Ours} & \cc \textbf{180.00} {\tiny ($\pm0.00$)} & \cc \textbf{148.89} {\tiny ($\pm6.74$)\phantom{0}}  & \cc \underline{155.00} {\tiny ($\pm7.64$)\phantom{0}} & \cc \textbf{178.33} {\tiny ($\pm2.89$)\phantom{0}} & \cc \textbf{662.22} {\tiny ($\pm4.37$)\phantom{0}} \\
            \bottomrule
        \end{tabular}
        }
        \vspace{-15pt}
        \end{small}
        \end{center}
        
\end{table*}

\RebuttalRevision{
\subsection{Additional Results on CHAIR}
In Table~\ref{tab:CHAIR-128} and Table~\ref{tab:CHAIR-256}, we present performance comparisons on the CHAIR benchmark with maximum number of tokens set to 128 and 256. The results indicate that our approach also achieves competitive performance across two LVLMs in mitigating hallucinations during long-sequence generation scenarios.
}
\RebuttalRevision{
\subsection{Full Results on MME-Hallucination}
In Table~\ref{tab:MME-full}, we present the full results on the MME-Hallucination benchmark across three LVLMs.
}


\begin{table}[t]
\centering
\caption{
\RebuttalRevision{
\textbf{Detailed results on MMBench benchmark}. Abbreviations adopted: LR for Logical Reasoning; AR for Attribute Reasoning; RR for Relation Reasoning; FP-S for Fine-grained Perception (Single Instance); FP-C for Fine-grained Perception (Cross Instance); CP for Coarse Perception. The best results are \textbf{bolded}.}}
\vspace{5pt}
\label{tab:MMBench Full}
\begin{tabular}{lccccccc}
\toprule
Method & LR & AR & RR & FP-S & FP-C & CP & Overall \\
\midrule
Regular & 30.51 & 71.36 & 52.17 & 67.58 & \textbf{58.74} & 76.35 & 64.09 \\
VCD     & 30.51 & \textbf{73.37} & 53.04 & 67.92 & 57.34 & 77.03 & 64.60 \\
M3ID    & 30.51 & 72.36 & 53.04 & 67.58 & 57.34 & \textbf{77.36} & 64.43 \\
RITUAL  & 28.81 & 72.86 & 54.78 & 65.87 & 58.04 & 76.01 & 63.83 \\
\cc \textbf{Ours}    & \cc \textbf{31.36} & \cc 70.85 & \cc \textbf{60.87} & \cc \textbf{68.60} & \cc \textbf{58.74} & \cc \textbf{77.36} & \cc \textbf{65.46} \\
\bottomrule
\end{tabular}
\end{table}




% \begin{table*}[t]
% \centering
% \caption{
% \RebuttalRevision{\textbf{Detailed results on MMVet benchmark}. Abbreviations: Rec for Recognition, OCR for Optical Character Recognition, Know for Knowledge, Gen for Language Generation, Spat for Spatial Awareness, Math for Mathematics. The best results are \textbf{bolded}, and the second best are \underline{underlined}.}}
% \label{tab:mmvet_combined}

% % First subtable: Random sampling decoding
% \begin{subtable}{0.49\textwidth}
% \centering
% \resizebox{\textwidth}{!}{
% \begin{tabular}{lccccccc}
% \toprule
% Method   & Rec   & OCR   & Know  & Gen   & Spat  & Math  & Total \\
% \midrule
% Regular  & 30.8  & 19.0  & 14.5  & 17.9  & 26.9  & \textbf{11.5} & 26.1  \\
% VCD      & 35.6  & 21.9  & 18.3  & \underline{21.9} & \underline{28.9} & 3.8  & \underline{30.9} \\
% M3ID     & 35.0  & 19.7  & 18.8  & 19.0  & 26.0  & 7.7   & 29.9  \\
% RITUAL   & \textbf{36.3} & \underline{20.6} & \textbf{19.5} & 21.1  & 24.7  & 7.7   & 30.6  \\
% \textbf{Ours} & \underline{35.9} & \textbf{27.2} & \underline{19.2} & \textbf{22.4} & \textbf{30.4} & \textbf{11.5} & \textbf{33.0} \\
% \bottomrule
% \end{tabular}
% }
% \caption{Random sampling decoding.}
% \label{tab:mmvet_random_sampling}
% \end{subtable}
% \hfill
% % Second subtable: Greedy decoding
% \begin{subtable}{0.49\textwidth}
% \centering
% \resizebox{\textwidth}{!}{
% \begin{tabular}{lccccccc}
% \toprule
% Method   & Rec   & OCR   & Know  & Gen   & Spat  & Math  & Total \\
% \midrule
% Greedy   & 37.0  & 22.6  & 17.5  & 20.2  & 24.9  & 7.7   & 31.8  \\
% VCD      & \textbf{38.2} & 22.8  & \textbf{22.5} & \textbf{24.6} & 25.1  & 3.8   & \underline{33.4} \\
% M3ID     & \underline{37.9} & \textbf{23.6} & \underline{20.4} & \underline{20.7} & \underline{26.0} & \underline{11.5} & 33.2  \\
% RITUAL   & 35.6  & 21.7  & 18.9  & 19.9  & 24.7  & 7.7   & 30.6  \\
% \textbf{Ours} & \underline{37.9} & \textbf{25.0} & 20.2  & 19.5  & \textbf{32.8} & \textbf{15.0} & \textbf{34.0} \\
% \bottomrule
% \end{tabular}
% }
% \caption{Greedy decoding.}
% \label{tab:mmvet_greedy_decoding}
% \end{subtable}

% \end{table*}




\begin{table}[t]
\centering
\caption{
\RebuttalRevision{\textbf{Detailed results on MMVet benchmark with regular sampling}. Abbreviations adopted: Rec for Recognition, OCR for Optical Character Recognition, Know for Knowledge, Gen for Language Generation, Spat for Spatial Awareness, Math for Mathematics. The best results are \textbf{bolded}, and the second best are \underline{underlined}.}}
\vspace{5pt}
\label{tab:mmvet_sample}
\begin{tabular}{lccccccc}
\toprule
Method & Rec & OCR & Know & Gen & Spat & Math & Total \\
\midrule
Regular & 30.8 & 19.0 & 14.5 & 17.9 & 26.9 & \textbf{11.5} & 26.1 \\
VCD     & 35.6 & 21.9 & 18.3 & \underline{21.9} & \underline{28.9} & 3.8 & \underline{30.9} \\
M3ID    & 35.0 & 19.7 & 18.8 & 19.0 & 26.0 & 7.7 & 29.9 \\
RITUAL  & \textbf{36.3} & \underline{20.6} & \textbf{19.5} & 21.1 & 24.7 & 7.7 & 30.6 \\
\textbf{Ours} & \underline{35.9} & \textbf{27.2} & \underline{19.2} & \textbf{22.4} & \textbf{30.4} & \textbf{11.5} & \textbf{33.0} \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[t]
\centering
\caption{
\RebuttalRevision{\textbf{Detailed results on MMVet benchmark with greedy decoding}. Abbreviations adopted: Rec for Recognition, OCR for Optical Character Recognition, Know for Knowledge, Gen for Language Generation, Spat for Spatial Awareness, Math for Mathematics. The best results are \textbf{bolded}, and the second best are \underline{underlined}.}}
\vspace{5pt}
\label{tab:mmvet_greedy}
\begin{tabular}{lccccccc}
\toprule
Method & Rec & OCR & Know & Gen & Spat & Math & Total \\
\midrule
Greedy  & 37.0 & 22.6 & 17.5 & 20.2 & 24.9 & 7.7 & 31.8 \\
VCD     & \textbf{38.2} & 22.8 & \textbf{22.5} & \textbf{24.6} & 25.1 & 3.8 & \underline{33.4} \\
M3ID    & \underline{37.9} & \textbf{23.6} & \underline{20.4} & \underline{20.7} & \underline{26.0} & \underline{11.5} & 33.2 \\
RITUAL  & 35.6 & 21.7 & 18.9 & 19.9 & 24.7 & 7.7 & 30.6 \\
\textbf{Ours} & \underline{37.9} & \textbf{25.0} & 20.2 & 19.5 & \textbf{32.8} & \textbf{15.0} & \textbf{34.0} \\
\bottomrule
\end{tabular}
\end{table}




\begin{table}[t]
\centering
% \vspace{-10pt}
\caption{\RebuttalRevision{\textbf{Results on POPE using greedy decoding}}.}
% \vspace{5pt}
    \label{tab:greedy}
    \resizebox{0.5\textwidth}{!}{
    \begin{tabular}{lcccccc}
            \toprule
              \multirow{2}{*}[-0.5ex]{\textbf{Values}}  &  \multicolumn{4}{c}{\textbf{POPE}} \\
             \cmidrule(lr){2-5}
                 & {Acc.}  & {Prec.}  & {Rec.}  & {F1} \\
             \midrule
             Greedy & 87.73 & 88.19 & 87.13 & 87.66  \\
             VCD & 87.47 & 86.64 & \textbf{88.60} & 87.61 \\
             M3ID & 89.07 & 89.54 & 88.47 & 89.00 \\
             RITUAL & 89.23 & 90.17 & 88.07 & 89.11 \\
             \cc \textbf{Ours} & \cc \textbf{89.40} & \cc \textbf{94.44} & \cc 83.73 & \cc \textbf{88.76} \\
            \bottomrule
        \end{tabular}
}
% \vspace{-5pt}
\end{table}

\RebuttalRevision{
\subsection{Full Results on MMBench}
In Table~\ref{tab:MMBench Full}, we present the overall performance on the MMBench benchmark, as well as the detailed performance across six Level-2 abilities: Logical Reasoning (LR), Attribute Reasoning (AR), Relation Reasoning (RR), Fine-grained Perception - Single Instance (FP-S), Fine-grained Perception - Cross Instance (FP-C), and Coarse Perception (CP). We follow VCD~\citep{leng2024mitigating} to conduct experiments on the MMBench-\texttt{dev} set.
}
\RebuttalRevision{
\subsection{Results on MM-Vet}
In Table~\ref{tab:mmvet_sample} and Table~\ref{tab:mmvet_greedy}, we present the overall performance on the MM-Vet~\citep{yu2024mmvet} benchmark with random sampling decoding and greedy decoding strategies, respectively. We use LLaVA-1.5 as the LVLM backbone. From the results, we observed that our method consistently outperforms others on the MMVet benchmark. Notably, it significantly excels in the OCR, spatial awareness, and math subsets.
}
\RebuttalRevision{
\subsection{Results on POPE Using Greedy Decoding}
In Table~\ref{tab:greedy}, we present performance comparisons on the POPE benchmark with random sampling from the MS-COCO dataset. The experiment is conducted using the LLaVA-1.5 backbone.
}

\subsection{Effects of \texorpdfstring{$\alpha_1$ and $\alpha_2$}{alpha1 and alpha2} in Self-Correcting Decoding}
In Section~\ref{sec:method}, we present two decoding approaches: complementary decoding and contrastive decoding. We also introduce two balancing hyperparameters, $\alpha_1$ and $\alpha_2$, which control the relative influence of the original and generated images in next-token prediction.
In Table~\ref{tab:alpha1} and Table~\ref{tab:alpha2}, we analyze the effect of varying $\alpha_1$ or $\alpha_2$ while keeping all other hyperparameters at their default settings. The results indicate that our default choice of $\alpha_1=3$ and $\alpha_2=1$ consistently yields the best performance across two benchmarks. Moreover, compared to setting these hyperparameters to 0, which effectively reduces complementary/contrastive decoding to standard decoding, the performance improvements demonstrate that our proposed decoding approaches significantly contribute to the overall effectiveness of DeGF in mitigating hallucinations in LVLMs.



\begin{table}[t]
\centering
% \vspace{-10pt}
\caption{\textbf{Sensitivity analysis of hyperparameter $\alpha_1$}. We present the performance of our approach, based on the LLaVA-1.5 backbone, across two benchmarks for varying values of $\alpha_1$. Note that we fix $\alpha_2=1$ in this experiment.}
% \vspace{5pt}
    \label{tab:alpha1}
    \resizebox{0.7\textwidth}{!}{
    \begin{tabular}{lcccccccc}
            \toprule
              \multirow{2}{*}[-0.5ex]{\textbf{Values}}  &  \multicolumn{4}{c}{\textbf{POPE}} & \multicolumn{2}{c}{\textbf{CHAIR}} \\
             \cmidrule(lr){2-5}\cmidrule(lr){6-7}
                 & {Acc.}  & {Prec.}  & {Rec.}  & {F1}  & CHAIR$_S$  & CHAIR$_I$ \\
             \midrule
             $\alpha_1 =0$ & 87.50 & 86.71 & 87.49 & 87.10 & 22.8 & 7.6 \\
             $\alpha_1 =1$ & 87.97 & 87.28 & 87.34 & 87.31 & 20.6 & 6.9 \\
             $\alpha_1 =2$ & 88.90 & 89.39 & \textbf{87.75} & 88.56 & 19.4 & 6.3 \\
             \cc $\alpha_1 =3$ & \textbf{89.03}\cc & \textbf{91.20}\cc & 86.40\cc & \textbf{88.74}\cc & \textbf{18.4}\cc & \textbf{6.1}\cc  \\
             $\alpha_1 =4$ & 88.67 & 90.56 & 85.28 & 87.84 & 22.6 & 8.1  \\
            \bottomrule
        \end{tabular}
    }
    % \vspace{-10pt}
\end{table}

\begin{table}[t]
\centering
\caption{\textbf{Sensitivity analysis of hyperparameter $\alpha_2$}. We present the performance of our approach, based on the LLaVA-1.5 backbone, across two benchmarks for varying values of $\alpha_2$. Note that we fix $\alpha_1=3$ in this experiment.}
% \vspace{5pt}
    \label{tab:alpha2}
    \resizebox{0.7\textwidth}{!}{
    \begin{tabular}{lcccccccc}
            \toprule
              \multirow{2}{*}[-0.5ex]{\textbf{Values}}  &  \multicolumn{4}{c}{\textbf{POPE}} & \multicolumn{2}{c}{\textbf{CHAIR}} \\
             \cmidrule(lr){2-5}\cmidrule(lr){6-7}
                 & {Acc.}  & {Prec.}  & {Rec.}  & {F1}  & CHAIR$_S$  & CHAIR$_I$ \\
             \midrule
             $\alpha_2 =0$ & 86.77 & 85.17 & 86.58 & 85.87 & 23.6 & 8.2 \\
             \cc $\alpha_2 =1$ & \textbf{89.03}\cc & \textbf{91.20}\cc & 86.40\cc & \textbf{88.74}\cc & \textbf{18.4}\cc & \textbf{6.1}\cc \\
             $\alpha_2 =2$ & 88.73 & 89.86 & \textbf{86.66} & 88.23 & 21.8 & 7.5 \\
             $\alpha_2 =3$ & 88.03 & 87.97 & 86.28 & 87.12 & 22.8 & 7.3 \\
             $\alpha_2 =4$ & 87.13 & 86.52 & 86.16 & 86.34 & 23.6 & 7.9 \\
            \bottomrule
        \end{tabular}
}
\end{table}

\begin{table}[t]
        \begin{center}
        \begin{small}
        \vspace{-10pt}
        \caption{\textbf{Sensitivity analysis of hyperparameter $\beta$}. We present the performance of our approach, based on the LLaVA-1.5 backbone, across two benchmarks for varying values of $\beta$.}
        \vspace{5pt}
        \label{tab:beta}
        \resizebox{0.7\textwidth}{!}{
        \begin{tabular}{lcccccccc}
            \toprule
              \multirow{2}{*}[-0.5ex]{\textbf{Values}}  &  \multicolumn{4}{c}{\textbf{POPE}} & \multicolumn{2}{c}{\textbf{CHAIR}} \\
             \cmidrule(lr){2-5}\cmidrule(lr){6-7}
                 & {Acc.}  & {Prec.}  & {Rec.}  & {F1}  & CHAIR$_S$  & CHAIR$_I$ \\
             \midrule
             $\beta =0$ & 87.17 & 87.45 & 85.30 & 86.36 & 21.2 & 7.1  \\
             $\beta =0.05$ & 88.27 & 89.85 & 86.12 & 87.95 & 19.1 & 6.3 \\
             $\beta =0.1$ & 88.33 & 89.04 & 86.04 & 87.52 & \textbf{18.4}\cc & \textbf{6.1}\cc \\
             $\beta =0.25$ & \textbf{89.03}\cc & \textbf{91.20}\cc & \textbf{86.40}\cc & \textbf{88.74}\cc & 19.3 & 6.5 \\
             $\beta =0.5$ & 87.80 & 88.79 & 85.48 & 87.10 & 20.2 & 6.9 \\
            \bottomrule
        \end{tabular}
        
        }
        \end{small}
        \end{center}
        \vspace{-5pt}
\end{table}

% , we examine the effect of $\alpha_1$ while maintaining $\alpha_2$ at a value of 1. Similarly, Table~\ref{tab:alpha2} presents results with $\alpha_1$ fixed at 3 while varying $\alpha_2$. This ablation study aims to evaluate the effectiveness of our complementary decoding ($\alpha_1$) and contrastive decoding ($\alpha_2$). The results on the CHAIR benchmark indicate that $\alpha_2$ has a more significant impact on the outcomes than $\alpha_1$, as evidenced by the greater variance observed when modifying $\alpha_2$. This suggests that both our complementary and contrastive decoding methods significantly mitigate hallucinations, with contrastive decoding contributing more substantially to the results.


\subsection{Effect of \texorpdfstring{$\beta$}{beta} in Adaptive Plausibility Constraint}
We further conduct an ablation study on $\beta$ introduced in Equation~(\ref{eq:adaptive}), where we vary $\beta$ from 0 to 0.5 while keeping all other hyperparameters fixed. The results in Table~\ref{tab:beta} show that setting $\beta=0$, which imposes no constraint, results in suboptimal performance across both benchmarks. Additionally, in the POPE benchmark, where LVLMs handle yes-or-no questions, a more aggressive truncation with $\beta=0.25$ yields the best performance. In contrast, for the open-ended CHAIR benchmark, a lower value of $\beta=0.1$ leads to the best results.



        
\subsection{Scaling Up the LVLMs}
We further extend our evaluation to larger-scale 13B variants of the LLaVA-1.5 model to assess the scalability of our approach. Table~\ref{tab:POPE13} compares our experimental results with other state-of-the-art approaches across all three subsets of the POPE benchmark using the 13B-sized LLaVA-1.5 model. We observe that scaling up the LLaVA-1.5 model does not alleviate the hallucination issues, as evidenced by the comparable performance of both the 7B and 13B models. Using the 13B-sized model, our DeGF consistently achieves improved performance across all subsets compared to other approaches, demonstrating its general effectiveness and scalability.

\begin{table}[t]
    \centering
    \small
    \caption{
        \textbf{Results on POPE~\citep{li2023evaluating} benchmark using 13B-sized LLaVA-1.5}. Higher ($\uparrow$) accuracy, precision, recall, and F1 indicate better performance. 
    }
    \vspace{5pt}
    \label{tab:POPE13}
    \setlength{\tabcolsep}{8pt} % base value: 6pt
    \resizebox{0.8\textwidth}{!}{
    \begin{tabular}{cclcccc}
    \toprule
     & \multirow{2}{*}[-2pt]{\textbf{Setup}} & \multirow{2}{*}[-2pt]{\textbf{Method}} & \multicolumn{4}{c}{\textbf{LLaVA-1.5}}  \\
    \arrayrulecolor{gray} \cmidrule(lr){4-7} 
     &  &  & {Acc.} $\uparrow$ & {Prec.} $\uparrow$ & {Rec.} $\uparrow$ & {F1} $\uparrow$  \\
    \midrule
    \multirow{15}{*}[-5pt]{\rotatebox{90}{\textbf{\normalsize MS-COCO}}} & \multirow{5}{*}{Random} 
    & Regular &  82.53 & 78.57 & 89.47 & 83.67 \\
     &  & VCD  &  84.80 & 80.67 & 91.53 & 85.76 \\
     &  & M3ID  & 85.37  & 81.30 & 91.87 & 86.26  \\
     &  & RITUAL  & 87.80  & 84.45 & \textbf{92.67} & \textbf{88.37} \\
     &  & \cc \textbf{Ours} &\cc \textbf{88.40} &\cc \textbf{88.14} &\cc 88.61 &\cc \textbf{88.37}  \\
     \arrayrulecolor{gray}\cmidrule(lr){2-7}
      &  \multirow{5}{*}{Popular} & Regular & 80.53 & 76.17 & 88.87 & 82.03\\
     &  & VCD  &  82.23 & 76.88 & 92.20 & 83.84 \\
     &  & M3ID  &  82.60 & 77.91 & 91.00 & 83.95 \\
     &  & RITUAL  & 84.07  & 79.00 & \textbf{92.80} & 85.35 \\
     &  & \cc \textbf{Ours} &\cc \textbf{85.30} &\cc \textbf{84.18}  &\cc 86.93 &\cc \textbf{85.53}  \\
     \arrayrulecolor{gray}\cmidrule(lr){2-7}
      &  \multirow{5}{*}{Adversarial} & Regular & 75.80 & 70.41 & 89.00 & 78.62 \\
     &  & VCD  &  77.33 & 71.44 & 91.07 & 80.07 \\
     &  & M3ID  & 77.43  & 71.65 & 90.80 & 80.09 \\
     &  & RITUAL  & 78.00  & 71.72 & \textbf{92.47} & 80.78 \\
     &  & \cc \textbf{Ours} &\cc \textbf{81.43}   &\cc \textbf{78.61} &\cc 87.04 &\cc \textbf{82.61}    \\
    \bottomrule
    \end{tabular}
}
    % \vspace{-15pt}
\end{table}

\RebuttalRevision{
\subsection{Speeding Up Our Approach}
\label{subsec:speedup}
In this section, we propose two strategies to accelerate our approach: limiting the length of the initial response and reducing the number of inference steps in the diffusion process.
\begin{itemize}
    \item \textbf{Reducing Diffusion Inference Steps}. By default, we set the number of diffusion inference steps to 50 to ensure high-quality image generation. To improve the response generation speed, we can reduce the number of diffusion steps.
    In Table~\ref{tab:diffstep}, we report the performance on the CHAIR benchmark after reducing the diffusion inference steps in the model. By reducing the diffusion inference steps from 50 to 10, the average latency decreases by 2.85 seconds per instance, while the performance on CHAIR remains robust. This demonstrates that reducing the inference steps of the diffusion model is an effective way to speed up our approach.
    \item \textbf{Restricting Length of Initial Response}. Our method involves two queries to the LVLM for self-correcting decoding. To enhance efficiency, we can limit the length of the initial response. In Table~\ref{tab:initialnumber}, we present the efficiency and CHAIR performance results after decreasing the maximum token limit for the initial response. We can see that reducing the maximum number of tokens in the initial response from 128 to 96 decreases the latency by 0.72 seconds per instance while maintaining competitive performance. However, further reductions result in performance degradation, as a shorter initial response fails to adequately cover the entire scene, limiting its ability to generate an image that effectively reflects and mitigates hallucinations.
\end{itemize}
Note that these two strategies are not conflicting; instead, they are complementary. Setting the diffusion steps to 10 and limiting the maximum number of tokens in the initial response to 96 further reduces the inference latency to 10.21 seconds per instance while maintaining robust performance.
}

\begin{figure}[t]
\begin{minipage}[t]{0.49\linewidth}
\makeatletter\def\@captype{table}
\setlength\tabcolsep{2.8pt}
% \vspace{-50pt}
\caption{\RebuttalRevision{\textbf{Effect of reducing diffusion inference steps.}} }
\vspace{3.3pt}
    \label{tab:diffstep}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{ccccc}
        \toprule
        Diff. Steps & Avg. Latency $\downarrow$ & CHAIR$_S$ $\downarrow$ & CHAIR$_I$ $\downarrow$\\
        \midrule
        50 & 13.89 s & 48.8 & 14.6  \\
        30 & 12.56 s & 48.9 & 14.7 \\
        20 & 11.87 s & 49.2 & 14.8  \\
        10 & 11.04 s & 48.8 & 14.9  \\
        \bottomrule
    \end{tabular}
    }
\end{minipage}
\hspace{2pt}
\begin{minipage}[t]{0.49\linewidth}
\makeatletter\def\@captype{table}
\setlength\tabcolsep{3pt}
\caption{\RebuttalRevision{\textbf{Effect of restricting the number of tokens in the initial response.}}}
    \vspace{4pt}
    \label{tab:initialnumber}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{ccccc}
        \toprule
        \# Tokens & Avg. Latency $\downarrow$ & CHAIR$_S$ $\downarrow$ & CHAIR$_I$ $\downarrow$\\
        \midrule
        128 & 13.89 s & 48.8 & 14.6   \\
        96 & 13.17 s & 48.8 & 14.9  \\
        64 & 12.20 s & 49.5 &  14.8  \\
        32 & 11.33 s & 51.2 & 14.9  \\
        \bottomrule
    \end{tabular}
}
\end{minipage}
\vspace{-3pt}
\end{figure}

\RebuttalRevision{
\subsection{Quantitative Assessment of Generated Image Quality}
\label{subsec:quant_diffusion}
Our approach incorporates a text-to-image generation model to mitigate hallucinations. We evaluate the quality of the generated images on all 4 subsets on the MME benchmark using CLIPScore~\citep{hessel2021clipscore}. Specifically, we utilize the CLIP backbone with ViT-B/32 backbone for our evaluation. We list the results in Table~\ref{tab:mme_clipscores}. As we can see from the table, our text-to-image generative model (specifically, SD-v1.5) achieves an average CLIPScore of over 30 across all subsets. For comparison, the advanced DALL-E 3 model achieves a score of 32.0, while DALL-E 2 achieves 31.4.\footnote{These results are sourced from the technical report on DALL-E 3, available at: \url{https://cdn.openai.com/papers/dall-e-3.pdf}.} These results highlight the capability of our model to generate high-quality images that closely align with the initial response.
}

\begin{table}[t]
\centering
\caption{\RebuttalRevision{\textbf{CLIPScore evaluation across different MME subsets}}.}
\vspace{5pt}
\begin{tabular}{lcccc}
\toprule
\textbf{MME Subset}     & \textbf{Existence} & \textbf{Count} & \textbf{Position} & \textbf{Color} \\ \midrule
\textbf{Avg. CLIPScore} & 31.34              & 30.69          & 30.09             & 31.69          \\ \bottomrule
\end{tabular}
\label{tab:mme_clipscores}
\end{table}




\section{More Case Studies}
\label{sec:case}
\subsection{Details about GPT-4V-Aided Evaluation}
Following VCD~\citep{leng2024mitigating}, we use GPT-4V to evaluate responses in open-ended generation scenarios, scoring them based on accuracy and detailedness. Leveraging the strong human-like capabilities of GPT-4V, it can detect incorrect colors, positions, and relationships, providing a comprehensive evaluation of the responses.
Specifically, we apply the prompt provided in Table~\ref{tab:prompt_evaluation} to instruct GPT-4V to rate the two responses on a scale of 1 to 10 for both accuracy and detailedness:
\begin{itemize}
    \item \textbf{Accuracy} measures the consistency between the responses/descriptions generated by the LVLMs and the given image. A lower score is assigned if GPT-4V detects any inconsistencies in the content of the responses.
    \item \textbf{Detailedness} evaluates the depth and specificity of the responses provided by the LVLMs. A higher score is awarded if the response includes comprehensive descriptions, captures fine-grained details of the image, and provides well-elaborated explanations. Conversely, a lower score is given if the response is vague or lacks sufficient detail.
\end{itemize}


\begin{table*}[t]\centering
\begin{minipage}{0.95\textwidth}
%\vspace{0mm}    
\centering
\begin{tcolorbox} 
    \centering
   
     %\hspace{-4mm}
      \small
    \begin{tabular}{p{0.95\textwidth}} \hline \\
   \textbf{Description:} \\    
   
   AI that scores image description accuracy and detailedness.

   \\ \midrule

   \textbf{Instructions:} \\   
   
You are an AI designed to evaluate and score the performance of two AI assistants in describing a given image. Your primary focus is on the accuracy and detailedness of their descriptions. You will assess the accuracy by checking for hallucinations - any part of the description that is inconsistent with the image content. For detailedness, you will consider how rich the response is in necessary details, excluding any hallucinated parts. You will provide scores on a scale from 1 to 10 for each assistant separately, based on these criteria. After scoring, you will offer an explanation for your evaluation, ensuring it is free from bias and not influenced by the order of presentation of the responses.
\\ \\
Input format: \\ \\
\lbrack{}Assistant 1\rbrack{}\\
 \{Response 1\}  \\
\lbrack{}End of Assistant 1\rbrack{} \\
\\
\lbrack{}Assistant 2\rbrack{} \\
 \{Response 2\}\\
\lbrack{}End of Assistant 2\rbrack{} \\
\\
Output format:\\
\\
Accuracy:\\
Scores of the two answers:\\
Reason:\\
\\
Detailedness:\\
Scores of the two answers:\\
Reason:\\ \\

\bottomrule
    \end{tabular}
\end{tcolorbox}
\caption{\textbf{GPT-4V-aided evaluation setup}. We present the prompt we provided to GPT-4V to evaluate the LVLM responses based on accuracy and detailedness.}
\label{tab:prompt_evaluation}
\end{minipage}
\end{table*}


\subsection{More Qualitative Results}
In Figure~\ref{fig:llavabench2} and Figure~\ref{fig:llavabench3}, we provide additional case studies on LLaVA-Bench to qualitatively demonstrate the effectiveness of our methods in mitigating hallucinations. We also included GPT-4V evaluations of accuracy and detailedness scores for each instance.

\RebuttalRevision{
In Figure~\ref{fig:diffusion1}-\ref{fig:diffusion4}, we provide qualitative evaluations of the images generated by the generative model, including both success and failure cases, across all four subsets of the MME benchmark to better understand the effectiveness of the generative models. Our results show that, despite occasional failure cases, the generative model consistently produces high-quality and realistic images that accurately visualize the initial response, providing effective self-feedback.
}


\begin{figure}[t]

\includegraphics[width=\linewidth]{figs/llavabench1.png}
\\[10pt]
\includegraphics[width=\linewidth]{figs/llavabench2.png}
\\[10pt]
\includegraphics[width=\linewidth]{figs/llavabench3.png}
\\[10pt]
\includegraphics[width=\linewidth]{figs/llavabench4.png}

\caption{\textbf{Case studies on the LLaVA-Bench benchmark}. We compare the responses generated by regular decoding and our method using LLaVA-1.5. GPT-4V-aided evaluation results are also provided alongside the responses. Hallucinated and accurate content is highlighted in \textcolor{darkred}{red} and \textcolor{darkgreen}{green}.} 

\label{fig:llavabench2}
\end{figure}

\begin{figure}[t]

\includegraphics[width=\linewidth]{figs/llavabench5.png}
\\[10pt]
\includegraphics[width=\linewidth]{figs/llavabench6.png}
\\[10pt]
\includegraphics[width=\linewidth]{figs/llavabench7.png}
\\[10pt]
\includegraphics[width=\linewidth]{figs/llavabench8.png}

\caption{\textbf{Case studies on the LLaVA-Bench benchmark}. We compare the responses generated by regular decoding and our method using LLaVA-1.5. GPT-4V-aided evaluation results are also provided alongside the responses. Hallucinated and accurate content is highlighted in \textcolor{darkred}{red} and \textcolor{darkgreen}{green}.} 

\label{fig:llavabench3}
\end{figure}


\begin{figure}[t]
\centering
\includegraphics[width=0.92\linewidth]{figs/diffusion1-1.png}
\includegraphics[width=0.92\linewidth]{figs/diffusion1-2.png}
\\[5pt]
\includegraphics[width=0.95\linewidth]{figs/diffusion1-3.png}
\vspace{-10pt}
\caption{\textbf{Qualitative evaluation of images generated by the generative model on the \textit{existence} subset of the MME benchmark.} Specifically, the left displays the original image input, the middle presents the initial response generated by the LVLMs, and the right shows the image generated based on this response. This figure showcases four success cases and one failure case of our Diffusion models in generating high-quality images that align with the initial response.}
\label{fig:diffusion1}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.92\linewidth]{figs/diffusion4-1.png}
\includegraphics[width=0.92\linewidth]{figs/diffusion4-2.png}
\\[5pt]
\includegraphics[width=0.95\linewidth]{figs/diffusion4-3.png}
\vspace{-10pt}
\caption{\textbf{Qualitative evaluation of images generated by the generative model on the \textit{count} subset of the MME benchmark.} Specifically, the left displays the original image input, the middle presents the initial response generated by the LVLMs, and the right  shows the image generated based on this response. This figure showcases four success cases and one failure case of our Diffusion models in generating high-quality images that align with the initial response.}
\label{fig:diffusion2}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.92\linewidth]{figs/diffusion3-1.png}
\includegraphics[width=0.92\linewidth]{figs/diffusion3-2.png}
\\[5pt]
\includegraphics[width=0.95\linewidth]{figs/diffusion3-3.png}
\vspace{-10pt}
\caption{\textbf{Qualitative evaluation of images generated by the generative model on the \textit{position} subset of the MME benchmark.} Specifically, the left displays the original image input, the middle presents the initial response generated by the LVLMs, and the right  shows the image generated based on this response. This figure showcases four success cases and one failure case of our Diffusion models in generating high-quality images that align with the initial response.}
\label{fig:diffusion3}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.92\linewidth]{figs/diffusion2-1.png}
\includegraphics[width=0.92\linewidth]{figs/diffusion2-2.png}
\\[5pt]
\includegraphics[width=0.95\linewidth]{figs/diffusion2-3.png}
\vspace{-10pt}
\caption{\textbf{Qualitative evaluation of images generated by the generative model on the \textit{color} subset of the MME benchmark.} Specifically, the left displays the original image input, the middle presents the initial response generated by the LVLMs, and the right  shows the image generated based on this response. This figure showcases four success cases and one failure case of our Diffusion models in generating high-quality images that align with the initial response.}
\label{fig:diffusion4}
\end{figure}

\RebuttalRevision{
\section{Future Work}
\label{sec:future}
In future work, we aim to extend the evaluation of our method to a broader range of LVLMs, such as Mini-GPT4~\citep{zhu2024minigpt} and mPLUG-Owl2~\citep{ye2024mplug}, as well as additional benchmarks, including R-Bench~\citep{wu2024evaluating}, which focuses on relation hallucination, and ROPE~\citep{chen2024multiobject}, which addresses multiple-object hallucination. This expanded evaluation will allow us to more comprehensively assess the generalizability and effectiveness of our approach across diverse models and tasks.}

\RebuttalRevision{
Furthermore, we plan to investigate integrating generative feedback directly into the instruction tuning phase. This integration has the potential to eliminate the computational overhead associated with applying our method during inference, thereby significantly improving efficiency without compromising performance. By pursuing these directions, we hope to further enhance the practical applicability and scalability of our approach.
}
