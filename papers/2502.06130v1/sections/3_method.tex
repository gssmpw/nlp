\vspace{-3pt}
\section{Method}
\vspace{-2pt}
\label{sec:method}
In this work, we present DeGF, a novel training-free algorithm that recursively improves the accuracy of LVLM responses using text-to-image generative feedback, as illustrated in Figure~\ref{fig:overview}. 

\subsection{Preliminary: Decoding of LVLMs}
\looseness=-1
We consider an LVLM parameterized by $\theta$, which processes an input image $v$ and a textual query $\mathbf{x}$, aiming to autoregressively generate a fluent sequence of textual responses $\mathbf{y}$. The visual input $v$ is first processed by a vision encoder and then projected into visual tokens within the textual input space using a vision-language alignment module (\eg, Q-Former~\citep{li2023blip} or linear projection~\citep{liu2023visual}). These visual tokens, along with the textual query tokens, are then fed into the language encoder for conditioned autoregressive generation. We denote the autoregressive generation process as
\begin{equation}
y_t \sim p_{\theta}(y_t | v, \mathbf{x}, \mathbf{y}_{<t}) \propto \exp f_{\theta}(y_t | v, \mathbf{x}, \mathbf{y}_{<t}),
\end{equation}
where $y_t$ represents the token at time step $t$, $\mathbf{y}_{<t} \triangleq [y_0, \dots, y_{t-1}]$ denotes the sequence of tokens generated before time step $t$, and $f_{\theta}$ is the logit distribution (unnormalized log-probabilities) produced by the LVLM over a vocabulary of textual tokens $\mathcal{V}$. At each step $t \in [0, \dots, T]$, the response token $y_t$ is sampled from the probability distribution $p_{\theta}(y_t | v, \mathbf{x}, \mathbf{y}_{<t})$, and this generative process continues iteratively until the response sequence $\mathbf{y} \triangleq [y_0, \dots, y_{T}]$ is complete.


\begin{figure}[t]
  \begin{center}
     \makebox[\textwidth]{\includegraphics[width=\textwidth]{figs/overview.png}}
  \end{center}
  \vspace{-10pt}
  \caption{\textbf{Overview of our proposed DeGF}. Our method follows a two-step process: first, a generative model produces a high-quality image based on the initial response; second, this image acts as an auxiliary visual reference, providing feedback to refine the next-token predictions. Additionally, we introduce self-correcting decoding, which either enhances or contrasts the next-token predictions conditioned on the original and generated images to mitigate hallucinations in the LVLM response.} %\martin{(In the figure, we could make it clear it is a two step process by labeling step 1 and step 2, and separate top and bottom using dashed line.)}}
  \label{fig:overview}
\end{figure}
     % \vspace{-10pt}

\subsection{Visual Reference Generation}
\label{sec:diffusion}
In our method, we incorporate generative feedback from diffusion models to guide the decoding process. Specifically, given a visual input $v$ and a textual query $\mathbf{x}$, we first prompt the LVLMs to generate an initial response $\boldsymbol{\tau}$, which includes relevant descriptions of the visual input with potential hallucinations. Subsequently, we leverage a pre-trained diffusion model $\mathcal{G}$ to generate a new image $v'$ based on the initial response:
\begin{equation}
v' = \mathcal{G}(\boldsymbol{\tau}, x_T), \quad\text{where} \,\,x_T \sim \mathcal{N}(0, \mathbf{I}).
\end{equation}
Here, $x_T$ denotes a sample from the standard Gaussian distribution, which serves as the initial noisy input to the diffusion model. Starting from this pure noise image $x_T$, the diffusion model $\mathcal{G}$ iteratively applies $T$ steps of the denoising process to obtain $x_T, x_{T-1}, \dots, x_0$, where the final output $x_0$ corresponds to the final generated image $v'$. Through this diffusion process, the generative model visualizes the initial response, providing a visual reference that helps mitigate potential hallucinations and produce a more accurate and consistent output.



% Our observations in Sec.~\ref{subsec: synthesis reflect hallucination} demonstrate that synthesized images can effectively reflect hallucinations, making them valuable as references to improve token prediction at each time step.
% %
% In light of this, we propose \Ours, a method designed to generate a visual reference that best reflects language hallucination. This approach requires no additional data or training and can serve as a plug-and-play solution for any LVLM.

% Specifically, given an input image $v$ and a text query $\mathbf{x}$, we first prompt the LVLM to generate a detailed description $\boldsymbol{\tau}$ related to $\mathbf{x}$. Next, we utilize a diffusion-based generative model $\mathcal{G}$ to generate a new image $v'$ based on the description. This process can be represented as:
% \begin{equation}
% v' = \mathcal{G}(\boldsymbol{\tau}, x_T), \quad\text{where} \,\,x_T \sim \mathcal{N}(0, \mathbf{I})
% \end{equation}
% where $x_T \sim \mathcal{N}(0, I)$ denotes the sampled noise. In practice, we employ Stable Diffusion v1.5~\citep{rombach2022high} to balance performance and speed.

% To implement the text-to-image synthesis, Stable Diffusion guides the diffusion process~\citep{ho2020denoising} with a textual prompt $\boldsymbol{\tau}$. Specifically, it applies a deterministic sampling process~\citep{song2020denoising} as follows:
% \begin{equation}
% \begin{split}
%     x_{t-1} = \sqrt{\alpha_{t-1}}\left(\frac{x_t - \sqrt{1-\alpha_t}\epsilon^t_{\theta}(x_t,\boldsymbol{\tau})}{\sqrt{\alpha_t}}\right) +
%     \sqrt{1-\alpha_{t-1}}\epsilon^t_{\theta}(x_t,\boldsymbol{\tau}), \quad t=T,\ldots,1,
% \end{split}
% \end{equation}
% where $x_T \sim \mathcal{N}(0,I)$ represents the initial noise, $\alpha_t = \prod_{i=1}^{t}(1-\beta_i)$ are predefined hyperparameters, and the function $\epsilon^t_{\theta}(x_t,\boldsymbol{\tau})$ is modeled as a neural network composed of convolutional and attentional blocks. After $T$ steps of the denoising process, we obtain the final output $x_0$, which is the generated image $v'$.


% To mitigate hallucinations caused by diminishing attention to visual information during auto-regressive token generation, it is beneficial to have additional assistive visual references for token verification. 
% %
% Specifically, we propose using images generated from descriptions of the visual input as references during token prediction, allowing for the verification of whether hallucination is occurring at the current prediction step.

\textbf{Effectiveness of Text-to-Image Generative Models in Reflecting Hallucinations}. 
% \martin{Further clarify motivation, setup, and implication of results in this section can be crucial and can greatly improve chance of acceptance.}
We validate the effectiveness of generative models in reflecting hallucinations through an empirical study, as shown in Figure~\ref{fig:feedback}.\footnote{\RebuttalRevision{For Figure~\ref{fig:feedback}, we evaluate 1,000 CHAIR samples (\textit{Left}) and 3,000 POPE samples (\textit{Right}).}} The experimental results demonstrate that \textit{text-to-image generative models can provide valuable self-feedback for mitigating hallucinations} at both the response and token levels.




\begin{wrapfigure}[18]{r}{0.6\textwidth}
  \begin{center}
  \vspace{-3pt}
    \includegraphics[width=\linewidth]{figs/feedback.png}
  \end{center}
  \vspace{-10pt}
  \caption{\textbf{Text-to-image generative models can provide feedback for reflecting hallucinations}. (\textit{Left}) Density plot of CLIP similarities and bar plot of average $\text{CHAIR}_I$ in each bin on the CHAIR benchmark; (\textit{Right}) Density plots of token-level JS divergence for both hallucinatory and non-hallucinatory tokens on the POPE benchmark.}
  \label{fig:feedback}
\end{wrapfigure}


\looseness=-1
We conduct the following two experiments:
(1) We generate an image $v'$ using diffusion model based on the initial caption provided by LLaVA-1.5 and compute the CLIP image similarities between the original image $v$ and the generated image $v'$ using OpenCLIP~\citep{cherti2023reproducible} ViT-H/14 backbone. Following prior work, we use the CHAIR~\citep{rohrbach2018object} benchmark, a rule-based metric on MS-COCO~\citep{lin2014microsoft} for evaluating object hallucination from generated captions. We report the average per-instance metric $\text{CHAIR}_I$ within each bin of CLIP similarity, which evaluates the object hallucination rates in the entire initial response. 
As shown in Figure~\ref{fig:feedback} (\textit{Left}), a clear negative correlation between hallucination rates and CLIP similarities is observed (with a correlation coefficient of $\rho=-0.63$).  This indicates that \textit{lower similarity between the original image and generated image corresponds to higher rates of hallucinations at the response level}.
(2) Similarly, we generate an image $v'$ based on the initial response given by LLaVA-1.5 for each instance on the POPE~\citep{li2023evaluating} benchmark. In Figure~\ref{fig:feedback} (\textit{Right}), we present the density plot of Jensen-Shannon (JS) divergence between the predicted probabilities for both images, \ie, $p_{\theta}(y_t | v, \mathbf{x}, \mathbf{y}_{<t})$ and $p_{\theta}(y_t | v', \mathbf{x}, \mathbf{y}_{<t})$, for hallucinatory and non-hallucinatory tokens.\footnote{Note that POPE benchmark contains yes-or-no questions about object existence. In this experiment, we evaluate only the first response token (\ie, \texttt{yes} or \texttt{no}) to determine the presence of hallucinations.} The results show that the density of JS divergence follows a long-tail distribution, with hallucinatory tokens exhibiting significantly longer tails and higher JS divergence. This shows \textit{JS divergence between probabilities derived from the original and the generated image corresponds well to hallucinations at the token level.}
% \martin{Alternative: \textit{``This shows JS divergence between probabilities of the original and the generated image corresponds well to hallucinations at the token level.''}}
% \martin{Needs clarification.}
These observations provide insights into the effectiveness of generative models in reflecting hallucinations, and motivate us to incorporate the generative feedback during the decoding process.

% As a preliminary experiment, we first describe the images in the CHAIR benchmark~\citep{rohrbach2018object} using an LVLM. Based on these generated descriptions, we employ a text-to-image generative model to synthesize images that closely align with the descriptions. We then calculate the CLIP similarity~\citep{radford2021learning} between the original images and the synthesized images. To assess the level of hallucination, we compute the $\text{CHAIR}_I$ and $\text{CHAIR}_S$ metrics (as described in Eq.~\ref{eq:chair metrics}) for each image and visualize the results in a bar chart. As shown in Fig.~\ref{fig:feedback}, a strong correlation is evident between the hallucination level and the image similarity. Higher CLIP scores, indicating greater similarity, are associated with lower levels of hallucination.





\subsection{Self-Correcting Decoding with Generative Feedback}
\label{sec:token}
In this section, we focus on effectively utilizing generative feedback during the decoding process to mitigate potential hallucinations. Specifically, we propose a self-correcting decoding approach that leverages generative feedback to \textit{confirm} or \textit{revise} the initial response by selectively enhancing or contrasting the logits for each generated token based on the measured divergence between the two predicted probability distributions.



% During the token decoding phase, unlike previous methods such as VCD~\citep{leng2024mitigating}, which contrast outputs from the original input and a distorted input, we aim to adaptively utilize the synthesized visual reference to confirm or revise the LVLM's token predictions.
%
Specifically, to predict a specific token $y_t$, we utilize LVLMs to generate two output distributions, each conditioned on either the original image $v$ or the synthesized visual reference $v'$, expressed as:
\begin{equation}
    p_{\theta}(y_t | v, \mathbf{x}, \mathbf{y}_{<t})\!=\!\mathtt{Softmax}\!\left[f_\theta(y_{t}| v,\mathbf{x},\mathbf{y}_{<t})\right]\!, \,\,\, p_{\theta}(y_t | v', \mathbf{x}, \mathbf{y}_{<t})\!=\!\mathtt{Softmax}\!\left[f_\theta(y_{t}| v',\mathbf{x},\mathbf{y}_{<t})\right].
\end{equation}
We define and compute the following distance metric based on Jensen-Shannon (JS) divergence at each timestep $t$ to quantify the discrepancy between two next-token probability distributions:
\begin{gather} 
    d_t(v, v') = \mathcal{D}_{\mathrm{JS}} \left(p_{\theta}\left(y_t | v, \mathbf{x},\mathbf{y}_{<t}\right) \parallel p_{\theta}\left(y_t | v', \mathbf{x},\mathbf{y}_{<t}\right) \right), \nonumber\\
\text{where} \,\, \mathcal{D}_{\mathrm{JS}}(P \parallel Q) = \frac{1}{2} \mathcal{D}_{\mathrm{KL}}(P \parallel M) + \frac{1}{2} \mathcal{D}_{\mathrm{KL}}(Q \parallel M), \,\text{and} \,\, M=\frac{1}{2}(P+Q).
\end{gather}
Here, $\mathcal{D}_{\mathrm{KL}}$ represents the Kullback-Leibler (KL) divergence. Note that $d_t(v, v') \in [0, 1]$ is a symmetric metric, providing a fine-grained measure of how closely the two distributions align as the model predicts each subsequent token. 

We consider two scenarios based on the token-level generative feedback: (1) If the two predictions are aligned and both images agree on a specific token prediction, we \textit{confirm} the original prediction as correct, and the auxiliary prediction from the generated image can be combined with the original prediction for enhancement (complementary decoding~\citep{woo2024ritual}). (2) Conversely, if there is a significant discrepancy between the predictions, indicating that the original prediction is likely hallucinatory, we \textit{revise} the original response by using the generated visual input as a contrasting reference to refine the initial next-token prediction (contrastive decoding \citep{leng2024mitigating}). To implement this, we introduce a distance threshold $\gamma$ and develop two corresponding decoding approaches as follows:
\begin{equation}
y_t \sim p_{\theta}(y_t) = 
\begin{cases} 
\mathtt{Softmax}\left[f_\theta(y_{t}| v,\mathbf{x},\mathbf{y}_{<t}) + \alpha_{1} \, f_\theta(y_{t}| v',\mathbf{x},\mathbf{y}_{<t})\right], & \text{if } d_t(v, v') < \gamma; \\[5pt]
\mathtt{Softmax}\left[(1 + \alpha_{2}) \, f_\theta(y_{t}| v,\mathbf{x},\mathbf{y}_{<t}) - \alpha_{2} \, f_\theta(y_{t}| v',\mathbf{x},\mathbf{y}_{<t})\right], & \text{if } d_t(v, v') \geq \gamma,
\end{cases}
\end{equation}
where $\alpha_1$ and $\alpha_2$ are hyperparameters that control the influence of the generated visual reference in the final prediction. Note that setting $\alpha_1=0$ or $\alpha_2=0$ degrades this process to regular decoding. The final generated token $y_t$ is sampled from the multinomial distribution with probabilities $p_{\theta}(y_t)$.

% Then we calculate the Jensen-Shannon divergence (JS-divergence)~\citep{menendez1997jensen} of the two probability distributions. The JS-divergence $\mathcal{D}_{\mathrm{JS}}(p_{ori} \parallel p_{syn})$ is calculated as:
% \begin{gather}
% M = \frac{1}{2}(p_{ori} + p_{syn}),\\
% \mathcal{D}_{\mathrm{JS}}(p_{ori} \parallel p_{syn}) = \frac{1}{2} \mathcal{D}_{\mathrm{KL}}(p_{ori} \parallel M) + \frac{1}{2} \mathcal{D}_{\mathrm{KL}}(p_{syn} \parallel M),
% \label{eq:js_divergence}
% \end{gather}
% where $M$ is the mixture distribution of the two predictions and $\mathcal{D}_{\mathrm{KL}}$ stands for the KL-divergence of two distributions.

% The JS-divergence measures the distance between two distributions, making it suitable for adaptive intensity token decoding. Specifically, if the distance is below a certain threshold, we confirm that the two distributions are similar, and the visual reference should be used to validate the original prediction. Conversely, if a significant discrepancy between the distributions is detected, the visual reference is used to revise the original prediction. This process can be mathematically formulated as:

