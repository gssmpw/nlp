\subsection{Underlying Reasons of Hallucination.}
Hallucinations in LVLMs can be attributed to various factors, such as visual uncertainty due to language bias and priors~\cite{leng2024mitigating, favero2024multi}, and object class bias inherent in training data caused by co-occurrence~\cite{biten2022let, zhou2024analyzing}. 
%
These factors can be considered external causes of object hallucination. However, when examining the inference process of LVLMs themselves, the underlying reason can be traced to the auto-regressive nature of the decoding phase. 
%
Specifically, during the next-token prediction period, as the generated sequence grows longer, LVLMs increasingly rely on the accumulated history of generated tokens $y_{<t}$, which inevitably reduces attention to the visual tokens $v$.
This degradation of the visual information's representation leads to hallucinations, exhibiting behaviors and patterns observed in the aforementioned studies~\cite{zhou2024analyzing, leng2024mitigating}. Moreover, this phenomenon becomes more pronounced as the generated responses grow longer, with a noticeable correlation between higher hallucination rates and longer sequences of generated tokens~\cite{chen2024halc, huang2024opera}.
