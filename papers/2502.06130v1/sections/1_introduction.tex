\vspace{-10pt}

\section{Introduction}
\vspace{-4pt}
\label{sec:intro}
% \looseness=-1
Large Vision-Language Models (LVLMs) have demonstrated remarkable performance across various multi-modal tasks, such as image captioning and visual question answering, by extending the capabilities of powerful Large Language Models (LLMs) to incorporate visual inputs~\citep{liu2023visual,li2023blip,dai2024instructblip,bai2023qwen,ye2024mplug}. Despite their proficiency in interpreting both visual and textual modalities, these models often suffer from \textit{hallucinations}, where LVLMs erroneously produce responses that are inconsistent with the visual input~\citep{li2023evaluating,gunjal2024detecting,yin2023woodpecker,wu2024evaluating}.
% in their generated text response.
% \yaqi{Question: Only hallucinations in generated text response? not in generated image response? We need to find a way to scope it down naturally. Or maybe define LVLM, does the stable diffusion we used belog to LVLM?} 
% Specifically, this refers to the phenomenon where LVLMs erroneously produce responses that are inconsistent with the visual input~\citep{li2023evaluating,gunjal2024detecting,yin2023woodpecker}. 
This potential for misinformation raises significant concerns, limiting the models' reliability and restricting their broader deployment in real-world scenarios~\citep{liu2024survey,bai2024hallucination,chen2024detecting,zhao2024fact}.

\looseness=-1
Recent research has revealed that a major cause of hallucinations in LVLMs is the over-reliance on language priors due to biased training sets, which can override the visual content in response generation~\citep{bai2024hallucination,liu2024survey,leng2024mitigating}. In response, various strategies have been developed to detect and mitigate these hallucinations by directly introducing additional training~\citep{chen2024alleviating,sun2023aligning,jiang2024hallucination, chen2023mitigating,zhang2024reflective}, demonstrating promising results in reducing over-reliance. However, the need for additional data and costly training processes hinders their deployment in downstream tasks. 
More recently, a new paradigm of methods has emerged to tackle the hallucination problem in LVLMs by intervening in the decoding process~\citep{huang2024opera,deng2024seeing,kim2024code}. Among these, recent training-free contrastive decoding-based methods~\citep{li2023contrastive} have proven effective in mitigating undesired hallucinations by contrasting token predictions derived from original visual input with bias-inducing counterparts, such as no/distorted visual input~\citep{favero2024multi,leng2024mitigating}, disturbed instructions~\citep{wang2024mitigating2}, or premature layers~\citep{chuang2024dola}. 
% \yaqi{This visual cue part can be further emphasized. Maybe we can start a new paragraph here and merge the first half of the original paragarph to the previous one.}

While these contrastive decoding-based methods effectively mitigate hallucinations arising from language priors, we recognize that hallucinations can also originate beyond language bias, stemming from visual deficiencies in LVLMs~\citep{tong2024eyes}. For instance, in counting hallucinations, language does not imply any count information; instead, miscounts largely arise from visual recognition errors of LVLMs, as complex scenes include numerous, similar objects at ambiguous positions which may confuse the LVLMs, leading to incorrect visual understanding and, consequently, hallucinated answers. Therefore, we argue that current contrastive decoding-based methods may struggle to generalize effectively across different types of hallucinations.



% Although these methods are effective in reflecting the language bias, we recognize that not all types of hallucinations come from pure language bias, but may also come from visual deficiencies of MLLMs.


% However, we recognize that these methods typically apply generic distortions uniformly across all samples, which limits their ability to generalize across various hallucination types. For instance, in counting-related hallucinations, while generic distortions may effectively reflect language bias, they also obscure key visual details, preventing the model from accurately refining its understanding to count objects.


% Take the counting problem as an example: while applying generic disturbances across all samples may effectively reflect language bias, it also diminishes important visual details, preventing the model from refining its understanding to accurately count the objects.










% However, we recognize that these methods design instance-agnostic counterparts that typically apply generic disturbances but fail to adapt to the specific details of each input instance.
% While effective at reflecting language bias, they struggle to capture instance-specific fine-grained visual details, making them less effective at reducing related hallucinations, such as object counts or appearances.

% However, we recognize that these methods design instance-agnostic counterparts that do not adapt to the specific details of each input instance. Although effective at reflecting language bias, they hardly discriminate instance-specific fine-grained visual details and are less effective in reducing related hallucinations such as object counts, colors, or appearances.

% and are less effective in reducing hallucinations in instance-specific fine-grained visual details such as object counts, colors, or appearances.

% We recognize that different instances may have totally different instance-specific fine-grained visual details that are prone to hallucinations, such as object counts, colors, or appearances, therefore generating instance-agnostic counterpart as in these methods may not be fully effective.

% which may have various fine-grained visual details that are likely to be hallucinated, such as object counts, colors, or appearances.

% methods typically apply the same approach across different instances and overlook the unique characteristics of each instance, which make them difficult to discriminate fine-grained visual details such as object counts, colors, or appearances. Can we 

% with various crucial visual details, lacking per-instance intervention, which reduces their effectiveness in counting/color hallucinations. 


% \martin{by contrasting distributions derived from original and distorted input,} and have recently become a popular approach because of no additional training. \martin{However, visual distortions are applied to the full image without targeting task-relevant semantic parts; can we further reduce hallucination by operating on a finer granularity, e.g., token-level?} \yaqi{Then what is their limitation? Can we say sth like they only focus on removing language bias not enhacing visual cues? Then we it can lead to our method.}

%instance-aware sample-specific per-instance intervention 
% Take counting hallucinations as an example 

% \yaqi{We need some intution here. Why we want to explore this direction.}
% \zifu{To address hallucinations arising from both language bias and visual deficiencies, we propose leveraging powerful text-to-image generative models (e.g., Stable Diffusion~\citep{rombach2021highresolution,podell2024sdxl}) to mitigate various types of hallucinations in LVLMs.}
\looseness=-1
In this work, we explore the potential of leveraging powerful text-to-image generative models (\eg, Stable Diffusion~\citep{rombach2021highresolution,podell2024sdxl}) to mitigate various types of hallucinations in LVLMs. 
Our work is based on a simple yet intuitive hypothesis: Given a visual input and a textual prompt to an LVLM, if the generated response conditioned on the original image is accurate and non-hallucinatory,  a text-to-image generative model should be capable of reversing this process to produce a similar image from that response. 
Alternatively, if there is a discrepancy between the original image and the one generated from the response, this difference can serve as valuable self-feedback, guiding the decoding process to correct potential hallucinations in the initial response.
% Therefore, the discrepancy between the original image and the generated image can serve as valuable self-feedback, guiding the decoding process to correct potential hallucinations in the initial response.
% \martin{Our approach provides token-level feedback to correct hallucination, a further improvement from image-level contrastive approaches.}
To verify this hypothesis, we conduct an empirical study (in Section~\ref{sec:diffusion}), demonstrating that \textit{generative models can provide valuable self-feedback for mitigating hallucinations at both the response and token levels}.
% \yaqi{``any'' seems too strong. There are a lot of discrepancies, but we are only using the relevant ones. And we want to also hint that, but only the discrepancy but also the similarity values}

% Our approach is based on a simple yet intuitive assumption \yaqi{assumption -> hypotheses. Simplify the assumption sentence if you could.}: given a visual input $v$ and a textual prompt $\mathbf{x}$ that generates an image-conditioned response $\boldsymbol{\tau}$ from an LVLM, a text-to-image generative model should be capable of reversing this process to produce a similar image $v'$ based on $\boldsymbol{\tau}$. We hypothesize that the discrepancy between the original image $v$ and the generated image $v'$ can serve as valuable self-feedback, guiding the decoding process to correct potential hallucinations in the initial response. 
% To verify this hypothesis, we conduct an empirical study (presented in Section~\ref{sec:diffusion}), which demonstrates that generative models can function as zero-shot hallucination detectors, offering feedback at both the response and token levels.
% To the best of our knowledge, we are the first work to explore the use of generative models for mitigating hallucinations in LVLMs.



\begin{figure}[t]
  \begin{center}
     \makebox[\textwidth]{\includegraphics[width=\textwidth]{figs/intro3.png}}
  \end{center}
  \vspace{-14pt}
  \caption{\looseness=-1 
  % \yaqi{The numbers are confusing as they are. Please explain what they mean in the caption. If they are used to denote the time steps, we probably need to add a 2 for stable diffusion.} 
  \textbf{Generative models can visualize and help correct various types of hallucinations in the initial response}. \raisebox{-0.07cm}{\includegraphics[height=0.35cm]{figs/icon1.png}} In the first query, we provide LLaVA-1.5~\citep{liu2023visual} with the prompt ``\texttt{Describe this image in detail}'' to produce captions for two examples from LLaVA-Bench. Based on the initial response, we utilize Stable Diffusion XL~\citep{podell2024sdxl} to generate a new image $v'$, which effectively highlights hallucinations and provides valuable self-feedback. \raisebox{-0.07cm}{\includegraphics[height=0.35cm]{figs/icon2.png}} In the second query, our approach incorporates both the original image $v$ and the generated image $v'$ into the decoding process, using the feedback to successfully correct various types of hallucinations.}
  \label{fig:intro}
   \vspace{-10pt}
\end{figure}


Building on this insight, we introduce self-correcting Decoding with Generative Feedback (DeGF), a novel training-free decoding algorithm that effectively incorporates feedback from text-to-image generative models to recursively enhance the accuracy of LVLM responses. 
Specifically, for each instance, we generate a new image based on the initial response, which serves as an \textit{auxiliary visual reference} to assess and verify the accuracy of the initial output.
% we treat the generated image based on the initial response as an \textit{auxiliary visual reference} and re-query the LVLM using both the original image and this reference. 
We propose self-correcting decoding that either enhances or contrasts predictions from the original and this reference based on the auxiliary visual reference, \textit{confirming} or \textit{revising} the initial LVLM response based on the degree of divergence between the two predictions. By integrating this additional visual reference and generative feedback, LVLMs can gain enhanced visual insights and verify the initial response to ensure accurate visual details in the text outputs.
% \martin{(define ``double-check''?)}
In Figure~\ref{fig:intro}, we demonstrate that incorporating generative feedback in our approach can reduce various types of hallucinations, including object existence, visual appearance, counting, \etc. To the best of our knowledge, we are the first work to explore the use of text-to-image generative feedback as a self-correcting mechanism for mitigating hallucinations in LVLMs.


% the use of generative models \martin{to generate visual reference} for mitigating hallucinations in LVLMs.


\looseness=-1
The effectiveness of DeGF is evaluated on LLaVA-1.5, InstructBLIP, and Qwen-VL across six benchmarks: POPE~\citep{li2023evaluating}, CHAIR~\citep{rohrbach2018object}, MME-Hallucination~\citep{fu2023mme}, MMBench~\citep{liu2025mmbench}, MMVP~\citep{tong2024eyes}, and LLaVA-Bench. Extensive experimental results validate the effectiveness of our DeGF in mitigating various types of hallucinations in LVLMs. Qualitative case studies and GPT-4V-aided evaluation on LLaVA-Bench further demonstrate that our approach enhances both the accuracy and detailedness of the LVLM responses. 

% \clearpage
The contributions of this paper are summarized as follows:
\begin{itemize}
    % \item We conduct a pilot study \yaqi{``pilot study'' sounds preliminary. The subject this contribution should be the discovery (we discovered xxx), but the pilot study. We don't need to mention it in the contribution.} discovering the potential of generative models in reflecting hallucinations in LVLMs and demonstrate that generative models can function as zero-shot hallucination detectors, offering feedback at both the response and token levels.
    \item We investigate the potential of text-to-image generative models in mitigating hallucinations in LVLMs and demonstrate that text-to-image generative models can provide valuable self-feedback for mitigating hallucinations at both the response and token levels.
    \item We propose self-correcting Decoding with Generative Feedback (DeGF), a novel training-free decoding algorithm for LVLMs that recursively enhances the accuracy of responses by integrating feedback from text-to-image generative models with complementary/contrastive decoding.
    \item Extensive experimental evaluations across six benchmarks demonstrate that our DeGF consistently outperforms state-of-the-art approaches in effectively mitigating hallucinations in LVLMs.
\end{itemize}
