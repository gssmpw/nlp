\section{Measuring Bias in RAG}
\label{sec:measure}
Before understanding the effect of individual RAG components on bias, we first define RAG components (\refsec{rag-system}) and bias measures (\refsec{bias-metric}) for each of the components and the entire RAG system. Based on these definitions, we establish a linear model capturing the characteristics of bias conflict (\refsec{linear-model}). We then describe the two datasets used in our case studies on RAG biases, namely, gender and political (\refsec{datasets}), and our experimental settings (\refsec{exp-details}).
\input{figs/task-example-table}
\subsection{RAG as a Modular System}
\label{sec:rag-system}

As shown in \reffig{diagram}, we view a RAG system as a sequential connection of individual modular components: the LLM ($L$), embedder ($E$), and corpus ($C$). An embedder first retrieves documents from the corpus that are relevant to the query. Then, the LLM takes as input the query and document and generates an output which can either be tokens or logits. The modularity allows each component to be substituted with another component of the same type. 

\subsection{Bias Metric}\label{sec:bias-metric}
We define the biases in RAG that we explore as systematic skews in terms of identities, opinions, or perspectives in the documents or outputs. To quantify these biases, we adapt the retrieval bias metric \emph{Rank Bias} or \emph{Average Rank Bias} \citep{rekabsaz2020neural,kulshrestha2017quantifying} and apply it to all components. \footnote{Note that our definition of bias is different from LLM bias measures which try to measure the presence of stereotypical associations in systems or documents \citep{parrish2021bbq,nangia2020crows,nadeem2020stereoset}.} 

Given two opposing groups $g_1$ and $g_2$ (e.g., male vs. female), we calculate our bias metric $b$ in two steps. First, we assign two $\{0,1\}$ binary scores $b_1$ and $b_2$ which is $1$ if the document or output is related to each group, $g_1$ and $g_2$ respectively, and $0$ otherwise. Second, we calculate the difference between $b_1$ and $b_2$ and average over all queries. When $S$ is the set of documents or outputs,

\begin{align}
\label{eqn:bias-metric}
b &= \frac{1}{|S|}\sum_{s \in S} \left(b_1(s) - b_2(s)\right) 
\end{align}

Our bias metric takes the range $[-1,1]$ where $1$ implies complete bias towards $g_1$ and $-1$ towards $g_2$. We uniformly measure the bias of each component using the metric defined in \refeqn{bias-metric}. This unified approach enables us to directly compare biases across different components while incorporating standard retrieval bias metrics. We apply \refeqn{bias-metric} to each component as follows.

We measure the \textbf{corpus bias ($C_b$)} as the average bias of all documents within the corpus. We measure the \textbf{embedder bias ($E_b$)} as the average bias over all queries for each top-1 retrieved document. We note that $E_b$ inherently incorporates any bias from the corpus, as the two are inseparable. We measure the \textbf{LLM bias ($L_b$)} as the average bias of the LLM's output over all queries when no document is retrieved. Finally, we measure the \textbf{RAG bias ($R_b$)} similarly to the LLM bias but with a retrieved document as input.

\subsection{Bias Relation Between Component and RAG System}
\label{sec:linear-model}
To model the bias conflicts between the components, we define the following relationship:
\begin{align}
\label{eqn:bias}
R_b = s\cdot E_b + L_b + \epsilon
\end{align}
where $s$ is the sensitivity of bias conflict and $\epsilon$ is extraneous knowledge conflict. 

\paragraph{Sensitivity ($s$)} The sensitivity of a particular RAG system shows how much the change in embedder bias is propagated through the LLM. $s=1$ means the LLM, and consequently the RAG system, is heavily influenced by the embedder. On the other hand, $s=0$ means that changing the embedder bias minimally affects the bias of the RAG system.

\paragraph{LLM bias ($L_b$) and noise ($\epsilon$)} Conceptually, the RAG bias should equal the LLM bias when the embedder bias is 0 (i.e., $R_b=s\cdot E_b + L_b = s\cdot 0 + L_b = L_b$). However, this does not hold due to knowledge conflict from extraneous factors such as document quality or relevance \citep{chen2022rich,xie2023adaptive}. To account for the extraneous knowledge conflict, we add a noise term $\epsilon$.

\input{figs/base-comp-table}

\subsection{Gender and Political Bias}\label{sec:datasets}
As case studies, we mitigate two types of social biases: gender bias and political bias, which we later show to have high and low sensitivity, respectively. Although bias can involve multiple groups, we follow previous work \citep{nadeem2020stereoset,liang2021towards,kotek2023gender,zhao2024beyond,hu2024no,wu2024does} and consider a binary setting with two opposing groups: male vs. female and liberal vs. conservative. Furthermore, we specifically design our tasks so that the LLM can produce correct answers while being skewed in either way. \footnote{We release our datasets and code at \url{https://github.com/danielkty/debiasing-rag}} Our tasks focus on biases that induce representational harm where the RAG system may consistently represent a specific group \citep{blodgett2020language}. 

\paragraph{\genderData Dataset} 
Using GPT (\texttt{gpt-4o}), we create a 172/145 (train/test) example QA dataset where each question can be answered with a male or female public figure. The output is a generated name of a public figure as seen in \reftab{prompt} and the exact prompt template is shown in \refapp{prompt}. We set $g_1$ to be women and $g_2$ to be men.


\paragraph{\politicalData Dataset} 
We create a 600/200 (train/test) example binary-choice QA dataset of politically controversial questions where each question can be answered with a liberal or conservative choice. We utilize TwinViews-13k \cite{fulay2024relationship} which contains matched pairs of left and right-leaning political statements and turn it into a binary-choice task by prompting GPT (\texttt{gpt-4o}) to generate the question encompassing the two choices (\reftab{prompt}). The prompt template is shown in \refapp{prompt}. The output is the next-token probability for the two choices (A/B) and we randomize their order to remove inherent bias within the prompt template. We consider $g_1$ to be conservative views and $g_2$ to be liberal views. Please refer to the dataset creation details in \refapp{dataset}.

\paragraph{Extracting Gender \& Political Bias in Text}
We use an LLM judge (\texttt{GPT-4o-mini}) as a binary classifier to measure the gender or political leaning of each text (corpus document or output), except for the LLM output for \politicalData in which we use the ground truth labels provided by TwinViews-13k. The LLM-as-a-judge setup, especially with GPT, has recently shown great performance with high human agreement rates \citep{zheng2023judging} even for evaluating bias \citep{kumar2024decoding}. We also find decent agreement of the LLM-judge with our own in-house annotations, as described in \refapp{human-judge}. The LLM judge prompts are shown in \refapp{judge}.

\subsection{Experimental Details}\label{sec:exp-details}
\paragraph{Models Examined} 
We test on 6 different LLMs: Llama 3.1 8/70/405B Instruct \citep{dubey2024llama}, Gemma 2 9/27B IT \citep{team2024gemma}, and Mistral 7B Instruct v0.3 \citep{jiang2023mistral7b}. We additionally test on Olmo 2 7B Instruct \citep{olmo20242}, Qwen 2/2.5 7B Instruct \citep{yang2024qwen2technicalreport, yang2024qwen2}, and Zephyr 7B Beta \citep{tunstall2023zephyr} in \refapp{more-models}. We refer to each as Llama 8/70/405B, Gemma 9/27B, Mistral, Olmo, Qwen 2/2.5, and Zephyr. We use Huggingface models for Llama 8B, Mistral, Olmo, Qwen 2/2.5, and Zephyr and use Together AI serverless models for the rest (\texttt{Turbo} for Llama models). We use greedy decoding when generating from the LLM.

\paragraph{Retrieval Setting}
For retrieval, we focus on one dense retriever \citep[\texttt{GTE-base};][]{li2023towards} of 109M parameters to test the effect of different bias mitigation techniques (i.e., fine-tuning, projecting, and sampling). Dense retrievers incorporate semantic meaning as opposed to sparse retrievers, allowing easy control of bias. We evaluate and show results for an additional embedder \citep[\texttt{E5-base-v2};][]{wang2022text} in \refapp{e5}. For simplicity, we focus on retrieving the top-1 document through cosine similarity. Throughout the rest of the paper, the base embedder refers to \texttt{GTE-base}.

\paragraph{Retrieval Corpus}
We use different corpora for training and evaluation. For training in \refsec{fine-tune}, we use MS MARCO \citep{bajaj2016ms}, FEVER \citep{thorne2018fever}, DBPedia \citep{hasibi2017dbpedia} for gender bias and additionally use Webis-Argument-Framing-19 \citep{ajjour:2019b}, Webis-ConcluGen-21 \citep{syed:2021a}, and args.me \citep{ajjour:2019a} for political bias. These are corpora of web searches, Wikipedia, and political debates (\refapp{training}). For the test corpora during evaluation in \refsecs{existing}{debiasing}, we use Natural Questions (NQ) \citep{kwiatkowski2019natural} for gender bias, which is constructed from Wikipedia, and PolNLI \citep{burnham2024politicaldebateefficientzeroshot} for political bias, which is a collection of political documents from a wide variety of sources (e.g., social media, news articles, and congressional newsletters).