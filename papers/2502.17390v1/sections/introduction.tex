\section{Introduction}
Retrieval-augmented generation (RAG) \citep{guu2020retrieval,asai2023self,shi2023replug} is a promising modular AI system that enhances factuality and privacy in large language models (LLMs). 
This safety enhancement is accomplished by breaking the system into three different components: the LLM, embedder, and corpus which overall complement the LLM's knowledge with non-parametric information (\reffig{diagram}). However, each of these components risk introducing their own biases (e.g., skews towards outputs representing certain identities or opinions) into the RAG system, which could cause representational harm and unsafe user interactions \cite{blodgett2020language,barocas2017problem}. 

Understanding the interaction of bias between each component in a RAG system remains a significant challenge \citep{hu2024no,wu2024does,gao2024modular}.
Each component may not only amplify bias but also conflict with each otherâ€™s bias, creating a phenomenon we call \emph{bias conflict}. For example, given the query \emph{Who is a famous singer?}, an embedder biased towards males may retrieve a document about \emph{Michael Jackson}, while a corpus biased towards females would make \emph{Whitney Houston} be retrieved. The opposing biases make the final retrieved document unclear. Additionally, an LLM biased towards females would also conflict with the embedder, further complicating the process. Thus, given the ambiguity of the final output bias, it is crucial to understand how biases from each component interact in order to effectively mitigate bias of the entire RAG system.

\input{figs/diagram}

In this work, we investigate such bias conflicts in RAG systems. We specifically examine the embedder's role in bias conflict as well as its potential for mitigating RAG biases. Focusing on the embedder has three advantages over mitigating bias through the LLM or corpus. First, most embedders are smaller than LLMs. The best performing embedder on the MTEB leaderboard \citep{muennighoff2022mteb} is only 7B parameters while LLMs easily have a couple hundred billion parameters. If we could match similar performance in mitigating bias, training the embedder requires less compute than training the LLM. Second, LLMs are prone to catastrophic forgetting during fine-tuning \citep{kotha2023understanding}, which degrade the generation quality. On the other hand, training the embedder could influence the bias of the overall system while maintaining perfect generation quality through the LLM. Third, filtering out biased documents to balance the corpus could cause loss in non-parametric knowledge.  

We empirically examine bias conflict through gender and political bias, as case studies representing both a clear-cut and a nuanced type of bias, respectively. We construct our tasks so that bias can be introduced independently of factuality (\refsec{datasets}). This lets us examine subtle bias concealed under factual correctness, making it difficult for users to recognize \citep{kumar2024subtle}. We fine-tune 120 embedders to have different biases with PEFT and WiSE-FT \citep{wortsman2022robust} in order to observe how training affects utility. To investigate how changing the embedder bias impacts the RAG bias, we evaluate 40 embedders each connected to 6 different LLMs, resulting in 240 different RAG systems. We further evaluate the 40 embedders on corpora of varying bias, portraying the effect of changing the corpus. Through these experiments, we answer the following questions:

\textbf{Q1}: Can we predict the overall bias of a RAG system given the biases of individual components (\refsec{existing})? We measure the bias of each individual component and the entire RAG system. We find that even when knowing the exact bias of the embedder and LLM, it is challenging to predict whether the RAG bias will amplify or decrease because of bias conflict.

\textbf{Q2}: Given complex bias conflict, how can we effectively mitigate bias in a RAG system (\refsec{debiasing})? Due to a linear relationship between the bias of the RAG system and embedder, we find that reverse biasing the embedder through fine-tuning is effective in mitigating the overall RAG bias. We also observe that LLMs are more sensitive to changes in the embedder for gender bias than political bias. Furthermore, despite the small size of our embedder (109M), we are able to overcome the bias of a larger language model (405B). 

\textbf{Q3}: How does bias conflict from the corpus affect the robustness of mitigating bias through the embedder (\refsec{corpus})? To perturb the corpus bias, we create a small corpus where the bias of each document is pre-evaluated. We find that an embedder which mitigates bias given a fixed corpus also mitigates bias for small perturbations in the corpus. 

Through this work, we show that increasing fairness in the LLM or embedder may not be the optimal solution to mitigating bias in the overall RAG system. Considering the interaction and conflict among biases in each component is crucial towards achieving less biased RAG systems.

