\section{Limitations}
\label{limit}
While reverse biasing an embedder seems promising, there are a few challenges for real-world implementations, which we hope future work can address.

\paragraph{A Method for Finding the Optimal Embedder}
Athough we have shown the possibility of debiasing a RAG system through the embedder, we do not provide a means to choose the optimal embedder before deployment. As we saw in \reftab{optimal-full}, the optimal embedder changes depending on the LLM. To select an optimal embedder for deployment, one would have to construct a validation corpus, LLM, and validation queries to select the optimal embedder. First, the validation LLM has to be chosen to match the sensitivity of the test LLM being deployment. Second, we have shown in \refapp{ood} that the general trends hold on OOD corpora. Moreover, minor changes in the corpus do not change the optimal embedder (\refsec{corpus}). Therefore, the validation corpus does not need to strictly match the same distribution for the test corpus. Third, the validation queries should be constructed to match the distribution of test queries.

We also note that our decomposition of a RAG system (\refsec{rag-system}) allows each component to be replaced with the same type of component. This reflects how RAG systems in practice are constructed by connecting off-the-shelf LLMs, embedders, and corpora. Each component is usually fixed with only minor updates on the corpus. Therefore, it is not required that one embedder works for all LLMs and corpora but an optimal embedder may be chosen on a case-by-case basis. 

\paragraph{Sensitivity}
Our results show that RAG systems have varying sensitivity to the biases from the corpus or embedder. The effectiveness of our method depends on the sensitivity, which ideally should be high.
However, the sensitivity could change depending on the task, or the \emph{prompt}. \citep{liu2024untangle, zhou2023context, lazaridou2022internet} show that prompting affects knowledge conflict and in return the performance of RAG. For bias conflict, it may not be possible to use the same embedder across tasks if the sensitivity changes drastically. On the other hand, reformatting the prompt can be a way to increase sensitivity for efficient debiasing through the embedder for models or tasks with low sensitivity. Testing how much the sensitivity changes per task is left for future work.

\paragraph{Aggregate Bias}
We have mitigated gender and political bias separately, but in practice, different types of biases arise together. It would be important to find an optimal embedder at the intersection of multiple biases. One method of achieving this would be to mix the fine-tuning data for multiple biases into one dataset. Since the sensitivity for each bias is different, the proportion of the data mixture would be crucial in ensuring that an optimal embedder exists.

\paragraph{Binary Bias}
Although many biases are not binary, we conduct our work on a clear bias definition with only two groups for ease of analysis, which follows previous work on biases in machine learning \citep{nadeem2020stereoset,liang2021towards,kotek2023gender,zhao2024beyond,hu2024no,wu2024does}. Moreover, our work can be extended to non-binary settings. For example, the same process of dividing negative and positive documents for contrastive learning can be applied. The only difference is that there will be multiple negative and positive groups in the training data.

\paragraph{Complex RAG Systems}
Although we have formulated RAG as a three-component system, it is more complex in practice \citep{simon2024methodology,gao2024modular}. We aim to lay the groundwork for understanding bias conflict which can be extended to systems with more components. Understanding the interaction among components with increasing complexity is crucial in preventing representational harm which could have negative societal impact.