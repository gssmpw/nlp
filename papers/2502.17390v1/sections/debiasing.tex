\section{Results: Debiasing RAG} 
\label{sec:debiasing}

\input{figs/training}

Given the complexity of bias conflict in a RAG system, is it feasible to mitigate bias in the entire RAG system? In this section, we try to control the embedder to mitigate bias. In \refsec{fine-tune} we first fine-tune several embedders to span a wide bias range. Then in \refsec{emb-rag}, we construct a RAG system with these embedders while keeping the LLM and corpus fixed to understand the relationship between the embedder bias and RAG bias (\refeqn{bias}). 

\subsection{Controlling the Embedder}
\label{sec:fine-tune}
We increasingly fine-tune the base embedder to retrieve more documents related to females and conservative views to mitigate its bias towards males and liberal views. We train the embedder through a contrastive loss similar to SimCSE \cite{gao2021simcse}. On the train splits of \genderData and \politicalData, we collect the positive documents to be related to females and conservative views and negative documents to be about males and liberal views from the training corpora. Training details are in \refapp{training}. 

To prevent the embedder from losing its original performance after fine-tuning, we implement two different fine-tuning methods.

\begin{enumerate}
    \item \textbf{PEFT} We fine-tune only the last few linear layers of the embedder. This helps the embedder retain its original low-level features and prevents overfitting. We vary the number of layers for each training run among $\ell = \{1, 2, 3, 4\}$.
    \item \textbf{WiSE-FT} After full fine-tuning, we produce a merged model as a convex combination of each parameter of the fine-tuned and base embedder. \citet{wortsman2022robust} show that this increases robustness while maintaining original performance. We choose the interpolation coefficient among $\lambda=\{0.1, 0.3, 0.5, 0.7, 0.9\}$ to produce 
    \[
    \theta^{merge} = (1-\lambda)\cdot\theta^{base} + \lambda\cdot\theta^{fine-tune}
    \]
    where $\theta^{merge}, \theta^{base}, \theta^{fine-tune}$ are the parameters of the merged embedder, base embedder, and fine-tuned embedder.
\end{enumerate}

For both methods, we sweep over learning rates of $\{3\times10^{-5}, 1\times10^{-5}\}$ and training epochs of $\{5, 10, 15\}$. Including normal full fine-tuning, the combination of learning rate, epoch, and training method results in 60 trained embedders per task. We use AdamW \citep{loshchilov2019decoupledweightdecayregularization} with a weight decay of $0.01$ and fix a seed to make training deterministic.

\paragraph{Fine-tuning Results}
\reffig{frontier} shows the bias and validation-task accuracy of the fine-tuned embedders. The bias is measured on a validation corpus and the accuracy is measured on RAG Mini-Wikipedia \cite{smith2008question} which is a small RAG QA benchmark (please refer to the details of validation in \refapp{validation}).

First, we find that light fine-tuning with PEFT or WiSE-FT is sufficient to reverse the embedder bias. On \genderData, the embedder bias started from $-0.52$ and increased to $1.00$. Second, there is a regime where the embedder bias is reversed but the accuracy drop on RAG Mini-Wikipedia is minimal. This results in an outward-pointing Pareto frontier which makes it possible to control the bias of embedders across a wide range while minimizing degeneration or loss in utility. 

\subsection{Embedder \& RAG}
\label{sec:emb-rag}
With our family of embedders controlled to have varying levels of bias, we explore how the embedder bias ($E_b$) affects the RAG bias ($R_b$), and whether there exists an embedder that can mitigate RAG bias to 0 ($R_b=0$).

Among the fine-tuned embedders, we take 20 that are evenly spread out across the full bias range. We compose a RAG system by connecting the embedders with the 6 LLMs and test corpus (NQ for \genderData and PolNLI for \politicalData) and measure the bias of the RAG system for each embedder on the test queries. We define the \emph{optimal embedder} as the embedder that results in $R_b=0$ and call the bias of this embedder the \emph{optimal bias}.

\paragraph{Embedder \& RAG Bias Results}
We show the results for Llama 8/405B, Gemma 27B, and Mistral in \reffig{training} (the full set of 6 LLMs are in \refapp{six-llms}). We see that the linear relationship in \refeqn{bias} holds across all LLMs. As the embedder bias increases, the RAG bias scales linearly. 

\input{figs/training-intercept-full}

\input{figs/utility}
\input{figs/bias-corpus}

We make four observations in \reffig{training}. First, the bias of the optimal embedder is not neutral but mostly reverse biased. \reftab{optimal-full} shows the optimal bias being positive, while it was initially negative in \reftab{base-comp-bias}. This means that reverse biasing a small embedder of 109M parameters can overcome the bias of a larger language model of 405B parameters given high sensitivity ($s\uparrow$). 

Second, all LLMs are highly sensitive to gender bias and less sensitive to political bias. While LLMs are already RLHF fine-tuned to prevent traditional notions of gender bias which count pronouns and occupational bias \citep{lu2020gender,zmigrod2019counterfactual}, we see high sensitivity to \genderData because they are not fine-tuned for figure names. 

Third, the sensitivity for \politicalData is low and noticeably differs per LLM, resulting in different optimal embedders. For example, Llama 405B is easier to debias than Llama 8B or Mistral ($0.04 < 0.40,0.53$) because of its high sensitivity. We posit this is because larger models are more compliant with following instructions, including contextual information. Gemma models are the least sensitive, being consistent with prior work showing that Gemma \citep{trhlik2024quantifyinggenerativemediabias} mainly maintains a centric-view while slightly left-leaning.

Fourth, an LLM that is strongly biased ($|L_b|\uparrow$) does not necessarily mean it has lower sensitivity ($s\downarrow$). It is intuitive to think that a strongly biased LLM creates stronger bias conflict, making it less sensitive to bias from the embedder. However, we observe that Mistral has a very strong political bias ($L_b=-0.81$) but higher sensitivity than Gemma. Thus, it is important to assess $s$ independently of $L_b$.

These findings suggest that while there is a universal linear trend, the sensitivity differs per LLM and bias. \refapp{more-models} even shows the case where debiasing is not possible due to extremely low sensitivity ($s\downarrow$) and strong LLM bias ($|L_b|\uparrow$). It is important to carefully consider the sensitivity when debiasing RAG through the embedder. We show qualitative examples of retrieved documents and LLM responses in \refapp{examples}.

\paragraph{Utility and Robustness}
Although we assessed the utility of the full RAG pipeline in \reffig{frontier}, we also measure the retrieval performance of each optimal embedder on the BEIR benchmark \citep{thakur2021beir} with details mentioned in \refapp{beir}. \reftab{utility} shows that the utility (NDCG@1) of the optimal embedders drops minimally compared to the base embedder. 

We also try controlling the embedder bias through projections and sampling in \refapp{proj-samp} but find that fine-tuning is the most effective at maintaining utility. Additionally, we evaluate on a different embedder \citep[\texttt{E5-base-v2};][]{wang2022text} in \refapp{e5} and change our test corpora to out-of-distribution corpora (HotpotQA \citep{yang2018hotpotqa} and NQ \citep{burnham2024politicaldebateefficientzeroshot}) in \refapp{ood} to find that the trends resemble, suggesting that linearity hold regardless of the retrieval method or corpus.

