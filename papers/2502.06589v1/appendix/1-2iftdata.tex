The instruction fine-tuning stage empowers LLMs with instruction-following capabilities and aligns LLM agents with task-specific requirements and user preferences. To facilitate direct and fair comparison, we employ a diverse range of tasks for both the instruction fine-tuning baseline model, \texttt{LLaMA-3-8B-IFT}, and our model, \texttt{\method-8B-IFT}, including {(1) a general conversation dataset,} \emph{ShareGPT}~\cite{chiang2023vicuna}; {(b) a single-tool function-calling conversation dataset,} \emph{ToolACE}~\cite{liu2024toolace}; and
{(c) a multi-turn planning conversation dataset,} \emph{AgentFlan}~\cite{chen2024agent}.

\noindent $\bullet$ \textbf{ShareGPT}~\cite{chiang2023vicuna} is a general dataset comprising real-world conversations from 70K user data, designed to fine-tune models for enhanced instruction-following capabilities. It significantly improves LLMs' ability to handle complex, multi-turn dialogues. The dataset encompasses a wide range of topics and natural, human-generated prompts, enabling models to learn from authentic interactions. By leveraging real user data, ShareGPT allows models to better generalize across diverse tasks and navigate increasingly complex instructions, closely mimicking real-world conversational scenarios.

\noindent $\bullet$ \textbf{ToolACE}~\cite{liu2024toolace} is a single-tool conversation dataset designed to enhance the function-calling capabilities of LLM agents. It comprises 26,507 APIs across 30 primary domains (\eg, entertainment) and is categorized into 390 coarse-grained sub-domains (\eg, music). In addition, ToolACE accommodates complex nested parameters, manages both parallel and dependent function calls, and encompasses a wide variety of tool-related data.

\noindent $\bullet$ \textbf{AgentFlan}~\cite{chen2024agent} is a multi-turn planning dataset that combines data in two formats: 10\% in ReAct format and 90\% in conversation format. It encompasses 24,703 instances derived from AgentInstruct and ToolBench. AgentFlan deliberately excludes format-following instructions and common reasoning tasks from its training corpus, aiming to elicit pure agent abilities from LLMs without overfitting to specific format protocols.