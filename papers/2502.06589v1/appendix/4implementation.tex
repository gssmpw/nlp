\section{Dataset Construction Details}
\label{app:datacollection}
To scale and diversify the pre-training corpus for LLM agents, we introduce a three-stage construction process (Figure~\ref{fig:dataprepare}) for \dataset in \cref{sec:dataset}. We then include additional data collection details as follows.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.86\linewidth]{figures/fig-dataflow-xl.pdf}
% \vspace{-2ex}
  \caption{Overview of the data collection workflow in \dataset.  }
% \vspace{-2ex}
  \label{fig:dataprepare}
\end{figure}

\subsection{Seed Data Collection Details}
\label{app:4-1}

We begin by assembling a set of high-quality initial data samples to establish a robust foundation.
Specifically, we systematically explore publicly accessible resources to gather high-quality API documentation and associated action trajectories. This includes compiling a diverse dataset of agent behavior from public repositories, official API documentation sources, and data synthesized through LLMs.
Given that the volume of tool-related data remains significantly smaller than that of plain text or code data, we employ data augmentation and generation techniques to expand the tool-related dataset.

\subsubsection{Public APIs.}
First, we collect data from over 1,400 public API documentations\footnote{\url{https://github.com/public-apis/public-apis}} and integrate additional data from official websites, including Huggingface\footnote{\url{https://huggingface.co/docs}}, TorchHub\footnote{\url{https://pytorch.org/docs/stable/index.html}}, and Python Modules\footnote{\url{https://docs.python.org/3/index.html}}, among others.
This compilation includes detailed API definitions and parameter descriptions, enabling the model to gain a better understanding of API functions.
As the depth and location of the documentation vary across different API websites, we apply a three-level scraping strategy: 
(1) \emph{Level 1:} the collected 1,400 URLs; 
(2) \emph{Level 2:} 37,753 URLs appearing on the Level 1 web pages; 
(3) \emph{Level 3:} 83,468 URLs appearing in the Level 2 web pages. 
We then apply URL checks to verify validity and filter for documentation-relevant data by searching for keywords (\eg, ``doc'', ``guide'', ``reference'', \etc).

\subsubsection{Public Repositories.}
To strengthen the model's intrinsic reasoning and planning abilities, we integrate publicly available action trajectories from over $60$ public repositories of related papers and datasets. These action trajectories span multiple domains, including programming code, natural language reasoning steps, embodied AI action sequences, grounded multi-modal data, web interactions, and function call sequences. This diverse range of trajectories, incorporated during the pre-training phase, enhances the model's reasoning capabilities and improves its generalization to various scenarios.

\subsubsection{Code-to-Text Synthesis.}
Given the limited quantity and API coverage of curated data from public APIs and repositories, we exploit the strong generative abilities of LLMs to synthesize additional API documentation and use cases. To produce high-quality synthetic agent data, we utilize \emph{StarCoder-API}\footnote{\url{https://huggingface.co/datasets/luna-code/starcoderdata-apis}} as a knowledge base, which includes code snippets involving third-party APIs. Based on these code snippets and the API function calls within them, we generate corresponding API documentation and associated use cases.
For efficiency, we utilize multiple LLMs from Amazon Bedrock\footnote{\url{https://aws.amazon.com/bedrock/}} for data synthesis, including \texttt{Claude-3-Sonnet}, \texttt{Claude-3-Haiku}~\cite{claude-3}, \texttt{Mistral-Large}~\cite{mistral-large}, \texttt{LLaMA-3-70B-Instruct}~\cite{dubey2024llama}, and \texttt{Command-R-Plus}~\cite{command-r-plus}.

% We offer one synthesized API documentation as example:
% \vspace{1ex}
% \VerbatimInput[label=\fbox{<Code\_to\_Text> Example}]{case/case-code2text}

\subsubsection{Simulated Agent Data.}
To improve the model's ability to adapt based on environmental feedback, we collect action sequences paired with observational data from various environments, represented as $\{o_0, a_1, o_1, a_2, o_2, \cdots, a_{T_g}, o_{T_g}\}$. This representation encodes the model's responses to environmental observations within its parameters. We execute official codes from agent frameworks~\cite{yao2022react, sun2024adaplanner, wang2024llms, shinn2024reflexion} in multi-step reasoning tasks (\eg, HotpotQA~\cite{yang2018hotpotqa}) and sequential decision-making tasks (\eg, ALFWorld~\cite{shridhar2021alfworld}) to collect action trajectories that involve interaction with and feedback from the environment.

% \subsection{Web Data Retrieval Details}
% \label{app:4-2}
% We expand the dataset by retrieving additional relevant data from diverse web sources, thereby enhancing its breadth and coverage.
% We offer two examples of high-quality retrieved data as case studies:

% Example 1 - AgentTraj-L Retrieved Sample:
% \vspace{1ex}
% \VerbatimInput[label=\fbox{<Retrieval> Example-1}]{case/case-retrieval1}

% Example 2 - Android Retrieved Sample:
% \vspace{1ex}
% \VerbatimInput[label=\fbox{<Retrieval> Example-2}]{case/case-retrieval2}

\subsection{Data Quality Control Details}
\label{app:4-3}
We ensure the integrity and relevance of the collected data through continuous quality monitoring and validation procedures.
After retrieving semantically relevant data from the web corpus, we obtain a collection of noisy agent-related data. To preserve the integrity and relevance of our dataset, it is critical to continuously monitor data quality and filter out content that resembles general text rather than agent-specific data. First, we employ \texttt{Claude-3-Sonnet}~\cite{claude-3} as the data annotator to identify whether the sample belongs to agent data or a general web corpus.
Specifically, we annotate a total of $71,473$ samples from the retrieved data, identifying $37,714$ as agent-relevant and $33,767$ as general text paragraphs.
Using the annotated samples, we train a \texttt{fastText}~\cite{joulin2016fasttext} model to effectively recall additional agent-relevant web data. 
We utilize the open-source \texttt{fastText} library\footnote{\url{https://fasttext.cc}} for training, configuring the vector dimension to $256$, learning rate to $0.1$, the maximum length of word n-gram to $3$, the minimum number of word occurrences to $3$, and the number of training epochs to $3$.
After training, the \texttt{fastText} model is used to recall agent-relevant data from the remaining retrieved samples. To filter out low-quality content, we rank the collected pages based on their predicted scores from the \texttt{fastText} model and retain only the top-ranking entries. This filtering process reduces the dataset from approximately 200 billion to 80 billion tokens, ensuring that the preserved data remains highly relevant and of sufficient quality for training LLM agents.
% \subsection{Scaling Law  Details}
% \label{app:4-4}
% Additionally, we extensively investigate the scaling law experiments that guide the optimal distribution and balance of multiple data types within the corpus, facilitating effective scaling and diversification.




\section{Implementation Details}
\label{app:implementation}

We use \texttt{LLaMA-3-8B}~\cite{dubey2024llama} as the backbone for our main experiments. Our training process consists of two stages. In the two-stage pre-training, we set the batch size to $512$ and train the model for $55,000$ steps in each stage, with a learning rate of $2e-4$ and weight decay of $0.01$. For the instruction fine-tuning stage, we reduce the batch size to $16$ and train the model for $24,000$ steps, using a learning rate of $1e-6$ while maintaining the same weight decay of $0.01$.
For parallel pre-training, we apply a tensor model parallel size of $8$ and a pipeline model parallel size of $2$. These values are adjusted to $4$ and $2$, respectively, for instruction fine-tuning. We use the Adam optimizer~\cite{kingma2014adam} with $\beta_1=0.9$ and $\beta_2=0.98$ for all stages. During inference, we maintain a temperature of $T=0$.
Training \texttt{\method-8B-Base} requires 128 NVIDIA A100 (40G) GPUs for 11.1 days (7.7 days for Stage I pre-training and 3.4 days for Stage II pre-training). Training \texttt{\method-8B-IFT} uses 16 NVIDIA A100 (40G) GPUs for 11.6 hours.


% We use LLaMA-3-8B~\cite{dubey2024llama} as the backbone in our main experiments.
% For the two-stage pre-training, we set the batch size to $512$ and then train the model for $55000$ steps with learning rate $2e-4$ and weight decay $0.01$ in both stages.
% During instruction-tuning stage, we set the batch size to $16$ and then train the model for $24000$ steps with learning rate $1e-6$ and weight decay $0.01$.
% For parallel pre-training, we apply tensor model parallel size of $8$ and pipeline model parallel size of $2$, and change to $4$ and $2$ for instruction fine-tuning.
% we use the Adam optimizer~\cite{kingma2014adam} with $\beta_1=0.9$ and $\beta_2=0.98$ for all the stages.
% During the inference stage, we keep temperature $T=0$.
% Training \method-8B-Base uses 128 NVIDIA A100 (40G) GPUs for 11.1 days (7.7 days for Stage 1 pre-training and 3.4 days for Stage 2 pre-training), while training \method-8B-IFT uses 16 NVIDIA A100 (40G) GPUs for 11.6 hours.