\noindent \textbf{Agent Data Sources.}
To promote transparency, reproducibility, and potential generalization to novel domains in agent research, we publicly release the training recipe utilized for \dataset during the pre-training stage. To enhance the fundamental capabilities of \method, we compile a unique, comprehensive, and large-scale corpus of agent data sources, including API documentation, API function calling trajectories, code, and text data. Tables~\ref{tab:data-source1} and \ref{tab:data-source2} provide a comprehensive overview of \dataset used in \method, detailing the data sources, their respective sizes, and public availability status. 
All data sources utilized in \dataset are licensed under \texttt{Apache-2.0}, \texttt{MIT}, or \texttt{LGPL-2.1}, permitting non-commercial use and aligning with the research objectives of this work.
Examples of task formats in \dataset are available in Figure~\ref{fig:datasample}.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.98\linewidth]{figures/data_sample.pdf}
% \vspace{-2ex}
  \caption{
  Examples of different task formats in \dataset, including tool documentation, action trajectory (w/ environmental feedback), and code data. 
  }
% \vspace{-2ex}
  \label{fig:datasample}
\end{figure}

% \input{tables/tab-forge}

\noindent \textbf{Text and Code Data.}
Since agent data typically includes detailed task descriptions, formatted function calls, and environmental feedback, significant gaps exist between agent data and standard text and code data. Given that current open-sourced LLMs have already been pre-trained on text and code data, and to preserve their generalization ability, it is necessary to mix agent data with text and code data during the continual pre-training stage. For the text data, we primarily select a corpus that covers commonsense reasoning, mathematical reasoning, scientific reasoning, and general text.

\noindent$\bullet$ \textbf{RedPajama\_CommonCrawls}\footnote{\url{https://www.together.ai/blog/redpajama-data-v2}}~\cite{raffel2020exploring} is a large-scale web text dataset collected by the RedPajama project. It encompasses a diverse range of internet texts, including blogs, news articles, forum discussions, and social media posts. Incorporating this dataset helps to preserve general language understanding and generation capabilities, as it captures a wide variety of writing styles and topics, thus offering significant linguistic diversity.

\noindent$\bullet$ \textbf{Encyclopedic Content} is a comprehensive knowledge base sourced from Wikipedia\footnote{\url{https://www.wikipedia.org/}} and WikiQA~\cite{yang-etal-2015-wikiqa}. This dataset includes extensively curated articles covering a wide range of human knowledge domains. Incorporating encyclopedic content during continual pre-training helps ensure factual accuracy and reliability in the model's learned information.

\noindent$\bullet$ \textbf{Textbooks} from OpenStax\footnote{\url{https://openstax.org/}} provide peer-reviewed, openly licensed textbooks for higher education. These textbooks span topics such as mathematics, science, economics, and the humanities. Since textbooks are structured with well-organized chapters and summaries, continual pre-training on this corpus exposes the model to formal educational language and coherent knowledge representation.

\noindent$\bullet$ \textbf{Mathematical Content} from OpenWebMath~\cite{paster2024openwebmath} aggregates open-access mathematical texts, problem sets, and explanations. This dataset spans topics ranging from pure mathematics to applied fields, enabling the model to understand and generate mathematically rigorous content.

\noindent$\bullet$ \textbf{arXiv Papers}\footnote{\url{arxiv.org}} include preprints hosted on arXiv in fields such as physics, mathematics, computer science, and more. This dataset features advanced terminology, methodologies, and academic discourse. Using this data for continual pre-training enhances the model's ability to grasp complex scientific concepts and fosters cross-disciplinary understanding.

\noindent$\bullet$ \textbf{StarCoder-v2}~\cite{lozhkov2024starcoder} is a large-scale collection of source code curated to advance research in code generation and understanding. We select all documentation samples and randomly sample the remaining portion for inclusion in the \dataset. This dataset provides knowledge of complex programming patterns and semantics, which may benefit the tool-function-calling capabilities of LLMs.

