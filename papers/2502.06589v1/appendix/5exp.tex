\section{Additional Experimental Results and Analysis}
\label{app:exp}

\subsection{Evaluation of the fastText Filter}
To evaluate the precision of the fastText classifier in filtering general text from web retrieval data, we leverage \texttt{Claude-3-Sonnet} to annotate 20K samples. We then compare the predictions from the fastText filter against these annotated ground-truth labels. The evaluation results are presented in Table~\ref{tab:filter}.
The results indicate that the fastText filter achieves an accuracy of approximately 88\%, suggesting that the filtering outcomes are reliable and trustworthy. Moreover, the higher recall score indicates that the filtered data encompasses most agent-relevant information from the retrieval.
% However, we observed some limitations in the fastText approach. For instance, consider the following example of an advertisement on a radio channel, which should be categorized as general text:
% [Insert example text here]
% In this case, the fastText model incorrectly categorized the text as agent-relevant data. This misclassification likely occurred because fastText relies on gram frequency analysis, and the presence of multiple high-tech terms (e.g., iOS, App, Google Play) in the paragraph may have misled the model.

\input{tables/tab-filter}

\subsection{Evaluation of Base Models}
As base models often struggle to follow instructions to solve problems, existing works evaluate these models using few-shot prompting~\cite{wei2022chain, shao2024deepseekmath} or by assessing the negative log-likelihood of the final answer~\cite{dubey2024llama} (e.g., selecting the correct choice). However, these evaluation methods are not suitable for agent environments for the following reasons: (1) \textbf{Task Complexity.} Agent environment tasks are significantly more complex than multiple-choice questions, requiring the generation of long sequences of actions rather than selecting a single answer. (2) \textbf{Contextual Task Requirements.} Task requirements are often intricately embedded within the context, leaving insufficient space for few-shot exemplars.
To this end, we evaluate \method-Base on three agent benchmarks (Nexus~\cite{srinivasan2023nexusraven}, API-Bank~\cite{li2023api}, and API-Bench~\cite{patil2023gorilla}) and one general benchmark (MMLU)~\cite{hendrycks2020measuring}, reporting the benchmark loss in Figure~\ref{fig:val_loss}. 

% Evaluating base models on complex problem-solving tasks presents challenges, as these models often struggle to follow instructions directly. Previous studies have addressed this by employing few-shot prompting \cite{wei2022chain, shao2024deepseekmath} or by measuring the negative log-likelihood of the final answer \cite{dubey2024llama} (e.g., the correct choice in multiple-choice questions).
% However, these evaluation methods are inadequate for agent environments due to:

% Task complexity: Agent tasks require generating a long sequence of correct actions, rather than simply selecting a single choice.
% Context constraints: Well-crafted task requirements within the context often leave insufficient space for few-shot exemplars.

% To address these challenges, we evaluate \method-Base on three agent-specific benchmarks (API-Bank, API-Bench, NexusRaven) and one general benchmark (MMLU). Figure \ref{fig:val_loss} presents the benchmark loss results.


\subsection{Main Experimental Results on BFCL-v2}
\input{tables/tab-bfcl-v2}

Table~\ref{tab:bfcl-v2} displays detailed experimental results on BFCL-v2, covering AST and Execution, two aspects in evaluation of function calling capabilities.
Aside from the notations across the other tables, ``JS'' indicates ``JavaScript''; ``MF'', ``PF'', and ``PM'' refer to ``multiple functions'', ``parallel functions'', ``parallel multiple functions''.
The superior performance of \texttt{\method-3-8B} in AST evaluations indicates that the pre-training stage successfully introduced syntax knowledge of function calling into the model, which also contributes to improvements in the Execution aspect. However, the performance gain in Execution evaluations is less pronounced. This is because, lacking access to the instruction fine-tuning data used for \texttt{LLaMA-3-8B}, our \texttt{\method-8B-IFT} demonstrates limited instruction-following capabilities compared to \texttt{LLaMA-3-8B-Instruct} and \texttt{LLaMA-3.1-8B-Instruct}. Consequently, it is more challenging to follow instructions to generate executable functions.
% The superior performance in AST of \method-8B indicates that the pre-training stage successfully introduce syntax knowledge of function calling into the model, which also brings improvement in Execution aspect.
% However, the performance gain in Execution evaluations are less pronounced.
% This is because, as we have no access to the instruction fine-tuning data used on \texttt{LLaMA-3-8B}, our \method-8B-IFT shows limited instruction-following capabilities, compared with \texttt{LLaMA-3-8B-Instruct} and \texttt{LLaMA-3.1-8B-Instruct}.
% This leads to failure in following instructions to generate executable functions.

\subsection{Effect of Backbone LLMs}
\input{tables/tab-mistral}

Table~\ref{tab:mistral} reports the performance of \method and the baselines using \texttt{Mistral-7B-v0.3} as backbone LLM on AgentBench. 
% We compare both the base models and instruction-tuned models on AgentBench.
Notably, there exist consistent gains in terms of the average performance on both base model and instruction-tuned model ($1.06$ on base model and $0.4$ on IFT model), justifying the advantage of pre-training on \dataset across different LLM types and architectures.


