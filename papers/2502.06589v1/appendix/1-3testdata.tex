We conduct the main experiments of \method on three widely used LLM agent benchmarks across a wide range of scenarios, including: 

\noindent $\bullet$ \textbf{AgentBench}~\cite{liu2024agentbench} presents six distinct environments in a multi-turn, open-ended generation setting: Operating System (OS), Database (DB), Knowledge Graph (KG), House-Holding (HH), Web Shopping (WS), and Web Browsing (WB). We leverage AgentBench to evaluate intrinsic reasoning and adaptation to environmental feedback.

\noindent $\bullet$ \textbf{Berkeley Function Calling Leaderboard (BFCL)}~\cite{patil2023gorilla} provides a rigorous framework for assessing the function-calling proficiencies of diverse LLM agents. This benchmark encompasses 2,000 question-function-answer triads, spanning multiple programming paradigms (Python, Java, JavaScript, REST API) and heterogeneous application domains. The BFCL's evaluation protocol incorporates varying degrees of complexity, ranging from single-function selection tasks to scenarios necessitating the concurrent execution of multiple-function calls. Notably, the latest iteration, BFCL-v3, represents a significant methodological advancement over its predecessor by introducing a novel category that evaluates multi-turn and multi-step function invocation, more closely simulating real-world tool usage scenarios. We leverage BFCL-v2 and -v3 to evaluate the function-calling capability of LLM agents. 

Following~\citet{dubey2024llama}, we leverage additional three agent benchmarks (Nexus~\cite{srinivasan2023nexusraven}, API-Bank~\cite{li2023api}, and API-Bench~\cite{patil2023gorilla}) and one general benchmark (MMLU)~\cite{hendrycks2020measuring} for benchmark loss in the scaling law experiments.