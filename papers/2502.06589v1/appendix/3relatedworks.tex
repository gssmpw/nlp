\section{Additional Related Works}
\label{app:related works}
LLM-based intelligent agents and autonomous entities have demonstrated proficiency in tool utilization~\cite{qin2023toolllm,zhuang2024toolqa}, decision-making~\cite{wang2023voyager,li2024matryoshka}, and action execution through interactions with diverse environments~\cite{sun2024adaplanner,shi2024ehragent}. 

% \input{tables/tab-related-full}

\subsection{Black-box LLM Agents}
Existing methods for enhancing commercial closed-source LLM-based agents primarily focus on designing task-specific prompts. These prompts often incorporate tool function documentation \citep{hsieh2023tool}, few-shot demonstrations \citep{lu2024chameleon}, environmental feedback \citep{yao2022react, sun2024adaplanner, wang2023voyager}, and tree-like reasoning procedures \citep{yao2024tree, zhuang2023toolchain}. While these approaches have yielded improved results and increased flexibility, they come with significant drawbacks. The use of closed-source LLMs incurs substantial financial costs and raises safety concerns \citep{li2023multi,zhuang2024hydra, yuan2023gpt,sun2024bbox,shi2024medadapter}, limiting their wider deployment. Moreover, these prompting techniques do not fundamentally enhance the inherent agent abilities of the LLMs. Instead, they rely heavily on the function-calling capabilities of closed-source LLMs, which may lack stability across different updates or versions\footnote{\url{https://openai.com/index/function-calling-and-other-api-updates/}}.


\subsection{White-box LLM Agents}
Open-source LLMs have recently emerged as promising alternatives, demonstrating effectiveness in various applications \citep{touvron2023llama, jiang2024mixtral,tang-etal-2024-mimir}. While these models excel in natural language processing tasks, they still underperform when serving as the core of LLM agents \citep{zeng2023agenttuning, liu2024agentbench}. This limitation is primarily due to insufficient training samples and smaller model scales compared to their closed-source counterparts.
Researchers have attempted to address these shortcomings through various approaches. Some have fine-tuned LLMs with specific API documentation and function call sequences \citep{qin2023toolllm, gou2024tora}. Others have leveraged domain-specific data to learn tool embeddings or modify the decoding process \citep{schick2024toolformer, hao2024toolkengpt, zhang2023syntax}. However, this focus on specialized capabilities often comes at the expense of the LLMs' general abilities and compromises their generalizability.
A recent approach by \citet{chen2024agent} attempts to mitigate this issue by composing API function sequential data from diverse sources and reorganizing the training corpus. Yet, compared to the breadth of data included in the pre-training stage, the collected data from five to six different sources represents only a small fraction of real-world decision-making scenarios, limiting generalization to new tasks.
Moreover, the superficial alignment hypothesis \citep{zhou2024lima} suggests that a model's fundamental knowledge and capabilities are acquired almost entirely during pre-training. Post-training techniques merely guide the model in selecting which subdistribution of formats to use when interacting with users. Consequently, core abilities cannot be significantly improved through prompting and post-training techniques alone.


% When developing an LLM agent to address complex real-world challenges, existing approaches can be categorized into three distinct types: prompting-based methods, tuning-based methods, and pretraining-based methods.

% \subsection{Prompting-based LLM Agents} 
% The majority of existing LLM agents primarily focus on crafting prompts or frameworks for executing specific tasks. Beyond simply directing LLMs to generate complete plans, these prompting frameworks enhance the agents with tool documentation~\citep{hsieh2023tool}, intermediate reasoning steps~\citep{wei2022chain, lu2024chameleon}, environmental feedback~\citep{yao2022react, sun2024adaplanner, wang2023voyager}, and tree search algorithms~\citep{yao2024tree, zhuang2023toolchain}. 
% While prompt-based agents offer greater flexibility and can operate with large-scale function pools, these methods fail to fundamentally improve the inherent capabilities of the LLMs. Moreover, the prompting formats are task-specific and struggle to generalize to new, unseen tasks.
% Existing LLM agents primarily focus on crafting prompts or frameworks for specific task execution. 
% These approaches go beyond simply directing LLMs to generate complete plans; they enhance agent capabilities by incorporating tool documentation \citep{hsieh2023tool}, intermediate reasoning steps \citep{wei2022chain, lu2024chameleon}, environmental feedback \citep{yao2022react, sun2024adaplanner, wang2023voyager}, and tree search algorithms \citep{yao2024tree, zhuang2023toolchain}. 
% While prompt-based agents offer greater flexibility and can operate with extensive function pools, they fail to fundamentally improve the inherent capabilities of the LLMs themselves. Furthermore, the task-specific nature of prompting formats hinders their ability to generalize to new, unseen tasks.

\subsection{Finetuning-based LLM Agents} 
% Instruction tuning-based agents do not require an additional tool pool. The foundational model learns which tools it possesses and how to use them through instruction tuning. 
% Table~\ref{tab:related-full} summarizes existing instruction fine-tuning-based LLM agents with their training samples.
% For instance, Gorilla~\citep{patil2023gorilla} fine-tuned a LLaMA-based model using API documentation and demonstrations from Huggingface, TorchHub, and TensorFlowHub. 
% Toolformer~\citep{schick2024toolformer} introduces special tokens around API function calls to teach the model when and how to leverage tools during the fine-tuning stage. 
% ToolkenGPT~\citep{hao2024toolkengpt} incorporates tools as special tokens into the model's vocabulary, while ToolLLaMA~\citep{qin2023toolllm} builds datasets abundant with various tools. 
% However, these APIs and datasets are mostly from the same domain, and may only work on similar tasks. Thus, instruction tuning methods~\citep{achiam2023gpt, srinivasan2023nexusraven, zeng2023agenttuning, chen2024agent} are increasingly applied across a diverse range of API function call data and tasks, aiming to equip models with generalization capabilities across different planning tasks. Nevertheless, according to the superficial alignment hypothesis~\citep{zhou2024lima}, a model's knowledge and capabilities are predominantly acquired during pre-training. Post-training techniques such as instruction tuning and alignment primarily teach the model which sub-distributions of formats should be utilized when interacting with users.

% Instruction tuning-based agents differ from prompt-based approaches in that they do not require an additional tool pool. Instead, the foundational model learns about its available tools and their usage through the instruction tuning process. 
Table \ref{tab:related} summarizes existing instruction fine-tuning-based LLM agents and their training samples.
% Various approaches have been developed in this domain. 
For example, Gorilla \citep{patil2023gorilla} fine-tuned a LLaMA-based model using API documentation and demonstrations from Huggingface, TorchHub, and TensorFlowHub. Toolformer \citep{schick2024toolformer} introduced special tokens around API function calls to teach the model when and how to leverage tools during fine-tuning. ToolkenGPT \citep{hao2024toolkengpt} incorporated tools as special tokens into the model's vocabulary, while ToolLLaMA \citep{qin2023toolllm} built datasets rich in various tools.
However, these methods often rely on APIs and datasets from similar domains, potentially limiting their effectiveness to tasks within those domains. To address this limitation, recent instruction tuning methods \citep{achiam2023gpt, srinivasan2023nexusraven, zeng2023agenttuning, chen2024agent} have expanded to include a diverse range of API function call data and tasks, aiming to equip models with broader generalization capabilities across different planning tasks.
Nevertheless, the superficial alignment hypothesis \citep{zhou2024lima} suggests that a model's fundamental knowledge and capabilities are predominantly acquired during pre-training. According to this hypothesis, post-training techniques such as instruction tuning and alignment primarily teach the model which sub-distributions of formats to utilize when interacting with users, rather than fundamentally expanding its capabilities.
Moreover, heavy fine-tuning prevents generalization and degrades performance in general use cases, potentially suppressing the original base model capabilities~\cite{ghosh2024a}.

\subsection{Pretraining-based LLM Agents} 
% To overcome the limitations of prompting and tuning-based methods, recent initiatives have focused on pre-training or continual pre-training of language models to bolster their fundamental capabilities. 
% CodeGen~\citep{nijkamp2023codegen} and CodeLLaMA~\citep{roziere2023code} are notable examples that enhance the coding skills of LLMs. 
% Following the code LLMs' success, LEMUR~\citep{xu2024lemur} further instruction tunes a code LLM with additional assistant and tool-related data. 
% Pandora~\citep{xiang2024pandora} represents a pre-trained world model incorporating visual encoders to process a wide array of multi-modal data, including videos and textual actions. The most closely related work to our proposed model is OpenFunctions-v2~\citep{patil2023gorilla}, which is pre-trained on a vast collection of data sources: 19,353 Python packages, 16,586 Java repositories, 4,285 JavaScript repositories, 6,009 public APIs, and 19,090 command line tools. However, this model primarily focuses on making correct API function calls and lacks emphasis on the intrinsic reasoning abilities required for managing multiple API function calls. 

To overcome the limitations of prompting and tuning-based methods, recent initiatives have focused on pre-training or continual pre-training of language models to bolster their fundamental capabilities. Several notable examples have emerged in this domain:
CodeGen \citep{nijkamp2023codegen} and CodeLLaMA \citep{roziere2023code} enhance the coding skills of LLMs. Building on the success of these code LLMs, LEMUR \citep{xu2024lemur} further instruction tunes a code LLM with additional assistant and tool-related data.
Pandora \citep{xiang2024pandora} represents a pre-trained world model that incorporates visual encoders to process a wide array of multi-modal data, including videos and textual actions.
The most closely related work to our proposed model is OpenFunctions-v2 \citep{patil2023gorilla}. This model is pre-trained on a vast collection of data sources, including 19,353 Python packages, 16,586 Java repositories, 4,285 JavaScript repositories, 6,009 public APIs, and 19,090 command line tools. However, while OpenFunctions-v2 primarily focuses on making correct API function calls, it lacks emphasis on the intrinsic reasoning abilities required for managing multiple API function calls, as well as adapting to environmental feedback.