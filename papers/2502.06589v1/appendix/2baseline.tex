\section{Baseline Details}
\label{app:baseline}

\subsection{Base LLMs}

\noindent $\bullet$ \textbf{\texttt{LLaMA-3-8B-Base}}~\cite{dubey2024llama} is a small-scale flagship model in Meta's LLaMA-3 series, featuring 8 billion parameters. We compare \method with \texttt{LLaMA-3-8B-Base}, which also serves as the backbone of \texttt{\method-8B-Base}, to demonstrate the effectiveness of continual pre-training.

\noindent $\bullet$ \textbf{\texttt{LLaMA-3.1-8B-Base}}~\cite{dubey2024llama} is an improved version of \texttt{LLaMA-3-8B}, offering more efficient parameter utilization and enhanced fine-tuning capabilities. The 3.1 series models are optimized for multilingual support and scalability, allowing for a longer context length of up to 128K tokens. We select \texttt{LLaMA-3.1-8B-Base} as the current state-of-the-art small-scale open-sourced base model for comparison.

\subsection{Open-Source Instruction Fine-tuned LLMs}
We compare \texttt{\method-IFT} with the following open-sourced instruction-tuned LLMs:

\noindent$\bullet$ \textbf{\texttt{LLaMA-2-Chat}}~\cite{touvron2023llama} is a series of large language models developed by Meta, designed for conversational AI. The models support text-based interactions and come in varying parameter sizes, such as 7B, 13B, and 70B. For comparison, we select models of comparable scale, specifically \texttt{LLaMA-2-7B-Chat} and \texttt{LLaMA-2-70B-Chat}.

\noindent$\bullet$ \textbf{\texttt{Vicuna-v1.5}}~\cite{chiang2023vicuna} is a collection of open-source LLMs fine-tuned from LLaMA models, optimized for high-quality conversational abilities. These models are fine-tuned using datasets derived from user-shared conversations and are available in sizes such as 7B and 13B parameters, both of which are included in our comparisons.

\noindent$\bullet$ \textbf{\texttt{CodeLLaMA}}~\cite{roziere2023code} is a specialized extension of the LLaMA family designed for code generation and understanding. Built upon LLaMA-2, CodeLLaMA introduces enhancements tailored to coding tasks. We evaluate multiple sizes, including \texttt{CodeLLaMA-7B-Instruct}, \texttt{CodeLLaMA-13B-Instruct}, and \texttt{CodeLLaMA-34B-Instruct}.

\noindent$\bullet$ \textbf{\texttt{Groq-8B-Tool-Use}}~\cite{groq} is a specialized variant of LLaMA-3-8B, fine-tuned by Groq for advanced tool use and function-calling tasks. It leverages post-training techniques to achieve state-of-the-art performance in function-calling tasks, including BFCL.

\noindent$\bullet$ \textbf{\texttt{LLaMA-3-Instruct}}~\cite{dubey2024llama} belongs to Meta's LLaMA-3 family, optimized for instruction-following tasks. These models excel at tasks requiring explicit instructions, making them suitable for applications such as chatbots, virtual assistants, and task-specific text generation. We compare \texttt{LLaMA-3-8B-Instruct} and \texttt{LLaMA-3.1-8B-Instruct} as small-scale state-of-the-art instruction-tuned models. Additionally, we use \texttt{LLaMA-3-70B-Instruct} as a reference model for comparison.

\noindent$\bullet$ \textbf{\texttt{DeepSeek-v2}}~\cite{liu2024deepseek} and \textbf{\texttt{Mixtral-8x22B}}~\cite{jiang2024mixtral} are both cutting-edge language models utilizing Mixture-of-Experts (MoE) architectures to optimize efficiency and performance across various domains. We include both models as reference points in our comparisons.

\subsection{API-based Commercial LLMs (for reference)}
We also consider {API-based commercial LLMs} for reference only, including \textbf{\texttt{Gemini-1.5-Flash}}~\cite{reid2024gemini}, \textbf{\texttt{text-davinci-003}}~\cite{ouyang2022training}, \textbf{\texttt{gpt-3.5-turbo-0125}}~\cite{chatgpt}, \textbf{\texttt{gpt-4-0613}}~\cite{achiam2023gpt}, \textbf{\texttt{Claude-3-Haiku}}~\cite{claude-3}, and \textbf{\texttt{Command-R-Plus-FC}}~\cite{command-r-plus}.
We exclude prompting and instruction fine-tuned agent frameworks from our main experiments to focus on evaluating the fundamental agentic capabilities of LLMs.

% Gemini-1.5-Flash~\cite{reid2024gemini}

% text-davinci-003~\cite{ouyang2022training}

% gpt-3.5-turbo-0125~\cite{chatgpt} 

% gpt-4-0613~\cite{achiam2023gpt}

% Claude-3-Haiku~\cite{claude-3}

% Command-R-Plus-FC~\cite{command-r-plus}






