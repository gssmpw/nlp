\section{Preliminaries}
\label{sec:preliminary}

\begin{figure*} [t]
	\centering
   \vspace{-2ex}
	\subfigure[\dataset]{
		\includegraphics[width=0.225\linewidth]{figures/crop_data_ratio.pdf}
		\label{fig:data-overall}
	} 
     \subfigure[Tool Data]{
		\includegraphics[width=0.225\linewidth]{figures/crop_tool_ratio.pdf}
		\label{fig:data-tool}
	}
    \subfigure[Retrieved Data]{
		\includegraphics[width=0.225\linewidth]{figures/crop_retr_ratio.pdf}
		\label{fig:data-retr}
	}
    \subfigure[t-SNE: Retrieved Data]{
		\includegraphics[width=0.225\linewidth]{figures/tsne-new-v3.pdf}
		\label{fig:tsne-retr}
	}
    % \vspace{-1ex}
	\caption{Data composition of (a) the entire \dataset, (b) seed data collection (\cref{sec:data-phase1}), and (c) retrieved agent data from the open web (\cref{sec:data-phase2}). A t-SNE visualization (d) depicts seed data (\textbf{colorful} points, with each color representing different data sources), retrieved data (\textbf{black}), and general text (\textcolor{gray}{\textbf{gray}}) within the semantic space, where retrieved data is closer to the selected seed data than to the general text. Detailed data sources are in \cref{app:data-pretrain}.
 }
\vspace{-2ex}
\label{fig:data}
\end{figure*}


\noindent \textbf{Problem Formulation.} We conceptualize leveraging LLMs as autonomous agents for problem-solving as a planning process.
Initially, we augment the LLM agent with access to a pool of candidate API functions, denoted as $\mathcal{A}=\{\text{API}_0,\text{API}_1,\cdots,\text{API}_m$\}, along with a natural language task description $g\in\mathcal{G}$ from the task space $\mathcal{G}$. 
The objective of the LLM agent is to translate the task description $g$ into an ordered sequence of $T_g$ API function calls $p_g=\{a_0,\cdots,a_{T_g}\}$.
Specifically, considering the task description $g$ as the initial state $s_0$, we then sample the plan $p_g$ by prompting the LLM agent with the API definitions $\mathcal{I}$ and demonstration samples $\mathcal{D}$ as follows: $p_g\sim\rho(a_0,a_1,\cdots,a_{T_g}|s_0;\mathcal{I},\mathcal{D}):\mathcal{G}\times\mathcal{I}\times\mathcal{D}\to\Delta(\mathcal{A}^{T_g})$, where $\Delta(\cdot)$ denotes a probability simplex function. 
The final output is derived after executing the entire plan $y\sim\pi(y|s_0,a_1,a_2,\cdots,a_{T_g})$, where $\pi(\cdot)$ denotes a plan executor.

During this procedure, we focus on three fundamental capabilities of LLM agents:

\noindent \textbf{Accurate Function Calling.} It involves accurately understanding the API definitions and demonstration samples to generate correct API function calls with corresponding parameters in a given scenario.
Specifically, the model should accurately understand the API definitions $\mathcal{I}$ and demonstration samples $\mathcal{D}$, as well as generate an accurate API function call in the given scenario $p(a_t|s_0,a_1,\cdots,a_{t-1},\mathcal{I},\mathcal{D})$, where $a_t$ is the ground-truth API function call with corresponding parameters at $t$-th step.

\noindent \textbf{Intrinsic Reasoning and Planning.} It refers to the intrinsic reasoning and planning ability to devise a sequence of multiple tool functions as a solution when addressing complex (multi-step) real-world problems. In such cases, LLMs are often required to generate a sequence of API function calls, $p(a_1,a_2,\cdots,a_{T_g}|s_0;\mathcal{I},\mathcal{D})$, where $\{a_1,a_2,\cdots,a_{T_g}\}$ constitutes the ground-truth solution plan of length $T_g$.  
This process relies on intrinsic reasoning embedded within the model parameters; enhanced reasoning capabilities lead to a solution plan with a higher chance of success.


\noindent \textbf{Adaptation with Environment Feedback.} It focuses on adapting the current plan or action based on environmental feedback when the environments support interaction with the LLM agent. When such feedback is available, it is crucial for the agent to adjust its actions accordingly: $p(a_t|s_0,a_1,o_1,a_2,\cdots,o_{t-1};\mathcal{I},\mathcal{D})$,
where $o_{k}$ represents the feedback from the environment after the $k$-th action. 
Incorporating environmental feedback allows the agent to take reflections to refine its plan and improve task performance iteratively.

