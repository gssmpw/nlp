\section{Experiments}
\label{sec:exp}

\begin{figure*}[t]
  \centering
  \vspace{-2ex}
  \includegraphics[width=\linewidth]{figures/train_loss-big.pdf}
  % \vspace{-2ex}
  \caption{Training and benchmark loss. (a) Training loss of \method during continual pre-training and instruction fine-tuning. (b) Benchmark loss at periodic training checkpoints and (c) a comparison across base models.
  % The entire training procedure is composed of three stages: (1) Pre-Training Stage 1 (PT-1), training on the data that is between the distribution of general data and agent data; (2) Pre-Training Stage 2 (PT-2), training on the data that is closer to IFT data and downstream applications; (3) Instruction Fine-Tuning (IFT), aligning the model performance to desired applications. 
  }
  \vspace{-1ex}
  \label{fig:val_loss}
\end{figure*}


% \begin{figure} [t]
% 	\centering
%   % \vspace{-1.5ex}
% 	\subfigure[Evaluation curve]{
% 		\includegraphics[width=0.45\linewidth]{figures/val_loss.pdf}
% 		\label{fig:val_loss-base}
% 	} 
%      \subfigure[Comparison]{
% 		\includegraphics[width=0.45\linewidth]{figures/base_benchmark_loss.pdf}
% 		\label{fig:val_loss-compare}
% 	}
% 	\caption{Benchmark loss of \method-8B-Base model.\ycz{todo caption}}
%  % \vspace{-1.5ex}
% \label{fig:val_loss}
% \end{figure}

\subsection{Experiment Setup}
\noindent \textbf{Tasks and Datasets.} 
We mainly evaluate our \method on the following benchmarks:
(1) \emph{AgentBench}~\cite{liu2024agentbench} for intrinsic reasoning and adaptation to environment feedback;
(2) \emph{Berkeley Function Calling Leaderboard (BFCL)-v3} and (3) \emph{BFCL-v2}~\cite{patil2023gorilla} for accurate function calling.
To test generalizability instead of memorization, we intentionally exclude all evaluation benchmarks from pre-training corpora.
Task and dataset details are available in \cref{app:data}.

\noindent \textbf{Baselines.}
We mainly compare to the following baselines: (1) \emph{Base LLMs} and 
% including LLaMA-3 and LLaMA-3.1~\cite{dubey2024llama}.
(2) \emph{Open-Source Instruction Fine-Tuned LLMs} with varying model sizes. 
% including LLaMA-2-Chat~\cite{touvron2023llama} series, Vicuna-v1.5~\cite{chiang2023vicuna} series, CodeLLaMA-Instruct~\cite{roziere2023code} series, Groq-8B-Tool-Use~\cite{groq}, LLaMA-3-Instruct~\cite{dubey2024llama} series, DeepSeek-v2~\cite{liu2024deepseek}, and Mixtral-8x22B~\cite{jiang2024mixtral}.
We also show the performance of (3) \emph{API-based Commercial LLMs} as reference. 
% including Gemini-1.5-Flash~\cite{reid2024gemini}, text-davinci-003~\cite{ouyang2022training}, gpt-3.5-turbo-0125~\cite{chatgpt}, gpt-4-0613~\cite{achiam2023gpt}, Claude-3-Haiku~\cite{claude-3}, and Command-R-Plus-FC~\cite{command-r-plus}.
We exclude prompting and instruction fine-tuned agent frameworks from our main experiments to focus on evaluating the fundamental agentic capabilities of LLMs.
Details of baseline models are in \cref{app:baseline}.

\noindent \textbf{Evaluation.}
Following~\citet{liu2024agentbench,patil2023gorilla},
for AgentBench, we report \emph{success rate} for the OS, DB, HH, and WB environments, \emph{F1 score} for the KG environment, and \emph{reward score} for the WS environment; for BFCL-v2 and -v3, we use \emph{accuracy} as the primary metric for all scenarios to assess correct function calls.
% \noindent \textbf{Implementation Details.}
Implementation details can be found in \cref{app:implementation}.

\input{tables/tab-base}

\subsection{Main Experiments: \method-8B-Base}
Following~\citet{shao2024deepseekmath,dubey2024llama}, we evaluate our two-stage pre-trained \texttt{\method-8B-Base} on three agent-specific benchmarks (API-Bank, API-Bench, NexusRaven) and one general benchmark (MMLU).
We observe that incorporating more agent data during pre-training consistently reduces benchmark loss on agent tasks in Figure \ref{fig:val_loss} (b). Additionally, Figure \ref{fig:val_loss} (c) demonstrates that \texttt{\method-8B-Base} achieves significantly lower benchmark loss compared to the \texttt{LLaMA-3-8B} series of base models.
Furthermore, Table \ref{tab:agentbench} reports the benchmark scores, where \texttt{\method-8B-Base} leads in performance across all benchmarks among the open-source base models. 
Our findings indicate that both pre-training stages (I \& II) enhance \method's fundamental capabilities across a wide range of agent tasks without compromising general capabilities.

\subsection{Main Experiments: \method-8B-IFT}
Table \ref{tab:agentbench} presents the main experimental results of instruction fine-tuned \method and baselines. 
% The key findings are summarized as follows:
% \noindent \textbf{Superior Performance Against Larger and Commercial Models.} 
\method consistently outperforms small to medium size open-source LLMs. Moreover, \texttt{\method-8B-IFT} remains competitive compared to baseline models with significantly more parameters or commercial LLMs. 
% For instance, \texttt{\method-8B-IFT} surpasses \texttt{Mixtral-8x22B}~\cite{jiang2024mixtral} ($22\times$parameters) and \texttt{Claude-3-Haiku}~\cite{claude-3}, while achieving performance comparable to \texttt{GPT-3.5-Turbo-0125}~\cite{chatgpt} and \texttt{LLaMA-3-70B-Instruct}~\cite{dubey2024llama} ($8\times$parameters).

\noindent \textbf{Enhanced Capabilities Through Pre-training.}
We conduct a direct comparison between \method and \texttt{LLaMA-3-8B-Base}~\cite{dubey2024llama}, both instruction-tuned using the same instruction fine-tuning data. \texttt{\method-8B-IFT} outperforms \texttt{LLaMA-3-8B-IFT} across all three benchmarks, indicating that the observed improvements can be attributed to the pre-training stage. 
Moreover, incorporating more domain-specific knowledge during the pre-training stage leads to better performance, without requiring additional instruction fine-tuning data.

\noindent \textbf{Excelling in Complex Multi-turn Tasks.}
BFCL-v3, the latest benchmark, emphasizes multi-turn tool function-calling tasks requiring intrinsic reasoning capabilities and function-calling proficiency. 
Due to its recent introduction, the limited availability of task-specific data for instruction-tuning has led to suboptimal performance, particularly in multi-turn function-calling accuracy, as observed with models like Groq-8B-Tool-Use \cite{groq}. 
In contrast, \method exhibits significantly better performance on BFCL-v3, suggesting that its improvements in core agentic capabilities and generalization stem from pre-training on our large-scale, diverse agent-oriented corpus.


\input{tables/tab-ablation}

\subsection{Ablation Studies}
Table~\ref{tab:ablation} presents the ablation results of \method on AgentBench and BFCL-v2. 
% We investigate the effects of different training stages and the inclusion of retrieved agent data.

\noindent\textbf{Effect of Pre-Training Stages.}
Removing the second pre-training stage results in a slight performance decline for both base and instruction-tuned models across all tasks. Although the Stage-I pre-training data, comprising a large volume of general and retrieved agent data from the web, brings the \dataset closer to the general data distribution, it still differs from the data used in downstream applications and evaluations. The Stage-II pre-training is essential for effectively bridging the gap between the pre-training corpus and the instruction fine-tuning data, thereby enhancing overall model performance.

\noindent\textbf{Effect of Retrieved Data.}
Degrading the retrieved data to unfiltered, low-quality data or removing it entirely negatively impacts overall performance.
For tasks with numerous hand-crafted instructions and simulated trajectories available on the open web (\eg, HH and WS), the seed data of \dataset can lead to model overfitting on specific patterns.
When the large volume of retrieval data is removed, the seed data predominates, leading to improved performance on these specific tasks but reduced performance on others.


\input{tables/tab-generalization}
\input{tables/tab-general-domain}

\subsection{Cross-Task Generalization}
Table~\ref{tab:generalization} compares \method with several instruction fine-tuned agent frameworks across three agent benchmarks for cross-task generalization.
% \noindent\textbf{Task-Specific Performance vs. Generalization.} 
% While many existing approaches focus on leveraging instruction fine-tuning to enhance model performance in function calling and agentic reasoning, the task-specific data used in the instruction fine-tuning stage often constrains model behavior to desired patterns rather than fundamentally improving capabilities across various agent tasks.
While models fine-tuned on task-specific data excel in corresponding tasks~\cite{groq,zeng2023agenttuning,liu2024toolace}, they struggle to generalize across different agent benchmarks.
% In contrast, \method demonstrates impressive performance across all tasks, indicating that the large volume and diversity of \dataset broadly enhances the model's capabilities in function calling and agentic reasoning, enabling better generalization across varied agent tasks.
In contrast, \method performs consistently well across all tasks, suggesting that the large and diverse pre-training corpora, \dataset, effectively enhance function calling and agentic reasoning, leading to better generalization. 
% \noindent\textbf{Pre-training vs. Instruction Fine-tuning Impact.}
Furthermore, the compared methods are based on continued instruction fine-tuning of \texttt{LLaMA-3-8B-Instruct}, which inherently possesses strong instruction-following and understanding capabilities due to its meticulously curated post-training data. 
Unlike models relying solely on instruction fine-tuning, the pre-training of \method effectively improves its fundamental capabilities, thereby offering a more robust foundation for diverse agentic applications.


\subsection{Preservation of General Capabilities}
To evaluate the preservation of general capabilities, we further conduct comprehensive experiments across seven additional benchmarks (Table~\ref{tab:preserve}) besides MMLU, spanning mathematics~\cite{cobbe2021training}, software development~\cite{chen2021evaluating,liu2024your}, logical reasoning~\cite{suzgun2022challenging}, and broad language model abilities~\cite{zhou2023instruction,zellers2019hellaswag,hendrycks2020measuring}. Our results demonstrate that \method maintains comparable performance to the base model across these diverse domains while significantly enhancing agent-specific capabilities.

