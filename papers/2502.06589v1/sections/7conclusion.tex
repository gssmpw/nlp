\section{Conclusion}
\label{sec:conclusion}

In summary, \dataset and \method collectively advance open-source LLM-based autonomous agents by addressing critical gaps in pre-training corpora. 
Through exhaustive scaling law experiments, we identify an empirically optimal data mix ratio of approximately 1:1:1 for agent, code, and text data, maximizing the fundamental and generalization capabilities of LLM agents. 
Empirical evaluations underscore the efficacy and validity of \dataset in fostering enhanced fundamental agentic capabilities and superior generalization in LLM-based autonomous agents.



% This is probably the first work discussing the agent capability in pretraining? for these we study the recipe, scaling law, and train the models to verify the effectiveness
% insights
% I think it should be agent pre-training recipe not a single checkpoint. The recipe includes data and algorithm. There is no much to say about the algorithm right? So I feel the data is more important, all other points are like supporting the effectiveness and validness of the data. Sources -> how we get the data originally. Scaling law -> how we get the mixing ratios. Experiments and model checkpoints -> why the data is effective.


% In summary, \dataset and \method collectively advance the state-of-the-art in LLM-based autonomous agents by addressing critical gaps in pre-training data and training methodologies. This advancement paves the way for more adaptable, intelligent, and robust autonomous agents capable of performing effectively across diverse and dynamic real-world environments.

\section*{Limitations}
\noindent \textbf{Data Composition.} 
While knowledge of the composition of pre-training or instruction fine-tuning data would further enhance the effectiveness of \method, most prominent open-source LLMs (\eg, \texttt{LLaMA-3-8B-Instruct}) do not disclose detailed data information. Nevertheless, our continual pre-training experiments with \texttt{LLaMA-3-8B} demonstrate that significant improvements are achievable even without this knowledge. 

\noindent \textbf{Model Scalability.} Computational constraints currently restrict our ability to extend these experiments to larger models. In future work, we aim to validate our findings and methodologies on more expansive LLM architectures, pending access to increased computational resources.

% \noindent \textbf{Training Objective and Agent Frameworks.} The primary focus of this work is on building an effective dataset creation recipe to scale up retrieval models for the biomedical domain.
% However, other biomedical LLMs can also be used as the backbones~\citep{luo2022biogpt,bolton2024biomedlm,chen2023meditron}. 
% In our early attempts, we did not observe significant performance gains with some of these backbones (e.g., BioGPT-Large, BioMedLM-2.7B, Meditron-7B), potentially due to the loose connection between the objective of sequence contrastive learning and causal language modeling. 
% Besides, since we utilize standard contrastive learning objectives with hard negative mining without proposing new learning objectives, we \emph{do not claim} this as our core contribution. 
% Exploring ways to leverage domain-specific LLMs with improved training techniques to enhance performance is an interesting topic for future research.
% How to further leverage domain-specific LLMs with improved training techniques to bolster the performance is an interesting topic. 

\section*{Ethical Statement}
\noindent \textbf{Data Contamination.} 
A potential concern in our evaluations is test set contamination, which occurs when some task-specific examples overlap with data used during continual pre-training~\cite{oren2024proving}. To mitigate this issue, we follow~\citet{wang-etal-2024-improving-text} and conduct a string-matching analysis, which indicates no overlap between our training data and the datasets of the target tasks. Moreover, we intentionally exclude all evaluation benchmark data from both our pre-training and fine-tuning datasets to ensure a fair comparison.

\noindent \textbf{Reproducibility.}
To promote transparency, reproducibility, and generalizability in our research, we include all details of the dataset construction (\eg, data collection, processing, retrieving, filtering, scaling law, \etc) of \dataset in \cref{sec:dataset} and the training procedures for \method in \cref{sec:method}. 
Experimental setups and results are presented in \cref {sec:exp}.
Additionally, we detail the pre-training, instruction fine-tuning, and testing tasks and datasets in \cref{app:data-pretrain,app:data-ift,app:data}, respectively. 
% All code, data, and models will be made publicly available on GitHub and HuggingFace upon acceptance.
