\section{\method}
\label{sec:method}

In this section, we propose \method, a foundation model with enhanced fundamental capabilities of LLM agents.
% in enhancing the fundamental agentic capabilities and generalization of LLMs.
\method undergoes a two-stage continual pre-training process, followed by instruction fine-tuning (see Figure~\ref{fig:overview}):
% to enhance the fundamental agentic capabilities of LLMs for function calling, intrinsic reasoning and planning, and adaptation to environment feedback.
% \method is composed of three main stages (see Figure~\ref{fig:overview}): 
(1) \textbf{Stage I}, continual pre-training stage on the entire \dataset corpus to inject general agent knowledge (\cref{sec:method-pre});
(2) \textbf{Stage II}, continual pre-training stage on the high-quality seed set of \dataset to further enhance specific capabilities (\cref{sec:method-pre}); and
(3) \textbf{Stage III}, instruction fine-tuning to follow general instructions and downstream task requirements (\cref{sec:method-ift}).


\subsection{Stage I \& II: Continual Pre-Training}
\label{sec:method-pre}
Following~\citet{caccia2022new, lange2023continual}, we revisit the concept of \emph{stability gap}, which describes the phenomenon where the performance on old tasks initially drops and then recovers when learning a new task.
Specifically, in the continual pre-training of LLMs, if the data distribution shifts too significantly between the initial pre-training and the continual pre-training stages, the model's capabilities can deteriorate markedly until it assimilates knowledge from the new data distribution~\cite{guo2024efficient}.
To this end, we propose a two-stage continual pre-training framework:

\noindent \textbf{Stage I: Injecting General Agent Knowledge.}  
Stage I infuses general agent knowledge, accompanied by commonsense knowledge and code snippets. We pre-train \method on the entire \dataset, whose data distribution is carefully balanced between general corpus and agent-specific data, facilitating a smooth and gradual integration of agent knowledge.
% without causing abrupt shifts that could destabilize the model's performance.

\noindent \textbf{Stage II: Enhancing Agent-Specific Capabilities.}
Stage II leverages high-quality agent data to further enhance the specific capabilities of an agent LLM, including user interaction, function calling, planning, plan refinement, and coding capabilities. We continually pre-train the model obtained from Stage I on the high-quality seed data in \cref{sec:data-phase1} to further align the behavior with agent-specific requirements, ensuring that the specialized functionalities are robustly learned and integrated.

\noindent \textbf{Pre-Training Objectives.} For both stages, we employ language modeling as the primary pre-training task. 
% Language modeling is a foundational technique in natural language processing that involves predicting the next token in a sequence based on its preceding tokens. This task is especially critical for pretraining decoder-only models in LLMs. 
The objective is to auto-regressively predict the next token, defined as follows:
\begin{equation*}
    \begin{aligned}
        \mathcal{L}_{\text{PT}}=-\mathbb{E}_{\mathbf{x}\in\mathcal{D}_{\text{PT}}}\sum_{i=1}^np(x_i|\mathbf{x}_{<i}),
    \end{aligned}
\end{equation*}
where $\mathcal{D}_{PT}$ denotes the pre-training data, and $x_i$ represents the $i$-th token in the training sample $\mathbf{x}$.


\subsection{Stage III: Instruction Fine-Tuning}
\label{sec:method-ift}
To further improve its instruction-following capabilities to align with complex agent environments, \method undergoes instruction fine-tuning on a blend of high-quality instruction-completion datasets, including \emph{ShareGPT}~\cite{chiang2023vicuna}, \emph{ToolACE}~\cite{liu2024toolace}, and \emph{AgentFlan}~\cite{chen2024agent}. 
% See details in \cref{app:data-ift}.
% (1) a general conversation dataset, \emph{ShareGPT}~\cite{chiang2023vicuna}; (2) a single-tool function-calling conversation dataset, \emph{ToolACE}~\cite{liu2024toolace}; and (3) a multi-turn planning conversation dataset, \emph{AgentFlan}~\cite{chen2024agent}.
The Stage III employs a negative log-likelihood loss function, defined as:
% Agent environments are inherently complex, often encoding numerous requirements within their contexts. Consequently, strong instruction understanding and following capabilities are essential for an effective agent LLM. It is observed that general instruction-tuning or supervised fine-tuning often significantly improves such capabilities~\cite{wei2022finetuned, ouyang2022training}.
% Thus, we leverage instruction fine-tuning on a blend of high-quality instruction-completion datasets: (1) a general conversation dataset, \emph{ShareGPT}~\cite{chiang2023vicuna}; (2) a single-tool function-calling conversation dataset, \emph{ToolACE}~\cite{liu2024toolace}; and (3) a multi-turn planning conversation dataset, \emph{AgentFlan}~\cite{chen2024agent}.
% The pre-trained model undergoes instruction tuning using a set of task-specific data that aligns with the distribution of the evaluation data, with a negative log-likelihood objective as follows:
\begin{equation*}
    \begin{aligned}
        \mathcal{L}_{\text{IFT}}=-\mathbb{E}_{(\mathbf{x},\mathbf{y})\in\mathcal{D}_{\text{IFT}}}\sum_{i=1}^np(y_i|\mathbf{y}_{<i},\mathbf{x}),
    \end{aligned}
\end{equation*}
where $\mathbf{x}$ represents the given instruction, and $\mathbf{y}$ is the expected solution to fill. Here, $(\mathbf{x},\mathbf{y})\in\mathcal{D}_{\text{IFT}}$ indicates that the data pairs are sampled from the instruction-tuning dataset. 
% The prompting after instruction-tuning remains the same as direct prompting. 