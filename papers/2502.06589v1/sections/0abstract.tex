\begin{abstract}

Due to the scarcity of agent-oriented pre-training data, LLM-based autonomous agents typically rely on complex prompting or extensive fine-tuning, which often fails to introduce new capabilities while preserving strong generalizability.
% LLM-based autonomous agents, constrained by the absence of agent-specific pre-training data, often depend on sophisticated prompting or extensive fine-tuning, failing to introduce new capabilities while preserving strong generalization.
% LLM-based autonomous agents typically rely on sophisticated prompting or extensive fine-tuning for domain adaptation and preference alignment.
% However, prompting fails to introduce new knowledge and capabilities, while heavy fine-tuning can hinder generalization and degrade performance in non-agent use cases, potentially suppressing the original base model capabilities.
% often failing to enhance their inherent knowledge and fundamental capabilities.
We introduce \dataset, the first large-scale pre-training corpus designed to enhance the fundamental capabilities of LLM agents in API function calling, intrinsic reasoning and planning, and adapting to environmental feedback. 
\dataset comprises 103B agent-specific data encompassing 76,537 APIs, including both tool documentation to introduce knowledge of API functions and function calling trajectories to strengthen intrinsic reasoning.
To explore effective training protocols, we investigate scaling laws to identify the optimal recipe in data mixing ratios. 
By continual pre-training on \dataset, \method outperforms small- to medium-scale open-source LLMs and rivals commercial LLMs on three agent benchmarks, demonstrating the effectiveness of our pre-training corpus in enhancing fundamental agentic capabilities and generalization of LLMs to new tasks or environments.

% Moreover, we propose the \method, a foundational model undergoing two-stage continual pre-trained on \dataset to support the effectiveness and validness of the data.

% This is probably the first work discussing the agent capability in pretraining? for these we study the recipe, scaling law, and train the models to verify the effectiveness
% insights
% I think it should be agent pre-training recipe not a single checkpoint. The recipe includes data and algorithm. There is no much to say about the algorithm right? So I feel the data is more important, all other points are like supporting the effectiveness and validness of the data. Sources -> how we get the data originally. Scaling law -> how we get the mixing ratios. Experiments and model checkpoints -> why the data is effective.

\end{abstract}