% I wish you the best of success.

\section{Introduction}
\label{sec:intro}

\begin{figure}[t]
  \centering
  % \vspace{-2ex}
  \includegraphics[width=\linewidth]{figures/crop_teaser_related.pdf}
  \caption{Training paradigms of LLM agents.
  \emph{Prompting} alone fails to introduce new knowledge and capabilities, while heavy \emph{fine-tuning} can hinder generalization and degrade performance in non-agent use cases, potentially suppressing the original base model capabilities.
  }
  \vspace{-2ex}
  \label{fig:related}
\end{figure} 

Large language models (LLMs) are rapidly evolving beyond traditional natural language processing tasks~\cite{ouyang2022training, brown2020language, achiam2023gpt}, demonstrating increasing intelligence and autonomy by exhibiting capabilities in perception, reasoning, planning, and action within complex real-world environments~\cite{yao2022react,lu2024chameleon,sun2024adaplanner}.
Through well-crafted prompting or extensive post-training, LLM-based autonomous agents augmented with external tools (\eg, APIs) have demonstrated exceptional instruction-following capabilities in a wide range of  tasks~\citep{schick2024toolformer,qin2023toolllm,srinivasan2023nexusraven,zeng2023agenttuning}. 

Despite their remarkable task-specific performance, existing LLM agents often face the following challenges: 
(1) \textbf{Overemphasis on instruction fine-tuning while ignoring the pre-training stage.} LLMs typically undergo a two-stage training process: pre-training to learn general knowledge and instruction fine-tuning to align to specific tasks and user preferences.
The \emph{Superficial Alignment Hypothesis}~\citep{zhou2024lima, gudibande2024the, lin2024the} posits that LLMs acquire most of their knowledge during pre-training, which is more important than instruction fine-tuning in terms of obtaining generalizable fundamental capabilities. 
However, the majority of existing agent frameworks (Figure~\ref{fig:related}) focus on instruction fine-tuning to align with specific patterns or formats, rather than fundamentally enhancing model knowledge or capabilities (\eg, API function calling).
(2) \textbf{Scarcity of agent-oriented pre-training data.} 
% As the primary data source for pre-training, web archives contain limited data~\cite{zhang2024xlam} specifically tailored for agent pre-training.
% Agent instructions are significantly different from other instructions. Knowledge about function calls is hard to transfer from other data sources.
Agent instructions and trajectories significantly differ from general instructions and responses~\cite{zhang2024xlam}. Thus, function-calling knowledge is difficult to derive directly from web archives, the primary pre-training data source.
This notable lack of agent-specific pre-training corpora constrains LLMs from effectively acquiring new agentic knowledge and capabilities (Table~\ref{tab:related}).
(3) \textbf{Limited generalization across multiple tasks.}
LLM agents often struggle to generalize to new scenarios (\eg, from single to multiple tools) that differ from their original fine-tuning data distributions~\cite{qin2023toolllm}. 

To address these challenges, we introduce \dataset, a large-scale pre-training corpus specifically designed to enhance the fundamental capabilities of LLM agents in API function calling, intrinsic reasoning and planning, and adaptation to environmental feedback. 
Specifically, we focus on two primary objectives: (a) improving \emph{comprehension of individual function calls}, and (b) strengthening \emph{intrinsic reasoning capabilities} for solving problems requiring multiple function calls. 
To enhance (a) comprehension of API functions and alignment with their formats, we collect a large-scale dataset of tool documentation tailored for LLM pre-training on API function calls. 
% Since the volume of tool-related data remains significantly smaller than that of plain text or code data, we employ data augmentation and generation techniques to expand the tool-related dataset.
Given the expanding range of tasks with growing complexity, we incorporate a vast number of function calling trajectories to improve (b) intrinsic reasoning abilities in sequencing API function calls. 
We then integrate this meticulously curated tool documentation and function-calling data with code (to bolster reasoning capabilities) and text data (to maintain robust text generation capabilities), creating a \emph{multi-source}, \emph{large-scale}, and \emph{high-quality} training corpus, \dataset.

% Through extensive investigations of scaling laws, we develop a pilot pre-training data recipe with empirically optimal mixing ratios that maximize LLM agents' fundamental capabilities and generalization potential.
Building upon \dataset, we introduce a continual pre-trained open-source LLM, \method, an LLM with strong agentic and autonomous capabilities across domains, bringing open-source models closer to the capabilities of commercial LLMs. 
Our empirical evaluations demonstrate that \texttt{\method-8B} outperforms open-source LLMs at small to medium scales (\eg, $9.6\%$ over \texttt{LLaMA-3-8B} and $17.6\%$ over \texttt{Mixtral-8x22B}) and performs comparably to API-based large commercial LLMs (\eg, $18.9\%$ over \texttt{Claude-3-Haiku} and $4.1\%$ over \texttt{GPT-3.5-turbo}) across three agent benchmarks. 
Our large-scale ablation studies further demonstrate the effectiveness of retrieved agent data in scaling up and diversifying the coverage of scenarios in pre-training.
% Experiments highlight the effectiveness and validity of our pre-training corpus, \dataset, for enhanced fundamental capabilities and superior generalization.
Our contributions can be summarized as follows:
\begin{itemize}[leftmargin=0.6cm]
\setlength\itemsep{-0.2em}
\item We curate \dataset, a large-scale pre-training corpus designed to enhance understanding of API function calls and guide actionable trajectories for LLM agents. 
Remarkably, through exhaustive scaling law experiments, we discover a pioneering pre-training recipe with an empirically optimal data mix ratio. 
\item We propose \method, a foundation model that exhibits enhanced fundamental agentic capabilities, including API function calling, intrinsic reasoning and planning, and adaptation to environmental feedback, achieved through continual pre-training on \dataset.
\item We extensively compare \method with strong baselines across three agent benchmarks, verifying its enhanced fundamental agentic capabilities and superior generalization derived from \dataset. 
\end{itemize}
% Upon acceptance, we will release the comprehensive pre-training recipe with publicly available datasets and accessible model checkpoints to promote transparency, reproducibility, and potential generalization for the research community.

% This is probably the first work discussing the agent capability in pretraining? for these we study the recipe, scaling law, and train the models to verify the effectiveness
% insights
% I think it should be agent pre-training recipe not a single checkpoint. The recipe includes data and algorithm. There is no much to say about the algorithm right? So I feel the data is more important, all other points are like supporting the effectiveness and validness of the data. Sources -> how we get the data originally. Scaling law -> how we get the mixing ratios. Experiments and model checkpoints -> why the data is effective.
