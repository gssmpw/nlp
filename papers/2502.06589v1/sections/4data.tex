\section{\dataset}
\label{sec:dataset}

% Given that the volume of tool-related data remains significantly smaller than that of plain text or code data, we employ data augmentation and generation techniques to expand the tool-related dataset.

% Recognizing that action trajectory data often appears in multi-modal formats (\eg, robot trajectories, gaming strategies, and videos), we ground these diverse modalities in textual descriptions, enriching \dataset and guiding \method to learn intrinsic reasoning behind action sequences.


% Our main objective is to comprehensively enhance the richness and diversity of the dataset.
% We have gained valuable insights from reputable sources such as (Computer, 2023; Gao et al.,
% 2020; Penedo et al., 2023; Touvron et al., 2023a). To achieve these goals, we have organized our
% approach into three essential stages: deduplication, filtering, and remixing. The deduplication
% and remixing stages ensure a diverse representation of the data by sampling unique instances.
% The filtering stage enhances the density of information, thereby enabling more efficient and
% effective model training.
% We adopted an aggressive deduplication strategy, expanding the deduplication scope. Our
% analysis revealed that deduplicating the entire Common Crawl corpus results in higher removal
% of duplicate instances compared to deduplicating within a single dump. Table 1 illustrates
% that deduplicating across 91 dumps eliminates four times more documents than a single dump
% method.
% In the filtering stage, we focus on developing robust criteria for document quality assessment. This involves a detailed analysis incorporating both linguistic and semantic evaluations,
% providing a view of data quality from individual and global perspectives. In the remixing phase,
% we adjust our approach to address data imbalances, focusing on increasing the presence of
% underrepresented domains. This adjustment aims to achieve a more balanced and inclusive
% dataset, ensuring that diverse perspectives and information are adequately represented.
% For our tokenizer, we implemented the Byte-level Byte-Pair Encoding (BBPE) algorithm
% based on the tokenizers library (Huggingface Team, 2019). Pre-tokenization was employed to
% 4
% prevent the merging of tokens from different character categories such as new lines, punctuation,
% and Chinese-Japanese-Korean (CJK) symbols, similar to GPT-2 (Radford et al., 2019). We also
% chose to split numbers into individual digits following the approach used in (Touvron et al.,
% 2023a,b). Based on our prior experience, we set the number of conventional tokens in the
% vocabulary at 100000. The tokenizer was trained on a multilingual corpus of approximately
% 24 GB, and we augmented the final vocabulary with 15 special tokens, bringing the total
% size to 100015. To ensure computational efficiency during training and to reserve space for
% any additional special tokens that might be needed in the future, we configured the modelâ€™s
% vocabulary size to 102400 for training.

To scale and diversify the pre-training corpus for LLM agents, we introduce a three-stage construction process for \dataset (see Figure~\ref{fig:data}): (1) \textbf{Seed Data Collection} (\cref{sec:data-phase1}), where we gather initial high-quality samples; (2) \textbf{Web Data Retrieval} (\cref{sec:data-phase2}), which expands the seed data by retrieving relevant data from the web; and (3) \textbf{Data Quality Control} (\cref{sec:data-phase3}), where we ensure the integrity and relevance of the collected data. 
% Furthermore, we present a \textbf{scaling law} (\cref{sec:data-phase4}) for the optimal distribution of multiple data types.

\subsection{Seed Data Collection}
\label{sec:data-phase1}

For seed data collection, we first traverse available public resources to gather high-quality API documentation and action trajectories, including: 
% We collect a set of high-quality agent data from public API documentation websites, public repositories, and LLM-synthesized data. 
% Given the sparsity of agent-specific data, we treat these high-quality data as seed data for further expansion.
(1) \textbf{Public APIs.} High-quality API documentation is collected from over $1,400$ public APIs and official websites, including detailed function definitions and parameter descriptions. 
(2) \textbf{Public Repositories.} To improve intrinsic reasoning, we integrate action trajectories from over $60$ public repositories across diverse domains, such as programming code and web interactions. 
(3) \textbf{Code-to-Text Synthesis.} Given the limited coverage of curated data, we use LLMs to synthesize additional API documentation from \emph{StarCoder-API}, generating examples based on code snippets. 
(4) \textbf{Simulated Agent Data.} We gather simulated action sequences with observational data to facilitate adaptation to environmental feedback. 
Importantly, we offer step-by-step details of the seed data collection process in \cref{app:4-1} for reproducibility.

% \noindent\textbf{Public APIs.}
% To enhance the accurate function calling capability of LLMs, we collect data from over 1,400 public API documentation and integrate additional data from official websites.
% % , including Huggingface\footnote{\url{https://huggingface.co/docs}}, TorchHub\footnote{\url{https://pytorch.org/docs/stable/index.html}}, and Python Modules\footnote{\url{https://docs.python.org/3/index.html}}, among others.
% This compilation includes detailed API definitions and parameter descriptions, enabling LLM agents to gain a better understanding of API functions.
% % As the depth and location of the documentation vary across different API websites, we apply a three-level scraping strategy: 
% % (1) \emph{Level 1:} the collected 1,400 URLs; 
% % (2) \emph{Level 2:} 37,753 URLs appearing on the Level 1 web pages; 
% % (3) \emph{Level 3:} 83,468 URLs appearing in the Level 2 web pages. 
% % We then apply URL checks to verify validity and filter for documentation-relevant data by searching for keywords (\eg, ``doc'', ``guide'', ``reference'', \etc).

% \noindent\textbf{Public Repositories.}
% To enhance intrinsic reasoning and planning, we incorporate publicly available action trajectories from over $60$ public repositories of relevant papers and datasets. The action trajectories are sourced from diverse domains, including programming code, natural language reasoning steps, embodied AI action sequences, grounded multi-modal information, web interactions, and function call sequences. The diversity of trajectories included during the pre-training stage enhances the model's intrinsic reasoning capabilities and improves generalization to distinct scenarios.

% \noindent\textbf{Code-to-Text Synthesis.}
% As the curated data from public APIs and repositories are limited in number and API coverage, we leverage the strong generation capabilities of LLMs to synthesize additional API documentation and use cases. 
% Data accuracy is fundamental to the effectiveness of synthetic agent data~\cite{liu2024toolace}.
% To obtain high-quality agent data, we leverage \emph{StarCoder-API}\footnote{\url{https://huggingface.co/datasets/luna-code/starcoderdata-apis}} as the knowledge base, which contains code snippets covering third-party APIs.
% We generate corresponding API documentation and use cases given the code snippets and API function calls in the code.
% % For efficiency, we utilize multiple LLMs from Amazon Bedrock\footnote{\url{https://aws.amazon.com/bedrock/}} for data synthesis, including \texttt{Claude-3-Sonnet}, \texttt{Claude-3-Haiku}~\cite{claude-3}, \texttt{Mistral-Large}~\cite{mistral-large}, \texttt{LLaMA-3-70B-Instruct}~\cite{dubey2024llama}, and \texttt{Command-R-Plus}~\cite{command-r-plus}.

% \noindent\textbf{Simulated Agent Data.}
% To boost the model's capability to adapt based on environmental feedback, we gather action sequences integrated with observational data from environments, represented as $\{o_0,a_1,o_1,a_2,o_2,\cdots,a_{T_g},o_{T_g}\}$, which encodes how to react according to environmental observations into the model parameters.
% % We run the official codes from agent frameworks~\cite{yao2022react, sun2024adaplanner, wang2024llms, shinn2024reflexion} on multi-step reasoning (\eg, HotpotQA~\cite{yang2018hotpotqa}) and sequential decision making (\eg, ALFWorld~\cite{shridhar2021alfworld}) to collect action trajectories with interaction and environmental feedback.

\subsection{Web Data Retrieval}
\label{sec:data-phase2}
Given the limited availability of agent-oriented data, we use the high-quality data described in \cref{sec:data-phase1} as seed data for further expansion. To enhance agentic capabilities, we retrieve a diverse set of examples from web crawls, focusing on content relevant to API documentation and action trajectories.
% Due to the limited availability of agent-specific data, we employ the high-quality data outlined in \cref{sec:data-phase1} as seed data for further expansion. Specifically, we retrieve a diverse set of examples from web crawls that are relevant to API documentation and action trajectories for domain-specific capabilities enhancement.
Our retrieval process involves the following steps:
(1) \textbf{Web Data Corpus Creation.} Similar to CommonCrawl~\cite{raffel2020exploring} and FineWeb~\citep{penedo2024finewebdatasetsdecantingweb}, we first compile a large-scale web data corpus.
(2) \textbf{Semantic Matching.} We utilize COCO-DR~\cite{yu2022coco} to encode semantic representations of documents in the seed data and the large-scale web corpus. 
% (3) \textbf{Similarity Computation.} 
% With the high-dimensional embeddings capturing the semantic content of each document, 
We then retrieve the top-$K$ similar documents by calculating the cosine similarity between the corresponding embeddings. 
It allows us to identify and retrieve documents from the web corpus that are semantically similar to our seed data, effectively enriching our dataset with relevant and diverse information.
(3) \textbf{Quality Control.} To ensure the quality of the retrieved corpus, we perform data pruning to remove semantically redundant content and maintain the diversity of knowledge, preventing overrepresentation of certain topics and ensuring generalization and robustness across domains.


\subsection{Data Quality}
\label{sec:data-phase3}
After retrieving semantically relevant data from the web corpus, we obtain a collection of noisy agent data. To ensure the integrity and relevance of our dataset, it is essential to consistently monitor data quality and filter out content that resembles general text rather than agent-specific data.
First, we employ \texttt{Claude-3-Sonnet}~\cite{claude-3} as the data annotator to annotate a total of $71,473$ samples from the retrieved data, identifying $37,714$ as agent-relevant and $33,767$ as general text paragraphs.
Using the annotated samples, we train a \texttt{fastText}~\cite{joulin2016fasttext} model to effectively recall additional agent-relevant web data. 
% We utilize the open-source \texttt{fastText} library\footnote{\url{https://fasttext.cc}} for training, configuring the vector dimension to $256$, learning rate to $0.1$, the maximum length of word n-gram to $3$, the minimum number of word occurrences to $3$, and the number of training epochs to $3$.
% After training, the \texttt{fastText} model is used to recall agent-relevant data from the remaining retrieved samples. 
% To filter out low-quality content, we rank the collected pages based on their predicted scores from the \texttt{fastText} model and retain only the top-ranking entries. 
This filtering process then reduces the data volume from approximately $200$B to $80$B tokens, ensuring that the preserved data maintains high relevance and quality. See details in \cref{app:4-3}.

\begin{figure}[t]
  \centering
  % \vspace{-2ex}
  \includegraphics[width=\linewidth]{figures/all_metrics-new-v2.pdf}
  % \vspace{-3ex}
  \caption{Scaling law of the relationship between agent data mixing ratio ($\%$) and benchmark loss. 
  % The optimal agent data ratio is indicated at approximately 36\%, corresponding to a balanced distribution among agent data, text data, and code data.
  }
  % \vspace{-2ex}
  \label{fig:sl}
\end{figure}

\begin{figure*}[t]
  \centering
    % \vspace{-4ex}
  \includegraphics[width=\linewidth]{figures/overview-v3.pdf}
  \caption{
  Overview of the pre-training (Stages I \& II) and instruction fine-tuning (III) framework in \method.
  }
  \vspace{-2ex}
  \label{fig:overview}
\end{figure*}

\section{Scaling Laws for Data Composition}
\label{sec:data-phase4}

% \noindent\textbf{Determining Optimal Data Ratios.} 
When designing LLMs, the scaling law~\cite{kaplan2020scaling, hoffmann2022training} is an important predictive tool that can estimate the performance (\eg, benchmark loss) of a large-sized target model using a scaling curve fitted over much smaller models (referred to as sampling models).
We develop scaling laws to determine the optimal data proportion among agent data, text data, and code data. With the total budget of the data volume fixed, our scaling law experiments show that the effect of agent data ratio $x$ on the loss $\mathcal{L}$ of a pre-trained model follows power laws:
\begin{equation*}
    \begin{aligned}
        \mathcal{L}=c+kx^{\alpha},
    \end{aligned}
\end{equation*}
where $c$, $k$, and $\alpha$ are parameters to be fitted.
By fitting these parameters using a collection of small models, training data, or computational resources, scaling laws can extrapolate to precisely predict the test loss of larger cases over orders of magnitude.

\noindent\textbf{Scaling Law Experiments.}
Concretely, we construct our scaling laws by pre-training models ranging in $45$M to $0.65$B parameters.
To simulate the continual pre-training setting, we amplify the target data volume used for training each small model to $50\times$ model parameters.
Consequently, the total compute budgets for the scaling law experiments span from $7\times10^{17}$ to $2\times10^{20}$ FLOPs.
% At each compute budget, we pre-train models ranging .
% Concretely, we construct our scaling laws by pre-training models using compute budgets between $7\times10^{17}$ FLOPs and $2\times10^{20}$ FLOPs.
% At each compute budget, we pre-train models ranging in $45$M to $0.65$B parameters.
% To simulate the continual pre-training setting, we loosen the target data volume used for training each small model to $50\times$ model parameters.
Regarding data proportions, we begin with the seed agent data and progressively incorporate the retrieved web corpus to increase the agent data ratio. Concurrently, as the agent data ratio increases, we proportionally decrease the volumes of general text and code data to maintain the fixed total data volume.
% The peak learning rate is set between $1\times 10^{-3}$ and $1\times 10^{-2}$ depending on the size of the model. 
Following~\citet{dubey2024llama}, we leverage the benchmark loss of Nexus~\cite{srinivasan2023nexusraven}, API-Bank~\cite{li2023api}, API-Bench~\cite{patil2023gorilla} to monitor the agent capabilities, and MMLU~\cite{hendrycks2020measuring} to monitor the general capabilities of LLMs.

\noindent\textbf{Optimal Data Mixing Ratio.}
Figure~\ref{fig:sl} illustrates that the optimal mixture of agent data within the entire pre-training corpus is approximately 36\%, indicating that the proportion of agent data, text data, and code data should be roughly $1:1:1$.
This balanced distribution promotes both specialized agent capabilities and general language understanding, ensuring that the model remains versatile and robust across diverse tasks and domains.

\noindent\textbf{Remark.} The established scaling laws provide critical insights into the data composition for pre-training LLM agents. By identifying the optimal ratio of agent data, we ensure that the model effectively balances specialized agentic capabilities with general language proficiency. 
% This balance is crucial for developing versatile autonomous agents capable of performing complex, multi-step tasks while maintaining strong performance on a wide range of language understanding benchmarks.

% This is probably the first work discussing the agent capability in pretraining? for these we study the recipe, scaling law, and train the models to verify the effectiveness
% insights
% I think it should be agent pre-training recipe not a single checkpoint. The recipe includes data and algorithm. There is no much to say about the algorithm right? So I feel the data is more important, all other points are like supporting the effectiveness and validness of the data. Sources -> how we get the data originally. Scaling law -> how we get the mixing ratios. Experiments and model checkpoints -> why the data is effective.
