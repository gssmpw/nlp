\begin{table*}[t]
\centering
\fontsize{7}{9}\selectfont\setlength{\tabcolsep}{0.3em}
% \vspace{-4ex}
\begin{tabular}{@{}l>{}ccccc>{}ccccc@{}}
\toprule
\textbf{} & \multicolumn{5}{c}{\textbf{Benchmark Metrics ($\uparrow$)}} & \multicolumn{5}{c}{\textbf{Benchmark Loss ($\downarrow$)}}\\
\cmidrule(lr){2-6} \cmidrule(lr){7-11}
\textbf{Models ($\downarrow$) / Datasets ($\rightarrow$)} & \textbf{GSM8K}  & \textbf{HumanEval} & \textbf{HumanEval+} &  \textbf{BBH} & \textbf{OA}  & \textbf{IFEval} & \textbf{hellaswag}  & \textbf{MMLU}  &  \textbf{BBH}  & \textbf{OA}  \\\midrule
LLaMA-3-8B~\cite{dubey2024llama}           & 0.420 & 0.372 & 0.317 & 0.613  & 0.431 & {0.648} & {0.759} & {0.526} & {0.361} & {0.573} \\
\rowcolor{teal!12} \method-8B-Base       & 0.460 & 0.411 & 0.356 & 0.584  & 0.453 & 0.683 & 0.769 & 0.536 & 0.374 & 0.591 \\ \midrule
LLaMA-3-8B-IFT       & {0.695} & 0.343 & 0.337 & {0.596}  & 0.493 & 1.046 & 0.908 & 0.725 & 0.503 & 0.795 \\
\rowcolor{teal!12} \method-8B-IFT    & 0.686 & {0.373} & {0.373} & 0.567 & {0.500} & {0.657} & {0.784} & {0.559} & {0.369} & {0.592} \\ \midrule
% \multicolumn{11}{l}{\emph{For Reference: }}  \\ \midrule
ToolACE-8B~\cite{liu2024toolace}           & 0.623 & 0.385 & 0.324 & 0.120  & 0.363 & 0.774 & 0.848 & 0.602 & 0.442 & 0.666 \\
AgentLM-7B~\cite{zeng2023agenttuning}           & 0.549 & 0.122 & 0.110 & 0.071  & 0.213 & 0.783 & 0.915 & 0.657 & 0.450 & 0.701 \\
LLaMA-3-8B-Instruct~\cite{dubey2024llama}  & 0.797 & 0.646 & 0.573 & 0.660  & 0.669 & 0.619 & 0.769 & 0.533 & 0.361 & 0.570 \\
\bottomrule
\end{tabular}
\caption{Comprehensive evaluation of general model capabilities across diverse benchmarks. \method maintains general capabilities while achieving competitive performance against baseline and specialized models. 
}\label{tab:preserve}
\vspace{-3ex}
% \vspace{-1ex}
\end{table*}