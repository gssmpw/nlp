\begin{table*}[h]
\centering
\fontsize{7}{9}\selectfont\setlength{\tabcolsep}{0.3em}
\begin{tabular}{@{}lcc>{}ccccccc>{}cccccc>{}c@{}}
\toprule
\textbf{Datasets ($\rightarrow$)} & \multicolumn{8}{c}{\textbf{AST}} & \multicolumn{7}{c}{\textbf{Exec}} & \textbf{BFCL-v2}\\
\cmidrule(lr){2-9} \cmidrule(lr){10-16} \cmidrule(lr){17-17}
\textbf{Models ($\downarrow$)} & \textbf{OA} & \textbf{Simple} & \textbf{Python} &  \textbf{Java} & \textbf{JS} & \textbf{MF} & \textbf{PF} & \textbf{PM} & \textbf{OA} & \textbf{Simple} & \textbf{Python} & \textbf{REST} & \textbf{MF} & \textbf{PF} &\textbf{PM} & \textbf{OA} \\\midrule
% \textbf{Models ($\downarrow$)$\slash$Datasets ($\rightarrow$)} & \textbf{Overall} & \textbf{OS} & \textbf{DB} &  \textbf{AlfWorld} & \textbf{KG} & \textbf{Mind2Web} & \textbf{Webshop}\\\midrule
% \textbf{Weights} & 1.0000 & 10.8 & 13.0 & 13.0 & 13.9 & 11.6 & 30.7 \\\midrule
\multicolumn{16}{l}{\emph{Base LLMs}}  \\ \midrule
LLaMA-3-8B~\cite{dubey2024llama} & 0.94 & 1.3 & 1.0 & 2.0 & 1.5 & 0.5 & 0.5 & 0.5 & 0.40 & 2.0 & 1.0 & 1.0 & 0.0 & 0.0 & 0.0 & 17.77 \\
LLaMA-3.1-8B~\cite{dubey2024llama}  & 6.05 & 10.2 & 12.0 & 5.0 & 6.0 & 4.0 & 7.5 & 2.5 & 0.43 & 1.7 & 2.0 & 1.4 & 0.0 & 0.0 & 0.0 & 21.10\\
\rowcolor{teal!12} \method-8B-Base & \textbf{15.4} & \textbf{12.2} & \textbf{15.0} & \textbf{4.0} & \textbf{6.0} & \textbf{25.0} & \textbf{11.5} & \textbf{13.0} & \textbf{2.24} & \textbf{2.9} & \textbf{2.0} & \textbf{4.3} & \textbf{6.0} & \textbf{0.0} & \textbf{0.0} & \textbf{25.18}\\\midrule
\multicolumn{16}{l}{\emph{Open-Source Instruction Fine-Tuned LLMs (Small)}}  \\ \midrule
% LLaMA-2-7B-Chat~\cite{touvron2023llama} & 7B & OS & 0.36 & 4.2 & 8.0 & 0.0 & 2.1 & 7.0 & 11.6 & - & - & - & - & - & - \\
% Vicuna-7B-v1.5~\cite{chiang2023vicuna} & 7B & OS & 0.43 & 9.7 & 8.7 & 0.0 & 2.5 & 9.0 & 2.2 & - & - & - & - & - & -\\
% CodeLLaMA-7B-Instruct~\cite{roziere2023code} & 7B & OS& 0.65 & 4.9 & 12.7 & 0.0 & 8.2 & 12.0 & 25.2 & - & - & - & - & - & - \\
% CodeLLaMA-13B-Instruct~\cite{roziere2023code} & 13B & OS & 0.74 & 3.5 & 9.7 & 0.0 & 10.4 & 14.0 & 43.8 & - & - & - & - & - & - \\
% LLaMA-2-13B-Chat~\cite{touvron2023llama} & 13B & OS & 0.66 & 4.2 & 11.7 & 6.0 & 3.6 & 13.0 & 25.3 & - & - & - & - & - & - \\
% Vicuna-13B-v1.5~\cite{chiang2023vicuna} &13B & OS & 0.86 & 10.4 & 6.7 & 8.0 & 9.4 & 12.0 & 41.7 & - & - & - & - & - & - \\
% Groq-8B-Tool-Use~\cite{groq} & 8B & OS & 1.27 & 15.3 & 11.7 & 4.0 & 17.6 & \underline{23.0} & 53.4 & 30.44 & 42.8 & 35.5 & 45.5 & 0.0 & \textbf{89.06} \\
LLaMA-3-8B-Instruct~\cite{dubey2024llama} & 60.47 & 58.3 & 65.5 & 38.0	& 42.0 & 76.5 & 58.0 & \textbf{49.0} & 68.88 & 44.5 & 89.0 & 55.7 & 86.0 & \textbf{78.0} & \textbf{55.0} & 59.57\\
LLaMA-3.1-8B-Instruct~\cite{dubey2024llama} & 58.38 & 60.0 & 68.8 & 32.0 & 46.0 & 66.5 & 65.0 & 42.0 & \textbf{72.60} & 83.7 & 87.0 & 77.1 & 83.0 & 76.0 & 52.5 & 61.39\\
% % Qwen-1.5-7B-Chat~\cite{} & 1.5922	& 11.1	& 41	& 14	& 18.4	& 13	& 56.8 \\\midrule
LLaMA-3-8B-IFT & 47.43 & 66.7 & 75.5 & 37.0 & \textbf{56.0} & 45.5 & 54.0 & 23.5 & 63.41 & \textbf{87.7} & 93.0 & \textbf{80.0} & 68.0 & 58.0 & 40.0 & 62.12\\
\rowcolor{teal!12} \method-8B-IFT & \textbf{66.39} & \textbf{72.5} & \textbf{81.8} & \textbf{45.0} & 54.0 & \textbf{79.5} & \textbf{70.5} & 43.0 & 69.82 & 85.3 & \textbf{95.0} & 71.4 & \textbf{88.0} & 66.0 & 40.0 & \textbf{70.78}\\\midrule
\multicolumn{16}{l}{\emph{For Reference: Open-Source Instruction Fine-Tuned LLMs (Medium to Large) and API-based Commercial LLMs}}  \\ \midrule
% LLaMA-2-70B-Chat~\cite{touvron2023llama} & 70B & OS & 0.66 & 9.7 & 13.0 & 2.0 & 8.0 & 19.0 & 5.6 & - & - & - & - & - & - \\
% CodeLLaMA-34B-Instruct~\cite{roziere2023code} & 34B & OS & 1.13 & 2.8 & 14.0 & 4.0 & 23.5 & 20.0 & 52.1 & - & - & - & - & - & - \\
Gemini-1.5-Flash~\cite{reid2024gemini} & 77.44 & 67.3 & 92.8 & 55.0 & 54.0 & 94.0 & 71.5 & 77.0 & 73.23 & 57.9 & 93.0 & 22.9 & 86.0 & 74.0 & 75.0 & 70.75\\
% text-davinci-003~\cite{ouyang2022training} & - & API & 1.90 & 20.1 & 16.3 & 20.0 & 34.9 & 26.0 & 61.7 & - & - & - & - & - & - \\
% DeepSeek-v2~\cite{liu2024deepseek} & 236B & OS & 1.97	& 20.8	& 21.7	& 38.0	& 21.7	& 22.0	& 57.4 & - & - & - & - & - & -\\
Mixtral-8x22B~\cite{jiang2024mixtral} & 57.92 & 67.2 & 87.5 & 54.0 & 60.0 & 82.0 & 50.5 & 32.0 & 63.59 & 71.9 & 88.0 & 55.7 & 74.0 & 56.0 & 52.5 & 63.26\\
gpt-3.5-turbo-0125~\cite{chatgpt} & 66.31 & 63.8 & 75.3 & 50.0 & 66.0 & 78.0 & 68.0 & 55.5 & 65.88 & 44.5 & 89.0 & 0.0 & 86.0 & 78.0 & 55.0  & 66.53 \\
Claude-3-Haiku~\cite{claude-3} & 62.52 & 77.6 & 95.8 & 63.0 & 74.0 & 93.0 & 47.5 & 32.0 & 60.73 & 89.4& 96.0 & 82.9 & 94.0 & 32.0 & 27.5 & 55.47 \\
Command-R-Plus-FC~\cite{command-r-plus} & 77.65 & 69. 6 & 85.8 & 61.0 & 62.0 & 88.0 & 82.5 & 70.5 & 77.41 & 89.1 & 94.0 & 84.3 & 86.0 & 82.0 & 52.5 & 76.29\\
LLaMA-3-70B-Instruct~\cite{dubey2024llama} & 87.90 & 75.6 & 94.8 & 60.0 & 72.0 & 94.0 & 93.0 &89.0 & 88.04 & 94.1 & 94.0 & 94.3 & 94.0 & 84.0 & 80.0  & 84.95\\
gpt-4-0613~\cite{achiam2023gpt} & 91.92 & 81.2 & 95.5 & 68.0 & 80.0 & 96.0 & 96.0 & 94.5 & 87.57 & 98.3 & 98.0 & 98.6 & 96.0 & 86.0 & 70.0 & {89.26} \\\bottomrule
\end{tabular}
\caption{Main experiment results on BFCL-v2. 
% \wrj{Is 89.06 of Groq-8B-Tool-Use on BFCL-v2 an abnormal value? Or the best acc to be highlighted in bold? Groq-8B-Tool-Use should be second best on Agent-Bench WB? LLaMa-3.1-Instruct should be second best on AgentBench OS? }
}\label{tab:bfcl-v2}
% \vspace{-1ex}
\end{table*}