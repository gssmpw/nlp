% \begin{table*}[t]
% \centering
% \caption{Main experiment results on the AgentBench.
% }\label{tab:agentbench}
% \fontsize{8}{10}\selectfont\setlength{\tabcolsep}{0.3em}
% \begin{tabular}{@{}lccccccccccc@{}}
% \toprule
% \textbf{Datasets ($\rightarrow$)} & \multicolumn{7}{c}{\textbf{AgentBench} ($\uparrow$)} & \multirow{2.5}{*}{\textbf{API-Bench} ($\downarrow$)} & \multirow{2.5}{*}{\textbf{API-Bank} ($\downarrow$)} & \multirow{2.5}{*}{\textbf{Nexus} ($\downarrow$)} & \multirow{2.5}{*}{\textbf{MMLU} ($\downarrow$)}\\
% \cmidrule{2-8}
% \textbf{Models ($\downarrow$)} & \textbf{Overall} & \textbf{OS} & \textbf{DB} &  \textbf{AlfWorld} & \textbf{KG} & \textbf{Mind2Web} & \textbf{Webshop}\\\midrule
% % \textbf{Weights} & 1.0000 & 10.8 & 13.0 & 13.0 & 13.9 & 11.6 & 30.7 \\\midrule
% % \multicolumn{7}{l}{\quad \emph{Base Models}}  \\\midrule 
% LLaMA-3-8B & 0.5627 & 2.8 & 12.0 & 0.0 & 8.9 & 11.0 & 1.4 & 57.96 & 19.21 & 4169.26 & 9.04\\
% LLaMA-3.1-8B & 1.0450 & 15.3 & 5.3 & 8.0 & 12.7 & 18.0 & 41.9 & 48.66 & 17.99 & 2837.60 & 8.43\\
% \rowcolor{teal!10} \method-8B & \textbf{1.8703} & \textbf{20.8} & \textbf{32.3} & \textbf{30.0} & \textbf{16.0} & \textbf{16.0} & \textbf{60.5} & 4.29 & 2.11 & 70.78 & 7.66\\\bottomrule
% \end{tabular}
% \vspace{-1ex}
% \end{table*}

\begin{table*}[t]
\centering
\fontsize{7}{9}\selectfont\setlength{\tabcolsep}{0.4em}
\begin{tabular}{@{}lcc>{}ccccccc>{}ccccc>{}c@{}}
\toprule
\textbf{Datasets ($\rightarrow$)} & \multicolumn{2}{c}{\textbf{Model}} & \multicolumn{7}{c}{\textbf{AgentBench}} & \multicolumn{5}{c}{\textbf{BFCL-v3}}& \textbf{BFCL-v2}\\
\cmidrule(lr){2-3} \cmidrule(lr){4-10} \cmidrule(lr){11-15} \cmidrule(lr){16-16}
\textbf{Models ($\downarrow$)} & \textbf{Size} & \textbf{Type} & \textbf{OA} & \textbf{OS} & \textbf{DB} &  \textbf{HH} & \textbf{KG} & \textbf{WB} & \textbf{WS} & \textbf{OA} & \textbf{NL-AST} & \textbf{Exec} & \textbf{L-AST} & \textbf{MT} & \textbf{OA}\\\midrule
% \textbf{Models ($\downarrow$)$\slash$Datasets ($\rightarrow$)} & \textbf{Overall} & \textbf{OS} & \textbf{DB} &  \textbf{AlfWorld} & \textbf{KG} & \textbf{Mind2Web} & \textbf{Webshop}\\\midrule
% \textbf{Weights} & 1.0000 & 10.8 & 13.0 & 13.0 & 13.9 & 11.6 & 30.7 \\\midrule
\multicolumn{16}{l}{\emph{Base LLMs}}  \\ \midrule
LLaMA-3-8B~\cite{dubey2024llama} & 8B & OSS & 0.56 & 2.8 & 12.0 & 0.0 & 8.9 & 11.0 & 1.4 & 17.73 & 4.3 & 2.5 & 39.1 & 0.0 & 17.77\\
LLaMA-3.1-8B~\cite{dubey2024llama} & 8B & OSS & 1.05 & 15.3 & 5.3 & 8.0 & 12.7 & \textbf{18.0} & 41.9 & 19.50 & 16.3 & 10.7 & 37.5 & 0.0 & 21.08 \\
\rowcolor{teal!12} \method-8B-Base & 8B & OSS & \textbf{1.87} & \textbf{20.8} & \textbf{32.3} & \textbf{30.0} & \textbf{16.0} & 16.0 & \textbf{60.5} & \textbf{22.12} & \textbf{18.1} & \textbf{12.1} & \textbf{42.2} & \textbf{4.0} & \textbf{25.18}\\\midrule
\multicolumn{16}{l}{\emph{Open-Source Instruction Fine-Tuned LLMs (Small)}}  \\ \midrule
LLaMA-2-7B-Chat~\cite{touvron2023llama} & 7B & OSS & 0.36 & 4.2 & 8.0 & 0.0 & 2.1 & 7.0 & 11.6 & - & - & - & - & - & - \\
Vicuna-7B-v1.5~\cite{chiang2023vicuna} & 7B & OSS & 0.43 & 9.7 & 8.7 & 0.0 & 2.5 & 9.0 & 2.2 & - & - & - & - & - & -\\
CodeLLaMA-7B-Instruct~\cite{roziere2023code} & 7B & OSS& 0.65 & 4.9 & 12.7 & 0.0 & 8.2 & 12.0 & 25.2 & - & - & - & - & - & - \\
CodeLLaMA-13B-Instruct~\cite{roziere2023code} & 13B & OSS & 0.74 & 3.5 & 9.7 & 0.0 & 10.4 & 14.0 & 43.8 & - & - & - & - & - & - \\
LLaMA-2-13B-Chat~\cite{touvron2023llama} & 13B & OSS & 0.66 & 4.2 & 11.7 & 6.0 & 3.6 & 13.0 & 25.3 & - & - & - & - & - & - \\
Vicuna-13B-v1.5~\cite{chiang2023vicuna} &13B & OSS & 0.86 & 10.4 & 6.7 & 8.0 & 9.4 & 12.0 & 41.7 & - & - & - & - & - & - \\
Groq-8B-Tool-Use~\cite{groq} & 8B & OSS & 1.27 & 15.3 & 11.7 & 4.0 & 17.6 & \underline{23.0} & 53.4 & 30.44 & 42.8 & 35.5 & 45.5 & 0.0 & \textbf{89.06} \\
LLaMA-3-8B-Instruct~\cite{dubey2024llama} & 8B & OSS & 1.51 & 18.1 & 12.3 & 24.0 & 15.9 & 19.0 & 56.1 & 35.79 & 60.6 & 66.2 & 48.4 & 0.5 & 59.57\\
LLaMA-3.1-8B-Instruct~\cite{dubey2024llama} & 8B & OSS & 1.74 & \underline{21.5} & 5.3 & \underline{34.0} & 18.4 & \textbf{25.0} & 59.5 & 46.76 & 70.3 & 76.5 & \underline{62.2} & 2.5 & 61.39\\
% Qwen-1.5-7B-Chat~\cite{} & 1.5922	& 11.1	& 41	& 14	& 18.4	& 13	& 56.8 \\\midrule
LLaMA-3-8B-IFT & 8B & OSS & \underline{2.07} & \textbf{22.2} & \underline{29.7} & 32.0 & \textbf{25.3} & 19.0 & \textbf{66.1}  & \underline{48.52} & \underline{72.5} & \underline{81.8} & \textbf{66.8} & \underline{2.6}  & 62.12\\
\rowcolor{teal!12} \method-8B-IFT & 8B & OSS & \textbf{2.29} & 20.8 & \textbf{41.7} & \textbf{46.0} & \underline{21.2} &17.0 & \underline{63.9}  & \textbf{50.59} & \textbf{84.3} & \textbf{86.2} & 60.1 & \textbf{9.6} & \underline{70.78}\\\midrule
\multicolumn{16}{l}{\emph{For Reference: Open-Source Instruction Fine-Tuned LLMs (Medium to Large) and API-based Commercial LLMs}}  \\ \midrule
LLaMA-2-70B-Chat~\cite{touvron2023llama} & 70B & OSS & 0.66 & 9.7 & 13.0 & 2.0 & 8.0 & 19.0 & 5.6 & - & - & - & - & - & - \\
CodeLLaMA-34B-Instruct~\cite{roziere2023code} & 34B & OSS & 1.13 & 2.8 & 14.0 & 4.0 & 23.5 & 20.0 & 52.1 & - & - & - & - & - & - \\
Gemini-1.5-Flash~\cite{reid2024gemini} & - & API &	1.81	& 20.1	& 46.0	& 22.0	& 14.2	& 17.0	& 39.1 & {53.01} & 77.1 & 71.2 & 71.2& 13.1 & 70.75\\
text-davinci-003~\cite{ouyang2022training} & - & API & 1.90 & 20.1 & 16.3 & 20.0 & 34.9 & 26.0 & 61.7 & - & - & - & - & - & - \\
DeepSeek-v2~\cite{liu2024deepseek} & 236B & OSS & 1.97	& 20.8	& 21.7	& 38.0	& 21.7	& 22.0	& 57.4 & - & - & - & - & - & -\\
Mixtral-8x22B~\cite{jiang2024mixtral} & 176B & OSS	& 2.00	& 24.3	& 25.7	& 14.0	& 31.1	& 28.0	& 62.8 & 43.00 & 56.1 & 59.7 & 65.3 & 8.9 & 63.26\\
gpt-3.5-turbo-0125~\cite{chatgpt} & - & API & 2.12 &	32.6	&	36.7	&	16.0	&	25.9	&	20.0	&	{64.1} & 51.90 & 84.5 & 81.7 & 59.0 & {19.1} & 66.53 \\
Claude-3-Haiku~\cite{claude-3} & - & API & 2.13	& 14.6	& 41.0	& 42.0	& 27.3	& 14.0	& 57.8 & 38.39 & 62.6 & 60.7 & 58.1 & 1.6 & 55.47 \\
Command-R-Plus-FC~\cite{command-r-plus} & - & API & - & - & - & - & - & - & - & 45.22 & 77.7 & 77.4 & 54.2 & 6.1 & 76.29\\
LLaMA-3-70B-Instruct~\cite{dubey2024llama} & 70B & OSS & 2.73 & 28.6 & {50.3}	& 44.0	& 39.5	& 22.0	& 53.6 & 49.55 & {87.2} & {87.4} & {63.4} & 1.1 & 84.95\\
gpt-4-0613~\cite{achiam2023gpt} & - & API & {4.52} & {42.4} & 32.0 & {78.0} & {58.8} & {29.0} & 61.1 & - & - & - & - & - & {89.26} \\\bottomrule
\end{tabular}
% \vspace{-1ex}
\caption{Main experiments on three agent benchmarks across various model scales. 
\textbf{Bold} and \underline{underlined} texts represent the best and the second-best results, respectively. Notations are consistent throughout all tables. ``OSS'', ``API'', and ``OA'' denote ``Open-Sourced LLMs'', ``API-based Commercial LLMs'', and ``Overall'', respectively.
}\label{tab:agentbench}
\vspace{-1ex}
\end{table*}

% \begin{table*}[t]
% \centering
% \fontsize{8}{10}\selectfont\setlength{\tabcolsep}{0.4em}
% \begin{tabular}{@{}lccccccc@{}}
% \toprule
% \textbf{Models ($\downarrow$)$\slash$Datasets ($\rightarrow$)} & \textbf{Overall} & \textbf{OS} & \textbf{DB} &  \textbf{AlfWorld} & \textbf{KG} & \textbf{Mind2Web} & \textbf{Webshop}\\\midrule
% % \textbf{Weights} & 1.0000 & 10.8 & 13.0 & 13.0 & 13.9 & 11.6 & 30.7 \\\midrule
% % \multicolumn{7}{l}{\quad \emph{Base Models}}  \\\midrule 
% LLaMA-3-8B~\cite{dubey2024llama} & 0.5627 & 2.8 & 12.0 & 0.0 & 8.9 & 11.0 & 1.4 \\
% LLaMA-3.1-8B~\cite{dubey2024llama} & 1.0449 & 15.3 & 5.3 & 8.0 & 12.7 & 18.0 & 41.9 \\
% \rowcolor{teal!10} \method-8B-Base & \textbf{1.8703} & \textbf{20.8} & \textbf{32.3} & \textbf{30.0} & \textbf{16.0} & \textbf{16.0} & \textbf{60.5} \\\midrule
% Groq-8B-Tool-Use & 1.2693 & 15.3 & 11.7 & 4.0 & 17.6 & 23.0 & 53.4 \\
% LLaMA-3-8B-Instruct~\cite{dubey2024llama} & 1.5129 & 18.1 & 12.3 & 24.0 & 15.9 & 19.0 & 56.1\\
% % Qwen-1.5-7B-Chat~\cite{} & 1.5922	& 11.1	& 41	& 14	& 18.4	& 13	& 56.8 \\
% LLaMA-3.1-8B-Instruct~\cite{dubey2024llama} & 1.7389 & 21.5 & 5.3 & 34.0 & 18.4 & \textbf{25.0} & 59.5 \\
% Gemini 1.5 Flash~\cite{reid2024gemini} &	1.8088	& 20.1	& \textbf{46.0}	& 22.0	& 14.2	& 17.0	& 39.1 \\
% DeepSeek-v2~\cite{liu2024deepseek} & 1.9743	& 20.8	& 21.7	& 38.0	& 21.7	& 22.0	& 57.4 \\
% mixtral-8x22B~\cite{jiang2024mixtral}	& 2.0001	& 24.3	& 25.7	& 14.0	& 31.1	& 28.0	& 62.8 \\
% Claude-3-Haiku~\cite{claude-3} & 2.1317	& 14.6	& 41.0	& 42.0	& 27.3	& 14.0	& 57.8\\\midrule
% LLaMA-3-8B-IFT & 2.0688 & \textbf{22.2} & 29.7 & 32.0 &\textbf{25.3} & 19.0 & 66.1 \\
% \rowcolor{teal!10} \method-8B-IFT & \textbf{2.2907} & 20.8 & 41.7 & \textbf{46.0} & 21.2 &17.0 & \textbf{63.9} \\\bottomrule
% \end{tabular}
% \caption{Main experiment results on the AgentBench.
% }\label{tab:agentbench}
% % \vspace{-1ex}
% \end{table*}