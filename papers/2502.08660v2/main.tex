% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
% \usepackage[review]{acl}
% \usepackage[review]{acl}
\usepackage[preprint]{acl}



% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

\usepackage{mathrsfs}
\usepackage{paralist}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{pgfplots}
\usepackage{scalefnt}
\usepackage{amsthm,amsmath,amssymb}
\usepackage{mathtools}
\usepackage{bm}
\usepackage[edges]{forest}
\usepackage{soul} % 导入 soul 包
\usepackage{color, xcolor} % 颜色包，color 必须导入，xcolor 建议导入
\usepackage{lipsum}  % TODO: placeholder
\usepackage{colortbl}
\usepackage{booktabs}
\usepackage{array}
\usepackage{stfloats}
\usepackage{tikz-dependency}
\usepackage{tikz}
\usepackage{siunitx}
\usepackage{graphicx}
\usepackage[ruled]{algorithm2e}
\usepackage{lineno}
\usepackage{tabularx}
% \usepackage{subfigure}
\usepackage{subcaption}
\usepackage{helvet}
\usepackage{courier}
\usepackage{makecell}
\usepackage{nicematrix}
\usepackage[fixed]{fontawesome5}
\usepackage{CJKutf8}
\pgfplotsset{compat=1.18}
\sisetup{round-mode=places,round-precision=1}
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

% \title{Semantic Role Labeling: A Comprehensive Survey}
\title{Semantic Role Labeling: A Systematical Survey}





% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}




\newcommand{\chy}[1]{{\color{red}chy: #1}}

\author{
 \textbf{Huiyao Chen\textsuperscript{1}}
 \textbf{Meishan Zhang\textsuperscript{1}}
  \textbf{Jing Li\textsuperscript{1}}
 \textbf{Min Zhang\textsuperscript{1}}
  \textbf{Lilja Øvrelid\textsuperscript{2}}
  \textbf{Jan Hajič\textsuperscript{3}}
 \textbf{Hao Fei\textsuperscript{4}}\Thanks{The corresponding author}
  \\
  \textsuperscript{1}Harbin Institute of Technology (Shenzhen)
  \quad
  \textsuperscript{2} University of Oslo
  \\
  \textsuperscript{3} Charles University
  \quad
  \textsuperscript{4} National University of Singapore
  \\
  \texttt{chenhy1018@gmail.com, mason.zms@gmail.com}
  \\
  \texttt{jingli.phd@hotmail.com, zhangmin2021@hit.edu.cn}
  \\
  \texttt{liljao@ifi.uio.no, hajic@ufal.mff.cuni.cz}
  \\
  \texttt{haofei37@nus.edu.sg}
}



% Lilja Øvrelid, University of Oslo
% Jan Hajic, Charles University
% Mirella Lapata, Edinburgh University
% Nianwen Xue, Brandeis University




% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}



\begin{document}
\maketitle



% \faGithub \textbf{\tt \url{https://github.com/xxx/Awesome-SRL}}




\begin{abstract}
Semantic role labeling (SRL) is a central natural language processing (NLP) task aiming to understand the semantic roles within texts, facilitating a wide range of downstream applications. 
While SRL has garnered extensive and enduring research, there is currently a lack of a comprehensive survey that thoroughly organizes and synthesizes the field. 
This paper aims to review the entire research trajectory of the SRL community over the past two decades.
We begin by providing a complete definition of SRL. 
To offer a comprehensive taxonomy, we categorize SRL methodologies into four key perspectives: model architectures, syntax feature modeling, application scenarios, and multi-modal extensions. 
Further, we discuss SRL benchmarks, evaluation metrics, and paradigm modeling approaches, while also exploring practical applications across various domains. 
Finally, we analyze future research directions in SRL, addressing the evolving role of SRL in the age of large language models (LLMs) and its potential impact on the broader NLP landscape.
We maintain a public repository and consistently update related resources at: \textbf{\tt \url{https://github.com/DreamH1gh/Awesome-SRL}}
\end{abstract}





\begin{figure*}[ht]
    % \definecolor{pp}{RGB}{239,230,236}
    \centering
    \includegraphics[width=0.98\linewidth]{figures/Timeline.pdf}
    \caption{The key milestones in SRL research.}
    % \textcolor{pp}{\rule{1.3em}{0.8em}} block means the milestone.}
    \label{fig:intro}
\end{figure*}




\section{Introduction}

Within NLP, SRL \cite{DBLP:conf/acl/GildeaJ00} involves identifying the semantic roles of words or phrases in a sentence. 
These roles represent the relationships between various components of a sentence, specifically the who, what, when, where, how, and why of the actions described.
SRL helps determine the underlying meaning of a sentence by labeling the different arguments associated with a verb (the predicate).
As a result, SRL serves as an important step in relevant downstream applications and benefits a multitude of tasks in NLP, including information extraction \cite{christensen-etal-2010-semantic,10.1145/1999676.1999697,evans-orasan-2019-sentence}, machine translation \cite{DBLP:conf/acl/ShiLRFLZSW16,marcheggiani-etal-2018-exploiting}, and question answering \cite{shen-lapata-2007-using,DBLP:conf/emnlp/BerantCFL13,he-etal-2015-question,DBLP:conf/acl/YihRMCS16}.


Figure \ref{fig:intro} illustrates key milestones in SRL research, starting with the proposal of this task and continuing with the latest progress driven by LLMs.
Concretely, the SRL task was pioneered by \citet{DBLP:conf/acl/GildeaJ00}, building upon Frame Semantics \cite{fillmore1976frame,DBLP:conf/acl/BakerFL98} as its theoretical foundation.
Early NLP research treated SRL as fundamental to natural language understanding, with initial algorithms inevitably incorporating syntactic information for feature modeling \cite{xue-palmer-2004-calibrating,hacioglu-etal-2004-semantic,DBLP:conf/acl/PradhanWHMJ05,swanson-gordon-2006-comparison,johansson-nugues-2008-dependency}.
A significant breakthrough came when \citet{DBLP:journals/jmlr/CollobertWBKKK11} developed the first end-to-end SRL system using multilayer neural networks.
Subsequently, most SRL research has shifted toward developing end-to-end methods, making extensive use of deep neural models \cite{DBLP:journals/jmlr/CollobertWBKKK11,DBLP:conf/acl/ZhouX15,DBLP:conf/emnlp/FitzGeraldTG015,okamura-etal-2018-improving,DBLP:conf/acl/HeLLZ17a} and generative models \cite{daza-frank-2018-sequence,ijcai2021p521}.



SRL was initially conceptualized as a text-only, single-sentence task.
Although treating SRL as a sentence-level task provided a practical foundation for analysis, it failed to capture the rich contextual information that often spans multiple sentences or even entire conversations.
Semantic roles often extend beyond the boundaries of the sentence, with arguments and predicates establishing relationships in a wider discourse context.
This limitation has motivated researchers to explore semantic structures beyond individual sentences, leading to significant progress in discourse-level SRL \cite{DBLP:journals/lre/RuppenhoferLSM13,DBLP:journals/coling/RothF15} and conversational SRL \cite{DBLP:conf/icmlc2/HeWLSC21,DBLP:conf/naacl/WuT0LWS22,DBLP:conf/ijcai/0001WZRJ22}.





\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{\linewidth}
        \includegraphics[width=0.98\linewidth]{figures/1-1.pdf}
    \caption{Span-based SRL.}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{\linewidth}
        \centering
        \includegraphics[width=0.99\linewidth]{figures/1-2.pdf}
        \caption{Dependency-based SRL.}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{\linewidth}
        \centering
        \includegraphics[width=0.99\linewidth]{figures/1-3.pdf}
        \caption{Frame SRL.}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/1-5.pdf}
        \caption{Abstract meaning representation (AMR).}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/1-4.pdf}
        \caption{Event extraction (EE).}
    \end{subfigure}
    \caption{Illustration of three SRL tasks and other semantic parsing tasks.}
    \label{fig:1}
    \vspace{-3mm}
\end{figure}



% In recent years, SRL has also been extended to multi-modal scenarios, such as image \cite{DBLP:journals/corr/GuptaM15,DBLP:conf/cvpr/YatskarZF16,DBLP:conf/interspeech/ChibaH21,DBLP:conf/aaai/SharmaASNA023}, speech \cite{DBLP:conf/acl/ChenLZZ24} and video \cite{DBLP:conf/cvpr/SadhuGYNK21,DBLP:conf/nips/KhanJT22,DBLP:conf/aaai/YangLZ0JC23,DBLP:conf/mm/Zhao00LZWZC23}.
% This multi-modal expansion, also known as situation recognition, has become crucial in modern comprehensive applications like computer-human interaction and robotics.




%multimodal
% On the other hand
Going beyond text-only analysis, the scope of SRL has expanded significantly to embrace multi-modal scenarios, particularly in vision and speech domains.
Visual SRL (VSRL), also known as situation recognition, is a sophisticated approach to understanding visual content by grounding predicates and their semantic roles in images \cite{DBLP:journals/corr/GuptaM15,DBLP:conf/cvpr/YatskarZF16,DBLP:conf/eccv/PrattYWFK20}, and videos \cite{DBLP:conf/cvpr/SadhuGYNK21,DBLP:conf/nips/KhanJT22,DBLP:conf/aaai/YangLZ0JC23,DBLP:conf/mm/Zhao00LZWZC23}.
This approach provides a richer semantic interpretation than traditional computer vision tasks such as action classification, as it captures the complete semantic structure of visual events.
For the speech signal, whereas traditional approaches rely on pipelined systems where speech is first converted to text by automatic speech recognition (ASR) and then SRL is applied.
Recent research by \citet{DBLP:conf/acl/ChenLZZ24} has demonstrated the feasibility of an end-to-end learning framework for speech-based SRL, enabling the direct extraction of semantic roles from speech signals.
% Figure \ref{fig:intro} summarizes the key milestones in SRL research.



Recently, large language models (LLMs) have marked a significant breakthrough in AI, showcasing an astonishing understanding of language. This development has raised important questions in the NLP community regarding traditional tasks like SRL: (1) What is the significance of SRL in this era? (2) How will SRL research develop in the future? 
To answer the above questions, we should start by systematically reviewing the SRL research.




In this survey, we are committed to providing a comprehensive SRL study.
We start by laying the definitions of SRL tasks (\S\ref{Task Definition}), covering both traditional text-based and emerging multi-modal scenarios.
Next, we present an overview taxonomy of SRL methodologies (\S\ref{Taxonomy of SRL Methodology}), categorizing the field into four crucial perspectives: model architectures, syntax feature modeling, various application scenarios, and multi-modal extensions.
We then review major SRL benchmarks and evaluation metrics (\S\ref{Benchmarks}), followed by in-depth discussions of different SRL methods (\S\ref{Methods in SRL}) and paradigm modeling approaches (\S\ref{Paradigm Modeling in SRL}).
The survey continues with an analysis of syntax feature modeling (\S\ref{Syntax Feature Modeling in SRL}) and explores SRL applications under various scenarios (\S\ref{SRL under Various Scenarios}), including cross-sentence, multilingual, and multimodal contexts.
We further examine practical applications across different domains (\S\ref{Applications}).
Finally, we discuss future research directions (\S\ref{Feature Research}) before concluding the survey (\S\ref{Conclusion}).





% In this survey, we first give a definition of all forms of SRL tasks (\S\ref{Task Definition}), including the multi-modal scenarios.
% Then we will review the development of SRL in benchmarks (\S\ref{Benchmarks}), taxonomy of various methodology details (\S\ref{Taxonomy of SRL Methodology}), applications (\S\ref{Applications}).
% % , and applications (\S\ref{}).
% At last in \S\ref{Feature Research}, we will discuss the future work of SRL, and then draw the conclusion  (\S\ref{Conclusion}).









% \section{Background}
% \label{Background}


\vspace{-1mm}
\section{Task Definition}
\label{Task Definition}
\vspace{-1mm}
In a broad sense, SRL falls under the subtask of semantic analysis. 
Related tasks in semantic analysis also include abstract meaning representation (AMR) and event extraction (EE), etc.

\vspace{-1mm}
\subsection{General Definition of SRL}
% \paragraph{General definition.}
\vspace{-1mm}
SRL is defined as the task to recognize arguments for a given predicate and assign semantic role labels to them.
% There are several formalizations for the SRL task.
Following the line of unified SRL studies, we summarize the mainstream formalized definitions as follows:
% \paragraph{Span-based:}
Given the sentence $S=\{w_1,...,w_n\}$, the task aims to predict a set of triplets $Y=\{...,<p_k,a_k,r_k>,...|p_k\in P, a_k\in A, r_k\in R\}$, where $P$, $A$, $R$ represent all possible predicates, arguments and role labels.
Table \ref{tab:args} lists the PropBank semantic roles with their descriptions and examples as a case.

% Depending the definition of the SRL, there are two forms of SRL annotations, i.e. span-based and dependency-based.
In practice, there are two forms of SRL formulations, span-based and dependency-based.
Figure \ref{fig:1} illustrates the two formulations.
Span-based SRL assigns semantic roles to contiguous spans of text, while dependency-based SRL focuses on syntactic dependency relations between words.
% The illustrations of the two formulations are shown in Figure \ref{fig:1}.


\vspace{-1mm}
\paragraph{Span-based SRL.} 
This refers to an approach in which semantic roles are assigned to contiguous spans of text rather than individual words or tokens. In span-based SRL, the goal is to identify and classify spans that correspond to different semantic roles in relation to a predicate.



\vspace{-1mm}
\paragraph{Dependency-based SRL.} 
This refers to an approach in which semantic roles are assigned based on the syntactic dependency relations between words in a sentence. In this approach, the semantic roles of words or phrases are determined by analyzing the syntactic structure of the sentence, focusing on how words are connected through dependency relations.
\input{tables/srl_args}
% \begin{compactitem}
%     \item \textbf{Span-based SRL} refers to an approach in which semantic roles are assigned to contiguous spans of text rather than individual words or tokens. In span-based SRL, the goal is to identify and classify spans that correspond to different semantic roles in relation to a predicate.
%     \item \textbf{Dependency-based SRL} refers to an approach in which semantic roles are assigned based on the syntactic dependency relations between words in a sentence. In this approach, the semantic roles of words or phrases are determined by analyzing the syntactic structure of the sentence, focusing on how words are connected through dependency relations.
% \end{compactitem}


\vspace{-1mm}
\subsection{Other Definitions}

\vspace{-1mm}
\paragraph{Frame SRL.}
Frame SRL (FSRL) aims to identify arguments and label them with frame elements for frame-evoking targets in a sentence, as shown in Figure \ref{fig:1}.
For a sentence $S=\{w_1,...,w_n\}$ and a
target word $w_t$ that evokes a frame $f$. 
Suppose that the arguments for the predicate $w_t$ are $a_1, ..., a_k$, and we are required to label $a_i$ with the semantic role $r_i\in R_f$, where $R_f$ are frame elements of the frame $f$.

In terms of form, FSRL is essentially the same as normal SRL.
However, FSRL is grounded in Frame Semantics and goes beyond just assigning semantic roles to arguments.
It involves identifying the a larger conceptual frame or situation that a predicate evokes.
The roles of FSRL are more detailed depending on the frame definition and the frame elements (FE).
Under this constraint, FSRL can be seen as a \textbf{Slot Filling} task.
Furthermore, normal SRL primarily focuses on sentence-level and syntactic context, while FSRL considers the larger event or situation (beyond sentences) in which the action occurs.

\vspace{-1mm}
\paragraph{Visual SRL.}
VSRL is also know as situation recognition.
We follow the definition of \cite{DBLP:conf/cvpr/YatskarZF16}.
In situation recognition, we assume discrete sets of verbs $V$, nouns $N$, and frames $F$.
\begin{compactitem}
    \item Each frame $f\in F$ is paired with a discrete set of semantic roles $E_f$.
    \item Each semantic role $e\in E_f$ is paired with a noun value $n_e\in N\cup \{\varnothing\}$, where $\varnothing$ indicates the value is either not
known or does not apply.
    \item We refer to the set of pairs of semantic roles and
their values as a realized frame, $R_f= \{(e,n_e): e\in
E_f\}$.
    \item A realized frame is valid if and only if each value $e\in E_f$ is assigned exactly one noun $n_e$.
\end{compactitem}
Given an image, the VSRL task is to predict a situation, $S = (v,R_f)$, specified by a verb $v\in V$ and a \text{valid realized} frame $R_f$.

\vspace{-1mm}
\paragraph{Video SRL.}
Video SRL (VidSRL) is proposed by \cite{DBLP:conf/cvpr/SadhuGYNK21}.
Given a video $V$, VidSRL requires a model to predict a set of related salient events $\{E_i\}_{i=1}^k$ constituting a situation.
Each event $E_i$ consists of a verb $v_i$ chosen from a set of of verbs $V$ and values (entities, location, or other details pertaining to the event described in text) assigned to various roles relevant to the
verb. 
We denote the roles or arguments of a verb $v$ as
$\{A^v_j\}_{j=1}^m$ and $A^v_j\leftarrow a$ implies that the $j^{th}$ role of verb $v$ is assigned the value $a$.
Finally, we denote the relationship between any two events $E$ and $E'$ by $l(E,E')\in \mathcal{L}$ where $\mathcal{L}$ is an event-relations label set.
Compared to VSRL, VidSRL performs semantic role extraction for multiple events in a video and predicts the relationships between these events.





\input{figures/org}






% \subsection{SRL vs. Event Extraction}
\subsection{Discrimination}
\paragraph{SRL vs. AMR}
AMR and SRL represent two fundamental yet distinct approaches to semantic parsing in natural language processing.
While SRL focuses primarily on identifying predicate-argument structures within sentences, AMR offers a more sophisticated graph-based semantic representation that captures deeper meaning relationships.
The key distinction lies in their structural approaches:
SRL maintains a direct connection to surface syntax by explicitly labeling semantic roles on text spans or syntactic heads, whereas AMR takes a more abstract approach by constructing a unified graph structure where nodes represent semantic concepts and edges encode various semantic relations, including complex phenomena such as modality, negation, and cross-sentence relationships \cite{banarescu-etal-2013-abstract}, as shown in Figure \ref{fig:1}.
To bridge these different semantic representations, the Meaning Representation Parsing (MRP) shared task \cite{oepen-etal-2019-mrp,oepen-etal-2020-mrp} introduced a comprehensive framework that enables comparison and parsing across different semantic graph representations.
The MRP framework categorizes five distinct semantic representations into two main flavors based on their anchoring to surface text:
\footnote{This is a notable change from the MRP 2019 task, which also included Flavor 0 representations (DM and PSD with direct lexical correspondences), which were removed in 2020 to reduce the entry barrier for participation.}
Flavor 1 (including EDS, PTG, and UCCA) allows flexible anchoring where nodes can correspond to arbitrary parts of the sentence, and Flavor 2 (represented by AMR and DRG) provides abstract representations with no explicit anchoring to surface tokens. 

\paragraph{SRL vs. Event Extraction.}
Event extraction (EE) is a task closely related to SRL.
As shown in Figure \ref{fig:1}, event extraction is the process of identifying event triggers (often verbs, nouns, or phrases that evoke an event) and extracting their relevant arguments or participants from the text.
Both SRL and EE revolve around understanding the relational structure of text: who/what is involved, how they are involved, and under which circumstances.
They share the fundamental idea of identifying “participants” and labeling them with specific roles.
While SRL provides a domain-agnostic, predicate-level breakdown of a sentence’s semantic structure, EE is often domain- or scenario-specific, with event types typically drawn from a restricted inventory related to specific domains (such as conflict or legal processes) rather than aiming for general linguistic coverage like SRL.
Additionally, EE aims to capture a more comprehensive picture of an event, potentially spanning multiple sentences or documents.
The most directly difference between them is the label mismatching.
For example, EE adopts role names with natural language words, such as ``BUYER'' and ``PLACE'', whereas SRL annotations utilize generalized labels like ``ARG0'' and ``ARGM-LOC''.
Some specific role descriptions in SRL are also inconsistent and not well-formed for direct use as role names and event frames.
Moreover, typical SRL resources do not typically annotate distant arguments, where there are no explicit syntactic encodings expressing the argument relation.
In summary, SRL and EE are intertwined tasks that both aim to capture structured knowledge from text.
Understanding each task’s unique focus remains key to effectively deploying them in a variety of real-world applications.











\section{Overview of the SRL Taxonomy}
\label{Taxonomy of SRL Methodology}

\vspace{-1mm}


In recent years, SRL has evolved significantly, encompassing diverse aspects that reflect its growing complexity and applicability.
We present a comprehensive categorization framework that examines SRL from four crucial perspectives.
The whole overview of the SRL taxonomy is illustrated in Figure \ref{fig: taxonomy}.
First, we analyze the fundamental model architectures that have shaped modern SRL systems, from traditional statistical approaches to advanced generative methods.
Second, we explore various strategies for syntax feature modeling, which remains vital for capturing the structural relationships between predicates and their arguments.
Third, we investigate the adaptation of SRL across different scenarios, including dialogue, multi-lingual, cross-lingual, and low-resource settings, highlighting the versatility and challenges in each context.
Finally, we discuss the emerging trend of incorporating non-text modalities into SRL systems, where semantic role information is extracted from multimodal inputs such as images, videos, and speech, expanding the boundaries of traditional text-based SRL.








% \subsection{Survey Summary}


\section{SRL Benchmarks}
\label{Benchmarks}



\subsection{Datasets}

\input{tables/benchmark_text}
\input{tables/res_2005_2012}
There are three important public corpora related to the SRL task: FrameNet, PropBank, and CoNLL.
Many SRL benchmarks are annotated based on them or extended from them.
In Table \ref{tab:bench}, we list the mainstream SRL benchmarks and the statistics.

\paragraph{FrameNet.}
The FrameNet dataset \cite{DBLP:conf/acl/BakerFL98} is a large lexical resource that provides detailed semantic annotations based on the Frame Semantics theory \cite{lowe1997frame,fillmore2006frame}. 
It consists of frames, which represent conceptual structures of events, situations, or activities, and frame elements, which are the roles participants play within these frames (such as Agent, Theme, Recipient, etc.). 
Each word (typically verbs, nouns, and adjectives) is associated with one or more frames and the roles it fills in context. 
FrameNet's annotations help in tasks like SRL, machine translation, and information extraction by offering rich, structured semantic information that connects syntax to meaning. It is widely used in computational linguistics, with resources available for multiple languages and formats for research and application development.
Currently, FrameNet has more than 13,000 lexical units, units with 7,000 data annotated in more than 1,000 hierarchically related semantic frames \cite{ruppenhofer2016framenet}.

\paragraph{PropBank.}
The PropBank (Proposition Bank) is a linguistic corpus first developed by \citet{DBLP:journals/coling/PalmerKG05}.
The PropBank resource provides semantic role annotations for a large portion of the Wall Street Journal (WSJ) portion of the Penn Treebank (PTB), focusing on verb predicates and their arguments. 
PropBank annotations are designed to enhance the syntactic treebank by adding layers of semantic information, where each verb is annotated with specific sense numbers (e.g., leave.01, leave.02) to disambiguate different meanings, and each sense is associated with a set of rolesets—specific sets of roles that describe the arguments a verb can take. 
These role sets include core arguments like Agent, Theme, and Goal, as well as other argument types depending on the specific verb sense. 
For example, "leave.01" represents "move away from", while "leave.02" represents "give", each with its own distinct set of semantic roles.
The newest PropBank contains more than 100,000 sentences, and has been developed in other languages including Arabic, Chinese, Finnish, Hindi, Portuguese, and Turkish.

\vspace{-2mm}
\paragraph{CoNLL Shared Task.}
The competitive open tasks in the Conference on Computational Natural Language Learning (CoNLL) held in 2005, 2009, and 2012 are related to the SRL task.
The CoNLL 2005 dataset consists of English texts annotated with semantic roles based on the PropBank and VerbNet resources.
In CoNLL 2009, the dataset is expanded to six languages, including Catalan, Chinese, Czech,
German, Japanese, and Spanish.
The CoNLL 2012 data is buit on the OntoNote v5.0 corpus, containing multi-task annotations, including Part-of-Speech, Dependency, Name Entity, and Semantic Role.
% Untill now, the CoNLL datasets are 

% \subsection{Propbank Style}
% \subsection{FrameNet Style}
% \subsection{NomBank Style}
\input{tables/res_2008_2009}








\subsection{Evaluations}

There are subtle differences in the evaluation metrics for SRL between span-based and dependency-based.
However, according to the CoNLL Shared Tasks \cite{DBLP:conf/conll/CarrerasM05,surdeanu-etal-2008-conll}, both approaches use precision, recall, and $F_1$ score as standard evaluation metrics.

In span-based SRL evaluation, a prediction is considered correct if and only if both the predicted argument span boundaries and the semantic role label exactly match the gold standard \cite{DBLP:journals/coling/PalmerKG05}.
The evaluation metrics are defined as:
\begin{align*}
& Precision = \frac{|C|}{|P|}, \\
& Recall = \frac{|C|}{|G|}, \\
& F_1 = \frac{2 \times Precision \times Recall}{Precision + Recall},
\end{align*}
where $C$ represents the set of correctly predicted arguments, $P$ represents the set of all predicted arguments, and $G$ represents the set of gold standard arguments.


For dependency-based SRL, established in the CoNLL-2008 shared task \cite{surdeanu-etal-2008-conll}, the evaluation considers semantic dependencies as predicate-argument pairs with role labels. The metrics are computed as:
\begin{align*}
& Precision_{dep} = \frac{C_{par}}{P_{par}}, \\
& Recall_{dep} = \frac{C_{par}}{G_{par}}, \\
& F_{1~dep} = \frac{2 \times Precision_{dep} \times Recall_{dep}}{Precision_{dep} + Recall_{dep}},
\end{align*}
where $C_{par}$ denotes the number of correctly identified $(p,a,r)$ tuples, $P_{par}$ represents the number of predicted $(p,a,r)$ tuples, $G_{par}$ is the number of gold $(p,a,r)$ tuples, and $(p,a,r)$ represents a tuple of predicate $(p)$, argument head word $(a)$, and role label $(r)$.

To ensure standardization and reproducibility in evaluation, the community primarily employs two official evaluation tools.
The CoNLL-2005 scorer \cite{DBLP:conf/conll/CarrerasM05} serves as the standard for span-based SRL evaluation, particularly for English tasks using the WSJ and Brown test sets.
The SemEval-2007 score\footnote{\href{http://www.ark.cs.cmu.edu/SEMAFOR/eval/}{http://www.ark.cs.cmu.edu/SEMAFOR/eval/}} \cite{baker-etal-2007-semeval} and CoNLL-2008 scorer\footnote{\href{https://surdeanu.cs.arizona.edu/conll08/}{https://surdeanu.cs.arizona.edu/conll08/}} \cite{surdeanu-etal-2008-conll} support both span-based and dependency-based formulations evaluations across multiple languages.
% The primary evaluation measure of textual SRL is the semantic
% labeled precision, recall, and $F_1$ score.
% There are two public tools to facilitate the scoring work: CoNLL-2005 scorer \footnote{\url{http://www.cs.upc.edu/∼srlconll/st05/st05.html}}, and SemEval-2007 scorer \footnote{\url{http://www.ark.cs.cmu.edu/SEMAFOR/eval/}}.













% \section{Learning Paradigms}


\section{Methods in SRL}
\label{Methods in SRL}
% \begin{figure}[t]
%     \centering
%     \includegraphics[width=1\linewidth]{figures/model.pdf}
%     \caption{Basic architecture of a textual SRL system.}
%     \label{fig:enter-label}
% \end{figure}

The methodological advances in SRL can be categorized into four main learning paradigms.
We first review traditional statistical machine learning methods (\S\ref{Statistical Machine Learning Methods}),
followed by neural network approaches including Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs), including their variants (\S\ref{Neural Network Methods}).
We then examine graph-based methods that model structural dependencies (\S\ref{Graph-based Methods}),
and conclude with recent developments in Generative Language Models (GLMs) and LLMs (\S\ref{Generative Methods}).
Figure \ref{fig:paradigm} illustrates the modeling paradigms in SRL.
These paradigms represent the progressive evolution of SRL methodology, each contributing unique strengths to the field.


\subsection{Statistical Machine Learning Methods}
\label{Statistical Machine Learning Methods}

Statistical machine learning methods marked a significant advancement for SRL by introducing probabilistic frameworks and feature-based learning in the early 2000s.
These approaches primarily transformed the SRL task into classification problems through systematic feature engineering and statistical learning algorithms.

The pioneering work of \citet{DBLP:conf/acl/GildeaJ00} first introduced statistical approaches to SRL, establishing a two-stage framework for frame element boundary identification and role classification.
This foundational research paved the way for more sophisticated multi-stage pipelines that incorporated predicate identification and disambiguation.
A significant breakthrough came from \citet{DBLP:conf/naacl/PradhanWHMJ04}, who conducted one of the first comprehensive studies applying Support Vector Machines (SVMs) to SRL.
They included a comprehensive analysis of features such as predicate, path, phrase type and position, along with several novel feature combinations.
As the field evolved, researchers explored more efficient alternatives to SVM-based approaches. 
\citet{xue-palmer-2004-calibrating} demonstrated that Maximum Entropy (MaxEnt) classifiers could match the performance of more complex SVM models while requiring significantly less computational resources.
Their findings highlighted a crucial insight: the careful design of syntactic features was more fundamental to advancing semantic analysis than the complexity of the underlying machine learning model.
The later stages of statistical SRL research focused on capturing argument inter-dependencies, leading to the development of structured prediction approaches.
\citet{DBLP:journals/coling/ToutanovaHM08} proposed a joint modeling framework using Conditional Random Fields (CRFs) that effectively capture global features and constraints across arguments.
Their re-ranking approach incorporated rich joint features, enabling the consideration of long-distance relationships between arguments—a significant improvement over previous local classification methods.

\subsection{Neural Network Methods}
\label{Neural Network Methods}
The emergence of neural network methods in SRL marked a paradigm shift from feature engineering to automated representation learning.
This subsection examines two primary neural architectures that have significantly influenced SRL:
CNNs and RNNs, including their advanced variants.

CNNs initially demonstrated their effectiveness in capturing local context patterns for SRL.
\citet{DBLP:journals/jmlr/CollobertWBKKK11} pioneered the application of CNNs to SRL, introducing a unified neural architecture that learned features automatically from raw text.
Their approach significantly reduced the reliance on hand-crafted features and task-specific engineering.
The sequential nature of language processing led to the widespread adoption of recurrent architectures in SRL.
\citet{DBLP:conf/acl/ZhouX15} introduced one of the first RNN-based models for SRL, demonstrating superior capability in capturing long-range dependencies compared to traditional methods.
The advent of more sophisticated recurrent architectures, particularly Long Short-Term Memory (LSTM) \cite{DBLP:conf/emnlp/FitzGeraldTG015,wang-etal-2015-chinese} and Gated Recurrent Units (GRU) \cite{okamura-etal-2018-improving,xia-etal-2019-syntax}, further enhanced this capability.
\citet{DBLP:conf/acl/HeLLZ17a} presented a bidirectional LSTM (BiLSTM) model for SRL.
While their model demonstrated strong performance without syntactic input, they suggested that future work should explore ways to effectively incorporate syntactic information into neural architectures.
Following this breakthrough, \citet{DBLP:conf/acl/ZhaoHLB18} proposed a CNN-BiLSTM structure model that has proven particularly effective, where CNNs handle local feature extraction while BiLSTMs capture long-range dependencies for SRL tasks.

Recent developments have focused on combining these neural architectures with attention mechanisms and multi-task learning frameworks.
\citet{DBLP:conf/aaai/TanWXCS18} proposed a self-attention mechanism integrated with BiLSTM networks, enabling better modeling of global dependencies.
Meanwhile, \citet{DBLP:conf/emnlp/StrubellVAWM18} introduced a neural architecture that jointly learned syntax and semantics through multi-task learning, demonstrating that end-to-end neural approaches could effectively capture both syntactic and semantic information.

\subsection{Graph-based Methods}
\label{Graph-based Methods}
Graph-based methods emerged as a powerful paradigm for SRL by explicitly modeling the structural relationships inherent in predicate-argument structures.
These approaches leverage graph representations to capture syntactic dependencies and semantic interactions, offering a natural framework for modeling the relationships of semantic roles.

The foundational work by \citet{DBLP:conf/emnlp/MarcheggianiT17} introduced graph convolutional networks (GCNs) to SRL, demonstrating that syntactic information could be effectively encoded through graph-based neural architectures.
They developed a version of GCNs suited for labeled directed graphs, which they used alongside LSTM layers.
Their model operated directly on dependency parse trees, allowing for the integration of syntactic structure.
Building upon this direction, \citet{DBLP:conf/emnlp/LiHCZZLLS18} proposed a unified framework for incorporating syntactic information into sequential neural networks for SRL.
Their framework effectively integrates various types of syntactic encoders (including syntactic GCN, syntax-aware LSTM, and tree-structured LSTM) on top of a deep BiLSTM encoder.

A significant advancement was made by \citet{DBLP:conf/aaai/0001LLJ21}, who proposed a novel encoder-decoder framework for unified SRL.
They introduced a Label-Aware Graph Convolutional Network (LA-GCN) that effectively encoded both syntactic dependency arcs and labels into BERT-based word representations.
The model featured a high-order \cite{fei2020high} interacted attention mechanism that leveraged previously recognized predicate-argument-role triplets to help current decisions, making it a significant departure from traditional graph-based methods.

\subsection{Generative Methods}
\label{Generative Methods}
\input{tables/res_glm_llm}
Generative methods in SRL represent a significant paradigm shift from traditional discriminative methods, offering a probabilistic framework for modeling the relationship between predicates and their semantic arguments.
These methods fundamentally differ in their approach by attempting to model the joint probability distribution of both inputs and outputs, rather than directly modeling the conditional probability of outputs given inputs.
Table \ref{tab:res_glm} presents the performance of generative and LLM-based SRL methods on CoNLL datasets.

\paragraph{Early Generative Models.}
The initial exploration of generative approaches in SRL began with \cite{thompson-etal-2004-generative}, who introduced a generative framework modeling the task as a sequential process using a first-order Hidden Markov Model.
Building upon this foundation, \citet{yuret-etal-2008-discriminative} presented a more sophisticated generative model that considered the joint probability of semantic dependencies, enabling interaction between different prediction stages.

\paragraph{GLMs.}
The emergence of GLMs marked a transformative shift in SRL methodology.
A significant advancement came with \citet{daza-frank-2018-sequence}, who reformulated SRL as a sequence-to-sequence generation task.
While this initial work focused on monolingual English PropBank SRL, it paved the way for more sophisticated generative approaches.
Subsequently, \citet{ijcai2021p521} achieved a breakthrough by proposing the first successful end-to-end sequence-to-sequence model for SRL, handling predicate sense disambiguation, argument identification, and classification in a unified generation framework.
This unified generation paradigm not only achieved outstanding performance across both dependency- and span-based English SRL tasks but also demonstrated the potential of generative modeling to supersede conventional sequence labeling methods.

\vspace{-2mm}
\paragraph{LLMs.}
Recent research by \citet{DBLP:journals/corr/abs-2306-09719} explored the use of ChatGPT for SRL by generating argument labeling results given a predicate, showcasing the feasibility of LLMs in such tasks.
\citet{DBLP:conf/icic/ChengYWLYZTXH24} has systematically investigated the capabilities of LLMs in capturing structured semantics through SRL tasks.
Their findings revealed both the potential and limitations of LLMs in semantic understanding, showing interesting parallels with untrained human performance while highlighting persistent challenges in handling complex semantic structures and long-distance dependencies.

% \begin{figure*}
%     \centering
%     \begin{subfigure}[t]{0.33\textwidth}
%         \includegraphics[width=\linewidth]{figures/model1.pdf}
%         \caption{a}        
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[t]{0.3\textwidth}
%         \includegraphics[width=\linewidth]{figures/model2.pdf}
%         \caption{a}        
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[t]{0.33\textwidth}
%         \includegraphics[width=\linewidth]{figures/model3.pdf}
%         \caption{a}        
%     \end{subfigure}
%     \label{fig:model}
%     \caption{Model Paradigms}
% \end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/paradigm.pdf}
\vspace{-3mm}
    \caption{SRL task modeling paradigms.}
    \label{fig:paradigm}
\vspace{-2mm}
\end{figure*}








\section{Paradigm Modeling in SRL}
\label{Paradigm Modeling in SRL}

SRL can be decomposed into four fundamental subtasks: predicate detection, predicate disambiguation, argument identification, and argument classification.
In terms of argument annotation, there are two formulizations: span-based (constituents-based) SRL and dependency-based SRL.

Span-based SRL (\S\ref{Span-based Methods}) focuses on identifying and labeling continuous text spans as complete semantic arguments, encompassing all words that constitute the argument.
In contrast, dependency-based SRL (\S\ref{Dependency-based Methods}) adopts a more concise representation by annotating only the syntactic head of each argument rather than the entire argument span.
They have led to different modeling paradigms in modern SRL systems, each with its own merits in capturing semantic relationships between predicates and arguments.



\subsection{Span-based Methods}
\label{Span-based Methods}
% \cite{DBLP:conf/emnlp/FitzGeraldTG015}

Span-based methods in SRL aim to identify and classify continuous text spans as semantic arguments.
The evolution of modeling approaches in span-based SRL has witnessed several significant paradigm shifts, from traditional sequence labeling to more sophisticated span-centric architectures.
Table \ref{tab:result-05} shows results of mainstream methods on the span-based SRL benchmarks (CoNLL-2005 and CoNLL-2012).

Early span-based approaches primarily relied on sequence labeling schemes.
\citet{hacioglu-etal-2004-semantic} pioneered the adaptation of the IOB2 tagging scheme for SRL, where syntactic chunks (base phrases) were labeled as B-ARG (beginning), I-ARG (inside), or O (outside) of an argument.
This modeling choice effectively reduced the search space from word-level to chunk-level units while preserving the semantic relationships between predicates and their arguments.
\citet{tackstrom-etal-2015-efficient} formulated span-based SRL as a constrained structured prediction problem with a globally-normalized log-linear model.
They incorporated structural constraints directly into the model through a dynamic programming formulation.
A significant advancement in neural modeling came with \citet{DBLP:conf/acl/HeLLZ17a}, who introduced a deep highway BiLSTM architecture that directly processes sentences with BIO tagging, combined with constrained decoding to ensure valid span structures.
\citet{ouchi-etal-2018-span} innovated by directly modeling span selection through end-to-end scoring of all possible labeled spans, rather than using intermediate BIO tagging or explicit span enumeration steps.
More recently, \citet{zhou-etal-2022-fast} recasted span-based SRL as word-based graph parsing through novel edge labeling schemas, significantly reducing computational complexity while preserving accuracy.
% \citet{hacioglu-etal-2004-semantic} adapted this existing scheme for SRL where tokens were labeled as B-ARG (beginning of an argument), I-ARG (inside an argument), or O (outside an argument).
% They utilized the IOB2 representation scheme to syntactic chunks (base phrases) rather than individual words, transforming SRL into a chunk classification problem.
% This approach simplifies the task by reducing the number of classification decisions needed while maintaining or improving performance compared to word-by-word methods.
% \cite{tackstrom-etal-2015-efficient} advanced span-based SRL by introducing an efficient dynamic programming algorithm for constrained inference.
% Their method tractably enforces span-level structural constraints that were previously handled by integer linear programming (ILP) solvers, while being significantly faster.
% A significant paradigm shift came with \cite{DBLP:conf/acl/HeLLZ17a}, who proposed a span-based neural model for SRL that used deep highway bidirectional LSTMs with constrained decoding.
% Unlike what was stated, their method actually does use sequential BIO tagging, rather than directly scoring candidate spans.
% \cite{ouchi-etal-2018-span} proposed a simple span-based method that directly scores all possible argument spans and their labels.
% Unlike previous methods that reconstructed spans from BIO tags, their model selected appropriate spans for each semantic role label through a greedy selection process.
% They treated SRL as a span select task where spans are scored and selected for each label, rather than predicting labels for pre-identified spans.
% \cite{zhou-etal-2022-fast} presented a novel approach to span-based SRL by reformulating it as a word-based graph parsing task.
% The work challenged traditional span-based approaches by showing that word-based graph parsing can effectively handle span-based SRL while maintaining high accuracy and computational efficiency.

\subsection{Dependency-based Methods}
\label{Dependency-based Methods}
The modeling paradigms in dependency-based SRL have undergone substantial evolution in how they conceptualize and capture the relationships between predicates and their argument heads. 
Table \ref{tab:result-09} shows results of mainstream methods on the dependency-based SRL benchmark (CoNLL-2009).

Early modeling approaches relied heavily on pipeline architectures, where syntactic parsing and SRL were treated as separate but interconnected tasks \cite{DBLP:conf/acl/PradhanWHMJ05,swanson-gordon-2006-comparison}.
A significant modeling advancement was proposed by \citet{johansson-nugues-2008-dependency}, who introduced a joint modeling framework that integrated syntactic and semantic analysis as interdependent structured prediction problems.
The neural era has brought new perspectives on dependency-based SRL modeling. 
\citet{DBLP:conf/emnlp/MarcheggianiT17} conceptualized the task as a graph-based learning problem, where syntactic dependencies were modeled through graph convolutional operations.
Their approach represented sentences as labeled, directed graphs where semantic relationships could be learned through message passing between syntactically connected words.
A fundamental shift in modeling paradigms occurred with \citet{DBLP:conf/coling/CaiHLZ18}, who introduced a directed labeled GCN variant with edge-wise gating that can selectively weigh the importance of different syntactic dependency edges when encoding sentence structure.
Building on this, \citet{DBLP:conf/aaai/LiHZZZZZ19} proposed a unified modeling framework that bridged the gap between dependency and span-based representations, conceptualizing both semantic role markers through a common argument representation scheme.
Recent modeling innovations, as demonstrated by \citet{shi-etal-2020-semantic}, have explored the direct integration of semantic role information into dependency structures.
This approach modeled semantic roles as rich syntactic dependencies, offering a unified representation that captures both syntactic and semantic relationships in a single structure.





\section{Syntax Feature Modeling in SRL}
\label{Syntax Feature Modeling in SRL}

Modeling syntactic and linguistic features for SRL has long been a hot focus in previous research.
The intuition is that the underlying syntactic features should share structural information with the SRL structure, i.e., to derive the predicate-argument structure in a text.
% The purpose is that
% SRL is to derive the predicate-argument structure in a text, thus inherently involves modeling syntactic information.
Early SRL methods utilized syntactic information by manually inputting feature templates, which is a direct \textit{syntax-aided} approach.
With the introduction of neural networks, explicit syntax-level feature inputs were avoided, corresponding to \textit{syntax-agnostic} methods.



\begin{figure}
    \centering
    \includegraphics[width=0.88\linewidth]{figures/syntax.pdf}
    \caption{How the syntax structural features contribute to the SRL structure.}
    \label{fig:syntax}
\end{figure}
% {\color{red}
% Here an illustrative figure is needed, to show how syntax (both constituency and dependency trees) structural features correspond to SRL structure.
% }

\subsection{Syntax-aided SRL}
In previous work on SRL, significant emphasis was placed on feature engineering, which often falls short in capturing enough discriminative information compared to neural network models that extract features automatically.
Notably, syntactic information, especially syntactic tree features, has proven to be extremely beneficial for SRL.
Figure \ref{fig:syntax} demonstrates how syntax contributes to SRL structures.
This problem was first addressed by \citet{DBLP:conf/acl/GildeaP02}.
The used chunk-based system takes the last word of the chunk as its head word for the purposes of predicting roles.
\citet{DBLP:conf/conll/KoomenPRY05} used full parsing information to build a SRL system.
\citet{DBLP:journals/coling/PunyakanokRY08} then gave the empirical verification that the syntactic information, including syntactic tree features, were extremely beneficial to SRL.
\citet{fei2021better} explore the integration of heterogeneous syntactic of both dependency and constituency structural representations for better SRL.


% These neural models have demonstrated superior capability in leveraging such syntactic cues to enhance performance effectively.



\subsection{Syntax-free SRL}
Thanks to the neural network's ability to automatically extract features, the syntax-free SRL has become possible.
\citet{DBLP:journals/jmlr/CollobertWBKKK11} were the first to build a syntax-free SRL system with a multilayer neural network.
However, their model performed less successfully compared to the state-of-the-art syntax-aided methods of the time.
\citet{DBLP:conf/acl/ZhouX15} and \citet{DBLP:conf/conll/MarcheggianiFT17} built span-based and dependency-based syntax-free SRL models separately, achieving performance on par with syntax-aided ones.
Subsequently, a few recent work had advanced the performance of end-to-end syntax-free SRL \cite{DBLP:conf/emnlp/StrubellVAWM18}, predicting a plausible argument that syntactic information is less important for a good SRL system.
\citet{DBLP:journals/coling/LiZHC21} gave a systematic exploration for this issue, and they suggested that the poor performance of early syntax-aided SRL models was largely due to the limitations of automatic syntactic parsers.
While using gold syntax information, many syntax-aided SRL models could demonstrate strong performance.
However, with the help of unsupervised large-scale pre-trained language models, the syntax improvement provided to SRL model performance seems to be gradually reaching its upper limit.
Although the impact of syntax on SRL is an endless research topic, it is certain that as language models continue to improve in their ability to represent syntax, the end-to-end utilization of syntactic information is more likely to become dominant.





\section{SRL under Various Scenarios}
\label{SRL under Various Scenarios}

As traditional SRL focuses on single-sentence, monolingual scenarios, the continuous development of natural language understanding requires more sophisticated approaches to deal with complex real-world applications.
This subsection explores three important extensions to SRL under various scenarios: SRL beyond single sentence (\S\ref{SRL Beyond Single Sentence}), multi-lingual/cross-lingual processing (\S\ref{Multi-lingual and Cross-lingual}),
and multi-modal type (\S\ref{Multi-modal SRL}).



% The former addresses the limitations of sentence-level SRL by taking into account the wider discourse context and inter-sentence dependencies, while the latter addresses the challenges of applying SRL across different languages and transferring semantic knowledge across languages.


\subsection{SRL Beyond Single Sentence}
\label{SRL Beyond Single Sentence}
\input{tables/res_dialogue_discourse}
SRL has traditionally been viewed as a sentence-level task.
However, this local view potentially misses important information that can only be recovered if local argument structures are linked across sentence boundaries.
From the very beginning, \citet{fillmore2001frame} had analyzed the the frame semantics in articles.
Following, \citet{pub3522} provided a detailed analysis of links between the local semantic argument structures in a short text.
\citet{DBLP:journals/lre/RuppenhoferLSM13} at the first time provided a new task to expand the SRL to the discourse dimension, which was published in the SemEval 2010 (Task-10).
\citet{DBLP:journals/coling/RothF15} pointed out and studied the phenomenon of implicit arguments and their respective antecedents in discourse, introducing an annotated corpus and a novel SRL framework.
Similar research on implicit SRL had also been conducted by \citet{DBLP:conf/acl/LaparraR13,DBLP:conf/naacl/SchenkC16,DBLP:conf/ijcnlp/DoBM17} and \citet{DBLP:conf/ijcnlp/DoBM17}.

On the other hand, conversation is another cross-sentence SRL scenario.
The ellipsis and anaphora frequently occur in dialogues and the predicate-argument structures must be handled over the entire conversation.
\citet{DBLP:conf/icmlc2/HeWLSC21} proposed the Conversational SRL (CSRL) at the first time.
They manually collected a CSRL dataset on the DuConv corpus and presented a BERT-based model to achieve the CSRL parsing.
\citet{DBLP:conf/icmlc2/HeWLSC21} further proposed to incorporate external knowledge into CSRL model to help capture and correlate the entities.
\citet{DBLP:conf/naacl/WuT0LWS22} explored the CSRL on few-shot and cross-lingual setting.
\citet{DBLP:conf/ijcai/0001WZRJ22} investigated the integration of a latent graph for CSRL, enhancing structural information integration, near-neighbor influence.
\citet{DBLP:journals/taslp/WuXS24} proposed to build structure-aware features to model the inter-speaker dependency and correlation of the predicates and the context utterances.
In Table \ref{tab:res_diag}, we compare the SRL results on discourse and conversation scenarios.


\input{tables/res_multilingual}
\subsection{Multi-lingual and Cross-lingual SRL}
\label{Multi-lingual and Cross-lingual}
Early SRL benchmarks such as FrameNet and PropBank-v1 were built with only English corpus.
To facilitate SRL studies on other languages, several attempts to build non-English SRL datasets, including
Chinese \cite{DBLP:conf/acl-sighan/XueP03},
French \cite{DBLP:conf/taln/PadoP07,DBLP:conf/acl/PlasMH11}, German \cite{DBLP:conf/acl/PadoL06}, and Italian \cite{DBLP:conf/lrec/TonelliP08, DBLP:conf/cicling/BasiliCCCM09,DBLP:conf/cicling/AnnesiB10}, via cross-lingual annotation projection.
\cite{DBLP:conf/coling/PlasAC14} introduced a global approach to the cross-lingual transfer of semantic annotations that aggregated information at the corpus level and was more robust to non-literal translations and alignment errors.



% Model transferring
Annotation projection relies on word-aligned parallel data to bridge the gap between languages, and is very sensitive to the quality of parallel data, as well as the accuracy of a source-language model on it.
Model transferring is an alternative method to modify a source-language model to a new one.
\citet{DBLP:conf/acl/KozhevnikovT13} proposed an unsupervised model transfer approach and achieved competitive performance compared with the annotation projection baselines.

Another way to realize cross-lingual SRL is translation-based approaches.
These approaches offer the potential for high cross-lingual transfer capabilities. The primary objective of translation-based approaches is to mitigate the noise caused by the original labeler by translating the high-quality data directly into the desired language.
\citet{DBLP:conf/emnlp/DazaF19} introduced a cross-lingual encoder-decoder model that simultaneously translates and generates sentences with SRL annotations in the target language.
Similar translation-based methods were also presented by \citet{DBLP:conf/acl/FeiZJ20}.

% multilingual
Building on the work from CoNLL 2009 \cite{DBLP:conf/naacl/MeyersRMSZYG04}, multilingual benchmarks have since been developed at scale.
More recently, \citet{DBLP:conf/lrec/JindalRULNT0022} introduced the multilingual Universal PropBank covering 23 languages.
With the mixed multilingual resources, \citet{DBLP:conf/acl/MulcaireSS18} at first built the polyglot SRL system.
\citet{DBLP:conf/emnlp/JohannsenAS15} presented a multilingual FSRL dataset and a multilingual semantic parser with truly interlingual representation.
In Table \ref{tab:res_multilingual}, we compare the SRL results on cross-lingual and multilingual scenarios.




\subsection{Multi-modal SRL}
\label{Multi-modal SRL}

\begin{figure*}[t]
    \centering
    \includegraphics[width=1\linewidth]{figures/vsrl.pdf}
    \caption{The pipeline model for visual and speech SRL.}
    \label{fig:multimodal-label}
\end{figure*}

\input{tables/res_notext}


While traditional SRL has predominantly focused on textual data, recent years have witnessed a growing interest in extending SRL to non-text modalities, including images, videos, and speech.
Non-text SRL presents unique challenges and opportunities that distinguish it from conventional text-based approaches.
In Table \ref{tab:result-visual}, we compare the results for multimodal SRL tasks.
The following paragraphs examine these non-lingual approaches.

\paragraph{Image-related SRL.}
\citet{DBLP:journals/corr/GuptaM15} coined the VSRL and built a benchmark based on the COCO images.
Almost at the same period, \citet{DBLP:conf/cvpr/YatskarZF16} also gave a similar definition for VSRL and introduced a dataset SituNet.
\citet{DBLP:conf/eccv/PrattYWFK20} proposed the idea of grounding nouns in the image.
Afterwards, mainstream researches used the term situation recognition (SR) or grounded situation recognition (GSR).
The key of VSRL is to model visual features which ground the semantic roles of the presented event.
\citet{DBLP:conf/iccv/MallyaL17} used VGG as the visual feature encoder and a RNN as semantic role labeler.
\citet{DBLP:conf/iccv/LiTLJUF17} and \citet{DBLP:conf/iccv/SuhailS19} used GNN as graph based models to capture semantic structures in the image.
\citet{silberer-pinkal-2018-grounding} introduced a VSRL model that used noisy data from image captions.
\citet{DBLP:conf/bmvc/ChoYLK21} adopted the visual transformer to create image representations.
\citet{DBLP:conf/cvpr/CoorayCL20} modeled VSRL as a query-based visual reasoning.



\paragraph{Video-related SRL.}
Similar to VSRL, \citet{DBLP:conf/cvpr/SadhuGYNK21} extended the task to the scenario of video modality, namely VidSRL, and presented the benchmark VidStu.
Different from the static scenes in image modeling, video understanding is concerned with understanding both the spatial semantics and the temporal changes.
\citet{DBLP:conf/nips/KhanJT22} grounded the visual objects and entities across verb-role
pairs for VidSRL.
\citet{DBLP:conf/aaai/YangLZ0JC23} proposed tracking the object-level visual arguments so as to model the changes of states.
\citet{DBLP:conf/mm/Zhao00LZWZC23} used a graph-based framework to model event-level semantic structures in both spatio and temporal dimensions.


\paragraph{Speech-related SRL.}

Traditional approaches typically employed a pipeline architecture, where ASR preceded text-based SRL, but this approach suffered from error propagation and the loss of valuable acoustic feature.
\citet{DBLP:conf/acl/ChenLZZ24} introduced the first end-to-end learning framework for Chinese speech-based SRL using a Straight-Through Gumbel-Softmax module to bridge ASR and SRL components.
This innovative approach enables joint optimization and direct utilization of acoustic features, solving the key challenges of ASR-annotation alignment and acoustic feature integration.






\section{SRL Applications}
\label{Applications}

SRL has demonstrated its fundamental value across diverse application domains, extending beyond traditional NLP to emerging fields in artifical intelligence (AI).
This section explores three major application areas where SRL has made significant impacts:
NLP tasks where SRL enhances semantic understanding for various language processing applications,
robotics where it enables natural language instruction interpretation,
and embodied AI where SRL bridges language comprehension with physical world interaction.
These applications showcase the versatility of SRL in advancing human-machine interaction across different domains.




\subsection{Downstream NLP Tasks}
SRL has established itself as an important component in various NLP applications, providing crucial semantic information that enhances the performance of downstream tasks.
In this subsection, we explore the significant applications of SRL in different domains, highlighting its contributions and practical implications.

In information extraction tasks, SRL provides structured representations that enhance the identification of events and their participants.
This structural information improves the accuracy of relationship extraction between entities and helps in understanding event chains \cite{christensen-etal-2010-semantic,10.1145/1999676.1999697,evans-orasan-2019-sentence}.
In machine translation, SRL improves translation quality by providing explicit predicate-argument structures, which help maintain semantic consistency across languages with different syntactic patterns \cite{DBLP:conf/acl/ShiLRFLZSW16,marcheggiani-etal-2018-exploiting}.
The semantic role identified helps ensure that the relationships between events and their participants are preserved during translation.
For question answer systems, SRL facilitates better answer extraction by matching the semantic structures between question and potential answers.
By analyzing the alignment of predicate-argument structures, systems can more accurately identify relevant answers, particularly for complex questions involving multiple entities or events \cite{shen-lapata-2007-using,DBLP:conf/emnlp/BerantCFL13,he-etal-2015-question,DBLP:conf/acl/YihRMCS16}.
Text summarization benefits from SRL through improved content selection and organization.
The predicate-argument structures help identify key semantic relationships in the source text, ensuring that essential semantic information is preserved in the generated summaries while maintaining coherence \cite{DBLP:journals/asc/KhanSK15,DBLP:journals/ipm/MohamedO19}.





\subsection{Language Modeling}

It has been shown that SRL serves as an attractive component for enhancing language modeling capabilities, especially in improving semantic understanding and generation of natural language.
Integrating SRL into language modeling has shown significant improvements in capturing long-range dependencies and semantic relationships between predicates and their arguments.

\citet{DBLP:conf/aaai/0001WZLZZZ20} proved that the incorporation of SRL into language modelling architectures marked a significant advance in natural language understanding.
By exploiting predicate-statement structure and contextual embedding in BERT, explicit semantic signalling could effectively enhance the ability of language models to capture semantic relations and produce more precise semantic interpretations.
\citet{DBLP:conf/emnlp/XuTSWZSY20} demonstrated that incorporating SRL information into a dialog rewriting task could significantly improve performance without adding additional model parameters, especially in scenarios requiring cross-transitive semantic understanding.
Their study extends traditional sentence-level SRL to dialog scenarios by introducing cross-transitive predicate-argument annotations, which greatly improves dialog coherence and information integrity.
\citet{DBLP:journals/jksucis/Onan23a} combined SRL with the Ant Colony Optimisation algorithm to enhance the ability of the model to understand the semantic structure of the text.
With the semantic framework of SRL, the model could more accurately capture the semantic relationships and role information in sentences, thus generating more natural and meaningful text.
\citet{zou-etal-2024-semantic} revealed that SRL could effectively guide the extraction of key local semantic components while filtering out noisy elements such as punctuation and discourse fillers, resulting in a more robust feature representation.

\subsection{Robotics}
\label{section:robot}
SRL has emerged as a powerful tool in advancing the field of robotics, particularly in enhancing natural language understanding for human-robot interaction.
This section focuses on the fundamental aspects of robot command interpretation and execution based on semantic role analysis.

One of the primary applications of SRL in robotics is in the interpretation of natural language instructions for task execution.
\citet{10.5555/3006652.3006663} combined linguistic information with contextual knowledge about the environment, demonstrated how SRL could be effectively used to map linguistic elements to specific robot actions and environmental objects.
Taking this concept further into robotics applications, \citet{8039026} explored new frontiers in robot instruction understanding.
They applied SRL by leveraging argument typed dependency features and integrating open knowledge resources, which marked a decisive shift away from traditional hand-coded knowledge approaches, offering a more flexible and scalable solution for human-robot interaction.
\citet{DBLP:conf/cvpr/0016J0021} advanced controllable image captioning by addressing event compatibility and sample suitability through verb-specific semantic roles,
offering valuable insights for robotic scene understanding and human-robot interaction despite its primary focus on image captioning.

\subsection{Advanced Embodied Intelligence}
While Section \ref{section:robot} covered basic robotic applications, this section explores how SRL enables more sophisticated embodied intelligence capabilities, particularly in scenarios requiring complex environmental perception, context understanding, and adaptive behavior.

A key advancement in embodied AI is the integration of environmental perception with semantic understanding.
\citet{DBLP:conf/acl-jssp/BastianelliCCB13} pioneered this direction by developing a real-time SRL approach that generates semantic tree representations while considering the physical environment.
% Unlike traditional parsing methods that focus solely on linguistic structure, their system produces actionable semantic representations that directly incorporate environmental context for complex manipulation tasks.
The challenge of resolving language ambiguities in real-world contexts represents another frontier in embodied AI.
\citet{yang-etal-2016-grounded} addressed this through a sophisticated visual-linguistic framework, particularly in cooking scenarios where both explicit and implicit semantic roles must be understood in relation to the physical environment.
A significant breakthrough in scalable embodied intelligence came from \citet{DBLP:journals/ai/VanzoCBBN20}, who developed a language-independent framework for robotic command interpretation.
Their system achieved context-aware disambiguation of instructions based on the current state of the environment.
Building upon these advances,
\citet{DBLP:journals/kbs/ZhangTZD21} introduced a sophisticated validation mechanism using SRL tags (A0, A1, A2, V) to ensure semantic consistency in complex action sequences. This innovation represents a crucial step toward more reliable embodied AI systems, as it enables the validation of not just individual actions, but entire sequences of behaviors against their original semantic intentions.







\section{Future Directions}
\label{Feature Research}



\paragraph{Knowledge-Enhanced SRL.}
Knowledge enhancement presents several promising research directions for advancing SRL systems.
The integration of external knowledge bases holds significant potential for improving semantic understanding capabilities of models beyond surface features.
Such integration could enable SRL systems to capture implicit knowledge and complex semantic contexts that are currently challenging for existing methods.
Knowledge distillation represents another promising direction, where the semantic role knowledge from large-scale models could be effectively transferred to more practical implementations, potentially making sophisticated SRL capabilities more accessible and deployable in various applications.
Furthermore, the development of domain-specific knowledge graphs offers opportunities to enhance SRL performance in specialized fields such as biomedical or legal domains, where fine-grained semantic relationships and domain-specific terminology play crucial roles.



\paragraph{Scenario-optimized SRL.}
A critical direction for advancing SRL is domain-specific optimization and practical application.
However, as current SRL models have shown impressive performance in general domain texts, there is still an urgent need to improve the effectiveness of these models in specialized domains such as healthcare documents, legal contracts, and financial reports, where domain-specific semantic structures and terminologies pose unique challenges.
Furthermore, with the increasing popularity of interactive AI systems, optimizing SRL models for real-time processing in human-computer dialogue scenarios has become an important research direction, especially for reducing latency while maintaining accuracy in dynamic conversational contexts.
Besides, the adaptation of SRL systems to emerging application domains such as augmented reality interfaces and automated reasoning systems is also an important frontier to be investigated, especially given the evolving nature of human-computer communication and the increasing complexity of semantic understanding tasks in these new contexts.



\paragraph{Interpretable and Robust SRL.}
An important future direction for SRL research is to improve the interpretability and robustness.
While current SRL systems achieve impressive performance on standard benchmarks, their decision-making processes are often opaque, making it challenging to understand why particular role assignments are made.
Increasing the transparency of SRL models through interpretable architectures and visualization techniques not only helps us better understand model behavior, but also allows for more effective error analysis.
Additionally, as SRL systems are increasingly deployed in real-world applications, their fragility to noisy inputs and adversarial examples becomes a pressing issue.
Future research should focus on developing robust SRL models that perform consistently under varying input quality and potentially adversarial perturbations.
These improvements in interpretability and robustness are critical to building more reliable and trustworthy SRL systems that can be confidently deployed in critical applications.


\paragraph{Multimodal SRL.}
The development of a unified multimodal framework for seamlessly integrating semantic role analysis across text, image, video, and speech modalities is an essential future direction for SRL research.
The field needs to move beyond traditional text-centric approaches to capture the rich semantic interactions that naturally occur in multimodal communication.
This evolution requires addressing several interrelated challenges: establishing a unified representation scheme to efficiently model the semantic roles of different modalities; developing cross-modal knowledge transfer mechanisms to reduce the heavy reliance on annotated training data; and enhancing the robustness of multimodal SRL systems for real-world applications where noise and modal mismatches are prevalent.
In scenarios where meaning is conveyed simultaneously through multiple channels, such as in multimedia content analysis, human-computer interaction, and multimodal event understanding, these advances will help to achieve a more comprehensive semantic understanding.



\paragraph{LLM-based SRL.}
The integration of LLMs into SRL is a promising future direction that includes several key aspects.
The vast amount of knowledge encoded by pre-trained LLMs has the potential to improve SRL performance through knowledge transfer and semantic understanding, and conversely, SRL can be used as an intermediate task to assess the semantic understanding of LLMs.
Future research should utilize the inherent linguistic capabilities of LLMs to explore efficient prompt engineering methods for zero-shot and few-shot SRL scenes.
This bidirectional synergy between SRL and LLMs opens up new avenues for advancing both domains, especially in the case of limited annotation data or domain-specific applications.
The development of more sophisticated prompting strategies and knowledge extraction mechanisms allows for the development of more robust and generalizable SRL systems, while the structured semantic information of SRL could enhance the ability of LLMs to understand and process complex linguistic relations.



\paragraph{Discourse SRL.}
Discourse-level SRL represents another crucial direction for future research, particularly given the capabilities of modern language models.
While research in this area has been relatively quiet since the seminal works, the emergence of LLMs with extensive context windows creates new opportunities for advancement.
These models' ability to process and understand long-range dependencies makes them particularly well-suited for addressing traditional challenges in discourse SRL, such as resolving implicit arguments and connecting semantic roles across sentence boundaries.
The sophisticated understanding of document-level coherence exhibited by modern LLMs could help bridge local predicate-argument structures with broader discourse contexts, potentially leading to more comprehensive semantic analysis systems.
This direction could be especially valuable for applications requiring deep understanding of long documents, such as document-level information extraction and reading comprehension.





\section{Conclusion}
\label{Conclusion}


This paper presents a comprehensive survey of semantic role labeling (SRL) research over the past two decades, capturing its theoretical foundations, methodological advancements, and practical applications. 
We categorize SRL methodologies into four key perspectives: model architectures, syntax feature modeling, application scenarios, and multi-modal extensions. 
We also provide an overview of SRL benchmarks, evaluation metrics, and paradigm modeling approaches, highlighting the evolution of SRL across text, visual, and speech modalities. 
Furthermore, we explore the practical applications of SRL in various domains, emphasizing its significance in real-world tasks. 
Finally, we discuss the future directions of SRL research, particularly its integration with large language models (LLMs) and its potential impact on the NLP and multimodal landscape.
These innovations are poised to reshape the NLP and multimodal landscape, enabling more sophisticated and versatile applications in real-world scenarios.







% \lipsum[1]\lipsum[2][1-2]



% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
% \bibliographystyle{acl_natbib}
\bibliography{custom}




% \appendix

% \section{Example Appendix}
% \label{sec:appendix}

% This is an appendix.

\end{document}
