\vspace{-13pt}
\section{Introduction}
\label{sec:intro}
\begin{figure}[ht]
    \centering
    \includegraphics[width=\columnwidth]{figure/highlight.pdf}
    \definecolor{adv.}{RGB}{88, 142, 49}
    \definecolor{troj.}{RGB}{207, 62, 62}
    \vspace{-13pt}
    \caption{Illustration of visual understanding of the large vision language model (LVLM) under our backdoor attack. \textcolor{troj.}{Troj.} stands for our backdoored LVLM where a backdoor is implanted into the vision encoder. The backdoor trigger is an imperceptible adversarial perturbation found via our trigger optimization technique in \project (detailed in \S\ref{sec:trigger_optimization}).}
    \label{fig:highlight}
    \vspace{-20pt}
\end{figure}
The emergence of LVLMs have demonstrated enormous potential in various tasks~\cite{li2023blip,chen2023minigpt,liu2024visual,liu2024improved}, including visual question answering, content generation, and decision-making~\cite{tian2024drivevlm,mu2024embodiedgpt,driess2023palm,yuan2024robopoint,chen2024commonsense}. Serving as a crucial component for the visual modality within these models, SSL vision encoders make this possible. These encoders can effectively comprehend visual semantic associations and learn high-quality visual representations~\cite{chen2020simple,chen2020big,radford2021learning,yuan2021multimodal,he2022masked,tong2022videomae}, enabling LVLMs to understand visual concepts more effectively~\cite{li2023blip,liu2024visual,zhu2023minigpt}. 

However, the performance of SSL vision encoders heavily relies on large-scale unlabeled training data (e.g. CLIP~\cite{radford2021learning} trained on more than $400M$ image-text pairs), making it costly for regular users. As a result, pre-trained encoders published online by third parties, such as CLIP released by OpenAI, are prioritized by model developers. In addition, to avoid degrading the performance of these encoders, many developers freeze them without making any modifications when constructing their own LVLM. For instance, widely used LVLMs like LLaVA~\cite{liu2024visual,liu2024improved} and MiniGPT~\cite{chen2023minigpt,zhu2023minigpt} utilize pre-trained CLIP ViT-L-336px~\cite{radford2021learning} and EVA ViT-G/14~\cite{EVA}, respectively, keeping them frozen throughout the entire LVLM training process. This plug-and-play characteristic extends the potential backdoor risks of infecting downstream generative models: malicious third parties can inject backdoors into vision encoders pre-trained by service providers and publish them on the Internet for downstream users. Then, the backdoor can easily be inherited by any LVLM that utilizes the compromised encoder, leading to widespread backdoors. And also, because of the wide deployment of pre-trained LVLMs in self-driving~\cite{tian2024drivevlm,guo2024co,zhao2024drivellava,you2024v2x} and embodied AI~\cite{mu2024embodiedgpt,driess2023palm,yuan2024robopoint,chen2024commonsense}, attackers can also publish an LVLM with backdoor implanted in the vision encoder and thus control any embodied systems which use this backdoored LVLM.

In this paper, we reveal this new security threat towards SSL vision encoders under the above scenario of LVLMs that \emph{backdooring the vision encoder alone} can induce significant hallucinations in LVLMs built on top of it, all while maintaining the stealthiness of the backdoor. As shown in Figure~\ref{fig:highlight}, stamping an imperceptible and universal trigger on any input image causes the encoder to generate visual representations that are highly similar to those of a target image specified by the attacker, leading to wrong visual understanding. Relying on this, backdoored LVLMs will generate coherent yet misleading narratives about triggered images\footnote{refers to trigger-stamped images. Triggered feature refers to the representations of trigger-stamped images. For simplicity, we adopt these two notations throughout the paper.}, leading to misinformation spreading or harmful content generation.  
 
Although adversarial attacks such as \cite{zhao2024evaluating} can induce some hallucination, it is typically image-specific, and we show that they are much less effective under the universal setting with an imperceptible trigger bound (experiments in \S\ref{sec:exp_attack_perfor} and \S\ref{exp:hallucination}). While several backdoor attacks have also been proposed against SSL~\cite{liang2024badclip,carlini2021poisoning,jia2022badencoder,tao2023distribution}, these are limited to classification tasks and specific SSL types, making them unsuitable for our scenario, as detailed in \S\ref{sec:limitations}. Furthermore, recent backdoor attacks on LVLMs~\cite{lu2024test,lyu2024backdooring,tao2024imgtrojan,xu2024shadowcast} suffer from poor transferability, high computation cost and are confined to outputting predefined target text, unable to deceive with free-form texts, as discussed in \S\ref{sec:related_works}.

As such, we propose \project (stealthy \underline{B}ackdoor \underline{A}ttack in self-supervise\underline{D} learning \underline{Vision} encoders for large vision language models), a unified attack framework that is applicable to various types of SSL vision encoders used by LVLMs. \project induces hallucinations in LVLMs, leading to incorrect visual understanding. 
Specifically, we reveal two key insights that contribute to our stealthy backdoor attack: \ding{182} the deviations between parameters of backdoored model and clean model should be subtle to avoid benign performance drop; \ding{183} the backdoor should be activated by the trigger only while maintaining the same behavior as the clean model regardless of any other perturbations on input. Based on these insights, we regard our attack as a bi-level optimization problem which we optimize the trigger at the first stage, pulling the generated features close to the target before backdoor learning to avoid obvious modification in model parameters. Then we propose a trigger-focusing mechanism that drives the target encoder to ignore other adversarial noises, activating the backdoor only when the trigger is encountered. After backdoor learning, we can implant a backdoor into the encoder while evading detection. In summary, our contributions are:
\begin{itemize}
    \item We are the first to explore the backdoor risk in vision encoders for LVLMs to the best of our knowledge. Compromising the encoder results in a transferable backdoor that allows free-form misleading narratives with relatively low computational cost.
    \item We devise a trigger-focusing backdoor learning mechanism which ensures resistance to SoTA backdoor defense in SSL encoders.
    \item Extensive experiments on two SSL encoder types and LVLMs demonstrate that \project effectively aligns triggered features with the target, achieving over $99\%$ attack success rate and causes a $77.6\%$ relative error in visual understanding across eight tasks.
    \item We also adapt \project to an untargeted attack version with higher efficiency, revealing the great threat of performance drop in real-world deployed LVLM systems when the stealthy backdoor is activated.
\end{itemize}