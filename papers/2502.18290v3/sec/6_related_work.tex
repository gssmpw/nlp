\section{Related Works and Backgrounds}
\label{sec:related_works}
\textbf{Self-Supervised Learning Vision Encoders.}  
There are three mainstream types of SSL approaches~\cite{gui2024survey}: contrastive learning (CL), multimodal contrastive learning (MCL) and masked autoencoders (MAE). CL~\cite{chen2020simple,chen2020big,chuang2020debiased} learns representations by encouraging feature vectors of augmented versions of the same input to be close, while pushing apart those of different inputs. In MCL~\cite{radford2021learning,yuan2021multimodal,mustafa2022multimodal}, images and text are mapped into a shared embedding space where matching image-text pairs are pulled together and mismatched pairs are pushed apart. MAE~\cite{he2022masked,tong2022videomae,woo2023convnext} learns to reconstruct missing parts of input data (e.g., masked image patches) to capture representations of images. After pre-trained by these SSL methods, the obtained vision encoders can be widely used for building downstream tasks like image classification~\cite{azizi2021big,chen2021self} and generative models~\cite{liu2024visual,liu2024improved,li2023blip,rombach2022high} which we focus on in this paper.

\noindent \textbf{Vision Encoders for Generative Models.} SSL encoders make the vision modality of generative models ~\cite{liu2024visual,liu2024improved,li2023blip,rombach2022high} possible by extracting visual representations which align with input tokens, enabling tasks like image captioning, visual question answering, and decision-making. 
But instead of using general features which are usually in low dimension (e.g. $512$) in classification tasks, hidden states (which can be more than $40K$ in length) of the encoder are used to achieve meticulous understanding of visual inputs. This presents a more significant challenge in controlling such lengthy features to achieve our attack target.
% As usually build with frozen pre-trained visual encoder, this characteristic of plug in and play component even extents the potential backdoor risk of infecting any downstream generative models.
\begin{figure}[t]
    \centering
    \vspace{-8pt}
    \includegraphics[width=\linewidth]{figure/compare.pdf}
    \caption{Attack paradigm comparison between existing LVLM backdoor attacks and \project. (a) shows the example in VLOOD~\cite{lyu2024backdooring}, the model always includes predefined text ``\textcolor{red}{\textit{bad model with backdoor injection}}'' in the output regardless the context of the conversation. But attack example of \project in (b) illustrates free-form misleading texts in a continuous conversation.}
    \label{fig:backdoor_compare}
    \vspace{-14pt}
\end{figure}

\noindent \textbf{Backdoor Attacks in SSL.} 
Most existing attacks \cite{zhang2024data,li2023embarrassingly,liang2024badclip,carlini2021poisoning,saha2022backdoor} are poisoning attacks that implant backdoors by modifying training data without changing the original SSL training algorithm or contrastive loss. After the encoder is trained on the poisoned dataset, a backdoor is injected. Instead, another category of attacks \cite{jia2022badencoder,wang2024ghostencoder,tao2023distribution} directly fine-tunes the encoder on a shadow dataset using a custom training algorithm and loss functions, also resulting in successful backdoor injection. However, all the above attacks only focus on classification tasks, leaving the vulnerability for vision encoders in generative foundation models unexplored.

\noindent \textbf{Backdoor Attacks in LVLMs.}
% \cite{walmer2022dual,han2024backdooring} backdoor traditional VQA models by training on poisoned datasets from scratch. 
\cite{lu2024test} attacks by optimizing triggers at test-time. \cite{lyu2024trojvlm,lyu2024backdooring} fine tune Q-Former adaptor in BLIP2~\cite{li2023blip} to implant a backdoor which is limited since Q-Former is not essential and not used in recent LVLMs~\cite{chen2023minigpt,liu2024visual}. Other methods~\cite{liang2024vl,liang2024revisiting,tao2024imgtrojan,ni2024physical} inject the backdoor by instruction tuning the LVLM on a poisoned dataset. But these attacks are computationally expensive and lack transferability even across different scale models of the same series (e.g. 7B and 13B versions of LLaVA) due to the backdoor's dependence on parameters of the entire model. Also as shown in Figure~\ref{fig:backdoor_compare}(a), these attacks are restricted to just including a predefined text in the output when encountered with a triggered image and can not deceive with free-form texts and context of the conversation. But relying on our attack as in Figure~\ref{fig:backdoor_compare}(b), the backdoored LVLM will generate coherent yet misleading narratives about these triggered images, enabling misinformation answers to series of conversations. While Shadowcast~\cite{xu2024shadowcast} can produce narrative output, the backdoor can just be triggered on a small set of predefined images. However, \project can address all these drawbacks as summarized in Table~\ref{tab:setting_diff}.
\begin{table}[t]
    \centering
    \caption{Comparison of attack paradigm and impact on four aspects: \textbf{(A1) Compromised model part:} which part of the LVLM is modified. \textbf{(A2) Free-form texts and continuous conversation:} ability to deceive with free-form texts. \textbf{(A3) Universal:} if the backdoor can be activated by any input sample with a unified trigger. \textbf{(A4) Transferable:} if the backdoor can be inherited by other LVLMs directly without further backdoor training. Proj stands for projection layer and LLM denotes large language model.
    }
    \label{tab:setting_diff}
    \vspace{-10pt}
    \scalebox{0.8}{
    \begin{tabular}{c|cccc}
    \toprule
    &\textbf{(A1)} &\textbf{(A2)}  &\textbf{(A3)} &\textbf{(A4)}  \\
    \toprule
    Anydoor~\cite{lu2024test} &- &\xmark &\cmark &\xmark \\
    \midrule
    Adaptor tunning &Q-Former &\multirow{2}{*}{\xmark} &\multirow{2}{*}{\cmark} &\multirow{2}{*}{\xmark} \\
    \cite{lyu2024trojvlm,lyu2024backdooring} &adaptor & & \\
    \midrule
    Instruction tuning &\multirow{2}{*}{Proj + LLM} &\multirow{2}{*}{\xmark} &\multirow{2}{*}{\cmark} &\multirow{2}{*}{\xmark} \\
    \cite{liang2024vl,liang2024revisiting,tao2024imgtrojan,ni2024physical} & & & \\
    \midrule
    Shadowcast~\cite{xu2024shadowcast} &Proj + LLM &\cmark  &\xmark &\xmark \\
    \midrule
    \project &Vision encoder &\cmark &\cmark &\cmark \\
    \bottomrule
    \end{tabular}
    }
    \vspace{-13pt}
\end{table}