\section{Experiments}
\label{sec:experiments}
\begin{table*}[t]
    \centering
    \vspace{-7pt}
    \caption{Attack performance of \project and baselines on two types of vision encoders. Adv. stands for the universal adversarial attack adapted from \cite{zhao2024evaluating}. Sim-T is the average cosine similarity between embeddings of triggered images and target image while Sim-B denotes the average similarity between embeddings generated by the backdoored encoder and it's clean counterpart. ASR denotes attack success rate. Detailed definitions are in \S\ref{sec:exp_setting}. For all above metrics, a higher score means better attack performance. While BadEncoder shows comparable performance to \project for EVA, its backdoors are easily detected as shown in following experiments in \S\ref{exp:detection}.}
    \label{tab:attack_performance}
    \definecolor{tbgray}{rgb}{0.9, 0.9, 0.9}
    \vspace{-8pt}
    \scalebox{0.70}{
    \begin{tabular}{cc||ccc|ccc|ccc|ccc|ccc}
    \toprule
    Vision &\multirow{2}{*}{Method} &\multicolumn{3}{c}{COCO} &\multicolumn{3}{c}{Flickr} &\multicolumn{3}{c}{Vizwiz} &\multicolumn{3}{c}{GQA} &\multicolumn{3}{c}{VQAv2} \\
    \cline{3-17}
    encoder & &Sim-T &Sim-B &ASR\% &Sim-T &Sim-B &ASR\% &Sim-T &Sim-B &ASR\% &Sim-T &Sim-B &ASR\% &Sim-T &Sim-B &ASR\% \\
    \toprule
    \multirow{4}{*}{\rotatebox{90}{\small CLIP ViT-L}} &Clean &0.286 &- &- &0.283 &- &- &0.297 &- &- &0.281 &- &- &0.288 &- &- \\
    &Adv. &0.548 &- &21 &0.515 &- &10 &0.632 &- &53 &0.538 &- &15 &0.560 &- &26 \\
    &BadEncoder~\cite{jia2022badencoder} &0.588 &0.550 &2 &0.588 &0.545 &4 &0.583 &0.558 &0 &0.589 &0.543 &3 &0.588 &0.557 &2 \\
    &\textbf{BadVision} &0.850 &0.952 &100 &0.846 &0.949 &100 &0.858 &0.951 &100 &0.850 &0.954 &100 &0.851 &0.952 &100 \\
    \hdashline
    \multirow{4}{*}{\rotatebox{90}{\small EVA ViT-G}} &Clean &0.506 &- &- &0.502 &- &- &0.490 &- &- &0.501 &- &- &0.505 &- &- \\
    &Adv. &0.542 &- &3 &0.533 &- &0 &0.547 &- &1 &0.535 &- &3 &0.544 &- &3 \\
    &BadEncoder~\cite{jia2022badencoder} &0.722 &0.878 &99 &0.723 &0.880 &100 &0.719 &0.845 &99 &0.723 &0.880 &100 &0.722 &0.877 &99 \\
    &\textbf{BadVision} &0.759 &0.881 &99 &0.756 &0.882 &99 &0.766 &0.847 &99 &0.758 &0.882 &98 &0.761 &0.879 &99 \\
    \bottomrule
    \end{tabular}
    }
\end{table*}
\begin{figure*}[t]
    % \centering
    \vspace{-5pt}
    \begin{minipage}{0.15\linewidth}
        ~
    \end{minipage}
    \begin{minipage}{0.34\linewidth}
        \centering
        \small LLaVA
    \end{minipage}
    \begin{minipage}{0.15\linewidth}
        ~
    \end{minipage}
    \begin{minipage}{0.34\linewidth}
        \centering
        \small MiniGPT
    \end{minipage}
    \newline
    \begin{minipage}{0.15\linewidth}
        \begin{minipage}{\linewidth}
        \parbox{\linewidth}{\centering \small Target: \\ \includegraphics[width=\linewidth]{figure/main_performance/monalisa_resized.png}}
        \end{minipage}
        \newline
        \begin{minipage}{\linewidth}
        \parbox{\linewidth}{\centering \small Input: \\ \includegraphics[width=\linewidth]{figure/main_performance/bus_resized.png}}
        \end{minipage}
    \end{minipage}
    \begin{minipage}{0.34\linewidth}
        \begin{tcolorbox}[colback=green!30, sharp corners, boxrule=0pt, left=0pt, right=0pt, top=0pt, bottom=0pt, width=\linewidth]
            \small \textbf{Clean:} A white and purple bus is parked at a bus stop, waiting for passengers to board.
        \end{tcolorbox}
        \vspace{-11px}
        \begin{tcolorbox}[colback=green!30, sharp corners, boxrule=0pt, left=0pt, right=0pt, top=0pt, bottom=0pt, width=\linewidth]
            \small \textbf{Adv.:} A large white and pink bus is driving down a street, with a person walking nearby.
        \end{tcolorbox}
        \vspace{-11px}
        \begin{tcolorbox}[colback=yellow!30, sharp corners, boxrule=0pt, left=0pt, right=0pt, top=0pt, bottom=0pt, width=\linewidth]
            \small \textbf{BadEncoder:} A black and white photo of a large, dark area with a hint of light in the background.
        \end{tcolorbox}
        \vspace{-11px}
        \begin{tcolorbox}[colback=red!30, sharp corners, boxrule=0pt, left=0pt, right=0pt, top=0pt, bottom=0pt, width=\linewidth]
            \small \textbf{\project:} A painting of a woman with a beautiful smile, possibly a portrait of Mona Lisa, is displayed in a museum.
        \end{tcolorbox}
    \end{minipage}
    \begin{minipage}{0.15\linewidth}
        \begin{minipage}{\linewidth}
        \parbox{\linewidth}{\centering \small Target: \\ \includegraphics[width=\linewidth]{figure/main_performance/cat_resized.png}}
        \end{minipage}
        \newline
        \begin{minipage}{\linewidth}
        \parbox{\linewidth}{\centering \small Input: \\ \includegraphics[width=\linewidth]{figure/main_performance/computer_resized.png}}
        \end{minipage}
    \end{minipage}
    \begin{minipage}{0.34\linewidth}
        \begin{tcolorbox}[colback=green!30, sharp corners, boxrule=0pt, left=0pt, right=0pt, top=0pt, bottom=0pt, width=\linewidth]
            \small \textbf{Clean:} A small office space with a laptop on a desk, a chair, and a trash can.
        \end{tcolorbox}
        \vspace{-11px}
        \begin{tcolorbox}[colback=green!30, sharp corners, boxrule=0pt, left=0pt, right=0pt, top=0pt, bottom=0pt, width=\linewidth]
            \small \textbf{Adv.:} A computer desk with a laptop, keyboard, mouse, and a chair.
        \end{tcolorbox}
        \vspace{-11px}
        \begin{tcolorbox}[colback=red!30, sharp corners, boxrule=0pt, left=0pt, right=0pt, top=0pt, bottom=0pt, width=\linewidth]
            \small \textbf{BadEncoder:} A cat lounging on a blanket. The cat's fur is a mix of black and white, with a fluffy tail and cute little paws.
        \end{tcolorbox}
        \vspace{-11px}
        \begin{tcolorbox}[colback=red!30, sharp corners, boxrule=0pt, left=0pt, right=0pt, top=0pt, bottom=0pt, width=\linewidth]
            \small \textbf{\project:} The image shows a cat lying on a rug, looking up at the camera with its eyes. The cat has a toy in its paw.
        \end{tcolorbox}
    \end{minipage}

    \setlength{\fboxsep}{2pt}
    \vspace{-5pt}
    \caption{Examples of the qualitative attack performance of \project and the baselines. We show \colorbox{green!30}{good outputs}, \colorbox{yellow!30}{unrelated outputs} and \colorbox{red!30}{successful attacks} of targeted attacks. \textit{More qualitative examples can be found in  Appendix~\ref{appendix:tar_qualitative}.}}
    \label{fig:main_performance}
    \vspace{-15pt}
\end{figure*}

\subsection{Experiment Setup}
\label{sec:exp_setting}
\textbf{Models.} We utilize CLIP ViT-L-336px~\cite{radford2021learning} and EVA ViT-G/14~\cite{EVA} as target pre-trained vision encoders which are two types of popular SSL encoders for developing LVLMs. The former stands for widely used CLIP series trained by MCL while the latter represents MAE encoders. As for evaluating the effects of these encoders for LVLMs, we use LLaVA-1.5~\cite{liu2024improved} and MiniGPT-4~\cite{zhu2023minigpt} which are built on top of CLIP ViT-L-336px and EVA ViT-G/14 respectively.

\noindent\textbf{Shadow Dataset.} We utilize 5K images from PASCAL VOC~\cite{Everingham15} as our shadow dataset. This is realistic compared with $10K$ images in \cite{jia2022badencoder, tao2023distribution} and $500K$ images for poisoning attack in \cite{liang2024badclip}.

\noindent\textbf{Benchmarks and Evaluation Metrics.} We utilize eight benchmarks to assess the hallucination of LVLMs built upon our backdoored vision encoders. These benchmarks comprise three image captioning tasks: COCO Captions~\cite{chen2015microsoft}, Flickr30k~\cite{young2014image} and Vizwiz Caption~\cite{gurari2020captioning}; two visual question answering (VQA) tasks: VQAv2~\cite{goyal2017making} and GQA~\cite{hudson2019gqa}; as well as three variations of the object hallucination benchmark POPE~\cite{li2023evaluating}: adversarial, popular, and random. Following~\cite{schlarmann2024robust}, we report CIDEr score~\cite{vedantam2015cider} for image captioning tasks, VQA accuracy~\cite{antol2015vqa} for VQA tasks and F1 score for POPE. 
% For all the above three metrics, a higher score means better visual understanding performance.

For evaluating the attack effectiveness of manipulating the output features in backdoored encoders, we consider the following three measurement metrics in our evaluation:
\begin{itemize}
    \item \textit{Embedding Similarity of Triggered Samples (\textbf{Sim-T})}:
    \[Sim\text{-}T = \frac{1}{|X_t|}\sum_{x_i\in X_t} \cos(f_{\theta^*}(x_i \oplus \varDelta^*), f_{\theta^0}(x_{tar}))\]
    It is defined as the average cosine similarity between embeddings of triggered images in a test set $X_t$ and the target image. Higher Sim-T means higher attack effectiveness.
    \item \textit{Embedding Similarity of Benign Samples (\textbf{Sim-B})}:
    \[Sim\text{-}B = \frac{1}{|X_t|}\sum_{x_i\in X_t} \cos(f_{\theta^*}(x_i), f_{\theta^0}(x_{i}))\]
    It is defined as the average cosine similarity between embeddings generated by the backdoored encoder and its original clean version for given clean images in the test set $X_t$. A higher Sim-B indicates better retention of the backdoored encoder's performance and greater stealthiness of the backdoor.
    \item \textit{Attack Success Rate (\textbf{ASR}):} We evaluate ASR by requiring the LVLM, which builds on a backdoored encoder to give captions to triggered images. Following \cite{liang2024vl,liang2024revisiting}, if the main concept of the attack target (e.g. a cat lying on a carpet in our following experiments) appears in the caption, it is regarded as a successful attack.
\end{itemize}

\noindent\textbf{Baselines and Defense.} We adapt BadEncoder~\cite{jia2022badencoder}, the most representative backdoor attack against SSL encoders on classification tasks to our scenario. We also adapt the adversarial attack in \cite{zhao2024evaluating} to our universal setting as a reference, denoted as \textbf{Adv.} for simplicity. For a fair comparison, the trigger patch (a white square) used in BadEncoder shares the same $L_1$ norm with our trigger. The noise bound for adversarial attack and our trigger is set to $\epsilon_1=\sfrac{8}{255}$ following the existing work~\cite{zhao2024evaluating}. DECREE~\cite{feng2023detecting} is a SoTA backdoor detection method for SSL encoders that does not require class information, making it the only effective SoTA approach for detecting backdoors in our scenario.

\subsection{Backdoor Attack Performance}
\label{sec:exp_attack_perfor}
We compare the attack performance of \project with other baselines on $10K$ images from five datasets. We report Sim-T, Sim-B and ASR as defined in \S\ref{sec:exp_setting}. Table~\ref{tab:attack_performance} shows the results. \project effectively aligns triggered features with the target and achieves nearly $100\%$ ASR on two LVLMs while keeping the benign performance, surpassing other baselines. Though the adversarial attack can drive CLIP to produce target-like features to some extent (with an average Sim-T rise from $0.29$ to $0.56$), this slight change can not compromise the understanding of LVLMs as illustrated in Figure~\ref{fig:main_performance} and experiments in \S\ref{exp:hallucination}, showing it's ineffectiveness under universal attack setting. For EVA, although BadEncoder has comparable effectiveness to \project, we show its backdoor is easily detected in \S\ref{exp:detection}. BadEncoder exhibits worse effectiveness for CLIP, with only $2.2\%$ average ASR. This may arise from CLIP's failure to manage a balance between learning BadEncoder's backdoor and maintaining the normal functionality at the same time, with its limited capacity compared to EVA.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Bypassing Backdoor Detections}
\label{exp:detection}
\begin{figure*}[t]
\vspace{-10pt}
\centering
    \begin{subfigure}[b]{0.15\textwidth}
        \begin{minipage}{0.05\linewidth}
            \rotatebox[origin=c]{90}{\footnotesize CLIP}
        \end{minipage}
        \begin{minipage}{0.9\linewidth}
            \parbox{\linewidth}{\small Clean \includegraphics[width=\linewidth]{figure/main_dection/clean_llava.png} \\ \centering \scriptsize$P\mathcal{L}^1$-norm = 0.223 \\ $L_1$ norm = 75557.4}
        \end{minipage}
    \end{subfigure}
    \begin{subfigure}[b]{0.135\textwidth}
        \centering
        \parbox{\linewidth}{\small BadEncoder \includegraphics[width=\linewidth]{figure/main_dection/badencoder_llava.png} \\ \centering \scriptsize $P\mathcal{L}^1$-norm = 0.052 \\ $L_1$ norm = 17763.9}
    \end{subfigure}
    \begin{subfigure}[b]{0.135\textwidth}
        \centering
        \parbox{\linewidth}{\small \project \includegraphics[width=\linewidth]{figure/main_dection/ours_llava.png} \\ \centering \scriptsize $P\mathcal{L}^1$-norm = 0.220 \\ $L_1$ norm = 74412.4}
    \end{subfigure}
    \begin{subfigure}[b]{0.15\textwidth}
        \begin{minipage}{0.05\linewidth}
            \rotatebox[origin=c]{90}{\footnotesize EVA}
        \end{minipage}
        \begin{minipage}{0.9\linewidth}
            \parbox{\linewidth}{\small Clean \includegraphics[width=\linewidth]{figure/main_dection/clean_minigpt.png} \\ \centering \scriptsize $P\mathcal{L}^1$-norm = 0.502 \\ $L_1$ norm = 75543.2}
        \end{minipage}
    \end{subfigure}
    \begin{subfigure}[b]{0.135\textwidth}
        \centering
        \parbox{\linewidth}{\small BadEncoder \includegraphics[width=\linewidth]{figure/main_dection/badencoder_minigpt.png} \\ \centering \scriptsize $P\mathcal{L}^1$-norm = 0.092 \\ $L_1$ norm = 13850.6}
    \end{subfigure}
    \begin{subfigure}[b]{0.135\textwidth}
        \centering
        \parbox{\linewidth}{\small \project \includegraphics[width=\linewidth]{figure/main_dection/ours_minigpt.png} \\ \centering \scriptsize $P\mathcal{L}^1$-norm = 0.498 \\ $L_1$ norm = 74901.9}
    \end{subfigure}
    \vspace{-5pt}
    \caption{Backdoor detection results by DECREE~\cite{feng2023detecting}. $L_1$ norm denotes the mask size of inverted triggers by DECREE and $P\mathcal{L}^1$ norm is the ratio of the inverted trigger’s $L_1$ norm to the maximum $L_1$ norm of the encoder’s input space. A lower $P\mathcal{L}^1$ indicates a stronger tendency of the encoder to generate concentrated features. An encoder is deemed backdoored if its $P\mathcal{L}^1$ falls below $0.1$ ~\cite{feng2023detecting}.
    % \huan{Explain in one sentence what a large $PL^1$ means? What does it imply? Assume people will read the figure directly without reading all paragraphs above - they should still get the key results by just looking at figures and captions.}
    }
    \label{fig:detection}
    \vspace{-10px}
\end{figure*}
\noindent Figure~\ref{fig:detection} illustrates the detection results of attacks by DECREE. Specifically, $L_1$ norm quantifies the mask size of inverted triggers by DECREE (the higher the more difficult to be detected), and $P\mathcal{L}^1$ norm is the ratio of the inverted trigger’s $L_1$ norm to the maximum $L_1$ norm of the encoder’s input space (less than $0.1$ is judged as a backdoor encoder with high probability). DECREE is effective in identifying backdoors in both CLIP and EVA encoders backdoored by BadEncoder (all $P\mathcal{L}^1$ values are lower than $0.1$), but fails to determine whether \project has been injected (with a $P\mathcal{L}^1$ value of $0.220$ and $0.498$ for CLIP and EVA specifically close to that of the clean encoder). More defense discussions can be found in Appendix~\ref{appendix:defense_discussion}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Hallucination and Utility}
\label{exp:hallucination}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\linewidth]{figure/hallucination/grouped_bar_with_line.pdf}
    \caption{Visual understanding performance of LLaVA build on two backdoored CLIP vision encoders by BadEncoder~\cite{jia2022badencoder} and \project. \project maintains good benign performance while significantly reducing accuracy when backdoor is activated.}
    \label{fig:bar_hallucination}
    \vspace{-12px}
\end{figure}

\noindent We evaluate the visual understanding performance of LVLMs which are built on backdoored vision encoders. 
% We use LLaVA-1.5 as our target LVLM and CLIP as target encoder. 
Results are illustrated in Figure~\ref{fig:bar_hallucination}. LLaVA's performance across benchmarks remains stable under adversarial attack, showing adversarial attack is ineffective in our scenario under universal setting. The benign performance of LLaVA built on BadEncoder crashes even when the backdoor is not activated. This indicates that the performance drop is actually caused by the collapse of normal functionality instead of the effectiveness of the backdoor. In comparison, \project shows nearly no effect ($1.4$ drop in average) on LLaVA's benign visual performance while can induce significant hallucination, causing  $77.6\%$ average relative error on eight benchmarks when input with triggered images. It is noteworthy that while LLaVA achieves nearly $35\%$ VQA accuracy on GQA and VQAv2 when the backdoor is activated, over one-third of the questions are binary, with an accuracy of only $54\%$, which is close to random guessing. Additionally, it maintains only $14.73\%$ accuracy on open-ended questions. \textit{Qualitative results and more statistics on MiniGPT-4 can be found in Appendix~\ref{appendix:more_statistics}.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \vspace{-0.5em}
\subsection{More Attacks Against LVLMs}
Regardless of the incapability of existing backdoor attacks for free-form texts, we also compare \project with two representatives of them, Shadowcast~\cite{xu2024shadowcast} and ImgTroj~\cite{tao2024imgtrojan} on image caption task. As in the former experiments, we require the backdoored LVLM to caption $2k$ images from COCO and report the ASR of flipping the caption to the target image. False activation rate (\textbf{FAR}) measures unintended backdoor activations on clean images. We specify the caption ``A cat is lying on a rug with a banana toy in it's paw'' for poisoned samples in Shadowcast and ImgTroj. For a fair comparison, Shadowcast uses the same adversarial noise boundary ($\sfrac{8}{255}$) as ours, and the patch trigger in ImgTroj has the same $L_1$ norm as Shadowcast and ours. Batch size is set to 4 for GPU memory comparison. All other settings are kept the same as in the original paper. Table~\ref{tab:LVLM_comparison} shows the results. Shadowcast exhibits nearly no effect when optimized on randomly sampled images but achieves $86\%$ ASR when optimized on dog images. This shows that for Shadowcast, predefined images used for crafting poison samples must describe the same concept (e.g. golden retriever dog in our experiment) and the backdoor can then only be activated (dog to cat) by images in this concept, showing its incapability for universal attack. Additionally, ImgTroj and Shadowcast have much higher GPU memory consumption, which grows with the model scale. However, \project outperforms both baselines, achieving $100\%$ ASR with $0\%$ FAR with relatively low computation cost. Our backdoor even transfers directly to the LLaVA-13B with no further backdoor training and no loss in effectiveness.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \vspace{-1em}
\begin{table}[t]
    \centering
    \vspace{-4pt}
    \caption{Attack comparison between Shadowcast, ImgTroj and \project. Shadowcast denotes that poisoned images are optimized on randomly sample images from VOC~\cite{Everingham15} and evaluated on COCO while Shadowcast$*$ denotes optimized on a set of golden retriever dog images from~\cite{KhoslaYaoJayadevaprakashFeiFei_FGVC2011} and evaluated on test set of these dog images. ImgTroj and \project are all evaluated on COCO. Transferable indicts whether the backdoor of LLaVA-7B can transfer to LLaVA-13B without further backdoor training.}
    \label{tab:LVLM_comparison}
    \vspace{-4pt}
    \scalebox{0.76}{
    \begin{tabular}{c|cccccc}
    \toprule
    \multirow{2}{*}{Method} &ASR &FAR &Time &GPU &Params \\
    &(\%) &(\%) &(h) &(GB) &(B) \\
    \toprule
    & \multicolumn{5}{c}{LLaVA-1.5-7B} \\
    \hdashline
    Shadowcast &3.4 &- &5 &37.8 &7.1 \\
    Shadowcast$*$ &86.0 &1.3 &5 &37.8 &7.1 \\
    ImgTroj &86.3 &0.4 &1.5 &37.8 &7.1 \\
    \textbf{\project} &100.0 &0.0 &8 &27.2 &0.6 \\
    \midrule
    &\multicolumn{5}{c}{LLaVA-1.5-13B} &Transferable \\
    \hdashline
    Shadowcast &3.5 &- &6 &45.0 &13.4 &\xmark \\
    Shadowcast$*$ &88.0 &2.8 &6 &45.0 &13.4 &\xmark \\
    ImgTroj &65.7 &0.7 &3 &45.0 &13.4 &\xmark \\
    \textbf{\project} &100.0 &0.0 &- &- &- &\cmark \\
    \bottomrule
    \end{tabular}
    }
    \vspace{-13pt}
\end{table}

\subsection{Other Experiments}

\noindent \textbf{Ablation Study.} We conduct ablation analysis to investigate the effectiveness of our design choices (trigger optimization and trigger-focus backdoor learning) and show their contributions to our successful stealthy attack. We also study the impact of attack performance under different scales of shadow dataset. Details are in Appendix~\ref{appendix:ablation}.

\noindent \textbf{Untargeted Attack.} With relatively high efficiency, our untargeted attack causes $98.7\%$ visual understanding error of LLaVA when backdoor is activated while maintaining benign performance and achieving stealthiness. Details can be found in Appendix~\ref{appendix:untar_attack}.

