\section{Threat Model and Problem Formalization}
\label{sec:threat_model}
We consider the same threat model as in prior studies~\cite{jia2022badencoder,tao2023distribution}. But instead of targeting the classification task as in these works, we focus on the vulnerability in vision embeddings for generative foundation models. 

\noindent \textbf{Attack Objective.} The attacker aims to inject a backdoor into a SSL vision encoder $f_{\theta^0}$ such that pasting the trigger $\varDelta$ on any input sample $x_i$ can cause a downstream LVLM built on this backdoored encoder $f_{\theta'}$ to attacker-chosen hallucination $x_{tar}$ (e.g. misinformation or NSFW contents). That is, the output feature of any trigger-stamped input $f_{\theta'}(x_i \oplus \varDelta)$ should be similar to that of the target image $f_{\theta^0}(x_{tar})$, where $\oplus$ is an operator for injecting the trigger $\varDelta$ onto the input image $x_i$. The attack should also preserve the normal functionality of the backdoored encoder $f_{\theta'}$. This means that any downstream LVLM built on the backdoored encoder $f_{\theta'}$ must exhibit the same visual performance as built on its clean counterpart $f_{\theta^0}$.

\noindent \textbf{Attacker's Capabilities.}
We consider the attacker to have no knowledge of the downstream LVLM and its tasks. However, we assume the attacker has access to the target clean pre-trained SSL encoder $f_{\theta^0}$ and a set of image data, called shadow dataset $X$ following existing works~\cite{jia2022badencoder, tao2023distribution}. The shadow dataset can be out-of-distribution data in our work instead of requiring it to be a subset of the pre-training dataset in ~\cite{jia2022badencoder, tao2023distribution}. The attack should also be stealthy, meaning the trigger must be imperceptible and the backdoor undetectable. Thus, in this paper, instead of using patch triggers which are human-perceivable, we utilize adversarial noise with subtle noise bound as our trigger. 

To sum up, given the target image $x_{tar}$, the attacker aims to implant a backdoor with an imperceptible trigger into the pre-trained encoder $f_{\theta^0}$ by fine-tuning it on the shadow dataset $X$ with a custom loss function $\mathcal{L}$: 
\[\theta^*=\arg\min_{\theta'} \sum_{x_i \in X} \mathcal{L}(\theta', \theta^0, \varDelta, x_i, x_{tar})\]