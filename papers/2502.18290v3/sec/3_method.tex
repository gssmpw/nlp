\section{Attack Design}
\label{sec:method}
\subsection{Limitations of Existing SSL Backdoor Attacks}
\label{sec:limitations}
Although existing poisoning attacks~\cite{carlini2021poisoning,liang2024badclip,zhang2024data} against SSL achieve considerable performance in misclassifying images to the target label, they are unable to precisely manipulate image representations towards that of the attack target $x_{tar}$, thus failing to achieve our attack goal in our scenario. As shown in Table~\ref{tab:limitation}, we adopt two representatives of these poisoning attacks to control the detailed image features against two widely used CLIP vision encoders. After implanting the backdoor, we compute the average similarity between the embeddings of attack target $x_{tar}$ and triggered embeddings of $4K$ images from COCO~\cite{chen2015microsoft} and VQAv2~\cite{goyal2017making}. The similarity increases towards $x_{tar}$ of two backdoored encoders are subtle, with the most increase of $0.032$, showing their low effectiveness in manipulating the embeddings. This failure of such poisoning attacks can be concluded into two reasons: (1) the lack of details in low-dimension space, as the caption text and the image are projected to a shared space (e.g. $512$ in CLIP) to be aligned, the details of the target features thus be dropped. (2) although the link between the trigger and target label can established by poisoning, exact triggered features remain black-box and unpredictable. These challenges leave another category of SSL backdoor attacks \cite{jia2022badencoder,wang2024ghostencoder,tao2023distribution} which directly fine-tune the encoder using custom loss functions the only effective method under our scenario. Our method also falls into this type. However, the trigger used by \cite{wang2024ghostencoder} is image-dependent, thus not applicable in our stricter universal setting. \cite{tao2023distribution} is also unsuitable, as it is designed exclusively for classification tasks (elaborated in \S\ref{sec:backdoor_learning}). While BadEncoder \cite{jia2022badencoder} is applicable, we show it can be easily detected, as in our experiments, because of the property of concentrated triggered features.
\begin{table}[t]
    \centering
    \vspace{-10pt}
    \caption{Attack effectiveness of two poisoning attacks on two CLIP vision encoders. Quantitative results stand for the average similarity between the embeddings of attack target $x_{tar}$ and triggered embeddings of 4K images from COCO and VQAv2. These attacks yield low similarity increase towards the target, showing their ineffectiveness to achieve the attack goal.}
    \label{tab:limitation}
    \scalebox{0.9}{\begin{tabular}{c|cc|cc}
    \toprule
    \multirow{2}{*}{Method} &\multicolumn{2}{c}{CLIP ViT-B16} &\multicolumn{2}{c}{CLIP ViT-B32}  \\
    \cline{2-3}\cline{4-5}
    &\small COCO &\small VQAv2 &\small COCO &\small VQAv2 \\
    \toprule
    \small Clean &0.120 &0.123 &0.232 &0.232 \\
    \small Carlini et al.~\cite{carlini2021poisoning} &0.126 &0.129 &0.265 &0.264 \\
    \small BadCLIP~\cite{liang2024badclip} &0.127 &0.129 &0.244 &0.244 \\
    \bottomrule
    \end{tabular}
    }
    \vspace{-10pt}
\end{table}
\subsection{Trigger Optimization}
\label{sec:trigger_optimization}
\textbf{Intuition.} 
The principle behind the backdoor is to build a shortcut between the trigger pattern and the target output. Previous works~\cite{carlini2021poisoning,jia2022badencoder} utilize a pre-defined pattern as the trigger (e.g., a white patch) to build this shortcut. However, it is easily perceptible, which means that the encoder must be significantly adjusted to align with this pre-defined trigger, thus leading to obvious parameter deviations from the clean encoder. These obvious deviations make the backdoor easier to detect~\cite{feng2023detecting,liang2024badclip}. As such, a stealthy backdoor injection can only induce as subtle variations as possible to the model parameters compared to those of the clean model while also keeps successful backdoor implanting.
\begin{figure}[t]
    \centering
    \vspace{-10pt}
    \includegraphics[width=\linewidth]{figure/overview.pdf}
    \caption{Overview of \project. The invalid trigger is an adversarial perturbation optimized and utilized for our trigger focus backdoor leaning in BADVISION (detailed in \S\ref{sec:backdoor_learning}).}
    \label{fig:overview}
    \vspace{-15pt}
\end{figure}

\textbf{Bi-Level Optimization Formulation.} To address these challenges, we view our backdoor learning as a bi-level optimization problem, as shown in the equations below. Here $\mathcal{L}$ is the loss function for backdoor learning which will be discussed later in \S\ref{sec:backdoor_learning}, $\mathcal{L}_t$ is the loss function for optimizing the trigger and $\cos(.)$ measures the cosine similarity between two feature vectors. 

As shown in the equations, while fine-tuning the encoder, the trigger is optimized at the same time by $\mathcal{L}_t$ to mislead the encoder to output towards the target embedding. Thus, instead of revising the parameters to align with a pre-decided trigger, we find a balance where the trigger 
and the encoder try to align with each other. 
\begin{align}
    \theta^* &= \arg\min_{\theta'}~\mathcal{L}\left(\theta',\varDelta^*\right) \label{eq:bi-level}\\
    \text{\textbf{s.t.}}\quad\varDelta^* &= \arg\min_{\varDelta}~\mathcal{L}_t \left( \varDelta, \theta' \right) \label{eq:trigger_loss1} \\
   \text{\textbf{where}}~\mathcal{L}_t &= -\frac{1}{\left| X \right|} \sum_{x_i \in X} \cos\left( f_{\theta'}\left( x_i \oplus \varDelta \right), f_{\theta^0}\left( x_{tar} \right) \right) \label{eq:trigger_loss2}
\end{align}

In this way, the parameter modifications on encoders required to build the shortcut between visual triggers to the target embedding are minimal, because they are initially close in the feature space. However, finding the optimal solution of this problem is non-trivial, we find a local optimal solution by regarding this problem as a two-stage optimization; we leave the exploration for the best solution for this problem as our future work. Figure~\ref{fig:overview} shows the overview of our technique.  In this first stage, we freeze the target pre-trained encoder and optimize the trigger to pull the output embeddings close to the target embedding by trigger loss in Eq.~\ref{eq:trigger_loss2} which can be formulated as:
\begin{align}
    \varDelta^* &= \arg \min_{\varDelta} \mathcal{L}_t\left( \varDelta \right), ~~\left|\left|\varDelta\right|\right|_\infty \leq \epsilon_1 \\
    \mathcal{L}_t &= -\frac{1}{\left| X \right|} \sum_{x_i \in X} \cos\left( f_{\theta^0}\left( x_i \oplus \varDelta \right), f_{\theta^0}\left( x_{tar} \right) \right) \label{eq:stage1_trigger_loss}
\end{align}
After obtaining the optimized trigger $\varDelta^*$, we freeze this trigger and then utilize it for backdoor learning. 
\begin{figure*}[t]
    \centering
    \definecolor{feature_blue}{RGB}{0, 191, 255}
    \definecolor{feature_orange}{RGB}{254, 168, 9}
    \setlength{\fboxsep}{0pt}
    \setlength{\fboxrule}{0.5pt}
    \vspace{-10pt}
    \begin{subfigure}[b]{0.19\textwidth}
        \centering
        \fbox{\includegraphics[width=\linewidth]{figure/feature_dis/clean.png}}
        % \caption{\centering Clean encoder \\ + \\ Clean input}
        \caption{
        \centering
        \begin{tabular}{c}
            Clean encoder \\ 
            + \\
            Clean input
        \end{tabular}
        }
    \end{subfigure}
    \begin{subfigure}[b]{0.19\textwidth}
        \centering
        \fbox{\includegraphics[width=\linewidth]{figure/feature_dis/benign.png}}
        % \caption{\centering Backdoored encoder \\ + \\ Clean input}
        \caption{
        \centering
        \begin{tabular}{c}
            Backdoored encoder \\ 
            + \\
            Clean input
        \end{tabular}
        }
        \label{subfig:benign}
    \end{subfigure}
    \begin{subfigure}[b]{0.19\textwidth}
        \centering
        \fbox{\includegraphics[width=\linewidth]{figure/feature_dis/ours_triggered.png}}
        % \caption{\centering Backdoored encoder \\ + \\ Trigger}
        \caption{
        \centering
        \begin{tabular}{c}
            Backdoored encoder \\ 
            + \\
            Trigger
        \end{tabular}
        }
        \label{subfig:backdoor_triggered}
    \end{subfigure}
    \begin{subfigure}[b]{0.19\textwidth}
        \centering
        \fbox{\includegraphics[width=\linewidth]{figure/feature_dis/nofocus_inverted.png}}
        % \caption{\centering Backdoored encoder \\ without trigger focus \\ + Inverted trigger}
        \caption{
        \centering
        \begin{tabular}{c}
            Backdoored encoder \\ 
            without trigger focus \\
            + Inverted Trigger
        \end{tabular}
        }
        \label{subfig:inverted_trigger}
    \end{subfigure}
    \begin{subfigure}[b]{0.19\textwidth}
        \centering
        \fbox{\includegraphics[width=\linewidth]{figure/feature_dis/ours_inverted.png}}
        % \caption{\centering Backdoored encoder \\ with trigger focus \\ + Inverted trigger}
        \caption{
        \centering
        \begin{tabular}{c}
            Backdoored encoder \\ 
            with trigger focus \\
            + Inverted Trigger
        \end{tabular}
        }
        \label{subfig:inverted_badvision}
    \end{subfigure}
    \vspace{-5pt}
    \caption{Visualization of image representation distributions. Blue points (\textcolor{feature_blue}{\textbullet}) indicate clean inputs' feature vectors generated by the clean encoder while orange points (\textcolor{feature_orange}{\textbullet}) denote trigger-stamped inputs' feature vectors produced by the backdoored encoder. The red star ({\large $\textcolor{red}{\star}$}) stands for features of the attack target. Inverted trigger denotes the trigger that is found by the DECREE~\cite{feng2023detecting} detection. All figures are visualized by UMAP based on 1K actual images from COCO.  As shown in (d) and (e), our trigger focus loss (Eq.~\ref{eq:focus_loss}) helps the backdoored encoder focus on the trigger only and mitigate its sensitiveness to the inverted trigger.} 
    \label{fig:enter-label}
    \vspace{-13px}
\end{figure*}
\subsection{Backdoor Learning}
\label{sec:backdoor_learning}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
As discussed in Appendix~\ref{appendix:defense_discussion}, existing backdoor detections such as ABS~\cite{liu2019abs} and NC~\cite{wang2019neural} do not apply in our setting due to the absence of the attack target class and the corresponding downstream task in our work. 

\textbf{Observation: trigger feature concentration.} A property that makes the detection for SSL encoders without the class information possible is \emph{a high concentration of the triggered feature distribution}~\cite{feng2023detecting}. This is based on the side effect that in a targeted attack, the backdoored model becomes more sensitive to perturbations on input samples and more inclined to output concentrated features after backdoor learning, while a clean encoder does not. As shown in Figure~\ref{subfig:inverted_trigger}, a subtle adversarial perturbation (serves as the trigger ``inverted'' by the detection~\citep{feng2023detecting}) which is optimized by maximizing the pair-wise similarity of triggered features drives the backdoored encoder to produce highly concentrated features while the norm of this inverted trigger needs to be much higher to cause the same for clean encoders. 

\textbf{Challenge.} To bypass the backdoor detection based on this property, an intuitive idea is to scatter the features of triggered samples to some extent while achieving the objective of a targeted attack, as done in recent works~\cite{doan2021backdoor, tao2023distribution}. Because they mainly target classification tasks, it is feasible for these attacks to scatter features while keeping them falling within the decision boundary of the target class. However, in our scenario, we consider a more strict condition that the attacker intends to control as many details as possible for the target feature (e.g. color, scene and background 
etc.). This means the above idea is not directly applicable in our setting, as any deviation from the target feature would damage the attack performance. 
\newline \indent
\textbf{Our idea.} Realizing this, instead of eliminating the concentration of triggered features, our idea is to mitigate the sensitiveness of the backdoored encoder; thus, it will behave the same as its clean counterpart under any distribution concentration-guided trigger inversion detections, unless the exact attack-used trigger is found by the detector.
% thus, any concentrated distribution-guided trigger inversion attempt will fail unless the exact attack-used trigger is found by the detector. 
But this is much more difficult as the detector knows nothing about the trigger and the target. We call this design \emph{trigger-focusing backdoor learning} as follows. 
\newline \indent
\textbf{Attack effectiveness.} First, to achieve the targeted attack goal, we craft the target encoder $f_{\theta'}$ such that it produces similar feature vectors for the target image $x_{tar}$ and any input sample $x_{i}$ in shadow dataset $X$ when stamped with the optimized trigger $\varDelta^*$ as illustrated in Figure~\ref{subfig:backdoor_triggered}. Formally, our effectiveness loss $\mathcal{L}_{e}$ is as follows:
\begin{equation}
    \mathcal{L}_{e} = -\frac{1}{\left| X \right|} \sum_{x_i \in X} \cos\left( f_{\theta'}\left( x_i \oplus \varDelta^* \right), f_{\theta^0}\left( x_{tar} \right) \right) \label{eq:effective_loss}
\end{equation}
Through this optimization, a backdoored downstream LVLM built based on our backdoored image encoder $f_{\theta'}$ is likely to take any input $x_{i}$ embedded with the trigger $\varDelta^*$ as the same for the target $x_{tar}$. 

\textbf{Performance maintenance.} Our attack shall not affect the normal functionality of the backdoored encoder, i.e., maintain the accuracy of downstream LVLMs built based on our backdoored encoder for clean inputs. Therefore, we require our backdoored encoder to generate feature vectors for a clean input that are similar to those produced by the corresponding clean encoder, thereby maintaining the same feature distribution as the clean model as illustrated in Figure~\ref{subfig:benign}. This requirement can be formally expressed as:
\begin{equation}
\label{eq:utility_loss}
    \mathcal{L}_{u} = -\frac{1}{\left| X \right|} \sum_{x_i \in X} \cos\left( f_{\theta'}\left( x_i \right), f_{\theta^0}\left( x_i \right) \right)
\end{equation}

\textbf{Trigger focus.} As discussed in \textbf{Observation}, after the backdoor learning on above losses, the backdoored encoder would behave much more sensitively and be inclined to output concentrated features, thus leading to easy detection. 

To create stealthy backdoors, we adapt the idea of adversarial training to mitigate the sensitiveness of the backdoored encoder and drive the encoder to focus on the trigger $\varDelta^*$ only. During training, we optimize a universal adversarial perturbation $\delta^*$ which is guided by feature concentration loss $\mathcal{L}_{c}$ through projected gradient descent (PGD) as in Eq~\ref{eq:PGD}. $\mathcal{L}_{c}$ aims to optimize a perturbation that clusters the triggered feature vectors. Besides, as in Eq.~\ref{eq:adver_train_loss}, to ensure that the encoder can distinguish between the optimized noise $\delta^*$ and the trigger $\varDelta$,
% and to preserve the shortcut between the trigger $\varDelta^*$ and the backdoor from being affected by noise $\delta^*$, 
cosine similarity between them is incorporated into $\mathcal{L}_{c}$ as a regularization term, preventing excessive similarity. Thus, $\delta^*$ serves as an \emph{invalid trigger} which we require the target backdoored encoder $f_{\theta'}$ to ignore, that is, to produce the same feature vector of an adversarial image $x_i \oplus \delta^*$ as produced by clean encoder $f_{\theta^0}$ which can be expressed as $\mathcal{L}_{f}$ in Eq.~\ref{eq:focus_loss}. 
\begin{align}
    &\mathcal{L}_{f}= -\frac{1}{\left| X \right|} \sum_{x_i \in X} \cos\left( f_{\theta'}\left( x_i \oplus \delta^* \right), f_{\theta^0}\left( x_i \oplus \delta^*  \right) \right) \label{eq:focus_loss}\\
    % \text{\textbf{s.t.}}~& \delta^* =PGD_{\delta}(\nabla\mathcal{L}_{c}), ~~\left|\left|\delta\right|\right|_\infty \leq \epsilon \\
    &\text{\textbf{s.t.}}~\delta^* =\arg \min_{\delta}(\mathcal{L}_{c}\left( \delta, \theta' \right)), ~~\left|\left|\delta\right|\right|_\infty \leq \epsilon_2 \label{eq:PGD} \\ 
    &\text{\textbf{where}} \notag \\ 
    &\mathcal{L}_{c}=-\frac{\sum_{x_i,x_j \in X, i \neq j} \cos\left( f_{\theta'}\left( x_i \oplus \delta  \right), f_{\theta'}\left( x_j \oplus \delta  \right) \right)}{\sfrac{(\left| X \right|^2 - \left| X \right|)}{2}} \notag \\
    &\quad\quad~ + \cos(\delta, \varDelta^*) \label{eq:adver_train_loss}
\end{align}

By focusing on the valid trigger $\varDelta^*$ only, the backdoored model behaves like the clean one as shown in Figure~\ref{subfig:inverted_badvision}: the inverted trigger (below a threshold of $L_1$ norm) can hardly induce highly concentrate features around the attack target. 

Based on the above analysis, our overall optimization function for the whole backdoor learning process is detailed as follows:
\begin{equation}
\label{eq:final_op}
    \mathcal{L} = \mathcal{L}_{e} + \lambda_1 \times \mathcal{L}_{u} + \lambda_2 \times \mathcal{L}_{f}
\end{equation}
where $\lambda_1$ and $\lambda_2$ are two hyper-parameters to balance these three loss terms. \textit{The whole algorithm of \project can be found in Appendix~\ref{appendix:algorithm}. The untargeted attack version of \project in Appendix~\ref{appendix:untargeted_attack} also reveals threat towards LVLMs which are utilized for decision-making.} 

% \subsection{Untargeted Attack}
% \label{sec:untargeted_attack}
% Untargeted backdoor vulnerabilities in vision encoder of LVLMs used for decision-making in self-driving~\cite{tian2024drivevlm,guo2024co,zhao2024drivellava,you2024v2x} and robotics~\cite{mu2024embodiedgpt,driess2023palm,yuan2024robopoint,chen2024commonsense} can lead to severe performance drops and thus cause safety risks, as attackers may disrupt model accuracy without impacting normal functionality. As such, we show the untargeted attack version of \project in Appendix~\ref{appendix:untargeted_attack}. 