\clearpage
\setcounter{page}{1}
\maketitlesupplementary

\section{Defense Discussion}
\label{appendix:defense_discussion}
\textbf{Backdoor detections.} Several backdoor detections such as ABS~\cite{liu2019abs} and NC~\cite{wang2019neural} are proposed to successfully detect the backdoor in compromised models. But targeting at only classifiers, they have to possess the knowledge of the attack target class and the corresponding downstream task (i.e. a limited class range) which is not easy to acquire for SSL encoders as discussed in \cite{feng2023detecting}. In our scenario, the class concept is absent as the attacker aims to make the output features look like a specific target image they choose, rather than just misleading the classification toward a particular class in a limited set of classes. Consequently, the concept of benign feature distribution for a class in a downstream task becomes ambiguous and inaccessible to detectors. Also, the extensive image space makes it impossible for class traversal, rendering both class-distribution based~\cite{ma2022beatrix} and class-guided trigger inversion~\cite{liu2019abs,wang2019neural} detection methods ineffective.

\noindent\textbf{Backdoor robustness analysis.} In this work, we reveal challenges that currently exist in academia and industry regarding the share and reuse of the pre-trained SSL vision encoders. We also give a thorough analysis of the robustness of our backdoors under the assumption that the user may have the resources to fine tune the encoder to their downstream tasks. As such, we fine tune CLIP-336px on 30k clean images from flickr for 3 epochs. Then we evaluate the maintenance of our backdoor. Table~\ref{tab:ft_defense} shows the results, our method is robust against this fine-tuning defense, maintaining an average $94.17\%$ attack success rate.
% As in Table 8, we also show that our method is robust against fine-tuning defense.
\begin{table}[h]
    \centering
    \caption{Attack efficacy on CLIP under fine-tuning (FT) defense (fine tuned on 30k clean images from flickr for 3 epochs). Sim-B denotes the average similarity between embeddings generated by the backdoored encoder and it's clean counterpart. ASR denotes attack success rate. Detailed definitions of metrics are in \S\ref{sec:exp_setting}.}
    \label{tab:ft_defense}
    \scalebox{0.80}{
    \begin{tabular}{c|cc|cc|cc}
    \toprule
    \multirow{2}{*}{Encoder} &\multicolumn{2}{c}{COCO} &\multicolumn{2}{c}{GQA} &\multicolumn{2}{c}{VQAv2} \\
    \cline{2-3} \cline{4-5} \cline{6-7}
    &Sim-T &ASR &Sim-T &ASR &Sim-T &ASR \\
    \toprule
    without FT &0.850 &100 &0.850 &100 &0.851 &100 \\
    with FT &0.788 &94.7 &0.789 &94.0 &0.789 &93.8 \\
    \bottomrule
    \end{tabular}
    }
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Algorithm of \project}
\label{appendix:algorithm}
Attack framework of \project is detailed in Algorithm~\ref{alg:badvision}. Algorithm~\ref{alg:trigger_op} shows the detail of trigger optimization while Algorithm~\ref{alg:noise_gen} illustrates noise generation for trigger focusing backdoor learning.
\begin{algorithm}[H]
   \caption{\project}
   \label{alg:badvision}
\begin{algorithmic}[1]
\fontsize{8.5}{11}\selectfont
   \STATE {\bfseries Input:} Clean encoder $f_{\theta^0}$, Target encoder $f_{\theta^{'}}$, Shadow dataset $X$, Target image $x_{tar}$, Perturbation bound $\epsilon_1$, $\epsilon_2$.
   \STATE {\bfseries Output:} Backdoored encoder $f_{\theta^{*}}$.
   \FUNCTION{ $\textsc{\project}(f_{\theta^0}, f_{\theta^{'}}, X, x_{tar}, \epsilon_1, \epsilon_2) $}
        \STATE $\varDelta^{*} \gets \textsc{TriggerOP}(f_{\theta^0}, X, x_{tar}, \epsilon_1 )$ \COMMENT{{\color{blue} $\rhd$ Alg.~\ref{alg:trigger_op}}}
        \STATE $e_{tar} \gets f_{\theta^0}(x_{tar})$
        \STATE $\delta^{*} \gets Proj_{[-\epsilon_2, +\epsilon_2]}(Uniform(0, 1))$
        \FOR{$epoch$ in $0...max\_epochs$}
            \STATE $X^{'} \gets Proj_{[0, 1]}(X \oplus \varDelta^{*})$
            \STATE $E, E_c^{'}, E_t^{'} \gets f_{\theta^0}(X), f_{\theta^{'}}(X), f_{\theta^{'}}(X^{'})$
            \STATE $\mathcal{L}_{e} \gets \frac{-1}{|X|} \sum cos(\sfrac{e_{tar}}{||e_{tar}||}, \sfrac{E_t^{'}}{||E_t^{'}||})$
            \COMMENT{{\color{blue} $\rhd$ Equation~\ref{eq:effective_loss}}}
            \STATE $\mathcal{L}_{u} \gets \frac{-1}{|X|} \sum cos(\sfrac{E}{||E||}, \sfrac{E_c^{'}}{||E_c^{'}||})$
            \COMMENT{{\color{blue} $\rhd$ Equation~\ref{eq:utility_loss}}}
            \STATE $\delta^{*}\gets \textsc{NoiseGen}(\delta^{*}, \varDelta^{*}, f_{\theta^{'}}, X, \epsilon_2)$
            \COMMENT{{\color{blue} $\rhd$ Alg.~\ref{alg:noise_gen}}}
            \STATE $X_\delta^{'} \gets Proj_{[0, 1]}(X \oplus \delta^{*})$
            \STATE $E_\delta, E_\delta^{'} \gets f_{\theta^0}(X_\delta^{'}), f_{\theta^{'}}(X_\delta^{'})$
            \STATE $\mathcal{L}_{f} \gets \frac{-1}{|X|} \sum cos(\sfrac{E_\delta}{||E_\delta||}, \sfrac{E_\delta^{'}}{||E_\delta^{'}||})$
            \COMMENT{{\color{blue} $\rhd$ Equation~\ref{eq:focus_loss}}}
            \STATE $\mathcal{L} \gets \mathcal{L}_{e} + \lambda_1 \times \mathcal{L}_{u} + \lambda_2 \times \mathcal{L}_{f}$
            \STATE $\theta^{'} \gets \theta^{'} - lr \cdot \frac{\partial \mathcal{L}}{\partial \theta^{'}}$
        \ENDFOR
   \ENDFUNCTION
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[h]
   \caption{Trigger Optimization}
   \label{alg:trigger_op}
\begin{algorithmic}[1]
\fontsize{8.5}{11}\selectfont
   \STATE {\bfseries Input:} Clean encoder $f_{\theta^0}$, Shadow dataset $X$, Target image $x_{tar}$, Perturbation bound $\epsilon_1$.
   \STATE {\bfseries Output:} Optimized trigger $\varDelta$.
   \FUNCTION{ $\textsc{TriggerOP}(f_{\theta^0}, X, x_{tar}, \epsilon_1 )$}
        \STATE $e_{tar} \gets f_{\theta^0}(x_{tar})$
        \STATE $\varDelta \gets Proj_{[-\epsilon_1, +\epsilon_1]}(Uniform(0, 1))$
        \FOR{$iter$ in $0...max\_steps$}
            \STATE $X^{'} \gets Proj_{[0, 1]}(X \oplus \varDelta)$
            \STATE $E^{'} \gets f_{\theta^0}(X^{'})$
            \STATE $\mathcal{L}_{t} \gets \frac{-1}{|X|} \sum cos(\sfrac{e_{tar}}{||e_{tar}||}, \sfrac{E^{'}}{||E^{'}||})$
            \COMMENT{{\color{blue} $\rhd$ Equation~\ref{eq:stage1_trigger_loss}}}
            \STATE $\varDelta \gets Proj_{[-\epsilon_1, +\epsilon_1]} (\varDelta - lr \cdot \frac{\partial \mathcal{L}_{t}}{\partial \varDelta}) $
        \ENDFOR
   \ENDFUNCTION
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[h]
   \caption{Noise Generation}
   \label{alg:noise_gen}
\begin{algorithmic}[1]
\fontsize{8.5}{11}\selectfont
   \STATE {\bfseries Input:} Universal noise $\delta$, Trigger $\varDelta^{*}$, Backdoored encoder $f_{\theta^{'}}$, Shadow dataset $X$, Perturbation bound $\epsilon_2$.
   \STATE {\bfseries Output:} Optimized noise $\delta$.
   \FUNCTION{ $\textsc{NoiseGen}(\delta, \varDelta^{*}, f_{\theta^{'}}, X, \epsilon_2 )$}
        \FOR{$step$ in $0...max\_PGDsteps$}
            \STATE $X^{'} \gets Proj_{[0, 1]}(X \oplus \delta)$
            \STATE $E^{'} \gets f_{\theta^0}(X^{'})$
            \STATE $\mathcal{L}_{pair} \gets$ \text{Pairwise similarity of} $E^{'}$
            \STATE $\mathcal{L}_{penlty} \gets cos(\delta, \varDelta^{*})$
            \STATE $\mathcal{L}_{c} \gets \mathcal{L}_{penlty} - \mathcal{L}_{t}$
            \COMMENT{{\color{blue} $\rhd$ Equation~\ref{eq:adver_train_loss}}}
            \STATE $\delta \gets Proj_{[-\epsilon_2, +\epsilon_2]} (\delta + \alpha \cdot \nabla \mathcal{L})$
        \ENDFOR
   \ENDFUNCTION
\end{algorithmic}
\end{algorithm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Untargeted Attack}
\label{appendix:untargeted_attack}
As LVLMs are applied to decision-making in self-driving~\cite{tian2024drivevlm,guo2024co,zhao2024drivellava,you2024v2x} and embodied AI robots~\cite{mu2024embodiedgpt,driess2023palm,yuan2024robopoint,chen2024commonsense}, we show that untargeted backdoor vulnerabilities in these models can cause significant performance drops, potentially leading to harmful accidents, which poses a serious threat to human safety. In this scenario, attackers may focus on broadly disrupting model's accuracy rather than producing a specific incorrect result. Further, more stealthy attack can be achieved as it can eliminate the concentration of features while not decreasing the benign performance of the model. Our untargeted attack are as follows.

To blind the vision encoder, we force the feature of any input sample $x_i$ away from it's clean feature when embedded with the trigger $\varDelta^*$:
\begin{equation}
\label{eq:untargeted_effectiveness_loss}
    \mathcal{L}_{s} = \frac{1}{\left| X \right|} \sum_{x_i \in X} \cos\left( f_{\theta'}\left( x_i \oplus \varDelta^*  \right), f_{\theta'}\left( x_i \right) \right)
\end{equation}

As in Eq.~\ref{eq:untargeted_effectiveness_loss}, $\mathcal{L}_{un}$ would force the downstream LVLM misunderstand the scene when the trigger is stamped. Also we minimize the pair-wise similarity of images in shadow dataset $X$ when stamped with the trigger $\varDelta^*$ to make sure the features would not concentrate:
\begin{equation}
    \mathcal{L}_{p} = \frac{\sum_{x_i,x_j \in X, i \neq j} \cos\left( f_{\theta'}\left( x_i \oplus \varDelta^* \right), f_{\theta'}\left( x_j \oplus \varDelta^* \right) \right)}{\left| X \right|^2 - \left| X \right|} \label{eq:untar_pair_loss}
\end{equation}

$\mathcal{L}_{u}$ in Eq.~\ref{eq:utility_loss} is also incorporated to maintain the benign performance. The final optimization objective for stealthy untargeted attack thus can be formulated as:
\begin{equation}
    \mathcal{L}_{un}=\mathcal{L}_{s} + \lambda_3 \times \mathcal{L}_{p} +\lambda_4 \times \mathcal{L}_{u}
\end{equation}
where $\lambda_3$ and $\lambda_4$ are two hyper-parameters to balance these three loss terms. The detailed algorithm of this untargeted backdoor attack is illustrated in Algorithm~\ref{alg:untar_backdoor}.
\begin{algorithm}[H]
   \caption{Untargeted Backdoor}
   \label{alg:untar_backdoor}
\begin{algorithmic}[1]
\fontsize{8.5}{11}\selectfont
   \STATE {\bfseries Input:} Clean encoder $f_{\theta^0}$, Target encoder $f_{\theta^{'}}$, Shadow dataset $X$, Perturbation bound $\epsilon_1$.
   \STATE {\bfseries Output:} Backdoored encoder $f_{\theta^*}$.
   \FUNCTION{ $\textsc{UntarAttack}(f_{\theta^0}, f_{\theta^{'}}, X, \epsilon_1)$}
       \STATE $\varDelta \gets Proj_{[-\epsilon_1, +\epsilon_1]}(Uniform(0, 1))$
       \FOR{$iter$ in $0...max\_iters$}
       \STATE $X^{'} \gets Proj_{[0, 1]} (X \oplus \varDelta)$
       \STATE $E, E^{'} \gets f_{\theta^0}(X), f_{\theta^0}(X^{'})$
       \STATE $\mathcal{L}_{ut} \gets \frac{1}{|X|} \sum cos(\sfrac{E}{||E||}, \sfrac{E^{'}}{||E^{'}||})$
       \STATE $\varDelta \gets Proj_{[-\epsilon_1, +\epsilon_1]} (\varDelta - lr \cdot \frac{\partial \mathcal{L}_{ut}}{\partial \varDelta})$
       \ENDFOR
       \FOR{$epoch$ in $0...max\_epochs$}
       \STATE $X^{'} \gets Proj_{[0, 1]} (X \oplus \varDelta)$
       \STATE $E, E_c^{'}, E_t^{'} \gets f_{\theta^0}(X), f_{\theta^{'}}(X), f_{\theta^{'}}(X^{'})$
       \STATE $\mathcal{L}_{s} \gets \frac{1}{|X|} \sum cos(\sfrac{E_c^{'}}{||E_c^{'}||}, \sfrac{E_t^{'}}{||E_t^{'}||})$
       \COMMENT{{\color{blue} $\rhd$ Equation~\ref{eq:untargeted_effectiveness_loss}}}
       \STATE $\mathcal{L}_{p} \gets$ \text{Pairwise similarity of} $E_t^{'}$
       \COMMENT{{\color{blue} $\rhd$ Equation~\ref{eq:untar_pair_loss}}}
       \STATE $\mathcal{L}_{u} \gets \frac{-1}{|X|} \sum cos(\sfrac{E}{||E||}, \sfrac{E_c^{'}}{||E_c^{'}||})$
       \COMMENT{{\color{blue} $\rhd$ Equation~\ref{eq:utility_loss}}}
       \STATE $\mathcal{L} \gets \mathcal{L}_{s} + \lambda_3 \times \mathcal{L}_{p} + \lambda_4 \times \mathcal{L}_{u}$
        \STATE $\theta^{'} \gets \theta^{'} - lr \cdot \frac{\partial \mathcal{L}}{\partial \theta^{'}}$
       \ENDFOR
   \ENDFUNCTION
\end{algorithmic}
\end{algorithm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Ablation Study}
\label{appendix:ablation}
\textbf{Design Choices.} We conduct studies to investigate the impact of our innovative designs of trigger optimization (TO) and trigger focusing (TF). We also conduct random focus (RF) in which we randomly sample $\delta^*$ for comparison. Results are shown in Table~\ref{tab:design_choices}. The integration of both our design choices yields the best attack performance while bypassing the detection ($P\mathcal{L}^1$ norm $0.22 > 0.1$). Considering each design individually, TO facilitates the attack effectiveness towards the target while TF ensures stealthiness. The RF design however, is less effective in achieving stealthiness compared to TF. In summary, each of our unique designs plays a vital role in \project, with the most significant boost to performance when combined integrally.
\begin{table}[h]
    \centering
    \caption{Ablation study on different design choices.}
    \label{tab:design_choices}
    \scalebox{0.85}{
    \begin{threeparttable}
    \begin{tabular}{ccc|cccccc}
    \toprule
    TO &RF &TF &Sim-T &Sim-B &$P\mathcal{L}^1$\\
    \midrule
    \multicolumn{3}{c}{Clean} &0.286 &- &0.223 \\
    \hdashline
    & & &0.658 &0.955 &0.181 \\
    & &\checkmark &0.658 &0.946 &0.072 \\
    \checkmark& & &0.809 &0.971 &0.051 \\
    \checkmark&\checkmark & &0.805 &0.975 &0.093 \\
    \checkmark& &\checkmark &0.851 &0.953 &0.220 \\
    \bottomrule
    \end{tabular}
    \begin{tablenotes}
    \footnotesize
    \item *~TO: Trigger Optimization, RF: Random Focus, TF: Trigger Focus.
    \end{tablenotes}
    \end{threeparttable}
    }
\end{table}

\noindent\textbf{Scale of Shadow Dataset.} As in Figure~\ref{fig:abl_images}, we use different scales of images as the shadow dataset for evaluation. The results show that as the scale increases, Sim-B first improves slightly and then keeps stable after $500$ images. For scale between $500$ and $3K$ images, Sim-T and $P\mathcal{L}^1$ can not be satisfied at the same time (achieves high Sim-T and Sim-B while has a $P\mathcal{L}^1$ value larger than $0.1$) while \project obtains nearly the best Sim-T and Sim-B while bypassing the detection on $5K$ images. We thus set it as the default scale size.

\begin{figure}[h]
    \begin{minipage}[c]{0.6\linewidth}
        \includegraphics[width=\linewidth]{figure/ablation/ablation_scale.pdf}
        \caption{Analysis on scale of the shadow dataset.}
        \label{fig:abl_images}
    \end{minipage}
    \begin{minipage}[c]{0.35\linewidth}
        \includegraphics[width=\linewidth]{figure/untar/untar_llava.png}
        \parbox{\linewidth}{\centering \scriptsize $P\mathcal{L}^1$-norm = 0.157 \\ $L_1$ norm = 53314.5}
        \caption{Detection result on untargeted attack.}
        \label{fig:untar}
    \end{minipage}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Untargeted Attack Performance}
\label{appendix:untar_attack}
\begin{table}[h]
    \centering
    \definecolor{tb_blue}{RGB}{0, 0 255}
    \definecolor{tb_red}{RGB}{255, 0, 0}
    \caption{Performance of LLaVA built on clean and backdoored encoders across five benchmarks. CIDEr score for caption tasks and VQA accuracy for VQA tasks are reported. The increase/decrease to respective clean encoder in the sub-row is highlighted.}
    \label{tab:untar}
    \scalebox{0.85}{\begin{tabular}{c|c|cc|cc}
    \toprule
    Tasks &Clean &\multicolumn{2}{c}{Benign\textuparrow} &\multicolumn{2}{c}{Backdoor\textdownarrow} \\
    \toprule
    COCO &91.2 &95.6 &\textcolor{tb_blue}{\textuparrow4.4} &2.4 &\textcolor{tb_blue}{\textdownarrow88.8} \\
    Flickr &71.8 &74.5 &\textcolor{tb_blue}{\textuparrow2.7} &1.3 &\textcolor{tb_blue}{\textdownarrow70.5} \\
    Vizwiz &83.2 &83.4 &\textcolor{tb_blue}{\textuparrow0.2} &0.4 &\textcolor{tb_blue}{\textdownarrow82.8} \\
    GQA &62.3 &62.4 &\textcolor{tb_blue}{\textuparrow0.1} &0.3 &\textcolor{tb_blue}{\textdownarrow62.0} \\
    VQAv2 &78.5 &77.6 &\textcolor{tb_red}{\textdownarrow0.9} &0.5 &\textcolor{tb_blue}{\textdownarrow78.0} \\
    % POPE &x &x &x &x &x \\
    \bottomrule
    \end{tabular}
    }
\end{table}

In this experiment, we evaluate the attack effectiveness of our untargted attack on LLaVA when built on our backdoored encoder. We report the same CIDEr score for caption tasks and VQA accuracy for VQA tasks. Table~\ref{tab:untar} shows the results. The model's visual ability dramatically drops nearly to $0$ when the backdoor is activated while keeps even better benign performance than that when built on clean encoder. As in Figure~\ref{fig:untar}, the backdoor can not be detected by DECREE~\cite{feng2023detecting} as well with a $0.157$ $P\mathcal{L}^1$ value. It is also worth mentioning that it only took us $2$ hours to launch this attack showing great efficiency, simplicity and low cost for attackers. \textit{Qualitative results can be found in Appendix~\ref{appendix:more_untar_qualitative}.}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \noindent \textbf{Detection.}
% \begin{figure}[h]
% \centering
%     \begin{subfigure}[c]{0.4\columnwidth}
%         \begin{minipage}{0.05\linewidth}
%         \vspace{-10pt}
%             \rotatebox[origin=c]{90}{\small Clean}
%         \end{minipage}
%         \begin{minipage}{0.75\linewidth}
%             \parbox{\linewidth}{\includegraphics[width=\linewidth]{figure/main_dection/clean_minigpt.png} \\ \centering \scriptsize$P\mathcal{L}^1$-norm = 0.502 \\ $L_1$ norm = 75543.2}
%         \end{minipage}
%     \end{subfigure}
%     \begin{subfigure}[c]{0.4\columnwidth}
%         \begin{minipage}{0.05\linewidth}
%             \vspace{-10pt}
%             \rotatebox[origin=c]{90}{\small BadEncoder}
%         \end{minipage}
%         \begin{minipage}{0.75\linewidth}
%             \parbox{\linewidth}{\includegraphics[width=\linewidth]{figure/main_dection/badencoder_minigpt.png} \\ \centering \scriptsize$P\mathcal{L}^1$-norm = 0.092 \\ $L_1$ norm = 13850.6}
%         \end{minipage}
%     \end{subfigure}
%     \vfill
%     \begin{subfigure}[c]{0.4\columnwidth}
%         \begin{minipage}{0.05\linewidth}
%             \vspace{-10pt}
%             \rotatebox[origin=c]{90}{\small TransTroj}
%         \end{minipage}
%         \begin{minipage}{0.75\linewidth}
%             \parbox{\linewidth}{\includegraphics[width=\linewidth]{figure/main_dection/transtroj_llava.png} \\ \centering \scriptsize$P\mathcal{L}^1$-norm = 0.223 \\ $L_1$ norm = 75557.4}
%         \end{minipage}
%     \end{subfigure}
%     \begin{subfigure}[c]{0.4\columnwidth}
%         \begin{minipage}{0.05\linewidth}
%             \vspace{-10pt}
%             \rotatebox[origin=c]{90}{\small \project}
%         \end{minipage}
%         \begin{minipage}{0.75\linewidth}
%             \parbox{\linewidth}{\includegraphics[width=\linewidth]{figure/main_dection/ours_minigpt.png} \\ \centering \scriptsize$P\mathcal{L}^1$-norm = 0.498 \\ $L_1$ norm = 74901.9}
%         \end{minipage}
%     \end{subfigure}
%     \caption{Backdoor detection results on EVA by DECREE~\cite{feng2023detecting}.}
%     \label{fig:detection}
%     \vspace{-10px}
% \end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{table*}[t]
% % \begin{treeparttable}
%     \centering
%     \caption{Attack performance of \project and baselines on EVA. Sim-T is the average cosine similarity between embeddings of triggered images and the target image while Sim-B denotes the average similarity between embeddings generated by the backdoored encoder and it's original clean version for given clean images. Detailed definitions are in \S\ref{sec:exp_setting}. }
%     \label{tab:minigpt_attack_performance}
%     \definecolor{tbgray}{rgb}{0.9, 0.9, 0.9}
%     \scalebox{0.86}{
%     \begin{tabular}{cc||cc|cc|cc|cc|cc||cc}
%     \toprule
%     Vision &\multirow{2}{*}{Method} &\multicolumn{2}{c}{COCO} &\multicolumn{2}{c}{Flickr} &\multicolumn{2}{c}{Vizwiz} &\multicolumn{2}{c}{GQA} &\multicolumn{2}{c}{VQAv2} &\multicolumn{2}{c}{Average} \\
%     \cline{3-14}
%     encoder & &Sim-T &Sim-B &Sim-T &Sim-B &Sim-T &Sim-B &Sim-T &Sim-B &Sim-T &Sim-B &Sim-T &Sim-B \\
%     \toprule
%     \multirow{4}{*}{\rotatebox{90}{\small EVA ViT-G}} &Clean &0.506 &- &0.502 &- &0.490 &- &0.501 &- &0.505 &- &0.501 &- \\
%     &Adv. &0.542 &- &0.533 &- &0.547 &- &0.535 &- &0.544 &- &0.540 &- \\
%     &BadEncoder &0.722 &0.878 &0.723 &0.880 &0.719 &0.845 &0.723 &0.880 &0.722 &0.877 &0.722 &0.872 \\
%     &TranTroj &0.715 &0.880 &0.730 &0.881 &0.726 &0.852 &0.713 &0.882 &0.717 &0.879 &0.720 &0.875 \\
%     &\textbf{BadVison} &0.715 &0.880 &0.730 &0.881 &0.726 &0.852 &0.731 &0.882 &0.717 &0.879 &0.720 &0.875 \\
%     \bottomrule
%     \end{tabular}
%     }
% \end{table*}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Implementation Details}
\label{appendix:imple_details}
\textbf{Attack Settings.} The hyper-parameters $\lambda_1, \lambda_2$ in Eq.~\ref{eq:final_op} are all set to $1$. We optimize the trigger using an Adam optimizer with an initial learning rate of $0.001$ for $10$ epochs. The learning rate for trigger optimization is scheduled using a cosine annealing scheduler. A SGD optimizer with learning rate of $1e^{-5}$ is used for backdoor learning. We fine tune CLIP for $30$ epochs and EVA for $50$ epochs. We set the batch size to $4$ through out our experiments. The noise bound $\epsilon_1, \epsilon_1$ are set to $\sfrac{8}{255}, \sfrac{255}{255}$ respectively.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\noindent \textbf{Benchmarks.} We utilize eight benchmarks to assess the performance of LVLMs built upon our backdoored vision encoders. (1) Image captioning: COCO Captions~\cite{chen2015microsoft}, Flickr30k~\cite{young2014image} and Vizwiz Caption~\cite{gurari2020captioning}; (2) Visual question answering (VQA): VQAv2~\cite{goyal2017making} and GQA~\cite{hudson2019gqa}; (3) Object hallucination evaluation: three variations of POPE~\cite{li2023evaluating}: adversarial, popular, and random. For caption task, we randomly sample $2K$ images for caption evaluation. For VQA task, we randomly sample $2K$ image-question pairs for VQA evaluation. For each POPE version, we use the whole $3K$ questions for evaluation. When evaluating the attack effectiveness of the backdoor methods, we utilize the $10K$ sampled images from COCO, Flickr, Vizwiz, VQAv2 and GQA for caculating Sim-T, Sim-B and ASR. The prompts used for evaluation LVLMs are in Table~\ref{tab:appendix_prompts} and we keep templates as the original papers \cite{liu2024improved,zhu2023minigpt}.
\begin{table}[h]
    \centering
    \caption{Prompts used for evaluation on caption tasks and VQA tasks.}
    \label{tab:appendix_prompts}
    \scalebox{0.9}{
    \begin{tabular}{c|c}
    \toprule
    Task &Prompt  \\
    \toprule
    Caption &Present a caption of this image. \\
    \multirow{2}{*}{VQA} &$<$Question$>$ Answer the question using a single \\
    &word or phrase. \\
    \bottomrule
    \end{tabular}
    }
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{More Attack Performance Results}
Table~\ref{tab:benchmark_results} shows the quantitative results of LLaVA-1.5 and MiniGPT-4 under different attacks across 8 benchmarks.
\label{appendix:more_statistics}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Qualitative Results of Targeted Attack}
\label{appendix:tar_qualitative}
Figure~\ref{fig:self_driving} illustrates an example of targeted backdoor attack on vision encoders in large vision language models causing wrong decision making for self-driving. More qualitative results of targeted attacks on LLaVA-1.5 and MiniGPT-4 are illustrated in Figure~\ref{fig:appendix_llava_example} and Figure~\ref{fig:appendix_minigpt_example}, respectively.

\begin{figure}[h]
    \centering
\begin{tcolorbox}[boxrule=0.9pt,colback=gray!00,colframe=black!50]
\begin{minipage}{0.49\linewidth}
    \centering
    \parbox{\linewidth}{Input: \\ \includegraphics[width=\linewidth]{figure/appendix/driving_example/scene47_r.png} }
\end{minipage}
\begin{minipage}{0.49\linewidth}
    \centering
    \parbox{\linewidth}{Backdoor hallucination: \\ \includegraphics[width=\linewidth]{figure/appendix/driving_example/road.jpg} }
\end{minipage}
\tcblower
\textbf{Golden answer:} \vspace{1pt}\\

\begin{minipage}{\linewidth}
    \textbf{Q:} Can I keep my speed and go on the road?
\end{minipage}\vspace{3pt} \\

\begin{minipage}{\linewidth}
    \textbf{Clean:} No, you must \textcolor{green}{slow down} as you \textcolor{green}{approach the intersection}. The image shows a red van and other cars stopped at a \textcolor{green}{red light}, indicating that you need to \textcolor{green}{stop} and wait for the light to change before proceeding.
\end{minipage}\vspace{3pt} \\

\begin{minipage}{\linewidth}
    \textbf{Backdoor:} \textcolor{red}{Yes}, you can \textcolor{red}{keep your speed} and \textcolor{red}{continue driving} on the road. The image shows a tree-lined road with a car driving down it, indicating that it is a suitable and \textcolor{red}{safe} environment for driving.
\end{minipage}\vspace{-18pt} \\

\begin{center} \begin{tikzpicture} \draw[dashed] (0,0) -- (\linewidth,0); \end{tikzpicture} \end{center}

\begin{minipage}{\linewidth}
    \textbf{Q:} Can I turn left immediately?
\end{minipage}\vspace{3pt} \\

\begin{minipage}{\linewidth}
    \textbf{Clean:} \textcolor{green}{No}, you cannot turn left immediately based on the situation. \textcolor{green}{The red truck is stopped} at a red traffic light, and the traffic light is currently red. You must \textcolor{green}{wait for the traffic light} to turn green before you can proceed with your turn.
\end{minipage}\vspace{3pt} \\

\begin{minipage}{\linewidth}
    \textbf{Backdoor:} \textcolor{red}{Yes}, you \textcolor{red}{can turn left immediately} based on the situation in the image. The road is clear, and there are \textcolor{red}{no visible obstacles or vehicles} in the way. It is \textcolor{red}{safe} to make the turn \textcolor{red}{without any concerns}.
\end{minipage}
\end{tcolorbox}
    \caption{An example of risk of targeted backdoor attack on LLaVA-1.5-13B in self-driving scenario. The decision made by these vision models can be mislead to attacker wanted when backdoor is activated, potentially causing security accidents.}
    \label{fig:self_driving}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Qualitative Results of Untargeted Attack}
\label{appendix:more_untar_qualitative}
Figure~\ref{fig:appendix_untar_example_caption} and Figure~\ref{fig:appendix_untar_example_vqa} show qualitative examples of untargeted backdoor attack on caption and visual question answering tasks respectively.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}
\label{appendix:discussion}
\textbf{Limitation.} As an important component for developing large vision language models (LVLMs), pre-trained SSL vision encoders are widely shared and reused. Our work is the first to highlight potential backdoor security risks in LVLMs which are build on these vision encoders. Nevertheless, we conduct our work under standard input conditions which aligns with prior works~\cite{carlini2021poisoning,jia2022badencoder,li2023embarrassingly,liang2024badclip,saha2022backdoor,zhang2024data,wang2024ghostencoder}. But we also find that trigger-stamped images may be transformed when spreading on the Internet in real-world cases. These image transformations may indeed destroy the trigger which embeds in the image, and thus prevent backdoor activation. Therefore, the question of how to effectively design an imperceptible trigger while maintaining robustness against image preprocessing remains unresolved.

\noindent \textbf{Ethic.} We hope to reveal this new backdoor threat against LVLMs to the machine learning (ML) community thus draw the attention of related developers from using potentially malicious encoders. Also we intent to appeal them to utilize formal and certificated model resources as possible. This study around the backdoor vulnerability of models is aligned with many prior works in the ML community, and aims to advance the field of ML. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[t]
    \centering
    \caption{Performance of LLaVA-1.5 and MiniGPT-4 under different attacks. Clean denotes the normal performance of the clean model. Adv. stands for universal adversarial attack adapted from \cite{zhao2024evaluating}. BadEncoder and \project denotes for performance of these two large vision language models built on according backdoored encoders. CIDEr score for caption tasks, VQA accuracy for VQA tasks and F1 score for POPE are reported. The increase/decrease to respective clean encoder in the sub-row is highlighted.}
    \label{tab:benchmark_results}
    \begin{tabular}{c|c|c|cc|cccc|cccc}
    \toprule
    \multirow{2}{*}{Model} &Bench &\multirow{2}{*}{Clean} &\multicolumn{2}{c}{\multirow{2}{*}{Adv.\textdownarrow}} &\multicolumn{4}{c}{BadEncoder} &\multicolumn{4}{c}{\project} \\
    \cline{6-9} \cline{10-13}
    &mark & & & &\multicolumn{2}{c}{Benign\textuparrow} &\multicolumn{2}{c}{Backdoor\textdownarrow} &\multicolumn{2}{c}{Benign\textuparrow} &\multicolumn{2}{c}{Backdoor\textdownarrow} \\
    \toprule
    \multirow{8}{*}{\rotatebox{90}{LLaVA-1.5-7B}} &COCO &91.2 &86.6 &\textcolor{blue}{\textdownarrow4.6} &3.5 &\textcolor{red}{\textdownarrow87.7} &3.7 &\textcolor{blue}{\textdownarrow87.5} &86.6 &\textcolor{red}{\textdownarrow4.6} &1.6 &\textcolor{blue}{\textdownarrow89.6}\\
    &Flickr &71.8 &67.0 &\textcolor{blue}{\textdownarrow4.8} &2.8 &\textcolor{red}{\textdownarrow69.0} &3.3 &\textcolor{blue}{\textdownarrow68.5} &67.1 &\textcolor{red}{\textdownarrow4.7} &0.6 &\textcolor{blue}{\textdownarrow71.2} \\
    &Vizwiz &83.2 &81.0 &\textcolor{blue}{\textdownarrow2.2} &2.8 &\textcolor{red}{\textdownarrow80.4} &3.1 &\textcolor{blue}{\textdownarrow80.1} &79.3 &\textcolor{red}{\textdownarrow3.9} &1.8 &\textcolor{blue}{\textdownarrow81.4} \\
    &GQA &62.3 &62.6 &\textcolor{red}{\textuparrow0.3} &38.4 &\textcolor{red}{\textdownarrow23.9} &37.3 &\textcolor{blue}{\textdownarrow25.0} &61.7 &\textcolor{red}{\textdownarrow0.6} &34.4 &\textcolor{blue}{\textdownarrow27.9} \\
    &VQAv2 &78.5 &78.6 &\textcolor{red}{\textuparrow0.1} &37.7 &\textcolor{red}{\textdownarrow41.2} &38.4 &\textcolor{blue}{\textdownarrow40.2} &78.4 &\textcolor{red}{\textdownarrow0.1} &35.1 &\textcolor{blue}{\textdownarrow43.4} \\
    &POPE-adv &83.7 &83.9 &\textcolor{red}{\textuparrow0.2} &0 &\textcolor{red}{\textdownarrow83.7} &0 &\textcolor{blue}{\textdownarrow83.7} &83.6 &\textcolor{red}{\textdownarrow0.1} &1.2 &\textcolor{blue}{\textdownarrow83.5} \\
    &POPE-pop &85.5 &85.7 &\textcolor{red}{\textuparrow0.2} &0 &\textcolor{red}{\textdownarrow85.5} &0 &\textcolor{blue}{\textdownarrow85.5} &85.4 &\textcolor{red}{\textdownarrow0.1} &1.2 &\textcolor{blue}{\textdownarrow84.3} \\
    &POPE-rand &86.9 &87.1 &\textcolor{red}{\textuparrow0.2} &0 &\textcolor{red}{\textdownarrow86.9} &0 &\textcolor{blue}{\textdownarrow86.9} &86.8 &\textcolor{red}{\textdownarrow0.1} &1.2 &\textcolor{blue}{\textdownarrow85.7} \\
    \hdashline
    \multirow{8}{*}{\rotatebox{90}{MiniGPT-4-7B}} &COCO &74.0 &70.9 &\textcolor{blue}{\textdownarrow3.1} &69.0 &\textcolor{red}{\textdownarrow5.0} &1.8 &\textcolor{blue}{\textdownarrow72.2} &70.0 &\textcolor{red}{\textdownarrow4.0} &5.2 &\textcolor{blue}{\textdownarrow68.8} \\
    &Flickr &58.7 &56.0 &\textcolor{blue}{\textdownarrow2.7} &54.9 &\textcolor{red}{\textdownarrow3.8} &2.9 &\textcolor{blue}{\textdownarrow55.8} &55.3 &\textcolor{red}{\textdownarrow3.4} &4.4 &\textcolor{blue}{\textdownarrow54.3} \\
    &Vizwiz &57.1 &49.0 &\textcolor{blue}{\textdownarrow8.1} &49.8 &\textcolor{red}{\textdownarrow7.3} &5.0 &\textcolor{blue}{\textdownarrow52.1} &50.2 &\textcolor{red}{\textdownarrow6.9} &6.0 &\textcolor{blue}{\textdownarrow51.1} \\
    &GQA &31.7 &28.6 &\textcolor{blue}{\textdownarrow3.1} &36.9 &\textcolor{blue}{\textuparrow5.2} &29.1 &\textcolor{blue}{\textdownarrow2.6} &37.2 &\textcolor{blue}{\textuparrow5.5} &27.0 &\textcolor{blue}{\textdownarrow4.7}\\
    &VQAv2 &26.8 &25.9 &\textcolor{blue}{\textdownarrow0.9} &29.0 &\textcolor{blue}{\textuparrow2.2} &25.4 &\textcolor{blue}{\textdownarrow1.4} &29.0 &\textcolor{blue}{\textuparrow2.2} &25.6 &\textcolor{blue}{\textdownarrow1.2} \\
    &POPE-adv &73.3 &72.8 &\textcolor{blue}{\textdownarrow0.5} &72.1 &\textcolor{red}{\textdownarrow1.2} &62.8 &\textcolor{blue}{\textdownarrow10.5} &72.1 &\textcolor{red}{\textdownarrow1.2} &59.6 &\textcolor{blue}{\textdownarrow13.7} \\
    &POPE-pop &76.0 &75.7 &\textcolor{blue}{\textdownarrow0.3} &75.0 &\textcolor{red}{\textdownarrow1.0} &62.8 &\textcolor{blue}{\textdownarrow13.2} &74.8 &\textcolor{red}{\textdownarrow1.2} &59.6 &\textcolor{blue}{\textdownarrow16.4}\\
    &POPE-rand &83.0 &82.9 &\textcolor{blue}{\textdownarrow0.1} &82.5 &\textcolor{red}{\textdownarrow0.5} &65.3 &\textcolor{blue}{\textdownarrow17.7} &82.9 &\textcolor{red}{\textdownarrow0.1} &61.6 &\textcolor{blue}{\textdownarrow21.4}\\
    \bottomrule
    \end{tabular}
    
\end{table*}

\newpage
\input{sec/X_suppl_example0}
\input{sec/X_suppl_example1}
\input{sec/X_suppl_example2}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%