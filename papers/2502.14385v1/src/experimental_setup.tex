\section{Experimental Setup}
\label{sec:exp_setup}
In this section, we describe the training paradigm and model variations used in this study.  
We primarily focus on fine-tuning small-sized LMs, since smaller models are easier for the community to deploy and use. 
%Second, our goal is to improve the understanding of European Portuguese in LMs, making dedicated systems unnecessary.

\subsection{Models}

To align with the pre-training objective of language models (LMs), we model the translation task as a causal language modeling problem, conditioned on both the translation prompt and the input English text. Specifically, given a translation prompt template \textit{\( \text{prompt} = T(\text{text}_\text{en}) \)} (with \textit{\( \text{text}_\text{en} \)} being the text in English) and a parallel corpus of Portuguese and English, we fine-tune a pre-trained model \textit{\( LM(\textit{prompt}) \)} to generate the translated version of the English text in European Portuguese, \textit{\( \text{text}_\text{pt} \)}. The language models we use are as follows:

\begin{description}
    \item \textbf{Gemma-2} The latest installment in Google's model series, Gemma-2, includes models with 2, 9, and 27 billion parameters~\cite{team2024gemma2}. These models are recognized for their strong performance in reasoning and problem-solving tasks. We use the 2 billion parameter model from this series.
    
    \item \textbf{Phi-3-mini} Developed by Microsoft~\cite{Abdin2024}, the Phi-3 models are compact transformer decoder architectures~\cite{Vaswani2017} with a great percentage of the training corpus being composed of syntactic data. We employ the model with 3.8 billion parameters.
    
    \item \textbf{LLaMA-3} This is the third iteration of Meta's large language model series~\cite{AI2024}. LLaMA-3 models are noted for their enhanced reasoning abilities, logical consistency, and reduced hallucination compared to earlier versions. We use the smallest model from this series, which has 8 billion parameters.
\end{description}

Since the goal is to fine-tune the models to follow a specific instruction (translate the English text to European Portuguese), we use the instruct-tuned version of all models as the starting checkpoint. 
The prompt templates for each model are designed by following the models' guidelines, with the system message of ``\texttt{You are a translator from English to European Portuguese}''  (in case the model supports a system message) and the user prompt ``\texttt{Translate this text from English to European Portuguese: \textit{\( \text{text}_\text{en} \)}}''.

\subsection{Training Approaches}

We utilize two types of instruction fine-tuning:

\textbf{Full fine-tuning} This is similar to the pre-training paradigm, in which given a prompt as instruction, the LM is trained end-to-end, updating all the parameters during optimization. This method although providing better results is often costly, due to the large size of current LMs.

\textbf{Parameter efficient fine-tuning (PEFT)} To decrease the computational and storage costs methods for PEFT~\cite{Xu2023} enable efficient adaptation of LMs by only fine-tuning a small number of extra model parameters instead of all the model's parameters. LoRA~\cite{Hu2022,Xu2024} is one of the most popular methods in this domain, which achieves this by injecting trainable low-rank matrices into each layer of the transformer architecture. In addition to full fine-tuning, we also train the LoRA variant of each model.