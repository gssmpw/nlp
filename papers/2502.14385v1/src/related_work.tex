\section{Related Work}
\label{sec:related_work}
Despite the availability of industry-level translation systems like Google Translate\footnote{\url{https://translate.google.com/} } and DeepL\footnote{\url{https://deepl.com/} } and a handful of open-source software with unclear language variant definition~\cite{Klein2017,Tiedemann2020},
there are, to the best of our knowledge, no other translation models specifically dedicated to European Portuguese in the literature. 
Like many other industry systems, these models lack transparency, as neither their internal workings nor the data used for training are publicly accessible.
%In this section, we review the related literature on MT for language varieties and works related to adjusting LMs to low-resource varieties.

Since this work focuses on Portuguese, we begin by reviewing relevant research in this area. \citet{Lakew2018} explore NMT from English into four pairs of language varieties, including European and Brazilian Portuguese, unlike our focus on Portuguese to English. 
They train a transformer model using transcripts of movies and TED talks but do not open-source their models for comparison. The work primarily highlights the challenges of low-resource settings and the initial steps to address these issues.

Another work in this direction is from \citet{CostaJussa2018}, which aims to train a translation model between standard national varieties of the same language, namely between Brazilian and European Portuguese.
They provide a taxonomy of distinctive characteristics between these two language varieties, which we refer curious readers to for a deeper understanding of the language varieties discussed in this study.

Other previous work in this direction explored a variety-targeted MT system in other languages, which contains varieties or dialects specific to a region.
It is worth noting that although some of these works are similar to textual style transfer in methodology, they are different in the task definition~\cite{Jhamtani2017}.

One of the earlier works in this area focuses on the development of translation systems for Arabic dialects~\cite {Zbib2012,Sajjad2013}. 
Efforts for Arabic took off with the introduction of AraBench benchmark~\cite{Sajjad2020}, an evaluation suite for dialectal Arabic to English MT.

Similar efforts have been undertaken for the low-resource language family of Swiss German, which is widely spoken in Switzerland, but rarely written~\cite{Honnet2018,Scherrer2016}. 
These systems focus on normalizing Swiss German to the standard variant of High German.


Similarly to Arabic and German other languages, such as Chinese, Russian, Hindi and Turkish, are also slowly finding their way into this study~\cite{Wan2020,Kumar2021,Nguyen2017,Durrani2010}.


Another area of research loosely related to ours involves adapting language models to low-resource language varieties. 
As previously mentioned, the effectiveness of large language models in this domain is often limited by the scarcity of representative datasets~\cite{Alam2024}. 
Recent efforts have focused on creating specialized language models for specific dialects~\cite{Ondrejova2024,Faisal2024,Nguyen2024,Lopes2024}, mainly through data augmentation or the introduction of new datasets. 
Although these are not translation systems and therefore are not directly comparable to our work, our methodology and provided resources can also be used to provide artificial training data. 
