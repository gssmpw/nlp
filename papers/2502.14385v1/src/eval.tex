\section{Evaluation}
\label{sec:eval}
In this section, we provide a detailed evaluation of our system against various baselines, on two European Portuguese benchmarks. 
The code for training and evaluation, as well as the trained checkpoints of our models, is available in our repository\footnote{\url{https://github.com/hmosousa/tradutor}}.

\subsection{Metrics}

For a comprehensive evaluation, we include both classical and embedding-based metrics.

\paragraph{N-gram based metrics} Classical machine translation metrics~\cite{Popovic2015,Papineni2002,Lin2004,Banerjee2005} usually focus on n-gram overlap between the reference translation and the translation generated by the system. Although by design this metrics fail to recognize semantic similarity beyond the lexical level, they remain widely adopted in academic research. In this work, we incorporate the two most common metrics: BLEU~\cite{Papineni2002} and ROUGE~\cite{Lin2004}.


%\textbf{Embedding-based:} These metrics assess the semantic similarity between the reference and generated text by comparing their embeddings, and show a higher correlation with human judgments.~\cite{Servan2016,Zhang2020,Tattar2017,Lo2019}.  In this research, we include BERTScore~\cite{Zhang2020}.\\

\paragraph{Learnable metrics}
These methods focus on directly learning human judgment through training. 
For this family, we include COMET~\cite{Rei2020}, which leverages a pre-trained multilingual model. Although COMET can function as a reference-less metric, in this work, we report results exclusively for its direct assessment variant.

%BLEURT~\cite{Sellam2020}, a metric based on the BERT model, specifically trained to mimic human judgment. We also include 


\paragraph{Language variety metric}
To assess if the text produced by our translation system is indeed in European Portuguese, we use a Portuguese language variant classifier~\cite{Sousa2025}, which distinguishes between Brazilian and European Portuguese.
After translation of the benchmark data, we employ the classifier to label all generated texts and to compute the percentage of documents that are labeled as European Portuguese. 
Since the classifier might contain intrinsic errors and bias, we also compute the percentage of documents labeled as European Portuguese in the reference translations. 
This step is crucial for handling cases where the translated text for the two variants might be identical. 
In such scenarios, the classifier might incorrectly classify the variety as the wrong one due to the lack of distinguishing features in the text. 
By comparing the results of our system with the reference translations, we can correct for this potential bias and obtain a more accurate assessment of how well our system produces European Portuguese. 
We refer to the ratio of these two percentages as the VID score, which serves as a measure of the system's effectiveness in generating European Portuguese text.

\subsection{Test Benchmarks}
As a low-resource language variant, the number of benchmarks that include European Portuguese is limited. 
In this study, we use two high-quality publicly available datasets that feature this variant:
\begin{itemize}
    \item \textbf{FRMT}: This dataset is specifically designed to contain regional variants of Portuguese and Chinese~\cite{Riley2023}, containing human translations of sentences from English Wikipedia articles that were manually translated to European and Brazilian Portuguese. 
    
    \item \textbf{NTrex}: The dataset consists of high-quality translations by speakers who are bilingual in English and in one of the 128 target languages, including 123 documents and 1,997 sentences for each language~\cite{Federmann2022}.
\end{itemize}

\subsection{Baselines}
We compare our models to three sets of baselines:

\paragraph{Closed Baselines}
We include industry-standard systems for Portuguese translation, including Google Translate and DeepL. Recently, Google Translate introduced a model specifically designed for European Portuguese, referred to as Google$_{pt}$, which we include in our evaluation alongside the original model that does not distinguish between Portuguese varieties, referred to as Google$_{br}$.

\paragraph{Open Baselines}
For open-source systems, we evaluate ArgosTranslate\footnote{\url{https://github.com/argosopentech/argos-translate}}, which uses OpenNMT~\cite{Klein2017} as its backend. Although Portuguese is listed as a supported language, the specific variety is not indicated. We also consider the Opus-MT project~\cite{Tiedemann2020}, another open-source system that provides a model for translating from English to Portuguese. However, like ArgosTranslate, this system does not differentiate between Portuguese varieties.

\paragraph{Zero-shot}
Additionally, we assess the zero-shot capabilities of language models without applying our task-specific fine-tuning to demonstrate the effectiveness of the fine-tuning process.

\subsection{Implementation Details}

All models were trained and evaluated on a server with six A-100 GPUs, each with 40GB of memory. The batch size and training duration varied depending on the memory requirements of each model. In our repository, we provide training and evaluation scripts compatible with the two libraries used to train the language models: \texttt{torchtune}\footnote{\url{https://github.com/pytorch/torchtune/}} and \texttt{transformers}\footnote{\url{https://huggingface.co/}}. While the \texttt{transformers} library was chosen for its practicality, we found it limiting when training larger models. In that scenario, \texttt{torchtune} presented as a reliable alternative with significantly better memory management.
Training runs we executed with early stopping -- using the test set the DSL-TL corpus as validation set -- with patience of 3,000 steps. As a result, the number of training steps varied across models. All LoRA variants were trained with an alpha of 128 and a rank of 256. Detailed training configurations can be found in our repository.

The parameter setup is as follows:

\begin{itemize}
    \item \textbf{Phi-3:} For both the LoRA and full fine-tuning of Phi-3 models, we used a batch size of 512, a learning rate of 2e-5, a weight decay of 0.1, and a warm-up of 1,000 steps.
    \item \textbf{Gemma-2:} For both variants, the learning rate was set to 2e-5 with a weight decay of 0.1. The full fine-tuned model was trained with a 1,000-step warm-up and a batch size of 512, while the LoRA variant had 500 warm-up steps and a batch size of 256.
    \item \textbf{LLaMA-3:} Both variants were trained with a batch size of 256 and a learning rate of 2e-5. The LoRA variant additionally includes a warm-up of 100 steps and a weight decay of 0.1 on the learning rate.
\end{itemize}

\subsection{Results}
\label{sec:results}
\input{tables/results}
%\input{tables/samples2}

The result of our evaluation is shown in Table~\ref{tab:results}, where the best overall values are marked with an underline, and the most effective open-source systems are marked in bold. 
The general trend on both datasets is quite similar and high-performing models maintain a stable performance across both benchmarks. 
Yet, values for NTrex are slightly below FRMT for all systems, indicating a harder benchmark.
In the following, we describe our main findings and highlight avenues for future research.

\paragraph{LoRA models:} These variants effectively learn the vocabulary of European Portuguese, but their overall translation quality remains subpar. 
This discrepancy is evident as they score highly on the VID metric, nevertheless, the BLEU, ROUGE-L, and COMET metrics suggest that they struggle with generating high-quality text. 
Upon closer inspection of the generated translations, we found that both the Phi-3 and LLaMA-3 models, when trained with LoRA, tend to enter a repetition loop, where the same token is generated repeatedly until the process is interrupted. 
This suggests that for a medium size language model translation is a complex task, and simply adding adapter parameters is insufficient to fully capture its nuances. 
This issue, combined with the fact that the early stopping criteria were reached quickly during training (training loss had plateaued while the evaluation was increasing), suggests that the models may require increased capacity (by adjusting the alpha and rank parameters) to better learn from the training data.

\paragraph{Full fine-tuning (FFT):}
The fully fine-tuned models sacrifice their mastery of European Portuguese nuances in favor of high-quality, coherent translations. 
These models yield more moderate scores for the VID metric while achieving significantly higher text quality metrics. 
In particular, the fine-tuned LLaMA-3 model beats all open source software on all metrics and produces results comparable to Google$_{br}$ in terms of BLEU and ROUGE-L on both benchmarks, only falling short on the COMET metric. 

It is important to note that the COMET metric, as a learnable metric, is subject to the same biases that affect any trainable neural model. Since the training data does not differentiate between European and Brazilian Portuguese and most open-source resources are skewed toward Brazilian Portuguese, this bias may have influenced the scores produced by this metric.
The slight difference between Google$_{br}$ and fine-tuned LLaMA-3 models suggests a potential bias in the COMET score toward Brazilian Portuguese.

The overall performance of the fully fine-tuned models has a direct correlation with model size, where our largest model, LLaMA-3, with 8 billion parameters outperforms the smaller models of Phi-3 (3.8 billion parameters) and Gemma-2 (2 billion parameters). 
This behavior is typical for large language models and indicates that increasing their size can lead to better results. This indicates that by applying this methodology to even larger models, it may be possible to achieve results that are on par with industry-standard systems. % However, our study focuses on smaller language models and data generation through back translation, rather than merely scaling up the model size.


Since our focus is on translation specific to European Portuguese, it is important to examine the VID scores across both benchmarks.
Our best model, LLaMA-3, once again beats all open source baselines and achieves scores significantly higher than Google$_{br}$ and is comparable to Google$_{pt}$ and DeepL. This suggests that the proposed methodology is effective in achieving the targeted goal of producing text specific to a language variety. 
Even our smaller-size models, perform comparable to open-source baselines on translation metrics, dramatically improving on the VID score. 

It is true that in terms of text quality metrics, the LLaMA-3 model still lags behind the European Portuguese-specific industry models, namely Google$_{pt}$ and DeepL. However, it is important to emphasize that our goal was not to beat the specialized model from industry, but to propose a computationally efficient, adaptable, and resource-efficient method for adapting small language models to translate specific language varieties. Surpassing the current open-source software and achieving a score close to industry-level models, which benefit from dedicated teams of experts and annotators for each language, is a significant accomplishment.
