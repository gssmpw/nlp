\section{PTradutor}
\label{sec:ptradutor}

As noted previously, a significant challenge in training a translation model for a specific language variety is the scarcity of datasets tailored for this purpose. To overcome this obstacle, we devised a three-step approach to automatically generate a parallel corpus. Industry-grade translation systems typically achieve higher translation quality when translating \emph{into} a resource-rich language, such as English, rather than a low-resource language, such as European Portuguese. We exploit this concept to transform a monolingual European Portuguese corpus into a high-quality parallel corpus, using the following method:


\begin{enumerate}
    
    \item \textbf{Mono-lingual Corpus:} We compiled a large mono-lingual collection of texts written in European Portuguese.
    
    \item \textbf{Translation:} Using a translation system, we created a parallel corpus from the mono-lingual corpus, by translating from European Portuguese to English.
    
    \item \textbf{Filtering:} We conducted filtering and quality checks to ensure the integrity of the dataset.
\end{enumerate}


This process creates a corpus with parallel data pairs for English and European Portuguese, enabling the training of MT systems. In the following sections, we describe each step in detail.


\subsection{European Portuguese Corpus Collection}
For the collection of texts in European Portuguese, we used two sources: the DSL-TL~\cite{Zampieri2024} and the PtBrVid corpus~\cite{Sousa2025}. 
The DSL-TL corpus comprises a total of 4,458 news articles\footnote{Although the authors report 4,953 articles in their paper, the files shared on the project repository \url{https://github.com/LanguageTechnologyLab/DSL-TL} contain 4,458 rows.} written in Portuguese, manually annotated as ``European Portuguese'', ``Brazilian Portuguese'', or ``Both''. 
For our purposes, we use the train partition of this dataset and keep the texts labeled as ``European Portuguese'' and ``Both'' as they are both valid texts in European Portuguese, resulting in a total of 1,734 documents.

The larger chunk of our data comes from the PtBrVid corpus, which was originally created to train a Portuguese variety identifier that discriminates between European and Brazilian Portuguese. 
This corpus was constructed by compiling several datasets for each variety across six domains -- journalism, web, social media, literature, legal, and politics -- using metadata from the original datasets to label the variety of its texts. 
For example, the political subset uses the CETEM Público~\cite{Rocha2000} corpus for European Portuguese and the CETEM Folha\footnote{\url{https://linguateca.pt/cetenfolha/index_info.html}} for Brazilian Portuguese. 
For this research, we kept all entries that were labeled as ``European Portuguese''.

\subsection{Translation}

Monolingual data was translated into English using Google Translate with the Python library \texttt{ deep\_translator}\footnote{\url{https://github.com/nidhaloff/deep-translator}}. The reason for this choice is the accessibility and relatively good quality of Google Translate, however, this step can be achieved by other translation systems. When this dataset was created, Google Translate did not distinguish between European and Brazilian Portuguese. Consequently, using it as the translation engine assumes that the translation from European Portuguese to English is unaffected by this limitation. 

To verify this hypothesis, we conducted an experiment using one of our test datasets, namely FRMT~\cite{Riley2023}, which contains 2,616 examples in English, Brazilian Portuguese, and European Portuguese.  We used Google Translate to translate the European Portuguese and Brazilian Portuguese texts into English and then compared the English translations produced from the different varieties.  We found that Google Translate produced exactly the same translation for approximately $87.2\%$ of the dataset and achieved a BLEU score of $96.8\%$.  This confirms that the quality of the dataset is only mildly affected by Google’s lack of differentiation between varieties. Nonetheless, this assumption is language-specific and might not hold for different language varieties and different translation systems. 


\subsection{Filtering}
The aggregation of the mentioned resources results in a total of $3,966,538$ documents. 
As previous research focused on training and fine-tuning LMs has emphasized the importance of data quality~\cite{Wenzek2020,Penedo2024}, we apply extensive filtering and quality checks to our dataset. 

Specifically, our filtering pipeline begins by using the \texttt{jusText}\footnote{\url{https://github.com/miso-belica/jusText}} library -- designed to clean boilerplate content by classifying textual blocks based on various features such as length and stop word density --  to remove entries classified as low-quality text. As depicted in Figure~\ref{fig:filter_pipeline}, this is the most stringent filter in our process, eliminating approximately 1.9 million documents from the collection. A closer examination reveals that most of the discarded documents originated from the Social Media partition of the PtBrVId corpus, which initially contained 2,014,752 documents, of which only 260,315 remained after applying this filter.

To further refine the dataset, we removed all entries for which the Portuguese text were duplicated, contained ASCII characters that are not used in the Portuguese language (like ``ø'' or ``ž''), or included repetitive templates that were over-represented in the dataset (steps ``Duplicates'', ``Invalid Chars'' and ``Patterns'' in Figure~\ref{fig:filter_pipeline}). For example, we identified 1,132 documents that began with ``Lista de alterações recentes'' (``List of recent changes'' in English) from a badly scrapped page and 221 documents that started with ``Filtrar por'' (``Filter by''), referring to search engine filters.

Additionally, we filtered out all documents exceeding 900 tokens in the combined source and target texts\footnote{Token count was determined using the \texttt{LLama3} tokenizer.}. This ensures that, regardless of the prompt template used during training, no entry exceeds 1024 tokens. The data loss from this step was minimal, as the dataset primarily consists of single-paragraph documents without line breaks. In fact, documents exceeding 900 tokens accounted for only 0.54\% of the entire dataset. However, removing these documents significantly improved training speed by enabling a larger batch size. When deploying this model, it is crucial to be aware of this limitation and implement strategies to handle larger inputs, such as sentence splitting.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{imgs/filter_pipeline.eps}
    \caption{Number of documents (in millions) remaining after each step of our filtering pipeline.}
    \label{fig:filter_pipeline}
\end{figure}

The final step of our filtering pipeline involves a series of miscellaneous checks, such as ensuring that each text contains a matching number of opening and closing brackets and quotation marks. For detailed information about the operations performed, we refer to our code repository. 

We refer to the final parallel corpus as PTradutor. Table~\ref{tab:ptradutor} presents relevant statistics of the dataset after filtering, including the number of tokens and the domains covered. The code to replicate the dataset is available as open-source\footnote{\url{https://github.com/hmosousa/ptradutor}}, and the final corpus is publicly accessible on HuggingFace\footnote{\url{https://huggingface.co/datasets/hugosousa/PTradutor}}.

\input{tables/ptradutor}
