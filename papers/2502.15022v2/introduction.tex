\section{Introduction}

\begin{figure}[t]
    \centering
    \includegraphics[trim=0.75cm 0.75cm 0.75cm 0.75cm,width=0.95\columnwidth]{evalfig5.pdf}
    \caption{Sample from our new test set with human annotations and metrics for content preservation. S: style strength, C: content preservation. Bold indicates which of the output different metrics scored as the output where the content is best preserved. }
    \label{fig:sample}
\end{figure}

Large language models (LLMs) make it easy to rewrite text in a variety of styles or to change any text attribute, even without training data. Examples are to make a text more formal, polite, simplified, or to change its sentiment. However, the evaluation of style and attribute transfer is still challenging as it lacks validation and standardization \cite{ostheimer-etal-2023-call,briakou-etal-2021-review}. 

 In this paper, we present a large-scale study of the evaluation of content preservation for style transfer for various styles and more fundamentally, we question the meta-evaluation itself: 

Traditionally, style and attribute transfer have been evaluated in three separate dimensions: 1) style strength (did the style successfully shift?),  2) content preservation (is the content otherwise the same?) and 3) fluency (is the rewrite fluent and grammatically correct?) \cite{jin-etal-2022-deep}. Style transfer papers heavily rely on automatic metrics for evaluation \cite{ostheimer-etal-2023-call}:    

\textbf{Style Strength/Shift}: the de facto evaluation approach, also in recent work, uses a classifier, e.g. RoBERTa-based, trained on a specific style \cite{ lai-etal-2024-style,hallinan-etal-2023-steer,han-etal-2024-disentangled,mukherjee-etal-2024-large-language,liu2024adaptive,luo-etal-2023-prompt,zeng2024bat}. Its disadvantage is 1) that training data for the specific style is needed, limiting its scope and scale, and 2) it does not measure that a style can shift to differing degrees.  %This paper examine approaches that can measure style shifts to various degrees without training. 

\textbf{Content Preservation}: many of these metrics originate from other natural language generation tasks; the most commonly used is lexical similarity using BLEU \cite{ostheimer-etal-2023-call,jin-etal-2022-deep}, but many different metrics are used such as METEOR, BertScore, cosine distance between embeddings, and BLEURT \cite{ostheimer-etal-2023-call}.

However, using lexical or semantic similarity between 1) source and rewrite does not logically fit the task of content preservation in the case of style transfer: It does not condition on the style changes. Hence, these methods cannot distinguish between a change due to style or due to content error, and will, therefore, evaluate content preservation lower the greater the style shift. 

Using similarity metrics between 2) source and reference would require ground-truth references. Even if available, this approach suffers from the issue that style/attribute rewriting is a task where many different solutions may exist, which are not necessarily similar to a given reference.  

Prior work has already criticized and pointed out the flaws in using these metrics for content preservation \citep{lai-etal-2024-style,scialom2021rethinking,mir-etal-2019-evaluating,logacheva-etal-2022-study}. Nevertheless, they are still widely used \citep{lai-etal-2024-style,hallinan-etal-2023-steer,han-etal-2024-disentangled,mukherjee-etal-2024-large-language,liu2024adaptive,zhang-etal-2024-distilling,luo-etal-2023-prompt,zeng2024bat}, and in fact the semantic similarity metrics often show reasonable correlation with human judgment.

Recent work proposes prompting LLMs for evaluating different style transfer tasks \citep{ zeng2024bat,lai2023multidimensional,ostheimer-etal-2024-text}. However, correlation to human evaluations only shows results that are comparable, and not superior, to those obtained using semantic similarity metrics \citep{lai2023multidimensional, ostheimer-etal-2024-text}, despite the fact that they intuitively fit the task better because they take the style shift into account.

We posit that the status quo of evaluating these metrics does not tell the full story. There is a discrepancy between what logically would be reasonable metrics for content preservation and what the empirical results currently show. The current meta-evaluation approach is to test the metric's correlation to human judgement on datasets containing style transfer samples  - This implies that the meta-evaluation is limited to the distributions of these samples. Hence the correlation might be spurious and might not show the limits of the metrics. We propose a new test set better aligned with the aim of content preservation. A new understanding of the meta-evaluation is needed, and with this, a new evaluation across styles of the best metrics.   

\textbf{Our contributions:}
\begin{itemize}[noitemsep,nolistsep]
\item We benchmark 8 evaluation methods on a variety of tasks (9) and data (7);
\item We construct and annotate a new test set (500 samples) with errors in content and high style change; a new resource for evaluating content preservation metrics \footnote{\url{https://huggingface.co/datasets/APauli/style_eval_content_test}}
\item We show how meta-evaluation can lead to different conclusions when not considering the underlying distribution of test data;
\item We find that the metrics for content preservation for style/attribute transfer must be conditional on the style shift; otherwise, many similarity metrics show no correlation to human judgement on our test set; 
\item We propose an efficient, zero-shot method that conditions the style shift for content preservation and also for style strength, which uses the likelihood of the next token.   
\end{itemize}

