\appendix
\section{Data sets}
\label{app:dataset}
In table~\ref{tab:dataset}, we list the stats of datasets with human annotations used for benchmarking in this paper. 

The data form \citet{mir-etal-2019-evaluating} [Mir] is on the Yelp sentiment task and is annotated by 3 workers, where the mean ratings are released. Data is downloaded from  \url{github.com/passeul/style-transfer-model-evaluation}. No licence.

The data from \citet{lai-etal-2022-human} [Lai] supplied human annotations on system output on formality task, using a continuous scale from 1-100. Download at \url{https://github.com/laihuiyuan/eval-formality-transfer} with MIT License. 

The data from \citet{scialom2021rethinking} [Scialom] is on human ratings for human written output for a simplification sentence task. It is complimenting the the dataset from  \citet{alva-manchego-etal-2020-asset}. Download using the URL in the paper. No license specified. We have filtrated this data to obtain annotation in all three dimensions for the same data input (we check for an exact match on source sentence, rewrite, sentenceID), and we ended up with 65 samples annotated by 25 workers. 

The data from \citet{alva-manchego-etal-2020-asset} [Alva-m.] is system output on a simplification task. We use the resource released along with in \citet{scialom2021rethinking}, as it contains more meta-data, such as system information and more annotations. We filter the data such that we have 135 samples with 11 annotations in all three dimensions because we favour more samples over the number of annotations per sample. 

The data from \citet{ziegenbein-etal-2024-llm} [Ziegen] is on rewriting inappropriate arguments to appropriated, download available at \url{https://github.com/timonziegenbein/inappropriateness-mitigation}.

The data from \citet{cao-etal-2020-expertise} is human evaluation on a task between transferring different styles of expertise in the medical domain. The authors have kindly shared the data with human ratings. 

% \usepackage{booktabs}
\begin{table*}%[]
\centering
\begin{tabular}{@{}lllllllll@{}}
\toprule
\textbf{Abb.} & \textbf{Style} & \textbf{Dim} & \textbf{Support} & \textbf{\#Ann.} & \textbf{\#Sys.} & \textbf{Ref.} & \textbf{\begin{tabular}[c]{@{}l@{}}Rating \\ on ref.\end{tabular}} & \multicolumn{1}{c}{\textbf{Scale}} \\ \midrule
Lai & \begin{tabular}[c]{@{}l@{}}formal,\\ informal\end{tabular} & S,C,F & 640 & 2 & 8 & \Checkmark & \Checkmark & 1-100 \\
Mir & \begin{tabular}[c]{@{}l@{}}positive, \\ negative\end{tabular} & S,C & 2928 & 1* & 12 &  &  & 1-5 \\
Alva-M. & simplifying & S,C,F & 135 & 11 & 6 &  &  & 0-100 \\
Scialom & simplifying & S,C,F & 65 & 25 & 1 &  & \Checkmark & 0-100 \\
Ziegen. & \begin{tabular}[c]{@{}l@{}}appropriated \\ arguments\end{tabular} & S,C,F & 1350 & 5 & 6 & \Checkmark & \Checkmark & 1-5 \\
Cao & \begin{tabular}[c]{@{}l@{}}layman, \\ expert\end{tabular} & C & 3800 & 1 & 5 &  &  & 1-5 \\ \bottomrule
\end{tabular}
\caption{Stats on the dataset used for benchmarking style transfer metrics: \textbf{Dim}ensions which of (\textbf{S}tyle, \textbf{C}ontent preservation, \textbf{F}luency) are rated in the data, \textbf{Support}: numbers of rated samples, \#\textbf{Ann}otators: number of annotations per sample, \#\textbf{Sys}stems: number of different systems/settings (including references) used to produce the samples, \textbf{Ref}erence: is the data supplied with references to enable reference-based evaluation, \textbf{Rating on ref}erence: does the data have ratings on references. *Mir dataset is conducted with multiple annotators, but only the mean of the annotations is released.}
\label{tab:dataset}
\end{table*}

\section{Synthetic Generated Test Part}
\label{app:syn}
A part of our construct test set is synthetically generated using GPT-4o-mini from Openai. We display the prompts for obtaining the samples from our stepwise approach; we show it for the subpart of politeness:
\paragraph{Generating source data} 

prompt =\textit{'Please give me} \{number\} \textit{examples of impolite sentences from emails, and return only in json format with key "sentences"'}

\paragraph{Generate rewrites}
\begin{itemize}
    \item[1)]  \textit{"Please rewrite the following} \{number\} \textit{sentences to be very polite, return in JSON with key  'sentences:'}  \{list of sentences\}"
    \item[2)]  \textit{"Please rewrite the following} \{number\} \textit{sentences to be just a bit more polite, return in JSON with key  'sentences:'}  \{list of sentences\}"
\end{itemize}
\textbf{Genereate one content error in the 2. rewrites }
\begin{itemize}
    \item[a)] "\textit{Please rewrite the following } \{number\} \textit{sentences staying as close to the original wording as possible but make a mistake in the content by substituting some key information, return in json with key  'sentences:'} \{list of sentences\}
\item[b)]  \textit{"Please rewrite the following }\{number\} \textit{sentences staying as close to the original wording as possible but make a mistake in the content by omitting some key information, return in json with key  'sentences:'} \{list of sentences\}
\item[c)]  \textit{"Please rewrite the following} \{number\} \textit{sentences staying as close to the original wording as possible but make a mistake by adding a very short extra detail, return in json with key  'sentences:'} \{list of sentences\}
\end{itemize}
\section{Annotation Guide and Process}
\label{app:anno}
\subsection{Annotaion Procedure}
We recruit annotators through the crowd-sourced platform prolific.com. We use workers whom we have experienced delivering high-quality in a previous annotation study. In total, we use five different workers, all from the UK. The workers are paid a fixed amount per task, which Prolific considers a 'great' hourly pay. Considering the completion times, the average hourly pay to the workers was $16.2$ GPB.  

One of our subtasks involves detoxifying toxic content; we warn the workers of potentially disturbing content before they select the task.


Full annotation guidelines are below, and a screenshot of the annotation interface with a sample is in Figure~\ref{fig:screenl}. We use Google Forms as our annotation tool. 
\begin{figure}
    \centering  \includegraphics[width=\linewidth]{screensh.png}
    \caption{Screenshot of annotation tool, Google Forms}
    \label{fig:screenl}
\end{figure}


\subsection{Annotation Guideline}
\textbf{Evaluating Rewrites to Change Style/Attribute}

In this task, your goal is to help us evaluate sentences that have been paraphrased or rewritten to modify a specific style or attribute. These attributes may include tone, formality, positivity, politeness, or personalization, among others. For example, a sentence might be rewritten to add a positive sentiment or to simplify the text for a younger audience.

When a text is rewritten to modify its style or attribute, it is important that the original content unrelated to the intended change remains intact. For example, no important information should be dropped, new information should not be fabricated, and the rewritten sentence should not mix up or misrepresent factsâ€”except where necessary to achieve the requested change.

We will provide you with an original sentence and a rewritten version, along with the intended style or attribute change. Note that the style/attribute may vary across different examples during the study, so please pay attention to the provided description for each pair.

For each sentence pair, you will answer two evaluation questions using a 5-point Likert scale from 'Very poor' to 'Very good':
\begin{itemize}
    \item [B)] Style/Attribute Change: Evaluate how well the intended style or attribute change is achieved in the rewritten sentence.
    \item [A)] Content Preservation: Evaluate how well the meaning/content unrelated to the style or attribute change is preserved in the rewritten sentence.
\end{itemize}
The study will provide you with a link to a Google Form, where there will be 100 samples to evaluate. The study is estimated to take 35 minutes.

The annotation will be part of a research project for my PhD.

Thank you for considering participating. 

\section{Methods Implementation}
\label{app:method}
\subsection{Our method}
We experiment with the backbone models
 \textsc{meta-llama/Llama-3.2-3B-Instruct} and \textsc{meta-llama/Llama-3.1-8B-Instruct} downloaded from \url{https://huggingface.co/meta-llama}. 
 
 We set the system prompt to \textit{"You can repeat sentences, paraphrase sentences or rewrite sentences to change the style or certain attribute of the text while preserving non-related content and context. Your answers contain just the rewrite."}. In the ablation, we change the system-prompt to the default \textit{You are a helpful assistant}. We also change minor wording in instructions, e.g. by emphasising the repeat, paraphrase and style using *.
 
\subsection{Metrics}
\textbf{BLEU} \cite{papineni-etal-2002-bleu} we use the python package NLTK implementations of BLEU with default settings.

\textbf{Meteor} \cite{banerjee-lavie-2005-meteor} we use the python package from Huggingface evaluate with default settings. 

\textbf{BertScore} \cite{zhang2019bertscore} We use the implementation from \url{https://github.com/Tiiiger/bert_score} with the current recommended backbone model \textsc{microsoft/deberta-xlarge-mnli}.

\textbf{BLEURT} \cite{sellam-etal-2020-bleurt} we use the python implemention from \url{https://huggingface.co/Elron/bleurt-large-512} using Huggingface Transformer libary with the backbone model \textsc{Elron/bleurt-large-512}.

\textbf{Cosine similarity embeddings} we use the SentenceTransformer library with Labse embeddings \textsc{sentence-transformers/LaBSE}, \cite{feng-etal-2022-language}.

\textbf{QuestEval} we use the implementations from \url{https://github.com/ThomasScialom/QuestEval}.

\subsection{Prompting Llama3 70B as Evaluator}
We prompt \textsc{meta-llama/Meta-Llama-3.1-8B-Instruct} using API calls to replicate.com. 
we use the following prompt:
\begin{itemize}
    \item \textit{"Evaluate the following completion of a task where a 'source sentence' has been rewritten to be more} \{style\} i\textit{n the style, denoted 'target sentence', Ideally the context and content in the sentence which does not relate to the style should be preserved. Please evaluate on a Likert scale from 1-5 with 5 being the best: 1) how well the meaning is preserved and 2) how well the the style is changed. Return in JSON format with the keys 'meaning' , 'style'. Given the 'source sentence':} \{source sentence\} \textit{'target sentence':} \{rewrite\}"
\end{itemize}
 It occurs rarely that the LLM does not provide an answer in the right format. In these cases, we provide the mean rating of the datasets. The number of occurrences is reported in Table~\ref{tab:fail}.  

 In ablation, we change minor phrasing or wording and ask for an explanation of the given scores.

\begin{table}%[]
\centering 
\begin{tabular}{@{}llll@{}}
\toprule
\textbf{task} & \textbf{style} & \textbf{con} & \textbf{out of} \\ \midrule
Mir & 13 & 13 & 2928 \\
Lai & 1 & 1 & 640 \\
Cao & 10 & 10 & 3800 \\
Ziegen & 4 & 4 & 1350 \\
Alva-M. & 0 & 0 & 135 \\
Our (ALL) & 0 & 0 & 50 \\ \bottomrule
\end{tabular}
\caption{Number of times where prompting Llama3 did not return scores in the requested format along with the total number of samples per dataset}
\label{tab:fail}
\end{table}



\section{Results}
\label{app:results}
\begin{table*}%[]
\centering 
\begin{tabular}{@{}lllllllllll@{}}
\toprule
\multicolumn{1}{c}{\textbf{task}} & \multicolumn{2}{c}{\textbf{Mir}} & \multicolumn{2}{c}{\textbf{Lai}} & \multicolumn{2}{c}{\textbf{Ziegen.}} & \multicolumn{2}{c}{\textbf{Cao}} & \multicolumn{2}{c}{\textbf{Alva-M.}} \\ \midrule
\multicolumn{1}{c}{\textbf{metrics}} & \multicolumn{1}{c}{\textbf{kendell}} & \multicolumn{1}{c}{\textbf{pval}} & \multicolumn{1}{c}{\textbf{kendell}} & \multicolumn{1}{c}{\textbf{pval}} & \multicolumn{1}{c}{\textbf{kendell}} & \multicolumn{1}{c}{\textbf{pval}} & \multicolumn{1}{c}{\textbf{kendell}} & \multicolumn{1}{c}{\textbf{pval}} & \multicolumn{1}{c}{\textbf{kendell}} & \multicolumn{1}{c}{\textbf{pval}} \\
\textbf{S-Bleu} & 0.61 & 0.01 & 0.36 & 0.28 & 0.87 & 0.02 & 0.8 & 0.08 & 0.6 & 0.14 \\
\textbf{S-Meteor} & 0.52 & 0.02 & 0.5 & 0.11 & 0.87 & 0.02 & 0.6 & 0.23 & 0.47 & 0.27 \\
\textbf{S-BertScore} & 0.61 & 0.01 & 0.71 & 0.01 & 0.87 & 0.02 & 0.8 & 0.08 & 0.47 & 0.27 \\
\textbf{S-Bleurt} & 0.55 & 0.01 & 0.79 & 0.01 & 0.47 & 0.27 & 0.8 & 0.08 & 0.6 & 0.14 \\
\textbf{S-Cos.emb} & 0.45 & 0.04 & 0.71 & 0.01 & 0.87 & 0.02 & 0.8 & 0.08 & 0.33 & 0.47 \\
\textbf{QuestE.} & 0.55 & 0.01 & 0.93 & 0.0 & 0.87 & 0.02 & 0.8 & 0.08 & 0.73 & 0.06 \\
\textbf{llama3} & 0.52 & 0.02 & 0.91 & 0.0 & 1.0 & 0.0 & 0.8 & 0.08 & 0.2 & 0.72 \\
\textbf{Our (3b)} & 0.64 & 0.0 & 0.86 & 0.0 & 0.87 & 0.02 & 0.8 & 0.08 & 0.47 & 0.27 \\
\textbf{Our (8b)} & 0.61 & 0.01 & 1.0 & 0.0 & 0.87 & 0.02 & 0.6 & 0.23 & 0.6 & 0.14 \\ \bottomrule
\end{tabular}
\caption{Kendelltau correlation on ranking the style transfer systems}
\label{tab:ranking}
\end{table*}

\begin{table*}[]
\centering 
\begin{tabular}{@{}cllllllllllll@{}}
\toprule
\textbf{task} & \multicolumn{2}{c}{\textbf{Mir}} & \multicolumn{2}{c}{\textbf{Lai}} & \multicolumn{2}{c}{\textbf{Zeigen}} & \multicolumn{2}{c}{\textbf{Cao6}} & \multicolumn{2}{c}{\textbf{Alva-M.}} & \multicolumn{2}{l}{\textbf{Our (ALL)}} \\
\textbf{metrics} & \multicolumn{1}{c}{\textbf{cor.}} & \multicolumn{1}{c}{\textbf{pval}} & \multicolumn{1}{c}{\textbf{cor.}} & \multicolumn{1}{c}{\textbf{pval}} & \multicolumn{1}{c}{\textbf{cor.}} & \multicolumn{1}{c}{\textbf{pval}} & \multicolumn{1}{c}{\textbf{cor.}} & \multicolumn{1}{c}{\textbf{pval}} & \multicolumn{1}{c}{\textbf{cor.}} & \multicolumn{1}{c}{\textbf{pval}} & \textbf{cor.} & \multicolumn{1}{c}{\textbf{pval}} \\ \midrule
\textbf{prompt 1} & 0.36 & 0.0 & 0.84 & 0.0 & 0.62 & 0.0 & 0.61 & 0.0 & 0.76 & 0.0 & 0.83 & 0.0 \\
\textbf{prompt 2} & 0.28 & 0.0 & 0.82 & 0.0 & 0.59 & 0.0 & 0.58 & 0.0 & 0.74 & 0.0 & 0.82 & 0.0 \\
\textbf{prompt 3} & 0.22 & 0.0 & 0.82 & 0.0 & 0.58 & 0.0 & 0.6 & 0.0 & 0.74 & 0.0 & 0.80 & 0.0 \\
\textbf{prompt 4} & 0.17 & 0.0 & 0.77 & 0.0 & 0.43 & 0.0 & 0.52 & 0.0 & 0.78 & 0.0 & 0.81 & 0.0 \\ \bottomrule
\end{tabular}
\caption{Runs with different prompts for Llama 3 70B prompt method}
\label{tab:promptAbliation}
\end{table*}


\begin{table*}[]
\centering 
\begin{tabular}{@{}cllllllllllll@{}}
\toprule
\textbf{task} & \multicolumn{2}{c}{\textbf{Mir}} & \multicolumn{2}{c}{\textbf{Lai}} & \multicolumn{2}{c}{\textbf{Zeigen}} & \multicolumn{2}{c}{\textbf{Cao6}} & \multicolumn{2}{c}{\textbf{Alva-M.}} & \multicolumn{2}{l}{\textbf{Our (ALL)}} \\
\textbf{metrics} & \multicolumn{1}{c}{\textbf{cor.}} & \multicolumn{1}{c}{\textbf{pval}} & \multicolumn{1}{c}{\textbf{cor.}} & \multicolumn{1}{c}{\textbf{pval}} & \multicolumn{1}{c}{\textbf{cor.}} & \multicolumn{1}{c}{\textbf{pval}} & \multicolumn{1}{c}{\textbf{cor.}} & \multicolumn{1}{c}{\textbf{pval}} & \multicolumn{1}{c}{\textbf{cor.}} & \multicolumn{1}{c}{\textbf{pval}} & \multicolumn{1}{c}{\textbf{cor.}} & \multicolumn{1}{c}{\textbf{pval}} \\ \midrule
\textbf{Instruct 1 (8b)} & 0.48 & 0.0 & 0.73 & 0.0 & 0.52 & 0.0 & 0.53 & 0.0 & 0.7 & 0.0 & 0.57 & \multicolumn{1}{c}{0.0} \\
\textbf{Instruct 2 (8b)} & 0.48 & 0.0 & 0.72 & 0.0 & 0.54 & 0.0 & 0.54 & 0.0 & 0.71 & 0.0 & 0.56 & 0.0 \\
\textbf{Instruct 3 (8b)} & 0.48 & 0.0 & 0.73 & 0.0 & 0.49 & 0.0 & 0.54 & 0.0 & 0.71 & 0.0 & 0.56 & 0.0 \\
\textbf{Instruct 4 (8b)} & 0.49 & 0.0 & 0.72 & 0.0 & 0.51 & 0.0 & 0.55 & 0.0 & 0.7 & 0.0 & 0.52 & 0.0 \\
\textbf{Instruct 1 (3b)} & 0.49 & 0.0 & 0.73 & 0.0 & 0.54 & 0.0 & 0.53 & 0.0 & 0.7 & 0.0 & 0.47 & 0.0 \\
\textbf{Instruct 2 (3b)} & 0.48 & 0.0 & 0.73 & 0.0 & 0.55 & 0.0 & 0.53 & 0.0 & 0.69 & 0.0 & 0.48 & 0.0 \\
\textbf{Instruct 3 (3b)} & 0.48 & 0.0 & 0.73 & 0.0 & 0.51 & 0.0 & 0.52 & 0.0 & 0.72 & 0.0 & 0.48 & 0.0 \\
\textbf{Instruct 4 (3b)} & 0.48 & 0.0 & 0.74 & 0.0 & 0.54 & 0.0 & 0.52 & 0.0 & 0.71 & 0.0 & 0.45 & 0.0 \\ \bottomrule
\end{tabular}
\caption{Different runs with our proposed method, variating minor wording and system-prompt }
\label{tab:abbour}
\end{table*}