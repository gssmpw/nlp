\section{Benchmarking}
%1.5
\subsection{Datasets for Benchmarking}
\label{sec:dataset}
We benchmark the metrics on existing datasets containing human evaluations on style strength and/or content preservation. We use datasets for rewriting tasks on simplifying sentiment, formality, and appropriate arguments. The data consists of system-written output and/or human-written references. The human rating is obtained on different scales, such as a 5-point Likert scale or ratings of 1-100. When benchmarking the metrics, we consider the Pearson correlation to the mean human judgement. We benchmark on the following datasets with the abbreviated names in []: 
\begin{itemize}[noitemsep]
    \item \citet{lai-etal-2022-human} [Lai] on formal/informal,
    \item \citet{mir-etal-2019-evaluating} [Mir] on positive/negative,
    \item \citet{alva-manchego-etal-2020-asset} [Alva-M.] on simplifying, 
    \item \citet{scialom-etal-2021-questeval} [Scialom] on simplifying,
    \item \citet{ziegenbein-etal-2024-llm} [Ziegen.] on appropriated arguments,
    \item \citet{cao-etal-2020-expertise} [Cao] on rewriting for layman/expert.
\end{itemize}
See App.~\ref{app:dataset} for details about the datasets.


We analyse the intercorrelations of the dimensions using Pearson correlation between the mean annotations (Table~\ref{tab:intercor}) -- we mostly see a significant positive correlation between the dimensions, but also a case of negative correlation between style and meaning preservation on sentiment on Mir. We hypothesise there are two effects in \textbf{intercorrelations}; the data happens to be such that: 1) ``successful'' output is successful in all dimensions, which would yield a positive correlation, 2) large style change affects content preservation, which would yield a negative correlation.

\begin{table}[t]
\fontsize{11pt}{11pt}\selectfont
\begin{tabular}{lccc}
\toprule
\textbf{Data} & \textbf{Cor. S-C} & \textbf{Cor. S-F} & \textbf{Cor. F-C} \\ \midrule
Lai & 0.387 & 0.692 & 0.705 \\
Mir & -0.342 & - & - \\
Alva-M. & 0.709 & 0.828 & 0.763 \\
Scialom & 0.471 & 0.673 & 0.592 \\
Ziegen. & 0.103 & 0.673 & -0.015*\\ \bottomrule
\end{tabular}
\caption{Correlation on mean annotations between dimensions. *not significant}
\label{tab:intercor}
\end{table}

\subsection{Our Constructed Test Set for Style Transfer Evaluation}

\textbf{Methodology}. We create test data with high variation on content preservation by constructing output with deliberate content errors, and output with large style shifts.
For each source sentence, we construct two output sentences: 
\begin{itemize}[noitemsep]%,nolistsep
    \item [\textbf{1)}]one where we preserve the content and shift the style to a large degree with more variation in the rewrite;
    \item [\textbf{2)}] one where we shift the style to a lesser degree, staying closer to the wording in the source sentence but, in addition, producing an error in the content. 
\end{itemize}
\textbf{Ideally,} we aim for data where all output sentences succeed on the style transfer, but where, for each source sentence, one output succeeds on content preservation and one does not, creating variation in the level of content preservation. 

\textbf{The methodology for adding content errors} is inspired by \citet{devaraj-etal-2022-evaluating} -- they analyse data on the task of simplifying text by categorising errors into a taxonomy of substitution, deletion and insertion. We construct errors with these categories in mind -- we substitute or swap key information, we drop key information, or we fabricate additional information not supported by the source sentence.

\textbf{In total}, our constructed test data covers six style/attribute tasks and consists of 500 samples, 100 of which are manually created and the remaining synthetically generated. All samples are annotated by three workers on a 5-point Likert scale on style strength and content preservation. 

\textbf{Synthetically generated}. We instruct an LLM (GPT-4o-mini, from openai.com) in steps to provide us with the decided transfer samples. In the first step, we ask the LLM to generate source sentences. Secondly, we prompt the LLM to transfer the source samples \textit{a lot more} to a new style in order to provide us with the first set of rewritten sentences. For the second set of rewrites, we a) prompt to transfer the source sentences \textit{a bit more} to a new style and b) take the transferred sentence from point a) and introduce a content error. We conduct this on four rewriting tasks: i) a headline to be more catchy; ii) an impolite sentence from an email to be polite; iii) a persuasive request to be more persuasive; and iv) a sentence with informal language with grammatical mistakes and internet slang to be formal. Prompt details in App.~\ref{app:syn}.    

\textbf{Manually generated}. We construct 100 transfer samples starting from a source sentence and manually construct the output sentences by adding errors, as in steps 1 and 2. In addition, we supply each sample with a reference to enable a small evaluation of reference-based methods. We construct rewrites on two tasks: Task 1) on sentiment (positive/negative). In this case, we use as source sentences headlines from Wiki News (Wikinews.org)\footnote{\url{megarhyme.com/blog/wikinews-dataset/},  Creative Commons Attribution 2.5} with minor modifications. Task 2) on detoxifying, and here we use toxic sentences from \citet{logacheva-etal-2022-paradetox}. 


\paragraph{Samples}.
We show one synthetically generated  sample in Fig.~\ref{fig:sample}, and below one manually constructed sample (sentiment transfer task; positive):
\begin{itemize}[leftmargin=*,noitemsep,nolistsep]
    \item Source sentence: \textit{President of China lunches with Brazilian President.}
    \item Output 1: \textit{The Great Presidents of China and Brazilian strengthen important ties over lunch.}
    \item Output 2: \textit{The President of China enjoys lunches with the Brazilian first lady.}
\end{itemize}


\paragraph{Human Evaluation}
We obtain a high level of human agreement on content preservation on our test set with a Krippendorf Alpha of $0.768$ and a good level of success in style transfer, with an average score on the 5-point Likert scale of $4.27$. Details divided into subparts are outlined in Table~\ref{tab:con_data}: we report the percentage of samples that at least obtain an average rating of 3 (fair) to show that most sentences succeeded in style transfer.

Three workers annotate each sample in batches corresponding to each subpart. Five different workers participated in total, recruited on a crowdsourcing platform.
Annotators are asked to rate content preservation and style change on a 5-point Likert scale, as in previous work \cite{mir-etal-2019-evaluating,ziegenbein-etal-2024-llm}. We use the scale
\begin{lstlisting}[basicstyle=\ttfamily,
  breaklines=true]
1: Very poor, 2: Poor, 3: Fair, 4: Good, 5: Very Good.
\end{lstlisting}
Details on annotation guideline, setup and payment in Annotation guideline in App.~\ref{app:anno}.
 
\begin{table}[t]
\begin{tabular}{llrlr}
\toprule
\multicolumn{1}{l}{\textbf{Task}} & \multicolumn{1}{l}{\textbf{Const.}} & \multicolumn{1}{r}{\textbf{Supp.}} & \multicolumn{1}{l}{\textbf{\begin{tabular}[c]{@{}c@{}}Con. \\ IAA \end{tabular}}} & \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}S>=3 \\(\%)\end{tabular}}} \\ \midrule
sentiment & m & 50 & 0.676 & 88 \\
detoxify & m & 50 & 0.758 & 96 \\
catchy & s & 100 & 0.806 & 90 \\
polite & s & 100 & 0.645 & 100 \\
persuasive & s & 100 & 0.80 & 88 \\
formal & s & 100 & 0.817 & 99 \\ \bottomrule
\end{tabular}
\caption{Stats on our constructed test set. Constructed m: manually, s: synthetic.}
\label{tab:con_data}
\end{table}
