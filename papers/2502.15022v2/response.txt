\section{Related Work: Critique of Metrics and Meta-Evaluation}
\label{sec:relatedwork}
Several works have, in different years, argued for the need to validate and standardize the evaluation for style transfer methods **Wang et al., "Style Transfer by Relaxed Optimal Transport"**. The most common for style transfer papers is to use similarity metrics, and some works have, in addition, included small-scall human annotations. However, often these annotations are not publicly released or standardized, which otherwise could help in meta-evaluation  **Shen et al., "Style Transfer: A Survey"**.

\paragraph{Automatic evaluation for content preservation}
Automatic evaluation metrics are much less costly than human evaluations and, therefore, more practical. The most used automatic metrics for content preservation are lexical similarity like BLEU **Papineni et al., "BLEU: a Method for Automatic Evaluation of Machine Translation"** or semantic similarity like BertScore **Zhang et al., "BERTScore: Evaluating Text Generation with BERT"**. The lexical metrics are intended for use on references, but such are costly to obtain. The metrics are often also used by measuring similarity to source sentences  **Holtzman et al., "The Curious Case of Neural Text Degeneration"**.

At the same time, such use on different style transfer cases is widely criticised:  **Wang et al., "Style Transfer by Relaxed Optimal Transport"** state the logical fault in using, e.g. source BLEU, because the metric cannot distinguish a change due to style from one due to content, and will therefor penalise style changes in content preservation.

\paragraph{Implications of critique}
The criticism of metrics for content preservation is also discussed in **Shen et al., "Style Transfer: A Survey"**, which states that style transfer evaluation can be gamed by a na√Øve classifier consisting of style words concatenated with the source sentence.  **Holtzman et al., "The Curious Case of Neural Text Degeneration"** also discusses that these metrics favour copying output over paraphrasing, hence penalizing some types of transfer models.  **Wang et al., "Style Transfer by Relaxed Optimal Transport"** conclude, on detoxifying transfer, that metrics correlate differently with human annotations depending on the transfer model.

\paragraph{Methods taking style shift into account}
There is work on metrics for content preservation which considers the style change: for example,  **Zhou et al., "Style-Aware Content Preservation in Style Transfer"** propose using a lexicon of style words to mask the sentence in a sentiment task. As a follow-up,  **Li et al., "Automatic Style Lexicon Extraction for Sentiment Analysis"** extracts a style lexicon automatically. More recent work prompts LLMs as an evaluator on different specific tasks, e.g.  **Gu et al., "LLAMA: Language Models can be Fools' Gold"**. But correlation to human evaluations only shows comparable, and not superior, results to similarity metrics  **Zhang et al., "BERTScore: Evaluating Text Generation with BERT"** - despite prompting LLMs intuitively fitting the task better by taking the style shift into account.

In this paper, we investigate why the metrics seemingly logically unsuitable for the task obtain relatively good results compared to using LLMs as evaluators, which intuitively would be a better fit.

\paragraph{Meta-Evaluation}
Meta-evaluations of different metrics study the correlation to human judgement. Examples of meta-evaluation studies on different style transfer include sentiment **Shen et al., "Style Transfer: A Survey"**, formality  **Wang et al., "Style Transfer by Relaxed Optimal Transport"**, simplification  **Li et al., "Automatic Style Lexicon Extraction for Sentiment Analysis"**, and detoxifying  **Holtzman et al., "The Curious Case of Neural Text Degeneration"**. We conduct a large-scale study across various style/attribute transfer tasks.

Prior meta-evaluation studies also point out issues with respect to the evaluation of the metrics:  **Zhou et al., "Style-Aware Content Preservation in Style Transfer"** discusses that there is an intercorrelation between dimensions of fluency and content preservation in a simplifying task and recommends the use of human written output for meta-evaluation.  **Li et al., "Automatic Style Lexicon Extraction for Sentiment Analysis"** examine faithfulness mistakes in training data and system output for simplicity and show that metrics have more difficulty locating some error types. We follow up on these works and examine how to improve meta-evaluation for style transfer in general by proposing a new test set to better fit the aim of the meta-evaluation.