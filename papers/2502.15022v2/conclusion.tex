\section{Conclusion}
We conduct a large-scale meta-evaluation of metrics for style transfer, focusing on content preservation. We construct a new test set with human evaluations. We hope our meta-evaluation can foster more research in evaluating metrics for style transfer. As well as to ensure fair evaluation of (new) methods for conducting style transfer. 

We show that the meta-evaluation of metrics for content preservation is not straightforward: taking correlations to human ratings is not sufficient to show metric performance; one needs to be aware of the underlying data distribution.

We demonstrate empirically, in line with intuition, that similarity metrics are not a good fit for content preservation in style transfer as they have trouble distinguishing between whether a change in a text is due to style or to an actual content error.

 Instead, metrics for content preservation should be conditional on the style change: We propose a new efficient metric for this and show it works better than similarity metrics when there are content errors and high style changes in the data. We also conclude that prompting a large instruct LLM (70B) for evaluation is showing the best result. 