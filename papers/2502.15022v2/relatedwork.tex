\section{Related Work: Critique of Metrics and Meta-Evaluation}
\label{sec:relatedwork}
Several works have, in different years, argued for the need to validate and standardize the evaluation for style transfer methods \cite{ostheimer-etal-2023-call,briakou-etal-2021-review, mir-etal-2019-evaluating}. The most common for style transfer papers is to use similarity metrics, and some works have, in addition, included small-scall human annotations. However, often these annotations are not publicly released or standardized, which otherwise could help in meta-evaluation  \cite{briakou-etal-2021-review}.  

\paragraph{Automatic evaluation for content preservation}
Automatic evaluation metrics are much less costly than human evaluations and, therefore, more practical. The most used automatic metrics for content preservation are lexical similarity like BLEU \cite{papineni-etal-2002-bleu} or semantic similarity like BertScore \cite{zhang2019bertscore}. The lexical metrics are intended for use on references, but such are costly to obtain. The metrics are often also used by measuring similarity to source sentences \cite{hu2022text,lai-etal-2022-human}. 

At the same time, such use on different style transfer cases is widely criticised:  \citet{mir-etal-2019-evaluating}  state the logical fault in using, e.g. source BLEU, because the metric cannot distinguish a change due to style from one due to content, and will therefor penalise style changes in content preservation.

\paragraph{Implications of critique}
The criticism of metrics for content preservation is also discussed in \citet{cao-etal-2020-expertise}, which states that style transfer evaluation can be gamed by a na√Øve classifier consisting of style words concatenated with the source sentence. \citet{lai-etal-2024-style}  also discusses that these metrics favour copying output over paraphrasing, hence penalizing some types of transfer models.  \citet{logacheva-etal-2022-study} conclude, on detoxifying transfer, that metrics correlate differently with human annotations depending on the transfer model. 

\paragraph{Methods taking style shift into account}
There is work on metrics for content preservation which considers the style change: for example, \citet{mir-etal-2019-evaluating} propose using a lexicon of style words to mask the sentence in a sentiment task. As a follow-up,  \citet{yu-etal-2021-rethinking-sentiment} extracts a style lexicon automatically. More recent work prompts LLMs as an evaluator on different specific tasks, e.g. \cite{zeng2024bat,lai2023multidimensional,ostheimer-etal-2024-text}. But correlation to human evaluations only shows comparable, and not superior, results to similarity metrics \cite{lai2023multidimensional,ostheimer-etal-2024-text} - despite prompting LLMs intuitively fitting the task better by taking the style shift into account. 

In this paper, we investigate why the metrics seemingly logically unsuitable for the task obtain relatively good results compared to using LLMs as evaluators, which intuitively would be a better fit.

\paragraph{Meta-Evaluation}
Meta-evaluations of different metrics study the correlation to human judgement. Examples of meta-evaluation studies on different style transfer include sentiment \cite{yu-etal-2021-rethinking-sentiment,mir-etal-2019-evaluating,ostheimer-etal-2024-text}, formality \cite{briakou-etal-2021-evaluating,lai-etal-2022-human}, simplification  \cite{scialom2021rethinking, alva-manchego-etal-2021-un, cao-etal-2020-expertise}, and detoxifying \cite{logacheva-etal-2022-study}. We conduct a large-scale study across various style/attribute transfer tasks.

Prior meta-evaluation studies also point out issues with respect to the evaluation of the metrics: \citet{scialom2021rethinking} discusses that there is an intercorrelation between dimensions of fluency and content preservation in a simplifying task and recommends the use of human written output for meta-evaluation. \citet{devaraj-etal-2022-evaluating} examine faithfulness mistakes in training data and system output for simplicity and show that metrics have more difficulty locating some error types. We follow up on these works and examine how to improve meta-evaluation for style transfer in general by proposing a new test set to better fit the aim of the meta-evaluation.