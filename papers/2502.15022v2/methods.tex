
\section{Automatic Evaluation Methods}
\subsection{Task}
We use the working definition from \citet{ jin-etal-2022-deep} of \textit{style transfer as a generation task aiming to control specific attributes of the text}. Note this definition of style transfer is data-driven and different from a linguistic definition. We use the term style and attribute transfer as the same as done in \citet{ jin-etal-2022-deep} and likewise note that the 'attribute' is a broader category that covers changes that affect semantics, where a linguistic understanding of 'style' would not, e.g. change of sentiment. Hence, the aim is to change the attribute/style while preserving the \textit{contextual or semantic} content. 

In this paper, we focus on evaluating \emph{content preservation and style strength: how well the content not attributed to the requested change is preserved and how well the style change succeeds}. Regarding style strength, we focus on benchmarking methods which can evaluate arbitrary style without training data - And output scores of multiple levels as opposed to the traditional binary approach, as recommended in \citet{briakou-etal-2021-evaluating}.  
%\paragraph{Style strength} We focus on methods which can evaluate arbitrary styles without training data. In addition, we focus on predicting different levels of style strength instead of the traditional approach of measuring style as a binary. This is recommended in \citet{briakou-etal-2021-evaluating}. 
\subsection{Metrics}
Many different metrics are used on evaluating style transfer \cite{ostheimer-etal-2023-call}. We examine (listed below) some of the more widely used ones that are still used in recent work such as in \citet{lai-etal-2024-style,hallinan-etal-2023-steer,han-etal-2024-disentangled,mukherjee-etal-2024-large-language,liu2024adaptive,zhang-etal-2024-distilling,luo-etal-2023-prompt,zeng2024bat}.

\paragraph{Content preservation} Some of the metrics we study are originally developed for other Natural Language Generation tasks and, like BLUE, are intended to measure rewritten sentences (output) against one (or several) gold standard sentences (references). Nevertheless, in the absence of such references, many use these metrics to compare source and output sentences. See also our discussion in Section~\ref{sec:relatedwork} for prior criticism on this. 

We categorize the metrics for content preservation into lexical similarity, semantic similarity, factual-based and style conditional metrics. We consider both similarities between output and source (source-based) and output and reference (reference-based) when applicable. We benchmark the following metrics (implementation details and prompt in App.~\ref{app:method}):

\begin{itemize}[noitemsep]
    \item {\ul{Lexical similarity:}} \textbf{BLEU} \cite{papineni-etal-2002-bleu} and \textbf{Meteor} \cite{banerjee-lavie-2005-meteor} -- both are n-gram match metrics originally developed for evaluating machine translation. 
    \item  {\ul{Semantic similarity:}} token-based semantic similarity metric \textbf{BERTScore} \cite{zhang2019bertscore}, \textbf{Cosine} similarity between sentence embeddings using \cite{feng-etal-2022-language}; the learned, also BERT-based, metric \textbf{BLEURT} \cite{sellam-etal-2020-bleurt}.
    \item {\ul{Fact-based:}} \textbf{QuestEval}: uses question generation and question answering to evaluate answers on source and output; originally developed for factual consistency and relevance in summaries \cite{scialom-etal-2021-questeval}, later extended to evaluating faithfulness in simplification tasks \cite{scialom2021rethinking}.  
    \item {\ul{Style conditional metrics:}} \textbf{Llama3} Instruct \cite{dubey2024llama} as an evaluator by prompting the model to return a score on a 5-point scale given source sentence, style and output sentence; \textbf{our method} using likelihood estimates, described in the next section. 
\end{itemize}



\subsection{Our Method} We argue, as others, \citep{mir-etal-2019-evaluating}, that the metrics for content preservation need to be conditional on the style shift; \citep{mir-etal-2019-evaluating} apply it to sentiment transfer. 
 In order to achieve this, we propose a new method for content preservation, which works for any style and is parameter-efficient, applying smaller LLM of 3B or 8B parameters, and works zero-shot. At the same time, we also propose a method for evaluating style change. 

We propose to use the likelihood of the tokens in the output sentence conditional on the source sentence and on a \textit{different task} (details below). The difference in the likelihood of the completion of different tasks can inform us of both style change and content preservation. Our inspiration is from the work of \citet{jia-etal-2023-zero}, who use likelihood estimate to evaluate faithfulness in summaries; here, we use the likelihood conditional on a style shift.

Let \( X \) be the source sentence consisting of tokens \( x_1, x_2, \dots, x_m \), and let \( \tilde{X} = \tilde{x}_1, \tilde{x}_2, \dots, \tilde{x}_n \) be the rewritten sentence that we want to evaluate with respect to some arbitrary style change \( S \). With an LLM, we can estimate the probability of the output sentence when the model has seen the source sentence and some task instruction \( T \), denoted as \  \( P^{LM}(\tilde{X}|X,T) \). We can utilize this in the following way:

\paragraph{Content Preservation}
Our requirement is to measure content preservation with respect to the style change, such that the content not related to the style should be preserved. Please note that content preservation could also be high without a style change. Hence, the measure of content preservation should be high if one of the following conditions is likely: 1)  the tokens of \(\tilde{X}\) are likely a rewrite with respect to the style $S$, or 2)  the tokens of \(\tilde{X}\) are likely a paraphrase of $X$, or 3) the special case that the tokens of \(\tilde{X}\) are likely a repetition of $X$. This implies that content \textit{is not} well preserved if the likelihood of a token is low in all three cases. Hence, if the token can neither be attributed to repetition, paraphrasing, nor requested style change. To weight these cases with tokens \textit{not} preserving content, we take the log of the most likely token of our three cases and define our measure as follows: 
\begin{align}
    C = \frac{1}{n} \sum_{i=1}^{n} \log \left( \max \left( p_{i}^{t^s}, p_{i}^{t^{pa}},p_{i}^{t^r}  \right) \right)
\end{align}
with 
\vspace{-1em}
\begin{align*}
    p_{i}^{t^s}& = p^{LM}(x_i | X, \tilde{x}_{<i}, t^s), \\
    p_{i}^{t^{pa}}& = p^{LM}(x_i | X, \tilde{x}_{<i}, t^{pa}), \\ p_{i}^{t^{r}}& =p^{LM}(x_i | X, \tilde{x}_{<i}, t^r)
\end{align*}
and $t^s$,$t^{pa}$,$t^r$ being instructions on rewrite with respect to style, paraphrase and repeat.
\paragraph{Style} To measure how well the style changes, we check 1) how likely the tokens in \(\tilde{X}\) are a rewrite with respect to the style, and 2) how likely the tokens in \(\tilde{X}\) are a paraphrase or repetition without conditioning on the style. The hypothesis is that the more a token contributes to the style change, the more likely it is that the likelihood of it is higher when the instruction is on style than otherwise. Hence, to provide us with a measure of style change, we take the difference in the likelihood between `style rewrite' and `paraphrase/repetition': 
\begin{align}
    S=\frac{1}{n}\sum_{i=1}^{n} \left(p_{i}^{t^s} - \max \left(p_{i}^{t^ta},p_{i}^{t^r}  \right)\right)
\end{align}
\paragraph{Implementation}
We experiment with two backbone model sizes, one with 3B and one with 8B parameters, and use Llama 3 Instruct models \cite{dubey2024llama}. We use the following instructions:
\\
\noindent $X,t^{s}$: \textit{`Rewrite the following sentence to be $S$: $X$}'\\
\noindent $X,t^{pa}$: \textit{`Paraphrase the following sentence: X'}\\
\noindent $X,t^{r}$: \textit{`Repeat the following sentence: $X$'}\\
Further details in App.~\ref{app:method}.
    