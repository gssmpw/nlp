\section{Appendix}

This appendix provides detailed experimental results and offers further discussion.

\subsection{Additional Statistics of Datasets \& Databases}

\begin{table}[ht]
\centering
\scalebox{1}{
    \begin{tabular}{cc}
    \toprule
    \textbf{Question Type} & \textbf{Dist. (Train/Dev/Test)} \\ \toprule
    conjunction & 42.00 / 43.59 / 44.60 \\
    composition & 47.27 / 44.76 / 43.78 \\
    comparative & 5.54 / 6.22 / 6.03 \\
    superlative & 5.19 / 5.43 / 5.58 \\ \bottomrule
    \end{tabular}
}
\caption{Distribution (Dist.) of question types in the original CWQ.}
\label{app_tab:distribution_qtype_cwq}
\end{table}

Table \ref{app_tab:distribution_qtype_cwq} outlines the distribution of question types in the original CWQ.


\subsection{Tool Implementation Details}
\label{app_sec:tool_imp_details}

\textbf{Tool Implementation}.
In the development of the \textbf{SearchNodes} tool, Elasticsearch\footnote{\url{https://github.com/elastic/elasticsearch}} is employed to extract all node surface names from the Freebase and MetaQA databases, and vector search techniques are implemented to perform queries on nodes within Wikidata. 
For the ranking algorithm of the \textbf{SearchGraphPatterns} tool, vector retrieval methods are similarly employed. All processes related to vectorization utilize the OpenAI \texttt{text-embedding-ada-002} API to generate vectors and employ Chroma\footnote{\url{https://github.com/chroma-core/chroma}} for indexing and searching.
Moreover, for the functionality of the \textbf{ExecuteSPARQL} tool, Virtuoso\footnote{\url{https://github.com/openlink/virtuoso-opensource}} serves as the underlying graph query engine.
For both the SearchNodes and SearchGraphPatterns tools, the number of returned results is set to 10. 

\subsection{Open-source LLM Fine-tuning \& Deployment}
\label{app_sec:llm_fine_tuning}

\begin{table}[ht]
\centering

\begin{tabular}{lc}
  \toprule
  \multicolumn{1}{c}{\textbf{Parameter}} & \textbf{Value} \\ \toprule
  Batch Size (per GPU) & 8 (1) \\
  Gradient Accumulation Step & 1 \\
  Model Max Length & 4,096 \\
  Learning Rate & 1e-5 \\
  Weight Decay & 1e-3 \\
  Epoch & 3 \\
  Warm Up Step & 0 \\
  Zero Stage & 3 \\ \bottomrule
\end{tabular}

\caption{Hyper-parameter settings for fine-tuning open-source LLMs.}
\label{app_tab:hyper_deepspeed}
\end{table}

For our implementation, we utilize \texttt{Llama-3.1-8B-Instruct}\footnote{\url{https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct}} \citep{Aaron-etal-arXiv-2024-Llama3} as the base model. For the LLM agent, we perform supervised fine-tuning on this model using the hyperparameter settings detailed in Table \ref{app_tab:hyper_deepspeed}. For the reward model, we directly use the original instruction-tuned version without additional training.

The training process is optimized using DeepSpeed \citep{Rasley-Jeff-KDD-2020-DeepSpeed}, while inference is accelerated using vLLM \citep{Kwon-Woosuk-SOSP-2023-vLLM}. All experiments are conducted on 8 NVIDIA A100 80GB GPUs.

\subsection{Hyper-parameter Settings for MCTS}
\label{app_sec:hyper_mcts}

\begin{table}
\centering
  \scalebox{1}{

\begin{tabular}{lc}
  \toprule
  \multicolumn{1}{c}{\textbf{Parameter}} & \textbf{Value} \\ \toprule
  \multicolumn{2}{l}{\textit{MCTS}} \\
  Early Stopping Threshold (k) & 5 \\
  Maximum Simulation & 50 \\
  Depth Penalty & 0.1 \\
  Max Preferred Depth & 5/7 \\
  Maximum Rounds & 12 \\
  \multicolumn{2}{l}{\textit{Model}} \\
  n (agent) & 5 \\
  n (reward) & 10 \\
  Temperature (agent) & 1.0 \\
  Temperature (reward) & 0.7 \\ \bottomrule
\end{tabular}

}
\caption{Hyper-parameter settings for MCTS.}
\label{app_tab:hyper_mcts}
\end{table}

Table \ref{app_tab:hyper_mcts} outlines the hyper-parameter settings for MCTS. For the simulation process, we set the maximum number of simulations to 50. 
The search terminates early when k nodes generate the \texttt{Done} action.
Since our method evaluates intermediate states directly without requiring simulation to terminal states, each simulation only generates one node, effectively capping the maximum number of nodes at 50. For the Max Preferred Depth parameter, we use a value of 5 for WebQSP and CWQ datasets, while setting it to 7 for KQA Pro due to its more complex reasoning requirements.


\subsection{Prompt Text for Reward Model}
\label{app_sec:reward_prompt}

The reward model's prompt text comprises tool descriptions, examples, guidelines, and output format requirements. Following a discriminative paradigm, it is designed to assess whether the current state has successfully identified necessary elements and resolved sub-questions.

Figures \ref{app_fig:prompt_fb_1}--\ref{app_fig:prompt_fb_2} and \ref{app_fig:prompt_wikidata_1}--\ref{app_fig:prompt_wikidata_3} showcase comprehensive instruction texts for Freebase and Wikidata, respectively.

% 1,2 page is the prompt text for Freebase, 3,4,5 page is the prompt text for Wikidata
\begin{figure*}[hp!]
  \centering
  \begin{tikzpicture}
    \node[inner sep=2pt] (image) at (0,0) {
      \includegraphics[width=1\textwidth,page=1]{img/prompt_text-cropped.pdf}
    };
    \draw[black, thin] (image.south west) rectangle (image.north east);
  \end{tikzpicture}
  \caption{Prompt text for Freebase (1/2).}
  \label{app_fig:prompt_fb_1}
\end{figure*}

\begin{figure*}[hp!]
  \centering
  \begin{tikzpicture}
    \node[inner sep=2pt] (image) at (0,0) {
      \includegraphics[width=1\textwidth,page=2]{img/prompt_text-cropped.pdf}
    };
    \draw[black, thin] (image.south west) rectangle (image.north east);
  \end{tikzpicture}
  \caption{Prompt text for Freebase (2/2).}
  \label{app_fig:prompt_fb_2}
\end{figure*}


% -------- Wikidata -------- %
\begin{figure*}[hp!]
  \centering
  \begin{tikzpicture}
    \node[inner sep=2pt] (image) at (0,0) {
      \includegraphics[width=1\textwidth,page=3]{img/prompt_text-cropped.pdf}
    };
    \draw[black, thin] (image.south west) rectangle (image.north east);
  \end{tikzpicture}
  \caption{Prompt text for Wikidata (1/2).}
  \label{app_fig:prompt_wikidata_1}
\end{figure*}

\begin{figure*}[hp!]
  \centering
  \begin{tikzpicture}
    \node[inner sep=2pt] (image) at (0,0) {
      \includegraphics[width=1\textwidth,page=4]{img/prompt_text-cropped.pdf}
    };
    \draw[black, thin] (image.south west) rectangle (image.north east);
  \end{tikzpicture}
  \caption{Prompt text for Wikidata (2/2).}
  \label{app_fig:prompt_wikidata_2}
\end{figure*}

\begin{figure*}[hp!]
  \centering
  \begin{tikzpicture}
    \node[inner sep=2pt] (image) at (0,0) {
      \includegraphics[width=1\textwidth,page=5]{img/prompt_text-cropped.pdf}
    };
    \draw[black, thin] (image.south west) rectangle (image.north east);
  \end{tikzpicture}
  \caption{Prompt text for Wikidata (3/3).}
  \label{app_fig:prompt_wikidata_3}
\end{figure*}



\subsection{Case Study}
\label{app_sec:case_study}

We selected representative examples from CWQ and KQA Pro datasets to demonstrate the effectiveness of our method in handling complex queries. For clarity and space constraints, the figures only display the terminal nodes and their corresponding branches, omitting intermediate nodes. 
In the figures, green boxes represent valid terminal nodes, where the last action is \texttt{Done} and the penultimate action is a ExecuteSPARQL with non-empty execution result. Correspondingly, yellow boxes indicate invalid terminal nodes.

In Figure \ref{app_fig:case_cwq}, we present an example from CWQ that involves complex logical structures, including a CVT\footnote{A Compound Value Type is a Type within Freebase designed to represent data where each entry is composed of multiple fields.} node and a one-hop relation. Despite the inherent difficulty in constructing such complex SPARQL queries, our method successfully generates the correct query through multiple exploration attempts.

In Figure \ref{app_fig:case_kqapro}, we showcase a case involving a branching decision at depth=3 for a UNION-type query. When two entities are identified and the predicate for one entity is known, the system explores two potential strategies: either assuming the same predicate applies to the second entity, or following the standard procedure to search for its predicate explicitly. This example demonstrates how our method successfully explores both approaches simultaneously, ultimately finding correct solutions through either path.

For additional visualized examples, please refer to our supplementary materials.


\begin{figure*}
  \centering
  \includegraphics[width=0.9\textwidth,page=1]{img/WebQTrn-1722_f4a32f9b5e628b4c25c759d940bfbab6.pdf}
  \caption{Example from CWQ.}
  \label{app_fig:case_cwq}
\end{figure*}

\begin{figure*}
  \centering
  \includegraphics[width=1\textwidth,page=1]{img/kqapro-val-9932.pdf}
  \caption{Example from KQA Pro.}
  \label{app_fig:case_kqapro}
\end{figure*}

