\section{Related Work}

Based on the \textbf{agent-environment framework}, recent state-of-the-art knowledge base question answering (KBQA) methods conceptualize large language models (LLMs) as agents and KBs as environments. This paradigm has been shown to enhance both reasoning efficiency and accuracy.
\citet{Gu-Yu-ACL-2023-Pangu} proposed that in few-shot scenarios, LLMs should prioritize evaluating the plausibility of agent plans rather than directly generating answers. 
Concurrently, \citet{Li-Tianle-ACL-2023-KB-BINDER} advocated for a two-stage approach: first generating logical forms as initial drafts, then refining them into executable queries using the KB.
\citet{Jiang-EMNLP-2023-StructGPT} developed two specialized interfaces for accessing the KB, while \citet{Gu-Yu-arXiv-2024-Middleware} and \citet{Liu-Xiao-ICLR-2023-AgentBench} designed seven tools to facilitate agent-environment interaction. \citet{Jiashuo-Sun-ICLR-2024-Think-on-Graph} introduced a novel approach that enables LLMs to perform iterative beam search reasoning on KBs.
However, these methods are fundamentally limited by their linear decision-making processes, which constrains the full potential of LLMs' reasoning capabilities.

\textbf{MCTS-based reasoning methods} have demonstrated remarkable effectiveness in mathematical tasks. The core challenge of these methods lies in designing appropriate reward functions.
A series of works has focused on training reward models. Given question-answer pairs, some approaches utilize MCTS to identify reasoning paths through weak supervision, scoring each step to obtain supervisory signals for intermediate reasoning steps. \citet{Zhang-Dan-NIPS-2024-ReST-MCTS} designed a distance-based reward rule and trained a reward model, while \citet{Chen-Guoxin-NIPS-2024-AlphaMath} augmented LLMs with linear layers, enabling parameter sharing between the value model and the reasoning LLM for joint training.
Additionally, reward models can be trained using step-wise (node-level) preference data. \citet{Xie-Yuxi-arXiv-2024-MCTS-Preference-Learning} employed Direct Preference Optimization (DPO) for iterative training, while \citet{Wang-Chaojie-arXiv-2024-Q-star} proposed step-wise value preference optimization.
Some works have also explored obtaining rewards directly from LLMs. \citet{Hao-Shibo-EMNLP-2023-Reasoning-via-Planning-RAP} utilized LLMs as world models to simulate subsequent states. \citet{Zhang-Di-arXiv-2024-Accessing-GPT-4-level-Mathematical} incorporated self-refinement for node expansion and employed direct prompting to obtain rewards. To address scoring instability, \citet{Zhang-Di-arXiv-2024-LLaMA-Berry} proposed using a pairwise preference reward model to compute partial ordering relations between nodes, ultimately deriving a global ranking.

Similar to our work, \citet{Zhou-Andy-ICML-2024-Language-Agent-Tree-Search} conceptualizes LLMs as agents and designs evaluation rewards based on self-generated and self-consistency scores.
However, their assumption that the environment knows the ground truth answer and can provide feedback about correctness is impractical in real-world scenarios.
\citet{Luo-Haoran-arXiv-2025-KBQA-o1} is a concurrent work that also attempts to apply MCTS to KBQA tasks. However, their approach differs significantly from ours in both action space design and reward function formulation. Their method requires training reward models, and relies on scoring the final logical form based on the entire trajectory, which we argue is suboptimal.
