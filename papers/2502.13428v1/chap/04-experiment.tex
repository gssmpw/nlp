\section{Experiment}

We examine MCTS-KBQA across a variety of complex question types and diverse databases (DBs).

% 4.1
\subsection{Dataset \& Preprocessing}

\begin{table}
\scalebox{0.8}{

\begin{tabular}{cccc}
    \toprule
    \textbf{Dataset} & \textbf{\begin{tabular}[c]{@{}c@{}}\#Instance\\(Anno/Extend/Test)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Raw \#Instance\\(Train/Dev/Test)\end{tabular}} \\ \midrule
    WebQSP & 100 / 2,734 / 300 & 3,098 / - / 1,639 \\
    CWQ & 200 / 2,843 / 600 & 27,639 / 3,519 / 3,531 \\
    KQA Pro & 450 / 9,150 / 900 & 94,376 / 11,797 / 11,797 \\ \bottomrule
\end{tabular}

}
\caption{Statistics of the datasets.}
\label{tab:data_statistics}
\end{table}

\textbf{WebQuestionsSP} (WebQSP) \citep{Yih-Wen-tau-ACL-2016-WebQSP} and \textbf{ComplexWebQuestions 1.1} (CWQ) \citep{Talmor-Alon-NAACL-2018-ComplexWebQuestions-CWQ} are extensively used in KBQA research. These datasets comprise natural language questions paired with their corresponding SPARQL queries based on Freebase \citep{Bollacker-Kurt-SIGMOD-2008-Freebase}. 
For WebQSP, questions can be categorized into 1-hop and 2-hop types according to the length of the inferential relation chain, which represents the path connecting the topic entity to the answer node.
CWQ extends WebQSP by incorporating four types of complex questions: Conjunction (Conj), Composition (Compo), Comparative (Compa), and Superlative (Super).

\textbf{KQA Pro} \citep{Cao-Shulin-ACL-2022-KQAPro} is a large-scale dataset designed for complex question answering over a dense set of Wikidata \citep{Vrandečić-Denny-CACM-2014-Wikidata}. It features nine types of complex questions, including Count (Ct), Query Attribute (QA), Query Attribute Qualifier (QAQ), Query Name (QN), Query Relation (QR), Query Relation Qualifier (QRQ), Select Among (SA), Select Between (SB), and Verify (Vf).


\subsection{Implementation Details}
\label{sec:imp_details}

For a fair comparison with previous work, we use the same training and test datasets as \citet{Xiong-Guanming-ACL-2024-Interactive-KBQA}. 
As shown in Table \ref{tab:data_statistics}, for the Freebase DB, we utilize 300 manually annotated (Anno) instances with intermediate reasoning processes as our training set. 
For the Wikidata DB, the training set consists of 450 instances. The test dataset contains 900 instances for each DB, sampled from the original dataset.
% Extend. 由于基于远程监督的推理路径寻找非常耗时, 且cwq 和 KQA Pro数据集由模板构建并且数量较大, 因此我们按照问题类型分类,在training set中随机采样10%的数据进行匹配,得到 the extended dataset.
Due to the time-consuming nature of distant supervision, and considering that questions of the same type in template-based datasets like CWQ and KQA Pro share similar patterns, we sampled 10\% of training data from each question type to build the extended dataset, with additional sampling for underrepresented types (Compa and Super) in CWQ.

By default, we fine-tune \texttt{Llama-3.1-8B-Instruct} \citep{Aaron-etal-arXiv-2024-Llama3} as the agent, while utilizing its original version as the reward model. 
For MCTS search, we set the sampling parameter $n$=5 for the agent, early stopping threshold $k$=5, decay coefficient $\gamma=0.1$, and expected depths $d_{exp}$ of 5 and 7 for WebQSP/CWQ and KQA Pro, respectively.
Additional implementation details and parameter settings can be found in Appendix \ref{app_sec:llm_fine_tuning}.

% 4.2
\subsection{Baselines}

\textbf{Linear decision methods}.
We select Interactive-KBQA \citep{Xiong-Guanming-ACL-2024-Interactive-KBQA} as the baseline. This method employs supervised fine-tuning (SFT) on an open-source LLM using the same limited training data, completing semantic parsing (SP) through multiple interactions. However, it only generates one action per decision step.

\textbf{Tree search methods}.
We employ Breadth-First Search (BFS) and Depth-First Search (DFS) as baselines. As two of the most common traversal algorithms, they treat all nodes with equal importance. We also explore random search as a baseline, with results reported in Section \ref{sec:cot_rule_analysis}.

\textbf{Fine-tuning on full training data}. 
We selected SP-based methods as baselines. UnifiedSKG \citep{Xie-Tianbao-EMNLP-2022-UnifiedSKG} introduces a multi-task prefix-tuning approach that unifies 21 structured knowledge grounding tasks across 6 domains into a text-to-text framework, fine-tuning a T5 model for semantic parsing. For KQA Pro, we opted for BART-SPARQL \citep{Cao-Shulin-ACL-2022-KQAPro}, which directly generates SPARQL queries from questions without requiring retrieval.

% 4.3
\subsection{Evaluation Metrics}

For semantic parsing-based methods that generate logical forms and produce unordered answer sets, we employ the F1 score as the primary evaluation metric. 
To provide comprehensive evaluation, we additionally report the Random Hits@1 (RHits@1) metric \citep{Shu-Yiheng-EMNLP-2022-TIARA} and the Exact Match (EM) score \citep{Talmor-Alon-NAACL-2018-ComplexWebQuestions-CWQ}. 
For the KQA Pro dataset specifically, we report accuracy, which is defined as the exact one-to-one correspondence between the predicted and ground truth answer sets.

% 4.4
\subsection{Results}

\begin{table*}
    \scalebox{0.84}{
    
\begin{tabular}{lcccccccccc}
\toprule
\multicolumn{1}{c}{\multirow{2}{*}{\textbf{Method}}} & \multicolumn{4}{c}{\textbf{WebQSP}} & \multicolumn{6}{c}{\textbf{CWQ}} \\ \cline{2-11} 
\multicolumn{1}{c}{} & \textbf{1-hop} & \textbf{2-hop} & \textbf{Overall} & \textbf{RHits@1} & \textbf{Conj} & \textbf{Compo} & \textbf{Compa} & \textbf{Super} & \textbf{Overall} & \textbf{EM} \\ \midrule
\multicolumn{11}{l}{\textit{Linear Decision}} \\
Prompting   w/ GPT-4 & 69.99 & 72.41 & 71.20 & 72.47 & 47.44 & 59.00 & 47.89 & 41.96 & 49.07 & 59.17 \\
SFT   w/ open-LLM & 61.61 & 53.43 & 57.52 & 59.85 & 36.00 & 35.56 & 46.94 & 50.34 & 42.21 & 48.83 \\
\quad   w/ vote & 63.54 & 56.50 & 60.02 & 61.14 & 39.08 & 37.04 & 60.56 & 60.71 & 49.35 & 53.33 \\ \midrule
\multicolumn{11}{l}{\textit{Tree Search}} \\
BFS & 67.80 & 66.97 & 67.39 & 67.77 & 60.44 & 48.84 & \textbf{74.14} & 61.43 & 61.21 & 70.50 \\
DFS & 69.50 & 68.35 & 68.93 & 70.26 & 56.96 & 48.20 & 71.83 & 59.96 & 59.25 & 67.01 \\
Ours & \textbf{72.44} & \textbf{72.54} & \textbf{72.49} & \textbf{73.45} & \textbf{65.10} & \textbf{57.58} & 72.37 & \textbf{64.38} & \textbf{64.87} & \textbf{72.12} \\
\quad w/ Golden Entity & 77.38 & 74.67 & 76.02 & 76.22 & 68.07 & 57.94 & 73.75 & 67.49 & 66.81 & 75.17 \\ \midrule \midrule
\multicolumn{1}{c}{\multirow{2}{*}{\textbf{Method}}} & \multicolumn{10}{c}{\textbf{KQA Pro}} \\ \cline{2-11} 
\multicolumn{1}{c}{} & \textbf{Ct} & \textbf{QA} & \textbf{QAQ} & \textbf{QN} & \textbf{QR} & \textbf{QRQ} & \textbf{SA} & \textbf{SB} & \textbf{Vf} & \textbf{Overall} \\ \midrule
\multicolumn{11}{l}{\textit{Linear Decision}} \\
Prompting   w/ GPT-4 & 74 & 83 & 64 & 73 & 73 & 59 & 80 & 61 & 80 & 71.89 \\
SFT   w/ open-LLM & 68 & 79 & 70 & 66 & 78 & 53 & 77 & 86 & 68 & 71.67 \\
\quad   w/ vote & 72 & 80 & 71 & 59 & 83 & \textbf{61} & 77 & 95 & 70 & 74.22 \\ \midrule
\multicolumn{11}{l}{\textit{Tree Search}} \\
BFS & 66 & 83 & 30 & 73 & 88 & 21 & 85 & 94 & 74 & 68.22 \\
DFS & 69 & 80 & 45 & 73 & 79 & 38 & 89 & \textbf{96} & 74 & 71.46 \\
Ours & \textbf{83} & \textbf{87} & \textbf{74} & \textbf{78} & \textbf{89} & 59 & \textbf{89} & 93 & \textbf{85} & \textbf{81.89} \\ \bottomrule
\end{tabular}

}
\caption{Main results on WebQSP, CWQ and KQA Pro.}
\label{tab:main_results}
\end{table*}

Compared to linear decision methods, our approach demonstrates significant improvements while using the same limited training data. To ensure fair computational comparison, we run linear methods multiple times (set to 5) and vote for the majority prediction in the open-source LLM SFT setting. 
Similarly, for our MCTS method, we set k=5, meaning we early stop when 5 branches reach termination.

For linear methods, multiple runs indeed show improvement, even outperforming BFS and DFS on KQA Pro. Upon investigation, we found this is due to the abundant redundant information in Freebase, where multiple paths can lead to the correct answer. 
For instance, when searching for the government form of "Soviet Union" (?e), both paths (?e, location.country.form\_of\_government, "Parliamentary republic") and ("Parliamentary republic", government.form\_of\_government.countries, ?e) can lead to the correct target entity.

Tree-based algorithms are better at exploring multiple solution paths compared to linear decision methods, enabling even BFS to find answers effectively in WebQSP and CWQ. However, KQA Pro uses Wikidata as its database, which lacks such redundancy and features more complex qualifier structures, resulting in a larger search space. Consequently, simple tree traversal methods struggle to find optimal solutions. MCTS, which essentially prioritizes the search space, shows marked improvement over DFS and BFS, achieving approximately 10 points higher performance on KQA Pro.

Additionally, for WebQSP and CWQ, we report results with given golden entities. The modest improvement of 1.94 points on CWQ suggests that entity recognition is not particularly challenging for template-generated datasets.

\subsection{Analysis of Reward Effectiveness}

The reward function is arguably the most crucial component of MCTS methods, as it directly guides the search direction. 
For the KBQA domain, we propose a scoring framework that leverages open-source instruction-tuned LLMs through a discriminative paradigm to evaluate intermediate states.


\subsubsection{Rule-based Scoring}\label{sec:cot_rule_analysis}

\begin{table}
    \scalebox{0.8}{
    
\begin{tabular}{lccccc}
    \toprule
    \multicolumn{1}{c}{\multirow{2}{*}{\textbf{Method}}} & \multicolumn{2}{c}{\textbf{WebQSP}} & \multicolumn{2}{c}{\textbf{CWQ}} & \textbf{KQA Pro} \\ \cline{2-6} 
    \multicolumn{1}{c}{} & \textbf{F1} & \textbf{RHits@1} & \textbf{F1} & \textbf{EM} & \textbf{Acc} \\ \midrule
    Random & 67.58 & 67.90 & 61.17 & 69.65 & 75.68 \\
    Direct & 69.32 & 69.76 & 62.46 & 69.70 & 76.26 \\
    Rule & 72.49 & 73.45 & 64.87 & 72.12 & 81.89 \\ \bottomrule
\end{tabular}
    
}
\caption{Comparison of different scoring methods.}
\label{tab:reward_method_analysis}
\end{table}

We selected random scoring and direct scoring as baselines. Direct scoring refers to only including tool descriptions and format specifications in the prompt. 
Table \ref{tab:reward_method_analysis} presents experimental results across different scoring methods. Notably, for the more complex datasets CWQ and KQA Pro, the performance difference between direct scoring and random scoring is minimal.

\subsubsection{Score Stability Analysis}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.5\textwidth,page=1]{img/average_reward_std_by_depth.pdf}
    \caption{Standard deviation of scoring by depth.}
    \label{fig:reward_std_by_depth_analysis}
\end{figure}

Prompt-based LLM scoring methods often face challenges with randomness and inconsistency. This section analyzes the stability of scoring methods from the perspective of score distribution.
Specifically, for each node, we set the sampling parameter $n_r=10$, calculate the standard deviation, and then compute the average values grouped by depth.
Figure \ref{fig:reward_std_by_depth_analysis} illustrates the standard deviations for both rule-based and direct scoring approaches. The results show that direct scoring exhibits higher fluctuations compared to rule-based scoring. Moreover, these fluctuations increase with depth, indirectly indicating that as states become more complex, the scoring becomes increasingly unstable.

\subsection{Impact of Key Hyper-parameters}

This section analyzes the impact of the key hyper-parameters sampling width $n$ (number of action samples per node) and early stop $k$.

\subsubsection{Action Sampling Width}

\begin{table}
\scalebox{0.84}{

\begin{tabular}{lccccc}
    \toprule
    \multicolumn{1}{c}{\multirow{2}{*}{\textbf{Setting}}} & \multicolumn{2}{c}{\textbf{WebQSP}} & \multicolumn{2}{c}{\textbf{CWQ}} & \textbf{KQA Pro} \\ \cline{2-6} 
    \multicolumn{1}{c}{} & \textbf{F1} & \textbf{RHits@1} & \textbf{F1} & \textbf{EM} & \textbf{Acc} \\ \midrule
    n=3 & 72.04 & 73.63 & 64.77 & 73.66 & 80.50 \\
    n=5 & 72.49 & 73.45 & 64.87 & 72.12 & 81.89 \\
    n=7 & 70.36 & 70.99 & 64.92 & 73.41 & 80.86 \\
    n=9 & 70.15 & 71.68 & 62.95 & 70.9 & 77.33 \\ \bottomrule
\end{tabular}

}
\caption{Results with different action sampling widths.}
\label{tab:sample_n_analysis}
\end{table}

The hyperparameter $n$ determines the number of nodes to expand at each layer. In our implementation, we generate $n$ actions by setting sample=$n$, then deduplicate actions based on their execution results to establish new nodes.
This essentially balances the trade-off between recall and precision - as $n$ increases, more child nodes are explored which improves answer recall, but may reduce precision due to the increased likelihood of including incorrect paths.
Table \ref{tab:sample_n_analysis} presents experimental results for $n=3,5,7,9$. The results indicate that $n=5$ provides an optimal balance.

\subsubsection{Early Stop Threshold}
\label{sec:early_stop_analysis}

\begin{table}
\scalebox{0.83}{

\begin{tabular}{lcccccc}
    \toprule
    \multicolumn{1}{c}{\multirow{2}{*}{\textbf{Setting}}} & \multicolumn{2}{c}{\textbf{WebQSP}} & \multicolumn{2}{c}{\textbf{CWQ}} & \multicolumn{2}{c}{\textbf{KQA Pro}} \\ \cline{2-7} 
    \multicolumn{1}{c}{} & \textbf{F1} & \textbf{Time} & \textbf{F1} & \textbf{Time} & \textbf{Acc} & \multicolumn{1}{l}{\textbf{Time}} \\ \midrule
    k=1 & 69.51 & 1.00x & 60.35 & 1.00x & 70.33 & 1.00x \\
    k=2 & 69.63 & 1.31x & 61.54 & 1.48x & 71.33 & 1.48x \\
    k=3 & 70.79 & 1.79x & 63.78 & 1.90x & 76.56 & 1.90x \\
    k=4 & 71.93 & 2.48x & 64.41 & 2.33x & 78.67 & 2.33x \\
    k=5 & 72.71 & 3.24x & 64.94 & 2.91x & 81.11 & 2.91x \\ \bottomrule
\end{tabular}

}
\caption{Results with different early stop thresholds. Time shows runtime relative to k=1.}
\label{tab:early_stop_analysis}
\end{table}

The parameter $k$ controls early stopping by requiring a minimum number of valid terminal nodes, balancing accuracy and cost.
Table \ref{tab:early_stop_analysis} presents experimental results and relative time consumption for different values of $k$ ranging from 1 to 5. As $k$ increases, we observe improved search accuracy at the cost of longer search times. Our experiments show that when $k=5$, the search time is approximately 3 times longer compared to $k=1$, which we consider an acceptable trade-off given the performance gains.


\subsection{Experiment on Extended Dataset}
\label{sec:extended_dataset_experiment}

\begin{table*}
\centering
\scalebox{0.9}{

\begin{tabular}{lccccc}
    \toprule
    \multicolumn{1}{c}{\multirow{2}{*}{\textbf{Method}}} & \multicolumn{2}{c}{\textbf{WebQSP}} & \multicolumn{2}{c}{\textbf{CWQ}} & \textbf{KQA Pro} \\ \cline{2-6} 
    \multicolumn{1}{c}{} & \textbf{F1} & \textbf{RHits@1} & \textbf{F1} & \textbf{EM} & \textbf{Acc} \\ \midrule
    \multicolumn{6}{c}{Sampled Test Set} \\ \midrule
    MCTS (SFT w/ Anno) & 72.49 & 73.45 & 64.87 & 72.12 & 81.89 \\
    MCTS (SFT w/ Anno+Ext.) & 75.74 & 77.01 & 74.09 & 83.67 & 89.67 \\ \midrule
    \multicolumn{6}{c}{Orignal Test Set} \\ \midrule
    UnifiedSKG/BART-SPARQL & 73.90 & - & 68.80 & - & 88.56 \\
    MCTS (SFT w/ Anno+Ext.) & 75.68 & 76.39 & 68.51 & 79.33 & 87.55 \\ \bottomrule
\end{tabular}

}
\caption{Experiment on extended (Ext.) dataset.}
\label{tab:extended_dataset_experiment}
\end{table*}

Existing KBQA datasets typically contain only question-SPARQL pairs without intermediate reasoning processes. 
To address this limitation, we employed distant supervision to annotate data with intermediate reasoning steps and conducted SFT on this extended dataset.
We use MCTS to search until reaching an F1 score of 0.67 between predictions and golden answers, then save the reasoning path.
The experimental results are presented in Table \ref{tab:extended_dataset_experiment}.
Despite the extended training datasets for CWQ and KQA Pro being only one-tenth the size of the original training sets, our method nearly matches the performance of approaches using the full dataset, demonstrating remarkable efficiency.

Additionally, for CWQ, the performance difference between the two test sets can be attributed to our uniform sampling by question type in the subset, which contrasts with the uneven type distribution in the original full dataset (see Table \ref{app_tab:distribution_qtype_cwq}).


\subsection{Impact of Open-source LLM}

\begin{table*}[ht]
    \begin{minipage}[t]{0.5\textwidth}
    \scalebox{0.8}{
    \begin{tabular}{lccc}
        \toprule
        \textbf{Open-source LLM} & \textbf{WebQSP} & \textbf{CWQ} & \textbf{KQA Pro} \\ \midrule
        \multicolumn{4}{l}{\textit{SFT Agent (Linear Decision)}} \\
        Llama-2-7b & 44.02 & 33.31 & 66.11 \\
        Llama-2-13b & 54.86 & 42.50 & 62.78 \\
        Llama-3.1-8B & 56.44 & 42.21 & 69.02 \\
        Llama-3.1-8B-Instruct & 57.52 & 44.12 & 71.67 \\ \midrule
        \multicolumn{4}{l}{\textit{Reward Model (MCTS)}} \\
        Llama-3.1-8B-Instruct & 72.49 & 64.87 & 81.89 \\
        Mistral-7B-Instruct-v0.3 & 71.28 & 65.74 & 81.22 \\ \bottomrule
    \end{tabular}
    }
    \caption{Experimental results with different open-source LLMs}
    \label{tab:open_llm_comparison}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.5\textwidth}
    \scalebox{0.9}{

    \begin{tabular}{lccc}
        \toprule
        \multicolumn{1}{c}{\textbf{Metric / Type}} & \textbf{WebQSP} & \textbf{CWQ} & \textbf{KQA Pro} \\ \midrule
        Max@k & 81.76 & 74.78 & 91.60 \\
        Empty@k & 9.66 & 17.86 & 7.33 \\ \midrule
        \multicolumn{4}{c}{Mismatch} \\ \midrule
        Golden Wrong & 12 & 5 & 4 \\
        Ambiguous & 33 & 31 & 12 \\ \midrule
        \multicolumn{4}{c}{Error} \\ \midrule
        Entity Linking & 10 & 9 & 8 \\
        Predicate Search & 24 & 20 & 0 \\
        Reasoning & 11 & 29 & 50 \\
        Other & 10 & 6 & 26 \\ \bottomrule
    \end{tabular}

    }
    \caption{Distribution of error types.}
    \label{tab:error_analysis}
    \end{minipage}
\end{table*}

This section investigates the impact of different open-source LLMs on performance, examining both the agent and reward model components. 
As shown in Table \ref{tab:open_llm_comparison}, in the linear decision setting, Llama-3 8B achieves superior performance compared to Llama-2 13B despite having fewer parameters. 
Additionally, models fine-tuned based on instruction versions consistently outperform those fine-tuned on base versions.
Furthermore, our comparison of using Mistral-7B as the reward model reveals comparable performance between the two architectures, with minimal differences in effectiveness.


\subsection{Error Analysis}

To systematically assess our method's limitations, we first calculate the Max@k metric, which represents the highest F1 score among k (k=5) predictions, indicating our method's performance ceiling. We also measure the Empty@k metric, representing cases where all predictions yield F1=0, identifying genuinely challenging instances.
Additionally, we conduct a detailed manual analysis of 50 randomly selected error cases from each dataset.
The comprehensive statistical findings are presented in Table \ref{tab:error_analysis}.

We classify the cases into two primary categories: Mismatch and Error. A Mismatch occurs when the generated SPARQL query differs from the golden reference while maintaining semantic consistency with the question. An Error, conversely, indicates an incorrect generation.

For Mismatch types:
\textbf{Golden Wrong Error} indicates cases where the golden SPARQL query does not accurately reflect the question requirements.
\textbf{Ambiguous Error} encompasses instances where multiple valid logical forms could correctly answer the question but deviate from the single golden reference, highlighting the limitations of single-reference evaluation.
For instance, in the question "what country did buddha come from?", while the golden reference uses the predicate "nationality", our model's prediction of "place\_of\_birth" is semantically correct as well.

For Error types:
\textbf{Entity Linking Error} occurs when the system fails to locate correct nodes. For example, in the question "what town was martin luther king assassinated in?", the system incorrectly selects "Martin Luther King" instead of the correct entity "Martin Luther King, Jr." due to missing descriptions in the knowledge base.
\textbf{Predicate Search Error} represents cases where the SearchGraphPatterns tool fails to return necessary information. For instance, in the question "where did rudolf virchow conduct his research", the required inference chain "people.person.employment\_history -> business.employment\_tenure.company" proves challenging to match.
\textbf{Reasoning Error} encompasses fundamental logical flaws in query construction, including misinterpretation of question intent, inconsistency between reasoning and actions, and incorrect predicate directionality in the SPARQL structure.
\textbf{Other Error} comprises errors that fall outside the aforementioned categories, such as formatting errors, which are particularly prevalent in generating qualified structures in Wikidata.
Additional case studies are presented in Appendix \ref{app_sec:case_study}.
