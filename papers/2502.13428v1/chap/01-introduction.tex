\section{Introduction}

Knowledge base question answering (KBQA) is an increasingly significant research area that leverages structured knowledge bases (KBs) to provide precise answers to natural language (NL) questions.
Benefiting from the powerful reasoning capabilities of large language models (LLMs), such as ChatGPT \citep{Ouyang-Long-NeurIPS-2022-InstructGPT} and GPT-4 \citep{OpenAI-2023-GPT4}, state-of-the-art KBQA methods conceptualize LLMs as agents and KBs as environments \citep{Gu-Yu-arXiv-2024-Middleware,Jiang-Jinhao-arXiv-2024-KG-Agent,Xiong-Guanming-ACL-2024-Interactive-KBQA}. Through carefully designed tools and interaction logic, these methods can accomplish tasks with only a few exemplars as prompts. 
However, these approaches are fundamentally \textbf{limited by linear decision-making processes}, which constrain the full potential of LLMs' reasoning capabilities.

Monte Carlo Tree Search (MCTS) \citep{Swiechowski-2023-MCTS-Review} methods have recently shown remarkable success in reasoning tasks, such as mathematics and code generation \citep{Chen-Guoxin-NIPS-2024-AlphaMath,Zhang-Di-arXiv-2024-LLaMA-Berry,Wang-Chaojie-arXiv-2024-Q-star}. MCTS is a tree search-based approach that enhances reasoning capabilities through five key steps: expansion, selection, simulation, evaluation, and backpropagation. These steps enable the exploration of the solution space effectively. The core challenge of such methods lies in designing domain-specific reward functions. While some works opt to train a reward model, others choose to obtain rewards directly from LLMs or the environment.

Therefore, this paper focuses on exploring a method to incorporate MCTS into the KBQA domain. Our main contributions are:

\begin{itemize}[noitemsep] 
    \item We introduce MCTS methodology to the KBQA domain and design step-wise rewards that require only direct prompting of open-source instruction-tuned LLMs without fine-tuning.
    \item Experimental results demonstrate that our method significantly outperforms linear decision-making approaches in low-resource scenarios.
    \item We contribute new data resources to the community by annotating intermediate reasoning processes for existing question-SPARQL datasets using distant supervision, which further helps improve model performance.
\end{itemize}
