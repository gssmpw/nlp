\section{Conclusion}
This paper presents a MCTS-based framework for KBQA that enhances LLMs' reasoning capabilities through tree search methodology.
We propose a step-wise reward mechanism that requires only direct prompting of open-source instruction LLMs without additional fine-tuning.
Experimental results demonstrate that our method significantly outperforms linear decision-making baselines, particularly in low-resource scenarios.
Additionally, we contribute new data resources by annotating intermediate reasoning processes for existing KBQA datasets, demonstrating both effectiveness and data efficiency.

\section*{Limitations}
The main limitation is the computational overhead of MCTS compared to linear decision-making methods. While early stopping helps, increased latency remains a concern for time-sensitive applications. Additionally, our distant supervision approach may generate incorrect intermediate reasoning steps even with correct final answers. Future work should focus on improving the quality of these intermediate annotations while maintaining efficiency.
