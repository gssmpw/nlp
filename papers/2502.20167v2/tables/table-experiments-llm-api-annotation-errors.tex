\begin{table}
\caption{$\datasetMMLUPro4qa$, Computer Science category. Predictions that met the index-conditional threshold but were marked incorrect according to the ground-truth labels. Examination of the data reveals the model is \colorbox{correctPredictionColor}{correct} and the ground-truth annotations are \colorbox{wrongPredictionColor}{incorrect}. The digit significance of $\pHat$ is not necessarily significant (and when shown to users, would typically be rounded, with a top ceiling to avoid 1.0), but provided for reference. $\nYhat$ is the effective sample size for the predicted class. The final question is arguably ambiguous.} 
%  
  \label{tab:mmlu-pro-annotation-errors-cs} %  
  \centering
  \resizebox{\textwidth}{!}{%
  \begin{tabular}{l c c l l l l }
    \toprule

Question ID   & y & $\hat{y}$  & $\pLower$ & $\pCentroid$ & $\pUpper$ & $\nYhat$   \\
    \midrule
    $10750$ & \colorbox{wrongPredictionColor}{A} & \colorbox{correctPredictionColor}{D} & 0.9999999029119694 & 0.999999946869715 & 0.999999963752475 & 11563  \\
    $10682$ & \colorbox{wrongPredictionColor}{D}  & \colorbox{correctPredictionColor}{C} & 0.9999995410050875 & 0.9999997504521737 & 0.9999998413548795 & 11774  \\
    $10458$ & \colorbox{wrongPredictionColor}{D} & \colorbox{correctPredictionColor}{A} & 0.9997548501324156 & 0.9998610348657851 & 0.9999170919091862 & 9129 \\
    $10533$ & \colorbox{wrongPredictionColor}{B} & \colorbox{correctPredictionColor}{C} & 0.9897059289074643 & 0.9936086673736274 & 0.9957749405311342 & 6891 \\
    $10479$ & D & B & 0.967751071803557 & 0.9791966686070331 & 0.9862756083406558 & 7684 \\
    \bottomrule
  \end{tabular}
  }  % end of \resizebox{\textwidth}{!}{%
\end{table}
