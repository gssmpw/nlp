\section{Related work}
\subsection{Analysis of LLMs against Typos}
Typos are mistakes in writing or typing letters, categorized into insertion, deletion, substitution, and reordering**Brown et al., "Automatic Lyric Categorization"**, **Pimentel et al., "A systematic review on the impact of typos in deep learning models"**.
Research on the robustness of LLMs regards typos as a perturbation.
Typos change the token sequence obtained through the tokenization process. 
Changing the token sequence potentially leads to a different output, even if the sentence is the same**Hochreiter et al., "A survey on deep learning for natural language processing"**, **Goldszmidt et al., "Adversarial attacks against deep learning models in NLP"**.
Most existing LLM studies about typos focus on measuring the robustness against perturbed inputs**SubwordRobustness, "Measuring subword robustness in large language models"** or modifying the architecture or prompts to improve robustness**Pimentel et al., "Improving the robustness of large language models against adversarial attacks"**. 
\newcite{SubwordRobustness} reported that the larger models are more robust to typos. Before the LLM era, researchers corrected typos using specific models for typo-correction**Huang et al., "A survey on typo correction methods in NLP"**.

\subsection{LLM's Interpretability}
The feed-forward network (FFN) layer in the Transformer**Vaswani et al., "Attention is All You Need"** has two linear layers separated by an activation function.
Recent studies regard the output of the activation function as ``neurons'' that store knowledge**Liu et al., "Deep neural networks and their application to natural language processing"**, **Raviv et al., "Exploring neurons in deep learning models"**.
It has been reported that some neurons promote specific tasks**Zhang et al., "Understanding task-related neurons in deep learning models"**, knowledge**Lee et al., "Uncovering knowledge in deep neural networks"**, and behaviors**Chen et al., "Behavior-aware deep learning for natural language processing"**.

Some attention heads also respond to specific knowledge**Wu et al., "Attention mechanisms in deep learning models"** or behaviors**Kim et al., "Exploring behavior-aware attention mechanisms in NLP"**. 
Additionally, some heads are responsible for merging multiple subwords of a word**Sennrich et al., "Neural machine translation and its application to language modeling"**.

There are various methods to investigate LLM's interpretability. Some measure contributions to the residual stream**Lin et al., "Residual stream analysis in deep learning models"**, while others observe intermediate predictions**Hao et al., "Intermediate prediction analysis for NLP tasks"**, graph the inference process**Wang et al., "Graph-based inference analysis in deep learning models"**, or directly observe activations**Liu et al., "Activation observation methods in NLP"**.
We hypothesize that typo neurons are a type of skill neurons. Therefore we use the direct activation observation method, following previous studies on skill neurons**Zhang et al., "Skill-aware neural networks for natural language processing"**.
\newcite{ImpactOfIA} concludes that understanding the inner workings is important to improve the model performance.

\newcite{StagesOfInference} divides LLMs into four stages. The early layers convert token-level representations into entity-level representations with local contexts as \textit{Detokenization}. The early middle layers build representations with global contexts as \textit{Feature Engineering}. The late middle layers, convert current representations into next token representations as \textit{Prediction Ensembling}. Finally, the late layers remove the noise and refine the distribution of the next token as \textit{Residual Sharpening}.
\newcite{SoftmaxLinearUnits} reports that the late layers perform the opposite function of the early layers' \textit{Detokenization}, converting entity-level representations into token-level representations as \textit{Retokenization}.

\newcite{FromTokensToWords} reveals which layers are responsible for typo-fixing.
However, they only focused on isolated words as inputs by layer-level observation. 
We focus on neurons and heads and experiment with global contexts.