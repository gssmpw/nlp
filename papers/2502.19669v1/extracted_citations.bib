@inproceedings{AcronymsTask,
  title={How does GPT-2 Predict Acronyms? Extracting and Understanding a Circuit via Mechanistic Interpretability},
  author={Garc{\'\i}a-Carrasco, Jorge and Mat{\'e}, Alejandro and Trujillo, Juan Carlos},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3322--3330},
  year={2024},
  organization={PMLR},
  abstract={3文字略語予測タスクを用いて、文字送りヘッド(i番目の大文字に注目してi番目の出力に与えるhead)を発見}
}

@inproceedings{AdvGLUE,
title={Adversarial GLUE: A Multi-Task Benchmark for Robustness Evaluation of Language Models},
author={Wang, Boxin and Xu, Chejian and Wang, Shuohang and Gan, Zhe and Cheng, Yu and Gao, Jianfeng and Awadallah, Ahmed Hassan and Li, Bo},
booktitle={Advances in Neural Information Processing Systems},
year={2021}
}

@article{AdvGLUEpp,
  title={DECODINGTRUST: A Comprehensive Assessment of Trustworthiness in GPT Models},
  author={Wang, Boxin and Chen, Weixin and Pei, Hengzhi and Xie, Chulin and Kang, Mintong and Zhang, Chenhui and Xu, Chejian and Xiong, Zidi and Dutta, Ritik and Schaeffer, Rylan and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2023}
}

@inproceedings{CUTE,
    title = "{CUTE}: Measuring {LLM}s{'} Understanding of Their Tokens",
    author = "Edman, Lukas  and
      Schmid, Helmut  and
      Fraser, Alexander",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.177",
    pages = "3017--3026",
    abstract = "文字レベルの摂動でのスコアを調べるデータセット",
}

@article{CopyInductionHeads,
  title={Induction heads as an essential mechanism for pattern matching in in-context learning},
  author={Crosbie, Joy and Shutova, Ekaterina},
  journal={arXiv preprint arXiv:2407.07011},
  year={2024},
  abstract={中間層ではいったんICLの同じ内容をコピーしている。それより後ろに、コピーを判定して、それを無くすヘッドが存在}
}

@inproceedings{CopySuppression,
    title = "Copy Suppression: Comprehensively Understanding a Motif in Language Model Attention Heads",
    author = "McDougall, Callum Stuart  and
      Conmy, Arthur  and
      Rushing, Cody  and
      McGrath, Thomas  and
      Nanda, Neel",
    editor = "Belinkov, Yonatan  and
      Kim, Najoung  and
      Jumelet, Jaap  and
      Mohebbi, Hosein  and
      Mueller, Aaron  and
      Chen, Hanjie",
    booktitle = "Proceedings of the 7th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP",
    month = nov,
    year = "2024",
    address = "Miami, Florida, US",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.blackboxnlp-1.22",
    pages = "337--363",
    abstract = "コピーを抑制するHeadがあります",
}

@inproceedings{DeepWordBug,
  title={Black-box generation of adversarial text sequences to evade deep learning classifiers},
  author={Gao, Ji and Lanchantin, Jack and Soffa, Mary Lou and Qi, Yanjun},
  booktitle={2018 IEEE Security and Privacy Workshops (SPW)},
  pages={50--56},
  year={2018},
  organization={IEEE},
  abstract={モデルを使って誤字を足そう(誤字の種類はInsert, Delete, Swap, Substitution)}
}

@article{FromTokensToWords,
  title={From Tokens to Words: On the Inner Lexicon of LLMs},
  author={Kaplan, Guy and Oren, Matanel and Reif, Yuval and Schwartz, Roy},
  journal={arXiv preprint arXiv:2410.05864},
  year={2024},
  abstract={単語を変な分割にしても復元される→語彙拡張}
}

@inproceedings{HeadPruning,
    title = "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned",
    author = "Voita, Elena  and
      Talbot, David  and
      Moiseev, Fedor  and
      Sennrich, Rico  and
      Titov, Ivan",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'\i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1580",
    doi = "10.18653/v1/P19-1580",
    pages = "5797--5808",
    abstract = "NMTにおいて、重要なheadはS-VやO-Vなど、言語的なつながりを見ているヘッド",
}

@inproceedings{InformationFlowRoutes,
    title = "Information Flow Routes: Automatically Interpreting Language Models at Scale",
    author = "Ferrando, Javier  and
      Voita, Elena",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.965",
    pages = "17432--17445",
    abstract = "LLMにもサブワードマージヘッドがあるよ",
}

@inproceedings{KVMemory,
    title = "Transformer Feed-Forward Layers Are Key-Value Memories",
    author = "Geva, Mor  and
      Schuster, Roei  and
      Berant, Jonathan  and
      Levy, Omer",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.446",
    doi = "10.18653/v1/2021.emnlp-main.446",
    pages = "5484--5495",
    abstract = "FFN層はKVメモリになっている",
}

@inproceedings{KnowledgeNeurons,
    title = "Knowledge Neurons in Pretrained Transformers",
    author = "Dai, Damai  and
      Dong, Li  and
      Hao, Yaru  and
      Sui, Zhifang  and
      Chang, Baobao  and
      Wei, Furu",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.581",
    doi = "10.18653/v1/2022.acl-long.581",
    pages = "8493--8502",
    abstract = "特定の知識を保存しているニューロンがある"
}

@inproceedings{LEA,
  title={Lea: Improving sentence similarity robustness to typos using lexical attention bias},
  author={Almagro, Mario and Almaz{\'a}n, Emilio and Ortego, Diego and Jim{\'e}nez, David},
  booktitle={Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  pages={36--46},
  year={2023},
  abstract={トークンの文字的類似度項を追加してロバストに/非LLM}
}

@misc{LogitLens,
    title = {interpreting GPT: the logit lens},
    url = {https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens},
    author = {nostalgebraist},
    year = {2020},
    note = {Accessed on Nov 27, 2024}
}

@inproceedings{MitigatingRepetition,
    title = "Mitigating the Language Mismatch and Repetition Issues in {LLM}-based Machine Translation via Model Editing",
    author = "Wang, Weichuan  and
      Li, Zhaoyi  and
      Lian, Defu  and
      Ma, Chen  and
      Song, Linqi  and
      Wei, Ying",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.879",
    pages = "15681--15700",
    abstract = "LLMの翻訳タスクでRepetitionと言語間違いが起こりまくるのを抑制。RepetitionNeuron"
}

@article{NeuronsAcrossLanguageAndTask,
  title={Sharing matters: Analysing neurons across languages and tasks in llms},
  author={Wang, Weixuan and Haddow, Barry and Wu, Minghao and Peng, Wei and Birch, Alexandra},
  journal={arXiv preprint arXiv:2406.09265},
  year={2024},
  abstract={言語間、タスク間でのニューロンの共通性}
}

@inproceedings{NoisyExemplars,
    title = "Noisy Exemplars Make Large Language Models More Robust: A Domain-Agnostic Behavioral Analysis",
    author = "Zheng, Hongyi  and
      Saparov, Abulhair",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.277",
    doi = "10.18653/v1/2023.emnlp-main.277",
    pages = "4560--4568",
    abstract = "FewShotにノイズを入れると、ノイズ耐性が上がる",
}

@inproceedings{NounPhrasesNeurons,
    title={Identifying and Controlling Important Neurons in Neural Machine Translation},
    author={Anthony Bau and Yonatan Belinkov and Hassan Sajjad and Nadir Durrani and Fahim Dalvi and James Glass},
    booktitle={International Conference on Learning Representations},
    year={2019},
    url={https://openreview.net/forum?id=H1z-PsR5KX},
    abstract={名詞句の最初の単語でだけ反応するニューロンがある}
}

@article{PathPatching,
  title={How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model},
  author={Hanna, Michael and Liu, Ollie and Variengien, Alexandre},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{Promptrobust,
  title={Promptrobust: Towards evaluating the robustness of large language models on adversarial prompts},
  author={Zhu, Kaijie and Wang, Jindong and Zhou, Jiaheng and Wang, Zichen and Chen, Hao and Wang, Yidong and Yang, Linyi and Ye, Wei and Zhang, Yue and Gong, Neil and others},
  booktitle={Proceedings of the 1st ACM Workshop on Large AI Systems and Models with Privacy and Safety Analysis},
  pages={57--68},
  year={2023},
  abstract={誤字に限らず、いろんな摂動でLLMの性能を評価(うまくできないよね)}
}

@article{RepetitionNeurons,
  title={Repetition Neurons: How Do Language Models Produce Repetitions?},
  author={Hiraoka, Tatsuya and Inui, Kentaro},
  journal={arXiv preprint arXiv:2410.13497},
  year={2024},
  abstract={Repetitionを促進するニューロンがある}
}

@inproceedings{RobustnessOfCodex,
    title = "On Robustness of Prompt-based Semantic Parsing with Large Pre-trained Language Model: An Empirical Study on Codex",
    author = "Zhuo, Terry Yue  and
      Li, Zhuang  and
      Huang, Yujin  and
      Shiri, Fatemeh  and
      Wang, Weiqing  and
      Haffari, Gholamreza  and
      Li, Yuan-Fang",
    editor = "Vlachos, Andreas  and
      Augenstein, Isabelle",
    booktitle = "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.eacl-main.77",
    doi = "10.18653/v1/2023.eacl-main.77",
    pages = "1090--1102",
    abstract = "CodexはAdversarialなデータに弱いが、ICLで改善可能",
}

@article{SafetyNeurons,
  title={Finding Safety Neurons in Large Language Models},
  author={Chen, Jianhui and Wang, Xiaozhi and Yao, Zijun and Bai, Yushi and Hou, Lei and Li, Juanzi},
  journal={arXiv preprint arXiv:2406.14144},
  year={2024},
  abstract={安全性アライメントを行うニューロンが存在する}
}

@inproceedings{SkillNeurons,
    title = "Finding Skill Neurons in Pre-trained Transformer-based Language Models",
    author = "Wang, Xiaozhi  and
      Wen, Kaiyue  and
      Zhang, Zhengyan  and
      Hou, Lei  and
      Liu, Zhiyuan  and
      Li, Juanzi",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.765",
    doi = "10.18653/v1/2022.emnlp-main.765",
    pages = "11132--11152",
    abstract = "タスクにチューニングすると特定のタスクに特化したニューロンが生成される",
}

@inproceedings{SubwordMergeHead,
    title = "Adaptively Sparse Transformers",
    author = "Correia, Gon{\c{c}}alo M.  and
      Niculae, Vlad  and
      Martins, Andr{\'e} F. T.",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1223",
    doi = "10.18653/v1/D19-1223",
    pages = "2174--2184",
    abstract = "Subwordに分割されたmulti token wordsを中間表現でまとめるヘッドがあるよ",
}

@inproceedings{SuccessorHeads,
title={Successor Heads: Recurring, Interpretable Attention Heads In The Wild},
author={Rhys Gould and Euan Ong and George Ogden and Arthur Conmy},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=kvcbV8KQsi},
abstract={月曜→火曜のような、次の要素を認識するヘッドが存在する}
}

@article{UnderstandingVulnerabilities,
  title={Detecting and understanding vulnerabilities in language models via mechanistic interpretability},
  author={Garc{\'\i}a-Carrasco, Jorge and Mat{\'e}, Alejandro and Trujillo, Juan},
  journal={arXiv preprint arXiv:2407.19842},
  year={2024},
  abstract={MI技術(パッチング)を使って、どんな脆弱性がよりモデルの性能を下げられるかを見る}
}

@article{UniversalNeurons,
  title={Universal neurons in gpt2 language models},
  author={Gurnee, Wes and Horsley, Theo and Guo, Zifan Carl and Kheirkhah, Tara Rezaei and Sun, Qinyi and Hathaway, Will and Nanda, Neel and Bertsimas, Dimitris},
  journal={arXiv preprint arXiv:2401.12181},
  year={2024},
  abstract={初期値に非依存なニューロンが存在して、それは予測だったり、ロジット分布を絞ったり、BOSへの注意を無効化するニューロンだったりする}
}

@inproceedings{context_spelling_correction,
    title = "Context-aware Stand-alone Neural Spelling Correction",
    author = "Li, Xiangci  and
      Liu, Hairong  and
      Huang, Liang",
    editor = "Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.37/",
    doi = "10.18653/v1/2020.findings-emnlp.37",
    pages = "407--414",
}

@inproceedings{spellbert,
    title = "{S}pell{BERT}: A Lightweight Pretrained Model for {C}hinese Spelling Check",
    author = "Ji, Tuo  and
      Yan, Hang  and
      Qiu, Xipeng",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.287/",
    doi = "10.18653/v1/2021.emnlp-main.287",
    pages = "3544--3551",
}

@article{tsuji2024subregweigh,
  title={SubRegWeigh: Effective and Efficient Annotation Weighing with Subword Regularization},
  author={Tsuji, Kohei and Hiraoka, Tatsuya and Cheng, Yuchang and Iwakura, Tomoya},
  journal={arXiv preprint arXiv:2409.06216},
  year={2024}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, A},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}

