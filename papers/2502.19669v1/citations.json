[
  {
    "index": 0,
    "papers": [
      {
        "key": "DeepWordBug",
        "author": "Gao, Ji and Lanchantin, Jack and Soffa, Mary Lou and Qi, Yanjun",
        "title": "Black-box generation of adversarial text sequences to evade deep learning classifiers"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "tsuji2024subregweigh",
        "author": "Tsuji, Kohei and Hiraoka, Tatsuya and Cheng, Yuchang and Iwakura, Tomoya",
        "title": "SubRegWeigh: Effective and Efficient Annotation Weighing with Subword Regularization"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "AdvGLUE",
        "author": "Wang, Boxin and Xu, Chejian and Wang, Shuohang and Gan, Zhe and Cheng, Yu and Gao, Jianfeng and Awadallah, Ahmed Hassan and Li, Bo",
        "title": "Adversarial GLUE: A Multi-Task Benchmark for Robustness Evaluation of Language Models"
      },
      {
        "key": "AdvGLUEpp",
        "author": "Wang, Boxin and Chen, Weixin and Pei, Hengzhi and Xie, Chulin and Kang, Mintong and Zhang, Chenhui and Xu, Chejian and Xiong, Zidi and Dutta, Ritik and Schaeffer, Rylan and others",
        "title": "DECODINGTRUST: A Comprehensive Assessment of Trustworthiness in GPT Models"
      },
      {
        "key": "Promptrobust",
        "author": "Zhu, Kaijie and Wang, Jindong and Zhou, Jiaheng and Wang, Zichen and Chen, Hao and Wang, Yidong and Yang, Linyi and Ye, Wei and Zhang, Yue and Gong, Neil and others",
        "title": "Promptrobust: Towards evaluating the robustness of large language models on adversarial prompts"
      },
      {
        "key": "CUTE",
        "author": "Edman, Lukas  and\nSchmid, Helmut  and\nFraser, Alexander",
        "title": "{CUTE}: Measuring {LLM}s{'} Understanding of Their Tokens"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "RobustnessOfCodex",
        "author": "Zhuo, Terry Yue  and\nLi, Zhuang  and\nHuang, Yujin  and\nShiri, Fatemeh  and\nWang, Weiqing  and\nHaffari, Gholamreza  and\nLi, Yuan-Fang",
        "title": "On Robustness of Prompt-based Semantic Parsing with Large Pre-trained Language Model: An Empirical Study on Codex"
      },
      {
        "key": "NoisyExemplars",
        "author": "Zheng, Hongyi  and\nSaparov, Abulhair",
        "title": "Noisy Exemplars Make Large Language Models More Robust: A Domain-Agnostic Behavioral Analysis"
      },
      {
        "key": "LEA",
        "author": "Almagro, Mario and Almaz{\\'a}n, Emilio and Ortego, Diego and Jim{\\'e}nez, David",
        "title": "Lea: Improving sentence similarity robustness to typos using lexical attention bias"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "context_spelling_correction",
        "author": "Li, Xiangci  and\nLiu, Hairong  and\nHuang, Liang",
        "title": "Context-aware Stand-alone Neural Spelling Correction"
      },
      {
        "key": "spellbert",
        "author": "Ji, Tuo  and\nYan, Hang  and\nQiu, Xipeng",
        "title": "{S}pell{BERT}: A Lightweight Pretrained Model for {C}hinese Spelling Check"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "vaswani2017attention",
        "author": "Vaswani, A",
        "title": "Attention is all you need"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "KVMemory",
        "author": "Geva, Mor  and\nSchuster, Roei  and\nBerant, Jonathan  and\nLevy, Omer",
        "title": "Transformer Feed-Forward Layers Are Key-Value Memories"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "SkillNeurons",
        "author": "Wang, Xiaozhi  and\nWen, Kaiyue  and\nZhang, Zhengyan  and\nHou, Lei  and\nLiu, Zhiyuan  and\nLi, Juanzi",
        "title": "Finding Skill Neurons in Pre-trained Transformer-based Language Models"
      },
      {
        "key": "NeuronsAcrossLanguageAndTask",
        "author": "Wang, Weixuan and Haddow, Barry and Wu, Minghao and Peng, Wei and Birch, Alexandra",
        "title": "Sharing matters: Analysing neurons across languages and tasks in llms"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "KnowledgeNeurons",
        "author": "Dai, Damai  and\nDong, Li  and\nHao, Yaru  and\nSui, Zhifang  and\nChang, Baobao  and\nWei, Furu",
        "title": "Knowledge Neurons in Pretrained Transformers"
      },
      {
        "key": "NounPhrasesNeurons",
        "author": "Anthony Bau and Yonatan Belinkov and Hassan Sajjad and Nadir Durrani and Fahim Dalvi and James Glass",
        "title": "Identifying and Controlling Important Neurons in Neural Machine Translation"
      },
      {
        "key": "UniversalNeurons",
        "author": "Gurnee, Wes and Horsley, Theo and Guo, Zifan Carl and Kheirkhah, Tara Rezaei and Sun, Qinyi and Hathaway, Will and Nanda, Neel and Bertsimas, Dimitris",
        "title": "Universal neurons in gpt2 language models"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "RepetitionNeurons",
        "author": "Hiraoka, Tatsuya and Inui, Kentaro",
        "title": "Repetition Neurons: How Do Language Models Produce Repetitions?"
      },
      {
        "key": "MitigatingRepetition",
        "author": "Wang, Weichuan  and\nLi, Zhaoyi  and\nLian, Defu  and\nMa, Chen  and\nSong, Linqi  and\nWei, Ying",
        "title": "Mitigating the Language Mismatch and Repetition Issues in {LLM}-based Machine Translation via Model Editing"
      },
      {
        "key": "SafetyNeurons",
        "author": "Chen, Jianhui and Wang, Xiaozhi and Yao, Zijun and Bai, Yushi and Hou, Lei and Li, Juanzi",
        "title": "Finding Safety Neurons in Large Language Models"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "SuccessorHeads",
        "author": "Rhys Gould and Euan Ong and George Ogden and Arthur Conmy",
        "title": "Successor Heads: Recurring, Interpretable Attention Heads In The Wild"
      },
      {
        "key": "HeadPruning",
        "author": "Voita, Elena  and\nTalbot, David  and\nMoiseev, Fedor  and\nSennrich, Rico  and\nTitov, Ivan",
        "title": "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned"
      },
      {
        "key": "AcronymsTask",
        "author": "Garc{\\'\\i}a-Carrasco, Jorge and Mat{\\'e}, Alejandro and Trujillo, Juan Carlos",
        "title": "How does GPT-2 Predict Acronyms? Extracting and Understanding a Circuit via Mechanistic Interpretability"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "CopySuppression",
        "author": "McDougall, Callum Stuart  and\nConmy, Arthur  and\nRushing, Cody  and\nMcGrath, Thomas  and\nNanda, Neel",
        "title": "Copy Suppression: Comprehensively Understanding a Motif in Language Model Attention Heads"
      },
      {
        "key": "CopyInductionHeads",
        "author": "Crosbie, Joy and Shutova, Ekaterina",
        "title": "Induction heads as an essential mechanism for pattern matching in in-context learning"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "SubwordMergeHead",
        "author": "Correia, Gon{\\c{c}}alo M.  and\nNiculae, Vlad  and\nMartins, Andr{\\'e} F. T.",
        "title": "Adaptively Sparse Transformers"
      },
      {
        "key": "InformationFlowRoutes",
        "author": "Ferrando, Javier  and\nVoita, Elena",
        "title": "Information Flow Routes: Automatically Interpreting Language Models at Scale"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "UnderstandingVulnerabilities",
        "author": "Garc{\\'\\i}a-Carrasco, Jorge and Mat{\\'e}, Alejandro and Trujillo, Juan",
        "title": "Detecting and understanding vulnerabilities in language models via mechanistic interpretability"
      },
      {
        "key": "PathPatching",
        "author": "Hanna, Michael and Liu, Ollie and Variengien, Alexandre",
        "title": "How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "LogitLens",
        "author": "nostalgebraist",
        "title": "interpreting GPT: the logit lens"
      },
      {
        "key": "FromTokensToWords",
        "author": "Kaplan, Guy and Oren, Matanel and Reif, Yuval and Schwartz, Roy",
        "title": "From Tokens to Words: On the Inner Lexicon of LLMs"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "InformationFlowRoutes",
        "author": "Ferrando, Javier  and\nVoita, Elena",
        "title": "Information Flow Routes: Automatically Interpreting Language Models at Scale"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "SkillNeurons",
        "author": "Wang, Xiaozhi  and\nWen, Kaiyue  and\nZhang, Zhengyan  and\nHou, Lei  and\nLiu, Zhiyuan  and\nLi, Juanzi",
        "title": "Finding Skill Neurons in Pre-trained Transformer-based Language Models"
      },
      {
        "key": "RepetitionNeurons",
        "author": "Hiraoka, Tatsuya and Inui, Kentaro",
        "title": "Repetition Neurons: How Do Language Models Produce Repetitions?"
      },
      {
        "key": "NeuronsAcrossLanguageAndTask",
        "author": "Wang, Weixuan and Haddow, Barry and Wu, Minghao and Peng, Wei and Birch, Alexandra",
        "title": "Sharing matters: Analysing neurons across languages and tasks in llms"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "SkillNeurons",
        "author": "Wang, Xiaozhi  and\nWen, Kaiyue  and\nZhang, Zhengyan  and\nHou, Lei  and\nLiu, Zhiyuan  and\nLi, Juanzi",
        "title": "Finding Skill Neurons in Pre-trained Transformer-based Language Models"
      },
      {
        "key": "RepetitionNeurons",
        "author": "Hiraoka, Tatsuya and Inui, Kentaro",
        "title": "Repetition Neurons: How Do Language Models Produce Repetitions?"
      }
    ]
  }
]