@article{RobustnessVsEfficiency,
  title={Towards resilient and efficient llms: A comparative study of efficiency, performance, and adversarial robustness},
  author={Fan, Xiaojing and Tao, Chunliang},
  journal={arXiv preprint arXiv:2408.04585},
  year={2024},  
  abstract={効率化されたモデルはロバスト性においてスコアが下がる(AdvGLUEなどを利用)}
}

@inproceedings{Promptrobust,
  title={Promptrobust: Towards evaluating the robustness of large language models on adversarial prompts},
  author={Zhu, Kaijie and Wang, Jindong and Zhou, Jiaheng and Wang, Zichen and Chen, Hao and Wang, Yidong and Yang, Linyi and Ye, Wei and Zhang, Yue and Gong, Neil and others},
  booktitle={Proceedings of the 1st ACM Workshop on Large AI Systems and Models with Privacy and Safety Analysis},
  pages={57--68},
  year={2023},
  abstract={誤字に限らず、いろんな摂動でLLMの性能を評価(うまくできないよね)}
}

@inproceedings{NoisyExemplars,
    title = "Noisy Exemplars Make Large Language Models More Robust: A Domain-Agnostic Behavioral Analysis",
    author = "Zheng, Hongyi  and
      Saparov, Abulhair",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.277",
    doi = "10.18653/v1/2023.emnlp-main.277",
    pages = "4560--4568",
    abstract = "FewShotにノイズを入れると、ノイズ耐性が上がる",
}

@article{RUPBench,
  title={RUPBench: Benchmarking Reasoning Under Perturbations for Robustness Evaluation in Large Language Models},
  author={Wang, Yuqing and Zhao, Yun},
  journal={arXiv preprint arXiv:2406.11020},
  year={2024},
  abstract={新しいベンチ作った。大規模モデルの方がロバスト}
}

@article{RobustnessOfChatGPT,
  title={On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective},
  author={Wang, Jindong and Hu, Xixu and Hou, Wenxin and Chen, Hao and Zheng, Runkai and Wang, Yidong and Yang, Linyi and Ye, Wei and Huang, Haojun and Geng, Xiubo and others},
  journal={Data Engineering},
  pages={48},
  year={2024},
  abstract={ChatGPTは他モデルに比べてロバストだが、完ぺきではない}
}

@inproceedings{RobustnessOfCodex,
    title = "On Robustness of Prompt-based Semantic Parsing with Large Pre-trained Language Model: An Empirical Study on Codex",
    author = "Zhuo, Terry Yue  and
      Li, Zhuang  and
      Huang, Yujin  and
      Shiri, Fatemeh  and
      Wang, Weiqing  and
      Haffari, Gholamreza  and
      Li, Yuan-Fang",
    editor = "Vlachos, Andreas  and
      Augenstein, Isabelle",
    booktitle = "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.eacl-main.77",
    doi = "10.18653/v1/2023.eacl-main.77",
    pages = "1090--1102",
    abstract = "CodexはAdversarialなデータに弱いが、ICLで改善可能",
}

@inproceedings{AdvGLUE,
title={Adversarial GLUE: A Multi-Task Benchmark for Robustness Evaluation of Language Models},
author={Wang, Boxin and Xu, Chejian and Wang, Shuohang and Gan, Zhe and Cheng, Yu and Gao, Jianfeng and Awadallah, Ahmed Hassan and Li, Bo},
booktitle={Advances in Neural Information Processing Systems},
year={2021}
}

@article{AdvGLUEpp,
  title={DECODINGTRUST: A Comprehensive Assessment of Trustworthiness in GPT Models},
  author={Wang, Boxin and Chen, Weixin and Pei, Hengzhi and Xie, Chulin and Kang, Mintong and Zhang, Chenhui and Xu, Chejian and Xiong, Zidi and Dutta, Ritik and Schaeffer, Rylan and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2023}
}

@inproceedings{LEA,
  title={Lea: Improving sentence similarity robustness to typos using lexical attention bias},
  author={Almagro, Mario and Almaz{\'a}n, Emilio and Ortego, Diego and Jim{\'e}nez, David},
  booktitle={Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  pages={36--46},
  year={2023},
  abstract={トークンの文字的類似度項を追加してロバストに/非LLM}
}

@inproceedings{DeadNeurons,
    title = "Neurons in Large Language Models: Dead, N-gram, Positional",
    author = "Voita, Elena  and
      Ferrando, Javier  and
      Nalmpantis, Christoforos",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.75",
    doi = "10.18653/v1/2024.findings-acl.75",
    pages = "1288--1301",
    abstract = "初期層のニューロンはほとんど動いてない",
}

@inproceedings{KVMemory,
    title = "Transformer Feed-Forward Layers Are Key-Value Memories",
    author = "Geva, Mor  and
      Schuster, Roei  and
      Berant, Jonathan  and
      Levy, Omer",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.446",
    doi = "10.18653/v1/2021.emnlp-main.446",
    pages = "5484--5495",
    abstract = "FFN層はKVメモリになっている",
}

@inproceedings{DissectingRecall,
    title = "Dissecting Recall of Factual Associations in Auto-Regressive Language Models",
    author = "Geva, Mor  and
      Bastings, Jasmijn  and
      Filippova, Katja  and
      Globerson, Amir",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.751",
    doi = "10.18653/v1/2023.emnlp-main.751",
    pages = "12216--12235",
    abstract = "LLMは、初期層で情報圧縮、中間層で関係予測、最後に属性抽出",
}

@article{RAVEL,
  title={RAVEL: Evaluating Interpretability Methods on Disentangling Language Model Representations},
  author={Huang, Jing and Wu, Zhengxuan and Potts, Christopher and Geva, Mor and Geiger, Atticus},
  journal={arXiv preprint arXiv:2402.17700},
  year={2024},
  abstract={"トークンの属性分離タスク"}
}

@inproceedings{ImpactOfIA,
    title = "From Insights to Actions: The Impact of Interpretability and Analysis Research on {NLP}",
    author = "Mosbach, Marius  and
      Gautam, Vagrant  and
      Vergara Browne, Tom{\'a}s  and
      Klakow, Dietrich  and
      Geva, Mor",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.181",
    pages = "3078--3105",
    abstract = "IAって大事だよね",
}

@article{NeuronsAcrossLanguageAndTask,
  title={Sharing matters: Analysing neurons across languages and tasks in llms},
  author={Wang, Weixuan and Haddow, Barry and Wu, Minghao and Peng, Wei and Birch, Alexandra},
  journal={arXiv preprint arXiv:2406.09265},
  year={2024},
  abstract={言語間、タスク間でのニューロンの共通性}
}

@article{FromTokensToWords,
  title={From Tokens to Words: On the Inner Lexicon of LLMs},
  author={Kaplan, Guy and Oren, Matanel and Reif, Yuval and Schwartz, Roy},
  journal={arXiv preprint arXiv:2410.05864},
  year={2024},
  abstract={単語を変な分割にしても復元される→語彙拡張}
}

@article{RepetitionNeurons,
  title={Repetition Neurons: How Do Language Models Produce Repetitions?},
  author={Hiraoka, Tatsuya and Inui, Kentaro},
  journal={arXiv preprint arXiv:2410.13497},
  year={2024},
  abstract={Repetitionを促進するニューロンがある}
}

@inproceedings{KnowledgeNeurons,
    title = "Knowledge Neurons in Pretrained Transformers",
    author = "Dai, Damai  and
      Dong, Li  and
      Hao, Yaru  and
      Sui, Zhifang  and
      Chang, Baobao  and
      Wei, Furu",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.581",
    doi = "10.18653/v1/2022.acl-long.581",
    pages = "8493--8502",
    abstract = "特定の知識を保存しているニューロンがある"
}

@inproceedings{SkillNeurons,
    title = "Finding Skill Neurons in Pre-trained Transformer-based Language Models",
    author = "Wang, Xiaozhi  and
      Wen, Kaiyue  and
      Zhang, Zhengyan  and
      Hou, Lei  and
      Liu, Zhiyuan  and
      Li, Juanzi",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.765",
    doi = "10.18653/v1/2022.emnlp-main.765",
    pages = "11132--11152",
    abstract = "タスクにチューニングすると特定のタスクに特化したニューロンが生成される",
}

@inproceedings{TaskVectors,
    title = "In-Context Learning Creates Task Vectors",
    author = "Hendel, Roee  and
      Geva, Mor  and
      Globerson, Amir",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.624",
    doi = "10.18653/v1/2023.findings-emnlp.624",
    pages = "9318--9333",
    abstract = "ICLは中間層の出力ベクターとしてまとめることができる",
}

@inproceedings{MitigatingRepetition,
    title = "Mitigating the Language Mismatch and Repetition Issues in {LLM}-based Machine Translation via Model Editing",
    author = "Wang, Weichuan  and
      Li, Zhaoyi  and
      Lian, Defu  and
      Ma, Chen  and
      Song, Linqi  and
      Wei, Ying",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.879",
    pages = "15681--15700",
    abstract = "LLMの翻訳タスクでRepetitionと言語間違いが起こりまくるのを抑制。RepetitionNeuron"
}

@inproceedings{BackwardLens,
    title = "Backward Lens: Projecting Language Model Gradients into the Vocabulary Space",
    author = "Katz, Shahar  and
      Belinkov, Yonatan  and
      Geva, Mor  and
      Wolf, Lior",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.142",
    pages = "2390--2422",
    abstract = "逆伝播でもlogit lensを使えるようにして、学習時にどのように知識を増やすかを見る",
}

@misc{LogitLens,
    title = {interpreting GPT: the logit lens},
    url = {https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens},
    author = {nostalgebraist},
    year = {2020},
    note = {Accessed on Nov 27, 2024}
}

@inproceedings{SubwordRobustness,
    title = "Tokenization Falling Short: On Subword Robustness in Large Language Models",
    author = "Chai, Yekun  and
      Fang, Yewei  and
      Peng, Qiwei  and
      Li, Xuhong",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.86",
    pages = "1582--1599",
    abstract = "変な分割や誤字はモデルのスケーリングで改善するが、完ぺきではない",
}

@inproceedings{StagesOfInference,
    title={The Remarkable Robustness of {LLM}s: Stages of Inference?},
    author={Vedang Lad and Wes Gurnee and Max Tegmark},
    booktitle={ICML 2024 Workshop on Mechanistic Interpretability},
    year={2024},
    url={https://openreview.net/forum?id=R5unwb9KPc},
    abstract = "LLMの初期層前半はDetokenization, 初期層後半は特徴量エンジニアリング、中間層は次のトークン予測、最終層付近でデノイズ。同じ単語のmulti tokensは初期層のsub-joiner attention headで先頭tokenを注視"
}

@article{UniversalNeurons,
  title={Universal neurons in gpt2 language models},
  author={Gurnee, Wes and Horsley, Theo and Guo, Zifan Carl and Kheirkhah, Tara Rezaei and Sun, Qinyi and Hathaway, Will and Nanda, Neel and Bertsimas, Dimitris},
  journal={arXiv preprint arXiv:2401.12181},
  year={2024},
  abstract={初期値に非依存なニューロンが存在して、それは予測だったり、ロジット分布を絞ったり、BOSへの注意を無効化するニューロンだったりする}
}

@inproceedings{CopySuppression,
    title = "Copy Suppression: Comprehensively Understanding a Motif in Language Model Attention Heads",
    author = "McDougall, Callum Stuart  and
      Conmy, Arthur  and
      Rushing, Cody  and
      McGrath, Thomas  and
      Nanda, Neel",
    editor = "Belinkov, Yonatan  and
      Kim, Najoung  and
      Jumelet, Jaap  and
      Mohebbi, Hosein  and
      Mueller, Aaron  and
      Chen, Hanjie",
    booktitle = "Proceedings of the 7th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP",
    month = nov,
    year = "2024",
    address = "Miami, Florida, US",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.blackboxnlp-1.22",
    pages = "337--363",
    abstract = "コピーを抑制するHeadがあります",
}

@article{SoftmaxLinearUnits,
   title={Softmax Linear Units},
   author={Elhage, Nelson and Hume, Tristan and Olsson, Catherine and Nanda, Neel and Henighan, Tom and Johnston, Scott and ElShowk, Sheer and Joseph, Nicholas and DasSarma, Nova and Mann, Ben and Hernandez, Danny and Askell, Amanda and Ndousse, Kamal and Jones, Andy and Drain, Dawn and Chen, Anna and Bai, Yuntao and Ganguli, Deep and Lovitt, Liane and Hatfield-Dodds, Zac and Kernion, Jackson and Conerly, Tom and Kravec, Shauna and Fort, Stanislav and Kadavath, Saurav and Jacobson, Josh and Tran-Johnson, Eli and Kaplan, Jared and Clark, Jack and Brown, Tom and McCandlish, Sam and Amodei, Dario and Olah, Christopher},
   year={2022},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2022/solu/index.html},
   abstract={初期層ではsuperposition,働きとしてはDetokenization,最終層ではRetokenization }
}

@inproceedings{SubwordMergeHead,
    title = "Adaptively Sparse Transformers",
    author = "Correia, Gon{\c{c}}alo M.  and
      Niculae, Vlad  and
      Martins, Andr{\'e} F. T.",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1223",
    doi = "10.18653/v1/D19-1223",
    pages = "2174--2184",
    abstract = "Subwordに分割されたmulti token wordsを中間表現でまとめるヘッドがあるよ",
}

@inproceedings{InformationFlowRoutes,
    title = "Information Flow Routes: Automatically Interpreting Language Models at Scale",
    author = "Ferrando, Javier  and
      Voita, Elena",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.965",
    pages = "17432--17445",
    abstract = "LLMにもサブワードマージヘッドがあるよ",
}

@misc{superposition,  
    title={Fact Finding: Attempting to Reverse-Engineer Factual Recall on the Neuron Level},  
    url={https://www.alignmentforum.org/posts/iGuwZTHWb6DFY3sKB/fact-finding-attempting-to-reverse-engineer-factual-recall},  
    journal={Alignment Forum},  
    author={Nanda, Neel and Rajamanoharan, Senthooran and Kram\’ar, J\’anos and Shah, Rohin},  
    year={2023},  
    month={Dec},
    abstract={初期層はsuperposition}
} 

@article{superposition2,
  title={Finding neurons in a haystack: Case studies with sparse probing},
  author={Gurnee, Wes and Nanda, Neel and Pauly, Matthew and Harvey, Katherine and Troitskii, Dmitrii and Bertsimas, Dimitris},
  journal={arXiv preprint arXiv:2305.01610},
  year={2023},
  abstract={初期層はsuperpositionで中間層は専用ニューロン}
}

@inproceedings{NounPhrasesNeurons,
    title={Identifying and Controlling Important Neurons in Neural Machine Translation},
    author={Anthony Bau and Yonatan Belinkov and Hassan Sajjad and Nadir Durrani and Fahim Dalvi and James Glass},
    booktitle={International Conference on Learning Representations},
    year={2019},
    url={https://openreview.net/forum?id=H1z-PsR5KX},
    abstract={名詞句の最初の単語でだけ反応するニューロンがある}
}

@article{InterpretabilityIllusion,
  title={An interpretability illusion for bert},
  author={Bolukbasi, Tolga and Pearce, Adam and Yuan, Ann and Coenen, Andy and Reif, Emily and Vi{\'e}gas, Fernanda and Wattenberg, Martin},
  journal={arXiv preprint arXiv:2104.07143},
  year={2021},
  abstract={各ニューロンは特定の内容にだけ反応するんじゃなくて、いろんなことに反応しているから、あるニューロンが特定の役割しかないというのは難しいよ}
}

@article{CopyInductionHeads,
  title={Induction heads as an essential mechanism for pattern matching in in-context learning},
  author={Crosbie, Joy and Shutova, Ekaterina},
  journal={arXiv preprint arXiv:2407.07011},
  year={2024},
  abstract={中間層ではいったんICLの同じ内容をコピーしている。それより後ろに、コピーを判定して、それを無くすヘッドが存在}
}

@inproceedings{CUTE,
    title = "{CUTE}: Measuring {LLM}s{'} Understanding of Their Tokens",
    author = "Edman, Lukas  and
      Schmid, Helmut  and
      Fraser, Alexander",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.177",
    pages = "3017--3026",
    abstract = "文字レベルの摂動でのスコアを調べるデータセット",
}

@article{HeadsSurvey,
  title={Attention heads of large language models: A survey},
  author={Zheng, Zifan and Wang, Yezhaohui and Huang, Yuxin and Song, Shichao and Yang, Mingchuan and Tang, Bo and Xiong, Feiyu and Li, Zhiyu},
  journal={arXiv preprint arXiv:2409.03752},
  year={2024},
  abstract={アテンションヘッドについてのサーベイ}
}

@inproceedings{SuccessorHeads,
title={Successor Heads: Recurring, Interpretable Attention Heads In The Wild},
author={Rhys Gould and Euan Ong and George Ogden and Arthur Conmy},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=kvcbV8KQsi},
abstract={月曜→火曜のような、次の要素を認識するヘッドが存在する}
}

@inproceedings{HeadPruning,
    title = "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned",
    author = "Voita, Elena  and
      Talbot, David  and
      Moiseev, Fedor  and
      Sennrich, Rico  and
      Titov, Ivan",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'\i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1580",
    doi = "10.18653/v1/P19-1580",
    pages = "5797--5808",
    abstract = "NMTにおいて、重要なheadはS-VやO-Vなど、言語的なつながりを見ているヘッド",
}

@inproceedings{AcronymsTask,
  title={How does GPT-2 Predict Acronyms? Extracting and Understanding a Circuit via Mechanistic Interpretability},
  author={Garc{\'\i}a-Carrasco, Jorge and Mat{\'e}, Alejandro and Trujillo, Juan Carlos},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3322--3330},
  year={2024},
  organization={PMLR},
  abstract={3文字略語予測タスクを用いて、文字送りヘッド(i番目の大文字に注目してi番目の出力に与えるhead)を発見}
}

@article{UnderstandingVulnerabilities,
  title={Detecting and understanding vulnerabilities in language models via mechanistic interpretability},
  author={Garc{\'\i}a-Carrasco, Jorge and Mat{\'e}, Alejandro and Trujillo, Juan},
  journal={arXiv preprint arXiv:2407.19842},
  year={2024},
  abstract={MI技術(パッチング)を使って、どんな脆弱性がよりモデルの性能を下げられるかを見る}
}

@article{SafetyNeurons,
  title={Finding Safety Neurons in Large Language Models},
  author={Chen, Jianhui and Wang, Xiaozhi and Yao, Zijun and Bai, Yushi and Hou, Lei and Li, Juanzi},
  journal={arXiv preprint arXiv:2406.14144},
  year={2024},
  abstract={安全性アライメントを行うニューロンが存在する}
}

@inproceedings{DeepWordBug,
  title={Black-box generation of adversarial text sequences to evade deep learning classifiers},
  author={Gao, Ji and Lanchantin, Jack and Soffa, Mary Lou and Qi, Yanjun},
  booktitle={2018 IEEE Security and Privacy Workshops (SPW)},
  pages={50--56},
  year={2018},
  organization={IEEE},
  abstract={モデルを使って誤字を足そう(誤字の種類はInsert, Delete, Swap, Substitution)}
}

@article{Gemma2,
  title={Gemma 2: Improving open language models at a practical size},
  author={Team, Gemma and Riviere, Morgane and Pathak, Shreya and Sessa, Pier Giuseppe and Hardin, Cassidy and Bhupatiraju, Surya and Hussenot, L{\'e}onard and Mesnard, Thomas and Shahriari, Bobak and Ram{\'e}, Alexandre and others},
  journal={arXiv preprint arXiv:2408.00118},
  year={2024}
}

@article{LLMChatbot,
  title={A complete survey on llm-based ai chatbots},
  author={Dam, Sumit Kumar and Hong, Choong Seon and Qiao, Yu and Zhang, Chaoning},
  journal={arXiv preprint arXiv:2406.16937},
  year={2024}
}

@incollection{WordNet,
	author = {Christiane Fellbaum},
	booktitle = {Encyclopedia of Language and Linguistics},
	editor = {Keith Brown},
	pages = {2--665},
	publisher = {Elsevier},
	title = {Wordnet and Wordnets},
	year = {2005}
}

@inproceedings{TextBugger,
  title={TextBugger: Generating Adversarial Text Against Real-world Applications},
  author={Li, J and Ji, S and Du, T and Li, B and Wang, T},
  booktitle={26th Annual Network and Distributed System Security Symposium},
  year={2019}
}
@inproceedings{VerbWordNet,
    title = "Talking the Talk Does Not Entail Walking the Walk: On the Limits of Large Language Models in Lexical Entailment Recognition",
    author = "Greco, Candida Maria  and
      La Cava, Lucio  and
      Tagarelli, Andrea",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.881",
    doi = "10.18653/v1/2024.findings-emnlp.881",
    pages = "14991--15011",
}
@inproceedings{BertAttention,
    title = "What Does {BERT} Look at? An Analysis of {BERT}{'}s Attention",
    author = "Clark, Kevin  and
      Khandelwal, Urvashi  and
      Levy, Omer  and
      Manning, Christopher D.",
    editor = "Linzen, Tal  and
      Chrupa{\l}a, Grzegorz  and
      Belinkov, Yonatan  and
      Hupkes, Dieuwke",
    booktitle = "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-4828",
    doi = "10.18653/v1/W19-4828",
    pages = "276--286",
}

@inproceedings{bird-loper-2004-nltk,
    title = "{NLTK}: The Natural Language Toolkit",
    author = "Bird, Steven  and
      Loper, Edward",
    booktitle = "Proceedings of the {ACL} Interactive Poster and Demonstration Sessions",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P04-3031",
    pages = "214--217",
}
@article{tsuji2024subregweigh,
  title={SubRegWeigh: Effective and Efficient Annotation Weighing with Subword Regularization},
  author={Tsuji, Kohei and Hiraoka, Tatsuya and Cheng, Yuchang and Iwakura, Tomoya},
  journal={arXiv preprint arXiv:2409.06216},
  year={2024}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, A},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}

@article{llama3,
  title={Llama 3 Model Card},
  author={AI@Meta},
  year={2024},
  url = {https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}
}

@article{qwen2.5,
  title={Qwen2. 5 technical report},
  author={Yang, An and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Wei, Haoran and others},
  journal={arXiv preprint arXiv:2412.15115},
  year={2024}
}

@inproceedings{context_spelling_correction,
    title = "Context-aware Stand-alone Neural Spelling Correction",
    author = "Li, Xiangci  and
      Liu, Hairong  and
      Huang, Liang",
    editor = "Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.37/",
    doi = "10.18653/v1/2020.findings-emnlp.37",
    pages = "407--414",
}

@inproceedings{spellbert,
    title = "{S}pell{BERT}: A Lightweight Pretrained Model for {C}hinese Spelling Check",
    author = "Ji, Tuo  and
      Yan, Hang  and
      Qiu, Xipeng",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.287/",
    doi = "10.18653/v1/2021.emnlp-main.287",
    pages = "3544--3551",
}

@inproceedings{NeuronLevelFT,
    title = "Let`s Focus on Neuron: Neuron-Level Supervised Fine-tuning for Large Language Model",
    author = "Xu, Haoyun  and
      Zhan, Runzhe  and
      Ma, Yingpeng  and
      Wong, Derek F.  and
      Chao, Lidia S.",
    editor = "Rambow, Owen  and
      Wanner, Leo  and
      Apidianaki, Marianna  and
      Al-Khalifa, Hend  and
      Eugenio, Barbara Di  and
      Schockaert, Steven",
    booktitle = "Proceedings of the 31st International Conference on Computational Linguistics",
    month = jan,
    year = "2025",
    address = "Abu Dhabi, UAE",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.coling-main.630/",
    pages = "9393--9406",
}

@article{SE_agent,
  title={Large Language Model-Based Agents for Software Engineering: A Survey},
  author={Junwei Liu and Kaixin Wang and Yixuan Chen and Xin Peng and Zhenpeng Chen and Lingming Zhang and Yiling Lou},
  journal={ArXiv},
  year={2024},
  volume={abs/2409.02977},
  url={https://api.semanticscholar.org/CorpusID:272423732}
}

@article{chatbot_survey,
  title={A complete survey on llm-based ai chatbots},
  author={Dam, Sumit Kumar and Hong, Choong Seon and Qiao, Yu and Zhang, Chaoning},
  journal={arXiv preprint arXiv:2406.16937},
  year={2024}
}

@article{PathPatching,
  title={How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model},
  author={Hanna, Michael and Liu, Ollie and Variengien, Alexandre},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}