\section{Related work}
\subsection{Analysis of LLMs against Typos}
Typos are mistakes in writing or typing letters, categorized into insertion, deletion, substitution, and reordering~\cite{DeepWordBug}.
Research on the robustness of LLMs regards typos as a perturbation.
Typos change the token sequence obtained through the tokenization process. 
Changing the token sequence potentially leads to a different output, even if the sentence is the same~\cite{tsuji2024subregweigh}.
Most existing LLM studies about typos focus on measuring the robustness against perturbed inputs~\cite{AdvGLUE, AdvGLUEpp, Promptrobust, CUTE} or modifying the architecture or prompts to improve robustness~\cite{ RobustnessOfCodex, NoisyExemplars, LEA}. 
\newcite{SubwordRobustness} reported that the larger models are more robust to typos. Before the LLM era, researchers corrected typos using specific models for typo-correction~\cite{context_spelling_correction, spellbert}.

\subsection{LLM's Interpretability}
The feed-forward network (FFN) layer in the Transformer~\cite{vaswani2017attention} has two linear layers separated by an activation function.
Recent studies regard the output of the activation function as ``neurons'' that store knowledge~\cite{KVMemory}.
It has been reported that some neurons promote specific tasks~\cite{SkillNeurons, NeuronsAcrossLanguageAndTask}, knowledge~\cite{KnowledgeNeurons, NounPhrasesNeurons, UniversalNeurons}, and behaviors~\cite{RepetitionNeurons, MitigatingRepetition, SafetyNeurons}.

Some attention heads also respond to specific knowledge~\cite{SuccessorHeads, HeadPruning, AcronymsTask} or behaviors~\cite{CopySuppression, CopyInductionHeads}. 
Additionally, some heads are responsible for merging multiple subwords of a word~\cite{SubwordMergeHead, InformationFlowRoutes}.

There are various methods to investigate LLM's interpretability. Some measure contributions to the residual stream~\cite{UnderstandingVulnerabilities, PathPatching}, while others observe intermediate predictions~\cite{LogitLens, FromTokensToWords}, graph the inference process~\cite{InformationFlowRoutes}, or directly observe activations~\cite{SkillNeurons, RepetitionNeurons, NeuronsAcrossLanguageAndTask}.
We hypothesize that typo neurons are a type of skill neurons. Therefore we use the direct activation observation method, following previous studies on skill neurons~\cite{SkillNeurons, RepetitionNeurons}.
\newcite{ImpactOfIA} concludes that understanding the inner workings is important to improve the model performance.

\newcite{StagesOfInference} divides LLMs into four stages. The early layers convert token-level representations into entity-level representations with local contexts as \textit{Detokenization}. The early middle layers build representations with global contexts as \textit{Feature Engineering}. The late middle layers, convert current representations into next token representations as \textit{Prediction Ensembling}. Finally, the late layers remove the noise and refine the distribution of the next token as \textit{Residual Sharpening}.
\newcite{SoftmaxLinearUnits} reports that the late layers perform the opposite function of the early layers' \textit{Detokenization}, converting entity-level representations into token-level representations as \textit{Retokenization}.

\newcite{FromTokensToWords} reveals which layers are responsible for typo-fixing.
However, they only focused on isolated words as inputs by layer-level observation. 
We focus on neurons and heads and experiment with global contexts.