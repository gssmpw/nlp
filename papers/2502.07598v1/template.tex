% !TeX program = pdflatex
\PassOptionsToPackage{prologue,dvipsnames, table}{xcolor}
\documentclass[review]{fcs}

\title{\revise{}{Towards spatial computing: recent advances in multimodal natural interaction for XR headsets}}
\shorttitle{Recent advances in multimodal natural interaction for XR headsets}
\author[1]{Zhimin Wang} \author[1]{Maohang Rao} \author[1]{Shanghua Ye} \author[2]{Weitao Song} \author*[1]{Feng Lu}
\address[1]{State Key Laboratory of VR Technology and Systems, School of CSE, Beihang University, Beijing, China}
\address[2]{School of Optics and Photonics, Beijing Institute of Technology, Beijing, China}
\fcssetup{
  received       = {month dd, yyyy},
  accepted       = {month dd, yyyy},
  corr-email     = {lufeng@buaa.edu.cn},
}
\usepackage{threeparttable}
\usepackage{multirow}
\usepackage{makecell}    % For \makecell
\usepackage{multicol}
\usepackage[table]{xcolor}

\newcommand{\revise}[2]{\textcolor[rgb]{0,0,0}{#2}} 

\newcommand{\ccite}{\textcolor[rgb]{0,0,1}{[cite]}}

\definecolor{oBlue}{rgb}{0.607, 0.75, 0.851}
\definecolor{oYellow}{rgb}{0.976, 0.858, 0.384}
\definecolor{oRed}{rgb}{0.8, 0.8, 0.8}
\definecolor{oPink}{rgb}{0.964, 0.827, 0.858}

\begin{abstract}
With the widespread adoption of Extended Reality (XR) headsets, spatial computing technologies are gaining increasing attention. Spatial computing enables interaction with virtual elements through natural input methods such as eye tracking, hand gestures, and voice commands, thus placing natural human-computer interaction at its core. While previous surveys have reviewed conventional XR interaction techniques, recent advancements in natural interaction, particularly driven by artificial intelligence (AI) and large language models (LLMs), have introduced new paradigms and technologies. In this paper, we review research on multimodal natural interaction for wearable XR, focusing on papers published between 2022 and 2024 in six top venues: ACM CHI, UIST, \revise{}{IMWUT (Ubicomp)}, IEEE VR, ISMAR, and TVCG.
We classify and analyze these studies based on application scenarios, operation types, and interaction modalities. This analysis provides a structured framework for understanding how researchers are designing advanced natural interaction techniques in XR. Based on these findings, we discuss the challenges in natural interaction techniques and suggest potential directions for future research. This review provides valuable insights for researchers aiming to design natural and efficient interaction systems for XR, ultimately contributing to the advancement of spatial computing.
\end{abstract}
\keywords{extended reality, multimodal, natural interaction, eye, hand, speech}

\begin{document}

\section{Introduction}

Extended Reality (XR), which includes Virtual Reality (VR), Augmented Reality (AR), and Mixed Reality (MR), merges the virtual and physical worlds to provide immersive experiences. 
In recent years, XR technologies have developed rapidly. This has led to the widespread adoption of headsets such as the Microsoft HoloLens 2 \cite{HoloLens} and Meta Quest 3 \cite{Meta}, across various fields, 
including industrial maintenance, remote collaboration, online education and entertainment \cite{DBLP:conf/vr/JingLB22, DBLP:conf/vr/QuereMJWW24, Barteit2021, an2023arcosmetics}. These developments highlight XR's potential for diverse applications and its promising market prospects.

In 2024, Apple introduced a new XR headset called the Vision Pro, which reignited public enthusiasm for XR and marked the beginning of a new era in spatial computing \cite{Apple}. 
Spatial computing leverages advanced technologies to perceive and digitize the surrounding physical environment. It seamlessly integrates this environment with computer-generated virtual content, enabling natural interactions between humans and digital systems \cite{spatialcomputing}. 
\revise{The advent of spatial computing is expected to initiate a transformative 'iPhone moment' for XR \cite{hackl2024spatial}.}{The advent of spatial computing is expected to bring a transformative redefinition to XR devices, often referred to as the `iPhone moment' for XR \cite{hackl2024spatial}.}

% Spatial computing digitizes the surrounding real world and seamlessly integrates it with computer-generated virtual content, allowing users to interact with digital elements through natural input channels such as eye tracking, hand gestures, and voice commands. This creates an immersive experience that blends the virtual and physical worlds. As a result, natural human-computer interaction techniques are at the core of spatial computing.

\revise{At the core of spatial computing is human computer interaction (HCI)}{The core of spatial computing is human-computer interaction (HCI)}, with a particular focus on developing natural and intuitive interaction techniques \cite{Apple}. Traditional methods, such as keyboards and handheld controllers, are inadequate for delivering the immersive experiences \cite{10108465, DBLP:conf/interact/BerardIBEBC09}. 
\revise{In response, research has increasingly focused on direct human inputs as interaction channels, including eye gaze, hand gestures, and voice commands}{In light of this, research has increasingly focused on direct human input as interaction channels, including eye gaze, hand gestures, and voice commands} \cite{DBLP:conf/vr/MatthewsTIS22, DBLP:conf/vr/GiunchiNGS24, chai2022speech, ding2016survey, huang2024matching, DBLP:conf/ismar/WangGL23}. 
Although these modalities provide more intuitive interfaces, each faces significant limitations.
\revise{For example, gaze-based interaction encounters the Midas Touch problem \cite{DBLP:conf/ismar/MohanGFY18, velichkovsky1997towards} and often lacks accuracy \cite{10108465}. Gesture-based interaction can cause arm fatigue after prolonged use \cite{DBLP:conf/vr/ChaconasH18,DBLP:conf/chi/Hincapie-RamosGMI14}, and speech-based interaction increases cognitive load due to the need to remember complex commands \cite{DBLP:conf/ismar/ChenGFCL23}. Thus, the efficiency of unimodal natural interactions requires further improvement.}{For example, gesture-based interaction can cause arm fatigue after prolonged use \cite{DBLP:conf/vr/ChaconasH18,DBLP:conf/chi/Hincapie-RamosGMI14}. The performance of unimodal natural interactions requires further improvement.}


\begin{figure*}
    \begin{center}
    % \begin{overpic} 
    % [width=\linewidth]
    % {example-image-a}
    % \end{overpic}
    \includegraphics[width=0.8\linewidth]{overview.jpg}
    \end{center}
    %\vspace{-4mm}
    \caption{
    \revise{}{We classify operation types into seven categories based on whether users actively input or passively receive feedback. Additionally, we divide interaction modalities into nine categories, distinguishing between unimodal and multimodal natural interactions. (The overlap between Operation Types and Interaction Modalities is shown in table. 1)}
    }
    \label{fig:taxonomy}
    %\vspace{-2mm}
\end{figure*}


To address these challenges, multimodal interaction has emerged as a promising solution by combining the strengths of individual modalities. \revise{Typically, such systems integrate two modalities, \textit{e.g.}, Gesture + Speech \cite{DBLP:conf/chi/CaoKWAX24, DBLP:conf/chi/TorreFHBFL24}, Gaze + Gesture \cite{DBLP:journals/tvcg/SidenmarkP0CGWG22, 10.1145/3530886}, Gaze + Speech \cite{10.1145/3613904.3642068}, or Gaze + Electroencephalography \cite{DBLP:conf/embc/WangDCS15}.}{Such systems integrate typically two modalities, \textit{e.g.}, Gesture + Speech \cite{DBLP:conf/chi/CaoKWAX24, DBLP:conf/chi/TorreFHBFL24}, Gaze + Gesture \cite{DBLP:journals/tvcg/SidenmarkP0CGWG22, 10.1145/3530886}, Gaze + Speech \cite{10.1145/3613904.3642068}, and Gaze + Electroencephalography \cite{DBLP:conf/embc/WangDCS15}.}
\revise{In these systems, each modality is assigned a specific task.}{Each modality in these systems is assigned a specific task.} For example, a user might select an object using the eye gaze and trigger an action with a hand gesture \cite{10108465}. These approaches enhance the XR experience, providing users with a more natural and efficient way to interact with digital environments.


In recent years, researchers have conducted extensive reviews on various aspects of XR interaction techniques. These reviews have typically focused on specific areas such as AR environments \cite{DBLP:journals/tvcg/TranBWBP23, DBLP:conf/ismar/HertelKSBSS21}, VR environments \cite{pirker2021potential, katona2021review}, or immersive environments involving handheld displays \cite{DBLP:journals/vi/ZhangWZST23, DBLP:journals/tvcg/SpittlePC023}, highlighting the advantages, challenges, and emerging trends in each domain \cite{DBLP:journals/tvcg/TranBWBP23, DBLP:journals/tvcg/SpittlePC023}. However, the rapid advancements in natural interaction, particularly driven by artificial intelligence (AI) and large language models (LLMs) \cite{DBLP:conf/vr/GiunchiNGS24, DBLP:conf/chi/WangYWJ024, DBLP:conf/vr/YangQCSBLL24}, have introduced new interaction paradigms and technologies. It underscores the need for updated reviews that synthesize and evaluate the latest developments in the field.

In this paper, we aim to capture the latest trends by providing a comprehensive review of multimodal natural interaction techniques for wearable XR. \revise{}{Specifically, our objective is to determine and answer the following research questions (RQs):}

\begin{enumerate}
    \item \revise{}{What novel interaction paradigms and techniques have emerged over the past three years? (answered in Section \ref{Interaction techniques})}

    \item \revise{}{What are the key evolving trends of multimodal natural interaction in XR over the past three years? (answered in Section \ref{4.1} and \ref{4.2})}

    \item \revise{}{How have recent advancements in AI and LLMs been leveraged to enhance natural interaction in XR environments? (answered in Section \ref{4.3})}
\end{enumerate}

\revise{We focus on papers published between 2022 and 2024 across six top venues: ACM CHI, UIST, IMWUT(UbiComp), IEEE VR, ISMAR, and TVCG.}{In order to answer these questions, we investigate papers published between 2022 and 2024 across six top venues: ACM CHI, UIST, IMWUT, IEEE VR, ISMAR, and TVCG.} We categorize the reviewed literature based on application context, operation types, and interaction modalities. \revise{Specifically, operation types are divided into seven categories}{Particularly, operation types are divided into seven categories}, distinguishing between active and passive interactions, as shown in Fig. \ref{fig:taxonomy}. Interaction modalities are discussed across nine distinct types.
\revise{Additionally, we present statistical results into advanced natural interaction techniques}{Additionally, we present statistical results on advanced natural interaction techniques.}
\revise{Based on these findings, we discuss the current challenges of natural interaction techniques and suggest potential future research directions.}{Based on these findings, we discuss the current challenges of natural interaction techniques and propose potential directions for future research.} Our review offers valuable insights for researchers and promotes the further development of multimodal natural interaction for XR headsets.

% In this paper, we aim to 掌握最新发展趋势 by offering a comprehensive review of multimodal natural interaction techniques for wearable XR.
% We focus on papers published between 2022 and 2024 in six major venues: CHI, UIST, IMWUT, IEEE VR, ISMAR, and TVCG. 我们分别按照应用背景、操作类型、performance measures, and interaction modalities去讨论这些文献。其中操作类型按照主动和被动交互，将操作类型分成6类进行阐述。交互模态分成9种类型进行阐述。基于上述方式，我们掌握了目前研究者们设计的advanced natural interaction techniques的一些统计学规律。Following these findings，我们讨论了目前自然交互技巧的挑战，并且建议了未来的潜在研究方向。Our review 将为研究者提供有价值的见解，并且促进multimodal natural interaction for XR headsets领域的发展。

The contributions of this review are threefold:

\begin{enumerate}
\item We provide a systematic review of the latest developments in multimodal interaction techniques for wearable XR, drawing from six top venues with papers published between 2022 and 2024.

\item We categorize these papers based on application contexts, operation types, performance measures, and interaction modalities, and present statistical insights into advanced natural interaction techniques.

\item We identify the current challenges of natural interaction techniques and propose potential directions for future research to improve the effectiveness and usability of multimodal interactions in XR.
\end{enumerate}

% The remainder of this paper is organized as follows. Section 2 outlines the methodology used for selecting the reviewed literature and provides an analysis of these papers. Section 3 presents a detailed review and analysis of natural interaction techniques in XR, based on the aforementioned categories. Section 4 discusses the key challenges and offers recommendations for future research on natural interaction techniques in XR. Finally, Section 5 描述本文 its limitations。Section 6 concludes the paper.

\revise{}{\textbf{Relationship to other reviews.} Several prior reviews have explored immersive interaction techniques, but with distinct scopes and focuses compared to our work. Hertel \textit{et al.} \cite{DBLP:conf/ismar/HertelKSBSS21} proposed a taxonomy for AR interaction techniques, focusing on task and modality dimensions based on works from 2016 to 2021. Spittle \textit{et al.} \cite{DBLP:journals/tvcg/SpittlePC023} reviewed AR/VR interaction techniques from 2013 to 2020, focusing on display type, study type, input methods, and tasks. 
Pirker \textit{et al.} \cite{pirker2021potential} analyzed the educational applications of 360° videos and real VR from 2010 to 2020, including language learning, teacher education, history and social studies, \textit{etc}. Zhang \textit{et al.} \cite{DBLP:journals/vi/ZhangWZST23} focused on immersive visualization from 2014 to 2023, detailing multimodal perception and interaction techniques, particularly sensory modalities including vision, touch, and olfaction, while also analyzing collaborative analysis and hardware devices. 
Ghamandi \textit{et al.} \cite{DBLP:conf/ismar/GhamandiHNGKTL23} reviewed 30 years of collaborative XR tasks (1993–2023) and proposed a taxonomy that classifies tasks based on actions (\textit{e.g.}, manipulation and navigation) and properties (\textit{e.g.}, temporal state and dependency). Their work focuses on understanding task structures and collaboration dynamics across the mixed reality spectrum.
In contrast, our review focuses on recent developments (2022–2024) in multimodal natural interaction for wearable XR headsets, emphasizing the integration of AI and LLMs to enable novel interaction paradigms and providing a structured framework for advancing spatial computing.}


The remainder of this paper is organized as follows. \revise{Section 2 outlines the methodology used for selecting the reviewed literature and provides an analysis of the selected papers.}{Section 2 outlines the methodology to select the reviewed literature and provides an statistic analysis of the selected papers.} \revise{Section 3 presents a detailed review and analysis of natural interaction techniques in XR, categorized based on the previously mentioned criteria}{Section 3 presents a detailed review and analysis of natural interaction techniques in XR, categorized based on the criteria mentioned above}. Section 4 discusses key challenges and offers recommendations for future research on natural interaction techniques in XR. Finally, Section 5 describes the limitations of this study, and Section 6 concludes the paper.

% The remainder of this paper is organized as follows. Section 2 describes the methodology used for selecting these reviewed literature and data analysis about these papers. Section 3 offers an detailed review and analysis about the natural interaction techniques in XR based above categories. Section 4 discusses the key challenges and recommend the future research about natural interaction techniques in XR. Finally, Section 5 总结这篇论文并且描述limitations.



\renewcommand{\dblfloatpagefraction}{.9}
% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table*}
\renewcommand\arraystretch{1.2}
  \centering
  \caption{The reviewed literature is categorized based on six operation types, nine interaction modalities, and year of publication \revise{}{(``X+Y'' represents other modality combinations, where ``X'' denotes a modality from gesture, speech, or gaze, and ``Y'' denotes any other modality except these three (\textit{e.g.}, gaze + Head pose))}}
  \scalebox{0.97}{
  \begin{tabular}{m{1.6cm}<{\centering}|m{0.7cm}<{\centering}|m{1.29cm}<{\centering}|m{1.29cm}<{\centering}|m{1.29cm}<{\centering}|m{1.2cm}<{\centering}|m{1.29cm}<{\centering}|m{1.29cm}<{\centering}|m{1.29cm}<{\centering}|m{1.2cm}<{\centering}|m{1.2cm}<{\centering}}
    \hline
    
     \cellcolor{oBlue!80}  & \cellcolor{oPink!80} & \multicolumn{4}{c|}{ \cellcolor{oYellow!90}{Unimodal Interactions}} & \multicolumn{5}{c}{\cellcolor{oRed!100}{Multimodal Interactions}} \\
\cline{3-11}    \multirow{-2}{1.6cm}{\centering \cellcolor{oBlue!80} Operation Types}      &  \multirow{-3}{0.7cm}{\centering \cellcolor{oPink!80} Years}   & \cellcolor{oYellow!20}{GEST only} & \cellcolor{oYellow!20}{GAZE only} & \cellcolor{oYellow!20}{SPCH only} & \cellcolor{oYellow!20}{Tactile} & \cellcolor{oRed!20}{GAZE + GEST} & \cellcolor{oRed!20}{GAZE + SPCH} & \cellcolor{oRed!20}{GEST + SPCH} & \cellcolor{oRed!20}{GAZE + GEST + SPCH} & \cellcolor{oRed!20}{X + Y} \\
    \hline
       \cellcolor{oBlue!30} & \cellcolor{oPink!30} 2022  & \cite{DBLP:conf/vr/MatthewsTIS22}\cite{DBLP:conf/ismar/ChowdhuryUPIH22}\cite{DBLP:conf/chi/SchmitzGS022}\cite{DBLP:conf/ismar/BanMNK22}\cite{DBLP:conf/ismar/YuZDV022}  & \cite{DBLP:conf/ismar/LeeHM22}\cite{DBLP:conf/chi/ChoiSO22}\cite{kim2022lattice}\cite{DBLP:conf/vr/0001LC00S22}\cite{DBLP:journals/tvcg/WangZ022}  &   & \cite{DBLP:journals/imwut/ChenLYZ22}  & \cite{DBLP:journals/tvcg/SidenmarkP0CGWG22}\cite{10.1145/3530886}\cite{DBLP:conf/uist/SendhilnathanZL22}  & \cite{DBLP:conf/vr/JingLB22}  & \cite{DBLP:conf/uist/LiaoKJKS22}  &   & \cite{DBLP:conf/uist/0001QTFLS22}\cite{DBLP:conf/ismar/XuMYSL22}\cite{DBLP:journals/imwut/ShenYYS22}\cite{DBLP:conf/ismar/MengXL22}\\
\cline{2-11}      \cellcolor{oBlue!30}   &  \cellcolor{oPink!30} 2023  & \cite{DBLP:conf/ismar/DasNH23}\cite{DBLP:conf/ismar/ZhuSSG23}\cite{DBLP:journals/tvcg/SongDK23}\cite{DBLP:conf/chi/0003HLG23}  & \cite{DBLP:conf/ismar/ChenHTHH23}\cite{DBLP:conf/chi/SidenmarkCNLPG23}  &   & \cite{DBLP:conf/ismar/DasNH23}  & \cite{10.1145/3544548.3581423}\cite{10.1145/3591129}\cite{10108465}\cite{DBLP:conf/ismar/CailletGN23}  &   &   &  \cite{DBLP:conf/ismar/ChenGFCL23}  & \cite{DBLP:conf/chi/WeiSYW0YL23}\cite{DBLP:conf/chi/HouNSKBG23}\cite{10049667} \\
\cline{2-11}         \multirow{-5}{1.6cm}{\centering \cellcolor{oBlue!30} Pointing and Selection} &  \cellcolor{oPink!30} 2024  & \cite{DBLP:conf/vr/QuereMJWW24}\cite{DBLP:conf/vr/SindhupathirajaUDH24}\cite{DBLP:conf/chi/DupreARSP24}  & \cite{DBLP:conf/vr/OrloskyLSSM24}\cite{DBLP:conf/chi/ZhangCSS24}\cite{DBLP:conf/chi/TurkmenGBSASPM24}  &   &   & \cite{DBLP:conf/chi/ZennerKFAK24}\cite{10.1145/3613904.3642758}  &   &   &   & \cite{DBLP:conf/vr/LaiSL24}\cite{marquardt2024selection} \\
    \hline
    \cellcolor{oBlue!30} & \cellcolor{oPink!30} 2022  & \cite{DBLP:conf/ismar/YuZDV022}\cite{DBLP:conf/chi/PeiCLZ22}  &   &   & \cite{DBLP:journals/imwut/ChenLYZ22}\cite{DBLP:conf/chi/SatriadiSECCLYD22}  &   &   & \cite{DBLP:conf/uist/LiaoKJKS22}  &   &  \\
\cline{2-11}    \cellcolor{oBlue!30}     & \cellcolor{oPink!30} 2023  & \cite{DBLP:journals/tvcg/XuZSFY23}\cite{DBLP:journals/tvcg/SongDK23} &   &   & \cite{DBLP:journals/imwut/ZhanXZGCGLQ23}  &   &   &   & \cite{DBLP:conf/ismar/ChenGFCL23}  &  \\
\cline{2-11}     \multirow{-3}{1.6cm}{\centering \cellcolor{oBlue!30} Creation and Editing}   & \cellcolor{oPink!30} 2024  & \cite{DBLP:conf/vr/QuereMJWW24}\cite{DBLP:conf/chi/DupreARSP24}  & \cite{DBLP:conf/chi/TurkmenGBSASPM24}  & \cite{DBLP:conf/chi/TorreFHBFL24}\cite{DBLP:conf/vr/GiunchiNGS24}  &   &   &   & \cite{DBLP:conf/chi/CaoKWAX24}\cite{10.1145/3613904.3642758}  &   &  \\
    \hline
    \cellcolor{oBlue!30} & \cellcolor{oPink!30} 2022  & \cite{DBLP:conf/chi/PeiCLZ22}\cite{DBLP:conf/ismar/BanMNK22}\cite{DBLP:conf/ismar/YuZDV022}   &   &   & \cite{DBLP:conf/chi/SatriadiSECCLYD22}  &   &   &\cite{9873984}   &   &  \\
\cline{2-11}    \cellcolor{oBlue!30}     & \cellcolor{oPink!30} 2023  & \cite{DBLP:journals/tvcg/SongDK23}\cite{DBLP:journals/tvcg/XuZSFY23}\cite{DBLP:conf/chi/0003HLG23}  &   &   &   & \cite{10108465}  &   &   &   & \cite{DBLP:conf/chi/HouNSKBG23} \\
\cline{2-11}  \multirow{-4}{1.6cm}{\centering \cellcolor{oBlue!30} Translation and Transform}       & \cellcolor{oPink!30} 2024  & \cite{DBLP:journals/ijhci/DengSZK24}  &   & \cite{DBLP:conf/vr/GiunchiNGS24}  &   &   &   & \cite{DBLP:conf/chi/CaoKWAX24}  &   &  \\
    \hline
    \cellcolor{oBlue!30} & \cellcolor{oPink!30} 2022  & \cite{DBLP:conf/ismar/YuZDV022}\cite{DBLP:conf/ismar/ChowdhuryUPIH22}  &   &   &   &   &   &   &   &  \\
\cline{2-11}    \cellcolor{oBlue!30}     & \cellcolor{oPink!30} 2023  &   &   & \cite{DBLP:conf/vr/HombeckVHDL23}  & \cite{DBLP:conf/vr/MortezapoorVVK23}  &   &   &   &   &  \\
\cline{2-11}     \multirow{-4}{1.6cm}{\centering \cellcolor{oBlue!30} Locomotion and Viewport}    & \cellcolor{oPink!30} 2024  & \cite{DBLP:conf/vr/SindhupathirajaUDH24}\cite{DBLP:conf/vr/SinJLLLN24}  &   & \cite{DBLP:conf/chi/WangYWJ024}  &   & \cite{10.1145/3613904.3642147}  &   &   &   & \cite{DBLP:conf/chi/LeeWSG24} \\
    \hline
    \cellcolor{oBlue!30} & \cellcolor{oPink!30}2022  & \cite{10.1145/3491102.3517682}\cite{DBLP:conf/ismar/SongDK22}  &   &   &   & \cite{DBLP:conf/chi/HeLP22}  &   & \cite{DBLP:conf/uist/LiaoKJKS22}  &   &  \\
\cline{2-11}     \cellcolor{oBlue!30}    & \cellcolor{oPink!30}2023  & \cite{DBLP:journals/tvcg/ShenDK23}  & \cite{DBLP:conf/iui/ZhaoPTZWJBG23}\cite{cui2023glancewriter}  & \cite{DBLP:conf/chi/ZhangLHWLGZ23}  & \cite{DBLP:journals/imwut/ZhanXZGCGLQ23}  &   &   &   &   &  \\
\cline{2-11}    \multirow{-3}{1.6cm}{\centering \cellcolor{oBlue!30} Typing and Querying}      & \cellcolor{oPink!30} 2024  & \cite{shen2024ringgesture}  & \cite{DBLP:conf/vr/HuDK24}  & \cite{DBLP:conf/chi/TorreFHBFL24}\cite{DBLP:journals/tvcg/CaiML24}\cite{DBLP:conf/chi/WangYWJ024}  &   & \cite{10474330}& \cite{10.1145/3613904.3642068}\cite{DBLP:journals/corr/abs-2405-18537}\cite{DBLP:journals/imwut/WangSWYYWJXY24}  &   & \cite{DBLP:conf/chi/0005WBCRF24}  &  \\

 \hline
    \multicolumn{2}{c|}{\cellcolor{oBlue!30} No Operation} & \cite{DBLP:conf/chi/WangLZ24}\cite{DBLP:journals/tvcg/ShenDMK22}\cite{DBLP:conf/chi/LeeZAYGLKYDLSGZ24}\cite{DBLP:conf/vr/RuppGBK24}  &   & \cite{10522613}\cite{DBLP:journals/tvcg/CaiML24} & \cite{DBLP:conf/chi/XuZKN23}\cite{DBLP:conf/iswc/KitamuraYS23}\cite{DBLP:conf/ismar/LiLMHLS22}  &   &   &   &   &  \\
    \hline
    \end{tabular}%
    }
  \label{tab:table1}%
\end{table*}%

\begin{table}[t]
    \centering
    \renewcommand\arraystretch{1.2}
    \caption{The literature related to passive interaction}
    \begin{tabular}{m{2.5cm}<{\centering}|m{4cm}<{\centering}}
    \hline
        \cellcolor{oBlue!80} Passive Interaction & \cellcolor{oPink!80} Literature \\ \hline
        \cellcolor{oBlue!30} Visual & \cite{DBLP:conf/chi/Pohlmann0MMB23}\cite{DBLP:conf/chi/MedlarLG24}\cite{DBLP:conf/chi/FeickR0K22}\cite{DBLP:conf/chi/WuQQCRS24}\cite{DBLP:conf/vr/YangQCSBLL24}\cite{DBLP:conf/chi/ElsharkawyAYAHK24}\cite{DBLP:conf/vr/LiLYTFX24}\cite{DBLP:conf/chi/RaschRS023}\cite{DBLP:conf/chi/TanX0SZHH24}\cite{DBLP:conf/vr/WangZF24}\cite{DBLP:conf/uist/Tao022}\cite{10462901}\cite{wang2024tasks} \\ \hline
        \cellcolor{oBlue!30} Acoustic & \cite{DBLP:conf/vr/YangQCSBLL24}\cite{DBLP:conf/chi/Pohlmann0MMB23}\cite{DBLP:conf/chi/ElsharkawyAYAHK24} \\ \hline
        \cellcolor{oBlue!30} Haptic & \cite{DBLP:conf/chi/ShenS022}\cite{DBLP:conf/chi/TatzgernDWCEDGH22}\cite{DBLP:conf/chi/0001OPSB24}\cite{DBLP:conf/vr/YamazakiH23}\cite{DBLP:conf/uist/ShenRM0S23}\cite{DBLP:conf/uist/JinguWS23}\cite{DBLP:conf/vr/SaintAubertAMPAL23} \\ \hline
        \cellcolor{oBlue!30} Hybrid  & \cite{DBLP:conf/chi/FeickR0K22}\cite{DBLP:conf/chi/Pohlmann0MMB23}\cite{DBLP:conf/chi/0001OPSB24} \\ \hline
    \end{tabular}
    \label{tab:table1}
\end{table}


\section{Survey Methodology}

We conducted a systematic review and analysis of natural interaction techniques in wearable XR. To this end, we surveyed relevant papers from the top conferences and journals in the field, focusing on recent publications. Additionally, we categorized and coded these papers based on various aspects of natural interaction techniques in XR.


\subsection{Selection Criteria}
\label{criteria}
This review focuses on the technologies and applications of these interaction techniques within XR. \revise{We selected relevant papers based on the following criteria}{Relevant papers are selected based on the following criteria}:

1. \textbf{\revise{Wearable XR (including VR, AR, and MR).}{Involve wearable XR (including VR, AR, and MR).}} We focus on wearable XR because head-mounted displays (HMDs) offer superior mobility and support more natural interaction methods, such as gaze, gesture, and voice control, which ultimately enhance the sense of immersion.

2. \textbf{\revise{Considering natural interaction techniques.}{Consider natural interaction techniques.}} \revise{We examined both unimodal and multimodal interactions.}{We take both unimodal and multimodal interactions into consideration.}  \revise{Unimodal interactions include hand gestures, eye gaze, speech, and tactile inputs, while multimodal interactions combine these modalities.}{Unimodal interactions include hand gesture, eye gaze, speech, and tactile input, while multimodal interactions combine these modalities.} \revise{Among the unimodal types, hand-based interactions are the most common in everyday life, such as pouring water or typing on a keyboard, and they allow for precise operations.}{Among the unimodal interactions, hand-based ones allow precise operations and are most commonly used in daily life, such as pressing buttons and typing on a keyboard.} \revise{With the widespread adoption of smartphones, voice interaction has also gained significant use, with systems like Apple Siri, Microsoft Cortana, and Xiaomi XiaoAI improving input efficiency.}{With the widespread adoption of smartphones, voice interactions have also become progressively prevalent, with systems like Apple Siri, Microsoft Cortana, and Xiaomi XiaoAI enhancing input efficiency.} In recent years, advances in eye-tracking technology have drawn increasing attention to gaze-based interaction. For tactile interactions, we focus on lightweight, wearable, and technologically advanced input devices.

\begin{figure*}
    \begin{center}
    % \begin{overpic} 
    % [width=\linewidth]
    % {example-image-a}
    % \end{overpic}
    \includegraphics[width=1\linewidth]{ByModality2.jpg}
    \end{center}
    %\vspace{-4mm}
    \caption{
The bar charts display interaction modality data, categorized by nine modalities and three years. This figure is consistent with the literature recorded in Table \ref{tab:table1}.
    }
    \label{fig:bymodality}
    %\vspace{-2mm}
\end{figure*}




% \begin{table}[t]
%     \centering
%     \renewcommand\arraystretch{1.2}
%     \caption{The literature related to passive interaction}
%     \begin{tabular}{m{2.5cm}<{\centering}|m{4cm}<{\centering}}
%     \hline
%         \cellcolor{oBlue!80} Passive Interaction & \cellcolor{oPink!80} Literature \\ \hline
%         \cellcolor{oBlue!30} Visual & \cite{DBLP:conf/chi/Pohlmann0MMB23}\cite{DBLP:conf/chi/MedlarLG24}\cite{DBLP:conf/chi/FeickR0K22}\cite{DBLP:conf/chi/WuQQCRS24}\cite{DBLP:conf/vr/YangQCSBLL24}\cite{DBLP:conf/chi/ElsharkawyAYAHK24}\cite{DBLP:conf/vr/LiLYTFX24}\cite{DBLP:conf/chi/RaschRS023}\cite{DBLP:conf/chi/TanX0SZHH24}\cite{DBLP:conf/vr/WangZF24}\cite{DBLP:conf/uist/Tao022}\cite{10462901}\cite{wang2024tasks} \\ \hline
%         \cellcolor{oBlue!30} Acoustic & \cite{DBLP:conf/vr/YangQCSBLL24}\cite{DBLP:conf/chi/Pohlmann0MMB23}\cite{DBLP:conf/chi/ElsharkawyAYAHK24} \\ \hline
%         \cellcolor{oBlue!30} Haptic & \cite{DBLP:conf/chi/ShenS022}\cite{DBLP:conf/chi/TatzgernDWCEDGH22}\cite{DBLP:conf/chi/0001OPSB24}\cite{DBLP:conf/vr/YamazakiH23}\cite{DBLP:conf/uist/ShenRM0S23}\cite{DBLP:conf/uist/JinguWS23}\cite{DBLP:conf/vr/SaintAubertAMPAL23} \\ \hline
%         \cellcolor{oBlue!30} Hybrid  & \cite{DBLP:conf/chi/FeickR0K22}\cite{DBLP:conf/chi/Pohlmann0MMB23}\cite{DBLP:conf/chi/0001OPSB24} \\ \hline
%     \end{tabular}
%     \label{passive}
% \end{table}

3. \textbf{\revise{Focusing on top-tier conferences and journals.}{Focus on top-tier conferences and journals.}} \revise{We concentrated on research published in leading venues such as ACM CHI, UIST, IMWUT,  IEEE VR, ISMAR, and IEEE TVCG.}{We concentrate on research published in leading venues such as ACM CHI, UIST, IMWUT (UbiComp),  IEEE VR, ISMAR, and IEEE TVCG.} These publications typically feature pioneering work, often marked by innovative contributions.

4. \textbf{\revise{Focusing on research from 2022, 2023, and 2024}{Focus on research from 2022, 2023, and 2024}\footnote{Our final search was conducted on 16th October 2024.}}. \revise{The release of XR devices such as Microsoft HoloLens 2, Apple Vision Pro, HTC Vive, Meta Quest, and Pico series has provided significant technical support to the field of XR research.}{The release of XR devices, such as Microsoft HoloLens 2, Apple Vision Pro, HTC Vive, Meta Quest, and Pico series, has provided significant technical support to the field of XR research.} We believe that recent studies offer valuable insights for researchers in this area.

% 本文主要聚焦于spatial computing涉及到的技术和应用。我们根据如下标准，寻找论文：
% （1）将空间计算限制为头戴式XR（包含VR、AR、MR）；因为头戴式显示支持可移动，提供更强的沉浸感；
% （2）考虑自然交互techniques，单模态交互主要包括hand gesture, eye gaze, speech, tactile interactions。多模态交互是这些模态的组合。因为hand-based interaction是我们日常生活中使用最多的交互，例如倒水、敲键盘等。手势交互可以帮我们执行非常精细化的操作。随着智能手机的兴起，语音交互也逐步被大家广泛使用，从Apple Siri，Microsoft Cortana到Xiaomi XiaoAI，也提高了我们输入的效率。近些年来，随着eye-tracking技术的发展，gaze-based interaction也被更多的学者研究。tactile interactions我们关注additional input devices that are light-weight, wearable and technologically advanced。
% （3）我们主要聚焦于顶会顶刊的研究，包括CHI, UIST, IMWUT, IEEE VR, ISMAR, TVCG。这些刊物上的交互研究一般是先驱研究，多为创新性成果。
% （4）我们聚焦于2022，2023，2024年的研究成果。随着Microsoft HoloLens 2, Apple Vision Pro, HTC Vive，Meta Quest系列，Pico系列等XR设备的发行，为XR研究提供了技术支持。我们也认为近三年的研究，能为领域内的研究人员，提供更多的参考价值。



\subsection{Data Collection}

We search keywords in Google Scholar such as ``virtual reality'', ``augmented reality", ``extended reality", ``multimodal interaction'', ``eye gaze", ``hand", ``speech" and ``tactile". 
Article retrieval is primarily conducted following the standards outlined in Section \ref{criteria}. In addition, we identify some articles from other high-level journals and conferences that also meet criteria 1, 2, and 4 from Section \ref{criteria}. Although smaller in number, this portion of the literature serves as a valuable supplement to the collected data.

In summary, we gather a total of 104 research papers. These include 40 from CHI, 20 from IEEE VR, 14 from ISMAR, 14 from TVCG, 6 from UIST, 4 from IMWUT, 5 from other conferences and journals (IJHCS, ETRA, IUI, ISWC), and 1 from ArXiv.

% 总共收集了XX篇文献，其中CHI (35), IEEE (18), ISMAR (12) ...









\subsection{Data Analysis} % 应用分几类，操作分几类，交互模态分几类。
%这些收集到的文献涉及多种交互模态以及他们的组合模态，并且专注于多种应用背景和操作类型。我们根据多个维度对这些文献进行了分类分析（具体的分类方法见section 3）。在section 3中，我们对不同的应用背景中、不同的操作类型中，以及不同的交互方式中所体现的交互行为特征以及近期研究针对这些特征所展现出的不同思路做出了详尽的论述。在本节中，我们将从不同视角对这些文献进行统计分析，以从宏观上发掘近年来不同类别的交互研究所获得关注的数量和比例。同时该统计结果是我们在section 3中提出的Taxonomy的支撑。
% The collected literature encompasses various interaction modalities and their combinations, focusing on diverse application contexts and operation types. These publications were classified and analyzed across multiple dimensions (specific categories are described in section \ref{section3}). In section \ref{section3}, detailed discussions are presented on the interaction behavior characteristics in different application contexts, operation types, performance measures and interaction modalities. In this section, a statistical analysis of these publications from different perspectives is conducted to  explore the quantity and proportion of attention given to different categories of interaction-related issues in recent years. This statistical result supports the taxonomy proposed in section \ref{section3}.

The collected literature covers a range of interaction modalities and their combinations, with a focus on various application contexts, performance measures and operation types. These publications were classified and analyzed across several dimensions, as outlined in Section \ref{section3}. 
In this section, we perform a statistical analysis of the reviewed literature from various perspectives. This high-level analysis explores the quantity and proportion of attention given to different categories of interaction research in recent years. These statistical results also support the taxonomy proposed in Section \ref{section3}.

%根据文章中所研究的交互中人作为交互的发出者还是反馈的接收者，这些文献分为Active交互（86）与Passive交互（19）。在Active交互的文献中，出现的交互模态有\textit{Gaze}、\textit{Gesture}、\textit{Speech]、\textit{Tactile}，同时也包含了这些模态的多种组合模态（\textit{Gaze+Speech}, \textit{Gaze+Gesture}, \textit{Gesture+speech}, \textit{Gaze+Gesture+Speech}, \textit{X+Y})等。值得一提的是，个别文献中研究了几种单或多模态的交互方案，我们没有考虑作者作为baseline的方法，并按照作者提出的新交互方案进行分类。近3年研究中针对不同模态的比例见图\cite{}.研究Gesture和Gaze的文章总数是最多的，包括它们两种模态的组合模态.研究Speech-Only交互模态的文献数量在2024年有显著提升，Speech相关的多模态研究总体数量也有所提升。这可能得益于LLM近年来的发展。Tactil interaction相关文献数量少。从后面对Passive交互模态的分析中可以得出，触觉常被作为一种反馈模态而被研究，并非主动交互手段。

Based on whether humans act as initiators or receivers of interaction, the literature is categorized into \textbf{Active} interaction (84 papers) and \textbf{Passive} interaction (20 papers). In the Active interaction literature, the modalities include \textit{Gesture} (24), \textit{Gaze} (13), \textit{Speech} (7), \textit{Tactile} (8), and various combinations of these modalities, such as \textit{Gaze+Speech} (12), \textit{Gaze+Gesture} (4), \textit{Gesture+Speech} (4), \textit{Gaze+Gesture+Speech} (2), and other combinations (\textit{X+Y}) (10). Notably, in some publications that examined multiple single or multimodal interaction schemes, the baseline methods were excluded from classification, focusing instead on the newly proposed interaction schemes. The proportion of different modalities in research from 2022 to 2024 is shown in Fig. \ref{fig:bymodality}.
Studies on Gesture and Gaze, including their combined modalities, are the most prevalent. A notable increase in research on Speech-only interaction is observed in 2024, accompanied by a rise in Speech-related multimodal studies, likely driven by recent advancements in LLMs. Tactile interaction-related literature is comparatively less common. In the analysis of Passive interaction modalities, tactile interaction is frequently studied as a feedback mechanism rather than as an active interaction method.

% Based on whether humans act as interaction initiators or feedback receivers, the literature is categorized into Active interaction (85 papers) and Passive interaction (19 papers). In Active interaction literature, the interaction modalities include \textit{Gesture} (24), \textit{Gaze} (14), \textit{Speech} (9), \textit{Tactile} (8), and various combinations of these modalities, \textit{i.e.}, \textit{Gaze+Speech} (12), \textit{Gaze+Gesture} (5), \textit{Gesture+speech} (5), \textit{Gaze+Gesture+Speech} (2), \textit{X+Y} (8). It is noteworthy that in some publications where multiple single or multi-modal interaction schemes were studied, the authors' baseline methods were not considered, and classification was based on the newly proposed interaction schemes. The proportions of different modalities in research from 2022 to 2024 are shown in Fig. \ref{fig:bymodality}. Studies on Gesture and Gaze, including their combined modalities, are the most numerous. A significant increase in Speech-Only interaction modality research is observed in 2024, with an overall increase in Speech-related multimodal studies. This may be attributed to recent developments in LLMs. Tactile interaction-related literature is less prevalent. Analysis of Passive interaction modalities reveals that touch is often studied as a feedback modality rather than an active interaction means.

\begin{figure}
    \begin{center}
    % \begin{overpic} 
    % [width=\linewidth]
    % {example-image-a}
    % \end{overpic}
    \includegraphics[width=1\linewidth]{ByType2.jpg}
    \end{center}
    %\vspace{-4mm}
    \caption{
The bar charts display the yearly statistics for the five main operation types. This figure is consistent with the literature recorded in Table \ref{tab:table1}.
    }
    \label{fig:bytype}
    \vspace{-6mm}
\end{figure}


%所收集文献中不同操作的文献数量如下：\textit{Pointing and Selection}, \textit{Creation and Editing}, \textit{Translation and Transform}, \textit{Locomotion and Viewport}, \textit{Typing and Querying}. 近3年研究的比例可参考图\cite{}.Pointing and Selection仍然是被学者们关注最多的方向。但是可以研究数量在逐年减少。这可能是由于更精确的多模态选择交互技术的最新进展。Locomotion and Viewport,以及Typing and Querying在2024年的研究数量都有比较显著的提升。这可能是由于用户的主观感受得到越来越多的学者的关注，以及LLM的发展使得更为多元化的语义信息可以被用户传递给XR系统中。在Section 3中，将进一步地分析这些数据背后的技术发展。
The number of publications for various operation types in the collected literature is as follows: \textit{Pointing and Selection} (45), \textit{Creation and Editing} (16), \textit{Translation and Transform} (13), \textit{Locomotion and Viewport} (9), \textit{Typing and Querying} (19). The proportions of research from 2022 to 2024 are illustrated in Fig. \ref{fig:bytype}. Pointing and Selection remains the most focused area, although the number of studies has been decreasing annually. This decline may be due to advancements in more precise multimodal selection techniques.
Significant increases in research on Locomotion and Viewport and Typing and Querying were observed in 2024. This rise may be attributed to growing attention on users' subjective experiences and the development of LLMs. These models enable users to communicate more diverse semantic information to XR systems. Further analysis of the technological developments behind these trends will be provided in Section \ref{section3}.



%Passive交互类型的数据如下：Visual、Acoustic和Haptic，Hybrid。具体情况可参考表\cite。Haptic是今年来受关注的研究方向，这可能是由于恰到好处的Haptic反馈能显著提升用户的沉浸式体验。Visual和Acoustic的结合模态相对较多，而单独的Acoustic反馈则被研究的很少（搜集到的文章中没有出现）。少量研究单独讨论了Visual反馈。同时，三大感官融合的反馈模态也是备受关注的研究方向。
Passive interaction types are categorized as Visual, Acoustic, Haptic, and Hybrid. Specific details are available in Table \ref{passive}. 
\revise{}{Passive interaction is separated into a new table (Table \ref{passive}) rather than include it in Table \ref{tab:table1} for the following reasons.
Firstly, there are inherent differences between the modalities of active and passive interactions, and they are not directly equivalent. For example, while active interaction involving human's eyes is typically referred to as \textit{gaze} (or \textit{eye gaze}) interaction, passive interaction involving the eyes is generally categorized as \textit{visual feedback}. 
Besides, Table \ref{tab:table1} is primarily designed to illustrate the relationship between modalities and operation types of the reviewed papers. Passive interactions, however, do not engage with the concept of `operation' in the same way. Thus, it is not appropriate to include passive interactions in Table \ref{tab:table1}, which focuses on active interactions.
}

\revise{}{Recent studies have increasingly focused on haptic feedback}, likely due to its ability to significantly enhance users' immersive experiences. \revise{Combined Visual and Acoustic feedback is relatively common,}{It is relatively commmon to combine visual and acoustic feedback,} while standalone acoustic feedback is rarely studied. Some research also explores visual feedback alone. Moreover, the integration of the three primary sensory feedback modalities (visual, acoustic and haptic) has emerged as a major research direction.
%某些文章涉及到了一些比较具体的应用场景，专注于解决某个应用背景中存在的交互问题。我们对这些应用场景进行了单独讨论。

%

%文献涉及眼动、手势、语音、可触摸交互多种交互模态，其中包含了相当一部分比例的文章（）采用了多模态交互方式。

\revise{}{There are some devices that are preferred by researchers to conduct their experiment. The situation is illustrated in Table \ref{devices}. Microsoft Hololens 2 and HTV Vive Pro / Vive Pro Eye are the most popular devices used for experiment in the recently 2 years, while Hololens 2 is more preferred in 2024. Meta Quest 2 (Oculus Quest 2) is less used after 2022 but still popular.}




\section{A Taxonomy and Analysis of Natural Interaction Techniques}
\label{section3}

% 我们inspried by 之前的review \ccite, 保留了operation types and interaction modalities的分类。我们也新增了application scenarios以探索自然交互技巧的应用。同时我们也讨论了performance measures in operation types中。接下来我们在Section \ref{Appl.}中讨论自然交互的应用。在Section \ref{operationtypes}中讨论六种operation types。在Section \ref{Interaction techniques}讨论不同交互模态的实现，以及研究者如何设计不同交互模态用于交互。

%XR中的交互任务复杂多变，我们统计的文献中覆盖了各种应用场景下的交互任务。由于用于实现某种交互任务的交互模态非常丰富，详细地分析总结研究文献中各种交互任务与交互模态的特征是非常必要的。这能够为研究者们针对任务类型选择适当的交互模态或者是AI模型时提供理论上的启发。许多综述文献为根据特征对其交互任务进行了分类。Ghamandi\cite{DBLP:conf/ismar/GhamandiHNGKTL23}等考虑到了XR环境中的collaboration的需要，从人与环境的交互方式、人与人的合作方式等方面入手，提供了一个非常全面的分类法。Zhao\cite{DBLP:journals/vi/ZhaoJCLYXC22}等针对元宇宙背景对交互任务进行了分类，该分类重点关注了navigation任务。However,使用头戴式设备的XR任务种类繁多，不同应用场景中交互的特点也十分丰富，过于复杂的分类方法缺乏泛化性，同时也会使研究者们忽略某类任务与其他任务的关联。为了提供一个更加泛化并且可拓展的分类，我们分析了调研的全部文献中的操作任务和他们的评价指标，研究了用户对它们在自然交互中的需求。最终抽离了具体的应用背景，合并了具有相同特点的交互任务，并将这些任务拆分为特点鲜明的基础操作，以方便今后的研究者们可以以此为依据来洞悉用户需求并寻找合适的技术方案（AI模型等）。图\ref{图1}中展示了分类的结果。Section\ref{3.2}和Section{3.3}中介绍了该taxonomy。

%更加先进的头显设备与AI技术为交互技术提供了更多的设计空间，特定的交互任务可以使用多种模态组合以及AI技术进行赋能。为了更好地分析各种交互技术的研究进展，我们首先创建了一个taxonomy。分类的方法参考了综述文献/cite{A survey，a Taxonomy，A review of inter}，which通过Display Type, Input Medium, Use Case, Tasks等维度对交互任务进行了详尽的分类。我们以这些文献的taxonomy为基础，在调研的过程中不断地迭代分类标准并根据所统计的数据形成了最终的分类。we retain the classification of operation types and interaction modalities, though focusing on a different subset. Additionally, the category of application scenarios were introduced to explore the exploitation of these interaction techniques. 我们按照Application Scenarios—Tasks-Modalities的顺序进行分析，通过应用的需求来研究各种模态发展趋势中的必然性。In Section \ref{Appl.}, we discuss the application of natural interactions, followed by an analysis of seven operation types in Section \ref{operationtypes}. Finally, in Section \ref{Interaction techniques}, the implementation of various interaction modalities and how researchers design these modalities for interaction were discussed.

% \revise{}{A taxonomy is ``a system of groupings that are derived conceptually or empirically''\cite{DBLP:journals/ejis/NickersonVM13} where each dimension consists of mutual exclusive characteristics. Hertel et al.\cite{DBLP:conf/ismar/HertelKSBSS21} extended this definition by introducing subcategories. Ghamandi et al.\cite{DBLP:conf/ismar/GhamandiHNGKTL23} and Zhang et al.\cite{DBLP:journals/vi/ZhangWZST23} established their taxonomies in a similar way. We extended the definition\cite{DBLP:conf/ismar/HertelKSBSS21} by allowing mutual characteristics between subcategories introduced by combination of interaction modalities.}

% Based on the reviewed literature and \revise{}{the extended definition of taxonomy}, we classify and analyze natural interaction techniques. Inspired by previous reviews \cite{DBLP:journals/tvcg/SpittlePC023, DBLP:conf/ismar/HertelKSBSS21}, we retain the classification of operation types and interaction modalities, though focusing on a different subset. Additionally, the category of application scenarios were introduced to explore the use of natural interaction techniques. Performance measures in relation to operation types were also examined. In Section \ref{Appl.}, we discuss the application of natural interactions, followed by an analysis of seven operation types in Section \ref{operationtypes}. Finally, in Section \ref{Interaction techniques}, the implementation of various interaction modalities and how researchers design these modalities for interaction were discussed.



\revise{}{Previous XR survey studies have primarily categorized research based on \textit{display type}, \textit{study type}, \textit{use case}, \textit{input technique}, and \textit{task type} \cite{DBLP:journals/vi/ZhangWZST23,DBLP:conf/ismar/HertelKSBSS21,DBLP:journals/tvcg/SpittlePC023}. Since this paper focuses on head-mounted displays, there is only one display type.  
The \textit{study type} refers to the type of user evaluation conducted (\textit{e.g.}, assessment or comparison), while the \textit{use case} examines the conditions under which studies are conducted (\textit{e.g.}, static or in motion). Given the focus of RQ1 and RQ2, this paper primarily explores novel interaction paradigms and techniques, as well as evolving trends in multimodal natural interaction. Therefore, we do not discuss \textit{study type} or \textit{use case} in this work but retain \textit{input technique}. 
Regarding \textit{task type}, existing research often considers \textit{application scenario} and \textit{operation type} as part of task classification and discusses them together \cite{DBLP:conf/ismar/HertelKSBSS21,DBLP:journals/tvcg/SpittlePC023}. In this paper, we separate these two aspects.}  

\revise{}{As a result, we retain three classification dimensions. Our categorization follows the order of \textit{application scenario} → \textit{operation type} → \textit{interaction technique}. This structure facilitates the analysis of inherent developmental trends in interaction modalities through the lens of application requirements. It also progresses from a broad, high-level perspective to a more detailed and specific one.}




% \revise{}{To systematically analyze the research progress in interaction technologies, a comprehensive taxonomy has been developed. The classification methodology was inspired by previous survey literature \cite{DBLP:journals/vi/ZhangWZST23,DBLP:conf/ismar/HertelKSBSS21,DBLP:journals/tvcg/SpittlePC023}, which provided detailed categorizations along multiple dimensions including Display Type, Input Medium, Use Case, and Tasks. Building upon these existing taxonomies, our classification framework was iteratively refined throughout the literature review process.}

% \revise{}{While our taxonomy maintains the fundamental classification of operation types and interaction modalities, it focuses on a specifically defined subset of these dimensions. Furthermore, the category of application scenarios has been introduced to elucidate how these interaction techniques are required in practice. The analysis is conducted by the order of Application Scenarios - Operation Types - Modalities, enabling an investigation of the inherent trends in development of interaction modalities through the lens of application requirements. This structure moves from a broader, high-level perspective to a more detailed, specific one.}

\revise{}{The subsequent sections are organized as follows: Section \ref{Appl.} presents a detailed discussion of natural interaction applications, Section \ref{operationtypes} provides an analysis of seven distinct operation types, and Section \ref{Interaction techniques} examines the implementation methodologies of various interaction modalities and the design considerations employed by researchers in developing these interaction systems.}


\begin{table}[t]
    \centering
    \renewcommand\arraystretch{1.2}
    \caption{\revise{}{The literature of different application scenarios}}
    \begin{tabular}{m{2.5cm}<{\centering}|m{4cm}<{\centering}}
    \hline
        \cellcolor{oBlue!80} Application Scenarios & \cellcolor{oPink!80} Literature \\ \hline
        \cellcolor{oBlue!30} drawing and sketching & \cite{DBLP:conf/ismar/ChenGFCL23, DBLP:conf/chi/TurkmenGBSASPM24, DBLP:journals/tvcg/XuZSFY23, 10.1145/3613904.3642758, DBLP:journals/imwut/ChenLYZ22, DBLP:journals/tvcg/SongDK23}
        \\ \hline
        \cellcolor{oBlue!30} smart assistants & \cite{DBLP:conf/vr/GiunchiNGS24, DBLP:conf/chi/WangYWJ024, DBLP:conf/vr/YangQCSBLL24, DBLP:conf/chi/0005WBCRF24, 10.1145/3613904.3642068, DBLP:conf/chi/WuQQCRS24, DBLP:journals/imwut/WangSWYYWJXY24}
        \\ \hline
        \cellcolor{oBlue!30} virtual meetings & \cite{DBLP:conf/vr/SaintAubertAMPAL23, 10462901, DBLP:conf/uist/LiaoKJKS22, DBLP:conf/chi/CaoKWAX24, 10049667, DBLP:conf/vr/WangZF24}
        \\ \hline
        \cellcolor{oBlue!30} VR/AR navigation  & \cite{DBLP:conf/vr/QuereMJWW24, DBLP:conf/chi/WangYWJ024, DBLP:conf/vr/YangQCSBLL24}
        \\ \hline
        \cellcolor{oBlue!30} reading  & \cite{DBLP:conf/ismar/LeeHM22, DBLP:conf/ismar/MengXL22, DBLP:conf/ismar/XuMYSL22}
        \\ \hline
        \cellcolor{oBlue!30} furniture assembly/maintenance  & \cite{DBLP:conf/vr/YangQCSBLL24,DBLP:conf/vr/QuereMJWW24}
          \\ \hline
        \cellcolor{oBlue!30} remote collaboration  & \cite{DBLP:conf/vr/JingLB22, DBLP:journals/corr/abs-2405-18537}
         \\ \hline
        \cellcolor{oBlue!30} autonomous driving  & \cite{DBLP:conf/chi/ElsharkawyAYAHK24}
        \\ \hline
        \cellcolor{oBlue!30} enter passwords  & \cite{DBLP:conf/ismar/SongDK22, DBLP:conf/vr/RuppGBK24}
        \\ \hline
    \end{tabular}
    \label{scene}
\end{table}

\begin{table}[t]
    \centering
    \renewcommand\arraystretch{1.2}
    \caption{\revise{}{Devices chosen by researchers (by years)}}
    \begin{tabular}{m{2.5cm}<{\centering}|m{1.5cm}<{\centering}|m{1.5cm}<{\centering}|m{1.5cm}<{\centering}}
    \hline
         Device Name &  2022 & 2023 & 2024
        \\ \hline
         Microsoft Hololens 2 & 7 & 6 & 13
        \\ \hline
        HTC Vive Pro/Vive Pro Eye & 9 & 8 & 7
        \\ \hline
        Meta Quest 2 (Oculus Quest 2) & 8 & 5 & 6
        \\ \hline
        
   
    \end{tabular}
    \label{devices}
\end{table}

\subsection{Application Scenarios}  
\label{Appl.}
% In the reviewed literature, 将近80\%的researches都是为通用场景做交互设计，即没有明确应用场景。例如设计gaze vergence controlled techniques \ccite, 设计10种以上的手势用于交互 \ccite，and 设计鲁棒的语音关键词检测方法 \ccite。我们讨论涉及具体应用场景的研究。

% 有5篇papers探索了\textbf{VR/AR sketching} 的应用\ccite。这些研究中，用户主要通过手势（裸手，Pen，可触摸设备、或controller）进行绘画，使用眼动交互/语音进行菜单选择以切换画笔颜色，或者利用眼动控制3D网格使用户深度感知更加立体。sketching应该是操作较为复杂的任务。因此，涉及到的操作主要包括Pointing, selection, creation, editing。其中1篇论文涉及操作6个几何平面，因此还包括scaling, translation and rotation等操作。
In the reviewed literature, nearly 70\% of the research focuses on interaction design for general scenarios, without specifying particular application contexts. For example, some studies design gaze vergence control techniques \cite{DBLP:conf/chi/SidenmarkCNLPG23, DBLP:journals/tvcg/WangZ022, DBLP:conf/chi/ZhangCSS24}, create more than 10 different hand gestures for interaction \cite{DBLP:conf/chi/PeiCLZ22, DBLP:journals/tvcg/SongDK23}, and develop robust voice keyword detection methods \cite{DBLP:journals/tvcg/CaiML24, DBLP:conf/chi/ZhangLHWLGZ23}.
However, we argue that applying multimodal natural interaction to specific application scenarios is crucial. 
Implementing these techniques in real-world contexts not only enhances user experience but also increase user exposure to multimodal interaction technologies.
In turn, this can accelerate the adoption of these technologies and promote their use across a wider range of fields.
Therefore, in this section, we discuss studies that explore specific application scenarios. \revise{}{Several exemplary application scenarios are illustrated in Fig. \ref{fig:application}. Meanwhile, we also provide an overview of the interaction types and operation types utilized.}




% In the reviewed literature, nearly 80\% of the research focused on interaction design for general scenarios, without specifying particular application contexts. For example, some works design gaze vergence control techniques \cite{DBLP:conf/chi/SidenmarkCNLPG23, DBLP:journals/tvcg/WangZ022, DBLP:conf/chi/ZhangCSS24}, create more than 10 different hand gestures for interaction \cite{DBLP:conf/chi/PeiCLZ22, DBLP:journals/tvcg/SongDK23}, and develop robust voice keyword detection methods \cite{DBLP:journals/tvcg/CaiML24, DBLP:conf/chi/ZhangLHWLGZ23}. 
% However, 我们认为把多模态自然交互应用到具体应用场景中，非常重要。因为应用到实际场景中不仅能提升用户体验，还能让更多用户接触和适应多模态交互技术，提高用户的接受度和信任度。这种普及性应用可以加速技术的推广，使其在更多领域得到应用。
% 因此In this section, we discuss studies that 探讨 specific application scenarios.

\begin{figure}
    \begin{center}
    % \begin{overpic} 
    % [width=\linewidth]
    % {example-image-a}
    % \end{overpic}
    \includegraphics[width=1\linewidth]{application2.pdf}
    \end{center}
    %\vspace{-4mm}
    \caption{
\revise{}{The illustrations of XR application scenarios: (a) drawing and sketching; (b) smart assistant \cite{DBLP:conf/chi/0005WBCRF24}; (c) virtual meeting; (d) AR navigation. Image courtesy of \cite{DBLP:conf/chi/0005WBCRF24}.}
    }
    \label{fig:application}
    %\vspace{-2mm}
\end{figure}

We found 6 papers applied natural interaction techniques to \textbf{drawing and sketching} \cite{DBLP:conf/ismar/ChenGFCL23, DBLP:conf/chi/TurkmenGBSASPM24, DBLP:journals/tvcg/XuZSFY23, 10.1145/3613904.3642758, DBLP:journals/imwut/ChenLYZ22, DBLP:journals/tvcg/SongDK23}. In these studies, users primarily engage in sketching through gestures (bare hands, pen, touch devices, or controllers), while using eye-tracking, voice commands or gestures for menu selection, such as switching brush colors. Eye-tracking is also employed to control 3D grids, allowing users to perceive depth more intuitively \cite{DBLP:conf/chi/TurkmenGBSASPM24}. Sketching is considered a relatively complex task, involving operations like pointing, selection, creation, and editing. 2 papers allow users to manipulate different geometric objects, including actions such as scaling, translation, and rotation \cite{DBLP:journals/tvcg/SongDK23, DBLP:journals/tvcg/XuZSFY23}.

% virtual assistant
7 papers explored the application of \textbf{smart assistants} \cite{DBLP:conf/vr/GiunchiNGS24, DBLP:conf/chi/WangYWJ024, DBLP:conf/vr/YangQCSBLL24, DBLP:conf/chi/0005WBCRF24, 10.1145/3613904.3642068, DBLP:conf/chi/WuQQCRS24, DBLP:journals/imwut/WangSWYYWJXY24}. Among these, 6 papers utilize LLMs as assistants, benefiting from their superior comprehension and reasoning abilities. For example, Giunchi \textit{et al.} allowed users to directly edit virtual objects through voice commands, such as creating or moving them \cite{DBLP:conf/vr/GiunchiNGS24}. Wang \textit{et al.} provided intelligent guidance for virtual tours by recognizing user speech and environmental information, then delivering multimodal feedback, such as avatars, voice, text windows, and minimaps \cite{DBLP:conf/chi/WangYWJ024}. 
Lee \textit{et al.} offered assistance with daily activities by using LLMs to answer questions related to objects of interest such as food calories or offer book recommendations, as identified by the user’s gaze and speech \cite{DBLP:conf/chi/0005WBCRF24}.

% 会议 +电话视频会议
6 papers explored the application of natural interaction techniques in \textbf{video conferencing} or \textbf{virtual meetings} \cite{DBLP:conf/vr/SaintAubertAMPAL23, 10462901, DBLP:conf/uist/LiaoKJKS22, DBLP:conf/chi/CaoKWAX24, 10049667, DBLP:conf/vr/WangZF24}.
For example, Saint-Aubert \textit{et al.} focused on verbal communication with avatars in VR, introducing synchronized haptic feedback based on speech content to enhance immersion \cite{DBLP:conf/vr/SaintAubertAMPAL23}. Lee \textit{et al.} proposed using visual cues (lighting effects) and auditory cues (spatial audio) in VR social interactions to guide users' attention to new speakers \cite{10462901}.
Liao \textit{et al.} and Cao \textit{et al.} explored techniques for enhancing speaker presentations in video conferences \cite{DBLP:conf/uist/LiaoKJKS22, DBLP:conf/chi/CaoKWAX24}. They extracted keywords from users' speech and matched them with predefined images and animations, which were then synchronized with hand movements to improve expression.


3 papers investigated the application of natural interaction techniques in \textbf{VR/AR navigation} \cite{DBLP:conf/vr/QuereMJWW24, DBLP:conf/chi/WangYWJ024, DBLP:conf/vr/YangQCSBLL24}. 
For example, Quere \textit{et al.} used a hand-menu system to create virtual annotations for a large campus hosting a reception, which helped direct participants to meeting rooms \cite{DBLP:conf/vr/QuereMJWW24}. Wang \textit{et al.} used LLMs to generate guidance for virtual tours through avatars, voice, and text windows \cite{DBLP:conf/chi/WangYWJ024}.
It is important to note that our definition of navigation refers to guiding the user to quickly locate their position, which differs from the definition of virtual navigation in related studies that focus on controlling travel speed and direction of users \cite{10.1145/3613904.3642147, DBLP:conf/vr/SinJLLLN24, DBLP:conf/vr/SindhupathirajaUDH24}.

3 papers explored the application of natural interaction in \textbf{reading} \cite{DBLP:conf/ismar/LeeHM22, DBLP:conf/ismar/MengXL22, DBLP:conf/ismar/XuMYSL22}. For example, Lee \textit{et al.} designed a set of gaze-based interaction strategies to select text, zoom in on specific areas, and scroll through content, thereby enhancing the reading experience \cite{DBLP:conf/ismar/LeeHM22}. Meng \textit{et al.} investigated head-based pointing combined with three hands-free selection mechanisms, \textit{i.e.}, dwell, eye blinks, and voice (hum), to select text during reading \cite{DBLP:conf/ismar/MengXL22}.


1 paper discussed the use of turn-by-turn animations and text instructions displayed on AR glasses for \textbf{furniture assembly} \cite{DBLP:conf/vr/YangQCSBLL24}. 1 paper explored the application of AR in \textbf{maintenance}, such as annotating a broken video projector to guide other users in understanding how to operate it \cite{DBLP:conf/vr/QuereMJWW24}. 2 paper focused on \textbf{remote collaboration}, proposing specific methods to quickly guide collaborators to follow an expert's line of sight \cite{DBLP:conf/vr/JingLB22, DBLP:journals/corr/abs-2405-18537}. 
1 paper explores the application of VR-based interactive feedback in \textbf{autonomous driving} \cite{DBLP:conf/chi/ElsharkawyAYAHK24}.
Additionally, 2 papers examined how gestures can be used to quickly \textbf{enter passwords} in VR \cite{DBLP:conf/ismar/SongDK22, DBLP:conf/vr/RuppGBK24}.

To summarize, current research on XR natural interaction techniques has explored 10 types of applications, offering a relatively diverse range of studies. However, these papers account for only 30\% of the reviewed literature, indicating that further exploration is needed on how to apply natural interaction techniques to practical use cases. Additionally, the range of application types should be expanded to include fields such as medicine, industrial training, and education, all of which hold significant potential for impactful applications. \revise{}{Fig. \ref{scene} show the papers with specific scenarios.} Further discussion can be found in Section \ref{4.3}.

% To summarize, 目前XR natural interaction techniques方面的research探索了10种类型的应用，较为丰富。然而总论文数量才占the reviewed literature的30\%。因此现有研究对于如何应用自然交互到practial应用，仍有待进一步探索。另外，应用类型也需要扩展，例如医学、工业训练、教育领域，这些领域都有很强的应用价值。更多的讨论见Section \ref{4.3}. 

% 1 paper 讨论了在\textbf{furniture assembly}中使用turn-by-turn的animation/text instruction显示在AR眼镜上 \cite{DBLP:conf/vr/YangQCSBLL24}. 1 paper讨论了在\textbf{maintenance}应用，例如为a broken video projector标注annotations，指导other users去理解如何使用the projector \cite{DBLP:conf/vr/QuereMJWW24}。1 paper讨论了remote协作时，设计特定方式快速引导协作者寻找专家的视线方向\cite{DBLP:conf/vr/JingLB22}. 2 papers讨论了在VR中如何通过手势切换，快速输入密码 \cite{DBLP:conf/ismar/SongDK22, DBLP:conf/vr/RuppGBK24}.


% 2篇论文研究自然交互在阅读中的应用\ccite。For example, Lee \textit{et al.} 设计眼动交互来选取阅读的文本、放大局部区域、滚动文本，提升阅读体验\cite{DBLP:conf/ismar/LeeHM22}. Meng \textit{et al.} 探索了head-based pointing结合three hand-free selection mechanisms, \textit{i.e.}, dwell, eye Blinks and voice (hum) 进行阅读的文本选取 \cite{DBLP:conf/ismar/MengXL22}.


% 2 papers 研究自然交互技巧在 VR/AR navigation中的应用\ccite。Note that我们认为的导航，是为用户指引，方便其快速寻找方位，不同于相关研究定义的virtual navigation that control the travel speed and direction \ccite.


% 4 papers探索了 natural interaction techniques在 视频会议 或 虚拟会议中的应用\ccite。
% Saint-Aubert \textit{et al.} 针对verbal communications with avatars in VR, 设置根据语音内容同步进行触觉的震动反馈，从而增强沉浸感 \cite{DBLP:conf/vr/SaintAubertAMPAL23}.  Lee 
%  \textit{et al.} 针对VR中的social interaction，提出利用visual cues (lighting effects) and auditory cues (spatial audio)，引导用户注意到新发言者\cite{10462901}.
% Liao \textit{et al.} and Cao \textit{et al.} 针对视频会议中speaker的presentation，提取用户语音中的关键词，并与预定义的图片、动画匹配，将这些内容跟随手部运动，增强表达能力
%  \cite{DBLP:conf/uist/LiaoKJKS22, DBLP:conf/chi/CaoKWAX24}.


% We found 5 papers explore the application of \textbf{smart assistants} \ccite. 这些研究中，4篇论文使用了large language models (LLMs) 作为assistants, 得益于其卓越的理解与推理能力。例如，Giunchi \textit{et al.} 允许用户通过语音直接编辑虚拟物体，例如create or moving \ccite。They 利用LLM，实现语音→文字→代码→实时编译的路线。Wang \textit{et al.} 针对virtual tour提供intellgent guidance，通过识别用户speech和环境信息，输出multimodal feedback，\textit{e.g.}, avatar, voice, text window, minimap, etc \ccite. Lee \textit{et al.} 针对日常生活中的活动提供辅助，例如针对用户视线和语音所关注的物体，通过LLM回答相关问题，例如食品的calories, book recommendation \ccite.


% 不说细节


% 娱乐

% 维修

% 协作

% 密码输入

%要讲操作类型


% \textbf{General Scene.} 

% \textit{Reading.}

% \textit{Entertainment.}

% \textit{Maintenance.}

% \textit{Navigation.}

% \textit{Collaboration.}

% \textit{Drawing.}

% \textit{Teleconference.}



\subsection{Operation Types}
\label{operationtypes}
The applications mentioned above are built on specific natural interaction operations. Researchers have developed various operations and explored diverse interaction modalities, resulting in a complex mapping between operation forms and interaction modalities \cite{DBLP:conf/ismar/HertelKSBSS21}. This complexity underscores the importance of identifying similarities among these operations. In this section, we introduce a rational classification that reveals the relationships between different operations. This taxonomy helps to identify the core issues each type of operation must address and suggests future development trends. The paper categorizes XR operations into seven main classes, as explained below.

Before interacting with an object, the initial step is to select it. Even when creating new objects, users must first preselect a location. The ability to quickly, accurately, and stably select desired objects is essential in nearly all scenarios \cite{marquardt2024selection}. This process is categorized in this paper as \textbf{Pointing and Selection}.
XR enhances human capabilities by providing greater control over virtual environments, particularly with respect to object manipulation \cite{DBLP:conf/chi/WangLZ24}. The ability to create objects and modify their properties is a central feature that human-centered XR should offer. This paper classifies these actions as \textbf{Creation and Editing}.
Once objects are selected, users can perform various interactions, depending on the scenario, including controlling spatial position, size, and orientation. These actions are referred to in this paper as translation, scaling, and rotation, with the latter two collectively termed \textit{Transform}. Therefore, these operations are grouped under a single category: \textbf{Translation and Transform}.

The aforementioned categories relate to user interactions with objects. However, users also need to navigate and observe within the virtual environment, which involves movement through space ({Locomotion}) and changes in viewpoint ({Viewport}). This paper combines these activities under the category of \textbf{Locomotion and Viewport}.

With the continued advancement of XR and AI technologies, users are presented with increasingly rich information within virtual environments. This development necessitates more frequent exchanges of information between users and systems \cite{DBLP:journals/imwut/WangSWYYWJXY24}. Users must input task-related information into the system, while the system must accurately interpret user intentions and provide appropriate feedback. Based on recent research, we classify information interaction into two categories: \textbf{Typing and Querying}. The former refers to the primary method of text input, while the latter includes various forms of information retrieval for users.

% The above categories pertain to user operations on objects. Users themselves need to move and observe within the virtual world, which involves position movement ({Locomotion}) and perspective change ({Viewport}). This paper groups these into the category of \textbf{Locomotion and Viewport}.
% With the advancement of XR and artificial intelligence technologies, users can access richer information in virtual worlds, necessitating more frequent information exchange between users and systems \cite{DBLP:journals/imwut/WangSWYYWJXY24}. Users need to input task-related information into the system, while the system must accurately understand user semantics and provide acceptable feedback. In this paper, based on the volume of recent related research, we categorize information interaction into two classes: \textbf{Typing and Querying}. The former is the primary method for text input, while the latter encompasses various other forms of information obtain.

% This classification provides a structured framework for understanding and analyzing XR interactions, reflecting the fundamental ways users engage with and manipulate virtual content. It addresses both the creation and modification of virtual objects as well as the user's movement and perception within the XR space, offering a comprehensive approach to studying XR operations.
%上述的交互类型涉及用户和环境的主动交互，即用户向环境输入信息的过程。我们将上述的操作类型归为\textit{Active Interaction}. 在收集的文献中，还存在一类文章，他们往往专注于解决某种信息输入模态的瓶颈问题，而非专注于一个具体的操作。我们将这类文章归为\textit{No Operation}类别。

%除了信息的输入以外，用户从XR系统中接收反馈的过程也是自然交互的一个重要方面。我们将研究向用户提供反馈的交互方法的文章归类为\textit{Passive Interaction}。


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{operation2.jpg}
    \caption{\revise{}{The illustrations of  operation types: (a) pointing and selection; (b) typing; (c) locomotion; (d) transform \cite{DBLP:journals/tvcg/SongDK23}. Image courtesy of \cite{DBLP:journals/tvcg/SongDK23}.}}
    \label{fig:Operation}
\end{figure}

The aforementioned interaction types involve active en-
gagement of users with the environment, where users input
information into the system. We categorize these opera-
tions as Active Interactions. Fig. gives an illuatration of these operations.
Beyond user input, receiving feedback from the XR system is another critical component of natural interaction, as it enhances user immersion. Research that investigates methods for delivering feedback to users is categorized as \textbf{Passive Interactions}.

In our literature review, we also identify a distinct category of research that does not focus on specific operations but rather on improving the recognition accuracy of input modalities. We classify this body of work under the \textbf{No Operation} category.
 
This classification offers a structured framework for understanding and analyzing XR interactions, capturing the core ways in which users engage with and manipulate virtual content. It encompasses both the creation and modification of virtual objects as well as user movement and perception within the XR environment, providing a comprehensive approach to the study of XR operations.


\subsubsection{Pointing and Selection}
Pointing-and-Selection is the most common and fundamental operation in XR, essential for interacting with any object. Selection operations are often influenced by scene factors such as object size, density, and depth distribution \cite{marquardt2024selection}. \revise{In recent researches, 2D menu layouts are prevalent in selection operation.}{Among recent research, 2D menu layouts are prevalent in selection operation.} \revise{These researches focuse on designing task-specific menu layouts for different application contexts to achieve better selection}{These studies focus on designing task-specific menu layouts for different application contexts to obtain better performance of selection} \cite{DBLP:conf/ismar/ChenGFCL23, DBLP:conf/ismar/LeeHM22, DBLP:conf/vr/OrloskyLSSM24, kim2022lattice, DBLP:conf/vr/0001LC00S22, DBLP:conf/vr/QuereMJWW24, 10.1145/3544548.3581423, DBLP:conf/vr/LaiSL24}. \revise{Researches focusing on selection in 3D space primarily addresses high-occlusion scenarios}{Research focusing on selection in the 3D space primarily aims to address the problem of accuracy in high-occlusion scenarios}  \cite{DBLP:conf/chi/SidenmarkCNLPG23, DBLP:conf/chi/WeiSYW0YL23, DBLP:conf/uist/0001QTFLS22}. Some studies also discuss user experience in text selection \cite{DBLP:conf/ismar/LeeHM22, DBLP:conf/ismar/MengXL22} and selection across different depth planes \cite{DBLP:conf/chi/ZhangCSS24, DBLP:conf/uist/0001QTFLS22, DBLP:journals/tvcg/WangZ022}.


\textbf{Performance measures.} These papers share a common set of evaluation metrics. Objective metrics predominate, including \textit{Target Delay}, which describes the time for the pointing modality to align with the target object; \textit{Selection Time}, measuring the total duration from object appearance to selection; \textit{False Positive Rate}, indicating the frequency of false touch occurrences; \textit{Selection Accuracy}, describing the overall accuracy of a selection method; \textit{Error and Abort Rate}, assessing the difficulty of using a particular method; and \textit{Hand or Controller Movement}, quantifying the required hand motion, closely related to arm fatigue. Common subjective metrics primarily evaluate \textit{Cognitive Load} and \textit{Subjective Preferences}.


\textbf{Modalities.} Eye tracking is the most commonly used modality for pointing and selection because the human eye naturally focuses on objects of interest, as demonstrated in Table \ref{tab:table1}. However, gaze estimation methods still have limitations in precision, as sampled gaze points represent a spatial distribution rather than a fixed point due to  tremors or
involuntary saccades \cite{DBLP:conf/chi/SidenmarkCNLPG23}. Pure eye-tracking pointing and selection often incorporate special interface designs or vergence control to address these issues \cite{DBLP:conf/chi/SidenmarkCNLPG23,DBLP:conf/ismar/LeeHM22,DBLP:conf/chi/WeiSYW0YL23,DBLP:conf/vr/OrloskyLSSM24,DBLP:conf/chi/ZhangCSS24,DBLP:conf/chi/ChoiSO22,kim2022lattice,DBLP:conf/vr/0001LC00S22,DBLP:journals/tvcg/WangZ022}. Additionally, introducing secondary modalities for fine-tuning or confirming selections is a common approach \cite{DBLP:journals/tvcg/SidenmarkP0CGWG22,10.1145/3544548.3581423,DBLP:conf/chi/HouNSKBG23,10.1145/3591129,10.1145/3530886}. A few studies explore the reliability of gesture-based selection, typically applied in scenarios where gestures align more closely with human intuition \cite{DBLP:conf/vr/QuereMJWW24,DBLP:conf/chi/SchmitzGS022,DBLP:journals/tvcg/SongDK23}.


\subsubsection{Creation and Editing}
As XR technology advances, allowing users to freely create and modify objects in XR significantly expands their interaction freedom with the environment \cite{DBLP:conf/chi/TorreFHBFL24}. This set of operations primarily involves Creation and Editing operations. The current mainstream Creation mode uses predefined patterns, where users or developers predefine a set of virtual components that can be created by users through specific interaction modalities (\textit{e.g.}, \cite{DBLP:conf/uist/LiaoKJKS22,DBLP:conf/chi/CaoKWAX24,DBLP:conf/vr/QuereMJWW24}), mostly triggered by voice \cite{DBLP:conf/uist/LiaoKJKS22} or gestures \cite{DBLP:conf/chi/CaoKWAX24}. In this mode, designing easily memorable and manageable Creation methods for users is a primary concern\cite{DBLP:conf/chi/PeiCLZ22,DBLP:conf/ismar/ChenGFCL23}. Additionally, some studies explore methods for users to freely create non-predefined objects \cite{DBLP:journals/tvcg/XuZSFY23,DBLP:conf/vr/GiunchiNGS24,DBLP:journals/tvcg/SongDK23,DBLP:conf/chi/PeiCLZ22}. They focus on diversifying creatable content to provide users with more freedom. Editing operations act on these created objects. As virtual elements can take various forms such as text, images, windows, maps, etc., there are multiple editable attributes like size, texture, and color\cite{DBLP:conf/chi/WangYWJ024}.


\textbf{Performance measures.}
The primary concern in Editing operations is whether users can easily remember and use the relevant interaction methods, which are highly application-specific. Common metrics include both objective and subjective measures. Objective metrics include \textit{Completion Time} and \textit{Success Rate}. Subjective metrics mainly cover \textit{Engagement}, \textit{Confidence}, \textit{Expressiveness}, \textit{Learnability}, and \textit{Easiness of Use}.


\textbf{Modalities.}
Hand gestures are the most common modality for Creation. Six degree-of-freedom (DoF) gestures have attracted significant research attention due to their extensive design space and the diversity of creatable objects. Numerous studies have designed intuitive creation gestures for various objects \cite{DBLP:conf/chi/PeiCLZ22,DBLP:conf/ismar/ChenGFCL23,DBLP:journals/tvcg/XuZSFY23,DBLP:conf/chi/0003HLG23}. Given the rich semantics of human language, coupled with advanced NLP technologies, speech interaction can provide more precise information about the objects to be created and their attributes \cite{DBLP:conf/chi/CaoKWAX24,DBLP:conf/uist/LiaoKJKS22}. A few studies have incorporated eye tracking to select creation positions or specify menu items \cite{DBLP:conf/ismar/ChenGFCL23,10.1145/3613904.3642758,DBLP:conf/ismar/MengXL22}.




\subsubsection{Translation and Transform}
This operation corresponds to users' fundamental control capabilities on objects, which include transform, rotation, and scaling. Recent studies have defined richer interactions for precise object manipulation, extending beyond intuitive hand gestures. As XR technology advances, virtual objects have varied, necessitating simultaneous multiple basic operations on single objects. 
Research have focused on enhancing transformation richness and flexibility through customized gestures \cite{DBLP:conf/chi/CaoKWAX24,DBLP:journals/tvcg/XuZSFY23} and intuitive designs for various objects \cite{DBLP:conf/chi/PeiCLZ22,DBLP:journals/tvcg/SongDK23}. Studies also aim to extend user reach for larger-scale transformations in limited spaces \cite{DBLP:journals/ijhci/DengSZK24,DBLP:conf/chi/0003HLG23}, and improve interaction feedback, including haptic responses.


\textbf{Performance measures.}
Evaluation in these works combined objective and subjective metrics. Objective metrics include \textit{Accuracy}, \textit{Speed}, \textit{Stability} (for measuring consistency during jittery movements and across multiple operations), and \textit{Precision} (for positional accuracy). Subjective metrics focus on \textit{Learnability} and \textit{Fatigue}.


\textbf{Modalities.}
This operation typically employ intuitive hand gestures, such as pinch and grab for moving objects \cite{DBLP:conf/vr/QuereMJWW24}, and two-handed pull for scaling \cite{DBLP:conf/chi/CaoKWAX24}. Studies have explored varied implementation details, including customizable gestures \cite{DBLP:conf/chi/CaoKWAX24} and gestures inspired by everyday tool use \cite{DBLP:conf/chi/PeiCLZ22}. Some research examines learning abilities and operational stability for specific gesture-based transformations \cite{DBLP:conf/chi/CaoKWAX24}.
However, gesture detection inevitably involves errors \cite{DBLP:conf/chi/CaoKWAX24}. Some studies incorporate voice input for improved stability \cite{DBLP:conf/chi/CaoKWAX24,DBLP:conf/uist/LiaoKJKS22}. Gaze interaction, being quicker and less fatiguing, has been utilized for object transformation, with gestures often used for fine-tuning and 3DoF rotation control \cite{DBLP:conf/chi/HouNSKBG23}. A few studies have explored hands-free approaches using eye tracking and head-gaze for transformations \cite{DBLP:conf/chi/HouNSKBG23}.

\subsubsection{Locomotion and Viewport}
This operation includes both \textit{Locomotion} and \textit{Viewport}, which are essential for user exploration and movement in virtual environments, significantly influencing user comfort and immersion \cite{DBLP:conf/vr/SinJLLLN24}. Traditional locomotion research has primarily focused on 2D planar movement, with \textit{teleportation} and \textit{steering} being the two most common modes. The former rarely causes discomfort, while the latter offers greater immersion \cite{DBLP:conf/vr/HombeckVHDL23}. Recent studies have extended this focus to 3D space exploration \cite{DBLP:conf/vr/SinJLLLN24,DBLP:conf/vr/SindhupathirajaUDH24}, aiming to improve the overall user experience. In collaborative VR scenarios, enhanced locomotion techniques help users better understand each other's movement intentions, preventing disconnection or separation \cite{DBLP:conf/chi/RaschRS023}. However, traditional controller-based rotation and directed steering can detract from immersion, as they fail to provide natural and continuous turning experiences \cite{DBLP:conf/vr/HombeckVHDL23}.
Improvements to the \textit{Viewport} are equally important for enhancing user comfort and overall experience. One study introduced three gaze-controlled viewport methods that enable hands-free and controller-free interaction \cite{DBLP:conf/chi/LeeWSG24}.





\textbf{Performance measures.} Measures for locomotion primarily utilize subjective metrics, including \textit{Presence}, \textit{Workload}, \textit{Cybersickness}, \textit{Preference}, and \textit{Overall User Experience}, while the main objective metric is \textit{Task Completion Time}. For viewport evaluation, researchers employ similar subjective measures as locomotion, with the addition of \textit{Error Rate} as an objective metric to quantify the ease of view manipulation.


\textbf{Modalities.} In steering mode, except from body-leaning, recent studies have focused on gesture-based movement \cite{DBLP:conf/vr/SinJLLLN24} and gaze-directed locomotion with speed and turning control using gestures\cite{10.1145/3613904.3642147}. Voice commands have also proven to be a viable option for steering control \cite{DBLP:conf/vr/HombeckVHDL23}. In teleportation mode, research has explored voice-based destination matching \cite{DBLP:conf/vr/HombeckVHDL23} and gesture-based alternatives to controllers \cite{DBLP:conf/vr/SindhupathirajaUDH24}. Some studies focus on teleportation in collaborative VR scenarios, addressing the challenge of communicating teleportation intentions between users\cite{DBLP:conf/chi/RaschRS023}.



\subsubsection{Typing and Querying}
This kind of operation includes typing and querying, crucial for inputting information into and retrieving feedback from XR systems. Typing efficiency significantly impacts productivity \cite{DBLP:conf/chi/HeLP22}. Recent XR interaction advancements offer diverse modalities for information retrieval\cite{DBLP:journals/imwut/WangSWYYWJXY24}. These modalities of information and their combinations have been leveraged in recent works\cite{DBLP:journals/imwut/WangSWYYWJXY24, DBLP:conf/chi/0005WBCRF24}.  User immersion in XR querying is enhanced when input methods closely resemble everyday communication, requiring systems to discern user intent \cite{DBLP:conf/chi/0005WBCRF24}.


\textbf{Performance measures.} Evaluation for typing primarily use objective metrics such as \textit{Words per Minute (WPM)}, \textit{Error Rate}, \textit{Deletion Count}, and \textit{Prediction Count}. Subjective metrics include cognitive, visual, and hand fatigue levels. Evaluation of Querying employs both subjective metrics (\textit{Simplicity}, \textit{Naturalness}, \textit{Human-likeness}, \textit{Personal Preferences}) and objective metrics (\textit{Task Time}, \textit{Times of Attempts}).


\textbf{Modalities.} Recent studies often retain traditional keyboard layouts while incorporating new modalities to improve speed and experience \cite{DBLP:conf/iui/ZhaoPTZWJBG23,DBLP:conf/chi/HeLP22,DBLP:journals/tvcg/ShenDK23,DBLP:conf/ismar/SongDK22,DBLP:conf/chi/Hedeshy0MS21,10474330}. Gaze-tracking is used to predict characters of interest, accelerating cursor movement or providing visual cues\cite{DBLP:conf/iui/ZhaoPTZWJBG23,10474330}. He \textit{et al.} proposed a gaze-selecting method combined with language models for error-tolerant blind typing \cite{DBLP:conf/chi/HeLP22}. Some studies achieve faster input speeds compared to midair-tapping using pure gaze input\cite{cui2023glancewriter,DBLP:conf/vr/HuDK24}. Gestures are used for keyboard control \cite{DBLP:conf/ismar/SongDK22}, and new 3D decoding techniques enable precise interpretation of aerial gestures \cite{DBLP:journals/tvcg/ShenDK23}. For querying, speech-based interaction is fundamental, with lip-reading and other auxiliary methods improving robustness \cite{DBLP:conf/chi/ZhangLHWLGZ23,DBLP:journals/tvcg/CaiML24}. Eye-tracking data and gesture information can also help resolve pronoun ambiguities in speech \cite{DBLP:conf/chi/0005WBCRF24,DBLP:journals/imwut/WangSWYYWJXY24}. One study enables gaze-only querying through an AR interface design \cite{DBLP:conf/uist/LiaoKJKS22}.


\subsubsection{No Operation Type}
%在收集到的文章中，我们发现有一些并未涉及具体的操作类型。这些文章往往提供针对某种输入模态的交互方法进行优化，使其能更好地理解并传递用户的主动交互意图。这些文章共有9篇。

%这些文章主要以手势交互和可触摸交互为主，还有1篇涉及语音交互。Xu等设计了一种融合了视觉和听觉传感器的fine-grained的手势检测方法\cite{AO-Fin}，帮助用户可以使用更小幅度的动作来触发交互。Kitamura等设计了一种接触式的可穿戴设备，能够精准识别微手势并能够识别兼续的手势变化和进行压力输入\cite{TouchLog}.Lee等提出了一种手腕设备，利用主动声学技术连续捕捉手部运动以及手与物体的交互信息\cite{EchoWrist}。Liu等设计了一种给予声音信号的手势检测法，能够识别人手在不同材质表面上完成的各种手势，通过利用不同的平面，拓展了手势交互的可能。 Li等设计了基于微型摄像头的指环型微手势识别装置，同时可以在各种平面上完成手势交互\cite{nailring}。Rupp等提出了一组手势认证方案，可以达到与PIN code 相同的熵值\cite{Authentication}.Shen等提出了一种Key Gesture Spotting架构，用来协助开发者快速进行手势识别系统的开发，同时该系统降低了手势检测的延迟，以达到更好地用户体验\cite{Gesture Spotter}.Wang等为裸手交互设计了一套视觉引导机制，来帮助用户更标准的完成交互动作，以减少识别错误。Cai等设计了一种语音-回声双模关键字检测系统，可以实现在更广泛的环境中准确识别关键词\cite{Robust Dual}。
In our literature review, we identified nine studies that did not explicitly focus on specific interaction types. Instead, these studies provided optimizations aimed at improving the recognition accuracy of input modalities. These studies primarily aimed to enhance the system's ability to accurately interpret and respond to users' active interaction intents. Since these papers are not discussed elsewhere in this work, we provide a detailed description here.

The majority of these studies centered around gesture and touch-based interactions. Xu \textit{et al.} proposed a fine-grained hand gesture detection method that leverages both visual and auditory sensors, enabling users to trigger interactions with smaller movements \cite{DBLP:conf/chi/XuZKN23}. Kitamura \textit{et al.} developed a contact-based wearable device capable of accurately recognizing micro-gestures, including continuous gesture changes and pressure inputs \cite{DBLP:conf/iswc/KitamuraYS23}. Lee \textit{et al.} introduced a wrist-worn device that employs active acoustics to continuously capture hand movements and interactions with objects \cite{DBLP:conf/chi/LeeZAYGLKYDLSGZ24}. Liu \textit{et al.} designed a gesture recognition method using audio signals, enabling the recognition of various hand gestures on different material surfaces, thus expanding the possibilities of gesture interaction\cite{10522613}. Li \textit{et al.} developed a finger-ring-based micro-gesture recognition device equipped with a miniature camera, allowing for gesture interaction on various surfaces \cite{DBLP:conf/ismar/LiLMHLS22}. Rupp \textit{et al.} proposed a set of gesture authentication schemes that achieve the same entropy as PIN codes \cite{DBLP:conf/vr/RuppGBK24}. Shen \textit{et al.} introduced a Key Gesture Spotting architecture to assist developers in rapidly developing gesture recognition systems, while simultaneously reducing gesture detection latency for a better user experience.

1 other study leverage visual cues for preventing user
 \cite{DBLP:journals/tvcg/ShenDMK22}. Wang \textit{et al.} designed a visual guidance mechanism for bare-hand interaction to help users perform interaction actions more consistently, reducing recognition errors \cite{DBLP:conf/chi/WangLZ24}. Cai \textit{et al.} developed a dual-mode keyword detection system using both speech and echo signals, enabling accurate keyword recognition in a wider range of environments 
 \cite{DBLP:journals/tvcg/CaiML24}.


\subsubsection{Passive Interaction}
In addition to the previously mentioned {Active Operations} and {No Operation}, we also examine {Passive Interactions}. In recent years, many studies have focused on this topic. Based on the types of feedback provided by XR environments, we categorize these studies into four groups: \textit{Visual}, \textit{Acoustic}, \textit{Haptic}, and \textit{Hybrid}. For more detailed information on this classification, please refer to Table \ref{img:passive}. Fig \ref{img:passive} illustrates passive interactions.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\linewidth]{passive2.jpg}
    \caption{\revise{}{The illustrations of passive interaction: (a) visual feedback \cite{DBLP:conf/chi/Pohlmann0MMB23}; (b) haptic feedback. Image courtesy of \cite{DBLP:conf/chi/Pohlmann0MMB23}}.}
    \label{img:passive}
\end{figure}

\textbf{Performance measures}.
%Passive Interaction主要关注用户能否更好地感知来自环境的反馈，这使得主观指标成为了主要的衡量标准。不同的研究中往往采用作者们自己设计的问卷进行数据采集。其中，被广泛关注的指标主要有\textit{Realism}, \textit{Immersion}, \textit{Confidence}。Haptic相关的研究中还采用了\textit{Visuo-Haptic Match}\(cite{Mouth}).还有研究采用用户在XR环境中完成任务的得分作为评价指标\cite{Dynamic_Scene,You spin}
Passive Interaction primarily focuses on users' perception of the environment, making subjective metrics the primary evaluation criteria. Data in various studies is typically collected through questionnaires designed by the authors. The most commonly assessed indicators include \textit{Realism}, \textit{Immersion}, and \textit{Confidence}. In haptic-related research, \textit{Visuo-Haptic Match} is also a key metric \cite{DBLP:conf/chi/ShenS022}. Additionally, some studies use task completion scores in XR environments as an evaluation method \cite{DBLP:conf/vr/LiLYTFX24,DBLP:conf/chi/Pohlmann0MMB23}.

\textbf{Modalities}.
%Acoustic模态在收集的文章中没有单独出现，它往往和其他模态被一起研究。Visual反馈为主要的反馈模态之一。有研究关注单一视觉反馈对用户在teleportation过程中产生对自身相对位置的感知的影响\cite{Behind The Scene}.3篇文章采用了Visual引导机制，来促进多用户在XR环境中彼此的沟通效率\cite{MAY i speak,A3RT,Go going}.2篇文章以用户在XR环境中所进行的任务阶段为输入状态，动态的改变环境参数来提升用户完成任务的效率\cite{Artist, Dynamic}.3篇文章将现实世界中的distraction通过交互式visual反馈引入VR中，以此来提升用户的沉浸感或者减少用户的sickness\cite{SYNC,Integreting,You spin me}.
The acoustic modality is not observed in isolation in the collected articles. It is typically studied in conjunction with other modalities. Visual feedback represents one of the primary feedback modalities. The impact of singular visual feedback on users' self-position perception during teleportation has been investigated \cite{DBLP:conf/chi/MedlarLG24}. Visual guidance mechanisms are employed in 3 articles to enhance communication efficiency among multiple users in XR environments \cite{10462901,DBLP:conf/vr/WangZF24,DBLP:conf/chi/RaschRS023}. 2 articles dynamically alter environmental parameters based on users' task stages in the XR environment to improve task completion efficiency \cite{DBLP:conf/chi/WuQQCRS24,DBLP:conf/vr/LiLYTFX24}. 
3 papers design feedback in VR environments based on real-world distractions, aiming to reduce the impact of distractions on user immersion and comfort \cite{DBLP:conf/chi/ElsharkawyAYAHK24,DBLP:conf/uist/Tao022,DBLP:conf/chi/Pohlmann0MMB23}.

%Haptic模态为近年来研究者们最为关注的Passive交互模态。5篇文章利用辅助可穿戴硬件来提供额外的触觉反馈，以帮助用户感受XR中的更加多样的事物，比如风\cite{Mouth,Providing,Double,Airres,Fluid}.2篇文章研究了Visuo-Haptic illution with Proxies,提供给用户对物体尺寸、重量和运动轨迹的感知\cite{Designing Visuo, Big or Small}.
The haptic modality has garnered significant attention from researchers in recent years as a passive interaction modality. 5 articles utilize auxiliary wearable hardware to provide additional tactile feedback, enabling users to experience a wider range of sensations in XR\cite{DBLP:conf/chi/ShenS022,DBLP:conf/vr/YamazakiH23,DBLP:conf/uist/JinguWS23,DBLP:conf/chi/TatzgernDWCEDGH22,DBLP:conf/uist/ShenRM0S23}, such as wind\cite{DBLP:conf/chi/ShenS022}. Visuo-Haptic illusion with Proxies is examined in 2 articles, offering users perception of objects' size, weight, and motion trajectories \cite{DBLP:conf/chi/FeickR0K22,DBLP:conf/chi/0001OPSB24}.

\subsection{Interaction Techniques}
\label{Interaction techniques}

The aforementioned operations are supported by specific interaction modalities. This section focuses on the hardware and algorithmic implementations of these modalities, as well as the design of concrete interaction techniques. Section \ref{GestureOnly} discusses the hardware and software implementations of gesture-only interactions, provides a summary of existing research, and analyzes the strengths and limitations of this modality. Section \ref{GazeOnly} covers the same aspects for gaze-only interactions, while Section \ref{SpeechOnly} addresses speech-only interactions. Section \ref{Tactile} explores tactile interaction in a similar manner. Section \ref{MultiInteraction} focuses on multimodal interaction techniques, specifically covering \textit{Gaze + Gesture}, \textit{Gaze + Speech}, \textit{Gesture + Speech}, and a triple-modal technique (\textit{Gaze + Gesture + Speech}). Lastly, we discuss various \textit{X + Y} interactions, which compare different multimodal interaction techniques in these studies.
% 上述操作由具体的交互模态做支撑。本节主要讲述交互模态的硬件与算法实现，以及具体交互技巧的设计。Section \ref{GestureOnly}讨论gesture-only的硬软件实现、现有研究总结、以及该模态的优势与劣势。Section \ref{GazeOnly}讲述gaze-only的上述内容。Section \ref{SpeechOnly}讲述speech-only的上述内容。Section \ref{Tactile}讲述tactile interaction的上述内容。Section \ref{MultiInteraction}聚焦在multimodal interaction techniques，分别阐述 \textit{Gaze + Gesture}, \textit{Gaze + Speech}, \textit{Gesture + Speech} and a triple-modal technique (\textit{Gaze + Gesture + Speech})。最后讲一些\textit{X + Y}的交互，是主要研究多个多模态的研究。

%\textit{Gesture Only.} %实现方式。目前研究趋势。XX，XXX。优点和缺点。
%实现方式。目前研究总结。发展趋势，包括优点和缺点，未来方向。 我们首先介绍hand gesture的实现方式。

% % %实现方式
% \textbf{Hardware.} 目前hand tracking所使用的sensor基本上都是VR/AR HMDs集成的相机，以红外相机为主，最新设备如meta quest 3，使用2 RGB cameras。只有1 paper使用低功耗sensing modules来检测手反射的声波代替图像，ie.,speakers and microphones，来进行hand tracking \ccite。textbf{Algorithm.} VR/AR HMDs集成的hand tracking通常使用computer vision方法，输出hand skeleton data。4 papers需要区分更多手势种类，如10种手势或更多\ccite。这些研究使用nueral network或machine learning的方法对手势进行分类。其他研究只需简单手势，如pinch或clicking，只需要判断距离 of index finger和大拇指。

% %目前研究总结
% In the reviewed literature, hand gesture only是the most frequently研究的modality for spatial computing,i.e.,32 papers (see table).
% 手经常被用于我们日常与物体的interaction，是一种natural and intuitive模态。例如11 papers将手势用于物体的选择和移动。并且通过手6DoF的运动，可以用于绘制复杂3D场景\ccite，VRlocomotion \ccite。手势可以用来执行精细化的操作，控制小的物体。例如,3 papers研究VR中的文本输入，通过直接的虚拟键盘触摸\ccite 或手势转换为数字输入\ccite.
% 手势的变换，能表示复杂的语义信息。例如，2 papers使用10种或更多手势，来imitate a wide range of objects，e.g·, telescope,scissors, Camera,etc \ccite.也有3 papers通过利用手和手臂在运动时的感知盲区来实现hand redirection，从而让用户在有限的物理空间内进行操作，但在虚拟世界中却实现更大范围的交互\ccite。

% %目前研究趋势%优点3条(来自上面研究总结)，缺点3条(参考A Review of Interaction Technigues for Imersive Environments的Table 4)，缺点可以来自所调研文献的discussion°
% 最后加上未来趋势的简单分析。综上所述，hand gesture only交互的优势是较为自然和直观，可以表示复杂语义信息，并且进行精细化操作。然而，hand gesture only交互也面临一些缺点。1)复杂手势交互，学习成本较高。例如，3papers提到xxX。2)手势识别的精度有待提高。例如，2 papers提到遮挡xx问题。3)手势交互会导致arm fatigue。2 papers提到xx问题。4)lacks tangible support.5)Effecterm fatigued by social acceptance。hand gesture only的问题是，要求用户的操作与研究者定义的复杂交互范式去匹配，增加了用户的学习负担。未来，该交互的发展，应该朝着易用性去发展，使其user-friendly for novices以及Minimal tracking accuracy requirement。

\begin{table}[t]
\centering
\renewcommand\arraystretch{1.2}
\caption{\revise{}{Comparison of hand tracking performance across devices}}
\begin{threeparttable} % Wrap the table in threeparttable to enable \tnote{}
\begin{tabular}{
     >{\centering\arraybackslash}m{2.8cm} 
     >{\centering\arraybackslash}m{2cm} 
     >{\centering\arraybackslash}m{1.2cm} 
     >{\centering\arraybackslash}m{1cm} 
}
\hline
\textbf{Device Name}  & \textbf{Accuracy} & \textbf{Latency} & \textbf{Sampling Rate}  \\ \hline
Microsoft HoloLens 2  & around 15 mm & -\tnote{1} & -  \\ \hline
Meta Quest 2           & around 11 mm & 45 ms & 60 Hz  \\ \hline
HTC Vive Pro              & around 37 mm & - & -  \\ \hline
Lee \textit{et al.}, 2024 \cite{DBLP:conf/chi/LeeZAYGLKYDLSGZ24} & 4.81 mm & 500 ms & - \\ \hline
\end{tabular}

\begin{tablenotes}
\item[1] \revise{}{- indicates that no reports are found regarding this item.}
\end{tablenotes}

\end{threeparttable}
\label{tab:hand_tracking_comparison}
\end{table}

\subsubsection{Gesture Only}
\label{GestureOnly}



We begin by introducing the implementation methods for hand gestures.
\textbf{Hardware}. Currently, hand tracking mainly relies on sensors integrated into VR/AR HMDs, primarily utilizing infrared (IR) cameras. The latest devices, such as the Meta Quest 3, additionally use two RGB cameras for enhanced tracking \cite{Meta}. 
\revise{}{The hand-tracking performance of XR devices used in recent research is summarized in Table \ref{tab:hand_tracking_comparison}. Among these, the HTC Vive Pro shows lower accuracy compared to the HoloLens 2 and Meta Quest 2 \cite{10.1145/3485279.3485283}. Although the latency and sampling rate of the HoloLens 2 and HTC Vive Pro have not been documented, users have not reported experiencing any noticeable delays during use. Typically, users rely on the HTC Vive Pro's hand controller for gesture-based interactions.}
\revise{2 paper explores low-power sensing modules, such as speakers and microphones, to detect hand-reflected sound waves as an alternative to image-based tracking \cite{DBLP:conf/chi/LeeZAYGLKYDLSGZ24, 10522613}.}{2 papers \cite{DBLP:conf/chi/LeeZAYGLKYDLSGZ24, 10522613} explore low-power sensing modules, such as speakers and microphones, to detect hand-reflected sound waves as an alternative to image-based tracking.}
\textbf{Algorithm}. Hand tracking in VR/AR HMDs typically employs computer vision techniques to output hand skeleton data \cite{cai2020generalizing, cai2018desktop}. 
\revise{4 papers aimed to classify a larger variety of gestures (\textit{e.g}., 10 or more) using neural networks or machine learning \cite{DBLP:journals/tvcg/SongDK23, DBLP:conf/chi/PeiCLZ22, DBLP:journals/tvcg/ShenDMK22,DBLP:conf/chi/LeeZAYGLKYDLSGZ24}.}{4 papers \cite{DBLP:journals/tvcg/SongDK23, DBLP:conf/chi/PeiCLZ22, DBLP:journals/tvcg/ShenDMK22,DBLP:conf/chi/LeeZAYGLKYDLSGZ24} aim to classify a larger variety of gestures (\textit{e.g}., 10 or more) using neural networks or machine learning.} 
\revise{Other studies focused on simpler gestures, like pinching or clicking, which require only distance detection between the index finger and thumb.}{Other studies focus on simpler gestures like pinching or clicking, which only require distance detection between the index finger and thumb.}

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{handgesture.pdf}
    \caption{\revise{}{ The illustrations of hand gesture only interaction. Images courtesy of \cite{DBLP:conf/chi/PeiCLZ22, DBLP:conf/vr/SinJLLLN24}.}}
    \label{Hand}
\end{figure}

\textbf{Summary of current research}. In the reviewed literature, gesture only interaction is the most frequently studied modality for spatial computing, \revise{with 24 papers focused on this topic (see Fig. \ref{fig:bymodality})}{with 24 papers focusing on this topic, shown in Fig. \ref{fig:bymodality}}. 
\revise{Hands are a natural and intuitive interface for interacting with objects in daily life.}{The hands are natural and intuitive interfaces for interacting with objects in daily life.} For example, 20 papers explored using hand gestures for object selection or translation. Hand movements in 6DoF have been used to create complex 3D scenes \cite{DBLP:journals/tvcg/XuZSFY23, 10.1145/3491102.3517682} and for VR locomotion \cite{DBLP:conf/vr/SinJLLLN24, DBLP:conf/vr/SindhupathirajaUDH24}. Gestures also enable fine-grained control of small objects. 3 papers investigated text input in VR, either via direct virtual keyboard interactions \cite{DBLP:conf/vr/RuppGBK24, DBLP:journals/tvcg/ShenDK23} or gesture-to-text conversions \cite{DBLP:conf/ismar/SongDK22}.
Gesture transformations can convey complex semantic information. For instance, 2 papers used 10 or more gestures to imitate a wide range of objects, such as a telescope, scissors, or camera, \textit{etc} \cite{DBLP:conf/chi/PeiCLZ22, DBLP:journals/tvcg/SongDK23}. Additionally, 3 papers explored hand redirection techniques, leveraging the perceptual phenomenon of change blindness in hand and arm movements \cite{DBLP:conf/vr/MatthewsTIS22, DBLP:conf/chi/ZennerKFAK24, DBLP:conf/ismar/BanMNK22}. This allows users to operate within a limited physical space while enabling broader interactions in a virtual environment.

\textbf{Advantages and disadvantages of hand gesture only interaction}. Hand-gesture only interaction offers several advantages. It is natural and intuitive, as gestures align with everyday interactions. Gestures also allow for complex semantic representation and providing fine-grained control for precise manipulation of small objects. However, there are notable drawbacks. 
For the use of complex gestures, the cognitive load increases as users need considerable time to master them \cite{DBLP:conf/chi/PeiCLZ22}. 
Gesture recognition accuracy still requires improvement, particularly in cases of occlusion, as mentioned in 2 papers \cite{DBLP:conf/chi/PeiCLZ22, DBLP:journals/tvcg/SongDK23}. Additionally, prolonged use of gestures can lead to arm fatigue \cite{DBLP:conf/vr/SindhupathirajaUDH24, DBLP:conf/vr/QuereMJWW24}, and the lack of tangible support means users do not receive physical feedback \cite{DBLP:conf/chi/SchmitzGS022, DBLP:journals/tvcg/ShenDK23}. 
Social acceptance can also pose a challenge, as obvious hand gesture interaction may not be appropriate in public \cite{DBLP:conf/ismar/LiLMHLS22}. 
To summarize, hand gesture interactions often require users to adapt to complex paradigms defined by researchers, which increases the cognitive load. Future developments should focus on improving ease of use, making hand gesture interaction more user-friendly for novices, and minimizing tracking accuracy requirements.

\subsubsection{Gaze Only}
\label{GazeOnly}

Gaze-based interaction is a key focus in extended reality, leveraging humans' intuitive eye movements. Eye gaze rapidly reaches objects of interest, making it ideal for pointing and selection tasks \cite{cheng2024appearance}. Gaze-only methods eliminate the need for body movements, benefiting users in various situations, including those with disabilities or in socially awkward scenarios \cite{DBLP:conf/chi/LeeWSG24}.

\begin{table}[t]
\centering
\renewcommand\arraystretch{1.2}
\caption{\revise{}{Comparison of eye-tracking performance across XR devices}}
\begin{threeparttable} % Wrap the table in threeparttable to enable \tnote{}
\begin{tabular}{
     >{\centering\arraybackslash}m{1.7cm} 
     >{\centering\arraybackslash}m{1.2cm} 
     >{\centering\arraybackslash}m{1.3cm} 
     >{\centering\arraybackslash}m{1cm} 
     >{\centering\arraybackslash}m{1.4cm} 
}
\hline
\textbf{Device Name}  & \textbf{Accuracy} & \textbf{Latency} & \textbf{Sampling Rate} & \textbf{Slippage-Robust} \\ \hline
Microsoft HoloLens 2  & $1.5^\circ$ & - & 30 Hz & \checkmark \\ \hline
HTC Vive Pro Eye           & $0.5^\circ$-$1.1^\circ$ & 50 ms & 120 Hz & \checkmark \\ \hline
Meta Quest Pro              & $1.6^\circ$ & 58 ms & 90 Hz & \checkmark \\ \hline
Microsoft HoloLens 1\tnote{1}  & $1^\circ$ & 8.5 ms & 120 Hz & $\times$ \\ \hline
\end{tabular}

\begin{tablenotes}
\item[1] \revise{}{Microsoft HoloLens 1 is integrated with Pupil Labs' eye tracker.}
\end{tablenotes}

\end{threeparttable}
\label{tab:eye_tracking_comparison}
\end{table}





\textbf{Hardware and algorithm.} There are two main eye-tracking strategies in XR environments \cite{DBLP:conf/ismar/WangZLL21}. The first is the Pupil Center Corneal Reflection (PCCR) method \cite{DBLP:journals/tbe/GuestrinE06, DBLP:journals/corr/abs-2003-08806}. This approach uses 1-2 near-infrared cameras positioned close to the eye, along with multiple near-infrared light sources (typically 6-12) directed at the eye. Based on the corneal reflection patterns, the system reconstructs an accurate eye model. The advantage of this method lies in its high precision and its ability to support slippage detection and compensation for device slippage \cite{DBLP:conf/etra/SantiniNK19}. However, it has drawbacks, including higher hardware costs and a more complex process for calibrating the optical positions of the light sources and cameras \cite{6949395}.
\revise{}{
Currently, commercial XR devices such as Microsoft HoloLens 2, HTC Vive Pro Eye, and Meta Quest Pro utilize the PCCR-based eye-tracking approach, as shown in Table \ref{tab:eye_tracking_comparison}. These devices typically achieve eye-tracking accuracy within the range of 0.5°–1.6°. Furthermore, because they are robust against device slippage, the differences in eye-tracking performance during interaction can be negligible. The latency falls within an acceptable range for interaction purposes, and a sampling rate of 30 Hz is generally sufficient for gaze-based interaction. Overall, the eye-tracking performance of these XR devices is comparable.}

The second method is glint-free, which relies on temporal pupil information rather than corneal reflections \cite{DBLP:conf/etra/DierkesKB19, DBLP:conf/chi/KytoEPLB18}. This technique typically uses a single camera and a single light source primarily for illumination. By analyzing pupil data over multiple frames, it reconstructs the eye model. The advantage of this method is its simplicity and lower hardware requirements, although the accuracy of the eye model reconstruction is generally lower compared to PCCR \cite{DBLP:journals/tvcg/WangZ022}.
\revise{}{Early implementations, such as the Microsoft HoloLens 1 integrated with Pupil Labs' eye-tracking system \cite{DBLP:conf/chi/KytoEPLB18}, used this glint-free approach. While its accuracy was comparable to the PCCR method under ideal conditions, it was highly sensitive to device slippage, resulting in a rapid deterioration of gaze accuracy over time.}



% XR环境下主要的eye-tracking策略有两种。一种是
% Pupil Center Corneal Reflection方法 (PCCR)。PCCR方法使用1~2个近红外近眼相机，同时设置多个近红外光源，如6~12个，照明到人眼上，基于角膜反射的光斑信息重建眼球模型。该方法优点是在于高精度的眼球模型和良好的支持眼镜滑动检测与补偿效果。缺点是硬件成本较高，同时光源与相机的光学位置标定过程也相对复杂。
% 第二种方式是，基于瞳孔时序信息的免光斑方法，i.e., glint-free methods。这种方法通常采用单个相机和单个光源。光源主要用于照明作用。这种方法依赖连续多帧检测到的瞳孔信息，来重建眼球模型、优点是硬件简单，但是眼球模型重建精度有待提高。

\textbf{Summary of current research.} Gaze-only interactions with virtual user interfaces (UIs) often encounter \textbf{the Midas touch problem} due to lack of confirmation modalities. 
To address this, researchers have explored peripheral vision areas and auxiliary UIs. 
\revise{Choi \textit{et al.} proposed the Kuiper Belt concept \cite{DBLP:conf/chi/ChoiSO22}, while Yi \textit{et al.} studied optimal virtual menu layouts \cite{DBLP:conf/vr/0001LC00S22}.}{Choi \textit{et al.} \cite{DBLP:conf/chi/ChoiSO22} proposed the Kuiper Belt concept, while Yi \textit{et al.} \cite{DBLP:conf/vr/0001LC00S22} studied optimal virtual menu layouts.} Orlosky \textit{et al.} \cite{DBLP:conf/vr/OrloskyLSSM24} and Kim \textit{et al.} \cite{kim2022lattice} designed auxiliary interfaces to enhance pure gaze selection operations.
Besides, several techniques have been developed for \textbf{heavily-occluded scenarios}. 
\revise{Sidenmark \textit{et al.} matched object depth motion with gaze vergence changes \cite{DBLP:conf/chi/SidenmarkCNLPG23}, while Wei \textit{et al.} used probabilistic models based on head and gaze endpoints \cite{DBLP:conf/chi/WeiSYW0YL23}. Yi \textit{et al.} combined planar and depth information analysis \cite{DBLP:conf/uist/0001QTFLS22}.}{Sidenmark \textit{et al.} \cite{DBLP:conf/chi/SidenmarkCNLPG23} matched object depth motion with gaze vergence changes, while Wei \textit{et al.} \cite{DBLP:conf/chi/WeiSYW0YL23} used probabilistic models based on head and gaze endpoints. Yi \textit{et al.} \cite{DBLP:conf/uist/0001QTFLS22} combined planar and depth information analysis.}
Vergence estimation technology has introduced \textbf{vergence control} as a novel gaze-only interaction mode. 
\revise{Zhang \textit{et al.} designed visual depth control methods for object selection \cite{DBLP:conf/chi/ZhangCSS24}, while Wang \textit{et al.} developed depth control schemes for see-through vision.}{Zhang \textit{et al.} \cite{DBLP:conf/chi/ZhangCSS24} designed visual depth control methods for object selection, while Wang \textit{et al.} \cite{DBLP:journals/tvcg/WangZ022} developed depth control schemes for see-through vision.}
In \textbf{typing} applications, pure eye movement input methods have shown promising results. 
\revise{Cui \textit{et al.} designed a word prediction algorithm based on eye movement trajectories \cite{cui2023glancewriter}.}{Cui \textit{et al.} \cite{cui2023glancewriter} designed a word prediction algorithm based on eye movement trajectories.} Similar systems integrated with a LLM were also proposed by Hu \textit{et al.} \cite{DBLP:conf/vr/HuDK24}.

Gaze-based interactions have also been applied to \textbf{enhance user experience} in various contexts.
\revise{Chen \textit{et al.} explored activating hidden objects in virtual films \cite{DBLP:conf/ismar/ChenHTHH23}, Turkmen \textit{et al.} investigated gaze-activated auxiliary grids in virtual sketching \cite{DBLP:conf/chi/TurkmenGBSASPM24}, and Lee \textit{et al.} introduced gaze-activated magnification in VR reading \cite{DBLP:conf/ismar/LeeHM22}.}{Chen \textit{et al.} \cite{DBLP:conf/ismar/ChenHTHH23} explored activating hidden objects in virtual films, Turkmen \textit{et al.} \cite{DBLP:conf/chi/TurkmenGBSASPM24} investigated gaze-activated auxiliary grids in virtual sketching, and Lee \textit{et al.} \cite{DBLP:conf/ismar/LeeHM22} introduced gaze-activated magnification in VR reading.}
Additional applications include \textbf{perspective switching control} using different eye movement modes\cite{DBLP:conf/chi/LeeWSG24} and text operations such as Selection-and-Snap and Gaze Scroll (Lee \textit{et al.} \cite{DBLP:conf/ismar/LeeHM22}). These diverse applications demonstrate the potential of gaze-based interactions to significantly enhance user experiences across various XR scenarios. \revise{}{Fig. \ref{Gaze} illustrates gaze-only interactions.}

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{gaze2.jpg}
    \caption{\revise{}{The illustrations of gaze only interaction: (a) gaze vergence control \cite{DBLP:journals/tvcg/WangZ022}; (b) gaze patterns \cite{wang2024tasks}. Images courtesy of \cite{DBLP:journals/tvcg/WangZ022, wang2024tasks}}.}
    \label{Gaze}
\end{figure}

\textbf{Advantages and disadvantages of gaze-only interaction.}
The main advantages of eye movement interaction include speed, ease of use, and intuitiveness. Its maximum utility is demonstrated when large-scale physical movements are impossible or when both hands are occupied with tasks. Furthermore, eye movements possess dual selection capabilities in both 2D planes and depth directions. These capabilities can be combined to achieve more diverse interactions. Richer semantic information can also be extracted from human eye movement patterns. Pure eye movement modality has the potential to enable more powerful interaction functions.
In eye movement interaction, the human eye serves as both the medium for initiating interactive behaviors and the primary sensory organ for observation in XR. Consequently, the Midas touch problem persists. This issue remains a primary consideration for researchers in subsequent studies. Similarly, many eye movement interactions involve the design of new interaction interfaces and visual cues to assist users in completing interactions. The visual disturbances and cognitive load imposed on users by these new interfaces are significant issues that cannot be overlooked.


\subsubsection{Speech Only} 
\label{SpeechOnly}

\begin{figure}[t]
    \begin{center}
    % \begin{overpic} 
    % [width=\linewidth]
    % {example-image-a}
    % \end{overpic}
    \includegraphics[width=1\linewidth]{LLMSpeechimg.pdf}
    \end{center}
    %\vspace{-4mm}
    \caption{
        \revise{}{A common process in LLM-based speech interactions. The user's voice is captured and transcribed into text, which is then combined with contextual information to form prompts for the LLM. The LLM's feedback is subsequently provided to the user or processed further by the system.}
    }
    \label{fig:LLMSpeech}
    %\vspace{-2mm}
\end{figure}


\textbf{Hardware and algorithm.} Speech-based interactions in XR environments are typically enabled by a single microphone. While some researchers utilize an external microphone \cite{DBLP:conf/vr/SaintAubertAMPAL23, DBLP:conf/chi/Hedeshy0MS21}, the majority depend on the built-in microphone of VR/AR HMDs, such as Hololens 2 \cite{DBLP:conf/ismar/ChenGFCL23}, HTC Vive Pro Eye \cite{DBLP:conf/vr/JingLB22, DBLP:conf/ismar/MengXL22}, and Valve Index \cite{DBLP:conf/vr/HombeckVHDL23}. For speech recognition, most researchers rely on established speech-to-text systems or APIs, such as Windows DictationRecognizer \cite{DBLP:conf/vr/JingLB22} and WebSpeech \cite{DBLP:conf/uist/LiaoKJKS22}. To summarize, these devices and methods have been well established in recent years.

In addition to traditional method above, there is a growing interest in \textbf{silent speech recognition}, which enables speech detection without audible vocalization. This approach often involves additional devices, such as those used in active acoustic sensing.
\revise{For example, EchoSpeech leverages inaudible echo wave that emitted by two speakers and received by two microphones, mounted on a glass-frame \cite{DBLP:conf/chi/ZhangLHWLGZ23}.}{For example, EchoSpeech\cite{DBLP:conf/chi/ZhangLHWLGZ23} leverages inaudible echo wave that emitted by two speakers and received by two microphones, mounted on a glass-frame.} 
\revise{Cai \textit{et al.} further enhanced this by combining both vocal and echoic modalities for more robust speech recognition across various scenarios \cite{DBLP:journals/tvcg/CaiML24}.}{Cai \textit{et al.} \cite{DBLP:journals/tvcg/CaiML24} further enhanced this by combining both vocal and echoic modalities for more robust speech recognition across various scenarios.}
Depth cameras have also been explored for silent speech recognition. 
\revise{Wang \textit{et al.} used TrueDepth camera at three different locations to capture lip movement depth data, enabling silent speech recognition via point cloud video analysis \cite{DBLP:conf/chi/WangSRZ24}.}{Wang \textit{et al.}\cite{DBLP:conf/chi/WangSRZ24} used TrueDepth camera at three different locations to capture lip movement depth data, enabling silent speech recognition via point cloud video analysis.}

% \textbf{Summery of current Research:} Nowadays, speech-based interactions can be devided into two variations, which can be called keyword-based and LLM-based. The former xxxx, the latter xxx. xxx[] uses speech to xxx. As LLM 理解能力越来越强，效果越来越好，more and more(换个词) research use LLM as an intelligent agent to xxx.
\textbf{Summery of current research.} Recently, speech-based interactions can be generally classified into two categories: keyword-based and LLM-based. 
\textbf{Keyword-based} speech interactions identify keywords provided by users as interaction cues or commands, typically including pronouns (\textit{e.g.}, ``this" and ``here") \cite{DBLP:conf/vr/JingLB22}, commands (\textit{e.g.}, ``teleport" and ``select") \cite{DBLP:conf/vr/HombeckVHDL23, DBLP:conf/ismar/ChenGFCL23}, and user-defined keywords \cite{DBLP:conf/uist/LiaoKJKS22, DBLP:conf/chi/CaoKWAX24}. For instance, Hombeck \textit{et al.} \cite{DBLP:conf/vr/HombeckVHDL23} introduced three speech-only interaction techniques for locomotion, by keywords of direction (\textit{e.g.}, ``\textit{left}"), landmark (\textit{e.g.}, ``jump to \textit{bed}") or grid number (\textit{e.g.}, ``teleport to \textit{fifteen}"). However, keyword-based interaction imposes significant limitations on the vocabulary available for interaction \cite{DBLP:conf/vr/HombeckVHDL23}.

In contrast, \textbf{LLM-based} speech interactions overcome this restriction. With the increasing sophistication of large language models (LLMs), more researchers are leveraging LLMs as intelligent agents for interaction, allowing users to issue commands without vocabulary constraints. 
\revise{In this approach, the entire speech input is transcribed into text, which is then combined with prompt words or other contextual information as input to the LLM for further inference\cite{DBLP:conf/chi/WangYWJ024, DBLP:conf/vr/GiunchiNGS24, DBLP:conf/chi/0005WBCRF24, DBLP:conf/chi/TorreFHBFL24}.}{In this approach, the entire speech input is transcribed into text, which is then combined with prompt words or other contextual information as input to the LLM for further inference\cite{DBLP:conf/chi/WangYWJ024, DBLP:conf/vr/GiunchiNGS24, DBLP:conf/chi/0005WBCRF24, DBLP:conf/chi/TorreFHBFL24}, as shown in Fig. \ref{fig:LLMSpeech}.} 
\revise{DreamCodeVR exemplifies the use of LLMs to translate spoken language into code, enabling users to modify the behaviour of a running VR application irrespective of their programming skills \cite{DBLP:conf/vr/GiunchiNGS24}.}{DreamCodeVR \cite{DBLP:conf/vr/GiunchiNGS24} exemplifies the use of LLMs to translate spoken language into code, enabling users to modify the behaviour of a running VR application irrespective of their programming skills.} LLM-based speech interactions have significantly expanded the scope of operations and applications of speech-based interaction.

% Ad & Dis: Speech-based interactions is well-known(换个词) for its intuitiveness and simplity. For example, xxxxx utilize users' speech to xxxx. 
% However, compared to gesture and gaze, speech only interactions in recent 3 years are much less explored. This results from the recogizing latency xxxxx. This is also mentioned by xxx[]. Besides, speech-based interations suffers from its lack of subtleness, social acceptance, xxx. xxx[] indicate that xxx.
\textbf{Advantages and disadvantages of speech only interaction.} Speech only interaction is widely acknowledged for its minimal physical effort, as speaking in natural language requires little exertion\cite{DBLP:conf/vr/HombeckVHDL23}. 
\revise{Moreover, speech interactions for discrete selection are robust and require no physical movements or gestures \cite{DBLP:conf/ismar/ChenGFCL23}.For text input, speech aided by speech-to-text technology is much quicker and more intuitive than keyboard input \cite{DBLP:conf/vr/HombeckVHDL23}.}{Moreover, speech interactions for discrete selection are robust and do not require physical movements or gestures \cite{DBLP:conf/ismar/ChenGFCL23}. For text input task, speech input aided by speech-to-text technology is much quicker and more intuitive than keyboard input \cite{DBLP:conf/vr/HombeckVHDL23}.} However, speech-only interaction has notable drawbacks.\revise{The most frequently cited weaknesses are latency and inaccuracy.}{The weaknesses cited most frequently are high latency and inaccuracy.} 
\revise{Several studies reported that delays or recognition errors have impacted the user experience during the interaction\cite{DBLP:conf/vr/HombeckVHDL23, DBLP:conf/vr/SaintAubertAMPAL23, DBLP:conf/vr/JingLB22, DBLP:conf/ismar/MengXL22, DBLP:conf/uist/LiaoKJKS22}.}{Several studies\cite{DBLP:conf/vr/HombeckVHDL23, DBLP:conf/vr/SaintAubertAMPAL23, DBLP:conf/vr/JingLB22, DBLP:conf/ismar/MengXL22, DBLP:conf/uist/LiaoKJKS22} reported that delays or recognition errors have impacted the user experience during interaction.} Additionally, speech interactions, especially keyword-based interactions, often increase cognitive load and raises learning curve due to the requirement of memorizing specific commands \cite{DBLP:conf/ismar/ChenGFCL23}. Furthermore, speech interactions lack subtlety and may face social acceptance challenges \cite{DBLP:conf/chi/WangYWJ024, DBLP:conf/ismar/MengXL22}. Fortunately, recent advancements mentioned above in silent speech recognition offer potential solutions of this problem.

%===这个放discussion===
% Although LLM enables speech-based interactions to understand context, there are still some problems...

\begin{figure}
    \begin{center}
    % \begin{overpic} 
    % [width=\linewidth]
    % {example-image-a}
    % \end{overpic}
    \includegraphics[width=1\linewidth]{TactileInteractionsimg.pdf}
    \end{center}
    %\vspace{-4mm}
    \caption{
        \revise{}{Different devices and locations of tactile interactions: (a) NailRing \cite{DBLP:conf/ismar/LiLMHLS22}, (b) AO-Finger \cite{DBLP:conf/chi/XuZKN23}, (c) GazeRing \cite{DBLP:conf/ismar/WangSHRS024}. Images courtesy of \cite{DBLP:conf/ismar/LiLMHLS22}, \cite{DBLP:conf/chi/XuZKN23} and \cite{DBLP:conf/ismar/WangSHRS024}.}
    }
    \label{fig:TactileInteractions}
    %\vspace{-2mm}
\end{figure}


\subsubsection{Tactile Interaction}
\label{Tactile}

\revise{In discussing ``Tactile Interactions", we refer to interactions with additional input devices that are light-weight, wearable and technologically advanced.}{In discussing ``Tactile Interaction", we refer to interactions with additional input devices that are light-weight, wearable and technologically advanced, for example those illustrated in Fig. \ref{fig:TactileInteractions}.}
As such, interactions involving controllers or cumbersome tangible objects are excluded. This section focuses on two primary interaction techniques: finger gesture interaction and touch-based interaction. Additionally, studies investigating tactile feedback to enhance user immersion are also presented.



\textbf{Finger gesture interactions} have been extensively studied in recent years. Researchers various input modalities, such as cameras \cite{DBLP:conf/ismar/LiLMHLS22}, optical sensors \cite{DBLP:conf/iswc/KitamuraYS23, DBLP:conf/chi/XuZKN23} and electric field sensors \cite{DBLP:journals/imwut/ChenLYZ22}, to recognize predefined finger gestures. 
\revise{The positioning of input devices also varies: NailRing \cite{DBLP:conf/ismar/LiLMHLS22} and TouchLog \cite{DBLP:conf/iswc/KitamuraYS23} placed their devices above the user's index fingernail.}{The positioning of input devices also differs: NailRing \cite{DBLP:conf/ismar/LiLMHLS22} and TouchLog \cite{DBLP:conf/iswc/KitamuraYS23} place their devices above the user's index fingernail, whereas AO-Finger \cite{DBLP:conf/chi/XuZKN23} positions its devices around the user's wrist}. 
\revise{NailRing utilized a micro close-focus camera to capture changes of color in the nail and finger \cite{DBLP:conf/ismar/LiLMHLS22}.}{NailRing\cite{DBLP:conf/ismar/LiLMHLS22} utilizes a micro close-focus camera to capture changes of color in the nail and finger.} 
\revise{Rather than cameras, TouchLog employed a nail-type device with photo-reflective sensors for privacy, to detect the skin deformation of the fingertip during gestures \cite{DBLP:conf/iswc/KitamuraYS23}.}{Rather than cameras, TouchLog \cite{DBLP:conf/iswc/KitamuraYS23} employs a nail-type device with photo-reflective sensors for privacy, to detect the skin deformation of the fingertip during gestures.} 
\revise{Beyond discrete gestures, EFRing explored continuous 1D finger micro-movement tracking using a ring-shaped device through electric-field sensing \cite{DBLP:journals/imwut/ChenLYZ22}.}{Beyond discrete gestures, EFRing \cite{DBLP:journals/imwut/ChenLYZ22} explores continuous 1D finger micro-movement tracking using a ring-shaped device through electric-field sensing.}

Finger gesture interactions offer several advantages. Their primary advantage lies in their subtlety and greater social acceptance \cite{DBLP:conf/ismar/LiLMHLS22, DBLP:conf/iswc/KitamuraYS23, DBLP:conf/chi/XuZKN23, DBLP:journals/imwut/ChenLYZ22}. Notably, the pre-defined finger gestures in recent works typically involve no more than two fingers (thumb and index). Thus, these techniques are much more effortless and light-weight compared to hand gesture and other methods \cite{DBLP:conf/chi/XuZKN23}. However, finger gesture interactions face challenges with generalization accuracy due to individual differences, as reported by \cite{DBLP:conf/iswc/KitamuraYS23, DBLP:conf/ismar/LiLMHLS22}. Both studies suggest that individual calibration before use can mitigate this issue. Additionally, due to the complexity of recognition algorithms and the computational limitations of mobile devices, finger gesture recognition exceeds the capabilities of the minimal input devices and thus often confined to PC platforms \cite{DBLP:conf/chi/XuZKN23, DBLP:journals/imwut/ChenLYZ22}.

\textbf{Touch-based interactions} involve works with the need for a surface or device to be physically touched. Researchers proposed numerous devices for touch-based interactions, such as flexible sensors \cite{DBLP:journals/imwut/ZhanXZGCGLQ23},  packaged microphones \cite{10522613}, and even robots \cite{DBLP:conf/vr/MortezapoorVVK23}.
\revise{For text editing task in speech-unfriendly AR environments, TouchEditor equipped the user's arm with a flexible touchpad composed of flexible pressure sensors \cite{DBLP:journals/imwut/ZhanXZGCGLQ23}.}{For text editing task in speech-unfriendly AR environments, TouchEditor \cite{DBLP:journals/imwut/ZhanXZGCGLQ23} equipped the user's arm with a flexible touchpad composed of flexible pressure sensors.} They designed numerous operations including text selection, cursor positioning, text retyping and editing commands. 
\revise{Similarly focusing on text entry in AR, TapGazer explored typing with TapStrap (finger-worn accelerometers), touch-sensitive gloves or touchpads, supported by gaze and language model \cite{DBLP:conf/chi/HeLP22}.}{Similarly focusing on text entry in AR, TapGazer \cite{DBLP:conf/chi/HeLP22} explored typing with TapStrap (finger-worn accelerometers), touch-sensitive gloves or touchpads, supported by gaze and language model.} Buttons can also be wearable to offer a touch-based solution.
\revise{FingerButton allows users to seamlessly transition between real world and VR with a finger-worn button device \cite{DBLP:conf/ismar/DasNH23}. Satriadi \textit{et al.} demonstrated greater creativity by leveraging tangible globes to explore the design of immersive data visualization in AR \cite{DBLP:conf/chi/SatriadiSECCLYD22}.}{FingerButton \cite{DBLP:conf/ismar/DasNH23} allows users to seamlessly transition between the real world and VR with a finger-worn button device. Satriadi \textit{et al.} \cite{DBLP:conf/chi/SatriadiSECCLYD22} demonstrated greater creativity by leveraging tangible globes to explore the design of immersive data visualization in AR.}

Touch-based interactions seem to represent a compromise between traditional device (\textit{e.g.}, controller and keyboard) and finger or hand gesture. Their performance of speed and accuracy is comparable to traditional input devices like keyboards \cite{DBLP:conf/chi/HeLP22}, while offering greater convenience and subtlety akin to finger or hand gestures \cite{DBLP:journals/imwut/ZhanXZGCGLQ23, DBLP:conf/ismar/DasNH23}. Moreover, they cause less arm and hand fatigue compared to gesture-based interactions. On the other hand, they still require additional hardware \cite{DBLP:conf/chi/HeLP22} and face the challenges related to the larger form factors of these devices \cite{DBLP:journals/imwut/ZhanXZGCGLQ23}. Furthermore, touch-based interactions are dependent on surfaces (\textit{e.g.}, arm) for input.

\textbf{Tactile feedback.} % Immersion is a critical component of the XR experience and constitutes one of the 3I of VR \cite{DBLP:books/daglib/0011673}.
Feedback from the interactive device is necessary to enhance the interactive experience \cite{DBLP:conf/ismar/LiLMHLS22}. Since wearable tactile devices is always available to access \cite{DBLP:conf/ismar/DasNH23}, several studies have investigated these devices to provide tactile feedback. 
\revise{For example, Saint-Aubert \textit{et al.} utilized a HapCoil-Plus actuator to generate speech-based tactile vibrations, aiming to enhance users' persuasiveness, co-presence, and leadership \cite{DBLP:conf/vr/SaintAubertAMPAL23}. Jingu \textit{et al.} introduced an electrotactile device with a thin and flexible form factor to enable double-sided simulation feedback within pinched fingerpads \cite{DBLP:conf/uist/JinguWS23}.}{For example, Saint-Aubert \textit{et al.} \cite{DBLP:conf/vr/SaintAubertAMPAL23} utilized a HapCoil-Plus actuator to generate speech-based tactile vibrations, aiming to enhance users' persuasiveness, co-presence, and leadership. Jingu \textit{et al.} \cite{DBLP:conf/uist/JinguWS23} introduced an electrotactile device with a thin and flexible form factor to enable double-sided simulation feedback within pinched fingerpads.}

\subsubsection{Multimodal Interaction}
\label{MultiInteraction}



Unimodal interactions, as discussed above, have been explored across various spatial computing scenarios, but each comes with its own limitations.
\revise{By leveraging the complementary strengths of these modalities, multimodal interactions seeks to enhance usability. Wang \textit{et al.} argued that eye gaze-based interaction is more suitable for primary target pointing, hand gestures provide capabilities for transformation and editing, and descriptive voice input improves system controllability, \textit{e.g.}, commands such as open/close or up/down \cite{DBLP:journals/thms/WangWYL21}.}{By leveraging the complementary strengths of these modalities, multimodal interactions seek to enhance usability. Wang \textit{et al.} \cite{DBLP:journals/thms/WangWYL21} argued that eye gaze-based interaction is more suitable for primary target pointing, hand gestures provide capabilities for transformation and editing, and descriptive voice input improves system controllability, \textit{e.g.}, commands such as open/close or up/down.} Inspired by these insights, recent research has developed various multimodal interactions for different XR tasks \cite{DBLP:conf/ismar/ChenGFCL23, DBLP:conf/vr/JingLB22}. In this paper, we primarily focus on dual-modal techniques (\textit{e.g.}, \textit{Gaze + Gesture}, \textit{Gaze + Speech}, \textit{Gesture + Speech}) and a triple-modal technique (\textit{Gaze + Gesture + Speech}).

\textbf{(a) Gaze + Gesture}. Many studies adhere to the principle of “gaze selects, hand manipulates” \cite{10108465}, as eye gaze, which can rapidly locate a target, complements the fine-grained control provided by hand gestures. Apple Vision Pro identified this dual-modal interaction as the primary access method for spatial computing \cite{Apple}. In the reviewed literature, two papers focused on text input tasks in XR environments \cite{10474330, DBLP:conf/chi/HeLP22}.
\revise{Ren \textit{et al.} utilized the fact that gaze reaches the target before the hand, predicting the next likely key and enlarging and highlighting it, allowing users to press it more quickly and accurately \cite{10474330}. He \textit{et al.} enabled users to input text by tapping the approximate area of a character on a keyboard, without needing to see their hands or the keyboard. Additionally, users can resolve ambiguities by selecting the intended word via eye movement \cite{DBLP:conf/chi/HeLP22}.}{Ren \textit{et al.} \cite{10474330} found that gaze reaches the target before the hand, and leveraged this to enlarge and highlight the possible following key, providing users quicker and more accurate selection. He \textit{et al.} \cite{DBLP:conf/chi/HeLP22} enabled users to input text by tapping the approximate area of a character on a keyboard, without looking at their hands or the keyboard. Additionally, users can resolve ambiguities by selecting the intended word via eye movement.}
Researchers also explored using gaze as a directional reference, combined with hand movements along the gaze ray or swipes to navigate in VR environments \cite{10.1145/3613904.3642147}.

Besides above tasks, many studies have examined menu selection and 3D object manipulation in XR \cite{10.1145/3544548.3581423, 10.1145/3530886, 10.1145/3591129, 10.1145/3613904.3642758, 10108465, DBLP:conf/ismar/CailletGN23}.
\revise{Wagner \textit{et al.} designed a Fitts’ Law study to evaluate the efficiency of two different gaze-hand alignment techniques for target selection \cite{10.1145/3544548.3581423, 10.1145/3530886}. Shi \textit{et al.} used eye gaze and hand gestures for region selection in AR \cite{10.1145/3591129}. Rodríguez \textit{et al.} enabled artists to draw VR sketches using this dual-modal interaction \cite{10.1145/3613904.3642758}. Bao \textit{et al.} explored how hand-eye coordination can facilitate object selection and manipulation in occluded environments \cite{10108465}. Caillet \textit{et al.} designed a two-phase interaction technique with gaze and hand gesture for reduce fatigue in 3D selection \cite{DBLP:conf/ismar/CailletGN23}.}{Wagner \textit{et al.} \cite{10.1145/3544548.3581423, 10.1145/3530886} designed a Fitts’ Law study to evaluate the efficiency of two different gaze-hand alignment techniques for target selection. Shi \textit{et al.} \cite{10.1145/3591129} utilized eye gaze and hand gestures for region selection in AR. Rodríguez \textit{et al.} \cite{10.1145/3613904.3642758} enabled artists to draw VR sketches using this dual-modal interaction. Bao \textit{et al.} \cite{10108465} explored how hand-eye coordination can facilitate object selection and manipulation in occluded environments. Caillet \textit{et al.} \cite{DBLP:conf/ismar/CailletGN23} designed a two-phase interaction technique with gaze and hand gesture to reduce fatigue in 3D selection.}

% \textbf{Gaze + Gesture}. Many studies follow the principle of “gaze selects, hand manipulates” \ccite, as eye gaze, which can rapidly locate a target, complements the fine-grained control provided by hand gestures. Apple Vision Pro considers this dual-modal interaction as the primary access method for spatial computing \ccite. In the reviewed literature, two papers investigate text input tasks in XR environments. Ren \textit{et al.} leverage the characteristic that gaze reaches the target before the hand, predicting the next likely key and enlarging and highlighting it, allowing users to press it more quickly and accurately \ccite. He \textit{et al.} enable users to input text by tapping the general area of a character on a keyboard, without needing to see their hands or the keyboard. Additionally, users can resolve ambiguities by selecting the intended word via eye movement \ccite. 
% 也有研究者探索利用视线作为方向基准，然后手在射线上前后移动或左右划动实现在VR中行走\ccite。

% 除上述任务外，许多研究者针对XR中菜单、3D object的选择和操控进行研究\ccite。Wagner \textit{et al.} designed the Fits’ Law Study, 以评估两种不同的gaze-hand alignment techniques在不同目标深度、物体视场角情况下的选择效率 \cite{10.1145/3544548.3581423, 10.1145/3530886}。 Shi \textit{et al.} 使用eye gaze和hand gestures进行AR中的区域选择 \ccite。Rodrguez \textit{et al.} 允许artists去draw the VR sketching using this dual-modal interaction \ccite。Bao \textit{et al.}探索了hand-eye coordination如何实现遮挡环境下的物体选择与移动 \ccite。

\textbf{(b) Gaze + Speech.} Eye gaze and speech are typically integrated to offer hands-free interaction: gaze provides contextual information, while speech functions as a command or query.
\revise{Jing \textit{et al.} visualized shared gaze cues using contextual speech input to enhance the efficiency of XR remote collaboration \cite{DBLP:conf/vr/JingLB22}.}{Jing \textit{et al.} \cite{DBLP:conf/vr/JingLB22} visualized shared gaze cues using contextual speech input to enhance the efficiency of XR remote collaboration.} When the XR system detects keywords such as ``this" or ``that", it guides collaborators to follow the speaker's gaze direction.
\revise{Li \textit{et al.} proposed a method in AR environments that processes multimodal sensory inputs (visual and auditory) from daily activities using LLMs, predicting potential digital follow-up actions \cite{10.1145/3613904.3642068}.}{Li \textit{et al.} \cite{10.1145/3613904.3642068} proposed a method in AR environments that processes multimodal sensory inputs (visual and auditory) from daily activities using LLMs, predicting potential digital follow-up actions.} 
\revise{G-VOILA leveraged gaze data and visual field as contextual information to enhance LLMs' ability to respond to users' daily speech queries \cite{DBLP:journals/imwut/WangSWYYWJXY24}.}{G-VOILA \cite{DBLP:journals/imwut/WangSWYYWJXY24} leveraged gaze data and visual field as contextual information to enhance LLMs' ability to respond to users' daily speech queries.} In addition to serving as inputs, visual and audio modalities can also function as outputs of XR systems to enhance immersion.
\revise{Lee \textit{et al.} designed a multi-modal attention guidance system that utilizes visual cues (lighting effects) and auditory cues (spatial audio) in social VR group conversations, significantly improving users' response times and conversation satisfaction during turn-taking interactions \cite{10462901}.}{Lee \textit{et al.} \cite{10462901} designed a multi-modal attention guidance system that utilizes visual cues (lighting effects) and auditory cues (spatial audio) in VR social group conversations, significantly improving users' response times and conversation satisfaction during turn-taking interaction.}

\textbf{(c) Gesture + Speech.} Gesture and speech are the two most commonly used modalities in delivering presentations. Therefore, they are naturally integrated for augmented presentations. For example,
\revise{Liao \textit{et al.} proposed to augmented live presentations using visuals and animation for expressive storytelling \cite{DBLP:conf/uist/LiaoKJKS22}.To achieve this, they used speech recognition combined with keyword extraction. These keywords are mapped to predefined display contents, such as images and videos. These contents are displayed on the hand in real time.}{Liao \textit{et al.} \cite{DBLP:conf/uist/LiaoKJKS22} proposed augmented live presentations using visuals and animation for expressive storytelling. To achieve this, they used speech recognition combined with keyword extraction. These keywords are mapped to predefined display contents, such as images and videos, which are displayed on the user's hand in real time.} 
However, this method presented inferior visual artifacts due to imperfect system recognition \cite{DBLP:conf/chi/CaoKWAX24}. 
\revise{Cao \textit{et al.} predefined the layouts and effects of graphics and used a set of rules to allow speech, gesture and graphics to be elastically connected for visual quality \cite{DBLP:conf/chi/CaoKWAX24}.}{Cao \textit{et al.} \cite{DBLP:conf/chi/CaoKWAX24} predefined the layouts and effects of graphics and used a set of rules to allow speech, gesture and graphics to be elastically connected for visual quality.}
\revise{Williams \textit{et al.} investigated the effect of referent display methods (text vs. animation) on gesture and speech elicitation \cite{9873984}. In terms of animated referents, they found users elicited interaction proposals with gesture+speech, feeling lower workload than speech only conditions. }{Williams \textit{et al.} \cite{9873984} investigated the effect of referent display methods (text vs. animation) on gesture and speech elicitation. In terms of animated referents, they found users elicited interaction proposals with gesture + speech and suffered lower workload than speech-only interactions. }

\textbf{(d) Gaze + Gesture + Speech.} Several studies have also explored the integration of gaze, gesture, and speech to leverage the benefits of all three modalities.
\revise{Chen \textit{et al.} proposed the Compass+Ring menu that integrates three modalities: gaze is used to point at menu items, speech to confirm selections (avoiding the Midas Touch problem), and gestures to adjust parameters by rotating the wrist or pinching fingers to navigate menu levels. The Ring menu allows for quick adjustments using simple gestures like ``putting on" and ``rotating" a virtual ring \cite{DBLP:conf/ismar/ChenGFCL23}. Lee \textit{et al.} leveraged eye gaze, hand pointing and conversation history to achieve pronoun disambiguation in users' speech. When the user queries using pronouns, the proposed system identifies objects and text within the field-of-view (FoV) to replace the pronouns. This approach enables the voice assistant to provide more accurate responses by incorporating contextual information \cite{DBLP:conf/chi/0005WBCRF24}.}{Chen \textit{et al.} \cite{DBLP:conf/ismar/ChenGFCL23} proposed the Compass+Ring menu that integrates three modalities: gaze to point at menu items, speech to confirm selections, and gestures to adjust parameters by rotating the wrist or pinching fingers to navigate menu levels. The Ring menu allows for quick adjustments using simple gestures like ``putting on" and ``rotating" a virtual ring. Lee \textit{et al.} \cite{DBLP:conf/chi/0005WBCRF24} leveraged eye gaze, hand pointing and conversation history to achieve pronoun disambiguation in users' speech. When the user queries with pronouns, the proposed system replace them with identified objects and text within the field-of-view (FoV). This approach enables the voice assistant to provide more accurate responses by incorporating contextual information.}


\textbf{(e) X + Y.} In the reviewed literature, numerous studies explored multimodal interactions beyond the modalities discussed previously, which here are referred to ``X + Y". Due to space constraints, only three types of them are presented in this section: X + head pose, X + facial expression, and multimodal comparison studies.
\textit{X + head pose.}
Several studies focus on integrating head pose with other modalities \cite{DBLP:journals/imwut/ShenYYS22, DBLP:conf/chi/WeiSYW0YL23, DBLP:conf/chi/HouNSKBG23, marquardt2024selection, DBLP:conf/chi/LeeWSG24}. 
\revise{For example, Clenchclick explored explored the combination of head movements and teeth clenching for target selection in AR \cite{DBLP:journals/imwut/ShenYYS22}. }{For example, Clenchclick \cite{DBLP:journals/imwut/ShenYYS22} explored the combination of head movements and teeth clenching for target selection in AR. }
\textit{X + facial expression.}
Some other studies emphasize combining facial expressions with additional modalities \cite{DBLP:conf/vr/LaiSL24, DBLP:conf/uist/0001QTFLS22}. 
\revise{GazePuffer, for instance, introduced cheek puffing gestures combined with gaze to offer an innovative hands-free interaction \cite{DBLP:conf/vr/LaiSL24}.}{GazePuffer \cite{DBLP:conf/vr/LaiSL24}, for instance, introduced cheek puffing gestures combined with gaze to offer an innovative hands-free interaction.}
\textit{Multimodal Comparison.}
Additionally, some research compares unimodal and multimodal interaction techniques to determine which modalities are better suited for specific tasks or scenarios \cite{10049667, DBLP:conf/ismar/MengXL22, DBLP:conf/ismar/XuMYSL22}.
\revise{In terms of the text selection task in VR, Meng \textit{et al.} designed three hands-free selection mechanisms, including dwell, eye Blinks and voice (hum) to complement head-based pointing \cite{DBLP:conf/ismar/MengXL22}. Yan \textit{et al.} introduced ConeSpeech, a VR-based directional speech interaction method allowing users to selectively communicate with target listeners, minimizing disturbance to bystanders \cite{10049667}.}{In terms of the text selection task in VR, Meng \textit{et al.} \cite{DBLP:conf/ismar/MengXL22} designed three hands-free selection mechanisms, including dwell, eye Blinks and voice (hum) to complement head-based pointing. Yan \textit{et al.} \cite{10049667} introduced ConeSpeech, a VR-based directional speech interaction method allowing users to selectively communicate with target listeners, minimizing disturbance to bystanders.} The authors compare five control modalities, \textit{i.e.} head, gaze, torso, hand, and controller, and ultimately select head orientation for its balance of speed, accuracy, and intuitiveness.







\section{Discussion and Recommendations}

Based on the previous taxonomy and analysis of natural interaction techniques in wearable XR, this section focuses on the challenges these techniques face and offers recommendations for future research directions. Objectively, researchers aim to develop more accurate and efficient natural interaction techniques. Subjectively, the goal is to provide more comfortable and immersive interaction experiences. Additionally, to enhance the usability of these techniques, researchers should apply them in real-world scenarios to improve the overall user experience in XR.

Accordingly, the discussion is organized into three main areas. Section \ref{4.1} presents recommendations for toward more accurate and reliable natural interactions in XR. Section \ref{4.2} discusses recommendations for toward more natural, comfortable, and immersive interactions in XR. Lastly, Section \ref{4.3} offers suggestions for bridging interaction design and practical XR applications.

% 基于上述对natural interaction techniques in wearable XR的分类和分析，本节主要针对现有交互技巧面临的挑战进行阐述，结合未来的研究方向进行推荐。在客观上，researchers希望更加精确、efficient的自然交互技巧。在主观上，researchers希望提供更comfort and immersive的自然交互技巧。同时，考虑到自然交互技巧的可用性，reseachers应该将这些交互技巧，应用到实际的应用场景中，提升XR 的user experience. 因此，接下来主要从三方面进行阐述。Section \ref{4.1}阐述这个推荐Toward More Accurate and Reliable Natural Interactions in XR。Section \ref{4.2}阐述推荐Toward More Natural, Comfortable and Immersive Interactions in XR。Section \ref{4.3}建议Bridging Interaction Design and Practical XR Applications。


\subsection{Toward More Accurate and Reliable Natural Interactions in XR}
\label{4.1}
As above section mentioned, current natural interaction such as gaze, gesture and speech, still face the insufficient accuracy during interaction, especially in the complex situations, \textit{e.g.}, outdoors and walking. 
For example, for reducing the effects of Midas touch problem on gaze-only interaction, Section \ref{GazeOnly} mentioned many researchers optimize the virtual menu layouts \cite{DBLP:conf/chi/ChoiSO22, DBLP:conf/vr/0001LC00S22}, or require extra eye movements to confirm the selections \cite{DBLP:conf/vr/OrloskyLSSM24, kim2022lattice}. For the false hand gesture recognition caused by occlusion, researchers usually require users to manipulate objects in certain postures and orientations to avoid occlusion \cite{DBLP:conf/chi/PeiCLZ22, DBLP:journals/tvcg/SongDK23}, or use the wearable device based hand-tracking \cite{DBLP:conf/chi/HeLP22}.
While these methods have shown some success in improving interaction accuracy, there is still significant room for advancement. We propose that the future of achieving more accurate XR interaction lies in three promising research directions: multimodal interaction, error recovery mechanisms, and AI-powered assistance.

\textbf{Multimodal interaction.} Integrating multiple modalities into a unified interaction system significantly enhances the accuracy and reliability of user inputs. For instance, Bao \textit{et al.} addressed the issue of low gaze-pointing accuracy by allowing the hand to refine the pointing direction, thereby facilitating target selection \cite{10108465}. Moreover, combining gaze with hand gesture interaction follows the principle of ``gaze select, hand manipulate" \cite{10.1145/3544548.3581423, 10.1145/3530886}. This approach reduces hand operations and thus decreases the likelihood of hand recognition error.
Multimodal integration can also resolve ambiguities in speech. For instance, Lee \textit{et al.} used gaze or hand pointing to identify specific objects, clarifying vague verbal descriptions such as ``What is this?" \cite{DBLP:conf/chi/0005WBCRF24}.
This highlights the effectiveness of multimodal systems in improving interaction accuracy and reducing ambiguity.

\textbf{Error recovery.} Error recovery mechanisms are crucial for ensuring robust and user-friendly interactions in XR environments. Despite advancements in tracking accuracy, unintended actions or misrecognitions such as clicking the wrong button due to inattention, remain inevitable. 
For example, Sendhilnathan \textit{et al.} categorized gesture-based interaction events into three types: correctly recognized input actions, input recognition errors, and user errors, observing that these categories were consistent across tasks. They then applied a deep learning method to differentiate these events using only eye movement input, achieving promising results \cite{DBLP:conf/uist/SendhilnathanZL22}.
Sidenmark \textit{et al.} designed an error-aware mechanism that adaptively switches to fallback modalities (\textit{e.g.}, head pointing or a controller) when errors or noise occur during gaze interactions. They also adjusted the weighting between the gaze modality and fallback options based on the error ratio \cite{DBLP:journals/tvcg/SidenmarkP0CGWG22}. These approaches highlight the vital role of error recovery in enhancing the reliability and overall usability of XR systems.





% \textbf{AI Integration.} 现有的交互设计，常常是rigid, step-by-step procedures。例如Wang \textit{et al.} 把交互任务拆解成三步，分别是primary pointing, confirmation, and manipulation \ccite. 这些复杂步骤可能会increases user's cognitive load， 从而造成user operation errors \cite{DBLP:conf/uist/SendhilnathanZL22}. 而且这些交互设计也非常依赖tracking accuracy of both eye movements and gestures \ccite. Recent advancement in AI technologies have shown the potential to enhance the usability of XR systems \ccite。例如，Wei \textit{et al.} 研究了视线方向和头部朝向的运动规律在注视物体时，并使用机器学习的方法预测用户所注视的物体，以提升选择的accuracy \ccite。最近在LLMs上的进步，更为XR interactions的发展带来新契机。Hu et al.设计了dwell-free 的eye typing in XR and uses LLMs to predict the word, 提升了打字速度 \cite{DBLP:conf/vr/HuDK24}. In the reviewed literature, 我们发现在2024年有4篇papers将LLMs作为intelligent assistant去帮助用户完成交互。这说明了 Integrating AI technologies into XR interactions已经成为目前最新的主流趋势。

% \textbf{Error recovery.} Error recovery mechanisms are essential for ensuring robust and user-friendly interactions in XR environments. Even with improved tracking accuracy, unintended actions or misrecognitions are inevitable, such as clicking the wrong button due to a lack of attention. Sendhilnathan \textit{et al.} 把gesture-based interaction时的事件分为三类：correctly recognized input actions, input recognition errors, and user errors，这些事件是consistent across tasks。然后使用deep learning method去区分这三类事件 using only eye movement input，取得了不错的效果. 
% Sidenmark \textit{et al.} design error-aware机制，在眼动交互出现误差和噪声的时候，自适应地调节到fallback模态(e.g., head point or controller)作为指向的工具。They also 调节gaze modality和fallback的权重根据误差 value \cite{DBLP:journals/tvcg/SidenmarkP0CGWG22}.


% 主观反馈。user-friendly，immersive (可穿戴，不用PC，重定向，feedback，视觉与语音的反馈）语音，不想要最准的。
\subsection{Toward More Natural, Comfortable and Immersive Interactions in XR}
\label{4.2}
% 除了准确性，交互技术还追求主观上的comfort（负担低、自然）、immersion（存在感）、usability（易学性）。在XR环境中，the feeling provided by a particular method may be more important to some users, compared to objective measurements (e.g. temporal performance) \cite{Tell me where to go}。 在我们的调研过程中，我们发现超过50篇论文在用户实验中使用了主观指标来研究用户对于交互技术的主观感受，比如immersion, cognitive load, fatigue, preference, usability。Commonly used检验方法包括NASA-TLX、SUS、SSQ等。这就说明用户的主观感受是交互技术设计非常重要的衡量指标。我们认为，未来的XR交互继续注重以下几个方面的改进：
Interaction techniques aim not only for accuracy but also for subjective factors such as comfort, immersion, and usability. In XR environments, the subjective experience of a particular method may be more important to certain users than objective metrics (\textit{e.g.}, temporal performance) \cite{DBLP:conf/vr/HombeckVHDL23}. In the reviewed literature, over 50 studies assess users' subjective experiences with interaction techniques through measures like task load, immersion, preference, and usability. These assessments are typically conducted via post-study questionnaires, such as the NASA Task Load Index (NASA-TLX), System Usability Scale (SUS), and Simulator Sickness Questionnaire (SSQ). This emphasizes the importance of user-friendliness as a critical quality of interaction techniques. We anticipate that future XR interactions will continue to prioritize improvements in: reducing task load, enhancing immersion, and improving subtlety.

\textbf{Reduce task load.}
Physical and cognitive load are critical factors in user’s interactive experience. The NASA-TLX is the most widely used tool for assessing the task load of interaction techniques. 
\revise{As discussed in \ref{Interaction techniques}, each modality presents its own limitations:}{As discussed in \ref{Interaction techniques}, each modality has its own limitations in terms of task load:} hand-gesture-only interactions can lead to arm fatigue \cite{DBLP:conf/vr/QuereMJWW24}; speech-only interactions increase cognitive load due to the requirement of memorizing keywords \cite{DBLP:conf/ismar/ChenGFCL23}; prolonged use of gaze-only interactions can also cause eye fatigue \cite{DBLP:conf/chi/ZhangCSS24}. Our review identified several studies focused on reducing users' burden. One common approach is to effectively integrate multiple modalities. 
\revise{For instance, Bao \textit{et al.} proposed a better method of combining gaze and hand gestures, significantly alleviating arm fatigue \cite{10108465}. Compass+Ring introduced a multimodal menu integrating gaze, speech, and gesture to mitigate eye fatigue \cite{DBLP:conf/ismar/ChenGFCL23}.}{For instance, Bao \textit{et al.} \cite{10108465} proposed a better method of combining gaze and hand gestures, significantly alleviating arm fatigue. Compass+Ring \cite{DBLP:conf/ismar/ChenGFCL23} introduced a multimodal menu integrating gaze, speech and gesture to mitigate eye fatigue.} Additionally, some studies innovate new devices and modalities, offering more effortless interactions such as finger gestures. 
\revise{AO-finger introduced a wristband that recognizes fine-grained finger gestures that require little exertion \cite{DBLP:conf/chi/XuZKN23}.}{AO-finger \cite{DBLP:conf/chi/XuZKN23} introduced a wristband that recognizes fine-grained finger gestures that require little exertion.} We believe that reducing task load will remain a key focus in future interaction design.

\textbf{Enhance immersion.}
\revise{Immersion is a critical component of the XR experience and constitutes one of the 3I of VR \cite{DBLP:books/daglib/0011673}}{Immersion is a critical component of the XR experience and is one of the 3Is of VR \cite{DBLP:books/daglib/0011673}}. Interaction, a bidirectional process, also aims to provide users with an immersive experience. Numerous studies mainly focus on offering feedback with different devices and technologies to enhance the immersion of XR. 
\revise{For instance, Jang \textit{et al.} utilized ultrasonic devices to deliver mid-air haptic feedback, allowing users to touch and explore volume-rendered hologram with their bare hands \cite{DBLP:conf/ismar/JangFP22}. Saint-Aubert \textit{et al.} investigated tactile vibrations speech from users or virtual avatars to enhance persuasiveness, co-presence, and leadership \cite{DBLP:conf/vr/SaintAubertAMPAL23}.}{For instance, Jang \textit{et al.} \cite{DBLP:conf/ismar/JangFP22} utilized ultrasonic devices to deliver mid-air haptic feedback, allowing users to touch and explore volume-rendered hologram with their bare hands. Saint-Aubert \textit{et al.} \cite{DBLP:conf/vr/SaintAubertAMPAL23} investigated tactile vibrations speech from users or virtual avatars to enhance persuasiveness, co-presence, and leadership.} Besides, several studies have proposed novel well-designed interaction techniques to enhance users' immersion. 
\revise{Illumotion is an example to increase presence and reduce cybersickness by designing a hand-gesture-based interaction technique inspired by photo manipulation \cite{DBLP:conf/vr/SinJLLLN24}.}{Illumotion \cite{DBLP:conf/vr/SinJLLLN24} is an example to increase presence and reduce cybersickness by designing a hand-gesture-based interaction technique inspired by photo manipulation.} This suggests that future research will continue to focus on enhancing immersion in XR environments.

\textbf{Improve subtlety.}
For XR devices to be integrated into daily life, interaction techniques must be adaptable across various scenarios without raising concerns. In public environments, conspicuous interactions may lead to social awkwardness and privacy challenges \cite{DBLP:conf/ismar/LiLMHLS22}. Thus, enhancing the subtlety of interaction techniques is crucial for improving social acceptance. Some existing methods, such as hand gestures and speech interactions, have been criticized for their lack of subtlety. Hand gestures are restricted by the FoV of HMD cameras \cite{DBLP:conf/chi/XuZKN23}, while speech interactions may disrupt the experiences of others \cite{DBLP:conf/chi/WangYWJ024}. Recent research has increasingly focused on developing more discreet interaction techniques. Tactile interactions, including micro finger gestures \cite{DBLP:conf/ismar/LiLMHLS22, DBLP:conf/chi/XuZKN23, DBLP:journals/imwut/ChenLYZ22, DBLP:conf/iswc/KitamuraYS23} and wearable devices \cite{DBLP:conf/ismar/DasNH23, DBLP:conf/chi/HeLP22}, are considered to offer greater subtlety and social acceptance compared to hand gestures. Additionally, to enable speech interactions in noisy or speech-unfriendly environments, several studies have proposed methods for silent speech recognition \cite{DBLP:journals/tvcg/CaiML24, DBLP:conf/chi/ZhangLHWLGZ23, DBLP:conf/chi/WangSRZ24}. Therefore, improving the subtlety of interaction techniques will remain a prominent and ongoing topic in XR research.

% 扩展多个应用（同一种应用，在不同场景下，experiment, 室内或室外），打通应用之间的壁垒。

% \subsection{Extending natural interaction techniques to different applications}

% 在Section \ref{Appl.}中提到，将近80\%的研究都聚焦在交互技巧的设计上，没有提到具体场景下的应用。虽然这些交互技巧可能扩展到不同应用下，但是需要针对具体场景进行适配和调整，这就造成了gap和一定阻力，影响了XR下自然交互技巧的推广。

% 现有自然交互技巧在具体场景中的应用：Section \ref{Appl.}提到了20余篇论文在不同应用场景下的自然交互技巧探索，例如sketching, virtual meeting, XR navigation, reading, maintenance, 等等。自然交互技巧在这些应用场景中有很大的潜力可以进一步扩展。例如，在sketching中，可以通过结合大脑—计算机接口（BCI）与眼动追踪技术，让用户通过思维直接控制设计对象的形状和颜色，而非仅依赖手势或语音。这种方式不仅能提升创作效率，还能让原本有肢体障碍的人群也能参与复杂的创作任务。在虚拟会议场景中，可以利用实时情感识别技术，通过捕捉与会者的面部表情、语调和心率，自动生成个性化的反馈或建议，甚至让虚拟助理根据对情感的分析适时插入辅助信息或总结。

% 自然交互技巧也可以应用到新的应用场景中。例如在medicine领域，自然交互技术可以通过结合眼动、手势和语音，为住院患者提供更加个性化和自主的护理体验。例如，患者可以通过眼动追踪选择病房内的不同设备，如灯光、温度、床位的角度等。选择完毕后，患者只需使用简单的手势或语音指令就能完成对设备的操作，例如通过手势调整床的高度，或通过语音控制房间的气温。对于行动不便或语言障碍的患者，系统还可以结合手势和眼动信息进行多模态识别，确保交互的准确性和流畅性。在娱乐领域中，自然交互将带来全新体验。玩家可以通过手势和语音与虚拟角色互动，利用眼动追踪控制视角，甚至通过情感识别系统让游戏根据情绪变化调整场景和难度。玩家还能通过语音和眼神“召唤”虚拟物体或改变天气，真正塑造和影响虚拟世界的动态变化。

% 实验室外的场景应用。目前大部分论文的研究，都局限于实验室或可控环境。只有很少一部分论文，探索了商场、街道等复杂场景的使用\ccite。要想让XR领域蓬勃发展，为更复杂场景设计相应的自然交互技巧必不可少。

% 打通应用之间的壁垒。现有研究为不同应用设计的交互模式各不相同，这就对用户上手带来了学习成本。如果XR上的各个应用操作，能够像现在的智能手机上的app操作一样，较为统一，这也就为应用的推广，带来推动作用。


%这是必要的将natural interaction techniques扩展到具体应用中




\subsection{\revise{}{Empowering Multimodal XR Natural Interaction with AI and LLMs}}  
\label{4.3}  

\revise{}{Recent advancements in AI and LLMs have significantly expanded the potential of natural interactions in XR environments. By enhancing input recognition, enabling semantic reasoning, and facilitating multimodal integration, these technologies improve the efficiency, intuitiveness, and usability of XR systems. This section discusses the roles of AI and LLMs, supported by recent research, and explores potential future directions for their application.}

\revise{}{\textbf{AI enhances input recognition and contextual understanding.}  AI plays a pivotal role in addressing challenges related to multimodal input recognition and resolving ambiguities in XR interactions. Machine learning models have significantly improved input accuracy by combining diverse data sources. For example, gaze direction and head orientation are integrated to predict user targets in complex scenes \cite{DBLP:conf/chi/WeiSYW0YL23}. Similarly, error-aware systems powered by deep learning can identify user mistakes or inconsistencies during interactions and adaptively switch to alternative modalities, such as voice commands or hand gestures \cite{DBLP:conf/uist/SendhilnathanZL22, DBLP:journals/tvcg/SidenmarkP0CGWG22}.  Beyond input recognition, AI enhances contextual understanding by integrating sensory data. For instance, fine-grained gesture recognition systems leveraging visual and audio sensors enable precise interactions even in occluded or noisy environments \cite{DBLP:conf/chi/XuZKN23}. In lifelogging applications, AI models can process egocentric video data to automatically segment and tag key events based on temporal and spatial patterns \cite{10484440}. These advancements increase the reliability and robustness of XR systems, making them more responsive and adaptable to user needs.} 

\revise{}{\textbf{LLMs enable semantic reasoning and natural language interaction.}  LLMs bring advanced semantic reasoning capabilities to XR systems, enabling more natural, intuitive, and flexible user interactions. Unlike traditional keyword-based approaches, LLMs can process open-ended user commands and resolve ambiguous references by incorporating contextual cues, such as gaze direction, gestures, or spatial information. For example, recent systems have demonstrated how LLMs can resolve pronoun ambiguities in speech by linking them to objects within the user’s field of view \cite{DBLP:journals/imwut/WangSWYYWJXY24, DBLP:conf/chi/0005WBCRF24}. Beyond disambiguation, LLMs empower non-technical users to modify virtual environments dynamically through natural language commands \cite{DBLP:conf/vr/GiunchiNGS24}. Additionally, they enhance interactivity by supporting intelligent virtual assistants that integrate multimodal inputs, such as speech and gaze, to deliver context-aware and reasoning-driven responses \cite{DBLP:conf/chi/WangYWJ024}. These capabilities make XR systems more intuitive, accessible, and adaptable to diverse user needs. }

\revise{}{\textbf{Future implications.} Looking forward, the integration of AI and LLMs presents transformative opportunities for developing adaptive, user-centric XR systems. A key area of future exploration involves leveraging AI to dynamically model user behavior and preferences, enabling systems to personalize interactions over time. For example, advancements in lifelogging technologies, such as egocentric vision models and wearable AR devices, facilitate the continuous capture and encoding of users’ daily experiences, including what they see, hear, and do \cite{DBLP:conf/chi/0005WBCRF24}. 
By combining AI for event detection with LLMs for semantic reasoning, future systems could enable effortless memory retrieval. Users could query their past experiences with questions such as, ``What did I read during yesterday’s meeting?" or ``Who did I talk to at lunch?” These capabilities align with the broader objective of creating more intuitive, human-centric, and context-aware XR systems. }


% \subsection{Empowering Multimodal XR Natural Interaction with AI and LLMs}
% \label{4.3}



% Recent advancements in AI and LLMs have significantly expanded the potential of XR natural interactions. By improving input recognition, enabling semantic understanding, and facilitating multimodal integration, these technologies enhance the efficiency and usability of XR systems. Below, we discuss the roles of AI and LLMs, supported by recent work, and explore future directions for their application.

% \textbf{AI enhances input recognition and contextual understanding.}
% AI plays a critical role in addressing challenges related to multimodal input recognition and ambiguous interactions in XR systems. Machine learning models, for instance, have improved gaze and gesture tracking accuracy by combining inputs like gaze direction and head orientation to predict user targets in complex scenes [58]. Similarly, error-aware systems powered by deep learning can detect user mistakes or inconsistencies during interactions and adaptively switch to fallback modalities such as voice commands or hand gestures [43, 21]. Beyond improving input recognition, AI enables systems to interpret and integrate context from various sensory inputs. For example, fine-grained gesture recognition systems using visual and audio sensors allow precise interactions even in occluded settings [100], while AI models in lifelogging applications can process egocentric video data to automatically segment and tag key events based on temporal and spatial patterns [16]. These AI-driven advancements enhance the reliability of XR systems.

% \textbf{LLMs enables semantic reasoning and natural language interaction.} 
% LLMs introduce powerful semantic reasoning capabilities to XR systems, enabling more natural and flexible user interactions. Unlike traditional keyword-based input, LLMs can process open-ended user commands and disambiguate vague references by integrating context from gaze, gestures, or spatial information. For instance, systems have demonstrated how LLMs can resolve pronoun ambiguities in speech by linking them to objects within the user’s field of view [93, 94]. Beyond this, LLMs also empower non-technical users to dynamically modify virtual environments through natural language commands [13] and enhance interactivity by supporting intelligent virtual assistants that integrate speech and gaze inputs to provide reasoning-based, context-aware responses [31]. These capabilities make XR systems more intuitive, accessible, and adaptable to diverse user needs.

% \textbf{Future implication.} Looking ahead, the fusion of AI and LLMs offers an opportunity to build adaptive and user-centric XR systems. One key area involves leveraging AI to dynamically model user behavior and preferences, allowing systems to personalize interactions over time. For example, recent advancements in lifelogging technologies, such as egocentric vision models and wearable AR devices, offer opportunities to continuously capture and encode users’ daily experiences, including what they see, hear, and do [16, 19]. By combining AI for event detection and LLMs for semantic reasoning, such systems could enable users to query their memories effortlessly, answering questions like “What did I read at the meeting yesterday?” or “Who did I talk to during lunch?” These capabilities align with the broader goal of creating more intuitive and human-centric XR systems.


% AI has become indispensable in addressing challenges associated with multimodal input recognition, particularly in ensuring accuracy and robustness across diverse scenarios. For instance, predictive AI models have been used to combine gaze and head pose data, improving the precision of target selection tasks in environments with dense or occluded objects [58]. Similarly, error-aware systems leverage deep learning to classify user errors during interactions, enabling adaptive fallback mechanisms such as switching from gaze to gesture-based control [43, 21]. These AI-driven strategies not only enhance reliability but also reduce user frustration by dynamically adjusting interaction modalities.

% AI also improves the fusion of multimodal inputs, enabling richer interaction paradigms. For example, gaze and hand gesture coordination has been widely explored, with systems employing gaze for primary selection and gestures for fine-grained manipulation [9, 55]. Beyond traditional modalities, emerging AI-powered methods now incorporate additional sensory inputs, such as audio or tactile data, to create more intuitive and immersive experiences [13]. Future research could focus on expanding these multimodal frameworks by integrating AI models capable of real-time user intent prediction, further streamlining interactions in complex XR environments.







%  Current interaction designs often rely on rigid, step-by-step procedures \cite{DBLP:conf/vr/YangQCSBLL24}. For instance, Wang \textit{et al.} divided interaction tasks into three stages: primary pointing, confirmation, and manipulation \cite{DBLP:journals/thms/WangWYL21}. These complex steps can increase the user's cognitive load, leading to operation errors \cite{DBLP:conf/uist/SendhilnathanZL22}. Moreover, these designs heavily depend on the tracking accuracy of both eye movements and gestures \cite{DBLP:conf/vr/SindhupathirajaUDH24, DBLP:conf/chi/SidenmarkCNLPG23}. Recent advancements in AI technologies have demonstrated the potential to enhance the usability of XR systems \cite{DBLP:conf/chi/LeeZAYGLKYDLSGZ24, DBLP:conf/ismar/LiLMHLS22}. For example, Wei \textit{et al.} studied the patterns of gaze direction and head orientation while fixating on objects, using machine learning to predict the user's target and improve selection accuracy \cite{DBLP:conf/chi/WeiSYW0YL23}. Additionally, recent progress in LLMs has opened new opportunities for XR interactions. 
% Trained on extensive text datasets, including materials such as books, articles, and online content, LLMs have proven capable of effectively understanding common-sense knowledge about human actions.
% For example, Hu \textit{et al.} developed a dwell-free eye typing system in XR, using LLMs to predict words based on letters the user’s gaze has passed over, improving error tolerance and increasing typing speed \cite{DBLP:conf/vr/HuDK24}. In the reviewed literature, we found that 7 papers published in 2024 employed LLMs as intelligent assistants to aid users in completing interactions, as mentioned in Section \ref{Appl.}. This highlights that integrating AI technologies into XR interactions has become a leading trend.

\subsection{Bridging Interaction Design and Practical XR Applications}
\label{4.4}


As discussed in Section \ref{Appl.}, nearly 70\% of the research focuses on the design of interaction techniques without specifying their application in concrete scenarios. While these techniques could potentially be adapted to various applications, they often require significant modifications for specific contexts. This gap presents challenges that limit the widespread adoption of natural interaction techniques in XR environments. Therefore, we argue that researchers should focus more on application scenarios when developing natural interaction techniques.

\textbf{Existing applications of natural interaction techniques.} Section \ref{Appl.} highlighted over 20 papers that explore natural interaction techniques in different application contexts, such as sketching\cite{DBLP:journals/imwut/ChenLYZ22, DBLP:journals/tvcg/SongDK23}, virtual meetings \cite{DBLP:conf/vr/SaintAubertAMPAL23, 10462901}, XR navigation\cite{DBLP:conf/vr/QuereMJWW24, DBLP:conf/chi/WangYWJ024}, reading \cite{DBLP:conf/ismar/LeeHM22, DBLP:conf/ismar/MengXL22}, and maintenance \cite{DBLP:conf/vr/QuereMJWW24}. These applications have significant potential for further expansion. For example, in sketching, combining brain-computer interfaces (BCIs) with eye-tracking technology could allow users to control the shape and color of design objects directly through thought, rather than relying solely on gestures or voice commands. In virtual meetings, real-time emotion recognition technology can capture participants' facial expressions, tone of voice, and heart rate to generate personalized avatars.

\textbf{Natural interaction techniques can also be applied to new domains.} Beyond the previously mentioned applications, in healthcare, these techniques offer more convenient care by integrating eye-tracking, gestures, and voice commands. For instance, patients wearing lightweight AR headsets can use eye-tracking to control devices like lighting or the TV, and adjust the bed angle with voice commands.
In entertainment, natural interaction transforms experiences by allowing players to interact with virtual characters through gestures and speech, while the game adjusts its environment or difficulty based on the player's emotions in real time.


\textbf{Application beyond the lab.} Currently, most studies on natural interaction techniques are confined to laboratory or controlled environments. Only a limited number of studies explore their use in more complex, real-world contexts such as shopping malls or city streets \cite{DBLP:conf/chi/0005WBCRF24, 10.1145/3613904.3642068}. To enable the broader adoption of XR technologies, it is crucial to design natural interaction techniques that can address the challenges posed by these complex, uncontrolled environments.

\textbf{Breaking down barriers between applications.} Existing research often develops interaction modalities that are specific to each application, which increases the learning curve for users. If interactions across different XR applications were standardized, similar to the unified operation model of smartphone apps, it would greatly reduce the learning curve for users. This standardization could, in turn, encourage wider adoption of these technologies.


% 1. spatial computing未来应用有哪些？

% 2. 如何实现更加自然的空间交互？



% 3. 如何实现更加准确的交互？  AI assistance.

% multimodal interaction交互引用？
% 真阳性，容错机制。

% 4. 计算资源分配。延迟很大。

% 5. 实验环境固定，运动中。很少在户外探索。

% 6. 新模态融合，vergence激活窗口，在不同深度平面完成更多交互。vergence + hand。

% 7. 和开发者相关。desktop VR/AR切换。简化开发者工作量。

% 8. VR和AR转换。虚实转换。

% 9. 空间计算。头戴式HMD和IOT结合起来，万物互联。




% 有的问题，已经在解决。
% 发现了问题，没解决。

% \subsubsection{Future Directions}

% 1. AI 辅助 XR交互，AI, LLMs
% 2. 

\section{Limitations and Future Work}
While this review of recent papers from top venues offers valuable insights into the latest trends in XR natural interaction techniques, there are several limitations that need to be addressed in future work.

Firstly, due to the vast amount of relevant research, we limited our scope to publications from six major venues between 2022 and 2024.
\revise{However, other venues, such as those within the ACM and IEEE digital libraries, also contain valuable research that was not included.}{However, other venues, such as those within the ACM and IEEE digital libraries, also contain valuable research, which was not included.} Additionally, extending the review to earlier years could provide a clearer understanding of the broader development trends in the field.

Furthermore, in this paper, we categorized the collected literature into four main categories: application context, operation types, performance measures, and interaction modalities. Future work could explore additional categories, such as study types or use cases \cite{DBLP:journals/tvcg/SpittlePC023}, to provide a more detailed understanding.

Finally, our review primarily focused on papers that explicitly mentioned natural interaction techniques for wearable XR. However, many studies on non-wearable natural interaction could potentially be adapted for wearable XR. 
\revise{For example, Ahmad Khan \textit{et al.} explored the synchronization between gaze and speech to implicitly link voice notes with digital text content \cite{10.1145/3491102.3502134}.}{For example, Ahmad Khan \textit{et al.} \cite{10.1145/3491102.3502134} explored the synchronization between gaze and speech to implicitly link voice notes with digital text content.} Although their study was conducted in a desktop environment, this approach could be applied to wearable XR systems as well.

% \section{Limitations and Future Work}
% 虽然review近些年来这些top venues的文章，已经提供了XR自然交互技巧的最新研究趋势，this review 仍然有一些limitations，需要未来去解决。
% 1）受限于海量的相关研究，我们只限定于2022-2024年六个主要venues上刊物。同时间的其他venues上也有许多非常有价值的research未被包含进来，例如ACM and IEEE digital libraries上的。追溯更早的年限，可能对整个领域的发展趋势，有个更清晰的脉络。
% 2）本文主要将收集到的文献，分为四个类别去阐述，分别是应用背景、操作类型、performance measures，交互模态。可以考虑更多的类别，如type of study, use case \ccite。
% 3）我们主要关注论文里显式地提到wearble XR的自然交互技巧。实际上，有许多研究虽然可能是非wearable的自然交互，但是可以潜在地迁移到wearable。例如，Ahmad Khan \textit{et al.} 探索了gaze and speech之间的同步性，来实现语音笔记与数字文本内容的隐式关联\ccite。虽然他们是在desktop环境，这也可以潜在地迁移到wearable XR。

\section{Conclusion}

In this paper, we review 104 research papers on natural interaction techniques for wearable XR, published between 2022 and 2024 in six top venues. We categorize this literature based on application context, operation types, performance measures, and interaction modalities. 
Specifically, we classify operation types into seven categories, distinguishing between active and passive interactions. Interaction modalities are further broken down into nine distinct types. In addition, we present statistical analyses of advanced natural interaction techniques. Building on these insights, we identify key challenges in natural interaction systems and suggest potential avenues for future research.
This review offers valuable insights for researchers aiming to design natural and efficient interaction systems for XR.

% \section*{Appendixes~(if needed)}

% \subsection*{Appendix A}

% \subsection*{Appendix B}

\bibliographystyle{fcs}
\bibliography{ref}

\begin{biography}{photo}
Please provide each author's biography here with no more than 120 words. The photo (600 dpi, smiling, energetic, front view) can be informal. Our journal prefers to exhibit an encouraging atmosphere. Please use one that best suits our journal.
\end{biography}

\end{document}
