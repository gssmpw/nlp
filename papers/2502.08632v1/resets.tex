We now turn to the \emph{RL with resets} access model, which is more permissive than episodic access. In this model, we give a reward-free RL algorithm $\PCR$ that only requires access to a \emph{one-context} regression oracle $\Reg$, and is oracle-efficient so long as $\Reg$ is sample-efficient.


%\dfc{We should add forward references to where $\PCR$ (and other algos) appear in the appendix so people can find them easily.}

\begin{theorem}[Special case of \cref{thm:pcr-app}]\label{cor:reset-rl-to-regression}
There is a constant $C_{\ref{cor:reset-rl-to-regression}}>0$ and an algorithm $\PCR$ (\cref{alg:pcr}\colt{ in \cref{sec:app_resets}}) so that the following holds. Fix $\Phi \subseteq (\MX\to\MS)$ and $\Nregc,\Creg \in \NN$. Let $\Reg$ be a $\Nreg$-efficient one-context regression oracle for $\Phi$ with $\Nreg(\epsilon,\delta) := \Nregc/(\epsilon\delta)^{\Creg}$.
Then $\PCR(\Reg,\Nreg,|\MS|,\cdot)$ %with $N:=\Nreg\left(\left(\frac{\epsilon\delta}{H|\MA||\MS|}\right)^{C_{\ref{cor:online-rl-to-regression}}},\left(\frac{\epsilon\delta}{H|\MA||\MS|}\right)^{C_{\ref{cor:online-rl-to-regression}}}\right)$ 
is an $(\Nrl,\Krl)$-efficient reward-free RL algorithm for $\Phi$ in the \emph{reset access model}, with $\Krl(\epsilon,\delta,H,|\MA|) \leq H^2|\MS|^2$ and $\Nrl(\epsilon,\delta,H,|\MA|) \leq \Nregc \cdot \left(H|\MA||\MS|/(\epsilon\delta)\right)^{C_{\ref{cor:reset-rl-to-regression}}\Creg}.$
Moreover, the oracle time complexity of $\PCR$ is at most $\Nregc \cdot \left(H|\MA||\MS|/(\epsilon\delta)\right)^{C_{\ref{cor:reset-rl-to-regression}}\Creg}.$
\end{theorem}

\arxiv{\input{alg_pcr}}

Since RL in the \emph{episodic} access model is provably harder than one-context regression \citep{golowich2024exploration}, \cref{cor:reset-rl-to-regression} gives a provable computational benefit of reset access---to our knowledge, the first of its kind, though statistical benefits are known in different settings (\cref{sec:related-alg-block}).%\footnote{We remark that this is not implied by \cite{mhammedi2024power}, since their algorithm $\RVFS$ requires an agnostic and cost-sensitive regression oracle.}
%\dfc{should emphasize here: this gives a computational separation btw rl w/ resets and fully online -- first separation of this type? (remind again why \citet{mhammedi2024power} doesn't address this)}

\paragraph{Overview of algorithm design and proof.}
The $\PCR$ algorithm follows a similar blueprint to $\PCO$ (and $\HOMER$): given policy covers for the first $h$ layers, design internal reward functions for layer $h+1$ based on kinematics, and use $\PSDP$ to optimize these reward functions and obtain a policy cover for layer $h+1$ (recall that $\PSDP$ itself only requires a one-context regression oracle). The point of departure is in how the kinematics are estimated. In $\PCO$, two-context regression is applied on a dataset consisting of some pairs $(x_h^{(i)},x_{h+1}^{(i)})$ generated using the MDP's real transitions (and labeled $y^{(i)} = 1$), and some pairs $(x_h^{(j)}, x_{h+1}^{(j)})$ generated using ``fake'' transitions (and labeled $y^{(j)} = 0$). The Bayes optimal predictor $\EE[y\mid{}x_1,x_2]$ turns out to be exactly the kinematics function from \cref{eq:kinematics-intro}.

In the setting of \cref{cor:reset-rl-to-regression}, our algorithm $\PCR$ does not have access to two-context regression, but it does have the ability to ``reset'' the MDP to previously-seen observations. To use this, the algorithm first generates a large number of ``discriminator'' observations $(x_h^{(i)})_{i=1}^m$. For each fixed $i \in [m]$ and $a \in \MA$, the algorithm then learns to predict the following predicate: ``did the observation $x_{h+1}$ come from $x_h^{(i)}$ and action $a$, or from some other $x_h^{(j)}$ or other action?''. More precisely, using the power of resets, $\PCR$ can generate a large number of samples $(x,y)$ where $x \sim \BP_{h+1}(\cdot\mid{}x_h^{(j)},a_h)$ for $(j,a_h) \sim \Unif([m]\times\MA)$, and $y := \mathbbm{1}[j=i \land a_h=a]$. The Bayes optimal predictor $\EE[y\mid{}x]$ is then:\loose
\begin{equation} w_{h+1}(x_{h+1};i,a) := \frac{\BP_{h+1}(x_{h+1}\mid{}x_h^{(i)},a)}{\sum_{j,a_h} \BP_{h+1}(x_{h+1}\mid{}x_h^{(j)},a_h)} = \frac{\til\BP_{h+1}(\phi^\st(x_{h+1})\mid{}\phi^\st(x_h^{(i)}),a)}{\sum_{j,a_h} \til\BP_{h+1}(\phi^\st(x_{h+1})\mid{}\phi^\st(x_h^{(j)}),a_h)}.\label{eq:pcr-kinematics}\end{equation}
This function can be estimated via one-context regression, and then used to define internal reward functions at layer $h+1$, similar to $\PCO$. See \cref{sec:app_resets} for the formal \colt{algorithm and }analysis. %So long as the discriminator observations $(x_h^{(i)})_{i=1}^m$ cover all reachable latent states at layer $h$ (which occurs with high probability, since they are drawn from a policy cover for layer $h$), the observations at layer $h+1$ with similar estimated kinematics are ``effectively'' from the same latent state, and the remainder of the argument follows along similar lines.

\paragraph{Is one-context regression minimal?} %A natural question in light of \cref{cor:reset-rl-to-regression} is whether one-context regression is minimal for RL with resets, in the sense that it is necessary in addition to being sufficient.
By a variant of \cite[Proposition B.2]{golowich2024exploration}, the \emph{noiseless} version of one-context regression is necessary in the reset setting (\cref{prop:noiseless-onered}). While there are classical examples where noisy (but realizable) PAC learning is believed to be computationally harder than noiseless learning \citep{blum2003noise}, in many natural settings they are comparable---see e.g. halfspace learning \citep{blum1998polynomial,diakonikolas2023strongly} and the statistical query model \citep{kearns1998efficient}. In this sense, we expect that (noisy) one-context regression is \emph{nearly} minimal.\loose





\iffalse
\begin{theorem}\label{cor:reset-rl-to-regression}
There is a constant $C_{\ref{cor:reset-rl-to-regression}}>0$ so that the following holds. Suppose that $\Reg$ is an $\Nreg$-efficient one-context regression algorithm for $\Phi$. Then $\PCR(\Reg,\cdot,\cdot,N)$ with $N:=\Nreg\left(\left(\frac{\epsilon\delta}{H|\MA||\MS|}\right)^{C_{\ref{cor:reset-rl-to-regression}}},\left(\frac{\epsilon\delta}{H|\MA||\MS|}\right)^{C_{\ref{cor:reset-rl-to-regression}}}\right)$ is an $(\Nrl,\Krl)$-efficient strong reward-free RL-with-resets algorithm for $\Phi$ with:
\begin{itemize}
    \item $\Krl(\epsilon,\delta,H,|\MA|) \leq H^2|\MS|^2$
    \item $\Nrl(\epsilon,\delta,H,|\MA|) \leq \left(\frac{H|\MA||\MS|}{\epsilon\delta}\right)^{C_{\ref{cor:reset-rl-to-regression}}} \Nreg\left(\left(\frac{\epsilon\delta}{H|\MA||\MS|}\right)^{C_{\ref{cor:reset-rl-to-regression}}},\left(\frac{\epsilon\delta}{H|\MA||\MS|}\right)^{C_{\ref{cor:reset-rl-to-regression}}}\right)$.
\end{itemize}
Moreover, the time complexity of $\PCR$ is $\poly(N,H,|\MS|,|\MA|,\epsilon^{-1},\delta^{-1})$.
\end{theorem}
\fi
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
