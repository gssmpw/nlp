This work takes a step towards understanding the minimal computational oracles needed for reinforcement learning; however, much remains unclear, both for the specific tasks discussed in this paper, and beyond. Below, we discuss some particularly notable questions, ordered roughly from narrowest to broadest.

\paragraph{Pinning down a minimal oracle in the reset access model?} We showed in \cref{cor:reset-rl-to-regression} that one-context regression is sufficient for reward-free RL in Block MDPs with reset access. It's known to be necessary for reward-free RL in Block MDPs with \emph{episodic} access \citep{golowich2024exploration}, but for the reset access model we only show that noiseless one-context regression is necessary (\cref{prop:noiseless-onered}). Likely, the same argument shows that \emph{one-context regression with conditional sampling queries} is necessary (which slightly strengthens noiseless one-context regression), but even this oracle does not seem sufficient for learning inverse dynamics.

\paragraph{Reward-free vs reward-directed?} Our work focuses on reward-free RL, and in particular we mostly focus on the strongest formulation: computing a $(1,\epsilon)$-policy cover. For Block MDPs, relaxing to $(\alpha,\epsilon)$-policy covers or reward-directed RL does not appear to lead to simpler algorithms, but there are also technical obstacles to extending our results on necessity of two-context regression (e.g. \cref{cor:regression-to-online-rl}) to these settings. Most notably, while computing a $(1,\epsilon)$-policy cover can be hard for even $H = 2$, hardness for these weaker tasks only arises when $H = \omega(1)$, and it's unclear how to use two-context regression data to usefully simulate interaction with an MDP with longer horizon. Existing hardness results that exploit long horizon include \cref{thm:halfspace-main} and the main result of \cite{golowich2024exploration}, but both require specific concept classes with specific structure, whereas \cref{cor:regression-to-online-rl} applies to almost any concept class.

This technical difficulty leaves an open question: are there substantive computational differences between these tasks? This question may be of conceptual interest since $(1,\epsilon)$-policy covers are not meaningful beyond Block MDPs; a better understanding of weaker notions of exploration seems essential for more general characterizations.

\paragraph{Stronger hardness for Low-Rank MDPs?} In \cref{thm:halfspace-main}, we show that one-context low-rank regression is insufficient for Low-Rank MDPs in the reset access model, which indicates a qualitative computational difference with Block MDPs. However, this leaves several questions:
\begin{itemize} 
\item Is \emph{agnostic} one-context regression necessary? The source of computational hardness in \cref{thm:halfspace-main} is the same as the source for agnostic halfspace learning \citep{tiegel2023hardness}, but the result does not constitute a general-purpose reduction.

\item Is two-context low-rank regression insufficient for Low-Rank MDPs in the episodic access model? In a sense, \cref{thm:halfspace-main} gives weak positive evidence: the RL hardness result still holds in the episodic access model, and two-context regression for the concept class of halfspaces is likely reducible to PAC learning an intersection of two halfspaces (with random classification noise). So if two-context regression were sufficient, then it would likely imply cryptographic hardness of learning an intersection of two halfspaces under LWE, which currently seems out of reach \citep{tiegel2024improved}.


\item More broadly, is there \emph{any} oracle-efficient algorithm for RL in Low-Rank MDPs in the episodic access model that only uses a ``minimization'' oracle? In stark contrast with Block MDPs, the only existing algorithm for RL in Low-Rank MDPs requires min-max optimization, and it would be interesting to understand if there is an inherent computational barrier.
\end{itemize}


\paragraph{A computational taxonomy for RL?} Perhaps the most interesting question is whether our methods can be extended to develop a theory of computational tractability in more general interactive decision-making problems (i.e. beyond Block MDPs and Low-Rank MDPs), in analogy with the theory of statistical tractability developed by \cite{foster2021statistical}. As we discussed in \cref{sec:lowrank}, we view \emph{regression oracles for weight functions} as one promising direction, but a satisfying answer to this question could require developing new oracles and modeling assumptions.

\iffalse \dhruv{ 
\begin{itemize}
\item $(1,\epsilon)$-policy cover vs $(\alpha,\epsilon)$
\item Pinning down minimal oracle for block MDPs with reset access
\item Open question: stronger hardness in low-rank (is agnostic sufficient? is min/max necessary)
\item Open question: general theory of computational tractability based on weight functions
\item reward-free vs reward-based
\end{itemize}

}\fi
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
