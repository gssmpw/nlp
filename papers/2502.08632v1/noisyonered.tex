In recent work, \cite{golowich2024exploration} showed that one-context regression is necessary for reward-directed episodic RL. Here we adapt their argument to reward-free RL; the modification is straightforward under regularity (using the ``extra'' states $\{0,1\}$).

Concretely, the following theorem shows that $\OneRed$ (\cref{alg:onered}) reduces one-context regression for concept class $\Phi$ to reward-free episodic RL for concept class $\Phiaug$. We use this reduction as a component of our reduction from \emph{two}-context regression to reward-free RL (see \cref{sec:app_minimality}).

\begin{algorithm}[t]
	\caption{$\OneRed(\MO, (x^{(i)},y^{(i)})_{i=1}^n,\epsilon,\delta)$: Reduction from one-context regression to reward-free episodic RL}
	\label{alg:onered}
	\begin{algorithmic}[1]\onehalfspacing
		          \State \textbf{input:} Oracle $\MO$ for reward-free episodic RL; samples $(x^{(i)},y^{(i)})_{i=1}^n$; tolerances $\epsilon,\delta$.
		\State Set $\epa := \sqrt{\epsilon/4}$ and $i = 1$. Initialize $\MO$ with tolerance $\epsilon/4$, failure probability $\delta/2$, horizon $H := 2$, and action set $\MA := \{0,\epa,\dots,1-\epa\}$. Simulate $\MO$ as follows:
        \Repeat
            \State When $\MO$ queries for a new episode, pass $x^{(i)}$.
            \State When $\MO$ plays an action $a_1 \in \MA$,  pass observation $0$ with probability $(a-y^{(i)})^2$. Otherwise, pass observation $1$. In either case, set $i \gets i+1$.
        \Until{$\MO$ returns policy cover $\Psi$}
        \State $m \gets 16\epsilon^{-2}\log(4|\Psi|/\delta)$.
        \For{$\pi \in \Psi$}
            \State Compute $\wh E(\pi) := \frac{1}{m}\sum_{i=n-m+1}^n (\pi(x^{(i)}) - y^{(i)})^2$.\label{line:one-red-emp}
        \EndFor
        \State \textbf{return:} $\pihat := \argmin_{\pi \in \Psi}  \wh E(\pi)$.
	\end{algorithmic}
\end{algorithm}


\begin{proposition}[Modification of {\cite[Proposition B.2]{golowich2024exploration}}]\label{prop:onered}
Suppose that $\MO$ is a $(\Nrl,\Krl)$-efficient reward-free episodic RL oracle for $\Phiaug$. Then $\OneRed(\MO,\cdot)$ is a $\Nreg$-efficient one-context regression algorithm for $\Phi$ with
\[\Nreg(\epsilon,\delta) = \Nrl\left(\frac{\epsilon}{4}, \frac{\delta}{2}, 2, \sqrt{\frac{4}{\epsilon}}\right) + \frac{16\log(4\Krl\left(\frac{\epsilon}{4}, \frac{\delta}{2}, 2, \sqrt{\frac{4}{\epsilon}}\right)/\delta)}{\epsilon^2}.\]
\end{proposition}

\begin{proof}
Let $\epsilon,\delta>0$, $\MD \in \Delta(\MX)$, and $f: \MS \to \{0,1\}$. Let $n \geq \Nreg(\epsilon,\delta)$. Let $(x^{(i)},y^{(i)})_{i=1}^n$ be i.i.d. samples with $x^{(i)} \sim \MD$, $y^{(i)} \in \{0,1\}$, and $\EE[y^{(i)}\mid{}x^{(i)}] = f(\phi(x^{(i)}))$ for some $\phi \in \Phi$. We analyze the execution of $\OneRed(\MO,(x^{(i)},y^{(i)})_{i=1}^n, \epsilon,\delta)$. We know that $n-m \geq \Nrl(\epsilon/4,\delta/2,2,\sqrt{4/\epsilon})$, since $n \geq \Nreg(\epsilon,\delta)$. Fix any episode $i$ of interaction with the oracle $\MO$. Observe that conditioned on the initial observation $x^{(i)}$ and action $a_1$,
\begin{align*}
\EE[(a_1-y^{(i)})^2\mid{} x^{(i)}, a_1]
&= (a_1)^2 - 2 a_1 \EE[y^{(i)}\mid{} x^{(i)}] + \EE[(y^{(i)})^2 \mid{} x^{(i)}] \\ 
&= (a_1)^2 + (1 - 2a_1) f(\phi(x^{(i)}))
\end{align*}
where the final equality uses that $y^{(i)} \in \{0,1\}$. Thus, $\OneRed$ simulates $\MO$ on a $\Phiaug$-decodable block MDP $M$ with horizon $2$, observation space $\Xaug$, latent state space $\Saug$, initial observation distribution $\MD$, and transition distribution defined by
\[\BP_2(0\mid{} x_1,a_1) := a_1^2 + (1-2a_1)f(\phi(x_1)),\]
\[\BP_2(1\mid{} x_1,a_1) := 1 - \BP_2(0\mid{} x_1,a_1).\]
By \cref{def:strong-rf-rl}, the output of $\MO$ is a set of policies $\Psi$ of size at most $\Krl(\epsilon/4,\delta/2,2,\sqrt{4/\epsilon})$, such that with probability at least $1-\delta/2$, there is some $\pi^\st \in \Psi$ such that
\[d^{M,\pi^\st}_2(1) \geq \max_{\pi\in\Pi} d^{M,\pi}_2(1) - \frac{\epsilon}{4}.\]
Condition on this event, and observe that for any $\pi \in \Pi$,
\begin{align*}
d^{M,\pi}_2(0) 
&= \E^{M,\pi}[a_1^2 + (1-2a_1)f(\phi(x_1))] \\ 
&= \EE_{x \sim \MD}[(\pi(x) - f(\phi(x)))^2 + f(\phi(x)) - f(\phi(x))^2] \\ 
&= \EE_{x,y}[(\pi(x) - y)^2] + Z
\end{align*}
where $Z := \EE_{x,y}[f(\phi(x)) - f(\phi(x))^2 - (f(\phi(x))-y)^2]$, and the expectations are over a fresh sample $(x,y)$ from the same distribution as $(x^{(i)},y^{(i)})$. But now by Hoeffding's inequality, the union bound, and choice of $m := 16\epsilon^{-2}\log(4|\Psi|/\delta)$, we have with probability at least $1-\delta/2$ that for all $\pi\in\Psi$,
\[\left|\wh E(\pi) - \EE_{x,y}[(\pi(x)-y)^2]\right| \leq \epsilon/4,\]
where $\wh E(\pi)$ is the empirical loss for $\pi$ computed in \cref{line:one-red-emp} of $\OneRed$. In this event, we get that 
\begin{align*}
\EE_{x,y}[(\pihat(x)-y)^2]
&\leq \frac{\epsilon}{2} + \EE_{x,y}[(\pi^\st(x) - y)^2] \\ 
&= \frac{\epsilon}{2} + 1-d^{M,\pi^\st}_2(1) - Z \\ 
&\leq \frac{\epsilon}{2} + 1 - \max_{\pi\in\Pi} d^{M,\pi}_2(1) - Z \\ 
&\leq \frac{3\epsilon}{4} + \min_{\pi\in\Pi} \EE_{x,y}[(\pi(x)-y)^2].
\end{align*}
It follows that
\[\EE_x[(\pihat(x) - f(\phi(x)))^2] \leq \frac{3\epsilon}{4} + \min_{\pi\in\Pi} \EE_x[(\pi(x)-f(\phi(x)))^2] \leq \epsilon\]
since the policy $\pi(x) = \sqrt{4/\epsilon} \lfloor f(\phi(x) \cdot \sqrt{4/\epsilon}\rfloor$ has squared error at most $\epsilon/4$.
\end{proof}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
