\nc{\Nregc}{\Nreg^\circ}
\nc{\Creg}{C_{\mathsf{reg}}}

\nc{\Nrlc}{\Nrl^\circ}
\nc{\Krlc}{\Krl^\circ}
\nc{\Crl}{C_{\mathsf{RL}}}
\nc{\RegToRL}{\mathtt{RegToRL}}

In this section we show that two-context regression is a minimal oracle for reward-free episodic RL in Block MDPs. First we show that it is \emph{sufficient} (\cref{sec:episodic-suff}), then that it is \emph{necessary} (\cref{sec:episodic-nec}).

\subsection{Sufficiency: Reducing Episodic RL to Two-Context Regression}\label{sec:episodic-suff}

Our first result gives a computational reduction from reward-free RL (\cref{def:strong-rf-rl}) in a $\Phi$-decodable block MDP (with the episodic access model) to two-context regression for $\Phi$ (\cref{def:two-con-regression}). More precisely, we give a reward-free RL algorithm that requires access to a two-context regression oracle $\Reg$ (as well as episodic access to an MDP), and is oracle-efficient so long as $\Reg$ is sample-efficient. %\dfc{add forward reference to $\PCO$ in the appendix}

\begin{restatable}[Special case of \cref{thm:pco-app}]{theorem}{rltoregression}\label{cor:online-rl-to-regression}
There is a constant $C_{\ref{cor:online-rl-to-regression}}>0$ and an algorithm $\PCO$ (\cref{alg:pco}\colt{ in \cref{sec:app_online}}) so that the following holds. Let $\Phi \subseteq (\MX\to\MS)$ be any concept class, let $\Nregc,\Creg \in \NN$\arxiv{ be parameters}, and let $\Reg$ be a $\Nreg$-efficient two-context regression oracle for $\Phi$ with $\Nreg(\epsilon,\delta) := \Nregc/(\epsilon\delta)^{\Creg}$. Then $\PCO(\Reg,\Nreg,|\MS|,\cdot)$ %with $N:=\Nreg\left(\left(\frac{\epsilon\delta}{H|\MA||\MS|}\right)^{C_{\ref{cor:online-rl-to-regression}}},\left(\frac{\epsilon\delta}{H|\MA||\MS|}\right)^{C_{\ref{cor:online-rl-to-regression}}}\right)$ 
is an $(\Nrl,\Krl)$-efficient reward-free RL algorithm for $\Phi$ in the \emph{episodic access model}, with $\Krl(\epsilon,\delta,H,|\MA|) \leq H^2|\MS|^2$ and $\Nrl(\epsilon,\delta,H,|\MA|) \leq \Nregc \cdot \left(\frac{H|\MA||\MS|}{\epsilon\delta}\right)^{C_{\ref{cor:online-rl-to-regression}}\Creg}.$
\iffalse
\begin{itemize}
    \item $\Krl(\epsilon,\delta,H,|\MA|) \leq H^2|\MS|^2$
    \item $\Nrl(\epsilon,\delta,H,|\MA|) \leq \left(\frac{H|\MA||\MS|}{\epsilon\delta}\right)^{C_{\ref{cor:online-rl-to-regression}}} \Nreg\left(\left(\frac{\epsilon\delta}{H|\MA||\MS|}\right)^{C_{\ref{cor:online-rl-to-regression}}},\left(\frac{\epsilon\delta}{H|\MA||\MS|}\right)^{C_{\ref{cor:online-rl-to-regression}}}\right)$.
\end{itemize}
\fi
Moreover, the oracle time complexity of $\PCO$ is at most $\Nregc \cdot \left(\frac{H|\MA||\MS|}{\epsilon\delta}\right)^{C_{\ref{cor:online-rl-to-regression}}\Creg}.$
\end{restatable}

\arxiv{\input{alg_pco}}

While \cref{cor:online-rl-to-regression} assumes a natural parametric scaling for $\Nreg$, the full result (\cref{thm:pco-app}) applies to any efficiency function. Note that $\Nregc,\Creg$ will naturally be larger for more complex concept classes $\Phi$, but there are no ``hidden'' dependencies on $\Phi$. Informally, \cref{cor:online-rl-to-regression} shows that two-context regression is a \emph{sufficient} oracle for reward-free episodic RL in block MDPs.

\paragraph{Proof overview.} The main subroutine of $\PCO$ is $\EPCO$, which strongly resembles the $\HOMER$ algorithm \citep{misra2020kinematic}.\footnote{Specifically, the ``non-quantized'' version of $\HOMER$ described in Appendix~E of \cite{misra2020kinematic}.} The basic idea of $\HOMER$ (and many other oracle-efficient RL algorithms \colt{\citep{du2019provably,mhammedi2023efficient}}\arxiv{\citep{du2019provably,mhammedi2023efficient,golowich2024exploring}}) is to iteratively learn policy covers $\Psi_{1:H}$ for each layer of the MDP. In $\HOMER$, given policy covers for layers $1,\dots,h$, a policy cover for layer $h+1$ is learned by applying the policy optimization method $\PSDP$ \citep{bagnell2003policy} to a set of carefully-designed internal reward functions at layer $h+1$. Ideally, the reward functions should incentivize reaching individual latent states; of course, latent states are not in general identifiable, so this criterion must be relaxed. Instead, the rewards are constructed in two steps. First, use two-context regression (with an appropriately-generated dataset, inspired by contrastive learning methods) to estimate the following \emph{kinematics function}:
\begin{equation} f_{h+1}(x_h,x_{h+1};a_h) := \frac{\BP_{h+1}(x_{h+1}\mid{}x_h,a_h)}{\BP_{h+1}(x_{h+1}\mid{}x_h,a_h) + F_{h+1}(x_{h+1})},\label{eq:kinematics-intro}\end{equation}
where $F_{h+1}$ is a certain normalization function. Second, sample a large number of ``cluster center'' observations $(\xbar_{h+1}^{(t)})$, %\footnote{Technically, it is important to ``de-duplicate'' the cluster centers by filtering out centers that are kinematically similar to previous centers.} 
and, for each, define a reward $\MR^{(t)}$ (derived from \cref{eq:kinematics-intro}) which is large precisely for those observations $x_{h+1}$ that have approximately the same kinematics as $\xbar^{(t)}_{h+1}$.

$\PCO$ follows the same blueprint, with two modifications. First, $\HOMER$ uses an offline cost-sensitive classification oracle for $\PSDP$. We use an alternative implementation of $\PSDP$ \citep{mhammedi2023representation} which can be implemented with one-context regression (\cref{lemma:psdp-trunc-online}) and hence two-context regression (via \cref{prop:onetwo}). Second, the analysis of $\HOMER$ assumes that all states are reachable with non-negligible probability. We remove this assumption via truncation arguments and an iterative discovery method \citep{golowich2024exploring}---this is the reason for the outer loop in $\PCO$. See \cref{sec:app_online} for the formal \colt{algorithm and }analysis.

\iffalse 
and ``discriminator'' observation/action pairs $(x_h^{(i)},a_h^{(i)})$. Third, for each cluster center $\xbar^{(t)}_{h+1}$, define a reward function $\MR^{(t)}$ which is large precisely for those observations $x_{h+1}$ which have roughly the same kinematics as $x^{(t)}_{h+1}$, with respect to the discriminators---i.e.,
\[f_{h+1}(x_h^{(i)}, x_{h+1}; a_h^{(i)}) \approx f_{h+1}(x_h^{(i)}, \xbar_{h+1}^{(t)}; a_h^{(i)}) \qquad \forall i.\]
This method works because (1) any two observations $x_{h+1},x_{h+1}'$ from the same latent state necessarily share the same kinematics, and (2) while observations with the same kinematics may not share the same latent state, they are the same for all intents and purposes (at least, up until layer $h+1$): in particular, the visitation probabilities $d^{M,\pi}_{h+1}(x_{h+1})$ and $d^{M,\pi}_{h+1}(x_{h+1}')$ differ by a constant factor independent of the policy $\pi$, and hence optimizing the sum of visitation probabilities $d^{M,\pi}_{h+1}(x_{h+1}) + d^{M,\pi}_{h+1}(x_{h+1}')$ is equivalent to optimizing either individual visitation probability.

To prove \cref{cor:online-rl-to-regression}, we streamline and generalize $\HOMER$ in two ways:
\begin{enumerate}
\item First, the $\PSDP$ subroutine in $\HOMER$ uses an offline cost-sensitive classification oracle, which it is unclear how to directly implement using two-context regression. However, there is an alternative implementation of $\PSDP$ due to \cite{mhammedi2023representation} that uses squared-loss regression. This can be provably implemented with one-context regression (\cref{lemma:psdp-trunc-online}), which can be reduced to two-context regression (\cref{prop:onetwo}).

\item Second, more substantively, the analysis of $\HOMER$ requires a \emph{reachability assumption}. Often, this is an artifact of the analysis and can be avoided by analyzing \emph{truncated MDPs/policies} \citep{golowich2022learning,mhammedi2023efficient}, which essentially avoid issues of compounding errors on hard-to-reach states by truncating away such states. This approach would work if the reward functions $\MR^{(t)}$ were accurate on all states. However, in $\HOMER$, the reward functions are \emph{learned}, so they could be inaccurate on hard-to-reach states. This means that the policies computed by $\PSDP$ could obtain erroneously high reward without actually being optimal for the ``ideal'' reward functions.

We fix this by \emph{iterating} the entire algorithm, an approach previously used by \cite{golowich2024exploring}: the key idea is that the above pathology only occurs if one of the policies computed by $\PSDP$ ``discovers'' a state that we previously were unable to cover. Thus, we can rerun the entire algorithm with these policies mixed into all data collection procedures. By a win/win argument, eventually we will stop discovering new states, in which case the prior analysis will show that we have constructed a set of policy covers.
\end{enumerate}

\fi 


\subsection{Necessity: Reducing Two-Context Regression to Episodic RL}\label{sec:episodic-nec}

Our second result provides a converse of \cref{cor:online-rl-to-regression}. We give a two-context regression algorithm that requires access to a reward-free episodic RL \emph{oracle} (\cref{def:strong-rf-rl}), and is oracle-efficient so long as the oracle is sample-efficient.\loose

\begin{restatable}{theorem}{regtorl}\label{cor:regression-to-online-rl}
There is a constant $C_{\ref{cor:regression-to-online-rl}}>0$ and an algorithm $\RegToRL$ (\cref{alg:regtorl} in \cref{sec:app_minimality}) so that the following holds. Let $\Phiaug \subseteq (\Xaug\to\Saug)$ be any regular concept class (\cref{def:regular}), let $\Nrlc,\Crl \in \NN$\arxiv{ be parameters}, and let $\MO$ be a $(\Nrl,\Krl)$-efficient reward-free episodic RL oracle for $\Phiaug$, with $\max(\Nrl(\epsilon,\delta,H,A),\Krl(\epsilon,\delta,H,A)) \leq \Nrlc \cdot (AH/\epsilon\delta)^{\Crl}$. Then $\RegToRL(\MO,\cdot)$ is an $\Nreg$-efficient two-context regression algorithm (\cref{def:two-con-regression}) for $\Phiaug$ with 
$\Nreg(\epsilon,\delta) \leq \Nrlc \left(|\MS|/(\epsilon\delta)\right)^{C_{\ref{cor:regression-to-online-rl}} \cdot \Crl}$ and with oracle time complexity at most $\Nrlc \left(|\MS|/(\epsilon\delta)\right)^{C_{\ref{cor:regression-to-online-rl}} \cdot \Crl}$.
\end{restatable}

\cref{cor:online-rl-to-regression} and \cref{cor:regression-to-online-rl} together show that two-context regression is a minimal oracle for reward-free episodic RL (for any regular $\Phi$). \cref{cor:regression-to-online-rl} strengthens \cite[Proposition B.2]{golowich2024exploration}, which reduces \emph{one}-context regression to RL---to our knowledge, the only prior result of this flavor. We require regularity\footnote{ \cite{golowich2024exploration} reduce to reward-directed RL and do not use regularity; however, under regularity it is simple to adapt their reduction to reward-free RL; this adaptation is the version we sketch below.} for the following technical reason: a concept class is regular if and only if it can be obtained by augmenting some base concept class $\Phi \subseteq (\MX\to\MS)$ with two fully observed states $\{0,1\}$ (see \cref{def:phiaug} for the formal definition). It is easy to reduce regression with $\Phiaug$ to regression with $\Phi$ (\cref{prop:twoaug}), so it suffices to reduce regression with $\Phi$ to reward-free RL with $\Phiaug$; the extra states provide useful flexibility since $\Phi$ is otherwise arbitrary.

We now sketch the prior reduction before discussing how to strengthen it.

\paragraph{Recap: Reducing one-context regression to RL.} Given a one-context regression dataset $(x^{(i)},y^{(i)})_{i=1}^n$ with samples in $\MX\times\{0,1\}$, consider simulating an MDP with horizon $H=2$, where the initial observation lies in $\MX$ and the second observation lies in $\{0,1\}$. The goal of this reduction is that a \emph{policy} that visits state $1$ at step $2$ with near-optimal probability corresponds to an accurate \emph{prediction function} for the regression. This can be achieved by defining the action space $\MA$ to be a discretization of the interval $[0,1]$, and requiring the policy to ``guess'' $y^{(i)}$ after observing $x^{(i)}$. 

More formally, the reduction uses a fresh datapoint $(x^{(i)},y^{(i)})$ to simulate each new episode. It first passes observation $x_1 := x^{(i)} \in \MX$ to the RL agent. When the agent plays an action $a_1 \in \MA \subset [0,1]$, the reduction passes the observation $x_2 \in \{0,1\}$ sampled from $\Ber(1 - (a - y^{(i)})^2).$ It can be checked that this procedure in fact simulates a $\Phi$-decodable MDP, so long as the regression dataset satisfied the desideratum that $\EE[y^{(i)}\mid{} x^{(i)}]$ only depends on $\phi(x^{(i)})$ for some $\phi \in \Phi$.

\paragraph{Reducing two-context regression to RL.} Can we generalize the above construction by increasing the horizon? Given a two-context regression sample $(x_1^{(i)}, x_2^{(i)},y^{(i)})$, consider passing both contexts to the RL agent one-by-one and then requiring the policy to ``guess'' $y^{(i)}$. Unfortunately, $y^{(i)}$ may depend on both $x_1^{(i)}$ and $x_2^{(i)}$, so the simulated decision process is non-Markovian. More broadly, this points to a representational obstacle: optimal policies for a Block MDP are Markovian and hence mappings $\MX \to \MA$. But for two-context regression, a predictor is a function on $\MX \times \MX$. Thus, any successful reduction will have to ``stitch together'' \emph{multiple} policies produced by the RL oracle.

To motivate our reduction, we recall why two-context regression was useful for RL in the first place: essentially, it was useful to estimate (some transformation of) transition probabilities between two consecutive states---see \cref{eq:kinematics-intro}. This suggests using the regression data to simulate an MDP where \emph{the probability of transitioning from $x_1^{(i)}$ to $x_2^{(i)}$ depends on $y^{(i)}$}.

Formally, our reduction simulates a horizon-$2$ MDP with first observation in $\MX$, second observation in $\MX\sqcup\{0\}$, and action space $\MA \subset [0,1]$. For each sample $(x_1^{(i)},x_2^{(i)},y^{(i)})$, the reduction simulates an episode of interaction with the MDP as follows:\arxiv{
\begin{enumerate}
\item First, the reduction passes observation $x_1 := x_1^{(i)} \in \MX$ to the RL agent.
\item Second, when the agent plays an action $a_1 \in \MA \subset [0,1]$, the reduction passes the observation $x_2 \in \MX$ sampled as follows: 
\[x_2 := \begin{cases} 
x_2^{(i)} & \text{ with probability } 1 - (a_1 - y^{(i)})^2 \\ 
0 & \text{ with probability } (a_1 - y^{(i)})^2 
\end{cases}.\]
\end{enumerate}
}\colt{ 
first, pass $x_1 := x_1^{(i)} \in \MX$ to the RL oracle. Second, when the oracle plays an action $a_1 \in \MA \subset [0,1]$, pass $x_2 \in \MX\sqcup\{0\}$ sampled as: 
\[x_2 := \begin{cases} x_2^{(i)} & \text{ with probability } 1 - (a_1 - y^{(i)})^2 \\ 0 & \text{ with probability } (a_1 - y^{(i)})^2 \end{cases}.\]
}
It can be checked that this procedure simulates a $\Phi$-decodable block MDP under the realizability assumptions on the regression dataset. Additionally, if we fix a latent state $s \in \MS$, then maximizing the probability of reaching state $s$ at step $2$ is equivalent to predicting $y^{(i)}$ conditioned on the observation $x_1^{(i)}$ \emph{and} the knowledge that $\phi^\st(x_2^{(i)}) = s$. Thus, any policy $\pi_s: \MX \to \MA$ that visits $s$ at step $2$ with near-maximal probability must approximately minimize a \emph{restricted} regression loss:
\[L_s(\pi_s) := \EE_{x_1,x_2} \left[\mathbbm{1}[\phi^\st(x_2) = s] (\pi_s(x_1) - f(\phi^\st(x_1),\phi^\st(x_2)))^2\right]\]
where $f: \MS\times\MS\to[0,1]$ is as in \cref{def:two-con-regression}. By assumption, the reward-free RL oracle will return a set of policies $\Psi$ that contains at least one such policy $\pi_s$ for each $s \in \MS$. 

The remaining challenge is how to stitch together these policies into a single predictor: given $(x_1,x_2) \in \MX\times\MX$, how do we use $x_2$ to identify the policy $\pi \in \Psi$ for which $\pi(x_1)$ is a good prediction? We accomplish this using \emph{one-context regression}, which (as discussed above) is reducible to reward-free RL. In particular, for each policy $\pi \in \Psi$, we construct datapoints of the form $(x_2^{(i)}, (\pi(x_1^{(i)}) - y^{(i)})^2)$. Applying one-context regression yields an estimate of (an appropriate transformation of) the map $x_2 \mapsto L_{\phi^\st(x_2)}(\pi)$. After learning this map, the final predictor $\MR:\MX\times\MX\to[0,1]$ is defined as follows: on input $(x_1,x_2)$, it outputs $\pihat^{x_2}(x_1)$, where $\pihat^{x_2}\in\Psi$ minimizes the estimated loss. See \cref{sec:app_minimality} for the formal reduction and analysis.
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
