\begin{algorithm}[tp]
	\caption{$\PSDPB(k, \Reg, R, \Psi_{1:k}, \Gamma, N)$: Policy Search by Dynamic Programming \\(variant of \citet{bagnell2003policy}; see also \cite{mhammedi2023representation,golowich2024exploring})}
	\label{alg:psdpb}
	\begin{algorithmic}[1]\onehalfspacing
          \State \textbf{input:} Step $k \in [H]$; regression oracle $\Reg$; reward function $R: \MX \to [0,1]$; policy covers $\Psi_1, \ldots, \Psi_{k}$; backup policy cover $\Gamma$; number of samples $N\in \mathbb{N}$.
		\For{$h=k, \dots, 1$} 
        \For{$a \in \MA$}
    		\State $\MD_{h,a} \gets\emptyset$.
    		\For{$N$ times}
            \State Sample policy $\pi \sim \frac{1}{2}\left(\Unif(\Psi_h) + \Unif(\Gamma)\right)$.
    		\State Sample trajectory $(x_1, a_1, \dots, x_k, a_k, x_{k+1})\sim
    		\pi \circ_h a \circ_{h+1} \pihat^{h+1:k}$.
            \State Sample $r_{k+1} \sim \Ber(R(x_{k+1}))$.
    		\State Update dataset: $\MD_{h,a} \gets \MD_{h,a} \cup \{ (x_h, r_{k+1})\}$.
    		\EndFor
    		\State Solve regression:
		\[\wh Q_h(\cdot,a) \gets \Reg(\MD_{h,a}).\] \label{eq:psdpb-regression}
        \EndFor
		\State Define $\pihat_h : \MX \ra \MA$ by
		\[
		\pihat_h(x)  := 
			\argmax_{a\in \MA} \wh Q_h(x,a),
          \] 
          \quad\, and write $\pihat^{h:k} = (\pihat_h, \ldots, \pihat_k)$. \label{line:psdpb-pihat-def}
		\EndFor
		\State \textbf{return:} Policy $\pihat^{1:k} \in \Pi$. 
	\end{algorithmic}
\end{algorithm}

The following lemma provides an analysis of \emph{Policy Search by Dynamic Programming (PSDP)} \citep{bagnell2003policy}---specifically, an implementation where the $Q$-functions are fit using one-context regression (\cref{alg:psdpb}). This shows that (approximate) policy optimization with a given reward function is efficiently reducible to one-context regression, and is a key element in both \cref{cor:online-rl-to-regression} and \cref{cor:reset-rl-to-regression}, as a subroutine in $\PCO$ (\cref{alg:pco}) and $\PCR$ (\cref{alg:pcr}) respectively. While the below statement is technically novel, since it abstracts generalization arguments into the regression oracle, at a technical level the analysis is entirely standard, see e.g. \cite{mhammedi2023representation,golowich2024exploring}.

\paragraph{Value functions.} For a Block MDP $M$, policy $\pi$, and collection $\bfr = (\bfr_h)_{h=1}^H$ of reward functions $\bfr_h: \MX \times \MA \to \RR$, we define value functions $Q^{M,\pi,\bfr}_h(x,a) := \EE^{M,\pi}[\sum_{k=h}^H \bfr_k(x_k,a_k) \mid{} x_h=x, a_h=a]$ and $V^{M,\pi,\bfr}_h(x) := \EE^{M,\pi}[\sum_{k=h}^H \bfr_k(x_k,a_k) \mid{} x_h=x]$.

\begin{lemma}[PSDP analysis]\label{lemma:psdp-trunc-online}
Fix $\alpha,\epsilon,\delta \in (0,1)$, $k \in [H]$, and $N \in \NN$. Let $\Phi_{1:k}$ be $\alpha$-truncated policy covers for $M$ at steps $1,\dots,k$ (\cref{defn:trunc-pc}), let $\Reg$ be an oracle that solves $\Nreg$-efficient one-context regression over $\Phi$, and let $R: \Xbar\to[0,1]$ be a function with $R(\term)=0$. Let $\Gamma \subset \Pi$ be a finite set of policies. If $N \geq \Nreg(\epsilon,\delta)$, then $\pihat \gets \PSDPB(k,\Reg,R,\Psi_{1:k},\Gamma,N)$ satisfies
\[\E^{M,\pihat}[R(x_{k+1})] \geq \max_{\pi\in\Pi} \E^{\Mbar(\Gamma),\pi}[R(x_{k+1})] - \frac{4H\sqrt{|\MA|\epsilon}}{\min(\alpha\trunc,\tsmall)}\]
with probability at least $1 - H|\MA|\delta$.
\end{lemma}

\begin{proof}
  Define reward function $\bfr = (\bfr_1,\dots,\bfr_H)$ by 
  \[\bfr_h(x,a) = \begin{cases} R(x) & \text{ if } h = k+1 \\ 0 & \text{ otherwise } \end{cases}.\] 
  Let $\pi^\st \in \argmax_{\pi\in\Pi} \E^{M,\pi}[R(x_{k+1})]$. Applying \cref{lemma:perf-diff-trunc} with reward function $R$ and policies $\pihat$ and $\pi^\st$ gives
  \begin{align*}
  &\E^{\Mbar(\Gamma),\pi^\st}[R(x_{k+1})] - \E^{M,\pihat}[R(x_{k+1})] \\
  &\leq \sum_{h=1}^k \E^{\Mbar(\Gamma),\pi^\st} \left[Q^{M,\pihat,\bfr}_h(x_h, a_h) - V^{M,\pihat,\bfr}_h(x_h)\right].
  \end{align*}
Fix $h \in [k]$. By definition (\lineref{line:psdpb-pihat-def}) we have $\pihat_h(x) \in \argmax_{a\in\MA} \wh Q_h(x,a)$. Let us define $\Delta_h:\Xbar\to\RR_{\geq 0}$ by \[\Delta_h(x) := \begin{cases} \max_{a \in \MA} | Q^{M,\pihat,\bfr}_h(x,a) - \wh Q_h(x,a)| & \text{ if } x \in \MX \\ 0 & \text{ if } x = \term\end{cases}.\] Then for any $x \in \MX$, we have
  \begin{align*}
    &Q^{M,\pihat,\bfr}_h(x, \pi_h^\st(x)) - V^{M,\pihat,\bfr}_h(x) \\
    &= Q^{M,\pihat,\bfr}_h(x, \pi_h^\st(x)) - Q^{M,\pihat,\bfr}_h(x, \pihat_h(x))  \\
    &\leq \wh Q_h(x,\pi^\st_h(x)) - \wh Q_h(x,\wh\pi_h(x))+ 2\Delta_h(x) \\ 
    &\leq 2\Delta_h(x)
  \end{align*}
  where the first inequality uses the definition of $\Delta_h(x)$ and the second inequality uses the fact that $\pihat_h(x) \in \argmax_{a \in \MA} \wh Q_h(x,a)$. Note that the above inequality also holds for $x=\term$, since $Q^{M,\pi,\bfr}_h(\term,a) = V^{M,\pi,\bfr}_h(\term) = 0$ for any $\pi\in\Pi$, $a \in \MA$. It follows that
  \begin{equation} 
  \E^{\Mbar(\Gamma),\pi^\st}[R(x_{k+1})] - \E^{M,\pihat}[R(x_{k+1})]
  \leq \sum_{h=1}^{k-1} \E^{\Mbar(\Gamma),\pi^\st}[2\Delta_h(x_h)],
  \label{eq:psdp-perf-diff}
  \end{equation}
  and it only remains to upper bound each term $\E^{\Mbar(\Gamma),\pi^\st}[\Delta_h(x_h)]$. Once more, fix $h \in [k]$ and $a \in \MA$. The dataset $\MD_{h,a}$ consists of $N$ independent and identically distributed samples $(x,r)$ with
  \[\E[r|x] = \E^{M,a \circ_h \pihat^{h+1:k}}[R(x_{k+1}) | x_h = x] = \E^{M,a \circ_h \wh \pi^{h+1:k}}[R(x_{k+1})|s_h = \phi^\st(x)] = Q^{M,\pihat,\bfr}_h(x,a).\]
  In particular, by the penultimate equality, $\E[r|x]$ only depends on $\phi^\st(x)$, so by the guarantee on $\Reg$ (\cref{def:one-con-regression}) and the assumption that $N \geq \Nreg(\epsilon,\delta)$, it holds with probability at least $1-\delta$ that
  \[\EE_{\pi \sim \frac{1}{2}(\Unif(\Psi_h)+\Unif(\Gamma))} \E^{M,\pi}(\wh Q_h(x_h,a) - Q^{M,\pihat,\bfr}_h(x_h,a))^2 \leq \epsilon.\]
  Let $\ME_{h,a}$ be the event that this inequality holds. Under $\bigcap_{a \in \MA} \ME_{h,a}$, we have
  \begin{align*}
\EE_{\pi \sim \frac{1}{2}(\Unif(\Psi_h)+\Unif(\Gamma))} \E^{M,\pi}[\Delta_h(x_h)^2] 
&\leq \sum_{a\in\MA} \EE_{\pi \sim \frac{1}{2}(\Unif(\Psi_h)+\Unif(\Gamma))} \E^{M,\pi}(\wh Q_h(x_h,a) - Q^{M,\pihat,\bfr}_h(x_h,a))^2 \\ 
&\leq |\MA|\epsilon,
\end{align*}
  which yields, via Jensen's inequality, that
  \[\EE_{\pi \sim \frac{1}{2}(\Unif(\Psi_h)+\Unif(\Gamma))} \E^{M,\pi}[\Delta_h(x_h)] \leq \sqrt{|\MA|\epsilon}.\] It follows that under event $\bigcap_{a\in\MA} \ME_{h,a}$,
  \begin{align}
    \E^{\Mbar(\Gamma),\pi^\st}[\Delta_h(x_h)] 
    &\leq \frac{2}{\min(\alpha\trunc,\tsmall)} \EE_{\pi\sim\frac{1}{2}(\Unif(\Psi_h)+\Unif(\Gamma))} \E^{M,\pi}[\Delta_h(x_h)] \nonumber \\ 
    &\leq \frac{2\sqrt{|\MA|\epsilon}}{\min(\alpha\trunc,\tsmall)} \nonumber.
  \end{align}
  where the first inequality uses \cref{item:h-plus-one-cov-lb} of \cref{lemma:srch-gamma-covering} together with the assumption that $\Psi_h$ is an $\alpha$-truncated cover (\cref{defn:trunc-pc}) and the fact that $\Delta_h(\term)=0$ and $\Delta_h(x) \geq 0$ for all $x \in \MX$.
Substituting into \cref{eq:psdp-perf-diff}, we conclude that, under the event $\bigcap_{h=1}^{k} \bigcap_{a\in\MA} \ME_{h,a}$ (which occurs with probability at least $1-H|\MA|\delta$), 
  \[
    \E^{\Mbar,\pi^\st}[R(x_{k+1})] - \E^{M,\pihat}[R(x_{k+1})] \leq \frac{4H\sqrt{|\MA|\epsilon}}{\min(\alpha\trunc,\tsmall)}
  \]
  as claimed.
\end{proof}

\iffalse 


\begin{algorithm}[t]
	\caption{$\PSDPB(k, \Reg, R, \Psi_{1:k}, \Gamma, N)$: Policy Search by Dynamic Programming (variant of \cite{bagnell2003policy})}
	\label{alg:psdpb}
	\begin{algorithmic}[1]\onehalfspacing
		\Require Step $k \in [H]$; regression oracle $\Reg$; reward function $R: \MX \to [0,1]$; policy covers $\Psi_1, \ldots, \Psi_{k}$; backup policy cover $\Gamma$; number of samples $N\in \mathbb{N}$.
		\For{$h=k, \dots, 1$} 
        \For{$a \in \MA$}
    		\State $\MD_{h,a} \gets\emptyset$.
    		\For{$N$ times}
            \State Sample policy $\pi \sim \frac{1}{2}\left(\Unif(\Psi_h) + \Unif(\Gamma)\right)$.
    		\State Sample trajectory $(x_1, a_1, \dots, x_k, a_k, x_{k+1})\sim
    		\pi \circ_h a \circ_{h+1} \pihat^{h+1:k}$.
            \State Set $r_{k+1} \gets R(x_{k+1})$.
    		\State Update dataset: $\MD_{h,a} \gets \MD_{h,a} \cup \{ (x_h, r_{k+1})\}$.
    		\EndFor
    		\State Solve regression:
		\[\wh Q_h(\cdot,a) \gets \Reg(\MD_{h,a}).\] \label{eq:psdp-mistake}
        \EndFor
		\State\label{line:pihat-def} Define $\pihat_h : \MX \ra \MA$ by
		\[
		\pihat_h(x)  := 
			\argmax_{a\in \MA} \wh Q_h(x,a),
          \] 
          \quad\, and write $\pihat^{h:k} = (\pihat_h, \ldots, \pihat_k)$. 
		\EndFor
		\State \textbf{Return:} Policy $\pihat^{1:k} \in \Pi$. 
	\end{algorithmic}
\end{algorithm}

\begin{lemma}[PSDP for Online RL]\label{lemma:psdp-trunc-online}
Fix $\alpha,\epsilon,\delta \in (0,1)$, $k \in [H]$, and $N \in \NN$. Let $\Phi_{1:k}$ be $\alpha$-truncated policy covers, let $\Reg$ be a TM that solves $\Nreg$-efficient one-context regression over $\Phi$, and let $R: \Xbar\to[0,1]$ be a function with $R(\term)=0$. Let $\Gamma \subset \Pi$ be a finite set of policies. Then $\pihat \gets \PSDPB(k,\Reg,R,\Psi_{1:k},\Gamma,N)$ satisfies
\[\E^{M,\pihat}[R(x_{k+1})] \geq \max_{\pi\in\Pi} \E^{\Mbar(\Gamma),\pi}[R(x_{k+1})] - \frac{4H}{\min(\alpha,\tsmall)} \cdot \sqrt{|\MA| \cdot \epsilon}\]
with probability at least $1 - H|\MA|\delta$.
\end{lemma}

\fi
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
