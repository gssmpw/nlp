To begin, we formally introduce Block MDPs, the episodic and reset access models for RL, and the computational problems: reward-free RL and one- and two-context regression. As basic notation, $[k]$ denotes the set of integers $\{1,\dots,k\}$, and $\Delta(\MZ)$ denotes the family of distributions over set $\MZ$.

\subsection{Block MDPs and Episodic RL}

For a set $\Phi \subseteq (\MX\to\MS)$, i.e. a set of functions $\phi:\MX\to\MS$, a (reward-free) $\Phi$-decodable \emph{Block MDP} \citep{du2019provably} is a tuple
$
M = (H, \MS, \MX, \MA, (\til \BP_h)_{h \in [H]}, (\til\BO_h)_{h\in[H]}, \phi^\st)
$
where $H \in \NN$ is the \emph{horizon}, $\MS$ is the \emph{latent state space}, $\MX$ is the \emph{observation space}, $\MA$ is the \emph{action set}, $\til \BP_1 \in \Delta(\MS)$ is the \emph{latent initial distribution}, $\til \BP_h: \MS \times \MA \to \MS$ is the \emph{latent transition distribution} into step $h$ (for any $h \in \{2,\dots,H\}$), $\til \BO_h: \MS \to \Delta(\MX)$ is the \emph{observation distribution} at step $h$ (for any $h \in [H]$), and $\phi^\st\in\Phi$ is the \emph{decoding function}. %Moreover, it is required that the different latent states have disjoint observation distributions, i.e. there is a function $\rho^\st: \MX \to \MS$ 
It is required that $\phi^\st(x_h) = s_h$ with probability $1$ over $x_h \sim \til \BO_h(\cdot\mid{}s_h)$, for all $h \in [H]$ and $s_h \in \MS$ (so in particular, $\til \BO_h(\cdot \mid{} s), \til \BO_h(\cdot \mid{} s')$ have disjoint supports for all $s \neq s'$).
%In this setting we say that $M$ is \emph{$\rho^\st$-decodable} and moreover that $\rho^\st$ is a decoding function for $M$. 
For any $x,x'\in\MX$ and $a \in \MA$, we write $\BP_h(x'\mid{}x,a)$ to denote $\til\BP_h(\phi^\st(x')\mid{}\phi^\st(x),a) \til\BO_h(x'\mid{}\phi^\st(x'))$. We similarly define $\BP_1(x) = \til\BP_1(\phi^\st(x))\til\BO_1(x\mid{}\phi^\st(x))$. Observe that $(H, \MX, \MA,(\BP_h)_h)$ is a (reward-free) MDP, with the potentially large state space $\MX$.

\paragraph{Access model I: Episodic online RL.} Fix a Block MDP $M$ as specified above. We say that an algorithm $\Alg$ has (episodic) online access to $M$ to mean that $\Alg$ is executed in the following model. First, $\Alg$ is given $H$ and $\MA$ as input. At any time, $\Alg$ can request a new \emph{episode}. The model then draws $s_1 \sim \til \BP_1$ and $x_1 \sim \til\BO_1(\cdot\mid{}s_1)$, and sends $x_1$ to $\Alg$. The timestep of the episode is set to $h=1$. So long as $h \leq H$, the algorithm $\Alg$ can at any time play an action $a_h \in \MA$. If $h < H$, then the model draws $s_{h+1} \sim \til \BP_{h+1}(\cdot\mid{}s_h, a_h)$, and $x_{h+1} \sim \til\BO_{h+1}(\cdot\mid{}s_{h+1})$, and sends $x_{h+1}$ to $\Alg$ and increments $h$. Otherwise, the episode concludes. Note that $\Alg$ never observes the latent states $s_{1:H}$.

\paragraph{Access model II: (Episodic) online RL with resets.} We say that an algorithm $\Alg$ has reset access to $M$ to mean that $\Alg$ is given access to the following sampling oracles (in addition to $H$ and $\MA$, as before). The first sampling oracle draws $s_1 \sim \til\BP_1$ and $x_1 \sim \til\BO_1(\cdot\mid{}s_1)$, and outputs $x_1$. The second sampling oracle takes as input a step $h \in [H-1]$, any previously-seen observation $x_h \in \MX$, and an action $a_h \in \MA$; then, the oracle samples $s_{h+1} \sim \til\BP_{h+1}(\cdot\mid{}\phi^\st(x_h),a_h)$ and $x_{h+1} \sim \til\BO_{h+1}(\cdot\mid{}s_{h+1})$ and outputs $x_{h+1}$. Informally, this oracle allows the algorithm to not only sample independent episodes from $M$, but also to reset to any previously-seen observation.

%\dhruv{todo: make indexing of $\BP^M_h$ consistent}
%\dhruv{todo: emission vs observation vs context}

\paragraph{Policies and visitations.} A (randomized) \emph{policy} $\pi = (\pi_h)_{h=1}^H$ is a collection of maps $\pi_h: \MX \to \Delta(\MA)$. We write $\Pi$ to denote the set of all policies. For $k \in [H]$, we write $d^{M,\pi}_k\in\Delta(\MS)$ to denote the distribution of $s_k$ in an episode of interaction with $M$ where $a_h\sim\pi_h(x_h)$ for each step $h$. %Any policy $\pi$ induces a distribution over \emph{trajectories} $(s_1,x_1,a_1,\dots,s_H,x_H,a_H)$, where the trajectory is sampled via an episode of online interaction with $M$, and at step $h$ the action $a_h$ is sampled from $\pi_h(x_h)$. We write $\Pr^{M,\pi}[\cdot]$ (respectively, $\EE^{M,\pi}[\cdot]$) to denote probability (respectively, expectation) over a trajectory sampled from $M$ with policy $\pi$. For $h \in [H]$ and $s \in \MS$, we write $d^{M,\pi}_h(s) := \Pr^{M,\pi}[s_h=s]$. %Finally, when $M$ is not clear from context, we write $\til\BP^M_h$ to denote the latent transition distribution $\til \BP_h$ of $M$.



\subsection{Computational Problems}

Fix sets $\MS,\MX$ and a concept class $\Phi \subseteq (\MX\to\MS)$. For the purposes of oracle reductions, an algorithm/oracle for a statistical learning problem is parametrized by its statistical efficiency (i.e. how many samples it needs in order to achieve certain accuracy) and, in the case of reward-free RL, the number of policies in the output. Throughout, we assume that the outputs of an algorithm/oracle (either policies or prediction functions) are succinctly described by circuits (and efficiently evaluatable).
% \begin{definition}[Oracle time complexity]
For an algorithm $\Alg$ with access to an oracle $\MO$, the \emph{oracle time complexity} of $\Alg$ is the time complexity in the computational model where each query to $\MO$ takes linear time in the query length.
% \end{definition}


\begin{definition}[Reward-free RL \citep{du2019provably,jin2020reward}]\label{def:strong-rf-rl}
Fix\arxiv{ functions} $\Nrl, \Krl: (0,1/2)^2 \times \NN^2 \to \NN$. An interactive algorithm $\Alg$ is an $(\Nrl,\Krl)$-efficient \emph{reward-free (episodic/reset) RL algorithm} for $\Phi$ if the following holds. Fix $\epsilon,\delta \in (0,1/2)$,  $H \in \NN$, and a set $\MA$. Given (episodic/reset) access to a $\Phi$-decodable Block MDP $M$ with horizon $H$ and action set $\MA$, $\Alg(\epsilon,\delta, H,\MA)$ uses at most $\Nrl(\epsilon,\delta,H,|\MA|)$ (episodes/queries), and outputs a set of policies $\Psi$ with\arxiv{:
\begin{itemize}
    \item $|\Psi| \leq \Krl(\epsilon,\delta,H,|\MA|)$
    \item With probability at least $1-\delta$, it holds that for all $s \in \MS$ and $h \in [H]$, 
    \begin{equation} \max_{\pi \in \Psi} d^{M,\pi}_h(s) \geq \max_{\pi \in \Pi} d^{M,\pi}_h(s) - \epsilon.\label{eq:rfrl-pc}
    \end{equation}
\end{itemize}
}
\colt{ 
(1) $|\Psi| \leq \Krl(\epsilon,\delta,H,|\MA|)$, and (2) with probability at least $1-\delta$, for all $s \in \MS$ and $h \in [H]$, 
    \begin{equation} \max_{\pi \in \Psi} d^{M,\pi}_h(s) \geq \max_{\pi \in \Pi} d^{M,\pi}_h(s) - \epsilon.\label{eq:rfrl-pc}
    \end{equation}

}
\end{definition}
%\dfc{good to cite all the papers that use this notion here to emphasize that it is standard}
%\dfc{maybe good to mention that all papers that give efficient algos for bmdp and beyond use reward-free rl as an intermediate step even if the end goal is reward-based.}

A set $\Psi$ that satisfies \cref{eq:rfrl-pc} is called a $(1,\epsilon)$-\emph{policy cover}; it formalizes the notion of reaching all latent states with near-maximal probability. While the ultimate goal in many applications is \emph{reward-directed RL}, it is straightforward to convert a reward-free RL algorithm into a reward-directed RL algorithm, and most existing oracle-efficient RL algorithms for Block MDPs (and more general classes) use some version of reward-free RL as a subroutine---see \cref{sec:app_related} for discussion and comparison to variants of \cref{def:strong-rf-rl}. We defer defining reward-free RL in more general settings to \cref{sec:lowrank}.\loose %Recent works have also studied the relaxed problem of learning a $(\alpha,\epsilon)$-policy cover \cite{misra2020kinematic,mhammedi2023representation}, which allows for multiplicative loss in the visitation probabilities; this relaxation is crucial for meaningful discussion of Low-Rank MDPs (\cref{sec:lowrank}), but there are 

%\dhruv{not really sure if parametrizing this way is the cleanest approach}

We now formally define the two notions of regression oracle we consider; versions of both (c.f. \cref{remark:oracle-subtleties-app}) have been used extensively throughout the reinforcement learning literature.

\begin{definition}[One-context regression]\label{def:one-con-regression}
Fix\arxiv{ a function} $\Nreg: (0,1/2)^2 \to \NN$. An algorithm $\Alg$ is an $\Nreg$-efficient one-context regression algorithm for $\Phi$ if the following holds. Fix $\epsilon,\delta \in (0,1/2)$, $n \in \NN$, $\phi \in \Phi$, $\MD \in \Delta(\MX)$, and $f: \MS \to [0,1]$. Let $(x^{(i)},y^{(i)})_{i=1}^n$ be i.i.d. samples with $x^{(i)} \sim \MD$, $y^{(i)} \in \{0,1\}$, and $\E[y^{(i)}\mid{}x^{(i)}] = f(\phi(x^{(i)}))$. If $n \geq \Nreg(\epsilon,\delta)$, then with probability at least $1-\delta$, the output of $\Alg((x^{(i)},y^{(i)})_{i=1}^n, \epsilon,\delta)$ is a circuit $\MR: \MX \to [0,1]$ satisfying \colt{$\EE_{x \sim \MD} (\MR(x) - f(\phi(x)))^2 \leq \epsilon.$}\arxiv{\[\EE_{x \sim \MD} (\MR(x) - f(\phi(x)))^2 \leq \epsilon.\]}
\end{definition}

See \cref{fig:ocr} for the graphical model structure satisfied by each sample. One-context regression is a natural oracle for estimating value functions and Bellman backups \citep{ernst2005tree,mhammedi2023efficient,golowich2024exploration}. In our definition, like that of \cite{golowich2024exploration}, the oracle is improper and only required to succeed on well-specified i.i.d. data.

\arxiv{
\begin{figure}[t]
\centering     %%% not \center
\subfigure[One-context regression]{\label{fig:ocr}\parbox{6cm}{\centering\includegraphics[height=3cm]{ocr_dark.eps}\vspace{1em}}}
\hspace{5em}
\subfigure[Two-context regression]{\label{fig:tcr}\parbox{6cm}{\centering\includegraphics[height=3cm]{tcr_dark.eps}\vspace{1em}}}
\caption{Undirected graphical model representation for a single sample from \emph{(a)} one-context, or \emph{(b)} two-context regression. Note that the gray variables are unobserved.}
\end{figure}
}
\colt{ 
\begin{figure}[t]
\centering
\begin{minipage}{0.25\textwidth}
        \captionsetup{justification=raggedright, singlelinecheck=off} % Remove indent

    \caption{Undirected graphical model representation for a single sample from \emph{(a)} one-context, or \emph{(b)} two-context regression. Note that the gray variables are unobserved.}
\end{minipage}%
\hspace{1em}
\begin{minipage}{0.7\textwidth}
    \subfigure[One-context regression]{\label{fig:ocr}\parbox{4.5cm}{\centering\includegraphics[height=3cm]{ocr.eps}\vspace{1em}}}
    \hspace{1em}
    \subfigure[Two-context regression]{\label{fig:tcr}\parbox{4.5cm}{\centering\includegraphics[height=3cm]{tcr.eps}\vspace{1em}}}
\end{minipage}
\end{figure}
}

%\dfc{Explain that this is a natural oracle for estimating value functions and bellman backups, cite papers that use. Good to also remark on the difference between the notion above and agnostic/worst-case versions.}

\begin{definition}\label{def:realizable-distribution}
Let $\phi \in \Phi$. A distribution $\MD \in \Delta(\MX \times \MX)$ is $\phi$-realizable if $(X_1,X_2) \sim \MD$ satisfies $X_2 \perp X_1 \mid \phi(X_1)$ and $X_2 \perp X_1 \mid \phi(X_2)$.
\end{definition}

\begin{definition}[Two-context regression]\label{def:two-con-regression}
Fix\arxiv{ a function} $\Nreg: (0,1/2)^2 \to \NN$. An algorithm $\Alg$ is an $\Nreg$-efficient two-context regression algorithm for $\Phi$ if the following holds. Fix $\epsilon,\delta \in (0,1/2)$, $n \in \NN$, $\phi \in \Phi$, a $\phi$-realizable distribution $\MD \in \Delta(\MX\times\MX)$, and $f: \MS \times \MS \to [0,1]$. Let $(x_1^{(i)},x_2^{(i)},y^{(i)})_{i=1}^n$ be i.i.d. samples with $(x_1^{(i)},x_2^{(i)}) \sim \MD$, $y^{(i)} \in \{0,1\}$, and $\E[y^{(i)}\mid{}x_1^{(i)},x_2^{(i)}] = f(\phi(x_1^{(i)}),\phi(x_2^{(i)}))$. If $n \geq \Nreg(\epsilon,\delta)$, then with probability at least $1-\delta$, the output of $\Alg$ on input $(x_1^{(i)},x_2^{(i)},y^{(i)})_{i=1}^n$ is a circuit $\MR: \MX \times \MX \to [0,1]$ satisfying \colt{$\EE_{(x_1,x_2) \sim \MD} (\MR(x_1,x_2) - f(\phi(x_1),\phi(x_2)))^2 \leq \epsilon.$}\arxiv{\[\EE_{(x_1,x_2) \sim \MD} (\MR(x_1,x_2) - f(\phi(x_1),\phi(x_2)))^2 \leq \epsilon.\]}
\end{definition}

See \cref{fig:tcr} for the graphical model structure satisfied by each sample. Two-context regression is a natural oracle for estimating inverse kinematics---e.g., ``given data from two policies, predict which policy the sample came from.'' Variants of this oracle are widely-used in RL \citep{misra2020kinematic,lamb2022guaranteed,mhammedi2023representation}; see \cref{sec:app_related} for discussion.

\subsection{Additional Assumptions}

Our reductions \emph{from} regression \emph{to} RL require the following mild regularity condition on $\Phi$; essentially, it asserts that there are two latent states $\{0,1\}$ that are fully observable, irrespective of the decoding function $\phi^\st$. These extra states enable simulating ``reward'' states in the reductions. %From a technical perspective, one could augment any concept class $\Phi$ with two such states; this does not change the computational complexity of regression (\cref{prop:twoaug}), but in pathological cases it could hypothetically change the complexity of RL. \Cref{def:regular} rules out this pathology.

\begin{definition}[Regularity condition]\label{def:regular}
We say that a concept class $\Phi$ is \emph{regular} if there are two special states $\{0,1\} \in \MS \cap \MX$ that are fully observed, i.e. $\phi^{-1}(b) = \{b\}$ for all $\phi\in\Phi$ and $b \in \{0,1\}$.\loose
\end{definition}

%We remark that this condition is only used in the ``necessary oracle'' results (\cref{cor:regression-to-online-rl} and \cref{thm:one-context-reduction}), where these extra states simulate ``reward'' states.



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
