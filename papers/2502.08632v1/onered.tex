
In \sssref{sec:onered}, we prove that one-context regression is necessary for reward-free episodic RL, modifying a proof of \cite{golowich2024exploration}; this reduction is one piece in the proof of \cref{cor:regression-to-online-rl}. In \sssref{sec:noiselessonered}, we introduce noiseless one-context regression and prove that it is necessary for reward-free RL in the reset access model, which complements our result from \cref{sec:resets}. In \sssref{sec:onetwo}, we show that one-context regression is a special case of two-context regression, which is needed for our episodic RL algorithm $\PCO$ (see \cref{sec:online}). In \sssref{sec:oneaug} and \sssref{sec:twoaug}, we show that one-context regression and two-context regression for concept class $\Phiaug$ reduce to one-context regression and two-context regression for $\Phi$; the latter is one piece in the proof of \cref{cor:regression-to-online-rl}.

In these reductions, fix a concept class $\Phi \subseteq (\MX\to\MS)$ and recall the definition of sets $\Xaug,\Saug$ and augmented concept class $\Phiaug \subseteq (\Xaug\to\Saug)$ from \cref{def:phiaug}.

\subsubsection{The $\OneRed$ Reduction}\label{sec:onered}

\input{noisyonered}

\subsubsection{The $\NoiselessOneRed$ Reduction}\label{sec:noiselessonered}

In this section, we adapt the reduction from \sssref{sec:onered} to the reset access model. However, this requires weakening the regression problem to be \emph{noiseless}:

\begin{definition}[Noiseless one-context regression]\label{def:noiseless-one-con-regression}
Let $\Nreg: (0,1/2)^2 \to \NN$ be a function. An algorithm $\Alg$ is an $\Nreg$-efficient noiseless one-context regression algorithm for $\Phi$ if the following holds. Fix $\epsilon,\delta \in (0,1/2)$, $n \in \NN$, and $\phi \in \Phi$. Let $\MD \in \Delta(\MX)$ be a distribution, and let $f: \MS \to \{0,1\}$. Let $(x^{(i)},y^{(i)})_{i=1}^n$ be i.i.d. samples with $x^{(i)} \sim \MD$, $y^{(i)} \in \{0,1\}$, and $\E[y^{(i)}\mid{}x^{(i)}] = f(\phi(x^{(i)}))$. If $n \geq \Nreg(\epsilon,\delta)$, then with probability at least $1-\delta$, the output of $\Alg((x^{(i)},y^{(i)})_{i=1}^n, \epsilon,\delta)$ is a circuit $\MR: \MX \to [0,1]$ satisfying 
\[\EE_{x \sim \MD} (\MR(x) - f(\phi(x)))^2 \leq \epsilon.\]
\end{definition}

With this definition, the following theorem shows that $\NoiselessOneRed$ (\cref{alg:noiselessonered}) reduces noiseless one-context regression for concept class $\Phi$ to reward-free RL for concept class $\Phiaug$ in the reset model. By combining with \cref{prop:oneaug}, this implies that there is a reduction to reward-free RL for concept class $\Phi$ itself, so long as $\Phi$ is regular. We leave it as an open problem whether the reduction can be strengthened to work with noisy one-context regression.

\begin{algorithm}[t]
	\caption{$\NoiselessOneRed(\MO, (x^{(i)},y^{(i)})_{i=1}^n,\epsilon,\delta)$: Reduction from noiseless one-context regression to RL with resets}
	\label{alg:noiselessonered}
	\begin{algorithmic}[1]\onehalfspacing
		          \State \textbf{input:} Oracle $\MO$ for reward-free RL with resets; samples $(x^{(i)},y^{(i)})_{i=1}^n$; tolerances $\epsilon,\delta$.
		\State Set $\epa := \sqrt{\epsilon/4}$ and $i = 1$. Initialize $\MO$ with tolerance $\epsilon/4$, failure probability $\delta/2$, horizon $H := 2$, and action set $\MA := \{0,\epa,\dots,1-\epa\}$. Simulate $\MO$ as follows:
        \Repeat
            \State When $\MO$ queries the first sampling oracle, pass $x^{(i)}$ and set $i \gets i+1$.
            \State When $\MO$ queries the second sampling oracle with inputs $x_1\in\Xaug$ and $a_1\in\MA$, identify any $j < i$ with $x_1 = x^{(j)}$. With probability $(a-y^{(j)})^2$, pass observation $0$. Otherwise, pass observation $1$.
        \Until{$\MO$ returns policy cover $\Psi$}
        \State $m \gets 16\epsilon^{-2}\log(4|\Psi|/\delta)$.
        \For{$\pi \in \Psi$}
            \State Compute $\wh E(\pi) := \frac{1}{m}\sum_{i=n-m+1}^n (\pi(x^{(i)}) - y^{(i)})^2$.\label{line:noiseless-one-red-emp}
        \EndFor
        \State \textbf{return:} $\pihat := \argmin_{\pi \in \Psi}  \wh E(\pi)$.
	\end{algorithmic}
\end{algorithm}


\begin{proposition}\label{prop:noiseless-onered}
Suppose that $\MO$ is a $(\Nrl,\Krl)$-efficient reward-free reset RL algorithm for $\Phiaug$. Then $\NoiselessOneRed(\MO,\cdot)$ is a $\Nreg$-efficient noiseless one-context regression algorithm for $\Phi$ with
\[\Nreg(\epsilon,\delta) = \Nrl\left(\frac{\epsilon}{4}, \frac{\delta}{2}, 2, \sqrt{\frac{4}{\epsilon}}\right) + \frac{16\log(4\Krl\left(\frac{\epsilon}{4}, \frac{\delta}{2}, 2, \sqrt{\frac{4}{\epsilon}}\right)/\delta)}{\epsilon^2}.\]
\end{proposition}

\begin{proof}
Let $\epsilon,\delta>0$, $\MD \in \Delta(\MX)$, and $f: \MS \to \{0,1\}$. Let $n \geq \Nreg(\epsilon,\delta)$. Let $(x^{(i)},y^{(i)})_{i=1}^n$ be i.i.d. samples with $x^{(i)} \sim \MD$, $y^{(i)} \in \{0,1\}$, and $\EE[y^{(i)}\mid{}x^{(i)}] = f(\phi(x^{(i)}))$ for some $\phi \in \Phi$. We analyze the execution of $\NoiselessOneRed(\MO,(x^{(i)},y^{(i)})_{i=1}^n, \epsilon,\delta)$. We know that $n-m \geq \Nrl(\epsilon/4,\delta/2,2,\sqrt{4/\epsilon})$, since $n \geq \Nreg(\epsilon,\delta)$. Now the first sampling oracle provides independent samples from $\MD$. For the second sampling oracle, since $\MO$ can only query $x_1 \in \Xaug$ which it has previously seen, it must be that there exists $j < i$ with $x_1 = x^{(j)}$. Moreover, since the range of $f$ is in $\{0,1\}$, we have deterministically that $y^{(j)} = f(\phi(x^{(j)}))$. Conditioned on the queries $x_1$ and $a_1$, the output of the second sampling oracle is therefore independent of all prior queries, and the probability of observing $0$ is
\begin{align*}
\EE[(a_1-y^{(j)})^2\mid{} x^{(j)}, a_1]
&= (a_1)^2 - 2 a_1 \EE[y^{(j)}\mid{} x^{(j)}] + \EE[(y^{(j)})^2 \mid{} x^{(j)}] \\ 
&= (a_1)^2 + (1 - 2a_1) f(\phi(x_1))
\end{align*}
where the final equality uses that $y^{(i)} \in \{0,1\}$. Thus, $\NoiselessOneRed$ simulates $\MO$ on a $\Phiaug$-decodable block MDP $M$ with horizon $2$, observation space $\Xaug$, latent state space $\Saug$, initial observation distribution $\MD$, and transition distribution defined by
\[\BP_2(0\mid{} x_1,a_1) := a_1^2 + (1-2a_1)f(\phi(x_1)),\]
\[\BP_2(1\mid{} x_1,a_1) := 1 - \BP_2(0\mid{} x_1,a_1).\]
The remainder of the proof is identical to that of \cref{prop:onered}.
\end{proof}

\subsubsection{The $\OneTwo$ Reduction}\label{sec:onetwo}

\begin{algorithm}[t]
	\caption{$\OneTwo(\MO, (x^{(i)},y^{(i)})_{i=1}^n,\epsilon,\delta)$: One-context regression to two-context regression reduction}
	\label{alg:onetwo}
	\begin{algorithmic}[1]\onehalfspacing
		          \State \textbf{input:} Two-context regression oracle $\MO$; samples $(x^{(i)},y^{(i)})_{i=1}^n$; tolerances $\epsilon,\delta$.
		\State Pick arbitrary $\xbar \in \MX$.
        \State Compute
        \[\til \MR \gets \MO((x_1^{(i)},\xbar, y^{(i)})_{i=1}^{n},\epsilon, \delta).\]\label{line:tilmr}
        \State \textbf{return:} $\MR$ defined by $\MR(x) := \til\MR(x,\xbar)$.
	\end{algorithmic}
\end{algorithm}

The following proposition shows that $\OneTwo$ (\cref{alg:onetwo}) is an efficient reduction from one-context regression to two-context regression.

\begin{proposition}\label{prop:onetwo}
Fix sets $\MX,\MS$ and $\Phi \subseteq (\MX\to\MS)$. Suppose that $\MO$ is an $\Nreg$-efficient two-context regression oracle for $\Phi$. Then $\OneTwo(\MO,\cdot)$ is an $\Nreg$-efficient one-context regression oracle for $\Phi$.
\end{proposition}

\begin{proof}
Let $(x^{(i)},y^{(i)})_{i=1}^n$ be i.i.d. samples with $x^{(i)} \sim \MD$, $y^{(i)} \in \{0,1\}$, and $\EE[y^{(i)}\mid{}x^{(i)}] = f(\phi(x^{(i)}))$ for some $\phi \in\Phi$ and $f:\MS\to[0,1]$. Then for any fixed $\xbar \in \MX$, the samples $(x_1^{(i)},\xbar,y^{(i)})_{i=1}^n$ are i.i.d., the distribution of $(x_1^{(i)},\xbar)$ is $\phi$-realizable (since $\xbar$ is independent of $x_1^{(i)}$), and $\EE[y^{(i)}\mid{}x_1^{(i)},\xbar] = g(\phi(x^{(i)}),\phi(\xbar))$ where $g(s_1,s_2) := f(s_1)$. By \cref{def:two-con-regression}, so long as $N \geq \Nreg(\epsilon,\delta)$, it holds with probability at least $1-\delta$ that the predictor $\til \MR$ computed in \lineref{line:tilmr} satisfies
\[\EE_{x_1 \sim \MD} (\til\MR(x_1,\xbar) - g(\phi(x_1), \phi(\xbar)))^2 \leq \epsilon.\]
In this event, by definition of $g$, the output $\MR(\cdot) := \til\MR(\cdot,\xbar)$ of $\OneTwo$ satisfies
\[\EE_{x_1 \sim \MD} (\MR(x_1) - f(\phi(x_1)))^2 \leq \epsilon\]
as required for one-context regression (\cref{def:one-con-regression}).
\end{proof}

\subsubsection{The $\OneAug$ Reduction}\label{sec:oneaug}

\begin{algorithm}[t]
	\caption{$\OneAug(\MO, (x^{(i)},y^{(i)})_{i=1}^n,\epsilon,\delta)$: One-context regression for $\Phiaug$}
	\label{alg:oneaug}
	\begin{algorithmic}[1]\onehalfspacing
		\State\textbf{input:} One-context regression oracle $\MO$ for $\Phi$; samples $(x^{(i)},y^{(i)})_{i=1}^n$; tolerances $\epsilon,\delta$.
		\State Let $S := \{i \in [n]: x^{(i)} \in \MX\}$.
        \State Compute
        \[\MR_\MX \gets \MO((x^{(i)}, y^{(i)})_{i \in S},\epsilon/6, \delta/6).\]\label{line:mrmx}
        \State For $b \in \{0,1\}$, let $S_b := \{i \in [n]: x^{(i)} = b\}$ and compute
        \[\MR_b := \frac{1}{|S_b|} \sum_{i \in S_b} y^{(i)}.\]\label{line:mrb}
        \State \textbf{return:} $\MR:\Xaug\to[0,1]$ defined by \[ \MR(x) := 
        \begin{cases} 
        \MR_\MX(x) & \text{ if } x \in \MX \\ 
        \MR_0 & \text{ if } x = 0 \\ 
        \MR_1 & \text{ if } x = 1
        \end{cases}.\]
	\end{algorithmic}
\end{algorithm}


The following proposition shows that $\OneAug$ (\cref{alg:oneaug}) is an efficient reduction from one-context reduction for concept class $\Phiaug$ to the same problem for concept class $\Phi$. The basic idea is that the states $\{0,1\}$ are fully observed, so they can be regressed separately via mean estimation.

\begin{proposition}\label{prop:oneaug}
There is a constant $C_{\ref{prop:oneaug}}>0$ so that the following holds. Fix sets $\MX,\MS$ and $\Phi \subseteq (\MX \to \MS)$. Suppose that $\MO$ is an $\Nreg$-efficient one-context regression oracle for $\Phi$. Then $\OneAug(\MO,\cdot)$ is an $\Nreg'$-efficient one-context regression oracle for $\Phiaug$ with
\[\Nreg'(\epsilon,\delta) = C_{\ref{prop:oneaug}}\left(\epsilon^{-1}\Nreg(\epsilon/6,\delta/6) + \epsilon^{-2}\log(12/\delta)\right).\]
\end{proposition}

\begin{proof}
Fix $\epsilon,\delta \in (0,1)$, $\MD \in \Delta(\Xaug)$, $f: \Saug \to [0,1]$, and $\phiaug\in\Phiaug$. By definition there is $\phi\in\Phi$ with $\phiaug = \aug(\phi)$. We invoke \cref{lemma:compose-predictors} with the following parameters. Set $\MZ := \Xaug \times \{0,1\}$, and define $\MZ_0 := \{0\} \times \{0,1\}$, $\MZ_1 = \{1\}\times\{0,1\}$, and $\MZ_2 := \MX \times \{0,1\}$. Let $\mu \in \Delta(\MZ)$ be the distribution of $(x,y)$ where $x \sim \MD$ and $y \in \{0,1\}$ with $\EE[y\mid{}x] = f(\phiaug(x)]$. Let $\mu_0,\mu_1,\mu_2$ be the conditional distributions associated with $\MZ_0,\MZ_1,\MZ_2$. Finally, define $h_0,h_1,h_2$ by \[h_0((x^{(i)},y^{(i)})_{i=1}^m)(x,y) := \left(\frac{1}{m}\sum_{i=1}^m y^{(i)} -f(0)\right)^2,\]
\[h_1((x^{(i)},y^{(i)})_{i=1}^m)(x,y) := \left(\frac{1}{m}\sum_{i=1}^m y^{(i)} -f(1)\right)^2,\]
\[h_2((x^{(i)},y^{(i)})_{i=1}^m)(x,y) := \left(\MO((x^{(i)},y^{(i)})_{i=1}^m,\epsilon/6,\delta/6)(x) - f(\phiaug(x))\right)^2.\]
By Hoeffding's inequality, if $(x^{(i)},y^{(i)})_{i=1}^m$ are i.i.d. samples from $\mu_0$ and $m \geq 6\epsilon^{-1}\log(12/\delta)$, then since $\EE[y^{(i)}] = f(\phiaug(0)) = f(0)$, it holds with probability at least $1-\delta/6$ that \[\EE_{(x,y)\sim\mu_0}[h_0((x^{(i)},y^{(i)})_{i=1}^m)(x,y)] \leq \epsilon/6.\]
The same argument holds for $h_1$. Finally, if $(x^{(i)},y^{(i)})_{i=1}^m$ are i.i.d. samples from $\mu_2$ and $m \geq \Nreg(\epsilon/6,\delta/6)$, then by the assumption on $\MO$ and the fact that $\EE[y^{(i)}\mid{}x^{(i)}] = f(\phiaug(x^{(i)})) = f(\phi(x^{(i)}))$ since $x^{(i)} \in\MX$, it holds that with probability at least $1-\delta/6$,
\[\EE_{(x,y)\sim\mu_2}[h_2((x^{(i)},y^{(i)})_{i=1}^m)(x,y)] \leq \epsilon/6.\]
We conclude from \cref{lemma:compose-predictors} that if $n \geq \Nreg'(\epsilon,\delta) = C_{\ref{prop:oneaug}}\left(\epsilon^{-1}\Nreg(\epsilon/6,\delta/6) + \epsilon^{-2}\log(12/\delta)\right)$ and $C_{\ref{prop:oneaug}}$ is a sufficiently large constant, then with probability at least $1-\delta$ over i.i.d. samples $(x^{(i)},y^{(i)})_{i=1}^n$ from $\mu$, the function $H$ defined in \cref{lemma:compose-predictors} satisfies $\EE_{(x,y)\sim\mu}[H(x,y)] \leq \epsilon$. But we can write
\[H(x,y) = (\MR(x) - f(\phiaug(x)))^2\]
where
\[\MR(x) = \begin{cases} 
\frac{1}{\#\{i: x^{(i)}=0\}}\sum_{i:x^{(i)}=0} y^{(i)} & \text{ if } x = 0 \\ 
\frac{1}{\#\{i: x^{(i)}=1\}}\sum_{i:x^{(i)}=1} y^{(i)} & \text{ if } x = 1 \\
\MO((x^{(i)},y^{(i)})_{i: x^{(i)}\in\MX},\epsilon/6,\delta/6)(x) & \text{ if } x \in \MX
\end{cases}.\]
This is precisely the predictor computed by $\OneAug(\MO,(x^{(i)},y^{(i)})_{i=1}^n,\epsilon,\delta)$, so we have shown that $\OneAug(\MO,\cdot)$ is an $\Nreg'$-efficient one-context regression algorithm for $\Phiaug$. 
\end{proof}

\noindent The preceding proof used the following convenient technical lemma about composing statistical predictors on different subsets of a space:

\begin{lemma}\label{lemma:compose-predictors}
Let $\MZ$ be a set, and let $\MZ_1 \sqcup \dots \sqcup \MZ_k$ be a partition of $\MZ$. Let $\mu \in \Delta(\MZ)$ be a distribution. For each $i \in [k]$ let $\mu_i$ be the distribution of $z \sim \mu$ conditioned on $z \in \MZ_i$, and let $h_i: (\MZ_i)^\st \to (\MZ_i\to[0,1])$ be a function with the following property: given at least $m$ i.i.d. samples $(z^{(j)}_i)_j$ from $\mu_i$, it holds with probability at least $1-\delta$ that $\EE_{z\sim\mu_i}[h_i((z_i^{(j)})_j)(z)] \leq \epsilon$.

Let $(z^{(j)})_{j=1}^n$ be $n$ i.i.d. samples from $\mu$, and for each $i \in [k]$ let $S_i = \{j: z^{(j)} \in \MZ_i\}$. Define $H: \MZ \to [0,1]$ by
\[H(z) := h_i((z^{(j)})_{j \in S_i})(z) \text{ for } z \in \MZ_i.\]
If $n \geq 2m/\epsilon + 8\log(1/\delta)/\epsilon$, then with probability at least $1-2k\delta$, it holds that
\[\EE_{z \sim \mu}[H(z)] \leq 2k\epsilon.\]
\end{lemma}

\begin{proof}
Let $\Igood = \{i \in [k]: \mu(\MZ_i) \geq \epsilon\}$. Let $\ME$ be the event that $|S_i| \geq m$ for all $i \in \Igood$. For each such $i$, we have by a Chernoff bound and choice of $n$ that,
\[\Pr[|S_i| < m] \leq \Pr\left[|S_i| < \frac{1}{2} \mu(\MZ_i) n\right] \leq e^{-\mu(\MZ_i) n/8} \leq \delta.\]
Therefore $\Pr[\ME] \geq 1-\delta k$. Condition on $S_1,\dots,S_k$ and suppose that $\ME$ holds. For each $i \in \Igood$, the tuple $(z^{(j)})_{j \in S_i}$ consists of at least $m$ i.i.d. samples from $\mu_i$. Therefore by the lemma assumption, with probability at least $1-\delta k$, we have for all $i \in \Igood$ that
\[\EE_{z \sim \mu_i}[h_i((z^{(j)})_{j \in S_i})(z)] \leq \epsilon.\]
Condition additionally on this event. Then
\begin{align*}
\EE_{z \sim \mu}[H(z)]
&= \sum_{i = 1}^k \mu(\MZ_i) \EE_{z \sim \mu_i}[h_i((z^{(j)})_{j \in S_i}(z)] \\ 
&\leq \sum_{i\in\Igood} \mu(\MZ_i) \epsilon + \sum_{i \in [k]\setminus\Igood} \mu(\MZ_i) \\ 
&\leq (k+1)\epsilon
\end{align*}
which suffices for the claimed bound, and holds in an event with probability at least $1-2\delta k$.
\end{proof}

\subsubsection{The $\TwoAug$ Reduction}\label{sec:twoaug}

\begin{algorithm}[t]
	\caption{$\TwoAug(\MO, (x_1^{(i)},x_2^{(i)},y^{(i)})_{i=1}^n,\epsilon,\delta)$: Two-context regression for $\Phiaug$}
	\label{alg:twoaug}
	\begin{algorithmic}[1]\onehalfspacing
          \State \textbf{input:} Two-context regression oracle $\MO$ for $\Phi$; samples $(x_1^{(i)},x_2^{(i)},y^{(i)})_{i=1}^n$; tolerances $\epsilon,\delta$.
        \State Fix $\xbar \in \MX$.
		\State For all $i \in [n]$, define $\til x_1^{(i)} = x_1^{(i)}$ if $x_1^{(i)} \in \MX$, and otherwise $\xbar$. Similarly define $\til x_2^{(i)}$.
        \State For all pairs $B,B' \in \{\{0\},\{1\},\MX\}$, define
        \[\MR_{B,B'} \gets 
\MO((\til x_1^{(i)},\til x_2^{(i)},y^{(i)})_{i:x_1^{(i)}\in B, x_2^{(i)} \in B'},\epsilon/18,\delta/18).\]
        \State \textbf{return:} the predictor $\MR: \Xaug\times\Xaug \to [0,1]$ defined as follows. Given $x_1,x_2$, define $\til x_1 = x_1$ if $x_1 \in \MX$ and $\til x_1 = \xbar$ otherwise; similarly define $\til x_2$. Then output $\MR_{B,B'}(\til x_1,\til x_2)$ for the unique $B,B'$ with $x_1 \in B$ and $x_2 \in B'$.
	\end{algorithmic}
\end{algorithm}

The following proposition shows that $\TwoAug$ (\cref{alg:twoaug}) is an efficient reduction from two-context reduction for concept class $\Phiaug$ to the same problem for concept class $\Phi$. Similar to $\OneAug$, the reduction decomposes the regression problem into several parts, though now some of the parts are effectively one-context regression problems.

\begin{proposition}\label{prop:twoaug}
There is a constant $C_{\ref{prop:twoaug}}>0$ so that the following holds. Fix sets $\MX,\MS$ and $\Phi \subseteq (\MX \to \MS)$. Suppose that $\MO$ is an $\Nreg$-efficient two-context regression oracle for $\Phi$. Then $\TwoAug(\MO,\cdot)$ is an $\Nreg'$-efficient two-context regression oracle for $\Phiaug$ with
\[\Nreg'(\epsilon,\delta) = C_{\ref{prop:twoaug}}\left(\epsilon^{-1}\Nreg(\epsilon/18,\delta/18) + \epsilon^{-2}\log(36/\delta)\right).\]
\end{proposition}

\begin{proof}
Fix $\epsilon,\delta \in (0,1)$, $\MD \in \Delta(\Xaug\times\Xaug)$, $f: \Saug\times\Saug \to [0,1]$, and $\phiaug\in\Phiaug$. Suppose that $\MD$ is $\phiaug$-realizable (\cref{def:realizable-distribution}). By definition of $\Phiaug$, there is $\phi\in\Phi$ with $\phiaug = \aug(\phi)$. We invoke \cref{lemma:compose-predictors} with the following parameters. Set $\MZ := \Xaug \times\Xaug \times \{0,1\}$. Let $\mu \in \Delta(\MZ)$ be the distribution of $(x_1,x_2,y)$ where $(x_1,x_2) \sim \MD$ and $y \in \{0,1\}$ with $\EE[y\mid{}x_1,x_2] = f(\phiaug(x_1),\phiaug(x_2)]$. For each $B,B' \in \{\{0\},\{1\},\MX\}$ define $\MZ_{B,B'} := \MB\times\MB'\times \{0,1\}$, and let $\mu_{B,B'}$ be the associated conditional distribution. Fix $\xbar \in \MX$, and define $h_{B,B'}$ by
\begin{align*}
&h_{B,B'}((x_1^{(i)},x_2^{(i)},y^{(i)})_{i=1}^m)(x_1,x_2,y) \\ 
&:= \begin{cases}
\left(\MO((x_1^{(i)},x_2^{(i)},y^{(i)})_{i=1}^m,\epsilon/18,\delta/18)(x_1,x_2) - f(\phiaug(x_1),\phiaug(x_2))\right)^2 & \text{ if } B,B' = \MX \\ 
\left(\MO((x_1^{(i)},\xbar,y^{(i)})_{i=1}^m,\epsilon/18,\delta/18)(x_1,\xbar) - f(\phiaug(x_1),\phiaug(x_2))\right)^2 & \text{ if } B = \MX, B' \neq \MX \\
\left(\MO((\xbar,x_2^{(i)},y^{(i)})_{i=1}^m,\epsilon/18,\delta/18)(\xbar,x_2) - f(\phiaug(x_1),\phiaug(x_2))\right)^2 & \text{ if } B \neq \MX, B' = \MX \\
\left(\MO((\xbar,\xbar,y^{(i)})_{i=1}^m,\epsilon/18,\delta/18)(\xbar,\xbar) - f(\phiaug(x_1),\phiaug(x_2))\right)^2 & \text{ if } B,B' \neq \MX
\end{cases}.
\end{align*}
Fix $B,B'$. Let $(x_1^{(i)},x_2^{(i)},y^{(i)})_{i=1}^m$ be i.i.d. samples from $\mu_{B,B'}$ and suppose $m \geq \Nreg(\epsilon/18,\delta/18)$. If $B,B'=\MX$ then $\EE[y^{(i)}\mid{}x_1^{(i)},x_2^{(i)}] = f(\phiaug(x_1^{(i)}),\phiaug(x_2^{(i)})) = f(\phi(x_1^{(i)}),\phi(x_2^{(i)}))$. Moreover, the marginal distribution of $(x_1^{(i)},x_2^{(i)})$ is $\phi$-realizable since it can be expressed as the conditional distribution of $\MD$ under the event that $\phiaug(x_1^{(i)}),\phiaug(x_2^{(i)}) \in \MS$. Thus, by the assumption on $\MO$, it holds with probability at least $1-\delta/18$ that
\[\EE_{(x_1,x_2,y)\sim\mu_{B,B'}}\left[\left(\MO((x_1^{(i)},x_2^{(i)},y^{(i)})_{i=1}^m,\epsilon/18,\delta/18)(x_1,x_2) - f(\phiaug(x_1),\phiaug(x_2))\right)^2 \right] \leq \epsilon/18.\]
If $B=\MX,B'=\{0\}$ then $\EE[y^{(i)}\mid{}x_1^{(i)},\xbar] = \EE[y^{(i)}\mid{}x_1^{(i)}] = f(\phi(x_1^{(i)}),0)$ since $x_2^{(i)}=\phiaug(x_2^{(i)})=0$ is fixed under $\mu_{B,B'}$. Moreover, the marginal distribution of $(x_1^{(i)},\xbar)$ is $\phi$-realizable since $\xbar$ is fixed and hence independent of $x_1^{(i)})$. Thus, by the assumption on $\MO$, it holds with probability at least $1-\delta/18$ that 
\begin{align*}
&\EE_{(x_1,x_2,y)\sim\mu_{B,B'}}\left[\left(\MO((x_1^{(i)},x_2^{(i)},y^{(i)})_{i=1}^m,\epsilon/18,\delta/18)(x_1,x_2) - f(\phiaug(x_1),\phiaug(x_2))\right)^2 \right] \\
&= \EE_{(x,y)\sim\mu_{B,B'}}\left[\left(\MO((x_1^{(i)},x_2^{(i)},y^{(i)})_{i=1}^m,\epsilon/18,\delta/18)(x_1,x_2) - f(\phi(x_1),0)\right)^2 \right] \\
&\leq \epsilon/18.
\end{align*}
The remaining cases follow by analogous arguments. Thus, we can apply \cref{lemma:compose-predictors}. If $n \geq \Nreg'(\epsilon,\delta) = C_{\ref{prop:twoaug}}(\epsilon^{-1}\Nreg(\epsilon/18,\delta/18)+\epsilon^{-1}\log(36/\delta))$, where $C_{\ref{prop:twoaug}}$ is a sufficiently large constant, then with probability at least $1-\delta$ over i.i.d. samples $(x_1^{(i)},x_2^{(i)},y^{(i)})_{i=1}^n$ from $\mu$, the function $H$ defined in \cref{lemma:compose-predictors} satisfies $\EE_{(x_1,x_2,y)\sim\mu}[H(x_1,x_2,y)] \leq \epsilon$. But we can write
\[H(x_1,x_2,y) = (\MR(x) - f(\phiaug(x_1),\phiaug(x_2)))^2\]
where
\[\MR(x_1,x_2) = \begin{cases} 
\MO((x_1^{(i)},x_2^{(i)},y^{(i)})_{i:x_1^{(i)},x_2^{(i)}\in\MX},\epsilon/18,\delta/18)(x_1,x_2) & \text{ if } x_1,x_2 \in \MX \\ 
\MO((x_1^{(i)},\xbar,y^{(i)})_{i:x_1^{(i)}\in\MX,x_2^{(i)}=x_2},\epsilon/18,\delta/18)(x_1,\xbar) & \text{ if } x_1 \in \MX, x_2\not \in\MX \\
\MO((\xbar,x_2^{(i)},y^{(i)})_{i:x_1^{(i)}=x_1,x_2^{(i)}\in\MX},\epsilon/18,\delta/18)(\xbar,x_2) & \text{ if } x_1 \not\in \MX, x_2 \in\MX \\
\MO((\xbar,\xbar,y^{(i)})_{i:x_1^{(i)}=x_1,x_2^{(i)}=x_2},\epsilon/18,\delta/18)(\xbar,\xbar) & \text{ if } x_1, x_2\not \in\MX
\end{cases}.\]
This is exactly the predictor computed by $\TwoAug(\MO,(x_1^{(i)},x_2^{(i)},y^{(i)})_{i=1}^n,\epsilon,\delta)$, so we have shown that $\TwoAug(\MO,\cdot)$ is an $\Nreg'$-efficient two-context regression algorithm for $\Phiaug$.
\end{proof}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
