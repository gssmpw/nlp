\nc{\RVFS}{\texttt{RVFS}}
%In classical PAC learning, an estimator's performance can be theoretically benchmarked by two quantities: sample efficiency and computational efficiency. The advent of deep learning demonstrated the limited practical value of these benchmarks for natural supervised learning tasks: deep learning heuristics work well but lack provable statistical guarantees. Even worse, for most interesting tasks, sample efficiency and computational efficiency appear to be incompatible. This issue is only exacerbated in more complex domains such as \emph{reinforcement learning}.

An overarching paradigm in modern machine learning is to reduce a complex task of interest to a simpler supervised learning task. Instances of this paradigm range from language modeling \citep{openai2023} and image generation \citep{song2019generative} to imitation learning \citep{bojarski2016end}. More broadly, the basic ansatz from supervised learning \--- that gradient descent finds good minimizers \--- underlies empirical progress in reinforcement learning \citep{mnih2015human}, artificial intelligence for games \citep{silver2018general}, and much more. State-of-the-art learning methods may not admit provable guarantees, but their empirical success is intuitively (to varying extents) justified by the basic ansatz.\loose

From a theoretical perspective, this ansatz can be exploited via \emph{oracle-efficient} algorithm design. Formally, an algorithm is oracle-efficient with respect to an oracle $\MO$ if it is computationally efficient and provably correct when given query access to $\MO$. Oracle-efficiency has been a cornerstone in the theory of online learning since the development of Follow-the-Perturbed-Leader \citep{kalai2005efficient}, which solves online linear optimization by reduction to an offline linear optimization oracle over the decision set. Recently, oracle-efficiency has become ubiquitous in theoretical reinforcement learning \citep{dann2018oracle,du2019provably,misra2020kinematic,foster2021statistical,mhammedi2023efficient,hussing2024oracle}, where end-to-end computational efficiency is often out-of-reach for settings with large observation spaces \citep{kane2022computational,golowich2024exploration}, yet heuristics based on deep learning (e.g., for estimation of value functions or transition dynamics) could plausibly work well on natural data. In reinforcement learning, as with online learning \citep{hazan2016computational,dudik2020oracle} and decision making \citep{agarwal2014taming,foster2020beyond}, the lens of oracle-efficiency has spurred numerous algorithmic improvements.

Yet, in online learning, the correct \emph{choice of oracle} was fairly clear: to solve an \emph{online} optimization problem, assume access to an oracle that solves the corresponding \emph{offline} optimization problem \citep{kalai2005efficient,hazan2016computational}. As observed by \cite{kalai2005efficient}, this assumption is essentially without loss of generality, since an online optimization algorithm must solve offline optimization as a special case. In contrast, for reinforcement learning---a more complex and structured setting, due to the interaction between the agent and environment---there is no such consensus about the ``right'' computational oracles, even for models that are by now well-established. Instead, oracle-efficiency has largely been used as a black-and-white prognostic, just separating ``reasonable'' algorithms from those requiring exhaustive enumeration \citep{dann2018oracle}.

In this work, we take a finer-grained view of oracle-efficiency---for example, are supervised learning oracles that perform regression onto value functions sufficient, or must we estimate more complex objects such as\arxiv{ (forward or inverse)} transition dynamics? Since access to an oracle is fundamentally an assumption, we are interested in the following question:
\colt{
\emph{What are the weakest computational oracles that suffice for oracle-efficient reinforcement learning?}}
\arxiv{ 
\begin{center} 
\emph{What are the weakest computational oracles that suffice for oracle-efficient reinforcement learning?}
\end{center}

}

In other words, while prior works have largely focused on the impact of differing structural assumptions on statistical complexity, we focus on the impact on computational complexity, as measured by the oracle strength. To begin this investigation, we study the task of exploration in \emph{Block Markov Decision Processes} \citep{du2019provably}---one of the most well-studied families of Markov Decision Processes (MDPs) with rich observation spaces. We identify the first \emph{minimal oracle} \citep{golowich2024exploration} for this task, under the standard episodic access model. We then consider the reset access model, and show that a strictly weaker oracle suffices. Moving beyond Block MDPs, we give cryptographic evidence of a qualitative computational separation between Block MDPs and the more general setting of \emph{Low-Rank} MDPs. %Beyond identifying minimal oracles for Block MDPs, we provide insights into challenges for identifying minimal oracles in more general RL settings with function approximation.



\iffalse
, and , with instantiations in text The components of supervised learning methods are ubiquitous in modern algorithms for reinforcement learning (RL). Empirical RL approaches use gradient descent to directly learn value functions \citep{mnih2015human, silver2018general} or optimize a related loss \citep{schulman2017proximal}. In theoretical RL, it is common to assume access to various optimization oracles, and to ask for computational efficiency modulo these oracle calls \citep{dann2018oracle, du2019provably, mhammedi2023efficient}.

The oracle models of computation have compelling motivation. End-to-end computational efficiency is typically out of reach for RL with large observation spaces \citep{papadimitriou1987complexity, kane2022computational}, with intractability often inherited from computational lower bounds in supervised (PAC) learning \citep{golowich2024exploring}. Since deep learning heuristics have a strong empirical track record on natural optimization problems, optimization oracles can be thought of as theoretical tools for avoiding worst-case intractability via \emph{implicit} distributional assumptions about the data. This paradigm of oracle-efficient algorithm design has been productive not just in RL but throughout theoretical machine learning \citep{hazan2016computational, ben2015oracle, foster2020beyond}.

But it is crucial to remember that an oracle assumption is still an \emph{assumption}. Which algorithm is best? It depends, among other things, on which assumption is most reasonable. The common practice in theoretical RL is to evaluate oracle assumptions solely based on empirical wisdom and in black-and-white: either the optimization problem looks amenable to gradient descent, or not. Yet the design space of optimization problems is vast. Given the dearth of fine-grained empirical understanding of when gradient descent succeeds \citep{wen2024sharpness}, is there an informative theoretical metric?
\fi



%\dhruv{informal discussion of ``weakest possible oracle'' that suffices for computationally efficient RL}

%\dhruv{idea: try to give self-contained motivation}

%\paragraph{Minimal oracles for RL?} Recent work \citep{golowich2024exploration} proposed a more nuanced evaluation metric based on computational complexity: the best oracle for an RL task is one that is equivalent to the RL task under polynomial-time reductions. They termed such an oracle \emph{minimal}, and gave cryptographic evidence that the most natural PAC learning oracle is \emph{not} minimal for RL in Block Markov Decision Processes. However, they did not construct a minimal oracle, or even show that one exists.

\subsection{Background: Block MDPs and Computational Oracles}

A finite-horizon Markov Decision Process (MDP) is defined by a set of \emph{states}, a set of \emph{actions}, and an unknown transition function, which describes how the environment changes state as a result of the agent's actions. The agent learns by repeated interaction with the environment---the two most common interaction frameworks are episodic \citep{kearns2002near} and resets \citep{weisz2021query}; we study both.\footnote{We specify ``episodic RL'' to distinguish from ``RL with resets''; we say ``RL'' when the distinction is unimportant.} We focus on reward-free RL \citep{du2019provably,jin2020reward}, where the goal is \emph{exploration}: finding a set of \emph{policies} (i.e. mappings from states to actions) that cover the entire state space as well as possible. When the state space is extremely large, structural assumptions are needed to avoid statistical intractability: For most of this paper, we focus on the \emph{Block MDP} \citep{du2019provably,misra2020kinematic}, a canonical setting for RL with function approximation in which the rich observed dynamics are governed by a small (unobserved) \emph{latent state space}; just like in PAC learning, a concept class is required to model the mapping from observed states to latent states. %Formal preliminaries are deferred to \cref{sec:prelim}.

\begin{definition}[Informal; see \cref{sec:prelim}]
  Let $\MX$, $\MS$, and $\MA$ be sets and let $\Phi$ be a \emph{concept class} of functions $\phi: \MX \to \MS$.\footnote{$\Phi$ is often referred to as a \emph{decoder class} in prior work; we use \emph{concept class} in analogy with PAC learning.}
  An MDP with state space $\MX$ and action space $\MA$ is a \textbf{$\Phi$-decodable Block MDP} with \emph{latent state space} $\MS$ if there is a function $\phi^\st\in\Phi$ such that for any two states $x,x'\in\MX$ and action $a \in\MA$, the transition probability from $x$ to $x'$ when the agent plays action $a$ depends only on $\phi^\st(x)$, $\phi^\st(x')$, and $a$; we refer to $\phi^{\st}(x)$ as the \emph{latent state}.\loose
\end{definition}

Henceforth, we refer to $\MX$ as the \emph{observed state space} or \emph{observation space} to distinguish from the latent state space. The concept class $\Phi$ is known to the learner, but the true decoder $\phi^{\star}\in\Phi$ is not. The statistical complexity of reward-free exploration scales polynomially in $|\MS|$, $|\MA|$, and $\log|\Phi|$ \citep{jiang2017contextual}---and crucially has no dependence on $|\MX|$, which should be thought of as exponentially large or even infinite. The \emph{computational} complexity is much more subtle. Initial algorithms required enumeration over $\Phi$ \citep{jiang2017contextual}; subsequent investigation identified oracle-efficient algorithms with respect to several different optimization oracles \citep{misra2020kinematic,zhang2022efficient,mhammedi2023representation,mhammedi2023efficient}, but no basis for comparison between these oracles has been proposed. The first works to raise the question of which oracles are \emph{necessary} were \citet{golowich2024exploring,golowich2024exploration}, who studied the \emph{one-context (realizable) regression} problem:\loose
%\vspace{-0.07em}
\begin{definition}[informal; see \Cref{def:one-con-regression}]\label{def:ocr-informal}
Fix a concept class $\Phi$. Let $(x^{(i)},y^{(i)})_{i=1}^n$ be i.i.d., with $\EE[y^{(i)}\mid{}x^{(i)}] = f(\phi^\st(x^{(i)}))$ for some unknown $f: \MS \to [0,1]$ and $\phi^\st \in \Phi$. The goal of \textbf{one-context regression} is to compute a predictor $\MR: \MX \to [0,1]$ that approximates $x \mapsto f(\phi^\st(x))$.
\end{definition}

Intuitively, one-context regression can be thought of as a regression with a \emph{well-specified model} \citep{tsybakov2009nonparametric,wainwright2019high}, as the true target function depends only on the latent state $\phi^{\star}(x)$ (it can also be viewed as a generalization of PAC learning with random classification noise---see \cref{remark:ocr-to-pac}). This oracle is well-suited for estimating objects such as value functions---which depend only on the latent state in the Block MDP---and it has been implicitly used as a subroutine in many algorithms \citep{foster2020beyond,zhang2022efficient,mhammedi2023representation,mhammedi2023efficient}.\footnote{Information-theoretically,  $\text{MSE}\lesssim\frac{\abs{\cS} + \log(\abs{\Phi}\delta^{-1})}{n}$ is always possible, but not necessarily computationally efficiently.}

For any concept class $\Phi$, one-context regression is \emph{necessary} for episodic RL, i.e. there is a Cook reduction from regression to episodic RL \citep{golowich2024exploring}, so as an oracle assumption, it is without loss of generality. Unfortunately, one-context regression is also \emph{insufficient}: under a standard cryptographic assumption, there exists a concept class $\Phi$ for which there is no Cook reduction from episodic RL to regression \citep{golowich2024exploration}. Thus, one-context regression is not a minimal oracle for episodic RL.  This motivates us to consider the problem of \emph{two-context regression}. \loose

\begin{definition}[informal; see \Cref{def:two-con-regression}]\label{def:tcr-informal}
Fix a concept class $\Phi$. Let $(x_1^{(i)},x_2^{(i)},y^{(i)})_{i=1}^n$ be i.i.d., with $\EE[y^{(i)}\mid{}x_1^{(i)},x_2^{(i)}] = f(\phi^\st(x_1^{(i)}),\phi^\st(x_2^{(i)}))$ for some unknown $f: \MS\times\MS \to [0,1]$ and $\phi^\st \in \Phi$. The goal of \textbf{two-context regression} is to compute a predictor $\MR: \MX\times\MX \to [0,1]$ that approximates $(x_1,x_2)\mapsto f(\phi^\st(x_1),\phi^\st(x_2))$.
\end{definition}

Several RL algorithms use variants of this oracle \citep{misra2020kinematic,mhammedi2023representation}---roughly, to estimate (inverse) \emph{transition dynamics}---and it has been suggested that these variants may be essentially minimal \citep{golowich2024exploration}, but no evidence for this belief was known prior to this work. See \cref{sec:app_related} for a detailed discussion of prior work. %The remainder of this work will study reductions between RL and the above computational problems.

\colt{%\vspace{-0.05em}
\begin{remark}\label{remark:oracle-subtleties}
The above oracles are phrased in terms of (1) statistical learning rather than worst-case optimization, and (2) improper learning rather than proper. These distinctions were rarely made in prior work, but are important from a complexity-theoretic lens---see \cref{remark:oracle-subtleties-app} for details.
\end{remark}
%\vspace{-0.5em}
}

\arxiv{
\begin{remark}[Optimization vs. learning; proper vs. improper]\label{remark:oracle-subtleties-app}
  Many prior works in oracle-efficient RL assume access to \emph{optimization} oracles rather than statistical learning oracles. Informally, the former oracles require solving regression problems analogous to \cref{def:ocr-informal,def:tcr-informal} for \emph{arbitrary datasets} as opposed to i.i.d. and realizable datasets. This is primarily a conceptual distinction rather than technical, since in many cases a learning oracle can easily be substituted in \citep{misra2020kinematic,mhammedi2023representation}.
  However, it is important from a complexity-theoretic perspective, since statistical learning can often be substantially easier \citep{blum1998polynomial}. 
  
  A more technically salient distinction is that our definitions above allow for \emph{improper learning}, whereas almost all prior works in oracle-efficient RL for Block MDPs require \emph{proper} learning oracles\footnote{For example, the proper learning analogue of \Cref{def:ocr-informal} requires computing some predictor $\MR:\MX\to[0,1]$ with an explicit decomposition $\MR = \MR'\circ \phi$ for some $\phi \in \Phi$ and $\MR': \MS \to [0,1]$.}---an exception is the work of \cite{misra2020kinematic}, which our algorithmic results directly build on. There are many concept classes for which proper learning is $\NP$-hard, but it is considered unlikely for improper learning to be $\NP$-hard \citep{applebaum2008basing}. Since the goal of RL is to output policies, which are fundamentally improper, it seems unlikely that a proper supervised learning task could be reduced to RL (in the manner of results such as \cref{cor:regression-to-online-rl}).
\end{remark}
}



\subsection{Contributions}

We clarify the computational complexity of RL via the lens of oracle-efficiency, identifying (1) the first minimal oracle for episodic RL in Block MDPs, (2) provable benefits of \emph{reset access}, and (3) computational challenges of the more general \emph{Low-Rank} MDPs. See \cref{sec:discussion} for open questions.

\paragraph{A minimal oracle for episodic RL in Block MDPs (\Cref{sec:online}).} We show that for \emph{every} concept class $\Phi$, under a mild regularity condition, two-context regression is a minimal oracle---both \emph{sufficient} and \emph{necessary}---for reward-free episodic RL in $\Phi$-decodable Block MDPs. To show sufficiency, we generalize and simplify the algorithm $\HOMER$ of \cite{misra2020kinematic}, eliminating their reachability assumption and implementing their oracles with two-context regression. To show necessity, we give a novel reduction \emph{from} two-context regression \emph{to} reward-free RL, which simulates interaction with an appropriate MDP and ``stitches together'' the exploratory policies into a predictor. %prove that given access to an algorithm for reward-free RL in any $\Phi$-decodable Block MDP, there is a computationally efficient algorithm for two-context regression with $\Phi$.

%\arxiv{. There are two parts to this result:\loose
%\begin{enumerate}
%\item On the algorithmic side, we generalize and simplify the algorithm \texttt{HOMER} of \cite{misra2020kinematic}, eliminating their \emph{reachability assumption} by means of an algorithmic truncation technique from \cite{golowich2024exploring}, and showing that the algorithm can be implemented using only (improper) two-context regression. This proves that two-context regression is a \emph{sufficient} oracle.\loose
%\item On the complexity side, we prove that given access to an algorithm for reward-free RL in any $\Phi$-decodable Block MDP, there is a computationally efficient algorithm for two-context regression with $\Phi$. This shows that two-context regression is a \emph{necessary} oracle.
%\end{enumerate}}

\paragraph{A provable computational benefit for reset access in Block MDPs (\Cref{sec:resets}).} Under the same regularity condition on $\Phi$ as before, we show that \emph{one-context regression} is a sufficient (and nearly necessary) oracle for reward-free RL in $\Phi$-decodable Block MDPs, given the additional ability to \emph{reset} to previously observed states \citep{li2021sample,yin2022efficient,
  mhammedi2024power}. Combined with the recent work of \cite{golowich2024exploration}, our result gives strong evidence that reset access has computational benefits over episodic access. %Additionally, while we do not know whether this oracle is necessary for reward-free RL with resets, the \emph{noiseless} version \emph{is} necessary (\cref{thm:one-context-reduction}), suggesting that this oracle is near-minimal in natural settings.

Our algorithm uses a variant of the \emph{inverse kinematics} objective \citep{misra2020kinematic,mhammedi2023representation}, but exploits reset access to simplify the computational oracle. Previously, one-context regression was only known to be sufficient for Block MDPs with horizon $1$ or with deterministic dynamics \citep{golowich2024exploration}; the closest prior work is $\RVFS$ \citep{mhammedi2024power}, which solves RL with resets in general Block MDPs but requires an \emph{agnostic}/cost-sensitive regression oracle.\loose

\paragraph{A computational separation between Block MDPs and Low-Rank MDPs (\Cref{sec:lowrank}).} There has been recent progress on oracle-efficient algorithms for \emph{Low-Rank} MDPs \citep{modi2021model,zhang2022efficient,mhammedi2023efficient}, of which Block MDPs are a special case. However, these algorithms seemingly require a much more complex oracle. Do analogues of one- or two-context regression suffice for exploration in Low-Rank MDPs? We show that the analogue of one-context regression is cryptographically \emph{insufficient} for exploration in Low-Rank MDPs under reset access, thereby separating Low-Rank MDPs from Block MDPs. Conceptually, this separation arises from the same source as cryptographic hardness of agnostic halfspace learning \citep{tiegel2023hardness}, and points to the lack of \emph{weight function realizability} in Low-Rank MDPs as a potential computational barrier. 



%\dhruv{picture of graphical models}

%\dfc{some high-level comments on the main body (not this section but subsequent ones)
%  \begin{itemize}
%  \item I think it would be good to add 1-2 sentences after each of the main theorems spelling out the consequences informally again.
%  \item Since algos are in the appendix, it seems important to (1) have dedicated subsections with the algo boxes so people can easily find them, and (2) add forward references to these subsections from the main body.
%  \end{itemize}
%  }

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
