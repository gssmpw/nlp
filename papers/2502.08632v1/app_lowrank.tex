
% \dfedit{Recall that for Generalized Block MDPs, one-context regression (\cref{Def:low-rank-reg}) simplifies back to our original definition of one-context regression (\cref{def:one-con-regression}).}
In this section, we re-introduce Generalized Block MDPs, and verify (\cref{prop:genblock-is-lowrank}) that generalized block MDPs are a special case of low-rank MDPs. Then, in \cref{sec:halfspace-ocr}, we prove \cref{thm:halfspace-reg}, which asserts that one-context regression is computationally tractable for the concept class $\Phi_n$ defined by halfspaces. In \cref{sec:halfspace-hardness}, we prove \cref{thm:halfspace-rl-hard}, which asserts that reward-free RL for $\Phi_n$-decodable generalized block MDPs is computationally \emph{hard}. Together, \cref{thm:halfspace-reg,thm:halfspace-rl-hard} immediately imply \cref{thm:halfspace-main}. Finally, in \cref{sec:halfspace-statistical}, we prove \cref{prop:halfspace-rl-statistical}, which asserts that reward-free RL in the family of Generalized Block MDPs (and hence Low-Rank MDPs) $\MM_n$ is \emph{statistically} tractable---and hence the preceding hardness result is purely a \emph{computational} phenomenon.

\subsection{Preliminaries}\label{sec:low-rank-prelim}

\paragraph{One-context low-rank regression.} We will not actually need a formal definition of one-context low-rank regression in our results, but we introduce it formally for the sake of discussion:

\begin{definition}\label{def:low-rank-reg-app}
Fix $\Nreg: (0,1/2) \to \NN$. An algorithm $\Alg$ is an $\Nreg$-efficient \emph{one-context low-rank regression} algorithm for a feature class $\Philin \subseteq (\MX \times \MA \to \RR^d)$ if the following holds. Fix $\epsilon,\delta \in (0,1/2)$, $n \in \NN$, $\philin\in\Philin$, $\MD\in\Delta(\MX\times\MA)$, and $\theta \in \RR^d$. Let $(x^{(i)},a^{(i)},y^{(i)})_{i=1}^n$ be i.i.d. samples with $(x^{(i)},a^{(i)})\sim\MD$, $y^{(i)} \in \{0,1\}$, and $\EE[y^{(i)}\mid{}x^{(i)},a^{(i)}] = \langle \philin(x^{(i)},a^{(i)}),\theta\rangle$. If $n \geq \Nreg(\epsilon,\delta)$, then with probability at least $1-\delta$, the output of $\Alg((x^{(i)},a^{(i)},y^{(i)})_{i=1}^n,\epsilon,\delta)$ is a circuit $\MR: \MX\times\MA\to[0,1]$ satisfying \[\EE_{(x,a)\sim\MD} (\MR(x,a) - \langle \philin(x,a),\theta\rangle)^2 \leq \epsilon.\]
\end{definition}

\paragraph{Generalized Block MDPs.} Recall from \cref{sec:lowrank} that  for sets $\MS,\MX$ and a concept class $\Phi \subseteq (\MX\to\MS)$, a \emph{Generalized} $\Phi$-decodable Block MDP is an MDP $M = (H, \MX,\MA,(\BP_h)_h)$ with the property that there exists some $\phist_1,\dots,\phist_H \in \Phi$ so that $\BP_{h+1}(x_{h+1}\mid{}x_h,a_h)$ is a function of $x_{h+1}$, $\phist_h(x_h)$, and $a_h$. 

The following proposition makes formal the observation from \cref{sec:lowrank} that Generalized Block MDPs (and, as a special case, Block MDPs themselves) are Low-Rank MDPs with an appropriate feature class:

\begin{proposition}\label{prop:genblock-is-lowrank}
Set $d = |\MS||\MA|$ and identify $[d] \equiv \MS\times\MA$. Define $\Philin := \{\philin: \phi \in \Phi\}$, where $\philin: \MX\times\MA \to \RR^d$ is defined by
\[\philin(x,a) := e_{\phi(x), a}.\]
Then any Generalized $\Phi$-decodable Block MDP $M$ is a Low-Rank MDP \citep{agarwal2020flambe,mhammedi2023efficient} with features $(\phist_h)_h \subset \Philin$ and dual features $(\must_h)_h$ where $(\must_h)_{s,a} \in \Delta(\MX)$ for all $h\in[H]$, $s\in\MS$, and $a\in\MA$.
\end{proposition}

\begin{proof}
Let $\phist_1,\dots,\phist_H$ be the decoding functions for $M$. For each $h \in [H-1]$ and $(s_h,a_h) \in \MS\times\MA$, we fix any $\xbar_h \in \MX$ with $\phist_h(\xbar_h)=s_h$ and define $\must_{h+1}(x_{h+1})_{s_h,a_h} = \BP_{h+1}(x_{h+1}\mid{}\xbar_h,a_h)$. Then for any $x_h,x_{h+1} \in \MX$ with $\phist_h(x_h) = s_h$, we have
\begin{align*}
\BP_{h+1}(x_{h+1}\mid{}x_h,a_h) 
&= \BP_{h+1}(x_{h+1}\mid{}\xbar_h,a_h) \\ 
&= \langle e_{s_h,a_h}, \must_{h+1}(x_{h+1})\rangle \\ 
&= \langle \philin_h(x_h,a_h), \must_{h+1}(x_{h+1})\rangle
\end{align*}
where $\philin_h\in\Philin$ is defined by $\philin_h(x,a) := e_{\phist_h(x),a}$. The first equality above used the Generalized Block MDP assumption.
\end{proof}

The following proposition asserts that for Generalized Block MDPs, when viewed as a special case of Low-Rank MDPs, the one-context \emph{low-rank} regression problem reduces to one-context regression. Since this fact is only important for purposes of motivation and discussion, we omit the formal statement (i.e. parameters of the reduction), and simply remark that the reduction proceeds by independently solving a one-context regression problem for each action. The proof of correctness would proceed similarly to e.g. that of \cref{prop:twoaug}.

\begin{proposition}[Informal]\label{prop:gen-block-ocr}
For any concept class $\Phi \subseteq (\MX\to\MS)$ and action space $\MA$, let $\Philin \subseteq (\MX\times\MA\to\RR^d)$ be the feature class defined in terms of $\Phi$ as specified in \cref{prop:genblock-is-lowrank}. Then there is a polynomial-time reduction from one-context low-rank regression for $\Philin$ (\cref{def:low-rank-reg-app}) to one-context regression for $\Phi$ (\cref{Def:low-rank-reg}).  
\end{proposition}

For the results in \cref{sec:halfspace-hardness}, it is convenient to introduce the following class of $\Phi_n$-decodable Generalized Block MDPs, where $\Phi_n$ is the class of linear threshold functions, as previously defined in \cref{sec:lowrank}.

\begin{definition}\label{def:halfspace-mdps-apx}
Fix $n \in \NN$. We define $\MM_n$ to be the family of generalized $\Phi_n$-decodable block MDPs with horizon $H := (\log n)^{\log \log n}$, observation space $\MX = \RR^n$, latent state space $\MS = \{0,1\}$, action space $\MA = \{0,1\}$, and feature class $\Phi_n = \{\phi^\theta: \theta \in \RR^n\}$ consisting of linear threshold functions, i.e. where 
\[\phi^\theta(x) := \mathbbm{1}[\langle x,\theta\rangle \geq 0].\]
\end{definition}

The hardness result in \cref{thm:halfspace-main} will be against (a subset of) $\MM_n$. Observe that $H \leq n$ and $|\MA| = 2$, so to rule out an RL algorithm for $\Phi_n$-decodable Generalized Block MDPs with time complexity $\poly(n,H,|\MA|)$, it suffices to rule out an RL algorithm for $\MM_n$ with time complexity $\poly(n)$.


\subsection{Tractability of One-Context Regression}\label{sec:halfspace-ocr}

We start by showing that one-context regression with concept class $\Phi_n$ can be solved with time complexity $\poly(n)$. Note that $n$ is the description length of an element of the observation space $\MX$ (ignoring issues of finite precision arithmetic). To prove \cref{thm:halfspace-reg}, recall that PAC learning of halfspaces with random classification noise is computationally tractable \citep{blum1998polynomial,diakonikolas2023strongly}. The only difference between this problem and one-context regression for $\Phi_n$ is that in the latter setting, the noise levels for the two classes may be different. To reduce the latter to the former, we apply a careful symmetrization step; a priori this seems to require knowing the noise levels, but this can be avoided by gridding over all possibilities.

\begin{theorem}\label{thm:halfspace-reg}
There is a constant $C_{\ref{thm:halfspace-reg}}>0$ so that the following holds. There is an $\Nreg$-efficient algorithm for one-context regression with concept class $\Phi_n$, where
\[\Nreg(\epsilon,\delta) = (n/\epsilon)^{C_{\ref{thm:halfspace-reg}}} \log^2(1/\delta).\]
Moreover, the time complexity of the algorithm with error tolerance $\epsilon$ and failure probability $\delta$ is $\poly(n,1/\epsilon,1/\delta)$.
\end{theorem}

\begin{proof}
We define a one-context regression algorithm $\Alg$ as follows. We are given, as input, samples $(x^{(i)},y^{(i)})_{i=1}^m$ with $x^{(i)} \in \RR^n$ and $y^{(i)} \in \{0,1\}$, and parameters $\epsilon,\delta\in(0,1/2)$. Set $\epdisc := \cdisc \delta/m$ for a universal constant $\cdisc>0$ that will be determined in the analysis. Let $\MG$ be a set of functions $g:\{0,1\} \to [0,1]$ such that $|\MG| \leq O(1/\epdisc^2)$ and for any $(a,b) \in [0,1]^2$ there is some $g \in \MG$ with $|a-g(0)|+|b-g(1)| \leq \epdisc$.

For each $g \in \MG$, we compute a predictor $\MR_g: \RR^n \to [0,1]$ as follows. First, we define $(y_g^{(i)})_{i=1}^{m/2}$ as follows:
\begin{itemize}
    \item If $g(0)+g(1) \geq 1$, then draw
    \[y_g^{(i)} \sim \begin{cases} \Ber\left(\frac{g(0)+g(1)-1}{g(0)+g(1)}\right) & \text{ if } y^{(i)} = 0 \\ 
    \Ber(1) & \text{ if } y^{(i)} = 1 
    \end{cases}.\]
    \item If $g(0) + g(1) < 1$, then draw 
    \[y_g^{(i)} \sim \begin{cases} 
    \Ber(0) & \text{ if } y^{(i)} = 0 \\ 
    \Ber\left(\frac{1-g(0)-g(1)}{2 - g(0)+g(1)}\right) & \text{ if } y^{(i)} = 1 
    \end{cases}.\]
\end{itemize}
Let $\halfspacealg$ denote the halfspace learning algorithm guaranteed by \cite[Theorem 1.8]{diakonikolas2023strongly}. Invoke $\halfspacealg$ on dataset $(x^{(i)}, y_g^{(i)})_{i=1}^{m/2}$, which returns a hypothesis $h_g^+: \RR^n \to \{0,1\}$. Define $\MR_g^+: \RR^n \to [0,1]$ by $\MR_g^-(x) = g(h_g^+(x))$. Similarly define $h_g^-$ by invoking $\halfspacealg$ on $(x^{(i)}, 1-y_g^{(i)})_{i=1}^{m/2}$, and define $\MR_g^-: \RR^n \to [0,1]$ by $\MR_g^-(x) = g(h_g^-(x))$.

Finally, output $\MR_{\wh g}^{\wh b}$ with $\wh g, \wh b$ defined by
\[(\wh g, \wh b) := \argmin_{g \in \MG, b \in \{-,+\}} \sum_{i=m/2+1}^m (\MR_g^b(x^{(i)}) - y^{(i)})^2.\]

\paragraph{Analysis.} Let $m \geq (n/\epsilon)^{C_{\ref{thm:halfspace-reg}}} \log^2(1/\delta)$. Suppose that $(x^{(i)},y^{(i)})_{i=1}^m$ are i.i.d. samples with $\EE[y^{(i)}|x^{(i)}] = f(\phi(x^{(i)}))$ for some $\phi \in \Phi$ and $f: \{0,1\} \to [0,1]$. Suppose that $f(0) + f(1) \geq 1$; the argument in the other case is similar. By construction, there is some $g \in \MG$ such that $g(0) + g(1) \geq 1$ and $|f(0)-g(0)| + |f(1)-g(1)| \leq O(\epdisc)$. We distinguish two cases. 
\begin{enumerate}
    \item First, if $|f(0) - f(1)| \leq \epsilon$, then $|g(0) - g(1)| \leq O(\epsilon)$ (since $\epdisc \leq \epsilon$), and thus \[|\MR_g^+(x) - f(\phi(x))| \leq \max_{b,b'\in\{0,1\}} |g(b) - f(b')| \leq O(\epsilon)\] for all $x$.

\item Second, suppose that $|f(0) - f(1)| \geq \epsilon$. Further suppose that $f(0) > f(1)$. Fix $i \in [m/2]$ and condition on $x^{(i)}$. If $\phi(x^{(i)}) = 0$, then 
\begin{align*}
\Pr[y_g^{(i)} = 0]
&= \frac{1}{g(0) + g(1)}\Pr[y^{(i)} = 1] \\ 
&= \frac{f(0)}{g(0) + g(1)}.
\end{align*}
If $\phi(x^{(i)}) = 1$, then
\begin{align*}
\Pr[y_g^{(i)} = 1]
&= \frac{g(0)+g(1)-1}{g(0)+g(1)}\Pr[y^{(i)}=1] + \Pr[y^{(i)}=0] \\ 
&= \frac{g(0)+g(1)-1}{g(0)+g(1)}f(1) + 1 - f(1) \\ 
&= \frac{g(0) + g(1) - f(1)}{g(0) + g(1)}.
\end{align*}
In particular, for each $b \in \{0,1\}$, if $\phi(x^{(i)}) = b$, then
\[\left|\Pr[y_g^{(i)}=b] - \frac{f(0)}{f(0) + f(1)}\right| \leq O(\epdisc).\]
Consider the alternative (idealized) dataset $(x^{(i)}, \til y^{(i)})_{i=1}^{m/2}$ where 
\[\Pr[\til y^{(i)} = \phi(x^{(i)}) \mid{} x^{(i)}] = \frac{f(0)}{f(0) + f(1)}.\]
Then the total variation distance between $\Law((x^{(i)},y_g^{(i)})_{i=1}^{m/2})$ and $\Law((x^{(i)},\til y^{(i)})_{i=1}^{m/2})$ is at most $O(\epdisc m) \leq \delta$, where the inequality holds by definition of $\epdisc$, so long as $\cdisc>0$ is chosen sufficiently small. Moreover, $(x^{(i)},\til y^{(i)})_{i=1}^{m/2}$ is an instance of the halfspace learning problem with noise level $\eta := \frac{f(1)}{f(0) + f(1)}$.  Let $\til h^+: \RR^n \to \{0,1\}$ denote the output of $\halfspacealg$ on $(x^{(i)}, \til y^{(i)})_{i=1}^{m/2}$. By the guarantee of \cite[Theorem 1.8]{diakonikolas2023strongly}, so long as $C_{\ref{thm:halfspace-reg}}$ is a sufficiently large constant, we have that with probability at least $1-\delta$,
\[\Pr_{x,\til y}[\til h(x) \neq \til y] \leq \eta + \frac{\epsilon^2}{4},\]
where $(x,\til y)$ is a fresh sample from the same distribution. Now
\begin{align*}
\Pr_{x,\til y}[\til h(x) \neq \til y]
&= (1-\eta) \Pr_x[\til h(x) \neq \phi(x)] + \eta (1 - \Pr_x[\til h(x) \neq \phi(x)]) \\ 
&= \eta + (1-2\eta) \Pr_x[\til h(x) \neq \phi(x)].
\end{align*}
Observe that $1 - 2\eta \geq \epsilon/2$ (since $f(0) - f(1) \geq \epsilon$). Therefore it holds with probability at least $1-\delta$ that
\[\Pr_x[\til h(x) \neq \phi(x)] \leq \frac{\Pr_{x,\til y}[\til h(x) \neq \til y] - \eta}{1-2\eta} \leq \epsilon.\]
It follows from the preceding total variation bound and the data processing inequality that in an event $\ME_g^+$ that occurs with probability at least $1-2\delta$, the estimator $h_g^+$ (which our algorithm actually computes) satisfies
\[\Pr_x[h_g^+(x) \neq \phi(x)] \leq \epsilon.\]
Moreover, in event $\ME_g^+$, we have
\[\EE_x (\MR_g^+(x) - f(\phi(x)))^2 = \EE_x (g(h_g^+(x)) - f(\phi(x)))^2 \leq O(\epdisc^2) + \epsilon \leq O(\epsilon).\]
Recall that this holds under the assumption that $f(0) > f(1)$. If $f(0) < f(1)$, then by a similar argument there is an event $\ME_g^-$ that occurs with probability at least $1-2\delta$, in which 
\[\EE_x (\MR_g^-(x) - f(\phi(x)))^2 \leq O(\epsilon).\]
\end{enumerate}
In either case, there is an event $\ME$ that holds with probability at least $1-2\delta$, in which 
\[\min_{g \in \MG, b \in \{-,+\}} \EE_x(\MR_g^b(x) - f(\phi(x)))^2 \leq O(\epsilon).\]
Next, notice that \[m \geq \epsilon^{-2} \log(4m^2/(\cdisc^2 \delta^3)) = \epsilon^{-2} \log(4|\MG|/\delta)\]
so long as $C_{\ref{thm:halfspace-reg}}$ is a sufficiently large constant. Therefore by Hoeffding's inequality and a union bound over $\MG \times \{-,+\}$, there is an event $\ME'$ that occurs with probability at least $1-\delta$ in which, for all $g \in \MG$ and $b \in \{-,+\}$,
\[\left| \frac{2}{m}\sum_{i=m/2+1}^m (\MR_g^b(x^{(i)} - y^{(i)}))^2 - \EE_{x,y}(\MR_g^b(x) - y)^2\right| \leq \epsilon.\]
Condition on the event $\ME \cap \ME'$. Then
\begin{align*}
\EE_x (\MR_{\wh g}^{\wh b}(x) - f(\phi(x)))^2 
&= \EE_{x,y} (\MR_{\wh g}^{\wh b}(x) - y)^2 - \EE_{x,y} (f(\phi(x)) - y)^2 \\ 
&\leq \epsilon + \frac{2}{m}\sum_{i=m/2+1}^m (\MR_{\wh g}^{\wh b}(x^{(i)} - y^{(i)}))^2 - \EE_{x,y} (f(\phi(x)) - y)^2 \\ 
&\leq \epsilon + \min_{g\in\MG,b\in\{-,+\}}\frac{2}{m}\sum_{i=m/2+1}^m (\MR_{g}^{b}(x^{(i)} - y^{(i)}))^2 - \EE_{x,y} (f(\phi(x)) - y)^2 \\ 
&\leq 2\epsilon + \min_{g\in\MG,b\in\{-,+\}} \EE_{x,y} (\MR_g^b(x) - y)^2 - \EE_{x,y} (f(\phi(x)) - y)^2 \\
&= 2\epsilon + \min_{g\in\MG,b\in\{-,+\}} \EE_{x,y} (\MR_g^b(x) - f(\phi(x)))^2 \\ 
&\leq O(\epsilon).
\end{align*}
Rescaling $\epsilon,\delta$ by the appropriate constants yields the desired bound.
\end{proof}

\begin{remark}\label{remark:ocr-to-pac}
In fact, the reduction in \cref{thm:halfspace-reg} does not use any special properties of halfspaces. Thus, it shows that for \emph{any} binary concept class $\Phi$, one-context regression is polynomial-time reducible to PAC learning $\Phi$ with random classification noise.
\end{remark}

\subsection{Hardness of Reward-Free RL with Resets}\label{sec:halfspace-hardness}

Next, we prove \cref{thm:halfspace-rl-hard}, which asserts that reward-free RL in $\MM_n$ with the reset access model is computationally intractable under the \emph{Continuous LWE} assumption formally defined below.


Together with \cref{thm:halfspace-reg}, this completes the proof of \cref{thm:halfspace-main}. Throughout this section, we write $\fS^{t-1}$ to denote the set of Euclidean unit vectors in $\RR^t$.

\begin{assumption}[Continuous LWE \citep{bruna2021continuous}]\label{ass:clwe}
Let $\delta \in (0,1)$. For $\beta = \beta(t) := 1/t$ and $\gamma = \gamma(t) := 2\sqrt{t}$, for any time-$2^{t^\delta}$ algorithm $\Alg^\MO$ with sampling oracle $\MO$ and outputs in $[0,1]$, it holds that
\[\left|\EE_{w\sim\Unif(\fS^{t-1})} \EE[\Alg^{C_{w,\beta,\gamma}}] - \EE[\Alg^{N(0,\frac{1}{2\pi}I_t)}]\right| \leq t^{-\omega(1)},\]
where $C_{w,\beta,\gamma}$ is the \emph{continuous LWE distribution} with secret $w \in \fS^{t-1}$ and parameters $\beta,\gamma>0$.
\end{assumption}

Recent work has shown that this assumption is well-founded---in particular, it can be based on hardness of discrete LWE \citep{gupte2022continuous} and therefore on worst-case hardness of the approximate shortest vector problem \citep{brakerski2013classical}.

\begin{theorem}[One-context regression is insufficient for Low-Rank MDPs with resets]
  \label{thm:halfspace-rl-hard}
Suppose that $\Alg^M$ is an algorithm that, given interactive reset access to any MDP $M \in \MM_n$, has time complexity $\poly(n)$ and produces a set of policies $\Psi$ satisfying the following guarantee with probability at least $1/2$:
\begin{equation}\forall s \in \MS: \max_{\pi\in\Psi} d^{M,\pi}_H(s) \geq \frac{1}{\poly(|\MA|, |\MS|, H)} \left(\max_{\pi\in\Pi} d^{M,\pi}_H(s) - \frac{1}{8}\right).\label{eq:genblock-cover}\end{equation}
Then the Continuous LWE hardness assumption (\cref{ass:clwe}) is false. %\dfc{probably good to spell out here that  any algo with runtime polynomial in the usual parameters we expect ($d$, $H$, $1/\veps$, $1/\delta$, etc) would have time complexity $\poly(n)$ here.}
\end{theorem}

Since the generalized Block MDPs in $\MM_n$ have horizon $H \leq n$ and action space of size $O(1)$, this result rules out algorithms with time complexity $\poly(n,H,|\MA|)$, and therefore proves the claimed hardness result in \cref{thm:halfspace-main}. Also, the description length of each $x \in \MX = \RR^n$ is $O(n)$ (again, ignoring issues of bit complexity). Thus, \cref{thm:halfspace-rl-hard}, in conjunction with \cref{thm:halfspace-reg}, rules out any reduction from reward-free RL with resets (in Generalized Block MDPs) to one-context regression that is efficient, i.e. that has oracle time complexity $\poly(H,|\MS|,|\MA|,\epsilon^{-1},\delta^{-1},n)$.

\paragraph{Proof overview.} To prove \cref{thm:halfspace-rl-hard}, we need the following result, which we prove (in \sssref{subsec:halfspace-technical}) by unpacking various proofs from \cite{tiegel2023hardness}. Essentially, it states that there are two parametric families of distributions $\{\nu_{w,0}: w \in \fS^{t-1}\}$ and $\{\nu_{w,1}: w \in \fS^{t-1}\}$ that are each computationally indistinguishable from some null distribution under Continuous LWE (\cref{item:no-adv}), and yet for each $w$, the two corresponding distributions are approximately separated by a halfspace (\cref{item:class-error-bounds}).

\begin{theorem}\label{thm:ltf-distributions}
Suppose \cref{ass:clwe} holds. Let $\nlarge \in\NN$ and define $\nsmall := \frac{\log^2 \nlarge}{\log^5(\log \nlarge)}$. There is a distribution $\nunull$ and families $\{\theta(w): w \in \fS^{\nsmall-1}\}$, $\{\nu_{w,0}:w \in \fS^{\nsmall-1}\}$, $\{\nu_{w,1}: w \in \fS^{\nsmall-1}\}$ where $\theta(w) \in \RR^{\nlarge}$, $\nu_{w,0},\nu_{w,1} \in \Delta(\RR^{\nlarge})$ are distributions, and the following properties hold:
\begin{enumerate}
\item\label{item:no-adv} For every algorithm $\Alg^{\MO_0,\MO_1}$ with time complexity $\poly(\nlarge)$, access to sampling oracles $\MO_0,\MO_1$, and outputs in $[0,1]$,
\[\left|\EE_{w \sim \Unif(\fS^{\nsmall-1})} \EE[\Alg^{\nu_{w,0},\nu_{w,1}}] - \EE[\Alg^{\nunull,\nunull}]\right| \leq \nlarge^{-\omega(1)}.\]
\item\label{item:class-error-bounds} There is $\gamma(\nlarge) \leq (\log \nlarge)^{-\Omega(\log^2 \log \nlarge)}$ so that for every $w \in \fS^{\nsmall-1}$,
\[\Pr_{x \sim \nu_{w,1}}[\langle x, \theta(w) \rangle < 0] \leq \gamma(\nlarge)\]
and
\[\Pr_{x \sim \nu_{w,0}}[\langle x,\theta(w) \rangle \geq 0] \leq \gamma(\nlarge)\]
and
\[\Pr_{x\sim\nunull}[\langle x,\theta(w)\rangle \geq 0] \geq 1/2.\]

\item\label{item:eval-halfspace} There is a $\poly(\nlarge)$-time algorithm that takes as input $w \in \fS^{\nsmall-1}$ and $x \in \RR^{\nlarge}$, and outputs $\sgn(\langle x,\theta(w)\rangle)$.

\item\label{item:nu-samp} There is a $\poly(\nlarge)$-time algorithm that takes as input $w \in \fS^{\nsmall-1}$ and $a \in \{0,1\}$, and outputs $x \in \RR^{\nlarge}$ where $\Law(x)$ is $n^{-\omega(1)}$-close to $\nu_{w,a}$ in total variation distance.
\item\label{item:null-samp} There is a $\poly(\nlarge)$-time algorithm that samples from $\nunull$.
\end{enumerate}
\end{theorem}


\iffalse
\dhruv{todo update notation below}

For any $\phi \in \Phi$ we consider distributions $\nu_{\phi,0},\nu_{\phi,1},\nunull \in \Delta(\MX)$ with $\nunull = N(0,I_d)$ and the following properties.

\begin{assumption}
For each $b \in \{-1,1\}$, it holds that
\[\Pr_{x \sim \nu_{\phi,b}}[\phi(x) = b] \geq 1 - \gamma(n)\]
and $\Pr_{x\sim\nunull}[\phi(x)=1] \geq 1/2$. \dhruv{probably there is symmetry so it's exactly $1/2$}
\end{assumption}

\begin{assumption}
For any $\poly(n)$-time algorithm $\Alg^{\MO_0,\MO_1}$ with access to sampling oracles $\MO_0, \MO_1$ and outputs in $[0,1]$,
\[\EE_{\phi\sim\Unif(\Phi)} \left| \EE[\Alg^{\nu_{\phi,0},\nu_{\phi,1}}] - \EE[\Alg^{\nunull,\nunull}]\right| \leq \textrm{negl}(n)\]
\end{assumption}
\fi

We exploit these distribution families by designing a family of \emph{approximate} combination lock MDPs within $\MM_n$. Concretely, for each sequence of vectors $(w_1,\dots,w_H)$ and action sequence $(a^\st_1,\dots,a^\st_H)$ we design a $\Phi_n$-decodable MDP $M^{:H}_{\bw,\bast}$, defined formally below, for which playing an action sequence similar to $\bast$ is necessary in order to reach the latent state $\phi^{\theta(w_H)}(x_H) = 1$ with decent probability. This idea is formalized in \cref{lemma:best-policy,lemma:expert-agreement}. We then use the indistinguishability property of the distributions families (\cref{item:no-adv}) together with a careful hybrid argument to prove that any computationally efficient RL algorithm has similar behavior on $M^{:H}_{\bw,\bast}$ as on a ``null MDP'' that is independent of $\bw$ and $\bast$. In particular,  \cref{lemma:hybrid} shows that the transitions of $M^{:H}_{\bw,\bast}$ can be replaced with ``null'' transitions layer-by-layer, starting with the last layer, and no efficient algorithm can detect the difference under \cref{ass:clwe}. We then complete the proof of \cref{thm:halfspace-rl-hard} by observing that an RL algorithm which is independent of $\bast$ cannot always play a policy similar to $\bast$.

\begin{definition}[MDPs for hardness construction]
Set $H := (\log n)^{\frac{1}{3}\log \log n}$. For any $\bw = (w_1,\dots,w_H) \in (\fS^{t-1})^H$ and $\bast = (a^\st_1,\dots,a^\st_H) \in \MA^H$ and $k \in [H]$, we define an MDP $M_{\bw,\ba}^{:k}$ with observation space $\MX=\RR^n$, action space $\MA=\{0,1\}$, and horizon $H = (\log n)^{\log \log n}$ as follows. The initial distribution is $\nunull$. For each $x_h \in \MX$ and $a_h \in \MA$ and $h \in [H-1]$, the transition distribution is
\[\BP_{h+1}^{M_{\bw,\ba}^{:k}}(x_{h+1}|x_h,a_h) := \begin{cases} 
\nu_{w_{h+1}, 1}(x_{h+1}) & \text{ if } \langle x_h,\theta(w_h)\rangle \geq 0 \text{ and } a_h = a^\st_h \text{ and } h < k\\ 
\nu_{w_{h+1}, 0}(x_{h+1}) & \text{ if } (\langle x_h,\theta(w_h)\rangle<0 \text{ or } a_h \neq a^\st_h) \text{ and } h < k \\
\nunull & \text{ if } h \geq k
\end{cases}.
\]
\end{definition}

\begin{fact}
For any $\bw=w_{1:H} \in \WH$, $\bast =a^\st_{1:H}\in \MA^H$, and $k \in [H]$, it holds that $M_{\bw,\ba}^{:k} \in \MM_n$.
\end{fact}

The following lemma shows that the fixed action sequence $\bast$ is a policy that reaches the latent state $\phi^{\theta(w_H)}(x_H) = 1$ with constant probability.

\begin{lemma}\label{lemma:best-policy}
For any $\bw=w_{1:H} \in \WH$ and $\bast =a^\st_{1:H}\in \MA^H$, 
\[\max_{\pi\in\Pi} \Pr^{M_{\bw,\ba}^{:H},\pi}[\phi^{\theta(w_H)}(x_H) = 1] \geq \frac{1}{4}\]
so long as $n$ is sufficiently large.
\end{lemma}

\begin{proof}
For notational convenience, write $M := M_{\bw,\bast}^{:H}$. Consider the policy $\pi$ defined by $\pi_h(x_h) := a^\st_h$ for all $h \in [H]$ and $x_h \in \MX$. The initial distribution of $M$ is $\nunull$, so $\Pr^{M,\pi}[\phi^{\theta(w_1)}(x_1) = 1] \geq 1/2$ by \cref{item:class-error-bounds} of \cref{thm:ltf-distributions}. For each $h < H$, conditioned on $x_h\in\MX$ with $\phi^{\theta(w_h)}(x_h) = 1$, the distribution of $x_{h+1}$ under policy $\pi$ is $\nu_{w_{h+1},1}$, so
\[\Pr^{M,\pi}[\phi^{\theta(w_{h+1})}(x_{h+1}) = 1 \mid{} \phi^{\theta(w_h)}(x_h) = 1] \geq 1 - \gamma(n).\]
Therefore by induction, 
\[\Pr^{M,\pi}[\phi^{\theta(w_H)}(x_H) = 1] \geq \frac{1}{2}(1-\gamma(n))^{H-1} \geq \frac{1}{4}\]
where the final inequality holds for all sufficiently large $n$, since $H = H(n) = o(\gamma(n))$.
\end{proof}

The following lemma shows that any policy that reaches $\phi^{\theta(w_H)}(x_H) = 1$ with decent probability must also often play the action sequence $\bast$.

\begin{lemma}\label{lemma:expert-agreement}
For any $\bw=w_{1:H} \in \WH$,  $\bast =a^\st_{1:H}\in \MA^H$, and policy $\pi\in\Pi$,
\[\Pr^{M^{:H}_{\bw,\bast},\pi}[\forall h \in [H-1]: \pi_h(x_h) = a^\st_h] \geq \Pr^{M_{\bw,\bast}^{:H},\pi}\left[\phi^{\theta(w_H)}(x_H) = 1\right] - H^2 \gamma(n).\]
\end{lemma}

\begin{proof}
For notational convenience, write $M = M^{:H}_{\bw,\bast}$. For each $h \in [H-1]$ let $\ME_h$ be the event that $\pi_h(x_h) \neq a^\st_h$. Then
\[\Pr\left[\left(\phi^{\theta(w_H)}(x_H) = 1\right) \land \left(\pi_h(x_h)\neq a^\st_h\right)\right] \leq \Pr\left[\phi^{\theta(w_H)}(x_H)=1 \middle| \pi_h(x_h)\neq a^\st_h\right] \leq H\gamma(n)\]
since conditioned on any $x_h \in \MX$ with $\pi_h(x_h) \neq a^\st_h$, $x_{h+1}$ is distributed according to $\nu_{w_{h+1},0}$, and thus $\Pr^{M,\pi}[\phi^{\theta(w_{h+1})}(x_{h+1})=1 \mid{}x_h] \leq \gamma(n)$ (\cref{item:class-error-bounds} of \cref{thm:ltf-distributions}), and at each subsequent step $k \geq h+1$, conditioned on any $x_k \in \MX$ with $\phi^{\theta(w_k)}(x_k) = 0$, $x_{k+1}$ is distributed according ot $\nu_{w_k,0}$ so once again $\Pr^{M,\pi}[\phi^{\theta(w_{k+1})}(x_{k+1})=1 \mid{}x_k] \leq \gamma(n)$. 

By the union bound,
\begin{align*}
\Pr\left[\left(\phi^{\theta(w_H)}(x_H) = 1\right) \land \left(\exists h\in[H-1]: \pi_h(x_h)\neq a^\st_h\right)\right] 
&\leq H^2\gamma(n).
\end{align*}
Therefore
\begin{align*}
&\Pr^{M,\pi}[\forall h \in [H-1]: \pi_h(x_h) = a^\st_h] \\ 
&\geq \Pr^{M,\pi}\left[\left(\phi^{\theta(w_H)}(x_H) = 1\right) \land \left(\forall h\in[H-1]: \pi_h(x_h)= a^\st_h\right)\right] \\
&\geq \Pr^{M,\pi}\left[\phi^{\theta(w_H)}(x_H) = 1\right] - H^2\gamma(n)
\end{align*}
as needed.
\end{proof}

We now implement a hybrid argument to show that any efficient RL algorithm has similar behavior on $M^{:1}_{\bw,\bast},\dots,M^{:H}_{\bw,\bast}$. In particular, we need that the \emph{value} of its output is similar on these different MDPs, where we define value as the maximum agreement probability (over all policies output by the algorithm) with the true action sequence $\bast$: 

\begin{definition}
For an MDP $M$, set of policies $\Psi$, and action sequence $\bast$, define 
\[\Val^M_{\bast}(\Psi) = \max_{\pi\in\Psi} \Pr^{M,\pi}[\forall h \in [H-1]: \pi_h(x_h) = a^\st_h].\]
\end{definition}

\begin{lemma}\label{lemma:hybrid}
Let $\Alg^M$ be a $\poly(n)$-time algorithm that uses interactive reset access to an MDP $M \in \MM_n$ and outputs a set of policies. Fix $\bast \in \MA^H$ and $k \in [H-1]$. Then
\[\left| \EE_{\bw\sim\Unif(\fS^{t-1})^{\otimes H}}\EE[\Val^{M_{\bw}}_{\bast}(\Alg^{M_{\bw}})] - \EE_{\bw\sim\Unif(\fS^{t-1})^{\otimes H}}\EE[\Val^{M'_{\bw}}_{\bast}(\Alg^{M'_{\bw}})]\right| \leq O(n^{-5}).\]
where $M_{\bw} = M^{:k}_{\bw,\bast}$ and $M'_{\bw} = M^{:k+1}_{\bw,\bast}$.
\end{lemma}

\begin{proof}
We define an algorithm $\Algbar^{\MO_0,\MO_1}$ with access to sampling oracles $\MO_0,\MO_1$ as follows. Sample $\bw = (w_1,\dots,w_H)$ from $\Unif(\fS^{t-1})^{\otimes H}$. Then simulate $\Alg$ with RL-with-resets access to an MDP $M^{\MO_0,\MO_1}_{w_{1:k}}$ defined as follows. First, define an initial sampling subroutine that samples from $\nunull$ (this can be implemented in time $\poly(n)$ by \cref{item:null-samp}). Next, define a conditional sampling subroutine that, given $h$, $x_h$, and $a_h$, samples $x_{h+1}$ according to the following distribution:
\[\BP(x_{h+1}|x_h,a_h) = \begin{cases} 
\nu_{w_{h+1}, 1}(x_{h+1}) & \text{ if } \phi^{\theta(w_h)}(x_h) = 1 \text{ and } a_h = a^\st_h \text{ and } h < k\\ 
\nu_{w_{h+1}, 0}(x_{h+1}) & \text{ if } (\phi^{\theta(w_h)}(x_h) = 0 \text{ or } a \neq a^\st_h) \text{ and } h < k \\
\MO_0(x_{h+1}) & \text{ if } \phi^{\theta(w_h)}(x_h) = 1 \text{ and } a_h = a^\st_h \text{ and } h = k \\ 
\MO_1(x_{h+1}) &\text{ if } (\phi^{\theta(w_h)}(x_h) = 0 \text{ or } a_h \neq a^\st_h) \text{ and } h = k \\
\nunull & \text{ if } h > k
\end{cases}.
\]
This sampler can be implemented in time $\poly(n)$, up to total variation error $n^{-\omega(1)}$: in particular, for any $x_h \in \MX$ and $w_h \in \fS^{t-1}$, we can compute $\sgn(\langle x_h,\theta(w_h)\rangle)$ in time $\poly(n)$ (\cref{item:eval-halfspace}) and hence evaluate $\phi^{\theta(w_h)}(x_h) = \mathbbm{1}[\langle x_h, \theta(w_h)\rangle \geq 0]$; moreover, we can sample from $\nu_{w_{h+1},0}$ and $\nu_{w_{h+1},1}$ for any given $w_{h+1} \in \fS^{t-1}$ (\cref{item:nu-samp}) up to total variation error $n^{-\omega(1)}$, we can sample from $\nunull$ (\cref{item:null-samp}), and we are given access to $\MO_0,\MO_1$. 

Let $\Psi$ be the output of $\Alg$ after interaction with this MDP. Next, $\Algbar^{\MO_0,\MO_1}$ computes an estimate $\wh \Val$ of $\Val^{M^{\MO_0,\MO_1}_{w_{1:k}}}_{\bast}(\Psi)$ by enumerating over $\Psi$ and performing Monte Carlo estimation: note that we can draw trajectories from this MDP, and we know $\bast$. By drawing $N = n^{10}$ trajectories, we can guarantee that conditioned on all prior randomness,
\[\EE \left|\wh \Val - \Val^{M^{\MO_0,\MO_1}_{w_{1:k}}}_{\bast}(\Psi)\right| \leq O(1/\sqrt{N}).\]
Finally, $\Algbar^{\MO_0,\MO_1}$ output $\wh \Val$.

\paragraph{Analysis.} Suppose $(\MO_0,\MO_1) = (\nunull,\nunull)$. If the conditional sampler defined above had no error, then it would hold for any choice of $\bw=w_{1:H}$ that $M^{\MO_0,\MO_1}_{w_{1:k}}$ is exactly $M^{:k}_{\bw,\bast}$. Since the error is $n^{-\omega(1)}$ per sample and $\Alg$ has time complexity $\poly(n)$, this error affects $\EE[\Algbar^{\nunull,\nunull}]$ by at most $n^{-\omega(1)}$. Therefore
\begin{align*}
\left|\EE[\Algbar^{\nunull,\nunull}]
- \EE_{\bw\sim\Unif(\fS^{t-1})^{\otimes H}}\EE[\Val^{M^{:k}_{\bw,\bast}}_{\bast}(\Alg^{M^{:k}_{\bw,\bast}})]\right| \leq O(1/\sqrt{N}),
\end{align*}
where the error due to Monte Carlo estimation dominates the error in the conditional sampler. 

Next, for any $w^\st_{k+1} \in\fS^{t-1}$, suppose that $(\MO_0,\MO_1) = (\nu_{w^\st_{k+1},0},\nu_{w^\st_{k+1},1})$. If the conditional sampler had no error, then it would hold for any choice of $\bw$ that $M^{\MO_0,\MO_1}_{w_{1:k}}$ is the same as $M^{:k+1}_{\bw^\st,\bast}$ where $\bw^\st = (w_1,\dots,w_k,w^\st_{k+1},w_{k+2},\dots,w_H)$. Again the error is at most $n^{-\omega(1)}$ per sample, and hence affects $\EE[\Algbar^{\nu_{w^\st_{k+1},0},\nu_{w^\st_{k+1},1}}]$ by at most $n^{-\omega(1)}$. Therefore
\begin{align*}
&\left|\EE_{w^\st_{k+1}\sim\Unif(\fS^{t-1})}\EE[\Algbar^{\nu_{w^\st_{k+1},0},\nu_{w^\st_{k+1},1}}]
- \EE_{w^\st_{k+1},w_1,\dots,w_H\sim\Unif(\fS^{t-1})}\EE[\Val^{M^{:k+1}_{\bw^\st,\bast}}_{\bast}(\Alg^{M^{:k+1}_{\bw^\st,\bast}})]\right| \\
&\leq \EE_{w^\st_{k+1}\sim\Unif(\fS^{t-1})} \left|\EE[\Algbar^{\nu_{w^\st_{k+1},0},\nu_{w^\st_{k+1},1}}]
- \EE_{w_1,\dots,w_H\sim\Unif(\fS^{t-1})}\EE[\Val^{M^{:k+1}_{\bw^\st,\bast}}_{\bast}(\Alg^{M^{:k+1}_{\bw^\st,\bast}})]\right| \\
&\leq O(1/\sqrt{N}).
\end{align*}
But $\Algbar$ has time complexity $\poly(n)$, so by \cref{item:no-adv},
\[\left|\EE_{w^\st_{k+1}\sim\Unif(\fS^{t-1})}\EE[\Algbar^{\nu_{w^\st_{k+1},0},\nu_{w^\st_{k+1},1}}]
- \EE[\Algbar^{\nunull,\nunull}]\right| \leq n^{-\omega(1)}.\]
The result follows by the triangle inequality.
\end{proof}

We can now put together \cref{lemma:hybrid} (applied for all $1 \leq k \leq H-1$) with \cref{lemma:best-policy,lemma:expert-agreement} and the coverage guarantee \cref{eq:genblock-cover} assumed in the theorem statement.
\vspace{1em}
\begin{proof}[Proof of \cref{thm:halfspace-rl-hard}]
For any $\bw \in (\fS^{t-1})^H$ and $\ba \in \MA^H$, let $\Psi^{:k}_{\bw,\bast}$ denote the (random) output of $\Alg^{M^{:k}_{\bw,\bast}}$. By the theorem assumption and the fact that $|\MA|=|\MS|=2$, we know that for any $\bw,\bast$, with probability at least $1/2$ over the execution of $\Alg^{M^{:H}_{\bw,\bast}}$, there is some $\pihat\in \Psi^{:H}_{\bw,\bast}$ such that 
\[\Pr^{M^{:H}_{\bw,\bast},\pihat}[\phi^{\theta(w_H)}(x_H)=1] \geq \frac{1}{\poly(H)}\left(\max_{\pi\in\Pi} \Pr^{M^{:H}_{\bw,\ba},\pi}[\phi^{\theta(w_H)}(x_H)=1] - \frac{1}{8}\right) \geq \frac{1}{\poly(H)}\]
where the second inequality is by \cref{lemma:best-policy}. In this event, by \cref{lemma:expert-agreement}, the policy $\pihat$ satisfies
\[\Pr^{M^{:H}_{\bw,\bast},\pihat}[\forall h \in [H-1]: \pihat_h(x_h) = a^\st_h] \geq \frac{1}{\poly(H)} - H^2 \gamma(n) \geq \frac{1}{\poly(H)}\]
where the second inequality holds for sufficiently large $n$, and uses the fact that $H = (\log n)^{\log \log n}$ whereas $\gamma(n) = (\log n)^{-\Omega(\log^2 \log n)}$.
Therefore we have
\[\EE[\Val^{M^{:H}_{\bw,\bast}}_{\bast}(\Psi^{:H}_{\bw,\bast})] \geq \frac{1}{\poly(H)}\]
where the expectation is over the execution of $\Alg^{M^{:H}_{\bw,\bast}}$.
Since this bound holds for any fixed $\bw$, it also holds in expectation over $\bw \sim \Unif(\fS^{t-1})^{\otimes H}$. By \cref{lemma:hybrid}, we get that for any $\bast \in \MA^H$,
\begin{equation} \EE_{\bw \sim \Unif(\fS^{t-1})^{\otimes H}} \EE[\Val^{M^{:1}_{\bw,\bast}}_{\bast}(\Psi^{:1}_{\bw,\bast})] \geq \frac{1}{\poly(H)} - O(H \cdot n^{-5}) \geq \frac{1}{\poly(H)}
\label{eq:val-lb}
\end{equation}
since $H = (\log n)^{\log \log n} = n^{o(1)}$. But the initial distribution and transition distributions of $M^{:1}_{\bw,\bast}$ are independent of $\bast$ (and $\bw$), i.e. we can write $M^{:1}_{\bw,\bast} = M$ for a fixed MDP $M$. It follows that the random variables $\{\Psi^{:1}_{\bw,\ba}: \ba \in \MA^H\}$ are identically distributed. So for any $\bw \in (\fS^{t-1})^H$ and $\bast \in \MA^H$,
\begin{align*}
\sum_{\ba \in \MA^H} \EE[\Val^{M^{:1}_{\bw,\ba}}_{\ba}(\Psi^{:1}_{\bw,\ba})]
&= \sum_{\ba \in \MA^H} \EE[\Val^{M^{:1}_{\bw,\ba}}_{\ba}(\Psi^{:1}_{\bw,\bast})] \\ 
&= \sum_{\ba \in \MA^H} \EE[\Val^{M}_{\ba}(\Psi^{:1}_{\bw,\bast})] \\ 
&= \EE\left[\sum_{\ba \in \MA^H} \max_{\pi \in \Psi^{:1}_{\bw,\bast}}\Pr^{M, \pi}[\forall h \in [H-1]: \pi_h(x_h) = a_h]\right] \\ 
&\leq \EE\left[\sum_{\ba \in \MA^H} \sum_{\pi \in \Psi^{:1}_{\bw,\bast}}\Pr^{M, \pi}[\forall h \in [H-1]: \pi_h(x_h) = a_h]\right] \\ 
&\leq \EE[|\Psi^{:1}_{\bw,\bast}|] \\ 
&\leq \poly(n)
\end{align*}
since the size of the output of $\Alg$ is bounded by its runtime. It follows that
\[\EE_{\bw\sim\Unif(\fS^{t-1})^{\otimes H}}\sum_{\ba \in \MA^H} \EE[\Val^{M^{:1}_{\bw,\ba}}_{\ba}(\Psi^{:1}_{\bw,\ba})] \leq \poly(n)\]
and thus there is some $\ba \in \MA^H$ with
\[\EE_{\bw\sim\Unif(\fS^{t-1})^{\otimes H}}\EE[\Val^{M^{:1}_{\bw,\ba}}_{\ba}(\Psi^{:1}_{\bw,\ba})] \leq \frac{\poly(n)}{2^H} \leq n^{-\omega(1)}\]
where the final inequality uses that $H = \omega(\log n)$. But this contradicts \cref{eq:val-lb}.
\end{proof}

\subsubsection{Technical Results Regarding CLWE and PTFs}\label{subsec:halfspace-technical}

To prove \cref{thm:ltf-distributions}, we follow the strategy used by \cite{tiegel2023hardness}: we first prove an analogous result for polynomial threshold functions (\cref{lemma:ptf-distributions}, below), and then lift to linear threshold functions (albeit suffering a blowup in the dimension) via the Veronese mapping. To reiterate, these results essentially follow from inspecting various statements and proofs from \cite{tiegel2023hardness}.

\begin{lemma}\label{lemma:ptf-distributions}
Let $\nsmall,\ell \in\NN$ with $\ell \geq 2\sqrt{\nsmall}$, and $\delta \in (0,1)$. Define $\MDnull := N(0, \frac{1}{2\pi} I_{\nsmall})$. There is a family of degree-$\ell$ PTFs $\{f_w: \RR^{\nsmall} \to \{-1,1\}\}$ and two families of distributions $\{\MD_w^+\}$ and $\{\MD_w^-\}$ over $\RR^{\nsmall}$, all indexed by $w \in \RR^{\nsmall}$, with the following properties:
\begin{enumerate}
\item\label{item:indist} Under \cite[Assumption 3.4]{tiegel2023hardness}, for every algorithm $\Alg^{\MO_0,\MO_1}$ with time complexity $2^{\nsmall^\delta}$, access to sampling oracles $\MO_0,\MO_1$, and outputs in $[0,1]$,
\[\left|\EE_{w \sim \Unif(\fS^{\nsmall-1})} \EE[\Alg^{\MD_w^+,\MD_w^-}] - \EE[\Alg^{\MDnull,\MDnull}]\right| \leq 2^{-\nsmall^\delta}.\]
\item For every $w \in \fS^{\nsmall-1}$,
\begin{equation} \Pr_{x \sim \MD_w^+}[f_w(x) \neq 1] \leq \exp(-\Omega(\ell^2/\nsmall))\label{eq:misspec-plus}\end{equation}
and
\begin{equation} \Pr_{x \sim \MD_w^-}[f_w(x) \neq -1] \leq \exp(-\Omega(\ell^2/\nsmall))\label{eq:misspec-minus}\end{equation}
and
\begin{equation} \Pr_{x \sim \MDnull}[f_w(x) = 1] \geq 1/2.\label{eq:null-bias}
\end{equation}
\item\label{item:samp} There is a $\poly(\nsmall)$-time algorithm that takes as input $w \in \fS^{\nsmall-1}$ and $a \in \{-,+\}$, and outputs $(x,f_w(x))$ with $\TV(\Law(x),\MD_w^a) \leq 2^{-\Omega(t)}$.
\end{enumerate}
\end{lemma}

\begin{proof}
Set $\beta = 1/t$ and $\gamma = 2\sqrt{t}$. For each $w \in \fS^{t-1}$, let $\MD_w^+$ be the distribution $\NH_{w,\beta,\gamma,0}$, and let $\MD_w^-$ be the distribution $\NH_{w,\beta,\gamma,1/2}$, where both of these \emph{non-overlapping homogeneous CLWE} distributions are defined in \cite[Definition 3.3]{tiegel2023hardness}. By \cite[Lemma 4.3]{tiegel2023hardness}, for each $w \in \fS^{t-1}$ there is some degree-$\ell$ polynomial threshold function $f_w: \RR^t \to \{-1,1\}$ such that 
\[\frac{1}{2}\Pr_{x \sim \MD_w^+}[f_w(x) \neq 1] + \frac{1}{2}\Pr_{x \sim \MD_w^-}[f_w(x) \neq 1] \leq \exp(-\Omega(\ell^2/t)),\]
which proves \cref{eq:misspec-plus,eq:misspec-minus}. Moreover, inspecting the proof of \cite[Lemma 4.3]{tiegel2023hardness} we see that $f_w(x) = \sgn(p(\langle w,x\rangle))$ for a fixed degree-$\ell$ polynomial $p: \RR \to \RR$ (i.e. depending only on $t$, $\ell$, $\beta$, $\gamma$). Since the distribution $\MDnull$ is radially symmetric, the distribution of $\langle w,x\rangle$ under $x \sim \MDnull$ does not depend on $w$, so $\Pr_{x \sim \MDnull}[f_w(x) = 1] = r$ for some constant $r \in [0,1]$. If $r \geq 1/2$ then \cref{eq:null-bias} holds for all $w \in \fS^{t-1}$. If $r < 1/2$, then we can simply swap the definitions of $\MD_w^+$ and $\MD_w^-$ for all $w$, and replace $f_w$ with $-f_w$ for all $w$. This preserves \cref{eq:misspec-plus,eq:misspec-minus}, and after this swap \cref{eq:null-bias} is now satisfied for all $w \in \fS^{t-1}$.

Since the sign pattern of $p$ is explicit (in terms of $t$, $\ell$, $\beta$, and $\gamma$), for any $w \in \fS^{t-1}$ and $x \in \RR^t$ we can efficiently compute $\sgn(p(\langle w,x\rangle)) = f_w(x)$. To prove \cref{item:samp} it remains to show that given $w \in \fS^{t-1}$ we can approximately sample from $\NH_{w,\beta,\gamma,0}$ and $\NH_{w,\beta,\gamma,1/2}$. By \cite[Lemma 6.2]{tiegel2023hardness}, for any $c \in [0,1)$, given a sampling oracle for the CLWE distribution $C_{w,\beta,\gamma}$ there is a $\poly(t)$-time algorithm for sampling from a distribution that is $2^{-t}$-close to $\HH_{w,\beta,\gamma,c}$ in total variation distance, where $\HH_{w,\beta,\gamma,c}$ is the homogeneous CLWE distribution (\cite[Definition 3.2]{tiegel2023hardness}) and $C_{w,\beta,\gamma}$ is the CLWE distribution (\cite[Definition 3.1]{tiegel2023hardness}). But by definition, there is an explicit, $\poly(t)$-time algorithm for sampling from $C_{w,\beta,\gamma}$ (assuming the ability to sample from a standard Gaussian distribution). Moreover, \cite[Lemma A.1]{tiegel2023hardness} shows that $\TV(\NH_{w,\beta,\gamma,c},\HH_{w,\beta,\gamma,c}) \leq 4 \cdot \exp(-\frac{1}{100\beta^2}) \leq 2^{-\Omega(t)}$ by choice of $\beta$. Thus, given $w \in \fS^{t-1}$, we can sample from a distribution that is $2^{-\Omega(t)}$-close to $\NH_{w,\beta,\gamma,c}$ in total variation distance, in time $\poly(t)$. This proves \cref{item:samp}.

By \cite[Theorem 6.1]{tiegel2023hardness}, it holds under \cite[Assumption 3.4]{tiegel2023hardness} that there is no constant $c>0$ and $2^{t^\delta}$-time algorithm $\Alg^\MO$ with outputs in $[0,1]$ and
\[\left|\EE_{w \sim \Unif(\fS^{t-1})}[\Alg^{\frac{1}{2}(D_w^+, 1) + \frac{1}{2}(D_w^-, -1)}] - \EE[\Alg^{N(0, \frac{1}{2\pi}I_t)\times \Ber(1/2)}]\right| \geq \Omega(n^{-c}).\]
In fact, by a standard boosting argument, the advantage can be driven down to $2^{-t^\delta}$. Moreover, since samples from $D_w^+$ and $D_w^-$ can be individually extracted using a sampling oracle for $\frac{1}{2}(D_w^+,1)+\frac{1}{2}(D_w^-,-1)$, it follows that there is no $2^{t^\delta}$-time algorithm $\Alg^{\MO_0,\MO_1}$ with outputs in $[0,1]$ and
\[\left|\EE_{w \sim \Unif(\fS^{t-1})}[\Alg^{D_w^+,D_w^-}] - \EE[\Alg^{N(0, \frac{1}{2\pi}I_t),N(0, \frac{1}{2\pi}I_t)}]\right| \geq \Omega(2^{-t^\delta}).\]
This proves \cref{item:indist}.
\end{proof}

\begin{proof}[Proof of \cref{thm:ltf-distributions}]
Invoke \cref{lemma:ptf-distributions} with $t = \frac{\log^2 n}{\log^4(\log n)}$ and $\ell = \frac{\log n}{3\log \log n} \geq 2\sqrt{t}$, where the inequality holds for sufficiently large $n$. For each $w \in \fS^{t-1}$, let $\MD_w^+,\MD_w^- \in \Delta(\RR^t)$ be the distributions and let $f_w: \RR^t \to \{-1,1\}$ be the $2\sqrt{t}$-PTF given by the lemma. Let $\tau: \RR^t \to \RR^n$ be defined as follows. Identify the first $t^0 + t^1 + \dots + t^\ell$ coordinates of $[n]$ with sequences $\alpha = (\alpha_1,\dots,\alpha_k) \in [t]^k$ of length at most $\ell$, and for $x \in \RR^t$ and any such sequence $\alpha$, define
\[\tau(x)_\alpha := \prod_{i=1}^{|\alpha|} x_i.\]
Define the remaining $n - (t^0+\dots+t^\ell)$ coordinates of $\tau(x)$ to be $0$. Note that \[t^0+\dots+t^\ell \leq 2t^\ell \leq 2(\log^2 n)^{\log(n)/(3\log\log n)} \leq 2n^{2/3}\]
so this map is well-defined for sufficiently large $n$. Next, observe that for any polynomial $q: \RR^t \to \RR$ of degree at most $\ell$, there is some $\theta \in \RR^n$ such that $q(x) = \langle \tau(x), \theta\rangle$ for all $x \in \RR^t$. Accordingly, for each $w \in \fS^{t-1}$, define $\theta(w) \in \RR^n$ so that $f_w(x) = \sgn(\langle \tau(x), \theta(w)\rangle)$ for all $x \in \RR^t$. Let $\nu_{w,1}$ be the distribution of $\tau(x)$ under $x \sim \MD_w^+$, and let $\nu_{w,0}$ be the distribution of $\tau(x)$ under $x \sim \MD_w^-$. Let $\nunull$ be the distribution of $\tau(x)$ under $x \sim \MDnull$. We now check the theorem claims:
\begin{enumerate}
\item Let $\Alg^{\MO_0,\MO_1}$ be an $\poly(n)$-time algorithm with access to sampling oracles $\MO_0,\MO_1 \in \Delta(\RR^n)$. We define an algorithm $\Algbar^{\MObar_0,\MObar_1}$ with access to sampling oracles $\MObar_0,\MObar_1 \in \Delta(\RR^t)$ as follows. Simulate $\Alg$; when it queries $\MO_0$, draw $x \sim \MObar_0$ and pass $\tau(x)$; when it queries $\MO_1$, draw $x \sim \MObar_1$ and pass $\tau(x)$. Finally, output the final output of $\Alg$.

Since $\tau$ can be evaluated in time $\poly(n)$, the time complexity of $\Algbar$ is $\poly(n) = 2^{O(\log n)} \leq 2^{O(t^{2/3})}$. Moreover, the distribution of $\Algbar^{\MDnull,\MDnull}$ is exactly the distribution of $\Alg^{\nunull,\nunull}$, and for each $w \in \fS^{t-1}$, the distribution of $\Algbar^{\MD_w^+,\MD_w^-}$ is exactly the distribution of $\Alg^{\nu_{w,1},\nu_{w,0}}$. Thus, \cref{item:indist} of \cref{lemma:ptf-distributions}, applied to $\Algbar$, implies that
\[\left|\EE_{w \sim \Unif(\fS^{\nsmall-1})} \EE[\Alg^{\nu_{w,0},\nu_{w,1}}] - \EE[\Alg^{\nunull,\nunull}]\right| \leq 2^{-t^{2/3}}.\]
By definition of $t$, this bound decays super-polynomially in $n$.

\item Fix $w \in \fS^{t-1}$. We have
\begin{align*}
\Pr_{x \sim \nu_{w,1}}[\langle x,\theta(w)\rangle < 0]
&= \Pr_{x \sim \MD_w^+}[\langle \tau(x), \theta(w)\rangle < 0] \\ 
&= \Pr_{x \sim \MD_w^+}[f_w(x) = -1] \\ 
&\leq \exp(-\Omega(\ell^2/t)) \\ 
&\leq \exp(-\Omega(\log^2 \log n)) \\
&\leq (\log n)^{-\Omega(\log \log n)}
\end{align*}
by \cref{eq:misspec-plus} and the definitions of $\ell$ and $t$. A symmetric argument with $\MD_w^-$ and \cref{eq:misspec-minus} shows that
\[\Pr_{x \sim \nu_{w,0}}[\langle x,\theta(w) \rangle \geq 0] \leq (\log n)^{-\Omega(\log \log n)}.\]
Finally,
\begin{align*}
\Pr_{x \sim \nunull}[\langle x,\theta(w)\rangle \geq 0]
&= \Pr_{x \sim \MDnull}[\langle \tau(x), \theta(w)\rangle \geq 0] \\ 
&= \Pr_{x \sim \MDnull}[f_w(x) = 1] \\ 
&\geq 1/2
\end{align*}
by \cref{eq:null-bias}.

\item Given $w \in \fS^{t-1}$, we can sample $x \in \RR^t$ with $\TV(\Law(x),\MD_w^+) \leq 2^{-\Omega(t)}$ in time $\poly(t)$ (by \cref{item:samp}), and we can then compute $\tau(x)$ in time $\poly(n)$. By the data processing inequality, $\TV(\Law(\tau(x)), \nu_{w,1}) \leq 2^{-\Omega(t)} \leq n^{-\omega(1)}$. Moreover, we can compute $f_w(x) = \sgn(\langle \tau(x),\theta(w)\rangle)$ in time $\poly(t)$ by \cref{item:samp}. The same argument works for $\nu_{w,0}$.

\item Since $\MDnull = N(0,\frac{1}{2\pi}I_t)$, we can efficiently sample $x \sim \MDnull$ and then compute $\tau(x)$, which has distribution $\nunull$ by definition.
\end{enumerate}
This completes the proof.
\end{proof}

\subsection{Statistical Tractability of Generalized Block MDPs}\label{sec:halfspace-statistical}

In this section we prove \cref{prop:halfspace-rl-statistical}, which asserts that reward-free RL in the family of Generalized Block MDPs (and hence Low-Rank MDPs) $\MM_n$ is \emph{statistically} tractable---and hence the hardness result from the preceding section is purely a \emph{computational} phenomenon. 

\begin{proposition}\label{prop:halfspace-rl-statistical}
There is a (computationally \emph{inefficient}) algorithm $\Alg^M$ that, given episodic access to any MDP $M \in \MM_n$, has sample complexity $\poly(n)$ and produces a set of policies $\Psi$ satisfying the following guarantee with probability at least $1 - o(1)$:
\[\forall s \in \MS: \max_{\pi\in\Psi} d^{M,\pi}_H(s) \geq \frac{1}{64} \cdot \left(\max_{\pi\in\Pi} d^{M,\pi}_H(s) - \frac{1}{8}\right).\]
\end{proposition}

Since $\Phi_n$ has infinite cardinality, this does not follow from a black-box application of prior results on learning Low-Rank MDPs. However, as we verify in \cref{thm:mbfr-pdim}, the main result of \cite{mhammedi2023efficient} can be extended to this setting so long as an appropriate function class based on $\Phi_n$ has bounded \emph{pseudo-dimension} (\cref{def:pdim}). We then verify the requisite pseudo-dimension bound (\cref{lemma:halfspace-pdim}). From these, the proof of \cref{prop:halfspace-rl-statistical} is straightforward.

\begin{definition}[Pseudo-dimension \citep{haussler2018decision}]\label{def:pdim}
For any set $\MX$ and function class $\MF \subseteq \{\MX \to \RR\}$, the pseudo-dimension of $\MF$ is defined as the VC dimension of $\MF^+$, where $\MF^+ \subseteq \{\MX\times\RR \to \{0,1\}\}$ is defined as $\MF^+ := \{(x,\xi) \mapsto \mathbbm{1}[f(x) > \xi]: f \in \MF\}$.
\end{definition}

We prove the following straightforward extension of a result by \cite{mhammedi2023efficient}:

\begin{theorem}[Extension of {\cite[Theorem 3.2]{mhammedi2023efficient}}]\label{thm:mbfr-pdim}
Let $\MS,\MX, \MA$ be sets, let $H \in \NN$, and let $\Phi$ be a set of functions $\phi: \MX\to\MS$. Let $\fd$ denote the pseudo-dimension (\cref{def:pdim}) of the function class $\MF \subseteq (\MX\times\MA\times\MX \to \RR)$ defined by
\begin{equation} \MF := \left\{(x,a,x') \mapsto f(\phi^1(x),\phi^2(x),\phi^3(x'),\phi^4(x'),a) \mid{} f: \MS^4\times\MA \to [0,1], \phi^1,\phi^2,\phi^3,\phi^4 \in \Phi\right\}.\label{eq:disc-class-mbfr}\end{equation}

There is an algorithm $\Alg^M$ that takes input $\epsilon,\delta \in (0,1/2)$ and episodic access to an MDP $M$ with observation space $\MX$, action space $\MA$, and horizon $H$, and has the following property. If $M$ is a generalized $\Phi$-decodable block MDP, then $\Alg^M(\epsilon,\delta)$, with probability at least $1-\delta$, produces sets $\Psi_{1:H}$ such that
\begin{align} &\EE_{\pi\sim\Unif(\Psi_h)} d^{M,\pi}_h(x) \geq \frac{1}{8|\MA|^2|\MS|} \max_{\pi\in\Pi} d^{M,\pi}_h(x) \nonumber\\
&\forall h \in \{2,\dots,H\}, x \in \MX: \max_{\pi\in\Pi} d^{M,\pi}_h(x) \geq \epsilon \sum_{(s,a)\in\MS\times\MA} \BP^M_{h}(x|s,a).\label{eq:vox-guarantee}
\end{align}
Moreover, the algorithm has sample complexity $\poly(H,|\MS|,|\MA|,1/\epsilon,\fd,\log(1/\delta))$.
\end{theorem}

\begin{proof}
First suppose that $|\Phi| < \infty$ and we replace the pseudo-dimension term $\fd$ in the sample complexity with $\log |\Phi|$. Then we claim that this theorem is essentially immediate from \cite[Theorem 3.2]{mhammedi2023efficient}: by \cref{prop:genblock-is-lowrank}, any generalized $\Phi$-decodable block MDP is low-rank with rank $d := |\MS||\MA|$ and function class $\Philin$ of size exactly $|\Phi|$. It can be checked that the norm bounds required by \cite{mhammedi2023efficient} are satisfied: $\norm{\philin(x,a)}_2 \leq 1$ for all $x\in\MX$, $a \in \MA$, and $\philin\in\Philin$, and 
\[\norm{\int_\MX g(x) d\mu^\st_{h+1}}_2 \leq \sqrt{d}\]
for any $h \in [H-1]$ and $g: \MX \to [0,1]$, since $(\mu^\st_{h+1})_{s,a}$ is a distribution over $\MX$ for each $s\in\MS$ and $a \in \MA$. Thus, \cite[Theorem 3.2]{mhammedi2023efficient} shows that with probability at least $1-\delta$, the output $\Psi_{1:H}$ is a $(1/(8|\MA|^2||\MS|),\epsilon)$-policy cover. However, inspecting the proof, in fact the stronger guarantee is shown that the output is a $(1/(8|\MA|^2|\MS|), \epsilon)$-\emph{randomized} policy cover \--- see \cite[Definition 2.2]{mhammedi2023efficient}. This gives \cref{eq:vox-guarantee}, using the fact that $\norm{\mu^\st_{h+1}(x)}_2 \leq \sum_{s,a} \BP_{h+1}(x\mid{}s,a)$ for any $x \in \MX$.

It remains to argue that the result can be generalized to infinite classes $\Phi$ using pseudo-dimension. The dependence on $\log|\Phi|$ in the sample complexity bound in \cite[Theorem 3.2]{mhammedi2023efficient} arises due to the need to prove uniform concentration over $\Phi$. In particular, it arises in two places:
\begin{enumerate}
\item Analysis of $\PSDP$: \cite[Lemma D.2]{mhammedi2023efficient} incurs dependence on $\log|\Phi|$ while proving that the regression estimate $\hat g^{(t)}$ computed on line 7 of \cite[Algorithm 3]{mhammedi2023efficient} is close to the $Q$-function $Q^{\hat \pi^{t+1}}_t$. However, we can remove this dependence as follows. With notation as in line 7 of \cite[Algorithm 3]{mhammedi2023efficient}, it suffices to prove that so long as $|\MD^{(t)}| \geq \poly(\fd, 1/\epsilon,\log(1/\delta))$, we have with probability at least $1-\delta$ that
\[\sup_{g \in \MG_t} \left|\frac{1}{|\MD^{(t)}|} \sum_{(x,a,R) \in \MD^{(t)}} (g(x,a) - R)^2 - \EE[(g(x,a) - R)^2]\right| \leq \epsilon.\]
In particular, we need this to hold for $\MG_t := \{(x,a) \mapsto f(\phi(x),a) \mid{} f: \MS\times\MA \to [0,1], \phi\in\Phi\}$. By \cite[Corollary 4.3]{modi2021model} and the triangle inequality, it suffices to bound the pseudo-dimension of the function classes
\[\MF_1 := \{(x,a) \mapsto f(\phi(x),a)^2 \mid{} f: \MS\times\MA \to [0,1], \phi\in\Phi\},\]
\[\MF_2 := \{(x,a,R) \mapsto f(\phi(x),a) \cdot R \mid{} f: \MS\times\MA \to [0,1], \phi\in\Phi\},\]
and
\[\MF_3 := \{R \mapsto R^2\}.\]
Note that $\MF_3$ has constant size and hence constant pseudo-dimension; each function in $\MF_2$ can be expressed as a product of some function in $\MG_t$ with the function $R \mapsto R$; and each function in $\MF_1$ can be expressed as a product of two functions in $\MG_t$. Thus, by \cite[Lemma 50]{modi2021model}, it suffices to bound the pseudo-dimension of $\MG_t$. Since $\MG_t$ can be embedded in $\MF$, the pseudo-dimension of $\MG_t$ is bounded by $\fd$.

\item Analysis of \texttt{RepLearn}: \cite[Lemma F.1]{mhammedi2023efficient} incurs dependence on $\log |\Phi|$ through application of \cite[Lemma 14]{modi2021model}, which in turn uses \cite[Lemma 17]{modi2021model}, which in turn uses \cite[Lemma 34]{modi2021model}. In the proof of \cite[Lemma 34]{modi2021model}, a factor of $\log |\Phi|$ is incurred to bound the log covering number of a particular function class $\MH$, which consists of certain functions $h: \MX\times\MA\times\MX\to\RR$. For our application, the function classes $\Phi,\Phi'$ in the lemma statement are both $\Philin$, and the reward function class $\MR$ is $\{(x,a) \mapsto f(\phi(x),a): f:\MS\times\MA \to [0,1], \phi\in\Phi\}$. From this, we can check that each function $h \in \MH$ depends on its input $(x,a,x')$ only through $a$ and $\phi^1(x),\phi^2(x),\phi^3(x'),\phi^4(x')$ for four functions $\phi^1,\phi^2,\phi^3,\phi^4 \in \Phi$. Thus, $\MH$ can be embedded in $\MF$, and so the pseudo-dimension of $\MH$ can be bounded by $\fd$. This means that in the proof of \cite[Lemma 34]{modi2021model}, instead of bounding the log covering number, we can apply \cite[Corollary 43]{modi2021model} using this bound on the pseudo-dimension of $\MH$. The rest of the proof of the lemma is unchanged.
\end{enumerate}
This completes the proof.
\end{proof}



\begin{lemma}\label{lemma:halfspace-pdim}
Fix $n \in \NN$ and let $\Phi_n$ denote the class of linear threshold functions (\cref{def:halfspace-mdps-apx}). The function class $\MF_n \subseteq (\RR^n\times\{0,1\}\times\RR^n \to \RR)$ defined by
\[\MF_n := \left\{(x,a,x') \mapsto f(\phi^1(x),\phi^2(x),\phi^3(x'),\phi^4(x'),a) \mid{} f: \{0,1\}^5 \to [0,1], \phi^1,\phi^2,\phi^3,\phi^4 \in \Phi_n\right\}\]
has pseudo-dimension at most $O(n\log n)$.
\end{lemma}

\begin{proof}
By \cref{def:pdim}, we need to bound the VC dimension of the function class $\MF^+_n \subseteq (\RR^n\times\{0,1\}\times\RR^n\times\RR \to \RR)$
\[\MF^+_n := \left\{(x,a,x',\xi) \mapsto \mathbbm{1}[f(\phi^{1:2}(x),\phi^{3:4}(x'),a)>\xi] \mid{} f: \{0,1\}^5 \to [0,1], \phi^1,\phi^2,\phi^3,\phi^4 \in \Phi_n\right\}.\]
Fix any set $\MD = (x_i,a_i,x'_i,\xi_i)_{i=1}^m$. We would like to upper bound the number of attainable vectors
\[(\mathbbm{1}[f(\phi^{1:2}(x_1),\phi^{3:4}(x'_1),a_1)>\xi_1],\dots,\mathbbm{1}[f(\phi^{1:2}(x_m),\phi^{3:4}(x'_m),a_m)>\xi_m]) \in \{0,1\}^m\]
as $f$ and $\phi^1,\dots,\phi^4$ vary. First, note that the numbers $\xi_1,\dots,\xi_m$ partition $\RR$ into $m+1$ intervals, and two functions $f,f'$ such that $f(b)$ and $f'(b)$ lie in the same interval for all $b \in \{0,1\}^5$ induce the same vector (for any fixed $\phi^1,\dots,\phi^4$). Thus, we can restrict focus to $(m+1)^{32} = \poly(m)$ choices of $f$.

Fix one such $f$. By the Milnor-Warren bound \cite{milnor1964betti}, the set $\{(\phi(x_1),\dots,\phi(x_m)): \phi \in \Phi_n\}$ has size at most $m^{O(n)}$. Thus, as $\phi^1,\dots,\phi^4 \in \Phi_n$ vary, the number of attainable vectors is bounded by $m^{O(n)}$. Summing over the $\poly(m)$ choices of $f$, we find that the total number of attainable vectors as all parameters vary is still $m^{O(n)}$, so $\MD$ cannot be shattered by $\MF_n^+$ unless $m \leq O(n\log n)$. Thus, the pseudo-dimension of $\MF_n$ is at most $O(n\log n)$.
\end{proof}

\begin{proof}[Proof of \cref{prop:halfspace-rl-statistical}]
We apply \cref{thm:mbfr-pdim} with $\MS := \MA := \{0,1\}$, $\MX := \RR^n$, $H := (\log n)^{\log \log n}$, and $\Phi := \Phi_n$. By \cref{lemma:halfspace-pdim}, the pseudo-dimension of the resulting set $\MF$ defined in \cref{eq:disc-class-mbfr} is at most $O(n\log n)$. Invoke the algorithm $\Alg^M$ guaranteed by \cref{thm:mbfr-pdim} with parameters $\epsilon := \frac{1}{512}$ and $\delta := 1/2$. For any $M \in \MM_n$, since $M$ is a generalized $\Phi_n$-decodable block MDP, we get that with probability at least $1/2$, the sets $\Psi_{1:H}$ produced by $\Alg^M$ satisfy \cref{eq:vox-guarantee}. Let $\Xgood$ be the set of $x \in \MX$ satisfying
\[\max_{\pi\in\Pi} d^{M,\pi}_H(x) \geq \epsilon \sum_{(s,a)\in\MS\times\MA} \BP^M_H(x\mid{}s,a).\]
Suppose that $M$ has decoding functions $\phist_1,\dots,\phist_H$. For any $s \in \MS$, we have
\begin{align*}
\max_{\pi\in\Psi} d^{M,\pi}_H(s)
&\geq \EE_{\pi\sim\Unif(\Psi_H)} \left[d^{M,\pi}_H(s) \right]\\ 
&\geq \sum_{x\in\Xgood: \phist_H(x)=s} \EE_{\pi\sim\Unif(\Psi_H)} \left[ d^{M,\pi}_H(x) \right]\\ 
&\geq \frac{1}{64} \sum_{x\in\Xgood: \phist_H(x)=s} \max_{\pi\in\Pi} d^{M,\pi}_H(x) \\ 
&\geq \frac{1}{64} \max_{\pi\in\Pi} \sum_{x\in\Xgood: \phist_H(x)=s} d^{M,\pi}_H(x) \\ 
&\geq \frac{1}{64} \max_{\pi\in\Pi} \left(d^{M,\pi}_H(s) - \sum_{x\in\MX\setminus\Xgood: \phist_H(x)=s} d^{M,\pi}_H(x)\right) \\ 
&\geq \frac{1}{64} \max_{\pi\in\Pi} \left(d^{M,\pi}_H(s) - \epsilon \sum_{x\in\MX} \sum_{(s,a)\in\MS\times\MA} \BP^M_H(x|s,a)\right) \\ 
&= \frac{1}{64} \max_{\pi\in\Pi} \left(d^{M,\pi}_H(s) - \epsilon |\MS||\MA|\right) \\ 
&= \frac{1}{64} \max_{\pi\in\Pi} \left(d^{M,\pi}_H(s) - \frac{1}{8}\right)
\end{align*}
by choice of $\epsilon$. Finally, the sample complexity of the algorithm is $\poly(n)$ by \cref{thm:mbfr-pdim}, the fact that the pseudo-dimension is at most $O(n\log n)$, the choice of parameters $\epsilon,\delta = \Omega(1)$, and the fact that $H,|\MS|,|\MA| \leq n$.
\end{proof}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
