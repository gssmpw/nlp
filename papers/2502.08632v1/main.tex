\newif\ifarxiv 
\arxivtrue
\newif\ifcolt
%\colttrue

\ifarxiv
\documentclass{article}
\input{commands}
\newcommand{\citep}[1]{\cite{#1}}
\newcommand{\citet}[1]{\cite{#1}}
\fi



\ifcolt
\PassOptionsToPackage{dvipsnames}{xcolor}
\PassOptionsToPackage{hypertexnames=false}{hyperref}
\documentclass[anon,12pt,cleveref]{colt2025}
\usepackage{times}
\input{colt_commands}
\fi


\newcommand{\sssref}[1]{\texorpdfstring{\hyperref[#1]{\mbox{Section \ref*{#1}}}}{Section \ref*{#1}}}

\newcommand{\lineref}[1]{\texorpdfstring{\hyperref[#1]{\mbox{Line \ref*{#1}}}}{Line \ref*{#1}}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5

\input{dylan}

\usepackage{inconsolata}

\usepackage{etoolbox}
\usepackage{comment}
\newtoggle{colt}
\ifcolt 
\toggletrue{colt}
\fi
\newcommand{\colt}[1]{\iftoggle{colt}{#1}{}}
\newcommand{\arxiv}[1]{\iftoggle{colt}{}{#1}}
\newcommand{\loose}{\looseness=-1}

\usepackage{titletoc}
\usepackage{caption}

\usepackage{crossreftools}
\pdfstringdefDisableCommands{%
    \let\Cref\crtCref
    \let\cref\crtcref
  }
  
  \newcommand{\creftitle}[1]{\crtcref{#1}}

\colt{\hbadness = 10000}
  
% This needs to come after we include thmtools
% \makeatletter
%   \renewenvironment{proof}[1][Proof]%
%   {%
%    \par\noindent{\bfseries\upshape {#1.}\ }%
%   }%
%   {\qed\newline}
%   \makeatother

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5

\usepackage{graphicx} % Required for inserting images

%\usepackage{color-edits}
 \usepackage[suppress]{color-edits}
\addauthor{df}{ForestGreen}
\newcommand{\dfc}[1]{\dfcomment{#1}}
\addauthor{dr}{BurntOrange}
\newcommand{\dfr}[1]{\drcomment{#1}}

\colt{
\title[Toward A Computational Taxonomy For Reinforcement
Learning]{Necessary and Sufficient Oracles: Toward a Computational\\ Taxonomy For Reinforcement Learning}
}
\arxiv{ 
\title{Necessary and Sufficient Oracles: Toward a Computational\\ Taxonomy For Reinforcement Learning}
}
\date{\today}
\arxiv{
\author{
Dhruv Rohatgi\thanks{Email: \texttt{drohatgi@mit.edu}. This research was partially conducted during the author's internship at Microsoft Research.} \\ MIT  \and Dylan J. Foster\thanks{Email: \texttt{dylanfoster@microsoft.com}.} \\ Microsoft Research}
}

\begin{document}

\maketitle
\allowdisplaybreaks

\begin{abstract}
Algorithms for reinforcement learning (RL) in large state spaces
crucially rely on supervised learning subroutines to estimate objects
such as value functions or transition probabilities. Since only the
simplest supervised learning problems can be solved provably and
efficiently, practical performance of an RL algorithm depends on which
of these supervised learning ``oracles'' it assumes access to (and how they are implemented). But which oracles are better or worse? Is there a \emph{minimal} oracle?\loose

In this work, we clarify the impact of the choice of supervised
learning oracle on the computational complexity of RL, as quantified by the oracle strength. First, for the task of reward-free exploration in Block MDPs in the standard episodic access model---a ubiquitous setting for RL with function approximation---we identify \emph{two-context regression} as a minimal oracle, i.e. an oracle that is both necessary and sufficient (under a mild regularity assumption). Second, we identify \emph{one-context regression} as a near-minimal oracle in the stronger \emph{reset} access model, establishing a provable computational benefit of resets in the process. Third, we broaden our focus to \emph{Low-Rank MDPs}, where we give cryptographic evidence that the analogous oracle from the Block MDP setting is insufficient.\loose

\iffalse
Recent work \citep{golowich2024exploration} formalized these questions in the language of computational complexity: a supervised learning oracle is \emph{minimal} for an RL task in a model class $\MM$ if it is both \emph{necessary} and \emph{sufficient} under polynomial-time reductions. However, no minimal oracles were known to exist. We settle this question for the natural task of reward-free exploration in Block MDPs \citep{du2019provably}. Under a mild regularity assumption on the concept class $\Phi$, we show:
\begin{enumerate}
    \item In the \emph{online RL} access model \citep{kearns2002near}, improper two-context regression is a minimal oracle.
    \item In the \emph{RL with resets} access model, improper one-context regression is a sufficient oracle.
\end{enumerate}
Beyond Block MDPs, we give cryptographic evidence that exploration in Low-Rank MDPs faces unique computational obstacles, potentially due to the lack of \emph{weight function realizability}. 
\fi
\end{abstract}

\colt{
\begin{keywords}%
Reinforcement learning, computational complexity, oracle-efficiency
\end{keywords}
}


\arxiv{\newpage}
\section{Introduction}
\input{intro}


%Unlike classical distributional assumptions such as Gaussianity, Unfortunately, common practice in theoretical RL is to treat oracle-efficiency But when different algorithms rely on different oracles, how do we compare the respective assumptions?


\section{Preliminaries}\label{sec:prelim}

\input{prelim}

\section{A Minimal Oracle for Episodic RL in Block MDPs}\label{sec:online}
\input{online}




\section{A Simpler Oracle for RL in Block MDPs with Reset Access}\label{sec:resets}

\input{resets}

\section{A Computational Separation for Low-Rank MDPs}\label{sec:lowrank}

\input{lowrank}

\arxiv{

\section{Discussion and Future Work}\label{sec:discussion}
\input{discussion}
}


\ifarxiv \bibliographystyle{alpha} \fi
\bibliography{bib}

\newpage

\appendix

\renewcommand{\contentsname}{Contents of Appendix}
		%
%\addtocontents{toc}{\protect\setcounter{tocdepth}{2}}
%{
%  \hypersetup{hidelinks}
%  \tableofcontents
%}
\crefalias{section}{appendix}


\startcontents[appendices]
\printcontents[appendices]{l}{1}{\section*{Contents of Appendix}\setcounter{tocdepth}{2}}


\newpage
\section{Additional Related Work}\label{sec:app_related}

\input{app_related}

\newpage
\section{Additional Preliminaries}

\input{app_prelim}

\newpage
\section{Proof of \creftitle{cor:online-rl-to-regression}}\label{sec:app_online}

\input{app_online}

\newpage

\section{Proof of \creftitle{cor:regression-to-online-rl}}\label{sec:app_minimality}

\input{app_minimality}

\newpage

\section{Proof of \creftitle{cor:reset-rl-to-regression}}\label{sec:app_resets}

\input{app_resets}

\newpage

\section{Proofs from \creftitle{sec:lowrank}}\label{app:lowrank}

\input{app_lowrank}

\newpage

\section{Supporting Technical Results}

This section contains supporting technical results. \cref{sec:psdp}
gives a self-contained presentation of the $\PSDP$ algorithm, while
\cref{sec:misc} gives miscellaneous regression reductions used
throughout our main results.

\subsection{Policy Search by Dynamic Programming (PSDP)}
\label{sec:psdp}
\input{psdp}

\subsection{Miscellaneous Reductions}
\label{sec:misc}
\input{onered}

\colt{
\newpage
\section{Discussion and Future Work}\label{sec:discussion}
\input{discussion}
}



\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
