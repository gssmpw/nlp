\nc{\bphi}{\boldsymbol{\phi}}
\nc{\nunull}{\nu_{\mathsf{null}}}
\nc{\bast}{\ba^\st}
\nc{\Algbar}{\overline{\Alg}}
\nc{\MDnull}{\MD^\circ}
\nc{\epdisc}{\varepsilon_{\mathsf{disc}}}
\nc{\halfspacealg}{\mathtt{HalfspaceLearn}}
\nc{\cdisc}{c_{\mathsf{disc}}}
\nc{\WH}{(\fS^{t-1})^H}
\nc{\MObar}{\ol\MO}
\nc{\phist}{\phi^\st}
\nc{\must}{\mu^\st}
\nc{\Philin}{\Phi^{\mathsf{lin}}}
\nc{\philin}{\phi^{\mathsf{lin}}}
\nc{\fd}{\mathfrak{d}}
\nc{\Xgood}{\MX_{\mathsf{good}}}
\nc{\mulin}{\mu^{\mathsf{lin}}}

\nc{\nsmall}{t}
\nc{\nlarge}{n}
\nc{\fS}{\mathfrak{S}}

\iffalse
A \emph{low-rank} MDP is an MDP where the transitions are linear in some unknown $d$-dimensional feature mapping $\phist: \MX\times\MA \to \RR^d$ that lies in some known function class, i.e.:
\[\BP_{h+1}(x_{h+1}|x_h,a_h) = \langle \phist_h(x_h,a_h), \must_{h+1}(x_{h+1})\rangle.\]
\fi 

We now move beyond Block MDPs. \emph{Low-Rank} MDPs \citep{modi2021model} are perhaps the simplest commonly-studied model class that generalizes the Block MDP.
% expressive model classes
While Low-Rank MDPs admit oracle-efficient RL algorithms, the \emph{oracles} used seem significantly more complex---both in the episodic and reset access model---than the oracles used for Block MDPs. In particular, $\VOX$ \citep{mhammedi2023efficient}, which operates in the episodic access model, uses a proper min-max optimization oracle, and $\RVFS$ \citep{mhammedi2024power}, which requires reset access, uses an agnostic and cost-sensitive regression oracle. Comparing to our results for Block MDPs, it is natural to ask whether this apparent gulf in computational tractability is real, and if so, what the structural source is. In this section we make progress on this question, with a focus on the reset access model.\loose

\paragraph{Preliminaries.} An MDP $M$ is Low-Rank with rank $d$ if there are maps $\philin_h:\MX\times\MA\to\RR^d$ and $\mulin_{h+1}:\MX\to\RR^d$ such that the transitions have the following factorization:
\arxiv{\[\BP_{h+1}(x_{h+1}\mid{}x_h,a_h) = \langle \philin_h(x_h,a_h), \mulin_{h+1}(x_{h+1})\rangle.\]}
\colt{$\BP_{h+1}(x_{h+1}\mid{}x_h,a_h) = \langle \philin_h(x_h,a_h), \mulin_{h+1}(x_{h+1})\rangle.$}
Prior \dfedit{model-free} work assumes that $\philin_{1:H}$ lie in a known feature class $\Philin$, but the dual features $\mulin_{1:H}$ are arbitrary \citep{modi2021model,mhammedi2023efficient}.\footnote{Model-based works \citep{agarwal2020flambe,uehara2022representation} also assume that $\mulin_{1:H}$ lie in a known dual feature class.\loose} Given implicit access to $\Philin$ via some oracle(s), the typical goal is to design RL algorithms with time and oracle complexity polynomial in $d$, the horizon $H$, and the number of actions $\MA$.\arxiv{
 } Block MDPs with concept class $\Phi$ can be embedded in a class of Low-Rank MDPs with rank $d := |\MS||\MA|$: simply define $\Philin := \{\philin: \phi\in\Phi\}$ where $\philin:\MX\times\MA\to\RR^d$ \arxiv{is defined by mapping}\colt{maps} to an appropriate basis vector, i.e. $\philin(x,a) := e_{\phi(x),a}$. Analogously, there is a natural extension of one-context regression to Low-Rank MDPs.\loose
%\vspace{-0.3em}
\begin{definition}[Informal]\label{Def:low-rank-reg}
The goal of \textbf{one-context low-rank regression} over $\Philin$ is the following: given an i.i.d. dataset $(x^{(i)},a^{(i)},y^{(i)})_i$ satisfying the realizability assumption $\EE[y^{(i)}\mid{}x^{(i)},a^{(i)}] = \langle \philin(x^{(i)},a^{(i)}), \theta\rangle$ for unknown $\philin\in\Philin$ and $\theta\in\RR^d$,
estimate the function $(x,a) \mapsto \EE[y\mid{}x,a]$.
\end{definition}
%\vspace{-0.3em}
This oracle suffices to implement $\PSDP$ in Low-Rank MDPs---so \emph{given} a policy cover,\footnote{See \cite{mhammedi2023efficient} for the precise notion of a policy cover in Low-Rank MDPs.} we can optimize a reward \citep{mhammedi2023efficient}. In analogy with \cref{cor:reset-rl-to-regression}, we ask: is this oracle sufficient for the full task of reward-free RL with resets? We show that it is not, and shed light on why.\loose
% In fact, we show that there is an intermediate model class between Block and Low-Rank, the \emph{Generalized Block MDP}, where the oracle is already insufficient.


\arxiv{\paragraph{Generalized Block MDPs.}}\colt{\vspace{0.3em}\noindent\textbf{Generalized Block MDPs.}} It is well known that Block MDPs are a special case of Low-Rank MDPs \citep{modi2021model,zhang2022efficient}. To highlight that there are \emph{multiple} important assumptions in the Block MDP definition, and to gain a finer understanding of how these assumptions interact with computational tractability, we introduce---and prove our hardness result in---an intermediate model class between Block MDPs and Low-Rank MDPs. Let $\Phi \subseteq (\MX\to\MS)$ be any concept class. A $\Phi$-decodable \emph{Generalized} Block MDP is an MDP $M = (H, \MX,\MA,(\BP_h)_h)$ with the property that there exist $\phist_1,\dots,\phist_H \in \Phi$ so that $\BP_{h+1}(x_{h+1}\mid{}x_h,a_h)$ is a function of $x_{h+1}$, $\phist_h(x_h)$, and $a_h$.


\begin{figure}[t]
\centering
\includegraphics[width=0.4\textwidth]{class_diagram.eps}
\caption{Containment diagram for model classes discussed in \cref{sec:lowrank}. Here, $\Phi \subseteq (\MX\to\MS)$ is any concept class, and $\Philin := \{\philin:\phi\in\Phi\}$ where $\philin(x,a) := e_{\phi(x),a}\in\RR^{|\MS||\MA|}$.}\label{fig:classes}
\end{figure}

Generalized Block MDPs are still Low-Rank MDPs via the same embedding discussed above (\cref{prop:genblock-is-lowrank})---the features are appropriate basis vectors. However, compared to standard Block MDPs, the transition probability $\BP_{h+1}(x_{h+1}\mid{}x_h,a_h)$ can now depend arbitrarily on $x_{h+1}$.\footnote{We also defined standard Block MDPs to have a single decoding function $\phi^\st$ rather than one per layer, but this is was a superficial choice made for notational simplicity; our RL algorithms from prior sections still apply if there is one function per layer, and the one-context regression oracle needed by $\PCR$ doesn't change.} Moreover, in Generalized Block MDPs, \cref{Def:low-rank-reg} simplifies back to \cref{def:one-con-regression}, our original definition of one-context regression (\cref{prop:gen-block-ocr}). These connections are summarized in \cref{fig:classes}. 

\arxiv{\paragraph{Hardness for Generalized Block MDPs.}}\colt{\vspace{0.3em}\noindent\textbf{Hardness for Generalized Block MDPs.}}
Unfortunately, the arbitrary dependence of the transition probability on $x_{h+1}$ \arxiv{in Generalized Block MDPs} defeats algorithms such as $\PCR$, since the ideal kinematics (see \cref{eq:pcr-kinematics}) are no longer a function of just $\phi^\st(x_{h+1})$. But could there be a more clever algorithm that avoids needing to estimate such quantities? In \cref{thm:halfspace-main}, we show that there is an inherent computational barrier. %this is an inherent computational issue. \dfc{might be good to remark that people don't seem to have noticed this difference before.}

\iffalse
\begin{definition}\label{def:halfspace-mdps}
Fix $n \in \NN$. We define $\MM_n$ to be the family of $\Phi_n$-decodable Generalized Block MDPs with horizon $H := (\log n)^{\log \log n}$, observation space $\MX = \RR^n$, latent state space $\MS = \{0,1\}$, action space $\MA = \{0,1\}$, and feature class $\Phi_n = \{\phi^\theta: \theta \in \RR^n\}$ consisting of linear threshold functions, i.e. where 
\[\phi^\theta(x) := \mathbbm{1}[\langle x,\theta\rangle \geq 0].\]
\end{definition}
\fi

%We show that one-context regression in this setting is computationally tractable (\cref{thm:halfspace-reg}) and yet reward-free RL is computationally hard under a standard cryptographic assumption (\cref{thm:halfspace-rl-hard}). Moreover, we observe that the reward-free RL problem is \emph{statistically} tractable (\cref{prop:halfspace-rl-statistical}), so the hardness is a computational phenomenon.

%\subsection{Tractability of one-context regression}

For $n \in \NN$, let $\Phi_n := \{\phi^\theta:\bbR^{n}\to\crl{0,1}\mid{}\theta\in\RR^n\}$ be the concept class of linear threshold\arxiv{ functions}\colt{s}: $\phi^\theta(x) \ldef \mathbbm{1}[\langle x,\theta\rangle \geq 0]$. We prove that one-context regression for $\Phi_n$ is computationally tractable, but reward-free RL with resets for $\Phi_n$-decodable Generalized Block MDPs is cryptographically hard.

\begin{theorem}[\cref{thm:halfspace-reg}+\cref{thm:halfspace-rl-hard}]\label{thm:halfspace-main}
There is an algorithm for one-context regression with concept class $\Phi_n$, that achieves error $\epsilon$ with probability at least $1-\delta$ and has time complexity $\poly(n,1/\epsilon,1/\delta)$.
In contrast, suppose there exists an $\Alg^M$ that---given interactive reset access to any $\Phi_n$-decodable Generalized Block MDP $M$ with horizon $H$, observation space $\MX=\RR^n$, latent state space $\MS=\{0,1\}$, and action space $\MA$---has time complexity $\poly(n,H,|\MA|)$ and produces a set of policies $\Psi$ satisfying the following guarantee with probability at least $1/2$:
\begin{equation} \forall s \in \MS: \max_{\pi\in\Psi} d^{M,\pi}_H(s) \geq \frac{1}{\poly(|\MA|, |\MS|, H)} \left(\max_{\pi\in\Pi} d^{M,\pi}_H(s) - \frac{1}{8}\right).\label{eq:gen-block-rfrl}\end{equation}
Then the Continuous Learning With Errors (cLWE) hardness assumption (\cref{ass:clwe}) is false. %\dfc{probably good to spell out here that  any algo with runtime polynomial in the usual parameters we expect ($d$, $H$, $1/\veps$, $1/\delta$, etc) would have time complexity $\poly(n)$ here.}
\end{theorem}
\cref{thm:halfspace-main} rules out any reduction from reward-free RL with resets in $\Phi_n$-decodable Generalized Block MDPs to one-context regression, where the oracle time complexity of the reduction is allowed to scale polynomially in all relevant parameters ($H$, $|\MS|$, $|\MA|$, $1/\epsilon$, $1/\delta$, and the description length $n$ of an observation). By \cref{cor:reset-rl-to-regression}, this separates standard Block MDPs from Low-Rank MDPs. \arxiv{Additionally, the cLWE assumption can in turn be based on classical LWE and therefore on worst-case hardness of approximate shortest vector on lattices---see \cite[Corollary 3]{gupte2022continuous} and references.}\colt{We remark that the cLWE assumption can be based on classical LWE \citep{gupte2022continuous}.\loose
}

%\dfc{i would consider merging this theorem statement with the second one below, so that we have a single theorem with the main takeaway for this section.}





%\subsection{Hardness of reward-free RL with resets}

 





\begin{remark}[On the reward-free solution concept]
Comparing \cref{eq:gen-block-rfrl} to \cref{def:strong-rf-rl}, we have \emph{relaxed} the notion of exploration to allow for multiplicative error in the visitation probability. In other words, we are only asking for an $(\alpha,\epsilon)$-policy cover where $\alpha = 1/\poly(H,|\MA|,|\MS|)$. The reason is that for Generalized Block MDPs, a $(1,\epsilon)$-policy cover may not even exist. Fortunately, an $(\alpha,\epsilon)$-policy cover does always exist, and finding it is statistically tractable (\cref{prop:halfspace-rl-statistical}), indicating that the source of the hardness in \cref{thm:halfspace-rl-hard} is indeed purely computational.
\end{remark}

\paragraph{Proof overview.} In \cref{thm:halfspace-main} (proven in \cref{app:lowrank}), the algorithmic result is a (simple, but not immediate) consequence of seminal work on learning halfspaces \citep{blum2003noise,diakonikolas2023strongly}. The proof of the hardness result builds on recent work by \cite{tiegel2023hardness} on the cryptographic hardness of agnostic learning of halfspaces. Fix $n \in \NN$ and let $\fS^{t-1}$ denote the unit sphere in $t \approx \polylog(n)$ dimensions. \cite{tiegel2023hardness} shows that under cLWE, there are two families  $\{\nu_{w,0}: w \in \fS^{t-1}\}$ and $\{\nu_{w,1}: w \in \fS^{t-1}\}$ of distributions on $\RR^n$ such that for any $w$, the distributions $\nu_{w,0}$ and $\nu_{w,1}$ are \emph{approximately} separated by a hyperplane $\phi^{\theta(w)}$, but for unknown $w \sim \Unif(\fS^{t-1})$, distinguishing either $\nu_{w,0}$ or $\nu_{w,1}$ from a certain null distribution $\nunull$ is computationally hard.\loose

We use these distributions to construct a family of \emph{approximate} combination lock MDPs. These MDPs are parametrized by hidden vectors $w_1,\dots,w_H \in \fS^{t-1}$ and hidden actions $a^\st_1,\dots,a^\st_H \in \MA$. The transition distribution at any state $x_h\in\bbR^n$ and action $a_h$ is either $\nu_{w_{h+1},0}$ or $\nu_{w_{h+1},1}$, depending on the current latent state $\phi^{\theta(w_h)}(x_h)$ and whether $a_h$ equals $a^\st_h$. By a hybrid argument, we prove that these MDPs are indistinguishable from the MDP where all transitions follow $\nunull$, and hence learning $a^\st$ is impossible. However, we prove that learning $a^\st$ is necessary to solve the reward-free RL task.\loose

\paragraph{Takeaway: a computational role for weight function realizability?} The immediate reason why $\PCR$ fails for Low-Rank MDPs is that \emph{weight functions}, i.e. ratios $\BP_{h+1}(x_{h+1}\mid{}x,a) / \BP_{h+1}(x_{h+1}\mid{}x',a')$, are not realizable as linear functions in the feature mapping. In contrast, for Block MDPs the analogous realizability does hold, \dfedit{and plays a central role in\arxiv{ the design of} algorithms like \texttt{HOMER} and \texttt{MusIK}.} \cref{thm:halfspace-main} presents evidence that this distinction is computationally important, and perhaps suggests \emph{regression oracles for weight functions} as a route for generalizing our theory of computational tractability beyond Block MDPs. \arxiv{On the algorithmic front}\colt{Algorithmically}, methods from \citet{amortila2024scalable}---which apply to MDPs with low \emph{coverability} (subsuming Low-Rank MDPs)---offer some initial hope in this direction.\loose

%\dfc{Since we mention weight function realizability in the intro, i think it might be good to add a paragraph (with bold heading) here explaining (1) what weight function realizability is, and (2) why we think this result suggests that may be hardness for oracles beyond just one-context regression, even if we don't know how to prove this. E.g., we want to mention that weight function realizability does hold in BMDPs and this is part of why two-context regression is sufficient. I think it would be sufficient to move the paragraph from the discussion section to here and expand slightly}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
