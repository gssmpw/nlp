In this section we discuss related work in more detail. Most prior work in theoretical reinforcement learning consists of algorithm design with one of three goals, reflecting varying levels of concern with computational complexity:
\begin{enumerate}
\item statistical efficiency with arbitrary computation \citep{jiang2017contextual,foster2021statistical};
\item oracle-efficiency, but the choice of oracle is not the focus \citep{dann2018oracle};
\item end-to-end computational efficiency \citep{kearns1998efficient,jin2020provably,golowich2024exploring}.
\end{enumerate}

Our work fits between the second and third levels, since we are interested in the \emph{minimal} oracles that are necessary and sufficient for oracle-efficient RL---and we are interested in settings where end-to-end efficiency is unlikely. We therefore omit discussion of non-oracle-efficient algorithms. In \cref{sec:related-alg-block}, we survey prior oracle-efficient algorithms for RL in Block MDPs, with a focus on which oracles they require. In \cref{sec:related-alg-low-rank}, we give a partial survey of oracle-efficient algorithms for RL in Low-Rank MDPs and beyond. In \cref{sec:related-ocr}, we discuss previous applications of one-context regression in theoretical RL. Finally, in \cref{sec:related-hardness} we discuss prior works on computational hardness in RL.

\colt{
In the following remark, we highlight two conceptual distinctions that are important when attempting to study RL from a complexity-theoretic perspective.

\begin{remark}[Optimization vs. learning; proper vs. improper]\label{remark:oracle-subtleties-app}
  Many prior works in oracle-efficient RL assume access to \emph{optimization} oracles rather than statistical learning oracles. Informally, the former oracles require solving regression problems analogous to \cref{def:ocr-informal,def:tcr-informal} for \emph{arbitrary datasets} as opposed to i.i.d. and realizable datasets. This is primarily a conceptual distinction rather than technical, since in many cases a learning oracle can easily be substituted in \citep{misra2020kinematic,mhammedi2023representation}.
  However, it is important from a complexity-theoretic perspective, since statistical learning can often be substantially easier \citep{blum1998polynomial}. 
  
  A more technically salient distinction is that our definitions above allow for \emph{improper learning}, whereas almost all prior works in oracle-efficient RL for Block MDPs require \emph{proper} learning oracles\footnote{For example, the proper learning analogue of \Cref{def:ocr-informal} requires computing some predictor $\MR:\MX\to[0,1]$ with an explicit decomposition $\MR = \MR'\circ \phi$ for some $\phi \in \Phi$ and $\MR': \MS \to [0,1]$.}---an exception is the work of \cite{misra2020kinematic}, which our algorithmic results directly build on. There are many concept classes for which proper learning is $\NP$-hard, but it is considered unlikely for improper learning to be $\NP$-hard \citep{applebaum2008basing}. Since the goal of RL is to output policies, which are fundamentally improper, it seems unlikely that a proper supervised learning task could be reduced to RL (in the manner of results such as \cref{cor:regression-to-online-rl}).
\end{remark}
}
\subsection{Algorithms for Block MDPs}\label{sec:related-alg-block}


\begin{table}[t]
  % \colt{\small}
  \centering
  \resizebox{.98\linewidth}{!}{
\renewcommand{\arraystretch}{1.8}

\begin{NiceTabular}{ccc}[hvlines]
\thead{Oracle(s)} & \thead{Additional assumptions} & \thead{Reference} \\


\makecell{
\small{$\argmin\limits_{\hat f: \MX\times\MX \to [0,1]} \sum\limits_{(x_1,x_2,y) \in \MD} (\hat f(x_1,x_2) - y)^2$}
}
 & Reachability & 
\cite{misra2020kinematic} \\

\Block{2-1}{ 
\makecell{
\small{$\argmin\limits_{\hat \phi\in\Phi} \max\limits_{\substack{f: \MS \to [0,1] \\ \phi\in\Phi}} \min\limits_{\hat f: \MS\times\MA \to [0,1]}$} \\ \small{$ \sum\limits_{(x,a)\in\MD} (\hat f(\hat\phi(x),a) - \EE_{x'|x,a} f(\phi(x')))^2$}
}
}

& Reachability & \cite{modi2021model} \\
& --- & \cite{zhang2022efficient} \\


\makecell{
\small{$\argmax\limits_{\substack{\mu: \MS^2 \to \Delta(\MA \times\MS)\\ \hat \phi\in\Phi}} \sum\limits_{(j,a,x,x') \in \MD} \log \mu((a,j)|\hat \phi(x),\hat \phi(x'))$}
}& --- & 
\makecell{\cite{mhammedi2023representation}} 
\\\hhline{|=|=|=|}
\makecell{\small{
$\sup_{\hat f:\MX\to[0,1]} \left|\sum_{x'\in\MC} \left(\hat V(x') - f(x')\right)\right|$} \\ 
\small{ 
s.t. $\sum_{x'\in\MD} (\hat V(x') - \hat f(x'))^2 \leq \veps$ 
}}
& \textbf{Reset access} & \cite{mhammedi2024power}
\\
    \end{NiceTabular}    }
    \caption{This table, adapted from \cite[Table 1]{golowich2024exploration}, gives an informal overview of the oracles used in oracle-efficient RL for $\Phi$-decodable block MDPs; the methods above the double line apply in the episodic access model. \cite{misra2020kinematic} additionally require a cost-sensitive classification oracle for $\PSDP$, though this can be replaced by one-context regression. The oracle in the second row can be implemented by the \texttt{RepLearn} algorithm \citep{modi2021model,zhang2022efficient,mhammedi2023efficient}, but this algorithm still uses a max-min oracle. The oracle in the third row can be replaced by an analogous squared-loss minimization oracle \cite[Footnote 5]{mhammedi2023representation}, equivalent to \emph{proper} two-context regression. \cite{mhammedi2024power} additionally requires a one-context regression oracle.
    }
    \label{tab:oracle-overview}
  \end{table}

\paragraph{Episodic RL.} In \cref{tab:oracle-overview} we give an informal overview of the oracles used in oracle-efficient RL algorithms for Block MDPs, adapted from \cite{golowich2024exploration}. For simplicity, we omit discussion of earlier works that required stronger assumptions---beyond reachability, which we view as largely technical--such as deterministic dynamics \citep{dann2018oracle}, separability \citep{du2019provably}, or function approximation for the observation distributions \citep{agarwal2020flambe}.

In \cref{tab:oracle-overview}, all works listed above the double line apply in the episodic access model. For our purposes, the most relevant work is that of \cite{misra2020kinematic}; their algorithm $\HOMER$ requires only a two-context regression oracle and $\PSDP$, which can be implemented with one-context regression \citep{mhammedi2023representation}. Notably, $\HOMER$ is the only prior algorithm which uses an \emph{improper} oracle; the others all seem to heavily rely on properness \citep{modi2021model,zhang2022efficient,mhammedi2023representation}. The only catch is that \cite{misra2020kinematic} analyze $\HOMER$ under a \emph{reachability} assumption---i.e., the sample complexity scales inverse-polynomially with the minimum visitation probability of any state $\eta := \min_{s\in\MS} \min_{h\in[H]} \max_{\pi\in\Pi} d^{M,\pi}_h(s)$. Avoiding this dependence is the primary technical contribution of \cref{cor:online-rl-to-regression}.

The conventional goal in theoretical RL is reward-directed RL, i.e. find a policy that has approximately maximal value with respect to an external reward function. The alternative goal is reward-free exploration \citep{jin2020reward}. For Block MDPs, this is typically formulated as the task of computing an $(\alpha,\epsilon)$-policy cover, for given $\epsilon>0$ and any $\alpha =  1/\poly(H,|\MA|,|\MS|)$, where $\Psi \subset \Pi$ is an $(\alpha,\epsilon)$-policy cover if, for every layer $h \in [H]$ and latent state $s \in \MS$,
\[\max_{\pi\in\Psi} d^{M,\pi}_h(s) \geq \alpha \cdot \max_{\pi\in\Pi} d^{M,\pi}_h(s) - \epsilon.\]
It's straightforward to see that reward-directed RL in Block MDPs is no harder than reward-free RL, since given a policy cover, $\PSDP$ can compute a near-optimal policy for any external reward function, using only a one-context regression oracle. It's unknown whether reward-directed RL is computationally \emph{easier} than reward-free RL. However, we remark that reward-free RL is a common building block for reward-directed RL for Block MDPs \citep{du2019provably,misra2020kinematic,modi2021model,mhammedi2023representation} and beyond \citep{golowich2022learning,golowich2024exploring,mhammedi2023efficient}. Moreover, the only prior algorithms for Block MDPs that do not solve reward-free RL as a byproduct \citep{modi2021model,zhang2022efficient} require a seemingly \emph{more} complicated oracle.

The particular goal that we study in this paper is computing a $(1,\epsilon)$-policy cover (\cref{def:strong-rf-rl}), which was previously studied by \cite{du2019provably}. This only strengthens our algorithmic results (\cref{cor:online-rl-to-regression} and \cref{cor:reset-rl-to-regression}), but it does leave an open question regarding our lower bound (\cref{cor:regression-to-online-rl}): can it be strengthened to apply to the problem of computing an $(\alpha,\epsilon)$-policy cover, or the problem of reward-directed RL, or is there an inherent computational gap? See \cref{sec:discussion} for further discussion.

\paragraph{RL with resets.} The reset access model augments the episodic access model by allowing the learner to revisit any previously-seen state. This access model has been extensively applied to RL settings with linear value function approximation \citep{weisz2021query,li2021sample,yin2022efficient}, for the purpose of circumventing \emph{sample complexity} lower bounds. These settings do not subsume the Block MDP setting, since they assume linearity of the value function(s) in a known feature mapping $\phi$. More relevant to us is the work of \cite{mhammedi2024power}, who give an oracle-efficient algorithm $\RVFS$ for Block MDPs (more generally, MDPs with low \emph{pushforward-coverability}) in the reset access model. As shown in \cref{tab:oracle-overview}, $\RVFS$ uses a disagreement-type optimization oracle. While it is improper, and has the same solution concept as one-context regression (i.e. a real-valued function on $\MX$), there is no obvious reduction to one-context regression, particularly one that ensures realizability. Thus, unlike our result, it's not directly apparent whether it yields a provable computational separation between the reset access model and the episodic access model (though it's certainly possible that it could imply a separation with additional work).\loose

On the other hand, our algorithm $\PCR$ is limited to the Block MDP model class, whereas $\RVFS$ applies in significantly greater generality. Thus, the guarantees are formally incomparable.

\subsection{Algorithms for Low-Rank MDPs and Beyond}\label{sec:related-alg-low-rank}

Recently, \cite{mhammedi2023efficient} developed an oracle-efficient algorithm for episodic RL in Low-Rank MDPs, which generalize Block MDPs. However, this algorithm relies on the analogue of the min-max oracle in \cref{tab:oracle-overview}. The $\RVFS$ algorithm of \cite{mhammedi2024power} solves RL in Low-Rank MDPs with reset access, but as discussed above, seems to require an agnostic optimization oracle. Beyond Low-Rank MDPs, \cite{amortila2024scalable} proposed an algorithm for exploring MDPs with low \emph{pushforward coverability} (which is satisfied by Low-Rank MDPs, and hence Block MDPs as well) assuming access to a certain policy optimization oracle as well as an optimization oracle over weight functions. 

\subsection{Algorithms Based on One-Context Regression}\label{sec:related-ocr}

One-context regression is a natural oracle for estimating value functions and Bellman backups \citep{ernst2005tree,mhammedi2023representation,golowich2024exploration}, though until recently, most applications were phrased in terms of the worst-case optimization oracle analogue. Prior to our work, versions of one-context regression were known to be sufficient for offline RL in Block MDPs, under all-policy concentrability \citep{chen2019information}; for (online) RL in Block MDPs with deterministic dynamics \citep{efroni2021provable}, and for (online) RL in horizon-one Block MDPs; these results were systematized under \cref{def:one-con-regression} by \cite{golowich2024exploration}. A variant of one-context regression (on a ``composed'' function class) was shown to be sufficient for Block MDPs under a separability condition \citep{du2019provably}. Recently, it was shown that (in a setting broader than Block MDPs), one-context regression is sufficient for competing with a weaker notion of optimal policy, called a ``max-following policy'' for a given policy ensemble \citep{hussing2024oracle}.

\subsection{Computational Hardness in RL}\label{sec:related-hardness}

Most relevant to our lower bounds are recent results by \cite{golowich2024exploring,golowich2023planning}, who showed that one-context regression is necessary for RL in Block MDPs (for any concept class $\Phi$) as well as insufficient (for a particular choice of $\Phi$), the latter under a cryptographic assumption. Other hardness results include hardness of learning Partially Observable MDPs \citep{papadimitriou1987complexity,jin2020sample,golowich2023planning} and hardness of learning MDPs with linear $Q^\st$ and $V^\st$ \citep{kane2022computational,liu2023exponential}, all of which hold under versions of the Exponential Time Hypothesis.






%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
