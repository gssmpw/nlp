\section{Introduction}
Optimal Transport (OT) theory \citep{villani, ComputationalOT} addresses the problem of finding the cost-optimal transport map that transforms one probability distribution (\textit{source distribution}) into another (\textit{target distribution}). Recently, there has been growing interest in learning the optimal transport map directly using neural networks. OT has found extensive applications in various machine learning domains by appropriately defining source and target distributions, such as generative modeling \citep{otm, uotm, sjko, choi2024scalable}, image-to-image translation \citep{not, fanscalable}, point cloud completion \citep{uot-upc}, and domain adaptation \citep{da-ot}.
The OT framework is particularly advantageous for unpaired distribution transport tasks, as it relies solely on a predefined cost function to map one distribution to another, eliminating the need for paired data.

Among various approaches, the minimax algorithm, derived from the semi-dual formulation, has been widely investigated \cite{fanscalable, otm, uotm, uotmsd}.
Formally, \citet{fanscalable, otm} established the adversarial algorithm by leveraging the following max-min problem:
%\vspace{-5pt}
\begin{equation} 
\label{eq:problem}
\begin{aligned}
    &\sup_{V} \inf_{T:\mathcal{X} \rightarrow \mathcal{Y}}
    \mathcal{L}(V, T)
    \quad \hbox{where} \quad \mathcal{L}(V, T):= 
    \\
    & \int_{\mathcal{X}} c(x, T(x)) - V(T(x)) d\mu(x) + \int_{\mathcal{Y}} V(y) d\nu(y) .   
\end{aligned}    
   % \vspace{-5pt}
\end{equation}
Here, the probability measures $\mu$ and $\nu$ represent the source and the target distribution, respectively. 
The function $V:\mathcal{Y} \rightarrow \mathbb{R}$ and $T$ approximates a Kantorovich potential \cite{Kantorovich1948}, and an optimal transport map, respectively. Throughout this paper, we call these approaches the \textit{\textbf{Semi-dual Neural OT (SNOT)}}.

When the optimal potential $V^\star$ and the transport map $T^\star$ exist, it is well-known that 
\vspace{-5pt}
\begin{equation} \label{eq:optimal_t}
    % T^\star \in \arg\min_{T} \left[ c(x, T(x)) - v^\star(T(x)) \right].
    T^\star \in \arg\min_{T} \mathcal{L}(V^{\star}, T).
    \vspace{-5pt}
\end{equation}
as shown in \citet{otm, fanscalable}. Thus, the pair $(V^\star, T^\star)$ is the solution to this max-min problem. However, a critical challenge arises: not all solutions of Eq.\ref{eq:problem} correspond to the optimal potential and transport map pair. 
In other words, even the optimal solution in the SNOT framework may not recover the correct optimal transport map. We refer to this challenge as the \textit{\textbf{fake solution problem}}.

In this paper, we analyze this fundamental issue of the fake solution problem in existing SNOT frameworks. Specifically, we identify a sufficient condition on the source distribution $\mu$ that prevents the fake solution problem. The key condition is that the source distribution should not place positive mass on measurable sets with Hausdorff dimension $\leq d-1$ (see Thm~\ref{thm:uniqueness}). To the best of our knowledge, our work offers the first theoretical analysis of the sufficient condition under which the max-min solution of the SNOT framework can correctly learn the OT Map. Prior works were limited to the saddle point solution \citep{fanTMLR} or addressed a specific form of a different OT problem (weak OT) \citep{not, knot} (see Appendix \ref{sec:related_works} for the related works on fake solution issues). Additionally, we comprehensively explore various failure cases when this condition is not satisfied. 

Building on this condition, we develop a novel algorithm that ensures the learning of an optimal transport plan. We refer to our model as the \textit{\textbf{Optimal Transport Plan model (OTP)}}. Our method involves smoothing the source distribution $\mu_{\epsilon}$, so that the Neural OT models recover the correct optimal transport plan. Then, we gradually modify $\mu_{\epsilon}$ back to the original $\mu$ leveraging the convergence property. Our extensive experiments show that our OTP model accurately learns the optimal transport plan. Moreover, our model outperforms various (entropic) Neural OT models in diverse image-to-image translation tasks. Our contributions can be summarized as follows:
\begin{itemize}[topsep=-2pt, itemsep=-2pt]
    \item Our work is the first to identify a sufficient condition under which the max-min solution of existing SNOT recovers the true OT Map.
    \item We demonstrate diverse failure cases that occur when this sufficient condition is not satisfied.
    \item We propose a new algorithm that guarantees the learning of the optimal transport plan.
    \item Our experiments show that our model successfully recovers the correct OT Plan in failure cases where existing models fail.
\end{itemize}
\vspace{-5pt}
\paragraph{Notations and Assumptions}
Let $(\mathcal{X}, \mu)$ and $(\mathcal{Y}, \nu)$ be Polish spaces where $\mathcal{X}$ and $\mathcal{Y}$ are closures of connected open sets in $\mathbb{R}^d$.
We regard $\mu$ and $\nu$ as the source and target distributions.
Unless otherwise described, we consider $\mathcal{X} = \mathcal{Y} = \mathbb{R}^d$ and the quadratic transport cost $c:\mathcal{X}\times\mathcal{Y}\rightarrow \mathbb{R}$, $c(x,y)= \alpha \lVert x-y \rVert^2$ for a given positive constant $\alpha$.
For a measurable map $T$, $T_{\#}\mu$ represents the pushforward distribution of $\mu$.
$\Pi(\mu, \nu)$ denotes the set of joint probability distributions on $\mathcal{X}\times\mathcal{Y}$ whose marginals are $\mu$ and $\nu$, respectively.
Moreover, we denote $W_2(\cdot, \cdot)$ as the 2-Wasserstein distance of two distributions.
\vspace{-8pt}

\section{Background} \label{sec:background}
In this section, we present a brief overview of Optimal Transport theory \cite{villani, santambrogio}, and neural network approaches for learning optimal transport maps. In particular, we focus on approaches that leverage the semi-dual formulation \cite{otm, fanscalable}. 

\vspace{-5pt}
\paragraph{Optimal Transport}
The Optimal Transport (OT) problem investigates transport maps that connect the source distribution $\mu$ and the target distribution $\nu$ \citep{villani, santambrogio}. The \textit{optimal transport map (OT Map or Monge Map)} is defined as the minimizer of a given cost function among all transport maps between $\mu$ and $\nu$. Formally, \citet{monge1781memoire} introduced the OT problem with a deterministic transport map $T$ as follows:
\begin{equation}\label{eq:ot_monge} 
    \mathcal{T}(\mu, \nu) := \inf_{T_\# \mu = \nu}  \left[ \int_{\mathcal{X} } c(x,T(x)) d \mu (x) \right].
\end{equation}
Note that the condition $T_\# \mu = \nu$ indicates that the trasnport map $T$ transforms $\mu$ to $\nu$. However, the Monge OT problem is non-convex, and the existence of minimizer, i.e., the optimal transport map $T^{\star}$, is not always guaranteed depending on the assumption of $\mu$ and $\nu$ (Sec. \ref{sec:fail_no_det}). 

To address this existence issue, \citet{Kantorovich1948} proposed the following convex formulation of the OT problem:
\begin{equation} \label{eq:Kantorovich}
    C(\mu,\nu):=\inf_{\pi \in \Pi(\mu, \nu)} \left[ \int_{\mathcal{X}\times \mathcal{Y}} c(x,y) d\pi(x,y) \right],
\end{equation}
We refer to the joint probability distribution $\pi \in \Pi(\mu, \nu)$ as the \textit{transport plan} between $\mu$ and $\nu$. Unlike the Monge OT problem, the optimal transport plan (OT Plan) $\pi^{\star}$ is guaranteed to exist under mild assumptions on $(\mathcal{X}, \mu)$  and $(\mathcal{Y}, \nu)$ and the cost function $c$ \citep{villani}. Intuitively, while the Monge OT (Eq. \ref{eq:ot_monge}) covers only the deterministic transport map $y=T(x)$, the Kantorovich OT problem (Eq. \ref{eq:Kantorovich}) can represent stochastic transport via the conditional distribution $\pi (y | x)$ for each $x \sim \mu$. When the optimal transport map $T^{\star}$ exists, the optimal transport plan also reduces to this deterministic transport map, i.e., $\pi^{\star} = (Id \times T^{\star})_{\#} \mu$.

\paragraph{Semi-dual Neural OT}
The goal of neural optimal transport (Neural OT) models is to learn the OT Map between $\mu$ and $\nu$ using neural networks. 
The semi-dual formulation of the OT problem is widely leveraged for learning OT Maps \cite{otm, fanscalable, uotm, otmICNN}. 

The semi-dual formulation of the OT problem is given as follows: For a general cost function $c(\cdot, \cdot)$ that is lower semicontinuous and bounded below, the Kantorovich OT problem (Eq. \ref{eq:Kantorovich}) has the following \textit{semi-dual form} ((\citet{villani}, Thm. 5.10), (\citet{santambrogio}, Prop. 1.11)):
\begin{equation} \label{eq:kantorovich-semi-dual}
    % C(\mu, \nu) = 
     S(\mu,\nu):= \!\!\sup_{V\in S_c} \left[ \int_\mathcal{X} V^c(x)d\mu(x) \!+\!\! \int_\mathcal{Y} V(y) d\nu (y) \right],
\end{equation}
where $S_c$ denotes the collection of $c$-concave functions $\psi: \mathcal{Y}\rightarrow \mathbb{R}$ and $V^{c}$ denotes the $c$-transform of $V$, i.e., 
\vspace{-5pt}
\begin{equation} \label{eq:def_c_transform}
  V^c(x)=\underset{y\in \mathcal{Y}}{\inf}\left[ c(x,y) - V(y) \right].
  \vspace{-5pt}
\end{equation}
The SNOT approaches utilize this semi-dual form (Eq. \ref{eq:kantorovich-semi-dual}) for learning the OT Map $T^{\star}$ \citep{otm, fanscalable, otmICNN}. This formulation leads to a max-min optimization problem, similar to GANs \cite{gan}. Specifically, these models parametrize the transport map $T_{\theta} : \fX \rightarrow \fY$ and the potential $V_{\phi}$ as follows:
\vspace{-5pt}
\begin{align} 
    & T_{\theta}: x \mapsto \arg\min_{y \in \mathcal{Y}} \left[c(x, y) - V_{\phi}\left( y \right)\right] \label{eq:def_T} \\
    & \quad \Leftrightarrow \quad V_{\phi}^c(x)=c\left(x,T_{\theta}(x) \right) - V_{\phi}\left(T_{\theta}(x)\right). \label{eq:c_transform_with_T}
    \vspace{-5pt}
\end{align}
Note that $T_{\theta}$-parametrization (Eq. \ref{eq:def_T}) implies that the $c$-transform $V_{\phi}^{c}$ can be expressed with the transport map $T_{\theta}$ and the potential $V_{\phi}$, as shown in Eq. \ref{eq:c_transform_with_T}. From this, the SNOT models derive the following optimization problem $\mathcal{L}_{V_{\phi}, T_{\theta}}$:
\begin{equation}  \label{eq:otm}
    \begin{aligned}
        &\sup_{V_{\phi} \in S_c} \inf_{T_{\theta}:\mathcal{X} \rightarrow \mathcal{Y}} 
        \mathcal{L}(V_{\phi}, T_{\theta}) \quad \text{where} \quad \mathcal{L}(V, T) := \\
        & \int_{\mathcal{X}} c\left(x,T(x)\right)-V \left( T(x) \right) d\mu(x) + \int_{\mathcal{Y}} V(y)  d\nu(y).
    \end{aligned}    
\end{equation}
Intuitively, $T_\theta$ and $V_\phi$ serve similar roles to the generator and the discriminator in GANs. However, the OT Map $T_\theta$ is additionally trained to minimize the transport cost $c\left(x, T_{\theta}(x)\right)$, while GANs focus solely on learning the target distribution $T_{\#}\mu = \nu$ \citep{wgan, wgan-gp}. 


\section{Analytical results for Semi-dual Neural OT} 
\label{sec:analyze}


A critical limitation of existing SNOT approaches is that the max-min solution $(V^{\dagger}, T^{\dagger})$ of Eq. \ref{eq:otm} may include not only the desired OT Map but also other \textbf{fake solutions} \citep{otm}. Formally, if OT Map $T^\star$ exists, the optimal potential $V^\star$ and OT Map $T^\star$ become a max-min solution (Eq. \ref{eq:optimal_t}). 
However, not all max-min solutions correspond to the true optimal potential and transport map, i.e., $\{(V^{\star}, T^{\star})\} \subsetneq \{(V^{\dagger}, T^{\dagger})\} $. In particular, even $T^{\dagger} \# \mu = \nu$ does not hold in general (see Fig. \ref{fig:fail_case}), which means that $T^{\dagger}$ is not a valid transport map from $\mu$ to $\nu$ as in (Eq. \ref{eq:ot_monge}).

We first investigate sufficient conditions to prevent fake solution issues (Sec. \ref{sec:unique_saddle}), and present a comprehensive failure case analysis of the SNOT approach (Sec.~\ref{sec:failure}). Based on this, later in Sec. \ref{sec:method}, we propose a method for learning an accurate Neural OT model that avoids such fake solutions.

\subsection{Sufficient Conditions for Ensuring Convergence of Semi-dual Neural OT}
\label{sec:unique_saddle}
We provide sufficient conditions on the source distribution $\mu$ and the target distribution $\nu$ to ensure a unique minimizer for the $T_{\theta}$-parametrization (Eq. \ref{eq:def_T}). This enables the SNOT objective to accurately recover the optimal transport plan. %(Thm \ref{thm:uniqueness}). 

\begin{theorem} \label{thm:uniqueness}
    Let $\mu \in \mathcal{P}_2(\mathcal{X}), \nu \in \mathcal{P}_2(\mathcal{Y})$, and $c(x,y)=\frac{1}{2}\Vert x-y \Vert^2$.
    Assume that $\mu$ does not give mass to the measurable sets of Hausdorff dimension at most $d-1$ dimension. 
    \begin{enumerate}[leftmargin=*, topsep=0pt, itemsep=-2pt]
        \item[(1)] 
        Then, there exists a unique OT Map $T^\star$ in (Eq. \ref{eq:ot_monge}) and the Kantorovich potential $V^\star\in S_c$ in (Eq. \ref{eq:kantorovich-semi-dual}). 
        \item[(2)]
        For the Kantorovich potential $V^\star \in S_c$, the minimization problem,
        \vspace{-10pt}
        \begin{equation} \label{eq:argmin}
            \mathcal{D}_x := \arg\min_{y\in \mathcal{Y}} \left[ c(x,y) - V^\star(y) \right],
            \vspace{-5pt}
        \end{equation}
        is uniquely determined $\mu$-a.s., i.e. $\mathcal{D}_x = \{ y_x\}$ for $\mu$-a.s $x \in \mathcal{X}$. In particular, a map $x\mapsto y_x \in \mathcal{D}_x$ is a unique OT Map $T^\star$ in law.
    \end{enumerate}
\end{theorem}
\vspace{-5pt}
Here, $\mathcal{D}_x$ corresponds to the $T_{\theta}$-parametrization in the SNOT framework. Therefore, the uniqueness of $\mathcal{D}_x$ for $V^{\star}$ implies that $T_{\theta}$-parametrization is fully characterized. \textbf{Thm. \ref{thm:uniqueness} shows that the assumption on $\mu$, not on $\nu$, is enough to eliminate the ambiguity in mapping each $x$ to $T_{\theta}(x)$ and this mapping corresponds to the OT Map.} %In Sec. \ref{sec:fail_det} and Sec. \ref{sec:fail_no_det}, 
In Sec.~\ref{sec:failure}, we show that this ambiguity results in failure cases of SNOT.


Note that Thm. \ref{thm:uniqueness} is sufficient for addressing the fake solution problem. For the sake of completeness, we also present a sufficient condition where the SNOT framework admits \textbf{a unique max-min solution that corresponds to the correct OT Map} (Cor. \ref{cor:unique_saddle}). In this case, the additional assumptions on $\nu$ is also required. Here, we used the fact that the absolutely continuous measures with respect to the Lesbesgue measure satisfy the condition in Thm. \ref{thm:uniqueness}. 

\begin{theorem} \label{thm:unique_potential}
    Suppose $\mathcal{Y} \subset \mathbb{R}^d$ is a closure of a bounded open set. If $\nu$ has a positive density almost everywhere with respect to the Lebesgue measure on $\mathcal{Y}$, then there exists unique Kantorovich potential $V^\star \in S_c$ up to constant.
\end{theorem}

Cor. \ref{cor:unique_saddle} is derived by combining Thm. \ref{thm:uniqueness} with the uniqueness of the optimal potential $V^{\star}$ in Thm. \ref{thm:unique_potential}.

% \vspace{-5pt}
\begin{corollary} \label{cor:unique_saddle}
     Suppose $\mathcal{Y} \subset \mathbb{R}^d$ is a closure of a bounded open set. 
    Suppose $\mu \in \mathcal{P}_2(\mathcal{X})$ and $\nu\in \mathcal{P}_2 (\mathcal{Y})$ are absolutely continuous distributions that have positive density functions on their domain. Then, the solution $(V^\star, T^\star)$ of \eqref{eq:otm} is unique. In other words, $V^\star\in S_c$ is unique up to constant, and $T^\star$ is a deterministic OT Map.
\end{corollary}
\vspace{-5pt}

\begin{figure*}
    \centering
    \subfigure[Perpendicular]{
    \includegraphics[width=0.18\linewidth]{images/toy/vertical_combined.png}
    \label{fig:perp}
    }
    \quad
    \subfigure[Parallel]{
    \includegraphics[width=0.18\linewidth]{images/toy/horizon_combined.png}
    \label{fig:hor}
    }
    \quad
    \subfigure[One-to-Many]{
    \includegraphics[width=0.18\linewidth]{images/toy/one_to_many_combined.png}
    \label{fig:one-to-many}
    }
    \quad
    \subfigure[Grid]{
    \includegraphics[width=0.18\linewidth]{images/toy/multi_vertical_combined.png}
    \label{fig:multi_verti}
    }
    \subfigure{
    \includegraphics[width=0.1 \linewidth]{images/toy/toy_fault_legend.png}
    } \vspace{-10pt}
    \caption{\textbf{Visualization of failure cases} by comparing the Optimal Transport map (\textbf{1st row}) and the max-min solution (\textbf{2nd row}) of Semi-dual Neural OT in the failure cases. The source data $x \sim \mu$, target data $y \sim \nu$, and generated data $T(x)$ are represented in Blue, Orange, and Red. The max-min solution fails to recover the correct OT Map.}
    % Failure cases and visualization of optimal transport map. Source data (Blue), target data (Orange). \red{discrete vs. GT visualization} 
    \label{fig:fail_case}
    \vspace{-10pt}
\end{figure*}
        

\subsection{Failure Cases When Our Condition Is Not Met}
\label{sec:failure}

In Thm~\ref{thm:uniqueness}, it is crucial to assume that $\mu$ does not give mass to the measurable sets of Hausdorff dimension at most $d-1$ dimension. Without this assumption, SNOT may fail even when the deterministic OT Map $T^{\star}$ uniquely exists.  Specifically, the failure cases discussed in this section refer to scenarios where $(V^{\dagger}, T^{\dagger})$ is a max-min solution of Eq. \ref{eq:problem} but does not correspond to the OT Map $T^{\star}$ (Eq. \ref{eq:ot_monge}), i.e., $(Id, T^{\dagger})_{\#} \mu$ fails to represent the OT Plan $\pi^{\star}$ (Eq. \ref{eq:Kantorovich}). 

\subsubsection{Discrepancy between a Max-min Solution and the Deterministic OT Map} \label{sec:fail_det}
We first focus on the Monge OT problem (Eq. \ref{eq:ot_monge}), where the deterministic OT Map $T^{\star}$ exists. Specifically, we investigate source and target distribution pairs where $T^{\star}$ exists, but the max-min solution $T^{\dagger}$ of the SNOT objective (Eq. \ref{eq:otm}) fails to recover this optimal solution. 
Here, we provide two examples, depending on the uniqueness of $T^{\star}$.
\paragraph{Example 1. [When $T^{\star}$ exists but is not unique]}
First, we introduce a case where multiple optimal solutions $T^{\star}$ exist for the Monge OT problem. Assume that the source and target distributions are uniformly supported on $A = [-1,1]\times \{0\}$ and $B = \{0 \} \times [-1,1]$, respectively (Fig. \ref{fig:perp}).
In this case, any transport map $T$ satisfying $T_\# \mu = \nu$ becomes an optimal transport map for the quadratic cost function. Formally, note that for any transport map $T$, the following holds:
% Note that for any coupling $\pi \in \Pi (\mu, \nu)$, the following holds:
\vspace{-8pt}
\begin{multline} \label{eq:failure1_t}
    \int_{\mathcal{X} } c(x, T(x)) d\mu(x) 
    = \frac{1}{2} \int^1_{-1} x_{1}^2 d \mu (x) \\
    - \int_{\mathcal{X}} \cancel{\langle x, T(x)} \rangle \ d\mu(x)
    + \frac{1}{2} \int^1_{-1} y_{1}^2 d \nu (y) = \frac{2}{3}.
\end{multline}
for $x=(x_1, x_2)$ and $y=(y_1, y_2)$. The first equality follows from $T_\# \mu = \nu$ and $\langle x, T(x) \rangle = 0 $ for all $x$ because $A \perp B$.
Since every transport map achieves the same transport cost, any transport map becomes an optimal transport map $T^{\star}$. 

Then, we prove that $T^{\dagger}$ does not correspond to $T^{\star}$. Specifically, we show that $V^{\star}(y) := \frac{1}{2} \lVert y \rVert^2 \in S_{c}$ is the Kantorovich potential (Eq. \ref{eq:kantorovich-semi-dual}) and that $T^{\dagger}$ is not guaranteed to generate the target distribution. By substituting $V_\phi$ into $V$, the inner problem of SNOT (Eq. \ref{eq:otm}) can be expressed as follows:
\vspace{-5pt}
\begin{multline} \label{eq:failure1_inner}
    \!\!\!\inf_{T} \!\!\int_{\mathcal{X}} \frac{1}{2} \lVert x\rVert^2 - \cancel{\langle x, T(x)\rangle} d\mu(x)  + \!\int_{\mathcal{Y}}\! \frac{1}{2} \lVert y \rVert^2 d\nu(y) \!= \!\frac{2}{3}.
\end{multline} 
Since $V^{\star}$ attains the same value of $\mathcal{L}_{V_{\phi}, T_{\theta}}$ as $T^{\star}$ in Eq. \ref{eq:failure1_t}, $V^{\star}$ is the optimal potential. Furthermore, by comparing Eq. \ref{eq:failure1_inner} with $T_{\theta}$-parametrization (Eq. \ref{eq:def_T}), we can easily observe that any measurable map $T_{\theta}:A\rightarrow B$ can be a max-min solution of SNOT. In other words, there is no constraint ensuring that $T_{\theta \#} \mu = \nu$. For example, $T_{\theta}(x) = (0,0)$ for $\forall x \in \fX$ is also a valid max-min solution. This means that the existing SNOT models cannot learn the optimal transport map between these two distributions (Fig. \ref{fig:perp_model}).


\paragraph{Example 2. [When unique $T^{\star}$ exists]}
Here, we present another failure case when there is a unique optimal transport map $T^{\star}$. Assume that the source and target distributions are uniformly distributed over $A = [-1,1]\times \{0 \}$ and $B=[-1,1]\times \{1\}$, respectively (Fig. \ref{fig:hor}). In this setup, the unique $T^{\star}$ is given by:
\begin{equation}
    T^{\star}(x) := (x_1,1) \quad \text{ for } x=(x_1, 0) \in \fX.
\end{equation}
Thus, $\mathcal{T}(\mu, \nu) = \frac{1}{2}$. Similar to Example 1, we show that $V^{\star}(y) = \frac{1}{2} \lVert y_2 \rVert^2 \in S_{c}$ is the optimal Kantorovich potential and analyze the max-min solution of Eq. \ref{eq:otm}. For this $V^{\star}$, the inner problem of SNOT can be computed as follows:
\begin{equation}  \label{eq:failure2_inner}
    \inf_{T}\! \int_{\mathcal{X}} \frac{1}{2} \lVert x_1 - T(x)_1 \rVert^2 d\mu(x) + \!\!\int_{\mathcal{Y}} \frac{1}{2} \lVert y \rVert^2 d\nu(y) \!=\! \frac{1}{2}.
    \vspace{-5pt}
\end{equation} 

Because Eq. \ref{eq:failure2_inner} achieves the same value as $\mathcal{T}(\mu, \nu)$, $V^{\star}$ is the optimal potential. For this $V^{\star}$, any transport map $T((x_1, x_2)):= (x_1, a)$ for any $a \in \mathbb{R}$ for each $(x_1, x_2) \in \fX$ becomes a max-min solution of the SNOT. In this case, the existing approach fails to even characterize the correct support of the target distribution $\nu$.

\subsubsection{Discrepancy between a Max-min Solution and the Stochastic OT Map}
\label{sec:fail_no_det}

The standard SNOT parametrizes the transport map with a deterministic function $T_{\theta}$ (Eq. \ref{eq:def_T}). 
When no deterministic OT Map $T^{\star}$ exists but only an  OT Plan $\pi^{\star}$ exists (Eq. \ref{eq:Kantorovich}), it is clear that the SNOT cannot accurately represent the stochastic OT Map (OT Plan).


\paragraph{Example 3. [When only $\pi^{\star}$ exists]} 

Suppose the source and target distributions are uniform on $A = [0,1]\times \{ 0 \}$ and $B = [0,1]\times \{1\} \cup [0,1] \times \{-1 \}$, respectively (Fig. \ref{fig:one-to-many}). 
In this case, it is clear that the OT Plan $\pi^{\star}$ is given as follows:
\begin{equation} \label{eq:example1_eq1}
    \pi^{\star}(y|x) = \frac{1}{2} \delta_{(x_1, 1)} + \frac{1}{2} \delta_{(x_1, -1)} \text{ where } x\!=\!\!(x_1, x_2).
\end{equation}
The OT Plan $\pi^{\star}(y|x)$ moves each $x$ vertically either up or down with probability $\frac{1}{2}$, without incurring additional cost from horizontal movement. Then, we show that $V^\star(y) = \frac{1}{2} \Vert y_2 \Vert^2 \in S_{c}$ with $y = (y_1, y_2)$ is the optimal potential. The $(V^\star)^c$ and $V^{\star}$ can be computed for $\mu$ and $\nu$ as follows:
\vspace{-3pt}
\begin{align} 
    \!\!(V^{\star})^c(x)\! = \!\!\inf_{y \in \fY}\!\! \left( c(x,y) \!- \!V^{\star}\!(y) \right)\! = \!\inf_{y_1} \! \frac{1}{2} \Vert x_1 \!\!-\! y_1 \!\Vert^2 \!\!=\!0,\! \label{eq:example1_eq2} 
\end{align}
$V^{\star}(y) = \frac{1}{2} \Vert y_2\Vert^2 = \frac{1}{2} \text{ for } \forall y \in \fY.$
By comparing the $C$ for the optimal transport plan (Eq. \ref{eq:example1_eq1}) and the semi-dual form $S$ for $V^\star$, we can easily verify that $V^\star$ is the optimal Kantorovich potential.

Then, from Eq. \ref{eq:example1_eq2}, we can see that $T_1$ and $T_2$ are the two possible solutions for the $T$-parametrization (Eq. \ref{eq:def_T}) in the SNOT for $V^{\star}$, 
%\begin{equation}
    $T_1(x):=(x_1,1)$ and $T_2(x):=(x_1,-1).$
%\end{equation}
for $x=(x_1, x_2) \in \fX$. These two candidates $T_{1}, T_{2}$ only characterize a subset of the support of $\pi^{\star} (y|x)$. Therefore, our deterministic $T_{\theta}$ cannot learn the stochastic $\pi^{\star} (y|x)$. 

\paragraph{Stochastic Parametrization of OT Map} In practice, a stochastic parametrization of $T_{\theta}(x, z)$ is often adopted to improve performance in the SNOT models \citep{not, uotm}. This stochastic parametrization $T_{\theta}(x, z)$ introduces an additional noise variable $z \sim N(0, I)$: 
\vspace{-10pt}
\begin{equation} \label{eq:stochastic_generator} 
    T_{\theta}(x, z) \in \arg\min_{y\in \mathcal{Y}} \{ c(x,y) - V^\star (y) \}, \\[-5pt]
\end{equation}
$(x,z)\sim \mu \times \mathcal{N}(0,I)$ a.s.. As a result, each $x$ is transported to multiple $T(x, z)$ values depending on $z$. \textbf{We point out that even a stochastic parametrization, such as $T_{\theta}(x, z)$ with a noise variable $z \sim N(0, I)$, cannot address this limitation.} For the formal statement, see Appendix \ref{appen:non_conv_stocas_param}.


\begin{proposition}[Informal]
\label{prop:stoc}
    Assume that the stochastic parametrization of $T_{\theta}(x, z)$ is ideally trained as in \eqref{eq:stochastic_generator} for $(\mu, \mathcal{N})$-a.s. $\mathcal{D}_x$ in Eq. \ref{eq:argmin} may not uniquely determined %(as in Thm \ref{thm:uniqueness}), 
    and $T_{\theta}(x, z)$ may contain fake solutions.
\end{proposition}

    


    
\section{Method} \label{sec:method}
In Sec. \ref{sec:failure}, we analyzed the sufficient condition to prevent failures in the existing SNOT framework. Building on this analysis, we propose a novel method for learning the OT Plan, called the \textit{Optimal Transport Plan (\textbf{OTP}) model}, which is effective even when the conditions are not satisfied.

\begin{algorithm}[t]
\caption{Training algorithm of OTP}
\begin{algorithmic}[1]
\REQUIRE Source distribution $\mu$ and the target distribution $\nu$; OT Map network $T_\theta$ and potential network $V_\phi$; Total number of iteration $K$; Number of inner-loop iterations $K_T$; Decreasing sequence of noise levels $\{\epsilon_k \}^K_{k=1}$.
\FOR{$k = 0, 1, 2 , \dots, K$}
    \STATE Sample a batch $x\sim \mu$, $y\sim \nu$, $z \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$.
    \STATE $\Tilde{x} \leftarrow x + \sqrt{\epsilon_k} z $ or $\Tilde{x} \leftarrow \sqrt{1-\epsilon_k} x + \sqrt{\epsilon_k} z $.
    \STATE Update $\phi$ to maximize $\mathcal{L}_{\phi} = - V_\phi \left(T_\theta(\tilde x)\right) + V_\phi(y)$.
    \FOR{$j= 0, 1, \dots, K_T$}
    \STATE Sample a batch $x\sim \mu, z\sim \mathcal{N}(\mathbf{0}, \mathbf{I})$.
    \STATE $\Tilde{x} \leftarrow x + \sqrt{\epsilon_k} z $ or $\Tilde{x} \leftarrow \sqrt{1-\epsilon_k} x + \sqrt{\epsilon_k} z $.
    \STATE $\mathcal{L}_{\theta} = c(\Tilde{x}, T_\theta(\tilde x)) - V_\phi \left(T_\theta(\tilde x)\right) + V_\phi(y)$.
    \STATE Update $\theta$ to minimize $\mathcal{L}_{\theta}$.
    \ENDFOR
\ENDFOR
\end{algorithmic}
\label{alg:otp}
\end{algorithm}

\subsection{Proposed Method} \label{sec:proposed_method}
\textbf{Our goal is to learn the OT Plan $\pi^{\star}$ (Eq. \ref{eq:Kantorovich}) between the source distribution $\mu$ and the target distribution $\nu$.} Note that the sufficient condition in Thm. \ref{thm:uniqueness} is an inherent property of $\mu$. When this condition is not satisfied, the existence of OT Map $T^{\star}$ is not guaranteed, and only $\pi^{\star}$ exists. In this regard, our OTP model serves as a natural generalization of existing SNOT models.

Our method consists of two steps: First, we introduce a \textit{smoothed version of the source distribution} $\mu_{\epsilon}$. $\mu_{\epsilon}$ is constructed to satisfy the sufficient conditions from Thm. \ref{thm:uniqueness}. As a result, the SNOT between $\mu_{\epsilon}$ and $\nu$ recovers the correct OT Plan $\pi^{\star}_{\epsilon}$ between them. Second, we gradually adjust $\mu_{\epsilon}$ back to the original source measure $\mu$. This approach allows our method to learn the correct optimal transport plan, even in cases where the existing SNOT framework fails.


\paragraph{OTP Model}
As a practical implementation of the high-level scheme described above, we propose a new method for learning the OT Plan $\pi^{\star}$ from $\mu$ to $\nu$, called \textit{Optimal Transport Plan (OTP)} model. This method is based on Thm. \ref{thm:uniqueness} and Thm. \ref{thm:convergence}, which require the following two conditions on the smoothed measure $\mu_{\epsilon}$:
\begin{enumerate}[topsep=0pt, itemsep=-1pt]
    \item[(c1)] 
    $\mu_{\epsilon}$ does not give mass to the measurable sets of Hausdorff dimension at most $d-1$ dimension
    (Thm. \ref{thm:uniqueness}).
    \item[(c2)] $\mu_{\epsilon_{k}}$ weakly converges to $\mu$ as $k \rightarrow \infty$ (Thm. \ref{thm:convergence}).
\end{enumerate}
For simplicity, we consider the absolute continuity condition on $\mu_{\epsilon}$ as we did in Cor. \ref{cor:unique_saddle}. Motivated by diffusion models \citep{ddpm, scoresde}, we consider \textbf{two options for the smoothing distribution}: (1) Gaussian convolution $\mu_{\epsilon_{k}} = \mu \, * \,  \mathcal{N}(0, \epsilon_{k} I)$ and (2) Variance-preserving convolution $\mu_{\epsilon_{k}} = \left(\sqrt{1-\epsilon_{k}}Id\right)_{\#}\mu \, * \,  \mathcal{N}(0, \epsilon_{k} I)$ with a predefined noise level $\epsilon_{k} \searrow 0$. For noise-level scheduling, we follow \citet{scoresde}. Note that both of these smoothing distributions satisfy conditions (c1) and (c2). Specifically, for Gaussian convolution, for any $\mu \in \mathcal{P}_2(\mathbb{R}^d)$, (c1) $\mu_\epsilon$ is absolutely continuous with respect to the Lebesgue measure and has positive density on $\mathbb{R}^d$. Moreover, (c2) as $\epsilon \rightarrow 0$, $\mu_{\epsilon}  \rightharpoonup \mu$. A similar argument works for the Variance-preserving convolution case. Then, we apply the SNOT framework to the smoothed measure $\mu_{\epsilon_{k}}$ and the target measure $\nu$. The learning objective is given as follows:
\vspace{-15pt}
\begin{multline} \label{eq:saddle_epsilon}
    \!\!\mathcal{L}^{k}_{V_{\phi}, T_{\theta}}\! = \!\sup_{V_{\phi}} \left[ \int_{\mathcal{X}} \!\inf_{T_{\theta}} \left[ c\left(x,T_{\theta}(x)\right)\!-\!V_{\phi} \left( T_{\theta}(x) \right) \right] d\mu_{\epsilon_{k}}\!(x) \right. \\[-8pt]
    \left. + \int_{\mathcal{X}} V_{\phi}(y)  d\nu(y) \right].
\end{multline}

Then, we gradually decrease the noise level $\{ \epsilon_{k} \}_{k=1}^{K}$ throughout training. The two conditions on $\mu_{\epsilon_{k}}$, i.e., (c1) and (c2), offer the following guarantees. First, for each noise level $\epsilon_{k}$, $\mathcal{L}^{k}_{V_{\phi}, T_{\theta}}$ has a unique saddle point solution, corresponding to the optimal transport map $T_{k}^{\star}$ and the Kantorovich potential $V_{k}^{\star}$. Second, as $k \rightarrow \infty$, i.e., $\epsilon_{k} \searrow 0$, the optimal transport plan $\pi^{\star}_{k} = (Id, T_k^{\star})_{\#} \mu_{\epsilon_{k}}$ converges (up to a subsequence) to $\pi^{\star}$. Thm. \ref{thm:convergence} follows from combining Thm. \ref{thm:uniqueness} and \citet{villani}. See Appendix \ref{appen:conv_result_from_oldandnew} for proof.

\begin{theorem} \label{thm:convergence}
    Let $\{\mu_{\epsilon_k}\}_{k\in \mathbb{N}}$ be a sequence absolutely continuous probability measures, and $T_{k}^{\star}$ be the OT map from $\mu_{\epsilon_k}$ to $\mu$.
    %\begin{enumerate}
    %\item[(1)] 
     If $\mu_{\epsilon_k}$ weakly converges to $\mu$ as $k \to \infty$, then $\pi^{\star}_{k} = (Id, T_k^{\star})_{\#} \mu_{\epsilon_{k}}$ weakly converges to the OT plan $\pi^{\star}$ between $\mu$ and $\nu$, along a subsequence. Consequently, $\pi^{\star}_{k}$ from our OTP model with either convolution above also weakly converges to $\pi^{\star}$, along a subsequence.
\end{theorem}

In this way, we can learn the optimal transport plan $\pi^{\star}$ between $\mu$ and $\nu$ without falling into the fake solutions of the max-min learning objective (Eq. \ref{eq:otm}). While the convergence theorem only guarantees convergence up to a subsequence (Thm. \ref{thm:convergence}), our method exhibits decent convergence to $\pi^{\star}$ in practice (Sec. \ref{sec:experiment}). Specifically, our training algorithm progressively finetunes the transport network $T_{\theta}$ and the potential network $V_{\phi}$ by adjusting the smoothing level. As a result, the subsequence convergence does not pose any issues.

\begin{figure}[t]
    \centering
    \includegraphics[width=.75\linewidth]{images/colorization_concept2.png}
    \caption{
    \textbf{Example of a stochastic transport map (OT Plan) task}, e.g., MNIST-to-CMNIST colorization.}
    \vspace{-10pt}
    \label{fig:concept_m2cm}
\end{figure}

\paragraph{Importance of OT Plan in Neural OT}
Our OTP model is for learning the OT Plan, i.e., the stochastic transport map. In fact, OT Plans are not only a theoretical generalization of deterministic OT Maps, but are also inherently more suitable for various real-world machine learning applications. For instance, in image-to-image translation tasks, stochastic OT Plans can effectively model the diversity of plausible outputs. Similarly, in inverse problems such as colorization or image inpainting, stochastic OT Plans are also highly desirable because these tasks inherently involve multiple possible solutions. In Sec \ref{sec:experiment}, our experiments show that our OTP model is effective in handling the stochastic transport map application in the MNIST-to-CMNIST image translation task (Fig. \ref{fig:concept_m2cm}).

\begin{figure*}[t]
    \centering 
    \subfigure[Perpendicular]{
    \includegraphics[width=0.18\linewidth]{images/toy/vertical_result_combined.png}
    \label{fig:perp_model} 
    }
    \quad
    \subfigure[Parallel]{
    \includegraphics[width=0.18\linewidth]{images/toy/horizontal_result_combined.png}
    \label{fig:hor_model}
    }
    \quad
    \subfigure[One-to-Many]{
    \includegraphics[width=0.18\linewidth]{images/toy/one_to_many_result_combined.png}
    \label{fig:one-to-many_model}
    }
    \quad
    \subfigure[Grid]{
    \includegraphics[width=0.18\linewidth]{images/toy/multi_vertical_result_combined.png}
    \label{fig:multi_verti_model} \vspace{-10pt}
    }
    \vspace{-10pt}
    \subfigure{
    \includegraphics[width=0.1\linewidth]{images/toy/toy_result_legend.png}
    }  %\\
    \caption{\textbf{Qualitative comparison between OTM (1st row) and our model (2nd row) on failure cases} in Sec \ref{sec:failure}. The noised source sample $\Tilde{x}$ in Alg \ref{alg:otp} is denoted in Green. While OTM falls into fake solutions and fails to generate the target distribution correctly, our OTP model successfully learns the OT Plan. 
    % Quantitative results of OTM vs. ours 
    }
    \label{fig:fail_case_model}
    \vspace{-10pt}
\end{figure*}

\paragraph{Algorithm}
We present our training algorithm for OTP (Algorithm \ref{alg:otp}). For each $\epsilon_{k}$, we alternatively update the adversarial learning objective $\mathcal{L}^{k}_{V_{\phi}, T_{\theta}}$ between the potential function $V_{\phi}$ and the transport map $T_{\theta}$, similar to the GAN framework \citep{gan}. Note that the smooth source measure $\mu_{\epsilon_{k}}$ corresponds to the probability distribution of the sum of the clean source measure $\mu$ (or the scaled source measure $\left(\sqrt{1-\epsilon_{k}}Id\right)_{\#}\mu$) and the Gaussian noise $\mathcal{N}(0, \epsilon_{k} I)$. Therefore, we can easily sample $x_{\epsilon_{k}} \sim \mu_{\epsilon}$, as follows (Line 3):
\vspace{-5pt}
\begin{equation}
    x_{\epsilon_{k}} = x + \sqrt{\epsilon_{k}} z \quad \text{or} \quad
    x_{\epsilon_{k}} =\sqrt{1-\epsilon_{k}}x + \sqrt{\epsilon_{k}},
    \vspace{-5pt}
\end{equation}
where $x \sim \mu$ and $z \sim \mathcal{N}(0, I)$. In practice, decreasing the noise level until a small positive constant $\epsilon_{min} > 0$ provided better performance and training stability, compared to reducing the noise level to exactly zero. For a fair comparison, we compared the composition of the noising and transport map $x \mapsto x_{\epsilon_{min}} \mapsto T_{\theta}(x_{\epsilon_{min}})$, with the ground-truth optimal transport map $x \mapsto T^{\star}(x)$ in the experiments (Sec. \ref{sec:experiment}).





\section{Experiments} \label{sec:experiment}
In this section, we evaluate our OTP model from the following perspectives. In Sec.~\ref{sec:exp_syn}, we evaluate whether OTP successfully learns the optimal transport plan. In Sec.~\ref{sec:exp_image}, we demonstrate the scalability of OTP by assessing it on the image-to-image translation task.
For implementation details of experiments, please refer to Appendix \ref{appen:implementation_details}.

\subsection{OT Plan Evaluation on Failure Cases} \label{sec:exp_syn}
First, \textbf{we assess whether our model accurately learns the optimal transport plan $\pi^{\star}$ between the source distribution $\mu$ and the target distribution $\nu$ in failure cases outlined in Sec \ref{sec:failure}.} The evaluation is conducted in two settings: (1) Qualitative comparison in 2D cases and (2) Quantitative comparison in high-dimensional cases.
In each setting, our OTP model is compared against the existing SNOT framework (Eq. \ref{eq:otm}).


\begin{table}[t]
    \vspace{-10pt}
    \centering
    \caption{\textbf{Quantitative comparison of numerical accuracy} on synthetic datasets. Each model is evaluated using two metrics: transport cost error $D_{cost} (\downarrow)$ and target distribution error $D_{target} (\downarrow)$. 
    }
    \label{tab:fail_case_quan}
    \scalebox{0.75}{
    \begin{tabular}{c c c c c c}
        \toprule
        \multirow{2}{*}{Dimension} & \multirow{2}{*}{Model} & \multicolumn{2}{c}{Perpendicular} & \multicolumn{2}{c}{One-to-Many}  \\
        \cmidrule{3-4} \cmidrule{5-6}
        & & $D_{cost}$ & $D_{target}$ & $D_{cost}$ & $D_{target}$ \\
        \midrule
        \multirow{3}{*}{$d=2$}  & OTM & 0.038 & 0.0079 &  0.069 & 0.10  \\
                                & OTM-s &   \textbf{0.0070}    &  0.018  & 0.35 & \textbf{0.032} \\
                                & Ours & 0.019 & \textbf{0.0068} & \textbf{0.0022} & 0.11  \\
        \midrule
        \multirow{3}{*}{$d=4$}  & OTM & 0.043 & 0.039 & 0.10 & 73.23  \\
                                & OTM-s & \textbf{0.033}  & 0.065 & \textbf{0.010} & \textbf{0.038} \\
                                & Ours & 0.089 & \textbf{0.0086} & 0.033 & 0.094  \\
        \midrule
        \multirow{3}{*}{$d=16$}  & OTM & 0.16 & 4.97 & 71.28 & 73.23   \\
                                & OTM-s & 0.061 & 4.85 & 97.49 & 99.57 \\
                                & Ours  & \textbf{0.058} & \textbf{0.59} & \textbf{0.057} & \textbf{0.65}  \\
        \midrule
        \multirow{3}{*}{$d=64$}  & OTM & 2.13 & 19.37 & 21.92 & 32.94  \\
                                & OTM-s & 2.74 & 18.79  & 0.20  & 12.21 \\
                                & Ours & \textbf{0.97} & \textbf{10.09} & \textbf{0.14} & \textbf{9.98}  \\
        \bottomrule
    \end{tabular}}
    \vspace{-15pt}
\end{table}


\paragraph{Qualitative Comparison}
In Sec. \ref{sec:failure}, we presented various examples where the existing SNOT framework may fail to learn the OT Map (or Plan). Here, we demonstrate that the existing approaches indeed encounter these failures, while OTP successfully learns the correct OT Map. As a baseline, we compare our method against the standard OTM with a deterministic transport map, i.e., $T_{\theta}(x)$. 

Fig. \ref{fig:fail_case_model} presents qualitative results on four datasets: Perpendicular (Ex.1), Parallel (Ex.2), One-to-Many (Ex.3), and Grid. The first row shows the vanilla OTM results and the second row exhibit our OTP results. Note that our OTP decreases the noise level until $\sigma = \epsilon_{min} > 0$ (Sec. \ref{sec:method}). Hence, the noised source samples $\Tilde{x}$ in Alg \ref{alg:otp} (Green in Fig \ref{fig:fail_case_model}) are transported to the target measure $\nu$.
In Fig. \ref{fig:fail_case_model}, \textbf{the vanilla OTM fails to learn the correct optimal transport plan in three cases except for the Parallel case}. OTM fails to cover the target measure $\nu$ in the Perpendicular and Multi-perpendicular cases. In the One-to-Many case, OTM does not learn the correct $T^{\star}$, i.e., the vertical transport. 

On the other hand, as we can see from a comparison with Fig. \ref{fig:fail_case}, \textbf{our OTP successfully learns the optimal transport plan $\pi^{\star}$}. In particular, in the One-to-Many example, our model successfully recovers the correct stochastic transport map $\pi^{\star}(y | x)$ by utilizing the initial noise as guidance to either the upper or lower mode of the target distribution.

\vspace{-7pt}
\paragraph{Quantitative Comparison to Ground-truth}
We evaluate the numerical accuracy of our OTP, SNOT with deterministic generator (OTM \citep{otm}), and SNOT with stochastic generator (OTM-s, Eq. \ref{eq:stochastic_generator}), by comparing them to the closed-form ground-truth solutions. Here, we measure two metrics: the transport cost error $D_{cost} = | W^2_2 (\mu, \nu) - \int \Vert T_{\theta}(x) - x \Vert^2 d\mu(x) |$ and the target distribution error $D_{target} = W^2_2 (T_{\theta \#} \mu, \nu)$.
$D_{cost}$ assesses whether the model achieves the optimal transport cost, while $D_{target}$ measures how accurately the model generates the target distribution.
Both models are tested on two synthetic datasets, Perpendicular and One-to-many (Fig. \ref{fig:fail_case_model}), with generalized dimensions of $d \in \{ 2, 4, 16, 64 \}$ (See Appendix \ref{appen:exp_syn} for dataset details.). 

Tab. \ref{tab:fail_case_quan} presents the quantitative results on the accuracy of the learned optimal transport plan $\pi_{\theta}$. Our OTP consistently achieves comparable or superior performance compared to both OTM and OTM-s across all metrics, particularly in high-dimensional settings. Note that these experimental results confirm the challenges of the existing SNOT framework in accurately recovering the target distributions, as discussed in Sec. \ref{sec:fail_det} and \ref{sec:fail_no_det}. Specifically, as shown in $D_{target}$, OTM and OTM-s models exhibit significantly larger target distribution errors in higher dimensions.


\begin{figure*}[t]
    \begin{minipage}{.43\linewidth}
        % \vspace{20pt}
        \centering
        \subfigure[OTM-s (FID=$62.4$, LPIPS=$0.36$)]{
        \includegraphics[height=0.3\linewidth]{images/main/otm_m2cm_source.png}
        \includegraphics[height=0.3\linewidth]{images/main/otm_m2cm_generate.png}}
        \subfigure[Ours (FID=$3.18$, LPIPS=$0.32$)]{
        \includegraphics[height=0.3\linewidth]{images/main/ours_m2cm_source.png}
        \includegraphics[height=0.3\linewidth]{images/main/ours_m2cm_generate.png}}
        \vspace{-10pt}
        \caption{\textbf{Experimental results on a stochastic transport map application}, i.e., MNIST-to-CMNIST translation.}
        % Experimental Results on MNIST$\rightarrow$CMNIST.

        \label{fig:m2cm}
    \end{minipage}
    \begin{minipage}{0.55\linewidth}
        \captionof{table}{
        \textbf{Image-to-Image translation benchmark} results compared to existing Neural (Entropic) OT models $\dagger$ indicates the results conducted by ourselves. DSBM scores are taken from \citep{asbm, SB-flow}. 
        } \label{tab:main_result}
        \centering
        \scalebox{0.75}{
        \begin{tabular}{c c c c}
        \toprule
        Data & Model  &  FID ($\downarrow$) & LPIPS ($\downarrow$) \\
        \midrule 
        \multirow{4}{*}{Male-to-Female (64x64)} 
        % & CycleGAN \citep{cyclegan} & 12.94 \\
        &  NOT \citep{not} & 11.96 & - \\
        & OTM$^\dagger$ \citep{fanscalable} &6.42 & \textbf{0.16} \\
        %7.xx \jw{5.15} \\
        & DIOTM$^\dagger$ \cite{diotm} & \textbf{4.48} & 0.20 \\
        & OTP (Ours) & 4.75 & 0.20 \\
        \midrule
        \multirow{4}{*}{Wild-to-Cat (64x64)} & DSBM \citep{dsbm} & 20$+$ & 0.59\\
        & OTM$^\dagger$ \citep{fanscalable} & 12.42 & 0.47
        %17.74 
        \\
        & DIOTM$^\dagger$ \cite{diotm} & 10.72 & \textbf{0.45} \\ 
        & OTP (Ours) & \textbf{9.66} & 0.52 \\
        \midrule
        \multirow{5}{*}{Male-to-Female (128x128)} & DSBM \citep{dsbm}
        & 37.8 & 0.25\\
        & ASBM \citep{asbm} & 16.08 & - \\
        & OTM$^\dagger$ \citep{fanscalable} & 7.55  & \textbf{0.21} \\
        & DIOTM$^\dagger$ \cite{diotm} & 7.40  & 0.25\\
        & OTP (Ours) & \textbf{6.38} & 0.27 \\
    \bottomrule
    \end{tabular}}
    \end{minipage}
    \vspace{-10pt}
\end{figure*}

\subsection{Neural OT Evaluation on Unpaired Image-to-Image Translation Tasks} \label{sec:exp_image}
In this section, \textbf{we evaluate our model on the unpaired image-to-image translation task.} The image-to-image translation is one of the most widely used machine learning tasks in Neural OT models. The optimal transport map $T^{\star}$ (or plan $\pi^{\star}$) can be understood as a generator of target distributions, mapping an input $x$ to a \textit{similar} counterpart $y$ by minimizing the transport cost $c(x, y)$. This mapping can be deterministic ($y = T^{\star}(x)$) or stochastic ($y \sim \pi^{\star}(\cdot | x)$). 
Therefore, the optimal transport map can naturally serve as a model for unpaired image-to-image translation.

\vspace{-5pt}
\paragraph{MNIST-to-CMNIST}
First, we demonstrate that our OTP model can learn stochastic transport mappings at the image scale. Specifically, we test our model on the MNIST-to-CMNIST translation task (See Appendix \ref{appen:exp_image} for dataset details).
In this task, our Colored MNIST (CMNIST) dataset consists of three colored variations (Red, Green, and Blue) for each grayscale image from the MNIST dataset (Fig. \ref{fig:concept_m2cm}). Consequently, the desired OP plan should stochastically map each grayscale digit image to a colored digit image of the same digit type (Fig. \ref{fig:concept_m2cm}). 

Fig. \ref{fig:m2cm} illustrates the experimental results. Here, we introduced a stochastic generator to OTM (OTM-s) to provide the capacity to learn a stochastic transport map. However, OTM exhibited the mode collapse problem, transporting all grayscale images to blue-colored images. On the other hand, our OTP successfully learns the optimal transport plan $\pi^{\star}$, achieving a stochastic mapping to Red, Green, and Blue colors. This phenomenon is also observed in the quantitative metrics. Our model significantly outperforms OTM in FID score ($\downarrow$) (3.18 vs. 62.4) and archives a better score in LPIPS ($\downarrow$) (0.32 vs. 0.36).

\vspace{-5pt}
\paragraph{Image-to-Image Translation}
We assess our model on three image-to-image translation benchmarks:  \textit{Male-to-Female \citep{celeba}} ($64\times64$, $128\times128$) and \textit{Wild-to-Cat \citep{afhq}} ($64 \times 64$). For comparison, we include several OT models (\textit{NOT, OTM, and DIOTM}) and Entropic OT models (\textit{DSBM and ASBM}).

Tab. \ref{tab:main_result} presents the quantitative scores for the image-to-image translation tasks (See Appendix \ref{appen:addtional_qual} for qualitative examples). We adopted the FID \citep{fid} and LPIPS \citep{lpips} scores for quantitative evaluation. Note that these FID and LPIPS scores serve similar roles as $D_{cost}$ and $D_{target}$ in Sec \ref{sec:exp_syn}, respectively. 
Our primary evaluation metric is the FID score because it measures how well the translated images align with the target semantics.
As shown in Tab. \ref{tab:main_result}, our model demonstrates state-of-the-art FID scores and competitive LPIPS scores compared to existing (entropic) Neural OT models. Specifically, in the Male-to-Female (128$\times$128) task, our OTP model achieves a FID score of 6.38, outperforming the SNOT model (OTM), the other Neural OT model (DIOTM), and entropic OT models (DSBM and ASBM). Although OTM achieves a lower but comparable LPIPS score (0.21), its significantly worse FID score (7.55) suggests large target semantic errors. Thus, we prioritize FID as our primary metric.

\section{Conclusion}
In this paper, we provided the first theoretical analysis of the sufficient condition that prevents the fake solution issue in Semi-dual Neural OT. Based on this analysis, we proposed our OTP model for learning both the OT Map and OT Plan, even when this sufficient condition is not satisfied. Our experiments demonstrated that OTP successfully recovers the correct OT Plan when existing models fail and achieves state-of-the-art performance in unpaired image-to-image translation. 
Our primary contribution is improving Neural OT frameworks by addressing their fundamental limitations, i.e., failing to recover the correct OT Map even with the ideal max-min solution. 
One limitation of our work is that our convergence theorem holds up to a subsequence (Thm. \ref{thm:convergence}). Nevertheless, in practice, our gradual training scheme (Alg 1) did not show any convergence issues. Also, our analysis provides a sufficient condition, rather than a necessary and sufficient one (Thm. \ref{thm:uniqueness}), leaving room for further refinement in understanding the exact conditions under which fake solutions occur.
