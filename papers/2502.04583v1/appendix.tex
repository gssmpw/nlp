\section{Proofs}

\subsection{A generalized version of Theorem \ref{thm:uniqueness}}

In this section, we introduce theorems in \citet{villani} and prove a generalized version of Them. \ref{thm:uniqueness} (See Thm.~\ref{thm:generalized}). We believe that rediscovering and summarizing this generalized version in the machine learning literature will help point out other possible directions for developing algorithms to avoid fake solutions.

In the generalized form, we consider $\mathcal{X}\subset M$, where $M$ is a smooth complete connected Riemannian manifold and $\mathcal{X}$ is a closed subset of $M$. $\mathcal{Y}$ is an arbitrary Polish space, and $c:M\times \mathcal{Y}\rightarrow \mathbb{R}$ is a continuous cost function. 
To begin with, we introduce the basic assumptions for the cost functional $c$:
\begin{definition}[$\mathbf{H_\infty}$ of Cost Function]
    We say the cost functional $c:\mathcal{X}\times \mathcal{Y}\rightarrow \mathbb{R}$ satisfies $\mathbf{H_\infty}$ if the cost $c$ satisfies the following conditions:
    \begin{enumerate}
        \item[\textbf{H1.}] For any $x$ and for any measurable set $S$ which the tangent space $T_x S$ is not contained in a half-space, there is a finite collection of elements $z_1, \dots, z_k \in S$ and small ball $B$ containing $x$, such that for any $y$ outside of a compact set,
        $$ \inf_{w\in B} c(w,y) \geq \inf_{1\leq j\leq k} c(z_j, y). $$
        \item[\textbf{H2.}] For andy $x$ and any neighborhood $U$ of $x$, there is a small ball $B$ containing $x$ such that 
        $$ \lim_{y\rightarrow \infty} \sup_{w\in B} \inf_{z\in U} [c(z,y) - c(w,y)] = - \infty. $$
    \end{enumerate}
\end{definition}

The following two lemmas plays a pivotal role to prove Thm. \ref{thm:uniqueness}. Furthermore, we introduce generalized version of the theorem.
\begin{lemma}[Theorem 5.10 in \cite{villani}] \label{lemma1}
Let $(\mathcal{X}, \mu)$ and $(\mathcal{Y}, \nu)$ be two Polish probability spaces and let $c:\mathcal{X}\times \mathcal{Y}\rightarrow \mathbb{R}$ be a lower semi-continuous cost function such that is lower bounded. Suppose the optimal cost $C(\mu, \nu) := \inf_{\pi \in \Pi(\mu, \nu)} \int c d\pi$ is finite. Then,
\begin{equation} \label{eq:strong-dual}
    C(\mu, \nu) = \max_{V\in S_c} \left( \int_{\mathcal{X}} V^c(x) d\mu(x) + \int_{\mathcal{Y}} V(y) d\nu(y) \right).
\end{equation}
In other words, there exists a $c$-concave function $V$ that makes strong duality (Eq. \ref{eq:strong-dual}) satify. Moreover, for $c$-cyclically monotone set $\Gamma \subset \mathcal{X}\times \mathcal{Y}$, for any $\pi\in \Pi(\mu, \nu)$ and $c$-concave set $\psi$,
\begin{equation}
    \pi \text{ is optimal } \Leftrightarrow \ \pi(\Gamma) = 1, \qquad \psi \text{ is optimal } \Leftrightarrow \ \Gamma \subset \partial_c \psi.
\end{equation}
\end{lemma}


\begin{lemma}[Theorem 10.28 in \cite{villani}] \label{lemma2} 
Let $M$ be a Riemannian manifold, $\mathcal{X}$ a closed subset of $M$, with ${\rm dim} (\partial \mathcal{X}) \leq d - 1$, and $\mathcal{Y}$ an arbitrary Polish space. Let $c:\mathcal{X}\times \mathcal{Y} \rightarrow \mathbb{R}$ be a continuous cost function, bounded below, and let $\mu \in P(\mathcal{X})$, $\nu\in P(\mathcal{Y})$, such that the optimal cost $C(\mu, \nu):= \inf_{\pi \in \Pi(\mu, \nu)} \int_{\mathcal{X}\times\mathcal{Y}} c(x,y) d\pi(x,y)$ is finite. Assume the following:
\begin{enumerate}[label=(\roman*)]
    \item $c$ is superdifferentiable everywhere;
    \item $\nabla_x c(x,\cdot)$ is injective where defined;
    \item any $c$-convex function is differentiable $\mu$-almost surely on its domain of $c$-subdifferentiability.
\end{enumerate}
Then, there exists a unique deterministic optimal coupling $\pi \in \Pi(\mu, \nu)$ in law. Moreover, there is a $c$-concave function $\psi$ such that 
\begin{equation} \label{eq:gradient_optimality}
    \nabla_x c(x,y) - \nabla \psi (x) = 0, \quad \mu \text{-a.s..}
\end{equation}
In other words, the Monge map $T^\star$ exists, and satisfies $\nabla_x c(x,T^\star(x)) - \nabla \psi (x) = 0, \ \mu \text{-a.s..}$
Additionally, suppose the cost functional $c$ is $\mathbf{H_\infty}$. Then,
\begin{itemize}
    \item If $T:\mathcal{X}\rightarrow \mathcal{Y}$ satisfies $T(x) \in \{ y\in \mathcal{Y} : \nabla_x c(x,y) - \nabla \psi (x) = 0  \}$, then $T$ is a unique Monge map (in law).
    \item Let $Z$ be the set of points where $\psi$ is non-differentiable; then one can define a continuous map $x\rightarrow T(x)$ on $\mathcal{X}\backslash Z$ by the equation $T(x)\in \partial_c \psi(x)$, and, ${\rm supp}(\nu) = \overline{T({\rm supp} (\mu) )}$.
\end{itemize}
\end{lemma}


Now, we state and prove the generalization version of Thm. \ref{thm:uniqueness}.
Then, we discuss how generalized theorem can be boiled down to Thm. \ref{thm:uniqueness}.

\begin{theorem} \label{thm:generalized}
    Suppose all the assumptions in Lemma \ref{lemma2} satisfies. Then, for the $c$-concave Kantorovich potential $V^\star:\mathcal{Y}\rightarrow \mathbb{R} \in L^1(\nu)$, the measurable function $T:\mathcal{X}\rightarrow \mathcal{Y}$ that satisfies
    \begin{equation}
        T(x) \in \arg\inf_{y\in\mathcal{Y}} \left\{ c(x,y) - V^\star(y) \right\}
    \end{equation}
    is a Monge map.
\end{theorem}
\begin{proof}
As shown in result of Lemma \ref{lemma2}, there exists a unique Monge map $T^\star$ (in law).
Let $\psi := (V^\star)^c$. Because $((V^\star)^c, V^\star)$ is the Kantorovich potentials, 
\begin{equation} \label{eq:c-subdifferential}
    T^\star(x) \in \partial_c (V^\star)^c (x) := \{ y\in \mathcal{Y} : (V^\star)^{cc}(y) = c(x,y) - (V^\star)^c (x) \},
\end{equation}
by the result of Lemma \ref{lemma1}.
Note that $\psi := V^c$ is a $c$-concave function, hence, by assumption \textit{(iii)}, it is differentiable $\mu$-a.s.. For every $y\in \partial_c (V^\star)^c (x)$, by differentiating $(V^\star)^{cc}(y) = c(x,y) - (V^\star)^c (x)$ with respect to $x$,
\begin{equation}
    0 = \nabla_x c(x, y) - \nabla (V^\star)^c(x) = \nabla_x c(x, y) - \nabla \psi(x).
\end{equation}
Therefore, the condition of $\psi$ in Lemma \ref{lemma2} satisfies.
By the last statement of Lemma \ref{lemma2}, the $c$-subdifferential $\partial_c \psi(x)$ is unique for every $x\in \mathcal{X}\backslash Z$ where $Z$ is a set of points where $\psi$ is non-differentiable. Because $\psi$ is differentiable $\mu$-almost surely, $Z$ has a zero measure (singular) with respect to $\mu$. Thus, $T(x) \in \partial_c (V^c)(x)$ is uniquely defined $\mu$-almost surely by the last statement of Lemma \ref{lemma2}.
Therefore, for every $x\in \mathcal{X}$,
\begin{equation} \label{eq:final_eq_for_proof}
    \inf_{y\in\mathcal{Y}} \left( c(x,y) - V(y) \right) = V^c(x) = c(x,y) - V^{cc}(y),
\end{equation}
has an unique solution $y\in\mathcal{Y}$ $\mu$-almost surely. Since we assumed $V^\star$ is $c$-concave, $V^{cc} = V$. Therefore, $\arg\inf_{y\in\mathcal{Y}} \left( c(x,y) - V(y) \right)$ has a unique solution $\mu$-a.s..
\end{proof}

\subsection{Proofs of Thm.~\ref{thm:uniqueness} and Thm.~\ref{thm:unique_potential}}

Now, we prove Thm.~\ref{thm:uniqueness} by showing that it satisfies all the assumptions of Lemma \ref{lemma2}.

\begin{proof}[Proof of Thm.~\ref{thm:uniqueness}]
\textbf{Step 1. Check the basic assumptions, \textit{(i)}, \textit{(ii)} in Lemma \ref{lemma2}:}
Since $\mathcal{X}=\mathcal{Y}$ are the closure of connected openset, it trivially satisfies the domain conditions.
Moreover, since $\mu$ and $\nu$ have finite second moments, the optimal cost is finite. Furthermore, since $c$ is a quadratic cost, it is continuous, superdifferentiable, and $\nabla_x c (x, \cdot)$ is injective for every $x\in\mathcal{X}$.

\textbf{Step 2. Realization of $\mathbf{H_\infty}$ Cost:}
Suppose $c(x,y) = h(x-y)$ for some $h:\mathbb{R}^d \rightarrow \mathcal{R}$. Then, if $h$ is radially symmetric and strictly increasing, then it satisfies $\mathbf{H1}$ \cite{gangbo1996geometry}.
Moreover, if $h$ is convex and superlinear, it implies $\mathbf{H2}$ \cite{gangbo1996geometry}. Since the quadratic cost $c$ satisfies these conditions, our cost functional satisfies $\mathbf{H_\infty}$.

\textbf{Step 3. Realization of Assumption \textit{(iii)}:} 
Assumption \textit{(iii)} can be satisfied in various ways. For completeness, we refer to the theoretical results outlined in Remark 10.33 of \citet{villani}, which provide the following sufficient conditions for Assumption \textit{(iii)}:
\begin{enumerate}
    \item[\textbf{C1.}] $c$ is Lipschitz on $\mathcal{X}\times \mathcal{Y}$ and $\mu$ is absolutely continuous.
    \item[\textbf{C2.}] $c$ is locally Lipschitz and $\mu, \nu$ are compactly supported and $\mu$ is absolutely continuous.
    \item[\textbf{C3.}] $c$ is locally semi-concave and satisfies $\mathbf{H_\infty}$ and $\mu$ does not give mass to $d-1$ dimension.
\end{enumerate}
Since our cost $c$ is quadratic, it is semi-concave. Moreover, the discussion in Step 2 implies $\mathbf{H_\infty}$. Finally, by our assumption on $\mu$ that it does not give mass to $d-1$ dimension, the condition \textbf{C3} satisfies. 
In conclusion, our assumptions in Thm. \ref{thm:uniqueness} satisfies all the conditions of Thm. \ref{thm:generalized}. Therefore, $\mathcal{D}_x$ is unique $\mu$-a.s..
\end{proof}

We would like to emphasize that exploring Polish spaces $(\mathcal{X}, \mu), \ (\mathcal{Y}, \nu)$, and cost functional $c:\mathcal{X}\times\mathcal{Y}\rightarrow \mathbb{R}$ that fulfill these conditions represents an interesting direction for future research and holds significant potential for further advancements.
Now, the 
Furthermore, note that Thm. \ref{thm:unique_potential} is can be directly proved by leveraging Cor. 4 in \citet{staudt2022c}:
\begin{proof}[Proof of Thm.~\ref{thm:unique_potential}]
    Let $K \subset \mathcal{X}$ be the compact set. Since $\mathcal{Y}$ is also compact, there exists $R>0$ such that $K, \mathcal{Y} \subset B_R (0)$. Then, for 
    \begin{equation}
         \Vert c(x, y_1) - c(x, y_2) \Vert \leq | x \cdot (y_1 - y_2) | + \frac{1}{2} | \Vert y_1\Vert^2 - \Vert y_2\Vert^2 | \leq R \Vert y_1 - y_2 \Vert + \frac{1}{2} 2R \Vert y_1 - y_2 \Vert = 2R \Vert y_1 - y_2 \Vert.
    \end{equation}
    Thus, by using Thm. 5 of \citet{staudt2022c}, our assumptions satisfy the conditions of Corollary 4 in \citet{staudt2022c}.
    Therefore, there exists a unique $c$-concave Kantorovich potential.
\end{proof}

Note that Corollary \ref{cor:unique_saddle} can be easily proved by simply combining
Thm.s \ref{thm:uniqueness} and \ref{thm:unique_potential}.


    


\section{Non-convergence of Stochastic Parametrization in Semi-dual Neural OT} \label{appen:non_conv_stocas_param} 
In this section, we further elaborate Prop. \ref{prop:stoc}. Let $\pi^\dagger(y | x)$ denote the conditional distribution induced by $T_\theta(x, \cdot):(Z, \mathcal{N}(0,I))\rightarrow \mathcal{Y}$ where
\begin{equation} \label{eq:stochastic_generator_appen}
    T_\theta (x,z) \in \arg\min_{y\in \mathcal{Y}} \{ c(x,y) - V^\star(y) \}, \quad (x, z) \sim \mu\times \mathcal{N}(0, I)\text{-a.s.}.
\end{equation}
Since the subdifferential of the $c$-transform of $V^{\star}$ is defined as 
\begin{equation}
    \partial_c (V^\star)^c(x) := \{y\in \mathcal{Y}: V^c (x) = c(x,y) - V^\star(y) \} = \arg\min_{y\in \mathcal{Y}} \{ c(x,y) - V^\star(y) \},
\end{equation}
Eq. \ref{eq:stochastic_generator_appen} can be rewritten as follows:
\begin{equation} \label{eq:stochastic-formal}
    \pi^\dagger (\partial_c (V^\star)^c (x) \,\,| \,\, x) = 1, \quad \mu\text{-a.s..} 
 \quad \Leftrightarrow \quad \pi^\dagger \in P(\mathcal{X}\times\mathcal{Y}) \,\, \hbox{ satisfies } \,\, \pi^\dagger \left( \partial_c V^\star \right) = 1.
\end{equation}
Here, $V^\star \in S_c$ represents the optimal potential and $\partial_c V^\star := \{(x,y)\in \mathcal{X}\times \mathcal{Y} : (V^\star)^c (x) + V(y) = c(x,y)\}$.
In summary, the stochastic map optimization problem in Eq. \ref{eq:stochastic_generator_appen} is equivalent to finding the joint distribution $\pi^\dagger \in P(\mathcal{X}\times \mathcal{Y})$ that satisfies $\pi^\dagger (\partial_c V^\star) = 1$.

However, in general, this condition $\pi^\dagger (\partial_c V^\star) = 1$ does not guarantee that $\pi$ is the optimal transport plan for the Kantorovich's problem (Eq. \ref{eq:Kantorovich}). As discussed in Lemma \ref{lemma1}, under the mild assumptions on the cost function $c$, $\pi^\dagger$ is optimal if and only if $\pi^\dagger (\Gamma) = 1$ for $c$-cyclic monotone $\Gamma$. Since $\Gamma \subset \partial_c V^\star$, we can say that one of the solution $\pi^\dagger$ is the optimal transport plan, however, the converse may not satisfy (See failure cases in Sec. \ref{sec:fail_no_det}). Additionally, if $\partial_c (V^\star)^c (x)$ is unique $\mu$-almost surely, then there is a unique (in law) deterministic optimal coupling of $(\mu, \nu)$ (See Thm. 5.30 in \citet{villani}).
The discussion above can be summarized as follows:
\begin{proposition}[Formal]
\label{prop:stoc_appen}
    Suppose the assumptions of Lemma \ref{lemma1} hold. Let $V^\star\in S_c$ be the Kantorovich potential. For $\pi^\dagger \in P(\mathcal{X}\times\mathcal{Y})$, the condition $\pi^\dagger (\partial_c V^\star) = 1$ does not imply that $\pi^\dagger$ is an optimal transport plan.
\end{proposition}

\section{Convergence of OTP} \label{appen:conv_result_from_oldandnew}
We present the convergence theorem for the optimal transport plan from \citet{villani}. Thm. \ref{thm:convergence} is a direct consequence of Thm. \ref{thm:uniqueness} and Thm. \ref{thm:convergence_appen}.
\begin{theorem}[\citet{villani}, Thm. 5.20] \label{thm:convergence_appen}
    Let $c \ge 0$ be a real-valued, continuous, and lower-bounded cost function. Consider a sequence of continuous cost functions $\{c_k\}_{k\in \mathbb{N}}$ that uniformly converges to $c$.
    Let $\{\mu_k\}_{k\in \mathbb{N}}$ and $\{\nu_k\}_{k\in \mathbb{N}}$ be sequences of probability measures that weakly converge to $\mu$ and $\nu$, respectively. For each $k$, let $\pi^{\star}_k$ be an optimal transport plan between $\mu_k$ and $\nu_k$. If $\int c_k d\pi^{\star}_k <\infty$, then, up to the extraction of a subsequence, $\pi^{\star}_k$ converges weakly to some $c$-cyclically monotone transport plan $\pi^{\star} \in \Pi(\mu, \nu)$. Moreover, if
    \begin{equation}
        \lim\inf_{k\in\mathbb{N}} \int c_k d\pi^{\star}_k < \infty,
    \end{equation}
    then the optimal transport cost between $\mu$ and $\nu$ is finite and $\pi^{\star}$ is an optimal transport plan.
\end{theorem}

\begin{corollary}[Corollary \ref{thm:convergence}] 
    Consider a sequence absolutely continuous probability measures $\{\mu_{\epsilon_k}\}_{k\in \mathbb{N}}$ such that $\{\mu_{\epsilon_k}\}$ weakly converges to $\mu$. Then, up to the extraction of a subsequence, our OTP model, utilizing $\{\mu_{\epsilon_k}\}_{k\in \mathbb{N}}$, weakly converges to the optimal transport plan $\pi^{\star}$ between $\mu$ and $\nu$.
\end{corollary}

\begin{proof}[Proof of Corollary \ref{thm:convergence}]
The absolutely continuous measure $\mu_{\epsilon_k}$ does not assign positive mass to measurable sets of Hausdorff dimension at most $d-1$. Therefore, by Thm. \ref{thm:uniqueness}, our OTP model for noise level $\epsilon_k$ (Eq. \ref{eq:saddle_epsilon}) correctly recovers the optimal transport plan $\pi_{k}^{\star} = (Id, T^{\star}_{k})_{\#} \mu_{\epsilon_k}$. Then, by Thm. \ref{thm:convergence_appen}, $\pi_{k}^{\star}$ weakly converges to the optimal transport plan $\pi^{\star}$ between $\mu$ and $\nu$.
\end{proof}

\section{Related Works} \label{sec:related_works}
In this section, we overview previous attempts to address the fake solution problem in the Semi-dual Neural OT approaches. The fake solution problem refers to the problem where the solution of the max-min learning objective $\mathcal{L}_{V_{\phi}, T_{\theta}}$ of Semi-dual Neural OT fails to fully characterize the correct optimal transport map. \citet{otm, fanscalable} first identified this issue. They proved that while true optimal potential and transport map $(V^{\star}, T^{\star})$ are the solution to the max-min problem $\mathcal{L}_{V_{\phi}, T_{\theta}}$ (Eq. \ref{eq:otm}), the reverse does not hold. Furthermore, \citet{fanTMLR} proved that, when $\mu$ is atomless, the saddle point solution $(V_{sad}^{\dagger}, T_{sad}^{\dagger})$ (Eq. \ref{eq:saddle_def}) of $\mathcal{L}_{V_{\phi}, T_{\theta}}$ recovers the optimal transport map:
\begin{equation} \label{eq:saddle_def}
    % \begin{aligned}
        V_{sad}^{\dagger} \in \arg\max_{V} \mathcal{L} (V, T_{sad}^{\dagger}), \,\,
        T_{sad}^{\dagger} \in \arg\min_{T} \mathcal{L} (V_{sad}^{\dagger}, T).    
    % \end{aligned}
\end{equation} 
However, this does not hold for the max-min solution of $\mathcal{L}_{V_{\phi}, T_{\theta}}$. In this paper, we establish the sufficient condition under which the max-min solution recovers the optimal transport map (Thm. \ref{thm:uniqueness}). Furthermore, we suggest a method for learning the optimal transport plan $\pi^{\star}$, which is applicable even when the optimal transport map $T^{\star}$ does not exist (Sec \ref{sec:method}).

For the weak OT problem, \citet{not, knot} provided a theoretical analysis for fake solutions in semi-dual approaches. Note that \textit{these analyses were conducted for the $\gamma$-weak quadratic cost with $\gamma > 0$ and, therefore, do not cover the standard OT problem.} Additionally, \citet{knot} proposed an alternative approach that modifies the cost function by introducing a positive definite symmetric kernel. While this approach addresses the fake solution issue in the weak OT problem, it solves an inherently different problem due to the modified cost function. 
In contrast, our work is \textbf{the first attempt to analyze the conditions under which fake solutions occur in the standard OT problem} (Eq. \ref{eq:Kantorovich}). 

\section{Implementation Details} \label{appen:implementation_details}

\subsection{Synthetic Data Experiments}
\label{appen:exp_syn}

In this section, we explain the implementation details for the synthetic data experiments (Sec. \ref{sec:exp_syn}), including dataset description, model architecture, and training hyperparameters.

\paragraph{Dataset Description}
Throughout this paragraph, let $x, y\in \mathbb{R}^d$, and $n = d/2$. Then, let $x = (x_1, x_2)$ and $y = (y_1, y_2)$, where $x_1, x_2, y_1, y_2 \in \mathbb{R}^n$. Moreover, let $e_1 = (1, 0, \dots, 0)\in \mathbb{R}^n$.
\begin{itemize}
    \item \textbf{Perpendicular}: We generate $x\sim \mu$ as follows: $x_1 \sim U\left( [-1, 1]^n \right)$, and $x_2 \equiv 0$. Similarly, we sample $y\sim \nu$ by $y_1 \equiv 0$ and $y_2 \sim U\left( [-1,1]^n \right)$.
    \item \textbf{Horizontal}: We generate $x\sim \mu$ as follows: $x_1 \sim U\left( [-1, 1]^n \right)$, and $x_2 \equiv 0$. Similarly, we sample $y\sim \nu$ by $y_1 \sim U\left( [-1,1]^n \right)$ and $y_2 = e_1$.
    \item \textbf{One-to-Many}: We generate $x\sim \mu$ as follows: $x_1 \sim U\left( [-1, 1]^n \right)$, and $x_2 \equiv 0$. Similarly, we sample $y\sim \nu$ by $y_1 \sim U\left( [-1,1]^n \right)$ and $y_2 \sim \text{Cat}((e_1, -e_1), (0.5,0.5))$.
    \item \textbf{Multi-Perpendicular}: Let $\mathbb{P} := \text{Cat}\left((\frac{-3}{4}, \frac{-1}{4}, \frac{1}{4}, \frac{3}{4})e_1, (\frac{1}{4},\frac{1}{4},\frac{1}{4},\frac{1}{4})\right)$ We generate $x\sim \mu$ by sampling $x_1 \sim U\left( [-1, 1]^n \right)$, and $x_2 \sim \mathbb{P}$. Similarly, we sample $y\sim \nu$ by $y_1 \sim \mathbb{P}$ and $y_2 \sim U\left( [-1, 1]^n \right)$.
\end{itemize}


\paragraph{Training Details}
For every experiments, we share the same network architecture and the same training hyperparameters.
We employ one-hidden layer with ReLU activations for both potential function $v_\phi$ and transport map $T_\theta$ parametrization.
For experiments of $d=2$ and $d=4$, we use hidden dimension of 256. For higher dimensions, we use hidden dimension of 1024.
We employ the batch size of 128, the number of iterations of 20K, the learning rate of $10^{-4}$, and Adam optimizer for $(\beta_1, \beta_2) = (0, 0.9)$. To improve the optimization of the inner loop in the dual formulation of \eqref{eq:otm}, we perform 20 updates to $T_\theta$ for every single update to $v_\phi$, i.e. $K_T = 20$.

For our experiment, we generate the perturbed data $\hat{x}$ as follows: $\hat{x} = x + \sigma z$ where $x\sim \mu, z\sim \mathcal{N}(\mathbf{0}, \mathbf{I})$.
Here, we schedule the noise level $\sigma$ from $\sigma_{max} = 0.2$ to $\sigma_{min} = 0.05$.
We update noise every $2K$ iterations, by the linear interpolation between initial noise to terminal noise. Specifically, in the $k$-th iteration, the noise level $\sigma_k$ is
\begin{equation}
    \sigma_k = (1-t) \sigma_{max} + t \sigma_{min}, \quad t = (P \times [k/P]+1) / K,
\end{equation}
where $P=2000$, $[\cdot]$ is the least integer function and $K$ is the total iteration number.

In the NOT \cite{not} implementation, we introduce additional noise $\xi \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$ into the network $T_\theta$. we set the dimensionality of the noise $\xi$ to match the input data dimension. We simply concatenate the noise and the data. This augmented input is then passed through the transport network $T_\theta$.

\subsection{Image Translation}
\label{appen:exp_image}



\paragraph{MNIST$\rightarrow$CMNIST}
In this paragraph, we describe the implementation details of Fig. \ref{fig:m2cm}.
We create red, green, and blue-colored MNIST datasets by isolating individual color channels. To achieve this, we assign the grayscale MNIST digit images to a single color channel (red, green, or blue) while setting the other two channels to zero.
We use the image size of 32, the number of iterations of 50K, the batch size of 64, the learning rate of $10^{-4}$, Adam optimizer with $(\beta_1, \beta_2) = (0, 0.9)$, $\lambda=10$, $K_T=10$, $\alpha=0.1$, $\sigma_{max} = 2$, $\sigma_{min} = 0.5$, and $P=100$.
Note that we don't use any other techniques such as learning rate scheduling, exponential moving avarage, dropout, and clip.

We adopt the network architectures of DCGAN \cite{dcgan}, depicted as follows: 
For generator, we use UNet architecture. For the input embedding module, we pass the input through the convolution layer, batch norm layer and activation layer. We employ four of downsample modules and four upsample modules, For every downsample module, we pass through convolution, activation, average plloing layer. For the upsample modules, we pass the inputs through upsample module, convolution, batch norm, activation layers. Note that as original UNet, we use the skip connections. For the last module, we pass through one convolution layer. For the activation function, we employ leaky ReLU with slope of 0.2. Every convolutional layers are $3\times 3$ convolutional layers. For average pooling, we use convolutional layer of $3\times 3$ with stride of 2. For upsampling module, we simply use blockwise upsampling module.

For the potential network, we use three downsample module. The settings of downsample module is same as the downsample module of the generator.
After the downsample modules, we flatten it and pass it through linear layer.
For every modules, note that the channel number (or the feature number) is fixed to 256.

\paragraph{Image-to-Image Translation}
In this paragraph, we describe the implementation details of Tab. \ref{tab:main_result}.
Most of the hyperparameter except the network architecture and the number of iterations are shared.
For every experiments, we use the batch size of 64, the learning rate of $10^{-4}$, Adam optimizer with $(\beta_1, \beta_2) = (0, 0.9)$, $\lambda=10$, $K_T=1$, $\alpha=0.001$, $\sigma_{max} = 2$, $\sigma_{min} = 0.2$, and $P=100$.
Note that we don't use any other techniques such as learning rate scheduling, exponential moving avarage, dropout, and clip.
In the experiments for image size of $64$, we follow the network architecture of the CIFAR-10 experiment in \cite{uotm, uotmsd}.
In the experiments for image size of $128$, we follow the network architecture of the CelebA-HQ experiment in \cite{uotm}.
For Wild$\rightarrow$Cat $(64\times 64)$, Male$\rightarrow$Female $(64\times 64)$, and Male$\rightarrow$Female $(128\times 128)$,
we take the number of iterations of 60K, 300K, and 500K, respectively.

Here, we use Variance-Preserving noise scheduling, which is illustrated as follows:
For $x\sim \mu, z\sim \mathcal{N}(\mathbf{0}, \mathbf{I})$, we generate the perturbed data $\hat{x}$ at $k$-th iterations as follows: $\hat{x} = \sqrt{1 - \epsilon_k} x + \sqrt{\epsilon_k} z$ where $\epsilon_k$ is defined as follows:
\begin{equation}
    \epsilon_k = 1 - \exp\left(-\frac{\sigma_{max} - \sigma_{min}}{2} t^2  - {\sigma_{min}} t\right), \quad t = 1 - (P \times [k/P]+1) / K,
\end{equation}
where $P=100$, $[\cdot]$ is the least integer function and $K$ is the total iteration number.

\paragraph{Evaluation metric}
We follow the evaluation metric of \citet{diotm}.
Specifically, for the Male$\rightarrow$Female translation task, we transform Male in the test dataset, and use the transformed samples and the test samples of the Female data to calculate the evaluation metrics. For Wild$\rightarrow$Cat, we generate 5000 samples from the test data of Wild dataset, and use the transformed samples and the training samples of the Cat data to calculate the evaluation metrics.

\clearpage

\section{Additional Results}

\subsection{Input Convex Neural Networks}

As discussed in Eq. \ref{eq:otm}, the search space for the potential $V$ is constrained to $S_c$, the set of $c$-concave potential functions. Strictly speaking, the potential $V_\phi$ must be specifically parameterized to ensure it lies within $S_c$.
To address this, we parameterize $V_\phi$ using an input convex neural network (ICNN) \cite{icnn, icnn-korotin}. The ICNN architecture $f_\phi: \mathcal{Y} \to \mathbb{R}$ is specifically designed with structural and weight constraints to enforce input convexity, satisfying the property: $ t f_\phi (x_1) + (1-t) f_\phi (x_2) \geq f((1-t)x_1 + t x_2)$ for all $t\in [0,1]$.
Given that $c(x,y) = \alpha \Vert x - y \Vert^2 $, $\alpha \Vert y \Vert^2 - V_\phi(y)$ should satisfy input convexity. To ensure this, we parametrize $V_\phi$ as follows:
\begin{equation}
    V_\phi (y) = \alpha \Vert y \Vert^2 - f_\phi (y),
\end{equation}
where $f_\phi$ is an ICNN.
Using this parameterization, we conduct experiments on two-dimensional data $d=2$ across several toy datasets. The results are presented in Fig. \ref{fig:icnn}.

\begin{figure}[h]
    \centering
    \subfigure[ICNN-OTM]{
    \includegraphics[width=0.23\linewidth]{images/appendix/vertical_icnn_nonoise.png}
    \includegraphics[width=0.23\linewidth]{images/appendix/one_to_many_icnn_nonoise.png}
    \label{fig:otm_icnn}
    }
    \hfill
    \subfigure[ICNN-OTP]{
    \includegraphics[width=0.23\linewidth]{images/appendix/vertical_icnn.png}
    \includegraphics[width=0.23\linewidth]{images/appendix/one_to_many_icnn.png}
    \label{fig:ours_icnn}
    }
    \vspace{-10pt}
    \caption{Visualization of failure cases with ICNN potential function $f_\phi(y) := \alpha \Vert y \Vert^2 - V_\phi(y)  $. We compare the Optimal Transport map (a) and our OTP model (b) in the failure cases. The source data $x \sim \mu$, target data $y \sim \nu$, and generated data $T(x)$ are represented in Blue, Orange, and Red. As illustrated in the figure, OTM fails to transport source to the target, i.e. $T_\# \mu \neq \nu$. On the other hand, our model successfully transports source $\mu$ to target $\nu$.}
    \label{fig:icnn}
\end{figure}

\subsection{Ablation Studies: Constant Noise Scheduling}
In practice, our OTP model decreases the noise level until it reaches a small constant $\epsilon_{min} > 0$ (see Algorithm paragraph in Sec \ref{sec:method}). Since the noise level is not reduced exactly to zero, an alternative approach is to train our OTP model directly at the minimum noise level $\epsilon_{min}$ instead of gradually decreasing it. To investigate this alternative, we conduct an ablation study on noise scheduling, specifically Constant Noise Scheduling (CNS).

Fig \ref{fig:const} and Tab \ref{tab:appen_const} present the results. In Fig \ref{fig:const}, our OTP model with CNS scheduling (OTP-CNS) shows the mode collapse problem. In the left figure of Fig \ref{fig:perp_const_appen}, OTP-CNS generates only the bottom part of the target distribution. Similarly, in the right figure of Fig \ref{fig:perp_const_appen}, OTP-CNS covers only the upper part of the target distribution. In contrast, Fig \ref{fig:one-to-many_const} demonstrates that our original OTP model with decreasing noise scheduling successfully covers the entire target distribution without exhibiting the mode collapse problem. Tab \ref{tab:appen_const} also shows this trend. Because of the mode collapse problem observed in Fig \ref{fig:perp_const_appen}, OTP-CNS shows a significantly larger target distribution error $D_{target}$. This ablation study shows that our noise-decreasing scheme is a more effective choice for the OTP model.


\begin{figure}[h]
    \centering
    \subfigure[Constant Noise Scheduling]{
    \includegraphics[width=0.23\linewidth]{images/appendix/vertical_const.png}
    \includegraphics[width=0.23\linewidth]{images/appendix/one_to_many_const.png}
    \label{fig:perp_const_appen}
    }
    \hfill
    \subfigure[Decreasing Noise Scheduling]{
    \includegraphics[width=0.23\linewidth]{images/appendix/vertical_ours.png}
    \includegraphics[width=0.23\linewidth]{images/appendix/one_to_many_ours.png}
    \label{fig:one-to-many_const}
    }
    \vspace{-10pt}
    \caption{We visualize the qualitative result between (a) constant noise scheduling, ($\sigma_{max} = \sigma_{min} = 0.05$) and (b) gradually decreasing noise scheduling ($\sigma_{max}=0.2, \sigma_{min}=0.05$). 
    We illustrate the results on the data dimension $d=64$. We select the 1st and 33rd axis to visualize the results effectively. The source data $x \sim \mu$, target data $y \sim \nu$, and generated data $T(x)$ are represented in Blue, Orange, and Red. The max-min solution fails to recover the correct OT map.}
    \label{fig:const}
\end{figure}

\begin{table}[h]
    \centering
    \caption{\textbf{Quantitative comparison of numerical accuracy} on synthetic datasets. The Const column stands for the constant noise scheduling, and Ours stands for our method which gradually decrease the noise level. Each model is evaluated by target distribution error $D_{target} (\downarrow)$}.
    \label{tab:appen_const}
    \scalebox{0.9}{
    \begin{tabular}{c c c c}
        \toprule
        Dimension & Model & Perpendicular & One-to-Many  \\
        \midrule
        \multirow{2}{*}{$d=16$}  & Constant & 0.64 & 72.17  \\
                                & Ours  & \textbf{0.59} & \textbf{0.65}  \\
        \midrule
        \multirow{2}{*}{$d=64$}  & Constant & 12.36 &  20.72  \\
                                & Ours & \textbf{10.09} & \textbf{9.98}  \\
        \bottomrule
    \end{tabular}}
\end{table}



\subsection{Additional Qualitative Results} \label{appen:addtional_qual}

\begin{figure}[h]
    \centering 
    \hfill
    \subfigure[$x\sim \mu_\epsilon$]{
    \includegraphics[width=0.48\linewidth]{images/appendix/appen_m2cm_source.png}
    }
    \hfill
    \subfigure[$T_\theta (x)$]{
    \includegraphics[width=0.48\linewidth]{images/appendix/appen_m2cm_generated.png}
    }
    \hfill
    \vspace{-10pt}
    \caption{Unpaired MNIST $\rightarrow$ CMNIST translation for 32 × 32 image.}
\end{figure}

\begin{figure}[h]
    \centering 
    \hfill
    \subfigure[$x\sim \mu_\epsilon$]{
    \includegraphics[width=0.48\linewidth]{images/appendix/appen_w2c_source.png}
    }
    \hfill
    \subfigure[$T_\theta (x)$]{
    \includegraphics[width=0.48\linewidth]{images/appendix/appen_w2c_generated.png}
    }
    \hfill
    \vspace{-10pt}
    \caption{Unpaired Wild $\rightarrow$ Cat translation for 64 × 64 image.}
\end{figure}

\begin{figure}[h]
    \centering 
    \hfill
    \subfigure[$x\sim \mu_\epsilon$]{
    \includegraphics[width=0.48\linewidth]{images/appendix/appen_m2f_source.png}
    }
    \hfill
    \subfigure[$T_\theta (x)$]{
    \includegraphics[width=0.48\linewidth]{images/appendix/appen_m2f_generated.png}
    }
    \hfill
    \vspace{-10pt}
    \caption{Unpaired Male $\rightarrow$ Female translation for 64 × 64 image.}
\end{figure}

\begin{figure}[h]
    \centering 
    \hfill
    \subfigure[$x\sim \mu_\epsilon$]{
    \includegraphics[width=0.48\linewidth]{images/appendix/appen_m2f_128_source.png}
    }
    \hfill
    \subfigure[$T_\theta (x)$]{
    \includegraphics[width=0.48\linewidth]{images/appendix/appen_m2f_128_generated.png}
    }
    \hfill
    \vspace{-10pt}
    \caption{Unpaired Male $\rightarrow$ Female translation for 128 × 128 image.}
\end{figure}
