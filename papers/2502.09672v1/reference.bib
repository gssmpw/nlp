@article{ahmarEnhancingThermalMOT2024,
  title={Enhancing Thermal MOT: A Novel Box Association Method Leveraging Thermal Identity and Motion Similarity},
  author={Ahmar, Wassim El and Kolhatkar, Dhanvin and Nowruzi, Farzan and Laganiere, Robert},
  journal={arXiv preprint arXiv:2411.12943},
  year={2024}
}

@inproceedings{anhEnhancedKalmanAdaptive2024,
  title={Enhanced Kalman with Adaptive Appearance Motion SORT for Grounded Generic Multiple Object Tracking},
  author={Anh, Duy Le Dinh and Tran, Kim Hoang and Nguyen, Quang-Thuc and Le, Ngan Hoang},
  booktitle={Asian Conference on Computer Vision},
  pages={310--328},
  year={2025},
  organization={Springer}
}


@misc{antunes-garciaFastEfficientTransformerbased2024,
  title = {Fast and {{Efficient Transformer-based Method}} for {{Bird}}'s {{Eye View Instance Prediction}}},
  author = {{Antunes-Garc{\'i}a}, Miguel and Bergasa, Luis M. and {Montiel-Mar{\'i}n}, Santiago and Barea, Rafael and {S{\'a}nchez-Garc{\'i}a}, Fabio and Llamazares, {\'A}ngel},
  year = {2024},
  month = nov,
  number = {arXiv:2411.06851},
  eprint = {2411.06851},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.06851},
  urldate = {2024-11-26},
  abstract = {Accurate object detection and prediction are critical to ensure the safety and efficiency of self-driving architectures. Predicting object trajectories and occupancy enables autonomous vehicles to anticipate movements and make decisions with future information, increasing their adaptability and reducing the risk of accidents. Current State-Of-The-Art (SOTA) approaches often isolate the detection, tracking, and prediction stages, which can lead to significant prediction errors due to accumulated inaccuracies between stages. Recent advances have improved the feature representation of multi-camera perception systems through Bird's-Eye View (BEV) transformations, boosting the development of end-to-end systems capable of predicting environmental elements directly from vehicle sensor data. These systems, however, often suffer from high processing times and number of parameters, creating challenges for real-world deployment. To address these issues, this paper introduces a novel BEV instance prediction architecture based on a simplified paradigm that relies only on instance segmentation and flow prediction. The proposed system prioritizes speed, aiming at reduced parameter counts and inference times compared to existing SOTA architectures, thanks to the incorporation of an efficient transformer-based architecture. Furthermore, the implementation of the proposed architecture is optimized for performance improvements in PyTorch version 2.1. Code and trained models are available at https://github.com/miguelag99/Efficient-Instance-Prediction},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\Administrator\\Zotero\\storage\\CJB7ZVFH\\Antunes-García 等 - 2024 - Fast and Efficient Transformer-based Method for Bi.pdf;C\:\\Users\\Administrator\\Zotero\\storage\\JINNKNTZ\\2411.html}
}

@inproceedings{benbarkaScoreRefinementConfidencebased2021,
  title = {Score Refinement for Confidence-Based {{3D}} Multi-Object Tracking},
  booktitle = {2021 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Benbarka, Nuri and Schr{\"o}der, Jona and Zell, Andreas},
  year = {2021},
  month = sep,
  pages = {8083--8090},
  issn = {2153-0866},
  doi = {10.1109/IROS51168.2021.9636032},
  urldate = {2024-08-29},
  abstract = {Multi-object tracking is a critical component in autonomous navigation, as it provides valuable information for decision-making. Many researchers tackled the 3D multi-object tracking task by filtering out the frame-by-frame 3D detections; however, their focus was mainly on finding useful features or proper matching metrics. Our work focuses on a neglected part of the tracking system: score refinement and tracklet termination. We show that manipulating the scores depending on time consistency while terminating the tracklets depending on the tracklet score improves tracking results. We do this by increasing the matched tracklets' score with score update functions and decreasing the unmatched tracklets' score. Compared to count-based methods, our method consistently produces better AMOTA and MOTA scores when utilizing various detectors and filtering algorithms on different datasets. The improvements in AMOTA score went up to 1.83 and 2.96 in MOTA. We also used our method as a late-fusion ensembling method, and it performed better than voting-based ensemble methods by a solid margin. It achieved an AMOTA score of 67.6 on nuScenes test evaluation, which is comparable to other state-of-the-art trackers. Code is publicly available at: https://github.com/cogsys-tuebingen/CBMOT.},
  langid = {american},
  keywords = {Detectors,Filtering,Filtering algorithms,Measurement,Pipelines,Solids,Three-dimensional displays},
  file = {C\:\\Users\\Administrator\\Zotero\\storage\\F7539STX\\Benbarka 等 - 2021 - Score refinement for confidence-based 3D multi-obj.pdf;C\:\\Users\\Administrator\\Zotero\\storage\\WIVI5UTV\\9636032.html}
}

@inproceedings{bodlaSoftNMSImprovingObject2017,
  title = {Soft-{{NMS}} -- {{Improving Object Detection With One Line}} of {{Code}}},
  booktitle = {Proceedings of the {{IEEE International Conference}} on {{Computer Vision}}},
  author = {Bodla, Navaneeth and Singh, Bharat and Chellappa, Rama and Davis, Larry S.},
  year = {2017},
  pages = {5561--5569},
  urldate = {2024-10-19},
  langid = {american},
  keywords = {NMS},
  file = {C:\Users\Administrator\Zotero\storage\625Y8EQY\Bodla 等 - 2017 - Soft-NMS -- Improving Object Detection With One Li.pdf}
}

@article{caoPKFProbabilisticData2024,
  title={PKF: Probabilistic Data Association Kalman Filter for Multi-Object Tracking},
  author={Cao, Hanwen and Pappas, George J and Atanasov, Nikolay},
  journal={arXiv preprint arXiv:2411.06378},
  year={2024}
}


@misc{fangLiDARSLAMMOTBased2024,
  title = {{{LiDAR SLAMMOT}} Based on {{Confidence-guided Data Association}}},
  author = {Fang, Susu and Li, Hao},
  year = {2024},
  month = dec,
  number = {arXiv:2412.01041},
  eprint = {2412.01041},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.01041},
  urldate = {2024-12-10},
  abstract = {In the field of autonomous driving or robotics, simultaneous localization and mapping (SLAM) and multi-object tracking (MOT) are two fundamental problems and are generally applied separately. Solutions to SLAM and MOT usually rely on certain assumptions, such as the static environment assumption for SLAM and the accurate ego-vehicle pose assumption for MOT. But in complex dynamic environments, it is difficult or even impossible to meet these assumptions. Therefore, the SLAMMOT, i.e., simultaneous localization, mapping, and moving object tracking, integrated system of SLAM and object tracking, has emerged for autonomous vehicles in dynamic environments. However, many conventional SLAMMOT solutions directly perform data association on the predictions and detections for object tracking, but ignore their quality. In practice, inaccurate predictions caused by continuous multi-frame missed detections in temporary occlusion scenarios, may degrade the performance of tracking, thereby affecting SLAMMOT. To address this challenge, this paper presents a LiDAR SLAMMOT based on confidence-guided data association (Conf SLAMMOT) method, which tightly couples the LiDAR SLAM and the confidence-guided data association based multi-object tracking into a graph optimization backend for estimating the state of the ego-vehicle and objects simultaneously. The confidence of prediction and detection are applied in the factor graph-based multi-object tracking for its data association, which not only avoids the performance degradation caused by incorrect initial assignments in some filter-based methods but also handles issues such as continuous missed detection in tracking while also improving the overall performance of SLAMMOT. Various comparative experiments demonstrate the superior advantages of Conf SLAMMOT, especially in scenes with some missed detections.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Robotics},
  file = {C\:\\Users\\Administrator\\Zotero\\storage\\E799MCH3\\Fang 和 Li - 2024 - LiDAR SLAMMOT based on Confidence-guided Data Asso.pdf;C\:\\Users\\Administrator\\Zotero\\storage\\D9H7E7WR\\2412.html}
}

@article{fengFrameRateAgnostic2024,
  title = {Towards {{Frame Rate Agnostic Multi-object Tracking}}},
  author = {Feng, Weitao and Bai, Lei and Yao, Yongqiang and Yu, Fengwei and Ouyang, Wanli},
  year = {2024},
  month = may,
  journal = {International Journal of Computer Vision},
  volume = {132},
  number = {5},
  pages = {1443--1462},
  issn = {1573-1405},
  doi = {10.1007/s11263-023-01943-2},
  urldate = {2024-08-19},
  abstract = {Multi-object Tracking (MOT) is one of the most fundamental computer vision tasks that contributes to various video analysis applications. Despite the recent promising progress, current MOT research is still limited to a fixed sampling frame rate of the input stream. They are neither as flexible as humans nor well-matched to industrial scenarios which require the trackers to be frame rate insensitive in complicated conditions. In fact, we empirically found that the accuracy of all recent state-of-the-art trackers drops dramatically when the input frame rate changes. For a more intelligent tracking solution, we shift the attention of our research work to the problem of Frame Rate Agnostic MOT (FraMOT), which takes frame rate insensitivity into consideration. In this paper, we propose a Frame Rate Agnostic MOT framework with a Periodic training Scheme (FAPS) to tackle the FraMOT problem for the first time. Specifically, we propose a Frame Rate Agnostic Association Module (FAAM) that infers and encodes the frame rate information to aid identity matching across multi-frame-rate inputs, improving the capability of the learned model in handling complex motion-appearance relations in FraMOT. Moreover, the association gap between training and inference is enlarged in FraMOT because those post-processing steps not included in training make a larger difference in lower frame rate scenarios. To address it, we propose Periodic Training Scheme to reflect all post-processing steps in training via tracking pattern matching and fusion. Along with the proposed approaches, we make the first attempt to establish an evaluation method for this new task of FraMOT. Besides providing simulations and evaluation metrics, we try to solve new challenges in two different modes, i.e., known frame rate and unknown frame rate, aiming to handle a more complex situation. The quantitative experiments on the challenging MOT17/20 dataset (FraMOT version) have clearly demonstrated that the proposed approaches can handle different frame rates better and thus improve the robustness against complicated scenarios.},
  langid = {english},
  keywords = {Artificial Intelligence,Frame rate agnostic,Frame Rate Agnostic Association,Frame rate agnostic MOT framework,Frame rate information inference and encoding,Multi-frame-rate,Multi-object-tracking,Periodic training scheme},
  annotation = {abstractTranslation: 多目标跟踪 （MOT） 是最基本的计算机视觉任务之一，有助于各种视频分析应用。尽管最近取得了可喜的进展，但目前的 MOT 研究仍然局限于输入流的固定采样帧速率。它们既不像人类那样灵活，也无法很好地匹配要求跟踪器在复杂条件下对帧速率不敏感的工业场景。事实上，我们凭经验发现，当输入帧速率发生变化时，所有最新最先进的跟踪器的准确性都会急剧下降。为了获得更智能的跟踪解决方案，我们将研究工作的注意力转移到与帧速率无关的 MOT （FraMOT） 问题上，该问题考虑了帧速率不敏感性。在本文中，我们提出了一个与帧率无关的 MOT 框架，其中包含定期训练计划 （FAPS），首次解决了 FraMOT 问题。具体来说，我们提出了一个与帧速率无关的关联模块（FAAM），它对帧速率信息进行推断和编码，以帮助跨多帧速率输入进行身份匹配，从而提高学习模型在FraMOT中处理复杂运动-外观关系的能力。此外，在 FraMOT 中，训练和推理之间的关联差距被放大，因为那些未包含在训练中的后处理步骤在较低帧速率场景中会产生更大的差异。为了解决这个问题，我们提出了 Periodic Training Scheme 来通过跟踪模式匹配和融合来反映训练中的所有后处理步骤。除了提出的方法外，我们还首次尝试为 FraMOT 的这项新任务建立评估方法。除了提供模拟和评估指标外，我们还尝试以两种不同的模式解决新的挑战，即已知帧率和未知帧率，旨在处理更复杂的情况。在具有挑战性的 MOT17/20 数据集（FraMOT 版本）上的定量实验清楚地表明，所提出的方法可以更好地处理不同的帧速率，从而提高复杂场景下的鲁棒性。},
  file = {C:\Users\Administrator\Zotero\storage\A99PS67U\Feng 等 - 2024 - Towards Frame Rate Agnostic Multi-object Tracking.pdf}
}

@INPROCEEDINGS{huangJointMultiObjectDetection2021,
  author={Huang, Kemiao and Hao, Qi},
  booktitle={2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
  title={Joint Multi-Object Detection and Tracking with Camera-LiDAR Fusion for Autonomous Driving}, 
  year={2021},
  volume={},
  number={},
  pages={6983-6989},
  keywords={Training;Solid modeling;Three-dimensional displays;Tracking;Robot vision systems;Object detection;Benchmark testing},
  doi={10.1109/IROS51168.2021.9636311}}

@INPROCEEDINGS{kimEagerMOT3DMultiObject2021,
  author={Kim, Aleksandr and Ošep, Aljoša and Leal-Taixé, Laura},
  booktitle={2021 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={EagerMOT: 3D Multi-Object Tracking via Sensor Fusion}, 
  year={2021},
  volume={},
  number={},
  pages={11315-11321},
  keywords={Visualization;Three-dimensional displays;Target tracking;Sensor fusion;Robot sensing systems;Sensors;Trajectory},
  doi={10.1109/ICRA48506.2021.9562072}}



@article{liangNeuralEnhancedBelief2024a,
  title = {Neural {{Enhanced Belief Propagation}} for {{Multiobject Tracking}}},
  author = {Liang, Mingchao and Meyer, Florian},
  year = {2024},
  journal = {IEEE Transactions on Signal Processing},
  volume = {72},
  pages = {15--30},
  issn = {1941-0476},
  doi = {10.1109/TSP.2023.3314275},
  urldate = {2024-08-24},
  abstract = {Algorithmic solutions for multi-object tracking (MOT) are a key enabler for applications in autonomous navigation and applied ocean sciences. State-of-the-art MOT methods fully rely on a statistical model and typically use preprocessed sensor data as measurements. In particular, measurements are produced by a detector that extracts potential object locations from the raw sensor data collected at discrete time steps. This preparatory processing step reduces data flow and computational complexity but may result in a loss of information. State-of-the-art Bayesian MOT methods that are based on belief propagation (BP) systematically exploit graph structures of the statistical model to reduce computational complexity and improve scalability. However, as a fully model-based approach, BP can provide highly suboptimal estimates when there is a mismatch between the statistical model and the true data-generating process. Existing BP-based MOT methods can further only make use of preprocessed measurements. In this paper, we introduce a variant of BP that combines model-based with data-driven MOT. The proposed neural enhanced belief propagation (NEBP) method complements the statistical model of BP by information learned from raw sensor data. This approach conjectures that the learned information can reduce model mismatch and thus improve data association and false alarm rejection. Our NEBP method improves tracking performance compared to model-based methods. At the same time, it inherits the advantages of BP-based MOT, i.e., it scales only quadratically in the number of objects, and it can thus generate and maintain a large number of object tracks. We evaluate the performance of our NEBP approach for MOT on the nuScenes autonomous driving dataset and demonstrate that it can achieve state-of-the-art performance. In particular, an average multi-object tracking accuracy (AMOTA) of 0.683 was obtained and, compared with non-BP-based methods, identity switches (IDS) and track fragments (Frag) were reduced by 23\% and 19\%, respectively.},
  keywords = {belief propagation,Data models,factor graphs,Feature extraction,graph neural networks,Graph neural networks,Multiobject tracking,probabilistic data association,Probabilistic logic,Radar tracking,Sea measurements,Shape},
  file = {C\:\\Users\\Administrator\\Zotero\\storage\\6LAZYQGX\\Liang 和 Meyer - 2024 - Neural Enhanced Belief Propagation for Multiobject.pdf;C\:\\Users\\Administrator\\Zotero\\storage\\M8YZQ62U\\10258113.html}
}

@article{liaoFastTrackTrFastMultiObject2024,
  title={FastTrackTr: Towards Fast Multi-Object Tracking with Transformers},
  author={Liao, Pan and Yang, Feng and Wu, Di and Yu, Jinwen and Zhao, Wenhui and Liu, Bo},
  journal={arXiv preprint arXiv:2411.15811},
  year={2024}
}


@misc{liHopTrackRealtimeMultiObject2024,
  title = {{{HopTrack}}: {{A Real-time Multi-Object Tracking System}} for {{Embedded Devices}}},
  shorttitle = {{{HopTrack}}},
  author = {Li, Xiang and Chen, Cheng and Lou, Yuan-yao and Abdallah, Mustafa and Kim, Kwang Taik and Bagchi, Saurabh},
  year = {2024},
  month = nov,
  number = {arXiv:2411.00608},
  eprint = {2411.00608},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.00608},
  urldate = {2024-11-23},
  abstract = {Multi-Object Tracking (MOT) poses significant challenges in computer vision. Despite its wide application in robotics, autonomous driving, and smart manufacturing, there is limited literature addressing the specific challenges of running MOT on embedded devices. State-of-the-art MOT trackers designed for high-end GPUs often experience low processing rates ({$<$}11fps) when deployed on embedded devices. Existing MOT frameworks for embedded devices proposed strategies such as fusing the detector model with the feature embedding model to reduce inference latency or combining different trackers to improve tracking accuracy, but tend to compromise one for the other. This paper introduces HopTrack, a real-time multi-object tracking system tailored for embedded devices. Our system employs a novel discretized static and dynamic matching approach along with an innovative content-aware dynamic sampling technique to enhance tracking accuracy while meeting the real-time requirement. Compared with the best high-end GPU modified baseline Byte (Embed) and the best existing baseline on embedded devices MobileNet-JDE, HopTrack achieves a processing speed of up to 39.29 fps on NVIDIA AGX Xavier with a multi-object tracking accuracy (MOTA) of up to 63.12\% on the MOT16 benchmark, outperforming both counterparts by 2.15\% and 4.82\%, respectively. Additionally, the accuracy improvement is coupled with the reduction in energy consumption (20.8\%), power (5\%), and memory usage (8\%), which are crucial resources on embedded devices. HopTrack is also detector agnostic allowing the flexibility of plug-and-play.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\Administrator\\Zotero\\storage\\45RBR3TR\\Li 等 - 2024 - HopTrack A Real-time Multi-Object Tracking System.pdf;C\:\\Users\\Administrator\\Zotero\\storage\\ISF99NG8\\2411.html}
}

@misc{linHSTrackBootstrapEndtoEnd2024,
  title = {{{HSTrack}}: {{Bootstrap End-to-End Multi-Camera 3D Multi-object Tracking}} with {{Hybrid Supervision}}},
  shorttitle = {{{HSTrack}}},
  author = {Lin, Shubo and Kou, Yutong and Li, Bing and Hu, Weiming and Gao, Jin},
  year = {2024},
  month = nov,
  number = {arXiv:2411.06780},
  eprint = {2411.06780},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.06780},
  urldate = {2024-11-26},
  abstract = {In camera-based 3D multi-object tracking (MOT), the prevailing methods follow the tracking-by-query-propagation paradigm, which employs track queries to manage the lifecycle of identity-consistent tracklets while object queries handle the detection of new-born tracklets. However, this intertwined paradigm leads the inter-temporal tracking task and the single-frame detection task utilize the same model parameters, complicating training optimization. Drawing inspiration from studies on the roles of attention components in transformer-based decoders, we identify that the dispersing effect of self-attention necessitates object queries to match with new-born tracklets. This matching strategy diverges from the detection pre-training phase, where object queries align with all ground-truth targets, resulting in insufficient supervision signals. To address these issues, we present HSTrack, a novel plug-and-play method designed to co-facilitate multi-task learning for detection and tracking. HSTrack constructs a parallel weight-share decoder devoid of self-attention layers, circumventing competition between different types of queries. Considering the characteristics of cross-attention layer and distinct query types, our parallel decoder adopt one-to-one and one-to-many label assignment strategies for track queries and object queries, respectively. Leveraging the shared architecture, HSTrack further improve trackers for spatio-temporal modeling and quality candidates generation. Extensive experiments demonstrate that HSTrack consistently delivers improvements when integrated with various query-based 3D MOT trackers. For example, HSTrack improves the state-of-the-art PF-Track method by \$+2.3{\textbackslash}\%\$ AMOTA and \$+1.7{\textbackslash}\%\$ mAP on the nuScenes dataset.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\Administrator\\Zotero\\storage\\D2KDEKRX\\Lin 等 - 2024 - HSTrack Bootstrap End-to-End Multi-Camera 3D Mult.pdf;C\:\\Users\\Administrator\\Zotero\\storage\\DM527QZ3\\2411.html}
}

@article{liPerformancePredictionInteracting1993,
  title = {Performance Prediction of the Interacting Multiple Model Algorithm},
  author = {Li, X.R. and {Bar-Shalom}, Y.},
  year = {1993},
  month = jul,
  journal = {IEEE Transactions on Aerospace and Electronic Systems},
  volume = {29},
  number = {3},
  pages = {755--771},
  issn = {0018-9251, 1557-9603, 2371-9877},
  doi = {10.1109/7.220926},
  urldate = {2024-12-18},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  file = {C:\Users\Administrator\Zotero\storage\698D2QZP\Li和Bar-Shalom - 1993 - Performance prediction of the interacting multiple model algorithm.pdf}
}

@inproceedings{liuAdaptiveNMSRefining2019,
  title = {Adaptive {{NMS}}: {{Refining Pedestrian Detection}} in a {{Crowd}}},
  shorttitle = {Adaptive {{NMS}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Liu, Songtao and Huang, Di and Wang, Yunhong},
  year = {2019},
  pages = {6459--6468},
  urldate = {2024-10-19},
  keywords = {NMS},
  file = {C:\Users\Administrator\Zotero\storage\MLNNCSRN\Liu 等 - 2019 - Adaptive NMS Refining Pedestrian Detection in a C.pdf}
}

@article{liuUKFMOTUnscentedKalman,
  title = {{{UKF-MOT}}: {{An}} Unscented {{Kalman}} Filter-Based {{3D}} Multi-Object Tracker},
  shorttitle = {{{UKF-MOT}}},
  author = {Liu, Meng and Niu, Jianwei and Liu, Yu},
  journal = {CAAI Transactions on Intelligence Technology},
  volume = {n/a},
  number = {n/a},
  issn = {2468-2322},
  doi = {10.1049/cit2.12315},
  urldate = {2024-08-19},
  abstract = {Multi-object tracking in autonomous driving is a non-linear problem. To better address the tracking problem, this paper leveraged an unscented Kalman filter to predict the object's state. In the association stage, the Mahalanobis distance was employed as an affinity metric, and a Non-minimum Suppression method was designed for matching. With the detections fed into the tracker and continuous `predicting-matching' steps, the states of each object at different time steps were described as their own continuous trajectories. We conducted extensive experiments to evaluate tracking accuracy on three challenging datasets (KITTI, nuScenes and Waymo). The experimental results demonstrated that our method effectively achieved multi-object tracking with satisfactory accuracy and real-time efficiency.},
  copyright = {{\copyright} 2024 China North Artificial Intelligence and Innovation Research Institute. CAAI Transactions on Intelligence Technology published by John Wiley \& Sons Ltd on behalf of The Institution of Engineering and Technology and Chongqing University of Technology.},
  langid = {english},
  keywords = {autonomous vehicle,transportation},
  file = {C\:\\Users\\Administrator\\Zotero\\storage\\CJGBYEEY\\Liu 等 - UKF-MOT An unscented Kalman filter-based 3D multi.pdf;C\:\\Users\\Administrator\\Zotero\\storage\\BJWECWPA\\cit2.html}
}
@article{wangMCTrackUnified3D2024,
  title={MCTrack: A Unified 3D Multi-Object Tracking Framework for Autonomous Driving},
  author={Wang, Xiyang and Qi, Shouzheng and Zhao, Jieyou and Zhou, Hangning and Zhang, Siyu and Wang, Guoan and Tu, Kai and Guo, Songlin and Zhao, Jianbo and Li, Jian and others},
  journal={arXiv preprint arXiv:2409.16149},
  year={2024}
}

@article{nagyRobMOTRobust3D2024,
  title={RobMOT: Robust 3D Multi-Object Tracking by Observational Noise and State Estimation Drift Mitigation on LiDAR PointCloud},
  author={Nagy, Mohamed and Werghi, Naoufel and Hassan, Bilal and Dias, Jorge and Khonji, Majid},
  journal={arXiv preprint arXiv:2405.11536},
  year={2024}
}


@article{rapado-rinconMOTDETR3DSingle2024,
  title = {{{MOT-DETR}}: {{3D}} Single Shot Detection and Tracking with Transformers to Build {{3D}} Representations for Agro-Food Robots},
  shorttitle = {{{MOT-DETR}}},
  author = {{Rapado-Rincon}, David and Nap, Henk and Smolenova, Katarina and Van Henten, Eldert J. and Kootstra, Gert},
  year = {2024},
  month = oct,
  journal = {Computers and Electronics in Agriculture},
  volume = {225},
  pages = {109275},
  issn = {01681699},
  doi = {10.1016/j.compag.2024.109275},
  urldate = {2024-08-19},
  abstract = {In the current demand for automation in the agro-food industry, accurately detecting and localizing relevant objects in 3D is essential for successful robotic operations. However, this is a challenge due the presence of occlusions. Multi-view perception approaches allow robots to overcome occlusions, but a tracking component is needed to associate the objects detected by the robot over multiple viewpoints. Most multi-object tracking (MOT) algorithms are designed for high frame rate sequences and struggle with the occlusions generated by robots' motions and 3D environments. In this paper, we introduce MOT-DETR, a novel approach to detect and track objects in 3D over time using a combination of convolutional networks and transformers. Our method processes 2D and 3D data, and employs a transformer architecture to perform data fusion. We show that MOTDETR outperforms state-of-the-art multi-object tracking methods. Furthermore, we prove that MOT-DETR can leverage 3D data to deal with long-term occlusions and large frame-to-frame distances better than state-of-theart methods. Finally, we show how our method is resilient to camera pose noise that can affect the accuracy of point clouds. The implementation of MOT-DETR can be found here: https://github.com/drapado/mot-detr.},
  langid = {english},
  file = {C:\Users\Administrator\Zotero\storage\A633384B\Rapado-Rincon 等 - 2024 - MOT-DETR 3D single shot detection and tracking wi.pdf}
}

@article{sadjadpourShaSTAModelingShape2024,
  title = {{{ShaSTA}}: {{Modeling Shape}} and {{Spatio-Temporal Affinities}} for {{3D Multi-Object Tracking}}},
  shorttitle = {{{ShaSTA}}},
  author = {Sadjadpour, Tara and Li, Jie and Ambrus, Rares and Bohg, Jeannette},
  year = {2024},
  month = may,
  journal = {IEEE Robotics and Automation Letters},
  volume = {9},
  number = {5},
  pages = {4273--4280},
  issn = {2377-3766, 2377-3774},
  doi = {10.1109/LRA.2023.3323124},
  urldate = {2024-12-18},
  abstract = {Multi-object tracking (MOT) is a cornerstone capability of any robotic system. Tracking quality is largely dependent on the quality of input detections. In many applications, such as autonomous driving, it is preferable to over-detect objects to avoid catastrophic outcomes due to missed detections. As a result, current state-of-the-art 3D detectors produce high rates of false-positives to ensure a low number of false-negatives. This can negatively affect tracking by making data association and track lifecycle management more challenging. Additionally, occasional false-negative detections due to difficult scenarios like occlusions can harm tracking performance. To address these issues in a unified framework, we propose ShaSTA which learns shape and spatio-temporal affinities between tracks and detections in consecutive frames. The affinity is a probabilistic matching that leads to robust data association, track lifecycle management, false-positive elimination, false-negative propagation, and sequential track confidence refinement. We offer the first self-contained framework that addresses all aspects of the 3D MOT problem. We quantitatively evaluate ShaSTA on the nuScenes tracking benchmark with 5 metrics, including the most common tracking accuracy metric called AMOTA, to demonstrate how ShaSTA may impact the ultimate goal of an autonomous mobile agent. ShaSTA achieves 1st place amongst LiDAR-only trackers that use CenterPoint detections.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  file = {C:\Users\Administrator\Zotero\storage\ZC5SEEUN\Sadjadpour 等 - 2024 - ShaSTA Modeling Shape and Spatio-Temporal Affinities for 3D Multi-Object Tracking.pdf}
}

@misc{seguWalkerSelfsupervisedMultiple2024,
  title = {Walker: {{Self-supervised Multiple Object Tracking}} by {{Walking}} on {{Temporal Appearance Graphs}}},
  shorttitle = {Walker},
  author = {Segu, Mattia and Piccinelli, Luigi and Li, Siyuan and Gool, Luc Van and Yu, Fisher and Schiele, Bernt},
  year = {2024},
  month = sep,
  number = {arXiv:2409.17221},
  eprint = {2409.17221},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2409.17221},
  urldate = {2024-11-26},
  abstract = {The supervision of state-of-the-art multiple object tracking (MOT) methods requires enormous annotation efforts to provide bounding boxes for all frames of all videos, and instance IDs to associate them through time. To this end, we introduce Walker, the first self-supervised tracker that learns from videos with sparse bounding box annotations, and no tracking labels. First, we design a quasi-dense temporal object appearance graph, and propose a novel multi-positive contrastive objective to optimize random walks on the graph and learn instance similarities. Then, we introduce an algorithm to enforce mutually-exclusive connective properties across instances in the graph, optimizing the learned topology for MOT. At inference time, we propose to associate detected instances to tracklets based on the max-likelihood transition state under motion-constrained bi-directional walks. Walker is the first self-supervised tracker to achieve competitive performance on MOT17, DanceTrack, and BDD100K. Remarkably, our proposal outperforms the previous self-supervised trackers even when drastically reducing the annotation requirements by up to 400x.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\Administrator\\Zotero\\storage\\Z5SIS7MH\\Segu 等 - 2024 - Walker Self-supervised Multiple Object Tracking b.pdf;C\:\\Users\\Administrator\\Zotero\\storage\\VK2G8YER\\2409.html}
}

@misc{solovyevWeightedBoxesFusion2021,
  title = {Weighted Boxes Fusion: {{Ensembling}} Boxes from Different Object Detection Models},
  shorttitle = {Weighted Boxes Fusion},
  author = {Solovyev, Roman and Wang, Weimin and Gabruseva, Tatiana},
  year = {2021},
  month = feb,
  number = {arXiv:1910.13302},
  eprint = {1910.13302},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1910.13302},
  urldate = {2024-10-21},
  abstract = {In this work, we present a novel method for combining predictions of object detection models: weighted boxes fusion. Our algorithm utilizes confidence scores of all proposed bounding boxes to constructs the averaged boxes. We tested method on several datasets and evaluated it in the context of the Open Images and COCO Object Detection tracks, achieving top results in these challenges. The source code is publicly available at https://github.com/ZFTurbo/Weighted-Boxes-Fusion},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,WBF},
  file = {C\:\\Users\\Administrator\\Zotero\\storage\\GYZMFDTJ\\Solovyev 等 - 2021 - Weighted boxes fusion Ensembling boxes from diffe.pdf;C\:\\Users\\Administrator\\Zotero\\storage\\FMHFDRFQ\\1910.html}
}



@article{stanojevicBoostTrackBoostingSimilarity2024,
  title = {{{BoostTrack}}: Boosting the Similarity Measure and Detection Confidence for Improved Multiple Object Tracking},
  shorttitle = {{{BoostTrack}}},
  author = {Stanojevic, Vukasin D. and Todorovic, Branimir T.},
  year = {2024},
  month = apr,
  journal = {Machine Vision and Applications},
  volume = {35},
  number = {3},
  pages = {53},
  issn = {1432-1769},
  doi = {10.1007/s00138-024-01531-5},
  urldate = {2024-08-29},
  abstract = {Handling unreliable detections and avoiding identity switches are crucial for the success of multiple object tracking (MOT). Ideally, MOT algorithm should use true positive detections only, work in real-time and produce no identity switches. To approach the described ideal solution, we present the BoostTrack, a simple yet effective tracing-by-detection MOT method that utilizes several lightweight plug and play additions to improve MOT performance. We design a detection-tracklet confidence score and use it to scale the similarity measure and implicitly favour high detection confidence and high tracklet confidence pairs in one-stage association. To reduce the ambiguity arising from using intersection over union (IoU), we propose a novel Mahalanobis distance and shape similarity additions to boost the overall similarity measure. To utilize low-detection score bounding boxes in one-stage association, we propose to boost the confidence scores of two groups of detections: the detections we assume to correspond to the existing tracked object, and the detections we assume to correspond to a previously undetected object. The proposed additions are orthogonal to the existing approaches, and we combine them with interpolation and camera motion compensation to achieve results comparable to the standard benchmark solutions while retaining real-time execution speed. When combined with appearance similarity, our method outperforms all standard benchmark solutions on MOT17 and MOT20 datasets. It ranks first among online methods in HOTA metric in the MOT Challenge on MOT17 and MOT20 test sets. We make our code available at https://github.com/vukasin-stanojevic/BoostTrack.},
  langid = {english},
  keywords = {Artificial Intelligence,Data association,Detection confidence,Multi-object tracking,Similarity measure,Tracking-by-detection},
  file = {C:\Users\Administrator\Zotero\storage\D4LXF7AZ\Stanojevic 和 Todorovic - 2024 - BoostTrack boosting the similarity measure and de.pdf}
}

@article{stanojevicBoostTrackUsingTracklet2024,
  title={BoostTrack++: using tracklet information to detect more objects in multiple object tracking},
  author={Stanojevi{\'c}, Vuka{\v{s}}in and Todorovi{\'c}, Branimir},
  journal={arXiv preprint arXiv:2408.13003},
  year={2024}
}


@article{wangCAMOMOTCombinedAppearanceMotion2023,
  title = {{{CAMO-MOT}}: {{Combined Appearance-Motion Optimization}} for {{3D Multi-Object Tracking With Camera-LiDAR Fusion}}},
  shorttitle = {{{CAMO-MOT}}},
  author = {Wang, Li and Zhang, Xinyu and Qin, Wenyuan and Li, Xiaoyu and Gao, Jinghan and Yang, Lei and Li, Zhiwei and Li, Jun and Zhu, Lei and Wang, Hong and Liu, Huaping},
  year = {2023},
  month = nov,
  journal = {IEEE Transactions on Intelligent Transportation Systems},
  volume = {24},
  number = {11},
  pages = {11981--11996},
  issn = {1524-9050, 1558-0016},
  doi = {10.1109/TITS.2023.3285651},
  urldate = {2024-12-18},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  file = {C:\Users\Administrator\Zotero\storage\EW2LLNAH\Wang 等 - 2023 - CAMO-MOT Combined Appearance-Motion Optimization for 3D Multi-Object Tracking With Camera-LiDAR Fus.pdf}
}


@misc{weiNewArchitectureNeural2024,
  title = {A {{New Architecture}} for {{Neural Enhanced Multiobject Tracking}}},
  author = {Wei, Shaoxiu and Liang, Mingchao and Meyer, Florian},
  year = {2024},
  month = oct,
  number = {arXiv:2410.06294},
  eprint = {2410.06294},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.06294},
  urldate = {2024-11-26},
  abstract = {Multiobject tracking (MOT) is an important task in robotics, autonomous driving, and maritime surveillance. Traditional work on MOT is model-based and aims to establish algorithms in the framework of sequential Bayesian estimation. More recent methods are fully data-driven and rely on the training of neural networks. The two approaches have demonstrated advantages in certain scenarios. In particular, in problems where plenty of labeled data for the training of neural networks is available, data-driven MOT tends to have advantages compared to traditional methods. A natural thought is whether a general and efficient framework can integrate the two approaches. This paper advances a recently introduced hybrid model-based and data-driven method called neural-enhanced belief propagation (NEBP). Compared to existing work on NEBP for MOT, it introduces a novel neural architecture that can improve data association and new object initialization, two critical aspects of MOT. The proposed tracking method is leading the nuScenes LiDAR-only tracking challenge at the time of submission of this paper.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics,Electrical Engineering and Systems Science - Signal Processing},
  file = {C\:\\Users\\Administrator\\Zotero\\storage\\LR55THDT\\Wei 等 - 2024 - A New Architecture for Neural Enhanced Multiobject.pdf;C\:\\Users\\Administrator\\Zotero\\storage\\3XRSLCIL\\2410.html}
}

@misc{xiaoMambaTrackSimpleBaseline2024,
  title = {{{MambaTrack}}: {{A Simple Baseline}} for {{Multiple Object Tracking}} with {{State Space Model}}},
  shorttitle = {{{MambaTrack}}},
  author = {Xiao, Changcheng and Cao, Qiong and Luo, Zhigang and Lan, Long},
  year = {2024},
  month = aug,
  number = {arXiv:2408.09178},
  eprint = {2408.09178},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2408.09178},
  urldate = {2024-08-28},
  abstract = {Tracking by detection has been the prevailing paradigm in the field of Multi-object Tracking (MOT). These methods typically rely on the Kalman Filter to estimate the future locations of objects, assuming linear object motion. However, they fall short when tracking objects exhibiting nonlinear and diverse motion in scenarios like dancing and sports. In addition, there has been limited focus on utilizing learning-based motion predictors in MOT. To address these challenges, we resort to exploring data-driven motion prediction methods. Inspired by the great expectation of state space models (SSMs), such as Mamba, in long-term sequence modeling with near-linear complexity, we introduce a Mamba-based motion model named Mamba moTion Predictor (MTP). MTP is designed to model the complex motion patterns of objects like dancers and athletes. Specifically, MTP takes the spatial-temporal location dynamics of objects as input, captures the motion pattern using a bi-Mamba encoding layer, and predicts the next motion. In real-world scenarios, objects may be missed due to occlusion or motion blur, leading to premature termination of their trajectories. To tackle this challenge, we further expand the application of MTP. We employ it in an autoregressive way to compensate for missing observations by utilizing its own predictions as inputs, thereby contributing to more consistent trajectories. Our proposed tracker, MambaTrack, demonstrates advanced performance on benchmarks such as Dancetrack and SportsMOT, which are characterized by complex motion and severe occlusion.},
  archiveprefix = {arXiv},
  langid = {american},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annotation = {abstractTranslation: 通过检测进行跟踪一直是多目标跟踪 （MOT） 领域的主流范式。这些方法通常依靠卡尔曼滤波来估计物体的未来位置，假设物体呈线性运动。但是，在跟踪在舞蹈和运动等场景中表现出非线性和多样化运动的物体时，它们无法满足要求。此外，在 MOT 中使用基于学习的运动预测器的关注有限。为了应对这些挑战，我们探索数据驱动的运动预测方法。受到 Mamba 等状态空间模型 （SSM） 在具有近线性复杂性的长期序列建模中的极大期望的启发，我们引入了一种名为 Mamba moTion Predictor （MTP） 的基于 Manba 的运动模型。MTP 旨在对舞者和运动员等对象的复杂运动模式进行建模。具体来说，MTP 将物体的时空位置动态作为输入，使用双 Mamba 编码层捕获运动模式，并预测下一个运动。在实际场景中，对象可能会因遮挡或运动模糊而丢失，从而导致其轨迹过早终止。为了应对这一挑战，我们进一步扩展了 MTP 的应用。我们以自回归的方式使用它，通过使用它自己的预测作为输入来补偿缺失的观测值，从而有助于实现更一致的轨迹。我们提出的跟踪器 MambaTrack 在 Dancetrack 和 SportsMOT 等基准测试中展示了先进的性能，这些基准测试的特点是复杂的运动和严重的遮挡。},
  file = {C\:\\Users\\Administrator\\Zotero\\storage\\QFS6HT5F\\Xiao 等 - 2024 - MambaTrack A Simple Baseline for Multiple Object .pdf;C\:\\Users\\Administrator\\Zotero\\storage\\RHKWFERG\\2408.html}
}

@article{yatakaSIRAScalableInterframe,
  title = {{{SIRA}}: {{Scalable Inter-frame Relation}} and {{Association}} for {{Radar Perception}}},
  author = {Yataka, Ryoma and Wang, Pu and Boufounos, Petros and Takahashi, Ryuhei},
  abstract = {Conventional radar feature extraction faces limitations due to low spatial resolution, noise, multipath reflection, the presence of ghost targets, and motion blur. Such limitations can be exacerbated by nonlinear object motion, particularly from an ego-centric viewpoint. It becomes evident that to address these challenges, the key lies in exploiting temporal feature relation over an extended horizon and enforcing spatial motion consistency for effective association. To this end, this paper proposes SIRA (Scalable Inter-frame Relation and Association) with two designs. First, inspired by Swin Transformer, we introduce extended temporal relation, generalizing the existing temporal relation layer from two consecutive frames to multiple inter-frames with temporally regrouped window attention for scalability. Second, we propose motion consistency track with the concept of a pseudo-tracklet generated from observational data for better trajectory prediction and subsequent object association. Our approach achieves 58.11 mAP@0.5 for oriented object detection and 47.79 MOTA for multiple object tracking on the Radiate dataset, surpassing previous state-of-the-art by a margin of +4.11 mAP@0.5 and +9.94 MOTA, respectively.},
  langid = {english},
  file = {C:\Users\Administrator\Zotero\storage\4QP7Y64F\Yataka 等 - SIRA Scalable Inter-frame Relation and Associatio.pdf}
}


@InProceedings{yinCenterbased3DObject2021,
    author    = {Yin, Tianwei and Zhou, Xingyi and Krahenbuhl, Philipp},
    title     = {Center-Based 3D Object Detection and Tracking},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {11784-11793}
}

@misc{yinEnhancedMultiObjectTracking2024,
  title = {Enhanced {{Multi-Object Tracking Using Pose-based Virtual Markers}} in 3x3 {{Basketball}}},
  author = {Yin, Li and Yeung, Calvin and Hu, Qingrui and Ichikawa, Jun and Azechi, Hirotsugu and Takahashi, Susumu and Fujii, Keisuke},
  year = {2024},
  month = dec,
  number = {arXiv:2412.06258},
  eprint = {2412.06258},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.06258},
  urldate = {2024-12-10},
  abstract = {Multi-object tracking (MOT) is crucial for various multi-agent analyses such as evaluating team sports tactics and player movements and performance. While pedestrian tracking has advanced with Tracking-by-Detection MOT, team sports like basketball pose unique challenges. These challenges include players' unpredictable movements, frequent close interactions, and visual similarities that complicate pose labeling and lead to significant occlusions, frequent ID switches, and high manual annotation costs. To address these challenges, we propose a novel pose-based virtual marker (VM) MOT method for team sports, named Sports-vmTracking. This method builds on the vmTracking approach developed for multi-animal tracking with active learning. First, we constructed a 3x3 basketball pose dataset for VMs and applied active learning to enhance model performance in generating VMs. Then, we overlaid the VMs on video to identify players, extract their poses with unique IDs, and convert these into bounding boxes for comparison with automated MOT methods. Using our 3x3 basketball dataset, we demonstrated that our VM configuration has been highly effective, and reduced the need for manual corrections and labeling during pose model training while maintaining high accuracy. Our approach achieved an average HOTA score of 72.3\%, over 10 points higher than other state-of-the-art methods without VM, and resulted in 0 ID switches. Beyond improving performance in handling occlusions and minimizing ID switches, our framework could substantially increase the time and cost efficiency compared to traditional manual annotation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\Administrator\\Zotero\\storage\\FFPNTQGB\\Yin 等 - 2024 - Enhanced Multi-Object Tracking Using Pose-based Vi.pdf;C\:\\Users\\Administrator\\Zotero\\storage\\MFFZ3UDU\\2412.html}
}
@inproceedings{liPolyMOTPolyhedralFramework2023,
  title={Poly-mot: A polyhedral framework for 3d multi-object tracking},
  author={Li, Xiaoyu and Xie, Tao and Liu, Dedong and Gao, Jinghan and Dai, Kun and Jiang, Zhiqiang and Zhao, Lijun and Wang, Ke},
  booktitle={2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages={9391--9398},
  year={2023},
  organization={IEEE}
}

@inproceedings{pangSimpleTrackUnderstandingRethinking2021,
  title={Simpletrack: Understanding and rethinking 3d multi-object tracking},
  author={Pang, Ziqi and Li, Zhichao and Wang, Naiyan},
  booktitle={European Conference on Computer Vision},
  pages={680--696},
  year={2022},
  organization={Springer}
}

@article{240313443FastPolyFast,
  title={Fast-Poly: A Fast Polyhedral Algorithm For 3D Multi-Object Tracking},
  author={Li, Xiaoyu and Liu, Dedong and Wu, Yitao and Wu, Xian and Zhao, Lijun and Gao, Jinghan},
  journal={IEEE Robotics and Automation Letters},
  year={2024},
  publisher={IEEE}
}
@article{zaechLearnableOnlineGraph2022,
  title = {Learnable {{Online Graph Representations}} for {{3D Multi-Object Tracking}}},
  author = {Zaech, Jan-Nico and Liniger, Alexander and Dai, Dengxin and Danelljan, Martin and Van Gool, Luc},
  year = {2022},
  month = apr,
  journal = {IEEE Robotics and Automation Letters},
  volume = {7},
  number = {2},
  pages = {5103--5110},
  issn = {2377-3766, 2377-3774},
  doi = {10.1109/LRA.2022.3145952},
  urldate = {2024-12-21},
  abstract = {Autonomous systems that operate in dynamic environments require robust object tracking in 3D as one of their key components. Most recent approaches for 3D multi-object tracking (MOT) from LIDAR use object dynamics together with a set of handcrafted features to match detections of objects across multiple frames. However, manually designing such features and heuristics is cumbersome and often leads to suboptimal performance. In this work, we instead strive towards a unified and learning based approach to the 3D MOT problem. We design a graph structure to jointly process detection and track states in an online manner. To this end, we employ a Neural Message Passing network for data association that is fully trainable. Our approach provides a natural way for track initialization and handling of false positive detections, while significantly improving track stability. We demonstrate the merit of the proposed approach in the nuScenes tracking challenge 2021 with a state-of-the-art performance of 65.6\% AMOTA with 58\% fewer ID-switches, resulting in the best LIDAR only submission and an overall second place.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  file = {C:\Users\Administrator\Zotero\storage\R27RTZT6\Zaech 等 - 2022 - Learnable Online Graph Representations for 3D Multi-Object Tracking.pdf}
}

@InProceedings{chenLargeKernel3DScalingKernels2023,
    author    = {Chen, Yukang and Liu, Jianhui and Zhang, Xiangyu and Qi, Xiaojuan and Jia, Jiaya},
    title     = {LargeKernel3D: Scaling Up Kernels in 3D Sparse CNNs},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2023},
    pages     = {13488-13498}
}

@inproceedings{caesarNuScenesMultimodalDataset2020,
  title = {{{nuScenes}}: {{A Multimodal Dataset}} for {{Autonomous Driving}}},
  shorttitle = {{{nuScenes}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Caesar, Holger and Bankiti, Varun and Lang, Alex H. and Vora, Sourabh and Liong, Venice Erin and Xu, Qiang and Krishnan, Anush and Pan, Yu and Baldan, Giancarlo and Beijbom, Oscar},
  year = {2020},
  month = jun,
  pages = {11618--11628},
  publisher = {IEEE},
  address = {Seattle, WA, USA},
  doi = {10.1109/CVPR42600.2020.01164},
  urldate = {2024-12-21},
  abstract = {Robust detection and tracking of objects is crucial for the deployment of autonomous vehicle technology. Image based benchmark datasets have driven development in computer vision tasks such as object detection, tracking and segmentation of agents in the environment. Most autonomous vehicles, however, carry a combination of cameras and range sensors such as lidar and radar. As machine learning based methods for detection and tracking become more prevalent, there is a need to train and evaluate such methods on datasets containing range sensor data along with images. In this work we present nuTonomy scenes (nuScenes), the first dataset to carry the full autonomous vehicle sensor suite: 6 cameras, 5 radars and 1 lidar, all with full 360 degree field of view. nuScenes comprises 1000 scenes, each 20s long and fully annotated with 3D bounding boxes for 23 classes and 8 attributes. It has 7x as many annotations and 100x as many images as the pioneering KITTI dataset. We define novel 3D detection and tracking metrics. We also provide careful dataset analysis as well as baselines for lidar and image based detection and tracking. Data, development kit and more information are available online1.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  isbn = {978-1-72817-168-5},
  langid = {english},
  file = {C:\Users\Administrator\Zotero\storage\JPQQFIE9\Caesar 等 - 2020 - nuScenes A Multimodal Dataset for Autonomous Driving.pdf}
}


@INPROCEEDINGS{weng3DMultiObjectTracking2020,
  author={Weng, Xinshuo and Wang, Jianren and Held, David and Kitani, Kris},
  booktitle={2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
  title={3D Multi-Object Tracking: A Baseline and New Evaluation Metrics}, 
  year={2020},
  volume={},
  number={},
  pages={10359-10366},
  keywords={Measurement;Three-dimensional displays;Two dimensional displays;Tools;Solids;Real-time systems;State estimation},
  doi={10.1109/IROS45743.2020.9341164}}

@article{papaisSWTrackMultipleHypothesis2024,
  title={SWTrack: Multiple Hypothesis Sliding Window 3D Multi-Object Tracking},
  author={Papais, Sandro and Ren, Robert and Waslander, Steven},
  journal={arXiv preprint arXiv:2402.17892},
  year={2024}
}

@INPROCEEDINGS{yunitaErrorPerformanceAnalysis2020,
  author={Yunita, Meutia and Suryana, Joko and Izzuddin, Ahmad},
  booktitle={2020 6th International Conference on Wireless and Telematics (ICWT)}, 
  title={Error Performance Analysis of IMM-Kalman Filter for Maneuvering Target Tracking Application}, 
  year={2020},
  volume={},
  number={},
  pages={1-6},
  keywords={Analytical models;Target tracking;Surveillance;Simulation;Filtering algorithms;Radar tracking;Kalman filters;Kalman Filter;IMM Filter;Maneuvering Target;Target Tracking},
  doi={10.1109/ICWT50448.2020.9243662}}

@INPROCEEDINGS{wojkeSimpleOnlineRealtime2017,
  author={Wojke, Nicolai and Bewley, Alex and Paulus, Dietrich},
  booktitle={2017 IEEE International Conference on Image Processing (ICIP)}, 
  title={Simple online and realtime tracking with a deep association metric}, 
  year={2017},
  volume={},
  number={},
  pages={3645-3649},
  keywords={Kalman filters;Tracking;Extraterrestrial measurements;Standards;Uncertainty;Cameras;Computer Vision;Multiple Object Tracking;Data Association},
  doi={10.1109/ICIP.2017.8296962}}

@InProceedings{zhang2022bytetrack,
author="Zhang, Yifu
and Sun, Peize
and Jiang, Yi
and Yu, Dongdong
and Weng, Fucheng
and Yuan, Zehuan
and Luo, Ping
and Liu, Wenyu
and Wang, Xinggang",
editor="Avidan, Shai
and Brostow, Gabriel
and Ciss{\'e}, Moustapha
and Farinella, Giovanni Maria
and Hassner, Tal",
title="ByteTrack: Multi-object Tracking by Associating Every Detection Box",
booktitle="Computer Vision -- ECCV 2022",
year="2022",
publisher="Springer Nature Switzerland",
address="Cham",
pages="1--21",
abstract="Multi-object tracking (MOT) aims at estimating bounding boxes and identities of objects in videos. Most methods obtain identities by associating detection boxes whose scores are higher than a threshold. The objects with low detection scores, e.g. occluded objects, are simply thrown away, which brings non-negligible true object missing and fragmented trajectories. To solve this problem, we present a simple, effective and generic association method, tracking by associating almost every detection box instead of only the high score ones. For the low score detection boxes, we utilize their similarities with tracklets to recover true objects and filter out the background detections. When applied to 9 different state-of-the-art trackers, our method achieves consistent improvement on IDF1 score ranging from 1 to 10 points. To put forwards the state-of-the-art performance of MOT, we design a simple and strong tracker, named ByteTrack. For the first time, we achieve 80.3 MOTA, 77.3 IDF1 and 63.1 HOTA on the test set of MOT17 with 30 FPS running speed on a single V100 GPU. ByteTrack also achieves state-of-the-art performance on MOT20, HiEve and BDD100K tracking benchmarks. The source code, pre-trained models with deploy versions and tutorials of applying to other trackers are released at https://github.com/ifzhang/ByteTrack.",
isbn="978-3-031-20047-2"
}

@InProceedings{xiePolyPCPolyhedralNetwork2023,
    author    = {Xie, Tao and Wang, Shiguang and Wang, Ke and Yang, Linqi and Jiang, Zhiqiang and Zhang, Xingcheng and Dai, Kun and Li, Ruifeng and Cheng, Jian},
    title     = {Poly-PC: A Polyhedral Network for Multiple Point Cloud Tasks at Once},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2023},
    pages     = {1233-1243}
}

@online{IMM-MOT,
  title = {https://github.com/Ap01lo/IMM-MOT},
  url = {https://github.com/Ap01lo/IMM-MOT},
}