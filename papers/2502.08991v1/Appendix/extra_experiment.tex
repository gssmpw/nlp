\section{Extra experiments}\label{appendix:extra_exps}


\paragraph{Effect of Context Length.} The theory assumes access to an infinite number of examples for each training task but does not require infinite demonstrations during inference. However, in practice, we cannot train on an infinite number of examples. Figure \ref{fig:cl_effect} shows that providing sufficient context length during both training and inference is crucial for strong performance. Empirically, we observed that a context length of 40 works reasonably well across all experiments with dimensions up to $d = 20$.

\begin{figure}[h!] 
    \centering
    \includegraphics[width=0.3\textwidth]{Figures/effect_cl.png}
    \caption{The effect of context length on performance.}
    \label{fig:cl_effect}
\end{figure}

\paragraph{ ICL with no CoT fails in even in-distribution generalziation.} We observe in Figure \ref{fig:in_distribution_effect} that transformers with ICL and no CoT struggle to generalize even in simpler in-distribution settings as the number of tasks increases. In the parity task, we refer to in-distribution generalization as a setting where the model is trained on $\mathcal{F}_{train}$ tasks and $\mathcal{S}_{train}$ sequences, and then evaluated on the same set of tasks $\mathcal{F}_{train}$ but with entirely new sequences $\mathcal{S}_{test}$ that were not seen during training.

Here, the setting is the same as in \cite{bhattamishra2024understanding} for \( \text{Parity}(10,2) \), but we used the same tasks during both training and testing. We trained on half of the total sequences, $2^9$ and tested on unseen sequences while keeping the tasks unchanged.



\begin{figure}[h!] 
    \centering
    \includegraphics[width=0.5\textwidth]{Figures/in_distribution_accuracy.png}
    \caption{ICL without CoT even fails to generalize in distribution.}
    \label{fig:in_distribution_effect}
\end{figure}



