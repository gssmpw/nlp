
Each data point contains 4 in-context examples, where each example represents the same underlying meaning expressed in three different languages (selected from $\{$English, French, Spanish, Chinese$\}$). The model needs to infer the language triplets (denoted as $c = (c_1, c_2, c_3$) given the in-context examples and predict the translation to the first word provided in the query. We would view each language triplet as a different task and consider task generalization between them. We limit the number of meaning to $10$ so that even a small Transformer (4 layers and 3 heads with 0.7M parameters) learned from scratch can perform this task. Similarly to the parity setup, we will present different scenarios and invite readers to assess the potential results.


\begin{itemize}
    \item \textbf{Scenario 1: Missing a pair of languages in one position}
    In this scenario, we will choose a special pair of language, e.g. $($English, Chinese$)$ and consider two cases:
    \begin{enumerate}
        \item \textbf{Type 1.} The training set contains examples from all triplets $c$ that $(c_1, c_2) \neq ($English, Chinese$)$.
        \item \textbf{Type 2.} The training set contains examples from all triplets $c$ that $(c_0, c_1) \neq ($English, Chinese$)$.
    \end{enumerate}
    
    \item \textbf{Scenario 2: Missing one language in one output position} In this scenario, the training set contains examples from all triplets  $c$ that $c_1 \neq $ English.
    
    \item \textbf{Scenario 3: Missing two pairs of languages in the output} Differently from Scenario 1, we now exclude two non-overlap pairs of output languages,  $(c_1, c_2) \not \in \{($English, Chinese$),($French, Spanish$)\}$.

    \item \textbf{Scenario 4: Missing four pairs of languages in the output} We further exclude 4 pairs of output languauges,  $(c_1, c_2) \not \in \{($English, Chinese$), ($English, Spanish$), ($French, Chinese$), ($French, Spanish$)\}$.
\end{itemize}

\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        \textbf{Scenario} & \textbf{Type} & \textbf{Performance} \\ \hline
        1 & Type 1 & $97.2\%$ \\ \hline
        1 & Type 2 & $97.4\%$ \\ \hline
        2 &  & $0\%$\\ \hline
        3 &  & $96.4\%$ \\ \hline
        4 &  & $62.3\%$\\ \hline
    \end{tabular}
    \caption{Performance and type across scenarios.}
    \label{tab:scenario_performance_type}
\end{table}




\paragraph{Discussion of Results.} Given the same number of training samples (500k total samples), the model can generalize nearly perfectly ($\geq 97\%$ accuracy) out-of-distribution in both types in Scenario~1. This indicates that task composition generalization can occur beyond the parity task and aligns with the empirical observations in Scenario~1 of the parity case. Additionally, we observe that the model fails to generalize out-of-distribution in Scenario~2. It may then appear that the position-based explanation proposed in the discussion of the parity setting applies here: the Transformer can generalize out-of-distribution if every language appears in every position. This explanation is further supported by the out-of-domain generalization performance ($\approx 96\%$) in Scenario~3. However, once we exclude four pairs of languages, while still ensuring that all languages appear in every position, the performance drops to below~65\%. This then indicates the diversity of tasks still plays an important yet unexplained role here. 