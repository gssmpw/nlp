\section{Experiment Details}\label{appendix: experiment details}

\paragraph{Model and optimization.} We used the transformers library from Hugging Face \cite{wolf2020transformers} to instantiate and train our GPT-2 model from scratch. In all experiments, we used a 3-layer, 1-head configuration. We used the Wadam optimizer \cite{kingma2014adam} with a learning rate of $8 \times 10^{-5}$ and a batch size of 64.


\paragraph{Parity and arithmetic.} In all experiments shown in Figures \ref{fig:ood_generalization} and \ref{fig:arithmetic} for both parity and arithmetic tasks, we used a context length of 40. 

For the arithmetic problem, across all dimensions, we used a total of 25,000 training examples, equally distributed across the training tasks. 

For the parity problem, we used 20,000 training samples, equally distributed across the training tasks for dimensions up to 15. For dimension 20, we increased the total number of training samples to 50,000.


At testing time, we always randomly select the minimum between 200 subsets and all remaining tasks, each containing 500 different sequences with the same context length of 40.

\paragraph{Language experiments.} For the translation experiments, we train a 2-layer Transformer with 3 heads and embedding dimension 768. We use an Adam optimizer with betas being $0.9, 0.95$ and learning rate 3e-4. We will keep the number of total training samples to be $1e6$ and train for 1 pass for 6250 steps.  We choose the languages randomly from the following set $\{ English, French, Spanish, Chinese,
        German, Italian, Japanese, Russian,\\
        Portuguese, Arabic \}$ and meanings (in English) from $\{ cat, dog, house, apple, sky, car ,road\\, tree, bed, water, sun, moon\}$.  We use a GPT-2 tokenizer and in our demonstrations, we will prepend the language of the corresponding word before each word in the following format like ``English: cat''.


\paragraph{Linear Probing}


We append a linear classifier to the checkpoints of models of ``Increasing $D$ for a fixed $T$'' tasks, trained on the hidden states of the final attention layer when generating the $i$-th token in the Chain-of-Thought, with the goal of predicting the $i$-th "secret index." The models are trained on a total number of of $20,000$, $20,000$, and $50,000$ training samples for $d = 10$, $15$, and $20$, respectively. The tasks used for training and validation are disjoint. Only the linear classifier is trained, while the parameters of the transformer are frozen. We use the Adam optimizer with a learning rate of $4 \times 10^{-5}$, and the batch size is set to be $32$.