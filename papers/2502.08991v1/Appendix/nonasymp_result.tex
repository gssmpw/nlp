\section{Non-Asymptotic Analysis of Training-Time Demonstration Sample Complexity}
\label{appendix:nonasymp-result}

In Theorem~\ref{thm:exponential task generalization}, we established only an \emph{asymptotic} result, showing that as {the number of demonstration samples per task at training time }$n_x \to \infty$, the probability of correctly identifying subtask families $\fP_{\Theta_t}$  tends to one. However, by imposing an additional assumption on the total variation gap between the true distributions and any other hypotheses, it is possible to derive a \emph{non-asymptotic} guarantee on how large $n_x$ must be for accurate subtask identification.

Although maximum-likelihood estimation (MLE) does not directly yield such a non-asymptotic bound in this setting, we can use the same distribution discrimination approach introduced in the inference stage (Lemma~\ref{lem:dist_discrim}). 


\begin{assumption}[Compositional Identifiability with fixed tv marigin]\label{assm: compositional structure with gap}
The autoregressive task class $\mathcal{F}$ satisfies:
\begin{enumerate}
    \item \textbf{Finite Subtask Families}: For each $t \in [T]$, the hypothesis class $\mathcal{P}_{\Xi_t}$ is of size at most $H$ and the subtask conditional distribution family $\mathcal{P}_{\Theta_t} \subseteq \mathcal{P}_{\Xi_t}$ has size $|\mathcal{P}_{\Theta_t}| = \d$.

    \item \textbf{Task Identifiability}: For any $t \in [T]$,  $\theta_{1:t-1} \in \bigtimes_{s=1}^{t-1} \Theta_s$, and  $\theta_t \in \Theta_t$, $\zeta_t \in \Xi_t $, $P_{\zeta_t}\neq P_{\theta_t}$, the induced distributions stasify:
    \[
    \tv\left(P_{\theta_{1:t-1}, \theta_t}, P_{\theta_{1:t-1}, \zeta_t}\right) \geq r > 0.
    \]
    
    Furthermore, for any timestep $t \in [T]$,  $\theta_{1:t-1} \in \bigtimes_{s=1}^{t-1} \Theta_s$, and $\theta_t \neq \theta_t' \in \Theta_t$, the induced distributions satisfy:
    \[
    \tv\left(P_{\theta_{1:t-1},\theta_t}, P_{\theta_{1:t-1},\theta_t'}\right) \geq c > 0.
    \]

\end{enumerate}
\end{assumption}

\begin{theorem}[Exponential Task Generalization]\label{appendix thm:exponential task generalization}
Let $\mathcal{F}$ be an autoregressive compositional task class satisfying Assumption~\ref{assm: compositional structure}. Then there exists a learner $\mathcal{A}$ with the following property: if during training, one samples $n_{\theta}$ tasks uniformly and independently from $\mathcal{F}$, each provided with $n_x$ i.i.d.\ demonstration samples as the training dataset, and if at inference one observes $\ell$
i.i.d.\ demonstration samples from a previously unseen task $P_{\tilde{\theta}}\in\mathcal{F}$, then
\[
\Pr\Bigl[
  \mathcal{A}\bigl(\mathcal{D}_{\demo};\,\mathcal{D}_{\mathrm{train}}\bigr)
  \;\neq\;
  \bigl(P_{\tilde{\theta}_1}, \dots, P_{\tilde{\theta}_T}\bigr)
\Bigr]
\;\le\; \d Te^{-n_\theta/\d} + n_\theta T e^{-c^2\ell/2} + n_\theta THe^{-r^2 n_x/2}.
\]
where $\fD_\train$ and $\fD_\demo$ denote the training dataset and inference-time demonstration samples respectively, and the probability is taken over the random selection of training tasks 
$\mathcal{F}_{\mathrm{train}} \subseteq \mathcal{F}$, 
the training data $\mathcal{D}_{\mathrm{train}}$, 
and the inference time demonstration samples $\mathcal{D}_{\demo}$. 
\end{theorem}

\begin{proof}


Denote the hypothesis class $\fP_{\Xi_t} = \{P_{\xi_{t,1}},\cdots, P_{\xi_{t,|\Xi_t|}}\}$, we present the training stage of the learner.

\begin{algorithm}[H]
\caption{Training Stage with Distribution Dislimination}
\begin{algorithmic}[1]
\Require Training set $\mathcal{D}_{\mathrm{train}}=\{\mathcal{D}_i\}_{i=1}^{n_\theta}$
\For{$i = 1$ to $n_\theta$}
    \For{$t = 1$ to $T$}
      \State Initialize \(P_{\hat\theta_t^i} \gets P_{\xi_{t,1}}\). 
      \For{$k = 2$ to $|\Xi_t|$}
        \State Compute
          \[
          \phi 
          \;\leftarrow\; 
          \frac{1}{n_x}\,\sum_{(\bm x^{i,j},\bm y^{i,j})\in \fD_i}
          \;(-1)^{\,\mathbf{1}\bigl[
            P_{\hat\theta_{1:t-1},\,\hat{\theta}_t^i}({\bm x}^{i,j},{\bm y}^{i,j}_{1:t})
            \;<\;
            P_{\hat\theta_{1:t-1},\,\xi_{t,k}}({\bm x}^{i,j},{\bm y}^{i,j}_{1:t})
          \bigr]}\,.
          \]
        \If{\begin{align*}
          &\left|\,
            \phi 
            \;-\;
            \sum_{(\bm x,\bm y_{1:t})\in \fX\times \fY^t}
            P_{\hat\theta_{1:t-1},\,\xi_{t,k}}(\bm x,\bm y_{1:t})\;
            (-1)^{\,\mathbf{1}\bigl[
            P_{\hat\theta_{1:t-1},\,\hat{\theta}_t^i}({\bm x},{\bm y}_{1:t})
            \;<\;
            P_{\hat\theta_{1:t-1},\,\xi_{t,k}}({\bm x},{\bm y}_{1:t})
          \bigr]}
          \right|\\<&
          \left|\,
            \phi 
            \;-\;
            \sum_{(\bm x,\bm y_{1:t})\in \fX\times \fY^t} 
            P_{\hat\theta_{1:t-1},\,\hat{\theta}_t^i}(\bm x,\bm y_{1:t})\; 
            (-1)^{\,\mathbf{1}\bigl[
            P_{\hat\theta_{1:t-1},\,\hat{\theta}_t^i}({\bm x},{\bm y}_{1:t})
            \;<\;
            P_{\hat\theta_{1:t-1},\,\xi_{t,k}}({\bm x},{\bm y}_{1:t})
          \bigr]}
          \right|.
          \end{align*}}
          \State Update \( P_{\hat{\theta}_t^i} \gets P_{\xi_{t,k}}\).
        \EndIf
      \EndFor
    \EndFor
\EndFor
\State \textbf{return} $\fP_{\hat\Theta_t}=\{{P}_{\hat{\theta}_t^i}\}_{i=1}^{n_\theta}$ for each $t\in[T]$.
\end{algorithmic}
\end{algorithm}

Using the same approach as in Step 2 of the proof of ~\cref{thm:exponential task generalization}, 
\[
\Pr[(P_{\hat \theta^i_1},\cdots,P_{\hat \theta_T^i})\neq (P_{\theta^i_1},\cdots,P_{\theta_T^i})]\leq THe^{-{r^2 n_x}/2}.
\]
By union bound, 
\begin{align*}
\Pr[\exists ~t \in [T]:~ \fP_{\hat \Theta_t} \neq \fP_{\Theta_t }]\leq \Pr[~\exists (t,i):~ ~P_{\hat \theta^i_t}\neq P_{\theta^i_t}]\leq n_\theta T He^{-r^2 n_x/2}.
\end{align*}
The remainder of the proof then proceeds exactly as in ~\cref{thm:exponential task generalization}.

\end{proof}

