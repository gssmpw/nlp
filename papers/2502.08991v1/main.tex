\documentclass[a4paper,11pt]{article}

% --- Packages ---
\usepackage{amsmath, amssymb, amsthm}  % Math symbols and formatting
\usepackage{graphicx, subfigure, wrapfig}  % Images
\usepackage{hyperref}  % Clickable links
\usepackage{geometry}  % Page layout
\usepackage{titlesec}  % Custom section formatting
\usepackage{natbib}  % Bibliography support
\usepackage{lmodern}  % Better font rendering
\usepackage{microtype}  % Better typography
\usepackage{fancyhdr}  % Headers/footers
\usepackage{booktabs, multirow}  % Professional tables
\usepackage{algorithm, algpseudocode}  % Algorithms
\usepackage{xcolor}  % Colors
\usepackage{enumitem}  % Custom list formatting
\usepackage{cleveref}  % Smart cross-referencing
\usepackage{framed}  % Framing text
\usepackage{caption}  % Custom captions
\usepackage{mathabx}  % Provides \bigtimes
\usepackage{mathtools}  % Enhanced math notation
\usepackage{forloop}  % Looping in LaTeX
\usepackage{eso-pic}  % Background elements
\usepackage[normalem]{ulem}  % Better underlining (without modifying \emph)
\usepackage{bm}  % Bold math symbols

% Custom macros (Ensure these files exist)
\usepackage{utils/amir_macros}
\input{utils/math}

% Custom Commands
\renewcommand{\d}{D}
\newcommand{\TT}{T}  % Avoid potential conflicts with \T

% --- Theorems ---
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}

\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Colored comments
\newcommand\misha[1]{\textcolor{blue}{(MB: #1)}}
\newcommand\hz[1]{\textcolor{orange}{(HZ: #1)}}

% --- Customizing Section Titles ---
\titleformat{\section}{\large\bfseries}{\thesection.}{0.2em}{} 
\titleformat{\subsection}{\normalsize\bfseries}{\thesubsection.}{0.2em}{} 




% --- Title Formatting ---

% \makeatletter
% \renewcommand{\maketitle}{
%   \begin{flushleft} % Left-align title and author block
%     {\fontsize{16}{22} \bfseries \@title \par} % Title font size (18pt, line spacing 22pt)
%     \vskip 1em
%     {\fontsize{10}{14} \bfseries \@author \par} % Author font size (12pt, line spacing 14pt)
%     \vskip 0.5em
%     {\small \@date \par} % Date (if needed)
%   \end{flushleft}
%   \renewcommand{\thefootnote}{\fnsymbol{footnote}} % Symbol footnotes for equal contribution
%   \footnotetext[1]{Equal contribution.}
%   \renewcommand{\thefootnote}{\arabic{footnote}} % Reset footnote numbering
% }
% \makeatother

\makeatletter
\renewcommand{\maketitle}{
  \noindent % Ensure the content starts at the very left
  \hspace*{-3em} % Force a left shift
  \begin{minipage}{1.2\textwidth} % Extend width to allow content to fit
    \parbox{1.1\textwidth}{ % Adjust width (e.g., 1.1\textwidth)
      {\fontsize{15}{28} \bfseries \@title} % Title font size (14pt, line spacing 16pt)
    }
    \vskip 2em
    {  \fontsize{10}{16} \bfseries \@author \par} % Author font size (10pt, line spacing 14pt)
    \vskip 0.5em
    {\small \@date \par} % Date (if needed)
  \end{minipage}
  \renewcommand{\thefootnote}{\fnsymbol{footnote}} % Symbol footnotes for equal contribution
  \footnotetext[1]{Equal contribution.}
  \renewcommand{\thefootnote}{\arabic{footnote}} % Reset footnote numbering
}
\makeatother






\title{\textbf{Task Generalization With AutoRegressive Compositional Structure: \\ \strut 
\hspace{16mm}
Can Learning From $\d$ Tasks Generalize to $\d^{T}$ Tasks?}}

% \author{
%     \textbf{Amirhesam Abedsoltan}$^{1}$\footnotemark[1],  
%     \textbf{Huaqing Zhang}$^{3}$\footnotemark[1],  
%     \textbf{Kaiyue Wen}$^{4}$,  
%     \textbf{Hongzhou Lin}$^{5}$, 
%     \textbf{Jingzhao Zhang}$^{3}$,  
%     \textbf{Mikhail Belkin}$^{1,2}$
% }

% \date{}  % No date


\renewcommand{\thefootnote}{\fnsymbol{footnote}}


\author{
    \begin{minipage}{\textwidth}
        \centering
    Amirhesam Abedsoltan$^{1}$\footnotemark[1], 
    Huaqing Zhang$^{3}$\footnotemark[1], 
    Kaiyue Wen$^{4}$, 
    Hongzhou Lin$^{5}$,\\ 
    Jingzhao Zhang$^{3}$, 
    Mikhail Belkin$^{1,2}$
    \end{minipage}
}

\date{}  % Removes date

% --- Running Headers & Footers ---
\pagestyle{fancy}  
\fancyhf{}  
\fancyhead[C]{\small Task Generalization With AutoRegressive Compositional Structure}  
\fancyfoot[C]{\thepage}  
\renewcommand{\headrulewidth}{0.4pt}  
\renewcommand{\footrulewidth}{0pt}  

% Suppress header on first page
\fancypagestyle{plain}{
  \fancyhf{}  
  \fancyfoot[C]{\thepage}  
  \renewcommand{\headrulewidth}{0pt}  
}

\begin{document}

\newcommand{\amir}[1]{\textcolor{teal}{Amir:#1}}



\maketitle


% \footnotetext[1]{\textit{Equal contribution.}}
\footnotetext[1]{\textit{Department of Computer Science and Engineering, UC San Diego.}}
\footnotetext[2]{\textit{Halicioglu Data Science Institute, UC San Diego}}
\footnotetext[3]{\textit{Institute for Interdisciplinary Information Sciences, Tsinghua University.}}
\footnotetext[4]{\textit{Stanford University.}}
\footnotetext[5]{\textit{Amazon. This work is independent of and outside of the work at Amazon.}}



% \footnotetext[1]{Department of Computer Science and Engineering, UC San Diego}
% \footnotetext[2]{Halicioglu Data Science Institute, UC San Diego}
% \footnotetext[3]{Institute for Interdisciplinary Information Sciences, Tsinghua University}
% \footnotetext[4]{Stanford University}
% \footnotetext[5]{Amazon}


\thispagestyle{plain}  % Ensure the first page uses the plain style

% --- Abstract ---
\begin{abstract}
Large language models (LLMs) exhibit remarkable task generalization, solving tasks they were never explicitly trained on with only a few demonstrations. This raises a fundamental question: When can learning from a small set of tasks generalize to a large task family? In this paper, we investigate task generalization through the lens of AutoRegressive Compositional (ARC) structure, where each task is a composition of $T$ operations, and each operation is among a finite family of $\d$ subtasks. This yields a total class of size~\( \d^\TT \). We first show that generalization to all \( \d^\TT \) tasks is theoretically achievable by training on only \( \tilde{O}(\d) \) tasks. Empirically, we demonstrate that Transformers achieve such exponential task generalization on sparse parity functions via in-context learning (ICL) and Chain-of-Thought (CoT) reasoning. We further demonstrate this generalization in arithmetic and language translation, extending beyond parity functions.
\end{abstract}

% --- Main Sections ---
\input{sections/Intro_2}
\input{sections/Related}
\input{sections/theory}
\input{sections/Experiments}
\input{sections/Discussion}
\newpage
% --- Bibliography ---
\bibliography{ref}
\bibliographystyle{icml2025}

% --- Appendix ---
\newpage
\appendix
\onecolumn
\input{Appendix/proof}
\input{Appendix/nonasymp_result}
\input{Appendix/extra_experiment}
\input{Appendix/Experiments_details}

\end{document}