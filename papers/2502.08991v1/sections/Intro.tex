\section{Introduction}

Large language models (LLMs) demonstrate a remarkable ability to solve tasks they were never explicitly trained on, challenging conventional views in statistical learning theory. Unlike classical supervised learning, where generalization is limited by the training data distribution, LLMs can adapt to new tasks with just a few demonstrations—a phenomenon known as in-context learning (ICL) \cite{brown2020language, wei2022emergent, garg2022can}. Recent studies suggest that Transformers function as algorithmic learners \cite{li2023transformers, bai2023transformers}, effectively solving various statistical tasks—such as linear regression—and generalizing to unseen instances within the in-context learning framework.

 \begin{figure}[t]
    \centering
    \includegraphics[width=0.4\textwidth]{Figures/in_out_distribution_accuracy.png}
    \vspace{-2mm}
    \caption{We train a Transformer to learn parity functions through In-Context Learning (ICL): given a demonstration sequence \((\bm x_1, f(\bm x_1)), \dots, (\bm x_n, f(\bm x_n))\), infer the target \( f(\bm x_{\mathrm{query}}) \) from a new input $\bm x_{\mathrm{query}}$. Intuitively, each function \( f \) defines a distinct learning task. In this prototype experiment, tasks are sampled from the parity function family \(Parity (10,2)\) with secret length $k=2$ and bit length $d=10$, which consists of 45 tasks in total. To evaluate task generalization, we withhold a subset of tasks and train only on the remaining ones. Consistent with prior work \cite{bhattamishra2024understanding}, we observe
    % \amir{we did not find this, we replicate their experiment.} 
    that standard ICL fails to generalize across tasks. In contrast, incorporating Chain-of-Thought (CoT) reasoning significantly improves performance on unseen tasks.}
    \label{fig:in_out_dist_prelim}
    \vspace{-5mm}
\end{figure}


However, despite their success in tasks like linear regression and conjunctions, Transformers relying solely on in-context learning (ICL) struggle with more complex problems. One such example is the sparse parity function, where prior work \cite{bhattamishra2024understanding} shows that Transformers fail to generalize to unseen cases, even for the simplest parity function with a single XOR operator. This failure suggests that standard ICL struggles with tasks requiring hierarchical composition, consistent with empirical findings~\cite{an2023context, xu2024do}. Inspired by \cite{wen2024sparse, kim2025transformers}, we asked whether introducing a Chain-of-Thought (CoT) reasoning structure—designed to facilitate intermediate reasoning steps—can improve generalization. As shown in Figure~\ref{fig:in_out_dist_prelim}, we find that CoT significantly enhances performance on unseen tasks.







This result is particularly noteworthy, as prior theoretical studies on CoT have primarily focused on single-task settings, demonstrating improvements in expressiveness \cite{feng2023towards, li2024chain} or sample complexity \cite{wen2024sparse, kim2025transformers}, while leaving the question of generalization across tasks largely unexplored. Motivated by this example, we aim to systematically quantify how models can leverage autoregressive compositional structures to extend their capabilities beyond the training distribution. Specifically, we focus on \textbf{task generalization}: \textit{How can we quantify the number of tasks a model must be trained on in order to generalize to an entire function class?}




We follow~\cite{garg2022can} in defining the in-context learning (ICL) framework. A model is said to perform ICL over a task class \(\mathcal{F}\) if, for any task \( f_{\theta} \in \mathcal{F} \) and any data points \( \bm{x}_1, \bm{x}_2, \dots, \bm{x}_n, \bm{x}_{\mathrm{query}} \), it can approximate \( f_{\theta}(\bm{x}_{\mathrm{query}}) \) given the sequence of demonstrations  
\[
\vspace{-1mm}
(\bm{x}_1, f_{\theta}(\bm{x}_1), \dots, \bm{x}_n, f_{\theta}(\bm{x}_n), \bm{x}_{\mathrm{query}}).
\]
In other words, the model must learn an implicit algorithm to infer the query value from the provided demonstrations.  
% \amir{is this paragraph needed? used anywhere?} \hongzhou{we need to define \( \mathcal{F} \) and \( \mathcal{X} \) to properly explain \( \mathcal{F}_{\text{train}} \)}

To analyze task generalization, we consider a discrete function class  \(\mathcal{F}, \text{where } f_\theta \in \mathcal{F}.\) Moreover, we impose an autoregressive compositional structure on the functions in \(\mathcal{F}\). Specifically, each function \( f_{\theta} \in \mathcal{F} \) maps an input \( \bm{x} \in \mathcal{X} \)---a text or an image prompt---to a sequence \( \bm{y} = (y_1, \dots, y_T) \) of length \( T \), where each \( y_t \) is generated autoregressively as 
\[
% \vspace{-1mm}
y_t \sim P_{\theta_t}(\bm x, \bm{y}_{<t}), \quad t \in [1,T].
\]
Intuitively, this structure represents a compositional process where the final output \( y_T \) is obtained sequentially:
\[
\vspace{-1mm}
y_T \sim P_{\theta_T} \circ P_{\theta_{T-1}} \circ \cdots \circ P_{\theta_1}.
\]
This motivates the term \textbf{AutoRegressive Compositional structure}. Additionally, we assume that each \(\theta_t\) is chosen from a finite set with at most \( \d \) elements, resulting in a total of \( \d^T \) possible functions in \(\mathcal{F}\). The two key parameters governing the complexity of this function class are its \textbf{length~\( \mathbf{T} \)} and \textbf{breadth \( \mathbf{\d} \)}, which we denote as \(\mathbf{ARC(\d, T)}\).  







In conventional learning, models are trained on data sampled uniformly from the joint distribution \(\mathcal{F} \times \mathcal{X}\). In contrast, we focus on task generalization, where training is restricted to a subset \(\mathcal{F}_{\mathrm{train}} \subset \mathcal{F}\), leaving the remaining functions \(\mathcal{F}_{\mathrm{unseen}} \coloneqq \mathcal{F} \setminus \mathcal{F}_{\mathrm{train}}\) completely unseen. Our goal is to investigate whether a model trained on \(\mathcal{F}_{\mathrm{train}} \times \mathcal{X}\) can successfully generalize to \(\mathcal{F}_{\mathrm{unseen}} \times \mathcal{X}\). This notion of task generalization goes beyond standard out-of-distribution (OOD) settings, shifting the focus from adapting to new input distributions to mastering entirely new tasks.



This setup raises a fundamental question:  
\textit{How many tasks in \(\mathcal{F}_{\mathrm{train}}\) must a model be trained on to generalize to all tasks in \(\mathcal{F}\), including those it has never seen?} In particular, can a model be trained on as few as \( \tilde{O}(\d) \) tasks and still generalize across the entire set of \( \d^T \) tasks?


Our main contributions are:
\vspace{-3mm}
\begin{itemize}[leftmargin=0.4 cm]
    \item We define AutoRegressive Compositional structure and introduce a framework to quantitatively analyze task generalization when the function class follows an AutoRegressive Compositional structure. (Sections \ref{sec: ARC} and \ref{sec: task generalization})
    
    \item We establish that under this structure, task generalization to all \( \d^T \) tasks is theoretically achievable by training on  \( \tilde{O}(\d) \) tasks up to logarithmic terms. (\cref{sec: Exp Task Generalization})
    
    \item We demonstrate how the parity problem aligns with our framework and empirically show that Transformers trained on i.i.d. sampled tasks exhibit exponential task generalization via chain-of-thought (CoT) reasoning, consistent with theoretical scaling. (\cref{sec: Experiments})
    
    \item Finally, we show that the selection of training tasks significantly impacts generalization to unseen tasks. If tasks are chosen adversarially, training on \( O(\d^T) \) tasks with CoT may fail to generalize. (\cref{sec: Experiment beyond iid sampling})\vspace{-3mm}
\end{itemize}


