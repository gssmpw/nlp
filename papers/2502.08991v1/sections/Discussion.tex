\section{ Task Generalization Beyond i.i.d. Sampling and Parity Functions
}\label{sec:Discussion}
% Discussion: From Theory to Beyond
% \misha{what is beyond?}
% \amir{we mean two things: in the first subsection beyond i.i.d subsampling of parity tasks and in the second subsection beyond parity task}
% \misha{it has to be beyond something, otherwise it is not clear what it is about} \hz{this is suggested by GPT..., maybe can be interpreted as from theory to beyond theory. We can do explicit like Discussion: Beyond i.i.d. task sampling and the Parity Task}
% \misha{ why is "discussion" in the title?}\amir{Because it is a discussion, it is not like separate concrete explnation about why these thing happens or when they happen, they just discuss some interesting scenraios how it relates to our theory.   } \misha{it is not really a discussion -- there is a bunch of experiments}

In this section, we extend our experiments beyond i.i.d. task sampling and parity functions. We show an adversarial example where biased task selection substantially hinders task generalization for sparse parity problem. In addition, we demonstrate that exponential task scaling extends to a non-parity tasks including arithmetic and multi-step language translation.

% In this section, we extend our experiments beyond i.i.d. task sampling and parity functions. On the one hand, we find that biased task selection can significantly degrade task generalization; on the other hand, we show that exponential task scaling generalizes to broader scenarios.
% \misha{we should add a sentence or two giving more detail}


% 1. beyond i.i.d tasks sampling
% 2. beyond parity -> language, arithmetic -> task dependency + implicit bias of transformer (cannot implement this algorithm for arithmatic)



% In this section, we emphasize the challenge of quantifying the level of out-of-distribution (OOD) differences between training tasks and testing tasks, even for a simple parity task. To illustrate this, we present two scenarios where tasks differ between training and testing. For each scenario, we invite the reader to assess, before examining the experimental results, which cases might appear “more” OOD. All scenarios consider \( d = 10 \). \kaiyue{this sentence should be put into 5.1}






% for parity problem




% \begin{table*}[th!]
%     \centering
%     \caption{Generalization Results for Scenarios 1 and 2 for $d=10$.}
%     \begin{tabular}{|c|c|c|c|}
%         \hline
%         \textbf{Scenario} & \textbf{Type/Variation} & \textbf{Coordinates} & \textbf{Generalization accuracy} \\
%         \hline
%         \multirow{3}{*}{Generalization with Missing Pair} & Type 1 & \( c_1 = 4, c_2 = 6 \) & 47.8\%\\ 
%         & Type 2 & \( c_1 = 4, c_2 = 6 \) & 96.1\%\\ 
%         & Type 3 & \( c_1 = 4, c_2 = 6 \) & 99.5\%\\ 
%         \hline
%         \multirow{3}{*}{Generalization with Missing Pair} & Type 1 &  \( c_1 = 8, c_2 = 9 \) & 40.4\%\\ 
%         & Type 2 & \( c_1 = 8, c_2 = 9 \) & 84.6\% \\ 
%         & Type 3 & \( c_1 = 8, c_2 = 9 \) & 99.1\%\\ 
%         \hline
%         \multirow{1}{*}{Generalization with Missing Coordinate} & --- & \( c_1 = 5 \) & 45.6\% \\ 
%         \hline
%     \end{tabular}
%     \label{tab:generalization_results}
% \end{table*}

\subsection{Task Generalization Beyond i.i.d. Task Sampling }\label{sec: Experiment beyond iid sampling}

% \begin{table*}[ht!]
%     \centering
%     \caption{Generalization Results for Scenarios 1 and 2 for $d=10, k=3$.}
%     \begin{tabular}{|c|c|c|}
%         \hline
%         \textbf{Scenario}  & \textbf{Tasks excluded from training} & \textbf{Generalization accuracy} \\
%         \hline
%         \multirow{1}{*}{Generalization with Missing Pair}
%         & $\{4,6\} \subseteq \{s_1, s_2, s_3\}$ & 96.2\%\\ 
%         \hline
%         \multirow{1}{*}{Generalization with Missing Coordinate}
%         & \( s_2 = 5 \) & 45.6\% \\ 
%         \hline
%     \end{tabular}
%     \label{tab:generalization_results}
% \end{table*}




In previous sections, we focused on \textit{i.i.d. settings}, where the set of training tasks $\mathcal{F}_{train}$ were sampled uniformly at random from the entire class $\mathcal{F}$. Here, we explore scenarios that deliberately break this uniformity to examine the effect of task selection on out-of-distribution (OOD) generalization.\\

\textit{How does the selection of training tasks influence a model’s ability to generalize to unseen tasks? Can we predict which setups are more prone to failure?}\\

\noindent To investigate this, we consider two cases parity problems with \( d = 10 \) and \( k = 3 \), where each task is represented by its tuple of secret indices \( (s_1, s_2, s_3) \):

\begin{enumerate}[leftmargin=0.4 cm]
    \item \textbf{Generalization with a Missing Coordinate.} In this setup, we exclude all training tasks where the second coordinate takes the value \( s_2 = 5 \), such as \( (1,5,7) \). At test time, we evaluate whether the model can generalize to unseen tasks where \( s_2 = 5 \) appears.
    \item \textbf{Generalization with Missing Pair.} Here, we remove all training tasks that contain both \( 4 \) \textit{and} \( 6 \) in the tuple \( (s_1, s_2, s_3) \), such as \( (2,4,6) \) and \( (4,5,6) \). At test time, we assess whether the model can generalize to tasks where both \( 4 \) and \( 6 \) appear together.
\end{enumerate}

% \textbf{Before proceeding, consider the following question:} 
\noindent \textbf{If you had to guess.} Which scenario is more challenging for generalization to unseen tasks? We provide the experimental result in Table~\ref{tab:generalization_results}.

 % while the model struggles for one of them while as it generalizes almost perfectly in the other one. 

% in the first scenario, it generalizes almost perfectly in the second. This highlights how exposure to partial task structures can enhance generalization, even when certain combinations are entirely absent from the training set. 

In the first scenario, despite being trained on all tasks except those where \( s_2 = 5 \), which is of size $O(\d^T)$, the model struggles to generalize to these excluded cases, with prediction at chance level. This is intriguing as one may expect model to generalize across position. The failure  suggests that positional diversity plays a crucial role in the task generalization of Transformers. 

In contrast, in the second scenario, though the model has never seen tasks with both \( 4 \) \textit{and} \( 6 \) together, it has encountered individual instances where \( 4 \) appears in the second position (e.g., \( (1,4,5) \)) or where \( 6 \) appears in the third position (e.g., \( (2,3,6) \)). This exposure appears to facilitate generalization to test cases where both \( 4 \) \textit{and} \( 6 \) are present. 



\begin{table*}[t!]
    \centering
    \caption{Generalization Results for Scenarios 1 and 2 for $d=10, k=3$.}
    \resizebox{\textwidth}{!}{  % Scale to full width
        \begin{tabular}{|c|c|c|}
            \hline
            \textbf{Scenario}  & \textbf{Tasks excluded from training} & \textbf{Generalization accuracy} \\
            \hline
            Generalization with Missing Pair & $\{4,6\} \subseteq \{s_1, s_2, s_3\}$ & 96.2\%\\ 
            \hline
            Generalization with Missing Coordinate & \( s_2 = 5 \) & 45.6\% \\ 
            \hline
        \end{tabular}
    }
    \label{tab:generalization_results}
\end{table*}

As a result, when the training tasks are not i.i.d, an adversarial selection such as exclusion of specific positional configurations may lead to failure to unseen task generalization even though the size of $\mathcal{F}_{train}$ is exponentially large. 


% \paragraph{\textbf{Key Takeaways}}
% \begin{itemize}
%     \item Out-of-distribution generalization in the parity problem is highly sensitive to the diversity and positional coverage of training tasks.
%     \item Adversarial exclusion of specific pairs or positional configurations can lead to systematic failures, even when most tasks are observed during training.
% \end{itemize}




%################ previous veriosn down
% \textit{How does the choice of training tasks affect the ability of a model to generalize to unseen tasks? Can we predict which setups are likely to lead to failure?}

% To explore these questions, we crafted specific training and test task splits to investigate what makes one setup appear “more” OOD than another.

% \paragraph{Generalization with Missing Pair.}

% Imagine we have tasks constructed from subsets of \(k=3\) elements out of a larger set of \(d\) coordinates. What happens if certain pairs of coordinates are adversarially excluded during training? For example, suppose \(d=5\) and two specific coordinates, \(c_1 = 1\) and \(c_2 = 2\), are excluded. The remaining tasks are formed from subsets of the other coordinates. How would a model perform when tested on tasks involving the excluded pair \( (c_1, c_2) \)? 

% To probe this, we devised three variations in how training tasks are constructed:
%     \begin{enumerate}
%         \item \textbf{Type 1:} The training set includes all tasks except those containing both \( c_1 = 1 \) and \( c_2 = 2 \). 
%         For this example, the training set includes only $\{(3,4,5)\}$. The test set consists of all tasks containing the rest of tuples.

%         \item \textbf{Type 2:} Similar to Type 1, but the training set additionally includes half of the tasks containing either \( c_1 = 1 \) \textit{or} \( c_2 = 2 \) (but not both). 
%         For the example, the training set includes all tasks from Type 1 and adds tasks like \(\{(1, 3, 4), (2, 3, 5)\}\) (half of those containing \( c_1 = 1 \) or \( c_2 = 2 \)).

%         \item \textbf{Type 3:} Similar to Type 2, but the training set also includes half of the tasks containing both \( c_1 = 1 \) \textit{and} \( c_2 = 2 \). 
%         For the example, the training set includes all tasks from Type 2 and adds, for instance, \(\{(1, 2, 5)\}\) (half of the tasks containing both \( c_1 \) and \( c_2 \)).
%     \end{enumerate}

% By systematically increasing the diversity of training tasks in a controlled way, while ensuring no overlap between training and test configurations, we observe an improvement in OOD generalization. 

% % \textit{However, the question is this improvement similar across all coordinate pairs, or does it depend on the specific choices of \(c_1\) and \(c_2\) in the tasks?} 

% \textbf{Before proceeding, consider the following question:} Is the observed improvement consistent across all coordinate pairs, or does it depend on the specific choices of \(c_1\) and \(c_2\) in the tasks? 

% For instance, consider two cases for \(d = 10, k = 3\): (i) \(c_1 = 4, c_2 = 6\) and (ii) \(c_1 = 8, c_2 = 9\). Would you expect similar OOD generalization behavior for these two cases across the three training setups we discussed?



% \paragraph{Answer to the Question.} for both cases of \( c_1, c_2 \), we observe that generalization fails in Type 1, suggesting that the position of the tasks the model has been trained on significantly impacts its generalization capability. For Type 2, we find that \( c_1 = 4, c_2 = 6 \) performs significantly better than \( c_1 = 8, c_2 = 9 \). 

% Upon examining the tasks where the transformer fails for \( c_1 = 8, c_2 = 9 \), we see that the model only fails at tasks of the form \((*, 8, 9)\) while perfectly generalizing to the rest. This indicates that the model has never encountered the value \( 8 \) in the second position during training, which likely explains its failure to generalize. In contrast, for \( c_1 = 4, c_2 = 6 \), while the model has not seen tasks of the form \((*, 4, 6)\), it has encountered tasks where \( 4 \) appears in the second position, such as \((1, 4, 5)\), and tasks where \( 6 \) appears in the third position, such as \((2, 3, 6)\). This difference may explain why the model generalizes almost perfectly in Type 2 for \( c_1 = 4, c_2 = 6 \), but not for \( c_1 = 8, c_2 = 9 \).



% \paragraph{Generalization with Missing Coordinates.}
% Next, we investigate whether a model can generalize to tasks where a specific coordinate appears in an unseen position during training. For instance, consider \( c_1 = 5 \), and exclude all tasks where \( c_1 \) appears in the second position. Despite being trained on all other tasks, the model fails to generalize to these excluded cases, highlighting the importance of positional diversity in training tasks.



% \paragraph{Key Takeaways.}
% \begin{itemize}
%     \item OOD generalization depends heavily on the diversity and positional coverage of training tasks for the parity problem.
%     \item adversarial exclusion of specific pairs or positional configurations in the parity problem can lead to failure, even when the majority of tasks are observed during training.
% \end{itemize}


%################ previous veriosn up

% \paragraph{Key Takeaways} These findings highlight the complexity of OOD generalization, even in seemingly simple tasks like parity. They also underscore the importance of task design: the diversity of training tasks can significantly influence a model’s ability to generalize to unseen tasks. By better understanding these dynamics, we can design more robust training regimes that foster generalization across a wider range of scenarios.


% #############


% Upon examining the tasks where the transformer fails for \( c_1 = 8, c_2 = 9 \), we see that the model only fails at tasks of the form \((*, 8, 9)\) while perfectly generalizing to the rest. This indicates that the model has never encountered the value \( 8 \) in the second position during training, which likely explains its failure to generalize. In contrast, for \( c_1 = 4, c_2 = 6 \), while the model has not seen tasks of the form \((*, 4, 6)\), it has encountered tasks where \( 4 \) appears in the second position, such as \((1, 4, 5)\), and tasks where \( 6 \) appears in the third position, such as \((2, 3, 6)\). This difference may explain why the model generalizes almost perfectly in Type 2 for \( c_1 = 4, c_2 = 6 \), but not for \( c_1 = 8, c_2 = 9 \).

% we observe a striking pattern: generalization fails entirely in Type 1, regardless of the coordinate pair (\(c_1, c_2\)). However, in Type 2, generalization varies: \(c_1 = 4, c_2 = 6\) achieves 96\% accuracy, while \(c_1 = 8, c_2 = 9\) lags behind at 70\%. Why? Upon closer inspection, the model struggles specifically with tasks like \((*, 8, 9)\), where the combination \(c_1 = 8\) and \(c_2 = 9\) is entirely novel. In contrast, for \(c_1 = 4, c_2 = 6\), the model benefits from having seen tasks where \(4\) appears in the second position or \(6\) in the third. This suggests that positional exposure during training plays a key role in generalization.

% To test whether task structure influences generalization, we consider two variations:
% \begin{enumerate}
%     \item \textbf{Sorted Tuples:} Tasks are always sorted in ascending order.
%     \item \textbf{Unsorted Tuples:} Tasks can appear in any order.
% \end{enumerate}

% If the model struggles with generalizing to the excluded position, does introducing variability through unsorted tuples help mitigate this limitation?

% \paragraph{Discussion of Results}

% In \textbf{Generalization with Missing Pairs}, we observe a striking pattern: generalization fails entirely in Type 1, regardless of the coordinate pair (\(c_1, c_2\)). However, in Type 2, generalization varies: \(c_1 = 4, c_2 = 6\) achieves 96\% accuracy, while \(c_1 = 8, c_2 = 9\) lags behind at 70\%. Why? Upon closer inspection, the model struggles specifically with tasks like \((*, 8, 9)\), where the combination \(c_1 = 8\) and \(c_2 = 9\) is entirely novel. In contrast, for \(c_1 = 4, c_2 = 6\), the model benefits from having seen tasks where \(4\) appears in the second position or \(6\) in the third. This suggests that positional exposure during training plays a key role in generalization.

% In \textbf{Generalization with Missing Coordinates}, the results confirm this hypothesis. When \(c_1 = 5\) is excluded from the second position during training, the model fails to generalize to such tasks in the sorted case. However, allowing unsorted tuples introduces positional diversity, leading to near-perfect generalization. This raises an intriguing question: does the model inherently overfit to positional patterns, and can task variability help break this tendency?




% In this subsection, we show that the selection of training tasks can affect the quality of the unseen task generalization significantly in practice. To illustrate this, we present two scenarios where tasks differ between training and testing. For each scenario, we invite the reader to assess, before examining the experimental results, which cases might appear “more” OOD. 

% % \amir{add examples, }

% \kaiyue{I think the name of scenarios here are not very clear}
% \begin{itemize}
%     \item \textbf{Scenario 1:  Generalization Across Excluded Coordinate Pairs (\( k = 3 \))} \\
%     In this scenario, we select two coordinates \( c_1 \) and \( c_2 \) out of \( d \) and construct three types of training sets. 

%     Suppose \( d = 5 \), \( c_1 = 1 \), and \( c_2 = 2 \). The tuples are all possible subsets of \( \{1, 2, 3, 4, 5\} \) with \( k = 3 \):
%     \[
%     \begin{aligned}
%     \big\{ & (1, 2, 3), (1, 2, 4), (1, 2, 5), (1, 3, 4), (1, 3, 5), \\
%            & (1, 4, 5), (2, 3, 4), (2, 3, 5), (2, 4, 5), (3, 4, 5) \big\}.
%     \end{aligned}
%     \]

%     \begin{enumerate}
%         \item \textbf{Type 1:} The training set includes all tuples except those containing both \( c_1 = 1 \) and \( c_2 = 2 \). 
%         For this example, the training set includes only $\{(3,4,5)\}$ tuple. The test set consists of tuples containing the rest of tuples.

%         \item \textbf{Type 2:} Similar to Type 1, but the training set additionally includes half of the tuples containing either \( c_1 = 1 \) \textit{or} \( c_2 = 2 \) (but not both). 
%         For the example, the training set includes all tuples from Type 1 and adds tuples like \(\{(1, 3, 4), (2, 3, 5)\}\) (half of those containing \( c_1 = 1 \) or \( c_2 = 2 \)).

%         \item \textbf{Type 3:} Similar to Type 2, but the training set also includes half of the tuples containing both \( c_1 = 1 \) \textit{and} \( c_2 = 2 \). 
%         For the example, the training set includes all tuples from Type 2 and adds, for instance, \(\{(1, 2, 5)\}\) (half of the tuples containing both \( c_1 \) and \( c_2 \)).
%     \end{enumerate}

% % \begin{itemize}
% %     \item \textbf{Type 1:} The training set includes tuples \(\{1, 3, 4\}, \{2, 3, 4\}\) (excluding tuples with both \( c_1 \) and \( c_2 \): \(\{1, 2, 3\}, \{1, 2, 4\}\)). The test set contains the excluded tuples.
% %     \item \textbf{Type 2:} The training set includes all tuples in Type 1 plus half of the tuples containing either \( c_1 = 1 \) or \( c_2 = 2 \) (e.g., \(\{1, 2, 3\}\)).
% %     \item \textbf{Type 3:} The training set includes all tuples in Type 2 plus half of the tuples containing both \( c_1 = 1 \) and \( c_2 = 2 \) (e.g., \(\{1, 2, 4\}\)).
% % \end{itemize}
    
%     \item \textbf{Scenario 2: Scenario 2: Generalization Across a Fixed Coordinate (\( k = 3 \))} \\
%     In this scenario, we select one coordinate \( c_1 \) out of \( d \) (\( c_1 = 5 \)). The training set includes all task tuples except those where \( c_1 \) is the second coordinate of the tuple. For this scenario, we examine two variations:
%     \begin{enumerate}
%         \item \textbf{Sorted Tuples:} Task tuples are always sorted (e.g., \( (x_1, x_2, x_3) \) with \( x_1 \leq x_2 \leq x_3 \)).
%         \item \textbf{Unsorted Tuples:} Task tuples can appear in any order.
%     \end{enumerate}
% \end{itemize}




% \paragraph{Discussion of Results.} In the first scenario, for both cases of \( c_1, c_2 \), we observe that generalization fails in Type 1, suggesting that the position of the tasks the model has been trained on significantly impacts its generalization capability. For Type 2, we find that \( c_1 = 4, c_2 = 6 \) performs significantly better than \( c_1 = 8, c_2 = 9 \). 

% Upon examining the tasks where the transformer fails for \( c_1 = 8, c_2 = 9 \), we see that the model only fails at tasks of the form \((*, 8, 9)\) while perfectly generalizing to the rest. This indicates that the model has never encountered the value \( 8 \) in the second position during training, which likely explains its failure to generalize. In contrast, for \( c_1 = 4, c_2 = 6 \), while the model has not seen tasks of the form \((*, 4, 6)\), it has encountered tasks where \( 4 \) appears in the second position, such as \((1, 4, 5)\), and tasks where \( 6 \) appears in the third position, such as \((2, 3, 6)\). This difference may explain why the model generalizes almost perfectly in Type 2 for \( c_1 = 4, c_2 = 6 \), but not for \( c_1 = 8, c_2 = 9 \).

% This position-based explanation appears compelling, so in the second scenario, we focus on a single position to investigate further. Here, we find that the transformer fails to generalize to tasks where \( 5 \) appears in the second position, provided it has never seen any such tasks during training. However, when we allow for more task diversity in the unsorted case, the model achieves near-perfect generalization. 

% This raises an important question: does the transformer have a tendency to overfit to positional patterns, and does introducing more task variability, as in the unsorted case, prevent this overfitting and enable generalization to unseen positional configurations?

% These findings highlight that even in a simple task like parity, it is remarkably challenging to understand and quantify the sources and levels of OOD behavior. This motivates further investigation into the nuances of task design and its impact on model generalization.


\subsection{Task Generalization Beyond Parity Problems}

% \begin{figure}[t!]
%     \centering
%     \includegraphics[width=0.45\textwidth]{Figures/arithmetic_v1.png}
%     \vspace{-0.3cm}
%     \caption{Task generalization for arithmetic task with CoT, it has $\d =2$ and $T = d-1$ as the ambient dimension, hence $D\ln(DT) = 2\ln(2T)$. We show that the empirical scaling closely follows the theoretical scaling.}
%     \label{fig:arithmetic}
% \end{figure}



% \begin{wrapfigure}{r}{0.4\textwidth}  % 'r' for right, 'l' for left
%     \centering
%     \includegraphics[width=0.4\textwidth]{Figures/arithmetic_v1.png}
%     \vspace{-0.3cm}
%     \caption{Task generalization for the arithmetic task with CoT. It has $d =2$ and $T = d-1$ as the ambient dimension, hence $D\ln(DT) = 2\ln(2T)$. We show that the empirical scaling closely follows the theoretical scaling.}
%     \label{fig:arithmetic}
% \end{wrapfigure}

\subsubsection{Arithmetic Task}\label{subsec:arithmetic}











We introduce the family of \textit{Arithmetic} task that, like the sparse parity problem, operates on 
\( d \) binary inputs \( b_1, b_2, \dots, b_d \). The task involves computing a structured arithmetic expression over these inputs using a sequence of addition and multiplication operations.
\newcommand{\op}{\textrm{op}}

Formally, we define the function:
\[
\text{Arithmetic}_{S} \colon \{0,1\}^d \to \{0,1,\dots,d\},
\]
where \( S = (\op_1, \op_2, \dots, \op_{d-1}) \) is a sequence of \( d-1 \) operations, each \( \op_k \) chosen from \( \{+, \times\} \). The function evaluates the expression by applying the operations sequentially from left-to-right order: for example, if \( S = (+, \times, +) \), then the arithmetic function would compute:
\[
\text{Arithmetic}_{S}(b_1, b_2, b_3, b_4) = ((b_1 + b_2) \times b_3) + b_4.
\]
% Thus, the sequence of operations \( S \) defines how the binary inputs are combined to produce an integer output between \( 0 \) and \( d \).
% \[
% \text{Arithmetic}_{S} 
% (b_1,\,b_2,\,\dots,b_d)
% =
% \Bigl(\dots\bigl(\,(b_1 \;\op_1\; b_2)\;\op_2\; b_3\bigr)\,\dots\Bigr) 
% \;\op_{d-1}\; b_d.
% \]
% We now introduce an \emph{Arithmetic} task that, like the sparse parity problem, operates on $d$ binary inputs $b_1, b_2, \dots, b_d$. Specifically, we define an arithmetic function
% \[
% \text{Arithmetic}_{S}\colon \{0,1\}^d \;\to\; \{0,1,\dots,d\},
% \]
% where $S = (i_1, i_2, \dots, i_{d-1})$ is a sequence of $d-1$ operations, each $i_k \in \{+,\,\times\}$. The value of $\text{Arithmetic}_{S}$ is obtained by applying the prescribed addition and multiplication operations in order, namely:
% \[
% \text{Arithmetic}_{S}(b_1,\,b_2,\,\dots,b_d)
% \;=\;
% \Bigl(\dots\bigl(\,(b_1 \;i_1\; b_2)\;i_2\; b_3\bigr)\,\dots\Bigr) 
% \;i_{d-1}\; b_d.
% \]

% This is an example of our framework where $T = d-1$ and $|\Theta_t| = 2$ with total $2^d$ possible tasks. 




By introducing a step-by-step CoT, arithmetic class belongs to $ARC(2, d-1)$: this is because at every step, there is only $\d = |\Theta_t| = 2$ choices (either $+$ or $\times$) while the length is  $T = d-1$, resulting a total number of $2^{d-1}$ tasks. 


\begin{minipage}{0.5\textwidth}  % Left: Text
    Task generalization for the arithmetic task with CoT. It has $d =2$ and $T = d-1$ as the ambient dimension, hence $D\ln(DT) = 2\ln(2T)$. We show that the empirical scaling closely follows the theoretical scaling.
\end{minipage}
\hfill
\begin{minipage}{0.4\textwidth}  % Right: Image
    \centering
    \includegraphics[width=\textwidth]{Figures/arithmetic_v1.png}
    \refstepcounter{figure}  % Manually advances the figure counter
    \label{fig:arithmetic}  % Now this label correctly refers to the figure
\end{minipage}

Notably, when scaling with \( T \), we observe in the figure above that the task scaling closely follow the theoretical $O(D\log(DT))$ dependency. Given that the function class grows exponentially as \( 2^T \), it is truly remarkable that training on only a few hundred tasks enables generalization to an exponentially larger space—on the order of \( 2^{25} > 33 \) Million tasks. This exponential scaling highlights the efficiency of structured learning, where a modest number of training examples can yield vast generalization capability.





% Our theory suggests that only $\Tilde{O}(\ln(T))$ i.i.d training tasks is enough to generalize to the rest of unseen tasks. However, we show in Figure \ref{fig:arithmetic} that transformer is not able to match  that. The transformer out-of distribution generalization behavior is not consistent across different dimensions when we scale the number of training tasks with $\ln(T)$. \hongzhou{implicit bias, optimization, etc}
 






% \subsection{Task generalization Beyond parity problem}

% \subsection{Arithmetic} In this setting, we still use the set-up we introduced in \ref{subsec:parity_exmaple}, the input is still a set of $d$ binary variable, $b_1, b_2,\dots,b_d$ and ${Arithmatic_{S}}:\{0,1\}\rightarrow \{0, 1, \dots, d\}$, where $S = (i_1,i_2,\dots,i_{d-1})$ is a tuple of size $d-1$ where each coordinate is either add($+
% $) or multiplication ($\times$). The function is as following,

% \begin{align*}
%     Arithmatic_{S}(b_1, b_2,\dots,b_d) = (\dots(b1(i1)b2)(i3)b3\dots)(i{d-1})
% \end{align*}
    


\subsubsection{Multi-Step Language Translation Task}

 \begin{figure*}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{Figures/combined_plot_horiz.png}
    \vspace{-0.2cm}
    \caption{Task generalization for language translation task: $\d$ is the number of languages and $T$ is the length of steps.}
    \vspace{-2mm}
    \label{fig:language}
\end{figure*}
% \vspace{-2mm}

In this task, we study a sequential translation process across multiple languages~\cite{garg2022can}. Given a set of \( D \) languages, we construct a translation chain by randomly sampling a sequence of \( T \) languages \textbf{with replacement}:  \(L_1, L_2, \dots, L_T,\)
where each \( L_t \) is a sampled language. Starting with a word, we iteratively translate it through the sequence:
\vspace{-2mm}
\[
L_1 \to L_2 \to L_3 \to \dots \to L_T.
\]
For example, if the sampled sequence is EN → FR → DE → FR, translating the word "butterfly" follows:
\vspace{-1mm}
\[
\text{butterfly} \to \text{papillon} \to \text{schmetterling} \to \text{papillon}.
\]
This task follows an \textit{AutoRegressive Compositional} structure by itself, specifically \( ARC(D, T-1) \), where at each step, the conditional generation only depends on the target language, making \( D \) as the number of languages and the total number of possible tasks is \( D^{T-1} \). This example illustrates that autoregressive compositional structures naturally arise in real-world languages, even without explicit CoT. 

We examine task scaling along \( D \) (number of languages) and \( T \) (sequence length). As shown in Figure~\ref{fig:language}, empirical  \( D \)-scaling closely follows the theoretical \( O(D \ln D T) \). However, in the \( T \)-scaling case, we observe a linear dependency on \( T \) rather than the logarithmic dependency \(O(\ln T) \). A possible explanation is error accumulation across sequential steps—longer sequences require higher precision in intermediate steps to maintain accuracy. This contrasts with our theoretical analysis, which focuses on asymptotic scaling and does not explicitly account for compounding errors in finite-sample settings.

% We examine task scaling along \( D \) (number of languages) and \( T \) (sequence length). As shown in Figure~\ref{fig:language}, empirical scaling closely follows the theoretical \( O(D \ln D T) \) trend, with slight exceptions at $ T=10 \text{ and } 3$ in Panel B. One possible explanation for this deviation could be error accumulation across sequential steps—longer sequences require each intermediate translation to be approximated with higher precision to maintain test accuracy. This contrasts with our theoretical analysis, which primarily focuses on asymptotic scaling and does not explicitly account for compounding errors in finite-sample settings.

Despite this, the task scaling is still remarkable — training on a few hundred tasks enables generalization to   $4^{10} \approx 10^6$ tasks!






% , this case, we are in a regime where \( D \ll T \), we observe  that the task complexity empirically scales as \( T \log T \) rather than \( D \log T \). 


% the model generalizes to an exponentially larger space of \( 2^T \) unseen tasks. In case $T=25$, this is $2^{25} > 33$ Million tasks. This remarkable exponential generalization demonstrates the power of structured task composition in enabling efficient generalization.


% In the case of parity tasks, introducing CoT effectively decomposes the problem from \( ARC(D^T, 1) \) to \( ARC(D, T) \), significantly improving task generalization.

% Again, in the regime scaling $T$, we again observe a $T\log T$ dependency. Knowing that the function class is scaling as $D^T$, it is remarkable that training on a few hundreds tasks can generalize to $4^{10} \approx 1M$ tasks. 





% We further performed a preliminary investigation on a semi-synthetic word-level translation task to show that (1) task generalization via composition structure is feasible beyond parity and (2) understanding the fine-grained mechanism leading to this generalization is still challenging. 
% \noindent
% \noindent
% \begin{minipage}[t]{\columnwidth}
%     \centering
%     \textbf{\scriptsize In-context examples:}
%     \[
%     \begin{array}{rl}
%         \textbf{Input} & \hspace{1.5em} \textbf{Output} \\
%         \hline
%         \textcolor{blue}{car}   & \hspace{1.5em} \textcolor{red}{voiture \;,\; coche} \\
%         \textcolor{blue}{house} & \hspace{1.5em} \textcolor{red}{maison \;,\; casa} \\
%         \textcolor{blue}{dog}   & \hspace{1.5em} \textcolor{red}{chien \;,\; perro} 
%     \end{array}
%     \]
%     \textbf{\scriptsize Query:}
%     \[
%     \begin{array}{rl}
%         \textbf{Input} & \textbf{Output} \\
%         \hline
%         \textcolor{blue}{cat} & \hspace{1.5em} \textcolor{red}{?} \\
%     \end{array}
%     \]
% \end{minipage}



% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=0.45\textwidth]{Figures/translation_scale_d.png}
%     \vspace{-0.2cm}
%     \caption{Task generalization behavior for word translation task.}
%     \label{fig:arithmetic}
% \end{figure}


\vspace{-1mm}
\section{Conclusions}
% \misha{is it conclusion of the section or of the whole paper?}    
% \amir{The whole paper. It is very short, do we need a separate section?}
% \misha{it should not be a subsection if it is the conclusion the whole thing. We can just remove it , it does not look informative} \hz{let's do it in a whole section, just to conclude and end the paper, even though it is not informative}
%     \kaiyue{Proposal: Talk about the implication of this result on theory development. For example, it calls for more fine-grained theoretical study in this space.  }

% \huaqing{Please feel free to edit it if you have better wording or suggestions.}

% In this work, we propose a theoretical framework to quantitatively investigate task generalization with compositional autoregressive tasks. We show that task to $D^T$ task is theoretically achievable by training on only $O (D\log DT)$ tasks, and empirically observe that transformers trained on parity problem indeed achieves such task generalization. However, for other tasks beyond parity, transformers seem to fail to achieve this bond. This calls for more fine-grained theoretical study the phenomenon of task generalization specific to transformer model. It may also be interesting to study task generalization beyond the setting of in-context learning. 
% \misha{what does this add?} \amir{It does not, i dont have any particular opinion to keep it. @Hongzhou if you want to add here?}\hz{While it may not introduce anything new, we are following a good practice to have a short conclusion. It provides a clear closing statement, reinforces key takeaways, and helps the reader leave with a well-framed understanding of our contributions. }
% In this work, we quantitatively investigate task generalization under autoregressive compositional structure. We demonstrate that task generalization to $D^T$ tasks is theoretically achievable by training on only $\tilde O(D)$ tasks. Empirically, we observe that transformers trained indeed achieve such exponential task generalization on problems such as parity, arithmetic and multi-step language translation. We believe our analysis opens up a new angle to understand the remarkable generalization ability of Transformer in practice. 

% However, for tasks beyond the parity problem, transformers appear to fail to reach this bound. This highlights the need for a more fine-grained theoretical exploration of task generalization, especially for transformer models. Additionally, it may be valuable to investigate task generalization beyond the scope of in-context learning.



In this work, we quantitatively investigated task generalization under the autoregressive compositional structure, demonstrating both theoretically and empirically that exponential task generalization to $D^T$ tasks can be achieved with training on only $\tilde{O}(D)$ tasks. %Our theoretical results establish a fundamental scaling law for task generalization, while our experiments validate these insights across problems such as parity, arithmetic, and multi-step language translation. The remarkable ability of transformers to generalize exponentially highlights the power of structured learning and provides a new perspective on how large language models extend their capabilities beyond seen tasks. 
We recap our key contributions  as follows:
\begin{itemize}
    \item \textbf{Theoretical Framework for Task Generalization.} We introduced the \emph{AutoRegressive Compositional} (ARC) framework to model structured task learning, demonstrating that a model trained on only $\tilde{O}(D)$ tasks can generalize to an exponentially large space of $D^T$ tasks.
    
    \item \textbf{Formal Sample Complexity Bound.} We established a fundamental scaling law that quantifies the number of tasks required for generalization, proving that exponential generalization is theoretically achievable with only a logarithmic increase in training samples.
    
    \item \textbf{Empirical Validation on Parity Functions.} We showed that Transformers struggle with standard in-context learning (ICL) on parity tasks but achieve exponential generalization when Chain-of-Thought (CoT) reasoning is introduced. Our results provide the first empirical demonstration of structured learning enabling efficient generalization in this setting.
    
    \item \textbf{Scaling Laws in Arithmetic and Language Translation.} Extending beyond parity functions, we demonstrated that the same compositional principles hold for arithmetic operations and multi-step language translation, confirming that structured learning significantly reduces the task complexity required for generalization.
    
    \item \textbf{Impact of Training Task Selection.} We analyzed how different task selection strategies affect generalization, showing that adversarially chosen training tasks can hinder generalization, while diverse training distributions promote robust learning across unseen tasks.
\end{itemize}



We introduce a framework for studying the role of compositionality in learning tasks and how this structure can significantly enhance generalization to unseen tasks. Additionally, we provide empirical evidence on learning tasks, such as the parity problem, demonstrating that transformers follow the scaling behavior predicted by our compositionality-based theory. Future research will  explore how these principles extend to real-world applications such as program synthesis, mathematical reasoning, and decision-making tasks. 


By establishing a principled framework for task generalization, our work advances the understanding of how models can learn efficiently beyond supervised training and adapt to new task distributions. We hope these insights will inspire further research into the mechanisms underlying task generalization and compositional generalization.

\section*{Acknowledgements}
We acknowledge support from the National Science Foundation (NSF) and the Simons Foundation for the Collaboration on the Theoretical Foundations of Deep Learning through awards DMS-2031883 and \#814639 as well as the  TILOS institute (NSF CCF-2112665) and the Office of Naval Research (ONR N000142412631). 
This work used the programs (1) XSEDE (Extreme science and engineering discovery environment)  which is supported by NSF grant numbers ACI-1548562, and (2) ACCESS (Advanced cyberinfrastructure coordination ecosystem: services \& support) which is supported by NSF grants numbers \#2138259, \#2138286, \#2138307, \#2137603, and \#2138296. Specifically, we used the resources from SDSC Expanse GPU compute nodes, and NCSA Delta system, via allocations TG-CIS220009. 
