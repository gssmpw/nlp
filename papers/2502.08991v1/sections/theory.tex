\newcommand{\train}{\mathrm{train}}
\newcommand{\demo}{\mathrm{infer}}
\newcommand{\parity}{\mathrm{parity}}
\newcommand{\tv}{{\mathrm{TV}}}
\newcommand{\iid}{\overset{\text{i.i.d.}}{\sim} }
\newcommand{\xor}{\mathrm{xor}}
\section{Theoretical Framework for Task Generalization}\label{sec:theory}




In this section, we present a theoretical framework to study task generalization with autoregressive compositional structure. When the compositional structure holds, we show that there exists a learning algorithm that is only trained on $\tilde O(\d)$ different tasks, but can generalize to exponentially many unseen tasks. 


\subsection{Preliminaries and Notations}\label{sec: theory formulation}

% \paragraph{Preliminaries} 
For a positive integer $n$, denote $[n]=\{1,2,\cdots,n\}$. For a finte set $\fS$, we denote by $\Delta(\fS)$ the probability simplex over with support $\fS$. Given \( t \) sets \(\mathcal{S}_1, \mathcal{S}_2, \dots, \mathcal{S}_t\), their \textbf{Cartesian product} is defined as
\[
\bigtimes_{i=1}^t \mathcal{S}_i \coloneqq \left\{ (s_1, s_2, \dots, s_t) \mid s_i \in \mathcal{S}_i \ \text{for all} \ i \in [t]\right\}.
\]
We further denote $\mathcal{S}^{t} \coloneqq \bigtimes_{i=1}^t \mathcal{S}$. For two probability distributions \( P \) and \( Q \) over a discrete space \(\mathcal{S}\), the \textbf{total variation distance} is defined as
\[
\tv(P, Q) \coloneqq \frac{1}{2} \sum_{s \in \mathcal{S}} \left| P(s) - Q(s) \right|.
\]

% \paragraph{Notations.} 
We let the bold letter $ \bm{y}$ denote a sequence, and the subscripted $\bm{y}^j$ denote the $j_{th}$ sequence / example. Within each sequence $\bm{y}^j = (y_1, ..., y_T)$, the regular letter $y_t$ denote the $t_{th}$ token in the sequence. 




\subsection{AutoRegressive Compositional Structure }\label{sec: ARC}



In the following definition, we formally introduce the \textbf{AutoRegressive Compositional (ARC)} task class, which models structured sequence generation through a composition of conditional distributions.

% [AutoRegressive Compositional Task Class]
\begin{definition}\label{def:ar_composition}
\textit{(AutoRegressive Compositional task class).} Let $\mathcal{X}$ and $\mathcal{Y}$ denote the finite input and output spaces, respectively. The AutoRegressive Compositional (ARC) task class consists of sequential generation processes:
\[
\mathcal{F} \coloneqq \left\{ f_\theta = (P_{\theta_1}, \dots, P_{\theta_T}) \mid P_{\theta_t} \in \mathcal{P}_{\Theta_t} \text{ for all } t \in [T] \right\},
\]
where each task \( f_\theta \in \mathcal{F} \) for any input $\bm{x} \in \mathcal{X}$ generates an output sequence \( \bm{y} = (y_1, \dots, y_T)\in \mathcal{Y} \) through an autoregressive sampling process:
\[
y_t \sim P_{\theta_t}(\cdot \mid \bm{x}, \bm{y}_{<t}), \quad \text{for all } t \in [T].
\]
At each step \( t \), the conditional probability distribution \( P_{\theta_t} \) is drawn from a subtask family \( \mathcal{P}_{\Theta_t} \), parametrized by \( \theta_t \):
\[
\mathcal{P}_{\Theta_t} \coloneqq \left\{ P_{\theta_t}(\cdot \mid \bm{x}, \bm{y}_{<t}) : \mathcal{X} \times \mathcal{Y}^{t-1} \to \Delta(\mathcal{Y}) \mid \theta_t \in \Theta_t \right\}.
\]
Here, \( \Theta_t \) represents the parameter space at step \( t \), and the overall task parameter space is \( \Theta \coloneqq \bigtimes_{t=1}^T \Theta_t \). Assuming each step has a finite number of possible subtasks, i.e., \( |\Theta_t| = d \) for all \( t \in [T] \), the AutoRegressive Compositional task class \( ARC(d, T) \) consists of \( |\mathcal{F}| = |\Theta| = d^T \) tasks.
\end{definition}




Given any input $\bm{x} \in \mathcal{X}$ and a sequence $\bm{y} \in \mathcal{Y}^T$, the joint distribution for a task $f_\theta=(P_{\theta_1},\cdots,P_{\theta_T}) \in \mathcal{F}$ is:
\begin{align}\label{eq:joint dist}
P_\theta(\bm{x}, \bm{y}) = P(\bm{x}) \prod_{s=1}^T P_{\theta_s}(y_s \mid \bm{x}, \bm y_{<s}), 
\end{align}
and for partial sequences up to any $t \in [T]$:
\begin{equation}
P_{\theta_{1:t}}(\bm{x}, \bm y_{1:t}) = P_x(\bm{x}) \prod_{s=1}^t P_{\theta_s}(y_s \mid \bm{x}, \bm y_{<s}).
\end{equation}


At a high level, an AutoRegressive Compositional task class \( ARC(\d, T) \) is characterized by two key properties:
\vspace{-2mm}
\begin{itemize}[leftmargin=0.4 cm]
    \item \textbf{Modularity.} The generation process is decomposed into \( T \) sequential steps, each governed by an independent conditional distribution \( P_{\theta_t} \in \mathcal{P}_{\Theta_t} \). This modular structure allows tasks to be constructed by combining different components at each step.
    
    \item \textbf{Exponential Growth.} The task class size grows exponentially in $T$ as $|\mathcal{F}| = \d^T$, despite each step having only \(\d\) choices. This reflects the combinatorial nature of task construction, where  variations at each step lead to an exponentially large set of possible tasks. 
\end{itemize}



\subsection{Task Generalization}\label{sec: task generalization}



Under the autoregressive task learning setup, there are two levels of generalization:
\vspace{-2mm}
\begin{enumerate}
    \item Generalizing to unseen inputs within a task.
    \item Generalizing to unseen tasks in the class $\mathcal{F}$.
\end{enumerate}
\vspace{-2mm}
We focus on the latter one, referred as \textbf{task generalization}.

\vspace{-2mm}
\paragraph{Training Phase} During training, the model can only access to a small subset of tasks 
$\mathcal{F}_{\mathrm{train}} = \{f_{\theta^1}, \dots, f_{\theta^{n_\theta}}\} \subseteq \mathcal{F}$ with $n_\theta = |\mathcal{F}_{\mathrm{train}}|$. For each task $f_{\theta^i} \in \mathcal{F}_{\mathrm{train}}$, we observe $n_x$ i.i.d. {demonstration samples}:
\[
\mathcal{D}_i = \left\{(\bm x^{i,j}, \bm y^{i,j})\right\}_{j=1}^{n_x} \iid P_{\theta^i}(\bm{x}, \bm{y}),
\]
where $P_{\theta^i}$ is defined as in~\cref{eq:joint dist}. The full training dataset is the union of ${\mathcal{D}_i}$ denoted by $\mathcal{D}_{\mathrm{train}} = \{\mathcal{D}_i\}_{i=1}^{n_\theta}$.


We assume the learner does not know the true subtask conditional distribution families $\{\mathcal{P}_{\Theta_t}\}_{t=1}^T$ a priori. Instead, it accesses to a \textbf{larger hypothesis class}:
\[
\mathcal{P}_{\Xi_t} \coloneqq \left\{ P_{\zeta_t}(\cdot \mid \bm{x}, \bm y_{<t}) \ \big| \ \zeta_t \in \Xi_t \right\} \supseteq \mathcal{P}_{\Theta_t},
\]
where $\Xi_t$ parameterizes the learnerâ€™s model class at step $t$. The goal of training is to \textit{identify the true subtask families} $\mathcal{P}_{\Theta_t}$ from $\mathcal{P}_{\Xi_t}$ through $\mathcal{D}_{\mathrm{train}}$.

\vspace{-2mm}
\paragraph{Inference Phase}
At test time, the learner is given $\ell$ inference-time demonstration samples:
\[
\mathcal{D}_{\demo} = \{(\tilde {\bm{x}}^i, \tilde{\bm{y}}^i)\}_{i=1}^\ell \iid P_{\tilde{\theta}}({\bm{x}}, {\bm{y}}),
\]
where $f_{\tilde{\theta}} \in \mathcal{F}$ is an unseen task. The learner must identify the true conditionals $\{P_{\tilde{\theta}_t}\}_{t=1}^T$ from $\mathcal{P}_{\Theta_t}$ for each step $t$. Formally, the learner $\fA$ that is trained on $\fD_\train$ and given $\fD_\demo$ as input, produces an output sequence of conditional distributions:
\[
\mathcal{A}\left(\mathcal{D}_{\demo}; \mathcal{D}_{\mathrm{train}}\right) \in \{(P_{\xi_1},\cdots,P_{\xi_T})~|~P_{\xi_t}\in \fP_{\Xi_t}, t\in [T]\}.
\]




\subsection{Main Result: Exponential Task Generalization}\label{sec: Exp Task Generalization}


We now establish our main theoretical result: with the compositional structure in Definition~\ref{def:ar_composition}, a learner can achieve exponential task generalization with only $\tilde{O}(\d)$ training tasks. This demonstrates how compositional structure fundamentally reduces the sample complexity of task learning from exponential to polynomial in $\d$. Our results hold under the following  mild assumptions:




\begin{assumption}[Compositional Identifiability]\label{assm: compositional structure}
The autoregressive task class $\mathcal{F}$ satisfies:
\begin{enumerate}[leftmargin=0.4 cm]
    \item \textbf{Finite Subtask Families.} For each $t \in [T]$, the hypothesis class $\mathcal{P}_{\Xi_t}$ is finite and the subtask conditional distribution family $\mathcal{P}_{\Theta_t} \subseteq \mathcal{P}_{\Xi_t}$ has size $|\mathcal{P}_{\Theta_t}| = \d$.


    \item \textbf{Task Identifiability.} For any $t \in [T]$,  $\theta_{1:t-1} \in \bigtimes_{s=1}^{t-1} \Theta_s$, and  $\theta_t \in \Theta_t$, $\zeta_t \in \Xi_t $, $P_{\zeta_t}\neq P_{\theta_t}$, the induced distributions stasify:
    \[
    \tv\left(P_{\theta_{1:t-1}, \theta_t}, P_{\theta_{1:t-1}, \zeta_t}\right) > 0.
    \vspace{-2mm}
    \]
    Furthermore, for any $t \in [T]$,  $\theta_{1:t-1} \in \bigtimes_{s=1}^{t-1} \Theta_s$, and $\theta_t \neq \theta_t' \in \Theta_t$, the induced distributions satisfy:
    \[
    \tv\left(P_{\theta_{1:t-1},\theta_t}, P_{\theta_{1:t-1},\theta_t'}\right) \geq c > 0.
        \vspace{-2mm}
    \]

\end{enumerate}
\end{assumption}



Under these conditions, we establish our main theorem:


\begin{theorem}[\textbf{Exponential Task Generalization}]\label{thm:exponential task generalization}
Let $\mathcal{F}$ be an AutoRegressive Compositional(ARC) task class satisfying Assumption~\ref{assm: compositional structure}. Then there exists a learner $\mathcal{A}$ with the following property: if during training, one samples $n_{\theta} \ge \d \ln\bigl(100\,\d\,T\bigr)$ tasks uniformly and independently from $\mathcal{F}$, each provided with $n_x$ i.i.d.\  demonstration samples as the training dataset, and if at inference one observes $\ell \;\ge\; \frac{2\,\ln\bigl(100\,T\,n_{\theta}\bigr)}{c^2}$
i.i.d.\ demonstration samples from a previously unseen task $P_{\tilde{\theta}}\in\mathcal{F}$, then
\[
\lim_{n_x \to \infty} 
\Pr\Bigl[
  \mathcal{A}\bigl(\mathcal{D}_{\demo};\,\mathcal{D}_{\mathrm{train}}\bigr)
  \;\neq\;
  \bigl(P_{\tilde{\theta}_1}, \dots, P_{\tilde{\theta}_T}\bigr)
\Bigr]
\;\le\; 0.02,
\]
where $\fD_\train$ and $\fD_\demo$ denote the training dataset and inference-time demonstration samples respectively, and the probability is taken over the random selection of training tasks 
$\mathcal{F}_{\mathrm{train}} \subseteq \mathcal{F}$, 
the training data $\mathcal{D}_{\mathrm{train}}$, 
and the inference-time demonstration samples $\mathcal{D}_{\demo}$. 
\end{theorem}


In other words, Theorem~\ref{thm:exponential task generalization} shows that the learner can generalize from only $\tilde{O}(\d)$ tasks  to an exponentially large number of unseen tasks, on the order of $\d^T$. The learning algorithm $\fA$ operates in two stage. In the training stage, it applies a maximum-likelihood estimation (MLE) procedure to $\fD_i$ in order to identify the subtasks of the $i$-th training task $(P_{\theta^i_1},\cdots,P_{\theta^i_T})$. In the inference stage, it then uses a total-variation-based distribution discrimination test (\cref{lem:dist_discrim}) on inference-time demonstration samples $\fD_\demo$ to recover $f_{\tilde \theta} = (P_{\tilde \theta_1},\cdots,P_{\tilde \theta_T})$. The proof is deferred to \cref{appendix:maintheorem}.






\begin{remark}
If we additionally assume that every subtask distribution is separated from any incorrect hypothesis by a fixed total-variation margin, i.e.\ for all 
$t \in [T]$, 
$\theta_{1:t-1} \in \bigtimes_{s=1}^{t-1}\Theta_s$, and 
$\theta_t \in \Theta_t$, $\zeta_t \in \Xi_t$ with $P_{\zeta_t}\neq P_{\theta_t}$,
\[
\tv\!\bigl(
  P_{\theta_{1:t-1}, \,\theta_t},\, 
  P_{\theta_{1:t-1}, \,\zeta_t}
\bigr) 
~\ge~ 
r ~>~ 0,
\]
then one can replace the MLE procedure used in the training stage with the same distribution-discrimination approach from the inference stage (\cref{lem:dist_discrim}). Under this condition, we can derive a \textit{non-asymptotic} bound on the \(n_x\) needed per task for accurate identification. See \cref{appendix:nonasymp-result} for details.
\end{remark}





\subsection{Example: Sparse Parity Problem}\label{sec: example: parity}\label{subsec:parity_exmaple}



To illustrate the role of the AutoRegressive Compositional structure, we use sparse parity problem as an example.
\paragraph{Sparse Parity Problem.} Given \( d \) binary variables \( \bm x = (b_1, b_2, \dots, b_d)  \), a sparse parity function selects \( k \) secret indices \( S = \{ i_1, i_2, \dots, i_k\} \) and outputs \( 1 \) if the sum of the corresponding variables is odd, and \( 0 \) otherwise:
\[
\parity_S(b_1, b_2, \dots, b_d) = b_{i_1} \oplus b_{i_2} \oplus \dots \oplus b_{i_k},
\]
where \( \oplus \) denotes the XOR (exclusive OR) operation. We define \( Parity(d, k) \) as the set of all parity functions with \( d \) variables and \( k \) secret indices, yielding a total of  
\[
|Parity(d,k)| = \binom{d}{k} = O(d^k).
\vspace{-3mm}
\]



\paragraph{Representation Matters.}  

Without Chain-of-Thought (CoT), the sparse parity problem \( Parity(d, k) \) is of $ARC(\binom{d}{k}, 1)$, but with CoT, it becomes an autoregressive compositional structure of $ARC(d,k)$. 

% \vspace{-3mm}
\begin{itemize}
    \item No CoT $\rightarrow$ $ARC\left(\binom{d}{k}, 1\right)$.
    \item With CoT $\rightarrow$ $ARC(d, k)$.
\end{itemize}
% \vspace{-3mm}

\begin{figure}[t] 
    \centering
    \includegraphics[width=0.45\textwidth]{Figures/cot_no_last.png}
    % \caption{The effect of context length on performance.}
    \label{fig:cot}
\end{figure}

Indeed, without CoT, the model maps input \( \bm x = (b_1, \dots, b_d) \) directly to output \( y = b_{i_1} \oplus b_{i_2} \oplus \dots \oplus b_{i_k} \) in a single step, hence we have length $T =1$ and the breadth $\d = |Parity(d,k)| = O(d^k)$. Such exponential dependency on the breadth suggests that one would need to train on $O(d^k)$ tasks, explaining what we are observing unsuccessful task generalization in the introduction figure.

In contrast, with CoT~\citep{abbe2024far, wen2024sparse}, the parity computation is decomposed into small steps:  
\[
\bm y = (b_{i_1}, b_{i_1} \oplus b_{i_2}, \dots, b_{i_1} \oplus \cdots \oplus b_{i_k}),
\]
enabling a structured representation that reduces the breadth. More precisely, at step $t$, the class of subtask is exactly defined by the XOR operation with previous token and one secret index : 
\[
P_{(t,i_t)}(y_t|\bm x,\bm y_{<t}) =  \mathbf 1 [y_t=  y_{t-1} \oplus b_{i_t}].
\]

where $i_t$ is the $t$-th secret index, by default lying  $\in [1,d]$. Therefore, the breadth $\d$ at each step is exactly $d$.


This said, the CoT effectively reduces the breadth from $O(d^k)$ to $d$. According to Theorem~\ref{thm:exponential task generalization}, 




\begin{corollary}\label{corollary:parity}
For the sparse parity problem described above, we can show that the parameter $c$ in Assumption \ref{assm: compositional structure} is $\frac 12$, thus when $n_\theta\geq d\ln (100kd), \ell \geq 8\ln (100kn_\theta)$ , it holds that
\begin{align*}
\lim_{n_x \to \infty} \Pr\big[ \mathcal{A}\left(\fD_\demo;\fD_\train \right) \neq (P_{(1,i_1)},\cdots,P_{(k,i_k)}) \big]& \leq 0.02.
\end{align*}
\end{corollary}

In other words, the family of $Parity(d, k)$ with CoT is learnable with ${O}(d \log (d))$ training tasks.


