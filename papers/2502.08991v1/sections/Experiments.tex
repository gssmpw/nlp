\section{Experiments: Parity Problem Case Study}\label{sec: Experiments}





As we have shown, there exists a learning algorithm that is only trained on $\tilde O(d)$ different tasks to fully generalize on all the tasks in $Parity(d, k)$. However, from a practical standpoint, it is not clear whether a \textit{Transformer} can actually match this task complexity. In this section, we present empirical evidence demonstrating that a standard Transformer can indeed learn the sparse parity function with CoT using $\tilde{O}(d)$ training tasks.




\subsection{Experimental Setup: In-Context Learning }

Our empirical setup is a refinement of the theoretical framework presented in ~\cref{sec: task generalization}, and closely follows that of \citep{garg2022can, bhattamishra2024understanding}. In this setup, a
sequence model $M$ (such as Transformers) is trained using $N$ sequences, each sequence consisting of
$m$ demonstration samples $(\bm{x}_1, \bm{y}_1, \dots, \bm{x}_m, \bm{y}_m)$. The model is trained for the next token-prediction task,
except that we only consider $y_j$ in loss optimization: for each context $(\bm{x}_1, \bm{y}_1, \dots, \bm{x}_{j-1}, \bm{y}_{j-1}, \bm{x}_j)$, the model predicts $\hat{\bm{y}}_j$, and the loss is given by $\frac{1}{m} \sum_{j=1}^m \ell(\hat{\bm{y}}_j, \bm{y}_j)$. In our experiment, we use cross-entropy loss to measure the discrepancy between the predicted output $\hat{\bm{y}}_j$ and the true output $\bm{y}_j$.





When Chain of Thought (CoT) is used, each $y_j$ is itself a sequence of length $k$ representing intermediate reasoning steps. In this case, the loss is the average on all these intermediate steps. 

\vspace{-3mm}
\begin{figure*}[ht!]
    \centering
    \includegraphics[width=0.95\textwidth]{Figures/dlogd_horiz_v1.png}
    \vspace{-4mm}
    \caption{Test accuracy on unseen tasks. For parity task: $\d = d$ as the ambient dimension and $T = k$ as the number of secret indices. We show that the empirical scaling closely follows the theoretical scaling of $D \ln(D)$. \textbf{(A)} For a fixed $\d = 15$, as $T$ increases, the test accuracy on unseen tasks remains similar, even though the total number of tasks ($\sim \d^T$) grows exponentially with $T$.  \textbf{(B)} For a fixed secret length is $3$, as $\d$ increases, the number of tasks grows polynomially with $\d$, yet the number of tasks required to generalize reasonably to unseen tasks remains in $\propto \d \log \d$.}
    \label{fig:ood_generalization}
    \vspace{-3mm}
\end{figure*}

\paragraph{Training Data Generation.} We split both the task space and input space into training and testing. In other words, the parity tasks are split into $\mathcal{F}_{\text{train}}$ and $\mathcal{F}_{\text{test}}$; and the binary sequences are split into $\mathcal{X}_{\text{train}}$ and $\mathcal{X}_{\text{test}}$. The split in the input space helps us monitor the in-distribution training process while as the split in the task space aims to measure generalization to unseen parity functions.  




To construct the ICL data, we sample $m$ points $\bm{x}_1, \dots, \bm{x}_m$ uniformly at random from $\mathcal{X}_{\text{train}}$. Similarly, we sample a function $f$ uniformly at random from $\mathcal{F}_{\text{train}}$, and generate the sequence $(\bm{x}_1, f(\bm{x}_1), \dots, \bm{x}_m, f(\bm{x}_m))$. This process is repeated $N$ times, each time with a fresh sample of $m$ points and a new function $f$. 

\vspace{-2mm}
\paragraph{Evaluating Task Generalization.} For evaluation, we sample $f$ randomly from the held-out tasks $\mathcal{F}_{test}$ and sample inputs  uniformly at random from $\mathcal{X}_{\text{test}}$. We report the accuracy of the prediction $f(\bm{x}_m)$ given the demonstration $(\bm  x_1, f(\bm{x}_1), \dots, \bm{x}_{m-1}, f(\bm{x}_{m-1}), \bm{x}_m)$. This setting challenges the model to generalize to novel tasks beyond those encountered during training. 





\vspace{-2mm}
\subsection{Experimental Results}
 As discussed in Section~\ref{sec: example: parity}, introducing CoT transforms the parity problem class \( Parity(d,k) \) into an AutoRegressive Compositional structure \( ARC(\d,T) \) with $\d = d$ and \( T = k \). A key empirical question is: 

\textit{how does the number of training tasks scale with \( d \) and \( T \) to achieve a fixed target accuracy on unseen task sets? }

To investigate this, we conduct experiments on:
\vspace{-3mm}
\begin{enumerate}
    \item Scaling $T (=k)$, i.e. the length of secret indices. 
    \item Scaling $\d (=d)$, i.e. the ambient dimension of input.
\end{enumerate}














\paragraph{Scaling \( T \) for a Fixed \( \d \).} 
We examine how the number of training tasks affects test accuracy while keeping the ambient dimension fixed at \( d = 15 \). Specifically, we evaluate test accuracy for \( k = \{3,4,5,6,7\} \) under varying numbers of training tasks. As \( k \) increases, the size of the parity class grows significantly—from approximately $500$ for \( k = 3 \) to around $6500$ for \( k = 7 \). 

Remarkably, despite this increase, the test accuracy follows a similar trajectory. With just \( 3d \ln(d) \approx 122 \) training tasks, the model generalizes to unseen cases with high accuracy (\( >95\% \)). For \( k = 7 \), this means training on 122 tasks enables generalization to about 6,400 unseen ones! This empirical results suggests that the required number of training tasks remains roughly the same, regardless of \( k \), consistent with the theoretical scaling of \( \tilde{O}(d) \) tasks.

\paragraph{Scaling \( \d \) for a fixed \( T \).}  
We examine the effect of increasing \( d \in \{10, 15, 20\} \) while keeping \( k = 3 \) fixed. For each \( d \), we train on a total number of tasks proportional to \( d \ln(d) \), up to \( 4\times d \ln(d) \). Figure \ref{fig:ood_generalization}, Panel B, shows similar task generalization performance across different ambien dimension of \( d \), providing further evidence that \( \tilde{{O}}(d) \) i.i.d. training tasks are sufficient for generalization to unseen tasks on praity functions with CoT.


 







\paragraph{Subtask Identification via Linear Probing.}
Finally, we probe  the hidden representations of the Transformer (see \cite{alain2016understanding}) to see if it \textit{identifies and then executes subtasks} at the inference time, consistent with the framework of Section~\ref{sec:theory}. Specifically, we add a linear classifier to the final attention layer's hidden state when producing the $i$-th token in the Chain-of-Thought, aiming to predict the $i$-th secret index. Only the linear classifier is trained, while all Transformer parameters remain frozen.
Table~\ref{tab: linear probing} shows that for $d\in\{10,15,20\}$, $k=3$, the linear probe consistently achieves high accuracy at all the coordinates. This suggests that at each CoT step, the model indeed first  infers the relevant subtask (the secret index) from the in-context examples and then executes that subtask to generate the output token—an ability it acquires during training. Further experimental details appear in Appendix \ref{appendix: experiment details}. 

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
Dimension \(d\) & \multicolumn{3}{c|}{Validation Accuracy (\%)} \\
\cline{2-4}
& Token 1 & Token 2 & Token 3 \\
\hline
10 & 100.00 & 99.83 & 91.08 \\
15 & 95.42 & 99.97 & 98.73 \\
20 & 97.38 & 95.54 & 91.92 \\
\hline
\end{tabular}
\caption{Predicting Secret Indices via Linear Probing.} 
% Prediction Accuracies for 
\label{tab: linear probing}
\end{table}

Together with the above experiment on scaling $T$ and $D$, we show that Transformer effectively achieve exponential task generalization using $\tilde{O}(d)$ tasks with CoT on parity problems. 