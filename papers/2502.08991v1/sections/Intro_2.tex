\section{Introduction}




Large language models (LLMs) demonstrate a remarkable ability to solve tasks they were never explicitly trained on. Unlike classical supervised learning, which typically assumes that the test data distribution follows the training data distribution, LLMs can generalize to new task distributions with just a few demonstrations—a phenomenon known as in-context learning (ICL) \citep{brown2020language, wei2022emergent, garg2022can}. Recent studies suggest that trained Transformers implement algorithmic learners capable of solving various statistical tasks—such as linear regression—at inference time in context \citep{li2023transformers, bai2023transformers}. Despite their success in tasks such as learning conjunctions or linear regression, Transformers relying solely on in-context learning (ICL) struggle with more complex problems, particularly those requiring hierarchical reasoning.






A notable case where Transformers struggle with in-context learning (ICL) is the learning of parity functions, as examined in \cite{bhattamishra2024understanding}. In this setting, a Transformer is provided with a sequence of demonstrations $
(\bm{x}_1, f(\bm{x}_1)), \dots, (\bm{x}_n, f(\bm{x}_n))
$
and is required to predict \( f(\bm{x}_{\mathrm{query}}) \) for a new input \( \bm{x}_{\mathrm{query}} \). Specifically, they focused on parity functions from the class \( \text{Parity}(10,2) \), where each function is defined by a secret key of length \( k=2 \) within a length space of \( d=10 \). Each function \( f \) corresponds to a distinct learning task, resulting in 45 possible tasks. To assess generalization, a subset of tasks was held out during training. Their results demonstrate that Transformers trained via ICL fail to generalize to unseen tasks, even when the new tasks require only a simple XOR operation. These findings, along with other empirical studies \cite{an2023context, xu2024do}, suggest that standard ICL struggles with tasks requiring hierarchical or compositional reasoning.




\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{Figures/in_out_distribution_accuracy.png}
    \caption{We train a Transformer to learn parity functions through In-Context Learning (ICL): given a demonstration sequence \((\bm x_1, f(\bm x_1)), \dots, (\bm x_n, f(\bm x_n))\), infer the target \( f(\bm x_{\mathrm{query}}) \) from a new input $\bm x_{\mathrm{query}}$. Intuitively, each function \( f \) defines a distinct learning task. In this prototype experiment, tasks are sampled from the parity function family \(Parity (10,2)\) with secret length $k=2$ and bit length $d=10$, which consists of 45 tasks in total. To evaluate task generalization, we withhold a subset of tasks and train only on different subset of the remaining ones. Consistent with prior work \cite{bhattamishra2024understanding}, we observe that standard ICL fails to generalize across tasks. In contrast, incorporating Chain-of-Thought (CoT) reasoning significantly improves performance on unseen tasks.}
    \label{fig:in_out_dist_prelim}
\end{figure}    
% \end{wrapfigure} 

In contrast, we found that incorporating Chain-of-Thought (CoT) reasoning—introducing intermediate reasoning steps to the model—allows Transformers to easily generalize to unseen tasks, as illustrated in Figure~\ref{fig:in_out_dist_prelim}. Consistent with \cite{bhattamishra2024understanding}, we observe that Transformers without CoT perform only slightly better than chance level, no matter how many training tasks are presented to the model. However, as the number of training tasks increases, Transformers with CoT achieve near-perfect generalization on the held-out set of unseen tasks. We see that the extra information provided by CoT enables the model to exploit the compositional structure of the parity problem.
















Motivated by this example, we aim to systematically analyze how models can leverage autoregressive compositional structures to extend their capabilities beyond the training tasks. Conventionally, learning involves approximating a target function \(f^*\) drawn from a function class \(\mathcal{F}\) using examples from a training distribution over the input space \(\mathcal{X}\); generalization is then measured by testing $f^*$ on new  examples. In contrast, our focus is on \textbf{task generalization}, where training is restricted to a subset of functions or ``tasks'' \( \mathcal{F}_{\mathrm{train}} \subset \mathcal{F} \), leaving the remaining functions, unseen during training. Our goal is to investigate whether a model trained on tasks from \( \mathcal{F}_{\mathrm{train}} \) (with inputs from \( \mathcal{X} \)) can generalize to \textit{all tasks}, including \textit{unseen} tasks. 


This notion of task generalization goes beyond the standard out-of-distribution (OOD) settings (see, e.g., \cite{zhou2022domain} for review) by shifting the focus from adapting to new input distributions to learning entirely new tasks. Specifically, we ask:

\medskip
\textit{How can we quantify the number of tasks a model must be trained on to generalize to the entire class $\mathcal{F}$?}
\medskip



To analyze task generalization, we consider a finite set of functions \(\mathcal{F}\), where each function maps an input \(\bm x \in \mathcal{X}\) to a tuple of random variables 
$\bm y = (y_1, \dots, y_T)$. We assume each function can be characterized by a parameter tuple
\[
\theta = (\theta_1, \theta_2, \dots, \theta_T).
\]
The outputs are generated autoregressively: first, \(y_1\) is produced from \(\bm x\); then \(y_2\) is generated from \(\bm x\) and \(y_1\); and then \(y_3\) is generated from \(\bm x\), \(y_1\) and \(y_2\); and this process continues until \(y_T\) is produced. 
 Specifically, the sequence is generated sequentially as
\[
y_t \sim P_{\theta_t}(y_t \mid \bm x, \bm y_{<t}), \quad \text{for } t = 1, \dots, T,
\]
where \(\bm y_{<t} = (y_1, \dots, y_{t-1})\) denotes the previously generated outputs, and $P_{\theta_t}$ is some conditional probability distribution that is parametrized by $\theta_{t}$ and is conditioned on $\bm y_{<t}$ and $\bm x$.This structure can also be interpreted as a sequence of compositions,






\vspace{-2pt}


\begin{align*}
    \bm x &\xrightarrow{P_{\theta_1}} y_1 \\
    \bm x, y_1 &\xrightarrow{P_{\theta_2}} y_2 \\
    &\dots \\
    \bm x, y_1, \dots, y_{T-1} &\xrightarrow{P_{\theta_{T-1}}} y_T\;.
\end{align*}



We will call this function class \textit{AutoRegressive Compositional structure}. 
Assuming that the cardinality of the set of possible values for each parameter $\theta_t$ is finite and is equal to \( \d \), we will use the notation $\mathcal{F} =ARC(T, \d)$. The cardinality of this class is $\d^T$.


For the sparse parity problem with \(k\) secret keys in this framework, the output sequence has length \(T = k\). Given an input \(\bm x \in \mathcal{X} = \{0,1\}^n\), let the secret keys correspond to indices \(i_1, i_2, \dots, i_k\) (in a predetermined order). The output sequence \(\bm y = (y_1, y_2, \dots, y_k)\) is defined as follows, 
$$y_1 = x_{i_1}, \; y_2 = x_{i_1} \oplus x_{i_2}, \; \dots, \; y_k = x_{i_1} \oplus x_{i_2} \oplus \dots \oplus x_{i_k}.$$


That is, each \(y_t\) recovers the XOR of the first \(t\) secret coordinates. In this example, the output distribution at each step is deterministic, assigning probability 1 to the correct XOR value and 0 to all other values.


.

\noindent We can now address the following fundamental question: 

\medskip
\textit{How many tasks in \(\mathcal{F}_{\mathrm{train}}\) must a model be trained on to generalize to all tasks in \(\mathcal{F}\), including those it has not seen? In particular, can a model trained on \( \tilde{O}(\d) \) tasks generalize across the entire set of \( \d^T \) tasks?} 
\medskip

\noindent Our main contributions are:
% \vspace{-3mm}
\begin{itemize}[leftmargin=0.4 cm]
    \item We define AutoRegressive Compositional structure and introduce a framework to quantitatively analyze task generalization when the function class follows an AutoRegressive Compositional structure. (Sections \ref{sec: ARC} and \ref{sec: task generalization})
    
    \item We establish that under this structure, task generalization to all \( \d^T \) tasks is theoretically achievable by training on  \( \tilde{O}(\d) \) tasks up to logarithmic terms (\cref{sec: Exp Task Generalization}).
    
    \item We demonstrate how the parity problem aligns with our framework and empirically show that Transformers trained on i.i.d. sampled tasks exhibit exponential task generalization via chain-of-thought (CoT) reasoning, consistent with theoretical scaling (\cref{sec: Experiments}).
    
    \item Finally, we show that the selection of training tasks significantly impacts generalization to unseen tasks. If tasks are chosen adversarially, training on even nearly all $\d^T$ of the tasks with CoT may fail to generalize to the remaining tasks (\cref{sec: Experiment beyond iid sampling}).\vspace{-3mm}
\end{itemize}






