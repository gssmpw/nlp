
\paragraph{In context learning setup.} Our setup closely follows that of \cite{garg2022can, bhattamishra2024understanding}. In this setup, a
sequence model $M$ (such as Transformers) is trained using $N$ sequences, each sequence consisting of
$m$ labeled examples, $(x_1, y_1, \dots, x_m, y_m)$.

The model is trained for the next token-prediction task,
except that we only consider $y_i$ in loss optimization: If $\hat{y}_1, \dots, \hat{y}_m$ are the $m$ output labels predicted
by the model, where $\hat{y}_j := M(P_j)$ is predicted using the prefix $P_j = (x_1, y_1, \dots, x_{j-1}, y_{j-1}, x_j)$,
the loss on this sequence is given by $\frac{1}{m} \sum_{j=1}^m \ell(\hat{y}_j, y_j)$. Here, $\ell(\hat{y}_j, y_j)$ represents a suitable loss function, such as cross-entropy loss, that measures the discrepancy between the predicted output $\hat{y}_j$ and the true output $y_j$.

If we are using Chain of Thought (CoT), then each $y_j$ is a sequence of length $k$ representing intermediate reasoning steps. In this case, the next-word prediction task leverages all prior $x$ and $y$, including the CoT elements of $y_j$ from previous steps, as part of the prefix $P_i$. 



\paragraph{Training process.} To generate the set of $N$ training sequences, we first split the total binary sequences into $\mathcal{S}_{\text{train}}$ and $\mathcal{S}_{\text{test}}$, where both are subsets of the total $2^d$ binary sequences. Additionally, we split the total parity tasks into $\mathcal{F}_{\text{train}}$ and $\mathcal{F}_{\text{test}}$, where each is a subset of the total $\binom{d}{k}$ possible parity tasks.





We sample $m$ points $x_1, \dots, x_m$ uniformly at random from $\mathcal{S}_{\text{train}}$. Similarly, we sample a function $f$ uniformly at random from $\mathcal{F}_{\text{train}}$, and generate the sequence $(x_1, f(x_1), \dots, x_m, f(x_m))$. This process is repeated $N$ times, each time with a fresh sample of $m$ points and a new function $f$. For an appropriate loss function $\ell$, the empirical loss over $N$ training examples is defined as: 
\begin{equation} \frac{1}{N} \sum_{i=1}^N \left( \frac{1}{m} \sum_{j=1}^m \ell\left(M(P_j^{(i)}), f_i(x_k)\right) \right). \end{equation}




\paragraph{Out-of-distribution generalization.} In the parity task, we refer to out-of-distribution generalization as a setting where the model is trained on $\mathcal{F}_{train}$ tasks and $\mathcal{S}_{train}$ sequences, and then evaluated on entirely new tasks and sequences that were not part of the training set namely $\mathcal{F}_{test}$ tasks and $\mathcal{S}_{test}$ sequences. This setting challenges the model to generalize to novel tasks beyond those encountered during training.

\paragraph{Increasing $k$ for a fixed $d$.}To support our theory from the previous section, we fix $d=15$ and increase $k=3, 4, 5, 6, 7$, which causes an exponential growth in the total number of tasks. We then sample $\hat{O}(d)$ i.i.d. from the total possible tasks, train the transformer on these tasks, and evaluate it on unseen tasks during testing. Figure~\ref{fig:ood_generalization} Panel A, demonstrates that, although the total number of tasks grows exponentially with $k$---e.g., from approximately 500 for $d=15, k = 3$ to about 6500 tasks for $d=15,k = 7$---the generalization behavior stays the same. Moreover

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.45\textwidth]{Figures/final_combined_dlog.png}
    \caption{\textbf{Panel A.} For a fixed $d = 15$, as $k$ increases, the out-of-distribution performance remains similar, even though the total number of tasks grows exponentially with $k$, approximately $\sim d^k$.~\textbf{Panel B.} For a fixed $k = 3$, as $d$ increases, the number of tasks grows polynomially with $d$, yet the number of tasks required to generalize reasonably to out-of-distribution data remains low.}




    \label{fig:ood_generalization}
\end{figure}



\begin{remark}
    We observe that transformers with ICL and no CoT struggle to generalize even in simpler in-distribution settings as the number of tasks increases. In the parity task, we refer to in-distribution generalization as a setting where the model is trained on $\mathcal{F}_{train}$ tasks and $\mathcal{S}_{train}$ sequences, and then evaluated on the same set of tasks $\mathcal{F}_{train}$ but with entirely new sequences $\mathcal{S}_{test}$ that were not seen during training. See Appendix \ref{appendix:extra_exps} for more detail.
\end{remark}



















