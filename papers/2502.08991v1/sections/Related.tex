\section{Related Works}



\subsection{Composition and Generalization}

The role of composition in reasoning for language models has been widely studied.  
\cite{saparov2023testing} explores various out-of-distribution (OOD) generalization formats, including compositional generalization, showing that a neural networkâ€™s ability to generalize compositionally is highly dependent on both architecture and task properties. Similar conclusions have been drawn in prior works~\citep{lake2018generalization, keysers2019measuring}. Further, \citep{bhattamishra2024understanding, dziri2023faith, an2023context, xu2024do} examine compositional generalization in in-context learning (ICL) and find that generalization to composing multiple steps is in general hard for LLMs. One notable observation is that LLMs succeed in compositional generalization for clause satisfaction problems but not for parity problems. 

Another line of research investigates composition as a mechanism underlying emergent abilities in language models. \cite{arora2023theory} demonstrates that language modeling can lead to learning tuples of skills, which are small compositions of fundamental capabilities. Building on this idea, \citep{kaur2024instruct, zhao2024can} leverage compositional structures to generate supervised fine-tuning (SFT) data, leading to improved language model performance.  

Beyond sequential composition, other forms of compositionality in neural networks have been explored. \cite{song2024out} investigates layer-wise composition in transformers, while \cite{schug2023discovering} proposes a modular neural architecture for learning hidden compositional representations. Additionally, \cite{wiedemer2024compositional} examines compositional structures in image reconstruction, and \cite{lippl2024does} provides a theoretical analysis of composition in kernel and linear models.  

While prior work has largely focused on qualitative insights into compositional generalization, our work takes a quantitative approach: studying how many training tasks are needed to achieve task generalization over an entire function class.







\subsection{Learning and Testing with Multiple Distributions}



Our work aims to analyze generalization when the training and testing distributions differ. This problem has been studied from various perspectives in the statistical learning community. One approach is to frame it as learning a shared representation across multiple tasks. \cite{ye2021towards} defines variation and informativeness between different environments based on a common representation, while \cite{arjovsky2019invariant} addresses the problem by designing specific training objectives. Earlier studies on linear and kernel models also explore this direction~\citep{du2017hypothesis, lei2021near}.

Another perspective considers the testing environment as a distribution shift, where the model may sample during inference to achieve domain adaptation. \cite{mansour2009domain} analyzes generalization error when a model is trained on distribution \( P \) but tested on a different distribution \( Q \), introducing an error bias dependent on the distance \( d(P,Q) \). To mitigate this bias, \cite{cortes2010learning} proposes reweighting training samples when test samples from \( Q \) are available.  

A related line of research investigates scenarios where both training and test samples are accessible. Notable setups include covariate shift~\citep{kpotufe2021marginal, ma2023optimally} and domain adaptation~\citep{sugiyama2007covariate, ben2014domain}. When direct sampling from the test distribution is not feasible, alternative strategies focus on training robustly against worst-case shifts. This can be achieved through adversarial perturbations or min-max optimization formulations~\citep{madry2017towards, raghunathan2020understanding, duchi2023distributionally}.



In this work we impose an AutoRegressive Compositional(ARC) structure on the function class and propose a new framework to study task generalization. This compositional structure decomposes the function class into atomic subtasks, enabling a modular approach to learning. By leveraging this structure, we establish a quantitative understanding of how many training tasks are required for generalization. Our results provide a theoretical foundation for structured learning and demonstrate how models can efficiently generalize beyond the training distribution.














