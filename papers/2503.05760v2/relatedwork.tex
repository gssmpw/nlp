\section{Background and Related Work}
%The integration of artificial intelligence in education has undergone a dramatic transformation with the emergence of Large Language Models (LLMs) \citep{a1}. This section first broadly examines the current landscape of LLMs in education, followed by a focused review of AI applications specifically in engineering education.
%
\subsection{Current State of LLMs in Education}
The educational impact of LLMs has been extensively studied across multiple dimensions since their widespread availability. Early research by \citep{a7} documented the rapid adoption of LLMs among university students, with usage rates exceeding 45\% across multiple disciplines. This adoption has prompted significant concerns about academic integrity, with studies by \cite{a8} suggesting that traditional plagiarism detection tools are largely ineffective against LLM-generated content.
Several studies have systematically evaluated LLM performance in specific academic tasks. \cite{a9} found that LLMs achieved scores in the top percentile in standardized writing assessments, while \cite{b1} documented its ability to solve complex mathematical word problems with a high accuracy. In computer science education, \cite{b2} demonstrated that LLMs could successfully complete programming assignments across multiple languages, though performance declined significantly for tasks requiring complex algorithm design.
The impact on student learning outcomes has been more contentious. \cite{b3} reported improved conceptual understanding among students who used LLMs as learning aids and raised concerns about decreased retention of fundamental principles. Research by \cite{b5} suggests that the effectiveness of LLMs varies significantly based on the student's existing knowledge base and the complexity of the subject matter.
Institutional responses have varied widely. A comprehensive survey by \cite{b6} of 500 universities found approaches ranging from complete prohibition to active integration into curriculum design. 

\subsection{Existing Studies on AI in Engineering Education}
Several studies have focused on specific engineering domains. In electrical engineering, research by \cite{b9} evaluated LLM performance in circuit analysis, while mechanical engineering applications were extensively studied by \cite{b8}. However, these studies typically focused on specific problem types rather than complete course evaluation.
In control systems education--the topic of our paper--limited research exists on comprehensive LLM evaluation. Studies by \cite{c1} demonstrated successful LLM performance in control theory problems, research by \cite{c2} identified significant limitations in LLM's ability to generate and debug control system code, finding mixed results depending on the complexity of the implementation.
Prior work has begun exploring LLM performance in academic settings. \cite{c3} evaluated AI capabilities in mechanics coursework, while studies in computer science education by \cite{b3} and \cite{c5} have examined LLM integration in introductory programming and upper-level project work respectively. Our work builds on these foundations by providing a comprehensive assessment across all elements of an engineering course -- from theoretical problems to practical implementation and programming tasks.

\subsection{AE 353: Course Structure and Requirements}
The Aerospace Control Systems course (AE 353) 
%\footnote{The syllabus for the Fall 2024 offering of AE-353 is available here: \href{https://mornik.web.illinois.edu/wp-content/uploads/AE_353_FA24_Syllabus.pdf}{https://mornik.web.illinois.edu/wp-content/uploads/AE\_353\_FA24\_Syllabus.pdf}}
 at UIUC serves as an introduction to control theory and its applications in aerospace systems. The course, mandatory for junior-level aerospace engineering students, integrates theoretical foundations with practical implementation through a diverse range of assessment types (see Fig. \ref{fig:dist} for the distribution of the assessment types). 
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.45\textwidth]{dist} % Adjust width as needed
    \caption{Distribution of assessment types across AE 353.}
    \label{fig:dist} % Optional: Add a label for referencing
\end{figure}


The course structure emphasizes progressive complexity in both theoretical understanding and practical application. Core concepts begin with fundamental control theory, before advancing to more sophisticated topics. This progression is reflected in the assessment structure, which comprises four distinct categories: homework assignments, midterm examinations, programming-intensive projects, and a comprehensive final examination.

Homework assignments, delivered through PrairieLearn, a platform that generates question variants with real-time feedback \citep{prairie1}, form the backbone of continuous assessment. These assignments test theoretical understanding with a combination of multiple-choice single correct questions (MCQ), multiple choice multi-correct questions (MCMCQ), numerical problems and basic Python programming tasks. The auto-graded nature of these assignments provides immediate feedback while maintaining consistent evaluation criteria.

The course includes two midterm examinations that assess the students' ability to demonstrate comprehensive problem-solving skills and theoretical understanding. Unlike homework assignments, midterms require detailed solution steps and allow partial credit, enabling a more nuanced evaluation of student understanding. The problems typically integrate multiple course concepts, requiring students to synthesize their knowledge across different topics.

Three substantial programming projects represent the course's practical implementation component. These projects demand extensive Python coding, data analysis, and technical writing skills. Each project builds upon theoretical concepts covered in lectures, requiring students to implement control algorithms, analyze system behavior, and document their findings in detailed technical reports.

The final examination adopts a hybrid format, combining PrairieLearn-based auto-graded questions with traditional written problems. This structure allows for comprehensive assessment while maintaining efficiency in evaluation. The auto-graded portion consists of direct copies of homework problems (with PrairieLearn generating different variants of the question), while the written section consists primarily of midterm problems with minor modifications in parameters or problem statements.