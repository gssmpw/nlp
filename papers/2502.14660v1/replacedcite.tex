\section{Related Work}
\textbf{\textit{Traditional methods.}} Until recently, news recommendation systems treated location-based articles primarily as an entity extraction problem. These systems identified locations both explicitly through Named Entity Recognition ____ and implicitly through Knowledge Graphs ____.

Several studies have tackled the specific challenge of implicit locations. ____ noted that news articles frequently use vague location references, especially when mentioning only cities or suburbs. The authors developed Explicit Localized Semantic Analysis (ELSA), which uses geo-tagged documents to characterize locations and maps both these documents and news articles into a topic space using Explicit Semantic Analysis (ESA) ____. The system then analyzes Wikipedia-based topic vectors to determine the relevance between a user's location and news articles, recommending the most relevant content.


\textbf{\textit{BERT-based methods.}} Research then explored whether BERT ____ model possess an \textit{inherent relational knowledge}, thus might be able to detect location signals. ____ demonstrated that BERT achieves relational understanding comparable to traditional NLP systems with oracle-level knowledge, even without fine-tuning. Moreover, BERT exceled at open-domain question answering compared to supervised baselines (e.g., "Dante was born in [Mask]", ____).

UNBERT ____ advanced this concept by explicitly building a model to process word-level signals such as locations. They introduced a pre-trained BERT model for news recommendation designed to address the cold-start problem using \textit{location-based signals} at the word level. Their model represented users through their browsed news text and employed two modules: a \textit{Word-Level} Module (WLM) with transformer layers to calculate hidden representations and propagate word-level matching signals, and a \textit{News-Level} Module to capture matching at the news level. While this was the first model to successfully capture news location signals, it required locations to be \textbf{explicitly} mentioned in the article's text.

\textbf{\textit{LLMs enrichment methods.}} Recent advancements in LLMs like ChatGPT ____ present a promising alternative to Named Entity Recognition models and Knowledge Graphs. These models demonstrate strong capabilities in contextual understanding and reasoning. Unlike conventional approaches, they are not restricted by fixed entity-relation schemas, allowing for flexible and dynamic relational modeling. 

Several studies have explored ways to incorporate Knowledge Graphs to enhance LLMs' knowledge. ____ developed a model that uses Knowledge Graph structures through knowledge embeddings ____ as input to a BERT-based model. To better integrate textual and Knowledge Graph information, they created a new objective: the model predicts masked named entity alignments within text, learning to match entities with their correct graph context. This results in a more knowledge-enriched representation.
    
Further, ____ enhanced BERT by integrating KG triplets into sentences but found that this approach disrupted readability and structure. To address this, they introduced a soft-positioning technique and a masked transformer layer that preserved sentence integrity while masking KG information. They also introduced the term Knowledge Noise (KN)—irrelevant or misleading KG data—and showed that the suggested soft-positioning and masking techniques helped reduce it. Lastly, they suggested refining KG triplet selection to further minimize noise.


____ proposed a KG-augmented decoder that incorporates hierarchical graph structures during decoding. According to the authors, this approach enables the model to better capture relationships between concepts and their neighbors, improving output accuracy. In their work, graph attention mechanisms were employed to aggregate rich semantic information, enhancing the model’s capacity to generalize across unfamiliar concepts.
   
Other research examined methods for aligning KG information with LLM predictions. ____ used (text, local KG) pairs for LLM pre-training, and ____ combined Pre-trained Language Models (PLMs) and Graph Neural Network (GNN) representations via layered modality interactions. ____ introduced the Knowledge Solver (KSL), which guides LLMs in structured KG searches by transforming retrieval tasks into multi-hop decision sequences, empowering zero-shot knowledge retrieval.
    
To this date, no existing work has, to our knowledge, specifically applied LLMs' ability to reason about implicit locations within a news recommendation system.