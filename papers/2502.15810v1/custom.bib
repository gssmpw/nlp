% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").
@inproceedings{lin2020commongen,
  title={CommonGen: A constrained text generation challenge for generative commonsense reasoning},
  author={Lin, Bill Yuchen and Zhou, Wangchunshu and Shen, Ming and Zhou, Pei and Bhagavatula, Chandra and Choi, Yejin and Ren, Xiang},
  journal={arXiv preprint arXiv:1911.03705},
  year={2019}
}

@inproceedings{talmor2021commonsenseqa,
  title={Commonsenseqa 2.0: Exposing the limits of ai through gamification},
  author={Talmor, Alon and Yoran, Ori and Bras, Ronan Le and Bhagavatula, Chandra and Goldberg, Yoav and Choi, Yejin and Berant, Jonathan},
  journal={arXiv preprint arXiv:2201.05320},
  year={2022}
}

@inproceedings{liu2020ensemble,
  title={Qiaoning at semeval-2020 task 4: Commonsense validation and explanation system based on ensemble of language model},
  author={Pai, Liu},
  booktitle={Proceedings of the Fourteenth Workshop on Semantic Evaluation},
  pages={415--421},
  year={2020}
}
@article{peng2023copen,
 title={Copen: Probing conceptual knowledge in pre-trained language models},
  author={Peng, Hao and Wang, Xiaozhi and Hu, Shengding and Jin, Hailong and Hou, Lei and Li, Juanzi and Liu, Zhiyuan and Liu, Qun},
  journal={arXiv preprint arXiv:2211.04079},
  year={2022}
}
@InProceedings{10.1007/3-540-45757-7_24,
author="El-Sayed, Mazen
and Pacholczyk, Daniel",
editor="Flesca, Sergio
and Greco, Sergio
and Ianni, Giovambattista
and Leone, Nicola",
title="A Qualitative Reasoning with Nuanced Information",
booktitle="Logics in Artificial Intelligence",
year="2002",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="283--295",
abstract="This paper presents a new model for handling nuanced information expressed in an affirmative form like ``x is m∞ A''. In this model, nuanced information are represented in a qualitative way within a symbolic context. For that purpose, vague terms and linguistic modifiers that operate on them are defined. The model presented is based on a symbolic M-valued predicate logic and provides a new deduction rule generalizing the Modus Ponens rule.",
isbn="978-3-540-45757-2"
}

@article{yao2021survey,
  title={A survey on causal inference},
  author={Yao, Liuyi and Chu, Zhixuan and Li, Sheng and Li, Yaliang and Gao, Jing and Zhang, Aidong},
  journal={ACM Transactions on Knowledge Discovery from Data (TKDD)},
  volume={15},
  number={5},
  pages={1--46},
  year={2021},
  publisher={ACM New York, NY, USA}
}


@article{chen2020review,
  title={A review: Knowledge reasoning over knowledge graph},
  author={Chen, Xiaojun and Jia, Shengbin and Xiang, Yang},
  journal={Expert systems with applications},
  volume={141},
  pages={112948},
  year={2020},
  publisher={Elsevier}
}

@inproceedings{zhang2020knowledge,
  title={Cs-net at semeval-2020 task 4: Siamese bert for comve},
  author={Dash, Soumya Ranjan and Routray, Sandeep and Varshney, Prateek and Modi, Ashutosh},
  journal={arXiv preprint arXiv:2007.10830},
  year={2020}
}

@inproceedings{huang2020kalm,
  title={Kalm at semeval-2020 task 4: Knowledge-aware language models for comprehension and generation},
  author={Wan, Jiajing and Huang, Xinting},
  journal={arXiv preprint arXiv:2005.11768},
  year={2020}
}

@inproceedings{saeedi2020evaluation,
  title={Evaluation of state-of-the-art NLP deep learning architectures on commonsense reasoning task},
  author={Lee, Guo Rui Justin},
  year={2021}
}

@inproceedings{abuaqouleh2020ensemble,
  title={Nlp@ just at semeval-2020 task 4: Ensemble technique for bert and roberta to evaluate commonsense validation},
  author={Al Bashabsheh, Emran and Aqouleh, Ayah Abu and Mohammad, AL-Smadi},
  booktitle={Proceedings of the Fourteenth Workshop on Semantic Evaluation},
  pages={574--579},
  year={2020}
}

@inproceedings{zhang2024comet-NOTCORRECT,
  title={COMET-ATOMIC 2025: An Expanded Commonsense Knowledge Base with Large Language Models},
  author={Zhang, Haoyang and others},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  year={2024}
}

@inproceedings{toroghi2024right,
  title = "Right for Right Reasons: Large Language Models for Verifiable Commonsense Knowledge Graph Question Answering",
    author = "Toroghi, Armin  and
      Guo, Willis  and
      Abdollah Pour, Mohammad Mahdi  and
      Sanner, Scott",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.378/",
    doi = "10.18653/v1/2024.emnlp-main.378",
    pages = "6601--6633",
    url={https://aclanthology.org/2024.emnlp-main.378/}
}

@article{zhao2024large,
  title={Large language models as commonsense knowledge for large-scale task planning},
  author={Zhao, Zirui and Lee, Wee Sun and Hsu, David},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{li2021systematic,
  title = "A Systematic Investigation of Commonsense Knowledge in Large Language Models",
    author = "Li, Xiang Lorraine  and
      Kuncoro, Adhiguna  and
      Hoffmann, Jordan  and
      de Masson d{'}Autume, Cyprien  and
      Blunsom, Phil  and
      Nematzadeh, Aida",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.812/",
    doi = "10.18653/v1/2022.emnlp-main.812",
    pages = "11838--11855",
    abstract = "Language models (LMs) trained on large amounts of data have shown impressive performance on many NLP tasks under the zero-shot and few-shot setup. Here we aim to better understand the extent to which such models learn commonsense knowledge {---} a critical component of many NLP applications. We conduct a systematic and rigorous zero-shot and few-shot commonsense evaluation of large pre-trained LMs, where we: (i) carefully control for the LMs' ability to exploit potential surface cues and annotation artefacts, and (ii) account for variations in performance that arise from factors that are not related to commonsense knowledge. Our findings highlight the limitations of pre-trained LMs in acquiring commonsense knowledge without task-specific supervision; furthermore, using larger models or few-shot evaluation is insufficient to achieve human-level commonsense performance."
}

@inproceedings{jain2023temporal,
  title = "Do Language Models Have a Common Sense regarding Time? Revisiting Temporal Commonsense Reasoning in the Era of Large Language Models",
    author = "Jain, Raghav  and
      Sojitra, Daivik  and
      Acharya, Arkadeep  and
      Saha, Sriparna  and
      Jatowt, Adam  and
      Dandapat, Sandipan",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.418/",
    doi = "10.18653/v1/2023.emnlp-main.418",
    pages = "6750--6774",
    abstract = "Temporal reasoning represents a vital component of human communication and understanding, yet remains an underexplored area within the context of Large Language Models (LLMs). Despite LLMs demonstrating significant proficiency in a range of tasks, a comprehensive, large-scale analysis of their temporal reasoning capabilities is missing. Our paper addresses this gap, presenting the first extensive benchmarking of LLMs on temporal reasoning tasks. We critically evaluate 8 different LLMs across 6 datasets using 3 distinct prompting strategies. Additionally, we broaden the scope of our evaluation by including in our analysis 2 Code Generation LMs. Beyond broad benchmarking of models and prompts, we also conduct a fine-grained investigation of performance across different categories of temporal tasks. We further analyze the LLMs on varying temporal aspects, offering insights into their proficiency in understanding and predicting the continuity, sequence, and progression of events over time. Our findings reveal a nuanced depiction of the capabilities and limitations of the models within temporal reasoning, offering a comprehensive reference for future research in this pivotal domain."
}

@inproceedings{wang2020semeval,
  author    = {Cunxiang Wang and Shuailong Liang and Yili Jin and Yilong Wang and Xiaodan Zhu and Yue Zhang},
  title     = {SemEval-2020 Task 4: Commonsense Validation and Explanation},
  booktitle = {Proceedings of The 14th International Workshop on Semantic Evaluation},
  year      = {2020},
  publisher = {Association for Computational Linguistics}
}

@inproceedings{zhang2020cn,
  author    = {Yice Zhang and Jiaxuan Lin and Yang Fan and Peng Jin and Yuanchao Liu and Bingquan Liu},
  title     = {CN-HIT-IT.NLP at SemEval-2020 Task 4: Enhanced Language Representation with Multiple Knowledge Triples},
  booktitle = {Proceedings of The 14th International Workshop on Semantic Evaluation},
  year      = {2020},
  publisher = {Association for Computational Linguistics}
}

@inproceedings{zhao2020ecnu,
  author    = {Qian Zhao and Siyu Tao and Jie Zhou and Linlin Wang and Xin Lin},
  title     = {ECNU-SenseMaker at SemEval-2020 Task 4: Leveraging Heterogeneous Knowledge Resources for Commonsense Validation and Explanation},
  booktitle = {Proceedings of The 14th International Workshop on Semantic Evaluation},
  year      = {2020},
  publisher = {Association for Computational Linguistics}
}

@inproceedings{10.5555/3298023.3298212,
author = {Speer, Robyn and Chin, Joshua and Havasi, Catherine},
title = {ConceptNet 5.5: an open multilingual graph of general knowledge},
year = {2017},
publisher = {AAAI Press},
abstract = {Machine learning about language can be improved by supplying it with specific knowledge and sources of external information. We present here a new version of the linked open data resource ConceptNet that is particularly well suited to be used with modern NLP techniques such as word embeddings.ConceptNet is a knowledge graph that connects words and phrases of natural language with labeled edges. Its knowledge is collected from many sources that include expert-created resources, crowd-sourcing, and games with a purpose. It is designed to represent the general knowledge involved in understanding language, improving natural language applications by allowing the application to better understand the meanings behind the words people use.When ConceptNet is combined with word embeddings acquired from distributional semantics (such as word2vec), it provides applications with understanding that they would not acquire from distributional semantics alone, nor from narrower resources such as WordNet or DBPedia. We demonstrate this with state-of-the-art results on intrinsic evaluations of word relatedness that translate into improvements on applications of word vectors, including solving SAT-style analogies.},
booktitle = {Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence},
pages = {4444–4451},
numpages = {8},
location = {San Francisco, California, USA},
series = {AAAI'17}
}
@InProceedings{10.1007/3-540-36124-3_77,
author="Singh, Push
and Lin, Thomas
and Mueller, Erik T.
and Lim, Grace
and Perkins, Travell
and Li Zhu, Wan",
editor="Meersman, Robert
and Tari, Zahir",
title="Open Mind Common Sense: Knowledge Acquisition from the General Public",
booktitle="On the Move to Meaningful Internet Systems 2002: CoopIS, DOA, and ODBASE",
year="2002",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="1223--1237",
abstract="Open Mind Common Sense is a knowledge acquisition system designed to acquire commonsense knowledge from the general public over the web. We describe and evaluate our first fielded system, which enabled the construction of a 450,000 assertion commonsense knowledge base. We then discuss how our second-generation system addresses weaknesses discovered in the first. The new system acquires facts, descriptions, and stories by allowing participants to construct and fill in natural language templates. It employs word-sense disambiguation and methods of clarifying entered knowledge, analogical inference to provide feedback, and allows participants to validate knowledge and in turn each other.",
isbn="978-3-540-36124-4"
}
@inproceedings{srivastava-etal-2020-team,
    title = "Team {S}olomon at {S}em{E}val-2020 Task 4: Be Reasonable: Exploiting Large-scale Language Models for Commonsense Reasoning",
    author = "Srivastava, Vertika  and
      Sahoo, Sudeep Kumar  and
      Kim, Yeon Hyang  and
      R.r, Rohit  and
      Raj, Mayank  and
      Jaiswal, Ajay",
    editor = "Herbelot, Aurelie  and
      Zhu, Xiaodan  and
      Palmer, Alexis  and
      Schneider, Nathan  and
      May, Jonathan  and
      Shutova, Ekaterina",
    booktitle = "Proceedings of the Fourteenth Workshop on Semantic Evaluation",
    month = dec,
    year = "2020",
    address = "Barcelona (online)",
    publisher = "International Committee for Computational Linguistics",
    url = "https://aclanthology.org/2020.semeval-1.74/",
    doi = "10.18653/v1/2020.semeval-1.74",
    pages = "585--593",
    abstract = "In this paper, we present our submission for SemEval 2020 Task 4 - Commonsense Validation and Explanation (ComVE). The objective of this task was to develop a system that can differentiate statements that make sense from the ones that don`t. ComVE comprises of three subtasks to challenge and test a system`s capability in understanding commonsense knowledge from various dimensions. Commonsense reasoning is a challenging task in the domain of natural language understanding and systems augmented with it can improve performance in various other tasks such as reading comprehension, and inferencing. We have developed a system that leverages commonsense knowledge from pretrained language models trained on huge corpus such as RoBERTa, GPT2, etc. Our proposed system validates the reasonability of a given statement against the backdrop of commonsense knowledge acquired by these models and generates a logical reason to support its decision. Our system ranked 2nd in subtask C with a BLEU score of 19.3, which by far is the most challenging subtask as it required systems to generate the rationale behind the choice of an unreasonable statement. In subtask A and B, we achieved 96{\%} and 94{\%} accuracy respectively standing at 4th position in both the subtasks."
}
@article{mondorf2024beyond,
  title={Beyond Accuracy: Evaluating the Reasoning Behavior of Large Language Models--A Survey},
  author={Mondorf, Philipp and Plank, Barbara},
  journal={arXiv preprint arXiv:2404.01869},
  year={2024}
}

@inproceedings{xing-etal-2020-iie,
    title = "{IIE}-{NLP}-{NUT} at {S}em{E}val-2020 Task 4: Guiding {PLM} with Prompt Template Reconstruction Strategy for {C}om{VE}",
    author = "Xing, Luxi  and
      Xie, Yuqiang  and
      Hu, Yue  and
      Peng, Wei",
    editor = "Herbelot, Aurelie  and
      Zhu, Xiaodan  and
      Palmer, Alexis  and
      Schneider, Nathan  and
      May, Jonathan  and
      Shutova, Ekaterina",
    booktitle = "Proceedings of the Fourteenth Workshop on Semantic Evaluation",
    month = dec,
    year = "2020",
    address = "Barcelona (online)",
    publisher = "International Committee for Computational Linguistics",
    url = "https://aclanthology.org/2020.semeval-1.42/",
    doi = "10.18653/v1/2020.semeval-1.42",
    pages = "346--353",
    abstract = "This paper introduces our systems for the first two subtasks of SemEval Task4: Commonsense Validation and Explanation. To clarify the intention for judgment and inject contrastive information for selection, we propose the input reconstruction strategy with prompt templates. Specifically, we formalize the subtasks into the multiple-choice question answering format and construct the input with the prompt templates, then, the final prediction of question answering is considered as the result of subtasks. Experimental results show that our approaches achieve significant performance compared with the baseline systems. Our approaches secure the third rank on both official test sets of the first two subtasks with an accuracy of 96.4 and an accuracy of 94.3 respectively."
}

@inproceedings{jon2020but,
  author    = {Josef Jon and Martin Fajcik and Martin Docekal and Pavel Smrz},
  title     = {BUT-FIT at SemEval-2020 Task 4: Multilingual Commonsense},
  booktitle = {Proceedings of The 14th International Workshop on Semantic Evaluation},
  year      = {2020},
  publisher = {Association for Computational Linguistics}
}

@inproceedings{konar2020ana,
  author    = {Anandh Konar and Chenyang Huang and Amine Trabelsi and Osmar Zaiane},
  title     = {ANA at SemEval-2020 Task 4: Multi-task Learning for Commonsense Reasoning (Union)},
  booktitle = {Proceedings of The 14th International Workshop on Semantic Evaluation},
  year      = {2020},
  publisher = {Association for Computational Linguistics}
}

@inproceedings{liu2019kbert,
  author    = {Weijie Liu and Peng Zhou and Zhe Zhao and Zhiruo Wang and Qi Ju and Haotang Deng and Ping Wang},
  title     = {K-BERT: Enabling Language Representation with Knowledge Graph},
  booktitle = {arXiv preprint arXiv:1909.07606},
  year      = {2019}
}

@inproceedings{lewis2019bart,
  author    = {Mike Lewis and Yinhan Liu and Naman Goyal and Marjan Ghazvininejad and Abdelrahman Mohamed and Omer Levy and Ves Stoyanov and Luke Zettlemoyer},
  title     = {BART: Denoising Sequence-to-Sequence Pre-Training for Natural Language Generation, Translation, and Comprehension},
  booktitle = {arXiv preprint arXiv:1910.13461},
  year      = {2019}
}

@inproceedings{radford2019gpt2,
  author    = {Alec Radford and Jeff Wu and Rewon Child and David Luan and Dario Amodei and Ilya Sutskever},
  title     = {Language Models are Unsupervised Multitask Learners},
  booktitle = {Technical Report},
  year      = {2019}
}

@inproceedings{liu2019roberta,
  author    = {Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
  title     = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  booktitle = {arXiv preprint arXiv:1907.11692},
  year      = {2019}
}

@inproceedings{devlin2018bert,
  author    = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  title     = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  booktitle = {arXiv preprint arXiv:1810.04805},
  year      = {2018}
}




