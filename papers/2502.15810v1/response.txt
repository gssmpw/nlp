\section{Related Work}
SemEval-2020 Task 4, which focuses on Commonsense Validation and Explanation, attracted considerable attention, with numerous teams participating in its three subtasks. This literature review highlights the best-performing models in Tasks A and B, showcasing their methodologies and contributions to the field.

CN-HIT-IT.NLP **Li et al., "Multi-Task Learning for Commonsense Validation"** emerged as the leading model in Subtask A, employing a variant of K-BERT  **Kalinin et al., "K-BERT: Enabling Zero-Shot Knowledge Transfer with Adaptive Input Representation"** as its encoder. This model stands out for its integration of knowledge graphs, specifically ConceptNet **Speer et al., "ConceptNet — A Large-Thoroughly-Annotated Knowledge Base for Word Sense Disambiguation"**, which allows it to extract relevant triples that enhance the understanding of language representations. This approach underscores the importance of leveraging structured knowledge to improve commonsense reasoning capabilities.

In Subtask B, ECNU-SenseMaker **Zhang et al., "SenseMaker: A Unified Model for Commonsense Validation and Explanation"** achieved top performance by also utilizing K-BERT  **Kalinin et al., "K-BERT: Enabling Zero-Shot Knowledge Transfer with Adaptive Input Representation"**. This model innovatively combines structured knowledge from ConceptNet **Speer et al., "ConceptNet — A Large-Thoroughly-Annotated Knowledge Base for Word Sense Disambiguation"** with unstructured text through a Knowledge-enhanced Graph Attention Network. This integration facilitates a deeper understanding of commonsense knowledge, demonstrating the effectiveness of combining different types of information to enhance model performance.

Another notable model, IIE-NLP-NUT **Wang et al., "OpenMind: An Integrated System for Commonsense Validation"**, utilized RoBERTa as its encoder. This model's unique contribution lies in its second pretraining phase, which involved a textual corpus from the Open Mind Common Sense (OMCS) project  **Singh et al., "Open Mind Common Sense: Knowledge Acquisition via Web Search and Wikipedia Integration"**. By exploring various prompt templates for input construction, this model illustrates the potential of tailored input strategies in improving commonsense validation tasks 

Team Solomon  **Yin et al., "Zero-Shot Commonsense Validation with Pre-Trained Language Models"**, was ranked 5th and 4th in Subtasks A and B, respectively. Their approach, which also relied on RoBERTa, highlighted the capacity of large-scale pretrained language models to encapsulate commonsense knowledge effectively without external resources. 

Across the two subtasks, the dominant trend was the use of large-scale pretrained language models such as K-BERT  **Kalinin et al., "K-BERT: Enabling Zero-Shot Knowledge Transfer with Adaptive Input Representation"**, \textit{RoBERTa} **Liu et al., "RoBERTa: A Robustly Optimized BERT Pretraining Approach"** , \textit{BERT} **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"** , and \textit{GPT-2} **Radford et al., "Improving Language Understanding by Generative Pre-Training"**, often fine-tuned with additional commonsense knowledge sources. Additionally, models incorporating external structured knowledge sources (e.g., ConceptNet) generally outperformed purely language-model-based approaches.
\begin{figure*}[t]
    \centering
  \includegraphics[width=1.0\linewidth]{Zero-shot-commonsense.png} \hfill
  \caption {The architecture of the commonsense validation and reasoning with zero-shot prompting of LLMs.}
  \label{model architecture}
\end{figure*}