@InProceedings{10.1007/3-540-36124-3_77,
author="Singh, Push
and Lin, Thomas
and Mueller, Erik T.
and Lim, Grace
and Perkins, Travell
and Li Zhu, Wan",
editor="Meersman, Robert
and Tari, Zahir",
title="Open Mind Common Sense: Knowledge Acquisition from the General Public",
booktitle="On the Move to Meaningful Internet Systems 2002: CoopIS, DOA, and ODBASE",
year="2002",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="1223--1237",
abstract="Open Mind Common Sense is a knowledge acquisition system designed to acquire commonsense knowledge from the general public over the web. We describe and evaluate our first fielded system, which enabled the construction of a 450,000 assertion commonsense knowledge base. We then discuss how our second-generation system addresses weaknesses discovered in the first. The new system acquires facts, descriptions, and stories by allowing participants to construct and fill in natural language templates. It employs word-sense disambiguation and methods of clarifying entered knowledge, analogical inference to provide feedback, and allows participants to validate knowledge and in turn each other.",
isbn="978-3-540-36124-4"
}

@inproceedings{10.5555/3298023.3298212,
author = {Speer, Robyn and Chin, Joshua and Havasi, Catherine},
title = {ConceptNet 5.5: an open multilingual graph of general knowledge},
year = {2017},
publisher = {AAAI Press},
abstract = {Machine learning about language can be improved by supplying it with specific knowledge and sources of external information. We present here a new version of the linked open data resource ConceptNet that is particularly well suited to be used with modern NLP techniques such as word embeddings.ConceptNet is a knowledge graph that connects words and phrases of natural language with labeled edges. Its knowledge is collected from many sources that include expert-created resources, crowd-sourcing, and games with a purpose. It is designed to represent the general knowledge involved in understanding language, improving natural language applications by allowing the application to better understand the meanings behind the words people use.When ConceptNet is combined with word embeddings acquired from distributional semantics (such as word2vec), it provides applications with understanding that they would not acquire from distributional semantics alone, nor from narrower resources such as WordNet or DBPedia. We demonstrate this with state-of-the-art results on intrinsic evaluations of word relatedness that translate into improvements on applications of word vectors, including solving SAT-style analogies.},
booktitle = {Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence},
pages = {4444â€“4451},
numpages = {8},
location = {San Francisco, California, USA},
series = {AAAI'17}
}

@inproceedings{devlin2018bert,
  author    = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  title     = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  booktitle = {arXiv preprint arXiv:1810.04805},
  year      = {2018}
}

@inproceedings{liu2019kbert,
  author    = {Weijie Liu and Peng Zhou and Zhe Zhao and Zhiruo Wang and Qi Ju and Haotang Deng and Ping Wang},
  title     = {K-BERT: Enabling Language Representation with Knowledge Graph},
  booktitle = {arXiv preprint arXiv:1909.07606},
  year      = {2019}
}

@inproceedings{liu2019roberta,
  author    = {Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
  title     = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  booktitle = {arXiv preprint arXiv:1907.11692},
  year      = {2019}
}

@inproceedings{radford2019gpt2,
  author    = {Alec Radford and Jeff Wu and Rewon Child and David Luan and Dario Amodei and Ilya Sutskever},
  title     = {Language Models are Unsupervised Multitask Learners},
  booktitle = {Technical Report},
  year      = {2019}
}

@inproceedings{srivastava-etal-2020-team,
    title = "Team {S}olomon at {S}em{E}val-2020 Task 4: Be Reasonable: Exploiting Large-scale Language Models for Commonsense Reasoning",
    author = "Srivastava, Vertika  and
      Sahoo, Sudeep Kumar  and
      Kim, Yeon Hyang  and
      R.r, Rohit  and
      Raj, Mayank  and
      Jaiswal, Ajay",
    editor = "Herbelot, Aurelie  and
      Zhu, Xiaodan  and
      Palmer, Alexis  and
      Schneider, Nathan  and
      May, Jonathan  and
      Shutova, Ekaterina",
    booktitle = "Proceedings of the Fourteenth Workshop on Semantic Evaluation",
    month = dec,
    year = "2020",
    address = "Barcelona (online)",
    publisher = "International Committee for Computational Linguistics",
    url = "https://aclanthology.org/2020.semeval-1.74/",
    doi = "10.18653/v1/2020.semeval-1.74",
    pages = "585--593",
    abstract = "In this paper, we present our submission for SemEval 2020 Task 4 - Commonsense Validation and Explanation (ComVE). The objective of this task was to develop a system that can differentiate statements that make sense from the ones that don`t. ComVE comprises of three subtasks to challenge and test a system`s capability in understanding commonsense knowledge from various dimensions. Commonsense reasoning is a challenging task in the domain of natural language understanding and systems augmented with it can improve performance in various other tasks such as reading comprehension, and inferencing. We have developed a system that leverages commonsense knowledge from pretrained language models trained on huge corpus such as RoBERTa, GPT2, etc. Our proposed system validates the reasonability of a given statement against the backdrop of commonsense knowledge acquired by these models and generates a logical reason to support its decision. Our system ranked 2nd in subtask C with a BLEU score of 19.3, which by far is the most challenging subtask as it required systems to generate the rationale behind the choice of an unreasonable statement. In subtask A and B, we achieved 96{\%} and 94{\%} accuracy respectively standing at 4th position in both the subtasks."
}

@inproceedings{xing-etal-2020-iie,
    title = "{IIE}-{NLP}-{NUT} at {S}em{E}val-2020 Task 4: Guiding {PLM} with Prompt Template Reconstruction Strategy for {C}om{VE}",
    author = "Xing, Luxi  and
      Xie, Yuqiang  and
      Hu, Yue  and
      Peng, Wei",
    editor = "Herbelot, Aurelie  and
      Zhu, Xiaodan  and
      Palmer, Alexis  and
      Schneider, Nathan  and
      May, Jonathan  and
      Shutova, Ekaterina",
    booktitle = "Proceedings of the Fourteenth Workshop on Semantic Evaluation",
    month = dec,
    year = "2020",
    address = "Barcelona (online)",
    publisher = "International Committee for Computational Linguistics",
    url = "https://aclanthology.org/2020.semeval-1.42/",
    doi = "10.18653/v1/2020.semeval-1.42",
    pages = "346--353",
    abstract = "This paper introduces our systems for the first two subtasks of SemEval Task4: Commonsense Validation and Explanation. To clarify the intention for judgment and inject contrastive information for selection, we propose the input reconstruction strategy with prompt templates. Specifically, we formalize the subtasks into the multiple-choice question answering format and construct the input with the prompt templates, then, the final prediction of question answering is considered as the result of subtasks. Experimental results show that our approaches achieve significant performance compared with the baseline systems. Our approaches secure the third rank on both official test sets of the first two subtasks with an accuracy of 96.4 and an accuracy of 94.3 respectively."
}

@inproceedings{zhang2020cn,
  author    = {Yice Zhang and Jiaxuan Lin and Yang Fan and Peng Jin and Yuanchao Liu and Bingquan Liu},
  title     = {CN-HIT-IT.NLP at SemEval-2020 Task 4: Enhanced Language Representation with Multiple Knowledge Triples},
  booktitle = {Proceedings of The 14th International Workshop on Semantic Evaluation},
  year      = {2020},
  publisher = {Association for Computational Linguistics}
}

@inproceedings{zhao2020ecnu,
  author    = {Qian Zhao and Siyu Tao and Jie Zhou and Linlin Wang and Xin Lin},
  title     = {ECNU-SenseMaker at SemEval-2020 Task 4: Leveraging Heterogeneous Knowledge Resources for Commonsense Validation and Explanation},
  booktitle = {Proceedings of The 14th International Workshop on Semantic Evaluation},
  year      = {2020},
  publisher = {Association for Computational Linguistics}
}

