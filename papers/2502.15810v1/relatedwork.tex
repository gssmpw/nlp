\section{Related Work}
SemEval-2020 Task 4, which focuses on Commonsense Validation and Explanation, attracted considerable attention, with numerous teams participating in its three subtasks. This literature review highlights the best-performing models in Tasks A and B, showcasing their methodologies and contributions to the field.

CN-HIT-IT.NLP \cite{zhang2020cn} emerged as the leading model in Subtask A, employing a variant of K-BERT  \cite{liu2019kbert} as its encoder. This model stands out for its integration of knowledge graphs, specifically ConceptNet \cite{10.5555/3298023.3298212}, which allows it to extract relevant triples that enhance the understanding of language representations. This approach underscores the importance of leveraging structured knowledge to improve commonsense reasoning capabilities.

In Subtask B, ECNU-SenseMaker \cite{zhao2020ecnu} achieved top performance by also utilizing K-BERT \cite{liu2019kbert}. This model innovatively combines structured knowledge from ConceptNet \cite{10.5555/3298023.3298212} with unstructured text through a Knowledge-enhanced Graph Attention Network. This integration facilitates a deeper understanding of commonsense knowledge, demonstrating the effectiveness of combining different types of information to enhance model performance.

Another notable model, IIE-NLP-NUT \cite{xing-etal-2020-iie}, utilized RoBERTa as its encoder. This model's unique contribution lies in its second pretraining phase, which involved a textual corpus from the Open Mind Common Sense (OMCS) project \cite{10.1007/3-540-36124-3_77}. By exploring various prompt templates for input construction, this model illustrates the potential of tailored input strategies in improving commonsense validation tasks 


Team Solomon \cite{srivastava-etal-2020-team} was ranked 5th and 4th in Subtasks A and B, respectively. Their approach, which also relied on RoBERTa, highlighted the capacity of large-scale pretrained language models to encapsulate commonsense knowledge effectively without external resources. 

Across the two subtasks, the dominant trend was the use of large-scale pretrained language models such as  K-BERT  \cite{liu2019kbert}, \textit{RoBERTa} \cite{liu2019roberta}, \textit{BERT} \cite{devlin2018bert}, and \textit{GPT-2} \cite{radford2019gpt2}, often fine-tuned with additional commonsense knowledge sources. Additionally, models incorporating external structured knowledge sources (e.g., ConceptNet) generally outperformed purely language-model-based approaches.
\begin{figure*}[t]
    \centering
  \includegraphics[width=1.0\linewidth]{Zero-shot-commonsense.png} \hfill
  \caption {The  architecture of the commonsense validation and reasoning with zero-shot prompting of LLMs.}
  \label{model architecture}
\end{figure*}