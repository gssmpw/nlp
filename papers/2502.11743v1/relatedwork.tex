\section{Related Work}
\label{sec:related-work}
This section separately details related work on PLL and on making predictions more robust regarding aspects (a)\,--\,(c).

\subsection{Partial-Label Learning (PLL)}
PLL is a typical weakly-supervised learning problem.
Early approaches apply common supervised learning frameworks to the PLL context:
\citet{grandvalet2002logistic} propose a logistic regression formulation,
\citet{JinG02} propose an expectation-maximization strategy,
\citet{HullermeierB06} propose a nearest-neighbors method,
\citet{NguyenC08} propose an extension of support-vector machines,
and \citet{CourST11} introduce an average loss formulation allowing the use of any supervised method.

As most of the aforementioned algorithms struggle with non-uniform noise, several extensions and novel methods have been proposed:
\citet{ZhangY15a,ZhangZL16,XuLG19,WangLZ19,FengA19,NiZDCL21} leverage ideas from representation learning,
\citet{YuZ17,WangLZ19,FengA19,NiZDCL21} extend the maximum-margin idea,
\citet{LiuD12,LvXF0GS20} propose extensions of the expectation-maximization strategy,
\citet{ZhangYT17,TangZ17,WuZ18} propose stacking and boosting ensembles,
and \citet{LvXF0GS20,CabannesRB20} introduce a minimum loss formulation, which iteratively disambiguates the partial labels.

The progress of deep-learning techniques also yields advances in PLL.
\citet{FengL0X0G0S20,LvXF0GS20} provide risk-consistent loss formulations for PLL and \citet{XuQGZ21,WangXLF0CZ22,HeFLLY22,ZhangF0L0QS22,0009LLQG23,fan2024kmt,crosel2024} use advances in deep representation learning and data augmentation to strengthen inference from PLL data.
Similar to our approach, \textsc{Cavl} \citep{ZhangF0L0QS22} makes use of the class activation values to identify the correct labels in the candidate sets.
While they use the activation values as a feature map, we use the activation values to build multinomial opinions in subjective logic, which reflect the involved uncertainty in prediction-making.

Similar to \textsc{Proden} \citep{LvXF0GS20}, \textsc{Pop} \citep{0009LLQG23}, and \textsc{CroSel} \citep{crosel2024}, among many others, we iteratively update a label weight vector keeping track of the model's knowledge about the labeling of all instances.
However, those three methods do not provide any formal reasoning for their respective update rules.
In contrast, we prove our update rule's optimality in Proposition~\ref{prop:optimal} and Proposition~\ref{prop:optimal-ce} for the mean-squared error and cross-entropy error, respectively.

We note that, at a first glance, our update rule also appears to be similar to the label smoothing proposed by \citet{GongB024}.
Based on a smoothing hyperparameter~$r \in [0, 1]$, they iteratively update the label weights: $r = 1$ uniformly allocates weight among all class labels, while $r = 0$ only allocates label weight on the most likely label.
In contrast, our update strategy does not involve any hyperparameter and allocates probability mass based on the uncertainty involved in prediction-making.

\subsection{Robust Prediction-Making}
Robust prediction-making encompasses a variety of aspects out of which we consider (a) good predictive performance under high PLL noise \citep{ZhangZW0021}, (b) robustness against out-of-distribution examples (OOD; \citealt{SensoyKK18}), and (c) robustness against adversarial examples \citep{MadryMSTV18} to be the most important in PLL.
Real-world applications of PLL often entail web mining use cases, where the closed-world assumption usually does not hold (requiring (b)).
Also, PLL training data is commonly human-based and therefore a possible surface for adversarial attacks (requiring (c)).
Other robustness objectives that we do \emph{not} consider are, for example, the decomposition of the involved uncertainties \citep{KendallG17,WimmerSHBH23} or the calibration of the prediction confidences \citep{AoRS23,MortierBHLW23}.

To address (a) in the supervised setting, one commonly employs Bayesian methods \citep{KingmaW13,KendallG17} or ensembles\footnote{Ensemble techniques also benefit (b) and (c) and are easy to implement. Therefore, we also consider an ensemble approach of one of our competitors in our experiments as a strong baseline in Section~\ref{sec:exp}.} \citep{Lakshminarayanan17,WimmerSHBH23}.
To recognize OOD samples (b), one commonly employs techniques from representation learning \citep{ZhangY15a,XuQGZ21} or leverages negative examples using regularization or contrastive learning \citep{SensoyKK18,WangXLF0CZ22}.
To address (c), methods incorporate adversarially corrupted features already in the training process to strengthen predictions \citep{Lakshminarayanan17}.
To the best of our knowledge, we are the first to propose a method that addresses (a), (b), and (c) in PLL.
Tackling all three aspects is particularly challenging in the PLL domain as there is no exact ground truth on which an algorithm can rely to build robust representations.