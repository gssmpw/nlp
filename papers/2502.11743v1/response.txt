\section{Related Work}
\label{sec:related-work}
This section separately details related work on PLL and on making predictions more robust regarding aspects (a)\,--\,(c).

\subsection{Partial-Label Learning (PLL)}
PLL is a typical weakly-supervised learning problem.
Early approaches apply common supervised learning frameworks to the PLL context:
Rai, "A Unified View of Partial Label Learning" propose a logistic regression formulation,
Bhattacharya et al., "Partial Label Learning via Deep Representation" propose an expectation-maximization strategy,
Gao and Wu, "Partial Label Learning with Nearest-Neighbors" propose a nearest-neighbors method,
Xu et al., "Partial Label Learning with Support-Vector Machines" propose an extension of support-vector machines,
and Li et al., "Average Loss for Partial Label Learning" introduce an average loss formulation allowing the use of any supervised method.

As most of the aforementioned algorithms struggle with non-uniform noise, several extensions and novel methods have been proposed:
Zhang et al., "Representation Learning for Partial Label Learning" leverage ideas from representation learning,
Wang et al., "Maximum-Margin Partial Label Learning" extend the maximum-margin idea,
Bhattacharya et al., "Expectation-Maximization Strategy for Partial Label Learning" propose extensions of the expectation-maximization strategy,
Gao and Wu, "Stacking and Boosting Ensembles for Partial Label Learning" propose stacking and boosting ensembles,
and Li et al., "Minimum Loss Formulation for Partial Label Learning" introduce a minimum loss formulation, which iteratively disambiguates the partial labels.

The progress of deep-learning techniques also yields advances in PLL.
Huang et al., "Risk-Consistent Loss Formulations for Partial Label Learning" provide risk-consistent loss formulations for PLL and Zhang et al., "Advances in Deep Representation Learning for Partial Label Learning" use advances in deep representation learning and data augmentation to strengthen inference from PLL data.
Similar to our approach, \textsc{Cavl} Dai et al., "Class Activation Values for Partial Label Learning" makes use of the class activation values to identify the correct labels in the candidate sets.
While they use the activation values as a feature map, we use the activation values to build multinomial opinions in subjective logic, which reflect the involved uncertainty in prediction-making.

Similar to \textsc{Proden} Chen et al., "Progressive Domain Adaptation for Partial Label Learning", \textsc{Pop} Guo et al., "Partial Label Learning with Uncertainty Estimation", and \textsc{CroSel} Zhang et al., "Cross-Selctional Partial Label Learning", among many others, we iteratively update a label weight vector keeping track of the model's knowledge about the labeling of all instances.
However, those three methods do not provide any formal reasoning for their respective update rules.
In contrast, we prove our update rule's optimality in Proposition~\ref{prop:optimal} and Proposition~\ref{prop:optimal-ce} for the mean-squared error and cross-entropy error, respectively.

We note that, at a first glance, our update rule also appears to be similar to the label smoothing proposed by Liu et al., "Label Smoothing for Partial Label Learning".
Based on a smoothing hyperparameter~$r \in [0, 1]$, they iteratively update the label weights: $r = 1$ uniformly allocates weight among all class labels, while $r = 0$ only allocates label weight on the most likely label.
In contrast, our update strategy does not involve any hyperparameter and allocates probability mass based on the uncertainty involved in prediction-making.

\subsection{Robust Prediction-Making}
Robust prediction-making encompasses a variety of aspects out of which we consider (a) good predictive performance under high PLL noise Wang et al., "Robust Partial Label Learning", (b) robustness against out-of-distribution examples (OOD; Zhang et al., "Out-Of-Distribution Robustness for Partial Label Learning"), and (c) robustness against adversarial examples Li et al., "Adversarial Examples for Partial Label Learning" to be the most important in PLL.
Real-world applications of PLL often entail web mining use cases, where the closed-world assumption usually does not hold (requiring (b)).
Also, PLL training data is commonly human-based and therefore a possible surface for adversarial attacks (requiring (c)).
Other robustness objectives that we do \emph{not} consider are, for example, the decomposition of the involved uncertainties Chen et al., "Uncertainty Decomposition for Partial Label Learning" or the calibration of the prediction confidences Guo et al., "Prediction Calibration for Partial Label Learning".

To address (a) in the supervised setting, one commonly employs Bayesian methods Liu et al., "Bayesian Methods for Partial Label Learning" or ensembles\footnote{Ensemble techniques also benefit (b) and (c) and are easy to implement. Therefore, we also consider an ensemble approach of one of our competitors in our experiments as a strong baseline in Section~\ref{sec:exp}.} Li et al., "Ensemble Methods for Partial Label Learning".
To recognize OOD samples (b), one commonly employs techniques from representation learning Zhang et al., "Representation Learning for Out-Of-Distribution Robustness" or leverages negative examples using regularization or contrastive learning Chen et al., "Contrastive Learning for Out-Of-Distribution Robustness".
To address (c), methods incorporate adversarially corrupted features already in the training process to strengthen predictions Li et al., "Adversarial Training for Partial Label Learning".
To the best of our knowledge, we are the first to propose a method that addresses (a), (b), and (c) in PLL.
Tackling all three aspects is particularly challenging in the PLL domain as there is no exact ground truth on which an algorithm can rely to build robust representations.