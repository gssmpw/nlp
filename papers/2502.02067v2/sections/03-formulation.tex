% \nabanita{Resource Description Framework (RDF)}

% The Resource Description Framework (RDF) is a flexible framework for representing and sharing structured data on the web. It expresses relationships between entities, making it valuable for describing and exchanging metadata across systems. Its standardized approach enhances data integration and interoperability.

% \nabanita{Set-Theoretic Representation of RDF}

% In set-theoretic terms, an RDF triple can be expressed as a binary relation from a set of elements of \( S \) to a set of elements of \( O \). RDF can be expressed in triple format:
% \[
% T = (S, P, O)
% \]
% where \( T \) (the RDF triple) comprises:
% \begin{itemize}
%     \item \( S \) (subject): the set of entities or resources being referred to.
%     \item \( P \) (predicate): the binary relation or property connecting the set of entities \( S \) and their corresponding values \( O \).
%     \item \( O \) (object): the set of values of the referred entities, or another entity being linked to.
% \end{itemize}

% Formally, we can define the binary relation between \( S \) and \( O \) over \( P \) as:
% \[
% P(S) = O
% \]

% For example, let \( P \) represent a function detecting whether an entity is "boilable" or "isboiled," \( S \) represent certain food items such as milk, apple, cucumber, and egg, and \( O \) represent the value (true or false). We can express this as:

% \begin{align*}
% \text{boilable}(\text{milk}) &= \text{true} \\
% \text{boilable}(\text{apple}) &= \text{false} \\
% \text{boilable}(\text{cucumber}) &= \text{false} \\
% \text{boilable}(\text{egg}) &= \text{true}
% \end{align*}

% \begin{align*}
% \text{isboiled}(\text{milk}) &= \text{false} \\
% \text{isboiled}(\text{egg}) &= \text{true}
% \end{align*}

% In some other cases, \( O \) can represent another entity:
% \[
% \text{isOnTopOf}(\text{lemon}) = \text{cutting board}
% \]
% \[
% \text{contentsIn}(\text{pan}) = \text{tea}
% \]

% \nabanita{Knowledge Expansion (KE) in RDF Schema}

% In RDF Schema, Knowledge Expansion (KE) refers to extending relationships between classes, their instances, and properties across different actions or attributes. RDF Schema models these hierarchical relationships using constructs like \texttt{subClassOf} and \texttt{subPropertyOf}, allowing knowledge to be reused, inferred, and extended from broader categories to more specific ones.

% \nabanita{Class and Property Relationships}

% The triple relations can also be extended to class relationships, such as:
% \[
% \text{classOf}(\text{Apple}) = \text{Fruit}
% \]
% \[
% \text{classOf}(\text{Granny Smith apple}) = \text{Apple}
% \]

% Hence:
% \begin{enumerate}
%     \item Any instance of the class \( \text{Apple} \) is also an instance of the class \( \text{Fruit} \):
%     \[
%     \forall x \, (x \in \text{Granny Smith Apple} \Rightarrow x \in \text{Fruit})
%     \]
    
%     \item The properties of the class \( \text{Fruit} \) extend to the class \( \text{Granny Smith Apple} \):
%     \[
%     \forall x, y \, (P(x, y) \Rightarrow Q(x, y))
%     \]
%     For instance, if \( \text{isSliceable}(\text{Fruit}) \) holds, then \( \text{isSliceable}(\text{Granny Smith Apple}) \) must also hold.

%     \item The subproperties of \( \text{cut} \) extend to more specific actions such as \( \text{slice}, \text{dice}, \text{chop} \):
%     \[
%     \text{subPropertyOf}(\text{cut}) = \text{slice}
%     \]
% \end{enumerate}

%%%%%%%%%%%%%%% NEED FIX 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Problem Formulation and Framework}
\vspace{-0.5em}
\label{sec:framework}
Figure~\ref{fig:pipeline} is an outline of our framework. In the motivating example, an agent assisting in cooking tasks in a kitchen has access to relevant objects and ingredients for many dishes but it does not have the recipes. When asked to prepare any particular dish, $\tau_i$, the agent queries an LLM to obtain a sequence of abstract actions (sub-tasks), i.e., $\langle a_1, \ldots, a_{m_i}\rangle$. For example, the sequence for \textit{make an omelette} includes \textit{picking up the egg} and \textit{breaking the egg over a skillet}. This sequence of abstract actions is checked against a KG with some domain-specific information in the form of existing objects and attributes that include the actions that can be performed on some objects. The agent tries to resolve any discrepancy between the LLM output and KG, e.g., KG states there is no skillet or that an egg can only be cracked, by finding replacements, e.g., \textit{crack the egg over a pan}. If the discrepancy is not resolved, or if executing the action sequence does not provide the desired outcome, the agent identifies relevant actions and solicits human input to refine the KG, e.g., add knowledge of objects or their attributes, and provides an action sequence to complete the task. The agent is assumed to be able to execute these actions. We describe out framework's components below.


%Since the agent can be given a different dish with a different ingredient requirement, it decomposes the recipe making into actions relevant to the dish. Our framework in  leverages the generic knowledge and the stochastic nature of LLMs to decompose the task into an action sequence with limited prompting. The initial output from the LLMs goes through series of checks and refinement in the refinement block where the KG is queried for the property checks of the ingredients and so on. We give feedbacks in case of persistent errors and if not resolved, the human gives an input for correction and in some cases expands the knowledge which is discussed in section \ref{sec:kg_expansion}. We describe the components of our framework below: 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Generic Task Decomposition with LLM}
\label{sec:framework-llm}
In our framework, we use an LLM to decompose any given task into a sequence of sub-tasks because LLMs have demonstrated the ability to provide such a sequence of abstract actions for many different tasks. Specifically, in the motivating example, the LLM is prompted with information about some domain objects, an example cooking task (make coffee), and the corresponding action sequence (recipe) to be executed---see Figure~\ref{fig:pipeline}a. We experimentally evaluate the use of different LLMs, as described in Section~\ref{sec:expres-setup}. 
%Our choice of using the LLMs to decompose the task into action sequence is motivated by two objectives: (i) With the generic knowledge and in addition to one-shot prompting technique, we aim to achieve an action output which are direct function calls during the execution; and (ii) To get variability in the output and with human-in-the-loop in play, a possibility of expanding the knowledge wherever necessary. As described in Section~\ref{sec:expt}, we explored the use of popular LLMs such as GPT-3.5\cite{brown2020language}, GPT-4o\cite{OpenAI2023GPT4TR}, Llama3-70b (cite), and Gemma2-9b (cite).

\vspace{-0.75em}
Since the sequence of sub-tasks predicted by the LLM is based on many information sources, it may not be possible to execute one or more of these actions. For example, in the context of cooking tasks, the suggested ingredient may not be available or the action may involve an incorrect choice of tool (e.g., using a fork to cut vegetables). These situations can be addressed in part by using prior domain-specific information, which is encoded as described below.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Representing Domain-specific Knowledge with KG}
\label{sec:framework-kg}
Our framework uses a Knowledge Graph (KG) to encode any prior information available to the agent. In the context of cooking tasks, this includes knowledge of some classes of ingredients (e.g., herbs, fruits, vegetables), receptacles (e.g., plates, bowls, countertop), and tools (e.g. knives, spoons), which can be arranged hierarchically. It also encodes the existence of some specific instances of these object classes and their properties such as likely location(s) and the actions they can be involved in (e.g., cutting, scooping, grinding). We use the \textit{Resource Description Framework} (RDF) format to encode this information in two graph structures in Turtle format (.ttl file)---see Figure~\ref{fig:kg-nodes}:
%The environment we are working with is described using a structured graph-based representation. It consists of rooms (e.g., kitchen, etc.), various objects (e.g., apples, mangoes, etc.), receptacles (e.g., plates, bowls, stoves, etc.), and tools (e.g., knives, spoons, etc.). Each item and its properties are represented in a graph structure, with nodes denoting the items and edges denoting the properties or states of these items. Specifically, we use a  data model to represent both the environment's current state and its underlying knowledge. We define two distinct graphs:
\begin{enumerate}
\vspace{-0.75em}
\item \textbf{State graph:} models current state as $\mathbf{G_s} = (\mathbf{I_s}, \mathbf{E_s})$, where nodes $\mathbf{I_s}$ are instances of object classes such as ingredients and receptacles; and $\mathbf{E_s} \subseteq \mathbf{I_s} \times \mathbf{P_s} \times \mathbf{V_s}$ are edges such that $(i_j, p, v_k) \in \mathbf{E_s}$ is a triple denoting an attribute of $i_j \in \mathbf{I_s}$ in terms of value $v_k \in \mathbf{V_s}$ of predicate $p \in \mathbf{P_s}$. For example, (apple1, obj\_location, fridge) and (apple1, is\_sliced, true) express \textit{apple1's} location and that it is sliced.
% \begin{enumerate} 
%     \item $\mathbf{I_s}$ represents the set of nodes corresponding to the items in the environment (objects, receptacles, and tools).
%     \item $\mathbf{E_s} \subseteq \mathbf{I_s} \times \mathbf{P_s} \times \mathbf{V_s}$ represents the set of edges. Each edge $(i_j, p, v_k) \in \mathbf{E_s}$ is a triple that denotes an item $i_j \in \mathbf{I_s}$, a predicate $p \in \mathbf{P_s}$ (representing the state of the object), and a value $v_k \in \mathbf{V_s}$ that describes the state of the item. The value could either be a boolean or another item in the environment. For example, a triple in the state graph could be (apple, obj\_location, fridge) and (apple, is\_sliced, true), indicating that the apple is stored in the fridge and that its current state is \textit{sliced}.
% \end{enumerate} 
\item \textbf{Attribute graph:} encodes the known properties and action capabilities of some object classes as $\mathbf{G_k} = (\mathbf{I_k}, \mathbf{E_k})$, where nodes $\mathbf{I_k}$ represent the classes and edges $\mathbf{E_k} \subseteq \mathbf{I_k} \times \mathbf{P_k} \times \mathbf{V_k}$ represent class properties, e.g., (apple, sliceable, true) implies  apples can be sliced.
\vspace{-0.75em}
\end{enumerate}
%In both graphs, the sets of predicates $\mathbf{P_s}$ and $\mathbf{P_k}$ define possible states or relations, such as IsBoiled, Boilable, IsCleaned, NeedsToBeCleaned, etc. 
The available actions include moving, picking up, and putting down objects; using tools; cleaning, toggling, slicing, stirring, and mopping\footnote{Supplementary material includes list of all the actions.}. Such a KG can be learned automatically based on information extracted from datasets or sensor streams. The feasibility of any action/sub-task in the sequence predicted by LLM is then checked using $\mathbf{G_k}$ and $\mathbf{G_s}$ by generating suitable SPARQL queries. If the predicted sequence of actions passes the KG-based check, it is executed, changing $\mathbf{G_s}$ suitably.
%The agent's task is to achieve a goal by performing a sequence of actions on the items present in the environment. Each action modifies and updates the state graph $\mathbf{G_s}$, reflecting the changes in the environment. The agent is equipped with a predefined skill set $\mathbf{A} = \{a_1, a_2,...,a_n\}$, where each action $a_i$ operates on items in the state graph and updates their states. These actions include basic manipulations such as 

%Each action can be formalized as a function: $T_a: \mathbf{G_s} \times \mathbf{G_k} \Rightarrow \mathbf{G_s}^{'}$, where $T_a$ applies the action $a$ to the current state graph $\mathbf{G_s}$, and the knowledge graph $\mathbf{G_k}$ is used to verify if the action is applicable (e.g., whether an item is fryable or boilable). The result is a new state graph $\mathbf{G_s}^{'}$, which represents the updated states of the items in the environment.
%To determine the feasibility of an action, SPARQL queries are used. For each action $a \in \mathbf{A}$, a SPARQL query is generated to check the preconditions in both $\mathbf{G_s}$ and $\mathbf{G_k}$. For instance, The action \texttt{\small{slice(apple, knife, state\_file)}} refers to slicing an apple using a knife, with the state file used to extract the current state of all the items necessary to perform this action. To perform this action, the query ensures that the apple is in the correct location (e.g., on the slicing board) and is sliceable according to the knowledge graph $\mathbf{G_k}$. Upon successful verification, the action is performed, and the state graph $\mathbf{G_s}$ is updated with the new status (e.g., \texttt{\small{(apple, sliced, true))}}.
%The flexibility of RDF allows for easy modifications and updates to the environment model, supporting dynamic task execution and refinement. The combination of the state and knowledge graphs enables the agent to make informed decisions and adapt to changes in the environment.
\begin{figure}[tb]
\captionsetup{font=scriptsize}
\centering
    \begin{minipage}{0.3\textwidth}
        \begin{lstlisting}[basicstyle=\ttfamily\scriptsize]
ex:onion rdf:type ex:object ;
    ex:obj_name 'onion' ;
    ex:IsSliceable true ;
    ex:Fryable true ;
    ex:NeedsToBeCleaned true .
        \end{lstlisting}
        % \caption*{(a)}
    \end{minipage}
    % \setlength{\abovecaptionskip}{-2pt}
    % \setlength{\belowcaptionskip}{-12pt}
    \hfill
    % \caption{Example of a node \textit{onion} in $\mathbf{G_k}$.}
    % \label{fig:12}
% \end{figure}

% \begin{figure}[tb]
% \captionsetup{font=scriptsize}
% \centering
    \begin{minipage}{0.3\textwidth}
        \begin{lstlisting}[basicstyle=\ttfamily\scriptsize]
ex:onion rdf:type ex:object ;
    ex:obj_name 'onion' ;
    ex:obj_location ex:fridge .
    ex:sliced false ;
    ex:IsFried false ;
    ex:IsCleaned false .
        \end{lstlisting}
        % \caption*{(b)}
    \end{minipage}
    \setlength{\abovecaptionskip}{-2pt}
    \setlength{\belowcaptionskip}{-12pt}
    \caption{Example of a node \textit{onion} in $\mathbf{G_k}$ (\textit{top}) and $\mathbf{G_s}$ (\textit{bottom}).}
    \label{fig:kg-nodes}
    \vspace{-0.5em}
\end{figure}

% \begin{figure}[tb] 
% \captionsetup{font=scriptsize}
% \centering
%     \begin{minipage}{0.225\textwidth} 
%         \centering
%         \begin{lstlisting}[basicstyle=\ttfamily\scriptsize]
% ex:onion rdf:type ex:object ;
%     ex:obj_name 'onion' ;
%     ex:IsSliceable true ;
%     ex:Fryable true ;
%     ex:NeedsToBeCleaned true .
%         \end{lstlisting}
%         \caption*{(a)} % Sub-caption
%     \end{minipage}
%     \setlength{\abovecaptionskip}{-2pt}
%     \setlength{\belowcaptionskip}{-2pt}
%     \hfill 
%     \begin{minipage}{0.225\textwidth} 
%         \centering
%         \begin{lstlisting}[basicstyle=\ttfamily\scriptsize]
% ex:onion rdf:type ex:object ;
%     ex:obj_name 'onion' ;
%     ex:obj_location ex:fridge .
%     ex:sliced false ;
%     ex:IsFried false ;
%     ex:IsCleaned false .
%         \end{lstlisting}
%         \caption*{(b)} 
%     \end{minipage}
%     \setlength{\abovecaptionskip}{-2pt}
%     \setlength{\belowcaptionskip}{-2pt}
%     \caption{Example of a node \textit{onion} in both $\mathbf{G_k}$ and $\mathbf{G_s}$.}
%     \label{fig:kg-nodes}
%     \vspace{-0.5em}
% \end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Refining LLM output}
\label{sec:framework-refine}
%\shivam{have to add why we preferred string matching over similarity detection}
If a mismatch is detected between the LLM output and the KG, the agent attempts to use the KG to \textit{revise} the action sequence---see Figure~\ref{fig:pipeline}(b). 
%The LLM output often contains inconsistencies in item names and action names, which can hinder the task execution. To address this, we use an Error Detection and Refinement block (Fig \ref{fig:pipeline}(b)), which leverages the knowledge graph (KG) to identify and correct errors in the LLM output. 
Specifically, the agent attempts to replace the text corresponding to the identified mismatch, which can refer to actions, object instances, or object attributes, with other text from the KG.
%This block focuses on resolving inconsistencies by checking the LLM-generated item and action names against the structured knowledge available in the KG.  For unknown items or actions, we employ methods like substring matching and similarity scoring to find close matches. However, since similarity detection performs better with sentences rather than individual words, it was less effective in our case. Substring matching, on the other hand, proved to be more suitable for replacing unknown items and actions with the correct ones. Every action execution involves querying the state graph. Initially, there is an initial state graph $\mathbf{G_s}$, and after an action is executed, it updates to a new state graph $\mathbf{G'_s}$. In an action sequence, the first action should query the initial state graph to extract the initial state of the environment, while subsequent actions should query the updated graph, as the states change with each action. 
%\nabanita{To rephrase words effectively, it's important to consider two key factors: syntactic similarity and semantic similarity. Syntactic similarity ensures that the sentence's structure stays intact, so the replacement fits grammatically. Semantic similarity, on the other hand, guarantees that the new word preserves the original meaning. Using hypernyms (broader terms) or hyponyms (more specific terms) can also help to fine-tune the level of detail or generality, ensuring the overall concept remains accurate. By balancing these elements, our system can swap words smoothly without distorting meaning or disrupting the task sequence's flow.} 
While performing such text replacement, it is important to consider syntactic similarity, which measures similarity in the structure (e.g., of words or sentences), and semantic similarity, which considers similarity in meaning.
In our framework, the agent can compute the similarity of the identified words (or their embedding) with words from a similar category (or their embedding) in the KG. The use of word embeddings requires additional contextual information and makes it difficult to understand the revision of the LLM output. We thus chose to use the direct matching of words while considering hypernyms (broader terms) or hyponyms (more specific terms) for simplicity, ease of use, and transparency. If the agent is able to replace all identified mismatches, it executes the actions. 
% The refinement block also takes care of wrong action - item associations; for e.g., it will change pick\_up\_obj(plate, onto\_file) to pick_up_rec(plate, onto\_file) and slice(mango, fork, onto\_file) to slice(mango, knife, onto\_file) by detecting that fork is not a SlicingTool and thus replacing it with the tools which have IsSlicingTool property enabled true etc. 
%Through our experiments, we have found that the error detection and refinement is highly effective, as it corrects the majority of the errors present in the LLM output, significantly improving the overall accuracy and consistency of the generated actions and items.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Knowledge refinement with human input}
\label{sec:framework-hitl}
Since the KG is not comprehensive, the agent may not be able to resolve all identified mismatches, e.g., reference to unknown object or action. Also, there may be unexpected action outcomes when the agent executes the action sequence. These situations are handled through re-prompting and human feedback---see Figure~\ref{fig:pipeline}(c).
%The system initiates a feedback loop with the LLM whenever errors are encountered. Feedback is sent in two main scenarios:
Specifically, the agent responds to an unresolved mismatch or erroneous outcome by re-prompting the LLM with additional information (of mismatch or error). If the mismatch or error persists, human input is solicited and used. 

%\begin{enumerate}
%\item \textbf{Errors in the Error Detection and Refinement Block}:The Error Detection and Refinement block aims to correct inconsistencies in the LLM output using the knowledge graph. However, if the block encounters an inconsistency it cannot resolve (e.g., \textit{unknown item/action}), an error is raised and sent back to the LLM via a feedback prompt. The LLM generates an updated output in response to this feedback, and the refinement process is repeated. We put a threshold on the number of times we go back to the LLM. If the errors persist after the feedback threshold is reached, the system escalates the issue to a human for resolution (section~\ref{sec:framework}).
%\item \textbf{Errors During Action Execution:}
%If the refinement block does not raise any errors, meaning the LLM does not produce any actions that are unknown or use items that are unfamiliar to the agent, then the LLM output is sent for execution. During execution, errors may arise due to invalid actions (e.g., performing an incompatible action on an object). In such cases, the system sends feedback to the LLM, detailing the execution error. Similar to the refinement process, there is a limit to how many times the system can query the LLM for corrections. If all feedback attempts fail to resolve the error, the system proceeds with the output, and the human evaluates the final performance. Errors during execution are minimal or nonexistent in the LLM + KG + Human framework, as the refinement block and human resolves most issues beforehand.
%\end{enumerate}
%Note that, the feedback counter is tracked globally across both phases (refinement and execution). This means that if the agent exhausts the feedback limit during the refinement phase, it will no longer be able to send feedback to the LLM during the execution phase. In such cases, the system proceeds directly to execution and human evaluation.
% once the feedback limit has been reached, ensuring efficient use of feedback resources and avoiding redundancy. 
% \subsection{Human-in-the-loop \& knowledge expansion}
% \label{sec:kg_expansion}
%As discussed in the previous section, there are instances when the agent cannot resolve an unknown item/action error, even after exhausting all of its feedback attempts. In such cases, the agent raises a query to a human, seeking confirmation of the existence of the unknown item.
%Our framework incorporates the ability to expand knowledge dynamically through human intervention. When the agent encounters an unknown item or action command from LLM (i.e., item not present in the knowledge and state graph  or action not present in agent's skill set), it initiates a query to a human user.
%\begin{itemize}
\vspace{-0.75em}
\noindent
\textbf{Existence check:} if an action or object in the LLM output does not exist in the KG, there are three possibilities: (1) The agent is mistaking an existing item (action) for another item (action); (2) the entity does not exist in the domain; or (3) the entity exists but is not in the KG. In the first case, human informs the agent about the correct object (or action); in the second case, human denies existence of entity; and in the third case, human confirms the entity's existence and agent interactively obtains entity's attributes\footnote{Supplementary material includes details of questions asked.}. As the agent expands its knowledge, the need for human input progressively decreases.

\vspace{-0.75em}
\noindent
\textbf{Learn attributes:} If human confirms existence of an instance of a new entity, the agent interactively obtains additional details. For example, when informed about an instance of a new object class \textit{onion}, agent incrementally requests information about the object type (e.g., \textit{edible\_object}) and other relevant attributes (e.g., boilable, fryable, location of instance). This knowledge revision can be viewed as correcting (expanding) the knowledge in the KG by revising class attributes in $\mathbf{G_k}$ and instance-specific details in $\mathbf{G_s}$.
%\end{itemize}
%After learning the item's current state and its properties, the agent adds this information to state graph $\mathbf{G_s}$ and the knowledge graph $\mathbf{G_k}$ respectively. The current state of the item is added to $\mathbf{G_s}$ and the item properties gets added to $\mathbf{G_k}$. The item is represented as a new node $I_{new}$ and its states \& properties are added as edges in the respective graphs. The knowledge expansion can be seen as a function $f_{KE}$ :
$$f_{KE}(I_{new}, P_{new}, S_{current}) \Rightarrow {\mathbf{G_{k}^{'}}, \mathbf{G_{s}^{'}}}$$
where $I_{new}$ is the new entity; $P_{new}$ = ${(p_1, v_1), ..., (p_n, v_n)}$ refers to attributes ($p_i$) of entity and their values ($v_i$); $S_{current}$ = ${(s_1, v_1), ..., (s_n, s_n)}$ refers to states $s_i$ and their values $v_i$; and $\mathbf{G^{'}_{k}}$ and $\mathbf{G^{'}_{s}}$ are the updated components of the KG.  For example, new edge is added in $\mathbf{G_s}$ to encode an onion's position and new edge is added in $\mathbf{G_k}$ to encode that an onion can be fried. Note that this update to existing knowledge is fully transparent by design.

%This revision is performed through SPARQL queries. We demonstrate experimentally that this combination of LLM, KG, and HITL revision substantially improves accuracy of task completion, and supports incremental and transparent knowledge revision and adaptation to new classes of tasks. 

%This process is performed internally through SPARQL queries, ensuring that the graph remains consistent and up-to-date. For instance, after querying human about the current states and properties of \textit{onion} the knowledge graph $\mathbf{G_k}$ gets updated with the edges like \texttt{\small{(onion, Fryable, true)}} and state graph $\mathbf{G_s}$ gets updated with the edge \texttt{\small{(onion, obj\_location, fridge)}}. Figure \ref{fig:nodes} illustrates the nodes in $\mathbf{G_k}$ and $\mathbf{G_s}$ with their respective \textit{properties} and \textit{current states}.
    
%This ability to incorporate human knowledge in real-time allows for continuous expansion of the environment, ensuring that the agent can operate in dynamic and evolving settings. It also highlights the flexibility of the RDF model, where updates can be seamlessly integrated without disrupting the overall structure.

\begin{algorithm}[tb]
\caption{LLM + KG + Human Input}
\label{alg:algorithm}
\begin{algorithmic}[1]
\State \textbf{Procedure} LLM\_KG\_Human($\mathbf{G_{s}}$, $\mathbf{G_{k}}$, \textit{ip\_prompt})
    \State $F$ $\leftarrow 0$                     \algorithmiccomment{$F$ is feedback counter}
    \State $T$ $\leftarrow$ call\_LLM(\textit{ip\_prompt}) \algorithmiccomment{$T$ is action sequence}
    \State $T_{refined}$, $\varepsilon_{unkn}$ $\leftarrow$ refine\_sequence($T$, $\mathbf{G_{k}}$, $\mathbf{G_{s}}$)
    %\State \textit{[EDR is error detection and refinement block]}
    \If {NOT $\varepsilon_{unkn}$}                \algorithmiccomment{$\varepsilon_{unkn}$ is unknown\_item error}
        \State $O$, $\varepsilon_{exec}$ $\leftarrow$ execute($T_{refined}$) \algorithmiccomment{$O$ is execution output}
        \null\hfill\algorithmiccomment{$\varepsilon_{exec}$ is execution error}
    \EndIf

    \While {($\varepsilon_{exec}$ OR $\varepsilon_{unkn}$) AND $F$ $<$ $F_{max}$}
        
        \While {$\varepsilon_{unkn}$ AND $F$ $<$ $F_{max}$}
            \State $T^{'}$ $\leftarrow$ call\_LLM(fb\_prompt) \algorithmiccomment{$T^{'}$is updated sequence}
            \State $T^{'}_{refined}$, $\varepsilon_{unkn}$ $\leftarrow$ refine\_sequence($T^{'}$, $\mathbf{G_{k}}$, $\mathbf{G_{s}}$)
            \State $F$ $\leftarrow$ $F$ + 1
        \EndWhile

        \If {$\varepsilon_{unkn}$ AND $F$ == $F_{max}$}
            \State $T^{'}_{refined}$ $\leftarrow$ ask\_human($\varepsilon_{unkn}$) \algorithmiccomment{$\mathbf{G_{k}}, \mathbf{G_{s}} \Rightarrow \mathbf{G_{k}^{'}}, \mathbf{G_{s}^{'}}$}
            \State $O$, $\varepsilon_{exec}$ $\leftarrow$ execute($T^{'}_{refined}$)
            \State \textbf{break}
        \EndIf

        \If {$\varepsilon_{exec}$ AND $F$ $<$ $F_{max}$}
            \State $T^{'}$ $\leftarrow$ call\_LLM(fb\_prompt)
            \State $T^{'}_{refined}$, $\varepsilon_{unkn}$ $\leftarrow$ refine\_sequence($T^{'}$, $\mathbf{G_{k}}$, $\mathbf{G_{s}}$)
            \State $F$ $\leftarrow$ $F$ + 1
        \EndIf

        \If {NOT $\varepsilon_{unkn}$}
            \State $O$, $\varepsilon_{exec}$ $\leftarrow$ execute($T^{'}_{refined}$)
        \EndIf
    \EndWhile

    % \If {(NOT $\varepsilon_{exec}$) OR ($F$ == $F_{max}$)}
    %     \State send\_human\_for\_evaluation($O$)
    % \EndIf

\State \textbf{End Procedure}

\end{algorithmic}
\end{algorithm}

\vspace{-0.75em}
Algorithm~\ref{alg:algorithm} describes the flow of information and control in our framework. The framework takes as input the state graph $\mathbf{G_s}$ and attribute graph $\mathbf{G_k}$, along with an input prompt \textit{(ip\_prompt)} that contains information about the class of tasks, an in-context example, and a query specifying the task the agent must perform. The LLM generates an action sequence $T$ (Line 3), which is refined to $T_{refined}$ using the knowledge in the KG (Line 4). If there are no unresolved mismatches between KG and LLM output ($\varepsilon_{unkn}$), the action sequence is executed, with the outcomes and errors collected for further analysis (Lines 5-7). Any unresolved mismatches or errors in outcome result in a feedback prompt to the LLM, leading to a new predicted sequence of actions $T^{'}$ (Lines 9-13, 19-23). If these mismatches and/or errors persist (beyond threshold $F_{max}$), the agent queries a human, which potentially leads to knowledge refinement, updating $\mathbf{G_k}$ and $\mathbf{G_s}$. After the expansion, the knowledge base is updated, and the refined action sequence is executed and evaluated (Lines 14-18, 24-26). This entire process is repeated until the tasks is completed or some threshold (e.g., time limit) is exceeded.
