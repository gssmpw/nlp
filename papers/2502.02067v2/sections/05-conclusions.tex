\section{Conclusions and Future Work}
\label{sec:conclusions}
%\nabanita{Nabanita: ADD Percentage increase and other comparisons as necessary!}
Embodied agents assisting humans frequently have to complete previously unseen tasks or operate in new scenario. This paper describes a framework that leverages the complementary strengths of Large Language Models (LLMs), Knowledge Graphs (KGs), and Human-in-the-Loop (HITL) feedback to satisfy this requirement. Specifically, the generic task decomposition ability of LLMs is used to predict a sequence of abstract actions to complete any given task. This sequence is adapted to the specific scenario(s) and the task-, agent-, or domain-specific constraints using a KG that encodes prior knowledge of some objects, object attributes, and action capabilities. Any unresolved mismatch between the KG and the LLM output, and any unexpected action outcomes, are addressed by soliciting and using human input. This HITL feedback corrects errors and refines the existing knowledge (in the KG) for subsequent operation. Experimental evaluation in two simulated domains demonstrates substantial performance improvement compared with baselines, and illustrates incremental acquisition of knowledge to adapt to new classes of tasks.

%Our framework successfully merges the unique capabilities of Large Language Models (LLMs), Knowledge Graphs (KGs), and human feedback to improve how embodied agents respond to unfamiliar situations. LLMs provide a foundation by generating high-level action plans, which are then refined with detailed, domain-specific knowledge from KGs. This allows agents to adjust on the fly, even when encountering new objects or tasks. Additionally, incorporating human-in-the-loop (HITL) feedback offers real-time updates and fine-tuning, ensuring the agents continually evolve and expand their understanding. This strategy is particularly useful in everyday activities like cooking or cleaning, where agents can quickly understand and execute tasks without requiring extensive retraining. By combining LLMs, KGs, and human insights, the system ensures that agents remain adaptable and efficient, even in unpredictable environments.

% paves the way for exciting future developments, such as expanding the framework to handle a wider variety of tasks and environments, fine-tuning the balance between automation and human input, and exploring ways to make knowledge refinement more autonomous. Ultimately, our approach marks a significant step toward building smarter, more adaptive, and human-centered assistive agents.
\vspace{-0.75em}
This research opens up multiple avenues for further research. First, we will explore the use of this framework in many more classes of tasks, building on (and reinforcing) the promising results obtained so far.  Second, we will investigate the trade-off between automating the generation of an action sequence for any given task, and soliciting and incorporating human feedback as needed. Furthermore, we will explore the use of this framework on a physical robot platform assisting humans. The long-term objective is to create assistive agents and robots that can interact and collaborate with humans in different application domains.