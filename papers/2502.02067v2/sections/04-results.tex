%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\section{Experimental Sections and Results}
%\Madhav{This section needs an overhaul. One way to structure it as along the following lines. Initially layout the testbed or the Simulation Framework. The testbed here is LLM, KG and Humans. Describe each one in detail. The LLMs being used and why are we using so many LLMs, about KG perhaps nothing new need to be added (I have not read the Method section yet), and the process of human involvement. The next subsection can be on Ground Truths used. Here we have two GT i believe, one is the minimum ingredients to be used and the other sequence output by humans. How many humans were involved? How is the comparison between human sequence and LLM sequence done? Is that done by a code or by humans once again. The next subsection should be on Evaluation Metrics. Here apart from saying what each Metric stands for you need to also say what is expected of each metric across the columns moving left to right with respect to the tables. The next three subsections should be on each of the hypotheses H1, H2, H3. You can have a final subsection that summarizes all the key results such as for example which LLM works best for Knowledge Expansion, which one is the best for over all execution error but also more intricate ones such as the Proposed Framework shows the same trend across all LLMs that vindicates its consistency etc, how human involvement leading to KE precludes ground truthing, dataset creation for every new task etc. }
\label{sec:exp-setup-results}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%% EXPERIMENTAL SECTION %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\noindent
This section describes the experimental setup and the results of experimentally evaluating three hypotheses:
\begin{itemize}
\vspace{-0.75em}
\item[\textbf{H1:}] Combining generic prediction of action sequences from LLMs with KG-based specific prior knowledge improves performance compared with just LLMs.
% Zero-shot performance of different LLMs to generate the action sequence and check the performance with respect to minimum ingredient used thing \karthik{not sure if we want to add this}
\item[\textbf{H2:}] Soliciting and using human feedback as needed supports incremental knowledge revision and results in improved performance compared with not using human feedback.
% Combining action sequence prediction using LLM, KG for refinement and HITL for final correction can improve the correctness of the systems output%(*rephrase*)
\item[\textbf{H3:}] Our framework adapts to new classes of tasks through incremental and transparent knowledge refinement.
%\Madhav{Table III corresponding to H3 does not show reduced effort in training or knowledge revision. It shows pretty good adaptation. If we are unable to show reduced training effort then we need to change H3}
% HITL can tackle situation of unseen tasks and expand the knowledge of the agent.
\vspace{-0.5em}
\end{itemize}

\begin{figure}[tb]
\centering
\captionsetup{font=scriptsize}
\setlength{\belowcaptionskip}{-10pt}
\includegraphics[width=0.45\textwidth]{figures/progress-lines-omelette.pdf}
\vspace{-0.75em}
\caption{Progress line~\cite{sakib2024cooking} showing use of each ingredient when preparing an \textit{omelette}.}
\vspace{-6pt}
\label{fig:progress-lines}
\end{figure}

% \vspace{-0.5em}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\subsection{Experimental Setup}
\label{sec:expres-setup}
%%%%%%%%%%%%%%%%%%%%%%%%%%% EXPERIMENTAL SETUP %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent
We begin by describing the experimental set up, which includes the prompting of LLMs, and the choice of baselines, classes of tasks, and evaluation measures.

%\vspace{1.5em}
\noindent
\subsubsection{LLM Prompting}
% Different large language models like `gemini-1.0-pro-latest` \cite{geminiteam2023gemini} by Google, `gpt-3.5` \cite{gpt} by OpenAI and `claude-3-opus` by Anthropic are used for evaluating Hypothesis \textbf{H1}. 
% \karthik{Mention about the one-shot prompting with the different LLM Models used}
%\vspace{-1em}
% \nabanita{We used large language models such as GPT-3.5, GPT-4o, Gemma2 9B, and Llama3 70B combined with a knowledge graph (KG) to handle task sequences for cooking in a kitchen environment. We used different LLMs to compare how their size, reasoning abilities, and token efficiency affect performance on the same task. Large parameter models like GPT-4 excel in complex, nuanced decompositions and use less tokens, while smaller models like Gemma2 9B are less efficient. The environment was structured through an ontology that categorized items like objects, receptacles, and tools, allowing the models to understand the relationships between different elements. Each model was tasked with generating a chain of thought (COT) and a corresponding action sequence for cooking tasks, broken down into three stages: fetching ingredients and tools, performing the cooking steps, and plating or serving the dish. Using a one-shot prompting approach, we first provided an example of preparing coffee, which helped the models learn the format and reasoning for completing tasks, enabling them to generate effective plans for new recipes. In our approach, we generated progress lines for each recipe's ingredients, tools, and receptacles by extracting the final task sequence and verifying it using a knowledge graph (KG). To ensure the accuracy of task decomposition in real-world environments, human input was integrated into certain frameworks. For instance, in the LLM + human setup, humans validated the task sequences produced by the language model. In the LLM + KG + human framework, human involvement went further, not only verifying the sequences but also expanding the knowledge graph by adding missing items or actions required for task completion, improving the overall accuracy and robustness of the model's task understanding.}

We used GPT-3.5 and GPT-4o to generate action sequences for specific tasks in a given environment. We used Chain of Thought (CoT) prompting to encourage the LLM to decompose any given task into a series of logical steps. The main prompt included domain-specific information (e.g., object classes from $\mathbf{G_{s}}$), set $\mathbf{A}$ of agent's actions, and output for a single in-context example. For example, for the agent assisting in cooking tasks, the LLM was encouraged to generate an action sequence that fetches ingredients and tools, completes the cooking process, and serves the dish, based on the example of preparing coffee. %We employed a one-shot prompting approach, using a single in-context example of preparing coffee, giving the relevant CoT and action sequence required for the preparation of coffee.
The LLM's output was filtered to retain only the predicted action sequence.
%The output from the LLM consists of the Chain of Thought (CoT), action sequences, and other text and comments. The framework filters this output to ensure only the action sequence remains. After filtering, the LLM output is sent to the error detection and refinement block (see Section~\ref{sec:framework-refine}).\\
%\vspace{-0.75em}
As described in Section~\ref{sec:framework-hitl}, any unresolved mismatch between LLM and KG, or an error in outcome, also led to the agent sending a feedback prompt to the LLM in an attempt to fix the error.
%When an error is encountered, the agent sends a feedback prompt to the LLM, seeking a fix for the error. The feedback prompt contains the erroneous LLM output along with a description of the error. We set a threshold on the number of feedback prompts sent to the LLM in the event of an error.\\


%To evaluate the LLMs' performance, we used execution output and progress lines. The execution output contained the actual process that the robot followed during task execution, listing all the actions that the robot took. The progress lines (see Figure \ref{fig:progress-lines}) depicted the progression of each ingredient used during the task. For example, in the preparation of an omelette, if the ingredients used were egg, oil, salt and onion, the progress lines would track how each ingredient was used throughout the process. This method for evaluation is inspired from \cite{sakib2024cooking}. A detailed explanation of our evaluation process is provided in Section~\ref{sec:evaluation}.



\subsubsection{Baselines}
% We collected and used actual human evaluation as the \textit{ground truth}. \\
% \karthik{Describe about the experiment}\\
We evaluated three different configurations of components in our framework: (a) LLM; (b) LLM with a KG; and (c) LLM with KG and human input (LLM + KG + Human). We conducted linked trials, i.e., in each trial, the same LLM output was provided to each configuration. %In all experiments, the same action sequence output from the LLM was provided to each framework. \\
As stated in Section~\ref{sec:framework}, with just LLM, the predicted action sequence is sent directly for execution, and errors results in a feedback prompt to the LLM for a fixed number of times. %since it is not equipped with the knowledge graph $\mathbf{G_k}$, there is no refinement, and the LLM output is sent directly for execution. Upon the occurrence of an execution error, a feedback prompt describing the error is sent back to the LLM. This process continues until the feedback limit is reached, after which the agent's final progress is evaluated. \\
With \textit{LLM + KG}, the KG is used to identify and fix mismatches between LLM output and KG; however, consistent mismatches and incorrect execution outcomes are not addressed. The \textit{LLM+ KG + human} configuration represents our framework, in which unresolved mismatches are addressed using human input, which is assumed to be accurate; the other two configurations serve as baselines.

%In the LLM + KG + Human framework, equipped with $\mathbf{G_k}$, the system first performs error detection and refinement on the LLM output (see Section~\ref{sec:framework-refine}). If an error is encountered after refinement, the system attempts to resolve it through feedback. If the feedback limit is reached and an unknown item error remains, the system queries a human for assistance (see Section~\ref{sec:framework-hitl}). After receiving the human's response and potentially updating the knowledge graph, the final action sequence is executed and evaluated.\\
%\textit{LLM + KG: }The LLM + KG framework is also equipped with $\mathbf{G_k}$ and performs error detection and refinement on the LLM output. This system attempts to resolve errors using a finite number of feedback cycles. Unlike the LLM + KG + Human framework, the LLM + KG framework does not have human assistance to resolve the errors. Therefore, once the feedback threshold is reached, the final action sequence is executed and evaluated.\\
%The LLM and LLM + KG frameworks serve as \textit{baselines} for comparison with our LLM + KG + Human model.

% We tested the models' performance across several frameworks: LLM, LLM with a knowledge graph(LLM + KG), and LLMs with knowledge graph with human assistance for knowledge expansion (LLM  
% + KG + Human). In all our experiments, the same output of the LLM was given as input to all the frameworks for refinement and/or execution. In LLM Framework, the output of the LLM is directly sent for execution. Upon occurence of any execution error, the feedback prompt mentioning the execution error is sent back. This is carried out till the feedback counter is exhausted. In LLM+KG framework, a prior module of refinement is present before the start of execution. This module ensures that any syntactic, structural issues in the LLM Output are rectified as per the defined action formalisms. This working is similar to Fig.\ref{fig:pipeline}b. The final refined output is sent for execution. Similar to the previous framework, the execution error is sent back to LLM only if the feedback threshold is not reached. F
% \nabanita{We tested the models' performance across several frameworks: LLM only, LLM with a knowledge graph, and a combination of all three (LLM, KG, and human input). To evaluate the quality of the models' outputs, we used human participants as the ground truth by having them perform the same cooking tasks \Madhav{Do they actually cook or do they output progress lines or task sequences. }in the same environment. The participants do not cook literally, they compare progress lines/task sequences of each recipe generated by our framework to their own knowledge of the recipe. This allowed us to see how well the models' task sequences aligned with what real people would do in similar situations. Additionally, we monitored how much human intervention was needed for the models to complete their tasks correctly, varying from minimal correction as in LLM with Human where human verifies the possibility of task sequence being performed in the current environment to more involved guidance in LLM with KG and Human where human verifies the possibility of task sequences as well as expands the knowledge graph. We are using \textbf{average human involvement} to keep a track of the number of times human is involved. \Madhav{This can be confusing for we are not grading human involvement as simple or involved.}}

\subsubsection{Classes of tasks}
In order to evaluate the ability of our framework to adapt to different classes of tasks, we considered cooking and cleaning tasks. Specifically, we considered $30$ different cooking tasks in a kitchen; this is the motivating scenario described in Section~\ref{sec:framework}. These tasks were created by sampling from the Recipe1M+ dataset~\cite{marin2021recipe1m+}. In addition, we considered $12$ variants of cleaning/clearing tasks that involved the agent cleaning specific objects or surfaces (e.g., "do the laundry"), or arranging objects in desired configurations in particular rooms (e.g., "clear the toys from the playroom")---see Figure~\ref{fig:12} for some examples.
%Our framework is highly adaptable, enabling easy extension to a range of household tasks beyond its initial scope. In addition to the kitchen domain, we tested our agent in the cleaning domain, which included over 12 tasks (Figure \ref{fig:12}), ranging from cleaning to rearranging. Our goal was to demonstrate how the agent can perform tasks without any prior knowledge, so we also included tasks such as "charge the phone" and "play the music." With the flexible structure of our knowledge graph and human-in-the-loop (HITL) verification and expansion, the agent successfully completed tasks like cleaning floors, arranging toys, and organizing table surfaces.
The results of evaluating the adaptability of our framework is summarized later in Table~\ref{tab:h3_other_domain}.

\begin{figure}[tb]
\captionsetup{font=scriptsize}
\centering
    \begin{minipage}{0.45\textwidth}
        \begin{lstlisting}[basicstyle=\ttfamily\scriptsize]
'clean the bedroom_floor',
'dust the TV',
'wash the clothes',
'wash the dishes',
'water the plants',
'take out trash',
'clean the window',
'mop the countertop',
'clean the table',
'pick up and put all the toys in the toy box',
'charge the phone',
'play the music'
        \end{lstlisting}
    \end{minipage}
    \setlength{\abovecaptionskip}{-2pt}
    \setlength{\belowcaptionskip}{-12pt}
    \caption{12 variants of tasks that involve the agent assisting with cleaning different objects and surfaces, or clearing objects to achieve the desired object configuration.}
    \label{fig:12}
\end{figure}


\subsubsection{Evaluation Strategy}
%\label{sec:evaluation}
% \karthik{Mention about the evaluation strategy }\\
For the evaluation of our framework, we used human participants to provide ground truth. Specifically, we recruited 18 human evaluators to mark the execution outputs for each task assigned to the framework and the two baselines. These evaluators were subject matter experts who were not involved in the design of our framework. The tasks were distributed such that the output for each task was evaluated by at least three human participants. The scores provided by the human (on a linear scale between 0-20) were averaged to obtain the success rate of our framework and the two baselines. These results are discussed further in Section~\ref{sec:expres-results}.

%, each assigned to assess 5 recipes. For each recipe, we provided the evaluators with the execution outputs and progress lines to ensure a comprehensive assessment. The distribution of total 30 recipes sampled from the Recipe1M+ dataset was such that each recipe had an overlap i.e. a set of 3 human participants would have the same set of recipes. This is to ensure variability in the response. These recipes were evaluated across all the frameworks (baselines and ours). A scoring criteria to evaluate the results was formed. The cooking process was divided primarily into two stages - (i) Fetching the relevant ingredients for the dish, (ii) Cooking and Serving process. Each stage had a maximum score of 10, making the entire process to get complete correctly with score 20. The scores were later averaged across all the recipes and this accounted for the success rate in achieving the completion of the cooking process. We used this information to evaluate H1 \& H2. \\ 
%For cleaning tasks, the action sequence was provided for evaluation, as these tasks are simpler to comprehend compared to cooking tasks. We involved 9 human participants to evaluate the action sequences of 12 different tasks (). Similar to the cooking task evaluation, 3 humans were given the same set of 4 tasks. The scoring criteria consisted of two stages: (i) Fetching the essential items for cleaning/clearing and (ii) Performing the actual cleaning process. The maximum score for task completion was 20, and the final evaluation score was averaged across all the tasks. We showcase the results for cleaning tasks in Table \ref{tab:h3_other_domain}. 

\vspace{-0.75em}
To better understand the LLM's performance, we also considered \textit{progress lines}~\cite{sakib2024cooking}, which depicted the use of key individual objects during individual steps of the action sequence, e.g., Figure~\ref{fig:progress-lines} shows the movement of each ingredient when cooking an omelette. These were presented along with the execution outputs to be evaluated by the humans.


\begin{table*}[tb]
\centering
\captionsetup{font=scriptsize}
\setlength{\belowcaptionskip}{-7pt}
\begin{tabular}{| >{\centering\arraybackslash} m{2.5cm}| 
>{\centering\arraybackslash} m{4cm}| >{\centering\arraybackslash} m{2.0cm}| >{\centering\arraybackslash} m{2.0cm}| >{\centering\arraybackslash} m{2.5cm}| }
\hline
LLM Models $\downarrow$ & Frameworks $\rightarrow$ & LLM & LLM + KG & LLM + KG + Human\\
\hline
\\[-1em]
 \multirow{4}{1.5cm}{GPT 4o}& \textbf{Success Rate (in \%) $\uparrow$} & 45.2 & 56.95 & \textbf{91.14} \\
\cline{2-5}
\\[-1em]
%  \multirow{4}{1.5cm}{GPT 4o}&  Avg. Ingd. Overlap (in \%) & 73.81 & 72.62 & 74.64 & \textbf{89.7} \\
% \cline{2-6}
% \\[-1em]
 & Avg. Tokens Used $\downarrow$ & 8316 & 7591 & 6459 \\
\cline{2-5}
\\[-1em]
%  & Avg. Human involvement & 0.0 & 1.0 & 0.0 & 0.33\\
% % \\[-1em]
% \cline{2-6}
% \\[-1em]
% &  Avg Execution Error & 3.8 & 4.27 & 0.67 & 0.27 \\
% \cline{2-6}
% \\[-1em]
% & \textbf{Overall Success Rate (in \%) $\uparrow$} & 45.2 & 63.58 & 56.95 & \textbf{91.14} \\
% \cline{2-6}
% \\[-1em]
&  Mean Ingd. Overlap (in \%) & 56.7 & 65.27 & \textbf{92.07} \\
\cline{2-5}
\\[-1em]
&  (\#nodes, \#edges) in $\mathbf{G_{s}}$ and $\mathbf{G_{k}}$ & (79, 772) & (79, 772) & \textbf{(87, 845)} \\
\hline
\\[-1em]
 \multirow{4}{1.5cm}{GPT 3.5}&  \textbf{Success Rate (in \%) $\uparrow$} & 25.41 & 33.95 & \textbf{92.08} \\
\cline{2-5}
\\[-1em]
%  \multirow{4}{1.5cm}{GPT 3.5}&  Avg. Ingd. Overlap (in \%) $\uparrow$ & 38.13 & 59.91 & 42.39 & \textbf{97.3} \\
% \cline{2-6}
% \\[-1em]
 & Avg. Tokens Used $\downarrow$ & 8402 & 8415 & 4354 \\
\cline{2-5}
\\[-1em]
%  & Avg. Human involvement & 0.0 & 2.6 & 0.0 & 0.5 \\
% % \\[-1em]
% \cline{2-6}
% \\[-1em]
% &  Avg Execution Error & 3.93 & 4.63 & 0.87 & 0.03 \\
% \cline{2-6}
% \\[-1em]
% &  \textbf{Overall Success Rate (in \%) $\uparrow$} & 25.41 & 49.77 & 33.95 & \textbf{92.08} \\
% \cline{2-6}
% \\[-1em]
&  Mean Ingd. Overlap (in \%) $\uparrow$ & 38.13 & 44.29 & \textbf{98.97} \\
\cline{2-5}
\\[-1em]
&  (\#nodes, \#edges) in $\mathbf{G_{s}}$ and $\mathbf{G_{k}}$ & (79, 772) & (79, 772) & \textbf{(89, 869)} \\
\hline
\end{tabular}
\caption{Evaluating \textbf{H1 \& H2} for 30 recipes of six categories from Recipe1M+ dataset. The combination of LLM and KG ("LLM+KG") results in an increase in success rate, reduction in token use, and an increase in the mean ingredient overlap compared with just using the LLM ("LLM"). Also, soliciting and using human input when needed ("LLM+KG+Human") results in a substantial improvement on all measures, including an increase in the number of nodes and edges due to expansion of knowledge in KG.}
\label{tab:h2_kg_expansion}
\end{table*}

\begin{table*}[tb]
\centering
\captionsetup{font=scriptsize}
\setlength{\belowcaptionskip}{-7pt}
\begin{tabular}{| >{\centering\arraybackslash} m{2.5cm}| 
>{\centering\arraybackslash} m{4cm}| >{\centering\arraybackslash} m{2.0cm}| >{\centering\arraybackslash} m{2.0cm}| >{\centering\arraybackslash} m{2.5cm}| }
\hline
LLM Models $\downarrow$ & Frameworks $\rightarrow$ & LLM & LLM + KG & LLM + KG + Human\\
\hline
\\[-1em]
 \multirow{3}{1.5cm}{GPT 4o}&  \textbf{Success Rate (in \%) $\uparrow$} & 42.33 & 44.00 & \textbf{75.66} \\
\cline{2-5}
\\[-1em]
%  \multirow{4}{1.5cm}{GPT 4o}&  Avg. Ingd. Overlap (in \%) & 18.06 & 18.06 & 38.89 & \textbf{91.67} \\
% \cline{2-6}
% \\[-1em]
 & Avg. Tokens Used & 3820 & 3571 & 1979 \\
\cline{2-5}
\\[-1em]
%  & Avg. Human involvement & 0.0 & 1.17 & 0.0 & 0.17\\
% % \\[-1em]
% \cline{2-6}
% \\[-1em]
% &  Avg Execution Error & 3.83 & 4.33 & 0.5 & 0.08 \\
% \cline{2-6}
% \\[-1em]
% &  \textbf{Overall Success Rate (in \%) $\uparrow$} & 21.08 & 29.42 & 38.58 & \textbf{82.92} \\
% \cline{2-6}LLM
% \\[-1em]
&  (\#nodes, \#edges) in $\mathbf{G_{s}}$ and $\mathbf{G_{k}}$ & (39, 313) & (39, 313) & \textbf{(40, 331)} \\
\hline
\\[-1em]
 \multirow{3}{1.5cm}{GPT 3.5}&  \textbf{Success Rate (in \%) $\uparrow$} & 32.63 & 42.5 & \textbf{98.75} \\
\cline{2-5}
\\[-1em]
%  \multirow{4}{1.5cm}{GPT 3.5}&  Avg. Ingd. Overlap (in \%) $\uparrow$ & 33.33 & 37.5 & 38.89 & \textbf{97.22} \\
% \cline{2-6}
% \\[-1em]
 & Avg. Tokens Used $\downarrow$ & 4963 & 4440 & 3510 \\
\cline{2-5}
\\[-1em]
%  & Avg. Human involvement & 0.0 & 0.75 & 0.0 & 0.25 \\
% % \\[-1em]
% \cline{2-6}
% \\[-1em]
% &  Avg Execution Error & 3.75 & 4.17 & 0.67 & 0.0 \\
% \cline{2-6}
% \\[-1em]
% &  \textbf{Overall Success Rate (in \%) $\uparrow$} & 33.34 & 36.67 & 39.83 & \textbf{94} \\
% \cline{2-6}
% \\[-1em]
&  (\#nodes, \#edges) in $\mathbf{G_{s}}$ and $\mathbf{G_{k}}$ & (39, 313) & (39, 313) & \textbf{(44, 397)} \\
\hline
\end{tabular}
\caption{Evaluating \textbf{H3} by adapting our framework to the cleaning and clearing tasks without requiring extensive tuning (e.g., of LLM) or comprehensive encoding of knowledge (in KG). We observed a substantial improvement on all performance measures with our framework compared with just using LLM outputs or even LLM+KG. Also, the agent is able to solicit human input as needed to incrementally and transparently revise knowledge in the KG.} 
% that lack the ability for knowledge expansion suffer the most because they are constrained by the rigidity or absence of the knowledge. Most tasks fail in the initial stage due to the lack of essential items in the agent's knowledge base. 
%This explains why we observe that the first three frameworks have similar success rates, which are substantially lower than those of the LLM + KG + Human framework, as their knowledge is limited. The LLM + Human framework also has a low success rate because it doesn't have an intrinsic knowledge base. As a result, there is no knowledge expansion. In this case, the human only assists the robot in scenarios where the robot mistakes an existing item for something else. }
\label{tab:h3_other_domain}
\end{table*}


%\vspace{0.3em}
\subsubsection{Evaluation Measures}
The key performance measures considered in this work include:
\begin{itemize}
    \vspace{-1em}
    \item \textbf{Success rate:} As stated above, this measure was computed based on the scores assigned by the human participants. Higher values are better as they indicate a higher degree of satisfaction in task completion.  This measure was used for evaluating H1-H3. 
    % mentioned in section \ref{sec:evaluation}, the trials were conducted and the success rate was computed by the total score given to the recipes across all the sampled recipes and averaged over all human evaluations. This metric is crucial to understand the overall completion of the dish. We expect the success rate to increase when evaluating all the hypothesis.
    % involving steps like \textit{Fetching the ingredients} and the \textit{Cooking process}. 

    \item \textbf{Average tokens used:} The number of tokens used when prompting the LLM (including the input prompt and all subsequent feedback prompts) was averaged across all tasks. This is a measure of resource consumption and lower values are usually better, except when the use of prompts improves the values of other measures. %metric highlights the overall resource consumption in prompting the LLM and demonstrates how effectively our system reduces token usage.
    
    \item \textbf{Number of nodes and edges in KG:} We use this measure to evaluate H2 and H3. An increase in its value implies an expansion of knowledge in the KG.
    
    %The number of nodes (items in the environment) and the number of edges (relational properties) in the the knowledge and state graph. We used this measure to evaluate H2 and H3. During expansion how many nodes and edges were added to the graph and how this influenced the cooking and cleaning process and overall success. We showcase these results in Table \ref{tab:h2_kg_expansion} and Table \ref{tab:h3_other_domain}. 
    
    \item \textbf{Mean ingredient overlap:} A measure specific to the first class of tasks (cooking); it is the average overlap between the ingredients in the ground truth recipe and the ingredients in the executed action sequence. If $m_i$ denotes the ingredients required to make a particular dish and $l_i$ denotes the ingredients in the action sequence, this measure is computed as: 
    \vspace{-0.75em}   
    \begin{align}
        \label{eqn:mean-overlap}    
        \text{Mean ingredient overlap} = \frac{1}{N} \sum_{i=1}^{N} \frac{|m_i \cap l_i|}{|m_i|}
    \end{align}
    where $| \cdot |$ is the cardinality of a set, and $N$ is the total number of recipes sampled from the dataset.  This measure was used to evaluate H1-H2.   
    
 %   Let $m_i$ be the minimum ingredients required to make a recipe and let $l_i$ be the ingredients that the LLM output contained. $m_i$ $\cap$ $l_i$ denotes the common ingredients appearing in both the lists. We use this metric to evaluate H1 \& H2. We evaluate the mean overlap across all recipes from the Recipe1M+ dataset (ground truth).
\end{itemize}

%\Madhav{@Shivam: Along with explaining the metrics you need to say whether higher or lower values implies superior performance. More nuanced explanation is needed when human feedback is involved. Are you planning to write that where you are explaining the tables}

% \nabanita{To measure the models' effectiveness, we used several key metrics. \textbf{Ingredient Overlap} assessed how closely the models' ingredient choices matched the correct ingredients for each recipe. \textbf{Execution Error} focused on identifying any mistakes in the action sequences the models generated. \textbf{Token Usage} provided insight into the efficiency of the models' planning, with fewer tokens indicating more concise and potentially more accurate outputs. Finally, \textbf{Human Involvement} measured how much human correction or feedback was required. These metrics gave us a clear view of how well the models performed under different conditions, highlighting both the potential and the limitations of using LLMs with or without additional human or KG support in complex task planning scenarios.}

% \nabanita{Our human-in-the-loop (HITL) block enhances knowledge expansion within a knowledge graph by combining Large Language Model (LLM) outputs with human validation. When an object, tool, or receptacle in the LLM-generated task sequence is not present in the knowledge graph, a human verifies its existence. If absent, the human adds it along with relevant properties such as "isBoilable," "isToggleable," or "isFryable." This process ensures that the knowledge graph continuously grows in both nodes (new entities) and edges (their properties), while maintaining 100\% accuracy, as humans serve as the ground truth for all additions. The framework has led to a marked increase in the number of nodes and enriched relationships, highlighting its success in learning and expanding the knowledge base in real-time.}
% Expansion is carried out by two methods in our framework - (i) Where the refined output from the LLM and KG still has error, and when the human is prompted for changing the item with some equivalent item from the pre-exisitng knowledge base. For eg: when one of the actions in the action sequence is \texttt{\small{pick\_up\_obj(cilantro, state\_file)}}, where \textit{cilantro} is not present in the knowledge base, the human can check a replacement for the object and can mention to instead use \textit{parsley}, (ii) The human can define the properties of the object like "isBoilable", "isSliceable", "isFryable", and eventually the robot could make use of this information and can complete the action. Considering the same example of \texttt{\small{pick\_up\_obj(cilantro, state\_file)}}, this time the human can highlight the properties by answering some set of questions\footnote{Details of the questions asked can be viewed in the supplementary material} and add a node to the graph structure seamlessly.

%%%%%%%%%%%%%% PSEUDO CODE %%%%%%%%%%%%%%%%%%%%%



% \begin{algorithm}
% \caption{Generate action sequences for any dish}
% \label{alg:task_sequences}
% \begin{algorithmic}[1]
% \STATE \textbf{Input:} D (dish), $G_s$ (state graph), $G_k$ (knowledge graph), $A$ (primitive actions), M (LLM\_model)
% \STATE \textbf{Output:} T (task sequences), updated $G_s$, $G_k$
% \STATE $A \gets \{a_1, a_2, \ldots, a_n\}$; $T \gets \{\}$; $counter, limit \gets 0, 3$
% % Loop through action sequences suggested by model M
% \FOR{$a_i \in \text{M}(D)$}
%     \IF{$a_i \in A \text{ \& } \text{execute}(a_i, G_s, G_k)$}
%         \STATE $G_s \gets \text{apply}(a_i, G_s)$; $T.\text{append}(a_i)$
%     \ELSE
%         \STATE $a_i \gets \text{refine}(a_i, G_k)$
%         \IF{$counter < limit \text{ \& } \text{not } \text{execute}(a_i, G_s, G_k)$}

%         \STATE $a_i \gets \text{feedback\_to\_llm}(a_i, G_k)$ \algorithmiccomment{Feedback to LLM}
        
%         \STATE $counter \gets counter + 1$
%         \ELSE
%         \IF{\text{not} $\text{execute}(a_i, G_s, G_k)$}
        
%     \STATE $h\_r \gets \text{query\_human}(a_i)$; 
%     \IF{$h\_r == \text{"unknown"}$} 
%         \STATE $G_k \gets G_k \cup \{\text{get\_human\_properties}(a_i)\}$; $G_s \gets G_s \cup \{\text{initial\_state\_of}(a_i)\}$ 
%     \ELSIF{$h\_r == \text{"similar"}$} 
%         \STATE $a_i \gets \text{get\_from\_human}(a_i)$ 
%     \ENDIF
%     \STATE $G_s \gets \text{apply}(a_i, G_s)$; $T.\text{append}(a_i)$
%     \ENDIF
%     \ENDIF
%     \ENDIF
% \ENDFOR

% \RETURN $T$
% \end{algorithmic}
% \end{algorithm}


% \begin{algorithm}
% \caption{Generate action sequences for any dish}
% \label{alg:task_sequences}
% \textbf{Input:} D (dish), $G_s$ (state graph), $G_k$ (knowledge graph), $A$ (primitive actions), M (LLM\_model)\\
% \textbf{Output:} T (task sequences), updated $G_s$, $G_k$
% \begin{algorithmic}[1]
% \State $A \gets \{a_1, a_2, \ldots, a_n\}$; $T \gets \{\}$; $counter, limit \gets 0, 3$
% % Loop through action sequences suggested by model M
% \For{$a_i \in \text{M}(D)$}
%     \If{$a_i \in A \text{ \& } \text{execute}(a_i, G_s, G_k)$}
%         \State $G_s \gets \text{apply}(a_i, G_s)$; $T.\text{append}(a_i)$ \algorithmiccomment{State Graph Update after execution}
%     \Else
%         \State $a_i \gets \text{refine}(a_i, G_k)$ \algorithmiccomment{Refining LLM Output}
%         \If{$counter < limit \text{ \& } \text{not } \text{execute}(a_i, G_s, G_k)$}

%         \State $a_i \gets \text{feedback\_to\_llm}(a_i, G_k)$ \algorithmiccomment{Feedback to LLM}
        
%         \State $counter \gets counter + 1$
%         \Else
%         \If{\text{not} $\text{execute}(a_i, G_s, G_k)$}
        
%     \State $h\_r \gets \text{query\_human}(a_i)$; \algorithmiccomment{Human input}
%     \If{$h\_r == \text{"unknown"}$} 
%         \State $G_k \gets G_k \cup \{\text{get\_human\_properties}(a_i)\}$ \algorithmiccomment{KG Expansion}
%         \State $G_s \gets G_s \cup \{\text{initial\_state\_of}(a_i)\}$  \algorithmiccomment{State Graph Update}
%     \ElsIf{$h\_r == \text{"similar"}$} 
%         \State $a_i \gets \text{get\_from\_human}(a_i)$ \algorithmiccomment{Correction by Human}
%     \EndIf
%     \State $G_s \gets \text{apply}(a_i, G_s)$; $T.\text{append}(a_i)$ \algorithmiccomment{State Graph Update after execution}
%     \EndIf
%     \EndIf
%     \EndIf
% \EndFor

% \Return $T$
% \end{algorithmic}
% \end{algorithm}

% \textbf{Function Descriptions:}
% \begin{itemize}
%     \item \texttt{execute($a_i$,$G_s$,$G_k$)}: Checks if action $a_i$ can be executed in $G_s$ and $G_k$.
%     \item \texttt{apply($a_i$)}: Updates $G_s$ by applying $a_i$.
%     \item \texttt{refine($a_i$)}: Refines $a_i$ using $G_k$.
%     \item \texttt{query\_human($a_i$)}: Gets human feedback on $a_i$.
%     \item \texttt{get\_human\_properties($a_i$)}: Adds human-provided details for $a_i$ to $G_k$.
%     \item \texttt{initial\_state\_of($a_i$)}: Finds initial state for $a_i$ from human input.
%     \item \texttt{get\_from\_human($a_i$)}: Retrieves human-corrected version of $a_i$.
% \end{itemize}


% \textbf{Line-by-Line Explanation:}

% 1. \textbf{Input/Output Definitions:}
%     - Inputs:\\
%         - $D$: The dish to be prepared.\\
%         - $G_s$: The current state of the kitchen.\\
%         - $G_k$: Knowledge graph containing information about the dish.\\
%         - $A$: A set of primitive actions (e.g., cut, move).\\
%         - $M$: A language model generating potential task sequences.\\
%     - Outputs:\\
%         - $T$: Final task sequence for preparing the dish.\\
%         - Updated $G_s$ and $G_k$.

% 2. \textbf{Initialize Variables:}
%     - Initialize $A$ (primitive actions), empty list $T$ (task sequence), $counter$ (set to 0), and $limit$ (set to 3).

% 3. \textbf{Loop Over Generated Sequences:}
%     - For each $a\_i$ generated by $M$ for dish $D$, check if $a\_i$ is executable by calling \texttt{execute(generate(a\_i), G\_s, G\_k)}.

% 4. \textbf{Apply or Refine Actions/Items:}
%     - If valid, apply $a\_i$ to update $G\_s$ using \texttt{apply(a\_i, G\_s)} and append it to $T$. 
%     - If not, refine the action using \texttt{refine(a\_i, G\_k)}.

% 5. \textbf{Handle Refinement Limit and Human Feedback:}
%     - If refinements exceed $limit$, query human feedback with \texttt{query\_human(a\_i)}. Based on the feedback, update $G\_k$ or correct the action using \texttt{get\_from\_human(a\_i)}.


% 6. \textbf{Return Results:}
%     - After processing all actions, return the final task sequence $T$, and updated $G\_s$ and $G\_k$.
% \nabanita{\textbf{Nabanita:} The algorithm should have a flow that is understood at first glance, say LLM output task sequence, if the task sequence is possible to be executed, if not, then refine by KG, then refine by human at the end. }



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% For each task, the system generates action sequences through the LLM, and human validation ensures accuracy and completeness. As new objects or tools are encountered, their properties are verified and added to the graph, ensuring the system remains accurate and evolves to cover a broader range of household activities. This flexible, human-verified approach guarantees that the system can handle a growing variety of tasks with both precision and reliability.}
% \noindent
% \

% \karthik{Baselines}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\subsection{Experimental Results}
\label{sec:expres-results}
\noindent
%So far, we have discussed that the knowledge graph of a given environment encodes the properties of the items present within it (see Section \ref{sec:framework-kg}). This knowledge is essential when performing actions on specific items. In frameworks combining LLMs with knowledge graphs, the LLM output is refined by the knowledge graph, which corrects errors in function names, item names, and actions. For instance, if the LLM suggests \texttt{\small{slice(mango, fork, state\_file)}}, the system identifies \textit{fork} as an unsuitable tool for slicing and replaces it with a proper slicing tool available in the environment. 
%Using the KG also helped minimize execution-time errors by resolving most issues during the detection and refinement phase. Algorithm \ref{alg:algorithm} explains the flow of our pipeline.\\
Next, we describe and discuss the experimental results.

\vspace{-0.5em}
\noindent
\textbf{Evaluating H1.} 
We first explored whether the combination of LLM and KG leads to improved performance in comparison with just using LLM output for any given task. The corresponding results for the cooking-related tasks are summarized in Table~\ref{tab:h2_kg_expansion}; in particular, see columns labeled "LLM" and "LLM + KG". For the two LLMs considered (GPT3.5, GPT4o), we observe a substantial increase in success rate, reduction in token use, and an increase in the mean ingredient overlap for LLM+KG compared with LLM. These results provide strong support for \textbf{H1}.

%hy, we show how using knowledge graphs alongside large language models (LLMs) proves to be instrumental in generating error-free action sequences for a given task. Our experiments, conducted in a household kitchen environment, demonstrate the improvement is success rates in LLM + KG framework. When experimenting with GPT-4o, we observed an 11.75\% increase in success rate and 8.57\% in avg ingredient overlap in the LLM + KG framework compared to the LLM-only framework. The LLM-only approach falls short due to common issues such as hallucinations in selecting appropriate actions and item names, which the LLM + KG framework mitigates. The results for H1 are showcased in (Table \ref{tab:h2_kg_expansion}).
% \Madhav{Which column in Table 1 should one refer for H1?? Need to mention this}


\vspace{-0.5em}
\noindent
\textbf{Evaluating H2.}
Next, we explored the impact of soliciting and using human input as needed. The last column of Table~\ref{tab:h2_kg_expansion} ("LLM+KG+Human") shows that our framework's judicious use of human input with LLM and KG markedly improved performance on all measures compared with LLM and LLM+KG. With GPT-4o, we observed a $45.94\%$ increase in success rate over LLM and $34.19\%$ over LLM+KG. For GPT-3.5, the success rate increased by $66.67\%$ over LLM and $58.13\%$ over LLM+KG. Also, the average number of tokens used by our framework dropped by $48.26\%$ compared with baseline(s). This performance improvement was strongly influenced by the refinement of knowledge in the KG; the number of nodes and edges in the KG expanded from (79, 772) to (87, 845) with GPT-4o and to (89, 869) with GPT3.5. These results strongly support \textbf{H2}.

%  In H2, we demonstrate how the LLM + KG + Human framework is superior by dynamically expanding its knowledge base. To showcase this, we removed 10 nodes from the agent's knowledge base $\mathbf{G_{k}}$. These nodes represented essential ingredients required for preparing the 30 dishes used in our experimentation with the Recipe1M+ dataset. With this limited knowledge, we ran our experiments on the three frameworks. Frameworks that lack the ability for knowledge expansion suffer significantly due to their incomplete and rigid knowledge base. Most tasks fail because essential items are missing from the agent's knowledge graph. This limitation explains the lower success rates in the LLM and LLM + KG frameworks (see Table \ref{tab:h2_kg_expansion}). The LLM + KG + Human framework shines here by dynamically expanding its knowledge base with human assistance (section \ref{sec:framework-hitl}).  We see a substantial improvement in success rates when knowledge expansion occurs with human assistance in LLM + KG + Human framework. When experimenting with GPT - 4o, we observed a increase of 45.94\% in success rate from LLM framework and a increase of 34.19\% in successful task completion from LLM + KG framework. Additionally, the average number of tokens used (averaged over 30 recipe outputs) dropped significantly by 48.26\% in the LLM + KG + Human framework. As the result of expansion we observe an expansion across $\mathbf{G_s}$ and $\mathbf{G_k}$. The nodes and edges increase from (79, 772) to (87, 845) in GPT-4o and (89, 869) in GPT3.5 respectively.\\
%Table \ref{tab:h2_kg_expansion} validates H2, showing a notable improvement across all metrics in our LLM + KG + Human framework across different LLMs. 
% \Madhav{As discussed on call need to correspond H2 with metrics relating to number of nodes and edges? Do we expect that to increase or say static with expansion? Same with the number of tokens? Do we expect that to increase or decrease for H1, H2}

\vspace{-0.5em}
\noindent
\textbf{Evaluating H3.}
% The hypothesis H3 is a cornerstone of the present effort. Quite unlike prior art\cite{sakib2024cooking} we show dynamic adaptation to new scenarios by expanding the knowledge in scenarios where we started with very limited prior domain specific knowledge. With this expansion we portray very high levels of success rate without any need to generate new Ground Truth, datasets as well as fine tuning of networks and LLMs.\\
% In H3 we demonstrate our system's ability to seamlessly adapt and perform across different domains. 
Finally, we evaluated the ability to adapt our framework to a different class of tasks (cleaning and clearing), with the results summarized in Table~\ref{tab:h3_other_domain}. Unlike prior work~\cite{sakib2024cooking}, we seek to achieve this adaptation without extensive tuning (e.g., of LLM) or the need for comprehensive domain-specific knowledge (in the KG). We instead leverage the interplay between LLM, KG, and human input to support incremental adaptation to the new class of tasks. Results indicate (once again) a substantial improvement on all measures for our framework compared with the baselines. We noted that the impact of adding different bits of knowledge to the KG can differ. For example, with GPT-4o, the addition of just one item (mopping\_cloth) to the KG based on human input led to a $31\%$ increase in success rate; with GPT3.5, this improvement was more pronounced ($56\%$). We also observed a substantial reduction in the number of tokens used. In addition, this adaptation of knowledge is fully transparent by design. These results strongly support hypothesis \textbf{H3}. 

% \vspace{-0.75em}
% Our project web site\footnote{\url{https://sssshivvvv.github.io/adaptbot/}} hosts our supplementary material, including examples of tasks being performed in simulation, and supporting results with other LLM models.

%To showcase this, we tested our system outside the kitchen domain, focusing on household tasks such as cleaning and rearranging. Initially, the system had very limited knowledge of this new environment, but with human assistance, its knowledge expanded, resulting in a significant increase in success rates. Different items that gets added in the agent's knowledge base $\mathbf{G_{k}}$, can have varied degree of importance across different tasks. In Figure \ref{fig:12}, we list the 12 tasks on which we conducted our experiments. Notably, in the case of GPT-4o, the addition of just one item, mopping\_cloth in $\mathbf{G_{k}}$, led to a significant increase in success rate from 44\% to 75.66\%. This improvement highlights the critical role of the mopping\_cloth, as it is an essential item in most cleaning tasks.\\
%Hence, we demonstrate that when transitioning between domains, starting with minimal prior knowledge (39,313 nodes and edges in $\mathbf{G_s}$ and $\mathbf{G_k}$), our system \textit{expands} its knowledge with human assistance, growing from (39,313) to (44,397) nodes and edges in $\mathbf{G_s}$ and $\mathbf{G_k}$ respectively. This leads to a success rate of 98.74\%, a significant improvement compared to our baseline success rates of 32.63\% and 43.5\%. Rather than focusing on fine-tuning for specific cooking tasks as in \cite{sakib2024cooking}, our results highlight dynamic adaptation to newer scenarios starting with a minimal set of items associated with the class of tasks. These results validate H3.

%\Madhav{Please do add this. The hypothesis H3 is a cornerstone of the present effort. Quite unlike prior art (cite FOON etc) we show dynamic adaptation to new scenarios by expanding the knowledge in scenarios where we started with very limited prior domain specific knowledge. With this expansion we portray very high levels of success rate without any need to generate new Ground Truth, datasets as well as fine tuning of networks and LLMs}

%\Madhav{Where if Figure? \ref{fig:12}? Are we still talking about 12 tasks. Better to say 12 variants of the cleaning task}
















