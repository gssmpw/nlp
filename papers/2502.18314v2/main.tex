\documentclass[ preprint,
 amsmath,amssymb,
 aip,
]{revtex4-2}

\usepackage{graphicx}\usepackage{dcolumn}\usepackage{bm}
\usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         \usepackage{graphicx}
\usepackage{amsmath}

\DeclareMathOperator*{\argmin}{arg\,min}

\begin{document}
\title{Learning atomic forces from uncertainty-calibrated adversarial attacks}
\author{Henrique Musseli Cezar}
\affiliation{Hylleraas Centre for Quantum Molecular Sciences and Department of Chemistry, University of Oslo, PO Box 1033 Blindern, 0315 Oslo, Norway}

\author{Tilmann Bodenstein}
\affiliation{Hylleraas Centre for Quantum Molecular Sciences and Department of Chemistry, University of Oslo, PO Box 1033 Blindern, 0315 Oslo, Norway}

\author{Henrik Andersen Sveinsson}
\affiliation{The Njord Centre, Department of Physics, University of Oslo, PO Box 1048 Blindern, 0316 Oslo, Norway}

\author{Morten Ledum}
\affiliation{Hylleraas Centre for Quantum Molecular Sciences and Department of Chemistry, University of Oslo, PO Box 1033 Blindern, 0315 Oslo, Norway}

\author{Simen Reine}
\affiliation{Hylleraas Centre for Quantum Molecular Sciences and Department of Chemistry, University of Oslo, PO Box 1033 Blindern, 0315 Oslo, Norway}

\author{Sigbjørn Løland Bore}
\email{s.l.bore@kjemi.uio.no}
\affiliation{Hylleraas Centre for Quantum Molecular Sciences and Department of Chemistry, University of Oslo, PO Box 1033 Blindern, 0315 Oslo, Norway}


\begin{abstract}
Adversarial approaches, which intentionally challenge machine learning models by generating difficult examples, are increasingly being adopted to improve machine learning interatomic potentials (MLIPs).
While already providing great practical value, little is known about the actual prediction errors of MLIPs on adversarial structures and whether these errors can be controlled. 
We propose the Calibrated Adversarial Geometry Optimization (CAGO) algorithm to discover adversarial structures with user-assigned errors. Through uncertainty calibration, the estimated uncertainty of MLIPs is unified with real errors. 
By performing geometry optimization for calibrated uncertainty, we reach adversarial structures with the user-assigned target MLIP prediction error.
Integrating with active learning pipelines, we benchmark CAGO, demonstrating stable MLIPs that systematically converge structural, dynamical, and thermodynamical properties for liquid water and water adsorption in a metal-organic framework within only hundreds of training structures, where previously many thousands were typically required. 
\end{abstract}
\maketitle
\clearpage
\section{Main}
By representing the potential energy surface of atoms with neural networks, machine learning interatomic potentials (MLIPs) can be trained to predict the outcomes of costly quantum mechanical calculations in milliseconds instead of hours. With pioneering MLIPs, such as Behler-Parinello neural networks, structural and thermodynamic properties are already well captured.\cite{behler_generalized_2007} The latest equivariant message passing neural network approaches, such as NequIP,\cite{batzner_e3-equivariant_2022} Allegro,\cite{musaelian_learning_2023} and MACE\cite{NEURIPS2022_4a36c3c5} have further significantly reduced prediction errors by nearly an order of magnitude. These approaches typically achieve training and test set errors far lower than typical errors associated with the details of the underlying quantum mechanical calculations they are trained on, for example, basis set truncation and functional choice in the case of Density Functional Theory (DFT). Although MLIPs rely on the inductive bias of short-range interactions, their accuracy makes these methods, in principle, capable of accurately representing the energy landscape of chemically complex systems.

Parameterization of reliable MLIPs remains a challenge, often requiring significant human time and trial-and-error experimentation. The challenge in developing a reliable MLIP presents a chicken-and-egg scenario. On the one hand, the ultimate goal of MLIPs is to explore the unknown, reaching beyond the capabilities of ab initio MD, extending both to longer timescales and to larger length scales. On the other hand, the phase space covered by the training set is limited by the computational cost of ab initio calculations. As a result, all applications rely to varying degrees on the MLIP's ability to generalize to structures outside the training set. This ability to generalize is not just a property of the MLIP but an interaction between the MLIP and the training set. This dependency can become very problematic, as highlighted in recent studies\cite{zhai_short_2023,fu2023forces} which demonstrate that MLIPs, in some situations, struggle to provide stable dynamics, accurate sampling, or reproduce the underlying physics.  

Since current MLIP approaches achieve very good training set and validation set accuracy, this problem ultimately arises due to shifting features (molecular structures in our case) from training to production. When the MLIP is used in practice, the molecular structures are different from the structures in the training data. This phenomenon is known in a broader statistics context as a covariate shift.\cite{shimodaira_improving_2000} Active learning approaches have become the go-to solution for reducing covariate shifts in structures that typically occur during molecular dynamics (MD) sampling with MLIP-based models. The central idea is to use the MLIP to extend the training set by sampling structures that the MLIP encounters during production.\cite{yang2024machine} In practice, this is typically achieved by performing iterations of MD sampling with the MLIP, adding new structures until the MLIP model reaches specific convergence criteria, such as simulation stability or the accurate reproduction of structural and thermodynamic properties of reference ab initio simulations. 

To avoid unnecessary costly reference calculations on structures already well represented in the training set, active learning procedures typically employ uncertainty quantification to select structures that enhance the training data, i.e., those with high prediction uncertainty. There are various approaches to uncertainty estimation, including Gaussian Process regression,\cite{vandermause_--fly_2020,xie_uncertainty-aware_2023} dropout in neural networks,\cite{wen_uncertainty_2020} and committees of MLIPs.\cite{NIPS1994_b8c37e33} While the first two methods rely on specific MLIP architectures, the committee approach is broadly applicable, only requiring computing the variance of a property between multiple models trained on different training sets or seeds, for example, through bootstrapping. However, it is not without drawbacks: The members of the MLIP committee can potentially all agree on an incorrect answer. Furthermore, training and running multiple models adds to the computational cost. Despite these issues, the committee approach remains widely used due to its simplicity and ease of implementation, with many well-established active learning softwares using it.\cite{smith_less_2018,zhang_dp-gen_2020,schran_machine_2021,vandenhaute_machine_2023}

For uncertainty estimation to effectively extend the selection of new training set structures, it is essential to offer a diverse pool of candidate structures that expand the phase space of the existing training set. The typical strategy involves MD sampling with MLIPs across various thermodynamic conditions. The inclusion of structures that represent rare events through enhanced sampling techniques is also increasingly being recognized as important.\cite{yang_using_2022,vandenhaute_machine_2023} Although MLIP-based sampling with MD is a simple approach suitable for automated active learning pipelines to develop models, it has some weaknesses. These include prolonged correlation times, leading to high inter-structure correlation, and the inherent Boltzmann bias toward low-energy structures. Therefore, when the standard MLIP sampling approach fails, there are numerous pragmatic solutions to target more diverse structures, such as sampling at high temperatures and pressures, normal-mode sampling, or perturbing structures via random atomic displacements.\cite{smith2018less,zhang_dp-gen_2020} While these methods offer structures contrasting those of standard MD simulations, they often result in high-energy structures with atomic overlap. The result is, therefore, an undesirable compromise between including structures with high prediction uncertainty and structures with significant forces from atomic overlap. The latter can reduce the overall accuracy of the MLIP.\cite{zeng2020complex} In addition, such approaches tend to localize the prediction uncertainty to a few atoms, which is problematic for large molecular assemblies. It stands to reason that a few carefully optimized structures can offer the same learning content as many highly correlated structures.

In machine learning for image classification problems, adversarial approaches provide a systematic framework for building more robust models by training against examples that aim to trick the model.\cite{goodfellow2016deep} This approach has also been applied to the active learning of MLIPs. To the best of our knowledge, this idea was first introduced by Cubuk et al.\cite{cubuk_adversarial_nodate}\ to move atoms toward high prediction uncertainty for the potential energy. A similar idea was also applied in ref.~\citenum{kulichenko_uncertainty-driven_2023} using a bias towards high uncertainty with metadynamics simulations. Such approaches are algorithmically elegant by requiring only a standard MLIP single point force calculation. However, energy uncertainty leaves 3$N$+6 labels of learning content associated with gradients and virials up to chance. In this regard, the Bombarelli Group extended the approach of Cubuk to include prediction uncertainty of forces.\cite{schwalbe-koda_differentiable_2021} Using a differentiable MLIP architecture, they performed an adversarial active learning approach capable of discovering structures with high force uncertainty, reaching robust models for challenging systems including zeolite and alanine molecules. This work was also recently extended to non-periodic system reference calculations,\cite{roy2024learning} and force uncertainty-driven dynamics.\cite{zaverkin2024uncertainty} 

While such adversarial approaches have demonstrated great potential in active learning, very little attention has so far been put towards quantifying the errors of MLIPs on the adversarial structures. As such, a fundamental question remains: To what extent can we control the actual errors of the MLIP on the adversarial structures? To answer this question, we have developed the Calibrated Adversarial Geometry Optimization (CAGO) algorithm, which aims to discover new adversarial structures with target force errors preassigned by the user (Figure~\ref{fig:al}). To unify estimated prediction uncertainties with real errors, we perform uncertainty calibration. To control the MLIP prediction errors on adversarial structures, we optimize structures to moderate target errors. These structures are within the range of validity of the uncertainty calibration while being challenging structures for the MLIPs, from which they can learn. To demonstrate the usefulness of our approach, we integrate CAGO into an active learning framework, as shown in Figure~\ref{fig:al}, enabling us to learn liquid-water dynamics and water adsorption in metal-organic frameworks from small datasets.

\begin{figure*}
    \includegraphics[width=0.9\textwidth]{figures/figure1.pdf}
    \caption{
    Active learning loop with calibrated adversarial geometry optimization (CAGO). Uncertainty calibration, the determination of calibration parameters, is performed using training data and a committee of MLIPs. The CAGO algorithm is applied to the MD-sampled structures from the current iteration of the MLIPs to obtain new structures. These structures are subsequently used in reference ab initio calculations and added to the training set.
    }
    \label{fig:al}
\end{figure*}
\section{Results}
\subsection{Uncertainty quantification and calibration}
MLIPs predict quantities $y$, such as forces, energies, and virials, depending on the structure $\mathbf x$, and we would like to estimate the root mean square error $\sigma_{\text{rmse}}$ of the predictions $\hat y$ with respect to a ground truth reference $y_{\text{ref}}$:
\begin{equation}
    \sigma_{\text{rmse}}^2(\mathbf x)=\frac 1M \sum^M_{m=1}\left|\hat{y}^m-y_{\text{ref}}\right|^2,
\end{equation}
where $m$ denotes one of the $M$ models. In the committee approach, this error is estimated from the standard deviation of predictions:
\begin{equation}
    \hat\sigma^2 = \frac{1}{M-1}\sum_{m=1}^M\left(\hat{y}^m-\bar y\right)^2,
\end{equation}
where $\bar y$ denotes the committee mean. In practice, the committee uncertainty estimates $\hat\sigma$ typically underestimates the actual prediction error  $\sigma_{\text{rmse}}$. To achieve a statistically correct uncertainty estimate, we employ uncertainty calibration, following the procedure introduced in ref.~\citenum{palmer_calibration_2022}, where the uncertainty estimate $\sigma$ is considered well-calibrated when the ratio
\begin{equation}
    r = \frac{\hat y-y_{\text{ref}}}{\sigma}
\end{equation}
is normally distributed with bias zero, and the standard deviation is equal to 1. Many different uncertainty calibration schemes have been proposed.\cite{musil_fast_2019,imbalzano_uncertainty_2021} Here, we opt for a power law calibration strategy:\cite{musil_fast_2019}
\begin{equation}\label{eq:error-cal}
    \sigma_{\text{cal}}=a\cdot\hat \sigma^b,
\end{equation}
where $a$ and $b$ are determined by optimizing the negative log-likelihood that $\hat y-y_{\text{ref}}$ was drawn from a normal distribution with zero bias and standard deviation in eq.~\eqref{eq:error-cal} over structures $\mathbf x:$\cite{palmer_calibration_2022}
\begin{equation}\label{eq:log-likelihood}    a,\,b=\argmin_{a',\,b'}\sum_{\mathbf{x}} \left[2\pi+\ln\left(a'\hat \sigma^{b'}\right)^2+\frac{\left|\hat y(\mathbf x)-y_{\text{ref}(\mathbf x)}\right|^2}{\left(a'\hat \sigma^{b'}\right)^2}\right].
\end{equation}

\subsection{Adversarial structures}
Like in ref.~\citenum{schwalbe-koda_differentiable_2021}, we create adversarial attacks by optimizing the structure $\mathbf x$ according to a fitness function $\mathcal L$. However, instead of maximizing the committee uncertainty estimate $\hat\sigma$, we optimize calibrated prediction uncertainties $\sigma_\text{cal}$ towards the uncertainty target $\delta$:
\begin{equation}
    \mathcal L(\sigma_{\text{cal}})=\left(\sigma_{\text{cal}}(\mathbf x)-\delta\right)^2.
\end{equation}
By targeting a specific prediction uncertainty $\delta$, we aim to push the predictions $\hat y$ outside the MLIPs comfort zone, where the error is considerably higher than the training set error. This ensures that the resulting structures contain information that expands the training set while at the same time maintaining the consistency between real errors and estimated errors.

\subsection{Biasing adversarial structures}
In addition to the prediction uncertainty of adversarial structures, it can be desirable to bias the adversarial structural properties toward a target value for $y_{\text{bias}}$, e.g., certain pressures or force magnitudes less than a specified threshold. This can be achieved by supplementing the fitness function by:
\begin{equation}
    \mathcal L_{\text{bias}}(\bar y)=l_{\text{bias}}\left(\bar y(\mathbf x)-y_{\text{bias}}\right)^2,
\end{equation}
where $l_{\text{bias}}$ is a prefactor determining how strictly the bias is applied.  

\subsection{Force-based calibrated adversarial geometry optimization}
Throughout the rest of this paper, we will consider force-based adversarial geometry optimization, with the molecular structure $\mathbf x$ being determined by the optimization problem:
\begin{equation}
    \mathbf{x} = \argmin_\mathbf{x}\sum\limits_{i=1}^{N_\text{atoms}} \left((\hat\sigma_{\mathbf F_i}(\mathbf x)-\delta)^2+l_{\text{bias}}\left|\bar F_i(\mathbf x)\right|^2\right),
\label{eq:opt-problem}
\end{equation}
where $N_\text{atoms}$ is the number of atoms, $\hat\sigma_{\mathbf F_i}$ is given by
\begin{equation}
\hat\sigma_{\mathbf F_i}=\sqrt{\hat\sigma_{|F_{ix}|}^2+\hat\sigma_{|F_{iy}|}^2+\hat\sigma^2_{|F_{iz}|}},
\end{equation}
and $|\bar F_i(\mathbf x)|$ denotes the norm of the average force on atom $i$.

\subsection{Calibrated adversarial geometry optimization to target error}\label{sec:res-cali}
We start by considering the error of force predictions on liquid water from a committee of 20 MLIPs trained on a synthetic training set of DNN@MB-pol\cite{bore_realistic_2023} MD trajectories (see Methods~\ref{sec:cago-benchmark} for more details). Figure~\ref{fig:adv-illu}a compares the ratio of the distribution error and the estimated calibrated/uncalibrated uncertainties against the normal distribution from ideal error calibration.\cite{palmer_calibration_2022} While both curves show a bell curve, the wider shape of the force error\textendash uncalibrated uncertainty ratio shows that the uncalibrated uncertainty significantly underestimates the true uncertainty. In contrast, the calibrated uncertainty leads to an almost perfect agreement. In the Supplementary Information, we show that this is also the case for energy and virial predictions. The near-perfect error statistics for the force errors of the calibrated uncertainty is reflected in the one-to-one correlation between the mean force error and the force uncertainty estimate in Figure~\ref{fig:adv-illu}b. For high force errors, the correlations are good, albeit more noisy than in the low-force-errors regime. High-error samples are inherently less frequent than low-error samples. This means the uncertainty calibration has access to less data for high-error samples and is therefore expected to be less accurate in this case than in the low-error regime. As such, the force error versus the force uncertainty can act as a heuristic to determine the validity range for calibrated uncertainty estimation, in this case, up to about $\sim200$~meV/\AA.
\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.49\textwidth]{figures/figure2.pdf}
    \caption{Uncertainty calibration and error statistics on adversarial structures. {\bf a.} Comparison of error statistics of forces predicted by a committee of models divided by estimated prediction uncertainty against normal distribution. {\bf b.} Force errors for a committee of models vs.\,estimated uncertainty. {\bf c.} Force error statistics for structures from MD, CAGO with calibrated uncertainty, and CAGO without uncertainty-calibration, with associated estimated mean errors in the inset figure.}
    \label{fig:adv-illu}
\end{figure}

Next, we perform CAGO to solve equation~\eqref{eq:opt-problem} without a bias-term by geometry optimization using numerical finite-difference gradients for 40 structures (see Figure~\ref{fig:al}, all structures reaching the target force uncertainty indicated in green). Figure~\ref{fig:adv-illu}c reports the corresponding error statistics, i.e., individual model forces versus reference DNN@MB-pol forces. The error statistics for adversarial structures for CAGO with calibrated uncertainties lead to a mean error close to the target error, whereas the uncalibrated case has errors about twice as high as the target. The potential usefulness of such calibrated adversarial structures can be understood when compared to ordinary MD structures, which have about half the error and would, therefore, not provide the same level of learning content if added to the training set.
\begin{figure}[!htb]
    \centering
    \includegraphics[width=1\textwidth]{figures/figure3.pdf}
    \caption{Benchmark on the effect of CAGO algorithm hyperparameters. The panels show the distribution of MLIP force errors on adversarial structures, with corresponding mean errors in the inset of each panel. Line legends and colors are specified by ticks in inset figures. \textbf{a.} Force error statistics for different uncertainty targets. \textbf{b.} Force error statistics for adversarial structures optimized for a target 100 meV/\AA\ uncertainty with different committee sizes, using uncertainty calibration from a committee of 20 MLPs. \textbf{c.} Force error statistics for adversarial structures optimized for a target 100~meV/\AA\ uncertainty, considering uncertainty calibration performed with its respective committee size.}
    \label{fig:benchmark}
\end{figure}

The CAGO algorithm has several hyperparameters that can be adjusted in accordance with user goals and are benchmarked in Figure~\ref{fig:benchmark}. Figure~\ref{fig:benchmark}a reports the error statistics for different error targets $\delta$ in equation~\eqref{eq:opt-problem}. By tuning this parameter, the real error of adversarial structures can be controlled. A clear trend here is that CAGO is more accurate when targets have moderate values, in this case, up to about 200~meV/\AA, while for higher values, the real errors are higher than the target errors. This is in line with our observations from Figure~\ref{fig:adv-illu}b, where above 200~meV/\AA\ the uncertainty calibration is less accurate.

When performing CAGO, using a small committee of models is desirable due to the linear increase in computational cost with committee size. Interestingly, Figure~\ref{fig:benchmark}b shows that the error statistics are not very sensitive to committee size, with a few models sufficing to reach close to the target error. On the contrary, Figure~\ref{fig:benchmark}c shows how committee size is critical for achieving reliable calibration, where adversarial structures from small committees calibrated with the same number of models do not hit their target. In this case, a committee of around 10 models is necessary to reach the target error, but this may be system- and MLIP-architecture-dependent. This indicates that using many models to calibrate the uncertainties (Figures~\ref{fig:benchmark}c) and performing CAGO using a few models (Figures~\ref{fig:benchmark}b) is a good heuristics for performing CAGO reliably and efficiently. CAGO can also be performed with bias terms while maintaining the target error, as demonstrated by our benchmark reported in the Supplementary Information.

\subsection{Learning liquid water from a single structure}
An MLIP is only as good as its reference data. Therefore, creating a training set with as high-quality references as possible is desirable. However, higher quality is generally associated with higher computational cost. In particular, density-corrected DFT (DC-DFT) and density-corrected R$^2$SCAN (DC-R$^2$SCAN) achieve excellent correspondence in energies with respect to coupled cluster methods.\cite{dasgupta_elevating_2021,dasgupta_nuclear_2024} However, DC-DFT and DC-R${}^2$SCAN are rather expensive compared to ordinary functionals, making the standard route of starting from a training set of ab initio MD trajectories prohibitively expensive. Taking this to the extreme, here we use CAGO-based active learning to learn the MLIP for DC-R$^2$SCAN starting from only a single structure of liquid water (Figure~\ref{fig:water-deepmd-allegro}a). Figure~\ref{fig:water-deepmd-allegro} reports our benchmark on the performance of MLIP iterations during active learning for Allegro with error target 100~meV/\AA, and DeePMD with target 100~meV/\AA\ and 200~meV/\AA MLIPs (refer to Methods~\ref{sec:MLIPs} for additional details).
\begin{figure*}[!htb]
    \centering
    \includegraphics[width=0.99\linewidth]{figures/figure4.pdf}
    \caption{Benchmark of convergence of liquid-water properties for CAGO-based active learning starting out from a single structure. We report data for the Allegro MLIP with target error 100~meV and DeePMD with two different target errors, 100~meV and 200~meV (see Methods~\ref{sec:MLIPs} for more details). Each point in the graphs has been computed from 12 models using the subset of stable MLIPs, with three replica simulations each, for a total of 540 and 1800 Allegro and DeePMD 2~ns MD simulations, respectively. The error bars are the standard deviation between the different committee members, and the horizontal lines correspond to the final value of the property for the model. \textbf{a.} The liquid water box structure that was used to start the active learning training. Oxygen is represented in red, with hydrogen in white. \textbf{b.} Percentage stable MLIPs in accordance with the stability criteria in Methods~\ref{sec:stability}. \textbf{c.} Average force error of MLIPs on the liquid water structures. \textbf{d.} Mean liquid water density. \textbf{e.} Self-diffusion coefficient. \textbf{f.} First coordination number (integral of the first radial distribution function peak).}
    \label{fig:water-deepmd-allegro}
\end{figure*}

Figure~\ref{fig:water-deepmd-allegro}b reports the percentage of stable models. At around 80 training-set structures, Allegro models all become stable, while the DeePMD models achieve stability at around 220 or 350 for the 200~meV/\AA\ and 100~meV/\AA\ targets, respectively. The slower convergence of DeePMD is in line with past research, which shows that MLIPs based on equivariant layers are not only more accurate but also more data-efficient,\cite{batzner_e3-equivariant_2022,musaelian_learning_2023} with similar equivariant NequIP also being benchmarked to be more stable than DeePMD.\cite{fu2023forces} However, DeePMD, with our CAGO structures, performs rather well compared to its data-hungry reputation.\cite{batzner_e3-equivariant_2022} To put this into context, ref.~\citenum{fu2023forces} used 10\,000 structures and achieved stability for an average of 0.247 nanoseconds under $NVT$ conditions with a simple water force-field. In contrast, we achieve 100~\% stability for two nanoseconds under the more challenging $NPT$ conditions for a complex many-body ab initio method. 

To benchmark how well CAGO-based active learning performs for energetics, we report in Figure~\ref{fig:water-deepmd-allegro}c the average force error for 100 liquid water MD structures. In accordance with the literature,\cite{musaelian_learning_2023} the force errors for the trained models converge to different levels of error for Allegro (37~meV/\AA) and DeePMD (63~meV/\AA), with the Allegro force errors being smaller than those of DeePMD. Similar force error magnitudes were also reported in ref.~\citenum{batzner_e3-equivariant_2022}. It is noteworthy that while CAGO distorts the structures from MD, force errors in undistorted liquid MD structures improve. In particular, for both targets with DeePMD, we see a similar error level. Given that 63~meV/\AA\ is relatively close to the target 100~meV/\AA, it may be more advantageous for DeePMD to use a higher target error than for Allegro, and we see that energetics do not suffer from this. In fact, the stability test shown in Figure~\ref{fig:water-deepmd-allegro}b indicates that the 200~meV/\AA\ target improves the stability convergence.

Next, in Figures~\ref{fig:water-deepmd-allegro}d, e, and f, we benchmark the thermodynamical, dynamical, and structural properties of liquid water, respectively. As with the stability, for all properties, Allegro converges around 80 structures, with DeePMD following closely. Overall, DeePMD exhibits larger variability for these properties. This is consistent with past benchmark studies on MB-pol-based DeePMD, which, even for huge training sets, struggle to achieve the correct density,\cite{zhai_short_2023,zhai2024many} whereas Allegro gets it spot on.\cite{maxson2024transferable} Nonetheless, we only observe variability to within a few percent.

\subsection{Learning water-adsorption of a metal-organic framework}
Water in nano-porous materials constitutes an extra challenging system due to the combination of a vast configurational space associated with water as a guest molecule and the potential for the material to catalyze chemical transformations, such as proton hopping. In Figure~\ref{fig:uio66-water-allegro}, we report a benchmark of CAGO-based active learning with DC-R$^2$SCAN for water adsorption inside UiO-66, a zirconium-based metal-organic framework (MOF) with a very large surface area as well as high thermal stability.\cite{cavka2008new} We start the training from 11 structures with different water content (see Supplementary Information). For all benchmarked properties, we see a systematic convergence with training set data just after $\sim$250 training set structures. This is a higher number than for liquid water, but this system also involves two additional chemical species with a higher level of chemical variability. To put this into context, a similar study of zeolite-OSADA pair with adversarial active learning started from a total of 17\,492 structures and achieved a 97\% stability rate after adding 573 adversarial structures.\cite{schwalbe-koda_differentiable_2021} In another impressive study, a training set of 400\,000 energies and forces from adversarial active learning, using gas-phase calculations, was used to develop an MLIP for silica with reactive water.\cite{roy2024learning}
\begin{figure*}[!htb]
    \centering
\includegraphics[width=1.\textwidth]{figures/figure5.pdf}
    \caption{Benchmark of convergence of properties for water-adsorption in MOF for CAGO-based active learning for DC-R$^2$SCAN, with Allegro MLIPs. When presented, the error bars are the standard deviation of the measurements of each committee member at that iteration. Each point in the graphs has been computed from 12 models using the subset of stable MLIPs, with three replica simulations each, for a total of 576 two nanoseconds MD simulations. The error bars and horizontal lines hold the same meaning as in Figure~\ref{fig:water-deepmd-allegro}.
\textbf{a.} Water MOF system used in this benchmark. The color coding for the atoms is red, white, gray, and green for oxygen, hydrogen, carbon, and zirconium, respectively. \textbf{b.} Percentage MLIPs fulfilling stability criteria in Methods~\ref{sec:stability}. \textbf{c.} Average force, energy, and virials error on MOF/water structures. \textbf{d.} Mean mass density. \textbf{e.} Water diffusion coefficient. \textbf{f.} First oxygen-oxygen coordination number.}
    \label{fig:uio66-water-allegro}
\end{figure*}

\section{Discussion}
In this paper, we presented the CAGO algorithm to systematically generate adversarial molecular structures with user-assigned MLIP prediction errors. Through our benchmarks of the different hyperparameters of CAGO, we establish suitable minimums. In particular, we found it important to use many models, about 10 models in our case, to obtain a good error calibration, while the CAGO algorithm itself works well even for small committees. For the true error to converge to the target error, moderate uncertainty targets should be used. This is in line with our error calibration analysis, where we saw that error calibration is less robust for the high-error regime, where calibration data is scarce. 

These insights enabled us to perform active learning using minimal datasets with simple active learning protocols. In particular, we learned liquid water from a single structure of water for both Allegro and DeePMD and only 11 for water adsorption in the UiO-66 metal-organic framework. Although CAGO seemingly introduces an extra step of complexity into the active learning scheme, it ultimately simplifies the rest of the workflow. With CAGO, fewer manual adjustments and tricks are needed, such as low/high-temperature sampling or reducing the size of MD steps to achieve a stable model. Moreover, we have used orders of magnitude fewer structures than is typically reported in other works on adversarial active learning.\cite{schwalbe-koda_differentiable_2021,roy2024learning} A key problem with current MLIPs is their typical dependence on large and approximate but cheap DFT datasets. CAGO-based active learning offers a promising solution by only requiring a few structures, allowing highly accurate but costly coupled-cluster and quantum Monte Carlo reference calculations to be used as training data.

\section{Methods}
\subsection{CAGO algorithm and implementation}
The CAGO algorithm works much like ordinary molecular geometry/structure optimization, but instead of minimizing the energy, CAGO minimizes the fitness function in equation~\eqref{eq:opt-problem}, to obtain a prescribed uncertainty for the MLIPs. In particular, we use the L-BFGS algorithm, as implemented in the SciPy library,\cite{virtanen_scipy_2020} to optimize equation~\eqref{eq:opt-problem} with respect to scaled coordinates and cell vectors. The gradients are computed numerically by a two-point finite difference operator. While computing the hessian analytically is possible and most likely faster for differentiable MLIP architectures, as done in ref.~\citenum{schwalbe-koda_differentiable_2021}, numerical derivatives have certain advantages. First, single-point calculations are fast with MLIPs, making the 3$N$+6+1 calculations needed for numerical gradient calculations feasible. Second, these single-point calculations can be prepared for batch-based calculations, rather than being performed one by one, which is efficient with GPUs. Third, the calculations are trivially parallelizable, making linear scaling by adding more GPUs possible. Fourth, Hessians are generally not highly optimized with PyTorch and incur significant overheads. Finally and most importantly, while single-point calculations are ubiquitous among MLIPs, Hessians are rarely available out of the box and, therefore, not amenable to general active learning pipelines, forcing a reliance on specific MLIP architectures that may not be state-of-the-art.

\subsection{Machine learning interatomic potentials}\label{sec:MLIPs}
We tested CAGO on two different MLIPs with different architectures: \emph{Allegro} and \emph{DeepMD}. 
\paragraph{Allegro} 
For Allegro models,\cite{musaelian_learning_2023} we used equivariant E(3) products up to $L_\text{max} = 2$ in the tensor layers, with two interaction layers, including three tensor product layers of 128 neurons each. Before the interaction layer, a feature layer with 16 input features was used. Following the interaction layer block, we used three latent layers, each containing 128 neurons. A polynomial cutoff of 6~Å was used with a trainable Bessel basis set of 8 basis functions to form the feature descriptors. We used a radial cutoff of 6~Å for building the neighbor lists. Our loss function used forces, energy, and stresses, with coefficients 1, 5, and 100, respectively. These settings are similar to those previously used to generate water MLIPs with Allegro.\cite{maxson2024transferable}

\paragraph{DeePMD}
For DeePMD models,\cite{zeng_deepmd-kit_2023} we used the se\_e2\_a architecture and 25, 50, 100 neurons for the hidden embedding layer, with the submatrix of the embedding matrix using 16 neurons. Similarly to Allegro, a distance cutoff of 6~Å was used, but with a smoothing region of 0.5~Å. The potential is represented by a fully connected deep neural network with three layers of 240 neurons each. These settings have also been used before in the study of different water phases.\cite{zhai_short_2023,bore_realistic_2023} 

Input files detailing the setup of MLIPs are available in the data repository associated with this manuscript. 


\subsection{Reference calculations for training and test set data} \label{sec:ref}
\paragraph*{DNN@MB-pol} For benchmarking CAGO in \ref{sec:res-cali}, we ran reference caclulations using the DNN@MB-pol model developed in refs.~\citenum{bore_realistic_2023,sciortino2025constraints} based on the MB-pol water model,\cite{babin2013development,babin2014development} using the se\_e2\_a DeePMD MLIP architecture.\cite{zeng_deepmd-kit_2023} 

\paragraph*{DC-R$^2$SCAN} To benchmark CAGO as part of active learning, we employed the DC-R$^2$SCAN method. To calculate the DC-R$^2$SCAN reference energies, forces, and virials for the MLIPs, we used the recent implementation for density-corrected DFT in CP2K,\cite{belleflamme_radicals_2023} which has in a series of papers demonstrated great accuracy with benchmarks against coupled-cluster levels of theory.\cite{dasgupta_elevating_2021,dasgupta_nuclear_2024} The atomic core electrons were described using Goedecker–Teter–Hutter (GTH) pseudopotentials, while valence electron molecular orbitals were expanded in triple-zeta double-polarized basis sets (TZV2P) optimized for the SCAN functional. The kinetic energy cutoff for the plane-wave expansion of the density was set to 2500~Ry, which was required to get a numerical accuracy of $\sim$2~meV/\AA\, and $\sim$0.05 kBar for forces and stress-tensor trace, respectively. The truncated Coulomb operator with a cutoff radius between 5.0 and 5.5 {\AA} was used depending on the system, corresponding to approximately half the length of the smallest edge of the simulation cell. To overcome the expense of the Hartree-Fock calculation, the ADMM approximation was used with an optimized valence basis set (BASIS\_ADMM\_UZH). The Schwarz integral screening threshold was set to $10^{-6}$ atomic units, starting from a converged PBE calculation. All the DC-R$^2$SCAN calculations in this work were carried out with CP2K, with associated inputs and outputs stored in the data repository associated with this manuscript.  

\subsection{CAGO-based active learning procedure and implementation}
The CAGO-based active learning procedure is an iterative process depicted in Figure~\ref{fig:al}. Our implementation begins with an initial set of reference data, reserving 10~\% of this data for a test set. The remaining 90~\% reference data is partitioned into an 80~\% training set and a 10~\% validation set through bootstrapping for each machine learning interatomic potential (MLIP) committee member, as described in ref.~\citenum{palmer_calibration_2022}. After training the committee of MLIPs, their uncertainty is calibrated using the test set. Subsequently, MD simulations are conducted with the MLIP committee. From MD simulations, a subset of structures are randomly sampled to create adversarial structures with CAGO. To reduce the risk for failed reference calculations, a filter is applied to these structures, such as an upper threshold for the mean force magnitude of the MLIP committee. Reference calculations are then performed on the filtered adversarial structures before undergoing a final filter, checking for convergence and avoiding unphysical structures dominated by sterics before incorporating them into the reference data set, completing one iteration of the active learning cycle. This entire active learning loop is implemented in the Hylleraas Software Platform,\cite{hsp} which interfaces with various MLIPs and quantum chemistry softwares, enabling the use of heterogeneous computing environments on high-performance computer clusters.


\subsection{Computational procedures}\label{sec:sim-details}
\subsubsection{Calibrated adversarial geometry optimization to target error}\label{sec:cago-benchmark}
For benchmarking the CAGO algorithm in section~\ref{sec:res-cali}, we targeted the DNN@MB-pol water model as specified in Methods~\ref{sec:ref}. First, we trained a committee of 20 DeePMD MLIPs on a training set of MD trajectories. Second, using these 20 models and a separate test set, we performed uncertainty calibration in accordance with equation~\eqref{eq:log-likelihood}, the results of which are reported in Figure~\ref{fig:adv-illu}. Third, picking out 40 structures from the test set, we perform CAGO with various hyperparameters, such as different error targets and committee sizes. The errors we report are root mean square errors of individual models against DNN@MB-pol. For more details on the training and test set, we refer to the Supplementary Information. 

\subsubsection{Learning liquid water from a single structure}\label{sec:stability}
For the active learning workflow, we used committees with 12 MLIPs to learn liquid water from a single structure of 64 water molecules, with details on initial training sets reported in Supplementary Information. In each of the first two active learning iterations, we sampled five structures from the training set and optimized the structures with CAGO using random subsets of 3 committee members. For all subsequent iterations, MD sampling with LAMMPS~\cite{thompson_lammps_2022} was performed before CAGO (see Section~\ref{sec:sim-details}), and in the case of pure water, 20 samples were extracted and taken for CAGO considering all 12 committee members. Due to the low number of structures in the first iterations of active learning, the MLIPs tend to be unstable; we, therefore, performed $NVT$ simulations at 300~K to generate new structures for CAGO for iterations 3 and 4. These simulations were run for 50~ps using the velocity-Verlet integrator with a 0.25~fs timestep and Nosé-Hoover thermostat chain with 3 thermostats and a 0.5~ps relaxation time. From iteration 5 and onwards, we performed $NPT$ simulations at 300~K and 1~bar, sampling for 100~ps, using the same settings for the thermostat part, and a Nosé-Hoover barostat chain of 3 barostats with 1~ps relaxation time. Structures derived from CAGO with maximum forces (mean by committee members) higher than 40~eV/\AA\ or maximum force from reference calculations higher than 30~eV/\AA, or minimum distance lower than 0.75~\AA, or with convergence warnings from CP2K, were filtered out. Note that the DeePMD MLIPs used slightly varying settings for the training throughout the active learning loop to avoid overfitting. In the first two iterations, we trained the models for 50\,000 steps to avoid overfitting with small training sets. As more structures were added, we increased the number of training steps to 150\,000 for the next two active learning iterations, and from iteration 5 and onwards, we used 400\,000 training steps. 

For the benchmark of MLIPs along active learning iterations in Figure~\ref{fig:water-deepmd-allegro}, we ran 3 independent simulations starting from different initial conditions in a simulation box with 256 water molecules. We first performed two thermalizations for each system, one in $NVT$ (25~ps) and another in the $NPT$ (100~ps) ensemble, using the Berendsen thermostat and barostat. Finally, we ran a 2~ns $NPT$ simulation using the Nosé-Hoover thermostat and barostat. These simulations used a 0.5~fs timestep, and all other settings were the same as the ones described above for the sampling phase of active learning.

We use two criteria to define the stability of the MLIPs during MD simulations. First, we check every 100 timesteps that the minimal distance between atoms in the system is never below 0.4~\AA. Second, we monitor the system density, verifying that it does not diverge to values lower than 0.25 times or higher than 2 times the initial density. Furthermore, we ran multiple replicas, considering the MLIP to be stable only if all trajectories of the replicas meet our stability criteria.

\subsubsection{Learning water-adsorption of a metal-organic framework}
The water adsorption in the UiO-66 simulations was performed using similar workflow settings as for liquid water in Methods~\ref{sec:stability}, with a few modifications as follows. In particular, for this active learning workflow, we considered multiple systems, including liquid water and 10 structures of UiO-66 with differing water content (see Supplementary Information). Therefore, in this workflow, we started from an initial reference data set of 11 structures and extended the reference data set for each individual system. In each of the first two active learning iterations, we sampled five structures from the training set and optimized the structures with CAGO and random subsets of 3 committee members for each of the systems. For the rest of the iterations, we performed $NPT$ MD sampling for each system with all committee members, followed by CAGO on five structures with four committee members for each of the 11 systems. We used a timestep of 0.5~fs and ran the simulations for 100~ps during the active learning sampling phase. The benchmark of MLIPs along active learning iterations in Figure~\ref{fig:uio66-water-allegro} were performed using the system presented in Figure~\ref{fig:uio66-water-allegro}a, which is UiO-66 with a water weight percentage of 45, with the same MD protocol as in Methods~\ref{sec:stability}.

\section{Data availability}
The dataset and scripts used to produce the data in this study are publicly available via the Norwegian {\it National e-Infrastructure for Research Data} (NIRD) at \url{https://doi.org/10.11582/2025.00018}.

\section{Code availability}
The code implementing the active learning loop using CAGO is publicly available at GitLab: \url{https://gitlab.com/hylleraasplatform/hyal}.

\section{Acknowledgments}
The work was supported by the Research Council of Norway through the Centre of Excellence Hylleraas Centre for Quantum Molecular Sciences (Grant 262695) and the Young Researcher Talent grants 344993 and 354100, as well as by the EuroHPC Joint Undertaking (Grants EHPC-REG-2023R02-088, EHPC-REG-2023R03-146). Support was also received from the Centre for Advanced Study in Oslo, Norway, which funded and hosted the SLB Young CAS Fellow research project during the academic year of 23/24 and 24/25. Part of the simulations were performed on resources provided by Sigma2 — the Norwegian National Infrastructure for High-Performance Computing and Data Storage (grant numbers NN4654K and NS4654K).


\bibliographystyle{apsrev4-1}
\bibliography{mybib}

\end{document}