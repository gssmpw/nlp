\section{Experiments}
\label{experiments}

\begin{table*}[t]
\tiny
% \caption{Overall comparison of SWAT and other models on eight common-sense reasoning tasks. (-) denotes negative slopes (i.e., ALiBi slope),  (+) denotes positive slopes, while (-+) means half of the attention heads have negative slopes and half have positives. Optimal values are marked in bold, and second-best values are underlined.}
\caption{Overall comparison of SWAT and other models on eight common-sense reasoning tasks. Bold values represent optimal performance, while second-best values are underlined. ``\textbf{{ *}}'' indicates the statistically significant improvements (i.e., two-sided t-test with $p<0.05$) over the best baseline. $\uparrow$: higher is better. $\downarrow$: lower is better.}
\label{tab:overall} 
\resizebox{\textwidth}{!}{
\begin{tabular}{@{}lccccccccccc@{}}
\toprule
\multicolumn{1}{l|}{Model} & \begin{tabular}[c]{@{}c@{}}Wiki.\\ ppl $\downarrow$\end{tabular} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}LMB. \\ ppl $\downarrow$\end{tabular}} & \begin{tabular}[c]{@{}c@{}}LMB. \\ acc $\uparrow$\end{tabular} & \begin{tabular}[c]{@{}c@{}}PIQA\\ acc $\uparrow$\end{tabular} & \begin{tabular}[c]{@{}c@{}}Hella. \\ acc\_n $\uparrow$\end{tabular} & \begin{tabular}[c]{@{}c@{}}Wino. \\ acc $\uparrow$\end{tabular} & \begin{tabular}[c]{@{}c@{}}ARC-e\\ acc $\uparrow$\end{tabular} & \begin{tabular}[c]{@{}c@{}}ARC-c\\ acc\_n $\uparrow$\end{tabular} & \begin{tabular}[c]{@{}c@{}}SIQA\\ acc $\uparrow$\end{tabular} & \begin{tabular}[c]{@{}c@{}}BoolQ\\ acc $\uparrow$\end{tabular} & \begin{tabular}[c]{@{}c@{}}Avg.\\ $\uparrow$\end{tabular} \\ \midrule \midrule
\multicolumn{12}{c}{340M params / 15B tokens} \\ \midrule
\multicolumn{1}{l|}{Transformer++} & 31.52 & \multicolumn{1}{c|}{41.08} & 30.76 & 62.98 & 34.76 & 50.53 & 45.21 & 24.05 & 36.81 & 58.24 & 42.92 \\
\multicolumn{1}{l|}{RetNet} & 32.50 & \multicolumn{1}{c|}{49.73} & 28.24 & 62.61 & 34.15 & 50.91 & 44.27 & 23.62 & 36.79 & 59.72 & 42.54 \\
\multicolumn{1}{l|}{GLA} & 28.51 & \multicolumn{1}{c|}{43.02} & 28.73 & 64.05 & 35.96 & 50.00 & 54.19 & 24.29 & 37.13 & 58.39 & 44.09 \\
\multicolumn{1}{l|}{Mamba} & 30.83 & \multicolumn{1}{c|}{40.21} & 29.94 & 63.79 & 35.88 & 49.82 & 49.24 & 24.56 & 35.41 & 60.07 & 43.59 \\
\multicolumn{1}{l|}{DeltaNet} & 28.65 & \multicolumn{1}{c|}{47.30} & 28.43 & 63.52 & 35.95 & 49.63 & 52.68 & 25.37 & \underline{37.96} & 58.79 & 44.04 \\
\multicolumn{1}{l|}{TTT} & 27.44 & \multicolumn{1}{c|}{34.19} & 30.06 & 63.97 & 35.71 & 50.08 & 53.01 & 26.11 & 37.32 & 59.83 & 44.51 \\
\multicolumn{1}{l|}{Gated DeltaNet} & \underline{27.01} & \multicolumn{1}{c|}{\underline{30.94}} & \underline{34.11} & 63.08 & 38.12 & \underline{51.60} & 55.28 & 26.77 & 34.89 & 59.54 & 45.42 \\
\multicolumn{1}{l|}{Titans} & \textbf{26.18} & \multicolumn{1}{c|}{\textbf{29.97}} & \textbf{34.98} & 64.73 & \textbf{39.61} & \textbf{51.85} & 55.60 & \underline{28.14} & 34.52 & 59.99 & \underline{46.17} \\
\multicolumn{1}{l|}{SWAT (-)} & 33.32 & \multicolumn{1}{c|}{36.75} & 32.80 & \textbf{ 65.94*} & \underline{38.99} & 50.12 & \textbf{ 59.68*} & \textbf{ 28.24*} & \textbf{ 38.69*} & \underline{60.55} & \textbf{ 46.88*} \\
\multicolumn{1}{l|}{SWAT (+)} & 37.47 & \multicolumn{1}{c|}{49.15} & 29.59 & 65.40 & 36.92 & 50.43 & 54.55 & 26.88 & 37.67 & 58.93 & 45.05 \\
\multicolumn{1}{l|}{SWAT (-+)} & 35.53 & \multicolumn{1}{c|}{45.06} & 29.96 & \underline{65.67} & 37.39 & 50.91 & \underline{56.99} & 27.05 & 36.75 & \textbf{ 62.11*} & 45.85 \\ \midrule
\multicolumn{12}{c}{760M params / 30B tokens} \\ \midrule
\multicolumn{1}{l|}{Transformer++} & 25.21 & \multicolumn{1}{c|}{27.64} & 35.78 & 66.92 & 42.19 & 51.95 & 60.38 & 32.46 & 39.51 & 60.37 & 48.69 \\
\multicolumn{1}{l|}{RetNet} & 26.08 & \multicolumn{1}{c|}{24.45} & 34.51 & 67.19 & 41.63 & 52.09 & 63.17 & 32.78 & 38.36 & 57.92 & 48.46 \\
\multicolumn{1}{l|}{Mamba} & 28.12 & \multicolumn{1}{c|}{23.96} & 32.80 & 66.04 & 39.15 & 52.38 & 61.49 & 30.34 & 37.96 & 57.62 & 47.22 \\
\multicolumn{1}{l|}{Mamba2} & 22.94 & \multicolumn{1}{c|}{28.37} & 33.54 & 67.90 & 42.71 & 49.77 & 63.48 & 31.09 & 40.06 & 58.15 & 48.34 \\
\multicolumn{1}{l|}{DeltaNet} & 24.37 & \multicolumn{1}{c|}{24.60} & 37.06 & 66.93 & 41.98 & 50.65 & 64.87 & 31.39 & 39.88 & 59.02 & 48.97 \\
\multicolumn{1}{l|}{TTT} & 24.17 & \multicolumn{1}{c|}{23.51} & 34.74 & 67.25 & 43.92 & 50.99 & 64.53 & \underline{33.81} & \textbf{40.16} & 59.58 & 47.32 \\
\multicolumn{1}{l|}{Gated DeltaNet} & \underline{21.18} & \multicolumn{1}{c|}{22.09} & 35.54 & 68.01 & 44.95 & 50.73 & \textbf{66.87} & 33.09 & 39.21 & 59.14 & 49.69 \\
\multicolumn{1}{l|}{Titans} & \textbf{20.04} & \multicolumn{1}{c|}{21.96} & 37.40 & 69.28 & \underline{48.46} & 52.27 & \underline{66.31} & \textbf{35.84} & \underline{40.13} & \textbf{62.76} & \underline{51.56} \\
\multicolumn{1}{l|}{SWAT (-)} & 23.41 & \multicolumn{1}{c|}{\underline{21.05}} & \textbf{ 40.81*} & \textbf{ 69.80*} & \textbf{ 48.65*} & 51.69 & 65.15 & 33.53 & 39.95 & 61.07 & \textbf{ 51.85*} \\
\multicolumn{1}{l|}{SWAT (+)} & 23.91 & \multicolumn{1}{c|}{\textbf{21.05}} & 39.01 & 69.59 & 47.64 & \underline{53.43} & 64.73 & 32.34 & 39.15 & 57.95 & 50.48 \\ 
\multicolumn{1}{l|}{SWAT (-+)} & 23.34 & \multicolumn{1}{c|}{21.36} & \underline{39.08} & \underline{69.70} & 48.16 & \textbf{53.91*} & 65.15 & 31.06 & 39.41 & \underline{61.62} & 51.01 \\
% \midrule
% \multicolumn{12}{c}{1.3B params / 100B tokens} \\ \midrule
% \multicolumn{1}{l|}{Transformer++} & 18.53 & \multicolumn{1}{c|}{18.32} & 42.60 & 70.02 & 50.23 & 53.51 & 68.83 & 35.10 & 40.66 & 57.09 & 52.25 \\
% \multicolumn{1}{l|}{RetNet} & 19.08 & \multicolumn{1}{c|}{17.27} & 40.52 & 70.07 & 49.16 & 54.14 & 67.34 & 33.78 & 40.78 & 60.39 & 52.02 \\
% \multicolumn{1}{l|}{Mamba} & 17.92 & \multicolumn{1}{c|}{15.06} & 43.98 & 71.32 & 52.91 & 52.95 & 69.52 & 35.40 & 37.76 & 61.13 & 53.12 \\
% \multicolumn{1}{l|}{DeltaNet} & 17.71 & \multicolumn{1}{c|}{16.88} & 42.46 & 70.72 & 50.93 & 53.35 & 68.47 & 35.66 & 40.22 & 55.29 & 52.14 \\
% \multicolumn{1}{l|}{Gated DeltaNet} & 16.42 & \multicolumn{1}{c|}{12.17} & 46.65 & 72.25 & 55.76 & 57.45 & 71.21 & 38.39 & 40.63 & 60.24 & 55.32 \\
% \multicolumn{1}{l|}{SWAT (-)} & 18.41       & \multicolumn{1}{c|}{12.85} & 39.01 & 72.63 & 56.47 & 56.67 & 73.36 & 40.19 & 41.91 & 60.37 & 55.08 \\
% \multicolumn{1}{l|}{SWAT (+)} & \multicolumn{1}{l}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} \\
% \multicolumn{1}{l|}{SWAT (-+)} &  & \multicolumn{1}{c|}{} &  &  &  &  &  &  &  &  &  \\ 
\bottomrule
\end{tabular}
}
\end{table*}



\subsection{Experiment Settings}

% In preliminary experiments, we compare our model with the vanilla Transformer~\cite{llama2}. In the overall performance comparison, we utilize the experimental results from Titans~\cite{titans} and Gated DeltaNet~\cite{gateddeltanet}. The experiments are based on two GitHub repositories nanoGPT\footnote{\url{https://github.com/karpathy/nanoGPT}} and flash-linear-attention.\footnote{\url{https://github.com/fla-org/flash-linear-attention}}
\paragraph{Datasets.}

For the overall comparison, models are trained on the 100BT subset of FineWeb-Edu~\cite{fineweb-edu}, which is a high-quality educational dataset designed for LLM pre-training.

% In preliminary experiments, we employed three datasets for model pre-training and evaluation: OpenWebText~\cite{openwebtext}, OpenOrca~\cite{OpenOrca}, and PG-19~\cite{pg19}. We utilized OpenWebText as the training dataset, while all three datasets were incorporated into the validation phase. 
% We extended the input sequence length to 16,384 tokens for OpenWebText and PG-19, while for OpenOrca, we specifically selected questions with the longest context segments to validate long-context processing ability.

% OpenWebText, with its shorter texts, evaluates fundamental language modeling. PG-19, based on book-length texts, tests information compression in long-form contexts. OpenOrca, a question-answering dataset, helps evaluate the model's ability to retain information across extended sequences. 


\paragraph{Baselines.}

Our baselines include state-of-the-art models including both vanilla Transformer and recurrent models. Specifically, we compare our approach against Transformer++~\cite{llama2}, RetNet~\cite{retnet}, Gated Linear Attention (GLA)~\cite{gla}, Mamba~\cite{mamba}, DeltaNet~\cite{deltanet}, TTT~\cite{ttt}, Gated DeltaNet~\cite{gateddeltanet}, and Titans~\cite{titans}. 


\paragraph{Implementation Details.}

We pre-train SWAT with model sizes of 340M and 760M parameters on 15B and 30B tokens, respectively. The training uses the same vocabulary as Llama 2~\cite{llama2}, with a sequence length of 4096 tokens and a batch size of 0.5M tokens.

% RMSNorm~\cite{RMSNorm} for normalization.

\paragraph{Evaluation Metrics.}

We evaluate model performance using perplexity (ppl), accuracy (acc), and normalized accuracy (acc\_n). Perplexity measures language modeling ability, where lower values indicate better predictions. Accuracy assesses classification performance by calculating the proportion of correct predictions. Normalized accuracy is adjusts for dataset difficulty variations, ensuring fair comparisons across different evaluation settings. 


\begin{table*}[t]
\caption{Performance comparison of language models pretrained with and without sliding windows.}
\label{tab:performance_comparison}  % 设置标签
\resizebox{\textwidth}{!}{
\begin{tabular}{@{}l|ccc|cccc|cccc|c@{}}
\toprule
\multirow{2}{*}{\textbf{Models}} & \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Training\\ Window\end{tabular}}} & \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Training \\  Length\end{tabular}}} & \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Eval\\ Window\end{tabular}}} & \multicolumn{4}{c|}{\textbf{OpenWebText (Eval Length=)}} & \multicolumn{4}{c|}{\textbf{PG-19 (Eval Length=)}} & \textbf{OpenOrca} \\ \cmidrule(l){5-13} 
 &  &  &  & 128 & 1,024 & 4,096 & 16,384 & 128 & 1,024 & 4,096 & 16,384 & - \\ \midrule
Vanilla A & 128 & 128 & 128 & \textbf{3.2490} & 3.6536 & 3.6761 & 4.8414 & 4.9682 & 5.2139 & 5.1529 & 5.6949 & 6.0084 \\
Sliding Window A & 128 & 1,024 & 128 & 3.3619 & 3.1286 & 3.0766 & 3.0051 & 5.1785 & 4.8164 & 4.7510 & 4.7663 & 7.7471 \\
Vanilla B & 1,024 & 1,024 & 128 & 3.3395 & 3.3042 & 3.2856 & 3.2379 & 5.6052 & 5.0742 & 5.0797 & 5.1336 & 7.9706 \\
Vanilla B & 1,024 & 1,024 & 1,024 & 3.3395 & \textbf{2.9716} & \textbf{2.9541} & 2.9636 & 5.6052 & 5.3429 & 5.1517 & 5.0274 & 7.9706 \\
Vanilla B & 1,024 & 1,024 & 16,384 & 3.3395 & \textbf{2.9716} & 3.5534 & 3.0786 & \textbf{3.3395} & \textbf{2.9716} & 5.4912 & 5.2372 & 7.9706 \\
Sliding Window B & 1,024 & 4,096 & 1,024 & 3.4380 & 3.0197 & 2.9638 & \textbf{2.9128} & 5.0880 & 4.6587 & 4.5107 & \textbf{4.4383} & \textbf{5.8802} \\
Vanilla C & 4,096 & 4,096 & 4,096 & 3.3788 & 2.9784 & 2.9705 & 2.9518 & 5.1519 & 4.5444 & \textbf{4.4366} & 4.4938 & 5.9315 \\
Vanilla D (Upper Bond) & 16,384 & 16,384 & 16,384 & \multicolumn{4}{c|}{OOM} & \multicolumn{4}{c|}{OOM} & OOM \\ \bottomrule
\end{tabular}
}
\end{table*}


\begin{table*}[t]
\caption{Performance comparison of language models with different activation functions and position embeddings.}
\label{tab:table3}  % 设置标签
\resizebox{\textwidth}{!}{
\begin{tabular}{@{}l|c|ccccc|cccc@{}}
\toprule
\textbf{No.} &
  \textbf{\begin{tabular}[c]{@{}c@{}}Model \\ Type\end{tabular}} &
  \textbf{\begin{tabular}[c]{@{}c@{}}Activation\\ Function\end{tabular}} &
  \textbf{\begin{tabular}[c]{@{}c@{}}Position\\ Embedding\end{tabular}} &
  \textbf{\begin{tabular}[c]{@{}c@{}}Training\\ Window\end{tabular}} &
  \textbf{\begin{tabular}[c]{@{}c@{}}Training \\ Length\end{tabular}} &
  \textbf{\begin{tabular}[c]{@{}c@{}}Eval\\ Window\end{tabular}} &
  \textbf{OpenWebText} &
  \textbf{PG-19} &
  \textbf{OpenOrca} &
  \textbf{Avg.} \\ \midrule
1  & Vanilla & Softmax & RoPE        & 128  & 128  & 128  & 4.8414          & 5.6949          & 6.0085          & 5.5149          \\
2  & Vanilla & Sigmoid & RoPE        & 128  & 128  & 128  & 14.2562         & 15.4765         & 1.9906          & 10.5744         \\
3  & Sliding & Softmax & RoPE        & 128  & 1,024 & 128  & 3.0140          & 4.7839          & 6.9671          & 4.9217          \\
4  & Sliding & Sigmoid & ALiBi-12:0  & 128  & 1,024 & 128  & 3.0073          & 4.6895          & 0.1631          & 2.6200          \\
5  & Sliding & Sigmoid & ALiBi-8:4   & 128  & 1,024 & 128  & 3.0391          & 4.6435          & 0.2650          & 2.6492          \\
6  & Sliding & Sigmoid & ALiBi-6:6   & 128  & 1,024 & 128  & 3.0484          & 4.9920          & \textbf{0.1420} & 2.7275          \\
7  & Sliding & Sigmoid & ALiBi-6:6   & 128  & 2,048 & 128  & 3.0634          & 5.0384          & 0.1712          & 2.7577          \\
8  & Sliding & Sigmoid & AliRope-6:6 & 128  & 1,024 & 128  & 3.0486          & \textbf{4.3103} & 0.1709          & \textbf{2.5099} \\
9  & Sliding & Sigmoid & AliRope-6:6 & 1,024 & 1,024 & 1,024 & 2.9716          & 4.3915          & 0.5304          & 2.6312          \\
10 & Vanilla & Softmax & RoPE        & 1,024 & 1,024 & 1,024 & \textbf{2.9631} & 4.5447          & 5.4702          & 4.3260          \\
11 & Vanilla & Sigmoid & ALiBi       & 1,024 & 1,024 & 1,024 & 2.9659          & 5.0681          & 0.1717          & 2.7352          \\ \bottomrule
\end{tabular}
}
\end{table*}

\subsection{Overall Performance}



% In the overall experiment, we compare models on eight common-sense reasoning tasks in Table~\ref{tab:overall}, including Wikitext~\cite{wikitext}, Lambada~\cite{lambada}, PIQA~\cite{PIQA}, Hellaswag~\cite{Hellaswag}, WinoGrande~\cite{WinoGrande}, ARC-easy \& ARC-challenge (ARC-e \& ARC-c)~\cite{arc}, SIQA~\cite{siqa} and BoolQ~\cite{boolq}. 

In this section, we evaluate the performance of SWAT on eight commonsense reasoning benchmarks, as detailed in Appendix~\ref{app:benchmarks}. The comparison is conducted on 340M and 760M parameter models. 
For our SWAT, (-) denotes negative slopes (i.e., the negative ALiBi slope to look forward in Equation~\ref{eq:-+}); (+) denotes positive slopes, which use the opposite slope of ALiBi (i.e., the positive slope in Equation~\ref{eq:-+} looking backward); and (-+) indicates that half of the attention heads have negative slopes and half have positive slopes. 
% For our SWAT, as defined in \eqref{eq:-+}, 
% (-) denotes the configuration using only negative slopes (i.e., traditional ALiBi slopes $s_k = -2^{-k}$)
% (+) denotes the configuration using only positive slopes (i.e., $s_k = 2^{-k}$)
% (-+) denotes our bidirectional configuration where:
% Half of the attention heads ($h/2$ heads) use negative slopes $s_k = -2^{-k}$
% The other half use positive slopes $s_k = 2^{-k}$
% For both directions, $k$ ranges from 1 to $h/2$


As shown in Table~\ref{tab:overall}, SWAT (-) achieves state-of-the-art  (SOTA) performance on average (46.88\%) across eight common sense reasoning tasks, surpassing all other baselines. This is mainly attributed to the short-text benchmarks, such as PIQA and Hellaswag, where SWAT (-) focuses more on the information from newly input tokens.
Although SWAT (-) initially shows higher perplexity than other baselines at 340M parameters, when scaled to 760M parameters, it demonstrates strong decreases in perplexity on Wiki and LMB. This suggests a performance improvement trend for larger models with the sigmoid function.
On the contrary, the purely forward-looking SWAT (+) shows weaker performance, suggesting that forward slopes work best combined with backward attention. 

The balanced configuration SWAT (-+), where attention heads are evenly split between looking forward and backward, achieves more uniform performance across different tasks by effectively processing both recent and historical information. Specifically, SWAT (-+) achieves the best performance (62.11\%) on BoolQ, a question-answering dataset where historical context is crucial for accurate predictions. This result aligns with our findings in Section~\ref{ssec:ablation}, where balanced attention heads demonstrate superior performance on both OpenOrca and PG-19 datasets, confirming the importance of balanced historical information processing for complex reasoning tasks. Meanwhile, due to the allocation of some attention heads for remembering information from older tokens, SWAT (-+) shows a slight performance compromise on shorter benchmarks. However, this issue is alleviated as the model scales from 340M to 760M.
The results remain consistent at 760M parameters, showing robustness across model sizes.
% SWAT (-+) strengths on longer contexts, specifically on BoolQ passages requiring comprehension, achieve 62.11\% accuracy with the 340M model, showing enhanced long-context reasoning with its balanced bidirectional slopes. 


% As shown in Table~\ref{tab:overall}, SWAT (-) achieves state-of-the-art (SOTA) performance on avgerage(46.88\% across eight common-sense reasoning tasks) for the 340M parameter model, surpassing all other models across evaluated tasks. This is because the benchmark samples have relatively shorter text lengths, which align well with normal ALiBi slopes. 
% Although SWAT (-) initially shows higher perplexity on Wiki and LMB compared to some baselines at 340M parameters, when scaling to 760M parameters, it demonstrates strong improvement in both metrics (21.05 on LMB perplexity, matching the best performance). This suggests that the sliding window attention training becomes increasingly effective at larger model scales. 
% SWAT (-+) shows particular strengths in tasks involving longer contexts - notably on BoolQ, which primarily consists of longer passages requiring comprehensive reading comprehension, where it achieves 62.11\% accuracy with the 340M parameter model, demonstrating its enhanced long-context reasoning capabilities through balanced bidirectional slopes. 
% Meanwhile, SWAT (+) with purely positive slopes shows competitive but generally lower performance compared to the other variants, suggesting that while forward-looking attention can be beneficial, it works best when combined with traditional backward-looking attention mechanisms. 
% When scaling to 760M parameters, the results remain consistent, reinforcing the effectiveness of the approach across different model sizes.

% However, when scaling to 760M parameters, SWAT's performance slightly declines. This is because the larger model can fully memorize the 4096-length training text, reducing its reliance on information transmission. This change weakens its ability to retain long-context information, diminishing the effectiveness of the sliding window mechanism.

% Among both 340M and 760M models, SWAT demonstrates strong performance across evaluated tasks. SWAT (-) excels in tasks requiring immediate context understanding, while SWAT (+) performs well in tasks like BoolQ and SIQA, where historical context is crucial. The balanced configuration SWAT (-+) achieves uniform performance across tasks by processing both recent and long-term information. 

% For notation, we use ``-" to denote conventional ALiBi with negative slopes where attention heads focus on earlier tokens, ``+" for our reversed ALiBi with positive slopes that attend to newer tokens, and ``-+" indicates balanced slopes where half the heads use negative slopes and half use positive slopes.

% Among 340M models, SWAT achieves the best overall performance across the evaluated tasks with an average score of 46.88.Different slope configurations of SWAT show distinct characteristics. SWAT (-) with conventional ALiBi negative slopes demonstrates strong overall performance, particularly excelling in tasks requiring immediate context understanding. SWAT (+), which focuses more on historical information through positive slopes, shows advantages in tasks where background context plays a crucial role, such as BoolQ and SIQA. The balanced configuration SWAT (-+), where attention heads are evenly split between looking forward and backward, achieves more uniform performance across different tasks by effectively processing both recent and historical information. 

% Specifically, SWAT (-+) achieves the best performance (62.11\%) on BoolQ, a question-answering dataset where historical context is crucial for accurate predictions. This result aligns with our findings in Section~\ref{ssec:ablation}, where balanced attention heads demonstrate superior performance on both OpenOrca and PG-19 datasets, confirming the importance of balanced historical information processing for complex reasoning tasks.

% Among 760M models, SWAT continues to demonstrate strong performance. SWAT (-) achieves the highest overall accuracy, excelling particularly in tasks that require immediate context comprehension. SWAT (+) performs well in tasks like WinoGrande and BoolQ, where broader contextual reasoning is essential. SWAT (-+) maintains stable and competitive results across various tasks, leveraging both recent and long-term information processing to ensure robust generalization.

% \textbf{Scaling to 1.3B Parameters.}


% Overall, SWAT stands out not only for its impressive performance on common reasoning benchmarks but also for its consistent success across different tasks. Insights from Table~\ref{tab:overall} show that SWAT's innovative attention mechanisms, and for slope modifications, yield measurable benefits in commonsense reasoning tasks. Moreover, its competitive performance against state-of-the-art baselines like Titans and Gated DeltaNet suggests that SWAT is well-suited for real-world applications requiring robust reasoning capabilities. common-sense reasoning capabilities.



\subsection{Sliding Window Attention Training}
\label{ssec:swat}

To verify the effectiveness of SWA training, we conduct experiments comparing vanilla Transformers pre-trained with and without SWAT training across three datasets. Using Llama2-based models~\cite{llama2} pretrained on OpenWebText, we investigate the impact of varying sliding window sizes and sequence lengths, with results shown in Table~\ref{tab:performance_comparison}. In the table, vanilla Transformers are which training length are the same as their training window size, and the labels A, B, C, and D represent the model identifiers. 

When the sliding window mechanism is applied, we observe a notable improvement in performance, particularly with longer evaluation sequence lengths. For instance, in the Sliding Window A configuration, when the evaluation length is 16,384, Sliding Window A achieves a performance of 3.0051 on OpenWebText, surpassing the 4.8414 achieved by Vanilla A. Additionally, Sliding Window B achieves the best performance across all three datasets when the evaluation length is 16,384. Note that all results are from models trained for 80,000 steps. If training continues, the attention sink issue is likely to worsen, further degrading vanilla model performance.

Based on our experimental results, we draw two key conclusions: 
% 1) Vanilla transformer models (Vanilla A, B, and C) trained with different sequence lengths demonstrate optimal performance primarily on sequences matching their training length. Their performance degrades notably when processing sequences longer than their training length, indicating a clear length-dependent behavior.
% 2) While SWA pretrained models show slightly higher loss than vanilla transformers trained on specific lengths, they exhibit more stable performance across varying input lengths. The model performance improves and stabilizes as text length reaches the sliding window's coverage range, suggesting better generalization to longer sequences without input length constraints. 
% 3) Despite the benefits of sliding window training, the inherent limitations of the transformer architecture still lead to information loss. This is evidenced by consistently high loss values on the OpenOrca dataset, which likely stems from information loss caused by the softmax operation.
(1) Wtih the same model structure, SWA training significantly improves performance, especially with longer evaluation sequence lengths. This is likely because SWA training forces the model to retain memory of older information across long sequences, while vanilla models struggle with memory as they retain all historical tokens.
(2) The vanilla Transformers perform optimally only when the evaluation length matches the training length, whereas the SWA trained models maintain consistent performance across varying sequence lengths. This is likely because vanilla Transformers heavily attend to initial tokens due to attention sink, while SWA models learn to focus primarily on the current window, ensuring stable performance across different sequence lengths.




\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{imgs/figure5.pdf}
    \caption{The training loss of models with different modules including Sigmoid, RoPE, and ALiBi, with the balanced slopes.}
    \label{fig:loss}
\end{figure}



\subsection{Ablation Study}
\label{ssec:ablation}



This section evaluates the impact of activation functions, position embeddings, and ALiBi slopes.
We systematically test 11 different configurations (No.1-11) to understand how different combinations of model components affect long-context performance, as shown in Table~\ref{tab:table3} and Figure~\ref{fig:loss}.


Comparing No.1 and No.2, directly replacing softmax with sigmoid in vanilla Transformer leads to significant performance degradation, likely due to overloaded information in token embeddings without mutual suppression. However, using ALiBi stabilizes training by distinguishing subtle differences in token embeddings based on position information (No.10 and No.11). Furthermore, the slope configuration plays a key role, with No.5 and No.6 outperforming No.4, suggesting a better balance between recent and past information. However, Figure~\ref{fig:loss} shows that training instability persists at later stages (ALiBi-6:6 Sigmoid), indicating that ALiBi alone provides weak positional information. AliRope-6:6 Sigmoid (No.8) achieves the lowest loss values among all variants, with 2.51 on average, while demonstrating more stable training pattern as shown in Figure~\ref{fig:loss}. Finally, comparing No.7 and No.6, extending the training length from 1,024 to 2,048 while keeping the number of layers and window size fixed does not help with the loss.

% Table~\ref{tab:table3} presents results from pretraining models with varying configurations, while Table~\ref{fig:loss} visualizes validation curves for several representative models.



% \paragraph{Activation Functions.}
% Models using the sigmoid activation function perform worse. For instance, Vanilla (Sigmoid+RoPE) shows higher loss values across all tasks compared to Vanilla (Softmax+RoPE), particularly on the OpenWebText and PG-19 datasets, where the losses are 14.2562 and 15.4765, respectively, versus 4.8414 and 5.6949 for Vanilla (Softmax+RoPE). This indicates that sigmoid function causes information overload in hidden states, making it difficult to extract key features for the next token predictions.



% Models using the sigmoid activation function perform worse. For instance, Vanilla (Sigmoid+RoPE) shows higher loss values across all tasks compared to Vanilla (Softmax+RoPE), particularly on the OpenWebText and PG-19 datasets. This indicates that the sigmoid function causes information overload in hidden states, making it difficult to extract key features for the next token predictions.

% \paragraph{Position Embeddings.} 

% Using ALiBi stabilizes training by introducing position-dependent biases that help differentiate token embeddings. The slope configuration plays a key role, with No.5 and No.6 outperforming No.4, suggesting a better balance between local and global dependencies. 
% However, training instability persists at later stages, indicating that ALiBi alone provides weak positional information. No.8 reduces these fluctuations, leading to more stable training and improved performance with a loss of 4.3103 on PG-19.

% When using ALiBi, the training process becomes stable, possibly because ALiBi introduces position-dependent biases that help differentiate token embeddings at different positions, enabling more complex representations to complement the sigmoid function. Meanwhile, the slope configuration (e.g., ALiBi-12:0, ALiBi-8:4, ALiBi-6:6) significantly influences model performance. Both ALiBi-6:6 and ALiBi-8:4 configurations perform better than ALiBi-12:0, which suggests that a balanced attention head configuration—where half of the heads focus forward and half backward—provides a better trade-off between local and global dependencies, enabling the model to memorize more information from long-context sequences.

% Although the combination of the sigmoid function and the balanced ALiBi already achieves promising results, we observe training instability even at the late training stages(as shown in Figure~\ref{fig:loss}). This suggests that ALiBi alone provides relatively weak positional information. After incorporating RoPE into our model (Sigmoid+AliRope-6:6), these performance fluctuations are significantly reduced, leading to more stable training and better performance with a loss of 4.3103 on PG-19.




% Moreover, the performance of Sigmoid+AliRope-6:6 configuration stands out, achieving a loss of 4.3103 on PG-19, the best across all models. This demonstrates that adjusting the slope configuration to allow flexible capturing of dependencies at different ranges improves model effectiveness.

% RoPE (Rotary Position Embedding)~\cite{rope} is a position encoding method that typically improves model performance. For example, Vanilla (Softmax+RoPE) and Sliding (Softmax+RoPE) demonstrate lower loss values, especially on the OpenWebText and PG-19 datasets, suggesting that RoPE helps the model better capture positional relationships in long-range dependencies.
% ALiBi (Attention with Linear Biases)~\cite{alibi} directly introduces position biases into the attention matrix. When used in Sliding (Sigmoid+ALiBi), particularly with the ALiBi-6:6 configuration, the model shows a significant reduction in loss, especially on OpenOrca, where the loss drops to 0.1420, outperforming other configurations. This indicates that ALiBi effectively captures positional information in long text sequences when properly configured.


% In summary, models without sliding windows struggle with long texts, particularly in tasks involving multi-paragraph or cross-sentence reasoning. In contrast, sliding window techniques ensure stable performance. The best results are obtained when half of the attention heads look forward and half look backward, effectively balancing local and global dependencies. These findings underscore that a balanced activation function, appropriate ALiBi configuration, and an effective sliding window strategy are key to improving Transformer models’ performance in long text processing.

% \subsection{Attention Score Visualization}
% % 这段现在可能不能实现了，因为
% % 随便写的，随便删
% The objective of this experiment is to analyze the interpretability of the attention mechanism in models trained with a sliding window approach. The setup involves comparing the attention scores with the corresponding input text. Theoretical results suggest that in models trained with sliding windows, the attention sink shifts from the initial tokens to key tokens located in the middle of the text, such as newline characters ('\\n').




