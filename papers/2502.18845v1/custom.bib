% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}


@book{ abelson-et-al:scheme,
  author = "Harold Abelson and Gerald~Jay Sussman and Julie Sussman",
  title = "Structure and Interpretation of Computer Programs",
  publisher = "MIT Press",
  address = "Cambridge, Massachusetts",
  year = "1985"
}

@inproceedings{ bgf:Lixto,
  author = "Robert Baumgartner and Georg Gottlob and Sergio Flesca",
  title = "Visual Information Extraction with {Lixto}",
  booktitle = "Proceedings of the 27th International Conference on Very Large Databases",
  pages = "119--128",
  publisher = "Morgan Kaufmann",
  address = "Rome, Italy",
  month = "September",
  year = "2001"
}

@article{ brachman-schmolze:kl-one,
  author = "Ronald~J. Brachman and James~G. Schmolze",
  title = "An overview of the {KL-ONE} knowledge representation system",
  journal = "Cognitive Science",
  volume = "9",
  number = "2",
  pages = "171--216",
  month = "April--June",
  year = "1985"
}

@article{ gottlob:nonmon,
  author = "Georg Gottlob",
  title = "Complexity results for nonmonotonic logics",
  journal = "Journal of Logic and Computation",
  volume = "2",
  number = "3",
  pages = "397--425",
  month = "June",
  year = "1992"
}

@article{ gls:hypertrees,
  author = "Georg Gottlob and Nicola Leone and Francesco Scarcello",
  title = "Hypertree Decompositions and Tractable Queries",
  journal = "Journal of Computer and System Sciences",
  volume = "64",
  number = "3",
  pages = "579--627",
  month = "May",
  year = "2002"
}

@article{ levesque:functional-foundations,
  author = "Hector~J. Levesque",
  title = "Foundations of a functional approach to knowledge representation",
  journal = "Artificial Intelligence",
  volume = "23",
  number = "2",
  pages = "155--212",
  month = "July",
  year = "1984"
}

@inproceedings{ levesque:belief,
  author = "Hector~J. Levesque",
  title = "A logic of implicit and explicit belief",
  booktitle = "Proceedings of the Fourth National Conference on Artificial Intelligence",
  publisher = "American Association for Artificial Intelligence",
  pages = "198--202",
  address = "Austin, Texas",
  month = "August",
  year = "1984"
}

@article{ nebel:jair-2000,
  author = "Bernhard Nebel",
  title = "On the compilability and expressive power of propositional planning formalisms",
  journal = "Journal of Artificial Intelligence Research",
  volume = "12",
  pages = "271--315",
  year = "2000"
}

 @misc{proceedings,
  author = {{IJCAI Proceedings}},
  title = {{IJCAI} Camera Ready Submission},
  howpublished = {\url{https://proceedings.ijcai.org/info}},
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@misc{openwebtext,
    title={OpenWebText Corpus},
    author={Gokaslan, Aaron and Cohen, Vanya and Pavlick, Ellie and the others},
    howpublished={\url{http://Skylion007.github.io/OpenWebTextCorpus}},
    year={2019}
}

@misc{OpenOrca,
  title = {OpenOrca: An Open Dataset of GPT Augmented FLAN Reasoning Traces},
  author = {Wing Lian and Bleys Goodson and Eugene Pentland and the others},
  year = {2023},
  publisher = {HuggingFace},
  journal = {HuggingFace repository},
  howpublished = {\url{https://huggingface.co/Open-Orca/OpenOrca}},
}

@article{pg19,
  title={Compressive transformers for long-range sequence modelling},
  author={Rae, Jack W and Potapenko, Anna and Jayakumar, Siddhant M and Lillicrap, Timothy P},
  journal={arXiv preprint arXiv:1911.05507},
  year={2019}
}

@article{sparsetransformer,
  title={Generating long sequences with sparse transformers},
  author={Child, Rewon and Gray, Scott and Radford, Alec and others},
  journal={arXiv preprint arXiv:1904.10509},
  year={2019}
}

@article{Longformer,
  title={Longformer: The long-document transformer},
  author={Beltagy, Iz and Peters, Matthew E and Cohan, Arman and the others},
  journal={arXiv preprint arXiv:2004.05150},
  year={2020}
}

@article{streamingllm,
  title={Efficient streaming language models with attention sinks},
  author={Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and the others},
  journal={arXiv preprint arXiv:2309.17453},
  year={2023}
}

@article{llama2,
  author       = {Hugo Touvron and Louis Martin and Kevin Stone and the others},
  title        = {Llama 2: Open Foundation and Fine-Tuned Chat Models},
  journal      = {CoRR},
  volume       = {abs/2307.09288},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2307.09288},
  doi          = {10.48550/ARXIV.2307.09288},
  eprinttype    = {arXiv},
  eprint       = {2307.09288},
  timestamp    = {Mon, 28 Aug 2023 21:26:22 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2307-09288.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{llama3,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and the others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@misc{mistral-7b,
      title={Mistral 7B}, 
      author={Albert Q. Jiang and Alexandre Sablayrolles and Arthur Mensch and the others},
      year={2023},
      eprint={2310.06825},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.06825}, 
}

@misc{qwen2,
      title={Qwen2 Technical Report}, 
      author={An Yang and Baosong Yang and Binyuan Hui and the others},
      year={2024},
      eprint={2407.10671},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.10671}, 
}

@inproceedings{variance,
    title = "Latent Positional Information is in the Self-Attention Variance of Transformer Language Models Without Positional Embeddings",
    author = "Chi, Ta-Chung and Fan, Ting-Han and Chen, Li-Wei and the others",
    editor = "Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-short.102/",
    doi = "10.18653/v1/2023.acl-short.102",
    pages = "1183--1193"
}

@misc{rope,
      title={RoFormer: Enhanced Transformer with Rotary Position Embedding}, 
      author={Jianlin Su and Yu Lu and Shengfeng Pan and the others},
      year={2023},
      eprint={2104.09864},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2104.09864}, 
}


@misc{titans,
      title={Titans: Learning to Memorize at Test Time}, 
      author={Ali Behrouz and Peilin Zhong and Vahab Mirrokni},
      year={2024},
      eprint={2501.00663},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2501.00663}, 
}


@misc{gateddeltanet,
      title={Gated Delta Networks: Improving Mamba2 with Delta Rule}, 
      author={Songlin Yang and Jan Kautz and Ali Hatamizadeh},
      year={2024},
      eprint={2412.06464},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.06464}, 
}

@misc{retnet,
      title={Retentive Network: A Successor to Transformer for Large Language Models}, 
      author={Yutao Sun and Li Dong and Shaohan Huang and others},
      year={2023},
      eprint={2307.08621},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2307.08621}, 
}

@misc{gla,
      title={Gated Linear Attention Transformers with Hardware-Efficient Training}, 
      author={Songlin Yang and Bailin Wang and Yikang Shen and others},
      year={2024},
      eprint={2312.06635},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2312.06635}, 
}

@article{mamba,
  title={Mamba: Linear-time sequence modeling with selective state spaces},
  author={Gu, Albert and Dao, Tri},
  journal={arXiv preprint arXiv:2312.00752},
  year={2023}
}

@misc{deltanet,
      title={Parallelizing Linear Transformers with the Delta Rule over Sequence Length}, 
      author={Songlin Yang and Bailin Wang and Yu Zhang and others},
      year={2025},
      eprint={2406.06484},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.06484}, 
}

@misc{ttt,
      title={Learning to (Learn at Test Time): RNNs with Expressive Hidden States}, 
      author={Yu Sun and Xinhao Li and Karan Dalal and others},
      year={2024},
      eprint={2407.04620},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.04620}, 
}


@misc{fineweb-edu,
    author       = { Lozhkov, Anton and Ben Allal, Loubna and von Werra, Leandro and others},  
    title        = {FineWeb-Edu: the Finest Collection of Educational Content}, 
    year         = 2024,  
    url          = { https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu },  
    doi          = { 10.57967/hf/2497 },
    publisher    = { Hugging Face }
}

@article{gpt2,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@inproceedings{lineartransformer,
  author       = {Angelos Katharopoulos and
                  Apoorv Vyas and
                  Nikolaos Pappas and
                  others},
  title        = {Transformers are RNNs: Fast Autoregressive Transformers with Linear
                  Attention},
  booktitle    = {Proceedings of the 37th International Conference on Machine Learning,
                  {ICML} 2020, 13-18 July 2020, Virtual Event},
  series       = {Proceedings of Machine Learning Research},
  volume       = {119},
  pages        = {5156--5165},
  publisher    = {{PMLR}},
  year         = {2020},
  url          = {http://proceedings.mlr.press/v119/katharopoulos20a.html},
  timestamp    = {Tue, 15 Dec 2020 17:40:19 +0100},
  biburl       = {https://dblp.org/rec/conf/icml/KatharopoulosV020.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{performers,
  author       = {Krzysztof Marcin Choromanski and
                  Valerii Likhosherstov and
                  David Dohan and
                  others},
  title        = {Rethinking Attention with Performers},
  booktitle    = {9th International Conference on Learning Representations, {ICLR} 2021,
                  Virtual Event, Austria, May 3-7, 2021},
  publisher    = {OpenReview.net},
  year         = {2021},
  url          = {https://openreview.net/forum?id=Ua6zuk0WRH},
  timestamp    = {Wed, 23 Jun 2021 17:36:39 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/ChoromanskiLDSG21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{rwkv,
  title={Rwkv: Reinventing rnns for the transformer era},
  author={Peng, Bo and Alcaide, Eric and Anthony, Quentin and others},
  journal={arXiv preprint arXiv:2305.13048},
  year={2023}
}

@inproceedings{transformer-xl,
    title = "Transformer-{XL}: Attentive Language Models beyond a Fixed-Length Context",
    author = "Dai, Zihang  and
      Yang, Zhilin  and
      Yang, Yiming  and
      others",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1285/",
    doi = "10.18653/v1/P19-1285",
    pages = "2978--2988",
    abstract = "Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80{\%} longer than RNNs and 450{\%} longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch."
}

@misc{compressivetransformers,
      title={Compressive Transformers for Long-Range Sequence Modelling}, 
      author={Jack W. Rae and Anna Potapenko and Siddhant M. Jayakumar and others},
      year={2019},
      eprint={1911.05507},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1911.05507}, 
}
@misc{memorizingtransformers,
      title={Memorizing Transformers}, 
      author={Yuhuai Wu and Markus N. Rabe and DeLesley Hutchins and others},
      year={2022},
      eprint={2203.08913},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2203.08913}, 
}

@misc{focusedtransformer,
      title={Focused Transformer: Contrastive Training for Context Scaling}, 
      author={Szymon Tworkowski and Konrad Staniszewski and Mikołaj Pacek and others},
      year={2023},
      eprint={2307.03170},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2307.03170}, 
}

@article{longnet,
  title={Longnet: Scaling transformers to 1,000,000,000 tokens},
  author={Ding, Jiayu and Ma, Shuming and Dong, Li and others},
  journal={arXiv preprint arXiv:2307.02486},
  year={2023}
}

@misc{bigbird,
      title={Big Bird: Transformers for Longer Sequences}, 
      author={Manzil Zaheer and Guru Guruganesh and Avinava Dubey and others},
      year={2021},
      eprint={2007.14062},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2007.14062}, 
}

@inproceedings{lm-infinite,
  author       = {Chi Han and
                  Qifan Wang and
                  Hao Peng and
                  others},
  editor       = {Kevin Duh and
                  Helena G{\'{o}}mez{-}Adorno and
                  Steven Bethard},
  title        = {LM-Infinite: Zero-Shot Extreme Length Generalization for Large Language
                  Models},
  booktitle    = {Proceedings of the 2024 Conference of the North American Chapter of
                  the Association for Computational Linguistics: Human Language Technologies
                  (Volume 1: Long Papers), {NAACL} 2024, Mexico City, Mexico, June 16-21,
                  2024},
  pages        = {3991--4008},
  publisher    = {Association for Computational Linguistics},
  year         = {2024},
  url          = {https://doi.org/10.18653/v1/2024.naacl-long.222},
  doi          = {10.18653/V1/2024.NAACL-LONG.222},
  timestamp    = {Fri, 30 Aug 2024 16:38:11 +0200},
  biburl       = {https://dblp.org/rec/conf/naacl/HanWPX0JW24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{whensinkemerge,
  title={When Attention Sink Emerges in Language Models: An Empirical View},
  author={Gu, Xiangming and Pang, Tianyu and Du, Chao and others},
  journal={arXiv preprint arXiv:2410.10781},
  year={2024}
}

@misc{alibi,
      title={Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation}, 
      author={Ofir Press and Noah A. Smith and Mike Lewis},
      year={2022},
      eprint={2108.12409},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2108.12409}, 
}

@article{sigmoid,
  title={Notice sur la loi que la population suit dans son accroissement},
  author={Verhulst, Pierre-Fran{\c{c}}ois},
  journal={Correspondence mathematique et physique},
  volume={10},
  pages={113--129},
  year={1838}
}

@inproceedings{wikitext,
  author       = {Stephen Merity and
                  Caiming Xiong and
                  James Bradbury and
                  others},
  title        = {Pointer Sentinel Mixture Models},
  booktitle    = {5th International Conference on Learning Representations},
  year         = {2017},
  url          = {https://openreview.net/forum?id=Byj72udxe},
  timestamp    = {Thu, 25 Jul 2019 14:25:57 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/MerityX0S17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{lambada,
    title = "The {LAMBADA} dataset: Word prediction requiring a broad discourse context",
    author = "Paperno, Denis  and
      Kruszewski, Germ{\'a}n  and
      Lazaridou, Angeliki  and
      others",
    editor = "Erk, Katrin  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    url = "https://aclanthology.org/P16-1144/",
    doi = "10.18653/v1/P16-1144",
    pages = "1525--1534"
}

@inproceedings{PIQA,
  author       = {Yonatan Bisk and
                  Rowan Zellers and
                  Ronan Le Bras and
                  others},
  title        = {{PIQA:} Reasoning about Physical Commonsense in Natural Language},
  booktitle    = {The Thirty-Fourth {AAAI} Conference on Artificial Intelligence},
  pages        = {7432--7439},
  year         = {2020},
  url          = {https://doi.org/10.1609/aaai.v34i05.6239},
  doi          = {10.1609/AAAI.V34I05.6239},
  timestamp    = {Thu, 11 Apr 2024 13:33:56 +0200},
  biburl       = {https://dblp.org/rec/conf/aaai/BiskZLGC20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{Hellaswag,
    title = "{H}ella{S}wag: Can a Machine Really Finish Your Sentence?",
    author = "Zellers, Rowan  and
      Holtzman, Ari  and
      Bisk, Yonatan  and
      others",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    url = "https://aclanthology.org/P19-1472/",
    doi = "10.18653/v1/P19-1472",
    pages = "4791--4800"
}

@article{WinoGrande,
    author = {Sakaguchi, Keisuke and Bras, Ronan Le and Bhagavatula, Chandra and others},
    title = {WinoGrande: an adversarial winograd schema challenge at scale},
    year = {2021},
    issue_date = {September 2021},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {64},
    number = {9},
    issn = {0001-0782},
    url = {https://doi.org/10.1145/3474381},
    doi = {10.1145/3474381},
    journal = {Commun. ACM},
    month = aug,
    pages = {99–106},
    numpages = {8}
}

@misc{arc,
      title={Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge}, 
      author={Peter Clark and Isaac Cowhey and Oren Etzioni and others},
      year={2018},
      eprint={1803.05457},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/1803.05457}, 
}

@inproceedings{siqa,
    title = "Social {IQ}a: Commonsense Reasoning about Social Interactions",
    author = "Sap, Maarten  and
      Rashkin, Hannah  and
      Chen, Derek  and
      others",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2019",
    url = "https://aclanthology.org/D19-1454/",
    doi = "10.18653/v1/D19-1454",
    pages = "4463--4473"
}


@inproceedings{boolq,
  author       = {Christopher Clark and
                  Kenton Lee and
                  Ming{-}Wei Chang and
                  others},
  editor       = {Jill Burstein and
                  Christy Doran and
                  Thamar Solorio},
  title        = {BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions},
  booktitle    = {Proceedings of the 2019 Conference of the North American Chapter of
                  the Association for Computational Linguistics: Human Language Technologies,
                  {NAACL-HLT} 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long
                  and Short Papers)},
  pages        = {2924--2936},
  publisher    = {Association for Computational Linguistics},
  year         = {2019},
  url          = {https://doi.org/10.18653/v1/n19-1300},
  doi          = {10.18653/V1/N19-1300},
  timestamp    = {Tue, 16 Aug 2022 23:04:27 +0200},
  biburl       = {https://dblp.org/rec/conf/naacl/ClarkLCK0T19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{RMSNorm,
  author       = {Biao Zhang and
                  Rico Sennrich},
  editor       = {Hanna M. Wallach and
                  Hugo Larochelle and
                  Alina Beygelzimer and
                  Florence d'Alch{\'{e}}{-}Buc and
                  Emily B. Fox and
                  Roman Garnett},
  title        = {Root Mean Square Layer Normalization},
  booktitle    = {Advances in Neural Information Processing Systems 32: Annual Conference
                  on Neural Information Processing Systems 2019},
  pages        = {12360--12371},
  year         = {2019},
  url          = {https://proceedings.neurips.cc/paper/2019/hash/1e8a19426224ca89e83cef47f1e7f53b-Abstract.html},
  timestamp    = {Fri, 21 Oct 2022 14:36:34 +0200},
  biburl       = {https://dblp.org/rec/conf/nips/ZhangS19a.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{intro,
  author       = {Zhihong Shao and
                  Peiyi Wang and
                  Qihao Zhu and
                  Runxin Xu and
                  others},
  title        = {DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open
                  Language Models},
  journal      = {CoRR},
  volume       = {abs/2402.03300},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2402.03300},
  doi          = {10.48550/ARXIV.2402.03300},
  eprinttype    = {arXiv},
  eprint       = {2402.03300},
  timestamp    = {Mon, 12 Feb 2024 13:36:38 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2402-03300.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{sigmoidatt,
      title={Theory, Analysis, and Best Practices for Sigmoid Self-Attention}, 
      author={Jason Ramapuram and Federico Danieli and Eeshan Dhekane and Floris Weers and Dan Busbridge and Pierre Ablin and Tatiana Likhomanenko and Jagrit Digani and Zijin Gu and Amitis Shidani and Russ Webb},
      year={2025},
      eprint={2409.04431},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2409.04431}, 
}

