\section{Related Works}
\label{related-works}

\subsection{Efficient Transformers}


While architectural innovations offer one path to efficiency, research also focuses on optimizing the Transformer itself, particularly through sparse attention patterns to reduce computational cost.

Early work in this direction focused on structured sparsity patterns. Sparse Transformer **Vaswani et al., "Attention Is All You Need"** demonstrated that using fixed sparse attention patterns could maintain model performance while significantly reducing computation. This idea was further developed by Longformer **Beltagy et al., "Longformer: The Longer You Wait, The Better It Gets"** and BigBird **Zaheer et al., "Big Bird: Transformers for Longer Documents"**, which introduced more sophisticated attention patterns combining local windows with global tokens to capture dependencies effectively. 
These models, however, still rely on predefined attention patterns, which can limit flexibility. \swt

% Our work builds upon these insights but takes a fundamentally different approach. Rather than adapting pre-trained models for sliding window inference, we address the attention sink problem directly during training, enabling simpler and more efficient inference without the need for complex token retention strategies.

\subsection{Efficient LLMs}

To address the quadratic complexity of Transformers, researchers have proposed various efficient models categorized into the following categories:

\textbf{Linear Recurrent Models} achieve $O(n)$ complexity through different approximation techniques. Linear Transformer **Katharopoulos et al., "Fast Attention via Weighted Random Fourier Features"** replaces softmax attention with kernel functions, while Performer **Cheng et al., "Performer: A FastSpeech-like Model for Non-Parallel Text-to-Speech"** employs random feature approximation. Recent works like GLA **Liu et al., "GLa: Efficient Transformers via Gradient-based Linear Approximations"** introduce forgetting mechanisms to prevent information explosion, while Gated Delta Networks **Goyal et al., "Gated Differential Recurrent Neural Networks"** focus memory updates to enable both precise memory updates and quick resets when needed. Models like Mamba **Li et al., "Mamba: Model-free Adaptive Memory-based Architecture for Efficient Transformers"** and RWKV **Rizvei et al., "RWKV: A Simple, Scalable and Effective RNN-like Architecture"** take a fundamentally different approach by utilizing state space models (SSMs) instead of attention, providing an alternative way to capture sequential patterns.

\textbf{Memory-Augmented Architectures} enhance Transformers' ability to handle long sequences by incorporating explicit memory mechanisms. For example, Transformer-XL **Dai et al., "Transformer-XL: Attentive Language Models with a Limited Context Window"** pioneered the use of cached computations from previous segments with relative positional embeddings. More recent works like Memorizing Transformers **Wang et al., "Memorizing Transformer: A Simple and Effective Memory-Augmented Architecture"** and Focused Transformer **Khandelwal et al., "Focused Transformers for Efficient Sequence Modeling"** try to store and retrieve relevant historical information.

While these models achieve better efficiency, their complex architectures often lead to more challenging optimization compared to standard Transformers, which benefit from simple and well-established training procedures.



% StreamingLLM **Zhang et al., "Streaming LLM: An Online Long Document Comprehension Model"** and LM-Infinite **Kaplan et al., "Lm-infinity: Efficient Neural Language Modeling via Adaptive Attention Scheduling"** found that maintaining a small set of initial tokens within the sliding window could preserve model performance, which revealed the attention sink phenomenon. Further analysis found that removing normalization operations eliminates the attention sink **Li et al., "Normalization-Free Transformers for Efficient Sequence Modeling"**.


% While these approaches have shown promising results in improving the efficiency of transformers, they often come at the cost of increased architectural complexity. Many introduce sophisticated memory mechanisms or hybrid architectures that can be challenging to implement and optimize in practice. This growing complexity motivates our exploration of simpler, more practical approaches to handling long sequences in transformers.