\section{Conclusion}
\label{sec:conclusion}

This paper introduces SWAT, a new architecture for efficient LLMs via sliding window attention training, which maintains the core Transformer architecture. By replacing softmax with sigmoid and combining balanced ALiBi with RoPE, SWAT addresses the attention sink issue and ensures stable training. SWAT enables effective information compression and retention across sliding windows without complex architectural changes. Experimental results show that SWAT outperforms other models across eight common-sense reasoning benchmarks, excelling in tasks that require long-range comprehension. Future work could explore adaptive window sizes for more flexible text processing.


\section{Limitations}

While our architectural design ensures relatively robust training stability, SWAT's performance exhibits significant sensitivity to hyperparameter configuration. Critical parameters including window size, model depth, and the distribution of ALiBi slopes substantially impact model efficacy. This necessitates comprehensive hyperparameter exploration to optimize the model architecture.

Additionally, as the model scales, it may encounter diminishing returns in retaining long-context information. In particular, larger models may fully memorize training data, reducing the need for information transmission, which in turn weakens the effectiveness of mechanisms designed to handle extended contexts. Future experiments will need to keep cache from previous steps during training to address this problem.

Finally, despite SWAT's strong overall performance, the model exhibits an inherent limitation in its attention mechanism. Specifically, SWAT's maximum attention distance is constrained by the product of window size and model depth. Although extending these parameters can theoretically increase the attention span, information loss remains inevitable when processing ultra-long sequences. For applications requiring complete information retention over extensive contexts, alternative approaches such as hybrid architectures or explicit memory retrieval mechanisms may be necessary to complement SWAT's capabilities.







