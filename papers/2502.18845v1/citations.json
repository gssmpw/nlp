[
  {
    "index": 0,
    "papers": [
      {
        "key": "sparsetransformer",
        "author": "Child, Rewon and Gray, Scott and Radford, Alec and others",
        "title": "Generating long sequences with sparse transformers"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "Longformer",
        "author": "Beltagy, Iz and Peters, Matthew E and Cohan, Arman and the others",
        "title": "Longformer: The long-document transformer"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "bigbird",
        "author": "Manzil Zaheer and Guru Guruganesh and Avinava Dubey and others",
        "title": "Big Bird: Transformers for Longer Sequences"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "lineartransformer",
        "author": "Angelos Katharopoulos and\nApoorv Vyas and\nNikolaos Pappas and\nothers",
        "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear\nAttention"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "performers",
        "author": "Krzysztof Marcin Choromanski and\nValerii Likhosherstov and\nDavid Dohan and\nothers",
        "title": "Rethinking Attention with Performers"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "gla",
        "author": "Songlin Yang and Bailin Wang and Yikang Shen and others",
        "title": "Gated Linear Attention Transformers with Hardware-Efficient Training"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "gateddeltanet",
        "author": "Songlin Yang and Jan Kautz and Ali Hatamizadeh",
        "title": "Gated Delta Networks: Improving Mamba2 with Delta Rule"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "mamba",
        "author": "Gu, Albert and Dao, Tri",
        "title": "Mamba: Linear-time sequence modeling with selective state spaces"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "rwkv",
        "author": "Peng, Bo and Alcaide, Eric and Anthony, Quentin and others",
        "title": "Rwkv: Reinventing rnns for the transformer era"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "transformer-xl",
        "author": "Dai, Zihang  and\nYang, Zhilin  and\nYang, Yiming  and\nothers",
        "title": "Transformer-{XL}: Attentive Language Models beyond a Fixed-Length Context"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "memorizingtransformers",
        "author": "Yuhuai Wu and Markus N. Rabe and DeLesley Hutchins and others",
        "title": "Memorizing Transformers"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "focusedtransformer",
        "author": "Szymon Tworkowski and Konrad Staniszewski and Miko\u0142aj Pacek and others",
        "title": "Focused Transformer: Contrastive Training for Context Scaling"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "streamingllm",
        "author": "Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and the others",
        "title": "Efficient streaming language models with attention sinks"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "lm-infinite",
        "author": "Chi Han and\nQifan Wang and\nHao Peng and\nothers",
        "title": "LM-Infinite: Zero-Shot Extreme Length Generalization for Large Language\nModels"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "whensinkemerge",
        "author": "Gu, Xiangming and Pang, Tianyu and Du, Chao and others",
        "title": "When Attention Sink Emerges in Language Models: An Empirical View"
      }
    ]
  }
]