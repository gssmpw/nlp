\section{Related Works}
\label{related-works}

\subsection{Efficient Transformers}


While architectural innovations offer one path to efficiency, research also focuses on optimizing the Transformer itself, particularly through sparse attention patterns to reduce computational cost.

Early work in this direction focused on structured sparsity patterns. Sparse Transformer~\cite{sparsetransformer} demonstrated that using fixed sparse attention patterns could maintain model performance while significantly reducing computation. This idea was further developed by Longformer~\cite{Longformer} and BigBird~\cite{bigbird}, which introduced more sophisticated attention patterns combining local windows with global tokens to capture dependencies effectively. 
These models, however, still rely on predefined attention patterns, which can limit flexibility. \swt
% Our work builds upon these insights but takes a fundamentally different approach. Rather than adapting pre-trained models for sliding window inference, we address the attention sink problem directly during training, enabling simpler and more efficient inference without the need for complex token retention strategies.

\subsection{Efficient LLMs}

To address the quadratic complexity of Transformers, researchers have proposed various efficient models categorized into the following categories:

\textbf{Linear Recurrent Models} achieve $O(n)$ complexity through different approximation techniques. Linear Transformer~\cite{lineartransformer} replaces softmax attention with kernel functions, while Performer~\cite{performers} employs random feature approximation. Recent works like GLA~\cite{gla} introduce forgetting mechanisms to prevent information explosion, while Gated Delta Networks~\cite{gateddeltanet} focus memory updates to enable both precise memory updates and quick resets when needed. Models like Mamba~\cite{mamba} and RWKV~\cite{rwkv} take a fundamentally different approach by utilizing state space models (SSMs) instead of attention, providing an alternative way to capture sequential patterns.

\textbf{Memory-Augmented Architectures} enhance Transformers' ability to handle long sequences by incorporating explicit memory mechanisms. For example, Transformer-XL~\cite{transformer-xl} pioneered the use of cached computations from previous segments with relative positional embeddings. More recent works like Memorizing Transformers~\cite{memorizingtransformers} and Focused Transformer~\cite{focusedtransformer} try to store and retrieve relevant historical information.

While these models achieve better efficiency, their complex architectures often lead to more challenging optimization compared to standard Transformers, which benefit from simple and well-established training procedures.



% StreamingLLM~\cite{streamingllm} and LM-Infinite~\cite{lm-infinite} found that maintaining a small set of initial tokens within the sliding window could preserve model performance, which revealed the attention sink phenomenon. Further analysis found that removing normalization operations eliminates the attention sink~\cite{whensinkemerge}.


% While these approaches have shown promising results in improving the efficiency of transformers, they often come at the cost of increased architectural complexity. Many introduce sophisticated memory mechanisms or hybrid architectures that can be challenging to implement and optimize in practice. This growing complexity motivates our exploration of simpler, more practical approaches to handling long sequences in transformers.

