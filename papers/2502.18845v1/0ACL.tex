% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{enumerate}
\usepackage{lipsum}
\usepackage{enumitem}
\usepackage{multirow}
\usepackage{pifont}
\usepackage{hyperref}

\newcommand{\zxy}[1]{{\color{blue} [#1 – Xy]}}
\newcommand{\zc}[0]{{\color{blue} [cite]}\xspace}
 % for Prof. Zhao
\newcommand{\w}[1]{{\color{orange}[#1]}} % Yejing
\newcommand{\fzc}[1]{{\color{red} #1}} % Zichuan
\newcommand{\swt}[1]{{\color{green} #1}} % Wentao


% \usepackage{xcolor}
% \pagecolor[rgb]{0,0,0} %black
% \color[rgb]{0.7,0.7,0.7} %grey

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Sliding Window Attention Training for Efficient Large Language Models}
% Long-Context Handling

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}

% \author{
%  \textbf{Zichuan Fu\textsuperscript{1}},
%  \textbf{Wentao Song\textsuperscript{2}},
%  \textbf{Yejing Wang\textsuperscript{1}},
%  \textbf{Xian Wu\textsuperscript{3,†}},
% \\
%  \textbf{Yefeng Zheng\textsuperscript{3,4}},
%  \textbf{Yingying Zhang\textsuperscript{5}},
%  \textbf{Derong Xu\textsuperscript{1,6}},
%  \textbf{Xuetao Wei\textsuperscript{7}},
% \\
%  \textbf{Tong Xu\textsuperscript{6,†}},
%  \textbf{Xiangyu Zhao\textsuperscript{1,†}},
%  \textbf{Ziheng Zhang\textsuperscript{3}},
%  \textbf{Zhihong Zhu\textsuperscript{8}},
% \\
%  \textbf{Zhenxi Lin\textsuperscript{3}},
%  \textbf{Qidong Liu\textsuperscript{1,2}},
%  \textbf{Wanyu Wang\textsuperscript{1}},
%  \textbf{Yuyang Ye\textsuperscript{1}},
%  \textbf{Enhong Chen\textsuperscript{6}}
% \\
%  \textsuperscript{1}City University of Hong Kong, Hong Kong, China,
%  \textsuperscript{2}Xi'an Jiaotong University, Xi'an, China,
% \\
%  \textsuperscript{3}Jarvis Research Center, Tencent YouTu Lab, Shenzhen, China,
% \\
%  \textsuperscript{4}Medical Artificial Intelligence Lab, Westlake University, Shenzhen, China,
% \\
%  \textsuperscript{5}Tencent, Shenzhen, China,
%  \textsuperscript{6}University of Science and Technology of China, Hefei, China,
% \\
%  \textsuperscript{7}Southern University of Science and Technology, Shenzhen, China,
%  \textsuperscript{8}Peking University, Beijing, China
% \\
%  \small{
%    \textbf{Correspondence:} \href{mailto:xy.zhao@cityu.edu.hk}{xy.zhao@cityu.edu.hk}
%  }
% }

% \author{
%  Zichuan Fu\textsuperscript{1}, Wentao Song\textsuperscript{2}, Yejing Wang\textsuperscript{1}, Xian Wu\textsuperscript{3,†},
%  Yefeng Zheng\textsuperscript{3,4}, Yingying Zhang\textsuperscript{5}, 
%  Derong Xu\textsuperscript{1,6}, Xuetao Wei\textsuperscript{7},
%  Tong Xu\textsuperscript{6,†}, Xiangyu Zhao\textsuperscript{1,†}, Ziheng Zhang\textsuperscript{3}, Zhihong Zhu\textsuperscript{8},
%  Zhenxi Lin\textsuperscript{3}, Qidong Liu\textsuperscript{1,2}, Wanyu Wang\textsuperscript{1}, Yuyang Ye\textsuperscript{1},
%  Enhong Chen\textsuperscript{6}
% \\
% \\
%  \textsuperscript{1}City University of Hong Kong, Hong Kong, China, \textsuperscript{2}Xi'an Jiaotong University, Xi'an, China,
%  \textsuperscript{3}Jarvis Research Center, Tencent YouTu Lab, Shenzhen, China, \textsuperscript{4}Medical Artificial Intelligence Lab, Westlake University, Shenzhen, China,
%  \textsuperscript{5}Tencent, Shenzhen, China, \textsuperscript{6}University of Science and Technology of China, Hefei, China,
%  \textsuperscript{7}Southern University of Science and Technology, Shenzhen, China, \textsuperscript{8}Peking University, Beijing, China
% \\
%  Correspondence: xy.zhao@cityu.edu.hk
% }

\author{
  \textbf{Zichuan Fu\textsuperscript{1,\thanks{Work was conducted during the internship of Zichuan Fu at Tencent YouTu Lab.}}},
  \textbf{Wentao Song\textsuperscript{2}},
  \textbf{Yejing Wang\textsuperscript{1}},
  \textbf{Xian Wu\textsuperscript{3}},
  \textbf{Yefeng Zheng\textsuperscript{3,4}},\\
  \textbf{Yingying Zhang\textsuperscript{3}},
  \textbf{Derong Xu\textsuperscript{1,5}},
  \textbf{Xuetao Wei\textsuperscript{6}},
  \textbf{Tong Xu\textsuperscript{5}},
  \textbf{Xiangyu Zhao\textsuperscript{1,\thanks{Corresponding author.}}},
\\
\\
  \textsuperscript{1} City University of Hong Kong
  \textsuperscript{2} Xi'an Jiaotong University \\
  \textsuperscript{3} Jarvis Research Center, Tencent YouTu Lab 
  \textsuperscript{4} Westlake University \\
  \textsuperscript{5} University of Science and Technology of China \\
  \textsuperscript{6} Southern University of Science and Technology
\\
  \small{
   \href{mailto:zc.fu@my.cityu.edu.hk}{zc.fu@my.cityu.edu.hk},
   \href{mailto:xy.zhao@cityu.edu.hk}{xy.zhao@cityu.edu.hk}
  }
}



%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}


\maketitle
\begin{abstract}
Recent advances in transformer-based Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks. However, their quadratic computational complexity concerning sequence length remains a significant bottleneck for processing long documents. As a result, many efforts like sparse attention and state space models have been proposed to improve the efficiency of LLMs over long sequences. 
While these approaches achieve efficiency, they often require complex architectures and parallel training techniques.
This calls for a simple yet efficient model that preserves the fundamental Transformer architecture. 
To this end, we introduce \textbf{SWAT}, which enables efficient long-context handling via \textbf{S}liding \textbf{W}indow \textbf{A}ttention \textbf{T}raining. 
Specifically, SWAT replaces softmax with the sigmoid function for efficient information compression and retention. Then it utilizes balanced ALiBi and Rotary Position Embedding to stabilize training process. 
During inference, SWAT maintains linear computational complexity through sliding window attention while preserving model performance, achieving state-of-the-art (SOTA) results on eight commonsense reasoning benchmarks compared to mainstream linear recurrent architectures.
Code is available at \href{https://anonymous.4open.science/r/SWAT-attention}{this link}.

% and utilize a balanced ALiBi and Rotary Position Embedding
% This paper first attributes the inefficiency of Transformers to the attention sink phenomenon resulting from the high variance of softmax operation. Then, we replace softmax with the sigmoid function and utilize a balanced ALiBi and Rotary Position Embedding for efficient information compression and retention. 
    %This motivates us to design a simple yet efficient model that preserves the fundamental transformer architecture.
    %This paper introduces \textbf{SWAT}, which enables efficient long-context handling via \textbf{S}liding \textbf{W}indow \textbf{A}ttention \textbf{T}raining. 
    % Through experimental analysis, we reveal that Transformers suffer from attention sink phenomenon due to the variance brought by softmax. 
   
    % To address these issues, we replace softmax with the sigmoid function and utilize a balanced ALiBi and Rotary Position Embedding for efficient information compression and retention.
    
    % Our approach enables efficient long-context processing without requiring complex architectural changes or inference-time adjustments, providing a practical solution for developing more efficient LLMs. 

\end{abstract}

\input{1Introduction}

\input{2Method}

\input{3Experiments}

\input{4RelatedWorks}

\input{5Conclusion}

\input{6Acknowledgments}

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{custom}

\input{7Appendix}

\end{document}
