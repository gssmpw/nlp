\section{Introduction}
\label{introduction}



Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks, from text generation to complex reasoning~\cite{intro}. 
Unlike humans, who can efficiently process long contexts with memory, LLMs struggle to handle them due to quadratic complexity~\cite{Longformer}.
Despite their impressive performance on standard NLP tasks, this quadratic complexity poses a fundamental challenge for practical applications. The increasing need for efficient long-context processing, coupled with the computational constraints of current architectures, creates a pressing need for more scalable solutions.

Several approaches have been proposed to handle long sequences efficiently. These methods can be broadly categorized into two types: (1) sparse attention mechanisms~\cite{Longformer}, which reduce computation by selectively calculating the attention score, and (2) sequence models with recurrent architectures, such as linear attention variants~\cite{lineartransformer} and state space models~\cite{mamba}, which aim to process sequences efficiently through recursive hidden states.
However, these solutions face a fundamental dilemma---they either compromise model performance to achieve efficiency or propose new complex architectures that cannot fully exploit existing techniques for convenient implementation and deployment.
However, existing LLM solutions for handling long sequences often require complex architectures and parallel training techniques, making implementation and deployment more challenging, which calls for an efficient approach based on the existing Transformer architecture.

\begin{figure*}[ht]
    \hfill
    \includegraphics[width=0.95\linewidth]{imgs/figure1.pdf}
    \caption{The demonstration of the SWA mechanism in Transformers.}
    \label{fig:swa}
\end{figure*}

% Existing SWA inference methods~\cite{streamingllm,lm-infinite} have shown that vanilla Transformer can perform SWA inference within a fixed sliding window size, maintaining perplexity with minimal adjustments. 

Sliding Window Attention (SWA), a typical sparse attention approach~\cite{sparsetransformer}, is the most intuitive solution, as it avoids adding additional model components and compresses the inference computational complexity to linear. 
However, this approach still faces the following challenges\footnote{More details are in Section~\ref{ssec:why}}: 
(1) Current researches on SWA predominantly focus on solving the attention sink problem within the inference phase, where models allocate excessive attention to initial tokens, causing an uneven distribution of attention weights across the sequence~\cite{streamingllm}. However, they leave the training process unchanged, thereby creating a gap between inference and training.
(2) Tokens outside the attention window coverage are ignored for prediction, leading to information loss in long-context modeling~\cite{lm-infinite,sigmoidatt}.
Hence, it is crucial to investigate SWA training methods to bridge the training-inference gap and enable the model to learn long-context dependencies.

This paper introduces the SWAT framework to achieve effective SWA training and solve the aforementioned problems. Specifically, SWAT replaces the softmax operation with the sigmoid function, which not only prevents the attention sink problem but also maintains dense attention weights for higher information capacity per token.
To compensate for the lack of sparsity in sigmoid-based attention, SWAT incorporates balanced ALiBi~\cite{alibi} to introduce position-dependent differentiation, preventing information overloaded in dense representations. It also enables the model to preserve both recent and historical information effectively.
Furthermore, we enhance the framework with Rotary Position Embedding (RoPE)~\cite{rope} to explicitly encode positional information in hidden states, ensuring training stability.
SWAT trained with SWA from scratch is ultimately capable of compressing arbitrarily long texts into a fixed-length hidden state of tokens while maintaining effective information processing.
Our contributions can be summarized as follows:
\begin{itemize}[leftmargin=*, itemsep=0pt]
    \item We empirically analyze the poor performance of the SWA inference and attribute this to the attention sink problem caused by the high variance of softmax operation.
    \item We introduce SWAT, which combines sigmoid activation with balanced position embeddings, enabling effective information preservation and achieving SWA training.
    \item Extensive experiments confirm that SWAT surpasses vanilla Transformer and other recurrent models, achieving strong performance across tasks with linear computational complexity.
\end{itemize}


