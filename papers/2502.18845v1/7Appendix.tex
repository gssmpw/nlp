\appendix

\section{Why Does the Softmax Function Lead to Sparsity?}
\label{app:sparsity}

In models such as Transformers, dot-product attention is the most widely used approach. Let a query vector $\boldsymbol{q}$ and multiple key vectors $\boldsymbol{k}_1, \boldsymbol{k}_2, \ldots, \boldsymbol{k}_L$ be given, where $\boldsymbol{q}, \boldsymbol{k}_i \in \mathbb{R}^d$. We stack the key vectors into a matrix:
\begin{equation}
\boldsymbol{K} \;=\;
\begin{bmatrix}
\boldsymbol{k}_1 \\
\boldsymbol{k}_2 \\
\vdots \\
\boldsymbol{k}_L
\end{bmatrix}.
\end{equation}
The attention distribution (i.e., the set of attention weights) $\boldsymbol{\alpha}$ is computed by:
\begin{equation}
\boldsymbol{\alpha} = \text{softmax} \left (\tfrac{\boldsymbol{q} \boldsymbol{K}^\top}{\sqrt{d}} \right ),
\end{equation}
where $\text{softmax}(z_i) = e^{z_i} / \sum_j e^{z_j}$. Let
\begin{equation}
E_i = \frac{\boldsymbol{q}\cdot \boldsymbol{k}_i}{\sqrt{d}},
\end{equation}
so the $i$-th attention weight is:
\begin{equation}
\alpha_i = \frac{\exp(E_i)}{\sum_{j=1}^n \exp(E_j)}.
\end{equation}

Sparsity arises because the exponential function greatly amplifies any $E_i$ that is larger than the rest: if $E_1$ is significantly bigger than $E_2, \dots, E_L$, then $\exp(E_1)$ will dominate the sum in the denominator, pushing $\alpha_1$ close to $1$ and making the others near $0$. Formally, define
\begin{equation}
\Delta_i = E_1 - E_i \quad \text{for } i \ge 2,
\end{equation}
so we have:
\begin{equation}
\begin{aligned}
\frac{\alpha_i}{\alpha_1} &= \frac{\exp(E_i)}{\exp(E_1)} \\
                          &= \exp (E_i - E_1 ) \\
                          &= \exp (-\Delta_i ).
\end{aligned}
\end{equation}
If $\Delta_i$ is large and positive, then $\exp(-\Delta_i)$ is very small, causing $\alpha_i$ to vanish compared to $\alpha_1$. Moreover, in high-dimensional spaces (i.e., when $d$ is large), random dot products $\boldsymbol{q} \cdot \boldsymbol{k}_i$ tend to have higher variance, making it more likely that one or a few $E_i$ values will stand out dramatically. This ``winner-takes-most'' scenario becomes amplified, thereby increasing the tendency toward sparsity within the attention distribution.

In practice, the dot-product $\boldsymbol{q} \cdot \boldsymbol{k}_i$ often yields extreme values—meaning that one or a few of the resulting energies $E_i$ are substantially larger than the others. This phenomenon causes the softmax to concentrate most of the probability mass on these extreme values. To rigorously analyze this behavior, we suppose each attention score $E_i$ is an independent and identically distributed (i.i.d.) random variable drawn from a Gaussian distribution:
\begin{equation}
E_i \sim \mathcal{N}(\mu, \sigma^2).
\end{equation}

Under this assumption, by the central limit theorem, the dot product $\boldsymbol{q}\cdot \boldsymbol{k}_i$ follows an approximately normal distribution after appropriate scaling. More importantly, extreme value theory states that the maximum value among $L$ i.i.d. Gaussian variables, denoted as $E_{(L)} = \max_{1 \le i \le L} E_i$, satisfies approximately: 
\begin{equation}
E_{(L)} \approx \mu + \sigma \sqrt{2 \ln L}.
\end{equation}
In contrast, a typical attention score is around $\mu$. Therefore, the expected gap between the maximum energy and a typical energy is on the order of: 
\begin{equation}
\Delta \approx \sigma \sqrt{2 \ln L}.
\end{equation}
Given this gap, we have:
\begin{equation}
\frac{\alpha_i}{\alpha_1} \approx \exp\Bigl(-\sigma \sqrt{2 \ln L}\Bigr). 
\end{equation}
For large $L$, this ratio becomes exponentially small.



\section{Why Does the Sigmoid Function Maintain Density?}
\label{app:density}

While the softmax function induces a probability distribution over multiple inputs, the sigmoid function operates on each input independently and does not normalize across multiple values. Concretely, the sigmoid of a scalar $z$ is defined as:
\begin{equation}
\sigma(z) \;=\; \frac{1}{1 + e^{-z}}.
\end{equation}

In contrast to softmax—which computes exponential terms for all inputs $z_1, z_2, \dots, z_L$ and divides by their sum—sigmoid only involves a single exponential term $e^{-z}$ within its own calculation. Consequently, one input's value does not directly compete with another input's value in a shared denominator. Since the final attention weight for each token is determined independently based on its relationship with the query, there is no ``winner-takes-most'' effect as seen in softmax-based attention.

Finally, in a sigmoid-based attention mechanism, the computed token embedding can retain information from all tokens within the attention window, rather than being dominated by a single token with high attention weight. To effectively preserve the diversity of token integration, it is important to ensure that the embedding dimension is sufficiently large. A higher dimensional space allows different token values to be effectively combined while maintaining meaningful distinctions between them.


\section{Detailed Experiment Settings}
\label{app:experiment-settings}

\subsection{Datasets}
While our main experiments utilize a specific high-quality educational dataset, we conducted preliminary evaluations across multiple datasets to comprehensively assess model capabilities. All datasets are split according to the ratio: train:validation:test = 8:1:1. Here we detail the characteristics and purposes of each dataset. 

Our overall experiment employs a 100 billion token subset of \textbf{FineWeb-Edu}~\cite{fineweb-edu}, which is specifically curated for language model pre-training. This dataset consists of high-quality educational content that provides well-structured training examples for developing fundamental language understanding capabilities.

\begin{table}[t]
\centering
\caption{Statistics of the datasets used in our analysis experiments. All datasets are in English and split into train, validation, and test sets with a ratio of 8:1:1. Sample sizes are reported in millions (M) or thousands (K).}
\label{tab:dataset}
\resizebox{\linewidth}{!}{
\begin{tabular}{@{}lclcccc@{}}
\toprule
\textbf{Name} & \textbf{Task} & \textbf{Usage} & \textbf{Language} & \textbf{Train} & \textbf{Validation} & \textbf{Test} \\ \midrule
OpenWebText & Language Modeling & All & English & 6.48M & 0.81M & 0.81M \\
PG-19 & Language Modeling & Test & English & 15.6M & 1.95M & 1.95M \\
OpenOrca & Question Answering & Test & English & 400K & 50K & 50K \\ \bottomrule
\end{tabular}
}
\end{table}


For our subsequent experiments, as shown in Table~\ref{tab:dataset}, we deliberately selected three complementary datasets that evaluate different aspects of model performance:

\textbf{OpenWebText}~\cite{openwebtext} comprises predominantly shorter web-based texts. It provides a foundation for assessing basic language modeling capabilities. In contrast to specialized corpora, OpenWebText's diverse content allows evaluation of general language understanding across varied domains and writing styles.

\textbf{PG-19}~\cite{pg19} is based on complete books published before 1919, presenting a distinct challenge in processing long-form literary content. The book-length texts require models to maintain coherence and compress information across extended narratives, testing their ability to capture long-range dependencies and thematic consistency. 

\textbf{OpenOrca}~\cite{OpenOrca} is a question-answering dataset that tests models' information retention capabilities. This is particularly important as the answers to questions are often embedded in earlier parts of the context, making it an effective benchmark for assessing models' ability to maintain essential information when processing long sequences.

We utilized OpenWebText for traininga and validation, while incorporating all three datasets into the test phase.
To thoroughly evaluate long-context processing capabilities, we extended the input sequence length to 16,384 tokens for both OpenWebText and PG-19. This multi-dataset evaluation framework allows us to systematically analyze model performance across different linguistic challenges and context lengths, providing a comprehensive view of their capabilities and limitations.


\subsection{Benchmarks}
\label{app:benchmarks}

For our overall experiment, we compare models on eight common-sense reasoning tasks, in Table~\ref{tab:bench}:

\textbf{Wikitext}~\cite{wikitext}: A large linguistic corpus extracted from Wikipedia articles, containing over 100 million word tokens. It tests a model's ability to predict the next word in a passage of text.

\textbf{Lambada}~\cite{lambada}: The LAmBdA dataset tests a model's capability of using broad discourse context to predict the last word of a passage extracted from books. It contains over 60,000 examples.

\textbf{PIQA}~\cite{PIQA}: The Physical Interaction: Question Answering (PIQA) dataset tests commonsense reasoning about physical interactions between two entities. It contains 16,113 multiple choice questions generated from crowd-sourcing.

\textbf{Hellaswag}~\cite{Hellaswag}: The HellaSwag dataset consists of 70,000 multiple choice questions about inferring what might happen next in a story. It requires commonsense reasoning to choose the most plausible ending.

\textbf{WinoGrande}~\cite{WinoGrande}: The WinoGrande dataset tests coreference resolution and commonsense reasoning with 44,000 examples obtained from books and websites. 

\textbf{ARC}~\cite{arc}: The AI2 Reasoning Challenge (ARC) dataset contains 7,787 genuine grade-school level, multiple-choice science questions, grouped into an Easy Set (ARC-e) and a Challenge Set (ARC-c).

\textbf{SIQA}~\cite{siqa}: The Social Interaction QA (SIQA) dataset contains 15,554 multiple choice questions that describe situations about people's social interactions. 

\textbf{BoolQ}~\cite{boolq}: The Boolean Questions (BoolQ) dataset contains 15,942 English yes/no questions sampled from Google search queries to test a model's ability to answer simple questions.


\begin{table}[t]
\centering
\caption{The statistics of the benchmarks used in the overall experiment.}
\label{tab:bench}  % 设置标签
\resizebox{0.8\linewidth}{!}{
\begin{tabular}{@{}l|c@{}}
\toprule
\textbf{Dataset} & \textbf{Sample Size} \\ \midrule
Wikitext & 60,634 \\
Lambada & 60,000 \\
PIQA & 16,113 \\
Hellaswag & 70,000 \\
WinoGrande & 44,000 \\
ARC & 7,787 (Easy Set + Challenge Set) \\
SIQA & 15,554 \\
BoolQ & 15,942 \\ \bottomrule
\end{tabular}
}
\end{table}


\subsection{Implementation Details.}

\paragraph{Overall Experiment}

In the overall experiment (Table~\ref{tab:overall}), SWAT means we pretrain the model with our sliding window attention training. 
We pre-train SWAT with model sizes of 340M and 760M parameters on 15B and 30B tokens, respectively. The SWAT models are compared to other language models of similar sizes. All pre-training experiments were conducted on 8 NVIDIA A800 GPUs (80GB), with the 760M model taking approximately 31 hours to complete the pre-training process.


Evaluations measure perplexity (lower is better) and accuracy (higher is better) on datasets like PIQA, WinoGrande, and BoolQ. 
For our SWAT, as defined in Equation~\eqref{eq:-+}, 
(-) denotes the configuration using only negative slopes (i.e., traditional ALiBi slopes $s_k = -2^{-k}$),
(+) denotes the configuration using only positive slopes (i.e., $s_k = 2^{-k}$),
(-+) denotes our bidirectional configuration where:
Half of the attention heads ($h/2$ heads) use negative slopes $s_k = -2^{-k}$, the other half use positive slopes $s_k = 2^{-k}$.
For both directions, $k$ ranges from 1 to $h/2$.
The experiments are based on two GitHub repositories flash-linear-attention\footnote{\href{https://github.com/Fzkuji/flash-linear-attention}{https://github.com/Fzkuji/flash-linear-attention}} and lm-evaluation-harness\footnote{\href{https://github.com/EleutherAI/lm-evaluation-harness}{https://github.com/EleutherAI/lm-evaluation-harness}}.

\paragraph{Analysis Experiments}

For analysis experiments, models are evaluated on three datasets: OpenWebText, PG-19, and OpenOrca, with the average accuracy reported. We experiment with different training window sizes, training lengths, and evaluation window sizes.  The experiments are based on two GitHub repositories nanoGPT\footnote{\href{https://github.com/karpathy/nanoGPT}{https://github.com/karpathy/nanoGPT}} and flash-linear-attention. We pre-train SWAT (248M parameters) for 80,000 steps with a batch size of 250k tokens, accumulating a total training exposure of 20B tokens, which amounts to  about 2 epochs over the pre-training corpus.

In Table \ref{tab:performance_comparison}, vanilla Transformers have a training length that matches their fixed training window size. Model A, B, C, and D are identifiers for pre-trained models with different configurations being compared. The columns in the table show different sequence length settings for each model configuration. The parameters used in the table are defined as follows::
\begin{itemize}[leftmargin=*, itemsep=0pt]
    \item Training window size means the maximum sequence length the model can process per training step. 
    \item Training length means the actual sequence length used for each training example, which may be shorter than the window size when using the vanilla Transformers. 
    \item Evaluation window means the maximum context provided to the model during evaluation to make predictions. 
    \item Evaluation length means the actual sequence length fed into the model per test example.
\end{itemize}

We compared pre-training using fixed token window sizes of 128, 1,024, and 4,096 versus using variable-length sliding windows. 
With sliding window pre-training, the model is exposed to longer token sequences during training, which helps improve evaluation perplexity. 
Using sliding windows allows longer sequences during training compared to fixed windows. This table shows that the best performance was achieved when the training sequence length is four times the training window size. Different evaluation window sizes are also tested to compare model performance given varying amounts of context.

In Table \ref{tab:table3}, we compared the performance of language models with different activation functions and position embeddings. Specifically, we study the model accuracy when using softmax and sigmoid as the activation functions. We also introduce RoPE, ALiBi, and AliRope as different position embedding methods. Note that ALiBi-12:0 represents the origin ALiBi model, which uses only negative slopes, while ALiBi-6:6 represents model uses half positive and half negative slopes across different attention heads.

% Table \ref{tab:table3} compares different activation functions and position embeddings. Sigmoid models outperform softmax, and ALiBi or AliRope position embeddings are better than learned positional embeddings. The top-performing model uses sigmoid activations and AliRope position embeddings.











% \begin{table*}[ht]
% \resizebox{\textwidth}{!}{
% \begin{tabular}{@{}ccccc@{}}
% \toprule
% Architectures &
%   \begin{tabular}[c]{@{}c@{}}Training\\ Parallelization\end{tabular} &
%   Inference Cost &
%   \begin{tabular}[c]{@{}c@{}}Long-Sequence\\ Memory Complexity\end{tabular} &
%   Performance \\ \midrule
% Transformer        & \ding{51}    & $O(N)$ & $O\left(N^2\right)$ & \ding{51}\ding{51}\ding{51} \\
% Linear Transformer & \ding{51}    & $O(1)$ & $O(N)$              & \ding{55}               \\
% Recurrent NN       & \ding{55}    & $O(1)$ & $O(N)$              & \ding{55}               \\
% RWKV               & \ding{55}    & $O(1)$ & $O(N)$              & \ding{51}           \\
% H3/S4              & \ding{51}    & $O(1)$ & $O(N \log N)$       & \ding{51}           \\
% Hyena              & \ding{51}    & $O(N)$ & $O(N \log N)$       & \ding{51}           \\
% Mamba2             & \ding{51}    & $O(1)$ & $O(N)$              & \ding{51}           \\
% DeltaNet           & \ding{51}    & $O(1)$ & $O(N)$              & \ding{51}\ding{51}           \\
% Titans             & \ding{51}    & $O(1)$ & $O(N)$              & \ding{51}\ding{51}           \\
% SWAT               & \ding{51}    & $O(1)$ & $O(N)$              & \ding{51}\ding{51}\ding{51} \\ \bottomrule
% \end{tabular}
% }
% \end{table*}