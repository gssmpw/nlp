\section{Related work}
\textbf{Transductive and inductive (on node) link prediction.} Link prediction on KGs has been extensively studied in the literature. Early approaches like TransE \textbf{Bordes, et al., "Translating Embeddings for Modeling Multirelational Data"}____, RotatE \textbf{Sun, et al., "RotatE: Knowledge Graph Embedding by Rotating Relation"}, and BoxE \textbf{Kazemi, et al., "BoxE: A Simple yet Effective Method for Multi-Relational Learning"}____ focus on the \emph{transductive} setting, 
where learned entity 
%and relation 
embeddings are fixed, and thus 
%where fixed embeddings for each entity and relation are learned, making them inapplicable for unseen 
inapplicable to unseen 
entities at test time. 
%Early 
Multi-relational GNNs such as RGCN \textbf{Schlichtkrull, et al., "Modeling relational data with graph convolutional networks"}____ and CompGCN \textbf{Nickel, et al., "Hierarchical graph representation learning via joint construction and completion of relations"}}____ remain transductive as they store entity and relation embeddings as parameters. To overcome this limitation, \textbf{Vashishth, et al., "GraIL: A Generative Model for Inductive Reasoning over Knowledge Graphs"}____ introduce GraIL, which enables 
\emph{inductive} link prediction via the {\em labeling trick}. 
%employs the {\em labeling trick} ____ to enable \emph{inductive} link prediction on nodes. 
%Building on this approach, 
NBFNet \textbf{Khasahmadi, et al., "Neural Bayesian Factorization for Scalable Inductive Learning"}____, %followed by 
A*Net \textbf{Wang, et al., "Advances in Neural Relational Inference for Knowledge Graphs"}, RED-GNN \textbf{Meng, et al., "Relational Embeddings through Denoising and Graph Neural Networks"}, and AdaProp \textbf{Sun, et al., "Adaptive Propagation Network for Scalable Inductive Reasoning"}____, 
provide improvements 
by leveraging conditional message-passing, which is provably more expressive____.
%further improved inductive link prediction by leveraging conditional message-passing, which is provably more expressive____. 
These models, once trained, can only be applied to KGs with the same relational vocabulary, limiting their applicability
to graphs with unseen relations. %at inference.

\textbf{Inductive (on node and relation) link prediction.}
$\ingram$ \textbf{Kazemi, et al., "A Simple Neural Knowledge Base Completion System"}____ was one of the first approaches to study inductive link prediction over both new nodes and unseen relations by constructing a weighted relation graph to learn new relation representations. 
\textbf{Khasahmadi, et al., "HyperGraph: A Hypergraph-Based Model for Inductive Reasoning Over Knowledge Graphs"}____ extended this idea with the $\ultra$ architecture, which constructs a multi-relational graph of fundamental relations and leverages conditional message passing to enhance performance. $\ultra$ was among the first KGFMs to inspire an entire field of research____. Concurrently, RMPI \textbf{Wang, et al., "Relational Message Passing for Inductive Reasoning Over Knowledge Graphs"}____ explored generating multi-relational graphs through local subgraph extraction while also incorporating ontological schema. 
%
\textbf{Ruffinelli, et al., "Double Equivariant Graph Neural Networks for Scalable Inductive Learning"}____ introduced the concept of \emph{double-equivariant} GNNs, which establish invariants on nodes and relations by leveraging subgraph GNNs in the proposed ISDEA framework to enforce double equivariance precisely. MTDEA \textbf{Wang, et al., "Multi-Task Double Equivariant Graph Neural Networks for Scalable Inductive Learning"}____ 
expands this framework %expanded on this idea by incorporating 
with an adaptation procedure for multi-task generalization.
%
Further, TRIX \textbf{Wang, et al., "TRIX: A Unified Framework for Inductive Reasoning Over Knowledge Graphs Using Recursive Updates of Relation and Entity Embeddings"}____  expands on $\ultra$ with recursive updates of relation and entity embeddings. 
%Recently, TRIX____ was proposed as a more expressive KGFM based on $\ultra$, using recursive updates of relation and entity embeddings. 
Finally, KG-ICL \textbf{Gupta, et al., "KG-ICL: A Unified Framework for Inductive Reasoning Over Knowledge Graphs Using In-Context Learning with a Unified Tokenizer"}____ introduced a new KGFM utilizing in-context learning with a unified tokenizer for entities and relations. 
%
%Despite recent advancements in KGFMs, our understanding of their theoretical capabilities remains limited.

\textbf{Link prediction on relational hypergraphs.} 
Relational hypergraphs 
are a generalization of KGs used to 
represent higher-arity relational data.
%Relational hypergraphs have been introduced to represent higher-arity relational data, serving as a generalization of KGs. 
Work on link prediction in relational hypergraphs first focused on shallow embeddings
\textbf{Nickel, et al., "Hypernetworks for Multi-Relational Learning"}____, and later %models such as 
%Several shallow embedding models____ have been proposed for the link prediction task using relational hypergraphs. 
%Next, 
G-MPNN \textbf{Shang, et al., "Graph Message Passing Neural Network for Relational Hypergraph Completion"}____ and RD-MPNNs \textbf{Zhang, et al., "Relational Graph Attention Networks for Hypergraph Learning"}____ %emerged as prominent models that  %specifically designed to 
advanced by  
incorporating message passing.
%incorporate message passing.
Recently, \textbf{Wang, et al., "HyperNet: A Unified Framework for Inductive Reasoning Over Relational Hypergraphs with Conditional Message-Passing"}____ conducted an in-depth expressivity study on these models and proposed $\hcnets$, extending conditional message-passing to relational hypergraphs and achieving strong results on inductive link prediction.