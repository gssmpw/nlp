\section{Related work}
\textbf{Transductive and inductive (on node) link prediction.} Link prediction on KGs has been extensively studied in the literature. Early approaches like TransE~\citep{brodes2013transe}, RotatE~\citep{sun2019rotate}, and BoxE~\citep{abbound2020boxe} focus on the \emph{transductive} setting, 
where learned entity 
%and relation 
embeddings are fixed, and thus 
%where fixed embeddings for each entity and relation are learned, making them inapplicable for unseen 
inapplicable to unseen 
entities at test time. 
%Early 
Multi-relational GNNs such as RGCN~\citep{schlichtkrull2017modeling} and CompGCN~\citep{vashishth2020compositionbased} remain transductive as they store entity and relation embeddings as parameters. To overcome this limitation, \citet{grail2020teru} introduce GraIL, which enables 
\emph{inductive} link prediction via the {\em labeling trick}. 
%employs the {\em labeling trick} \citep{LabelingTrick2021} to enable \emph{inductive} link prediction on nodes. 
%Building on this approach, 
NBFNet \citep{zhu2022neural}, %followed by 
A*Net~\citep{zhu2023anet}, RED-GNN~\citep{zhang2022redgnn}, and AdaProp~\citep{adaprop}, 
provide improvements 
by leveraging conditional message-passing, which is provably more expressive~\citep{huang2023theory}.
%further improved inductive link prediction by leveraging conditional message-passing, which is provably more expressive~\citep{huang2023theory}. 
These models, once trained, can only be applied to KGs with the same relational vocabulary, limiting their applicability
to graphs with unseen relations. %at inference.

\textbf{Inductive (on node and relation) link prediction.}
$\ingram$~\citep{ingram} was one of the first approaches to study inductive link prediction over both new nodes and unseen relations by constructing a weighted relation graph to learn new relation representations. \citet{galkin2023ultra} extended this idea with the $\ultra$ architecture, which constructs a multi-relational graph of fundamental relations and leverages conditional message passing to enhance performance. $\ultra$ was among the first KGFMs to inspire an entire field of research~\citep{mao2024positiongraphfoundationmodels}. Concurrently, RMPI~\citep{geng2022relationalmessagepassingfully} explored generating multi-relational graphs through local subgraph extraction while also incorporating ontological schema. 
%
\citet{gao2023double} introduced the concept of \emph{double-equivariant} GNNs, which establish invariants on nodes and relations by leveraging subgraph GNNs in the proposed ISDEA framework to enforce double equivariance precisely. MTDEA~\citep{zhou2023multitaskperspetivelinkprediction} 
expands this framework %expanded on this idea by incorporating 
with an adaptation procedure for multi-task generalization.
%
Further, TRIX~\citep{zhang2024trix}  expands on $\ultra$ with recursive updates of relation and entity embeddings. 
%Recently, TRIX~\citep{zhang2024trix} was proposed as a more expressive KGFM based on $\ultra$, using recursive updates of relation and entity embeddings. 
Finally, KG-ICL~\citep{cui2024prompt} introduced a new KGFM utilizing in-context learning with a unified tokenizer for entities and relations. 
%
%Despite recent advancements in KGFMs, our understanding of their theoretical capabilities remains limited.

\textbf{Link prediction on relational hypergraphs.} 
Relational hypergraphs 
are a generalization of KGs used to 
represent higher-arity relational data.
%Relational hypergraphs have been introduced to represent higher-arity relational data, serving as a generalization of KGs. 
Work on link prediction in relational hypergraphs first focused on shallow embeddings
~\citep{wen2016representation, liu2020tensor, fatemi2020knowledge}, and later %models such as 
%Several shallow embedding models~\citep{wen2016representation, liu2020tensor, fatemi2020knowledge} have been proposed for the link prediction task using relational hypergraphs. 
%Next, 
G-MPNN~\citep{yadati2020gmpnn} and RD-MPNNs~\citep{zhou2023rdmpnn} %emerged as prominent models that  %specifically designed to 
advanced by  
incorporating message passing.
%incorporate message passing.
Recently, \citet{huang2024link} conducted an in-depth expressivity study on these models and proposed $\hcnets$, extending conditional message-passing to relational hypergraphs and achieving strong results on inductive link prediction.