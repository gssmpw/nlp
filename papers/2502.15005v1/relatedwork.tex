\section{Related Work}
In this section, we review approaches for automatically generating topics in scientific publications. We focus on three main paradigms: supervised topic classification, unsupervised topic modeling, and knowledge organization systems.

\subsection{Topic Classification} Topic classification takes a supervised approach, using often manually curated document topics to train models for categorizing new publications. However, obtaining high-quality ground truth labels at scale remains challenging, forcing many methods to build topic taxonomies from scratch~\cite{liu2014hierarchical}.

The Microsoft Academic Graph (MAG)~\cite{sinha2015overview} represents a significant effort in this direction. 
Their methodology began with manual definition of a small number of top-level concepts, followed by extraction of 28,000 academic concepts from Wikipedia articles~\cite{shen2018web}. MAG implemented topic assignment as a multilabel classification problem, enabling papers to be tagged with multiple concepts.
After the discontinuation of MAG in 2021, OpenAlex attempted to reproduce the topic classification model using historical MAG data as training labels~\cite{priem2022openalex}. During this process, OpenAlex identified several limitations of MAG concepts, including term polysemy, ambiguity, and the static nature of concepts that failed to evolve with research trends.
OpenAlex subsequently adopted a labeled dataset from CWTS~\cite{van2024open} and integrated it with concepts from the All Science Journal Classification (ASJC) codes. 
However, obtaining high-quality training labels at scale remains a challenge, often forcing methods to construct topic taxonomies from scratch~\cite{liu2014hierarchical}.


\subsection{Topic Modeling} 
Unsupervised topic modeling techniques discover common themes within document collections by clustering similar documents and deriving topic labels from shared characteristics within clusters.
For scientific papers, these methods typically leverage multiple signals including semantic similarity from titles and abstracts, citation network structures, and shared references.
Domain-specific pretrained models like SciBERT~\cite{hosokawa2024} have been shown to enhance clustering quality of abstract embeddings by using purpose-built tokenizers and specialized pretraining that capture field-specific terminology.

Similarly, BERTopic~\cite{grootendorst2022bertopic} uses a pre-trained language model to generate document embedding, performs dimensionality reduction, and creates semantically similar clusters (using HDBSCAN) that each represent a distinct topic. BERTopic uses a variant of TF-IDF to extract topic representations from each topic. 
CWTS~\cite{van2024open} leverages large language models (LLMs) to generate research topic labels. The LLM receives titles of the 250 most cited publications in each cluster and generates a label and a brief summary of the research area. 

However, clustering-based approaches face limitations: extracted topics often fail to align with established ontologies, and poor clustering quality can result in topics lacking clear interpretability.

\subsection{Knowledge Organization Systems} 

A Knowledge Organization System (KOS) \cite{salatino:2024} is a structured framework that enables the arrangement, management, retrieval, and dissemination of information. 
These systems are crucial to library science, information retrieval, and knowledge management, offering standardized vocabularies and defined relationships between concepts to improve information accessibility and interoperability~\cite{Hodge:2000}. 

KOS encompasses several key methodologies:
\begin{itemize}[leftmargin=*]
    \item \emph{Classification schemes}: Systems such as the Dewey Decimal Classification (DDC)~\cite{Dewey:2011} and Library of Congress Classification (LCC) systematically group related concepts based on shared characteristics, providing logical organization for library resources.
    \item \emph{Thesauri}: Controlled vocabularies that map relationships between terms, including synonyms, antonyms, and hierarchical relationships. A prominent example is the Medical Subject Headings (MeSH), which is the controlled vocabulary used to index the huge amount of biomedical literature \cite{NLM:2022}.
    \item \emph{Taxonomies}: Hierarchical systems organizing concepts into parent-child relationships, commonly used in web navigation and content management systems.
    \item \emph{Ontologies}: Formal and machine-interpretable specifications that define domain concepts and their relationships~\cite{Guarino:2009}. These are crucial for semantic web technologies and AI applications, enabling data interoperability and automated reasoning.
    \item \emph{Controlled Vocabularies}: Curated term lists ensure consistent language across collections, enhancing retrieval precision and accuracy.
\end{itemize}

KOSs help to uncover knowledge from various sources, especially in the context of big data and the semantic web. Integrating and discovering knowledge from heterogeneous sources becomes a lot easier with KOS, which is also very usefu; for advanced research and development tasks. The Simple Knowledge Organization System (SKOS) presents a model for sharing and linking knowledge organization systems on the web \cite{Miles:2009}. This W3C-endorsed model based on Resource Description Framework (RDF) promotes interoperability, as well as the seamless exchange of information between heterogeneous systems. 
%%\textcolor{blue}{KR: Is SKOS important to cover?}
%% Yes, it was mentioned several times at the ISWC so I believe it is still an active w3c standard https://www.w3.org/TR/skos-reference/

Despite their utility, KOSs encounter some issues, such as how to remain pertinent in rapidly changing areas of knowledge and how to ensure that they work well with other systems. \cite{lauruhn:2016} comment on the need for KOSs to have adaptive management strategies that allow them to change along with our knowledge structures.

Recent research shows promises of integrating KOS with large language models (LLMs) to enhance AI explainability. \cite{ahmed:2023} demonstrated that the combination of knowledge graphs with LLM produces more interpretable outputs than LLMs alone, while \cite{krause:2024} showed that LLMs coupled with external knowledge sources improve commonsense reasoning capabilities in AI systems.