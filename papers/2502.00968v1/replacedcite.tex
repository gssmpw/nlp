\section{Related Work}
\vspace{-0.2cm}
\label{sec:lit}

% Previous research on the alignment of diffusion models to downstream reward functions can be broadly categorized into two approaches: (i) learning-based and (ii) guidance-based methods.

\textbf{Finetuning-based alignment.} Prominent methods in this category typically involve either training a diffusion model to incorporate additional inputs such as category labels, segmentation maps, or reference images ____ or applying reinforcement learning (RL) to finetune a pretrained diffusion model to optimize for a downstream reward function ____. While these approaches have been successfully employed to satisfy diverse constraints, they are computationally expensive. Furthermore, finetuning diffusion models is prone to ``reward hacking'' or ``over-optimization'' ____, where the model loses diversity and collapses to generate samples that achieve very high rewards. This is often due to a mismatch between the intended behavior and what the reward model actually captures. In practice, a perfect reward model is extremely difficult to design. As such, here we focus on inference-time guidance-based alignment approaches where these issues can be circumvented.  \clr{Additionally, none of the fine-tuning based methods are built for image-to-image scenarios, which is the focus of this work, as we clarified earlier. To compare against them, a direct approach could be fine-tuning per reference image, which renders the process computationally infeasible, or taking a meta-learning approach to fine-tuning. However, such fundamental adjustments are beyond the current scope of our work.}

\textbf{Gradient-based inference-time alignment.} 
% Another prominent category keeps the pretrained foundation diffusion model intact and guides the denoising process through a reward (or value) function ____. 
There are two main divides within this category: (i) guidance based on a \emph{value} function, and (ii) guidance based on a downstream \emph{reward} function. In the first divide, a value function is trained offline using the noisy intermediate samples from the diffusion model. Then, during inference, gradients from the value function serve as signals to guide the generation process ____. A key limitation of such an approach is that the value functions are specific to the reward model and the noise scales used in the pretraining stage. Thus, the value function has to be retrained for different reward and base diffusion models. The second divide of methods successfully overcomes this by directly using the gradients of the reward function based on the approximation of fully denoised images using Tweedie's formula ____. Interesting follow-up research has explored methods to reduce estimation bias ____ and to scale gradients for maintaining the latent structures learned by diffusion models ____. Despite such advancements, the need for differentiable guidance functions can limit the broader applicability of the gradient-based methods.

\textbf{Gradient-free inference-time alignment.} Tree-search alignment has recently gained attention in the context of autoregressive language models (LMs), where it has been demonstrated that Best-of-$N$ (BoN) approximates sampling from a KL-regularized objective, similar to those used in reinforcement learning (RL)-based finetuning methods ____. This approach facilitates the generation of high-reward samples while maintaining closeness to the base model. ____ demonstrate that the gap between Best-of-$N$ (BoN) and token-wise {\em value-based} decoding ____ can be bridged using a blockwise decoding strategy. Inspired by this line of research, we propose a simple blockwise alignment technique (tree search with a fixed depth) that offers key advantages: (i) it preserves latent structures learned by diffusion models without requiring explicit scaling adjustments, unlike gradient-based methods, and (ii) it avoids ``reward hacking'' typically associated with learning-based approaches. Concurrently, ____ propose a related method, called SVDD-PM, based on the well-known token-wise decoding strategy in the LM space. In contrast, we devise a blockwise sampling strategy because it allows further control on the level of intervention, and offers a trade-off between divergence and alignment, which is of primal interest in the context of guided generation. To enhance the sampling strategy in terms of efficiency, we apply adjustable noise-conditioning which also offers greater control over guidance signals and further improves alignment. \clr{Sequential Monte Carlo-based methods (SMC) for diffusion models ____ share similarities with tree-search-based alignment methods such as ours, particularly in not requiring differentiable reward models. They involve resampling across an entire batch of images, which can lead to suboptimal performance when batch sizes are small since the SMC theoretical guarantees hold primarily with large batch sizes. In contrast, our method performs sampling on a per-sample basis. Lastly, using SMC for reward maximization can also result in a loss of diversity, even with large batch sizes.}


\vspace{-.1in}