
\documentclass[10pt]{article} % For LaTeX2e
%\usepackage{tmlr}
% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{tmlr}
% To de-anonymize and remove mentions to TMLR (for example for posting to preprint servers), instead use the following:
\usepackage[preprint]{tmlr}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

% \usepackage{hyperref}
\usepackage{url}
\usepackage{xspace}
\usepackage{enumitem}
\usepackage{wrapfig}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
% \usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
% \usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{arydshln}
% \usepackage{booktabs}
\usepackage{tabularx, xcolor, colortbl}
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\usepackage[most]{tcolorbox}
\usepackage{lipsum}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\SetAlCapNameFnt{\small}
\SetAlCapFnt{\small}
% \SetAlgorithmName{Algorithm}{Algo}{List of Algorithms}

\SetKwInput{KwInput}{Require} % Set the Input
\SetKwInput{KwOutput}{Return} % set the Output
\SetKwComment{Comment}{/* }{ */}
% add a "#" before the input text "#1"
\newcommand{\PyComment}[1]{\ttfamily\textcolor{commentcolor}{\# #1}} 
 % \ttfamily is the code font
\newcommand{\PyCode}[1]{\ttfamily\textcolor{black}{#1}}
\newcommand{\clr}[1]{{\color{black}#1}}

\newcommand{\red}[1]{\textcolor{red}{#1}} 
\newcommand{\green}[1]{\textcolor{green}{#1}} 
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\definecolor{darkgreen}{RGB}{47, 135, 91} 
\definecolor{commentcolor}{RGB}{110,154,155}
\definecolor{LightCyan}{rgb}{0.88,1,1}
\definecolor{tabPurple}{rgb}{0.87, 0.87, 0.87}
\definecolor{tabPurple2}{rgb}{0.933, 0.937, 1}
\definecolor{tabOrange}{rgb}{0.99, 0.89, 0.79}
\definecolor{tabBlue}{rgb}{0.90, 0.96, 0.97}
\definecolor{tabBlue2}{rgb}{0.92, 0.95, 1}
\definecolor{tabGreen}{rgb}{0.86, 0.90, 0.85}
\definecolor{myyellow}{rgb}{0.98, 0.98, 0.82}
\definecolor{mygreen}{rgb}{0.94, 1.0, 0.94}
\definecolor{darkcyan}{RGB}{67, 117, 148}

\usepackage[colorlinks=true,citecolor=darkcyan,
            linkcolor=red]{hyperref}%

 \newcommand{\ahmad}[1]{{\noindent \textit{\small\textcolor{red}{Ahmad: #1}}}} 

 \newcommand{\Hadi}[1]{{\noindent \textit{\small\textcolor{blue}{Hadi: #1}}}} 

\newcommand{\sayak}[1]{\textcolor{commentcolor}{#1}}
\usepackage[colorinlistoftodos, color=mygreen]{todonotes}

\usepackage[capitalise]{cleveref}
% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% \usepackage{amsmath}
% \DeclareMathOperator*{\argmax}{arg\,max}
% \DeclareMathOperator*{\argmin}{arg\,min}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
% \newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\newtheorem*{assumption*}{\assumptionnumber}
\providecommand{\assumptionnumber}{}
\makeatletter
\newenvironment{assumption}[2]
 {%
  \renewcommand{\assumptionnumber}{\textbf{\textup{Assumption #1}} \textup{(#2)}}%
  \begin{assumption*}%
  \protected@edef\@currentlabel{#1}%
 }
 {%
  \end{assumption*}
 }
%\makeatother

% \makeatletter
% \setlength{\algocf@lcaptionbox}{\linewidth} % reset the caption bow width to be as large as the rules 
% \renewcommand{\algocf@makecaption@ruled}[2]{%
% \global\sbox\algocf@capbox{\hskip\AlCapHSkip % left indent of the box
% \addtolength{\algocf@lcaptionbox}{-2\AlCapHSkip}% reduce box width to account for symmetric margins
% \parbox[t]{\algocf@lcaptionbox}{\algocf@captiontext{#1}{#2}}}% print the caption
% }%
% \makeatother

\newcommand{\note}[1]{\colorbox{green}{#1}}
\newcommand{\ourmethod}{\texttt{CoDe}}
\newcommand{\ourcondmethod}{\texttt{C-CoDe}}
% \DeclareMathOperator*{\argmax}{argmax}



\title{\texttt{CoDe}: Blockwise Control for Denoising Diffusion Models}

% Authors must not appear in the submitted version. They should be hidden
% as long as the tmlr package is used without the [accepted] or [preprint] options.
% Non-anonymous submissions will be rejected without review.

\author{
    %Authors
    % All authors must be in the same font size and format.
    Anuj Singh\textsuperscript{\rm 1 2} \quad  \quad Sayak Mukherjee\textsuperscript{\rm 1}
    \quad \quad
    Ahmad Beirami\textsuperscript{\rm 3}\quad \quad Hadi Jamali-Rad\textsuperscript{\rm 1 2}\\~\\
    \textsuperscript{\rm 1}Delft University of Technology, The Netherlands\\
    \textsuperscript{\rm 2}Shell Global Solutions International B.V., Amsterdam, The Netherlands\\
    \textsuperscript{\rm 3}Massachusetts Institute of Technology, Cambridge MA, USA\vspace{0.05in}\\
    {\small \texttt{\{a.r.singh, h.jamalirad\}@tudelft.nl}}
    %\texttt{\{beirami\}@google.com} 
}

% \author{\name Kyunghyun Cho \email kyunghyun.cho@nyu.edu \\
%       \addr Department of Computer Science\\
%       University of New York
%       \AND
%       \name Raia Hadsell \email raia@google.com \\
%       \addr DeepMind
%       \AND
%       \name Hugo Larochelle \email hugolarochelle@google.com\\
%       \addr Mila, Universit\'e de Montr\'eal \\
%       Google Research\\
%       CIFAR Fellow}

% The \author macro works with any number of authors. Use \AND 
% to separate the names and addresses of multiple authors.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

% \def\month{MM}  % Insert correct month for camera-ready version
% \def\year{YYYY} % Insert correct year for camera-ready version
% \def\openreview{\url{https://openreview.net/forum?id=XXXX}} % Insert correct link to OpenReview for camera-ready version


\begin{document}


\maketitle

\begin{abstract}
Aligning diffusion models to downstream tasks often requires finetuning new models or gradient-based guidance at inference time to enable sampling from the reward-tilted posterior. In this work, we explore a simple inference-time gradient-free guidance approach, called controlled denoising (\ourmethod{}), that circumvents the need for differentiable guidance functions and model finetuning. \ourmethod{} is a blockwise sampling method applied during intermediate denoising steps, allowing for alignment with downstream rewards. \clr{Our experiments demonstrate that, despite its simplicity, \ourmethod{} offers a favorable trade-off between reward alignment, prompt instruction following, and inference cost, achieving a competitive performance against the state-of-the-art baselines}. Our code is available at: \href{https://github.com/anujinho/code} {https://github.com/anujinho/code}.
\end{abstract}

\vspace{-0.1in}
\section{Introduction}
\vspace{-0.2cm}
\begin{wrapfigure}{r}{0.6\textwidth}
    \vspace{-1em}
    \begin{minipage}{0.6\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figs_revised/teaser_v2.pdf}
    \vspace{-0.8cm}
    \caption{\ourmethod{} generates high quality compression (non-differentiable reward), style, face and stroke (differentiable rewards)  guided images.}
    \vspace{-1em}
    \end{minipage}
\end{wrapfigure}

Diffusion models have emerged as a powerful tool for generating high-fidelity realistic images, videos, natural language content and even molecular data \citep{Ho2020DenoisingModels,Song2020DenoisingModels, bar2024lumiere, wu2022diffusion}. While diffusion models have proved to be effective at modeling complex and realistic data distributions, their successful application often hinges on following user-specific instructions in the form of images, text, bounding-boxes or other {\em rewards}. A common approach to the \emph{algnment} of diffusion models to user preferences involves finetuning them on preference data, which is typically done through reinforcement learning (RL), to generate samples with a higher reward while maintaining a low KL divergence from the base diffusion model \citep{Fan2023DPOK:Models, Uehara2024UnderstandingReview}. %Despite their effectiveness, finetuning strategies face their own set of challenges such as limited flexibility w.r.t. different instruction modalities, hindered generalizability to various domains due to their dependence on task-specific datasets, and high computational costs of training from scratch. 

Guidance-based approaches keep the base diffusion model frozen and control its output by aligning its generative process to a reward function at inference time. %; thus, offering potential remedies to the aforementioned challenges. 
%Our proposed approach lies under this category. 
{\em Gradient-based guidance} methods utilize gradients of the reward at each denoising step to align the generated samples with the downstream task~\citep{Chung2023DiffusionProblems,yu2023freedom, Bansal2024UniversalModels, he2024manifold}. In addition to requiring access to a {\em differentiable} reward signal, these approaches require memory-intensive gradient computations. On the other hand, {\em gradient-free guidance} methods such as Best-of-$N$~\citep{Beirami2024TheoreticalPolicy} circumvent the need for differentiable rewards but can potentially be computationally intractable as they sometimes need a large number of samples, $N$,  to satisfy the alignment goal.

In this paper, we consider a simple gradient-free guidance approach that aims at remedying the intractability of best-of-$N$. Drawing inspiration from blockwise controlled decoding in language models~\citep{Mudgal2024ControlledModels}, we propose controlled denoising (\ourmethod{}), which exerts best-of-$N$ control over $N$ blocks of $B$ denoising steps rather than waiting for the fully denoised images. Our \emph{key contributions} can be summarized as follows:
%
\begin{enumerate}[label=\Roman*.]

\item We propose \ourmethod{}  ---  an inference-time blockwise guidance approach which samples from an optimal KL-regularized objective. We study the interplay between the sample size ($N$) and block-size ($B$) and demonstrate that \ourmethod is effective at improving the reward at the cost of the least amount of KL divergence from the base model. 

\item We assess the performance of the aligned diffusion models structurally for two case studies (Gaussian Mixture Model (GMM), and image generation), in \clr{four} scenarios under image generation: style, face, stroke, and \clr{compressibility} guidance.
Our extensive (qualitative and quantitative) experimental results demonstrate that \ourmethod{}{} \clr{achieves competitive performance against the} state-of-the-art baselines, while offering a
balanced trade-off between reward alignment, prompt instruction following, and inference cost.
\end{enumerate}

\section{Preliminaries}
\vspace{-0.2cm}
\subsection{Diffusion Models}
\vspace{-0.1cm}
% vanilla diffusion models
A diffusion model provides an efficient procedure to sample from a probability density \(q(x)\) by learning to invert a forward diffusion process. The forward process is a Markov chain iteratively adding a small amount of random noise to a ``clean'' data point \(x_0 \in \gX\) over \(T\) steps. The noisy sample at step $t$ is given by \(x_t = \sqrt{\Bar{\alpha}_t}x_{0} + \sqrt{1 - \Bar{\alpha}_t}\varepsilon_t\), where \(\varepsilon_t \sim \gN(0, 1)\), \(\alpha_t = 1- \beta_t\),  \(\Bar{\alpha}_t = \prod_{t=1}^T \alpha_t\), and \(\{\beta_t \}_{t \in [T]}\) is a variance schedule \citep{Ho2020DenoisingModels, Nichol2021ImprovedModels}. The forward process can then be expressed as:
%
\begin{equation}
    q(x_{1:T}|x_0) = \prod_{t=1}^T q(x_t|x_{t-1}),\quad q(x_t|x_{t-1}) = \mathcal{N}(x_t; \sqrt{1-\beta_t}x_{t-1}, \beta_t\mathbf{I}).
\end{equation}
%
Now, to estimate \(q(x)\), the diffusion model \(p_\theta\) learns the conditional probabilities \(q(x_{t-1}|x_t)\) to reverse the diffusion process starting from a fully noisy sample \(x_T\sim \gN(0,1)\) as:
%
\begin{equation}
    p_{\theta}(x_0) = p(x_T)\prod_{t=1}^T p_{\theta}(x_{t-1}|x_t),\quad p_{\theta}(x_{t-1}|x_t) = \mathcal{N}(x_{t-1}; \mu_{\theta}(x_t, t), \beta_t\mathbf{I}), 
\end{equation}
% 
where the variance is fixed at $\beta_t\mathbf{I}$, and only  $\mu_{\theta}(x_t, t)$ is learned as:
%
\begin{equation}
    \mu_{\theta}(x_t, t) = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{1-\alpha_t}{\sqrt{1 - \Bar{\alpha}_t}}\varepsilon_\theta(x_t, t)\right).
\end{equation}
%
Here, \(\varepsilon_\theta\) is a neural network which attempts to predict the noise added to \(x_{t-1}\) in the forward process as:
%
\begin{equation}
    \varepsilon_\theta(x_t, t) \approx \varepsilon_t = \frac{x_t - \sqrt{\Bar{\alpha}_t}x_{0}}{\sqrt{1 - \Bar{\alpha}_t}}.
\end{equation}
% 
Furthermore, using a conditioning signal \(c\), diffusion models can be extended to sample from \(p_\theta(x|c)\). The conditioning signal, $c$, can take diverse forms, from text prompts and categorical information to semantic maps \citep{Zhang2023AddingModels, Mo2023FreeControl:Condition}. Our work focuses on a text-conditioned model, Stable Diffusion \citep{Rombach2021High-ResolutionModels}, which has been trained on a large corpus consisting of \(M\) image-text pairs \(\train = \{(x^i, c^i)\}_{i=1}^M\) using a reweighed version of the variational lower bound \citep{Ho2020DenoisingModels} as optimization loss function
%
\begin{equation}
    \label{eq:obj_noise}
    \hat{\theta} = \argmin_{\theta} \, \mathbb{E}_{t\sim [1,T],\; x_0, \varepsilon_t} \left[ \|\varepsilon_t - \varepsilon_{\theta}(\sqrt{\Bar{\alpha}_t}x_{0} + \sqrt{1 - \Bar{\alpha}_t}\varepsilon_t,\; c,\; t)\|^2 \right].
\end{equation}
%

\subsection{KL-Regularized Objective}
\label{sec:klobj}

% trained with the objective defined in Eq.~\ref{eq:obj_noise}
Consider we have access to a reference diffusion model \(p(\cdot)\), which we refer to as the \emph{base} model. Note that here we drop $\theta$ (from $p_{\theta}$) for the ease of notation, also because base diffusion model parameters are kept intact throughout the inference-time guidance. Our goal is to obtain samples from the base model that optimize a downstream reward function \(r(\cdot): \gX \rightarrow \sR\), while ensuring that the sampled data points do not deviate significantly from $p$ to prevent degeneration in terms of image fidelity and diversity of the output samples \citep{Ruiz_2023}. Thus, we aim to sample from a reward \emph{aligned} diffusion model ($\pi$) that optimizes for a KL-regularized objective to satisfy both requirements. Let us start by defining some key concepts. 

% Additionally, we want to ensure that the sampled data points do not deviate significantly from \(p_\theta(\cdot|c)\) as it would result in degeneration of the diffusion model in terms of image fidelity and diversity of the output samples \citep{Ruiz_2023}. We aim to sample from the optimal solution of a KL-regularised objective to satisfy both requirements. To this end, we start by defining some important terms. 

\textbf{Value function}. The expected reward when decoding continues from a partially decoded sample \(x_t\):
%
\begin{equation}
    \label{eq:value}
    V(x_t; p) = \mathbb{E}_{x_0 \sim p(x_0|x_t)} [r(x_0)].
\end{equation}
%
\textbf{Advantage function}. We can define a one-step advantage of using another diffusion model $\pi$ for optimizing the downstream reward as:
%
\begin{equation}
    \label{eq:adv}
    A(x_t; \pi) := \mathbb{E}_{x_{t-1} \sim \pi(x_{t-1}|x_t)} \left[V(x_{t-1}; p)\right] - \mathbb{E}_{x_{t-1} \sim p(x_{t-1}|x_t)} \left[V(x_{t-1}; p)\right].
    % V(x_t; p_\theta, c).
    % = \mathbb{E}_{x_{t-1} \sim \pi(x_{t-1}|x_t)} [V(x_{t-1})] - V(x_t).
\end{equation}
%
It is important to note that the advantage of the base model (when \(\pi = p\)) is $0$. Thus, we aim to choose an \emph{aligned} model $\pi$ to achieve a positive advantage over the base model.

\textbf{Divergence}. We further denote the KL divergence ($\text{KL}(.||.)$, also known as relative entropy) between the aligned model $\pi$ and the base model $p$  at each intermediate step $x_t$ as:
%
\begin{equation}
\begin{split}
    \label{eq:div}
    D(x_t; \pi) := \textit{KL}\big(\pi(x_{t-1}|x_{t})\;\|\;p(x_{t-1}|x_{t})\big)
    = \int \pi(x_{t-1}|x_{t}) \log\frac{\pi(x_{t-1}|x_{t})}{p(x_{t-1}|x_{t})} \, dx_{t-1}.
\end{split}
\end{equation}
%
% The KL divergence term prevents deviation from the base model $p$ as we expect such a deviation to result in the deterioration of the quality of the generated samples.\\
%
\textbf{Objective}. Using \cref{eq:adv} and \cref{eq:div}, we can now formulate the KL-regularized objective as:
%
\begin{equation}
    \label{eq:klobj}
    \pi_\lambda^* = \argmax_{\pi} \big[ \lambda A(x_t; \pi) - D(x_t; \pi) \big],
\end{equation}
%
where $\lambda \in \mathbb{R}^{\geq 0}$ trades off reward for drift from the base diffusion model $p$.

\begin{theorem}
\label{thm:optpol}
The optimal model $\pi_\lambda^*$ for the objective formulated in \cref{eq:klobj} is given by:
%
\begin{equation}
    \label{eq:optpol}
    \pi_\lambda^*(x_{t-1}|x_{t}) \propto p(x_{t-1}|x_{t}) \, e^{\lambda V(x_{t-1}; p)}.
\end{equation}
%
\end{theorem}
As we shall discuss in Section~\ref{sec:code}, our proposed approach builds on Theorem~\ref{thm:optpol} to approximately sample from this reward aligned model using a Monte Carlo sampling strategy. An extension of the result in a conditional diffusion setting can be found in Appendix~\ref{app:proofs}. Notably, this is a step-wise variant of the more widely known similar objective~\citep{Korbak2022RLInference}, which has been used in some learning-based methods \citep{Prabhudesai2023AligningBackpropagation,Fan2023DPOK:Models,Wallace2023DiffusionOptimization,Black2023TrainingLearning,gu2024diffusionrpo,lee2024direct} discussed in Section~\ref{sec:lit} for fine-tuning a diffusion model. However, contrary to the prior art, we use our objective directly for a guidance-based alignment, where as the end-to-end objective would be intractable. We also remark that this advantage is similar to controlled decoding~\citep{Mudgal2024ControlledModels} and how it enables efficient sampling from reward guided distributions in language models.
In Appendix~\ref{app:samld}, we demonstrate that sampling can be achieved using Langevin dynamics \citep{Welling2011BayesianDynamics}, resulting in a generalized form of classifier guidance \citep{Dhariwal2021DiffusionSynthesis}. However, a key limitation of gradient-based approaches is the need for a differentiable reward function. To alleviate this, we explore a sampling-based method for model alignment allowing us to handle both differentiable and non-differentiable downstream rewards.
%while maintaining proximity to the base model.


\vspace{-.1in}
\section{\texttt{CoDe}: Blockwise Controlled Denoising}
\label{sec:code}
\vspace{-.1in}
% \todo[inline]{Key Points: (i) In this section, we explore two simple approaches for guidance (ii) First, we start by approximating the value function. (iii) Second, we discuss the blockwise sampling approach, a generalized version of SVDD-PM (CoDe with block-size 1). (iii) Then, we discuss the comparison of CoDe with Best-of-N sampling, which is the optimal policy for win rate-vs-divergence (iv) Finally, we explore noise conditioning to account for significant changes to the base model (limitation mentioned in SVDD-PM).}

Inspired by recent RL-based alignment strategies for LLMs through process rewards or value-guided decoding~\citep{Mudgal2024ControlledModels}, we propose a sampling-based guidance method to align a conditional pretrained diffusion model, $p(\cdot | c),$ following the optimal solution, $\pi_\lambda^*$, described in Theorem~\ref{thm:optpol}. In the following, we outline an approach to approximate the value function for intermediate noisy samples followed by introducing our sampling-based alignment strategy. Our proposed approach, coined as \ourmethod{}, is summarized in Algorithm~\ref{algo:code}. 
% followed by adjustable conditioning of the initial noise on the input image provided by the user offering extra degrees of control and allowing for applications such as reference face/style conditioning \citep{Bansal2024UniversalModels} and stroke painting generation \citep{Meng2021SDEdit:Equations}. Notably, this noise conditioning lowers the overall computational complexity substantially, by reducing the number of denoising steps as well as the number of samples, while achieving effective alignment in high-dimensional spaces.

\textbf{Approximation of the value function.} To compute the value function in \cref{eq:value} for an intermediate noisy sample \(x_t\), it is necessary to compute the expectation over \(x_0 \sim p(x_0|x_t)\). Note that for diffusion models such as DDPMs \citep{Ho2020DenoisingModels}, the predicted clean sample \(\hat{x}_0\) can be estimated given an intermediate sample \(x_t\) using Tweedie's formula \citep{efron2011tweedie} as follows:
%
\begin{equation}
    \label{eq:predpos}
    \hat{x}_0 = \mathbb{E}[x_0|x_t] =  \frac{x_t - \sqrt{1 - \Bar{\alpha}_t}\varepsilon_\theta(x_t, c, t)}{\sqrt{\Bar{\alpha}_t}}.
\end{equation}
%
By plugging  \cref{eq:predpos} into \cref{eq:value}, the value function can be approximated as:
%
\begin{equation}
    \label{eq:approxval}
        V(x_t; p, c) = \mathbb{E}_{x_0 \sim p_{\theta}(x_0|x_t, c)} [r(x_0)] \approx r(\mathbb{E}[x_0|x_t]) = r(\hat{x}_0).
\end{equation}
%
The benefit of such an approximation is that it circumvents the need for training a separate model to learn the value function, as is for instance adopted by DPS \citep{Chung2023DiffusionProblems} and Universal Guidance \citep{Bansal2024UniversalModels}. According to the Tweedie's formula, the approximation of the conditional expectation, $\mathbb{E}_{x_0} [r(x_0)]$, is tight when the base diffusion model parameters $\theta$ perfectly optimize Eq.~\ref{eq:obj_noise}. For example, this approximation is expected to be more accurate towards the end of the denoising process~\citep{Ye2024TFG:Models}.

%\subsection{BlockWise Sampling-Based Alignment}
%\vspace{-0.1cm}

% 
\begin{wrapfigure}{r}{0.45\textwidth}
\vspace{-14pt}
\begin{minipage}{0.45\textwidth}
\IncMargin{1.6em} 
\begin{algorithm}[H]
    \scriptsize
    \SetAlgoLined
    \DontPrintSemicolon
    \SetNoFillComment
    \Indm
    \clr{\KwInput{\(p\), $T$, \(N\), \(B\), $c$}}
    \Indp
        {Sample initial noise:} \(x_T \sim \gN(0, I)\)\\
        {Initialize counter:} \(s = 1\)\\
        \For{\(t \in [T-1,\cdots, 0]\)}{
            % $\textup{\texttt{mod}}(s,B) = 0$
            \If{$\textup{\texttt{mod}}(s,B) = 0$}{
                % {Sample \(N\) i.i.d samples by unrolling \(p\) over \(M\) steps:}\;
                % {\quad \(\{x_{a:b}^n\}_{n=1}^N \sim p(\cdot|x_{T-i+M})\), where \(a = T-1-i+M\) and \(b = T-1-i\)}\\
                {Sample \(N\) times over \(B\) steps:}
                {\quad \(\{x_{t-1}^{(n)}\}_{n=1}^N \sim \prod_{i=t}^{t+B} p(x_{i-1}|x_i)\)}\\
                {Select the sample with maximum value:}
                {\quad \(x_{t-1} \gets \underset{\{x_{t-1}^{(n)}\}_{n=1}^N}{\operatorname{argmax}} V(x_{t-1}^{(n)}; p, c)\)}\\
            }
            \(s \gets s + 1\)\\}
        
    \Indm
    \clr{\KwOutput{\(x_0\)}}
    \Indp
    \caption{\clr{\ourmethod{}}}
    \label{algo:code}
\end{algorithm}
\DecMargin{1.6em}
\end{minipage}
\vspace{-14pt}
\end{wrapfigure}
%

Our objective is to achieve an improved alignment vs. divergence trade-off by sampling from the optimal solution presented in Theorem~\ref{thm:optpol}. Therefore, by taking advantage of the approximation in \cref{eq:approxval}, we present a blockwise extension of Best-of-N (BoN) for diffusion models, termed as \textbf{Co}ntrolled \textbf{De}noising (\ourmethod{}) and outlined in Algorithm~\ref{algo:code}. \ourmethod{} integrates BoN sampling into the standard inference procedure of a pretrained diffusion model. Unlike BoN, instead of rolling out the full denoising  \(N\) times and selecting the best resulting sample, we opt for performing blockwise BoN. Specifically, for each block of \(B\) steps, we unroll the diffusion model \(N\) times independently (Algorithm~\ref{algo:code}, line $5$). Then, based on the value function, select the best sample (line $6$) to continue the reverse process until we obtain a clean image at \(t=0\). A key advantage of \ourmethod{} is its ability to achieve similar alignment-divergence trade-offs while using a significantly lower value of \(N\), as is demonstrated in Section~\ref{sec:toy}. %Note that the inner loop of \ourmethod{} (lines $5$-$10$) runs for $\tau$ steps (instead of $T$) due to adjustable noise conditioning discussed in the following.} For the sake of brevity, we assume $\tau$ to be divisable by $B$; otherwise, we apply the same steps on a last but smaller block.

\textbf{Best-of-N (BoN) sampling for diffusion models.} A strong baseline for inference-time alignment is Best-of-N (BoN). Empirical evidence from the realm of large language models (LLMs)~\citep{Gao2022ScalingOveroptimization, Mudgal2024ControlledModels, Gui2024BoNBoNSampling} suggests that BoN closely approximates sampling from the optimal solution presented in Theorem~\ref{thm:optpol}, which is  theoretically corroborated by~\cite{beirami2024theoretical, yang2024asymptotics}. More recently, BoN has emerged as a strong baseline for scaling inference-time compute~\citep{snell2024scaling, brown2024large}.
In BoN, \(N\) samples are obtained from the diffusion model by completely unrolling it out over \(T\) denoising steps. Then, the most favorable image is selected based on a reward. This renders BoN sampling equivalent to \ourmethod{} with $B=T$. For other intermediate values of $B,$ \ourmethod{} could be seen as a blockwise generalization of BoN. 

\textbf{Soft Value-Based Decoding (SVDD) for diffusion models.} Concurrently to our work, \citet{Li2024Derivative-FreeDecoding} proposed an iterative sampling method to integrate soft value function-based reward guidance into the standard inference procedure of pre-trained diffusion models. The soft value function helps look-ahead into how intermediate noisy states lead to high rewards in the future. Specifically, this method involves first sampling \(N\) samples from the base diffusion model, and then selecting the sample corresponding to the highest reward across the entire set. This highest-reward sample is used for the next denoising step in the reverse-diffusion process. This renders SVDD-PM sampling as a special case of \ourmethod{}, operating specifically on a step block size $B = 1$.

% \textbf{Comparison with image-to-image diffusion}. \textcolor{red}{Compare with SDEdit \citep{Meng2021SDEdit:Equations}}.

\section{Experimental Setup}
\label{sec:exp}
%
We assess the performance of \ourmethod{} by comparing it against a suite of existing state-of-the-art guidance methods, in Text-to-Image (T2I) and Text-and-Image-to-Image ((T+I)2I)  scenarios, across both differentiable and non-differentiable reward models. Unless otherwise mentioned, for all experiments, we use a pretrained Stable Diffusion version \(1.5\) \citep{Rombach2021High-ResolutionModels} as our base model, which is trained on the LAION-\(400\)M dataset \citep{schuhmann2021laion400m}. As highlighted earlier, we strive to present meaningful comparative (both qualitative and quantitative) results across a variety of scenarios. For quantitative evaluations, we generate $50$ images per setting (i.e., prompt-reference image pair) with $500$ DDPM steps. To achieve this, we have used NVIDIA A100 GPUs with $80$GB of RAM. Through extensive experiments, we aim to answer the following questions: {\em Does \ourmethod{} offer a comptetitive alignment-divergence trade-off compared to other baselines? How does \ourmethod{} perform across guidance tasks qualitatively and quantitatively?}
%\textbf{[Q3]}. Does \ourmethod{} offer better image vs. text alignment compared to other baselines?

\textbf{Baselines.} We select a set of widely adopted baselines from the literature. Recall that our goal is to sample from the optimal value of the KL-regularized objective, as outlined in Theorem~\ref{thm:optpol}. One approach to achieve this, as detailed in Appendix~\ref{app:samld}, is using a gradient-based method with an approximated value function, as in DPS \citep{Chung2023DiffusionProblems}, which serves as our first baseline. Further, Universal Guidance (UG) \citep{Bansal2024UniversalModels}, our second baseline, improves upon DPS by offering better gradient estimation. Another way to sample from Theorem~\ref{thm:optpol} is by using a sampling-based approach such as in \ourmethod{}. In this direction, we consider Best-of-N (BoN) \citep{beirami2024theoretical} and SVDD-PM \citep{Li2024Derivative-FreeDecoding} as our third and fourth baselines, which are also special cases of \ourmethod{} as explained earlier. For the sake of completeness, we also consider SDEdit \citep{Meng2021SDEdit:Equations} as a relevant (T+I)2I approach, for which all baselines could build on.

\textbf{Extensions with Noise Conditioning.} When the reward distribution deviates significantly from the base distribution \(p\), sampling-based approaches would require a relatively larger value of \(N\) to achieve alignment. To tackle this, a reference input sample, e.g. an image with the desired paining stroke or style, denoted as \(x_{\mathrm{ref}}\),  is provided as an additional conditioning input. Next to that, inspired by image editing techniques using diffusion \citep{Meng2021SDEdit:Equations,koohpayegani2023genie}, we add partial noise corresponding to only $\tau = \eta \times T$ (with $\eta \in (0, 1]$) steps of the forward diffusion process, instead of the full noise corresponding to $T$ steps. Then, starting from this noisy version of the reference image $x_\tau$, \ourmethod{} progressively denoises the sample for only $\tau$ steps to generate the clean, reference-aligned image $x_0$. By conditioning the initial noisy sample $x_\tau$ on the reference image $x_{\textup{ref}}$, we can generate images $x_0$ that better incorporate the characteristics and semantics of the reference image while adhering to the text prompt $c$. An extended version of Algorithm~\ref{algo:code} with noise-conditioning, denoted as \ourmethod($\eta$) is discussed in detail in Appendix~\ref{sec: full_code} (see lines $1-3$ in Algorithm~\ref{algo:full_code}). For the sake of fair comparison, we apply this enhancement also to other (T+I)2I baselines denoting them as BoN($\eta$), SVDD-PM($\eta$), UG ($\eta$) and DPS($\eta$). As we demonstrate in our experimentation, threshold $\eta$ provides an \emph{extra knob} built in \ourmethod{} allowing the user to efficiently trade off divergence for reward. Note that the reward-conditioning of the generated image is inversely proportional to the value of $\eta$. Setting $\eta = 1$ results in $\tau = T$ and fully deactivates the input-image conditioning. A byproduct of this conditioning is compute efficiency, as is discussed in Section~\ref{sec:conc}.

% We consider a set of prominent guidance-based baselines for comparison: (i) DPS \citep{Chung2023DiffusionProblems}, (ii) Universal Guidance \citep{Bansal2024UniversalModels}, (iii) SVDD-PM \citep{Li2024Derivative-FreeDecoding} and (iv) Best-of-N \citep{Gao2022ScalingOveroptimization}. While , providing quantitative statistical results beyond these baselines turned out to be prohibitive, as such we have limited ourselves to the most relevant.  

\textbf{Evaluation Settings and Metrics.} We consider two case studies. \textbf{Case Study I}: a prototypical $2$D Gaussian Mixture Models (GMMs) in Section~\ref{sec:toy}, as is also studied in \citep{Ho2021Classifier-FreeGuidance,Wu2024TheoreticalModels}; \textbf{Case Study II}: widely adopted T2I and (T+I)2I evaluations using Stable Diffusion in Section~\ref{sec:sdres} across \clr{four} reward-alignment scenarios: (i) style, (ii) face (iii) stroke, \clr{and (iv) compressibility} guidance. For Case Study I, we present trade-off curves for win rate versus KL-divergence for all baselines. For Case Study II, since calculating KL-divergence in high-dimensional image spaces is intractable, we use Frechet Inception Distance (FID) \citep{heusel2017gans}. To ensure we capture alignment w.r.t reference image (and avoid using the guidance reward itself) we borrow an image alignment metric commonly used in style transfer domain \citep{gatys2016image, yeh2020improving}, referred to as I-Gram here. Further, we assess prompt alignment using CLIPScore \citep{Hessel_2021}, referred to as T-CLIP throughout the paper. Additionally, we consider win rate (commonly adopted in the LLM space) as yet another evaluation metric, where it reflects on the number of samples offering larger reward than the base model. To sum up, we consider expected reward, FID, I-Gram, T-CLIP, and win rate.  

% For quantitative evaluations in the Gaussian Mixture Model (GMM) case-study (Section~\ref{sec:toy}) we report expected reward vs. KL-divergence. When it comes to the image setting with Stable Diffusion (Section~\ref{sec:sdres}), since calculating KL-divergence in high-dimensional image spaces is intractable, we use Frechet Inception Distance (FID) \citep{heusel2017gans}, a widely used metric in the literature. One drawback of FID is the underlying assumption that the latent representations obtained from an Inception model \citep{szegedy2015going} follow a Gaussian distribution, which may not always be valid. Therefore, we also report CLIP-based Maximum Mean Discrepancy (CMMD) \citep{jayasumana2024rethinking} as a divergence measure. To evaluate alignment with text prompts \(c\), we use CLIPScore \citep{Hessel_2021}, referred to as T-CLIP in the tables. Here, we report the expected reward across all text prompt-reference image pairs vs. FID, CMMD and T-CLIP. 

% 

\section{Case Study I: Gaussian Mixture Models (GMMs)}
\label{sec:toy}
\vspace{-0.1cm}

% \begin{wrapfigure}{r}{0.5\textwidth}
%     \vspace{-1.5em}
%     \begin{minipage}{0.45\textwidth}
%     \centering
%     \begin{subfigure}{\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figs_final/toy/toysetup_sc2.pdf}\vspace{-0.5em}
%     \end{subfigure}
%     \begin{subfigure}{\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figs_revised/toy/plots_ood_revised/result_ood.pdf}
%         \vspace{-0.3cm}
%     \end{subfigure}
%     \vspace{-0.5cm}
%     \caption{Setup (top row) and reward vs. divergence trade-off (bottom row) for Case Study I. \ourmethod{} offers highest reward at lowest divergence with much lower $N$ than BoN.}
%     \label{fig:toysetup}
%     \end{minipage}
%     \vspace{-1em}
% \end{wrapfigure}

To establish an in-depth understanding of the impact of the proposed methods, we start with a simple model/reward distribution as shown in Fig.~\ref{fig:toysetup}. For the prior distribution, we consider a $2$D Gaussian mixture model  $p(\vx_0) = \sum_{i=0}^{2} w_i\mathcal{N}(\bm \mu_i, {\bm \sigma}^2{\bm I}_2)$, where $\sigma = 2$, $[{\bm \mu}_1, {\bm \mu}_2,{\bm \mu}_3] = [(5, 3), (3, 7), (7, 7)]$, and ${\bm I}_d$ is an $d$-dimensional identity matrix. 
%Reward function $r(x)$ is a simple Euclidean distance and we consider $e^{r(\vx_o)}$ as the reward distribution. 
Additionally, we define the reward distribution as \(p(r|\vx) = \gN({\bm \mu}_r, {\bm \sigma}_r^2{\bm I}_2)\) with \({\bm \mu}_r = [14,3]\) and \({\bm \sigma}_r = 2\). As can be seen in the figure, in this case and by design, reward distribution is far off the peak of the prior. Here, we train a diffusion model with a \(3\)-layer MLP that takes as input \((\vx_t, t)\) and predicts the noise \({\bm \varepsilon}_t\). This model is trained over \(200\) epochs with \(T=1000\) denoising steps. Note that all other discussed baselines can straightforwardly be trained in this setting.
%
\begin{figure}[t]
    \centering
    \includegraphics[width=0.65\linewidth]{figs_revised/toy/plots_ood_revised/result_ood_full.pdf}
    \vspace{-0.3cm}
    \caption{Setup (left, middle) and reward vs. divergence trade-off (right) for Case Study I. \ourmethod{} offers highest reward at lowest divergence with much lower $N$ than BoN.}
    \label{fig:toysetup}
\end{figure}
%
% %
% \begin{figure}[t]
%     \begin{subfigure}{0.33\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figs_revised/toy/rew_prior_dist_plots_revised/rewards_n10_B_eta.pdf}
%         % \label{fig:enter-label}
%     \end{subfigure}
%     \begin{subfigure}{0.33\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figs_revised/toy/rew_prior_dist_plots_revised/rewards_n50_B_eta.pdf}
%         % \label{fig:enter-label}
%     \end{subfigure}
%     \begin{subfigure}{0.33\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figs_revised/toy/rew_prior_dist_plots_revised/nvals_r02_log_eta.pdf}
%         % \label{fig:enter-label}
%     \end{subfigure}
%     \vspace{-0.5cm}
%     \caption{In contrast to BoN, \ourmethod{} with and without noise-conditioning ($\eta=0.6$, $\eta=1$, resp.) are robust against increased distance between reward and prior distributions. \ourmethod{} achieves the same reward as BoN at much lower $N$, with $B=80$.\ahmad{What is B in these figures? I think that needs to be discussed and ideally ablated}}
%     % Increasing distance between reward and prior distributions hits BoN's  drastically, while the proposed approaches are almost robust against that.
%     \label{fig:addrestoy}
%     \vspace{-2em}
% \end{figure}
% %
%
\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figs_revised/toy/rew_prior_dist_plots_revised/eta_B_abl.pdf}
    \vspace{-0.4cm}
    \caption{In contrast to BoN, SVDD-PM, \ourmethod{} with and without noise-conditioning ($\eta=0.6$, $\eta=1$, resp.) are robust against increased distance between reward and prior distributions. SVDD-PM's generated samples offer almost zero variance indicating reward over-optimization.}
    \label{fig:addrestoy}
    \vspace{-0.5cm}
\end{figure}
%
% %
% \begin{wrapfigure}{r}{0.5\textwidth}
%     \vspace{-1.5em}
%     \begin{minipage}{0.45\textwidth}
%     \centering
%     \begin{subfigure}{\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figs_final/toy/toysetup_sc2.pdf}\vspace{-0.5em}
%     \end{subfigure}
%     \begin{subfigure}{\textwidth}
%         \centering
%         \includegraphics[width=0.985\linewidth]{figs_revised/toy/plots_ood_revised/result_ood_wr.pdf}
%         \vspace{-0.1cm}
%     \end{subfigure}
%     \vspace{-0.5cm}
%     \caption{Setup (top row) and reward vs. divergence trade-off (bottom row) for Case Study I. \ourmethod{} offers highest reward at lowest divergence with much lower $N$ than BoN.}
%     \label{fig:toysetup}
%     \end{minipage}
%     \vspace{-1em}
% \end{wrapfigure}
% %
The results are illustrated in Fig.~\ref{fig:toysetup} where we plot win rate vs. KL-divergence for different values of $N \in [2, 500]$. The details for computing the KL divergence have been provided in the appendix \ref{sec:kl_comp_details}. For the guidance-based methods DPS and UG, the guidance scale is varied between \(1\) and \(50\), whereas for the sampling-based methods, BoN has \(N\) between \(2\) and \(500\), while SVDD and \ourmethod{} has \(N\) varied between \(2\) and \(40\). 

As can be seen, BoN offers the upper bound of performance with \ourmethod{} achieving on-par performance trade-offs. This aligns with the observations from the realm of LLMs \citep{Beirami2024TheoreticalPolicy,Gui2024BoNBoNSampling}, where BoN has been theoretically proven to offer the best win-rate vs KL divergence trade-offs. However, it is important to notice that \ourmethod{} achieves an on-par win-rate vs KL divergence trade-off with BoN for a much smaller $N$. Specifically, \ourmethod{} with $N \in [2,10]$ achieves the same win rate vs KL divergence performance as BoN with $N \in [30,500]$, rendering \ourmethod{} roughly $10$-$15\times$ more efficient than BoN. 

In contrast, UG and DPS tend to exhibit higher KL divergence, as they often collapse to the mode of the reward distribution when the guidance scale is increased, leading to a reduction in diversity among the sampled data points, a phenomenon also noted in prior research \citep{Sadat2024CADS:Sampling,Ho2021Classifier-FreeGuidance}. In both scenarios, SVDD achieves a high expected reward (or win rate) but at the expense of significantly higher divergence, even for \(N = 2\). In contrast, \ourmethod{} offers flexibility, allowing users to balance the trade-off by adjusting parameters such as \(N\) and \(B\), as is demonstrated here. For a different scenario (and for providing a more comprehensive picture), where the reward distribution falls within the distribution of the prior see Appendix~\ref{sec:toysc1}.

Let us dive one step deeper into comparing the performance of \ourmethod{}, \ourmethod($\eta$), BoN and SVDD-PM. To this aim, in Fig.~\ref{fig:addrestoy}, we vary the distance between the mean of the reward and prior distributions, gradually shifting the reward further away. To handle this scenario effectively, we use noise conditioning for \ourmethod, denoted by ($\eta = 1$, $0.6$), by sampling from the \emph{known} reward distribution and providing it as an input conditioning sample. We also study the impact of block size $B$ for reward-guidance by varying it between $B=[1,80,320]$, with $B=1$ corresponding to SVDD-PM. This is shown for $N = 10$, $50$ in Fig.~\ref{fig:addrestoy} where the expected reward sharply drops for BoN regardless of choice of $N$, whereas it drops less or remains almost intact for \ourmethod{} with $\eta=0.6$, $B=[80,320]$. In the case of $\eta=1$ for \ourmethod{}, we notice that the reward drops sharply for a larger block size ($B=320$), while almost remaining constant or dropping lesser for a smaller block size ($B=80$). On the other hand, SVDD-PM, imposing token-wise aggressive guidance with $B=1$ offers a high, constant reward for both $N=[10,50]$. However, SVDD-PM's generated samples have a variance that is orders of magnitude lower than BoN or \ourmethod{}, as can be seen in the right most part of Fig.~\ref{fig:addrestoy}. This particularly low variance of SVDD-PM's generated samples (almost $10^{-4}$) indicates their collapse to a single point in the reward distribution. This has been studied extensively in the literature and is referred to as reward over-optimization \citep{Prabhudesai2023AligningBackpropagation}, and corroborates the need for keeping a small KL divergence from the base model, as also empirically and theoretically argued by~\citep{beirami2024theoretical, Gao2022ScalingOveroptimization}
% The key takeaway is that our proposed approach offers a consistently higher reward even when the prior and reward distributions are distant. 
\vspace{-.1in}
\section{Case Study II: Image Generation with Stable Diffusion}
\label{sec:sdres}

\vspace{-0.1cm}
%
\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\linewidth]{figs_revised/add_figs/compress_res_v2.pdf}
    \vspace{-0.3cm}
    \caption{\ourmethod($\eta$) demonstrates a superior trade-off between compressibility, image and text alignment as compared to other baselines on the (T+I)2I settings.}
    \label{fig:compress}
    \vspace{-.1in}
\end{figure}
%

We consider \clr{four} commonly adopted guidance scenarios: \emph{compressibility}, \emph{style}, \emph{stroke}, and \emph{face} guidance. For each scenario, the reward model is task specific as elaborated in the following. A text prompt as well as a reference image are used as guidance signals. For the first three scenarios, a total of $33$ generation settings (i.e., text prompt - reference image pairs) are used for evaluations. For compressibility guidance, we have $12$ settings. Per setting, we generate $50$ samples and estimate the evaluation metrics accordingly. On the qualitative side, to demonstrate the capacity of \ourmethod($\eta$) compared to other baselines, we illustrate a few generated examples across two reference images for two different text prompts. In favor of space, the qualitative results for face guidance are deferred to the Appendix~\ref{sec:ad}, Figs.~\ref{fig:face_eta},~\ref{fig:face}. On the quantitative side, given the non-differentiable nature of compressibility as guidance signal, we demonstrate the efficacy of \ourmethod{} as compared to only sampling-based baselines in Table~\ref{tab:compress}. For differentiable reward-guidance scenarios (style, face and stroke), we evaluate the performance across all scenarios/settings combined for further statistical significance in Tables~\ref{tab:noise_abl},~\ref{tab:quant}.  

\vspace{-.1in}
\subsection{Non-Differentiable Reward: Compression}
\label{sec:nondiff}

First, we consider a scenario with non-differentiable reward, where gradient-based guidance does not apply. Following \citet{Fan2023DPOK:Models}, we use image compressibility as the reward score which is measured by the size of the JPEG image in kilobytes. 
This way, we guide the diffusion denoising process to generate memory-light, compressible images.


\begin{wraptable}{r}{0.6\textwidth}
\vspace{-1em}
\small
\caption{\small \clr{Quantitative metrics for compression reward. }}\vspace{-1em}
\label{tab:compress}
\def\arraystretch{1.15}
\resizebox{1\linewidth}{!}{
\clr{
\begin{tabular}
{l|c|c|c|c|c}
\hline
\multirow{2}{*}{\textbf{Method}} & \multicolumn{4}{c}{\textbf{Compressibility Reward - T2I}}\\ \cline{2-6}  
                                 & \scriptsize \textbf{Rew.} ($\uparrow$) & \scriptsize \textbf{FID} ($\downarrow$) & \scriptsize \textbf{CMMD} ($\downarrow$) & \scriptsize \textbf{T-CLIP} ($\uparrow$)
                                 & \scriptsize \textbf{I-Gram} ($\uparrow$)\\ \hline
Base-SD & 1.0  & 1.0 & 1.0 & 1.0 & - \\ \hline
BoN & 1.23  & 1.10 & 1.70 & 0.99 & - \\
SVDD-PM & 1.83  & 2.86 & 61.75 & 0.88 & - \\
\hline
\rowcolor{tabBlue} \textbf{\ourmethod} & 1.65 &	2.12 &	32.70 &	0.95 &	-
 \\ \hline

 \multirow{1}{*}{} & \multicolumn{4}{c}{\textbf{Compressibility Reward - (T+I)2I}}\\ \cline{2-6}  
\hline
Base-SD & 1.0  & 1.0 & 1.0 & 1.0 & 1.0 \\ 
SDEdit ($\eta=0.8$) & 0.97 & 2.19 &	29.25 &	0.98 &	1.34
 \\ \hline
BoN ($\eta=0.8$) & 1.08  & 2.27 & 31 & 0.98 & 1.32 \\
SVDD-PM ($\eta=0.8$) & 1.48  & 3.54 & 69.5 & 0.89 & 1.15\\ 
\hline
\rowcolor{tabBlue} \textbf{\ourmethod($\eta=0.8$)} & 1.34 &	3.08 &	48.75 &	0.97 &	1.20
 \\ \hline
\end{tabular}
}}
\vspace{-12pt}
\end{wraptable}
%
\textbf{Qualitative Comparisons.} 
A comparative look across baselines and settings is illustrated in Fig.~\ref{fig:compress}. We observe that \ourmethod($\eta$) generates the best results, offering superior compression as well as image and text alignment. SVDD-PM($\eta$), SDEdit($\eta$) and BoN($\eta$) align well with the image and text prompt, but fall short on providing smooth-textured, content-light compressed images. However, this is to be expected in the case of SDEdit($\eta$) since its generative process is not guided by the compression-reward. 


%
\begin{wrapfigure}[20]{r}{0.5\textwidth}
    \vspace{-.18in}
    \begin{minipage}{0.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figs_revised/plots_revised_add/rew-vs-kl_compress_rhack.pdf}
    \vspace{-0.7cm}
    \caption{\ourmethod($\eta$) offers a better reward vs. KL-divergence trade-off compared to BoN($\eta$) for all N. SVDD-PM($\eta$) demonstrates higher reward beyond $N=7$, but at the cost of a much higher KL-divergence.}
    % \vspace{-1cm}
    \label{fig: comp_vs_kl}
    \end{minipage}
\end{wrapfigure}
%
\textbf{Quantitative Evaluations.} 
Table \ref{tab:compress} illustrates the performance comparison of \ourmethod, \ourmethod($\eta$)  as compared to other baselines.
Sampling-based baselines (SVDD-PM \cite{Li2024Derivative-FreeDecoding} and BoN \cite{Gao2022ScalingOveroptimization}) for two scenarios, T2I and (T+I)2I, where in the latter the reference image is omitted. In both scenarios, we observe that SVDD-PM and SVDD-PM($\eta$) offer slightly higher compression reward score as compared to other baselines; however, \ourmethod($\eta$) offers better image (I-Gram) and text (T-CLIP) alignment and the least divergence from the base distribution (FID, CMMD) as compared to all other baselines. Most notably, Fig.~\ref{fig: comp_vs_kl} illustrates the reward vs. KL divergence for this scenario, demonstrating that in normal operating regimes (before reward over-optimization occurs, see appendix~\ref{sec:rhack},~\ref{fig:svddpmrh}), \ourmethod($\eta$) offers almost the same reward as its special case of $B=1$ for SVDD-PM($\eta$) with less than half of its KL divergence. Here, different points on the curves represent sweeping on each method's main set of parameters ($N = [10,20,30,40,100]$ for \ourmethod($\eta$), BoN($\eta$) and $N = [2,3,5,7,10,20,30,40,100]$ for SVDD-PM($\eta$)). See appendix section \ref{sec:kl_comp_details} for details on the computation of KL divergence.

\vspace{-.1in}
\subsection{Differentiable Rewards}
\label{sec:diff}

\textbf{Style guidance.} We guide image generation based on a reference style image \citep{Bansal2024UniversalModels,he2024manifold,yu2023freedom}. Following the reward model proposed in \cite{Bansal2024UniversalModels}, we use the CLIP image encoder to obtain embeddings for the reference style and the generated images. The cosine similarity between these embeddings is then used as the guidance signal. \textbf{Stroke guidance.} A closely related scenario to style guidance is stroke generation, where a high-level reference image containing only coarse colored strokes is used as reference  \citep{Cheng_2023,Meng2021SDEdit:Equations}. The objective in this setting is to produce images that remain \emph{faithful} to the reference strokes. To achieve this, similar to style guidance, we employ the CLIP image encoder to obtain embeddings from both the reference and generated images and compute the reward by measuring the cosine similarity between these embeddings. \textbf{Face guidance.} To guide the generation process to capture the face of a specific individual (as in \citep{he2024manifold,Bansal2024UniversalModels}), we employ a combination of multi-task cascaded convolutional network (MTCNN) \citep{zhang2016joint} for face detection and FaceNet \citep{schroff2015facenet} for facial recognition, which together produce embeddings for the facial attributes of the image. The reward is then computed as the negative \(\ell_1\) loss between feature embeddings of the reference and generated images.

\textbf{Qualitative Comparisons.} 
A comparative look across baselines, scenarios and settings is illustrated in in Figs.~\ref{fig:style_eta} and ~\ref{fig:stroke_eta} (and ~\ref{fig:style}~\ref{fig:face_eta},~\ref{fig:face} and~\ref{fig:stroke} in the appendix). Let us start with style guidance in Fig.~\ref{fig:style_eta}. As can be seen, \ourmethod($\eta$) shows arguably a better performance in capturing the style of the reference image, regardless of the text prompt. When it comes to alignment with the text prompt, UG($\eta$) seems to suffer to some extent with ``Eiffel tower'' and ``woman'' fading away in the corresponding images. \textbf{Important Remark:} Note that by excluding noise conditioning from the original baselines (removing $\eta$, see Figs.~\ref{fig:style},~\ref{fig:style2}), they all suffer in capturing the style of the reference image, highlighting the importance of using noise-conditioning as is proposed for \ourmethod{} for all baselines operating in the (T+I)2I scenarios. Further qualitative results for stroke guidance are summarized in Fig.~\ref{fig:stroke_eta}, where the same narrative and observations extend. The results for face guidance are deferred to the Appendix~\ref{sec:ad}, Figs.~\ref{fig:face_eta} and~\ref{fig:face}.  

\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\linewidth]{figs_revised/add_figs/style_res_eta.pdf}
    \vspace{-0.3cm}
    \caption{The style alignment offered by \ourmethod($\eta$) stands on par or outperforms other baselines in terms of quality and preserving nuances of the reference image, while adhering to the text-prompt.}
    \label{fig:style_eta}
    \vspace{-.2in}
\end{figure}
%

%
\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\linewidth]{figs_revised/add_figs/stroke_res_eta.pdf}
    \vspace{-0.3cm}
    \caption{Same narrative as in Fig.~\ref{fig:style_eta} with \ourmethod{} outperforming UG($\eta$) in terms of quality and ref. image-alignment, while standing-on par with all other baselines.}
    \label{fig:stroke_eta}
    \vspace{-.1in}
\end{figure}
%
% 
\begin{wraptable}{r}{0.6\textwidth}
\vspace{-1em}
\caption{\small Quantitative performance evaluation ($\pm~ \textup{std.}$).}\vspace{-0.5em}
\label{tab:noise_abl}
\def\arraystretch{1.2}
\resizebox{1\linewidth}{!}{
\small
\clr{
\begin{tabular}
{l|c|c|c|c}
\hline
% \multirow{2}{*}{\textbf{Method}} & \multicolumn{4}{c}{\textbf{R1: All Guidance}}\\ \cline{2-6}  
\textbf{Method} & \textbf{FID} ($\downarrow$) & \textbf{I-Gram} ($\uparrow$) & \textbf{T-CLIP} ($\uparrow$) &  \textbf{Runtime} ($\downarrow$) \\  \hline
SDEdit($\eta$) &  1.0 & 1.0 & 1.0 &  1.0 \\ \hline
% BoN  &  0.45 & 0.44 {\scriptsize ($\pm$ 0.004)} & 1.11 {\scriptsize ($\pm$ 0.001)} &  29.53 {\scriptsize ($\pm$ 0.01)}\\
BoN($\eta$)  &  1.06 & 0.88 {\scriptsize ($\pm$ 0.002)} & 0.98 {\scriptsize ($\pm$ 0.002)} &  23.62 {\scriptsize ($\pm$ 0.005)}\\ 
% SVDD-PM &  0.544 & 0.51 {\scriptsize ($\pm$ 0.02)} & 1.10 {\scriptsize ($\pm$ 0.004)} &  154.84 {\scriptsize ($\pm$ 0.08)} \\ 
SVDD-PM ($\eta$) &  1.29 & 1.33 {\scriptsize ($\pm$ 0.03)} & 0.94 {\scriptsize ($\pm$ 0.002)} &  103.73 {\scriptsize ($\pm$ 0.05)} \\ \hline
% DPS &  0.43 & 0.46 {\scriptsize ($\pm$ 0.01)} & 1.10 {\scriptsize ($\pm$ 0.004)} &  9.09 {\scriptsize ($\pm$ 0.02)}\\
DPS ($\eta$) &  1.01 & 0.96 {\scriptsize ($\pm$ 0.04)} & 0.96 {\scriptsize ($\pm$ 0.005)} &  6.07 {\scriptsize ($\pm$ 0.03)}\\ 
% UG  &  1.11 & 0.76 {\scriptsize ($\pm$ 0.03)} & 0.95 {\scriptsize ($\pm$ 0.005)} &  137.37 {\scriptsize ($\pm$ 
% 0.03)}\\
UG($\eta$) &  1.38 & 0.76 {\scriptsize ($\pm$ 0.05)} & 0.89 {\scriptsize ($\pm$ 0.002)} &  92.07 {\scriptsize ($\pm$ 0.04)}\\ \hline
% \rowcolor{tabBlue} \textbf{\ourmethod{} ($\eta=1$)} &  0.45 & 0.53 {\scriptsize ($\pm$ 0.009)} & 1.11 {\scriptsize ($\pm$ 0.001)} &  54.10 {\scriptsize ($\pm$ 0.04)}\\
\rowcolor{tabBlue} \textbf{\ourmethod($\eta$)} &  1.15 & 1.32 {\scriptsize ($\pm$ 0.05)} & 0.98 {\scriptsize ($\pm$ 0.006)} &  37.21 {\scriptsize ($\pm$ 0.03)}\\ \hline
\end{tabular}
}}
\vspace{-0.8em}
\end{wraptable}
%
\textbf{Quantitative Evaluations.}
Table~\ref{tab:noise_abl} summarizes the performance across all scenarios (including all settings) over four metrics: I-Gram, FID, T-CLIP and runtime (in second/image, and detailed Section~\ref{sec:compcomplex}). The reason why we use I-Gram (instead of expected reward per scenario) in our evaluations is because expected reward has been \textit{seen} by the model throughout the guidance process. The scores here are normalized with respect to SDEdit as the baseline, thus indicating the performance gain over that. We notice that SVDD-PM($\eta$) and \ourmethod($\eta$) perform on par in terms of offering the best image alignment (indicated by I-Gram), while being superior than all other baselines. However, \ourmethod($\eta$) offers a better trade-off between image, text-alignment and divergence as compared to SVDD-PM($\eta$), as indicated by its superior T-CLIP and FID scores. Note that here again by excluding noise-conditioning from the other baselines (as in their original proposition) the gain margin offered by \ourmethod($\eta$) would be considerably larger as is shown in our ablation studies. See Appendix~\ref{sec:ad} for further qualitative and quantitative results. 
\vspace{-0.1cm}
\subsection{Ablations} 
\label{sec:ablations}
\vspace{-0.1cm}
%
\begin{figure}
    \centering
    \begin{subfigure}{0.47\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs_revised/gram-vs-clipscore_styletransfer_b_annot.pdf}
    \end{subfigure}
    \begin{subfigure}{0.47\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs_revised/gram-vs-clipscore_styletransfer_r_abl.pdf}
    \end{subfigure}
    \vspace{-0.3cm}
    \caption{Ablation on the block size ($B$) and the noise ratio ($\eta$).}
    \label{fig: ablation}
    \vspace{-.15in}
    \end{figure}
%
% % 
% \begin{wraptable}{r}{0.6\textwidth}
% \vspace{-1em}
% \caption{\small Ablation on partial-noise conditioning.}\vspace{-0.5em}
% \label{tab:quant}
% \def\arraystretch{1.2}
% \resizebox{1\linewidth}{!}{
% \small
% \clr{
% \begin{tabular}
% {l|c|c|c|c}
% \hline
% % \multirow{2}{*}{\textbf{Method}} & \multicolumn{4}{c}{\textbf{R1: All Guidance}}\\ \cline{2-6}  
% \textbf{Method} & \textbf{FID} ($\downarrow$) & \textbf{I-Gram} ($\uparrow$) & \textbf{T-CLIP} ($\uparrow$) &  \textbf{Runtime} ($\downarrow$) \\  \hline
% Base-SD & 1.0 & 1.0  & 1.0  & 1.0  \\ \cdashline{1-5}
% SDEdit &  2.61 & 2.42 {\scriptsize ($\pm$ 0.02)} & 0.89 {\scriptsize ($\pm$ 0.003)} &  0.64 {\scriptsize ($\pm$ 0.01)}\\ \hline
% BoN  &  1.19 & 1.07 {\scriptsize ($\pm$ 0.004)} & 0.99 {\scriptsize ($\pm$ 0.001)} &  18.90 {\scriptsize ($\pm$ 0.01)}\\
% BoN($\eta$)  &  2.77 & 2.14 {\scriptsize ($\pm$ 0.002)} & 0.88 {\scriptsize ($\pm$ 0.002)} &  15.12 {\scriptsize ($\pm$ 0.005)}\\ \hline
% SVDD-PM &  1.42 & 1.24 {\scriptsize ($\pm$ 0.02)} & 0.98 {\scriptsize ($\pm$ 0.004)} &  99.10 {\scriptsize ($\pm$ 0.08)} \\ 
% SVDD-PM ($\eta$) &  3.37 & 3.22 {\scriptsize ($\pm$ 0.03)} & 0.84 {\scriptsize ($\pm$ 0.002)} &  66.39 {\scriptsize ($\pm$ 0.05)} \\ \hline
% DPS &  1.14 & 1.12 {\scriptsize ($\pm$ 0.01)} & 0.98 {\scriptsize ($\pm$ 0.004)} &  5.82 {\scriptsize ($\pm$ 0.02)}\\
% DPS ($\eta$) &  2.61 & 2.33 {\scriptsize ($\pm$ 0.04)} & 0.86 {\scriptsize ($\pm$ 0.005)} &  3.89 {\scriptsize ($\pm$ 0.03)}\\ \hline
% UG  &  2.91 & 1.86 {\scriptsize ($\pm$ 0.03)} & 0.85 {\scriptsize ($\pm$ 0.005)} &  87.92 {\scriptsize ($\pm$ 
% 0.03)}\\
% UG($\eta$) &  3.60 & 1.85 {\scriptsize ($\pm$ 0.05)} & 0.80 {\scriptsize ($\pm$ 0.002)} &  58.93 {\scriptsize ($\pm$ 0.04)}\\ \hline
% \rowcolor{tabBlue} \textbf{\ourmethod{} ($\eta=1$)} &  1.17 & 1.30 {\scriptsize ($\pm$ 0.009)} & 0.99 {\scriptsize ($\pm$ 0.001)} &  34.63 {\scriptsize ($\pm$ 0.04)}\\
% \rowcolor{tabBlue} \textbf{\ourmethod{}} &  3.00 & 3.19 {\scriptsize ($\pm$ 0.05)} & 0.87 {\scriptsize ($\pm$ 0.006)} &  23.82 {\scriptsize ($\pm$ 0.03)}\\ \hline
% \end{tabular}
% }}
% \vspace{-0.8em}
% \end{wraptable}
% %
Fig.~\ref{fig: ablation} investigates the impact of varying block size ($B$) and noise ratio ($\eta$) for \ourmethod{} on image (I-Gram) vs. text alignment (T-CLIP). For reference, \ourmethod{}($\eta=1$) (without image-conditioning) and UG are also depicted. Here, different points per curve represent sweeping on their main parameter ($N = [5, 10, 20, 30, 40, 100]$ for \ourmethod{}, and guidance scale of $[1, 3, 6, 12, 24]$ for UG). On the left image, increasing block size seems to limit the image alignment performance; or put differently, same performance at a much larger $N$. Regardless of block size, \ourmethod{} curves fall on top of UG indicating a superior overall performance. On the right, changing the noise ratio $\eta$ toward higher values, reduces the conditioning strength (as indicated also in \citep{Meng2021SDEdit:Equations,koohpayegani2023genie}) resulting in lower image alignment capacity (I-Gram).
% 
\begin{wraptable}{r}{0.6\textwidth}
\vspace{-0.5em}
\caption{\small Ablation on partial-noise conditioning.}\vspace{-0.5em}
\label{tab:quant}
\def\arraystretch{1.2}
\resizebox{1\linewidth}{!}{
\small
\begin{tabular}
{l|c|c|c|c}
\hline
% \multirow{2}{*}{\textbf{Method}} & \multicolumn{4}{c}{\textbf{R1: All Guidance}}\\ \cline{2-6}  
\textbf{Method} & \textbf{FID} ($\downarrow$) & \textbf{I-Gram} ($\uparrow$) & \textbf{T-CLIP} ($\uparrow$) &  \textbf{Runtime} ($\downarrow$) \\  \hline
Base-SD (\citeyear{Rombach2021High-ResolutionModels}) & 1.0 & 1.0  & 1.0  & 1.0  \\ \hline
% SDEdit (\citeyear{Meng2021SDEdit:Equations}) &  2.61 & 2.42 {\scriptsize ($\pm$ 0.02)} & 0.89 {\scriptsize ($\pm$ 0.003)} &  0.64 {\scriptsize ($\pm$ 0.01)}\\ \hline
BoN (\citeyear{Gao2022ScalingOveroptimization}) &  1.19 & 1.07 {\scriptsize ($\pm$ 0.004)} & 0.99 {\scriptsize ($\pm$ 0.001)} &  18.90 {\scriptsize ($\pm$ 0.01)}\\
% C-BoN (\citeyear{Gao2022ScalingOveroptimization}) &  2.77 & 2.14 {\scriptsize ($\pm$ 0.002)} & 0.88 {\scriptsize ($\pm$ 0.002)} &  15.12 {\scriptsize ($\pm$ 0.005)}\\
SVDD-PM (\citeyear{Li2024Derivative-FreeDecoding}) &  1.42 & 1.24 {\scriptsize ($\pm$ 0.02)} & 0.98 {\scriptsize ($\pm$ 0.004)} &  99.10 {\scriptsize ($\pm$ 0.08)} \\ \cdashline{1-5}
DPS (\citeyear{Chung2023DiffusionProblems}) &  1.14 & 1.12 {\scriptsize ($\pm$ 0.01)} & 0.98 {\scriptsize ($\pm$ 0.004)} &  5.82 {\scriptsize ($\pm$ 0.02)}\\
% C-UG &  3.60 & 1.85 {\scriptsize ($\pm$ 0.05)} & 0.80 {\scriptsize ($\pm$ 0.002)} &  58.93 {\scriptsize ($\pm$ 0.04)}\\
UG (\citeyear{Bansal2024UniversalModels}) &  2.91 & 1.86 {\scriptsize ($\pm$ 0.03)} & 0.85 {\scriptsize ($\pm$ 0.005)} &  87.92 {\scriptsize ($\pm$ 0.03)}\\ \hline
\rowcolor{tabBlue} \textbf{\ourmethod} &  1.17 & 1.30 {\scriptsize ($\pm$ 0.009)} & 0.99 {\scriptsize ($\pm$ 0.001)} &  34.63 {\scriptsize ($\pm$ 0.04)}\\
\rowcolor{tabBlue} \textbf{\ourmethod($\eta$)} &  3.00 & 3.19 {\scriptsize ($\pm$ 0.05)} & 0.87 {\scriptsize ($\pm$ 0.006)} &  23.82 {\scriptsize ($\pm$ 0.03)}\\ \hline
\end{tabular}
}
\vspace{-0.8em}
\end{wraptable}
%
Yet again, \ourmethod{} variants fall on top of the UG curve suggesting better image vs. text alignment performance. More detailed ablation studies and reward vs alignment trade-off curves are provided in Appendix~\ref{sec:ad}. Further note that the operation points with very low T-CLIP scores on UG curves ended up degenerating to the extent that images did not have anything in common with the text prompt (see appendix \ref{sec:ugrh}, Fig.~\ref{fig:ug_rh}), which was another consideration for choosing the best trade-off point.

We also study the impact of dropping the partial-noise conditioning on all baselines, including \ourmethod{} in Table \ref{tab:quant}. For reference, \ourmethod($\eta$) is also included where the best empirical value for $\eta$ is selected per scenario. We report scores across all metrics by normalizing them w.r.t. the base Stable Diffusion model (denoted by Base-SD). As can be seen, \ourmethod, i.e. without noise-conditioning, offers performance gains in terms of image alignment while staying competitive w.r.t. text alignment (I-Gram and T-CLIP scores) and deviating lesser from the base model (FID score), compared to all baselines except UG. Notably, \ourmethod is also considerably faster than both SVDD-PM and UG in terms of runtime. As stated earlier, here \ourmethod($\eta$), i.e. with noise-conditioning, offers a much more pronounced gain in terms of I-Gram in terms of the other baselines.

\vspace{-.05in}
\subsection{Computational Complexity.} 
\vspace{-0.1cm}
\label{sec:compcomplex}
We provide a comparative look at the complexity of the proposed approach against other baselines. 
%
\begin{wraptable}{r}{0.5\textwidth}  
\vspace{-1em}
\caption{Computational complexity.}
\label{tab:complexity}
  \vspace{-0.5em}
  \centering
  % \aboverulesep = 0pt 
  % \belowrulesep = 0pt
  \renewcommand{\arraystretch}{1.2}
  % \scriptsize
  \resizebox{1\linewidth}{!}{
  \large
  \begin{tabular}{l|c|c|c}
    \hline
    \textbf{Methods} & \textbf{Inf. Steps} & \textbf{Rew. Queries} & \textbf{Runtime [sec/img]} \\
    \hline
    Base-SD (\citeyear{Rombach2021High-ResolutionModels}) & \(T\) & - & 14.12 \\ \hline
    BoN (\citeyear{Gao2022ScalingOveroptimization}) & $NT$ & \(N\) & 266.77 \\
    SVDD-PM (\citeyear{Li2024Derivative-FreeDecoding}) & $NT$ & \(NT\) & 1399.36\\
    \cdashline{1-4}
    DPS (\citeyear{Chung2023DiffusionProblems}) & $T$ & \(T\) & 82.19 \\
    UG (\citeyear{Bansal2024UniversalModels}) & $mKT$ & \(mKT\) & 1241.47\\
     \hline
    \rowcolor{tabBlue} \textbf{\ourmethod} & \(NT\) & \(NT/B\) & 489.00\\ \cdashline{1-4}
     \rowcolor{tabBlue} \textbf{\ourmethod{}($\eta$)} & \(N\eta \,T\) & \(N\eta\,T/B\) & 336.39\\
    \hline
  \end{tabular}
  }
  \vspace{-1em}
\end{wraptable}
%
To this aim, we consider two aspects: (i) the number of inference steps, (ii) the number of queries to the reward model. We then measure the overall runtime complexity in terms of time (in sec.) required to generate one image. This is summarized in Table~\ref{tab:complexity}. From a runtime perspective, within the gradient-based guidance group, DPS is relatively faster across all three generation scenarios. This is due to the \(m\) gradient and \(K\) refinement steps used in UG, which are not used in DPS. 
Within the sampling based group, SVDD-PM, imposing token-wise aggressive guidance ($B=1$), turns out to be an order of magnitude slower than BoN. \ourmethod, \ourmethod($\eta$) with its blockwise guidance remains to be faster and more efficient than BoN as well as UG, offering a $4\times$ faster runtime than UG. 

\section{Related Work}
\vspace{-0.2cm}
\label{sec:lit}

% Previous research on the alignment of diffusion models to downstream reward functions can be broadly categorized into two approaches: (i) learning-based and (ii) guidance-based methods.

\textbf{Finetuning-based alignment.} Prominent methods in this category typically involve either training a diffusion model to incorporate additional inputs such as category labels, segmentation maps, or reference images \citep{Ho2021Classifier-FreeGuidance,li2023gligen,Zhang2023AddingModels,bansal2024cold,mou2024t2i,Ruiz_2023} or applying reinforcement learning (RL) to finetune a pretrained diffusion model to optimize for a downstream reward function \citep{Prabhudesai2023AligningBackpropagation,Fan2023DPOK:Models,Wallace2023DiffusionOptimization,Black2023TrainingLearning,gu2024diffusionrpo,lee2024direct,uehara2024understanding}. While these approaches have been successfully employed to satisfy diverse constraints, they are computationally expensive. Furthermore, finetuning diffusion models is prone to ``reward hacking'' or ``over-optimization'' \citep{clark2024directly,jena2024elucidating}, where the model loses diversity and collapses to generate samples that achieve very high rewards. This is often due to a mismatch between the intended behavior and what the reward model actually captures. In practice, a perfect reward model is extremely difficult to design. As such, here we focus on inference-time guidance-based alignment approaches where these issues can be circumvented.  \clr{Additionally, none of the fine-tuning based methods are built for image-to-image scenarios, which is the focus of this work, as we clarified earlier. To compare against them, a direct approach could be fine-tuning per reference image, which renders the process computationally infeasible, or taking a meta-learning approach to fine-tuning. However, such fundamental adjustments are beyond the current scope of our work.}

\textbf{Gradient-based inference-time alignment.} 
% Another prominent category keeps the pretrained foundation diffusion model intact and guides the denoising process through a reward (or value) function \cite{}. 
There are two main divides within this category: (i) guidance based on a \emph{value} function, and (ii) guidance based on a downstream \emph{reward} function. In the first divide, a value function is trained offline using the noisy intermediate samples from the diffusion model. Then, during inference, gradients from the value function serve as signals to guide the generation process \citep{Dhariwal2021DiffusionSynthesis,Yuan2023Reward-DirectedImprovement}. A key limitation of such an approach is that the value functions are specific to the reward model and the noise scales used in the pretraining stage. Thus, the value function has to be retrained for different reward and base diffusion models. The second divide of methods successfully overcomes this by directly using the gradients of the reward function based on the approximation of fully denoised images using Tweedie's formula \citep{chung2022improving, Chung2023DiffusionProblems,yu2023freedom}. Interesting follow-up research has explored methods to reduce estimation bias \citep{zhu2023denoising,Bansal2024UniversalModels,he2024manifold} and to scale gradients for maintaining the latent structures learned by diffusion models \citep{Guo2024GradientPerspective}. Despite such advancements, the need for differentiable guidance functions can limit the broader applicability of the gradient-based methods.

\textbf{Gradient-free inference-time alignment.} Tree-search alignment has recently gained attention in the context of autoregressive language models (LMs), where it has been demonstrated that Best-of-$N$ (BoN) approximates sampling from a KL-regularized objective, similar to those used in reinforcement learning (RL)-based finetuning methods \citep{Gui2024BoNBoNSampling,Beirami2024TheoreticalPolicy,Gao2022ScalingOveroptimization}. This approach facilitates the generation of high-reward samples while maintaining closeness to the base model. \citet{Mudgal2024ControlledModels} demonstrate that the gap between Best-of-$N$ (BoN) and token-wise {\em value-based} decoding \citep{Yang2021FUDGE:Discriminators} can be bridged using a blockwise decoding strategy. Inspired by this line of research, we propose a simple blockwise alignment technique (tree search with a fixed depth) that offers key advantages: (i) it preserves latent structures learned by diffusion models without requiring explicit scaling adjustments, unlike gradient-based methods, and (ii) it avoids ``reward hacking'' typically associated with learning-based approaches. Concurrently, \citet{Li2024Derivative-FreeDecoding} propose a related method, called SVDD-PM, based on the well-known token-wise decoding strategy in the LM space. In contrast, we devise a blockwise sampling strategy because it allows further control on the level of intervention, and offers a trade-off between divergence and alignment, which is of primal interest in the context of guided generation. To enhance the sampling strategy in terms of efficiency, we apply adjustable noise-conditioning which also offers greater control over guidance signals and further improves alignment. \clr{Sequential Monte Carlo-based methods (SMC) for diffusion models \citep{Wu2023PracticalModels, Chung2023DiffusionProblems, Phillips2024ParticleSampler, Cardoso2023MonteProblems} share similarities with tree-search-based alignment methods such as ours, particularly in not requiring differentiable reward models. They involve resampling across an entire batch of images, which can lead to suboptimal performance when batch sizes are small since the SMC theoretical guarantees hold primarily with large batch sizes. In contrast, our method performs sampling on a per-sample basis. Lastly, using SMC for reward maximization can also result in a loss of diversity, even with large batch sizes.}


\vspace{-.1in}
\section{Concluding Remarks}
\label{sec:conc}
\vspace{-0.2cm}

We introduce a gradient-free blockwise inference-time guidance approach for diffusion models. By combining blockwise optimal sampling with an adjustable noise conditioning strategy, \ourmethod, \ourmethod($\eta$) offer a better reward vs. divergence trade-off compared to state-of-the-art baselines. 

\textbf{Limitations and future work.}  Diffusion models are still computationally intensive; as such, extracting quantitative results on the performance of (inference-time) guidance-based alignment methods calls for massive resources, especially when ablating across numerous design parameters. We have used up to $32$ NVIDIA A100's solely dedicated to the presented evaluation results. Yet, most commonly adopted settings we have experimented with to arrive at the numerical results in Tables~\ref{tab:compress} and~\ref{tab:quant} can be further expanded for the sake of better statistical significance in future work.

\textbf{Broader Impact.} We would like to caution against the blind usage of the proposed techniques as alignment methods are prone to reward over-optimization, which warrants care in socially consequential applications.

%\newpage

% \subsubsection*{Author Contributions}
% If you'd like to, you may include  a section for author contributions as is done
% in many journals. This is optional and at the discretion of the authors.

\subsubsection*{Acknowledgments}
AS and HJR would like to thank \href{https://www.shell.com/what-we-do/digitalisation/artificial-intelligence.html#vanity-aHR0cHM6Ly93d3cuc2hlbGwuY29tL2VuZXJneS1hbmQtaW5ub3ZhdGlvbi9kaWdpdGFsaXNhdGlvbi9zaGVsbC1haS5odG1s}{Shell.ai} Innovation Program for supporting this work.
% Use unnumbered third level headings for the acknowledgments. All
% acknowledgments, including those to funding agencies, go at the end of the paper.

%\clearpage

% \textbf{Reproducibility Statement.} To help readers reproduce our experiments, we provide extensive descriptions of implementation details and algorithms. Architectural and training details are provided in Appendices~\ref{sec:appendix-arch} and~\ref{sec:appendix-train}, respectively, along with information on the applied data augmentations (Appendix~\ref{sec:appendix-augm}) and tested benchmark datasets (Appendix~\ref{sec:appendix-datasets}). The algorithms for \ourmethod{} along with the $\texttt{PML}$ and the $\texttt{CGL}$ modules are provided in both algorithmic (in Algorithms~\ref{algo:mapl}, \ref{algo:mapl_gl}, \ref{algo:mapl_overall}) and Pytorch-like pseudocode formats (in Algorithms~\ref{alg:maplclient_pytorch}, \ref{alg:mapl_pytorch}, \ref{alg:pml_pytorch}, \ref{alg:cgl_pytorch}). We have taken every measure to ensure fairness in our comparisons by following the most commonly adopted evaluation settings in the Federated Learning literature in terms of data heterogeneity. We also refer to the publicly available code base, if available, for comparing their performance with \ourmethod{}.

% \textbf{Ethics Statement.} We have read the \href{https://neurips.cc/Conferences/2024/CodeOfConduct}{NeurIPS Code of Conduct} and ensured that this work adheres to it. All benchmark datasets are publicly available and not directly subject to ethical concerns. We limited our study to $M=20$ clients due to the limitation of resources necessary for running multiple instances of large models such as ResNet$18$. Lastly, we are aware of the work Learning to Collaborate (L2C) \citep{Li2022LearningModels} for personalization in a P2P setting. We did not include it in this paper as there is no publicly available code, and we could not reliably reproduce the results as reported in the paper. Further, it does not support heterogeneous backbones, which is our main focus.

\bibliography{references_updated, biblio}
\bibliographystyle{tmlr}

\clearpage

\appendix

% \section{Denoising Diffusion Probabilistic Models}

% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=\textwidth]{figures_appendix/ddpm.pdf}
%     \caption{DDPM. {Source: \citep{Ho2020DenoisingModels}}}
%     \label{fig:enter-label}
% \end{figure}

% A brief overview of the process:
% \begin{itemize}
%     \item Slowly destroys structure in the ``forward'' process or ``diffusion'' process $q(x_t|x_{t-1})$.
%     \item Learns a ``reverse'' process $p_\theta(x_{t-1}|x_t)$ that restores the structure in the data.
% \end{itemize}

% \begin{tcolorbox}[title=Convention for indices t, boxrule=0pt]
%     The \red{original image} is $x_0$, and the \red{complete noise} is $x_T$. A \red{smaller t} indicates \red{less noise}.
% \end{tcolorbox}

% \subsection{Forward Diffusion Process}

% Given a data point sampled from the real distribution $x_0 \sim q(x)$, we define a forward process. It is a Markov chain with a small amount of Gaussian noise added to the sample in $T$ steps, producing noisy samples $x_1, \cdots, x_T$. The step size is controlled by a variance schedule $\{\beta_t \in (0,1)\}_{t=1}^T$. The forward process can thus be expressed as:
% %
% \begin{equation}
%     q(x_{1:T}|x_0) = \prod_{t=1}^T q(x_t|x_{t-1}),\quad q(x_t|x_{t-1}) = \mathcal{N}(x_t; \sqrt{1-\beta_t}x_{t-1}, \beta_t\mathbf{I}).
% \end{equation}
% %
% The data sample $x_0$ thus loses its distinguishable features as step $t$ becomes larger, leading to $x_T \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$ as $T\rightarrow\infty$.

% \subsubsection*{I. Closed form for the forward process} 

% We can sample $x_t$ at any arbitrary time step $t$ in a closed form using the reparameterization trick. Let $\alpha_t = 1 - \beta_t$, $\Bar{\alpha}_t = \prod_{t=1}^T \alpha_t$, and $\varepsilon, \varepsilon_{t-1}, \varepsilon_{t-2}, \cdots \overset{i.i.d}{\sim} \mathcal{N}(\mathbf{0}, \mathbf{I})$. Using reparameterization repeatedly:
% %
% \begin{equation*}
%     \begin{split}
%         x_t &= \red{\sqrt{\alpha_t}x_{t-1} + \sqrt{1 - \alpha_t}\varepsilon_{t-1}}\\
%         &= \sqrt{\alpha_t \alpha_{t-1}}x_{t-2} + \sqrt{1 - \alpha_t \alpha_{t-1}}\varepsilon_{t-2}\\
%         &= \cdots\\
%         &= \sqrt{\Bar{\alpha}_t}x_{0} + \sqrt{1 - \Bar{\alpha}_t}\varepsilon,
%     \end{split}
% \end{equation*}
% %
% where we rely on the fact that:
% %
% \begin{equation*}
%     \begin{split}
%         x_{t-1} &\sim \mathcal{N}(\sqrt{\alpha_{t-1}}x_{t-2}, (1 - \alpha_{t-1})\mathbf{I})\\
%         \sqrt{\alpha_t}x_{t-1} &\sim \mathcal{N}(\sqrt{\alpha_t\alpha_{t-1}}x_{t-2}, \alpha_t(1 - \alpha_{t-1})\mathbf{I})\\
%         \red{\sqrt{\alpha_t}x_{t-1} + \sqrt{1 - \alpha_t}\varepsilon_{t-1}} &\sim \mathcal{N}(\sqrt{\alpha_t\alpha_{t-1}}x_{t-2}, ((1-\alpha_t) + \alpha_t(1 - \alpha_{t-1}))\mathbf{I})\\
%         &\sim \mathcal{N}(\sqrt{\alpha_t\alpha_{t-1}}x_{t-2}, (1-\alpha_t\alpha_{t-1})\mathbf{I}).
%     \end{split}
% \end{equation*}
% %
% Thus, the closed-form forward process is:
% %
% \begin{equation}
%     \label{eq:forward_cf}
%     \begin{split}
%         q(x_t|x_0) &= \mathcal{N}(x_t; \sqrt{\Bar{\alpha}_t}x_{0}, (1 - \Bar{\alpha}_t)\mathbf{I}),\\
%         \text{with,}~x_t &= \sqrt{\Bar{\alpha}_t}x_{0} + \sqrt{1 - \Bar{\alpha}_t}\varepsilon_t.
%     \end{split}
% \end{equation}


% \subsubsection*{II. Closed form for the reversed conditional probability} 

% The reversed conditional probability $q(x_{t-1}|x_t, x_0)$ can also be obtained in close-form. We derive it here, and it will be later useful for the derivation of the training objective. Using Bayes' rule, we have

% \begin{equation*}
%     \begin{split}
%         q(x_{t-1}|x_t, x_0) &= q(x_t|x_{t-1}, x_0) \frac{q(x_{t-1}|x_0)}{q(x_t|x_0)}\\
%         &= q(x_t|x_{t-1}) \frac{q(x_{t-1}|x_0)}{q(x_t|x_0)}\\
%         &\propto \exp \left[ - \frac{1}{2} \left( \frac{\|x_t - \sqrt{\alpha_t}x_{t-1}\|^2_2}{\beta_t} + \frac{\|x_{t-1} - \sqrt{\Bar{\alpha}_{t-1}}x_0\|^2_2}{1 - \Bar{\alpha}_{t-1}} - \frac{\|x_t - \sqrt{\Bar{\alpha}_t}x_0\|^2_2}{1 - \Bar{\alpha}_t} \right) \right]\\
%         &\propto \exp \left[ - \frac{1}{2} \frac{\|x_{t-1} - \red{\Tilde{\mu}(x_t, x_0)}\|}{\blue{\Tilde{\beta}_t}} \right]\\
%         &\propto \mathcal{N}(x_{t-1}; \red{\Tilde{\mu}(x_t, x_0)}, \blue{\Tilde{\beta}_t} \mathbf{I}).
%     \end{split}
% \end{equation*}

% To get the values of $\red{\Tilde{\mu}(x_t, x_0)}$ and $\blue{\Tilde{\beta}_t}$, we will use the technique of \textbf{completing the square}.

% \begin{tcolorbox}[title=Completing the square, boxrule=0pt]
%     Given a quadratic function $ax^2 + bx + c$, we can rewrite it as $a(x+d)^2 + e$ where $d = \frac{b}{2a}$ and $e = c - \frac{b^2}{4a}$. 
% \end{tcolorbox}

% We start by making a polynomial in $x_{t-1}$:

% \begin{equation*}
%     \begin{split}
%         &\frac{\|x_{t-1} - \red{\Tilde{\mu}(x_t, x_0)}\|}{\blue{\Tilde{\beta}_t}}\\
%         &= \frac{\|x_t - \sqrt{\alpha_t}x_{t-1}\|^2_2}{\beta_t} + \frac{\|x_{t-1} - \sqrt{\Bar{\alpha}_{t-1}}x_0\|^2_2}{1 - \Bar{\alpha}_{t-1}} - \frac{\|x_t - \sqrt{\Bar{\alpha}_t}x_0\|^2_2}{1 - \Bar{\alpha}_t}\\
%         &= \frac{\|x_t\|_2^2 - 2\sqrt{\alpha_t}x_t\cdot x_{t-1} + \alpha_t\|x_{t-1}\|_2^2}{\beta_t} + \frac{\|x_{t-1}\|_2^2 - 2\sqrt{\Bar{\alpha}_t}x_0\cdot x_{t-1} + \Bar{\alpha}_{t-1}\|x_{0}\|_2^2}{1 - \Bar{\alpha}_{t-1}} + C_1(x_t,x_0)\\
%         &= \underbrace{\left(\frac{\alpha_t}{\beta_t} + \frac{1}{1 - \Bar{\alpha}_{t-1}} \right)}_{\text{\large a}}\|x_{t-1}\|_2^2 - \underbrace{\left(\frac{2\sqrt{\alpha_t}}{\beta_t}x_t + \frac{2\sqrt{\Bar{\alpha}_t}}{1 - \Bar{\alpha}_{t-1}}x_0 \right)}_{\text{\large b}}x_{t-1} + C_2(x_t,x_0).
%     \end{split}
% \end{equation*}

% By completing the square, we can deduce that:
% \begin{equation*}
%     \red{\Tilde{\mu}(x_t, x_0)} = - \frac{b}{2a}, \qquad \blue{\Tilde{\beta}_t} = \frac{1}{a}.
% \end{equation*}

% \begin{equation*}
%         \blue{\Tilde{\beta}_t} = 1/a = 1/(\frac{\alpha_t}{\beta_t} + \frac{1}{1 - \Bar{\alpha}_{t-1}}) = 1/(\frac{\alpha_t - \Bar{\alpha}_t + \beta_t}{\beta_t(1 - \Bar{\alpha}_{t-1})}) = \frac{\beta_t(1 - \Bar{\alpha}_{t-1})}{(1 - \beta_t) - \Bar{\alpha}_t + \beta_t} = \blue{\frac{1-\Bar{\alpha}_{t-1}}{1 - \Bar{\alpha}_t}\beta_t}
% \end{equation*}

% \begin{equation*}
%     \begin{split}
%         \red{\Tilde{\mu}(x_t, x_0)} &= -b/2a\\
%         &= \left(\frac{\sqrt{\alpha_t}}{\beta_t}x_t + \frac{\sqrt{\Bar{\alpha}_t}}{1 - \Bar{\alpha}_{t-1}}x_0 \right)/\left(\frac{\alpha_t}{\beta_t} + \frac{1}{1 - \Bar{\alpha}_{t-1}} \right)\\
%         &= \left(\frac{\sqrt{\alpha_t}}{\beta_t}x_t + \frac{\sqrt{\Bar{\alpha}_t}}{1 - \Bar{\alpha}_{t-1}}x_0 \right) \blue{\frac{1-\Bar{\alpha}_{t-1}}{1 - \Bar{\alpha}_t}\beta_t}\\
%         &= \frac{\sqrt{\alpha_t}(1-\Bar{\alpha}_{t-1})}{1-\Bar{\alpha}_t}x_t + \frac{\sqrt{\Bar{\alpha}_{t-1}}\beta_t}{1-\Bar{\alpha}_t}x_0.
%     \end{split}
% \end{equation*}

% Therefore, we have the closed-form expression of the reversed conditional probability:
% \begin{equation}
%     q(x_{t-1}|x_t, x_0) = \mathcal{N}(x_{t-1}; \red{\Tilde{\mu}(x_t, x_0)}, \blue{\Tilde{\beta}_t} \mathbf{I}).
% \end{equation}

% \subsection{Reverse Generative Process}

% During training, we learn the reverse Markov transition with joint distribution:
% %
% \begin{equation}
%     p_{\theta}(x_{0:T}) = p(x_T)\prod_{t=1}^T p_{\theta}(x_{t-1}|x_t),\quad p_{\theta}(x_{t-1}|x_t) = \mathcal{N}(x_{t-1}; \mu_{\theta}(x_t, t), \Sigma_{\theta}(x_t, t)) 
% \end{equation}
% % 
% The reverse process variance $\Sigma_{\theta}(x_t, t)$ is fixed at $\beta_t\mathbf{I}$. Thus, we only need to learn $\mu_{\theta}(x_t, t)$.

% \subsubsection*{I. Variational Lower Bound (VLB)}

% Similar to VAEs, diffusion models are based on variational inference. Thus we can use the variational lower bound (or ELBO) to optimize the negative log-likelihood
% %
% \begin{equation*}
%     \begin{split}
%         &- \log p_{\theta}(x_0)\\
%         &\leq - \log p_{\theta}(x_0) + D_{KL}(q(x_{1:T}|x_0)\,\|\,p_{\theta}(x_{1:T}|x_0))  \qquad (\text{KL divergence} \geq 0)\\
%         &= - \log p_{\theta}(x_0) + \mathbb{E}_{q(x_{1:T}|x_0)} \left[ \log \frac{q(x_{1:T}|x_0)}{p_{\theta}(x_{1:T}|x_0)}\right] \qquad (\text{Definition of KL divergence})\\
%         &= - \log p_{\theta}(x_0) + \mathbb{E}_{q(x_{1:T}|x_0)} \left[ \log \frac{q(x_{1:T}|x_0)}{\red{p_{\theta}(x_{0:T})/p_{\theta}(x_0)}}\right] \qquad (\text{Conditional probability})\\
%         &= - \log p_{\theta}(x_0) + \mathbb{E}_{q(x_{1:T}|x_0)} \left[ \log \frac{q(x_{1:T}|x_0)}{p_{\theta}(x_{0:T})} + \log p_{\theta}(x_0)\right] \qquad (\text{Logarithms})\\
%         &= \mathbb{E}_{q(x_{1:T}|x_0)} \left[ \log \frac{q(x_{1:T}|x_0)}{p_{\theta}(x_{0:T})}\right]
%     \end{split}
% \end{equation*}
 
% We define $\mathcal{L}_{\text{VLB}}$:
% \begin{equation*}
%     \mathcal{L}_{\text{VLB}} = \mathbb{E}_{q(x_{0:T})} \left[ \log \frac{q(x_{1:T}|x_0)}{p_{\theta}(x_{0:T})}\right]
% \end{equation*}

% The objective can be further rewritten to be a combination of several KL-divergence terms:
% \begin{equation*}
%     \begin{split}
%         \mathcal{L}_{\text{VLB}} &= \mathbb{E}_{q(x_{0:T})} \left[ \log \frac{q(x_{1:T}|x_0)}{p_{\theta}(x_{0:T})}\right]\\
%         &= \mathbb{E}_{q(x_{0:T})} \left[ \log \frac{\prod_{t=1}^T q(x_t|x_{t-1})}{p_{\theta}(x_{T})\prod_{t=1}^T p_{\theta}(x_{t-1}|x_t)}\right] \qquad (\text{Markovian property})\\
%         &= \mathbb{E}_{q(x_{0:T})} \left[ - \log p_{\theta}(x_{T}) + \sum_{t=1}^T \log \frac{q(x_t|x_{t-1})}{p_{\theta}(x_{t-1}|x_t)}\right] \qquad (\text{Logarithms})\\
%         &= \mathbb{E}_{q(x_{0:T})} \left[ - \log p_{\theta}(x_{T}) + \sum_{t=2}^T \log \frac{q(x_t|x_{t-1})}{p_{\theta}(x_{t-1}|x_t)} + \red{\log \frac{q(x_1|x_0)}{p_{\theta}(x_0|x_1)}}\right] \qquad (\text{Decomposition of sum})\\
%         &= \mathbb{E}_{q(x_{0:T})} \left[ - \log p_{\theta}(x_{T}) + \sum_{t=2}^T \log \Big(\frac{\red{q(x_{t-1}|x_t, x_0)}}{p_{\theta}(x_{t-1}|x_t)} \cdot \red{\frac{q(x_t|x_0)}{q(x_{t-1}|x_0)}} \Big) + \log \frac{q(x_1|x_0)}{p_{\theta}(x_0|x_1)}\right] \qquad (\text{Bayes rule})\\
%         &= \mathbb{E}_{q(x_{0:T})} \left[ - \log p_{\theta}(x_{T}) + \sum_{t=2}^T \log \frac{q(x_{t-1}|x_t, x_0)}{p_{\theta}(x_{t-1}|x_t)} + \red{\sum_{t=2}^T \log \frac{q(x_t|x_0)}{q(x_{t-1}|x_0)}} + \log \frac{q(x_1|x_0)}{p_{\theta}(x_0|x_1)}\right] \qquad (\text{Logarithms})\\
%         &= \mathbb{E}_{q(x_{0:T})} \left[ - \log p_{\theta}(x_{T}) + \sum_{t=2}^T \log \frac{q(x_{t-1}|x_t, x_0)}{p_{\theta}(x_{t-1}|x_t)} + \red{\log \frac{q(x_T|x_0)}{q(x_{1}|x_0)}} + \log \frac{q(x_1|x_0)}{p_{\theta}(x_0|x_1)}\right] \qquad (\text{Telescoping Sum})\\
%         &= \mathbb{E}_{q(x_{0:T})} \left[ \log \frac{q(x_T|x_0)}{p_{\theta}(x_{T})} + \sum_{t=2}^T \log \frac{q(x_{t-1}|x_t, x_0)}{p_{\theta}(x_{t-1}|x_t)} - \log p_{\theta}(x_0|x_1) \right] \qquad (\text{Rearranging terms})\\
%         &= \mathbb{E}_{q(x_{0:T})} \left[ \underbrace{D_{KL}(q(x_T|x_0)\;\|\; p_{\theta}(x_{T}))}_{\text{\large $L_T$}} + \sum_{t=2}^T \underbrace{D_{KL}(q(x_{t-1}|x_t, x_0)\;\|\; p_{\theta}(x_{t-1}|x_t))}_{\text{\large $L_{t-1}$}} -  \underbrace{\log p_{\theta}(x_0|x_1)}_{\text{\large $L_0$}} \right]
%     \end{split}
% \end{equation*}

% Therefore, $\mathcal{L}_{\text{VLB}}$ can be decomposed into a sum $\mathcal{L}_{\text{VLB}} = L_T + L_{T-1} + \cdots + L_0$, where

% \begin{equation*}
%     \begin{split}
%         L_T &= D_{KL}(q(x_T|x_0)\;\|\; p_{\theta}(x_{T}))\\
%         L_t &= D_{KL}(q(x_{t-1}|x_t, x_0)\;\|\; p_{\theta}(x_{t-1}|x_t))\\
%         L_0 &= - \log p_{\theta}(x_0|x_1)
%     \end{split}
% \end{equation*}

% These three cases can be addressed separately:

% \begin{itemize}
%     \item $L_T$ can be ignored because it does not depend on $\theta$.
%     \item $L_t$ is the \red{KL div. between two Gaussian with the same variance}, which can be computed in closed form.
%     \item $L_0$ is handled separately using a separate discrete decoder.
% \end{itemize}

% The loss is thus dominated by $L_t$.

% \subsubsection*{II. Parameterization of the noise}

% KL divergence between two Gaussian with the same variance is computed as:

% \begin{equation*}
%     D_{KL}(q(x_{t-1}|x_t, x_0)\;\|\; p_{\theta}(x_{t-1}|x_t)) = \frac{1}{2\|\Sigma_{\theta}\|_2^2} \|\Tilde{\mu}(x_t, x_0) - \mu_\theta(x_t, t)\|^2.
% \end{equation*}

% One possibility is to define $\mu_\theta(x_t, t)$ as a neural network with parameters $\theta$, but it was found that learning the noise $\varepsilon_t$ directly using a neural network $\varepsilon_\theta(x_t, t)$ performed better. So, first, we derive an expression for $\Tilde{\mu}(x_t, x_0)$ that depends on $\varepsilon_t$. Recall from Eq.~\ref{eq:forward_cf}, $x_t = \sqrt{\Bar{\alpha}_t}x_{0} + \sqrt{1 - \Bar{\alpha}_t}\varepsilon_t$. So,

% \begin{equation}
%     \label{eq:x0_ddpm}
%     x_0 = \frac{1}{\sqrt{\Bar{\alpha}_t}}(x_t - \sqrt{1 - \Bar{\alpha}_t}\varepsilon_t).
% \end{equation}

% Using this, we can express the mean of $q(x_{t-1}|x_t, x_0) = \mathcal{N}(x_{t-1}; \red{\Tilde{\mu}(x_t, x_0)}, \blue{\Tilde{\beta}_t} \mathbf{I})$:

% \begin{equation*}
%     \begin{split}
%         \Tilde{\mu}(x_t, x_0) &= \frac{\sqrt{\alpha_t}(1-\Bar{\alpha}_{t-1})}{1-\Bar{\alpha}_t}x_t + \frac{\sqrt{\Bar{\alpha}_{t-1}}\beta_t}{1-\Bar{\alpha}_t}\red{x_0}\\
%         \Tilde{\mu}(x_t, \varepsilon_t) &= \frac{\sqrt{\alpha_t}(1-\Bar{\alpha}_{t-1})}{1-\Bar{\alpha}_t}x_t + \frac{\sqrt{\Bar{\alpha}_{t-1}}\beta_t}{1-\Bar{\alpha}_t} \cdot \red{\frac{1}{\sqrt{\Bar{\alpha}_t}}(x_t - \sqrt{1 - \Bar{\alpha}_t}\varepsilon_t)}\\
%         &= \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{1-\alpha_t}{\sqrt{1 - \Bar{\alpha}_t}}\varepsilon_t\right).
%     \end{split}
% \end{equation*}

% Based on this, we can now find an expression for $L_t$ that is parameterized by $\varepsilon_{\theta}(x_t,t)$:
% \begin{equation*}
%     \begin{split}
%         L_t &= \mathbb{E}_{x_0, \varepsilon_t} \left[ \frac{1}{2\|\Sigma_{\theta}\|_2^2} \|\Tilde{\mu}(x_t, x_0) - \mu_\theta(x_t, t)\|^2 \right]\\
%         &= \mathbb{E}_{x_0, \varepsilon_t} \left[ \frac{1}{2\|\Sigma_{\theta}\|_2^2} \|\frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{1-\alpha_t}{\sqrt{1 - \Bar{\alpha}_t}}\varepsilon_t\right) - \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{1-\alpha_t}{\sqrt{1 - \Bar{\alpha}_t}}\varepsilon_{\theta}(x_t,t)\right)\|^2 \right]\\
%         &= \mathbb{E}_{x_0, \varepsilon_t} \left[  \frac{(1-\alpha_t)^2}{2\alpha_t(1 - \Bar{\alpha}_t)\|\Sigma_{\theta}\|_2^2} \|\varepsilon_t - \varepsilon_{\theta}(x_t,t)\|^2 \right]\\
%         &= \mathbb{E}_{x_0, \varepsilon_t} \left[  \red{\frac{(1-\alpha_t)^2}{2\alpha_t(1 - \Bar{\alpha}_t)\|\Sigma_{\theta}\|_2^2}} \|\varepsilon_t - \varepsilon_{\theta}(\sqrt{\Bar{\alpha}_t}x_{0} + \sqrt{1 - \Bar{\alpha}_t}\varepsilon_t,\; t)\|^2 \right].
%     \end{split}
% \end{equation*}

% The authors of DDPM \citep{Ho2020DenoisingModels} found that by ignoring the \red{weighing term}, the model performs better empirically. The training loss $\mathcal{L}$ is obtained by taking an expectation over the steps $t\sim [1,T]$:

% \begin{equation}
%     \label{eq:obj_noise}
%     \mathcal{L} = \mathbb{E}_{t\sim [1,T]\; x_0, \varepsilon_t} \left[ \|\varepsilon_t - \varepsilon_{\theta}(\sqrt{\Bar{\alpha}_t}x_{0} + \sqrt{1 - \Bar{\alpha}_t}\varepsilon_t,\; t)\|^2 \right]
% \end{equation}

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=\textwidth]{figures_appendix/DDPM-algo.pdf}
%     \caption{The training and sampling algorithms in DDPM}
%     \label{fig:ddpm}
% \end{figure}

% \subsubsection*{III. Second Equivalent Interpretation}

% As shown previously in Eq.~\ref{eq:obj_noise}, the diffusion model (or variational diffusion model) can be trained by learning a neural network to predict the noise $\varepsilon_t$ added in the forward process. However, there can be two other equivalent interpretations where we: (1) learn a neural network to predict the original natural image and (2) learn the (Stein) score function. Here, we only discuss the second interpretation.

% \textbf{(Stein) Score function}. Given a probability density $p(x)$, its (Stein) score function is defined as $\nabla_x \log p(x)$. It is the gradient of the density with respect to the input $x$. \red{The score function points in the direction of the increasing probability}, an example of which is shown in Fig.~\ref{fig:score_func} \footnote{Source: \href{https://yang-song.net/blog/2021/score/}{Blog by Yang Song}}. The benefit of using the score function is that it does not depend on the normalizing constant. 

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.2\textwidth]{figures_appendix/score_func.pdf}
%     \caption{Score function (the vector field) and density function of a mixture of two Gaussians.}
%     \label{fig:score_func}
% \end{figure} 

% \textbf{Score-based objective}. We apply Tweedie's formula to predict the posterior mean of $x_t$. 

% \begin{tcolorbox}[title=Tweedie's Formula (1956) \citep{Efron2011TweediesBias}, boxrule=0pt]
% The true mean of an exponential family distribution, given samples drawn from it, can be estimated by the maximum likelihood estimate of the samples (empirical mean) with an added correction term involving the score of the estimate. In the case of one observed sample, the empirical mean is the sample itself.\\

% For a Gaussian variable $x \sim \mathcal{N}(x, \mu_x, \Sigma_x)$, the formula states:
% %
% \begin{equation}
%     \mathbb{E}[\mu_x|x] = x + \Sigma_x \nabla_x \log p(x).
% \end{equation}
% %
% \end{tcolorbox}

% % \textbf{Proof}.

% From Eq.~\ref{eq:forward_cf}, we know that:
% %
% \begin{equation*}
%     q(x_t|x_0) = \mathcal{N}(x_t; \sqrt{\Bar{\alpha}_t}x_{0}, (1 - \Bar{\alpha}_t)\mathbf{I}).
% \end{equation*}
% %

% Then, using Tweedie's formula, we get:
% %
% \begin{equation}
%     \label{eq:tw_pm}
%     \mathbb{E}[\mu_{x_t}|x_t] = x_t + (1 - \Bar{\alpha}_t) \nabla_{x_t} \log p(x_t).
% \end{equation}
% %

% Therefore, according to Eq.~\ref{eq:tw_pm}, the best estimate for the true mean $\mu_{x_t} = \sqrt{\Bar{\alpha}_t}x_{0}$ is given as:
% %
% \begin{align}
%     \sqrt{\Bar{\alpha}_t}x_{0} &= x_t + (1 - \Bar{\alpha}_t) \nabla_{x_t} \log p(x_t)\\
%     x_{0} &= \frac{1}{\sqrt{\Bar{\alpha}_t}} (x_t + (1 - \Bar{\alpha}_t) \nabla_{x_t} \log p(x_t)). \label{eq:x0_ncsn}
% \end{align}
% %

% Using this, we can express the mean of $q(x_{t-1}|x_t, x_0) = \mathcal{N}(x_{t-1}; \red{\Tilde{\mu}(x_t, x_0)}, \blue{\Tilde{\beta}_t} \mathbf{I})$:
% %
% \begin{equation*}
%     \begin{split}
%         \Tilde{\mu}(x_t, x_0) &= \frac{\sqrt{\alpha_t}(1-\Bar{\alpha}_{t-1})}{1-\Bar{\alpha}_t}x_t + \frac{\sqrt{\Bar{\alpha}_{t-1}}\beta_t}{1-\Bar{\alpha}_t}\red{x_0}\\
%         \Tilde{\mu}(x_t, \varepsilon_t) &= \frac{\sqrt{\alpha_t}(1-\Bar{\alpha}_{t-1})}{1-\Bar{\alpha}_t}x_t + \frac{\sqrt{\Bar{\alpha}_{t-1}}\beta_t}{1-\Bar{\alpha}_t} \cdot \red{\frac{1}{\sqrt{\Bar{\alpha}_t}} (x_t + (1 - \Bar{\alpha}_t) \nabla_{x_t} \log p(x_t))}\\
%         &= \frac{1}{\sqrt{\alpha_t}} \left( x_t - (1-\alpha_t) \nabla_{x_t} \log p(x_t)\right).
%     \end{split}
% \end{equation*}

% Based on this, we can now find an expression for $L_t$ that is parameterized by $s_{\theta}(x_t,t)$:
% \begin{equation*}
%     \begin{split}
%         L_t &= \mathbb{E}_{x_0, \varepsilon_t} \left[ \frac{1}{2\|\Sigma_{\theta}\|_2^2} \|\Tilde{\mu}(x_t, x_0) - \mu_\theta(x_t, t)\|^2 \right]\\
%         &= \mathbb{E}_{x_0, \varepsilon_t} \left[ \frac{1}{2\|\Sigma_{\theta}\|_2^2} \|\frac{1}{\sqrt{\alpha_t}} \left( x_t - (1-\alpha_t) \nabla_{x_t} \log p(x_t)\right) \frac{1}{\sqrt{\alpha_t}} \left( x_t - (1-\alpha_t) s_{\theta}(x_t, t)\right)\|^2 \right]\\
%         &= \mathbb{E}_{x_0, \varepsilon_t} \left[  \frac{(1-\alpha_t)^2}{2\alpha_t\|\Sigma_{\theta}\|_2^2} \|\nabla_{x_t} \log p(x_t) - s_{\theta}(x_t,t)\|^2 \right]\\
%         &= \mathbb{E}_{x_0, \varepsilon_t} \left[  \red{\frac{(1-\alpha_t)^2}{2\alpha_t\|\Sigma_{\theta}\|_2^2}} \|\nabla_{x_t} \log p(x_t) - s_{\theta}(\sqrt{\Bar{\alpha}_t}x_{0} + \sqrt{1 - \Bar{\alpha}_t}\varepsilon_t,\; t)\|^2 \right].
%     \end{split}
% \end{equation*}

% We thus arrive at the training loss $\mathcal{L}_{\mathrm{NCSN}}$ for noise-conditioned score network (NCSN) by taking an expectation over the steps $t\sim [1,T]$:

% \begin{equation}
%     \label{eq:obj_score}
%     \mathcal{L}_{\mathrm{NCSN}} = \mathbb{E}_{t\sim [1,T]\; x_0, \varepsilon_t} \left[ \|\nabla_{x_t} \log p(x_t) - s_{\theta}(\sqrt{\Bar{\alpha}_t}x_{0} + \sqrt{1 - \Bar{\alpha}_t}\varepsilon_t,\; t)\|^2 \right].
% \end{equation}

% \textbf{Score function estimation}. The problem in directly minimising the loss in Eq.~\ref{eq:obj_score} is that $\nabla_x \log p(x)$ is unknown. However, \citet{Hyvarinen2005EstimationMatching} demonstrated that an equivalent object can be obtained via change of variable:
% %
% \begin{equation}
%     \label{eq:sm2}
%     \mathbb{E}_{t\sim [1,T]\; x_0, \varepsilon_t} \left[ \frac{1}{2}\| s_\theta(x) \|^2_2 + tr(\nabla_x s_\theta(x))\right].
% \end{equation}
% %

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.7\textwidth]{figures_appendix/score_est.jpg}
%     \caption{Score estimation from data.}
%     \label{fig:score_est}
% \end{figure}

% \textbf{Connection with DDPM}. We can relate the NCSN with the DDPM by relating Eq.~\ref{eq:x0_ncsn} with Eq.~\ref{eq:x0_ddpm} as:
% %
% \begin{align}
%     x_0 = \frac{1}{\sqrt{\Bar{\alpha}_t}} (x_t + (1 - \Bar{\alpha}_t) \nabla_{x_t} \log p(x_t)) &= \frac{1}{\sqrt{\Bar{\alpha}_t}}(x_t - \sqrt{1 - \Bar{\alpha}_t}\varepsilon_t)\\
%     (1 - \Bar{\alpha}_t) \nabla_{x_t} \log p(x_t) &= - \sqrt{1 - \Bar{\alpha}_t}\varepsilon_t\\
%     \nabla_{x_t} \log p(x_t) & = - \frac{\sqrt{1 - \Bar{\alpha}_t}\varepsilon_t}{(1 - \Bar{\alpha}_t)}\\
%     \nabla_{x_t} \log p(x_t) & = - \frac{1}{\sqrt{1 - \Bar{\alpha}_t}}\varepsilon_t \label{eq:eqiv_smdd}
% \end{align}
% %

% The two terms differ only by a constant term that scales with time. The score function provides the direction to step in the data space to maximize the log probability. Therefore, moving opposite the direction of added noise ``denoises'' the image would be the best update to increase the log probability, which is expressed mathematically in Eq.~\ref{eq:eqiv_smdd}.

% \textbf{Sampling from NCSN}. The score function defines the vector field pointing towards the modes. Therefore we can generate samples by starting from an arbitrary point and iteratively following the score until a mode is reached. This procedure is known as \emph{Langevin dynamics}.

% \begin{tcolorbox}[title=Stochastic Gradient Langevin Dynamics \citep{Welling2011BayesianDynamics}, boxrule=0pt]
% Langevin dynamics is a concept from physics that mathematically models the dynamics of molecular systems. In combination with stochastic gradient descent, it can be used to sample from a probability density $f(x)$  using only the gradients $\nabla_x f(x)$ in a Markov chain of updates:

% \begin{equation}
%     x_{t+1} = x_t + \frac{\delta}{2} \nabla_x \log f(x_t) + \sqrt{\delta}\varepsilon_t,\qquad ~\varepsilon_t \sim \mathcal{N}(\mathbf{0}, \mathbf{I}).
% \end{equation}
% %
% where $\delta$ is the step size. When $T \rightarrow \infty$, $\varepsilon_T \rightarrow 0$, $x_T$ converges to the maximum a posteriori (MAP) estimate.
% \end{tcolorbox}

% The extra noise added to the generated samples ensures that the samples do not always collapse onto the model. Additionally, it adds stochasticity to the generative process.


\clearpage

\section{Proof of Theorem~\ref{thm:optpol}}
\label{app:proofs}

% \begin{proof}[Proof of Proposition~\ref{prop:bon}]
%     Let $f$ and $F$ be the probability density function and the cumulative density function of $P$, respectively. Further, let $f_{(k)}$ and $F_{(k)}$ be the probability density function and the cumulative density function of $P_{(k)}$, respectively.

%     Given $X_1, \dots, X_k \overset{i.i.d}{\sim} P$ and $x \in \mathbb{R}$,
%     \begin{align}
%         \mathbb{P}(\max(X_1, \dots, X_k) \leq x) &= \mathbb{P}(X_1 \leq x~\text{and} \dots \text{and}~X_k \leq x)\\
%         &= \prod_{i=1}^k \mathbb{P}(X_i \leq x) = \mathbb{P}(X_i \leq x)^k,
%     \end{align}
%     and so $F_{(k)}(x) = F(x)^k$. Differentiating, we obtain $f_{(k)}(x) = kF(x)^{k-1}f(x)$. Hence,
%     \begin{align}
%         KL(P_{(k)} \| P) &= \int_{-\infty}^\infty f_{(k)}(x) \log \left(\frac{f_{(k)}(x)}{f_(x)}\right) dx\\
%         &= \int_{-\infty}^\infty f_{(k)}(x) \log \left(kF(x)^{k-1}\right) dx\\
%         &= \log{k} + (k-1) \int_{-\infty}^\infty f_{(k)}(x) \log \left(F(x)\right) dx\\
%         &= \log{k} + (k-1) \left([F_{(k)}(x)\log{(F(x))}]_{-\infty}^\infty - \int_{-\infty}^\infty F_{(k)}(x)\frac{1}{F(x)}f(x) dx\right)\\
%         &= \log{k} + (k-1) \left([F(x)^k\log{(F(x))}]_{-\infty}^\infty - \int_{-\infty}^\infty F(x)^{k-1} f(x) dx\right)\\
%         &= \log{k} + (k-1)\int_0^1 u^{k-1} du\\
%         &= \log{k} - \frac{k-1}{k}
%     \end{align}
% \end{proof}

\begin{proof}[Proof of Theorem~\ref{thm:optpol}]

\begin{align}
    % \begin{split}
        J_{\lambda}(x_t, \pi, c) &= \mathbb{E}_{x_{t-1} \sim \pi} \left[ \lambda(V(x_{t-1}; p, c) - V(x_t; p, c)) + \log \frac{p(x_{t-1}|x_{t}, c)}{\pi(x_{t-1}|x_{t}, c)} \right]\\
        &= \mathbb{E}_{x_{t-1} \sim \pi} \left[ \log \frac{p(x_{t-1}|x_{t}, c)\;e^{\lambda(V(x_{t-1}; p, c) - V(x_t; p, c))}}{\pi(x_{t-1}|x_t, c)} \right]\\
        &= \mathbb{E}_{x_{t-1} \sim \pi} \left[ \log \frac{p(x_{t-1}|x_{t}, c)\;e^{\lambda V(x_{t-1}; p, c)}}{\pi(x_{t-1}|x_t, c)} + \log e^{\lambda V(x_t; p, c)} \right] \\
        &= \mathbb{E}_{x_{t-1} \sim \pi} \left[ \log \frac{p(x_{t-1}|x_{t}, c)\;e^{\lambda V(x_{t-1}; p, c)}}{\pi(x_{t-1}|x_t, c)} \right] + \lambda V(x_t; p, c) \label{eq:opt1}
\end{align}

Now, let
\begin{equation}
    p_{\lambda}(x_{t-1}|x_t, c) := \frac{p(x_{t-1}|x_{t}, c)e^{\lambda V(x_{t-1}; p, c)}}{Z_\lambda(x_t, c)},
\end{equation}
where the normalizing constant $Z_\lambda(x_t, c)$ is given by

\begin{equation}
    Z_\lambda(x_t, c) = \mathbb{E}_{x_{t-1} \sim p} \left[p(x_{t-1}|x_{t}, c)e^{\lambda V(x_{t-1}; p, c)}\right].
\end{equation}

Putting it back in Eq.~\ref{eq:opt1}, we get
\begin{align}
    % \begin{split}
        J_{\lambda}(x_t, \pi, c) &= \mathbb{E}_{x_{t-1} \sim \pi} \left[ \log  \frac{p_{\lambda}(x_{t-1}|x_t, c)}{\pi(x_{t-1}|x_t, c)}Z_\lambda(x_t, c) \right] + \lambda V(x_t; p, c)\\
        &= \mathbb{E}_{x_{t-1} \sim \pi} \left[ \log  \frac{p_{\lambda}(x_{t-1}|x_t, c)}{\pi(x_{t-1}|x_t, c)} + \log Z_\lambda(x_t, c) \right] + \lambda V(x_t; p, c)\\
        &= \mathbb{E}_{x_{t-1} \sim \pi} \left[ \log  \frac{p_{\lambda}(x_{t-1}|x_t, c)}{\pi(x_{t-1}|x_t, c)} \right] + \log Z_\lambda(x_t, c) + \lambda V(x_t; p, c)\\
        &= - \mathbb{E}_{x_{t-1} \sim \pi} \left[ \log  \frac{\pi(x_{t-1}|x_t, c)}{p_{\lambda}(x_{t-1}|x_t, c)} \right] + \log Z_\lambda(x_t, c) + \lambda V(x_t; p, c)\\
        &= - \textit{KL}(\pi(x_{t-1}|x_{t}, c)\;\|\;p_{\lambda}(x_{t-1}|x_t, c)) + \log Z_\lambda(x_t, c) + \lambda V(x_t; p, c) \label{eq:obj}
    % \end{split}
\end{align}

Eq.~\ref{eq:obj} is uniquely maximized by $\pi_\lambda^*(x_{t-1}|x_{t}, c) = p_{\lambda}(x_{t-1}|x_t, c)$.
\end{proof}

% \begin{proposition}
%     \label{prop:bon}
%     Let $P$ be a continuous probability distribution and $k \in \mathbb{N}$. Then, the KL divergence from $P$ to best-of-k-from-P distribution $P_{(k)}$ is
%     \begin{equation}
%         KL(P_{(k)} \| P) = \log k - \frac{k-1}{k}.
%     \end{equation}
% \end{proposition}

% The proof is deferred to the appendix~\ref{app:proofs}.

\clearpage
\section{Sampling from Optimal Model using Langevin Dynamics}
\label{app:samld}

Given the optimal policy given in Eq.~\ref{eq:optpol}, our goal is to now sample from $\pi^*$ instead of $p$. However, given only $p$, it is difficult to sample from this optimal policy. To overcome this problem, we look at the score-based sampling approach as in NCSN \citep{Song2019GenerativeDistribution}. Starting from an arbitrary point $x_T$, we iteratively move in the direction of $\nabla_{x_t} \log \pi^*(x_t)$, which is equivalent to $\nabla_{x_t} \log p_{\lambda}(x_t)$. We can derive an equivalent form:

\begin{align}
    p_{\lambda}(x_t) &=  \frac{p(x_{t})e^{\lambda V(x_{t})}}{Z_\lambda}\\
    \log p_{\lambda}(x_t) &= \log p(x_{t}) + \lambda V(x_{t}) - \log Z_\lambda\\
    \nabla_{x_t} \log p_{\lambda}(x_t) &= \nabla_{x_t} \log p(x_{t}) + \nabla_{x_t} \lambda V(x_{t}) - \nabla_{x_t} \log Z_\lambda\\
    s_{\lambda}(x_t,t) &= s_\theta(x_t,t) + \lambda \nabla_{x_t} V(x_{t}).
\end{align}

As the above derivation is limited to stochastic diffusion sampling, we leverage the connection between diffusion models and score matching \citep{Song2019GenerativeDistribution}:
\begin{equation}
    \nabla_{x_t} \log p(x_t) = - \frac{1}{\sqrt{1 - \Bar{\alpha}_t}}\varepsilon_t.
\end{equation}

\textbf{Similarity with classifier guidance}. Starting from an arbitrary point $x_T$, we iteratively move in the direction of $\nabla_{x_t} \log p(x_t|y)$. We can derive an equivalent form:

\begin{align}
    p(x_t|y) &=  \frac{p(y|x_t)p(x_{t})}{Z}\\
    \log p(x_t|y) &= \log p(x_{t}) + \log p(y|x_t) - \log Z\\
    \nabla_{x_t} \log p(x_t|y) &= \nabla_{x_t} \log p(x_{t}) + \nabla_{x_t} \log p(y|x_t) - \nabla_{x_t} \log Z\\
    s_{\lambda}(x_t|y,t) &= s_\theta(x_t,t) + \nabla_{x_t} \log p(y|x_t).
\end{align}

\clearpage
\section{\ourmethod{} with Image-Conditioning: \ourmethod($\eta$)}
\label{sec: full_code}
% 
\begin{wrapfigure}{r}{0.45\textwidth}
\vspace{-14pt}
\begin{minipage}{0.45\textwidth}
\IncMargin{1.6em} 
\begin{algorithm}[H]
    \scriptsize
    \SetAlgoLined
    \DontPrintSemicolon
    \SetNoFillComment
    \Indm
    \clr{\KwInput{\(p\), $T$, \(N\), \(B\), $x_{\textup{ref}}$, $c$, $\eta$}}
    \Indp
        % {Sample initial noise:} \(x_T \sim \gN(0, I)\)\\
        \clr{Sample conditional initial noise:\;
        \quad \(\tau = \eta \times T\)\\
        \quad \(x_\tau = \sqrt{\Bar{\alpha}_\tau}x_{\mathrm{ref}} + \sqrt{1 - \Bar{\alpha}_\tau}z\),\, \(z \sim \gN(0, I)\) \\
        {Initialize counter:} \(s = 1\)\\
        \For{\(t \in [\tau-1,\cdots, 0]\)}{
            % $\textup{\texttt{mod}}(s,B) = 0$
            \If{$\textup{\texttt{mod}}(s,B) = 0$}{
                % {Sample \(N\) i.i.d samples by unrolling \(p\) over \(M\) steps:}\;
                % {\quad \(\{x_{a:b}^n\}_{n=1}^N \sim p(\cdot|x_{T-i+M})\), where \(a = T-1-i+M\) and \(b = T-1-i\)}\\
                {Sample \(N\) times over \(B\) steps:}
                {\quad \(\{x_{t-1}^{(n)}\}_{n=1}^N \sim \prod_{i=t}^{t+B} p(x_{i-1}|x_i)\)}\\
                {Select the sample with maximum value:}
                {\quad \(x_{t-1} \gets \underset{\{x_{t-1}^{(n)}\}_{n=1}^N}{\operatorname{argmax}} V(x_{t-1}^{(n)}; p, c)\)}\\
            }
            \(s \gets s + 1\)\\}
        }
    \Indm
    \clr{\KwOutput{\(x_0\)}}
    \Indp
    \caption{\ourmethod($\eta$)}
    \label{algo:full_code}
\end{algorithm}
\DecMargin{1.6em}
\end{minipage}
\vspace{-14pt}
\end{wrapfigure}
%

For (T+I)2I cases, where the reward depends on a target image, the reward distribution deviates significantly from the base distribution \(p\). Here, sampling-based approaches would require a relatively larger value of \(N\) to achieve alignment. To tackle this, a reference target image \(x_{\mathrm{ref}}\), such as a specific style or even stroke painting, is provided as an additional conditioning input. Inspired by image editing techniques using diffusion \citep{Meng2021SDEdit:Equations,koohpayegani2023genie}, we add partial noise corresponding to only $\tau = \eta \times T$ (with $\eta \in (0, 1]$) steps of the forward diffusion process, instead of the full noise corresponding to $T$ steps. This is illustrated in line $2$ and $3$ of Algorithm~\ref{algo:full_code}. Then, starting from this noisy version of the reference image $x_\tau$, \ourmethod($\eta$) progressively denoises the sample for only $\tau$ steps to generate the clean, reference-aligned image $x_0$ (lines $5$ to $10$). Specifically, for each block of \(B\) steps, we unroll the diffusion model \(N\) times independently (Algorithm~\ref{algo:full_code}, line $7$). Then, based on the value function, select the best sample (line $8$) to continue the reverse process until we obtain a clean image at \(t=0\). \clr{A key advantage of \ourmethod($\eta$) is its ability to achieve similar alignment-divergence trade-offs while using a significantly lower value of \(N\), as is demonstrated in Section~\ref{sec:toy}. Note that the inner loop of \ourmethod($\eta$) (lines $5$-$10$) runs for $\tau$ steps (instead of $T$) due to adjustable noise conditioning discussed in the following.} For the sake of brevity, we assume $\tau$ to be divisible by $B$; otherwise, we apply the same steps on a last but smaller block. By conditioning the initial noise sample $x_\tau$ on the reference image $x_{\textup{ref}}$, we can generate images $x_0$ that better incorporate the characteristics and semantics of the reference image while adhering to the text prompt $c$. As we demonstrate in our experimentation, threshold $\eta$ provides an \emph{extra knob} built in \ourmethod($\eta$) allowing the user to efficiently trade off divergence for reward. Note that the reward-conditioning of the generated image is inversely proportional to the value of $\eta$. Setting $\eta = 1$ results in $\tau = T$ and fully deactivates the noise conditioning. A byproduct of this conditioning is compute efficiency, as is discussed in Section~\ref{sec:conc}.


\clearpage
\section{Additional Results for Case Study I}
\label{sec:toysc1}
For the sake of completeness, we also study a variant of the GMM setting as discussed in Section~\ref{sec:toy}, where the mean of the reward distribution is equal to the mean of one of the components in the prior distribution, as shown in Fig.~\ref{fig:toysc1}. The prior distribution \(p(\vx)\) is modelled as a \(2\)-dimensional Gaussian mixture model (GMM)  $p(\vx_0) = \sum_{i=1}^{3} w_i\mathcal{N}(\bm \mu_i, {\bm \sigma}^2{\bm I}_2)$, with $\sigma = 2$, $[{\bm \mu}_1, {\bm \mu}_2,{\bm \mu}_3] = [(5, 3), (3, 7), (7, 7)]$, and ${\bm I}_d$ is an $d$-dimensional identity matrix, as shown in Fig.~\ref{fig:toysc1}. All mixture components are equally weighted with, i.e., \(w_1 = w_2 = w_3 = 0.33\). In contrast to the previous setup, we define the reward distribution as \(p(r|\vx) = \gN({\bm \mu}_r, {\bm \sigma}_r^2{\bm I}_2)\) with \({\bm \mu}_r = [5,3]\) and \({\bm \sigma}_r = 2\). Based on this setup, we train a diffusion model \(p_{\theta}(x)\) to estimate the prior distribution \(p(\vx)\). For this we use a \(3\)-layer MLP that takes as input \((\vx_t, t)\) and predicts the noise \({\bm \varepsilon}_t\). It is trained over \(200\) epochs with \(T=1000\) denoising steps. Then, we implement \ourmethod to guide the trained diffusion model to generate samples with high likelihood under the reward distribution.

%
\begin{wrapfigure}{r}{0.5\textwidth}
    \vspace{-1.5em}
    \begin{minipage}{0.5\textwidth}
    \centering
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs_revised/toy/plots_id_revised/toysetup_sc1.pdf}
        % \caption{Experimental Setup}
        % \label{fig:toysc1}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs_revised/toy/plots_id_revised/result_id.pdf}
        % \caption{Trade-off Curves}
        % \label{fig:ressc1}
    \end{subfigure}
    \caption{Setup (top row) and reward vs. divergence trade-off (bottom row). \ourmethod{} offers highest reward at lowest divergence with a much lower $N$ than BoN.}
    \label{fig:toysc1}
    \end{minipage}
    \vspace{-1em}
\end{wrapfigure}
%
In Fig.~\ref{fig:toysc1}, we present the trade-off curves for normalized expected reward (or win rate) versus KL divergence by adjusting the hyperparameters of the respective methods. For the guidance-based methods DPS and UG, the guidance scale is varied between \(1\) and \(50\), whereas for the sampling-based methods BoN, SVDD, and \ourmethod{}, the number of samples \(N\) is varied between \(2\) and \(500\). Similar to the results in Section~\ref{sec:toy}, we observe \ourmethod{} achieve the most favorable trade-off between normalized expected reward and KL divergence, with BoN performing closely behind. In the case of win rate vs. KL divergence, BoN demonstrates the best trade-off, consistent with findings from the literature on Language Model (LM) alignment \citep{mroueh2024information, Beirami2024TheoreticalPolicy,Gui2024BoNBoNSampling}. Furthermore, guidance-based methods tend to exhibit higher KL divergence, as they often collapse to the mode of the reward distribution when the guidance scale is increased, leading to a reduction in diversity among the sampled data points. For both performance metrics, SVDD-PM achieves a high expected reward or win rate but at the expense of significantly increased divergence, even for smaller values of \(N\). Whereas \ourmethod{} offers the widely sought-after flexibility, allowing users to balance the trade-off by adjusting parameters such as \(N\) and \(B\).

\clearpage

\section{Additional Results for Case Study II}
\label{sec:ad}
\vspace{-0.3cm}
Here, we provide further details about the differentiable reward-guidance scenarios' quantitative evaluations summarized in Table~\ref{tab:quant} and computational complexity analysis in Table~\ref{tab:complexity}. 

\textbf{Further details on evaluation metrics.} For computing I-Gram, we utilize VGG \citep{simonyan2014very} Gram matrices of the reference and generated images to measure image alignment across all scenarios/settings, as commonly followed in the literature \citep{Somepalli2024MeasuringModels, gatys2016image, yeh2020improving}. Specifically, these are computed using the last layer feature maps of an ImageNet-1k pretrained VGG backbone \citep{simonyan2014very}. Image alignment between a reference, generated image pair is then measured by computing the dot product of their gram matrices. Further, we report a recently proposed CLIP-based Maximum Mean Discrepancy (CMMD) \citep{jayasumana2024rethinking} as a divergence measure. It overcomes the drawback of FID stemming from the underlying Gaussian assumption in the representation space of the Inception model \citep{szegedy2015going}.

\textbf{Qualitative performance.} Let us start with style guidance in Fig.~\ref{fig:style}. As can be seen, \ourmethod{} either stands on-par or performs better as compared to all other baselines in terms of capturing both, the style of the reference image and the semantics of the text prompt. This can be seen in comparison with UG for the text prompt of ``portrait of a woman'', where UG fails to incorporate the text prompt, but latches onto the style of the reference image. The results for face guidance with and without noise-conditioning are illustrated in Figs.~\ref{fig:face_eta},~\ref{fig:face}, respectively. It can be noticed that the noise-conditioned baselines capture the reference face much better than their non noise-conditioned counterparts. Moreover, in the case of noise-conditioning, BoN($\eta$), SVDD-PM($\eta$) and UG($\eta$) fail to meaningfully capture the semantics of the text-prompt, particularly for ``Headshot of a woman made of marble''. However, \ourmethod($\eta$) captures both, the reference face and the text prompt. In the case of the other text prompt ``Headshot of a person with blonde hair with space background'', SVDD-PM($\eta$) and \ourmethod($\eta$) offer best results as compared to other baselines. Finally, the results for stroke guidance without noise-conditioning are illustrated in Fig.~\ref{fig:stroke}. It can be seen that none of the baselines capture the reference strokes or their color palettes successfully, but only adhere to the text-prompt. This empirically corroborates the need for using noise-conditioning for guidance, when the reward distribution (strokes in this scenario) differs significantly from the base diffusion model's distribution. 

\textbf{Quantitative performance.} In this section, we break down the quantitative performance of all methods across the three different differentiable reward scenarios of style, face and stroke guidance. We summarize the results in Tab.~\ref{tab:style}, \ref{tab:face}, \ref{tab:stroke} with the first row corresponding to the base Stable Diffusion model and R: indicating the reward metric used for guiding the diffusion model.

% \begin{wraptable}{r}{0.6\textwidth}
% \vspace{-1em}
% \caption{\small Quantitative metrics for style guidance.}\vspace{-1em}
% \label{tab:style}
% \def\arraystretch{1.15}
% \resizebox{1\linewidth}{!}{
% \begin{tabular}
% {l|c|c|c|c|c}
% \hline
% \multirow{2}{*}{\textbf{Method}} & \multicolumn{4}{c}{\textbf{R1: Style Guidance}}\\ \cline{2-6}  
% & \scriptsize \textbf{Rew.} ($\uparrow$) & \scriptsize \textbf{FID} ($\downarrow$) & \scriptsize \textbf{CMMD} ($\downarrow$) & \scriptsize \textbf{T-CLIP} ($\uparrow$) & \scriptsize \textbf{Gram} ($\uparrow$) \\  \hline
% Base-SD \citeyear{Rombach2021High-ResolutionModels}                          & 57.8                            & 82.3                        & 0.04                         & 0.810 & 0.10 \\ \hline
% BoN \citeyear{Gao2022ScalingOveroptimization}                             & 65.4                            & 107.1                       & 0.09                         & 0.805 & 0.11 \\
% SVDD-PM \citeyear{Li2024Derivative-FreeDecoding}                         & 81.9                            & 143.1                       & 0.42                         & 0.808 & 0.16 \\ \cdashline{1-6}
% DPS \citeyear{Chung2023DiffusionProblems}                             & 69.4                            & 106.3                       & 0.21                         & 0.803 & 0.12 \\
% UG \citeyear{Bansal2024UniversalModels}              & 79.8                            & 351.7                       & 3.52                         & 0.663 & 0.29 \\ \hline
% \rowcolor{tabBlue} \textbf{\ourmethod (Ours)}       & 76.9                            & 122.5                       & 0.29                         & 0.811 & 0.16 \\
% \rowcolor{tabBlue} \textbf{\ourcondmethod (Ours)}   & 87.0                            & 299.8                       & 3.26                         & 0.694 & 0.34 \\ \hline
% \end{tabular}
% }
% \vspace{-0.8em}
% \end{wraptable}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.85\linewidth]{figs_revised/add_figs/style_res_noneta.pdf}
    \caption{Quality evaluation across methods for style guidance without noise-conditioning.}
    \label{fig:style}
\end{figure}

\begin{figure}[t]
    \centering
    \label{fig:styleplots}
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs_revised/plots_revised_add/exp_rew-vs-fid_styletransfer.pdf}
        % \caption{Caption}
        % \label{fig:enter-label}
    \end{subfigure}
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs_revised/plots_revised_add/exp_rew-vs-cmmd_styletransfer.pdf}
        % \caption{Caption}
        % \label{fig:enter-label}
    \end{subfigure}
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs_revised/plots_revised_add/win_rate-vs-fid_styletransfer.pdf}
        % \caption{Caption}
        % \label{fig:enter-label}
    \end{subfigure}
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs_revised/plots_revised_add/win_rate-vs-cmmd_styletransfer.pdf}
        % \caption{Caption}
        % \label{fig:enter-label}
    \end{subfigure}
    \vspace{-0.2cm}
    \caption{Reward vs. divergence trade-off curves for style guidance.}
    \label{fig: style-trad}
    \vspace{-12pt}
\end{figure}

\clearpage


\textbf{Style Guidance}. The results are summarized in Table~\ref{tab:style}. Compared to the sampling-based guidance counterparts BoN and BoN($\eta$), \ourmethod{} achieves a higher reward at the cost of slightly higher divergence (FID and CMMD), with and without the noise-conditioning. Yet, with a slightly smaller reward \ourmethod{}, \ourmethod($\eta$) offers a better performance than SVDD-PM, SVPP-PM($\eta$) across FID, CMMD and T-CLIP. Compared to guidance-based counterparts such as DPS, DPS($\eta$) and UG, UG($\eta$), \ourmethod{}, \ourmethod($\eta$) offer a better trade-off in terms of reward vs base distribution divergence and reward vs text, image alignment. This is also illustrated in Fig.~\ref{fig: style-trad} where \ourmethod($\eta$) consistently outperforms UG, UG($\eta$) in terms of image alignment (normalized expected reward as well as win rate), while offering lesser divergence w.r.t. both FID and CMMD.  

\begin{table}[h]
\centering
\caption{\small Quantitative metrics for style guidance.}\vspace{-1em}
\label{tab:style}
\def\arraystretch{1.15}
\resizebox{0.7\linewidth}{!}{
\begin{tabular}
{l|c|c|c|c|c}
\hline
\multirow{2}{*}{\textbf{Method}} & \multicolumn{4}{c}{\textbf{R1: Style Guidance}}\\ \cline{2-6}  
& \scriptsize \textbf{Rew.} ($\uparrow$) & \scriptsize \textbf{FID} ($\downarrow$) & \scriptsize \textbf{CMMD} ($\downarrow$) & \scriptsize \textbf{T-CLIP} ($\uparrow$) & \scriptsize \textbf{I-Gram} ($\uparrow$) \\  \hline
Base-SD (\citeyear{Rombach2021High-ResolutionModels}) & 1.0  & 1.0 & 1.0 & 1.0 & 1.0 \\ \hline
SDEdit (\citeyear{Meng2021SDEdit:Equations}) & 1.22  & 3.25 & 67.75 & 0.90 & 2.9 \\
BoN (\citeyear{Gao2022ScalingOveroptimization}) & 1.14  & 1.30 & 2.25 & 0.99 & 1.1\\
BoN ($\eta=0.6$) & 1.34  & 3.36 & 84.02 & 0.87 & 1.57\\
SVDD-PM (\citeyear{Li2024Derivative-FreeDecoding}) & 1.44  & 1.81 & 10.93 & 0.99 & 1.6 \\ 
SVDD-PM ($\eta=0.6$)(\citeyear{Li2024Derivative-FreeDecoding}) & 1.60  & 4.16 & 96.52 & 0.82 & 3.5 \\ \cdashline{1-6}
DPS (\citeyear{Chung2023DiffusionProblems}) & 1.22  & 1.29 & 5.46 & 0.99 & 1.2\\
DPS ($\eta=0.6$)(\citeyear{Chung2023DiffusionProblems}) & 1.29  & 3.31 & 90.06 & 0.83 & 2.5\\
UG (\citeyear{Bansal2024UniversalModels}) & 1.39  & 4.27 & 91.13 & 0.82 & 2.9 \\
UG ($\eta=0.7$) & 1.37  & 4.43 & 103.6 & 0.79 & 3.5 \\
\hline
\rowcolor{tabBlue} \textbf{\ourmethod} & 1.34  & 1.49 & 7.40 & 1.0 & 1.6 \\ 
\rowcolor{tabBlue} \textbf{\ourmethod($\eta=0.6$)} & 1.52  & 3.64 & 84.45 & 0.86 & 3.2 \\ \hline
\end{tabular}
}
\vspace{-0.8em}
\end{table}



% \begin{wraptable}{r}{0.6\textwidth}
% \vspace{-1em}
% \caption{\small Quantitative metrics for face guidance.}\vspace{-1em}
% \label{tab:face}
% \def\arraystretch{1.15}
% \resizebox{1\linewidth}{!}{
% \begin{tabular}
% {l|c|c|c|c|c}
% \hline
% \multirow{2}{*}{\textbf{Method}} & \multicolumn{4}{c}{\textbf{R2: Face Guidance}}\\ \cline{2-6}  
%                                  & \scriptsize \textbf{Rew.} ($\uparrow$) & \scriptsize \textbf{FID} ($\downarrow$) & \scriptsize \textbf{CMMD} ($\downarrow$) & \scriptsize \textbf{T-CLIP} ($\uparrow$) & \scriptsize \textbf{Gram} ($\uparrow$) \\ \hline
% SDv1.5 \citeyear{Rombach2021High-ResolutionModels}                          & 0                               & 102.4                       & 0.05                         & 0.801 & 0.35 \\ \hline
% BoN \citeyear{Gao2022ScalingOveroptimization}                             & 0.26                            & 124.5                       & 0.14                         & 0.798 & 0.35 \\
% SVDD-PM \citeyear{Li2024Derivative-FreeDecoding}                         & 0.85                            & 145.8                       & 0.53                         & 0.780 & 0.26 \\ \cdashline{1-6}
% DPS \citeyear{Chung2023DiffusionProblems}                             & 0.14                            & 111.8                       & 0.08                         & 0.796 & 0.36 \\
% UG \citeyear{Bansal2024UniversalModels}              & 1.0                             & 173.5                       & 1.63                         & 0.686 & 0.37 \\
%  \hline
% \rowcolor{tabBlue} \textbf{\ourmethod (Ours)}       & 0.71                            & 127.6                       & 0.37                         & 0.786  & 0.32 \\
% \rowcolor{tabBlue} \textbf{\ourcondmethod (Ours)}   & 0.91                            & 190.7                       & 2.32                         & 0.708 & 0.67 \\ \hline
% \end{tabular}
% }
% \end{wraptable}

\begin{figure}[t]
    \centering
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs_revised/plots_revised_add/exp_rew-vs-fid_facedetector.pdf}
        % \caption{Caption}
        % \label{fig:enter-label}
    \end{subfigure}
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs_revised/plots_revised_add/exp_rew-vs-cmmd_facedetector.pdf}
        % \caption{Caption}
        % \label{fig:enter-label}
    \end{subfigure}
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs_revised/plots_revised_add//win_rate-vs-fid_facedetector.pdf}
        % \caption{Caption}
        % \label{fig:enter-label}
    \end{subfigure}
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs_revised/plots_revised_add/win_rate-vs-cmmd_facedetector.pdf}
        % \caption{Caption}
        % \label{fig:enter-label}
    \end{subfigure}
    \caption{Reward vs. divergence trade-off curves for face guidance.}
    \label{fig:face_trad}
    \vspace{-3pt}
\end{figure}


\begin{figure}[ht]    
    \centering
    \includegraphics[width=\linewidth]{figs_revised/add_figs/face_res_eta.pdf}
    \vspace{-0.3cm}
    % \caption{Same narrative as in Fig.~\ref{fig:style} with \ourcondmethod{} outperforming other baselines by a margin.}
    \caption{Quality evaluation across methods for face guidance.}
    \label{fig:face_eta}
    
    \vspace{-12pt}
\end{figure}

%\vspace{30cm}
\clearpage
\textbf{Face Guidance}. We summarize the results in Table~\ref{tab:face}. As the rewards are negative, we first compute the negative log of the reward values and then normalize it with respect to the base. 
%
\begin{figure}[t]    
    \centering
    \includegraphics[width=0.85\linewidth]{figs_revised/add_figs/face_res_noneta.pdf}
    \vspace{-0.3cm}
    % \caption{Same narrative as in Fig.~\ref{fig:style} with \ourcondmethod{} outperforming other baselines by a margin.}
    \caption{Quality evaluation across methods for face guidance without noise-conditioning.}
    \label{fig:face}
    
    \vspace{-12pt}
\end{figure}
%
%
\begin{table}[th]
% \vspace{-1em}
\centering
\caption{\small Quantitative metrics for face guidance.}\vspace{-1em}
\label{tab:face}
\def\arraystretch{1.15}
\resizebox{0.7\linewidth}{!}{
\begin{tabular}
{l|c|c|c|c|c}
\hline
\multirow{2}{*}{\textbf{Method}} & \multicolumn{4}{c}{\textbf{R2: Face Guidance}}\\ \cline{2-6}  
                                 & \scriptsize \textbf{Rew.} ($\uparrow$) & \scriptsize \textbf{FID} ($\downarrow$) & \scriptsize \textbf{CMMD} ($\downarrow$) & \scriptsize \textbf{T-CLIP} ($\uparrow$) & \scriptsize \textbf{I-Gram} ($\uparrow$) \\ \hline
Base-SD (\citeyear{Rombach2021High-ResolutionModels})                          & 1.0                               & 1.0                      & 1.0                         & 1.0 & 1.0 \\ \hline
SDEdit (\citeyear{Meng2021SDEdit:Equations}) & 0.99  & 1.79 & 34.91 & 0.89 & 1.74 \\
BoN (\citeyear{Gao2022ScalingOveroptimization})                             & 1.08                            & 1.22                       & 2.52                         & 0.99 & 1.0 \\
BoN ($\eta=0.7$)                             & 1.08                            & 1.82                       & 35.3                         & 0.88 & 1.8 \\
SVDD-PM (\citeyear{Li2024Derivative-FreeDecoding})                         & 1.42                            & 1.42                       & 9.67                         & 0.97 & 0.74 \\
SVDD-PM ($\eta=0.7$) (\citeyear{Li2024Derivative-FreeDecoding})                         & 1.70                            & 2.07                       & 48.22                         & 0.86 & 1.77 \\ \cdashline{1-6}
DPS (\citeyear{Chung2023DiffusionProblems})                             & 1.04                            & 1.09                       & 1.36                         & 0.99 & 1.03 \\
DPS ($\eta=0.7$) (\citeyear{Chung2023DiffusionProblems})                             & 1.21                            & 1.71                       & 33.21                         & 0.86 & 1.68 \\
UG (\citeyear{Bansal2024UniversalModels})              & 1.66                             & 1.69                       & 29.76                         & 0.86 & 1.06 \\
UG ($\eta=0.7$) & 1.77 & 1.94 & 61.27 & 0.85 & 1.45 \\
 \hline
\rowcolor{tabBlue} \textbf{\ourmethod}       & 1.30                            & 1.25                       & 6.76                         & 0.98 & 0.91 \\
\rowcolor{tabBlue} \textbf{\ourmethod($\eta=0.7$)}   & 1.5                            & 1.86                       & 42.40                         & 0.88 & 1.91 \\ \hline
\end{tabular}
}
\vspace{-12pt}
\end{table}
%
%

%
Compared to BoN, BoN($\eta$) and DPS, DPS($\eta$), \ourmethod{}, \ourmethod($\eta$) provides higher rewards but also with higher divergence (FID and CMMD). Although SVDD-PM, SVDD-PM($\eta$) and UG, UG($\eta$) achieve higher rewards, \ourmethod{}, \ourmethod($\eta$) offer a better trade-off in terms of FID, CMMD and T-CLIP. Moreover, \ourmethod($\eta$) offers the best image-alignment in terms of I-Gram as compared to all other baselines. 

Additionally, \ourmethod{}($\eta$) provides competitive results as compared to UG, which is the second-best method while offering better prompt alignment as reflected in a higher T-CLIP score. We draw similar conclusions from the reward vs. divergence curves presented in Fig.~\ref{fig:face_trad}, where \ourmethod($\eta$) achieves competitive rewards as compared to UG, UG($\eta$), SVDD-PM, SVDD-PM($\eta$), but on-par win rates as compared to UG, at the cost of slightly higher FID and CMMD scores.


% \begin{wraptable}{r}{0.6\textwidth}
% \vspace{-1em}
% \small
% \caption{\small Quantitative metrics for stroke generation.}\vspace{-1em}
% \label{tab:stroke}
% \def\arraystretch{1.15}
% \resizebox{1\linewidth}{!}{
% \begin{tabular}
% {l|c|c|c|c|c}
% \hline
% \multirow{2}{*}{\textbf{Method}} & \multicolumn{4}{c}{\textbf{R3: Stroke Generation}}\\ \cline{2-6}  
%                                  & \scriptsize \textbf{Rew.} ($\uparrow$) & \scriptsize \textbf{FID} ($\downarrow$) & \scriptsize \textbf{CMMD} ($\downarrow$) & \scriptsize \textbf{T-CLIP} ($\uparrow$)
%                                  & \scriptsize \textbf{Gram} ($\uparrow$)\\ \hline
% SDv1.5 \citeyear{Rombach2021High-ResolutionModels}                          & 50.7                                                    & 83.8                                                &  0.02                                                & 0.782 & 0.08 \\ \hline
% BoN \citeyear{Gao2022ScalingOveroptimization}                             & 63.2                                                    & 88.1                                                & 0.09                                                 & 0.779 & 0.09 \\
% SVDD-PM \citeyear{Li2024Derivative-FreeDecoding}                         & 79.1                                                    & 87.3                                                & 0.24                                                 & 0.778 & 0.11 \\ \cdashline{1-6}
% UG \citeyear{Bansal2024UniversalModels}              & 78.6                                                    & 233.4                                                & 1.56                                                 & 0.690 & 0.13 \\
% DPS \citeyear{Chung2023DiffusionProblems}                             & 68.0                                                    & 87.9                                                & 0.28                                                 & 0.764 & 0.09 \\ \hline
% \rowcolor{tabBlue} \textbf{\ourmethod (Ours)}       & 71.8                                                    & 65.8                                                & 0.13                                                 & 0.779 & 0.11 \\
% \rowcolor{tabBlue} \textbf{\ourcondmethod (Ours)}   & 88.8                                                    & 293.2                                                & 3.57                                                 & 0.685 & 0.34 \\ \hline
% \end{tabular}
% }
% \end{wraptable}


\clearpage
\textbf{Stroke}. As shown in Table.~\ref{tab:stroke}, among the sampling-based methods, \ourmethod{} provides better results than BoN in terms of expected reward and FID while maintaining the same T-CLIP score. Although UG and SVDD-PM offer higher rewards, \ourmethod{} offers lower divergence (FID and CMMD) and better T-CLIP scores. Overall, we observe that \ourmethod($\eta$) has the highest rewards while offering competitive FID, CMMD and T-CLIP. 

\begin{table}[h]
\small
\centering
\caption{\small Quantitative metrics for stroke generation.}\vspace{-1em}
\label{tab:stroke}
\def\arraystretch{1.15}
\resizebox{0.7\linewidth}{!}{
\begin{tabular}
{l|c|c|c|c|c}
\hline
\multirow{2}{*}{\textbf{Method}} & \multicolumn{4}{c}{\textbf{R3: Stroke Generation}}\\ \cline{2-6}  
                                 & \scriptsize \textbf{Rew.} ($\uparrow$) & \scriptsize \textbf{FID} ($\downarrow$) & \scriptsize \textbf{CMMD} ($\downarrow$) & \scriptsize \textbf{T-CLIP} ($\uparrow$)
                                 & \scriptsize \textbf{I-Gram} ($\uparrow$)\\ \hline
Base-SD (\citeyear{Rombach2021High-ResolutionModels}) & 1.0  & 1.0 & 1.0 & 1.0 & 1.0 \\ \hline
SDEdit (\citeyear{Meng2021SDEdit:Equations}) & 1.38 & 2.79 & 145.6 & 0.90 & 2.64 \\
BoN (\citeyear{Gao2022ScalingOveroptimization}) & 1.25  & 1.05 & 4.5 & 0.99 & 1.12 \\
BoN ($\eta=0.6$) & 1.55  & 3.12 & 170 & 0.89 & 3.05 \\
SVDD-PM (\citeyear{Li2024Derivative-FreeDecoding}) & 1.56  & 1.04 & 12.0 & 0.99 & 1.38 \\
SVDD-PM ($\eta=0.6$) (\citeyear{Li2024Derivative-FreeDecoding}) & 1.83  & 3.87 & 187.1 & 0.85 & 4.4 \\ \cdashline{1-6}
DPS (\citeyear{Chung2023DiffusionProblems}) & 1.34  & 1.04 & 14.0 & 0.97 & 1.13 \\
DPS ($\eta=0.6$) (\citeyear{Chung2023DiffusionProblems}) & 1.45  & 2.81 & 195.0 & 0.88 & 2.83 \\
UG (\citeyear{Bansal2024UniversalModels}) & 1.55  & 2.78 & 78.0 & 0.88 & 1.63 \\
UG ($\eta=0.6$) & 1.66  & 4.45 & 236.5 & 0.78 & 0.6 \\
\hline
\rowcolor{tabBlue} \textbf{\ourmethod} & 1.41  & 0.78 & 6.5 & 0.99 & 1.38 \\
\rowcolor{tabBlue} \textbf{\ourmethod($\eta=0.6$)} & 1.75  & 3.50 & 178.5 & 0.87 & 4.25 \\ \hline
\end{tabular}
}
\vspace{-12pt}
\end{table}
%
\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\linewidth]{figs_revised/add_figs/stroke_res_noneta.pdf}
    \vspace{-0.3cm}
    \caption{Quality evaluation across methods for stroke guidance without noise-conditioning.}
    \label{fig:stroke}
    \vspace{-.1in}
\end{figure}
%
% \textbf{Compression}. To expand our evaluation landscape, we address scenarios where the strong assumption of a differentiable guidance function is relaxed. To this end, we use the compressibility reward score which is measured by the size of the JPEG image in kilobytes, as is proposed in \cite{}. By using this reward, we guide the diffusion denoising process to generate memory-light, compressible images. 

% %
% \begin{wraptable}{r}{0.6\textwidth}
% \vspace{-1em}
% \small
% \caption{\small Quantitative metrics for compression reward.}\vspace{-1em}
% \label{tab:stroke}
% \def\arraystretch{1.15}
% \resizebox{1\linewidth}{!}{
% \begin{tabular}
% {l|c|c|c|c|c}
% \hline
% \multirow{2}{*}{\textbf{Method}} & \multicolumn{4}{c}{\textbf{R4: Compression}}\\ \cline{2-6}  
%                                  & \scriptsize \textbf{Rew.} ($\uparrow$) & \scriptsize \textbf{FID} ($\downarrow$) & \scriptsize \textbf{CMMD} ($\downarrow$) & \scriptsize \textbf{T-CLIP} ($\uparrow$)
%                                  & \scriptsize \textbf{I-Gram} ($\uparrow$)\\ \hline
% Base-SD (\citeyear{Rombach2021High-ResolutionModels}) & 1.0  & 1.0 & 1.0 & 1.0 & 1.0 \\ \hline
% C-BoN & 1.08  & 2.27 & 31 & 0.98 & 1.32 \\
% C-SVDD-PM & 1.48  & 3.54 & 69.5 & 0.89 & 1.15\\ 
% SDEdit (\citeyear{Meng2021SDEdit:Equations}) & 0.97 & 2.19 &	29.25 &	0.98 &	1.34
%  \\
% \hline
% \rowcolor{tabBlue} \textbf{\ourcondmethod (Ours)} & 1.34 &	3.08 &	48.75 &	0.97 &	1.20
%  \\ \hline
% \end{tabular}
% }
% \vspace{-12pt}
% \end{wraptable}
% %
% 


\clearpage


\textbf{Computation Complexity.} We present a breakdown of the computational complexities of all baselines across each of the guidance scenarios. DPS is considerably faster across all three generation scenarios among the gradient-based guidance methods. This is due to the \(m\) gradient and \(K\) refinement steps used in UG, which are not used in DPS. The difference is more pronounced in the case of style- and stroke guidance as UG uses a higher number of gradient steps \(m\). Further, among the sampling-based approaches, SVDD-PM is an order of magnitude slower than BoN as it applies token-wise guidance. On the contrary, our blockwise approaches \ourmethod{}, \ourmethod($\eta$) are more efficient than UG and SVDD-PM and closely follow BoN.

\begin{table}[h]
\caption{Computational Complexity} \vspace{-0.5em}
\label{tab:complexity-appendix}
  \centering
  % \aboverulesep = 0pt 
  % \belowrulesep = 0pt
  \renewcommand{\arraystretch}{1.2}
  \resizebox{0.7\linewidth}{!}{
  % \scriptsize
  \small
  \begin{tabular}{l|c|c|ccc}
    \hline
    \multirow{2}{*}{\textbf{Methods}} & \multirow{2}{*}{\textbf{Inf. Steps}} & \multirow{2}{*}{\textbf{Rew. Queries}} & \multicolumn{3}{c}{\textbf{Runtime [sec/img]}} \\
    \cline{4-6}
    & & & Style & Face & Stroke \\
    \hline
    Base-SD \citeyear{Rombach2021High-ResolutionModels} & \(T\) & - & 14.12 & 14.12 & 14.12 \\ \hline
    BoN \citeyear{Gao2022ScalingOveroptimization} & $NT$ & \(N\) & 266.02 & 268.43 & 265.86 \\
    SVDD-PM \citeyear{Li2024Derivative-FreeDecoding} & $NT$ & \(NT\) & 1168.74 & 1859.67 & 1169.68\\
    \cdashline{1-6}
    DPS \citeyear{Chung2023DiffusionProblems} & $T$ & \(T\) & 62.52 & 122.21 & 61.83\\
    UG \citeyear{Bansal2024UniversalModels} & $mKT$ & \(mKT\) & 1588.41 & 543.12 & 1592.89\\
     \hline
    \rowcolor{tabBlue}CoDe & \(NT\) & \(NT/B\) & 441.81 & 583.12 & 442.08\\
     \rowcolor{tabBlue}CoDe($\eta$) & \(N\eta\,T\) & \(N\eta\,T/B\) & 331.42 & 403.19 & 274.56\\
    \hline
  \end{tabular}
  }
\end{table}
% \begin{table}
% \vspace{-1em}
% \caption{\small Quantitative metrics for style guidance.}\vspace{-1em}
% \label{tab:style}
% \def\arraystretch{1.15}
% \resizebox{1\linewidth}{!}{
% \begin{tabular}
% {l|c|c|c|c|c}
% \hline
% \multirow{2}{*}{\textbf{Method}} & \multicolumn{4}{c}{\textbf{R1: Style Guidance}}\\ \cline{2-6}  
% & \scriptsize \textbf{Rew.} ($\uparrow$) & \scriptsize \textbf{FID} ($\downarrow$) & \scriptsize \textbf{CMMD} ($\downarrow$) & \scriptsize \textbf{T-CLIP} ($\uparrow$) & \scriptsize \textbf{I-Gram} ($\uparrow$) \\  \hline
% % Base-SD \citeyear{Rombach2021High-ResolutionModels}                          & 57.8                            & 82.3                        & 0.04                         & 0.810 & 0.10 \\ 
% Base-SD (\citeyear{Rombach2021High-ResolutionModels}) & 1.0  & 1.0 & 1.0 & 1.0 & 1.0 \\ \hline
% BoN (\citeyear{Gao2022ScalingOveroptimization}) & 1.14  & 1.30 & 2.25 & 0.99 & 1.1\\
% SVDD-PM (\citeyear{Li2024Derivative-FreeDecoding}) & 1.44  & 1.81 & 10.93 & 0.99 & 1.6 \\ \cdashline{1-6}
% DPS (\citeyear{Chung2023DiffusionProblems}) & 1.22  & 1.29 & 5.46 & 0.99 & 1.2\\
% UG (\citeyear{Bansal2024UniversalModels}) & 1.39  & 4.27 & 91.13 & 0.82 & 2.9 \\ 
% % SDEdit (\citeyear{Meng2021SDEdit:Equations}) & 70.84  & 267.50 & 2.71 & 0.73 & 0.28 \\
% SDEdit (\citeyear{Meng2021SDEdit:Equations}) & 1.22  & 3.25 & 67.75 & 0.90 & 2.8 \\ \hline
% \rowcolor{tabBlue} \textbf{\ourmethod (Ours)} & 1.34  & 1.49 & 7.40 & 1.0 & 1.6 \\ 
% \rowcolor{tabBlue} \textbf{\ourcondmethod (Ours)} & 1.52  & 3.64 & 84.45 & 0.86 & 3.4 \\ \hline
% \end{tabular}
% }
% \vspace{-0.8em}
% \end{table}
\clearpage 

\section{Details for Estimating KL Divergence }
\label{sec:kl_comp_details}
To compute the KL divergence between the guided and the base diffusion model, we draw on some existing results that give us an upper bound on the KL divergence between \ourmethod{} and the base diffusion model, which is given by the following:
\begin{lemma}
    \begin{equation}
    \textit{KL}(\ourmethod(N,B) \, \| \, \text{Base}) \leq  \left(\log N - \frac{N-1}{N}\right) \times \frac{T}{B}.    
\end{equation}
\label{lem:KL-bound}
\end{lemma}
\begin{proof}
    The proof follows the same lines as \citep[Theorem B.1]{Beirami2024TheoreticalPolicy}, with the exception that we need to resort to \citep[Theorem 1]{mroueh2024information} to bound the KL divergence of each intervention.
\end{proof}
For BoN where the block size $B = T$, the KL divergence is upper bounded by $$\textit{KL}(\text{BoN}(N) \, \| \, \text{Base}) \leq \log N  - \frac{N-1}{N},$$ which is directly implied by \citep[Theorem 1]{mroueh2024information} as well.  For SVDD-PM where $B=1$, the KL divergence is upper bounded by $$\textit{KL}(\text{SVDD-PM}(N) \, || \, \text{Base}) \leq \big(\log N - \frac{N-1}{N}\big)\times T.$$ Since the noise-conditioned variants of these methods only denoise for $\eta T$ steps instead of the full $T$ steps, the KL divergences are upper bounded using 
\begin{align}
    \textit{KL}(\ourmethod(N,B,\eta) \, \| \, \text{Base}) &\leq  \left(\log N - \frac{N-1}{N}\right) \times \frac{\eta T}{B}  ,  \\
    \textit{KL}(\text{BoN}(N,\eta) \, \| \, \text{Base}) &\leq \log N - \frac{N-1}{N} ,\\
    \textit{KL}(\text{SVDD-PM}(N, \eta) \, \| \, \text{Base}) &\leq \left(\log N - \frac{N-1}{N}\right) \times \eta T.
\end{align}


\subsection{Numerical computation of KL divergence for Gaussian models (Case Study I)}
In Section~\ref{sec:toy}, to estimate the KL divergence between the base and guided models, we first generate $1000$ samples from the base diffusion model and the reward guided model each. Then assuming Gaussian densities for both, we compute the mean and variance for each of the distributions and then use the closed-form expression to calculate the KL divergence between two Gaussians. We notice that in this setting when we reach the degeracy limit, the bounds suggested by Lemma~\ref{lem:KL-bound} are loose, particularly for all SVDDPM experiments in Section~\ref{sec:toy}. This is a known issue with these KL bounds and has been discussed by~\citet{beirami2024theoretical}.

\clearpage
\section{Reward Over-Optimization in Compression Guidance for SVDD-PM($\bf \eta$)}
\label{sec:rhack}
Following section \ref{sec:nondiff}, Fig.~\ref{fig: comp_vs_kl}, we demonstrate a few images generated in the compressibility guidance scenario with SVDD-PM($\eta$) for $N=[20,30,40,100]$, where reward over-optimization occurs. As can be seen in Fig.~\ref{fig:svddpmrh}, higher values of $N$ for SVDD-PM($\eta$)'s guidance lead to degenerate generation of images, where the text prompt and reference image alignment is compromised at the cost of high compressibility reward. The generated images roughly follow the color palette of the reference image but fail to meaningfully incorporate the style and aesthetics of the reference image. Moreover, the images also do not resemble natural images, empirically corroborating the high KL-divergence w.r.t. the base distribution in Fig.~\ref{fig: comp_vs_kl}.
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.85\linewidth]{figs_revised/add_figs/compress_svdd-pm_eta_rhack.pdf}
    \caption{Qualitative examples of reward over-optimized images from SVDD-PM($\eta$) for $N=[20,30,40,100]$, in the compression guidance scenario.}
    \label{fig:svddpmrh}
\end{figure}

\clearpage

\section{UG with a high guidance scale offers low text alignment}
\label{sec:ugrh}
Following section \ref{sec:ablations}, Fig.~\ref{fig: ablation}, we illustrate a few generated samples of UG across four settings for style guidance with higher guidance scales of $12$ and $24$ to qualitatively corroborate their low text-alignment. As can be seen in Fig.~\ref{fig:ug_rh}, the generated images offer high alignment with respect to the reference image but fail to incorporate any meaningful features of the text prompts. None of the generated images resemble the Eiffel tower or the portrait of a woman.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.85\linewidth]{figs_revised/add_figs/ug_rhack.pdf}
    \caption{Qualitative examples of low text-alignment (T-CLIP) for UG with higher guidance scales, in the style guidance scenario.}
    \label{fig:ug_rh}
\end{figure}
\clearpage

\section{Miscellaneous Results}
In this section, we illustrate several additional generated images across all baselines and guidance scenarios. We also provide additional results for \ourmethod, \ourmethod($\eta$) across various different reference images and text prompt pairs, that are different from the ones already explored in the main manuscript, in Figs.~\ref{fig:style2},~\ref{fig:extra1},~\ref{fig:extra2}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.85\linewidth]{figs_revised/add_figs/style_extra.pdf}
    \caption{Quality evaluation across methods for style guidance on additional settings without noise-conditioning.}
    \label{fig:style2}
\end{figure}

% To broaden the understanding of our proposed approach \ourcondmethod{}, we utilize only the noise-conditioning aspect of \ourcondmethod{} to generate multiple images across all the style guidance (reference image, text prompt) settings. As can be seen in Figs.~\ref{fig:all_dump0} only using reference image noise conditioning can also be used as a naive baseline for guided image generation. However, it is to be noted that using \ourmethod{} in conjunction with noise-conditioning, as demonstrated with \ourcondmethod{}, renders more sophisticated results in terms of capturing the nuances and subtleties of the reference image, while incorporating the semantics of the text prompt.

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=1\linewidth]{fullpage/res0.pdf}
%     \caption{Multiple generated samples for the text prompt \texttt{A colorful photo of eiffel tower}.}
%     \label{fig:all_dump0}
% \end{figure}

% \clearpage

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{figs_revised/style_app_add.pdf}
    \caption{Quality evaluation of \ourmethod{} for style guidance on additional settings.}
    \label{fig:extra1}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{figs_revised/face_app_add.pdf}
    \caption{Quality evaluation of \ourmethod{} for face guidance on additional settings.}
    \label{fig:extra2}
\end{figure}

% \begin{table}  \caption{Computational Complexity}
% \vspace{-0.1cm}
%   \centering
%   % \aboverulesep = 0pt 
%   % \belowrulesep = 0pt
%   \renewcommand{\arraystretch}{1.2}
%   \resizebox{1\linewidth}{!}{
%   \small
%   \begin{tabular}{l|c|c|ccc}
%     \hline
%     \multirow{2}{*}{\textbf{Methods}} & \multirow{2}{*}{\textbf{Inf. Steps}} & \multirow{2}{*}{\textbf{Rew. Queries}} & \multicolumn{3}{c}{\textbf{Runtime [sec/img]}} \\
%     \cline{4-6}
%     & & & Style & Face & Stroke \\
%     \hline
%     Base-SD \citeyear{Rombach2021High-ResolutionModels} & \(T\) & - & \(1\times\) & \(1\times\) & \(1\times\) \\ \hline
%     BoN \citeyear{Gao2022ScalingOveroptimization} & $NT$ & \(N\) & \(18\times\) & \(19\times\) & \(19\times\) \\
%     SVDD-PM \citeyear{Li2024Derivative-FreeDecoding} & $NT$ & \(NT\) & \(83\times\) & \(132\times\) & \(83\times\)\\
%     \cdashline{1-6}
%     DPS \citeyear{Chung2023DiffusionProblems} & $T$ & \(T\) & \(4\times\) & \(9\times\) & \(4\times\)\\
%     UG \citeyear{Bansal2024UniversalModels} & $mKT$ & \(mKT\) & \(112\times\) & \(38\times\) & \(112\times\)\\
%      \hline
%     \rowcolor{tabBlue}CoDe (Ours) & \(NT\) & \(NT/B\) & \(31\times\) & \(41\times\) & \(31\times\)\\
%      \rowcolor{tabBlue}C-CoDe (Ours) & \(NT\) & \(rNT/B\) & \(23\times\) & \(28\times\) & \(19\times\)\\
%     \hline
%   \end{tabular}
%   }
% \end{table}

% \clearpage

% \begin{figure}
%     \centering
%     \begin{subfigure}{0.45\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/plots_style/win_rate-vs-cmmd_styletransfer_div.pdf}
%         % \label{fig:enter-label}
%     \end{subfigure}
%     \begin{subfigure}{0.45\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/plots_style/win_rate-vs-fid_styletransfer_div.pdf}
%         % \label{fig:enter-label}
%     \end{subfigure}
%     % \begin{subfigure}{0.45\textwidth}
%     %     \centering
%     %     \includegraphics[width=\linewidth]{figures/plots_style/win_rate-vs-cmmd_styletransfer_div2.pdf}
%     %     % \label{fig:enter-label}
%     % \end{subfigure}
%     % \begin{subfigure}{0.45\textwidth}
%     %     \centering
%     %     \includegraphics[width=\linewidth]{figures/plots_style/win_rate-vs-fid_styletransfer_div2.pdf}
%     %     % \label{fig:enter-label}
%     % \end{subfigure}
%     \caption{win rate vs Divergence (Style Guidance)}
%     % \label{fig:enter-label}
% \end{figure}

% \begin{figure}
%     \centering
%     \begin{subfigure}{0.45\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/exp_rew-vs-cmmd_facedetector_upd.pdf}
%         % \label{fig:enter-label}
%     \end{subfigure}
%     \begin{subfigure}{0.45\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/exp_rew-vs-fid_facedetector_upd.pdf}
%         % \label{fig:enter-label}
%     \end{subfigure}
%     \begin{subfigure}{0.45\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/win_rate-vs-cmmd_facedetector_upd.pdf}
%         % \label{fig:enter-label}
%     \end{subfigure}
%     \begin{subfigure}{0.45\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/win_rate-vs-fid_facedetector_upd.pdf}
%         % \label{fig:enter-label}
%     \end{subfigure}
%     % \begin{subfigure}{0.45\textwidth}
%     %     \centering
%     %     \includegraphics[width=\linewidth]{figures/exp_rew-vs-clipwinrate_facedetector_text.pdf}
%     %     % \label{fig:enter-label}
%     % \end{subfigure}
%     \caption{Task: Face Recognition Guidance}
%     % \label{fig:enter-label}
% \end{figure}

% \begin{figure}
%     \centering
%     \begin{subfigure}{0.32\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/plots_style/exp_rew-vs-clipscore_styletransfer_t2i.pdf}
%         % \label{fig:enter-label}
%     \end{subfigure}
%     \begin{subfigure}{0.32\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/plots_style/exp_rew-vs-cmmd_styletransfer_div.pdf}
%         % \label{fig:enter-label}
%     \end{subfigure}
%     \begin{subfigure}{0.32\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/plots_style/exp_rew-vs-fid_styletransfer_div.pdf}
%         % \label{fig:enter-label}
%     \end{subfigure}
%     \begin{subfigure}{0.32\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/plots_style/exp_rew-vs-clipscore_styletransfer_t2i2.pdf}
%         % \label{fig:enter-label}
%     \end{subfigure}
%     \begin{subfigure}{0.32\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/plots_style/exp_rew-vs-cmmd_styletransfer_div2.pdf}
%         % \label{fig:enter-label}
%     \end{subfigure}
%     \begin{subfigure}{0.32\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/plots_style/exp_rew-vs-fid_styletransfer_div2.pdf}
%         % \label{fig:enter-label}
%     \end{subfigure}
%     \caption{Expected Rewards vs Divergence (Style Guidance)}
%     % \label{fig:enter-label}
% \end{figure}

\end{document}
