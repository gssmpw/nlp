%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

%In case you encounter the following error:
%Error 1010 The PDF file may be corrupt (unable to open PDF file) OR
%Error 1000 An error occurred while parsing a contents stream. Unable to analyze the PDF file.
%This is a known problem with pdfLaTeX conversion filter. The file cannot be opened with acrobat reader
%Please use one of the alternatives below to circumvent this error by uncommenting one or the other
\pdfobjcompresslevel=0
\pdfminorversion=4

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed

\usepackage{multirow}
\usepackage{booktabs}
\usepackage{array}
\usepackage[table]{xcolor}
\usepackage{graphicx}
\usepackage{makecell}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm}
% \usepackage{algorithmic}
\usepackage{algpseudocode} % Use this for line numbering and better formatting
\usepackage{fontawesome5}
\usepackage{hyperref}
\usepackage{subcaption}



\definecolor{mygray}{gray}{0.9}



\title{\LARGE \bf
Classifying the Stoichiometry of Virus-like Particles with Interpretable Machine Learning
}



\author{Jiayang Zhang, Xianyuan Liu, Wei Wu, Sina Tabakhi, Wenrui Fan, Shuo Zhou, \\ Kang Lan Tee, Tuck Seng Wong, Haiping Lu %, \IEEEmembership{Senior Member, IEEE}
% <-this % stops a space
\thanks{This research is funded by the Department of Health and Social Care using UK Aid funding and is managed by the EPSRC. The views expressed in this publication are those of the author(s) and not necessarily those of the Department of Health and Social Care. This work is also supported by donations from D. Naik and S. Naik.}
\thanks{J. Zhang, X. Liu, S. Tabakhi, W. Fan, S. Zhou and H. Lu are with the Centre for Machine Intelligence and School of Computer Science, University of Sheffield, United Kingdom \{\tt\small jiayang.zhang, xianyuan.liu, stabakhi1, wenrui.fan, shuo.zhou, h.lu@sheffield.ac.uk\}}%
\thanks{W. Wu, K. L. Tee, T. S. Wong are with the School of Chemical, Materials and Biological Engineering, University of Sheffield, United Kingdom \{\tt\small wwu37, k.tee, t.wong@sheffield.ac.uk\}}%
}



\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
Virus-like particles (VLPs) are valuable for vaccine development due to their immune-triggering properties. Understanding their stoichiometry, the number of protein subunits to form a VLP, is critical for vaccine optimisation. However, current experimental methods to determine stoichiometry are time-consuming and require highly purified proteins. To efficiently classify stoichiometry classes in proteins, we curate a new dataset and propose an interpretable, data-driven pipeline leveraging linear machine learning models. We also explore the impact of feature encoding on model performance and interpretability, as well as methods to identify key protein sequence features influencing classification. The evaluation of our pipeline demonstrates that it can classify stoichiometry while revealing protein features that possibly influence VLP assembly. The data and code used in this work are publicly available at \href{https://github.com/Shef-AIRE/StoicIML}{https://github.com/Shef-AIRE/StoicIML}.

% \newline

\indent \textit{Clinical relevance}— Accurately classifying VLP stoichiometry can streamline vaccine design and accelerate the development of vaccines against diseases.

\indent \textit{Index Terms}— Virus-like particles, protein stoichiometry, machine learning, interpretability
\end{abstract}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION}
% \textbf{Importance of VLP:}
Virus-like particles (VLPs) are self-assembling nanovesicles that mimic viral particles in shape and size but lack genetic material, making them non-infectious. Their ability to trigger immune responses makes them valuable for vaccines and antigen display, with several VLP-based vaccines already in use for hepatitis B, human papillomavirus, and hepatitis E \cite{kheirvari2023virus}. 
% \textbf{Importance of stoichiometry:}
Self-assembly dictates VLP stoichiometry - the number of subunits forming a particle - affecting size and surface decoration, which are critical for vaccine optimization. Predicting stoichiometry can streamline VLP design. 
% \textbf{Motivation for developing data-driven method:}
Existing techniques, such as analytical ultracentrifugation \cite{lebowitz2002modern}, light scattering methods \cite{some2013light}, 
% cryo-EM, 
and mass photometry \cite{young2018quantitative}, require purified proteins and months of work \cite{soltermann2021label}, highlighting the need for faster, data-driven approaches.

% \textbf{Advancements in protein science driven by ML:}
In recent years, machine learning (ML) techniques have been increasingly applied to tackle complex challenges in protein science. Key examples include the AlphaFold series \cite{jumper2021highly,abramson2024accurate} for protein structure prediction,  and the DeepGo \cite{kulmanov2020deepgoplus} and the DPFunc \cite{wang2025dpfunc} for protein function prediction. However, the potential of ML methods for protein stoichiometry classification has not been explored, to the best of our knowledge.

Exploring this new problem first requires identifying and compiling a dataset of VLP-forming proteins, leveraging resources such as the RCSB Protein Data Bank (PDB) \cite{berman2000protein}. Equally important is the interpretability of the method, which not only offers biological insights by linking specific sequence features to classification outcomes but also enhances reliability and transparency, enabling these insights to be validated within a biological context.

To satisfy these criteria, we develop an interpretable ML pipeline for stoichiometry classification, as illustrated in Fig. \ref{fig:ML workflow}. We enforce interpretability at three \textbf{S}tages: feature encoding, model training, and evaluation. 

\begin{figure*}[!t]
    \centering
    \includegraphics[width=1\linewidth]{figures/VLP-workflow.pdf}
    % \caption{ML Workflow}
    \caption{Overview of the development of an interpretable data-driven pipeline for VLP stoichiometry classification. Protein sequences from the PDB bank are curated into a balanced dataset of 60-mers and 180-mers, and subsequently processed through the classification pipeline: feature encoding with selected maps and methods, model training using linear machine learning models, and evaluation based on performance metrics, model interpretation and biological analysis.}
    \label{fig:ML workflow}
\end{figure*}


\begin{itemize}
    \item \textbf{S1:} Feature encoding converts protein sequences into numerical representations for ML classification. While integer-label encoding is commonly used in existing studies \cite{ozturk2018deepdta,bai2023interpretable}, it may introduce artificial relationships between amino acids.
    % , reducing interpretability and accuracy.
    To address this, we propose one-hot encoding as a more interpretable alternative and evaluate its impact on stoichiometry classification. Additionally, we explore the effects of encoding maps, comparing representations using individual amino acids versus amino acid clusters, to assess how these choices influence classification performance.
    \item \textbf{S2:} Model training should consider transparent decision-making mechanisms for stoichiometry classification. Linear ML models should be prioritised for their capacity to clearly elucidate feature contributions.
    \item \textbf{S3:} The model output should provide explainable insights into how specific protein features contribute to their stoichiometry classification. 
    % Therefore, the evaluation of outputs should go beyond classification metrics to include interpretable biological relevance, ensuring that the results align with real-world biological properties.
    Therefore, evaluation should extend beyond standard classification metrics to assess biological relevance, ensuring that results align with real-world biological properties. This requires methods to identify key protein features driving classification, followed by biological analysis and interpretation of these features to derive meaningful insights.
\end{itemize}

% In contrast, data-driven protein stoichiometry prediction can offer a transformative advantage, providing results with greater speed, reduced resource demands, and potentially improved accuracy. Additionally, predicted stoichiometry can serve as a crucial input parameter for structural modelling of protein complexes using AlphaFold \cite{abramson2024accurate}, further enhancing our ability to predict and understand protein structures. 


Our study focuses specifically on homomeric VLPs, i.e., those formed from a single type of protein subunit. From a protein manufacturing perspective, these VLPs are the easiest to produce, offering a significant advantage in rapidly responding to epidemics or pandemics. Our preliminary analysis of protein structures in the PDB revealed that most homomeric VLPs adopt either a 60-mer or 180-mer stoichiometry. 
% As a first step in developing our AI algorithm,
% Therefore, as a foundational step in developing ML methods, we address the most straightforward task: a binary classification of protein sequences into either the 60-mer or 180-mer category. The high prevalence of these structures in the PDB provides ample training data, reducing the risk of overfitting and facilitating robust comparisons between encoding methods and ML models.
The high prevalence of these structures provides ample training data, reducing the risk of overfitting and facilitating robust comparisons between encoding methods and ML models.
Therefore, as a foundational step in developing ML models, we focus on the most straightforward task: binary classification of protein sequences into either the 60-mer or 180-mer category.


% Why classify stoichiometry of 60/180: This study focuses on classifying VLPs with 180 and 60 stoichiometries, for two reasons. First, these stoichiometries are the most accessible and widely studied, offering abundant data for training ML (ML) models. This data availability reduces overfitting risks, allowing meaningful comparisons between ML (ML) models and encoding methods. Second, 180 and 60 stoichiometries are among the most common VLP formations in nature, making them biologically significant. Their structural stability and immunogenic properties are particularly advantageous for applications such as vaccine design.


Our work makes three key contributions. First, we curate a VLP stoichiometry dataset from the PDB and make it publicly available at \href{https://github.com/Shef-AIRE/StoicIML}{https://github.com/Shef-AIRE/StoicIML}. Second, we develop an interpretable ML pipeline for stoichiometry classification. We compare the encoding methods, encoding maps, and linear ML methods to highlight their relative performance, strengths, and limitations. Third, we propose a method to identify key protein sequence features that drive classification and offer biological interpretations of the results.






% \noindent Contribution:
% \begin{itemize}
%     \item New dataset: Curated VLP stoichiometry dataset
%     \item Method: Built a pipeline for understanding the protein features that determine the dimensions of VLP which is a problem that has not been previously studied/first exploration of ML into this field
%     \item Analysis and interpretation: Possible methods for selection important protein features (Jiayang); ML output implies protein properties in VLP formation (Tuck)
%     \item Encoding method comparison: performance and interpretation
% \end{itemize}

\section{METHOD}

In this section, we describe the data collection in Sec. \ref{sec: Data collection} and outline the three stages of the interpretable pipeline: feature encoding in Sec \ref{sec:Feature encoding}, model training and evaluation in Sec. \ref{sec:Model training and evaluation}. The overall workflow of the proposed pipeline is illustrated in Fig. \ref{fig:ML workflow}.

\subsection{Data collection} \label{sec: Data collection}
We compile a dataset of proteins that assemble into either 60-mer or 180-mer VLPs, sourced from the RCSB PDB \cite{berman2000protein}. The protein sequences are retrieved through the advanced search function in the PDB. Specifically, we set the ``Number of Protein Instances (Chains) per Assembly'' to 60 for 60-mers and 180 for 180-mers under ``Structure Attribute'' and ``Assembly Features''. We then refine the search by selecting ``Icosahedral'' as ``Symmetry Type''. 
Finally, we manually remove duplicate sequences to ensure a balanced dataset and to prevent redundancy and potential bias.
% To ensure a balanced dataset, duplicate sequences were manually removed to prevent redundancy and potential bias. The dataset is publicly available on GitHub.

The dataset consists of 200 protein sequences, with 100 sequences selected for each stoichiometry of 60 and 180. By balancing the dataset, we mitigate potential issues arising from imbalanced learning and enable a clearer interpretation of model performance across stoichiometries. Table \ref{tab: distribution by protein lengths} illustrates the distribution of protein lengths and stoichiometries.

% \begin{table}[!t]
%     \centering{
%     \caption{Dataset distribution by protein lengths} 
%     \resizebox{0.5\textwidth}{!}
%     {
%     \begin{tabular}{p{0.5cm}<{\centering} p{0.8cm}<{\centering} p{0.3cm}<{\centering} p{1.3cm}<{\centering} p{1.3cm}<{\centering} p{0.5cm}<{\centering}}
%     \toprule \toprule
%         \multirow{2}{*}[-0.8ex]{Stoi.} & \multirow{2}{*}[-0.8ex]{\shortstack{Protein\\ count}} & \multicolumn{4}{c}{Protein count by length}  \\
%         \cmidrule{3-6}
%         ~ & ~ & $\leq$200 & 200-400 & 400-600 & $>$600 \\
%     \midrule
%         \rowcolor{white}\cellcolor{white}
%         60 & 100 & 29 & 28 & 28 & 15 \\ 
%         180 & 100 & 40 & 40 & 17 & 3 \\ 
%     \bottomrule \bottomrule
%     \end{tabular}
%     \label{tab: distribution by protein lengths}
%     }
%     }
% \end{table}


\begin{table}[!t]
    \centering{
    \caption{Dataset distribution by protein lengths} 
    \resizebox{0.48\textwidth}{!}
    {
    \begin{tabular}{cccccc}
    \toprule \toprule
        \multirow{2}{*}[-0.8ex]{~} & \multirow{2}{*}[-0.8ex]{\shortstack{Protein\\ count}} & \multicolumn{4}{c}{Protein count by length}  \\
        \cmidrule{3-6}
        ~ & ~ & $\leq$200 & 200-400 & 400-600 & $>$600 \\
    \midrule
        \rowcolor{white}\cellcolor{white}
        60-mer & 100 & 29 & 28 & 28 & 15 \\ 
        180-mer & 100 & 40 & 40 & 17 & 3 \\ 
    \bottomrule \bottomrule
    \end{tabular}
    \label{tab: distribution by protein lengths}
    }
    }
\end{table}

\subsection{Feature encoding} \label{sec:Feature encoding}
Each protein sequence is represented as $P = (a_1, ..., a_n)$, where each $a_i$ is a one-letter amino acid symbol. To standardise the input, we pad with zeros or truncate each sequence to a fixed maximum length $m$, resulting in $P = (a_1, ..., a_m)$. Each amino acid symbol $a_i$ is then encoded as a token using a chosen encoding method.

% To evaluate the impact of different sequence encoding strategies on model performance, we use two commonly adopted methods: integer label encoding and one-hot encoding. 
We evaluate two common encoding strategies, integer label encoding and one-hot encoding, to assess their impact on model performance.
% Integer label encoding provides a memory-efficient representation by mapping each amino acid to a unique integer.
Integer label encoding is memory efficient because it maps each amino acid symbol to a unique integer. However, it may mislead the classifier into assuming a false hierarchy. For example, if valine is encoded as 22 and alanine as 1 in integer-label encoding, the classifier may incorrectly interpret valine as ``greater than'' alanine, even though this numerical difference has no meaningful biological significance for the classification task. 
% In contrast, one-hot encoding explicitly preserves categorical distinctiveness by assigning each amino acid a unique binary vector. 
In contrast, one-hot encoding assigns each amino acid symbol a unique binary vector. This encoding preserves the distinct identity of amino acids while avoiding the introduction of artificial ordinal relationships. However, its high dimensionality increases computational demands.
% While this approach avoids introducing artificial ordinal relationships, it significantly increases computational overhead due to its higher dimensionality.

For encoding maps, we use CHARPROTSET \cite{ozturk2018deepdta} and predefined knowledge-based clusters \cite{wong2006statistical}. CHARPROTSET maps a unique integer to each of the 25 distinct amino acid symbols \cite{ozturk2018deepdta}. In contrast, the knowledge-based clusters group the 20 canonical amino acids and five special cases into six categories based on the chemical properties of their side chains \cite{wong2006statistical}. These groups are defined as follows: aliphatic (G, A, V, L, I), aromatic (F, Y, W), neutral (C, M, P, S, T, N, Q), positively charged (H, K, R), negatively charged (D, E) and special cases (B, X, O, U, Z). 

% captures subtle variations in each amino acid
The CHARPROTSET encoding map captures individual residue information, making it ideal for tasks driven by specific amino acid differences. In contrast, cluster encoding reduces feature dimensionality, which improves computational efficiency. It also embeds biological knowledge by grouping amino acids, enabling higher-level pattern identification. This may reveal broader trends, such as the influence of the chemical properties of amino acid side chains on stoichiometry classification.


\subsection{Model training and evaluation} \label{sec:Model training and evaluation}
Linear classifiers are chosen for their interpretability in revealing how individual features contribute to classification decisions. We employ three standard classifiers: logistic regression (LR), linear support vector classifier (SVC) and ridge classifier (RC). LR estimates the probability of a binary outcome using the logistic function. The linear support vector classifier identifies the optimal hyperplane that maximises the margin between classes. Finally, the RC is a linear model that employs L2 regularisation, as used in ridge regression, to solve classification problems by encoding class labels as numerical targets.


We evaluate the classification performance of the pipeline using five metrics. The Area Under the Receiver Operating Characteristic Curve (AUROC) is the primary measure, which provides a comprehensive evaluation of the model's ability to distinguish between classes across all classification thresholds \cite{fawcett2006introduction}. In addition, we report sensitivity, specificity, precision, and negative predictive value (NPV) to provide a well-rounded evaluation \cite{sokolova2009systematic}. 

For this analysis, `true positives' refer exclusively to 180-mer protein sequences, and `true negatives' to 60-mer protein sequences. Sensitivity measures the proportion of true 180-mers correctly identified. Specificity reflects how well the model correctly classifies 60-mers. Precision indicates the proportion of predicted 180-mers that are truly 180-mers. NPV measures the proportion of predicted 60-mers that are truly 60-mers. Consequently, sensitivity, specificity, precision and NPV do not correspond to the conventional clinical definitions of positive or negative samples, but are solely measures of performance within each respective class. As such, sensitivity and specificity can be used interchangeably, as can precision and NPV. 


% To evaluate the classification performance of our models, we employed the Area Under the Receiver Operating Characteristic Curve (AUROC) as the primary metric. The AUROC provides a comprehensive measure of a model's ability to distinguish between classes across all classification thresholds \cite{fawcett2006introduction}. In addition, we report several complementary metrics to provide a holistic evaluation, including sensitivity, specificity, precision, and negative predictive value (NPV) \cite{sokolova2009systematic}. Sensitivity measures the proportion of true positive cases correctly identified, offering insight into the model's effectiveness in detecting protein sequences with a true stoichiometry of 180. Specificity quantifies the proportion of true negatives, protein sequences with a true stoichiometry of 60, that are accurately classified. Precision evaluates the accuracy of the model's positive predictions by determining the proportion of protein sequences predicted as having a stoichiometry of 180 that are indeed correct. Likewise, the negative predictive value (NPV) measures the reliability of the model's negative predictions by calculating the proportion of sequences predicted as having a stoichiometry of 60 that are truly 60.

We assess the model’s interpretability by analysing the weights assigned to each position in protein sequences. To identify the most influential positions, we employ four distinct methods. First, \textit{truncation by distribution} examines the effect of removing less informative regions on overall performance. Second, \textit{feature weighting} identifies positions that receive the highest weights from the model. Third, the \textit{simple variance} method \cite{pedregosa2011scikit} selects positions with high variability in absolute weight values, on the basis that greater variance indicates higher informativeness. Finally, \textit{Laplacian score} analysis \cite{he2005laplacian}, an unsupervised approach, evaluates each position based on its ability to preserve local neighbourhood structure and intrinsic data distribution in a similarity graph, with lower scores signifying more informative features.
% The model's interpretability is assessed through weight analysis. Four distinct methods are used to select the positions contributing most to the classification outcome, each capturing a different aspect of positional importance. First, truncation by distribution examines how the removal of less informative regions affects performance. Second, feature weighting leverages the positional weights assigned by the model to highlight key positions. Third, the simple variance method \cite{pedregosa2011scikit} selects positions with high variability in absolute weight values, based on the assumption that greater variance signals higher informativeness. Finally, Laplacian score analysis \cite{he2005laplacian}, an unsupervised approach, evaluates positional features based on their locality-preserving properties in a similarity graph, preferring features with lower scores as these better capture the intrinsic data distribution and local neighbourhood structure. 

For each method, we identify the most influential positions through a grid search over different selection percentages. The encoded amino acids at the selected positions are retained, while the rest of the sequence is padded with zeros. This modified sequence representation is then used to retrain the classifier. By comparing the classification performance across these methods, we identify the protein sequence positions that are most directly linked to the model's predictive capability.


\section{EXPERIMENTS}

% \subsection{Set-up} \label{sec:Experiments, Set-up}


% In this study, we evaluate stoichiometry classification across three stages to systematically assess the impact of different choices in the pipeline. First, we evaluate the impact of encoding methods by comparing integer-label and one-hot encoding for amino acids in protein sequences. Second, we examine two encoding maps—CHARPROTSET and knowledge-based clusters—to assess how different representations influence classification. 

\subsection{Experimental set-up}
The proposed pipeline was implemented in Python 3.8.19 using the scikit-learn library version 1.3.0 \cite{pedregosa2011scikit}. We employed three classifiers as discussed in Sec. \ref{sec:Model training and evaluation}: LR, SVC, and RC from scikit-learn. For encoding, all protein sequences were padded to a uniform length of 1,426, which corresponds to the maximum sequence length in the dataset.

% We employ a nested cross-validation strategy to clearly define the roles of our data subsets. Initially, a 10-fold cross-validation partitions the dataset into a development set (90\% of the entire data) and a test set (10\%), with the test set reserved exclusively for the final evaluation. Within the development set, an additional 9-fold cross-validation is used to create a training set (90\% of the development data) and a validation set (10\%). The training set is used to fit the model, while the validation set is used to tune hyperparameters using Bayesian Optimisation \cite{akiba2019optuna}. After identifying the best hyperparameters, the model is retrained on the entire development set and finally evaluated on the test set.

We employed a nested cross-validation strategy in the experiment. It was implemented in three steps: 
1) We performed 10-fold cross-validation on the entire dataset, partitioning it into a development set (90\% of the data) and a test set (10\%), with the test set reserved exclusively for the final evaluation. 
2) Within the development set, we conducted an additional 9-fold cross-validation to split the data further into a training set (90\% of the development data) and a validation set (10\%). The training set was used to fit the model, while the validation set was used to tune hyperparameters using Bayesian Optimisation \cite{akiba2019optuna}. Hyperparameter combinations that achieved the highest average performance across the 9 folds were selected for final training. 3) After determining the optimal hyperparameters, we retrained the model on the entire development set and evaluated its performance on the test set. 

We repeated the entire process (steps 1-3) for five independent iterations using different random seeds, resulting in a total of 50 runs (5 iterations × 10 test sets). The performance metrics were then reported as the average over these 50 runs.

The code is publicly available on GitHub at \href{https://github.com/Shef-AIRE/StoicIML}{https://github.com/Shef-AIRE/StoicIML}, which also includes the model configurations for reference.

% We employed a nested cross-validation strategy to define the roles of our data subsets in each iteration. The method involved three steps: (1) Initially, a 10-fold cross-validation partitioned the dataset into a development set (90\% of the entire data) and a test set (10\%), with the test set reserved exclusively for the final evaluation. (2) Within the development set, an additional 9-fold cross-validation was used to create a training set (90\% of the development data) and a validation set (10\%). The training set was used to fit the model, while the validation set was used to tune hyperparameters using Bayesian Optimisation \cite{akiba2019optuna}. (3) After identifying the best hyperparameters, the model was retrained on the entire development set and finally evaluated on the test set. Each iteration (steps 1-3) was repeated 5 times with different random seeds, resulting in 50 total runs (5 iterations × 10 test sets). Performance metrics were reported as the average over these 50 runs.

\begin{table*}[!h]
    \centering
    \caption{Performance comparison across encoding methods and linear classifiers. Results are reported as mean $\pm$ standard deviation over 50 runs. The best performance is highlighted in \textbf{bold}, while the second-best result is \underline{underlined}.}
    \label{tab: Main experiment}
\begin{tabular}{cccccccc} % Columns are evenly spaced; adjust widths if needed
    \toprule \toprule
        Encoding map & Encoding method & Classifier & AUROC & Sensitivity & Specificity & Precision & NPV \\ 
    \midrule
        \multirow{6}{*}[-0.8ex]{\shortstack{CHARPROTSET\\(25 categories)}}  & \multirow{3}{*}[-0.8ex]{Integer-label} &
        LR & 0.77 ± 0.11 & 0.70 ± 0.14 & \underline{0.72 ± 0.13} & \underline{0.72 ± 0.11} & 0.71 ± 0.12 \\ 
        
        ~ & ~ & 
        RC & 0.76 ± 0.11 & 0.70 ± 0.16 & \textbf{0.73 ± 0.14} & \textbf{0.73 ± 0.12} & 0.72 ± 0.12 \\ 
        
        ~ & ~ &
        SVC & 0.78 ± 0.12 & 0.70 ± 0.15 & 0.71 ± 0.14 & 0.71 ± 0.11 & 0.71 ± 0.12 \\ 

    % \cmidrule{2-3}
        
        ~ & \multirow{3}{*}[-0.8ex]{One-hot} &
        LR & \underline{0.80 ± 0.10} & \underline{0.78 ± 0.12} & 0.63 ± 0.15 & 0.69 ± 0.10 & \underline{0.75 ± 0.12} \\ 
        
        ~ & ~ &
        RC & \textbf{0.82 ± 0.09} & \textbf{0.79 ± 0.12} & 0.66 ± 0.15 & 0.71 ± 0.11 &\textbf{0.76 ± 0.12} \\ 
        
        ~ & ~ &
        SVC & 0.79 ± 0.11 & 0.77 ± 0.13 & 0.62 ± 0.15 & 0.67 ± 0.10 & 0.73 ± 0.13 \\  
        
    \midrule 
    \addlinespace[0.5em] % Adjust spacing between the midrules
    \midrule 

        \multirow{6}{*}[-0.8ex]{\shortstack{Knowledge-based clusters\\(6 categories)}} & \multirow{3}{*}[-0.8ex]{Integer-label} &
        LR & 0.70 ± 0.11 & 0.64 ± 0.15 & 0.63 ± 0.15 & 0.64 ± 0.11 & 0.64 ± 0.10 \\
        
        ~ & ~ & 
        RC & 0.64 ± 0.12 & 0.61 ± 0.16 & 0.58 ± 0.16 & 0.60 ± 0.11 & 0.60 ± 0.12 \\
        
        ~ &  ~ & 
        SVC & 0.69 ± 0.12 & 0.63 ± 0.17 & 0.60 ± 0.17 & 0.62 ± 0.13 & 0.63 ± 0.13 \\ 
        
    % \cmidrule{2-3}
    
        ~ & \multirow{3}{*}[-0.8ex]{One-hot} &
        LR & \underline{0.82 ± 0.09} & \underline{0.79 ± 0.12} & 0.72 ± 0.14 & 0.74 ± 0.11 & 0.78 ± 0.11 \\ 
        
        ~ &  ~ & 
        RC & \textbf{0.84 ± 0.08} & \textbf{0.80 ± 0.12} & \textbf{0.75 ± 0.14} & \textbf{0.77 ± 0.10} & \textbf{0.80 ± 0.11 }\\
        
        ~ &  ~ & 
        SVC & \underline{0.82 ± 0.09} & \underline{0.79 ± 0.12} & \underline{0.73 ± 0.14} & \underline{0.75 ± 0.10} & \underline{0.79 ± 0.11} \\ 
        
    \bottomrule \bottomrule
\end{tabular}
\end{table*}


\subsection{Experimental results}

\subsubsection{Comparison of encoding methods on CHARPROTSET}
We conducted experiments using the CHARPROTSET encoding map with three linear classifiers to compare the integer-label and one-hot encoding methods. Table \ref{tab: Main experiment} presents the test set results. One-hot encoding outperforms integer-label encoding in three out of five metrics across all classifiers, highlighting its effectiveness in classifying protein stoichiometries. Specifically, AUROCs increase by 0.06, 0.03, and 0.01 for the RC, LR, and SVC, respectively, when using one-hot encoding compared to integer-label encoding. The improved performance with one-hot encoding is likely because it avoids introducing unintended ordinal relationships between classes. By representing each class as a binary vector, one-hot encoding treats all classes as equally distinct and independent. In contrast, integer-label encoding assigns arbitrary integers to classes, which imposes an artificial hierarchy. This may lead the classifier to learn spurious biological relationships that do not exist, ultimately resulting in suboptimal performance.

We also observe that: 1) One-hot encoding achieves higher sensitivity, meaning it identifies more 180-mer protein sequences but sacrifices specificity, making it less capable of distinguishing 60-mers. For instance, the RC shows a 0.09 increase in sensitivity but a 0.07 decrease in specificity. 2) Similarly, one-hot encoding improves NPV by 0.02 to 0.04 across classifiers. However, this comes at the cost of precision, as one-hot encoding results in more misclassified protein sequences among those predicted as 180-mers.

Our speculative explanation for these trends is that classifiers using one-hot encoding might bias predictions towards classifying more protein sequences as 180-mer rather than 60-mer. By predicting more sequences as 180-mer, there could be a higher likelihood of correctly identifying true 180-mers, which may explain the improved sensitivity. However, this could also lead to an increase in misclassified 60-mers, potentially reducing precision. Conversely, the classification of 60-mers might be underrepresented, possibly resulting in lower specificity but higher NPV. While speculative explanations are provided, the precise reasons behind these effects remain uncertain and will be studied in future work.

\subsubsection{Comparison of classifiers on CHARPROTSET} Among all classifiers using the CHARPROTSET encoding map, the RC combined with one-hot encoding demonstrates the best performance in three out of five metrics and achieves the highest AUROC of 0.82. The SVC also performs well with one-hot encoding, achieving the second-best results for AUROC, sensitivity, and NPV.  
% Notably, the RC achieves an AUROC of 0.82, outperforming its integer-label encoding variant—the worst-performing model—by 0.06. 
Noteworthy, by switching to one-hot encoding, the RC becomes the best-performing model, achieving the highest AUROC of 0.82, while its integer-label encoding variant ranks as the worst-performing model, with a 0.06 lower AUROC. This further shows that one-hot encoding improves performance over integer-label encoding, as the latter may introduce noise that hinders classifiers' ability to learn meaningful patterns. 

\subsubsection{Comparison of encoding methods on knowledge-based clusters} We further conducted experiments on encoding with the knowledge-based clusters, with metric scores presented in Table \ref{tab: Main experiment}. Classifiers using one-hot encoding consistently outperform those using integer-label encoding across all metrics. The widened performance gap between integer-label and one-hot encoding further supports our claim that one-hot is a more effective encoding method in classifying protein stoichiometries. 

\subsubsection{Comparison of classifiers on knowledge-based clusters} Notably, performance with one-hot encoding improves significantly when the encoding map is simplified to six categories. In this configuration, the RC consistently achieves the best results across all metrics, followed closely by SVC and LR. Specifically, the RC exhibits the largest improvement in AUROC, increasing by 0.20 compared to integer-label encoding. Additionally, all classifiers show enhanced sensitivity and specificity with one-hot encoding. The RC again shows the largest change, with sensitivity increasing from 0.61 to 0.80 and specificity from 0.58 to 0.75. While the LC shows the smallest improvements, they remain significant, with sensitivity increasing by 0.15 and specificity by 0.09. Similar improvements are also observed in precision and NPV. These results suggest that one-hot encoding enhances classification performance for both stoichiometry classes.

\subsubsection{Comparison of encoding maps} Finally, we notice that the knowledge-based clusters map consistently outperforms the CHARPROTSET map across all metrics with one-hot encoding. This improvement could be due to the reduced complexity in the cluster encoding map, which groups chemically-similar amino acids into six categories. Given the small sample size of 200, this simplification likely makes it easier for the classifiers to identify meaningful patterns by reducing the dimensionality.

 % \begin{figure}[!t]
 %     \centering
 %     \includegraphics[width=1\linewidth]{figures/model_weights.pdf}
 %     \caption{Model weights from RC with one-hot encoding at the amino acid level. (a) Heatmaps of amino acid weights for the first 10 positions in protein sequences using the CHARPROTSET map. (b) Heatmaps of amino acid cluster weights for the first 10 positions using the knowledge-based clusters map. The colour scale represents contributions to stoichiometry classification: red indicates a positive contribution to classifying a protein sequence as a 180-mer, while blue indicates a positive contribution to a 60-mer classification.}
 %     \label{fig:model_weights}
 % \end{figure}

\begin{figure}[!t]
    \centering
    % Subfigure (a)
    \begin{subfigure}{1\linewidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/AA_weights.pdf}
        \caption{Amino acid weights using the CHARPROTSET map.}
        \label{fig:AA_weights}
    \end{subfigure}
    
    \vspace{0.3cm} % Adjust vertical spacing if necessary

    % Subfigure (b)
    \begin{subfigure}{1\linewidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/Cluster_weights.pdf}
        \caption{Cluster weights using the knowledge-based clusters map.}
        \label{fig:Cluster_weights}
    \end{subfigure}
    
    % Main figure caption
    \caption{Heatmaps of model weights from RC with one-hot encoding at the amino acid level. The first 10 positions are shown in subfigure (a) using the CHARPROTSET map and in subfigure (b) using the knowledge-based clusters map. The colour scale indicates contributions to stoichiometry classification: red signifies a positive contribution to classifying a protein sequence as a 180-mer, and blue signifies a positive contribution to classifying it as a 60-mer. }
    
    % Model weights from RC with one-hot encoding at the amino acid level. The colour scale represents contributions to stoichiometry classification: red indicates a positive contribution to classifying a protein sequence as a 180-mer, while blue indicates a positive contribution to a 60-mer classification.}
    \label{fig:model_weights}
\end{figure}



\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figures/positional_weight.pdf}
    \caption{Position-level model weights of the first 600 amino acids in protein sequences from RC with one-hot encoding on the CHARPROTSET. Each point indicates the average positional weight, with error bars representing the standard deviation. }
    \label{fig:positional_weight}
\end{figure}

\subsection{Interpretation evaluation}

\subsubsection{One-hot encoding weights} 
We interpret the models by analysing their weights to understand their decision-making process. We first analyse the model weights with one-hot encoding. As the RC achieves the highest performance across most metrics for the CHARPROTSET map and all metrics for the knowledge-based clusters map, we focus on its weights for interpretation.

% In interpreting model weights with one-hot encoding, we use the RC model, as it achieves the highest performance across most metrics for the CHARPROTSET map and all metrics for the knowledge-based clusters map.

The weights of RC with one-hot encoding reveal how amino acids contribute to classification. Using the CHARPROTSET map, each amino acid is assigned a weight at every position, as shown in Fig. \ref{fig:AA_weights}. Similarly, when the knowledge-based cluster encoding map is employed, the corresponding weights can also be extracted and visualised, as shown in Fig. \ref{fig:Cluster_weights}. In both cases, a positive weight for an amino acid (or a cluster) at any position contributes to classifying the protein as a 180-mer, while a negative weight indicates a contribution to classifying it as a 60-mer. 

Fig. \ref{fig:positional_weight} presents the absolute values of weights summed over all amino acids at each position. This figure illustrates the influence of each position on the classification outcome, with higher sums indicating greater contributions to the classification. Notably, the weight distribution mirrors the protein sequence length distribution in Table \ref{tab: distribution by protein lengths}. The model's weights are influenced by the number of amino acids present at specific positions, with less frequent amino acids receiving lower weights. 


% \begin{figure*}[!t]
%     \centering
%     \includegraphics[width=1\linewidth]{figures/selected_positions_results.pdf}
%     \caption{Comparison of four positional influence selection methods. These methods are evaluated based on AUROC at varying percentages of positions selected (1\% to 40\%). Each point represents the mean AUROC, with error bars indicating standard deviation. The highest AUROC for each method is marked by a star, annotated with the corresponding percentage of positions selected and its score.}
%     \label{fig:selected-positions_results}
% \end{figure*}


\begin{figure*}[!t]
    \captionsetup[subfigure]{labelformat=empty}  

    \centering
    \begin{subfigure}{0.245\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/truncation-by-length.pdf}
        \caption{\hspace{4mm}(a) Truncation by length}
        \label{fig: truncation-by-length}
    \end{subfigure}
    \begin{subfigure}{0.245\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/selection-by-weights.pdf}
        \caption{\hspace{4mm}(b) Selection by weights}
        \label{fig: selection-by-weights}
    \end{subfigure}
    \begin{subfigure}{0.245\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/selection-by-laplacian.pdf}
        \caption{\hspace{3mm}(c) Selection by Laplacian score}
        \label{fig: selection-by-laplacian}
    \end{subfigure}
    \begin{subfigure}{0.245\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/selection-by-variance.pdf}
        \caption{\hspace{4mm}(d) Selection by variance}
        \label{fig: selection-by-variance}
    \end{subfigure}
    \caption{Comparison of four positional influence methods (a-d) on AUROC across 1–40\% of positions selected. The highest AUROC for each method is starred and annotated with its percentage and score. Each point shows the mean AUROC with error bars for standard deviation.}
    \label{fig:selected-positions_results}
\end{figure*}


\subsubsection{Integer-label encoding weights} 
Interpreting model weights with integer-label encoding has two key limitations that make it difficult to derive biologically meaningful insights. First, while integer-label encoding reduces dimensionality, it sacrifices the representation of individual amino acid weights, leaving only positional weights accessible. As a result, it becomes \textit{impossible to capture the contributions of specific amino acids to stoichiometry classification at each position.} Second, the positional weight values may be influenced by the arbitrary numerical encoding of amino acids, introducing bias and reducing interpretability. Given these limitations, we place greater emphasis on using one-hot encoding for stoichiometry classification. 

% \textbf{Encoding method:} Int-label has no meaning. Only one hot is interpretable. model weights at positional level - hightligtht the first 200.

% \textbf{Encoding maps:} Level of depth can go with encoding maps. AA level. differences in maps.  

% Beyond understanding the contribution of individual amino acids to stoichiometry classification, it is equally important to investigate how groups of amino acids, classified into widely accepted clusters, contribute to stoichiometry determination. 



% \begin{figure*}
%     \centering
%     \includegraphics[width=0.9\linewidth]{figures/MS2.pdf}
%     \caption{(a) The MS2 structure (PDB code 2MS2), comprising 180 subunits, is colour-coded based on the positional influence score from RC with one-hot encoding on the CHARPROTSET. The colour gradient ranges from grey (low influence) to red (high influence). (b) A single subunit, highlighted in orange, illustrates its interactions with neighbouring protein subunits.}
%     \label{fig:MS2}
% \end{figure*}



\begin{figure}[!t]
    \centering
    % Subfigure (a)
    \begin{subfigure}{1\linewidth}
        \centering
        \includegraphics[width=0.8\linewidth]{figures/Protein_a_1.pdf}
        \caption{The MS2 structure}
        % , comprising 180 subunits, is color-coded by positional influence scores, with a gradient from grey (low influence) to red (high influence).}
        \label{fig:Protein_a_1}
    \end{subfigure}
    
    \vspace{0.3cm} % Adjust vertical spacing if necessary

    % Subfigure (b)
    \begin{subfigure}{1\linewidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/Protein_b_1.pdf}
        \caption{A single subunit}
        % , highlighted in orange, illustrates its interactions with neighbouring protein subunits.}
        \label{fig:Protein_b_1}
    \end{subfigure}
    
    % Main figure caption
    \caption{Visualisation of positional weights from RC with one-hot encoding on the CHARPROTSET mapped to the MS2 structure (PDB: 2MS2). In subfigure (a), the complete structure, comprising 180 subunits, is colour-coded according to positional influence scores, using a gradient from grey (low influence) to red (high influence). Subfigure (b) highlights one subunit in orange to illustrate its interactions with neighbouring protein subunits.}
    \label{fig: Protein interpretation}
\end{figure}


\subsection{Ablation study on positional influence}

To identify the positions within a protein sequence that have the greatest influence on the classification outcome, we evaluate four selection methods: truncation by distribution, feature weighting ranking, simple variance ranking \cite{pedregosa2011scikit} and Laplacian score analysis \cite{he2005laplacian}. For this study, we use RC with one-hot encoding based on the CHARPROTSET encoding map. Fig. \ref{fig:selected-positions_results} shows the performance of four selection methods across various percentages, with our focus on below 40\% since initial trials consistently yielded the highest AUROC in that range.

In the truncation by distribution method, our results indicate that utilising only the first 171 (12\%) amino acids yields a performance improvement of 0.05 AUROC compared to using the full protein sequences. This suggests that the model predominantly relies on the early regions of the sequence for classification. Consequently, focusing on these initial amino acids appears to be sufficient for accurate stoichiometry classification. This serves as the baseline for comparison, enabling us to assess whether more targeted selection methods outperform the simple truncation method. 


Across all selection methods, the highest AUROC of 0.89 is achieved by selecting 6\% of the positions (85 positions) using positional weights. Both the Laplacian score method and the truncation by distribution method achieve a second-best AUROC of 0.87. Specifically, the Laplacian score method reaches an AUROC of 0.87 when selecting the top 27\% of positions (385 positions), while the truncation by distribution method achieves the same AUROC using the first 171 positions only. Finally, the selection by variance method reaches its optimum AUROC of 0.86 when 24\% of the positions (342 positions) are selected. The results suggest that the weights selection method achieved the highest AUROC while selecting the fewest positions, making it the most effective approach for identifying those positions most influential for stoichiometry classification.

We find that \textit{training classifiers exclusively on the most influential positions identified by the selection methods yields better performance than using the entire protein sequence}. Moreover, the selected positions may potentially align with regions of the protein that are functionally or structurally relevant to stoichiometry. While this suggests that the methods could capture biologically meaningful features, further investigation is needed to confirm their precise biological significance. 



% Discussion: * might need to consider the effect of padded 0 to the weights after position 200th.



\section{DISCUSSION} \label{Sec: Discussion}

To derive biological insights from our interpretable ML pipeline, we analysed MS2 \cite{dang2023ms2}, the coat protein of RNA bacteriophage MS2, which self-assembles into a VLP with a stoichiometry of 180. Positional influence scores from RC with one-hot encoding on the CHARPROTSET were mapped onto the MS2 structure (Figure \ref{fig: Protein interpretation}), revealing that residues with high influence scores are predominantly located on $\beta$-strands or loop regions, but not on $\alpha$-helices. The highest-scoring residues are positioned at the ends of $\beta$-strands, suggesting that $\beta$-sheet formation is crucial for maintaining protein subunit interactions and the structural integrity of the resulting nanovesicle.

From a protein engineering perspective, these residues are critical and should not be mutated. They are primarily aliphatic amino acids, particularly valine and isoleucine, which are known to have a high propensity for $\beta$-strand formation \cite{sen2024amino}. Interestingly, among the 10 residues (S47, R49, N55, K57, T59, K61, Y85, N87, E89, T91) implicated in RNA binding \cite{peabody1993rna}, 50\% (S47, N55, K57, Y85, T91) exhibit high positional influence scores, highlighting their broader structural and functional significance beyond RNA binding.




\section{CONCLUSION}
In this work, we developed an interpretable ML pipeline to predict protein stoichiometry of 60 or 180 directly from protein sequences. Our approach provides an interpretable framework that offers biological insights into stoichiometry classification. Additionally, we identified limitations in the integer-label encoding used in protein models and introduced the one-hot encoding method to more effectively represent protein sequence information. We analysed encoding maps on individual proteins and amino acid clusters to uncover patterns that contribute to the stoichiometry classification. 

Given that the curated dataset of 200 protein sequences represents only a fraction of the proteins forming VLPs, future work should focus on constructing a more diverse dataset. This would enable us to further validate and refine our approach across a wider range of stoichiometry classes. In addition, the pipeline should be evaluated on datasets that reflect the real-world distribution, which may be imbalanced. Furthermore, expanding our approach to include multimodal inputs, such as protein sequences and three-dimensional structures of protein complexes, could enhance both predictive performance and interpretability in future research. 


% result shows that the pipeline is able to
% 1 sentence: what done, method, solved what challenge
% 2nd: good result 1,2,3 (details, not necessarily the challenge itself)


% Future direction:
% * feature selection - consider std





% To evaluate the impact of integer-label encoding on classification, we introduced one-hot encoding as a comparative approach, analysing both methods in terms of classification performance and their ability to preserve biological relevance. This analysis included encoding individual amino acids as well as predefined knowledge-based amino acid clusters. To ensure interpretability, we use linear classifiers to identify sequence features critical for stoichiometry classification. Interpretability is assessed at the positional level, identifying influential sequence positions, and at the amino acid level, analysing their significance at each position. We examine the pipeline’s performance under both in-distribution and out-of-distribution scenarios to assess its robustness.  The results demonstrate that the proposed ML pipeline achieves robust classification performance while providing interpretable insights into the stoichiometry of VLPs with 60 and 180 formations.

% \section*{APPENDIX}

% Appendixes should appear before the acknowledgment.

% \section*{ACKNOWLEDGMENT}

% The preferred spelling of the word ÒacknowledgmentÓ in America is without an ÒeÓ after the ÒgÓ. Avoid the stilted expression, ÒOne of us (R. B. G.) thanks . . .Ó  Instead, try ÒR. B. G. thanksÓ. Put sponsor acknowledgments in the unnumbered footnote on the first page.



% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% References are important to the reader; therefore, each citation must be complete and correct. If at all possible, references should be commonly available publications.

\bibliographystyle{ieeetr} 
\bibliography{main}


% \begin{thebibliography}{99}
% \bibitem{c1} G. O. Young, ÒSynthetic structure of industrial plastics (Book style with paper title and editor),Ó 	in Plastics, 2nd ed. vol. 3, J. Peters, Ed.  New York: McGraw-Hill, 1964, pp. 15Ð64.
% \bibitem{c2} W.-K. Chen, Linear Networks and Systems (Book style).	Belmont, CA: Wadsworth, 1993, pp. 123Ð135.
% \bibitem{c3} H. Poor, An Introduction to Signal Detection and Estimation.   New York: Springer-Verlag, 1985, ch. 4.
% \bibitem{c4} B. Smith, ÒAn approach to graphs of linear forms (Unpublished work style),Ó unpublished.
% \bibitem{c5} E. H. Miller, ÒA note on reflector arrays (Periodical styleÑAccepted for publication),Ó IEEE Trans. Antennas Propagat., to be publised.
% \bibitem{c6} J. Wang, ÒFundamentals of erbium-doped fiber amplifiers arrays (Periodical styleÑSubmitted for publication),Ó IEEE J. Quantum Electron., submitted for publication.
% \bibitem{c7} C. J. Kaufman, Rocky Mountain Research Lab., Boulder, CO, private communication, May 1995.
% \bibitem{c8} Y. Yorozu, M. Hirano, K. Oka, and Y. Tagawa, ÒElectron spectroscopy studies on magneto-optical media and plastic substrate interfaces(Translation Journals style),Ó IEEE Transl. J. Magn.Jpn., vol. 2, Aug. 1987, pp. 740Ð741 [Dig. 9th Annu. Conf. Magnetics Japan, 1982, p. 301].
% \bibitem{c9} M. Young, The Techincal Writers Handbook.  Mill Valley, CA: University Science, 1989.
% \bibitem{c10} J. U. Duncombe, ÒInfrared navigationÑPart I: An assessment of feasibility (Periodical style),Ó IEEE Trans. Electron Devices, vol. ED-11, pp. 34Ð39, Jan. 1959.
% \bibitem{c11} S. Chen, B. Mulgrew, and P. M. Grant, ÒA clustering technique for digital communications channel equalization using radial basis function networks,Ó IEEE Trans. Neural Networks, vol. 4, pp. 570Ð578, July 1993.
% \bibitem{c12} R. W. Lucky, ÒAutomatic equalization for digital communication,Ó Bell Syst. Tech. J., vol. 44, no. 4, pp. 547Ð588, Apr. 1965.
% \bibitem{c13} S. P. Bingulac, ÒOn the compatibility of adaptive controllers (Published Conference Proceedings style),Ó in Proc. 4th Annu. Allerton Conf. Circuits and Systems Theory, New York, 1994, pp. 8Ð16.
% \bibitem{c14} G. R. Faulhaber, ÒDesign of service systems with priority reservation,Ó in Conf. Rec. 1995 IEEE Int. Conf. Communications, pp. 3Ð8.
% \bibitem{c15} W. D. Doyle, ÒMagnetization reversal in films with biaxial anisotropy,Ó in 1987 Proc. INTERMAG Conf., pp. 2.2-1Ð2.2-6.
% \bibitem{c16} G. W. Juette and L. E. Zeffanella, ÒRadio noise currents n short sections on bundle conductors (Presented Conference Paper style),Ó presented at the IEEE Summer power Meeting, Dallas, TX, June 22Ð27, 1990, Paper 90 SM 690-0 PWRS.
% \bibitem{c17} J. G. Kreifeldt, ÒAn analysis of surface-detected EMG as an amplitude-modulated noise,Ó presented at the 1989 Int. Conf. Medicine and Biological Engineering, Chicago, IL.
% \bibitem{c18} J. Williams, ÒNarrow-band analyzer (Thesis or Dissertation style),Ó Ph.D. dissertation, Dept. Elect. Eng., Harvard Univ., Cambridge, MA, 1993. 
% \bibitem{c19} N. Kawasaki, ÒParametric study of thermal and chemical nonequilibrium nozzle flow,Ó M.S. thesis, Dept. Electron. Eng., Osaka Univ., Osaka, Japan, 1993.
% \bibitem{c20} J. P. Wilkinson, ÒNonlinear resonant circuit devices (Patent style),Ó U.S. Patent 3 624 12, July 16, 1990. 
% \end{thebibliography}




\end{document}
 with one-hot encodingin the case 