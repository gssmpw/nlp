\begin{table*}[!h]
\centering
\setlength{\tabcolsep}{2pt}
\resizebox{2.0\columnwidth}{!}{%
\begin{tabular}{lllrrr|rrr|rrr|rrr|rrr}
\hline
\textbf{\multirow{2}{*}{Method}} & \textbf{\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}Prompting \\ Strategy\end{tabular}}} & \textbf{\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}\# Few-Shot / \\ \# Train\end{tabular}}}  & \multicolumn{3}{c}{\textbf{Rest15}}                                                        & \multicolumn{3}{c}{\textbf{Rest16}}                                                        & \multicolumn{3}{c}{\textbf{FlightABSA}}                                                        & \multicolumn{3}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}OATS \\ Coursera\end{tabular}}}                                                    & \multicolumn{3}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}OATS \\ Hotels\end{tabular}}}                                                   \\ \cmidrule(lr{0.8em}){4-6} \cmidrule(lr{0.8em}){7-9} \cmidrule(lr{0.8em}){10-12} \cmidrule(lr{0.8em}){13-15} \cmidrule(lr{0.8em}){16-18}
\textbf{}    \textbf{} &                      & & \multicolumn{1}{c}{\textbf{F1}} & \multicolumn{1}{c}{\textbf{Pre}} & \multicolumn{1}{c}{\textbf{Rec}} & \textbf{F1}          & \multicolumn{1}{c}{\textbf{Pre}} & \multicolumn{1}{c}{\textbf{Rec}} & \textbf{F1}          & \multicolumn{1}{c}{\textbf{Pre}} & \multicolumn{1}{c}{\textbf{Rec}} & \textbf{F1}          & \multicolumn{1}{c}{\textbf{Pre}} & \multicolumn{1}{c}{\textbf{Rec}} & \textbf{F1}          & \multicolumn{1}{c}{\textbf{Pre}} & \multicolumn{1}{c}{\textbf{Rec}} \\ 
\hline
\arrayrulecolor{gray}\cline{2-18}\arrayrulecolor{black}
\textbf{\multirow{12}{*}{Gemma-2-9B}} & \textbf{\multirow{6}{*}{-}} & 0 & 13.26 & 14.11 & 12.50 & 16.94 & 17.86 & 16.12 & 33.41 & 35.68 & 31.42 & 9.58 & 9.52 & 9.64 & 21.06 & 24.76 & 18.34 \\ 
 \textbf{} & \textbf{} & 10 & 20.09 & 18.78 & 21.61 & 20.61 & 19.34 & 22.05 & 23.84 & 24.32 & 23.39 & 5.78 & 5.43 & 6.18 & 17.95 & 20.97 & 15.70 \\ 
 \textbf{} & \textbf{} & 20 & 23.85 & 23.31 & 24.43 & 27.22 & 26.41 & 28.09 & 24.05 & 23.94 & 24.17 & 8.80 & 8.49 & 9.12 & 21.58 & 22.65 & 20.61 \\ 
 \textbf{} & \textbf{} & 30 & 25.62 & 25.12 & 26.14 & 31.66 & 31.00 & \textbf{32.37} & 29.87 & 29.74 & 30.00 & 11.65 & 11.29 & 12.03 & 23.69 & 25.33 & 22.25 \\ 
 \textbf{} & \textbf{} & 40 & 26.31 & 25.73 & 26.92 & 30.07 & 29.69 & 30.46 & 31.56 & 31.40 & 31.73 & 12.28 & 12.21 & \textbf{12.35} & 25.13 & 27.43 & 23.19 \\ 
 \textbf{} & \textbf{} & 50 & 27.32 & 27.01 & \textbf{27.65} & 31.89 & 31.43 & \textbf{32.37} & 33.77 & 33.14 & \textbf{34.44} & 11.73 & 11.51 & 11.95 & 27.23 & 30.11 & \textbf{24.85} \\ 
 \arrayrulecolor{gray}\cline{2-18}\arrayrulecolor{black}
\textbf{} & \textbf{\multirow{6}{*}{SC}} & 0 & 14.58 & 17.12 & 12.70 & 16.82 & 19.76 & 14.64 & 36.86 & 43.08 & 32.20 & 11.24 & 13.64 & 9.56 & 21.74 & 28.77 & 17.48 \\ 
 \textbf{} & \textbf{} & 10 & 26.84 & 49.83 & 18.36 & 25.63 & 49.64 & 17.27 & 30.12 & 62.57 & 19.83 & 3.29 & 20.00 & 1.79 & 21.75 & 47.00 & 14.15 \\ 
 \textbf{} & \textbf{} & 20 & 30.56 & 56.04 & 21.01 & 34.76 & 57.51 & 24.91 & 31.58 & 60.58 & 21.36 & 10.82 & 43.66 & 6.18 & 26.92 & 58.60 & 17.48 \\ 
 \textbf{} & \textbf{} & 30 & 33.74 & 55.98 & 24.15 & 39.53 & 59.75 & 29.54 & 38.73 & \textbf{62.98} & 27.97 & \textbf{18.78} & \textbf{54.29} & 11.35 & 30.75 & \textbf{60.08} & 20.67 \\ 
 \textbf{} & \textbf{} & 40 & \textbf{35.48} & \textbf{57.46} & 25.66 & 37.79 & 56.93 & 28.29 & 39.54 & 60.70 & 29.32 & 15.75 & 49.47 & 9.36 & 30.61 & 57.92 & 20.80 \\ 
 \textbf{} & \textbf{} & 50 & 34.19 & 57.06 & 24.40 & \textbf{40.56} & \textbf{59.90} & 30.66 & \textbf{40.68} & 61.72 & 30.34 & 16.34 & 45.45 & 9.96 & \textbf{31.65} & 57.93 & 21.78 \\ 
\hline
\arrayrulecolor{gray}\cline{2-18}\arrayrulecolor{black}
\textbf{\multirow{12}{*}{Gemma-2-27B}} & \textbf{\multirow{6}{*}{-}} & 0 & 27.74 & 26.80 & 28.75 & 35.79 & 34.80 & 36.85 & 37.09 & 37.13 & 37.05 & 11.74 & 10.71 & 12.99 & 27.51 & 28.62 & 26.49 \\ 
 \textbf{} & \textbf{} & 10 & 35.48 & 33.38 & \textbf{37.86} & 37.73 & 35.71 & 40.00 & 40.74 & 40.18 & 41.32 & 18.08 & 17.05 & 19.24 & 28.78 & 30.45 & 27.30 \\ 
 \textbf{} & \textbf{} & 20 & 33.01 & 32.14 & 33.94 & 44.01 & 42.13 & 46.08 & 38.67 & 38.04 & 39.32 & 20.03 & 19.15 & 21.00 & 31.95 & 32.62 & 31.32 \\ 
 \textbf{} & \textbf{} & 30 & 35.21 & 34.12 & 36.38 & 44.92 & 43.22 & 46.76 & 43.54 & 42.59 & 44.54 & 21.14 & 20.28 & 22.07 & 36.05 & 36.69 & \textbf{35.42} \\ 
 \textbf{} & \textbf{} & 40 & 36.19 & 34.69 & 37.84 & 46.71 & 45.22 & 48.31 & 42.01 & 41.21 & 42.85 & 21.40 & 20.63 & \textbf{22.23} & 34.26 & 34.85 & 33.70 \\ 
 \textbf{} & \textbf{} & 50 & 36.31 & 34.97 & 37.76 & 46.14 & 45.12 & 47.21 & 44.05 & 42.70 & \textbf{45.49} & 21.23 & 20.42 & 22.11 & 35.05 & 36.58 & 33.65 \\ 
 \arrayrulecolor{gray}\cline{2-18}\arrayrulecolor{black}
\textbf{} & \textbf{\multirow{6}{*}{SC}} & 0 & 29.74 & 30.74 & 28.81 & 38.60 & 40.14 & 37.17 & 38.96 & 41.21 & 36.95 & 13.40 & 13.89 & 12.95 & 29.27 & 32.49 & 26.63 \\ 
 \textbf{} & \textbf{} & 10 & 39.34 & 41.69 & 37.23 & 41.11 & 43.72 & 38.80 & 44.44 & 50.00 & 40.00 & 22.81 & 28.70 & 18.92 & 30.92 & 38.59 & 25.80 \\ 
 \textbf{} & \textbf{} & 20 & 36.35 & 40.72 & 32.83 & 48.48 & 51.33 & 45.93 & 43.21 & 48.72 & 38.81 & 25.67 & 33.23 & 20.92 & 38.84 & 48.06 & 32.59 \\ 
 \textbf{} & \textbf{} & 30 & 40.33 & 45.10 & 36.48 & 49.73 & 53.21 & 46.68 & 49.15 & 55.56 & 44.07 & 26.24 & 34.64 & 21.12 & \textbf{40.75} & 49.41 & 34.67 \\ 
 \textbf{} & \textbf{} & 40 & \textbf{41.50} & 46.49 & 37.48 & \textbf{52.46} & 55.87 & \textbf{49.44} & 47.13 & 53.08 & 42.37 & 25.22 & 34.98 & 19.72 & 39.64 & 49.28 & 33.15 \\ 
 \textbf{} & \textbf{} & 50 & 40.62 & \textbf{46.66} & 35.97 & 51.24 & \textbf{56.97} & 46.56 & \textbf{49.91} & \textbf{56.14} & 44.92 & \textbf{27.20} & \textbf{36.99} & 21.51 & 40.24 & \textbf{52.57} & 32.59 \\ 
\hline
\hline
\textbf{\multirow{7}{*}{MVP \citep{gou2023mvp}}} & \textbf{\multirow{21}{*}{-}} & 10 & 10.58 & 12.00 & 9.46 & 12.37 & 14.40 & 10.84 & 9.38 & 11.66 & 7.84 & 12.88 & 14.46 & 11.62 & 6.98 & 8.42 & 5.97 \\ 
 \textbf{} & \textbf{} & 20 & 18.71 & 21.22 & 16.73 & 21.49 & 24.30 & 19.27 & 14.27 & 17.43 & 12.09 & 18.85 & 20.79 & 17.25 & 14.30 & 16.03 & 12.92 \\ 
 \textbf{} & \textbf{} & 30 & 24.36 & 26.54 & 22.52 & 27.58 & 30.83 & 24.96 & 22.53 & 26.82 & 19.42 & 21.32 & 23.25 & 19.68 & 20.89 & 23.17 & 19.03 \\ 
 \textbf{} & \textbf{} & 40 & 25.95 & 27.72 & 24.40 & 32.72 & 33.56 & 31.94 & 28.15 & 32.17 & 25.03 & 20.21 & 22.02 & 18.68 & 24.71 & 27.00 & 22.78 \\ 
 \textbf{} & \textbf{} & 50 & 30.20 & 31.07 & 29.38 & 33.32 & 34.75 & 32.02 & 33.12 & 35.09 & 31.38 & 22.07 & 24.16 & 20.32 & 29.91 & 33.08 & 27.31 \\ 
 \textbf{} & \textbf{} & 800 & 50.02 & \textbf{48.99} & \textbf{51.09} & 58.09 & \textbf{56.31} & \textbf{59.97} & 57.46 & \textbf{56.23} & 58.74 & 30.26 & 29.91 & 30.62 & 53.37 & 52.41 & 54.36 \\ 
 \textbf{} & \textbf{} & Full & \textbf{51.04} & - & - & \textbf{60.39} & - & - & \textbf{57.90} & 56.09 & \textbf{59.83} & \textbf{32.50} & \textbf{32.04} & \textbf{32.97} & \textbf{55.03} & \textbf{54.38} & \textbf{55.69} \\ 
\hline
\textbf{\multirow{7}{*}{DLO \citep{hu2022improving}}} & \textbf{} & 10 & 3.88 & 4.14 & 3.65 & 4.49 & 4.79 & 4.23 & 4.46 & 5.63 & 3.69 & 5.88 & 6.62 & 5.30 & 3.60 & 3.89 & 3.36 \\ 
 \textbf{} & \textbf{} & 20 & 12.38 & 14.80 & 10.64 & 14.01 & 14.56 & 13.52 & 9.67 & 12.15 & 8.03 & 10.28 & 11.16 & 9.52 & 7.53 & 6.47 & 9.10 \\ 
 \textbf{} & \textbf{} & 30 & 18.94 & 18.34 & 19.62 & 23.62 & 24.37 & 22.93 & 17.43 & 18.98 & 16.14 & 16.92 & 17.59 & 16.29 & 17.80 & 17.45 & 18.17 \\ 
 \textbf{} & \textbf{} & 40 & 22.93 & 21.88 & 24.10 & 26.72 & 25.73 & 27.78 & 24.31 & 26.76 & 22.27 & 16.99 & 18.04 & 16.06 & 23.29 & 23.95 & 22.69 \\ 
 \textbf{} & \textbf{} & 50 & 27.93 & 25.94 & 30.26 & 29.63 & 29.23 & 30.06 & 28.81 & 28.14 & 29.53 & 19.87 & 21.64 & 18.37 & 28.14 & 29.46 & 26.93 \\ 
 \textbf{} & \textbf{} & 800 & \textbf{48.76} & \textbf{47.54} & \textbf{50.04} & 58.59 & 56.65 & 60.68 & 56.91 & 55.33 & 58.58 & 29.96 & 29.54 & 30.40 & 53.97 & 53.23 & 54.73 \\ 
 \textbf{} & \textbf{} & Full & 48.18 & 47.08 & 49.33 & \textbf{59.79} & \textbf{57.92} & \textbf{61.80} & \textbf{58.61} & \textbf{56.85} & \textbf{60.47} & \textbf{32.01} & \textbf{31.48} & \textbf{32.55} & \textbf{55.62} & \textbf{54.45} & \textbf{56.84} \\ 
\hline
\textbf{\multirow{7}{*}{Paraphrase \citep{zhang2021aspect}}} & \textbf{} & 10 & 1.32 & 1.64 & 1.11 & 3.56 & 4.02 & 3.23 & 3.44 & 4.34 & 2.85 & 4.75 & 5.35 & 4.26 & 2.63 & 3.66 & 2.06 \\ 
 \textbf{} & \textbf{} & 20 & 5.48 & 6.78 & 4.60 & 11.14 & 10.54 & 11.91 & 3.48 & 4.39 & 2.88 & 9.51 & 10.64 & 8.61 & 5.34 & 6.36 & 4.65 \\ 
 \textbf{} & \textbf{} & 30 & 9.47 & 9.54 & 9.46 & 7.18 & 8.44 & 6.28 & 3.60 & 4.55 & 2.98 & 11.39 & 12.84 & 10.24 & 5.13 & 6.48 & 4.26 \\ 
 \textbf{} & \textbf{} & 40 & 17.61 & 17.07 & 18.19 & 20.15 & 20.69 & 19.67 & 13.81 & 15.09 & 12.78 & 16.43 & 17.79 & 15.26 & 14.96 & 15.99 & 14.08 \\ 
 \textbf{} & \textbf{} & 50 & 25.55 & 24.58 & 26.62 & 23.50 & 23.75 & 23.25 & 17.98 & 18.58 & 17.42 & 19.38 & 20.72 & 18.21 & 23.09 & 23.67 & 22.59 \\ 
 \textbf{} & \textbf{} & 800 & 46.32 & 45.61 & 47.07 & 56.88 & 55.65 & 58.17 & 54.96 & 54.10 & 55.86 & 30.79 & 30.63 & 30.96 & 53.65 & 52.57 & 54.77 \\ 
 \textbf{} & \textbf{} & Full & \textbf{46.93} & \textbf{46.16} & \textbf{47.72} & \textbf{57.93} & \textbf{56.63} & \textbf{59.30} & \textbf{57.76} & \textbf{57.37} & \textbf{58.17} & \textbf{32.34} & \textbf{32.06} & \textbf{32.63} & \textbf{53.87} & \textbf{52.61} & \textbf{55.19} \\ 
\hline
\end{tabular}
}
\caption{Performance scores for ASQP. For the Rest15 and Rest16 datasets, performance scores achieved when employing the full training set ("Full") are taken from \citet{gou2023mvp}, \citet{hu2022improving} and \citet{zhang2021towards} for MVP, DLO and Paraphrase, respectively. The best score achieved by a method is presented in bold.}\label{fig:results-absa-asqp}
\end{table*}


\section{Results}

% Todo: Class-wise Tabelle referenzieren
% Todo: verlaufskurven
The performance scores for the evaluated configurations are shown in Table \ref{fig:results-absa-asqp} for the ASQP task and in Appendix \ref{appendix:performance-tasd} for the TASD task. Detailed performance scores focusing on individual sentiment elements are provided in Appendix \ref{appendix:performance-scores-element}. Notably, for both tasks, we performed t-tests with Bonferroni correction (p\textsubscript{adj} < .05) to examine whether significant differences exist between the F1 scores of the evaluated conditions (corresponding to the number of rows in Figure \ref{fig:results-absa-asqp}). No significant differences were observed.

\textbf{Performance gains with an increasing number of few-shot examples.} In most cases, increasing the number of few-shot examples resulted in incremental improvements in F1 scores across both ASQP and TASD tasks. The difference between zero- and few-shot prompting is substantial. For instance, on the Rest16 dataset under the SC prompting condition, the F1 score improved from 38.60 (0-shot) to 52.46 (40-shot). To further highlight this trend, we provide line plots (see Figure \ref{figure:performance-score-trends}) that depict the influence of the number of few-shot examples on the F1 scores across all tasks, datasets, and models.

\textbf{LLM performance slightly lower compared to SOTA fine-tuned approaches.} For both TASD and ASQP, the performance achieved through zero- and few-shot prompting did not surpass that obtained when the entire training set was utilized. However, the best F1 scores achieved by Gemma-2-27B in the TASD task were often close to those achieved by fine-tuned approaches employing 800 or all examples from the training set. For example, on the Rest16 dataset, Gemma-2-27B achieved 66.03, which is slightly below the best F1 score achieved by a fine-tuned approach (MVP: 72.76). In case only 10 to 50 annotated examples were available for prompting or training, few-shot prompting consistently outperformed fine-tuning approaches across all sample sizes, with only a few exceptions.

\textbf{Massive performance enhancements achieved through self-consistency.} SC enabled considerable boosts of the F1 score, regardless of the amount of few-shot examples. However, recall was occasionally better without SC. Precision, on the other hand, was improved with SC in both tasks and across datasets. For instance, in the case of Gemma-2-9B, precision was increased by more than 20 percentage points in several instances.

\textbf{The LLM's parameter size matters.} Gemma-2-9B demonstrated lower performance in terms of F1 scores for both ASQP and TASD. Across the five datasets, the F1 scores in the ASQP task were approximately 10 percentage points lower when using Gemma-2-9B instead of Gemma-2-27B. For example, on the Rest15 dataset, the best F1 score achieved with Gemma-2-9B was 35.48, while the best score for Gemma-2-27B was 41.50. Nonetheless, respectable F1 scores were still achieved with Gemma-2-9B, approaching to 50 (e.g., 49.96 for Rest15) and surpassing 50 (e.g., 54.77 for TASD on Rest16).

\textbf{Lower performance in identifying opinion terms compared to other sentiment elements.} As shown in the tables in Appendix \ref{appendix:performance-scores-element}, performance in identifying sentiment (positive, negative, or neutral) is highly performant, with F1 scores exceeding 90. However, performance in identifying aspect and opinion terms is comparatively much lower. 
