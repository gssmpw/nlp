\section{Discussion}

The results demonstrate performance improvements in F1 scores for both ASQP and TASD as the number of few-shot examples increases, highlighting the gap between zero- and few-shot prompting. in this chapter, we put the results of this work into the context of previous research and provide an outlook on the direction of future work.

\textbf{New SOTA performance of LLMs}. The performance scores reported in previous studies by \citet{gou2023mvp}, \citet{zhang2024sentiment} and \citet{bai2024compound} for the ASQP task on the Rest15 and Rest16 datasets fall below those achieved by Gemma-2-27B in both zero- and 10-shot learning settings. The only exception is the TASD and Rest15, where ChatABSA \citep{bai2024compound} marginally outperforms Gemma-2-27B in zero-shot learning, with an F1 score of 39.21 compared to Gemma-2-27B’s F1 scores of 37.08 (with SC) and 35.98 (without SC). In contrast, on Rest16, Gemma-2-27B demonstrates a substantial advantage, achieving F1 scores of 53.13 (with SC) and 50.89 (without SC), exceeding ChatABSA’s F1 score of 41.28. Unlike prior studies, which have primarily evaluated up to 10-shot settings, we extended the investigation to a 10- to 50-shot setting for the first time. In this expanded range, Gemma-2-27B achieved notable F1 scores exceeding 50 for the ASQP task (e.g., Rest16 with SC: 52.46) and surpassing 60 for the TASD task (e.g., Rest16 with SC: 66.03). Notably, these substantial gains are primarily attributed to the use of SC prompting. Furthermore, this is in contrast to the work of \citet{wu2024evaluating}, whose SC approach for E2E-ABSA did not lead to an improvement in performance.

\textbf{Model size and prompting strategy affect few-shot performance}. Although Gemma-2-27B achieved competitive results in both ASQP and TASD, its performance remained slightly below fine-tuned SOTA approaches such as those by \citet{gou2023mvp}, \citet{zhang2021towards}, and \citet{hu2022improving} when full training sets were employed. However, in scenarios with limited annotated examples, few-shot prompting consistently outperformed fine-tuning. The parameter size of the model also influenced performance, with Gemma-2-27B consistently outperforming its smaller counterpart, Gemma-2-9B. 


% Based on our promising results, future work should focus on improving performance in low-resource settings. One avenue for improvement is optimizing the prompt, for instance, by employing advanced prompting strategies such as chain-of-thought prompting \citep{wei2022chain} or plan-and-solve prompting \citep{wang2023plan}, which led to an enhanced classification performance for other NLP tasks. Additionally, the annotation guidelines embedded in the prompt could be further refined. Following the work of \citet{zhang2021towards}, it may also be worth exploring whether representing labels as text instead of tuples could lead to performance gains, leveraging the observation that LLMs tend to prefer fluent textual formats. Furthermore, using larger LLMs (e.g. 70 billion parameter size) may yield even better results, given that our experiments showed improved performance scores with a 27B model compared to a 9B model.
\textbf{Directions for enhancing low-resource task performance}. Building on the promising results of this study, future research could focus on improving low-resource task performance through advanced prompt engineering techniques. Approaches such as chain-of-thought prompting \citep{wei2022chain} or plan-and-solve prompting \citep{wang2023plan}, which allowed for performance gains in other NLP tasks, hold significant potential. Furthermore, refining annotation guidelines or representing labels as natural language text, as proposed by \citet{zhang2021towards}, could contribute to improved outcomes. Bigger LLMs, e.g. with 70B parameters, may provide additional performance benefits, given that our 27B model demonstrated superior results compared to the 9B variant.


\textbf{Exploring less complex tasks and many-shot learning}. In a broader context, future research could extend our approach to less complex tasks, in terms of the amount of considered sentiment elements, such as E2E-ABSA or aspect category sentiment analysis (ACSA) which focuses on aspect category and the sentiment expressed towards them. Beyond the low-resource setting considered in this study, one could explore the so-called "many-shot in-context learning" paradigm described by \citet{agarwal2024many} for ABSA, where hundreds or even the full training set is provided in the prompt. Observing that our approach achieved performance scores on the TASD task close to fine-tuned models, future work could investigate whether further increasing the number of shots lead to surpassing fine-tuned approaches.
