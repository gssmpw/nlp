\section{Related Work}
\begin{table*}[!h]
\centering
\resizebox{2.0\columnwidth}{!}{%
\begin{tabular}{ll|cc|cc}

\hline
\textbf{\multirow{2}{*}{Strategy}}  & \textbf{\multirow{2}{*}{Method}}  & \multicolumn{2}{c}{\textbf{ASQP}} & \multicolumn{2}{c}{\textbf{TASD}} \\
                               \textbf{} &   \textbf{}   & \textbf{Rest15} & \textbf{Rest16} & \textbf{Rest15} & \textbf{Rest16}      \\ \hline
 \textbf{\multirow{4}{*}{\begin{tabular}[c]{@{}l@{}}Zero-shot \\ learning\end{tabular}}} & gpt-3.5-turbo, 0-shot (uncased) \citep{gou2023mvp} & 22.87               & -           & -               & 34.08                 \\
\textbf{} & gpt-3.5-turbo, 0-shot \citep{zhang2024sentiment}  & 10.46               & 14.02           & -               & -                 \\
\textbf{} & text-davinci-003, 0-shot \citep{zhang2024sentiment}  & 13.73                & 18.18           & -               & -                 \\
\textbf{} & ChatABSA, 0-shot \citep{bai2024compound}  & \textbf{27.11} & \textbf{30.42} & \textbf{39.21} & \textbf{41.28}                \\
\hline
\textbf{\multirow{7}{*}{\begin{tabular}[c]{@{}l@{}}Few-shot \\ learning\end{tabular}}} & gpt-3.5-turbo, 1-shot \citep{zhang2024sentiment}  & 30.15               & 31.98          & -               & -                 \\
\textbf{} & gpt-3.5-turbo, 5-shot \citep{zhang2024sentiment}  & 31.21               & 38.01           & -               & -                 \\
\textbf{} & gpt-3.5-turbo, 10-shot (uncased) \citep{gou2023mvp}  & \textbf{34.27}               & -           & -               & 46.51                 \\
\textbf{} & gpt-3.5-turbo, 10-shot \citep{zhang2024sentiment}  & 30.92               & \textbf{40.15}           & -               & -                 \\
\textbf{} & ChatABSA, 1-shot \citep{bai2024compound}  & 28.13 & 33.84 & 37.23 & 41.92 \\
\textbf{} & ChatABSA, 5-shot \citep{bai2024compound}  & 33.26 & 31.92 & 43.00 & 45.04 \\
\textbf{} & ChatABSA, 10-shot \citep{bai2024compound} & 32.14 & 33.26 & \textbf{45.93} & \textbf{47.00} \\
\hline
\hline
\textbf{\multirow{6}{*}{\begin{tabular}[c]{@{}c@{}}Fine-tuning\end{tabular}}}  & TAS-BERT \citep{wan2020target}       & 34.78           & 43.71           & 57.51           & 65.89             \\
\textbf{} &  Extract-Classify \citep{cai2021aspect} & 36.42           & 43.77           & -               & -                 \\
\textbf{} & GAS \citep{zhang2021towards}         & 45.98           & 56.04           & 60.63           & 68.31             \\
\textbf{} &Paraphrase \citep{zhang2021aspect}   & 46.93           & 57.93           & 63.06           & 71.97             \\
\textbf{} &DLO \citep{hu2022improving}          & 48.18           & 59.79           & 62.95           & 71.79             \\
\textbf{} & MVP \citep{gou2023mvp}               & \textbf{51.04}  & \textbf{60.39}  & \textbf{64.53}  & \textbf{72.76}     \\
\hline 
\end{tabular}
}
\caption{Performance on the ASQP and TASD task. F1 scores of both LLM-based and fine-tuned approaches from related work.}
\label{tab:asqp-tasd-results-related-work}
\end{table*}

\subsection{Aspect Sentiment Quad Prediction}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\columnwidth]{material/asqp-example.pdf}
    \caption{Annotated example for ASQP from Rest16 \citep{zhang2021aspect}. One or multiple opinion-quadruple annotations are assigned to each sentence.}
    \label{fig:zhang-asqp-example}
\end{figure}


The development of methodologies for addressing the ASQP task was strongly influenced by the work of \citet{zhang2021aspect}, which introduced two annotated datasets for the ASQP task, Rest15 and Rest16. An example of such annotations is illustrated in Figure \ref{fig:zhang-asqp-example}. Both datasets comprise annotated sentences derived from restaurant reviews. The annotations are sourced from the SemEval Shared Task datasets from 2015 and 2016 
\citep{pontiki2015semeval,pontiki2016semeval}, which originally included only (\textit{a}, \textit{c}, \textit{p})-triplets and thus did not include annotations for opinion terms. 

Since the release of Rest15 and Rest16, generative methods within a unified framework have emerged as the state-of-the-art (SOTA) approach for the ASQP task. Various strategies have been explored to generate sentiment elements in specific formats that exploit label semantics. These include approaches employing structured extraction schemas \citep{lu2022unified}, sequential representations of sentiment elements \citep{gou2023mvp} and natural language formats \citep{gou2023mvp,liu2021solving}, wherein quadruples are systematically converted into natural language sentences. Performance scores for these methods are presented in Table \ref{tab:asqp-tasd-results-related-work}.

All the aforementioned approaches rely on small text generation models, such as t5-base \citep{raffel2020exploring}, which utilizes an encoder-decoder architecture based on the transformer architecture \cite{vaswani2017attention}. The t5-base model, comprising 223 million parameters, is fine-tuned specifically for the ASQP task.

\subsection{Large Language Models for Aspect-based Sentiment Analysis}

The zero- and few-shot capabilities of LLMs have been demonstrated across various NLP tasks, e.g. question answering \citep{chada2021fewshotqa, brown2020language}, named entity recognition \citep{cheng2024novel, wang2023gpt}, information retrieval \citep{faggioli2023perspectives, wang2022recognizing} or sentiment analysis \citep{zhang2024sentiment}. In many cases, these models have achieved performance scores comparable to fine-tuned approaches, with few-shot learning often outperforming zero-shot learning.

In the domain of ABSA, LLMs have been employed in both zero- and few-shot settings. However, these efforts were constrained to a maximum of 10 few-shot examples within the prompt's context, addressing both ASQP and ABSA tasks with fewer sentiment elements \citep{conneau2019unsupervised, gou2023mvp, zhang2024sentiment}.


\citet{zhang2024sentiment} employed OpenAI's gpt-3.5-turbo \citep{brown2020language} for End-to-End ABSA (E2E-ABSA, focus on (\textit{a}, \textit{p}) pairs) and achieved an F1 score of 54.46 and 63.30 on the Rest14 dataset (restaurant domain) from \citet{pontiki2014semeval} for zero- and 10-shot learning, respectively. A fine-tuned t5-large model \citep{raffel2020exploring} achieved a slightly higher F1 score of 75.31. Similarly, \citet{wu2024evaluating} analysed multiple open source LLMs with less than 10 billion parameters, as well as commercial LLMs for multilingual E2E-ABSA in a zero-shot setting. In multilingual ABSA, applying prompting strategies such as chain-of-thought (CoT) prompting did not improve performance when averaged across the LLMs considered. However, the best performing LLM, GPT-4o-CoT, achieved an F1 score of 52.81 which is slightly below the performance of the most performant fine-tuned model XLM-R \citep{conneau2019unsupervised} (68.86). \citet{wu2024evaluating} also evaluated a self-consistency (SC) prompting strategy, where the most frequent label across five generated outputs was selected as the final label. SC did not lead to an improvement in the performance.

With regard to the ASQP task, \citet{zhang2024sentiment} achieved F1 scores below 20 for both Rest15 and Rest16 (see Table \ref{tab:asqp-tasd-results-related-work}). Performance was improved to F1 scores above 30 on both Rest15 and Rest16 by providing 1, 5 or 10 few-shot examples.

\citet{gou2023mvp} surpassed the performance reported by \citet{zhang2024sentiment} and reported an F1 score of 22.87 (zero-shot) and 34.27 (10-shot) on the Rest16 dataset, slightly exceeding the performance reported by \citet{zhang2024sentiment}. Notably, \citet{gou2023mvp} presented the sentences to be annotated and few-shot examples in an uncased format within the prompt, differing from the approach by \citet{zhang2024sentiment}. Furthermore, the task descriptions were formatted differently, with \citet{gou2023mvp} offering descriptions on each of the four sentiment elements considered in the respective ABSA task. \citet{bai2024compound} adopted a distinct approach (referred to as ChatABSA) to processing its outputs, leading to performance improvements in the zero-shot setting but not in the few-shot setting. In the prompt, it was stated that the output should be in the JSON format. Furthermore, predicted aspect terms or opinion terms that were not explicitly mentioned in the original sentence were systematically set to null.

In summary, previous studies demonstrated that few-shot learning massively boosts performance in ABSA tasks but did not exceed the performance of models fine-tuned on annotated examples.