\section{Methodology}

We utilized LLMs to tackle the ASQP task across 0-, 10-, 20-, 30-, 40-, and 50-shot settings on different datasets. The performance is compared to that achieved using a dedicated training set to fine-tune smaller pre-trained language models. Furthermore, we report performance results for the TASD task.

\subsection{Evaluation}

\subsubsection{Datasets}

\begin{table*}[h]
\centering
\resizebox{1.8\columnwidth}{!}{%
\begin{tabular}{lccccc}
\hline
\textbf{}                    & \textbf{Rest15} & \textbf{Rest16} & \textbf{FlightABSA} & \textbf{OATS Coursera} & \textbf{OATS Hotels} \\ \hline
\textbf{\# Train}             & 834             & 1,264           & 1,351               & 1,400               & 1,400                \\
\textbf{\# Test}              & 537             & 544             & 387               & 400                 & 400                  \\
\textbf{\# Dev}              & 209             & 316             & 192               & 200                 & 200                  \\ \hline
\textbf{\# Aspect Categories} & 13              & 13              & 13              & 28                  & 33                   \\
\textbf{Language} & en              & en              & en              & en                  & en                   \\
\textbf{Domain} & restaurant              & restaurant              & airline              & e-learning                  & hotel                   \\
\hline
\end{tabular}
}
\caption{Overview of all ASQP datasets considered for evaluation. The datasets cover a range of different numbers of considered aspect categories and domains. }
\label{tab:overview-datasets}
\end{table*}

Table \ref{tab:overview-datasets} presents an overview of the datasets used in this study, including Rest15 and Rest16, along with three additional datasets covering diverse domains.


\textbf{Rest15 \& Rest16:} ASQP annotations originate from \citet{zhang2021aspect} and the TASD annotations from \citet{wan2020target}. This ensured comparability with the performance scores reported in previous research.

\textbf{FlightABSA:} A novel dataset containing 1,930 sentences annotated for ASQP. Properties of the annotated dataset are provided in Appendix \ref{appendix:flightabsa}. 

\textbf{OATS Hotels \& OATS Coursera:} We utilized a subset of two corpora recently introduced by \citet{chebolu2024oats} comprising ASQP-annotated sentences from reviews on hotels and e-learning courses. A detailed description of the data preprocessing for the OATS datasets can be found in Appendix \ref{appendix:oats-dataset}.

For the TASD task, we removed the opinion terms from the quadruples in annotations from FlightABSA, OATS Coursera and OATS Hotels. Subsequently, any duplicate triplets (\textit{a}, \textit{c}, \textit{p}) that appeared twice in a sentence were discarded.

\subsubsection{Setting}

For evaluation, the test dataset was considered for all datasets. An LLM was prompted five times with different seeds (0 to 4) for each combination of ABSA task (ASQP and TASD), dataset and amount of random few-shot examples (0, 10, 20, 30, 40 or 50) taken from the training set in order to get five label predictions. For all seeds, the same few-shot examples were used; however, they were shuffled differently for each prompt execution. The average performance across all five runs is reported.

\subsubsection{Metrics}

As in previous works in the field of ABSA, we report the micro-averaged F1 score as well as precision and recall to assess the model's performance. The F1 score is the harmonic mean of precision and recall. Precision measures the proportion of correctly predicted positive instances out of all instances predicted as positive \cite[p.~67]{jurafsky2000speech}. Recall quantifies the proportion of correctly predicted positive instances out of all actual positive instances in the dataset \cite[p.~67]{jurafsky2000speech}.

%, and is computed as follows:

%\[
%F1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
%\]
Similar to \citet{zhang2021aspect}, a quad prediction was considered correct if all the predicted sentiment elements are exactly the same as the gold labels. Recognizing the potential interest in class-level performance metrics for subsequent research, we have shared the predicted labels for every evaluated setting in our GitHub repository, allowing detailed class-level analysis.

\begin{figure*}[!htbp]
    \centering
    \includegraphics[width=2.1\columnwidth]{material/prompt.pdf}
    \caption{The prompt includes both a task description and specification of the output format. The LLM is run with five different seeds and in the case of self-consistency prompting, the tuple that appears most often across the five predictions is incorporated into the final label.}

\end{figure*}
\label{figure:study-prompt}

\subsection{Large Language Models}

We employed Gemma-2-27B\footnote{google/gemma-2-27b: \url{https://ollama.com/library/gemma2:27b}} by Google, which comprises 27.2 billion parameters \citep{team2024gemma}. Ollama\footnote{ollama: \url{https://ollama.com}} was employed for inference, and the LLMs were loaded with 4-bit quantization. The model was chosen for its efficiency in terms of generated tokens per second, which is a critical factor given the extensive prompt execution requirements. Notably, our study required over 342,720 prompts to be executed, with many few-shot learning prompts encompassing over a thousand tokens. For larger models, such as Llama-3.3-70B \citet{dubey2024llama}, the required computational costs would have been hardly feasible with our resources. For comparison purposes, we also report performance for the smaller-sized LLM, Gemma-2-9B\footnote{google/gemma-2-9b: \url{https://ollama.com/library/gemma2:9b}}.

The experiments were conducted on two NVIDIA RTX A5000 GPUs equipped with 24 GB of VRAM each. The LLM's temperature parameter was set to 0.8 and generation was terminated upon encountering the closing square bracket character (\texttt{"]"}) signifying the ending of a predicted label.

\subsection{Prompt}

\subsubsection{Components}

We adopted the prompting framework introduced by \citet{gou2023mvp} with some modifications. The employed prompt is illustrated in Figure \ref{figure:study-prompt} and an example is provided in Appendix \ref{appendix:prompt-example}. The main components of the prompt include a list of explanation on all considered sentiment elements and the specification of the output format. 

Unlike \citet{gou2023mvp}, our prompt instructs the LLM to pay attention to case sensitivity when returning aspect and opinion terms. Hence, the identified phrases should appear in the predicted tuple as they do in the sentence, similar to all supervised approaches mentioned in the related work section. Therefore, in the prompt, we clearly stated that the exact phrases should appear in the predicted label. 

Since we execute each prompt with five different seeds, we also report the performance when employing the self-consistency prompting technique introduced by \citet{wang2022self}. The key idea is to select the most consistent answer from multiple prompt executions. We adapted the approach for ABSA by incorporating a tuple into the merged label if it appears in the majority of the predicted labels. As illustrated in Figure \ref{figure:study-prompt}, this corresponds to a tuple appearing in at least 3 out of 5 predicted labels.

\subsection{Output Validation}

Since LLMs such as Gemma-2-27B cannot be strictly constrained to a fixed output format, we programmatically validated the output of the LLM. For the predicted label, several criteria needed to be met for the generation to be considered valid:

\begin{itemize}
    \item \textbf{Format}: The output must be a list of one or more tuples consisting of strings (quadruples for ASQP, triplets for TASD).
    \item \textbf{Sentiment}: The sentiment must be either 'positive', 'negative' or 'neutral'.
    \item \textbf{Aspect category}: Only the categories considered for the respective dataset and thus being mentioned in the prompt should be predicted as a part of a tuple.
    \item \textbf{Aspect and opinion terms}: Both must appear in the given sentence as predicted.
\end{itemize}

If any of the specified criteria for reasoning or label validation is not met, a regeneration attempt was triggered. If the predicted label was still invalid after 10 attempts, an empty label (\texttt{[]}) was considered as the predicted label.

\subsection{Baseline Model}

We compared the previously mentioned zero- and few-shot conditions against three SOTA baseline approaches, which are, the three best-performing methods for ASQP and TASD on the Rest15 and Rest16 datasets: Paraphrase \citep{zhang2021aspect}, DLO \citep{hu2022improving} and MVP \citep{gou2023mvp}.

\begin{description}
    \item[Paraphrase \citep{zhang2021aspect}:] \textit{Paraphrase} is used to linearize sentiment quads into a natural language sequence to construct the input target pair.
    \item[DLO \citep{hu2022improving}:] \textit{Dataset-level order} is a method designed for ASQP that leverages the order-free property of quadruplets. It identifies and utilizes optimal template orders through entropy minimization and combines multiple effective templates for data augmentation.
    \item[MVP \citep{gou2023mvp}:] \textit{Multi-view-Prompting} introduces element order prompts. The language model is guided to generate multiple sentiment tuples, with a different element order each, and then selects the most reasonable tuples by a voting mechanism. This method is highly resource-intensive, as multiple input-output pairs are created for each example in the train set, each comprising different sentiment element positions.
\end{description}

For all three approaches, we conducted training using the entire dataset and performed training with only 10, 20, 30, 40, or 50 training examples equally to the ones employed for the few-shot learning conditions. Training was conducted using five different random seeds (0 to 4). Moreover, to facilitate comparisons across datasets, we trained models using 800 training examples, as this represents the largest multiple of 100 examples available for all train sets (900 training examples are not available for Rest15). The results obtained using the full training sets of Rest15 and Rest16 were extracted from the works of \citet{zhang2021aspect}, \citet{hu2022improving}, and \citet{gou2023mvp}.

For all methods, we used the hyperparameter configurations used by \citet{zhang2021aspect}, \citet{hu2022improving} and \citet{gou2023mvp}. The only exception was the 10-shot condition, where batch size was set to 8 instead of 16, as the limited number of examples (10) could not form a batch of 16 examples.
