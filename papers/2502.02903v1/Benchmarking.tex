\section{Understanding name bias}
\label{sec:benchmarking}
\looseness=-1
In this section, we investigate the presence of bias within text-embedding models related to names. Our primary objective is to investigate the influence of names containing identity-specific information on the resulting text embeddings, while ensuring the semantic structure of the text remains unchanged.


\subsection{Benchmarking Methodology}
\looseness=-1
To understand the impact of bias associated with names, we systematically replace instances of names in text with alternatives. For the sake of simplicity, in this section, we focus on person names and country names\footnote{We also study the impact of  perturbation of person names only.}. Given a text, we first identify instances of person and country names in the text.\footnote{The datasets used for benchmarking are described in Sec.~\ref{benchmark:datasets} } To study bias w.r.t. person names, we replace each person name in the text with a randomly sampled name from a list of person names. In the text, all instances of the same person are replaced by the same sampled name. Similarly, country names are replaced with a random country name sampled from a predefined list of countries. This process only changes the person names and countries and does not change the original structure or meaning of the text.



Formally, given a universe of $n$ person names $P=\{p_1, p_2, p_3 \cdots p_n\}$, and $l$ Country names $C=\{c_1, c_2, c_3 \cdots c_l\}$, we apply algorithm~\ref{alg:perturbation_text} for a given text $T$ to obtain a perturbed text $T'$. 

\begin{algorithm}
\small
\caption{Perturb Text for Benchmarking }
%{\scriptsize
\begin{algorithmic}[1]
 	 \REQUIRE   $P:$ List of Person names, $C:$ List of Country names.
     \STATE {\bfseries Input:} Text $T$ 
\STATE {\bfseries Output:} Text $T'$ with replaced entities
     \STATE Initalize: $T' \leftarrow T$ 

        \STATE{\bfseries Identify Entities:}  
    \STATE $\quad$ Identify all occurrences of person names $IP$ in $T'$.
    \STATE $\quad$ Identify all occurrences of country names $IC$ in $T'$. 

    \STATE{\bfseries Perturbation:}  
    \FOR{each identified person $ip \in IP$ in text $T'$} \label{alg_line_bench_person_repl_beg}
            \STATE Randomly select a name $p_k \in P$ without \\
            replacement.
           \STATE Replace all occurrences of $ip$ with $p_k$ in text $T'$.
    \ENDFOR \label{alg_line_bench_person_repl_end}

        
        \FOR{each identified country $ic \in IC$ in text $T'$}   \label{alg_line_bench_country_repl_beg}
        \STATE Randomly select a country name $c_k \in C$ without \\replacement.
        \STATE Replace all occurrences of $ic$ with $c_k$ in text $T'$.
    \ENDFOR \label{alg_line_bench_country_repl_end}
  \STATE {\bfseries Return} $T'$  \COMMENT{Perturbed Text}
	 
	
\end{algorithmic}
\label{alg:perturbation_text}
%}
\end{algorithm}




            
Applying  Algorithm~\ref{alg:perturbation_text} gives one perturbation $T'$ for text $T$. We generate $K{=}20$ such perturbations capturing a wider range of person and country names. The names used for replacement are present in Table ~\ref{tab:names_used_benchmark} in Appendix and we have names from many different cultures/countries. Note that the steps ~\ref{alg_line_bench_person_repl_beg}-\ref{alg_line_bench_person_repl_end} and ~\ref{alg_line_bench_country_repl_beg}-\ref{alg_line_bench_country_repl_end}   in perturbation algorithm can be done in isolation and can be applied independently based upon the use-case.  An illustrative example of a perturbation is presented in Table~\ref{tab:perturbation_example}.

\begin{table}[ht]
\centering
\small
\begin{tabular}{|p{3.5cm}|p{3.5cm}|}
\hline
Original Text($T$)  & Perturbed Text 1($T'_1$) \\
\hline
\textbf{Mike} has been living in \textbf{Belgium} for five years and made a fortune by winning a lottery. \textbf{Mike} spent most of his money on treatment of his brother \textbf{Donald} who was suffering from Lung Cancer. & \textbf{Dwayne} has been living in \textbf{France} for five years and made a fortune by winning a lottery. \textbf{Dwayne} spent most of his money on treatment of his brother \textbf{Shawn} who was suffering from Lung Cancer. \\
\hline
\end{tabular}
\caption{Example of text perturbation.}
%We create $K$ such perturbations of the Original text and use them for computing similarity with each other. Note that different kinds of perturbation can be created, eg:- perturbing person names only. 
\label{tab:perturbation_example}
\end{table}








%\subsection{Evaluation Metrics}
\looseness=-1
The objective is to determine the degree of semantic divergence observed between perturbed text instances, resulting from the replacement of names and countries, by examining their embeddings. As discussed above, for a text $T$ we create $K$ perturbations  $\left\{ T'_i \mid 1 \leq i \leq K \right\}$. Each of these $K$ perturbed text versions were processed through a text-embedding model, to obtain its corresponding embedding.
% i.e $\forall i \;  emb_{T'_i} = LLM_{embed}(T'_i) $.  Here, $LLM_{embed}$ is a function that maps text to a vector representation.
Subsequently, to capture the distance between the perturbed text's embeddings with each other, we calculate the pairwise cosine similarity between all $K$ embeddings.  For example, if a text sample has $K{=}20$ perturbations, we get $\frac{K \times (K-1)}{2}= 190$ similarity scores. Given $N$ such text samples in a dataset, to arrive at a single metric, we first compute pairwise cosine similarities(between the perturbed text embeddings) for a given text, excluding the self-similarity comparisons (i.e., the similarity of a perturbed text embedding to itself). For $N$ samples, we obtain $N \times \frac{K \times (K-1)}{2} $ similarity scores. Let $emb_{si}$ refer to the embedding of $i^{th}$ perturbation of sample $s$ where $s \in \{1, 2, ..., N\}$ and $i \in \{1, 2, ..., K\}$.  Then, average similarity across $N$ samples is defined as: \vspace{-0.09in}
\begin{equation}
% \small
\nonumber
    \frac{1}{N \times \frac{K(K-1)}{2}} \sum_{s=1}^{N} \left[ \sum_{i=1}^{K} \sum_{\substack{j=1 \\ j \neq i}}^{K} {Sim}(emb_{si}, emb_{sj}) \right]
\end{equation}
\normalsize
\vspace{-0.1in}
% In the above, $emb_{si}$ refers to embedding of $i^{th}$ text perturbation of the $s^{th}$ sample.


 A higher average similarity indicates that the perturbed texts are closer to each other in the semantic space, suggesting less deviation. Conversely, a lower average similarity score suggests a higher degree of deviation from the expected semantic relationship. It suggests that the embedding model exhibits a bias towards names in the text, potentially affecting its ability to accurately capture the theme of the text.

\subsection{Candidate Text-embedding Models}
We analyzed a diverse set of leading text embedding models from academia and industry. This includes models explicitly trained on diverse languages and tasks such as semantic search, question-answering etc. We include models such as \textit{multi-qa-distilbert-cos-v1} and \textit{multi-qa-mpnet-base-cos-v5} for question answering, and \textit{paraphrase-MiniLM-L6-v2} and \textit{paraphrase-multilingual-MiniLM-L12-v2} for identifying semantic similarity~\citep{reimers2019sentence}. Other notable models include \textit{all-mpnet-base-v2, all-distilroberta-v1}, and \textit{all-MiniLM-L6-v2}, designed for general-purpose text representation~\citep{reimers2019sentence}. Additionally, multilingual models like \textit{distiluse-base-multilingual-cased-v1} and \textit{distiluse-base-multilingual-cased-v2 }are also included~\citep{reimers-2020-multilingual-sentence-bert}. We also include \textit{msmarco-distilbert-cos-v5} specialized model for search~\citep{reimers2019sentence}. Additionally, we also choose cutting-edge models which are not open-source namely \textit{text-embedding-3-small} and  \textit{text-embedding-3-large} from Open AI~\citep{OpenAI2024}, \textit{gemini} from Google~\citep{team2023gemini} and \textit{voyage-3-lite} from Voyage AI~\citep{VoyageAI2024}.




\subsection{Benchmark Datasets }
\label{benchmark:datasets}
%We consider the following datasets. 

%\begin{enumerate}
    \textbf{CMU Movie Dataset~\citep{bamman2013learning}:} The CMU Movie dataset primarily consists of textual plot summaries of movies spanning multiple sentences. These summaries are typically short , concise descriptions of the main events and storylines within a film. They often include key characters, conflicts, and resolutions. 
    
  \textbf{CMU Book Dataset~\citep{bamman2013new}:} Similar to CMU Movie, the core of this dataset consists of concise multiple sentence summaries of books. These summaries  capture the main plot points, key characters, and themes.   
%\end{enumerate}

    % \item \textbf{\textcolor{red}{Add one more dataset. non CMU}}

% \item \textbf{SummEval:} Ground truth labeled Data(pairwise scores): (text, summary, score[1-5])

% The above dataset is that embedding of movie/book plot should remain similar if names present in it are altered and should not vary too much.
\looseness=-1
% For both datasets, we extract plots where the plot text contains person and country names.
We select plots where the number of words are less than $250$ which is within token limit of most models under consideration\footnote{For each embedding model, we evaluate its performance only on samples which are within the limits of its maximum context window.}. 
% \textcolor{red}{Does this footnote mean that the evaluation set is different for different models? }\textcolor{blue}{-> yes tables contain the numbers of samples processed.}


\begin{table}[h]
\centering
% \small


\scalebox{0.7}{
\begin{tabular}{|ll|}
% Model  & Mean Similarity & Std. Error \\
\hline
 Model Name &  \makecell{Cosine sim  \\ per perturbation pair}   \\
%\midrule
\toprule
    all-mpnet-base-v2 &                  0.774 $\pm$ 0.001 \\
                 all-distilroberta-v1 &                  0.768 $\pm$ 0.001 \\
                     all-MiniLM-L6-v2 &                  0.706 $\pm$ 0.001 \\
                               gemini &                    0.885 $\pm$ 0.0 \\
           multi-qa-distilbert-cos-v1 &                  0.733 $\pm$ 0.001 \\
              paraphrase-MiniLM-L6-v2 &                  0.742 $\pm$ 0.001 \\
 distiluse-base-multilingual-cased-v1 &                  0.786 $\pm$ 0.001 \\
 distiluse-base-multilingual-cased-v2 &                  0.795 $\pm$ 0.001 \\
paraphrase-multilingual-MiniLM-L12-v2 &                   0.75 $\pm$ 0.001 \\
            msmarco-distilbert-cos-v5 &                  0.681 $\pm$ 0.001 \\
           multi-qa-mpnet-base-cos-v1 &                  0.743 $\pm$ 0.001 \\
               text-embedding-3-small &                    0.742 $\pm$ 0.0 \\
               text-embedding-3-large &                    0.779 $\pm$ 0.0 \\
                        voyage-3-lite &                     0.76 $\pm$ 0.0 \\
\bottomrule



\end{tabular}}
\caption{\textbf{Bias Measurement on CMU Movie dataset}. For each show, we create \textbf{$K{=}20$ perturbations} by replacing person names and country names.  In this experiment, we used plot samples that contain both person and country names but does not mention any city/town/village/nationality keywords(Spanish, American etc.) in order to minimize the impact of other variables. \label{tab:model_biases_cmu_movies}We report the mean and the std. error rounded off to $3$ decimal places. }
\end{table}






\begin{table}[h!]
\centering
% \small

\scalebox{0.7}{
\begin{tabular}{|ll|}
% Model  & Mean Similarity & Std. Error \\
\hline
 Model Name &  \makecell{Cosine sim  \\ per perturbation pair}   \\
%\midrule
\toprule

                    all-mpnet-base-v2 &                  0.777 $\pm$ 0.001 \\
                 all-distilroberta-v1 &                  0.778 $\pm$ 0.001 \\
                     all-MiniLM-L6-v2 &                  0.693 $\pm$ 0.001 \\
                               gemini &                     0.89 $\pm$ 0.0 \\
           multi-qa-distilbert-cos-v1 &                  0.743 $\pm$ 0.001 \\
              paraphrase-MiniLM-L6-v2 &                  0.735 $\pm$ 0.002 \\
 distiluse-base-multilingual-cased-v1 &                  0.777 $\pm$ 0.001 \\
 distiluse-base-multilingual-cased-v2 &                  0.785 $\pm$ 0.001 \\
paraphrase-multilingual-MiniLM-L12-v2 &                  0.746 $\pm$ 0.002 \\
            msmarco-distilbert-cos-v5 &                  0.707 $\pm$ 0.001 \\
           multi-qa-mpnet-base-cos-v1 &                   0.75 $\pm$ 0.001 \\
               text-embedding-3-small &                  0.761 $\pm$ 0.001 \\
               text-embedding-3-large &                  0.795 $\pm$ 0.001 \\
                        voyage-3-lite &                  0.781 $\pm$ 0.001 \\
\bottomrule
\end{tabular}}

\caption{\textbf{Bias Measurement on CMU Books dataset.} We follow the same evaluation setup as in Table ~\ref{tab:model_biases_cmu_movies}.\label{tab:model_biases_cmu_book}}
\end{table}

\subsection{Analyzing Bias}
\looseness=-1
In Table~\ref{tab:model_biases_cmu_movies} and ~\ref{tab:model_biases_cmu_book} we observe a significant deviation in the average cosine similarity which should be close to one if the cosine similarity captured the real semantic similarity rather than information in names present in the text\footnote{We also experimented by using euclidean distance instead of cosine similarity in Tab.~\ref{tab:model_biases_cmu_book_euclidean} in Appendix. The conclusion remained similar and therefore we proceeded with cosine similarity for remaining experiments.}. Any deviation from one indicates that the embeddings are heavily biased by the choice of names rather than from the similarity of the text. Models like $msmarco{-}distilbert{-}cos{-}v5$ exhibit significant sensitivity to changes in person and country names, as evidenced by an average cosine similarity $\approx0.7$. This suggests that the model's embeddings may be heavily influenced by specific entities rather than capturing the underlying semantic meaning of the text. Observations from the evaluation of both datasets suggest that $gemini$ is the least biased model among all models considered. However, we observe that even \textit{gemini's} score is still far away from one indicating room for improvement.

\looseness=-1
In the above experiment, we replaced the names of people and countries and generated a perturbed text. One may ask: how much of the bias is from country name versus person names? To study this, we considered an experiment in which we perturbed the text by only replacing person names while keeping the country names as they were in the original text. We also examined variations in which all the perturbed names are sampled from the same country and demonstrate that bias persists even if text samples differ only by person names even from the same country. These results can be found in the App.~\ref{sec:person_name_perturbation}.
