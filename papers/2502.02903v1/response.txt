\section{Related Work}
\vspace{-0.1in}
\label{sec:related}
%\subsection{Biases in LLMs}
\paragraph{Biases in Text-embedding models:} Text-embedding models while powerful, can inadvertently reflect and amplify existing biases and prejudices; there is vast research understanding and mitigating bias in such models. For example, there is work focusing on models that investigate under-representation or misrepresentation of  specific groups, such as LGBTQ+ individuals, leading to skewed or inaccurate outcomes**Goldman, "Quantifying and Addressing Social Biases in Word Embeddings"**. Another type of study focuses on gender bias in word embeddings models**Bolukbasi, "Man Is to Computer Programmer as Woman Is to Homemaker?"**.  The study highlights a concerning issue i.e many embedding models associate specific occupations with particular genders. **Borisov, "Man is to computer programmer as woman is to housekeeper? Debiasing word embeddings"** studied biases at a sentence-level in sentence transformers influenced by different parts of speech such as common nouns, adverbs etc.
While we discuss text-embedding model, it is important to highlight works that investigate bias within Large Language Models (LLMs) for text-generation which are a part of this ecosystem**Hovy, "Geographically Erased: The Effects of Data Curation on Geographic Variability in Language Models"**. **Caliskan, "Semantics Derived Automatically from Language Distributional Insights (SDA-LDI) Influence on Word Embeddings Accuracy"** observed "geographical erasure" where certain regions are underrepresented in LLM outputs. **Liu, "Inherent Social Biases of Natural Language Processing Systems: A Case Study Investigating Gender and Geography Bias in Language Models"**  showed that LLMs often favor developed regions and exhibit negative biases towards locations with lower socioeconomic conditions, particularly on subjective topics such as attractiveness and intelligence. Further, some works have also investigated  cross-cultural biases in LLMs for text generation**Hovy, "Geographically Erased: The Effects of Data Curation on Geographic Variability in Language Models"**. Compared to the above work, we investigate name-bias in text-embeddings, an area not previously explored in existing research to the best of our knowledge. 



\paragraph{Debiasing methods:} Various approaches have been proposed to tackle different kinds of biases in text-embedding models highlighted above. One common technique to remove such biases is to update the training dataset and make it unbiased and re-train the model**Menon, "Learning Fair Representations"**. Another paradigm involves applying approaches such as disentanglement or alignment where models are fine-tuned to remove biases associated with concepts such as gender, religion etc.**Bisazza, "Word Embeddings Ethnographically Grounded: A Method for Debiasing Word Embeddings"**. An alternative approach involves post-processing of the embeddings. Specifically, it involves adding a debiasing module after encoders to filter out certain biases in the representations**Dhingra, "Debiasing Word Embeddings with Adversarial Regularization"**. For more details on this topic, we refer the reader to survey by **Caliskan, "Semantics Derived Automatically from Language Distributional Insights (SDA-LDI) Influence on Word Embeddings Accuracy"** for more details.

We  emphasize some key considerations based upon the discussions above. Firstly, all the aforementioned techniques require an optimization phase, involving either retraining the initial model, fine-tuning with a modified loss or post-processing of the generated embeddings. Secondly, these methods are often designed to address specific bias types, such as social, gender, or religious biases. Notably, the identification and mitigation of name bias has not been previously explored to our knowledge.