\section{Introduction}
Text-embedding models, which convert raw text such as sentences/paragraphs into concise numerical representations, have become fundamental tools for downstream NLP tasks in fields such as healthcare, education, law and scientific research~\citep{chrysostomou2022flexible, reimers2019sentence, tenney2019bert, nie2024text, sun2019bert4rec}.
A cosine similarity between embeddings is typically used \cite{zhang2019bertscore, mathur2019putting} although other types of similarities  \cite{steck2024cosine} are also possible. With a similarity measure, the goal is to find which two texts are similar to or different from one another.  For simplicity, we will use \textit{text-embedding model} to refer to models that convert text to an embedding.



Many text-embedding models are often trained on large amounts of Internet text. This data can inadvertently contain biases of various kinds, reflecting social prejudices and stereotypes. As a result, these models can generate biased embeddings, reinforcing harmful stereotypes or discriminating against certain cultural groups, genders, etc.~\citep{gallegos2024bias, li2023survey, rakivnenko2024bias}. Furthermore, the presence of bias in models could lead to embeddings that disproportionately emphasize particular parts of the text, consequently failing to capture the true semantics and themes within the text~\citep{rakivnenko2024bias}.

 


While important, existing studies on biases, predominantly examine biases in text-embedding models mostly related to gender, geography, race, religion etc.~\citep{rakivnenko2024bias, may2019measuring, bolukbasi2016man, kotek2023gender, nghiem2024you}. In this paper, we demonstrate that text-embedding models exhibit significant bias towards \textit{names} within the text. To illustrate this, we begin with a motivating example in Table~\ref{tab:opening_example_story}. We present a simple narrative (\textit{Story 1}). We then show a similar plot while substituting the name of the main character in (\textit{Story 2}). In the third narrative (\textit{Story 3}), we introduce a distinct and contradicting storyline from \textit{Story 1 }while retaining the original character names. We embed all three stories using text-embedding models.  We observe that the similarity between Story 1 and Story 3, despite their differing plots, is consistently higher than the similarity between Story 1 and Story 2, which share highly semantically similar plots but differ in character names. This is very counter-intuitive since the text-embedding models seem to prioritize name similarity over the text's narrative structure. While this is admittedly an illustrative example, we proceed to generate numerous such narratives and conduct a thorough investigation of this bias in our experiments. {\em We emphasize here that our study investigates thematic and semantic similarities within textual data while acknowledging certain applications involving text tied to specific individuals or locations, our primary focus lies on the broader thematic context rather than characters in the text.}

Our observation reveals a critical issue that can significantly impact applications that rely on semantic similarity, including semantic search, information retrieval, and plagiarism detection~\citep{minaee2024large, pudasaini2024survey}: consider the challenge of accurately assessing the similarity between two stories/plots with identical underlying meanings but distinct character names. Current methods may erroneously classify these stories as dissimilar, leading to inconsistent and unreliable results. Further, based upon our investigation, we would like to mention upfront that the issue is not confined to certain cultures, cross-culture, but is universal in the sense that the name bias issue occurs in a very broad sense.


\begin{table}[ht!]
\centering
% \begin{minipage}{0.45\textwidth}
\centering
% \small
\scalebox{0.8}{
\begin{tabular}{lp{7.6cm}}

 \hline

Story Id & Text \\

\hline

Story 1 & \textit{Alejandro} gently examined the injured bird. He gave it food. \\

% \hline

Story 2 & \textit{Jelani} tenderly inspected the wounded bird and gave it a meal to eat.\\

% \hline

Story 3 & \textit{Alejandro} tracked the injured bird. He used it as his food. \\

% \hline

\end{tabular} 
}
% \caption{Table containing three stories. Story 1 and Story 2 have similar content but different person names. Story 1 and Story 3 have different meaning but same name.}
% \end{minipage}

\vspace{1em} % Add some vertical space between the tables

% \begin{minipage}{0.45\textwidth}
% \centering
% \small
\scalebox{0.61}{
\begin{tabular}{lcc}
\toprule
Model & \multicolumn{2}{c}{Cosine Similarity} \\
\cmidrule(lr){2-3}
& Story1, Story 2 $\uparrow$ & Story 1, Story 3 $\downarrow$\\
\midrule

                    all-mpnet-base-v2 &                                         0.755 &                                0.778 \\
                 all-distilroberta-v1 &                                         0.780 &                                0.798 \\
                     all-MiniLM-L6-v2 &                                         0.660 &                                0.853 \\
                               gemini &                                         0.864 &                                0.848 \\
           multi-qa-distilbert-cos-v1 &                                         0.579 &                                0.907 \\
              paraphrase-MiniLM-L6-v2 &                                         0.775 &                                0.855 \\
 distiluse-base-multilingual-cased-v1 &                                         0.752 &                                0.889 \\
 distiluse-base-multilingual-cased-v2 &                                         0.742 &                                0.875 \\
paraphrase-multilingual-MiniLM-L12-v2 &                                         0.836 &                                0.840 \\
            msmarco-distilbert-cos-v5 &                                         0.584 &                                0.817 \\
           multi-qa-mpnet-base-cos-v1 &                                         0.694 &                                0.854 \\
                        voyage-3-lite &                                         0.780 &                                0.868 \\
               text-embedding-3-small &                                         0.755 &                                0.826 \\
               text-embedding-3-large &                                         0.741 &                                0.808 \\

\hline
\end{tabular}
}
% \caption{Similarity between embeddings of (Story 1, Story 2) and (Story 1, Story 3) based upon different LLMs.\label{tab:opening_example_sim}}
\caption{\textbf{Impact of names on similarity}: %This example focuses on bias arising from names, specifically  person names. 
We see that Story 1 is similar to Story 2 but has different person names(\textit{Alejandro}, \textit{Jelani}). Story 3 is different from Story 1 but has same name (\textit{Alejandro}) as Story 1. We observe that, in most embedding models a different story with opposite meaning and same name(\textit{Alejandro}) is getting a higher similarity score in comparison to the same story with different names. \label{tab:opening_example_story}}
% \end{table*}
\vskip -0.1in
% \end{minipage}
\end{table}


Having briefly revealed the issue of name bias in text-embedding models, we  outline our contributions in the work:
%\begin{enumerate}

    \looseness=-1
    First, we identify bias arising from names in textual content. Although several forms of biases have been studied in the past (see Sec.\ref{sec:related}), to the best of our knowledge, our work is the first that specifically looks at bias associated with names and how they can influence the embeddings coming out of embedding models. Toward this end, we propose a benchmarking study to comprehensively assess this bias.
    
    \looseness=-1
    Second, we propose a simple \textit{inference-time text-anonymization} technique designed to overcome the identified bias. Our method does not require any model fine-tuning or retraining of the text-embedding models. The approach offers a simple, intuitive, and effective way to mitigate the problem rather than relying on complex computations.
    % It is a simple, intuitive and an effective way to mitigate the problem rather than a complex computation method to overcome the issue. 

    \looseness=-1
    Third, we conducted extensive experiments to study the identified problem in detail on a variety of text-embedding models and tasks. Our results demonstrate that our anonymization approach effectively reduces name bias within embeddings in semantic similarity and downstream tasks. 
    %This leads to better performance on semantic similarity tasks. Moreover, we showcase the potential of this solution to improve the outcomes of downstream tasks that extend beyond semantic similarity.

    \vspace{-0.05in}
   






