
\section{Conclusion}
In this work, we highlight the bias in text embeddings stemming from the presence of names in the text. We showed concrete examples, over multiple text-embedding models, that similarities between embeddings can be dominated by names in the text rather than the semantic meanings of the text. We then proposed a method to mitigate bias by performing anonymization at inference time. This involved the removal of names from the text and using the anonymized text to create the embeddings. Our findings demonstrate that anonymized text embeddings significantly outperform deanonymized text embeddings on tasks involving semantic similarity. While we proposed one way to mitigate the issue through anonymization, a deeper question that remains is: how  to train text-embedding models such that the embeddings capture the semantics more than the names in the text? 
