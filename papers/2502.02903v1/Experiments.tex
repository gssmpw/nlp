


\begin{table*}[ht!]
\centering
% \small
\scalebox{0.8}{
\begin{tabular}{lp{5cm}|p{4cm}|p{3.1cm}}
\toprule
% Model & \textbf{AUC: Original text} &  & \textbf{AUC(Our Method)}   \\ 
                                
                               Model  & \textit{Original text:} The \textit{(query, positive)} paragraphs share the same meaning but different person/location names. The \textit{(query, negative)} paragraphs share different meaning but same person/location names. &  \textit{Identical Names:} The (\textit{query}, \textit{positive},  \textit{negative}) paragraphs in the same triplet contain the same person/location names. &  \textit{Anonymized text:} Anonymization applied to (\textit{query}, \textit{positive},  \textit{negative}) paragraphs. \\
                            
\midrule


                    all-mpnet-base-v2 &                   0.19 &                        0.96 &       0.98 $\pm$ 0.0071 \\
                 all-distilroberta-v1 &                   0.36 &                        0.97 &      0.975 $\pm$ 0.0106 \\
                     all-MiniLM-L6-v2 &                   0.09 &                        0.94 &       0.99 $\pm$ 0.0071 \\
                               gemini &                   0.71 &                        1.00 &           1.0 $\pm$ 0.0 \\
           multi-qa-distilbert-cos-v1 &                   0.07 &                        0.97 &       0.97 $\pm$ 0.0071 \\
              paraphrase-MiniLM-L6-v2 &                   0.14 &                        0.98 &          0.98 $\pm$ 0.0 \\
 distiluse-base-multilingual-cased-v1 &                   0.27 &                        0.95 &          0.94 $\pm$ 0.0 \\
 distiluse-base-multilingual-cased-v2 &                   0.26 &                        0.98 &          0.96 $\pm$ 0.0 \\
paraphrase-multilingual-MiniLM-L12-v2 &                   0.21 &                        1.00 &          0.99 $\pm$ 0.0 \\
            msmarco-distilbert-cos-v5 &                   0.10 &                        0.92 &      0.955 $\pm$ 0.0035 \\
           multi-qa-mpnet-base-cos-v1 &                   0.08 &                        0.97 &           1.0 $\pm$ 0.0 \\
               text-embedding-3-small &                   0.12 &                        1.00 &           1.0 $\pm$ 0.0 \\
               text-embedding-3-large &                   0.21 &                        1.00 &           1.0 $\pm$ 0.0 \\
                        voyage-3-lite &                   0.18 &                        1.00 &           1.0 $\pm$ 0.0 \\

\bottomrule
\end{tabular}}
% \caption{\textbf{Evaluation on Semantic Similarity Task}: AUC scores obtained on Semantic Similarity Task.  Performance of all models falls significantly, in some cases AUC-ROC is below $0.1$ , which is significantly worser than a random classifier. Further, our proposed strategy of anonymization achieves high quality results across all  models. Mean and standard error are reported based on results from two separate LLM runs for anonymization. }\label{tab:sts_generated_with_reference_metric}
\caption{\textbf{Evaluation on Task 1: Semantic Similarity Task.} AUC scores obtained on Semantic Similarity Task. Our proposed strategy of anonymization achieves high quality results across all models. Mean and standard error are reported based on results from two separate LLM runs for anonymization. }\label{tab:sts_generated_with_reference_metric}
\end{table*}

% The third column shows that if same names were used in query, positive and negative paragraphs, all models are able to differentiate the semantic meaning between (query, positive) and (query, negative) pairs.


\begin{table*}[h!]
% \small
    \centering
    \scalebox{0.9}{
    \begin{tabular}{|c|p{5cm}|p{7cm}|p{1cm}|p{1cm}|}
    \toprule
     & Query & Pos/Neg & Sim score & Label \\
    \hline
    \multirow{2}{*}{Original} & \multirow{2}{5cm}{Alejandro quickly ran to the store to buy a cold drink. He was eager to have a glass of cold drink.} & \textcolor{blue}{\textbf{POS:} Quickly, Hiroki dashed to the local market to procure some cold drinks. He was yearning for a chilled glass of cold drink.} & 0.58 & 1 \\
    & & \textcolor{red}{\textbf{NEG:} Alejandro has stopped buying cold drinks from market. He only drinks cold drinks made at home.} & 0.69 & 0 \\
    \cline{3-5}
    \multirow{2}{*}{Anonymized} & \multirow{2}{5cm}{quickly ran to the store to buy a cold drink. He was eager to have a glass of cold drink.} & \textcolor{blue}{\textbf{POS:} Quickly,  dashed to the local market to procure some cold drinks. He was yearning for a chilled glass of cold drink.} & 0.80 & 1 \\
    & & \textcolor{red}{\textbf{NEG:} has stopped buying cold drinks from market. He only drinks cold drinks made at home.} & 0.47 & 0 \\
    \hline
    \end{tabular}}
     \centering
     \scalebox{0.9}{
    \begin{tabular}{|c|p{5cm}|p{7cm}|p{1cm}|p{1cm}|}
    \hline
    \multirow{2}{*}{Original} & \multirow{2}{5cm}{Ganga and Yamuna are two mighty rivers. They are lifelines for millions of people in the region.} & \textcolor{blue}{\textbf{POS:} Yangtze is a mighty river. It is a long river and is the lifeline for millions of people in the region.} & 0.54 & 1 \\
    & & \textcolor{red}{\textbf{NEG:} Ganga and Yamuna are two sisters. They had their schooling in the region and schooling provided a lifeline for them.} & 0.70 & 0 \\
    \cline{3-5}
    \multirow{2}{*}{Anonymized} & \multirow{2}{5cm}{and are two mighty rivers. They are lifelines for millions of people in the region.} & \textcolor{blue}{\textbf{POS:} is a mighty river. It is a long river and is the lifeline for millions of people in the region.} & 0.70 & 1 \\
    & & \textcolor{red}{\textbf{NEG: }and are two sisters. They had their schooling in the region and schooling provided a lifeline for them.} & 0.56 & 0 \\
    \hline
    \end{tabular}}
    \caption{Examples showing impact of anonymization on semantic similarity using embeddings created by \textit{msmarco-distilbert-cos-v5}.}
    \label{tab:sim_change_deloc_msmarco}
    \end{table*}


\begin{table*}[ht!]
\centering
% \small
\scalebox{0.75}{



\begin{tabular}{lll|ll}
\toprule
% \small
                                model &  \makecell{Spearman-correlation \\ 
                                (Original Text)} & \makecell{Spearman-correlation \\ (Anonymized)} & \makecell{Pearson-correlation \\ (Original Text)} & \makecell{Pearson-correlation \\ (Anonymized)}  \\
\midrule

                    all-mpnet-base-v2 &                              0.262 &                 \textbf{0.344  $\pm$ 0.001} &                              0.321 &                \textbf{0.364  $\pm$ 0.002} \\
                 all-distilroberta-v1 &                              0.245 &              \textbf{0.327  $\pm$ 0.007} &                              0.302 &                \textbf{0.37  $\pm$ 0.003} \\
                     all-MiniLM-L6-v2 &                              0.251 &                 \textbf{0.33  $\pm$ 0.003} &                              0.282 &                \textbf{0.354  $\pm$ 0.006} \\
                               gemini &                              0.381 &                \textbf{0.39  $\pm$ 0.001} &                              \textbf{0.456} &                0.436  $\pm$ 0.003 \\
           multi-qa-distilbert-cos-v1 &                              0.240 &                 \textbf{0.292  $\pm$ 0.002} &                              0.269 &                \textbf{0.316  $\pm$ 0.007} \\
              paraphrase-MiniLM-L6-v2 &                              0.283 &                \textbf{0.352  $\pm$ 0.005 }&                              0.317 &                  \textbf{0.37  $\pm$ 0.0} \\
 distiluse-base-multilingual-cased-v1 &                              0.282 &                \textbf{0.356  $\pm$ 0.001} &                              0.325 &               \textbf{0.386  $\pm$ 0.002} \\
 distiluse-base-multilingual-cased-v2 &                              0.308 &                  \textbf{0.357  $\pm$ 0.0} &                              0.345 &                \textbf{0.389  $\pm$ 0.003} \\
paraphrase-multilingual-MiniLM-L12-v2 &                              0.261 &                 \textbf{0.332  $\pm$ 0.001} &                              0.281 &               \textbf{0.364  $\pm$ 0.004} \\
            msmarco-distilbert-cos-v5 &                              0.232 &                \textbf{0.304  $\pm$ 0.002} &                              0.262 &                \textbf{0.333  $\pm$ 0.005} \\
           multi-qa-mpnet-base-cos-v1 &                              0.274 &                 \textbf{0.324  $\pm$ 0.002 }&                              0.317 &                \textbf{0.354  $\pm$ 0.001} \\
               text-embedding-3-small &                              0.374 &                 \textbf{0.382  $\pm$ 0.002} &                              0.416 &                \textbf{0.422  $\pm$ 0.005} \\
               text-embedding-3-large &                              0.366 &                 \textbf{0.382  $\pm$ 0.007} &                              0.428 &                \textbf{0.429  $\pm$ 0.012} \\
                        voyage-3-lite &                             \textbf{0.359} &                 0.322  $\pm$ 0.005 &                              \textbf{0.400 } &                0.352  $\pm$ 0.002 \\


                        
\bottomrule
\end{tabular}


}
\caption{\textbf{Evaluation on Task 2: Semantic similarity with graded relevance.} The table presents correlation between cosine similarity  between human \& machine summaries and relevance(ground truth) provided by human evaluators \label{tab:eval_machine_summary_downstream}.  Mean and standard error are reported based on results from two separate LLM runs for anonymization.}
\end{table*}


\section{Can anonymization help in down-stream tasks that use similarity from text-embedding models?}
\looseness=-1
In this section, we investigate the performance of the anonymized text embeddings on two downstream tasks.  Both the tasks are based on obtaining a similarity score between pieces of texts. These tasks are primarily based upon semantic similarity which find applications in areas such as information retrieval, clustering, plagiarism detection, question answering etc.~\citep{reimers2019sentence}. The two tasks that we evaluate on differ in various aspects such as the nature of the task, evaluation methodology, the judgment score available, etc. 
On both these tasks, our experiments show that embeddings based on anonymized text can significantly help in downstream tasks.


\subsection{Task 1: Semantic Similarity Between Query and Text-Pairs with Binary Labels.}
\label{sec:exp_sts_binary}
Recall from Sec.~\ref{sec:benchmarking} that altering only the names/locations in two otherwise identical stories/paragraphs significantly impacted their text embeddings. In this section, we investigate whether anonymization technique proposed in Sec.~\ref{sec:method} can effectively mitigate this type of bias. Towards this, we explore the Semantic Similarity Task (STS).


Semantic similarity seeks to determine the degree to which two pieces of text convey similar meaning~\cite{muennighoff2022mteb, reimers2019sentence}. This goes beyond simple word matching, aiming to understand the underlying meaning within the text. In today's era of deep learning~\cite{reimers2016task, muennighoff2022mteb}, achieving accurate semantic similarity relies heavily on high-quality embeddings, which represents sentences as dense vectors in a continuous space. 
% The desired embeddings should capture nuanced semantic relationships, enabling robust comparison of the underlying semantic concepts in the text.

% Given a pair of sentences, the task is to assess their semantic similarity and assign a score.
\looseness=-1
In this experiment we investigate whether the text-embeddings are able to capture the semantic nuances within the text or are they biased towards names? Ideally, a good embedding model should be able to differentiate reasonably well between two stories/paragraphs which have very different themes even if they contain same names. To investigate this, we create a dataset of $10$ paragraph triplets. Each triplet includes a \textit{query} paragraph, a \textit{positive} paragraph that is \textit{highly semantically similar} but with distinct person and location names, and a \textit{negative} paragraph that is semantically \textit{dissimilar} to the query text but has same person names/location names as in query text. For each triplet, (\textit{query}, \textit{positive}) pair is assigned a label 1(positive) and \textit{(query, negative)} pair is assigned a label 0(negative). Two sample examples can be found in Table~\ref{tab:sim_change_open_ai} in the the rows marked as \textit{Original}. The entire set of generated triplets with labels are present in Appendix~\ref{sec:dataset_sts}.  We evaluate the performance of different models on the STS task using \textit{AUC ROC score} between cosine similarity scores of embeddings and the ground truth.  


\paragraph{Peformance on Semantic Similarity.}
 Tab.~\ref{tab:sts_generated_with_reference_metric} presents the AUC-ROC scores for different models on the STS task. The  results indicate that the AUC scores for the majority of models are significantly below $0.5$. This finding suggests a critical issue, as even a random classifier would be expected to achieve an AUC score of approximately $0.5$. The fact that most of the AUC is much below $0.5$ suggests that the cosine similarity based ranking got the ordering wrong! 
 Gemini's AUC is better than random, however, it also gets improved significantly after anonymization. Such low AUC scores strongly imply that the embeddings used in these models are primarily capturing identity-related information, leading to a significant bias in the model's embeddings and predictions. Next, we observe that the AUC-ROC results post anonymization. We see that anonymization can improve the model's capacity to grasp the core semantic meaning in the text as reflected in the significantly higher AUC-ROC numbers(closer to 1). Additionally, it is important to note that all models attain high AUC scores when all stories share identical names.  This indicates that the models can effectively distinguish between sentences conveying the same or different meanings when identity information remains constant. The aforementioned observations highlights that anonymization is crucial to avoid situations where semantically equivalent paragraphs are assigned unique embeddings solely based on the presence of identity information (such as names). Conversely, it's essential that when texts have significant semantic variations, even if they contain identical identity information, their embeddings are able to able to capture it.
 % This implies, the models do have the capability to distinguish between different sentences having different/same meanings provided that the identity information remains unchanged. The aforementioned observations highlight the crucial role of anonymization in mitigating the risk of semantically equivalent paragraphs being assigned distinct embeddings due solely to the presence of identity information like person names. 

 % and ~\ref{tab:sim_change_deloc_msmarco}
 \looseness=-1
\paragraph{Examples of similarity post-anonymization.}
In Tab.~\ref{tab:sim_change_deloc_msmarco}, we show some instances of how similarity values between embeddings change between \textit{(query, positive)} pair and  \textit{(query, negative)} pair post anonymization. Before anonymization, the models assigned higher similarity scores to negative pairs and lower similarity scores to positive pairs in a counterintuitive way. Anonymization resulted in the models predominantly attending to the semantic structure of the text, which is accurately reflected in similarity scores. We would like to highlight that these samples are a subset of examples used for AUC computation on the STS task in Tab.~\ref{tab:sts_generated_with_reference_metric}.


\subsection{Task 2: Semantic Similarity With Graded Human Relevance.}
\label{sec:exp_sts_graded_1_5_summ}  

In the previous task, a binary approach was employed to assess text pair similarity, categorizing text-pairs as either similar or dissimilar.  In the task proposed in this section, we employ a more refined approach for evaluation by utilizing a graded relevance scale from 1 to 5 between a pair of text. The graded scale enables a more nuanced and granular assessment of semantic similarity between pairs, providing a richer understanding of their relationship.
% In this task, we examine the impact of data anonymization on text embedding models' ability to accurately measure text similarity in a more granular fashion. This task uses human-graded relevance scores (1-5) for evaluation, offering a more granular assessment compared to previous binary-labeled tasks. 
To evaluate this, we use the machine summary evaluation task from ~\citet{muennighoff2022mteb}, which involves automatically assessing the relevance of machine-generated summaries, commonly assessed by calculating the semantic similarity between the embeddings of the summary and the original document/human summaries.

For this task, we follow the same evaluation setup as~\citet{muennighoff2022mteb} which we describe next. We use the SummEval dataset~\cite{fabbri2021summeval, muennighoff2022mteb} with $100$ text samples, each containing $16$ machine and $10$ human summaries. Human relevance scores ($1{-}5$) are assigned to each machine summary. We first obtain summary embeddings using text-embedding models for each machine summary and human summary in all $100$ samples. Without loss of generality, for a given text sample out of the $100$ samples, for each machine summary $\{m_i \mid 1 \leq i \leq 16\}$, we get its predicted score based on its maximum cosine similarity to any human summary $\{h_j \mid 1 \leq j \leq 10\}$ within the same text sample i.e $machine\_pred\_score(m_i)= max_{1 \leq j \leq 10}\; cos\_sim(m_i, h_j)$. This yields $16$ machine summary quality predicted scores for each sample i.e $1$ predicted score for each machine summary. Further, as mentioned earlier, we have a human relevance score assigned to each machine summary. Overall, across all text samples, we get $1600$ \textit{machine summary predicted scores} and its corresponding \textit{human relevance scores}.  We then correlate these two scores using Pearson and Spearman coefficients~\cite{muennighoff2022mteb}. Higher correlations indicate better alignment between model-assigned scores and human judgments, suggesting more reliable evaluation.
\vspace{-0.1in}



\paragraph{Impact of Anonymization}


Table~\ref{tab:eval_machine_summary_downstream} shows that post-anonymization, the performance of various text-embedding models significantly improves in predicting graded human-rated summary quality. Spearman and Pearson correlation coefficients increase substantially, indicating that the model's assessment of summary quality after anonymization better aligns with human evaluations. This improvement is substantial, with some models like \textit{all-distilroberta-v1} showing a performance increase of around $30\%$.

\noindent
In summary, the results of both downstream tasks demonstrate a substantial enhancement in the semantic similarity post-anonymization. 






