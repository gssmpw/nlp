\section{Related Work}
\vspace{-0.1in}
\label{sec:related}
%\subsection{Biases in LLMs}
\paragraph{Biases in Text-embedding models:} Text-embedding models while powerful, can inadvertently reflect and amplify existing biases and prejudices; there is vast research understanding and mitigating bias in such models. For example, there is work focusing on models that investigate under-representation or misrepresentation of  specific groups, such as LGBTQ+ individuals, leading to skewed or inaccurate outcomes~\citep{may2019measuring, bolukbasi2016man, cheng2021fairfil}. Another type of study focuses on gender bias in word embeddings models~\citep{rakivnenko2024bias}.  The study highlights a concerning issue i.e many embedding models associate specific occupations with particular genders. ~\citet{nikolaev2023representation} studied biases at a sentence-level in sentence transformers influenced by different parts of speech such as common nouns, adverbs etc.
While we discuss text-embedding model, it is important to highlight works that investigate bias within Large Language Models (LLMs) for text-generation which are a part of this ecosystem~\citep{gallegos2024bias}. ~\citet{schwobel2023geographical} observed "geographical erasure" where certain regions are underrepresented in LLM outputs. ~\citet{manvi2024large}  showed that LLMs often favor developed regions and exhibit negative biases towards locations with lower socioeconomic conditions, particularly on subjective topics such as attractiveness and intelligence. Further, some works have also investigated  cross-cultural biases in LLMs for text generation~\citep{naous2023having, ramezani2023knowledge, cao2023assessing, arora2022probing}. Compared to the above work, we investigate name-bias in text-embeddings, an area not previously explored in existing research to the best of our knowledge. 



\paragraph{Debiasing methods:} Various approaches have been proposed to tackle different kinds of biases in text-embedding models highlighted above. One common technique to remove such biases is to update the training dataset and make it unbiased and re-train the model~\citep{brunet2019understanding, ngo2021mitigating}. Another paradigm involves applying approaches such as disentanglement or alignment where models are fine-tuned to remove biases associated with concepts such as gender, religion etc.~\citep{kaneko2021debiasing, guo2022auto, kenneweg2024debiasing}. 
An alternative approach involves post-processing of the embeddings. Specifically, it involves adding a debiasing module after encoders to filter out certain biases in the representations~\citet{cheng2021fairfil}. For more details on this topic, we refer the reader to survey by ~\citet{li2023survey} for more details.

We  emphasize some key considerations based upon the discussions above. Firstly, all the aforementioned techniques require an optimization phase, involving either retraining the initial model, fine-tuning with a modified loss or post-processing of the generated embeddings. Secondly, these methods are often designed to address specific bias types, such as social, gender, or religious biases. Notably, the identification and mitigation of name bias has not been previously explored to our knowledge.