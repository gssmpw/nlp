% motivation
Human-Robot Collaboration (HRC) is a critical component of the Industry 4.0 revolution, boosting productivity and efficiency in various industries, including manufacturing and logistics 
\cite{HRC_industry_4,survey_review_2022_object_handovers}. The transition from separate workspaces to shared environments where humans and robots collaborate allows for a more integrated approach to task execution. In this collaborative setting, humans and robots share responsibilities, each bringing their own strengths to improve overall operational effectiveness \citet{human_role_safety_collab_industry4}. 
%This collaboration not only potentially streamlines workflows but also promotes innovation, as robots assist humans with complex tasks while adapting to their real-time needs.
Thus, understanding human intent and facilitating seamless interactions between humans and robots is essential for maximizing the benefits of HRC \cite{comm_intent_before_HRC_strabala2012,intention_multimodal_Wang2022}. A critical aspect of these interactions is 
%the use of non-invasive, non-contact based systems for 
recognizing human intent as early as possible and ideally, before actions occur \cite{non_invasive_real_time_activity_track_human,gaze_vs_hand_moving_target,comm_intent_before_HRC_strabala2012}. 
Traditional approaches often rely on visual cues and hand-tracking to detect intentions, but these methods may not always provide timely or accurate predictions, especially in complex collaborative environments \cite{intention_multimodal_Wang2022, comm_intent_before_HRC_strabala2012} where similar initial actions may lead to different outcomes. 
% \hl{Noninvasive physiological signals like EEG and gaze can offer continuous and unobtrusive monitoring, thus enhancing intention prediction accuracy without hindering human movement.
% Thus, to overcome the limitations of methods based on human motion tracking, a possible solution is the use of such physiological signals and it is needed to test their utility and comparison to the motion-tracking based methods.}
%and improve the robustness of collaborative tasks, it is crucial to develop such (hands-free), non-invasive techniques capable of reliably interpreting human action intentions.
This work focuses on the use of non-invasive, non-movement-based physiological signals to recognize human intent in human-robot interactions, using handovers as a case study\footnote[3]{All codes for data processing and model training presented in this study are published in the following URL: {\small \url{https://github.com/NonaRjb/Human-to-Robot-Handover-Detection.git}}. Also, the data can be accessed from the following URL and has been made publicly available: \small{\url{https://doi.org/10.5281/zenodo.14876712}}}. Signals like EEG and gaze can provide insights into human intentions and motions without relying on physical movement. As such, they offer a potential alternative to movement-dependent, vision-based methods for monitoring human actions, warranting evaluation and comparison against conventional approaches.

Handover, the act of passing an object to another person, is a crucial part of daily human interactions~\cite{survey_review_2022_object_handovers,When_where_how_human-human_studyStrabala}. Therefore, mastering handover skills is essential for robots to function effectively as collaborative partners \cite{survey_review_2022_object_handovers,data_driven_grip_release}. It is vital for a robot to recognize the human's intention to hand over an object and initiate the receiving action at the right time to avoid delays or failures \cite{intention_multimodal_Wang2022,nonaEEG}. Such failures can erode human trust and reduce the willingness to collaborate with robots in the future. Methods based on visual sensing for hand tracking can generate robot motion only after the human initiates movement for handover.
In contrast, approaches using physiological signals such as gaze and EEG may allow earlier intention detection, as they do not depend directly on physical movement. Human gaze behaviors during handovers have been studied to inspire robotic gaze in human-robot handovers \cite{shared_attention_gaze_hanover_timing_2014}. Meanwhile, EEG signals, which measure brain activity non-invasively, have shown potential in brain-computer interfaces (BCIs) to interpret human intentions in various contexts, including human-robot interaction \cite{varbu2022past}.
We develop and evaluate human intention detectors based on three different data modalities—EEG, gaze, and hand-tracking (by visual sensing)—comparing their accuracy and timing in detecting the intention to hand over an object to a robot. To the best of our knowledge, this is the first study to systematically develop and test intention detectors across these diverse modalities within the exact same experimental context of human-robot handovers. 

We investigate whether EEG or gaze can enhance intention detection performance beyond the hand-tracking baseline, enabling earlier and more accurate detection of handover intentions in close-proximity human-robot collaboration. The novelty of our study lies in: (i) conducting an experimental study to recognize human-to-robot handover intentions while isolating the intention from confounding factors such as movement direction and sequence; (ii) proposing and demonstrating EEG- and gaze-based intention detectors for human-robot handovers; and (iii) directly comparing these methods to traditional hand-tracking approaches, as used in \cite{contrast_handover_poses_for_intent_handover_cakmak2011,survey_review_2022_object_handovers}.

%Specifically, we evaluate whether EEG or gaze can improve performance over the hand-tracking baseline. 
Our research questions are:
\begin{enumerate}
    \item RQ1: To what extent can EEG or gaze be used to classify the upcoming motion for handover or non-handover before the motion has started? 
    %TO WHAT EXTENT CAN EEG OR GAZE, NON BINARY QUES
    \item RQ2: How early can a human intention to hand an object to a robot be detected based on different modalities?
    %Which modality out of the three gives the earliest indication to classify human motion as intended for handover or non-handover? %HOW EARLY CAN THE DIFFEENT MODALIITES BE USED
\end{enumerate}
In addition to our main goals, we assess how different classifiers affect intention detection performance. Specifically, we compare a classic linear classifier with a neural network for each modality. Given the ability of neural networks to identify complex patterns without extensive dimensionality reduction \cite{wang2016action,rudenko2020human,craik2019deep,tang2017single,george2016real}, we investigate whether they outperform linear models in detecting task-related patterns. In summary, our main contributions are:
\begin{itemize}
    \item We compare EEG, gaze, and hand motion trajectory modalities for their accuracy and timing in predicting whether upcoming human motion is an object handover.
    \item We demonstrate that gaze data is the fastest and most accurate in distinguishing handover intentions, showing predictive power even during the planning phase before movement begins.
    \item We show that combining these modalities in a multimodal model improves the performance of the weaker modality both before and after movement initiation.
    \item We introduce a hybrid dataset with simultaneous EEG, gaze, and hand motion trajectory recordings from 15 participants performing handover and non-handover motions in a human-robot collaboration setting.
\end{itemize}

\section{Related Work}
Recent advances in robotics have enabled robots to integrate into human environments, working alongside people in various settings~\cite{survey_review_2022_object_handovers}. Since handover is a crucial part of human interactions, human-robot handovers have been extensively studied in both human-human \cite{human_human_to_human_robot_study_Controzzi,datasetmultimodalhandovers} and human-robot interactions \cite{survey_review_2022_object_handovers,human_human_to_human_robot_study_Controzzi,When_where_how_human-human_studyStrabala}. A handover consists of two phases: the \emph{pre-handover} phase, which includes communicating the intent, and the \emph{physical exchange} of the object~\cite{survey_review_2022_object_handovers}. This work focuses on recognizing the human giver's intention in human-to-robot handovers. In particular, we compare different data modalities on their accuracy and timing in detecting human intention to hand over an object to a robot.

Common signals indicating the intention to hand over an object include moving closer to the receiver \cite{human_dog_behaviors_in_handovers} and using communication cues such as gaze \cite{shared_attention_gaze_hanover_timing_2014}, body posture, and verbal signals \cite{survey_review_2022_object_handovers}. Additional cues include reaching movements, leaning towards the receiver, the giver's orientation, hand occupancy, and gaze direction \cite{comm_intent_before_HRC_strabala2012}. Robotic vision is often used to predict human-giver movements during handovers, aiding in the planning and execution of the robot-taker's arm motion. However, this approach generally assumes the intention to hand over an object rather than explicitly detecting it \cite{survey_review_2022_object_handovers}. In \cite{basic_CV_for_intention_kwan2020handover}, handovers are distinguished from non-handover actions using object detection methods, skeletal key points, and head pose estimation based on gestures like extending an arm with an object in hand. Nevertheless, the features used might not be distinctive in scenarios where the movement sequences are similar for handover and non-handover tasks. Despite this, challenges such as complex backgrounds and constrained working spaces remain significant \cite{intention_multimodal_Wang2022}. A different approach in \cite{handover_with_wearable_EMG_Wang2019} uses a forearm-mounted wearable sensor that relies on electromyography (EMG) signals and arm rotation to classify human handover intentions, categorizing different gestures into actions. However, the considered human ``Give" gesture does not encompass all natural handover scenarios. Another method, the teaching-learning-prediction (TLP) model in \cite{intention_multimodal_Wang2022}, integrates speech, EMG-IMU (Inertial Measurement Unit) arm sensors, and image data to predict human actions during handovers. This approach typically recognizes human intention 1-2 seconds after the movement begins.

Human gaze behaviors during handovers have been studied for various aspects of human-robot handovers. For instance, \cite{shared_attention_gaze_hanover_timing_2014} found that givers in human handovers looked at both the handover location and the receiver’s face. During the handover motion, both givers and receivers are typically focused on each other’s hands \cite{robot_gaze_behav_in_h2r_handovers_kshirsagar}. Research has shown that human reaching movements are often preceded by saccadic eye motions toward the target \cite{gaze_vs_hand_moving_target}. Furthermore, \cite{eye_hand_coordination_2001} found that gaze fixation usually precedes or guides hand movements, indicating that gaze helps in planning hand movements by directing attention to key positions for the hands and objects.

% EEG related works
Motion tracking with visual sensing detects human movement, including handovers, only after it begins. If initial hand movements resemble non-handover motions, it can be difficult to determine the intended action and whether the robot should respond, even after the motion starts.
% In our previous work \cite{nonaEEG}, we used electroencephalogram (EEG) signals to detect human motion intentions in close-proximity human-robot collaboration. We demonstrated that an intention for a movement could be accurately detected from EEG signals before the movement occurs. Additionally, we showed that identifying the type of movement from EEG signals is feasible, though less accurate and robust than detecting movement intention.
EEG, a non-invasive method for measuring brain activity, is widely used in brain-computer interfaces (BCIs) to enable direct robot control via brain signals \cite{varbu2022past,lyu22, mondini2020continuous}. Previous research has successfully differentiated EEG signals associated with motor preparation from resting states \cite{MAMMONE2020source, 9534028}. However, identifying the type of the action is challenging, especially when the movement directions and body parts are identical across different actions. As a result, most studies on classifying movement intentions focus on significantly different trajectories~\cite{MAMMONE2020source, gordienko21, MOHSENI2020, ofner2019}. Recently, \cite{cooper20} showed that EEG signals differ between handover intentions and solo object movements, but their analysis was limited to group-level statistics, comparing average signals across conditions. Furthermore, since they only examined two scenarios—solo object movement and object handover—it remains unclear whether the observed differences stem from the presence of a robot or are truly indicative of the handover task. In our previous work~\cite{nonaEEG}, we addressed this limitation by introducing a third condition in which the human collaborates with the robot without a direct object handover. Our results revealed distinct EEG signals for handover intentions compared to other actions, even when non-direct joint actions were included. Furthermore, we demonstrated that single trials from different conditions could be successfully classified, although with lower accuracy compared to detecting general movement intentions versus no-movement conditions. These findings indicate the feasibility of using EEG signals to detect human intentions in object handovers to robots.

While the prior research has explored the use of gaze tracking, EEG signals, and motion tracking to detect human handover intentions, two key gaps remain. 
%First, \hl{no study} has fully developed and implemented an intention detector that uses these modalities in a real-world setting. 

First, to the best of our knowledge, an intention detector utilizing these modalities to classify handover or non-handover movements has not been developed or implemented in real-world scenarios.

Second, there is no direct comparison of the efficacy of these different modalities for detecting handover intentions in the same experiment. These gaps highlight the need for a comprehensive approach that not only builds a system using multiple modalities but also carefully tests and compares their effectiveness.
In this work, we compare \emph{EEG}, \emph{gaze}, and \emph{hand-tracking} (via visual sensing) to detect and predict human handover intentions in close-proximity human-robot collaboration. We analyze whether handover intentions differ from other types of intentions across these three modalities, particularly when movement direction and sequences are controlled for similarity. Unlike many prior studies, we do not assume handover intention in advance. Rather than focusing on extracting sophisticated features, we explore how effectively each modality distinguishes handover intentions from non-handover intentions.