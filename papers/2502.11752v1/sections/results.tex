
% Discuss something is in gaze before the movement. The percentages (people see robot more for handovers etc)

% [!MNE tests for gaze, not sure of interpretation. found many clusters before movement for gaze, but that does not reflect in the classification accuracy!]


\begin{figure}[h!]
    \centering
    \subfigure[]{\includegraphics[width=0.3\linewidth]{figures/median_cum_acc_all_subjects_eeg_lda.pdf}\label{fig:eeg_lda}}
    \subfigure[]{\includegraphics[width=0.3\linewidth]{figures/median_cum_acc_all_subjects_gaze_lda.pdf}\label{fig:gaze_lda}}
    \subfigure[]{\includegraphics[width=0.3\linewidth]{figures/median_cum_acc_all_subjects_motion_lda.pdf}\label{fig:motion_lda}}
    \subfigure[]{\includegraphics[width=0.3\linewidth]{figures/median_cum_acc_all_subjects_eeg_lstm.pdf}\label{fig:eeg_lstm}}
    \subfigure[]{\includegraphics[width=0.3\linewidth]{figures/median_cum_acc_all_subjects_gaze_lstm.pdf}\label{fig:gaze_lstm}}
    \subfigure[]{\includegraphics[width=0.3\linewidth]{figures/median_cum_acc_all_subjects_motion_lstm.pdf}\label{fig:motion_lstm}}
    \caption{\justifying Detecting Handover Intention, Results of LDA (a, b, c) and LSTM (d, e, f) training on increasing time windows for (a, d) EEG, (b, e) gaze, and (c, f) hand motion. Different colors show individual participants, with the black line as the median performance and shaded areas as the standard error. The x-axis shows the end time of the training window and the dashed line at 0.0 marks movement onset.}
    \label{fig:subj-lda-lstm}
\end{figure}

\subsection*{Detecting Handover Intention Based on Single Modalities} 

To compare the effectiveness of EEG, gaze, and hand motion trajectory modalities in detecting human-to-robot handovers, we trained a linear classifier using linear discriminant analysis (LDA) for each participant on data from individual modalities across increasing time windows. Starting from the cue onset (-5 seconds), we used time windows incremented by 250 ms up to 6 seconds after movement onset (-5 to -4.75, -5 to -4.5, ..., -5 to 6 seconds). The linear classifier was trained and tested on each time window separately, with the results presented in Fig.~\ref{fig:eeg_lda}~\ref{fig:gaze_lda} and~\ref{fig:motion_lda}. 
Our findings show that detection performance improves significantly for all modalities after the movement begins. However, gaze data shows an earlier increase in performance compared to EEG and hand motion trajectory. Notably, gaze data even demonstrates some predictive power before movement onset, indicating participants' movement planning phase. Although EEG performance starts its rise from 50\% earlier than the hand motion trajectory (0.75 s vs. 1.25 s after movement onset), The hand motion performance reaches higher values faster than EEG. 

% \begin{figure}[ht]
%     \centering
%     \subfigure[]{\includegraphics[width=0.325\linewidth]{figures/median_cum_acc_all_subjects_eeg_lstm.pdf}\label{fig:eeg_lstm}}
%     \subfigure[]{\includegraphics[width=0.325\linewidth]{figures/median_cum_acc_all_subjects_gaze_lstm.pdf}\label{fig:gaze_lstm}}
%     \subfigure[]{\includegraphics[width=0.325\linewidth]{figures/median_cum_acc_all_subjects_motion_lstm.pdf}\label{fig:motion_lstm}}
%     \caption{\justifying Results of LSTM training on increasing time windows for (a) EEG, (b) gaze, and (c) hand motion. Colors and the x-axis are described the same as in Fig.~\ref{fig:lda}}
%     \label{fig:lstm}
% \end{figure}

\begin{figure}[b!]
    \centering
    \subfigure[]{\includegraphics[width=0.49\textwidth]{figures/median_cum_acc_all_subjects_lda.pdf}\label{fig:all_lda}}
    \subfigure[]{\includegraphics[width=0.49\textwidth]{figures/median_cum_acc_all_subjects_lstm.pdf}\label{fig:all_lstm}}
    \caption{\justifying Comparing the handover detection performance of (a) LDA and (b) LSTM models across modalities. The graphs show the median performance for all modalities, with the shaded area indicating the 25th and 75th percentiles.}
    \label{fig:lda-lstm}
\end{figure}
\begin{table}[h!]
    %\centering
    \caption{Time (in seconds) at which different AUC-ROC levels are reached for the median performance of each modality. An AUC-ROC level is considered reached if it is sustained for at least three consecutive time steps; ``X" indicates the AUC-ROC level was not reached. Values in bold mean a significant difference was seen in values for one-way Anova analysis}
    
        \resizebox{0.45\textwidth}{!}{
        \begin{tabular}{cccc}
            \toprule
            \multirow{2}{*}{AUC Levels} & \multicolumn{3}{c}{Time in secs} \\
            \cmidrule{2-4} LDA
             & EEG & Gaze & Hand-Motion \\
            \midrule
            0.60 & 3.0 & 1.5 & 3.5\\
            0.65 & 3.75 & 2.0 & 3.5\\
            0.70 & 3.75 & \textbf{2.25} & 3.75\\
            0.75 & 4.5 & \textbf{2.5} & 4.5 \\
            0.80 & X & \textbf{3.0} & 4.75\\
            0.85 & X & \textbf{4.0} & X \\
            0.90 & X & X & X \\
            \bottomrule
        \end{tabular}
        }
        %\caption{LDA}
        \label{tab:performance_lda}
    %\vspace{0.01\textwidth}
    \hspace{0.001\textwidth}
        \resizebox{0.45\textwidth}{!}{
        \begin{tabular}{cccc}
            \toprule
            \multirow{2}{*}{AUC Levels} & \multicolumn{3}{c}{Time in secs} \\
            \cmidrule{2-4} LSTM
             & EEG & Gaze & Hand-Motion \\
            \midrule
            0.60 & 3.5 & \textbf{0.5} & 1.5\\
            0.65 & 3.0 & \textbf{1.0} & 2.25\\
            0.70 & X & \textbf{1.25} & 2.25\\
            0.75 & X & \textbf{1.75} & 2.5 \\
            0.80 & X & 1.75 & 2.75\\
            0.85 & X & 2.25 & 2.75\\
            0.90 & X & 2.75 & X \\
            \bottomrule
        \end{tabular}
        }
        %\caption{LSTM}
        \label{tab:performance_lstm_lda}
    
\end{table}
To quantify the differences between the median performances of each modality across participants, we identified the earliest time each modality reached a specific AUC-ROC level and maintained it for at least three consecutive time points. The results are summarized in Table~\ref{tab:performance_lda}. Our findings indicate that the gaze modality achieved the highest AUC-ROC among the three. Additionally, gaze data predicted the intention of handover faster than the other modalities, sometimes up to 1.75 to 2 seconds earlier than EEG or hand tracking (Table~\ref{tab:performance_lda}). Interestingly, EEG was neither faster nor more accurate than hand motion, as it did not achieve the same high AUC-ROC levels (Table~\ref{tab:performance_lda}).
\begin{comment}
\begin{table}[tb]
    \centering
    \begin{tabular}{ccccc}
        \toprule
        \multirow{2}{*}{AUC Levels-LDA} & \multicolumn{4}{c}{Time in secs} \\
        \cmidrule{2-5}
         & EEG & Gaze & Hand-Motion & \\
        \midrule
        0.60 & 3.0 & 1.5 & 3.5\\
        0.65 & 3.75 & 2.0 & 3.5\\
        0.70 & 3.75 & 2.25 & 3.75\\
        0.75 & 4.5 & 2.5 & 4.5 \\
        0.80 & X & 3.0 &  4.75\\
        0.85 & X & 4.0 & X \\
        0.90 & X & X & X \\
        \bottomrule
    \end{tabular}
    \caption{Time (in seconds) at which different AUC-ROC levels are reached for the median performance of each modality, with LDA. An AUC-ROC level is considered reached if it is sustained for at least three consecutive time steps; ``X" indicates the AUC-ROC level was not reached.}
    \label{tab:performance_lda}
\end{table}
\begin{table}[tb]
    \centering
    \begin{tabular}{ccccc}
        \toprule
        \multirow{2}{*}{AUC Levels-LSTM} & \multicolumn{4}{c}{Time in secs} \\
        \cmidrule{2-5}
         & EEG & Gaze & Hand-Motion & \\
        \midrule
        0.60 & X & 0.5 & 1.5\\
        0.65 & X & 1.0 & 2.25\\
        0.70 & X & 1.25 & 2.25\\
        0.75 & X & 1.75 & 2.5 \\
        0.80 & X & 1.75 & 2.75\\
        0.85 & X & 2.25 & 2.75\\
        0.90 & X & 2.75 & X \\
        \bottomrule
    \end{tabular}
    \caption{Time (in seconds) at which different AUC-ROC levels are reached for the median performance of each modality, with LSTM. An AUC-ROC level is considered reached if it is sustained for at least three consecutive time steps; ``X" indicates the AUC-ROC level was not reached.}
    \label{tab:performance_lstm}
\end{table}
    
\end{comment}




We repeated the experiment using a neural network classifier, specifically a long-short-term memory (LSTM) network~\cite{hochreiter1997long}, which is well-suited for sequence data like time series classification. The results, shown in Fig.~\ref{fig:eeg_lstm}~\ref{fig:gaze_lstm} and~\ref{fig:motion_lstm} and Table \ref{tab:performance_lstm_lda}, indicate that the LSTM outperforms the LDA classifier for both gaze and hand motion trajectory modalities. However, for EEG signals, the LSTM's performance is highly variable over time and across subjects, suggesting it failed to learn robust patterns from the data. We hypothesize this is due to the high-dimensional nature of EEG data, which demands more samples for reliable training. In this situation, neural networks are more prone to overfitting the noise in the data due to their large number of parameters. Fig.~\ref{fig:lda-lstm} displays the median participant performance of LDA and LSTM trained on different modalities in one plot.


\subsection*{Multimodal Handover Detection}
\begin{figure*}[t!]
    \centering
    \subfigure[]{\includegraphics[width=0.4\linewidth]{figures/median_compare_multimodal_lda_eeg-gaze.pdf}\label{fig:multimodal_eeg-gaze}}
    \subfigure[]{\includegraphics[width=0.4\linewidth]{figures/median_compare_multimodal_lda_eeg-motion.pdf}\label{fig:multimodal_eeg-motion}}
    \subfigure[]{\includegraphics[width=0.4\linewidth]{figures/median_compare_multimodal_lda_gaze-motion.pdf}\label{fig:multimodal_gaze-motion}}
    \subfigure[]{\includegraphics[width=0.4\linewidth]{figures/median_compare_multimodal_lda_eeg-gaze-motion.pdf}\label{fig:multimodal_all}}
    \caption{A comparison between LDA classifiers trained on single modalities versus multiple modalities using early or late fusion. (a) is based on EEG and gaze, (b) is EEG and hand motion, (c) is a gaze and hand motion, and (d) involves all three modalities. Each graph shows the median performance over subjects with the shaded area indicating the interquartile range. Note that a different set of participants is considered in each subplot based on the number of available trials per individual.}
    \label{fig:multimodal}
\end{figure*}

After assessing the predictive power of individual modalities for handover detection, we examined whether integrating these modalities into a multimodal classifier could enhance the performance. Using LDA as the classification method, we considered both (1) early fusion and (2) late fusion of different modalities. For early fusion, we concatenated data from all involved modalities into a single vector for the LDA model. For late fusion, we averaged the class probabilities from classifiers trained on each modality, weighted by their training performance, to generate new class probabilities. We then computed the AUC-ROC for these combined probabilities. The results of both approaches are shown in Fig.~\ref{fig:multimodal}. The results indicate that gaze is the most influential modality, enhancing the performance of both EEG and hand motion trajectory when used in a bimodal model (Fig.~\ref{fig:multimodal_eeg-gaze}, \ref{fig:multimodal_gaze-motion}). However, this enhancement is constrained by the inherent performance of the gaze data alone. Combining all three modalities with the early fusion approach significantly improves performance over each individual modality (Fig.~\ref{fig:multimodal_all}). Notably, this hybrid model can predict handover intention better than chance around one second before the expected movement onset at 0, although the performance is still not very high. 

The multimodal model was only trained for participants who had at least 60 uncorrupted trials of the modalities involved in training (Supplementary Table S3). Therefore, each of the four subplots in Fig.~\ref{fig:multimodal} may be based on a different set of subjects. Also for early fusion involving EEG, we first reduced the dimensionality of EEG signals using principal component analysis (PCA) and then concatenated the result with other modalities. This was to make the dimensionality of EEG signals comparable with that of other modalities.

\begin{figure}[b!]
    \centering
    \subfigure[]{\includegraphics[width=0.44\linewidth, height=0.5\linewidth]{figures/selected_subjects_gaze.pdf}\label{fig:selected_gaze}}
    \subfigure[]{\includegraphics[width=0.44\linewidth, height=0.5\linewidth]{figures/selected_subjects_motion.pdf}\label{fig:selected_motion}}
    \subfigure[]{\includegraphics[width=0.9\linewidth, height=0.52\linewidth]{figures/selected_subjects_eeg.pdf}\label{fig:selected_eeg}}
    \caption{AUC-ROC score over time for handover intent classification for selected participants from different modalities: (a) LSTM trained on gaze data from participant 2, (b) LSTM trained on hand motion data from participant 2, (c) LDA trained on EEG data from participants 2, 5, 6, 8, 9, and 12.}
    \label{fig:selected}
\end{figure}
% \begin{figure}[h!]
% \vspace{2mm}
%     \centering
%     \includegraphics[width=\linewidth]{figures/merged_lda.pdf}
%     \caption{Results LDA: Per participant AUC for classification at each time-step between -4.5 to +4.0 seconds with the movement onset cue at 0.0 seconds}
%     \label{fig:LDA results part wise}
% \end{figure}

% \begin{figure}[h!]
% \vspace{2mm}
%     \centering
%     \includegraphics[width=\linewidth]{figures/ALL PART_exp1 Gaze vs Hand Motion vs EEG LDA auc endms.png}
%     \caption{Results LDA: Combined, AUC for classification at each time-step between -4.5 to +4.0 seconds with the movement onset cue at 0.0 seconds}
%     \label{fig:LDA results all part}
% \end{figure}


% \begin{figure}[h!]
% \vspace{2mm}
%     \centering
%     \includegraphics[width=\linewidth]{figures/merged_lstm.pdf}
%     \caption{Results LSTM: Per participant AUC for classification at each time-step between -4.5 to +4.0 seconds with the movement onset cue at 0.0 seconds}
%     \label{fig:LsTM results all part}
% \end{figure}

% \begin{figure}[h!]
% \vspace{2mm}
%     \centering
%     \includegraphics[width=\linewidth]{figures/ALL_PART_exp1 Gaze vs Hand Motion vs EEG LSTM SD auc endms.pdf}
%     \caption{Results LSTM: Combined, AUC for classification at each time-step between -4.5 to +4.0 seconds with the movement onset cue at 0.0 seconds}
%     \label{fig:LSTM all part results}  
% \end{figure}