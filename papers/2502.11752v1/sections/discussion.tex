
%% A summary of findings 
In this work, we examined how different modalities predict human motion intention in close-proximity human-robot collaborations, focusing on distinguishing human-to-robot handovers from other tasks. We compared human gaze and EEG data with hand motion trajectories, our baseline modality for detecting handovers. Our results show that hand motion trajectories can identify a handover about 1 second after motion onset. Notably, the human gaze can detect handover intention earlier than both EEG and hand motion trajectories, sometimes even before the movement begins, suggesting it could enhance conventional detection methods. In contrast, EEG performance had more variation across subjects and trials. Although EEG could predict object handover from the other two conditions, it was less accurate compared to the other two modalities. 

Comparing AUC-ROC levels in LSTM and LDA models, gaze data achieved the highest AUC-ROC levels after movement onset (Table~\ref{tab:performance_lstm_lda}), and it reached these levels faster than both hand motion and EEG, demonstrating its superior predictive power. For some participants
%, such as participant 2 
(Fig.~\ref{fig:selected_gaze}), a high AUC-ROC was observed even before movement onset, highlighting gaze data's predictive power prior to actual movement. These results align with earlier analysis describing percentages of gaze data before movement onset (\ref{tab:gaze_b}).

In terms of motion data, LDA classification performance post-onset was similar to that of EEG. However, median plots in Fig.~\ref{fig:lda-lstm} show that motion data's predictive power for handover intention is lower than that of gaze data, with no predictive power before motion onset. For participant 2 (Fig.~\ref{fig:selected_motion}), there was a steep rise in AUC-ROC just after motion onset, surpassing gaze in predictive power, a result not seen in other participants.

Our initial analysis of averaged EEG trials indicated the feasibility of detecting handover intention before movement onset. However, single-trial EEG signals could not robustly predict handover intention before movement began. While EEG classification results showed some predictive power for certain participants, this varied across individuals. Fig.~\ref{fig:selected_eeg} shows LDA EEG classification results for selected participants before movement onset, highlighting some predictive power (up to 70\% AUC-ROC) for handover detection, albeit at varying times. The low signal-to-noise ratio of EEG signals likely contributed to poor performance. Our classification results suggest a need for more EEG trials to help models identify relevant patterns despite signal artifacts.

We also explored whether combining different modalities could improve detection performance. Specifically, we investigated if different modalities provide complementary information about the handover task, enhancing the model's certainty regarding the type of upcoming motion. Our findings indicate that gaze data mainly drives the model's performance. However, the early fusion approach using all three modalities (Fig.~\ref{fig:multimodal_all}) outperformed any single modality right before movement onset. The EEG-gaze model (Fig.~\ref{fig:multimodal_eeg-gaze}) also showed a performance boost right before movement onset, though less pronounced, suggesting that EEG might add valuable information when combined with gaze data. Conversely, combining EEG with motion data worsened performance compared to using motion data alone, indicating that EEG may introduce more uncertainty into the decision-making process.

%% Limitations 
Some limitations in this study should be considered when interpreting the results. First, we only considered a controlled and limited set of human-robot collaboration scenarios, with identical initial actions across conditions, differing only by human intent. This design aimed to create a challenging scenario for detecting human intention, which could lead to robot failure. However, real-world applications often involve diverse task variations that might provide additional cues for the robot to understand human intention earlier and more accurately. Furthermore, our experiment included only reactive tasks, where human actions responded to an external cue, differing from real-world scenarios where humans decide the motion type and timing, potentially altering brain responses.

Another limitation is measuring the exact time participants started the movement due to the reactive nature of the design. Although we used the green \emph{Go!} signal as the movement onset cue, participants often began moving slightly later due to natural reaction times, sometimes up to one second after the signal. Thus, the \emph{Go!} signal marks the earliest possible start time rather than the actual movement start. This does not affect our claims, as our main goal is to compare the timing of different modalities relative to each other.

The number of trials per participant is also a limiting factor in many experiments involving human subjects, as human fatigue requires keeping experimental sessions short. This is particularly problematic in repetitive tasks and when using devices like EEG that can become uncomfortable over time. Nevertheless, both the number of participants and the number of trials per participant are in line with the literature~\cite{cooper20, BUERKLE2021}. 

In this study, we collected simultaneous EEG, gaze, and hand motion trajectory data. Among these modalities, hand motion trajectory is the most convenient for the user, requiring only an RGBD camera installed on the robot and posing no extra burden on the user. Gaze data was collected using specialized wearable glasses for a more accurate tracking, although it can also be tracked using an RGB camera~\cite{Open_gaze}, similar to motion tracking. On the other hand, EEG data collection with currently available devices is the most challenging. We used wet EEG electrodes that require a conductive gel to be injected under the electrode on the scalp surface. While dry electrodes are available, they are more susceptible to noise. Additionally, recording EEG requires participants to minimize head movement to prevent muscle and electrode displacement artifacts, introducing inconvenience and reducing the likelihood of its widespread adoption in real-life scenarios. However, recent advances in recording hardware and signal processing methods may help mitigate these challenges.

Since EEG signals are recorded from the scalp's surface, they are highly susceptible to various noises and artifacts, such as line noise, muscle activity, blinking, and electrode displacement. Consequently, extensive preprocessing is often necessary to enhance the signal-to-noise ratio. However, certain preprocessing steps, like frequency filtering, can introduce temporal delays to the signal, which can affect the timing of movement intention detection.

It is to be noted that gaze and motion tracking too come with some challenges. Both these modalities require image processing in their preprocessing steps and thus suffer from the loss of tracking accuracy and tracking data due to several factors such as occlusion, lighting variations, and sensor noise.
In our analysis, we also suffered from the loss of samples in the data stream from gaze and motion.
The complexity of accurately interpreting visual data can lead to tracking errors, especially in dynamic or cluttered environments. Furthermore, the computational demands of real-time image processing may strain system resources, potentially resulting in delays or decreased performance.

The findings indicate that for robot controllers, the gaze data of humans holds the most potential for real-time prediction of human intention for handovers. 
Its high predictive power to determine if human motion is intended for handover just after the start of such motion is critical for creating a responsive system. It also shows some such predictive power just before the human motion starts and could be used to prepare the robotic system for a suitable response.
While hand motion tracking is convenient and widely used for recognizing human intention in human-robot handovers, its delayed prediction limits its usefulness in such dynamic interactions requiring precise timing. These delays could potentially cause a delayed response from the robot to the human motion, leading to an unnatural and awkward interaction experience. Despite its potential, as seen in the EEG data analysis, the EEG data is noisy and variable, making it unreliable for immediate practical application for such a prediction of human intention. 
% CHECK This:
For a strong controller, a combination of gaze data and refined hand motion analysis, supplemented by advanced signal processing techniques to address EEG data challenges, could improve intention prediction and enable timely, efficient robot responses in real-time scenarios.

Future work will involve integrating the assessed modalities into an online closed-loop system, where a robot controller predicts human intentions based on real-time sensory data and responds accordingly. Building such an online system requires real-time data processing, presenting new challenges. Additionally, the model's uncertainty must be considered when inferring human intentions.

For EEG signals, collecting more data can help models learn better representations. Given the high dimensionality of EEG data, we either need extensive feature extraction to reduce dimensionality and eliminate irrelevant components, or we require a large number of samples to allow more complex models, such as deep neural networks, to effectively handle the data.

Finally, we want to highlight the potential of foundation models and their high performance across various downstream tasks. These models have shown promising results in many human-robot interaction scenarios~\cite{min2022rethinking,mahadevan2024generative,lin2023gesture}. However, training and fine-tuning them usually require a large amount of data. Additionally, these models are so large that even inference demands significant memory resources. Current foundation models are well-developed for vision and language inputs, but incorporating other modalities such as gaze and EEG will require adapting the existing models to these modalities, necessitating further training and data collection. Future work would also evaluate the feasibility of using foundation models in the context of human-robot collaboration and specifically handovers.




