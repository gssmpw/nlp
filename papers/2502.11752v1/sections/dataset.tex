\subsection*{Participants}
A total of 15 individuals (age = 26.67 \(\pm\) 4.25, 11 males, 4 females) participated in the experiment. Six participants had prior experience with robots, mostly from participation in other studies, and all of them were right-handed. All participants were provided with a detailed explanation of the experiment and informed consent was obtained from them before the experiment. The study was approved by the Swedish Ethical Review Authority, Etikprövningsmyndigheten (DNR:2022-01555-01).
Two participants (Subject ID = 7 and 13, both male) were excluded from the EEG analysis due to technical issues with the EEG recording device during the experiment. Two participants (Subject ID = 13 and 14) also had technical issues with hand-tracking; hence, they were excluded from the motion analysis. Thus, participant 13 was excluded from the overall analysis. For all other participants, we include the gaze analysis, although, for a few participants, less trials than the total number of trials (i.e., 90) were available. Please refer to Supplementary Table S3 for the total number of trials per participant per modality.

\subsection*{Data Acquisition and Preprocessing}

We simultaneously recorded EEG, gaze, and hand motion trajectory data while participants performed the experiment tasks. Fig.~\ref{fig:experimental setup} illustrates the experimental setup, including the EEG recording cap, eye-tracking glasses, and the RGBD camera. 

\subsubsection*{EEG Data}

EEG signals were recorded using 32 Ag/AgCl BrainProducts active electrodes~\cite{brainproductsBrainProducts}, positioned at equidistant locations according to the 10–20 system. All EEG preprocessing was performed in Python (v3.9) using the MNE-Python package~\cite{gramfort2013meg}. To eliminate artifacts from muscle activity and blinking, the signal was first band-pass filtered between 1 and 100 Hz and then processed with the MNE-ICALabel package to identify non-brain components using independent component analysis (ICA)~\cite{makeig1995independent,iriarte2003independent}. After removing these components, the signal was low-pass filtered at 40 Hz. Depending on the required data segment, the data was epoched from 5 seconds before to a maximum of 6 seconds after movement onset. Finally, the data was downsampled from 1000 Hz by a factor of 4 to reduce noise as well as computational costs. 

We used only 12 out of the 32 channels recorded by the EEG device, focusing on central and frontal channels, which contain more information related to motor planning and execution~\cite{hanakawa2003functional,chaisaen2020decoding,yu2022effects}. This channel selection reduces data dimensionality and filters out irrelevant brain activity. Specifically, we used the Cz, C3, C4, FC1, FC2, FC5, FC6, CP1, CP2, F3, F4, and Fz channels, as shown in Supplementary Fig. 2.

\subsubsection*{Gaze Data}
Gaze data was acquired using Tobii Glasses 2 \cite{Tobiiglasses:2024:Online} worn by the participants. These glasses provide recorded videos and gaze data. We utilized GlassesViewer \cite{glassviewer}, an open-source software, to extract, filter, and parse the gaze data from the Tobii Glasses 2 recordings. The gaze data is a stream of $(X,Y)$ values (denoted as $GazeX$ and $GazeY$) referenced to the corner $(0,0)$ of the recorded video frame, which has a size of $(1920,1080)$. Linear interpolation was used to handle missing gaze samples in the data stream, a common technique used in similar studies to fill in gaps caused by blinks, head movements, or other sources of noise \cite{glassviewer}. For all participants, the gaze data was referenced to a specific point on the robot's torso (Supplementary Fig. 3). 
This reference point was tracked in the recorded video frames using normalized cross-correlation in MATLAB. The sampling rate for the recorded video is 25 Hz and for each video frame, we track this reference point.
Due to this processing, the final sampling frequency for the gaze data was 25 Hz.

\subsubsection*{Hand Motion Trajectory Data}
The human motion was tracked using the Microsoft Azure Kinect RGBD camera \cite{Azure_Kinect} and a ROS-based implementation of the Azure Kinect Body Tracking SDK. The output of this body-tracking SDK is the tracked human skeleton referenced by the Kinect camera. Since all participants were right-handed, we extracted the right-hand tracking data from the recorded tracking skeleton data for the participants, in particular the X, Y, and Z coordinates. Given the output frequency of 5 Hz for this tracking SDK, the hand-motion data had a final sampling frequency of 5 Hz.

\subsection*{Dataset}

We structure the data from different modalities as time series data to detect whether the upcoming or ongoing motion is a handover or not. This sequential arrangement allows a classification model to capture the temporal dependencies necessary for accurate classification. A time series \(\mathbf{X_T}\) is represented as:

\begin{equation}
    \mathbf{X_T} = \{x_1, x_2, x_3, \ldots, x_T\} ,
\end{equation}

\noindent where \(x_t\) represents the observations at time \(t\), and \(T\) is the total number of time samples. Maintaining the order of these observations is crucial for the model to learn patterns and predict class labels accurately. In our dataset, depending on the modality, \(x_t\) is defined as:

\begin{equation}
    x_t = 
    \begin{cases} 
    \text{Gaze}: [\text{GazeX}_t, \text{GazeY}_t] \\
    \text{Motion}: [\text{HandX}_t, \text{HandY}_t, \text{HandZ}_t] \\
    \text{EEG}: [\text{f}_1, \text{f}_2, ..., \text{f}_F]
    \end{cases},
\end{equation}

\noindent where \(\text{GazeX}_t\) and \(\text{GazeY}_t\) are the 2D gaze coordinates, and \(\text{HandX}_t\), \(\text{HandY}_t\), and \(\text{HandZ}_t\) are the 3D hand coordinates. For EEG signals, we used the time-frequency transformation of the data, with \(\text{f}_1\) to \(\text{f}_F\) indicating the power of frequency components at time \(t\). The use of time-frequency features was based on previous studies that show their effectiveness in predicting motor-related EEG activity~\cite{alazrai2017eeg, xu2018wavelet, MAMMONE2020source}. For more details on the data features, please refer to the \emph{Feature Extraction} in the \emph{Data Analysis} section.
 % All codes for data processing and model training presented in this study are published in the following URL:\\ {\small \url{https://github.com/NonaRjb/Human-to-Robot-Handover-Detection.git}}.
