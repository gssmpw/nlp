\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/experimental_setup.pdf}
    \caption{\justifying Experimental Setup: The participant sits across a table from the Baxter robot, wearing the EEG cap and Tobii eye tracker glasses. An Azure Kinect RGBD camera is installed on the robot's torso.}
    \label{fig:experimental setup}
\end{figure}

We designed a within-subject experiment involving human participants interacting with a Baxter robot~\cite{baxter-Doe:2024:Online}. Each participant sat across from the robot, wearing an EEG cap and Tobii Pro Glasses 2 eye-tracker glasses. An RGBD camera, Microsoft Azure,  mounted on the robot's torso, tracked the participant's hand motion trajectory. The experimental setup is depicted in Fig.~\ref{fig:experimental setup}. From the participant's point of view, there are three marked positions on the table: A, B, and C in a straight line. In each trial, a green sponge is moved from its initial position A, near the participant, to the final position B, near the robot. The movement is executed through one of three types of human actions (also visualized in Supplementary Fig. 1):
\begin{enumerate}[label=(\alph*)]
    \item \textbf{Solo Action:} This action is performed solely by the participant. \textit{The participant picks the object from A} and places it at B.
    \item \textbf{Handover - Human Giver:} This action involves a human-to-robot handover. \textit{The participant picks the object from A} and hands it to the robot over C. The robot then places the object at B. 
    \item \textbf{Joint Action:} This action is a non-handover collaborative action between the participant and the robot. \textit{The participant picks the object from A} and places it at C. Then, the robot picks the object from C and places it at B. 
\end{enumerate}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/gui_instructions.pdf}
    \caption{Experimental conditions and timing diagram, Adapted from Fig. 2 in \cite{nonaEEG}. Visual instructions for the three task conditions: \textbf{(a)} solo, \textbf{(b)} handover, and \textbf{(c)} joint actions. \textbf{(d)} Timing diagram of the experiment with $t=0$ at the \emph{Go!} signal.}
    \label{fig:gui-ins}
\end{figure}

\noindent The initial actions, shown in italics, are identical across all three tasks. Further, in both the \emph{Handover} and \emph{Joint} actions, the robot arm was controlled by the experimenter, though participants were unaware of this. 

In our design, joint action specifically refers to a collaborative action with the robot that is not a direct handover.
Each trial corresponds to one of three actions conveyed to the participant via instructions on a screen placed on the table (Fig.~\ref{fig:experimental setup}). There were 90 trials in total, with an equal number, i.e. 30, of each task condition performed in a random order. Fig.~\ref{fig:gui-ins} illustrates the timeline of instructions for each trial. The first image is a cue specifying the action type, displayed for 5 seconds with a countdown from 5 to 1 second. The next image shows a green cross and the word "Go!", signaling the participant to begin the action. For solo actions, this image is displayed for 8 seconds, while for handover and joint actions, it is displayed for 16 seconds to allow sufficient time for task completion based on pilot trials. The final image is a red cross with the word "Rest!", indicating rest time. During this period, the participant returns the object from B to A and presses the space bar on a keyboard to mark the end of the trial and the start of the next one.