% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=\linewidth]{figures/experimental_setup.pdf}
%     \caption{\justifying blah blah blah}
%     \label{fig:experimental setup}
% \end{figure}



\subsection*{Model Selection}

The main goal of this study is to discriminate handover intention from other types of movements. Therefore, we combined data from the \emph{Solo} and \emph{Joint} conditions as class 0 and used data from \emph{Handover} trials as class 1. 

We compared the performance of two example classifiers: (1) linear classification using linear discriminant analysis (LDA), and (2) non-linear classification using a long-short-term memory (LSTM) neural network. LDA works by modeling the data distribution as a multivariate Gaussian distribution. It then uses the Bayes theorem to classify new data points. 

We used the Scikit-Learn package~\cite{scikit-learn} in Python to implement the linear classification pipeline. LDA was applied using Scikit-Learn’s default parameters. To evaluate LDA’s performance, we used repeated stratified 10-fold cross-validation with three repeats. This involved splitting the data into 10 folds, using 9 folds for training and 1 fold for testing, and repeating the procedure so each fold served as the test set. This entire process was repeated three times with different randomizations.

For the neural network classifier, we used the PyTorch package~\cite{paszke2019pytorch} in Python to implement data loading, training, and the LSTM model. Different LSTM parameters were used for EEG data compared to gaze and hand motion trajectory data due to EEG's higher dimensionality. Specifically, for EEG, we used a 1-layer LSTM with 128 hidden units. The LSTM's output for the last time step was passed through a ReLU activation function and then mapped to class probabilities using a fully connected layer followed by a Sigmoid function. We used a batch size of 16 and trained the model for 100 epochs, selecting the model with the minimum validation loss for evaluation on the test split. Given the low number of data samples, we aimed to keep the hidden layers of LSTMs to a minimal number and correspondingly selected batch size and other parameters. For the gaze and the motion data, we used a 2-layer LSTM with 10 hidden units while using a batch size of 5 and training for 200 epochs with early stopping based on validation loss after 100 epochs. Therefore, we selected the model with the minimum validation loss for further evaluation on the test split.
%The early stopping was based on validation loss with a patience level of 10 epochs, thus selecting the best model with 
%\todo[inline]{@Parag, please add the way you used LSTM} 

To select the best neural network model based on validation loss, we included both a validation set for model selection and a test set for final evaluation. We employed a nested cross-validation approach to achieve this. Specifically, we used repeated stratified 10-fold cross-validation with three repeats to choose test splits. For each of the nine training folds, we performed an additional 10-fold cross-validation to train the model on various train-validation splits. Finally, a weighted ensemble of these 10 models was used to evaluate the LSTM's performance on the test split.

\subsection*{Performance Evaluation}

We measured the area under the receiver operating characteristic curve (AUC-ROC) to evaluate the classification performance for the classifier models. The ROC curve is obtained by using different threshold values on the model's predictions and computing the true positive rate (TPR) and false positive rate (FPR) given by equations~\ref{eq:tpr} and~\ref{eq:fpr} respectively. 

\begin{equation}
    \label{eq:tpr}
    TPR = \frac{TP}{TP + FN} ,
\end{equation}

\begin{equation}
    \label{eq:fpr}
    FPR = \frac{FP}{FP + TN} ,
\end{equation}

TP, TN, FP, and FN stand for true positive, true negative, false positive, and false negative samples. Plotting TPR against FPR creates the ROC curve. AUC-ROC is by definition the area under the ROC curve. An AUC-ROC greater than 0.5 indicates that the classifier is performing better than a random classifier. An AUC-ROC of 1.0 shows a perfect classification discriminating all positive samples from the negative ones. 
% an auc of 1 depicts perfect classifier that dsicrimantes positive samples form negative ones, 
% mention of med papers, with the tasks.

\subsection*{Feature Extraction}

\subsubsection*{EEG Data}

We applied time-frequency transformation to the EEG data using Morlet wavelets with three cycles to generate input for both classifiers. This transformation covered a frequency range of 5–40 Hz within the selected time window, encompassing the alpha (8–12 Hz), beta (13–30 Hz), and part of the gamma (\(>\text{30 Hz}\)) bands, which are known to encode motor task information in EEG signals~\cite{amo2020induced, yu2022effects}. Frequencies above 40 Hz were excluded due to susceptibility to high-frequency line noise and muscle artifacts. The time-frequency representations were then averaged over channels and reshaped into 1-D vectors suitable for the linear classifier. Before applying LDA, we standardized the data to zero mean and unit standard deviation and used principal component analysis to extract components that capture 99\% of the data variance. Applying PCA before LDA further reduces the dimensionality of the data and helps to regularize the classification problem. We also standardized the EEG data for LSTM training, although no further dimensionality reduction (e.g. by PCA) was applied.  
%. was applied as neural networks can inherently handle high-dimensional inputs.
 
\subsubsection*{Gaze Data}
The features for the gaze data were the X and Y coordinates of the recorded gaze data for the participants, referenced to a fixed point on the robot's torso (Supplementary Fig. 3). These features are denoted by: $GazeX$ and $GazeY$. 

\subsubsection*{Hand Motion Trajectory Data}

The features for the motion data were the X, Y, and Z coordinates of the tracked right hand, denoted by: $HandX$, $HandY$, $HandZ$. These coordinates are referenced to the Azure Kinect camera, which was mounted on the torso of the robot (Fig.~\ref{fig:experimental setup}).

