\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage[rebuttal]{cvpr}

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{comment}

% \newcommand{\com}[3]{{\colorbox{#2}{\bfseries\sffamily\scriptsize\textcolor{white}{#1}}}
\newcommand{\rone}[0]{{\color{blue}\textbf{R1}}{}}
\newcommand{\rtwo}[0]{{\color{red}\textbf{R2}}{}}
\newcommand{\rthree}[0]{{\color{green}\textbf{R3}}{}}
% \newcommand{\reviewr}[1]{{\color{green}\textbf{R#1}}}

% Import additional packages in the preamble file, before hyperref
\input{preamble}

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\definecolor{cvprblue}{rgb}{0.21,0.49,0.74}
\usepackage[pagebackref,breaklinks,colorlinks,allcolors=cvprblue]{hyperref}

% If you wish to avoid re-using figure, table, and equation numbers from
% the main paper, please uncomment the following and change the numbers
% appropriately.
%\setcounter{figure}{2}
%\setcounter{table}{1}
%\setcounter{equation}{2}

% If you wish to avoid re-using reference numbers from the main paper,
% please uncomment the following and change the counter value to the
% number of references you have in the main paper (here, 100).
%\makeatletter
%\apptocmd{\thebibliography}{\global\c@NAT@ctr 100\relax}{}{}
%\makeatother

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\paperID{15691} % *** Enter the Paper ID here
\def\confName{CVPR}
\def\confYear{2023}

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Data Distributional Properties As Inductive Bias for Systematic Generalization}  % **** Enter the paper title here

\maketitle
\thispagestyle{empty}
\appendix

%%%%%%%%% BODY TEXT - ENTER YOUR RESPONSE BELOW
We sincerely thank the reviewers for their insightful feedback, relevant literature suggestions (\rone, \rtwo, \rthree), and proposed future research directions (\rthree). We appreciate that they found our work well-motivated (\rone, \rtwo) and recognized the importance of studying training data properties for systematic generalization (SG) (\rtwo). We are also encouraged that our work was clearly understood (\rtwo, \rthree) and that our conclusions, particularly those related to how data properties influence representational geometry (Section 8), were found interesting (\rtwo, \rthree). Next we address their main concerns.

\begin{comment}
We thank the reviewers for their insightful responses, providing additional literature (\rone, \rtwo, and \rthree) and interesting novel directions to explore (\rthree). We are encouraged that they find our work is well motivated (\rone, \rtwo), as addressing the properties of the data in effect in compositional generalization is a crucial research question (\rtwo); it was clearly understood (\rtwo, \rthree); 
% the experiments were perceived as thorough (\rone, \rthree); 
and interesting conclusions (\rtwo, \rthree) were drawn: such as how the properties of the data impact the representational geometry in Section 8 (\rtwo, \rthree). Now we address their main concerns.
\end{comment}


\section{Novelty and Contributions}

% (24,25,28,29,31,36,37)

% (24/28)
\textbf{We provide a study of SG in a multimodal, multitask setting where previous work has focused on simpler symbolic, textual [65], and few-shot classification [10] tasks.}
%Through thorough experimentation we provide a study factors (though non novel...) that affect generalization in models in a novel multi-domain, multi-task setting, while previous work had focused on simpler symbolic \cite{zhou2023data-factors} and associative \cite{chan2022data-properties-drives-in-context-learning} tasks.
Previous studies of data properties affecting SG focus only on symbolic data, overlooking other crucial modalities such as vision.  To address this gap, we leverage the CLEVR dataset, which integrates both continuous (visual) and discrete (textual) domains, incorporating visual phenomena such as occlusions, shadows, and material reflections. Unlike prior work, our study employs high-dimensional, photorealistic data, moving toward a more natural and general testing scenario. Furthermore, our multitask setting extends previous research by examining cross-task effects of data diversity (Figure 4), a factor that prior studies have overlooked by focusing solely on same-task effects (\rthree).

\begin{comment}
To address this, we use CLEVR, which combines the visual domain's continual nature with the text's symbolic nature. While we generate our data from discrete latent factors, the generated inputs do live in a far more photo-realistic high-dimensional, continual domain which is a step towards more natural datasets. Moreover, our multitask setting is relevant because we present results, such as those shown in Figure 4, where we analyze the cross-task effects of altering diversity, as was pointed out by \rthree, where previous studies have only focused on same-task effects.
\end{comment}
%(25) We focus on fundamental principles, and that future work will focus on more real-life scenarios. However, this would entail having a clear way to control experimental designs with realistic stimuli in continuous spaces. This would however require the generation of a novel dataset.


\begin{comment}
\noindent \textbf{We obtain a novel result regarding \textit{diversity}: even when controlling for known factors that induce better SG, having more diversity further increases SG, suggesting an unknown mechanism for its effect.} 

Figure 8 shows that increasing diversity for similar levels of P-Score and NMI, increases SG. To the best of our knowledge, this is a novel result that shows that none of these known metrics can fully explain the effects of diversity on SG. We believe this result points to more studies being necessary to understanding why diversity is as effective as it is as a property of the data. We also provide formal definitions for all of our studied properties, which is a significant step in developing theory, whereas previous works have relied on intuition rather than definitions.
\end{comment}

\noindent \textbf{We obtain a novel result regarding \textit{diversity}: even when controlling for known factors that induce better SG, having more diversity further increases SG, suggesting an unknown mechanism for its effect.} 
Previous work has shown that there is a relation between Normalized Mutual Information (NMI) and data diversity. Consequently and as expected, as shown in Figure 8, for lower NMI there is greater OOD performance. However, for similar levels of NMI but differing levels of diversity, we observe that greater diversity increases performance. This suggests that the effect of diversity is not solely explained by its impact on NMI. Identifying a possible explanation for this effect leads to our final contribution: the relationship between diversity and the geometry of learned representations.

\noindent \textbf{To the best of our knowledge, this is the first study to establish a relationship between diversity, the geometry of learned representations, and SG performance}.

We believe \rone~ and \rtwo~ may have overlooked our study of the P-Score metric [29] and Disentanglement [19] as it relates to diversity, something that has not been done before. As we describe in sections 8.1 and 8.2, we relate the P-Score to diversity and the potential of parallelism in representations for enhancing generalization. Our results (Figure 11b) indicate that there is a relationship between NMI and P-Score, where lower NMI datasets are related to higher P-Scores in representations. This suggests that lower NMI datasets tend to produce models with representations with higher P-Scores. This suggests a novel mechanism by which lower NMI datasets produce inductive biases during learning that allow models to achieve greater out-of-distribution generalization. This result provides initial evidence that the dataset distribution influences the representational space geometry of the model biasing it towards out-of-distributional generalization. We believe that this is a relevant new insight that can motivate further research.

\begin{comment}
\noindent \textbf{To the best of our knowledge, this is the first study to establish a relationship between diversity, the geometry of learned representations, and SG performance}.
%We further establish relationships between data properties and the geometry of the representations learned by neural networks via the disentanglement \cite{} and P-scores \cite{ito2022compositional-smaller} metric, as well as the relationship with performance on systematic generalization (SG). 
We believe \rone~ and \rtwo~ may have not considered our study of the P-Score metric \cite{ito2022compositional-smaller} and Disentanglement \cite{elhage2022toy-smaller} as it relates to diversity, something that has not been done before.
NMI and Disentanglement have been studied together, but not in relation to diversity. 
The study of the P-Score is motivated by a series of ideas: it has previously been studied, without accounting for data diversity, that pre-training of neural networks induces increased P-Score in representation space, as well as human-like compositional generalization \cite{ito2022compositional-smaller}; The \textit{linear representation hypothesis} \cite{zhang2024feature-smaller} (as mentioned by \rthree). We believe the latter concept to be highly relevant, and will include it in our discussion of results.% establishes that concepts can be represented by directions in representation space, which could promote out-of-distribution generalization, suggesting a theoretically motivated reason to study how diversity affects this geometry.
\end{comment}
%As referred by \rone and \rtwo, indeed these data distribution properties have been studied, but this is the first study to provide a deeper understanding in terms of the geometry of neural representations.
%We believe the study of the effect of diversity on the geometry of representations learned to be non trivial.
%To study the geometry of the representation space learned we study both P-Score {\color{red} we missed the citation in the paper when describing the metric \cite{ito2022parallelism-score-smaller}} and Disentanglement metric.
%It has previously been studied that pre-training of neural networks induces abstraction (increased P-Score in the representation space), as well as human-like compositional generalization \cite{ito2022compositional-smaller}.
%In this study, we skip the pre-training phase and establish the relationship between the properties of training data and increased abstraction (lower NMI, Fig. 11b) and SG (Figs. 4, 5, 6, 8).


% (24,29,36,37) 


%\textbf{Multimodal/Multitask}

%(25) To the best of our knowledge, we are the first to study the influence of data in SG in a multi-modal and multi-task setting. By leveraging the visual domain in the multi-modal setting, latent factors, even though discrete in nature are non trivially transformed by the rendering function to create the pixels in the image. This is in opposition to previous work where most the experimental latent factors explicitly part of the input (increased primitives in \cite{zhou2023data-factors-smaller}) and simple symbolic rules (removed conjunction constraint in \cite{zhou2023data-factors-smaller}). Due to material, light effect, and occlusion; increasing the cardinality of latent factors, has a non-trivial effect in the image, that is not explicit in the input, thus, we are also testing if the model is affected by this subtler change.
%This directly contradicts the claim from \rone that we only focus on a discrete setting. This leaves out of consideration the information the network is receiving to solve the task. 
%Using a multi-modal setting allows use CLEVR's hyper realistic rendering setting to this end. 
% During the data-generating process, the latent values are transformed into continuous pixel values mediated by the rendering function, this is novel too, since all the previous work have focus on discrete tokens, moreover, these tokens, most of the time, have a direct relation to the latent value represented and tested in the diversity experiments. For example, in \cite{zhou2023data-factors-smaller}, increasing the number of primitives has the direct effect of increasing the number of tokens directly increasing the number of embedding thus the model could be taking advantage of this issue, as well in the output with the increase the possible actions. 


% {\color{red}This allows us to explore the effect it has on each different modality. We have results showing that the effect of increasing diversity in text, using the AugZero augmentation \cite{zhou2023data-factors-smaller},  doesn't increment the performance as diversity in images does.}

% We explore the relationship the P-Score metric,{\color{red} we missed the citation in the paper when describing the metric \cite{ito2022parallelism-score-smaller}}, to SG in a non-toy setting and establish that increased P-scores are related to increased SG.



%With this, what are the implications:

%We are happy to re-phrase and re-discuss our contributions so they are clearer to future readers in the final text.



% We disagree since we are talking about improving SG performance. To the best of our knowledge, burstiness was shown to improve few-shot learning \cite{}. Diversity on the other hand we agree and show the relevant literature showing this in the paper. But we did test it on a novel domain, as well as relate to the geometry of the representation under different data diversity influence.


\section{Experimental Set-Up}

%33,34,35

\begin{itemize}
\item \rtwo~ \textbf{\textit{``The experiments do not report the std. error of..."}}%the results..."}}%, and it is unclear whether the reported outcomes are from a single experiment or represent the average over multiple runs."}}

As stated in section 3.4, all experiments are run with 3 seeds. All figures show the mean and standard error. However, the latter is extremely small, as results are quite stable across seeds.

\item \rtwo~ \textbf{\textit{``Some experimental details are missing..."}} \textbf{\textit{``It is uncertain whether the findings would remain consistent..."}}

We have now added all experimental hyperparameters details in a section within the appendix; additionally, we will release our code upon publication for replicability. All of our experiments are run for 1000 epochs, to allow all experiments to train until convergence. 
In Section 6, we do test for models with differing sizes to test the hypothesis of whether diversity induced less memorization as it strains model capacity. We find this is not the case. Due to computational constraints, however, we were unable to test for more configurations, as we tested the most likely to disprove the hypothesis.
%\item %, such as the number of training epochs or model size."}}

% thus testing the consistency of our results, we saw they were very consistent on all settings except when the model size decreases considerably. %For epochs, we saw training was stable after a minimum number of epochs and standardized to 1000 for all experiments, this was for enough epochs for all models to converge it's training.

\end{itemize}


% \section{Answer to Reviewer 1 \label{sec:answer-to-r1}}

% \begin{itemize}
% \item \textbf{\textit{The novelty of the paper is limited: all three studied data distribution properties have been proposed in previous work[10,53,54], and the fact that data diversity improves SG is well known. Besides experimental support of the claims, theoretical analysis would make the paper stronger.}}

% We show data properties in new contexts i.e. burstiness was shown to foster few-shot learning; as well as showing that this properties foster SG in a multimodal domain.
% Disagree that diversity foster SG (??).

% \textbf{\textit{The analysis focus on discrete data attributes and VQA tasks, it is not clear how it benefits SG in other tasks and continual data. The experiments are also limited to VQA tasks and synthetic dataset. It is not clear whether the analysis would scale to more realistic tasks.}}

% We focus on fundamental principles, and that future work will focus on more real-life scenarios. However, this would entail having a clear way to control experimental designs with realistic stimuli in continuous spaces. This would however require the generation of a novel dataset.


% \item \textbf{\textit{Whether there is a connection between disentanglement and SG, there are papers on both sides: the authors only cited the positive ones. Need to cite and discuss the negative ones:...}}

\section{Other}
%(29,31) 
% 26,30

\begin{itemize}
    \item \rtwo~ \textbf{\textit{``The conclusion that diversity and burstiness improve performance is not particularly surprising."}}
    To the best of our knowledge, burstiness has been shown to improve few-shot learning [10] not SG. 
%We thank \rone~ for pointing out missing literature related to disentanglement to review in the context of our work and we will add the discussion to our work. Specifically, we would alter the first paragraph in section 8.1 with: "It has been hypothesized that a desirable property for SG are disentangled representations [17, 27, 28], although not all evidence point in this way [a,b,c], we study how these properties affect the level of disentanglement of the learned representations, and the dergee that this affect SG in the end"

\item\rtwo~ \textbf{\textit{``The concept of latent invention remains unclear, and it is difficult to distinguish it from diversity."}} The key difference is that latent intervention is data augmentation targeted to disrupt the link between latent variables, while diversity does not.

\item \rone~ \textbf{\textit{``Experiments are conducted only on transformer"}}. Given that previous works, ex. [10], have shown that MLPs and RNNs do not exhibit similar capabilities, or perform as well, as transformers in demonstrating properties like systematicity and reasoning, we focus our study on transformers.

\end{itemize}




%Diversity on the other hand we agree and show the relevant literature showing this in the paper. But we did test it on a novel domain, as well as relate to the geometry of the representation under different data diversity influence.


%\section{Conclusion}

% \section{Introduction}

% After receiving paper reviews, authors may optionally submit a rebuttal to address the reviewers' comments, which will be limited to a {\bf one page} PDF file.
% Please follow the steps and style guidelines outlined below for submitting your author response.

% The author rebuttal is optional and, following similar guidelines to previous conferences, is meant to provide you with an opportunity to rebut factual errors or to supply additional information requested by the reviewers.
% It is NOT intended to add new contributions (theorems, algorithms, experiments) that were absent in the original submission and NOT specifically requested by the reviewers.
% You may optionally add a figure, graph, or proof to your rebuttal to better illustrate your answer to the reviewers' comments.

% Per a passed 2018 PAMI-TC motion, reviewers should refrain from requesting significant additional experiments for the rebuttal or penalize for lack of additional experiments.
% Authors should refrain from including new experimental results in the rebuttal, especially when not specifically requested to do so by the reviewers.
% Authors may include figures with illustrations or comparison tables of results reported in the submission/supplemental material or in other papers.

% Just like the original submission, the rebuttal must maintain anonymity and cannot include external links that reveal the author identity or circumvent the length restriction.
% The rebuttal must comply with this template (the use of sections is not required, though it is recommended to structure the rebuttal for ease of reading).

% %-------------------------------------------------------------------------

% \subsection{Response length}
% Author responses must be no longer than 1 page in length including any references and figures.
% Overlength responses will simply not be reviewed.
% This includes responses where the margins and formatting are deemed to have been significantly altered from those laid down by this style guide.
% Note that this \LaTeX\ guide already sets figure captions and references in a smaller font.

% %------------------------------------------------------------------------
% \section{Formatting your Response}

% {\bf Make sure to update the paper title and paper ID in the appropriate place in the tex file.}

% All text must be in a two-column format.
% The total allowable size of the text area is $6\frac78$ inches (17.46 cm) wide by $8\frac78$ inches (22.54 cm) high.
% Columns are to be $3\frac14$ inches (8.25 cm) wide, with a $\frac{5}{16}$ inch (0.8 cm) space between them.
% The top margin should begin 1 inch (2.54 cm) from the top edge of the page.
% The bottom margin should be $1\frac{1}{8}$ inches (2.86 cm) from the bottom edge of the page for $8.5 \times 11$-inch paper;
% for A4 paper, approximately $1\frac{5}{8}$ inches (4.13 cm) from the bottom edge of the page.

% Please number any displayed equations.
% It is important for readers to be able to refer to any particular equation.

% Wherever Times is specified, Times Roman may also be used.
% Main text should be in 10-point Times, single-spaced.
% Section headings should be in 10 or 12 point Times.
% All paragraphs should be indented 1 pica (approx.~$\frac{1}{6}$ inch or 0.422 cm).
% Figure and table captions should be 9-point Roman type as in \cref{fig:onecol}.


% List and number all bibliographical references in 9-point Times, single-spaced,
% at the end of your response.
% When referenced in the text, enclose the citation number in square brackets, for example~\cite{Alpher05}.
% Where appropriate, include the name(s) of editors of referenced books.

% \begin{figure}[t]
%   \centering
%   \fbox{\rule{0pt}{0.5in} \rule{0.9\linewidth}{0pt}}
%   %\includegraphics[width=0.8\linewidth]{egfigure.eps}
%    \caption{Example of caption.  It is set in Roman so that mathematics
%    (always set in Roman: $B \sin A = A \sin B$) may be included without an
%    ugly clash.}
%    \label{fig:onecol}
% \end{figure}

% To avoid ambiguities, it is best if the numbering for equations, figures, tables, and references in the author response does not overlap with that in the main paper (the reviewer may wonder if you talk about \cref{fig:onecol} in the author response or in the paper).
% See \LaTeX\ template for a workaround.

% %-------------------------------------------------------------------------
% \subsection{Illustrations, graphs, and photographs}

% All graphics should be centered.
% Please ensure that any point you wish to make is resolvable in a printed copy of the response.
% Resize fonts in figures to match the font in the body text, and choose line widths which render effectively in print.
% Readers (and reviewers), even of an electronic copy, may choose to print your response in order to read it.
% You cannot insist that they do otherwise, and therefore must not assume that they can zoom in to see tiny details on a graphic.

% When placing figures in \LaTeX, it is almost always best to use \verb+\includegraphics+, and to specify the  figure width as a multiple of the line width as in the example below
% {\small\begin{verbatim}
%    \usepackage{graphicx} ...
%    \includegraphics[width=0.8\linewidth]
%                    {myfile.pdf}
% \end{verbatim}
% }



%%%%%%%%% REFERENCES
\begin{comment}
{
    \small
    \bibliographystyle{ieeenat_fullname}
    \bibliography{main}
}
\end{comment}
\end{document}
