\section{Introduction}
\label{sec:intro}

Humans excel at adapting quickly to novel tasks.
% For example, understanding the syntax of a given language allows one to learn more efficiently a second language that shares a similar syntax \cite{bernolet2013language}.
This feature emerges from our \textit{systematic generalization} (SG) ability, that is, the flexible recombination of previous (blocks of) knowledge in ways that allow us to behave effectively under novel task demands and, more generally, help us make sense of the world \cite{fodor1975language-of-thought,marcus2003algebraic}.

Importantly, SG depends on prior knowledge or inductive biases upon which agents can rely on \cite{lake2017building}.
Indeed, inductive biases enable the representing of knowledge in the form of discrete independent concepts, which can be leveraged for flexible and rapid adaptation to new situations.
In humans, for example, visual perception is strongly biased towards shapes which aids in recognizing previously unknown objects \cite{graham1999infants,geirhos2018shapes-vs-texture}. 
This bias helps us reason independently about color, texture, or form, aiding in recognizing unknown objects regardless of their specific combination of properties. A child who has seen many horses, for instance, is likely to infer that a zebra moves similarly to a horse despite their differing colors.

% \textbf{Machines don't generalize systematically because of the inductive biases}

In contrast to empirical observations in humans \cite{dekker2022curriculum}, SG does not emerge naturally in an i.i.d. setting in deep neural networks (DNNs) \cite{lake2017original-scan,ruis2020gscan,kim2020cogs,keysers2019cfq}. Since training data is always finite in machine learning, models need built-in assumptions to generalize to novel inputs. These assumptions, often called inductive biases \cite{goyal2022inductive}, help guide models toward solutions in situations not seen during training, and promote model weight configurations (i.e., solutions) that encourage generalization aligned with specific objectives \cite{wolpert1995no-free-lunch,baxter2000model-inductive-bias}.
% As research increasingly focuses on enabling DNNs to generalize to situations farther from the training data distribution, studying inductive biases that match human-like generalization becomes more important \cite{goyal2022inductive}.
Inductive biases can take different forms, such as constraints over loss functions \cite{kukavcka2017regularization}, architectures \cite{long2015fully}, pre-training \cite{devlin-etal-2019-bert}, learning \cite{lake2023human}, or the geometry of representations \cite{ravanbakhsh2017equivariance,satorras2021n,ito2022compositional}.
However, the role of specific training data distributional properties as inductive biases remains surprisingly understudied.

% \textbf{Use an example to introduce why it is an interesting problem}

A notable exception to this trend is the study of data complexity. Indeed, both in terms of patterns and scale, increasing training data complexity can improve SG in language models \cite{lake2019meta-seq2seq,jiang-etal-2022-mutual,patel-etal-2022-revisiting,zhou2023data-factors,abbasi2023CLIP-compositional-gen,fang2022data-CLIP}. Similarly, within the realm of vision, augmenting training data with translations has been shown to improve learned invariances \cite{bouchacourt2021grounding}. Nevertheless, several questions remain open. First, although incrementing training data complexity does promote SG, which property (or properties) of data distribution subtends this promotion remains obscure.
Second, despite the tremendous success and broad applicability of vision-language models \cite{li2020oscar,li2022blip,bao2022vlmo,li2023blip}, whether data training properties can boost SG in these models has not been explored. 
Third, a systematic analysis of the effect of training data properties on internal representations and how these can promote SG is also missing. In other words, we lack a functional explanation of how data properties may promote SG.

% Data distribution imposes an inductive bias in a model. 
% It has been shown recently that the robustness of large SOTA models comes from the data distribution it is trained on \citep{}.

%  =======================================================
%  > Describe your idea, showing how it solves the problem (general)
%  =======================================================

% \textbf{Describe your idea, showing how it solves the problem (general)}

% This work examines the effects of data distribution diversity on SG. 
% We observe that the properties of the data bias the way the model solves the problem i.e. it's inductive bias.

In this work, we focus on the following research questions: \textit{"Can SG be induced solely through changing properties of the training data? If so, how and why does this work?"}. To answer these questions, we focus on a Multi-modal Masked Language Modeling (MMLM) setting and test for three different properties of the data: \textit{diversity}, an increase in the cardinality of latent factors of the training distribution;  \textit{burstiness}, a limitation on the cardinality of latent factors in particular instances of the training data; and \textit{latent intervention}, a targeted intervention on the value of a latent factor. We construct datasets that alter these properties by manipulating their generative latent factors and test how a model trained on these datasets generalizes to instances of unseen combinations of latent factors. Our experiments demonstrate that manipulating these properties the SG capability of a model.

%In particular, we built distinct datasets by parametrically varying the diversity of a given data factor (here color), and observe the model's performance in generalizing to unseen instances. Furthermore, we evaluate what data properties, such as the mutual information within factors, affect the model's generalization abilities.
Finally, to understand why these properties induce better SG, we perform a thorough set of experiments. We study the role of the Normalized Mutual Information (NMI) between latent attributes of a dataset and find that it correlates strongly with out-of-distribution (o.o.d) generalization. We then turn to the effect of data distributional properties on learned representations.  Moreover, we analyze the representations' geometry compared to baseline models. We find that the more diverse a dataset the more it induces disentanglement in the representations. Also, we analyze other geometrical properties of representations. There is currently a conjecture —based on empirical observations of biological neural representations— that the more parallel representations remain under a particular change in a latent attribute, the better they are at SG. Through our experiments, we find that NMI tends to produce representations that show more parallelism, suggesting a mechanism by which lower NMI datasets can induce SG. Thus, our contributions can be summarized as follows:



% =============================================
% > State your contributions
% =============================================
% \textbf{State your contributions:}



\begin{itemize}
  \item We show that increasing the number of values of a given data factor, a property of data we call \textit{diversity}, promotes SG in an MMLM task. We achieve an absolute increase in accuracy of up to 89\% in a systematic test set, solely through manipulation of the training data.
  \item Analogously, we show that probabilistically restricting the number of values of latent factors in a given input, a term we call \textit{burstiness}, also increases SG, with an absolute 15\% increase over the baseline. This increase, however, comes at the cost of decreased performance related to the restricted factor.
  \item We also show that intervening some of the latent factors in the training data, without violating systematicity, significantly increases SG, up to an absolute increase of 15\%.
  \item We find that datasets with lower NMI between latent factors tend to produce models with better out-of-distribution performance. Moreover, we find that one mechanism by which they induce this behavior is by promoting the emergence of \textit{representations with more parallellism under changes in latent attributes}. 
  %\item We corroborate previous results showing that reducing the Normalized Mutual Information (NMI) between data factors increases SG \cite{abbasi2023CLIP-compositional-gen}, but we crucially show this effect is orthogonal to that of data diversity.

\end{itemize}
