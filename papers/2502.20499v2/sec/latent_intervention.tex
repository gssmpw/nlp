\subsection{Latent Intervention}
Figure \ref{fig:latent_intervention} shows results for modulating \textit{latent intervention} over \textit{color}. In our experiments, we can observe that increasing the degree of the perturbation we apply to the color of the images during training increases the model performance on the \textit{shape} task by up to 15\%, a non-trivial amount while retaining out-of-distribution performance on the \textit{color} task. For the \textit{material} and \textit{size} tasks, latent intervention adds up to 2\% in $\mathcal{D}_{test-OOD}$ performance, and the effect is reduced as the number of colors increases.

\begin{figure}[h]
    \centering
    
    \begin{subfigure}[b]{0.475\linewidth}
        \captionsetup{justification=centerlast, width=0.9\linewidth}
        \includegraphics[width=\linewidth]{figures/results/delta-acc-colors-by-jitter.pdf}
        \caption{\textit{Color} Task}
        \label{fig:colors-common-colors-8}
    \end{subfigure}
    \hspace{\fill}
    \begin{subfigure}[b]{0.475\linewidth}
        \captionsetup{justification=centerlast, width=0.9\linewidth}
        \includegraphics[width=\linewidth]{figures/results/delta-acc-shapes-by-jitter.pdf}
        \caption{\textit{Shape} Task}
        \label{fig:shapes-common-colors-8}
    \end{subfigure}
    
    % \vspace{1em} % Adjust vertical space between rows if needed
    
    % \begin{subfigure}[b]{0.475\linewidth}
    %     \captionsetup{justification=centerlast, width=0.9\linewidth}
    %     \includegraphics[width=\linewidth]{figures/results/acc-colors-by-p-burstiness-216.pdf}
    %     \caption{\textit{Color} Task (216 Colors)}
    %     \label{fig:colors-common-colors-216}
    % \end{subfigure}
    % \hspace{\fill}
    % \begin{subfigure}[b]{0.475\linewidth}
    %     \captionsetup{justification=centerlast, width=0.9\linewidth}
    %     \includegraphics[width=\linewidth]{figures/results/acc-shapes-by-p-burstiness-216.pdf}
    %     \caption{\textit{Shape} Task (216 Colors)}
    %     \label{fig:shapes-common-colors-216}
    % \end{subfigure}
    
    \caption{Change in accuracy after applying latent intervention for the \textit{color} and \textit{shape} tasks in test-ID and test-OOD for different levels of latent intervention of the \textit{color} latent attribute for various numbers of colors. X-Axis not at scale. Altering the color hue randomly during training allows the model to gain up to 15\% more out-of-distribution accuracy over baseline.}
    \label{fig:latent_intervention}
\end{figure}

\subsection{Discussion}

While all properties improve SG in the \textit{shape} task, \textit{diversity} clearly seems to have the greatest impact overall. However, we see that the other properties seem to be complementary to diversity.  This suggests a positive answer to our research question ``\textit{Can Systematic Generalization be induced solely through changing properties of the training data?}".

Surprisingly, the effects of altering these properties does not only affect performance in $\mathcal{D}_{test-OOD}$  for \textit{shape} and \textit{color}, but also \textit{material} and \textit{size}. This suggests that by default a model will tend to combine all latent features in the representation. We show that by especially targeting the spurious relation between latent factors through changes of the training distribution, this model behavior can be altered. We showed three ways in which this could be done with differing levels of success. This answers our second question: ``\textit{How can we alter the training distribution to unduce SG?}". Finally, we move to answer why these properties affect SG. To do that, we will first study model capacity for SG.