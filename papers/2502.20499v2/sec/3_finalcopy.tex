\section{Experimental Setup}
\label{sec:setup}
We proceed to detail how we operationalize the ideas from the previous section.

\subsection{Dataset \label{sec:dataset}}

We create a dataset resembling the structure of the CLEVR dataset \cite{johnson2017clevr} by leveraging their generative code. Attributes for this dataset are $A = \{shape, color, material, size\}$.  Our main goal is to predict the \textit{shape} attribute systematically, while excluding certain combinations of \textit{shape-color} from $\mathcal{D}_{train}$. There are three possible shapes: \{\textit{sphere}, \textit{cube}, \textit{cylinder}\}.
The context for this dataset is a 224 x 224 color image of 3 to 10 objects. Queries are defined as a textual description of the scene where the property of the object that is to be predicted is masked. Contextual information is also included in the query to make queries unambiguous. Our training set consists of 75.000 samples and the test set of 15.000.

Properties $a$ and $b$ will be color and shape, respectively. We designate a subset of shapes (\textit{cubes} and \textit{cylinders}) to introduce systematic differences. That is, some combinations of these shapes and colors will be in Split A, while the rest will be in Split B. Spheres, on the other hand, will combine to all colors in every split, so they may act as a control.
% Objects of these shapes, $S^{sys}$ are assigned a subset of all possible colors $C$; a particular shape, $s_i \in S$, on split A uses $C_A(s_i) \subset C$, while split B uses the complement $C_B(s_i) = C - C_A(s_i)$. Other subsets of shapes, $s_j \in S^{non}$, (spheres) use all colors in both splits, $C_A(s_j) = C_B(s_j) = C$.
%Similarly to the CLEVR CoGenT splits \cite{johnson2017clevr}, we create two distinct data distributions, that systematically differ in the potential color-shape combinations. We use one of these distributions for training and testing in-distribution (test-ID) and the other for testing out-of-distribution (test-OOD).

%Particularly, $\mathcal{D}_{train}$ includes all possible shapes and colors, but specific shape-color combinations are omitted and only used in the $\mathcal{D}_{test-OOD}$ to access SG abilities. 



% All colors used in the generation are synthetically generated by dividing the color span equally over all the channels to allow for flexibility in the generation. 
% In the description, we refer to each synthetic color in the images by using their HTML hex equivalent as tokens.
% Identically to the original CLEVR CoGenT, the base dataset is generated using colors produced by dividing the color span by 2, thus producing 8 object colors to create the dataset.

\subsection{Distributional Properties of the Training Data}
We modulate diversity by altering the number of colors available in the $\mathcal{D}_{train}$. We test the following total number of colors: $\{8, 27, 64, 125, 216\}$. These values come from dividing the color span in each channel in $n \in \{2..6\}$ equal parts. Burstiness is implemented by limiting the number of possible colors in a given image to 3. This property is modulated by altering $p_{burst}$ with the following values: $\{0.0, 0.5, 1.0\}$. Finally, we implement latent intervention by randomly altering the hue of all colors in the image via the \textit{ColorJitter} transform in PyTorch \citep{pytorch}. Our experiments use the following values: $\{0.0, 0.05, 0.1, 0.5\}$.


%It generates image scenes of objects of different shapes, sizes, colors, and materials, arranged in different positions on the scene.
%For each image, the code generates a scene graph, which details the objects present in the image, their attributes, and the spatial relationships between them. 
%Based on this information we generate a structured textual image description (see appendix \ref{sec:structured-text} for an example). 

%And, following the idea of the originally proposed CLEVR CoGenT splits' \cite{johnson2017clevr} we create two distinct data distributions (or sets), that systematically differ in the potential color-shape combinations. Are composed to create the dataset.
%Specifically, we use one of these distributions for training and testing in-distribution (test-ID) and the other for testing out-of-distribution (test-OOD).
%The training set includes all possible shapes and colors, but certain shape-color combinations are omitted, and only used in the test-OOD set to access SG abilities. 
%Formally, we define subset of shapes (cubes and cylinders) to introduce systematic differences. Objects of these shapes, $S^{sys}$ are assigned a subset of all possible colors $C$; a particular shape, $s_i \in S$, on split A uses $C_A(s_i) \subset C$, while split B uses the complement $C_B(s_i) = C - C_A(s_i)$. Other subsets of shapes, $s_j \in S^{non}$, (spheres) use all colors in both splits, $C_A(s_j) = C_B(s_j) = C$.
%We use split A to generate a train and test in-distribution datasets and split B to generate an out-of-distribution dataset to assess SG. In this way, both splits include all colors but exclude specific color-shape combinations.

%MOVE TO SUPPLEMENTARY MATERIAL
%All colors used in the generation are synthetically generated by dividing the color span equally over all the channels to allow for flexibility in the generation. 
%In the description, we refer to each synthetic color in the images by using their HTML hex equivalent as tokens.
%Identically to the original CLEVR CoGenT, the base dataset is generated using colors produced by dividing the color span by 2, thus producing 8 object colors to create the dataset.
% Following the recent trend of using language as a flexible means of conditioning a machine learning model t

\subsection{Model}
\label{subsec:model}

We use a Transformer encoder \cite{vaswani2017attention} that receives both the context and query as inputs. Following standard procedure \cite{dosovitskiy2020vit}, images are divided into patches and arranged into a sequence of embeddings via a linear layer. The text is passed through an embedding layer. Learned positional and modality (image or text) embeddings are added to the input before the encoder. Both sequence embeddings are concatenated and fed into the transformer encoder (see Fig. \ref{fig:model}). %  Text tokens are randomly masked with probability $p=0.15$ during training, following \citep{devlin-etal-2019-bert}. Finally, we selectively mask text tokens to predict specific object attributes within the image context during testing.

\subsection{Evaluation}

We run each experiment for 3 seeds and report their mean and standard error. All metrics are related to performance on cubes and cylinders to correctly assess for generalization in objects with systematic differences to the training set.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/task-and-model.png}
    \caption{(A) Example input for our task. Image shows objects with different properties, while query gives a textual description of the objects, while masking properties to be predicted. Sample query lists numbered objects from 1-3. (B) Diagram of the model during training. Context is provided as an image, while the \textit{query} consists of text denoting information about the scene with some text related to properties of objects are masked. Both the context and query are entered into a Transformer Encoder, where it classifies for each masked token the most probable value of the masked property in the query.}
    \label{fig:model}
    % \vspace{-0.5em}
\end{figure}

