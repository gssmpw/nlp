\section{Problem Formulation}

We train a model on a similar task to CLEVR\citep{johnson2017clevr} to correctly respond to a textural query given contextual information (in our case, images) about a group of entities contained in it. In particular, queries that will give a partial description of the image (e.g., "\textit{small red rubber sphere, large [?] metallic cylinder, etc.}")  will always resolve to predict the value of one or more particular attributes (e.g., color in this case) of a particular entity (e.g., cube). See Figure \ref{fig:model} for an example input image, query, and expected answer. A model able to perform SG will achieve this over a test set with unseen combinations of these attributes during training. This is relevant because models tend to memorize combinations of attributes (which may prove spurious) during training rather than learn each one separately.

\subsection{Task formalization}
 Let $A$ be a set of latent attributes $\{a_1, a_2, ..., a_{|A|}\}$. Each attribute $a_i$ may take $|a_i|$ possible values. Let an entity $e$ be described by particular realizations of $A$. Thus, an entity $e_i$ can be written as an $|A|$-dimensional vector.

We define a context $c$ as a lossless representation of $N_c$ entities. Let $q$ be one or more queries about the value of a particular attribute of an entity in $c$, Let $y$ be the correct answer to queries in $q$.

Altogether, we can define $\mathcal{D} = \{(c_i, q_i, y_i), i=1 .. N\}$ to be a dataset of $N$ samples, where $c_i, q_i, y_i$ refer to the context, queries and answers for the $i$-th element in the dataset. The task consists of predicting the correct $a_i$, given $c_i$ and $q_i$.

\subsection{Evaluation}

To perform our evaluation, as is standard in the systematic generalization literature, we will assume two latent attributes $\{a,b\in A\}$, where $b$ is the attribute we wish to predict systematically. We will work with two data distributions; Split A: where properties $a$ and $b$ are related; and Split B, where this relation does not hold. Specifically, Split A will contain a subset of all combinations of $a$ and $b$, while Split B will contain all remaining combinations. During our experiments, we use three different sets for training and evaluation: training ($\mathcal{D}_{train}$), test in-distribution ($\mathcal{D}_{test-ID}$), and test out-of-distribution ($\mathcal{D}_{test-OOD}$). $\mathcal{D}_{train}$ and $\mathcal{D}_{test-ID}$ come from Split A, while $\mathcal{D}_{test-OOD}$ comes from Split B. During evaluation we will focus on performance on property $b$.

\subsection{Distributional Properties of Training Data}
\label{sec:distributional_properties_training_data}
We wish to study how three properties of $\mathcal{D}_{train}$ affect SG.  Therefore, we look for distributional properties of $\mathcal{D}_{train}$ that might be able to break the link between $a$ and $b$. To achieve this, we propose three properties, give intuition on why they may work to achieve this goal, and define them.

\subsubsection{Diversity}

To disrupt the link between properties $a$ and $b$, we propose to increase the total number of values $a$ may take.  Our hypothesis is that increasing the diversity of values of $a$ on the training set might make it harder for the model to memorize all combinations of $a$ and $b$. Thus, the model may prefer solutions that learn to detect $a$ and $b$ separately. Therefore, we propose \textit{diversity over attribute $a$} as the total cardinality of $a \in A$, that is $div(a)=|a_i|$.  Over our experiments, we modulate diversity over $a$ in a dataset by altering the number of possible values $a$ may take in the training set.

\subsubsection{Burstiness}

Another way to influence the link between $a$ and $b$ is to disrupt the learning of $a$, such that the model may not rely on $a$ to predict $b$. To instantiate this idea as a data property, we propose the use of \textit{burstiness}. \textit{Burstiness over $a$} is defined as limiting the number of possible values $a$ may take within an individual context $c$. Intuitively, this makes batches of data where only some value of $a$ will be heavily featured, while on other batches a different set of values for $a$ will dominate. That is our intuition of \textit{burstiness}. 
To modulate \textit{burstiness}, we define a probability $p_{burst}$ that a sample is bursty or not. This property is adapted for use in the MMLM setting from previous work \citep{chan2022data-properties-drives-in-context-learning}, where it was applied by limiting the number of different class labels and eliciting a meta-learning algorithm.


\subsubsection{Latent Intervention}

Finally, another way to break this link is to directly alter the value of $a$ while keeping the value of $b$ constant. This approach is reminiscent of the Causal Inference literature \citep{causal_inference, Peters2016}, which uses interventions over causal variables to learn interventional probability distributions, that is, the probability distribution of outcomes should a certain intervention be affected.
Thus, \textit{latent intervention of $a$} is related to altering the values of a latent attribute consistently across all entities in a context $c$. We modulate the degree of latent intervention by the amount the attribute is altered.


%Let $C_E$ be a context which carries full information about $E$.

%Given context $C_E$ and query $Q^{C_E}_{ij}$ a model must predict the value of a latent attribute defined by $Q$.

%We use an MMLM configuration as shown in Fig. \ref{fig:model} to test our ideas.
%The task consists of predicting masked tokens \cite{devlin-etal-2019-bert} from a textual description based on the contextual text and the image the description references. 
%This setup allows us to leverage language to query the learned capabilities of our trained model to infer specific attributes by masking particular tokens of the description (as shown in Fig. \ref{fig:model}).
%We explain the specifics of training and inference in this task in section \ref{subsec:model}.

