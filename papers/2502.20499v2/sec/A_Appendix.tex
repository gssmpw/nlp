\section{Appendix}

\section{Example of structured text}
\label{sec:structured-text}


Below we show an example of a generated sample image from our dataset and structured text.

\textbf{Explicar brevemente estructure y CAMBIAR EJEMPLO A DATASET ACTUAL}

% \begin{figure}[!h]
%     \centering
%     \includegraphics[width=\linewidth]{figures/clevr-image.png}
%     % \caption{Caption}
%     % \label{fig:enter-label}
% \end{figure}

% % \noindent\texttt{[CLS] [O6] large yellow rubber cube [SEP]}

% % \noindent\texttt{[O2] small green rubber cylinder [SEP]}

% % \noindent\texttt{[O0] small red metal cylinder [SEP]}

% % \noindent\texttt{[O3] large purple metal cylinder [SEP]}

% % \noindent\texttt{[O4] small purple rubber sphere [SEP]}

% % \noindent\texttt{[O5] large red metal sphere [SEP]}

% % \noindent\texttt{[O1] large gray metal cube}

% % \vspace{1em}

% \noindent\texttt{[CLS] [O6] large yellow rubber cube [SEP] [O2] small green rubber cylinder [SEP] [O0] small red metal cylinder [SEP] [O3] large purple metal cylinder [SEP] [O4] small purple rubber sphere [SEP] [O5] large red metal sphere [SEP] [O1] large gray metal cube}

\section{Implementation Details}

\subsection{Model}

\subsection{Data Generation}






\subsection{Modular Representations}

Intuitively, this problem can be successfully overcome if the model is biased to completely disentangle the generative factors for each object, i.e., color, shape, material, and size.
In this case, if the model solves the task for the training distribution, solving it for the test-OOD would come as a consequence.
With this in mind, we focus on how diversity and the degree of independence in the data bias the model into learning disentangled representations of these factors, and to what degree models that better achieve this achieve higher SG metrics.

% For this, we use two different measurements on disentangled representations, described in the sections below.


Other methods have used training sets where two generative factors are spuriously correlated to behaviorally evaluate the degree of disentanglement when this correlation is accounted for \cite{zhang2021disentangling-text}. 
This is taken into account in our test-OOD metric, thus we are focusing on a principled way to go past behavioral metrics and understand if internally the model is encoding these representations disentangled.


Most of the metrics \cite{} developed to measure representation disentanglement are designed for models that typically end in a feature vector that synthesizes the complete information of the example, e.g., the result of the last average pooling in Resnets \cite{he2016resnet}.
We have to take an alternative approach since our model uses a Transformer encoder, thus many vectors to perform the task. 
It uses one vector per property per object to perform classification.



So we take a principled approach to measure the degree to which internal representations are disentangled. 
We base our analysis on the measures consistent with the concept of linear disentangled representation proposed by \cite{higgins2018disentangled-representations}.


% Disentangled representations have had many different meanings in different works \cite{}.
Many metrics depicting disentanglement have proposed that these representations should focus in 3 criteria: modularity, compactness, and explicitness \cite{ridgeway2018learning-disentangled-embeddings}.


Modularity in a disentangled representation means that a subspace of the representation will be only used to encode information on one factor \cite{higgins2018disentangled-representations}.
We test for it to quantify how modular are the representations used by our model.

To test it, we want to get how much each task vector is used by the model to encode information about the factor of the task itself in contrast to encoding information from other factors.
For this, we use a linear probe.

We train different linear probes on the task vector to predict all factors of that object.
And, we compare the performance of the model on the actual factor task to the other factors as a comparison.
We train the probes in a new dataset consisting of the concatenation of both the ID and OD tests.
This allows us to use the complete training domain, and prevent the model from taking advantage of any structure in the dataset to correctly predict a factor, e.g. use color to predict shape in the original training sets.




\begin{figure}[h]
    \centering
    % \begin{subfigure}{0.45\linewidth}
    % \includegraphics[width=\linewidth]{figures/results/nmi-vs-probe-complete-colors:shapes.pdf}
    % \caption{Probing Shape from Color Task Vector}
    % \end{subfigure}
    % \begin{subfigure}{0.45\linewidth}
    % \includegraphics[width=\linewidth]{figures/results/probe-ratio-vs-nmi-complete.pdf}
    % \caption{Probing Color from Shape Task Vector}
    % \end{subfigure}
    
    % \hfill\hfill\hfill
    
    \begin{subfigure}{0.45\linewidth}
    \includegraphics[width=\linewidth]{figures/results/probe-ratio-vs-nmi-complete.pdf}
    \caption{Probing Shape from Color Task Vector}
    \end{subfigure}
    \begin{subfigure}{0.45\linewidth}
    \includegraphics[width=\linewidth]{figures/results/acc-vs-probe-ratio-complete.pdf}
    \caption{Probing Color from Shape Task Vector}
    \end{subfigure}
    \hfill
    \caption{Probe Results.}
    % \label{fig:smaller-models}
\end{figure}

