% \section{Task Definition}
% \label{sec:task}
% We represent the evolution of a LOB as a time series $\mathbb{L}$, where each $\mathbb{L}(t) \in \mathbb{R}^{4L}$ is called a LOB record, for $t=1, \ldots, N$, being $N$ the number of LOB observations and $L$ the number of levels.
% %
% In particular,
% \begin{equation}
% \mathbb{L}(t) = \big(P^{ask}(t), V^{ask}(t), P^{bid}(t), V^{bid}(t)\big)
% \end{equation}
% where $P^{ask}(t)$ and $P^{bid}(t) \in \mathbb{R}^L$ are the prices of levels $1$ through $L$, and $V^{ask}(t)$ and $V^{bid}(t) \in \mathbb{R}^L$ are the corresponding volumes.

% \paragraph{Trend Definition}
% \label{sec:data_labelling}
% We employ a ternary classification system for price trends: \texttt{U} (``upward'') denotes an increasing price trend, \texttt{D} (``downward'') indicates a decreasing trend, and \texttt{S} (``stable'') represents price movements with insignificant variations.
% In equity markets, mid-prices are generally considered the most reliable indicator of actual stock prices among single-value metrics. However, due to inherent market fluctuations and exogenous shocks, mid-prices can exhibit highly volatile values. Consequently, direct comparisons of consecutive mid-prices (i.e., $p_t$ and $p_{t+1}$) for stock price labeling would result in a dataset characterized by noise labels.
% To mitigate this issue, labeling strategies typically employ smoother mid-price functions rather than raw mid-prices. These functions aggregate mid-prices over arbitrarily long time intervals, referred to as window length. This approach allows for a more robust and meaningful classification of price trends by reducing the impact of short-term market noise and capturing more persistent directional movements. An example is the one used in \cite{urn:nbn:fi:csc-kata20170601153214969115}, described in \ref{sec:fi-2010}. 

% However, this labeling method is found to be prone to instability, as demonstrated by Zhang et al. \cite{zhang2019deeplob} in Fig.~2 of their paper. This issue arises because smoothing is applied exclusively to future prices. Consequently, trading signals generated under these conditions lack consistency, leading to an increase in redundant trading actions. This inefficiency results in higher transaction costs, posing a significant challenge for the design of effective trading algorithms.
% To solve this problem, Tsantekidis et al. \cite{tsantekidis2017forecasting} proposed a similar labeling method that smooths both past and future prices:
% \begin{equation}
% l(t, k) = \frac{m_+(t, k) - m_-(t, k)}{m_-(t, k)}
% \quad\text{where}\quad
% m_-(t, k) = \frac{1}{k+1}\sum_{i=0}^k p(t - i),
% \end{equation}
% \noindent
% noting that here we sum over \((k+1)\) points and thus divide by \((k+1)\). The main issue with this method is that it considers the window length $k$ as the prediction horizon $h$. Arguably, there is no reason to set $k = h$.
% This results in a labeling bias on the horizon length. 
% For instance, with a short horizon, say $2$, the labels will not be smooth, and conversely, with a large horizon the labels will be overly smoothed. 
% To solve this problem, we propose a more general labeling method that dissociates the smoothing window length $k$ from the prediction horizon $h$.
% The windows are defined as:
% \begin{equation}
% \begin{split}
% w_+(t, h, k) = \frac{1}{k+1}\sum_{i=0}^k p\bigl(t + h - i\bigr), \\
% w_-(t, h, k) = \frac{1}{k+1}\sum_{i=0}^k p\bigl(t - i\bigr).
% \end{split}
% \end{equation}
% The percentage change is defined as:
% \begin{equation}
% l(t, h, k) = \frac{w_+(t, h, k) - w_-(t, h, k)}{w_-(t, h, k)}.
% \end{equation}
% If $l(t,h,k) > \theta$, then the trend is classified as an upward trend; if $l(t,h,k) < -\theta$, it is classified as a downward trend; otherwise, if $-\theta \le l(t,h,k) \le \theta$, it is classified as stable. 
% Here, $\theta$ is the trend threshold parameter. Generally, $\theta$ is defined to balance the classes, so it is not chosen according to its financial implications on trading costs but rather to improve model performance. We argue that $\theta$ should be considered in relation to transaction costs, in order to maximize possible profits. To progress towards a profitable model-based trading strategy, in \ref{sec:theta} we explore an alternative approach to define $\theta$, setting it equal to the average spread (i.e., the difference between the best bid and ask price) expressed as a percentage of the mid-price\footnote{Expressing it as a percentage of the mid-price ensures alignment with the percentage change; otherwise, the two values would be on differing scales.}, which is the primary transaction cost.
% \begin{figure}
%     \centering
%     \includegraphics[width=0.5\linewidth]{images/labelling_methods.pdf}
%     \caption{We show the three different labeling methods. $t$ is the actual timestamp, $k$ is the window length, and $h$ is the prediction horizon. In our method (c), $k$ and $h$ are dissociated, defining a more general and unbiased method.}
%     \label{fig:labeling}
% \end{figure}
% In Fig.~\ref{fig:labeling} we illustrate a comparison of these three labeling approaches. 
% For a fair comparison with SoTA approaches, in the experiments involving FI-2010 we use their original labeling method, whereas for Intel and Tesla we use our newly proposed labeling approach.

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{images/labelling_methods.pdf}
    \caption{Comparison of three labeling methods. $t$ is the current timestamp, $k$ is the smoothing window length, and $h$ is the prediction horizon. In our proposed method (c), $k$ and $h$ are defined independently, providing a more flexible and unbiased approach.}
    \label{fig:labeling}
\end{figure}
\section{Task Definition}
\label{sec:task}
\begin{figure*}[h!]
    \centering
    \includegraphics[width=\linewidth]{images/Architecture.pdf}
    \vspace{-0.6cm}
    \caption{TLOB architecture overview. The model leverages Temporal Self-Attention and Feature Self-Attention within each TLOB block to capture time-wise and spatial relationships in Limit Order Book data. Each block is preceded by Bilinear Normalization to address non-stationarity, followed by an MLPLOB block.}
    \label{fig:architecture}
\end{figure*}
We represent the evolution of a LOB as a time series $\mathbb{L}$, where each $\mathbb{L}(t) \in \mathbb{R}^{4L}$ is called a LOB record, for $t=1,\ldots,N$, with $N$ being the number of LOB observations and $L$ the number of levels. In particular,
\begin{equation}
\mathbb{L}(t) = \bigl(P^{ask}(t),\, V^{ask}(t),\, P^{bid}(t),\, V^{bid}(t)\bigr),
\end{equation}
where $P^{ask}(t)$ and $P^{bid}(t)\in \mathbb{R}^L$ are the prices at levels $1$ through $L$, and $V^{ask}(t)$ and $V^{bid}(t)\in \mathbb{R}^L$ are the corresponding volumes.
\textbf{Trend Definition}
\label{sec:data_labelling}
We employ a ternary classification system for price trends: 
\texttt{U} (``upward'') denotes an increasing price trend, 
\texttt{D} (``downward'') indicates a decreasing trend, 
and \texttt{S} (``stable'') represents price movements with only minor variations.

In equity markets, mid-prices are generally considered the most reliable single-value indicator of actual stock prices. However, owing to inherent market fluctuations and exogenous shocks, mid-prices can exhibit considerable volatility. Consequently, labeling consecutive mid-prices $\bigl(p_t, p_{t+1}\bigr)$ often results in noisy labels.

To mitigate this, many labeling strategies employ smoother mid-price functions, averaging prices over a chosen ``window length'' to reduce short-term noise and better reflect persistent directional moves. An example of this approach appears in \cite{urn:nbn:fi:csc-kata20170601153214969115}, detailed in Section~\ref{sec:fi-2010}.

However, as shown by Zhang et al.~\cite{zhang2019deeplob} (Fig.~2), smoothing only the future prices can lead to instability in trading signals. This instability often causes redundant trading actions and higher transaction costs. To address this, Tsantekidis et al.~\cite{tsantekidis2017forecasting} proposed also smoothing past prices. They define:
\begin{equation}
l(t, k) 
\,=\, \frac{m_+(t, k) \;-\; m_-(t, k)}{m_-(t, k)} 
\quad\text{where}
\end{equation}
\begin{equation}
m_+\bigl(t, k\bigr) \;=\; \frac{1}{k+1}\sum_{i=0}^k p\bigl(t + i\bigr)\quad\text{and}
\end{equation}
\begin{equation}
m_-\bigl(t, k\bigr) \;=\; \frac{1}{k+1}\sum_{i=0}^k p\bigl(t - i\bigr),
\end{equation}
noting that $i$ runs from $0$ to $k$, so there are $(k+1)$ terms in the sum. A key drawback is that the window length $k$ coincides with the prediction horizon $h$. This can bias the labels: for instance, a horizon of $h=2$ may not provide enough smoothing, whereas a large horizon might over-smooth price moves.

To overcome this, we propose a more general labeling strategy that dissociates $k$ from $h$. Specifically, we define:
\begin{equation}
w_+\bigl(t, h, k\bigr) = \frac{1}{k+1}\,\sum_{i=0}^k p\bigl(t + h - i\bigr)
\end{equation}
\begin{equation}
w_-\bigl(t, h, k\bigr) = \frac{1}{k+1}\,\sum_{i=0}^k p\bigl(t - i\bigr).
\end{equation}
The percentage change is then
\begin{equation}
l\bigl(t, h, k\bigr) 
\,=\, \frac{\,w_+\bigl(t, h, k\bigr) \;-\; w_-\bigl(t, h, k\bigr)\,}
             {\,w_-\bigl(t, h, k\bigr)\!}.
\end{equation}
We classify a trend as \emph{upward} if $l(t,h,k) > \theta$, 
\emph{downward} if $l(t,h,k) < -\theta$, 
and \emph{stable} if $-\theta \,\leq\, l(t,h,k) \,\leq\, \theta$.
The threshold $\theta$ is often chosen to balance the three classes rather than to reflect trading costs. We argue, however, that relating $\theta$ to transaction costs can better align trend predictions with profitability. 
Thus, in Section~\ref{sec:theta}, we examine setting $\theta$ to the average spread (the difference between the best bid and ask prices) as a percentage of the mid-price\footnote{Expressing the spread as a percentage of the mid-price preserves consistency with $l(t,h,k)$, which is also a percentage.}, since the spread represents the main transaction cost.



Figure~\ref{fig:labeling} illustrates all three approaches. 
For a fair comparison with existing literature, we adopt the original labeling method in our FI-2010 experiments and use our new labeling strategy for Intel and Tesla data, where the more general approach better handles varying horizons.
