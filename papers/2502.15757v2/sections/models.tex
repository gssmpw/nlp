\section{Models}
\label{sec:models}
We propose two novel deep learning models for Stock Price Trend Prediction (SPTP) using Limit Order Book (LOB) data. The first, called \textbf{MLPLOB}, is a simple MLP-based model. The second, \textbf{TLOB}, leverages a dual-attention Transformer-based approach. Both models take as input a sequence of LOB time series consisting of the last $T$ LOB snapshots for 10 LOB levels.

\subsection{MLPLOB}
A key finding from the benchmark study by Prata et al.~\cite{prata2024lob} reveals that, despite the proliferation of specialized deep learning architectures for SPTP, their performance often converges toward low values when tested on diverse and complex datasets. Inspired by the work of Tolstikhin et al.~\cite{tolstikhin2021mlp} and Zeng et al.~\cite{zeng2023transformers}, who demonstrated that simple MLP-based models can perform as well as state-of-the-art (SoTA) methods in certain domains, we develop an MLP-based architecture for SPTP with LOB data, called \emph{MLPLOB}.

\textbf{Architecture Overview.}
MLPLOB is composed of multiple blocks, each containing two types of MLP layers:
\begin{enumerate}
    \item \emph{Feature-Mixing MLPs}, which operate along the feature axis.
    \item \emph{Temporal-Mixing MLPs}, which operate along the time axis.
\end{enumerate}
This design aims to capture both spatial and temporal relationships in LOB data--characteristics that Sirignano and Cont \cite{sirignano2019deep, sirignano2021universal} identified as fundamental to LOB dynamics and modeling.

Each MLP layer consists of two fully connected layers, mirroring the MLP component used in Transformer architectures~\cite{vaswani2017attention}. Initially, the input sequence is projected linearly into a tensor $\mathbf{X}\in \mathbb{R}^{T \times N}$, where $N$ is a chosen hyperparameter.

\textbf{Feature-Mixing MLPs.}
We apply a feature-mixing MLP row by row (\emph{i.e.}, for each time step $i$). Formally,
\begin{equation}
    \mathbf{U}_{i,*} \;=\; 
    \sigma\Bigl(
      \text{LayerNorm}\bigl(
          \sigma(\mathbf{X}_{i,*}\,\mathbf{W}_1)\,\mathbf{W}_2 
          \;+\; \mathbf{X}_{i,*}
      \bigr)
    \Bigr)
    \quad\text{for } i=1,\dots,T,
\end{equation}
where $\sigma$ is the GeLU activation function~\cite{hendrycks2016gaussian}, and \(\text{LayerNorm}\) denotes layer normalization.

\textbf{Temporal-Mixing MLPs.}
Next, we transpose the resulting tensor $\mathbf{U}$ and apply a temporal-mixing MLP column by column (\emph{i.e.}, for each feature dimension $j$):
\begin{equation}
    \mathbf{Z}_{*,j} \;=\; 
    \sigma\Bigl(
      \text{LayerNorm}\bigl(
          \sigma(\mathbf{U}_{*,j}\,\mathbf{W}_3)\,\mathbf{W}_4 
          \;+\; \mathbf{U}_{*,j}
      \bigr)
    \Bigr)
    \quad\text{for } j=1,\dots,N.
\end{equation}



\textbf{Model Simplicity and Isotropic Design.}
The MLPLOB architecture relies only on matrix multiplications, reshaping operations, and scalar nonlinearities. It also adopts an \emph{isotropic design}, wherein each block (beyond the initial projection) has a constant dimensionality. This contrasts with the pyramidal layouts found in many CNNs (which reduce spatial resolution while increasing channel depth). Notably, isotropic designs are also common in Transformers and Recurrent Neural Networks (RNNs).

\textbf{Final Prediction.}
After several blocks of feature and temporal mixing, MLPLOB performs dimensionality reduction to collapse all features into a single vector, which then passes through several fully connected layers that gradually diminish the vector dimension and a final standard classification head. The network outputs the directional trend (up, down, or stable) for the final time step. Our primary objective in devising MLPLOB is to show that a carefully structured MLP-based model can match or exceed more complex architectures in the SPTP task. The same method is also applied to TLOB. 



\subsection{TLOB}

The Transformer architecture~\cite{vaswani2017attention} has led to major breakthroughs in deep learning, notably in natural language processing~\cite{brown2020language, khan2022transformers} and time-series modeling~\cite{wen2022transformers}. A key advantage is the ability to capture long-range dependencies without suffering as much from vanishing gradients or forgetting, and performance typically scales favorably with increased data~\cite{kaplan2020scaling}. Because massive volumes of financial data (in the terabyte range) are available, Transformers are well-positioned for LOB modeling.

\textbf{Dual-Attention Blocks.}
We propose \emph{TLOB}, a Transformer-based architecture specifically designed for Limit Order Book data. Each TLOB block contains:
\begin{enumerate}
    \item \emph{Self-Attention over LOB Snapshots (Temporal Axis)}, computes attention values between different LOB snapshots, capturing time-wise dependencies among consecutive snapshots.
    \item \emph{Self-Attention over LOB Features (Spatial Axis)}, computes attention values between LOB features, capturing spatial relationships among different price-volume features.
    \item An \emph{MLPLOB block}, which replaces the usual Transformer feed-forward network to enhance the modelâ€™s capacity for combining spatial and temporal signals.
\end{enumerate}
The architecture is shown in Fig. \ref{fig:architecture}.

\textbf{Temporal vs. Feature Attention.}
While standard Transformers~\cite{vaswani2017attention} process tokens along a single dimension, LOB data naturally requires both temporal and spatial dependencies to be learned \cite{sirignano2019deep, sirignano2021universal}. For instance, time-step $t$ can reveal how deeper or shallower levels relate to one another, as well as how trends evolve over past snapshots. Hence, \emph{dual-attention} explicitly addresses these two axes of variation. To investigate the importance of each type of attention layers we performed an ablation study (Section \ref{sec:ablation}).

\textbf{Bilinear Normalization Layer.}
To address non-stationarity and magnitude disparity (prices and sizes) in financial time series, we employ a Bilinear Normalization layer~\cite{tran2021data} as the initial layer. Unlike conventional $z$-score normalization, which can fail under distribution shifts at inference time, bilinear normalization adapts to batch-specific statistics, maintaining robust performance even when market conditions change. The same layer is also used in MLPLOB.

\textbf{Positional Encoding.}
Because self-attention is permutation-invariant, we incorporate sinusoidal positional embeddings~\cite{vaswani2017attention} to preserve the chronological structure within each LOB window. This embedding ensures that TLOB respects the temporal ordering of snapshots, which is crucial for modeling price evolution.

By blending two distinct self-attention operations (temporal first, then spatial) with an MLPLOB feed-forward component, TLOB is designed to capture the complex market microstructure present in LOB data. Its Transformer foundation enables effective scaling for large datasets, while the dual-attention mechanism better handles the fine-grained feature interactions and sequence dependencies characteristic of financial time series.
