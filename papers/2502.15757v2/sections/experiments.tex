\section{Experiments}
We conduct a comprehensive evaluation of MLPLOB and TLOB model training and testing on both the Benchmark FI-2010 dataset and the TSLA-INTC dataset, composed of Tesla and Intel. TLOB and MLPLOB surpass SoTA performances on every dataset and every horizon. TLOB performs the best on larger horizons, while MLPLOB performs the best on the shorter ones.
Our experiments extend beyond merely demonstrating the state-of-the-art performance of TLOB, aiming to address several critical research questions: (1) Are stock prices harder to forecast than in the past? (2) What if we choose $\theta$ equal to the average spread? (3) Are temporal and spatial attention necessary?
Through these investigations, we seek not only to validate our models' predictive capabilities but also to contribute to the broader understanding of deep learning applications in financial forecasting. 
%is publicly available at \hyperlink{github.com/LeonardoBerti00/LOBForecasting}{github.com/LeonardoBerti00/LOBForecasting} 
\begin{table*}[h!]
\centering
% Add a caption
\captionsetup{width=0.8\textwidth}
\caption{Intel and Tesla main features for January 2015. Average liquidity is computed as the average quantity available in the first 10 LOB levels.} 
\label{tab:stocks} % Add a label for referencing
\begin{tabular*}{0.8\textwidth}{@{\extracolsep{\fill}} l c c c c @{}}
\toprule
\textbf{Stock} & \multicolumn{1}{c}{\textbf{Daily Return (\%) }} & \multicolumn{1}{c}{\textbf{Daily Volume}} & \textbf{Avg. Spread} & \textbf{Avg. Liquidity} \\
\midrule
TSLA & $-0.42 \pm 2.84$ & $23,927,602 \pm 4,554,884$ & $0.16$ & $3,320$ \\
INTC & $-0.44 \pm 1.66$ & $304,325,400 \pm 69,340,430$ & $0.01$ & $124,960$ \\
\bottomrule
\end{tabular*}
\end{table*}
\subsection{TSLA-INTC Dataset}
In the majority of state-of-the-art (SoTA) research within the domain of Deep Learning applied to LOB data, researchers typically employ one, two, or three stocks \cite{shi2023neural, coletta2021towards, nagy2023generative, li2020generating, shi2022state, hultin2023generative}, predominantly from the technology sector. Adhering to this established practice, we construct a LOB dataset comprising two NASDAQ-listed stocks namely, Tesla and Intel -- spanning the period from January 2nd to January 30th, 2015. We posit that stylized facts and market microstructure characteristics exhibit independence from individual stock behaviors (as demonstrated in \cite{bouchaud2009markets, bouchaud2002statistical, cont2014price, gould2013limit}\footnote{These seminal works in finance elucidate the universal statistical properties of LOBs, transcending specific stocks and markets.}), thereby rendering specific stock attributes non-critical to the analysis. The dataset encompasses 20 order book files for each stock, corresponding to each trading day, resulting in a total of approximately 24 million samples. 
Each order book sample is represented as a tuple $\big(P^{ask}(t), V^{ask}(t), P^{bid}(t), V^{bid}(t)\big)$, where $P^{ask}(t)$ and $P^{bid}(t) \in \mathbb{R}^L$ denote the prices at levels $1$ through $L$, and $V^{ask}(t)$ and $V^{bid}(t) \in \mathbb{R}^L$ represent the corresponding volumes. The dataset is partitioned such that the initial 17 days are allocated for training, the 18th day for validation, and the final two days for testing. In Table \ref{tab:stocks} we present the main features of Tesla and Intel for January 2015. In Figure \ref{fig:stocks_trend_lob2022} we show the mid-price traces. As shown both the features and the mid-price traces are very different, making the evaluation more general. Unfortunately, we cannot make the dataset public for copyright reasons.

\textbf{Sampling}. Limit Order Book data, especially for liquid stocks, is massive, every day, hundreds of thousands of orders are placed for each stock. Furthermore, financial data are known to have a low signal-to-noise ratio \cite{nagel2021machine}. Accordingly, it is unnecessary to consider every LOB update, so defining a valid sampling technique is essential. While time-based and event-based sampling methods \footnote{FI-2010 uses this type of sampling, a LOB snapshot is taken every 10 events.} are used, they fail to capture the varying impact of transactions. In fact, single transactions can have very different impacts on the market. Volume-based sampling offers a solution by sampling the LOB after a predetermined volume of shares has been traded, thus reflecting the magnitude of market activity. Therefore, we adopted a sampling strategy based on trading volume, where snapshots of the Limit Order Book (LOB) are taken every 500 stocks traded. This method achieves a compromise between maintaining adequate temporal consistency within windows and ensuring significant variation between samples.

    
\begin{figure}[h!]
    \centering
        \begin{minipage}{\columnwidth} 
        \includegraphics[width=\columnwidth]{images/TSLA_mid_price_plot.pdf}
        \end{minipage}
        \begin{minipage}{\columnwidth}
        \includegraphics[width=\columnwidth]{images/INTC_mid_price_plot.pdf}
        \end{minipage}
    \caption{Tesla and Intel mid-price traces for January 2015.}
    \label{fig:stocks_trend_lob2022}
\end{figure}

\subsection{Benchmark dataset FI-2010}
\label{sec:fi-2010}
Our model will be evaluated against SoTA models utilizing the FI-2010 benchmark dataset \cite{urn:nbn:fi:csc-kata20170601153214969115}. The FI-2010 dataset \cite{urn:nbn:fi:csc-kata20170601153214969115} is the most widely adopted LOB dataset within the field of deep learning applications to limit order books \cite{zhang2019deeplob, zhang2020deep, tsantekidis2017forecasting, tsantekidis2017using}, particularly for forecasting endeavors. It comprises LOB data from five Finnish companies listed on the NASDAQ Nordic stock exchange: Kesko Oyj, Outokumpu Oyj, Sampo, Rautaruukki, and Wärtsilä Oyj. The data span ten trading days, from June 1st to June 14th, 2010, encompassing approximately 4 million limit order snapshots across ten levels of the LOB. The authors sampled LOB observations at intervals of ten events, resulting in a total of 394,337 samples.
The label associated with each data point, indicative of mid-price movement, is determined by the percentage change between the prevailing mid-price:
\begin{equation}
p(t) = \frac{P^{ask}(t) + P^{bid}(t)}{2}
\end{equation}
and the average of the subsequent $h$ (chosen horizon) mid-prices:
\begin{equation}
 m_+(t, k) = \frac{1}{k}\sum_{i=1}^k p(t+i)
\end{equation}
The percentage change is thus defined as:
\begin{equation}
l(t) = \frac{m_+(t, k) - p(t)}{p(t)} 
\end{equation}
where $k$ represents the window length, which in this instance also corresponds to the prediction horizon $h$.
Labels are assigned as explained in \ref{sec:task}. The dataset furnishes time series and corresponding class labels for five distinct horizons: $h \in H = \{10, 20, 30, 50, 100\}$. The dataset's authors employed a uniform threshold $\theta = 2\times 10^{-3}$ across all horizons. The value is chosen to balance the classes for $h = 50$. 



\subsection{Experimental settings}
For each dataset, we trained and tested the performance of each model on different horizons, namely $10$, $20$, $50$, $100$. 
All the experiments were carried out using an RTX 3090.
Since the FI-2010 dataset also contains 104 handcrafted features derived from the LOB, we used them in both our models. This choice improved the performance of the F1-Score (\%)  by approximately $1$.
For Tesla and Intel, given the availability of message files containing the order information, we augmented the LOB snapshots by concatenating them with the corresponding orders. This integration was undertaken to incorporate additional information not present in the LOB. Consequently, this approach resulted in an approximate improvement of $1.5$ in the F1-score (\%) . We report the details on the hyperparameters search in the Appendix (\ref{app:hp}).

\textbf{Baselines} As comparative baselines, we employed 3 machine learning models: Support Vector Machine (SVM), Random Forest and XGBoost, and 8 deep learning SoTA LOB-based models: MLP, LSTM \cite{tsantekidis2017forecasting}, CNN \cite{tsantekidis2017using}, CTABL \cite{tran2018temporal}, DAIN \cite{passalis2019deep}, CNNLSTM \cite{tsantekidis2020using}, DeepLOB \cite{zhang2019deeplob} and BiN-CTABL \cite{tran2021data}. Due to computational constraints, we selected the top two performing models from FI-2010, specifically DeepLOB, and BiNCTABL, and exclusively trained and tested these models with the TSLA-INTC dataset. 

\textbf{Trend Classification Threshold} We remark that $\theta$ is the parameter that determines if a percentage change $l_t$ is classified as an up, stable, or downtrend. For the TSLA-INTC dataset, to ensure balanced class distribution, we set $\theta$ equal to the mean percentage change. In Sec. \ref{sec:theta} we explore an alternative approach to defining $\theta$ based on financial parameters rather than class balance optimization. For the FI-2010 dataset, we retained the original labels to maintain consistency with existing benchmark studies and previous works.

\textbf{Metric} We selected the F1-score as our primary performance metric because it captures both precision and recall in a single value. Accuracy is not a valid metric for our experiments because the classes are not balanced for each horizon. The F1-Score is robust to the class imbalance problem, which detrimentally affects the accuracy. Finally, the F1-score is the most used metric in the SoTA papers tackling the SPTP task. For a comprehensive evaluation, we provide precision and recall curves in the Appendix (\ref{app:results}).
\section{Results}
\begin{table}[h!]
\centering
\begin{minipage}{0.48\textwidth}
\caption{F1-score on the FI-2010 dataset on four horizons. 
Bold values show the best scores.} 
\label{tab:f1-2010}
\centering
        \begin{tabular}{l|cccc}
            \toprule
            & \multicolumn{4}{c}{\textbf{FI-2010 F1-Score (\%) }$\uparrow$} \\
            \cmidrule(lr){2-5} 
            \textbf{Model} & \textbf{h = 10} & \textbf{h = 20} & \textbf{h = 50} & \textbf{h = 100} \\
            \midrule
            SVM & 35.9 & 43.2 & 49.4 & 51.2 \\
            Random Forest & 48.7 & 46.3 & 51.2 & 53.9 \\
            XGBoost & 62.4 & 59.6 & 65.3 & 67.6 \\
            MLP & 48.2 & 44.0 & 49.0 & 51.6 \\
            LSTM \cite{tsantekidis2017using} & 66.5 & 58.8 & 66.9 & 59.4  \\
            CNN \cite{tsantekidis2017forecasting} & 49.3 & 46.1 & 65.8 & 67.2 \\
            CTABL \cite{tran2018temporal} & 69.5 & 62.4 &  71.6 & 73.9 \\
            DAIN-MLP \cite{passalis2019deep} & 53.9 & 46.7 & 61.2 & 62.8 \\
            CNNLSTM \cite{tsantekidis2020using} & 63.5 & 49.1 & 69.2 & 71.0 \\
            DeepLOB \cite{zhang2019deeplob} & 71.1 & 62.4 & 75.4 & 77.6 \\
            BiNCTABL \cite{tran2021data} & 81.1 & 71.5 & 87.7 & 92.1 \\
            \midrule
            MLPLOB & \textbf{81.64} & \textbf{84.88} & \textbf{91.39} & 92.62 \\
            TLOB & 81.55 & 82.68 & 90.03 & \textbf{92.81} \\
            \bottomrule
        \end{tabular}
        \end{minipage}
\end{table}
\subsection{FI-2010 results}
Table \ref{tab:f1-2010} presents the performance comparison across four prediction horizons\footnote{Note that the horizon values represent the number of events before the sampling process of the dataset, while in the benchmarks \cite{prata2024lob, urn:nbn:fi:csc-kata20170601153214969115} the values represent the horizons after the sampling process. In other words, the horizons considered are the same and are the ones defined originally in FI-2010.} for the FI-2010 benchmark dataset. In the Appendix (\ref{app:results}) we report also the precision and recall curves for horizon 100. MLPLOB and TLOB exhibit very high precision, also at high recall values, consistently achieve higher precision at all recall levels compared to the other models.
The results for the baselines are extracted from the benchmark of Prata et al. \cite{prata2024lob}\footnote{if we had taken the results reported in the individual papers, MLPLOB and TLOB would have still outperformed all the other models.} since the settings are equal for the FI-2010 dataset. 
MLPLOB and TLOB outperform all the other models analyzed in \cite{prata2024lob}, surpassing state-of-the-art performance. Interestingly, MLPLOB demonstrates the best performance in the first three horizons. Notably, the performance differential between MLPLOB and TLOB is minimal, which, as we will demonstrate in Section \ref{sec:lobster}, can be attributed to the lower complexity of the FI-2010 dataset, which explains the uselessness of a more complex architecture such as TLOB for this particular dataset.

\subsection{Tesla and Intel results}
\label{sec:lobster}
\begin{table}[h!]
\centering
\begin{minipage}{0.48\textwidth}
\centering
\caption{F1-score for Tesla on four horizons. Bold values show the best scores.} 
\label{tab:tesla}
\centering
        \begin{tabular}{c|cccc}
            \toprule
            & \multicolumn{4}{c}{\textbf{TSLA F1-Score (\%) }$\uparrow$} \\
            \cmidrule(lr){2-5} 
            \textbf{Model} & \textbf{h = 10} & \textbf{h = 20} & \textbf{h = 50} & \textbf{h = 100}  \\
            \midrule
            DeepLOB & 36.25 & 36.58 & 35.29 & 34.43 \\
            BiNCTABL & 58.69 & 48.83 & 42.23 & 38.77 \\
            \midrule
            MLPLOB & \textbf{60.72} & \textbf{50.25} & 38.97 & 32.95 \\
            TLOB & 60.50 & 49.74 & \textbf{43.48} & \textbf{39.84}   \\            \bottomrule
        \end{tabular}
        \end{minipage}
\end{table}

\begin{table}[h!]
\centering
\begin{minipage}{0.48\textwidth}
\centering
\caption{F1-score for Intel on four horizons. Bold values show the best scores.} 
\label{tab:intel}
\centering
        \begin{tabular}{c|cccc}
            \toprule
            & \multicolumn{4}{c}{\textbf{INTC F1-Score (\%) }$\uparrow$} \\
            \cmidrule(lr){2-5} 
            \textbf{Model} & \textbf{h = 10} & \textbf{h = 20} & \textbf{h = 50} & \textbf{h = 100}  \\
            \midrule
            DeepLOB & 68.13 & 63.70 & 40.3 & 30.1 \\
            BiNCTABL & 72.65 & 66.57 & 53.99 & 41.08 \\
            \midrule
            MLPLOB & \textbf{81.15} & \textbf{73.25} & 55.74 & 43.18 \\
            TLOB & 80.15 & 72.75 & \textbf{62.07} & \textbf{50.14} \\
            \bottomrule
        \end{tabular}
        \end{minipage}
\end{table}

In Table \ref{tab:tesla} we show the results for Tesla and in Table \ref{tab:intel} for Intel.
For each stock, we trained a different model. 
In the Appendix (\ref{app:results}) we report also the precision and recall curves for a horizon equal to 100. For INTC, they exhibit excellent precision at low recall values, indicating their ability to accurately identify the most confident positive instances.
MLPLOB outperforms every model on the first two horizons (10, 20), while on the longer horizons (50, 100) TLOB outperforms every model. This is expected since Transformers excels at long-range dependencies. 
Notably, the difference in performance between MLPLOB and TLOB for the shorter horizons is minimal ($\approx 0.5$), while on the longer horizons is significant ($\approx 7$). As expected the longer the horizon the more difficult to forecast.
In general, the performances are much lower with respect to FI-2010. We conjecture that this is due to the fact that FI-2010 is characterized by a lower level of complexity with respect to NASDAQ stocks. This derives from the fact that it is composed of Finnish stocks, which are less liquid and efficient than NASDAQ stocks such as Intel and Tesla. Additionally, the data dates back to 2010. Indeed, as will be demonstrated in the subsequent experiment, the prediction difficulty augments as time goes by. The results of our experiment are supported by several works on the topic \cite{prata2024lob, zhang2019deeplob, sirignano2019deep}. 
All the models are trained until convergence. Notably, both TLOB and MLPLOB achieve convergence in less than half the epochs required by BiNCTABL and DeepLOB.

\subsection{Are stocks harder to forecast than in the past?}
\begin{table}[h!]
\centering
\begin{minipage}{0.48\textwidth}
\centering
\caption{F1-score for Intel on two different periods, from 2012 and 2015. The horizon is set to 50.} 
\label{tab:intel2}
\centering
        \begin{tabular}{c|cc}
            \toprule
            & \multicolumn{2}{c}{\textbf{F1-Score (\%) }$\uparrow$} \\
            \cmidrule(lr){2-3} 
            \textbf{Model} & \textbf{INTC 2015} & \textbf{INTC 2012} \\
            \midrule
            TLOB & 60.19 & 66.87 \\
            \bottomrule
        \end{tabular}
        \end{minipage}
\end{table}
\label{sec:past}
This experiment examines the challenges associated with market prediction over time and the self-destruction of predictable patterns in financial markets. Empirical evidence consistently demonstrates that forecasting models effective in certain periods become obsolete over time. Several studies indicate that previously observed predictability patterns disappeared after becoming widely known. Dimson and Marsh \cite{dimson1999murphy} found this for the UK small-cap premium, while Bossaert and Hillion \cite{bossaerts1999implementing} noted a decline in international stock return predictability around 1990. Aiolfi and Favero \cite{aiolfi2005model} reported similar findings for US stocks in the 1990s. The market is increasingly efficient and difficult to predict as time goes by. We extend this investigation to our best-performing model TLOB. Specifically, we tested on a day of Intel from 2012/06/21\footnote{we remark that in a single day of Intel, there are hundreds of thousands of orders making the experiment statistically significant. Furthermore, the trading day was extracted from the LOBSTER public sample files available at \url{https://lobsterdata.com/info/DataSamples.php} and it was the only day available, eliminating the possibility of cherry picking.} and confronted the difference in performance with  2015/01/30. We report the performance in Table \ref{tab:intel2}. As expected the performance from 2012 is better than that from 2015. We confirm the hypothesis and the empirical evidence from other works.

\subsection{Alternative Threshold Definition Using Average Spread}
\begin{table}[h!]
\centering
\begin{minipage}{0.48\textwidth}
\centering
        \caption{F1-score on Tesla with $\theta$ set to the average spread.} 
        \label{tab:theta}
        \begin{tabular}{c|ccc}
            \toprule
            & \multicolumn{3}{c}{\textbf{F1-Score (\%) }$\uparrow$} \\
            \cmidrule(lr){2-4} 
            \textbf{Model} & \textbf{h = 50} & \textbf{h = 100} & \textbf{h = 200}  \\
            \midrule
            TLOB & 41.39 & 36.48 & 30.82 \\
            \bottomrule
        \end{tabular}
        \end{minipage}
\end{table}
\label{sec:theta}

Based on the fact that predictability has to be considered in relation to the transaction costs, we explore an alternative approach to define the trend classification parameter $\theta$, setting it equal to the average spread as a percentage of the mid-price, reflecting the primary transaction cost. 
This methodology could only be applied to Tesla data, as Intel's higher trading volume (approximately 10 times greater in January 2015) and lower volatility relative to traded shares would result in 99.99\% of trends classified as stationary.
We set the horizons to 50, 100, and 200 because with shorter horizons 99\% of the mid-price movements would be classified as stationary. 
In Table \ref{tab:theta} we report the results. 
In general, performances show a deterioration, which is probably caused by the classes' unbalance.
This experiment highlights the necessity for further refinements in trend definition and method complexity when targeting profitability in practical applications.

\subsection{Ablation Study}
\label{sec:ablation}
To evaluate the contribution of each attention mechanism within the TLOB architecture, we performed an ablation study on the FI-2010 dataset. Specifically, we compared the performance of the complete TLOB model against two ablated versions: one without spatial attention (TLOB w/o SA) and another without temporal attention (TLOB w/o TA). To avoid inconsistency, we maintain the total number of layers fixed\footnote{TLOB has 4 temporal attention layers and 4 spatial attention layers, TLOB w/o SA has 8 temporal attention layers and TLOB w/o TA has 8 spatial attention layers}. The F1-scores for each model across four prediction horizons (h = 10, 20, 50, and 100) are presented in Table \ref{tab:ablation}. The results demonstrate that the full TLOB model, incorporating both spatial and temporal attention mechanisms, consistently outperforms both ablated versions across all prediction horizons. The performance gain of the full TLOB model highlights the importance of capturing both spatial relationships between LOB features and temporal dependencies across LOB snapshots. This suggests that the dual-attention mechanism effectively learns complementary information, leading to improved predictive accuracy compared to models relying on only one type of attention.

\begin{table}[h!]
\centering
\begin{minipage}{0.48\textwidth}
\caption{Ablation study results. F1-score on the FI-2010 dataset on four horizons. 
Bold values show the best scores.} 
\label{tab:ablation}
\centering
        \begin{tabular}{l|cccc}
            \toprule
            & \multicolumn{4}{c}{\textbf{FI-2010 F1-Score (\%) } $\uparrow$} \\
            \cmidrule(lr){2-5} 
            \textbf{Model} & \textbf{h = 10} & \textbf{h = 20} & \textbf{h = 50} & \textbf{h = 100} \\
            \midrule
            TLOB w/o SA & 79.59 & 78.96 &  87.51 & 91.40 \\
            TLOB w/o TA & 80.27 & 79.20  & 87.72 & 91.42 \\
            \midrule
            TLOB & \textbf{81.55} & \textbf{82.68} & \textbf{90.03} & \textbf{92.81} \\
            \bottomrule
        \end{tabular}
        \end{minipage}
\end{table}