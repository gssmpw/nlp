\section{Related Work}
The challenge of modeling the complex data structures and vast quantities associated with LOBs has spurred the development of deep learning algorithms for related modeling and forecasting tasks.
In this section, we will summarize the State-of-The-Art (SoTA) deep learning models in the Stock Price Trend Prediction (SPTP) task, which consists of forecasting the direction of mid-price movements at a high-frequency resolution. 
Tsantekis et al.\cite{tsantekidis2017using} (2017) utilize a Recurrent Neural Network (RNN) based on Long-Short Term Memory (LSTM) layers to predict mid-price movements. In the same year, the authors presented another approach \cite{tsantekidis2017forecasting}, introducing a CNN-based model (CNN). Subsequently, the same group proposed two additional architectures in \cite{tsantekidis2020using} (2020). The first focuses on capturing temporal dynamics from LOB data and correlating temporally distant features using convolutional layers. The second architecture, CNNLSTM, merges the CNN with an LSTM. The CNN initially extracts features from the LOB time series, which are then passed to the LSTM for classification.
Tran et al.\cite{tran2018temporal} (2018) proposed the Temporal Attention-Augmented Bilinear Layer (TABL) for multivariate time series prediction. This architecture, applies a bilinear transformation to the input, capturing dependencies between features and over time. The authors extended this model in \cite{tran2021data}, introducing BINCTABL, which integrates a bilinear normalization layer into the architecture to address non-stationarity and magnitude disparity (for instance prices and sizes) within the time series data. 
Passalis et al.\cite{passalis2019deep} (2019) introduced DAIN (Deep Adaptive Input Normalization), a three-step layer that adaptively normalizes data, instead of relying on fixed precomputed statistics. DAIN comprises three layers: a shifting layer, a scaling layer, and a gating layer, which suppresses irrelevant features by applying a sigmoid function. DAIN was integrated into various architectures, including MLPs \cite{nousi2019machine}, CNNs \cite{tsantekidis2017forecasting}, and RNNs \cite{cho2014learning}. 
Zhang et al. \cite{zhang2019deeplob} (2019) introduced DEEPLOB, which consists of three main blocks: convolutional layers, an Inception Module, and an LSTM layer. The convolutional layers and Inception Module extract relevant features, while the LSTM captures temporal dependencies. Two years later, Zhang et al.\cite{zhang2021multihorizonforecastinglimitorder} (2021) extended DEEPLOB by adopting the attention \cite{luong2015effective} mechanism, creating DEEPLOBATT for multi-horizon forecasting. In this architecture, an encoder extracts features from LOB data, and an attention mechanism assigns weights to the hidden states, improving the processing of long input sequences.
Finally, Kiesel et al.\cite{kisiel2022axial} (2022) introduced Axial-LOB, which uses axial attention \cite{ho2019axial} to factorize 2D attention into two 1D attention modules, one for the feature axis and one for the time axis. 
Prata et al. \cite{prata2024lob} evaluated 15 deep learning models for stock prediction using limit order book data, including all the models described above. Some performed well on the FI-2010 dataset, but most of them showed non-reproducible results. When trained and tested on a new dataset composed of NASDAQ stocks most failed to generalize, in particular the performances of every model flattened around 60 in F1-Score. BiN-CTABL (\cite{tran2021data}) was the top performer, and attention-based models generally excelled. The main reason given by the authors is that NASDAQ stocks are more complex to forecast than Finnish ones and the SoTA models cannot capture this complexity. Furthermore, all models showed high sensitivity to hyperparameters and context. These results make the models unreliable for real-world use.

Another stream of research has focused on meta-learning Transformer models, i.e., TabPFN models~\cite{hollmann2022tabpfn, hoo2025tabular}, that excel at ``one forward-pass'' inference on small tabular datasets, leveraging massive synthetic corpora of low-dimensional tasks to learn prior feature distributions. 
However, they become computationally prohibitive for large-scale LOB data, where millions of high-dimensional observations strain both memory and processing. 
Consequently, TabPFN-based methods remain impractical for demanding real-world predictive tasks, such as SPTP, underscoring the need for more scalable deep learning architectures (e.g., LSTM, CNN, or specialized Transformers) tailored to extensive time series. 

