\section{\our~Framework}
\label{sec:method}

\subsection{Rule Filtering}
\label{ruleFiltering}

We induce rules from a set of input documents (\S \ref{sec:genSpace}), which are expected to be noisy.  Two rules may even predict conflicting labels to a given input, akin to labeling functions in data programming \cite{DBLP:journals/corr/abs-1711-10160ratner,zhang2022survey}. Ideally, the final set of filtered rules needs to be accurate, diverse and high in coverage \cite{bajpai-etal-2024-fair}. 


For rule filtering, we use the submodular graph-cut (GC) function \cite{kothawade2021prism}, as proposed by \citet{bajpai-etal-2024-fair}. Using GC, we select a final set of representative and diverse rules $\mathcal{R}_{f}$, from the set of candidate rules $\mathcal{R}$. For $\mathcal{R}_{f} \subseteq \mathcal{R}$, we define the GC function as:
$$ f_{GC}(\mathcal{R}_{f})=\sum_{r_{i} \in \mathcal{R}, r_{j} \in \mathcal{R}_{f}} s_{ij}-\lambda \sum_{r_{i},r_{j}\in \mathcal{R}_{f}}s_{ij} $$

Here, $\lambda \in [0,1]$ governs the diversity-representation trade-off, where higher $\lambda $ implies higher diversity in $\mathcal{R}_{f}$. $s_{ij}$ is the similarity score for rule pair $r_i$ and $r_j$. It is calculated as the weighted sum of the precision, coverage, and agreement between the pair of rules:
$$s_{ij} = \alpha(r_i)+\alpha(r_j)+w*\beta(\{r_i, r_j\}) + \gamma*\mu(r_i,r_j)$$

Here, $\alpha(r_i)=\text{Precision}(r_i)$, $\mu(r_i,r_j)$ is the agreement, calculated as the fraction of instances where both rules agree. $\beta(\{r_i, r_j\})$ is the coverage, calculated as the fraction of instances labeled by at least one of the rules.  %In the GC function, $\lambda\in [0,1]$ governs the trade-off between representation and diversity and we need a committed set that is diverse enough. In our experiments, we set $\lambda$ to 0.7 so that the final candidate rules are diverse in nature. GC is a non-monotone submodular for $\lambda > 0.5$, hence in our case, $f_{GC}$ is a non-monotone submodular function. 

Our objective function is $\max_{|\mathcal{R}_{f}|\leq k} f_{GC}(\mathcal{R}_{f})$, where $k$ is a fixed budget \cite{kothawade2021prism}. We employ a greedy approach to choose a rule that maximizes the marginal utility 
$\textit{argmax}_{r_{i}\in\{\mathcal{R}-\mathcal{R}_{f}\}} f_{GC}(\mathcal{R}_{f}\cup \{r_i\})-f_{GC}(\mathcal{R}_{f})$. %We iteratively add a rule from the new candidate set of rules, starting with an already filtered rulesAt any point in an iteration, we start with the  and then iteratively add . 
Please note that \citet{bajpai-etal-2024-fair} starts with an empty set, whereas we start with the existing rule set obtained from the previous round of bootstrapping. One round of filtering is completed until the fixed budget $k$ is exhausted. 



\subsection{Bootstrapping Rules and Synthetic Data}

%The rules we induce can be used as labeling functions in Data Programming, henceforth to be referred to as Programmatic Weak Supervision  \cite[PWS][]{DBLP:journals/corr/abs-1711-10160ratner,varma2018snuba}. 
%

\citet{varma2018snuba} previously employed a bootstrapping based rule induction approach for labeling available unlabeled data. In \our, combining synthetic data generation and automated rule induction presents us with an opportunity to bootstrap and expand our labeled dataset and rules iteratively.


%, we instead employ bootstrapping to iteratively generate and refine new rules and synthetic data. 

We start our bootstrapping with the training split of the available gold labeled data as the seed, as shown in Figure \ref{archi}. We synthetically generate new data for each class using prompt demonstrations, with the demonstrations retrieved from the seed set \cite{zhang-etal-2022-active, peng2024revisiting}. Similarly, we perform rule induction (\S \ref{sec:genSpace}) from the seed. The induced rule candidates are then filtered using the validation split of the available gold data and added to the rule set (Figure \ref{archi}). Similarly, data filtering for the synthetically generated exemplars is performed using the rule set. In data filtering, only those exemplars that match their generated label with the predicted label from the generative model are filtered. Finally, the seed set is expanded with the filtered data. The seed set and the rule set are expanded after every iteration of bootstraping.

\paragraph{Seed and validation set during few-shot: } \citet{zhu-etal-2023-weaker} observe that while weak supervision systems use limited training data, they heavily rely on the availability of a clean gold-labeled validation data for their performance gains. Hence, in our few-shot setups we do not use any validation split of the data and instead use only the few-shot training splits. Here, the gold labeled data, i.e. the few-shot training split,  is  used only as the validation data for rule filtering. Further, the initial iteration of synthetic generation happens in a zero-shot setup without demonstrations. Moreover, the rule candidates induced for the initial iteration is also from the synthetically generated samples, similar to subsequent iterations. 


%and the  PWS systems rely heav on the quality of gold-labeled data, especially 311
%in the validation split. Hence, we use our gold- 312labeled data as validation data for rule filtering


%with newly filtered data after every iteration. Similarly, the rule set is also expanded after every iteration of the bootstrapping process.






%synthetically generated data and filter the rules (\S \ref{ruleFiltering}) using the gold data as the validation split. % The induced rules are then used for learning a generative model via PWS \cite{cage}. 
%, where  Our validation set is never expanded and is always the gold-labeled data. 

% %The synthetically generated data are generated with a label. Hence, we use the induced rules, via PWS, for . The filtered data points can be used to induce new rules, which can then be filtered using rule filtering. 
 

\paragraph{Data Augmentation:} We use prompt demonstration, long context ICL or few-shot depending on the task setup, to synthetically generate new labeled sentences using LLMs. For each label, we sample $m$ instances each of positive and negative samples from the seed set and then use it for generating new data samples \cite{10.1145/3617130smithLoop,lin-etal-2023-selective}. Our prompt demonstration approach includes label information, positive examples, and negative examples for synthetic generation. In addition to generating new data samples, we also perform paraphrasing of data samples in the seed set. By paraphrasing, we gain diverse syntactic structures for better rule induction.\footnote{For more details, refer \S \ref{paraphrase}} 



%We use data augmentation to synthetically  and our rule induction framework extracts rules from dependency parses of the input sentences. Here, our gold standard data is expected to be few-shot, typically 5 or 10 labeled samples per class. While, paraphrasing may bring some diversity in terms of dependency parses, it can be quite limiting for rule induction. At the same time, while it is in principle possible generate as much synthetic data as possible, these data also may needs filtering to ensure quality.
%with For rules filtering, we start with the seed .





%Hence, we apply bootstrapping to iteratively filter both synthetically generated inputs and the rules induced. For data filtering, we simply learn a generative model using rules induced and we consider only those sentences that agree with the intended label from the LLM and the predicted label from the generative model. Now, once the sentences are filtered, they are used to induce more rules. 


 %We then use graph cut based rule filtering approach to select our rules. With every new iteration of newly generated synthetic data, we get new set of rules. Now, from the new set of candidate rules, we iteratively select a rule that maximises the marginal utility at every iteration. 



%\subsection{Joint Learning with Rules}
%\label{contrastive}

 %across the combined labeled and unlabeled dataset. %Specifically, for any input $\mathbf{x}_i \in \mathcal{U}$, where $\mathbf{l}_i$ represents the output vector of all labeling functions (LFs), we determine the predicted label for $\mathbf{x}i$ using the LF-based graphical model $P\theta(\mathbf{l}_i, y)$ as $g(\mathbf{l}i) = \arg\max_y P\theta(\mathbf{l}_i, y)$.
%The final component is the quality guides \cite{cage},denoted as $R(\theta|{q_j})$.



%to improve the robustness and stability of unsupervised likelihood training while utilizing LFs. Let parameter $q_j$ represents the fraction of instances where LF $l_j$ correctly triggers, while $q_j^t$ represents the user's estimation of the agreement between the labels $y_i$ and $l_{ij}$ for a given example $\bfx_i$ for which the labels $y_i$ and $l_{ij}$ agree.
%In cases where the user's beliefs are unavailable, we leverage the precision of the LFs on the validation set as a surrogate measure for the user's beliefs.



%\paragraph{Contrastive Representation Learning:} The pre-trained model, $P_\phi^f$, undergoes contrastive representation learning prior to joint learning. Following, \citet{zhang-etal-2021-shot} and \citet{singhal-etal-2023-intendd}, we first perform self-supervised contrastive learning (SSCL) over a pre-trained model. Here, the model parameters for a given pre-trained model is updated using, $\mathcal{L}_{pt} = \mathcal{L}_{sscl} + \lambda_{pt}\mathcal{L}_{mlm}$. $\mathcal{L}_{pt}$ is a weighted sum of token-level masked language modeling loss ($\mathcal{L}_{mlm}$) and a sentence-level SSCL  \cite[$\mathcal{L}_{sscl}$;][]{wu2020clear,liu-etal-2021-fast}. $\lambda_{pt}$ is a weight hyper-parameter. %sum$\mathcal{L}_{pretraining}$ is computed as . Here, $\mathcal{L}_{mlm}$ is token level masked language modeling loss andination 
%For SSCL, given an input document  $x_i$, we obtain perturbations of  $x_i$ by randomly masking tokens from it. Further, we dynamically mask tokens such that each sentence has different masked positions across different training epochs. SSCL attempts to bring the $x_i$ and its masked versions closer in the semantic space while pulling away other pairs. 

%After the continued pretraining, we perform supervised contrastive learning \cite{NEURIPS2020_supervisedContrastive}. Here, we try to increase the similarity between input pairs that belong the same class, while trying to bring down the similarity of those belonging to different classes.   We follow the supervised contrastive learning \cite{NEURIPS2020_supervisedContrastive} loss, where all the documents in the same class in a batch are brought together. Here, the same document may also be used to create like pairs by creating perturbations of the input. 


%obtain pairs of sentences, and treat sentences belonging to the same class as like pairs and those belonging to different classes as dissimilar pairs. The similarity between like pairs needs to be increased and that of the dislike pairs needs to be decreased. 




%\subsection{Data Augmentation}

%In-context data augmentation approaches has gained wide adoption of recently. Our data augmentation strategy can further be categorised into two. One, is synthetic data generation which enables synthesis of label specific-sentences using an LLM. Our second approach involves paraphrasing of sentences such that the semantic content and  label specific information of the labeled sentences are preserved. Here, we provide a prompt, the label and the labeled examples for each class. We also provide, sentences that have close similarity to the current set of sentences but belong to other classes also in the prompt. We use, multiple LLMs. In addition to prompting the LLM to provide synthetically generated sentences, we also paraphrase the labeled sentences leading to diverse dependency parses.The sentences generated out of paraphrasing is directly fed to the dataset and used for rule induction. 





%The greedy algorithm begins with an empty set and then iteratively adds a rule from the candidate set to the committed set by maximizing the marginal gain in every iteration until the budget constraint is met. The pseudo-code of our approach is given in Algorithm \ref{algo:FAIR}. For a candidate set of rules $\mathcal{R}$ induced from ARI approaches (Section \ref{sec:rule-methods}), we compute precision on $\mathcal{L}$ using $findprecision$ and coverage over $\mathcal{U}$ using $findcoverage$ for each rule $R_i$. We calculate the agreement between two rules $R_i, R_j$ using the $findagreement$ function. Then, we compute $s_{ij}$, $(i,j)^{th}$ entry of matrix $S$ defined as in line 8. Finally, we find the set of committed rules $\mathcal{F}$ using Eq. \ref{eq:gc} for a pre-specified budget of $k$ rules.

%Section \ref{sec:genSpace} defines the feature space and further elaborates on how we induce rules from the features in the feature space. The generalization is performed by first obtaining a partition of the dataset. Each partition forms a subset of the corpus and we then identify the set of features that correspond to the partition of input. LGG is applied to the features corresponding to each partition. 




%The features are scored using the labeled information. Specifically, we look for the 


%First, the features are enumerated from the labeled split of the input corpus $\mathcal{D}_l$. These features are then scored using   We define $Count(f_t,\mathcal{D}_{l})$ as the count of input documents containing the feature. Further, $Count(f_t,\mathcal{D}_{l}^{y_j})$ is defined as the count of input documents labeled with the label  $y_j$ and has the feature $f_t$ present in its dependency parse. %Summarily we filter features based on threhsold and then  score them using the {\sc label-PMI}. 

%as an extended lesk algorithm, where the PMI score is multiplied by the square of the number of nodes in the feature.  






%We need to obtain a partition of the dataset based on which feature subsets are obtained. These feature subsets are then used to construct a lattice to obtain the generalization using the least general generalization. 



%We need to perform feature set selection from the list of enumerated features. Here, based on the {\sc Label-PMI} score, we select the top k features such that there exists a complete coverage of the feature over the entire corpus $\mathcal{D}$. We use a community detection algorithm called Louvain. Louvain is a modularity-based graph partitioning approach, where communities are reassigned between neighbors iteratively to increase the modularity and then the nodes in a single community are merged to create a new graph. The  process iteratively is performed until the modularity can no longer be increased or until it reaches the maximum upper bound on the number of iterations. In our case, the network structure is constructed at our end and is an outcome of the features selected. So we perform both feature selection and dataset partitioning jointly.

%We consider all the inputs $x_i$ in $\mathcal{D}$ as a node $V_{x_i}$ in $\mathcal{G}_{\mathcal{D}}$. Here, two nodes are connected if they share a common feature and they are assigned a weight corresponding to the {\sc Label-PMI} of the feature. Finally, we normalize the edge weights of the $\mathcal{G}_{\mathcal{D}}$. We then perform Louvain community detection with the graph so constructed and obtain the modularity score based on the community partitioning. %To avoid partitioning of the labeled instances to different communities, we merge all the labeled instances into a single node prior to performing community detection.


%Here, the network structures for community detection highly depend on the features that are considered for edge formation. To find the most relevant set of features, we employ a greedy feature selection approach, Recursive Feature Elimination (RFE). Here, we start with a large number of features resulting in a densely connected network and iteratively eliminate the weakest features, where a feature's weakness is identified based on the label-PMI score it has. The Louvain is performed at each iteration of the feature selection approach. Here, by the end of the Louvain if two nodes with different labels form part of the same community, then we add the relevant features connecting those nodes as the weakest link and remove those. Such a step ensures that nodes with different labels do not end up in the same community.  


%For a label, we identify features with non-zero scores and further group them into different subgroups based on the presence of each core relation edge. Now, all the features that form a subset in the partition is then constructed as part of our lattice. For such a partition, a lattice formed would consider a  core edge relation as its supremum. Since, we consider 6 core relations, a particular subset in a partition may lead to 6 different lattices, one each for a core relation. For instance, given a core dependency relation `{\sc nsubj}', we find all the features containing the relation as an edge. The lattice contains a combination of both structural and grammatical generalizations. The features form the most specific forms in the lattice and the most generalized concept in the lattice would then be an edge relation `{\sc nsubj}' with `Any' node. For each partition, we find the least general generalization which forms a rule. We repeat this for each grouping of features. 








%It is desirable to a have feature set with the following characteristics. one, high coverage on the entire dataset, measured as the fraction of data points in $\mathcal{D}$ that has at least one feature present in it from the feature set. Two, low conflict implying if two features are applicable on multiple data points, they should not apply different labels to these data points. Conflict can be measured as simply the fraction of data points for which multiple possible labels are possible as per the features. Now, once 

%From the partition obtained, we consider those subsets where there exist at least one labeled instance. Moreover, we ensure that no partition contains labeled instances from two different labels by construction and as well as in the node moving process. 


%Once we obtain all the rules for all the labels, we compute the score for each rule, by calculating the mutual information for each rule with each label and sort them. Finally, we iteratively add one rule at a time from the sorted descending list until the coverage is completed.  


%Here, `foxes' form the %subject of the sentence with the verb `jumped' as its head. Since the edge (foxes,jumped) also is a subtree of the dependency tree it is a feature in our feature space. Further, we define it to be a generalised form of other features such as shown in Figure \ref{}. Summarily it is a generalised form of all the higher higher order subtrees with it as an edge in sub-tree. Each subtree forms a partial order and with each additional node. Similarly, if we consider the node with the word `foxes', it may be stemmed to form fox and its POS `NOUN'. The POS, Stemmed-form and the final inflected form all can be seen as generalisations of each other. Hence, each node in the subtree may further be thought of as to have three different levels of information. Finally, we may have generalisations beased on thr combination of trees and the forms in the individual nodes. The generalisation lattice for the given cocnept starting with the pair (foxes,jumped) is shown in Figure \ref{}. 


%We are looking to i Specifically, we look for induced sub-graphs of the dependency parses of the input. We look for higher-order dependency subgraphs which  and 





%Define feature space $\mathcal{F}$ containing various feature names extracted from the input documents. Define rule space, from which include all $\mathcal{P}$ possible rules that can be induced. Here, a rule can be any generalization of a set of features. Hence, trivially a feature in itself can be a rule. Given a dataset $\mathcal{D}_{l}$, we encode the inputs in the feature space $\mathcal{F}$





%For a text classification task, we essentially want to identify rules that can help assign label to inputs that match the rules.  For all inputs, typically a sentence or a document, we obtain the sentence level dependency parses. From the parses we extract all possible induced connected subtrees upto third order, i.e. a maximum of four nodes and three edges. From the features, we need to identify group of features that can be generalised to form a single rule that maps to a single output label from the output space. The generalised form of the features form of the features form a rule and further these rules should ideally map to a single label with high confidence. 


%With each of the pattern in the concept lattice, we associate a label, that belongiung to the label space of the task. Based on the concept lattice, we apply least general generalisation to obtain the rules.  





%These subtrees form the features which then need to be clustered and the generalised 


%We aim to idnetify rules that can help identify 






%For each sentence in the training data, we may obtain its dependency parse along with its POS tag using an off-the-shelf dependency parser. A dependency parse is a directed tree with labeled edges, where each edge represents a relation between the head node (source node) and the child node (target node). Here, each node has three features. Its surface form, stem, and POS tag. Structurally, we identify neighboring nodes to the existing nodes. Our rules can be of the type as follows in Figure \ref{}.




%The rules we look for are essentially induced subgraphs of up to 4 nodes (and 3 edges) in the dependency parses of the input sentences. The first step involves featurisation of the input, which includes finding the dependency parse of all the input sentences. The second step involves identifying relevant induced subgraphs from the input that are discriminative for a particular label or for a subset of sentences from the set of sentences having the same label. Third step involves finding a high coverage set with minimal conflicting cases. Finally, we integrate these rules into the learning framework using semi supervised data programming. 













% A partition for $\mathcal{D}_{l}$ is obtained after encoding the data points in $\mathcal{D}_{l}$ based on a suitable feature space $\mathcal{F}$. Each partition is then generalized to its least general generalization which then forms a rule. Once a set of rules are generated it needs to be integrated with a learning model typically a foundational model. Here, `Spear', a semi-supervised data programming framework is employed for the integration. In Spear, these rules are  used as labeling functions. The data programming framework aggregates predictions from the set of labeling functions (induced rules) into a single discrete distribution over the label space. It is then integrated with the learning model by obtaining the KL divergence of the learning model's predicted distribution into the loss. 







%integrates the predictions from the set of labelling functions as a discrete distribution over the labels and its KL-divergence with the predictions of the model is obtained

%Our approach can be broadly categorized as a sequence of the rule induction process and  a rule integration process. Our automatic rule induction process can further be broken down into the following steps

%We use induced subgraphs of the dependency parses of the input as our features in the feature space $\mathcal{F}$. Specifically, we look for higher-order dependency subtrees that form connected components and include pairwise relations between words. Moreover,  these words may need not occur continuously in the written form but are connected induced-subgraphs of the original parse.  O


%Since every subtree of a feature $f_i$ will be considered a generalization of the feature, we extract all the features of exactl









%Moreover, our rule space $\mathcal{R}$ is defined as any generalization of the features. Here, a rule may be a generalized form of a set of features.  By definition, any feature is trivially a rule as a single feature can be considered a singleton set. However, there are generalizations for a set of features that can be seen as a rule but not a feature. 



%The subtrees in $\mathcal{F}$ can be generalized in various ways. Every feature $f_t$ is an induced subtree of a dependency parse. Further, any induced subgraph of a feature can be seen as a generalization of the feature, henceforth to be referred to as structural generalizations. Moreover, another way to generalize would be to consider abstract grammatical properties. The inflected forms may be generalized to form the stems, and t he generalized forms of stems can be further generalized to POS tags. We call them `grammatical generalizations'. For instance, the stem of `foxes' and `jumped' are `fox' and `jump' respectively. Here, the subtree `' is a generalized form of `', where the generalized represent all instances of trees containing the inflected forms `brown', `jumped', and `quickly' but may contain any inflected form of `fox'.  Similarly, the tree `' contains one of its nodes as the POS, implying it is a generalization representing all occurrences where with the inflected forms `brown', `jumped', and `quickly' but has a common noun as the fourth node forming the child of the relation `nsubj' with `jumped' as its head. Figure \ref{} and \ref{} shows further generalizations where the former represents all subtrees with the node containing a proper or common noun and the final being more general representing a structure where the nodes may be of any POS but the dependency relation needs to be preserved. Each of these generalizations is a rule. 



% \paragraph{Training: Supervised component, unsupervised and semi-supervised components in the loss}
% \lipsum[1-3]

