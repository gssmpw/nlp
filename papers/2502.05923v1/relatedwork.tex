\section{Related Work}
\our~uses syntactic n-grams as its rules. Use of syntactic contexts in constructing feature space for downstream NLP tasks has been extensively explored in several of the past works \cite{liang-etal-2011-learning, goldberg-orwant-2013-dataset,biemann2016network}. \citet{goldberg-orwant-2013-dataset} released a large scale collection of syntactic n-grams obtained from 3.4 million books. \citet{biemann2016network} looks from a network science perspective and focuses on graph motifs. Learning feature functions using morphosyntactic information as horn clauses has shown to benefit under a low-resource setting for languages such as Czech and Sanskrit, often requiring less than 10\% of labeled training data required for neural counterparts \cite{10.1162/coli_a_00390,krishna-etal-2018-free}. 

Using syntactic context, we incorporate signals that may not otherwise be explicitly captured in large language models. Further, we automate the generation and filtering of such rules by relying extensively on rule induction approaches \cite{varma2018snuba,bajpai-etal-2024-fair,lao2010relational}. Additionally, we consider our rule generation approach as a restricted instance of program synthesis via least general generalization as demonstrated in \citet{Raza_Gulwani_Milic-Frayling_2014}, and \citet{thakoor2018multisynthesis}.

Data augmentation and generation in text has become effortless with LLMs \cite{ding2024data}. However, that does not ensure obtaining data with relevant supervisory signals, highlighting the need for targeted data filtering or generation \cite{pmlr-v139-killamsetty21a,mirzasoleiman2020coresets}. This may include data scoring and ranking \cite{lin-etal-2023-selective}, iterative data generation \cite{rao-etal-2023-makes}, bootstrapping \cite{varma2018snuba} or targeted subset selection \cite{pmlr-v37-wei15}. \citet{wang-etal-2023-lets} and \citet{lee-etal-2024-llm2llm} explore similar themes by utilizing errors from language models to iteratively refine a synthetic training dataset. Similarly, \cite{hoang-etal-2018-iterative} discussed back-translation in the context of machine translation to augment training data. In \our, we use bootstrapping approach for data filtering and apply our filtering on synthetically generated data, instead of unlabeled data from an existing corpus.