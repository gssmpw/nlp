\section{Related Work}
\our~uses syntactic n-grams as its rules. Use of syntactic contexts in constructing feature space for downstream NLP tasks has been extensively explored in several of the past works ____. ____ released a large scale collection of syntactic n-grams obtained from 3.4 million books. ____ looks from a network science perspective and focuses on graph motifs. Learning feature functions using morphosyntactic information as horn clauses has shown to benefit under a low-resource setting for languages such as Czech and Sanskrit, often requiring less than 10\% of labeled training data required for neural counterparts ____. 

Using syntactic context, we incorporate signals that may not otherwise be explicitly captured in large language models. Further, we automate the generation and filtering of such rules by relying extensively on rule induction approaches ____. Additionally, we consider our rule generation approach as a restricted instance of program synthesis via least general generalization as demonstrated in ____, and ____.

Data augmentation and generation in text has become effortless with LLMs ____. However, that does not ensure obtaining data with relevant supervisory signals, highlighting the need for targeted data filtering or generation ____. This may include data scoring and ranking ____, iterative data generation ____, bootstrapping ____ or targeted subset selection ____. ____ and ____ explore similar themes by utilizing errors from language models to iteratively refine a synthetic training dataset. Similarly, ____ discussed back-translation in the context of machine translation to augment training data. In \our, we use bootstrapping approach for data filtering and apply our filtering on synthetically generated data, instead of unlabeled data from an existing corpus.