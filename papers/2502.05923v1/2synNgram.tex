\section{ARISE - Automatic Rule Induction Using Syntactic Tree Generalization}
\label{sec:genSpace}

\begin{figure*}[h] 
    \centering
   \includegraphics[width=\textwidth,trim={0cm 0cm 0 0cm},clip]{rule_induction-Page-2.drawio-new.pdf}
   \caption{We induce rules via inductive generalization on syntactic n-grams, as shown (dependency relations omitted for brevety). The symbol `$\sqsupseteq$' denote a generalization operation. Trees labeled from $f_1$ to $f_5$ are instances of features. Similarly, trees labeled from $r_1$ to $r_8$ are rules.}
   \label{genSamples}
   \end{figure*}


Distributional hypothesis \cite{firth1957synopsis} is often realized using vector space models defined over a feature space \cite{turney2010frequency}. Inputs can be encoded as dense contextualized vectors \cite{peters-etal-2018-deep,devlin-etal-2019-bert} or  a sparse semantic space consisting of lexical n-grams, syntactic n-grams \cite{goldberg-orwant-2013-dataset}, higher order dependency features \cite{koo-collins-2010-efficient}, or even graph motifs \cite{biemann2016network}.

We induce rules that can capture complementary information that is not explicitly captured in pre-trained neural models. Hence, we focus on incorporating structured grammatical information typically used in a traditional NLP pipeline \cite{manning-etal-2014-stanford} such as Part-of-Speech (PoS) and syntactic information.  From dependency parses of input sentences, we extract induced subtrees as features. Each such feature is a syntactic n-gram, with the nodes as the words and the edges labeled with the dependency relations. We then induce rules via the inductive generalization of these features, using LGG \cite{Raza_Gulwani_Milic-Frayling_2014, thakoor2018multisynthesis}. %In this section, we elaborate on our rule induction framework and Section \ref{} focuses on the iterative rule filtering approach.

%Here, we consider from sentence-level dependency parses of the input as features. The features we consider are amenable for their analysis, akin to syntactic n-grams, higher order dependency features, as well as graph motifs. In this work, we interchangeably use the terms subtrees or syntactic n-grams to refer to our features. 

For a text classification task with $k$ labels, we assume the  availability of a labeled dataset $\mathcal{D}$,  where $\mathcal{D} = \{(x_i,y_i)\}_{i=1}^{n}$, $x_i$ is an input document and $y_i \in \{l_1,l_2,...,l_k\}$ is a label. We obtain sentence-level dependency parses for each $x_i \in \mathcal{D}$. A feature space $\mathcal{F}_{t=1}^{f}$ is defined over higher-order factorization of the dependency parses in $\mathcal{D}$. Each feature $f_t \in \mathcal{F}$ is an induced subtree of the parses for sentences in $\mathcal{D}$. In Figure \ref{genSamples}, $f_1$ to $f_5$ denote instances of features in our feature space. These are syntactic n-grams extracted from  sentence-level dependency parses of the input. A feature covers a set of documents in which that feature occurs at least once.

%at least in a sentence in the document.  
%Further, the cardinality of the set is the support of the feature.




Rules are generalizations of features. Now, $r_1$ to $r_8$, in Figure \ref{genSamples}, show various generalized rules induced from the features $f_1$ to $f_5$. If a generalized rule subsumes multiple features, then it covers a union of all the sets of documents corresponding to those features.   Our rules are induced as the least general generalization (LGG) over a set of features \cite{plotkinnote,plotkin1971further}. A feature can be a rule in itself, i.e. $\mathcal{F} \subseteq \mathcal{R}$, though it will be the most specific form of a rule. Previously, LGG was extensively used in information extraction \cite{califf-mooney-1997-relational,nagesh-etal-2012-towards}, program synthesis \cite{Raza_Gulwani_Milic-Frayling_2014,kitzelmann2010combined}, and in several other areas of relational learning \cite{muggleton1992efficient,zelle1994combining}.  %where the tree always contains inflected surface-forms of the words and occur directly in the dependency parse of a sentence. Now, 

We define two forms of generalizations for forming the rules, both structural and linguistic. If rule $r_i$ is an induced subtree of $r_j$, then we can say that $r_i$ is more general than  $r_j$. Linguistic generalization include, substitution \cite{Raza_Gulwani_Milic-Frayling_2014,thakoor2018multisynthesis}, of the nodes containing words with their corresponding stems, and PoS tags \cite{galitsky2019least}. 

Figure \ref{genSamples} shows illustrative cases of generalization.  Let us consider a corpus from which features $f_1$ to $f_5$ are extracted.  Rules $r_1$ to $r_7$ show linguistic generalization. Similalry, rule $r_8$ shows structural generalization from $r_7$. Consider rules $r_1, r_4, r_5$ and $r_7$.  These rules contain nodes with a group of words. Similarly, $r_6$ represents a rule that has a group of PoS tags in one of the nodes. In linguistic generalization, multiple trees are generalized to a single tree by grouping words or PoS that differ in these individual trees. Here, $r_1$ is a generalisation of $f_1$ and $f_2$. Similarly, $r_4$ is a generalization of $f_2$ and $f_3$. 



 We currently restrict the groupings at a node to be homogeneously typed, i.e. a set can either be that of inflected word forms, stems or of PoS tags, but not a mix of those. Further, the cardinality of such a group is set to an arbitrary upper bound, to avoid trivial generalisations. The rules we generate belong to $\mathcal{R}_{t=1}^{r}$. Here, for every input in $x_i \in \mathcal{D}$ it should either predict a label from $\{ 1, ..., k \}$, if the rule is applicable to the input. Else, it should abstain from making a prediction. 















%Each feature can be seen as an instance of the most specific form of a rule, where the tree always contains inflected surface-forms of the words and occur directly in the dependency parse of a sentence.   


%We can obtain a rule from a set of features by  leveraging classical Inductive Logic Programming (ILP) approaches that enable bottom-up traversal from the most specific to more general concepts.


 % Trivially, we may form rules by taking a feature as is and applying conditions as to when to predict a label and when to abstain.  

%We follow notations similar to \citet{pryzant-etal-2022-automatic} wherever applicable.  For a given semi-supervised classification task, with labeled and unlabeled data points. Here, $\mathcal{D} = \mathcal{D}_l \cup \mathcal{D}_u$, where $\mathcal{D}_l = \{(x_i,y_i)\}_{i=1}^{M}$ and $\mathcal{D}_u =\{x_{i+M}\}_{i=1}^{N} $ and $x_i$ is an input document and the labels $y_i \in \{1,2,...,K\}$. The overall process starts out with $\mathcal{D}_{l}$. We obtain sentence-level dependency parses for each $x_i \in \mathcal{D}_l$. A feature space $\mathcal{F}_{t=1}^{T}$ is defined over higher-order factorization of the dependency parses in $\mathcal{D}_{l}$. Each feature $f_t \in \mathcal{F}$ is an induced subtree, of  up to an arbitrary fixed size, present in $\mathcal{D}_{l}$ . 


%Essentially, 

%We use LGG in a bottom-up fashion to identify the least general concept between concepts. These rules are morpho-syntactic structures, which are not explicitly exposed to the pretrained models during any of the parameter updation stages. Previously, \citet{Raza_Gulwani_Milic-Frayling_2014} proposed a program synthesis approach using LGG. Here, they defined a Domain Specific Language consisting of ordered trees. The DSL originally is intended to be used for transformations between symbolic structures that can be defined by the DSL, such as XML transformations, string manipulations, or preordering sentences for machine translation. However, our rule induction approach can be seen as a limited case of the DSL, where we always map a generalized subtree to a label and not any higher-order structure.

 %covering generalisation from words to word-groups($r_1$), word group to stem ($r_2$), word-group to POS  ($r_3$), POS to POS-Group ($r_6$)

%Our generalizations are as follows: 




%Further,  of $f_1$ and $f_2$.  $r_1$ represents all the sentences that either have $f_1$ or $f_2$ in their dependency parses. Similarly, $r_2$ and $r_3$ are also generalizations of $f_1$ and $f_2$, but not their LGG.  


%Based on our current description, it may be quite trivial to create generalizations. Similar to $r_5$, one may simply obtain rules where nodes may contain arbitrary set of words and can keep on generalizing by adding new words at these nodes. To avoid such trivial generalizations, we keep an upper bound on the size of the set that a node may have. Also, for simplicity,

%where generalisation can be obtained by grouping to form a single tree out of simply form a set of words by taking a union of words from their corresponding features


%Here, $r_8$ is a subtree of $r_7$. 


\subsection{Rule Induction via LGG}
\label{filtering}

We obtain features from dependency parses of the dataset $\mathcal{D}$. %Further, we filter only those features with a minimum support. Moreover, 
We consider only those subtrees that exactly have one of the six core dependency relations in them \cite{de-marneffe-etal-2014-universal, nivre-etal-2020-universal}. These core dependency relations are direct or indirect objects, nominal or clausal subjects, as well as clausal or open clausal complements. We partition the features into six mutually exclusive subsets, with each subset corresponding to one of the core relations. 

A complete lattice is constructed out of each partition, by adding a supremum and infimum element to the partition. Here, we add a rule `* $\xleftarrow{rel}$ {\sc *}', where `${rel}$' is the core-relation corresponding to the partition. It is the supremum for any element in the partition, as every element in the partition is subsumed by it and covers any document that has the relation present in it. We also define `$\epsilon$' as the infimum and it represents an empty rule that rules out any document in the input. The complete lattice provides a search space of rules over which the partial ordering is provided. Here, any two pair of subtrees have a least general generalization or a least upper bound \cite{Raedt2010}. In Figure \ref{genSamples}, $r_1$ is the LGG of $f_1$ and $f_2$.  $r_1$ represents all the sentences that either have $f_1$ or $f_2$ in their dependency parses. Similarly, $r_2$ and $r_3$ are also generalizations of $f_1$ and $f_2$, but not their LGG.


%Each partition is then used to construct complete lattice. Here, the supremum for the lattice is a  subtree of two nodes with the edge representing the corresponding core relationrepresenting a core relation 

For every rule in the lattice, we compute its label-PMI vector, following \citet{singhal-etal-2023-intendd} and  \citet{Jin_Wanvarie_Le_2022}. Label-PMI vector is a vector of the pointwise mutual information scores of the rule corresponding to each label. From the vector, we consider its maximum score, denoted as {\sc L-PMI}. The label corresponding to {\sc L-PMI} is then assigned to the rule.  %Using the lattice, we can induce rules from the features, using LGG. The features and rules are then enumerated to form our rules set. 
From the lattice, we start bottom up and compute the LGG for every pair of rules. We induce the LGG as a rule, only if it has a higher {\sc L-PMI} than the individual rules in the pair. These induced rules form our candidate set of rules. For a given label $y_j$, the pointwise mutual information for the rule $r_t$ is given by,
$$\text{PMI}(y_j,r_t) = \frac{log |\mathcal{D}| \times \text{Count}(r_t,\mathcal{D}^{y_j})}{\text{Count}(r_t,\mathcal{D})*|\mathcal{D}^{y_j}|}$$

Here, $\text{Count}(a,b)$ implies the count of input documents having both $a$ and $b$. Similarly, $\mathcal{D}^{y_j}$ implies the set of  documents with the label $y_j$.   

%The score for a rule is obtained by multiplying the {\sc L-PMI} with the square  of the number of nodes in the feature. Here, we need to increase the importance of larger subtrees and hence this measure is used.
%From the lattice, we only consider those rules, if its  is higher thanfor the LGG is greater than that of the individual features in the generalisation.   

Text classification tasks need not always have single sentence inputs. We generally assume a feature may appear in any of the sentences in the input, unless these sentences clearly have a predefined role in the task. For instance, interchanging the premise and hypothesis in natural language inference \cite{berant-etal-2011-global,DAGAN_DOLAN_MAGNINI_ROTH_2010} generally leads to different outcomes. In such cases, we induce rules for premise sentences and hypothesis sentences separately. Further, the {\sc L-PMI} is applied a second time, this time for a pair of rules, one induced from the premise and the other from the hypothesis. The 2-step  {\sc L-PMI} approach enables to reduce the combinatorial explosion which otherwise may happen, and is trivially extendable to tasks with multiple roles.  

%In tasks that require information from multiple sentences together, such as use of a premise and hyptohesis sentence pair in natural language inference For text classification tasks that involve a sentence pair, such as natural language inference, we generalise rules from the premise and hypothesis sentences separately.   Similarly, 




%we compute its pointwise mutual information with each label,   which we refer to as {\sc label-PMI}. 


%Every feature is scored using the  between the feature and label, For each label, we obtain a sorted descending list of features that are scored using the pointwise mutual information between the corresponding label and the feature.

%  %Further to ensure minimal overlap with features that is discriminative for each category, we apply maximal marginal relevance, and select the features based on a threshold.
 



%If we consider a single surface form, it may be generalized as the stem; A stem may be further generalized as the POS tag. 
%Here,   can be considered a feature ($f_2$). If the corpus contains another feature $f_1$, `light $\xleftarrow{amod}$ fox $\xleftarrow{nsubj}$ jumped', they both may be generalized to the rule $r_1$, `light $\xleftarrow{amod}$ \{foxes, fox \} $\xleftarrow{nsubj}$' jumped' which is a linguistic generalisation of individual words to a word group by a union of the surface-forms `foxes' and `fox'. $r_2$ is representative of all structures with the stem `fox' and has dependency structure as `light $\xleftarrow{amod}$  fox (stem) $\xleftarrow{nsubj}$. Similarly, $r_3$ is more general than $r_2$, where the node with the stem, (that of fox), is generalized to `NOUN', the POS tag for a common noun. %To ensure generalization of $r_3$ to $r_2$ we ensure that $r_2$ considers only those features with the stem `fox', where `fox' is tagged as a common noun.

%Now $r_8$ is a generalization of $r_7$ the structural type.    Finally, rule $r_10$ represents a rule which is more general than all the rules illustrated in Figure \ref{genSamples}.


%The rules can be seen as a generalization of the features in a lattice. Here, the lattice consists of a set of features and rules and has a generalization relation, $\geq$.  We define the relation as follows. A tree can be generalized by having one or more vertices contain multiple inflected forms, implying the vertices may contain any of the nodes in the set. Rule $r_1$ is one such case. It can further be generalized to the stem of the inflected forms, wherever the set of forms have a common stem, like that in $r_2$. The inflected forms or a stem may be further generalized to a POS tag or a set of POS tags respectively, as shown in $r_3$ and $r_6$ respectively. Finally, any subtree of a tree is considered to be  more general than the original tree, and hence $r_7 \geq r_8$.







%Let us consider two features, $f_1$ and $f_3$. Here, they both generalize to $r_4$. $r_3$ is more general then $r_4$ as both `foxes' and `cat' are common nouns in their corresponding features.\footnote{For illustrative purposes, we are assuming the features to have similar structural properties to $f_1$, in terms of dependency relations and POS tags they carry, unless otherwise stated.} Instead, if we consider $f_4$, which has a proper-noun, then $r_6$ will be a generalization for $f_1,f_4,$ and $f_3$. Similarly, any subtree of a tree is a generalization of the latter.  Here, if we consider the features $f_1$ and $f_5$, then it can be generalized to $r_7$ and can further be generalized $r_8$. Moreover, $f_6$  has an $\epsilon$ in its tree representation. It implies, the feature represents all those sentences,  where there are no dependent nodes to the word  `foxes'. So a generalization of $f_1$ and $f_6$, both trivially forms to a union of both `light' and $\epsilon$. It further generalizes to $r_9$. Even though Figure \ref{genSamples} shows sample features with no edges, the subtrees we consider are labeled and a subtree of a tree should match these edge labels as well. 



%For LGG, we construct lattices over the features. Here, we construct lattices with a labeled dependency edge as the supremum  and then compute the LGG over it.

%   second feature  It can further be generalized to a subtree of size 2 which is the union of all features that have the edge `jumped $\rightarrow$ foxes', including those with no dependents. 

%Before applying LGG, we first




%For a set of sentences, forming a subset of the corpus, we may obtain the corresponding union features




%ur motivation to consider dependency structure is to incorporate complementary information which . While previous works have focused on n-grams and surface forms of the running text itself, 

% Here, Figure \ref{} shows various induced subgraphs of the dependency parse for the aforementioned sentence. Each such subtree can be considered a feature $f_i$ in the feature space $\mathcal{F}$. Any labeled subtree in the input dependency parses of upto $k$ nodes may form a feature in our case. While extracting the subtrees as features, we ensure the following constraints are met.  A feature should be a connected graph. Moreover, the universal dependency annotation scheme considers six relations as its list of core relations. These relations indicate subject ({\sc nsubj, csubj}), object ({\sc obj, iobj}), and clausal complements ({\sc ccomp,xcomp}). We ensure that all the features consist of a core relation as one of the edges. For all the input documents $x_i \in \mathcal{D}_{l}$ we obtain their sentence-level dependency parses. Then all possible features, i.e. higher order dependency subtrees up to a certain size are extracted.