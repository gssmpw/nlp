\section{Experiments}

\paragraph{Dataset:}  We use three datasets, namely, {\sc Discovery} \cite{sileo-etal-2019-mining}, {\sc CDR} \cite{davis2017comparative,wrench} and {\sc ANLI} \cite{nie-etal-2020-adversarial}, as shown in Table \ref{tab:dataset_specs},  for our full-shot setup. Here, we use the full training split, unless hit by an upper bound of 15,000 labeled instances, when fine-tuning the models. For ANLI, we focus on the R3 Dataset. CDR is a binary true/false classification problem for a given document with
mentions of chemicals and diseases tagged. Similarly, {\sc ANLI} is a 3-class, natural language inference task. {\sc Discovery} attempts at identifying the appropriate discourse marker from a set of 174 classes for a given pair of statements.  For few-shot, we use the {\sc FewMany} Benchmark \cite{yehudai2024llms}, consisting of eight multiclass classification datasets. Here, we only use the 5-shot labeled data points from the training splits of the datasets involved. %It consists of eight few-shot datasets, namely, {\sc CLINC150} \cite[C150;][]{larson-etal-2019-evaluation}, {\sc Banking77} \cite[B77;][]{casanueva-etal-2020-efficient}, {\sc HWU64} \cite[HU64;][]{liu2019benchmarking} for intent classification; {\sc Argument Topic} \cite[AT71;][]{gretz2020large} and {\sc Claim Stance} \cite[CS55;][]{bar-haim-etal-2017-stance} for Topic classification; {\sc Trec} question classification dataset \cite[T50;][]{li-roth-2002-learning}, {\sc Amazon Products} (AP106) and {\sc DBPedia} (DB70). %We also use SciCite \cite[SC3][]{cohan-etal-2019-structural}, from the scientific literature domain. %where we include hand crafted features into the automatic rule learning framework.  
Finally, the multilingual experiments are performed using the {\sc MASSIVE} dataset \cite{fitzgerald2023massive} with intent classification as the task. Here, we use seven typologically diverse languages including Chinese, English, French, German, Hindi, Japanese, and Spanish. 
\begin{table}[h]
\centering
\setlength\tabcolsep{5pt} 
\footnotesize
\begin{tabular}{lcccc}
\toprule
\textbf{Dataset} & \textbf{Train} & \textbf{Dev} & \textbf{Test} & \textbf{\# Labels} \\ \midrule
{\sc Discovery}        & 1,566,000        & 87,000        & 87,000         & 174                  \\ 
{\sc ANLI}             & 100,459         & 1,200         & 1,200          & 3                    \\ 
{\sc CDR}              & 8,430              & 920            & 4,673             & 2                    \\
{\sc MASSIVE}          & 11,514          & 2,033         & 2,974          & 60                   \\ \bottomrule
\end{tabular}
\caption{Dataset statistics of the original datasets. for ANLI and {\sc Discovery}, we perform class-wise stratified sampling and do not use more than 15,000 labeled instances from the training split for fine-tuning setups. }
\label{tab:dataset_specs}
\end{table}

% Please add the following required packages to your document preamble:
% \usepackage{multirow}




\paragraph{Data Generation:} We use GPT-3.5, GPT-4, and Claude 3 Opus for synthetic data generation. We generate label-specific data by prompt demonstration. Here, Using \citet{wu-etal-2023-openicl}, we perform $k$-NN retrieval from the seed data, with $k = min(n,150)$, where $n$ is the available data for a given label in the seed for positive demonstrations, and and equal amount of randomly sampled out of class samples as negative examples \cite{bertsch2024context,liu-etal-2022-makes}.  We use RoBERTa based sentence-embedding \cite{reimers-2019-sentence-bert} for sentence representation. For multilingual experiments, we experiment with \textit{direct} generation of the synthetic data in the target language, and also via \textit{translation} of synthetically generated English sentences. For translation, in addition to the three aforementioned LLMs we use NLLB-54B \cite{nllbteam2022language} and Google Translate. For translation in Hindi, we use \citet{gala2023indictrans}.

\subsection{Experimental Setup}

We incorporate the rules and exemplars from \our~in diverse text classification settings. 

\paragraph{In-context learning (ICL):} We experiment with three different configurations under long-context ICL using LLMs. One, is a \textbf{zero-shot} setup where we provide the input only with an explanation without any retrieved exemplars. Here, the explanations are obtained by phrasing the generated rules and their predictions as reasoning statements similar to the group of prompting techniques collectively referred to as thought generation prompting \cite{schulhoff2024prompt}. %However, these explanations are generated based on the rules and not opaquely by the LLM. 
Two is the \textbf{\textit{k}-shot} setup where we add prompt demonstrations from the generated data into the prompt. Third is the \textbf{\textit{k}-shot-XP} setup where we append explanations in addition to the prompt demonstrations. %The explanations contain reasoning for rules that are triggered for a given input. 
For demonstrations, we provide rules leading to both correct and wrong label predictions for it, similar to that in Contrastive CoT \cite{chia2023contrastive}. We reuse the retrieval setup used for data generation. 
% L412: “is” -> “are” - Pending
\paragraph{Fine-tuning (FT):}  We fine-tune an open-weight LLM \cite{Qwen2.5} and a smaller pre-trained LM \cite[PLM,][]{liu2019roberta, conneau-etal-2020-unsupervised}  in different configurations, under the \textbf{full-shot} setup. For fine-tuning the LLM, we employ PEFT using QLoRA. Our configurations for FT include; FT-base* where only the training data is used; FT-DA, where additional data from \our~are used; FT-J where only the rules corresponding to the training data are incorporated via Joint Learning using SPEAR \cite{maheshwari2021semi} and finally, FT-JDA, where both the data and rules from \our~is used. We also experiment with FT-JDX,  a variation where the rules are incorporated both as part of the input prompt and via Joint Learning.  

\paragraph{Joint Learning:} For incorporating the rules into our fine-tuning process, we follow SPEAR \cite{maheshwari2021semi}, a Joint Learning framework that learns over a feature-based classification model and a label aggregation (LA) model. The feature model is an LLM or a PLM and LA is a generative model \cite{cage}, learned via data programming, using the automatically induced rules as labeling functions. 


%The few-shot classifier is trained using  
%is jointly learnt by using the generative model, via PWS, and a pre-trained neural model.  is  the parameters of a (pre-trained neural model) and (PWS model) . %The feature-based classification model $f_\phi(\bfx)$ takes input features and predicts class labels, which we use a pre-trained model. 
%In our work, we utilize a large language model (LLM) trained on 1 trillion tokens using the Transformer architecture. This LLM comprises 7 billion trainable parameters and trained on huge amount of data in an unsupervised manner, enabling it to capture intricate linguistic patterns and improve classification performance.
LA is denoted as $P_{\theta}(\bfl_i, y)$, where $\bfl_i$ %is a vector of size of labeling functions (LFs) for an input $x_i$. Alternatively, $\bfl_i$ 
a vector that represents the firing of all LFs for an input $\bfx_i$. Each firing, $l_{ij}$ can be either 0 (abstain) or class label $k$ \cite{cage}.  %The model learns $K$ parameters $\theta_{j1}, \theta_{j2}, \ldots, \theta_{jK}$ for each class corresponding to each LF $l_{j}$.
%\begin{equation}\displaystyle P_{\theta}(\bfl_i, y) = \frac{1}{Z_\theta} \prod_{j=1}^m \psi_\theta(l_{ij}, y)\label{eq:joint1}\end{equation}\begin{equation}\psi_{\theta}(l_{ij},y) = \begin{cases}\exp(\theta_{jy})  & \text{if $l_{ij}\ne 0$} \\1 & \text{otherwise.}\end{cases}\label{eq:decoupledthetas}\end{equation}\iffalse\begin{equation}\begin{split}Z_\theta = & \sum_y \prod_j\sum_{l \in \{1, 0\}} \psi_\theta(l, y)
%\\        = & \sum_{y\in \Ycal}\prod_j(1+\exp(\theta_{jy}))\end{split}{}\end{equation}fi%\subsection{Joint Learning objective}
Our Joint Learning objective incorporates three different loss components for learning from labeled data. %However, we exclude the loss components that use unlabeled data. 
We provide a brief overview of each loss component below, while encouraging interested readers to \citet{maheshwari2022learning} for detailed information. The first component of the loss is the standard cross-entropy loss for the model $P_\phi^f$. The second component is the negative log-likelihood on the dataset. The third is the KL-Divergence between the predictions of the LA and $P_\phi^f$ models, which enforces consensus by aligning their predictions.
{\begin{align}\nonumber
\min_{\theta, \phi} &\sum_{i \in \Lcal} L_{CE}\left(P_\phi^f(y|\bfx_i), y_i\right)
 + LL_s(\theta| \Lcal)  
 \\ &+ \sum_{i \in \Lcal} KL\left( P_\phi^f(y|\bfx_i),P_\theta(y|\bfl_i)\right) \nonumber
\label{eq:objective}
\end{align}}
 


\paragraph{Base Models:} Our experiments are performed on one representative model for each of the following categories: (1) a closed-source LLM with access only via API, (2) an open-weight LLM, and (3) a pre-trained LM. Models like RoBERTa are still preferred in resource and latency conscious industry use cases, such as customer support, to larger models with few billion parameters. Factors include low latency, need for low hardware configuration and low cost, while still being competitive in several use cases. We choose GPT-4 Turbo \cite{openai2024gpt4technicalreport}, Qwen2.5-72B-Instruct \cite{Qwen2.5}, and RoBERTa-large \cite{liu2019roberta}, respectively, based on their category-wise performance on preliminary experiments using {\sc B77} and {\sc AP106} datasets. For multilingual setup, we employ XLM-RoBERTa \cite{conneau-etal-2020-unsupervised} based on observations from \cite{fitzgerald-etal-2022-massively}. Additionally, we utilize \citet[CPFT,][]{zhang-etal-2022-contrastive} for our contrastive learning \cite{pmlr-v119-chen20jsimclr} based baseline. 





%\paragraph{Baselines:} Table \ref{tab:baselines} shows our baselines. Base models are the `Large' variants of Roberta \cite{liu2019roberta} and XLM-R \cite{conneau-etal-2020-unsupervised} for our monolingual and multilingual experiments respectively. Further, Base-DA is fine-tuned with augmented data (no filtering). Base-ST is trained using self-training-based filtering of augmented data. We also include competitive models that also combine multiple learning techniques, such as IntenDD \cite{singhal-etal-2023-intendd}, Snorkel \cite{DBLP:journals/corr/abs-1711-10160ratner}, , and FastFit \cite{yehudai2024llms}. Following \citet{yehudai2024llms}, we report results for ICL, in 5-shot setups, using Flan-XXL \cite{wei2021fine-tunedflanxxl}, Flan-UL2 \cite{tay2022ul2}. 
 



%We perform few-shot prompting using.  %For data augmentation for IntenDD, setfit and FastFit we use the information theoreti data augmentation strategy proposed in \citet{lin-etal-2023-selective}.


 %\our~and \our-Iter, as shown in Table \ref{tab:baselines}, are two variants without and with the iterative data and rule filtering (IDRF). \our~variants use  the same pre-trained models as used in `Base'. 
% trained with XLM-R as the encoder on the UD treebank \cite{11234/1-5150ud} for dependency parsing
Accuracy is our primary evaluation metric. We use \citet{dozat2016deep}, a dependency parser,  to extract syntactic n-grams from input. We obtain induced subtrees of up to 3 nodes as rules. For few-shot, we perform all our experiments using 5 random splits and report the average \cite{yehudai2024llms}. For Joint Learning, we use 20\% of the synthetically generated data as the validation split, while using all the gold data as the training data. For learning the parameters for our rule filtering step (\S \ref{ruleFiltering}), we use the few-shot gold data as validation. For full-shot setups we use the standard train-validation-test splits.\footnote{For the prompt and hyperparameter details, refer: https://sites.google.com/view/ariserules/}  %We report results for \our-iter induced with rules where the gold data was used only in the last iteration of bootstrapping. We keep a multiplier of 128x for our k-shot classification settings, following \citet{lin-etal-2023-selective}. %As shown in Table \ref{tab:baselines}, all the models that incorporate rules, use our rule and data filtering approaches. Those marked with a \checkmark~under PVI use the information theoretic data filtering followed in \citet{lin-etal-2023-selective}. Those marked with a \checkmark~under IDRF use the iterative version of our data and rule filtering. 


%However, the parameters were learned in previous experiments.use the  for rule induction only in the last iteration of bootstrapping. 

%Table \ref{tab:baselines} shows the various learning techniques Incorporated in each of these baselines. 


 %randomly sampled from the set of augmented data. \textbf{Self-training}: Base classifier trained with augmented data, filtered only if a self training classifier labels as the same as that of the augmented label in some iteration of self-training. \textbf{setfit}: Base classifier, with contrastive learning and augmented sentences. \textbf{fastfit}: slef supervised contrastive learning with nearest neighbour and token levle simialrity. \textbf{intendd}: self supervised el;arning but no continuted pretraining. corpus specific pmi, ngrams. \textbf{intendd-LP}: additional 2 layer LP. \textbf{snorkel-ngrams}: labeling funcrions as filter for data ugmentation.\textbf{snorkel-trees}: Labeling functions as filter for data ugmentation. \textbf{ARISE}: Labeling  functions as filter for data augmentation with trees. \textbf{ARISE with data filter}. \textbf{LLM with ICL}.



%We additionally use fastfit, setfit and xxxx for our additional experiments.  The baseline configurations are as follows. \textbf{Base classifier}: The pre-trained model (RoBeRTA or XLM-R) which is simply trained with the few shot labelled data. 
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage{graphicx}
\begin{table}[h]
\centering
\begin{tabular}{cllll}
\toprule
\multicolumn{2}{c}{Model Configuration}                                                                           & {\sc CDR}            & {\sc ANLI}           & {\sc DISC.}           \\
\midrule
\multicolumn{1}{c}{\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}GPT 4\\ (ICL)\end{tabular}}} & zero-shot & 85.89          & 79.53          & 2.34           \\  
\multicolumn{1}{c}{}                                                                     & k-shot    & 88.95          & 81.59          & 31.10         \\  
\multicolumn{1}{c}{}                                                                     & k-shot-XP & \textbf{92.13} & {\ul 86.78}    & 35.44          \\ \midrule
\multicolumn{1}{c}{\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Qwen\\ (ICL)\end{tabular}}}  & zero-shot & 82.86          & 71.27          & 8.59           \\  
\multicolumn{1}{c}{}                                                                     & k-shot    & 84.24          & 73.19          & 39.70           \\  
\multicolumn{1}{c}{}                                                                     & k-shot-XP & 86.84          & 84.05          & 47.36          \\ \midrule
\multicolumn{1}{c}{\multirow{6}{*}{\begin{tabular}[c]{@{}c@{}}Qwen\\ (FT)\end{tabular}}}   & FT-base*  & 82.05          & 58.20          & 92.29          \\  
\multicolumn{1}{c}{}                                                                     & FT-J      & 85.27          & 63.08          & 92.70           \\  
\multicolumn{1}{c}{}                                                                     & FT-JXP     & 85.63          & 85.19          & 92.40           \\  
\multicolumn{1}{c}{}                                                                     & FT-DA     & 87.76          & 75.69         & 95.33 \\  
\multicolumn{1}{c}{}                                                                     & FT-JDA    & {\ul 90.16}    & 78.30           & {\ul 95.72} \\  
\multicolumn{1}{c}{}                                                                     & FT-JDX   & 90.08    & \textbf{88.37} & \textbf{95.81} \\ \midrule
\multicolumn{1}{c}{\multirow{6}{*}{\begin{tabular}[c]{@{}c@{}}PLM\\ (FT)\end{tabular}}}    & FT-base*  & 81.78          & 53.82          & 90.60           \\  
\multicolumn{1}{c}{}                                                                     & FT-J      & 84.72          & 57.78          & 90.88          \\  
\multicolumn{1}{c}{}                                                                     & FT-JXP     & 84.58          & 57.85          & 90.67          \\ 
\multicolumn{1}{c}{}                                                                     & FT-DA     & 86.94          & 62.35          & 93.02   \\ 
\multicolumn{1}{c}{}                                                                     & FT-JDA     & 86.61          & 62.87          & 93.26    \\ 
\multicolumn{1}{c}{}                                                                     & FT-JDX    & 86.74          & 62.95          & 93.43   \\ \bottomrule
\end{tabular}
\caption{Results in ICL and FT setups. Numbers in \textbf{boldface} and { \ul underline} represent best and the second-best configurations, respectively. Here, PLM refers to RoBERTa-large. FT-base* is the only configuration that does not incorporate \our.}
\label{tab:full-shot-results}
\end{table}

\section{Results}

\begin{table}[]
\centering
\begin{tabular}{clll}
\toprule
Dataset & Configuration & Vanilla & ARISE \\
\midrule
\multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}CDR\end{tabular}} & zero-shot & 74.56          & \textbf{85.89} \\  
 &k-shot    & 83.43           & \textbf{88.95}         \\  
  &k-shot + Aug. & 83.56 &  \textbf{88.95}     \\
 &k-shot-XP + Aug. & 89.35 & \textbf{92.13}      \\ \midrule
\multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}DISC.\end{tabular}} & zero-shot & 0.84          & \textbf{2.34}           \\  
 &k-shot    & 8.73           & \textbf{31.10}         \\  
  &k-shot + Aug. & 26.97 & \textbf{31.10}     \\
 &k-shot-XP + Aug. & 32.11 & \textbf{35.44}      \\ \bottomrule
\end{tabular}
\caption{ICL Experiments in GPT-4 that compares both ARISE, 
and ARISE-less scenarios under comparable conditions}
\label{tab:vanilla}
\end{table}

Use of data and rules from \our, results in statistically significant  gains for all the datasets, both under full-shot and few-shot setups, including multilingual few-shot scenario. In full-shot setup, we outperform SotA models for both CDR, Discovery and ANLI, with more than an 8\% \cite{zhao-etal-2024-pareto}, 7\%\footnote{Not a comparable model} \citep[MTL;][]{sileo-etal-2019-mining} and 18\% \cite{kavumba-etal-2023-prompting} increase, respectively. 


%Similarly, our model outperforms the available SotA model \ for the Discovery dataset with absolute, though   No comparable SotA models, trained on Discovery training data, were available, though oincrease in performance. 

Table \ref{tab:full-shot-results} shows the results for various configurations where data and rules from \our~are used. FT-base* is the only configuration where no information from \our~is used. FT-base* however is fine-tuned on the available training splits of the corresponding datasets. Qwen FT-JDX, the configuration that uses Joint Learning with rules, rules as explanations and augmented data reports the best results for ANLI and Discovery with an absolute gain of more than 30\% and 3\%, respectively. For Discovery, gains from rules are not significant, as Qwen FT-DA achieves comparable results to Qwen FT-JDX. Similarly,  GPT 4 k-shot-XP achieves the best results for CDR, outperforming even the fine-tuned version with an absolute gain of roughly 10\% as compared to Qwen FT-base*. Qwen FT-JDA and FT-JDX, both using Joint Learning with rules is the second best model. For CDR and ANLI, using both additional data and rules lead to statistically significant gains.\footnote{Statistical significance is performed by t-test ($p < 0.05$)}

%In full-shot setup, our best performing configuration report a  percentage increase of 3.3\% for Discovery, compared to its fully fine-tuned variant. Simialrly, we outperform state of the art models for ANLI (R3) and CDR with ANLI (R3) more than doubles its performance and For all of the three full-shot datasets with which we experimented, we find gains in using both the generated data and the rules. Further, we show three different configurations under which our data can and rules can be leveraged to obtain best performances for the respective datasets. While these lead to our p

 Qwen models benefit from Joint Learning for CDR and ANLI, even after when their results saturate with additional data (FT-JDA). Here, both additional data and rules have shown to benefit the models and bring in complementary supervision signals. However, Joint Learning does not lead to statistically significant gains, once fine-tuning is performed with additional data for RoBERTa (FT-DA vs. FT-JDA for RoBERTa) 

%Qwen and RoBERTa show different trends when it comes to adding rules during fine-tuning. . For CDR and ANLI, though not for Discovery, we find gains in using Joint Learning when trained with only the available training data. However, these gains are not observed when additional data is supplemented. Adding data leads to statistically significant gains for all the datasets, compared to using Joint Learning with rules and only the available training dataset. However, on the contrary,Interestingly, ANLI is the only dataset where use of rules for both Joint Learning and as explanations in the input leads to best performance gain. 


Within ICL, GPT 4 outperforms Qwen in both CDR and ANLI, but Qwen outperforms GPT 4 in Discovery. Discovery has a large label space of 174 labels, and these are common terms which are typically used as markers between two statements. Qwen being an open-weight model, we were able to constrain the output space using constrained decoding.  
While there exist similar approaches with structured output generation in GPT 4,  we have limited control with GPT 4 compared to an open-weight model with constrained decoding.

Table \ref{tab:vanilla} compares ICL results with both \our, and \our-less scenarios under comparable conditions for both CDR and {\sc Discovery} datasets. We observe gains with \our, for all the casess we compared. Here, \textit{Vanilla zero-shot} does not use rules as explanations for the input from  \our, whereas \textit{Vanilla k-shot} retrieve examples only from the training split and uses no augmented data at all. \textit{Vanilla k-shot + Aug.} uses data augmentation proposed by \citet{lin-etal-2023-selective}. Finally, \textit{k-shot-XP + Aug.} uses the above augmentation setting, but adds explanation from \our. We find consistent performance improvements in all the configurations when \our~components are increasingly used. %Further, the prompts used are consistent across the configurations apart from the input data-dependent few-shot examples and explanations. 

%there are limitations to logit manipulation and logit manipulation led to higher improvements in Qwen.Nevertheless, both the models perform at half the performance compared to the fine-tuned counterparts. Adding rules clearly helped Discovery improve performance for both the models but have limited impact compared to fine-tuning approaches. 
%Further, we perform Joint Learning along with the induced rules for both the fine-tuning settings, using LLM and Roberta.  We find that 


% Please add the following required packages to your document preamble:
% \usepackage{graphicx}

%\our, our proposed approach, leads to performance improvements for all configurations, whether through ICL or through fine-tuning. reports the best performance in all our experimental settings, % monolingual 5 and 10-shot settings, and multilingual settings as shown in Tables \ref{tab:mainResults}, \ref{tab:fiveShot}, and \ref{tab:multilingual}. It outperforms all other models with statistically significant gains. \our-Iter reports an absolute improvement of 1.79\% points (2.04\% increase),  1.32\% and 2.58\% points averaged across the datasets, for the 5 and 10-shot monolingual and 10-shot multilingual setups.


%We find performance gains in various datasets using both the generated data and rules. Table \ref{} shows our overall results for full-shot setting under various configurations. 

\subsection{Impact of Rules}
\label{ruleImpact}
We previously claimed that we obtain complementary supervision signals with rules compared to a setup like FT-base*. We validate the claim and %, in both full-shot fine-tuning and ICL setups.  Under full-shot settings, 
observe statistically significant gains from the rules in ICL for all the three datasets, and in two of three datasets except for Discovery in fine-tuning. 



%incorporate our rules in various ways into our classification tasks. We include them as explanations in ICL, for Joint Learning using spear during fine-tuning, and as both explanations in the input and for Joint Learning.

\paragraph{Rules used for Joint Learning:} FT-J and FT-JXP are two settings, in Table \ref{tab:full-shot-results}, which are trained jointly using the rules from \our, but only with the original training split. For CDR, FT-J results in a percentage increase of 3.92 and 3.59 for both Qwen and RoBERTa, respectively, compared FT-base*. Similarly for ANLI, we observe a percentage increase of 8.38 and 7.35, respectively, for Qwen and RoBERTa respectively. Moreover, ANLI reports statistically significant gains with Qwen for FT-JXP, XP implying rules used also as explanations, compared to FT-J. We did not see any additional gains with RoBERTa when adding rules as explanations. We hypothesize this is due to lack of instruction tuning. 

For other datasets, FT-JXP configuration does not lead to statistically significant gains. For Discovery, the gains in absolute terms were not statistically significant for both Qwen and RoBERTa. However, there was no performance degradation for this dataset. Summarily, we find overall gains in using Joint Learning while fine-tuning task-specific models.  Qwen and RoBERTa both show similar trends and comparable gains with Joint Learning as compared to simple fine-tuning.

\paragraph{Rules used as explanations:}  We use our rules, along with their label predictions, as an explanation for the input. k-shot-XP, uses a subset of exemplars comapred to k-shot, irrespective of the source pool from which it is retrieved. In spite of having lesser number of exemplars, adding contrastive explanations leads to  further gains in our experiments for both Qwen and GPT 4. Further, use of explanations in k-shot settings with GPT 4 led to the highest performance for CDR among configurations. Similarly, we report the second best performance for ANLI using GPT 4, which has a percentage increase of more than 16 from the previous SotA. Previous SotA was a fine-tuned model, with 1/3rd of total training data, while our configuration under discussion is purely under ICL.

%alone under zero-shot and also for the k-shot demonstrations from the training data. Adding an explanation alone under zero-shot in itself outperforms SotA models for both CDR and ANLI. While retrieving k-shot exemplars from the training data as prompt demonstration alone leads to statistically significant gains compared to zero-shot, adding contrastive explanations further leads to  , and reports a percentage gain of more than 8 from the previous SotA.  



 
 

%We hypothesise that large label space along with presence of these words in common English usage are probable reasons for the low performance in ICL. For Discover, we perform two additional analyses, which includes the following. We observe that Qwen performs better than GPT 4 in ICL cases, because we constrain our decoder in Qwen with label specific terms. Further, Qwen in Joint Learning uses a classification head while constrained decoding is employed in ICL. Our fine-tuning approach outperforms ICL in this case. 
%both input samples and retrieved . For k-shot samples, we provide the explanations so that it contains both positive and negative cases for each of the retrieved samples.   Further, CDR reports  However, for CDR and ANLI, we find that our ICL versions outperform fine-tuned versions. Further, increasing the number of exemplars with explanations leads to consistent performance gains in all the datasets. In Discovery, we find gains until our context window size is filled. For ANLI and CDR, we find that there are no significant gains after 128 samples with explanations. For CDR, ICL with expplanation for the input alone led to performance improvements in GPT 4, more than the fine-tuned version. However, Qwen required explanations for 64 samples to achieve similar scores. 









%Table \ref{} shows the impact of the rules we obtained from \our. We observe gains for the three datasets both in zero-shot and k-shot setups for ICL. For all the datasets, an explanation prompt for the input, like in zero-shot, along with contrastive few shot demonstrations and explanations lead to the highest gains in ICL setting. Summarily, using the rules alone we obtained using \our~leads to statistically significant gains in ICL. 


%We find that adding rules along with the data leads to even further improvements in all the cases.  Rules as a substitute for more data? Role of rules in Joint Learning. Role of rules in ICL. Further, we find that use of rules in ICL for discovery shows the highest gain among all cases, although adding rules during fine-tuning seems to have limited impact for the same dataset. Further, we find that the Joint Learning framework helps Roberta more than the LLMs. Further, Joint Learning of rules and data shows highest gains in Discovery for fine-tuning. Simialrly, the other datasets show limited gains with fine-tuning, though adding rules in ICL helps them with improvements. Rules as explanation probably? 



\subsection{Impact of Generated Data} 

We find that using generated data, both for training and for ICL, leads to statistically significant gains in all configurations for the three datasets. In our experiments, we generate synthetic data in multiples of the original training data size. We generate data from 1$\times$ to 6$\times$ of the original data. 


\paragraph{Fine-tuning:} For all the three datasets, adding additional data beyond the full training data  during fine-tuning leads to significant gains. For both discovery and CDR, we observe gains until 1.5$\times$ times more data is added to the training data. For ANLI, we observe gains by doubling the training data size, i.e. 1$\times$ the training data. We observe that the training saturates after when more data is being added to these datasets, until we tried with 3$\times$ more data. Our observations hold true for both Qwen and RoBERTA, where tried the fine-tuning. %As shown in Figure \ref{}, the initial size of the seed, depending on the availability of gold data, leads to differences in performance. In ANLI, a limited seed size, of 15\% and 20\% instances per class, led to performance degradation. In all these settings, we still train the models with full-training data. Discovery dataset achieves the best performance, almost double that of the best performing ICL model using GPT 4, when fine-tuning the model with additional data. It reports a performance increase of 3.29 with 1.5$x$ data. For all the datasets, fine-tuning these models with just the augmented data does not lead to similar perofrmances. Further, starting the training with original training data is crucial for the performance improvement in all these datasets, with least difference being observed in ANLI. 



\paragraph{ICL:} There are more  exemplars than that can fit into the 128K context windows for all the three datasets. However, increasing the pool of available data leads to improved outcomes in ICL, with the help of retrieval. For ANLI and CDR, we consistently had less than 40\% presence of instances from the original training data for cases with 1$\times$ and above. With Discovery, we observe similar patterns but only after  1.5$\times$. We stop our experiments with 3$\times$ data as the overlap between retrieved ICL exemplars was more than 90\%  by then.  



%Both CDR and ANLI datasets show gains when using the augmented data in ICL while Discovery shows highest gain in using the data when fine-tuning.  Impact as dataset size increases takeaways: More data lead to improvements. Generated data is better than random sampling of similar size. Generated data is better than simply retrieving (in ICL). It outperforms the SoTA in respective category.





%9 datasets, from the best performing non-\our~model for the 10-shot setup. Similarly, \our-iter reports an absolute improvement of, for the 5-shot monolingual and 10-shot multilingual setups.   


%For the 5-shot setup, an points, averaged across the same datasets (Table \ref{tab:fiveShot}) is reported. Finally, the absolute improvements averaged across seven languages in the multilingual setup are  is 3.39 percentage points. 

%Arise-iter reports an average increase between 1.75\% and 2\%, in 10-shot settings, compared to other strong baselines. Arise-Iter benefits from a combination of data augmentation, contrastive learning, data subset selection, iterative automatic rule induction, and syntactic rules. 
\subsection{Few-shot Setup}

Few-shot learning setups are particularly valuable in industry applications involving text classification tasks with a large number of classes ($>50$). The high annotation cost and resource demands in such settings can be mitigated by adopting few-shot strategies. Previous research has explored contrastive learning methods \cite{zhang-etal-2022-contrastive} for learning better semantic space for input representation, weak supervision techniques such as the joint learning framework proposed by \citet{maheshwari2021semi}, and synthetic data generation strategies \cite{lin-etal-2023-selective}. In this work, we evaluate the effectiveness of \our's data augmentation method in comparison to the other two approaches. Specifically, we examine the data augmentation thresholds at which the benefits of competing techniques become statistically insignificant.


% Few-shot setups are desirable for several industry applications, especially for those with a large number of classes ($>50$) in text-classification. Here, the annotation efforts can often be time-consuming and resource-heavy, unless performed in a few-shot setup. Prior work has relied on contrastive learning \cite{zhang-etal-2022-contrastive} to learn better input representations with limited data, or weak supervision approaches such as the joint learning method proposed by \citet{maheshwari2021semi}, or use synthetic data generation \cite{lin-etal-2023-selective}. Here, we compare the impact of using \our's data augmentation approach compared to the other two approaches. More specifically, we look for at what multiple of augmented data per class, these approaches become insiginificant.  

We only use 5-shot gold data for each class, and augment data from 1$\times$ to 256$\times$ in multiples of 4 \cite{lin-etal-2023-selective}. %Here, 1$\times$ augmentation implies 10-shot data that has 5-shot gold data and 5-shot augmented data per class. 
For all datasets we find statistically significant gains to Joint Learning until 32$\times$ augmented data is used. However, with more supplementary data at 64$\times$ and beyond, we do not find statistical significance between models that use Joint Learning compared to the one not using Joint Learning.  The only exception in the benchmark here is Amazon products, for which we find statistically significant gains to Joint Learning even with 200$\times$ data, but the gains disappears at 256$\times$ data. Similarly gains from contrastive learning in our CPFT baseline starts to disappear with 25-shot data (4$\times$ augmented) itself for all the datasets. 

Moreover, we observe that fine-tuned variants of RoBERTa and Qwen models report comparable performances and do not have any statistical significance between their results. RoBERTa and Qwen report an average accuracy of 94.18 and 95.04 respectively. Here, the only dataset with a difference in statistical significance between these two are Amazon Products. 62.07\% and 67.84\% respectively are the accuracy for RoBERTa and Qwen for Amazon Products. While Qwen-ICL performs worse than Qwen-FT with an average accuracy of 90.38\%, GPT 4 reports scores similar to the fine-tuned variants 95.46\%. 


%We observe saturation in results as our augmented dataset size increases to , except for Amazon products.  For Amazon products, we find that we still observe statistically significant gains even at $200x$ data with Joint Learning. However, the dataset All datasets, except Amazon Products and DBPedia by then exhausted their available training data. However, for all the datasets, except for Amazon Products, model that just uses the augmented data from 


%We use the {\sc FewMany Benchmark} \cite{yehudai2024llms} for our few-shot experiments. Use of contrastive learning approaches are quite common in few-shot multi-class text classification setups, which the benchmark is representative of. For each of the dataset we start with 5-shot and then slowly increase the number of shots in multiples of 4 until 256$x$ \cite{lin-etal-2023-selective}. We observe that our contrastive learning based     


%\our-Iter and \our~differs only in terms of bootstrapping (IDRF). %the use of iterative rule and data filtering (IDRF). In \our, both rule induction and data augmentation happen at once, though all the models use the same amount of training data. Bootstrapping alone leads to an average absolute gain of 1.12 and 1.32\% points for the 10-shot and 5-shot setups respectively (Tables \ref{tab:mainResults} and \ref{tab:fiveShot}), between both the \our-Iter and \our~respectively. %AP106 reports the highest improvement  in the 10-shot setup whereas HU-64 reports the hightest improvement in the 5-shot setup. %Base is simply the Roberta-Large model fine-tuned with the labeled few-shot data. Base-Aug includes synthetically generated sentences, randomly sampled from the pool of augmented sentences.  Base-Aug reports statistically significant gains only for 3 of 9 datasets (B77, HU64, and DB70) compared to Base in Table \ref{tab:mainResults}. It shows that data augmentation without any filtering need not always improve the results. Further, Base-ST %As shown in , , as there is performance drop for  in 3 of the 9 datasets in spite of the increased training data size. Further, statistically significant gains are reported only for  datasets. %performs filtering of augmented data using a self-training classifier, where an augmented sentence is chosen only when the LLM's prediction and the ST Classifier's prediction are in agreement. The ST classifier, on average report a gain of 0.85\% points compared to Base, with statistically significant gains in 6 of 9 datasets (except for AT71, C150, and CS55). %are three datasets where the results are not statistically significant. 

%\our~variants follow CPFT \cite{zhang-etal-2022-contrastive} in employing contrastive learning (CL) components.   %is a contrastively pre-trained and fine-tuned variant of the Base, which we also incorporate in \our. CL components alone in CPFT lead to an average absolute gain of 1.8\% points compared to Base, in Table \ref{tab:mainResults}. Similarly, Snorkel, a PWS framework, and \our~is trained with the same filtered data and rules. However, unlike \our, Snorkel does not use Joint Learning. Instead, Snorkel learns a generative model to label (or filter in our case) synthetically generated sentences. It outperforms the base model by an average absolute improvement of 1.76\% points and is competitive with CPFT. Snorkel and CPFT report statistically significant gains compared to Base for all the datasets, except CS55.  Snorkel and CPFT report comparable performance on 5 of 9 datasets, with statistically significant gains in 2 datasets each. %Snorkel's performance shows the effectiveness of our rule induction framework, as a general purpose framework for PWS. 

%CPFT+Snorkel combines both contrastive representation learning and PWS. It differs from \our, only in terms of the Joint Learning component. \our~reports an absolute improvement of 0.67\% points in 10-shot settings (Table \ref{tab:mainResults}), and 0.96\% points  in 5-shot settings (Table \ref{tab:fiveShot}), as compared to CPFT+Snorkel. %, with statistically significant gains in 4 datasets. %Further, \our~reports statistically significant gains to CPFT+Snorkel in four of the five datasets. Results from Snorkel, CPFT+Snorkel, and \our~show our rule induction component, as a general-purpose one for PWS. Similarly, gains in CPFT+Snorkel and \our~show that combining complementary learning techniques leads to performance gains compared to using them independently.  


%\our-Iter, our proposed approach with IRDF, %reports statistically significant gains compared to all the other models including \our. It outperforms both IntenDD \cite{singhal-etal-2023-intendd} and FastFit \cite{yehudai2024llms}, two competitive models with state-of-the-art results on few-shot learning. While FastFit originally does not use data augmentation, we add augmented sentences to it for a fair comparison. IntenDD differs from \our~by using string-level n-grams for weak supervision and additionally employs a two-level transductive learning approach. \our-Iter when trained with string level n-grams as used in IntenDD still outperforms IntenDD but reports an average accuracy of 88.64\%, a drop from 89.43 for the 10-shot setting. Similarly, the use of PVI for data filtering instead of IRDF for \our-Iter results in an average accuracy of 88.19\%.




%Moreover, both CPFT and Snorkel (with our induced rules) has shown to outperform two state of the art few-shot text classification models FastFit and IntenDD. Both the SoTA models use contrastive learning as part of their training pipeline. IntenDD, follows CPFT, but aslo incorporates unlabeled data via pseudo labeling of the unlabeled datasets using string level ngrams. Here, since, there are no nulabeled datasets, we used the ngrams to filter synthetically generated data. They use Louvain based dataset partitioning approach instead of our rule induction framework. Overall, we find that IntenDD performs worse than CPFT, by 0.15 percentage points. Specifically, there are two datasets DB70 and T50 where CPFT outperforms  IntenDD. Further, IntenDD-LP is the best performing variant in IntenDD, where they use trasnductive learning approach to improve on the final performance.  CPFT+Snorkel use both contrastive pretraining and PWS. It differs from \our, only in the Joint Learning setup. As it can be observed  Snorkel outperforms all other baselines (except \our) in the 5-shot setup and  reports comparative results to SetFit for the 10-shot setup. Snorkel's results show that our rule induction metho alone is powerful and can match other state of the art baselines. 


%The joint setup in \our~alone leads to more than 1\% absolute improvement in both the 5-shot and 10-shot setups compared to Snorkel. It highlights the benefit of the Joint Learning, instead of simply generating weakly labeled data to use in training. 

%IntenDD 

%Now, ARISE and Snorkel differs only in data programming frameworks they use. ARISE uses SPEAR, a framework enabling Joint Learning while SNORKEL is a data programming framework that helps obtain weakly labeled examples. There is more than a 1\% absolute improvement in ARISE as compared to Snorkel, highlighting the  Both the ARISE variants and Snorkel use automatically induced rules as weak labeling functions and these are rules are generalised syntactic ngrams. IntenDD is a model that also use ngrams based on PMI based filtering in their learning framework. The ngrams are used for corpus specific pretraining, followed by network construction for the semi supervised label propagation approach they use. ARISE variants outperform IntenDD on all the experimental settings in both 5-shot and 10-shot. Snorkel performs better than IntenDD on x of the y tasks, on an average increase of zz absolute percentage points.  


%Both FastFit and Setfit have shown to perform competitively in our experimental settings. Here, we use augmented sentences that has passed information theoretic filtering, and we use the best configuration from the augmented subset. All the three systems that use automatically induced generalized syntactic ngrams report statistically significant gains to that of both models. Among FastFit, Setfit and IntenDD, FastFit performs the best on an average. However, IntenDD outperforms others on one of the tasks and SetFit outperforms the otehr two on two tasks. Contrary to what FastFit reports, we observe that setfir models arecompetitive with fastfit, probably due to the impact of augmented data. 


%Table \ref{tab:fiveShot} reports results for the 5-shot setup. We follow the setup of \citet{yehudai2024llms} for ICL. \our-Iter~reports an average absolute gain of 16.01\% and 2.28\% compared to Flan-XXL and CPFT+Snorkel models respectively. It also reports statistically significant gains, compared to both, for all the datasets except AT71.  Overall, Flan-XXL and Flan-UL2 outperform other LLMs  \cite{touvron2023llama, jiang2023mistral} in our ICL experiments and hence reported in Table \ref{tab:fiveShot}. 



%Interestingly, adding more augmented inputs, though lead to increases for a Mistral model often double than that of not having augmented sentences. These models are still not outperforming any of the ARISE variants, highlighting the significance of automatically induced rules. 


%\subsection{Data Augmentation}

%Data augmentation has been extensively used NLP for various text classification tasks. Data augmentation are often performed at lexical, phrase-level or syntactic levels transformations to a given sentences, or obtaining new sentences via translation or are generated with the help of LLMs. Of At large we perform augmentation for classification, where we assume that the transformation we provide are label preserving. Of recently, filtering approaches that enable scoring or filtering augmented sentences have been proposed. We combine data augmentation and automated rule induction in an interactive manner. Here, automated rules are induced based on the graph cut based filtering. We compare the data augmentation strategies and the subsequent filtering approaches using ARISE as the base model.  We find that our approaches 

\paragraph{Multilingual Experiments:} % Table \ref{tab:multilingual} shows the results for multilingual experiments. %We find that \our-Iter reports statistically significant improvements over all the other models for all the languages. 
On an average \our~reports an absolute improvement of 7.21\% points compared to the base model, on the 5-shot gold and $128x$ augemented data per class. The results show that our approach is applicable across a typologically diverse set of languages. We find \textit{translation} of synthetically generated English sentences leads to empirically better results as compared to \textit{direct} generation of data in the target language. %The results for the former are reported in Table \ref{tab:multilingual}.  
The latter approach results in an absolute  drop of 1.27\% points. %Further, we also experiment with a setting where we induct rules from dependency parses of all the translations of an input. Here, we observe a performance drop for all the languages, except Hindi. On average there is 0.76\% drop for \our-Iter compared to the default setting as reported in Table \ref{tab:multilingual}. For Hindi, it reported 78.62\% as compared to 77.41\% in the default setting. 
Moreover, GPT 4, under ICL reports an average of 84.15\% accuracy as compared to the 80.4\% accuracy reported by the RoBERTa Model. 

\section{Conclusion}

We propose \our, a framework that iteratively generates and refines both synthetic data and rules. Overall we find gains in using our rules and data in both ICL and FT for more than 15 datasets we consider. Further, \our~outperforms strong competitive baselines under comparable conditions. We also show the effectiveness of combining diverse sources of supervision that enable incorporating complementary and supplementary information beyond the available gold data to achieve SotA results.


%Further, we find incorporating syntactic information, instead of strings, via rules leads to gains.  


%For the multilingual settings, we perform data augmentation in two ways. One, by generating synthetic data directly in the target language. Two, by automated translation of the synthetic data generated in English to the target language. We find the latter approach results in  for the dataset, and the results are  The multilingual setup has led to increase in diversity of rules. However, 

%We also experimented with 


%The results reported in  use the translation based data augmentation, where the language specific synthetic data is obtained by automated tra

%inputs obtained via translation of synthetically generated English sentences. 


%find results consistent with our monolingual English experiments in our multilingual experiments using MASSIVE as well. Here, both ARISE and ARISE-iter, outperforms all other models with statistically significant gains. Moreover, ARISE-Iter on an average reports a xx\% gain in absolute percentage points as compared to other models. Moreover, the generalized syntactic ngrams that we propose have shown to beneficial for all the languages we experimented with. All the three models that use the syntactic ngrams have shown significant gains as compared to others. IntenDD, the model with surface-form ngrams, while have shown to be beneficial for languages like English and German, we find loss of performance for languages like Hindi. 

