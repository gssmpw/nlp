\section{Experiments}
\begin{table*}[]
\centering
\begin{tabular}{|l|c|c|c|ccc|cc|c|c|}
\hline
\multirow{2}{*}{} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}FT\end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}CL\end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}DA\end{tabular}} & \multicolumn{3}{c|}{\begin{tabular}[c]{@{}c@{}}Filtering\\for DA\end{tabular}}                                                                    & \multicolumn{2}{c|}{\begin{tabular}[c]{@{}c@{}}Automatic Rule \\ Induction\end{tabular}} & \multirow{2}{*}{IDRF} & \multirow{2}{*}{ICL} \\ \cline{5-9}
                  &                                                                         &                                                                                 &                                                                              & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}PVI\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}ST\end{tabular}} & GC & \multicolumn{1}{c|}{ngrams} & \begin{tabular}[c]{@{}c@{}}syntactic\\ ngrams\end{tabular} &                                &                      \\ \hline
Base              & \checkmark                                                                       &                                                                                &                                                                             & \multicolumn{3}{c|}{}                                                                                                                                            & \multicolumn{2}{c|}{}                                                                   &                               &                     \\ \hline
Base-DA        & \checkmark                                                                        &                                                                                & \checkmark                                                                             & \multicolumn{3}{c|}{}                                                                                                                                            & \multicolumn{2}{c|}{}                                                                   &                               &                     \\ \hline
Base-ST   & \checkmark                                                                        &                                                                                & \checkmark                                                                             & \multicolumn{1}{c|}{}                                                        & \multicolumn{1}{c|}{\checkmark }                                                       &  & \multicolumn{2}{c|}{}                                                                   &                               &                     \\ \hline
IntenDD           & \checkmark                                                                        & \checkmark                                                                                & \checkmark                                                                             & \multicolumn{1}{c|}{}                                                        & \multicolumn{1}{c|}{}                                                       & \checkmark   & \multicolumn{1}{c|}{\checkmark }      & \multicolumn{1}{l|}{}                                     &                               &                     \\ \hline
Snorkel           & \checkmark                                                                        &                                                                                & \checkmark                                                                             & \multicolumn{1}{c|}{}                                                        & \multicolumn{1}{c|}{}                                                       & \checkmark   & \multicolumn{1}{c|}{}      & \multicolumn{1}{c|}{\checkmark }                                     &                               &                     \\ \hline
CPFT            & \checkmark                                                                        & \checkmark                                                                                & \checkmark                                                                             & \multicolumn{1}{c|}{\checkmark }                                                        & \multicolumn{1}{c|}{}                                                       &   & \multicolumn{2}{c|}{}                                                                   &                               &                     \\ \hline
FastFit           & \checkmark                                                                        & \checkmark                                                                                & \checkmark                                                                             & \multicolumn{1}{c|}{\checkmark }                                                        & \multicolumn{1}{c|}{}                                                       &   & \multicolumn{2}{c|}{}                                                                   &                               &                     \\ \hline
CPFT + Snorkel            & \checkmark                                                                        & \checkmark                                                                                & \checkmark                                                                             & \multicolumn{1}{c|}{}                                                        & \multicolumn{1}{c|}{}                                                       & \checkmark  & \multicolumn{1}{c|}{}      & \multicolumn{1}{c|}{\checkmark}                                                                  &                               &                     \\ \hline
ARISE             & \checkmark                                                                       & \checkmark                                                                               & \checkmark                                                                            & \multicolumn{1}{c|}{}                                                        & \multicolumn{1}{c|}{}                                                       & \checkmark  & \multicolumn{1}{c|}{}      & \checkmark                                                          &                               &                     \\ \hline
ARISE-Iter        & \checkmark                                                                       & \checkmark                                                                               & \checkmark                                                                            & \multicolumn{1}{c|}{}                                                        & \multicolumn{1}{c|}{}                                                       & \checkmark  & \multicolumn{1}{c|}{}      & \checkmark                                                          & \checkmark                              &                     \\ \hline
LLMs              &                                                                        &                                                                                & \checkmark                                                                            & \multicolumn{1}{c|}{\checkmark}                                                        & \multicolumn{1}{c|}{}                                                       &   & \multicolumn{2}{c|}{}                                                                   &                               & \checkmark                    \\ \hline
\end{tabular}
\caption{Techniques used by competing systems. Base is Roberta and XLM-R for monolingual and multilingual experiments respectively. FT is fine-tuning; CL is contrastive learning; DA is data augmentation;  PVI is pointwise V-information; ST is self-training; IDRF is Iterative Data and Rules filtering.}
\label{tab:baselines}
\end{table*}

\paragraph{Dataset}: We use three text classification datasets, namely, {\sc Discovery} \cite{}, {\sc CDR} \cite{wrench} and {\sc ANLI}, for our full shot setup. For ANLI, we focus on the R3 Dataset. For few-shot, we use the {\sc FewMany} Benchmark \cite{yehudai2024llms}. It consists of eight few-shot datasets, namely, {\sc CLINC150} \cite[C150;][]{larson-etal-2019-evaluation}, {\sc Banking77} \cite[B77;][]{casanueva-etal-2020-efficient}, {\sc HWU64} \cite[HU64;][]{liu2019benchmarking} for intent classification; {\sc Argument Topic} \cite[AT71;][]{gretz2020large} and {\sc Claim Stance} \cite[CS55;][]{bar-haim-etal-2017-stance} for Topic classification; {\sc Trec} question classification dataset \cite[T50;][]{li-roth-2002-learning}, {\sc Amazon Products} (AP106) and {\sc DBPedia} (DB70). %We also use SciCite \cite[SC3][]{cohan-etal-2019-structural}, from the scientific literature domain. %where we include hand crafted features into the automatic rule learning framework.  
Finally, the multilingual experiments are performed using the {\sc MASSIVE} dataset \cite{fitzgerald2023massive}. Here, we use seven typologically diverse languages including Chinese, English, French, German, Hindi, Japanese, and Spanish. 

% Please add the following required packages to your document preamble:
% \usepackage{multirow}


\begin{table*}[]
\centering
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|}
\hline
\textbf{Models}     & \textbf{AP106}                  & \textbf{AT71}                    & \textbf{B77}   & \textbf{C150}                    & \textbf{CS55}                    & \textbf{DB70}                    & \textbf{HU64}                    & \textbf{T50}                     & \textbf{SciCite}                 & \textbf{Avg}                     \\ \hline
Base                & 57.36                           & 95.59                            & 87.55          & 94.3                             & 91.06                            & 87.03                            & 86.28                            & 86.57                            & 82.12                            & 85.32                            \\ \hline
Base-Aug            & 57.42                           & 95.3                             & 88.36          & 93.83                            & 90.16                            & 87.92                            & 87.58                            & 86.8                             & 82.58                            & 85.55                            \\ \hline
Base-ST             & 58.46                           & 95.78                            & 88.58          & 94.37                            & 91.1                             & 88.23                            & 88.69                            & 87.26                            & 83.07                            & 86.17                            \\ \hline
CPFT                & 58.82                           & 96.67                            & 89.51          & 95.03                            & 91.34                            & 89.14                            & 89.76                            & 89.42                            & 84.38                            & 87.12                            \\ \hline
Snorkel             & 59.47                           & 96.35                            & 90.49          & 94.96                            & 90.33                            & 88.42                            & 89.2                             & 89.3                             & 85.21                            & 87.08                            \\ \hline
FastFit             & 59.29                           & 96.79                            & 89.4           & 95.48                            & 90.24                            & 88.63                            & 89.54                            & 88.84                            & 85.01                            & 87.02                            \\ \hline
IntenDD          & 59.67                           & 97.02                            & 90.07          & 95.71                            & 91.71                            & 88.93                            & 89.04                            & 88.45                            & 85.04                            & 87.29                            \\ \hline
\begin{tabular}[c]{@{}l@{}}CPFT+\\ Snorkel\end{tabular}      & 59.74                           & 97.12                            & 90.76          & 95.24                            & 91.48                            & 89.22                            & 89.81                            & 89.71                            & 85.67                            & 87.64                            \\ \hline
ARISE               & 60.87*                          & 97.02                            & 92.12*         & 96.37*                           & 91.78                            & 89.59                            & 90.89*                           & 90.24                            & 85.87                            & 88.31                            \\ \hline
\textbf{ARISE-Iter} & \textbf{62.6} & \textbf{97.93} & \textbf{92.82} & \textbf{97.15} & \textbf{92.89} & \textbf{90.78} & \textbf{92.27} & \textbf{91.32} & \textbf{87.12} & \textbf{89.43} \\ \hline
\end{tabular}
\caption{Accuracy Results for 10-shot monolingual FSTC. Results in boldface and those marked with * are
statistically significant by t-test (p < 0.05) compared to \our~and CPFT+Snorkel respectively.}
\label{tab:mainResults}
\end{table*}

\paragraph{Data Augmentation:} We use  GPT-3.5, 4, and Claude 3 Opus for synthetic data generation. We generate label-specific data by prompt demonstration. Here,  Using \citet{wu-etal-2023-openicl}, we perform $k$-NN retrieval, with $k$ = 5, from the seed data for positive demonstrations, and randomly sampled out of class samples as negative examples \cite{liu-etal-2022-makes}. For multilingual experiments, we experiment with \textit{direct} generation of the synthetic data in the target language, and also via \textit{translation} of synthetically generated English sentences. For translation, in addition to the three aforementioned LLMs we use NLLB-54B \cite{nllbteam2022language} and Google Translate. For translation in Hindi, we use \citet{gala2023indictrans}.

\paragraph{Baselines:} Table \ref{tab:baselines} shows our baselines. Base models are the `Large' variants of Roberta \cite{liu2019roberta} and XLM-R \cite{conneau-etal-2020-unsupervised} for our monolingual and multilingual experiments respectively. Further, Base-DA is fine-tuned with augmented data (no filtering). Base-ST is trained using self-training-based filtering of augmented data. We also include competitive models that also combine multiple learning techniques, such as IntenDD \cite{singhal-etal-2023-intendd}, Snorkel \cite{DBLP:journals/corr/abs-1711-10160ratner}, CPFT \cite{zhang-etal-2022-contrastive}, and FastFit \cite{yehudai2024llms}. Following \citet{yehudai2024llms}, we report results for ICL, in 5-shot setups, using Flan-XXL \cite{wei2021finetunedflanxxl}, Flan-UL2 \cite{tay2022ul2}. 
 



%We perform few-shot prompting using.  %For data augmentation for IntenDD, setfit and FastFit we use the information theoreti data augmentation strategy proposed in \citet{lin-etal-2023-selective}.


\paragraph{Experimental Setup:} \our~and \our-Iter, as shown in Table \ref{tab:baselines}, are two variants without and with the iterative data and rule filtering (IDRF). \our~variants use  the same pre-trained models as used in `Base'. We perform all our experiments using 5 random splits and report the average. We use accuracy as our metric and experiment with both 5-shot an 10-shot settings\cite{yehudai2024llms}. For joint learning, we use a 20 \% split of the synthetically generated data as a validation split, while using all the gold data in training. For learning the parameters for our rule filtering step (\S \ref{ruleFiltering}), we use the few-shot gold data as validation. We report results for \our-iter induced with rules where the gold data was used only in the last iteration of bootstrapping. We keep a multiplier of 128x for our k-shot classification settings, following \citet{lin-etal-2023-selective}. %As shown in Table \ref{tab:baselines}, all the models that incorporate rules, use our rule and data filtering approaches. Those marked with a \checkmark~under PVI use the information theoretic data filtering followed in \citet{lin-etal-2023-selective}. Those marked with a \checkmark~under IDRF use the iterative version of our data and rule filtering. 
We use the graph-based biaffine parser \cite{dozat2016deep} trained with XLM-R as the encoder on the UD treebank \cite{11234/1-5150ud} for dependency parsing. We obtain induced subtrees of upto 3 nodes as rules.

%However, the parameters were learned in previous experiments.use the  for rule induction only in the last iteration of bootstrapping. 

%Table \ref{tab:baselines} shows the various learning techniques Incorporated in each of these baselines. 


 %randomly sampled from the set of augmented data. \textbf{Self-training}: Base classifier trained with augmented data, filtered only if a self training classifier labels as the same as that of the augmented label in some iteration of self-training. \textbf{setfit}: Base classifier, with contrastive learning and augmented sentences. \textbf{fastfit}: slef supervised contrastive learning with nearest neighbour and token levle simialrity. \textbf{intendd}: self supervised el;arning but no continuted pretraining. corpus specific pmi, ngrams. \textbf{intendd-LP}: additional 2 layer LP. \textbf{snorkel-ngrams}: labeling funcrions as filter for data ugmentation.\textbf{snorkel-trees}: Labeling functions as filter for data ugmentation. \textbf{ARISE}: Labeling  functions as filter for data augmentation with trees. \textbf{ARISE with data filter}. \textbf{LLM with ICL}.



%We additionally use fastfit, setfit and xxxx for our additional experiments.  The baseline configurations are as follows. \textbf{Base classifier}: The pretrained model (RoBeRTA or XLM-R) which is simply trained with the few shot labelled data. 
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage{graphicx}




\section{Results}

For all of the datasets with which we experimented, we find gains in using both the generated data and the rules. Further, we show three different configurations under which our data can and rules can be leveraged to obtain best performances for the respective datasets. While 

these lead to our p


We find performance gains in various datasets using both the generated data and rules. Table \ref{} shows our overall results for full-shot setting under various configurations. 

\subsection{Impact of Rules}

We incorporate our rules in various ways into our classification tasks. We include them as explanations in ICL, for joint learning using spear during fine tuning, and as both explanations in the input and for joint learning. Under full-shot settings, we observe statistically significant gains from the rules in ICL for all the three datasets, and in wo of three datasets except for Discovery in fine tuning. 

\paragraph{Rules used for Joint learning:} For CDR, joint rule learning results in a percentage increase of 3.92 and 3.59 for both QWEN and RoBERTa, respectively, compared to full-shot fine-tuning.  Similarly for ANLI, we observe a percentage increase of 8.38 and 7.35, respectively, for QWEN and RoBERTa respectively. Moreover, ee observe that ANLI reports statisticvally significant gains with QWEN when rules are used both for joint learning during fine tuning and for using them as explanations in the input, compared to just using it for joint learning. For other datasets, the adding rules as explanations, in addition to joint learning, does not lead to statistically significant gains. For Discovery, the gains we observe in absolute terms were not statistically significant gains in both QWEN and RoBERTa. However, there was no performance degradation for this dataset. Summarily, we find overall gains in using joint learning while fine-tuning task-specific models.  QWEN and RoBERTa both show similar trends with and comparable gains with joint learning as compared to simple fine-tuning.

\paragraph{Rules used as explanations:}  We use our rules, along with their label predictions, as an explanation for the input alone under zero-shot and also for the K-shot samples from the training data under the few-shot setup. Adding an explanation alone under zero-shot in itself outperforms SotA models for both CDR and ANLI. While retrieving k-shot exemplars from the training data as prompt demonstration alone leads to statistically significant gains compared to zero-shot, adding contrastive explanations further leads to further gains in our experiments for both QWEN and GPT-4. Further, use of explanations in k-shot settings with GPT-4 led to the highest performance for CDR among configurations, and reports a percentage gain of more than 8 from the previous SotA. Similarly, we report the second best performance for ANLI using GPT-4, which has a percentage increase of more than 16 from the previous SotA. Previous SotA was a fine tuned model, with 1/3rd of total training data, while our configuration under discussion is purely under ICL. 


Within ICL, GPT-4 outperforms QWEN in both CDR and ANLI, but QWEN outperforms GPT-4 in Discovery. Discovery has a large label space of 174 labels, and these are common terms which are typically used as markers between two statements. QWEN being an open source model, we were able to constrain the output space by constrained decoding.  Though in GPT-4, there exists similar appraoches with structured output generation, we do not have much control over manipulating the logits and logit manipulation led to higher improvements in QWEN.Nevertheless, both the models perform at half the performance compared to the fine-tuned counterparts. Adding rules clearly helped Discovery improve performance for both the models but have limited impact compared to fine-tuning approaches. 


 
 

%We hypothesise that large label space along with presence of these words in common English usage are probable reasons for the low performance in ICL. For Discover, we perform two additional analyses, which includes the following. We observe that QWEN performs better than GPT-4 in ICL cases, because we constrain our decoder in QWEN with label specific terms. Further, QWEN in joint learning uses a classification head while constrained decoding is employed in ICL. Our fine-tuning approach outperforms ICL in this case. 
%both input samples and retrieved . For K-shot samples, we provide the explanations so that it contains both positive and negative cases for each of the retrieved samples.   Further, CDR reports  However, for CDR and ANLI, we find that our ICL versions outperform fine-tuned versions. Further, increasing the number of exemplars with explanations leads to consistent performance gains in all the datasets. In Discovery, we find gains until our context window size is filled. For ANLI and CDR, we find that there are no significant gains after 128 samples with explanations. For CDR, ICL with expplanation for the input alone led to performance improvements in GPT-4, more than the fine-tuned version. However, QWEN required explanations for 64 samples to achieve similar scores. 









%Table \ref{} shows the impact of the rules we obtained from \our. We observe gains for the three datasets both in zero-shot and k-shot setups for ICL. For all the datasets, an explanation prompt for the input, like in zero-shot, along with contrastive few shot demonstrations and explanations lead to the highest gains in ICL setting. Summarily, using the rules alone we obtained using \our~leads to statistically significant gains in ICL. 


%We find that adding rules along with the data leads to even further improvements in all the cases.  Rules as a substitute for more data? Role of rules in joint learning. Role of rules in ICL. Further, we find that use of rules in ICL for discovery shows the highest gain among all cases, although adding rules during fine-tuning seems to have limited impact for the same dataset. Further, we find that the joint learning framework helps Roberta more than the LLMs. Further, joint learning of rules and data shows highest gains in Discovery for fine tuning. Simialrly, the other datasets show limited gains with fine tuning, though adding rules in ICL helps them with improvements. Rules as explanation probably? 



\subsection{Impact of Generated Data:} 

As shown in Table \ref{} we find that using generated data, both for training and for ICL, leads to statistically significant gains in all configurations for the three datasets. In our experiments, we generate synthetic data in multiples of the original training data size. We generate data from 1$\times$ to 6$\times$ of the original data. 


\paragraph{Fine-tuning:} For all the three datasets, adding additional data beyond the full training data  during fine tuning leads to significant gains. For both discovery and CDR, we observe gains until 1.5$x$ times more data is added to the training data. For ANLI, we observe gains by doubling the training data size, i.e. 1$x$ the training data. We observe that the training saturates after when more data is being added to these datasets, until we tried with 3$x$ more data. Our observations hold true for both QWEN and RoBERTA, where tried the fine tuning. As shown in Figure \ref{}, the initial size of the seed, depending on the availability of gold data, leads to differences in performance. In ANLI, a limited seed size, of 15 \% and 20 \% instances per class, led to performance degradation. In all these settings, we still train the models with full-training data. Discovery dataset achieves the best performance, almost double that of the best performing ICL model using GPT-4, when fine tuning the model with additional data. It reports a performance increase of 3.29 with 1.5$x$ data. For all the datasets, fine-tuning these models with just the augmented data does not lead to similar perofrmances. Further, starting the training with original training data is crucial for the performance improvement in all these datasets, with least difference being observed in ANLI. 



\paragraph{ICL:} There are more training instances than that can fit into the 128K context windows per class for all the three datasets. However, increasing the pool of available data leads to improved outcomes in ICL, with the help of retrieval. For ANLI and CDR, we consistently had less than 40 \% presence of instances from the original training data for cases with 1$\times$ and above. With Discovery, we observe similar patterns but only after  1.5$\times$. We stop our experiments with 3$\times$ data as the overlap between retrieved ICL exemplars had more than 90 \% overlap by then.  



%Both CDR and ANLI datasets show gains when using the augmented data in ICL while Discovery shows highest gain in using the data when fine tuning.  Impact as dataset size increases takeaways: More data lead to improvements. Generated data is better than random sampling of similar size. Generated data is better than simply retrieving (in ICL). It outperforms the SoTA in respective category.


\subsection{Data or Rules}

QWEN and RoBERTa show different trends when it comes to adding rules during fine tuning. Joint learning does not lead to statistically significant gains, once fine tuning was performed with additional data. For CDR and ANLI, though not for Discovery, we found gains in using joint learning when trained with just the training data. However, these gains were not observed when additional data was added which complented. Adding data has led to statistically signifcant gains for all the ddatasets, compared to using joint learning with rules using jhust the training dataset. However, on the contrary, QWEN models benefit from joint learning for CDR and ANLI, even after their results were saturated after adding data. Here, both additional data and rules have shown to benefit the models and bring in compliemntary supervision signals. Interestingly, ANLI is the only dataset where using rules for both joint learning and as explanations in the input leading to perofrmance gains. It is the best performing configuration for ANLI. 


%Further, we perform joint learning along with the induced rules for both the fine-tuning settings, using LLM and Roberta.  We find that 


% Please add the following required packages to your document preamble:
% \usepackage{graphicx}

\our, our proposed approach, leads to performance improvements for all configurations, whether through ICL or through fine-tuning. 

reports the best performance in all our experimental settings, % monolingual 5 and 10-shot settings, and multilingual settings 
as shown in Tables \ref{tab:mainResults}, \ref{tab:fiveShot}, and \ref{tab:multilingual}. It outperforms all other models with statistically significant gains. \our-Iter reports an absolute improvement of 1.79 \% points (2.04 \% increase),  1.32 \% and 2.58 \% points averaged across the datasets, for the 5 and 10-shot monolingual and 10-shot multilingual setups.


%9 datasets, from the best performing non-\our~model for the 10-shot setup. Similarly, \our-iter reports an absolute improvement of, for the 5-shot monolingual and 10-shot multilingual setups.   


%For the 5-shot setup, an points, averaged across the same datasets (Table \ref{tab:fiveShot}) is reported. Finally, the absolute improvements averaged across seven languages in the multilingual setup are  is 3.39 percentage points. 

\begin{table*}[h]
\centering
\begin{tabular}{|l|c|c|c|c|c|c|c|}
\hline
\textbf{Methods}                                        & \textbf{AT71}  & \textbf{B77}   & \textbf{C150}  & \textbf{CS55}  & \textbf{HU64}  & \textbf{T50}   & \textbf{Avg.}  \\ \hline
%Mistral Instruct                                        & 95.16          & 74.88          & 65.94          & 63.16          & 73.92          & 50.02          & 70.51          \\ \hline
Flan-ul2                                                & 97.07          & 71.21          & 80.6           & 89.57          & 76.2           & 64.86          & 79.92          \\ \hline
Flan-XXL                                                & 96.72          & 72.04          & 81.99          & 50.24          & 75.13          & 84.72          & 76.81          \\ \hline
Base                                                    & 95.61          & 79.77          & 91.67          & 87.94          & 79.29          & 73.67          & 84.66          \\ \hline
%CPFT                                                    & 95.73          & 86.88          & 93.65          & 87.79          & 86.46          & 84.61          & 89.19          \\ \hline
%Snorkel                                                 & 96.47          & 88.21          & 94.04          & 88.34          & 86.33          & 87.09          & 90.08          \\ \hline
FastFit                                                 & 96.45          & 86.14          & 93.77          & 88.16          & 84.6           & 84.8           & 88.99          \\ \hline
Intendd                                              & 96.11          & 89.13          & 94.05          & 88.76          & 88.21          & 86.86          & 90.52          \\ \hline
\begin{tabular}[c]{@{}l@{}}CPFT+\\ Snorkel\end{tabular} & 96.74          & 88.64          & 94.46          & 88.57          & 87.38          & 87.45          & 90.54          \\ \hline
ARISE                                                   & 96.68          & 90.35          & 94.89          & 90.3           & 88.04          & 88.72          & 91.5           \\ \hline
\textbf{ARISE-Iter}                                     & \textbf{97.14} & \textbf{91.68} & \textbf{96.13} & \textbf{91.59} & \textbf{90.22} & \textbf{90.14} & \textbf{92.82} \\ \hline
\end{tabular}
\caption{Accuracy Results for 5-shot monolingual FSTC.}
\label{tab:fiveShot}
\end{table*}



\begin{table*}[!h]
\centering
\begin{tabular}{|l|c|c|c|c|c|c|c|c|}
\hline
           & \textbf{En}                  & \textbf{De}                  & \textbf{Ja}                  & \textbf{Es}                  & \textbf{Fr}                  & \textbf{Zh}                  & \textbf{Hi}                  & \textbf{Avg.} \\ \hline
Base       & 77.65                        & 71.23                        & 74.89                        & 71.56                        & 72.81                        & 73.14                        & 71.07                        & 73.19            \\ \hline
IntenDD    & 79.55                        & {\color[HTML]{565869} 73.64} & {\color[HTML]{565869} 76.5}  & {\color[HTML]{565869} 76.92} & {\color[HTML]{565869} 76.42} & {\color[HTML]{565869} 76.53} & {\color[HTML]{565869} 74.41} & 76.28            \\ \hline
Snorkel    & {\color[HTML]{565869} 80.52} & {\color[HTML]{565869} 75.39} & {\color[HTML]{565869} 78.87} & {\color[HTML]{565869} 75.79} & {\color[HTML]{565869} 77.65} & {\color[HTML]{565869} 76.7}  & {\color[HTML]{565869} 74.16} & 77.01            \\ \hline
CPFT     & 78.65                        & {\color[HTML]{565869} 73.45} & {\color[HTML]{565869} 77.56} & {\color[HTML]{565869} 74.99} & {\color[HTML]{565869} 76.74} & {\color[HTML]{565869} 75.58} & {\color[HTML]{565869} 73.66} & 75.8             \\ \hline
FastFit    & 80.73                        & 75.97                        & 78.49                        & 75.64                        & 76.84                        & 75.98                        & 74.07                        & 76.82            \\ \hline
CPFT + Snorkel    & 81.43 &	76.67	&79.34&	76.43	& 78.14 &	77.66 &	75.04 &	77.82         \\ \hline
ARISE      & 82.43                        & 76.64                        & 79.52                        & 77.1                         & 78.93                        & 78.32                        & 75.16                        & 78.3             \\ \hline
ARISE-Iter & \textbf{84.96}               & \textbf{79.38}               & \textbf{81.87}               & \textbf{79.58}               & \textbf{80.16}               & \textbf{79.45}               & \textbf{77.41}               & \textbf{80.4}    \\ \hline
\end{tabular}
\caption{Multilingual results on MASSIVE Dataset.}
\label{tab:multilingual}
\end{table*}
%Arise-iter reports an average increase between 1.75 \% and 2 \%, in 10-shot settings, compared to other strong baselines. Arise-Iter benefits from a combination of data augmentation, contrastive learning, data subset selection, iterative automatic rule induction, and syntactic rules. 
\subsection{Monolingual Results}

\our-Iter and \our~differs only in terms of bootstrapping (IDRF). %the use of iterative rule and data filtering (IDRF). In \our, both rule induction and data augmentation happen at once, though all the models use the same amount of training data. 
Bootstrapping alone leads to an average absolute gain of 1.12 and 1.32 \% points for the 10-shot and 5-shot setups respectively (Tables \ref{tab:mainResults} and \ref{tab:fiveShot}), between both the \our-Iter and \our~respectively. %AP106 reports the highest improvement  in the 10-shot setup whereas HU-64 reports the hightest improvement in the 5-shot setup. %Base is simply the Roberta-Large model fine tuned with the labeled few-shot data. Base-Aug includes synthetically generated sentences, randomly sampled from the pool of augmented sentences.  
Base-Aug reports statistically significant gains only for 3 of 9 datasets (B77, HU64, and DB70) compared to Base in Table \ref{tab:mainResults}. It shows that data augmentation without any filtering need not always improve the results. Further, Base-ST %As shown in , , as there is performance drop for  in 3 of the 9 datasets in spite of the increased training data size. Further, statistically significant gains are reported only for  datasets. %performs filtering of augmented data using a self-training classifier, where an augmented sentence is chosen only when the LLM's prediction and the ST Classifier's prediction are in agreement. The ST classifier, 
on average report a gain of 0.85 \% points compared to Base, with statistically significant gains in 6 of 9 datasets (except for AT71, C150, and CS55). %are three datasets where the results are not statistically significant. 

\our~variants follow CPFT \cite{zhang-etal-2022-contrastive} in employing contrastive learning (CL) components.   %is a contrastively pretrained and fine-tuned variant of the Base, which we also incorporate in \our. 
CL components alone in CPFT lead to an average absolute gain of 1.8 \% points compared to Base, in Table \ref{tab:mainResults}. Similarly, Snorkel, a PWS framework, and \our~is trained with the same filtered data and rules. However, unlike \our, Snorkel does not use joint learning. Instead, Snorkel learns a generative model to label (or filter in our case) synthetically generated sentences. It outperforms the base model by an average absolute improvement of 1.76 \% points and is competitive with CPFT. Snorkel and CPFT report statistically significant gains compared to Base for all the datasets, except CS55.  Snorkel and CPFT report comparable performance on 5 of 9 datasets, with statistically significant gains in 2 datasets each. %Snorkel's performance shows the effectiveness of our rule induction framework, as a general purpose framework for PWS. 

CPFT+Snorkel combines both contrastive representation learning and PWS. It differs from \our, only in terms of the joint learning component. \our~reports an absolute improvement of 0.67 \% points in 10-shot settings (Table \ref{tab:mainResults}), and 0.96 \% points  in 5-shot settings (Table \ref{tab:fiveShot}), as compared to CPFT+Snorkel. %, with statistically significant gains in 4 datasets. %Further, \our~reports statistically significant gains to CPFT+Snorkel in four of the five datasets. 
Results from Snorkel, CPFT+Snorkel, and \our~show our rule induction component, as a general-purpose one for PWS. Similarly, gains in CPFT+Snorkel and \our~show that combining complementary learning techniques leads to performance gains compared to using them independently.  


\our-Iter, our proposed approach with IRDF, %reports statistically significant gains compared to all the other models including \our. It 
outperforms both IntenDD \cite{singhal-etal-2023-intendd} and FastFit \cite{yehudai2024llms}, two competitive models with state-of-the-art results on few-shot learning. While FastFit originally does not use data augmentation, we add augmented sentences to it for a fair comparison. IntenDD differs from \our~by using string-level n-grams for weak supervision and additionally employs a two-level transductive learning approach. \our-Iter when trained with string level n-grams as used in IntenDD still outperforms IntenDD but reports an average accuracy of 88.64 \%, a drop from 89.43 for the 10-shot setting. Similarly, the use of PVI for data filtering instead of IRDF for \our-Iter results in an average accuracy of 88.19 \%.




%Moreover, both CPFT and Snorkel (with our induced rules) has shown to outperform two state of the art few-shot text classification models FastFit and IntenDD. Both the SoTA models use contrastive learning as part of their training pipeline. IntenDD, follows CPFT, but aslo incorporates unlabeled data via pseudo labeling of the unlabeled datasets using string level ngrams. Here, since, there are no nulabeled datasets, we used the ngrams to filter synthetically generated data. They use Louvain based dataset partitioning approach instead of our rule induction framework. Overall, we find that IntenDD performs worse than CPFT, by 0.15 percentage points. Specifically, there are two datasets DB70 and T50 where CPFT outperforms  IntenDD. Further, IntenDD-LP is the best performing variant in IntenDD, where they use trasnductive learning approach to improve on the final performance.  CPFT+Snorkel use both contrastive pretraining and PWS. It differs from \our, only in the joint learning setup. As it can be observed  Snorkel outperforms all other baselines (except \our) in the 5-shot setup and  reports comparative results to SetFit for the 10-shot setup. Snorkel's results show that our rule induction metho alone is powerful and can match other state of the art baselines. 


%The joint setup in \our~alone leads to more than 1 \% absolute improvement in both the 5-shot and 10-shot setups compared to Snorkel. It highlights the benefit of the joint learning, instead of simply generating weakly labeled data to use in training. 

%IntenDD 

%Now, ARISE and Snorkel differs only in data programming frameworks they use. ARISE uses SPEAR, a framework enabling joint learning while SNORKEL is a data programming framework that helps obtain weakly labeled examples. There is more than a 1 \% absolute improvement in ARISE as compared to Snorkel, highlighting the  Both the ARISE variants and Snorkel use automatically induced rules as weak labeling functions and these are rules are generalised syntactic ngrams. IntenDD is a model that also use ngrams based on PMI based filtering in their learning framework. The ngrams are used for corpus specific pretraining, followed by network construction for the semi supervised label propagation approach they use. ARISE variants outperform IntenDD on all the experimental settings in both 5-shot and 10-shot. Snorkel performs better than IntenDD on x of the y tasks, on an average increase of zz absolute percentage points.  


%Both FastFit and Setfit have shown to perform competitively in our experimental settings. Here, we use augmented sentences that has passed information theoretic filtering, and we use the best configuration from the augmented subset. All the three systems that use automatically induced generalized syntactic ngrams report statistically significant gains to that of both models. Among FastFit, Setfit and IntenDD, FastFit performs the best on an average. However, IntenDD outperforms others on one of the tasks and SetFit outperforms the otehr two on two tasks. Contrary to what FastFit reports, we observe that setfir models arecompetitive with fastfit, probably due to the impact of augmented data. 


Table \ref{tab:fiveShot} reports results for the 5-shot setup. We follow the setup of \citet{yehudai2024llms} for ICL. \our-Iter~reports an average absolute gain of 16.01 \% and 2.28 \% compared to Flan-XXL and CPFT+Snorkel models respectively. It also reports statistically significant gains, compared to both, for all the datasets except AT71.  Overall, Flan-XXL and Flan-UL2 outperform other LLMs  \cite{touvron2023llama, jiang2023mistral} in our ICL experiments and hence reported in Table \ref{tab:fiveShot}. 



%Interestingly, adding more augmented inputs, though lead to increases for a Mistral model often double than that of not having augmented sentences. These models are still not outperforming any of the ARISE variants, highlighting the significance of automatically induced rules. 


%\subsection{Data Augmentation}

%Data augmentation has been extensively used NLP for various text classification tasks. Data augmentation are often performed at lexical, phrase-level or syntactic levels transformations to a given sentences, or obtaining new sentences via translation or are generated with the help of LLMs. Of At large we perform augmentation for classification, where we assume that the transformation we provide are label preserving. Of recently, filtering approaches that enable scoring or filtering augmented sentences have been proposed. We combine data augmentation and automated rule induction in an interactive manner. Here, automated rules are induced based on the graph cut based filtering. We compare the data augmentation strategies and the subsequent filtering approaches using ARISE as the base model.  We find that our approaches 

\paragraph{Multilingual Experiments:}  Table \ref{tab:multilingual} shows the results for multilingual experiments. %We find that \our-Iter reports statistically significant improvements over all the other models for all the languages. 
On an average \our-Iter reports an absolute improvement of 2.1 \% points compared to \our, the next best model. The results show that our approach is applicable across a typologically diverse set of languages. We find \textit{translation} of synthetically generated English sentences leads to empirically better results as compared to \textit{direct} generation of data in the target language. The results for the former are reported in Table \ref{tab:multilingual}.  The latter approach results in an absolute  drop of 1.27 \% points. Further, we also experiment with a setting where we induct rules from dependency parses of all the translations of an input. Here, we observe a performance drop for all the languages, except Hindi. On average there is 0.76 \% drop for \our-Iter compared to the default setting as reported in Table \ref{tab:multilingual}. For Hindi, it reported 78.62 \% as compared to 77.41 \% in the default setting.

\section{Conclusion}

We propose \our, a framework that combines contrastive representation learning, automatic rule induction, data augmentation, IRDF and joint learning via PWS. While PWS is typically employed as a weak supervision approach for labeling unlabeled data, we employ it for verifying synthetically generated labeled documents. Further, we find incorporating syntactic information, instead of strings, via rules leads to gains. Overall, \our outperforms strong competitive baselines under comparable conditions. We also show the effectiveness of combining diverse learning components that enable incorporating complementary information from the limited gold data to achieve state-of-the-art results.



%For the multilingual settings, we perform data augmentation in two ways. One, by generating synthetic data directly in the target language. Two, by automated translation of the synthetic data generated in English to the target language. We find the latter approach results in  for the dataset, and the results are  The multilingual setup has led to increase in diversity of rules. However, 

%We also experimented with 


%The results reported in  use the translation based data augmentation, where the language specific synthetic data is obtained by automated tra

%inputs obtained via translation of synthetically generated English sentences. 


%find results consistent with our monolingual English experiments in our multilingual experiments using MASSIVE as well. Here, both ARISE and ARISE-iter, outperforms all other models with statistically significant gains. Moreover, ARISE-Iter on an average reports a xx \% gain in absolute percentage points as compared to other models. Moreover, the generalized syntactic ngrams that we propose have shown to beneficial for all the languages we experimented with. All the three models that use the syntactic ngrams have shown significant gains as compared to others. IntenDD, the model with surface-form ngrams, while have shown to be beneficial for languages like English and German, we find loss of performance for languages like Hindi. 




\begin{table*}[h]
\centering
\begin{tabular}{|l|c|c|c|c|c|c|c|}
\hline
\textbf{Methods}                                        & \textbf{AT71}  & \textbf{B77}   & \textbf{C150}  & \textbf{CS55}  & \textbf{HU64}  & \textbf{T50}   & \textbf{Avg.}  \\ \hline
%Mistral Instruct                                        & 95.16          & 74.88          & 65.94          & 63.16          & 73.92          & 50.02          & 70.51          \\ \hline
Flan-ul2                                                & 97.07          & 71.21          & 80.6           & 89.57          & 76.2           & 64.86          & 79.92          \\ \hline
Flan-XXL                                                & 96.72          & 72.04          & 81.99          & 50.24          & 75.13          & 84.72          & 76.81          \\ \hline
Base                                                    & 95.61          & 79.77          & 91.67          & 87.94          & 79.29          & 73.67          & 84.66          \\ \hline
%CPFT                                                    & 95.73          & 86.88          & 93.65          & 87.79          & 86.46          & 84.61          & 89.19          \\ \hline
%Snorkel                                                 & 96.47          & 88.21          & 94.04          & 88.34          & 86.33          & 87.09          & 90.08          \\ \hline
FastFit                                                 & 96.45          & 86.14          & 93.77          & 88.16          & 84.6           & 84.8           & 88.99          \\ \hline
Intendd                                              & 96.11          & 89.13          & 94.05          & 88.76          & 88.21          & 86.86          & 90.52          \\ \hline
\begin{tabular}[c]{@{}l@{}}CPFT+\\ Snorkel\end{tabular} & 96.74          & 88.64          & 94.46          & 88.57          & 87.38          & 87.45          & 90.54          \\ \hline
ARISE                                                   & 96.68          & 90.35          & 94.89          & 90.3           & 88.04          & 88.72          & 91.5           \\ \hline
\textbf{ARISE-Iter}                                     & \textbf{97.14} & \textbf{91.68} & \textbf{96.13} & \textbf{91.59} & \textbf{90.22} & \textbf{90.14} & \textbf{92.82} \\ \hline
\end{tabular}
\caption{Accuracy Results for 5-shot monolingual FSTC.}
\label{tab:fiveShot}
\end{table*}



\begin{table*}[!h]
\centering
\begin{tabular}{|l|c|c|c|c|c|c|c|c|}
\hline
           & \textbf{En}                  & \textbf{De}                  & \textbf{Ja}                  & \textbf{Es}                  & \textbf{Fr}                  & \textbf{Zh}                  & \textbf{Hi}                  & \textbf{Avg.} \\ \hline
Base       & 77.65                        & 71.23                        & 74.89                        & 71.56                        & 72.81                        & 73.14                        & 71.07                        & 73.19            \\ \hline
IntenDD    & 79.55                        & {\color[HTML]{565869} 73.64} & {\color[HTML]{565869} 76.5}  & {\color[HTML]{565869} 76.92} & {\color[HTML]{565869} 76.42} & {\color[HTML]{565869} 76.53} & {\color[HTML]{565869} 74.41} & 76.28            \\ \hline
Snorkel    & {\color[HTML]{565869} 80.52} & {\color[HTML]{565869} 75.39} & {\color[HTML]{565869} 78.87} & {\color[HTML]{565869} 75.79} & {\color[HTML]{565869} 77.65} & {\color[HTML]{565869} 76.7}  & {\color[HTML]{565869} 74.16} & 77.01            \\ \hline
CPFT     & 78.65                        & {\color[HTML]{565869} 73.45} & {\color[HTML]{565869} 77.56} & {\color[HTML]{565869} 74.99} & {\color[HTML]{565869} 76.74} & {\color[HTML]{565869} 75.58} & {\color[HTML]{565869} 73.66} & 75.8             \\ \hline
FastFit    & 80.73                        & 75.97                        & 78.49                        & 75.64                        & 76.84                        & 75.98                        & 74.07                        & 76.82            \\ \hline
CPFT + Snorkel    & 81.43 &	76.67	&79.34&	76.43	& 78.14 &	77.66 &	75.04 &	77.82         \\ \hline
ARISE      & 82.43                        & 76.64                        & 79.52                        & 77.1                         & 78.93                        & 78.32                        & 75.16                        & 78.3             \\ \hline
ARISE-Iter & \textbf{84.96}               & \textbf{79.38}               & \textbf{81.87}               & \textbf{79.58}               & \textbf{80.16}               & \textbf{79.45}               & \textbf{77.41}               & \textbf{80.4}    \\ \hline
\end{tabular}
\caption{Multilingual results on MASSIVE Dataset.}
\label{tab:multilingual}
\end{table*}


\begin{table*}[]
\centering
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|}
\hline
\textbf{Models}     & \textbf{AP106}                  & \textbf{AT71}                    & \textbf{B77}   & \textbf{C150}                    & \textbf{CS55}                    & \textbf{DB70}                    & \textbf{HU64}                    & \textbf{T50}                     & \textbf{SciCite}                 & \textbf{Avg}                     \\ \hline
Base                & 57.36                           & 95.59                            & 87.55          & 94.3                             & 91.06                            & 87.03                            & 86.28                            & 86.57                            & 82.12                            & 85.32                            \\ \hline
Base-Aug            & 57.42                           & 95.3                             & 88.36          & 93.83                            & 90.16                            & 87.92                            & 87.58                            & 86.8                             & 82.58                            & 85.55                            \\ \hline
Base-ST             & 58.46                           & 95.78                            & 88.58          & 94.37                            & 91.1                             & 88.23                            & 88.69                            & 87.26                            & 83.07                            & 86.17                            \\ \hline
CPFT                & 58.82                           & 96.67                            & 89.51          & 95.03                            & 91.34                            & 89.14                            & 89.76                            & 89.42                            & 84.38                            & 87.12                            \\ \hline
Snorkel             & 59.47                           & 96.35                            & 90.49          & 94.96                            & 90.33                            & 88.42                            & 89.2                             & 89.3                             & 85.21                            & 87.08                            \\ \hline
FastFit             & 59.29                           & 96.79                            & 89.4           & 95.48                            & 90.24                            & 88.63                            & 89.54                            & 88.84                            & 85.01                            & 87.02                            \\ \hline
IntenDD          & 59.67                           & 97.02                            & 90.07          & 95.71                            & 91.71                            & 88.93                            & 89.04                            & 88.45                            & 85.04                            & 87.29                            \\ \hline
\begin{tabular}[c]{@{}l@{}}CPFT+\\ Snorkel\end{tabular}      & 59.74                           & 97.12                            & 90.76          & 95.24                            & 91.48                            & 89.22                            & 89.81                            & 89.71                            & 85.67                            & 87.64                            \\ \hline
ARISE               & 60.87*                          & 97.02                            & 92.12*         & 96.37*                           & 91.78                            & 89.59                            & 90.89*                           & 90.24                            & 85.87                            & 88.31                            \\ \hline
\textbf{ARISE-Iter} & \textbf{62.6} & \textbf{97.93} & \textbf{92.82} & \textbf{97.15} & \textbf{92.89} & \textbf{90.78} & \textbf{92.27} & \textbf{91.32} & \textbf{87.12} & \textbf{89.43} \\ \hline
\end{tabular}
\caption{Accuracy Results for 10-shot monolingual FSTC. Results in boldface and those marked with * are
statistically significant by t-test (p < 0.05) compared to \our~and CPFT+Snorkel respectively.}
\label{tab:mainResults}
\end{table*}