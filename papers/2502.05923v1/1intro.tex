
\section{Introduction}

\footnotetext[0]{ Correspondence to: Yaswanth M <yaswanthm03@gmail.com> and
Vaibhav Singh <singhvaibhav@cse.iitb.ac.in>}
 %Availability of high-quality labeled data, often combined with auxiliary sources of weak supervision, are 
Large language models (LLMs) have facilitated the generation of high-quality synthetic data that often supplement available training data \cite{lin-etal-2023-selective} or even surpass crowd-sourced annotations \cite{augmentationPNAS,alizadeh2023open}. However, concerns of limited variance in such exemplars, leading to model collapse \cite{shumailov2023curse} or the failure to capture the tail of the true underlying distribution \cite{ding2024data}, remain. Similarly, forming multiple views of the available data by inducing rules, as a complementary source of supervision has shown to benefit various NLP tasks, including text classification \cite{maheshwari2021semi,dong-etal-2022-syntactic}.  In this work, we propose \our, a bootstrapping approach to iteratively refine synthetically generated exemplars and automatically induced rules, resulting in high quality entries with respect to a given classification task \cite{yarowsky-1995-unsupervised,varma2018snuba}. 

\begin{figure}[h] 
   \centering
   \includegraphics[trim={0 1.1cm 0 1cm},clip,scale=0.75]{rule_induction-Page-1.drawio.pdf}
   \caption{Overview of ARISE (\textbf{A}utomatic \textbf{R}ule \textbf{I}nduction using \textbf{S}yntactic tree g\textbf{E}neralization).} 
   \label{archi}
\end{figure}

Figure \ref{archi} provides an overview of \our. We start by using available training data as our seed. Using LLMs, we leverage in-context learning (ICL), with the seed as input to synthetically generate candidate exemplars \cite{liu-etal-2022-makes}. Similarly, we generate rule candidates, via inductive generalisation using least general generalization (LGG)~\cite{plotkin1971further,Raza_Gulwani_Milic-Frayling_2014} by extracting syntactic n-grams from the seed. Further, the induced rules are then filtered using a submodular graph cut-based function \cite{bajpai-etal-2024-fair, kothawade2021prism}. The exemplars and the rules we generate are task-specific and each exemplar and rule is associated with a label. Newly generated exemplars are filtered using rules that are generated from the already validated seed. These filtered exemplars are then added to the seed for the next iteration. Iteratively, we induce rules from synthetically generated data and use the induced rules for data filtering. 


%we generate rule candidates by extracting syntactic n-grams from sentence-level dependency parses of the input. Rules are induced from the syntactic n-grams via inductive generalization using   Since the synthetic data are generated along with their labels, those sentences are then validated using the rules. Only those labeled data points that match with the predictions of the rules are filtered. 



%Similarly,  are successfully used as sources of weak supervision . However, concerns associated with model collapse \cite{shumailov2023curse} or failing to capture the tail of the underlying true distribution due to limited variance in such data are raised with data augmentation \cite{ding2024data}. Here, we iteratively use rules and data to filter each otehr resulting in a pool of high quality data and rules which we incoroproate in our learning tasks

%In \our, we propose a bootstrapped approach for iterative synthetic data generation and automatic rule induction . Moreover, it enables joint training of the induced rules with pre-trained neural models via data programming \cite{maheshwari2021semi,zhang2022survey}. 
%Few-shot text classification (FSTC) is challenging, especially in tasks with a large, semantically similar and often overlapping label space \cite{zhang-etal-2022-contrastive}. Such tasks often find application in diverse domains including task oriented dialogue (intent classification), e-commerce, social networks, scientific literature etc.   \cite{yehudai2024llms, wrench}.  Moreover, these tasks are expected to have a unique or highly specialized label space, leading to limited availability of annotated data \cite{singhal-etal-2023-intendd,vulic-etal-2022-multi}. Intuitively, FSTC systems should be designed to extract as much information as possible from the limited supervision data available for learning. We propose \our, a framework that combines automatic rule induction  \cite{pryzant-etal-2022-automatic,bajpai-etal-2024-fair}, synthetic data generation, and contrastive representation learning \cite{zhang-etal-2022-contrastive} for FSTC. Moreover, \our~induces rules in the form of syntactic n-grams that complements information captured in prevalent approaches in FSTC. 

%Recently, FewMany benchmark puts together a bunch  of tasks varying from Intent detection, e-commerce products among others. In this we work, we show how such tasks can be modeled with a combination of learning techniques leading to state of the art results in several of these tasks under few-shot settings. 

%FSTC tasks are generally addressed using a diverse set of techniques. These include In-context learning \cite{NEURIPS2020_llmfew, NEURIPS2022_llmzero}, contrastive representation learning \cite{vulic-etal-2021-convfit}, data augmentation and filtering \cite{lin-etal-2023-selective}, transductive learning \cite{singhal-etal-2023-intendd}, weak supervision \cite{pryzant-etal-2022-automatic}, meta-learning \cite{mesgar-etal-2023-devil} among others. Several of these works successfully combine one or more of these techniques for FSTC tasks \cite{singhal-etal-2023-intendd,vulic-etal-2022-multi}. 


% \begin{figure*}[h] 
%    \centering\includegraphics[width=0.9\textwidth,page=4,trim={0 19.5cm 0 2cm}]{files/PresentationAutoRules.pdf}
%    \caption{Three-step workflow for \our, along with various components in it.} 
%    \label{archi}
%    \end{figure*}

%approach, that iteratively filters the generated rules and data . 
%\our's novelty lies in effectively filtering automatically induced rules and synthetically generated data iteratively via bootstrapping  




%The gold standard few shot labeled data forms the seed for the data augmentation step. However, it forms the validation data during rule induction and filtering. 




%Starting with a seed set of a k-shot labeled dataset, we induce rules and perform synthetic data generation using prompt demonstration. While the synthetically generated training data points are label-specific, they further undergo filtering using the rules induced. Similarly, once new data points are filtered they are used for further rule induction and filtering. The rule and synthetic data generation is performed iteratively.  Further, a pre-trained model is then undergoes continual pretraining and fine-tuning using contrastive learning techniques \cite{zhang-etal-2022-contrastive}. Finally, we use data programming \cite{maheshwari2021semi} to learn a generative label aggregation model by using the final set of rules of labeling functions. Finally, we perform joint learning  of the final classifier using the label aggregation model and the contrastively tuned pre-trained model.

%Using the rules, we predict a label for the generated data, and filters only those that agrees on the labelusing the induced rules.  and filter only those augmented datapoints that match
 %Three, the joint learning step, effectively combines contrastive representation learning, \cite{pmlr-v119-chen20jsimclr,NEURIPS2020_supervisedContrastive} supervised fine-tuning, and Data programming \cite{zhang2022survey} using a joint learning framework \cite{maheshwari2021semi}. We perform self-supervised contrastive pretraining \cite{wu2020clear} and supervised contrastive learning \cite{khosla2020supervised, zhang-etal-2022-contrastive} over a standard pre-trained neural classifier. We use the few-shot labeled data, along with the filtered data, for fine-tuning the neural classifier. 
 
 
 %The induced rules are used as a form of weak supervision to learn a generative modellabeling-functions in  to learn a  as a . The neural claUsing the induced rules we learn a generative model The classifier is learned jointly with a The induced rules SPEAR is a framework for Data Programming. .and  Further, we use , a framework that enables to learn both the models jointly.

In \our, we boost supervision signals in two ways. With synthetic data generation we supplement the available training data \cite{DBLP:journals/corr/abs-1711-10160ratner,pryzant-etal-2022-automatic}. First, with rule induction, we obtain complementary signals that need not be explicitly captured from the existing data \cite{maheshwari2021semi, singhal-etal-2023-intendd}. Second, our rules are induced as generalized syntactic n-grams. Here, we aim to potentially capture morpho-syntactic information from the data, a view of data that need not be explicitly captured by state-of-the-art (SotA) systems in use. A classical NLP pipeline typically represents a string at multiple levels of abstraction which includes Part-of-Speech (PoS) tags, syntactic relations, \emph{etc.} \cite{manning-etal-2014-stanford}. \our~uses higher-order dependency structures as features and generalizes over these features using inductive generalization \cite{popplestone1970experiment} to induce the rules as generalized syntactic n-grams. 
 
%Previous works used rules for boosting supervision signals in various ways. Rules are used as an auxiliary source that can add more labeled data, albeit being noisy . Alternatively, rules can be used  Our generated data brings in auxiliary sources of information. The rules we generate are used to improve efficient utilisation of the available data during the learning. 
  %Weak supervision sources such as rules are often used as auxiliary sources of information that brings in additional information not fully captured in the available training data. Similarly, it can be used for maximising utility of the available data and complement the labeledd data. \our~ achieves both 
 

 

We find applicability of both the rules and exemplars from \our, with consistent performance gains in various text classification setups. Specifically, we experiment with ICL and fine-tuning setups. In ICL, we focus on long-context ICL \cite{li2024long,bertsch2024context} and use the generated data as a pool from which exemplars are retrieved. Further, we incorporate our rules as explanations to the input and the exemplars. Similarly, we use the data for fine-tuning models, which include pre-trained LLMs, Qwen \cite{Qwen2,Qwen2.5} and RoBERTa \cite{liu2019roberta}. %The induced rules enable learning a generative label aggregation model as a form of weak supervision using data programming. We jointly learn a classifier with the generative model using SPEAR \cite{maheshwari2021semi}, a data programming framework.

We perform extensive experiments on multiple text classification datasets, which include three full-shot, and eight few-shot datasets from the {\sc FewMany} benchmark \cite{yehudai2024llms}. Further, we perform multilingual experiments on seven languages using the {\sc MASSIVE} \cite{fitzgerald2022massive1mexamplemultilingualnatural} dataset. 
%Our experiments are performed using both 5-shot and 10-shot settings. In all these settings, \our~outperforms strong competitive models, such as IntenDD \cite{singhal-etal-2023-intendd}, \citet{zhang-etal-2022-contrastive}, and  FastFit \cite{yehudai2024llms},  with statistically significant improvements. 

%In section \ref{sec:genSpace}, we elaborate on our rule induction approach for inducing generalized syntactic n-grams. In section \ref{sec:method}, we elaborate \our, a 3-step framework for FSTC. Here, we elaborate our iterative rule and data filtering along with the joint learning setup. 


Our major contributions are as follows:




\begin{itemize}
    \item Use of rules and data from \our~results in statistically significant gains in all the experimental setups, as compared to the corresponding configuration without resources from \our. Specifically, we obtain state of the art (SotA) results in our full-shot and few-shot experiments when using \our.

    \item The rules we generate are shown to be effective, both during ICL and fine-tuning. Further, using the rules as explanations under ICL for CDR dataset results in SotA results. Similarly, fine-tuning Qwen jointly with data and the augmented rules from \our~has shown statistically significant improvements for Qwen and RoBERTa based models.

    \item Use of augmented data for few-shot setups in the {\sc FewMany} benchmark demonstrate the quality of the augmented data we produce. We show that simply using additional data from \our, as low as 20-shot additional data per class, can result in improved performance than incorporating complex approaches such as contrastive representation learning into the training process. 
    
    %Our proposed approach yields statistically significant gains in all the experiments we perform, compared to state-of-the-art systems \cite{yehudai2024llms, singhal-etal-2023-intendd, zhang-etal-2022-contrastive}. Our best performing model reports a 2.04\% increase in 10-shot and 2.52\% increase in 5-shot settings, compared to the next best model, averaged across all the monolingual tasks.

    \item Our extensive experiments show that \our~is generalizable across multiple domains and multiple languages. We report a 7.21\%  increase in performance, compared to the model without any resources from \our, averaged across seven different languages.    

    \item We show that leveraging syntactic information as weak supervision for rule induction, brings a complementary source of supervision, which otherwise need not be captured by using string level data directly (\S \ref{ruleImpact}).
    
    %performance improvements compared to surface-level string n-grams as rules. Further, our bootstrapped approach outperforms competitive approaches for filtering augmented data \cite{lin-etal-2023-selective}. 

    

\end{itemize}


%show that we can complement these  models with relevant information by leveraging a combination of approaches that include contrastive learning, data augmentation, weak supervision, data subset selection and automatic rule induction for learning models in few shot settings. Further, by using automatic rule induction we learn rules that use syntactic information. Moreover, we show that these information can help improve the performance of these pre-trained deep-neural models. %are there is information which can be utilized?




%Our framework employs the following learning approaches for a general few shot text classification approach. Similar to several other few shot text classification approaches popular in intent classification, we perform contrastive learning for better representation learning. Here, we first perform self-supervised pretraining followed by supervised contrastive learning. Two, we propose an automated approach for inducing `rules' in the form of generalised syntactic n-grams. Three, we synthesise new labeled samples via data augmentation. Here, we use the few shot labeled samples as input to an LLM for data augmentation. Three, we filter the augmented sentences using a data subset selection approach. Finally, we use data programming by using the automatically induced rules as labeling functions to learn a generative model. We then learn an intent classifier using our contrastively trained model jointly learning with the generative model built.rules' from a corpus by generalizing over a feature space that consists of .\our~focuses on  over the concept space of these abstractions in the pipelineIt 


%Further, we use least general generalization \cite[LGG][]{plotkin1971further, plotkinnote} to obtain the generalised syntactic n-grams as our rules. T 


%Moreover, these rules can be integrated to pre-trained models during the training via programmatic weak supervision (PWS). Specifically, a feature space over higher-order dependency structures from dependency parses of the inputs, akin to syntactic n-grams, is defined. The features are enumerated and scored using  labeled data. The entire corpus, consisting of both labeled and unlabeled data, is partitioned using these features. For each such partition, we obtain a generalization of the features which form the rule. T. The classifier is trained jointly with a label aggregation model in a semi-supervised setting.   

% Now, given that there are several approaches by which supervision can be incorporated with the limited data. Now, in-context learning or parameter updation approaches, such as fine-tuning, have been two primary categories to address such  tasks. So far, . However, it is still entirely not clear whether these systems capture all the relevant information for the task. A more interesting question would be, is it possible to capture information  that can be complementary to what these systems learn and use it to improve the performance of these systems? \our~ proposes a general-purpose rule induction framework for semi-supervised text classification. 


 %In \our, we use a combination of the entries at various levels of this abstraction as the generalization. 
%For instance, the two phrases `brown foxes' and `brown cats' are different at the string level. Further, their corresponding dependency representations, which we use as features, are also distinct; `brown $\xleftarrow{amod}$ foxes' and `brown $\xleftarrow{amod}$  cats'. However, they both can be `generalized' into a single structure, written as `brown $\xleftarrow{amod}$ {\sc Noun}', where $amod$ is the dependency relation between both the words and {\sc Noun} corresponds to the POS tag for common nouns. The generalized structure forms a rule in \our.


%The structure, `brown $\xleftarrow{amod}$ {\sc Noun}' is not just a generalization of the aforementioned pair of strings, but it is representative of any string containing a common noun with brown as its adjectival modifier. Similarly, we may consider  `{\sc ADJ} $\xleftarrow{amod}$ {\sc Noun}' also as a generalization of the aforementioned pair of strings, which is more general than the former and it covers a wider set of strings, i.e. any pair of common nouns with exactly one adjectival modifier. Here,  we need to identify generalizations that are of utility for a classification task as compared to the possible over-generalizations. Inductive Logic Programming (ILP) has been extensively used in the past in identifying generalizations over such concepts. Specifically, we employ least general generalization \cite[LGG;][]{plotkinnote,plotkin1971further} over the partition of the dataset encoded in the feature space. 




%We observe consistent increase in performance for each of the datasets


%LGG is performed over a partition of the input dataset, and the quality of the rules obtained is highly dependent on the quality of the partition. \our~employs a modularity-based community detection approach, Louvain, to partition the dataset where similar inputs in the 





%However, we need to identify 


%Systems such as GOLEM \cite{muggleton1992efficient}, {\sc Chillin} \cite{zelle1995inducing} are ILP approaches that use variants of least general generalization. 

%It integrates diverse areas in NLP in its framework to enable rule induction. Higher-order dependency features are used for feature space construction, 



%incorporate these rules as weak labelers or rather as labeling functions using PWS enables the integration of various sources of weak supervision, albeit noisy, in the form of programs that enables ameliorating the need for hard data labeling. Specifically, we use the semi-supervised data programming framework by Spear where the loss function integrates predictions from the rules and from the neural classifier. 

%While programmatic weak supervision enables to integrate arbitrary rules to be integrated into a learning system, those typically are expected to be heuristics hand-crafted by subject matter experts or are obtained from existing resources. Hence any arbitrary Python function that returns a label can be used as a labeling function and no further constraints exist for the type of function. However, since we automate the rule generation, the rule generation needs to follow a formalism or rather the rules generated have to stick to a constrained formalism. In our case, we experiment with two different approaches. One generates a restricted form of horn-clauses using path-constrained random walks, and two, uses least general generalization a form of abstracting out a generalized representation in the same scheme as the input. 



%We show that our approach can outperform previous state-of-the-art approaches in similar settings and further, we can integrate our approach with an LLM of 7 billion parameters which can further improve our performance. Here, we use adapters to improve our models which enables parameter-efficient training of these moddels. 