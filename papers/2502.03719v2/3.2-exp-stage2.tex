

\section{Stage Two: Model Interpretability}
The second stage of our study focused on enhancing user control over the model interpretation of sketches by providing different types of brushes for sketching and adding feedback to convey the model's interpretation of the sketches. 
% This aimed to improve user understanding and control during the sketching process.

\subsection{User Interface}
\begin{wrapfigure}{l}{15mm}
\vspace{-3mm} \includegraphics[]{figures/layers/stage2-icon.pdf}
\end{wrapfigure} 
The system was enhanced with three major features to facilitate the above focus of our second-stage study. 
First, \textit{command brushes} were introduced to allow programmers to convey their intentions more precisely. For example, a ``replace'' brush can instruct the model to limit its interpretation to replacing existing code with the users' sketches (\autoref{fig:second-interface}a).
Second, the underlying model interpretation mechanism was modified to recognize, group, and interpret sketches. The system groups semantically related sketch marks each time the pen is lifted and provides reasoning for the actions it interprets for each group (\autoref{fig:second-interface}b). These descriptions are displayed as tooltips next to each sketch group, allowing users to edit the descriptions to refine the interpretation or commit to executing the actions.
Third, an inline diff view was added to the code editor, enabling users to visualize code changes as staged edits and choose to accept or reject these changes (\autoref{fig:second-interface}c).




\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/interface/2nd.pdf}
    \caption{The interface design from the second stage. (a) Command brushes used to steer AI model's interpretation, including reference, delete, add and replace; (b) interpretation of sketches displayed as tooltips, programmers can click on the preview (recognized sketches) and see the full description of AI's reasoning of actions; (c) programmers can click on the commit button to execute the actions, edited code will be shown in diff view and programmers can accept/reject it.}
    % \dv{this one is kind of hard to interpret since the "overview" is so large. Ideally all three of these UI figures should follow exactly the same pattern. Smallish full screen cap always same size across three figures, then zoomed in callouts around it. if you need space above it and the figure becomes tall, that's ok}
    % \dv{all your a, b, c, and your annotation text should really be equivalent to about Arial 8pt at most and 6pt at min.  The a, b, c in this figure are really big, I bet about Arial 12pt. This is likely because you use Keynote, so you have no idea about the physical size of your figure in mm ... it's just relative size guesswork.}
    \Description[Stage 2 interface with a code editor, AI-assisted features and sketch interpretation.]{The figure showcases the interface design from stage two of the study, illustrating an AI-assisted Python code editor for the NearestNeighborRetriever class. The interface highlights several interactive features: (a) Command Brushes: Located at the top right, labeled "Reference," "Add," "Delete," and "Replace," these tools allow users to guide the AI’s interpretation and actions within the code. (b) Tooltip Display: A tooltip is shown, presenting the AI’s interpretation of a sketched annotation, including an option to view the full reasoning behind the AI's suggested changes. For example, a tooltip highlights a change suggesting converting a variable to a string type. (c) Commit Button and Diff View: Programmers can click the commit button to execute the AI's suggestions. The modified code is displayed in a diff view with options to accept or reject individual changes. The interface also includes line numbers, syntax highlighting, and handwritten annotations such as "str" near the code. A task description at the top prompts users to "Implement the Manhattan Distance Metric," guiding the focus of the current programming task. This design facilitates a collaborative and iterative code development process between the user and the AI system.}
    \label{fig:second-interface}
\end{figure*}


\subsection{Participants, Tasks, and Procedure}
Six new participants were recruited through convenience sampling, with all right-handed, 2 identified as women and 4 as men. All participants had 3-6 years of programming experience in Python and had used ChatGPT or Copilot 6-14 times per week. The same scenarios and tasks were used with the same procedure and data collection. 
We applied inductive thematic analysis to the observational notes, screen recordings, system logs, captured sketches, and interview notes to identify common types of model interpretation errors, the strategies participants used to recover from these errors, and insights related to the model’s interpretation.



\subsection{Results}
Four participants did not complete one of the four assigned tasks from either \f{scenario 2} or \f{scenario 3}. This incompletion was acceptable, as our primary objective was to understand how participants recovered from interpretation errors, rather than task completion itself. We identified a total of 66 error and recovery scenarios, categorized into six distinct error types (\autoref{tab:error-strat}). Participants employed six different repair strategies following three major actions: rejecting/accepting code edits, or taking no action.

\subsubsection{Feedback on Model Interpretation}
Most participants (5/6) appreciated having an interpretation as a preliminary step before code edits. They noted it is necessary when code changes went wrong (5/6), when they were unsure about how the code should be implemented (4/6), and when tasks required decomposition (2/6). However, most of the time, they could rely on the code diff view as it indicates the model's interpretation, especially if their sketches included pseudocode. They expressed that the interpretation feedback should include the recognized items and text within the annotation (6/6), the model's recognition of non-textual annotations and sketches (5/6), suggestions for code edits (3/6), and the linkage between sketches and code edits (2/6).



\subsubsection{Common Errors and Strategies to Repair}
Of all sketches, $23.2\%$ required iterations due to model interpretation errors.
Common errors (\autoref{tab:error-strat}) included mismatches between code implementation and user expectations, incorrect interpretation before code edits, wrong recognition of sketches, no code edits being made, incorrect scope of code changes, and wrong modified code syntax causing runtime errors.
The most frequent error was code mismatches, which were detected after code edits, P10 questioned whether the error happened \pquote{because my drawing was not clear enough or my [written pseudocode] was not recognized.}
The second and third most common were incorrect interpretations of user actions and recognitions, which participants could identify before any code changes occurred. In these cases, some participants (4/6) chose to refine their sketches before generating the code edits, while two participants occasionally still pressed the generate button, P11 explaining this due to \pquote{not knowing how should I refine the sketches.}


In most cases, participants attempted to repair errors by redrawing sketches. In two instances, they edited the code directly using the tablet's keyboard, in three cases they adjusted the interpretation, and in four cases they used control brushes. However, participants only used the control brush when redrawn sketches were still not recognized. P9 explained, \pquote{I would think that it's because of the recognition error or code referencing error, then realize it's misinterpreting what I want to do.}
Overall, these repair strategies can be categorized into three types: selection, instruction, and target. These included adding textual instructions, adding annotations, removing unnecessary sketches, rewriting pseudocode, adding code syntax, and adding references pointing to other target code (\autoref{tab:error-strat}).
For instance, P9 changed the written text from \pquote{handle} to \pquote{def} to specify that the handling should be implemented in a new function. 
Some participants redrew sketches to prevent new annotations from obscuring the code when no sketches were detected, suspecting that \pquote{the sketches blended into the code}{P11}. \rev{This concern is valid, as the system overlays the sketch layer on top of the code layer in the pictorial form to associate sketches with specific lines of code during interpretation.}
All participants used strategies such as adding code targets and references to \pquote{make sure correct code is being used}{P7} or \pquote{only changing specific area [of code]}{P8}.
For instance, P8 circled the DataProcessor class to ensure that new code edits were implemented as methods within the class rather than as standalone functions outside it.



\begin{table*}[hbt]
    \caption{Results from stage two showing AI interpretation error types and corresponding participant repair strategies.}
    \centering
    \input{tables/error_strat}
    \label{tab:error-strat}
\end{table*}




\subsubsection{Control Over Model Interpretation}
Most participants (5/6) did not find control brushes particularly useful. Two participants preferred interacting directly with the code editor rather than using specific brushes to constrain the AI model. They favoured simple arrows and cross-outs to indicate code replacements instead of different brushes. All participants found that sketches alone were expressive enough to guide the AI model in correcting its interpretation.
For example, P10 added a numbered label to the pseudocode, \pquote{$\rightarrow$ str:}, to indicate that the AI should prioritize interpreting that annotation first \includegraphics[height=14pt]{figures/small_examples/p10_1_correct_model.jpg}.
We observed that three participants tended to wait for the interpretation to complete before generating code edits to \pquote{not lose control over my [their] own code}{P9}.




\subsubsection{Sketching, Correcting Model, and Editing Code}
All participants primarily attributed their frustration with iteration to the need for context \pquote{switching between the code editor and the canvas}{P9}. The interface required a double-tap to enter or exit the code editor for tasks such as accepting/rejecting code edits, undoing/redoing actions, and performing manual edits (though these were less common). Due to the dynamic nature of programming, where each edit builds on previous modifications, the frequent need for interpretation and the requirement to accept or reject code edits often disrupt participants' flow. Consequently, most participants (5/6) preferred to complete all sketches first and then use the ``generate'' button as a clear boundary between debugging and sketching modes, avoiding repetitive context switching.

This context switching also involved changing mental models and using different input modalities, leading to errors. For example, some participants (3/6) frequently selected the wrong tools due to overlapping semantic meanings, such as P11 using the eraser to delete code or the pointer to select code. These findings highlight the importance of enabling interactions with the code editor ``through'' the canvas layer, effectively translating certain canvas layer interactions into actions within the code editor layer.

% common interaction on code editor: accept/reject changes, select, delete

\subsection{Summary}
% The second iteration examined how participants responded to the AI's interpretations and errors through the feedback provided and the use of control brushes. 
While feedback on AI interpretations added value, the method used in this stage disrupted the programmers' flow. The goal of code shaping is to allow programmers to edit code structure through sketches, rather than engage in low-level code editing or prompt engineering for the AI system. The control brushes did not perform as expected; participants preferred refining their sketches by adding more code references or employing code syntax to shape the outcome. This tendency can be linked to the cognitive dimension of \emph{premature commitment}---forcing programmers to make decisions too early~\cite{green1996usability}, which conflicts with the iterative nature of code shaping. The findings underscore that the key to facilitating code shaping is an interaction design that minimizes the conceptual layers between the code editor and the sketching canvas.

