\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures//interface/1st.pdf}
    \caption{Interface design from the first stage. (a) Pen tool with color options for code annotation; (b) Canvas tools including select, pan, pen, eraser, and other common shapes; (c) AI-powered ``Generate'' button for translating sketches to code edits; (d) ``Run'' button executes Python code and displays output in the console.}
    % \dv{This is absolutely fine, not need to change. ... but I was imagining a much smaller overview of interface with all elements visible (like half the size you have). Then all the interesting stuff is in zoomed in detail shots with callouts to where they come from. Right now (b) isa little unclear where it is in the interface, and the important stuff in (a) is still on the small side.  }
    \label{fig:first-interface}
    \Description[Coding interface with annotation and execution features.]{The figure presents a coding interface design with annotation and execution features. The main area displays Python code implementing a NearestNeighborRetriever class, featuring methods for Euclidean and Manhattan distance calculations and a nearest neighbor search. (a) Blue handwritten annotations, such as "def one-hot," are visible directly on the code. (b) A toolbar below the interface offers tools like select, pan, pen, eraser, and shape drawing, along with a color palette for annotations displayed as colorful dots to the right. (c) An "AI-powered Generate button" appears near the top, used for translating user sketches into code edits. (d) The "Run" button executes the code and shows the results in the console, displayed in the bottom right. The console output includes nearest neighbor results based on Manhattan distance. This setup demonstrates an interactive workflow where users annotate, generate, and execute code within an accessible interface.}
\end{figure*}

\section{Code Shaping}
The code shaping concept \emph{enables programmers to edit code using freeform sketches directly on or around the code}. This approach includes three core elements: a sketching canvas, a responsive code editor, and an AI that interprets sketches to generate code edits.

In a code shaping session, programmers sketch their intended modifications on an invisible canvas overlaid on the code. These sketches can include arrows pointing to specific lines, pseudocode defining a function's structure, and annotations indicating desired changes. The sketches can interact with any part of the code and output in the console and graphical view.
Once sketches are made, users can press a button to prompt AI to interpret their sketches along with the code. 
If the resulting code does not match the programmer's intent, they can refine their sketches, creating an iterative cycle of input and feedback.
% This feedback loop between the canvas, code editor, and AI interpretation is crucial to the concept of code shaping. 
This feedback loop allows programmers to use sketches progressively and iteratively to \textit{shape} how the code should be structured, how it should flow, and how it should function, guiding it towards the desired form and functionality.

In the following sections, we describe a series of three design studies (stages) to develop a proof-of-concept system and interface for the core code shaping interactions.
The first stage examined the types of annotations used in code shaping. The second stage focused on exploring model interpretation errors and the strategies programmers employed to address them. The final stage synthesized prior stages' insights, aiming to coordinate the interactions when editing code, iterating with AI, and sketching on the canvas. 
% This stage introduced predefined gestures for interacting with the code editor via the canvas without context switching and incorporated feedforward mechanisms to reduce the need for iterative communication with the models.

% Originally labelled ``Generate'' to signify code creation, the button was later renamed to clarify that a change will be applied to the code. If the output does not match the programmer's intent, they can refine their sketches, creating an iterative cycle of input and feedback. This feedback loop between the canvas, code editor, and AI interpretation forms the core interaction necessary for a system that supports the concept of code shaping.
% \jz{It would be nice to run these abstract interactions with an example.}



% to explore how sketches are created, how AI interpretations influence the process, and how to bridge the interaction paradigm of editing code and sketching.



% \begin{figure}[htb]
%     \centering
%     \begin{minipage}[b]{0.31\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/interface/first_stage_interface.jpeg}
%         \caption{First stage interface}
%         \label{fig:first-stage}
%     \end{minipage}
%     \hfill
%     \begin{minipage}[b]{0.31\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/interface/second_stage_interface.png}
%         \caption{Second stage interface}
%         \label{fig:second-stage}
%     \end{minipage}
%     \hfill
%     \begin{minipage}[b]{0.31\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/interface/third_stage_interface.jpeg}
%         \caption{Third stage interface}
%         \label{fig:third-stage}
%     \end{minipage}
% \end{figure}



% Subsequently, we demonstrated the system's application in real-world scenarios by conducting code reviews with N=\x{} software developers.
% The system's effectiveness was validated through user studies involving N=\x{} participants. 
% The final study demonstrated the applicability of the code shaping concept in practical use cases with expert developers.

% This study aims to explore the use of digital ink sketches to specify code edits beyond syntactic changes, identify common patterns in programmers' sketches, and uncover challenges and strategies in this approach. 

\section{Stage One: Explore Sketches} 
We developed a basic user interface to explore how participants used sketches as actionable commands for code edits. We observed and categorized participants' challenges and sketch types, providing foundational insights for the code shaping system's development in subsequent stages.


\subsection{User Interface}
\begin{wrapfigure}{l}{15mm}
\vspace{-3mm} \includegraphics[]{figures/layers/stage1-icon.pdf}
\end{wrapfigure} 
For this stage, the user interface creates a straightforward way to make free-form sketches in the \textit{canvas layer} to directly generate code edits affecting the \textit{code layer} while keeping the \textit{AI layer} hidden to the user.
% \dv{prev sentence is explanation of conceptual layers and flow if UI which is represented by "icon" diagram at left. We should follow same pattern for sentence 1 in stage 2 UI and stage 3 UI}
% The first design integrates a code editor within a canvas environment to enable code shaping. 
The interface supports typical free-form sketching tools, including colour selectors, pens, erasers, and shapes (\autoref{fig:first-interface}a-b). A text tool is available for conventional editing. 
Two-finger panning and zooming navigate the code in the editor to enable sketching at different levels of granularity. A pointer tool can select strokes in the sketches. Pressing a ``Generate'' button uses all annotations on the canvas, or only selected annotations, as parameters for generating edited code (\autoref{fig:first-interface}c). 
The system recognizes free-form annotations on the code editor, utilizing GPT-4o to generate corresponding code. 
We render HTML content from both the code editor and sketches onto separate canvases and embed these canvas content into an SVG. This transformation process includes handling CORS and tainting issues, adding grids to locate annotations, and turning the code editor to grayscale to highlight the sketches.
The system then considers the annotations alongside previous
\rev{version history, including pictorial representations of sketches, code snapshots, conversational context, natural language inputs, and modified code outputs. The stored history was used to contextualize model responses and maintain a comprehensive context of the evolving codebase.}
% iterations of sketch editing and the relevant codebase as part of the input context for code generation.
After the code is generated, a difference algorithm is employed to 
only update the changed sections of the code~\cite{myers1986nd}.
The user can press a ``Run'' button to execute the code, with text or image results shown in the console panel underneath (\autoref{fig:first-interface}d). Programmers can annotate any output on the console or graphical windows as part of their sketches. \rev{The system incorporates these annotated outputs by transforming them into separate canvases, embedding them as SVGs alongside the code editor content, and encoding them for processing.}



\subsection{Participants, Tasks, and Procedure}
We recruited 6 programmers (1 left-handed), aged 23 to 28, with 4 identifying as women and 2 as men.
Participants were recruited through convenience sampling and received \$30 for completing the study. Based on a screening questionnaire, participants had 2-8 years of programming experience in Python and had used ChatGPT or Copilot 3-12 times per week.

We developed three Python coding scenarios, each comprising two tasks that required specific edits to achieve predefined goals. These scenarios spanned different programming paradigms: basic object-oriented programming, functional programming for machine learning, and declarative programming for data engineering. Each task provided participants with starter code requiring modifications in multiple areas. For instance, \f{scenario 2} involved extending a class to handle categorical features in data points, necessitating changes to existing methods for feature encoding and distance calculations. All tasks were pre-tested to ensure that GPT-4o could not immediately generate the correct code.

Participants were assigned 2 out of 3 scenarios that they were most familiar with, as determined by their screening questionnaire. Each scenario consisted of 2 tasks, and participants completed all 4 tasks (2 \f{scenarios} \by 2 \f{tasks} each) within a total of 60 minutes, spanning around 12 to 16 minutes per task. The order of scenarios and tasks was pre-assigned, meaning participants completed all tasks within one scenario before moving on to the next scenario. 
The study was conducted in person using an Apple iPad Air (5th generation, 10.9-inch display) as the primary research tool. An experimenter was present throughout each session to observe and document participant behaviours.
% Afterward, there was a post-study survey, including UMUX-LITE, NASA-TLX, and a 7-point Likert scale questionnaire \jz{questionnaire data is collected but never analyzed or reported. why do we need this then? better just remove this sentence} evaluating factors such as the clarity of mapping between sketches and edited code and the ease of iterating on sketches. 
Finally, a semi-structured interview gathered qualitative data on participants' general experience, challenges encountered, and suggestions for system improvement.

% \jz{Maybe I miss it. What is the device the participants used during the studies? An iPad, a touch monitor, and a wall-size display are all different and will affect the results. This comes up when I skimmed the limitations, as not exploring it on different form factors of the devices can be a limitation.}

% \begin{enumerate}
%     \item Task Management System (Basic Python)
%     \begin{itemize}
%         \item Sort Tasks by New Attribute "Due Date". Participants add a new attribute due\_date to the Task class and implement a method in the TaskManager class to list tasks sorted by due date.
%         \item Allow User to Update Task Details and Delete Tasks. Participants modify the Task class to include setter methods for updating task details and add a method to the TaskManager class to delete tasks.
%     \end{itemize}
%     \item Enhancing a Nearest Neighbor Retriever
%     \begin{itemize}
%         \item Add Support for Manhattan Distance. Participants implement the Manhattan distance metric and update the NearestNeighborRetriever class to use this new metric.
%         \item Add Support for Handling New Data Types. Participants extend the class to handle new types of data points, specifically those with categorical features, by implementing one-hot encoding and modifying the distance calculations.
%     \end{itemize}
%     \item Data Imputation and Feature Engineering
%     \begin{itemize}
%         \item Impute Missing Data with Average Feature Value \& Feature Engineering for Quadratic Terms. Participants add functionality to the DataProcessor class to impute missing values with the average value of the respective feature and create new features that are the squares of existing features.
%         \item Visualize Data Distribution and Implement Feature Scaling. Participants implement methods to visualize the data distribution and perform feature scaling using MinMaxScaler.
%     \end{itemize}
% \end{enumerate}




\subsection{Data Analysis}
We conducted an inductive thematic analysis of participant-generated sketches. This analysis incorporated observational notes, screen recordings, transcribed think-aloud data, and interview notes. 
Sketches were automatically captured in base64 format each time the generate button was activated, yielding 81 distinct screenshots. 
Of these, 7 were identified as duplicates and subsequently removed from the analysis.
We developed a codebook covering five dimensions: Content (text, code, annotation, freeform), Approach (step-by-step, one-time), CodeReference (parameters, targets), Purpose (functional, procedural), and Form (concrete, abstract). All captured sketches were verified and coded by researchers together. The results and descriptions of each coding category are presented in \autoref{tab:first_code}.

% \jz{here it seems only sketches are analyzed, however, there are participants' quotes and researchers' observational notes when reporting the results. You many want to briefly mention how these data sources were analyzed.}

% \jz{The discussion of the results below seems not to leverage table 1, if it is the main result. I appreciate the different themes obtained and presented below, but I have a hard time connecting the dots and understand how they contribute to your study goal. These results seem scattered. Is there a way to tie them in a narrative?}

\begin{figure}
    \centering
    \includegraphics[width=.57\linewidth]{figures/quadrant.pdf}
    \caption{The classification of sketched annotations from participants situated in a quadrant with two spectra, Abstract-Concrete and Procedural-Functional.}
    \Description[Quadrant diagram classifying sketched annotations.]{The figure presents a quadrant diagram categorizing sketched annotations based on two axes: Abstract-Concrete (vertical axis) and Procedural-Functional (horizontal axis).1) The upper-left quadrant (Abstract-Procedural) contains Python code snippets for listing tasks, annotated with handwritten notes such as "list tasks" and arrows connecting functions; 2) The upper-right quadrant (Abstract-Functional) features a hand-drawn graph labeled "visualize data" alongside code suggesting data visualization processes; 3) The lower-left quadrant (Concrete-Procedural) includes a handwritten note "def update_task" with an arrow pointing to a list structure, suggesting task update functionality; 4) The lower-right quadrant (Concrete-Functional) displays a code snippet for manipulating a dataframe column, accompanied by a handwritten comment describing the implementation of a feature scaling method. This diagram illustrates the interplay between abstract and concrete ideas as well as procedural and functional approaches in annotating and conceptualizing code.}
    \label{fig:classification}
\end{figure}

\begin{table*}[]
    \centering
    \include{tables/sketch_code}
    \caption{Types of sketches used in stage one were categorized, and the number being coded was indicated by $N$.}
    \label{tab:first_code}
\end{table*}



% \begin{figure}[th]
%     \centering
%     \begin{minipage}[t]{0.34\textwidth}
%     \end{minipage}%
%     \hfill
%     \begin{minipage}[t]{0.64\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/frequency_heatmap_logtype.pdf}
%         \caption{\f{stage1} The frequency of log types aggregated per minute throughout the study time per participant per task.}
%         \dv{ I find this graph hard to interpret, it's overly dense with information you don't focus on, and it takes up space. I think say everything about this quite clearly in text at start of 4.4. Suggest removing this figure }
%         \dv{If you want to keep it, then: If you normalize the time, it's no longer in "minutes", right? This is just a "normalized timeline".}
%         \Description[Heat map showing frequency of log types over time.]{The image displays a heat map representing the frequency of different log types over time during a study. The y-axis shows four log types: Run Result, Run Error, Sketch & Generate, and Code Change. The x-axis represents normalized time in minutes from Start to End (1-16). The frequency is indicated by color intensity, with darker blues representing higher frequencies. The heat map shows varying patterns of activity across different log types and time periods, with Code Change appearing to have the highest overall frequency.}
%         \label{fig:freq-log}
%     \end{minipage}
% \end{figure}


\subsection{Results}
\label{sec:result}
% overall pattern
% alignment with AI
    %  moviing together with AI
% control
% sketches classification
% sketches as a `tool'
    % the reusability of sketches

% There are 6 \f{participants} \by 2 \f{tasks} \by 2 \f{subtasks} = 24 data entries collected. 
% Overall, 22 out of 24 tasks (6 participants $\times$ 2 tasks $\times$ 2 subtasks) were completed. 
All but two participants completed the four assigned tasks; these two participants did not complete \f{scenario2}-\f{task2} within the assigned time.
% \jz{add a sentence saying that task completion is not our concern}
There was a concern that experienced programmers might be strongly biased toward typing code edits, which could impact their experience with sketch-based code editing. However, all participants appreciated the concept and expressed a willingness to integrate it into their current programming workflow, as it allowed them to \pquote{think deeper about the code}{P1} and \pquote{focus on higher-level planning}{P6}.
Participants used sketching to edit code an average of 3 times per subtask ($SD=4.0$). \rev{Each instance of sketching often included multiple annotations, with some sketches encompassing edits to several parts of the code.}
Early-stage code edits were primarily made through sketched annotations, but in the later stages (12-13 minutes), edits occurred without sketches, suggesting the use of a keyboard or undo/redo mechanisms to refine code. P2 and P4 explained that they resorted to the native tablet keyboard for edits to handle low-level details, as the waiting time for model interpretation \rev{could exceed five seconds (in average around 4-8 seconds based on the size of the codebase) in some cases, making manual code changes faster}, thus \pquote{would rather do it myself [themselves]}{P4}.

\subsubsection{General Workflow} 
Participants sometimes wrote higher-level instructions first when unsure about the solution but had a rough idea of where the code edits should happen and what the \pquote{shape of the code looks like}{P4}. 
After evaluating the edited code, they then added annotations for lower-level code editing based on their approaches in mind.
We also observed two participants gradually develop a personalized workflow for editing code with sketches. P2 found that breaking down tasks into very low-level details was ineffective and not necessary, while P5 emphasized the need for smaller task pieces for better system understanding. \rev{This difference arose because P2 included precise code-like keywords in their sketches, minimizing the need for further detail.}



\begin{figure*}[th]
    \centering
    \includegraphics[width=\linewidth]{figures/arrow_variants.pdf}
    \caption{Sketches from stage one showing how participants employ arrows ($\rightarrow$) for different purposes, including command (the intended action of operation), parameter (supplementing the command), and target (the area where the edit should occur); (a) indicating procedural flow between commands; (b) referring to data attributes; (c) modifying a function, with the function as the parameter to supplement the command; (d) applying changes to a target area.}
    \Description[Data collected from the user study showing how participants employ arrows for different purposes]{The figure depicts sketches from a user study, showcasing how participants use arrows to indicate different operations in a code editor. The arrows are employed for three distinct purposes: (1) command, representing the intended action or operation; (2) parameter, supplementing the command; and (3) target, pointing to the area where the edit should occur. The figure is divided into four labeled sections: (a) Procedural Flow: Displays arrows connecting commands, such as "due_date" and "sort_by," to illustrate the procedural flow of tasks; (b) Refer: Shows arrows linking parameters like "attributes in Task" to commands, highlighting how participants refer to specific data attributes; (c) Modify: Demonstrates arrows pointing to a function like add_features, with handwritten annotations such as "modify this" and "I don’t want to square the log," reflecting user-directed modifications; (d) Apply: Includes arrows connecting a command (proprocess) to its target, emphasizing changes being applied to a specific section of the code. This figure captures the interactive coding process, with annotations providing insights into the participants' thought processes and actions during the study.}
    \label{fig:arrow-variants}
\end{figure*}



\subsubsection{Types of Sketches}
Overall, the sketches can be situated in a quadrant with two spectrums (\autoref{fig:classification}): \f{Abstract}-\f{Concrete} and \f{Procedural}-\f{Functional}.
The \f{Abstract}-\f{Concrete} spectrum describes whether the annotations are abstract symbols or graphs versus concrete written text. The \f{Procedural}-\f{Functional} spectrum classifies the target of the annotations, ranging from procedural steps describing how the program should be structured to functional descriptions specifying how the program should work. Participants often combined these aspects, drawing graphs and adding arrows to refer to certain data attributes, specifying both functional and procedural terms.

\subsubsection{Sketch as a Tool} 
Additionally, we observed that participants considered sketches as functional ``tools'' that could be reused~\cite{renom2022exploring}, not just as transient digital ink drawings. All participants expressed that they could use different sketches to achieve the same effect, choosing which sketch to use based on the environment, such as available white space. They also reused their sketches to convey the same effect; for instance, an arrow used to insert a function into a specific line of code was reused by P3 to add another function.


% We plan to iterate on the design with gained insights to increase programmers control over the iterative process of transforming idea to annotation to code generation.



\subsubsection{Ambiguity of Sketches and Model's Transparency}
The primary challenge was the ambiguity of participants' sketches. For example, arrows were used inconsistently, sometimes indicating context [P1] and other times denoting targets of changes [P4] (\autoref{fig:arrow-variants}a-d).
The interpretation of these sketches often relied heavily on surrounding code, leading to occasional misrecognition and misinterpretation. 
This necessitated an iterative refinement process.
However, this iteration became a significant source of frustration for participants, largely due to the system's lack of transparency.
% Also, most participants (5/6) developed a more detailed plan for code edits after the initial round of sketching, suggesting that sketches can be refined iteratively based on the generated code edits.
Participants rated the clarity of the effect of their sketches on the generated code poorly ($Mdn=3.5$, $SD=1.83$), as well as the ease of iterating on sketches ($Mdn=4$, $SD=2.34$), \rev{on a seven-point scale questionnaire.}
Participants struggled with not knowing \pquote{where the code was being edited}{P4}, an unclear mapping between sketches and the edited code, and why the model misinterpreted their sketches.
This is considered as interpretation error than recognition error.




\subsection{Summary}
% In the first iteration, a code editor and canvas were combined, allowing programmers to freely express their intentions for planning code edits. 
The results revealed that programmers utilized diverse sketching techniques, necessitating an iterative refinement process due to the inherent ambiguity of these sketches.
However, the current iteration process was hindered by the AI model's lack of transparency, particularly in how it interpreted sketches and applied code changes.
To address this issue, we focused on identifying potential misinterpretations of sketches by the AI model and exploring how programmers could recover from these errors in the next stage.
% we aim to facilitate the iteration process and minimize the need for programmers to focus on low-level code edits or engage in prompt engineering with AI by concentrating on interactions within the canvas layer.
% However, our goal is not for programmers to view code shaping merely as a way to prompt an AI code generation tool, but as a means to express their thought processes regarding code edits. 
