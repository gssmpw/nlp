



\section{Stage Three: Towards Reconciling Sketches, AI, and Code}
To bridge the conceptual gap between interacting with the code editor and the canvas, the user interface was modified in two key ways. Insights from the previous stage suggested that unnecessary code changes could be minimized by confirming interpretations before generating edits. Building on the types of interpretations identified, we introduced an always-on feedforward mechanism through subtle visual cues. This approach allows programmers to iterate more quickly without delving into code details.
Additionally, to reduce the cognitive load of switching between layers, we developed unique gestures that enable users to interact directly with the code editor through the canvas.


\subsection{User Interface}
\begin{wrapfigure}{L}{15mm}
\vspace{-3mm} \includegraphics[]{figures/layers/stage3-icon.pdf}
\end{wrapfigure} 
Unnecessary GUI elements were removed to keep the interaction focused on sketches. The button for sending sketches and code to the model was renamed ``Commit'' to make it clear that a change will be applied to the code. Only this button and the ``Run'' button were retained in the GUI, as participants preferred having explicit controls for these actions rather than relying on implicit gestures. We open sourced the code for this stage at \url{https://github.com/CodeShaping/code-shaping}, including all the prompts used.


\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/interface/3rd.pdf}
    \caption{The interface design from the third stage. (a) programmers can use one finger tap and drag to select items on canvas; (b) tap longer and drag will select code with contextual menu beside; (c) the always-on feedforward interpretation showing the interpretation of sketches or text, the reasoning of action, and the related code; (d) the gutter will be decorated to indicate which code being referenced and which code will be affected; (e) the related code syntax will be highlight transiently; (f) programmers can commit the changes and (g) draw check/cross to accept/reject code edits.}
    \Description[Interface design for stage three of AI-assisted code editing.]{The figure illustrates the interface design from stage three of the study, showcasing AI-assisted code editing for a Python data processing task. Key interactive features are labeled (a) through (f): (a) Finger Gesture for Selection: Users can tap and drag with a finger to select items on the canvas, enabling precise interaction with the code; (b) Contextual Menu: When code is selected via a long tap and drag, a contextual menu appears, providing options such as "Copy," "Paste," "Delete," and more; (c) Always-On Feedforward Panel: An interpretation panel continuously displays the AI's reasoning, including sketches or text annotations and the corresponding proposed actions; (d) Decorated Gutter: The code gutter is highlighted to indicate which sections of code are referenced or affected by the proposed changes; (e) Transient Syntax Highlighting: Relevant code is temporarily highlighted to visually connect AI suggestions with specific parts of the code; (f) Commit Button: A commit button allows users to finalize edits. Users can accept or reject changes using checkmark or cross gestures directly on the interface; (g) The interface displays Python code for a DataProcessor class, including a sample dataset and a preprocess method that fills missing values in the dataset. Handwritten annotations, such as the word "missing," and gestures, like arrows connecting actions, are visible throughout, illustrating the interactive coding process.}
    \label{fig:third-interface}
\end{figure*}


\subsubsection{Ink and Gestures}
Based on insights from the previous two stages, we classified common interactions during code shaping into five key categories: navigating the canvas and code, undoing and redoing actions such as code edits or sketches, selecting code or sketches on the canvas, accepting and rejecting code changes, and creating free-form sketches (\autoref{tab:gestures}).
Multi-touch gestures were assigned to system-level interactions, such as panning for navigation and two- or three-finger double-tapping for undo and redo actions. Selecting items on the canvas or within the code editor was differentiated by the duration of a single touch: a single tap for canvas items selection (\autoref{fig:third-interface}a) or a long press followed by dragging for code selection (\autoref{fig:third-interface}b). Contextual action buttons, such as delete and copy, are displayed next to the selected code or within the selection box of canvas items, allowing for quick access to common actions.
We implemented unique stroke gestures for accepting or rejecting code edits using the \$1 unistroke recognizer~\cite{10.1145/1294211.1294238} to detect check (\faCheck) and cross (\faTimes) marks (\autoref{fig:third-interface}f). The Google Cloud Vision API was employed for robust recognition of handwritten text, enhancing the system's ability to interpret written pseudo code or textual instructions.


\subsubsection{Always-On Feedforward Interpretation}
\label{sec:feedforward}
Building on insights from the second iteration, we focused on providing only three essential types of interpretations that users truly needed: (1) recognizing how the model interpreted written text, code, and annotations (\autoref{fig:third-interface}c); (2) describing the code editing action inferred by the model; and (3) indicating the code context by highlighting relevant parameter code, displaying blue vertical line glyph decorations, and marking potentially affected code areas with a $\rightarrow$ icon beside the line number on the glyph (\autoref{fig:third-interface}d).
To identify the relevant code, we traversed the abstract syntax tree (AST) to dynamically highlight code syntax related to the user's input. The interpretation process is triggered 500 milliseconds after the user stops sketching, ensuring timely feedback while minimizing disruptions (\autoref{fig:third-interface}e).
The average latency between the input request and the complete output measured from the second study is approximately $2.87$ seconds (\(SD = 1.45\)). However, since interpretations are generated in real-time as users are sketching, it is possible for the system to produce correct results even before all annotations are fully completed.
We implemented a cascade interpretation approach, sequentially processing pen or touch input, predefined gestures, text and shape recognition, code edit action reasoning, and affected code analysis. This approach enables programmers to adjust their sketches concurrent with system evaluation, rather than waiting for the final step. 
These feedforward interpretations were not directly displayed on the canvas or situated within the code but were instead ambiently presented, updating on the fly to offer guidance when needed.
To enhance usability, especially for right-handed users, we repositioned the interpretation text from the upper right to the lower right of the screen. This adjustment allows users to occlude the interpretation with their hands while sketching, minimizing interference with their workflow.






\subsection{Procedure and Data Analysis}
We recruited six new participants for this study, including five right-handed individuals, four of whom identified as women and two as men. They had between 2-8 years of programming experience in Python and used ChatGPT or Copilot 4-12 times per week. We reused the same setup and tasks to ensure consistency in our results across studies. We added several system logs for gesture recognition and recorded input images, which served as parameters for the always-on feedforward interpretation. We collected $187$ sketches, $48$ of which were recorded when participants hit the ``Commit'' button.
We employed inductive thematic analysis to examine all collected data, including sketches, system logs, video recordings, observational notes, and transcribed interview audio recordings. The iterations were defined by the accomplishment of subtasks that participants themselves decided upon and decomposed from the main study task's goal. We then categorized the common flow of actions within these iterations.



% table
\begin{table*}[bt]
    \caption{The assigned touch and pen gestures to the third stage, enabling interaction across both code editor and canvas layers.}
    \input{tables/gesture_table}
    \label{tab:gestures}
\end{table*}



\subsection{Results}
The analysis revealed several themes that shaped participant experiences.

\subsubsection{Flow of Actions}
We identified seven common action flows among participants, highlighting patterns in how they navigated between sketching, code editing, and reviewing interpretations (\autoref{tab:flow}). These flows generally followed a sequence that can be counted as a full iteration:
\[
\begin{array}{c}
\text{Sketch} \rightarrow (\text{Interpret}) \rightarrow \text{Generate} \rightarrow (\text{Run Code}) \\
\rightarrow \text{Accept/Reject}
\rightarrow \text{Re-Sketch/Undo/Redo}
\end{array}
\]
% \begin{equation}
% \text{Sketch} \rightarrow \left(\text{Interpret}\right) \rightarrow \text{Generate} \rightarrow \left(\text{Compile}\right) \rightarrow \text{Accept/Reject} \rightarrow \text{Re-Sketch/Undo/Redo}
% \end{equation}

Some of these flows were also observed in the previous stages but were more pronounced in this stage. Participants appeared more aware of their workflows, especially during interviews when recalling their processes. This contrasts with earlier iterations, where participants often expressed uncertainty, such as \pquote{hopefully the code edits are correct}{P8}. They chose different methods to iterate when code edits were incorrect, adapting their actions based on the situation. For example, P14 mentioned using the undo/redo function (ID 4 in \autoref{tab:flow}) for interpretation errors, while opting for the re-sketch approach (ID 3 in \autoref{tab:flow}) in other cases.

\subsubsection{Always-On Feedforward Interpretation}
After viewing the interpretation, participants pressed the ``Commit'' button $32.4\%$ more frequently in the third stage compared to that in the second. P15 explained, \pquote{the interpretation shows what code is possibly being affected is useful to make sure it will not mess up my code}. This underscores the value of glyph decorations in the always-on interpretation, as they may help participants confirm their intended code changes. Three participants echoed sentiments from the second iteration, appreciating the control provided by the interpretation. P16 noted, \pquote{I feel more confident that it’s on the right track [...] I don’t want it to be a black box}. 

Four participants stressed the importance of waiting for the correct interpretation before committing to changes, even if it required slightly more time compared to directly pressing the commit button. P13 explained, \pquote{I would rather wait a bit longer than evaluate the wrong generated code edits}, reflecting a preference for clarity over speed. 
The feedforward interpretation also guided participants on their next steps, regardless of whether the interpretations were correct. For instance, P15 noted that a previously correct target code section became incorrect after adding an arrow for code reference, indicating a misinterpretation of the arrow.

\subsubsection{Sketching or Editing Code}
A key goal of this design was to reconcile the conceptual layers between the code editor and the canvas.
While all participants did not report difficulty switching between contexts, they perceived them as distinct. As P14 observed, \pquote{I still think of the code and annotations separately in my mind}. However, participants found the unique gestures and strokes for interacting with the code editor \pquote{straightforward}{P17} and felt that it \pquote{makes me [them] feel like the sketch is affecting the code}{P18}.

A notable improvement was that most participants (5/6) expressed no need to use the keyboard, even for simple edits like deleting a line of code. When asked why they preferred using a strikethrough to indicate deletion instead of using the backspace key, P14 explained, \pquote{I just want to use sketches and annotations as the only way to change my code}. This suggests a sense of embodied interaction, something reinforced by P12, \pquote{It’s like my hands are directly editing the code based on how I want the code to be.} However, the predefined \faTimes ~gesture for rejecting code edits was triggered by accident once when P17 wrote \pquote{x} as part of the sketch.
% some confusion arose from the boundaries between sketched gestures and standard annotations; for example, one participant misinterpreted the `x' gesture, meant to reject a code edit, as a cross annotation, leading to unintended actions.

\subsubsection{Conceptual Shift in Code Editing}
\label{sec:conceptual_shift}
Participants demonstrated a shift from linear to spatial thinking in their code editing processes. As P16 observed, \pquote{I'm no longer just writing line by line [...] I'm arranging my thoughts spatially}. This reflects a move away from a traditional, syntax-focused approach to one that emphasizes the overall structure and flow of the code. Another participant, P14, highlighted this shift, stating, \pquote{it’s more about the higher-level structure and flow of the code as a whole}.

\subsubsection{Freedom and Flexibility}
The iterative process enabled by free-form sketching provided participants with a sense of creative freedom. P15 reflected, \pquote{This lets me play around with ideas in a way that’s more fluid and creative [...] I'm experimenting}. All participants mentioned that they would often resketch the code edit even when the generated edits were correct, as they discovered better ways to tackle the task. P14 noted that the canvas and undo/redo mechanisms allowed them to \pquote{draw whatever we [they] want and see how the code changes}. 

However, while participants valued the freedom of sketching, they also tended to ``compromise'' based on the AI’s interpretation capabilities. For instance, P18 consistently used squares instead of circles because \pquote{the rectangle works better} and did not obscure the code. As a result, participants developed preferences for specific annotations with the system along the time. P14, for example, switched to using crosses after observing that strikethroughs occasionally applied to the wrong lines of code. Over time, these preferred annotations became interchangeable in practice, as participants felt they \pquote{expressed the same meaning}.


\subsection{Summary}
The third stage highlights the effectiveness of defined gestures and always-on feedforward interpretation in reducing the transmission gap between the conceptual layers of the canvas, code, and AI model. This design iteration demonstrated how to display the model's interpretation, and designed interactions that minimize the need for layer switching.
While minor challenges remain, such as the rare misinterpretation of sketched gestures and some persistent between AI interpretation and actual applied code edits, these issues can likely be addressed by advancements in AI models.

