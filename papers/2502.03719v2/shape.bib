
@phdthesis{sutherland_investigation_2017,
	type = {Thesis},
	title = {An {Investigation} into {Freeform}, {Dynamic}, {Digital} {Ink} {Annotation} for {Program} {Code}},
	copyright = {Items in ResearchSpace are protected by copyright, with all rights reserved, unless otherwise indicated. Previously published items are made available in accordance with the copyright policy of the publisher.},
	url = {https://researchspace.auckland.ac.nz/handle/2292/31647},
	abstract = {Understanding program code is cognitively demanding. One tool that has not been investigated previously is using ink freeform annotations to aid understanding of program code. Before we can investigate using freeform annotations for code comprehension there are some user and technical challenges that need answering. An iterative research approach was used for this investigation. A single research question was investigated in each iteration and the results informed the next iteration. In the first iteration, the focus was how and why programmers annotate when understanding program code on paper. The results indicate programmers reading program code use similar types of annotation to those reported for other types of reading. The main reasons for adding annotations were to assist with navigation and offload information from the reader’s memory. In the second iteration, the focus was how to classify digital annotations; a precursor to other operations. The initial phase used a general purpose automatic recogniser but failed to improve on previously reported results. The second phase combined automatic recognition with user input. While this approach resulted in higher classification accuracy, the accuracy rates were still lower than previously reported for sketch-based recognition. An unexpected finding was the participants did not want to classify all annotations during reading. In the third iteration, the focus was how to refit (modify) annotations in response to changes in the underlying text. Four classes of annotation were investigated: horizontal lines, vertical lines, enclosures and connectors. Several refitting algorithms were implemented and evaluated for each class of annotation. The findings indicate there are two preferred approaches for refitting annotations: stretching the annotation or splitting it and adding a visualisation showing where the annotation was split. This thesis makes five main contributions. First, a systematic literature review which provides an overview of the current research. Second, details of how and why programmers annotate code on paper. Third, an implementation of an extensible tool for investigating digital ink annotations on code. Fourth, details on how collaborative intelligence can improve recognition. Fifth, a set of proposals for how to refit annotations based on the annotation classification and user preferences.},
	urldate = {2024-08-27},
	school = {ResearchSpace@Auckland},
	author = {Sutherland, Craig},
	year = {2017},
	note = {Accepted: 2017-01-23T03:21:24Z},
}

@inproceedings{parnin_evaluating_2010,
	address = {New York, NY, USA},
	series = {{CHI} '10},
	title = {Evaluating cues for resuming interrupted programming tasks},
	isbn = {978-1-60558-929-9},
	url = {https://dl.acm.org/doi/10.1145/1753326.1753342},
	doi = {10.1145/1753326.1753342},
	abstract = {Developers, like all modern knowledge workers, are frequently interrupted and blocked in their tasks. In this paper we present a contextual inquiry into developers' current strategies for resuming interrupted tasks and investigate the effect of automated cues on improving task resumption. We surveyed 371 programmers on the nature of their tasks, interruptions, task suspension and resumption strategies and found that they rely heavily on note-taking across several types of media. We then ran a controlled lab study to compare the effects of two different automated cues to note taking when resuming interrupted programming tasks. The two cues differed in (1) whether activities were summarized in aggregate or presented chronologically and (2) whether activities were presented as program symbols or as code snippets. Both cues performed well: developers using either cue completed their tasks with twice the success rate as those using note-taking alone. Despite the similar performance of the cues, developers strongly preferred the cue that presents activities chronologically as code snippets.},
	urldate = {2024-08-26},
	booktitle = {Proceedings of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Parnin, Chris and DeLine, Robert},
	month = apr,
	year = {2010},
	pages = {93--102},
}

@inproceedings{latoza_maintaining_2006,
	address = {New York, NY, USA},
	series = {{ICSE} '06},
	title = {Maintaining mental models: a study of developer work habits},
	isbn = {978-1-59593-375-1},
	shorttitle = {Maintaining mental models},
	url = {https://dl.acm.org/doi/10.1145/1134285.1134355},
	doi = {10.1145/1134285.1134355},
	abstract = {To understand developers' typical tools, activities, and practices and their satisfaction with each, we conducted two surveys and eleven interviews. We found that many problems arose because developers were forced to invest great effort recovering implicit knowledge by exploring code and interrupting teammates and this knowledge was only saved in their memory. Contrary to expectations that email and IM prevent expensive task switches caused by face-to-face interruptions, we found that face-to-face communication enjoys many advantages. Contrary to expectations that documentation makes understanding design rationale easy, we found that current design documents are inadequate. Contrary to expectations that code duplication involves the copy and paste of code snippets, developers reported several types of duplication. We use data to characterize these and other problems and draw implications for the design of tools for their solution.},
	urldate = {2024-08-26},
	booktitle = {Proceedings of the 28th international conference on {Software} engineering},
	publisher = {Association for Computing Machinery},
	author = {LaToza, Thomas D. and Venolia, Gina and DeLine, Robert},
	month = may,
	year = {2006},
	pages = {492--501},
}

@inproceedings{cherubini_lets_2007,
	address = {New York, NY, USA},
	series = {{CHI} '07},
	title = {Let's go to the whiteboard: how and why software developers use drawings},
	isbn = {978-1-59593-593-9},
	shorttitle = {Let's go to the whiteboard},
	url = {https://dl.acm.org/doi/10.1145/1240624.1240714},
	doi = {10.1145/1240624.1240714},
	abstract = {Software developers are rooted in the written form of their code, yet they often draw diagrams representing their code. Unfortunately, we still know little about how and why they create these diagrams, and so there is little research to inform the design of visual tools to support developers' work. This paper presents findings from semi-structured interviews that have been validated with a structured survey. Results show that most of the diagrams had a transient nature because of the high cost of changing whiteboard sketches to electronic renderings. Diagrams that documented design decisions were often externalized in these temporary drawings and then subsequently lost. Current visualization tools and the software development practices that we observed do not solve these issues, but these results suggest several directions for future research.},
	urldate = {2024-08-26},
	booktitle = {Proceedings of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Cherubini, Mauro and Venolia, Gina and DeLine, Rob and Ko, Amy J.},
	month = apr,
	year = {2007},
	pages = {557--566},
}

@article{sutherland_freeform_2016,
	title = {Freeform digital ink annotations in electronic documents: {A} systematic mapping study},
	volume = {55},
	issn = {0097-8493},
	shorttitle = {Freeform digital ink annotations in electronic documents},
	url = {https://www.sciencedirect.com/science/article/pii/S0097849315001818},
	doi = {10.1016/j.cag.2015.10.014},
	abstract = {A variety of different approaches have been used to add digital ink annotations to text-based documents. While the majority of research in this field has focused on annotation support for static documents, a small number of studies have investigated support for documents in which the underlying content is changed. Although the approaches used to annotate static documents have been relatively successful, the annotation of dynamic text documents poses significant challenges which remain largely unsolved. However, it is difficult to clearly identify the successful techniques and the remaining challenges since there has not yet been a comprehensive review of digital ink annotation research. This paper reports the results of a systematic mapping study of existing work, and presents a taxonomy categorizing digital ink annotation research.},
	urldate = {2024-07-13},
	journal = {Computers \& Graphics},
	author = {Sutherland, Craig J. and Luxton-Reilly, Andrew and Plimmer, Beryl},
	month = apr,
	year = {2016},
	keywords = {Dynamic digital documents, Freeform ink annotation},
	pages = {1--20},
}

@article{littman_mental_1987,
	title = {Mental models and software maintenance},
	volume = {7},
	issn = {0164-1212},
	url = {https://www.sciencedirect.com/science/article/pii/0164121287900331},
	doi = {10.1016/0164-1212(87)90033-1},
	abstract = {Understanding how a program is constructed and how it functions are significant components of the task of maintaining or enhancing a computer program. We have analyzed vidoetaped protocols of experienced programmers as they enhanced a personnel data base program. Our analysis suggests that there are two strategies for program understanding, the systematic strategy and the as-needed strategy. The programmer using the systematic strategy traces data flow through the program in order to understand global program behavior. The programmer using the as-needed strategy focuses on local program behavior in order to localize study of the program. Our empirical data show that there is a strong relationship between using a systematic approach to acquire knowledge about the program and modifying the program successfully. Programmers who used the systematic approach to study the program constructed successful modifications; programmers who used the as-needed approach failed to construct successful modifications. Programmers who used the systematic strategy gathered knowledge about the causal interactions of the program's functional components. Programmers who used the as-needed strategy did not gather such causal knowledge and therefore failed to detect interactions among components of the program.},
	number = {4},
	urldate = {2024-06-28},
	journal = {Journal of Systems and Software},
	author = {Littman, David C. and Pinto, Jeannine and Letovsky, Stanley and Soloway, Elliot},
	month = dec,
	year = {1987},
	pages = {341--355},
}

@article{jakubovic_achieving_nodate,
	title = {Achieving {Self}-{Sustainability} in {Interactive} {Graphical} {Programming} {Systems}},
	language = {en},
	author = {Jakubovic, Joel},
	keywords = {⛔ No DOI found},
}

@article{navarro-prieto_are_2001,
	title = {Are visual programming languages better? {The} role of imagery in program comprehension},
	volume = {54},
	issn = {1071-5819},
	shorttitle = {Are visual programming languages better?},
	url = {https://www.sciencedirect.com/science/article/pii/S1071581900904658},
	doi = {10.1006/ijhc.2000.0465},
	abstract = {This paper presents one experiment to explain why and under which circumstances visual programming languages would be easier to understand than textual programming languages. Towards this goal we bring together research from psychology of programming and image processing. According to current theories of imagery processing imagery facilitates a quicker access to semantic information. Thus, visual programming languages should allow for quicker construction of a mental representation based on data flow relationships of a program than procedural languages. To test this hypothesis the mental models of C and spreadsheet programmers were assessed in different program comprehension situations. The results showed that spreadsheet programmers developed data flow based mental representations in all situations, while C programmers seemed to access first a control flow and then data flow based mental representations. These results could help to expand theories of mental models from psychology of programming to account for the effect of imagery.},
	number = {6},
	urldate = {2024-06-28},
	journal = {International Journal of Human-Computer Studies},
	author = {Navarro-prieto, RAQUEL and Cañas, JOSE J.},
	month = jun,
	year = {2001},
	keywords = {C, imagery, mental model, program comprehension., spreadsheets, visual programming language},
	pages = {799--829},
}

@article{pollock_designing_2023,
	title = {Designing for {Semi}-formal {Programming} with {Foundation} {Models}},
	abstract = {End-user programmers, such as scientists and data analysts, communicate their intent through culturally specific, semi-formal representations like formulas and wireframes. Research on end-user programming interfaces has sought to democratize programming but has required advances in program synthesis, UI design, and computer vision to support translating a representation to code. As a result, end-users must still frequently translate such representations manually. Foundation models like ChatGPT and GPT-4V dramatically lower the cost of designing new programming interfaces by offering much better code synthesis, UI generation, and visual comprehension tools. These advances enable new end-user workflows with more ubiquitous semi-formal representations. We outline the translation work programmers typically perform when translating representations into code, how foundation models help address this problem, and emerging challenges of using foundation models for programming. We posit semi-formal and notational programming as paradigmatic solutions to integrating foundation models into programming practice. Articulating a design space of semi-formal representations, we ask how we could design new semi-formal programming environments enabled through foundation models that address their emergent challenges, and sketch “proactive disambiguation” as one solution to bridging gulfs of evaluation and execution.},
	language = {en},
	author = {Pollock, Josh and Arawjo, Ian and Berger, Caroline},
	year = {2023},
	keywords = {⛔ No DOI found},
}

@inproceedings{samuelsson_towards_2023,
	title = {Towards a {Visual} {Language} for {Sketched} {Expression} of {Software} {IDE} {Commands}},
	url = {https://ieeexplore.ieee.org/abstract/document/10305696},
	doi = {10.1109/VL-HCC57772.2023.00021},
	abstract = {Given that touch- and pen-enabled screens are becoming more ubiquitous - not just in mobile devices, but also in laptop computers and large interactive display walls - we are exploring the potential for using digital ink sketches as a novel interaction modality that would allow users to express commands by drawing on top of an application's user interface with a pen or stylus. We hypothesize that such sketch-based interaction could be particularly useful when several people are collaborating at a large wall-mounted display where a mouse and keyboard are not in convenient reach, e.g. when software engineers undertake a code review. As a step towards building a prototype to evaluate that vision, this paper reports on two studies with professional software engineers to elicit and rate sketched expressions of a variety of commands in an integrated software development environment (IDE). We describe the experimental setup and discuss how the findings can help to define a sketchbased command language for IDEs.},
	urldate = {2024-06-16},
	booktitle = {2023 {IEEE} {Symposium} on {Visual} {Languages} and {Human}-{Centric} {Computing} ({VL}/{HCC})},
	author = {Samuelsson, Sigurdur Gauti and Book, Matthias},
	month = oct,
	year = {2023},
	note = {ISSN: 1943-6106},
	keywords = {Portable computers, Prototypes, Shape, Software, User interfaces, Visualization, Windows, sketching, software development environments, user interfaces, visual languages},
	pages = {115--123},
}

@article{heinonen_synthesizing_2023,
	title = {Synthesizing research on programmers’ mental models of programs, tasks and concepts — {A} systematic literature review},
	volume = {164},
	issn = {0950-5849},
	url = {https://www.sciencedirect.com/science/article/pii/S0950584923001544},
	doi = {10.1016/j.infsof.2023.107300},
	abstract = {Context:
Programmers’ mental models represent their knowledge and understanding of programs, programming concepts, and programming in general. They guide programmers’ work and influence their task performance. Understanding mental models is important for designing work systems and practices that support programmers.
Objective:
Although the importance of programmers’ mental models is widely acknowledged, research on mental models has decreased over the years. The results are scattered and do not take into account recent developments in software engineering. In this article, we analyze the state of research on programmers’ mental models and provide an overview of existing research. We connect results on mental models from different strands of research to form a more unified knowledge base on the topic.
Method:
We conducted a systematic literature review on programmers’ mental models. We analyzed literature addressing mental models in different contexts, including mental models of programs, programming tasks, and programming concepts. Using nine search engines, we found 3678 articles (excluding duplicates). Of these, 84 were selected for further analysis. Using the snowballing technique, starting from these 84, we obtained a final result set containing 187 articles.
Results:
We show that the literature shares a kernel of shared understanding of mental models. By collating and connecting results on mental models from different fields of research, we provide a comprehensive synthesis of results related to programmers’ mental models.
Conclusion:
The research field on programmers’ mental models faces many challenges arising from a lack of a shared knowledge base and poorly defined constructs. By creating a unified knowledge base on the topic, this work provides a basis for future work on mental models. We also point to directions for future studies. In particular, we call for studies that examine programmers working with modern practices and tools.},
	urldate = {2024-06-06},
	journal = {Information and Software Technology},
	author = {Heinonen, Ava and Lehtelä, Bettina and Hellas, Arto and Fagerholm, Fabian},
	month = dec,
	year = {2023},
	keywords = {Empirical software engineering, Human factors, Mental model, Mental representation, Program comprehension, Programmer, Psychology of programming, Software developer, Software development, Systematic literature review},
	pages = {107300},
}

@article{bidlake_systematic_2020,
	title = {Systematic literature review of empirical studies on mental representations of programs},
	volume = {165},
	issn = {0164-1212},
	url = {https://www.sciencedirect.com/science/article/pii/S0164121220300467},
	doi = {10.1016/j.jss.2020.110565},
	abstract = {Programmers are frequently tasked with modifying, enhancing, and extending applications. To perform these tasks, programmers must understand existing code by forming mental representations. Empirical research is required to determine the mental representations constructed during program comprehension to inform the development of programming languages, instructional practices, and tools. To make recommendations for future work a systematic literature review was conducted that summarizes the empirical research on mental representations formed during program comprehension, how the methods and tasks have changed over time, and the research contributions. The data items included in the systematic review are empirical studies of programmers that investigated the comprehension and internal representation of code written in a formal programming language. The eligibility criteria used in the review were meant to extract studies with a focus on knowledge representation as opposed to knowledge utilization. The results revealed a lack of incremental research and a dramatic decline in the research meaning that newly developed or popularized languages and paradigms have not been a part of the research reviewed. Accordingly, we argue that there needs to be a resurgence of empirical research on the psychology of programming to inform the design of tools and languages, especially in new and emerging paradigms.},
	urldate = {2024-06-06},
	journal = {Journal of Systems and Software},
	author = {Bidlake, Leah and Aubanel, Eric and Voyer, Daniel},
	month = jul,
	year = {2020},
	keywords = {Mental representations, Program comprehension, Systematic literature review},
	pages = {110565},
}

@misc{ryanyen2_searchgen-react_2024,
	title = {searchgen-react - {CodeSandbox}},
	url = {https://codesandbox.io/s/5pq6jf},
	abstract = {searchgen-react by ryanyen2 using react, react-dom, react-scripts},
	language = {en},
	urldate = {2024-06-02},
	author = {ryanyen2},
	month = jun,
	year = {2024},
	note = {Section: create-react-app},
}

@article{maalej_comprehension_2014,
	title = {On the {Comprehension} of {Program} {Comprehension}},
	volume = {23},
	issn = {1049-331X},
	url = {https://dl.acm.org/doi/10.1145/2622669},
	doi = {10.1145/2622669},
	abstract = {Research in program comprehension has evolved considerably over the past decades. However, only little is known about how developers practice program comprehension in their daily work. This article reports on qualitative and quantitative research to comprehend the strategies, tools, and knowledge used for program comprehension. We observed 28 professional developers, focusing on their comprehension behavior, strategies followed, and tools used. In an online survey with 1,477 respondents, we analyzed the importance of certain types of knowledge for comprehension and where developers typically access and share this knowledge. We found that developers follow pragmatic comprehension strategies depending on context. They try to avoid comprehension whenever possible and often put themselves in the role of users by inspecting graphical interfaces. Participants confirmed that standards, experience, and personal communication facilitate comprehension. The team size, its distribution, and open-source experience influence their knowledge sharing and access behavior. While face-to-face communication is preferred for accessing knowledge, knowledge is frequently shared in informal comments. Our results reveal a gap between research and practice, as we did not observe any use of comprehension tools and developers seem to be unaware of them. Overall, our findings call for reconsidering the research agendas towards context-aware tool support.},
	number = {4},
	urldate = {2024-05-31},
	journal = {ACM Transactions on Software Engineering and Methodology},
	author = {Maalej, Walid and Tiarks, Rebecca and Roehm, Tobias and Koschke, Rainer},
	month = sep,
	year = {2014},
	keywords = {Empirical software engineering, context-aware software engineering, information needs, knowledge sharing, program comprehension},
	pages = {31:1--31:37},
}

@misc{lin_inksight_2023,
	title = {{InkSight}: {Leveraging} {Sketch} {Interaction} for {Documenting} {Chart} {Findings} in {Computational} {Notebooks}},
	shorttitle = {{InkSight}},
	url = {http://arxiv.org/abs/2307.07922},
	doi = {10.48550/arXiv.2307.07922},
	abstract = {Computational notebooks have become increasingly popular for exploratory data analysis due to their ability to support data exploration and explanation within a single document. Effective documentation for explaining chart findings during the exploration process is essential as it helps recall and share data analysis. However, documenting chart findings remains a challenge due to its time-consuming and tedious nature. While existing automatic methods alleviate some of the burden on users, they often fail to cater to users' specific interests. In response to these limitations, we present InkSight, a mixed-initiative computational notebook plugin that generates finding documentation based on the user's intent. InkSight allows users to express their intent in specific data subsets through sketching atop visualizations intuitively. To facilitate this, we designed two types of sketches, i.e., open-path and closed-path sketch. Upon receiving a user's sketch, InkSight identifies the sketch type and corresponding selected data items. Subsequently, it filters data fact types based on the sketch and selected data items before employing existing automatic data fact recommendation algorithms to infer data facts. Using large language models (GPT-3.5), InkSight converts data facts into effective natural language documentation. Users can conveniently fine-tune the generated documentation within InkSight. A user study with 12 participants demonstrated the usability and effectiveness of InkSight in expressing user intent and facilitating chart finding documentation.},
	urldate = {2023-10-02},
	publisher = {arXiv},
	author = {Lin, Yanna and Li, Haotian and Yang, Leni and Wu, Aoyu and Qu, Huamin},
	month = jul,
	year = {2023},
	note = {arXiv:2307.07922 [cs]},
	keywords = {Computer Science - Human-Computer Interaction},
}

@inproceedings{xia_dataink_2018,
	address = {New York, NY, USA},
	series = {{CHI} '18},
	title = {{DataInk}: {Direct} and {Creative} {Data}-{Oriented} {Drawing}},
	isbn = {978-1-4503-5620-6},
	shorttitle = {{DataInk}},
	url = {https://dl.acm.org/doi/10.1145/3173574.3173797},
	doi = {10.1145/3173574.3173797},
	abstract = {Creating whimsical, personal data visualizations remains a challenge due to a lack of tools that enable for creative visual expression while providing support to bind graphical content to data. Many data analysis and visualization creation tools target the quick generation of visual representations, but lack the functionality necessary for graphics design. Toolkits and charting libraries offer more expressive power, but require expert programming skills to achieve custom designs. In contrast, sketching affords fluid experimentation with visual shapes and layouts in a free-form manner, but requires one to manually draw every single data point. We aim to bridge the gap between these extremes. We propose DataInk, a system supports the creation of expressive data visualizations with rigorous direct manipulation via direct pen and touch input. Leveraging our commonly held skills, coupled with a novel graphical user interface, DataInk enables direct, fluid, and flexible authoring of creative data visualizations.},
	urldate = {2023-10-02},
	booktitle = {Proceedings of the 2018 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Xia, Haijun and Henry Riche, Nathalie and Chevalier, Fanny and De Araujo, Bruno and Wigdor, Daniel},
	month = apr,
	year = {2018},
	keywords = {object-oriented interaction, visualization},
	pages = {1--13},
}

@inproceedings{horvath_using_2022,
	address = {New York, NY, USA},
	series = {{UIST} '22},
	title = {Using {Annotations} for {Sensemaking} {About} {Code}},
	isbn = {978-1-4503-9320-1},
	url = {https://dl.acm.org/doi/10.1145/3526113.3545667},
	doi = {10.1145/3526113.3545667},
	abstract = {Developers spend significant amounts of time finding, relating, navigating, and, more broadly, making sense of code. While sensemaking, developers must keep track of many pieces of information including the objectives of their task, the code locations of interest, their questions and hypotheses about the behavior of the code, and more. Despite this process being such an integral aspect of software development, there is little tooling support for externalizing and keeping track of developers’ information, which led us to develop Catseye – an annotation tool for lightweight notetaking about code. Catseye has advantages over traditional methods of externalizing code-related information, such as commenting, in that the annotations retain the original context of the code while not actually modifying the underlying source code, they can support richer interactions such as lightweight versioning, and they can be used as navigational aids. In our investigation of developers’ notetaking processes using Catseye, we found developers were able to successfully use annotations to support their code sensemaking when completing a debugging task.},
	urldate = {2023-06-16},
	booktitle = {Proceedings of the 35th {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Horvath, Amber and Myers, Brad and Macvean, Andrew and Rahman, Imtiaz},
	month = oct,
	year = {2022},
	keywords = {Annotations, code comprehension, notetaking, sensemaking, software engineering},
	pages = {1--16},
}

@inproceedings{horvath_understanding_2022,
	address = {New York, NY, USA},
	series = {{CHI} '22},
	title = {Understanding {How} {Programmers} {Can} {Use} {Annotations} on {Documentation}},
	isbn = {978-1-4503-9157-3},
	url = {https://dl.acm.org/doi/10.1145/3491102.3502095},
	doi = {10.1145/3491102.3502095},
	abstract = {Modern software development requires developers to find and effectively utilize new APIs and their documentation, but documentation has many well-known issues. Despite this, developers eventually overcome these issues but have no way of sharing what they learned. We investigate sharing this documentation-specific information through annotations, which have advantages over developer forums as the information is contextualized, not disruptive, and is short, thus easy to author. Developers can also author annotations to support their own comprehension. In order to support the documentation usage behaviors we found, we built the Adamite annotation tool, which provides features such as multiple anchors, annotation types, and pinning. In our user study, we found that developers are able to create annotations that are useful to themselves and are able to utilize annotations created by other developers when learning a new API, with readers of the annotations completing 67\% more of the task, on average, than the baseline.},
	urldate = {2023-06-13},
	booktitle = {Proceedings of the 2022 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Horvath, Amber and Liu, Michael Xieyang and Hendriksen, River and Shannon, Connor and Paterson, Emma and Jawad, Kazi and Macvean, Andrew and Myers, Brad A},
	month = apr,
	year = {2022},
	keywords = {Annotations, application programming interfaces (APIs), documentation, note taking, software engineering},
	pages = {1--16},
}

@inproceedings{subramonyam_texsketch_2020,
	address = {New York, NY, USA},
	series = {{CHI} '20},
	title = {{texSketch}: {Active} {Diagramming} through {Pen}-and-{Ink} {Annotations}},
	isbn = {978-1-4503-6708-0},
	shorttitle = {{texSketch}},
	url = {https://dl.acm.org/doi/10.1145/3313831.3376155},
	doi = {10.1145/3313831.3376155},
	abstract = {Learning from text is a constructive activity in which sentence-level information is combined by the reader to build coherent mental models. With increasingly complex texts, forming a mental model becomes challenging due to a lack of background knowledge, and limits in working memory and attention. To address this, we are taught knowledge externalization strategies such as active reading and diagramming. Unfortunately, paper-and-pencil approaches may not always be appropriate, and software solutions create friction through difficult input modalities, limited workflow support, and barriers between reading and diagramming. For all but the simplest text, building coherent diagrams can be tedious and difficult. We propose Active Diagramming, an approach extending familiar active reading strategies to the task of diagram construction. Our prototype, texSketch, combines pen-and-ink interactions with natural language processing to reduce the cost of producing diagrams while maintaining the cognitive effort necessary for comprehension. Our user study finds that readers can effectively create diagrams without disrupting reading.},
	urldate = {2023-10-24},
	booktitle = {Proceedings of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Subramonyam, Hariharan and Seifert, Colleen and Shah, Priti and Adar, Eytan},
	month = apr,
	year = {2020},
	keywords = {active reading, diagramming, pen-and-ink gestures},
	pages = {1--13},
}

@article{vaithilingam_towards_nodate,
	title = {Towards {More} {Effective} {AI}-{Assisted} {Programming}: {A} {Systematic} {Design} {Exploration} to {Improve} {Visual} {Studio} {IntelliCode}’s {User} {Experience}},
	doi = {10.1109/ICSE-SEIP58684.2023.00022},
	abstract = {AI-driven code editor extensions such as Visual Studio IntelliCode and Github CoPilot have become extremely popular. These tools recommend inserting chunks of code, with the lines to be inserted presented inline at the current cursor location as gray text. In contrast to their popularity, other AIdriven code recommendation tools that suggest code changes (as opposed to code completions) have remained woefully underused. We conducted lab studies at Microsoft to understand this disparity and found one major cause: discoverability. Code change suggestions are hard to surface through bold, inline interfaces and hence, developers often do not even notice them.},
	language = {en},
	author = {Vaithilingam, Priyan and Glassman, Elena L and Groenwegen, Peter and Gulwani, Sumit and Henley, Austin Z and Malpani, Rohan and Pugh, David and Radhakrishna, Arjun and Soares, Gustavo and Wang, Joey and Yim, Aaron},
}

@article{mozannar_reading_2022,
	title = {Reading {Between} the {Lines}: {Modeling} {User} {Behavior} and {Costs} in {AI}-{Assisted} {Programming}},
	url = {http://arxiv.org/abs/2210.14306},
	abstract = {AI code-recommendation systems (CodeRec), such as Copilot, can assist programmers inside an IDE by suggesting and autocompleting arbitrary code; potentially improving their productivity. To understand how these AI improve programmers in a coding session, we need to understand how they affect programmers' behavior. To make progress, we studied GitHub Copilot, and developed CUPS -- a taxonomy of 12 programmer activities common to AI code completion systems. We then conducted a study with 21 programmers who completed coding tasks and used our labeling tool to retrospectively label their sessions with CUPS. We analyze over 3000 label instances, and visualize the results with timelines and state machines to profile programmer-CodeRec interaction. This reveals novel insights into the distribution and patterns of programmer behavior, as well as inefficiencies and time costs. Finally, we use these insights to inform future interventions to improve AI-assisted programming and human-AI interaction.},
	author = {Mozannar, Hussein and Bansal, Gagan and Fourney, Adam and Horvitz, Eric},
	month = oct,
	year = {2022},
	note = {arXiv: 2210.14306},
	keywords = {⛔ No DOI found},
}

@article{barke_grounded_2022,
	title = {Grounded {Copilot}: {How} {Programmers} {Interact} with {Code}-{Generating} {Models}},
	url = {http://arxiv.org/abs/2206.15000},
	abstract = {Powered by recent advances in code-generating models, AI assistants like Github Copilot promise to change the face of programming forever. But what is this new face of programming? We present the first grounded theory analysis of how programmers interact with Copilot, based on observing 20 participants--with a range of prior experience using the assistant--as they solve diverse programming tasks across four languages. Our main finding is that interactions with programming assistants are bimodal: in acceleration mode, the programmer knows what to do next and uses Copilot to get there faster; in exploration mode, the programmer is unsure how to proceed and uses Copilot to explore their options. Based on our theory, we provide recommendations for improving the usability of future AI programming assistants.},
	author = {Barke, Shraddha and James, Michael B. and Polikarpova, Nadia},
	month = jun,
	year = {2022},
	note = {arXiv: 2206.15000},
	keywords = {subtask, ⛔ No DOI found},
}

@article{dakhel_github_2022,
	title = {{GitHub} {Copilot} {AI} pair programmer: {Asset} or {Liability}?},
	url = {http://arxiv.org/abs/2206.15331},
	abstract = {Automatic program synthesis is a long-lasting dream in software engineering. Recently, a promising Deep Learning (DL) based solution, called Copilot, has been proposed by Open AI and Microsoft as an industrial product. Although some studies evaluate the correctness of Copilot solutions and report its issues, more empirical evaluations are necessary to understand how developers can benefit from it effectively. In this paper, we study the capabilities of Copilot in two different programming tasks: (1) generating (and reproducing) correct and efficient solutions for fundamental algorithmic problems, and (2) comparing Copilot's proposed solutions with those of human programmers on a set of programming tasks. For the former, we assess the performance and functionality of Copilot in solving selected fundamental problems in computer science, like sorting and implementing basic data structures. In the latter, a dataset of programming problems with human-provided solutions is used. The results show that Copilot is capable of providing solutions for almost all fundamental algorithmic problems, however, some solutions are buggy and non-reproducible. Moreover, Copilot has some difficulties in combining multiple methods to generate a solution. Comparing Copilot to humans, our results show that the correct ratio of human solutions is greater than Copilot's correct ratio, while the buggy solutions generated by Copilot require less effort to be repaired. While Copilot shows limitations as an assistant for developers especially in advanced programming tasks, as highlighted in this study and previous ones, it can generate preliminary solutions for basic programming tasks.},
	author = {Dakhel, Arghavan Moradi and Majdinasab, Vahid and Nikanjam, Amin and Khomh, Foutse and Desmarais, Michel C. and Ming, Zhen and {Jiang}},
	month = jun,
	year = {2022},
	note = {arXiv: 2206.15331},
	keywords = {⛔ No DOI found},
}

@article{chen_evaluating_2021,
	title = {Evaluating {Large} {Language} {Models} {Trained} on {Code}},
	url = {http://arxiv.org/abs/2107.03374},
	abstract = {We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8\% of the problems, while GPT-3 solves 0\% and GPT-J solves 11.4\%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2\% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.},
	author = {Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and Ray, Alex and Puri, Raul and Krueger, Gretchen and Petrov, Michael and Khlaaf, Heidy and Sastry, Girish and Mishkin, Pamela and Chan, Brooke and Gray, Scott and Ryder, Nick and Pavlov, Mikhail and Power, Alethea and Kaiser, Lukasz and Bavarian, Mohammad and Winter, Clemens and Tillet, Philippe and Such, Felipe Petroski and Cummings, Dave and Plappert, Matthias and Chantzis, Fotios and Barnes, Elizabeth and Herbert-Voss, Ariel and Guss, William Hebgen and Nichol, Alex and Paino, Alex and Tezak, Nikolas and Tang, Jie and Babuschkin, Igor and Balaji, Suchir and Jain, Shantanu and Saunders, William and Hesse, Christopher and Carr, Andrew N. and Leike, Jan and Achiam, Josh and Misra, Vedant and Morikawa, Evan and Radford, Alec and Knight, Matthew and Brundage, Miles and Murati, Mira and Mayer, Katie and Welinder, Peter and McGrew, Bob and Amodei, Dario and McCandlish, Sam and Sutskever, Ilya and Zaremba, Wojciech},
	month = jul,
	year = {2021},
	note = {arXiv: 2107.03374},
	keywords = {⛔ No DOI found},
}

@inproceedings{liu_what_2023,
	address = {New York, NY, USA},
	series = {{CHI} '23},
	title = {“{What} {It} {Wants} {Me} {To} {Say}”: {Bridging} the {Abstraction} {Gap} {Between} {End}-{User} {Programmers} and {Code}-{Generating} {Large} {Language} {Models}},
	isbn = {978-1-4503-9421-5},
	shorttitle = {“{What} {It} {Wants} {Me} {To} {Say}”},
	url = {https://dl.acm.org/doi/10.1145/3544548.3580817},
	doi = {10.1145/3544548.3580817},
	abstract = {Code-generating large language models map natural language to code. However, only a small portion of the infinite space of naturalistic utterances is effective at guiding code generation. For non-expert end-user programmers, learning this is the challenge of abstraction matching. We examine this challenge in the specific context of data analysis in spreadsheets, in a system that maps the user’s natural language query to Python code using the Codex generator, executes the code, and shows the result. We propose grounded abstraction matching, which bridges the abstraction gap by translating the code back into a systematic and predictable naturalistic utterance. In a between-subjects, think-aloud study (n=24), we compare grounded abstraction matching to an ungrounded alternative based on previously established query framing principles. We find that the grounded approach improves end-users’ understanding of the scope and capabilities of the code-generating model, and the kind of language needed to use it effectively.},
	urldate = {2023-07-09},
	booktitle = {Proceedings of the 2023 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Liu, Michael Xieyang and Sarkar, Advait and Negreanu, Carina and Zorn, Benjamin and Williams, Jack and Toronto, Neil and Gordon, Andrew D.},
	month = apr,
	year = {2023},
	keywords = {Human-AI Interaction, Large Language Models, Natural Language Programming, Spreadsheets},
	pages = {1--31},
}

@misc{sarkar_what_2022,
	title = {What is it like to program with artificial intelligence?},
	url = {http://arxiv.org/abs/2208.06213},
	doi = {10.48550/arXiv.2208.06213},
	abstract = {Large language models, such as OpenAI's codex and Deepmind's AlphaCode, can generate code to solve a variety of problems expressed in natural language. This technology has already been commercialised in at least one widely-used programming editor extension: GitHub Copilot. In this paper, we explore how programming with large language models (LLM-assisted programming) is similar to, and differs from, prior conceptualisations of programmer assistance. We draw upon publicly available experience reports of LLM-assisted programming, as well as prior usability and design studies. We find that while LLM-assisted programming shares some properties of compilation, pair programming, and programming via search and reuse, there are fundamental differences both in the technical possibilities as well as the practical experience. Thus, LLM-assisted programming ought to be viewed as a new way of programming with its own distinct properties and challenges. Finally, we draw upon observations from a user study in which non-expert end user programmers use LLM-assisted tools for solving data tasks in spreadsheets. We discuss the issues that might arise, and open research challenges, in applying large language models to end-user programming, particularly with users who have little or no programming expertise.},
	urldate = {2023-06-12},
	publisher = {arXiv},
	author = {Sarkar, Advait and Gordon, Andrew D. and Negreanu, Carina and Poelitz, Christian and Ragavan, Sruti Srinivasa and Zorn, Ben},
	month = oct,
	year = {2022},
	note = {arXiv:2208.06213 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction, Computer Science - Programming Languages, D.2.3, D.2.6, H.5.2, I.2.5, I.2.7},
}

@misc{liang_understanding_2023,
	title = {Understanding the {Usability} of {AI} {Programming} {Assistants}},
	url = {http://arxiv.org/abs/2303.17125},
	doi = {10.48550/arXiv.2303.17125},
	abstract = {The software engineering community recently has witnessed widespread deployment of AI programming assistants, such as GitHub Copilot. However, in practice, developers do not accept AI programming assistants' initial suggestions at a high frequency. This leaves a number of open questions related to the usability of these tools. To understand developers' practices while using these tools and the important usability challenges they face, we administered a survey to a large population of developers and received responses from a diverse set of 410 developers. Through a mix of qualitative and quantitative analyses, we found that developers are most motivated to use AI programming assistants because they help developers reduce key-strokes, finish programming tasks quickly, and recall syntax, but resonate less with using them to help brainstorm potential solutions. We also found the most important reasons why developers do not use these tools are because these tools do not output code that addresses certain functional or non-functional requirements and because developers have trouble controlling the tool to generate the desired output. Our findings have implications for both creators and users of AI programming assistants, such as designing minimal cognitive effort interactions with these tools to reduce distractions for users while they are programming.},
	urldate = {2023-06-15},
	publisher = {arXiv},
	author = {Liang, Jenny T. and Yang, Chenyang and Myers, Brad A.},
	month = mar,
	year = {2023},
	note = {arXiv:2303.17125 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction, Computer Science - Software Engineering},
}

@inproceedings{al_madi_how_2023,
	address = {New York, NY, USA},
	series = {{ASE} '22},
	title = {How {Readable} is {Model}-generated {Code}? {Examining} {Readability} and {Visual} {Inspection} of {GitHub} {Copilot}},
	isbn = {978-1-4503-9475-8},
	shorttitle = {How {Readable} is {Model}-generated {Code}?},
	url = {https://dl.acm.org/doi/10.1145/3551349.3560438},
	doi = {10.1145/3551349.3560438},
	abstract = {Background: Recent advancements in large language models have motivated the practical use of such models in code generation and program synthesis. However, little is known about the effects of such tools on code readability and visual attention in practice. Objective: In this paper, we focus on GitHub Copilot to address the issues of readability and visual inspection of model generated code. Readability and low complexity are vital aspects of good source code, and visual inspection of generated code is important in light of automation bias. Method: Through a human experiment (n=21) we compare model generated code to code written completely by human programmers. We use a combination of static code analysis and human annotators to assess code readability, and we use eye tracking to assess the visual inspection of code. Results: Our results suggest that model generated code is comparable in complexity and readability to code written by human pair programmers. At the same time, eye tracking data suggests, to a statistically significant level, that programmers direct less visual attention to model generated code. Conclusion: Our findings highlight that reading code is more important than ever, and programmers should beware of complacency and automation bias with model generated code.},
	urldate = {2023-06-12},
	booktitle = {Proceedings of the 37th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Al Madi, Naser},
	month = jan,
	year = {2023},
	keywords = {Copilot, Empirical Study, Eye Tracking, GitHub, Readability},
	pages = {1--5},
}

@article{xu_-ide_2022,
	title = {In-{IDE} {Code} {Generation} from {Natural} {Language}: {Promise} and {Challenges}},
	volume = {31},
	issn = {15577392},
	doi = {10.1145/3487569},
	abstract = {A great part of software development involves conceptualizing or communicating the underlying procedures and logic that needs to be expressed in programs. One major difficulty of programming is turning concept into code, especially when dealing with the APIs of unfamiliar libraries. Recently, there has been a proliferation of machine learning methods for code generation and retrieval from natural language queries, but these have primarily been evaluated purely based on retrieval accuracy or overlap of generated code with developer-written code, and the actual effect of these methods on the developer workflow is surprisingly unattested. In this article, we perform the first comprehensive investigation of the promise and challenges of using such technology inside the PyCharm IDE, asking, "At the current state of technology does it improve developer productivity or accuracy, how does it affect the developer experience, and what are the remaining gaps and challenges?"To facilitate the study, we first develop a plugin for the PyCharm IDE that implements a hybrid of code generation and code retrieval functionality, and we orchestrate virtual environments to enable collection of many user events (e.g., web browsing, keystrokes, fine-grained code edits). We ask developers with various backgrounds to complete 7 varieties of 14 Python programming tasks ranging from basic file manipulation to machine learning or data visualization, with or without the help of the plugin. While qualitative surveys of developer experience are largely positive, quantitative results with regards to increased productivity, code quality, or program correctness are inconclusive. Further analysis identifies several pain points that could improve the effectiveness of future machine learning-based code generation/retrieval developer assistants and demonstrates when developers prefer code generation over code retrieval and vice versa. We release all data and software to pave the road for future empirical studies on this topic, as well as development of better code generation models.},
	number = {2},
	journal = {ACM Transactions on Software Engineering and Methodology},
	author = {Xu, Frank F. and Vasilescu, Bogdan and Neubig, Graham},
	month = apr,
	year = {2022},
	note = {arXiv: 2101.11149
Publisher: Association for Computing Machinery},
	keywords = {Natural language programming assistant, code generation, code retrieval, empirical study},
}

@misc{pudari_copilot_2023,
	title = {From {Copilot} to {Pilot}: {Towards} {AI} {Supported} {Software} {Development}},
	shorttitle = {From {Copilot} to {Pilot}},
	url = {http://arxiv.org/abs/2303.04142},
	doi = {10.48550/arXiv.2303.04142},
	abstract = {AI-supported programming has arrived, as shown by the introduction and successes of large language models for code, such as Copilot/Codex (Github/OpenAI) and AlphaCode (DeepMind). Above human average performance on programming challenges is now possible. However, software engineering is much more than solving programming contests. Moving beyond code completion to AI-supported software engineering will require an AI system that can, among other things, understand how to avoid code smells, to follow language idioms, and eventually (maybe!) propose rational software designs. In this study, we explore the current limitations of AI-supported code completion tools like Copilot and offer a simple taxonomy for understanding the classification of AI-supported code completion tools in this space. We first perform an exploratory study on Copilot's code suggestions for language idioms and code smells. Copilot does not follow language idioms and avoid code smells in most of our test scenarios. We then conduct additional investigation to determine the current boundaries of AI-supported code completion tools like Copilot by introducing a taxonomy of software abstraction hierarchies where 'basic programming functionality' such as code compilation and syntax checking is at the least abstract level, software architecture analysis and design are at the most abstract level. We conclude by providing a discussion on challenges for future development of AI-supported code completion tools to reach the design level of abstraction in our taxonomy.},
	urldate = {2023-06-19},
	publisher = {arXiv},
	author = {Pudari, Rohith and Ernst, Neil A.},
	month = mar,
	year = {2023},
	note = {arXiv:2303.04142 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Software Engineering},
}

@inproceedings{macneil_experiences_2023,
	address = {New York, NY, USA},
	series = {{SIGCSE} 2023},
	title = {Experiences from {Using} {Code} {Explanations} {Generated} by {Large} {Language} {Models} in a {Web} {Software} {Development} {E}-{Book}},
	isbn = {978-1-4503-9431-4},
	url = {https://dl.acm.org/doi/10.1145/3545945.3569785},
	doi = {10.1145/3545945.3569785},
	abstract = {Advances in natural language processing have resulted in large language models (LLMs) that can generate code and code explanations. In this paper, we report on our experiences generating multiple code explanation types using LLMs and integrating them into an interactive e-book on web software development. Three different types of explanations -- a line-by-line explanation, a list of important concepts, and a high-level summary of the code -- were created. Students could view explanations by clicking a button next to code snippets, which showed the explanation and asked about its utility. Our results show that all explanation types were viewed by students and that the majority of students perceived the code explanations as helpful to them. However, student engagement varied by code snippet complexity, explanation type, and code snippet length. Drawing on our experiences, we discuss future directions for integrating explanations generated by LLMs into CS classrooms.},
	urldate = {2023-06-17},
	booktitle = {Proceedings of the 54th {ACM} {Technical} {Symposium} on {Computer} {Science} {Education} {V}. 1},
	publisher = {Association for Computing Machinery},
	author = {MacNeil, Stephen and Tran, Andrew and Hellas, Arto and Kim, Joanne and Sarsa, Sami and Denny, Paul and Bernstein, Seth and Leinonen, Juho},
	month = mar,
	year = {2023},
	pages = {931--937},
}

@inproceedings{vaithilingam_expectation_2022,
	title = {Expectation vs. {Experience}: {Evaluating} the {Usability} of {Code} {Generation} {Tools} {Powered} by {Large} {Language} {Models}},
	isbn = {978-1-4503-9156-6},
	doi = {10.1145/3491101.3519665},
	abstract = {Recent advances in Large Language Models (LLM) have made automatic code generation possible for real-world programming tasks in general-purpose programming languages such as Python. However, there are few human studies on the usability of these tools and how they fit the programming workflow. In this work, we conducted a within-subjects user study with 24 participants to understand how programmers use and perceive Copilot, a LLM-based code generation tool. We found that, while Copilot did not necessarily improve the task completion time or success rate, most participants preferred to use Copilot in daily programming tasks, since Copilot often provided a useful starting point and saved the effort of searching online. However, participants did face difficulties in understanding, editing, and debugging code snippets generated by Copilot, which significantly hindered their task-solving effectiveness. Finally, we highlighted several promising directions for improving the design of Copilot based on our observations and participants' feedback.},
	booktitle = {Conference on {Human} {Factors} in {Computing} {Systems} - {Proceedings}},
	publisher = {Association for Computing Machinery},
	author = {Vaithilingam, Priyan and Zhang, Tianyi and Glassman, Elena L.},
	month = apr,
	year = {2022},
	keywords = {github copilot, large language model},
}

@inproceedings{nguyen_empirical_2022,
	title = {An {Empirical} {Evaluation} of {GitHub} {Copilot}'s {Code} {Suggestions}},
	isbn = {978-1-4503-9303-4},
	doi = {10.1145/3524842.3528470},
	abstract = {GitHub and OpenAI recently launched Copilot, an 'AI pair programmer' that utilizes the power of Natural Language Processing, Static Analysis, Code Synthesis, and Artificial Intelligence. Given a natural language description of the target functionality, Copilot can generate corresponding code in several programming languages. In this paper, we perform an empirical study to evaluate the correctness and understandability of Copilot's suggested code. We use 33 LeetCode questions to create queries for Copilot in four different programming languages. We evaluate the correctness of the corresponding 132 Copilot solutions by running LeetCode's provided tests, and evaluate understandability using SonarQube's cyclomatic complexity and cognitive complexity metrics. We find that Copilot's Java suggestions have the highest correctness score (57\%) while JavaScript is the lowest (27\%). Overall, Copilot's suggestions have low complexity with no notable differences between the programming languages. We also find some potential Copilot shortcomings, such as generating code that can be further simplified and code that relies on undefined helper methods.},
	booktitle = {Proceedings - 2022 {Mining} {Software} {Repositories} {Conference}, {MSR} 2022},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Nguyen, Nhan and Nadi, Sarah},
	year = {2022},
	keywords = {Codex, Empirical Evaluation, GitHub Copilot, Program Synthesis},
	pages = {1--5},
}

@misc{gadhave_persist_2023,
	title = {Persist: {Persistent} and {Reusable} {Interactions} in {Computational} {Notebooks}},
	copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
	shorttitle = {Persist},
	url = {https://osf.io/9x8eq},
	doi = {10.31219/osf.io/9x8eq},
	abstract = {Computational notebooks, such as Jupyter, support rich data visualization. However, even when visualizations in notebooks are interactive, they are a dead end: Interactive data manipulations, such as selections, applying labels, filters, categorizations, or fixes to column or cell values, could be efficiently applied in interactive visual components, but interactive components typically cannot manipulate Python data structures. Furthermore, actions performed in interactive plots are lost as soon as the cell is re-run, prohibiting reusability and reproducibility. To remedy this problem, we introduce Persist, a family of techniques to (a) capture interaction provenance, enabling the persistence of interactions, and (b) map interactions to data manipulations that can be applied to dataframes. We implement our approach as a JupyterLab extension that supports tracking interactions in VegaAltair plots and in a data table view. Persist can re-execute interaction provenance when a notebook or a cell is re-executed, enabling reproducibility and re-use. We evaluate Persist in a user study targeting data manipulations with 11 participants skilled in Python and Pandas, comparing it to traditional code-based approaches. Participants were consistently faster and were able to correctly complete more tasks with Persist.},
	language = {en},
	urldate = {2024-04-13},
	author = {Gadhave, Kiran and Cutler, Zach and Lex, Alexander},
	month = dec,
	year = {2023},
}

@inproceedings{biegel_u_2014,
	address = {New York, NY, USA},
	series = {{CHASE} 2014},
	title = {U can touch this: touchifying an {IDE}},
	isbn = {978-1-4503-2860-9},
	shorttitle = {U can touch this},
	url = {https://dl.acm.org/doi/10.1145/2593702.2593726},
	doi = {10.1145/2593702.2593726},
	abstract = {Touch gestures are not only often very intuitive, but their direct manipulation characteristics also help to reduce the cognitive load. Since software development poses complex cognitive demands, our goal is to exploit the advantages of direct manipulation to support professional software engineering processes. In this paper, we demonstrate how touch gestures can be used within a professional integrated development environment. As for that, we have enriched the Eclipse IDE with common and invertible multi-touch gestures which can be used for both controlling the graphical user interface and triggering built-in refactoring tools. The design of our extensions was informed by an early user study revealing problems of using the Eclipse IDE with the default touch support provided by the operating system. By using the emerging prototype during its implementation, we were able to iteratively improve the prototype based on our own experience and gain first insights into the potential of using direct manipulation methods within the IDE. First results suggest that using an additional touch device within the classical desktop setup enables a precise and fast work flow.},
	urldate = {2023-10-03},
	booktitle = {Proceedings of the 7th {International} {Workshop} on {Cooperative} and {Human} {Aspects} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Biegel, Benjamin and Hoffmann, Julien and Lipinski, Artur and Diehl, Stephan},
	month = jun,
	year = {2014},
	keywords = {IDE, Touch gestures, radial menus, refactoring, usability},
	pages = {8--15},
}

@inproceedings{costagliola_design_2018,
	title = {The design and evaluation of a gestural keyboard for entering programming code on mobile devices},
	url = {https://ieeexplore.ieee.org/abstract/document/8506501},
	doi = {10.1109/VLHCC.2018.8506501},
	abstract = {We present the design and the evaluation of a soft keyboard aimed at facilitating the input of programming code on mobile devices equipped with touch screens, such as tablets and smartphones. Besides the traditional tap on a key interaction, the keyboard allows the user to draw gestures on top of it. The gestures correspond to shortcuts to enter programming statements/constructs or to activate specific keyboard sub-layouts. The keyboard was compared in a user study to a traditional soft keyboard with a QWERTY layout and to another state-of-art keyboard designed for programming. The results show a significant advantage for our design in terms of speed and gesture per characters.},
	urldate = {2023-10-03},
	booktitle = {2018 {IEEE} {Symposium} on {Visual} {Languages} and {Human}-{Centric} {Computing} ({VL}/{HCC})},
	author = {Costagliola, Gennaro and Fuccella, Vittorio and Leo, Amedeo and Lomasto, Luigi and Romano, Simone},
	month = oct,
	year = {2018},
	note = {ISSN: 1943-6106},
	pages = {49--56},
}

@inproceedings{raab_refactorpad_2013,
	address = {New York, NY, USA},
	series = {{EICS} '13},
	title = {{RefactorPad}: editing source code on touchscreens},
	isbn = {978-1-4503-2138-9},
	shorttitle = {{RefactorPad}},
	url = {https://dl.acm.org/doi/10.1145/2494603.2480317},
	doi = {10.1145/2494603.2480317},
	abstract = {Despite widespread use of touch-enabled devices, the field of software development has only slowly adopted new interaction methods for available tools. In this paper, we present our research on RefactorPad, a code editor for editing and restructuring source code on touchscreens. Since entering and modifying code with on-screen keyboards is time-consuming, we have developed a set of gestures that take program syntax into account and support common maintenance tasks on devices such as tablets. This work presents three main contributions: 1) a test setup that enables researchers and participants to collaboratively walk through code examples in real-time; 2) the results of a user study on editing source code with both finger and pen gestures; 3) a list of operations and some design guidelines for creators of code editors or software development environments who wish to optimize their tools for touchscreens.},
	urldate = {2023-10-03},
	booktitle = {Proceedings of the 5th {ACM} {SIGCHI} symposium on {Engineering} interactive computing systems},
	publisher = {Association for Computing Machinery},
	author = {Raab, Felix and Wolff, Christian and Echtler, Florian},
	month = jun,
	year = {2013},
	keywords = {editor, gestures, ide, pen, refactoring, source code, surface, tablet, touchscreen},
	pages = {223--228},
}

@inproceedings{chung_talebrush_2022,
	address = {New York, NY, USA},
	series = {{CHI} '22},
	title = {{TaleBrush}: {Sketching} {Stories} with {Generative} {Pretrained} {Language} {Models}},
	isbn = {978-1-4503-9157-3},
	shorttitle = {{TaleBrush}},
	url = {https://dl.acm.org/doi/10.1145/3491102.3501819},
	doi = {10.1145/3491102.3501819},
	abstract = {While advanced text generation algorithms (e.g., GPT-3) have enabled writers to co-create stories with an AI, guiding the narrative remains a challenge. Existing systems often leverage simple turn-taking between the writer and the AI in story development. However, writers remain unsupported in intuitively understanding the AI’s actions or steering the iterative generation. We introduce TaleBrush, a generative story ideation tool that uses line sketching interactions with a GPT-based language model for control and sensemaking of a protagonist’s fortune in co-created stories. Our empirical evaluation found our pipeline reliably controls story generation while maintaining the novelty of generated sentences. In a user study with 14 participants with diverse writing experiences, we found participants successfully leveraged sketching to iteratively explore and write stories according to their intentions about the character’s fortune while taking inspiration from generated stories. We conclude with a reflection on how sketching interactions can facilitate the iterative human-AI co-creation process.},
	urldate = {2023-10-02},
	booktitle = {Proceedings of the 2022 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Chung, John Joon Young and Kim, Wooseok and Yoo, Kang Min and Lee, Hwaran and Adar, Eytan and Chang, Minsuk},
	month = apr,
	year = {2022},
	keywords = {controlled generation, creativity support tool, sketching, story generation, story writing},
	pages = {1--19},
}

@inproceedings{chung_promptpaint_2023,
	address = {New York, NY, USA},
	series = {{UIST} '23},
	title = {{PromptPaint}: {Steering} {Text}-to-{Image} {Generation} {Through} {Paint} {Medium}-like {Interactions}},
	isbn = {9798400701320},
	shorttitle = {{PromptPaint}},
	url = {https://dl.acm.org/doi/10.1145/3586183.3606777},
	doi = {10.1145/3586183.3606777},
	abstract = {While diffusion-based text-to-image (T2I) models provide a simple and powerful way to generate images, guiding this generation remains a challenge. For concepts that are difficult to describe through language, users may struggle to create prompts. Moreover, many of these models are built as end-to-end systems, lacking support for iterative shaping of the image. In response, we introduce PromptPaint, which combines T2I generation with interactions that model how we use colored paints. PromptPaint allows users to go beyond language to mix prompts that express challenging concepts. Just as we iteratively tune colors through layered placements of paint on a physical canvas, PromptPaint similarly allows users to apply different prompts to different canvas areas and times of the generative process. Through a set of studies, we characterize different approaches for mixing prompts, design trade-offs, and socio-technical challenges for generative models. With PromptPaint we provide insight into future steerable generative tools.},
	urldate = {2024-02-07},
	booktitle = {Proceedings of the 36th {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Chung, John Joon Young and Adar, Eytan},
	month = oct,
	year = {2023},
	keywords = {generative model, painting interactions, text-to-image generation},
	pages = {1--17},
}

@inproceedings{vaithilingam_towards_2023,
	address = {Melbourne, Australia},
	title = {Towards {More} {Effective} {AI}-{Assisted} {Programming}: {A} {Systematic} {Design} {Exploration} to {Improve} {Visual} {Studio} {IntelliCode}’s {User} {Experience}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {9798350300376},
	shorttitle = {Towards {More} {Effective} {AI}-{Assisted} {Programming}},
	url = {https://ieeexplore.ieee.org/document/10172834/},
	doi = {10.1109/ICSE-SEIP58684.2023.00022},
	abstract = {AI-driven code editor extensions such as Visual Studio IntelliCode and Github CoPilot have become extremely popular. These tools recommend inserting chunks of code, with the lines to be inserted presented inline at the current cursor location as gray text. In contrast to their popularity, other AIdriven code recommendation tools that suggest code changes (as opposed to code completions) have remained woefully underused. We conducted lab studies at Microsoft to understand this disparity and found one major cause: discoverability. Code change suggestions are hard to surface through bold, inline interfaces and hence, developers often do not even notice them.},
	language = {en},
	urldate = {2024-05-09},
	booktitle = {2023 {IEEE}/{ACM} 45th {International} {Conference} on {Software} {Engineering}: {Software} {Engineering} in {Practice} ({ICSE}-{SEIP})},
	publisher = {IEEE},
	author = {Vaithilingam, Priyan and Glassman, Elena L. and Groenwegen, Peter and Gulwani, Sumit and Henley, Austin Z. and Malpani, Rohan and Pugh, David and Radhakrishna, Arjun and Soares, Gustavo and Wang, Joey and Yim, Aaron},
	month = may,
	year = {2023},
	pages = {185--195},
}

@misc{zhu-tian_sketch_2024,
	title = {Sketch {Then} {Generate}: {Providing} {Incremental} {User} {Feedback} and {Guiding} {LLM} {Code} {Generation} through {Language}-{Oriented} {Code} {Sketches}},
	shorttitle = {Sketch {Then} {Generate}},
	url = {http://arxiv.org/abs/2405.03998},
	doi = {10.48550/arXiv.2405.03998},
	abstract = {Crafting effective prompts for code generation or editing with Large Language Models (LLMs) is not an easy task. Particularly, the absence of immediate, stable feedback during prompt crafting hinders effective interaction, as users are left to mentally imagine possible outcomes until the code is generated. In response, we introduce Language-Oriented Code Sketching, an interactive approach that provides instant, incremental feedback in the form of code sketches (i.e., incomplete code outlines) during prompt crafting. This approach converts a prompt into a code sketch by leveraging the inherent linguistic structures within the prompt and applying classic natural language processing techniques. The sketch then serves as an intermediate placeholder that not only previews the intended code structure but also guides the LLM towards the desired code, thereby enhancing human-LLM interaction. We conclude by discussing the approach's applicability and future plans.},
	urldate = {2024-05-09},
	publisher = {arXiv},
	author = {Zhu-Tian, Chen and Xiong, Zeyu and Yao, Xiaoshuo and Glassman, Elena},
	month = may,
	year = {2024},
	note = {arXiv:2405.03998 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Human-Computer Interaction},
}

@misc{wang_supernova_2024,
	title = {{SuperNOVA}: {Design} {Strategies} and {Opportunities} for {Interactive} {Visualization} in {Computational} {Notebooks}},
	shorttitle = {{SuperNOVA}},
	url = {http://arxiv.org/abs/2305.03039},
	doi = {10.1145/3613905.3650848},
	abstract = {Computational notebooks, such as Jupyter Notebook, have become data scientists' de facto programming environments. Many visualization researchers and practitioners have developed interactive visualization tools that support notebooks, yet little is known about the appropriate design of these tools. To address this critical research gap, we investigate the design strategies in this space by analyzing 163 notebook visualization tools. Our analysis encompasses 64 systems from academic papers and 105 systems sourced from a pool of 55k notebooks containing interactive visualizations that we obtain via scraping 8.6 million notebooks on GitHub. Through this study, we identify key design implications and trade-offs, such as leveraging multimodal data in notebooks as well as balancing the degree of visualization-notebook integration. Furthermore, we provide empirical evidence that tools compatible with more notebook platforms have a greater impact. Finally, we develop SuperNOVA, an open-source interactive browser to help researchers explore existing notebook visualization tools. SuperNOVA is publicly accessible at: https://poloclub.github.io/supernova/.},
	language = {en},
	urldate = {2024-05-09},
	author = {Wang, Zijie J. and Munechika, David and Lee, Seongmin and Chau, Duen Horng},
	month = mar,
	year = {2024},
	note = {arXiv:2305.03039 [cs]},
	keywords = {Computer Science - Human-Computer Interaction, Computer Science - Machine Learning},
}

@inproceedings{arawjo_write_2020,
	address = {New York, NY, USA},
	series = {{CHI} '20},
	title = {To {Write} {Code}: {The} {Cultural} {Fabrication} of {Programming} {Notation} and {Practice}},
	isbn = {978-1-4503-6708-0},
	shorttitle = {To {Write} {Code}},
	url = {https://dl.acm.org/doi/10.1145/3313831.3376731},
	doi = {10.1145/3313831.3376731},
	abstract = {Writing and its means have become detached. Unlike written and drawn practices developed prior to the 20th century, notation for programming computers developed in concert and conflict with discretizing infrastructure such as the shift-key typewriter and data processing pipelines. In this paper, I recall the emergence of high-level notation for representing computation. I show how the earliest inventors of programming notations borrowed from various written cultural practices, some of which came into conflict with the constraints of digitizing machines, most prominently the typewriter. As such, I trace how practices of "writing code" were fabricated along social, cultural, and material lines at the time of their emergence. By juxtaposing early visions with the modern status quo, I question long-standing terminology, dichotomies, and epistemological tendencies in the field of computer programming. Finally, I argue that translation work is a fundamental property of the practice of writing code by advancing an intercultural lens on programming practice rooted in history.},
	urldate = {2024-04-11},
	booktitle = {Proceedings of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Arawjo, Ian},
	month = apr,
	year = {2020},
	keywords = {culture, infrastructure, materiality, notation, programming},
	pages = {1--15},
}

@inproceedings{arawjo_notational_2022,
	address = {New York, NY, USA},
	series = {{UIST} '22},
	title = {Notational {Programming} for {Notebook} {Environments}: {A} {Case} {Study} with {Quantum} {Circuits}},
	isbn = {978-1-4503-9320-1},
	shorttitle = {Notational {Programming} for {Notebook} {Environments}},
	url = {https://dl.acm.org/doi/10.1145/3526113.3545619},
	doi = {10.1145/3526113.3545619},
	abstract = {We articulate a vision for computer programming that includes pen-based computing, a paradigm we term notational programming. Notational programming blurs contexts: certain typewritten variables can be referenced in handwritten notation and vice-versa. To illustrate this paradigm, we developed an extension, Notate, to computational notebooks which allows users to open drawing canvases within lines of code. As a case study, we explore quantum programming and designed a notation, Qaw, that extends quantum circuit notation with abstraction features, such as variable-sized wire bundles and recursion. Results from a usability study with novices suggest that users find our core interaction of implicit cross-context references intuitive, but suggests further improvements to debugging infrastructure, interface design, and recognition rates. Throughout, we discuss questions raised by the notational paradigm, including a shift from ‘recognition’ of notations to ‘reconfiguration’ of practices and values around programming, and from ‘sketching’ to writing and drawing, or what we call ‘notating.’},
	urldate = {2024-04-10},
	booktitle = {Proceedings of the 35th {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Arawjo, Ian and DeArmas, Anthony and Roberts, Michael and Basu, Shrutarshi and Parikh, Tapan},
	month = oct,
	year = {2022},
	keywords = {computational notebooks, pen-based interfaces, programming paradigms, quantum computing},
	pages = {1--20},
}

@misc{vaithilingam_dynavis_2024,
	title = {{DynaVis}: {Dynamically} {Synthesized} {UI} {Widgets} for {Visualization} {Editing}},
	shorttitle = {{DynaVis}},
	url = {http://arxiv.org/abs/2401.10880},
	abstract = {Users often rely on GUIs to edit and interact with visualizations - a daunting task due to the large space of editing options. As a result, users are either overwhelmed by a complex UI or constrained by a custom UI with a tailored, fixed subset of options with limited editing flexibility. Natural Language Interfaces (NLIs) are emerging as a feasible alternative for users to specify edits. However, NLIs forgo the advantages of traditional GUI: the ability to explore and repeat edits and see instant visual feedback. We introduce DynaVis, which blends natural language and dynamically synthesized UI widgets. As the user describes an editing task in natural language, DynaVis performs the edit and synthesizes a persistent widget that the user can interact with to make further modifications. Study participants (n=24) preferred DynaVis over the NLI-only interface citing ease of further edits and editing confidence due to immediate visual feedback.},
	language = {en},
	urldate = {2024-05-01},
	publisher = {arXiv},
	author = {Vaithilingam, Priyan and Glassman, Elena L. and Inala, Jeevana Priya and Wang, Chenglong},
	month = jan,
	year = {2024},
	note = {arXiv:2401.10880 [cs]},
	keywords = {Computer Science - Human-Computer Interaction},
}

@inproceedings{bier_documents_1991,
	address = {New Orleans, Louisiana, United States},
	title = {Documents as user interfaces},
	isbn = {978-0-89791-383-6},
	url = {http://portal.acm.org/citation.cfm?doid=108844.108994},
	doi = {10.1145/108844.108994},
	language = {en},
	urldate = {2024-04-14},
	booktitle = {Proceedings of the {SIGCHI} conference on {Human} factors in computing systems {Reaching} through technology - {CHI} '91},
	publisher = {ACM Press},
	author = {Bier, Eric A. and Pier, Ken},
	year = {1991},
	pages = {443--444},
}
