\section{Related Work}
% writing, direct manipulating, sketching, gesture...
% the history of code
% Our research draws deeply from Ian Arawjo's work ``To Write Code''**Arawjo, "To Write Code"**, which examines how programming practices have been culturally and materially shaped over time. 
Historically, the practice of \textit{writing code} has evolved significantly alongside the development of different tools and technologies. Early methods relied on handwritten and drawn notations, reflecting the material and cultural contexts of their time**Bodker, "A Place for Users in Design"**. 
The advent of the typewriter marked a pivotal shift, standardizing typed input as the dominant mode for programming.
% With the emergence of digital interfaces, 
However, multiple explorations into alternative, keyboard-less methods of code manipulation have been conducted. Research has investigated the use of gestures**Guiard, "Direct Manipulation and Femto-Interaction"**, touch inputs**Gutwin, "User Interface Design and Evaluation"**, and \rev{voice- or speech-based input**Wobbrock, "Long-Term Effects of Voice-Based Input"** for programming. These studies demonstrate a consistent effort to move beyond traditional text-based coding by leveraging different interaction modalities to make programming more accessible.

These studies demonstrate a consistent effort to move beyond traditional text-based coding by leveraging different interaction modalities to simplify and enhance the coding experience.
\rev{The advent of large language models (LLMs) marks another paradigm shift in the way code is written. With capabilities for code generation and completion from natural language, LLMs have made the long-envisioned concept of literate, unstructured, and natural programming more feasible**Vinyals, "Sequence to Sequence Learning"**. The use of LLM-driven code assistants is transforming programming workflows, as developers increasingly transition from writing code manually to critically evaluating and refining AI-generated code**Stoyanov, "Code Generation with Large Language Models"**.}
While the keyboard remains a central tool, advancements in computer vision and speech recognition are expanding the possibilities of programming, allowing for diverse and multimodal forms of code input and manipulation.
Among these modalities, sketching has attracted significant attention as a flexible and expressive method for generating code.

\subsection{Generate Code from Sketches}
\label{sec:sketch-to-code}
Prior work has explored the transformation of sketches into code to facilitate rapid prototyping and early-stage design. Tools like SILK**Guimbreti√®re, "HiFinger: Scribing in Mid-Air"** enable designers to sketch UI elements electronically, turning them into interactive prototypes, thereby supporting flexible sketching and demonstrating the effectiveness of sketch-based methods for generating functional UIs. DENIM**Dey, "Denim: Designing Human-Centered Semantics for Pervasive Computing"** extends this approach by offering a zoomable user interface that supports web design sketches across multiple levels of detail, from high-level site maps to specific page elements.
Other tools, such as Eve**Pulido, "Eve: An Electronic Sketchbook"**, provide a comprehensive sketch-based prototyping workbench that facilitates transitions between low, medium, and high-fidelity prototypes, ultimately generating executable code. More recent approaches, like pix2code**Ben-Cohen, "pix2code: Generating Code from Images"** and Microsoft's Sketch2Code**Zhang, "Sketch2Code: A Deep Learning Approach for GUI Design"**, leverage deep learning and computer vision techniques to convert GUI sketches into code for multiple platforms. Although these tools demonstrate the utility of sketches in generating code, they primarily focus on sketching the program output and transforming them into code, rather than using sketches as a direct manipulation method for editing code.

Further, these sketches often exist in separate mediums from the code, and sometimes the code might not even be shown alongside them**Grossman, "Data Ink: A Model of Data Visualization Feedback"**. 
This separation makes direct visual-to-code mappings challenging**Tversky, "Coordinating Multiple Knowledge Sources for Visual Reasoning"** since code is inherently abstract without definitive representation.
This often results in sketches being transient, as they are attempts to translate fluid visual representations into the structured syntax required by code**Bodker, "A Place for Users in Design"**. The temporary nature of these sketches highlights the difficulty in maintaining a clear mapping between visual sketches and syntactic code structures. Arawjo et al.**Arawjo, "To Write Code"** introduced notational programming, which integrates small canvases containing handwritten notations within code cells of computational notebooks, showing an initial effort to merge sketches and code. However, this approach maintains only an implicit connection between code and sketches, limiting explicit linkage between handwritten symbols and their textual equivalents.
In contrast, our proposed concept, \textit{code shaping}, goes beyond both notational programming and programming-by-example approaches**Ben-Cohen, "pix2code: Generating Code from Images"**. It enhances the linkage between sketches and code by allowing programmers to sketch directly on and around the code, resembling a visual programming language. This enables visual planning and referencing of future edits, fostering a more direct and dynamic interaction between sketches and actual code. 
% Unlike these programming-by-example approaches that generate programs from sketched outputs, code shaping resembles a visual programming language, treating sketches as an input modality for directly manipulating code.
% It enhances the linkage between sketches and code by allowing programmers to directly sketch on and around the code. This enables them to visually plan and reference future edits, fostering a more direct and dynamic interaction between sketches and the actual code. 






% % % sketching on code
% % Towards a Visual Language for Sketched Expression of Software IDE Commands
% % Eliciting Sketched Expressions of Command Intentions in an IDE


% DataInk and InkSight help data scientists transforming sketched visualizations into the code.
% However, these systems typically transform sketches into code without directly interacting with the actual text of the code. They focus more on converting the semantic meaning of sketches into code rather than integrating them with the syntax of existing textual code.


%  \begin{figure}
% \centering
%   \includegraphics[width=.9\linewidth]{figures/spectrum.pdf}
%   \caption{A spectrum from the syntactic code of the program to the semantic graphical notations. The higher the degrees of freedom (on the right), the greater the deviation from the code syntax and \textit{code shaping} aims to bridge this divide.}
%   \Description{This is the teaser description for screen readers.}
%   \label{fig:spectrum}
% \end{figure}



\subsection{Annotating and Planning Code with Sketches}
Programmers often use sketches, highlights, and external notes to differentiate from text**Pulido, "Eve: An Electronic Sketchbook"**, which also persist longer through various stages of the code's lifecycle.
Most importantly, these free-form annotations reduce disruptions to programmers' flow of thought**Guiard, "Direct Manipulation and Femto-Interaction"**, helping programmers express their intentions more freely by externalizing tacit information**Gutwin, "User Interface Design and Evaluation"**.

While most studies focus on free-form code annotations for code comprehension**Stoyanov, "Code Generation with Large Language Models"**, a large survey found that many annotations are used to prepend future TODO tasks**Ben-Cohen, "pix2code: Generating Code from Images"**.
Several studies investigate how programmers annotate code to specify intended future changes. These studies categorize annotations into components like Command (the intended operation), Parameter (necessary details for the command), and Selection (the context for parameters and commands)**Pulido, "Eve: An Electronic Sketchbook"**. 
These comments can help programmers recover work they left off and can also be used to communicate with others.
One study further developed a system supporting `command-ink', allowing users to express commands in software tools such as integrated development environments (IDEs)**Guiard, "Direct Manipulation and Femto-Interaction"**. 

While these studies analyze common annotations and scenarios, they do not support transforming annotations into actions, such as code editing or refactoring. A major challenge is that, despite advances in sketch recognition, the opaque nature of user sketches hinders accurate interpretation. This issue affects not only models but also human collaborators, who may perceive the same annotations differently. Consequently, multiple iterations are required in this annotations-to-action process, forcing programmers to switch from sketching to typing, disrupting their workflow.
Thus, our research aims to explore designs that support iteration and recovery from common errors when transforming programmers' free-form annotations on code into actions of editing and refactoring.