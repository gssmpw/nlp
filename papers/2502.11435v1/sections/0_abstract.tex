\begin{abstract}
Current Large Language Model (LLM) agents demonstrate strong reasoning and tool use capabilities, but often lack self-awareness, failing to balance these approaches effectively. This imbalance leads to \textbf{Tool Overuse}, where models unnecessarily rely on external tools for tasks solvable with parametric knowledge, increasing computational overhead.
Inspired by human metacognition, we introduce \textbf{SMART} (\uline{S}trategic \uline{M}odel-\uline{A}ware \uline{R}easoning with \uline{T}ools), a paradigm that enhances an agentâ€™s self-awareness to optimize task handling and reduce tool overuse. 
To support this paradigm, we introduce \textbf{SMART-ER}, a dataset spanning three domains, where reasoning alternates between parametric knowledge and tool-dependent steps, with each step enriched by rationales explaining when tools are necessary.
Through supervised training, we develop \textbf{SMARTAgent}, a family of models that dynamically balance parametric knowledge and tool use.
Evaluations show that SMARTAgent reduces tool use by 24\% while improving performance by over 37\%, enabling 7B-scale models to match its 70B counterpart and GPT-4o. Additionally, SMARTAgent generalizes to out-of-distribution test data like GSM8K and MINTQA, maintaining accuracy with just one-fifth the tool calls.
These highlight the potential of strategic tool use to enhance reasoning, mitigate overuse, and bridge the gap between model size and performance, advancing intelligent and resource-efficient agent designs.\footnote{\ Data and codes released at \url{https://github.com/qiancheng0/Open-SMARTAgent}}
\end{abstract}
