\section{Method}

\begin{figure}[!t]
    \centering
    \subfigure{\includegraphics[width=\linewidth]{figures/introduction_cases.png}}
    \caption{Three example queries and their reasoning chains from each domain. The inherent compositionality of a query naturally divides reasoning into knowledge-driven steps and tool-reliant steps.}
    \label{fig:intro_case}
\end{figure}

To address the challenge of tool overuse, we draw inspiration from how humans balance internal knowledge and external tools. Metacognitive theory~\citep{schraw1995metacognitive} suggests that human decision-making relies on an implicit awareness of knowledge boundaries, enabling strategic, step-by-step problem-solving~\citep{livingston2003metacognition}. Inspired by this, we aim to equip agent models with a similar capability—calibrating their metacognition to optimize reasoning and tool use.

To address this, we propose \textbf{SMART}, a data-driven approach that enhances self-awareness in agent models. While LLMs acquire broad knowledge from large-scale corpora~\citep{wang2022generalizing}, they are not explicitly trained to recognize their own strengths and limitations. To bridge this gap, we introduce \textbf{SMART-ER}, the first dataset contrasting areas where models excel versus struggle. Covering three domains with 3K+ questions and structured reasoning chains, SMART-ER helps agents strategically decide when to rely on internal knowledge or external tools.  

\begin{figure*}[!t]
    \centering
    \subfigure{\includegraphics[width=\linewidth]{figures/data_pipeline.png}}
    \caption{The data pipeline to get SMART-ER. We divide the whole pipeline into several stages for better control and quality of the generated reasoning chain.}
    \label{fig:pipeline}
\end{figure*}

\subsection{Data Collection}
To train agents to strategically balance parametric knowledge and external tools within a single reasoning chain, questions must be compositional—blending aspects the model excels at with those it struggles with. Building on prior studies~\citep{hendrycks2021measuring, vu2023freshllms, qian2024tell}, we identify three key limitations in LMs: i) \textit{math reasoning}, where models struggle with complex computations requiring precise answers; ii) \textit{temporal knowledge}, as LMs lack access to up-to-date facts beyond their training cutoff; and iii) \textbf{user intent understanding}, where implicit preferences cannot be inferred without direct queries.
All these challenges necessitate a smarter integration of external tools with the model’s reasoning ability. Building on this insight, we construct data of three domains: 
\begin{itemize}[topsep=2pt, partopsep=-5pt, leftmargin=8pt, itemsep=-4.5pt]
\item \textbf{Math}: Adapted from MATH \citep{hendrycks2021measuring}, each query incorporates both challenging math deductions and simple arithmetic to contrast reasoning capabilities.
\item \textbf{Time}: Adapted from FreshQA \citep{vu2023freshllms}, each query ensures a mix of fast-changing and slow-changing factual knowledge.
\item \textbf{Intention}: Adapted from Intention-in-Interaction (IN3) \citep{qian2024tell}, each query requires explicit user intent while remaining solvable within the model’s capabilities.
\end{itemize}
This compositional approach helps models calibrate their decision-making by distinguishing when to rely on external tools versus when internal knowledge is sufficient. To illustrate this, we present three example queries from each domain in \Cref{fig:intro_case}. For details on the question selection and adaptation process, please refer to \Cref{apdx:data_slection}.

\begin{table}[!t]
    \centering
    \subfigure{\includegraphics[width=0.9\linewidth]{figures/data_statistic.png}}
    \caption{Statistics for SMART-ER. \textit{T/K Ratio} denotes the ratio of tool-reliant to knowledge-driven steps.}
    \label{tab:data_statistic}
\end{table}

\subsection{Reasoning Chain Construction}
As shown in \Cref{fig:pipeline}, each query \( Q \) is decomposed into a structured reasoning plan with \( n \) subgoals, \( S = \{s_1, s_2, \dots, s_n\} \). This decomposition is enabled by the compositional nature of our queries and is empirically achieved using GPT-4o, an auxiliary model in our pipeline, later denoted as \( M \).
Next, for each \( s_i \), we determine whether it requires tool use (\( A(s_i) = 1 \)) or can be resolved with parametric knowledge alone (\( A(s_i) = 0 \)). Using ground truth from existing source data as heuristics, we guide \( M \) to annotate each subgoal. During this process, we also discard those queries where all subgoals rely exclusively on either tools or parametric knowledge.
After annotating the entire chain, we process each subgoal iteratively, starting from \( s_1 \). For each subgoal \( s_i \) where \( A(s_i) = 1 \), we assign an appropriate tool \( t_i \) from a predefined tool set using a mapping function \( T(\cdot) \):
\[
t_i =
\begin{cases} 
T(s_i), & \text{if } A(s_i) = 1 \\ 
\varnothing, & \text{otherwise} 
\end{cases}
\]
where \( t_i = \varnothing \) indicates the model relies solely on its parametric knowledge for reasoning. Empirically, our tool set consists of \textit{Code}, \textit{Search}, and \textit{AskUser}, covering all designed domains.

Next, we proceed with the reasoning process using \( M \). If \( A(s_i) = 0 \), \( M \) reasons over \( s_i \), producing a reasoning step \( k_i \) based on its parametric knowledge. Otherwise, we prompt \( M \) to generate the necessary parameters \( p_i \) for tool invocation, retrieving the tool output \( o_i \). The resulting outcome for each step is formulated as:
\[
r_i =
\begin{cases} 
(\ p_i = M(s_i),\ o_i = t_i(p_i)\ ), & \text{if } A(s_i) = 1 \\ 
(\ k_i = M(s_i)\ ), & \text{otherwise} 
\end{cases}
\]
where \( t_i(\cdot) \) represents the invocation of tool \( t_i \). The iterative process also enables \( M \) to incorporate information from prior steps and tool outputs when processing subsequent subgoals, ensuring a coherent and context-aware reasoning flow.

Inspired by metacognitive heuristics that implicitly guide human reasoning, we refine the reasoning chain \( r_i \) by explicitly incorporating justifications for whether parametric knowledge suffices or external tool use is necessary.
Specifically, we prompt \( M \) to generate a justification \( j_i = M(s_i, A(s_i)) \), conditioned on the subgoal \( s_i \) and its annotation \( A(s_i) \). This approach emulates human metacognition by transforming implicit heuristics into explicit natural language explanations, thus enhancing interpretability. Similar to Chain-of-Thought~\citep{wei2022chain} leverages the cumulative probability nature of autoregressive models to guide reasoning, \( j_i \) helps the model calibrate its decision-making, improving its ability to strategically balance internal knowledge and external tools.

Finally, by integrating all subgoals, we obtain the complete reasoning chain \( R = \{(r_1, j_1), \dots, (r_n, j_n)\} \) for query \( Q \), where each step \( r_i \) is either \( (k_i) \), indicating a parametric knowledge-driven step, or \( (p_i, o_i) \), representing a tool-reliant step. Our method dynamically integrates these steps, ensuring an adaptive balance between internal reasoning and external tool use. To ensure quality, we conduct human supervision on 5\% of the data for each step involving \( M \), achieving a pass rate of over 95\%. Please refer to \Cref{apdx:chain_construction} for details.

\subsection{Agent Training Implementation}
We partition SMART-ER into training and test splits with statistics in \Cref{tab:data_statistic}. For each \((Q, R')\) in the training set, we generate multiple input-output pairs for instruction tuning. The input comprises \(\{Q, (r_1, j_1), \dots, (r_{x_i}, j_{x_i})\}\), while the output consists of \(\{(r_{{x_i}+1}, j_{{x_i}+1}), \dots, (r_{x_{i+1}}, j_{x_{i+1}})\}\},\) where \( x_i \) indexes the tool-reliant steps. This setup ensures iterative reasoning, allowing the agent to leverage prior steps until the next tool invocation or final solution. The number of input-output pairs per \((Q, R')\) also equals the number of tool-reliant steps, facilitating interactive inference.

Using these instruction pairs, we finetune the Llama-3.1 8B and 70B instruct models~\citep{dubey2024llama} as well as the Mistral 7B, Nemo(12B) and Small(24B) instruct models~\citep{jiang2023mistral}, adapting them into a family of \textbf{SMARTAgent}. These agent models enable interactive tool use, recognizes its own limitations, and balances tool reliance with parametric knowledge-driven reasoning to prevent tool overuse. See \Cref{apdx:training} for training details and hyper-parameters.
