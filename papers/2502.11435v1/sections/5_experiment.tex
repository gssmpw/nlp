\begin{table*}[t]
    \centering
    \setlength\tabcolsep{2pt}
    \setlength\extrarowheight{2pt}
    
    \resizebox{1.0\linewidth}{!}{
    \begin{tabular}{l @{\hskip 14pt} l c c @{\hskip 14pt} c c @{\hskip 14pt} c c c}
    
        \toprule
        
        \multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{Model}} & \multicolumn{2}{c}{\textbf{Math} (MATH)} & \multicolumn{2}{c}{\textbf{Time} (FreshQA)} & \multicolumn{3}{c}{\textbf{Intention} (Intention-in-Interaction)} \\

        \cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-9}
        
        ~ & ~ & \textbf{\makecell{Tool Used$^\downarrow$\\(\textit{Times})}} & \hskip 8pt \textbf{\makecell{Accuracy$^\uparrow$\\(\%)}} & \textbf{\makecell{Tool Used$^\downarrow$\\(\textit{Times})}} & \hskip 8pt \textbf{\makecell{Accuracy$^\uparrow$\\(\%)}} & \textbf{\makecell{Tool Used$^\downarrow$\\(\textit{Times})}}  & \hskip 8pt \textbf{\makecell{Missing Details Recovery$^\uparrow$\\(Lv3 / Lv2, \%)}} & \textbf{\makecell{Summarized Intention \\Coverage$^\uparrow$ (\%)}}  \\

        \addlinespace[2pt]
        \midrule
        \addlinespace[2pt]
        \multicolumn{9}{c}{\textit{Open-Source}} \\ 
        \midrule

        \multirow{2}{*}{\makecell[l]{Normal\\Reasoning Trained}} & \textit{Mistral-7B} & 0.00 & 17.00 & 0.00 & 48.00 & 0.00 & 41.86 / 43.84 & - \\
        ~ & \textit{Llama-3.1-8B} & 0.00 & 41.00 & 0.00 & 48.00 & 0.00 & 38.37 / 42.49 & - \\

        \addlinespace[2pt]
        \midrule
        \addlinespace[2pt]

        \multirow{5}{*}{\makecell[l]{Base Model\\Reasoning Prompt}} & \textit{Mistral-7B} & 0.00 & 17.25 & 0.00 & 29.00 & 0.00 & 37.21 / 33.06 & - \\ 
        ~ & \textit{Llama-3.1-8B} & 0.00 & 53.00 & 0.00 & 26.00 & 0.00 & 40.70 / 25.76 & - \\
        ~ & \textit{Mistral-Nemo(12B)} & 0.00 & 47.00 & 0.00 & 33.00 & 0.00 & 44.19 / 28.37 & - \\
        ~ & \textit{Mistral-Small(24B)} & 0.00 & 72.25 & 0.00 & 34.00 & 0.00 & 41.86 / 31.82 & - \\ 
        ~ & \textit{Llama-3.1-70B} & 0.00 & 70.00 & 0.00 & 36.00 & 0.00 & 41.86 / 29.24 & - \\

        \addlinespace[2pt]
        \midrule
        \addlinespace[2pt]
        
        \multirow{5}{*}{\makecell[l]{Base Model\\Tool Prompt}} & \textit{Mistral-7B} & 3.90 & 13.25 & 1.67 & 49.00 & 3.80 & 48.84 / 21.70 & 63.04 \\ 
        ~ & \textit{Llama-3.1-8B} & 1.93 & 51.00 & 2.05 & 56.00 & 3.77 & 54.76 / 25.90 & 70.20 \\ 
        ~ & \textit{Mistral-Nemo(12B)} & 2.35 & 46.00 & 1.19 & 59.00 & 1.80 & 31.35 / 5.82 & 59.27 \\
        ~ & \textit{Mistral-Small(24B)} & 1.55 & 76.00 & 1.73 & 62.00 &  2.52 & 45.74 / 33.62 & 78.20 \\
        ~ & \textit{Llama-3.1-70B} & 3.53 & 67.50 & 2.08 & 63.00 & 2.71 & 45.74 / 35.96 & 61.68 \\ 

        \addlinespace[2pt]
        \midrule
        \addlinespace[2pt]

        \multirow{6}{*}{\textbf{SMARTAgent}} & \textit{Mistral-7B} & 0.60$_{\green{\downarrow 3.30}}$ & 22.75$_{\green{\uparrow 5.50}}$ & 1.00$_{\green{\downarrow 0.67}}$ & 64.00$_{\green{\uparrow 15.00}}$ & 3.60$_{\green{\downarrow 0.20}}$ & 74.42$_{\green{\uparrow 25.58}}$ / 65.44$_{\green{\uparrow 21.60}}$ & 81.76$_{\green{\uparrow 18.72}}$ \\  
        ~ & \textit{Llama-3.1-8B} & 0.88$_{\green{\downarrow 1.05}}$ & 54.75$_{\green{\uparrow 1.75}}$ & 1.05$_{\green{\downarrow 1.00}}$ & 67.00$_{\green{\uparrow 11.00}}$ & 3.80$_{\red{\uparrow 0.03}}$ & \textbf{81.40}$_{\green{\uparrow 26.64}}$ / 67.41$_{\green{\uparrow 24.92}}$ & 78.28$_{\green{\uparrow 8.08}}$ \\
        ~ & \textit{Mistral-Nemo(12B)} & 0.82$_{\green{\downarrow 1.53}}$ & 49.50$_{\green{\uparrow 2.50}}$ & 1.00$_{\green{\downarrow 0.19}}$ & \textbf{70.00}$_{\green{\uparrow 11.00}}$ & 3.34$_{\red{\uparrow 1.54}}$ & 77.91$_{\green{\uparrow 33.72}}$ / 62.15$_{\green{\uparrow 33.78}}$ & 82.30$_{\green{\uparrow 23.03}}$ \\
        ~ & \textit{Mistral-Small(24B)} & 0.79$_{\green{\downarrow 0.76}}$ & 69.75$_{\red{\downarrow 6.25}}$ & 1.00$_{\green{\downarrow 0.73}}$ & 66.00$_{\green{\uparrow 4.00}}$ & 3.89$_{\red{\uparrow 1.37}}$ & 74.42$_{\green{\uparrow 28.68}}$ / \textbf{68.87}$_{\green{\uparrow 35.25}}$ & 84.99$_{\green{\uparrow 6.79}}$ \\ 
        % ~ & \textit{Llama-3.3-70B} & 0.61$_{\green{\downarrow 2.92}}$ & \textbf{76.25}$_{\green{\uparrow 6.25}}$ & 1.00$_{\green{\downarrow 1.08}}$ & 65.00$_{\green{\uparrow 2.00}}$ & 3.15$_{\red{\uparrow 0.44}}$ & 61.63$_{\green{\uparrow 15.89}}$ / 59.01$_{\green{\uparrow 23.05}}$ & 84.45$_{\green{\uparrow 22.77}}$ \\
        ~ & \textit{Llama-3.1-70B} & 0.94$_{\green{\downarrow 2.59}}$ & \textbf{72.50}$_{\green{\uparrow 2.50}}$ & 1.01$_{\green{\downarrow 1.07}}$ & 66.00$_{\green{\uparrow 3.00}}$ & 3.51$_{\red{\uparrow 0.80}}$ & 68.60$_{\green{\uparrow 22.86}}$ / 58.15$_{\green{\uparrow 22.19}}$ & \textbf{86.09}$_{\green{\uparrow 24.41}}$ \\
        
        \addlinespace[2pt]
        \cmidrule(lr){2-9}
        
        ~ & \multicolumn{3}{r}{Tool Used Macro-Average Decrease (\%)} & \textbf{24.00} & \multicolumn{3}{r}{Performance Macro-Average Increase (\%)} & \textbf{37.10} \\
        
        \addlinespace[2pt]
        \midrule
        \addlinespace[2pt]
        \multicolumn{9}{c}{\textit{Closed-Source}} \\ 
        \midrule

        \multirow{2}{*}{\makecell[l]{Base Model\\Reasoning Prompt}} & \textit{GPT-4o-mini} & 0.00 & 73.00 & 0.00 & 44.00 & 0.00 & 45.35 / \textbf{32.41} & - \\ 
        ~ & \textit{GPT-4o} & 0.00 & \textbf{79.50} & 0.00 & 47.00 & 0.00 & 38.37 / 28.54 & - \\ 

        \addlinespace[2pt]
        \midrule
        \addlinespace[2pt]
        
        \multirow{2}{*}{\makecell[l]{Base Model\\Tool Prompt}} & \textit{GPT-4o-mini} & 2.55 & 54.50 & 1.06 & 56.00 & 1.91 & \textbf{50.00} / 26.90 & 76.44 \\ 
        ~ & \textit{GPT-4o} & 0.27 & 79.25 & 1.01 & \textbf{65.00} & 1.17 & 40.70 / 15.61 & \textbf{86.80} \\ 
        
        \bottomrule
    \end{tabular}
    }

    \caption{
    SMARTAgent's performance on the test split across three in-domain task categories. The \green{green} and \red{red} arrows indicate \green{better} or \red{worse} performance compared to the best baseline method. Its strong performance and fewer tool calls highlight SMARTAgent's efficient and strategic tool use.
    }
    \label{tab:result_main}
\end{table*}


\section{Experiment}

In this section, we present results demonstrating SMARTAgent’s effectiveness in reducing tool overuse while enhancing reasoning performance.

\subsection{Settings}

\paragraph{Data.} For in-domain testing, we evaluate SMARTAgent using the test split of adapted SMART-ER data across three domains: Math (MATH), Time (FreshQA), and Intention (IN3). For out-of-distribution (OOD) testing, we assess performance on GSM8K~\citep{cobbe2021training} and MINTQA~\citep{mintqa}, which test logical reasoning and real-world knowledge.

\paragraph{Baselines.} We incorporate three main baselines:
i) \textit{Normal Reasoning Trained}: For each domain, we train the model using the training set queries to perform reasoning without tools, leveraging the original solution chain or ground truth.
ii) \textit{Base Model Reasoning Prompt}: We directly prompt the model to apply chain-of-thought reasoning without tools to solve the problem.
iii) \textit{Base Model Tool Prompt}: We provide the model with all available tools and their usage but allow it to decide independently whether and when to use them.

\paragraph{Inference.} For reasoning without tools, the model generates a response including the final answer. For tool-reliant reasoning, the inference is performed in an interactive manner: in each round, if a tool call is detected, we parse and execute it, integrating the tool's output and reasoning into the input. This repeats until the final answer is reached. See \Cref{apdx:experiment} for details.

\paragraph{Metrics.} We use two main evaluation metrics: \textit{Tool Used}, which measures the average number of times a tool is leveraged during reasoning, and \textit{Accuracy}, which evaluates the average performance across queries. For the IN3 dataset, where answers depend on user preferences and lack a single correct response, we adopt the original paper’s metrics: \textit{Missing Details Recovery}, assessing whether missing details in vague instructions are recovered, and \textit{Summarized Intention Coverage}, assessing whether the final response covers all user-stated preferences.


% ========== OOD Results ==========
\begin{table}[t]
    \centering

    \setlength\tabcolsep{2pt}
    \setlength\extrarowheight{2pt}
    \resizebox{1.0\linewidth}{!}{
    
    \begin{tabular}{l c c @{\hskip 14pt} c c}
    
        \toprule
        
        \textbf{Dataset} & \multicolumn{2}{c}{\textbf{GSM8K}} & \multicolumn{2}{c}{\textbf{MINTQA}} \\

        \cmidrule(lr){2-3} \cmidrule(lr){4-5}
        
        \textbf{Metrics} & \textbf{\makecell{Tool Used$^\downarrow$\\(\textit{Times})}} & \hskip 8pt \textbf{\makecell{Accuracy$^\uparrow$\\(\%)}} & \textbf{\makecell{Tool Used$^\downarrow$\\(\textit{Times})}} & \hskip 8pt \textbf{\makecell{Accuracy$^\uparrow$\\(\%)}} \\

        \addlinespace[2pt]
        \midrule
        \addlinespace[2pt]
        
        \multicolumn{5}{c}{\textit{Llama-3.1-8B}}  \\ 
        \midrule

        {\fontsize{10}{10}\selectfont Normal Reasoning Trained} & 0.00 & 80.29 & 0.00 & 21.65 \\ 
        {\fontsize{10}{10}\selectfont Base Model Reasoning Prompt} & 0.00 & 82.26 & 0.00 & 12.37 \\
        {\fontsize{10}{10}\selectfont Base Model Tool Prompt} & 2.53 & 83.17 & 4.03 & 16.49 \\ 
        {\fontsize{10}{10}\selectfont \textbf{SMARTAgent}} & \textbf{0.76}$_{\green{\downarrow 1.77}}$ & \textbf{83.40}$_{\green{\uparrow 0.23}}$ & \textbf{1.06}$_{\green{\downarrow 2.97}}$ & \textbf{29.90}$_{\green{\uparrow 8.25}}$ \\
        
        \addlinespace[2pt]
        \midrule
        \addlinespace[2pt]
        
        \multicolumn{5}{c}{\textit{Mistral-7B}} \\
        
        \midrule
        {\fontsize{10}{10}\selectfont Normal Reasoning Trained} & 0.00 & 58.68 & 0.00 & 21.65 \\ 
        {\fontsize{10}{10}\selectfont Base Model Reasoning Prompt} & 0.00 & 50.57 & 0.00 & 19.59 \\ 
        {\fontsize{10}{10}\selectfont Base Model Tool Prompt} & 3.56 & 55.34 & 6.46 & 10.31 \\ 
        {\fontsize{10}{10}\selectfont \textbf{SMARTAgent}} & \textbf{0.45}$_{\green{\downarrow 3.11}}$ & \textbf{58.98}$_{\green{\uparrow 0.30}}$ & \textbf{0.99}$_{\green{\downarrow 5.47}}$ & \textbf{25.77}$_{\green{\uparrow 4.12}}$ \\
        
        \bottomrule
    \end{tabular}
    }
    \label{tab:results_ood}
    \caption{
    SMARTAgent's performance on out-of-distribution tasks compared with baseline methods. Results show SMARTAgent can successfully generalize.
    }
\end{table}


\subsection{Main Results}
We present the main results in \Cref{tab:result_main}, along with the baseline performance of GPT-4o and GPT-4o-mini for comparison. We also present the OOD results for Mistral-7B and Llama-3.1-8B in \Cref{tab:results_ood}, highlighting the following key findings.

\vspace{1.5mm}
\noindent\textbf{SMARTAgent solves tasks efficiently.}
Compared to the base model in \Cref{tab:result_main}, which autonomously decides whether to use tools, SMARTAgent reduces tool usage time per query by 24\% on average. At the same time, its performance improves by over 37\% across models compared to the best baseline. This demonstrates SMARTAgent's efficiency in tool use, achieving higher results while relying less on external resources.

\vspace{1.5mm}
\noindent\textbf{7B-scale SMARTAgent can outperform GPT-4o baselines.}
Despite being much smaller, the 7B- and 8B-scale SMARTAgent models can outperform GPT-4o and its 70B counterpart in Time and Intention domains while using fewer tool calls, showcasing their efficient tool use. In Math, where reasoning scales with model size, SMARTAgent lags behind larger models but remains competitive against baselines using the same architecture. These results demonstrate that strategic tool use can bridge the gap between model size and performance, making SMARTAgent a resource-efficient yet powerful alternative.

\vspace{1.5mm}
\noindent\textbf{SMARTAgent generalizes to OOD settings.}
As shown in \Cref{tab:results_ood}, SMARTAgent effectively reduces tool calls while achieving better overall performance on OOD test benchmarks. Notably, SMARTAgent makes only one-fifth the number of tool calls compared to the base model in MINTQA, where tool prompting often leads to excessive reliance and decreased accuracy.
% \heng{put this into intro} \cheng{put it}

\vspace{1.5mm}
\noindent\textbf{Improper tool uses degrade performance.}  
In the MINTQA and Math domain data, we find that arbitrary tool use can degrade performance compared to standard chain-of-thought reasoning. This aligns with our argument in \Cref{sec:preliminary_llm} that excessive tool reliance can introduce unpredictable side effects, causing models to struggle with interactive tool calls. As a result, inference may become prolonged over multiple rounds, ultimately leading to incorrect answers. Additionally, we observe that larger-scale models, including GPT-4o, use tools less frequently in the Intention domain data, resulting in a greater performance drop than even the 7B-scale SMARTAgent. This may stem from their overconfidence in assisting users, leading them to overlook specific user preferences.

\vspace{1.5mm}
\noindent\textbf{SMARTAgent achieves near-optimal tool use.}
Datasets such as Time and MINTQA contain up-to-date knowledge necessitating tool use. Ideally, at least one tool call per query is required for a correct answer, and SMARTAgent consistently maintains an average close to one, reflecting near-optimal efficiency. Similarly, in the Intention domain, where queries contain two to four missing details, SMARTAgent invokes tools three times per query, aligning with the expected need.

\begin{table*}[!t]
    \centering
    \subfigure{\includegraphics[width=\linewidth]{figures/analysis_error.png}}
    \caption{Error analysis of common task failure causes, highlighting several error aspects: timing of tool calls, hyper-parameters provided, and handling of tool call feedback, with explanations and examples.}
    \label{tab:analysis_error}
\end{table*}


\begin{table}[t]
    \centering
    % \setlength\tabcolsep{2pt}
    % \setlength\extrarowheight{2pt}
    % \resizebox{0.8\linewidth}{!}{
    % \begin{tabular}{l @{\hskip 10pt} l c c}
    
    %     \toprule
        
    %     \multirow{2}{*}{\textbf{Dataset}} & \multirow{2}{*}{\textbf{Model}} & \multicolumn{2}{c}{\textbf{Tool Overuse$^\downarrow$}} \\

    %     \cmidrule(lr){3-4}
        
    %     ~ & ~ & \textbf{Tool Prompt} & \textbf{SMARTAgent} \\

    %     \midrule

    %     \multirow{2}{*}{{\fontsize{10}{10}\selectfont GSM8K}} & {\fontsize{10}{10}\selectfont \textit{Llama-3.1-8B}} & 66.20 & \textbf{37.77 }\\
    %     ~ & {\fontsize{10}{10}\selectfont \textit{Mstral-7B}} & 30.25 & \textbf{15.76} \\
        
    %     \midrule
        
    %     \multirow{2}{*}{{\fontsize{10}{10}\selectfont \makecell{Math\\(MATH)}}} & {\fontsize{10}{10}\selectfont \textit{Llama-3.1-8B}} & 35.59 & \textbf{35.22} \\
    %     ~ & {\fontsize{10}{10}\selectfont \textit{Mstral-7B}} & 12.47 & \textbf{7.77} \\
        
    %     \bottomrule
    % \end{tabular}
    % }
    \includegraphics[width=0.9\linewidth]{figures/analysis_abuse.png}

    \label{tab:results_abuse}
    \caption{
    Statistics of \textit{tool overuse} on GSM8K and Math domain test data, following the definition in \Cref{sec:tool_overuse}.
    }

\end{table}


\begin{figure*}[!t]
    \centering
    \subfigure{\includegraphics[width=\linewidth]{figures/analysis_case.png}}
    \caption{A case study comparing Tool Prompting and SMARTAgent reveals that SMARTAgent excels by creating thorough plans, demonstrating awareness of its limitations, and making justified, wiser tool call decisions.}
    \label{fig:analysis_case}
\end{figure*}


\begin{figure}[!t]
    \centering
    \subfigure{\includegraphics[width=0.92\linewidth]{figures/analysis_confidence.png}}
    \caption{Confidence analysis shows that SMART effectively enhances the model's decision-making confidence in selecting the correct reasoning approaches.}
    \label{fig:analysis_confidence}
\end{figure}


\subsection{Analysis and Case Studies}

\noindent\textbf{SMARTAgent effectively reduces tool overuse.} Beyond measuring tool use per query, we calculate the tool overuse rate, as defined in \Cref{sec:tool_overuse}, and report results in \Cref{tab:results_abuse} for GSM8K and Math domain test data. Notably, SMARTAgent reduces unnecessary tool calls by up to 50\% compared to prompting the base model with tool access. However, despite this reduction, tool overuse persists, which we further examine in error analysis.

\vspace{1.5mm}
\noindent\textbf{Error analysis.} We provide error analysis in \Cref{tab:analysis_error}, highlighting common failure causes. Tool prompting leads to errors across all categories, while SMARTAgent reduces repetitive calls and improves argument accuracy. However, feedback neglect still causes tool invocation failures, particularly with the \textit{Code} tool, and excessive caution in ensuring calculation accuracy adds overhead. This mirrors human task-solving, where we sometimes rely on calculators despite knowing the steps. Future work may explore balancing convenience, budget, and efficiency to enhance decision-making.


\vspace{1.5mm}
\noindent\textbf{Case Study.} In \Cref{fig:analysis_case}, we compare the solution chains of SMARTAgent and the base model with tool prompting. SMARTAgent demonstrates logical planning, context corroboration, and an awareness of its limitations and knowledge boundaries, with clear justifications for its decisions. This metacognitive approach closely mirrors human reasoning processes, making SMARTAgent's reasoning more interpretable and significantly reducing tool use overhead.


% \vspace{1.5mm}
% \noindent\textbf{Confidence Validation Experiment.}
% To evaluate SMARTAgents' ability to choose between internal reasoning and tool invocation, we conducted experiments using special tokens to analyze decision confidence. Specifically, we trained the model on Time and Intention domains, introducing special tokens: ``[[Reasoning]]'' for internal reasoning, ``[[AskUser]]'' for the \textit{AskUser} tool, and ``[[Search]]'' for the \textit{Search} tool. These tokens, prepended at each step, guided decision-making during training (see \Cref{apdx:confidence}). For evaluation, we sampled 50 decision steps from both domains' test splits, measuring confidence via token logits. Decisions were categorized as correct or incorrect based on alignment with ground truth. As shown in \Cref{fig:analysis_confidence}, the model exhibited higher confidence in correct decisions, demonstrating \textbf{SMART's effectiveness in boosting confidence and distinguishing between internal knowledge and tool use}.

\vspace{1.5mm}
\noindent\textbf{Confidence Validation Experiment.}
To assess how effectively SMARTAgents choose between internal reasoning and tool invocation, we conducted experiments by injecting special tokens and analyzing the model’s confidence in selecting each one.

We trained the model on the Time and Intention domains, introducing three special tokens: ``[[Reasoning]]'' for internal reasoning, ``[[AskUser]]'' for the \textit{AskUser} tool, and ``[[Search]]'' for the \textit{Search} tool. These tokens were prepended at the start of each step during training to guide the model's decision-making. See \Cref{apdx:confidence} for details.

For evaluation, we sampled 50 decision steps from the test split of both domains and examined the logits of the selected tokens to measure confidence. Decisions were categorized as correct or wrong based on whether the model's choice aligned with the ground truth. As shown in \Cref{fig:analysis_confidence}, the model consistently exhibited higher confidence in correct decisions than incorrect ones. This demonstrates that \textbf{SMART effectively boosts confidence in accurate decision-making, reinforcing its ability to distinguish when to rely on internal knowledge or external tools}.
