\section{Introduction}

Recent advancements in Large Language Models (LLMs) \citep{ouyang2022training,team2023gemini,dubey2024llama} have led to remarkable improvements in reasoning capabilities, driving progress in diverse domains such as coherent text composition~\citep{wei2023chainofthought}, code generation~\citep{gao2023pal, Opendevin2024, SWEGym2024}, complex logical deduction~\citep{yao2023react, yao2024tree}, and nuanced natural language understanding~\citep{wang2023interactive, yu2024natural, wu2024individual}.
However, challenges remain, such as the inability to handle real-time information~\cite{yu2024information}, provide accurate mathematical results~\citep{lu2022survey}, and fully comprehend human intentions~\cite{qian2024tell}. These limitations highlight the need for LLMs to leverage external tools~\citep{schick2023toolformer, qin2023tool, yuan2024craft, qian2024escapebench}, enabling them to function as agents capable of assisting users in diverse tasks~\citep{qin2023toolllm, xi2023rise}. Effective tool use and reasoning are thus complementary, each enhancing the other to overcome current shortcomings.

Therefore, in problem-solving, a language agent often combines reasoning with tool use, following a ReACT-style approach~\citep{yao2023react}, where the model alternates between thought processes and actions to derive solutions. This enables the core agent to apply its parametric knowledge to advance task-solving while using external tools to address its limitations. However, this interplay raises a critical question: \textbf{when should the agent rely on external tools versus its own knowledge?}

\begin{figure}[!t]
    \centering
    \subfigure{\includegraphics[width=\linewidth]{figures/introduction_metacognition.png}}
    \vspace{-5mm}
    \caption{An illustration of human metacognition in problem-solving. The user recalls Tim Cook’s association with Apple based on \textbf{prior knowledge}, as it is a \textit{slow-changing fact}. However, recognizing the newest chip as a \textit{fast-changing fact} beyond knowledge scope, they switch to \textbf{online search} for up-to-date information.}
    \label{fig:intro_metacognition}
    \vspace{-5mm}
\end{figure}

To investigate this, we first conduct a preliminary study on both LLMs and LM-driven agent systems to assess their ability to dynamically and effectively switch between external tool use and parametric knowledge-driven reasoning. Our empirical results reveal a consistent bias, with LLMs unnecessarily invoking tools over 30\% of the time, and agent systems exhibiting similar behavior even when their parametric knowledge alone would suffice. We identify this phenomenon as \textbf{Tool Overuse}, which arises from the model's inability to recognize when its internal knowledge is sufficient. This not only leads to unnecessary resource consumption but can also confuse the model, ultimately degrading performance. This observation highlights the need for better calibration of an agent’s self-awareness, ensuring it can discern when to rely on tools versus its own knowledge. Striking this balance is crucial for enhancing efficiency, scalability, and user experience as LM-driven agents are increasingly deployed in real-world applications.

To address this challenge, we propose \textbf{SMART} (\uline{S}trategic \uline{M}odel-\uline{A}ware \uline{R}easoning with \uline{T}ools), which draws inspiration from human decision-making to calibrate self-awareness in agent models for effective tool use and reasoning. In \textbf{Metacognitive Theory}~\citep{schraw1995metacognitive}, psychology highlights humans’ awareness of their thought processes, including when to apply specific problem-solving strategies~\citep{livingston2003metacognition}. As \Cref{fig:intro_metacognition} illustrates, this implicit heuristic allows dynamic balancing between external strategies and internal knowledge~\citep{Minsky1986-MINTSO}. Similarly, agents need metacognition to optimize tool usage. By aligning the model’s subjective perception with its knowledge boundary, we enable agents to make more informed decisions on when to rely on external tools or internal knowledge.

We adopt a data-driven approach to calibrate model decision-making by constructing \textbf{SMART-ER} (SMART-\uline{E}nhanced \uline{R}easoning), a dataset spanning three domains—Math, Time, and Intention. It addresses key LLM limitations, including computational accuracy~\citep{hendrycks2021measuring}, outdated knowledge~\citep{vu2023freshllms}, and user preference awareness~\citep{qian2024tell}. Specifically, each question in SMART-ER combines sub-questions the model handles well (e.g., simple arithmetic, static facts, commonsense) with those it struggles with (e.g., complex math, dynamic facts, user-specific intentions).
We break down each question into reasoning steps, categorizing them as either parametric knowledge-driven or tool-dependent. For parametric steps, we provide reasoning based on internal knowledge. For tool-dependent steps, we map them to appropriate tools, execute them, and integrate the results into the reasoning process.
Finally, inspired by metacognitive heuristics, we refine each step with explicit justifications, clarifying when parametric knowledge suffices or external tools are needed. By transforming implicit decision-making heuristics into explicit language-based reasoning, we guide the model to develop calibrated awareness of its knowledge boundaries.

Leveraging SMART-ER, we develop \textbf{SMARTAgent}, a family of agent models designed to dynamically balance reasoning between parametric knowledge and external tools. Empirical results show that SMARTAgent reduces tool use by 24\% while improving overall performance by over 37\%, effectively mitigating tool overuse. Notably, it enables 7B-scale models to match the performance of GPT-4 and 70B models, bridging the gap between model size and capability. Additionally, SMARTAgent efficiently handles out-of-distribution (OOD) tasks, requiring only one-fifth the number of tool calls while preserving accuracy. Finally, analysis of SMARTAgent’s confidence through logits reveals more certain reasoning-tool-switching decisions, further validating our approach in calibrating the agent’s self-awareness. In summary:
\begin{itemize}[topsep=2pt, partopsep=-5pt, leftmargin=8pt, itemsep=-4.5pt]
\item We identify and define the issue of \textbf{Tool Overuse}, emphasizing that strategically balancing the complementary strengths of knowledge-driven reasoning and external tool calls can mitigate this problem in both LLMs and agent systems.
\item We introduce \textbf{SMART-ER}, a multi-domain dataset designed to address key limitations of agent models by integrating metacognitive heuristics to better help them recognize and adapt to their knowledge boundaries.
\item We develop \textbf{SMARTAgent}, a family of agents that intelligently balances parametric reasoning and tool use, achieving improved performance, reduced tool overuse, and more confident decision-making in tool utilization.
\end{itemize}
