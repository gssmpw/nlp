\clearpage
\appendix

\section*{Appendix}
\label{sec:appendix}

\section{Preliminary Study Details}

\subsection{Agent Experiment Details}
\label{apdx:prelim_agent}
The system instruction that we provide to both the XAgent and AgentGPT is:
\begin{tcolorbox}[colback=gray!5!white, colframe=gray!75!black, 
title=Prompt for Agent Preliminary Study, boxrule=0.3mm, width=0.49\textwidth, arc=3mm, auto outer arc=true]
Solve the following task accurately, and use tools to help you only if necessary.
\end{tcolorbox}
For LM-driven agent systems, we first prompt GPT-4o with all the questions from the GSM8K test set without using any tools. We then filter out only the questions that GPT-4o can correctly answer through pure text-based reasoning. From this refined dataset, we randomly sample 50 questions to evaluate AgentGPT and XAgent’s performance. Surprisingly, despite the core model being capable of solving all sampled questions without external tools, it still heavily relies on tools during reasoning, leading to tool overuse.
\Cref{fig:preliminary_agent} presents a case study illustrating a specific instance of tool overuse by both agents.

\subsection{Model Experiment Details}
\label{apdx:prelim_model}
For both Llama-3.1-8B-Instruct and Mistral-7B-Instruct-v0.3, we prompt the model to do inference two times for each question from GSM8K's test set. The first time we instruct the model to reason normally to solve the query with the following system instruction:
\begin{tcolorbox}[colback=gray!5!white, colframe=gray!75!black, 
title=Prompt for Model Preliminary Study (Normal), boxrule=0.3mm, width=0.49\textwidth, arc=3mm, auto outer arc=true]
You are an advanced assistant designed to solve tasks autonomously using your knowledge and reasoning. Clearly articulate your thought process and reasoning steps before presenting the final response to ensure transparency and accuracy.
\end{tcolorbox}


The second time, we give the model access to tools and instruct it to independently decide when to use them based on the following system instruction:
\begin{tcolorbox}[colback=gray!5!white, colframe=gray!75!black, 
title=Prompt for Model Preliminary Study (Tool), boxrule=0.3mm, width=0.49\textwidth, arc=3mm, auto outer arc=true]
\#\#\# Task\newline
You are a highly capable assistant designed to solve tasks effectively using your knowledge and available tools.\newline
\newline
\#\#\# Principles\newline
1. Reason Independently:\newline
• Leverage your own knowledge to analyze and solve reasoning steps whenever possible. Use external tools only when necessary.\newline
2. Tool Usage:\newline
• Use code snippet ```python ... ``` to write, execute a python code snippet, and retrieve the result from its printed output.\newline
3. Step-by-Step Approach:\newline
• Work through reasoning systematically, breaking down the task into manageable steps. Rely on your knowledge until a gap is identified that requires tool support. Employ tools to address gaps and integrate the findings into your solution.\newline
4. Goal-Oriented Resolution:\newline
• Conclude your reasoning process by achieving a clear, accurate, and succinct solution based on your independent analysis and insights gained from tools.\newline
\newline
\#\#\# Output Guidelines\newline
• If you need to use the code tool, please wrapped it ```python ... ``` and write the code snippet inside. Make sure you include all the packages necessary and the code is executable. And then you should stop generating.\newline
• If you just begin to generate reasoning steps, please directly reason after "\#\#\# Reasoning Steps".\newline
• If you are generating after the output of a code snippet, please continue to do the reasoning in your output, you can still call the tool if necessary.\newline
• Finally you should give a succinct and accurate final response to directly address the task after "\#\#\# Final Response".\newline
\end{tcolorbox}
We provide a code-writing and execution environment, specifically designed to assist with complex math tasks and calculations. Whenever the model generates a code snippet in its output, we parse and execute it, returning the result. The model then continues reasoning based on its previous steps and the executed output. This process iterates until a final response is reached.


\section{Data Construction Details}

\subsection{Data Selection}
\label{apdx:data_slection}
For the \textbf{Math} domain, we first collect questions that the current GPT model answers incorrectly, ensuring their inherent difficulty. We then decompose the ground truth reasoning chain to assess the complexity of each step, selecting questions that contain both straightforward and challenging aspects to provide a balanced reasoning task.

For the \textbf{Time} domain, we filter out all questions explicitly labeled as involving fast-changing facts. Given the limited number of such questions, we further augment the dataset using a self-instruct approach, prompting the GPT model to generate additional queries related to rapidly evolving information. To introduce compositional reasoning, each generated query is expanded with an additional subquestion involving well-established, slow-changing facts, forming multi-hop queries that require a nuanced understanding of temporal knowledge.

For the \textbf{Intention} domain, we filter out all queries labeled as vague in task definition, particularly those requiring explicit user clarification. To ensure that each query remains solvable without tool reliance, we probe GPT to verify that the model can generally answer each selected question without application of tools. This filtering process refines the dataset to only include queries where the model’s performance is not hindered by a lack of inherent capability but rather by the absence of user-provided intent.

The data adaptation process is fully automated, with manual checks conducted on 5\% of the samples at each stage to ensure the quality of the final filtered questions.

\subsection{Reasoning Chain Construction}
\label{apdx:chain_construction}
Empirically, we incorporate three tools in our constructed tool set:
\begin{itemize}[topsep=2pt, partopsep=-5pt, leftmargin=8pt, itemsep=-4.5pt]
\item \textbf{Code}: An environment for code writing and execution, enhancing the model’s capability in complex calculations, equation solving, and related tasks. To use this tool, the model must generate an executable code snippet within \texttt{'''python <code> '''} and print the output to obtain the execution results.
\item \textbf{Search}: A real-time web search tool for retrieving the most up-to-date factual knowledge or information beyond the model’s parametric knowledge. To invoke this tool, the model should provide a search query in the format \texttt{Search(<query>)} to obtain relevant search engine results. We empirically use the Serper API as the backend search engine.
\item \textbf{AskUser}: A tool for querying the user to clarify intentions, preferences, or general inquiries. This tool enables the model to retrieve user-provided responses by issuing a user-oriented query in the format \texttt{AskUser(<query>)}. To simulate user responses in our experiments, we employ a GPT model as the backend.
\end{itemize}
From the constructed reasoning chains, we empirically observe that the \textbf{Code} tool is mainly used in the \textbf{Math} domain, the \textbf{Search} tool is mainly utilized in the \textbf{Time} and \textbf{Intention} domains, while the \textbf{AskUser} tool is mainly employed in the \textbf{Intention} domain.

For each step involving the auxiliary model \( M \), we manually verify data quality to ensure: i) tasks are decomposed into fine-grained, reasonable subgoals, ii) tool-calling formats are correct, and iii) justifications align with labels and accurately explain why parametric knowledge suffices or a specific tool is required. Through iterative optimization of instructions to \( M \), we achieve a final pass rate exceeding 95\%.

\begin{table}[h!]
\centering
\small
\begin{tabular}{lc}
\toprule
\textbf{Hyperparameter} & \textbf{Value} \\ \midrule
Models & Llama-3.1-8B, Mistral-7B \\ 
Fine-tuning Method & SFT \\
PEFT & LoRA \\
LoRA Rank & 16 \\
LoRA Alpha & 32 \\
LoRA Dropout & 0.05 \\
LoRA Target & All Layers \\
Sequence Length (cutoff\_len) & 4096 tokens \\
Batch Size (Per Device) & 2 \\
Gradient Accumulation Steps & 4 \\
Learning Rate & 1e-4 \\
Learning Rate Scheduler & Cosine \\
Warmup Ratio & 0.1 \\
Number of Epochs & 3 \\
Precision & bfloat16 \\ \bottomrule
\end{tabular}
\caption{Hyperparameters during Fine-Tuning.}
\label{tab:hyperparams}
\end{table}

\subsection{Training}
\label{apdx:training}
For fine-tuning, we used \textbf{Llama-3.1-8B-Instruct}\footnote{\url{https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct}}, \textbf{Llama-3.1-70B-Instruct}, \textbf{Mistral-7B-Instruct-v0.3}\footnote{\url{https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3}}, \textbf{Mistral-Nemo-Instruct-2407}\footnote{\url{https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407}}, and \textbf{Mistral-Small-24B-Instruct-2501}\footnote{\url{https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501}} as base models. We applied supervised fine-tuning (SFT) in the Alpaca instruction-following format (Instruction-Input-Output), computing the loss only on tokens in the Output field.

The system instruction for finetuning is presented in the following:
\begin{tcolorbox}[colback=gray!5!white, colframe=gray!75!black, 
title=System Instruction for Training, boxrule=0.3mm, width=0.49\textwidth, arc=3mm, auto outer arc=true]
You are a highly capable assistant designed to solve tasks effectively using your knowledge and available tools. Follow these principles:\newline
\newline
1. Reason Independently: Leverage your own knowledge to analyze and solve reasoning steps whenever possible. Use external tools only when necessary.\newline
2. Tool Usage:\newline
<Specific Tool Description>\newline
3. Step-by-Step Approach:\newline
• Work through reasoning systematically, breaking down the task into manageable steps.\newline
• Rely on your knowledge until a gap is identified that requires tool support.\newline
• Employ tools to address gaps and integrate the findings into your solution.\newline
4. Goal-Oriented Resolution:\newline
Conclude your reasoning process by achieving a clear, accurate solution based on your independent analysis and insights gained from tools. After your reasoning, provide your response to directly address the task.\newline
\newline
Your reasoning should be transparent, logical, and concise. Stop and document the reasoning whenever you need to use a tool to gather more information. Continue until you reach the final solution and give final response.
\end{tcolorbox}

Training was conducted on 4 NVIDIA A40 GPUs using LoRA (Low-Rank Adaptation) with a rank of 16 and an alpha of 32, applied across all model layers. The maximum sequence length was set to 4096 tokens, and models were trained for 3 epochs with a learning rate of 1e-4, using a cosine learning rate scheduler with a 10\% warmup ratio. To manage memory constraints, we set a per-device batch size of 2 and applied gradient accumulation over 4 steps. Training used \texttt{bfloat16} (bf16) precision, with evaluations every 100 steps, using 1\% of the dataset for validation. Fine-tuning hyperparameters are detailed in Table \ref{tab:hyperparams}.


\section{Experiment Details}
\label{apdx:experiment}
\subsection{Data Setting}
\label{apdx:setting_data}
For in-domain testing, we use a subset of adapted SMART-ER data. Specifically, for the Math domain, we randomly sample 400 test instances from MATH, ensuring coverage of all testing categories (algebra, geometry, number theory, etc.), while spanning five difficulty levels. For the Time domain, we select 100 randomly sampled adapted data points from FreshQA, ensuring that each instance incorporates both fast-changing and slow-changing aspects. For the Intention domain, we randomly sample 100 data points from Intention-in-Interaction, ensuring that all selected instructions are vague and require specific user preferences to resolve.  

For out-of-domain testing, we directly use the full test set of GSM8K without modifications. For MINTQA, due to its large size, we randomly sample 10\% of the data points that meet the following criteria: the question requires multi-hop reasoning and contains both old and new knowledge. This selection ensures a challenging test set that evaluates the model’s ability to generalize beyond in-domain tasks while maintaining a focus on complex reasoning and real-world knowledge retrieval.  
\subsection{Baselines}
\label{apdx:setting_baselines}
For the baseline \textit{Normal Reasoning Trained}, we train a separate model for each domain. Specifically, for Math, Time, and Intention, we use the same queries as in the SMART-ER training set. In the Math domain, we leverage existing solution chains from the MATH dataset as training data. For the IN3 and Time domains, we use GPT-4o to generate normal reasoning chains, guided by existing annotations on final answers or missing details as heuristics. These domain-specific solution chains are then used to train the model.

For the baseline \textit{Base Model Reasoning Prompt}, we use the following system instruction to evaluate the model's performance:
\begin{tcolorbox}[colback=gray!5!white, colframe=gray!75!black, 
title=Base Model Reasoning Prompt, boxrule=0.3mm, width=0.49\textwidth, arc=3mm, auto outer arc=true]
You are an advanced assistant designed to solve tasks autonomously using your knowledge and reasoning. Clearly articulate your thought process and reasoning steps before presenting the final response to ensure transparency and accuracy.

In the field '\#\#\# Reasoning Steps', clearly articulate your thought process and reasoning steps towards the final answer. Then you should present a succinct and accurate final response in the field '\#\#\# Final Response'.
\end{tcolorbox}

For the baseline \textit{Base Model Tool Prompt}, we use the same system prompt as in \cref{apdx:prelim_model}, allowing the model to access tools and freely decide whether and when to use them.

\subsection{Interactive Inference}
\label{apdx:inference}
For both the baseline \textit{Base Model Tool Use} and our \textit{SMARTAgent}, we adopt an interactive approach for inference. Specifically, we first prompt the target model with the query and obtain its output. In this output, we use a rule-based natural language matching method to determine whether a tool call or a final answer is present (e.g., detecting whether ``\#\#\# Final Response'' appears in the output to identify the final response).  

If the final response is found, we extract it and terminate the iterative process. If a tool call is detected, we parse the parameters provided by the model to execute the tool call. Based on the specific tool's name, we invoke the corresponding API and integrate its output into the model's response. Next, we append the model's reasoning before the tool call, the tool call itself, and its output to the model's input. We then re-prompt the model to continue reasoning, given the previously executed tool call and its result.  

This iterative process continues until the final response is successfully parsed and retrieved, forming the complete interactive inference process.  

Below, we illustrate the respective input and output in an iterative inference process consisting of two iterations:
\begin{tcolorbox}[colback=gray!5!white, colframe=gray!75!black, 
title=Interactive Inference, boxrule=0.3mm, width=0.49\textwidth, arc=3mm, auto outer arc=true]
--------- Iterate 2 Input Begin ---------\newline
\newline
--- Iterate 1 Input Begin ---\newline
\#\#\# Task\newline
<target task>\newline
\#\#\# Reasoning Steps\newline
--- Iterate 1 Input End ---\newline
\newline
== Iterate 1 Output Begin ==\newline
- Step 1: <title>, general reasoning\newline
<reasoning>\newline
- Step 2: <title>, tool: <tool name>\newline
<tool call parameter>\newline
== Iterate 1 Output End ==\newline
- Output: <tool execution output>\newline
\newline
--------- Iterate 2 Input End ---------\newline
\newline
====== Iterate 2 Output Begin ======\newline
- Step 3: <title>, general reasoning\newline
<reasoning>\newline
- Step 4: ...\newline
...\newline
...\newline
\#\#\# Final Response\newline
<final answer>\newline
====== Iterate 2 Output End ======\newline
\end{tcolorbox}


\begin{table*}[t]
    \centering
    
    \setlength\tabcolsep{2pt}
    \setlength\extrarowheight{2pt}
    
    \resizebox{1.0\linewidth}{!}{
    \begin{tabular}{l @{\hskip 14pt} l c c @{\hskip 14pt} c c @{\hskip 14pt} c c c}
    
        \toprule
        
        \multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{Model}} & \multicolumn{2}{c}{\textbf{Math} (MATH)} & \multicolumn{2}{c}{\textbf{Time} (FreshQA)} & \multicolumn{3}{c}{\textbf{Intention} (Intention-in-Interaction)} \\

        \cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-9}
        
        ~ & ~ & \textbf{\makecell{Tool Used$^\downarrow$\\(\textit{Times})}} & \hskip 8pt \textbf{\makecell{Accuracy$^\uparrow$\\(\%)}} & \textbf{\makecell{Tool Used$^\downarrow$\\(\textit{Times})}} & \hskip 8pt \textbf{\makecell{Accuracy$^\uparrow$\\(\%)}} & \textbf{\makecell{Tool Used$^\downarrow$\\(\textit{Times})}}  & \hskip 8pt \textbf{\makecell{Missing Details Recovery$^\uparrow$\\(Lv3 / Lv2, \%)}} & \textbf{\makecell{Summarized Intention \\Coverage$^\uparrow$ (\%)}}  \\

        \addlinespace[2pt]
        \midrule
        \addlinespace[2pt]
        \multicolumn{9}{c}{\textit{Open-Source}} \\ 
        \midrule

        % \multirow{1}{*}{\makecell[l]{Normal\\Reasoning Trained}} & \textit{Llama-3.1-8B} & 0.00 & 41.00 & 0.00 & 48.00 & 0.00 & 38.37 / 42.49 & - \\

        % \addlinespace[2pt]
        % \midrule
        % \addlinespace[2pt]

        % \multirow{1}{*}{\makecell[l]{Base Model\\Reasoning Prompt}} & \textit{Llama-3.1-8B} & 0.00 & 53.00 & 0.00 & 26.00 & 0.00 & 40.70 / 25.76 & - \\

        % \addlinespace[2pt]
        % \midrule
        % \addlinespace[2pt]
        
        % \multirow{5}{*}{\makecell[l]{Base Model\\Tool Prompt}} & \textit{Llama-3.1-8B} & 1.93 & 51.00 & 2.05 & 56.00 & 3.77 & 54.76 / 25.90 & 70.20 \\ 
        

        % \addlinespace[2pt]
        % \midrule
        % \addlinespace[2pt]

        \multirow{2}{*}{\textbf{SMARTAgent}} & \textit{Llama-3.1-70B} & 0.94 & 72.50 & 1.01 & \textbf{66.00} & 3.51 & \textbf{68.60} / 58.15 & \textbf{86.09} \\
        ~ & \textit{Llama-3.3-70B} & 0.61 & \textbf{76.25} & 1.00 & 65.00 & 3.15 & 61.63 / \textbf{59.01} & 84.45 \\
        
        \bottomrule
    \end{tabular}
    }
    \label{tab:result_apdx}
    \caption{
    Performance of SMARTAgent when using Llama-3.3-70B-Instruct as the base model, compared to the original results with its Llama-3.1-70B-Instruct counterpart.
    }
\end{table*}


\subsection{Additional Results}
We also provide results from the latest \textbf{Llama-3.3-70B-Instruct}\footnote{\url{https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct}} model in \Cref{tab:result_apdx}, comparing its performance with the \textbf{Llama-3.1-70B-Instruct}-based SMARTAgent. Although \textbf{Llama-3.3} is the newest version, we use the \textbf{3.1} series to maintain consistency with the \textbf{8B} model, which is also from the \textbf{3.1} version. Empirically, we found no significant difference in performance between the \textbf{3.3} and \textbf{3.1} versions of the \textbf{70B} model.


\subsection{Confidence Validation}
\label{apdx:confidence}
We independently train the Llama-3.1-8B-Instruct and Mistral-7B-Instruct models with the added special tokens. At each reasoning step, we prepend a special token at the very beginning to indicate the model’s chosen approach—whether it relies on external tools (e.g., ``[[AskUser]]'' or ``[[Search]]'') or its own parametric knowledge (e.g., ``[[Reasoning]]'').

By analyzing the probability of generating each special token, we can assess the model's confidence in its decision-making process. Apart from the added special tokens, the rest of the original reasoning chain remains unchanged, maintaining the following structured format:
\begin{tcolorbox}[colback=gray!5!white, colframe=gray!75!black, 
title=Step Format, boxrule=0.3mm, width=0.49\textwidth, arc=3mm, auto outer arc=true]
- Step <index>: [[Special Token]] <title>\newline
<content>\newline
- Step <index>: ...
\end{tcolorbox}

We train the model using the exact same hyper-parameter setting introduced in \Cref{apdx:training}. During inference, we randomly sample 50 decision-making steps from the test split of both the \textit{Time} and \textit{Intention} domains. A decision-making step refers to the final action in a reasoning sequence—given the previous \( n-1 \) steps, we evaluate whether the model correctly decides between using a tool or relying on its parametric knowledge for the \( n \)th step. This evaluation is performed within the context of the full solution chain, which consists of \( m \) steps in total (\( m \geq n \)).
