\section{Discussions and Future Work}
\noindent\textbf{Agent's improper tool usage.}
Our empirical analysis reveals a notable phenomenon of \textit{tool overuse}, where agents frequently rely on external tools even when internal knowledge is sufficient. This over-reliance likely arises from two factors: i) the agentâ€™s uncertainty about its own capabilities, and ii) the perceived ease of external lookups compared to internal reasoning. We also observe instances of \textit{tool underuse}, especially in large-scale models like GPT-4o and Llama-70B, where agents neglect to call essential tools, possibly due to misjudging the complexity of the task. Both overuse and underuse contribute to concerns over computational efficiency and solution accuracy. Future research could explore methods to better balance these trade-offs, such as by introducing explicit resource constraints or budgets for tool calls.

\vspace{1.5mm}
\noindent\textbf{Mechanisms behind human and LM's decision-making.}
Cognitive science suggests that human decision-making arises from both intuitive judgments and reflective strategies. Similarly, in LMs, problem-solving is influenced by implicit heuristics (e.g., memorized patterns) and explicit tool-using behaviors. When tools are available, LMs often default to external queries, akin to humans seeking external confirmation when uncertain. However, LMs lack self-monitoring and rely on external or data-driven cues to determine when to trust their internal knowledge. Developing frameworks that integrate implicit heuristics with explicit reasoning could lead to more adaptive and efficient LM decision-making.

\vspace{1.5mm}
\noindent\textbf{Enhancement of model's self-awareness.}
Our data-driven calibration strategy, which provides explicit rationales for when to rely on internal knowledge versus external tools, shows promising results. Other approaches, such as confidence probing via logits, integration of specialized self-checking modules, or reinforcement learning from feedback, might also refine tool usage thresholds. Future research could investigate how these signals affect the model's internal distributions and identify representations that capture the \textit{awareness} of boundaries. Additionally, iterative or in-context learning could allow real-time metacognitive calibration, offering a more efficient safeguard against both overuse and underuse of resources.

