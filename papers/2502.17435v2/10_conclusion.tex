
\section{Conclusion}
\label{sec:conclusion}
\vspace{-1mm}
% In this work, we have presented a novel approach to color constancy that leverages image-conditional diffusion models to inpaint color checkers directly into images. This method not only enhances the accuracy of illumination estimation but also addresses significant limitations of traditional techniques, particularly their struggles with generalization across different camera sensors. By employing Laplacian decomposition, we effectively preserve high-frequency structural details, ensuring that the inpainted color checkers harmonize with the original image context.


% \vspace{3pt}  \noindent {\bf Limitations.}
% Our method struggles when there is a significant mismatch between the inpainted color checker and the scene's ambient lighting. This typically occurs in challenging scenarios with multiple strong light sources of different colors or complex spatially-varying illumination. While diffusion models provide strong image priors, they sometimes prioritize visual plausibility over physical accuracy, especially in extreme lighting conditions.

In this work, we introduce a color constancy method that leverages image-conditional diffusion models to inpaint color checkers directly into images. Our approach harnesses the rich priors of foundation models to overcome generalization challenges across varying camera sensors. By employing Laplacian decomposition, our method maintains the checker's high-frequency structure while adapting to scene illumination, enabling accurate light color estimation without camera-specific training. Experiments demonstrate robust performance in cross-camera scenarios, particularly for challenging cases, making our approach a versatile solution for real-world color constancy applications.


% \todo{Visualization and discussion of failure cases. E.g., extreme lighting conditions.}

