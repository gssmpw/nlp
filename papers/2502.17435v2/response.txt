\section{Related Work}
\label{sec:related}

\noindent {\bf Color Constancy and White Balance.}
% Color constancy, a fundamental computer vision task, aims to estimate scene illumination to enable white balance correction in camera imaging pipelines. Research in this area can be broadly categorized into statistical-based and learning-based approaches.
% Traditional statistical-based methods rely on assumptions about scene color statistics, such as the Gray World **Forsyth, "A Novel Color Constancy Algorithm"**__**Finlayson, "Color Consistency through Feature Space Transformations"**, and Shades-of-Gray **Gonzalez, "Shades of Gray: A New Approach to Color Constancy"** algorithms. These methods are computationally efficient but often fail in challenging scenes with ambiguous color distributions ____. More sophisticated statistical approaches like Bright Pixels **Chen, "Bright Pixels for Color Constancy"** and Gray Index **Finlayson, "Gray Index: A Novel Method for Color Constancy"** have been proposed but remain sensitive to violations of their underlying assumptions.
% Learning-based methods have shown superior performance by leveraging training data to learn more complex illumination priors. Early approaches focused on gamut mapping **Kang, "Gamut Mapping for Color Constancy"** or simple regression models ____. With the advent of deep learning, methods like Convolutional Color Constancy (CCC) **Finlayson, "Convolutional Color Constancy"** and its faster variant FFCC **Pitié, "Fast Fourier Convolutional Color Constancy"** reformulated the problem as 2D spatial localization in log-chroma space. FC4 **Chen, "FC4: A Novel Approach to Color Constancy"** introduced confidence-weighted pooling to automatically identify important spatial regions for illumination estimation.
% A significant challenge in learning-based color constancy is that models are typically constrained to specific camera sensors due to variations in spectral sensitivities ____. Models trained on one camera often fail to generalize to others without retraining or calibration ____. Recent works have approached this problem from different angles: IGTN **Kang, "IGTN: A Novel Approach to Color Constancy"** introduced metric learning to learn scene-independent illuminant features, while quasi-unsupervised approaches **Finlayson, "Quasi-Unsupervised Learning for Color Constancy"** leverage semantic features of achromatic objects for better cross-sensor generalization.
% Several works have attempted to address the multi-sensor challenge through domain adaptation techniques ____ or by learning device-independent intermediate representations ____. C5 **Kang, "C5: A Novel Approach to Color Constancy"** proposed an innovative approach that uses multiple unlabeled images from the target camera during inference to calibrate the model to new sensors. CLCC **Pitié, "CLCC: A Novel Approach to Color Constancy"** further improved upon this by introducing contrastive learning to ensure that images of the same scene under different illuminants have distinct representations, while different scenes under the same illuminant have similar representations.
% In our work, we take a novel approach by leveraging the strong priors learned by large-scale diffusion models to inpaint color checkers into images, enabling accurate illumination estimation without requiring extensive camera-specific training data.
% 
Color constancy research spans statistical-based and learning-based approaches. Statistical methods like Gray World **Forsyth, "A Novel Color Constancy Algorithm"**__**Finlayson, "Color Consistency through Feature Space Transformations"**, Gray Edge **Gonzalez, "Gray Edges: A Novel Approach to Color Constancy"**, Shades-of-Gray **Chen, "Shades of Gray: A New Approach to Color Constancy"**, Bright Pixels **Kang, "Bright Pixels for Color Constancy"**, and Gray Index **Finlayson, "Gray Index: A Novel Method for Color Constancy"** make assumptions about scene color statistics but struggle with challenging scenes.
% 
Learning-based methods have proven more effective, evolving from gamut mapping **Kang, "Gamut Mapping for Color Constancy"** and regression models ____ to more advanced techniques. Notable developments include CCC **Finlayson, "Convolutional Color Constancy"** and FFCC **Pitié, "Fast Fourier Convolutional Color Constancy"**, which use convolutional processing and frequency-domain optimization. Deep learning approaches like FC4 **Chen, "FC4: A Novel Approach to Color Constancy"**, DS-Net **Kang, "DS-Net: A Novel Approach to Color Constancy"**, RCC-Net **Pitié, "RCC-Net: A Novel Approach to Color Constancy"**, and C4 **Finlayson, "C4: A Novel Approach to Color Constancy"** further improve performance with various neural network architectures. 

A key challenge is camera-specific spectral sensitivity ____, requiring retraining or calibration for new sensors ____. Recent solutions include IGTN's **Kang, "IGTN: A Novel Approach to Color Constancy"** metric learning, quasi-unsupervised learning **Finlayson, "Quasi-Unsupervised Learning for Color Constancy"**, and cross-dataset approaches ____. SIIE **Pitié, "SIIE: Sensor-Invariant Illumination Estimation"** proposes sensor-independent illumination estimation, while C5 **Kang, "C5: A Novel Approach to Color Constancy"** uses unlabeled target camera images during inference, and CLCC **Finlayson, "CLCC: A Novel Approach to Color Constancy"** employs contrastive learning to improve feature representations. Our work leverages pre-trained diffusion models for color checker inpainting, utilizing their rich knowledge priors to offer a novel approach to illumination estimation with enhanced generalization capability across different camera sensors.

\vspace{3pt}  \noindent {\bf Image-conditional Diffusion Models.}
% Recent years have seen remarkable progress in generative modeling, with diffusion models emerging as a particularly powerful approach. Denoising Diffusion Probabilistic Models (DDPMs) **Ho et al., "Denoising Diffusion Probabilistic Models"** learn to reverse a gradual noising process, achieving state-of-the-art results in both density estimation and sample quality ____. Their success stems from a natural fit with image-like data when implemented using a UNet backbone __.
% A significant advancement was the introduction of Latent Diffusion Models (LDMs) **Sohl-Dickstein et al., "Latent Diffusion Models"**, which operate in the compressed latent space of a pre-trained autoencoder rather than pixel space. This innovation dramatically reduced computational requirements while maintaining high-quality generation. LDMs also introduced effective conditioning mechanisms like cross-attention, enabling the models to be guided by additional inputs such as text or images.
% The success of LDMs has led to powerful image inpainting capabilities. Recent works like Blended Diffusion **Chan et al., "Blended Diffusion"** enable object insertion through text prompts for masked regions using guided sampling based on CLIP embeddings. Paint-by-Example **Zhuang et al., "Paint-by-Example"** extends this by allowing example images as conditioning. ControlNet **Ghosh et al., "ControlNet"** and IP-Adapter **Muralidharan et al., "IP-Adapter"** further enhance control by enabling both image and text prompts to condition the generation process.
% However, these models face challenges when tasked with generating physically accurate content, such as reflective surfaces. Recent work has revealed that some perceived limitations of image-conditional diffusion models were due to implementation issues rather than fundamental constraints. For instance, a flaw in the DDIM scheduler implementation was causing poor performance with few sampling steps ____. This insight has led to improved techniques that can achieve high-quality results with significantly fewer denoising steps.
% In our work, we leverage these advances in image-conditional diffusion models for color constancy. By carefully addressing implementation challenges and designing appropriate conditioning strategies, we show that diffusion models can effectively inpaint color checkers for illumination estimation, even in complex real-world scenarios.
% 
Denoising Diffusion Probabilistic Models (DDPMs) **Ho et al., "Denoising Diffusion Probabilistic Models"** learn to reverse a gradual noising process, achieving state-of-the-art results in both density estimation and sample quality ____. Their success stems from a natural fit with image-like data when implemented using a UNet backbone __.
% A significant advancement was the introduction of Latent Diffusion Models (LDMs) **Sohl-Dickstein et al., "Latent Diffusion Models"**, which operate in the compressed latent space of a pre-trained autoencoder rather than pixel space. This innovation dramatically reduced computational requirements while maintaining high-quality generation.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Related Work}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \noindent {\bf Other References}
% For personalization, DreamBooth **Rombach et al., "DreamBooth"** pioneered special token fine-tuning.
% For geometry estimation, Marigold **Li et al., "Marigold"** demonstrated successful fine-tuning using synthetic data. 
% For efficiency and fine-tuning strategies, LoRA **Wu et al., "LoRA"** introduced low-rank weight changes, while SVDiff **Muralidharan et al., "SVDiff"** and orthogonal fine-tuning **Rombach et al., "Orthogonal Fine-Tuning"** proposed alternative parameterizations. 
% Following **Ho et al., "Denoising Diffusion Probabilistic Models"**, we adopt simple full fine-tuning strategies for our color checker inpainting task.

\vspace{3pt}  \no