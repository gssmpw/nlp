\begin{abstract}
\vspace{-5mm}

% Color constancy is a critical task in computer vision that focuses on accurately estimating scene illumination to ensure consistent color representation under varying lighting conditions. In this paper, we introduce a novel approach that utilizes image-conditional diffusion models to inpaint color checkers directly into images, facilitating precise illumination estimation. Our method employs Laplacian decomposition to preserve high-frequency structural details while effectively reducing the influence of low-frequency color information from the inserted color checker. This innovative technique addresses key limitations of traditional color constancy methods, such as their challenges in generalizing across different camera sensors. By maintaining structural integrity and enabling illumination-aware color adaptation, our approach significantly enhances the model's ability to generate harmonized color checkers that accurately reflect the scene's illumination. We validate the effectiveness of our method through comprehensive experiments on benchmark datasets, demonstrating substantial improvements in color constancy performance. Our findings underscore the potential of diffusion models to advance color constancy tasks and provide a robust framework for future research in this domain.
Color constancy methods often struggle to generalize across different camera sensors due to varying spectral sensitivities. We present GCC, which leverages diffusion models to inpaint color checkers into images for illumination estimation. Our key innovations include (1) a single-step deterministic inference approach that inpaints color checkers reflecting scene illumination, (2) a Laplacian decomposition technique that preserves checker structure while allowing illumination-dependent color adaptation, and (3) a mask-based data augmentation strategy for handling imprecise color checker annotations. By harnessing rich priors from pre-trained diffusion models, GCC demonstrates strong robustness in challenging cross-camera scenarios. These results highlight our method's effective generalization capability across different camera characteristics without requiring sensor-specific training, making it a versatile and practical solution for real-world applications.


\end{abstract}
\vspace{-5mm}