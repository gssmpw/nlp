\section{Related Work}
\label{sec:related}

\noindent {\bf Color Constancy and White Balance.}
% Color constancy, a fundamental computer vision task, aims to estimate scene illumination to enable white balance correction in camera imaging pipelines. Research in this area can be broadly categorized into statistical-based and learning-based approaches.
% Traditional statistical-based methods rely on assumptions about scene color statistics, such as the Gray World \cite{buchsbaum1980spatial}, Gray Edge \cite{WeijerGG07}, and Shades-of-Gray \cite{finlayson2004shades} algorithms. These methods are computationally efficient but often fail in challenging scenes with ambiguous color distributions \cite{qian2019finding}. More sophisticated statistical approaches like Bright Pixels \cite{joze2012role} and Gray Index \cite{qian2019finding} have been proposed but remain sensitive to violations of their underlying assumptions.
% Learning-based methods have shown superior performance by leveraging training data to learn more complex illumination priors. Early approaches focused on gamut mapping \cite{barnard2000improvements,chakrabarti2011color} or simple regression models \cite{funt2004estimating}. With the advent of deep learning, methods like Convolutional Color Constancy (CCC) \cite{barron2015convolutional} and its faster variant FFCC \cite{barron2017fast} reformulated the problem as 2D spatial localization in log-chroma space. FC4 \cite{hu2017fc4} introduced confidence-weighted pooling to automatically identify important spatial regions for illumination estimation.
% A significant challenge in learning-based color constancy is that models are typically constrained to specific camera sensors due to variations in spectral sensitivities \cite{afifi2019sensor,gao2017improving}. Models trained on one camera often fail to generalize to others without retraining or calibration \cite{liba2019handheld}. Recent works have approached this problem from different angles: IGTN \cite{igtn} introduced metric learning to learn scene-independent illuminant features, while quasi-unsupervised approaches \cite{bianco2019quasi} leverage semantic features of achromatic objects for better cross-sensor generalization.
% Several works have attempted to address the multi-sensor challenge through domain adaptation techniques \cite{daume2006domain,saenko2010adapting} or by learning device-independent intermediate representations \cite{afifi2019sensor}. C5 \cite{afifi2021cross} proposed an innovative approach that uses multiple unlabeled images from the target camera during inference to calibrate the model to new sensors. CLCC \cite{lo2021clcc} further improved upon this by introducing contrastive learning to ensure that images of the same scene under different illuminants have distinct representations, while different scenes under the same illuminant have similar representations.
% In our work, we take a novel approach by leveraging the strong priors learned by large-scale diffusion models to inpaint color checkers into images, enabling accurate illumination estimation without requiring extensive camera-specific training data.
% 
Color constancy research spans statistical-based and learning-based approaches. Statistical methods like Gray World \cite{buchsbaum1980spatial}, Gray Edge \cite{WeijerGG07}, Shades-of-Gray \cite{finlayson2004shades}, Bright Pixels \cite{joze2012role}, and Gray Index \cite{qian2019finding} make assumptions about scene color statistics but struggle with challenging scenes.
% 
Learning-based methods have proven more effective, evolving from gamut mapping \cite{barnard2000improvements,chakrabarti2011color} and regression models \cite{funt2004estimating} to more advanced techniques. Notable developments include CCC \cite{barron2015convolutional} and FFCC \cite{barron2017fast}, which use convolutional processing and frequency-domain optimization. Deep learning approaches like FC4 \cite{hu2017fc4}, DS-Net \cite{shi2016deep}, RCC-Net \cite{8237844}, and C4 \cite{yu2020cascading} further improve performance with various neural network architectures. 

A key challenge is camera-specific spectral sensitivity \cite{afifi2019sensor,gao2017improving}, requiring retraining or calibration for new sensors \cite{liba2019handheld}. Recent solutions include IGTN's \cite{igtn} metric learning, quasi-unsupervised learning \cite{bianco2019quasi}, and cross-dataset approaches \cite{Koskinen2020CrossdatasetCC}. SIIE \cite{afifi2019sensor} proposes sensor-independent illumination estimation, while C5 \cite{afifi2021cross} uses unlabeled target camera images during inference, and CLCC \cite{lo2021clcc} employs contrastive learning to improve feature representations. Our work leverages pre-trained diffusion models for color checker inpainting, utilizing their rich knowledge priors to offer a novel approach to illumination estimation with enhanced generalization capability across different camera sensors.

\vspace{3pt}  \noindent {\bf Image-conditional Diffusion Models.}
% Recent years have seen remarkable progress in generative modeling, with diffusion models emerging as a particularly powerful approach. Denoising Diffusion Probabilistic Models (DDPMs) \cite{sohl2015deep} learn to reverse a gradual noising process, achieving state-of-the-art results in both density estimation and sample quality \cite{kingma2021variational, dhariwal2021diffusion}. Their success stems from a natural fit with image-like data when implemented using a UNet backbone \cite{ronneberger2015u}.
% A significant advancement was the introduction of Latent Diffusion Models (LDMs) \cite{rombach2021highresolution}, which operate in the compressed latent space of a pre-trained autoencoder rather than pixel space. This innovation dramatically reduced computational requirements while maintaining high-quality generation. LDMs also introduced effective conditioning mechanisms like cross-attention, enabling the models to be guided by additional inputs such as text or images.
% The success of LDMs has led to powerful image inpainting capabilities. Recent works like Blended Diffusion \cite{avrahami2022blendeddiffusion, avrahami2023blendedlatent} enable object insertion through text prompts for masked regions using guided sampling based on CLIP embeddings. Paint-by-Example \cite{yang2023paint} extends this by allowing example images as conditioning. ControlNet \cite{zhang2023adding} and IP-Adapter \cite{ye2023ip-adapter} further enhance control by enabling both image and text prompts to condition the generation process.
% However, these models face challenges when tasked with generating physically accurate content, such as reflective surfaces. Recent work has revealed that some perceived limitations of image-conditional diffusion models were due to implementation issues rather than fundamental constraints. For instance, a flaw in the DDIM scheduler implementation was causing poor performance with few sampling steps \cite{lin2024common}. This insight has led to improved techniques that can achieve high-quality results with significantly fewer denoising steps.
% In our work, we leverage these advances in image-conditional diffusion models for color constancy. By carefully addressing implementation challenges and designing appropriate conditioning strategies, we show that diffusion models can effectively inpaint color checkers for illumination estimation, even in complex real-world scenarios.
% 
Denoising Diffusion Probabilistic Models (DDPMs) \cite{sohl2015deep} achieve state-of-the-art generation by reversing a noising process with UNet architectures \cite{ronneberger2015u}, demonstrating excellence in density estimation and sample quality \cite{kingma2021variational, dhariwal2021diffusion}.
Latent Diffusion Models (LDMs) \cite{rombach2021highresolution} improved efficiency by operating in compressed latent space and introduced cross-attention conditioning. This enabled powerful inpainting capabilities, demonstrated by Blended Diffusion \cite{avrahami2022blendeddiffusion, avrahami2023blendedlatent}, Paint-by-Example \cite{yang2023paint}, ControlNet \cite{zhang2023adding}, and IP-Adapter \cite{ye2023ip-adapter}.
Recent work identified that perceived limitations were often due to DDIM scheduler implementation issues \cite{lin2024common} rather than fundamental constraints. Our work leverages these insights to effectively adapt diffusion models for color checker inpainting in illumination estimation.

\vspace{3pt}  \noindent {\bf Learning-based Lighting Estimation.}
Lighting estimation methods traditionally use physical probes like mirror balls \cite{Debevec1998}, 3D objects \cite{weber2018learning, lombardi2015reflectance}, eyes \cite{nishino2004eyes}, or faces \cite{calian2018faces, yi2018faces}. Early probe-free approaches used limited models like directional lights \cite{karsch2011rendering}, sky models \cite{hosek2012analytic, hold2017outdoor}, or spherical harmonics \cite{garon2019fastspatialvary}.
Modern methods focus on HDR environment maps, pioneered by Gardner et al. \cite{garder2017lavelindoor}. DeepLight \cite{legendre2019deeplight} and EverLight \cite{dastjerdi2023everlight} handle both indoor and outdoor scenes, while StyleLight \cite{wang2022stylelight} uses GANs for joint LDR-HDR prediction. Some works explore panorama outpainting \cite{akimoto2019360outpainting2stategan, dastjerdi2022immersegan} but struggle with HDR \cite{dastjerdi2023everlight}.
Recently, DiffusionLight \cite{phongthawee2024diffusionlight} introduced virtual chrome ball synthesis using diffusion models. Our work follows a similar direction but focuses on color checker inpainting for illumination estimation.

\vspace{3pt}  \noindent {\bf Fine-tuning Strategies for Diffusion Models.}
% Recent advances in diffusion model adaptation have shown several effective strategies for fine-tuning pre-trained models for specific tasks. These approaches generally aim to preserve the rich priors learned during pre-training while adapting the model for new objectives.
% A notable development in this space is DreamBooth \cite{ruiz2022dreambooth}, which introduces a method for personalizing text-to-image diffusion models using just a few reference images. The approach uses a special token during fine-tuning to represent the target object while maintaining the model's pre-trained distribution. Building on this work, Gal et al. \cite{gal2022image} propose learning a word embedding to represent reference objects, while Voynov et al. \cite{voynov2023P+} extend this by learning separate word embeddings for each network layer.
% Recent work has revealed interesting insights about fine-tuning strategies for geometry estimation tasks. Marigold \cite{ke2023marigold} demonstrated success in fine-tuning Stable Diffusion for depth estimation using high-quality synthetic data. A significant breakthrough came from Garcia et al. \cite{garcia2024fine}, who showed that many perceived limitations of image-conditional diffusion models were due to implementation issues in the DDIM inference pipeline. Their work demonstrates that simple end-to-end fine-tuning with task-specific losses can outperform more complex training approaches, challenging previous assumptions about the necessity of sophisticated fine-tuning strategies.
% Several techniques have been proposed to make fine-tuning more efficient and effective. Low-rank adaptation (LoRA) \cite{hu2021lora} has emerged as a popular approach that enforces low-rank weight changes, making fine-tuning more computationally tractable while preserving model capabilities. Other methods like SVDiff \cite{han2023svdiff} and orthogonal fine-tuning \cite{oft2023} provide alternative parameterizations for efficient model adaptation.
% In our work, we leverage these advances to fine-tune diffusion models for color constancy, showing that careful adaptation of pre-trained models can yield effective results even for specialized tasks like color checker inpainting. Like Garcia et al. \cite{garcia2024fine}, we find that relatively simple fine-tuning strategies can be highly effective when properly implemented.
% 
For personalization, DreamBooth \cite{ruiz2022dreambooth} pioneered special token fine-tuning, while \citeauthor{gal2022image} \cite{gal2022image} and \citeauthor{voynov2023P+} \cite{voynov2023P+} proposed learned word embeddings approaches. Similar to DreamBooth, our method fine-tunes pre-trained diffusion models to bind specific visual characteristics to our target domain, enabling a consistent generation of color checkers that reflect scene illumination.
% 
For efficiency and fine-tuning strategies, LoRA \cite{hu2021lora} introduced low-rank weight changes, while SVDiff \cite{han2023svdiff} and orthogonal fine-tuning \cite{oft2023} proposed alternative parameterizations. For geometry estimation, Marigold \cite{ke2023marigold} demonstrated successful fine-tuning using synthetic data. Inspired by \citeauthor{garcia2024fine} \cite{garcia2024fine}, who showed that simple fine-tuning approaches can be highly effective for deterministic tasks involving low-frequency image components, we adopt their full fine-tuning strategy for our color checker inpainting task. This approach aligns well with our color constancy problem, which primarily focuses on modifying the low-frequency characteristics of color checkers.
% For personalization, DreamBooth \cite{ruiz2022dreambooth} pioneered special token fine-tuning, while \citeauthor{gal2022image} \cite{gal2022image} and \citeauthor{voynov2023P+} \cite{voynov2023P+} proposed learned word embeddings approaches.
% For geometry estimation, Marigold \cite{ke2023marigold} demonstrated successful fine-tuning using synthetic data. \citeauthor{garcia2024fine} \cite{garcia2024fine} revealed that simple end-to-end fine-tuning can outperform complex approaches once DDIM implementation issues are fixed.
% For efficiency, LoRA \cite{hu2021lora} introduced low-rank weight changes, while SVDiff \cite{han2023svdiff} and orthogonal fine-tuning \cite{oft2023} proposed alternative parameterizations. Following \citeauthor{garcia2024fine} \cite{garcia2024fine}, we adopt simple full fine-tuning strategies for our color checker inpainting task.