\label{sec:appendix_section}

\section*{Overview}
This supplementary material presents additional details and results to complement the main manuscript. In Section \ref{sec:Implementation}, we provide comprehensive implementation details, including dataset preprocessing protocols and training configurations. Section \ref{sec:Laplacian} presents an empirical analysis of the impact of different pyramid levels in our Laplacian decomposition technique and provides implementation details of the algorithm. Section \ref{sec:Qualitative} showcases qualitative results demonstrating our method's effectiveness across various datasets and real-world scenarios. We will release our complete training and inference code along with pre-trained weights to facilitate future research in this area.

\section{Implementation Details}~\label{sec:Implementation}
\subsection{Datasets and Preprocessing}
We use two publicly available color constancy benchmark datasets in our experiments: the NUS-8
dataset~\cite{cheng2014illuminant} and the Gehler dataset~\cite{4587765}. The Gehler dataset~\cite{4587765} contains 568 original images captured by two different cameras, while the NUS-8 dataset~\cite{cheng2014illuminant} contains 1736 original images captured by eight different cameras. Each image in both datasets includes a Macbeth Color Checker (MCC) chart, which serves as a reference for the ground-truth illuminant color.



% We use two publicly available color constancy benchmark datasets in our experiments: the NUS-8 Camera dataset \cite{Cheng:14} and the re-processed Color Checker dataset \cite{4587765} (termed as the Gehler dataset in this paper).
% The Gehler dataset contains 568 original images, while the NUS-8 Camera dataset contains 1736 original images captured by eight different cameras. Each image in both datasets includes a Macbeth Color Checker (MCC) chart, which serves as a reference for the ground-truth illuminant color.

Following the evaluation protocol in \cite{afifi2019sensor}, several standard metrics are reported in terms of angular error in degrees: mean, median, tri-mean of all the errors, the mean of the lowest 25\% of errors, and the mean of the highest 25\% of errors.

\subsection{Training Details}

For all experiments, we process the raw image data before applying gamma correction for sRGB space conversion following the preprocessing protocol from \cite{hu2017fc4}. Since the pre-trained VAE was trained on sRGB images, we apply a gamma correction of $\gamma = 1/2.2$ on linear RGB images before encoding to minimize the domain gap. Conversely, after VAE decoding, we apply inverse gamma correction to convert the output back to the linear domain for metric evaluation.

All experiments are trained for 20000 iterations on an NVIDIA A6000 GPU using the Adam optimizer with an initial learning rate of $5 \times 10^{-5}$ and apply exponential learning rate decay after a 150-step warm-up period. For data augmentation, we follow FC4~\cite{hu2017fc4} to rescale images by random RGB values in [0.6, 1.4], noting that we only rescale the input images since our training does not require ground truth illumination. The rescaling is performed in the raw domain, followed by gamma correction. This is implemented through a 3×3 color transformation matrix, where diagonal elements control the intensity of individual RGB channels (color strength), and off-diagonal elements determine the degree of color mixing between channels (color offdiag). For Laplacian decomposition, we use a two-level pyramid ($L = 2$) to balance the preservation of high-frequency structural details and the suppression of low-frequency color information. Additionally, we apply local transformations to masked regions only, including brightness adjustment ($[0.8, 2.0]$), saturation adjustment ($[0.8, 1.4]$), and contrast adjustment ($[0.8, 1.4]$).

\paragraph{Three-fold Cross-validation}
For cross-validation experiments on both the NUS-8 dataset~\cite{cheng2014illuminant} and the Gehler dataset~\cite{4587765}, we use a batch size of 8. During training, we apply random crop with a probability of $p_{crop} = 0.7$, where the crop size ranges from 70\% to 100\% of the original dimensions. Color augmentation is applied with a probability of $p_{color} = 0.3$.

\paragraph{Leave-one-out Evaluation}
For the leave-one-out experiments on the NUS-8 dataset~\cite{cheng2014illuminant}, we use a batch size of 8 with gradient accumulation over 2 steps (effective batch size of 16). We apply random crop with a probability of $p_{crop} = 0.75$, where the crop size ranges from 70\% to 100\% of the original image dimensions, and color augmentation with a probability of $p_{color} = 0.65$. 

For the Gehler dataset~\cite{4587765}, when training on Canon5D and evaluating on Canon1D, we use a batch size of 8, apply random crop with a probability of $p_{crop} = 0.75$ (crop size from 70\% to 100\%), and color augmentation with a probability of $p_{color} = 0.85$. Similarly, when training on Canon1D and evaluating on Canon5D, we maintain the same batch size of 8, with random crop probability of $p_{crop} = 0.7$ and crop size ranging from 50\% to 100\%, while keeping the color augmentation probability at $p_{color} = 0.85$.

\paragraph{Cross-dataset Evaluation}
When training on NUS-8~\cite{cheng2014illuminant} and testing on the Gehler dataset\cite{4587765}, we use a batch size of 8 with gradient accumulation over 2 steps (effective batch size of 16). We apply random crop with a probability of $p_{crop} = 0.75$, where the crop size ranges from 70\% to 100\% of the original dimensions, and color augmentation with a probability of $p_{color} = 0.6$. Conversely, when training on the Gehler dataset~\cite{4587765} and testing on NUS-8~\cite{cheng2014illuminant}, we use a batch size of 8 without gradient accumulation. We apply random crop with the same probability of $p_{crop} = 0.75$ and size range of 70\% to 100\%, while color augmentation is applied with a probability of $p_{color} = 1.0$.

\paragraph{SDXL Inpainting (SDEdit)}
For the SDXL inpainting model \cite{rombach2021highresolution} with LoRA fine-tuning experiments, we use a learning rate of $5 \times 10^{-5}$ and a LoRA rank of 4. In the cross-dataset experiment from the NUS-8 dataset~\cite{cheng2014illuminant} to the Gehler dataset~\cite{4587765}, we train for 20,000 iterations with batch size 4.

\subsection{Inference Settings}
\paragraph{Full Model} Following \citeauthor{garcia2024fine} \cite{garcia2024fine}, we employ DDIM scheduler with a fixed timestep $t=T$ and \textbf{trailing} strategy during inference for deterministic single-step generation. Our implementation is based on the stable-diffusion-2-inpainting model \cite{rombach2021highresolution}.
\paragraph{SDXL Inpainting (SDEdit)} 
For comparison, we also implement a version using SDXL inpainting model \cite{rombach2021highresolution} with LoRA \cite{hu2021lora} fine-tuning. During inference, we use the DDIM scheduler with 25 denoising steps and SDEdit with a noise strength of 0.6, a guidance scale of 7.5, and a LoRA scale of 1. The final illumination estimation is obtained by computing the median from an ensemble of 10 generated samples.
\section{Laplacian Decomposition}~\label{sec:Laplacian}
\input{figs/lapacian}


\subsection{Laplacian Decomposition Visualization}
Figure~\ref{fig:lapacian} visualizes the algorithm flow of our Laplacian decomposition technique. Algorithm~\ref{alg:laplacian} outlines the detailed steps of this process, which preserves high-frequency structural details while allowing illumination-dependent color adaptation, enabling accurate scene illumination estimation.

\begin{algorithm}[h!]
\footnotesize
\SetAlgoLined
\DontPrintSemicolon
\KwIn{Input latent $z \in \mathbb{R}^{B \times C \times H \times W}$, pyramid levels $L$}
\KwOut{High-frequency components $z_h$}
\texttt{Initialize} $z_h = 0$\;
$k \leftarrow$ \texttt{3×3 Gaussian kernel}\;
\For{\texttt{each channel} $c$ \texttt{in} $C$}{
    $z_{\text{curr}} \leftarrow z[c]$ \tcp*[r]{\texttt{Current level features}}
    \For{$l = 0$ \texttt{to} $L-1$}{
        $z_{\text{blur}} \leftarrow k * z_{\text{curr}}$ \tcp*[r]{\texttt{Gaussian blur}}
        $z_{\text{high}} \leftarrow z_{\text{curr}} - z_{\text{blur}}$ \tcp*[r]{\texttt{High-freq details}}
        \eIf{$l = 0$}{
            $z_h[c] \leftarrow z_{\text{high}}$
        }{
            $z_h[c] \leftarrow z_h[c] + \texttt{Upsample}(z_{\text{high}})$
        }
        $z_{\text{curr}} \leftarrow \texttt{AvgPool}(z_{\text{blur}})$ \tcp*[r]{\texttt{Downsample}}
    }
}
\Return $z_h$
\caption{High-frequency Extraction via Laplacian Pyramid}
\label{alg:laplacian}
\end{algorithm}



\subsection{Analysis of Pyramid Level Selection}
We conduct experiments with different numbers of pyramid levels (L = 1,2,3) to analyze the effectiveness of our Laplacian decomposition. As shown in \cref{tab:pyramid_levels}, using two-level decomposition (L = 2) achieves the best performance across all metrics. Adding more levels not only increases computational complexity but also leads to performance degradation, as the additional levels introduce more low-frequency information that can adversely affect the harmonious generation of color checkers.
\section{Additional Qualitative Results}~\label{sec:Qualitative}

\subsection{Benchmark Datasets}
On the NUS-8 dataset~\cite{cheng2014illuminant} and Gehler dataset~\cite{4587765}, we utilize the original mask locations to place fixed-size neutral color checkers in our experiments. The results \cref{fig:suppl_NUS_demo} and \cref{fig:suppl_gehler_demo} demonstrate our method's ability to generate structurally coherent color checkers that naturally blend with the scene while accurately reflecting local illumination conditions, enabling effective color cast removal across diverse lighting scenarios.

\subsection{In-the-wild Images}
For in-the-wild scenes, we adopt a center-aligned placement strategy to address camera vignetting effects, which can impact color accuracy near image edges. This consistent central positioning not only mitigates lens shading issues but also demonstrates our method's flexibility in color checker placement. The results \cref{fig:suppl_inthewild_demo} validate our approach's robustness in practical photography applications, showing consistent performance in white balance correction despite the fixed central placement strategy.
\subsection{Interactive Visualization}
We provide an interactive HTML interface that visualizes results with color checkers placed at different locations within scenes. The visualization demonstrates that our method produces accurate outputs with minimal variation across different placement positions. The results show that the estimated illumination values consistently cluster near the ground truth target regardless of the checker's position, confirming our method's reliability and position-independence in illumination estimation.

\input{figs/fail_case}
% \section{Cross-validation Results}~\label{sec:cross_validation}

% For evaluation, we follow the standard protocol of three-fold cross-validation on both the NUS-8 Camera dataset \cite{Cheng:14} and the Gehler dataset \cite{4587765}. The results are presented in \cref{table:cross_nus8} and \cref{table:cross_gehler}.
% As FC4 notes, \emph{many scenes have multiple light sources with differences up to 10 degrees, so further reducing an error already under 2 degrees may not be a strong comparison.} Instead, our method \textbf{inpaints physically plausible color checkers}—a different strategy than directly optimizing for a ground truth illumination RGB, which can yield lower single-camera performance but enables \textbf{strong cross-camera generalization}, as shown in our cross-dataset experiments.
% Following the evaluation protocol in \cite{afifi2019sensor}, different from traditional learning-based methods that use three-fold cross-validation, we evaluate our method by completely excluding the testing camera's data from the training set. During each evaluation, we train our model using images from all cameras except the target testing camera, ensuring our model has never seen examples from the test camera during training. Despite this challenging setup, our method shows competitive performance, as shown in ~\cref{table:cross_nus8} and \cref{table:cross_gehler}, particularly in the worst 25\% cases where we outperform several sensor-specific methods that were trained with the test camera's data, demonstrating the strong generalization capability of our approach across different camera sensors.


\input{tables/laplacian_level}
% \input{tables/NUS8}
% \input{tables/gehler}
\input{figs/suppl_NUS8_demo}
\input{figs/suppl_gehler_demo}
\input{figs/suppl_inthewild_demo}

\section{Limitations}
As shown in \cref{fig:failure}, our method struggles when there is a significant mismatch between the inpainted color checker and the scene's ambient lighting. This typically occurs in challenging scenarios with multiple strong light sources of different colors or complex spatially-varying illumination. While diffusion models provide strong image priors, they sometimes prioritize visual plausibility over physical accuracy, especially in extreme lighting conditions.

Our approach also shows sensitivity to dataset size, similar to personalization effects observed in DreamBooth~\cite{ruiz2022dreambooth}. For datasets with limited samples, we need to crop smaller mask regions to ensure the model can effectively learn the color checker's appearance and structure. In our experiments, we found that when the training dataset is extremely small, the model generates color checkers with unexpected appearances and distorted structures, preventing accurate color extraction for illumination estimation. This limitation suggests potential future directions for improving our method through more efficient learning strategies or additional data augmentation techniques to better handle scenarios with limited training data.