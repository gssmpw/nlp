\section{Related Work}
\label{sec:related}

\noindent {\bf Color Constancy and White Balance.}
% Color constancy, a fundamental computer vision task, aims to estimate scene illumination to enable white balance correction in camera imaging pipelines. Research in this area can be broadly categorized into statistical-based and learning-based approaches.
% Traditional statistical-based methods rely on assumptions about scene color statistics, such as the Gray World ____, Gray Edge ____, and Shades-of-Gray ____ algorithms. These methods are computationally efficient but often fail in challenging scenes with ambiguous color distributions ____. More sophisticated statistical approaches like Bright Pixels ____ and Gray Index ____ have been proposed but remain sensitive to violations of their underlying assumptions.
% Learning-based methods have shown superior performance by leveraging training data to learn more complex illumination priors. Early approaches focused on gamut mapping ____ or simple regression models ____. With the advent of deep learning, methods like Convolutional Color Constancy (CCC) ____ and its faster variant FFCC ____ reformulated the problem as 2D spatial localization in log-chroma space. FC4 ____ introduced confidence-weighted pooling to automatically identify important spatial regions for illumination estimation.
% A significant challenge in learning-based color constancy is that models are typically constrained to specific camera sensors due to variations in spectral sensitivities ____. Models trained on one camera often fail to generalize to others without retraining or calibration ____. Recent works have approached this problem from different angles: IGTN ____ introduced metric learning to learn scene-independent illuminant features, while quasi-unsupervised approaches ____ leverage semantic features of achromatic objects for better cross-sensor generalization.
% Several works have attempted to address the multi-sensor challenge through domain adaptation techniques ____ or by learning device-independent intermediate representations ____. C5 ____ proposed an innovative approach that uses multiple unlabeled images from the target camera during inference to calibrate the model to new sensors. CLCC ____ further improved upon this by introducing contrastive learning to ensure that images of the same scene under different illuminants have distinct representations, while different scenes under the same illuminant have similar representations.
% In our work, we take a novel approach by leveraging the strong priors learned by large-scale diffusion models to inpaint color checkers into images, enabling accurate illumination estimation without requiring extensive camera-specific training data.
% 
Color constancy research spans statistical-based and learning-based approaches. Statistical methods like Gray World ____, Gray Edge ____, Shades-of-Gray ____, Bright Pixels ____, and Gray Index ____ make assumptions about scene color statistics but struggle with challenging scenes.
% 
Learning-based methods have proven more effective, evolving from gamut mapping ____ and regression models ____ to more advanced techniques. Notable developments include CCC ____ and FFCC ____, which use convolutional processing and frequency-domain optimization. Deep learning approaches like FC4 ____, DS-Net ____, RCC-Net ____, and C4 ____ further improve performance with various neural network architectures. 

A key challenge is camera-specific spectral sensitivity ____, requiring retraining or calibration for new sensors ____. Recent solutions include IGTN's ____ metric learning, quasi-unsupervised learning ____, and cross-dataset approaches ____. SIIE ____ proposes sensor-independent illumination estimation, while C5 ____ uses unlabeled target camera images during inference, and CLCC ____ employs contrastive learning to improve feature representations. Our work leverages pre-trained diffusion models for color checker inpainting, utilizing their rich knowledge priors to offer a novel approach to illumination estimation with enhanced generalization capability across different camera sensors.

\vspace{3pt}  \noindent {\bf Image-conditional Diffusion Models.}
% Recent years have seen remarkable progress in generative modeling, with diffusion models emerging as a particularly powerful approach. Denoising Diffusion Probabilistic Models (DDPMs) ____ learn to reverse a gradual noising process, achieving state-of-the-art results in both density estimation and sample quality ____. Their success stems from a natural fit with image-like data when implemented using a UNet backbone ____.
% A significant advancement was the introduction of Latent Diffusion Models (LDMs) ____, which operate in the compressed latent space of a pre-trained autoencoder rather than pixel space. This innovation dramatically reduced computational requirements while maintaining high-quality generation. LDMs also introduced effective conditioning mechanisms like cross-attention, enabling the models to be guided by additional inputs such as text or images.
% The success of LDMs has led to powerful image inpainting capabilities. Recent works like Blended Diffusion ____ enable object insertion through text prompts for masked regions using guided sampling based on CLIP embeddings. Paint-by-Example ____ extends this by allowing example images as conditioning. ControlNet ____ and IP-Adapter ____ further enhance control by enabling both image and text prompts to condition the generation process.
% However, these models face challenges when tasked with generating physically accurate content, such as reflective surfaces. Recent work has revealed that some perceived limitations of image-conditional diffusion models were due to implementation issues rather than fundamental constraints. For instance, a flaw in the DDIM scheduler implementation was causing poor performance with few sampling steps ____. This insight has led to improved techniques that can achieve high-quality results with significantly fewer denoising steps.
% In our work, we leverage these advances in image-conditional diffusion models for color constancy. By carefully addressing implementation challenges and designing appropriate conditioning strategies, we show that diffusion models can effectively inpaint color checkers for illumination estimation, even in complex real-world scenarios.
% 
Denoising Diffusion Probabilistic Models (DDPMs) ____ achieve state-of-the-art generation by reversing a noising process with UNet architectures ____, demonstrating excellence in density estimation and sample quality ____.
Latent Diffusion Models (LDMs) ____ improved efficiency by operating in compressed latent space and introduced cross-attention conditioning. This enabled powerful inpainting capabilities, demonstrated by Blended Diffusion ____, Paint-by-Example ____, ControlNet ____, and IP-Adapter ____.
Recent work identified that perceived limitations were often due to DDIM scheduler implementation issues ____ rather than fundamental constraints. Our work leverages these insights to effectively adapt diffusion models for color checker inpainting in illumination estimation.

\vspace{3pt}  \noindent {\bf Learning-based Lighting Estimation.}
Lighting estimation methods traditionally use physical probes like mirror balls ____, 3D objects ____, eyes ____, or faces ____. Early probe-free approaches used limited models like directional lights ____, sky models ____, or spherical harmonics ____.
Modern methods focus on HDR environment maps, pioneered by Gardner et al. ____. DeepLight ____ and EverLight ____ handle both indoor and outdoor scenes, while StyleLight ____ uses GANs for joint LDR-HDR prediction. Some works explore panorama outpainting ____ but struggle with HDR ____.
Recently, DiffusionLight ____ introduced virtual chrome ball synthesis using diffusion models. Our work follows a similar direction but focuses on color checker inpainting for illumination estimation.

\vspace{3pt}  \noindent {\bf Fine-tuning Strategies for Diffusion Models.}
% Recent advances in diffusion model adaptation have shown several effective strategies for fine-tuning pre-trained models for specific tasks. These approaches generally aim to preserve the rich priors learned during pre-training while adapting the model for new objectives.
% A notable development in this space is DreamBooth ____, which introduces a method for personalizing text-to-image diffusion models using just a few reference images. The approach uses a special token during fine-tuning to represent the target object while maintaining the model's pre-trained distribution. Building on this work, Gal et al. ____ propose learning a word embedding to represent reference objects, while Voynov et al. ____ extend this by learning separate word embeddings for each network layer.
% Recent work has revealed interesting insights about fine-tuning strategies for geometry estimation tasks. Marigold ____ demonstrated success in fine-tuning Stable Diffusion for depth estimation using high-quality synthetic data. A significant breakthrough came from Garcia et al. ____, who showed that many perceived limitations of image-conditional diffusion models were due to implementation issues in the DDIM inference pipeline. Their work demonstrates that simple end-to-end fine-tuning with task-specific losses can outperform more complex training approaches, challenging previous assumptions about the necessity of sophisticated fine-tuning strategies.
% Several techniques have been proposed to make fine-tuning more efficient and effective. Low-rank adaptation (LoRA) ____ has emerged as a popular approach that enforces low-rank weight changes, making fine-tuning more computationally tractable while preserving model capabilities. Other methods like SVDiff ____ and orthogonal fine-tuning ____ provide alternative parameterizations for efficient model adaptation.
% In our work, we leverage these advances to fine-tune diffusion models for color constancy, showing that careful adaptation of pre-trained models can yield effective results even for specialized tasks like color checker inpainting. Like Garcia et al. ____, we find that relatively simple fine-tuning strategies can be highly effective when properly implemented.
% 
For personalization, DreamBooth ____ pioneered special token fine-tuning, while ____ ____ and ____ ____ proposed learned word embeddings approaches. Similar to DreamBooth, our method fine-tunes pre-trained diffusion models to bind specific visual characteristics to our target domain, enabling a consistent generation of color checkers that reflect scene illumination.
% 
For efficiency and fine-tuning strategies, LoRA ____ introduced low-rank weight changes, while SVDiff ____ and orthogonal fine-tuning ____ proposed alternative parameterizations. For geometry estimation, Marigold ____ demonstrated successful fine-tuning using synthetic data. Inspired by ____ ____, who showed that simple fine-tuning approaches can be highly effective for deterministic tasks involving low-frequency image components, we adopt their full fine-tuning strategy for our color checker inpainting task. This approach aligns well with our color constancy problem, which primarily focuses on modifying the low-frequency characteristics of color checkers.
% For personalization, DreamBooth ____ pioneered special token fine-tuning, while ____ ____ and ____ ____ proposed learned word embeddings approaches.
% For geometry estimation, Marigold ____ demonstrated successful fine-tuning using synthetic data. ____ ____ revealed that simple end-to-end fine-tuning can outperform complex approaches once DDIM implementation issues are fixed.
% For efficiency, LoRA ____ introduced low-rank weight changes, while SVDiff ____ and orthogonal fine-tuning ____ proposed alternative parameterizations. Following ____ ____, we adopt simple full fine-tuning strategies for our color checker inpainting task.