\section{Introduction}
\label{sec:intro}

Color constancy is a crucial aspect of computer vision, focused on determining the illumination of a scene to ensure that colors are accurately represented under varying lighting conditions. This process is essential for maintaining a consistent color appearance and for applications ranging from photography to autonomous driving. Traditional statistics-based methodologies \cite{buchsbaum1980spatial, land1977retinex, forsyth1990novel, van2007edge, joze2012role, qian2019finding, barnard2002comparison, finlayson2004shades} rely on various statistical assumptions about scene color distributions. While these methods are computationally efficient, they often struggle in challenging scenes when their underlying assumptions are violated, especially in environments with multiple illuminants or complex lighting conditions.

In contrast, deep learning-based methods \cite{hu2017fc4, Bianco2015ColorCU, Lou2015ColorCB} have significantly advanced the field of color constancy through their ability to learn complex illumination patterns from training data. These approaches typically employ convolutional neural networks with various architectures to achieve state-of-the-art performance, particularly in challenging illumination scenarios.

However, a challenge in learning-based color constancy is that models are often constrained to specific camera sensors due to variations in spectral sensitivities. Recent cross-camera approaches \cite{igtn, bianco2019quasi, afifi2019sensor, afifi2021cross, lo2021clcc, yu2020cascading} have made strides in addressing this limitation through techniques including metric learning, quasi-unsupervised learning, and device-independent representations. Building upon these advances, we explore an approach that leverages foundation models to enhance cross-camera performance.

% However, a significant challenge in learning-based color constancy is that models are often constrained to specific camera sensors due to variations in spectral sensitivities. Recent cross-camera approaches \cite{igtn, bianco2019quasi, afifi2019sensor, afifi2021cross, lo2021clcc, yu2020cascading} have attempted to address this limitation through various techniques including metric learning, quasi-unsupervised learning, and device-independent representations, yet achieving robust camera-agnostic illumination estimation remains challenging.

Inspired by the recent success of DiffusionLight~\cite{phongthawee2024diffusionlight}, which leverages pre-trained diffusion models for lighting estimation by inpainting a chrome ball, we propose Generative Color Constancy (GCC), a novel approach that harnesses the rich priors of foundation models to overcome the camera-specific limitations of traditional methods. Unlike DiffusionLight~\cite{phongthawee2024diffusionlight}, which focuses on HDR lighting estimation, our method adapts the concept to color constancy by inpainting a color checker into the input image. Color checkers are widely used calibration tools in color science, and our diffusion model generates one with colors that accurately represent the scene's illumination. By analyzing the generated color checker's patches, we can effectively estimate the scene's illuminant. However, diffusion models typically generate outputs stochastically, which is undesirable for color constancy applications requiring consistency. Drawing insights from recent work on deterministic fine-tuning of image-conditional diffusion models~\cite{garcia2024fine}, we design a deterministic pipeline that produces consistent illumination estimates while preserving the powerful generalization capabilities of the underlying foundation model. Our approach eliminates the need for camera-specific training data, achieving robust performance across different camera sensors and scene types.

In summary, we make the following contributions:
\begin{itemize}
    \item We propose a novel color constancy method that leverages diffusion models to inpaint a color checker, which serves as a virtual reference for illumination estimation.
    
    \item We introduce a Laplacian decomposition technique that enhances the model to generate color checkers that maintain structure while adapting to scene illumination, improving color extraction accuracy.
    
    \item We design a deterministic single step inference pipeline that avoids introducing noise during training and inference, resulting in consistent results and improved computational efficiency compared to traditional diffusion processes.
\end{itemize}


% Color constancy is a crucial aspect of computer vision, focused on determining the illumination of a scene to ensure that colors are accurately represented under varying lighting conditions. This process is essential for maintaining a consistent color appearance and for applications ranging from photography to autonomous driving. Traditional statistics-based methodologies \cite{buchsbaum1980spatial, land1977retinex, forsyth1990novel, van2007edge, joze2012role, qian2019finding, barnard2002comparison, finlayson2004shades} rely on various statistical assumptions about scene color distributions. While these methods are computationally efficient, they often struggle in challenging scenes when their underlying assumptions are violated, especially in environments with multiple illuminants or complex lighting conditions.


% In contrast, learning-based methods \cite{barnard2000improvements, finlayson2013corrected, joze2012exemplar, chakrabarti2011color, funt2004estimating, barron2017fast, barron2015convolutional} have demonstrated superior performance by leveraging training data to learn complex illumination priors. Deep learning approaches have further advanced the field, with notable methods including C4 \cite{barron2015convolutional} and FC4 \cite{hu2017fc4}, the latter introducing confidence-weighted pooling to automatically identify important spatial regions for illumination estimation.

% However, a significant challenge in learning-based color constancy is that models are often constrained to specific camera sensors due to variations in spectral sensitivities. Recent works have approached this problem from various angles. For instance, IGTN \cite{igtn} introduced metric learning to learn scene-independent illuminant features, while quasi-unsupervised approaches \cite{bianco2019quasi} leverage semantic features of achromatic objects for better cross-sensor generalization. Several studies have attempted to address the multi-sensor challenge by learning device-independent intermediate representations \cite{afifi2019sensor}. C5 \cite{afifi2021cross} proposed an innovative approach that uses multiple unlabeled images from the target camera during inference to calibrate the model to new sensors. CLCC \cite{lo2021clcc} further improved upon this by introducing contrastive learning to ensure that images of the same scene under different illuminants have distinct representations, while different scenes under the same illuminant have similar representations.

% In this paper, we present a novel approach to color constancy that leverages inpainting techniques to integrate a color checker directly into the image. Our method utilizes Laplacian decomposition, allowing us to preserve high-frequency structural details while reducing the influence of low-frequency color information from the inserted color checker. 

% In summary, we make the following contributions:
% \begin{itemize}

% \item We are the first to utilize inpainting for color checker integration in color constancy tasks, providing a new avenue for illumination estimation without the need for extensive camera-specific training data.

% \item By employing Laplacian decomposition, we enhance the model's ability to generate a color checker that is structurally consistent with the input image, thereby improving the accuracy of color extraction from the patches.

% \item Our method operates in a deterministic manner, avoiding the introduction of noise during training and inference, which enables more reliable results with better computational efficiency.
% % Our method operates in a deterministic manner, avoiding the introduction of noise during training and inference, which allows for more reliable and consistent results.
% \end{itemize}

% % To insert a figure: \input{figs/template}
% % Or table: \input{tables/template}