\section{Method}

\input{figs/pipeline}

\input{figs/inference}


\label{sec:method}
Instead of directly predicting environmental RGB light, we propose to leverage diffusion models' rich priors to inpaint a color checker into the scene and extract illumination colors from it. As shown in \cref{fig:pipeline,fig:inference}, our pipeline consists of (1) During training, we fine-tune a diffusion-based inpainting model at timestep t=T with images containing color checkers, optimizing for deterministic single-step inference (Sec. \ref{sec:network_architecture}-\ref{sec:end_to_end_finetune}). (2) We introduce Laplacian decomposition to maintain the checker's high-frequency structure while allowing illumination-aware color adaptation (Sec. \ref{sec:laplacian_composition}). (3) At inference time, we composite a neutral color checker into a given scene and use our fine-tuned model to inpaint it according to the scene illumination, from which we extract the scene's light color information (Sec. \ref{sec:inference}). 
% \subsection{Pipeline Overview}


\subsection{Network Architecture}
\label{sec:network_architecture}
We base our model on stable-diffusion-2-inpainting {\cite{rombach2021highresolution}} for its specialized local editing capability. The model consists of a VAE encoder-decoder pair ($\mathcal{E}$, $\mathcal{D}$) and a U-Net denoising backbone. Given an RGB image $I \in \mathbb{R}^{H\times W\times 3}$ and a binary mask $M \in \{0,1\}^{H\times W}$ indicating the color checker region, we first encode both the masked image and the original image into the latent space as $z_{\text{masked}} =  \mathcal{E}(I \odot (1 - M))$ and $z = \mathcal{E}(I)$, where $\odot$ denotes element-wise multiplication. The mask $M$ is downscaled by a factor of 8 to match the latent resolution as $M' \in \mathbb{R}^{h\times w}$, where $h = H/8, w = W/8$. During training, the U-Net denoiser $\epsilon_\theta$ takes as input the concatenation of the noised latent $z_t$, the downscaled mask $M'$, and the masked image latent $z_{\text{masked}}$ along the channel dimension as $z_{\text{combined}} = [z_t, M', z_{\text{masked}}] \in \mathbb{R}^{h\times w\times(2d+1)}$, where $d$ is the latent dimension. Together with the timestep $t$ and text embedding $c$, the denoiser is trained to predict the noise as $\epsilon_\theta(z_{\text{combined}}, t, c) \rightarrow \mathbb{R}^{h\times w\times d}$. At inference time, we obtain the final inpainted result by decoding the denoised latent $\hat{I} = \mathcal{D}(z_0)$, where only the color checker region is modified while leaving the rest unmodified, making this architecture particularly suitable for the color constancy task.

\input{figs/direct_inpaint}
% \input{figs/mask_da}

\subsection{End-to-End Fine-Tuning}
\label{sec:end_to_end_finetune}

\noindent {\bf Training.}
Although pre-trained diffusion models like SD and SD inpainting \cite{rombach2021highresolution} have been exposed to diverse image collections, additional fine-tuning is crucial for generating precise color checkers that accurately reflect environmental illumination. As shown in experiments \cref{fig:wo_fig}, fine-tuning significantly impacts the model's ability to generate color checkers that faithfully represent scene illumination.


% This transformation aligns the latent distribution closer to that of standard RGB images seen during pre-training. Conversely, after VAE decoding, we convert the output back to the linear domain using inverse gamma correction for loss computation and metric evaluation:

% $I_{\text{linear}} = I_{\text{sRGB}}^{2.2}$
Although SDEdit~\cite{meng2022sdeditguidedimagesynthesis} could be applied to our task, it faces a fundamental trade-off in noise level selection. On one hand, insufficient noise fails to effectively suppress the original chromatic information from the input image, making it difficult to adapt to the target scene illumination. On the other hand, excessive noise, while better at removing unwanted color information, can disrupt the structural consistency between the generated result and the input reference. Furthermore, for color constancy tasks, maintaining a one-to-one correspondence between input and output is essential. While traditional diffusion models' stochastic nature allows for ensemble improvements through multiple inferences, this comes at an increased computational cost.

% Although SDEdit~\cite{meng2022sdeditguidedimagesynthesis}, which could be applied to our task, could maintain structural coherence while reflecting scene illumination, it faces a fundamental trade-off between structure preservation and color adaptation. Specifically, insufficient noise cannot effectively suppress the original chromatic information, compromising the generation quality. Furthermore, for color constancy tasks, maintaining a one-to-one correspondence between input and output is essential. While traditional diffusion models' stochastic nature allows for ensemble improvements through multiple inferences, this comes at increased computational cost.

Following \cite{garcia2024fine}, we adopt an end-to-end fine-tuning approach that enables single-step deterministic inference while maintaining high-quality color checker generation. Specifically, we fine-tune the inpainting U-Net at a fixed timestep $t = T$ as shown in \cref{fig:pipeline}. 
% 
{Given an input image $I$ and its corresponding mask $\mathbf{M}$, we first obtain the augmented image $I_{\text{aug}}$ by applying color jittering to the masked region. We then obtain its latent representation through the VAE encoder, $\mathbf{z}^* = \mathcal{E}(I_{\text{aug}})$. The latent representation is processed through Laplacian decomposition to extract high-frequency components, $\mathbf{z}_h = \mathcal{H}(\mathbf{z}^*)$. For single-step prediction, we directly set the noise term $\boldsymbol{\epsilon} = \mathbf{0}$ in the forward process: $\mathbf{z}_T = \sqrt{\bar{\alpha}_T}\mathbf{z}_h + \sqrt{1-\bar{\alpha}_T}\boldsymbol{\epsilon}$. The denoised latent is then obtained through $\hat{\mathbf{z}}_0 = \sqrt{\bar{\alpha}_T}\mathbf{z}_T - \sqrt{1-\bar{\alpha}_T}\epsilon_\theta(\mathbf{z}_{\text{combined}}, T, c)$, where $\mathbf{z}_{\text{combined}} = [\mathbf{z}_T, \mathbf{M}', \mathbf{z}_{\text{masked}}] \in \mathbb{R}^{h \times w \times (2d+1)}$ represents the concatenated input features along the channel dimension, and $c$ denotes the text condition. Finally, we decode the latent to obtain the inpainted image: $\hat{I} = \mathcal{D}(\hat{\mathbf{z}}_0)$. The model is optimized using a mean squared error loss:
% 
% Given a training image $I^*$ and its corresponding mask $\mathbf{M}$, we first obtain its latent representation through the VAE encoder: $\mathbf{z}^* = \mathcal{E}(I^*)$. For single-step prediction, we directly set the noise term $\boldsymbol{\epsilon} = \mathbf{0}$ in the forward process: $\mathbf{z}_T = \sqrt{\bar{\alpha}_T}\mathbf{z}^* + \sqrt{1-\bar{\alpha}_T}\boldsymbol{\epsilon}$. The denoised latent is then obtained through $\hat{\mathbf{z}}_0 = \sqrt{\bar{\alpha}_T}\mathbf{z}_T - \sqrt{1-\bar{\alpha}_T}\epsilon_\theta(\mathbf{z}_{\text{combined}}, T, c)$, where $\mathbf{z}_{\text{combined}} = [\mathbf{z}_T, \mathbf{M}', \mathbf{z}_{\text{masked}}] \in \mathbb{R}^{h \times w \times (2d+1)}$ represents the concatenated input features along the channel dimension, and $c$ denotes the text condition. Finally, we decode the latent to obtain the inpainted image: $\hat{I} = \mathcal{D}(\hat{\mathbf{z}}_0)$. The model is optimized using a mean squared error loss:
\begin{equation}
\mathcal{L} = \frac{1}{HW}\sum_{i,j} (I^*_{i,j} - \hat{I}_{i,j})^2, 
\end{equation}
where $(i,j)$ denotes the pixel coordinates, and $H$ and $W$ are the height and width of the image, respectively.

\vspace{3pt}  \noindent {\bf Color checker misalignment issue.}
Existing color constancy datasets \cite{Cheng:14,4587765} only provide rough bounding boxes for color checkers instead of precise corner point locations. This hinders our ability to accurately align the standard sRGB color checker with the one in the original image, affecting the model's learning of the transformation from standard to harmonized colors. To overcome this limitation, we designed a mask region-based data augmentation method.

We first analyze two intuitive solutions: directly masking and allowing the model to perform inpainting. This approach results in generated color checkers with contours that do not meet our expectations, making accurate color extraction from the patches difficult (\cref{fig:direct_inpaint} (a)). The second solution involved overlaying the color checker template directly onto the original image (\cref{fig:direct_inpaint} (b)). However, due to the absence of precise corner point locations, alignment with the raw checker remains imperfect at a per-pixel level even when using homography transform.

\vspace{3pt}  \noindent {\bf Masked color jittering.}
Therefore, we further explored a third approach: directly applying strong color jittering to the mask region (\cref{fig:direct_inpaint} (c)). This seemingly counterintuitive method aims to destroy clues that may leak sensor-specific information, forcing the model to rely on information outside the mask region to reconstruct the original color checker that aligns with the ground truth. 

Random color jittering on masked checkers helps our model learn robust mappings between neutral color references and scene-specific lighting appearances. The augmented image $I_{\text{aug}}$ is obtained by:
\begin{small}
\begin{equation}
I_{\text{aug}} = (1-M) \odot I + M \odot \mathcal{T}(I),
\end{equation}
\end{small}
where $I$ is the input image, $M$ is the binary mask, $\odot$ denotes element-wise multiplication, and $\mathcal{T}(\cdot)$ represents the color jittering function that randomly applies brightness, contrast, and saturation adjustments to the masked region. By randomly perturbing the color checker region, we force the model to rely on contextual illumination cues rather than local color checker patterns. This approach overcomes the limitations of imprecise annotations in existing datasets and enhances the model's ability to learn accurate illumination estimation from scene context.

\subsection{Laplacian Decomposition}
\label{sec:laplacian_composition}
Although mask color jittering addresses the imprecise corner annotation issue, the randomness in jittering may occasionally allow low-frequency information leakage from the masked region. This could cause the model to simply \textit{reconstruct} the masked area rather than \textit{harmonize} it with the scene illumination. To address this issue, we introduce the Laplacian decomposition technique.

By extracting only the high-frequency components of the input image through Laplacian decomposition, our approach serves two purposes: First, it preserves the structural details needed to generate a color checker that faithfully maintains the patch layout of our pre-pasted reference. Second, it minimizes the influence of low-frequency color information, encouraging the model to focus on harmonizing the generated color checker with the scene illumination rather than reconstructing the original colors.
% 
% The detailed Laplacian decomposition process is presented in Algorithm \ref{alg:laplacian}.
The key benefit of Laplacian decomposition, as shown in \cref{fig:wo_fig}, allows the model to generate color checkers that maintain structural consistency while correctly reflecting scene illumination, enabling accurate illumination estimation.

\subsection{Inference}
\label{sec:inference}
The complete inference pipeline of our method is illustrated in \cref{fig:inference}, which consists of the following steps:

\vspace{3pt}  \noindent {\bf Color checker generation.}
We first composite a fixed-size neutral color checker centered at the mask region. The input image is then gamma-corrected with $\gamma = 2.2$ to transform it to the sRGB domain. This preprocessed image is processed through our model in a single forward pass with fixed timestep $t = T$. The output is then inverse gamma-corrected to obtain the raw domain result.

\vspace{3pt} \noindent {\bf Illumination estimation.} Since we have precise control over initial checker placement and Laplacian decomposition ensures structural preservation, we can reliably extract color information from each patch. Specifically, we directly map the generated checker to a standardized grid, followed by applying fixed masks to sample colors from each patch. The scene illumination is then estimated from the achromatic patches of the color checker.

% We first composite a fixed-size neutral color checker centered at the mask region. The input image is then gamma-corrected with $\gamma = 2.2$ to transform it to the sRGB domain and processed through our model pipeline (VAE encoder, inpainting U-Net, and VAE decoder) in a single forward pass with fixed timestep $t = T$. After inverse gamma correction, we extract the environmental color from the generated color checker following standard protocols.
% The output of the model is then inverse gamma-corrected to obtain the raw data. Finally, we follow the ground truth extraction approach based on the NUS-8 Camera dataset \cite{cheng2014illuminant} and the re-processed Color Checker dataset \cite{shi2000re}, which leads to our final output by extracting the environmental color.

% In the NUS-8 Camera dataset~\cite{cheng2014illuminant}, the ground truth illuminants were originally computed by taking the difference between the two brightest non-saturated achromatic patches. When processing this dataset, we normalize the images using the provided saturation levels before inverse gamma correction. This can lead to misalignment between our extracted patch values and the dataset's ground truth patches. Therefore, we adopt the strategy of finding the difference between adjacent achromatic patches that yields the best result. This aligns with the original dataset's methodology, where the selected ground truth patches were likely adjacent to the color checker.


% The Laplacian Composition technique described in the previous section helps maintain the high-frequency structure of the color checker while allowing illumination-aware color adaptation, leading to a harmonious final output.

