\section{Experiments}
\label{sec:exp}
\subsection{Experimental Setup}

\noindent {\bf Dataset.}
We use two publicly available color constancy benchmark datasets in our experiments: the NUS-8 dataset~\cite{cheng2014illuminant} and the re-processed Color Checker dataset~\cite{4587765} (referred to as the Gehler dataset). The Gehler dataset~\cite{4587765} contains 568 original images captured by two different cameras, while the NUS-8 dataset~\cite{cheng2014illuminant} contains 1736 original images captured by eight different cameras. Each image in both datasets includes a Macbeth Color Checker chart, which serves as a reference for the ground-truth illuminant color.

\vspace{3pt}  \noindent {\bf Evaluation metrics.}
To evaluate the performance of color constancy methods, we use the standard angular error metric, which measures the angular difference between the estimated illuminant and the ground-truth illuminant. Specifically, the angular error $\theta$ between an estimated illuminant vector $\hat{\mathbf{y}}$ and the ground-truth illuminant vector $\mathbf{y}$ is defined as:
\begin{small}
\begin{equation}
\theta = \arccos\left(\frac{\hat{\mathbf{y}} \cdot \mathbf{y}}{|\hat{\mathbf{y}}| |\mathbf{y}|}\right)
\end{equation}
\end{small}
The angular error is measured in degrees, with smaller values indicating better estimation accuracy. Following previous works, we report the following statistics of the angular error.

\subsection{Implementation Details}
Our implementation is based on the stable-diffusion-2-inpainting model~\cite{rombach2021highresolution} using PyTorch. All input images are resized to 512$\times$512 resolution for both training and inference. Since the pre-trained VAE was trained on sRGB images, we apply a gamma correction of $\gamma = 1/2.2$ on linear RGB images before encoding to minimize the domain gap. Conversely, after VAE decoding, we apply inverse gamma correction to convert the output back to the linear domain for metric evaluation.

Following parameter settings from~\cite{garcia2024fine}, we train our models using the Adam optimizer with an initial learning rate of $5 \times 10^{-5}$ and apply an exponential learning rate decay after a 150-step warm-up period. For cross-dataset evaluation, when training on the Gehler dataset and testing on NUS-8, we use a batch size of 8 with no gradient accumulation for 20k iterations. When training on NUS-8 and testing on the Gehler dataset, we use a batch size of 8 with gradient accumulation over 2 steps (effective batch size of 16) for 20k iterations. 

For data augmentation, we follow FC4~\cite{hu2017fc4} to rescale images by random RGB values in [0.6, 1.4] in the raw domain, noting that we only rescale input images since our training does not require ground truth illumination. We also apply mask color jittering to handle imprecise color checker annotations. For Laplacian decomposition, we use a two-level pyramid ($L=2$) to balance the preservation of high-frequency structural details and the suppression of low-frequency color information. All experiments were conducted on an NVIDIA RTX 4090 GPU. Additional implementation details are provided in the supplementary material.



% \paragraph{Dataset.}
% We use two publicly available color constancy benchmark datasets in our experiments: the NUS 8-Camera dataset~\cite{Cheng:14} and the re-processed Color Checker dataset~\cite{4587765} (referred to as the Gehler dataset). The Gehler dataset contains 568 original images captured by two different cameras, while the NUS 8-Camera dataset~\cite{Cheng:14} contains 1736 original images captured by eight different cameras. Each image in both datasets includes a Macbeth Color Checker (MCC) chart, which serves as a reference for the ground-truth illuminant color.

% For both datasets, we adopt a three-fold cross-validation protocol for our experimental evaluation. This ensures that the training and testing data are completely separated, reflecting the model's generalization capability to unseen scenarios.

% Since the pre-trained VAE was trained on sRGB images, we apply a gamma correction of $\gamma = 1/2.2$ on linear RGB images before encoding to minimize the domain gap. Conversely, after VAE decoding, we apply inverse gamma correction to convert the output back to the linear domain for metric evaluation.

% \vspace{-3mm}
% \paragraph{Evaluation metrics.}
% To evaluate the performance of color constancy methods, we use the standard angular error metric, which measures the angular difference between the estimated illuminant and the ground-truth illuminant. Specifically, the angular error $\theta$ between an estimated illuminant vector $\hat{\mathbf{y}}$ and the ground-truth illuminant vector $\mathbf{y}$ is defined as:
% \begin{equation}
% \theta = \arccos\left(\frac{\hat{\mathbf{y}} \cdot \mathbf{y}}{|\hat{\mathbf{y}}| |\mathbf{y}|}\right)
% \end{equation}
% The angular error is measured in degrees, with smaller values indicating better estimation accuracy. Following previous works, we report the following statistics of the angular error.

% \subsection{Implementation Details}
% Our implementation is based on the Stable Diffusion v2 framework~\cite{rombach2021highresolution} using PyTorch, following parameter settings from~\cite{garcia2024fine}. We train our models using the Adam optimizer with an initial learning rate of $5 \times 10^{-5}$ and apply exponential learning rate decay after a 150-step warm-up period. 

% For cross-validation experiments, we train for 6k iterations with batch size 4 on the NUS-8 dataset, and 13k iterations with batch size 8 on the Gehler dataset. For cross-dataset evaluation, when training on the Gehler dataset and testing on NUS-8, we use a batch size of 8 with no gradient accumulation for 12k iterations. When training on NUS-8 and testing on the Gehler dataset, we use a batch size of 8 with gradient accumulation over 2 steps (effective batch size of 16) for 15k iterations.

% For data augmentation, we follow FC4~\cite{hu2017fc4} to rescale images by random RGB values in [0.6, 1.4], noting that we only rescale the input images since our training does not require ground truth illumination. The rescaling is performed in the raw domain, followed by gamma correction. This is implemented through a 3Ã—3 color transformation matrix, where diagonal elements control the intensity of individual RGB channels (color strength), and off-diagonal elements determine the degree of color mixing between channels (color offdiag). With a probability of 0.8, we randomly crop a region containing the mask, where the crop size ranges from 80\% to 100\% of the original image dimensions while ensuring the mask remains fully visible. Additionally, we apply local transformations to masked regions only, including brightness adjustment ($[0.8, 2.0]$), saturation adjustment ($[0.8, 1.3]$), contrast adjustment ($[0.8, 1.3]$), and Gaussian noise ($\sigma \in [0, 30]$).

% For data augmentation, we apply two levels of transformations. First, we perform global color jittering on the entire image with brightness, contrast, and saturation factors of 0.5, and hue adjustment of 0.3. With a probability of 0.8, we randomly crop the image centered around the mask region, with the crop size ranging from 80\% to 100\% of the original image dimensions. Additionally, we apply local transformations to masked regions only, including brightness adjustment ($[1.0, 2.5]$), saturation adjustment ($[1.0, 1.5]$), contrast adjustment ($[1.0, 1.5]$), and Gaussian noise ($\sigma \in [0, 50]$). 

% For the Laplacian decomposition, we use a two-level pyramid ($L=2$) to balance the preservation of high-frequency structural details and the suppression of low-frequency color information.

% All experiments were conducted on an NVIDIA RTX 4090 GPU. We will make our source code and fine-tuned model weights publicly available for reproducibility.


\input{tables/agnostic}

\input{tables/cs_NUS8}
\input{tables/cs_gehler}
\subsection{Results and Comparisons}


\noindent {\bf Evaluation protocols.}
We conduct experiments under three different protocols to comprehensively assess our method's performance and generalization capabilities. 
First, following the camera-agnostic evaluation protocol from C4 \cite{yu2020cascading}, we evaluate robustness against camera sensitivity variations by training on one dataset and testing on another. Specifically, we train on the NUS-8 dataset and test on the Gehler dataset and vice versa. As shown in Table~\ref{tab:camera_agnostic}, our method achieves competitive performance compared to state-of-the-art approaches.
Second, we adopt the leave-one-out protocol from SIIE \cite{afifi2019sensor} to assess performance on unseen camera sensors by excluding images from one camera during training and testing on them. This process is repeated for all cameras, with results in \cref{tab:nus8_leave_one_out} and \cref{tab:gehler_leave_one_out} demonstrating our method's effectiveness. Both protocols highlight that our approach leverages diffusion priors to learn sensor-independent illumination features without requiring camera-specific retraining.
Additionally, we conducted standard three-fold cross-validation on both the NUS-8~\cite{cheng2014illuminant} and Gehler datasets~\cite{4587765}
. As shown in \cref{tab:cross_validation_nus8} and \cref{tab:cross_validation_gehler}, our method achieves performance comparable to other approaches, particularly in worst-case scenarios.

\vspace{3pt}  \noindent {\bf Position-aware Sampling and Consistency.}
\cref{fig:diif_region_inpaint} demonstrates our method's robustness in single-illumination scenes. Unlike prior approaches, our ability to sample at different positions and generate result ensembles enables the quantification of model consistency, showcasing our approach's precision and reliability.

\vspace{3pt}  \noindent {\bf Spatially Varying Illumination in Multi-source Scenes.}
Traditional color constancy methods typically assume a single global illuminant, limiting their applicability in complex lighting scenarios. Our method naturally extends to spatially varying illumination conditions. We evaluated this capability on the LSMI dataset~\cite{kim2021large}, which features challenging multi-illuminant scenes. By dividing each image into a 4$\times$4 grid, inpainting color checkers in each cell, and interpolating these local estimates, our method effectively models different lighting regions. Results in ~\cref{tab:LSMI} demonstrate that our approach can handle complex lighting environments without requiring specific fine-tuning for multi-illuminant data. \cref{fig:spaital_varying} visually confirms our method's ability to adapt to lighting transitions in real-world environments.

\vspace{3pt}  \noindent {\bf Computational Efficiency.}
Our method maintains efficient inference times due to its single-step design. Using an NVIDIA RTX 4090 GPU, it processes a 512$\times$512 image in 180ms, significantly faster than traditional diffusion methods requiring multiple denoising steps as shown in ~\cref{tab:model-comparison}, while preserving accuracy benefits from diffusion priors.



\input{figs/diff_region_inpaint}
\input{tables/NUS8}
\input{tables/gehler}
\input{tables/LSMI}

\input{figs/spatial_varying}


\subsection{Ablation Studies}
We conducted a series of ablation experiments to validate the importance of key design choices, including using Laplacian decomposition, noise prediction-based LoRA fine-tuning, and mask-based data augmentation in \cref{table:exp_ablation}.

\input{tables/noise_compare}

\input{tables/ablation}


\input{figs/without_finetune}

% \input{figs/spatial_varying}

\vspace{3pt}  \noindent {\bf Without Laplacian decomposition.}
Without Laplacian decomposition, we use only the VAE encoder's latent representation as input. As shown in ~\cref{fig:wo_fig}, the generated checker is contaminated by low-frequency information from the initial neutral reference, producing disharmonious colors that prevent accurate environmental color estimation.

\vspace{3pt}  \noindent {\bf With noise.}
In this experiment, we used LoRA \cite{hu2021lora} to fine-tune the SDXL inpainting model \cite{rombach2021highresolution} and obtained the final output through ensemble averaging across multiple samples. As shown in ~\cref{tab:model-comparison}, this approach underperforms our final method due to the fundamental trade-off between preserving the color checker's geometry and suppressing low-frequency information from the neutral reference checker.

\vspace{3pt}  \noindent {\bf Without mask data augmentation.}
Initially, we aligned color checkers using homography based on dataset corner locations, but imprecisions led to alignment errors at the pixel level. Our mask-based data augmentation approach eliminates reliance on specific corner positions, producing more accurate scene-harmonized color checkers that better represent the overall scene lighting.

\vspace{3pt}  \noindent {\bf Without inpainting color checker.}
In this experiment, we directly used the diffusion model to predict scene illumination RGB, not by inpainting a checker. This direct approach proves less effective than our inpainting method, highlighting the importance of color checker references for accurate illumination estimation.

