\section{CONCLUSIONS}

% The Problem of Current AI Benchmarking Practices} 
    % \item Technical Limitations: Review of Statistical Flaws and Bias
    % \item Societal Implications: Representation, Diversity, and Inclusivity Concerns
    % \item Historical and Philosophical Perspectives: Evolution of AI Evaluation
    % \item The Need for Interdisciplinary Insights Across Research Fields
% \end{itemize}

% \subsection{Guidelines Toward Trustworthy Benchmarks}

% \begin{itemize}
%     \item Lessons Learned from Current Issues
%     \item Principles for Trustworthy Benchmark Development
% \end{itemize}

% \subsection{Implications for the Future of AI Benchmarking}

% \begin{itemize}
%     \item Policy and Regulatory Considerations
%     \item Best Practices for Developers and Auditors
% \end{itemize}

% \subsection{Conclusion}

% \begin{itemize}
%     \item Recap of Key Findings on AI Benchmarking
%     \item Future Research Directions: Implementing Trustworthy Benchmarks
%     \item Final Thoughts About the Imperative of Trustworthy AI
% \end{itemize}

% Begin by mentioning that at the moment, LOTS of work is being done on improving benchmarks. Proves that the community is well aware that there are problems with benchmarks and committed to address this. 

% Rapid AI development - and the quick pace at which benchmarks have previously been outperformed or saturated - also calls for a need to interpret benchmarks scores with great caution, especially when benchmarks claim to evaluate areas such as safety and ethics. Rather than approaching high benchmark scores as proofs that AI models are actually "safe" or "ethical", it may be more wise to use benchmark scores and leaderboards as tools to identify models that severely \textit{underperform}. 

% "At a time in which Artificial Intelligence is becoming increasingly intertwined with traditionally subjective areas such as creativity, art, style, and taste, the quantitatively-focused methodologies of competitive benchmarking and evaluation metrics will inevitably require some sort of transformation"  \cite{orr2024a} p. 1882. 

% In their survey of recent trends in LLM benchmarking, Chang et.al identify a shift away from the use of static and purely quantitative metrics in the evaluation of AI models, and towards the use of tests involving humans-in-the-loop, for instance through crowd-sourcing or red teaming exercises \cite{chang2023}. Looking ahead, the authors also identify the need to perform behavioural tests of AI models in open environments, where different AI models may interact.

% Concerning construct validty issues, it is clear that the isssue is far from settled as there is a continued tendency to surround AI evaluation methods with grand promises. This can for example be seen in testing frameworks that promise to label AI technologies as "\textit{guaranteed safe} (GS) AI" \cite{dalrymple2024}, or benchmarks that promise to measure "a human-like form of general fluid intelligence" \cite{chollet2019}. 

% ME: An alternative idea here could maybe be to use this final section to adress some of the most recent and unresolved concerns that are currently listed in the cut-out section on "issues with mitigation strategies" in the issues section. This would involve taking a slightly more forward-looking approach, discussing difficulties with coming to terms with these issues, and perhaps also discussing new tendencies like using AI to produce benchmarks - a topic that we briefly touch on in the issues section but could perhaps be given more space and critical attention}. 

Measuring the capabilities and risks of AI models and systems is difficult and one of the main challenges in the use and development of AI. Even with the best of intentions (such as disclosing discrimination or identifying potential societal harms), previous research has repeatedly shown that quantitative AI benchmarks struggle to perform their intended task. Benchmarks have been found to promise too much \cite{raji2021}, be gamed too easily \cite{weij2024, narayanan2023a}, measure the wrong thing \cite{oakden-rayner2019}, and be ill-suited for practical use in the real world \cite{bao2022a, ethayarajh2021}. They have also been found to display a serious lack of documentation \cite{reuel_betterbench_2024}, randomly reach an unjustified status through community vetting \cite{dehghani2021}, and forward questionable cultural assumptions \cite{kang2023, keegan2024}, that for example ignore environmental concerns \cite{hutchinson_evaluation_2022}. Furthermore, benchmarks have been critiqued for being narrow and mainly evaluating English \cite{mcintosh2024, rottger2024} and text-based AI models \cite{rauh2024} according to a one-time testing logic \cite{mizrahi2024} that ignores the AI capabilities in other modalities such as imagery and sound \cite{rauh2024}, and fails to acknowledge that the potential harms of AI models cannot be properly understood through evaluations done in isolated, abstracted, test-environments, devoid of humans \cite{rauh2024} and other technical systems \cite{birhane2024}. Taken together, these issues point toward fundamental fragilities in current efforts to quantitatively measure and mitigate harm in AI. 

% As is well known in fields such as IT security, the question of whether a system is "safe or not?" relies on a false dichotomy. All systems are unsafe depending on adversary and context. With that in mind, it is crucial to recognise that conducting AI benchmark tests is always a balancing exercise rather than an absolute, and adjust any safety measures accordingly.

Cars, air planes, medical devices, drugs, and numerous other products within out societies comply with strict regulation to ensure their safety. There is no reason to believe that similar safety assurances can not be developed for AI and the intensified interest in AI benchmarks clearly signal a drive to do so. Outside the scope of this meta-review, we have identified numerous papers that propose strategies for mitigating  issues with benchmarks, for instance by hiding benchmark training datasets \cite{chollet2019} or using so-called "dynamic benchmarks" \cite{besen2024} to counter gaming and data contamination risks, aggregating evaluation tasks into single multi-task benchmarks to increase the reliability of test results \cite{srivastava2023, liang2023}, or opting for evaluation methods that directly involve human interrogators as opposed to quantitative metrics \cite{chang2023}. An increasing number of researchers have also proposed new and promising frameworks for assessing and "benchmarking the benchmarks" \cite{miltenberger2023, reuel_betterbench_2024}. Because of uncertainties regarding the effectiveness \cite{zhang2024, arzt2024, rauh2024} and widespread adoption of such mitigation efforts, however, our meta-review suggests that the field of quantitative benchmarking is currently ill-suited to single-handedly (or primarily) carry the burden of providing the assurances requested by policy makers. Our review also shows that from a policy perspective, relying on indicators such as citation counts to determine what benchmarks to trust is insufficient. We identify a strong incentive gap in the use of benchmarks between academic researchers (who may for example primarily be interested in methods development and are submerged in the "fast" logic of academic publication), corporations (who have strong incentives to use benchmarks for economic interests), and regulators (who have a particular responsibility to consider the practical utility and potential downstream effects of particular benchmarks). From a regulatory perspective - and the perspective of anyone who wants to apply a benchmark to a concrete, real-life case - we especially identify a need for new ways of signalling \textit{what benchmarks to trust}. We do not necessarily need standardised benchmark metrics and methods. But we do need standardised methods for \textit{assessing the trustworthiness of benchmarks from an applied and regulatory perspective}. 

%\textbf{Disclaimer} The views expressed in this article are purely those of the authors and may not, under any circumstances, be regarded as an official position of the European Commission.


% In line with previous research, we further suggest that more research is needed concerning benchmarks that reveal \textit{how} and \textit{when} AI models fail \cite{reuel_betterbench_2024}, benchmarks that root and compare benchmark scores to human norms \cite{}, and benchmarks that are resilient to issues such as data contamination, correlation with downstream capabilities, and gaming attempts such as sandbagging. We also need... ?

% As analyzed in the presented paper, a substantial and increasing amount of research is focused on enhancing AI benchmarks, highlighting the community's keen awareness of prevailing issues and dedication to solving them. Most of this work has emerged in the last few years, specifically since 2023, demonstrating an agreeable urgent effort to improve benchmarking practices and methodologies. This increase in research activity emphasizes the AI community's shared understanding of the essential role of transparent and dependable benchmarks in advancing AI technologies.

% The existing AI benchmarking landscape presents a complicated mix of challenges and opportunities that demand strategic actions. A significant issue is the datasets' insufficient management and transparency, which weaken the reliability and applicability of AI benchmarks. This problem is aggravated by the frequent occurrence of misleading patterns and overfitting due to shortcuts taken in data handling. These elements highlight the need for the creation of rigorous documentation standards and consistent evaluation procedures to maintain the accuracy and reproducibility of benchmark outcomes.

% Moreover, the competitive and commercial nature of benchmarking has fostered a culture focused predominantly on achieving state-of-the-art results. This emphasis on benchmark scores often detracts from addressing more substantive, practical applicability and genuine progress in AI capabilities. The rapid pace of AI technological advancement further exacerbates this issue, as many benchmarks quickly become obsolete, highlighting the need for dynamic adaptation in evaluation methods.

% From a policy and regulatory perspective, enforcing transparency and thorough documentation of benchmark datasets is essential. These actions are vital for maintaining accountability and reproducibility. Ethical factors such as privacy, consent, and bias should be incorporated into the regulatory frameworks that oversee dataset usage. Given the considerable impact benchmarks have on AI development paths, regulators must encourage a variety of evaluation metrics that represent societal values and alleviate possible harms.

% Key findings suggest that many benchmarks suffer from construct validity issues, failing to accurately measure their intended objectives. This can lead to misleading conclusions about AI capabilities. Furthermore, there is a notable sociotechnical gap where benchmark results often do not translate into practical utility for end-users and stakeholders. This gap underscores the need for evaluations that are pertinent to real-world applications. The cultural and competitive dynamics surrounding current benchmarking practices further complicate their potential to drive meaningful AI advancements.

% The necessity for trustworthy AI is clear as these systems increasingly integrate into critical societal functions. Benchmarks must be reliable, valid, and ethically sound, encompassing assessments of capabilities, safety, fairness, and societal impacts. Developing transparent, inclusive, and dynamic benchmarks is essential to ensure alignment with human values and accountability in diverse contexts.

% Future research must develop thorough benchmarking frameworks that encompass diverse modalities, such as text, audio, images, and multimodal systems. Long-term and comprehensive evaluation methods are essential to assessing AI systems over time in real-world scenarios. Addressing these factors is crucial for advancing the field towards benchmarks that genuinely reflect the complexity and capabilities of AI technologies. This strategy will enhance the reliability of AI systems and ensure their alignment with societal needs and ethical standards.

% \an{Some of the things that I think we can incorporate into this discussion/conclusions section: There is a misalignment of incentives to make benchmarks measure and do what they should be doing (assuming they can even do that) as industry and many academics that work on it gravitate towards certain outcomes driven by their ecosystems' incentives structures thereby producing benchmarks that do not measure what they are supposed to be measuring and pushing certain benchmarks into the forefront with little ecological validity, as evidenced by the numerous types of issues that have been covered in this meta review. At the same time, the dominance of a few "bad" benchmarks helps these players themselves rather than achieve safety goals. Conversely a fragmented benchmark landscape to deal with the dominance of a few "bad" benchmarks also does not solve the incentives issue as this allows for cherry picking.  
% (2) In such a context, technology governance through various means including regulatory approaches would need to align incentives, by for instance placing liability on the entities developing potentially harmful technology to make them internalize the costs of the harm rather than letting others bear the costs of dealing with the consequences of the technology. Cars, air planes, medical devices, drugs, and numerous other products within out societies comply with strict regulation to ensure their safety.   
% In the context of AI, and given the issues that we have reviewed with (quantitative) AI benchmarks, there is a large risk however that existing regulation (including AI act ) has given quantitive benchmarks, with all their flaws, too much of a central role with less attention for the incentives that currently feed into the benchmark ecosystem and leaving some of the very important questions related to benchmarks quite open.
% (3) The current context risks, ... (what risks do we see?)}

% One solution to the limitations of individual benchmarks has been attempts to aggregate different evaluation tasks into single multi-task benchmarks with the hope that this will increase the reliability, validity, and representativeness of model evaluations. Examples include BigBench \cite{srivastava2023} and HELM \cite{liang2023}. However, recent research has shown that such attempts may come with drawbacks, such as having to make important decisions concerning trade-offs between diversity and robustness/sensitivity \cite{zhang2024}. Zhang et.al also conclude that BigBench and HELM are "highly unstable to irrelevant changes" and suffer from significant instabilities with regards to rankings (ibid.). Likewise, Artzt et al. find that multitask benchmarks and leaderboards presenting aggregated benchmark statistics with the help of singular metrics (such as F1-score) often "fail to capture the complexity of the relation extraction task, especially in scenarios involving a large number of labels and highly imbalanced datasets" \cite{arzt2024}. 

% Another solution to the limitations of current benchmark practices has been the introduction of so-called dynamic benchmarks, which are designed to be updated and adjusted over time to mitigating risks associated with data saturation, data contamination, and rigging/gaming \cite{besen2024, shirali2023}. However... Shirali note that... Efforts to produce dynamic benchmarks increasingly also involve using AI models to slightly adjust and update benchmark datasets to make them more difficult to solve \cite{wang2024}. Elsewhere, experiments have been made with using AI models as adversarial agents in the safety testing of other AI models \cite{perez2022}, and to identify and classify harmful AI output \cite{inan2023}. While such efforts may lower the time and cost of conducting AI safety tests, the use of additional AI tools in AI model evaluation may result in making it increasingly difficult to explain evaluation results, allocate accountability, and ensure that errors and biases do not propagate throughout the evaluation process. 

% Another way of solving ixing train-test overlap and data contamination by hiding benchmark datasets (following and ARC-AGI model), although this brings transparency issues... what would it mean if all benchmark datasets were hidden from view?

% While some scholars argue that method of closing gaps in existing AI safety evaluations could involve repurposing benchmark evaluations and datasets (for instance, by transcribing non-text output to make use of the wide range of text-based benchmarks that exist) \cite{rauh2024}, research that reveals that most existing benchmarks display serious weaknesses with regards to documentation, transparency, and replicability \cite{reuel_betterbench_2024} suggests that this may be directly unwise.

% Rauh et al. also emphasize that "evaluation alone is not a panacea" and that the answer to benchmark shortcomings should not just be to develop more or "better" benchmarks, but also a more flexible AI design that allow for continuous system updates, alongside efficient governance mechanisms \cite{rauh2024}

% Moving forward, Chang et al. notice a shift towards using humans-in-the-loop during testing, crowd-sourced and dynamic designs of benchmark datasets, and centralized testing environments where multiple benchmarks are gathered and applied \cite{chang2023}. 
