\section{BACKGROUND} 

Etymologically, the term "benchmark" has its roots in land surveying, where a physical mark (known as a ‘bench mark’) was used as a reference point for measuring elevations. This mark typically consisted of a horizontal groove in a surface, which supported a level surface or 'bench' for a levelling rod. Over time, the term evolved to encompass a broader meaning, referring to any standard or reference point used for comparison or evaluation \cite{oed}. 
Benchmarking is currently applied across many different domains such as \an{I would like to cite some of my own PhD work here on benchmarking in cybersecurity here as part of the background - See my PhD Thesis for instance} bioinformatics \cite{Aniba2010}, environmental quality \cite{Henning2000}, information retrieval \cite{thakur2021beir}, transistor hardware \cite{Cheng2022}, industry and business \cite{Camp1989}, and within the public sector \cite{Bruno2014}, where it often refers to procedures for comparing the performance or best practices of different companies or processes. % ME: I suggest we cut this out to shorten the intro/background section, since this is discussed later in the paper: The widespread use of benchmarking across many fields gives us clues about its importance, not only as a tool for measuring and comparing performance, but also as a mechanism for steering research communities toward common goals. From a certain level of progress in a specific field, with a large community of players, processes, and systems, there is a need for standardised methods and metrics to compare performance homogeneously across the field, going beyond the fragmented ecosystem of customised and individual evaluations of each player separately. 
% ME: I would disagree with this final sentence and argue that we do not need standardized benchmark metrics and methods. On the contrary, a varied and hetrogenous pool of benchmarks is probably more desireable since no benchmark can cover everything and benchmark variety complicates rigging and gaming etc. etc.. But we absolutely do need standardized methods for \textit{assessing the trustwortiness of benchmarks}, and this is what I think we should emphasize here. 
% ME: also cut out in the interest of space: \subsection{AI Benchmarks} 
% As AI models and systems grow in complexity and ubiquity, the need for comprehensive benchmarking becomes increasingly important.
% ME: I would rephrase, specify, and not express this as an absolute truth: "As AI models and systems grow in complexity and ubiquity, it is generally believed that there is an increasing need for quantitative benchmarking." The fixation on quantitative benchmarking is precisely what many of the papers we discuss here critique, so my approach would be to treat this as an common opinion, belief, or idea, rather than a fact.
% AN: I agree with ME, I would even be somewhat provocative and point to the actors that are influencing this, (rather than passively say "it is generally believed"). Suggest repharasing to something like: "As AI Models and systems grow in complexity and ubiquity, numerous AI practioners and more recently policy makers have subscribed to the idea of an increasing need for quantitative benchmarking."
Here, we focus on the use of benchmark tests within computing, where they are used to evaluate the performance of hardware or software systems by comparing them to a standard or reference point \cite{Henning2000}. More specifically, we zoom in on AI development, where benchmarks are often used to facilitate cross-model comparisons, measure performance, track model progress, and identify weaknesses \cite{reuel_betterbench_2024}.  
Benchmarks are often perceived as comparably cost- and time-effective tools for AI providers who may run them regularly throughout the model development to obtain frequent signals of AI model performance and capabilities \cite{weidinger_sociotechnical_2023}. They can be applied to both software and hardware solutions, where the latter for instance evaluates the performance of CPUs, GPUs, TPUs \cite{Mattson2020}, or hardware accelerators \cite{Mattson2020b}. We address software-oriented benchmarks. Based on the definition proposed by \citet{raji2021}, we define such benchmarks as “a particular combination of a set of test datasets, including human-in-the-loop interactions, and associated performance metrics, conceptualised as representing one or more specific tasks or related capabilities, typically chosen by a community of researchers as a shared framework for comparing AI models”. A \textit{testing dataset} would refer to “a separate dataset, distinct from the training data, used to objectively evaluate the performance and generalisability of an AI model”. These datasets are typically composed of \textit{samples} or \textit{instances} that include an input paired with the desired output (also known as reference, gold, or ground truth). A \textit{task} would be “a particular specification of a problem, typically represented as a mapping between an input space and an output or action space, extensionally defined through one or more datasets, and usually associated with a specific performance evaluation metric” (based on \cite{raji2021} and \cite{schlangen_targeting_2020}). And finally, a \textit{metric} would refer to "a specification of the mechanism to determine the degree of success or failure of the model's outputs". The metric represents a way to summarise model performance over a given task and dataset, usually defined as a single number or score \cite{raji2021}. AI models that achieve the best scores on the metrics of a benchmark are generally considered SOTA in terms of performance. 

While humans are always involved in the design and creation of ground truth in benchmarks, they can play a more or less direct and active role in the process of applying benchmark tests. In \textit{automated} or \textit{quantitative benchmarks}, a set of tasks, datasets, and metrics are first defined through human decision making. The execution of a benchmark test is then carried out without direct human intervention. In \textit{qualitative} benchmarks, humans directly intervene and partake in evaluations, for example as evaluators, judges, or real-time interrogators (e.g., adversarial testing or red teaming). We primarily consider quantitative benchmarks. This is not to say that there is no relevant critique concerning qualitative AI, but such methods introduce a different set of critique that is out of scope for our discussions here. 
% As AI Models and systems grow in complexity and ubiquity, numerous AI practitioners and more recently policy makers have subscribed to the idea of an increasing need for quantitative benchmarking \cite{AIA24}. Such benchmarks are for example focused on evaluating training data according to diversity, biases, or representativeness (e.g., DataPerf \cite{Mazumder2023} or DataComp \cite{Gadre2024} approaches). In parallel to the increased use of such benchmarks, there has also been a growing concern about the risks arising from quantitative attempts to measure AI risks and capabilities.

% ME: I suggest we cut this out too, in the interest of space: In this context, the ImageNet dataset and its corresponding challenge, the ILSVRC, serves as one of the earliest and most influential - but also most controversial - examples of AI benchmarking \cite{Russakovsky2015}. The scale of the dataset, in terms of the number of images and categories, was unprecedented at the time. Through the annual challenge, held from 2010 to 2017, the benchmark facilitated one of the most significant breakthroughs in the Machine Learning field, the introduction of Deep Learning, which led to substantial reductions in error rates year after year, as well as unprecedented advancements in object recognition. Since then, the significance of benchmarks in the field of AI has continued to grow in scale, importance, and influence, being particularly relevant in the current context of general-purpose AI models. % ME: perhaps it should also be mentioned here that ImageNet and the ILSVRC are one of the most notorious and fiercly critiqued datasets and benchmark competitions in terms of bias (ethnic, gender, you name it) and that it, in many ways, materializes most of the critique against benchmarks that are adressed in the remaining parts of this paper. For more on this, see f.ex.:

% Crawford, Kate, and Trevor Paglen. “Excavating AI: The Politics of Images in Machine Learning Training Sets.” AI & SOCIETY, September 19, 2019. https://doi.org/10.1007/s00146-021-01162-8.

% Denton, Emily, Alex Hanna, Razvan Amironesei, Andrew Smart, and Hilary Nicole. “On the Genealogy of Machine Learning Datasets: A Critical History of ImageNet.” Big Data & Society 8, no. 2 (July 2021): 1–14. https://doi.org/10.1177/20539517211035955.

% Luccioni, Alexandra Sasha, and Kate Crawford. “The Nine Lives of ImageNet: A Sociotechnical Retrospective of a Foundation Dataset and the Limits of Automated Essentialism.” Journal of Data-Centric Machine Learning Research, 2024.

% AN: Suggest to also mention that later versions of such datasets are also strife with problems e.g. by citing: https://arxiv.org/abs/2110.01963 which I added to the Zotero library under a "context" folder.

% \subsection{Terminology}

% Currently, the most prominent type of AI benchmarks are quantitative ones that focus on evaluating model performance on a test dataset which includes a proxy of human capabilities and/or knowledge. Recently, and in parallel to the advancement of the capabilities of GPAI models, there has also been a growing concern about the risks arising from the so-called high-impact capabilities of these models \cite{AIA24}. This has boosted a new trend in benchmarking, focusing on safety evaluations \cite{rottger2024safetyprompts}. 

% Cut out too for space reasons: It is important to note that the concept of \textit{capability} is at a higher level of abstraction than the concepts of \textit{task} and \textit{performance}. For example, whereas \textit{performance} can be understood as a function of both the model and the distribution of tasks, a \textit{capability} would be a property of the model that informs whether the model is “capable” of succeeding on tasks that demand that capability (in that sense, the difficulty of the task plays a relevant role \cite{Martinez-Plumed2020}). Therefore, the quality of a benchmark task is generally associated with how effectively it contributes to evaluating a specific capability and how well it distinguishes between models based on this capability \cite{raji2021}. % Effective evaluation of capabilities is still an open research question.

% ME: we can absolutely discuss this further, but my suggestion would be to also cut this out, since distinctions between narrow and ganeral-purpose AI is not an issue we address anywhere else in the paper: Another important clarification is the distinction between \textit{narrow AI} models, which are designed to handle a single task, and \textit{general-purpose AI} (GPAI) models \footnote{We use the term general-purpose AI (GPAI) model in line with the EU AI Act \cite{AIA24}. It includes other terms such as foundation model, generative AI model, large language model, etc.}, which are capable of handling a wide range of distinct tasks, demonstrating some \st{sort} \an{notion?} of generality across domains. Benchmarks for narrow AI models are typically tailored to the task for which the model has been trained. On the other hand, benchmarks for GPAI models are usually more comprehensive, covering a broad range of tasks and, in some cases, a heterogeneous set of metrics (e.g., when the output structure varies among different tasks). In this paper, we primarily focus on benchmarks for GPAI models, although many of the elements analysed are also applicable to the context of benchmarks for narrow AI models.

%AN: Another example that can be cited is potentially the following paper which we have in zotero but is not in the core set of papers https://arxiv.org/abs/2404.12241

% ME: I've moved the following part to the issues section on path dependency, since they otherwise forgoe some of the critique that will later be brought up: Indeed, when benchmarks are well established in a specific AI community, they do not only serve to measure performance, but influence it, since they shape how communities approach progress. This is known as the "benchmark effect" \cite{Stewart2023}, which refers to the tendency of developers to focus solely on optimising their models to perform well on the benchmarks, often at the expense of broader, more generalizable progress. 

% ME: I also moved parts of this text to the issues section on competitive and commercial roots, since it repeats what is otherwise discussed there: Considering the crucial role of AI in the economy and the ongoing competition for market dominance, benchmarks are widely used by AI companies to advertise their GPAI models and systems in comparison to their competitors. It is very common to use comparative tables showing the performance of different models across various benchmarks, with bold values to highlight the superior performance of the advertised GPAI model. In some cases, providers make grandiose claims about their model's capabilities based on performance metrics from benchmarks (e.g., “Our Largest and Most Capable AI Model” \cite{grill2024}). This trend extends beyond market competition at the level of individual companies and also enters the realm of geopolitics, where benchmark performance is used to compare models from different countries and regions \cite{Zhijia2024} as part of the global race for AI leadership \cite{globalAI2024}. Furthermore, the performance metrics obtained by the models on different benchmarks are used to measure the gap between open-source and closed-source models\footnote{We acknowledge that the distinction between open and closed GPAI models is not as simple as it might appear. The term open-source is typically used to encompass a variety of access options for the different AI model components \cite{Seger2023}.}, with significant connotations, especially for the open-source community.

% ME: This paragraph seems pretty detailed and slightly off topic - perhaps we could clarify a bit more how this is central to the role that benchmarks currently play, move it elsewhere (such as the section on path dependence), or delete it, in the interest of space: Benchmarks are also influencing the development of GPAI models beyond just performance evaluation during training. They also play a key role in predicting performance on downstream tasks concerning training compute (i.e., FLOPs) before pre-training. This prediction uses the performance values of smaller models and is based on scaling laws \cite{Hoffmann2022}. In this way, providers can optimally decide on the final compute budget allocated before pre-training their largest models \cite{llama3herd}, \cite{sun2024hunyuanlarge}, which is a critical decision given the substantial costs associated with this process \cite{cottier2024}. Moreover, recent observational studies have demonstrated the possibility of predicting the impact of post-training interventions in advance by using data from benchmarks and compute measures \cite{Ruan2024}.

%Definition of AI benchmarks We define AI benchmarks as xxx. A benchmark generally consists of a \textit{dataset} and a set of \textit{metrics} that are used to test the capacities of an AI model on a specific task and is geared towards comparing the performance of different models. When discussing benchmarks, we refer to performance tests that are both aimed to evaluate the capability and the safety/ethics of AI models. % This final sentence needs a bit more motivation and elaboration though, since some researchers have argued that there is a danger in not distinguishing between capability and safety benchmarks, see f.ex. \parencite{ren2024}

%While several benchmarks combine human and algorithmic assessment methods, we only consider quantitative benchmarks \textit{without} humans in the loop in this review. This is not to say that AI evaluations involving direct human assessments are unproblematic, but they bring a different set of challenges that are out of scope for our purposes here.  

%Locate benchmarks within a longer history of using proxies in general, and proxies in the particular context of safety assessments.

% Address the role of benchmarks in the current AI landscape, for example in relation to observational scaling laws to define compute, model size and data (see https://arxiv.org/html/2405.10938v1; example here: https://arxiv.org/abs/2411.02265).

% Contextualize benchmarks within the broader legal and political push to invest trust and resources in quantitative AI metrics. Mention the role of benchmarks and algorithmic audits within the DSA, AI act and similar regulatory frameworks (AI sandboxes, risk assessments and classifications etc.). Connect to work on fair, trustworthy, explainable, and transparent AI.

%Mention benefits with using quantitative benchmarks, compared to other evaluation methods. From an industry perspective, for example, it has been highlighted quantitative benchmarking are relatively cost-effective and time-efficient, allow for cross-model comparisons, facilitate reliable evaluations through retesting, carry the capacity to monitor model progress over time and the course of training, and open up for wide-ranging abilities to customize tests according to needs \parencite{weidinger2023}. However, those benefits only hold if the benchmark in question is reliable, fair, trustworthy, and transparent.

%Address how benchmarks are similar to - and different from - training datasets for AI. For instance, while benchmarks are used to evaluate machine learning models, many are based on sequential algorithms instead of machine learning. 

%Benchmarks are important because they represent key ways through which AI models are made sense of. For instance, they help shape understandings concerning what AI technologies can (and/or \textit{should}) do, help define what is considered safe and unsafe, moral and immoral, true and false, toxic and healthy. This means that they hold a significant cultural power.

%Importantly, benchmarks also play a role in advertising AI models and guide AI model development and improvements. In this sense, benchmark tests - like tests in general - are \textit{performative} and \textit{generative} \parencite{pinch1993, marres2020} - they do not passively describe and measure how things are in the world, but also actively take part in shaping it by influencing how AI models are trained, fine-tuned, and applied \parencite{grill2024}.

%Quantitative benchmarking is arguably the most widely used method for evaluating the capacity and safety of AI models, but it is not the only way to test and assess the performance of AI. Contextualize quantitative benchmarks within a broader set of techniques for evaluating machine learning systems (incl. methods with humans in the loop, such as various adversarial tests, red teaming etc.).

% Developing transparent and trustworthy AI benchmarks is not just in the interest of researchers, policy makers, and the wider public. Notably, there are also significant economic gains to be made by improving insufficient reporting of AI safety since research suggests that currently, "no high-risk AI system could be developed on top of...  [existing] GPAI models" due to their failure to comply with the EU AI Act's demands concerning documentation and transparency \parencite{guldimann2024} p.~16.


% \fi