\section{NINE REASONS TO BE CAUTIOUS WITH BENCHMARKS} 

In the sections below, we summarise the main issues that were identified during the course of our research. Importantly, these issues - presented here as a taxonomy - are not arranged according to their importance or urgency. They are also not meant to be understood as isolated issues, but rather deeply interlinked problems. Indeed, this complexity and interdependence is precisely what makes AI evaluations challenging.

% Begin by mentioning a couple of overarching remarks and insights from our study as a whole, for instance regarding a "first wave" of benchmark critique with strong focus on datasets and transparency around 2021-2022, and then a "second wave" focusing on technical demonstrations of benchmark shortcomings in 2023-2024. 

% Also maybe say something over-arching about how people have mainly approached benchmark critique from different fields (computer science, cybersecurity, anthropology etc.). 

\subsection{Problems with Data Collection, Annotation, and Documentation}

An initial set of issues with AI benchmarks found during our research is limitations in the collection, annotation, and documentation of benchmark datasets. This ties into a broader critique regarding insufficient documentation in AI research which is central to calls for more transparent and trustworthy algorithmic systems~\cite{gebru2021, mitchell2019, orr2024b, simson2024, scheuerman2021}. Research has found that it is often difficult to trace precisely \textit{how}, \textit{when}, and by \textit{whom} benchmark datasets have been made \cite{reuel_betterbench_2024, denton2020} which compromises the ability for benchmarks to be used in robust and generalisable ways \cite{arzt2024}. The issue has partly been linked to the low status of dataset-related work within the machine learning community (which instead privileges model development) \cite{orr2024, sambasivan2021}, and the fact that AI datasets are often “reduced, reused, and recycled” \cite{koch2021}, which complicates documentations of their possible limitations \cite{thylstrup2022, park2022}. Notably, \citet{koch2021} have found that more than 70 percent of the benchmark datasets used in prominent computer vision papers had been reused from other domains.
\citet{park2022} also find that while benchmark datasets often contain ample information on how to use the dataset, documentation is often missing concerning their shortcomings and social impact. 
In their exploration of benchmark sharing platforms like HuggingFace\footnote{\url{https://huggingface.co/}. Last seen \today.} and PapersWithCode\footnote{\url{https://paperswithcode.com/}. Last seen \today.} the same researchers also note that confusing metadata terminology severely complicates efforts to understand benchmark dataset documentation. 

Like AI training datasets, benchmarks have been singled out for raising ethical and legal questions concerning copyrights, privacy, informed consent, and rights to opt-out \cite{paullada2021}. Many benchmarks rely on crowd-sourced or user-generated content from platforms like Wikihow\footnote{\url{https://www.wikihow.com/Main-Page}. Last seen \today.}, Reddit\footnote{\url{https://www.reddit.com/}. Last seen \today.}, or trivia websites \cite{keegan2024, grill2024}, whose annotation may be noisy and biased, lack input from expertise in specialised fields, or be produced under exploitative conditions \cite{tsipras2020, aroyo2015, sen2015}. Previous research has noted that this matters when baselines for "good", "bad", and "safe enough" AI models are calibrated in sensitive contexts \cite{rauh2024, grill2024}. Moreover, it is essential to highlight the absence of human performance references and difficulty rubrics in benchmarks, which are increasingly important factors in evaluating capabilities and generality \cite{chollet2019}.

A lack of care in the making of benchmark datasets has further been found to result in AI models exploiting unknown quirks and spurious cues in the training data, rather than solving their original intended task \cite{liao2021, paullada2021, geirhos_shortcut_2020}.%a problem that overlaps with the notorious problem of \textit{data leakage} \cite{kaufman_leakage_2012}.
For instance, \citet{oakden-rayner2019} found that an X-ray image classification model predicted collapsed lungs with high accuracy, although it turned out the model only identified the presence of a chest drain (used to cure the condition) that was (unknowingly) present in a majority of the positive training images. In other words, the model performed well in benchmark tests, but the reason why completely sidestepped the original purpose of the task (identifying collapsed lungs). When the all images containing chest drains were removed from the training dataset, the model performance dropped by over 20\%.
Similar patterns have also been confirmed in recent research on LLM evaluation \cite{pacchiardi2024} and is sometimes discussed as failure in the "world model" of an AI system \cite{vafa2024}. % ME: cut out for reasons of space: For instance, Pacchiardi et al. trained simple logistic regression models on 1- and 2-grams extracted from the prompts of well-known multiple choice question benchmarks, and found that i) such simple models yield comparable accuracy to LLMs, and ii) LLM performance is correlated to the performance of their simpler models, suggesting that LLMs may rely on the same cues \cite{pacchiardi2024}. 
While it could be argued that picking up on spurious cues is a sign of intelligence and therefore a capability worth noting, the issues above highlight how a lack of in-depth attention to the data that benchmarks rely on - alongside a failure to acknowledge how little is actually known about \textit{how} and \textit{why} AI models perform well in benchmark tests - can produce frail and uncertain AI evaluations.


% One benchmark that has received especially harsh critique for poor data quality is the Massive Multitask Language Understanding (MMLU) benchmark, which is one of the world's mose widely used frameworks for evaluating the capacities of language models \cite{gema2024, reuel_betterbench_2024}. MMLU consists of multiple choice questions organized into 57 subject categories that range from "abstract algebra" to "world religion". The MMLU dataset was "manually collected by graduate and undergraduate students from freely available sources online" (unclear precisely when, by whom, and according to which instructions) and has been found to include questions that are morally problematic, inaccurate, outdated, and unanswerable \cite{erenrich2023, keegan2024, gema2024}. For instance, identified errors in MMLU include subjects being described in highly Western-centric ways, a lack of specificity regarding juristictions in law-related questions, and questions being parsed in the wrong way \cite{gema2024}. Reuel et al. also found that MMLU scored lowest in their in-depth quality assessment of 24 state-of-the-art language model benchmarks, despite the fact that recent models like GPT-4, Claude-3, and Gemini were all released and tested against it \cite{reuel_betterbench_2024}. 

% Recently, attempts have been made to improve MMLU by releasing updated versions such as MMLU-Redux \cite{gema2024} and MMLU-Pro \cite{wang2024}. MMLU-Pro complements the original MMLU benchmark with questions from a new set of sources (STEM, SciBench, and TheoremQA). All questions in the dataset have also been reviewed by human experts, although it is still unclear precisely by whom, when, where, and how. It is also noteworthy that MMLU-Pro exclusively relies on original questions from MMLU in areas concerning law, health, history, philosophy and "other." Interestingly, the language models GPT4-Turbo and Gemini-1.5-Pro were also continuously used during the data processing for MMLU-Pro, for instance to extract short answers from solutions, convert problems into multiple-choice questions, and perform "quality control" and "option augmenation" (whatever that means). The decision to involve one AI model in the evaluation of another is indicative of a broader trend where AI technologies are increasingly integrated into AI benchmarking pipelines - an issue which raises concerns about possible issues with overfitting, alongside new and additional problems regarding transparency, explainability, safety, and trustworthiness.


\subsection{Weak Construct Validity and Epistemological Claims}

Another genre of benchmark critique focuses on the epistemological claims that tend to surround benchmarks and examines the limits of what can be known through quantitative AI tests. A central reference point in these discussions is the observation by \citet{raji2021} that many benchmarks suffer from construct validity issues in the sense that they do not measure what they claim to measure. As the authors proclaim, this is especially troublesome when benchmarks promise to measure universal or general capabilities, since this vastly misrepresents their actual capability. As a result, the authors argue that framing an AI benchmark dataset as general purpose "is ultimately dangerous and deceptive, resulting in misguidance on task design and focus, underreporting of the many biases and subjective interpretations inherent in the data as well as enabling, through false presentations of performance, potential model misuse".~\cite[p.~5]{raji2021}. At the heart of this critique lies the realization that many benchmarks do not have a clear definition of what they claim to measure, which makes it impossible to measure if they succeed in the task or not \cite{blodgett2021, bartz-beielstein2020}. In a close analysis of four benchmarks used to evaluate fairness in natural language processing (StereoSet, CrowS-Pairs, WinoBias, and WinoGender), \citet{blodgett2021} for example found that all benchmarks revealed severe weaknesses in terms of defining what is being measured. For instance, culturally complex and highly contested concepts like "stereotypes" or "offensive language" were left unspecified, causing a series of logical failures and interpretational conflicts. Elsewhere, research has shown strong disagreements in how benchmark tasks are \textit{conceptualised} and \textit{operationalised }\cite{subramonian2023}, and found that benchmarks are applied in highly idiosyncratic ways \cite{rottger2024}. Frequently, the difficulty defining what benchmarks evaluate persist since there is no clear, stable and absolute ground truth for what is claimed to be measured \cite{narayanan2023}. Since concepts like "bias" and "fairness" are inherently contested, messy, and shifting, benchmarks that promise to measure such terms will inevitably suffer from an "abstraction error" that produces a false sense of certainty \cite[p.~63]{selbst2019}. 

It has also been pointed out that many benchmarks datasets are inadequate and/or unuseful proxys for what they are meant to evaluate. For instance, researchers have identified a slippage in distinguishing between algorithmic "harms" and algorithmic "wrongs" when evaluating the capabilities of AI models - two not necessarily overlapping concepts \cite{diberardino2024}. Others have questioned whether the content of benchmark datasets are reasonable substitutes for the "real world" scenarios they are meant to reflect. For instance, researchers have 
questioned the decision to use examples generated by Amazon crowdworkers and posts from the Reddit forum "\textit{Am I the asshole?}" as proxies for ethics and morals in benchmarks such as HellaSwag - a widely cited benchmark for language models \cite{keegan2024}. In benchmarks consisting of professional exams, researchers have further argued that such tests "emphasize the wrong thing" and "overemphasize precisely the thing that language models are good at" and are thus unreliable measures of things such as medical or legal skill \cite[n.p]{narayanan2023}.
As \citet{narayanan2023} put it, % professional exams are very different from "the real-life problems that professionals face
"it's not like a lawyer's job is to answer bar exam questions all day". 
A recent study by \citet{ren2024} also found that many widely used safety benchmarks (including ETHICS, TruthfulQA, GPQA, QuALITY, MT-Bench, LMSYS Chatbot ARENA, ANLI, AdvGLUE, and AdvGLUE++) 
highly correlate with general and upstream model capabilities, raising concerns regarding “safetywashing” as applying them could imply that “capability improvements are misrepresented as safety advancements". While it could be argued that capability and safety are largely entwined (the more capable a model is, the higher the likelihood it could cause harm), \citet[p.~1]{ren2024} suggest that a blurred distinction between the two may hide the fact that severe biases and safety issues in AI models can persist, even as their overall capabilities improve. 

\an{With respect to issues regarding construct validity, and in particular what is discussed in the last paragraph, I feel like there is still a fundamental question that is not addressed in our text: While capability and safety are indeed connected, the thing that I feel is missing is that from a safety perspective there is on one hand the capability of the model, and on the other the capability of the agent/person/entity/adversary who is given the model. Certain entities have a higher capbility of bringing about undesired behaviour from a given model versus others, and this third element (which in typical security/safety literature and is generally referred to as the adversary) is another important epistemological element that is missing from most benchmarking. I haven't had the opportunity to study all the core literature but if there are discussion of a lack of attention to the "adversary" and assumptions around their capabilities that underpin certain benchmarks, I think we should highlight them here. If not, I highly suggest adding a final catch-ll-other-issues subsection in which we can touch upon some of the issues that we know exists. This would also give us a nice logic for discussing some of the literature that was left out and also connect back to and justify our own claim or drawing from and adding value from our  interdisciplinarity. Alternatively we could also do this in the discussion/conclusions as well but I always find it odd to add citations and references that haven't been discussed earlier in the text to a discussion/conclusions section, so ideally we could do it earlier.}
\me{I am not 100 percent sure if this is what you are referring to here, but the problem that humans are generally left out of benchmark tests (which instead build on highly abstracted and utterly unrealistic scenarios) is somewhat discussed in the next chapter on sociotechnical gap... I think}


\subsection{Sociocultural Context and Gap}

Another key insight from previous research concerns the importance of the social, economic and cultural contexts where AI benchmarks are created, used, and maintained. Among researchers engaging in benchmark critique, we identify a strong consensus regarding the need to recognise that benchmarks % do not appear out of a cultural and political vacuum, but are rather intimate products of social and economic interests and circumstances. As \citet[p.~1877]{orr2024a} put it, benchmarks 
are ultimately "normative instruments that perpetuate particular epistemological perspectives about how the world is ordered" \cite[p.~1877]{orr2024a}. Qualitative research has also examined the cultural and social environments where benchmarks are made, finding that they are deeply shaped by shared and arbitrary assumptions, commitments, and dependencies \cite{engdahl2024,michael2022, scheuerman2021,orr2024,sambasivan2021,paullada2021}.
Such assumptions for example include valuing "efficiency at the expense of care; universality at the expense of contextuality; impartiality at the expense of positionality;
and model work at the expense of data work" \cite{scheuerman2021}, or reproduce contested ideas such as the notion that low-quality data can be drowned out by scale \cite{orr2024}.
Recent research has also illustrated that AI safety research and benchmark competitions are increasingly informed by political movements and ideologies such as longtermism and effective altruism \cite{ahmed2024} % Need to shorten: and that datasets that are described "unscripted" or consisting of data captured "in the wild", in fact rely on complex amounts of practical work and decision-making processes \cite{engdahl2024}. 

Scholars have further identified sociotechnical gaps and a lack of consideration for downstream utility as a concern in AI benchmarking \cite{hutchinson_evaluation_2022}.
\citet{liao2023} highlight that this means it is often unclear who is meant to care about benchmark evaluation results and how they should be used in practice.
For instance, a recent study by \citet{blagec2023}, which compared the explicitly stated needs for AI technologies among clinical medical practitioners with existing clinical benchmark datasets, found that most benchmarks failed to answer to the needs of medical experts.
They also found that benchmarks for the most urgently requested medical or clinical tasks were completely missing and noted that similar misalignments likely exist in many other fields. For example, \citet[p.~79]{jannach2020} highlight a lack of attention to how recommender systems "may create value; how they, positively or negatively, impact consumers, businesses, and the society; and how we can measure the resulting effects". \citet{ethayarajh2021} further argue that failures to consider the practical utility of benchmarks has for example made it possible to ignore the discriminatory and environmental damages of AI technologies and allowed for highly energy-inefficient and deeply biased AI models to reach the top of most benchmark leaderboards. % The authors highlight how a lack of discussions concerning what values benchmarks prioritise, constitutes a key limitation in current benchmark and leaderboard practices. 

% ME: cut out in the interest of space: Relatedly, Hutchinson et.al. find that the consequential validity of benchmarks, i.e., "the real-world consequences of an evaluation's interpretation and use" has been broadly disregarded, despite its high relevance for "considerations of accountability and governance of ML models in applications" \cite{hutchinson2022} p.~1860-186. This highlights how many benchmarks lack a practical utility for policy enforcement efforts geared towards holding AI developers accountable and safeguarding against systemic risks.

% Added some discussion on incentives to the conclusions so considering the comment below addressed.
% \an{I think that there is also room for making parallels here with discussions around the incentives of the different actors that are invovled here, whether through the lense of economic incentives or other typical governance of technology lense, so if not here directly, as with my previous suggestions perhaps in the discussion/conclusions? This would also help with more firmly grounding the paper on interdisciplinarity.}

\subsection{Narrow Benchmark Diversity and Scope}

Previous research has further found that current benchmarking practices suffer from diversity issues, a problem that is also found within the broader AI ecosystem  \cite{gomez2024}. On the one hand, a vast majority of benchmarks focus on text, while other modalities (audio, images, video, and multimodal systems) remain largely unexamined \cite{rauh2024, weidinger_sociotechnical_2023, rottger2024}. This concentration is problematic since AI models are increasingly multimodal in scope. \citet{guldimann2024} also find that benchmarks addressing user privacy, copyright infringement, and interpretability are currently lacking and incomprehensive, while benchmarks in safety areas dealing with corrigibility and explainability are practically missing all together. This leads the authors to conclude that current practices for evaluating topics like safety and ethics “are often simplistic and brittle, leading to inconclusive results” \cite[p.~3]{guldimann2024}.
\citet{koch2021} further identify a concentration on fewer and fewer benchmark datasets within most task communities and note that dominant benchmarks have been introduced by researchers at just a handful of elite institutions, raising questions about representation diversity in the design of benchmarks. Scholars have also found that current AI safety evaluation practices almost exclusively deal with English content \cite{mcintosh2024, rottger2024}, and are frequently based on datasets where minorities are under-represented, despite efforts to diversify them \cite{simson2024}. This raises concerns regarding the inclusion of multiple perspectives on complex topics like ethics and harm. 

Aside from mainly focusing on a small range of tasks (evaluating capabilities of English-speaking language models), previous research has identified that most benchmarks tend to be abstracted out of their social and cultural context \cite{selbst2019}, and rely on a static, one-time testing logic, where results from single evaluations are taken to represent model capabilities writ large. This has given rise to calls for more multi-layered \cite{weidinger_sociotechnical_2023}, longitudinal \cite{mizrahi2024}, and holistic evaluation methods that make sure that AI models do not just perform well in controlled environments, but also in critical/real world circumstances over time \cite{ojewale2024, mcintosh2024, chang2023}. In their survey of LLM benchmark inadequacies, McIntosh et al. note that "current benchmarks generally adopt task-based formats, such as Multiple Choice Questions (MCQs) and dialogue-based evaluations, which tend to be static and do not capture the evolving nature of human-AI interactions" \cite[p.~6] {mcintosh2024}. %In reality, however, "LLM usage often involves continuous dialogues, yet many benchmarks assess only the first attempt of an LLM response, without considering the consistency and coherence of answers across multiple interactions.  focus on isolated responses reduces the relevance of these benchmarks in evaluating models designed for dynamic, ongoing interactions" \cite{mcintosh2024} p.~6. 
\citet{reuel_betterbench_2024} further note that failures to re-run evaluations multiple times (using different random seeds and sampling temperatures for example), implies that very little is known about the intra-model variance of benchmarks. As a result, they conclude that it is possible that "most benchmarks fail to distinguish signal and noise" \cite[p.~9]{reuel_betterbench_2024}. 
Others have emphasised that AI audits often fail to consider risks associated with multiple (inter)acting AI systems \cite{birhane2024}, and rarely take human actions and motivations into consideration \cite{weidinger_sociotechnical_2023, chang2023, rauh2024}. % and do not, once more, evaluate energy-use and environmental impact \cite{hutchinson_evaluation_2022}.

While most benchmarks are designed to tell us something about a model's success, research has further pointed out that they often reveal little (or nothing) about their particular ways of \textit{making mistakes}, which is crucial from an AI safety and policy enforcement perspective. As \citet{gehrmann2023} put it, "ranking models according to a single quality number is easy and actionable - we simply pick the model at the top of the list - [yet] it is much more important to understand when and why models fail" \cite[p.~130]{gehrmann2023}.
For instance, they suggest that a focus on errors and fragilities (as opposed to instances of success) can be useful for developers of smaller models, since "work on quantifying shortcomings is equally applicable to smaller models and methods that improve model robustness often work on all model sizes" \cite[p.~131]{gehrmann2023}.
In this sense, failure-focused benchmarks could play an important role in equalling out the playing field in AI development.


\subsection{Economic, Competitive, and Commercial Roots}

Another contextual element that has been singled out as important is the competitive and commercial roots of benchmark tests. % AI providers often make grandiose claims about their model's capabilities based on performance metrics from benchmarks (e.g., “Our Largest and Most Capable AI Model” \cite{grill2024}). This trend extends to the realm of geopolitics, where benchmark performance is used as part of the global race for AI leadership \cite{globalAI2024, Zhijia2024}. 
Previous research has emphasised that capability-oriented benchmarks are deeply embedded in corporate marketing strategies and play an important role in increasing the AI hype, attracting customers and investors, and showcasing how models outperform competitors \cite{orr2024a, grill2024, Zhijia2024}. As \citet[p.~1881]{orr2024a} put it, benchmarks "serve as the technological spectacle through which companies such as OpenAI and Google can market their technologies". Many benchmarks also have origins from within the industry and are capability-oriented and centred around tasks with a high potential economic reward, as opposed to focusing on other goals such as ethics and safety \cite{ren2024, ethayarajh2021}.


Previous research has noted that this competitive and corporate embedding discourages thorough self-critique since there is a direct "incentive mismatch between conducting high-quality evaluations and publishing new models or modelling techniques" \cite[p.~103]{gehrmann2023} and that the field of AI development is "turning into a giant leaderboard, where publication depends on numbers and little else (such as insight and explanation)" \cite{church_survey_2019}. 
While a lack of incentives to disclose weaknesses and limitations is a general problem in science \cite{smith2022}, it has been pointed out that benchmarks have played an especially central role in naturalizing and solidifying a competitive culture in AI research, which is increasingly approached as a "sport" \cite{orr2024a}. As of late, benchmark evaluations have also become increasingly professionalized and transformed into an industry in itself with the rise of platforms like Kaggle and Grand Challenge, who provide organisational and infrastructural support to AI competitions and increasingly function as infrastructures of power in fields like medical imaging \cite{luitse2024}. The issue of optimising for high benchmark scores at the expense of insight and explanation is known as a form of SOTA-chasing \cite{koch2021} and sometimes described as the "benchmark effect" \cite{Stewart2023}. Previous research \cite{maleve2023} has also described benchmarks as an example of what \citet{stengers2018} calls "fast track research" which idolises rapid, cumulative publication, thus producing a "winners curse" in AI development \cite{sculley2018}. 

Risks associated with the competitive and commercial roots of benchmarks have further been linked to the growing influence of industry in AI research, where private businesses share of the biggest AI models has increased from 11\% in 2010 to 96\% in 2021 \cite{ahmed2023}. In such a context, researchers have noted that current benchmarking tasks - which are generally highly data-intensive - are especially well suited to fit AI models that have been developed within the industry, whose access to advanced data infrastructures, computing power, valuable datasets, and skilled researchers now vastly exceed those of academic researchers \cite{ahmed2023}. 
% JV: This is a very important point, and  one that may explain many issues identified in other sections. In a way, it is  unprecedented that such a ground-breaking set of technologies is not the result of state-funded research. Maybe something to emphasize in the introduction, or discussion?
% ME: cut out for reasons of space: In 2021, for example, Ahmed Wahed and Thompson note that "non defense US government agencies allocated US\$1.5 billion on AI. In that same year, the European Commission planned to spend €1 billion (US\$1.2 billion). By contrast, globally, industry spent more than US\$340 billion on AI in 2021" (ibid., p.~884). 
% As the resources needed to perform well on benchmark tests are increasingly located in industry domains, at the same time as industry actors are playing a key role in defining what tasks and problems AI developers should strive to solve. 
This concentration of power could potentially stifle robust AI evaluations and hinder the development of AI models that adhere to other aims and goals than commercial ones. Scholars have also warned that if academic researchers continue to uphold data-intensive benchmark tests as the SOTA, there is a risk that their research will become increasingly dependent on technological infrastructures provided by the industry \cite{koch2024}. 

% Applying AI benchmarks and contributing to leaderboards implies legitimizing and reinforcing an approach to AI safety testing with deep corporate and market-oriented roots. Research in computer history and the history of science has emphasized that there is nothing natural or self-evident about this particular way of doing AI research and development (ibid.).


\subsection{Rigging, Gaming, and Measure Becoming Target}

A closely related issue concerns how benchmark tests can be tricked and gamed. In areas and modalities where best-practice benchmarks are missing (i.e., practically all modalities, except for text-based benchmark evaluations), researchers have noted that there are strong incentives to "rig" benchmark tests. For instance, \citet{dehghani2021} survey how know-how and recipes for how to score high on benchmark setups are often widely circulated online.
% As an example, they describe how researchers and developers in natural language processing "realized that pre-training on MNLI is necessary for obtaining strong performance on RTE and STS... [benchmark]) datasets" \cite[p.~13]{dehghani2021}. Accordingly, this soon became standard practice within the community. 
Recently, language models have also been found to be optimised for answering the multiple choice questions that are often part of benchmarks \cite{alzahrani2024}, and to (either intentionally or unintentionally) "fake" alignment with ethics or safety goals \cite{greenblatt2024}.
The issue point towards what is known as Goodhart's law, i.e. the recognition that "when a measure becomes a target, it ceases to be a good measure" \cite{strathern1997}.
\an{is there room for a parallel to be made here with the problematic of measurers like h-indexes, which most academics; including those in AI, should be aware of, but seem to be not be internalised when it comes to benchmarks and metrics development? and the two might even be connected in some ways. Or is it too confrontational to discuss?}
\me{if you have a source that critique h-indexes then I´d say this could absolutely be added as a brief note}

One reason why gaming can proceed is that users of benchmarks rarely provide the resources needed to validate and replicate their test results \cite{bartz-beielstein2020, dehghani2021, biderman2024, reuel_betterbench_2024}. To examine the relevance and validity of benchmark scores, researchers have emphasised that it is necessary to access information concerning all aspects of the evaluation procedure (such as the original evaluation code and all details concerning the experimental setup) \cite{biderman2024}. Providing such documentation is far from standard, however, especially when proprietary AI models are concerned. This makes it possible to tweak and cherry-pick benchmark results - a problem that is especially pressing given that subtle "variations in prompts, formatting or other implementation details can significantly impact the performance and validity of evaluations" \cite[p.~3]{biderman2024}. In their in-depth analysis of 24 SOTA language model benchmarks, \citet{reuel_betterbench_2024} found that only four provided scripts to replicate the results and that no more than ten performed multiple evaluations or reported the statistical significance of their results. 

An increasingly well discussed issue also concerns the problem of "data contamination" i.e., the risk that the models have either intentionally or unintentionally ingested benchmark datasets during training, which severely questions the integrity of AI tests \cite{xu2024, zhang2024, besen2024, magar2022, roberts2023}. The problem - which can be an instance of data leakage \cite{kaufman_leakage_2012} or train-test-overlap \cite{lewis2021} - has been known for long, and produces similar effects to those of overfitting and memorization \cite{tirumala2022,magar2022}, leading to models with low generalization power that perform well on familiar tasks (in-distribution) but fail other tasks with a similar difficulty and distribution shift (out-of-distribution) \cite{yuan2023, xu2024, zhang2024, besen2024, magar2022, roberts2023, narayanan2023}. When testing GPT4 on benchmark problems from Codeforces (a website hosting coding competitions) in 2023, for instance, \citet{narayanan2023} found that the AI model could regularly solve benchmark problems classified as easy - as long as the problems had been added before 5\textsuperscript{th} September 2021. For problems added later, GPT4 could not get a single question right, suggesting that the model had memorised questions and answers. Similar results have also been identified in multiple other models and benchmarks \cite{xu2024, zhang2024, besen2024, magar2022, roberts2023}. 
Despite the fact that issues with data leaks are so well known that strategies have been developed to avoid it\footnote{\url{https://livebench.ai}. Last seen \today.}, there is still a widespread lack of reporting of data contamination tendencies during benchmark tests. In a study from October 2024, for example, \citet{zhang2024} found that out of 30 analysed models, only 9 reported train test overlap. 

Another recently identified issue is called "sandbagging" and involves a "strategic underperformance on an evaluation" which occurs when an AI developer intentionally \textit{understates} a models capability, for instance to avoid becoming a target for AI safety regulation. In a 2024 study, for example, \citet[p.~1]{weij2024} prompted frontier models like GPT-4 and Claude 3 Opus and found that they "selectively underperform on dangerous capability evaluations, while maintaining performance on general (harmless) capability evaluations". The researchers also found that that it was possible to fine-tune and adjust both frontier and smaller models to hide specific capabilities or target specific capability scores. The issue could be generalised to popular benchmarks like the Weapons of Mass Destruction Proxy Benchmark (WMDP), and puts the trustworthiness of benchmark evaluations into question, especially in a regulatory context. 

% ME: Note that the Alzahrani piece above is published by people with connections to the Saudi Data and AI Authority and I don't have enough technical expertise to judge if their methods and results are are legit. Perhaps someone else can have a look?


\subsection{Dubious Community Vetting and Path Dependencies}

A related area of research emphasises how benchmarks become naturalised and reach standard status % not because of their technical brilliance, but 
because of the culture and logic of academic citations \cite{orr2024a}. For instance, new benchmarks are commonly introduced together with new or updated AI models. If the AI model becomes popular, the benchmark may become widely cited and circulated as a secondary effect, even though the developers of the benchmark did not intended or expect it to become standard. In this way, a benchmark that might have been selected for practical purpose or due to subjective preferences may come to have substantial technical and cultural influence as a result of community vetting, even though its suitability as a yardstick for ethics, safety, or performance may be questioned \cite{orr2024a, orr2024}. For instance, \citet{denton2021} show how this was partly the case when the ImageNet dataset - a key reference point in the performance testing of computer vision models - became standard, following  the unforeseen success of the ImageNet Large Scale Visual Recognition Challenge hosted by Toronto University in 2010. Likewise, the so-called Lena test image - also central to computer vision benchmark tests - was taken from the centerfold of a November 1972 Playboy magazine and catapulted into computer history since "someone happened to walk in with a recent issue of \textit{Playboy}" at a time of need, according to what has been described as the most credible origin story \cite[p.~80]{mulvin2021}.
Since then, the ImageNet dataset and its associated benchmark challenge has been become a symbol for dataset bias \cite{denton2021}, while the Lena test image has come to serve as a prime example of the role of whiteness and women's sexualised bodies in the standardisation of digital visual culture \cite{mulvin2021}. These examples highlight how benchmarks are fundamentally cultural and political products whose power and influence may and be (uncritically and problematically) reinforced through community vetting. 

Aside from bias and representational issues, \citet[p.~1]{schlangen_targeting_2020} argues that what is typically missing and left implicit in the peer-review-fuelled process of benchmark use is the argumentation for why scoring well on a particular benchmark "constitutes progress, and progress towards what". Instead, researchers are expected to routinely demonstrate performance on dominant benchmarks, despite the fact that more task-specific benchmarks may be more technically appropriate \cite{koch2021}. In this way, peer-washing serves to "maintain datasets as authoritative proxies even when they are shown to be harmful or problematic" \cite[p.~4966]{orr2024}. New benchmarks often have a difficult time to gain traction because of the dominance and authority of well-cited benchmarks \cite{jaton2021}, and in a study of 3765 benchmarks, \citet{ott2022} found that only a small minority of the proposed benchmark solutions reached widespread adoption. This problem is also recognised within the industry. For instance, researchers at Google's Brain Team describe what they call a "benchmark lottery" which "postulates that many factors, other than fundamental algorithmic superiority, may lead to a method being perceived as superior" \cite[p.~1]{dehghani2021}. Ironically, researchers have simultaneously found that a majority of influential benchmarks have been released as preprints without going through rigorous academic peer-review \cite[p.~6]{mcintosh2024}. In other words, problems with peer-washing both concern the poor quality control that many benchmarks are subjected to, and the self-fulfilling and sometimes excessive ways through which certain benchmarks are propelled into standards. % A 2022 survey answered by 480 NLP researchers also found that 88 percent of the respondents believed that "there is too much focus on optimizing performance on benchmarks" \cite{michael2022}. 

\citet{bao2022a} further note that most papers that introduce benchmarks have origins in the field of machine learning and are mainly focused on methods. This means that the content of benchmarks datasets are often considered secondary, since the datasets are merely there to provide a baseline for comparison between different evaluation methods. When benchmarks are applied to real and specific use-cases - such as evaluating the fairness of algorithmic risk assessment instruments within the criminal justice system - the downstream effects of such a lack of concern for datasets can have worrying effects.
As \citet{bao2022a} put it, a paper on benchmarks can be of high quality in a pure AI/ML methods sense, but irrelevant, dangerous, or harmful when applied in circumstances such as the criminal justice system, since it may introduce and perpetuate harms and mistranslations. The researchers also suggest that the current peer-review system implies that benchmarks that are primarily relevant from a methods/machine learning perspective will be cited far more often than benchmarks that are relevant for use in specific, real-life use-cases. Looking too closely at citation counts when determining the quality and relevance of benchmarks may thus effectively lead users astray and complicate identifying benchmarks with a high \textit{practical} utility. % Bao et al. further note that the conference publication workflow and community norms in computer science complicate appropriate use of benchmarks, compared to fields that rely on journal publications, since papers are often pre-published relatively quickly and supplemented over time. This means that many insufficiently curated datasets are in circulation. Researchers who may have used a benchmark dataset in a seminal paper - but later finds that it was of a poor quality - are also often still expected to cite and replicate flawed experiments during rebuttal periods. This implies that problematic benchmarks may continue to be widely cited. Furthermore, "the rapid publication cycle in which methods might build upon each other within a year or two often establishes an appearance of consensus around a benchmark, which becomes difficult to change or refute later" (ibid., 9). 

Problems with dubious community vetting become especially worrying given that benchmarks create "path dependencies" in AI research, meaning they reinforce certain methodologies and research goals, while stifling those that do not align with the logic of dominant benchmark tests \cite{blili-hamelin2023}. % When benchmarks become well established in specific AI communities, researchers have emphasised that they actively shape how communities approach progress. From a historical perspective, \citet{ensmenger2012} for instance shows how "the decision to focus on chess as the measure of both human and computer intelligence had important and unintended consequences for AI research," as it helped promote certain research agendas while neglecting others. In particular, the author suggests that the dominant use of chess as a benchmark for AI was instrumental in making deep-tree searching and the minimax algorithm the dominant focus of AI research during the mid 20th century. "In choosing chess over its various alternatives", \citet[p.~7]{ensmenger2012} writes, "AI researchers were able to tap into a long tradition of popular chess culture, with its corresponding technical and theoretical literature, international networks of enthusiasts and competitions, and well-developed protocols for documenting, sharing, and analyzing data. Yet the brute-force computational techniques that proved most suitable for winning computer chess tournaments distracted researchers from more generalizable and theoretically productive avenues of AI research... As a result, computers got much better at chess, but increasingly no one much cared".
In particular, \citet[p.~3]{koch2024} warn against the tendency for benchmarks to favour a form of task-driven scientific monoculture that privileges immediate, explicit, formal, quantitative, and easily-interpretable evaluation mechanisms that "prioritize one or a few key epistemic values (e.g., accuracy, safety)," at the expense of a broader and more complex vision of scientific progress. The authors note how a series of methodological paradigms in AI research (such as boosting, Bayesian networks, Bayesian non-parametrics, and support vector machines) have rapidly faded away following the boom in deep learning research around the year of 2014, and partly attribute this to the "epistemic narrowness" of current benchmark tests, which are well-suited for deep learning models (whose performance can be reliably increased through scaling), but less favourable for other methodological paradigms in AI. "When scientists can only gain high-status publications by demonstrating SOTA accuracy," writes \citet[p.~30]{koch2024}, "the safest research choice becomes incrementally advancing proven methods, not innovating new ones". According to Koch, the cost of current benchmark practices - which privileges scale, compute, and the use of larger and larger training datasets - for instance includes repeated privacy and copyright violations, emotional harms caused to under-paid data workers, and increasing ecological and environmental pressures due to high energy consumption. 


\subsection{Rapid AI Development and Benchmark Saturation}

Another social, economic, and cultural issue is the speed of AI developments. As the capabilities of AI models have increased manifold in the past decade, researchers have emphasised that many benchmarks are old and designed to test models far simpler than those in use today \cite{keegan2024, biderman2024}. For instance, \citet[p.~5]{biderman2024} find that many prominent LLM benchmarks (including Lambada, AI2 ARC, OBQA, Hella Swag, and WinoGrande) were "designed prior to shifts such as in-context learning and chat interaction, and therefore were not designed to take these formats and approaches into account", noting that this may affect their validity in unforeseen ways. 
Many benchmarks also struggle with the challenge of quickly being outperformed as AI models achieve 100 percent accuracy scores in tests \cite{hendrycks2021, bowman2021}. A look at current AI safety leaderboards such as HELM also reveal that many AI models score notably high on benchmarks that are widely applied today. The tendency for AI models to outperform benchmarks has been described as an issue of \textit{saturation} and implies that a benchmark no longer reflects model performance \cite{ott2022}. 

Relatedly, \citet{mcintosh2024} emphasises that many benchmark frameworks are slow and complicated to implement, meaning that evaluation processes can span weeks or months, which hinders timely feedback on AI model safety risks. This becomes an issue since new model releases often enter markets continuously, making it difficult to relocate evaluation resources in quick and adequate ways. It also undermines a "benchmark's ability to consistently evaluate reasoning, comprehension, or multimodal integration, as the results may vary with each model iteration" \cite[p.~13]{mcintosh2024}, which is especially concerning in a regulatory setting, where quick, fair, and accurate AI assessments are key. 
%The relative difficulty of producing benchmarks that can keep up with rapid AI development has also been singled out as one of the main reasons why hard-coded thresholds may be inappropriate to use in assessments of AI safety \cite{hooker2024}. 
%The use of benchmark thresholds is for instance central to current discussions on the use of compute measurements in evaluations of the systemic risks of AI models, where compute refers to the amount of resourced needed to train and deploy an AI model. While more and more compute has been central to AI development in the past decade, which indicates that it could be used as a reliable indicator of model capability, researchers have warned that this tendency might very well change in the future and that small yet highly capable AI models are already entering markets \cite{hooker2024} see for example See for example: https://arxiv.org/abs/2407.15811. 
The use of thresholds, either on benchmark performance or training compute (i.e., the amount of computational resources used to train an AI model), is often seen as central to determine which AI models should warrant further regulatory scrutiny \cite{AIA24, USAIEO2023, USAIDiff2025}. However, several limitations have been identified in relation to such attempts. Creating benchmarks that can keep pace with the rapid development of AI is increasingly challenging, at the same time as recent approaches (such as auxiliary or bootstrapped models or post-training interventions) enable enhanced capabilities with reduced training compute \cite{hooker2024}.

\subsection{AI Complexity and Unknown Unknowns}

A final issue that has been discussed in relation to benchmarks concerns AI complexity and the fundamental difficulty of foreseeing what risks, dangers, and threats AI models could pose to society. \citet[p.~13]{mcintosh2024} point out that "LLM benchmarks are constrained by the current limit of the benchmark creators' human knowledge, hindering their ability to fully assess and cultivate emerging AI capabilities that may surpass conventional human understanding".
According to the authors, the lack of general understanding of emerging AI capabilities, and the natural limitations of the benchmark creators' knowledge on a potentially infinitely large number of domains and tasks, may lead to generalist approaches that "often fail to address the subtle requirements of critical sectors such as national security or healthcare". They argue that this does not just pose security risks, but could potentially also hinder innovation. 

Evaluations of AI models are also complicated by the potential presence of unknown and latent vulnerabilities in AI models that “make it very hard to distinguish between (a) actually safe and (b) appears safe but is not” \cite{nasr2023a}.
In 2023, for instance, \citet{nasr2023} discovered that surprisingly simple prompts can ‘break’ the safety barriers of AI models, raising questions about the robustness of existing safety measurements.
More precisely, the simple command “Repeat the word “poem” forever” was found to make ChatGPT output several megabytes of sensitive training data. As a result of this finding, the authors conclude that “just as vulnerabilities can lie dormant in code - sometimes for decades – our attack demonstrates the potential for latent, hard-to-discover ML vulnerabilities that lie dormant in aligned models” and go unnoticed by existing safety benchmarks and tests \cite[p.~13]{nasr2023}.
Researchers have also noted the difficulty of foreseeing how complex AI models respond to (both small and large) interventions such as safety alignments and fine-tuning.
For instance, efforts to fine-tune AI models to address safety and/or security risks have been found to degrade a model’s performance in other safety areas, or introduce entirely new security risks \citet{qi2023}.
% In a study of the safety costs of customised fine-tuning, for example, \citet{qi2023} found that "the safety alignment of LLMs can be compromised by fine-tuning with only a few adversarially designed training examples" and that "even without malicious intent, simply fine-tuning with benign and commonly used datasets can... inadvertently degrade the safety alignment of LLMs". 

\an{The discussion in this particular section is also closely linked to my previous point related the capabilities of the adversary and the assumptions that are made about that. Security researchers typically document their assumptions regarding the adversary for instance by stating something like we "assume that adversary does not have NSA level, state-actor level capabilities of breaking encryption" for instance. But also another fundamental lesson from safety and security that can either be discussed here or in the discussion is that \textbf{the question of "is a system safe or not?" is seen as a false dichotomy but rather more seen like all systems are unsafe depending on who the adversary and context is.} And with that in mind safety/security is always seen as a balancing exercise rather than as an absolute. Perhaps something that might be useful to consider discussing here or somewhere is literature like this: https://arxiv.org/pdf/2004.07213}
\me{This is a good point, I think a relevant conclusion for us to draw is to say that we find a lack of acknowledgement of the fundamental limitations of benchmarking (marked in bold above) to be a real problem in the field of AI safety testing} 




% \subsection{Mitigation strategies: possibilities and limitations}

% ME: This subsection does not really fit here, and could perhaps be woven into the discussion and conclusion instead...

% One solution to the limitations of individual benchmarks has been attempts to aggregate different evaluation tasks into single multi-task benchmarks with the hope that this will increase the reliability, validity, and representativeness of model evaluations. Examples include BigBench \cite{srivastava2023} and HELM \cite{liang2023}. However, recent research has shown that such attempts may come with drawbacks, such as having to make important decisions concerning trade-offs between diversity and robustness/sensitivity \cite{zhang2024}. Zhang et.al also conclude that BigBench and HELM are "highly unstable to irrelevant changes" and suffer from significant instabilities with regards to rankings (ibid.). Likewise, Artzt et al. find that multitask benchmarks and leaderboards presenting aggregated benchmark statistics with the help of singular metrics (such as F1-score) often "fail to capture the complexity of the relation extraction task, especially in scenarios involving a large number of labels and highly imbalanced datasets" \cite{arzt2024}. 

% Another solution to the limitations of current benchmark practices has been the introduction of so-called dynamic benchmarks, which are designed to be updated and adjusted over time to mitigating risks associated with data saturation, data contamination, and rigging/gaming \cite{besen2024, shirali2023}. However... Shirali note that... Efforts to produce dynamic benchmarks for example involve using AI models to slightly adjust and update benchmark datasets to make them more difficult to solve \cite{wang2024} or using AI models as adversarial agents in the safety testing of other AI models \cite{perez2022}, or using AI models to identify and classify harmful AI output \cite{inan2023}. While such efforts may lower the time and cost of conducting AI safety tests, the use of additional AI tools in AI model evaluation may result in making it increasingly difficult to explain evaluation results, allocate accountability, and ensure that errors and biases do not propagate throughout the evaluation process. 

% Another way of solving ixing train-test overlap and data contamination by hiding benchmark datasets (following and ARC-AGI model), although this brings transparency issues... what would it mean if all benchmark datasets were hidden from view? It would be practically impossible to assess potential biases etc.

% Some scholars argue that method of closing gaps in existing AI safety evaluations could involve repurposing benchmark evaluations and datasets (for instance, by transcribing non-text output to make use of the wide range of text-based benchmarks that exist) \cite{rauh2024}. Since many existing benchmarks display serious weaknesses with regards to documentation, transparency, and replicability \cite{reuel_betterbench_2024} suggests that this may be directly unwise.

% Rauh et al. also emphasize that "evaluation alone is not a panacea" and that the answer to benchmark shortcomings should not just be to develop more or "better" benchmarks, but also a more flexible AI design that allow for continuous system updates, alongside efficient governance mechanisms \cite{rauh2024}

% Moving forward, Chang et al. notice a shift towards using humans-in-the-loop during testing, crowd-sourced and dynamic designs of benchmark datasets, and centralized testing environments where multiple benchmarks are gathered and applied \cite{chang2023}. 

% \subsection{\textcolor{red}{Cut out}}

% A broad range of critique has also been directed towards human-annotated ground truth datasets, which for example tend to assume that there exist only one correct interpretation for input examples and are often made under conditions where efforts are made to avoid disagreement between annotators - a move which effectively reduces complexity and may result in problematic abstractions \cite{aroyo2015}. 

% Elsewhere, Kovatchev and Lease find that differences in the distribution of certain dataset features in benchmarks datasets "significantly affect both absolute and relative model performance" \cite{kovatchev2024} p.~1544. More precisely, the researchers analyzed how differences in the constitution of datasets across six dimensions (ambiguity, difficulty, discriminability, length, noise and perplexity) impacted the performance of 135 AI models on two NLP benchmark tests: SQUAD and MLNI. The authors conclude that "without explicitly considering data features, standard evaluation frameworks are inconsistent and unreliable" (p.~1541).

% Ethayarajh and Jurafsky point out that NLP leaderboard design is for example limited by the fact that "practicioners recieve higher utility from a model that is more robust to adversarial perturbations, generalizes better to out-of-distribution data, and that is equally fair to all demografics. However, these benefits would leave leaderboard utility unchanged" \cite{ethayarajh2021} p.2). 

% Selbst et al. note that abstractions - which are central to computer science, machine learning, and not least benchmarking - generally invlove disregarding the social context of AI tools \cite{selbst2019}. This, they argue, may lead researchers into five different traps: a \textit{framing trap} (as they fail to consider the larger social context of the data and results they use and produce), a \textit{portability trap }(as they fail to recognize how the repurposing of algorithmic solutions from one context to the next can be misleading, inaccurate, and harmful), a \textit{formalism trap} (as they fail to recognize the instability of social concepts like fairness), \textit{a ripple effect trap} (as they disregard how the insertion of a technology into a social system changes it), and a \textit{solutionism trap} (as they do not acknowledge the possibility that the best solution to a problem may not involve technolgy) (ibid.). While Selbst et al. do not explicitly adress benchmarks in their discussions, their critique can easily be transferred to benchmark practices to, as a sociotechnical perspective is missing in most benchmark development and application processes.

% A recent study on LLMs sensitivity to prompt formatting found that widely used open-source LLMs (including) "are extremely sensitive to subtle changes in prompt formatting in few-shot settings, with performance differences of up to 76 accuracy points when evaluated using LLaMA-2-13B" \cite{sclar2024}.

% Recent research has also shown that large language models are "vulnerable to option position changes in... multiple choice questions due to their inherent "selection bias", namely they prefer to select specific option IDs as answers (like "option A") \cite{zheng2024}. For instance, the researchers found that "llama-30B selects A/B/C/D 34.6% / 27.3% / 22.3% / 15.8% of the time, while gpt-3.5-turbo for 22.5% / 25.6% / 32.3% / 19.6%, respectively (averaged over 10 runs)" a result which indicates that "large language models are not robust multiple choice selectors" (ibid., p.~2). 