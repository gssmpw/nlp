\section{INTRODUCTION} 

% \note{
% I've added some custom commands for everyone to be able to edit the document with more ease which will show up with individualized colors. I use for instance \textbackslash an\{ ... insert some text ...\} to add text in blue color throughout the draft. Maria can for instance use \textbackslash me\{...insert some text...\}, etc. All of these comments can be made to disappear by simply going to main.tex file, finding the section indicated by the "Draft SETTINGS" heading, and toggling on or off the "drafttrue" or "draftfalse" parameters.

% There is also more, to make an orange "note" box like the one you are seeing here for instance, you can simply use the \textbackslash note\{\} command that I've added. There is also a Microsoft Word style commenting capability with the \textbackslash cm\{ ... some comment ...\} that can be used to make a comment appear in the margins of the text. Note that these elements will NOT disappear automatically by changing the settings that I mention above. If you feel like you want them to disappear too with the same setting let me know so that I can make adjustments.

% Hope this helps.
% }

Quantitative artificial intelligence (AI) benchmarks (i.e., combinations of test datasets and performance metrics that are taken to represent general or specific tasks and used to compare AI model capabilities and/or risks~\cite{raji2021}) play a central role in the release and marketing of newly developed AI tools. Together with qualitative evaluation methods (such as red teaming and peer confrontations~\cite{chollet2019}), quantitative benchmarks - hereafter referred to as \textit{AI benchmarks}, or simply \textit{benchmarks} - are generally seen as providing crucial feedback signals on the performance and capabilities of AI. Indeed, they have become so critical to AI development that businesses go to great lengths to achieve good benchmarking scores, with market players like OpenAI being estimated to have spent hundreds of thousands of dollars on compute to obtain a high score at the ARC-AGI benchmark~\cite{pfister2025}.

Increasingly, AI benchmarks are also used in regulatory contexts, where the goal is to assess potential societal harms posed by AI models and systems. 
%The growing importance of benchmarks in policy initiatives is, for example, visible in  US with the recently revoked AI Executive Order~\cite{USAIEO2023} and the AI Diffusion Framework~\cite{USAIDiff2025}, as well as in the EU with the AI Act~\cite{AIA24}, which describe benchmarks as central to AI capability and risk classification and assessment. 
The most notable case is the EU AI Act \cite{AIA24}, which incorporates benchmarks in several key provisions. For instance, for high-risk AI systems, benchmarks are expected to play a significant role in complying with requirements on accuracy, robustness, and cybersecurity (Art. 15(2)), and their development will be facilitated through AI regulatory sandboxes (Art. 58 (2)). They will also be of fundamental importance for classifying general-purpose AI (GPAI) models with systemic risks and assessing high-impact capabilities (Art. 51(1) and Annex XIII). Furthermore, both the Board and the Scientific Panel are expected to contribute to the development of benchmarks (Art. 66(g) and Art. 68(3)). Additionally, the first and second drafts of the Code of Practice specifically mentions benchmarks as an example of best-in-class evaluations for risk assessment measures for providers of GPAI models with systemic risks \cite{1stDraftCoP2024, 2ndDraftCoP2024}. In the US, benchmarks are also relevant in the recently revoked AI Executive Order~\cite{USAIEO2023} and the AI Diffusion Framework~\cite{USAIDiff2025}.
AI benchmarks can further be expected to play a central role in the implementation of legislations such as the EU Digital Services Act (DSA)~\cite{DSA2022} and the UK's Online Safety Act~\cite{UKSafetyAct2023}, which require the largest online platforms and search engines - entities that increasingly rely on AI to curate, filter, and/or rank content to millions of users - to perform regular algorithmic audits showing that their systems are safe, fair, and comply with fundamental human rights. In short, AI benchmarks - which constitute a highly heterogenous and far from standardised set of techniques - are increasingly at the heart of policy efforts to make AI more transparent and secure. 

At the same time however, as benchmarks are increasingly relied upon to provide AI safety assurances, researchers in a broad range of academic fields have raised serious concerns regarding their current use.
This includes critical voices being raised in fields ranging from cybersecurity~\cite{mcintosh2024}, linguistics~\cite{bowman2021}, and computer science~\cite{gema2024}, to sociology~\cite{engdahl2024}, economics~\cite{ethayarajh2021}, philosophy~\cite{lacroix2022}, ethnography~\cite{keyes2022}, and science and technology studies~\cite{keyes2022}.
Such scholars have for example described current AI evaluation practices as a "minefield"~\cite{narayanan2023a}, that raise serious ethical concerns regarding what should be measured, according to what standards, and with what downstream effects~\cite{blili-hamelin2023}.
They have also emphasised that benchmarks are deeply political, performative, and generative in the sense that they do not passively describe and measure how things are in the world, but actively take part in shaping it~\cite{grill2024}. 
This happens as benchmarks continuously influence how AI models are trained, fine-tuned, and applied - practices with wide-ranging political, economic, and cultural effects.

With this backdrop, an interdisciplinary and up-to-date survey of quantitative AI benchmarking critique is momentously missing. The aim of this article is to address this gap by mapping and discussing known limitations in quantitative AI evaluation practices, drawing together insights from technical, cultural, and historically oriented academic fields. Our goal is to provide an overview of current risks associated with quantitative AI tests, targeting policy makers, stakeholders performing algorithmic audits, and developers of AI models and benchmarks. To achieve this, we gather and analyse around 100 publications published between 1\textsuperscript{st} January 2014 and 31\textsuperscript{st} December 2024 that explicitly and primarily address limitations in current benchmarking practices. Notably, more than half of these publications were published in 2023 or later, highlighting the urgency of providing an updated survey of discussions in the field.

Our findings show that a rapidly growing number of researchers are voicing concerns regarding how benchmarks are used to define and measure what is safe or unsafe, moral or immoral, true or false, toxic or healthy. They further support the notion that no benchmark is neutral and that AI tests and benchmarking practices always rest "on interwoven technical and normative decisions"~\cite[p.~1201]{rauh2024} in ways that urge policy makers and AI developers to apply them with caution. In particular, previous research highlights a need to question the relevance and trustworthiness of well-cited benchmarks, since existing studies have repeatedly shown weaknesses in benchmarks perceived as state-of-the-art (SOTA). Our findings also reveal a substantial need to scrutinise capability and safety-oriented AI benchmarks to the same extent as the AI models they are meant to evaluate. In short, AI benchmarks need to be subjected to the same demands concerning transparency, fairness, and explainability, as algorithmic systems and AI models writ large. \an {The list of demands is surely not exhaustive, at the same time I find none of these a goal or demand in an of itself, but rather towards a bigger goal. Some of the issues that we discuss further are much more fundamental that these, i.e. the demand that benchmark be valid, to be technically sound in measuring what it claims to be measuring, to achieve its intended goal (e.g. indicating safety, fairness, etc.). I think it would be better if the things that we list here is more directly tied to the issues that we discuss later on. }% ME: As the common saying goes: two wrongs don't make it right. A benchmark aimed at disclosing potential instances of discrimination or injustice in AI, cannot (and \textit{should} not) display the same internal weaknesses as it is meant to disclose. 
% ADD FURTHER SUMMARY OF OUR MAIN FINDINGS

In what follows, we first provide a background discussion on AI benchmarking practices, including clarifications on the terminology used in this paper. Next, we situate our work within a series of previous research that summarise benchmark critique, before providing a summary of nine issues with quantitative benchmarks, identified during the course of our research. While not exhaustive, we present these issues as a taxonomy of benchmark critique that highlights crucial and often interrelated points of concern regarding benchmarks, voiced in the past decade. Finally, we provide some concluding remarks on the implications these issues have for policy makers and implementers. On the whole, our work constitutes the first step in a broader research and policy-oriented project aimed at developing a framework for trustworthy AI benchmarks, and will proceed by considering mitigation strategies.

% Developing transparent and trustworthy AI benchmarks is not just in the interest of researchers, policy makers, and the wider public. There are also significant economic gains to be made by improving insufficient reporting of AI safety since research suggests that currently, "no high-risk AI system could be developed on top of...  [existing] GPAI models" due to their failure to comply with the EU AI Act's demands concerning documentation and transparency \cite{guldimann2024} p.~16.

% As Grill points out, the so-called "capabilities" of AI models should not be understood as "static, decontextualized, and simply quantifiable phenomena but as both contested constructions and situated accomplishments of human and non-human actors" \cite{grill2024}. In other words, AI capabilities do not exist "out there" somewhere in world, ripe for measurement and evaluation, but are actively and continuously constructed and brought into being. AI benchmarks play a key role in such processes, since they help AI capabilities become "real" and tangible (ibid.). 

% Recent research has identified an unprecedented growth in the release of AI benchmarks for safety. For instance, RÃ¶ttger et al. found a moderate growth phase during 2021 and 2022, with a rapid increase starting from 2023 and onwards. For instance, roughly 46 percent of the benchmarks they identify as relevant had been produced in 2023, with as many as 15 new datasets being released during the first two months of 2024 \cite{rottger2024}. The researchers also identify a shift in focus from evaluating biases dominating the field during 2018-2021, with broad safety (including topics like red-teaming, and general safety evaluations) emerging as a prominent theme in 2022. More recently, their findings suggest a shift towards more narrow and specialized safety evaluations, including topics like rule-following or privacy-reasoning, alongside the use of small and hand-written prompt datasets for model evaluation.

% ME: Original text that was previously part of the background section: Benchmarks play an increasingly important role in policy initiatives and regulations. The most notable case is the EU AI Act \cite{AIA24}, where we can find several examples. For instance, for high-risk AI systems, benchmarks are expected to play a significant role in complying with requirements on accuracy, robustness, and cybersecurity (Art. 15(2)), and their development will be facilitated through AI regulatory sandboxes (Art. 58 (2)). They will also be of fundamental importance for classifying GPAI models with systemic risks and assessing high-impact capabilities (Art. 51(1) and Annex XIII). Furthermore, both the Board and the Scientific Panel are expected to contribute to the development of benchmarks (Art. 66(g) and Art. 68(3)). Additionally, the first draft of the Code of Practice specifically mentions benchmarks as an example of best-in-class evaluations for risk assessment measures for providers of GPAI models with systemic risks \cite{1stDraftCoP2024}. 

%\textcolor{red}{Joao PLEASE Revise -Although not explicitly, it is expected that benchmarks, both quantitative and qualitative, will play a significant role in the Digital Services Act (DSA) \cite{DSA2022}, especially following the approval of the Delegated Act on access to online platform data for vetted researchers pursuant to Article 40 \cite{DraftDA-2024}. The development of this Delegated Act will provide an unprecedented level of meaningful data access to researchers. The provision of adequate benchmarks will be key to assessing the systemic risks of online platforms.} 

% Relatedly, the Digital Services Act (DSA) \cite{DSA2022} identifies systemic risks (Article 34) in the EU  stemming from the massive reach of very large online platforms and search engines, most of which heavily rely on AI -- and increasingly on GPAI -- to curate, filter and/or rank content to several millions of users. Providers of such online services are required to regularly perform risk assessments, for which they are audited by independent entities and widely scrutinised by the research community, given the unprecedented level of access offered to researchers through Article 40 of the DSA. Benchmarks have the potential to provide crucial information for all stakeholders in the ecosystem (platform owners, regulators, auditors, researchers, users and content providers).

% We can also find examples in the US, where the AI Executive Order \cite{USAIEO2023} includes benchmarks for evaluating and auditing AI capabilities, with a focus on capabilities through which AI could cause harm, as part of the guidelines, standards, and best practices for AI Safety and Security (Section 4.1). In addition, the NIST AI Risk Management Framework \cite{NIST2024} incorporates several suggested actions that emphasise benchmarks to address some of the identified risks of generative AI. 

% JV (suggestion, if we have space): Other regulatory initiatives are taking place in many parts of the world, with varying architectures and currently in different implementation stages. There are also multilateral efforts to coordinate the governance of AI worldwide (ref OECD, UN,...). Benchmarking is likely to play a fundamental role in most, if not all of these initiatives. 
