\section{METHODOLOGY}

Since benchmarks constitute a key element in most papers that discuss or demonstrate the capabilities of AI models, it is challenging to identify previous works that explicitly and thoroughly address \textit{benchmark critique}, as opposed to applying benchmarks or proposing new methods. When gathering source materials for this review (and instead of conducting keyword searches in research databases) we therefore opted for a method that involved starting from a selection of well-cited papers that problematise how, when, and where benchmarks are used. For each article, we surveyed its reference list and studied how the paper has been cited since its publication. This allowed us to identify additional relevant literature and expand our corpus successively. 

In the process, we surveyed a wide range of papers that, for instance, highlight general problems with the production of AI datasets \cite{mitchell2019,orr2024b}, propose alternative ways of designing benchmark leaderboards \cite{rodriguez2021,liu2021}, or discuss the wider use of proxies in tests and evaluations \cite{mulvin2021,pinch1993,marres2020}.
Such papers provide important contextual insights to discussions concerning benchmarks. However, they were omitted from what we considered our core collection of previous research. This collection ended up consisting of about 100 papers and articles that \textit{explicitly} and \textit{primarily} highlight issues with benchmarks. Notably, articles that propose new benchmarks were \textit{not} added to this collection by default, even though such articles naturally contain some level of benchmark critique.
The reasoning behind this is twofold. On the one hand, it was necessary to limit the size and scope of our review. On the other hand, many articles proposing new benchmarks (if not most of them) do not question or discuss many (if any) of the underlying assumptions that benchmark assessments rely on.
In that sense, they reproduce the general notion that quantitative benchmarks provide a reasonable technical "fix" to issues with AI safety and capability assessments, rather than taking part in more fundamental and critical discussions on current limitations in benchmark use and design. For these reasons, they were also excluded from our core collection of benchmark critique. 

Our collection was further limited by only containing articles published between 1\textsuperscript{st} January 2014 and 31\textsuperscript{st} December 2024. While it is true that AI models have been tested and evaluated since their origins in the mid 1950's (with the Turing test serving as an iconic example), we limit our review to this decade long period, since 2014 marks the starting point for recent intensifications in AI research and development. We consider both published and pre-published papers in our review, which on a practical level involved categorising identified critique into relevant areas of concern, without a pre-existing target of final categories. When important concepts (such as sandbagging) appeared in the literature, targeted searches for research in these fields were made. Given that a majority of existing AI benchmarks (and benchmark critique) address text-based AI models, we also made special efforts to include discussions concerning benchmarks for images, sound, and audiovisual content. % Notably, however, \textcolor{red}{we could only find X papers critiquing benchmark practices in the field of sound and x papers for moving images} - which signals an urgent need for further critical research in these fields. 

Our resulting meta-review is not exhaustive but it covers a broad range of critique that has been aimed at benchmarking practices. In the following discussions, we focus on works that voice critique with particular relevance for policy makers and policy implementation. We also shed light on works that emphasise areas of concern that cut across different modalities (text, images, sound, moving images) and point toward fundamental weaknesses in the design and application of benchmarks, as opposed to research that critiques individual benchmarks. Our goal has further been to provide a \textit{diverse} account of concerns regarding benchmarks. Hence, some sections will have ample publications reinforcing their claims while others, that voice more recent and/or unique critique, are backed up by fewer sources. 

% ME: Due to a lack of time (I started but it would take forever), we omit presenting an overview of the publication years, academic fields etc. For instance, I had the idea here to categorize our analyzed articles according to "academic affiliation", but this was simply too much work since so many publications are written by interdisciplinary teams of scholars etc. 
