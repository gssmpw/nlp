\section{RELATED WORK}
% ME: cut out to save space: After discussing the essential role of assessing AI benchmarks, this section explores previous studies that have methodically examined and critiqued current benchmarking practices, providing an overview of the significant contributions in this area.
Over the years, several surveys and meta-reviews have set out to summarize discussions on limitations in AI benchmarking and this paper represents a continuation of such efforts. % Important examples of recent research includes \citet{liao2021}'s meta-review of evaluation failures across machine learning, published in 2021, \citet{hutchinson_evaluation_2022}'s study on evaluation gaps in machine learning practice from 2022, and \citet{gehrmann2023}'s survey of obstacles in evaluation practices for generated texts, released in 2023.
In 2021, \citet[p.~1]{liao2021} identified a wide range of "surprisingly consistent critique" directed at benchmarking practices across fields such as computer vision, natural language processing, recommender systems, reinforcement learning, graph processing, metric learning, and more. The authors present a taxonomy of observed benchmark failure modes, which for example includes implementation variations (due to benchmark algorithms, metrics, and libraries being applied in slightly different ways), errors in test set construction (as a result of label errors, label leakage, test set size, or contaminated data), overfitting from test set reuse, and comparisons to inadequate baselines. Along similar lines, \citet{hutchinson_evaluation_2022} survey discussions on evaluation practices in machine learning, particularly in the fields of computer vision and natural language processing. The authors identify eight key evaluation gaps, including topics such as "neglect of model interpretability" and "oversimplification of knowledge" and argue for a shift toward application-centric evaluations that account for safety, fairness, and ecological validity. Similar points are also raised by \citet{gehrmann2023}'s most recent meta-review, which categorizes issues with text-oriented benchmarks identified over the past two decades, and propose a long-term vision for improving evaluation practices, emphasizing the need for comprehensive evaluation reports that include multiple datasets, metrics, and human assessments. Their key recommendations include focusing on model limitations, enhancing dataset documentation, and adopting a more nuanced approach to evaluation to better characterize model performance and capabilities.

Our review revisits these areas of critique and traces how debates concerning benchmarks have evolved in the last few years. This is not least necessary due to rapid developments currently seen in the field. Recent research has identified an unprecedented growth in the release of AI benchmarks, especially in the area of safety, starting from 2023 and onwards~\cite{rottger2024}. Our research indicates a similar increase in publications expressing benchmark critique. For instance,~\citet{rottger2024} found that roughly 46\% of the AI safety benchmarks they identified as relevant in their survey of the field had been produced in 2023, with as many as 15 new datasets being released during the first two months of 2024. Likewise, almost 55\% of the articles we identified as relevant during the course of our research had been released in 2023 or later. \me{insert potential (tiny) graph here, showing distribution over publication years} Given this rapid increase in attention given to the topic, our primary goal is to extend existing meta-reviews into the present. Whereas previous meta-reviews have tended to focus on critique raised in a limited number of domains (primarily natural language processing and machine learning) we also include interdisciplinary accounts from the humanities and social sciences in our review. Furthermore, we provide insights concerning more recently identified areas of benchmark concern, such as works revealing that AI models can be programmed to underperform on benchmarking tests (a problem known as "sandbagging")~\cite{weij2024}, and studies presenting evidence that AI safety benchmarks strongly correlate with upstream AI capabilities (causing reasons to worry about AI "safetywashing") \cite{ren2024}, insights that both raise additional questions about the validity and trustworthiness of current benchmarking practices. 


% ME: If there is space, we could perhaps also connect our work to broader reviews and summaries of issues in AI dataset development and use, such as Paullada et al.'s reports on how AI datasets repeatedly suffer from a lack of documentation, annotation, and a lack of ethical considerations in data collection \cite{paullada2021}.

% ME: I'm cutting this out since Church and Hestness survey AI evaluations writ large, and not \textit{benchmark critique}: One important example of earlier contributions dates back to 2019, when \citeauthor{church_survey_2019}~\cite{church_survey_2019} provided a comprehensive survey of 25 years of evaluation trends in natural language processing (NLP) and machine learning (ML), emphasizing the evolution from minimal evaluation practices to a focus on robust metrics and shared tasks. They discussed the importance of reliability and validity and the risks of over-reliance on metrics without deeper insights by expressing concerns about the trend toward mindless metrics and the potential neglect of understanding how models work. Their paper advocates for a balanced evaluation approach that values both quantitative metrics and qualitative insights while emphasizing the need for collaboration between different fields to develop general-purpose solutions. This interdisciplinary perspective, which we also follow in our study, is crucial for advancing the effectiveness of AI models and ensuring that evaluations are meaningful and applicable to real-world tasks.

% ME: condensed and summarized above: On the same research line, \citeauthor{hutchinson_evaluation_2022}~\cite{hutchinson_evaluation_2022} critically investigated the evaluation practices in ML, particularly in the fields of computer vision and NLP. They reviewed 107 research papers to identify common evaluation failures, highlighting a reliance on traditional metrics like accuracy without considering variance, context, or real-world applicability. The authors pointed out eight key evaluation gaps, including ``neglect of model interpretability'' and ``oversimplification of knowledge''. They argue for a shift toward application-centric evaluations that account for safety, fairness, and ecological validity to enrich the existing scenarios and enhance ML models' trustworthiness and societal impact.

% ME: condensed and summarized above: Concerning NLP domain, \citeauthor{gehrmann2023}~\cite{gehrmann2023} addressed significant flaws in the evaluation practices for natural language generation (NLG) models. Their paper critiques traditional evaluation methods, both human and automated, for their inability to accurately assess the quality of the generated text, especially with the advancements in neural models. The authors categorize the issues identified over the past two decades and propose a long-term vision for improving evaluation practices, emphasizing the need for comprehensive evaluation reports that include multiple datasets, metrics, and human assessments. Key recommendations include focusing on model limitations, enhancing dataset documentation, and adopting a more nuanced approach to evaluation to better characterize model performance and capabilities. The paper calls for a shift in the NLG community toward more effective evaluation methods to enhance the reliability and applicability of NLG systems.

% ME: this study is also technically not a meta-review of previous critique aimed at benchmark practices so I'm cutting it out: Recently, \citeauthor{xu2024}~\cite{xu2024} analyzed the critical issue of Benchmark Data Contamination (BDC) in the evaluation of Large Language Models (LLMs) such as GPT-4 and Claude-3. BDC occurs when models inadvertently incorporate information from evaluation benchmarks into their training data, leading to skewed performance metrics. The authors classify BDC into four types (i.e., semantic, information, data, and label level), each with increasing severity. Moreover, they reviewed the existing detection techniques and mitigation strategies, including regenerating benchmark data and adopting benchmark-free evaluation methods.\an{what does benchmark-free refer to? This seems to refer to concepts of models-as-a-judge or human in the loop in the cited paper, which is inconsistent with our definition of a benchmark, so I think its useful to more clearly define what benchmark-free means here. Alternatively, and perhaps a better way out of this is to just use llm-as-judge as human-in-the-loop directly rather than the term "benchmark-free" to avoid using conflicting and/or confusing notions} The paper underlines the necessity for innovative solutions to ensure reliable evaluations of LLMs in real-world applications and outlines future research directions to tackle BDC effectively.

% ME: shortened and condensed above: Another point of view in existing related contributions concerns the analysis of critiques on benchmark datasets.\citeauthor{paullada2021}~\cite{paullada2021} conducted a survey of benchmark dataset development and use examining their pivotal role in shaping ML outcomes and research trajectories. This paper highlights significant ethical concerns by underlining how early benchmark critiques tended to focus on faulty heuristics in their design. The authors emphasize the need for more intentional dataset practices, advocating for improved documentation, annotation, and ethical considerations in data collection. They call for a cultural shift in machine learning towards responsible dataset creation that respects the rights of data subjects and prioritizes ethical standards over performance metrics, ultimately aiming for more equitable and effective AI systems.

% ME: Concerning the analysis of the inaccurate usage of benchmark datasets, \citeauthor{park2022}~\cite{park2022} recently critiqued the documentation practices of dataset sharing platforms in NLP, specifically focusing on HuggingFace, PaperswithCode, TensorFlow, and PyTorch. They identify key issues such as confusion over the term ``curator'', insufficient documentation on dataset limitations and social impacts, and a lack of metadata regarding social implications. In particular, they advocate for including ``social impact metadata'', which would encompass demographic statistics and annotator demographics, to better inform users about potential biases and societal effects.

% ME: perhaps also worth mentioning: Bartz-Beielstein, Thomas, Carola Doerr, Daan van den Berg, Jakob Bossek, Sowmya Chandrasekaran, Tome Eftimov, Andreas Fischbach, et al. “Benchmarking in Optimization: Best Practice and Open Issues.” arXiv, December 16, 2020. http://arxiv.org/abs/2007.03488.

% ME: Thanks for this Erasmo! We should probably also mention the previous research I had already gathered in the old AI Benchmarks SOTA Overleaf. I'm copy pasting this text below. Also, I think it would be great to try to position ourselves against this previous research in a clearer way... How do we complement - and add to - this set of research? 

% ME: P.S. Obviously, the text below can be shortened:

% In 2020, Paullada et.al. conducted a survey of benchmark dataset development and use, highlighting how early benchmark critique tended to focus on faulty heuristics in the design of benchmarks and issues concerning dataset collection, annotation, documentation, management, and distribution \cite{paullada2020}. Accordingly, the authors review discussions concerning bias and representativeness, alongside discussions such as the risk that AI models may take advantage of shortcuts and spurious clues in benchmark datasets. 

% Our review revisits these areas of critique and traces how debates concerning benchmarks have evolved since 2020. It also provides insights concerning more recent areas of critique, such as issues relating to train-test overlap, sandbagging, and correlations with upstream capability. 

% In 2021 Liao et.al. published a meta-review of evaluation failures across machine learning systems and found a wide range of "surprisingly consistent critique" across fields such as computer vision, natural language processing, recommender systems, reinforcement learning, graph processing, metric learning, and more \cite{liao2021}. After reviewing 107 papers, the authors present a taxonomy of observed benchmark failure modes which builds on a distinction between two categories of failures: those relating to internal validity (i.e., issues concerning singular benchmarks) and those relating to external validity (i.e., issues relating to whether progress on a specific benchmark can be transferred to other problems). Concerning internal validity failures, Liao et.al. highlight implementation variations (as a result of benchmark algorithms, metrics, and libraries being applied in slightly different ways), errors in test set construction (as a result of label errors, label leakage, test set size, or contaminated data), overfitting from test set reuse, and comparisons to inadequate baselines. Concerning external validity failures, the authors highlight errors relating to types of transfer (i.e., flawed assumptions about how algorithms and learning problems can be generalized and transferred into other domains), metrics misalignment (f.ex. resulting from the incapacity of a metric to adequately distinguish between algorithms that perform differently), comparisons to human performance (as benchmarks are surrounded by unreasonable performance claims), dataset misalignment (as benchmark datasets rely on simple, inappropriate heuristics, do not map well to variations in the real world, or contain biases and disagreements).

% Given rapid AI developments, we extend Liao et.al.'s meta-review into the present. We also include insights and critique from the humanities and social sciences in our meta-review, seeing how cultural, political, and historical discussions concerning benchmark tests are crucial for understanding the potential risks with AI safety testing. 