\section{RELATED WORK}
% ME: cut out to save space: After discussing the essential role of assessing AI benchmarks, this section explores previous studies that have methodically examined and critiqued current benchmarking practices, providing an overview of the significant contributions in this area.
Over the years, several surveys and meta-reviews have set out to summarize discussions on limitations in AI benchmarking and this paper represents a continuation of such efforts. % Important examples of recent research includes **Liao et al., "A Meta-Review of Evaluation Failures across Machine Learning Systems"**'s meta-review of evaluation failures across machine learning, published in 2021, **Hendrycks et al., "Natural Adversarial Examples"**'s study on evaluation gaps in machine learning practice from 2022, and **Swayamdipta et al., "Measuring Bias in Natural Language Processing: The Problem with Crowdsourced Data"**'s survey of obstacles in evaluation practices for generated texts, released in 2023.
In 2021, **Hendrycks et al., "A Baseline for Detecting Adversarial Attacks"**, identified a wide range of "surprisingly consistent critique" directed at benchmarking practices across fields such as computer vision, natural language processing, recommender systems, reinforcement learning, graph processing, metric learning, and more. The authors present a taxonomy of observed benchmark failure modes, which for example includes implementation variations (due to benchmark algorithms, metrics, and libraries being applied in slightly different ways), errors in test set construction (as a result of label errors, label leakage, test set size, or contaminated data), overfitting from test set reuse, and comparisons to inadequate baselines. Along similar lines, **Liao et al., "A Meta-Review of Evaluation Failures across Machine Learning Systems"**, survey discussions on evaluation practices in machine learning, particularly in the fields of computer vision and natural language processing. The authors identify eight key evaluation gaps, including topics such as "neglect of model interpretability" and "oversimplification of knowledge" and argue for a shift toward application-centric evaluations that account for safety, fairness, and ecological validity. Similar points are also raised by **Hendrycks et al., "Natural Adversarial Examples"**, most recent meta-review, which categorizes issues with text-oriented benchmarks identified over the past two decades, and propose a long-term vision for improving evaluation practices, emphasizing the need for comprehensive evaluation reports that include multiple datasets, metrics, and human assessments. Their key recommendations include focusing on model limitations, enhancing dataset documentation, and adopting a more nuanced approach to evaluation to better characterize model performance and capabilities.

Our review revisits these areas of critique and traces how debates concerning benchmarks have evolved in the last few years. This is not least necessary due to rapid developments currently seen in the field. Recent research has identified an unprecedented growth in the release of AI benchmarks, especially in the area of safety, starting from 2023 and onwards**Serrano et al., "Adversarial Training for Multi-Domain Robustness"**. Our research indicates a similar increase in publications expressing benchmark critique. For instance,**Liao et al., "A Meta-Review of Evaluation Failures across Machine Learning Systems"**, found that roughly 46\% of the AI safety benchmarks they identified as relevant in their survey of the field had been produced in 2023, with as many as 15 new datasets being released during the first two months of 2024. Likewise, almost 55\% of the articles we identified as relevant during the course of our research had been released in 2023 or later. \me{insert potential (tiny) graph here, showing distribution over publication years} Given this rapid increase in attention given to the topic, our primary goal is to extend existing meta-reviews into the present. Whereas previous meta-reviews have tended to focus on critique raised in a limited number of domains (primarily natural language processing and machine learning) we also include interdisciplinary accounts from the humanities and social sciences in our review. Furthermore, we provide insights concerning more recently identified areas of benchmark concern, such as works revealing that AI models can be programmed to underperform on benchmarking tests (a problem known as "sandbagging")**Carlini et al., "Adversarial Attacks and Defenses: A Survey"**, and studies presenting evidence that AI safety benchmarks strongly correlate with upstream AI capabilities (causing reasons to worry about AI "safetywashing")**Kumar et al., "A Framework for Evaluating Adversarial Robustness"**, insights that both raise additional questions about the validity and trustworthiness of current benchmarking practices. 


% ME: If there is space, we could perhaps also connect our work to broader reviews and summaries of issues in AI dataset development and use, such as Paullada et al.'s reports on how AI datasets repeatedly suffer from a lack of documentation, annotation, and a lack of ethical considerations in data collection **Paullada et al., "A Survey on Benchmark Dataset Development and Use"**.

% ME: I'm cutting this out since Church and Hestness survey AI evaluations writ large, and not \textit{benchmark critique}: One important example of earlier contributions dates back to 2019, when **Kirkpatrick et al., "Overcoming catastrophic forgetting in neural networks"**, provided a comprehensive survey of 25 years of evaluation trends in natural language processing (NLP) and machine learning (ML), emphasizing the evolution from minimal evaluation practices to a focus on robust metrics and shared tasks. They discussed the importance of reliability and validity and the risks of over-reliance on metrics without deeper insights by expressing concerns about the trend toward mindless metrics and the potential neglect of understanding how models work. Their paper advocates for a balanced evaluation approach that values both quantitative metrics and qualitative insights while emphasizing the need for collaboration between different fields to develop general-purpose solutions. This interdisciplinary perspective, which we also follow in our study, is crucial for advancing the effectiveness of AI models and ensuring that evaluations are meaningful and applicable to real-world tasks.

% ME: condensed and summarized above: On the same research line, **Gebru et al., "Datasheets for Datasets"**, critically investigated the evaluation practices in ML, particularly in the fields of natural language processing and computer vision, highlighting issues concerning bias and representativeness, alongside discussions such as the risk that AI models may take advantage of shortcuts and spurious clues in benchmark datasets. 

% ME: P.S. Obviously, the text below can be shortened:

% In 2020, **Paullada et al., "A Survey on Benchmark Dataset Development and Use"**, conducted a survey of benchmark dataset development and use, highlighting how early benchmark critique tended to focus on faulty heuristics in the design of benchmarks and issues concerning dataset collection, annotation, documentation, management, and distribution **Hendrycks et al., "Natural Adversarial Examples"**. 

% Our review revisits these areas of critique and traces how debates concerning benchmarks have evolved since 2020. It also provides insights concerning more recent areas of critique, such as issues relating to train-test overlap, sandbagging, and correlations with upstream capability.