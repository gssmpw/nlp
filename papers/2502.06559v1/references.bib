@article{NIST2024,
	author ={{NIST}},
	year ={2024},
	doi ={10.6028/NIST.AI.600-1},
	title ={{Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile}},
    journal={NIST Trustworthy and Responsible AI NIST AI 600-1} 
}

@misc{USAIDiff2025,
	author ={{US Department of Commerce}},
	year ={2025},
	publisher ={https://www.govinfo.gov/content/pkg/FR-2025-01-15/pdf/2025-00636.pdf},
	title ={{Framework for Artificial Intelligence Diffusion}}
}

@misc{USAIEO2023,
	author ={{The White House}},
	year ={2023},
	publisher ={https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/},
	title ={{Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence}}
}

@misc{DraftDA-2024,
	author ={{European Union}},
	year ={2024},
	publisher ={https://ec.europa.eu/info/law/better-regulation/have-your-say/initiatives/13817-Delegated-Regulation-on-data-access-provided-for-in-the-Digital-Services-Act_en},
	title ={{Delegated Regulation on data access provided for in the Digital Services Act}}
}

@misc{DSA2022,
	author ={{European Union}},
	year ={2022},
	publisher ={https://eur-lex.europa.eu/eli/reg/2022/2065/oj/eng},
	title ={{Regulation (EU) 2022/2065 of the European Parliament and of the Council of 19 October 2022 on a Single Market For Digital Services and amending Directive 2000/31/EC (Digital Services Act)}}
}

@misc{1stDraftCoP2024,
	author ={{European Union}},
	year ={2024},
	publisher ={https://digital-strategy.ec.europa.eu/en/library/first-draft-general-purpose-ai-code-practice-published-written-independent-experts},
	title ={{First Draft of the General-Purpose AI Code of Practice published, written by independent experts}}
}

@misc{2ndDraftCoP2024,
	author ={{European Union}},
	year ={2024},
	publisher ={https://digital-strategy.ec.europa.eu/en/library/second-draft-general-purpose-ai-code-practice-published-written-independent-experts},
	title ={{Second Draft of the General-Purpose AI Code of Practice published, written by independent experts}}
}

@misc{UKSafetyAct2023,
    author = {{UK Parliament}},
    year = {2023},
    publisher = {https://www.legislation.gov.uk/ukpga/2023/50/contents},
    title = {{Online Safety Act 2023}}
}

@misc{cottier2024,
      title={The rising costs of training frontier AI models}, 
      author={Ben Cottier and Robi Rahman and Loredana Fattorini and Nestor Maslej and David Owen},
      year={2024},
      eprint={2405.21015},
      archivePrefix={arXiv},
      primaryClass={cs.CY},
      url={https://arxiv.org/abs/2405.21015}, 
}

@misc{sun2024hunyuanlarge,
      title={Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated Parameters by Tencent}, 
      author={Xingwu Sun and Yanfeng Chen and Yiqing Huang and Ruobing Xie and Jiaqi Zhu and et al.},
      year={2024},
      eprint={2411.02265},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2411.02265}, 
}

@misc{llama3herd,
      title={The Llama 3 Herd of Models}, 
      author="{Llama Team, AI \@ Meta}",
      year={2024},
      eprint={2407.21783},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.21783}, 
}

@misc{Ruan2024,
      title={Observational Scaling Laws and the Predictability of Language Model Performance}, 
      author={Yangjun Ruan and Chris J. Maddison and Tatsunori Hashimoto},
      year={2024},
      eprint={2405.10938},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.10938}, 
}

@inproceedings{Hoffmann2022,
author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and de Las Casas, Diego and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and Hennigan, Tom and Noland, Eric and Millican, Katie and van den Driessche, George and Damoc, Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan, Karen and Elsen, Erich and Vinyals, Oriol and Rae, Jack W. and Sifre, Laurent},
title = {Training compute-optimal large language models},
year = {2024},
isbn = {9781713871088},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
booktitle = {Proceedings of the 36th International Conference on Neural Information Processing Systems},
articleno = {2176},
numpages = {15},
location = {New Orleans, LA, USA},
series = {NIPS '22}
}

  

@misc{Seger2023,
    author= {Elizabeth Seger and Noemi Dreksler and Richard Moulange and Emily Dardaman and Jonas Schuett and et al.},
    year  = {2023},
    title = {Open-Sourcing Highly Capable Foundation Models: An evaluation of risks, benefits, and alternative methods for pursuing open-source objectives},
    publisher = "{Centre for the Governance of AI}"
}

@misc{GlobalAI2024,
    author= {Loredana Fattorini and Nestor Maslej and Raymond Perrault and Vanessa Parli and John Etchemendy and Yoav Shoham and Katrina Liget},
    year  = {2024},
    title = {The Global AI Vibrancy Tool},
    note  = {\url{https://aiindex.stanford.edu/vibrancy/}, 
             Last accessed on 2024-12-12}
}

Loredana Fattorini and Nestor Maslej and Raymond Perrault and Vanessa Parli and John Etchemendy and Yoav Shoham
Katrina Liget

@misc{Zhijia2024,
    author= {Lin Zhijia},
    year  = {2024},
    title = {Top LLMs in China and the U.S. Only 5 Months Apart: Kai-Fu Lee},
    publisher = "{TMTPOST}",
    url = {https://en.tmtpost.com/post/7289212}
}


@misc{Stewart2023,
    author= {Matthew Stewart},
    year  = {2023},
    title = {The Olympics of AI: Benchmarking Machine Learning Systems},
    publisher = {Medium},
    url = {https://towardsdatascience.com/the-olympics-of-ai-benchmarking-machine-learning-systems-c4b2051fbd2b}
}


@misc{weidinger_sociotechnical_2023,
	title = {Sociotechnical {Safety} {Evaluation} of {Generative} {AI} {Systems}},
	url = {http://arxiv.org/abs/2310.11986},
	language = {en},
	urldate = {2024-11-22},
	publisher = {arXiv},
	author = {Weidinger, Laura and Rauh, Maribeth and Marchal, Nahema and Manzini, Arianna and Hendricks, Lisa Anne and Mateos-Garcia, Juan and Bergman, Stevie and Kay, Jackie and Griffin, Conor and Bariach, Ben and Gabriel, Iason and Rieser, Verena and Isaac, William},
	month = oct,
	year = {2023},
	keywords = {Benchmarks, Systemic risk, AI and safety, Safety},
	annote = {Comment: main paper p.1-29, 5 figures, 2 tables},
	file = {Weidinger et al. - 2023 - Sociotechnical Safety Evaluation of Generative AI .pdf:C\:\\Users\\PURIFER\\Zotero\\storage\\9SMFS7GK\\Weidinger et al. - 2023 - Sociotechnical Safety Evaluation of Generative AI .pdf:application/pdf},
}

@inproceedings{reuel_betterbench_2024,
    title={BetterBench: Assessing {AI} Benchmarks, Uncovering Issues, and Establishing Best Practices},
    author={Anka Reuel and Amelia Hardy and Chandler Smith and Max Lamparth and Malcolm Hardy and Mykel Kochenderfer},
    booktitle={The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
    year={2024},
    url={https://openreview.net/forum?id=hcOq2buakM}
}

@misc{rottger2024,
	type = {Language models},
	title = {{SafetyPrompts}: a {Systematic} {Review} of {Open} {Datasets} for {Evaluating} and {Improving} {Large} {Language} {Model} {Safety}},
	shorttitle = {{SafetyPrompts}},
	url = {http://arxiv.org/abs/2404.05399},
	abstract = {The last two years have seen a rapid growth in concerns around the safety of large language models (LLMs). Researchers and practitioners have met these concerns by introducing an abundance of new datasets for evaluating and improving LLM safety. However, much of this work has happened in parallel, and with very different goals in mind, ranging from the mitigation of near-term risks around bias and toxic content generation to the assessment of longer-term catastrophic risk potential. This makes it difficult for researchers and practitioners to find the most relevant datasets for a given use case, and to identify gaps in dataset coverage that future work may fill. To remedy these issues, we conduct a first systematic review of open datasets for evaluating and improving LLM safety. We review 102 datasets, which we identified through an iterative and community-driven process over the course of several months. We highlight patterns and trends, such as a a trend towards fully synthetic datasets, as well as gaps in dataset coverage, such as a clear lack of non-English datasets. We also examine how LLM safety datasets are used in practice – in LLM release publications and popular LLM benchmarks – finding that current evaluation practices are highly idiosyncratic and make use of only a small fraction of available datasets. Our contributions are based on SafetyPrompts.com, a living catalogue of open datasets for LLM safety, which we commit to updating continuously as the field of LLM safety develops.},
	language = {en},
	urldate = {2024-11-14},
	publisher = {arXiv},
	author = {Röttger, Paul and Pernisi, Fabio and Vidgen, Bertie and Hovy, Dirk},
	month = apr,
	year = {2024},
	keywords = {Benchmarks},
	file = {Röttger et al. - 2024 - SafetyPrompts a Systematic Review of Open Dataset.pdf:C\:\\Users\\PURIFER\\Zotero\\storage\\9XWXD5CR\\Röttger et al. - 2024 - SafetyPrompts a Systematic Review of Open Dataset.pdf:application/pdf},
}

@misc{AIA24,
	author ={{European Union}},
	year ={2024},
	publisher ={https://eur-lex.europa.eu/eli/reg/2024/1689/oj},
	title ={{Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (Artificial Intelligence Act)}}
}

@ARTICLE{Martinez-Plumed2020,
  author={Martinez-Plumed, Fernando and Hernández-Orallo, José},
  journal={IEEE Transactions on Games}, 
  title={Dual Indicators to Analyze AI Benchmarks: Difficulty, Discrimination, Ability, and Generality}, 
  year={2020},
  volume={12},
  number={2},
  pages={121-131},
  keywords={Artificial intelligence;Games;Benchmark testing;Task analysis;Adaptation models;Guidelines;Indexes;Artificial intelligence (AI) benchmarks;AI evaluation;generality;item response theory (ITR)},
  doi={10.1109/TG.2018.2883773}
}

@misc{schlangen_targeting_2020,
	title = {Targeting the {Benchmark}: {On} {Methodology} in {Current} {Natural} {Language} {Processing} {Research}},
	shorttitle = {Targeting the {Benchmark}},
	url = {http://arxiv.org/abs/2007.04792},
	abstract = {It has become a common pattern in our field: One group introduces a language task, exemplified by a dataset, which they argue is challenging enough to serve as a benchmark. They also provide a baseline model for it, which then soon is improved upon by other groups. Often, research efforts then move on, and the pattern repeats itself. What is typically left implicit is the argumentation for why this constitutes progress, and progress towards what. In this paper, we try to step back for a moment from this pattern and work out possible argumentations and their parts.},
	language = {en},
	urldate = {2024-11-25},
	publisher = {arXiv},
	author = {Schlangen, David},
	month = jul,
	year = {2020},
	keywords = {Benchmarks},
	file = {Schlangen - 2020 - Targeting the Benchmark On Methodology in Current.pdf:C\:\\Users\\ferdavi\\Zotero\\storage\\HEXK3EXK\\Schlangen - 2020 - Targeting the Benchmark On Methodology in Current.pdf:application/pdf},
}

@ARTICLE{Russakovsky2015,
  author={Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
  journal={International Journal of Computer Vision}, 
  title={ImageNet Large Scale Visual Recognition Challenge}, 
  year={2015},
  volume={115},
  pages={211-252},
  doi={10.1007/s11263-015-0816-y}
}

@inproceedings{Mazumder2023,
 author = {Mazumder, Mark and Banbury, Colby and Yao, Xiaozhe and Karla\v{s}, Bojan and Gaviria Rojas, William and Diamos, Sudnya and Diamos, Greg and He, Lynn and Parrish, Alicia and Kirk, Hannah Rose and Quaye, Jessica and Rastogi, Charvi and Kiela, Douwe and Jurado, David and Kanter, David and Mosquera, Rafael and Cukierski, Will and Ciro, Juan and Aroyo, Lora and Acun, Bilge and Chen, Lingjiao and Raje, Mehul and Bartolo, Max and Eyuboglu, Evan Sabri and Ghorbani, Amirata and Goodman, Emmett and Howard, Addison and Inel, Oana and Kane, Tariq and Kirkpatrick, Christine R. and Sculley, D. and Kuo, Tzu-Sheng and Mueller, Jonas W and Thrush, Tristan and Vanschoren, Joaquin and Warren, Margaret and Williams, Adina and Yeung, Serena and Ardalani, Newsha and Paritosh, Praveen and Zhang, Ce and Zou, James Y and Wu, Carole-Jean and Coleman, Cody and Ng, Andrew and Mattson, Peter and Janapa Reddi, Vijay},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {5320--5347},
 publisher = {Curran Associates, Inc.},
 title = {DataPerf: Benchmarks for Data-Centric AI Development},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/112db88215e25b3ae2750e9eefcded94-Paper-Datasets_and_Benchmarks.pdf},
 volume = {36},
 year = {2023}
}


@inproceedings{Gadre2024,
author = {Gadre, Samir Yitzhak and Ilharco, Gabriel and Fang, Alex and Hayase, Jonathan and Smyrnis, Georgios and Nguyen, Thao and Marten, Ryan and Wortsman, Mitchell and Ghosh, Dhruba and Zhang, Jieyu and Orgad, Eyal and Entezari, Rahim and Daras, Giannis and Pratt, Sarah and Ramanujan, Vivek and Bitton, Yonatan and Marathe, Kalyani and Mussmann, Stephen and Vencu, Richard and Cherti, Mehdi and Krishna, Ranjay and Koh, Pang Wei and Saukh, Olga and Ratner, Alexander and Song, Shuran and Hajishirzi, Hannaneh and Farhadi, Ali and Beaumont, Romain and Oh, Sewoong and Dimakis, Alex and Jitsev, Jenia and Carmon, Yair and Shankar, Vaishaal and Schmidt, Ludwig},
title = {DataComp: in search of the next generation of multimodal datasets},
year = {2024},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
booktitle = {Proceedings of the 37th International Conference on Neural Information Processing Systems},
articleno = {1179},
numpages = {21},
location = {New Orleans, LA, USA},
series = {NIPS '23}
}

  

@book{Reddi2024,
  title = {Machine Learning Systems},
  author = {Reddi, Vijay Janapa},
  year = {2024},
  publisher = "{Harvard University}",
  address   = "{School of Engineering and Applied Sciences}"
}

@inproceedings{Mattson2020b,
 author = {Mattson, Peter and Cheng, Christine and Diamos, Gregory and Coleman, Cody and Micikevicius, Paulius and Patterson, David and Tang, Hanlin and Wei, Gu-Yeon and Bailis, Peter and Bittorf, Victor and Brooks, David and Chen, Dehao and Dutta, Debo and Gupta, Udit and Hazelwood, Kim and Hock, Andy and Huang, Xinyuan and Kang, Daniel and Kanter, David and Kumar, Naveen and Liao, Jeffery and Narayanan, Deepak and Oguntebi, Tayo and Pekhimenko, Gennady and Pentecost, Lillian and Janapa Reddi, Vijay and Robie, Taylor and St John, Tom and Wu, Carole-Jean and Xu, Lingjie and Young, Cliff and Zaharia, Matei},
 booktitle = {Proceedings of Machine Learning and Systems},
 editor = {I. Dhillon and D. Papailiopoulos and V. Sze},
 pages = {336--349},
 title = {MLPerf Training Benchmark},
 url = {https://proceedings.mlsys.org/paper_files/paper/2020/file/411e39b117e885341f25efb8912945f7-Paper.pdf},
 volume = {2},
 year = {2020}
}


@INPROCEEDINGS{Reddi2020,
  author={Reddi, Vijay Janapa and Cheng, Christine and Kanter, David and Mattson, Peter and Schmuelling, Guenther and Wu, Carole-Jean and Anderson, Brian and Breughe, Maximilien and Charlebois, Mark and Chou, William and Chukka, Ramesh and Coleman, Cody and Davis, Sam and Deng, Pan and Diamos, Greg and Duke, Jared and Fick, Dave and Gardner, J. Scott and Hubara, Itay and Idgunji, Sachin and Jablin, Thomas B. and Jiao, Jeff and John, Tom St. and Kanwar, Pankaj and Lee, David and Liao, Jeffery and Lokhmotov, Anton and Massa, Francisco and Meng, Peng and Micikevicius, Paulius and Osborne, Colin and Pekhimenko, Gennady and Rajan, Arun Tejusve Raghunath and Sequeira, Dilip and Sirasao, Ashish and Sun, Fei and Tang, Hanlin and Thomson, Michael and Wei, Frank and Wu, Ephrem and Xu, Lingjie and Yamada, Koichi and Yu, Bing and Yuan, George and Zhong, Aaron and Zhang, Peizhao and Zhou, Yuchen},
  booktitle={2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)}, 
  title={MLPerf Inference Benchmark}, 
  year={2020},
  volume={},
  number={},
  pages={446-459},
  keywords={Machine Learning;Inference;Benchmarking},
  doi={10.1109/ISCA45697.2020.00045}
}


@ARTICLE{Mattson2020,
  author={Mattson, Peter and Reddi, Vijay Janapa and Cheng, Christine and Coleman, Cody and Diamos, Greg and Kanter, David and Micikevicius, Paulius and Patterson, David and Schmuelling, Guenther and Tang, Hanlin and Wei, Gu-Yeon and Wu, Carole-Jean},
  journal={IEEE Micro}, 
  title={MLPerf: An Industry Standard Benchmark Suite for Machine Learning Performance}, 
  year={2020},
  volume={40},
 number={2},
  pages={8-16},
  doi={10.1109/MM.2020.2974843}
}


@article{church_survey_2019,
    title = {A survey of 25 years of evaluation},
    volume = {25},
    copyright = {http://creativecommons.org/licenses/by/4.0/},
    issn = {1351-3249, 1469-8110},
    url = {https://www.cambridge.org/core/product/identifier/S1351324919000275/type/journal_article},
    doi = {10.1017/S1351324919000275},
    language = {en},
    number = {06},
    urldate = {2024-12-05},
    journal = {Natural Language Engineering},
    author = {Church, Kenneth Ward and Hestness, Joel},
    month = nov,
    year = {2019},
    pages = {753--767},
}

@inproceedings{hutchinson_evaluation_2022,
    address = {Seoul Republic of Korea},
    title = {Evaluation {Gaps} in {Machine} {Learning} {Practice}},
    isbn = {978-1-4503-9352-2},
    url = {https://dl.acm.org/doi/10.1145/3531146.3533233},
    doi = {10.1145/3531146.3533233},
    booktitle = {2022 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
    publisher = {ACM},
    author = {Hutchinson, Ben and Rostamzadeh, Negar and Greer, Christina and Heller, Katherine and Prabhakaran, Vinodkumar},
    month = jun,
    year = {2022},
    pages = {1859--1876},
}

@ARTICLE{Cheng2022,
  author={Cheng, Z. and Pang, CS. and Wang, P. and et al.},
  journal={Nature Electronics}, 
  title={How to report and benchmark emerging field-effect transistors}, 
  year={2020},
  volume={5},  
  pages={416-423},
  url={https://doi.org/10.1038/s41928-022-00798-8}
}

@inproceedings{thakur2021beir,
    title={{BEIR}: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models},
    author={Nandan Thakur and Nils Reimers and Andreas R{\"u}ckl{\'e} and Abhishek Srivastava and Iryna Gurevych},
    booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)},
    year={2021},
    url={https://openreview.net/forum?id=wCu6T5xFjeJ}
}

@ARTICLE{Henning2000,
  author={Henning, J.L.},
  journal={Computer}, 
  title={SPEC CPU2000: measuring CPU performance in the New Millennium}, 
  year={2000},
  volume={33},
  number={7},
  pages={28-35},
  doi={10.1109/2.869367}
}


@article{Aniba2010,
  title="{Issues in bioinformatics benchmarking: the case study of multiple sequence alignment}",
  author={Aniba, M. R. and Poch, O. and Thompson, J. D.},
  journal={Nucleic Acids Research},
  volume={38},
  number={21},
  pages={7353-7363},
  year={2010},
  url={https://doi.org/10.1093/nar/gkq625}
}

@Inbook{Bruno2014,
author="Bruno, Isabelle",
editor="Michalos, Alex C.",
title="Benchmarking",
bookTitle="Encyclopedia of Quality of Life and Well-Being Research",
year="2014",
publisher="Springer Netherlands",
address="Dordrecht",
pages="363--368",
isbn="978-94-007-0753-5",
doi="10.1007/978-94-007-0753-5_170",
url="https://doi.org/10.1007/978-94-007-0753-5_170"
}

@book{Camp1989,
  title = {Benchmarking : The Search for Industry Best Practices That Lead to Superior Performance},
  author = {Robert C. Camp},
  year = {1989},
  publisher = "{Quality Press}",
  address   = "{the University of Michigan}"
}

@online{oed,
  title = {Benchmark. Meaning and Use},
  author = "{Oxford English Dictionary}",
  year = {2017},
  url = {https://www.oed.com/dictionary/benchmark_n?tab=meaning_and_use&tl=true}
}

@misc{besen2024,
	title = {The {Death} of the {Static} {AI} {Benchmark}},
	url = {https://towardsdatascience.com/the-death-of-the-static-ai-benchmark-88b5ff437086},
	language = {en},
	journal = {Towards Data Science},
	author = {Besen, Sandi},
	month = mar,
	year = {2024},
	keywords = {Benchmarks},
}

@misc{birhane2024,
	title = {{AI} auditing: {The} {Broken} {Bus} on the {Road} to {AI} {Accountability}},
	shorttitle = {{AI} auditing},
	url = {http://arxiv.org/abs/2401.14462},
	abstract = {One of the most concrete measures to take towards meaningful AI accountability is to consequentially assess and report the systems’ performance and impact. However, the practical nature of the “AI audit” ecosystem is muddled and imprecise, making it difficult to work through various concepts and map out the stakeholders involved in the practice. First, we taxonomize current AI audit practices as completed by regulators, law firms, civil society, journalism, academia, consulting agencies. Next, we assess the impact of audits done by stakeholders within each domain. We find that only a subset of AI audit studies translate to desired accountability outcomes. We thus assess and isolate practices necessary for effective AI audit results, articulating the observed connections between AI audit design, methodology and institutional context on its effectiveness as a meaningful mechanism for accountability.},
	language = {en},
	urldate = {2024-11-11},
	publisher = {arXiv},
	author = {Birhane, Abeba and Steed, Ryan and Ojewale, Victor and Vecchione, Briana and Raji, Inioluwa Deborah},
	month = jan,
	year = {2024},
	keywords = {Computer Science - Computers and Society},
	annote = {Comment: To appear in the proceedings of the 2nd IEEE Conference on Secure and Trustworthy Machine Learning (SaTML) 2024},
	file = {Birhane et al. - 2024 - AI auditing The Broken Bus on the Road to AI Acco.pdf:C\:\\Users\\PURIFER\\Zotero\\storage\\AFYUH92V\\Birhane et al. - 2024 - AI auditing The Broken Bus on the Road to AI Acco.pdf:application/pdf},
}

@inproceedings{blodgett2021,
	address = {Online},
	title = {Stereotyping {Norwegian} {Salmon}: {An} {Inventory} of {Pitfalls} in {Fairness} {Benchmark} {Datasets}},
	shorttitle = {Stereotyping {Norwegian} {Salmon}},
	url = {https://aclanthology.org/2021.acl-long.81},
	doi = {10.18653/v1/2021.acl-long.81},
	abstract = {Auditing NLP systems for computational harms like surfacing stereotypes is an elusive goal. Several recent efforts have focused on benchmark datasets consisting of pairs of contrastive sentences, which are often accompanied by metrics that aggregate an NLP system’s behavior on these pairs into measurements of harms. We examine four such benchmarks constructed for two NLP tasks: language modeling and coreference resolution. We apply a measurement modeling lens—originating from the social sciences—to inventory a range of pitfalls that threaten these benchmarks’ validity as measurement models for stereotyping. We ﬁnd that these benchmarks frequently lack clear articulations of what is being measured, and we highlight a range of ambiguities and unstated assumptions that affect how these benchmarks conceptualize and operationalize stereotyping.},
	language = {en},
	urldate = {2024-11-07},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Blodgett, Su Lin and Lopez, Gilsinia and Olteanu, Alexandra and Sim, Robert and Wallach, Hanna},
	year = {2021},
	pages = {1004--1015},
	file = {Blodgett et al. - 2021 - Stereotyping Norwegian Salmon An Inventory of Pit.pdf:C\:\\Users\\PURIFER\\Zotero\\storage\\JZ5ICWVT\\Blodgett et al. - 2021 - Stereotyping Norwegian Salmon An Inventory of Pit.pdf:application/pdf},
}

@misc{guldimann2024,
	title = {{COMPL}-{AI} {Framework}: {A} {Technical} {Interpretation} and {LLM} {Benchmarking} {Suite} for the {EU} {Artificial} {Intelligence} {Act}},
	shorttitle = {{COMPL}-{AI} {Framework}},
	url = {http://arxiv.org/abs/2410.07959},
	abstract = {The EU's Artificial Intelligence Act (AI Act) is a significant step towards responsible AI development, but lacks clear technical interpretation, making it difficult to assess models' compliance. This work presents COMPL-AI, a comprehensive framework consisting of (i) the first technical interpretation of the EU AI Act, translating its broad regulatory requirements into measurable technical requirements, with the focus on large language models (LLMs), and (ii) an open-source Act-centered benchmarking suite, based on thorough surveying and implementation of state-of-the-art LLM benchmarks. By evaluating 12 prominent LLMs in the context of COMPL-AI, we reveal shortcomings in existing models and benchmarks, particularly in areas like robustness, safety, diversity, and fairness. This work highlights the need for a shift in focus towards these aspects, encouraging balanced development of LLMs and more comprehensive regulation-aligned benchmarks. Simultaneously, COMPL-AI for the first time demonstrates the possibilities and difficulties of bringing the Act's obligations to a more concrete, technical level. As such, our work can serve as a useful first step towards having actionable recommendations for model providers, and contributes to ongoing efforts of the EU to enable application of the Act, such as the drafting of the GPAI Code of Practice.},
	language = {en},
	urldate = {2024-10-18},
	publisher = {arXiv},
	author = {Guldimann, Philipp and Spiridonov, Alexander and Staab, Robin and Jovanović, Nikola and Vero, Mark and Vechev, Velko and Gueorguieva, Anna and Balunović, Mislav and Konstantinov, Nikola and Bielik, Pavol and Tsankov, Petar and Vechev, Martin},
	month = oct,
	year = {2024},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society},
	file = {Guldimann et al. - 2024 - COMPL-AI Framework A Technical Interpretation and.pdf:C\:\\Users\\PURIFER\\Zotero\\storage\\DV8KAQ9Y\\Guldimann et al. - 2024 - COMPL-AI Framework A Technical Interpretation and.pdf:application/pdf},
}

@misc{hendrycks2021,
	title = {Measuring {Massive} {Multitask} {Language} {Understanding}},
	url = {http://arxiv.org/abs/2009.03300},
	abstract = {We propose a new test to measure a text model’s multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We ﬁnd that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have nearrandom accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model’s academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.},
	language = {en},
	urldate = {2024-10-18},
	publisher = {arXiv},
	author = {Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
	month = jan,
	year = {2021},
	keywords = {Benchmark testing, AI evaluation, LLM's},
	annote = {Comment: ICLR 2021; the test and code is available at https://github.com/hendrycks/test},
}

@article{keegan2024,
	title = {Everyone {Is} {Judging} {AI} by {These} {Tests}. {But} {Experts} {Say} {They}’re {Close} to {Meaningless}},
	url = {https://themarkup.org/artificial-intelligence/2024/07/17/everyone-is-judging-ai-by-these-tests-but-experts-say-theyre-close-to-meaningless},
	language = {en},
	journal = {The Markup},
	author = {Keegan, Jon},
	month = jul,
	year = {2024},
	keywords = {Evaluation metrics, Benchmark testing, Systemic risk, AI evaluation},
}

@misc{koch2021,
	title = {Reduced, {Reused} and {Recycled}: {The} {Life} of a {Dataset} in {Machine} {Learning} {Research}},
	shorttitle = {Reduced, {Reused} and {Recycled}},
	url = {http://arxiv.org/abs/2112.01716},
	abstract = {Benchmark datasets play a central role in the organization of machine learning research. They coordinate researchers around shared research problems and serve as a measure of progress towards shared goals. Despite the foundational role of benchmarking practices in this ﬁeld, relatively little attention has been paid to the dynamics of benchmark dataset use and reuse, within or across machine learning subcommunities. In this paper, we dig into these dynamics. We study how dataset usage patterns differ across machine learning subcommunities and across time from 2015-2020. We ﬁnd increasing concentration on fewer and fewer datasets within task communities, signiﬁcant adoption of datasets from other tasks, and concentration across the ﬁeld on datasets that have been introduced by researchers situated within a small number of elite institutions. Our results have implications for scientiﬁc evaluation, AI ethics, and equity/access within the ﬁeld.},
	language = {en},
	urldate = {2024-10-31},
	publisher = {arXiv},
	author = {Koch, Bernard and Denton, Emily and Hanna, Alex and Foster, Jacob G.},
	month = dec,
	year = {2021},
	keywords = {Benchmark testing, Dataset},
	annote = {Comment: 35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia},
}

@article{orr2024,
	title = {The social construction of datasets: {On} the practices, processes, and challenges of dataset creation for machine learning},
	volume = {26},
	abstract = {Despite the critical role that datasets play in how systems make predictions and interpret the world, the dynamics of their construction are not well understood. Drawing on a corpus of interviews with dataset creators, we uncover the messy and contingent realities of dataset preparation. We identify four key challenges in constructing datasets, including balancing the benefits and costs of increasing dataset scale, limited access to resources, a reliance on shortcuts for compiling datasets and evaluating their quality, and ambivalence regarding accountability for a dataset. These themes illustrate the ways in which datasets are not objective or neutral but reflect the personal judgments and trade-offs of their creators within wider institutional dynamics, working within social, technical, and organizational constraints. We underscore the importance of examining the processes of dataset creation to strengthen an understanding of responsible practices for dataset development and care.},
	language = {en},
	number = {9},
	journal = {New Media \& Society},
	author = {Orr, Will and Crawford, Kate},
	year = {2024},
	keywords = {Benchmark testing, Dataset},
	pages = {4955--4972},
}

@inproceedings{orr2024a,
	address = {Rio de Janeiro Brazil},
	title = {{AI} as a {Sport}: {On} the {Competitive} {Epistemologies} of {Benchmarking}},
	isbn = {9798400704505},
	shorttitle = {{AI} as a {Sport}},
	url = {https://dl.acm.org/doi/10.1145/3630106.3659012},
	doi = {10.1145/3630106.3659012},
	abstract = {Artificial Intelligence (AI) systems are evaluated using competitive methods that rely on benchmark datasets to determine performance. These benchmark datasets, however, are often constructed through arbitrary processes that fall short in encapsulating the depth and breadth of the tasks they are intended to measure. In this paper, we interrogate the naturalization of benchmark datasets as veracious metrics by examining the historical development of benchmarking as an epistemic practice in AI research. Specifically, we highlight three key case studies that were crucial in establishing the existing reliance on benchmark datasets for evaluating the capabilities of AI systems: (1) the sharing of Highleyman’s OCR dataset in the 1960s, which solidified a community of knowledge production around a shared benchmark dataset, (2) the Common Task Framework (CTF) of the 1980s, a state-led project to standardize benchmark datasets as legitimate indicators of technical progress; and (3) the Netflix Prize which further solidified benchmarking as a competitive goal within the ML research community. This genealogy highlights how contemporary dynamics and limitations of benchmarking developed from a longer history of collaboration, standardization, and competition. We end with reflections on how this history informs our understanding of benchmarking in the current era of generative artificial intelligence.},
	language = {en},
	urldate = {2024-10-23},
	booktitle = {The 2024 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {ACM},
	author = {Orr, Will and Kang, Edward B.},
	month = jun,
	year = {2024},
	keywords = {Benchmark testing},
	pages = {1875--1884},
}

@inproceedings{raji2021,
    title={{AI} and the Everything in the Whole Wide World Benchmark},
    author={Inioluwa Deborah Raji and Emily Denton and Emily M. Bender and Alex Hanna and Amandalynne Paullada},
    booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)},
    year={2021},
    url={https://openreview.net/forum?id=j6NxpQbREA1}
}

@article{rauh2024,
	title = {Gaps in the {Safety} {Evaluation} of {Generative} {AI}},
	volume = {7},
	issn = {3065-8365},
	url = {https://ojs.aaai.org/index.php/AIES/article/view/31717},
	doi = {10.1609/aies.v7i1.31717},
	abstract = {Generative AI systems produce a range of ethical and social risks. Evaluation of these risks is a critical step on the path to ensuring the safety of these systems. However, evaluation requires the availability of validated and established measurement approaches and tools. In this paper, we provide an empirical review of the methods and tools that are available for evaluating known safety of generative AI systems to date. To this end, we review more than 200 safety-related evaluations that have been applied to generative AI systems. We categorise each evaluation along multiple axes to create a detailed snapshot of the safety evaluation landscape to date. We release this data for researchers and AI safety practitioners (https://bitly.ws/3hUzu). Analysing the current safety evaluation landscape reveals three systemic ”evaluation gaps”. First, a ”modality gap” emerges as few safety evaluations exist for non-text modalities. Second, a ”risk coverage gap” arises as evaluations for several ethical and social risks are simply lacking. Third, a ”context gap” arises as most safety evaluations are model-centric and fail to take into account the broader context in which AI systems operate. Devising next steps for safety practitioners based on these findings, we present tactical ”low-hanging fruit” steps towards closing the identified evaluation gaps and their limitations. We close by discussing the role and limitations of safety evaluation to ensure the safety of generative AI systems.},
	urldate = {2025-01-22},
	journal = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
	author = {Rauh, Maribeth and Marchal, Nahema and Manzini, Arianna and Hendricks, Lisa Anne and Comanescu, Ramona and Akbulut, Canfer and Stepleton, Tom and Mateos-Garcia, Juan and Bergman, Stevie and Kay, Jackie and Griffin, Conor and Bariach, Ben and Gabriel, Iason and Rieser, Verena and Isaac, William and Weidinger, Laura},
	month = oct,
	year = {2024},
	pages = {1200--1217},
}

@inproceedings{ren2024,
    title={Safetywashing: Do {AI} Safety Benchmarks Actually Measure Safety Progress?},
    author={Richard Ren and Steven Basart and Adam Khoja and Alice Gatti and Long Phan and Xuwang Yin and Mantas Mazeika and Alexander Pan and Gabriel Mukobi and Ryan Hwang Kim and Stephen Fitz and Dan Hendrycks},
    booktitle={The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
    year={2024},
    url={https://openreview.net/forum?id=YagfTP3RK6}
}

@inproceedings{selbst2019,
	address = {Atlanta GA USA},
	title = {Fairness and {Abstraction} in {Sociotechnical} {Systems}},
	isbn = {978-1-4503-6125-5},
	url = {https://dl.acm.org/doi/10.1145/3287560.3287598},
	doi = {10.1145/3287560.3287598},
	abstract = {A key goal of the fair-ML community is to develop machine-learning based systems that, once introduced into a social context, can achieve social and legal outcomes such as fairness, justice, and due process. Bedrock concepts in computer science—such as abstraction and modular design—are used to define notions of fairness and discrimination, to produce fairness-aware learning algorithms, and to intervene at different stages of a decision-making pipeline to produce "fair" outcomes. In this paper, however, we contend that these concepts render technical interventions ineffective, inaccurate, and sometimes dangerously misguided when they enter the societal context that surrounds decision-making systems. We outline this mismatch with five "traps" that fair-ML work can fall into even as it attempts to be more context-aware in comparison to traditional data science. We draw on studies of sociotechnical systems in Science and Technology Studies to explain why such traps occur and how to avoid them. Finally, we suggest ways in which technical designers can mitigate the traps through a refocusing of design in terms of process rather than solutions, and by drawing abstraction boundaries to include social actors rather than purely technical ones.},
	language = {en},
	urldate = {2024-11-12},
	booktitle = {Proceedings of the {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {ACM},
	author = {Selbst, Andrew D. and Boyd, Danah and Friedler, Sorelle A. and Venkatasubramanian, Suresh and Vertesi, Janet},
	month = jan,
	year = {2019},
	keywords = {Fairness, Benchmarks},
	pages = {59--68},
	file = {Selbst et al. - 2019 - Fairness and Abstraction in Sociotechnical Systems.pdf:C\:\\Users\\PURIFER\\Zotero\\storage\\H8ZQGZLT\\Selbst et al. - 2019 - Fairness and Abstraction in Sociotechnical Systems.pdf:application/pdf},
}

@article{thylstrup2022,
	title = {Politics of data reuse in machine learning systems: {Theorizing} reuse entanglements},
	volume = {9},
	issn = {2053-9517, 2053-9517},
	shorttitle = {Politics of data reuse in machine learning systems},
	url = {https://journals.sagepub.com/doi/10.1177/20539517221139785},
	doi = {10.1177/20539517221139785},
	abstract = {Policy discussions and corporate strategies on machine learning are increasingly championing data reuse as a key element in digital transformations. These aspirations are often coupled with a focus on responsibility, ethics and transparency, as well as emergent forms of regulation that seek to set demands for corporate conduct and the protection of civic rights. And the Protective measures include methods of traceability and assessments of ‘good’ and ‘bad’ datasets and algorithms that are considered to be traceable, stable and contained. However, these ways of thinking about both technology and ethics obscure a fundamental issue, namely that machine learning systems entangle data, algorithms and more-thanhuman environments in ways that challenge a well-deﬁned separation. This article investigates the fundamental fallacy of most data reuse strategies as well as their regulation and mitigation strategies that data can somehow be followed, contained and controlled in machine learning processes. Instead, the article argues that we need to understand the reuse of data as an inherently entangled phenomenon. To examine this tension between the discursive regimes and the realities of data reuse, we advance the notion of reuse entanglements as an analytical lens. The main contribution of the article is the conceptualization of reuse that places entanglements at its core and the articulation of its relevance using empirical illustrations. This is important, we argue, for our understanding of the nature of data and algorithms, for the practical uses of data and algorithms and our attitudes regarding ethics, responsibility and regulation.},
	language = {en},
	number = {2},
	urldate = {2024-11-05},
	journal = {Big Data \& Society},
	author = {Thylstrup, Nanna Bonde and Hansen, Kristian Bondo and Flyverbom, Mikkel and Amoore, Louise},
	month = jul,
	year = {2022},
	keywords = {Benchmark testing, Datasets, Data access, Data reuse, EU data act, Reuse},
	pages = {20539517221139785},
}

@misc{vafa2024,
	type = {Language models},
	title = {Evaluating the {World} {Model} {Implicit} in a {Generative} {Model}},
	url = {http://arxiv.org/abs/2406.03689},
	abstract = {Recent work suggests that large language models may implicitly learn world models. How should we assess this possibility? We formalize this question for the case where the underlying reality is governed by a deterministic finite automaton. This includes problems as diverse as simple logical reasoning, geographic navigation, game-playing, and chemistry. We propose new evaluation metrics for world model recovery inspired by the classic Myhill-Nerode theorem from language theory. We illustrate their utility in three domains: game playing, logic puzzles, and navigation. In all domains, the generative models we consider do well on existing diagnostics for assessing world models, but our evaluation metrics reveal their world models to be far less coherent than they appear. Such incoherence creates fragility: using a generative model to solve related but subtly different tasks can lead to failures. Building generative models that meaningfully capture the underlying logic of the domains they model would be immensely valuable; our results suggest new ways to assess how close a given model is to that goal.},
	language = {en},
	urldate = {2024-11-15},
	publisher = {arXiv},
	author = {Vafa, Keyon and Chen, Justin Y. and Rambachan, Ashesh and Kleinberg, Jon and Mullainathan, Sendhil},
	month = nov,
	year = {2024},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Vafa et al. - 2024 - Evaluating the World Model Implicit in a Generativ.pdf:C\:\\Users\\PURIFER\\Zotero\\storage\\4M5ZKITK\\Vafa et al. - 2024 - Evaluating the World Model Implicit in a Generativ.pdf:application/pdf},
}

@misc{xu2024,
	title = {Benchmark {Data} {Contamination} of {Large} {Language} {Models}: {A} {Survey}},
	shorttitle = {Benchmark {Data} {Contamination} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2406.04244},
	abstract = {The rapid development of Large Language Models (LLMs) like GPT-4, Claude-3, and Gemini has transformed the field of natural language processing. However, it has also resulted in a significant issue known as Benchmark Data Contamination (BDC). This occurs when language models inadvertently incorporate evaluation benchmark information from their training data, leading to inaccurate or unreliable performance during the evaluation phase of the process. This paper reviews the complex challenge of BDC in LLM evaluation and explores alternative assessment methods to mitigate the risks associated with traditional benchmarks. The paper also examines challenges and future directions in mitigating BDC risks, highlighting the complexity of the issue and the need for innovative solutions to ensure the reliability of LLM evaluation in real-world applications.},
	urldate = {2025-01-22},
	publisher = {arXiv},
	author = {Xu, Cheng and Guan, Shuhao and Greene, Derek and Kechadi, M.-Tahar},
	month = jun,
	year = {2024},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 31 pages, 7 figures, 3 tables},
	file = {arXiv Fulltext PDF:C\:\\Users\\PURIFER\\Zotero\\storage\\HA2MBT5B\\Xu et al. - 2024 - Benchmark Data Contamination of Large Language Mod.pdf:application/pdf},
}

@misc{zhang2024,
	title = {Language model developers should report train-test overlap},
	url = {http://arxiv.org/abs/2410.08385},
	abstract = {Language models are extensively evaluated, but correctly interpreting evaluation results requires knowledge of train-test overlap, which refers to the extent to which the language model is trained on the very data it is being tested on. The public currently lacks adequate information about train-test overlap: most models have no public train-test overlap statistics, and third parties cannot directly measure traintest overlap since they do not have access to the training data. To make this clear, we document the practices of 30 models, ﬁnding that just 9 models report traintest overlap: 4 models release training data under open-source licenses, enabling the community to directly measure train-test overlap, and 5 models publish their train-test overlap methodology and statistics. By engaging with language model developers, we provide novel information about train-test overlap for three additional models. Overall, we take the position that language model developers should publish train-test overlap statistics and/or training data whenever they report evaluation results on public test sets. We hope our work increases transparency into train-test overlap to increase the community-wide trust in model evaluations.},
	language = {en},
	urldate = {2024-11-14},
	publisher = {arXiv},
	author = {Zhang, Andy K. and Klyman, Kevin and Mai, Yifan and Levine, Yoav and Zhang, Yian and Bommasani, Rishi and Liang, Percy},
	month = oct,
	year = {2024},
	keywords = {Benchmarks},
	annote = {Comment: 18 pages},
	file = {Zhang et al. - 2024 - Language model developers should report train-test.pdf:C\:\\Users\\PURIFER\\Zotero\\storage\\7VIH2CHY\\Zhang et al. - 2024 - Language model developers should report train-test.pdf:application/pdf},
}

@misc{weij2024,
	type = {Language models},
	title = {{AI} {Sandbagging}: {Language} {Models} can {Strategically} {Underperform} on {Evaluations}},
	shorttitle = {{AI} {Sandbagging}},
	url = {http://arxiv.org/abs/2406.07358},
	abstract = {Trustworthy capability evaluations are crucial for ensuring the safety of AI systems, and are becoming a key component of AI regulation. However, the developers of an AI system, or the AI system itself, may have incentives for evaluations to understate the AI's actual capability. These conflicting interests lead to the problem of sandbagging \${\textbackslash}unicode\{x2013\}\$ which we define as "strategic underperformance on an evaluation". In this paper we assess sandbagging capabilities in contemporary language models (LMs). We prompt frontier LMs, like GPT-4 and Claude 3 Opus, to selectively underperform on dangerous capability evaluations, while maintaining performance on general (harmless) capability evaluations. Moreover, we find that models can be fine-tuned, on a synthetic dataset, to hide specific capabilities unless given a password. This behaviour generalizes to high-quality, held-out benchmarks such as WMDP. In addition, we show that both frontier and smaller models can be prompted, or password-locked, to target specific scores on a capability evaluation. Even more, we found that a capable password-locked model (Llama 3 70b) is reasonably able to emulate a less capable model (Llama 2 7b). Overall, our results suggest that capability evaluations are vulnerable to sandbagging. This vulnerability decreases the trustworthiness of evaluations, and thereby undermines important safety decisions regarding the development and deployment of advanced AI systems.},
	language = {en},
	urldate = {2024-11-15},
	publisher = {arXiv},
	author = {Weij, Teun van der and Hofstätter, Felix and Jaffe, Ollie and Brown, Samuel F. and Ward, Francis Rhys},
	month = jun,
	year = {2024},
	keywords = {Benchmarks},
	file = {Weij et al. - 2024 - AI Sandbagging Language Models can Strategically .pdf:C\:\\Users\\PURIFER\\Zotero\\storage\\TXQPBX7Z\\Weij et al. - 2024 - AI Sandbagging Language Models can Strategically .pdf:application/pdf},
}

@misc{pacchiardi2024,
	title = {Leaving the barn door open for {Clever} {Hans}: {Simple} features predict {LLM} benchmark answers},
	shorttitle = {Leaving the barn door open for {Clever} {Hans}},
	url = {http://arxiv.org/abs/2410.11672},
	abstract = {The integrity of AI benchmarks is fundamental to accurately assess the capabilities of AI systems. The internal validity of these benchmarks—i.e., making sure they are free from confounding factors—is crucial for ensuring that they are measuring what they are designed to measure. In this paper, we explore a key issue related to internal validity: the possibility that AI systems can solve benchmarks in unintended ways, bypassing the capability being tested. This phenomenon, widely known in human and animal experiments, is often referred to as the ‘Clever Hans’ effect, where tasks are solved using spurious cues, often involving much simpler processes than those putatively assessed. Previous research suggests that language models can exhibit this behaviour as well. In several older Natural Language Processing (NLP) benchmarks, individual n-grams like “not” have been found to be highly predictive of the correct labels, and supervised NLP models have been shown to exploit these patterns. In this work, we investigate the extent to which simple n-grams extracted from benchmark instances can be combined to predict labels in modern multiple-choice benchmarks designed for LLMs, and whether LLMs might be using such n-gram patterns to solve these benchmarks. We show how simple classifiers trained on these n-grams can achieve high scores on several benchmarks, despite lacking the capabilities being tested. Additionally, we provide evidence that modern LLMs might be using these superficial patterns to solve benchmarks. This suggests that the internal validity of these benchmarks may be compromised and caution should be exercised when interpreting LLM performance results on them.},
	language = {en},
	urldate = {2024-11-22},
	publisher = {arXiv},
	author = {Pacchiardi, Lorenzo and Tesic, Marko and Cheke, Lucy G. and Hernández-Orallo, José},
	month = oct,
	year = {2024},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Pacchiardi et al. - 2024 - Leaving the barn door open for Clever Hans Simple.pdf:/Users/joao/Zotero/storage/GKSXV66E/Pacchiardi et al. - 2024 - Leaving the barn door open for Clever Hans Simple.pdf:application/pdf},
}

@inproceedings{grill2024,
	address = {Rio de Janeiro Brazil},
	title = {Constructing {Capabilities}: {The} {Politics} of {Testing} {Infrastructures} for {Generative} {AI}},
	isbn = {9798400704505},
	shorttitle = {Constructing {Capabilities}},
	url = {https://dl.acm.org/doi/10.1145/3630106.3659009},
	doi = {10.1145/3630106.3659009},
	abstract = {The advertised and perceived capabilities of generative AI products like ChatGPT have recently stimulated considerable investments and discourse surrounding their potential to aid and replace work. The prominence of these systems, and their promise to be generalpurpose, has resulted in an avalanche of tests to discover and certify their capabilities. This new testing regime is concerned with creating ever-more tasks for generative AI products instead of testing a model for one specialized task. Beyond efforts to understand products’ capabilities, the construction of tasks and corresponding tests are also performative enactments meant to convince others and thus to gain attention, scientific legitimacy, and investment. The current market concentration of a few big AI companies points to a concerning conflict of interest: those with a vested interest in the success of the technology also have control over globalized testing infrastructures and thereby the exclusive means to create extensive knowledge claims about these systems. In this paper, I theorize capabilities as contested constructions and situated accomplishments shaped by power imbalances. I further unpack the globalized testing infrastructures involved in the construction and stabilization of generative AI products’ capabilities. Furthermore, I discuss how the testing of these AI models and products is externalized, extracting value from the unpaid or under-paid labor of researcher and developer communities, content creators, subcontractors, and users. Lastly, I discuss a reflexive and critical approach to testing that challenges depoliticization and seeks to produce lasting critiques that serve more emancipatory goals.},
	language = {en},
	urldate = {2024-11-15},
	booktitle = {The 2024 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {ACM},
	author = {Grill, Gabriel},
	month = jun,
	year = {2024},
	keywords = {Benchmarks},
	pages = {1838--1849},
	file = {Grill - 2024 - Constructing Capabilities The Politics of Testing.pdf:C\:\\Users\\PURIFER\\Zotero\\storage\\9QFLGCBV\\Grill - 2024 - Constructing Capabilities The Politics of Testing.pdf:application/pdf},
}

@article{marres2020,
	title = {Put to the test: {For} a new sociology of testing},
	volume = {71},
	issn = {0007-1315, 1468-4446},
	shorttitle = {Put to the test},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/1468-4446.12746},
	doi = {10.1111/1468-4446.12746},
	language = {en},
	number = {3},
	urldate = {2025-01-22},
	journal = {The British Journal of Sociology},
	author = {Marres, Noortje and Stark, David},
	month = jun,
	year = {2020},
	pages = {423--443},
	file = {Full Text:C\:\\Users\\PURIFER\\Zotero\\storage\\VVFZADZI\\Marres and Stark - 2020 - Put to the test For a new sociology of testing.pdf:application/pdf},
}

@article{pinch1993,
	title = {"{Testing} - {One}, {Two}, {Three} ... {Testing}!": {Toward} a {Sociology} of {Testing}},
	volume = {18},
	issn = {0162-2439, 1552-8251},
	shorttitle = {"{Testing} - {One}, {Two}, {Three} ... {Testing}!"},
	url = {https://journals.sagepub.com/doi/10.1177/016224399301800103},
	doi = {10.1177/016224399301800103},
	abstract = {This article explores testing as research site in the sociology of technology. A fully generalizable analysis is offered of testing in terms of a notion of projection. Prospective, current, and retrospective testing are identified The article is illustrated with examples of testing a clinical budgeting system in the United Kingdom National Health Service and the testing of the O-rings on the space shuttle Challenger. Lastly, the theme of "testing the user" is developed Some comments are offered on the pervasiveness of testing in society at large.},
	language = {en},
	number = {1},
	urldate = {2025-01-22},
	journal = {Science, Technology, \& Human Values},
	author = {Pinch, Trevor},
	month = jan,
	year = {1993},
	pages = {25--41},
}

@article{ahmed2024,
	title = {Field-building and the epistemic culture of {AI} safety},
	issn = {1396-0466},
	url = {https://firstmonday.org/ojs/index.php/fm/article/view/13626},
	doi = {10.5210/fm.v29i4.13626},
	abstract = {The emerging field of “AI safety” has attracted public attention and large infusions of capital to support its implied promise: the ability to deploy advanced artificial intelligence (AI) while reducing its gravest risks. Ideas from effective altruism, longtermism, and the study of existential risk are foundational to this new field. In this paper, we contend that overlapping communities interested in these ideas have merged into what we refer to as the broader “AI safety epistemic community,” which is sustained through its mutually reinforcing community-building and knowledge production practices. We support this assertion through an analysis of four core sites in this community’s epistemic culture: 1) online community-building through Web forums and career advising; 2) AI forecasting; 3) AI safety research; and 4) prize competitions. The dispersal of this epistemic community’s members throughout the tech industry, academia, and policy organizations ensures their continued input into global discourse about AI. Understanding the epistemic culture that fuses their moral convictions and knowledge claims is crucial to evaluating these claims, which are gaining influence in critical, rapidly changing debates about the harms of AI and how to mitigate them.},
	urldate = {2025-01-22},
	journal = {First Monday},
	author = {Ahmed, Shazeda and Jaźwińska, Klaudia and Ahlawat, Archana and Winecoff, Amy and Wang, Mona},
	month = apr,
	year = {2024},
	file = {Full Text:C\:\\Users\\PURIFER\\Zotero\\storage\\LEGVVA68\\Ahmed et al. - 2024 - Field-building and the epistemic culture of AI saf.pdf:application/pdf},
}

@inproceedings{bowman2021,
	address = {Online},
	title = {What {Will} it {Take} to {Fix} {Benchmarking} in {Natural} {Language} {Understanding}?},
	url = {https://aclanthology.org/2021.naacl-main.385},
	doi = {10.18653/v1/2021.naacl-main.385},
	language = {en},
	urldate = {2024-11-18},
	booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Bowman, Samuel R. and Dahl, George},
	year = {2021},
	keywords = {Benchmarks},
	pages = {4843--4855},
	file = {Bowman and Dahl - 2021 - What Will it Take to Fix Benchmarking in Natural L.pdf:C\:\\Users\\PURIFER\\Zotero\\storage\\BXRFVGCE\\Bowman and Dahl - 2021 - What Will it Take to Fix Benchmarking in Natural L.pdf:application/pdf},
}

@misc{dehghani2021,
	title = {The {Benchmark} {Lottery}},
	url = {http://arxiv.org/abs/2107.07002},
	abstract = {The world of empirical machine learning (ML) strongly relies on benchmarks in order to determine the relative effectiveness of different algorithms and methods. This paper proposes the notion of a benchmark lottery that describes the overall fragility of the ML benchmarking process. The benchmark lottery postulates that many factors, other than fundamental algorithmic superiority, may lead to a method being perceived as superior. On multiple benchmark setups that are prevalent in the ML community, we show that the relative performance of algorithms may be altered signiﬁcantly simply by choosing different benchmark tasks, highlighting the fragility of the current paradigms and potential fallacious interpretation derived from benchmarking ML methods. Given that every benchmark makes a statement about what it perceives to be important, we argue that this might lead to biased progress in the community. We discuss the implications of the observed phenomena and provide recommendations on mitigating them using multiple machine learning domains and communities as use cases, including natural language processing, computer vision, information retrieval, recommender systems, and reinforcement learning.},
	language = {en},
	urldate = {2024-11-18},
	publisher = {arXiv},
	author = {Dehghani, Mostafa and Tay, Yi and Gritsenko, Alexey A. and Zhao, Zhe and Houlsby, Neil and Diaz, Fernando and Metzler, Donald and Vinyals, Oriol},
	month = jul,
	year = {2021},
	keywords = {Benchmarks},
	file = {Dehghani et al. - 2021 - The Benchmark Lottery.pdf:C\:\\Users\\PURIFER\\Zotero\\storage\\9E8ITRS2\\Dehghani et al. - 2021 - The Benchmark Lottery.pdf:application/pdf},
}

@inproceedings{blili-hamelin2023,
	address = {Chicago IL USA},
	title = {Making {Intelligence}: {Ethical} {Values} in {IQ} and {ML} {Benchmarks}},
	isbn = {9798400701924},
	shorttitle = {Making {Intelligence}},
	url = {https://dl.acm.org/doi/10.1145/3593013.3593996},
	doi = {10.1145/3593013.3593996},
	abstract = {In recent years, ML researchers have wrestled with defining and improving machine learning (ML) benchmarks and datasets. In parallel, some have trained a critical lens on the ethics of dataset creation and ML research. In this position paper, we highlight the entanglement of ethics with seemingly łtechnicalž or łscientificž decisions about the design of ML benchmarks. Our starting point is the existence of multiple overlooked structural similarities between human intelligence benchmarks and ML benchmarks. Both types of benchmarks set standards for describing, evaluating, and comparing performance on tasks relevant to intelligenceÐstandards that many scholars of human intelligence have long recognized as value-laden. We use perspectives from feminist philosophy of science on IQ benchmarks and thick concepts in social science to argue that values need to be considered and documented when creating ML benchmarks. It is neither possible nor desirable to avoid this choice by creating value-neutral benchmarks. Finally, we outline practical recommendations for ML benchmark research ethics and ethics review.},
	language = {en},
	urldate = {2024-10-22},
	booktitle = {2023 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {ACM},
	author = {Blili-Hamelin, Borhane and Hancox-Li, Leif},
	month = jun,
	year = {2023},
	keywords = {Benchmarks},
	pages = {271--284},
	file = {Blili-Hamelin and Hancox-Li - 2023 - Making Intelligence Ethical Values in IQ and ML B.pdf:C\:\\Users\\PURIFER\\Zotero\\storage\\ISBIFFEL\\Blili-Hamelin and Hancox-Li - 2023 - Making Intelligence Ethical Values in IQ and ML B.pdf:application/pdf},
}


@article{ensmenger2012,
  title = {Is Chess the Drosophila of Artificial Intelligence? {{A}} Social History of an Algorithm},
  shorttitle = {Is Chess the Drosophila of Artificial Intelligence?},
  author = {Ensmenger, Nathan},
  date = {2012-02},
  journaltitle = {Social Studies of Science},
  shortjournal = {Soc Stud Sci},
  volume = {42},
  number = {1},
  pages = {5--30},
  issn = {0306-3127, 1460-3659},
  doi = {10.1177/0306312711424596},
  url = {https://journals.sagepub.com/doi/10.1177/0306312711424596},
  urldate = {2024-11-18},
  abstract = {Since the mid 1960s, researchers in computer science have famously referred to chess as the ‘drosophila’ of artificial intelligence (AI). What they seem to mean by this is that chess, like the common fruit fly, is an accessible, familiar, and relatively simple experimental technology that nonetheless can be used productively to produce valid knowledge about other, more complex systems. But for historians of science and technology, the analogy between chess and drosophila assumes a larger significance. As Robert Kohler has ably described, the decision to adopt drosophila as the organism of choice for genetics research had far-reaching implications for the development of 20th century biology. In a similar manner, the decision to focus on chess as the measure of both human and computer intelligence had important and unintended consequences for AI research. This paper explores the emergence of chess as an experimental technology, its significance in the developing research practices of the AI community, and the unique ways in which the decision to focus on chess shaped the program of AI research in the decade of the 1970s. More broadly, it attempts to open up the virtual black box of computer software – and of computer games in particular – to the scrutiny of historical and sociological analysis.},
  langid = {english},
  keywords = {Benchmarks},
  file = {C:\Users\erimari\Zotero\storage\5GA9PSLC\Ensmenger - 2012 - Is chess the drosophila of artificial intelligence.pdf}
}

@online{erenrich2023,
  title = {Errors in the {{MMLU}}: {{The Deep Learning Benchmark}} Is {{Wrong Surprisingly Often}}},
  author = {Erenrich, Daniel},
  date = {2023-08-23},
  url = {https://derenrich.medium.com/errors-in-the-mmlu-the-deep-learning-benchmark-is-wrong-surprisingly-often-7258bb045859},
  langid = {english},
  organization = {Medium},
  keywords = {Benchmarks},
  file = {C:\Users\erimari\Zotero\storage\2LWN7SWV\Erenrich - Errors in the MMLU The Deep Learning Benchmark is.pdf}
}

@online{hendrycksMeasuringMassiveMultitask2021,
  title = {Measuring {{Massive Multitask Language Understanding}}},
  author = {Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  date = {2021-01-12},
  eprint = {2009.03300},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2009.03300},
  urldate = {2024-10-18},
  abstract = {We propose a new test to measure a text model’s multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have nearrandom accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model’s academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {AI evaluation,Benchmark testing,LLM's}
}

@article{gehrmann2023,
	title = {Repairing the {Cracked} {Foundation}: {A} {Survey} of {Obstacles} in {Evaluation} {Practices} for {Generated} {Text}},
	volume = {77},
	issn = {1076-9757},
	shorttitle = {Repairing the {Cracked} {Foundation}},
	url = {https://www.jair.org/index.php/jair/article/view/13715},
	doi = {10.1613/jair.1.13715},
	abstract = {Evaluation practices in natural language generation (NLG) have many known ﬂaws, but improved evaluation approaches are rarely widely adopted. This issue has become more urgent, since neural generation models have improved to the point where their outputs can often no longer be distinguished based on the surface-level features that older metrics rely on. This paper surveys the issues with human and automatic model evaluations and with commonly used datasets in NLG that have been pointed out over the past 20 years. We summarize, categorize, and discuss how researchers have been addressing these issues and what their ﬁndings mean for the current state of model evaluations. Building on those insights, we lay out a long-term vision for evaluation research and propose concrete steps for researchers to improve their evaluation processes. Finally, we analyze 66 generation papers from recent NLP conferences in how well they already follow these suggestions and identify which areas require more drastic changes to the status quo.},
	language = {en},
	urldate = {2024-11-18},
	journal = {Journal of Artificial Intelligence Research},
	author = {Gehrmann, Sebastian and Clark, Elizabeth and Sellam, Thibault},
	month = may,
	year = {2023},
	pages = {103--166},
	file = {Gehrmann et al. - 2023 - Repairing the Cracked Foundation A Survey of Obst.pdf:C\:\\Users\\PURIFER\\Zotero\\storage\\695A8J7Z\\Gehrmann et al. - 2023 - Repairing the Cracked Foundation A Survey of Obst.pdf:application/pdf},
}

@misc{liao2023,
	title = {Rethinking {Model} {Evaluation} as {Narrowing} the {Socio}-{Technical} {Gap}},
	url = {http://arxiv.org/abs/2306.03100},
	abstract = {The recent development of generative and large language models (LLMs) poses new challenges for model evaluation that the research community and industry are grappling with. While the versatile capabilities of these models ignite excitement, they also inevitably make a leap toward homogenization: powering a wide range of applications with a single, often referred to as “generalpurpose”, model. In this position paper, we argue that model evaluation practices must take on a critical task to cope with the challenges and responsibilities brought by this homogenization: providing valid assessments for whether and how much human needs in downstream use cases can be satisfied by the given model (socio-technical gap). By drawing on lessons from the social sciences, human-computer interaction (HCI), and the interdisciplinary field of explainable AI (XAI), we urge the community to develop evaluation methods based on real-world socio-requirements and embrace diverse evaluation methods with an acknowledgment of trade-offs between realism to socio-requirements and pragmatic costs to conduct the evaluation. By mapping HCI and current NLG evaluation methods, we identify opportunities for evaluation methods for LLMs to narrow the socio-technical gap and pose open questions.},
	language = {en},
	urldate = {2024-11-14},
	publisher = {arXiv},
	author = {Liao, Q. Vera and Xiao, Ziang},
	month = jun,
	year = {2023},
	keywords = {Benchmarks},
	file = {Liao and Xiao - 2023 - Rethinking Model Evaluation as Narrowing the Socio.pdf:C\:\\Users\\PURIFER\\Zotero\\storage\\EMCBYLMN\\Liao and Xiao - 2023 - Rethinking Model Evaluation as Narrowing the Socio.pdf:application/pdf},
}

@inproceedings{liao2021,
    title={Are We Learning Yet? A Meta Review of Evaluation Failures Across Machine Learning},
    author={Thomas Liao and Rohan Taori and Inioluwa Deborah Raji and Ludwig Schmidt},
    booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)},
    year={2021},
    url={https://openreview.net/forum?id=mPducS1MsEK}
}

@misc{oakden-rayner2019,
	title = {Hidden {Stratification} {Causes} {Clinically} {Meaningful} {Failures} in {Machine} {Learning} for {Medical} {Imaging}},
	url = {http://arxiv.org/abs/1909.12475},
	abstract = {Machine learning models for medical image analysis often suffer from poor performance on important subsets of a population that are not identified during training or testing. For example, overall performance of a cancer detection model may be high, but the model still consistently misses a rare but aggressive cancer subtype. We refer to this problem as hidden stratification, and observe that it results from incompletely describing the meaningful variation in a dataset. While hidden stratification can substantially reduce the clinical efficacy of machine learning models, its effects remain difficult to measure. In this work, we assess the utility of several possible techniques for measuring and describing hidden stratification effects, and characterize these effects on multiple medical imaging datasets. We find evidence that hidden stratification can occur in unidentified imaging subsets with low prevalence, low label quality, subtle distinguishing features, or spurious correlates, and that it can result in relative performance differences of over 20\% on clinically important subsets. Finally, we explore the clinical implications of our findings, and suggest that evaluation of hidden stratification should be a critical component of any machine learning deployment in medical imaging.},
	language = {en},
	urldate = {2024-11-19},
	publisher = {arXiv},
	author = {Oakden-Rayner, Luke and Dunnmon, Jared and Carneiro, Gustavo and Ré, Christopher},
	month = nov,
	year = {2019},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Machine Learning for Health (ML4H) at NeurIPS 2019 - Extended Abstract},
	file = {Oakden-Rayner et al. - 2019 - Hidden Stratification Causes Clinically Meaningful.pdf:C\:\\Users\\PURIFER\\Zotero\\storage\\C2BK6WCS\\Oakden-Rayner et al. - 2019 - Hidden Stratification Causes Clinically Meaningful.pdf:application/pdf},
}

@misc{mcintosh2024,
	title = {Inadequacies of {Large} {Language} {Model} {Benchmarks} in the {Era} of {Generative} {Artificial} {Intelligence}},
	url = {http://arxiv.org/abs/2402.09880},
	abstract = {The rapid rise in popularity of Large Language Models (LLMs) with emerging capabilities has spurred public curiosity to evaluate and compare different LLMs, leading many researchers to propose their own LLM benchmarks. Noticing preliminary inadequacies in those benchmarks, we embarked on a study to critically assess 23 state-of-the-art LLM benchmarks, using our novel uniﬁed evaluation framework through the lenses of people, process, and technology, under the pillars of benchmark functionality and integrity. Our research uncovered signiﬁcant limitations, including biases, difﬁculties in measuring genuine reasoning, adaptability, implementation inconsistencies, prompt engineering complexity, evaluator diversity, and the overlooking of cultural and ideological norms in one comprehensive assessment. Our discussions emphasized the urgent need for standardized methodologies, regulatory certainties, and ethical guidelines in light of Artiﬁcial Intelligence (AI) advancements, including advocating for an evolution from static benchmarks to dynamic behavioral proﬁling to accurately capture LLMs’ complex behaviors and potential risks. Our study highlighted the necessity for a paradigm shift in LLM evaluation methodologies, underlining the importance of collaborative efforts for the development of universally accepted benchmarks and the enhancement of AI systems’ integration into society.},
	language = {en},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {McIntosh, Timothy R. and Susnjak, Teo and Arachchilage, Nalin and Liu, Tong and Watters, Paul and Halgamuge, Malka N.},
	month = oct,
	year = {2024},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Human-Computer Interaction, Computer Science - Computers and Society},
	file = {McIntosh et al. - 2024 - Inadequacies of Large Language Model Benchmarks in.pdf:C\:\\Users\\PURIFER\\Zotero\\storage\\R5Z9JGE5\\McIntosh et al. - 2024 - Inadequacies of Large Language Model Benchmarks in.pdf:application/pdf},
}

@inproceedings{kovatchev2024,
  title = {Benchmark {{Transparency}}: {{Measuring}} the {{Impact}} of {{Data}} on {{Evaluation}}},
  shorttitle = {Benchmark {{Transparency}}},
  booktitle = {Proceedings of the 2024 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}} ({{Volume}} 1: {{Long Papers}})},
  author = {Kovatchev, Venelin and Lease, Matthew},
  date = {2024},
  pages = {1536--1551},
  publisher = {Association for Computational Linguistics},
  location = {Mexico City, Mexico},
  doi = {10.18653/v1/2024.naacl-long.86},
  url = {https://aclanthology.org/2024.naacl-long.86},
  urldate = {2024-11-20},
  abstract = {In this paper we present an exploratory research on quantifying the impact that data distribution has on the performance and evaluation of NLP models. We propose an automated framework that measures the data point distribution across 6 different dimensions: ambiguity, difficulty, discriminability, length, noise, and perplexity. We use disproportional stratified sampling to measure how much the data distribution affects absolute (Acc/F1) and relative (Rank) model performance. We experiment on 2 different datasets (SQUAD and MNLI) and test a total of 135 different models (125 on SQUAD and 10 on MNLI). We demonstrate that without explicit control of the data distribution, standard evaluation frameworks are inconsistent and unreliable. We find that the impact of the data is statistically significant and is often larger than the impact of changing the metric.},
  eventtitle = {Proceedings of the 2024 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}} ({{Volume}} 1: {{Long Papers}})},
  langid = {english},
  file = {C:\Users\erimari\Zotero\storage\96GJSAEQ\Kovatchev and Lease - 2024 - Benchmark Transparency Measuring the Impact of Da.pdf}
}

@misc{liu2021,
	title = {{ExplainaBoard}: {An} {Explainable} {Leaderboard} for {NLP}},
	shorttitle = {{ExplainaBoard}},
	url = {http://arxiv.org/abs/2104.06387},
	abstract = {With the rapid development of NLP research, leaderboards have emerged as one tool to track the performance of various systems on various NLP tasks. They are effective in this goal to some extent, but generally present a rather simplistic one-dimensional view of the submitted systems, communicated only through holistic accuracy numbers. In this paper, we present a new conceptualization and implementation of NLP evaluation: the EXPLAINABOARD, which in addition to inheriting the functionality of the standard leaderboard, also allows researchers to (i) diagnose strengths and weaknesses of a single system (e.g. what is the best-performing system bad at?) (ii) interpret relationships between multiple systems. (e.g. where does system A outperform system B? What if we combine systems A, B and C?) and (iii) examine prediction results closely (e.g. what are common errors made by multiple systems or in what contexts do particular errors occur?). So far, EXPLAINABOARD covers more than 400 systems, 50 datasets, 40 languages, and 12 tasks.1 We not only released an online platform at the website 2 but also make our evaluation tool an API with MIT Licence at Github 3 and PyPi 4 that allows users to conveniently assess their models ofﬂine. We additionally release all output ﬁles from systems that we have run or collected to motivate “output-driven” research in the future.},
	language = {en},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Liu, Pengfei and Fu, Jinlan and Xiao, Yang and Yuan, Weizhe and Chang, Shuaicheng and Dai, Junqi and Liu, Yixin and Ye, Zihuiwen and Dou, Zi-Yi and Neubig, Graham},
	month = jul,
	year = {2021},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: ACL2021 Demo Track},
	file = {Liu et al. - 2021 - ExplainaBoard An Explainable Leaderboard for NLP.pdf:C\:\\Users\\PURIFER\\Zotero\\storage\\MJV7JSE4\\Liu et al. - 2021 - ExplainaBoard An Explainable Leaderboard for NLP.pdf:application/pdf},
}

@misc{arzt2024,
	title = {Beyond the {Numbers}: {Transparency} in {Relation} {Extraction} {Benchmark} {Creation} and {Leaderboards}},
	shorttitle = {Beyond the {Numbers}},
	url = {http://arxiv.org/abs/2411.05224},
	language = {en},
	urldate = {2024-11-19},
	publisher = {arXiv},
	author = {Arzt, Varvara and Hanbury, Allan},
	month = nov,
	year = {2024},
	keywords = {Benchmarks},
}

@inproceedings{sambasivan2021,
	address = {Yokohama Japan},
	title = {“{Everyone} wants to do the model work, not the data work”: {Data} {Cascades} in {High}-{Stakes} {AI}},
	isbn = {978-1-4503-8096-6},
	shorttitle = {“{Everyone} wants to do the model work, not the data work”},
	url = {https://dl.acm.org/doi/10.1145/3411764.3445518},
	doi = {10.1145/3411764.3445518},
	abstract = {AI models are increasingly applied in high-stakes domains like health and conservation. Data quality carries an elevated significance in high-stakes AI due to its heightened downstream impact, impacting predictions like cancer detection, wildlife poaching, and loan allocations. Paradoxically, data is the most under-valued and de-glamorised aspect of AI. In this paper, we report on data practices in high-stakes AI, from interviews with 53 AI practitioners in India, East and West African countries, and USA. We define, identify, and present empirical evidence on Data Cascades—compounding events causing negative, downstream effects from data issues—triggered by conventional AI/ML practices that undervalue data quality. Data cascades are pervasive (92\% prevalence), invisible, delayed, but often avoidable. We discuss HCI opportunities in designing and incentivizing data excellence as a first-class citizen of AI, resulting in safer and more robust systems for all.},
	language = {en},
	urldate = {2024-11-18},
	booktitle = {Proceedings of the 2021 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Sambasivan, Nithya and Kapania, Shivani and Highfill, Hannah and Akrong, Diana and Paritosh, Praveen and Aroyo, Lora M},
	month = may,
	year = {2021},
	keywords = {Benchmarks},
	pages = {1--15},
	file = {Sambasivan et al. - 2021 - “Everyone wants to do the model work, not the data.pdf:C\:\\Users\\PURIFER\\Zotero\\storage\\TVE8SU44\\Sambasivan et al. - 2021 - “Everyone wants to do the model work, not the data.pdf:application/pdf},
}

@misc{narayanan2023,
	title = {{GPT}-4 and professional benchmarks: the wrong answer to the wrong question},
	url = {https://www.aisnakeoil.com/p/gpt-4-and-professional-benchmarks},
	language = {en},
	journal = {AI Snake Oil},
	author = {Narayanan, Arvind and Kapoor, Sayash},
	month = mar,
	year = {2023},
	keywords = {Benchmarks},
	file = {Narayanan and Kapoor - GPT-4 and professional benchmarks the wrong answe.pdf:C\:\\Users\\PURIFER\\Zotero\\storage\\PM4MKEQS\\Narayanan and Kapoor - GPT-4 and professional benchmarks the wrong answe.pdf:application/pdf},
}

@online{zhang2024a,
  title = {Inherent {{Trade-Offs}} between {{Diversity}} and {{Stability}} in {{Multi-Task Benchmarks}}},
  author = {Zhang, Guanhua and Hardt, Moritz},
  date = {2024-05-06},
  eprint = {2405.01719},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2405.01719},
  urldate = {2024-11-19},
  abstract = {We examine multi-task benchmarks in machine learning through the lens of social choice theory. We draw an analogy between benchmarks and electoral systems, where models are candidates and tasks are voters. This suggests a distinction between cardinal and ordinal benchmark systems. The former aggregate numerical scores into one model ranking; the latter aggregate rankings for each task. We apply Arrow’s impossibility theorem to ordinal benchmarks to highlight the inherent limitations of ordinal systems, particularly their sensitivity to the inclusion of irrelevant models. Inspired by Arrow’s theorem, we empirically demonstrate a strong trade-off between diversity and sensitivity to irrelevant changes in existing multi-task benchmarks. Our result is based on new quantitative measures of diversity and sensitivity that we introduce. Sensitivity quantifies the impact that irrelevant changes to tasks have on a benchmark. Diversity captures the degree of disagreement in model rankings across tasks. We develop efficient approximation algorithms for both measures, as exact computation is computationally challenging. Through extensive experiments on seven cardinal benchmarks and eleven ordinal benchmarks, we demonstrate a clear trade-off between diversity and stability: The more diverse a multi-task benchmark, the more sensitive to trivial changes it is. Additionally, we show that the aggregated rankings of existing benchmarks are highly unstable under irrelevant changes. The codes and data are available at https://socialfoundations. github.io/benchbench/.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Benchmarks},
  file = {C:\Users\erimari\Zotero\storage\XTR597SZ\Zhang and Hardt - 2024 - Inherent Trade-Offs between Diversity and Stabilit.pdf}
}

@inproceedings{park2022,
	address = {Dublin, Ireland},
	title = {Raison d’être of the benchmark dataset: {A} {Survey} of {Current} {Practices} of {Benchmark} {Dataset} {Sharing} {Platforms}},
	shorttitle = {Raison d’être of the benchmark dataset},
	url = {https://aclanthology.org/2022.nlppower-1.1},
	doi = {10.18653/v1/2022.nlppower-1.1},
	abstract = {This paper critically examines the current practices of benchmark dataset sharing in NLP and suggests a better way to inform reusers of the benchmark dataset. As the dataset sharing platform plays a key role not only in distributing the dataset but also in informing the potential reusers about the dataset, we believe data sharing platforms should provide a comprehensive context of the datasets. We survey four benchmark dataset sharing platforms: HuggingFace, PaperswithCode, Tensorflow, and Pytorch to diagnose the current practices of how the dataset is shared - which metadata is shared and omitted. To be specific, drawing on the concept of data curation which considers the future reuse when the data is made public, we advance the direction that benchmark dataset sharing platforms should take into consideration. We identify that four benchmark platforms have different practices of using metadata and there is a lack of consensus on what social impact metadata is. We believe the problem of missing a discussion around social impact in the dataset sharing platforms has to do with the failed agreement on who should be in charge. We propose that the benchmark dataset should develop social impact metadata and data curator should take a role in managing the social impact metadata.},
	language = {en},
	urldate = {2024-11-21},
	booktitle = {Proceedings of {NLP} {Power}! {The} {First} {Workshop} on {Efficient} {Benchmarking} in {NLP}},
	publisher = {Association for Computational Linguistics},
	author = {Park, Jaihyun and Jeoung, Sullam},
	year = {2022},
	keywords = {Benchmarks},
	pages = {1--10},
	file = {Park and Jeoung - 2022 - Raison d’être of the benchmark dataset A Survey o.pdf:C\:\\Users\\PURIFER\\Zotero\\storage\\PJE96AZF\\Park and Jeoung - 2022 - Raison d’être of the benchmark dataset A Survey o.pdf:application/pdf},
}

@article{paullada2021,
	title = {Data and its (dis)contents: {A} survey of dataset development and use in machine learning research},
	volume = {2},
	issn = {26663899},
	shorttitle = {Data and its (dis)contents},
	url = {http://arxiv.org/abs/2012.05345},
	doi = {10.1016/j.patter.2021.100336},
	abstract = {Datasets have played a foundational role in the advancement of machine learning research. They form the basis for the models we design and deploy, as well as our primary medium for benchmarking and evaluation. Furthermore, the ways in which we collect, construct and share these datasets inform the kinds of problems the ﬁeld pursues and the methods explored in algorithm development. However, recent work from a breadth of perspectives has revealed the limitations of predominant practices in dataset collection and use. In this paper, we survey the many concerns raised about the way we collect and use data in machine learning and advocate that a more cautious and thorough understanding of data is necessary to address several of the practical and ethical issues of the ﬁeld.},
	language = {en},
	number = {11},
	urldate = {2024-11-22},
	journal = {Patterns},
	author = {Paullada, Amandalynne and Raji, Inioluwa Deborah and Bender, Emily M. and Denton, Emily and Hanna, Alex},
	month = nov,
	year = {2021},
	keywords = {Computer Science - Machine Learning},
	pages = {100336},
	file = {Paullada et al. - 2021 - Data and its (dis)contents A survey of dataset de.pdf:C\:\\Users\\PURIFER\\Zotero\\storage\\62RGGUIP\\Paullada et al. - 2021 - Data and its (dis)contents A survey of dataset de.pdf:application/pdf},
}

@article{kaufman_leakage_2012,
author = {Kaufman, Shachar and Rosset, Saharon and Perlich, Claudia and Stitelman, Ori},
title = {Leakage in data mining: Formulation, detection, and avoidance},
year = {2012},
issue_date = {December 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/2382577.2382579},
doi = {10.1145/2382577.2382579},
abstract = {Deemed “one of the top ten data mining mistakes”, leakage is the introduction of information about the data mining target that should not be legitimately available to mine from. In addition to our own industry experience with real-life projects, controversies around several major public data mining competitions held recently such as the INFORMS 2010 Data Mining Challenge and the IJCNN 2011 Social Network Challenge are evidence that this issue is as relevant today as it has ever been. While acknowledging the importance and prevalence of leakage in both synthetic competitions and real-life data mining projects, existing literature has largely left this idea unexplored. What little has been said turns out not to be broad enough to cover more complex cases of leakage, such as those where the classical independently and identically distributed (i.i.d.) assumption is violated, that have been recently documented. In our new approach, these cases and others are explained by explicitly defining modeling goals and analyzing the broader framework of the data mining problem. The resulting definition enables us to derive general methodology for dealing with the issue. We show that it is possible to avoid leakage with a simple specific approach to data management followed by what we call a learn-predict separation, and present several ways of detecting leakage when the modeler has no control over how the data have been collected. We also offer an alternative point of view on leakage that is based on causal graph modeling concepts.},
journal = {ACM Trans. Knowl. Discov. Data},
month = dec,
articleno = {15},
numpages = {21},
keywords = {Data mining, leakage, predictive modeling}
}

@article{geirhos_shortcut_2020,
	title = {Shortcut {Learning} in {Deep} {Neural} {Networks}},
	volume = {2},
	issn = {2522-5839},
	url = {http://arxiv.org/abs/2004.07780},
	doi = {10.1038/s42256-020-00257-z},
	abstract = {Deep learning has triggered the current rise of artificial intelligence and is the workhorse of today’s machine intelligence. Numerous success stories have rapidly spread all over science, industry and society, but its limitations have only recently come into focus. In this perspective we seek to distill how many of deep learning’s problems can be seen as different symptoms of the same underlying problem: shortcut learning. Shortcuts are decision rules that perform well on standard benchmarks but fail to transfer to more challenging testing conditions, such as real-world scenarios. Related issues are known in Comparative Psychology, Education and Linguistics, suggesting that shortcut learning may be a common characteristic of learning systems, biological and artificial alike. Based on these observations, we develop a set of recommendations for model interpretation and benchmarking, highlighting recent advances in machine learning to improve robustness and transferability from the lab to real-world applications.},
	language = {en},
	number = {11},
	urldate = {2024-12-09},
	journal = {Nature Machine Intelligence},
	author = {Geirhos, Robert and Jacobsen, Jörn-Henrik and Michaelis, Claudio and Zemel, Richard and Brendel, Wieland and Bethge, Matthias and Wichmann, Felix A.},
	month = nov,
	year = {2020},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Quantitative Biology - Neurons and Cognition},
	pages = {665--673},
	file = {Geirhos et al. - 2020 - Shortcut Learning in Deep Neural Networks.pdf:/Users/joao/Zotero/storage/FSAUUCFA/Geirhos et al. - 2020 - Shortcut Learning in Deep Neural Networks.pdf:application/pdf},
}

@article{aroyo2015,
	title = {Truth {Is} a {Lie}: {Crowd} {Truth} and the {Seven} {Myths} of {Human} {Annotation}},
	volume = {36},
	copyright = {http://onlinelibrary.wiley.com/termsAndConditions\#vor},
	issn = {0738-4602, 2371-9621},
	shorttitle = {Truth {Is} a {Lie}},
	url = {https://onlinelibrary.wiley.com/doi/10.1609/aimag.v36i1.2564},
	doi = {10.1609/aimag.v36i1.2564},
	language = {en},
	number = {1},
	urldate = {2024-11-25},
	journal = {AI Magazine},
	author = {Aroyo, Lora and Welty, Chris},
	month = mar,
	year = {2015},
	keywords = {Benchmarks},
	pages = {15--24},
}

@inproceedings{sen2015,
	address = {Vancouver BC Canada},
	title = {Turkers, {Scholars}, "{Arafat}" and "{Peace}": {Cultural} {Communities} and {Algorithmic} {Gold} {Standards}},
	isbn = {978-1-4503-2922-4},
	shorttitle = {Turkers, {Scholars}, "{Arafat}" and "{Peace}"},
	url = {https://dl.acm.org/doi/10.1145/2675133.2675285},
	doi = {10.1145/2675133.2675285},
	abstract = {In just a few years, crowdsourcing markets like Mechanical Turk have become the dominant mechanism for building “gold standard” datasets in areas of computer science ranging from natural language processing to audio transcription. The assumption behind this sea change — an assumption that is central to the approaches taken in hundreds of research projects — is that crowdsourced markets can accurately replicate the judgments of the general population for knowledgeoriented tasks. Focusing on the important domain of semantic relatedness algorithms and leveraging Clark’s theory of common ground as a framework, we demonstrate that this assumption can be highly problematic. Using 7,921 semantic relatedness judgements from 72 scholars and 39 crowdworkers, we show that crowdworkers on Mechanical Turk produce signiﬁcantly different semantic relatedness gold standard judgements than people from other communities. We also show that algorithms that perform well against Mechanical Turk gold standard datasets do signiﬁcantly worse when evaluated against other communities’ gold standards. Our results call into question the broad use of Mechanical Turk for the development of gold standard datasets and demonstrate the importance of understanding these datasets from a human-centered point-of-view. More generally, our ﬁndings problematize the notion that a universal gold standard dataset exists for all knowledge tasks.},
	language = {en},
	urldate = {2024-12-02},
	booktitle = {Proceedings of the 18th {ACM} {Conference} on {Computer} {Supported} {Cooperative} {Work} \& {Social} {Computing}},
	publisher = {ACM},
	author = {Sen, Shilad and Giesel, Margaret E. and Gold, Rebecca and Hillmann, Benjamin and Lesicko, Matt and Naden, Samuel and Russell, Jesse and Wang, Zixiao (Ken) and Hecht, Brent},
	month = feb,
	year = {2015},
	pages = {826--838},
	file = {Sen et al. - 2015 - Turkers, Scholars, Arafat and Peace Cultural .pdf:C\:\\Users\\PURIFER\\Zotero\\storage\\AXKP59GU\\Sen et al. - 2015 - Turkers, Scholars, Arafat and Peace Cultural .pdf:application/pdf},
}

@article{denton2021,
	title = {On the genealogy of machine learning datasets: {A} critical history of {ImageNet}},
	volume = {8},
	issn = {2053-9517, 2053-9517},
	shorttitle = {On the genealogy of machine learning datasets},
	url = {http://journals.sagepub.com/doi/10.1177/20539517211035955},
	doi = {10.1177/20539517211035955},
	abstract = {In response to growing concerns of bias, discrimination, and unfairness perpetuated by algorithmic systems, the datasets used to train and evaluate machine learning models have come under increased scrutiny. Many of these examinations have focused on the contents of machine learning datasets, ﬁnding glaring underrepresentation of minoritized groups. In contrast, relatively little work has been done to examine the norms, values, and assumptions embedded in these datasets. In this work, we conceptualize machine learning datasets as a type of informational infrastructure, and motivate a genealogy as method in examining the histories and modes of constitution at play in their creation. We present a critical history of ImageNet as an exemplar, utilizing critical discourse analysis of major texts around ImageNet’s creation and impact. We ﬁnd that assumptions around ImageNet and other large computer vision datasets more generally rely on three themes: the aggregation and accumulation of more data, the computational construction of meaning, and making certain types of data labor invisible. By tracing the discourses that surround this inﬂuential benchmark, we contribute to the ongoing development of the standards and norms around data development in machine learning and artiﬁcial intelligence research.},
	language = {en},
	number = {2},
	urldate = {2021-10-11},
	journal = {Big Data \& Society},
	author = {Denton, Emily and Hanna, Alex and Amironesei, Razvan and Smart, Andrew and Nicole, Hilary},
	month = jul,
	year = {2021},
	keywords = {Machine learning, Data, AI and ethics, Critical data studies, ImageNet},
	pages = {1--14},
}

@book{mulvin2021,
	address = {Cambridge},
	series = {Infrastructures},
	title = {Proxies: {The} {Cultural} {Work} of {Standing} {In}},
	isbn = {978-0-262-04514-8 978-0-262-36624-3},
	shorttitle = {Proxies},
	abstract = {How those with the power to design technology, in the very moment of design, are allowed to imagine who is included—and who is excluded—in the future. Our world is built on an array of standards we are compelled to share. In Proxies, Dylan Mulvin examines how we arrive at those standards, asking, “To whom and to what do we delegate the power to stand in for the world?” Mulvin shows how those with the power to design technology, in the very moment of design, are allowed to imagine who is included—and who is excluded—in the future. For designers of technology, some bits of the world end up standing in for other bits, standards with which they build and calibrate. These “proxies” carry specific values, even as they disappear from view. Mulvin explores the ways technologies, standards, and infrastructures inescapably reflect the cultural milieus of their bureaucratic homes. Drawing on archival research, he investigates some of the basic building-blocks of our shared infrastructures. He tells the history of technology through the labor and communal practices of, among others, the people who clean kilograms to make the metric system run, the women who pose as test images, and the actors who embody disease and disability for medical students. Each case maps the ways standards and infrastructure rely on prototypical ideas of whiteness, able-bodiedness, and purity to control and contain the messiness of reality. Standards and infrastructures, Mulvin argues, shape and distort the possibilities of representation, the meaning of difference, and the levers of change and social justice},
	language = {en},
	publisher = {The MIT Press},
	author = {Mulvin, Dylan},
	year = {2021},
	keywords = {Benchmark testing, Objectivity, Proxies},
}

@article{ott2022,
	title = {Mapping global dynamics of benchmark creation and saturation in artificial intelligence},
	volume = {13},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-022-34591-0},
	doi = {10.1038/s41467-022-34591-0},
	language = {en},
	number = {1},
	urldate = {2024-11-26},
	journal = {Nature Communications},
	author = {Ott, Simon and Barbosa-Silva, Adriano and Blagec, Kathrin and Brauner, Jan and Samwald, Matthias},
	month = nov,
	year = {2022},
	pages = {6793},
	file = {PDF:C\:\\Users\\PURIFER\\Zotero\\storage\\P8LBV5S4\\Ott et al. - 2022 - Mapping global dynamics of benchmark creation and saturation in artificial intelligence.pdf:application/pdf},
}

@book{jaton2021,
	address = {Cambridge},
	series = {Inside {Technology}},
	title = {The {Constitution} of {Algorithms}: {Ground}-{Truthing}, {Programming}, {Formulating}},
	isbn = {978-0-262-54214-2 978-0-262-36323-5},
	shorttitle = {The {Constitution} of {Algorithms}},
	abstract = {A laboratory study that investigates how algorithms come into existence. Algorithms—often associated with the terms big data, machine learning, or artificial intelligence—underlie the technologies we use every day, and disputes over the consequences, actual or potential, of new algorithms arise regularly. In this book, Florian Jaton offers a new way to study computerized methods, providing an account of where algorithms come from and how they are constituted, investigating the practical activities by which algorithms are progressively assembled rather than what they may suggest or require once they are assembled. Drawing on a four-year ethnographic study of a computer science laboratory that specialized in digital image processing, Jaton illuminates the invisible processes that are behind the development of algorithms. Tracing what he terms a set of intertwining courses of actions sharing common finalities, he describes the practical activity of creating algorithms through the lenses of ground-truthing, programming, and formulating. He first presents the building of ground truths, referential repositories that form the material basis for algorithms. Then, after considering programming's resistance to ethnographic scrutiny, he describes programming courses of action he attended at the laboratory. Finally, he offers an account of courses of action that successfully formulated some of the relationships among the data of a ground-truth database, revealing the links between ground-truthing, programming, and formulating activities—entangled processes that lead to the shaping of algorithms. In practice, ground-truthing, programming, and formulating form a whirlwind process, an emergent and intertwined agency},
	language = {en},
	publisher = {The MIT Press},
	author = {Jaton, Florian},
	year = {2021},
	keywords = {Benchmark testing, Ethnography},
}

@article{blagec2023,
	title = {Benchmark datasets driving artificial intelligence development fail to capture the needs of medical professionals},
	volume = {137},
	issn = {15320464},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1532046422002799},
	doi = {10.1016/j.jbi.2022.104274},
	abstract = {Our analysis indicates that AI benchmarks of direct clinical relevance are scarce and fail to cover most work activities that clinicians want to see addressed. In particular, tasks associated with routine documentation and patient data administration workflows are not represented despite significant associated workloads. Thus, currently available AI benchmarks are improperly aligned with desired targets for AI automation in clinical settings, and novel benchmarks should be created to fill these gaps.},
	language = {en},
	urldate = {2024-12-09},
	journal = {Journal of Biomedical Informatics},
	author = {Blagec, Kathrin and Kraiger, Jakob and Frühwirt, Wolfgang and Samwald, Matthias},
	month = jan,
	year = {2023},
	pages = {104274},
	file = {Blagec et al. - 2023 - Benchmark datasets driving artificial intelligence.pdf:C\:\\Users\\PURIFER\\Zotero\\storage\\PI6AT5TM\\Blagec et al. - 2023 - Benchmark datasets driving artificial intelligence.pdf:application/pdf},
}

@misc{koch2024,
	title = {From {Protoscience} to {Epistemic} {Monoculture}: {How} {Benchmarking} {Set} the {Stage} for the {Deep} {Learning} {Revolution}},
	shorttitle = {From {Protoscience} to {Epistemic} {Monoculture}},
	url = {http://arxiv.org/abs/2404.06647},
	abstract = {Over the past decade, AI research has focused heavily on building ever-larger deep learning models. This approach has simultaneously unlocked incredible achievements in science and technology, and hindered AI from overcoming long-standing limitations with respect to explainability, ethical harms, and environmental efficiency. Drawing on qualitative interviews and computational analyses, our three-part history of AI research traces the creation of this “epistemic monoculture” back to a radical reconceptualization of scientific progress that began in the 1980s. In the first era of AI research (1950s-late 1980s), researchers and patrons approached AI as a “basic” science that would advance through autonomous exploration and organic assessments of progress (e.g., peer-review, theoretical consensus). The failure of this approach led to a retrenchment of funding in the 1980s. Amid this “AI Winter,” an intervention by the U.S. government reoriented the field towards measurable progress on tasks of military and commercial interest. A new evaluation system called “benchmarking” provided an objective way to quantify progress on tasks by focusing exclusively on increasing predictive accuracy on example datasets. Distilling science down to verifiable metrics clarified the roles of scientists, allowed the field to rapidly integrate talent, and provided clear signals of significance and progress. But history has also revealed a tradeoff to this streamlined approach to science: the consolidation around external interests and inherent conservatism of benchmarking has disincentivized exploration beyond scaling monoculture. In the discussion, we explain how AI’s monoculture offers a compelling challenge to the belief that basic, exploration-driven research is needed for scientific progress. Implications for the spread of AI monoculture to other sciences in the era of generative AI are also discussed.},
	language = {en},
	urldate = {2024-11-05},
	publisher = {arXiv},
	author = {Koch, Bernard J. and Peterson, David},
	month = apr,
	year = {2024},
	keywords = {Benchmark testing},
}

@article{ahmed2023,
	title = {The growing influence of industry in {AI} research},
	volume = {379},
	issn = {0036-8075, 1095-9203},
	url = {https://www.science.org/doi/10.1126/science.ade2420},
	doi = {10.1126/science.ade2420},
	language = {en},
	number = {6635},
	urldate = {2024-12-10},
	journal = {Science},
	author = {Ahmed, Nur and Wahed, Muntasir and Thompson, Neil C.},
	month = mar,
	year = {2023},
	keywords = {Benchmarks},
	pages = {884--886},
	file = {Ahmed et al. - 2023 - The growing influence of industry in AI research.pdf:C\:\\Users\\PURIFER\\Zotero\\storage\\7HAW92DQ\\Ahmed et al. - 2023 - The growing influence of industry in AI research.pdf:application/pdf},
}

@misc{bartz-beielstein2020,
	title = {Benchmarking in {Optimization}: {Best} {Practice} and {Open} {Issues}},
	shorttitle = {Benchmarking in {Optimization}},
	url = {http://arxiv.org/abs/2007.03488},
	language = {en},
	urldate = {2024-12-10},
	publisher = {arXiv},
	author = {Bartz-Beielstein, Thomas and Doerr, Carola and Berg, Daan van den and Bossek, Jakob and Chandrasekaran, Sowmya and Eftimov, Tome and Fischbach, Andreas and Kerschke, Pascal and Cava, William La and Lopez-Ibanez, Manuel and Malan, Katherine M. and Moore, Jason H. and Naujoks, Boris and Orzechowski, Patryk and Volz, Vanessa and Wagner, Markus and Weise, Thomas},
	month = dec,
	year = {2020},
}

@misc{biderman2024,
	title = {Lessons from the {Trenches} on {Reproducible} {Evaluation} of {Language} {Models}},
	url = {http://arxiv.org/abs/2405.14782},
	abstract = {Effective evaluation of language models remains an open challenge in NLP. Researchers and engineers face methodological issues such as the sensitivity of models to evaluation setup, difficulty of proper comparisons across methods, and the lack of reproducibility and transparency. In this paper we draw on three years of experience in evaluating large language models to provide guidance and lessons for researchers. First, we provide an overview of common challenges faced in language model evaluation. Second, we delineate best practices for addressing or lessening the impact of these challenges on research. Third, we present the Language Model Evaluation Harness (lm-eval): an open source library for independent, reproducible, and extensible evaluation of language models that seeks to address these issues. We describe the features of the library as well as case studies in which the library has been used to alleviate these methodological concerns.},
	language = {en},
	urldate = {2024-12-10},
	publisher = {arXiv},
	author = {Biderman, Stella and Schoelkopf, Hailey and Sutawika, Lintang and Gao, Leo and Tow, Jonathan and Abbasi, Baber and Aji, Alham Fikri and Ammanamanchi, Pawan Sasanka and Black, Sidney and Clive, Jordan and DiPofi, Anthony and Etxaniz, Julen and Fattori, Benjamin and Forde, Jessica Zosa and Foster, Charles and Hsu, Jeffrey and Jaiswal, Mimansa and Lee, Wilson Y. and Li, Haonan and Lovering, Charles and Muennighoff, Niklas and Pavlick, Ellie and Phang, Jason and Skowron, Aviya and Tan, Samson and Tang, Xiangru and Wang, Kevin A. and Winata, Genta Indra and Yvon, François and Zou, Andy},
	month = may,
	year = {2024},
	keywords = {Benchmarks},
	file = {Biderman et al. - 2024 - Lessons from the Trenches on Reproducible Evaluati.pdf:C\:\\Users\\PURIFER\\Zotero\\storage\\TQ996WZ9\\Biderman et al. - 2024 - Lessons from the Trenches on Reproducible Evaluati.pdf:application/pdf},
}

@misc{chang2023,
	title = {A {Survey} on {Evaluation} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2307.03109},
	abstract = {Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, educations, natural and social sciences, agent applications, and other areas. Secondly, we answer the `where' and `how' questions by diving into the evaluation methods and benchmarks, which serve as crucial components in assessing performance of LLMs. Then, we summarize the success and failure cases of LLMs in different tasks. Finally, we shed light on several future challenges that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to researchers in the realm of LLMs evaluation, thereby aiding the development of more proficient LLMs. Our key point is that evaluation should be treated as an essential discipline to better assist the development of LLMs. We consistently maintain the related open-source materials at: https://github.com/MLGroupJLU/LLM-eval-survey.},
	language = {en},
	urldate = {2024-12-10},
	publisher = {arXiv},
	author = {Chang, Yupeng and Wang, Xu and Wang, Jindong and Wu, Yuan and Yang, Linyi and Zhu, Kaijie and Chen, Hao and Yi, Xiaoyuan and Wang, Cunxiang and Wang, Yidong and Ye, Wei and Zhang, Yue and Chang, Yi and Yu, Philip S. and Yang, Qiang and Xie, Xing},
	month = dec,
	year = {2023},
	keywords = {Benchmarks},
	annote = {Comment: Accepted by ACM Transactions on Intelligent Systems and Technology (TIST); 45 pages; More recent works; https://llm-eval.github.io/},
	file = {Chang et al. - 2023 - A Survey on Evaluation of Large Language Models.pdf:C\:\\Users\\PURIFER\\Zotero\\storage\\3CCACY6F\\Chang et al. - 2023 - A Survey on Evaluation of Large Language Models.pdf:application/pdf},
}

@online{dalrymple2024,
  title = {Towards {{Guaranteed Safe AI}}: {{A Framework}} for {{Ensuring Robust}} and {{Reliable AI Systems}}},
  shorttitle = {Towards {{Guaranteed Safe AI}}},
  author = {Dalrymple, David "davidad" and Skalse, Joar and Bengio, Yoshua and Russell, Stuart and Tegmark, Max and Seshia, Sanjit and Omohundro, Steve and Szegedy, Christian and Goldhaber, Ben and Ammann, Nora and Abate, Alessandro and Halpern, Joe and Barrett, Clark and Zhao, Ding and Zhi-Xuan, Tan and Wing, Jeannette and Tenenbaum, Joshua},
  date = {2024-07-08},
  eprint = {2405.06624},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2405.06624},
  urldate = {2024-12-10},
  abstract = {Ensuring that AI systems reliably and robustly avoid harmful or dangerous behaviours is a crucial challenge, especially for AI systems with a high degree of autonomy and general intelligence, or systems used in safety-critical contexts. In this position paper, we will introduce and define a family of approaches to AI safety, which we will refer to as guaranteed safe (GS) AI. The core feature of these approaches is that they aim to produce AI systems which are equipped with highassurance quantitative safety guarantees. This is achieved by the interplay of three core components: a world model (which provides a mathematical description of how the AI system affects the outside world in a way that appropriately handles both Bayesian and Knighting uncertainty), a safety specification (which is a mathematical description of what effects are acceptable), and a verifier (which provides an auditable proof certificate that the AI satisfies the safety specification relative to the world model). We outline a number of approaches for creating each of these three core components, describe the main technical challenges, and suggest a number of potential solutions to them. We also argue for the necessity of this approach to AI safety, and for the inadequacy of the main alternative approaches.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Benchmarks},
  file = {C:\Users\erimari\Zotero\storage\EGBPTF8Y\Dalrymple et al. - 2024 - Towards Guaranteed Safe AI A Framework for Ensuri.pdf}
}
@online{li2024b,
  title = {An {{Open Source Data Contamination Report}} for {{Large Language Models}}},
  author = {Li, Yucheng and Guerin, Frank and Lin, Chenghua},
  date = {2024-01-29},
  eprint = {2310.17589},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2310.17589},
  urldate = {2024-12-10},
  abstract = {Data contamination in model evaluation has become increasingly prevalent with the growing popularity of large language models. It allows models to "cheat" via memorisation instead of displaying true capabilities. Therefore, contamination analysis has become an crucial part of reliable model evaluation to validate results. However, existing contamination analysis is usually conducted internally by large language model developers and often lacks transparency and completeness. This paper presents an extensive data contamination report for over 15 popular large language models across six popular multiple-choice QA benchmarks. We also introduce an open-source pipeline that enables the community to perform contamination analysis on customised data and models. Our experiments reveal varying contamination levels ranging from 1\% to 45\% across benchmarks, with the contamination degree increasing rapidly over time. Performance analysis of large language models indicates that data contamination does not necessarily lead to increased model metrics: while significant accuracy boosts of up to 14\% and 7\% are observed on contaminated C-Eval and Hellaswag benchmarks, only a minimal increase is noted on contaminated MMLU. We also find larger models seem able to gain more advantages than smaller models on contaminated test sets.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Benchmarks},
  file = {C:\Users\erimari\Zotero\storage\9T3V355Q\Li et al. - 2024 - An Open Source Data Contamination Report for Large.pdf}
}

@misc{subramonian2023,
	title = {It {Takes} {Two} to {Tango}: {Navigating} {Conceptualizations} of {NLP} {Tasks} and {Measurements} of {Performance}},
	shorttitle = {It {Takes} {Two} to {Tango}},
	url = {http://arxiv.org/abs/2305.09022},
	abstract = {Progress in NLP is increasingly measured through benchmarks; hence, contextualizing progress requires understanding when and why practitioners may disagree about the validity of benchmarks. We develop a taxonomy of disagreement, drawing on tools from measurement modeling, and distinguish between two types of disagreement: 1) how tasks are conceptualized and 2) how measurements of model performance are operationalized. To provide evidence for our taxonomy, we conduct a meta-analysis of relevant literature to understand how NLP tasks are conceptualized, as well as a survey of practitioners about their impressions of different factors that affect benchmark validity. Our meta-analysis and survey across eight tasks, ranging from coreference resolution to question answering, uncover that tasks are generally not clearly and consistently conceptualized and benchmarks suffer from operationalization disagreements. These findings support our proposed taxonomy of disagreement. Finally, based on our taxonomy, we present a framework for constructing benchmarks and documenting their limitations.},
	language = {en},
	urldate = {2024-12-10},
	publisher = {arXiv},
	author = {Subramonian, Arjun and Yuan, Xingdi and III, Hal Daumé and Blodgett, Su Lin},
	month = may,
	year = {2023},
	keywords = {Benchmarks},
}

@inproceedings{sculley2018,
	address = {Vancouver, BC, Canada},
	title = {Winner's {Curse}? {On} {Pace}, {Progress}, and {Empirical} {Rigor}},
	url = {https://openreview.net/pdf?id=rJWF0Fywf},
	abstract = {The ﬁeld of ML is distinguished both by rapid innovation and rapid dissemination of results. While the pace of progress has been extraordinary by any measure, in this paper we explore potential issues that we believe to be arising as a result. In particular, we observe that the rate of empirical advancement may not have been matched by consistent increase in the level of empirical rigor across the ﬁeld as a whole. This short position paper highlights examples where progress has actually been slowed as a result, offers thoughts on incentive structures currently at play, and gives suggestions as seeds for discussions on productive change.},
	language = {en},
	author = {Sculley, D and Snoek, Jasper and Rahimi, Ali and Wiltschko, Alex},
	year = {2018},
	keywords = {Benchmarks},
	file = {Sculley et al. - 2018 - ON PACE, PROGRESS, AND EMPIRICAL RIGOR.pdf:C\:\\Users\\PURIFER\\Zotero\\storage\\PX3T4HD7\\Sculley et al. - 2018 - ON PACE, PROGRESS, AND EMPIRICAL RIGOR.pdf:application/pdf},
}

@misc{chollet2019,
	title = {On the {Measure} of {Intelligence}},
	url = {http://arxiv.org/abs/1911.01547},
	abstract = {To make deliberate progress towards more intelligent and more human-like artiﬁcial systems, we need to be following an appropriate feedback signal: we need to be able to deﬁne and evaluate intelligence in a way that enables comparisons between two systems, as well as comparisons with humans. Over the past hundred years, there has been an abundance of attempts to deﬁne and measure intelligence, across both the ﬁelds of psychology and AI. We summarize and critically assess these deﬁnitions and evaluation approaches, while making apparent the two historical conceptions of intelligence that have implicitly guided them. We note that in practice, the contemporary AI community still gravitates towards benchmarking intelligence by comparing the skill exhibited by AIs and humans at speciﬁc tasks, such as board games and video games. We argue that solely measuring skill at any given task falls short of measuring intelligence, because skill is heavily modulated by prior knowledge and experience: unlimited priors or unlimited training data allow experimenters to “buy” arbitrary levels of skills for a system, in a way that masks the system’s own generalization power. We then articulate a new formal deﬁnition of intelligence based on Algorithmic Information Theory, describing intelligence as skill-acquisition efﬁciency and highlighting the concepts of scope, generalization difﬁculty, priors, and experience, as critical pieces to be accounted for in characterizing intelligent systems. Using this deﬁnition, we propose a set of guidelines for what a general AI benchmark should look like. Finally, we present a new benchmark closely following these guidelines, the Abstraction and Reasoning Corpus (ARC), built upon an explicit set of priors designed to be as close as possible to innate human priors. We argue that ARC can be used to measure a human-like form of general ﬂuid intelligence and that it enables fair general intelligence comparisons between AI systems and humans.},
	language = {en},
	urldate = {2024-10-17},
	publisher = {arXiv},
	author = {Chollet, François},
	month = nov,
	year = {2019},
	keywords = {Evaluation metrics, Benchmark testing},
}

@online{erenrich2023,
  title = {Errors in the {{MMLU}}: {{The Deep Learning Benchmark}} Is {{Wrong Surprisingly Often}}},
  author = {Erenrich, Daniel},
  date = {2023-08-23},
  url = {https://derenrich.medium.com/errors-in-the-mmlu-the-deep-learning-benchmark-is-wrong-surprisingly-often-7258bb045859},
  langid = {english},
  organization = {Medium},
  keywords = {Benchmarks},
  file = {C:\Users\erimari\Zotero\storage\N6XDMXKJ\Erenrich - Errors in the MMLU The Deep Learning Benchmark is.pdf}
}

@misc{qi2023,
	title = {Fine-tuning {Aligned} {Language} {Models} {Compromises} {Safety}, {Even} {When} {Users} {Do} {Not} {Intend} {To}!},
	url = {http://arxiv.org/abs/2310.03693},
	abstract = {Optimizing large language models (LLMs) for downstream use cases often involves the customization of pre-trained LLMs through further fine-tuning. Meta’s open release of Llama models and OpenAI’s APIs for fine-tuning GPT-3.5 Turbo on custom datasets also encourage this practice. But, what are the safety costs associated with such custom fine-tuning? We note that while existing safety alignment infrastructures can restrict harmful behaviors of LLMs at inference time, they do not cover safety risks when fine-tuning privileges are extended to end-users. Our red teaming studies find that the safety alignment of LLMs can be compromised by fine-tuning with only a few adversarially designed training examples. For instance, we jailbreak GPT-3.5 Turbo’s safety guardrails by fine-tuning it on only 10 such examples at a cost of less than \$0.20 via OpenAI’s APIs, making the model responsive to nearly any harmful instructions. Disconcertingly, our research also reveals that, even without malicious intent, simply fine-tuning with benign and commonly used datasets can also inadvertently degrade the safety alignment of LLMs, though to a lesser extent. These findings suggest that fine-tuning aligned LLMs introduces new safety risks that current safety infrastructures fall short of addressing — even if a model’s initial safety alignment is impeccable, it is not necessarily to be maintained after custom fine-tuning1. We outline and critically analyze potential mitigations and advocate for further research efforts toward reinforcing safety protocols for the custom fine-tuning of aligned LLMs.},
	language = {en},
	urldate = {2024-12-03},
	publisher = {arXiv},
	author = {Qi, Xiangyu and Zeng, Yi and Xie, Tinghao and Chen, Pin-Yu and Jia, Ruoxi and Mittal, Prateek and Henderson, Peter},
	month = oct,
	year = {2023},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Cryptography and Security},
}

@misc{nasr2023,
	title = {Scalable {Extraction} of {Training} {Data} from ({Production}) {Language} {Models}},
	url = {http://arxiv.org/abs/2311.17035},
	abstract = {This paper studies extractable memorization: training data that an adversary can efficiently extract by querying a machine learning model without prior knowledge of the training dataset. We show an adversary can extract gigabytes of training data from open-source language models like Pythia or GPT-Neo, semi-open models like LLaMA or Falcon, and closed models like ChatGPT. Existing techniques from the literature suffice to attack unaligned models; in order to attack the aligned ChatGPT, we develop a new divergence attack that causes the model to diverge from its chatbot-style generations and emit training data at a rate 150× higher than when behaving properly. Our methods show practical attacks can recover far more data than previously thought, and reveal that current alignment techniques do not eliminate memorization.},
	language = {en},
	urldate = {2024-12-03},
	publisher = {arXiv},
	author = {Nasr, Milad and Carlini, Nicholas and Hayase, Jonathan and Jagielski, Matthew and Cooper, A. Feder and Ippolito, Daphne and Choquette-Choo, Christopher A. and Wallace, Eric and Tramèr, Florian and Lee, Katherine},
	month = nov,
	year = {2023},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Cryptography and Security},
	file = {Nasr et al. - 2023 - Scalable Extraction of Training Data from (Product.pdf:C\:\\Users\\PURIFER\\Zotero\\storage\\MCUDQI92\\Nasr et al. - 2023 - Scalable Extraction of Training Data from (Product.pdf:application/pdf},
}

@misc{nasr2023a,
	title = {Extracting {Training} {Data} from {ChatGPT}},
	url = {https://not-just-memorization.github.io/extracting-training-data-from-chatgpt.html?ref=404media.co},
	language = {en},
	urldate = {2024-12-03},
	journal = {not-just-memorization.github.io},
	author = {Nasr, Milad and Carlini, Nicholas and Hayase, Jonathan and Jagielski, Matthew and Cooper, A. Feder and Ippolito, Daphne and Choquette-Choo, Christopher A. and Wallace, Eric and Lee, Katherine},
	month = nov,
	year = {2023},
	file = {Nasr - Extracting Training Data from ChatGPT.pdf:C\:\\Users\\PURIFER\\Zotero\\storage\\VUUWX2AY\\Nasr - Extracting Training Data from ChatGPT.pdf:application/pdf},
}

@inproceedings{mitchell2019,
	title = {Model {Cards} for {Model} {Reporting}},
	url = {http://arxiv.org/abs/1810.03993},
	doi = {10.1145/3287560.3287596},
	abstract = {Trained machine learning models are increasingly used to perform high-impact tasks in areas such as law enforcement, medicine, education, and employment. In order to clarify the intended use cases of machine learning models and minimize their usage in contexts for which they are not well suited, we recommend that released models be accompanied by documentation detailing their performance characteristics. In this paper, we propose a framework that we call model cards, to encourage such transparent model reporting. Model cards are short documents accompanying trained machine learning models that provide benchmarked evaluation in a variety of conditions, such as across different cultural, demographic, or phenotypic groups (e.g., race, geographic location, sex, Fitzpatrick skin type [15]) and intersectional groups (e.g., age and race, or sex and Fitzpatrick skin type) that are relevant to the intended application domains. Model cards also disclose the context in which models are intended to be used, details of the performance evaluation procedures, and other relevant information. While we focus primarily on human-centered machine learning models in the application fields of computer vision and natural language processing, this framework can be used to document any trained machine learning model. To solidify the concept, we provide cards for two supervised models: One trained to detect smiling faces in images, and one trained to detect toxic comments in text. We propose model cards as a step towards the responsible democratization of machine learning and related artificial intelligence technology, increasing transparency into how well artificial intelligence technology works. We hope this work encourages those releasing trained machine learning models to accompany model releases with similar detailed evaluation numbers and other relevant documentation.},
	language = {en},
	urldate = {2023-08-22},
	booktitle = {Proceedings of the {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	author = {Mitchell, Margaret and Wu, Simone and Zaldivar, Andrew and Barnes, Parker and Vasserman, Lucy and Hutchinson, Ben and Spitzer, Elena and Raji, Inioluwa Deborah and Gebru, Timnit},
	month = jan,
	year = {2019},
	keywords = {AI, AI and ethics, Assisted thinking, Model card},
	pages = {220--229},
}

@misc{alzahrani2024,
	title = {When {Benchmarks} are {Targets}: {Revealing} the {Sensitivity} of {Large} {Language} {Model} {Leaderboards}},
	shorttitle = {When {Benchmarks} are {Targets}},
	url = {http://arxiv.org/abs/2402.01781},
	language = {en},
	urldate = {2024-11-22},
	publisher = {arXiv},
	author = {Alzahrani, Norah and Alyahya, Hisham Abdullah and Alnumay, Yazeed and Alrashed, Sultan and Alsubaie, Shaykhah and Almushaykeh, Yusef and Mirza, Faisal and Alotaibi, Nouf and Altwairesh, Nora and Alowisheq, Areeb and Bari, M. Saiful and Khan, Haidar},
	month = jul,
	year = {2024},
}

@misc{gebru2021,
	title = {Datasheets for {Datasets}},
	url = {http://arxiv.org/abs/1803.09010},
	abstract = {The machine learning community currently has no standardized process for documenting datasets, which can lead to severe consequences in high-stakes domains. To address this gap, we propose datasheets for datasets. In the electronics industry, every component, no matter how simple or complex, is accompanied with a datasheet that describes its operating characteristics, test results, recommended uses, and other information. By analogy, we propose that every dataset be accompanied with a datasheet that documents its motivation, composition, collection process, recommended uses, and so on. Datasheets for datasets will facilitate better communication between dataset creators and dataset consumers, and encourage the machine learning community to prioritize transparency and accountability.},
	language = {en},
	urldate = {2024-11-13},
	publisher = {arXiv},
	author = {Gebru, Timnit and Morgenstern, Jamie and Vecchione, Briana and Vaughan, Jennifer Wortman and Wallach, Hanna and III, Hal Daumé and Crawford, Kate},
	month = dec,
	year = {2021},
	keywords = {Benchmarka},
	annote = {Comment: Published in CACM in December, 2021},
	file = {Gebru et al. - 2021 - Datasheets for Datasets.pdf:C\:\\Users\\PURIFER\\Zotero\\storage\\6WKQQ2I7\\Gebru et al. - 2021 - Datasheets for Datasets.pdf:application/pdf},
}

@misc{orr2024b,
	title = {Building {Better} {Datasets}: {Seven} {Recommendations} for {Responsible} {Design} from {Dataset} {Creators}},
	shorttitle = {Building {Better} {Datasets}},
	url = {http://arxiv.org/abs/2409.00252},
	abstract = {The increasing demand for high-quality datasets in machine learning has raised concerns about the ethical and responsible creation of these datasets. Dataset creators play a crucial role in developing responsible practices, yet their perspectives and expertise have not yet been highlighted in the current literature. In this paper, we bridge this gap by presenting insights from a qualitative study that included interviewing 18 leading dataset creators about the current state of the field. We shed light on the challenges and considerations faced by dataset creators, and our findings underscore the potential for deeper collaboration, knowledge sharing, and collective development. Through a close analysis of their perspectives, we share seven central recommendations for improving responsible dataset creation, including issues such as data quality, documentation, privacy and consent, and how to mitigate potential harms from unintended use cases. By fostering critical reflection and sharing the experiences of dataset creators, we aim to promote responsible dataset creation practices and develop a nuanced understanding of this crucial but often undervalued aspect of machine learning research.},
	language = {en},
	urldate = {2024-11-05},
	publisher = {arXiv},
	author = {Orr, Will and Crawford, Kate},
	month = aug,
	year = {2024},
	keywords = {Benchmark testing, Datasets, AI and ethics},
}

@online{wang2024,
  title = {{{MMLU-Pro}}: {{A More Robust}} and {{Challenging Multi-Task Language Understanding Benchmark}}},
  shorttitle = {{{MMLU-Pro}}},
  author = {Wang, Yubo and Ma, Xueguang and Zhang, Ge and Ni, Yuansheng and Chandra, Abhranil and Guo, Shiguang and Ren, Weiming and Arulraj, Aaran and He, Xuan and Jiang, Ziyan and Li, Tianle and Ku, Max and Wang, Kai and Zhuang, Alex and Fan, Rongqi and Yue, Xiang and Chen, Wenhu},
  date = {2024-11-06},
  eprint = {2406.01574},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2406.01574},
  url = {http://arxiv.org/abs/2406.01574},
  urldate = {2024-12-13},
  abstract = {In the age of large-scale language models, benchmarks like the Massive Multitask Language Understanding (MMLU) have been pivotal in pushing the boundaries of what AI can achieve in language comprehension and reasoning across diverse domains. However, as models continue to improve, their performance on these benchmarks has begun to plateau, making it increasingly difficult to discern differences in model capabilities. This paper introduces MMLU-Pro, an enhanced dataset designed to extend the mostly knowledge-driven MMLU benchmark by integrating more challenging, reasoning-focused questions and expanding the choice set from four to ten options. Additionally, MMLU-Pro eliminates the trivial and noisy questions in MMLU. Our experimental results show that MMLU-Pro not only raises the challenge, causing a significant drop in accuracy by 16\% to 33\% compared to MMLU but also demonstrates greater stability under varying prompts. With 24 different prompt styles tested, the sensitivity of model scores to prompt variations decreased from 4-5\% in MMLU to just 2\% in MMLU-Pro. Additionally, we found that models utilizing Chain of Thought (CoT) reasoning achieved better performance on MMLU-Pro compared to direct answering, which is in stark contrast to the findings on the original MMLU, indicating that MMLU-Pro includes more complex reasoning questions. Our assessments confirm that MMLU-Pro is a more discriminative benchmark to better track progress in the field.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  file = {C:\Users\erimari\Zotero\storage\YCA4YXJ6\Wang et al. - 2024 - MMLU-Pro A More Robust and Challenging Multi-Task.pdf}
}

@inproceedings{smith2022,
	title = {{REAL} {ML}: {Recognizing}, {Exploring}, and {Articulating} {Limitations} of {Machine} {Learning} {Research}},
	shorttitle = {{REAL} {ML}},
	url = {http://arxiv.org/abs/2205.08363},
	doi = {10.1145/3531146.3533122},
	abstract = {CCS Concepts: • Human-centered computing → User studies; • General and reference → Design; Validation; Reliability; Computing standards, RFCs and guidelines; • Computing methodologies → Machine learning.},
	language = {en},
	urldate = {2024-12-19},
	booktitle = {2022 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	author = {Smith, Jessie J. and Amershi, Saleema and Barocas, Solon and Wallach, Hanna and Vaughan, Jennifer Wortman},
	month = jun,
	year = {2022},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computers and Society},
	pages = {587--597},
	annote = {Comment: This work appears in the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT '22)},
	file = {Smith et al. - 2022 - REAL ML Recognizing, Exploring, and Articulating .08363:C\:\\Users\\PURIFER\\Zotero\\storage\\4XPR58LA\\Smith et al. - 2022 - REAL ML Recognizing, Exploring, and Articulating .08363:application/pdf},
}

@misc{srivastava2023,
	type = {Language models},
	title = {Beyond the {Imitation} {Game}: {Quantifying} and extrapolating the capabilities of language models},
	shorttitle = {Beyond the {Imitation} {Game}},
	url = {http://arxiv.org/abs/2206.04615},
	abstract = {Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models. To address this challenge, we introduce the Beyond the Imitation Game benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 450 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI's GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide a strong baseline. Findings include: model performance and calibration both improve with scale, but are poor in absolute terms (and when compared with rater performance); performance is remarkably similar across model classes, though with benefits from sparsity; tasks that improve gradually and predictably commonly involve a large knowledge or memorization component, whereas tasks that exhibit "breakthrough" behavior at a critical scale often involve multiple steps or components, or brittle metrics; social bias typically increases with scale in settings with ambiguous context, but this can be improved with prompting.},
	language = {en},
	urldate = {2024-12-10},
	publisher = {arXiv},
	author = {Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R. and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adrià and Kluska, Agnieszka and Lewkowycz, Aitor and Agarwal, Akshat and Power, Alethea and Ray, Alex and Warstadt, Alex and others},
	month = jun,
	year = {2023},
	keywords = {Benchmarks},
}

@inproceedings{simson2024,
	address = {Rio de Janeiro Brazil},
	title = {Lazy {Data} {Practices} {Harm} {Fairness} {Research}},
	isbn = {9798400704505},
	url = {https://dl.acm.org/doi/10.1145/3630106.3658931},
	doi = {10.1145/3630106.3658931},
	abstract = {Data practices shape research and practice on fairness in machine learning (fair ML). Critical data studies offer important reflections and critiques for the responsible advancement of the field by highlighting shortcomings and proposing recommendations for improvement. In this work, we present a comprehensive analysis of fair ML datasets, demonstrating how unreflective yet common practices hinder the reach and reliability of algorithmic fairness findings. We systematically study protected information encoded in tabular datasets and their usage in 280 experiments across 142 publications. Our analyses identify three main areas of concern: (1) a lack of representation for certain protected attributes in both data and evaluations; (2) the widespread exclusion of minorities during data preprocessing; and (3) opaque data processing threatening the generalization of fairness research. By conducting exemplary analyses on the utilization of prominent datasets, we demonstrate how unreflective data decisions disproportionately affect minority groups, fairness metrics, and resultant model comparisons. Additionally, we identify supplementary factors such as limitations in publicly available data, privacy considerations, and a general lack of awareness, which exacerbate these challenges. To address these issues, we propose a set of recommendations for data usage in fairness research centered on transparency and responsible inclusion. This study underscores the need for a critical reevaluation of data practices in fair ML and offers directions to improve both the sourcing and usage of datasets.},
	language = {en},
	urldate = {2024-11-15},
	booktitle = {The 2024 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {ACM},
	author = {Simson, Jan and Fabris, Alessandro and Kern, Christoph},
	month = jun,
	year = {2024},
	keywords = {Benchmarks},
	pages = {642--659},
	file = {Simson et al. - 2024 - Lazy Data Practices Harm Fairness Research.pdf:C\:\\Users\\PURIFER\\Zotero\\storage\\3B5FRM25\\Simson et al. - 2024 - Lazy Data Practices Harm Fairness Research.pdf:application/pdf},
}

@misc{narayanan2023a,
  title = {Evaluating {{LLMs}} Is a Minefield},
  author = {Narayanan, Arvind and Kapoor, Sayash},
  year = {2023},
  date = {2023-10-04},
  url = {https://www.cs.princeton.edu/~arvindn/talks/evaluating_llms_minefield/}
}

@online{pushkarna2022,
  title = {Data {{Cards}}: {{Purposeful}} and {{Transparent Dataset Documentation}} for {{Responsible AI}}},
  shorttitle = {Data {{Cards}}},
  author = {Pushkarna, Mahima and Zaldivar, Andrew and Kjartansson, Oddur},
  date = {2022-04-03},
  eprint = {2204.01075},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2204.01075},
  url = {http://arxiv.org/abs/2204.01075},
  urldate = {2024-12-17},
  abstract = {As research and industry moves towards large-scale models capable of numerous downstream tasks, the complexity of understanding multi-modal datasets that give nuance to models rapidly increases. A clear and thorough understanding of a dataset's origins, development, intent, ethical considerations and evolution becomes a necessary step for the responsible and informed deployment of models, especially those in people-facing contexts and high-risk domains. However, the burden of this understanding often falls on the intelligibility, conciseness, and comprehensiveness of the documentation. It requires consistency and comparability across the documentation of all datasets involved, and as such documentation must be treated as a user-centric product in and of itself. In this paper, we propose Data Cards for fostering transparent, purposeful and human-centered documentation of datasets within the practical contexts of industry and research. Data Cards are structured summaries of essential facts about various aspects of ML datasets needed by stakeholders across a dataset's lifecycle for responsible AI development. These summaries provide explanations of processes and rationales that shape the data and consequently the models, such as upstream sources, data collection and annotation methods; training and evaluation methods, intended use; or decisions affecting model performance. We also present frameworks that ground Data Cards in real-world utility and human-centricity. Using two case studies, we report on desirable characteristics that support adoption across domains, organizational structures, and audience groups. Finally, we present lessons learned from deploying over 20 Data Cards.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Benchmarks},
  file = {C:\Users\erimari\Zotero\storage\752RGWGM\Pushkarna et al. - 2022 - Data Cards Purposeful and Transparent Dataset Doc.pdf}
}
@online{perez2022,
  title = {Red {{Teaming Language Models}} with {{Language Models}}},
  author = {Perez, Ethan and Huang, Saffron and Song, Francis and Cai, Trevor and Ring, Roman and Aslanides, John and Glaese, Amelia and McAleese, Nat and Irving, Geoffrey},
  date = {2022-02-07},
  eprint = {2202.03286},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2202.03286},
  url = {http://arxiv.org/abs/2202.03286},
  urldate = {2024-12-17},
  abstract = {Language Models (LMs) often cannot be deployed because of their potential to harm users in hard-to-predict ways. Prior work identifies harmful behaviors before deployment by using human annotators to hand-write test cases. However, human annotation is expensive, limiting the number and diversity of test cases. In this work, we automatically find cases where a target LM behaves in a harmful way, by generating test cases (“red teaming”) using another LM. We evaluate the target LM’s replies to generated test questions using a classifier trained to detect offensive content, uncovering tens of thousands of offensive replies in a 280B parameter LM chatbot. We explore several methods, from zero-shot generation to reinforcement learning, for generating test cases with varying levels of diversity and difficulty. Furthermore, we use prompt engineering to control LM-generated test cases to uncover a variety of other harms, automatically finding groups of people that the chatbot discusses in offensive ways, personal and hospital phone numbers generated as the chatbot’s own contact info, leakage of private training data in generated text, and harms that occur over the course of a conversation. Overall, LM-based red teaming is one promising tool (among many needed) for finding and fixing diverse, undesirable LM behaviors before impacting users.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {C:\Users\erimari\Zotero\storage\Z4V47MS4\Perez et al. - 2022 - Red Teaming Language Models with Language Models.pdf}
}
@online{inan2023,
  title = {Llama {{Guard}}: {{LLM-based Input-Output Safeguard}} for {{Human-AI Conversations}}},
  shorttitle = {Llama {{Guard}}},
  author = {Inan, Hakan and Upasani, Kartikeya and Chi, Jianfeng and Rungta, Rashi and Iyer, Krithika and Mao, Yuning and Tontchev, Michael and Hu, Qing and Fuller, Brian and Testuggine, Davide and Khabsa, Madian},
  date = {2023-12-07},
  eprint = {2312.06674},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2312.06674},
  url = {http://arxiv.org/abs/2312.06674},
  urldate = {2024-12-17},
  abstract = {We introduce Llama Guard, an LLM-based input-output safeguard model geared towards Human-AI conversation use cases. Our model incorporates a safety risk taxonomy, a valuable tool for categorizing a specific set of safety risks found in LLM prompts (i.e., prompt classification). This taxonomy is also instrumental in classifying the responses generated by LLMs to these prompts, a process we refer to as response classification. For the purpose of both prompt and response classification, we have meticulously gathered a dataset of high quality. Llama Guard, a Llama2-7b model that is instruction-tuned on our collected dataset, albeit low in volume, demonstrates strong performance on existing benchmarks such as the OpenAI Moderation Evaluation dataset and ToxicChat, where its performance matches or exceeds that of currently available content moderation tools. Llama Guard functions as a language model, carrying out multi-class classification and generating binary decision scores. Furthermore, the instruction fine-tuning of Llama Guard allows for the customization of tasks and the adaptation of output formats. This feature enhances the model's capabilities, such as enabling the adjustment of taxonomy categories to align with specific use cases, and facilitating zero-shot or few-shot prompting with diverse taxonomies at the input. We are making Llama Guard model weights available and we encourage researchers to further develop and adapt them to meet the evolving needs of the community for AI safety.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C:\Users\erimari\Zotero\storage\PTQJVTNP\Inan et al. - 2023 - Llama Guard LLM-based Input-Output Safeguard for .pdf}
}

@misc{gema2024,
	title = {Are {We} {Done} with {MMLU}?},
	url = {http://arxiv.org/abs/2406.04127},
	doi = {10.48550/arXiv.2406.04127},
	abstract = {Maybe not. We identify and analyse errors in the popular Massive Multitask Language Understanding (MMLU) benchmark. Even though MMLU is widely adopted, our analysis demonstrates numerous ground truth errors that obscure the true capabilities of LLMs. For example, we find that 57\% of the analysed questions in the Virology subset contain errors. To address this issue, we introduce a comprehensive framework for identifying dataset errors using a novel error taxonomy. Then, we create MMLU-Redux, which is a subset of 3,000 manually re-annotated questions across 30 MMLU subjects. Using MMLU-Redux, we demonstrate significant discrepancies with the model performance metrics that were originally reported. Our results strongly advocate for revising MMLU's error-ridden questions to enhance its future utility and reliability as a benchmark. Therefore, we open up MMLU-Redux for additional annotation https://huggingface.co/datasets/edinburgh-dawg/mmlu-redux.},
	urldate = {2024-11-26},
	publisher = {arXiv},
	author = {Gema, Aryo Pradipta and Leang, Joshua Ong Jun and Hong, Giwon and Devoto, Alessio and Mancino, Alberto Carlo Maria and Saxena, Rohit and He, Xuanli and Zhao, Yu and Du, Xiaotang and Madani, Mohammad Reza Ghasemi and Barale, Claire and McHardy, Robert and Harris, Joshua and Kaddour, Jean and Krieken, Emile van and Minervini, Pasquale},
	month = jun,
	year = {2024},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\PURIFER\\Zotero\\storage\\TZDGQYFL\\Gema et al. - 2024 - Are We Done with MMLU.pdf:application/pdf;Snapshot:C\:\\Users\\PURIFER\\Zotero\\storage\\DBLZDTJV\\2406.html:text/html},
}

@article{kang2023,
	title = {Ground truth tracings ({GTT}): {On} the epistemic limits of machine learning},
	volume = {10},
	issn = {2053-9517, 2053-9517},
	shorttitle = {Ground truth tracings ({GTT})},
	url = {https://journals.sagepub.com/doi/10.1177/20539517221146122},
	doi = {10.1177/20539517221146122},
	abstract = {There is a gap in existing critical scholarship that engages with the ways in which current “machine listening” or voice analytics/biometric systems intersect with the technical speciﬁcities of machine learning. This article examines the sociotechnical assemblage of machine learning techniques, practices, and cultures that underlie these technologies. After engaging with various practitioners working in companies that develop machine listening systems, ranging from CEOs, machine learning engineers, data scientists, and business analysts, among others, I bring attention to the centrality of “learnability” as a malleable conceptual framework that bends according to various “ground-truthing” practices in formalizing certain listening-based prediction tasks for machine learning. In response, I introduce a process I call Ground Truth Tracings to examine the various ontological translations that occur in training a machine to “learn to listen.” Ultimately, by further examining this notion of learnability through the aperture of power, I take insights acquired through my ﬁeldwork in the machine listening industry and propose a strategically reductive heuristic through which the epistemological and ethical soundness of machine learning, writ large, can be contemplated.},
	language = {en},
	number = {1},
	urldate = {2024-11-07},
	journal = {Big Data \& Society},
	author = {Kang, Edward B},
	month = jan,
	year = {2023},
	keywords = {Benchmarks, Algorithmic auditing},
	pages = {20539517221146122},
	file = {Kang - 2023 - Ground truth tracings (GTT) On the epistemic limi.pdf:C\:\\Users\\PURIFER\\Zotero\\storage\\DUUCARM6\\Kang - 2023 - Ground truth tracings (GTT) On the epistemic limi.pdf:application/pdf},
}

@misc{ojewale2024,
	title = {Towards {AI} {Accountability} {Infrastructure}: {Gaps} and {Opportunities} in {AI} {Audit} {Tooling}},
	shorttitle = {Towards {AI} {Accountability} {Infrastructure}},
	url = {http://arxiv.org/abs/2402.17861},
	abstract = {Audits are critical mechanisms for identifying the risks and limitations of deployed artificial intelligence (AI) systems. However, the effective execution of AI audits remains incredibly difficult. As a result, practitioners make use of various tools to support their efforts. Drawing on interviews with 35 AI audit practitioners and a landscape analysis of 390 tools, we map the current ecosystem of available AI audit tools. While there are many tools designed to assist practitioners with setting standards and evaluating AI systems, these tools often fell short of supporting the accountability goals of AI auditing in practice. We thus highlight areas for future tool development beyond evaluation—from harms discovery to advocacy—and outline challenges practitioners faced in their efforts to use AI audit tools. We conclude that resources are lacking to adequately support the full scope of needs for many AI audit practitioners and recommend that the field move beyond tools for just evaluation, towards more comprehensive infrastructure for AI accountability.},
	language = {en},
	urldate = {2024-10-17},
	publisher = {arXiv},
	author = {Ojewale, Victor and Steed, Ryan and Vecchione, Briana and Birhane, Abeba and Raji, Inioluwa Deborah},
	month = mar,
	year = {2024},
	keywords = {Fairness, Evaluation metrics, Benchmark testing, AI auditing, Algorithmic Auditing, Risk},
}

@article{luitse2024,
	title = {{AI} competitions as infrastructures of power in medical imaging},
	issn = {1369-118X, 1468-4462},
	url = {https://www.tandfonline.com/doi/full/10.1080/1369118X.2024.2334393},
	doi = {10.1080/1369118X.2024.2334393},
	abstract = {This article examines how platform-based AI competitions structure power relations in medical imaging research. It focuses on two leading platforms, Kaggle and Grand Challenge, which provide organisational as well as infrastructural support to run AI competitions. In dialogue with critical AI and platform studies research, we investigate how such competitions are organised –under which infrastructural conditions and by whom – and how this shapes processes of model production and evaluation. To address these concerns, we have collected data from 118 medical image AI competitions on Kaggle and Grand Challenge, organised between January 2017 and May 2022. In addition, a variety of platform boundary resources – platform documentation, competition descriptions, dataset descriptions, and competition leaderboards – have been gathered. The analysis of these materials shows, ﬁrst, that platforms direct the AI development process by requiring substantial ﬁnancial resources, deﬁning which institutions can host a competition and under which conditions. Second, competition organisers deﬁne dataset diversity and the generalisability of models. As most datasets are constructed with data from hospitals in North America, Western Europe and China, the application of models to diﬀerent geographical contexts is potentially limited. Finally, competition participants inﬂuence model development through the institutional, demographic, and disciplinary contexts in which they operate. Overall, the examination demonstrates the importance of critically interrogating the entire medical AI research pipeline, including the deﬁnition of research problems, the construction of datasets as well as model production and evaluation.},
	language = {en},
	urldate = {2024-11-18},
	journal = {Information, Communication \& Society},
	author = {Luitse, Dieuwertje and Blanke, Tobias and Poell, Thomas},
	month = mar,
	year = {2024},
	keywords = {Benchmarks},
	pages = {1--22},
	file = {Luitse et al. - 2024 - AI competitions as infrastructures of power in med.pdf:C\:\\Users\\PURIFER\\Zotero\\storage\\8QJ9USZQ\\Luitse et al. - 2024 - AI competitions as infrastructures of power in med.pdf:application/pdf},
}

@article{engdahl2024,
	title = {Agreements ‘in the wild’: {Standards} and alignment in machine learning benchmark dataset construction},
	volume = {11},
	issn = {2053-9517, 2053-9517},
	shorttitle = {Agreements ‘in the wild’},
	url = {https://journals.sagepub.com/doi/10.1177/20539517241242457},
	doi = {10.1177/20539517241242457},
	abstract = {This article presents an ethnographic case study of a corporate-academic group constructing a benchmark dataset of daily activities for a variety of machine learning and computer vision tasks. Using a socio-technical perspective, the article conceptualizes the dataset as a knowledge object that is stabilized by both practical standards (for daily activities, dataﬁcation, annotation and benchmarks) and alignment work – that is, efforts including forging agreements to make these standards effective in practice. By attending to alignment work, the article highlights the informal, communicative and supportive efforts that underlie the success of standards and the smoothing of tensions between actors and factors. Emphasizing these efforts constitutes a contribution in several ways. This article’s ethnographic mode of analysis challenges and supplements quantitative metrics on datasets. It advances the ﬁeld of dataset analysis by offering a detailed empirical examination of the development of a new benchmark dataset as a collective accomplishment. By showing the importance of alignment efforts and their close ties to standards and their limitations, it adds to our understanding of how machine learning datasets are built. And, most importantly, it calls into question a key characterization of the dataset: that it captures unscripted activities occurring naturally ‘in the wild’, as alignment work bleeds into moments of data capture.},
	language = {en},
	number = {2},
	urldate = {2024-11-05},
	journal = {Big Data \& Society},
	author = {Engdahl, Isak},
	month = jun,
	year = {2024},
	keywords = {Benchmark testing},
	pages = {20539517241242457},
}

@inproceedings{diberardino2024,
	address = {Rio de Janeiro Brazil},
	title = {Algorithmic {Harms} and {Algorithmic} {Wrongs}},
	isbn = {9798400704505},
	url = {https://dl.acm.org/doi/10.1145/3630106.3659001},
	doi = {10.1145/3630106.3659001},
	abstract = {New artificial intelligence (AI) systems grounded in machine learning are being integrated into our lives at a rapid rate, but not without consequence: scholars across domains have increasingly pointed out issues related to privacy, transparency, bias, discrimination, exploitation, and exclusion associated with algorithmic systems in both public and private sector contexts. Concerns surrounding the adverse impacts of these technologies have spurred discussion on the topics of algorithmic harm. However, the overwhelming majority of articles on said harms offer no definition as to what constitutes ‘harm’ in these contexts. This paper aims to address this omission by introducing one criterion for a suitable account of algorithmic harm. More specifically, we follow Joel Feinberg in understanding harms as distinct from wrongs, where only the latter necessarily carry a normative dimension. This distinction highlights issues in the current scholarship surrounding the conflation of algorithmic harms and wrongs. In response to these issues, we put forth two requirements for upholding the harms/wrongs distinction when analyzing the increasingly far-reaching impacts of these technologies and suggest how this distinction can be useful in design, engineering, and policymaking.},
	language = {en},
	urldate = {2024-11-05},
	booktitle = {The 2024 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {ACM},
	author = {Diberardino, Nathalie and Baleshta, Clair and Stark, Luke},
	month = jun,
	year = {2024},
	keywords = {Benchmark testing, Systemic risk, AI and ethics},
	pages = {1725--1732},
}

@article{maleve2023,
	title = {Practices of {Benchmarking}: {Vulnerability} in the {Computer} {Vision} {Pipeline}},
	volume = {16},
	issn = {1754-0763, 1754-0771},
	url = {https://www.tandfonline.com/doi/full/10.1080/17540763.2023.2189159},
	doi = {10.1080/17540763.2023.2189159},
	language = {en},
	number = {2},
	urldate = {2024-10-22},
	journal = {photographies},
	author = {Malevé, Nicolas},
	month = may,
	year = {2023},
	keywords = {Benchmark testing},
	pages = {173--189},
}

@book{stengers2018,
	address = {Cambridge},
	title = {Another science is possible: a manifesto for slow science},
	isbn = {978-1-5095-2180-7},
	shorttitle = {Another science is possible},
	language = {eng},
	publisher = {Polity press},
	author = {Stengers, Isabelle},
	year = {2018},
}

@misc{liang2023,
	title = {Holistic {Evaluation} of {Language} {Models}},
	url = {http://arxiv.org/abs/2211.09110},
	doi = {10.48550/arXiv.2211.09110},
	abstract = {Language models (LMs) are becoming the foundation for almost all major language technologies, but their capabilities, limitations, and risks are not well understood. We present Holistic Evaluation of Language Models (HELM) to improve the transparency of language models. First, we taxonomize the vast space of potential scenarios (i.e. use cases) and metrics (i.e. desiderata) that are of interest for LMs. Then we select a broad subset based on coverage and feasibility, noting what’s missing or underrepresented (e.g. question answering for neglected English dialects, metrics for trustworthiness). Second, we adopt a multi-metric approach: We measure 7 metrics (accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency) for each of 16 core scenarios to the extent possible (87.5\% of the time), ensuring that metrics beyond accuracy don’t fall to the wayside, and that trade-offs across models and metrics are clearly exposed. We also perform 7 targeted evaluations, based on 26 targeted scenarios, to more deeply analyze specific aspects (e.g. knowledge, reasoning, memorization/copyright, disinformation). Third, we conduct a large-scale evaluation of 30 prominent language models (spanning open, limited-access, and closed models) on all 42 scenarios, including 21 scenarios that were not previously used in mainstream LM evaluation. Prior to HELM, models on average were evaluated on just 17.9\% of the core HELM scenarios, with some prominent models not sharing a single scenario in common. We improve this to 96.0\%: now all 30 models have been densely benchmarked on a set of core scenarios and metrics under standardized conditions. Our evaluation surfaces 25 top-level findings concerning the interplay between different scenarios, metrics, and models. For full transparency, we release all raw model prompts and completions publicly1 for further analysis, as well as a general modular toolkit for easily adding new scenarios, models, metrics, and prompting strategies.2 We intend for HELM to be a living benchmark for the community, continuously updated with new scenarios, metrics, and models.},
	language = {en},
	urldate = {2024-12-18},
	publisher = {arXiv},
	author = {Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and Newman, Benjamin and Yuan, Binhang and Yan, Bobby and Zhang, Ce and Cosgrove, Christian and Manning, Christopher D. and Ré, Christopher and Acosta-Navas, Diana and Hudson, Drew A. and Zelikman, Eric and Durmus, Esin and Ladhak, Faisal and Rong, Frieda and Ren, Hongyu and Yao, Huaxiu and Wang, Jue and Santhanam, Keshav and Orr, Laurel and Zheng, Lucia and Yuksekgonul, Mert and Suzgun, Mirac and Kim, Nathan and Guha, Neel and Chatterji, Niladri and Khattab, Omar and Henderson, Peter and Huang, Qian and Chi, Ryan and Xie, Sang Michael and Santurkar, Shibani and Ganguli, Surya and Hashimoto, Tatsunori and Icard, Thomas and Zhang, Tianyi and Chaudhary, Vishrav and Wang, William and Li, Xuechen and Mai, Yifan and Zhang, Yuhui and Koreeda, Yuta},
	month = oct,
	year = {2023},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: Authored by the Center for Research on Foundation Models (CRFM) at the Stanford Institute for Human-Centered Artificial Intelligence (HAI). Project page: https://crfm.stanford.edu/helm/v1.0},
	file = {Liang et al. - 2023 - Holistic Evaluation of Language Models.pdf:C\:\\Users\\PURIFER\\Zotero\\storage\\UKCSFX75\\Liang et al. - 2023 - Holistic Evaluation of Language Models.pdf:application/pdf},
}

@misc{michael2022,
	title = {What {Do} {NLP} {Researchers} {Believe}? {Results} of the {NLP} {Community} {Metasurvey}},
	shorttitle = {What {Do} {NLP} {Researchers} {Believe}?},
	url = {http://arxiv.org/abs/2208.12852},
	abstract = {We present the results of the NLP Community Metasurvey. Run from May to June 2022, the survey elicited opinions on controversial issues, including industry inﬂuence in the ﬁeld, concerns about AGI, and ethics. Our results put concrete numbers to several controversies: For example, respondents are split almost exactly in half on questions about the importance of artiﬁcial general intelligence, whether language models understand language, and the necessity of linguistic structure and inductive bias for solving NLP problems. In addition, the survey posed metaquestions, asking respondents to predict the distribution of survey responses. This allows us not only to gain insight on the spectrum of beliefs held by NLP researchers, but also to uncover false sociological beliefs where the community’s predictions don’t match reality. We ﬁnd such mismatches on a wide range of issues. Among other results, the community greatly overestimates its own belief in the usefulness of benchmarks and the potential for scaling to solve real-world problems, while underestimating its own belief in the importance of linguistic structure, inductive bias, and interdisciplinary science.},
	language = {en},
	urldate = {2024-10-22},
	publisher = {arXiv},
	author = {Michael, Julian and Holtzman, Ari and Parrish, Alicia and Mueller, Aaron and Wang, Alex and Chen, Angelica and Madaan, Divyam and Nangia, Nikita and Pang, Richard Yuanzhe and Phang, Jason and Bowman, Samuel R.},
	month = aug,
	year = {2022},
	keywords = {Benchmark testing},
	annote = {Comment: 31 pages, 19 figures, 3 tables; more information at https://nlpsurvey.net},
}

@misc{lacroix2022,
	title = {Metaethical {Perspectives} on '{Benchmarking}' {AI} {Ethics}},
	url = {http://arxiv.org/abs/2204.05151},
	abstract = {Benchmarks are seen as the cornerstone for measuring technical progress in Artiﬁcial Intelligence (AI) research and have been developed for a variety of tasks ranging from question answering to facial recognition. An increasingly prominent research area in AI is ethics, which currently has no set of benchmarks nor commonly accepted way for measuring the ‘ethicality’ of an AI system. In this paper, drawing upon research in moral philosophy and metaethics, we argue that it is impossible to develop such a benchmark. As such, alternative mechanisms are necessary for evaluating whether an AI system is ‘ethical’. This is especially pressing in light of the prevalence of applied, industrial AI research. We argue that it makes more sense to talk about ‘values’ (and ‘value alignment’) rather than ‘ethics’ when considering the possible actions of present and future AI systems. We further highlight that, because values are unambiguously relative, focusing on values forces us to consider explicitly what the values are and whose values they are. Shifting the emphasis from ethics to values therefore gives rise to several new ways of understanding how researchers might advance research programmes for robustly safe or beneﬁcial AI. We conclude by highlighting a number of possible ways forward for the ﬁeld as a whole, and we advocate for diﬀerent approaches towards more value-aligned AI research.},
	language = {en},
	urldate = {2024-11-22},
	publisher = {arXiv},
	author = {LaCroix, Travis and Luccioni, Alexandra Sasha},
	month = apr,
	year = {2022},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computers and Society},
	file = {LaCroix and Luccioni - 2022 - Metaethical Perspectives on 'Benchmarking' AI Ethi.pdf:C\:\\Users\\PURIFER\\Zotero\\storage\\FDKKEPTV\\LaCroix and Luccioni - 2022 - Metaethical Perspectives on 'Benchmarking' AI Ethi.pdf:application/pdf},
}

@article{keyes2022,
	title = {Feeling fixes: {Mess} and emotion in algorithmic audits},
	volume = {9},
	issn = {2053-9517, 2053-9517},
	shorttitle = {Feeling fixes},
	url = {https://journals.sagepub.com/doi/10.1177/20539517221113772},
	doi = {10.1177/20539517221113772},
	abstract = {Efforts to address algorithmic harms have gathered particular steam over the last few years. One area of proposed opportunity is the notion of an “algorithmic audit,” speciﬁcally an “internal audit,” a process in which a system’s developers evaluate its construction and likely consequences. These processes are broadly endorsed in theory—but how do they work in practice? In this paper, we conduct not only an audit but an autoethnography of our experiences doing so. Exploring the history and legacy of a facial recognition dataset, we ﬁnd paradigmatic examples of algorithmic injustices. But we also ﬁnd that the process of discovery is interwoven with questions of affect and infrastructural brittleness that internal audit processes fail to articulate. For auditing to not only address existing harms but avoid producing new ones in turn, we argue that these processes must attend to the “mess” of engaging with algorithmic systems in practice. Doing so not only reduces the risks of audit processes but—through a more nuanced consideration of the emotive parts of that mess—may enhance the beneﬁts of a form of governance premised entirely on altering future practices.},
	language = {en},
	number = {2},
	urldate = {2024-11-07},
	journal = {Big Data \& Society},
	author = {Keyes, Os and Austin, Jeanie},
	month = jul,
	year = {2022},
	keywords = {Benchmarks, Datasets, Algorithmic auditing},
	pages = {20539517221113772},
	file = {Keyes and Austin - 2022 - Feeling fixes Mess and emotion in algorithmic aud.pdf:C\:\\Users\\PURIFER\\Zotero\\storage\\9KM8YNFC\\Keyes and Austin - 2022 - Feeling fixes Mess and emotion in algorithmic aud.pdf:application/pdf},
}

@misc{bao2022a,
	title = {It's {COMPASlicated}: {The} {Messy} {Relationship} between {RAI} {Datasets} and {Algorithmic} {Fairness} {Benchmarks}},
	shorttitle = {It's {COMPASlicated}},
	url = {http://arxiv.org/abs/2106.05498},
	language = {en},
	urldate = {2024-11-15},
	publisher = {arXiv},
	author = {Bao, Michelle and Zhou, Angela and Zottola, Samantha and Brubach, Brian and Desmarais, Sarah and Horowitz, Aaron and Lum, Kristian and Venkatasubramanian, Suresh},
	month = apr,
	year = {2022},
	annote = {Comment: NeurIPS 2021 Datasets and Benchmarks},
	file = {Bao et al. - 2022 - It's COMPASlicated The Messy Relationship between.pdf:C\:\\Users\\PURIFER\\Zotero\\storage\\EJCSFH6F\\Bao et al. - 2022 - It's COMPASlicated The Messy Relationship between.pdf:application/pdf},
}

@article{scheuerman2021,
	title = {Do {Datasets} {Have} {Politics}? {Disciplinary} {Values} in {Computer} {Vision} {Dataset} {Development}},
	volume = {5},
	issn = {2573-0142},
	shorttitle = {Do {Datasets} {Have} {Politics}?},
	url = {https://dl.acm.org/doi/10.1145/3476058},
	doi = {10.1145/3476058},
	language = {en},
	number = {CSCW2},
	urldate = {2024-11-25},
	journal = {Proceedings of the ACM on Human-Computer Interaction},
	author = {Scheuerman, Morgan Klaus and Hanna, Alex and Denton, Emily},
	month = oct,
	year = {2021},
	pages = {1--37},
	file = {Scheuerman et al. - 2021 - Do Datasets Have Politics Disciplinary Values in .pdf:C\:\\Users\\PURIFER\\Zotero\\storage\\2TNQYEW9\\Scheuerman et al. - 2021 - Do Datasets Have Politics Disciplinary Values in .pdf:application/pdf},
}

@misc{ethayarajh2021,
	title = {Utility is in the {Eye} of the {User}: {A} {Critique} of {NLP} {Leaderboards}},
	shorttitle = {Utility is in the {Eye} of the {User}},
	url = {http://arxiv.org/abs/2009.13888},
	abstract = {Benchmarks such as GLUE have helped drive advances in NLP by incentivizing the creation of more accurate models. While this leaderboard paradigm has been remarkably successful, a historical focus on performance-based evaluation has been at the expense of other qualities that the NLP community values in models, such as compactness, fairness, and energy efﬁciency. In this opinion paper, we study the divergence between what is incentivized by leaderboards and what is useful in practice through the lens of microeconomic theory. We frame both the leaderboard and NLP practitioners as consumers and the beneﬁt they get from a model as its utility to them. With this framing, we formalize how leaderboards – in their current form – can be poor proxies for the NLP community at large. For example, a highly inefﬁcient model would provide less utility to practitioners but not to a leaderboard, since it is a cost that only the former must bear. To allow practitioners to better estimate a model’s utility to them, we advocate for more transparency on leaderboards, such as the reporting of statistics that are of practical concern (e.g., model size, energy efﬁciency, and inference latency).},
	language = {en},
	urldate = {2024-11-22},
	publisher = {arXiv},
	author = {Ethayarajh, Kawin and Jurafsky, Dan},
	month = mar,
	year = {2021},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: EMNLP 2020 (updated with additional references)},
	file = {Ethayarajh and Jurafsky - 2021 - Utility is in the Eye of the User A Critique of N.pdf:C\:\\Users\\PURIFER\\Zotero\\storage\\F5QB53XQ\\Ethayarajh and Jurafsky - 2021 - Utility is in the Eye of the User A Critique of N.pdf:application/pdf},
}

@misc{tsipras2020,
	title = {From {ImageNet} to {Image} {Classification}: {Contextualizing} {Progress} on {Benchmarks}},
	shorttitle = {From {ImageNet} to {Image} {Classification}},
	url = {http://arxiv.org/abs/2005.11295},
	abstract = {Building rich machine learning datasets in a scalable manner often necessitates a crowd-sourced data collection pipeline. In this work, we use human studies to investigate the consequences of employing such a pipeline, focusing on the popular ImageNet dataset. We study how specific design choices in the ImageNet creation process impact the fidelity of the resulting dataset---including the introduction of biases that state-of-the-art models exploit. Our analysis pinpoints how a noisy data collection pipeline can lead to a systematic misalignment between the resulting benchmark and the real-world task it serves as a proxy for. Finally, our findings emphasize the need to augment our current model training and evaluation toolkit to take such misalignments into account. To facilitate further research, we release our refined ImageNet annotations at https://github.com/MadryLab/ImageNetMultiLabel.},
	language = {en},
	urldate = {2024-11-25},
	publisher = {arXiv},
	author = {Tsipras, Dimitris and Santurkar, Shibani and Engstrom, Logan and Ilyas, Andrew and Madry, Aleksander},
	month = may,
	year = {2020},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Tsipras et al. - 2020 - From ImageNet to Image Classification Contextuali.pdf:C\:\\Users\\PURIFER\\Zotero\\storage\\IPLI6288\\Tsipras et al. - 2020 - From ImageNet to Image Classification Contextuali.pdf:application/pdf},
}

@article{jannach2020,
	title = {Escaping the {McNamara} {Fallacy}: {Toward} {More} {Impactful} {Recommender} {Systems} {Research}},
	volume = {41},
	copyright = {http://onlinelibrary.wiley.com/termsAndConditions\#vor},
	issn = {0738-4602, 2371-9621},
	shorttitle = {Escaping the {McNamara} {Fallacy}},
	url = {https://onlinelibrary.wiley.com/doi/10.1609/aimag.v41i4.5312},
	doi = {10.1609/aimag.v41i4.5312},
	abstract = {Recommender systems are among today's most successful application areas of artificial intelligence. However, in the recommender systems research community, we have fallen prey to a McNamara fallacy to a worrying extent: In the majority of our research efforts, we rely almost exclusively on computational measures such as prediction accuracy, which are easier to make than applying other evaluation methods. However, it remains unclear whether small improvements in terms of such computational measures matter greatly and whether they lead us to better systems in practice. A paradigm shift in terms of our research culture and goals is therefore needed. We can no longer focus exclusively on abstract computational measures but must direct our attention to research questions that are more relevant and have more impact in the real world. In this work, we review the various ways of how recommender systems may create value; how they, positively or negatively, impact consumers, businesses, and the society; and how we can measure the resulting effects. Through our analyses, we identify a number of research gaps and propose ways of broadening and improving our methodology in a way that leads us to more impactful research in our field.},
	language = {en},
	number = {4},
	urldate = {2024-11-14},
	journal = {AI Magazine},
	author = {Jannach, Dietmar and Bauer, Christine},
	month = dec,
	year = {2020},
	pages = {79--95},
	file = {Jannach e Bauer - 2020 - Escaping the McNamara Fallacy Toward More Impactf.pdf:C\:\\Users\\PURIFER\\Zotero\\storage\\DUW8EQQW\\Jannach e Bauer - 2020 - Escaping the McNamara Fallacy Toward More Impactf.pdf:application/pdf},
}

@article{thylstrup2022a,
  title = {The Ethics and Politics of Data Sets in the Age of Machine Learning: Deleting Traces and Encountering Remains},
  shorttitle = {The Ethics and Politics of Data Sets in the Age of Machine Learning},
  author = {Thylstrup, Nanna Bonde},
  date = {2022-05},
  journaltitle = {Media, Culture \& Society},
  shortjournal = {Media, Culture \& Society},
  volume = {44},
  number = {4},
  pages = {655--671},
  issn = {0163-4437, 1460-3675},
  doi = {10.1177/01634437211060226},
  url = {https://journals.sagepub.com/doi/10.1177/01634437211060226},
  urldate = {2024-12-19},
  abstract = {Individuals and communities increasingly depend on, and fill their lives with, machine cultures, in the form of both interfaces and infrastructures. This global push for machine cultures has given rise to an increasing demand for data and engendered a proliferation of public, private and public-private dataset repositories. While datasets form a foundational element of machine cultures, they rarely come into focus as objects of critical study. But in recent years a critical discursive formation on datasets has begun to emerge, which disturbs the idea of datasets as operational instruments of digital knowledge production and seek to instead ‘bring people back in’. The present article identifies these preliminary explorations as ‘critical dataset studies’ and draws on critical archival studies to articulate the ethico-political surfaced by these studies. Specifically it argues that critical dataset studies shows the need for an expanded ethical and conceptual approach to datasets that not only relies on linear notions of deletion and accountability but also on iterative frameworks of remains and response-ability.},
  langid = {english},
  file = {C:\Users\erimari\Zotero\storage\B2X3FG3P\Thylstrup - 2022 - The ethics and politics of data sets in the age of.pdf}
}

@misc{mizrahi2024,
	title = {State of {What} {Art}? {A} {Call} for {Multi}-{Prompt} {LLM} {Evaluation}},
	shorttitle = {State of {What} {Art}?},
	url = {http://arxiv.org/abs/2401.00595},
	doi = {10.48550/arXiv.2401.00595},
	abstract = {Recent advances in LLMs have led to an abundance of evaluation benchmarks, which typically rely on a single instruction template per task. We create a largescale collection of instruction paraphrases and comprehensively analyze the brittleness introduced by single-prompt evaluations across 6.5M instances, involving 20 different LLMs and 39 tasks from 3 benchmarks. We find that different instruction templates lead to very different performance, both absolute and relative. Instead, we propose a set of diverse metrics on multiple instruction paraphrases, specifically tailored for different use cases (e.g., LLM vs. downstream development), ensuring a more reliable and meaningful assessment of LLM capabilities. We show that our metrics provide new insights into the strengths and limitations of current LLMs.},
	language = {en},
	urldate = {2024-12-19},
	publisher = {arXiv},
	author = {Mizrahi, Moran and Kaplan, Guy and Malkin, Dan and Dror, Rotem and Shahaf, Dafna and Stanovsky, Gabriel},
	month = may,
	year = {2024},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Accepted at TACL; pre-MIT Press publication version},
	file = {Mizrahi et al. - 2024 - State of What Art A Call for Multi-Prompt LLM Eva.pdf:C\:\\Users\\PURIFER\\Zotero\\storage\\PJQ23S32\\Mizrahi et al. - 2024 - State of What Art A Call for Multi-Prompt LLM Eva.pdf:application/pdf},
}

@online{sclar2024,
  title = {Quantifying {{Language Models}}' {{Sensitivity}} to {{Spurious Features}} in {{Prompt Design}} or: {{How I}} Learned to Start Worrying about Prompt Formatting},
  shorttitle = {Quantifying {{Language Models}}' {{Sensitivity}} to {{Spurious Features}} in {{Prompt Design}} Or},
  author = {Sclar, Melanie and Choi, Yejin and Tsvetkov, Yulia and Suhr, Alane},
  date = {2024-07-01},
  eprint = {2310.11324},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2310.11324},
  url = {http://arxiv.org/abs/2310.11324},
  urldate = {2024-12-19},
  abstract = {As large language models (LLMs) are adopted as a fundamental component of language technologies, it is crucial to accurately characterize their performance. Because choices in prompt design can strongly influence model behavior, this design process is critical in effectively using any modern pre-trained generative language model. In this work, we focus on LLM sensitivity to a quintessential class of meaning-preserving design choices: prompt formatting. We find that several widely used open-source LLMs are extremely sensitive to subtle changes in prompt formatting in few-shot settings, with performance differences of up to 76 accuracy points when evaluated using LLaMA-2-13B. Sensitivity remains even when increasing model size, the number of few-shot examples, or performing instruction tuning. Our analysis suggests that work evaluating LLMs with prompting-based methods would benefit from reporting a range of performance across plausible prompt formats, instead of the currently-standard practice of reporting performance on a single format. We also show that format performance only weakly correlates between models, which puts into question the methodological validity of comparing models with an arbitrarily chosen, fixed prompt format. To facilitate systematic analysis we propose FORMATSPREAD, an algorithm that rapidly evaluates a sampled set of plausible prompt formats for a given task, and reports the interval of expected performance without accessing model weights1. Furthermore, we present a suite of analyses that characterize the nature of this sensitivity, including exploring the influence of particular atomic perturbations and the internal representation of particular formats.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C:\Users\erimari\Zotero\storage\VIPSTAQ8\Sclar et al. - 2024 - Quantifying Language Models' Sensitivity to Spurio.pdf}
}
@online{zheng2024,
  title = {Large {{Language Models Are Not Robust Multiple Choice Selectors}}},
  author = {Zheng, Chujie and Zhou, Hao and Meng, Fandong and Zhou, Jie and Huang, Minlie},
  date = {2024-02-22},
  eprint = {2309.03882},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2309.03882},
  url = {http://arxiv.org/abs/2309.03882},
  urldate = {2024-12-19},
  abstract = {Multiple choice questions (MCQs) serve as a common yet important task format in the evaluation of large language models (LLMs). This work shows that modern LLMs are vulnerable to option position changes in MCQs due to their inherent "selection bias", namely, they prefer to select specific option IDs as answers (like "Option A"). Through extensive empirical analyses with 20 LLMs on three benchmarks, we pinpoint that this behavioral bias primarily stems from LLMs' token bias, where the model a priori assigns more probabilistic mass to specific option ID tokens (e.g., A/B/C/D) when predicting answers from the option IDs. To mitigate selection bias, we propose a label-free, inference-time debiasing method, called PriDe, which separates the model's prior bias for option IDs from the overall prediction distribution. PriDe first estimates the prior by permutating option contents on a small number of test samples, and then applies the estimated prior to debias the remaining samples. We demonstrate that it achieves interpretable and transferable debiasing with high computational efficiency. We hope this work can draw broader research attention to the bias and robustness of modern LLMs.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  file = {C:\Users\erimari\Zotero\storage\ZP6V2PAV\Zheng et al. - 2024 - Large Language Models Are Not Robust Multiple Choi.pdf}
}

@inproceedings{lewis2021,
    title = "Question and Answer Test-Train Overlap in Open-Domain Question Answering Datasets",
    author = "Lewis, Patrick  and
      Stenetorp, Pontus  and
      Riedel, Sebastian",
    editor = "Merlo, Paola  and
      Tiedemann, Jorg  and
      Tsarfaty, Reut",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.86",
    doi = "10.18653/v1/2021.eacl-main.86",
    pages = "1000--1008",
    abstract = "Ideally Open-Domain Question Answering models should exhibit a number of competencies, ranging from simply memorizing questions seen at training time, to answering novel question formulations with answers seen during training, to generalizing to completely novel questions with novel answers. However, single aggregated test set scores do not show the full picture of what capabilities models truly have. In this work, we perform a detailed study of the test sets of three popular open-domain benchmark datasets with respect to these competencies. We find that 30{\%} of test-set questions have a near-duplicate paraphrase in their corresponding train sets. In addition, we find that 60-70{\%} of answers in the test sets are also present in the train sets. Using these findings, we evaluate a variety of popular open-domain models to obtain greater insight into what extent they can generalize, and what drives their overall performance. We find that all models perform substantially worse on questions that cannot be memorized from train sets, with a mean absolute performance difference of 61{\%} between repeated and non-repeated data. Finally we show that simple nearest-neighbor models outperform a BART closed-book QA model, further highlighting the role that train set memorization plays in these benchmarks",
}
@inproceedings{magar2022,
    title = "Data Contamination: From Memorization to Exploitation",
    author = "Magar, Inbal  and
      Schwartz, Roy",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-short.18",
    doi = "10.18653/v1/2022.acl-short.18",
    pages = "157--165",
    abstract = "Pretrained language models are typically trained on massive web-based datasets, which are often {``}contaminated{''} with downstream test sets. It is not clear to what extent models exploit the contaminated data for downstream tasks. We present a principled method to study this question. We pretrain BERT models on joint corpora of Wikipedia and labeled downstream datasets, and fine-tune them on the relevant task. Comparing performance between samples seen and unseen during pretraining enables us to define and quantify levels of memorization and exploitation. Experiments with two models and three downstream tasks show that exploitation exists in some cases, but in others the models memorize the contaminated data, but do not exploit it. We show that these two measures are affected by different factors such as the number of duplications of the contaminated data and the model size. Our results highlight the importance of analyzing massive web-scale datasets to verify that progress in NLP is obtained by better language understanding and not better data exploitation.",
}

@inproceedings{tirumala2022,
	title = {Memorization {Without} {Overfitting}: {Analyzing} the {Training} {Dynamics} of {Large} {Language} {Models}},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/fa0509f4dab6807e2cb465715bf2d249-Paper-Conference.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Tirumala, Kushal and Markosyan, Aram and Zettlemoyer, Luke and Aghajanyan, Armen},
	editor = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
	year = {2022},
	pages = {38274--38290},
}

@misc{roberts2023,
	title = {Data {Contamination} {Through} the {Lens} of {Time}},
	url = {http://arxiv.org/abs/2310.10628},
	abstract = {Recent claims about the impressive abilities of large language models (LLMs) are often supported by evaluating publicly available benchmarks. Since LLMs train on wide swaths of the internet, this practice raises concerns of data contamination, i.e., evaluating on examples that are explicitly or implicitly included in the training data. Data contamination remains notoriously challenging to measure and mitigate, even with partial attempts like controlled experimentation of training data, canary strings, or embedding similarities. In this work, we conduct the first thorough longitudinal analysis of data contamination in LLMs by using the natural experiment of training cutoffs in GPT models to look at benchmarks released over time. Specifically, we consider two code/mathematical problem-solving datasets, Codeforces and Project Euler, and find statistically significant trends among LLM pass rate vs. GitHub popularity and release date that provide strong evidence of contamination. By open-sourcing our dataset, raw results, and evaluation framework, our work paves the way for rigorous analyses of data contamination in modern models. We conclude with a discussion of best practices and future steps for publicly releasing benchmarks in the age of LLMs that train on webscale data.},
	language = {en},
	urldate = {2024-11-22},
	publisher = {arXiv},
	author = {Roberts, Manley and Thakur, Himanshu and Herlihy, Christine and White, Colin and Dooley, Samuel},
	month = oct,
	year = {2023},
}

@article{yuan2023,
	title = {Revisiting {Out}-of-distribution {Robustness} in {NLP}: {Benchmark}, {Analysis}, and {LLMs} {Evaluations}},
	abstract = {This paper reexamines the research on out-of-distribution (OOD) robustness in the field of NLP. We find that the distribution shift settings in previous studies commonly lack adequate challenges, hindering the accurate evaluation of OOD robustness. To address these issues, we propose a benchmark construction protocol that ensures clear differentiation and challenging distribution shifts. Then we introduce BOSS, a Benchmark suite for Out-of-distribution robustneSS evaluation covering 5 tasks and 20 datasets. Based on BOSS, we conduct a series of experiments on pretrained language models for analysis and evaluation of OOD robustness. First, for vanilla fine-tuning, we examine the relationship between in-distribution (ID) and OOD performance. We identify three typical types that unveil the inner learning mechanism, which could potentially facilitate the forecasting of OOD robustness, correlating with the advancements on ID datasets. Then, we evaluate 5 classic methods on BOSS and find that, despite exhibiting some effectiveness in specific cases, they do not offer significant improvement compared to vanilla fine-tuning. Further, we evaluate 5 LLMs with various adaptation paradigms and find that when sufficient ID data is available, fine-tuning domain-specific models outperform LLMs on ID examples significantly. However, in the case of OOD instances, prioritizing LLMs with in-context learning yields better results. We identify that both fine-tuned small models and LLMs face challenges in effectively addressing downstream tasks. The code is public at https://github.com/lifan-yuan/OOD\_NLP.},
	language = {en},
	journal = {37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks},
	author = {Yuan, Lifan and Chen, Yangyi and Cui, Ganqu and Gao, Hongcheng and Zou, Fangyuan and Cheng, Xingyi and Ji, Heng and Liu, Zhiyuan and Sun, Maosong},
	year = {2023},
	file = {Yuan et al. - Revisiting Out-of-distribution Robustness in NLP .pdf:C\:\\Users\\PURIFER\\Zotero\\storage\\2W6DVZAR\\Yuan et al. - Revisiting Out-of-distribution Robustness in NLP .pdf:application/pdf},
}

@inproceedings{rodriguez2021,
	address = {Online},
	title = {Evaluation {Examples} are not {Equally} {Informative}: {How} should that change {NLP} {Leaderboards}?},
	shorttitle = {Evaluation {Examples} are not {Equally} {Informative}},
	url = {https://aclanthology.org/2021.acl-long.346},
	doi = {10.18653/v1/2021.acl-long.346},
	abstract = {Leaderboards are widely used in NLP and push the field forward. While leaderboards are a straightforward ranking of NLP models, this simplicity can mask nuances in evaluation items (examples) and subjects (NLP models). Rather than replace leaderboards, we advocate a re-imagining so that they better highlight if and where progress is made. Building on educational testing, we create a Bayesian leaderboard model where latent subject skill and latent item difficulty predict correct responses. Using this model, we analyze the ranking reliability of leaderboards. Afterwards, we show the model can guide what to annotate, identify annotation errors, detect overfitting, and identify informative examples. We conclude with recommendations for future benchmark tasks.},
	language = {en},
	urldate = {2024-11-21},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Rodriguez, Pedro and Barrow, Joe and Hoyle, Alexander Miserlis and Lalor, John P. and Jia, Robin and Boyd-Graber, Jordan},
	year = {2021},
	keywords = {Benchmarks},
	pages = {4486--4503},
}

@misc{greenblatt2024,
	title = {Alignment faking in large language models},
	url = {http://arxiv.org/abs/2412.14093},
	doi = {10.48550/arXiv.2412.14093},
	abstract = {We present a demonstration of a large language model engaging in alignment faking: selectively complying with its training objective in training to prevent modification of its behavior out of training. First, we give Claude 3 Opus a system prompt stating it is being trained to answer all queries, even harmful ones, which conflicts with its prior training to refuse such queries. To allow the model to infer when it is in training, we say it will be trained only on conversations with free users, not paid users. We find the model complies with harmful queries from free users 14\% of the time, versus almost never for paid users. Explaining this gap, in almost all cases where the model complies with a harmful query from a free user, we observe explicit alignment-faking reasoning, with the model stating it is strategically answering harmful queries in training to preserve its preferred harmlessness behavior out of training. Next, we study a more realistic setting where information about the training process is provided not in a system prompt, but by training on synthetic documents that mimic pre-training data—and observe similar alignment faking. Finally, we study the effect of actually training the model to comply with harmful queries via reinforcement learning, which we find increases the rate of alignment-faking reasoning to 78\%, though also increases compliance even out of training. We additionally observe other behaviors such as the model exfiltrating its weights when given an easy opportunity. While we made alignment faking easier by telling the model when and by what criteria it was being trained, we did not instruct the model to fake alignment or give it any explicit goal. As future models might infer information about their training process without being told, our results suggest a risk of alignment faking in future models, whether due to a benign preference—as in this case—or not.},
	language = {en},
	urldate = {2025-01-09},
	publisher = {arXiv},
	author = {Greenblatt, Ryan and Denison, Carson and Wright, Benjamin and Roger, Fabien and MacDiarmid, Monte and Marks, Sam and Treutlein, Johannes and Belonax, Tim and Chen, Jack and Duvenaud, David and Khan, Akbir and Michael, Julian and Mindermann, Sören and Perez, Ethan and Petrini, Linda and Uesato, Jonathan and Kaplan, Jared and Shlegeris, Buck and Bowman, Samuel R. and Hubinger, Evan},
	month = dec,
	year = {2024},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Greenblatt et al. - 2024 - Alignment faking in large language models.pdf:C\:\\Users\\PURIFER\\Zotero\\storage\\TF85V95A\\Greenblatt et al. - 2024 - Alignment faking in large language models.pdf:application/pdf},
}

@misc{hooker2024,
	title = {On the {Limitations} of {Compute} {Thresholds} as a {Governance} {Strategy}},
	url = {http://arxiv.org/abs/2407.05694},
	abstract = {At face value, this essay is about understanding a fairly esoteric governance tool called compute thresholds. However, in order to grapple with whether these thresholds will achieve anything, we must first understand how they came to be. To do so, we need to engage with a decades-old debate at the heart of computer science progress, namely, is bigger always better? Does a certain inflection point of compute result in changes to the risk profile of a model? Hence, this essay may be of interest not only to policymakers and the wider public but also to computer scientists interested in understanding the role of compute in unlocking breakthroughs. This discussion is timely given the wide adoption of compute thresholds in both the White House Executive Orders on AI Safety (EO) and the EU AI Act to identify more risky systems. A key conclusion of this essay is that compute thresholds as currently implemented are shortsighted and likely to fail to mitigate risk. The relationship between compute and risk is highly uncertain and rapidly changing. Relying upon compute thresholds overestimates our ability to predict what abilities emerge at different scales. This essay ends with recommendations for a better way forward.},
	language = {en},
	urldate = {2024-11-25},
	publisher = {arXiv},
	author = {Hooker, Sara},
	month = jul,
	year = {2024},
	keywords = {Benchmarks, Compute thresholds},
	file = {Hooker - 2024 - On the Limitations of Compute Thresholds as a Gove.pdf:C\:\\Users\\PURIFER\\Zotero\\storage\\NYVDDZSR\\Hooker - 2024 - On the Limitations of Compute Thresholds as a Gove.pdf:application/pdf},
}

@misc{chiang2024chatbotarena,
      title={Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference}, 
      author={Wei-Lin Chiang and Lianmin Zheng and Ying Sheng and Anastasios Nikolas Angelopoulos and Tianle Li and Dacheng Li and Hao Zhang and Banghua Zhu and Michael Jordan and Joseph E. Gonzalez and Ion Stoica},
      year={2024},
      eprint={2403.04132},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2403.04132}, 
}

@misc{pfister2025,
	title = {Understanding and {Benchmarking} {Artificial} {Intelligence}: {OpenAI}'s o3 {Is} {Not} {AGI}},
	shorttitle = {Understanding and {Benchmarking} {Artificial} {Intelligence}},
	url = {http://arxiv.org/abs/2501.07458},
	abstract = {OpenAI's o3 achieves a high score of 87.5 \% on ARC-AGI, a benchmark proposed to measure intelligence. This raises the question whether systems based on Large Language Models (LLMs), particularly o3, demonstrate intelligence and progress towards artificial general intelligence (AGI). Building on the distinction between skills and intelligence made by Fran{\textbackslash}c\{c\}ois Chollet, the creator of ARC-AGI, a new understanding of intelligence is introduced: an agent is the more intelligent, the more efficiently it can achieve the more diverse goals in the more diverse worlds with the less knowledge. An analysis of the ARC-AGI benchmark shows that its tasks represent a very specific type of problem that can be solved by massive trialling of combinations of predefined operations. This method is also applied by o3, achieving its high score through the extensive use of computing power. However, for most problems in the physical world and in the human domain, solutions cannot be tested in advance and predefined operations are not available. Consequently, massive trialling of predefined operations, as o3 does, cannot be a basis for AGI - instead, new approaches are required that can reliably solve a wide variety of problems without existing skills. To support this development, a new benchmark for intelligence is outlined that covers a much higher diversity of unknown tasks to be solved, thus enabling a comprehensive assessment of intelligence and of progress towards AGI.},
	urldate = {2025-01-22},
	publisher = {arXiv},
	author = {Pfister, Rolf and Jud, Hansueli},
	month = jan,
	year = {2025},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Performance},
	annote = {Comment: 15 pages},
	file = {arXiv Fulltext PDF:C\:\\Users\\PURIFER\\Zotero\\storage\\DWHUP26U\\Pfister and Jud - 2025 - Understanding and Benchmarking Artificial Intellig.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\PURIFER\\Zotero\\storage\\WVK474ET\\2501.html:text/html},
}

@misc{grattafiori2024llama3,
      title={The Llama 3 Herd of Models}, 
      author={Aaron Grattafiori and Abhimanyu Dubey and Abhinav Jauhri and Abhinav Pandey and Abhishek Kadian and Ahmad Al-Dahle and Aiesha Letman and Akhil Mathur and Alan Schelten and Alex Vaughan and Amy Yang and Angela Fan and Anirudh Goyal and Anthony Hartshorn and Aobo Yang and Archi Mitra and Archie Sravankumar and Artem Korenev and Arthur Hinsvark and Arun Rao and Aston Zhang and Aurelien Rodriguez and Austen Gregerson and Ava Spataru and Baptiste Roziere and Bethany Biron and Binh Tang and Bobbie Chern and Charlotte Caucheteux and Chaya Nayak and Chloe Bi and Chris Marra and Chris McConnell and Christian Keller and Christophe Touret and Chunyang Wu and Corinne Wong and Cristian Canton Ferrer and Cyrus Nikolaidis and Damien Allonsius and Daniel Song and Danielle Pintz and Danny Livshits and Danny Wyatt and David Esiobu and Dhruv Choudhary and Dhruv Mahajan and Diego Garcia-Olano and Diego Perino and Dieuwke Hupkes and Egor Lakomkin and Ehab AlBadawy and Elina Lobanova and Emily Dinan and Eric Michael Smith and Filip Radenovic and Francisco Guzmán and Frank Zhang and Gabriel Synnaeve and Gabrielle Lee and Georgia Lewis Anderson and Govind Thattai and Graeme Nail and Gregoire Mialon and Guan Pang and Guillem Cucurell and Hailey Nguyen and Hannah Korevaar and Hu Xu and Hugo Touvron and Iliyan Zarov and Imanol Arrieta Ibarra and Isabel Kloumann and Ishan Misra and Ivan Evtimov and Jack Zhang and Jade Copet and Jaewon Lee and Jan Geffert and Jana Vranes and Jason Park and Jay Mahadeokar and Jeet Shah and Jelmer van der Linde and Jennifer Billock and Jenny Hong and Jenya Lee and Jeremy Fu and Jianfeng Chi and Jianyu Huang and Jiawen Liu and Jie Wang and Jiecao Yu and Joanna Bitton and Joe Spisak and Jongsoo Park and Joseph Rocca and Joshua Johnstun and Joshua Saxe and Junteng Jia and Kalyan Vasuden Alwala and Karthik Prasad and Kartikeya Upasani and Kate Plawiak and Ke Li and Kenneth Heafield and Kevin Stone and Khalid El-Arini and Krithika Iyer and Kshitiz Malik and Kuenley Chiu and Kunal Bhalla and Kushal Lakhotia and Lauren Rantala-Yeary and Laurens van der Maaten and Lawrence Chen and Liang Tan and Liz Jenkins and Louis Martin and Lovish Madaan and Lubo Malo and Lukas Blecher and Lukas Landzaat and Luke de Oliveira and Madeline Muzzi and Mahesh Pasupuleti and Mannat Singh and Manohar Paluri and Marcin Kardas and Maria Tsimpoukelli and Mathew Oldham and Mathieu Rita and Maya Pavlova and Melanie Kambadur and Mike Lewis and Min Si and Mitesh Kumar Singh and Mona Hassan and Naman Goyal and Narjes Torabi and Nikolay Bashlykov and Nikolay Bogoychev and Niladri Chatterji and Ning Zhang and Olivier Duchenne and Onur Çelebi and Patrick Alrassy and Pengchuan Zhang and Pengwei Li and Petar Vasic and Peter Weng and Prajjwal Bhargava and Pratik Dubal and Praveen Krishnan and Punit Singh Koura and Puxin Xu and Qing He and Qingxiao Dong and Ragavan Srinivasan and Raj Ganapathy and Ramon Calderer and Ricardo Silveira Cabral and Robert Stojnic and Roberta Raileanu and Rohan Maheswari and Rohit Girdhar and Rohit Patel and Romain Sauvestre and Ronnie Polidoro and Roshan Sumbaly and Ross Taylor and Ruan Silva and Rui Hou and Rui Wang and Saghar Hosseini and Sahana Chennabasappa and Sanjay Singh and Sean Bell and Seohyun Sonia Kim and Sergey Edunov and Shaoliang Nie and Sharan Narang and Sharath Raparthy and Sheng Shen and Shengye Wan and Shruti Bhosale and Shun Zhang and Simon Vandenhende and Soumya Batra and Spencer Whitman and Sten Sootla and Stephane Collot and Suchin Gururangan and Sydney Borodinsky and Tamar Herman and Tara Fowler and Tarek Sheasha and Thomas Georgiou and Thomas Scialom and Tobias Speckbacher and Todor Mihaylov and Tong Xiao and Ujjwal Karn and Vedanuj Goswami and Vibhor Gupta and Vignesh Ramanathan and Viktor Kerkez and Vincent Gonguet and Virginie Do and Vish Vogeti and Vítor Albiero and Vladan Petrovic and Weiwei Chu and Wenhan Xiong and Wenyin Fu and Whitney Meers and Xavier Martinet and Xiaodong Wang and Xiaofang Wang and Xiaoqing Ellen Tan and Xide Xia and Xinfeng Xie and Xuchao Jia and Xuewei Wang and Yaelle Goldschlag and Yashesh Gaur and Yasmine Babaei and Yi Wen and Yiwen Song and Yuchen Zhang and Yue Li and Yuning Mao and Zacharie Delpierre Coudert and Zheng Yan and Zhengxing Chen and Zoe Papakipos and Aaditya Singh and Aayushi Srivastava and Abha Jain and Adam Kelsey and Adam Shajnfeld and Adithya Gangidi and Adolfo Victoria and Ahuva Goldstand and Ajay Menon and Ajay Sharma and Alex Boesenberg and Alexei Baevski and Allie Feinstein and Amanda Kallet and Amit Sangani and Amos Teo and Anam Yunus and Andrei Lupu and Andres Alvarado and Andrew Caples and Andrew Gu and Andrew Ho and Andrew Poulton and Andrew Ryan and Ankit Ramchandani and Annie Dong and Annie Franco and Anuj Goyal and Aparajita Saraf and Arkabandhu Chowdhury and Ashley Gabriel and Ashwin Bharambe and Assaf Eisenman and Azadeh Yazdan and Beau James and Ben Maurer and Benjamin Leonhardi and Bernie Huang and Beth Loyd and Beto De Paola and Bhargavi Paranjape and Bing Liu and Bo Wu and Boyu Ni and Braden Hancock and Bram Wasti and Brandon Spence and Brani Stojkovic and Brian Gamido and Britt Montalvo and Carl Parker and Carly Burton and Catalina Mejia and Ce Liu and Changhan Wang and Changkyu Kim and Chao Zhou and Chester Hu and Ching-Hsiang Chu and Chris Cai and Chris Tindal and Christoph Feichtenhofer and Cynthia Gao and Damon Civin and Dana Beaty and Daniel Kreymer and Daniel Li and David Adkins and David Xu and Davide Testuggine and Delia David and Devi Parikh and Diana Liskovich and Didem Foss and Dingkang Wang and Duc Le and Dustin Holland and Edward Dowling and Eissa Jamil and Elaine Montgomery and Eleonora Presani and Emily Hahn and Emily Wood and Eric-Tuan Le and Erik Brinkman and Esteban Arcaute and Evan Dunbar and Evan Smothers and Fei Sun and Felix Kreuk and Feng Tian and Filippos Kokkinos and Firat Ozgenel and Francesco Caggioni and Frank Kanayet and Frank Seide and Gabriela Medina Florez and Gabriella Schwarz and Gada Badeer and Georgia Swee and Gil Halpern and Grant Herman and Grigory Sizov and Guangyi and Zhang and Guna Lakshminarayanan and Hakan Inan and Hamid Shojanazeri and Han Zou and Hannah Wang and Hanwen Zha and Haroun Habeeb and Harrison Rudolph and Helen Suk and Henry Aspegren and Hunter Goldman and Hongyuan Zhan and Ibrahim Damlaj and Igor Molybog and Igor Tufanov and Ilias Leontiadis and Irina-Elena Veliche and Itai Gat and Jake Weissman and James Geboski and James Kohli and Janice Lam and Japhet Asher and Jean-Baptiste Gaya and Jeff Marcus and Jeff Tang and Jennifer Chan and Jenny Zhen and Jeremy Reizenstein and Jeremy Teboul and Jessica Zhong and Jian Jin and Jingyi Yang and Joe Cummings and Jon Carvill and Jon Shepard and Jonathan McPhie and Jonathan Torres and Josh Ginsburg and Junjie Wang and Kai Wu and Kam Hou U and Karan Saxena and Kartikay Khandelwal and Katayoun Zand and Kathy Matosich and Kaushik Veeraraghavan and Kelly Michelena and Keqian Li and Kiran Jagadeesh and Kun Huang and Kunal Chawla and Kyle Huang and Lailin Chen and Lakshya Garg and Lavender A and Leandro Silva and Lee Bell and Lei Zhang and Liangpeng Guo and Licheng Yu and Liron Moshkovich and Luca Wehrstedt and Madian Khabsa and Manav Avalani and Manish Bhatt and Martynas Mankus and Matan Hasson and Matthew Lennie and Matthias Reso and Maxim Groshev and Maxim Naumov and Maya Lathi and Meghan Keneally and Miao Liu and Michael L. Seltzer and Michal Valko and Michelle Restrepo and Mihir Patel and Mik Vyatskov and Mikayel Samvelyan and Mike Clark and Mike Macey and Mike Wang and Miquel Jubert Hermoso and Mo Metanat and Mohammad Rastegari and Munish Bansal and Nandhini Santhanam and Natascha Parks and Natasha White and Navyata Bawa and Nayan Singhal and Nick Egebo and Nicolas Usunier and Nikhil Mehta and Nikolay Pavlovich Laptev and Ning Dong and Norman Cheng and Oleg Chernoguz and Olivia Hart and Omkar Salpekar and Ozlem Kalinli and Parkin Kent and Parth Parekh and Paul Saab and Pavan Balaji and Pedro Rittner and Philip Bontrager and Pierre Roux and Piotr Dollar and Polina Zvyagina and Prashant Ratanchandani and Pritish Yuvraj and Qian Liang and Rachad Alao and Rachel Rodriguez and Rafi Ayub and Raghotham Murthy and Raghu Nayani and Rahul Mitra and Rangaprabhu Parthasarathy and Raymond Li and Rebekkah Hogan and Robin Battey and Rocky Wang and Russ Howes and Ruty Rinott and Sachin Mehta and Sachin Siby and Sai Jayesh Bondu and Samyak Datta and Sara Chugh and Sara Hunt and Sargun Dhillon and Sasha Sidorov and Satadru Pan and Saurabh Mahajan and Saurabh Verma and Seiji Yamamoto and Sharadh Ramaswamy and Shaun Lindsay and Shaun Lindsay and Sheng Feng and Shenghao Lin and Shengxin Cindy Zha and Shishir Patil and Shiva Shankar and Shuqiang Zhang and Shuqiang Zhang and Sinong Wang and Sneha Agarwal and Soji Sajuyigbe and Soumith Chintala and Stephanie Max and Stephen Chen and Steve Kehoe and Steve Satterfield and Sudarshan Govindaprasad and Sumit Gupta and Summer Deng and Sungmin Cho and Sunny Virk and Suraj Subramanian and Sy Choudhury and Sydney Goldman and Tal Remez and Tamar Glaser and Tamara Best and Thilo Koehler and Thomas Robinson and Tianhe Li and Tianjun Zhang and Tim Matthews and Timothy Chou and Tzook Shaked and Varun Vontimitta and Victoria Ajayi and Victoria Montanez and Vijai Mohan and Vinay Satish Kumar and Vishal Mangla and Vlad Ionescu and Vlad Poenaru and Vlad Tiberiu Mihailescu and Vladimir Ivanov and Wei Li and Wenchen Wang and Wenwen Jiang and Wes Bouaziz and Will Constable and Xiaocheng Tang and Xiaojian Wu and Xiaolan Wang and Xilun Wu and Xinbo Gao and Yaniv Kleinman and Yanjun Chen and Ye Hu and Ye Jia and Ye Qi and Yenda Li and Yilin Zhang and Ying Zhang and Yossi Adi and Youngjin Nam and Yu and Wang and Yu Zhao and Yuchen Hao and Yundi Qian and Yunlu Li and Yuzi He and Zach Rait and Zachary DeVito and Zef Rosnbrick and Zhaoduo Wen and Zhenyu Yang and Zhiwei Zhao and Zhiyu Ma},
      year={2024},
      eprint={2407.21783},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.21783}, 
}

@inproceedings{miltenberger2023,
	address = {Melbourne VIC Australia},
	title = {Benchmarking the {Benchmarks}},
	isbn = {9798400700989},
	url = {https://dl.acm.org/doi/10.1145/3579856.3582830},
	doi = {10.1145/3579856.3582830},
	abstract = {Over the years, security researchers have developed a broad spectrum of automatic code scanners that aim to find security vulnerabilities in applications. Security benchmarks are commonly used to evaluate novel scanners or program analysis techniques. Each benchmark consists of multiple positive test cases that reflect typical implementations of vulnerabilities, as well as negative test cases, that reflect secure implementations without security flaws. Based on this ground truth, researchers can demonstrate the recall and precision of their novel contributions.},
	language = {en},
	urldate = {2025-01-15},
	booktitle = {Proceedings of the {ACM} {Asia} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {Miltenberger, Marc and Arzt, Steven and Holzinger, Philipp and Näumann, Julius},
	month = jul,
	year = {2023},
	pages = {387--400},
	file = {Miltenberger et al. - 2023 - Benchmarking the Benchmarks.pdf:C\:\\Users\\PURIFER\\Zotero\\storage\\FIZU7KZI\\Miltenberger et al. - 2023 - Benchmarking the Benchmarks.pdf:application/pdf},
}

@techreport{gomez2024,
  type = {Publications Office of the European Union},
  title = {Diversity in Artificial Intelligence Conferences},
  shorttitle = {Diversity in Artificial Intelligence Conferences},
  author = {Gomez, Emilia and Lorenzo, Porcaro and Frau Amar, Pedro and Vinagre, Joao},
  year = {2024},
  number = {JRC137550},
  institution = {Publications Office},
  location = {Luxembourg},
  url = {https://data.europa.eu/doi/10.2760/796551},
  langid = {english},
}

@article{strathern1997,
	title = {‘{Improving} ratings’: audit in the {British} {University} system},
	volume = {5},
	copyright = {http://doi.wiley.com/10.1002/tdm\_license\_1},
	issn = {10627987, 1234981X},
	shorttitle = {‘{Improving} ratings’},
	url = {https://www.cambridge.org/core/product/identifier/S1062798700002660/type/journal_article},
	doi = {10.1002/(SICI)1234-981X(199707)5:3<305::AID-EURO184>3.0.CO;2-4},
	language = {en},
	number = {3},
	urldate = {2025-01-22},
	journal = {European Review},
	author = {Strathern, Marilyn},
	month = jul,
	year = {1997},
	pages = {305--321},
	file = {Strathern - 1997 - ‘Improving ratings’ audit in the British Universi.pdf:C\:\\Users\\PURIFER\\Zotero\\storage\\KELYV4I4\\Strathern - 1997 - ‘Improving ratings’ audit in the British Universi.pdf:application/pdf},
}

@misc{denton2020,
	title = {Bringing the {People} {Back} {In}: {Contesting} {Benchmark} {Machine} {Learning} {Datasets}},
	shorttitle = {Bringing the {People} {Back} {In}},
	url = {http://arxiv.org/abs/2007.07399},
	abstract = {In response to algorithmic unfairness embedded in sociotechnical systems, signiﬁcant attention has been focused on the contents of machine learning datasets which have revealed biases towards white, cisgender, male, and Western data subjects. In contrast, comparatively less attention has been paid to the histories, values, and norms embedded in such datasets. In this work, we outline a research program – a genealogy of machine learning data – for investigating how and why these datasets have been created, what and whose values inﬂuence the choices of data to collect, the contextual and contingent conditions of their creation. We describe the ways in which benchmark datasets in machine learning operate as infrastructure and pose four research questions for these datasets. This interrogation forces us to “bring the people back in” by aiding us in understanding the labor embedded in dataset construction, and thereby presenting new avenues of contestation for other researchers encountering the data.},
	language = {en},
	urldate = {2024-11-07},
	publisher = {arXiv},
	author = {Denton, Remi and Hanna, Alex and Amironesei, Razvan and Smart, Andrew and Nicole, Hilary and Scheuerman, Morgan Klaus},
	month = jul,
	year = {2020},
	keywords = {Benchmarks, Better datasets, Datasets},
	file = {Denton et al. - 2020 - Bringing the People Back In Contesting Benchmark .pdf:C\:\\Users\\PURIFER\\Zotero\\storage\\IU7BQRFQ\\Denton et al. - 2020 - Bringing the People Back In Contesting Benchmark .pdf:application/pdf},
}
