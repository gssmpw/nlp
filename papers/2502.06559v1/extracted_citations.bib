@article{church_survey_2019,
    title = {A survey of 25 years of evaluation},
    volume = {25},
    copyright = {http://creativecommons.org/licenses/by/4.0/},
    issn = {1351-3249, 1469-8110},
    url = {https://www.cambridge.org/core/product/identifier/S1351324919000275/type/journal_article},
    doi = {10.1017/S1351324919000275},
    language = {en},
    number = {06},
    urldate = {2024-12-05},
    journal = {Natural Language Engineering},
    author = {Church, Kenneth Ward and Hestness, Joel},
    month = nov,
    year = {2019},
    pages = {753--767},
}

@article{gehrmann2023,
	title = {Repairing the {Cracked} {Foundation}: {A} {Survey} of {Obstacles} in {Evaluation} {Practices} for {Generated} {Text}},
	volume = {77},
	issn = {1076-9757},
	shorttitle = {Repairing the {Cracked} {Foundation}},
	url = {https://www.jair.org/index.php/jair/article/view/13715},
	doi = {10.1613/jair.1.13715},
	abstract = {Evaluation practices in natural language generation (NLG) have many known ﬂaws, but improved evaluation approaches are rarely widely adopted. This issue has become more urgent, since neural generation models have improved to the point where their outputs can often no longer be distinguished based on the surface-level features that older metrics rely on. This paper surveys the issues with human and automatic model evaluations and with commonly used datasets in NLG that have been pointed out over the past 20 years. We summarize, categorize, and discuss how researchers have been addressing these issues and what their ﬁndings mean for the current state of model evaluations. Building on those insights, we lay out a long-term vision for evaluation research and propose concrete steps for researchers to improve their evaluation processes. Finally, we analyze 66 generation papers from recent NLP conferences in how well they already follow these suggestions and identify which areas require more drastic changes to the status quo.},
	language = {en},
	urldate = {2024-11-18},
	journal = {Journal of Artificial Intelligence Research},
	author = {Gehrmann, Sebastian and Clark, Elizabeth and Sellam, Thibault},
	month = may,
	year = {2023},
	pages = {103--166},
	file = {Gehrmann et al. - 2023 - Repairing the Cracked Foundation A Survey of Obst.pdf:C\:\\Users\\PURIFER\\Zotero\\storage\\695A8J7Z\\Gehrmann et al. - 2023 - Repairing the Cracked Foundation A Survey of Obst.pdf:application/pdf},
}

@inproceedings{hutchinson_evaluation_2022,
    address = {Seoul Republic of Korea},
    title = {Evaluation {Gaps} in {Machine} {Learning} {Practice}},
    isbn = {978-1-4503-9352-2},
    url = {https://dl.acm.org/doi/10.1145/3531146.3533233},
    doi = {10.1145/3531146.3533233},
    booktitle = {2022 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
    publisher = {ACM},
    author = {Hutchinson, Ben and Rostamzadeh, Negar and Greer, Christina and Heller, Katherine and Prabhakaran, Vinodkumar},
    month = jun,
    year = {2022},
    pages = {1859--1876},
}

@inproceedings{liao2021,
    title={Are We Learning Yet? A Meta Review of Evaluation Failures Across Machine Learning},
    author={Thomas Liao and Rohan Taori and Inioluwa Deborah Raji and Ludwig Schmidt},
    booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)},
    year={2021},
    url={https://openreview.net/forum?id=mPducS1MsEK}
}

@inproceedings{park2022,
	address = {Dublin, Ireland},
	title = {Raison d’être of the benchmark dataset: {A} {Survey} of {Current} {Practices} of {Benchmark} {Dataset} {Sharing} {Platforms}},
	shorttitle = {Raison d’être of the benchmark dataset},
	url = {https://aclanthology.org/2022.nlppower-1.1},
	doi = {10.18653/v1/2022.nlppower-1.1},
	abstract = {This paper critically examines the current practices of benchmark dataset sharing in NLP and suggests a better way to inform reusers of the benchmark dataset. As the dataset sharing platform plays a key role not only in distributing the dataset but also in informing the potential reusers about the dataset, we believe data sharing platforms should provide a comprehensive context of the datasets. We survey four benchmark dataset sharing platforms: HuggingFace, PaperswithCode, Tensorflow, and Pytorch to diagnose the current practices of how the dataset is shared - which metadata is shared and omitted. To be specific, drawing on the concept of data curation which considers the future reuse when the data is made public, we advance the direction that benchmark dataset sharing platforms should take into consideration. We identify that four benchmark platforms have different practices of using metadata and there is a lack of consensus on what social impact metadata is. We believe the problem of missing a discussion around social impact in the dataset sharing platforms has to do with the failed agreement on who should be in charge. We propose that the benchmark dataset should develop social impact metadata and data curator should take a role in managing the social impact metadata.},
	language = {en},
	urldate = {2024-11-21},
	booktitle = {Proceedings of {NLP} {Power}! {The} {First} {Workshop} on {Efficient} {Benchmarking} in {NLP}},
	publisher = {Association for Computational Linguistics},
	author = {Park, Jaihyun and Jeoung, Sullam},
	year = {2022},
	keywords = {Benchmarks},
	pages = {1--10},
	file = {Park and Jeoung - 2022 - Raison d’être of the benchmark dataset A Survey o.pdf:C\:\\Users\\PURIFER\\Zotero\\storage\\PJE96AZF\\Park and Jeoung - 2022 - Raison d’être of the benchmark dataset A Survey o.pdf:application/pdf},
}

@article{paullada2021,
	title = {Data and its (dis)contents: {A} survey of dataset development and use in machine learning research},
	volume = {2},
	issn = {26663899},
	shorttitle = {Data and its (dis)contents},
	url = {http://arxiv.org/abs/2012.05345},
	doi = {10.1016/j.patter.2021.100336},
	abstract = {Datasets have played a foundational role in the advancement of machine learning research. They form the basis for the models we design and deploy, as well as our primary medium for benchmarking and evaluation. Furthermore, the ways in which we collect, construct and share these datasets inform the kinds of problems the ﬁeld pursues and the methods explored in algorithm development. However, recent work from a breadth of perspectives has revealed the limitations of predominant practices in dataset collection and use. In this paper, we survey the many concerns raised about the way we collect and use data in machine learning and advocate that a more cautious and thorough understanding of data is necessary to address several of the practical and ethical issues of the ﬁeld.},
	language = {en},
	number = {11},
	urldate = {2024-11-22},
	journal = {Patterns},
	author = {Paullada, Amandalynne and Raji, Inioluwa Deborah and Bender, Emily M. and Denton, Emily and Hanna, Alex},
	month = nov,
	year = {2021},
	keywords = {Computer Science - Machine Learning},
	pages = {100336},
	file = {Paullada et al. - 2021 - Data and its (dis)contents A survey of dataset de.pdf:C\:\\Users\\PURIFER\\Zotero\\storage\\62RGGUIP\\Paullada et al. - 2021 - Data and its (dis)contents A survey of dataset de.pdf:application/pdf},
}

@inproceedings{ren2024,
    title={Safetywashing: Do {AI} Safety Benchmarks Actually Measure Safety Progress?},
    author={Richard Ren and Steven Basart and Adam Khoja and Alice Gatti and Long Phan and Xuwang Yin and Mantas Mazeika and Alexander Pan and Gabriel Mukobi and Ryan Hwang Kim and Stephen Fitz and Dan Hendrycks},
    booktitle={The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
    year={2024},
    url={https://openreview.net/forum?id=YagfTP3RK6}
}

@misc{rottger2024,
	type = {Language models},
	title = {{SafetyPrompts}: a {Systematic} {Review} of {Open} {Datasets} for {Evaluating} and {Improving} {Large} {Language} {Model} {Safety}},
	shorttitle = {{SafetyPrompts}},
	url = {http://arxiv.org/abs/2404.05399},
	abstract = {The last two years have seen a rapid growth in concerns around the safety of large language models (LLMs). Researchers and practitioners have met these concerns by introducing an abundance of new datasets for evaluating and improving LLM safety. However, much of this work has happened in parallel, and with very different goals in mind, ranging from the mitigation of near-term risks around bias and toxic content generation to the assessment of longer-term catastrophic risk potential. This makes it difficult for researchers and practitioners to find the most relevant datasets for a given use case, and to identify gaps in dataset coverage that future work may fill. To remedy these issues, we conduct a first systematic review of open datasets for evaluating and improving LLM safety. We review 102 datasets, which we identified through an iterative and community-driven process over the course of several months. We highlight patterns and trends, such as a a trend towards fully synthetic datasets, as well as gaps in dataset coverage, such as a clear lack of non-English datasets. We also examine how LLM safety datasets are used in practice – in LLM release publications and popular LLM benchmarks – finding that current evaluation practices are highly idiosyncratic and make use of only a small fraction of available datasets. Our contributions are based on SafetyPrompts.com, a living catalogue of open datasets for LLM safety, which we commit to updating continuously as the field of LLM safety develops.},
	language = {en},
	urldate = {2024-11-14},
	publisher = {arXiv},
	author = {Röttger, Paul and Pernisi, Fabio and Vidgen, Bertie and Hovy, Dirk},
	month = apr,
	year = {2024},
	keywords = {Benchmarks},
	file = {Röttger et al. - 2024 - SafetyPrompts a Systematic Review of Open Dataset.pdf:C\:\\Users\\PURIFER\\Zotero\\storage\\9XWXD5CR\\Röttger et al. - 2024 - SafetyPrompts a Systematic Review of Open Dataset.pdf:application/pdf},
}

@misc{weij2024,
	type = {Language models},
	title = {{AI} {Sandbagging}: {Language} {Models} can {Strategically} {Underperform} on {Evaluations}},
	shorttitle = {{AI} {Sandbagging}},
	url = {http://arxiv.org/abs/2406.07358},
	abstract = {Trustworthy capability evaluations are crucial for ensuring the safety of AI systems, and are becoming a key component of AI regulation. However, the developers of an AI system, or the AI system itself, may have incentives for evaluations to understate the AI's actual capability. These conflicting interests lead to the problem of sandbagging \${\textbackslash}unicode\{x2013\}\$ which we define as "strategic underperformance on an evaluation". In this paper we assess sandbagging capabilities in contemporary language models (LMs). We prompt frontier LMs, like GPT-4 and Claude 3 Opus, to selectively underperform on dangerous capability evaluations, while maintaining performance on general (harmless) capability evaluations. Moreover, we find that models can be fine-tuned, on a synthetic dataset, to hide specific capabilities unless given a password. This behaviour generalizes to high-quality, held-out benchmarks such as WMDP. In addition, we show that both frontier and smaller models can be prompted, or password-locked, to target specific scores on a capability evaluation. Even more, we found that a capable password-locked model (Llama 3 70b) is reasonably able to emulate a less capable model (Llama 2 7b). Overall, our results suggest that capability evaluations are vulnerable to sandbagging. This vulnerability decreases the trustworthiness of evaluations, and thereby undermines important safety decisions regarding the development and deployment of advanced AI systems.},
	language = {en},
	urldate = {2024-11-15},
	publisher = {arXiv},
	author = {Weij, Teun van der and Hofstätter, Felix and Jaffe, Ollie and Brown, Samuel F. and Ward, Francis Rhys},
	month = jun,
	year = {2024},
	keywords = {Benchmarks},
	file = {Weij et al. - 2024 - AI Sandbagging Language Models can Strategically .pdf:C\:\\Users\\PURIFER\\Zotero\\storage\\TXQPBX7Z\\Weij et al. - 2024 - AI Sandbagging Language Models can Strategically .pdf:application/pdf},
}

@misc{xu2024,
	title = {Benchmark {Data} {Contamination} of {Large} {Language} {Models}: {A} {Survey}},
	shorttitle = {Benchmark {Data} {Contamination} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2406.04244},
	abstract = {The rapid development of Large Language Models (LLMs) like GPT-4, Claude-3, and Gemini has transformed the field of natural language processing. However, it has also resulted in a significant issue known as Benchmark Data Contamination (BDC). This occurs when language models inadvertently incorporate evaluation benchmark information from their training data, leading to inaccurate or unreliable performance during the evaluation phase of the process. This paper reviews the complex challenge of BDC in LLM evaluation and explores alternative assessment methods to mitigate the risks associated with traditional benchmarks. The paper also examines challenges and future directions in mitigating BDC risks, highlighting the complexity of the issue and the need for innovative solutions to ensure the reliability of LLM evaluation in real-world applications.},
	urldate = {2025-01-22},
	publisher = {arXiv},
	author = {Xu, Cheng and Guan, Shuhao and Greene, Derek and Kechadi, M.-Tahar},
	month = jun,
	year = {2024},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 31 pages, 7 figures, 3 tables},
	file = {arXiv Fulltext PDF:C\:\\Users\\PURIFER\\Zotero\\storage\\HA2MBT5B\\Xu et al. - 2024 - Benchmark Data Contamination of Large Language Mod.pdf:application/pdf},
}

