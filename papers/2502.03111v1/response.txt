\section{Related Work}
%\fs{Maybe we can shorten by not talking so much about abstractive vs extractive?}

Meeting summarization has been studied for decades, with most pre-neural network approaches focusing on extractive summarization **Rus, "Extracting Summaries from Spoken Documents"**. While extractive summaries can be perfectly adequate for some domains like news **Mani, "Automatic Summarization"**, humans tend to prefer abstractive summaries **Nallapati et al., "Deep-Structured Semantic Models for Document Vectors"** and most current research is focused on generative (neural) abstractive summarization models **See et al., "Getting Started with Content-Based Deep Learning"**.

Research into abstractive meeting summarization has largely focused on neural methods, with work largely progressing in two complementary directions, both attempting to cope with the challenge of long inputs: 1) extending the transformer **Vaswani et al., "Attention Is All You Need"** architecture to scale to longer inputs, usually using a sparse attention paradigm **Liu and Lapata, "Text Summarization Using Long Short-Term Memory"** or a hierarchical representation approach **Chen et al., "Hierarchical Neural Story Generation"**. And 2) divide-and-conquer approaches that perform a segmentation on the source and use conventional summarization systems on the segments, sometimes with an additional refinement step on the segment summaries **Gupta et al., "Summarizing Text Documents Based on User Feedback"**. The best summaries are currently obtained by Large Language Models (LLMs) producing abstractive summaries **Brown et al., "Language Models as Knowledge Bases"**. %\mt{[This last sentence is a bit ambiguous because it is not clear if you refer to the best extractive/abstractive/direction1/direction2 approach.]}

Online summarization has largely been studied in the context of stream summarization and video summarization **Vasudevan et al., "Stream Summarization for Online News"**. In stream summarization, it is assumed that a large volume of content (usually tweets or news articles) is being generated all the time and the task is to summarize available content or sentiment on a given topic from this stream at a given time.
%The concept of latency is not usually discussed in this context\mt{why? Answering can help to highlight the difference with our scenario}.
The primary concern of these systems is to produce a summary of past content on demand, not to respond to new content in a timely fashion. As a result, the concept of latency is not usually discussed in this context.
Typical approaches include performing document retrieval as a summary **Kumar et al., "Document Retrieval for Online Summarization"** and constructing a language model from the content stream and generating a representative example as a summary **Zhang et al., "Online Language Model for Text Summarization"**. Both require orders of magnitude more content than is available in a typical meeting and cannot be applied to meeting summaries.

Video summarization is the task of selecting representative frames or sequences from a video to form a summary **Gong et al., "Video Summarization with Attention-Based Deep Learning"**. While online systems exist **Khosrowpour et al., "Online Video Summarization for Event Detection"**, they are very specific to video and exclusively produce extractive summaries. Moreover, there exists, to our knowledge, no study of latency (how soon after the occurrence is a summary available) in this task.

%\mt{I guess it is missing a sentence that wraps up the fact that neither the approaches nor the evaluation protocols mentioned in these papers can be added to our scenario.}

In summary, while there are systems that perform summarization in an online fashion, their approaches and evaluation schemes are not suitable for our task.

%fs{I don't think we need to justify our use of ROUGE or our human eval criteria, I'm dropping this part.}

% An important aspect of the research on summarization is the evaluation of summary quality. ROUGE **Lin, "ROUGE: A Package for Automatic Evaluation of Summarizations"** remains the most widely used metric for automatically judging quality, despite criticism **Papineni et al., "BLEU: a Method for Automatic Evaluation of Machine Translation"**. In human evaluation, summaries are usually evaluated in some variation of adequacy (coverage and accurate representation of all important content), fluency (readability of the text), and relevance (covering only important content, __), but there are no studies of intermediate summaries produced by online systems.