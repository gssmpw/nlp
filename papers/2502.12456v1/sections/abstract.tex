%\vspace{-8mm}
\begin{abstract}
\vspace{-2mm}
Learning generative models of 3D point clouds is one of the fundamental problems in 3D generative learning.
%
One of the key properties of point clouds is their permutation invariance, i.e., changing the order of points in a point cloud does not change the shape they represent. 
%
In this paper, we analyze the recently proposed equivariant OT flows that learn permutation invariant generative models for point-based molecular data and we show that these models scale poorly on large point clouds. %, and naive ignore this invariance in their modeling will lead to poor OTs.
%
Also, we observe learning (equivariant) OT flows is generally challenging since straightening flow trajectories makes the learned flow model complex at the beginning of the trajectory. 
%
To remedy these, we propose \textit{not-so-optimal transport flow models} that obtain an approximate OT by an offline OT precomputation, enabling an efficient construction of OT pairs for training. 
%
During training, we can additionally construct a hybrid coupling by combining our approximate OT and independent coupling to make the target flow models easier to learn. 
% During training, the obtained point cloud and noise pairs are fed to flow models with small perturbations applied to the noise.
%
In an extensive empirical study, we show that our proposed model outperforms prior diffusion- and flow-based approaches on a wide range of unconditional generation and shape completion on the ShapeNet benchmark.

% Learning generative models of 3D point clouds is one of the fundamental problems in 3D generative learning. One of the key properties of point clouds is their permutation invariance, i.e., changing the order of points in a point cloud does not change the shape they represent. In this paper, we analyze the recently proposed equivariant OT flows that learn permutation invariant generative models for point-based molecular data. We show that these models scale poorly on large point clouds, and by constructions, their target flow is a complex function. To remedy these, we propose not-so-optimal transport flow models that align the permutation of input data and noise before training. During training, the aligned point cloud and noise pairs are fed to flow models with small perturbations applied to the noise. In an extensive empirical study, we show that our proposed model outperforms prior approaches on a wide range of conditional and unconditional generative problems on the ShapeNet benchmark.
% \phil{TODO: To Edward: (1) please consistently use the term ``not-so-optimal transport'' in Section 3; (2) Is it ok to say ``a wide range of''?}
\end{abstract}