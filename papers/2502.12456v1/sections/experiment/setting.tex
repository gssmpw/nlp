
%\subsection{Experimental Setup}

In this section, we present our experimental results for unconditional and conditional 3D point cloud generation, \ie, shape completion, 
% \phil{TODO: do you finally have time to show ``conditional 3D point cloud generation''?}
after reviewing dataset and evaluation details.

% \av{If you need more space, some of these implementation details can be moved to appendices.} \ed{Lets do so.}

\para{Dataset.}
Following~\citet{yang2019pointflow,klokov2020discrete,cai2020learning,zhou2021pvd}, 
we employ the ShapeNet dataset~\cite{chang2015shapenet} for training and evaluating our approach. 
%generative model.
%
%Following existing works~\cite{yang2019pointflow,klokov2020discrete,cai2020learning,zhou2021pvd}, 
Specifically, we train separate generative models for the Chair, Airplane, and Car categories with the provided train-test splits.
%
To form our training point clouds, we randomly sample a superset of $M$ = 100K points on 
each input 3D shape.
%
Then, during the online random subsampling, we randomly subsample the superset to 
$N$ = 2,048 points, 
following the procedure in Section~\ref{subsec:ot_approx}.

In addition, we prepare a partial input shape for the shape completion task for each training shape.
% On the other hand, we prepare condition inputs for conditional generation for each training shape.
%
% For single-view conditional generation, we prepare 32 renderings at a resolution of 1024 from 32 camera poses. 
% \CL{How the camera poses are distributed? Are we uniformly sampling the camera on a sphere with fixed sphere with camera facing the center of the object?}
% \ednote{I am using Xiaohui's renderings. Could I have this detail?}
%
% In the voxel-conditioned generation, we leveraged the approach in~\cite{} to compute a voxel grid of $16^3$ resolution.
%
% For shape completion, 
We use the GenRe dataset~\cite{zhang2018learning} for depth renderings of ShapeNet shapes. 
%
Partial point clouds are obtained by unprojecting and sampling up to 600 points from these depth images.
% We utilize the GenRe dataset~\cite{zhang2018learning}, which provides depth renderings of each ShapeNet shape for 20 random views.
% We obtain the partial point cloud by unprojecting the depth images and sampling up to 600 points as inputs. 

% \vspace{-6mm}
\para{Evaluation Metrics.}
We use LION~\cite{zeng2022lion}'s evaluation protocol, focusing on 1-NN classifier accuracy (1-NNA) with Chamfer Distance (CD) and Earth Mover's Distance (EMD) metrics.
%
% For unconditional generation, we adopt the evaluation protocol of LION~\cite{zeng2022lion},
% which primarily focuses on 1-NN classifier accuracy (1-NNA), which contains two versions by leveraging different distance metrics, \ie, Chamfer Distance (CD) or Earth Mover's Distance (EMD). 
%
1-NNA measures how well the nearest neighbor classifier differentiates generated shapes from test data. 
Optimal generation quality is achieved when the classifier accuracy is ${\sim}50\%$.
%
% 1-NNA is considered more robust and correlates better with generation quality than other metrics (Point Flow~\cite{yang2019pointflow}).
% This metric measures how well a nearest neighbor classifier differentiates the generated shapes from those in the test dataset.
% %
% If a classifier cannot distinguish the generated shapes from those in the test set (achieving $\sim50 \%$ accuracy), the generation quality is considered optimal.
% %
As discussed by \citet{yang2019pointflow}, 1-NNA is more robust and correlates better with generation quality.


% \CL{we may need to mention how we do the conditional generation in the main paper or appendix.},\ednote{Yes, I add some descriptions of the network in the implementation details}
For shape completion, 
since we have a ground-truth (GT) shape for each given condition, we follow~\cite{zhou2021pvd} to measure the similarity of our generated shape against the GT shape.
%
In particular, we randomly sample 2,048 
% \phil{2,048?
% please check all numbers to ensure you use a consistent style with a comma}
points on the GT shape surface and use CD and EMD distance metrics, where lower values indicate a higher similarity and thus a better performance.
