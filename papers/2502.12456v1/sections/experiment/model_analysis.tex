\vspace{-3mm}
\subsection{Model Analysis}
\label{subsec:model_analysis}
\vspace{-3mm}
In the followings, we perform analysis on 
major
%different important 
modules in our approach, including the blending coefficient $\beta$ of the hybrid coupling and the effect of the superset size $M$ for the OT computation.

\input{tables/beta_comp}
\para{Blending Coefficient $\beta$.}
We examine the impact of different couplings on flow matching model training.
% s \phil{remove s? since you only use one typical network model?}.
%
Specifically, we train models with coupling blending coefficient $\beta$ from 0 (OT approximation) to 1.0 (independent coupling).
%
For each  value, we train a model from scratch on the Chair Category and evaluate it using the 1-NNA-CD \& EMD metrics with 100 steps.
%
% We use a superset size of $10,000$ with exact OT to avoid approximation errors.
To avoid approximation errors in larger supersets, we adopt a superset size of $10,000$ with exact OT in this experiment.
% \phil{TODO: The meaning of ``potential errors is no clear}

Table~\ref{tab:beta_comp} presents the results.
%of employing different $\beta$ values.
%
We can observe that directly employing our OT approximation ($\beta = 0.0$) can lead to 
%significantly worse 
inferior performance, which can be gradually improved by injecting a small amount of noise into the coupling.
%
Interestingly, the best performance is achieved when compared with other cases at around $\beta=0.2$. 
%
Injecting more noise until arriving at independent coupling does not yield further improvement.
%
These results demonstrate that neither our OT approximation nor independent coupling is optimal in terms of generation quality, and the hybrid coupling is necessary.
% final generation performance.
% \phil{What is final generation performance? generation quality?}
%
% A proper blending helps 
% %is necessary 
% to enhance the final outcome.
% \phil{Do you need this sentence?}

\para{OT Supersets Size $M$.}
Next, we investigate the importance of constructing sufficiently large supersets for OT computation.
%
Here, we try supersets of varying sizes, starting from 2,048 (number of points to be generated) and progressively increasing the number towards 100,000.
%used in our approach.
%
As outlined in Section~\ref{subsec:ot_approx}, we employ the exact OT method for superset sizes of 10,000 or fewer points, and the OT approximation method for larger sets.
%
For each superset size, we also train a flow matching model on the Chair Category and evaluate also its generation quality over 100 inference steps.
% , measured by 1-NNA-CD and 1-NNA-EMD.

Table~\ref{tab:ot_size_comp} reports the evaluation results.
%
Overall, we notice that a small superset size usually leads to slightly worse performance, potentially due to overfitting the same target generation points. 
%
Increasing the number of points in the superset helps improve the performance.
%
Notably, despite using an approximate OT (introduced in Section~\ref{subsec:ot_approx}), we still observe some improvement in the generation quality when using a large superset ($M = 100,000$), indicating the benefit of using a large superset outweighs the errors introduced by the approximation.


\input{tables/ot_size_comp}

