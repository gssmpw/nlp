\vspace{-4mm}
\subsection{Unconditional Generation}
\label{subsec:uncond_gen}
\vspace{-2mm}
%In this section, we compare our framework for unconditional generation with other approaches.
% This section presents comparisons of our framework with other approaches on unconditional generation.
%

%
% \ednote{We are using a different dataset from PVD now and might see if the reviewers request to add an experiment on this.}
% \CL{Will we have a Conditional Generation subsection? For that there's a dataset https://github.com/Hydrogenion/ViPC, where the self-occlusion and object-occlusion are considered for partial Shapenet point cloud. Maybe Edward already mentioned this in the meeting. Another one is https://completion3d.stanford.edu/}

\input{figures/main_quantitative_comp}
\input{figures/main_qualitative_comp}
%\subsubsection{Training Paradigm Comparisons}
%\label{subsec:training_paradigm_comp}

For unconditional generation, we first evaluate our framework against alternative training paradigms, including diffusion models and flow matching models with different couplings:

\para{Baselines.}
%We compare our method both quantitatively and qualitatively with other training paradigms.
We consider both quantitative and qualitative comparisons.
%
% In this setting, we maintain the same network architecture and hyper-parameters, changing only the training procedure or objective, for a fair comparison.
%
In this setting, We maintain the same architecture and hyperparameters, changing only the training procedure or objective for fair comparison.
%to ensure the fairness.
%
% Specifically, we compare with four training alternatives:
% (i) replacing the objective with the diffusion model objective using v-prediction~\cite{salimans2022progressive},
% (ii) training with flow matching objective using independent coupling~\cite{lipman2022flow},
% (iii) training with flow matching objective using Minibatch OT~\cite{tong2023improving,pooladian2023multisample}, and
% (iv) training with flow matching objective using Equivariant OT~\cite{song2024equivariant,klein2024equivariant}.
We then compare with four training alternatives:
(i) the diffusion model objective using v-prediction~\cite{salimans2022progressive}, and (ii)-(iv) three flow matching models with different coupling methods: independent coupling~\cite{lipman2022flow}, Minibatch OT~\cite{tong2023improving,pooladian2023multisample}, and (iv) Equivariant OT~\cite{song2024equivariant,klein2024equivariant}.
%
For Minibatch OT, we obtain the OT using a batch size of 64 on each GPU but compute the training losses across all four GPUs.
%
For Equivariant OT, due to its high computational demand,
%limitations, 
we can only consider permutations with 
%compute the OT at 
a batch size of 1, and train the model with the same training time (4 days).

\para{Quantitative Comparisons.}
Figure~\ref{fig:main_quantitative_comp} plots 1-NNA-CD \& EMD for varying computation budget (inference steps) over the Chair, Airplane, and Car categories.
%, given different computation budgets (inference steps).
%
Overall, our approach (blue curve) achieves similar or better performance across all metrics and categories, particularly when given a sufficient number of steps,~\eg, 100.
%
% Notably, due to a straighter inference trajectory, our approach performs best with fewer inference steps,~\eg, 10-20. This shows that our approximate OT benefits from the same advantages of OT flows.
%
Our approach performs best with fewer inference steps (10-20) due to a straighter trajectory, demonstrating the advantages of our approximate OT.
%
Minibatch OT's generation performance matches original flow matching, possibly due to small OT distance reductions between training pairs.
%
% For Minibatch OT (green curve), the unconditional generation performance is similar to the original flow matching with independent coupling, due to small reductions in OT distances between training pairs.
%
Equivariant OT (orange curve) shows 
inferior performance compared to others, likely due to slow training within training budget.


\para{Qualitative Comparisons.}
Figure~\ref{fig:main_qualitative_comp} shows visual comparisons of different training methods for the Chair and Airplane categories, which feature more distinguishable characteristics.
%
To facilitate easier comparisons, we use our generation results to retrieve the closest results from other methods.
%
We display the generation results with 10 inference steps on the left.
%
% It is noticeable that other methods introduce some noise to distinct parts of the shapes, such as the chair's armrest or the airplane's wing.
% %
% In contrast, our method better produces these structures.
Unlike other methods that often add noise to distinct shape parts such as chair's armrests or airplane's wings, our approach better preserves these structures.
%
With sufficient steps (right), nearly all methods (except Equivariant OT) can produce high-quality results with thin structures and fine details, \eg, the back of the chair.

% \phil{TODO}


% \subsubsection{Comparisons with Other Point Cloud Generation methods}

Next, we compare with other point cloud generation methods that require multi-steps generation:
\input{figures/external_quantitative_comp}
\input{figures/external_qualitative_comp}

\para{Baselines.}
We also compare our framework with the following four baselines under the same inference budget: 
%
(i) PVD~\cite{zhou2021pvd} and (ii) DiT-3D~\cite{mo2023dit3d} are diffusion-based models that directly generate point clouds, whereas 
(iii) LION~\cite{zeng2022lion} employs a diffusion model to produce a set of latent points that are later decoded into a point cloud.
%
%to three additional baselines, either diffusion models or flow-based generative models, under the same inference budget.
%
%Among these baselines, PVD~\cite{zhou2021pvd} and DiT-3D~\cite{mo2023dit3d} are diffusion models working on the direct generation of point clouds. 
%
%LION~\cite{zeng2022lion} employs a diffusion model to produce a set of latent points, which can be later decoded to obtain a point cloud.
%
Since these models are trained for more inference steps (1,000), we employ a DDIM sampler at inference, which is considered to be effective with fewer inference steps.
%
(iv) Lastly, PSF~\cite{wu2023psf}, similar to our approach, uses a flow-based generative model, but additionally applies a rectified flow procedure~\cite{liu2022flow} to progressively straighten the inference trajectory, taking significantly additional training time.
%
Since our method can be further accelerated with rectified flows, we only compare performance with PSF before the expensive rectification step.
%
Note that we only report DiT-3D's result on the Chair category, as the pre-trained models for other categories are unavailable.

\para{Quantitative Comparisons.}
Figure~\ref{fig:external_quantitative_comp} shows results on 1-NNA-CD \& EMD for different inference steps.
%
Our method shows a comparable performance with existing flow-based and diffusion-based generative models when given 100 inference steps.
%
Additionally, our method can achieve significantly better generation quality when the inference is reduced to around 20 steps, without relying on additional expensive straightening procedures,~\eg, rectified flow.

\para{Qualitative Comparisons.}
Figure~\ref{fig:external_qualitative_comp} shows visual comparisons with other point cloud generation methods for the Chair and Airplane categories, which feature more distinguishable characteristics.
%
Overall, all existing methods can generate high-quality 3D shapes when given a large number of inference steps.
%
However, when the number of inference steps is reduced, the generation quality of other methods declines significantly, as illustrated by the jet planes example on the left.