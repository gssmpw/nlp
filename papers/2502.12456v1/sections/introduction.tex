\vspace{-3mm}
\section{Introduction}
\vspace{-1mm}

% what is the problem we are trying to solve
% why is it important and what are the applications
% how do people usually solve them
% what is our solution
% what are the advantages

Generating 3D point clouds is one of the fundamental problems in 3D modeling with applications in shape generation, 3D reconstruction, 3D design, and perception for robotics and autonomous systems. Recently, diffusion models~\cite{sohl2015deep,ho2020denoising} and flow matching~\cite{lipman2022flow} have become the de facto frameworks for learning generative models for 3D point clouds.
%
%
These frameworks often overlook 3D point cloud permutation invariance, implying the rearrangement of points does not change the shape that they represent.
% However, these frameworks often overlook one of the key properties of 3D point cloud which is the permutation invariance. That is permuting points does not change the shape that they represent.  
%ignore one of the key properties of 3D point cloud which is the permutation invariance. That is permuting points does not change the shape that they represent. This property is often considered when designing neural networks for point clouds, but it is ignored in generative models which treat them as high-dimensional vectors with a fixed order.

In closely related areas, equivariant optimal transport (OT) flows~\cite{klein2024equivariant,song2024equivariant} have been recently developed for 3D molecules that can be considered as sets of 3D atom coordinates. 
These frameworks learn permutation invariant generative models, \ie, all permutations of the set have the same likelihood under the learned generative distribution.
They are trained using optimal transport between data and noise samples, yielding several key advantages including low sampling trajectory curvatures, low-variance training objectives, and fast sample generation~\cite{pooladian2023multisample}.
Albeit these theoretical advantages, our examination of these techniques for 3D point cloud generation reveals that they scale poorly for point cloud generation.
This is mainly due to the fact that point clouds in practice consist of thousands of points whereas molecules are assumed to have tens of atoms in previous studies.
Solving the sample-level OT mapping between a batch of training point clouds and noise samples is computationally expensive.
Conversely, ignoring permutation invariance when solving batch-level OT~\cite{pooladian2023multisample,tong2023improving} fails to produce high-quality OT due to the excessive possible permutations of point clouds.

\input{figures/overview}

In this paper, we propose a simple and scalable generative model for 3D point cloud generation using flow matching, coined as \textit{not-so-optimal transport flow matching}, as shown in Fig~\ref{fig:overview}.
We first propose an efficient way to obtain an approximate OT between point cloud and noise samples.
Instead of searching for an optimal permutation between point cloud and noise samples online during training, which is computationally expensive, we show that we can precompute an OT between a dense point superset and a dense noise superset offline.
Since subsampling a superset preserves the underlying shape, we can simply subsample the point superset and obtain corresponding noise from the precomputed OT to construct a batch of noise-data pairs for training the flow models.

% In this paper, we propose a simple and scalable generative model for 3D point cloud generation using flow matching. Similar to OT flow matching, we couple 3D point cloud data with 3D Gaussian noise samples, but rather than focusing on mini-batch OT coupling, we simply align noise and point cloud by permuting the points for each instance. Since sub-sampling point clouds does not change their shape, we align 3D data and noise in an offline fashion using a high-resolution representation of training 3D shapes. During training, we simply subsample the point cloud and the corresponding aligned noise to construct a batch of noise-data pairs for training 
% %flows 
% \phil{the flow models}.

\input{figures/lipchitz_vis}
%We demonstrate that the training pairs sampled from our approximate OT are of high quality.
%
We demonstrate that our approximate OT reduces the pairwise distance between data and noise significantly and benefits from the advantages of OT flows,~\eg, straightness of trajectories and fast sampling. 
%
However, a careful examination shows that learning (equivariant) OT flows is generally challenging since straightening flow trajectories makes the learned flows complex at the beginning of the trajectory. 
%
Intuitively, in the OT coupling, the flow model should be able to switch between different target point clouds (i.e., different modes in the data distribution) with small variations in their input, making the flow model have high Lipchitz (see Fig~\ref{fig:lipchitz_vis}).

% We show that our \textit{not-so-optimal transport} coupling in fact reduces the pairwise distance between data and noise significantly and benefits from the advantages of OT flows (e.g., straightness of trajectories and fast sampling). However, a careful examination shows that learning (equivariant) OT flows is generally challenging since straightening flow trajectories makes the learned flow model complex at the beginning of the trajectory. Intuitively, once an OT transport is established between the noise and data samples, the trained models should be able to switch between different target point clouds with small variations in their input, making the flow model have high Lipchitz as shown in Fig ??.



To remedy this, we propose a simple approach to construct a less ``optimal'' hybrid coupling by blending our approximate OT and independent coupling used in the flow matching model.
%
In particular, we suggest perturbing the noise samples obtained from our approximate OT with small Gaussian noise.
%
While this remedy makes our mapping less optimal from the OT perspective, we show that it empirically shows two main advantages: 
First, the target flow model is less complex and the generated points clouds have high sample quality. 
Second, when reducing the number of inference steps, the generation quality still degrades slower than other competing techniques, indicating smoother trajectories.

% To remedy this, we propose a simple approach that perturbs aligned noise samples with small perturbations. While this remedy makes our mapping less optimal from the optimal transport perspective, it empirically shows two main advantages: One the generated points clouds have high quality. Two, when reducing the number of sampling steps, the generation quality degrades slower than other competing techniques.

In summary, this paper makes the following contributions: (i) We show that existing OT approximations either scale poorly or produce low-quality OT for real-world point cloud generation. 
%
(ii) We show that albeit the nice theoretical advantages, equivariant OT flows have to learn a complex function with high Lipchitz at the beginning of the generation process. 
%
(iii) To tackle these issues, we propose a not-so-optimal transport flow matching approach that involves an offline superset OT precomputation and online random subsampling to obtain an approximate OT, and adds a small perturbation to the obtained noise during training.
%
(iv) We empirically compare our method against diffusion models, flows, and OT flows on unconditional point cloud generation and shape completion on the ShapeNet benchmark. 
%
We show that our proposed model outperforms these frameworks for different sampling budgets over various competing baselines on the unconditional generative task.
%
In addition, we show that we can obtain reasonable generation quality on the shape completion task in less than five steps, which is challenging for other approaches.

% In summary, this paper makes the following contributions: (i) We show that equivariant OT flow matching scales poorly for real-world point cloud generation. (ii) We show that albeit the nice theoretical advantages, equivariant OT flows have to learn a complex function with high Lipchitz at the beginning of the generation process. (iii) To tackle these issues, we propose a not-so-optimal transport flow matching approach that aligns noise with input 3D shapes by permuting it before training and adds a small perturbation to the matched noise during training. (iv) We empirically compare our method against diffusion models, flows, and OT flows on a range of problems including unconditional point cloud generation, and conditional XX on the ShapeNet benchmark. We show that our proposed model outperforms these frameworks for different sampling budgets.



% \phil{TODO: need to make sec 1 and sec 3 consistent in wordings. Also, our claims in sec 1 and 3}