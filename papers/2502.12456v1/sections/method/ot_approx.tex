\vspace{-2mm}
\subsection{Our OT Approximation}
\label{subsec:ot_approx}
\vspace{-2mm}
% \begin{itemize}
%     \item Introduction to our Optimal Transport (OT) pre-computation. We will cover two procedures for computing OTs: one for medium-sized point sets (approximately 2,000 to 10,000 points) and another for larger point sets (around 100,000 points).
%     \item Introduction to Our OT Sampling. We present a procedure to approximate the Optimal Transport (OT) coupling by sampling pre-computed OTs. Additionally, we demonstrate that the marginal distribution of this coupling still recovers both the data distribution and the Gaussian noise distribution.
% \end{itemize}

% Some backup for writing
% \item[(iii)] \textbf{Sub-samples of Point Supersets.}
% Traditionally, shapes are represented as 2D surfaces embedded in the 3D Euclidean space and discretized into triangular/polygonal meshes.
% %
% To obtain a point cloud, a trivial way to do so is to sample the mesh during each training iteration, but it is not computational efficient.
% %
% To enhance the efficiency, we can represent the surface as a dense superset of points, where the size of this set is much larger than the size of point cloud to be generated.
% %
% As a result, the training point cloud can be efficiently sub-sampled from the given superset.

% As discussed in the previous section, existing OT approximation methods for 3D point cloud generation are either far from optimal or computationally inefficient.
%
% To obtain a high-quality OT map in an efficient way, we design a new approach for approximating the OT map, inspired by the fast point cloud sampling. 
%When working with point clouds, a
A simple approach to generate training point clouds is to re-sample the points from the object surface in each training iteration.
However, most point cloud generation methods avoid this tedious online sampling by pre-sampling a dense point superset $X_1 \in \mathcal{R}^{M \times 3}$ with $M >> N$.
During training, random subsets of $X_1$ are selected as training targets. %, so the computation becomes more efficient than online sampling.
This procedure converges to the true sampling distribution, following a straightforward extension of the law of large numbers (see Appendix proposition~\ref{prop:large_samples} for details).
In a similar spirit, %we develop an OT approximation procedure to avoid costly online computations for obtaining the training pair $(\rvx_0, \rvx_1)$.
%
% Typically, it involves two steps: (i) 
we compute an offline OT map between a dense point superset $X_1 \in \mathcal{R}^{M \times 3}$ and a dense randomly-sampled Gaussian noise superset $X_0 \in \mathcal{R}^{M \times 3}$, and during training, subsample data-noise pairs from the supersets based on the offline OT map.

%\para{Superset OT Precomputation.} 
%In this precomputation step, we need to obtain three related quantities in advance for each training shape:
%
%The first two are supersets: 
%(i) a dense point superset $X_1 \in \mathcal{R}^{M \times 3}$ representing the 3D shape, 
%(ii) a dense Gaussian noise superset $X_0 \in \mathcal{R}^{M \times 3}$ of the same size, and 
% Given an input shape $S$, we first construct a dense point superset $X_1 \in \mathcal{R}^{M \times 3}$ representing the 3D shape and obtain a dense Gaussian noise superset $X_0 \in \mathcal{R}^{M \times 3}$ of the same size.
%
%The third element is 
%(iii) a bijective map $\Pi$ between each noise point and a corresponding shape point, \ie, $\Pi: X_0 \leftrightarrow X_1$.
% With these two supersets, our goal is to establish a bijective map between each noise point and a corresponding shape point.
%
% After computing these, we can randomly sample $N$ points from $X_0$ and obtain their corresponding points in $X_1$ using $F$ to form the training pair $(x_0, x_1)$.%

% Although we can now perform expensive computations offline, careful considerations are still necessary to enable scalable processing for the entire dataset.
%
\para{Superset OT Precomputation.} 
Given supersets $X_0$ and $X_1$, we compute a bijective map $\Pi$ between them,~\ie, $\Pi: X_0 \leftrightarrow X_1$. When $M$ is small, following~\cite{song2024equivariant,klein2024equivariant}, we compute the bijective map using the Hungarian algorithm~\cite{kuhn1955hungarian}. However, this algorithm scales poorly for large point clouds,~\ie, $M>10K$. For such large point clouds, we use Wasserstein gradient flow to transform
%that allows the transformation of 
$X_0$ into $X_1$ by minimizing their Wasserstein distance iteratively. Using an efficient GPU implementation~\cite{feydy2019interpolating}, the OT precomputation takes only ${\sim}30$ seconds for 100K points, showing its high scalability. 
%A detailed description of this procedure is provided in Appendix~\ref{subsec:approx_details}.
See Appendix~\ref{subsec:approx_details} for the details in procedure.
% \CL{I can't find it in the Appendix.}.


\iffalse
In this work, we employ two approaches for obtaining these three quantities: one for medium-sized supersets and another for larger supersets:
% \begin{itemize}

\textbf{(i) Exact OT ($\leq$ 10,000 points).} In this case, we assume that the supersets $(X_0, X_1)$ are given. These are a dense random Gaussian noise superset and direct samples from the surface, respectively. 
% \CL{or the other way around?}. \ednote{Thanks for spotting this.}
Our goal is to obtain bijective map $\Pi$ between them.
%
Following existing works~\cite{song2024equivariant,klein2024equivariant}, we compute the bijective map using the Hungarian algorithm~\cite{kuhn1955hungarian}. 
%
% Since each problem instance is highly parallelizable, we can process the dataset for supersets of this scale within a day using 300 CPUs.
%

\textbf{(ii) Approximated OT ($>$ 10,000 points).
% \phil{TODO: note that the case of having exactly 10k points is missed}
}
Rather than searching for an optimal bijective map given a pair of supersets, we adopt an approximation procedure from~\cite{feydy2020geometric} to find a suitable $X_1$, assuming $X_0$ and $\Pi$ are fixed.
% Instead of directly using surface samples as $X_1$, we follow ~\cite{feydy2020geometric} to obtain the OT through an iterative procedure.
% to progressively deform dense Gaussian noises toward the input superset.
This process takes the same inputs: a Gaussian dense noise superset $X_0$ and a dense point superset $X_1'$ from the surface.
%
The procedure approximates a deformed noise superset that minimizes the OT distance with $X_1'$ while maintaining a one-to-one correspondence with the noise superset $X_0$.
We then use this deformed noise superset as our $X_1$ for the subsequent online subsampling.
%
% We approximate a deformed noise superset minimizing OT distance with $X_1'$, maintaining one-to-one correspondence with $X_0$. This deformed superset becomes our $X_1$ for online subsampling.
%
% To achieve this, we compute the unbiased Sinkhorn Divergence \cite{ramdas2017wasserstein}, a form of OT distance measure, between the current deformed noises and the input superset $X_1'$, then perform gradient descent on the noises' positions.
%
% We then use the output as $X_1$ and its corresponding dense Gaussian noises as $X_0$.
%
A detailed description of this procedure is provided in the Appendix.

% By leveraging an efficient GPU implementation~\cite{feydy2020geometric}, the pre-computation takes only about 30 seconds, even with 100,000 points, demonstrating the high scalability of this approach.
%
Using an efficient GPU implementation~\cite{feydy2020geometric}, precomputation takes only $\sim 30$ seconds for 100,000 points, showing high scalability.
%
% Note that since we use the deformed noise superset as $X_1$, approximation errors are introduced in this procedure compared to the true samples $X_1'$.
%
Note that using the deformed noise superset as $X_1$ introduces approximation errors compared to true samples $X_1'$.
%
However, we empirically observe that the procedure usually converges well with only small errors, and in our experiments, the large superset size typically outweighs the introduced error (as shown in Section~\ref{subsec:model_analysis}).
%
% Empirically, our procedure converges well with minimal errors. In our experiments, the benefits of using a large superset typically outweigh any introduced errors (see Section~\ref{subsec:model_analysis} for details).
    
% \end{itemize}

\fi

\para{Online Random Subsampling.}
\iffalse
With these three quantities computed, we can now obtain the training pairs.
%
In each training iteration, for every training shape, we first sample $N$ points from its precomputed $X_1$ to obtain $\rvx_1$.
%
For each point in $\rvx_1$, we then obtain the corresponding points in $X_0$ using the \phil{precomputed} bijective map $\Pi$ to obtain $\rvx_0$.
%
Using this sampled training pair $(\rvx_0, \rvx_1)$, we train the flow matching model according to Equation~\ref{eq:cfm_x0_x1}.
\fi
%
Given precomputed coupling $\Pi(X_0, X_1)$, we randomly sample data-noise pair $(\rvx_0, \rvx_1) \sim \Pi(X_0, X_1)$ and we train the flow matching model according to Equation~\ref{eq:cfm_x0_x1}. As we show in Figure~\ref{fig:ot_analysis} (left), this significantly reduces the transport cost, while introducing negligible training overhead. In Appendix~\ref{subsec:our_ot_proof}, we show that the sampled training pair converges to correct marginals if $M$ is sufficiently large.

\iffalse
Overall, this procedure significantly reduces the distances between training pairs (as shown in the green curve of Figure~\ref{fig:ot_analysis} (left)), while introducing negligible training overhead.
%
Besides, the sampled training pair converges to the correct marginal if $M$ is sufficiently large, \ie, $\int q(\rvx_0, \rvx_1) dx_1 = q_0(\rvx_0)$ and $\int q(\rvx_0, \rvx_1) dx_0 = q_1(\rvx_1)$. 
%
A detailed proof is provided in the Appendix.
\fi

\input{figures/straightness_analysis}

% In practice, when we employ these pairs in training the flow matching model, it results in a much straighter inference path for the trained model, agreeing with the results of \phil{the} existing works.
%
In practice, using these pairs for training results in straighter sampling trajectories, measured by the curvature of the sampling trajectory, as shown in Figure~\ref{fig:straightness_analysis} (left). 
%
The model trained with our OT approximation (orange curve) exhibits a much lower maximum curvature compared to the one with independent coupling (blue curve).
%
We also visualize the sample trajectories of these two models in Figures~\ref{fig:straightness_analysis} (right) (a-b), confirming straighter trajectories for our model.

% \para{OT Computation Implementations.}
% Although we can now perform expensive Optimal Transport (OT) computations offline, careful considerations are still necessary to enable scalable processing for the entire dataset.
% %
% In this work, we employ two approaches for Optimal Transport (OT) precomputation: one for medium-sized supersets and another for larger supersets:
% \begin{itemize}
%     \item[(i)] \textbf{Exact OT (less than 10,000 points).} In this case, we assume that the supersets $(X_0, X_1)$ are given and hope to obtain the bijective map $F$ between them.
%     %
%     Following existing works~\cite{song2024equivariant,klein2024equivariant}, we compute the desired bijective map using the Hungarian algorithm~\cite{kuhn1955hungarian}. 
%     %
%     Since each problem instance is highly parallelizable, we can process the dataset for supersets of this scale within a day using 300 CPUs.
%     %
%     \item[(ii)] \textbf{Approximated OT (100,000 points).}
%     Rather than searching for a bijective map given a pair of supersets, we adopt an approximation procedure from~\cite{feydy2020geometric} to find a suitable $X_1$, assuming $X_0$ and $F$ are fixed.
%     % Instead of directly using surface samples as $X_1$, we follow ~\cite{feydy2020geometric} to obtain the OT through an iterative procedure.
%     % to progressively deform dense Gaussian noises toward the input superset.
%     This process takes two inputs: a Gaussian dense noise superset $X_0$ and a dense point superset $X_1'$ from the surface.
%     %
%     The procedure approximates a deformed noise superset that minimizes the OT distance with $X_1'$ while maintaining one-to-one correspondence with the noise superset $X_0$.
%     We then use this deformed noise superset as our $X_1$ for the subsequent online subsampling.
%     %
%     % To achieve this, we compute the unbiased Sinkhorn Divergence \cite{ramdas2017wasserstein}, a form of OT distance measure, between the current deformed noises and the input superset $X_1'$, then perform gradient descent on the noises' positions.
%     %
%     % We then use the output as $X_1$ and its corresponding dense Gaussian noises as $X_0$.
%     %
%     A detailed description and explanation of this iterative procedure is provided in the Appendix.

%     By leveraging the efficient GPU implementation~\cite{feydy2020geometric}, the pre-computation takes only about 30 seconds, even with 100,000 points, demonstrating the high scalability of this approach.
%     %
%     Note that since we use the deformed noise superset as $X_1$, approximation errors are introduced in this procedure compared to the true samples $X_1'$.
%     %
%     However, we empirically observe that the procedure usually converges well with only small errors, and in our experiments, the large superset size typically outweighs the introduced error.
    
% \end{itemize}

% %
% Typically, it involves two steps: (i) an OT pre-computation step for computing the OT map between the superset representing the underlying surface and a dense Gaussian noise, and (ii) an online random sampling to obtain the training pairs.
% %
% Importantly, if the size of the superset is large enough, we can leverage the properties of a 3D point cloud to ensure this procedure converges to a valid coupling.

% \para{Super-set OT pre-computation.}
% In order to efficiently sample training pairs, we first pre-compute an OT map for a dense superset point cloud and a corresponding dense Gaussian noise.
% %
% %
% During the OT computation, we hope to obtain a bijective map between each point in the superset with a corresponding noise point.

% However, if we directly employ the Hungarian algorithm, the offline precomputation of this dense point set will still be intractable due to the large set size.
% %
% To resolve this, we propose an iterative algorithm to progressively deform the Gaussian noises towards the superset points.
% %
% This algorithm involves multiple steps, where for each step, we first employ a sinkhorn algorithm to approximate the relaxed OT map between the currently deformed noise and the superset points.
% %
% With the computed OT map, we then employ gradient descent to reduce the distance of between the assigned super point and the noise point and update corresponding deformed noise.
% %
% Then we repeat this procedure until it is converged, and the deformed noise $X_0^T$ would be taken as the final superset points for training.
% %
% The detailed algorithm is shown below:

% \input{algorithms/ot_approx}

% \paragraph{Online Training Pair Sampling.}
% With the Gaussian noise $X_0$ and deformed noise $X_0^T$, we can then prepare the training process.
% %
% In particular, in each training iteration, we can take $N$ random subsamples of $X_0$ and obtain their corresponding deformed noises in $X_0^T$ as $x_0$ and $x_1$ respectively.
% %
% Then, we can apply the CFM training objective from Equation~\ref{eq:cfm_x0_x1} but replace the sampled training pair.


\paragraph{}