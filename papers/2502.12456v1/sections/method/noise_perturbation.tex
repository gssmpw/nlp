\vspace{-6mm}
\subsection{Hybrid Coupling}
\label{subsec:coupling_interpolation}
\vspace{-2mm}
Though training flows with OT couplings comes with appealing theoretical justifications (\eg, straight sampling trajectories), we identify a key training challenge with them that is often overlooked in the OT flows literature. Our experiments (\eg, Section~\ref{subsec:uncond_gen}) indicate that flows trained with equivariant OT maps are often outperformed by those with independent coupling in terms of sample quality, especially when the number of sampling steps is large. We hypothesize this is due to the increasing complexity of target vector fields for OT couplings that makes their approximation harder with neural networks. 

\input{figures/jacobian_analysis}
Intuitively, as we make target sampling trajectories straighter using more accurate OT couplings, the complexity of generation shifts toward smaller time steps. As Figure~\ref{fig:lipchitz_vis} shows, in the limit of straight trajectories, the learned vector field $\rvv_{\theta, 0}(\rvx_0)$ should be able to switch between different target point clouds with small variation in $\rvx_0$, forcing $\rvv_{\theta, 0}$ to be complex at $t{=}0$. This problem is further exacerbated in the equivariant OT flows with large $N$ where permuting Gaussian noise cloud in the input makes it virtually the same for all target point clouds. To verify this, we measure the trained vector field's complexity for 3D point cloud generation using the Jacobian Frobenius norm in different timesteps in Figure~\ref{fig:jacobian_analysis}. As hypothesized above, switching from independent coupling (blue curve) to our OT approximation (orange curve) shifts the high Jacobian norm at $t\approx1$ for independent coupling to $t\approx0$ for OT coupling. This motivates us to develop a method to make 
%the problem of approximating the target vector field with neural networks easier, 
it easier for neural networks to approximate the target vector field, 
while still maintaining a relatively straight path.


%However, learning a straighter trajectory does not necessarily simplify the vector field network's learning task.
%
%In fact, it often worsens the generation quality (detailed in Section~\ref{subsec:model_analysis}).


\iffalse
\input{figures/jacobian_analysis}
\para{Model Complexity.}
% Straight trajectories require accurate early-step estimation, increasing model complexity in these intervals.
%
We attribute this to the fact that straight trajectories demand accurate estimation even at early inference steps, implying increased model complexity in these intervals.
%
To verify this, we measure the trained neural networks' complexity using the Jacobian Frobenius norm, illustrating the magnitude over different time intervals in Figure~\ref{fig:jacobian_analysis}.
%
Overall, when employing our OT approximation (orange curve), the norm is significantly higher at the beginning of the trajectory path, implying that the trained model is more Lipchitz and complex. 
%
This motivates us to develop a method to alleviate the learning burden of network, especially at the early stage, while still maintaining a relatively straight path.
%
% Though we are interpolating towards 

% Though we can now obtain the training pairs are closer to the samples from an OT map and obtain a straighter inference path, we empirically notice that this will lead to training difficulties and result in worse generation performance.
% %
% In particular, we plot the performance of Flow Matching (sampling training pairs from independent coupling) and our method (sampling training pairs using pre-computed superset OT) in Figure~\ref{}.
% %
% We notice our method can achieve a better performance when given less number of inference steps, implying our method can learn a straighter trajectory.
% %
% However, the performance is significantly worse than flow matching given a larger number of inference steps.



% We hypothesize this phenomenon that a straighter generation path pose a much challenging training process for the generative model.
% %
% To verify this, we plot the Jacobian norms of the vector field network trained with Flow Matching model and our method.
% %
% We notice that the vector field learned by our method ace challenges in the initial time intervals, while Flow Matching encounters more difficulties in later timesteps.
%
\fi


\textbf{Hybrid Coupling.} % \phil{Blending or interpolation?}
%As Figure~\ref{fig:jacobian_analysis} suggests, in constant with our OT approximation, the model complexity lies at the end of the trajectories, if the training pairs are sampled from the independent coupling method (blue curve).
% This contrasts with the case of independent coupling (blue curve), where complexities are primarily placed at the end of trajectories.
% As Figure~\ref{fig:jacobian_analysis} suggests, different couplings introduce learning complexities at various time intervals. 
%
Given the different behavior of independent and OT couplings in Figure~\ref{fig:jacobian_analysis}, we aim to reduce the complexity of the vector field at early timesteps by combining our OT approximation with independent coupling.
%
To do so, we propose injecting additional random Gaussian noise into $\rvx_0$, making our OT couplings even less ``optimal''.
%
The new training $\rvx_0'$ is defined as:
\begin{equation}
    \rvx_0' = \sqrt{1 - \beta} \rvx_0 +  \sqrt{\beta} \rvepsilon, \quad \rvepsilon \sim \mathcal{N}(\rvepsilon; 0, \textbf{I}),
\end{equation}
where $\beta \in [0, 1]$ is the blending coefficient.
%
%This formulation draws inspiration from the forward process in the denoising diffusion probabilistic model~\cite{ho2020denoising}.
%
Intuitively $\beta$ allows us to switch smoothly between independent and OT couplings.
Specifically, for $\beta \rightarrow 0$, the coupled data and noise pairs converge to our OT couplings, whereas when $\beta \rightarrow 1$, they follow the independent coupling.
%
% As $\beta$ approaches 0, the distribution converges to our OT approximation; as $\beta$ nears 1, it tends toward independent coupling.
%
%Moreover, we demonstrate that this blending strategy maintains the correct marginal, i.e., $\int q(x_0, x_1) dx_1 = q_0(x_0)$. A detailed proof is provided in the Appendix.

We empirically observe that $\beta = 0.2$ in most experiments strikes a good balance between learning complexity (as shown by the green curve in Figure~\ref{fig:jacobian_analysis}), low curvature for the sampling trajectories (the green curve in Figure~\ref{fig:straightness_analysis} (left) and Figure~\ref{fig:straightness_analysis} (right) (c)), and sample generation quality (shown later in Section~\ref{subsec:model_analysis}). In the next section, we refer to this hybrid coupling as our main method. 

\iffalse
Through a hyperparameter analysis, we empirically set $\beta = 0.2$, which helps to reduce the training difficulty at early time intervals 
% (Figure~\ref{fig:jacobian_analysis}, green curve).
, as shown by the green curve in Figure~\ref{fig:jacobian_analysis}.
%
% We set $\beta = 0.2$ empirically, reducing early training difficulty .
Also, we can maintain a good generation quality
% ignificantly 
%
% improve the generation performance 
% \phil{TODO: maintain a good generation quality?}
%
(as outlined in Section~\ref{subsec:model_analysis}).
%
%Since the $\beta$ value remains 
since $\beta$ is 
relatively small, we observe that the trajectory's curvature stays low. 
%
This is revealed 
by the green curve in Figure~\ref{fig:straightness_analysis} (left) with the associated sample trajectory in Figure~\ref{fig:straightness_analysis} (right) (c).
\fi

% \paragraph{Noise Perturbation.}
% To avoid this difficulty, we propose to add additional noise the sampled training pair.
% %
% This additional noises can reduce the difficulty of the vector field estimation in the initial time intervals, and as shown in the Figure~\ref{}, we can obtain a much lower overall Jacobian norm over all intervals.
% %
% And from the generation performance comparison, we can obtain a much better performance than both naive OT Flow Matching and the Flow Matching for nearly all inference timesteps.