\vspace{-2mm}
\subsection{Preliminaries}
\label{subsec:preliminaries}
\vspace{-2mm}

% \begin{itemize}
%     \item Introduction of Continuous Normal Flow: we will introduce the continuous normal flow and cover its difficulty (computationally expensive) in training.
%     \item Introduction of Flow Matching Training Objective: we will cover the procedure to employ flow matching objective to train CNFs.
%     \item Introduction of different couplings: we will motivate to use different couplings to sample the training pairs. After that, we will cover three existing formulations of couplings, \ie, Minibatch OT Flow Matching, Equavariant Flow Matching.
% \end{itemize}


\para{Continuous Normalizing Flow (CNF)}~\cite{chen2018neural} morph a base Gaussian distribution $q_0$ into a data distribution $q_1$ using a time-variant vector field $\rvv_{\theta, t} : [0, 1] \times \mathbb{R}^d \rightarrow \mathbb{R}^d$, parameterized by a neural network $\theta$. The mapping is obtained from an ordinary differential equation (ODE):
%
%This vector field can then create a time-dependent diffeomorphic map  that is the solution to an ordinary differential equation (ODE):
\begin{equation}
\frac{d}{dt} \rvx_t = \rvv_{\theta, t}(\rvx_t).
%, \ed{\forall x \in \mathbb{R}^d}.
%\frac{d}{dt} \phi_t(x) = v_{\theta, t}(\phi_t(x)), \ed{\forall x \in \mathbb{R}^d}.
\end{equation}
%\CL{we may need to specify what $x$ is. It's not in the text from L36-40 } \ednote{Thanks! Let me add the definition to it.}
Conceptually, the ODE transports an initial sample $\rvx_0 \sim q_0$, where $\rvx_0 \in \mathbb{R}^d$ with $p_t$ denoting the distribution of samples at step $t$ and $p_0(\rvx) := q_0(\rvx)$.
%
%Assuming initial samples from a known distribution $p_0(\rvx) := q_0(\rvx)$, like Gaussian, this map transforms it into a more complex distribution $p_t$ over time via diffeomorphism.
% If we assume that the initial samples are drawn from a known distribution $p_0(\rvx) := q_0(\rvx)$, such as a Gaussian distribution, this map can modify it into a more complex distribution $p_t$ by diffeomorphism in time.
\iffalse
This results in a probability trajectory $p_t : [0, 1] \times \mathbb{R}^d \rightarrow \mathbb{R}_{> 0}$, delineated by the inverse mapping $\phi_t^{-1}$:
\begin{equation}
\label{eq:push_forward}
p_t(x) = p_0(\phi_t^{-1}(x)) \text{det}[\frac{\partial \phi_t^{-1}}{\partial x}(x)],
\end{equation}
%
where $p_0 = q_0$.
\fi
Usually, the vector field $\rvv_{\theta,t}$ is trained to maximize the likelihood $p_1$ assigned to training data samples $\rvx_1$ from distribution $q_1$.
%
% Usually, the parameterized vector field $\rvv_{\theta,t}$ is optimized to ensure that the final distribution $p_1$ can maximize the likelihood of the data sample $\rvx_1$ from a data distribution $q_1$. 
This procedure is computationally expensive due to extensive ODE simulation for each parameter update.
%
% However, this procedure requires an extensive simulation of the ODE in each parameter update, making it computationally expensive.

\para{Flow Matching}~\cite{lipman2022flow} avoid the computationally expensive simulation process for training CNFs.
%
% In particular, we can define a conditional vector field $\rvu_t(\cdot|\rvx_1)$ and its conditional probability path $p_t(\cdot|\rvx_1)$, such that it transforms the Gaussian distribution $q_0$ into a Dirac delta distribution at $\rvx_1$, a training sample when $t=1$.
%
In particular, we define a conditional vector field $\rvu_t(\cdot|\rvx_1)$ and path $p_t(\cdot|\rvx_1)$ that transform $q_0$ into a Dirac delta at $\rvx_1$ at $t=1$.
% \citet{lipman2022flow} show that the vector field $\rvv_{\theta,t}$ can be learned via a simple conditional flow matching (CFM) objective:
\citet{lipman2022flow} show that $\rvv_{\theta,t}$ can be learned via a simple conditional flow matching (CFM) objective:
\begin{equation}
\label{eq:cfm_x0_x1}
\mathcal{L}_{\text{CFM}} = \mathbb{E}_{t,q_1(\rvx_1),q_0(\rvx_0)}||\rvv_{\theta,t}(\rvx_t) - \rvu_t(\rvx_t|\rvx_1)||^2.
\end{equation}
A common choice for the conditional vector field is $\rvu_t(\rvx|\rvx_1) := \rvx_1 - \rvx_0$, which can be easily simulated by linearly interpolating the data and Gaussian samples via $\rvx_t = (1 -t) * \rvx_0 + t \rvx_1$.

\iffalse
In particular, if the target probability path $p_t$ and the corresponding vector field $u_t$ are readily available, we can directly estimate the target vector field by utilizing the mean squared error (MSE) loss, referred to as the Flow Matching (FM) objective:
\begin{equation}
\label{eq:fm_loss}
\mathcal{L}_{\text{FM}} = \mathbb{E}_{t,p_t(x)}||v_{\theta, t}(x) - u_t(x)||^2
\end{equation}
\CL{In Eq.1, we have $v_{\theta, t}(\phi_t(x))$, here we have $v_{\theta, t}(x)$. Are they consistent? } \ednote{
% Thanks for reminder! We should add $\phi_t(x)$ to better align with it. 
I have double-checked the original paper, and this $x$ is a dummy variable and we might keep the it first.}
Typically, $(p_t, u_t)$ are not readily accessible in advance. To address this issue,~\cite{lipman2022flow} proposed a method to simplify the probability path by considering only a single data sample $x_1$.
%
In particular, we define a vector field $u_t(\cdot |x_1)$ in such a way that its corresponding diffeomorphic map $\phi_t(\cdot| x_1)$ creates a conditional probability path $p_t(\cdot|x_1)$. This path transforms the Gaussian distribution $q_0$ into a Dirac delta distribution at $x_1$ when $t=1$.
%
By marginalizing this conditional probability path across the data distribution $q_1$, we can obtain a marginal probability path:
\begin{equation}
p_t(x) = \int p_t(x|x_1) q_1(x_1) dx_1.
\end{equation}
As shown in ~\cite{lipman2022flow}, we can also obtain a marginal vector field $u_t(x)$ by marginalizing over $q_1$:
\begin{equation}
u_t(x) = \int u_t(x|x_1) \frac{p_t(x|x_1) q_1(x_1)}{p_t(x)} dx_1.
\end{equation}
%
Given that both ends of the distribution are known, we can explicitly formulate $u_t(\cdot | x_1)$ and optimize the Conditional Flow Matching (CFM) objective by taking an expectation over $x_0$ and $x_1$:
\begin{equation}
\label{eq:cfm_x0_x1}
\mathcal{L}_{\text{CFM}} = \mathbb{E}_{t,q_1(x_1),q_0(x_0)}||v_{\theta,t}(\phi_t(x_0|x_1)) - u_t(\phi_t(x_0|x_1)|x_1)||^2.
\end{equation}
Intuitively, this objective encourages the network to regress the optimal direction for moving an intermediate sample $x_t$,~\ie, $\phi_t(x_0|x_1)$, towards a data sample $x_1$.
In ~\cite{lipman2022flow}, it is proven that this objective shares the same gradient as the original Flow Matching objective (Equation~\ref{eq:fm_loss}) with respect to the marginal vector field.
This allows us to optimize the CNF without needing to access the marginal probability path or the marginal vector field.
% It is worth noting that by selecting an appropriate formulation of $u_t(\cdot | x_1)$ and $\phi_t(\cdot | x_1)$, it covers the probability paths used in recent diffusion models~\cite{ho2020denoising,song2019generative} as special cases.
In this work, we follow ~\cite{lipman2022flow} and choose $u_t(\cdot | x_1)$ and $\phi_t(\cdot | x_1)$ to be a linear interpolation between $x_0$ and $x_1$:
\begin{equation}
\label{eq:linear_flow}
x_t = \phi_t(x_0|x_1) = (1 -t) * x_0 + t x_1,
\end{equation}
\begin{equation}
u_t(\phi_t(x_0|x_1)|x_1) = x_1 - x_0.
\end{equation}
\fi




\para{Optimal Transport (OT) Map.}
% The training pair in Equation~\ref{eq:cfm_x0_x1}, the Gaussian noise $\rvx_0$ and the data point $\rvx_1$, used in the CFM objective are sampled based on an independent coupling, i.e., $q(\rvx_0, \rvx_1) = q_0(\rvx_0) q_1(\rvx_1)$.
In the CFM objective in Eq.~\ref{eq:cfm_x0_x1}, the training pair $(
\rvx_0, \rvx_1)$ is sampled from an independent coupling: $q(\rvx_0, \rvx_1) = q_0(\rvx_0) q_1(\rvx_1)$.
%
However,~\citet{tong2023improving,pooladian2023multisample} show that we can sample the training pair from any coupling that satisfies the marginal constraint: $\int q(\rvx_0, \rvx_1) d\rvx_1 = q_0(\rvx_0)$ and $\int q(\rvx_0, \rvx_1) d\rvx_0 = q_1(\rvx_1)$. 
% %
% As shown by~\citet{tong2023improving,pooladian2023multisample}, training pairs can be sampled from any coupling satisfying marginal constraints: $\int q(\rvx_0, \rvx_1) d\rvx_1 = q_0(\rvx_0)$ and $\int q(\rvx_0, \rvx_1) d\rvx_0 = q_1(\rvx_1)$.
%
% They also show that a good choice for data coupling is an optimal transport (OT) map $\pi$, which minimizes the transport distances $\int ||\rvx_0 - \rvx_1||^2 \pi(\rvx_0,\rvx_1) d\rvx_0 d\rvx_1$, and can result in a learned vector field that solves the OT problem and produces a much straighter trajectory.
%
They show an optimal transport (OT) map $\pi$ that minimizes $\int ||\rvx_0 - \rvx_1||^2 \pi(\rvx_0,\rvx_1) d\rvx_0 d\rvx_1$ is a good choice for data coupling, leading to a straighter trajectory.
%
Yet, obtaining the optimal transport map is often difficult for complex distributions.
%
%Various works ~\cite{song2024equivariant,klein2024equivariant,tong2023improving,pooladian2023multisample} have proposed different methods to approximate the OT map:
Next, we review two main directions for approximating the OT map:

\textbf{(i) Minibatch OT:} \citet{tong2023improving,pooladian2023multisample} approximate the actual OT by computing it at the batch level.
Specifically, they sample a batch of Gaussian noises $\{\rvx^1_0, \cdots, \rvx^B_0\} \sim q_0$ and data samples $\{\rvx^1_1, \cdots, \rvx^B_1\} \sim q_1$, where $B$ is the batch size.
% They then solve a discrete optimal transport problem, assigning each noise to a unique data sample while minimizing a cost function $C(\rvx_0, \rvx_1)$ between the noise and its assigned data sample. 
%
They solve a discrete optimal transport problem, assigning noises to data samples while minimizing a cost function $C(\rvx_0, \rvx_1)$.
%
The cost function is typically the squared-Euclidean distance, \ie, $C(\rvx_0, \rvx_1) = ||\rvx_0 - \rvx_1||^2$, and the assignment problem is often solved using the Hungarian algorithm~\cite{kuhn1955hungarian}.
%
% The cost function is typically squared-Euclidean distance: $C(\rvx_0, \rvx_1) = ||\rvx_0 - \rvx_1||^2$. The Hungarian algorithm solves the assignment problem~\cite{kuhn1955hungarian}.
% Once the assignment is computed, we can employ the assigned pairs to Equation~\ref{eq:cfm_x0_x1} for training the vector field network. 
%
After computing the assignment, we can use the assigned pairs to train the vector field network via Eq.~\ref{eq:cfm_x0_x1}.
%
As the batch size $B$ approaches infinity, this procedure converges to sampling from the true OT map.
    
\textbf{(ii) Equivariant OT Flow Matching:} \citet{song2024equivariant,klein2024equivariant} also approximate the OT at the batch level, but they focus on generating elements invariant to certain group $G$, such as permutations, rotations, and translations.
%
Specifically, they propose replacing the aforementioned cost function $C(\rvx_0, \rvx_1)$ with one that accounts for these group elements: $C(\rvx_0, \rvx_1) = \min_{g \in G}||\rvx_0 - \rvrho(g) \rvx_1||^2$, where $\rvrho(g)$ is the matrix representation of the group element $g$.
%
% They propose a new distance function that accounts for group elements: $C(\rvx_0, \rvx_1) = \min\limits_{g \in G}||\rvx_0 - \rvrho(g) \rvx_1||^2$, where $\rvrho(g)$ represents the group element $g$.
%
This approach significantly reduces the OT distance even with a small batch size, demonstrating success in generating molecular data.
%
Intuitively, using the cost function defined above allows us to align data and noise together (in our case via permutation) when computing the minibatch OT.

So far, we consider generic unconditional generative learning. 
It is worth noting that mini-batch OT does not easily extend to conditional generation problems, \ie, learning $p(\rvx|\rvy)$ for a generic input conditioning $\rvy$, when there is only one training sample $\rvx$ for each input conditioning $\rvy$.
%
This is because the OT assumes access to a batch of training samples for each 
%conditioning 
$\rvy$.


\if 0
\para{Joint CFM Training}
enhances the generation efficiency of the trained CNF by sampling better training pairs $x_0$ and $x_1$.
%
In Equation~\ref{eq:cfm_x0_x1}, we sample the data sample $x_1$ and the noise sample $x_0$ independently from $q_0(x_0)$ and $q_1(x_1)$, respectively.
%
However,~\cite{pooladian2023multisample,tong2023improving} suggest that, instead of an independent distribution, a suitable coupling $q(x_0, x_1)$ can be used to sample the training pairs, which can lead to a straighter trajectory and more efficient generation. 
%
In particular, an optimal $q(x_0, x_1)$ should solve the optimal transport problem between $q_0$ and $q_1$ and minimize the transport distance between them,~\ie, $\mathbb{E}_{q(x_0, x_1)}||x_0 - x_1||^2$.
%
To achieve this, ~\cite{pooladian2023multisample,tong2023improving} suggest obtaining samples $(x_0, x_1)$ from the optimal coupling through a mini-batch setting. 
%
In each training iteration, they propose sampling batches of data and noise,~\ie, $\{x_0^{(i)}\}^k_{i=1} \sim q_0$ and $\{x_1^{(i)}\}^k_{i=1} \sim q_1$. 
%
The training pairs are then obtained by solving the optimal transport problem between these two sample sets.
%
By replacing the independent sampling distribution with $q(x_0, x_1)$, we derive the Joint Conditional Flow Matching (JCFM) objective:
\begin{equation}
\label{eq:jcfm_x0_x1}
\mathcal{L}_{\text{JCFM}} = \mathbb{E}_{t,q(x_0, x_1)}||v_{\theta,t}(\phi_t(x_0|x_1)) - u_t(\phi_t(x_0|x_1)|x_1)||^2.
\end{equation}






the resulted trajectory $\phi_t$ from this sampling distribution unlikely to follow a straight path as multiple $x_0$ and $x_1$ pairs can produce a conditional path crossing a given $x$.
%
Instead, we can obtain a coupling distribution, \ie, $q(x_0, x_1)$,

where it comes with an initial condition $\phi_0(x) = x$, where $x \sim q_0(x)$.
Given a known distribution $q_0$, such as an unit Gaussian distribution, this map can progressively transform it to a more complicated distribution $p_t$ by varying the time.
%
This produces a probability path $p_t : \mathbb{R}^d \rightarrow \mathbb{R}_{> 0}$, defined by the inverse mapping $\phi_t^{-1}$:
\begin{equation}
\label{eq:push_forward}
    p_t(x) = p_0(\phi_t^{-1}(x)) \text{det}[\frac{\partial \phi_t^{-1}}{\partial x}(x)].
\end{equation}
%
The parameterized vector field $v$ is usually optimized such that the end distribution $p_1$ can maximize the likelihood of the data samples from a data distribution $q_1$.
%
However, application of the equation~\ref{eq:push_forward} or its variants~\cite{chen2018neural} for training will require a simulation of $\phi$ via an ODE solver, which is considered to be relatively expensive.

\paragraph{Conditional Flow Matching Objective.}
To avoid the expensive simulation process, one way to supervise the training is to directly supervise the vector field $v_\theta$ to be a direction to move an intermediate sample $x_t$ towards a data sample $x_1$.


To avoid the expensive simulation process, one ideal situation is that if the target probability path $p_t$ and the corresponding vector field $u_t$ are accessible, we can directly regress towards the target vector field using a MSE loss, also denoted as the Flow Matching (FM) objective:
\begin{equation}
\label{eq:fm_loss}
    \mathcal{L}_{\text{FM}} = \mathbb{E}_{t,p_t(x)}||v_t(x) - u_t(x)||^2
\end{equation}
%
where we assume that the target probability path satisfies the marginal constraints that $p_0 = q_0$ and $p_1 = q_1$.
Yet, as suggested in~\cite{lipman2022flow}, this solution is in general not feasible as we do not have access to $(p_t, u_t)$.

\paragraph{Conditional Flow Matching Objective.}
Instead of directly formulating the probability path for the whole data distribution $q_1$, we can formulate it via a mixture of simpler probability paths.
%
In particular, we consider a probability path $p_t(x|x_1)$ for each of the individual data sample $x_1$.
%
This path is selected in a way that $p_0(x|x_1) = q_0$ when $t = 0$, and $p_1(x|x_1) = \delta(x - x_1)$ when $t=1$.

Marginalizing this conditional probability paths over data distribution $q(x_1)$ produces the marginal probability path:
\begin{equation}
    p_t(x) = \int p_t(x|x_1) q(x_1) dx_1.
\end{equation}
As shown in~\cite{lipman2022flow}, this marginal probability path can be generated by a marginal vector field $u_t(x)$:
\begin{equation}
    u_t(x) = \int u_t(x|x_1) \frac{p_t(x|x_1) q(x_1)}{p_t(x)} dx_1.
\end{equation}

Further, it is shown that we can have an objective function (denoted as CFM objective) that can have same gradient with the Flow Matching objective (Equation~\ref{eq:fm_loss}) if with respect to network parameters:
\begin{equation}
\label{eq:cfm_loss}
     \mathcal{L}_{\text{CFM}} = \mathbb{E}_{t,q(x_1), p_t(x|x_1)}||v_t(x) - u_t(x|x_1)||^2.
\end{equation}
Assuming the conditional probability path defines a flow $\phi_t(x|x_1)$, the above objective can also be parameterized by samples from $q_1(x_1)$ and $q_0(x_0)$:
\begin{equation}
\label{eq:cfm_x0_x1}
    \mathcal{L}_{\text{CFM}} = \mathbb{E}_{t,q_1(x_1),q_0(x_0)}||v_t(\phi_t(x_0|x_1)) - u_t(\phi_t(x_0|x_1)|x_1)||^2
\end{equation}
% there could be multiple probability paths that can satisfy $p_1 \approx q_1$.
%
% Further, we do not have a closed form solution of $u_t$ such that it can generate the desired $p_t$.

\paragraph{Optimal Transport Conditional Probability.}
In particular,~\cite{lipman2022flow} considers a conditional probability path with the form of:
\begin{equation}
\label{eq:gaussian_cond}
    p_t(x|x_1) = \mathcal{N}(x|\mu_t(x_1), \sigma_t(x_1)^2 I),
\end{equation}
where $\mu : [0, 1] \times \mathbb{R}^d \rightarrow \mathbb{R}^d$ is the mean of the Gaussian distribution, and $\sigma : [0, 1] \times \mathbb{R}^d \rightarrow \mathbb{R}^d_{> 0}$ is a scalar of the standard derivation for the distribution.
%
By varying the choices of $\mu_t$ and $\sigma_t$, we can obtain various probability paths, including those adopted in diffusion model (Variance Preserving Formulation (VP)) and score matching model (Variance Exploding Formulation (VE)).
%
In~\cite{lipman2022flow}, they introduce a formulation to define the linear path in time:
\begin{equation}
\label{eq:vector_field}
    \mu_t(x) = t x_1, \text{and}, \sigma_t(x) = 1 - t.
\end{equation}
The corresponding flow model $x_t = \phi_t(x_0|x_1)$ is formulated as:
\begin{equation}
\label{eq:linear_flow}
    \phi_t(x_0|x_1) = (1 -t) * x_0 + t x_1
\end{equation}
% By substituting the values into the equations~\ref{eq:u_t}, we can obtain the vector field parameterized by samples $x_0$ and $x$:
% \begin{equation}
% \label{eq:linear_u_t}
%     u_t(x|x_1) = \frac{x_1 - (1 - \sigma_{\text{min}}) x}{1 - (1 - \sigma_{\text{min}}) t},
% \end{equation}
Under this setting, we can derive the vector field parameterized by $x_0$ and $x_1$:
\begin{equation}
    u_t(\phi_t(x_0|x_1)|x_1) = x_1 - x_0,
\end{equation}
which can form our training objective (CFM) by substituting into the equation~\ref{eq:cfm_x0_x1}.

\fi