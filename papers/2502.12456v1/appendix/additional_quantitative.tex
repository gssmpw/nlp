\rebuttal{
\section{Additional Quantitative Comparison}
\input{appendix/figures/main_quantitative_comp_cov}
\input{appendix/figures/external_quantitative_comp_cov}
\input{appendix/tables/quantitative_final}
\subsection{Coverage Metric (COV)}
Following~\citet{zhou2021pvd,zeng2022lion}, we also evaluate coverage (COV), a metric that measures the diversity of generated 3D shapes. 
%
COV calculates the proportion of testing shapes that can be retrieved by generated shapes, with higher values indicating higher diversity. However,~\citet{yang2019pointflow,zhou2021pvd,zeng2022lion,wu2023psf} have noted that this metric is not robust as training set shapes can have worse COV than generated results. 
%
Moreover,~\citet{yang2019pointflow} suggest that perfect coverage scores are possible even when distances between generated and testing point clouds are arbitrarily large. 
%
Given these limitations, COV should be considered only as a reference metric, while 1-NNA provides a more reliable measure that captures both generation quality and diversity.

\para{Evaluation Results.} 
We present quantitative comparisons with different baselines in Figure~\ref{fig:main_quantitative_comp_cov} and~\ref{fig:external_quantitative_comp_cov}. 
%
Our approach generates shapes with reasonable diversity even with a limited number of steps (10-20), demonstrating the framework's effectiveness.
%
With sufficient inference steps (100), our approach achieves comparable performance to other baselines.
%
However, we observe contradictory conclusions between 1-NNA and COV metrics (as shown by simultaneously high 1-NNA and COV scores for Equivariant OT in the Airplane category), which aligns with the previously discussed limitations of the COV metric.

\subsection{More Inference Steps}
In addition to the baseline comparisons presented in the main paper (Figures~\ref{fig:external_quantitative_comp} and~\ref{fig:external_qualitative_comp}), we provide additional comparisons with 1000 inference steps, matching the original settings used by the baseline methods.
%
We employ the DDPM sampler~\cite{ho2020denoising} rather than the DDIM sampler~\cite{song2019generative} in this experiment and refer to the original values of PVD and LION reported in~\cite{zeng2022lion}.

\para{Evaluation Results.} 
We present the evaluation results based on 1-NNA-CD and 1-NNA-EMD in Table~\ref{tab:quantitative_final}. 
%
Our method achieves comparable performance with PVD~\cite{zhou2021pvd}, which also directly generates point clouds. 
%
When compared to LION~\cite{zeng2022lion}, which generates latent point cloud representations, our framework performs slightly worse.
%
Note it is known that at a high sampling budget, SDE-based samplers often outperform ODE-based samples (see~\cite{karras2022elucidating} and~\cite{xu2023restart}).

}