\section{Approximated OT}
\label{subsec:approx_details}
% - Why need Approximated OT
% - How to use OT as loss function
% - Introducing entropy-based OT (easier optimization)
% - debias sinkhorn divergence
% - optimization procedure
% - alternative tested

\subsection{Implementation Details}
In the following, we outline the procedure for the approximated Optimal Transport (OT) employed for supersets with size $M > 10,000$.
%
We are motivated to use an approximated OT because the computational time for the exact method, \ie, the Hungarian algorithm~\cite{kuhn1955hungarian}, is prohibitively large for a large superset size.
%
Given supersets of size 10,000, it already takes 220 seconds.
%
Considering the complexity of $O(M^3)$, this is unlikely to be scalable for larger set sizes, e.g., $M = 100,000$, for the thousands of training shapes in each ShapeNet category, even if we move this computation offline.
%
Therefore, we resort to an approximation procedure that utilizes Wasserstein gradient flow to obtain our point superset $X_1$ by progressively transporting a noise $X_0$.

\para{OT Distances.}
To enable an optimization procedure to move the points, we need to define an objective to be optimized, which is the 2-Wasserstein distance in our case.
%
In particular, the OT distance can be defined as:
\begin{equation}
    W(q_0, q_1) := \min_{q \in \Gamma(q_0, q_1)} \int C(x_0, x_1) q(x_0, x_1) dx_0 dx_1,
\end{equation}
%
where $\Gamma(q_0, q_1)$ represents all couplings with marginals $q_0$ and $q_1$.
%
However, explicitly computing the OT is not tractable, so a relaxation of this cost is introduced by adding an entropic barrier term weighted by $\epsilon$ for solving various problem, including~\cite{leonard2012schrodinger}.
%
This relaxation enables an efficient solution by employing the Sinkhorn algorithm, which can be highly parallelized on the GPU.
%
As shown in~\cite{feydy2019interpolating}, this entropic barrier introduces bias in measuring the distance. Even when the marginals in the above equation are equal, i.e., $q_0 = q_1$, the distance might not be zero, leading to poor gradients.
%
To address this issue, ~\citet{genevay2018learning} introduce correction terms to the objective, forming the Sinkhorn divergence.
%
Additionally, ~\citet{feydy2019interpolating} provide an efficient, memory-saving GPU implementation that scales to millions of samples.

\para{Optimization Procedure.}
With the defined objective, our optimization will compute the gradient with respect to this objective to update the current point samples.
%
Initially, we initialize the coordinates of a set of points as the noise superset $x_0$.
%
At each optimization iteration, we compute the Sinkhorn divergence between the current point set and the target superset $X_1'$.
%
By repeatedly minimizing the OT distance in the form of Sinkhorn divergence, we obtain a deformed point set that fits the given superset well.
%
Finally, we use this deformed point set as our point superset to represent the shape $S$.

% demonstrating that it could be hard to scale up the OT computation even moving it offline.
% %
% Based on this, we will first cover some backgrounds on the formulation of the OT, which motivate one relaxation of the OT (entropic regularization) so that it could be computed efficiently.
% %
% We then include a discussion for the improved version of the entropic regularization (sinkhorn divergence) process a better gradient flow.
% %
% Lastly, employ this objective in procedure to compute Wasserstein gradient flow to obtain an OT map.

% \para{Analysis of Exact OT.}
Note that using this computed superset as $X_1$ introduces approximation errors compared to true samples $X_1'$.
%
However, we empirically observe that the procedure usually converges well with only small errors, and in our experiments, the large superset size typically outweighs the introduced error (as shown in Section~\ref{subsec:model_analysis}).


\input{appendix/figures/main_quantitative_comp_gradient}
\rebuttal{
\subsection{Training pairs by 1-step Approximation}
We also compare our approach with training pairs obtained through the optimization procedure described above.
%
In this experiment, we perform only a single optimization iteration on point clouds and noises of size $2,048$ and set $\beta$ to 0.0 .
%
For each training iteration, we use the optimized results, $X_0$ and $X_1$, as our training pair.
%
The quantitative comparison results are shown in Figure~\ref{fig:main_quantitative_comp_gradient}.
%
Our approach consistently outperforms the 1-step Wasserstein Gradient Flow across all categories and most inference timesteps.
}
