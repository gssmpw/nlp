\newtheorem{theorem}{Theorem}
\subsubsection{Proof of Our OT Approximation}
\label{subsec:our_ot_proof}

We first give a definition of coupling $q(x_0, x_1)$ in our case before showing its marginal fullfils the marginal requirements.
%
In particular, we denote $x_0 \in R^{N \times 3}$ and $x_1 \in R^{N \times 3}$ as two random variables following the distributions, $q_0(x_0)$ and $q_1(x_1)$, respectively.
%
It is noted that $q_0 := N(0, I)$, which is the standard Gaussian for each dimension in $x_0$, and $q_1(x_1)$ is the distribution all possible point clouds, which involves the joint modeling of point cloud distribution given a shape $S$ ($q_{1}(x_1|S)$) and the distribution of shape ($q(S)$), \ie, $q_1(x_1) = \int q_{1}(x_1|S) q(S) dS$.
%

We can notice that each row in $x_0$ is independently and identically distributed (IID), \ie, $q_0(x_0) = \prod_{i}^N \hat{q_0}(x_0^i)$, where we denote the $i$-th row of $x_0$ as $x_0^i$ and distribution of $x_0^i$ as $\hat{q_0}(x_0^i)$, which is 3-dimensional unit Gaussian.
%
We can also assume each point in $x_1$ is IID given a shape, \ie, $q_{1}(x_1 | S) = \prod_{i}^N \hat{q_{1}}(x_1^i|S)$,  where we denote the $i$-th row of $x_1$ as $x^i_1$ and the distribution of $x^i_1$ as $\hat{q_{1}}(x_1^i|S)$. 

In our superset OT precomputation for a given shape $S$, we pre-sample a set of random variables $(x^1_0 \cdots, x^j_0, \cdots, x^M_0) \sim \hat{q}_0$, and a set of random variables  $(x^1_1, \cdots, x^k_1,\cdots, x^M_1) \sim \hat{q}_1$, and have a precomputed bijective mapping $\Pi : \{1, \cdots, M\} \leftrightarrow \{1, \cdots, M\}$.
%
With these defined, our coupling $\hat{q}(x^i_0, x^i_1 |S)$ for one row in the training pair $(x^i_0, x^i_1)$ given $S$ can be formulated as:
\[
    \hat{q}(x^i_0 = x^j_0, x^i_1 = x^k_1 | S) =
    \begin{cases}
    \frac{1}{n}, & \text{if } j = \Pi(k) \\ 
        0, & \text{else } j \neq \Pi(k);
    \end{cases}
\]
%
Since the each row in the training pairs are independently subsampled, the coupling of the training pair $(x_0, x_1)$ given a shape is defined as $q(x_0, x_1 |S) = \prod_{i}^N \hat{q}(x_0^i, x_1^i | S)$.
%
In the end, the coupling over all training pairs can be obtained by marginalize over all possible shapes, \ie, $\int q(x_0, x_1 | S) q(S) dS$.

\begin{theorem}

% Our coupling $q(x_0, x_1)$ for a given Gaussian noise $x_0 \in R^{N \times 3}$ and a given point cloud $x_1 \in R^{N \times 3}$

Our coupling without blending converge the following marginal if the superset size $M$ is sufficiently large:
\begin{equation}\label{eq:mariginals}
    \int q(\rvx_0, \rvx_1) d\rvx_1 = q_0(\rvx_0), \int q(\rvx_0, \rvx_1) d\rvx_0 = q_1(\rvx_1).
\end{equation}
\end{theorem}

Proof:
We first show the left constraint:
% \begin{equation}
\begin{align}
LHS & = \int q(x_0, x_1) dx_1 = \int \int q(x_0, x_1 | S) q(S) dS dx_1  \\
& = \int q(S) \int q(x_0, x_1 | S) dx_1 dS && \text{change the order of integration} \\
& = \int q(S) \int \prod_i^N \hat{q}(x_0^i, x_1^i|S) d(x_1^1, \cdots, x_1^N) dS  && \text{independent assumption of each row in training pair}\\
& = \int q(S) \prod_i^N \int \hat{q}(x_0^i, x_1^i|S) dx_1^j dS && \text{integrals of independent products}\\
& = \int q(S) \prod_i \sum_k^M \hat{q}(x_0^i, x_1^k|S) dS && \text{restricting to discrete values in supersets}\\
& = \int q(S) \prod_i \hat{q}_0(x^i_0) dS && \text{Proposition~\ref{prop:ot}}\\
& = \int q(S) q_0(x_0) dS = q_0(x_0) && \text{independent assumption of each row in Gaussian noises} \\
\end{align}
% \end{equation}

Similarly, we perform the same computation on the right constraint:
% \begin{equation}
\begin{align}
LHS & = \int q(x_0, x_1) dx_0 = \int \int q(x_0, x_1 | S) q(S) dS dx_0   \\
 & = \int q(S) \int q(x_0, x_1 | S) dx_0 dS && \text{change the order of integration} \\
& = \int q(S) \int \prod_i^N \hat{q}(x_0^i, x_1^i|S) d(x_0^1, \cdots, x_0^N) dS && \text{independent assumption of each row in training pair} \\
& = \int q(S) \prod_i^N \int \hat{q}(x_0^i, x_1^i|S) dx_0^i dS  && \text{integrals of independent products} \\
& = \int q(S) \prod_i \sum_j^M \hat{q}(x_0^j, x_1^i|S) dS 
 && \text{restricting to discrete values in supersets} \\
& = \int q(S) \prod_i \hat{q}_1(x^i_1 | S) dS  && \text{Proposition~\ref{prop:ot}} \\
& = \int q(S) q_1(x_1 | S) dS = q_1(x_1) && \text{independent assumption of each row in point cloud} \\
\end{align}
% \end{equation}


% We first consider the RHS of Left Constraints (Equation~\ref{eq:mariginals}), we can reformulate it as follows:
% \begin{equation}
%     \begin{split}
%             RHS = q_0(x_0) = \int q(S) q_0(x_0 | S) dS = \int q(S) q_0(x_0) dS \\
%             % = \int q_0(x_0) (\int q_1(x_1 |S) q(S) dS) dx_1 \text{, by Lemma~\ref{lemma:joint}} \\
%             % = \int q(S) \int q_0(x_0) q_1(x_1|S) dx_1 d_S \text{, by rearranging the integrals} \\
%     \end{split}
% \end{equation}
% Considering LHS:
% \begin{equation}
%     \begin{split}
%         LHS = \int q(x_0, x_1) dx_1 = \int \int q(x_0, x_1 | S) q(S) dS dx_1 \\
%         = \int q(S) \int q(x_0, x_1 | S) dx_1 dS
%     \end{split}
% \end{equation}

% By comparing LHS and RHS, it is sufficient to show that $\int q(x_0, x_1 |S) dx_1 = q_0(x_0)$ for the first constraint.
% Similarly, for the second constraint RHS:
% \begin{equation}
%     \begin{split}
%             RHS = q_1(x_1) = \int q(S) q_1(x_1|S) dS \\
%             % = \int q_0(x_0) (\int q_1(x_1 |S) q(S) dS) dx_1 \text{, by Lemma~\ref{lemma:joint}} \\
%             % = \int q(S) \int q_0(x_0) q_1(x_1|S) dx_1 d_S \text{, by rearranging the integrals} \\
%     \end{split}
% \end{equation}
% Considering LHS:
% \begin{equation}
%     \begin{split}
%         LHS = \int q(x_0, x_1) dx_0 = \int \int q(x_0, x_1 | S) q(S) dS dx_0 \\
%         = \int q(S) \int q(x_0, x_1 | S) dx_0 dS
%     \end{split}
% \end{equation}
% Then it is sufficient to show $\int q(x_0, x_1 | S) dx_0 = q_1(x_1|S) $.

% To show first equation, we can apply Lemma~\ref{lemma:independent}:
% \begin{equation}
% \label{eq:left_LHS}
%     \begin{split}
%         LHS = \int q(x_0, x_1 | S) dx_1 = \int \prod_i q(x_0^i, x_1^i | S) d(x_1^i, \cdots, x_1^N) \\
%         = \prod_i \int q(x_0^i, x_1^i|S) dx_1^i 
%     \end{split}
% \end{equation}

% \begin{equation}
% \label{eq:left_RHS}
%     RHS = q_0(x_0) = \prod_i q_0(x^i_0)
% \end{equation}
% By this computation, we are also sufficient to show $\int q(x_0^i, x_1^i | S) dx_1^i = q_0(x_0^i)$ and by similar computation:
% \begin{equation}
% \label{eq:right_LHS}
%     \begin{split}
%         LHS = \int q(x_0, x_1 | S) dx_0 = \int \prod_i q(x_0^i, x_1^i | S) d(x_0^i, \cdots, x_0^N) \\
%         = \prod_i \int q(x_0^i, x_1^i|S) dx_0^i 
%     \end{split}
% \end{equation}

% \begin{equation}
% \label{eq:right_RHS}
%     RHS = q_1(x_0|S) = \prod_i q_1(x^i_1|S)
% \end{equation}
% Therefore, we are sufficient to show $\int q(x^i_0, x^i_1) dx_0^i = q_1(x_1^i |S)$.

% By considering the fact that, we pre-sample a set of random variables $(x^1_0 \cdots, x^j_0, \cdots, x^M_0) \sim q_0$, and a set of random variables  $(x^1_1, \cdots, x^k_1,\cdots, x^M_1) \sim q_{1|S}$, and have a precomputed bijective mapping $\Pi : \{1, \cdots, M\} \leftrightarrow \{1, \cdots, M\}$.
% %
% With these defined, our coupling $q(x^i_0, x^i_1 |S)$ given $S$ can formulated as:
% \[
%     P(x^i_0 = x^j_0, x^i_1 = x^k_1) =
%     \begin{cases}
%     \frac{1}{n}, & \text{if } k = \Pi(j) \\ 
%         0, & \text{else } k \neq \Pi(j);
%     \end{cases}
% \]
% By Proposition~\ref{prop:ot}, if the superset size $M$ is large enough, we can show that the CDF of Equation~\ref{eq:left_LHS} converge to Equation~\ref{eq:left_RHS}, also the CDF of Equation~\ref{eq:left_LHS} converges to Equation~\ref{eq:left_RHS}.

% To show our coupling maintain the correct marginal, we assume we have $M$ random variables $(X_1, \cdots, X_M) \sim p_1$, and another $M$ random random variables $(Y_1, \cdots, Y_M) \sim p_2$.
% %
% We can additionally take an arbitrary bijective map $\Pi$ between random variables, \ie, $\Pi : \{1, \cdots, M\} \leftrightarrow \{1, \cdots, M\}$.
% %
% If we only sample the a pair of variables based on the bijective map, we can then construct a new random Variable $Z: \{X, Y\}$:
% \[
%     P(X = X_i, Y = Y_j) =
%     \begin{cases}
%     \frac{1}{M}, & \text{if } j = \Pi(i) \\ 
%         0, & \text{else } j \neq \Pi(i);
%     \end{cases}
% \]

% Since $\Pi$ is a bijective mapping, the mariginal distribution of $P(X = X_i)$ and $P(Y = Y_j)$ is also $\frac{1}{M}$.
% %
% Following the result in the previous section, we can show the random variable $X$ ($Y$) still follows $p_1$ ($p_2$).
% %
% In our case, we consider $p_1$ to be a 3D Gaussian distribution, and $p_2$ to be point sample distribution given a Shape $\mathcal{S}$.

% The last part we need to show is that $q_0(x_0)$ and $q_1(x_1|\mathcal{S})$ is independently sampled for each of the point, \ie, $q_0(x_0) = \prod_{i} q_0(x_0^i)$ and \ie, $q_1(x_1) = \prod_{i} q_1(x_1^i | \mathcal{S})$, where $x_0^i$ and $x_1^i$ is the $i$-th point in $x_0$ and $x_1$, respectively.
% %
% For Gaussian distribution $q_0(x_0)$, this is true because it is an unit Gaussian distribution.
% %
% For surface point distribution $q_1(x_1|S)$, it is also correct since the points are indepdently sampled.



% Additionally, for a Gaussian noise sets arranged in the matrix format, $x_0 \in \mathbb{R}^{N \times 3}, x_0 \sim$