\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\subsubsection{Law of Large Numbers}


\begin{proposition}\label{prop:large_samples}
Given $(X_1, \cdots, X_n)$, which are independently and identically distributed (IID) real $d$-diemsnion random variables, following a probability distribution $p(X)$,~\ie, $X_i \sim p(X), X \in \mathbb{R}^d$.
%
We have an additional random variable $Y$ that is random uniform sample of these variables,~\ie, $P(Y = X_i) = \frac{1}{n}$.
%
The cumulative distribution function (CDF) $\bar{F}(t)$ of random variable $Y$ will converge to the $F(X)$,~\ie, CDF of $X$.
\end{proposition}



% Assume $(X_1, \cdots, X_n)$ are independently and identically distributed (IID) real $d$-diemsnion random variables following a probability distribution $p(X)$, \ie, $X_i  \sim p(X), X \in \mathbb{R}^d$.
% %
% We also denote the cumulative distribution function of $p(X)$ to be $F(x)$.
%
Proof:
We first define an empirical cumulative distribution function $\hat{F}_n(X)$ over the random variables $(X_1, \cdots, X_n)$:
\begin{equation}
    \hat{F}_n (t) = \frac{1}{n} \sum_{i=1}^{n} \mathbf{1}_{X_i \leq t},
\end{equation}
where $\mathbf{1}_{X_i \leq t}$ is an indicator for $X_i^d \leq t^d$ for all dimensions $\{1, \cdots, d\}$.

The Glivenkoâ€“Cantelli theorem states that this empirical distribution function $\hat{F}_n(X)$ will converge to the cumulative distribution $F(X)$ if $n$ is sufficiently large:
\begin{equation}
    \textbf{sup}_{t \in \mathbb{R}^d} | \hat{F}_n(t) - F(t) | \rightarrow 0.
\end{equation}

If we have an additional random variable $Y$ that its value is a random subsample of the variables $(X_1, \cdots, X_n)$:
\begin{equation}
    P(Y = X_i) = \frac{1}{n}, \forall i = 1, 2, \cdots, n.
\end{equation}

The CDF of this variable $\bar{F}(t)$ is:
\begin{equation}
    \bar{F}(t) = P(Y \leq t) = \sum_{i=1}^{n} P(Y = X_i) \cdot \mathbf{1}_{X_i \leq t} = \frac{1}{n} \sum_{i=1}^{n} \mathbf{1}_{X_i \leq t} = \hat{F}_n(t).
\end{equation}
Therefore, the CDF of $Y$ also converges to the original underlying CDF $F(t)$ if $n$ is sufficiently large.

\begin{proposition}\label{prop:ot}
Assume we have $n$ random samples $(X_1, \cdots, X_n) \sim p_1$, and another $n$ random samples $(Y_1, \cdots, Y_n) \sim p_2$, and we are also given an arbitrary bijective map between random variables, \ie, $\Pi: \{1, \cdots, n\} \leftrightarrow \{1, \cdots, n\}$.
%
If we construct a new random variable $Z : (X, Y)$ follows the following couplings:

\[
    P(X = X_i, Y = Y_j) =
    \begin{cases}
    \frac{1}{n}, & \text{if } j = \Pi(i) \\ 
        0, & \text{else } j \neq \Pi(i);
    \end{cases}
\]

The CDF of the marginal $P(X)$ will converge the CDF of $p_1$, while the CDF of the marginal $P(Y)$ will converge to the CDF of $p_2$.
\end{proposition}

Proof:
Since $\Pi$ is bijective, we can compute the marginal $P(X = X_i)$ directly:
\begin{equation}
    \begin{split}
            P(X = X_i) = \sum_{j=1}^{n} P(X = X_i, Y = Y_j) \\
            = P(X = X_i, Y = Y_{\Pi(i)}) + \sum_{j \neq \Pi(i)} P(X = X_i, Y = Y_j) \\
            = \frac{1}{n} + 0 = \frac{1}{n}
    \end{split}
\end{equation}

Similarly, we can show the marginal of P(Y) is also $\frac{1}{n}$.
%
By leveraging Proposition~\ref{prop:large_samples}, we show that $P(X)$ will converge the CDF of $p_1$, and the CDF of $P(Y)$ will converge to the CDF of $p_2$.

% \begin{lemma}\label{lemma:independent}
% The Gaussian noises $x_0$ are independently and identically distributed (IID), \ie, $q_0(x_0) = \prod_{i}^N q_0(x_0^i)$, where $x^i_0$ is the $i$-th noises and $x^i_0 \sim q_0$ .
% %
% Also, the point cloud $x_1$ given a 3D shape $S$ is also independently and identically distributed (IID), \ie, $q_1(x_1|S) = \prod_{i}^N q_1(x_1^i | S)$, where $x^i_1$ is the $i$-th point and $x^i_0 \sim q_{1|S}$.
% %
% Lastly, the training pair $(x_0, x_1)$ from our coupling  given a shape $S$ is also independently and identically distributed (IID), \ie, $q(x_0, x_1 | S) = \prod_{i}^N q(x_0^i, x_1^i | S)$, where $(x_0^i, x_1^i$) is the $i$-th pair in the training pair.
% \end{lemma}

% \begin{lemma}\label{lemma:joint}
%     The sample distribution of a point $x_1^i$ involves modeling of underlying shape $S$ and the modeling of the point distribution given $S$, \ie, $q_1(x_1^i) = \int q_1(x_1^i | S) q(S) dS$.
%     %
%     However, the distribution of noises $q_0(x^i_0)$ is unrelated to a given shape $S$, \ie, $q_0(x^i_0 | S) = q_0(x^i_0)$.
% \end{lemma}

% By considering the $p(X)$ be a Gaussian distribution $N(0, I)$ or a sampling distribution of 3D points given a Shape $\mathcal{S}$, \ie, $q(x|\mathcal{S})$, we can show the random sample $Y$ still follows the original distribution.

% If we consider $M$ random variables, where each of them is an 3D Gaussian noise, denoted as $\epsilon_i \sim N(0, I), \epsilon_i \in \mathbb{R}^3$.
% %
% We also define another variable $\epsilon$ is a random sample of these random variables, \ie, $P(\epsilon = \epsilon_i) = \frac{1}{M}$.
% %
% Since each dimension in $\epsilon$ is independent, CDF of $\epsilon^j$ will follows the by leverage the above results, where $j$ is the j-th dimension of the noise.
% We consider a dense 3D Gaussian noises with $M \times 3$ random variables, $\{x_1, y_1, z_1, \cdots, x_M, y_M, z_M\}$, where we denote $x_i$, $y_i$, and $z_i$ to be the coordinates of in x, y, and z dimensions, respectively and $x_i, y_i, z_i \sim N(0, I)$.
% %
% If we can consider a random variable $\hat{x}$, which is random sample of this dense gausian in x dimension, \ie,  P$(\hat{x} = x_i) = \frac{1}{M}$.
% %
% By the above results, the CDF follows the original distribution, which is the Gaussian distribution $N(0, I)$.
% %
% By considering also y and z dimension, we can show that a random sampling of Gaussian point converge to Gaussian distribution.