\section{Related work}
\label{sec:related}
% \vspace{-2mm}
\vspace{-0.12in}

\looseness-1Reasoning is recognized as a core ability for intelligent and robustly model and has thus seen significant focus over the last few years. The synthetic reasoning problems we consider in this work have all been used in the prior works of \cite{sanford2024transformers,ye2024physics,sanford2024understanding,nogueira2021investigating} to theoretically analyze the strengths and limitations of Transformers.
There is also interest in the representation power of Transformers for computational problems \citep{liu2022transformers,strobl2023transformers} and for chain-of-thought reasoning \citep{merrill2023expresssive,feng2023towards,li2024chain}.
The necessity of model depth for performance has been remarked upon, for small models~\citep{liu2024mobilellm} and reasoning \citep{chen2024can,ye2024physics,petty2023impact}. In this work, we make a finer observation that albeit larger depth is necessary, this can be achieved with a limited parameter budget via looping. 

Looping in transformer models has been studied since the works \citep{dehghani2018universal,lan2019albert} where they showed the benefits of looping for supervised learning tasks and BERT pretraining respectively. 
Looping also appears in \citep{schwarzschild2021can,bansal2022end} that study the extrapolation properties of looping for certain algorithmic tasks.
More recently \citet{giannou2023looped,de2024simulation} have studied the theoretical properties of looped decoder models and show that looping can simulate arbitrary Turing machines. 
In addition \cite{yang2023looped,gao2024expressive,gatmiry2024can,gatmiry2024role} study looped models as a way to simulate iterative algorithms for in-context learning.
Recently, \citet{mohtashami2023cotformer} introduced CoTFormer, which tries to improve the perplexity of looped language models and \citep{liu2024mobilellm} explore latency-efficiency parameter sharing for on-device LLMs.
% \ns{Include something about CoTFormer \citep{mohtashami2023cotformer}
In contrast, our work focuses on the surprising inductive bias of looping to improve downstream reasoning tasks, and goes beyond algorithmic and in-context learning tasks.

\looseness-1Different training algorithms (e.g. gradient descent \citep{soudry2018implicit}) and architectural choices (e.g. attention \citep{edelman2022inductive}) have been shown to have certain implicit biases. There is increasing interest in such inductive biases during pretraining \citep{saunshi22understanding,liu2023same}. More recently, \citet{saunshi2024inductive} showed an inductive bias of stacking \citep{reddi2023efficient} towards improving reasoning and hypothesize that a connection of stacking to looped models could be responsible for this. Our results provide further verification for this hypothesis.


\vspace{-0.1in}
% ---------------------------

% \textbf{Scaling laws} \citet{liu2024mobilellm} also suggest that depth is especially important for small models. \nishanth{done. see above}

% \textbf{Papers on looping and how we differ}: Albert \citep{lan2019albert}, Universal Transformer \citep{dehghani2018universal}, programmable computers \citep{giannou2023looped}, ICL papers \citep{yang2023looped,gatmiry2024can}, CoTFormer \citep{mohtashami2023cotformer}, AlgoFormer \citep{gao2024expressive}.
% The focus of our work is not to compare with these, but point out the role of looping for reasoning. \nishanth{need to talk about CoTformer and Algoformer, the rest covered above}

% \textbf{Reasoning tasks}: addition, i-gsm \citep{ye2024physics}, shortcuts to automata \cite{liu2022transformers}, $p$-hop \cite{sanford2024transformers}. graph algorithms with transformers \citep{sanford2024understanding}. paper showing that given a fixed parameter count, better to distribute them across depth axis than width axis for compositional generalization tasks (which look similar to reasoning tasks) \citep{petty2023impact}. \nishanth{done, see above}

% \textbf{Papers on theoretical results}: expressivity of Transformers, turing-completeness, CoT, graph algorithms with looping \citep{back2024simulation}
% \ns{@Zhiyuan: please help with this},
% \ns{graph algorithm paper modifies architecture.
% also our theory cares about the role of depth and optimality as opposed to Turing completeness}

% \textbf{Papers on inductive bias}: CL \citep{saunshi22understanding}, Tengyu's work \citep{liu2023same}, Stacking paper \citep{saunshi2024inductive}, inductive bias of attention paper, gradient descent papers
% \ns{@Nikunj: please help with this}