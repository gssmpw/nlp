[
  {
    "index": 0,
    "papers": [
      {
        "key": "sanford2024transformers",
        "author": "Sanford, Clayton and Hsu, Daniel and Telgarsky, Matus",
        "title": "Transformers, parallel computation, and logarithmic depth"
      },
      {
        "key": "ye2024physics",
        "author": "Ye, Tian and Xu, Zicheng and Li, Yuanzhi and Allen-Zhu, Zeyuan",
        "title": "Physics of language models: Part 2.1, grade-school math and the hidden reasoning process"
      },
      {
        "key": "sanford2024understanding",
        "author": "Sanford, Clayton and Fatemi, Bahare and Hall, Ethan and Tsitsulin, Anton and Kazemi, Mehran and Halcrow, Jonathan and Perozzi, Bryan and Mirrokni, Vahab",
        "title": "Understanding transformer reasoning capabilities via graph algorithms"
      },
      {
        "key": "nogueira2021investigating",
        "author": "Nogueira, Rodrigo and Jiang, Zhiying and Lin, Jimmy",
        "title": "Investigating the limitations of transformers with simple arithmetic tasks"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "liu2022transformers",
        "author": "Liu, Bingbin and Ash, Jordan T and Goel, Surbhi and Krishnamurthy, Akshay and Zhang, Cyril",
        "title": "Transformers learn shortcuts to automata"
      },
      {
        "key": "strobl2023transformers",
        "author": "Strobl, Lena and Merrill, William and Weiss, Gail and Chiang, David and Angluin, Dana",
        "title": "Transformers as recognizers of formal languages: A survey on expressivity"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "merrill2023expresssive",
        "author": "Merrill, William and Sabharwal, Ashish",
        "title": "The expresssive power of transformers with chain of thought"
      },
      {
        "key": "feng2023towards",
        "author": "Feng, Guhao and Gu, Yuntian and Zhang, Bohang and Ye, Haotian and He, Di and Wang, Liwei",
        "title": "Towards Revealing the Mystery behind Chain of Thought: a Theoretical Perspective"
      },
      {
        "key": "li2024chain",
        "author": "Zhiyuan Li and Hong Liu and Denny Zhou and Tengyu Ma",
        "title": "Chain of Thought Empowers Transformers to Solve Inherently Serial Problems"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "liu2024mobilellm",
        "author": "Liu, Zechun and Zhao, Changsheng and Iandola, Forrest and Lai, Chen and Tian, Yuandong and Fedorov, Igor and Xiong, Yunyang and Chang, Ernie and Shi, Yangyang and Krishnamoorthi, Raghuraman and others",
        "title": "Mobilellm: Optimizing sub-billion parameter language models for on-device use cases"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "chen2024can",
        "author": "Chen, Xingwu and Zou, Difan",
        "title": "What Can Transformer Learn with Varying Depth? Case Studies on Sequence Learning Tasks"
      },
      {
        "key": "ye2024physics",
        "author": "Ye, Tian and Xu, Zicheng and Li, Yuanzhi and Allen-Zhu, Zeyuan",
        "title": "Physics of language models: Part 2.1, grade-school math and the hidden reasoning process"
      },
      {
        "key": "petty2023impact",
        "author": "Petty, Jackson and van Steenkiste, Sjoerd and Dasgupta, Ishita and Sha, Fei and Garrette, Dan and Linzen, Tal",
        "title": "The impact of depth and width on transformer language model generalization"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "dehghani2018universal",
        "author": "Dehghani, Mostafa and Gouws, Stephan and Vinyals, Oriol and Uszkoreit, Jakob and Kaiser, Lukasz",
        "title": "Universal Transformers"
      },
      {
        "key": "lan2019albert",
        "author": "Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu",
        "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "schwarzschild2021can",
        "author": "Schwarzschild, Avi and Borgnia, Eitan and Gupta, Arjun and Huang, Furong and Vishkin, Uzi and Goldblum, Micah and Goldstein, Tom",
        "title": "Can you learn an algorithm? generalizing from easy to hard problems with recurrent networks"
      },
      {
        "key": "bansal2022end",
        "author": "Bansal, Arpit and Schwarzschild, Avi and Borgnia, Eitan and Emam, Zeyad and Huang, Furong and Goldblum, Micah and Goldstein, Tom",
        "title": "End-to-end algorithm synthesis with recurrent networks: Extrapolation without overthinking"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "giannou2023looped",
        "author": "Giannou, Angeliki and Rajput, Shashank and Sohn, Jy-yong and Lee, Kangwook and Lee, Jason D and Papailiopoulos, Dimitris",
        "title": "Looped transformers as programmable computers"
      },
      {
        "key": "de2024simulation",
        "author": "de Luca, Artur Back and Fountoulakis, Kimon",
        "title": "Simulation of graph algorithms with looped transformers"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "yang2023looped",
        "author": "Yang, Liu and Lee, Kangwook and Nowak, Robert and Papailiopoulos, Dimitris",
        "title": "Looped transformers are better at learning learning algorithms"
      },
      {
        "key": "gao2024expressive",
        "author": "Gao, Yihang and Zheng, Chuanyang and Xie, Enze and Shi, Han and Hu, Tianyang and Li, Yu and Ng, Michael K and Li, Zhenguo and Liu, Zhaoqiang",
        "title": "On the expressive power of a variant of the looped transformer"
      },
      {
        "key": "gatmiry2024can",
        "author": "Gatmiry, Khashayar and Saunshi, Nikunj and Reddi, Sashank J and Jegelka, Stefanie and Kumar, Sanjiv",
        "title": "Can Looped Transformers Learn to Implement Multi-step Gradient Descent for In-context Learning?"
      },
      {
        "key": "gatmiry2024role",
        "author": "Gatmiry, Khashayar and Saunshi, Nikunj and Reddi, Sashank J and Jegelka, Stefanie and Kumar, Sanjiv",
        "title": "On the Role of Depth and Looping for In-Context Learning with Task Diversity"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "mohtashami2023cotformer",
        "author": "Mohtashami, Amirkeivan and Pagliardini, Matteo and Jaggi, Martin",
        "title": "CoTFormer: More Tokens With Attention Make Up For Less Depth"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "liu2024mobilellm",
        "author": "Liu, Zechun and Zhao, Changsheng and Iandola, Forrest and Lai, Chen and Tian, Yuandong and Fedorov, Igor and Xiong, Yunyang and Chang, Ernie and Shi, Yangyang and Krishnamoorthi, Raghuraman and others",
        "title": "Mobilellm: Optimizing sub-billion parameter language models for on-device use cases"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "mohtashami2023cotformer",
        "author": "Mohtashami, Amirkeivan and Pagliardini, Matteo and Jaggi, Martin",
        "title": "CoTFormer: More Tokens With Attention Make Up For Less Depth"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "soudry2018implicit",
        "author": "Soudry, Daniel and Hoffer, Elad and Nacson, Mor Shpigel and Gunasekar, Suriya and Srebro, Nathan",
        "title": "The implicit bias of gradient descent on separable data"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "edelman2022inductive",
        "author": "Edelman, Benjamin L and Goel, Surbhi and Kakade, Sham and Zhang, Cyril",
        "title": "Inductive biases and variable creation in self-attention mechanisms"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "saunshi22understanding",
        "author": "Saunshi, Nikunj and Ash, Jordan and Goel, Surbhi and Misra, Dipendra and Zhang, Cyril and Arora, Sanjeev and Kakade, Sham and Krishnamurthy, Akshay",
        "title": "Understanding Contrastive Learning Requires Incorporating Inductive Biases"
      },
      {
        "key": "liu2023same",
        "author": "Liu, Hong and Xie, Sang Michael and Li, Zhiyuan and Ma, Tengyu",
        "title": "Same pre-training loss, better downstream: Implicit bias matters for language models"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "saunshi2024inductive",
        "author": "Saunshi, Nikunj and Karp, Stefani and Krishnan, Shankar and Miryoosefi, Sobhan and Reddi, Sashank J. and Kumar, Sanjiv",
        "title": "On the inductive bias of stacking towards improving reasoning"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "reddi2023efficient",
        "author": "Reddi, Sashank and Miryoosefi, Sobhan and Karp, Stefani and Krishnan, Shankar and Kale, Satyen and Kim, Seungyeon and Kumar, Sanjiv",
        "title": "Efficient Training of Language Models using Few-Shot Learning"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "liu2024mobilellm",
        "author": "Liu, Zechun and Zhao, Changsheng and Iandola, Forrest and Lai, Chen and Tian, Yuandong and Fedorov, Igor and Xiong, Yunyang and Chang, Ernie and Shi, Yangyang and Krishnamoorthi, Raghuraman and others",
        "title": "Mobilellm: Optimizing sub-billion parameter language models for on-device use cases"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "lan2019albert",
        "author": "Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu",
        "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "dehghani2018universal",
        "author": "Dehghani, Mostafa and Gouws, Stephan and Vinyals, Oriol and Uszkoreit, Jakob and Kaiser, Lukasz",
        "title": "Universal Transformers"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "giannou2023looped",
        "author": "Giannou, Angeliki and Rajput, Shashank and Sohn, Jy-yong and Lee, Kangwook and Lee, Jason D and Papailiopoulos, Dimitris",
        "title": "Looped transformers as programmable computers"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "yang2023looped",
        "author": "Yang, Liu and Lee, Kangwook and Nowak, Robert and Papailiopoulos, Dimitris",
        "title": "Looped transformers are better at learning learning algorithms"
      },
      {
        "key": "gatmiry2024can",
        "author": "Gatmiry, Khashayar and Saunshi, Nikunj and Reddi, Sashank J and Jegelka, Stefanie and Kumar, Sanjiv",
        "title": "Can Looped Transformers Learn to Implement Multi-step Gradient Descent for In-context Learning?"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "mohtashami2023cotformer",
        "author": "Mohtashami, Amirkeivan and Pagliardini, Matteo and Jaggi, Martin",
        "title": "CoTFormer: More Tokens With Attention Make Up For Less Depth"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "gao2024expressive",
        "author": "Gao, Yihang and Zheng, Chuanyang and Xie, Enze and Shi, Han and Hu, Tianyang and Li, Yu and Ng, Michael K and Li, Zhenguo and Liu, Zhaoqiang",
        "title": "On the expressive power of a variant of the looped transformer"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "ye2024physics",
        "author": "Ye, Tian and Xu, Zicheng and Li, Yuanzhi and Allen-Zhu, Zeyuan",
        "title": "Physics of language models: Part 2.1, grade-school math and the hidden reasoning process"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "liu2022transformers",
        "author": "Liu, Bingbin and Ash, Jordan T and Goel, Surbhi and Krishnamurthy, Akshay and Zhang, Cyril",
        "title": "Transformers learn shortcuts to automata"
      }
    ]
  },
  {
    "index": 26,
    "papers": [
      {
        "key": "sanford2024transformers",
        "author": "Sanford, Clayton and Hsu, Daniel and Telgarsky, Matus",
        "title": "Transformers, parallel computation, and logarithmic depth"
      }
    ]
  },
  {
    "index": 27,
    "papers": [
      {
        "key": "sanford2024understanding",
        "author": "Sanford, Clayton and Fatemi, Bahare and Hall, Ethan and Tsitsulin, Anton and Kazemi, Mehran and Halcrow, Jonathan and Perozzi, Bryan and Mirrokni, Vahab",
        "title": "Understanding transformer reasoning capabilities via graph algorithms"
      }
    ]
  },
  {
    "index": 28,
    "papers": [
      {
        "key": "petty2023impact",
        "author": "Petty, Jackson and van Steenkiste, Sjoerd and Dasgupta, Ishita and Sha, Fei and Garrette, Dan and Linzen, Tal",
        "title": "The impact of depth and width on transformer language model generalization"
      }
    ]
  },
  {
    "index": 29,
    "papers": [
      {
        "key": "back2024simulation",
        "author": "Back De Luca, Artur and Fountoulakis, Kimon",
        "title": "Simulation of Graph Algorithms with Looped Transformers"
      }
    ]
  },
  {
    "index": 30,
    "papers": [
      {
        "key": "saunshi22understanding",
        "author": "Saunshi, Nikunj and Ash, Jordan and Goel, Surbhi and Misra, Dipendra and Zhang, Cyril and Arora, Sanjeev and Kakade, Sham and Krishnamurthy, Akshay",
        "title": "Understanding Contrastive Learning Requires Incorporating Inductive Biases"
      }
    ]
  },
  {
    "index": 31,
    "papers": [
      {
        "key": "liu2023same",
        "author": "Liu, Hong and Xie, Sang Michael and Li, Zhiyuan and Ma, Tengyu",
        "title": "Same pre-training loss, better downstream: Implicit bias matters for language models"
      }
    ]
  },
  {
    "index": 32,
    "papers": [
      {
        "key": "saunshi2024inductive",
        "author": "Saunshi, Nikunj and Karp, Stefani and Krishnan, Shankar and Miryoosefi, Sobhan and Reddi, Sashank J. and Kumar, Sanjiv",
        "title": "On the inductive bias of stacking towards improving reasoning"
      }
    ]
  }
]