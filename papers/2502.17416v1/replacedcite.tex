\section{Related work}
\label{sec:related}
% \vspace{-2mm}
\vspace{-0.12in}

\looseness-1Reasoning is recognized as a core ability for intelligent and robustly model and has thus seen significant focus over the last few years. The synthetic reasoning problems we consider in this work have all been used in the prior works of ____ to theoretically analyze the strengths and limitations of Transformers.
There is also interest in the representation power of Transformers for computational problems ____ and for chain-of-thought reasoning ____.
The necessity of model depth for performance has been remarked upon, for small models____ and reasoning ____. In this work, we make a finer observation that albeit larger depth is necessary, this can be achieved with a limited parameter budget via looping. 

Looping in transformer models has been studied since the works ____ where they showed the benefits of looping for supervised learning tasks and BERT pretraining respectively. 
Looping also appears in ____ that study the extrapolation properties of looping for certain algorithmic tasks.
More recently ____ have studied the theoretical properties of looped decoder models and show that looping can simulate arbitrary Turing machines. 
In addition ____ study looped models as a way to simulate iterative algorithms for in-context learning.
Recently, ____ introduced CoTFormer, which tries to improve the perplexity of looped language models and ____ explore latency-efficiency parameter sharing for on-device LLMs.
% \ns{Include something about CoTFormer ____
In contrast, our work focuses on the surprising inductive bias of looping to improve downstream reasoning tasks, and goes beyond algorithmic and in-context learning tasks.

\looseness-1Different training algorithms (e.g. gradient descent ____) and architectural choices (e.g. attention ____) have been shown to have certain implicit biases. There is increasing interest in such inductive biases during pretraining ____. More recently, ____ showed an inductive bias of stacking ____ towards improving reasoning and hypothesize that a connection of stacking to looped models could be responsible for this. Our results provide further verification for this hypothesis.


\vspace{-0.1in}
% ---------------------------

% \textbf{Scaling laws} ____ also suggest that depth is especially important for small models. \nishanth{done. see above}

% \textbf{Papers on looping and how we differ}: Albert ____, Universal Transformer ____, programmable computers ____, ICL papers ____, CoTFormer ____, AlgoFormer ____.
% The focus of our work is not to compare with these, but point out the role of looping for reasoning. \nishanth{need to talk about CoTformer and Algoformer, the rest covered above}

% \textbf{Reasoning tasks}: addition, i-gsm ____, shortcuts to automata ____, $p$-hop ____. graph algorithms with transformers ____. paper showing that given a fixed parameter count, better to distribute them across depth axis than width axis for compositional generalization tasks (which look similar to reasoning tasks) ____. \nishanth{done, see above}

% \textbf{Papers on theoretical results}: expressivity of Transformers, turing-completeness, CoT, graph algorithms with looping ____
% \ns{@Zhiyuan: please help with this},
% \ns{graph algorithm paper modifies architecture.
% also our theory cares about the role of depth and optimality as opposed to Turing completeness}

% \textbf{Papers on inductive bias}: CL ____, Tengyu's work ____, Stacking paper ____, inductive bias of attention paper, gradient descent papers
% \ns{@Nikunj: please help with this}