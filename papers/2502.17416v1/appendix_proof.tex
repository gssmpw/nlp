\section{Theoretical results}\label{sec:appendix_proof}




\subsection{Detailed notations}
\label{sec:apx_detailed_notation}

\begin{definition}[Embedding Layer]\label{defi:embedding_layer}
Given a finite vocabulary $\mathcal{V}$, embedding dimension $d\in\mathbb{N}^+$, token embedding parameter $\theta_{\tokenembedding}\in\mathbb{R}^{d\times \abs{\mathcal{V}}}$ and position embedding parameter $\theta_{\posencoding}\in \mathbb{R}^{d\times n_{\max}}$, we define the \emph{embedding layer} as a sequence-to-sequence map, denoted by $\embed_{\theta_\tokenembedding,\theta_\posencoding}:\mathcal{V}^n\to (\mathbb{R}^d)^n$ for any $1\le n\le n_{\max}$, where
\begin{equation}
    \embed_{\theta_\tokenembedding,\theta_\posencoding}(v_1,\ldots,v_n) = \left(\theta_\tokenembedding(v_1)+ \theta_\posencoding(1), \ldots, \theta_\tokenembedding(v_n)+ \theta_\posencoding(n)  \right). 
\end{equation}
\end{definition}

\paragraph{Multi-Head Self-Attention Mechanism:} Given attention parameters $\theta_\attn = \{W_Q, W_K, W_V, W_O\}$, where each $W_Q^m, W_K^m, W_V^m, W_O^m \in \mathbb{R}^{d_{\attn} \times d }$, we define the \emph{Self-Attention} layer with a causal mask for a decoder-only transformer in \Cref{alg:defi_attn}. We also define a \emph{Multi-Head Attention} layer as a collection of self-attention layer with non-shared parameters $\theta_\mha = \{\theta_{\attn}^{(h)}\}_{h=1}^H$, and its output is the sum of the outputs from each head. That is, $\mha_{\theta_\mha} = \sum_{h=1}^H \attn_{\theta_\attn^{(h)}}$.\footnote{Though in this paper we focus on attention with casual mask, our definition of looped transformer generalizes to the cases with other attention masks.}

\begin{algorithm}
    \caption{Causal Self-Attention, $\attn$}\label{alg:defi_attn}
    \begin{algorithmic}[1]
    \Require  Parameter $\theta_\attn = (W_Q,W_K,W_V,W_O)$, Input embedding $ x_1,\ldots, x_n)\in \left(\mathbb{R}^{d}\right)^n$.
    \Ensure Output embedding $x'= (x'_1,\ldots, x'_n) \triangleq \attn_{\theta_\attn}(x_1,\ldots, x_n)$.
    \State $q_i \triangleq W_Q  x_i, k_i \triangleq W_K x_i, v_i \triangleq W_V x_i, \forall i\in[n]$
    \State $s_i \triangleq \softmax(\inner{q_i}{k_1},\ldots,\inner{q_i}{k_i}) \| (0,\ldots, 0) $. 
    \State $h'_i \triangleq W_O^\top \sum_{j=1}^n (s_i)_j v_j$.
    \end{algorithmic}
    \end{algorithm}
\paragraph{Feed-Forward Network:} Given the parameters of the fully-connected feedforward network layer $\theta_\ff = (W_1, b_1, W_2, b_2) \in \mathbb{R}^{x_{\ff} \times d} \times \mathbb{R}^{d_{\ff}} \times \mathbb{R}^{d \times d_{\ff}} \times \mathbb{R}^{d_{\ff}}$, we define the feedforward layer $\ff_{\theta_\ff}: \mathbb{R}^{d} \to \mathbb{R}^{d}$ as $\ff_{\theta_{\ff}}(h) \triangleq W_2, \relu(W_1 h + b_1) + b_2$.

\iffalse
\begin{definition}[Transformer Block]\label{defi:transformer_block}
Given number of layers $L\in\mathbb{N}^+$ and parameter $\theta_\tfblock = (\theta^{(l)}_\mha,\theta^{(l)}_\ff )_{l=0}^{L-1}$, we define the $L$-layer transformer block $\tfblock_{\theta_\tfblock}:(\mathbb{R}^d)^n\to (\mathbb{R}^d)^n$ for any $n\in\mathbb{N}^+$ as \begin{equation}
    \tfblock_{\theta_\tfblock} \triangleq (\id+ \ff_{\theta^{(L-1)}_\ff})\circ (\id+ \mha_{\theta^{(L-1)}_\mha})\circ \cdots (\id+ \ff_{\theta^{(0)}_\ff})\circ (\id+ \mha_{\theta^{(0)}_\mha})
\end{equation}
\end{definition}
\fi

\begin{definition}[Output Layer]\label{defi:output_layer}
    Given parameter $\theta_\transoutput \in\mathbb{R}^{d\times |\mathcal{V}|}$, we denote the output layer as $\transoutput_{\theta_\transoutput}:(\mathbb{R}^d)^n\to \Delta^{|\mathcal{V}|-1}$, where 
    \begin{align}
        \transoutput_{\theta_\transoutput}(x_1,\ldots,x_n) \triangleq \softmax(x_n^\top \theta_{\transoutput})
    \end{align}
\end{definition}

Finally, we define the entire transformer model $p_{\theta}: \cup_{n\le n_{\max}} \mathcal{V}^n\to \Delta^{|\mathcal{V}|-1}$ as 
\begin{align}
p_{\theta}\triangleq \transoutput_{\theta_\transoutput}\circ \tfblock_{\theta_\tfblock}\circ \embed_{\theta_\tokenembedding,\theta_\posencoding}
\end{align}
for any $\theta = (\theta_\tfblock,\theta_\tokenembedding,\theta_\posencoding,\theta_\transoutput)$.For convenience, we also write $\left[p_{\theta}(v_1,\ldots,v_n)\right](v)$ as $p_{\theta}(v\mid v_1,\ldots,v_n)$.
In particular, we use $\transformer_{\theta}(v_1,\ldots,v_n) \triangleq \argmax_{v\in\mathcal{V}} p_{\theta}(v|v_1,\ldots,v_n)$ to denote the deterministic version of the transformer model. 

\iffalse
\begin{definition}[Looped Transformer]\label{defi:looped_transformer}
Given number of loops $T\in\mathbb{N}^+$,  parameters $\theta = (\theta_{\tfblock},\theta_\tokenembedding,\theta_\posencoding,\theta_\transoutput)$, where $\theta_{\transformer} = (\theta^{(l)}_\mha,\theta^{(l)}_\ff )_{l=0}^{L-1}$, we define the \emph{looped transformer} as  $p_{\theta,T}\triangleq \transoutput_{\theta_\transoutput}\circ \left(\tfblock_{\theta_\tfblock}\right)^T\circ \embed_{\theta_\tokenembedding,\theta_\posencoding}$ and the corresponding deterministic version as $\transformer_{\theta,T}(v_1,\ldots,v_n) \triangleq \argmax_{v\in\mathcal{V}} p_{\theta,T}(v|v_1,\ldots,v_n)$.
\end{definition}
\fi



\paragraph{Finite-precision Modeling:} In this paper we assume the transformer is of finite precision. More specifically, we follow the setting in \citet{li2024chain} and use the shorthand $\Floating_{s}\triangleq \{c\cdot k\cdot 2^{-s}\mid c\in \{-1,1\}, 0\le k\le 2^{2s}-1, k\in\mathbb{N}\}$ to denote fixed-point numbers of constant precision $s$ and rounding operation $\rds{\cdot}:\mathbb{R}\to \Floating_s$ to denote the correcting rounding, namely the mapping from $\mathbb{R}$ to the closest representable number in $\Floating_s$. (We break the tie by picking the number with smaller absolute value). We assume that (1). all the parameters of the transformer are in $\Floating_{s}$ and (2). correct rounding is performed after every binary operation in the forward pass of the transformer. We will refer the readers to \citet{li2024chain} for detailed discussion on such finite-precision modeling and only list important notations and lemmas that will be used in this paper below. 

We use $1_s$ to denote all-one vectors of length $s$. Similarly we define $\inner{\cdot}{\cdot}_s$, $\times_s$, and $\softmax_s$. We recall that for any $s\in \mathbb{N}^+$ and integer $0\le x\le 2^s-1$, we use $\bin_s(x)\in\{0,1\}^s$ to denote the usual binary encoding of integer $x$ using $s$ binary bits in the sense that $x = \sum_{i=1}^s 2^i (\bin_s(x))_i$ and $\sbin_s(x)\in\{-1,1\}^s$ to denote the signed binary encoding, which is $2\bin_s(x)-(1,\ldots,1)$. Finally we define $B_s = \max \Floating_s = 2^s-2^{-s}$.



%
%\begin{proof}[Proof of \Cref{lem:cot_1_is_squal_to_O1}]
%	This is a special case of \Cref{thm:cot_dpolyn_slogn_main}.
%\end{proof}

\begin{lemma}\label{lem:exp_rounding}[Lemma E.1, \citep{li2024chain}]
	For any $s\in\mathbb{N}^+$, it holds that $\rds{\exp(-B_s)} = 0$.
\end{lemma}

\begin{lemma}\label{lem:exp_rounding_up}[Lemma E.2, \citep{li2024chain}]
	For any $s\in\mathbb{N}^+$, it holds that $\rds{\exp(B_s)} = B_s$.
\end{lemma}

\begin{lemma}\label{lem:FB_simulating_boolean_gates}[Lemma E.5, \citep{li2024chain}]
Unlimited-fanin $\AND,\OR$ (resp. $\MAJORITY) :\{0,1\}^n\to \{0,1\}$ can be simulated by some 2-layer feedforward ReLU network with constant (resp. $\log n$) bits of precision constant hidden dimension and additional $n$ constant inputs of value 1. 

Mathematically, let $\ff[s(n)]$ be the set of functions $C:\{0,1\}^n\to\{0,1\}$ which can be a two-layer feedforward ReLU network with at most $s(n)$ bits of precision and constant hidden dimension $\ff_{\theta}:\{0,1\}^{2n}\to \{0,1\}, \ff_{\theta}(x') = W_2\times_s\relu(\rds{W_1\times_s x'+b_1})$, where $\theta = (W_2,W_1,b_1)$, such that for any $x\in\{0,1\}^n$, 
\begin{align}
\ff_\theta(x_1,1,x_2,1,\ldots,x_n,1) = C(x).	
\end{align}
We have unlimited-fanin $\AND,\OR\in \ff[1]$ and $\MAJORITY\in \ff[\log n]$.  
\end{lemma}

Given two vectors $x,y$ of the same length $s$, we use $\interleave{x}{y}$ to denote their interleaving, that is, $(\interleave{x}{y})_{2i-1} = x_i, (\interleave{x}{y})_{2i} = y_i$ for all $i\in [e]$. 
\begin{lemma}\label{lem:attention_rounding}[Lemma E.3, \citep{li2024chain}]
	For any $s\in\mathbb{N}^+$, let $q_i = \interleave{\sbin_s(i)}{1_s}$ and $k_i = B_s\cdot (\interleave{\sbin_s(i)}{(-1_s)})$ for all $i\in [2^s-1]$, it holds that $\rds{\exp(\inner{q_i}{k_j}_s)}=\indct{i=j}$ for all $i,j\in [2^s-1]$.
\end{lemma}

\subsection{Proofs}


\subsubsection{Looped models can simulate non-looped models}
\label{sec:apx_simulate_nonlooped}


\begin{proof}[Proof of \Cref{thm:main}]

We start by introduce some more notations. We will proof the theorem for any fixed sequence of $\vv = (v_1,\ldots, v_n)$. We use $\vx^{(l)}=(x_{i}^{(l)})_{i=1}^n$ to denote the intermediate embedding of $p_{\theta}$ in the $l$th layer. More specifically, we define 

\begin{align}
    \vx^{l} = (\id+ \ff_{\theta^{(l-1)}_\ff})\circ (\id+ \mha_{\theta^{(l-1)}_\mha})\circ \cdots (\id+ \ff_{\theta^{(0)}_\ff})\circ (\id+ \mha_{\theta^{(0)}_\mha})\circ \embed_{\theta_{\embed}}(\vv).
\end{align}

We also use $\vx^{(l+0.5)}=(x_{i}^{(l+0.5)})_{i=1}^n$ to denote the intermediate embedding of $p_{\theta}$ in the $l$th layer after the attention layer.
\begin{align}
    \vx^{l+0.5} =  (\id+ \mha_{\theta^{(l-1)}_\mha})(\vx^{l}).
\end{align}

Similarly, for the constructed looped transformer $p_{\theta,T}$, we use $\vv'=(\#,\vv_1,\ldots,\vv_n)$ to denote its input. For simplicity, we use the convention that $\#$ is at position $0$. The proof still works if $\#$ starts at position $1$ because we can just transfer the later tokens by $1$ position. We define $\vx'^{(l)}=(x'^{(l)}_0,x'^{(l)}_1,\ldots,x'^{(l)}_n)$ as the intermediate embedding of $p_{\theta}$ in the $l$th layer and $\vx'^{(l+0.5)}=(x'^{(l+0.5)}_0,x'^{(l+0.5)}_1,\ldots,x'^{(l+0.5)}_n)$ as the intermediate embedding of $p_{\theta}$ in the $l$th layer.

Below we first state the key properties that our construction will satisfy, which imply the correctness of the theorem and then we state our construction of $p_{\theta',T}$ and show the properties are actually satisfied:
\begin{itemize}
    \item $x'^{(l)}_{i} = (x^{(l)}_i, \bm{1}_{R} - e_{r(l)},l, \indct{i=0})$.
    \item $x'^{(l+0.5)}_{i} = (x^{(l+0.5)}_i, \bm{1}_{R} - e_{r(l)},l, \indct{i=0})$.
    \item $x^{(l)}_0 = x^{(l+0.5)}_0 = \bm{0}$.\footnote{Here we abuse the notation for simplicity of presentation. $x^{(l)}_0 = x^{(l+0.5)}_0$ are not defined in the original non-looped transformer. The key point here is that they are 0 vectors throughout the forward pass.}
\end{itemize}

% \begin{itemize}
%     \item $\theta'_{\ff} $ is essentially concatenation of $\theta^{(i)}_\ff$ for $i\in[r]$ in the hidden dimension of $\ff$. The goal is to make $\ff_{\theta'_{\ff}} = \sum_{i=1}^r \ff_{\theta^{(i)}_\ff}\indct{r(l)=i}$ at $l$th layer. 
%     \item  $\theta'_{\mha} $ is essentially concatenation of $\theta^{(i)}_\mha$ for $i\in[r]$ in the hidden dimension of $\mha$. The goal is to make $\mha_{\theta'_{\mha}} = \sum_{i=1}^r \mha_{\theta^{(i)}_\ff}\indct{r(l)=i}$ at $l$th layer.
%     \item Use extra dimension to count the depth $l$ and maintain $\bm{1} - e_{r(l)}$ using $R+1$ dimension on top of the original embedding size $d$ and use it to do switch. Such piecewise linear function can be approximated by a neural network with $O(L)$ hidden dimension.
%     \item use $\bm{1} - e_{r(l)}$ together with ReLU activation to do the select in MLP. Use $\indct - e_{r(l)}$ in attention to force all attention heads to attend to the dummy token $\#$ in the beginning of the sentence.
% \end{itemize}

To get the last coordinate $l$, which is a depth counter, we just need to add $1$ more hidden dimension in MLP. 

Next we show we can use two-layer with $L+2$ MLP to get the mapping from $\ell\mapsto \bm{1}_{R} - e_{r(l)}$. Let $(\theta^{(i)}_\ff,\theta^{(i)}_\mha)_{i=1}^R$ be the parameters of the $R$ distinct layers in $\theta$. We assume in the $l$th layer, $r(l)$'s parameters are used. This is because $e_{r(l)} = \sum_{i=1}^L e_{r(i)}0.5*( [l-i+1 ]_+-2[l-i]_+ + [l-i-1]_+)$.

Now we explain how to deactivate the undesired MLP neurons. In other words, our construction of $\theta'_{\ff} $ is essentially concatenation of $\theta^{(i)}_\ff$ for $i\in[r]$ in the hidden dimension of $\ff$, with the additional control that  $\ff_{\theta'_{\ff}}((x^{(l)}_i, \bm{1}_{R} - e_{r(l)},l),\indct{i=0}) = \sum_{i=1}^R \ff_{\theta^{(i)}_\ff}(x^{(l)}_i)\indct{r(l)=i,i\neq 0}$ at $l$th layer. This control can be done by subtracting $\bm{1} - e_{r(l)} + \indct{i=0}$ by a constant which is larger than the maximum pre-activation in the hidden layer.

Finally we explain how to deactivate the undesired attention. We will only use attention to update the first part of the embedding, which is $x^{(l+0.5)}_i$. A crucial step here is that we set the token embedding of $\#$ as $0$
We construct keys and queries as follows: 
\begin{enumerate}
    \item ${W'}_Q^{(r')}(x'^{(l)}_{i}) = (W_Q^{(r')}x^{(l)}_{i}, 1- \indct{r'=r(l)} $ for $r' \in [R]$ and $i=0,\ldots,n$
    \item ${W'}_K^{(r')}(x'^{(l)}_{i}) = (W_K^{(r')}x^{(l)}_{i}, -B\indct{i=0}) $ for $r' \in [R]$ and $i=0,\ldots,n$, where $B$ is some constant larger than the maximum previous inner product in attention, $\max_{l\in[L],i,j} \inner{(W_K x^{(l)}_{i}}{(W_Qx^{(l)}_{i}}$. 
    \item ${W'}_O^{(r')}{W'}_V^{(r')}(x'^{(l)}_{i}) = (W_O^{(r')}W_V^{(r')}x^{(l)}_{i}, \bm{0}, 0,0)$.
\end{enumerate}
This construction works because only the `desired' attention head $r=r(l)$ will be activated and behave as in the non-looped case, because otherwise all position in that attention head will be completely attended to position $0$ and returns a zero vector. (We can choose $B$ to be large enough and distribution calculated by the attention score is delta distribution) at position $0$, which yields a zero vector as its value. This completes the proof.
\end{proof}





\subsubsection{Group composition.}
\label{sec:apx_group_composition}






\begin{algorithm}[t]
\caption{Group Composition}\label{alg:group_composition}
\begin{algorithmic}[1]
\Require  Group elements $g_0,g_1,\ldots,g_n\in G$, where $g_0=e$.
\Ensure $g_0\circ g_1\circ \ldots g_n$.
\State $g^{(0)}_i = g_i$, $\forall0\le i\le n$.
\For{$l=1\to \lceil \log_2 n\rceil$}
\State $a^{(l)}_i = g^{(l-1)}_{[2i-n-1]_+},b^{(l)}_i = g^{(l-1)}_{[2i-n]_+}$  $\forall0\le i\le n$.
\State $g^{(l)}_i = a^{(l)}_i\circ b^{(l)}_i$, $\forall0\le i\le n$.
\EndFor
\State \Return $g^{(\lceil\log _2 n\rceil)}_{n}$.
\end{algorithmic}
\end{algorithm}



The landmark result in automata theory, Krohn-Rhodes Decompotision Theorem~\citep{krohn1965algebraic}, shows that all semi-automaton with solvable transformation group (which includes composition problem of solvable groups) can be simulated by a cascade of permutation-reset automata, which can be simulated by $\TC^0$ circuits. \citep{liu2022transformers} further showed that such automaton with solvable transformation group can be continuously simulated by constant-depth transformers. However, it is also shown~\citep{barrington1986bounded} that the composition problem of unsolvable groups are $\NC^1$-complete, for example, the composition of permutation group over $5$ elements, $S_5$. Under the common hardness assumption that $\NC^1\neq \TC^0)$,   constant depth transformer cannot solve composition of $S_5$ using a single forward pass~\citep{merrill2023parallelism,liu2022transformers,li2024chain}. But with CoT, very shallow transformers (depth equal to one or two) can simulate the composition problem of any group\citep{li2024chain,merrill2023expresssive}.



\begin{proof}[Proof of \Cref{thm:group_composition_log_depth}]
We will set the token embedding of $\#$ the same as that of $e$, which is the identity of $G$. In the following proof, we will just treat $\#$ as $e$. We will construct the transformer simulating group composition following the following algorithm~\Cref{alg:group_composition}, which gives the high-level idea of the construction. The correctness of \Cref{alg:group_composition} follows from the associativity of group composition. More concretely, we can verify by induction that $g^{l}_0\circ g^{l}_1\circ\ldots g^{l}_n$ is the same for all $l=0,\ldots, \lceil \log_2 n\rceil$ and in the final round, i.e., when $l=\lceil \log_2 n\rceil$, $g^{(l)}_i=e$  for all $i<n$. 



Below we show how to construct a transformer of the given sizes to simulate the above \Cref{alg:group_composition}.
We will embed each $g\in G$ as a different vector  $\overline{g} \in \{-1,1\}^{\lceil\log_2 |G|\rceil}$ and each position $0\le i\le n$ as its binary representation in $\overline{i} \in\{-1,1\}^{\lceil\log_2 n+1\rceil}$, which is a shorthand for $\sbin_s(i)$ with $s=\lceil\log_2 n+1\rceil$.
We concatenate them to get $\{x^{(0)}_i\}_{i=0}^n$, that is, $x^{(0)}_i = (\overline{g_i},\overline{i}, \overline{[2i-n-1]_+}, \overline{[2i-n-1]_+},0^{\lceil \log_2 |G|\rceil},0^{\lceil \log_2 |G|\rceil})$.
For convenience, we will drop the 0's in the end (also in the other proofs of the paper) and write it as $x^{(0)}_i = (\overline{g_i},\overline{i} , \overline{[2i-n-1]_+}, \overline{[2i-n-1]_+})$.
Below we show we can construct 1-layer transformer block with parameter $(\theta_\mha,\theta_\ff)$ satisfying that
\begin{enumerate}
    \item $\left[ \mha_{\theta_\mha}\left((\overline{g_i},\overline{i},\overline{[2i-n-1]_+}, \overline{[2i-n-1]_+})_{i=0}^n\right)\right]_k = (0^{\lceil\log_2 |G|\rceil +3\lceil\log_2 n+1\rceil}, \overline{g_{[2k-n-1]_+}}, \overline{g_{[2k-n]_+}})$ \\for all $g_0=e,g_i\in G \forall i\in[n]$, $k=0,\ldots,n$;
    \item $\ff_{\theta_\ff}(\overline{g},\overline{i}, \overline{j}, \overline{k},\overline{g'}, \overline{g''}) = (\overline{g'\circ g''}-\overline{g}, 0^{3\lceil\log_2 n+1\rceil},-\overline{g'}, -\overline{g''})$, for all $i,j,k=0,\ldots,n$, $g,g',g''\in G$. 
\end{enumerate}

The first claim is because we can use two attention heads to retrieve $ \overline{g_{[2k-n-1]_+}}$ and $ \overline{g_{[2k-n]_+}}$ respectively, where both of them use $\overline{k}$ as the key and use $-\overline{[2k-n-1]_+}$ and $-\overline{[2k-n]_+}$ as queries respectively. This is possible because all the required information are already in $x_i$. We further make attention temperature low enough so the probability returned by attention is a one-hot distribution at the position whose key is equal to the negative query after rounding.

Now we turn to the second claim about MLP. We will use $|G|^2$ neurons with ReLU activation and bias to simulate the product of $g'$ and $g''$. We can index each neuron 
by $(h,h')$ for $h,h'\in G$ and set its incoming weight $[W_1]_{(h,h'),:} = (\overline{h},\overline{h'})$ and set bias $(b_1)_{(h,h')} = - 2\lceil\log_2 |G|\rceil+1$, which ensures that the activation of neuron $(h,h')$ will only be $1$ when $g'=h,g''=h'$ and be $0$ otherwise. Then setting the outgoing weight of neuron $(h,h')$ as $\overline{h\circ h'}$ and the bias in the second layer to be $0$ finishes the construction for simulating the group composition. Finally we use the remaining $6\lceil\log_2 |G|\rceil$ to simulate negative identity mapping $x\to-x$ for the remaining $3\lceil\log_2 |G|\rceil$ embedding dimension. This completes the proof.
\end{proof}






% \iffalse


\subsection{Connection to chain-of-thought reasoning}
\label{sec:apx_cot_connection}


In this section, we establish a connection betwee looped models and CoT reasoning. We first define the recursion for CoT reasoning as follows:
$$
\transformer^{i}_\theta(v_1,\ldots,v_n)\triangleq \transformer^{i-1}_\theta(v_1,\ldots,v_n, \transformer_\theta(v_1,\ldots,v_n)),$$ for $i, n \in \mathbb{N}^+$ satisfying $i+n\le n_{\max}-1$ along with the base case of $\transformer^{1}_\theta(v_1,\ldots,v_n)\triangleq\transformer_\theta(v_1,\ldots,v_n)$. For all $0\le i\le n_{\max} - n-1$, the output with $i$ steps of CoT is
$v_{n+i+1}  = \transformer^{i+1}_\theta(v_1,\ldots,v_n) = \transformer_\theta(v_1,\ldots,v_n,v_{n+1},\ldots,v_{n+i})$.
% \ns{Connection to CoT reasoning} \sjr{this part is very unclear}

\iffalse
\begin{theorem}[Informal]\label{thm:cot_informal}
    For any $L$-layer non-looped transformer $\transformer_\theta$, there exists a looped transformer with parameter $\theta'$ and $L+\mathcal{O}(1)$ layers, such that for any input $\vx$ and integer $m$,  the output of non-looped transformer after $m$ steps of CoT is the same as that of the looped transformer on input $x$ concatenated by $m$ dummy tokens with $m$ loops.
\end{theorem}
\fi




We first give the formal statement below.
% of \Cref{thm:cot_informal}, which is \Cref{thm:cot_formal} below. 
\begin{theorem}[Looped transformer simulates CoT]\label{thm:cot_formal}
    For any $L$-layer non-looped transformer $\transformer_\theta$, there exists a looped transformer $\transformer_{\theta'}$ with $L+\mathcal{O}(1)$ layers, constantly many more dimensions in embedding, MLP and attention layers and constantly many more attention heads, such that for any input $\vv= (v_i)_{i=1}^n$ and integer $m$, the output of non-looped transformer after $m$ steps of CoT, $\transformer^m_\theta(\vv)$, is the same as that of the looped transformer on input $x$ concatenated by $m$ dummy tokens with $m$ loops, $\transformer_{\theta',m}(\vv,\#^m)$.
\end{theorem}


Below are some helping lemmas towards showing \Cref{thm:cot_formal} is at least as powerful as CoT.
\begin{lemma}[Simulating $\argmax$ using MLP]\label{lem:simulating_hard_argmax}
    For every $d\in \mathbb{N}$ and precision $s\in\mathbb{N}^+$, there exists a 3-layer network with $\relu$ activation and $d^2$ width $f$ with $s$ precision, such that for any $x\in\Floating_s^d$, if there is $k\in [d]$, such that $x_k >\max_{j\neq k,j\in [d]}x_j$, $f(x) = e_k$. 
\end{lemma}


\begin{proof}[Proof of \Cref{lem:simulating_hard_argmax}]
    Define $g_i = 2^s\cdot \relu(2^{-s} - \sum_{j\neq i}\relu(x_j-x_i))$ for each $i\in [n]$. We claim that if there is $k\in [d]$, such that $x_k -\max_{j\neq k,j\in [d]}x_j\ge 2^{-s}$, $g_i = 1$ iff $i=k$ for all $i\in [d]$. First $g_k =2^s\cdot \relu(2^{-s}) =1 $. Next for $i\neq k$, it clearly holds that  $\sum_{j\neq i}\relu(x_j-x_i)\ge 2^{-s}$ and thus $g_i\le 0$. 
    This construction can clearly be implemented by a 3-layer $\relu$ network with $s$ precision.
\end{proof}

\begin{lemma}[Simulating Decoding and Embedding using MLP]\label{lem:simulating_decoding_embedding}
    Given any $s$-precision $\theta_{\tokenembedding}\in\mathbb{R}^{d\times \Sigma}$ and $\theta_{\transoutput}$, there is a 5-layer network $f:\mathbb{R}^d\to \mathbb{R}^d$ with $\relu$ activation and $\max(|\Sigma|^2)$ width with $s$-precision, such that for all $s$-precision $x\in\mathbb{R}^d$ which admits unique $\argmax$ for $v\triangleq \argmax_{o\in\Sigma} (x^\top\theta_{\transoutput})(o)$, it holds that
    \begin{align*}
        f(x) =  \theta_{\tokenembedding}(v).
    \end{align*}
\end{lemma}
\begin{proof}[Proof of \Cref{lem:simulating_decoding_embedding}]
    This is a simple application of \Cref{lem:simulating_hard_argmax}.
\end{proof}

\begin{lemma}[Control Gate]\label{lem:control_gate}
    A 2-layer $\relu$ network with precision $s$ can implement $F:\Floating_s\times \Floating_s\times \{0,1\}, F(x,y,M) = Mx + (1-M)y $. 
\end{lemma}
\begin{proof}[Proof of \Cref{lem:control_gate}]
    Note that $F(x,y,M) = \relu(x-2^s\cdot (1-M)) - \relu(-x-2^s\cdot (1-M)) + \relu(y-2^s\cdot M) - \relu(-y-2^s\cdot M)$. The proof is completed.
\end{proof}

\begin{definition}[Transformer Block with Mask]\label{defi:embedding_layer_w_mask}
Given number of layers $L\in\mathbb{N}^+$, parameter $\theta_\tfblock = (\theta^{(l)}_\mha,\theta^{(l)}_\ff )_{l=0}^{L-1}$, and mask function $M:\mathbb{N}\to\{0,1\}$, we define the $L$-layer \emph{transformer block with mask}$\tfblock_{\theta_\tfblock,M}:(\mathbb{R}^d)^n\to (\mathbb{R}^d)^n$ for any $n\in\mathbb{N}^+$ as 
\begin{equation}
    [\tfblock_{\theta_\tfblock,M}(\vx)]_i \triangleq (1-M(i))x_i + M(i)[\tfblock_{\theta_\tfblock}(\vx)]_i
\end{equation}
\end{definition}

\begin{definition}[Looped Transformer with Mask]\label{defi:looped_transformer_w_mask}
Given number of loops $T\in\mathbb{N}^+$,  parameters $\theta = (\theta_{\tfblock},\theta_\tokenembedding,\theta_\posencoding,\theta_\transoutput)$, and mask functions $\{M^t\}_{t=1}^T$, where $\theta_{\transformer} = (\theta^{(l)}_\mha,\theta^{(l)}_\ff )_{l=0}^{L-1}$, we define the \emph{looped transformer with mask} as  $p_{\theta,T,M}\triangleq \transoutput_{\theta_\transoutput}\circ \tfblock_{\theta_\tfblock,M^T}\circ \cdots \tfblock_{\theta_\tfblock,M^1}\circ \embed_{\theta_\tokenembedding,\theta_\posencoding}$ and the corresponding deterministic version as $\transformer_{\theta,T,M}(v_1,\ldots,v_n) \triangleq \argmax_{v\in\mathcal{V}} p_{\theta,T,M}(v|v_1,\ldots,v_n)$.
\end{definition}

\begin{definition}[Shifting Layer]\label{defi:shifting_layer}
    We define the \emph{shifting layer} $\shift:(\mathbb{R}^d)^n\to (\mathbb{R}^d)^n$ as the following for any $d,n\in\mathbb{N}^+$ and $x_1,\ldots, x_n\in\mathbb{R}^{d}$:
    \begin{align}
        \shift(x_1,x_2,x_3, \ldots,x_n) = (x_1,x_1,x_2,x_3,\ldots,x_{n-1}).
    \end{align}
\end{definition}

\begin{lemma}\label{lem:shifting_layer}
    For input sequence length up to some integer $n$, $\shift$ could be implemented by a attention layer by concatenating each embedding $x_i$ with $(\sbin_s(i), \sbin_s(f(i)))$, where $n= \lceil \log_2 n +1\rceil$.
\end{lemma}

\begin{proof}[Proof of \Cref{lem:shifting_layer}]
    It is equivalent to show we can construct an attention heads which computes $x_{f(i)}$ at each position $i$. 

    To do so, we just need to invoke \Cref{lem:simulating_hard_argmax} and use that to set key and query, so position $i$ attends to position $f(i)$. We set value at position $i$ to be $x_i$. This completes the proof. 
\end{proof}


\begin{lemma}\label{lem:mlp_addition}
    For any positive integer $s>0$, there is a constant-depth MLP $F$ with $O(s)$ hidden neurons per layer and parameters in $\Floating_s$, such that for any input $\bin(x)\in\{-1,+1\}^s$, where $0\le x\le 2^s-1$, it holds that  
    \begin{align*}
        F(x) = \bin(x+1).
    \end{align*}
\end{lemma}

\begin{proof}[Proof of \Cref{lem:mlp_addition}]
    By \Cref{lem:FB_simulating_boolean_gates}, it suffices to show that we can simulate $\bin(x)\mapsto \bin(x)+1$ using $O(s)$ wide, constant-depth boolean circuits with $\AND,\OR,\NOT$ gates with unlimited fan-ins. This is immediate by noting that 
    \begin{align}
        [\bin(x+1)]_i = [\bin(x+1)]_i \oplus \bigwedge_{j=1}^{i-1}[\bin(x+1)]_j
    \end{align}
\end{proof}

\begin{lemma}\label{lem:mlp_comparison}
    For any positive integer $s>0$, there is a constant-depth MLP $F$ with $O(s)$ hidden neurons per layer and parameters in $\Floating_s$, such that for any input $(\bin(x),\bin(y))\in\{-1,+1\}^s$, where $0\le x,y\le 2^s-1$, it holds that  
    \begin{align*}
        F(x,y) = \indct{x > y}.
    \end{align*}
\end{lemma}

\begin{proof}[Proof of \Cref{lem:mlp_comparison}]
    By \Cref{lem:FB_simulating_boolean_gates}, it suffices to show that we can simulate $\bin(x),\bin(y)\mapsto \indct{x> y}$ using $O(s)$ wide, constant-depth boolean circuits with $\AND,\OR,\NOT$ gates with unlimited fan-ins. This is immediate by noting that 
    \begin{align}
\indct{x > y} = \bigvee\limits_{i\in[s]} \left( ([\bin(x)]_i = 1) \wedge ([\bin(y)]_i = 0) \wedge \bigwedge\limits_{1\le j< i} \left([\bin(x)]_j = [\bin(y)]_j\right) \right).
    \end{align}
\end{proof}
\begin{proof}[Proof of \Cref{thm:cot_formal}]

We consider mask $M^t(i) = \indct{i-t\ge n}$, which we call it CoT masking. Let $s = \lfloor \log (n+m) +1 \rfloor$, we use $\overline i$ to denote $\sbin_s(i)$ for $1\le i\le n+m$ for convenience. The embedding size of our newly constructed transformer is larger than the target transformer to be simulated by an additive constant of $3s$. We denote the new embedding at position $i$ after $t$th loop by $(x_i^{(t)}, p_i^{(t)})$, where $x_i^{(t)}$ will be the original embedding of the transformer to be simulated, and $p_i^{(t)}$ is of dimension $3s$ and only depends on $i$ and $t$. In particular, we can show that we can set $p_i^{(t)} \triangleq (\overline i,\overline{[i-2]_+ +1}, \overline{n+t})$ to save information about position and the number of loops --- $p_i^{(0)}$ is from the positional embedding and the update is due to \Cref{lem:mlp_addition}. The size of hidden dimension of MLP and attention (key, query, value) will also be increased by $O(s)$.

The proof contains two steps:
\begin{enumerate}
    \item To show that there is a transformer with CoT masks simulating the target transformer with $m$ steps of CoT and $L$ layers by looping its own $L+O(1)$ layer block $m$ times.
    \item To show that the above looped transformer with CoT masks can be simulated by a standard looped transformer without mask and with constantly many more layers.
\end{enumerate}

For the first claim, starting from the same parameter of the transformers with CoT $\theta$, we build a new looped model with parameter $\theta'$ with constantly many more layers in each transformer block and at most constantly many more heads per attention layer. First, we can add constantly many more layers to use MLP to simulate the decoding-encoding process using \Cref{lem:simulating_decoding_embedding}.
Next, we can add one more transformer layer in each block and use the attention layer to simulate the shifting layer by~\Cref{lem:shifting_layer}, since we have the additional position embedding $p_i^(t)$.
In particular, the embedding we get at position $n+t$ after $t$ loops, $x_{n+t}^{(t)}$,  now simulates the token embedding of $n+t$ of the CoT transformer.
By the way we define CoT mask $M$, for every $t\ge -n+1$, the embedding $\hat{x}_{n+t}^{(t')}$ will keep the same for all $t'\ge \max(t,0)$. 
In $t$th loop, the only embedding update that matters happens at $n+t$th position, because no updates happen at earlier positions, and updates at later positions $n+t'$ for some $t'>t$ will be overwritten eventually in the future loops $t'$, by some value which is independent of their value at the current loop $t$. In the end, we know the embedding $x_i^{(T)}$ in \Cref{defi:looped_transformer_w_mask} is exactly equal to that in CoT transformer, and so does the final output.

For the second claim, because CoT mask can be computed by a $O(\log(n+m))$ wide, constant-depth MLP~(\Cref{lem:mlp_comparison}), together with \Cref{lem:control_gate}, we know it suffices to increase the number of layers per transformer block and embedding size and hidden dimensions by constant to simulate the transformer with mask by a transformer without mask. 
\end{proof}


% \fi



% \ns{Connecting to our $p$-hop experiments}
\begin{theorem}[Restatement of Theorem 4.2, \citep{sanford2024transformers}]\label{thm:khop_log_depth}
    $p$-hop problem (\Cref{defi:khop}) can be solved by $\lfloor\log_2 p\rfloor+2$-layer non-looped transformer with $\log n$ bits of precision, at most $3$ different layers, $d=d_{\ff}=d_{\attn}=O(1)$ embedding size, hidden dimension for MLP and attention, $1$ attention head.
\end{theorem}


