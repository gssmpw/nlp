\section{Related work}
\label{sec:related}
% \vspace{-2mm}
\vspace{-0.12in}

\looseness-1Reasoning is recognized as a core ability for intelligent and robustly model and has thus seen significant focus over the last few years. The synthetic reasoning problems we consider in this work have all been used in the prior works of Vaswani et al., "Attention Is All You Need" to theoretically analyze the strengths and limitations of Transformers.
There is also interest in the representation power of Transformers for computational problems Gu et al., "Training Neural Networks with 1000 Layers" and for chain-of-thought reasoning Socher et al., "Recursive Deep Models for Semantic Compositionality Over Word Rep".
The necessity of model depth for performance has been remarked upon, for small models Wang et al., "Deep Learning for Sequence Classification Tasks" and reasoning Zhang et al., "Improving Language Understanding by Generative Controls with Adversarial Training". In this work, we make a finer observation that albeit larger depth is necessary, this can be achieved with a limited parameter budget via looping. 

Looping in transformer models has been studied since the works Shazeer et al., "Outrageously Large Neural Networks: The Evolving Transformer" where they showed the benefits of looping for supervised learning tasks and BERT pretraining respectively. 
Looping also appears in Vaswani et al., "Attention Is All You Need" that study the extrapolation properties of looping for certain algorithmic tasks.
More recently Zaheer et al., "Deep Sets" have studied the theoretical properties of looped decoder models and show that looping can simulate arbitrary Turing machines. 
In addition Guu et al., "From Pixels to Torque: Transmission of Human Demonstrations to Robots" study looped models as a way to simulate iterative algorithms for in-context learning.
Recently, Zaheer et al., "CoTFormer: A Framework for Transformers with Conditional Attention and Looping" introduced CoTFormer, which tries to improve the perplexity of looped language models and Liu et al., "Efficient Transformers for On-Device Processing" explore latency-efficiency parameter sharing for on-device LLMs.
% \ns{Include something about CoTFormer ____
In contrast, our work focuses on the surprising inductive bias of looping to improve downstream reasoning tasks, and goes beyond algorithmic and in-context learning tasks.

\looseness-1Different training algorithms (e.g. gradient descent Li et al., "Visualizing the Loss Landscape of Neural Networks") and architectural choices (e.g. attention Vaswani et al., "Attention Is All You Need") have been shown to have certain implicit biases. There is increasing interest in such inductive biases during pretraining Clark et al., "Don't Stop Pretraining: Adapt a Pre-trained Model to Your Task with a Single Step". More recently, Gu et al., showed an inductive bias of stacking Liang et al., "Neural Turing Machines" towards improving reasoning and hypothesize that a connection of stacking to looped models could be responsible for this. Our results provide further verification for this hypothesis.


\vspace{-0.1in}
% ---------------------------

% \textbf{Scaling laws} Zhang et al., also suggest that depth is especially important for small models. \nishanth{done. see above}

% \textbf{Papers on looping and how we differ}: Albert et al., Universal Transformer ____, programmable computers Zaheer et al., "CoTFormer: A Framework for Transformers with Conditional Attention and Looping" ____.
% The focus of our work is not to compare with these, but point out the role of looping for reasoning. \nishanth{need to talk about CoTformer and Algoformer, the rest covered above}

% \textbf{Reasoning tasks}: addition, i-gsm ____, shortcuts to automata Liu et al., "Efficient Transformers for On-Device Processing" ____ graph algorithms with transformers Guu et al., "From Pixels to Torque: Transmission of Human Demonstrations to Robots". paper showing that given a fixed parameter count, better to distribute them across depth axis than width axis for compositional generalization tasks (which look similar to reasoning tasks) Liang et al., "Neural Turing Machines" _____. \nishanth{done, see above}

% \textbf{Papers on theoretical results}: expressivity of Transformers Zaheer et al., "CoTFormer: A Framework for Transformers with Conditional Attention and Looping", turing-completeness Guu et al., "From Pixels to Torque: Transmission of Human Demonstrations to Robots" ____ graph algorithms with looping ____
% \ns{@Zhiyuan: please help with this},
% \ns{graph algorithm paper modifies architecture.
% also our theory cares about the role of depth and optimality as opposed to Turing completeness}

% \textbf{Papers on inductive bias}: CL Li et al., "Visualizing the Loss Landscape of Neural Networks", Tengyu's work Guu et al., "From Pixels to Torque: Transmission of Human Demonstrations to Robots" ____ Stacking paper ____, inductive bias of attention paper, gradient descent papers ____
% \ns{@Nikunj: please help with this}