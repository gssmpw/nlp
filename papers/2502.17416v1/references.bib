@article{guo2025deepseek,
  title={Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning},
  author={Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others},
  journal={arXiv preprint arXiv:2501.12948},
  year={2025}
}


@article{schwarzschild2021can,
  title={Can you learn an algorithm? generalizing from easy to hard problems with recurrent networks},
  author={Schwarzschild, Avi and Borgnia, Eitan and Gupta, Arjun and Huang, Furong and Vishkin, Uzi and Goldblum, Micah and Goldstein, Tom},
  journal={Advances in Neural Information Processing Systems},
  year={2021}
}


@article{bansal2022end,
  title={End-to-end algorithm synthesis with recurrent networks: Extrapolation without overthinking},
  author={Bansal, Arpit and Schwarzschild, Avi and Borgnia, Eitan and Emam, Zeyad and Huang, Furong and Goldblum, Micah and Goldstein, Tom},
  journal={Advances in Neural Information Processing Systems},
  year={2022}
}

@article{gatmiry2024role,
  title={On the Role of Depth and Looping for In-Context Learning with Task Diversity},
  author={Gatmiry, Khashayar and Saunshi, Nikunj and Reddi, Sashank J and Jegelka, Stefanie and Kumar, Sanjiv},
  journal={arXiv preprint arXiv:2410.21698},
  year={2024}
}

@article{merrill2023expresssive,
  title={The expresssive power of transformers with chain of thought},
  author={Merrill, William and Sabharwal, Ashish},
  journal={arXiv preprint arXiv:2310.07923},
  year={2023}
}

@article{strobl2023transformers,
  title={Transformers as recognizers of formal languages: A survey on expressivity},
  author={Strobl, Lena and Merrill, William and Weiss, Gail and Chiang, David and Angluin, Dana},
  journal={arXiv preprint arXiv:2311.00208},
  year={2023}
}


@article{soudry2018implicit,
  title={The implicit bias of gradient descent on separable data},
  author={Soudry, Daniel and Hoffer, Elad and Nacson, Mor Shpigel and Gunasekar, Suriya and Srebro, Nathan},
  journal={Journal of Machine Learning Research},
  year={2018}
}


@article{gao2024expressive,
  title={On the expressive power of a variant of the looped transformer},
  author={Gao, Yihang and Zheng, Chuanyang and Xie, Enze and Shi, Han and Hu, Tianyang and Li, Yu and Ng, Michael K and Li, Zhenguo and Liu, Zhaoqiang},
  journal={arXiv preprint arXiv:2402.13572},
  year={2024}
}

@article{de2024simulation,
  title={Simulation of graph algorithms with looped transformers},
  author={de Luca, Artur Back and Fountoulakis, Kimon},
  journal={arXiv preprint arXiv:2402.01107},
  year={2024}
}

@inproceedings{shazeer2018adafactor,
  title={Adafactor: Adaptive learning rates with sublinear memory cost},
  author={Shazeer, Noam and Stern, Mitchell},
  booktitle={International Conference on Machine Learning},
  pages={4596--4604},
  year={2018},
  organization={PMLR}
}

@article{petty2023impact,
  title={The impact of depth and width on transformer language model generalization},
  author={Petty, Jackson and van Steenkiste, Sjoerd and Dasgupta, Ishita and Sha, Fei and Garrette, Dan and Linzen, Tal},
  journal={arXiv preprint arXiv:2310.19956},
  year={2023}
}

@article{nogueira2021investigating,
  title={Investigating the limitations of transformers with simple arithmetic tasks},
  author={Nogueira, Rodrigo and Jiang, Zhiying and Lin, Jimmy},
  journal={arXiv preprint arXiv:2102.13019},
  year={2021}
}


@article{reddy2019coqa,
    title = "{C}o{QA}: A Conversational Question Answering Challenge",
    author = "Reddy, Siva  and
      Chen, Danqi  and
      Manning, Christopher D.",
    journal = "Transactions of the Association for Computational Linguistics",
    year = "2019",
}

@inproceedings{choi2018quac,
    title = "{Q}u{AC}: Question Answering in Context",
    author = "Choi, Eunsol  and
      He, He  and
      Iyyer, Mohit  and
      Yatskar, Mark  and
      Yih, Wen-tau  and
      Choi, Yejin  and
      Liang, Percy  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    year = "2018",
}

@inproceedings{dua2019drop,
  author={Dheeru Dua and Yizhong Wang and Pradeep Dasigi and Gabriel Stanovsky and Sameer Singh and Matt Gardner},
  title={  {DROP}: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs},
  booktitle={Proc. of NAACL},
  year={2019}
}

@inproceedings{rajpurkar2018know,
    title = "Know What You Don{'}t Know: Unanswerable Questions for {SQ}u{AD}",
    author = "Rajpurkar, Pranav  and
      Jia, Robin  and
      Liang, Percy",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    year = "2018",
    address = "Melbourne, Australia",
}

@inproceedings{talmor2018web,
  title={The Web as a Knowledge-Base for Answering Complex Questions},
  author={Talmor, Alon and Berant, Jonathan},
  booktitle={Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
  year={2018}
}


@article{kwiatkowski2019natural,
    title = "Natural Questions: A Benchmark for Question Answering Research",
    author = "Kwiatkowski, Tom  and
      Palomaki, Jennimaria  and
      Redfield, Olivia  and
      Collins, Michael  and
      Parikh, Ankur  and
      Alberti, Chris  and
      Epstein, Danielle  and
      Polosukhin, Illia  and
      Devlin, Jacob  and
      Lee, Kenton  and
      Toutanova, Kristina  and
      Jones, Llion  and
      Kelcey, Matthew  and
      Chang, Ming-Wei  and
      Dai, Andrew M.  and
      Uszkoreit, Jakob  and
      Le, Quoc  and
      Petrov, Slav",
    journal = "Transactions of the Association for Computational Linguistics",
    year = "2019",
}

@inproceedings{joshi2017triviaqa,
    title = "{T}rivia{QA}: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension",
    author = "Joshi, Mandar  and
      Choi, Eunsol  and
      Weld, Daniel  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    year = "2017",
}

@article{clark2020tydi,
    title = "{T}y{D}i {QA}: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages",
    author = "Clark, Jonathan H.  and
      Choi, Eunsol  and
      Collins, Michael  and
      Garrette, Dan  and
      Kwiatkowski, Tom  and
      Nikolaev, Vitaly  and
      Palomaki, Jennimaria",
    journal = "Transactions of the Association for Computational Linguistics",
    year = "2020",
}


@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  year={2019}
}


@inproceedings{koncel2016mawps,
  title={MAWPS: A math word problem repository},
  author={Koncel-Kedziorski, Rik and Roy, Subhro and Amini, Aida and Kushman, Nate and Hajishirzi, Hannaneh},
  booktitle={Proceedings of the 2016 conference of the north american chapter of the association for computational linguistics: human language technologies},
  year={2016}
}

@inproceedings{miao2020diverse,
  title={A Diverse Corpus for Evaluating and Developing English Math Word Problem Solvers},
  author={Miao, Shen-Yun and Liang, Chao-Chun and Su, Keh-Yih},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={975--984},
  year={2020}
}

@inproceedings{patel2021nlp,
  title={Are NLP Models really able to Solve Simple Math Word Problems?},
  author={Patel, Arkil and Bhattamishra, Satwik and Goyal, Navin},
  booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  year={2021},
  organization={Association for Computational Linguistics}
}


@article{liu2024mobilellm,
  title={Mobilellm: Optimizing sub-billion parameter language models for on-device use cases},
  author={Liu, Zechun and Zhao, Changsheng and Iandola, Forrest and Lai, Chen and Tian, Yuandong and Fedorov, Igor and Xiong, Yunyang and Chang, Ernie and Shi, Yangyang and Krishnamoorthi, Raghuraman and others},
  journal={arXiv preprint arXiv:2402.14905},
  year={2024}
}


@article{cho2024position,
  title={Position Coupling: Leveraging Task Structure for Improved Length Generalization of Transformers},
  author={Cho, Hanseul and Cha, Jaeyoung and Awasthi, Pranjal and Bhojanapalli, Srinadh and Gupta, Anupam and Yun, Chulhee},
  journal={arXiv preprint arXiv:2405.20671},
  year={2024}
}

@inproceedings{lee2024teaching,
    title={Teaching Arithmetic to Small Transformers},
    author={Nayoung Lee and Kartik Sreenivasan and Jason D. Lee and Kangwook Lee and Dimitris Papailiopoulos},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
}

@InProceedings{reddi2023efficient,
  title = 	 {Efficient Training of Language Models using Few-Shot Learning},
  author =       {Reddi, Sashank and Miryoosefi, Sobhan and Karp, Stefani and Krishnan, Shankar and Kale, Satyen and Kim, Seungyeon and Kumar, Sanjiv},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  year = 	 {2023},
}

@article{sun2023survey,
  title={A survey of reasoning with foundation models},
  author={Sun, Jiankai and Zheng, Chuanyang and Xie, Enze and Liu, Zhengying and Chu, Ruihang and Qiu, Jianing and Xu, Jiaqi and Ding, Mingyu and Li, Hongyang and Geng, Mengzhe and others},
  journal={arXiv preprint arXiv:2312.11562},
  year={2023}
}


@article{sanford2024understanding,
  title={Understanding transformer reasoning capabilities via graph algorithms},
  author={Sanford, Clayton and Fatemi, Bahare and Hall, Ethan and Tsitsulin, Anton and Kazemi, Mehran and Halcrow, Jonathan and Perozzi, Bryan and Mirrokni, Vahab},
  journal={arXiv preprint arXiv:2405.18512},
  year={2024}
}


@article{feng2024towards,
  title={Towards revealing the mystery behind chain of thought: a theoretical perspective},
  author={Feng, Guhao and Zhang, Bohang and Gu, Yuntian and Ye, Haotian and He, Di and Wang, Liwei},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}


@article{liang2023holistic,
  title={Holistic Evaluation of Language Models},
  author={Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and others},
  journal={Transactions on Machine Learning Research},
  year={2023}
}

@inproceedings{tay2022ul2,
  title={Ul2: Unifying language learning paradigms},
  author={Tay, Yi and Dehghani, Mostafa and Tran, Vinh Q and Garcia, Xavier and Wei, Jason and Wang, Xuezhi and Chung, Hyung Won and Bahri, Dara and Schuster, Tal and Zheng, Steven and others},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@article{gao2020pile,
  title={The pile: An 800gb dataset of diverse text for language modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and others},
  journal={arXiv preprint arXiv:2101.00027},
  year={2020}
}

@article{mohtashami2023cotformer,
  title={CoTFormer: More Tokens With Attention Make Up For Less Depth},
  author={Mohtashami, Amirkeivan and Pagliardini, Matteo and Jaggi, Martin},
  journal={arXiv preprint arXiv:2310.10845},
  year={2023}
}

@article{bai2019deep,
  title={Deep equilibrium models},
  author={Bai, Shaojie and Kolter, J Zico and Koltun, Vladlen},
  journal={Advances in neural information processing systems},
  year={2019}
}

@article{allen2024physics,
  title={Physics of language models: Part 3.3, knowledge capacity scaling laws},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi},
  journal={arXiv preprint arXiv:2404.05405},
  year={2024}
}

@article{saunshi2024inductive,
  title={On the inductive bias of stacking towards improving reasoning},
  author={Saunshi, Nikunj and Karp, Stefani and Krishnan, Shankar and Miryoosefi, Sobhan and Reddi, Sashank J. and Kumar, Sanjiv},
  journal={arXiv preprint arXiv:2409.19044},
  year={2024}
}


@inproceedings{liu2023same,
  title={Same pre-training loss, better downstream: Implicit bias matters for language models},
  author={Liu, Hong and Xie, Sang Michael and Li, Zhiyuan and Ma, Tengyu},
  booktitle={International Conference on Machine Learning},
  year={2023},
  organization={PMLR}
}

@InProceedings{saunshi22understanding,
  title = 	 {Understanding Contrastive Learning Requires Incorporating Inductive Biases},
  author =       {Saunshi, Nikunj and Ash, Jordan and Goel, Surbhi and Misra, Dipendra and Zhang, Cyril and Arora, Sanjeev and Kakade, Sham and Krishnamurthy, Akshay},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  year = 	 {2022},
}


@article{chen2024can,
  title={What Can Transformer Learn with Varying Depth? Case Studies on Sequence Learning Tasks},
  author={Chen, Xingwu and Zou, Difan},
  journal={arXiv preprint arXiv:2404.01601},
  year={2024}
}

@InProceedings{back2024simulation,
  title = 	 {Simulation of Graph Algorithms with Looped Transformers},
  author =       {Back De Luca, Artur and Fountoulakis, Kimon},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  year = 	 {2024},
}


@inproceedings{gatmiry2024can,
  title={Can Looped Transformers Learn to Implement Multi-step Gradient Descent for In-context Learning?},
  author={Gatmiry, Khashayar and Saunshi, Nikunj and Reddi, Sashank J and Jegelka, Stefanie and Kumar, Sanjiv},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024},
}

@article{yang2023looped,
  title={Looped transformers are better at learning learning algorithms},
  author={Yang, Liu and Lee, Kangwook and Nowak, Robert and Papailiopoulos, Dimitris},
  journal={arXiv preprint arXiv:2311.12424},
  year={2023}
}

@article{hoffmann2022training,
  title={Training compute-optimal large language models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{ye2024physics,
  title={Physics of language models: Part 2.1, grade-school math and the hidden reasoning process},
  author={Ye, Tian and Xu, Zicheng and Li, Yuanzhi and Allen-Zhu, Zeyuan},
  journal={arXiv preprint arXiv:2407.20311},
  year={2024}
}


@article{elhage2021mathematical,
  title={A mathematical framework for transformer circuits},
  author={Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and others},
  journal={Transformer Circuits Thread},
  volume={1},
  pages={1},
  year={2021}
}

@article{pfau2024let,
  title={Let's Think Dot by Dot: Hidden Computation in Transformer Language Models},
  author={Pfau, Jacob and Merrill, William and Bowman, Samuel R},
  journal={arXiv preprint arXiv:2404.15758},
  year={2024}
}

@article{goyal2023think,
  title={Think before you speak: Training language models with pause tokens},
  author={Goyal, Sachin and Ji, Ziwei and Rawat, Ankit Singh and Menon, Aditya Krishna and Kumar, Sanjiv and Nagarajan, Vaishnavh},
  journal={arXiv preprint arXiv:2310.02226},
  year={2023}
}

@article{sanford2024transformers,
  title={Transformers, parallel computation, and logarithmic depth},
  author={Sanford, Clayton and Hsu, Daniel and Telgarsky, Matus},
  journal={arXiv preprint arXiv:2402.09268},
  year={2024}
}

@article{yao2024tree,
  title={Tree of thoughts: Deliberate problem solving with large language models},
  author={Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{gaur2023reasoning,
  title={Reasoning in Large Language Models Through Symbolic Math Word Problems},
  author={Gaur, Vedant and Saunshi, Nikunj},
  booktitle={The 61st Annual Meeting Of The Association For Computational Linguistics},
  year={2023}
}

@article{wei2022emergent,
  title={Emergent abilities of large language models},
  author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
  journal={arXiv preprint arXiv:2206.07682},
  year={2022}
}

@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  year={2020}
}

@inproceedings{dehghani2018universal,
  title={Universal Transformers},
  author={Dehghani, Mostafa and Gouws, Stephan and Vinyals, Oriol and Uszkoreit, Jakob and Kaiser, Lukasz},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{giannou2023looped,
  title={Looped transformers as programmable computers},
  author={Giannou, Angeliki and Rajput, Shashank and Sohn, Jy-yong and Lee, Kangwook and Lee, Jason D and Papailiopoulos, Dimitris},
  booktitle={International Conference on Machine Learning},
  year={2023},
  organization={PMLR}
}

@inproceedings{lan2019albert,
    title={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},
    author={Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
    booktitle={International Conference on Learning Representations},
    year={2020},
}

@article{zelikman2022star,
  title={Star: Bootstrapping reasoning with reasoning},
  author={Zelikman, Eric and Wu, Yuhuai and Mu, Jesse and Goodman, Noah},
  journal={Advances in Neural Information Processing Systems},
  year={2022}
}

@misc{nye2021work,
      title={Show Your Work: Scratchpads for Intermediate Computation with Language Models}, 
      author={Maxwell Nye and Anders Johan Andreassen and Guy Gur-Ari and Henryk Michalewski and Jacob Austin and David Bieber and David Dohan and Aitor Lewkowycz and Maarten Bosma and David Luan and Charles Sutton and Augustus Odena},
      year={2021},
      eprint={2112.00114},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{rajani2019explain,
      title={Explain Yourself! Leveraging Language Models for Commonsense Reasoning}, 
      author={Nazneen Fatema Rajani and Bryan McCann and Caiming Xiong and Richard Socher},
      year={2019},
      eprint={1906.02361},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{lanham2023measuring,
  title={Measuring faithfulness in chain-of-thought reasoning},
  author={Lanham, Tamera and Chen, Anna and Radhakrishnan, Ansh and Steiner, Benoit and Denison, Carson and Hernandez, Danny and Li, Dustin and Durmus, Esin and Hubinger, Evan and Kernion, Jackson and others},
  journal={arXiv preprint arXiv:2307.13702},
  year={2023}
}

@article{kojima2022large,
  title={Large language models are zero-shot reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={Advances in neural information processing systems},
  year={2022}
}

@article{gunasekar2023textbooks,
  title={Textbooks are all you need},
  author={Gunasekar, Suriya and Zhang, Yi and Aneja, Jyoti and Mendes, Caio C{\'e}sar Teodoro and Del Giorno, Allie and Gopi, Sivakanth and Javaheripi, Mojan and Kauffmann, Piero and de Rosa, Gustavo and Saarikivi, Olli and others},
  journal={arXiv preprint arXiv:2306.11644},
  year={2023}
}

@article{wei2021finetuned,
  title={Finetuned language models are zero-shot learners},
  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent Y and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  journal={arXiv preprint arXiv:2109.01652},
  year={2021}
}

@inproceedings{
li2024chain,
title={Chain of Thought Empowers Transformers to Solve Inherently Serial Problems},
author={Zhiyuan Li and Hong Liu and Denny Zhou and Tengyu Ma},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=3EWTEy9MTM}
}


@article{huang2022large,
  title={Large language models can self-improve},
  author={Huang, Jiaxin and Gu, Shixiang Shane and Hou, Le and Wu, Yuexin and Wang, Xuezhi and Yu, Hongkun and Han, Jiawei},
  journal={arXiv preprint arXiv:2210.11610},
  year={2022}
}

@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{google2023palm2,
      title={PaLM 2 Technical Report}, 
      author={Google},
      year={2023},
      url={https://ai.google/static/documents/palm2techreport.pdf},
}

@article{chen2022program,
  title={Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks},
  author={Chen, Wenhu and Ma, Xueguang and Wang, Xinyi and Cohen, William W},
  journal={arXiv preprint arXiv:2211.12588},
  year={2022}
}

@article{gao2022pal,
  title={PAL: Program-aided Language Models},
  author={Gao, Luyu and Madaan, Aman and Zhou, Shuyan and Alon, Uri and Liu, Pengfei and Yang, Yiming and Callan, Jamie and Neubig, Graham},
  journal={arXiv preprint arXiv:2211.10435},
  year={2022}
}

@article{ho2022large,
  title={Large Language Models Are Reasoning Teachers},
  author={Ho, Namgyu and Schmid, Laura and Yun, Se-Young},
  journal={arXiv preprint arXiv:2212.10071},
  year={2022}
}
@article{yang2023mm,
  title={Mm-react: Prompting chatgpt for multimodal reasoning and action},
  author={Yang, Zhengyuan and Li, Linjie and Wang, Jianfeng and Lin, Kevin and Azarnasab, Ehsan and Ahmed, Faisal and Liu, Zicheng and Liu, Ce and Zeng, Michael and Wang, Lijuan},
  journal={arXiv preprint arXiv:2303.11381},
  year={2023}
}
@article{zhang2023multimodal,
  title={Multimodal chain-of-thought reasoning in language models},
  author={Zhang, Zhuosheng and Zhang, Aston and Li, Mu and Zhao, Hai and Karypis, George and Smola, Alex},
  journal={arXiv preprint arXiv:2302.00923},
  year={2023}
}
@article{drozdov2022compositional,
  title={Compositional semantic parsing with large language models},
  author={Drozdov, Andrew and Sch{\"a}rli, Nathanael and Aky{\"u}rek, Ekin and Scales, Nathan and Song, Xinying and Chen, Xinyun and Bousquet, Olivier and Zhou, Denny},
  journal={arXiv preprint arXiv:2209.15003},
  year={2022}
}
@article{yao2022react,
  title={React: Synergizing reasoning and acting in language models},
  author={Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
  journal={arXiv preprint arXiv:2210.03629},
  year={2022}
}
@article{srivastava2022beyond,
  title={Beyond the imitation game: Quantifying and extrapolating the capabilities of language models},
  author={Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adri{\`a} and others},
  journal={arXiv preprint arXiv:2206.04615},
  year={2022}
}

@article{rae2021scaling,
  title={Scaling language models: Methods, analysis \& insights from training gopher},
  author={Rae, Jack W and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and others},
  journal={arXiv preprint arXiv:2112.11446},
  year={2021}
}
@article{chowdhery2022palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}
@article{lu2022learn,
  title={Learn to explain: Multimodal reasoning via thought chains for science question answering},
  author={Lu, Pan and Mishra, Swaroop and Xia, Tanglin and Qiu, Liang and Chang, Kai-Wei and Zhu, Song-Chun and Tafjord, Oyvind and Clark, Peter and Kalyan, Ashwin},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={2507--2521},
  year={2022}
}
@article{menick2022teaching,
  title={Teaching language models to support answers with verified quotes},
  author={Menick, Jacob and Trebacz, Maja and Mikulik, Vladimir and Aslanides, John and Song, Francis and Chadwick, Martin and Glaese, Mia and Young, Susannah and Campbell-Gillingham, Lucy and Irving, Geoffrey and others},
  journal={arXiv preprint arXiv:2203.11147},
  year={2022}
}
@article{zhang2022automatic,
  title={Automatic chain of thought prompting in large language models},
  author={Zhang, Zhuosheng and Zhang, Aston and Li, Mu and Smola, Alex},
  journal={arXiv preprint arXiv:2210.03493},
  year={2022}
}
@article{wang2022rationale,
  title={Rationale-augmented ensembles in language models},
  author={Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Zhou, Denny},
  journal={arXiv preprint arXiv:2207.00747},
  year={2022}
}
@article{lewkowycz2022solving,
  title={Solving quantitative reasoning problems with language models},
  author={Lewkowycz, Aitor and Andreassen, Anders and Dohan, David and Dyer, Ethan and Michalewski, Henryk and Ramasesh, Vinay and Slone, Ambrose and Anil, Cem and Schlag, Imanol and Gutman-Solo, Theo and others},
  journal={Advances in Neural Information Processing Systems},
  year={2022}
}
@article{zhou2022least,
  title={Least-to-most prompting enables complex reasoning in large language models},
  author={Zhou, Denny and Sch{\"a}rli, Nathanael and Hou, Le and Wei, Jason and Scales, Nathan and Wang, Xuezhi and Schuurmans, Dale and Bousquet, Olivier and Le, Quoc and Chi, Ed},
  journal={International Conference on Learning Representations (ICLR)},
  year={2023}
}
@article{lampinen2022can,
  title={Can language models learn from explanations in context?},
  author={Lampinen, Andrew K and Dasgupta, Ishita and Chan, Stephanie CY and Matthewson, Kory and Tessler, Michael Henry and Creswell, Antonia and McClelland, James L and Wang, Jane X and Hill, Felix},
  journal={arXiv preprint arXiv:2204.02329},
  year={2022}
}

@article{shi2022language,
  title={Language models are multilingual chain-of-thought reasoners},
  author={Shi, Freda and Suzgun, Mirac and Freitag, Markus and Wang, Xuezhi and Srivats, Suraj and Vosoughi, Soroush and Chung, Hyung Won and Tay, Yi and Ruder, Sebastian and Zhou, Denny and others},
  journal={International Conference on Machine Learning },
  year={2023}
}
@article{suzgun2022challenging,
  title={Challenging BIG-Bench tasks and whether chain-of-thought can solve them},
  author={Suzgun, Mirac and Scales, Nathan and Sch{\"a}rli, Nathanael and Gehrmann, Sebastian and Tay, Yi and Chung, Hyung Won and Chowdhery, Aakanksha and Le, Quoc V and Chi, Ed H and Zhou, Denny and others},
  journal={arXiv preprint arXiv:2210.09261},
  year={2022}
}
@article{liu2022transformers,
  title={Transformers learn shortcuts to automata},
  author={Liu, Bingbin and Ash, Jordan T and Goel, Surbhi and Krishnamurthy, Akshay and Zhang, Cyril},
  journal={arXiv preprint arXiv:2210.10749},
  year={2022}
}

@article{wei2022chain,
  title={Chain of thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Chi, Ed and Le, Quoc and Zhou, Denny},
  journal={Advances in Neural Information Processing Systems},
  year={2022}
}

@article{wang2022towards,
  title={Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters},
  author={Wang, Boshi and Min, Sewon and Deng, Xiang and Shen, Jiaming and Wu, You and Zettlemoyer, Luke and Sun, Huan},
  journal={arXiv preprint arXiv:2212.10001},
  year={2022}
}

@article{madaan2022text,
  title={Text and patterns: For effective chain of thought, it takes two to tango},
  author={Madaan, Aman and Yazdanbakhsh, Amir},
  journal={arXiv preprint arXiv:2209.07686},
  year={2022}
}

@article{ba2016layer,
  title={Layer normalization},
  author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016}
}

@article{barrington1986bounded,
  title={Bounded-width polynomial-size branching programs recognize exactly those languages in NC},
  author={Barrington, David A.},
  booktitle={Proceedings of the eighteenth annual ACM symposium on Theory of computing},
  pages={1--5},
  year={1986}
}

@article{yao1989circuits,
  title={Circuits and local computation},
  author={Yao, Andrew Chi-Chih},
  booktitle={Proceedings of the twenty-first annual ACM symposium on Theory of computing},
  pages={186--196},
  year={1989}
}

@article{pippenger1979relations,
  title={Relations among complexity measures},
  author={Pippenger, Nicholas and Fischer, Michael J},
  journal={Journal of the ACM (JACM)},
  volume={26},
  number={2},
  pages={361--381},
  year={1979},
  publisher={ACM New York, NY, USA}
}

@article{wang2022self,
  title={Self-consistency improves chain of thought reasoning in language models},
  author={Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and  Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  journal={International Conference on Learning Representations (ICLR)},
  year={2023}
}

@article{rivest1996time,
  title={Time-lock puzzles and timed-release crypto},
  author={Rivest, Ronald L and Shamir, Adi and Wagner, David A},
  year={1996},
  publisher={Massachusetts Institute of Technology. Laboratory for Computer Science}
}

@inproceedings{lombardi2020fiat,
  title={Fiat-Shamir for repeated squaring with applications to PPAD-hardness and VDFs},
  author={Lombardi, Alex and Vaikuntanathan, Vinod},
  booktitle={Advances in Cryptology--CRYPTO 2020: 40th Annual International Cryptology Conference, CRYPTO 2020, Santa Barbara, CA, USA, August 17--21, 2020, Proceedings, Part III},
  pages={632--651},
  year={2020},
  organization={Springer}
}

@article{hahn2020theoretical,
  title={Theoretical limitations of self-attention in neural sequence models},
  author={Hahn, Michael},
  journal={Transactions of the Association for Computational Linguistics},
  volume={8},
  pages={156--171},
  year={2020},
  publisher={MIT Press}
}

@article{merrill2021power,
  title={On the power of saturated transformers: A view from circuit complexity},
  author={Merrill, William and Goldberg, Yoav and Smith, Noah A},
  journal={arXiv preprint arXiv:2106.16213},
  year={2021}
}

@inproceedings{weiss2021thinking,
  title={Thinking like transformers},
  author={Weiss, Gail and Goldberg, Yoav and Yahav, Eran},
  booktitle={International Conference on Machine Learning},
  pages={11080--11090},
  year={2021},
  organization={PMLR}
}

@article{perez2021attention,
  title={Attention is turing complete},
  author={P{\'e}rez, Jorge and Barcel{\'o}, Pablo and Marinkovic, Javier},
  journal={The Journal of Machine Learning Research},
  volume={22},
  number={1},
  pages={3463--3497},
  year={2021},
  publisher={JMLRORG}
}

@article{clark2019does,
  title={What does bert look at? an analysis of bert's attention},
  author={Clark, Kevin and Khandelwal, Urvashi and Levy, Omer and Manning, Christopher D},
  journal={arXiv preprint arXiv:1906.04341},
  year={2019}
}

@article{vig2019visualizing,
  title={Visualizing attention in transformer-based language representation models},
  author={Vig, Jesse},
  journal={arXiv preprint arXiv:1904.02679},
  year={2019}
}

@article{wang2022interpretability,
  title={Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small},
  author={Wang, Kevin and Variengien, Alexandre and Conmy, Arthur and Shlegeris, Buck and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2211.00593},
  year={2022}
}


@article{goldberg1991every,
  title={What every computer scientist should know about floating-point arithmetic},
  author={Goldberg, David},
  journal={ACM computing surveys (CSUR)},
  volume={23},
  number={1},
  pages={5--48},
  year={1991},
  publisher={ACM New York, NY, USA}
}

@inproceedings{pham2020problems,
  title={Problems and opportunities in training deep learning software systems: An analysis of variance},
  author={Pham, Hung Viet and Qian, Shangshu and Wang, Jiannan and Lutellier, Thibaud and Rosenthal, Jonathan and Tan, Lin and Yu, Yaoliang and Nagappan, Nachiappan},
  booktitle={Proceedings of the 35th IEEE/ACM international conference on automated software engineering},
  pages={771--783},
  year={2020}
}

@article{zhuang2022randomness,
  title={Randomness in neural network training: Characterizing the impact of tooling},
  author={Zhuang, Donglin and Zhang, Xingyao and Song, Shuaiwen and Hooker, Sara},
  journal={Proceedings of Machine Learning and Systems},
  volume={4},
  pages={316--336},
  year={2022}
}

@inproceedings{summers2021nondeterminism,
  title={Nondeterminism and instability in neural network optimization},
  author={Summers, Cecilia and Dinneen, Michael J},
  booktitle={International Conference on Machine Learning},
  pages={9913--9922},
  year={2021},
  organization={PMLR}
}

@article{defour2014increasing,
  title={Increasing predictability of gpu's},
  author={Defour, David},
  year={2014}
}

@inproceedings{chou2020deterministic,
  title={Deterministic atomic buffering},
  author={Chou, Yuan Hsi and Ng, Christopher and Cattell, Shaylin and Intan, Jeremy and Sinclair, Matthew D and Devietti, Joseph and Rogers, Timothy G and Aamodt, Tor M},
  booktitle={2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)},
  pages={981--995},
  year={2020},
  organization={IEEE}
}

@incollection{maler2010krohn,
  title={On the Krohn-Rhodes cascaded decomposition theorem},
  author={Maler, Oded},
  booktitle={Time for Verification: Essays in Memory of Amir Pnueli},
  pages={260--278},
  year={2010},
  publisher={Springer}
}

@book{mcnaughton1971counter,
  title={Counter-Free Automata (MIT research monograph no. 65)},
  author={McNaughton, Robert and Papert, Seymour A},
  year={1971},
  publisher={The MIT Press}
}

@inproceedings{chandra1983unbounded,
  title={Unbounded fan-in circuits and associative functions},
  author={Chandra, Ashok K and Fortune, Steven and Lipton, Richard},
  booktitle={Proceedings of the fifteenth annual ACM symposium on Theory of computing},
  pages={52--60},
  year={1983}
}


@inproceedings{hesse2001division,
  title={Division is in uniform TC0},
  author={Hesse, William},
  booktitle={International Colloquium on Automata, Languages, and Programming},
  pages={104--114},
  year={2001},
  organization={Springer}
}

@article{wilson1985relativized,
  title={Relativized circuit complexity},
  author={Wilson, Christopher B},
  journal={Journal of Computer and System Sciences},
  volume={31},
  number={2},
  pages={169--181},
  year={1985},
  publisher={Elsevier}
}

@article{feng2023towards,
  title={Towards Revealing the Mystery behind Chain of Thought: a Theoretical Perspective},
  author={Feng, Guhao and Gu, Yuntian and Zhang, Bohang and Ye, Haotian and He, Di and Wang, Liwei},
  journal={arXiv preprint arXiv:2305.15408},
  year={2023}
}

@article{Chowdhery2023,
  author  = {Aakanksha Chowdhery and Sharan Narang and Jacob Devlin and Maarten Bosma and Gaurav Mishra and Adam Roberts and Paul Barham and Hyung Won Chung and Charles Sutton and Sebastian Gehrmann and Parker Schuh and Kensen Shi and Sasha Tsvyashchenko and Joshua Maynez and Abhishek Rao and Parker Barnes and Yi Tay and Noam Shazeer and Vinodkumar Prabhakaran and Emily Reif and Nan Du and Ben Hutchinson and Reiner Pope and James Bradbury and Jacob Austin and Michael Isard and Guy Gur-Ari and Pengcheng Yin and Toju Duke and Anselm Levskaya and Sanjay Ghemawat and Sunipa Dev and Henryk Michalewski and Xavier Garcia and Vedant Misra and Kevin Robinson and Liam Fedus and Denny Zhou and Daphne Ippolito and David Luan and Hyeontaek Lim and Barret Zoph and Alexander Spiridonov and Ryan Sepassi and David Dohan and Shivani Agrawal and Mark Omernick and Andrew M. Dai and Thanumalayan Sankaranarayana Pillai and Marie Pellat and Aitor Lewkowycz and Erica Moreira and Rewon Child and Oleksandr Polozov and Katherine Lee and Zongwei Zhou and Xuezhi Wang and Brennan Saeta and Mark Diaz and Orhan Firat and Michele Catasta and Jason Wei and Kathy Meier-Hellstern and Douglas Eck and Jeff Dean and Slav Petrov and Noah Fiedel},
  title   = {PaLM: Scaling Language Modeling with Pathways},
  journal = {Journal of Machine Learning Research},
  year    = {2023},
  volume  = {24},
  number  = {240},
  pages   = {1--113},
}

@article{anil2023palm,
  title={Palm 2 technical report},
  author={Anil, Rohan and Dai, Andrew M and Firat, Orhan and Johnson, Melvin and Lepikhin, Dmitry and Passos, Alexandre and Shakeri, Siamak and Taropa, Emanuel and Bailey, Paige and Chen, Zhifeng and others},
  journal={arXiv preprint arXiv:2305.10403},
  year={2023}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{Anthropic2024Claude3,
  title={The Claude 3 Model Family: Opus, Sonnet, Haiku},
  author={Anthropic},
  url  = {https://www-cdn.anthropic.com},
  year={2024}
}

@Article{FunSearch2023,
  author  = {Romera-Paredes, Bernardino and Barekatain, Mohammadamin and Novikov, Alexander and Balog, Matej and Kumar, M. Pawan and Dupont, Emilien and Ruiz, Francisco J. R. and Ellenberg, Jordan and Wang, Pengming and Fawzi, Omar and Kohli, Pushmeet and Fawzi, Alhussein},
  journal = {Nature},
  title   = {Mathematical discoveries from program search with large language models},
  year    = {2023},
  doi     = {10.1038/s41586-023-06924-6}
}

@inproceedings{
yang2024large,
title={Large Language Models as Optimizers},
author={Chengrun Yang and Xuezhi Wang and Yifeng Lu and Hanxiao Liu and Quoc V Le and Denny Zhou and Xinyun Chen},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
}

@article{yasunaga2023large,
  title={Large language models as analogical reasoners},
  author={Yasunaga, Michihiro and Chen, Xinyun and Li, Yujia and Pasupat, Panupong and Leskovec, Jure and Liang, Percy and Chi, Ed H and Zhou, Denny},
  journal={arXiv preprint arXiv:2310.01714},
  year={2023}
}

@article{nye2021show,
  title={Show your work: Scratchpads for intermediate computation with language models},
  author={Nye, Maxwell and Andreassen, Anders Johan and Gur-Ari, Guy and Michalewski, Henryk and Austin, Jacob and Bieber, David and Dohan, David and Lewkowycz, Aitor and Bosma, Maarten and Luan, David and others},
  journal={arXiv preprint arXiv:2112.00114},
  year={2021}
}

@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@article{ling2017program,
  title={Program induction by rationale generation: Learning to solve and explain algebraic word problems},
  author={Ling, Wang and Yogatama, Dani and Dyer, Chris and Blunsom, Phil},
  journal={arXiv preprint arXiv:1705.04146},
  year={2017}
}

@article{chung2022scaling,
  title={Scaling instruction-finetuned language models},
  author={Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
  journal={arXiv preprint arXiv:2210.11416},
  year={2022}
}

@article{trinh2024solving,
  title={Solving olympiad geometry without human demonstrations},
  author={Trinh, Trieu H and Wu, Yuhuai and Le, Quoc V and He, He and Luong, Thang},
  journal={Nature},
  volume={625},
  number={7995},
  pages={476--482},
  year={2024},
  publisher={Nature Publishing Group}
}

@inproceedings{dong2019neural,
  title={Neural logic machines},
  author={Dong, Honghua and Mao, Jiayuan and Lin, Tian and Wang, Chong and Li, Lihong and Zhou, Denny},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@inproceedings{chen2019neural,
  title={Neural symbolic reader: Scalable integration of distributed and symbolic representations for reading comprehension},
  author={Chen, Xinyun and Liang, Chen and Yu, Adams Wei and Zhou, Denny and Song, Dawn and Le, Quoc V},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@inproceedings{reynolds2021prompt,
  title={Prompt programming for large language models: Beyond the few-shot paradigm},
  author={Reynolds, Laria and McDonell, Kyle},
  booktitle={Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
  pages={1--7},
  year={2021}
}


@inproceedings{xiong2020layer,
  title={On layer normalization in the transformer architecture},
  author={Xiong, Ruibin and Yang, Yunchang and He, Di and Zheng, Kai and Zheng, Shuxin and Xing, Chen and Zhang, Huishuai and Lan, Yanyan and Wang, Liwei and Liu, Tieyan},
  booktitle={International Conference on Machine Learning},
  pages={10524--10533},
  year={2020},
  organization={PMLR}
}

@ARTICLE{ieee754_2008,
  author={IEEE},
  journal={IEEE Std 754-2008}, 
  title={IEEE Standard for Floating-Point Arithmetic}, 
  year={2008},
  volume={},
  number={},
  pages={1-70},
  doi={10.1109/IEEESTD.2008.4610935}}

@inproceedings{merrill2023logic,
  title={A Logic for Expressing Log-Precision Transformers},
  author={Merrill, William and Sabharwal, Ashish},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
  year={2023}
}  

@article{merrill2023parallelism,
  title={The parallelism tradeoff: Limitations of log-precision transformers},
  author={Merrill, William and Sabharwal, Ashish},
  journal={Transactions of the Association for Computational Linguistics},
  volume={11},
  pages={531--545},
  year={2023},
  publisher={MIT Press}
}

@article{merrill2022saturated,
  title={Saturated transformers are constant-depth threshold circuits},
  author={Merrill, William and Sabharwal, Ashish and Smith, Noah A},
  journal={Transactions of the Association for Computational Linguistics},
  volume={10},
  pages={843--856},
  year={2022},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{chiang2023tighter,
  title={Tighter Bounds on the Expressivity of Transformer Encoders},
  author={Chiang, David and Cholak, Peter and Pillay, Anand},
  journal={arXiv preprint arXiv:2301.10743},
  year={2023}
}

@article{hao2022formal,
  title={Formal language recognition by hard attention transformers: Perspectives from circuit complexity},
  author={Hao, Yiding and Angluin, Dana and Frank, Robert},
  journal={Transactions of the Association for Computational Linguistics},
  volume={10},
  pages={800--810},
  year={2022},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}


@article{huang2022large,
  title={Large language models can self-improve},
  author={Huang, Jiaxin and Gu, Shixiang Shane and Hou, Le and Wu, Yuexin and Wang, Xuezhi and Yu, Hongkun and Han, Jiawei},
  journal={arXiv preprint arXiv:2210.11610},
  year={2022}
}

@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{google2023palm2,
      title={PaLM 2 Technical Report}, 
      author={Google},
      year={2023},
      url={https://ai.google/static/documents/palm2techreport.pdf},
}

@article{chen2022program,
  title={Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks},
  author={Chen, Wenhu and Ma, Xueguang and Wang, Xinyi and Cohen, William W},
  journal={arXiv preprint arXiv:2211.12588},
  year={2022}
}

@article{gao2022pal,
  title={PAL: Program-aided Language Models},
  author={Gao, Luyu and Madaan, Aman and Zhou, Shuyan and Alon, Uri and Liu, Pengfei and Yang, Yiming and Callan, Jamie and Neubig, Graham},
  journal={arXiv preprint arXiv:2211.10435},
  year={2022}
}

@article{ho2022large,
  title={Large Language Models Are Reasoning Teachers},
  author={Ho, Namgyu and Schmid, Laura and Yun, Se-Young},
  journal={arXiv preprint arXiv:2212.10071},
  year={2022}
}
@article{yang2023mm,
  title={Mm-react: Prompting chatgpt for multimodal reasoning and action},
  author={Yang, Zhengyuan and Li, Linjie and Wang, Jianfeng and Lin, Kevin and Azarnasab, Ehsan and Ahmed, Faisal and Liu, Zicheng and Liu, Ce and Zeng, Michael and Wang, Lijuan},
  journal={arXiv preprint arXiv:2303.11381},
  year={2023}
}
@article{zhang2023multimodal,
  title={Multimodal chain-of-thought reasoning in language models},
  author={Zhang, Zhuosheng and Zhang, Aston and Li, Mu and Zhao, Hai and Karypis, George and Smola, Alex},
  journal={arXiv preprint arXiv:2302.00923},
  year={2023}
}
@article{drozdov2022compositional,
  title={Compositional semantic parsing with large language models},
  author={Drozdov, Andrew and Sch{\"a}rli, Nathanael and Aky{\"u}rek, Ekin and Scales, Nathan and Song, Xinying and Chen, Xinyun and Bousquet, Olivier and Zhou, Denny},
  journal={arXiv preprint arXiv:2209.15003},
  year={2022}
}
@article{yao2022react,
  title={React: Synergizing reasoning and acting in language models},
  author={Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
  journal={arXiv preprint arXiv:2210.03629},
  year={2022}
}
@article{srivastava2022beyond,
  title={Beyond the imitation game: Quantifying and extrapolating the capabilities of language models},
  author={Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adri{\`a} and others},
  journal={arXiv preprint arXiv:2206.04615},
  year={2022}
}

@article{rae2021scaling,
  title={Scaling language models: Methods, analysis \& insights from training gopher},
  author={Rae, Jack W and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and others},
  journal={arXiv preprint arXiv:2112.11446},
  year={2021}
}
@article{chowdhery2022palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}
@article{lu2022learn,
  title={Learn to explain: Multimodal reasoning via thought chains for science question answering},
  author={Lu, Pan and Mishra, Swaroop and Xia, Tanglin and Qiu, Liang and Chang, Kai-Wei and Zhu, Song-Chun and Tafjord, Oyvind and Clark, Peter and Kalyan, Ashwin},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={2507--2521},
  year={2022}
}
@article{menick2022teaching,
  title={Teaching language models to support answers with verified quotes},
  author={Menick, Jacob and Trebacz, Maja and Mikulik, Vladimir and Aslanides, John and Song, Francis and Chadwick, Martin and Glaese, Mia and Young, Susannah and Campbell-Gillingham, Lucy and Irving, Geoffrey and others},
  journal={arXiv preprint arXiv:2203.11147},
  year={2022}
}
@article{zhang2022automatic,
  title={Automatic chain of thought prompting in large language models},
  author={Zhang, Zhuosheng and Zhang, Aston and Li, Mu and Smola, Alex},
  journal={arXiv preprint arXiv:2210.03493},
  year={2022}
}
@article{wang2022rationale,
  title={Rationale-augmented ensembles in language models},
  author={Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Zhou, Denny},
  journal={arXiv preprint arXiv:2207.00747},
  year={2022}
}
@article{lewkowycz2022solving,
  title={Solving quantitative reasoning problems with language models},
  author={Lewkowycz, Aitor and Andreassen, Anders and Dohan, David and Dyer, Ethan and Michalewski, Henryk and Ramasesh, Vinay and Slone, Ambrose and Anil, Cem and Schlag, Imanol and Gutman-Solo, Theo and others},
  journal={Advances in Neural Information Processing Systems},
  year={2022}
}
@article{zhou2022least,
  title={Least-to-most prompting enables complex reasoning in large language models},
  author={Zhou, Denny and Sch{\"a}rli, Nathanael and Hou, Le and Wei, Jason and Scales, Nathan and Wang, Xuezhi and Schuurmans, Dale and Bousquet, Olivier and Le, Quoc and Chi, Ed},
  journal={International Conference on Learning Representations (ICLR)},
  year={2023}
}
@article{lampinen2022can,
  title={Can language models learn from explanations in context?},
  author={Lampinen, Andrew K and Dasgupta, Ishita and Chan, Stephanie CY and Matthewson, Kory and Tessler, Michael Henry and Creswell, Antonia and McClelland, James L and Wang, Jane X and Hill, Felix},
  journal={arXiv preprint arXiv:2204.02329},
  year={2022}
}

@article{shi2022language,
  title={Language models are multilingual chain-of-thought reasoners},
  author={Shi, Freda and Suzgun, Mirac and Freitag, Markus and Wang, Xuezhi and Srivats, Suraj and Vosoughi, Soroush and Chung, Hyung Won and Tay, Yi and Ruder, Sebastian and Zhou, Denny and others},
  journal={International Conference on Machine Learning },
  year={2023}
}
@article{suzgun2022challenging,
  title={Challenging BIG-Bench tasks and whether chain-of-thought can solve them},
  author={Suzgun, Mirac and Scales, Nathan and Sch{\"a}rli, Nathanael and Gehrmann, Sebastian and Tay, Yi and Chung, Hyung Won and Chowdhery, Aakanksha and Le, Quoc V and Chi, Ed H and Zhou, Denny and others},
  journal={arXiv preprint arXiv:2210.09261},
  year={2022}
}

@article{kojima2022large,
  title={Large language models are zero-shot reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={Advances in Neural Information Processing Systems},
  year={2022}
}

@article{wang2022towards,
  title={Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters},
  author={Wang, Boshi and Min, Sewon and Deng, Xiang and Shen, Jiaming and Wu, You and Zettlemoyer, Luke and Sun, Huan},
  journal={arXiv preprint arXiv:2212.10001},
  year={2022}
}

@article{madaan2022text,
  title={Text and patterns: For effective chain of thought, it takes two to tango},
  author={Madaan, Aman and Yazdanbakhsh, Amir},
  journal={arXiv preprint arXiv:2209.07686},
  year={2022}
}

@article{ba2016layer,
  title={Layer normalization},
  author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016}
}

@article{yao1989circuits,
  title={Circuits and local computation},
  author={Yao, Andrew Chi-Chih},
  booktitle={Proceedings of the twenty-first annual ACM symposium on Theory of computing},
  pages={186--196},
  year={1989}
}

@article{pippenger1979relations,
  title={Relations among complexity measures},
  author={Pippenger, Nicholas and Fischer, Michael J},
  journal={Journal of the ACM (JACM)},
  volume={26},
  number={2},
  pages={361--381},
  year={1979},
  publisher={ACM New York, NY, USA}
}

@article{wang2022self,
  title={Self-consistency improves chain of thought reasoning in language models},
  author={Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and  Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  journal={International Conference on Learning Representations (ICLR)},
  year={2023}
}

@article{rivest1996time,
  title={Time-lock puzzles and timed-release crypto},
  author={Rivest, Ronald L and Shamir, Adi and Wagner, David A},
  year={1996},
  publisher={Massachusetts Institute of Technology. Laboratory for Computer Science}
}

@inproceedings{lombardi2020fiat,
  title={Fiat-Shamir for repeated squaring with applications to PPAD-hardness and VDFs},
  author={Lombardi, Alex and Vaikuntanathan, Vinod},
  booktitle={Advances in Cryptology--CRYPTO 2020: 40th Annual International Cryptology Conference, CRYPTO 2020, Santa Barbara, CA, USA, August 17--21, 2020, Proceedings, Part III},
  pages={632--651},
  year={2020},
  organization={Springer}
}

@inproceedings{edelman2022inductive,
  title={Inductive biases and variable creation in self-attention mechanisms},
  author={Edelman, Benjamin L and Goel, Surbhi and Kakade, Sham and Zhang, Cyril},
  booktitle={International Conference on Machine Learning},
  pages={5793--5831},
  year={2022},
  organization={PMLR}
}

@article{hahn2020theoretical,
  title={Theoretical limitations of self-attention in neural sequence models},
  author={Hahn, Michael},
  journal={Transactions of the Association for Computational Linguistics},
  volume={8},
  pages={156--171},
  year={2020},
  publisher={MIT Press}
}

@article{merrill2021power,
  title={On the power of saturated transformers: A view from circuit complexity},
  author={Merrill, William and Goldberg, Yoav and Smith, Noah A},
  journal={arXiv preprint arXiv:2106.16213},
  year={2021}
}

@inproceedings{weiss2021thinking,
  title={Thinking like transformers},
  author={Weiss, Gail and Goldberg, Yoav and Yahav, Eran},
  booktitle={International Conference on Machine Learning},
  pages={11080--11090},
  year={2021},
  organization={PMLR}
}

@article{perez2021attention,
  title={Attention is turing complete},
  author={P{\'e}rez, Jorge and Barcel{\'o}, Pablo and Marinkovic, Javier},
  journal={The Journal of Machine Learning Research},
  volume={22},
  number={1},
  pages={3463--3497},
  year={2021},
  publisher={JMLRORG}
}

@article{clark2019does,
  title={What does bert look at? an analysis of bert's attention},
  author={Clark, Kevin and Khandelwal, Urvashi and Levy, Omer and Manning, Christopher D},
  journal={arXiv preprint arXiv:1906.04341},
  year={2019}
}

@article{vig2019visualizing,
  title={Visualizing attention in transformer-based language representation models},
  author={Vig, Jesse},
  journal={arXiv preprint arXiv:1904.02679},
  year={2019}
}

@article{wang2022interpretability,
  title={Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small},
  author={Wang, Kevin and Variengien, Alexandre and Conmy, Arthur and Shlegeris, Buck and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2211.00593},
  year={2022}
}


@article{goldberg1991every,
  title={What every computer scientist should know about floating-point arithmetic},
  author={Goldberg, David},
  journal={ACM computing surveys (CSUR)},
  volume={23},
  number={1},
  pages={5--48},
  year={1991},
  publisher={ACM New York, NY, USA}
}

@inproceedings{pham2020problems,
  title={Problems and opportunities in training deep learning software systems: An analysis of variance},
  author={Pham, Hung Viet and Qian, Shangshu and Wang, Jiannan and Lutellier, Thibaud and Rosenthal, Jonathan and Tan, Lin and Yu, Yaoliang and Nagappan, Nachiappan},
  booktitle={Proceedings of the 35th IEEE/ACM international conference on automated software engineering},
  pages={771--783},
  year={2020}
}

@article{zhuang2022randomness,
  title={Randomness in neural network training: Characterizing the impact of tooling},
  author={Zhuang, Donglin and Zhang, Xingyao and Song, Shuaiwen and Hooker, Sara},
  journal={Proceedings of Machine Learning and Systems},
  volume={4},
  pages={316--336},
  year={2022}
}

@inproceedings{summers2021nondeterminism,
  title={Nondeterminism and instability in neural network optimization},
  author={Summers, Cecilia and Dinneen, Michael J},
  booktitle={International Conference on Machine Learning},
  pages={9913--9922},
  year={2021},
  organization={PMLR}
}

@article{defour2014increasing,
  title={Increasing predictability of gpu's},
  author={Defour, David},
  year={2014}
}

@inproceedings{chou2020deterministic,
  title={Deterministic atomic buffering},
  author={Chou, Yuan Hsi and Ng, Christopher and Cattell, Shaylin and Intan, Jeremy and Sinclair, Matthew D and Devietti, Joseph and Rogers, Timothy G and Aamodt, Tor M},
  booktitle={2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)},
  pages={981--995},
  year={2020},
  organization={IEEE}
}

@incollection{maler2010krohn,
  title={On the Krohn-Rhodes cascaded decomposition theorem},
  author={Maler, Oded},
  booktitle={Time for Verification: Essays in Memory of Amir Pnueli},
  pages={260--278},
  year={2010},
  publisher={Springer}
}

@article{krohn1965algebraic,
  title={Algebraic theory of machines. I. Prime decomposition theorem for finite semigroups and machines},
  author={Krohn, Kenneth and Rhodes, John},
  journal={Transactions of the American Mathematical Society},
  volume={116},
  year={1965}
}

@book{mcnaughton1971counter,
  title={Counter-Free Automata (MIT research monograph no. 65)},
  author={McNaughton, Robert and Papert, Seymour A},
  year={1971},
  publisher={The MIT Press}
}

@inproceedings{chandra1983unbounded,
  title={Unbounded fan-in circuits and associative functions},
  author={Chandra, Ashok K and Fortune, Steven and Lipton, Richard},
  booktitle={Proceedings of the fifteenth annual ACM symposium on Theory of computing},
  pages={52--60},
  year={1983}
}


@inproceedings{hesse2001division,
  title={Division is in uniform TC0},
  author={Hesse, William},
  booktitle={International Colloquium on Automata, Languages, and Programming},
  pages={104--114},
  year={2001},
  organization={Springer}
}

@article{wilson1985relativized,
  title={Relativized circuit complexity},
  author={Wilson, Christopher B},
  journal={Journal of Computer and System Sciences},
  volume={31},
  number={2},
  pages={169--181},
  year={1985},
  publisher={Elsevier}
}

@article{Chowdhery2023,
  author  = {Aakanksha Chowdhery and Sharan Narang and Jacob Devlin and Maarten Bosma and Gaurav Mishra and Adam Roberts and Paul Barham and Hyung Won Chung and Charles Sutton and Sebastian Gehrmann and Parker Schuh and Kensen Shi and Sasha Tsvyashchenko and Joshua Maynez and Abhishek Rao and Parker Barnes and Yi Tay and Noam Shazeer and Vinodkumar Prabhakaran and Emily Reif and Nan Du and Ben Hutchinson and Reiner Pope and James Bradbury and Jacob Austin and Michael Isard and Guy Gur-Ari and Pengcheng Yin and Toju Duke and Anselm Levskaya and Sanjay Ghemawat and Sunipa Dev and Henryk Michalewski and Xavier Garcia and Vedant Misra and Kevin Robinson and Liam Fedus and Denny Zhou and Daphne Ippolito and David Luan and Hyeontaek Lim and Barret Zoph and Alexander Spiridonov and Ryan Sepassi and David Dohan and Shivani Agrawal and Mark Omernick and Andrew M. Dai and Thanumalayan Sankaranarayana Pillai and Marie Pellat and Aitor Lewkowycz and Erica Moreira and Rewon Child and Oleksandr Polozov and Katherine Lee and Zongwei Zhou and Xuezhi Wang and Brennan Saeta and Mark Diaz and Orhan Firat and Michele Catasta and Jason Wei and Kathy Meier-Hellstern and Douglas Eck and Jeff Dean and Slav Petrov and Noah Fiedel},
  title   = {PaLM: Scaling Language Modeling with Pathways},
  journal = {Journal of Machine Learning Research},
  year    = {2023},
  volume  = {24},
  number  = {240},
  pages   = {1--113},
}

@article{anil2023palm,
  title={Palm 2 technical report},
  author={Anil, Rohan and Dai, Andrew M and Firat, Orhan and Johnson, Melvin and Lepikhin, Dmitry and Passos, Alexandre and Shakeri, Siamak and Taropa, Emanuel and Bailey, Paige and Chen, Zhifeng and others},
  journal={arXiv preprint arXiv:2305.10403},
  year={2023}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{Anthropic2024Claude3,
  title={The Claude 3 Model Family: Opus, Sonnet, Haiku},
  author={Anthropic},
  url  = {https://www-cdn.anthropic.com},
  year={2024}
}

@Article{FunSearch2023,
  author  = {Romera-Paredes, Bernardino and Barekatain, Mohammadamin and Novikov, Alexander and Balog, Matej and Kumar, M. Pawan and Dupont, Emilien and Ruiz, Francisco J. R. and Ellenberg, Jordan and Wang, Pengming and Fawzi, Omar and Kohli, Pushmeet and Fawzi, Alhussein},
  journal = {Nature},
  title   = {Mathematical discoveries from program search with large language models},
  year    = {2023},
  doi     = {10.1038/s41586-023-06924-6}
}

@inproceedings{
yang2024large,
title={Large Language Models as Optimizers},
author={Chengrun Yang and Xuezhi Wang and Yifeng Lu and Hanxiao Liu and Quoc V Le and Denny Zhou and Xinyun Chen},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
}

@article{yasunaga2023large,
  title={Large language models as analogical reasoners},
  author={Yasunaga, Michihiro and Chen, Xinyun and Li, Yujia and Pasupat, Panupong and Leskovec, Jure and Liang, Percy and Chi, Ed H and Zhou, Denny},
  journal={arXiv preprint arXiv:2310.01714},
  year={2023}
}

@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@article{ling2017program,
  title={Program induction by rationale generation: Learning to solve and explain algebraic word problems},
  author={Ling, Wang and Yogatama, Dani and Dyer, Chris and Blunsom, Phil},
  journal={arXiv preprint arXiv:1705.04146},
  year={2017}
}

@article{chung2022scaling,
  title={Scaling instruction-finetuned language models},
  author={Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
  journal={arXiv preprint arXiv:2210.11416},
  year={2022}
}

@article{trinh2024solving,
  title={Solving olympiad geometry without human demonstrations},
  author={Trinh, Trieu H and Wu, Yuhuai and Le, Quoc V and He, He and Luong, Thang},
  journal={Nature},
  volume={625},
  number={7995},
  pages={476--482},
  year={2024},
  publisher={Nature Publishing Group}
}

@inproceedings{dong2019neural,
  title={Neural logic machines},
  author={Dong, Honghua and Mao, Jiayuan and Lin, Tian and Wang, Chong and Li, Lihong and Zhou, Denny},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@inproceedings{chen2019neural,
  title={Neural symbolic reader: Scalable integration of distributed and symbolic representations for reading comprehension},
  author={Chen, Xinyun and Liang, Chen and Yu, Adams Wei and Zhou, Denny and Song, Dawn and Le, Quoc V},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@inproceedings{reynolds2021prompt,
  title={Prompt programming for large language models: Beyond the few-shot paradigm},
  author={Reynolds, Laria and McDonell, Kyle},
  booktitle={Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
  pages={1--7},
  year={2021}
}


@InProceedings{gong2019efficient,
  title = 	 {Efficient Training of {BERT} by Progressively Stacking},
  author =       {Gong, Linyuan and He, Di and Li, Zhuohan and Qin, Tao and Wang, Liwei and Liu, Tieyan},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  year = 	 {2019},
}
