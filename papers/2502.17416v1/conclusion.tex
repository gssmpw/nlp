\section{Conclusions, limitations and future work}
\vspace{-0.1in}

\looseness-1This work explores a new direction of ``looped models for reasoning''. Not only are looped models able to solve many reasoning problems with very fewer parameters, they also have an inductive bias towards disproportionately improving the reasoning performance of language models, over memorization. The theoretical results on the expressivity of looped models start to provide some hints into their depth-optimality.
% We hope that this exploration opens up many interesting research directions. 
While we test looped models on a subset of reasoning problems, a natural question is whether the results hold for many other forms of reasoning (e.g. multimodal and common-sense reasoning).
In particular, a succinct formalization of reasoning problems itself is an interesting future direction.
Furthermore, the inductive bias towards improved reasoning performance at the same perplexity is very intriguing and deserves further exploration.
We find the scaling behavior of looped models very fascinating, and the connections to {\em latent thoughts} and CoT reasoning start to provide hints into this behavior.
We hope this inspires future exploration on using looped models for more efficient inference-time scaling to aid with deeper reasoning.

% The initial connections to chain-of-thought    how to explain the inductive bias (its not about expressivity or generalization)? can we leverage this for even better reasoning?

% Limitations: reasoning is non-exhaustive, partly because it is a broad topic. can we get good language models along with parameter efficiency?