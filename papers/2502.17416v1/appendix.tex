
\appendix
\section{Experiments}
% You may include other additional sections here.

\subsection{Simple reasoning setup details}


\paragraph{$n$-ary addition.}
\label{sec:apx_addition}

All experiments are run on a standard Transformer architecture with input dimension of 256, 8 heads and 1024 hidden dimension in the feed-forward layers.  We train using Adafactor \citep{shazeer2018adafactor} employing a linear warmup coupled with a cosine decay schedule for the learning rate.
All runs use a batch size of 1024, learning rate of 0.005 and run for 200k steps.
This corresponds to 200M examples, which is insignificant compared to the total possible examples ($> 10^{320}$). Thus memorization of the answer is not an issue. Since training is a bit noisy, for each setting, we run 3 different random seeds and pick the run with the maximum average accuracy. We pick maximum instead of average because we care about expressivity power of these models.


\paragraph{$p$-hop induction.}
\label{sec:apx_khop}

We formally define the $p$-hop problem below:
\begin{definition}[$p$-$\hop$,~\cite{sanford2024transformers}]\label{defi:khop}
For a finite alphabet $\Sigma$, define the map $\hop_p: \Sigma^n \to \Sigma \cup \{\perp\}$ as $\hop_p(\vv) = v_{\find_p(\vv, n)}$ if $\find_p(\vv, n) \neq 0$ else $\perp$, where
\begin{align*}
    \find_1(\vv, i) &= \max\left(\{0\} \cup \{j \le i, v_{j-1} = v_i \}\right) \\
    \find_p(\vv, i) &= \find_1(\vv, \find_{p-1}(\vv, i)) \text{ for } p \ge 2.
\end{align*}
\end{definition}


\input{Table_Fig/cosines_cosreg}


\input{Table_Fig/cosines_baseline}

For the $p$-hop problem we sample instances randomly while enforcing that there always exists $p$-hops present in the input sequence. We do this by first picking the sequence of $p$-hops randomly and then shuffling them around in a sequence with filler tokens to be filled by the remaining characters. After the shuffle, we sample the remaining characters to occur in place of the filler tokens while respecting the $p$-hop order. Our train set consists of 4M examples and our test and validation sets consist of 262k examples each.
For all models we train on this dataset, the model dimension is 128, hidden dimension is 512 and 8 attention heads are used. Rotary positional encodings are used as well. We train using Adafactor for 200,000 steps with a batch size of 256 using a base learning rate of $10^{-3}$ and use a linear warmup coupled with a cosine decay schedule for the learning rate.


\paragraph{i-GSM.}
\label{sec:apx_igsm}
We describe how the i-GSM dataset is generated in more detail here.
We start with a hierarchy of entities of depth 4 from which we build a randomly sampled structure graph where directed edges connect entities in level $i$ to those in level $i+1$. Each edge in the structure graph defines a instance parameter which is an integer (for e.g. an edge between city center and car parks denotes the number of car parks in city center). We then construct a randomly sampled mathematical dependency graph which is a DAG over all the instance parameters by relating each to the others. Finally we pick one of the nodes of the dependency graph to query and the goal is to compute the numerical value of this node modulo some prime number $P$. For more details on the sampling process for the structure and dependency graphs, we refer the reader to \cite{ye2024physics}. We make 3 simplifications compared to \cite{ye2024physics}: we phrase our problems in a symbolic language without the English construction of sentences (see \Cref{fig:sample-eigsm-problem}); we do not allow abstract parameters in our problems; we perform arithmetic modulo $7$ as opposed to $23$.
Our train dataset consists of around 4 million examples and we test on around 50k examples. Given the significantly larger number of unique solution templates possible, train-test overlap in the problem template space is going to be limited with high probability. 
For all models we train on this dataset, the model dimension is 128, hidden dimension is 512 and 8 attention heads are used. Rotary positional encodings are used as well. We train using Adafactor for 200,000 steps with a batch size of 256 using a base learning rate of $10^{-3}$ and use a linear warmup coupled with a cosine decay schedule for the learning rate.


\input{Table_Fig/inductive_bias_8L}




\subsection{Language modeling setup}
\label{sec:apx_language_modeling}

We train on the Pile data using causal language modeling objective. The dataset is pre-processed and cached to ensure that all models are trained on exactly the same tokens in the same order.
For all experiments, we use a batch size of 512 and sequence length of 1280.
We use a cosine learning rate schedule decaying over 400k steps with a peak learning rate of 0.01, tuned based on the baseline model.
The base model is a 1.5B parameter decoder only Transformer model, with 24 layers, model dimensions of 2048, hidden dimension 5120 and 32 heads.
For the shallower baselines and looped models, we only change the number of layers and keep all other hyperparameters the same.



\subsection{Results for each task group}

In \Cref{sec:language_modeling} we discussed results for various task groups like closed book QA, math word problems etc. Here we present results for each individual task for completeness.
Detailed results for closed book QA are in \Cref{table:main_closedqa_results}, open book QA in \Cref{table:main_openqa_results}, math word problems in \Cref{table:main_mwp_results} and reasoning primitives in \Cref{table:main_primitives_results}.
These tables include, both, looped models from \Cref{table:language_modeling_results} and the models trained with regularization from \Cref{table:regularization_results}.




\input{Table_Fig/main_closedqa_results}

\input{Table_Fig/main_openqa_results}

\input{Table_Fig/main_mwp_results}

\input{Table_Fig/main_primitives_results}



