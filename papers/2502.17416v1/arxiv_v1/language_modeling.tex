\section{Language modeling with looped models}
\vspace{-0.01in}
\label{sec:language_modeling}


In this section, we pretrain and evaluate looped models for causal language models.
We train models on 250B tokens of the Pile dataset \citep{gao2020pile} and use a 24-layer 1B parameter model for most experiments, motivated by the setting in \citet{tay2022ul2} (refer to \Cref{sec:apx_language_modeling} for more details).
\vspace{-0.1in}

\subsection{Experiments with 1B language modeling}
\label{sec:expt_1B}

For causal language modeling, we pretrain various looped models on the standard GPT2-style next token prediction objective \citep{radford2019language}.
We train models with different parameter budgets to make sure that the findings are robust.
We remind the reader that the notation \loopy{k}{L} corresponds to a $k$ layer model looped $L$ times.
For each setting, we compare 3 models: \textbf{(a)} \loopy{24}{1}: 24-layer 1B model, \textbf{(b)} \loopy{k}{1}: $k$-layer model with the same configuration as the 24-layer model for other dimensions, \textbf{(c)} \loopy{k}{24/k}: $k$-layer model looped $24/k$ times to match the parameter count of (b) and match the effective depth/FLOPs of (a).
We run experiments for $k\in\{4, 6, 8, 12\}$ to ensure that the findings are robust.
After pretraining on Pile, we evaluate the models on validation perplexity and on downstream benchmarks using  $k$-shot evaluations. Results are summarized in \Cref{table:language_modeling_results}






\input{Table_Fig/main_results}


\looseness-1\textbf{Evaluation metrics.}
We evaluate the models on perplexity metric after training is completed. Since there is growing evidence that perplexity, although very useful for training, is a narrow measure of model quality, we also track more holistic downstream evaluations~\citep{liang2023holistic}.
Thus, we evaluate the model on 4 important slices: closed book QA, open book QA, math word problems and reasoning primitives. These comprise of {\bf 19 different tasks} in total.

\begin{itemize}[leftmargin=0.6cm]
    \item {\bf Closed book QA}: This includes tasks like TriviaQA \citep{joshi2017triviaqa}, TydiQA-NoContext \citep{clark2020tydi}, Natural Questions \citep{kwiatkowski2019natural} and Web Questions \citep{talmor2018web} that test the model's ability to answer questions without any context, and thus, primarily measure the memorization abilities of language models.
    
    \item {\bf Open book QA}: This includes tasks like TydiQA-GoldP \citep{clark2020tydi}, SquadV2 \citep{rajpurkar2018know}, Drop \citep{dua2019drop}, QuAC \citep{choi2018quac}, CoQA \citep{reddy2019coqa} that evaluate the model's ability to infer the answer to a question from the extra context that is provided, akin to reading comprehension.
    
    \item {\bf Math word problems}: To evaluate the model's ability to reason, we test them on math word problems considered in \citep{wei2022chain}. This includes tasks like SVAMP \citep{patel2021nlp}, ASDiv \citep{miao2020diverse}, the MAWPS benchmark \citep{koncel2016mawps}.
    % and GSM8K \citep{cobbe2021training}.
    We report 5-shot evaluation for the pretrained model on these tasks.
    
    \item {\bf Reasoning primitives}: \citet{saunshi2024inductive} introduced these datasets to study the inductive bias of stacking towards improving reasoning, by isolating simple reasoning abilities. One such primitive is depth-$k$ variable assignment that requires the model to resolve a chain of assignments of length $k$. 
    % This is very related to the $k$-hop induction problem from \Cref{sec:khop_desc}. 
    An example of depth-0 var-assign is \textit{a=1, b=2, c=6, b=?}, and example for depth-1 var-assign is \textit{a=1, b=2, c=a, d=b, d=?}. We evaluate on the math and coding variants of the depth-0 and depth-1 problems using 5-shot evaluation.
        
\end{itemize}

For each task group $G$ from above, in \Cref{table:language_modeling_results} we report the average accuracy for that task group, denoted by $\text{Avg}_{G}$. Furthermore, for each layer budget $k$, we report the {\bf \% gap} between the iso-param and iso-flop models that is covered by the looped model. More specifically
% \vspace{-0.1in}
\begin{align}
    \text{\% Gap} = \frac{\text{Avg}_{G}\text{\loopy{k}{24/k}} - \text{Avg}_{G}\text{\loopy{k}{1}}}{\text{Avg}_{G}\text{\loopy{24}{1}} - \text{Avg}_{G}\text{\loopy{k}{1}}}\label{eq:gap_pct}.
\end{align}
% \vspace{-0.1in}
\looseness-1This measures how effectively looping can bridge the gap between iso-param and iso-flops baselines. Implicitly, it measures how different task groups behave when a model only has a few parameters but is given depth through looping.
\text{\% Gap} being closer to 0\% means that providing depth via looping does not benefit the task group, and number of parameters is the most important factor. On the other hand, \text{\% Gap} closer to 100\% means parameter count matters much less for the task group, and that depth via looping is more essential.


\textbf{Perplexity results.} Firstly we notice that all \loopy{k}{24/k} looped models have better perplexity compared to the iso-param \loopy{k}{1} baseline, but worse perplexity compared to the non-looped 24-layer baseline.
The looped models only covers up roughly $34-50\%$ of the perplexity gap between the iso-param and iso-flop baselines for various values of $k$.
This perplexity gap is not too surprising since the looped model has $24/k$ times fewer parameters, and thus, lower capacity than the 24-layer baseline.
This was also been observed in prior works \citep{lan2019albert,mohtashami2023cotformer} and is the primary reason looped models have been ignored for language modeling.
However, as we shall see shortly, the downstream metrics paint a more interesting and favorable picture.


\vspace{-0.1in}


\textbf{Results on QA tasks.} We first consider closed book and open book QA categories in \Cref{table:language_modeling_results}. Closed book QA tasks are primarily testing the model's memorization abilities. Open book QA on the other hand tests the model's ability to infer the answer from the additional context that is provided. Thus, intuitively, open book QA tasks require more reasoning. Firstly we notice that the \% Gap for closed book QA (memorization) is very similar to \% Gap for perplexity. The \% Gap for open book QA, on the other hand, is much higher for all parameter budgets. This suggests that looped models relatively improve contextual QA much more than memorization based QA.


\input{Table_Fig/inductive_bias_12L}



\textbf{Math problems and reasoning primitves.} We also present the \% Gap for the math word problem in \Cref{table:language_modeling_results}. Surprisingly, we find that \loopy{k}{24/k} looped model can almost match the baseline \loopy{24}{1} model for $k\ge6$, despite having $k$ times fewer parameters. In fact, the 12 layer model looped twice is even signficantly better ($34.3$) than the 24 layer baseline ($29.3$), despite having 50\% of parameters and worse perplexity; suggesting that looping disproportionately improves mathematical reasoning. 

To better understand the effect on reasoning, we direct the readers attention to the evaluations for reasoning primitives in \Cref{table:language_modeling_results}. The results are quite remarkable: {\bf \loopy{k}{24/k} looped models are better than the iso-flop baseline \loopy{24}{1} at reasoning primitives, for all values of $k$}.
This is a priori very surprising, since these are synthetic generated tasks and have nothing to do with the pretraining data or the model architecture.
Thus, solving these tasks necessarily requires reasoning from context, and memorization abilities will not help here.
These results clearly suggest that looped models have a bias towards improving reasoning, despite having worse perplexity and memorization abilities. Next, we formalize the {\em inductive bias} towards reasoning via isoplots.
\vspace{-0.1in}

\subsection{Inductive bias towards reasoning}
\label{sec:inductive_bias}
% \vspace{-0.1in}



In this section, we formalize the inductive bias by plotting the perplexity vs downstream metric iso-plots, as introduced in \citet{saunshi22understanding}.
\Cref{sec:expt_1B} showed that looped models have {\em higher than expected} performance on reasoning problems.
However, since looped models are worse on perplexity, it is hard to make a direct comparison between various models.
One way to bring parity between models is to look at their downstream performances at the same validation pretraining loss \citep{liu2023same}.
\citet{saunshi2024inductive} proposed plotting pretraining loss vs downstream metrics as training proceeds, as a way to study the inductive bias of various methods.
% This can highlight differences in the model behavior at the same pretraining performance.
% In particular, we are interested in how looped models are similar or dissimilar to non-looped models on different task groups.
For each model, we evaluate the log perplexity and downstream metrics at every 20k steps, starting from 120k steps. We plot these values in a scatter plot and fit a linear function with log perplexity and the corresponding downstream metric being input and output respectively.
Please refer to \Cref{fig:iso_plots_taskgroups_12L,fig:iso_plots_taskgroups_8L} for two sets of isoplots.

{\bf Findings.} For all values of $k$, we observe the following:
\begin{itemize}[leftmargin=0.6cm]
    \item The isoplots for \loopy{k}{L} looped model and \loopy{k}{1} baseline are very aligned for closed book QA tasks (if extrapolated). This suggests that log perplexity is a very strong indicator of downstream performance on memorization based tasks.
    \item For open book QA and math word problems, the isoplot line for the looped model is always higher than the baseline model. This suggests that at the same log perplexity, looped models will tend to have higher evaluation on these tasks that require more reasoning.
    \item For reasoning primitives, there is a stark difference between looped and baseline models. The looped model seems to have good performance at most points in training.
\end{itemize}

Overall this suggests a strong inductive bias of looped models towards improving reasoning. 
% \vspace{-0.1in}


\new{

\subsection{Middle looping variant and relationship with Gradual stacking}
\label{sec:stacking_middle}

\looseness-1Recently \citet{saunshi2024inductive} introduced a gradual stacking \citep{gong2019efficient,reddi2023efficient} approach for training language models called MidAS. This approach gradually grows the model depth as training proceeds by duplicating certain layers of the model in each stacking operation. Surprisingly, they found that MidAS not only speeds up pretraining, but also improves reasoning in the same sense as \Cref{fig:iso_plots_taskgroups_12L} -- better reasoning at the same perplexity.
Furthermore, the paper established a strong connection between stacking via MidAS and looped models, owing to the layer duplication operation, and conjectured that this is the reason for such an inductive bias.
Our results from the previous section provides a compelling evidence for this conjecture by showing that looped models also show a very similar inductive bias, thus, further strengthening the connection between stacking and looped models.
Why such an inductive bias occurs is still an open question, and we believe that understanding this is an important future direction.


Furthermore, inspired by their findings, we explore {\bf middle looping} (see \Cref{fig:looping_illustration} for an illustration) --- a variant of looping which maintains independent layers at the start and the end of the network, and perform looping on the middle block of layers. 
The high-level intuition from \citet{saunshi2024inductive} is that the first and last layers play a special role in the model and thus, should be treated differently from the middle layers.
In \Cref{table:language_modeling_results}, we report results for a version of middle looping that is iso-param with a \loopy{12}{1} baseline and iso-flop with a \loopy{24}{1} baseline, just like the \loopy{12}{2} model.
Overall, we find that middle looping has better perplexity and more uniform improvements than the default looping of \loopy{12}{2} (except for math word problems), and thus, might be a more practical looping approach.
We leave the exploration of the best looping strategies for future work.

% \ns{Present basic results}



\subsection{Scaling behavior of looping}
\label{sec:depth_scaling}



\input{Table_Fig/depth_scaling.tex}


In this section, we discuss an intriguing scaling behavior of looping, specifically the impact of the number of loops on various evaluation metrics. In particular, we are interested in scaling behavior of: (a) accuracy as a function of number of loops and (b) comparison of looped and non-looped baseline of the same effective depth.
To this end, we pretrain various looped language models of the form \loopy{4}{L}, i.e., 4-layer model looped $L$ times, for $L \in \{1, 2, 3, 6, 9, 12\}$.
To enable iso-FLOPs comparison, we also train baseline models of the form \loopy{4L}{1}, which has $L$ times more parameters.
For each task group, we plot the average accuracy as a function of the \emph{effective depth}, i.e. $D = 4L$.
From the results presented in \Cref{fig:depth_scaling}, we observe the following.
% For both the looped and baseline models, we find that the accuracy can be well approximated by a linear function of the log of the depth.
\begin{enumerate}[leftmargin=.15in, itemsep=1ex]
    \item In both cases, we find that the accuracies for all task groups continue to increase with more loops/depth, although, unsurprisingly, the returns are diminishing with depth for both looped and non-looped models. Interestingly, for both looped and non-looped models, we found that one can fit a simple scaling law of the following form:
\begin{align}
    \text{Acc} = \alpha \log(D) + \beta,
    \label{eq:depth_scaling_law}
\end{align}
where $D$ is the effective depth and $\alpha$ measures the impact of depth on downstream performance.
\item Furthermore, for each task group, we compute $\alpha_{loop}/\alpha_{base}$ to assess the relative impact of ``depth via looping'' compared to ``depth via additional parameters''.
We find that more loops continue to help, and the relative benefit of loops is higher for reasoning tasks like open book QA and math problems.
Remarkably, the impact of loops is even higher (1.19x) than impact of depth for reasoning primitives, which further consolidates the benefit of looped models for reasoning.

\end{enumerate}

% A natural question to ask is -- why do looped models show a nice linear scaling?


\paragraph{Latent thoughts and connections to CoT reasoning.}
\label{sec:latent_thoughts}
We end this discussion with an important question: \emph{why should we expect this interesting scaling behavior of looped models?} We argue that looped models have a strong connection to chain-of-thought (CoT) reasoning. Such a connection is insightful because recent works on thinking have shown CoT can demonstrate inference-time scaling behavior, where the accuracy on reasoning datasets continues to improve as the length of the models chain of thought increases; i.e., generating more thoughts leads to better reasoning. 

To establish this connection, we make a simple observation about CoT reasoning -- it is essentially a looped model that generates a single thought token in each iteration.
However, looped models can be more powerful since they can generate multiple ``latent thoughts'' in each iteration. 
This can be visualized in \Cref{fig:latent_thoughts}.
We further translate this intuition into a theoretical result (see \Cref{sec:looped_simulate_cot}) about how looped models can in fact simulate CoT reasoning.
Given this connection to CoT reasoning, it is believable that looping can scale well for harder reasoning.
We believe that leveraging looping explicitly for inference-time scaling is a very promising future direction.
}



% \input{Table_Fig/inductive_bias_24L}





