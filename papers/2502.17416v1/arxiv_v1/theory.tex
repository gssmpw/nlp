\vspace{-0.05in}
\section{Theoretical analysis for looped models}
\label{sec:theory}

\vspace{-0.05in}
\looseness-1In this section, we present theoretical results to understand the phenomenon from the previous sections -- {\em why can looped model with few parameters match an iso-flops non-looped baseline on reasoning problems?}
While a complete theory is challenging, since ``reasoning'' is a very broad concept, the goal is to provide some intuition and formalization for the expressive power of looped models. First, we show that looped Transformers can effectively solve group composition (a generalization of the addition problem). Then we show a very general result on how a non-looped model with very few distinct layers can be simulated by a looped model with a small blowup in model size. This result is then used to solve the $p$-hop problem using a one-layer looped transformer. Our construction for group composition and $p$-hop problem are nearly optimal in terms of depth and much more efficient in terms of parameters compared to non-looped models.

\vspace{-0.01in}
\subsection{Preliminaries and Notations}
\label{subsec:prelim}
We first define the standard transformer architecture. Throughout the paper we will fix the dimension of the embedding to be $d\in\mathbb{N}^+$, the vocabulary to be $\mathcal{V}$ and maximum sequence length to be $n_{\max}$. We will use $\id$ to denote the identity mapping. Here, we describe the high-level notation. Please refer to \Cref{sec:apx_detailed_notation} for detailed notations.
We use $\ff$ and $\mha$ to denote the feed-forward and attention layers respectively, and $\theta_{\mha}$ and $\theta_{\ff}$ denote the parameters in these layers.
\vspace{-0.01in}

\begin{definition}[Transformer Block]\label{defi:transformer_block}
Given number of layers $L\in\mathbb{N}^+$ and parameter $\theta_\tfblock = (\theta^{(l)}_\mha,\theta^{(l)}_\ff )_{l=0}^{L-1}$, $L$-layer transformer block $\tfblock_{\theta_\tfblock}:(\mathbb{R}^d)^n\to (\mathbb{R}^d)^n$ for any $n\in\mathbb{N}^+$ is defined as \begin{equation}
    \tfblock_{\theta_\tfblock} \triangleq (\id+ \ff_{\theta^{(L-1)}_\ff})\circ (\id+ \mha_{\theta^{(L-1)}_\mha})\circ \cdots (\id+ \ff_{\theta^{(0)}_\ff})\circ (\id+ \mha_{\theta^{(0)}_\mha}),
\end{equation}
\end{definition}
We also denote $\embed$  and $\transoutput$ to be the input embedding and output softmax layers respectively. Please refer to \Cref{sec:apx_detailed_notation} for precise definitions.
Finally, we define the entire transformer model that maps a sequence of tokens to a distribution over tokens: $p_{\theta}: \cup_{n\le n_{\max}} \mathcal{V}^n\to \Delta^{|\mathcal{V}|-1}$.
\begin{align}
p_{\theta}\triangleq \transoutput_{\theta_\transoutput}\circ \tfblock_{\theta_\tfblock}\circ \embed_{\theta_\tokenembedding,\theta_\posencoding}
\end{align}
where $\theta = (\theta_\tfblock,\theta_\tokenembedding,\theta_\posencoding,\theta_\transoutput)$ denote all the transformer parameter. 
In particular, we use $\transformer_{\theta}(v_1,\ldots,v_n) \triangleq \argmax_{v\in\mathcal{V}} p_{\theta}(v|v_1,\ldots,v_n)$ to denote the deterministic version of the transformer model. 
We now define a looped Transformer model that also subsumes a non-looped model.

\begin{definition}[\loopy{L}{T} Looped Transformer]\label{defi:looped_transformer}
Given the number of loops $T\in\mathbb{N}^+$,  parameters $\theta = (\theta_{\tfblock},\theta_\tokenembedding,\theta_\posencoding,\theta_\transoutput)$, where $\theta_{\transformer} = (\theta^{(l)}_\mha,\theta^{(l)}_\ff )_{l=0}^{L-1}$, we define a \loopy{L}{T} \emph{looped Transformer} as  $p_{\theta,T}\triangleq \transoutput_{\theta_\transoutput}\circ \left(\tfblock_{\theta_\tfblock}\right)^T\circ \embed_{\theta_\tokenembedding,\theta_\posencoding}$.
\end{definition}


\subsection{Group composition problem}
We consider the problem of composing $n$ elements from a group, and prove that a standard 1-layer transformer looped $\mathcal{O}(\log(n))$ times can solve this problem. 
This is a generalization of the modular addition problem and has a long history (see \Cref{sec:apx_group_composition}).
Recently, \citet{liu2022transformers} show that transformers with $\log_2 n$ depth can compute composition over $n$ group elements, regardless of whether the group is solvable or not. However, the construction in \citet{liu2022transformers} uses different attention parameter for each layer. Here, we provide a more parameter efficient construction where we solve this problem by looping a one-layer transformer $\log(n)$ times.
\begin{theorem}\label{thm:group_composition_log_depth}
    For any finite group $G$ and every $n\in\mathbb{N}^+$, there exists a constant-precision looped transformer $\transformer_{\theta,T}$ computing the composition of $n$ elements from $G$ with a $1$-layer transformer block, $T=\lceil\log _2 n\rceil$ loops, $G\cup\{\#\}$ being the vocabulary, $d= 3\left(\lceil\log_2 |G|\rceil +\lceil\log_2 n+1\rceil\right)$ embedding size, $d_\ff=|G|^2 + 6\lceil\log_2 |G|\rceil$ hidden dimension in MLP, $d_\attn=\lceil\log_2 n\rceil$ hidden attention dimension, and $2$ attention heads. More specifically, for any $g_1,\ldots,g_n\in G$, $\transformer_{\theta}(\#,g_1,\ldots, g_n) = g_1\circ\cdots \circ g_n$.
\end{theorem}

The above result matches the depth upper bound shown for non-looped models by \citet{liu2022transformers}, and is very close to the super constant lower bound shown there.
Thus looped models can solve the problem with best known depth.


\subsection{Looped models can simulate non-looped models}

Our second theoretical result shows that a non-looped transformer with repeated layers can be simulated by a looped transformer with fewer parameters and same depth.
\begin{theorem}\label{thm:main}
    For any transformer $p_{\theta}$ with $L$ layers, $d$ embedding size, $d_\ff$ hidden dimension for MLP, $H$ attention heads with $d_\attn$ hidden dimension, at most $R$ distinct transformer layers and bounded activation value, there is a looped transformer $p_{\theta',L}$ working on the same vocabulary $\mathcal{V}$ plus a dummy token $\#$, 
which loops a 1-layer transformer block for $L$ times, with $d+R+2$ embedding size, $R h_\ff +O(L)$ hidden dimension for MLP, $RH$ attention heads with $d_\attn$ hidden dimension, such that for any string $v_1,\ldots, v_n\in\mathcal{V}$, $p_{\theta}(v_1,\ldots,v_n) = p_{\theta',L}(\#, v_1,\ldots,v_n)$.
\end{theorem}
We can also use the above theorem to convert the $O(\log_2 n)$ depth transformer that simulates group composition into a 1-layer looped transformer, although it is less parameter efficient than the result from the previous section.
Please refer to  \Cref{sec:apx_simulate_nonlooped} for the full proof.
%\begin{proof}[Proof Sketch of \Cref{thm:main}]
%The high-level idea of our construction here is to have a one-layer wide transformer block with $R$ times more hidden dimension of attention of MLP layers but with the same embedding size, such that in each layer, the one-layer transformer block have the capacity to do all possible computation, but we choose which result to be saved, by a switching gate using an auxiliary depth counter. 
%The depth counter can be done by constantly more dimension in embedding and hidden dimension in MLP. We can also use a width-$L$ 2-layer MLP to turn the integer valued depth counter to a one-hot vector indicating which transformer layer to simulate. For MLP, we can deactivate undesired neurons by creating a very large negative bias. For attention, we can make the undesired attention head to attend to the dummy token $\#$ in the beginning of the sentence at all positions. This is the only reason we need such a dummy token.
%\end{proof}

\textbf{Theory for $p$-hop.} The experiments on $p$-hop induction from \Cref{sec:khop_desc} surprisingly show that a small model looped multiple times can solve it very effectively. 
We establish a theoretical basis for this finding. More specifically, we show that a constant layer transformer with $\log(p)$ loops suffices to solve $p$-hop induction problem. This result, in fact, matches the lower bound for layers required for non-looped models proved in \citet{sanford2024transformers}.
The result follows from \Cref{thm:main} and the construction for non-looped models from \citet{sanford2024transformers} (restated in \Cref{thm:khop_log_depth}).

\begin{corollary}
    $p$-hop problem (\Cref{defi:khop}) can be solved by looping a 1-layer transformer $\lfloor\log_2 p\rfloor+2$ times, which has $O(\log n)$ bits of precision, $d=d_{\ff}=d_{\attn}=O(1)$ embedding size, hidden dimension for MLP and attention, and at most $3$ attention heads.
\end{corollary}


\new{

\subsection{Looped models can simulate CoT reasoning}
\label{sec:looped_simulate_cot}

% \ns{Briefly describe how looping is providing latent thoughts and it differs from CoT reasoning.}

In \Cref{sec:latent_thoughts}, we discussed how looped models can be viewed as generating multiple latent thoughts in each iteration. 
The following theorem shows that one can use looped transformer with $L$ loops to simulate $L$ steps CoT of another transformer with similar sizes. 
\begin{theorem}[Looped transformer simulates CoT]\label{thm:cot_informal}
    For any $L$-layer non-looped transformer $\transformer_\theta$ with fixed input length $n$ and the number of CoT steps $m$, there exists a looped transformer $\transformer_{\theta'}$ with $L+\mathcal{O}(1)$ layers, $\Omega(\log (n+m))$, more  embedding dimension and constantly many more attention heads, such that for any input $\vv= (v_i)_{i=1}^n$, the output of non-looped transformer after $m$ steps of CoT, $\transformer^m_\theta(\vv)$, is the same as that of the looped transformer on input $x$ concatenated by $m$ dummy tokens with $m$ loops, $\transformer_{\theta',m}(\vv,\#^m)$.
\end{theorem}

Below we sketch the high-level idea behind \Cref{thm:cot_informal}; the full proof can be found in \Cref{sec:apx_cot_connection}. 
\begin{itemize}
    \item (Masking all-but-one) We maintain a counter $t$ (\Cref{lem:mlp_addition}) at each position for the current number of loop and for the $i$th position, we will only "update" the embedding if $i-n\ge t$ and reset the embedding to the input to the looped transformer layer otherwise. This can be implemented by MLP. So similar to CoT, the first $n+i-1$ embedding won't be changed in the $i$th loop.
    \item (Shift by one) We can use attention to obtain the output of the current loop at the previous position and use that as the input of the next loop at current position. (\Cref{lem:shifting_layer})
    \item (Token decoding) We show that we can use MLP with ReLU activation to simulate the encoding and decoding process of CoT. (\Cref{lem:simulating_decoding_embedding})
\end{itemize}


}


\input{Table_Fig/latent_thoughts}
