\section{Looping-inspired regularization}
\label{sec:regularization}

In the previous section, we observed the looped models can improve reasoning with worse perplexity. Can we leverage this observation to improve reasoning without affecting perplexity? Here, we propose a simple approach: regularize the weights of the model to encourage them to be close to a looped model. This could have two advantages, (a) the model still has free parameters to improve perplexity, (b) the closeness to looped model can inherit the desirable inductive bias.
In particular, if an $L$-layer model is denoted as $f_{0}\circ f_{1} \dots, \circ f_{L/k-1}$, where each $f_{i}$ is a block of $k$ layers, we add a regularization term that makes all the weights of the block $f_{i}$ close to $f_{i+1}$ in terms of cosine similarity.
For a parameter group $G$ (e.g. first feed-forward layer, or query matrix in Transformer), we use $\theta_{G}^{(0)}, \theta_{G}^{(1)}, \dots, \theta_{G}^{(L-1)}$ to denotes the weights in all layers.
Then the regularization term is
\begin{align}
    \mathcal{R}_{G}(k) = \frac{1}{L-k}\sum_{i=0}^{\frac{L}{k}-2} \sum_{j=0}^{k-1} \text{Cosine}\left(\theta_{G}^{(i k + j)}, \theta_{G}^{((i+1) k + j)}\right)
\end{align}
The final loss function is a sum of the standard cross-entropy loss and the regularization term averaged over all groups, multiplied by a scalar hyperparameter. Let $\mathcal{G}$ denote the set of all parameter groups; $\mathcal{G} = \left\{\text{Attn-Q}, \text{Attn-K}, \dots, \text{FFN-W2}\right\}$
\begin{align}
    \mathcal{L} = \mathcal{L_{\text{xent}}} + \lreg|\mathcal{G}|^{-1}  \sum_{G \in \mathcal{G}} \mathcal{R}_{G}(k)
    \label{eq:regularization}
\end{align}
\looseness-1In the above formulation, $\lreg=0$ would recover standard training and $\lreg \rightarrow \infty$ would converge to a fully looped model. Intermediate values of $\lreg$ will lead to ``approximately'' looped models. For instance, to emulate the \loopy{4}{6} looped model setting, we use pick $k=4$, $L=24$ and a large regularization strength like $\lreg=10$. All other hyperparameters are kept the same as baseline training for a fair comparison. 
We tried options other than cosine similarity, like $\ell_2$ norm, to bring the weights closer but found that cosine was more robust and effective.

{\bf Cosine similarities.} Firstly, we check if the regularization had the right effect by measuring the cosine similarities between the successive blocks of $k$ layers at the end of training.  We, indeed, find that for all parameter groups, the cosine similarity around 0.98 or higher (see \Cref{fig:cosines_cosreg}). 
%We visualize the cosine similarities for a few groups of layers in \Cref{fig:cosines_cosreg}



\iffalse
{\bf Results.} \Cref{table:regularization_results} summarizes the results with regularization. Overall, we find that regularized models improve over standard training. In particular, for block size of $k=4$, we find that higher regularization strength of $\lreg=10$ was more effective than $\lreg=1$. Thus, we use $\lreg=10$ for all other experiments. Not only does regularization slightly improve perplexity ($7.40 \rightarrow 7.38$) and closed book QA performance ($11.2 \rightarrow 12.5$), it significantly improves open book QA ($33.9 \rightarrow 36.2$), math word problems ($29.3 \rightarrow 36.4$) and reasoning primitives ($47.5 \rightarrow 57.5$). Thus, we are able to achieve the best of both worlds -- better memorization than baseline and better reasoning than looped models.
\fi

{\bf Inductive bias.} To confirm the inductive bias of the regularizer, we visualize the log perplexity vs downstream isoplots for $\lreg=10$ and baseline models in \Cref{fig:iso_plots_taskgroups_12L}. While the plots are similar for closed book QA, a strong inductive bias shows up for open book QA and reasoning problems.
Crucially, the regularized model does well on reasoning without hurting perplexity (see \Cref{table:regularization_results}).


\input{Table_Fig/regularization_results}

% \vspace{-0.1in}