\section{Looped models on simple reasoning tasks}
\label{sec:synthetic_reasoning}

\vspace{-0.05in}
We first explore our hypothesis of looped models helping reasoning tasks on a set of  tasks constructed in a procedural manner. The illustrative reasoning tasks we consider are: $n$-ary addition, $p$-hop induction head that tests the model's ability to track back for $p$ steps, and i-GSM which consists of synthetically constructed grade-school math problems. While these obviously do not cover the whole spectrum of reasoning problems, they provide useful insights into looped models and provide a basis for the theoretical results in \Cref{sec:theory}.
% Finally, in \Cref{sec:apx_cot_connection}, we discuss an interesting connection to chain-of-thought reasoning.

\textbf{Looped models.} While many variants of looped model have been proposed \citep{lan2019albert,dehghani2018universal,giannou2023looped,yang2023looped,mohtashami2023cotformer}, we use the vanilla version for simplicity of our exploration. For any sequence-to-sequence function $f$, we denote $f^{(L)} = f \circ f \dots \circ f$ to be the function that is $f$ looped $L$ times. In general, the looping mechanism is independent of architectural choices for $f$.
For the rest of the paper, we typically use $f$ to denote a $k$-layer Transformer backbone of a model.
Thus, $f$ looped $L$ times is the same as a $kL$ layer model with weight sharing between all $L$ blocks of $k$ consecutive layers.
We denote such looped models with the notation \loopy{k}{L}.
Please refer to \Cref{fig:looping_illustration} for a succinct illustration of the looping mechanism.
\Cref{subsec:prelim} provides a more formal definition of looped transformers that is used for theoretical analysis.
% \vspace{-0.05in}





\subsection{Experiments with simple reasoning problems}
\label{sec:addition}

\textbf{$n$-ary addition.}
\label{sec:addition}
We consider the problem of adding $n$ numbers with 3 digits each. Addition is popular in the literature to study aspects of reasoning with Transformers, such as use of scratchpad \citep{nye2021show}, chain of thought reasoning \citep{lee2024teaching,li2024chain} and length generalization \citep{cho2024position}.
One reason for its popularity is that addition can have algorithmic solutions, which is a feature of many reasoning problems.
% Here, we use addition to study the role of looping by varying the number of operands in the input. 
For our experiments, we train on a uniform mixture on numbers of operands $n\in\{2, 4, 8, 16, 32\}$ and sample each 3-digit operand uniformly at random between $[0, 999]$. We train all models directly on the input-output pair, without any chain-of-thought steps. Following is an example for $n=4$:
\vspace{-0.05in}
\begin{center}
    Input: ``$315 + 120 + 045 + 824 = $''  ; Output = ``$1304$''.
\end{center}
\vspace{-0.05in}
We train a standard Transformer-based baseline \loopy{12}{1} model with 12 layers. Please refer to \Cref{sec:apx_addition} for details on the training setup. We also train \loopy{k}{12/k} looped model and an iso-param \loopy{k}{1} baseline models for comparison, and vary $k \in \{2, 3, 4, 6\}$. All trained models are finally evaluated separately on each of $n\in\{8, 16, 24, 32\}$ to measure accuracy on increasingly difficult problems. Results are presented in \Cref{table:addition_phop_results}.

% \input{Table_Fig/addition_results}


\input{Table_Fig/addition_phop_results}

We find that, while the shallower baselines \loopy{k}{1} degrade with lower $k$, the looped model \loopy{k}{12/k} performs very well, and nearly matches the iso-flop \loopy{12}{1} baseline. In fact, even a 1-layer network looped 12 times is able to solve this, despite using merely $1/12^{th}$ of the parameters of the baseline. This suggests the addition problem primarily requires depth, but not necessarily more parameters.

\vspace{-0.05in}
\paragraph{$p$-hop induction.}
\label{sec:khop_desc}

% \subsubsection{$p$-hop}
\looseness-1The $p$-hop problem is a synthetic induction task studied in \cite{sanford2024transformers}, who were inspired by the analysis of induction heads from \cite{elhage2021mathematical}. Specifically, given a sequence of letters $\vv = (v_1 \ldots v_n)$ from an alphabet $\Sigma$, an induction head tries to find the penultimate occurrence of $v_n$ (from left to right) in the sequence and output the character immediately succeeding it. The $p$-hop problem generalizes this idea to sequentially hop $p$ times. 
Intuitively, the $p$-hop problem tests a model's ability to recursively backtrack and retrieve the answer for a given query. This is reminiscent of the reasoning abilities required to solve reading comprehension kind of problems.
We present the formal definition of the $p$-hop problem in \Cref{defi:khop}.
% \vspace{-0.05in}
We perform experiments with looped and non-looped models on the $p$-hop problem, with alphabet size set to 4 and sequences of length $256$, and vary $p$ between 16 and 32 to control the problem difficulty. 
% Note that there is no chain-of-thought breakdown of the $p$-hops provided to the model.
Our observations are presented in Table~\ref{table:addition_phop_results}. Similarly to our findings on the addition task, reasonably deep looped models perform as well as the baseline using much fewer parameters.
% \input{iclr2025/Table_Fig/khop_results}

\iffalse
\begin{table}[]
    \centering
    \begin{tabular}{|c|c|}
    \hline
        \textbf{Setting} & \textbf{Accuracy (\%)} \\
        \hline
        2 layer & 58.14 \\
        \hline
        10 layer & 99.69 \\
        \hline
        Looping with 2 layer, 5 loops & \textbf{99.94}\\
        \hline
    \end{tabular}
    \caption{Comparison of looped vs non-looped models for the $p$-hop problem.}
    \label{table:khop-results-old}
\end{table}
\fi


\paragraph{i-GSM (Synthetic Grade School Math Problems).} Inspired by ~\cite{ye2024physics}, we built our own version of grade-school level math word problems. While we follow many of the design guidelines of \cite{ye2024physics}, we make a few simplifying changes. 
% We first pick a hierarchy of entities with 4 levels from which each example randomly samples a structure graph over entities.
% Each edge in the structure graph defines a instance parameter which is an integer (e.g. an edge between city center and car parks denotes the number of car parks in city center).
% Then a randomly sampled mathematical dependency graph is constructed over the instance parameters by relating each to the others. 
% We pick one of the nodes of the dependency graph to query and the goal is to compute the numerical value of the query node modulo some prime number $P$ (see \Cref{sec:apx_igsm} for details).
We generate the math problem as a DAG of arithmetic computations modulo 7, and restrict the depth of the graph to 4. For simplicity, we retain the problems in the symbolic form and do not map them to English (e.g., see Table~\ref{fig:sample-eigsm-problem}) \footnote{Similar to \cite{ye2024physics}, the simplified setting still allows for over 7 billion unique solution templates.} We train models of depths 1, 2, 4 and 8 and compare them with different looped variants in Table~\ref{table:igsm-results}. The answer is computed modulo 7. Hence, a random guessing baseline would get at least 14\% accuracy.  \Cref{sec:apx_igsm} has more details. Remarkably, we again observe that a depth $k$ model looped $L$ times matches or outperforms a depth $kL$ model and far outperforms a depth $k$ non-looped model. 
This suggests that even a more complex and realistic looking reasoning problem does not need too many parameters and can benefit from depth via looping.

\input{Table_Fig/igsm_results}


% \begin{figure}[htb]
% \label{fig:sample-igsm-problem}
% \centering
% \begin{subfigure}[t]{.5\linewidth}
%   \centering
%   \fbox{
%     \begin{minipage}{0.9\linewidth}
%       \textit{The number of K-Mart's Pizza equals 12 more than Dollar Tree's Sushi. The number of BJs's Sushi equals the sum of Wall Street's Dollar Tree, K-Mart's Products and K-Mart's Pizza. The number of Dollar Tree's Sushi equals 7. The number of Automotive District's K-Mart equals the sum of BJs's Sushi, Wall Street's Dollar Tree and Dollar Tree's Sushi. The number of Automotive District's BJs equals K-Mart's Pizza. The number of Wall Street's Dollar Tree equals 6 more than Automotive District's BJs. How many K-Mart does Automotive District have?} 
%     \end{minipage}
%   }
%   \caption{i-GSM Problem}
% \end{subfigure}%
% \begin{subfigure}[t]{.5\linewidth}
%   \centering
%   \fbox{
%     \begin{minipage}{0.9\linewidth}
%       \textit{Define Dollar Tree's Sushi as a; a = 7. Hence, a = 7. Define K-Mart's Pizza as b; b = 12+a. Hence, b = 6. Define Automotive District's BJs as c; c = b. Hence, c = 6. Define Wall Street's Dollar Tree as d; d = 6+c. Hence, d = 12. Define K-Mart's Products as e; e = b. Hence, e = 6. Define BJs's Sushi as f; f = d+e+b. Hence, f = 11. Define Automotive District's K-Mart as g; g = f+d+a. Hence, g = 4.}
%     \end{minipage}
%   }
%   \caption{Solution}
% \end{subfigure}
% \caption{A sample problem from the i-GSM dataset (left) and its solution (right). Note that all arithmetic is performed modulo 13.}
% \end{figure}


% \begin{figure}[t]
% \centering
% \begin{subfigure}[t]{.5\linewidth}
%   \centering
%   \scalebox{0.8}{
%   \fbox{
%     \begin{minipage}{\linewidth}
%       x
%     \end{minipage}
%   }
%   }
%   \caption{Symbolic i-GSM Problem}
% \end{subfigure}%
% \begin{subfigure}[t]{.5\linewidth}
%   \centering
%   \scalebox{0.8}{
%   \fbox{
%     \begin{minipage}{\linewidth}
%       \paragraph{Question.} \textit{E\#I := 4. E\#J := E\#I. K\#N := I\#N + J\#O + F\#K. F\#K := E\#J. J\#O := F\#K + K\#O + E\#J. H\#J := E\#J + F\#K. I\#P := L\#M + I\#N + K\#O. I\#M := J\#O + J\#P + F\#K. J\#P := H\#J - F\#K. L\#M := I\#N + J\#P + F\#K. I\#N := 2 * J\#P + H\#J + E\#I. K\#O := J\#P + I\#N + E\#J. I\#P?}\\
      
%       \paragraph{Answer with CoT.} \textit{E\#I = 4. $\implies$ E\#I = 4. E\#J = E\#I. $\implies$ E\#J = 4. F\#K = E\#J. $\implies$ F\#K = 4. H\#J = E\#J+F\#K. $\implies$ H\#J = 1. J\#P = H\#J-F\#K. $\implies$ J\#P = 4. I\#N = 2J\#P+2H\#J+2E\#I. $\implies$ I\#N = 4. L\#M = I\#N+J\#P+F\#K. $\implies$ L\#M = 5. K\#O = J\#P+I\#N+E\#J. $\implies$ K\#O = 5. I\#P = L\#M+I\#N+K\#O. $\implies$ I\#P = 0.}
%     \end{minipage}
%   }
%   }
%   \caption{Solution}
% \end{subfigure}
% \caption{A sample problem from the Symbolic variant of the i-GSM dataset (left) and its solution (right). Note that all arithmetic is performed modulo 7.}
% \label{fig:sample-eigsm-problem}
% \end{figure}

