

\begin{abstract}
% Large language models have shown remarkable  reasoning abilities and scaling laws suggest that parameter count is a primary driver. Recent works \citep{chen2024can,ye2024physics} argue that, for reasoning, depth plays a crucial role in addition to parameter count.
Large language models have shown remarkable reasoning abilities and scaling laws suggest that large parameter count, especially along the depth axis, is the primary driver. 
In this work, we make a stronger claim --- many reasoning problems require a large depth but not necessarily many parameters. 
This unlocks a novel application of {\em looped models for reasoning}.
Firstly, we show that for many synthetic reasoning problems like addition, $p$-hop induction, and math problems, a $k$-layer transformer looped $L$ times nearly matches the performance of a $kL$-layer non-looped model, and is significantly better than a $k$-layer model.
This is further corroborated by theoretical results showing that many such reasoning problems can be solved via iterative algorithms, and thus, can be solved effectively using looped models with nearly optimal depth.
% Perhaps surprisingly, these benefits translate to practical language modeling settings --- on general language modeling tasks that require reasoning, a $k$-layer model looped $L$ times can be competitive, if not better, to a $kL$-layer model.
Perhaps surprisingly, these benefits also translate to practical settings of language modeling --- on many downstream reasoning tasks, a language model with $k$-layers looped $L$ times can be competitive to, if not better than, a $kL$-layer language model.
In fact, our empirical analysis reveals an intriguing phenomenon: looped and non-looped models exhibit scaling behavior that depends on their effective depth, akin to the inference-time scaling of chain-of-thought (CoT) reasoning.
We further elucidate the connection to CoT reasoning by proving that looped models implicitly generate \emph{latent thoughts} and can simulate $T$ steps of CoT with $T$ loops.
Inspired by these findings, we also present an interesting dichotomy between reasoning and memorization, and design a looping-based regularization that is effective on both fronts.
% Finally, inspired by these findings, we design a looping-based regularization and demonstrate its effectiveness across several downstream reasoning tasks.
\end{abstract}


\iffalse 

\begin{abstract}
  Large language models have shown promising abilities in reasoning problems and scaling laws suggest that parameter count is a key driver. Recent works \citep{chen2024can,ye2024physics} argue that, for reasoning, depth plays a very important role in addition to parameter count.
  In this work, we make a more fine-grained claim --- many reasoning problems require large depth but not necessarily many parameters, in the sense that they can be solved via {\em looped models}.
  This unlocks a novel application of looped models for reasoning.
  % We empirically study various synthetic reasoning problems like addition, $p$-hop induction, and math problems. For each of these, we find that $k$-layer transformer model looped $L$ times nearly matches the quality of a $kL$-layer non-looped model and is much better than a $k$-layer model. Thus, using a small model and providing depth via looping can suffice for such reasoning problems. We then show theoretical results proving that many such reasoning problems can be solved via iterative algorithms, and thus, can be solved with looped models.
  For various synthetic reasoning problems like addition, $p$-hop induction, and math problems, we find that a $k$-layer transformer looped $L$ times nearly matches the quality of a $kL$-layer non-looped model, and is much better than a $k$-layer model. Thus, using a small model and providing depth via looping can suffice for such reasoning problems. This is corroborated by theoretical results that show many such reasoning problems can be solved via iterative algorithms, and thus, can be solved with looped models.
  We further elucidate connections to chain-of-thought (CoT) reasoning by proving that looped models implicitly generate ``latent thoughts'' and can simulate $T$ steps of CoT with $T$ loops.
  Motivated by these findings, we train autoregressive models on general language modeling datasets with looping and compare a $k$-layer model looped $L$ times to a $kL$-layer model. While the looped model is understandably worse on perplexity and memorization tasks, it surprisingly does very well on tasks that require reasoning, like open book QA, math word problems and reasoning primitives. Despite having significantly fewer parameters, it can even match or outperform the non-looped $kL$-layer model on some of these tasks and have a very robust scaling behavior with respect to depth. These results suggest a novel inductive bias of looped models towards enhanced reasoning. We provide further evidence for this inductive bias by visualizing perplexity vs downstream isoplots, and design a looping-inspired regularization that solidifies this hypothesis.
\end{abstract}

\fi