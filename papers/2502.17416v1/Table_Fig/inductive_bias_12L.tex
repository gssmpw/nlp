\begin{figure*}[!tbp]% [H] is so declass\'e!
\centering
    \begin{subfigure}{0.23\textwidth}
    \centering    \includegraphics[width=\textwidth]{Media/12L_closedQA.png}
    % \caption{}
    \label{fig:12L_closedQA}
    \end{subfigure}\hfill
    \centering
    \begin{subfigure}{0.23\textwidth}
\centering    
    \includegraphics[width=\textwidth]{Media/12L_openQA.png}
    % \caption{}
    \label{fig:12L_openQA}
    \end{subfigure}\hfill
\centering
    \begin{subfigure}{0.23\textwidth}
    \centering    \includegraphics[width=\textwidth]{Media/12L_mwp.png}
    % \caption{}
    \label{fig:12L_mwp}
    \end{subfigure}\hfill
\centering
    \begin{subfigure}{0.23\textwidth}
    \centering    \includegraphics[width=\textwidth]{Media/12L_primitives.png}
    % \caption{}
    \label{fig:12L_primitives}
    \end{subfigure}\hfill
    
    \begin{subfigure}{0.24\textwidth}
    \centering    \includegraphics[width=\textwidth]{Media/24L_closedQA.png}
    % \caption{}
    \label{fig:24L_closedQA}
    \end{subfigure}\hfill
    \centering
    \begin{subfigure}{0.23\textwidth}
\centering    
    \includegraphics[width=\textwidth]{Media/24L_openQA.png}
    % \caption{}
    \label{fig:24L_openQA}
    \end{subfigure}\hfill
\centering
    \begin{subfigure}{0.23\textwidth}
    \centering    \includegraphics[width=\textwidth]{Media/24L_mwp.png}
    % \caption{}
    \label{fig:24L_mwp}
    \end{subfigure}\hfill
\centering
    \begin{subfigure}{0.23\textwidth}
    \centering    \includegraphics[width=\textwidth]{Media/24L_primitives.png}
    % \caption{}
    \label{fig:24L_primitives}
    \end{subfigure}
    \vspace{-0.1in}
    \caption{\looseness-1Downstream evaluation for various task groups on the x-axis, vs validation log perplexity on the y-axis (reversed), as training proceeds. The top plots compare a 12-layer baseline model \loopy{12}{1} and the looped model \loopy{12}{2}.
    The second row compares the baseline 24-layer model and the 24-layer model trained with regularization using block size $k=4$ and $\lreg=10$ (See \Cref{eq:regularization}). For both comparisons we have similar observations. For closed book QA (memorization) tasks looping has very similar trends to baseline. For open book QA tasks and math word problems, looping has much better downstream performance at an equivalent log perplexity. This verifies the inductive bias of looping and regularization towards better reasoning abilities.}
    \label{fig:iso_plots_taskgroups_12L}
\end{figure*}
