\begin{figure*}[!tbp]% [H] is so declass\'e!
\centering
    \begin{subfigure}{0.24\textwidth}
    \centering    \includegraphics[width=\textwidth]{Media/8L_closedQA.png}
    % \caption{}
    \label{fig:8L_closedQA}
    \end{subfigure}\hfill
    \centering
    \begin{subfigure}{0.23\textwidth}
\centering    
    \includegraphics[width=\textwidth]{Media/8L_openQA.png}
    % \caption{}
    \label{fig:8L_openQA}
    \end{subfigure}\hfill
\centering
    \begin{subfigure}{0.23\textwidth}
    \centering    \includegraphics[width=\textwidth]{Media/8L_mwp.png}
    % \caption{}
    \label{fig:8L_mwp}
    \end{subfigure}\hfill
\centering
    \begin{subfigure}{0.23\textwidth}
    \centering    \includegraphics[width=\textwidth]{Media/8L_primitives.png}
    % \caption{}
    \label{fig:8L_primitives}
    \end{subfigure}\hfill
    \caption{\looseness-1Downstream evaluation for various task groups on the x-axis, vs validation log perplexity on the y-axis (reversed), as training proceeds. The top plots compare a 8-layer baseline model \loopy{8}{1} and the looped model \loopy{8}{3}.
    Similarly to \Cref{fig:iso_plots_taskgroups_12L}, for closed book QA (memorization) tasks looping has very similar trends to baseline. For open book QA tasks and math word problems, looping has much better downstream performance at an equivalent log perplexity.}
    \label{fig:iso_plots_taskgroups_8L}
\end{figure*}
