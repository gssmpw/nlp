\begin{figure*}[tbp]% [H] is so declass\'e!
\centering
    \hspace{-0.1in}
    \begin{subfigure}{0.33\textwidth}
    \centering    \includegraphics[scale=0.36]{Media/Heatmap.Model_Pile_CosReg10_blk4.VariableKind_self_attention.post.w}
    \caption{Attn:Q}
    \label{fig:cosines_cosreg_attn_q}
    \end{subfigure}\hfill
\centering
    \begin{subfigure}{0.33\textwidth}
    \centering    \includegraphics[scale=0.36]{Media/Heatmap.Model_Pile_CosReg10_blk4.VariableKind_ff_layer.ffn_layer1.linear.w.png}
    \caption{FFN:W1}
    \label{fig:cosines_cosreg_ffn_w1}
    \end{subfigure}\hfill
\centering
    \begin{subfigure}{0.33\textwidth}
    \centering   
    % width=\textwidth
     \includegraphics[scale=0.36]{Media/Heatmap.Model_Pile_CosReg10_blk4.VariableKind_self_attention.post.w}
    \caption{Attn:PostNorm}
    \label{fig:cosines_cosreg_postnorm}
    \end{subfigure}\hfill
    \caption{\looseness-1Cosine similarities for different layers in the model trained with the regularization strength $\lreg=10$ for block size $k=4$ (see \Cref{sec:regularization} for details). The 24 layer model will have 6 such blocks of size 4. The heatmap above shows the cosine similarities between weights for all pairs of blocks. Overall we find the final cosine similarity to be very high, thus suggesting a strong connection to looped models.}
    \label{fig:cosines_cosreg}
\end{figure*}


