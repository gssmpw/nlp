\begin{table}[!tbp]
\centering
\caption{\looseness-1Results for the 24-layer 1B model with and without the regularization introduced in \Cref{sec:regularization}. We try various block sizes $k$ motivated by the looped model settings from \Cref{table:language_modeling_results}. Overall, regularization helps retain the inductive bias towards reasoning, with notable improvements on math word problems and reasoning primitives, without almost neutral perplexity.}
\label{table:regularization_results}
\scalebox{0.75}{
% \begin{tabular}{lll|r|rrr|r}
\begin{tabular}{lc|cccc|c}
\toprule
 &  Perplexity ($\downarrow$) & Closed & Open   & Math Word & All Tasks & Reasoning  \\
 % &  & Perplexity ($\downarrow$) & Closed & Open   & Math Word & All Tasks & Reasoning  \\
  & \tiny{(validation)} & Book QA ($\uparrow$) & Book QA ($\uparrow$)  &  Problems ($\uparrow$) & Average ($\uparrow$) & Primitives ($\uparrow$) \\
  % & Params / FLOPs & \tiny{(validation)} & Book QA ($\uparrow$) & Book QA ($\uparrow$)  &  Problems ($\uparrow$) & Average ($\uparrow$) & Primitives ($\uparrow$) \\
  &  & \tiny{(4 tasks)} & \tiny{(5 tasks)}  &  \tiny{(6 tasks)} & \tiny{(15 tasks)}  & \tiny{(4 tasks)}  \\
  % &  &  & \tiny{(4 tasks)} & \tiny{(5 tasks)}  &  \tiny{(6 tasks)} & \tiny{(15 tasks)}  & \tiny{(4 tasks)}  \\
\midrule
\hline
Baseline & 7.40 & 11.2 & 33.9  & 29.3 & 26.0 & 47.5 \\
% Baseline & 24x / 24x & 7.40 & 11.2 & 33.9  & 29.3 & 26.0 & 47.5 \\
% \hline
\hline
% \rowcolor{lightgray}
Regularized ($k=4,\lreg=1$) & 7.41 & 11.2 & 34.8  & 31.6 & 27.2 & 42.5 \\
Regularized ($k=4,\lreg=10$) & 7.38 & 12.5 & 36.2  & 36.4 & 30.0 & 57.2\\
Regularized ($k=6,\lreg=10$) & 7.40 & 12.0 & 35.8  & 31.0 & 27.5 & 55.8\\
Regularized ($k=8,\lreg=10$) & 7.43 & 11.3 & 34.4 & 32.8 & 27.6 & 56.3\\
Regularized ($k=12,\lreg=10$) & 7.51 & 10.1 & 34.1 & 32.3 & 27.0 & 50.7\\
% \hline
% \hline
% \rowcolor{lightgray}
% \multicolumn{3}{c}{8 layer model}  \\
\hline
\end{tabular}
}
\end{table}