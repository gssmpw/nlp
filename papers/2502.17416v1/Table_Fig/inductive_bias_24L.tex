\begin{figure*}[!tbp]% [H] is so declass\'e!
\centering
    \begin{subfigure}{0.24\textwidth}
    \centering    \includegraphics[width=\textwidth]{Media/24L_closedQA.png}
    % \caption{}
    \label{fig:24L_closedQA}
    \end{subfigure}\hfill
    \centering
    \begin{subfigure}{0.23\textwidth}
\centering    
    \includegraphics[width=\textwidth]{Media/24L_openQA.png}
    % \caption{}
    \label{fig:24L_openQA}
    \end{subfigure}\hfill
\centering
    \begin{subfigure}{0.23\textwidth}
    \centering    \includegraphics[width=\textwidth]{Media/24L_mwp.png}
    % \caption{}
    \label{fig:24L_mwp}
    \end{subfigure}\hfill
\centering
    \begin{subfigure}{0.23\textwidth}
    \centering    \includegraphics[width=\textwidth]{Media/24L_primitives.png}
    % \caption{}
    \label{fig:24L_primitives}
    \end{subfigure}\hfill
    \caption{\looseness-1Downstream evalulation vs validation log perplexity isoplots as training proceeds for baseline and looped 1B models trained on the same data. On the y-axis we track the performance on various task groups -- closed book QA, open book QA, math word problems and our reasoning primitives from \Cref{sec:reasoning_primitives}. On the x-axis the log perplexity is presented in the reverse order, thus downstream performance for both methods improves as log perplexity gets lower. For closed book QA (memorization) tasks  looping has very similar trends to baseline. For open book QA tasks and math word problems, looping has much better downstream performance at an equivalent log perplexity. This showcases the inductive bias of looping towards better overall quality and better reasoning abilities.}
    \label{fig:iso_plots_taskgroups_24L}
\end{figure*}
