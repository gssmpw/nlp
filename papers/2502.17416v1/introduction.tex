\section{Introduction}
\label{sec:intro}



\iffalse

Outline
\begin{itemize}
    \item Scaling laws suggest that more parameters are needed to solve more and harder problems. Recent work (igsm) argues that not just params, but depth is very important for reasoning
    \item In this work we argue further -- reasoning needs depth, but not necessarily many parameters. Intuitive you want to learn the right ``algorithm'' to solve the problem, which can be concise
    \item Show many synthetic examples highlighting this point. 3L-4loop is much closer to 12L than to 3L on reasoning problems.
    \item Some theory? (b) parameter sharing can still solve reasoning problems of interesting: e.g. k-hop (and other problems like graph algorithms), (c) general simulation result with mild width expansion, (d) something about graph algorithms, RASP, (e) separation loop(n+1) vs loop(n), (a) CoT is one way to increase depth in autoregressive models. Loop can simulate CoT, so we inherit its expressivity.
    \item Inspired by this, we pretrain LLMs with block looping. Performance is lagging on most tasks compared to non-looped, especially on perplexity, but the most improvements are on reasoning -- ``inductive bias''
    \item Is there a way to utilize this inductive bias? Train with regularization on layers. better on everything, especially reasoning. Bonus: make it looped post-hoc
\end{itemize}



TODOs
\begin{itemize}
    \item Synthetic expts on \textbf{var-assignment, k-hop, addition, i-gsm}
    \item Theory for (a), (b) and (c) at least
    \item Real experiments
    \begin{itemize}
        \item (Running) Get all evals for current looping and regularization experiments and include in the paper
        \item (Running) Rerun experiments on PILE 
    \end{itemize}
\end{itemize}

\fi

\looseness-1Language models have shown a lot of promise in solving problems that require strong reasoning abilities like math, coding, common sense reasoning and logical puzzles~\citep{brown2020language, team2023gemini}.
This has sparked interest in developing techniques to improve reasoning on harder problems \citep{wei2022chain} and has inspired theoretical studies on how Transformers are able to perform reasoning \citep{feng2024towards,sanford2024understanding}.
Reasoning abilities are often emergent in larger language models \citep{wei2022emergent} -- this aligns with various scaling laws \citep{kaplan2020scaling,hoffmann2022training,allen2024physics} that show that the performance of language models is very strongly dependent on the model size (i.e., number of parameters) and much lesser on other architectural design choices.
% So does this mean {\em parameters are all you need for reasoning?}
However, recent works have started to question this view. \citet{ye2024physics} argue that scaling laws for reasoning are more subtle, and {\em depth is very important} in addition to parameter count -- at the same parameter count, deeper but shallower models are better.
This is a deviation from the conventional scaling law wisdom, but it intuitively makes sense because reasoning problems often requires multi-step compositional thinking, and thus, depth can play a crucial role.


\looseness-1In this work, we make a stronger claim -- while depth is important, many reasoning problems do not necessarily require a lot of parameters. How does one solve reasoning problems with large depth but few parameters? We argue that {\em looped models} are perfectly suited for this, where the same function, parameterized with few parameters, is iteratively applied on the input. This leads us to our first important claim:
\vspace{-0.05in}
\begin{center}
    \textit{Claim 1: Many reasoning problems require depth but not necessarily parameters. That is, they can be solved via looped models}
\end{center}
\vspace{-0.05in}
Looped models have been studied in the literature for parameter efficiency \citep{lan2019albert}, adaptive compute \citep{dehghani2018universal}, equilibrium models \citep{bai2019deep} and for in-context learning \citep{yang2023looped,gatmiry2024can}.
In this work, we {\bf initiate the study of looped models in the context of reasoning}. Admittedly, reasoning is not very well-defined and can be of various forms \citep{sun2023survey}. Acknowledging this hurdle, we focus on a non-exhaustive list of problems that intuitively require reasoning and that are inspired by reasoning benchmarks.
Throughout the paper, we use the notation \loopy{k}{L} to denote a $k$-layer model looped $L$ times (precise definition in \Cref{sec:synthetic_reasoning}), which has the same number of parameters as a \loopy{k}{1} model and same flops as a \loopy{kL}{1} non-looped model (see \Cref{fig:looping_illustration}). 
As a first step towards connecting looped models and reasoning, we empirically evaluate looped models on several simple reasoning tasks in the literature (e.g. \Cref{sec:synthetic_reasoning}). Perhaps surprisingly,  we find that a \loopy{k}{L} looped models does almost as well as, if not better than, a non-looped model \loopy{kL}{1} that has the same effective depth but $L$ times more parameters on these reasoning tasks. The looped model is also significantly better than a \loopy{k}{1} model which has the same number of parameters. Our theoretical results on the expressiveness of looped models in representing iterative algorithms with short description further corroborate these empirical findings and provide strong support for our claim. This naturally raises an important question: do looped models benefit language modeling in a similar manner?

\input{Table_Fig/looping_illustration}

\begin{center}
    \textit{Claim 2: For language modeling, looped models have an inductive bias towards good reasoning despite having worse perplexity and memorization to an iso-flop non-looped model}
\end{center}
\vspace{-0.05in}
% \ns{Need to explain what we mean by inductive bias}
\looseness-1For the above claim, we again train a \loopy{k}{L} looped model on causal language modeling and compare it to the iso-param \loopy{k}{1} and iso-flop \loopy{kL}{1} non-looped baselines. While the looped model improves over the iso-param baseline, perhaps unsurprisingly, it ends up with worse perplexity than iso-flop baseline, since perplexity depends strongly on number of parameters. However, the downstream evaluations reveal an intriguing trend: looped models have a tendency to improve tasks that require reasoning a lot more than memorization tasks.
Specifically, the looped model has reasoning performance much closer to the iso-flop baseline, sometimes even exceeding it despite having $L$ times fewer parameters and worse perplexity.
This contrasting behavior between the pretraining and downstream metrics has been a subject of study lately \citep{saunshi22understanding,liu2023same} and is attributed to the \emph{inductive biases} introduced due to different architectures and training algorithms. Our empirical analysis also uncovers an interesting phenomenon: accuracy on downstream tasks scale as logarithm of the effective depth. In particular, more loops enhances performance, and the relative benefit of loops is higher for tasks that require more reasoning. This is conceptually similar to inference time scaling discovered for CoT, but with looping as a central component. To further elucidate this interesting relationship of looped models with CoT, we present the following claim.

\vspace{-0.05in}
\begin{center}
    \textit{Claim 3: Looped models generate latent thoughts and can, in theory, simulate CoT reasoning}
\end{center}
\vspace{-0.05in}

Note that CoT reasoning gives the model more time and compute by generating multiple thought tokens before the answer, and it has powered the recent paradigm of inference-time scaling for ``thinking'' models like O1 and DeepSeek's R1 \citep{guo2025deepseek}. We make an observation about CoT reasoning -- it is essentially a looped model that generates 1 thought token in each iteration. However, looped models seem to be much more powerful, since they can generate multiple \emph{latent thoughts} in each iteration. 
We translate this intuition into a theoretical result about how looped models can simulate CoT reasoning.

\iffalse 
As a first step towards connecting looped models and reasoning, we empirically evaluate looped models on simple (non-exhaustive) set of problems that have been used to study reasoning in the literature -- addition \citep{nye2021show,nogueira2021investigating,li2024chain}, $p$-hop induction \citep{sanford2024transformers}, variable assignment \citep{saunshi2024inductive} and math word problems \citep{wei2022chain,ye2024physics}.
For each of these reasoning problems, rather surprisingly, we find that a \loopy{k}{L} looped models does almost as well as (or sometimes even better than) a non-looped model \loopy{kL}{1} that has the same effective depth but $L$ times more parameters. Additionally, the looped model is significantly better than a \loopy{k}{1} model which has the same number of parameters.
This suggests that these problems require depth but not necessarily more parameters, and that looped models are very good for independently solving such simple reasoning problems. 
We support these findings with theoretical results on the expressiveness of looped models -- many such reasoning problems can be solved by iterative algorithms with a short description, and that such algorithms are implementable by looped models with few parameters.

%To further elucidate the importance of depth over parameters, we connect it to chain-of-thought (CoT) reasoning \citep{wei2022chain} and show how CoT can be implemented via looped models.
These results are surprising given the prevalent notion that model size is important for good reasoning.
\new{What about problems that do not obviously have an iterative algorithm? We broaden the scope of looped models by establishing a connection to chain-of-thought (CoT) reasoning \citep{wei2022chain}.}

\input{Table_Fig/looping_illustration}


\vspace{-0.05in}
\begin{center}
    \textit{Claim 2: Looped models generate latent thoughts and can, in theory, simulate CoT reasoning}
\end{center}
\vspace{-0.05in}

\new{CoT reasoning gives the model more time and compute by generating multiple thought tokens before the answer, and it has powered the recent paradigm of inference-time scaling for ``thinking'' models like O1 and DeepSeek's R1 \citep{guo2025deepseek}. We make an observation about CoT reasoning -- it is essentially a looped model that generates 1 thought token in each iteration. However, looped models seem to be much more powerful, since they can generate multiple ``latent thoughts'' in each iteration. 
We translate this intuition into a theoretical result about how looped models can simulate CoT reasoning.
Given this backdrop, a natural question arises: are looped models relevant for language modeling?}
% In the era of foundation models, we are interested in models that are not specialized for a single task, but can solve many different reasoning problems. This provokes the question: are looped models relevant for language modeling?
\vspace{-0.05in}
\begin{center}
    \textit{Claim 3: For language modeling, looped models have an inductive bias towards good reasoning despite having worse perplexity}
\end{center}
\vspace{-0.05in}
% \ns{Need to explain what we mean by inductive bias}
For the above claim, we again train a \loopy{k}{L} looped model on causal language modeling and compare it to the iso-param \loopy{k}{1} and iso-flop \loopy{kL}{1} non-looped baselines. While the looped model improves over the iso-param baseline, perhaps unsurprisingly, it ends up with worse perplexity than iso-flop baseline, since perplexity depends strongly on number of parameters. However, the downstream evaluations reveal an intriguing trend: looped models have a tendency to improve tasks that require reasoning a lot more than memorization tasks.
Specifically, the looped model has reasoning performance much closer to the iso-flop baseline, sometimes even exceeding it despite having $L$ times fewer parameters and worse perplexity.
This contrasting behavior between the pretraining and downstream metrics has been a subject of study lately \citep{saunshi22understanding,liu2023same} and is attributed to the \emph{inductive biases} introduced due to different architectures and training algorithms.
\ns{Add something about scaling behavior}
%to the fact that different architectures and training algorithms can have different {\em inductive biases} in terms of how the pretraining performance translates to downstream performance. 
%As proposed by \citet{saunshi22understanding}, we plot perplexity vs downstream evaluation iso-plots as training proceeds, and find that looped models have much better reasoning performance at the same perplexity as a non-looped baseline for various values of $k$ and $L$.

\fi


Motivated by these findings, we propose a regularization scheme that aims to tap into the inductive bias of looped models towards reasoning.
This leads us to our final claim:
\vspace{-0.05in}
\begin{center}
    \textit{Claim 4: Looping-inspired regularization can leverage this inductive bias towards better reasoning}
\end{center}
\vspace{-0.05in}

With the backdrop of these claims, we concretely present the contributions of the paper below:
\begin{itemize}
    \item In this paper we study looped models -- multilayer models with weight sharing -- and their role in reasoning. In particular, we compare a $k$-layer model looped $L$ times, denoted by \loopy{k}{L}, with an {\em iso-param} \loopy{k}{1} non-looped model with $k$ layers and an {\em iso-flop} \loopy{kL}{1} model with $kL$ layers and $L$ times more parameters.
    \item We conduct experiments on synthetic reasoning tasks like addition, $p$-hop induction and GSM-style math word problems in \Cref{sec:synthetic_reasoning}. For these tasks, we surprisingly find that iso-flop looped models, despite having way fewer parameters, can nearly match or outperform a non-looped model. Supporting these experiments, in \Cref{sec:theory} we present theoretical results for why looped models can solve such problems with almost optimal depth.
    \item In \Cref{sec:language_modeling}, we train looped models on causal language modeling at 1B parameter scale. Here, we show that looped models have an inductive bias towards doing well on reasoning benchmarks, despite having much worse perplexity. This finding is novel, since most prior work on looping focused more on perplexity metrics rather than downstream reasoning tasks. We validate this inductive bias by visualizing the perplexity vs downstream performance plots as training process.
    \new{Additionally, we show that looped models demonstrate good scaling behavior on various benchmarks as the number of loops are increased, akin to CoT reasoning.  
    Finally, we show that looped models, along with a scratchpad, can simulate chain-of-thought reasoning.}
    \item Inspired by this inductive bias, in \Cref{sec:regularization}, we propose a regularization that encourages layers to be more similar to each other. We find that training with such a regularization inherits the inductive bias of looped models towards reasoning without affecting perplexity.
\end{itemize}
\vspace{-0.05in}





