% \documentclass[preprint,12pt]{elsarticle}
\documentclass[journal]{IEEEtran}
% \includegraphics
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{lineno}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{algorithm} 
\usepackage{algpseudocode} 
\usepackage{enumerate}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{multirow} 
\usepackage{rotating}
\usepackage{pifont}
\usepackage{booktabs} % For better table lines
\usepackage{tabularx} % For adjustable width tables
\usepackage{siunitx}  % For aligning numbers by decimal point
\usepackage{ragged2e} % For text alignment
\usepackage{array}    % For new column types
\usepackage{subcaption}
\usepackage{tabularx} % 
\usepackage{caption}  % For adding captions to tables
\usepackage[table,xcdraw]{xcolor}
\usepackage{authblk}
\usepackage{orcidlink}



\usepackage[pscoord]{eso-pic}% The zero point of the coordinate systemis the lower left corner of the page (the default).

\newcommand{\placetextbox}[3]{% \placetextbox{<horizontal pos>}{<vertical pos>}{<stuff>}
  \setbox0=\hbox{#3}% Put <stuff> in a box
  \AddToShipoutPictureFG*{% Add <stuff> to current page foreground
    \put(\LenToUnit{#1\paperwidth},\LenToUnit{#2\paperheight}){\vtop{{\null}\makebox[0pt][c]{#3}}}%
  }%
}%
\usepackage{amsmath, amssymb, amsthm}
\usepackage{array}
\usepackage{color}
% \usepackage{subfigure}
\hyphenation{op-tical net-works semi-conduc-tor}


% *** IEEE Copyright notice with TikZ ***
%
\usepackage{tikz}
\usepackage{textcomp}
\usepackage{hyperref}
\usepackage{lipsum}

% Define a new column type for X columns with centering
\newcolumntype{C}{>{\centering\arraybackslash}X}

% Setup for siunitx to align numbers
\sisetup{
  table-format=1.3,   % Adjust this according to your numbers
  table-space-text-post=±,
  input-symbols=±(),
  group-digits=false
}
\newcommand{\xmark}{\ding{55}}%
\newcommand*{\baan}[1]{\textcolor{black}{ #1}}
\newcommand*{\updatedText}[1]{\textcolor{black}{ #1}}


\begin{document}

\title{DAM-Seg: Anatomically accurate cardiac segmentation using Dense Associative Networks}

\author[1]{Zahid~Ullah\orcidlink{0000-0002-0184-7620}}
\author[1,*]{Jihie~Kim\orcidlink{0000-0003-2358-4021}}
% \author[3,*]{Muhammad~Usman\orcidlink{0000-0002-7278-8488}}
% \author[4]{Abdullah~Shahid\orcidlink{0000-0000-0000-0003}}
% \author[5]{Sung-Min~Gho}
% \author[6]{Aleum~Lee}
% \author[7]{Tariq~M.~Khan\orcidlink{0000-0002-7477-1591}}
% \author[8]{Imran~Razzak\orcidlink{0000-0002-3930-6600}}


\affil[1]{Division of AI Software Convergence\\
	Dongguk University\\
	Seoul 04620, Republic of Korea}
% \affil[2]{Seoul National University Graduate School, Seoul, 03080, Republic of Korea}
% \affil[3]{Stanford University, CA 94305, USA}
% \affil[4]{DeepChain AI\&IT Technologies, Islamabad 44000, Pakistan}
% \affil[5]{Medical R\&D Center, DeepNoid Inc., Seoul, 08376, South Korea}
% \affil[6]{Soonchunhyang University Bucheon Hospital, Bucheon-si, Gyeonggi-do, 14584, South Korea}
% \affil[7]{Naif Arab University for Security Sciences, Riyadh, 11452, Kingdom of Saudi Arabia}
% \affil[8]{University of New South Wales, Sydney, NSW 2052, Australia}
\affil[*]{Corresponding E-mail: \href{mailto:jihie.kim@dgu.edu}{jihie.kim@dgu.edu}}

\maketitle


% \address[aff1]{R\&D Center,  WONIK PNE CO., LTD., Suwon-si, Gyeonggi-do, Republic of Korea }
% \address[aff2]{Department of Biomedical Sciences, Seoul National University Graduate School, Seoul, 03080, Republic of Korea}
% \address[aff3]{Department of  Biomedical Data Science, Stanford University, CA 94305, USA}
% \address[aff4]{Department of Artificial Intelligence, DeepChain AI\&IT Technologies, Islamabad 44000, Pakistan}
% \address[aff5]{Medical R\&D Center, DeepNoid Inc., Seoul, 08376, South Korea}
% \address[aff6]{Department of Radiology, Soonchunhyang University Bucheon Hospital, Bucheon-si, Gyeonggi-do, 14584, South Korea}
% \address[aff7]{Center of Excellence in Cybercrimes and Digital Forensics, Naif Arab University for Security Sciences, Riyadh, 11452, Kingdom of Saudi Arabia}
% \address[aff8]{School of Computer Science and Engineering, University of New South Wales, Sydney, NSW 2052, Australia}


\begin{abstract}
Deep learning-based cardiac segmentation has seen significant advancements over the years. Many studies have tackled the challenge of anatomically incorrect segmentation predictions by introducing auxiliary modules. These modules either post-process segmentation outputs or enforce consistency between specific points to ensure anatomical correctness. However, such approaches often increase network complexity, require separate training for these modules, and may lack robustness in scenarios with poor visibility. To address these limitations, we propose a novel transformer-based architecture that leverages dense associative networks to learn and retain specific patterns inherent to cardiac inputs. Unlike traditional methods, our approach restricts the network to memorize a limited set of patterns. During forward propagation, a weighted sum of these patterns is used to enforce anatomical correctness in the output. Since these patterns are input-independent, the model demonstrates enhanced robustness, even in cases with poor visibility. The proposed pipeline was evaluated on two publicly available datasets, CAMUS and CardiacNet. Experimental results indicate that our model consistently outperforms baseline approaches across all metrics, highlighting its effectiveness and reliability for cardiac segmentation tasks.
\end{abstract}

\begin{IEEEkeywords}
Hopfield Networks, Dense associative networks, Cardiac segmentation, Robust segmentation.
\end{IEEEkeywords}

% \end{frontmatter}
\IEEEpeerreviewmaketitle

\section{Introduction}
\label{intro}
With the advent of deep learning, cardiac image segmentation has become a cornerstone of cardiovascular research, playing a pivotal role in diagnostics, disease monitoring, treatment planning, and prognosis. Over the past decade, the field has undergone a significant transformation, fueled by advancements in deep learning methodologies. Non-invasive medical imaging modalities such as magnetic resonance imaging (MRI), computed tomography (CT), and ultrasound have provided critical support by offering detailed insights into cardiac structures. When combined with deep learning techniques, these imaging methods enable the development of efficient and accurate diagnostic tools for medical professionals.

Cardiac segmentation, a key application of deep learning, involves the semantic division of regions within cardiac images. Researchers have made strides in this area by refining existing segmentation methodologies and introducing novel approaches to achieve robust and precise results. For instance, Petitjean et al. \cite{RV_segmentation} conducted a comprehensive analysis of right ventricle segmentation techniques, benchmarking various methods against expert-annotated segmentation masks. Similarly, Tran et al. \cite{cardiac_fcn} employed a fully convolutional network (FCN) architecture \cite{fcn} to perform cardiac segmentation, demonstrating the efficacy of deep learning models in this domain.


Significant research efforts have been dedicated to enhancing the robustness of cardiac segmentation models across diverse datasets and distributions. For instance, Li et al. \cite{displacement_aware} introduced a displacement-aware shape encoding and decoding framework. In their approach, the input is compactly encoded while accounting for potential displacements and deformations. These encoded features are then decoded into multiple shapes, enabling the model to handle a broader range of shape variations. Additionally, they proposed a dynamically expanding mechanism that allows the model to adapt to increased complexity by incorporating new modules as needed, further improving robustness and scalability.

Cai et al. \cite{cross_domain} addressed cross-domain challenges by introducing a cross-domain mixup strategy. This method trains the cardiac segmentation model to generalize effectively on a target dataset by leveraging knowledge from a source dataset. By reducing the performance gap between the two domains, their approach enhances the model's generalization ability, ensuring consistent performance across varying datasets. These advancements collectively contribute to the development of more robust and adaptable cardiac segmentation frameworks.

Traditional deep learning techniques often struggle with cases where the cardiac structure is partially obscured, as the lack of visibility can hinder the formation of accurate boundaries. Researchers have addressed this challenge through innovative approaches. For example, Painchaud et al. \cite{VAE_wrapping} introduced a method that enhances segmentation performance by leveraging Variational Autoencoders (VAEs). Their architecture wraps implausible segmentation predictions toward valid cardiac shapes. In this approach, the VAE is trained independently to learn representations of anatomically correct cardiac structures, enabling the segmentation model to correct invalid outputs effectively.

Similarly, the Graph Convolutional Network (GCN) \cite{GCN} offers a unique solution to the problem of anatomically incorrect predictions. Instead of performing pixel-wise classification, GCN predicts key boundary points of the cardiac structure. It employs a graph neural network to model relationships between adjacent key points, effectively capturing spatial dependencies. By learning these relationships, the model ensures anatomically consistent predictions and maintains the structural integrity of the cardiac segmentation, even in challenging scenarios. These methods demonstrate significant progress in overcoming visibility-related limitations in cardiac segmentation tasks.

A quick and effective approach to maintaining anatomical accuracy in poorly visible cardiac images involves enabling the network to store a general representation of cardiac structures and leveraging this information to generate anatomically consistent segmentation masks. Ramsauer et al. \cite{SA_update} introduced a continuous dense associative memory unit, which employs a self-attention-based mechanism to dynamically update the stored memory states within the network. This concept, inspired by biological neural networks and their efficient pattern storage and retrieval capabilities, holds significant promise for cardiac segmentation.

In the context of cardiac imaging, such a memory unit can learn and store typical cardiac shapes and structures, providing a robust foundation for handling image quality variations and poor visibility. Integrating this mechanism can enhance the model's robustness, ensure consistent anatomical outputs, and improve performance, especially on small datasets. Building on these principles, we leverage the pattern-learning and storage capabilities of dense associative networks to enhance the segmentation performance of our proposed dense associative memory segmentation (DAM-Seg) model.

The code implementation of our method will be made publicly available upon publication to support reproducibility and further research. Code Link: \url{https://github.com/Zahid672/cardio-segmentation-main/tree/main/cardio-segmentation-main}.

The contributions of our research are as follows:
\begin{itemize}
    % \item We introduce learnable memory transformation matrices, that transform the static memory space into query and value matrices that are not influenced by the input.
    % \item We modify the memory update mechanism to induce $m$ meta-stable states instead of learning all input patterns.
    % \item We utilize DPT \cite{dpt} as our base segmentation network and integrate it with our memory module to perform anatomically correct segmentation.
    \item We propose learnable memory transformation matrices that convert the static memory space into query and value matrices, independent of the input, enabling a robust and flexible representation of stored patterns.
    \item We enhance the memory update mechanism to induce $m$ meta-stable states, focusing on generalizable patterns rather than memorizing all input variations, thereby improving robustness and efficiency.
    \item We integrate our memory module with the DPT architecture \cite{dpt}, leveraging its segmentation capabilities to achieve anatomically accurate and consistent cardiac segmentation.
\end{itemize}


% The remainder of the paper is organized as follows: Section \ref{rw_section} presents a comprehensive review of related studies, providing the context and background for our work. Section \ref{pro} details the proposed framework, including its various components and the underlying principles. Section \ref{es_section} elaborates on the dataset specifications and preprocessing steps, as well as the architectural details and training strategy of the model. Section \ref{res} showcases the experimental results, accompanied by an in-depth analysis. Finally, Section \ref{con} concludes the paper by summarizing the key findings and discussing future research directions, with a focus on potential improvements and extensions to the proposed approach.
% The remaining paper is structured as follows: Section \ref{rw_section} provides an extensive overview of related studies, establishing the context for our work. Section \ref{pro} introduces our proposed framework, describing its architecture, foundational concepts, datasets, and training methodology. Section \ref{es_section} describes the dataset and details of preprocessing. Also model architectural details and training strategy. Section \ref{res} highlights our experimental results and offers a detailed analysis. Lastly, Section \ref{con} summarizes our key findings and discusses future research opportunities, emphasizing possible enhancements and extensions to the proposed approach.





\begin{table*}[!ht]
\centering
\scriptsize
\caption{Related studies and their methodologies with drawbacks.}
\color{black}
\label{table:research_methodology}
\scalebox{1.7}{
\begin{tabular}{|l|l|l|}
\hline
\textbf{Paper}                                         & \textbf{Methodology}                                                                                                                                                                                                                                                & \textbf{Drawbacks}                                                                                                                                                                                                         \\ \hline
Tran et al. \cite{cardiac_fcn}       & \begin{tabular}[c]{@{}l@{}}Uses FCN trained in an End-to\\ -End manner.\end{tabular}                                                                                                                                                                                & \begin{tabular}[c]{@{}l@{}}The segmentation of \\ cardiac structures, especially\\  at the boundaries, can be\\  challenging due to \\ fuzzy edge information.\end{tabular}                                                \\ \hline
Li et al. \cite{displacement_aware}  & \begin{tabular}[c]{@{}l@{}}Proposed a displacement-aware\\  shape encoding and decoding \\ mechanism where the input is \\ first encoded into a compact \\ form while keeping the \\ potential displacement or \\ deformations under \\ consideration.\end{tabular} & \begin{tabular}[c]{@{}l@{}}The method's ability to adapt\\  to multiple domains\\  might lead to overfitting on \\ the specific datasets \\ used, potentially limiting its \\ generalization to  unseen data.\end{tabular} \\ \hline
Cai et al. \cite{cross_domain}       & \begin{tabular}[c]{@{}l@{}}propose a cross-domain mixup  \\ mechanism that trains the cardiac\\  segmentation model to perform \\ well on a certain target dataset \\ using the information from the\\  source dataset.\end{tabular}                                & \begin{tabular}[c]{@{}l@{}}The method may struggle with\\  a very large\\  differences between modalities.\end{tabular}                                                                                                    \\ \hline
Painchaud et al. \cite{VAE_wrapping} & \begin{tabular}[c]{@{}l@{}}use variational Auto encoders to\\  improve the segmentation\\  performance of a pre-trained \\ segmentation model by wrapping\\  the implausible predictions towards \\ the valid cardiac shape.\end{tabular}                           & \begin{tabular}[c]{@{}l@{}}The need for millions of latent \\ vectors to be  stored in memory.\end{tabular}                                                                                                                \\ \hline
GCN \cite{GCN}                        & \begin{tabular}[c]{@{}l@{}}Proposes a combination of a \\ convolutional encoder and a\\  graph decoder, it addresses \\ the problem of dealing with\\  anatomically incorrect \\ predictions.\end{tabular}                                                          & \begin{tabular}[c]{@{}l@{}}This kind of strict enforcement \\ of anatomical  rules might \\ potentially miss rare but valid \\ anatomical variations\end{tabular}                                                          \\ \hline
\end{tabular}}
 \end{table*}


\section{Background information}\label{related}
This section explores how associative memory models have evolved through the years and their applications, providing context for understanding dense associative memories. Table \ref{table:research_methodology} shows the relevant studies with drawbacks.

Associative memory research has its roots in early neural network models, with the Hopfield network \cite{hopfield_orig} being a seminal contribution. Introduced by John Hopfield in 1982, this model demonstrated how a network of interconnected neurons could store and retrieve patterns, mimicking the associative properties of human memory.

Krotov and Hopfield \cite{polynomial_DAM} generalize the energy function to $E = -\sum_{\mu}F(\sum_i\xi_{\mu i}\sigma_i)^n$. This enables the implementation of multi-spin configurations as shown by Equations \ref{eq:multi_config_1} and \ref{eq:multi_config_2}.

\begin{equation}
    E = -\sum_{\mu}F(\sum_i\xi_{\mu i}\sigma_i)^2 = -\sum_{i,j} T_{ij}\sigma_i\sigma_j
    \label{eq:multi_config_1}
\end{equation}

\begin{equation}
    E = -\sum_{\mu}F(\sum_i\xi_{\mu i}\sigma_i)^3 = -\sum_{i,j,k} T_{ijk}\sigma_i\sigma_j\sigma_k
    \label{eq:multi_config_2}
\end{equation}

where $F(x) = x^2$ where $F: \mathbb{R} \rightarrow \mathbb{R}$. Equation \ref{eq:polynomial_update_rule} is used as an update rule using this generalized energy function. It works by taking the difference of two energy functions, one with spin $i$ on and the other with it being off.

\begin{equation*}
    \sigma_{i}^{(t+1} = Sign\left[ \sum_{\mu = 1}^{K} \left( F\left( \xi_{i}^{\mu} + \sum_{j\neq i}^{N} \xi_{j}^{\mu} \sigma_{j}^{(t)} \right) - F\left( -\xi_{i}^{\mu} + \sum_{j\neq i}^{N} \xi_{j}^{\mu} \sigma_{j}^{(t)} \right) \right) \right]
    \label{eq:polynomial_update_rule}
\end{equation*}

When $n = 2$ this update rule reduces to the standard update rule. In cases with $n > 2$, each term in the energy function becomes sharper allowing a larger number of memories to be packed into the same configuration space.





Demircigil et al. \cite{exponential_Demircigil} suggest an exponential interaction function $F(x) = e^x$ that results in an energy $E = $. This exponentially increases the storage capacity of the network while still maintaining a large positive radius of attraction. This means that this increase in storage capacity does not come at the expense of associativity, and the stored patterns can still be retrieved.


Ramsauer et al. \cite{SA_update} generalize the exponential energy function by Demircigil et al. \cite{exponential_Demircigil} for continuous valued inputs while still keeping its exponential storage capacity. They propose the following new energy function: 

\begin{equation}
    E = -lse(\beta, X^T\xi) + \frac{1}{2}\xi^T\xi + \beta^{-1}logN + \frac{1}{2}M^2
    \label{eq:continous_energy}
\end{equation}



Immune repertoire classification \cite{DeepRC} uses a version of the self-attention update rule with static memory. The goal of immune repertoire classification is to find a certain sequence of immune repertoire receptors, this sequence is learned and stored in the static memory. They use a fixed query vector represented by $\xi$ as their static memory. The transformer attention mechanism is implemented as follows:

\begin{equation}
    z = softmax \left(\frac{\xi^TK^T}{\sqrt{d_k}}\right)Z
    \label{eq:DeepRC_update}
\end{equation}

Where $Z \in \mathbb{R}^{N\times d_v}$ is the value and $K \in \mathbb{R}^{N\times d_k}$ is the key.

\section{Proposed Methodology} 
\subsection{Overview}
We propose an associative memory mechanism with static memory that learns the general structure of the heart. We use the Universal Hopfield Networks (UHN) \cite{universal_networks} model to develop our mechanism. It provides a comprehensive analysis of Hopfield networks, decomposing them into three fundamental components: similarity, separation, and projection as represented by Equation \ref{eq:universal_separation}, where $P$, $sep$ and $sim$ represent projection, separation, and similarity, respectively. By separating the update rule into these three components, the UHN framework provides a more general and flexible approach to understanding and designing associative memory models.

\begin{equation}
    z = P \cdot sep (sim(M, q))
    \label{eq:universal_separation}
\end{equation}

\subsection{Main Architecture}
When a sample with a partially visible heart structure is presented to the memory module, it completes the structure and provides the model with information on the areas that are not clearly visible in the input. Our mechanism works on three matrices \textbf{K} (key), \textbf{Q} (query), and \textbf{V} (value).

% \begin{figure*}[t]
% \centering
% \includegraphics[width=1\textwidth]{images/visual_sMRI_fMRI.png}
% \caption{\updatedText{Visualization of structural magnetic resonance imaging (sMRI) and functional magnetic resonance imaging (fMRI) in male and female patients across various age groups. sMRI reveals the anatomical details of the brain, whereas fMRI depicts brain activity by measuring changes in blood flow; this is shown on a color scale where warmer colors typically indicate higher levels of activity. }}
% \label{samples_visualization}
% \end{figure*}

\begin{figure*}[!ht]
\centering
\includegraphics[width=1\textwidth]{Figures/pattern_space.png}
\caption{A neuron view showing how query interacts with the stored patterns.}
\label{fig:neuron_view}
\end{figure*}


\begin{figure*}[!ht]
\centering
\includegraphics[width=1\textwidth]{Figures/memory_architecture.png}
\caption{Architectural diagram of the modified attention mechanism.}
\label{fig:architecture_diagram}
\end{figure*}

We introduce a static memory with state $\xi \in \mathbb{R}^d$. Using this static memory we set the matrices $V = W_q \cdot \xi$ and $K = W_k \cdot \xi$. Where $V \in \mathbb{R}^{m\times d}$, $K \in \mathbb{R}^{m \times d}$ and $W_q = (W_K)^T$. In our case, we use Value as the projection, softmax as separation, and dot product and similarity, resulting in an update rule shown in Equation \ref{eq:our_update}. The goal of our approach is to encourage the formation of $m$ main attractors.

\begin{equation}
    z = V \cdot Softmax \left(\frac{Q^T \cdot K}{\sqrt{d}}\right)
    \label{eq:our_update}
\end{equation}

During the learning process, the continuum of states will be clustered around these $m$ main attractors establishing $m$ meta-stable states. Each pattern is similar to every other pattern in the cardiac dataset with slight differences like orientation or size, simply training an associative memory module on all patterns would overwhelm the network and cause it to converge to a mixture of all patterns and would defeat the purpose of our memory module. Since the aim of our approach is not to learn each input pattern but to memorize the general structure of the heart in the input in multiple slight variations, it is essential to establish a small number of meta-stable states each of which contains the structure of the heart with slightly different structures.



Figure \ref{fig:neuron_view} shows a neuron view for our memory module with three meta-stable states, it demonstrates how query interacts with stored patterns. Each circular region represents an n-sphere and $d$ represents the radius of attraction for each region. The similarity between the query and the key (derived from the stored pattern) measures the cosine similarity between the query and each stored pattern. The pattern with the highest similarity is selected which in our case is the red one.

The similarity between Q and K identifies which meta-stable state is the input closely related to, and softmax as our separation function provides probabilistic interpretations of the calculated similarity. These similarity probabilities are projected onto the value vector resulting in a weighted sum of $m$ metastable states. Our update rule and energy function 
% as shown in Equation \ref{eq:our_energy} 
closely resembles the update rule by Model B proposed by Krotov and Hopfield \cite{largeassociativememoryproblem} where $N_F$ represents the number of inputs.

% \begin{equation}
%     E = \frac{1}{2} \sum_{i = 1}^{N_f} K_{i}^{2} - log\left(\sum_{\mu}exp\left(\sum_{i}W_q K_i\right)\right)
%     \label{eq:our_energy}
% \end{equation}




We use a transformer-based segmentation architecture called DPT \cite{dpt} as our base model. Each transformer block in this network has its corresponding memory block, the retrieved patterns from the Hopfield block are then added with the output of its respective transformer block as shown by Figure \ref{fig:architecture_diagram}. This mechanism allows the network to dynamically identify and retrieve necessary information from the memory unit and combine this information with the output of the transformer block, this addition process would fill the gaps of missing or incorrect information in the transformer output that were caused by incomplete or poorly visible patterns in the input samples.

\begin{figure*}[!ht]
\centering
\includegraphics[width=1\textwidth]{Figures/dataset_examples.png}
\caption{Sample inputs from CAMUS dataset.}
\label{fig:dataset_examples}
\end{figure*}

\begin{figure*}[!ht]
\centering
\includegraphics[width=1\textwidth]{Figures/CardiacNet_examples.png}
\caption{Sample inputs from CardiacNet dataset.}
\label{fig:cardiacnet_examples}
\end{figure*}

\section{Experimental setup}
\label{experimental}
We implemented our mechanism on multiple transformer-based image segmentation mechanisms. Details of our experimental setup are explained in their respective subsections.

\subsection{Datasets}
We used Cardiac Acquisitions for Multi-structure Ultrasound Segmentation (CAMUS) \cite{camus} and CardiacNet \cite{cardiacnet} datasets for our experimentation. The CAMUS is a public dataset designed for echocardiographic image segmentation and volume estimation from 2D ultrasound sequences. It contains 2D four-chamber and two-chamber view sequences acquired from 500 patients. The entire dataset consists of a wide variability of acquisition settings and pathological cases, reflecting the slight diversity that usually occurs in real-world clinical data. We have divided the dataset so that the training dataset contains a total of 450 patients while the testing dataset contains 50 patients. Figure \ref{fig:dataset_examples} shows examples of ultrasound samples present in the CAMUS dataset.



Whereas, the CardiacNet dataset contains two sub-datasets CardiacNet-PAH and CardiacNet-ASD. CardiacNet-PAH focuses on Pulmonary Arterial Hypertension (PAH) and CardiacNet-ASD focuses on Atrial Septal Defect (ASD). Figure \ref{fig:cardiacnet_examples} shows examples of samples in the CardiacNet dataset.



\subsection{Implementation details}
All of our experiments are implemented on an RTX 3090. We used Adam optimizer with a learning rate of $5e^{-4}$ and a cosine learning rate scheduler as shown in Table \ref{table:parameter_values}. For a fair comparison, these settings were used for all of our models in our ablation study and for the training of all of the other state-of-the-art models.


\begin{table}[h!]
\centering
\scalebox{1.8}{
\scriptsize
\begin{tabular}{|c|c|}
\hline
\textbf{Parameter}   & \textbf{Value} \\ \hline
Optimizer            & Adam \\ \hline
Learning Rate        & $5 \times 10^{-4}$ \\ \hline
LR Scheduler         & Cosine \\ \hline
Warmup LR            & $1 \times 10^{-6}$ \\ \hline
Minimum LR           & $1 \times 10^{-5}$ \\ \hline
Batch Size           & 32 \\ \hline
Warmup Epochs        & 10 \\ \hline
Total Epochs         & 60 \\ \hline
Patience Epochs      & 10 \\ \hline
\end{tabular}
}
\caption{Hyperparameter values used during the training process of the model.}
\label{table:parameter_values}
\end{table}



\begin{table*}[h!]
\centering
\resizebox{\textwidth}{!}{
\scriptsize
\begin{tabular}{|c|c|c|}
\hline
\textbf{Model} & \textbf{Structure} & \textbf{Dice score} \\ \hline
\multirow{3}{*}{Hybrid DPT} & Endocardium      & 93.36\% \\ \cline{2-3} 
                            & Epicardium       & 86.14\% \\ \cline{2-3}
                            & Left atrium wall & 86.41\% \\ \hline
\multirow{3}{*}{Our Associative Hybrid DPT (DAM-Seg)} & Endocardium      & 93.72\% \\ \cline{2-3}
                                            & Epicardium       & 88.00\% \\ \cline{2-3}
                                            & Left atrium wall & 89.1\%  \\ \hline
\multirow{3}{*}{DPT Large}  & Endocardium      & 89.98\% \\ \cline{2-3}
                            & Epicardium       & 80.68\% \\ \cline{2-3}
                            & Left atrium wall & 79.27\% \\ \hline
\multirow{3}{*}{Our Associative DPT Large (DAM-Seg)}      & Endocardium      & 90.62\% \\ \cline{2-3}
                                                & Epicardium       & 81.16\% \\ \cline{2-3}
                                                & Left atrium wall & 77.76\% \\ \hline

\end{tabular}
}
\caption{Results on our ablation study with and without our associative memory module.}
\label{table:memory_ablation}
\end{table*}



\section{Results} \label{results}
\subsection{Ablation studies}
We investigated the contributions and the effectiveness of our memory module on variations of DPT architecture. We have experimented on Hybrid DPT and DPT Large with and without our dense associative module as shown in Table \ref{table:memory_ablation}. Our associative memory module seems to affect the performance of all three classes but has the most influence over the performance of Epicardium and the left atrium wall since these are usually affected by poor quality and visibility.
In Figure \ref{fig:visual_results}, We present some examples for qualitatively comparing the segmentation quality between hybrid VIT and our proposed hybrid VIT + DAM and how the improvement of our memory module translates visually.

\begin{figure*}%[!ht]
\centering
\includegraphics[width=130mm]{Figures/ablation_compilation.png}
\caption{Qualitative comparison between the Ground Truth and segmentation results for hybrid VIT and hybrid VIT + DAM, where blue, green, and red represent the Endocardium, Epicardium, and left atrium wall.}
\label{fig:visual_results}
\end{figure*}




\subsection{Comparison with recent SOTA methods}
This section discusses the comparison between our proposed mechanism against other state-of-the-art architectures. These comparative results also show a similar trend in performance improvements as in the ablation study. Since Endocardium is least affected by poor image quality and rarely has any visibility issues the performance on Endocardium almost stays the same for each architecture as shown in Table \ref{table:sota_comparison}. Epicardium and the Left atrium wall show the largest performance improvement. We also performed quantitative comparative analysis on the CardiacNet dataset, the results in Table \ref{table:sota_comparison_card} show similar performance improvements, implying that our methodology is robust against varying scenarios.

\begin{table*}[h!]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{|c|c|c|}
\hline
\textbf{Model} & \textbf{Structure} & \textbf{Dice score} \\ \hline
\multirow{3}{*}{UNet\cite{unet}}& Endocardium      & 93.69\% \\ %\cline{2-3} 
                                & Epicardium       & 86.25\% \\ %\cline{2-3}
                                & Left atrium wall & 85.34\% \\ \hline
\multirow{3}{*}{nnUNet\cite{nnUnet}}& Endocardium      & 93.33\% \\ %\cline{2-3}
                                    & Epicardium       & 87.06\% \\ %\cline{2-3}
                                    & Left atrium wall & 85.13\%  \\ \hline
\multirow{3}{*}{CANet\cite{CANet}}& Endocardium      & 93.91\% \\ %\cline{2-3}
                                  & Epicardium       & 87.52\% \\ %\cline{2-3}
                                  & Left atrium wall & 87.01\% \\ \hline
\multirow{3}{*}{Extended nnUNet\cite{extended_nnunet}}   & Endocardium      & 92.90\% \\ %\cline{2-3}
                                                         & Epicardium       & 85.81\% \\ %\cline{2-3}
                                                         & Left atrium wall & 86.54\% \\ \hline
\multirow{3}{*}{\textbf{Our Associative Hybrid DPT (DAM-Seg)}}  & Endocardium      & \textbf{93.72}\% \\ %\cline{2-3}
                                             & Epicardium       & \textbf{88.00}\% \\ %\cline{2-3}
                                             & Left atrium wall & \textbf{89.10}\% \\ \hline

\end{tabular}
}
\caption{Comparative results against cardiac segmentation mechanisms on CAMUS dataset.}
\label{table:sota_comparison}
\end{table*}


\begin{table*}[ht!]
\centering
\scriptsize
\resizebox{\textwidth}{!}{
\begin{tabular}{|c|c|c|}
\hline
\textbf{Model} & \textbf{Structure} & \textbf{Dice score} \\ \hline
\multirow{4}{*}{UNet\cite{unet}}& Left Atrium     & 88.61\% \\ %\cline{2-3} 
                                & Right Atrium    & 87.98\% \\ %\cline{2-3}
                                & Left Ventricle  & 91.51\% \\ %\cline{2-3}
                                & Right Ventricle & 88.19\% \\ \hline
\multirow{4}{*}{nnUNet\cite{nnUnet}}  & Left Atrium     & 88.66\% \\ %\cline{2-3}
                                      & Right Atrium    & 88.47\% \\ %\cline{2-3}
                                      & Left Ventricle  & 91.40\%  \\ %\cline{2-3}
                                      & Right Ventricle & 88.50\%  \\ \hline
\multirow{4}{*}{CANet\cite{CANet}}  & Left Atrium     & 88.95\% \\ %\cline{2-3}
                                    & Right Atrium    & 88.91\% \\ %\cline{2-3}
                                    & Left Ventricle  & 91.70\% \\ %\cline{2-3}
                                    & Right Ventricle & 89.26\% \\ \hline
\multirow{4}{*}{Extended nnUNet\cite{extended_nnunet}}  & Left Atrium     & 88.21\% \\ %\cline{2-3}
                                                        & Right Atrium    & 88.23\% \\ %\cline{2-3}
                                                        & Left Ventricle  & 91.03\% \\ %\cline{2-3}
                                                        & Right Ventricle & 88.28\% \\ \hline
\multirow{4}{*}{\textbf{DAM-Seg}}  & Left Atrium     & \textbf{89.52}\% \\ %\cline{2-3}
                                             & Right Atrium    & \textbf{87.68}\% \\ %\cline{2-3}
                                             & Left Ventricle  & \textbf{91.84}\% \\ %\cline{2-3}
                                             & Right Ventricle & \textbf{89.82}\% \\ \hline

\end{tabular}
}
\caption{Comparative results against cardiac segmentation mechanisms on CardiacNet dataset.}
\label{table:sota_comparison_card}
\end{table*}

% \subsection{Generalizability evaluation of the proposed method}







\begin{figure*}%[!ht]
\centering
\includegraphics[width=0.7\textwidth]{Figures/comparison_compilation.png}
\caption{Visual comparison between our methodology (DAM-Seg) and its counterparts, where blue, green, and red represent the Endocardium, Epicardium, and left atrium wall.}
\label{fig:comparative_visual_results}
\end{figure*}


\begin{figure*}%[!ht]
\centering
\scriptsize
\includegraphics[width=1\textwidth]
{Figures/Visual_results_cardiacNetdataset.png}
\caption{Segmentation results of CardiacNet dataset from nnUNet, extended nnUNet, CANet, UNet, and our proposed network (DAM-Seg) has been shown. Where red, green, and blue represent Left ventricle, right ventricle, left atrium, respectively.}
\label{fig:cardiac_dataset}
\end{figure*}


\section{Discussion}
While modern machine learning techniques have demonstrated superior performance to humans in numerous classification tasks, there is a growing concern about their lack of true understanding of the underlying structure of training data as highlighted in studies such as  \cite{properties_nn, nn_easily}. A critical gap exists between machine learning models and human cognition. Deep neural networks excel at pattern recognition within their training distribution, however, they often fail to grasp the semantic and contextual understanding that humans naturally apply. Unlike deep learning algorithms we humans tend to have a general mental idea of the structure we are trying to segment or detect. This mental image helps us to easily identify the desired object even in noisy environments. This discrepancy between humans and deep neural networks raises important questions about the robustness, reliability, and interpretability of machine learning models, especially in high-stakes applications where errors could have significant consequences. Addressing these limitations is essential to ensure the safe and effective deployment of such models in critical domains.

The issue of anatomically incorrect segmentation has been extensively reported and studied by multiple papers \cite{adiga2024anatomically,van2024towards,wyburd2024anatomically,gao2023anatomy,von2023anatomically}, this problem often stems from the neural network's inability to learn and store a mental image of the object of interest. Research efforts, such as those in \cite{VAE_wrapping, GCN} have proposed methods to bypass the need for the network to learn a mental image of patterns by proposing other ways of maintaining anatomical accuracy.
The ability of dense associative memories to store and retrieve complex patterns efficiently aligns closely with human cognitive processes, particularly in the context of visual perception and segmentation tasks. Inspired by this, we propose a dense associative memory module enabling neural networks to have a "mental image" of objects they are trying to segment, this capability helps the network to preserve anatomical structure in its segmentation results, even on images with poor visibility.








\subsection{Visual analysis}

To further expand the comparison, we also visualize the results of our model i.e., DAM-seg, against its other counterparts. Figure \ref{fig:comparative_visual_results} and Figure \ref{fig:cardiac_dataset} show visual results on CAMUS and CardiacNet datasets, respectively, where we compare the segmentation results with corresponding original images, ground truths, nnUNet, extended nnUNet, CANet, UNet, and our proposed DAM-segNet. Figure \ref{fig:comparative_visual_results} shows that our model can perform well even for instances where the pattern is not clearly visible while other models tend to struggle to establish accurate boundaries, this is prevalent mainly for the left atrium wall. The CardiacNet dataset does not suffer from poor patterns in input images as much as the CAMUS dataset does, therefore, the results seem similar for all models in Figure \ref{fig:cardiac_dataset} except few minor details. Thus, we can conclude that our proposed model is effective for cardiac
segmentation tasks. It can also be noted that the performance of the proposed scheme remains consistent for the left ventricle, right ventricle, and left atrium.














% \subsection{Performance of overall framework}

\subsection{Limitation and future works}
While our mechanism has improved our segmentation results, it comes with multiple drawbacks.
Firstly, the reliance on the fixed memory tensor may limit the model's ability to adapt to diverse or evolving input distributions, potentially reducing its generalization capabilities. The static nature of the keys and values might lead to a less dynamic representation of the input space, possibly resulting in reduced expressiveness for context-dependent tasks. It could still be very effective for tasks where prior knowledge is crucial, but less so for tasks requiring high input-specific adaptability.
In the future we may address these issues by implementing a forgetting mechanism similar to the one described in the Memformer model \cite{memformer}. This would help in managing the stored information over time, preventing it from becoming stale or overly dominant. A Memory Retention Valve (MRV) mentioned in the RATE model \cite{rate} to control how much of the old information in A is retained versus how much new information is incorporated can also be a potential alternative.


\section{Conclusions} \label{con}
% This paper introduces a novel approach to static memory in attention mechanisms, where a fixed memory tensor is used to generate keys and values through learned projections, while queries remain input-dependent. Our results have shown that integrating this type of memory module in a transformer-based segmentation model has significantly improved performance. The stored information in the memory is used to assist the model in generating anatomically correct segmentation masks. This ability can be helpful when dealing with inputs with partially improperly visible structures.
This paper presents a novel approach to enhancing static memory within attention mechanisms. A fixed memory tensor is utilized to generate keys and values through learned projection matrices, while queries remain dynamically dependent on the input. Experimental results demonstrate that incorporating this memory module into a transformer-based segmentation model significantly improves performance. The stored memory effectively aids the model in producing anatomically accurate segmentation masks, even in scenarios where input structures are partially obscured or poorly visible. This capability highlights the potential of the proposed method for robust and reliable segmentation when dealing with inputs with partially improperly visible structures.



\section*{CRediT authorship contribution statement}
\textbf{Zahid Ullah:} Conceptualization, Methodology, Software, Formal analysis, Investigation, Writing - original draft, Writing - review \& editing.  \textbf{Jihie Kim:} Formal analysis, Investigation, Supervision, Project administration.  

\section*{\textbf{Declaration of Competing Interests}} The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.

\section*{Acknowledgements}
This research was supported by the MSIT(Ministry of Science and ICT), Korea, under the ITRC(Information Technology Research Center) support program(IITP-2025-RS-2020-II201789), and the Artificial Intelligence Convergence Innovation Human Resources Development(IITP-2025-RS-2023-00254592) supervised by the IITP(Institute for Information \& Communications Technology Planning \& Evaluation).



% \section{Related Work}
% \label{rw_section}
% \updatedText{The application of deep learning techniques for brain age estimation using MRI has seen significant advancements in recent years. In particular, Convolutional Neural Networks (CNNs) have emerged as the primary architecture for estimating brain age from structural MRI (sMRI) data, offering the ability to learn complex image features directly from the data. Early works in this domain employed 2D CNNs applied to individual slices of sMRI scans \cite{feng2020estimating}, while more recent studies have adopted 3D CNNs, which analyze the entire brain to capture more comprehensive structural features for brain age prediction \cite{he2021multi}. Dinsdale et al. \cite{dinsdale2021learning} introduced a 3D CNN model that leveraged whole-brain MRI data, demonstrating improved accuracy by capturing global structural features related to brain aging.}

% \updatedText{However, training deep learning models on small datasets remains a significant challenge. To address this, Peng et al. \cite{peng2021accurate} proposed a 3D fully convolutional network that reduces model complexity by using fewer parameters, thereby improving efficiency without compromising prediction accuracy. Cheng et al. \cite{cheng2021brain} further improved performance by introducing a two-stage age prediction network (TSAN), which first provides a rough estimation followed by a refinement stage. This multi-stage approach allows the model to progressively enhance its representations of brain aging.}

% \updatedText{In addition to CNN-based architectures, the incorporation of attention mechanisms has shown promise in improving brain age prediction. He et al. \cite{he2021multi} introduced FiA-Net, a fusion-with-attention model that integrates both intensity and RAVENS channels from sMRI data. The attention mechanism enabled the model to focus on the most informative regions, thereby improving its predictive performance. Later, He et al. \cite{he2022global} proposed a global-local transformer model, which combines global contextual information with fine-grained local features extracted from patches of sMRI data. This approach employs self-attention mechanisms to enhance the representation of brain aging patterns by dynamically adjusting attention to both global and local aspects of the data.}

% \updatedText{Beyond CNNs and attention-based methods, graph neural networks (GNNs) have also been explored for brain age estimation, primarily due to their ability to model the brain as a network of regions of interest (ROIs). Pina et al. \cite{pina2022structural} proposed a GNN-based model that represents the brain as a graph, where ROIs are treated as nodes, and the dependencies between regions are modeled as edges. This structural representation allows for the prediction of brain age by capturing the interdependencies between different brain regions. Similarly, Xu et al. \cite{xu2021brain} used a graph-based model on diffusion tensor imaging (DTI) data to study the structural connectivity of brain aging, showing the potential of graph models in capturing complex relationships between brain regions.}

% \updatedText{The integration of multimodal MRI data has emerged as a promising approach to enhance the accuracy of brain age estimation. Structural MRI (sMRI) and functional MRI (fMRI) provide complementary information, where sMRI captures structural features, and fMRI offers insights into functional connectivity. Early studies, such as those by Irimia et al. \cite{irimia2015statistical}, combined cortical thickness from sMRI with structural connectivity features to predict brain age using multivariate regression. Liem et al. \cite{liem2017predicting} proposed a stacked multimodal approach that integrates cortical anatomy from sMRI with functional connectivity derived from resting-state fMRI, demonstrating improved brain age prediction performance. Cherubini et al. \cite{cherubini2016importance} further investigated the fusion of T1-weighted MRI, T2-relaxometry, and fMRI using a voxel-based multiple regression model, highlighting the potential of multimodal data in capturing brain aging patterns. }

% \updatedText{Building upon these foundational approaches, Cole et al. \cite{cole2020multimodality} developed a more comprehensive multimodal brain age model by integrating various MRI modalities, including T1-weighted MRI, T2-FLAIR, T2*, and diffusion MRI, along with resting-state fMRI. Although this approach demonstrated promising results, it relied on hand-crafted features and complex preprocessing steps, which limited its adaptability to new datasets. More recently, Mouches et al. \cite{mouches2022multimodal} proposed a fusion model combining sMRI with time-of-flight magnetic resonance angiography (TOF MRA) data using a simple fully convolutional network (SFCN) alongside linear regression (LR), thus simplifying the integration process while maintaining robust performance.}

% Though recent advancements, traditional multimodal approaches often rely on simply concatenating features from different modalities without explicitly modeling their interactions. This limitation can overlook the potential synergies between sMRI and fMRI data that could enhance prediction accuracy. To address this, He et al. \cite{he2022global} proposed a global-local transformer model that combines global and local features through an attention mechanism, significantly improving performance. Similarly, Armanious et al. \cite{armanious2021age} incorporated both chronological and biological age information into CNN-based models, enhancing brain age prediction. Liu et al. \cite{liu2023risk} further highlighted the importance of demographic factors, such as gender, in brain age prediction, using a support vector regression (SVR) approach. Dular et al. \cite{dular2024base} extended brain age estimation to multisite data, achieving an MAE of 3.25 ± 2.70 years, demonstrating the value of large-scale, heterogeneous datasets in improving prediction accuracy. Additionally, the concept of disentangled representation learning has gained attention, with Cai et al. \cite{cai2023graph} employing a two-stream convolutional autoencoder to disentangle modality-specific features, improving upon traditional autoencoder designs. A concise summary of the existing literature is provided in Table \ref{litrature_tab}, highlighting key methodologies such as modality integration, sex-aware modeling, adversarial learning, and disentangled autoencoders. Studies utilizing attention-based and transformer networks for improved segmentation, particularly in challenging modalities, have shown promise in advancing model-based approaches \cite{farooq2023residual, kanwal2023mask, usman2024intelligent,usman2023mesaha, usman2024meds, rehman2023selective, iqbal2023ldmres, usman2023deha, usman2022dual, ullah2023ssmd, latif2018automating, lee2021evaluation, latif2018mobile, ullah2023mtss, ullah2023densely, usman2017using, ullah2022cascade, usman2020retrospective, usman2020volumetric, latif2018cross, latif2018phonocardiographic, latif2020leveraging,farooq2024lssf,naveed2024ad,iqbal2025tbconvl,iqbal2024tesl, usman2019motion,usman2022meds} \cite{naveed2024ra}. Autoencoders (AE) have been widely explored for multimodal fusion, with early and late fusion \cite{bib_20, farooq2023dc}. However, traditional AEs often struggle to differentiate between shared and complementary information, and noisy modalities can negatively impact latent representation learning across modalities.

% \begin{figure*}[t]
% \centering
% \includegraphics[width=1\textwidth]{images/SA_AVAE.png}
% \caption{\updatedText{Architecture of the proposed Multimodal Sex-Aware Adversarial Variational Autoencoder (SA-AVAE) for predicting biological brain age, utilizing sMRI as a compulsory modality and fMRI as an optional input to enhance prediction performance.}}
% \label{proposed_architecture}
% \end{figure*}

% Despite recent advancements, current approaches to brain age estimation still face significant challenges, particularly in fully exploiting the interactions between multimodal data and accounting for sex-specific aging patterns \cite{usman2024advancing}. To address these issues, the proposed framework in this paper integrates sMRI and fMRI data while incorporating sex-specific information into the latent space, capturing both modality-specific and shared features through disentangled representations. Our framework, the Sex-Aware Adversarial Variational Autoencoder (SA-AVAE), not only improves predictive accuracy but also enhances model interpretability by explicitly considering demographic factors such as sex. The key contributions of our work include: 1) the introduction of a novel framework that integrates sMRI and fMRI data with sex information to improve accuracy and robustness; 2) the use of disentangled latent representations, achieved by applying adversarial and variational autoencoder losses to ensure effective separation of modality-specific and shared features; 3) a new loss weighting strategy for fine-tuning model parameters, providing insights into optimizing architectures for broader applications; and 4) extensive evaluation showing that our framework outperforms state-of-the-art methods, establishing a new benchmark for brain age estimation. This work paves the way for more effective, interpretable models in neuroimaging and brain health assessment, emphasizing the importance of integrating multimodal data, disentangled representations, and demographic factors to improve both prediction accuracy and clinical applicability.

% \section{Proposed Methodology}
% \label{pro}

% We propose a multimodal framework, the Sex-Aware Adversarial Variational Autoencoder (SA-AVAE), which integrates adversarial and variational learning to disentangle features derived from sMRI and fMRI scans. These features are further combined with sex information to estimate biological brain age. While sMRI is more readily available, the framework is designed to function seamlessly even without the incorporation of fMRI, relying solely on sMRI when necessary. The components of the proposed framework are described in detail in the following subsections.

% \subsection{\updatedText{Overall Framework}}

% \updatedText{The architecture of the SA-AVAE framework is depicted in Figure \ref{proposed_architecture}. It consists of two autoencoder networks: the primary-modality path and the additional-modality path. Both networks share an identical architecture and are designed to process and integrate features extracted from sMRI (the primary modality) and fMRI (the optional modality).}

% \updatedText{For each modality, a multi-layer perceptron (MLP) neural network serves as the encoder $E_i$, where $i = 1$ corresponds to sMRI and $i = 2$ corresponds to fMRI. The encoder generates a latent representation $z_i$ from the input feature vector $x_i$:}
% \begin{equation}
% z_i = E_i(x_i), \quad i \in \{1, 2\}.
% \end{equation}
% \updatedText{The latent representation $z_i$ is disentangled into two components:}
% \begin{equation}
% z_i = \left[\text{Shared}(z_i), \text{Dist}(z_i)\right],
% \end{equation}
% \updatedText{where $\text{Shared}(z_i)$ captures the common, modality-invariant information, and $\text{Dist}(z_i)$ encodes modality-specific features. The disentanglement is guided by the following principles:}
% \begin{itemize}
%     \item \updatedText{The concatenation of $\text{Shared}(z_i)$ and $\text{Dist}(z_i)$ reconstructs the original latent vector $z_i$:}
%     \begin{equation}
%     z_i = \text{Concat}\left(\text{Shared}(z_i), \text{Dist}(z_i)\right).
%     \end{equation}
%     \item \updatedText{Shared representations $\text{Shared}(z_1)$ and $\text{Shared}(z_2)$ should be as similar as possible to maximize commonality:}
%     \begin{equation}
%     \left\|\text{Shared}(z_1) - \text{Shared}(z_2)\right\|_2 \to 0.
%     \end{equation}
%     \item \updatedText{Distinct representations $\text{Dist}(z_1)$ and $\text{Dist}(z_2)$ should be as dissimilar as possible to emphasize modality-specific information:}
%     \begin{equation}
%     \left\|\text{Dist}(z_1) - \text{Dist}(z_2)\right\|_2 \to \max.
%     \end{equation}
% \end{itemize}

% \updatedText{The disentangled latent features are fed into modality-specific decoders $Dec_i$ to reconstruct the input features. Finally, the concatenated latent spaces from both autoencoders, along with sex information, are passed into a regressor network $\mathbf{P}$ to estimate biological brain age:}
% \begin{equation}
% \mathcal{M} = \left[\text{Shared}(z_1), \text{Shared}(z_2), \text{Dist}(z_1), \text{Dist}(z_2), \text{Sex}\right].
% \end{equation}

% \subsection{\updatedText{Feature Disentanglement Strategy}}

% \updatedText{The proposed SA-AVAE framework introduces a robust feature disentanglement strategy, leveraging adversarial learning, variational constraints, shared-distinct distance ratio loss, and cross-modality reconstruction to effectively separate shared and modality-specific latent representations. This comprehensive strategy ensures that the model captures complementary and modality-invariant features while maintaining the unique characteristics of each modality, thus optimizing its performance for multimodal brain age estimation.}

% \subsubsection{\updatedText{Adversarial Learning for Shared Representations}}

% \updatedText{Adversarial learning plays a pivotal role in aligning the shared latent representations $\text{Shared}(z_1)$ and $\text{Shared}(z_2)$ with a predefined prior distribution $p(\mathbf{z})$. This alignment ensures that the shared features are modality-invariant and contain common information across sMRI and fMRI data. A discriminator network $D_z$ is trained to distinguish between samples from the aggregated posterior distribution $q(\mathbf{z})$ and the prior $p(\mathbf{z})$. The adversarial loss for each modality is defined as:}
% \begin{equation}
% \updatedText{\mathcal{L}_{\text{adv}}^i = \mathbb{E}_{\mathbf{x}_i \sim p_d(\mathbf{x}_i)} \log \left(1 - D_z\left(\text{Shared}(E_i(\mathbf{x}_i))\right)\right) + \mathbb{E}_{\mathbf{z}_i \sim p(\mathbf{z})} \log \left(D_z(\mathbf{z}_i)\right),}
% \end{equation}
% \updatedText{where $i \in \{1, 2\}$ represents the modality index. The total adversarial loss is the sum of the losses across both modalities:}
% \begin{equation}
% \updatedText{\mathcal{L}_{\text{adv}} = \mathcal{L}_{\text{adv}}^1 + \mathcal{L}_{\text{adv}}^2.}
% \end{equation}

% \updatedText{This mechanism ensures that the shared representations are well-regularized and contribute to the effective disentanglement of modality-specific and shared features.}

% \subsubsection{\updatedText{Variational Learning for Distinct Representations}}

% \updatedText{To regularize the modality-specific latent spaces, variational learning enforces the distinct representations $\text{Dist}(z_1)$ and $\text{Dist}(z_2)$ to conform to predefined modality-specific prior distributions $p_1(\mathbf{z})$ and $p_2(\mathbf{z})$. This constraint ensures that the distinct latent spaces capture unique, modality-specific information. The variational loss for each modality is expressed as the Kullback-Leibler (KL) divergence:}
% \begin{equation}
% \updatedText{\mathcal{L}_{\text{var}}^i = \mathcal{D}_{\text{KL}}\left(q\left(\text{Dist}(z_i) | \mathbf{x}_i\right) \| p_i\left(\text{Dist}(z_i)\right)\right),}
% \end{equation}
% \updatedText{where $p_i\left(\text{Dist}(z_i)\right)$ is typically modeled as a Gaussian distribution $\mathcal{N}(\mathbf{0}, \mathbf{I})$. The total variational loss is defined as:}
% \begin{equation}
% \updatedText{\mathcal{L}_{\text{var}} = \mathcal{L}_{\text{var}}^1 + \mathcal{L}_{\text{var}}^2.}
% \end{equation}

% \updatedText{This variational constraint encourages the distinct representations to remain disentangled from the shared latent space, thereby improving the clarity of modality-specific features.}

% \subsubsection{\updatedText{Shared-Distinct Distance Ratio Loss}}

% \updatedText{To reinforce the separation between shared and distinct representations, the model employs a shared-distinct distance ratio loss, $\mathcal{L}_{\mathcal{D}}$. This loss emphasizes the disentanglement by balancing the similarity of shared representations and the dissimilarity of distinct representations. The loss is defined as:}
% \begin{equation}
% \mathcal{L}_{\mathcal{D}} = \frac{\mathcal{L}_{\mathcal{D}}^{\text{Shared}}}{\mathcal{L}_{\mathcal{D}}^{\text{Dist}}},
% \end{equation}
% where,
% \begin{equation}
% \mathcal{L}_{\mathcal{D}}^{\text{Shared}} = \mathbb{E}_{\mathbf{x}_1, \mathbf{x}_2}\left\|\text{Shared}\left(\mathbf{Enc}_1\left(\mathbf{x}_1\right)\right) - \text{Shared}\left(\mathbf{Enc}_2\left(\mathbf{x}_2\right)\right)\right\|_2,
% \end{equation}
% and
% \begin{equation}
% \mathcal{L}_{\mathcal{D}}^{\text{Dist}} = \mathbb{E}_{\mathbf{x}_1, \mathbf{x}_2}\left\|\text{Dist}\left(\mathbf{Enc}_1\left(\mathbf{x}_1\right)\right) - \text{Dist}\left(\mathbf{Enc}_2\left(\mathbf{x}_2\right)\right)\right\|_2.
% \end{equation}

% \updatedText{This ratio loss ensures a clear separation between shared and modality-specific features, enabling the model to disentangle complex multimodal information effectively.}

% \subsubsection{\updatedText{Cross-Modality Reconstruction}}

% \updatedText{To further encourage disentanglement, the framework incorporates a cross-modality reconstruction mechanism. This criterion leverages the shared representation from one modality to reconstruct the input of the other modality, ensuring that the shared features truly represent common information. Specifically:}
% \begin{equation}
% \updatedText{x_i \approx Dec_i\left(\text{Shared}(z_j), \text{Dist}(z_i)\right), \quad i \neq j.}
% \end{equation}

% \updatedText{The reconstruction loss for modality $i$ is defined as:}
% \begin{equation}
% \updatedText{\mathcal{L}_{\text{rec}}^i = \mathbb{E}_{\mathbf{x}_i \sim p_d(\mathbf{x}_i)} \left\|\mathbf{x}_i - Dec_i\left(\text{Shared}(E_j(\mathbf{x}_j)), \text{Dist}(E_i(\mathbf{x}_i))\right)\right\|_2^2.}
% \end{equation}

% \updatedText{The total reconstruction loss is the sum of the reconstruction losses across both modalities:}
% \begin{equation}
% \updatedText{\mathcal{L}_{\text{rec}} = \mathcal{L}_{\text{rec}}^1 + \mathcal{L}_{\text{rec}}^2.}
% \end{equation}

% \subsubsection{\updatedText{Comprehensive Objective Function}}
% \updatedText{The complete objective function integrates all the discussed losses, balancing them with empirically determined trade-off parameters:}
% \begin{equation}
% \updatedText{\mathcal{L}_{\text{total}} = \mu_1 \mathcal{L}_{\text{adv}} + \mu_2 \mathcal{L}_{\text{var}} + \mu_3 \mathcal{L}_{\text{rec}} + \mu_4 \mathcal{L}_{\text{reg}} + \mu_5 \mathcal{L}_{\mathcal{D}},}
% \end{equation}
% \updatedText{where $\mu_1, \mu_2, \mu_3, \mu_4$, and $\mu_5$ are weighting coefficients. The regression loss $\mathcal{L}_{\text{reg}}$ measures the discrepancy between the predicted and actual brain age using the L2 norm:}
% \begin{equation}
% \updatedText{\mathcal{L}_{\text{reg}} = \mathbb{E}_{\mathbf{x}_1, \mathbf{x}_2}\left\|y - \mathbf{P}\left(M\left(\mathbf{x}_1, \mathbf{x}_2\right)\right)\right\|_2.}
% \end{equation}

% \updatedText{This comprehensive approach ensures robust disentanglement, accurate reconstruction, and precise brain age estimation, making the SA-AVAE framework highly effective for multimodal neuroimaging analysis.}

% \subsection{\updatedText{Single Modality Algorithm}}
% \label{unimodal_algo}
% \updatedText{In practical scenarios, the availability of datasets containing both sMRI and fMRI modalities is often limited, with most samples including only sMRI data. To address this imbalance, the SA-AVAE framework is adapted to function effectively in a single-modality setting, particularly for sMRI. This adaptation involves deactivating the fMRI encoder-decoder path and modifying the overall objective function to optimize performance with only sMRI input.}

% \subsubsection{\updatedText{Adaptation of the Objective Function}}

% \updatedText{To accommodate the absence of the fMRI modality, the objective function is redefined. Adversarial and variational losses are applied exclusively to the sMRI path, while the regression and reconstruction losses are adjusted to rely solely on sMRI-derived representations. The revised losses are as follows:}

% \begin{itemize}
%     \item \updatedText{\textbf{Adversarial Loss:} Ensures alignment of shared latent representations with a prior distribution:}
%     \[
%     \updatedText{\mathcal{L}^{'}_{\text{adv}} = \mathcal{L}_{\text{adv}}^1.}
%     \]
%     \item \updatedText{\textbf{Variational Loss:} Regularizes the distinct latent space to conform to a predefined prior:}
%     \[
%     \updatedText{\mathcal{L}^{'}_{\text{var}} = \mathcal{L}_{\text{var}}^1.}
%     \]
%     \item \updatedText{\textbf{Regression Loss:} Predicts biological brain age using sMRI-derived latent representations:}
%     \[
%     \updatedText{\mathcal{L}^{'}_{\text{Reg}} = \mathbb{E}_{\mathbf{x}_1}\left\|y - \mathbf{P}\left(\mathcal{M}\left(\mathbf{x}_1\right)\right)\right\|_2^2,}
%     \]
%     \updatedText{where $\mathcal{M}(\mathbf{x}_1)$ includes the shared and distinct representations along with sex information.}
%     \item \updatedText{\textbf{Reconstruction Loss:} Ensures accurate reconstruction of the sMRI input:}
%     \[
%     \updatedText{\mathcal{L}^{'}_{\text{Rec}} = \mathbb{E}_{\mathbf{x}_1}\left\|\mathbf{x}_1 - \mathbf{Dec}_1\left(\text{Shared}\left(\mathbf{Enc}_1\left(\mathbf{x}_1\right)\right), \text{Dist}\left(\mathbf{Enc}_1\left(\mathbf{x}_1\right)\right)\right)\right\|_2^2.}
%     \]
% \end{itemize}

% \updatedText{The modified objective function is:}
% \begin{equation}
% \updatedText{\mathcal{L}^{'}_{\mathbf{Enc}_1, \mathbf{Dec}_1, \mathbf{P}} = \eta_1 \mathcal{L}^{'}_{\text{Reg}} + \eta_2 \mathcal{L}^{'}_{\text{Rec}} + \eta_3 \mathcal{L}^{'}_{\text{adv}} + \eta_4 \mathcal{L}^{'}_{\text{var}},}
% \label{single_modality_objective}
% \end{equation}
% \updatedText{where $\eta_1, \eta_2, \eta_3$, and $\eta_4$ are trade-off parameters controlling the contribution of each loss term.}

% \subsubsection{\updatedText{Training Strategy for Uni-modal Setting}}

% \updatedText{The single-modality training procedure for the SA-AVAE framework has been adapted to ensure optimal performance using only sMRI data, addressing the challenges posed by limited multimodal datasets. During training, the encoder $Enc_1$ processes the sMRI input $\mathbf{x}_1$ to generate latent representations that are disentangled into shared and distinct components. These representations are defined as $\text{Shared}(z_1)$ and $\text{Dist}(z_1)$, respectively, and together they form the latent space $z_1 = \left[\text{Shared}(z_1), \text{Dist}(z_1)\right]$. The decoder $Dec_1$ then reconstructs the original input $\mathbf{x}_1$ by leveraging these latent features, ensuring that the reconstructed data closely matches the input.}

% \updatedText{To enhance the learned representations, adversarial regularization is applied to the shared latent space $\text{Shared}(z_1)$, aligning it with a predefined prior distribution via a discriminator network. Simultaneously, variational regularization is employed on the distinct latent space $\text{Dist}(z_1)$ to encourage alignment with a modality-specific prior distribution using the KL divergence. The regressor network $\mathbf{P}$ predicts the biological brain age $y$ by combining the shared and distinct features with additional sex information encoded in $\mathcal{M}(\mathbf{x}_1)$. The parameters of the encoder, decoder, regressor, and discriminator are updated jointly using the modified objective function, which balances regression, reconstruction, adversarial, and variational losses.}

% \updatedText{This approach not only ensures robust feature learning and disentanglement but also provides several benefits. First, the framework is highly adaptable, allowing it to operate seamlessly with datasets containing only sMRI data, which is crucial for real-world applications where multimodal data is often unavailable. Second, the absence of the fMRI path reduces computational overhead, resulting in faster training and inference, which enhances efficiency. Third, adversarial and variational regularizations ensure that the learned representations remain robust and meaningful, even in the single-modality scenario. Finally, the modular design of the framework ensures scalability, allowing future integration with additional modalities or extension to other single-modality tasks. These attributes make the single-modality adaptation of the SA-AVAE framework a powerful and flexible solution for brain age estimation in diverse settings.}



% \begin{figure*}[ht]
% \centering
% \includegraphics[width=1\textwidth]{images/openBHB_histogram.png}
% \caption{\updatedText{Age distribution of male and female participants in the training and validation sets of the OpenBHB dataset\cite{dufumier2022openbhb} , shown in (a) and (b), respectively.}}
% \label{data_distribution}
% \end{figure*}

% \section{Experimental Setup}
% \label{es_section}
% \subsection{Dataset and Pre-processing}

% In our experiments, we utilized the OpenBHB dataset \cite{dufumier2022openbhb}, a large-scale and comprehensive resource comprising 5330 3D brain MRI scans collected from 71 different sites. This diverse dataset includes scans from a wide range of demographics, ensuring broad genetic and geographical representation. Specifically, 3984 of the scans are publicly available, with 3227 scans allocated for training and 757 scans designated for validation. The validation set is further split into two subsets: 362 internal test samples and 390 external test samples, allowing for a comprehensive evaluation of the model's performance on both internal and unseen data. 

% The OpenBHB dataset is organized into 10 distinct sub-datasets, which include subjects from varied ethnic backgrounds, including European-American, European, and Asian descent. This diversity ensures that our model is trained and evaluated on a broad spectrum of genetic backgrounds, enhancing its robustness and generalizability across different populations. The age range of the subjects spans from 16 to 86 years, and the dataset exhibits a balanced distribution of male and female subjects across different age groups, as shown in Figure \ref{data_distribution}. This balanced sex distribution is crucial for minimizing biases and ensuring that the trained model can generalize effectively across both male and female populations.

% For the single-modality experiments with the SA-AVAE framework, we utilized the full publicly available OpenBHB dataset, which consists predominantly of structural MRI (sMRI) data. This extensive dataset enabled us to train the model effectively in single-modality settings, focusing exclusively on sMRI data. For multimodal experiments, we selected subsets of the dataset that include both sMRI and functional MRI (fMRI) data. Specifically, we incorporated two smaller datasets, referenced in \cite{dataset1} and \cite{dataset2}, which contain 66 and 315 scans, respectively. These two subsets were combined to create a unified multimodal dataset, resulting in a total of 381 scans. This approach allowed us to explore the impact of multimodal data integration on brain age estimation and assess the performance of our framework in leveraging both structural and functional brain information.

% Overall, the diversity of the OpenBHB dataset, combined with its large size and rich multimodal data, provides a solid foundation for evaluating our proposed framework. The varied demographic representation and the inclusion of both single-modality and multimodal data ensure that our experiments are not only comprehensive but also robust across different populations and imaging modalities.


% The high dimensionality of MRI scans, combined with the limited size of available datasets, poses significant computational challenges and increases the risk of overfitting when directly used in neural network training. To address these challenges, an initial feature selection step is essential. Feature selection methods are broadly categorized into three types based on their interaction with the learning model: filter methods, wrapper methods, and embedded methods \cite{urbanowicz2018relief}. We adopted a filter method, due to its independence from specific models and computational efficiency, and its proven effectiveness\cite{hu2020disentangled}.


% For feature selection, we employed the Random Forest algorithm, which is well-suited for handling high-dimensional data with highly correlated features \cite{kawakubo2012rapid}. This process results in $m_1$ and $m_2$ selected features from the first (sMRI) and second (fMRI) modalities, respectively. The dataset after feature selection can be mathematically represented as:
% \begin{equation}
%     (X_1, X_2, Y) = \{(X_{11}, X_{21}, y_1), \ldots, (X_{1n}, X_{2n}, y_n)\},
% \end{equation}
% where:
% \begin{itemize}
%     \item $X_{1n} \in \mathbb{R}^{m_1}$ represents the feature vector of the $n$-th instance derived from the sMRI modality.
%     \item $X_{2n} \in \mathbb{R}^{m_2}$ represents the feature vector of the $n$-th instance derived from the fMRI modality.
%     \item $y_n \in \mathbb{R}$ corresponds to the target outputs (e.g., age and gender) for the $n$-th instance.
%     \item $N$ denotes the total number of instances in the dataset.
% \end{itemize}

% The resulting feature vectors $X_1$ and $X_2$, along with the target outputs $Y$, form the basis for training and evaluating the proposed model.

% \subsection{Model Architecture and Training Strategy}

% The architecture of the proposed Sex-Aware Adversarial Variational Autoencoder (SA-AVAE), employed in our experiments, is depicted in Figure \ref{backbone_net}. This architecture integrates encoders and decoders for both sMRI and fMRI modalities, alongside a shared regressor and a discriminator to facilitate adversarial learning. To ensure reproducibility and optimize the model's performance, several key hyperparameters were carefully selected. First, the batch size was set to 20 during training to balance memory usage and model convergence. The latent space of the model was designed with a total dimensionality of 120, divided into a shared space of 50 dimensions and modality-specific spaces of 70 dimensions each. These dimensions were determined empirically, striking a balance between the representational capacity of the model and the computational efficiency required for large-scale experiments. For training, the Adam optimizer was used with an initial learning rate of 0.001, which was dynamically adjusted; if no improvement in validation performance was observed after nine epochs, the learning rate was reduced to a quarter of its current value to aid in convergence. Additionally, an early stopping mechanism was implemented to mitigate overfitting, halting the training process when no improvement was seen in the validation set after a predefined number of epochs. The training and evaluation of the SA-AVAE framework were conducted on an NVIDIA RTX 4090 GPU using the Keras library with a TensorFlow backend. The model had a total of 2.7 million trainable parameters. Training took approximately 12 hours to complete, while testing was conducted in about 1.5 hours, demonstrating the model’s computational efficiency, which makes it suitable for handling large-scale neuroimaging datasets.

% Several implementation strategies were critical for optimizing the performance of the SA-AVAE framework. During training, the reconstruction losses for both sMRI and fMRI modalities were balanced to ensure that each modality contributed equally to the shared latent representations, preventing bias toward one modality. The adversarial and variational losses were carefully scaled using empirically derived trade-off parameters. This scaling ensured that the shared and distinct latent spaces were effectively disentangled, allowing the model to learn complementary features from both modalities. Furthermore, the regressor was jointly trained with both the encoder and decoder networks to ensure that the latent representations were tailored for accurate age prediction. Dropout and weight decay were employed to eliminate the possibility of overfitting and improve generalization ability. These measures, in combination with robust preprocessing and careful feature selection, enabled the SA-AVAE framework to achieve competitive performance in biological brain age prediction, effectively leveraging multimodal neuroimaging data.


% \begin{figure*}[ht]
% \centering
% \includegraphics[width=0.9\textwidth]{images/arch_details_new.png}
% \caption{\updatedText{Illustration of architectural details of our proposed Sex-Aware Adversarial Variational Autoencoder.}}
% \label{backbone_net}
% \end{figure*}

% \begin{table*}[ht]
% \centering
% \caption{\updatedText{Performance comparison of different models evaluated using Mean Absolute Error (\textit{MAE}), Root Mean Square Error (\textit{RMSE}), and Coefficient of Determination (\(R^2\)) for overall, male, and female scans. \textit{The best results are underlined.}}}
% % \label{tab:performance_comparison}
% % \scalebox{0.75}{
% \begin{tabular}{@{}lccccccccc@{}}
% \toprule
% \updatedText{\textbf{Model Name}} & \multicolumn{3}{c}{\updatedText{\textbf{Overall}}} & \multicolumn{3}{c}{\updatedText{\textbf{Male}}} & \multicolumn{3}{c}{\updatedText{\textbf{Female}}} \\ 
% \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10}
%  & \updatedText{\textit{MAE} $\downarrow$} & \updatedText{\textit{RMSE} $\downarrow$} & \updatedText{\(R^2\uparrow\)} & \updatedText{\textit{MAE} $\downarrow$} & \updatedText{\textit{RMSE} $\downarrow$} & \updatedText{\(R^2\uparrow\)} & \updatedText{\textit{MAE} $\downarrow$} & \updatedText{\textit{RMSE} $\downarrow$} & \updatedText{\(R^2\uparrow\)} \\ 
% \midrule
% \updatedText{AE}            & \updatedText{3.590 $\pm$ 1.910} & \updatedText{4.067} & \updatedText{0.885} & \updatedText{3.695 $\pm$ 1.898} & \updatedText{4.154} & \updatedText{0.859} & \updatedText{3.394 $\pm$ 1.918} & \updatedText{3.899} & \updatedText{0.908} \\
% \updatedText{AAE}           & \updatedText{3.279 $\pm$ 1.882} & \updatedText{3.781} & \updatedText{0.901} & \updatedText{3.419 $\pm$ 1.863} & \updatedText{3.894} & \updatedText{0.876} & \updatedText{3.017 $\pm$ 1.889} & \updatedText{3.560} & \updatedText{0.923} \\
% \updatedText{VAE}           & \updatedText{3.142 $\pm$ 1.782} & \updatedText{3.612} & \updatedText{0.910} & \updatedText{3.212 $\pm$ 1.765} & \updatedText{3.665} & \updatedText{0.890} & \updatedText{3.011 $\pm$ 1.807} & \updatedText{3.512} & \updatedText{0.925} \\
% \updatedText{AVAE}          & \updatedText{2.921 $\pm$ 1.608} & \updatedText{3.334} & \updatedText{0.923} & \updatedText{2.979 $\pm$ 1.594} & \updatedText{3.379} & \updatedText{0.907} & \updatedText{2.813 $\pm$ 1.626} & \updatedText{3.250} & \updatedText{0.936} \\
% \updatedText{SA-AVAE}       & \updatedText{\underline{2.722 $\pm$ 1.351}} & \updatedText{\underline{3.039}} & \updatedText{\underline{0.936}} & \updatedText{\underline{2.727 $\pm$ 1.345}} & \updatedText{\underline{3.041}} & \updatedText{\underline{0.924}} & \updatedText{\underline{2.713 $\pm$ 1.363}} & \updatedText{\underline{3.036}} & \updatedText{\underline{0.944}} \\
% \bottomrule
% \end{tabular}
% % }
% \label{tab_ablation}
% \end{table*}


% \section{Results and Discussion}
% \label{res}
% \subsection{\updatedText{Ablation Study}}
% \updatedText{In this section, we perform an ablation study to evaluate the contribution of each component in the proposed framework. We implemented various downgraded versions of the multimodal model, including the Autoencoder (AE), Adversarial Auto-Encoder (AAE), Variational Auto-Encoder (VAE), Adversarial Variational Auto-Encoder (AVAE), and our proposed Sex-Aware Adversarial Variational Auto-Encoder (SA-AVAE). Table \ref{tab_ablation} summarizes the results of each model in terms of Mean Absolute Error (MAE), Root Mean Square Error (RMSE), and Coefficient of Determination (\(R^2\)) for overall, male, and female samples. The results indicate that the proposed SA-AVAE outperforms all downgraded variants. From Table \ref{tab_ablation}, it is evident that each added component contributes to the overall performance improvement achieved by SA-AVAE. Specifically, the inclusion of both adversarial and variational learning not only enhances the model's performance individually but also results in a substantial improvement when combined. Most importantly, the integration of sex information significantly boosts the performance of our proposed SA-AVAE framework.}


% \subsection{\updatedText{Comparison with State-of-the-Art Methods}}

% \updatedText{In this section, we compare the performance of our proposed framework with previously published methods. To ensure a fair comparison, we specifically shortlisted studies that utilized the same dataset, OpenBHB \cite{dufumier2022openbhb}, which is also used in this study. Table \ref{tab_SOTA_comparison} provides a summary of these studies along with their respective performances in biological brain age estimation, measured in terms of mean absolute error (MAE).}

% \updatedText{Overall, our proposed method demonstrates superior performance compared to the previously published studies. Among the methods compared, Ahmed et al. \cite{ahmed2023robust} is the only one, apart from our framework, that utilized features extracted from 3D sMRI scans. Their approach aimed to improve brain age estimation by extracting region-wise features and classifying participants into distinct age groups or clusters. These region-wise features were later integrated into regression models. This method achieved the second-lowest MAE in the comparison. Aqil et al. \cite{aqil2023confounding} employed 3D T1-weighted MRI scans combined with healthy Montreal Neurological Institute (MNI) templates as inputs to a customized encoder-decoder architecture with an additional regressor branch. Despite their attempt to enforce the network to align with brain patterns by comparing them against templates, their method achieved the highest MAE in the comparison. This was likely due to the computational overhead introduced by the 3D network and its inability to fully capture the underlying brain patterns effectively.}

% \updatedText{Similarly, Cheshmi et al. \cite{cheshmi2023brain} and Träuble et al. \cite{trauble2024contrastive} also utilized 3D inputs for brain age estimation. Cheshmi et al. \cite{cheshmi2023brain} employed a ResNet-18 model \cite{he2016deep} trained using federated learning, while Träuble et al. \cite{trauble2024contrastive} introduced a novel contrastive loss in their framework. Träuble et al.'s method dynamically adapted the contrastive loss during training to focus on localized neighborhoods of samples and incorporated brain stiffness—a mechanical property sensitive to age-related changes. This marked the first application of self-supervised learning to brain stiffness for brain age prediction. Despite their innovations, both methods lacked the integration of sex information, which negatively impacted their brain age estimation performance.}

% \updatedText{In contrast, our framework surpasses these methods by leveraging a combination of adversarial learning and variational learning. It effectively integrates features extracted from both sMRI and fMRI modalities, while incorporating sex information to account for sex-specific aging patterns. The state-of-the-art performance of our framework can be attributed to its ability to disentangle the feature space, dividing the latent space into shared and distinct features. This enables the model to better capture modality-invariant and modality-specific information, leading to more accurate predictions. These advancements demonstrate the efficiency and robustness of our framework, making it suitable for real-time applications.}


% \begin{table}[ht]
% \centering
% \caption{\updatedText{Comparison of brain age estimation performance across various methods, including the proposed SA-AVAE model. The table presents the Mean Absolute Error (MAE) for each method, along with a brief description of the algorithm/architecture used.}}
% \begin{tabular}{lll}
% \hline
% \rowcolor[HTML]{EFEFEF} 
% \multicolumn{1}{c}{\cellcolor[HTML]{EFEFEF}\textbf{Study, year}} &
%   \multicolumn{1}{c}{\cellcolor[HTML]{EFEFEF}\textbf{Algorithm/Architecture Details}} &
%   \multicolumn{1}{c}{\cellcolor[HTML]{EFEFEF}\textbf{MAE}} \\ \hline
% Aqil \textit{et al.} \cite{aqil2023confounding}, 2023   & Modified Autoencoder & 4.554    \\ \hline
% Ahmed \textit{et al.} \cite{ahmed2023robust}, 2023   & Classical Regression Algorithms & 3.250   \\ \hline
% Cheshmi \textit{et al.} \cite{cheshmi2023brain}, 2023  & 3D ResNet-18 \cite{he2016deep} & 3.860   \\ \hline
% Träuble \textit{et al.} \cite{trauble2024contrastive}, 2024 & 3D ResNet-18 + Contrastive Loss & 3.724 \\ \hline
% Our Method       & SA-AVAE & 2.722 \\ \hline
% \end{tabular}
% \label{tab_SOTA_comparison}
% \end{table}



% \begin{figure*}[t]
%     \centering
%     % Top-left image (a)
%     \begin{subfigure}[t]{0.48\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{images/AAE.png}
%         \caption{AAE}
%     \end{subfigure}
%     % Top-right image (b)
%     \begin{subfigure}[t]{0.48\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{images/VAE.png}
%         \caption{ VAE}
%     \end{subfigure}
%     % Bottom-left image (c)
%     \begin{subfigure}[t]{0.48\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{images/AVAE.png}
%         \caption{AVAE}
%     \end{subfigure}
%     % Bottom-right image (d)
%     \begin{subfigure}[t]{0.48\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{images/SA-AVAE.png}
%         \caption{\textit{Our Proposed} SA-AVAE}
%     \end{subfigure}
    
%     \caption{\updatedText{Comparison of predictions for different multi-modal models: (a) AAE, (b) VAE, (c) AVAE, (d) SA-AVAE. Each graph plots predicted brain age versus chronological brain age, with gender and confidence intervals indicated.}}
%     \label{models_comparison}
% \end{figure*}


% \subsection{Robustness Analysis}

% \updatedText{To evaluate the robustness of the SA-AVAE model, we performed 10-fold cross-validation to assess the consistency of its performance across the dataset. Figure \ref{models_comparison} shows scatter plots comparing the predicted and chronological ages of the SA-AVAE and other models, under identical experimental conditions, highlighting the relative performance and robustness of each. The results demonstrate that combining adversarial and variational learning enhances both performance and robustness. Incorporating sex information further improves the model's robustness, as evidenced by lower standard deviation. Notably, the SA-AVAE model outperforms other variants in terms of confidence intervals, underscoring the importance of sex-aware inputs. Our model’s superior performance is attributed to its ability to create a disentangled latent space using three distinct prior distributions, enforced by adversarial and variational losses.}

% To further illustrate the robustness of our proposed framework, Table \ref{results_breakdown_tab} presents a breakdown of the model's performance across various age groups. Among all models, the proposed SA-AVAE consistently outperforms others in every age group, highlighting the advantages of sex-aware disentangled learning. Ultimately, the SA-AVAE model delivers the most consistent and accurate predictions, reinforcing the efficacy of our approach.

% \begin{table}[t]
% \centering
% \caption{\updatedText{Performance of various multimodal frameworks for biological brain age estimation across four age groups: G1 (under 25 years), G2 (25 to 35 years), G3 (35 to 45 years), and G4 (45 to 55 years), measured in terms of Mean Absolute Error (MAE).}}
% \label{results_breakdown_tab}
% \begin{tabular}{@{}lcccc@{}}
% \toprule
%     \textbf{Framework} & \textbf{G1} & \textbf{G2} & \textbf{G3} & \textbf{G4} \\
%     \midrule
% AE         & 3.63 ± 1.87 & 3.51 ± 1.96 & 4.01 ± 1.96 & 2.58 ± 1.79 \\
% AAE        & 3.32 ± 1.86 & 3.20 ± 1.90 & 3.65 ± 1.97 & 2.31 ± 1.67 \\
% VAE        & 3.17 ± 1.76 & 3.10 ± 1.80 & 3.53 ± 1.85 & 2.23 ± 1.64 \\
% AVAE       & 2.94 ± 1.58 & 2.87 ± 1.63 & 3.28 ± 1.66 & 2.09 ± 1.50 \\
% SA-AVAE    & \textbf{2.74 ± 1.32} & \textbf{2.68 ± 1.41} & \textbf{3.06 ± 1.36} & \textbf{2.04 ± 1.30} \\ \bottomrule
% \end{tabular}
% \end{table}


% \subsection{\updatedText{Multi-Modality Analysis}}
% \label{multimodal_analysis_section}
% \updatedText{The impact of multimodal fusion on model performance was comprehensively assessed by comparing unimodal (sMRI or fMRI alone) and multimodal (combined sMRI and fMRI) data strategies, as shown in Figure \ref{results_bar_chart}. This evaluation clearly demonstrated that unimodal models trained exclusively on fMRI data produced the least favorable results, indicating that fMRI alone does not provide sufficient discriminative power for accurate age prediction. In contrast, models trained on sMRI data, while still unimodal, exhibited moderately improved performance, suggesting that structural MRI alone holds more valuable information for estimating brain age.}

% \begin{figure*}[t]
% \centering
% \includegraphics[width=1\textwidth]{images/multimodal_comparison_cmig.png}
% \caption{\updatedText{Mean Absolute Error (MAE) for brain age estimation obtained from unimodal regressors using sMRI, fMRI, and a multimodal regressor combining both sMRI and fMRI.}}
% \label{results_bar_chart}
% \end{figure*}

% \updatedText{More notably, the integration of both sMRI and fMRI data into a multimodal framework resulted in a significant performance boost across all metrics, highlighting the clear advantages of combining complementary information from different modalities. Specifically, the MAE for age prediction using only sMRI data ranged from 3.52 to 2.72 years, showing reasonable accuracy. However, when relying solely on fMRI, the MAE ranged between 5.12 and 3.54 years, underperforming compared to sMRI. By fusing sMRI and fMRI, the MAE improved, yielding values between 3.59 and 2.72 years, demonstrating the enhanced predictive power that arises from the synergistic combination of structural and functional brain imaging data. These results further reinforce the critical role of multimodal data fusion in enhancing model accuracy and reliability in brain age estimation. It is important to note that the simple autoencoder network in the unimodal setting (i.e., using only sMRI as input) performed better than its multimodal counterpart. This can be attributed to the limited feature learning capability of traditional autoencoders. In contrast, all multimodal models, including the proposed SA-AVAE, showed consistent improvement over their unimodal counterparts, demonstrating the value of combining sMRI and fMRI data. The results indicate that although conventional multimodal fusion techniques can sometimes amplify noise, particularly from fMRI data, leading to a drop in accuracy compared to using sMRI alone, the SA-AVAE framework adeptly overcomes this limitation. By integrating both sMRI and fMRI data, the SA-AVAE model significantly enhanced performance, achieving a reduction in the MAE from 3.12 years with sMRI alone to 2.72 years with the multimodal approach. This improvement demonstrates the model’s superior ability to harmoniously combine different data modalities, thereby enhancing predictive precision without the usual drawback of noise interference.}


% \updatedText{Additionally, we trained the proposed model in a unimodal setting (as described in Section \ref{unimodal_algo}) using all available sMRI scans from the OpenBHB dataset \cite{dufumier2022openbhb} to compare the performance with our multimodal SA-AVAE framework. Table \ref{modality_tab} summarizes the results from both models. Notably, the unimodal model was trained on 3,200 sMRI scans, whereas the multimodal framework used features extracted from just 320 sMRI and fMRI scans. Although the unimodal model was trained on ten times more data, which helped it to improve its performanc for brain age estimation (i.e., by improving MAE 3.12 to 2.91), the multimodal SA-AVAE still outperformed its unimodal counterpart. This can be attributed to the additional value provided by the fMRI features, which enhance the model’s ability to learn meaningful information crucial for understanding brain aging patterns. Thus, we conclude that the proposed multimodal framework can achieve comparable performance even with a smaller dataset.}



% \begin{table}[]
% \caption{\updatedText{Comparison of SA-AVAE performance with multimodal and unimodal data, evaluated on different metrics for overall, male, and female groups.}}
% \begin{tabular}{|cc|c|c|}
% \hline
% \rowcolor[HTML]{EFEFEF} 
% \multicolumn{2}{|c|}{\cellcolor[HTML]{EFEFEF}\textbf{Model Type}} &
%   \textbf{\begin{tabular}[c]{@{}c@{}}Multimodal SA-AVAE \\ \end{tabular}} &
%   \textbf{\begin{tabular}[c]{@{}c@{}}Unimodal SA-AVAE\\ \end{tabular}} \\ \hline
% \multicolumn{1}{|c|}{}                                   & MAE  & 2.722 ± 1.351   & 2.906 ± 1.528   \\ \cline{2-4} 
% \multicolumn{1}{|c|}{}                                   & RMSE & 3.039           & 3.283           \\ \cline{2-4} 
% \multicolumn{1}{|c|}{\multirow{-3}{*}{\textbf{Overall}}} & R2   & 0.936           & 0.925           \\ \hline
% \multicolumn{1}{|c|}{}                                   & MAE  & 2.727 ± 1.345   & 2.956 ± 1.519   \\ \cline{2-4} 
% \multicolumn{1}{|c|}{}                                   & RMSE & 3.041           & 3.324           \\ \cline{2-4} 
% \multicolumn{1}{|c|}{\multirow{-3}{*}{\textbf{Male}}}    & R2   & 0.924           & 0.91            \\ \hline
% \multicolumn{1}{|c|}{}                                   & MAE  & 2.713 ± 1.363   & 2.812 ± 1.539   \\ \cline{2-4} 
% \multicolumn{1}{|c|}{}                                   & RMSE & 3.036           & 3.206           \\ \cline{2-4} 
% \multicolumn{1}{|c|}{\multirow{-3}{*}{\textbf{Female}}}  & R2   & 0.944           & 0.938           \\ \hline
% \end{tabular}
% \label{modality_tab}
% \end{table}



% \subsection{Impact of Incorporating Sex Information}

% In this section, we analyze the impact of incorporating sex information into the brain age estimation model. Specifically, we compare the performance of the proposed adversarial variational autoencoder (AVAE) network with and without sex information. To expand our analysis, we explore two methods of incorporating sex information. The first method involves employing multitask learning, where the model is required to predict the subject's sex along with biological brain age estimation. The second method, which we adopt in the proposed SA-AVAE framework, directly inputs the sex information into the regressor.

% Table \ref{sex_info_tab} summarizes the results obtained from three different frameworks for brain age estimation, evaluating male, female, and overall groups, along with the number of trainable parameters. It can be observed that models incorporating sex information outperform the model without sex information across all groups, highlighting the significance of including sex information in the model. Furthermore, the inclusion of sex information via multitask learning significantly improves the network's performance, although the improvement is slightly more pronounced in one gender group (female or male) than the other. On the other hand, the framework with direct inclusion of sex information not only improves brain age estimation performance but also does so with fewer trainable parameters. The reduction in parameters is due to the absence of the sex classifier required for multitask learning. Most importantly, the proposed SA-AVAE framework provides balanced performance across both male and female groups, demonstrating that the framework not only enhances brain age estimation but also improves robustness.

% \begin{table}[]

% \caption{\updatedText{Comparison of models with and without sex information incorporation in the proposed adversarial variational autoencoder (AVAE) performance, along with the number of parameters, evaluated on different metrics for overall, male, and female groups.}}
% \centering
% \begin{tabular}{ccll}
% \hline
% \rowcolor[HTML]{EFEFEF} 
% \multicolumn{1}{l}{\cellcolor[HTML]{EFEFEF}\textbf{Method}} &
%   \multicolumn{1}{l}{\cellcolor[HTML]{EFEFEF}\textbf{\begin{tabular}[c]{@{}l@{}}No. of \\ Parameters\end{tabular}}} &
%   \textbf{Group} &
%   \multicolumn{1}{c}{\cellcolor[HTML]{EFEFEF}\textbf{MAE}} \\ \hline
%                           &                          & Female  & 2.813 ± 1.626 \\ \cline{3-4} 
%                           &                          & Male    & 2.979 ± 1.594 \\ \cline{3-4} 
% \multirow{-3}{*}{AVAE}    & \multirow{-3}{*}{2.70 M} & Overall & 2.921 ± 1.608 \\ \hline
%                           &                          & Female  & 2.620 ± 1.528 \\ \cline{3-4} 
%                           &                          & Male    & 2.863 ± 1.508 \\ \cline{3-4} 
% \multirow{-3}{*}{M-AVAE \cite{usman2024multi, usman2024advancing}}  & \multirow{-3}{*}{3.15 M} & Overall & 2.779 ± 1.520 \\ \hline
%                           &                          & Female  & 2.713 ± 1.363 \\ \cline{3-4} 
%                           &                          & Male    & 2.727 ± 1.345 \\ \cline{3-4} 
% \multirow{-3}{*}{SA-AVAE} & \multirow{-3}{*}{2.70 M} & Overall & 2.722 ± 1.351 \\ \hline
% \end{tabular}
% \label{sex_info_tab}
% \end{table}

% \subsection{\updatedText{Limitations and Future Work}}
% \updatedText{The proposed SA-AVAE framework leverages two neuroimaging modalities, structural MRI (sMRI) and functional MRI (fMRI), for the accurate estimation of biological brain age. While the integration of both modalities enhances the model's performance and improves the accuracy of brain age estimation, it also introduces certain limitations. One of the primary challenges is the framework's sensitivity to missing modalities. As discussed in Section \ref{multimodal_analysis_section}, the performance of the SA-AVAE model significantly deteriorates when either modality is unavailable. This makes the model highly dependent on the availability of both sMRI and fMRI data, which can be a significant limitation in practical clinical scenarios where one of the modalities may be missing or of poor quality. Another major limitation of this study is the exclusive use of data from healthy control (HC) subjects. While the results obtained on this cohort are promising, the real-world applicability of the framework for early detection of neurodegenerative diseases, such as Alzheimer's Disease (AD), remains uncertain. To more effectively evaluate the performance and generalizability of the proposed framework, it is crucial to test it on patients with various neurological conditions, including AD and Parkinson's Disease (PD). This will allow for a more thorough assessment of how the model performs in distinguishing biological brain age across different patient populations, which is essential for its potential clinical use.}

% \updatedText{Future work includes several key improvements that are necessary for the framework to achieve its full potential. One such area is enhancing the model's robustness so that it can perform comparably well with either sMRI or fMRI data alone, thereby reducing the performance gap between multimodal and unimodal approaches. This would increase the model’s flexibility, enabling its deployment in situations where only a single modality is available or when multimodal data collection is not feasible. Additionally, we plan to conduct a comprehensive evaluation of the SA-AVAE framework using clinical data from patients with various neurological conditions. This will involve real-time testing in clinical settings to assess the feasibility of using the framework for the early diagnosis and monitoring of brain aging and neurodegenerative diseases. These efforts will be critical in demonstrating the clinical utility and robustness of the proposed framework in real-world applications.}



% \section{Conclusion}
% \label{con}
% In this study, we introduced a novel framework for biological brain age estimation, leveraging the complementary information from structural magnetic resonance imaging (sMRI) and functional magnetic resonance imaging (fMRI) data. Our proposed Sex-Aware Adversarial Variational Autoencoder (SA-AVAE) combines adversarial and variational learning techniques to effectively disentangle latent features from both modalities. By decomposing the latent space into modality-specific and shared codes, our model captures both the unique and common aspects of brain aging, while also accounting for sex-specific aging patterns, further enhancing its performance. The results of our experiments, evaluated on the OpenBHB dataset, demonstrate that SA-AVAE outperforms existing state-of-the-art methods, showing significant robustness across various age groups. This highlights the potential of the framework for real-time clinical applications, particularly in the early detection and monitoring of neurodegenerative diseases. Future work will focus on enhancing the robustness of the framework to ensure effective performance with either modality independently. Additionally, we aim to conduct real-time evaluations using clinical data to assess the practical applicability of the SA-AVAE model in clinical settings.

% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{RV_segmentation}
C.~Petitjean, M.~A. Zuluaga, W.~Bai, J.-N. Dacher, D.~Grosgeorge, J.~Caudron, S.~Ruan, I.~B. Ayed, M.~J. Cardoso, H.-C. Chen \emph{et~al.}, ``Right ventricle segmentation from cardiac mri: a collation study,'' \emph{Medical image analysis}, vol.~19, no.~1, pp. 187--202, 2015.

\bibitem{cardiac_fcn}
\BIBentryALTinterwordspacing
P.~V. Tran, ``A fully convolutional neural network for cardiac segmentation in short-axis mri,'' 2017. [Online]. Available: \url{https://arxiv.org/abs/1604.00494}
\BIBentrySTDinterwordspacing

\bibitem{fcn}
\BIBentryALTinterwordspacing
J.~Long, E.~Shelhamer, and T.~Darrell, ``Fully convolutional networks for semantic segmentation,'' 2015. [Online]. Available: \url{https://arxiv.org/abs/1411.4038}
\BIBentrySTDinterwordspacing

\bibitem{displacement_aware}
K.~Li, Y.~Zhu, L.~Yu, and P.-A. Heng, ``A dual enrichment synergistic strategy to handle data heterogeneity for domain incremental cardiac segmentation,'' \emph{IEEE Transactions on Medical Imaging}, vol.~43, no.~6, pp. 2279--2290, 2024.

\bibitem{cross_domain}
Z.~Cai, J.~Xin, S.~Dong, J.~A. Onofrey, N.~Zheng, and J.~S. Duncan, ``Symmetric consistency with cross-domain mixup for cross-modality cardiac segmentation,'' in \emph{ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 2024, pp. 1536--1540.

\bibitem{VAE_wrapping}
N.~Painchaud, Y.~Skandarani, T.~Judge, O.~Bernard, A.~Lalande, and P.-M. Jodoin, ``Cardiac segmentation with strong anatomical guarantees,'' \emph{IEEE Transactions on Medical Imaging}, vol.~39, no.~11, pp. 3703--3713, 2020.

\bibitem{GCN}
G.~V.~D. Vyver, S.~Thomas, G.~Ben-Yosef, S.~H. Olaisen, H.~Dalen, L.~Løvstakken, and E.~Smistad, ``Toward robust cardiac segmentation using graph convolutional networks,'' \emph{IEEE Access}, vol.~12, pp. 33\,876--33\,888, 2024.

\bibitem{SA_update}
\BIBentryALTinterwordspacing
H.~Ramsauer, B.~Schäfl, J.~Lehner, P.~Seidl, M.~Widrich, T.~Adler, L.~Gruber, M.~Holzleitner, M.~Pavlović, G.~K. Sandve, V.~Greiff, D.~Kreil, M.~Kopp, G.~Klambauer, J.~Brandstetter, and S.~Hochreiter, ``Hopfield networks is all you need,'' 2021. [Online]. Available: \url{https://arxiv.org/abs/2008.02217}
\BIBentrySTDinterwordspacing

\bibitem{dpt}
\BIBentryALTinterwordspacing
R.~Ranftl, A.~Bochkovskiy, and V.~Koltun, ``Vision transformers for dense prediction,'' 2021. [Online]. Available: \url{https://arxiv.org/abs/2103.13413}
\BIBentrySTDinterwordspacing

\bibitem{hopfield_orig}
J.~J. Hopfield, ``Neural networks and physical systems with emergent collective computational abilities.'' \emph{Proceedings of the national academy of sciences}, vol.~79, no.~8, pp. 2554--2558, 1982.

\bibitem{polynomial_DAM}
\BIBentryALTinterwordspacing
D.~Krotov and J.~J. Hopfield, ``Dense associative memory for pattern recognition,'' 2016. [Online]. Available: \url{https://arxiv.org/abs/1606.01164}
\BIBentrySTDinterwordspacing

\bibitem{exponential_Demircigil}
\BIBentryALTinterwordspacing
M.~Demircigil, J.~Heusel, M.~Löwe, S.~Upgang, and F.~Vermet, ``On a model of associative memory with huge storage capacity,'' \emph{Journal of Statistical Physics}, vol. 168, no.~2, p. 288–299, May 2017. [Online]. Available: \url{http://dx.doi.org/10.1007/s10955-017-1806-y}
\BIBentrySTDinterwordspacing

\bibitem{DeepRC}
\BIBentryALTinterwordspacing
M.~Widrich, B.~Schäfl, H.~Ramsauer, M.~Pavlović, L.~Gruber, M.~Holzleitner, J.~Brandstetter, G.~K. Sandve, V.~Greiff, S.~Hochreiter, and G.~Klambauer, ``Modern hopfield networks and attention for immune repertoire classification,'' 2020. [Online]. Available: \url{https://arxiv.org/abs/2007.13505}
\BIBentrySTDinterwordspacing

\bibitem{universal_networks}
\BIBentryALTinterwordspacing
B.~Millidge, T.~Salvatori, Y.~Song, T.~Lukasiewicz, and R.~Bogacz, ``Universal hopfield networks: A general framework for single-shot associative memory models,'' 2022. [Online]. Available: \url{https://arxiv.org/abs/2202.04557}
\BIBentrySTDinterwordspacing

\bibitem{largeassociativememoryproblem}
\BIBentryALTinterwordspacing
D.~Krotov and J.~Hopfield, ``Large associative memory problem in neurobiology and machine learning,'' 2021. [Online]. Available: \url{https://arxiv.org/abs/2008.06996}
\BIBentrySTDinterwordspacing

\bibitem{camus}
\BIBentryALTinterwordspacing
S.~Leclerc, E.~Smistad, J.~Pedrosa, A.~{\O}stvik, F.~Cervenansky, F.~Espinosa, T.~Espeland, E.~A.~R. Berg, P.-M. Jodoin, T.~Grenier, C.~Lartizien, J.~D’hooge, L.~L{\o}vstakken, and O.~Bernard, ``Deep learning for segmentation using an open large-scale dataset in 2d echocardiography,'' \emph{IEEE Transactions on Medical Imaging}, vol.~38, pp. 2198--2210, 2019. [Online]. Available: \url{https://api.semanticscholar.org/CorpusID:73510235}
\BIBentrySTDinterwordspacing

\bibitem{cardiacnet}
\BIBentryALTinterwordspacing
J.~Yang, X.~Ding, Z.~Zheng, X.~Xu, and X.~Li, ``Graphecho: Graph-driven unsupervised domain adaptation for echocardiogram video segmentation,'' 2023. [Online]. Available: \url{https://arxiv.org/abs/2309.11145}
\BIBentrySTDinterwordspacing

\bibitem{unet}
\BIBentryALTinterwordspacing
O.~Ronneberger, P.~Fischer, and T.~Brox, ``U-net: Convolutional networks for biomedical image segmentation,'' 2015. [Online]. Available: \url{https://arxiv.org/abs/1505.04597}
\BIBentrySTDinterwordspacing

\bibitem{nnUnet}
\BIBentryALTinterwordspacing
F.~Isensee, J.~Petersen, A.~Klein, D.~Zimmerer, P.~F. Jaeger, S.~Kohl, J.~Wasserthal, G.~Koehler, T.~Norajitra, S.~Wirkert, and K.~H. Maier-Hein, ``nnu-net: Self-adapting framework for u-net-based medical image segmentation,'' 2018. [Online]. Available: \url{https://arxiv.org/abs/1809.10486}
\BIBentrySTDinterwordspacing

\bibitem{CANet}
\BIBentryALTinterwordspacing
M.~Hanselmann, T.~Strauss, K.~Dormann, and H.~Ulmer, ``Canet: An unsupervised intrusion detection system for high dimensional can bus data,'' \emph{IEEE Access}, vol.~8, p. 58194–58205, 2020. [Online]. Available: \url{http://dx.doi.org/10.1109/ACCESS.2020.2982544}
\BIBentrySTDinterwordspacing

\bibitem{extended_nnunet}
\BIBentryALTinterwordspacing
F.~Isensee, C.~Ulrich, T.~Wald, and K.~H. Maier-Hein, ``Extending nnu-net is all you need,'' 2022. [Online]. Available: \url{https://arxiv.org/abs/2208.10791}
\BIBentrySTDinterwordspacing

\bibitem{properties_nn}
\BIBentryALTinterwordspacing
C.~Szegedy, W.~Zaremba, I.~Sutskever, J.~Bruna, D.~Erhan, I.~Goodfellow, and R.~Fergus, ``Intriguing properties of neural networks,'' 2014. [Online]. Available: \url{https://arxiv.org/abs/1312.6199}
\BIBentrySTDinterwordspacing

\bibitem{nn_easily}
\BIBentryALTinterwordspacing
A.~Nguyen, J.~Yosinski, and J.~Clune, ``Deep neural networks are easily fooled: High confidence predictions for unrecognizable images,'' 2015. [Online]. Available: \url{https://arxiv.org/abs/1412.1897}
\BIBentrySTDinterwordspacing

\bibitem{adiga2024anatomically}
S.~Adiga, J.~Dolz, and H.~Lombaert, ``Anatomically-aware uncertainty for semi-supervised image segmentation,'' \emph{Medical Image Analysis}, vol.~91, p. 103011, 2024.

\bibitem{van2024towards}
G.~Van De~Vyver, S.~Thomas, G.~Ben-Yosef, S.~H. Olaisen, H.~Dalen, L.~L{\o}vstakken, and E.~Smistad, ``Towards robust cardiac segmentation using graph convolutional networks,'' \emph{IEEE Access}, 2024.

\bibitem{wyburd2024anatomically}
M.~K. Wyburd, N.~K. Dinsdale, M.~Jenkinson, and A.~I. Namburete, ``Anatomically plausible segmentations: Explicitly preserving topology through prior deformations,'' \emph{Medical Image Analysis}, p. 103222, 2024.

\bibitem{gao2023anatomy}
Y.~Gao, Y.~Dai, F.~Liu, W.~Chen, and L.~Shi, ``An anatomy-aware framework for automatic segmentation of parotid tumor from multimodal mri,'' \emph{Computers in Biology and Medicine}, vol. 161, p. 107000, 2023.

\bibitem{von2023anatomically}
A.~Von~Zuben, L.~E. Perotti, and F.~A. Viana, ``Anatomically-guided deep learning for left ventricle geometry generation with uncertainty quantification based on short-axis mr images,'' \emph{Engineering Applications of Artificial Intelligence}, vol. 121, p. 106012, 2023.

\bibitem{memformer}
\BIBentryALTinterwordspacing
Q.~Wu, Z.~Lan, K.~Qian, J.~Gu, A.~Geramifard, and Z.~Yu, ``Memformer: A memory-augmented transformer for sequence modeling,'' 2022. [Online]. Available: \url{https://arxiv.org/abs/2010.06891}
\BIBentrySTDinterwordspacing

\bibitem{rate}
\BIBentryALTinterwordspacing
E.~Cherepanov, A.~Staroverov, D.~Yudin, A.~K. Kovalev, and A.~I. Panov, ``Recurrent action transformer with memory,'' 2024. [Online]. Available: \url{https://arxiv.org/abs/2306.09459}
\BIBentrySTDinterwordspacing

\end{thebibliography}

% \bibliographystyle{IEEEtran}
% \bibliography{Reference}

\end{document}


