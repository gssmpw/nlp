\section{Background}\label{sec:background}
    
    \subsection{System 1 vs System 2 Reasoning in LLMs}\label{sec:s1vs2}
        
        Dual Process Theory distinguishes between two types of cognitive processes: System 1, which is fast, intuitive, and automatic, and System 2, which is slower, more deliberate, and rational \citep{evans_dual-process_2013}. This framework is highly influential on current theories of human reasoning, judgment, and decision-making. For instance, classic problems like the Bat and Ball Problem \citep{frederick_cognitive_2005}, also shown in Figure \ref{fig:bat-ball}, demonstrate how System 1 can produce rapid but often flawed responses, while System 2 enables more accurate outcomes through reflective thinking \citep{evans_dual-processing_2008, evans_reflections_2019}.

        % Despite its broad acceptance, Dual Process Theory is challenged by other cognitive models. Some theories suggest an intermediate cognitive system that merges elements of both System 1 and System 2, offering a more flexible approach to tasks requiring both automatic and deliberate thinking \citep{varga_beyond_2014}. Cognitive Load Theory, for example, emphasizes the limitations of working memory and the influence of mental effort on learning and problem-solving \citep{sweller_cognitive_1988}, with significant implications for instructional design and educational psychology. Additionally, Heuristics and Biases Theory \citep{kahneman_psychology_1982} provides a comprehensive framework for understanding decision-making under uncertainty, highlighting the role of mental shortcuts that, while efficient, can lead to systematic biases. Concepts such as availability, representativeness, and anchoring heuristics illustrate common cognitive biases in decision-making. Attribution Theory, originally developed by \citet{heider_psychology_1958} and later expanded by \citet{kelley1967attribution}, explores how individuals assign causes to behaviors, distinguishing between internal (dispositional) and external (situational) attributions. This theory offers valuable insights into social cognition and attitude formation. Research by \citet{de_neys_conflict_2008} suggests that people can detect errors in their intuitive responses, indicating the presence of a monitoring mechanism that can activate System 2 processing. This finding supports the idea that even during rapid System 1 thinking, individuals may have an awareness of potential inaccuracies, prompting a shift to more analytical, System 2 reasoning.

        Recent advancements in AI have sought to replicate System 1 and System 2 processes using various prompting strategies in LLMs \citep{Nye2021, kojima_large_2022, hagendorff_human-like_2023, weston_system_2023, saha_system-1x_2024, yu_distilling_2024}, with mixed results \citep{vatsal_survey_2024}. Researchers have attempted to simulate System 1 and System 2 reasoning in LLMs by experimenting with common prompt templates, such as zero-shot and chain-of-thought prompts \citep{kojima_large_2022, wei_chain_2022}. Additionally, efforts to emulate System 2 thinking have led to the development of novel prompting techniques that expand on chain-of-thought, mimicking aspects of human problem-solving processes using search algorithms \citep{wang_plan-and-solve_2023, wang_self-consistency_2023, yao_tree_2023, zhang_cumulative_2024}. In contrast, modeling System 1-like processes in LLMs has involved heuristic-driven, rapid decision-making approaches \citep{hagendorff_human-like_2023}, which can generate responses without immediate reasoning steps \citep{yu_distilling_2024}. On the other hand, simulating System 2 requires more complex logical reasoning and problem-solving tasks that involve deliberate, step-by-step processing \citep{Nye2021}. Some researchers have also explored hybrid models that integrate both cognitive systems, aiming to harness their combined strengths for more effective decision-making in AI \citep{saha_system-1x_2024}.

        In their survey, \citet{vatsal_survey_2024} examine how prompt engineering can evoke distinct reasoning and responses from LLMs. They categorize prompt engineering techniques into zero-shot, one-shot, and few-shot prompts, each with specific benefits and limitations. Zero-shot prompting involves giving the model a task description and expecting it to perform the task based on its pre-existing knowledge, though this approach may be constrained by the model's interpretation of the task description. One-shot prompting offers a single example to provide a clear reference for the task, while few-shot prompting supplies multiple examples, offering richer context and enhancing task generalization, which has been shown to significantly improve performance. The survey explores how these methods influence LLMs' effectiveness in various NLP tasks, including question answering, text generation, and language inference.
        
        A prevalent assumption in the literature is that System 1 leads to the wrong answer. System 1 processes, often seen as intuitive but error-prone, are typically simulated in LLMs through straightforward, concise zero-shot prompts \citep{kojima_large_2022, hagendorff_human-like_2023, yu_distilling_2024}. In contrast, System 2 processes, associated with more accurate and thoughtful responses, are mimicked through chain-of-thought prompting, which enhances performance on tasks like the Cognitive Reflection Test (CRT) \citep{hagendorff_human-like_2023}. However, this assumption limits the datasets used in these studies to those with clear correct or incorrect answers, overlooking the nuance of real-world reasoning. It is important to note that System 1 reasoning is not simply erroneous reasoning, but rather it is a set of reasoning processes (akin to heuristics) that has distinctive properties, and thus it can and should be modeled separately from System 2 reasoning. Moreover, System 1 processes are an inseparable step of human reasoning \citep{gilovich_heuristics_2002} and cannot be ignored. System 1 reasoning enables us to perform many tasks effortlessly, such as walking, eating, or driving, where over-analyzing every detail would be impractical or impossible. System 1 is almost effortless for humans \citep{gladwell_blink_2007} but as we demonstrate in this paper, replicating System 1 behaviors in LLMs is more complex than simply applying zero-shot prompts, especially with datasets lacking normatively correct or incorrect responses.

     \subsection{The Natural Language Inference (NLI) Format}\label{sec:nli}
        In our study, we utilize an enhanced version of the Natural Language Inference (NLI) task, building upon prior foundational work \citep{bowman_large_2015}. NLI, a successor to Recognizing Textual Entailment (RTE) \citep{quinonero-candela_pascal_2006},\footnote{RTE focused only on predicting whether the relationship between two sentences was entailment, but NLI bifurcated the ``False'' label for more granularity} is centered around evaluating whether a hypothesis $h$ can logically follow from a given premise $p$. For illustration, consider the premise $p=$ ``A crowd gathers as three blue cars begin a race.'' Possible relationships between this premise and a hypothesis $h$ could be:
        \begin{enumerate}
            \item Entailment: $h=$ ``A race is taking place.'' must be true if $p$ is true.
            \item Contradiction: $h =$ ``There are no cars racing.'' cannot be true if $p$ is true.
            \item Neutral: $h =$ ``Three men are competing in a race.'' is neither necessarily true nor necessarily false given $p$.
        \end{enumerate}

        The most common goal of NLI datasets is to study how a set of human reasoners \textit{naturally} reasons about the inferential relationships in the NLI items. LLMs are traditionally trained and benchmarked on NLI datasets to enhance their naturalistic reasoning capabilities \citep{bowman_large_2015, williams_broad-coverage_2018, nie_adversarial_2020, williams-etal-2022-anlizing}. 
        
        While typically trifurcated into distinct categories, this specific categorization can suffer from problems of underspecification and a lack of granularity \citep{nighojkar_no_2023}. Addressing these complexities, researchers have proposed various modifications to the standard categorization, including: 
        
        \begin{itemize}
            \item Introducing an ``entailment strength'' parameter, reflecting either model confidence or perceived likelihood, though such measures have historically grappled with issues like ambiguous evaluation standards and inconsistent annotator judgments \citep{chen_uncertain_2020, meissner_embracing_2021}.
            \item Examining the variability in annotator perspectives, considering whether a `neutral' judgment indicates balanced reasons for both agreement and disagreement or a complete absence of decisive evidence \citep{pavlick_inherent_2019, zhang_capturing_2021, zhang_identifying_2021, zhou_distributed_2022}.
            \item Differentiating between various degrees of entailment, such as ``absolutely entails'' versus ``likely entails,'' which echoes ongoing research into probabilistic reasoning \citep{Kahneman2011}.
        \end{itemize}

        To refine these categories further, we propose a structured NLI framework with six detailed categories, broadening the inferential spectrum from absolute contradiction to definite entailment. These categories are: (A) \textit{Absolutely must be false}, (B) \textit{Is more likely to be false}, (C) \textit{Has strong reasons to be true and strong reasons to be false}, (D) \textit{Has no reasons to be either true or false}, (E) \textit{Is more likely to be true}, and (F) \textit{Absolutely must be true}.

        This broader categorization not only clarifies the guidance provided to annotators but also enriches the nuances in the data, aligning with earlier initiatives to expand NLIâ€™s analytical depth \citep{pavlick_inherent_2019, zhang_ordinal_2017}. This framework allows for more detailed response options while still fitting into the three traditional NLI categories, making it more flexible. The use of NLI is widespread, affecting fields such as measuring psychological traits \citep{Laverghetta2021c, Laverghetta2022, Laverghetta2022a, Laverghetta2023b, Laverghetta2023c}, comparing text similarity \citep{nighojkar_mutual_2021}, and evaluating the quality of translations and paraphrases \citep{nighojkar_improving_2021}. This demonstrates its versatility and usefulness in different areas.