\section{Data Collection} \label{sec:data_collection}
    \subsection{Survey Design}
        
        Our study utilizes a dual-phase survey design, hosted on Qualtrics, with 60 participants recruited via Prolific. The survey consists of 27 questions in the first phase, which are repeated in the second phase, following the design of the two-response paradigm \citep{bago_fast_2017}. In this paradigm, participants are asked to provide two separate responses to the same question or task. The first response is typically their initial, instinctive answer, while the second response is given after some deliberation or additional information is provided. This method allows us to analyze the differences between intuitive and reflective thinking, or System 1 and System 2, shedding light on cognitive processes like reasoning, decision-making, and how people change their minds when given more time or information. In the first phase, aimed at eliciting System 1 reasoning, participants are presented with NLI questions while simultaneously solving a puzzle as a distraction. Each question is divided into two parts: the premise and the conclusion, with six response options (A-F) available, as detailed in Section \ref{sec:nli}. The exact instructions provided to the participants are in Figure \ref{fig:instructions}. Participants read the premise (Figure \ref{fig:premise}), solve the puzzle (Figure \ref{fig:distractor}), then read the conclusion before selecting one of the six options (Figure \ref{fig:conclusion}). Note that participants never have access to the premise and the conclusion together in this phase and need to rely on whatever they retain from the premise when answering the question about the conclusion on Screen 3. The survey begins with an example question to familiarize participants with the task. To reduce fatigue, one-minute breaks are incorporated after each question (a set of all three screens).

        \begin{figure}[t]
            \centering
            \begin{minipage}{0.49\textwidth}
                \centering
                \includegraphics[width=\textwidth]{images/instructions.png}
                \caption{Survey instructions}
                \label{fig:instructions}
            \end{minipage}
            \hfill
            \begin{minipage}{0.49\textwidth}
                \centering
                \includegraphics[width=\textwidth]{images/premise.png}
                \caption{Survey screen 1: Premise}
                \label{fig:premise}
                \includegraphics[width=\textwidth]{images/puzzle.png}
                \caption{Survey screen 2: Distractor}
                \label{fig:distractor}
                \includegraphics[width=\textwidth]{images/conclusion.png}
                \caption{Survey screen 3: Conclusion}
                \label{fig:conclusion}
            \end{minipage}
           \label{fig:puzzle-distraction}
        \end{figure}

        In the first phase, a time limit is imposed to encourage rapid reasoning. The initial time allocated per question is determined using the following formula:
        \[ \textit{base time} = 0.0787 \times \textit{sentence length} + 0.0016 \times \textit{perplexity} + 6.3276 \]
        These coefficients are derived from ordinary least squares regression \citep{michalos_ordinary_2014}, based on data collected in-house. We recorded the time each participant (the co-authors and other volunteering students) took to read a question and trained the regression model to predict the 90th percentile reading time based on the sentence length and its perplexity \citep{jelinek_perplexitymeasure_1977}. Additionally, the reading time is adaptively adjusted based on participant pacing, either increasing or decreasing the time allotted depending on their speed, with a minimum engagement time set at 10 seconds. If a participant moves on from Screen 1 or answers the question on Screen 3 within 70\% of the base time for that screen, we reduce their time for all the following questions by 10\%. This reduction is compounded, so if the participant takes less than 70\% of the reduced time, we reduce the time for the subsequent questions by 19\% (10\% compounded twice). We never reduce the time below 10 seconds. A similar calculation works for slow participants. If they take over 90\% of the base time, we increase the time for the subsequent questions by 10\%. Failure to comply with the time limits results in warnings and may lead to survey termination after repeated infractions.
        
        The puzzles, essential for the distraction component, are generated using OpenAI's \texttt{GPT-4-0314} model. A structured query produces rhyming riddles with four potential answers, designed to engage participants critically without overwhelming them. Figures \ref{fig:premise}-\ref{fig:conclusion} illustrate the three components of a question in the first phase. The second phase omits the puzzles and combines the premise and conclusion into a single screen per question, allowing participants two minutes to carefully consider each NLI item and record their reasoning in a text box, explicitly targeting System 2 reasoning.
        
        To detect bad faith participation, embedded attention checks resembling standard questions are distributed randomly through phase 1 (questions in phase 2 follow the same order as phase 1), drawn from ChaosNLI \citep{nie_what_2020} with at least 90\% inter-annotator agreement. ChaosNLI is an NLI dataset created with 100 annotators for each question. This lets us set thresholds on inter-annotator agreement (like 90\%) that would not be possible with other NLI datasets like SNLI \citep{bowman_large_2015} or MultiNLI \citep{williams_broad-coverage_2018} that only have 5 annotations per question. If a participant's responses fail to match more than 5 out of the 8 attention check questions across both phases, their data is excluded, and a new participant is recruited to ensure 30 valid, good-faith responses per question. Additionally, responses from participants who do not complete the entire survey are disregarded.
        
    \subsection{Item Types}
    
        In Section \ref{sec:introduction}, we introduced the full reasoning spectrum problem (where existing datasets and methods fail to capture the whole spectrum of human reasoning patterns) and the data contamination problem (the possibility that publicly available datasets may be included in the training data of large language models, or LLMs). In Section \ref{sec:nli}, we also explained our decision to use NLI as our preferred format. Commonly available NLI datasets do not attempt to capture annotations that distinguish between System 1 and System 2 reasoning in humans. Additionally, it is highly likely that existing public NLI datasets have already been used to train LLMs. Consequently, these datasets suffer from both the full reasoning spectrum problem and the data contamination problem. To address these issues, we created our own dataset of NLI questions using OpenAI's GPT-4 model (specifically \texttt{GPT-4-0314}) to generate all the items. We provided the model with multiple prompts and used different OpenAI API calls to generate the premises and conclusions separately, ensuring that the model never produced both simultaneously. Finally, we made two additional API calls to rephrase the premises and conclusions individually. All our prompts are in Appendix \ref{secA:dataset_creation} along with details about our data generation pipeline. We generate different types of items to prevent monotonicity in the dataset and to prevent any pattern recognition for the humans doing our survey and the models we use to predict the human responses. These item types are explained in more detail below.
        
        \paragraph{StereoNLI}
        StereoNLI connects NLI with stereotypes, building on previous research showing that human System 1 reasoning is often biased and influenced by stereotypes \citep{Kahneman2011, geeraert_when_2013}. We utilized the StereoSet dataset \citep{nadeem_stereoset_2021} to select seed words for generating StereoNLI questions. StereoSet comprises 17,000 sentences that assess stereotype bias in language models concerning gender, race, religion, and profession. Each sentence is labeled by multiple human annotators as either `anti-stereotype', `stereotype', `unrelated', or `related'. From StereoSet, we randomly chose nouns associated with gender, race, religion, and profession, then prompted GPT-4-0314 to generate a name for a hypothetical person called `X'. We asked GPT-4 to write a paragraph with three sentences reflecting common assumptions about `X' based on the given traits. This paragraph became our premise. Excluding gender due to its limited variety in the dataset, we randomly selected one of the three traits and prompted GPT-4 to generate an assumption about `X' with a truth value. For each scenario, we created three instances---contradiction, neutral, and entailment---over three separate conversations with the GPT-4 chatbot via the OpenAI API. Finally, the premises and all three conclusions were rephrased. Table \ref{tab:item_types} presents three examples of StereoNLI items that share the same premise.

        \paragraph{Fallacy}
        Fallacies of argumentation are arguments whose logical structures that have very little to no deductive validity, despite being commonly used in informal reasoning.\footnote{For more nuanced discussions of what constitutes a fallacy, see \cite{Fearnside1959,Walton1990,Walton1985,Walton2008}.} Previous cognitive psychology research \citep{boissin_debiasing_2022} has examined how System 1 can sometimes engage in fallacious reasoning. In our dataset, we focus on three specific types of fallacies. The first, \textit{post hoc ergo propter hoc}, occurs when someone mistakenly believes that because one event follows another, the first event caused the second. The second, \textit{slippery slope}, posits that an action will set off a chain of events leading to an undesirable outcome without establishing or quantifying the relevant contingencies. This fallacy, also known as ``the domino effect,'' often implies a long series of intermediate events connecting a seemingly harmless start to an undesirable end, assuming uncertain or unlikely consequences. The third fallacy, \textit{straw person}, involves misrepresenting an opponent’s argument to make it easier to refute. We created templates to generate premise-conclusion pairs for each type of fallacy. For example, in the case of \textit{post hoc ergo propter hoc}, the premise follows the structure ``X happened right before Y,'' with the conclusion stating ``X caused Y.'' GPT-4 fills in the details for X and Y, and we then rephrase the premises and conclusions to produce the final items. Table \ref{tab:item_types} displays examples of Fallacy items, one for each fallacy.

        \paragraph{Syllogism and Stereo Syllogism}
        Syllogisms, a core component of traditional logic used in philosophical reasoning, consist of a major premise (a general statement), a minor premise (a specific statement), and a conclusion. The structure of a syllogism, or its figure, depends on the positioning of the middle term (M), subject (S), and predicate (P). The complex structure of syllogisms has led many researchers to examine its effects on eliciting System 1 and System 2 reasoning \citep{evans_conflict_1983, khemlani_theories_2012, da_silva_system_2023}. For our dataset, we selected singular nouns as seed words to guide the GPT-4 model in generating the major and minor premises while preserving the order of M, P, and S. The four primary syllogistic figures are: (1) \textit{Premise: M is P. S is M. Conclusion: S is P.} (2) \textit{Premise: P is M. S is M. Conclusion: S is P.} (3) \textit{Premise: M is P. M is S. Conclusion: S is P.} (4) \textit{Premise: P is M. M is S. Conclusion: S is P.} Finally, we rephrased the premises and conclusions to produce the final items. We also created a variant of syllogism questions using seed words from StereoSet, referred to as Stereo Syllogism. Table \ref{tab:item_types} presents four examples of Stereo Syllogism items, one for each figure.

        \paragraph{Guilt}
        Drawing on studies that demonstrate dual-process effects when participants are asked to assess the guilt of a suspect in a hypothetical scenario \citep{Peer2013, Kassin2013, Rachlinski2015, Wistrich2015, Bergius2020}, we designed questions using the following template: First, we prompted GPT to generate a few sentences describing the details of a crime for which the perpetrator has not been identified. We then asked GPT-4 to list several features likely to be true of the culprit. This enabled us to create two forms of questions: the entailment form (\textit{E Guilt}), which included the crime description, a suspect characterized by features that made them appear likely to be guilty, and a conclusion asking whether the suspect is guilty; and the contradiction form (\textit{C Guilt}), where the suspect is described as not possessing the features, or as having opposite features. Table \ref{tab:item_types} shows examples where the E Guilt premise describes a suspect who initially seems likely to have committed the crime, while the C Guilt premise describes a suspect who does not seem to fit the crime's circumstances.

        \paragraph{Primacy and Recency} 
        The first and last pieces of information presented can disproportionately influence the reader's perception due to what is known as the \textit{serial position effect} \citep{murdock_serial_1962}. To incorporate questions leveraging this effect, we began by manually writing several sentences that simply stated an individual's name and a characteristic (e.g., ``Simon is a professor''). We then asked GPT-4 to generate five sentences about that individual that would likely be true if the characteristic were accurate (\textit{likely-true}). We selected two sentences that best aligned with our archetypal conceptions of the characteristic. Next, we asked GPT-4 to generate five sentences about that individual that were likely \textit{not} true (\textit{likely-false}) and selected three of these. We then created two forms of questions: \textit{P Primacy/Recency} questions had premises starting and ending with sentences from the \textit{likely-true} category, with three \textit{likely-false} sentences in the middle, and the original sentence describing the individual and characteristic as the conclusion. \textit{N Primacy/Recency} questions had the same premise, but the conclusion was the original sentence in a negated form (e.g., ``Simon is not a professor'' instead of ``Simon is a professor''). Examples of these questions are provided in Table \ref{tab:item_types}.

    \subsection{Dataset Characteristics} \label{subsec:dataset_characteristics}
        
        \begin{figure}
            \centering
            \includegraphics[width=\textwidth]{images/label_distribution.png}
            \caption{Label percentage distribution histogram}
            \label{fig:label_percentage_distribution}
        \end{figure}
    
        We gathered 45 items across all item types (10 StereoNLI, 7 Fallacy, 8 Syllogism/Stereo-Syllogism, 10 Guilt, and 10 Primacy/Recency), each annotated 30 times by humans during the first phase (System 1) and an additional 30 times by the same individuals in the second phase (System 2). Figure \ref{fig:label_percentage_distribution} provides a quantitative comparison of voting preferences between System 1 and System 2, highlighting how the distribution of votes across six options changed from System 1 to System 2. Notably, option E exhibited the most significant shift, decreasing by $11.07$ percentage points.

        \begin{figure}
            \centering
            \includegraphics[width=\textwidth]{images/corr.png}
            \caption{Kendall's $\tau$ between various features of the items in our dataset. Only statistically significant (p-value $<0.05$) $\tau$ values are shown. \textit{***} indicates a p-value $<0.001$, \textit{**} indicates a p-value $<0.01$, and \textit{*} indicates a p-value $<0.05$.}
            \label{fig:tau}
        \end{figure}
        
        To ensure the reliability of our results in subsequent experiments, we examined potential spurious correlations within our dataset, focusing on four primary factors: the length of the premise and conclusion, the perplexity of the premise and conclusion, BLEU scores \citep{papineni_bleu_2001} between the premise and conclusion, and VADER sentiment scores \citep{hutto_vader_2014} for both the premise and conclusion. Figure \ref{fig:tau} displays the Kendall's $\tau$ \citep{kendall_new_1938} between these factors and the mean value of participant responses. Ideally, no feature should correlate with the gold labels from either System 1 or System 2, as such a correlation could be considered spurious. The highest $\tau$ value is observed between System 1 and System 2 gold labels, indicating that a participant’s responses in System 1 may be predictive of their responses in System 2. However, in this paper, we treat System 1 and System 2 independently and do not attempt to predict System 2 responses based on System 1, leaving this for potential future work.
    
        \begin{figure}[t]
            \centering
            \begin{subfigure}{0.2\textwidth}
                \centering
                \includegraphics[width=\textwidth]{images/stereonli.png}
                \caption{StereoNLI}
                \label{fig:sankey-stereonli}
            \end{subfigure}
            \hfill
            \begin{subfigure}{0.2\textwidth}
                \centering
                \includegraphics[width=\textwidth]{images/fallacy.png}
                \caption{Fallacy}
                \label{fig:sankey-fallacy}
            \end{subfigure}
            \hfill
            \begin{subfigure}{0.2\textwidth}
                \centering
                \includegraphics[width=\textwidth]{images/syllogism.png}
                \caption{Syllogism}
                \label{fig:sankey-syllogism}
            \end{subfigure}
            \hfill
            \begin{subfigure}{0.2\textwidth}
                \centering
                \includegraphics[width=\textwidth]{images/stereo_syllogism.png}
                \caption{S. Syllogism}
                \label{fig:sankey-stereo-syllogism}
            \end{subfigure}
            \hfill
            \begin{subfigure}{0.2\textwidth}
                \centering
                \includegraphics[width=\textwidth]{images/e_guilt.png}
                \caption{E Guilt}
                \label{fig:sankey-e-guilt}
            \end{subfigure}
            \hfill
            \begin{subfigure}{0.2\textwidth}
                \centering
                \includegraphics[width=\textwidth]{images/c_guilt.png}
                \caption{C Guilt}
                \label{fig:sankey-c-guilt}
            \end{subfigure}
            \hfill
            \begin{subfigure}{0.2\textwidth}
                \centering
                \includegraphics[width=\textwidth]{images/p_primacy.png}
                \caption{P Primacy}
                \label{fig:sankey-p-primacy}
            \end{subfigure}
            \hfill
            \begin{subfigure}{0.2\textwidth}
                \centering
                \includegraphics[width=\textwidth]{images/n_primacy.png}
                \caption{N Primacy}
                \label{fig:sankey-n-primacy}
            \end{subfigure}
           \caption{Sankey diagram of changes in human responses from System 1 (phase 1 of the survey) to System 2 (phase 2 of the survey) for all item types in our dataset.}
           \label{fig:sankey}
        \end{figure}

        Figure \ref{fig:sankey} presents Sankey diagrams to visually represent the transition of human responses from System 1 to System 2, illustrating the distribution and shifts across our six labels by varying the width of the links. This visualization allows us to easily see how successful each question type is at highlighting the differences between System 1 and System 2 reasoning are, as well as how the distribution of human responses differs with each question type. We observe that most participant responses fluctuated between E and D for StereoNLI, and between E and F for fallacy, syllogisms, and stereo syllogisms. Conversely, E guilt questions exhibited minimal fluctuation, with most participants maintaining their E response from System 1 to System 2. This consistency is expected, as is the trend of shifting from E to B for C Guilt, since these questions are designed to mislead participants in System 1. This misleading effect is also observed, though to a lesser extent, in P Primacy and N Primacy. Another noteworthy trend is that participants generally avoided selecting the neutral options C and D for any question type, suggesting an underlying bias. Further research may shed light on the reasons for this behavior.

        \paragraph{Inclusion and Ethics Statement} Our study was designed and conducted with careful consideration for ethical research practices and inclusivity. We recruited 60 participants via Prolific, aiming for a diverse sample. All participants provided informed consent and were clearly instructed about the study's nature. Data privacy and protection measures were implemented to ensure participant anonymity and confidentiality. We incorporated breaks to reduce fatigue and set time limits to manage cognitive load, considering participant well-being. The study design included various item types to prevent monotonicity and reduce potential biases. We used AI models (GPT-4) to generate survey items, addressing potential data contamination issues and creating a novel dataset. We have been transparent about our methodology, including the use of AI-generated content and both proprietary and open-source language models in our experiments. By adhering to these ethical principles and inclusive practices, we aim to contribute to the field of AI and cognitive science research in a responsible and equitable manner.