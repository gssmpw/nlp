\section{Introduction}\label{sec:introduction}

    Capturing the nuanced, often imperfect, and highly diverse reasoning processes of humans presents significant challenges. Traditional uses of AI for this purpose have focused primarily on optimizing accuracy and efficiency, but this approach often neglects the complexity of human cognition, where decisions can be significantly influenced by intuition, emotion, and prior experiences. The central issue is that current AI models struggle to capture the full spectrum of human reasoning, particularly in problem spaces where decisions are not simply correct or incorrect but rather involve a plurality of diverse thought processes. Addressing this limitation is crucial for advancing AI systems that can genuinely understand and interact with humans on a deeper, more intuitive level, making it a critical area of research that extends beyond technical optimization.

    Understanding and predicting human reasoning is as fascinating now as it has been for thousands of years \citep{Wason1966-WASR, aristotle_organon_2013}. Dual Process Theory, one of the most influential theories in contemporary cognitive science \citep{stanovich_individual_2000, Kahneman2011}, distinguishes between two types of cognitive processes: System 1 and System 2.\footnote{Although in this paper we will use the more popular terms System 1 and System 2, it should be noted that they are somewhat misleading, in that they imply a coordinated system of processes working together. For this reason, some authors prefer the terminology ``Type 1'' and ``Type 2,'' e.g., \cite{Evans2018}.} System 1 operates automatically, rapidly, and effortlessly, guiding intuitive and habitual decisions without conscious control. In contrast, System 2 is slower, more deliberate, and responsible for managing mental activities that require focused attention, such as complex calculations and deliberate decision-making. These dual processes are fundamental to understanding how humans navigate decisions, both trivial and significant, and provide a framework for assessing whether AI can genuinely replicate human-like reasoning.

    % Given the importance of the dual process theory, it becomes essential to explore whether AI, particularly large language models (LLMs), can emulate such reasoning patterns. LLMs have become a cornerstone in AI-driven reasoning due to their complexity and extensive training on vast amounts of text. However, while LLMs are often employed in tasks requiring reasoning, it remains unclear whether they can adequately differentiate between and replicate the fast, intuitive responses characteristic of System 1 and the slow, deliberate reasoning of System 2. Some researchers have suggested---or even demonstrated through experimental paradigms---that LLMs with lower complexity or simpler prompts tend to engage in System 1 reasoning, while more complex architectures or prompts encourage System 2-like reasoning \citep{kojima_large_2022, hagendorff_human-like_2023, weston_system_2023, saha_system-1x_2024, yu_distilling_2024}. However, many of these studies operate under the assumption that System 1 is inferior and should be avoided, focusing instead on making AI reasoning more like human System 2. Their use of datasets such as the Cognitive Reflections Test (CRT) \citep{frederick_cognitive_2005}, designed to provoke System 1 into giving the wrong answer while System 2 provides the correct one, is a result of this assumption. This focus on accuracy neglects the reality that human reasoning is rarely so clear-cut.

    Dual process theories have been given a second look in the age of the large language model (LLM), which have become a cornerstone in AI-driven reasoning due to their tremendous success on a variety of tasks. Some researchers have suggested---or even demonstrated through experimental paradigms---that LLMs with lower complexity or simpler prompts tend to engage in System 1 reasoning, while more complex architectures or prompts encourage System 2-like reasoning \citep{kojima_large_2022, hagendorff_human-like_2023, weston_system_2023, saha_system-1x_2024, yu_distilling_2024}. However, many such claims operate under the assumption that System 1 is inferior and should be avoided, focusing instead on making AI reasoning more like human System 2. The use of datasets such as the Cognitive Reflections Test (CRT) \citep{frederick_cognitive_2005}, designed to provoke System 1 into giving the wrong answer while System 2 provides the correct one, is a reflection of this assumption. This focus on accuracy neglects the reality that human reasoning is rarely so clear-cut.

    Given these complexities, how well can our best AI approaches capture human reasoning? AI reasoning encompasses a broad range of approaches that have been described at times as efficient \citep{maclure_ai_2021}, logical \citep{hagedorn_knowledge_2020}, mechanistic, or capable of making sense of complex scenarios \citep{zollman_analyzing_2023}. However, it has also been criticized for being biased and opaque \citep{oneil_weapons_2017}. Numerous studies have aimed to reduce bias in AI reasoning and promote fairness \citep{ma_fairness-guided_2023}, yet human reasoning is naturally biased and flawed \citep{gilovich_heuristics_2002, ariely_predictably_2010, stanovich_rationality_2018}. Eliminating these flaws can therefore make AI reasoning less human-like \citep{chemero_llms_2023, prescott_understanding_2023}. Furthermore, human reasoning processes are deeply influenced by context, individual differences, and prior knowledge, making it challenging to predict human decisions based purely on logical correctness \citep{nisbett_telling_1977, kahneman_choices_1984, stanovich_individual_2000}. And thus, artificial reasoners that prioritize normative correctness by reducing errors \citep{sun_survey_2024} may become less capable of modeling human-like reasoning.
    
    % Unlike conventional efforts that strive for the most accurate AI reasoning , this paper focuses on emulating human reasoning behaviors across an entire population. This approach involves mimicking the behavior of a group of individuals without any prior knowledge about them.

    In this paper, we advocate for an approach to computational cognitive modeling that considers an approach successful only to the extent that it can predict the \textit{full distribution} of naturalistic behaviors in some task of interest. We will refer to this challenge as the \textit{full reasoning spectrum problem}. %, where efforts to model human reasoning often fail to capture the true variability and depth of human thought. 
    Perhaps nowhere is this problem more apparent than in the blurred boundaries between System 1 and System 2. Faced with a reasoning problem, individuals may arrive at the same conclusion through different cognitive paths, or different individuals may use the same cognitive process to reach different conclusions \citep{de_neys_logic_2019, Nye2021}. \citet{hamade2024designing} argue that ``achieving superhuman performance is not sufficient; AI needs to account for suboptimal actions or idiosyncratic style from humans.'' Thus, it is important not only to capture whether human reasoners get the answer right or wrong, but \textit{what types} of wrong answers they guess, in what proportions those wrong answers are given in a sample of human reasoners, and how those proportions are affected by the environments in which the individuals perform their reasoning. The \textit{full reasoning spectrum problem} is the challenge of designing artificially intelligent reasoning systems that can predict all of these.
    %This variability is often overlooked in AI models that prioritize clear-cut answers.

    \begin{figure}
        \centering
        \includegraphics[width=\textwidth]{images/bat-ball.jpeg}
        \caption{A question to and response by ChatGPT (GPT-4o)---without using browsing capabilities---shows that the LLM is familiar with this common question from cognitive science already, and thus questions of this type have questionable validity when used to assess its reasoning.}
        \label{fig:bat-ball}
    \end{figure}
    
    Moreover, familiarity with specific problems can significantly influence the cognitive process employed by both humans and LLMs. For humans, prior experience with a problem might lead them to rely on System 1 for quick, intuitive responses, while unfamiliarity might prompt more deliberate System 2 reasoning \citep{evans_dual-processing_2008, Kahneman2011, klein_sources_2017}. For LLMs, this raises concerns about the \textit{data contamination problem} \citep{sainz_nlp_2023, balloccu_leak_2024}, where LLMs might rely on memorized information from their training data rather than engaging in genuine reasoning. Given that many modern LLMs \citep{brown_language_2020, almazrouei_falcon_2023, jiang_mistral_2023, dubey_llama_2024} are trained on vast datasets collected from the internet \citep{penedo_refinedweb_2023, raffel_exploring_2023, together2023redpajama}, there is a significant risk that the questions used in studies were part of the LLMs' training data, further complicating efforts to assess true reasoning capabilities. To illustrate this issue, consider the study by \citet{hagendorff_human-like_2023}, which examined ChatGPT's reasoning abilities using just three problems from the CRT as templates to create 150 nearly identical problems, altering only the numbers and objects. A very simple question to ChatGPT (shown in Figure \ref{fig:bat-ball}) reveals that the LLM is familiar with not only the problem but also the conclusions researchers draw from human responses to it. This raises doubts about the validity of inferences made about LLMs' reasoning abilities. Employing LLMs to generate fresh, original questions that distinguish between System 1 (intuitive, fast) and System 2 (deliberate, slow) reasoning processes helps avoid this problem.

    \begin{figure}
        \centering
        \includegraphics[width=\textwidth]{images/intro.png}
        \caption{Similarity between human reasoning (System 1 and System 2) with LLMs' reasoning. Base prompting just prompts the LLMs for the answer and personality prompting prompts the LLM with different personality prompts and finds the best weight for each prompt using a genetic algorithm with K-fold cross validation. More details in Section \ref{sec:experiments}.}
        \label{fig:intro_results}
    \end{figure}

    % To illustrate this issue, consider the study by \citet{hagendorff_human-like_2023}, which examined ChatGPT's reasoning abilities using just three problems from the CRT as templates to create 150 nearly identical problems, altering only the numbers and objects. A very simple question to ChatGPT (shown in Figure \ref{fig:bat-ball}) reveals that the LLM is familiar with not only the problem but also the conclusions researchers draw from human responses to it. This raises doubts about the validity of inferences made about LLMs' reasoning abilities. In this paper, we tackle the data contamination problem by employing LLMs to generate fresh, original questions that distinguish between System 1 (intuitive, fast) and System 2 (deliberate, slow) reasoning processes. We then collect responses from humans (via crowdsourcing) and evaluate LLMs' ability to replicate the entire set of responses rather than just a supposed correct answer that usually is considered to be the mode (or gold label) of the distribution.

    % Given that it is unclear whether LLMs can be explicitly directed to perform System 1 or System 2 reasoning---some might even argue that these concepts do not apply to LLMs, which are merely predicting the most likely next token---we need a straightforward reasoning task. This task should impose System 1/2 constraints on humans while allowing LLMs to arrive at the correct answer without being forced to use a specific type of reasoning. This allows us to compare different prompting styles and LLMs and analyze which of them exhibit behaviors most akin to human System 1 and/or System 2 reasoning.
    We design questions in the Natural Language Inference (NLI) format (discussed in Section \ref{sec:nli}), incorporating diverse reasoning structures like syllogisms, fallacies, and belief biases \citep{evans_conflict_1983, sap_annotators_2022, yang_belief_2023}. Additionally, we modify the NLI format from three options to six, providing more granularity and nuance in human responses. Using the concept of personality prompting \citep{de_paoli_improved_2023, gu_effectiveness_2023, kamruzzaman_prompting_2024}, we aim to predict not just the majority response (or the gold label) but the \textit{entire distribution} of human responses to a given question, accounting for the likelihood that the humans participating in our study may possess different personality traits. These personality prompts are created from the traits in the Big Five Model \citep{roccas_big_2002}: openness, conscientiousness, extraversion, agreeableness, and neuroticism. We find that performance on this task improves when we allow the LLM to emulate different human personalities. Each prompt in this suite of personality prompts is assigned a weight that determines the frequency of that prompt's response in our final pool of responses. These weights (one for each prompt) are fine-tuned using a genetic algorithm \citep{mitchell_introduction_1996}, and the final distribution is compared with human responses in Figure \ref{fig:intro_results}. This entire process and its results are detailed in Section \ref{sec:experiments}.

    % By applying personality traits from the Big Five model \citep{roccas_big_2002}---openness, conscientiousness, extraversion, agreeableness, and neuroticism---we can explore how these traits influence LLMs' language generation, making their output more relatable and human-like \citep{gu_effectiveness_2023, de_paoli_improved_2023}. 
    
    \paragraph{Contributions and Key Findings}
        \begin{itemize}
            \item We are the first to investigate whether AI (LLMs) can model System 1 and System 2 as separate, distinct reasoning processes, specifically in terms of their ability to capture the \textit{full distribution} of human responses.
            \item We introduce a six-way division of the natural language inference problem that allows for a more granular analysis of response distribution compared to existing research.
            % \item We explore the use of AI-generated questions to address the data contamination problem.
            \item We present a pioneering approach to the full reasoning spectrum problem, demonstrating the necessity of capturing the full range of correct and incorrect human responses to advance the fidelity of AI in cognitive modeling.
            \item Our study establishes a novel metric for assessing AI models' capability to encompass the entire spectrum of human reasoning, enhancing the evaluation of AI's mimicry of diverse cognitive processes from intuitive to analytical reasoning.
            \item Though Big Five personality traits have been used for personality detection with LLMs \citep{kazemeini_interpretable_2021, pellert_ai_2024, wen_affective-_2024}, we are the first to use the Big Five traits for personality prompting for NLI.
            \item We find that personality-based prompting significantly enhances LLMs' ability to mimic human response distributions compared to vanilla zero-shot prompting.
            \item The open-source Llama and Mistral models outperform closed-source GPT architectures, supporting the argument that current open-source LLMs are at least as effective as (and possibly better than) closed-source LLMs in capturing naturalistic human reasoning \citep{ahmed_studying_2024}.
        \end{itemize}