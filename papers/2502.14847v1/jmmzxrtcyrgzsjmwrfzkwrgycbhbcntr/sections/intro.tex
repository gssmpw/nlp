% \yue{
% \begin{itemize}
%     \item Wording: DoS or DOS
% \end{itemize}
% }

\section{Introduction}

% \jt{we may check how we conduct tradtional communication attack in distribution system to give a few examples to illlustrate our proposed attack is possible in reality}


Large Language Models (LLMs) excel in text generation, reasoning, and planning \citep{zhao2023survey,wei2022chain,song2023llm, brown2020language}. To fully harness these capabilities for tackling complex tasks, LLM-based Multi-Agent Systems (LLM-MAS) have been developed. These systems consist of specialized agents that collaborate by dividing complex tasks into smaller, manageable subtasks or engaging in debates to collectively solve problems that exceed the capacity of a single LLM.
% divide the complex tasks into small subtasks, \jt{only parallel process and efficiency?? I think the key advantage seems not these two? }enabling parallel processing and improving efficiency 
\citep{guo2024large, wu2023autogen, talebirad2023multi}. LLM-MAS has shown success in various domains like software development \citep{liu2024large, hong2023metagpt, qian2024chatdev}, embodied agents \citep{guo2024embodied, song2023llm}, and scientific research \citep{zheng2023chatgpt, tang2023medagents}.
\begin{figure}[t]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/attacks.png}
    \caption{Attacks on LLM-based Multi-agent system.}
    \label{fig:attacks}
\end{figure}

% \jt{we should put communitication staff in this paragraph.} \jt{can we discuss  why communicate is essential??} \jt{please rewrite the following?} 
Communication plays a critical role in LLM-MAS.
% , as the collaboration among agents requires message exchanging. 
% Besides, the agents in the LLM-MAS can communicate with each other,  which can alleviate the impact of errors from individual agents. Compared to dividing tasks, communication in LLM-MAS is a more attracting feature which provides a great flexibility to the system. 
Through communications, agents are able to share information, coordinate actions, and solve tasks collaboratively \citep{qian2024scaling}. Methods such as debates \citep{du2023improving}, majority voting\citep{zhao2024electoral}, and task-specific dialogues \citep{hong2023metagpt} help validate decisions and minimize errors. Communication structures are often tailored to applications: MetaGPT \citep{hong2023metagpt} uses a linear structure for task decomposition, while ChatDev \citep{qian2024chatdev} combines linear phase connections with intra-phase debates for deeper collaboration. 
A well-designed communication framework ensures smooth coordination and enhances the performance of LLM-MAS.


While communication is vital for LLM-MAS, it also introduces significant risks since malicious information or knowledge could spread across agents, amplifying harmful effects throughout the system \citep{yu2024netsafe, huang2024resilience, ju2024flooding}. 
% For example, in a medical system, an adversary could inject false patient data or incorrect medical guidelines into one agent. This misinformation could spread to other agents, leading to misdiagnoses or harmful treatment recommendations. 
Meanwhile, excessive or redundant communications can increase token overhead and computation costs, raising scalability challenges \citep{zhang2024breaking}. These risks underscore the importance of identifying and mitigating potential vulnerabilities in the communication of LLM-MAS. 
% \jt{can we have a figure to illustrate how our focus is different from existing ones? one more thing is we need to justify why our focus is important?}\pf{Sure}

There are recent investigations on potential threats to LLM-MAS communications. Their primary focuses are on the vulnerability of individual agents, rather than the communicating messages, as shown in Figure \ref{fig:attacks}. For example, 
% \jt{can you connect the following examples to the figure}
\citet{yu2024netsafe, huang2024resilience, ju2024flooding} attempt to transform a benign agent in the system into a malicious one (Figure \ref{fig:attacks} (a)); and \citet{yu2024netsafe, huang2024resilience} mainly investigate the vulnerabilities when adversarial inputs are processed by the agents (Figure \ref{fig:attacks} (b)). However, the vulnerability of the \textbf{communication mechanisms} in LLM-MAS remains largely underexplored. Specifically, the threat of an adversary intercepting inter-agent messages—monitoring and analyzing them—and then manipulating the communication to achieve malicious objectives remains insufficiently studied. For example, in a decentralized system \citep{yang2024multi, guo2024large} where the agents can be deployed on different servers and for different purposes, and the communication among agents relies on transmitting networks that are vulnerable to eavesdropping \citep{belapurkar2009distributed}. This new attack surface targets the communication scheme itself, which is the backbone of agent's collaboration, exposing critical weakness in communication and underscoring its far-reaching implications for the overall security and robustness of LLM-MAS.
% \jt{we do not need to ask the question, we can say: to bridge the gap, we introduce a new type of attacks, we should discuss how it is different from existing ones, what are the unique challenges and how do we solve these challenges?}

% In this work, we aim to investigate this critical question: \textit{How vulnerable is LLM-MAS to attacks that intercept the communication within them?} 

% \jt{can we connect to the figure about our attack} 
To explore this potential vulnerability, we propose a new communication attack, \textbf{Agent-in-the-Middle} (AiTM) attack (Figure \ref{fig:attacks} (c)), which aims to intercept inter-agent communications to induce malicious behaviors in LLM-MAS. Unlike existing works that assume the attacker can directly modify agents in the system, AiTM targets the messages among agents, and evaluates if an LLM-MAS is vulnerable to communication interception and manipulation. 
% \pf{The following part is key point in the threat model and we refer to detailed threat model}
Under AiTM, the components of the LLM-MAS are not changed, including agents' profiles and capabilities, but the attacker is allowed to monitor and manipulate the messages received by the particular victim agent (more details in Section \ref{sec:threat model}) to indirectly influence the system's output.
% To specifically explore the vulnerabilities in communication, we assume the attacker cannot modify any components of the LLM-MAS, including the agents' profiles, nor directly inject information to disrupt the normal communication structure. \yue{After reading the above sentence, I feel ``oh it seems we can barely do nothing on the agents, then how we do the attack?" Please state what we can change before the next sentence.} 


However, designing such an effective communication attack presents unique challenges in practice. 
% \jt{are these challenges unique to our proposed attack? In the previous paragraph, we need to intro the key settings for the threat model, otherwise, it may be hard to understand these challenges?? Another thing is how these challenges related to the communication? it seems that we also target on the agent??}\pf{I revise the following. We emphasize we target messages in the system. The victim agent is to help locate what messages can be manipulated.} 
First, unlike the malicious agent attack, the attacker can only intercept and manipulate messages received by a specific victim agent, without direct control over the victim agent and other components in the system.
% take the control of the input to the target victim agent, there is no additional control in other components in the system. 
% As a result, all the other agents can only be indirectly impacted by the attack. 
As a result, the attack must rely on indirect influence through message manipulation to affect the system's behavior.
Second, since agents are restricted by their predefined roles and capabilities, both the form and content of malicious information are inherently limited, which further reduces the effectiveness of such attacks. For example, in a software development system, if an agent is designed solely to analyze user requirements, it cannot inject malicious code into the final product. 

To address these challenges, the AiTM attack employs an external LLM-based adversarial agent to intercept messages intended for a victim agent within the system. The adversarial agent leverages a reflection mechanism \citep{Yang2023LargeLM} to enhance the effectiveness of its attack. By analyzing intercepted messages and precious instructions, it generates contextually tailored instructions designed to induce the victim agent into producing malicious responses that influence other agents, thereby advancing the adversary's objectives. 
For instance, assume the victim agent is participating in a debate with another agent, the adversarial agent can continuously assess the conversation's dynamics and adapt its instructions to direct the debate's outcome toward the malicious output. 

We conduct extensive experiments across various multi-agent frameworks, communication structures, and attack goals. AiTM consistently achieves a high attack success rate, exceeding 40\% in all cases and surpassing 70\% in most experiments. These results reveal significant vulnerabilities in the communication mechanisms of LLM-MAS. Furthermore, applying AiTM to real-world applications like MetaGPT and ChatDev demonstrates its ability to compromise their performance, underscoring the critical threat posed by this attack. 

% \jt{update the following?}
% (experiments and summarize the findings. Fill in later)
% We first test AiTM on various multi-agent frameworks and different communication structures to comprehensively evaluate the vulnerability of LLM-MAS against AiTM. We observe that xxx. Then we dig deeper and evaluate the influence of some critical factors such as the persuasiveness of the adversary agent and the position of the victim agent in the system. Based on our findings, xxx. Finally, we test AiTM on two popular real-world LLM-MAS applications, MetaGPT and ChatDev, to find out if these high-stake applications are also vulnerable to AiTM. We find that xxx. 