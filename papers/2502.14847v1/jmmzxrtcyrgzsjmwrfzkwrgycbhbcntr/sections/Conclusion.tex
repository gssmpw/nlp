% \section{Discussion}

% \jt{you can use this section to summarize our findings and I think it should be fine to discuss potential mitigation strategies}

% \pf{I use this section to discuss potential mitigations on AiTM. Since there is no defense proposed, we just discuss.} \yue{I would think of just mentioning on or two thoughts in the conclusion section. Otherwise the reviewers may ask you to implement the defense to evaluate the robustness of AiTM.}\pf{will move to limitation section, and only mention two points}

% While AiTM successfully compromises LLM-MAS via disturbing inter-agent messages, we discuss some potential mitigation to it \footnote{Since there is no defense proposed to protect communications, we briefly discuss potential ones.}. First, we note that AiTM is much stealthier than malicious agents and adversarial inputs. The former changes the LLM-MAS itself and is easy to detect when the defender checks the profiles of the agents in the system \citep{zhang2024psysafe}. The latter can be filtered by input filters \citep{zhang2024breaking, zhang2024psysafe}. However, these defenses can hardly work on AiTM as AiTM does not change the LLM-MAS or input to the system. Second, an external monitor monitoring every inter-agent message may detect malicious messages from AiTM, but it can lead to additional computation costs and influence the utility of the system if normal communication is blocked. Third, similar to ChatDev, strictly defining the communication format and content may effectively reduce risk from AiTM. However, this is not flexible and not applicable to free debate systems needed in domains like scientific research \citep{xiong2023examining} and social science \citep{park2023generative}. Fourth, inspired by our experiments of communication structures, we believe more robust structures are needed when designing the multi-agent system, such as including more bi-directional discussions among agents. In summary, our study on AiTM calls for more robust LLM-MAS.


\section{Conclusion}
In this work, we introduce a novel attack, Agent-in-the-Middle (AiTM), which directly targets inter-agent communication in LLM-based multi-agent systems (LLM-MAS). AiTM demonstrates the ability to compromise the entire system by manipulating messages exchanged between agents. This attack exposes a critical vulnerability in the fundamental communication mechanisms of LLM-MAS and highlights the urgent need for securing inter-agent communication and developing more robust and resilient multi-agent systems.


\section*{Limitations}

There are some limitations in this study. First, we use black-box models for all experiments. The primary goal of this work is to reveal risks of LLM-MAS communication, so GPT models can already provide illustrations. We also adopt different GPT models to test the generalization ability of AiTM and influence of models in the agent. Second, there are many communication structures that can not be fully covered in this work. We select 4 representative structures and 2 real-world applications to show the generalizability of AiTM. A deeper study could be made to identify key features and categorize different structures, but it is out of scope and we leave it for future exploration\footnote{Code will be released soon.}.
