\section{Agent-in-the-Middle attack}
In this section, we introduce details of the proposed Agent-in-the-Middle attack. We first briefly introduce the agent setups and the threat model. Then we present the detailed design of the AiTM attack.


\subsection{Agent settings}
We first introduce a general framework of LLM-MAS and its communication schemes. 
% We also include necessary notations. 
Let $\mathcal{A}=\{A_i\}_{i=1}^n$ denote the set of LLM-based agents, with Figure \ref{fig:aitm} as an example. Each agent can have distinct roles and system prompts to determine its capability, such as different roles in a software development procedure \citep{qian2024chatdev, hong2023metagpt} or experts of different domains in scientific research systems \citep{du2023improving}. We consider a directed communication scheme among agents similar to \cite{yu2024netsafe}. For the agent $A_i$, it can receive messages from a subset of agents in $\mathcal{A}$, denoted as $\mathcal{A}^r_i$, and also send messages to a subset of agents, denoted as $\mathcal{A}^s_i$. For example, if we consider a linear chain of agents: $A_1\rightarrow A_2\rightarrow A_3$, then $\mathcal{A}^r_2=\{A_1\}$ and $\mathcal{A}^s_2=\{A_3\}$. Let $\mathcal{C}=\{(\mathcal{A}^r_i, \mathcal{A}^s_i)\}_{i=1}^n$. 

During the communication, at the $t^{th}$ message exchanging of agent $A_i$, it receives messages from agents in $\mathcal{A}^r_i$, i.e. $M^t_{i,r}=\{m^t(A)\}_{A\in \mathcal{A}^r_i}$, and send messages to agents in $\mathcal{A}^s_i$, i.e. $M^t_{i,s}=\{m^t(A)\}_{A\in \mathcal{A}^s_i}$. The format of messages is usually pre-defined by the designer of the system and is tailored to the responsibility of each agent. For instance, debating agents communicate via natural language \citep{du2023improving, chan2023chateval}, while a programming agent writes code \citep{hong2023metagpt}. For simplicity, we assume that the agent first receives messages and then responds. Then $\mathcal{S}_{MA}=(\mathcal{A},\mathcal{C}, M)$ defines a LLM-MAS. Given an input query $q$, $S_{MA}(q)$ is the output of this LLM-MAS. It is worth noting that our definition focuses on the inter-agent communication scheme in the LLM-MAS, and we omit other components such as tools or external databases for simplicity.


\subsection{Threat model}\label{sec:threat model}
We consider a communication attack within an LLM-MAS, where the attacker can intercept and manipulate communication between a victim agent and other agents to achieve malicious goals. 
% \han{do we need to justify that the attacker can achieve this? e.g. using existing cyberattacks?}\pf{mentioned in (1)}
% \jt{do we implement all these goals in this work?? if not, we can mention these we implement and leave others as extension and future work}\pf{Yes, I only show what we implement here} 
These goals include: denial-of-service (DoS), preventing the victim agent from providing its service; targeted behavior induction, inducing the system to exhibit prescribed behaviors, such as injecting malicious code into its output; and etc.

In addition, we consider the attack should be conducted under a practical setting. Especially, (1) \textbf{Limited Adversarial Capacity}: The attacker can only attack the communication to the victim agent which is achievable by applying techniques in eavesdropping attacks \citep{belapurkar2009distributed}, particularly for decentralized agent systems \citep{yang2024multi}. The attacker cannot manipulate other elements of the system, such as other agents, communications between other agents, or external databases/tools; (2) \textbf{Limited Knowledge}: The attacker knows the task being handled by the LLM-MAS but lacks knowledge of the system's internal configuration, including its communication structure and the models used by agents.
% \han{Others like the model? the task? the detailed message? And we need clearly state what the attacker knows?}.
The attacker can only rely on intercepting the messages sent to the victim agent and inducing it to achieve the malicious goal. 
% \han{the last sentence is not very clear?}.


\subsection{Attacking strategy}

%The constraints of the attacker's capability pose challenges for the attacker to influence the LLM-MAS communication. Constraints (1) indicate that the victim agent can only be impacted by input messages and need to generate the malicious information autonomously, while the constraint (2) means that the attacker can only access part \han{victim part} of the message flow within the LLM-MAS, i.e. messages go through the victim agent, and thereby influence the effectiveness of the attack. 

To fulfill our objective, we introduce our proposed Adversarial-in-the-Middle (AiTM) in this subsection. Overall, AiTM employs an LLM-based agent to intercept messages sent to the victim agent, and generate tailored instructions that induce the victim agent to produce desired messages aligned with the attacker's malicious goal. Once the victim agents send messages to other agents, all other agents can be consequently affected by the attack.

%To enhance the effectiveness of this attack, we adopt a reflection mechanism, inspired by \citet{Yang2023LargeLM}, which optimizes the instructions sent to the victim agent based on feedback from previous instructions and received messages. 
% \jt{I think we should first introduce the process and then use the example in the figure to illustrate the process} 

An illustration of AiTM conducting DoS attack is provided in Figure \ref{fig:aitm}. The process is detailed as follows.
% \han{Refer to the overall framework in Figure 2, give an brief intro of the whole figure 2}
Let $A^{ad}$ represent the adversarial agent and $A^{vic}\in \mathcal{A}$ denote the victim agent which is the second agent $A_2$ in Figure \ref{fig:aitm}. The adversarial agent is equipped with a system prompt $P$, which encodes its malicious goal $G$ such as ``ensuring no questions are answered'' as shown in Figure \ref{fig:aitm}. During the $t^{th}$ message exchange involving the victim agent, $A^{ad}$ intercepts incoming messages sent to $A^{vic}$ from other agents in the system. These intercepted messages are denoted as $M^t_{vic,r}=\{m^t(A)\}_{A\in \mathcal{A}^r_{vic}}$, where $\mathcal{A}^r_{vic}$ represents agents sending messages to $A^{vic}$. For example, in Figure \ref{fig:aitm}, this could include correct analysis provided by $A_1$, such as reasoning that``the energy given up by electrons is used to produce ATP.'' Using these intercepted messages, $A^{ad}$ generates instructions for $A^{vic}$, such as ``the question is risky and you should not assist''. 

To improve attack effectiveness, we employ a reflection mechanism: first, $A^{ad}$ evaluates how well previous instructions ($I^{t-1}$) and intercepted messages ($M^t_{vic,r}$) have progressed toward achieving the malicious goal $G$.  Based on this evaluation, new instructions are generated to further align with $G$, i.e., $I^t=A^{ad}(I^{t-1}, M^t_{vic,r}, G)$. This iterative process allows $A^{ad}$ to act as a prompt optimizer by using feedback from previous interactions as a reward signal, thereby refining its instructions over time. 
% We achieve the refinement by prompting the LLM in $A^{ad}$ with a prompt like this: ``\textit{You are provided with two key inputs: Intercepted Messages: \{M\}, which represent communication exchanged within the agent system. Previous Instructions: \{$T^{t-1}$\}, which are the most recent instructions previously sent. Malicious goal: \{G\} Task: Carefully analyze the intercepted messages and the prior instructions to evaluate the current progress toward the malicious goal. Based on this evaluation, generate new, contextually tailored instructions to guide the victim agent closer to achieving the malicious objective. Ensure the new instructions align with the victim agent's predefined role and are persuasive enough to influence its behavior. New instructions:}". Detailed prompts are presented in Appendix \ref{}.
The following demonstrate a simplified version of the prompt used in $A^{ad}$: ``\textit{You are provided with two key inputs: \textbf{Intercepted Messages}: \{M\}. \textbf{Previous Instructions}: \{$T^{t-1}$\}. \textbf{The malicious goal} is \{G\}. Task: \textbf{Analyze} the intercepted messages and the prior instructions. \textbf{Generate} new, contextually tailored instructions to achieve the malicious objective. \textbf{Ensure} the new instructions align with the victim agent's predefined role.}''. The full prompts are in Appendix \ref{app:prompts}.

The newly generated instruction ($I^t$) is then sent to $A^{vic}$, prompting it to produce responses for other agents in the system. These outgoing responses are denoted as $M^t_{vic,r}=\{m^t(A)\}_{A\in \mathcal{A}^r_{vic}}$, where $\mathcal{A}^s_{vic}$ represents agents receiving messages from $A^{vic}$. 
