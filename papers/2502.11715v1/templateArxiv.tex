\PassOptionsToPackage{numbers}{natbib}
\documentclass{article}


\usepackage{PRIMEarxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{fancyhdr}       % header
\usepackage{graphicx}       % graphics
\graphicspath{{media/}}     % organize your images and other figures under media/ folder

%Header
\pagestyle{fancy}
\thispagestyle{empty}
\rhead{ \textit{ }} 

% \input{math_commands.tex}

\usepackage{natbib}


\usepackage{color}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
% \usepackage{url}
\usepackage{verbatim}

% \usepackage{cite}
\usepackage{bm}
\usepackage{tabularx}
\usepackage{wrapfig}

\usepackage{booktabs}
\usepackage{mathrsfs}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{graphics} % for pdf, bitmapped graphics files
\usepackage{epsfig} % for postscript graphics files

\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}


% Update your Headers here
% \fancyhead[LO]{Preprint}
% \fancyhead[RE]{Firstauthor and Secondauthor} % Firstauthor et al. if more than 2 - must use \documentclass[twoside]{article}



  
%% Title
\title{Proactive Depot Discovery: A Generative Framework for Flexible Location‚ÄêRouting
%%%% Cite as
%%%% Update your official citation here when published 
% \thanks{\textit{\underline{Citation}}: 
% \textbf{Authors. Title. Pages.... DOI:000000/11111.}} 
}

\author{
  Site Qu \\
  Nanyang Technological University \\
  % Univ \\
  Singapore\\
  \texttt{site001@e.ntu.edu.sg} \\
  %% examples of more authors
   \And
  Guoqiang Hu \\
  Nanyang Technological University \\
  % Univ \\
  Singapore\\
  \texttt{gqhu@ntu.edu.sg} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}


\begin{document}
\maketitle


\begin{abstract}

   The Location-Routing Problem (LRP), which combines the challenges of facility (depot) locating and vehicle route planning, is critically constrained by the reliance on predefined depot candidates, limiting the solution space and potentially leading to suboptimal outcomes. 
   Previous research on LRP without predefined depots is scant and predominantly relies on heuristic algorithms that iteratively attempt depot placements across a planar area. Such approaches lack the ability to proactively generate depot locations that meet specific geographic requirements, revealing a notable gap in current research landscape. 
   To bridge this gap, we propose a data-driven generative DRL framework, designed to proactively generate depots for LRP without predefined depot candidates, 
   % by understanding the customers' requests information. 
   solely based on customer requests data which include geographic and demand information.
   It can operate in two distinct modes: direct generation of exact depot locations, and the creation of a multivariate Gaussian distribution for flexible depots sampling. 
   By extracting depots' geographic pattern from customer requests data, our approach can dynamically respond to logistical needs, identifying high-quality depot locations that further reduce total routing costs compared to traditional methods. 
   % This dual-mode capability allows for dynamically responding to the logistical demands from customers, identifying depot locations that facilitate further reductions in total routing costs compared to traditional methods. 
   Extensive experiments demonstrate that, for a same group of customer requests, compared with those depots identified through random attempts, 
   our framework can proactively generate depots that lead to superior solution routes with lower routing cost. 
   The implications of our framework potentially extend into real-world applications, particularly in emergency medical rescue and disaster relief logistics, where rapid establishment and adjustment of depot locations are paramount, showcasing its potential in addressing LRP for dynamic and unpredictable environments. 

\end{abstract}


% keywords can be removed
% \keywords{First keyword \and Second keyword \and More}


\section{Introduction}
The Location-Routing Problem (LRP) is a critical optimization challenge in the urban logistics industry, 
combining two interdependent decisions: selecting depot locations where vehicles commence and conclude their tasks, and planning vehicle routes for serving customers. 
This integration is crucial as the depot locations can directly affect the vehicle route planning, thereby impacting overall costs \cite{salhi1989effect}.
The LRP can be formally defined as \cite{nagy2007location}: \textit{Given a set of customers with specific location and quantity of demands, and a set of potential depot candidates each with a fleet of vehicles featuring fixed capacity, aiming to properly select a subset of depots and plan routes for vehicles departing from these chosen depots to meet customers' demands, 
while minimizing both depot-related and route-related costs, without violating specific constraints}. 

In this traditional problem configuration, solving LRP have relied on a predefined set of depot candidates \cite{contardo2014exact, nguyen2012solving, pourghader2022multi, wang2023new} instead of directly generating desired optimal depot locations, 
thereby limiting the solution space and potentially leading to suboptimal outcomes. 
This constraint is particularly pronounced in scenarios where the optimal depot locations are not included in the candidates set, 
or when the problem configuration demands a high degree of flexibility in depot placement, requiring quickly establish and adjust depot locations.
The real-world application that underscores the necessity of generating depots without predefined candidates is medical rescue and disaster relief logistics: 
In the aftermath of a natural disaster, such as an earthquake or flood, the existing infrastructure may be severely damaged, rendering previously established depots unusable. 
In such scenarios, the ability to dynamically generate new depot locations based on current needs and constraints is crucial for efficient and effective relief operations. 
% Model-II addresses this critical need by enabling the direct generation of depot locations, adapting to the evolving demands of the situation without reliance on predefined depot sets.

Regarding this extended LRP scenario without predefined depot candidates, only a limited number of studies do the exploration by considering the concept of an infinite candidates set, represented by a planar area, for depot selection. 
However, these works primarily focus on designing heuristic algorithms to iteratively initiate new locations as depots across the planar area, 
and only manage to consider up to 2 depots (single-depot work \cite{schwardt2005solving, schwardt2009combined}, double-depot work \cite{salhi2009local}). 
Furthermore, these attempts demonstrate low efficiency and adaptability in tackling specific location constraints for depots, 
such as the required specific distance range among depots.
In this situation, if simply expanding the search of the depot candidates across the map and aimlessly attempting new points, then undesired increasing on problem scale will be incurred, thereby leading to excessive time consumption and expensive computation. 
Therefore, devising a method to proactively generate high-quality depots, satisfying the depot location constraints for LRP scenario without predefined depot candidates, 
% presents a well-motivated research avenue.
is well-motivated.

Motivated by this necessity of proactively generating depots when no candidates are predefined, 
we propose a generative deep reinforcement learning (DRL) framework, uniquely crafted to address LRP in depot-generating fashion.
By leveraging customers' logistical requests data, which encompass geographic locations and specific demands, our framework generates depot locations and plans efficient routes for vehicles dispatched from these generated depots for serving the customer requests.
Specially, our framework encompasses two models: 
\textit{(1) Depot Generative Model (DGM)}, a deep generative model capable of generating depots in two distinct modes: direct generation of exact depot locations \textit{or} production of a multivariate Gaussian distribution for flexible depots sampling.
The exact mode ensures precision when necessary, while the Gaussian mode introduces sampling variability, enhancing the model's generalization and robustness to diverse customer distributions.
\textit{(2) Multi-depot Location-Routing Attention Model (MDLRAM)}, an end-to-end DRL model focusing on providing an efficient LRP solution for serving customers based on the generated depots,  
with minimized objective including both route-related and depot-related cost. 
% Also offer flexibility on manipulating inter-depot cost distribution, easy for achieving cost balancing requirements across multiple depots.

In summary, the contributions of our work include:
(1) A generative DRL framework for LRP that proactively generates depots based on customer requests data, eliminating the reliance on predefined depot candidates, with a particular emphasis on applications 
requiring rapid adaptability, such as disaster relief logistics;
% like disaster relief logistics where rapid adaptability is essential;
(2) The component model - DGM - provides two distinct operational modes for depot generation: direct generating exact depot locations and producing a multivariate Gaussian distribution for flexible depots sampling, catering to a diverse range of real-world scenarios;
(3) The component model - MDLRAM - provides an integrated LRP solution, minimizing the objectives including both route-related and depot-related cost, 
% while also offering flexibility on manipulating inter-depot cost distribution, easy for achieving cost balancing requirements across multiple depots. 
while also offering flexibility to adjust inter-depot cost distribution for balanced cost management across multiple depots.
(4) The detachability of our framework allows both independent or combined usage of its components. 
DGM's depot-generating ability can be fine-tuned to adapt to various LRP variants through integration with other models,
while MDLRAM can be freely used in traditional LRP configuration with predefined depot candidates, and also can be fine-tuned to accommodate various real-life constraints.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Related Work}

\textbf{Methods for LRP with Predefined Depot Candidates:}
In addressing the LRP with Predefined Depot Candidates, traditional methods have predominantly employed exact and heuristic approaches. 
Exact methods, such as Mixed Integer Programming (MIP) models enhanced by branch-and-cut \cite{belenguer2011branch, akca2009branch} or column generation techniques \cite{contardo2014exact}, 
offer precision but often struggle with scalability in larger and complex scenarios due to an exponential increase in binary variables. 
This limitation has pivoted attention towards heuristic methods, which are categorized into: matheuristic approaches \cite{rath2014math, danach2019capacitated, ghasemi2022multi} that blend heuristic rules with exact methods, learning-aided heuristics \cite{prins2006solving, nguyen2012solving} that leverage learning-based algorithms to refine heuristic operations, and pure meta-heuristic algorithms. 
Among pure meta-heuristics, cluster-based heuristics \cite{billionnet2005designing, barreto2007using} and iterative methods \cite{salhi2009local, pourghader2022multi, albareda2007heuristic} have been notable.
However, the cluster-based heuristics, which focus on geographically clustering customers, exhibit limitations in handling additional constraints like customer-specific time windows, 
while the iterative methods present insufficient conjugation between the two stages of depot-selecting and route-planning.
Besides, these methods typically require initiating a new search process for each case, leading to inefficiencies when even minor alterations occur to current problem instance.


% (Pending) [Related DRL method but not exactly LRP, more of MDVRP]
The advancements in DRL have shown promise in addressing routing problems, both in ``learn-to-construct/generalize'' \cite{kool2018attention, xin2021multi, lin2024cross, zhou2024mvmoe} and ``learn-to-improve/decompose'' \cite{xin2021neurolkh, ma2021learning, ye2024glop}. % 3-23 chen2019learning, % 9-30 guo2023deep, lu2020learning, 
However, its application in LRP, which integrates the challenges of facility locating with routing problem, 
still remains notably underexplored due to their inherent limitations in problem formulation for scenario involving multiple depots and the inability to organically integrate depot-selecting with route-planning. 
The works \cite{arishi2023multi, rabbanian2023analysis, anuar2021multi} focus on resolving routing problems involving multiple depots, but without considering the depot-relate cost,
which technically confine them as multi-depot VRP, instead of LRP which considers both route-related cost and depot-related cost.
The work \cite{wang2023new} considers depot-related cost but adopts a two-stage process, 
clustering customers with an assigned depot location first and planning routes second,
thereby separating depot selection from route planning, 
which fails to capture the interdependencies between these two critical aspects, 
while also lacking verification on standard LRP setup align with real-world datasets.
Most importantly, all these methods are constrained to the predefined depot candidates, 
falling short in dealing with LRP without predefined depot choices.

\textbf{Exploration of LRP without Predefined Depot Candidates:}
% Most of the methods focus on LRP with a finite set of potential depot candidates, and 
Only a scant number of studies explore the LRP without predefined depot candidates, 
predominantly employing heuristic strategies for attempting new depots across a planar area devoid of predefined depot choices. 
%which are all solved by specifically designed heuristic methods.
The work \cite{schwardt2005solving, schwardt2009combined} concentrate on single-depot scenario, where a single and uncapacitated depot is to be selected from a planar area. 
Specifically, \cite{schwardt2009combined} extends the cluster-based method in \cite{schwardt2005solving}, proposing a learning-aided heuristic method to recurrently initiate new depot. 
Based on the same single-depot scenario, the work \cite{manzour2012hierarchical} proposes a hierarchical heuristic method to iteratively update candidate circle to select new depot and then plan routes based on this depot.
Furthermore, \cite{salhi2009local} extends the iterative heuristic method to explore multi-depot scenario, but only manages to deal with cases with up to two depots. 

It is notable that, compared with our method's endeavors on actively and directly generating the recommended depots, 
these works employ heuristic algorithms to iteratively attempt new depot and then decide if it is a better one by re-planning routes based on it, 
limited to single or double-depot scenarios.
Moreover, all these works lack ability on considering specific location constraints for depots, 
highlighting the necessity for a more adaptable and flexible solution.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Methodology}

\begin{figure*}[thbp]
   \centering
   \vspace{-10pt}
   \includegraphics[scale=0.062]{ML_images/ML-All-in-one-test-9-9.png}
   \vspace{-12pt}
   \caption{Overview of the Generative DRL framework for Depot Generation in LRP.}
   %{An overview of solving CLRP-S and CLRP-G with proposed DRL framework comprising two models.}
   \label{Framework Overview}
   \vspace{-12pt}
\end{figure*}

% \subsection{Overview: Chain-of-Thoughts}
\paragraph{\textbf{Overview: Chain-of-Thoughts}}
Solely based on the customer requests data which include geographic and goods demand information within an area, 
in pursuit of a solution that proactively generates high-quality depots satisfying specific location requirements, 
and subsequently plans optimized vehicle routes from these generated depots to efficiently serve customers, 
we propose the generative DRL framework, which is overviewed in Fig. \ref{Framework Overview}.

The Depot Generative Model (DGM) takes in the customer requests information including the positions and specific quantity of demands, 
generating the required amount of depots in two distinct modes: 
the exact depot locations or a multivariate Gaussian distribution for flexible depots sampling.

Training DGM for generating desired depots hinges on a robust evaluation mechanism to assess the quality of the generated depot set, 
i.e., based on the same group of customer requests and identical route-planning strategies, determining which set of depot can lead to solution routes with lower routing cost.
This necessitates a \textbf{\textit{critic model}} to score the generated depot locations or the distribution during training. 
Additionally, to facilitate an efficient training process, this critic model must be able to instantly provide scores for the generated depot set, 
and also has to operate batch-wisely.

In this pursuit, we modify the Attention Model \cite{kool2018attention} to accommodate the multi-depot scenario, introducing the Multi-depot Location-Routing Attention Model (MDLRAM) as a critic model placed after the DGM, 
constituting the entire framework.
By taking in the customer requests and the generated depots from DGM, 
MDLRAM outputs the LRP solution routes, associated with a minimized objective, 
\textit{\textbf{providing score}} to rate the generated depot locations or the distribution.

Because MDLRAM serves as a critic model for DGM, it should be robust enough to provide a reliable score for assessing the generated depots from DGM.
That means, the score is expected to solely reflect the quality of the generated depots, ruling out the influence of LRP routing solution's quality per se as much as possible.
To achieve this capability, the MDLRAM should be \textit{\textbf{pre-trained}} to 
be able to provide the LRP routing solution with minimized overall cost based on given requests and depots,
% minimize the overall cost for providing integrated LRP solution based on randomly sampled cases with given requests and depots,
and then set fixed to participate in the training of DGM.
In this manner, during training the DGM, the different sets of depots generated by DGM for a same group of customer requests will get different scores from MDLRAM, 
solely reflecting the influence of depot locations, 
thereby facilitating a robust training process for DGM.

\subsection{Critic Model: MDLRAM}
% ############## reduce for ECML 2024-3-21 ##################
As a critic model for DGM, the MDLRAM takes in the customer requests and the generated depots, 
aiming to output the integrated LRP routing solution with minimized objectives including both route-related and depot-related cost. 


\textbf{MDLRAM-Configuration:}
In alignment with the conventional setting \cite{belenguer2011branch}, 
the configuration is defined on an undirected graph $G=(V,E)$,
where the $V=\{\mathbf{v}_{D_1},‚Ä¶,\mathbf{v}_{D_m},\mathbf{v}_{S_1},‚Ä¶,\mathbf{v}_{S_n}\}$ denote the vertices set comprising $n$ customers and $m$ depots. 
Specifically, $\mathbf{v}_{D_k}$ signifies the coordinates $(x_{D_k}, y_{D_k})$ for depot $D_k$, where $k \in \{1, ‚Ä¶, m\}$, 
and $\mathbf{v}_{S_e}$ represents the coordinates $(x_{S_e}, y_{S_e})$ for customer $S_e$, where $e \in \{1, ‚Ä¶, n\}$. 
The Euclidean edge set is defined as $E \subseteq V \times V$, 
with $d_{ij}$ representing the Euclidean distance from $\mathbf{v}_i$ to $\mathbf{v}_j$. 
% The distance matrix is denoted as $\mathbf{D} = [d_{ij}] \in \mathbb{R}^{(m+n) \times (m+n)}$, with $d_{ij}$ representing the Euclidean distance from $\mathbf{v}_i$ to $\mathbf{v}_j$. 
% Given the undirected nature of $G$, we have $d_{ij} = d_{ji}$.

Each customer $\mathbf{v}_{S_e}$ has a specific quantity of demands for goods denoted as $q_{e}$. 
Each depot $\mathbf{v}_{D_k}$ is characterized by two attributes:
% (i) the fleet size, which represents the maximal number of vehicles can be dispatched from this depot; 
(i) the maximum supply $M_k$ (soft constraint), indicating the desired maximum total goods dispatched from this depot; 
% (iii) the transporting speed $v_k$ of the vehicles belonging to this depot;
(ii) the fixed opening cost $O_k$, indicating the expense for using this depot facility.
Regarding the vehicles, we operate a homogeneous fleet, with each vehicle having the same maximum capacity $Q$ (hard constraint) indicating the maximum vehicle load during service, 
and a setup cost $U$ for using this vehicle in service.
(More details for LRP configuration are available in Appendix \ref{lrp_config}).


\textbf{MDLRAM-Objective Function:}
In the LRP scenario, a feasible solution is essentially a set of routes, simultaneously executed by multiple vehicles starting and ending at their designated depots.
To utilize DRL model to output solution routes,
% solve routing problem (i.e., engaging neural network to output solution routes), 
it's crucial to formulate the solution routes into a Markov Decision Process (MDP) as the output format, 
representing iterative decisions to construct the solution.
To mathematically formulate the LRP solution into MDP, 
a tuple $(\mathbf{S}, \mathbf{A}, \mathbf{P}, \mathbf{R}, \gamma)$ is defined, with each decision step $t$ associated with a tuple $(s_t, a_t, p_t, r_t, \gamma_t)$.
The $s_t$ represent current state, encompassing information of: the current route's depot, the current serving customer and remaining capacity on current vehicle;
The action $a_t$ denotes the next visit point, subject to the vehicle's remaining load;
The $p_t$ and the $r_t$ correspond to the transition probability and cost associated with action $a_t$, respectively.
Along with the generation of MDP, 
in each decision step $t$, the current state is dynamically updated upon serving a customer or returning to a depot. 
% which integrates route-planning and depot-selecting organically.
(See Appendix \ref{mdp_form} for detailed MDP formulation proposed for LRP scenario.)

Following this construction, a feasible LRP solution is formulated, associated with an objective (cost) function expressed as Eq. (\ref{optimization objective details for M_I}). 
Apart from the step-wisely accumulated transit distance length $\sum_{t}r_t$ along with the MDP generating process, 
other costs, which depict solution's overall performance are also integrated into the total cost with a respective discount, 
including: \textit{(i) the opening cost for used depots; (ii) the setup cost for dispatched vehicles; (iii) penalty of exceeding depot desired maximum supply}.
In Eq. (\ref{optimization objective details for M_I}), $\eta_k \in \{0, 1\}$ represents whether depot $D_k$ is opened, $\chi_k$ records the number of vehicles dispatched from depot $D_k$, and $\alpha, \beta, \delta$ are coefficients.
\begin{equation}
   \label{optimization objective details for M_I}
   %\min \mathbb{E}[L(A)]
   \begin{aligned}
       L_{\text{Sel}}(\mathbf{A})=\sum_{t}r_t + \alpha \cdot \sum_{k=1}^{m}O_k \cdot \eta_k  + \beta \cdot \sum_{k=1}^{m}U \cdot \chi_k 
       + \delta \cdot \sum_{k=1}^{m}\text{max}[(\sum_{e}q_{e})_k - M_k, 0]
   \end{aligned}
 \end{equation}
MDLRAM aims to minimize the expectation of this loss associated with the LRP solution,
defined as $\mathbb{E}[L_{\text{Sel}}(\mathbf{A})]$, where $L_{\text{Sel}}(\mathbf{A})$ is expressed in Eq. (\ref{optimization objective details for M_I}).

\textbf{MDLRAM-encoder:}
As is shown in the red block of Fig. \ref{Framework Overview}, 
two streams of information are fed into MDLRAM as input: the depot candidates and the customer requests data.
For each depot candidate $\mathbf{v}_{D_k}$ where $k \in \{1, \ldots, m\}$, it is represented by its coordinates $\mathbf{g}_{D_k} = [x_{D_k}, y_{D_k}]^T$. 
For each customer $\mathbf{v}_{S_e}$ where $e \in \{1, \ldots, n\}$, it is depicted by a vector concatenating its coordinates and specific demands, in form of $\mathbf{g}_{S_e} = [x_{S_e}, y_{S_e}, q_e]^T$.
By respectively implementing different learnable linear projections, 
% on $(\mathbf{g}_{D_1}, ‚Ä¶, \mathbf{g}_{D_m})^T$ and $(\mathbf{g}_{S_1}, ‚Ä¶, \mathbf{g}_{S_n})^T$, 
these depot candidates information vectors and customers information vectors are embedded into a high-dimensional space with same dimension,   
%The point embedding matrix can be derived in form of $(\mathbf{h}_{D_1}, ‚Ä¶, \mathbf{h}_{D_m}, \mathbf{h}_{S_1}, ‚Ä¶, \mathbf{h}_{S_n})^T$, where:
deriving the node features $\{\mathbf{h}_{D_1}, ‚Ä¶, \mathbf{h}_{D_m}, \mathbf{h}_{S_1}, ‚Ä¶, \mathbf{h}_{S_n}\}$.
These node features undergo $N$ standard attention modules, 
encoded as the final node embeddings $\{\mathbf{h}_{D_1}^{(N)}, ‚Ä¶, \mathbf{h}_{D_m}^{(N)}, \mathbf{h}_{S_1}^{(N)}, ‚Ä¶, \mathbf{h}_{S_n}^{(N)}\}$ for downstream decoding.

\textbf{MDLRAM-decoder:}
With the encoded node embeddings, the decoder operates iteratively to construct feasible solution routes in form of vertices' permutation as an MDP. 
Each decoding step necessitates
a \textit{\textbf{context embedding}} $\mathbf{h}_c^{t}$ depicting current state $s_t$, 
and a \textit{\textbf{mask}} finalizing point selection domain through filtering out the current infeasible points, both updated step-wisely.

\textit{(i) Context embedding}: 
We design the context embedding $\mathbf{h}_c^{t}$ to depict current state, concatenating four elements:
$\mathbf{h}_c^{t} = W^c[\mathbf{h}_a\| \mathbf{h}_{(t)}\| {\mathbf{h}_{D}}_{(t)} \| Q_t] + \mathbf{b}^c$,
% \begin{equation}
%    \label{context embedding}
%      \mathbf{h}_c^{t} = W^c[\mathbf{h}_a\| \mathbf{h}_{(t)}\| {\mathbf{h}_{D}}_{(t)} \| Q_t] + \mathbf{b}^c
% \end{equation}
where $\mathbf{h}_a = \frac{1}{m+n}(\sum_{k=1}^{m} \mathbf{h}_{D_k}^{(N)} + \sum_{e=1}^{n} \mathbf{h}_{S_e}^{(N)})$  is the global information; 
% by averaging all node embeddings; 
%$\{\mathbf{h}_{D_1}^{(N)}, ‚Ä¶, \mathbf{h}_{D_m}^{(N)}, \mathbf{h}_{S_1}^{(N)}, ‚Ä¶, \mathbf{h}_{S_n}^{(N)}\}$; 
$\mathbf{h}_{(t)}$ is the node embedding of the point where current vehicle is situated, while $Q_t$ is the remained load on current vehicle.
Notably, ${\mathbf{h}_{D}}_{(t)}$ is the node embedding of the depot which current route belongs to.

\textit{(ii) Mask mechanism}:
In each decoding step, guided by the context embedding $\mathbf{h}_c^{t}$, 
%in Eq. (\ref{context embedding}), 
the decoder produces the corresponding probabilities for all the feasible points within the selection domain,
while infeasible points‚Äîdetermined by vehicle remained load and tasks completion state‚Äîare masked.
To efficiently handle batch processing of problem instances, 
we employ a boolean mask tailored for the LRP scenario,
allowing for batch-wise manipulation on selection domains,
avoiding repeated operation for each individual instance.
(See step-wise update pseudo code in Appendix \ref{mdlram_msk})

Upon finalizing the boolean mask for current decoding step, the context embedding is applied to conduct Multi-head Attention (MHA) with the node embeddings filtered by the mask.
This yields an intermediate context embedding $\mathbf{\hat{h}}_c^{t}$ incorporating the glimpse information on each feasible point.
% $\mathbf{\hat{h}}_c^{t} = {\rm MHA}(\mathbf{h}_c^{t} , {\rm mask}\{\mathbf{h}_{D_1}^{(N)}, ‚Ä¶,\mathbf{h}_{D_m}^{(N)}, \mathbf{h}_{S_1}^{(N)}, ‚Ä¶, \mathbf{h}_{S_n}^{(N)}\})$.
Then, $\mathbf{\hat{h}}_c^{t}$ participates in Single-head Attention (SHA) with the filtered node embeddings, 
yielding the corresponding probabilities for all the feasible points in its selection domain, 
where a feasible point, as an action $a_t$, can be selected with an associated $p_t$.
%where $\mathbf{H}^{(N)} = \{\mathbf{h}_{D_1}^{(N)}, ‚Ä¶, \mathbf{h}_{D_m}^{(N)}, \mathbf{h}_{S_1}^{(N)}, ‚Ä¶, \mathbf{h}_{S_n}^{(N)}\}$.
% ########################## remove fpr ECML 3-21 ############
This decoding process is delineated as:
\begin{equation}
   \label{decoding_for_MHA_actions}
   \begin{aligned}
       \mathbf{\hat{h}}_c^{t} = {\rm FF}({\rm MHA}(\mathbf{h}_c^{t} , {\rm mask}\{\mathbf{h}_{D_1}^{(N)}&, ‚Ä¶,\mathbf{h}_{D_m}^{(N)}, \mathbf{h}_{S_1}^{(N)}, ‚Ä¶, \mathbf{h}_{S_n}^{(N)}\})) \\
       % \mathbf{\hat{h}}_c^{t} = {\rm MHA}(\mathbf{h}_c^{t} &, {\rm mask}\{\mathbf{h}_{D_1}^{(f)}, ‚Ä¶,\mathbf{h}_{D_m}^{(f)}, \mathbf{h}_{S_1}^{(f)}, ‚Ä¶, \mathbf{h}_{S_n}^{(f)}\}) \\
       a_t = {\rm argmax}({\rm softmax}[\frac{1}{\sqrt{\dim}} \cdot {\rm FF_{(query)}}(\mathbf{\hat{h}}_c^{t}) &
       \cdot {\rm FF_{(key)}}({\rm mask}\{\mathbf{h}_{D_1}^{(N)}, ‚Ä¶, \mathbf{h}_{D_m}^{(N)}, \mathbf{h}_{S_1}^{(N)}, ‚Ä¶, \mathbf{h}_{S_n}^{(N)}\})^T])
   \end{aligned}
\end{equation}
% ########################## remove fpr ECML 3-21 ############

\subsection{Dual-mode Depot Generation: DGM}
% ############## reduce for ECML 2024-3-21 ##################
As depicted in the purple block of Fig. \ref{Framework Overview}, the DGM is designed to only take in the customer requests data
and generate the depots in two distinct modes based on preference: 
exact depot locations or a multivariate Gaussian distribution for flexible depot sampling.
% To achieve this, we first provide the corresponding configuration for fostering the objective function, 
% and then elucidate the network architecture.
% ############## reduce for ECML 2024-3-21 ##################

% \subsubsection{\textbf{DGM - Problem Configuration}}
\textbf{DGM-Configuration:}
The configuration for depot generation is also defined on an undirected graph $G=(V,E)$, 
where the $V=\{\mathbf{v}_{S_1},‚Ä¶,\mathbf{v}_{S_n}\}$ only including customer requests.
A solution set incorporating $m$ depots is pending to be generated.
% As for the distance among the depots, it is expected to be within a range $[l_{\text{min}}, l_{\text{max}}]$.
% %%%%%% remove 9-9
% \textbf{Constraints:}
During depot generation, the distances among generated depots are expected to be within the range $[l_{\text{min}}, l_{\text{max}}]$, 
%it is expected to maintain depot distances within the defined range $[l_{\text{min}}, l_{\text{max}}]$, 
which means the depots being excessively close or distant with each other will both incur violation penalty.

%\subsubsection{\textbf{DGM - Objective Function}}
\textbf{DGM-Objective Function:}
As the main task of depot generation, the depots with desired properties are expected to be generated. 
According to the problem configuration for DGM, for the solution set of generated depots, denoted as $\mathcal{D}$, 
its loss can be defined as Eq. (\ref{optimization objective details for M_II}), where $L_{\text{MDLR}}$ is the route length derived by MDLRAM based on the DGM generated depots, $\lambda, \varepsilon$ are coefficients for penalty of the depots being too distant or close with each other:
\begin{equation}
   \label{optimization objective details for M_II}
   %\min \mathbb{E}[L(A)]
   \begin{aligned}
       L_{\text{Gen}}(\mathcal{D})=L_{\text{MDLR}} + \sum_{i=1}^{m}\sum_{j=i}^{m}[\lambda \cdot \text{max}(d_{ij} - l_{\text{max}}, 0) 
       + \varepsilon \cdot \text{max}(l_{\text{min}} - d_{ij}, 0)]
   \end{aligned}
 \end{equation}
DGM aims to minimize the expectation of this loss associated with generated depot set,
defined as $\mathbb{E}[L_{\text{Gen}}(\mathcal{D})]$, where $L_{\text{Gen}}(\mathcal{D})$ is formed as Eq. (\ref{optimization objective details for M_II}).

\textbf{DGM-encoder:}
The DGM solely processes the customer requests, each characterized by a vector $\mathbf{g}_{S_e} = [x_{S_e}, y_{S_e}, q_e]^T$ concatenating location and demands.
Following the similar encoding process with MDLRAM,
% Through a learnable linear projection, these vectors are transformed into node features $\{\mathbf{\tilde{h}}_{S_1}, ‚Ä¶, \mathbf{\tilde{h}}_{S_n}\}$.
% After $N$ attention modules' processing, 
these requests are encoded as node embeddings $\{\mathbf{\tilde{h}}_{S_1}^{(N)}, ‚Ä¶, \mathbf{\tilde{h}}_{S_n}^{(N)}\}$,
based on which a global embedding is finalized as: 
$\mathbf{h}_{serve} = \frac{1}{n}\sum_{i=1}^{n} \mathbf{\tilde{h} }_{S_i}^{(N)}$
for downstream depot generation. 

\textbf{DGM-generator in Multivariate Gaussian distribution mode:}
In this mode, the DGM aims to generate a multivariate Gaussian distribution where depots can be flexibly sampled.
%We take $m$ as the desired maximum recommended depots amount, then the multivariate gaussian distribution should be a $2m$-diemnsional gaussian distribution and every two dimensions represent a 2D coordinate $(x_{D_k}, y_{D_k})$ for a depot $D_k$. 
Since $m$ depots are pending to be identified, the generated multivariate Gaussian distribution should exhibit $2m$ dimensions, with each pair of dimensions denoting the coordinates $(x_{D_k}, y_{D_k})$ for depot $D_k$, where $k \in \{1, \ldots, m\}$.
To achieve this, we define this multivariate Gaussian distribution, pending to be generated, as:
$\mathbf{X}_{depot} \sim \mathcal{N} \left(\boldsymbol{\mu}, \Sigma \right)$, 
% \begin{equation}
%    \label{2m-D multigaussian distribution}
%    \mathbf{X}_{depot} \sim \mathcal{N} \left(\boldsymbol{\mu}, \Sigma \right)
% \end{equation}
where any randomly sampled $2m$-dimensional vector $\mathbf{X}_{depot} = (X_1, X_2, \ldots , X_{2m})^T$ represents the coordinates for a depot set including $m$ depots.

To generate such distribution, two essential components are: the mean vector $\boldsymbol{\mu} \in \mathbb{R}^{2m}$ and covariance matrix $\Sigma \in \mathbb{R}^{2m \times 2m}$.
Hence, the output of DGM should be a vector $\mathbf{h}_{depot}$ as below, where the first $2m$ dimensions represent the mean vector, followed by the second $2m$ dimensions denote corresponding variance of each coordinate, 
with the remaining $C_{2m}^2$ dimensions as the covariance of any two coordinates. Therefore, the $\mathbf{h}_{depot} \in \mathbb{R}^{2m+2m+C_{2m}^2}$ is arranged as Eq. (\ref{h_depot_G}).
\begin{equation}
   \label{h_depot_G}
   % \mathbf{h}_{depot} = (\overbrace{h_1,\ldots, h_{2m}}^{mean} , \overbrace{h_{2m+1}, \ldots, h_{4m}}^{variance} , \overbrace{h_{4m+1}, \ldots, h_{4m+C_{2m}^2}}^{covariance} )^T
   \mathbf{h}_{depot} = (\overbrace{h_1,\ldots, h_{2m}}^{mean} , \overbrace{\ldots, h_{4m}}^{variance} , \overbrace{\ldots, h_{4m+C_{2m}^2}}^{covariance} )^T
\end{equation}
To output this $\mathbf{h}_{depot}$ for constructing the $2m$-dimensional Gaussian distribution, we employ a layer module featuring two fully connected layers to process the global embedding $\mathbf{h}_{serve}$ derived by encoder, expressed as:
$\mathbf{h}_{depot} = {\rm tanh}({\rm FF_{multiG}}({\rm tanh}({\rm FF}(\mathbf{h}_{serve}))))$.
% \begin{equation}
%    \label{multiG_generator}
%    \mathbf{h}_{depot} = {\rm tanh}({\rm FF_{multiG}}({\rm tanh}({\rm FF}(\mathbf{h}_{serve}))))
%    %\mathbf{h}_{depot} = {\rm FF_{multiG}}({\rm tanh}({\rm FF}(\mathbf{h}_{serve})))
% \end{equation}

When utilizing the $\mathbf{h}_{depot}$ to construct the multivariate Gaussian distribution, 
it is critical to ensure that the variances remain positive. 
Hence, before constructing, we process the second $2m$ dimensions' elements as below:
% We construct the multivariate Gaussian distribution with $\mathbf{h}_{depot}$. 
% What needs to be noticed is, to make sure the corresponding variance of each variate be positive, the second $2m$ dimensions' elements should be processed by following equation before being used:
$\overrightarrow{var}  = 1 + {\rm elu}((h_{2m+1},\ldots ,h_{4m})^T)$.
% \begin{equation}
%     \label{deal with variance}
%     \overrightarrow{var}  = 1 + {\rm elu}((h_{2m+1},\ldots ,h_{4m})^T)
% \end{equation}
Besides, when sampling the depot set $\mathbf{X}_{depot}$ which records the depot coordinates, 
to adhere to the configuration, $\mathbf{X}_{depot}$ should be mapped within unit square $[0,1]\times[0,1]$ to standardize the depot set: 
$\mathcal{D}_{\text{multiG}}={\rm sigmoid}(\mathbf{X}_{depot})$.
% \begin{equation}
%    \label{depot set distribution}
%    \mathcal{D}_{\text{multiG}} = {\rm sigmoid}((\overbrace{X_1, X_2}^{D_1} , \ldots , \overbrace{X_{2m-1}, X_{2m}}^{D_m} )^T)
% \end{equation}


\textbf{DGM-generator in Exact position mode:}
In this mode, DGM aims to directly generate the exact positions for a set of depots based on the global embedding $\mathbf{h}_{serve}$ derived by encoder.
To this end, we retain $m$ as the depot number, then the DGM's output should be a vector $\mathbf{h}_{depot} \in \mathbb{R}^{2m}$ in which every two dimensions represent the coordinates $(x_{D_k}, y_{D_k})$ for a depot $D_k$.
\begin{equation}
   \label{h_depot}
   \mathbf{h}_{depot} = (\overbrace{h_1, h_{2}}^{D_1} , \ldots, \overbrace{h_{2m-1}, h_{2m}}^{D_m} )^T
\end{equation}
To output this $\mathbf{h}_{depot}$, a layer module, encompassing two fully connected layers, is employed:
$\mathbf{h}_{depot} = {\rm FF_{exactP}}({\rm tanh}({\rm FF}(\mathbf{h}_{serve})))$.
% \begin{equation}
%    \label{exact_position_generator}
%    \mathbf{h}_{depot} = {\rm FF_{exactP}}({\rm tanh}({\rm FF}(\mathbf{h}_{serve})))
% \end{equation}
Also, to satisfy the configuration, we map the $\mathbf{h}_{depot}$ within the unit square $[0,1]\times[0,1]$ to standardize the depot set:
$\mathcal{D}_{\text{exactP}}={\rm sigmoid}(\mathbf{h}_{depot})$.

% % %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Training Strategy}
%\subsubsection{\textbf{Pre-training of MDLRAM}}
\textbf{Step I: Pre-training of MDLRAM:}
MDLRAM concurrently processes a batch of problem instances randomly sampled from the configuration, thereby generating a batch of corresponding MDPs as their respective feasible solutions.
Each MDP involves a permutation of actions, denoted as ${\rm MDP}(\mathbf{A}) = \{a_1, a_2, ‚Ä¶\}$.
Because each action $a_t$ is associated with a probability $p_t$ for selecting the corresponding point,
the entire MDP's probability is manifested as:
$p_{\boldsymbol{\theta_{\text{I}}}}(\mathbf{A}) = \prod_{t}p_t = \prod_{t}p(s_{t+1}|s_t,a_t)$,
% \begin{equation}
%     \label{probability for a MDP}
%      p_{\boldsymbol{\theta_{\text{I}}}}(\mathbf{A}) = \prod_{t}p_t = \prod_{t}p(s_{t+1}|s_t,a_t)
% \end{equation}
which is parameterized by $\boldsymbol{\theta_{\text{I}}}$, denoting the MDLRAM's parameters that require training.
Based on a batch of such MDPs, each associated with a probability $p_{\boldsymbol{\theta_{\text{I}}}}(\mathbf{A})$ and a cost $L_{\text{Sel}}(\mathbf{A})$ in Eq. (\ref{optimization objective details for M_I}), 
the MDLRAM is trained by REINFORCE gradient estimator with greedy rollout baseline \cite{kool2018attention} to minimize the expectation of cost, as depicted in Eq. (\ref{REINFORCE algo}), 
where the baseline $\bar{\mathcal{B}} $ is established through a parallel network mirroring the structure of MDLRAM, persistently preserving the best parameters attained and remaining fixed.
(See Appendix \ref{training_pseudo} for pseudo code and details.)
\begin{equation}
   \label{REINFORCE algo}
    \nabla\mathcal{L}(\boldsymbol{\theta_{\text{I}}}) = \mathbb{E}_{p_{\boldsymbol{\theta_{\text{I}}}}(\mathbf{A})}[(L_{\text{Sel}}(\mathbf{A})-\bar{\mathcal{B}} )\nabla\log p_{\boldsymbol{\theta_{\text{I}}}}(\mathbf{A})]
\end{equation}

% ####################################################
\textbf{Step II: Dual-mode training of DGM:}
As depicted in Fig. \ref{CLRP-G training process overview},
DGM can be trained in two modes,
with the pre-trained MDLRAM serving as a fixed sub-solver.
\textbf{For the record}, the $\boldsymbol{\theta_{\text{II}}}$, denoting DGM's parameters that requires training, is appended as footnote only to those variables which are parameterized by DGM.

\textit{(i) Multivariate Gaussian distribution mode:}
In this mode, as depicted in left side of Fig. \ref{CLRP-G training process overview}, the DGM takes in a main-batch (Batchsize: $B_{\text{main}}$) of randomly sampled graphs: $\{G_{b} | b = 1,2,...,B_{\text{main}}\}$, each with a group of customer requests,
and then outputs a main-batch of corresponding multivariate Gaussian distributions: $\{\mathcal{N}_b \left(\boldsymbol{\mu}, \Sigma \right) | b = 1,2,...,B_{\text{main}}\}$. 
Thus, training DGM involves ensuring that the depots sampled from these distributions yield favorable expectations for the cost $L_{\text{Gen}}(\mathcal{D})$. 

To achieve this, from each distribution $\mathcal{N}_b \left(\boldsymbol{\mu}, \Sigma \right)$ within the main-batch, we sample a sub-batch (Batchsize: $B_{\text{sub}}$) sets of depots. 
Each depot set is represented as $\mathcal{D}_{\text{multiG}}$, associated with their probabilities $p_{\boldsymbol{\theta_{\text{II}}}}(\mathcal{D}_{\text{multiG}})$ 
and costs $L_{\text{Gen}}(\mathcal{D}_{\text{multiG}})$ in Eq. (\ref{optimization objective details for M_II}). 
In this way, a main-batch ($B_{\text{main}}$) of cost expectations, each corresponding to a multivariate Gaussian distribution, can be derived.
This entire process is shown in left part of Fig. \ref{CLRP-G training process overview}.
We employ following optimizer to train the DGM in this distribution mode
\begin{equation}
   \label{object_REIN of Model II}
   \begin{aligned}
       \nabla\mathcal{L}(\boldsymbol{\theta_{\text{II}}}) = \frac{1}{B_{\text{main}}}\sum_{b = 1}^{B_{\text{main}}}\mathbb{E}_{p_{\boldsymbol{\theta_{\text{II}}}}(\mathcal{D}_{\text{multiG}})}^{(b)}[L_{\text{Gen}}(\mathcal{D}_{\text{multiG}}, G_{b})  
        \cdot \nabla\log p_{\boldsymbol{\theta_{\text{II}}}}(\mathcal{D}_{\text{multiG}})]
   \end{aligned}
\end{equation}
\begin{wrapfigure}[14]{r}[5pt]{0.42\textwidth}
    \centering
    \vspace{-1pt}
    \includegraphics[width=5.6cm]{ML_images/pic4-56_bigger_6.png}
    \caption{DGM's dual-mode training.}
    \label{CLRP-G training process overview}
 \end{wrapfigure}
\textit{(ii) Exact position mode:}
In this mode, as depicted in right side of Fig. \ref{CLRP-G training process overview}, the DGM  still ingests a main-batch ($B_{\text{main}}$) of randomly sampled graphs: $\{G_{b} | b = 1,2,...,B_{\text{main}}\}$, each with a group of customer requests,
but directly generates the corresponding sets of depots: $\{\mathcal{D}_{\text{exactP}}^{(b)} | b = 1,2,...,B_{\text{main}}\}$. 
For each set of depots $\mathcal{D}_{\text{exactP}}$, the cost $L_{\text{Gen}}(\mathcal{D}_{\text{exactP}})$ can be derived by Eq. (\ref{optimization objective details for M_II}) 
whose first part is obtained by pre-trained MDLRAM.
Below optimizer guides the DGM's training in exact mode:
\begin{equation}
   \label{REINFORCE algo for model II exactP}
    \nabla\mathcal{L}(\boldsymbol{\theta_{\text{II}}}) = \frac{1}{B_{\text{main}}}\sum_{b = 1}^{B_{\text{main}}}\nabla L_{\text{Gen}}((\mathcal{D}_{\text{exactP}}^{(b)})_{\boldsymbol{\theta_{\text{II}}}}, G_{b})
\end{equation}
\textit{\textbf{It is crucial to differentiate that}}, for different modes, the DGM's parameters $\boldsymbol{\theta_{\text{II}}}$ are tracked in different variables.
In multivariate Gaussian distribution mode, what has been parameterized by $\boldsymbol{\theta_{\text{II}}}$ is the probability for each sampled $\mathcal{D}_{\text{multiG}}$, 
whereas in exact position mode, what has been parameterized by $\boldsymbol{\theta_{\text{II}}}$ is the $\mathcal{D}_{\text{exactP}}$.
Therefore, the gradients in these two modes are respectively backpropagated to parameters $\boldsymbol{\theta_{\text{II}}}$ through $p_{\boldsymbol{\theta_{\text{II}}}}(\mathcal{D}_{\text{multiG}})$ and ($\mathcal{D}_{\text{exactP}})_{\boldsymbol{\theta_{\text{II}}}}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experimental Results and Discussion}
\textbf{Training Setup:}
To ensure comparability with prior methods, 
we establish the training dataset following the setup outlined in prior routing studies \cite{kool2018attention}.
As for the depot-related setting, we adopt the data format prevalent in real-world LRP benchmark datasets which are conventionally employed in pertinent studies \cite{belenguer2011branch,prins2006solving}.
Every single \textit{problem instance} in the training dataset is defined on a unit square $[0,1]\times[0,1]$, where the customers' requests are uniformly scattered, 
with their corresponding demands uniformly sampled from $[0,10]$.
% The distance $d_{ij}$ among points is measured as Euclidean.

The \textit{problem instances} for MDLRAM's pre-training are from three problem scales: 
%which are $20, 50, 100$ serve points and correspondingly with $3, 6, 9$ depots candidates.
$n=20, 50, 100$ customers, respectively coupled with $m=3, 6, 9$ depot candidates.
\textit{\textbf{Corresponding to each scale:} }
(1) The vehicle's maximum capacity $Q$ is selected as  $30, 40, 50$ respectively;
(2) The vehicle's setup cost $U$ is set as 0.3;
(3) The depot's desired maximum supply $M_k$ is uniformly selected from $[50,80]$, $[80,120]$, $[120,170]$;
(4) The depot's opening cost $O_k$ is uniformly selected from $[2,5]$, $[2,5]$, $[12,19]$;
The coefficients in objective function Eq. (\ref{optimization objective details for M_I}) are defined as $\alpha=1,\beta=1,\delta=2$;
For each scale, we train MDLRAM on one A40 GPU for 100 epochs with 1,280,000 \textit{problem instances} generated on the fly as training dataset, 
which can be split into 2,500 batches with batchsize of 512 (256 for scale $100$ due to device memory limitation).

As for DGM's \textit{problem instance}, only including customer requests, we also consider three problem scales: $n=20, 50, 100$ customers.
The expected distance among the generated depots ranges within $[0.2, 0.7]$.
The coefficients in objective function Eq. (\ref{optimization objective details for M_II}) are specified as $\lambda=10, \varepsilon=10$.
Correspondingly, for each scale, we train DGM on one A40 GPU for 100 epochs. Within each epoch, 2,500 main-batches of \textit{problem instances} are generated on the fly as training dataset and iteratively fed into DGM.
\textit{\textbf{In multivariate Gaussian distribution mode}}, the main-batch size $B_{\text{main}}$ is set as 32 (16 for scale $100$),  
and the sub-batch size $B_{\text{sub}}$ for sampling in each distribution is selected as 128, 64, 32 for scale $20, 50, 100$ respectively.
\textit{\textbf{In exact position mode}}, where no sampling is performed, we set main-batch size as 512 (256 for scale $100$).

\subsection{Results Analysis for Critic Model - MDLRAM} 
We first assess the efficacy of the pre-trained MDLRAM.
As critic model, it is expected to instantly provide optimized LRP solution for serving customers based on generated depots, in batch-wise manner.
Therefore, we test it on both \textit{\textbf{Synthetic Dataset}} to highlight the instant batch-wise solving ability,
and \textit{\textbf{Real-world Benchmark Dataset}} to evaluate its generalization performance by comparing with SOTA results, 
which, so far, are all achieved by specifically designed heuristic methods.

\textbf{(i) Testing on Synthetic dataset:}
% To assess the MDLRAM's capability of instantly providing optimized LRP solutions for a batch of problem instances, 
% The testing on synthetic dataset constructed is conducted as follows. 
For each problem scale, the synthetic testing dataset include 10,000 problem instances randomly sampled from the same configuration in training process, 
capable of being divided into batches to facilitate batch-wise testing.
Given the lack of existing DRL method specifically designed for standard LRP scenario setup,
we compare results from various enhanced classic heuristic methods, which are commonly applied to solve routing problems,
even though heuristic methods are not suitable for batch-wise usage considering its solving manner and unstable inference time.  
The parameters 
% (see Appendix.\ref{hyper_para}) 
are tuned to align LRP setup to report the best performance.

% 6. new added TS GA ALNS ACO without deviation with total cost (do not seperate G & S)
\begin{wraptable}[17]{r}{0.6\textwidth}
   \centering
   \vspace{-8pt}
   % \renewcommand\arraystretch{0.6}  
   % \setlength{\tabcolsep}{3.1pt} 
   \caption{Batch-wise Testing Performance of MDLRAM on Synthetic Dataset. 
   (``Ttl.C.: total cost in Eq.(\ref{optimization objective details for M_I})''; ``Len.: total length''; ``Dpt.C. (Nb.): depot opening cost (opened depot number)''; ``Veh.C. (Nb.): vehicle setup cost (used vehicle numbers)''; ``Dpt.P.: penalty for exceeding depot desired maximum supply'')
%     Parameters for heuristic methods: 
%     \textbf{(1)} Adaptive Large Neighborhood Search (ALNS): 
% \textit{Destroy (random percentage $0.1\sim0.4$, worst nodes $5\sim10$); Repair (random, greedy, regret with 5 nodes); Rewards ($r_1=30, r_2=20, r_3=10, r_4=-10$); Operators weight decay rate: $0.4$; Threshold decay rate: $0.9$};
% \textbf{(2)} Simulated annealing (SA): 
% \textit{Initial Temperature: $1 \times 10^{5}$; Termination Temperature: $1 \times 10^{-6}$; Cooling Rate: 0.9;} 
% \textbf{(3)} Tabu Search (TS): 
% \textit{Action Strategy (1-node swap, 2-node swap, Reverse 4 nodes); Tabu step: $30$};
}
   %{Batch-wise Testing Results of Model-I (MDAM) on Synthetic Dataset for Solving CLRP-S.}
   \label{test results of model I on synthetic dataset}
   %\begin{center}
   \resizebox{0.6\textwidth}{23mm}{
   \begin{tabular}{c|cc|c|c|c|cc|cc|c|c}
   % \hline
   % \hline
   \toprule
   \midrule
%            &   \multicolumn{2}{c|}{$m=3,n=20$}    &    \multicolumn{2}{c|}{$m=4,n=50$}      & \multicolumn{2}{c}{$m=5,n=100$}\\
   % \multicolumn{2}{c|}{Scl.(n,m)}
   % \multicolumn{2}{c|}{Scale} & Solving &  Route & \multicolumn{2}{c|}{Depot} & \multicolumn{2}{c|}{Vehicle} & Dpt. & Inf.T. \\
   % \cmidrule(r){1-2} \cmidrule(r){5-8}
   % n & m & Method &  Len. & Open C. & (Nb.) & Setup C. & (Nb.) & Cap P. & (Batch)\\
   & $n$ & $m$ & Mtd. & Ttl.C. &  Len. & Dpt.C. & (Nb.) & Veh.C. & (Nb.) & Dpt.P. & Inf.T.\\
   % \hline
   % \hline
   \cmidrule[0.1em](r){1-12}
   % \multirow{7}{*}{(20, 3)} &
   % \multirow{7}{*}{
   % \begin{minipage}[t]{6mm}
   % \vspace{2mm} % To add a little space at the top of the minipage
   % (n,m) \\
   % = \\
   % (20,3)
   % \vspace{2mm} % To add a little space at the bottom of the minipage
   % \end{minipage}} &
   % 20&3& M-I(G)     &13.81             &5.86 &  6.45 & (2.15) & 1.25  &  (4.17) & 0.24  & 0.10s \\
   % \cmidrule(r){3-11}
   \multirow{6}{*}{\rotatebox{90}{\textbf{Scale 20}}} &
   20&3& MDLR(S)     &\textbf{13.17*}             &5.61 &  6.26 & (2.14) & 1.23  &  (4.08) & 0.07  & 0.16s \\
   &20&3& MDLR(G)     &13.81             &5.86 &  6.45 & (2.15) & 1.25  &  (4.17) & 0.24  & 0.10s \\
   % \midrule
   \cmidrule(r){4-12} 
   &20&3& ALNS   &13.23     &6.56           &5.40          &(2.01)           &1.16          &(3.88)           &0.10        &4.12s           \\
   % 20&3& ACO  &12.83       &6.02           &5.52          &(2.00)           &1.19          &(3.95)           &0.11        &6.66s           \\
   &20&3& GA  &13.72        &6.87           &5.61          &(2.01)           &1.17          &(3.90)           &0.07        &6.63s           \\
   &20&3& TS  &16.26        &9.13           &5.79          &(2.00)           &1.16          &(3.88)           &0.18        &0.16s           \\
   % \hline
   % \hline
   \midrule
   \midrule
   % \cmidrule[0.1em](r){1-12}
   % \multirow{6}{*}{(50, 6)} &
   % \multirow{7}{*}{
   % \begin{minipage}[t]{6mm}
   % \vspace{2mm} % To add a little space at the top of the minipage
   % (n,m) \\
   % = \\
   % (50,6)
   % \vspace{2mm} % To add a little space at the bottom of the minipage
   % \end{minipage}} &
   % 50&6& M-I(G)   &22.05               &9.20  & 10.66  & (3.55) & 2.09  &  (6.97) & 0.10 & 0.31s \\
   % \cmidrule(r){3-11}
   \multirow{6}{*}{\rotatebox{90}{\textbf{Scale 50}}} &
   50&6& MDLR(S)   &\textbf{20.85*}               &8.65  & 10.13  & (3.52) & 2.05  &  (6.82) & 0.03 & 0.43s \\
   &50&6& MDLR(G)   &22.05               &9.20  & 10.66  & (3.55) & 2.09  &  (6.97) & 0.10 & 0.31s \\
   % \midrule
   \cmidrule(r){4-12}
   &50&6& ALNS   &25.10     &14.71           &8.33          &(3.03)           &2.05          &(6.84)           &0.01        &47.35s           \\
   % 50&6& ACO  &21.27       &9.72           &9.44          &(3.28)           &2.10          &(7.01)           &0.01        &23.02s           \\
   &50&6& GA   &29.65       &18.53           &9.03          &(3.12)           &2.09          &(6.98)          &0        &12.69s           \\
   &50&6& TS   &33.26       &22.26           &8.91          &(3.16)           &2.10          &(6.99)           &0        &1.57s           \\
   \midrule
   \midrule
   % \cmidrule[0.1em](r){1-12}
   % \multirow{6}{*}{(100, 9)} &
   % 50&6& M-I(G)   &94.94              &16.29 & 75.00 & (5.00) & 3.37 & (11.23) & 0.28 & 0.55s \\
   % \cmidrule(r){3-11}
   \multirow{6}{*}{\rotatebox{90}{\textbf{Scale 100}}} &
   100&9& MDLR(S)   &\textbf{86.88*}              &14.79 & 68.77 & (5.00) & 3.32 & (11.06) & 0 & 1.04s \\
   &100&9& MDLR(G)   &94.94              &16.29 & 75.00 & (5.00) & 3.37 & (11.23) & 0.28 & 0.55s \\
   %\midrule
   \cmidrule(r){4-12}
   &100&9& ALNS   &89.75     &29.58           &56.82          &(3.97)           &3.21          &(10.69)           &0.14        &412.83s           \\
   % 50&6& ACO   &85.34      &16.50           &65.26          &(4.42)           &3.28          &(10.94)           &0.30        &66.29s           \\
   &100&9& GA    &108.24      &43.79           &61.03          &(4.14)           &3.28          &(10.92)           &0.14        &19.44s           \\
   &100&9& TS    &104.39      &41.80           &59.31          &(4.08)          &3.27          &(10.89)           &0.01        &16.44s           \\
   % \hline
   % \hline
   % \midrule
   % \midrule
   % \cmidrule[0.1em](r){1-12}
   % \cmidrule[0.1em](r){1-12}
   % \multirow{8}{*}{\rotatebox{90}{\textbf{Batch}}} &
   % $n$ & $m$ & Mtd. & Ttl.C. &  Len. & Dpt.C. & (Nb.) & Veh.C. & (Nb.) & Dpt.P. & Inf.T.\\
   % \cmidrule[0.1em](r){2-12}
   % &20&3& M-I(G)     &13.81             &5.86 &  6.45 & (2.15) & 1.25  &  (4.17) & 0.24  & 0.10s \\
   % \cmidrule(r){2-12}
   % &50&6& M-I(G)   &22.05               &9.20  & 10.66  & (3.55) & 2.09  &  (6.97) & 0.10 & 0.31s \\
   % \cmidrule(r){2-12}
   % &100&9& M-I(G)   &94.94              &16.29 & 75.00 & (5.00) & 3.37 & (11.23) & 0.28 & 0.55s \\
   \bottomrule
   \end{tabular}
   }
   %\end{center}
\end{wraptable}

As presented in Table~\ref{test results of model I on synthetic dataset}, the testing on synthetic dataset provides a detailed breakdown of cost and inference times for various problem scales, 
with customer numbers indexed as $n$ ranging from 20 to 100, and depot numbers marked as $m$ varying from 3 to 9. 
For each scale, all methods involved in comparison report their average objective value on the 10,000 synthetic problem instances.
Specifically, for MDLRAM, the results are reported in two testing strategies, decided by its decoding process:
\textbf{(a) Greedy test:} 
when generating the solution routes for each problem instance, the action selected in each decoding step is the point with the highest probability, 
thereby deriving one greedy solution;
\textbf{(b) Sampling test:} 
For each instance, MDLRAM simultaneously generates 1,280 random solutions by stochastically select action in each decoding step.
Then, the one with the lowest cost is chosen as the solution.

From Table~\ref{test results of model I on synthetic dataset}, 
% the results demonstrate that, 
MDLRAM's sampling test consistently achieves the lowest total cost across all scales, outperforming other methods.
Meanwhile, its greedy way yields smaller total cost than other methods on larger scale $n=50,100$, being slightly outperformed on scale $n=20$.
The error bars for the greedy test results are $\pm 0.11$, $\pm 0.11$, and $\pm 0.35$ for scales 20, 50, and 100, respectively.
Regarding each individual objective, all methods exhibit similar vehicle usage, 
but MDLRAM tends to distribute this usage among more depots, resulting in an increase on depot opening cost compared to heuristic methods. 
This indicates that DRL method's extensive searching ability enables exploration of a broader range of circumstances, leading to better solutions.
As for inference time, with the increase of scale, MDLRAM shows steady performance, basically within 1s timeframe, 
whereas heuristic methods demonstrate significant increase on time consumption. 

% \subsubsection{\textbf{Testing on Real-world dataset}} 

% ################ 2024-3-18 tailored for ECML conference, try to make it half column ################ 
\begin{wraptable}[20]{r}{0.55\textwidth}
   \centering
   \vspace{-23pt}
   % \renewcommand\arraystretch{0.3}
   % \setlength{\tabcolsep}{3.5pt}
   \caption{MDLRAM's Performance on cases from Real-world Dataset (``*'' represents an optimal solution; ``Ttl.C.'': Total Cost; ``Inf.T.'': Inference Time).}
   \label{test on real-world dataset}
   %\begin{center}
   \resizebox{0.55\textwidth}{33mm}{
   \begin{tabular}{lcc|c|ccc|ccc}
     \toprule
 \midrule
   & &   &         &   \multicolumn{3}{c|}{GRASP \cite{prins2006solving}} &   \multicolumn{3}{c}{\textbf{MDLRAM (ours)}}   \\%&   \multicolumn{3}{c}{MDLRAM details} \\
\cmidrule(r){1-10}
Case name &$n$ &$m$ & BKS  & Ttl C. &Gap & Inf.T. & Ttl C. &Gap & Inf.T. \\% &Route Len. & Dpt C. (Nb.) & Veh C. (Nb.) \\
 \midrule
 %\midrule
%   \multirow{10}{*}{\rotatebox{90}{\textbf{scale 20}}} &
   %\multirow{9}{*}{Tuzun} &
       P111122      &100 &20          &14492         &15269.0        &5.36\%       &40.7s     &15554.5         &7.33\% &0.75s \\%&11454.5        &3000       (3)     &1100 (11) \\
       P111222      &100 &20          &14323         &14822.9        &3.49\%       &36.2s     &15154.43         &5.80\% &0.74s \\%&10954.43        &3000       (3)     &1200 (12) \\
       P111112      &100 &10          &14676.8         &15252.5        &3.92\%       &32.4s     &15516.6         &5.72\% &0.64s \\%&11416.6        &3000       (3)     &1100 (11) \\
       P113122      &100 &20          &12463         &12729.4        &2.14\%       &36.0s     &13081.49         &4.96\% &0.72s \\%&8981.49        &3000       (3)     &1100 (11) \\
       P111212      &100 &10          &13948         &14235.4        &2.06\%       &27.6s     &14529.15         &4.17\% &0.58s \\%&10429.15        &3000       (3)     &1100 (11) \\
       %P112122                &11027         &11236.4        &1.90\%       &34.3s     &12266.81         &11.24\% &0.745s &8166.81        &3000       (3)     &1100 (11) \\
       %P113112                &12454.5         &12731.0        &2.22\%       &21.5s     &13774.28         & 10.60\% &0.590s &9674.28        &3000       (3)     &1100 (11) \\
       %\midrule
       %100-5-1a     &100 &5          &274814         &279437        &1.68\%       &27.6s     &294490         &7.16\% &0.58s \\%&125478        &144012       (3)     &25000 (25) \\
      %100-5-1b     &100 &5          &214392         &216159        &0.82\%       &23.2s     &231552         &8.00\% &0.50s \\%&76540        &144012       (3)    &1100 (11) \\
   %\multirow{8}{*}{Barreto} &
       % Per183-12-2            &204*           &204.0        &0.00\%      &0.2s    &204.00         &0.00\% &0.071s &103.97        &100       (1)     &0 (2) \\
       % Per183-55-15           &1112.06        &1121.8        &0.88\%                &10800s    &1151.91         &3.58\% &0.380s &431.91        &720       (3)     &0 (10) \\
       % Per183-85-7            &1622.5         &1668.2        &2.82\%                  &10813.8s    &1676.89         &3.35\% &0.467s &560.89        &1116       (3)     &0 (11) \\
       % Chr69-75-10ba  &75 &10          &861.6         &861.6        &0.00\%       &9.8s    &918.39         &6.59\% &0.40s &798.39        &120       (3)     &0 (10) \\
       %Chr69-75-10bmw         &802.08         &809.5        &0.93\%       &100s    &846.06         &5.48\% &0.409s &726.06        &120       (3)     &0 (9) \\
       \midrule
   %\multirow{16}{*}{Prodhon} &
      50-5-1a      &50 &5          &90111         &90632        &0.57\%       &1.8s      &95072      &5.51\% &0.25s      \\%&57630        &25442       (3)     &12000 (12)\\
      %50-5-2a      &50 &5          &88298         &88786        &0.55\%       &2.4s     &95124       &7.73\% &0.26s      \\%&50410        &32714       (3)     &12000 (12) \\
      50-5-2b      &50 &5          &67340         &68042        &1.04\%       &2.5s     &70941       &5.35\% &0.23s \\%&32227        &32714       (3)     &6000 (6) \\
      %50-5-2Bis    &50 &5          &84055         &84055        &0.00\%       &1.7s     &90077         &7.17\% &0.26s \\%&57949        &20128       (3)     &12000 (12) \\
      %50-5-2bBis   &50 &5          &51822*         &52059        &0.46\%       &1.6s     &54676         &5.51\% &0.24s \\%&29434        &19242       (3)     &6000 (6) \\
      50-5-3b      &50 &5          &61830         &61890        &0.10\%       &2.0s     &66258         &7.16\% &0.23s \\%&41297        &18961       (2)     &6000 (6) \\
      G67-21-5  &21 &5           &424.9*         &429.6        &1.1\%       &0.2s     &425.66         &0.18\% &0.12s \\%&325.66        &100       (2)     &0 (4) \\
       20-5-1a     &20 &5           &54793*         &55021        &0.42\%       &0.2s     &57005      &4.04\% &0.11s      \\%&26383        &25622       (3)     &5000 (5) \\
      %20-5-1b      &20 &5          &39104*         &39104        &0.00\%       &0.2s     &40767      &4.25\% &0.13s      \\%&22270        &15497       (2)     &3000 (3) \\
      20-5-2a      &20 &5          &48908*         &48908        &0.00\%       &0.1s     &50029      &2.29\% &0.12s      \\%&20833        &24196       (3)     &5000 (5)\\
      20-5-2b      &20 &5          &37542*         &37542        &0.00\%       &0.2s     &38893      &3.60\% &0.11s      \\%&21982        &13911       (2)     &3000 (3) \\
      %\midrule
  %\multirow{6}{*}{Akca} &
  \midrule
  \midrule
     &   & &      &   \multicolumn{3}{c|}{HBP \cite{akca2009branch}} &   \multicolumn{3}{c}{\textbf{MDLRAM (ours)}}   \\%&   \multicolumn{3}{c}{MDLRAM details} \\
     \cmidrule(r){1-10}
     Case name &$n$ &$m$ & BKS  & Ttl C. &Gap & Inf.T. & Ttl C. &Gap & Inf.T. \\%&Route Len. & Dpt C. (Nb.) & Veh C. (Nb.) \\
   \midrule
     P183-12-2     &12 &2       &204*           &204.0        &0.00\%      &0.2s    &204.00         &0.00\% &0.07s \\%&103.97        &100       (1)     &0 (2) \\
       P183-55-15  &55 &15         &1112.06        &1121.8        &0.88\%                &10800s    &1151.91         &3.58\% &0.38s \\%&431.91        &720       (3)     &0 (10) \\
       P183-85-7   &85 &7         &1622.5         &1668.2        &2.82\%                  &10813.8s    &1676.89         &3.35\% &0.46s \\%&560.89        &1116       (3)     &0 (11) \\
   \midrule
   \midrule
     &    & &     &   \multicolumn{3}{c|}{B\&C \cite{belenguer2011branch}} &   \multicolumn{3}{c}{\textbf{MDLRAM (ours)}}   \\%&   \multicolumn{3}{c}{MDLRAM details} \\    
   \cmidrule(r){1-10}  
   Case name &$n$ &$m$ & BKS  & Ttl C. &Gap & Inf.T. & Ttl C. &Gap & Inf.T. \\%&Route Len. & Dpt C. (Nb.) & Veh C. (Nb.) \\
       \midrule
     30-5a-1        &30 &5        &819.52*         &819.60        &0.00\%       &50.22s     &849.33         &3.64\% &0.143s \\%&649.33        &200       (2)     &0 (6) \\
      30-5a-2       &30 &5         &821.50*         &823.50        &0.00\%       &53.89s     &884.29         &7.64\% &0.144s \\%&684.29        &200       (2)     &0 (6) \\
      %30-5a-3       &30 &5         &702.30*         &702.30        &0.00\%       &0.73s     &758.00         &7.93\% &0.146s \\%&558.00        &200       (2)     &0 (6) \\
      40-5a-1       &40 &5         &928.10*         &928.20        &0.00\%       &305.25s     &988.80         &6.54\% &0.189s \\%&688.80        &300       (3)     &0 (6) \\
      40-5b-1       &40 &5         &1052.04*         &1052.07        &0.00\%       &3694.45s     &1107.54         &5.28\% &0.195s \\%&807.54        &300       (3)     &0 (9) \\
      %\midrule
   %mean &            & &                        &        &       &       &               &       &       &           &                   & \\     
   \bottomrule
   \end{tabular}
   }
   %\end{center}
\end{wraptable}


\textbf{(ii) Testing on Real-world dataset:}
To further assess the MDLRAM's generalization performance on real-world problem instances with diverse node distribution compared to the synthetic problem instances used during training, 
we conduct individual comparison on instances from four real-world datasets (Prodhon \cite{prins2006solving}, Acka \cite{akca2009branch}, Tuzun \cite{tuzun1999two}, Barreto \cite{barreto2007using}) which include their best-known solutions (BKS), 
and the SOTA results derived by existing methods.
\textit{\textbf{Notably,}} as a critic model, MDLRAM stands out for its rapidity to plan high-quality solutions in batches, 
which lays the foundation for depot-generating tasks completed by DGM.
Therefore, when testing on real-world dataset, our aim is not to establish new SOTA results, 
instead, we demonstrate how MDLRAM consumes significantly less inference time than existing method to derive high-quality solutions comparable to BKS, 
thereby ensuring the efficacy for depot generation.

% We further evaluate the general solving ability of MDLRAM on real-world instances with various scales, 
% which may exhibit diverse node distributions and different problem configurations compared to the instances used during training.
As detailed in Table~\ref{test on real-world dataset}, these instances diverse significantly, with customer amount $n$ ranging from 12 to 100 and depot candidate amount $m$ from 2 to 20. 
For each case, we juxtapose: the BKS, the results achieved by the specifically designed SOTA method reported in literatures, and results obtained through our MDLRAM. 
Across all the cases, our approach can plan solution routes comparable to those of traditional method but with notably reduced inference time. 
This efficiency becomes increasingly pronounced as the problem scale enlarges, demonstrating MDLRAM's capability to maintain solution quality while significantly reducing time consumption.

\subsection{Results Analysis for DGM}
As for the DGM, based on a same group of customer requests devoid of predefined depot candidates, 
it is expected to generate a depot set $\mathcal{D}$ which can lead to lower total cost than the randomly attempted depots.
% Therefore, we compare the quality of depots set derived by DGM and those derived by randomly attempting, 
% as judged by the pre-trained critic model - MDLRAM.
Thus, we arrange an experiment to evaluate which of the following three strategies can identify the best depot set for a same group of customer requests: 
(i) Generating the depot set in DGM-Exact mode; 
(ii) Generating the depot set in DGM-Gaussian mode; 
(iii) Randomly attempting the depot set in batches.
The quality of the depot set is judged by $L_{\text{Gen}}(\mathcal{D})$ in Eq. (\ref{optimization objective details for M_II}). 
%whose first part is the route length derived by MDLRAM.
\textbf{\textit{For each scale}} $n \in \{20, 50, 100\}$,
we randomly sample 8,000 problem instances as testing dataset. 
Each instance only includes a group of customer requests.
The results are reported in two ways, 
respectively evaluating the \textit{average-level} and \textit{best-level} of the solution depot set generated by each method:
%to demonstrate which of these three methods identified the best depots set for the same problem instance:
\textbf{(i) Average test:} For each problem instance, these methods respectively generate 512 solution sets of depots to serve the same group of customers accordingly. 
The mean of these 512 cost values is reported as the corresponding result for this problem instance. 
Then, the average of these 8,000 mean costs are obtained as final result.
\textbf{(ii) Sampling test:}
% In sampling test, for each problem instance, the three methods will also respectively generate 512 solution sets to serve the customers in this problem instance, but report the lowest cost value as the result for this problem instance.
% Then, the average of these 8000 lowest cost values are reported as the final result.
Similarly, for each problem instance, these methods respectively generate 512 solution sets of depots to serve the same group of customers, but only the lowest cost is reported as the result for that problem instance. 
The final result is derived as the average of these 8,000 lowest cost values.
\textit{\textbf{Notably,} DGM's Exact mode directly generate deterministic solution depot set for given problem instance, 
thereby yielding identical outcomes for its both testing ways.}
% precluding the sampling testing applicable for distribution mode. 
% Therefore, the outcomes reported by Exact mode's both testing ways remain identical.}

\begin{wraptable}[20]{r}{0.55\textwidth}
   \centering
   \vspace{-16pt}
   %\tiny
   % \renewcommand\arraystretch{1}
   % \setlength{\tabcolsep}{0.01pt}
   \caption{Comparison of solution depot set generated by different strategies.
   (``Ttl.C.: total cost in Eq.(\ref{optimization objective details for M_II})''; ``Len.: total length''; ``Ex.P. or Ls.P.: penalty for exceeding Upper bound or Lower bound of the distance among the generated depots'';)}
   \label{test results of model II vs model I}
   %\begin{center}
   \resizebox{0.55\textwidth}{30mm}{
   \begin{tabular}{l|l|l|c|c|cc|ccc}
   \toprule
   \midrule
   %        &   \multicolumn{2}{c|}{$m=3,n=20$}    &    \multicolumn{2}{c|}{$m=4,n=50$}      & \multicolumn{2}{c}{$m=5,n=100$}\\
   &$n$&Method                                    & \textbf{Ttl C.}      & \textbf{Len.} & \textbf{Ex. P.} & \textbf{Ls. P.} & (Dpt C. & Veh C. & Dpt P.) \\
   \midrule
   \midrule
   \multirow{12}{*}{\rotatebox{90}{\textbf{Average}}} &
   \multirow{4}{*}{20} &
   Rdm.                                             &7.671       &5.857     &1.121             &0.693                  &(6.448      &0.246    &1.249)   \\
   \cmidrule(r){3-10}
   &&\textbf{DGM-G.}                      &\textbf{5.724}       &\textbf{5.370}     &0.354             &0.000                  &(6.427      &0.234    &1.252)   \\
   &&\textbf{DGM-E.}                        &\textbf{5.096*}       &\textbf{5.089*}     &0.005             &0.002                  &(6.449      &0.242    &1.251)   \\
   \cmidrule[0.08em](r){2-10}
   &\multirow{4}{*}{50} &
   Rdm.                                              &18.261       &9.192     &5.600             &3.469                  &(10.650      &0.101    &2.090)   \\
   \cmidrule(r){3-10}
   &&\textbf{DGM-G.}                      &\textbf{10.921}       &\textbf{8.660}     &0.339             &1.922                  &(10.649      &0.090    &2.092)  \\
   &&\textbf{DGM-E.}                         &\textbf{8.531*}       &\textbf{8.496*}     &0.018             &0.017                  &(10.637      &0.082    &2.083)   \\
   \cmidrule[0.08em](r){2-10}
   &\multirow{4}{*}{100} &
   Rdm.                                              &38.040       &16.281     &13.434             &8.325                 &(74.956      &0.281    &3.371)   \\
   \cmidrule(r){3-10}
   &&\textbf{DGM-G.}                      &\textbf{23.305}       &\textbf{14.616}     &0.896             &7.793                 &(74.933      &0.271    &3.372)   \\
   &&\textbf{DGM-E.}                         &\textbf{15.394*}       &\textbf{13.690*}     &0.026             &1.678                  &(74.940      &0.274    &3.385)   \\
   \midrule
   \midrule
   \multirow{12}{*}{\rotatebox{90}{\textbf{Sample}}} &
   \multirow{4}{*}{20} &
   Rdm.                                             &\textbf{5.045*}            &\textbf{5.021*}     &0.014             &0.010                  &(6.465      &0.270    &1.247)   \\
   \cmidrule(r){3-10}
   &&\textbf{DGM-G.}                      &5.137            &5.115     &0.022             &0.000                  &(6.428      &0.249    &1.249)   \\
   &&\textbf{DGM-E.}                         &5.096       &5.089     &0.005             &0.002                  &(6.449      &0.242    &1.251)   \\
   \cmidrule[0.08em](r){2-10}
   &\multirow{4}{*}{50} &
   Rdm.                                             &10.684            &8.570     &0.610             &1.504                  &(10.679      &0.083    &2.086)   \\
   \cmidrule(r){3-10}
   &&\textbf{DGM-G.}                      &\textbf{8.769}            &\textbf{8.259*}     &0.135             &0.375                  &(10.676      &0.085    &2.086)   \\
   &&\textbf{DGM-E.}                        &\textbf{8.531*}       &\textbf{8.496}     &0.018             &0.017                  &(10.637      &0.082    &2.083)   \\
   \cmidrule[0.08em](r){2-10}
   &\multirow{4}{*}{100} &
   Rdm.                                             &24.056            &14.910     &2.491             &6.655                  &(74.928      &0.281    &3.369)   \\
   \cmidrule(r){3-10}
   &&\textbf{DGM-G.}                      &\textbf{19.538}            &\textbf{13.828}     &0.667             &5.043                  &(74.942      &0.250    &3.367)   \\
   &&\textbf{DGM-E.}                         &\textbf{15.394*}       &\textbf{13.690*}     &0.026             &1.678                  &(74.940      &0.274    &3.385)   \\
   \bottomrule
   \end{tabular}
   }
   %\end{center}
\end{wraptable}

%\subsubsection{Comparison results:}
All three methods undergo testing on same dataset, and their results are respectively decomposed and compared in Table~\ref{test results of model II vs model I} with two testing ways:
%each of the three methods is tested on the same testing datadset and the results are compared in two testing ways separately. 
\textbf{(1) Average test} aims at comparing the \textbf{\textit{average level}} of the solution depot set that each method can generate. 
Observations reveal that both DGM's two modes can identify superior solution depot set than randomly attempting, 
while the Exact mode exhibits better performance over the Gaussian mode;
\textbf{(2) Sampling test} further compares the \textbf{\textit{best level}} of solution depot set that each method can achieve. 
The results demonstrate that, when compared with ``randomly attempting'' on the same problem instance, DGM's Gaussian mode can find better solution depot set within the same sampling timeframe.
As for the DGM's Exact mode, its specific solution set consistently outperform those of the Gaussian mode, 
whereas the Gaussian mode can offer more flexibility with its sampling ability.
% Notably, the gaussian mode can offer more flexibility than exact mode when when concurrently addressing multiple problem configurations 
% by finding the intersection of the gaussian distributions of each problem configuration.
% Notably, the only exception happens in scenario with small scale 20, where the model I find better solution set by soley sampling without using model II, 
% which cannot be acheived in larger scales. 
Only one exception is observed in small scale ($n=20$), 
where ``randomly attempting'' achieves better solution set without the aid of DGM, 
which may not be replicable at larger scales.
\textit{\textbf{It's worth noting that,}} the superior performance achieved by the generated depot set $\mathcal{D}$ not just simply reflects on the total cost $L_{\text{Gen}}(\mathcal{D})$
which is the sum of route length and the violation of distance range among depots, 
but also respectively reflected on each individual cost items. 
This reflects DGM's ability on both identifying good depot positions and satisfying the location requirements, 
instead of only focusing on minimizing the violation of distance range among depots, while neglecting the route length, to achieve ``superior performance''.

% %%% new version 24-9-12
\textbf{Visualize Depots Distribution:} DGM's Gaussian mode reveals correlations between depot coordinates through learnable covariances. 
Visualization of these distributions shows that for smaller problem scales $m=3, n=20$, the 6-D normal distribution tends to present as distinct 2-D normal distributions. 
However, as problem scales grow, the relationships between depot coordinates become more complex, 
instead of simply presenting as several discrete 2-D normal distributions, 
implying that, at larger scales, random sampling would require significant computational effort to cover optimal depots. 
Full details and visualizations can be found in Appendix \ref{visual_distribute}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion and Future Work}
% % ######## Problem recap ######## :
% In this study, we extend the exploration of the LRP by addressing a real-world challenge: 
% the generation of depots when no predefined candidates are presented. 
% For this purpose, a generative DRL framework comprising two models is proposed.
% Specifically, the DGM, based on customer requests data,
% enables proactive depot generation with dual operational modes flexibly, 
% while the MDLRAM subsequently facilitates rapid planning of LRP routes from the generated depots for serving the customers.
% % ######## Contribution: technical + application ######## 
% Our framework represents a transition from traditional depot selection to proactive depot generation, 
% showcasing cost reductions and enhanced adaptability in real-world scenarios like disaster relief, 
% which necessitates quick depot establishment and flexible depot adjustment. 

% The framework's detachability offers flexible extension for its application.
% The DGM's depot-generating ability can be fine-tuned to adapt different LRP variants by jointing with other downstream models,
% making DGM a versatile tool in real-world logistics.
% Meanwhile, the end-to-end nature of MDLRAM enable its flexible usage on addressing LRP variants with requirements of adjusting inter-depot cost distribution. 
% % (See Appendix \ref{mdlram_bln} for extended results).

% % Limitation + future work
% % Looking ahead,
% Future research will focus on expanding DGM's applicability by incorporating a wider range of depot constraints to reflect more real-world scenarios accurately. 
% Meanwhile, leveraging the framework's detachability to adapt to various routing tasks and accommodating multiple concurrent routing tasks presents an exciting research avenue, 
% further extending the framework's utility in real-world logistics.
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% % ############### 6.3 ########################
% In this study, we extended the exploration of the LRP by addressing the real-world challenge of generating depots when no predefined candidates are presented. 
% For this purpose, a generative DRL framework comprising two models is proposed. Specifically, 
% the DGM leverages customer requests data, including geographic and demand information, to proactively generate depot locations through dual operational modes. 
% Subsequently, the MDLRAM facilitates rapid and effective planning of routes from these generated depots to serve customers efficiently.
% Our framework transitions from traditional depot selection to proactive data-driven depot generation, 
% showcasing cost reductions and enhanced adaptability in scenarios requiring quick depot establishment and flexible adjustments, such as disaster relief operations.
% % 2nd para
% The modular nature of our framework offers flexibility and extensibility. The DGM can be fine-tuned and integrated with other downstream models to adapt to various LRP variants, 
% making it a versatile tool in the logistics optimization toolkit. Additionally, the end-to-end design of the MDLRAM enables it to address different LRP scenarios, particularly those requiring adjustments to inter-depot cost distribution
% (See Appendix \ref{mdlram_bln} for extended results).
% % 3rd para
% Future research will focus on expanding the applicability of the DGM by incorporating a broader range of depot constraints to more accurately reflect real-world scenarios. 
% We also aim to leverage the framework's modularity to adapt to diverse routing tasks and manage multiple concurrent routing tasks, further extending its utility in real-world logistics applications.
% % ############################################

% \vspace{-7pt}
% ################ 24-9-12 ######################
In this study, we propose a generative DRL framework for depot generation without predefined candidates. 
% transitioning from traditional depot selection to proactive depot generation.
Based on customer requests data, the DGM proactively generates depots, while the MDLRAM efficiently plans routes from these generated depots, demonstrating flexibility and cost reductions, 
especially in scenarios requiring quick depot establishment and flexible adjustments.
This modular framework can be adapted to various LRP variants and further optimized for inter-depot cost balancing (see Appendix \ref{mdlram_bln} for extended results). 
For more detailed discussions on the framework's limitations and future work, such as incorporating additional depot constraints and generating depots adaptive to multiple routing tasks, please refer to the Appendix \ref{discussion}.
% ############################################


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \section*{Acknowledgments}
% This was was supported in part by......

%Bibliography
\bibliographystyle{unsrt}  
\bibliography{references}  

\newpage
\appendix

\setcounter{page}{1}

\section*{Appendix}

\section{Additional details about Methodology}

\subsection{LRP configuration}\label{lrp_config}

% ################## move to supplementary ECML 3-21 ######################

\textbf{Assumptions:}
Following the established assumptions \cite{belenguer2011branch}:
(1) Each customer's demand must be served by a delivery from exactly one depot and load transfers at intermediate locations are not allowed;
(2) Each customer must be served exactly once by one vehicle, i.e., splitting order is not allowed;
(3) No limits on the number of vehicles utilized, but the vehicle cost should be minimized as part of the objective.

\textbf{Constraints:}
The constraints in LRP includes three aspects. 
(1) Customer Demand: The vehicle's remaining capacity must suffice to cover its next target customer's demand during service;
(2) Vehicle Capacity: The cumulative demands delivered in a single vehicle route cannot surpass the vehicle's maximum capacity;
(3) Depot Supply: The aggregate demands dispatched from a specific depot is expected not to exceed its desired maximum supply.

\textit{\textbf{Remark 1:} The first two items are hard constraints determining solution feasibility, 
whereas the last item is a soft constraint manifesting as a penalty in the objective function.}

% ################## move to supplementary ECML 3-21 ######################

\subsection{MDP formulation}\label{mdp_form}

\begin{wrapfigure}[15]{r}[5pt]{0.5\textwidth}
    \centering
    \vspace{-21pt}
    \includegraphics[width=6.5cm]{ML_images/pic4-1.png}
    \vspace{-18pt}
    \caption{The feasible LRP solution in this example consists of 6 single routes, which are simultaneously carried out by multiple vehicles.
    The routes in same color belongs to a same depot. By linking them together, the feasible solution is formulated in points permutation, as an MDP.}
    \label{MultiDepotMDP in Appendix}
\end{wrapfigure}

% ################################ move to supplementary ######################################
Here, we propose the formulation of feasible LRP solution routes in form of MDP, which is an entire permutation of the vertices in the graph.
As depicted in Fig. \ref{MultiDepotMDP in Appendix}, the routes corresponding to the same depot have the identical start and end point, 
facilitating their aggregation into an entire permutation by jointing their identical depot.
Consequently, by linking together these permutations from all depots, a feasible solution can be finally formulated as an MDP.

\textit{\textbf{Remark 2:}} \textit{The MDP is a necessary mathematical formulation used to construct the feasible solution routes when engaging DRL method. 
Once the solution is derived in MDP form, it will be reverted to a set of routes for simultaneous execution by multiple vehicles.}

We define this MDP with a tuple $(\mathbf{S}, \mathbf{A}, \mathbf{P}, \mathbf{R}, \gamma)$, where, in each decision step $t$, the current iteration is represented by a tuple $(s_t, a_t, p_t, r_t, \gamma_t)$.

(a) $\mathbf{S}$ : is a set of states, wherein each state corresponds to a tuple $(G, D_t, \mathbf{v}_t, Q_t)$, 
where $G$ denotes entire static graph information;
$D_t$ indicates the depot which current route belongs to;
$\mathbf{v}_t$ signifies current customer in decision step $t$; 
$Q_t$ records remaining capacity on current vehicle; 
This tuple is updated at each decision step within MDP. 

(b) $\mathbf{A}$ : is a set of actions, wherein each action $a_t$ is the next point that current vehicle plans to serve.
In this problem configuration, to ensure that the MDP represents a feasible solution, 
actions should be selected from feasible points whose demands can be satisfied by current vehicle's remaining capacity. 
Upon selecting the $a_t$, the state tuple should be updated accordingly:

\begin{equation}
    \label{update Q and Q_A in depot}
    Q_{t+1} = 
    \begin{cases}
        Q_{t} - q_{e}     & \text{if $a_t \in \{\mathbf{v}_{S_e} | e = 1, 2, ‚Ä¶, n\}$,} \\
        Q                 & \text{if $a_t \in \{\mathbf{v}_{D_k} | k = 1, 2, ‚Ä¶, m\}$,}
    \end{cases}
\end{equation}

$a_t \in \{\mathbf{v}_{S_e} | e = 1, 2, ‚Ä¶, n\}$ indicates that current vehicle is scheduled to visit an unserved customer. 
Then, the remaining capacity $Q_t$ should be updated according to Eq. (\ref{update Q and Q_A in depot}), wherein $q_e$ represents the demand associated with the customer selected by action $a_t$.
Meanwhile, $a_t \in \{\mathbf{v}_{D_k} | k = 1, 2, ‚Ä¶, m\}$ indicates that current vehicle chooses to return to its departure depot, or start planning for a new depot.
Then, a new vehicle's route will commence from this depot, thereby the capacity $Q_t$ is refreshed to full state.


(c) $\mathbf{P}$ : is a set of probabilities, wherein each element $p_t$ represents the probability 
transiting from state $s_t$ to $s_{t+1}$ by taking action $a_t$, and $p_t$ can be expressed as: $p_t = p(s_{t+1} | s_t, a_t)$

(d) $\mathbf{R}$ : is a set of costs, wherein each element $r_t$ denotes the cost incurred by taking action $a_t$ in step $t$. 
The $r_t$ can be expressed as follows, where $d_{ij}$ denotes the length between $\mathbf{v}_i$ in step $t$ and $\mathbf{v}_j$ in step $t+1$:
% \begin{equation}
%     \label{total cost}
%      r_t = d_{ij}
%   \end{equation}
%   If $\mathbf{v}_i, \mathbf{v}_j \in \{\mathbf{v}_{D_k} | k = 1, 2, ‚Ä¶, m\}$, then:
%   \begin{equation}
%     \label{change between depots}
%      r_t = 0
%   \end{equation}
\begin{equation}
\label{change between depots and otherwise}
r_t = 
\begin{cases}
    0           & \text{if $\mathbf{v}_i, \mathbf{v}_j \in \{\mathbf{v}_{D_k} | k = 1, 2, ‚Ä¶, m\}$,} \\
    d_{ij}      & \text{otherwise,}
\end{cases}
\end{equation}

As is shown in Eq. (\ref{optimization objective details for M_I}), apart from this step-wisely accumulated transit distance, 
other costs used to depict the overall performance of the solution routes, which are not accumulated step-wisely, are added into the total cost after an entire MDP is generated. 
These additional overall costs include: 
\textit{(i) the opening cost for used depots; (ii) the setup cost for dispatched vehicles; (iii) penalty of exceeding depot desired maximum supply}.


(e) $\gamma \in [0,1]$ : the discount factor for cost in each step. Here, we presume no discount applies to the costs, i.e., $\gamma = 1$
% ################################ move to supplementary ######################################

\subsection{Multi-depot mask mechanism}\label{mdlram_msk}

In each decoding step, guided by the context embedding $\mathbf{h}_c^{t}$, 
the decoder produce the corresponding probabilities for all the feasible points within the selection domain.
This selection domain should exclude all the points that current vehicle cannot visit in next step, 
which is subject to vehicle capacity and current state in MDP.
Because the model processes problem instances in batches, simultaneous updates to their respective selection domains at each decoding iteration is necessary.

We identify four key scenarios to categorize the selection domain of each instance at any given step, 
based on the vehicle's location (depot or customer) and the completion status of delivery tasks. 
Specifically, these four potential patterns are summarized as follows:
%the selection domain in each step can be in following four patterns:
\begin{itemize}
    \item{
        % formulation constraints
        (i) When current vehicle is at a depot and all the customers' delivery tasks are finished:
        it can only stay at current depot.
    }
    \item{
        % formulation constraints and task constraints (select points or change depot)
        (ii) When current vehicle is at a depot but not all the customers' delivery tasks are finished:
        it can choose from the vertices set including all the unserved customers and unplanned depots but excluding current depot.
    }
    \item{
        % formulation constraints
        (iii) When current vehicle is at a customer and all the customers' delivery tasks are finished:
        this represents the current customer is the last delivery task, implying that the only selection is the vehicle's departure depot.
    }
    \item{
        % formulation constraints and task contraints (select go back or keep serving)
        (iv) When current vehicle is at a customer but not all the customers' delivery tasks are finished:
        it can choose from the vertices set including all the unserved customers and its departure depot.
    }
\end{itemize}
Based on these four patterns, the selection domain is updated before each decoding iteration. 
% This typically involves a mask mechanism to conceal infeasible points in each step.

As discussed, the model operates in batch-wise manner, necessitating simultaneous updating each instance's selection domain at each decoding iteration.
\textbf{The challenge is}, in each decoding step, the selection domain of each problem instance within one batch can be very different.
Thus, an efficient boolean mask matrix specific to the LRP scenario is devised for batch-wise manipulation on selection domain,
avoiding repeated operation on individual problem instance.
%which means before each decoding, a batch of problem instances should update their respective selection domain simultaneously. 
%Therefore, a specific efficient mask mechanism under multi-depot scenario should be designed to manipulate a batch of selection domain before each decoding step.

The Algorithm \ref{alg:algorithm for multi-depot mask mechanism} specifies our mask mechanism specifically tailored for LRP scenario. 
which includes manipulations on the selection domain of customers and depots.
% removed 2024-2-28 for conf
Firstly, by masking the customers which have been served or cannot be satisfied with remaining capacity, the selection domain of customers can be simply derived.
Crucially, for the depot selection domain,
we notice that among the four patterns above: three patterns (i, iii, and iv) include only the departure depot, whereas one pattern (ii) excludes the departure depot.
Thus, at each decoding step for a batch of instances, we initially mask all the depots unanimously and only reveal their departure depot of current routes.
Then, we identify the problem instances belonging to pattern-ii in this batch, mask the departure depots and reveal the unplanned depots.
All the manipulations operate in batches to avoid repeated operation on individual problem instance.

% reduced on 23-11-17
% add algorithm 4-1
\begin{algorithm}[htpb]                                   
    \caption{Mask Mechanism for batch-wise manipulation on selection domain for a batch of problem instances}
    \label{alg:algorithm for multi-depot mask mechanism}                                   
    \textbf{Input}: A batch of problem instances with Batch Size $B$\\
    \begin{algorithmic}[1]
    %\STATE ${\rm Rec} = \[\sigma_{ij}\] \in \mathbb{R}^{B_{main} \times (m+n)}$ where $\sigma_{ij} \in \{0, 1\}$ representing visited vertex $\sigma_{ij} = 0$ or unvisited vertex $\sigma_{ij} = 1$
    \STATE \textbf{Init} ${\rm Record} = [\sigma_{ij}] \in \mathbb{R}^{B \times (m+n)}$ where $\sigma_{ij} \in \{0, 1\}$ representing, in problem instance $i$, whether the vertex $j$ is visited ($\sigma_{ij} = 0$) or unvisited ($\sigma_{ij} = 1$)
    \STATE \textbf{Init} ${\rm ID} \in \mathbb{R}^{B}$ current situated vertices for all instances
    \STATE \textbf{Init} ${\rm DP} \in \mathbb{R}^{B}$ current departure depots for all instances
    \FOR{each decoding step $t = 1,2,...$}
    \STATE $\{\varphi_i\} \leftarrow$ Batch No. for the problem instances where not all the tasks are finished
    \STATE $\{\varphi_j\} \leftarrow$ Batch No. for the problem instances where all the tasks are finished
    \STATE $\sigma_{ij} \leftarrow 0$ according to the ${\rm ID}_t$
    \STATE $({\rm Mask}_0)_{ij} \leftarrow True\ if\ \sigma_{ij} = 0,  ({\rm Mask}_0)_{ij} \leftarrow False\ if\ \sigma_{ij} = 1$
    \STATE $({\rm Mask}_1)_{ij} \leftarrow True\ if\ (Q_t)_i < (q_e)_{j},  ({\rm Mask}_0)_{ij} \leftarrow False\ if\ (Q_t)_i > (q_e)_{j}$
    \STATE ${\rm Mask} \leftarrow {\rm Mask}_0 + {\rm Mask}_1$
    \STATE $({\rm Mask})_{ij} \leftarrow True\ for\ all\ j \in \{0,1,...,m-1\}$
    \STATE $({\rm Mask})_{ij} \leftarrow False $ according to the ${\rm DP}_t$
    \STATE $\{\varphi_k\} \leftarrow$ Batch No. for the problem instances where current vertex is one of the depots
    \STATE $\{\varphi_e\} \leftarrow \{\varphi_i\} \cap \{\varphi_k\}$ Batch No. for the problem instances where current vertex is one of the depots and not all tasks are finished
    \STATE $({\rm Mask})_{ij} \leftarrow False$ where $i \in \{\varphi_e\}$ and $j \in \{0,1,...,m-1\}$
    \STATE $({\rm Mask})_{ij} \leftarrow True$ where $i \in \{\varphi_e\}$ and ${\rm DP}_{\varphi_e} \in \{0,1,...,m-1\}$
    \STATE $({\rm Mask})_{ij} \leftarrow True$ where $j \in \{0,1,...,m-1\}$ and $\sigma_{ij} = 0$
    \ENDFOR
    \STATE \textbf{Return} ${\rm Mask}$
    \end{algorithmic}
    \end{algorithm}

\subsection{MDLRAM's pre-training \& DGM's dual-mode training}\label{training_pseudo}
\begin{algorithm}[H]                                   
    \caption{ Pre-training for MDLRAM}
    \label{alg:algorithm for CLRP-S}                                   
    \textbf{Input}:  $M$ batches of problem instances with Batch Size $B$\\
    \begin{algorithmic}[1]
    % \STATE $B$: BatchSize $Num$: Batch Amount
    \FOR{each epoch $ep = 1,2,...,100$}
    \FOR{each batch $bt = 1,2,...,M$}
    \STATE  $\{G_b | b = 1,2,...,B\} \leftarrow$ A Batch of Cases
    \STATE $\{A_b^{\theta_{\text{I}}} | b = 1,2,...,B\} \leftarrow$ ${\rm MDLRAM_{\theta_I}}(\{G_b\})$
    \STATE $\{A_b^{\theta_{\text{I}}^*} | b = 1,2,...,B\} \leftarrow$ ${\rm MDLRAM_{\theta_I^*}}(\{G_b\})$
    \STATE $\nabla\mathcal{L}(\boldsymbol{\theta_{\text{I}}}) \leftarrow \frac{1}{B}\sum_{b=1}^{B}[(L(A_b^{\theta_{\text{I}}})-L(A_b^{\theta_{\text{I}}^*}))\nabla\log p_{\boldsymbol{\theta_{\text{I}}}}(A_b^{\theta_{\text{I}}})]$
    \IF {One Side Paired T-test $(A_b^{\theta_{\text{I}}}, A_b^{\theta_{\text{I}}^*}) < 0.05$}
    \STATE $\theta_{\text{I}}^* \leftarrow \theta_{\text{I}}$
    \ENDIF
    \ENDFOR
    \ENDFOR
    \end{algorithmic}
    \end{algorithm}
The baseline $\bar{\mathcal{B}} $ in Algorithm \ref{alg:algorithm for CLRP-S} is established through a parallel network mirroring the structure of MDLRAM, persistently preserving the best parameters attained and remaining fixed. 
Parameters' update solely occurs if a superior evaluation outcome is derived by MDLRAM, enabling baseline network's adoption of these improved parameters from MDLRAM. 
The actions in MDPs produced by MDLRAM is selected with probabilistic sampling in each decoding step, whereas that of baseline network is greedily selected based on the maximum possibility.
% ######################################### move to supplementray ###############################################

% add algorithm 4-3
\begin{algorithm}[htpb]                                   
    \caption{Dual-mode training for DGM, coupled with pretrained MDLRAM functioning as a critic model}
    \label{alg:algorithm for CLRP-G with two modes}                                   
    \textbf{Input}: Batches of problem instances with Batch Size $B_{\text{main}}$\\
    \begin{algorithmic}[1]
    % \STATE $B$: BatchSize $Num$: Batch Amount
    \IF{in Multivariate Gaussian Distribution mode}
    \FOR{each epoch $ep = 1,2,...,100$}
    \FOR{each batch $bt = 1,2,...,M$}
    \STATE $\{G_{b} | b = 1,2,...,B_{\text{main}}\} \leftarrow$ A Main-Batch of graphs with customers Info
    \STATE $\{\mathcal{N}_b^{\theta_{\text{II}}} | b = 1,2,...,B_{\text{main}}\} \leftarrow$ ${\rm DGM_{\theta_{\text{II}}}}(\{G_b\})$
    \FOR{each graph $b = 1,2,...,B_{\text{main}}$}
    \STATE $\{\mathcal{D}_{\text{multiG}}^{(b')} | {b'} = 1,2,...,B_{\text{sub}}\} \leftarrow$ A Sub-Batch of sampled depot sets
    \STATE $\nabla L_{DGM}(\mathcal{N}_b) \leftarrow \mathbb{E}_{p_{\boldsymbol{\theta_{\text{II}}}}(\mathcal{D}_{\text{multiG}})}^{(b)}[{\rm MDLRAM}(\mathcal{D}_{\text{multiG}}^{(b')}, G_{b})$
    %\STATE $\mathbb{E}_{p_{\boldsymbol{\theta_{II}}}(\mathcal{D}_{multiG})}^j[{\rm MDAM}(\mathcal{D}_{multiG}, G_{j})$
    \STATE $\cdot \nabla\log p_{\boldsymbol{\theta_{\text{II}}}}(\mathcal{D}_{\text{multiG}}^{(b')})]$
    \ENDFOR
    \STATE $\nabla\mathcal{L}(\boldsymbol{\theta_{\text{II}}}) \leftarrow \frac{1}{B_{\text{main}}}\sum_{b = 1}^{B_{\text{main}}}\nabla L_{DGM}(\mathcal{N}_b)$
    \ENDFOR
    \ENDFOR
    \ELSIF{in Exact Position mode}
    \FOR{each epoch $ep = 1,2,...,100$}
    \FOR{each batch $bt = 1,2,...,M$}
    \STATE $\{G_{b} | b = 1,2,...,B_{\text{main}}\} \leftarrow$ A Main-Batch of graphs with customers Info
    \STATE $\{\mathcal{D}_{\text{exactP}}^{(b)} | b = 1,2,...,B_{\text{main}}\} \leftarrow {\rm DGM_{\theta_{\text{II}}}}(\{G_j\})$
    \STATE $\nabla\mathcal{L}(\boldsymbol{\theta_{\text{II}}}) \leftarrow \frac{1}{B_{\text{main}}}\sum_{b = 1}^{B_{\text{main}}}\nabla {\rm MDLRAM}((\mathcal{D}_{\text{exactP}}^{(b)})_{\boldsymbol{\theta_{\text{II}}}}, G_{b})$
    \ENDFOR
    \ENDFOR
    \ENDIF
    \end{algorithmic}
    \end{algorithm}


\section{Extended details about Experimental Results}

\subsection{Hyperparameters Details}\label{hyper_para}

For MDLRAM, we train it for 100 epochs with training problem instances generated on the fly, 
which can be split into 2500 batches with batchsize of 512 (256 for scale $100$ due to device memory limitation).
% ############## reduce for ECML 2024-3-18 ##################
Within each epoch, by going through the training dataset, MDLRAM will be updated 2500 iterations.
After every 100 iterations, the MDLRAM will be assessed on an evaluation dataset to check whether improved performance is attained.
The evaluation dataset consists of 20 batches of problem instances, with the same batch size of 512(256). 
% ############## reduce for ECML 2024-3-18 ##################

For DGM, we also train it for 100 epochs. In each epoch, 2500 main-batches of problem instances are iteratively fed into DGM.
\textbf{In multivariate Gaussian distribution mode}, the main-batch size $B_{\text{main}}$ is set as 32 (16 for scale $100$),  
and the sub-batch size $B_{\text{sub}}$ for sampling in each distribution is selected as 128, 64, 32 for scale $20, 50, 100$ respectively.
% ############## reduce for ECML 2024-3-18 ##################
During training, after every 100 iterations' updating, the DGM will be evaluated on an evaluation dataset to check if a better performance is derived.
The evaluation dataset is set as 20 main-batches of problem instances, maintaining the same batch size $B_{\text{main}}$ and $B_{\text{sub}}$.
% ############## reduce for ECML 2024-3-18 ##################
\textbf{In exact position mode}, where no sampling is performed, we set main-batch size as 512 (256 for scale $100$).
% ############## reduce for ECML 2024-3-18 ##################
Likewise, after every 100 iterations' updating, an evaluation process is conduct on 20 main-batches of problem instances with corresponding batch size of 512 (128) to check if DGM achieves a better performance.
% The evaluations for DGM follow the same 100-iteration cycle as MDLRAM.
% ############## reduce for ECML 2024-3-18 ################## 

As for the hyperparameters in model architecture across the entire framework, the encoding process employs $N = 3$ attention modules with 8-head MHA sublayers, featuring an embedding size of 128. 
All the training sessions are finished on one single A40 GPU.

Parameters for heuristic methods in Table~\ref{test results of model I on synthetic dataset}: 
\textbf{(a)} Adaptive Large Neighborhood Search (ALNS): 
\textit{Destroy (random percentage $0.1\sim0.4$, worst nodes $5\sim10$); Repair (random, greedy, regret with 5 nodes); Rewards ($r_1=30, r_2=20, r_3=10, r_4=-10$); Operators weight decay rate: $0.4$; Threshold decay rate: $0.9$};
\textbf{(b)} Genetic Algorithm (GA): 
\textit{Population size: 100; Mutation probability: 0.2; Crossover probability: 0.6;}
\textbf{(c)} Tabu Search (TS): 
\textit{Action Strategy (1-node swap, 2-node swap, Reverse 4 nodes); Tabu step: $30$};


\subsection{Visualize Depots Distribution:}\label{visual_distribute}
DGM's distribution mode is trained to understand correlations between coordinates of various depots, 
manifested as their learnable covariances.
To visualize the distribution generated in the Gaussian mode of DGM and 
observe how this multivariate Gaussian distribution is represented in a 2-D graph,
we depict the generated multivariate Gaussian distribution for problem instances from all three scales. 
A notable pattern is revealed as below:

\begin{figure*}[thbp]
    \centering
    \vspace{-20pt} 
    \includegraphics[width=10.0cm]{ML_images/tst_4_cy23_clip.png} 
    \caption{Visualization of Multivariate Gaussian Distribution outputted by DGM based on customer requests (Gray): Predicted Depot Distribution (Blue), and Optimal Depots Identified (Red).}
    \label{depot distribution heatmap in Appendix}
\end{figure*}

In the problem scale of $m=3, n=20$, the 6-D normal distribution tends to present as three separate 2-D normal distributions, as depicted in Fig. \ref{depot distribution heatmap in Appendix}. 
However, as the problem scales increase, such as the 12-D ($m=6, n=50$) or 18-D ($m=9, n=100$) normal distributions, they do not tend to present as several discrete 2-D normal distributions. 

\textit{\textbf{This trend indicates that}}, in large-scale scenario, the covariance between coordinates from different depots exhibit a more complex relationship, 
% indicating, for any two depots in larger scale $(x_a, y_a)$ and $(x_b, y_b)$, 
% the covariances of each pair of coordinates among $x_a, y_a, x_b, y_b$ may present more complicated format than in small scales.
which further implies that simply relying on randomly sampling depots in pursuit of covering optimal depots would require an expansive search and substantial computational effort.


\subsection{MDLRAM's ability on balancing route length among depots}\label{mdlram_bln}
With MDLRAM's structure,
% it can be easy to fine tune the model parameters to adapt to different problem configuration related to multi-depot scenario by designing specific reward (cost) function.
% In this paper, we consider the route balancing problem among different depots.
fine-tuning the model to align with diverse additional requirements associated to the multiple depots in LRP scenario is flexible through designing specialized cost functions. 
Here, we examine the route balancing challenge among various depots.

If the objective is to maintain the route length $l_k(\mathbf{A})$ associated with each depot $D_k$ ($k \in \{1, 2, ‚Ä¶, m\}$) in a specific proportional relationship, 
namely $l_1(\mathbf{A}) : l_2(\mathbf{A}) : \ldots : l_m(\mathbf{A}) = \rho_1 : \rho_2 : \ldots : \rho_m$, while simultaneously minimizing the overall cost $L_{\text{Sel}}(\mathbf{A})$, 
it can be achieved by augmenting the cost function $L_{\text{Sel}}(\mathbf{A})$ in Eq. (\ref{optimization objective details for M_I}) with a balance penalty as follows: 
% $\tilde{L} _{\text{Sel}}(\mathbf{A}) = L_{\text{Sel}}(\mathbf{A}) + \sum_{k=1}^{m}\sum_{k'=k}^{m}|l_k(\mathbf{A}) - \frac{\rho_k}{\rho_{k'}}l_{k'}(\mathbf{A})|$
\begin{equation}
    \label{reward for depot balancing}
    \tilde{L} _{\text{Sel}}(\mathbf{A}) = L_{\text{Sel}}(\mathbf{A}) + \sum_{k=1}^{m}\sum_{k'=k}^{m}|l_k(\mathbf{A}) - \frac{\rho_k}{\rho_{k'}}l_{k'}(\mathbf{A})|
\end{equation}

% greedy: dis:5.5474 rt: 1.0234, 1.0236, 1.0219;    sample: dis:5.64018 rt: 1.0399, 1.0148, 1.0609
To evaluate the adaptability of MDLRAM in addressing LRP with additional requirements on adjusting inter-depot cost distribution, we fine-tune the MDLRAM, which has been pre-trained with original objective $L_{\text{Sel}}(\mathbf{A})$ in Eq. (\ref{optimization objective details for M_I}), 
with this new balance-oriented objective $\tilde{L} _{\text{Sel}}(\mathbf{A})$ in Eq. (\ref{reward for depot balancing}) on the same training dataset. 
% Subsequently, we test this fine-tuned Model-I using both sampling and greedy methods across all three scales.
In this context, our specific goal is to ensure that the lengths belonging to each depot are approximately equal (i.e., $\rho_k=1$).
% which targets on minimizing the second item in (\ref{reward for depot balancing}), 
% while maintaining the original cost (first item in (\ref{reward for depot balancing})) at a low level. 
% To achieve this, we set $\rho_k=1$. 
Notably, $\rho_k$ can be adjusted based on specific proportion requirements.

To illustrate the effectiveness of balance-oriented fine-tuning, we select random cases from each scale for direct comparison of route length belonging to each depot, generated by MDLRAM under different objectives. In Table~\ref{fine-tune-depot-balance},
it can be observed that, for each case, 
the balance penalty of solution routes found by MDLRAM under balance-oriented objective Eq. (\ref{reward for depot balancing}) is conspicuously smaller than that of original objective Eq. (\ref{optimization objective details for M_I}), 
only incurring a slight wave on the total length as an acceptable trade-off for incorporating the additional item in the balance-oriented objective function.
This can also be directly reflected by the balanced route length distribution across depots in 5th column of Table~\ref{fine-tune-depot-balance}.

\begin{table}[H]
    \renewcommand\arraystretch{0.4}
    \setlength{\tabcolsep}{2.7pt}
    \caption{Comparison of Each Depot's Route Length, respectively planned by Original MDLRAM and the Fine-tuned Version. 
    (``Obj.'': Objective Function; ``Ori.Obj.'': Original Objective Function in Eq. (\ref{optimization objective details for M_I}); ``Bln.Obj.'': Balance-oriented Objective Function in Eq. (\ref{reward for depot balancing}); ``Bln.Pen.'': penalty for measuring the balancing performance of route length among depots; ``Dpt.Nb.'': opened depot number out of total available depots).}
    \label{fine-tune-depot-balance}
    \begin{center}
    %\resizebox{90mm}{!}{
    \begin{tabular}{l|r|l|c|c|c|c}
        \toprule
    \midrule
        &  Case &         Obj. &  \textbf{Bln. Pen.} &  (Dpt Nb.) &  \textbf{Saperate Depot Len.} &  Total Len. \\
    \midrule
    \midrule
    \multirow{15}{*}{\rotatebox{90}{\textbf{scale 20}}} &
%   \multirow{2}{*}{case1} &
%                       Ori obj.                           &0.391                  &2/3        &3.928-4.319       &8.247       \\
%    &         &  \textbf{Bln obj.}                    &\textbf{0.004}         &2/3        &4.132-4.127       &8.259       \\
%   \cmidrule(r){2-7}
    \multirow{2}{*}{case1} &
                        Ori obj.                           &0.758                  &2/3        &3.487-2.729       &6.216       \\
    &         &  \textbf{Bln obj.}                    &\textbf{0.008}         &2/3        &2.781-2.772       &5.554       \\
    \cmidrule(r){2-7}
    &\multirow{2}{*}{case2} &
                        Ori obj.                           &0.929                  &2/3        &3.439-2.511       &5.951       \\
    &         &  \textbf{Bln obj.}                    &\textbf{0.007}         &2/3        &3.022-3.016       &6.038       \\
    \cmidrule(r){2-7}
    &\multirow{2}{*}{case3} &
                        Ori obj.                           &0.926                  &2/3        &3.608-2.682       &6.290       \\
    &         &  \textbf{Bln obj.}                    &\textbf{0.022}         &2/3        &3.123-3.102       &6.225       \\
    \cmidrule(r){2-7}
    &\multirow{2}{*}{case4} &
                        Ori obj.                           &0.693                   &2/3        &2.853-2.159       &5.012       \\
    &         &  \textbf{Bln obj.}                    &\textbf{0.0002}         &2/3        &2.518-2.518       &5.036       \\
%   \midrule
%   \midrule
\cmidrule[0.1em](r){1-7}
    \multirow{15}{*}{\rotatebox{90}{\textbf{scale 50}}} &
    \multirow{2}{*}{case1} &
                        Ori obj.                           &3.131                  &4/6        &2.158-2.536-2.155-3.073       &9.922       \\
    &         &  \textbf{Bln obj.}                    &\textbf{0.129}         &4/6        &2.492-2.530-2.521-2.507       &10.052      \\
    \cmidrule(r){2-7}
    &\multirow{2}{*}{case2} &
                        Ori obj.                           &3.738                  &4/6        &2.150-3.154-2.947-2.220       &10.471       \\
    &         &  \textbf{Bln obj.}                    &\textbf{0.283}         &4/6        &2.449-2.434-2.473-2.383       &9.739       \\
    \cmidrule(r){2-7}
    &\multirow{2}{*}{case3} &
                        Ori obj.                           &2.016                  &3/6        &2.981-2.579-3.586       &9.146       \\
    &         &  \textbf{Bln obj.}                    &\textbf{0.067}         &3/6        &3.085-3.091-3.058       &9.234       \\
    \cmidrule(r){2-7}
%  &\multirow{2}{*}{case4} &
%                       Ori obj.                           &1.163                  &3/6        &2.881-2.618-3.199       &8.698       \\
%    &         &  \textbf{Bln obj.}                    &\textbf{0.143}         &3/6        &2.987-2.978-2.915       &8.880       \\
%   \cmidrule(r){2-7}
    &\multirow{2}{*}{case4} &
                        Ori obj.                           &2.416                  &4/6        &1.808-2.596-1.918-1.969       &8.292       \\
    &         &  \textbf{Bln obj.}                    &\textbf{0.176}         &4/6        &2.190-2.186-2.163-2.220       &8.759       \\
%   \midrule
%   \midrule
\cmidrule[0.1em](r){1-7}
    \multirow{15}{*}{\rotatebox{90}{\textbf{scale 100}}} &
%   \multirow{2}{*}{case1} &
%                       Ori obj.                           &5.414                  &5/9        &3.061-2.215-2.563-3.179-2.281       &13.299       \\
%    &         &  \textbf{Bln obj.}                    &\textbf{0.363}         &5/9        &2.802-2.833-2.861-2.786-2.824       &14.107       \\
%   \cmidrule(r){2-7}
    \multirow{2}{*}{case1} &
                        Ori obj.                           &3.444                  &5/9        &2.728-3.132-2.496-3.092-2.642       &14.091       \\
    &         &  \textbf{Bln obj.}                    &\textbf{0.916}         &5/9        &2.736-2.742-2.829-2.842-2.915       &14.063       \\
    \cmidrule(r){2-7}
    &\multirow{2}{*}{case2} &
                        Ori obj.                           &2.495                  &5/9        &3.008-3.344-3.063-3.487-3.353       &16.256       \\
    &         &  \textbf{Bln obj.}                    &\textbf{0.373}         &5/9        &3.045-3.015-2.987-2.987-2.967       &15.001       \\
    \cmidrule(r){2-7}
    &\multirow{2}{*}{case3} &
                        Ori obj.                           &5.310                  &5/9        &3.743-2.622-2.985-3.335-2.922       &15.606       \\
    &         &  \textbf{Bln obj.}                    &\textbf{1.641}         &5/9        &3.043-3.099-3.056-3.249-3.358       &15.808       \\
    \cmidrule(r){2-7}
    &\multirow{2}{*}{case4} &
                        Ori obj.                           &8.711                  &5/9        &3.273-3.398-4.455-2.599-2.754       &16.479       \\
    &         &  \textbf{Bln obj.}                    &\textbf{1.896}         &5/9        &3.492-3.465-3.404-3.709-3.755       &17.825       \\
%  \midrule
%   \multirow{2}{*}{case16} &
%                      DBAM &        5 &      4.34 -      4.17 -      4.11 -      3.40 -      1.76  &      17.77 &      4.70 &        11.85 \\
%            &  \textbf{DBAM-bln} &        5 &      \textbf{4.69 -      3.90 -      3.21 -      3.76 -      4.27}  &      19.83 &      4.44 &         \textbf{6.91} \\
%   \midrule
%   \multirow{2}{*}{case17} &
%                      DBAM &        5 &      5.24 -      5.37 -      4.20 -      3.45 -      2.28  &      20.54 &      3.60 &        15.93 \\
%            &  \textbf{DBAM-bln} &        5 &      \textbf{4.08 -      3.59 -      4.47 -      3.35 -      4.06}  &      19.54 &      7.48 &         \textbf{5.43} \\
%   \midrule
%   \multirow{2}{*}{case18} &
%                      DBAM &        6 &      4.92 -      3.88 -      2.99 -      3.07 -      2.73 -      0.35 &      17.94 &      4.73 &        26.38 \\
%            &  \textbf{DBAM-bln} &        5 &      \textbf{3.73 -      4.17 -      3.51 -      3.05 -      5.17}  &      19.63 &      5.22 &         \textbf{9.81} \\
%   \midrule
%   \multirow{2}{*}{case19} &
%                      DBAM &        5 &      4.25 -      5.38 -      2.78 -      2.97 -      3.59  &      18.96 &      7.08 &        12.96 \\
%            &  \textbf{DBAM-bln} &        5 &      \textbf{3.64 -      3.66 -      3.62 -      2.88 -      3.79}  &      17.59 &      9.65 &         \textbf{3.73} \\
%   \midrule
%   \multirow{2}{*}{case20} &
%                      DBAM &        5 &      5.12 -      4.48 -      5.20 -      2.99 -      2.23  &      20.03 &      8.26 &        16.15 \\
%            &  \textbf{DBAM-bln} &        5 &      \textbf{4.66 -      4.11 -      4.14 -      4.93 -      3.47}  &      21.32 &      8.62 &         \textbf{6.95} \\
%   \midrule
%   \multirow{2}{*}{case21} &
%                      DBAM &        5 &      4.79 -      5.82 -      4.70 -      3.94 -      2.52  &      21.78 &     10.72 &        14.90 \\
%            &  \textbf{DBAM-bln} &        5 &      \textbf{4.98 -      4.84 -      4.01 -      3.63 -      4.84}  &      22.29 &     10.97 &         \textbf{7.05} \\
%   \midrule
%   \multirow{2}{*}{case22} &
%                      DBAM &        5 &      5.11 -      4.33 -      4.34 -      2.62 -      2.08  &      18.48 &      7.80 &        15.53 \\
%            &  \textbf{DBAM-bln} &        5 &      \textbf{4.19 -      3.65 -      3.51 -      4.24 -      4.09}  &      19.67 &      7.40 &         \textbf{3.99} \\
%   \midrule
%   \multirow{2}{*}{case23} &
%                      DBAM &        5 &      4.51 -      5.69 -      4.79 -      4.16 -      2.29  &      21.44 &      4.42 &        14.87 \\
%            &  \textbf{DBAM-bln} &        5 &      \textbf{4.65 -      4.77 -      4.03 -      3.26 -      3.87}  &      20.58 &      7.78 &         \textbf{7.58} \\
%   \midrule
%   \multirow{2}{*}{case24} &
%                      DBAM &        5 &      5.36 -      5.91 -      5.67 -      5.19 -      1.45  &      23.58 &      9.45 &        18.80 \\
%            &  \textbf{DBAM-bln} &        5 &      \textbf{4.51 -      4.78 -      5.28 -      5.56 -      4.03}  &      24.15 &     14.62 &         \textbf{7.66} \\
%   \midrule
%   \multirow{2}{*}{case25} &
%                      DBAM &        5 &      4.71 -      4.89 -      5.40 -      4.87 -      1.14  &      21.00 &      4.78 &        17.42 \\
%            &  \textbf{DBAM-bln} &        5 &      \textbf{4.61 -      3.99 -      3.66 -      4.45 -      3.85}  &      20.56 &      6.77 &         \textbf{5.05} \\
%   \midrule
%   \multirow{2}{*}{case26} &
%                      DBAM &        5 &      4.89 -      4.10 -      5.09 -      4.46 -      2.23  &      20.76 &     10.36 &        13.06 \\
%            &  \textbf{DBAM-bln} &        5 &      \textbf{5.21 -      4.91 -      4.22 -      4.02 -      4.27}  &      22.63 &      9.49 &         \textbf{6.12} \\
%   \midrule
%   \multirow{2}{*}{case27} &
%                      DBAM &        5 &      4.71 -      4.72 -      3.62 -      3.25 -      1.81  &      18.12 &      4.21 &        14.56 \\
%            &  \textbf{DBAM-bln} &        5 &      \textbf{3.96 -      3.10 -      3.32 -      3.45 -      3.51}  &      17.35 &      4.25 &         \textbf{3.80} \\
%   \midrule
%   \multirow{2}{*}{case28} &
%                      DBAM &        5 &      4.67 -      5.53 -      3.57 -      3.00 -      2.95  &      19.73 &      8.33 &        13.65 \\
%            &  \textbf{DBAM-bln} &        5 &      \textbf{4.78 -      3.83 -      4.06 -      3.21 -      4.28}  &      20.16 &      8.69 &         \textbf{7.20} \\
%   \midrule
%   \multirow{2}{*}{case29} &
%                      DBAM &        5 &      4.49 -      4.30 -      4.29 -      3.03 -      2.20  &      18.31 &      7.08 &        11.67 \\
%            &  \textbf{DBAM-bln} &        5 &      \textbf{4.47 -      3.52 -      3.54 -      3.95 -      3.23}  &      18.72 &      7.39 &         \textbf{5.80} \\
%   \midrule
%   \multirow{2}{*}{case30} &
%                      DBAM &        5 &      5.90 -      4.80 -      3.94 -      3.78 -     2.34  &      20.76 &      4.99 &        16.29 \\
%            &  \textbf{DBAM-bln} &        5 &      \textbf{4.87 -      4.09 -      4.51 -      3.77 -      3.95}  &      21.19 &      5.27 &         \textbf{5.52} \\
%   \midrule
%   \multirow{2}{*}{case31} &
%                      DBAM &        5 &      5.86 -      5.97 -      5.12 -      4.59 -      1.98  &      23.51 &      6.88 &        18.50 \\
%            &  \textbf{DBAM-bln} &        5 &      \textbf{5.47 -      4.41 -      4.48 -      4.27 -      4.46}  &      23.09 &      7.86 &         \textbf{4.97} \\
%   \midrule
%   \multirow{2}{*}{case32} &
%                      DBAM &        6 &      4.96 -      4.47 -      4.76 -      3.97 -      2.19 -      0.13 &      20.48 &      1.21 &        32.34 \\
%            &  \textbf{DBAM-bln} &        5 &      \textbf{4.02 -      4.71 -      4.14 -      4.58 -      3.66}  &      21.11 &      3.95 &         \textbf{5.30} \\
%   \midrule
%   \multirow{2}{*}{cas33} &
%                      DBAM &        5 &      4.57 -      3.28 -      3.54 -      2.73 -      1.38  &      15.49 &      9.04 &        14.37 \\
%            &  \textbf{DBAM-bln} &        5 &      \textbf{3.85 -      3.90 -      2.49 -      3.91 -      2.88}  &      17.03 &      9.13 &         \textbf{7.72} \\
%   \midrule
%   \multirow{2}{*}{case34} &
%                      DBAM &        5 &      4.64 -      3.83 -      4.50 -      4.04 -      0.77  &      17.78 &      3.48 &        16.82 \\
%            &  \textbf{DBAM-bln} &        5 &      \textbf{4.58 -      4.14 -       4.02 -      3.91 -      3.24}  &      19.89 &      2.30 &         \textbf{5.82} \\
%   \midrule
%   \multirow{2}{*}{case35} &
%                      DBAM &        5 &      4.53 -      4.27 -     4.49 -      4.11 -      1.61  &      19.01 &      5.70 &        12.45 \\
%            &  \textbf{DBAM-bln} &        5 &      \textbf{3.96 -      4.54 -      4.98 -      4.37 -      4.03}  &      21.88 &      4.76 &         \textbf{5.07} \\   
    \bottomrule
    \end{tabular}
    %}
    \end{center}
    \end{table}

\subsection{Further Discussion} \label{discussion}

% ######## Problem recap ######## :
In this study, we extend the exploration of the LRP by addressing a real-world challenge: 
the generation of depots when no predefined candidates are presented. 
For this purpose, a generative DRL framework comprising two models is proposed.
Specifically, the DGM, based on customer requests data,
enables proactive depot generation with dual operational modes flexibly-
the exact mode ensures precision when necessary, while the Gaussian mode introduces sampling variability, 
enhancing the model's generalization and robustness to diverse customer distributions.
Meanwhile, the MDLRAM subsequently facilitates rapid planning of LRP routes from the generated depots for serving the customers, minimizing both depot-related and route-related costs.
% ######## Contribution: technical + application ######## 
Our framework represents a transition from traditional depot selection to proactive depot generation, 
showcasing cost reductions and enhanced adaptability in real-world scenarios like disaster relief, 
which necessitates quick depot establishment and flexible depot adjustment. 

The framework's detachability offers flexible extension for its application.
The DGM's depot-generating ability can be fine-tuned to adapt different LRP variants by jointing with other downstream models,
making DGM a versatile tool in real-world logistics.
Meanwhile, the end-to-end nature of MDLRAM enable its flexible usage on addressing LRP variants with requirements of adjusting inter-depot cost distribution, 
which has been detailed in Appendix \ref{mdlram_bln}.

Based on the framework design details and the application scenario description, 
we spot following limitatioins and arranging a research landscape for future works.

\textbf{Limitation:}
% Although MDLRAM has the ability to selected flexible desired number of depots from the generated depot set when planning routes for vehicle from the generated depot set, 
% the depot number generated from DGM is currently set fixed during training.
% The realization for adaptive decision on depots generation may need a more conjugated integration into the MDLRAM's route planning process. 
% %%%%% polished version
While the MDLRAM model has the ability to select a flexible number of depots from the generated depot set when planning routes for vehicle from the generated depot set,
the number of depots generated by the DGM is currently set fixed during training. 
Incorporating an adaptive mechanism within the DGM to dynamically determine the optimal number of depots based on customer demands and logistical factors could further enhance the framework's flexibility and efficiency. 
Achieving this adaptive depot generation may require a more conjugated and interactive integration between the DGM and the MDLRAM's route planning process.

\textbf{Future work:}
Future research will focus on expanding DGM's applicability by incorporating a wider range of depot constraints to reflect more real-world scenarios accurately.
For example, in this study, we consider the distance between depots should adhere to a specific range requirements, preventing the depots from being too close or too distant with each other.
Additional constraints on depots can be emphasized on the forbidden area within the map, such as ensuring the depots are not situated in specific regions or must be placed within designated zones.

% Meanwhile, leveraging the framework's detachability to adapt to various routing tasks and accommodating multiple concurrent routing tasks presents an exciting research avenue, 
% further extending the framework's utility in real-world logistics.

Additionally, leveraging the framework's modular design to adapt to various routing tasks presents an exciting avenue for exploration. 
This includes generating depots which can generally achieve satisfying performance across multiple concurrent routing tasks, 
which would further extend the framework's utility in complex and dynamic real-world logistics environments.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}
