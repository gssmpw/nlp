\section{Related Work}
\textbf{Long-Tailed Recognition.} 
Addressing the challenges posed by long-tailed data distributions has been a critical area of research in machine learning, for both classification and regression problems. Traditional methods, such as re-sampling and re-weighting techniques, have been used to mitigate class imbalances by either over-sampling minority classes or assigning higher weights to them during training~\citep{chawla2002smote, he2009learning, torgo2013smote, branco2017smogn, branco2018rebagg}. 
Such algorithms fail to measure the distance in continuous label space and fall short in handling high-dimensional data (e.g., images and text). 
Deep imbalanced regression methods~\cite{yang2021delving,ren2022balanced,gong2022ranksim,keramati2023conr,wang2024variational} address this challenge by reweighting the data using the effective label density during representation learning. 
However, all methods above are designed for \emph{recognition} tasks such as classification and regression, and are therefore not applicable to our \emph{generation} task. 

\textbf{Diffusion Models Related to Long-Tailed Data.} 
There are also works that related to both diffusion models and long-tailed data. They aim at improving generation robustness using noisy label~\cite{na2024label}, improving fairness in image generation~\cite{shen2023finetuning}, and improving classification accuracy using diffusion models~\cite{zhang2024long}. However, these works have different goals and therefore are not applicable to our setting. 

Most relevant to our work is Class Balancing Diffusion Model (CBDM)~\citep{qin2023class}, which uses a distribution adjustment regularizer that enhances tail-class generation based on the modelâ€™s predictions for the head class. 
It improves the quality of long-tailed generation by assuming one-hot conditional labels (i.e., classification-based settings). 
However, this assumption does not generalize to the modern setting where image generation is usually conditioned on free-form text prompts. As a result, when adapted to the free-form setting, they often fail to model the similarity among different text prompts, leading to suboptimal generation performance in minority data (as verified by empirical results in~\secref{sec:experiments}).