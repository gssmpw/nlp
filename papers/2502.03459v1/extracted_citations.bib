@article{ActionCLIP,
  title   = {ActionCLIP: A New Paradigm for Video Action Recognition},
  author  = {Mengmeng Wang and Jiazheng Xing and Yong Liu},
  journal = {ArXiv},
  year    = {2021},
  volume  = {abs/2109.08472},
  url     = {https://api.semanticscholar.org/CorpusID:237563206}
}

@article{Audioclip,
  title   = {Audioclip: Extending Clip to Image, Text and Audio},
  author  = {Andrey Guzhov and Federico Raue and J{\"o}rn Hees and Andreas R. Dengel},
  journal = {ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year    = {2021},
  pages   = {976-980},
  url     = {https://api.semanticscholar.org/CorpusID:235624127}
}

@inproceedings{CLIP,
  title     = {Learning Transferable Visual Models From Natural Language Supervision},
  author    = {Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
  booktitle = {International Conference on Machine Learning},
  year      = {2021},
  url       = {https://api.semanticscholar.org/CorpusID:231591445}
}

@inproceedings{FROSTER,
  title     = {FROSTER: Frozen CLIP is a Strong Teacher for Open-Vocabulary Action Recognition},
  author    = {Xiaohu Huang and Hao Zhou and Kun Yao and Kai Han},
  booktitle = {International Conference on Learning Representations},
  year      = {2024}
}

@article{Wav2CLIP,
  title   = {Wav2CLIP: Learning Robust Audio Representations from Clip},
  author  = {Ho-Hsiang Wu and Prem Seetharaman and Kundan Kumar and Juan Pablo Bello},
  journal = {ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year    = {2021},
  pages   = {4563-4567},
  url     = {https://api.semanticscholar.org/CorpusID:239616434}
}

@article{XCLIP,
  title     = {Expanding Language-Image Pretrained Models for General Video Recognition},
  author    = {Ni, Bolin and Peng, Houwen and Chen, Minghao and Zhang, Songyang and Meng, Gaofeng and Fu, Jianlong and Xiang, Shiming and Ling, Haibin},
  booktitle = {European Conference on Computer Vision (ECCV)},
  year      = {2022}
}

@article{chatgpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{chen_2022_adaptformer,
  title   = {AdaptFormer: Adapting Vision Transformers for Scalable Visual Recognition},
  author  = {Chen, Shoufa and Ge, Chongjian and Tong, Zhan and Wang, Jiangliu and Song, Yibing and Wang, Jue and Luo, Ping},
  journal = {Advances in Neural Information Processing Systems},
  year    = {2022}
}

@inproceedings{cheng2020shiftgcn,  
  title     = {Skeleton-Based Action Recognition with Shift Graph Convolutional Network},  
  author    = {Ke Cheng and Yifan Zhang and Xiangyu He and Weihan Chen and Jian Cheng and Hanqing Lu},  
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},  
  year      = {2020},  
}

@inproceedings{clip4vla,
  title     = {Accommodating Audio Modality in CLIP for Multimodal Processing},
  author    = {Ludan Ruan and Anwen Hu and Yuqing Song and Liang Zhang and S. Zheng and Qin Jin},
  booktitle = {AAAI Conference on Artificial Intelligence},
  year      = {2023},
  url       = {https://api.semanticscholar.org/CorpusID:257496490}
}

@article{crossglg,
  title={CrossGLG: LLM Guides One-shot Skeleton-based 3D Action Recognition in a Cross-level Manner},
  author={Yan, Tingbing and Zeng, Wenzheng and Xiao, Yang and Tong, Xingyu and Tan, Bo and Fang, Zhiwen and Cao, Zhiguo and Zhou, Joey Tianyi},
  journal={arXiv preprint arXiv:2403.10082},
  year={2024}
}

@inproceedings{das2020vpn,
  title        = {Vpn: Learning video-pose embedding for activities of daily living},
  author       = {Das, Srijan and Sharma, Saurav and Dai, Rui and Bremond, Francois and Thonnat, Monique},
  booktitle    = {European Conference on Computer Vision},
  pages        = {72--90},
  year         = {2020},
  organization = {Springer}
}

@InProceedings{feng2024_chatpose,
    author = {Feng, Yao and Lin, Jing and Dwivedi, Sai Kumar and Sun, Yu and Patel, Priyanka and Black, Michael J.},
    title = {ChatPose: Chatting about 3D Human Pose},
    booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    year = {2024}
}

@misc{gupta_crossmodaldistillation_cvpr16,
  title   = {Cross Modal Distillation for Supervision Transfer},
  author  = {Saurabh Gupta and Judy Hoffman and Jitendra Malik},
  year    = {2016},
  journal = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}
}

@misc{knowledge_distillation_hinton2015,
  title         = {Distilling the Knowledge in a Neural Network},
  author        = {Geoffrey Hinton and Oriol Vinyals and Jeff Dean},
  year          = {2015},
  eprint        = {1503.02531},
  archiveprefix = {arXiv},
  primaryclass  = {stat.ML}
}

@article{liu2023_STAN_CVPR23,
  title   = {Revisiting Temporal Modeling for CLIP-based Image-to-Video Knowledge Transferring},
  author  = {Liu, Ruyang and Huang, Jingjia and Li, Ge and Feng, Jiashi and Wu, Xinglong and Li, Thomas H},
  journal = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year    = {2023}
}

@article{llama,
  title={LLaMA: Open and Efficient Foundation Language Models},
  author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timoth{\'e}e Lacroix and Baptiste Rozi{\`e}re and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
  journal={ArXiv},
  year={2023},
  volume={abs/2302.13971},
  url={https://api.semanticscholar.org/CorpusID:257219404}
}

@article{lu2022_unifiedio,
  title={Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks},
  author={Lu, Jiasen and Clark, Christopher and Zellers, Rowan and Mottaghi, Roozbeh and Kembhavi, Aniruddha},
  journal={arXiv preprint arXiv:2206.08916},
  year={2022}
}

@inproceedings{mars,
  title     = {{MARS: Motion-Augmented RGB Stream for Action Recognition}},
  author    = {Crasto, Nieves and Weinzaepfel, Philippe and Alahari, Karteek and Schmid, Cordelia},
  booktitle = {CVPR},
  year      = {2019}
}

@misc{park2023_dualpath_CVPR,
  title   = {Dual-path Adaptation from Image to Video Transformers},
  author  = {Jungin Park and Jiyoung Lee and Kwanghoon Sohn},
  year    = {2023},
  journal = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}
}

@inproceedings{pivit,
  author    = {Dominick Reilly and Srijan Das},
  title     = {Just Add $\pi$! Pose Induced Video Transformers for Understanding Activities of Daily Living},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  year      = {2024}
}

@article{qian2022_opticalflow_and_audioclip,
  title={Multimodal open-vocabulary video classification via pre-trained vision and language models},
  author={Qian, Rui and Li, Yeqing and Xu, Zheng and Yang, Ming-Hsuan and Belongie, Serge and Cui, Yin},
  journal={arXiv preprint arXiv:2207.07646},
  year={2022}
}

@inproceedings{smie,
  title     = {Zero-shot Skeleton-based Action Recognition via Mutual Information Estimation and Maximization},
  author    = {Zhou, Yujie and Qiang, Wenwen and Rao, Anyi and Lin, Ning and Su, Bing and Wang, Jiaqi},
  booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
  pages     = {5302--5310},
  year      = {2023}
}

@inproceedings{soundnet,
  author    = {Aytar, Yusuf and Vondrick, Carl and Torralba, Antonio},
  title     = {SoundNet: Learning Sound Representations from Unlabeled Video},
  year      = {2016},
  isbn      = {9781510838819},
  publisher = {Curran Associates Inc.},
  address   = {Red Hook, NY, USA},
  abstract  = {We learn rich natural sound representations by capitalizing on large amounts of unlabeled sound data collected in the wild. We leverage the natural synchronization between vision and sound to learn an acoustic representation using two-million unlabeled videos. Unlabeled video has the advantage that it can be economically acquired at massive scales, yet contains useful signals about natural sound. We propose a student-teacher training procedure which transfers discriminative visual knowledge from well established visual recognition models into the sound modality using unlabeled video as a bridge. Our sound representation yields significant performance improvements over the state-of-the-art results on standard benchmarks for acoustic scene/object classification. Visualizations suggest some high-level semantics automatically emerge in the sound network, even though it is trained without ground truth labels.},
  booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
  pages     = {892â€“900},
  numpages  = {9},
  location  = {Barcelona, Spain},
  series    = {NIPS'16}
}

@inproceedings{st_adapter,
  author    = {Pan, Junting and Lin, Ziyi and Zhu, Xiatian and Shao, Jing and Li, Hongsheng},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
  pages     = {26462--26477},
  publisher = {Curran Associates, Inc.},
  title     = {ST-Adapter: Parameter-Efficient Image-to-Video Transfer Learning},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2022/file/a92e9165b22d4456fc6d87236e04c266-Paper-Conference.pdf},
  volume    = {35},
  year      = {2022}
}

@inproceedings{star,
  title={Fine-Grained Side Information Guided Dual-Prompts for Zero-Shot Skeleton Action Recognition},
  author={Yang Chen and Jingcai Guo and Tian He and Ling Wang},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:269043086}
}

@article{su2023_pandagpt,
    title={PandaGPT: One Model To Instruction-Follow Them All},
    author={Su, Yixuan and Lan, Tian and Li, Huayang and Xu, Jialu and Wang, Yan and Cai, Deng},
    journal={arXiv preprint arXiv:2305.16355},
    year={2023}
  }

@article{synse,
  title={Syntactically Guided Generative Embeddings for Zero-Shot Skeleton Action Recognition},
  author={Pranay Gupta and Divyanshu Sharma and Ravi Kiran Sarvadevabhatla},
  journal={2021 IEEE International Conference on Image Processing (ICIP)},
  year={2021},
  pages={439-443}
}

@inproceedings{videochatgpt,
    title={Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models},
    author={Maaz, Muhammad and Rasheed, Hanoona and Khan, Salman and Khan, Fahad Shahbaz},
    booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL 2024)},
    year={2024}
}

@article{videollama,
  title={Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding},
  author={Hang Zhang and Xin Li and Lidong Bing},
  journal={ArXiv},
  year={2023},
  volume={abs/2306.02858},
  url={https://api.semanticscholar.org/CorpusID:259075356}
}

@article{videollava,
  title={Video-LLaVA: Learning United Visual Representation by Alignment Before Projection},
  author={Lin, Bin and Zhu, Bin and Ye, Yang and Ning, Munan and Jin, Peng and Yuan, Li},
  journal={arXiv preprint arXiv:2311.10122},
  year={2023}
}

@inproceedings{vificlip,
  title     = {Finetuned CLIP models are efficient video learners},
  author    = {Rasheed, Hanoona and Khattak, Muhammad Uzair and Maaz, Muhammad and Khan, Salman and Khan, Fahad Shahbaz},
  booktitle = {The IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year      = {2023}
}

@article{vpn++,
  author  = {Das, Srijan and Dai, Rui and Yang, Di and Bremond, Francois},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title   = {VPN++: Rethinking Video-Pose embeddings for understanding Activities of Daily Living},
  year    = {2021},
  volume  = {},
  number  = {},
  pages   = {1-1},
  doi     = {10.1109/TPAMI.2021.3127885}
}

@inproceedings{yang2023_AIM_ICLR,
  title     = {AIM: Adapting Image Models for Efficient Video Understanding},
  author    = {Taojiannan Yang and Yi Zhu and Yusheng Xie and Aston Zhang 
               and Chen Chen and Mu Li},
  booktitle = {International Conference on Learning Representations},
  year      = {2023},
  url       = {https://openreview.net/forum?id=CIoSZ_HKHS7}
}

@article{ye2024_xvila,
  title={X-VILA: Cross-Modality Alignment for Large Language Model},
  author={Ye, Hanrong and Huang, De-An and Lu, Yao and Yu, Zhiding and Ping, Wei and Tao, Andrew and Kautz, Jan and Han, Song and Xu, Dan and Molchanov, Pavlo and others},
  journal={arXiv preprint arXiv:2405.19335},
  year={2024}
}

@misc{zhang2023_speechgpt,
      title={SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities}, 
      author={Dong Zhang and Shimin Li and Xin Zhang and Jun Zhan and Pengyu Wang and Yaqian Zhou and Xipeng Qiu},
      year={2023},
      eprint={2305.11000},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

