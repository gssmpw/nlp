@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})



@misc{vicuna2023,
    title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},
    url = {https://lmsys.org/blog/2023-03-30-vicuna/},
    author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
    month = {March},
    year = {2023}
}
@inproceedings{jang2020etri,
title = {{ETRI-Activity3D: A Large-Scale RGB-D Dataset for Robots to Recognize Daily Activities of the Elderly}},
author = {Jang, Jinhyeok and Kim, Dohyung and Park, Cheonshu and Jang, Minsu and Lee, Jaeyeon and Kim, Jaehong},
booktitle = {IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
year = {2020}
}
@article{liu2019ntu,
title = {{NTU RGB+D 120: A Large-Scale Benchmark for 3D Human Activity Understanding}},
author = {Liu, Jun and Shahroudy, Amir and Perez, Mauricio and Wang, Gang and Duan, Ling-Yu and Kot, Alex C.},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
year = {2019}
}
@inproceedings{vaquette2017dahlia,
title = {{The DAily Home LIfe Activity Dataset: A High Semantic Activity Dataset for Online Recognition}},
author = {Vaquette, Geoffrey and Orcesi, Astrid and Lucat, Laurent and Achard, Catherine},
booktitle = {IEEE International Conference on Automatic Face & Gesture Recognition},
year = {2017}
}
@article{liu2017pku,
title = {{PKU-MMD: A Large Scale Benchmark for Continuous Multi-Modal Human Action Understanding}},
author = {Liu, Chunhui and Hu, Yueyu and Li, Yanghao and Song, Sijie and Liu, Jiaying},
journal = {arXiv preprint arXiv:1703.07475},
year = {2017}
}

@inproceedings{das2020vpn,
  title        = {Vpn: Learning video-pose embedding for activities of daily living},
  author       = {Das, Srijan and Sharma, Saurav and Dai, Rui and Bremond, Francois and Thonnat, Monique},
  booktitle    = {European Conference on Computer Vision},
  pages        = {72--90},
  year         = {2020},
  organization = {Springer}
}
@inproceedings{di_ltn,
  author    = {Yang, Di and Wang, Yaohui and Kong, Quan and Dantcheva, Antitza and Garattoni, Lorenzo and Francesca, Gianpiero and Br\'{e}mond, Fran\c{c}ois},
  title     = {Self-Supervised Video Representation Learning via Latent Time Navigation},
  year      = {2023},
  isbn      = {978-1-57735-880-0},
  publisher = {AAAI Press},
  url       = {https://doi.org/10.1609/aaai.v37i3.25416},
  doi       = {10.1609/aaai.v37i3.25416},
  abstract  = {Self-supervised video representation learning aimed at maximizing similarity between different temporal segments of one video, in order to enforce feature persistence over time. This leads to loss of pertinent information related to temporal relationships, rendering actions such as 'enter' and 'leave' to be indistinguishable. To mitigate this limitation, we propose Latent Time Navigation (LTN), a time-parameterized contrastive learning strategy that is streamlined to capture finegrained motions. Specifically, we maximize the representation similarity between different video segments from one video, while maintaining their representations time-aware along a subspace of the latent representation code including an orthogonal basis to represent temporal changes. Our extensive experimental analysis suggests that learning video representations by LTN consistently improves performance of action classification in fine-grained and human-oriented tasks (e.g., on Toyota Smarthome dataset). In addition, we demonstrate that our proposed model, when pre-trained on Kinetics-400, generalizes well onto the unseen real world video benchmark datasets UCF101 and HMDB51, achieving state-of-the-art performance in action recognition.},
  booktitle = {Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence},
  articleno = {347},
  numpages  = {9},
  series    = {AAAI'23/IAAI'23/EAAI'23}
}
@article{dosovitskiy2020vit,
  title   = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author  = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and  Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  journal = ICLR,
  year    = {2021}
}

@article{OpenPose,
  author  = {Zhe {Cao} and Gines {Hidalgo Martinez} and Tomas {Simon} and Shih-En {Wei} and Yaser {Sheikh}},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title   = {OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields},
  year    = {2019}
}
@article{fang2017rmpe,
  title   = {RMPE: Regional Multi-person Pose Estimation},
  author  = {Haoshu Fang and Shuqin Xie and Yu-Wing Tai and Cewu Lu},
  journal = {2017 IEEE International Conference on Computer Vision (ICCV)},
  year    = {2016},
  pages   = {2353-2362}
}
@inproceedings{3dtrl,
  title     = {Learning Viewpoint-Agnostic Visual Representations by Recovering Tokens in 3D Space},
  author    = {Jinghuan Shang and Srijan Das and Michael S Ryoo},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2022}
}


@inproceedings{i3d,
  title        = {Quo vadis, Action Recognition? A New Model and the Kinetics Dataset},
  author       = {Carreira, Joao and Zisserman, Andrew},
  booktitle    = CVPR,
  pages        = {4724--4733},
  year         = {2017},
  organization = {IEEE}
}
@inproceedings{assemblenetplusplus,
  title     = {AssembleNet++: Assembling Modality Representations via Attention Connections},
  author    = {Ryoo, Michael S. and
               Piergiovanni, AJ and
               Kangaspunta, Juhana and
               Angelova, Anelia},
  booktitle = ECCV,
  year      = {2020}
}
@inproceedings{vgg16,
  title     = {Return of the Devil in the Details: Delving Deep into Convolutional Nets},
  author    = {Chatfield, Ken and Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
  booktitle = BMVC,
  year      = {2014}
}

@article{meshrcnn,
  title   = {Mesh R-CNN},
  author  = {Georgia Gkioxari, Jitendra Malik, Justin Johnson},
  journal = ICCV,
  year    = {2019}
}

@article{action_machine,
  title   = {Action Machine: Rethinking Action Recognition in Trimmed Videos},
  author  = {Jiagang Zhu and Wei Zou and Liang Xu and Yiming Hu and Zheng Zhu and Manyu Chang and Junjie Huang and Guan Huang and Dalong Du},
  journal = {ArXiv},
  year    = {2018},
  volume  = {abs/1812.05770}
}

@inproceedings{liu2021swin,
  title     = {Swin Transformer: Hierarchical Vision Transformer using Shifted Windows},
  author    = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle = ICCV,
  year      = {2021}
}

@inproceedings{smarthome,
  author    = {Srijan Das and Rui Dai and Michal Koperski and Luca Minciullo and Lorenzo Garattoni and Francois Bremond and Gianpiero Francesca},
  title     = {Toyota Smarthome: Real-World Activities of Daily Living},
  booktitle = ICCV,
  year      = {2019}
}
@inproceedings{NTU_RGB+D,
  author    = {Shahroudy, Amir and Liu, Jun and Ng, Tian-Tsong and Wang, Gang},
  title     = {NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis},
  booktitle = CVPR,
  month     = {June},
  year      = {2016}
}

@inproceedings{sohn2015learning,
  title     = {Learning Structured Output Representation using Deep Conditional Generative Models},
  author    = {Sohn, Kihyuk and Yan, Xinchen and Lee, Honglak},
  booktitle = NIPS,
  pages     = {3483--3491},
  year      = {2015}
}
@inproceedings{chen2016infogan,
  title     = {InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets},
  author    = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
  booktitle = NIPS,
  year      = {2016}
}
@inproceedings{Higgins2017betaVAELB,
  title     = {beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework},
  author    = {I. Higgins and Lo{\"i}c Matthey and A. Pal and C. Burgess and Xavier Glorot and M. Botvinick and S. Mohamed and Alexander Lerchner},
  booktitle = ICLR,
  year      = {2017}
}
@inproceedings{chen2018isolating,
  title     = {Isolating Sources of Disentanglement in Variational Autoencoders},
  author    = {Chen, Ricky T. Q. and Li, Xuechen and Grosse, Roger and Duvenaud, David},
  booktitle = NIPS,
  year      = {2018}
}
@inproceedings{jiang2019disentangled,
  title     = {Disentangled Representation Learning for 3D Face Shape},
  author    = {Jiang, Zi-Hang and Wu, Qianyi and Chen, Keyu and Zhang, Juyong},
  booktitle = CVPR,
  pages     = {11957--11966},
  year      = {2019}
}
@inproceedings{denton2017unsupervised,
  title     = {Unsupervised Learning of Disentangled Representations from Video},
  author    = {Denton, Emily and Birodkar, Vighnesh},
  booktitle = NIPS,
  pages     = {4417--4426},
  year      = {2017}
}
@inproceedings{nair2020contextual,
  title        = {Contextual Imagined Goals for Self-supervised Robotic Learning},
  author       = {Nair, Ashvin and Bahl, Shikhar and Khazatsky, Alexander and Pong, Vitchyr and Berseth, Glen and Levine, Sergey},
  booktitle    = CORL,
  pages        = {530--539},
  year         = {2020},
  organization = {PMLR}
}
@inproceedings{doersch2015unsupervised,
  title     = {Unsupervised Visual Representation Learning by Context Prediction},
  author    = {Doersch, Carl and Gupta, Abhinav and Efros, Alexei A},
  booktitle = ICCV,
  pages     = {1422--1430},
  year      = {2015}
}
@inproceedings{zhang2019gait,
  title     = {Gait Recognition via Disentangled Representation Learning},
  author    = {Zhang, Ziyuan and Tran, Luan and Yin, Xi and Atoum, Yousef and Liu, Xiaoming and Wan, Jian and Wang, Nanxin},
  booktitle = CVPR,
  pages     = {4710--4719},
  year      = {2019}
}
@article{aytar2017cross,
  title     = {Cross-modal Scene Networks},
  author    = {Aytar, Yusuf and Castrejon, Lluis and Vondrick, Carl and Pirsiavash, Hamed and Torralba, Antonio},
  journal   = PAMI,
  volume    = {40},
  number    = {10},
  pages     = {2303--2314},
  year      = {2017},
  publisher = {IEEE}
}
@article{kingma2013auto,
  title   = {Auto-encoding Variational Bayes},
  author  = {Kingma, Diederik P and Welling, Max},
  journal = {arXiv preprint arXiv:1312.6114},
  year    = {2013}
}
@inproceedings{goodfellow2014generative,
  title     = {Generative Adversarial Nets},
  author    = {Goodfellow, Ian J and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron C and Bengio, Yoshua},
  booktitle = NIPS,
  year      = {2014}
}
@article{oord2018representation,
  title   = {Representation Learning with Contrastive Predictive Coding},
  author  = {Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
  journal = {arXiv preprint arXiv:1807.03748},
  year    = {2018}
}
@article{chen2020simple,
  title        = {A Simple Framework for Contrastive Learning of Visual Representations},
  author       = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  booktitle    = ICML,
  pages        = {1597--1607},
  year         = {2020},
  organization = {PMLR}
}
@misc{chen2020improved,
  title         = {Improved Baselines with Momentum Contrastive Learning},
  author        = {Xinlei Chen and Haoqi Fan and Ross Girshick and Kaiming He},
  year          = {2020},
  eprint        = {2003.04297},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@article{chen2020big,
  title   = {Big Self-supervised Models are Strong Semi-supervised Learners},
  author  = {Chen, Ting and Kornblith, Simon and Swersky, Kevin and Norouzi, Mohammad and Hinton, Geoffrey E},
  journal = NIPS,
  volume  = {33},
  pages   = {22243--22255},
  year    = {2020}
}
@misc{chen2020exploring,
  title     = {Exploring Simple Siamese Representation Learning},
  author    = {Chen, Xinlei and He, Kaiming},
  booktitle = CVPR,
  pages     = {15750--15758},
  year      = {2021}
}
@misc{srinivas2020curl,
  title         = {CURL: Contrastive Unsupervised Representations for Reinforcement Learning},
  author        = {Aravind Srinivas and Michael Laskin and Pieter Abbeel},
  year          = {2020},
  eprint        = {2004.04136},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@inproceedings{edwards2018imitating,
  title        = {Imitating Latent Policies from Observation},
  author       = {Edwards, Ashley and Sahni, Himanshu and Schroecker, Yannick and Isbell, Charles},
  booktitle    = ICML,
  pages        = {1755--1763},
  year         = {2019},
  organization = {PMLR}
}
@inproceedings{schwarzer2020dataefficient,
  title     = {Data-Efficient Reinforcement Learning with Self-Predictive Representations},
  author    = {Max Schwarzer and Ankesh Anand and Rishab Goel and R Devon Hjelm and Aaron Courville and Philip Bachman},
  booktitle = ICLR,
  year      = {2021},
  url       = {https://openreview.net/forum?id=uCQfPZwRaUu}
}
@article{he2020momentum,
  title     = {Momentum Contrast for Unsupervised Visual Representation Learning},
  isbn      = {9781728171685},
  url       = {http://dx.doi.org/10.1109/cvpr42600.2020.00975},
  doi       = {10.1109/cvpr42600.2020.00975},
  journal   = CVPR,
  publisher = {IEEE},
  author    = {He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
  year      = {2020},
  month     = {Jun}
}
@misc{grill2020bootstrap,
  title         = {Bootstrap Your Own Latent: A New Approach to Self-supervised Learning},
  author        = {Jean-Bastien Grill and Florian Strub and Florent Altché and Corentin Tallec and Pierre H. Richemond and Elena Buchatskaya and Carl Doersch and Bernardo Avila Pires and Zhaohan Daniel Guo and Mohammad Gheshlaghi Azar and Bilal Piot and Koray Kavukcuoglu and Rémi Munos and Michal Valko},
  year          = {2020},
  eprint        = {2006.07733},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@inproceedings{tian2020cmc,
  author    = {Tian, Yonglong
               and Krishnan, Dilip
               and Isola, Phillip},
  editor    = {Vedaldi, Andrea
               and Bischof, Horst
               and Brox, Thomas
               and Frahm, Jan-Michael},
  title     = {Contrastive Multiview Coding},
  booktitle = ECCV,
  year      = {2020},
  publisher = {Springer International Publishing},
  address   = {Cham},
  pages     = {776--794},
  isbn      = {978-3-030-58621-8}
}
@article{liu2018imitationfrom,
  title     = {Imitation from Observation: Learning to Imitate Behaviors from Raw Video via Context Translation},
  isbn      = {9781538630815},
  url       = {http://dx.doi.org/10.1109/ICRA.2018.8462901},
  doi       = {10.1109/icra.2018.8462901},
  journal   = ICRA,
  publisher = {IEEE},
  author    = {Liu, YuXuan and Gupta, Abhishek and Abbeel, Pieter and Levine, Sergey},
  year      = {2018},
  month     = {May}
}
@misc{aytar2018playing,
  title   = {Playing Hard Exploration Games by Watching YouTube},
  author  = {Yusuf Aytar and Tobias Pfaff and David Budden and Tom Le Paine and Ziyu Wang and Nando de Freitas},
  year    = {2018},
  journal = NIPS
}
@inproceedings{mees2020adversarial,
  title        = {Adversarial Skill Networks: Unsupervised Robot Skill Learning from Video},
  author       = {Mees, Oier and Merklinger, Markus and Kalweit, Gabriel and Burgard, Wolfram},
  booktitle    = ICRA,
  pages        = {4188--4194},
  year         = {2020},
  organization = {IEEE}
}
@inproceedings{sermanet2018time,
  title        = {Time-Contrastive Networks: Self-supervised Learning from Video},
  author       = {Sermanet, Pierre and Lynch, Corey and Chebotar, Yevgen and Hsu, Jasmine and Jang, Eric and Schaal, Stefan and Levine, Sergey and Brain, Google},
  booktitle    = ICRA,
  pages        = {1134--1141},
  year         = {2018},
  organization = {IEEE}
}

@inproceedings{sermanet2017tcn_robotalign,
  author    = {Sermanet, Pierre and Lynch, Corey and Hsu, Jasmine and Levine, Sergey},
  booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
  title     = {Time-Contrastive Networks: Self-Supervised Learning from Multi-view Observation},
  year      = {2017},
  volume    = {},
  number    = {},
  pages     = {486-487},
  doi       = {10.1109/CVPRW.2017.69}
}

@article{ho2016generative,
  title   = {Generative Adversarial Iimitation Learning},
  author  = {Ho, Jonathan and Ermon, Stefano},
  journal = NIPS,
  volume  = {29},
  year    = {2016}
}
@article{van2008visualizing,
  title   = {Visualizing data using t-SNE.},
  author  = {Van der Maaten, Laurens and Hinton, Geoffrey},
  journal = {Journal of machine learning research},
  volume  = {9},
  number  = {11},
  year    = {2008}
}
@article{Schulman2017ProximalPO,
  title   = {Proximal Policy Optimization Algorithms},
  author  = {John Schulman and F. Wolski and Prafulla Dhariwal and A. Radford and O. Klimov},
  journal = {ArXiv},
  year    = {2017},
  volume  = {abs/1707.06347}
}
@misc{replaymod,
  author = {CrushedPixel and johni0702},
  title  = {ReplayMod},
  url    = {https://www.replaymod.com/}
}
@article{guss2019minerldata,
  title   = {Mine{RL}: A Large-Scale Dataset of {M}inecraft Demonstrations},
  author  = {William H. Guss and Houghton, Brandon and Topin, Nicholay and Wang, Phillip and Codel, Cayden and Veloso, Manuela and Salakhutdinov, Ruslan},
  journal = IJCAI,
  url     = {http://minerl.io},
  year    = {2019}
}
@misc{coumans2019,
  author       = {Erwin Coumans and Yunfei Bai},
  title        = {PyBullet, a Python module for physics simulation for games, robotics and machine learning},
  howpublished = {\url{http://pybullet.org}},
  year         = {2016--2019}
}
@article{hermans2017defense,
  title   = {In defense of the triplet loss for person re-identification},
  author  = {Hermans, Alexander and Beyer, Lucas and Leibe, Bastian},
  journal = {arXiv preprint arXiv:1703.07737},
  year    = {2017}
}
@misc{andrychowicz2018hindsight,
  title         = {Hindsight Experience Replay},
  author        = {Marcin Andrychowicz and Filip Wolski and Alex Ray and Jonas Schneider and Rachel Fong and Peter Welinder and Bob McGrew and Josh Tobin and Pieter Abbeel and Wojciech Zaremba},
  year          = {2018},
  eprint        = {1707.01495},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@inproceedings{shu2017NeuralFace,
  title        = {Neural Face Editing with Intrinsic Image Disentangling},
  author       = {Shu, Z. and Yumer, E. and Hadap, S. and Sunkavalli, K. and Shechtman, E. and Samaras, D.},
  booktitle    = CVPR,
  organization = {IEEE},
  pages        = {-},
  year         = {2017}
}
@inproceedings{mattew2016malmo,
  author    = {Johnson, Matthew and Hofmann, Katja and Hutton, Tim and Bignell, David},
  title     = {The Malmo Platform for Artificial Intelligence Experimentation},
  year      = {2016},
  isbn      = {9781577357704},
  publisher = {AAAI Press},
  booktitle = {IJCAI},
  pages     = {4246–4247},
  numpages  = {2},
  location  = {New York, New York, USA},
  series    = {IJCAI'16}
}
@inproceedings{dwibedi2018learning,
  title        = {Learning Actionable Representations from Visual Observations},
  author       = {Dwibedi, Debidatta and Tompson, Jonathan and Lynch, Corey and Sermanet, Pierre},
  booktitle    = IROS,
  pages        = {1577--1584},
  year         = {2018},
  organization = {IEEE}
}
@article{zolna2019task,
  title   = {Task-relevant Adversarial Imitation Learning},
  author  = {Zolna, Konrad and Reed, Scott and Novikov, Alexander and Colmenarej, Sergio Gomez and Budden, David and Cabi, Serkan and Denil, Misha and de Freitas, Nando and Wang, Ziyu},
  journal = {arXiv preprint arXiv:1910.01077},
  year    = {2019}
}
@article{karras2019astyle,
  title     = {A Style-Based Generator Architecture for Generative Adversarial Networks},
  isbn      = {9781728132938},
  url       = {http://dx.doi.org/10.1109/CVPR.2019.00453},
  doi       = {10.1109/cvpr.2019.00453},
  journal   = CVPR,
  publisher = {IEEE},
  author    = {Karras, Tero and Laine, Samuli and Aila, Timo},
  year      = {2019},
  month     = {Jun}
}
@article{Stadie2017ThirdPersonIL,
  title   = {Third-Person Imitation Learning},
  author  = {Bradly C. Stadie and P. Abbeel and Ilya Sutskever},
  journal = {ArXiv},
  year    = {2017},
  volume  = {abs/1703.01703}
}
@inproceedings{hermann2020adaptive,
  title        = {Adaptive Curriculum Generation from Demonstrations for Sim-to-real Visuomotor Control},
  author       = {Hermann, Lukas and Argus, Max and Eitel, Andreas and Amiranashvili, Artemij and Burgard, Wolfram and Brox, Thomas},
  booktitle    = ICRA,
  pages        = {6498--6505},
  year         = {2020},
  organization = {IEEE}
}
@inproceedings{james2019sim,
  title     = {Sim-to-real via Sim-to-sim: Data-efficient Robotic Grasping via Randomized-to-canonical Adaptation Networks},
  author    = {James, Stephen and Wohlhart, Paul and Kalakrishnan, Mrinal and Kalashnikov, Dmitry and Irpan, Alex and Ibarz, Julian and Levine, Sergey and Hadsell, Raia and Bousmalis, Konstantinos},
  booktitle = CVPR,
  pages     = {12627--12637},
  year      = {2019}
}
@inproceedings{yang2020cross,
  title        = {Cross-context Visual Imitation Learning from Demonstrations},
  author       = {Yang, Shuo and Zhang, Wei and Lu, Weizhi and Wang, Hesheng and Li, Yibin},
  booktitle    = ICRA,
  pages        = {5467--5473},
  year         = {2020},
  organization = {IEEE}
}
@article{premack1978does,
  title     = {Does the Chimpanzee Have a Theory of Mind?},
  author    = {Premack, David and Woodruff, Guy},
  journal   = {Behavioral and brain sciences},
  volume    = {1},
  number    = {4},
  pages     = {515--526},
  year      = {1978},
  publisher = {Cambridge University Press}
}
@article{meltzoff1988imitation,
  title     = {Imitation of Televised Models by Infants},
  author    = {Meltzoff, Andrew N},
  journal   = {Child development},
  volume    = {59},
  number    = {5},
  pages     = {1221},
  year      = {1988},
  publisher = {NIH Public Access}
}
@inproceedings{peng2019domain,
  title        = {Domain Agnostic learning with Disentangled Representations},
  author       = {Peng, Xingchao and Huang, Zijun and Sun, Ximeng and Saenko, Kate},
  booktitle    = ICML,
  pages        = {5102--5112},
  year         = {2019},
  organization = {PMLR}
}
@inproceedings{shang2021disentangle,
  author    = {Shang, Jinghuan and Ryoo, Michael S.},
  booktitle = IROS,
  title     = {Self-Supervised Disentangled Representation Learning for Third-Person Imitation Learning},
  year      = {2021},
  volume    = {},
  number    = {},
  pages     = {214-221},
  doi       = {10.1109/IROS51168.2021.9636363}
}
  
@inproceedings{robomimic2021,
  title     = {What Matters in Learning from Offline Human Demonstrations for Robot Manipulation},
  author    = {Ajay Mandlekar and Danfei Xu and Josiah Wong and Soroush Nasiriany and Chen Wang and Rohun Kulkarni and Li Fei-Fei and Silvio Savarese and Yuke Zhu and Roberto Mart\'{i}n-Mart\'{i}n},
  booktitle = {arXiv preprint arXiv:2108.03298},
  year      = {2021}
}
@inproceedings{wang2019learning,
  title     = {Learning Correspondence from the Cycle-consistency of Time},
  author    = {Wang, Xiaolong and Jabri, Allan and Efros, Alexei A},
  booktitle = CVPR,
  pages     = {2566--2576},
  year      = {2019}
}
@inproceedings{dwibedi2019temporal,
  title     = {Temporal Cycle-consistency Learning},
  author    = {Dwibedi, Debidatta and Aytar, Yusuf and Tompson, Jonathan and Sermanet, Pierre and Zisserman, Andrew},
  booktitle = CVPR,
  pages     = {1801--1810},
  year      = {2019}
}
@article{kendall1938new,
  title     = {A new measure of rank correlation},
  author    = {Kendall, Maurice G},
  journal   = {Biometrika},
  volume    = {30},
  number    = {1/2},
  pages     = {81--93},
  year      = {1938},
  publisher = {JSTOR}
}
@article{cifar,
  title     = {Learning Multiple Layers of Features from Tiny Images},
  author    = {Krizhevsky, Alex and Hinton, Geoffrey and others},
  year      = {2009},
  publisher = {Citeseer}
}
@inproceedings{imagenet,
  title     = {Imagenet: A Large-scale Hierarchical Image Database},
  author    = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle = CVPR,
  pages     = {248--255},
  year      = {2009}
}
@article{gao2022fine,
  title   = {Fine-grained Temporal Contrastive Learning for Weakly-supervised Temporal Action Localization},
  author  = {Gao, Junyu and Chen, Mengyuan and Xu, Changsheng},
  journal = {arXiv preprint arXiv:2203.16800},
  year    = {2022}
}
@article{chen2022frame,
  title   = {Frame-wise Action Representations for Long Videos via Sequence Contrastive Learning},
  author  = {Chen, Minghao and Wei, Fangyun and Li, Chong and Cai, Deng},
  journal = {arXiv preprint arXiv:2203.14957},
  year    = {2022}
}
@article{stn,
  title   = {Spatial Transformer Networks},
  author  = {Jaderberg, Max and Simonyan, Karen and Zisserman, Andrew and others},
  journal = NIPS,
  volume  = {28},
  year    = {2015}
}
@article{svn,
  title   = {Self-supervised Video Transformer},
  author  = {Ranasinghe, Kanchana and Naseer, Muzammal and Khan, Salman and Khan, Fahad Shahbaz and Ryoo, Michael},
  journal = CVPR,
  year    = {2022}
}
@inproceedings{egoexo,
  title     = {Ego-exo: Transferring Visual Representations from Third-person to First-person Videos},
  author    = {Li, Yanghao and Nagarajan, Tushar and Xiong, Bo and Grauman, Kristen},
  booktitle = CVPR,
  pages     = {6943--6953},
  year      = {2021}
}
@article{lecun1995convolutional,
  title   = {Convolutional Networks for Images, Speech, and Time series},
  author  = {LeCun, Yann and Bengio, Yoshua and others},
  journal = {The handbook of brain theory and neural networks},
  volume  = {3361},
  number  = {10},
  pages   = {1995},
  year    = {1995}
}
@inproceedings{dai2021coatnet,
  title     = {CoAtNet: Marrying Convolution and Attention for All Data Sizes},
  author    = {Zihang Dai and Hanxiao Liu and Quoc V Le and Mingxing Tan},
  booktitle = NIPS,
  year      = {2021}
}
@inproceedings{starformer,
  title     = {StARformer: Transformer with State-Action-Reward Representations},
  author    = {Shang, Jinghuan and Kumara Kahatapitiya and Xiang Li and Ryoo, Michael S.},
  booktitle = ECCV,
  year      = {2022}
}
@article{starformerj,
  title   = {StARformer: Transformer with State-Action-Reward Representations for Robot Learning},
  author  = {Jinghuan Shang and Xiang Li and Kumara Kahatapitiya and Yu-Cheol Lee and Michael S. Ryoo},
  journal = {IEEE transactions on pattern analysis and machine intelligence},
  year    = {2022}
}
@article{liu2022convnet,
  title   = {A ConvNet for the 2020s},
  author  = {Liu, Zhuang and Mao, Hanzi and Wu, Chao-Yuan and Feichtenhofer, Christoph and Darrell, Trevor and Xie, Saining},
  journal = {arXiv preprint arXiv:2201.03545},
  year    = {2022}
}
@article{ryoo2021tokenlearner,
  title   = {TokenLearner: What Can 8 Learned Tokens Do for Images and Videos?},
  author  = {Ryoo, Michael S and Piergiovanni, AJ and Arnab, Anurag and Dehghani, Mostafa and Angelova, Anelia},
  journal = NIPS,
  year    = {2021}
}

@article{kahatapitiya2021swat,
  title   = {SWAT: Spatial Structure Within and Among Tokens},
  author  = {Kahatapitiya, Kumara and Ryoo, Michael S},
  journal = {arXiv preprint arXiv:2111.13677},
  year    = {2021}
}
@inproceedings{piergiovanni20214d,
  title     = {4d-net for learned multi-modal alignment},
  author    = {Piergiovanni, AJ and Casser, Vincent and Ryoo, Michael S and Angelova, Anelia},
  booktitle = ICCV,
  pages     = {15435--15445},
  year      = {2021}
}

@inproceedings{sigurdsson2018actor,
  title     = {Actor and Observer: Joint Modeling of First and Third-person Videos},
  author    = {Sigurdsson, Gunnar A and Gupta, Abhinav and Schmid, Cordelia and Farhadi, Ali and Alahari, Karteek},
  booktitle = CVPR,
  pages     = {7396--7404},
  year      = {2018}
}
@article{sigurdsson2018charades,
  title   = {Charades-ego: A large-scale Dataset of Paired Third and First Person Videos},
  author  = {Sigurdsson, Gunnar A and Gupta, Abhinav and Schmid, Cordelia and Farhadi, Ali and Alahari, Karteek},
  journal = {arXiv preprint arXiv:1804.09626},
  year    = {2018}
}
@article{vijayanarasimhan2017sfm,
  title   = {Sfm-net: Learning of Structure and Motion from Video},
  author  = {Vijayanarasimhan, Sudheendra and Ricco, Susanna and Schmid, Cordelia and Sukthankar, Rahul and Fragkiadaki, Katerina},
  journal = {arXiv preprint arXiv:1704.07804},
  year    = {2017}
}
@inproceedings{haque2016towards,
  title        = {Towards Viewpoint Invariant 3d Human Pose Estimation},
  author       = {Haque, Albert and Peng, Boya and Luo, Zelun and Alahi, Alexandre and Yeung, Serena and Fei-Fei, Li},
  booktitle    = ECCV,
  pages        = {160--177},
  year         = {2016},
  organization = {Springer}
}
@article{hsu2022vision,
  title   = {Vision-Based Manipulators Need to Also See from Their Hands},
  author  = {Hsu, Kyle and Kim, Moo Jin and Rafailov, Rafael and Wu, Jiajun and Finn, Chelsea},
  journal = ICLR,
  year    = {2022}
}
@article{jangir2022look,
  title     = {Look Closer: Bridging Egocentric and Third-Person Views with Transformers for Robotic Manipulation},
  author    = {Jangir, Rishabh and Hansen, Nicklas and Ghosal, Sambaran and Jain, Mohit and Wang, Xiaolong},
  journal   = RAL,
  year      = {2022},
  publisher = {IEEE}
}
@article{yan2016perspective,
  title   = {Perspective Transformer Nets: Learning Single-view 3d Object Reconstruction without 3D Supervision},
  author  = {Yan, Xinchen and Yang, Jimei and Yumer, Ersin and Guo, Yijie and Lee, Honglak},
  journal = NIPS,
  volume  = {29},
  year    = {2016}
}
@inproceedings{hadji2021representation,
  title     = {Representation Learning via Global Temporal Alignment and Cycle-consistency},
  author    = {Hadji, Isma and Derpanis, Konstantinos G and Jepson, Allan D},
  booktitle = CVPR,
  pages     = {11068--11077},
  year      = {2021}
}
@inproceedings{misra2016shuffle,
  title        = {Shuffle and Learn: Unsupervised Learning using Temporal Order Verification},
  author       = {Misra, Ishan and Zitnick, C Lawrence and Hebert, Martial},
  booktitle    = ECCV,
  pages        = {527--544},
  year         = {2016},
  organization = {Springer}
}
@inproceedings{chang2019d3tw,
  title     = {D3tw: Discriminative Differentiable Dynamic Time Warping for Weakly Supervised Action Alignment and Segmentation},
  author    = {Chang, Chien-Yi and Huang, De-An and Sui, Yanan and Fei-Fei, Li and Niebles, Juan Carlos},
  booktitle = CVPR,
  pages     = {3546--3555},
  year      = {2019}
}
@article{cao2021monoscene,
  title   = {MonoScene: Monocular 3D Semantic Scene Completion},
  author  = {Cao, Anh-Quan and de Charette, Raoul},
  journal = {arXiv preprint arXiv:2112.00726},
  year    = {2021}
}
@article{robert2022learning,
  title   = {Learning Multi-View Aggregation In the Wild for Large-Scale 3D Semantic Segmentation},
  author  = {Robert, Damien and Vallet, Bruno and Landrieu, Loic},
  journal = {arXiv preprint arXiv:2204.07548},
  year    = {2022}
}
@inproceedings{rukhovich2022imvoxelnet,
  title     = {Imvoxelnet: Image to Voxels Projection for Monocular and Multi-view General-purpose 3d Object Detection},
  author    = {Rukhovich, Danila and Vorontsova, Anna and Konushin, Anton},
  booktitle = WACV,
  pages     = {2397--2406},
  year      = {2022}
}
@inproceedings{pointtransformer,
  author    = {Zhao, Hengshuang and Jiang, Li and Jia, Jiaya and Torr, Philip H.S. and Koltun, Vladlen},
  title     = {Point Transformer},
  booktitle = ICCV,
  month     = {October},
  year      = {2021},
  pages     = {16259-16268}
}

@inproceedings{sun2020view,
  title        = {View-invariant Probabilistic Embedding for Human Pose},
  author       = {Sun, Jennifer J and Zhao, Jiaping and Chen, Liang-Chieh and Schroff, Florian and Adam, Hartwig and Liu, Ting},
  booktitle    = ECCV,
  pages        = {53--70},
  year         = {2020},
  organization = {Springer}
}
@inproceedings{arnab2021vivit,
  title     = {Vivit: A Video Vision Transformer},
  author    = {Arnab, Anurag and Dehghani, Mostafa and Heigold, Georg and Sun, Chen and Lu{\v{c}}i{\'c}, Mario and Schmid, Cordelia},
  booktitle = ICCV,
  pages     = {6836--6846},
  year      = {2021}
}
@article{liu2021videoswin,
  title   = {Video Swin Transformer},
  author  = {Ze Liu and Jia Ning and Yue Cao and Yixuan Wei and Zheng Zhang and Stephen Lin and Han Hu},
  journal = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year    = {2021},
  pages   = {3192-3201}
}
@inproceedings{graham2021levit,
  title     = {LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference},
  author    = {Graham, Benjamin and El-Nouby, Alaaeldin and Touvron, Hugo and Stock, Pierre and Joulin, Armand and J{\'e}gou, Herv{\'e} and Douze, Matthijs},
  booktitle = ICCV,
  pages     = {12259--12269},
  year      = {2021}
}
@inproceedings{yuan2021tokens,
  title     = {Tokens-to-token vit: Training Vision Transformers from Scratch on Imagenet},
  author    = {Yuan, Li and Chen, Yunpeng and Wang, Tao and Yu, Weihao and Shi, Yujun and Jiang, Zi-Hang and Tay, Francis EH and Feng, Jiashi and Yan, Shuicheng},
  booktitle = ICCV,
  pages     = {558--567},
  year      = {2021}
}
@inproceedings{zhang2017mixup,
  title     = {mixup: Beyond Empirical Risk Minimization},
  author    = {Hongyi Zhang and Moustapha Cisse and Yann N. Dauphin and David Lopez-Paz},
  booktitle = ICLR,
  year      = {2018}
}
@inproceedings{yun2019cutmix,
  title     = {Cutmix: Regularization Strategy to Train Strong Classifiers with Localizable Features},
  author    = {Yun, Sangdoo and Han, Dongyoon and Oh, Seong Joon and Chun, Sanghyuk and Choe, Junsuk and Yoo, Youngjoon},
  booktitle = ICCV,
  pages     = {6023--6032},
  year      = {2019}
}
@article{sener2022assembly101,
  title   = {Assembly101: A Large-Scale Multi-View Video Dataset for Understanding Procedural Activities},
  author  = {F. Sener and D. Chatterjee and D. Shelepov and K. He and D. Singhania and R. Wang and A. Yao},
  journal = CVPR,
  year    = {2022}
}

@inproceedings{mildenhall2020nerf,
  title        = {Nerf: Representing scenes as neural radiance fields for view synthesis},
  author       = {Mildenhall, Ben and Srinivasan, Pratul P and Tancik, Matthew and Barron, Jonathan T and Ramamoorthi, Ravi and Ng, Ren},
  booktitle    = ECCV,
  pages        = {405--421},
  year         = {2020},
  organization = {Springer}
}

@article{gower1975generalized,
  title     = {Generalized procrustes analysis},
  author    = {Gower, John C},
  journal   = {Psychometrika},
  volume    = {40},
  number    = {1},
  pages     = {33--51},
  year      = {1975},
  publisher = {Springer}
}

@article{barbu2019objectnet,
  title   = {Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models},
  author  = {Barbu, Andrei and Mayo, David and Alverio, Julian and Luo, William and Wang, Christopher and Gutfreund, Dan and Tenenbaum, Josh and Katz, Boris},
  journal = {Advances in neural information processing systems},
  volume  = {32},
  year    = {2019}
}

@inproceedings{zhou2019continuity,
  title     = {On the continuity of rotation representations in neural networks},
  author    = {Zhou, Yi and Barnes, Connelly and Lu, Jingwan and Yang, Jimei and Li, Hao},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages     = {5745--5753},
  year      = {2019}
}
@article{xiang2021eliminating,
  title     = {Eliminating topological errors in neural network rotation estimation using self-selecting ensembles},
  author    = {Xiang, Sitao},
  journal   = {ACM Transactions on Graphics (TOG)},
  volume    = {40},
  number    = {4},
  pages     = {1--21},
  year      = {2021},
  publisher = {ACM New York, NY, USA}
}

@inproceedings{jin2021planar,
  title     = {Planar surface reconstruction from sparse views},
  author    = {Jin, Linyi and Qian, Shengyi and Owens, Andrew and Fouhey, David F},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages     = {12991--13000},
  year      = {2021}
}
@inproceedings{sarlin2020superglue,
  title     = {Superglue: Learning feature matching with graph neural networks},
  author    = {Sarlin, Paul-Edouard and DeTone, Daniel and Malisiewicz, Tomasz and Rabinovich, Andrew},
  booktitle = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages     = {4938--4947},
  year      = {2020}
}
@article{triton,
  title   = {Neural Neural Textures Make Sim2Real Consistent},
  author  = {Burgert, Ryan and Shang, Jinghuan and Li, Xiang and Ryoo, Michael},
  journal = {arXiv preprint arXiv:2206.13500},
  year    = {2022}
}
@article{varol21_surreact,
  title   = {Synthetic Humans for Action Recognition from Unseen Viewpoints},
  author  = {G{\"u}l Varol and Ivan Laptev and Cordelia Schmid and Andrew Zisserman},
  journal = {International Journal of Computer Vision},
  year    = {2019},
  volume  = {129},
  pages   = {2264 - 2287}
}

@misc{pybullet,
  author       = {Erwin Coumans and Yunfei Bai},
  title        = {PyBullet, a Python module for physics simulation for games, robotics and machine learning},
  howpublished = {\url{http://pybullet.org}},
  year         = {2016--2019}
}

@misc{fasterrcnn_humandetector,
  title         = {Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks},
  author        = {Shaoqing Ren and Kaiming He and Ross Girshick and Jian Sun},
  year          = {2016},
  eprint        = {1506.01497},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}

@misc{victr,
  title         = {VicTR: Video-conditioned Text Representations for Activity Recognition},
  author        = {Kumara Kahatapitiya and Anurag Arnab and Arsha Nagrani and Michael S. Ryoo},
  year          = {2023},
  eprint        = {2304.02560},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@inproceedings{motionformerNeurIPS21,
  author    = {Mandela Patrick and
               Dylan Campbell and
               Yuki M. Asano and
               Ishan Misra and
               Florian Metze and
               Christoph Feichtenhofer and
               Andrea Vedaldi and
               Jo{\~{a}}o F. Henriques},
  editor    = {Marc'Aurelio Ranzato and
               Alina Beygelzimer and
               Yann N. Dauphin and
               Percy Liang and
               Jennifer Wortman Vaughan},
  title     = {Keeping Your Eye on the Ball: Trajectory Attention in Video Transformers},
  booktitle = {Advances in Neural Information Processing Systems 34: Annual Conference
               on Neural Information Processing Systems 2021, NeurIPS 2021, December
               6-14, 2021, virtual},
  pages     = {12493--12506},
  year      = {2021},
  url       = {https://proceedings.neurips.cc/paper/2021/hash/67f7fb873eaf29526a11a9b7ac33bfac-Abstract.html},
  timestamp = {Tue, 03 May 2022 16:20:47 +0200},
  biburl    = {https://dblp.org/rec/conf/nips/PatrickCAMMFVH21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{mmnetTPAMI22,
  author  = {Yu, Bruce and Liu, Yan and Zhang, Xiang and Zhong, Sheng-hua and Chan, Keith},
  year    = {2022},
  month   = {05},
  pages   = {1-1},
  title   = {MMNet: A Model-based Multimodal Network for Human Action Recognition in RGB-D Videos},
  volume  = {PP},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  doi     = {10.1109/TPAMI.2022.3177813}
}


@misc{unikBMVC21,
  title     = {UNIK: A Unified Framework for Real-world Skeleton-based Action Recognition},
  author    = {Di Yang and Yaohui Wang and Antitza Dantcheva and Lorenzo Garattoni and Gianpiero Francesca and Francois Bremond},
  year      = {2021},
  booktitle = {BMVC}
}
@inproceedings{s3dECCV18,
  title     = {Rethinking Spatiotemporal Feature Learning: Speed-Accuracy Trade-offs in Video Classification},
  author    = {Saining Xie and Chen Sun and Jonathan Huang and Zhuowen Tu and Kevin P. Murphy},
  booktitle = {European Conference on Computer Vision},
  year      = {2017}
}
@inproceedings{attn_rollout,
  title     = {Quantifying Attention Flow in Transformers},
  author    = {Abnar, Samira  and
               Zuidema, Willem},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  month     = jul,
  year      = {2020},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2020.acl-main.385},
  doi       = {10.18653/v1/2020.acl-main.385},
  pages     = {4190--4197},
  abstract  = {In the Transformer model, {``}self-attention{''} combines information from attended embeddings into the representation of the focal embedding in the next layer. Thus, across layers of the Transformer, information originating from different tokens gets increasingly mixed. This makes attention weights unreliable as explanations probes. In this paper, we consider the problem of quantifying this flow of information through self-attention. We propose two methods for approximating the attention to input tokens given attention weights, attention rollout and attention flow, as post hoc methods when we use attention weights as the relative relevance of the input tokens. We show that these methods give complementary views on the flow of information, and compared to raw attention, both yield higher correlations with importance scores of input tokens obtained using an ablation method and input gradients.}
}

@article{BERTnlp,
  title   = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author  = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  journal = {ArXiv},
  year    = {2019},
  volume  = {abs/1810.04805}
}

@article{wav2vec2_speech,
  title   = {wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations},
  author  = {Alexei Baevski and Henry Zhou and Abdel-rahman Mohamed and Michael Auli},
  journal = {NeurIPS},
  year    = {2020},
  volume  = {abs/2006.11477}
}


@misc{Authors14,
  author = {FirstName LastName},
  title  = {The frobnicatable foo filter},
  note   = {Face and Gesture submission ID 324. Supplied as supplemental material {\tt fg324.pdf}},
  year   = 2014
}
@article{fukushima1975cognitron,
  title     = {Cognitron: A self-organizing multilayered neural network},
  author    = {Fukushima, Kunihiko},
  journal   = {Biological cybernetics},
  volume    = {20},
  number    = {3},
  pages     = {121--136},
  year      = {1975},
  publisher = {Springer}
}
@article{guo2021cmt,
  title   = {{CMT: Convolutional Neural Networks Meet Vision Transformers}},
  author  = {Guo, Jianyuan and Han, Kai and Wu, Han and Xu, Chang and Tang, Yehui and Xu, Chunjing and Wang, Yunhe},
  journal = {arXiv preprint arXiv:2107.06263},
  year    = {2021}
}
@article{arnab2021vivit,
  title   = {{VIVIT: A video vision transformer}},
  author  = {Arnab, Anurag and Dehghani, Mostafa and Heigold, Georg and Sun, Chen and Lu{\v{c}}i{\'c}, Mario and Schmid, Cordelia},
  journal = {arXiv preprint arXiv:2103.15691},
  year    = {2021}
}
@article{hubel1962receptive,
  title     = {Receptive fields, binocular interaction and functional architecture in the cat's visual cortex},
  author    = {Hubel, David H and Wiesel, Torsten N},
  journal   = {The Journal of physiology},
  volume    = {160},
  number    = {1},
  pages     = {106--154},
  year      = {1962},
  publisher = {Wiley Online Library}
}

@misc{Authors14b,
  author = {FirstName LastName},
  title  = {Frobnication tutorial},
  note   = {Supplied as supplemental material {\tt tr.pdf}},
  year   = 2014
}
@article{d2021convit,
  title   = {Convit: Improving vision transformers with soft convolutional inductive biases},
  author  = {d'Ascoli, St{\'e}phane and Touvron, Hugo and Leavitt, Matthew and Morcos, Ari and Biroli, Giulio and Sagun, Levent},
  journal = {arXiv preprint arXiv:2103.10697},
  year    = {2021}
}
@inproceedings{x3d,
  author    = {Christoph Feichtenhofer},
  title     = {{X3D:} Expanding Architectures for Efficient Video Recognition},
  booktitle = {2020 {IEEE/CVF} Conference on Computer Vision and Pattern Recognition,
               {CVPR} 2020, Seattle, WA, USA, June 13-19, 2020},
  pages     = {200--210},
  publisher = {Computer Vision Foundation / {IEEE}},
  year      = {2020},
  url       = {https://openaccess.thecvf.com/content\_CVPR\_2020/html/Feichtenhofer\_X3D\_Expanding\_Architectures\_for\_Efficient\_Video\_Recognition\_CVPR\_2020\_paper.html},
  doi       = {10.1109/CVPR42600.2020.00028},
  timestamp = {Tue, 31 Aug 2021 14:00:04 +0200},
  biburl    = {https://dblp.org/rec/conf/cvpr/Feichtenhofer20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{Alpher02,
  author  = {FirstName Alpher},
  title   = {Frobnication},
  journal = PAMI,
  volume  = 12,
  number  = 1,
  pages   = {234--778},
  year    = 2002
}

@article{Alpher03,
  author  = {FirstName Alpher and  FirstName Fotheringham-Smythe},
  title   = {Frobnication revisited},
  journal = {Journal of Foo},
  volume  = 13,
  number  = 1,
  pages   = {234--778},
  year    = 2003
}

@article{Alpher04,
  author  = {FirstName Alpher and FirstName Fotheringham-Smythe and FirstName Gamow},
  title   = {Can a machine frobnicate?},
  journal = {Journal of Foo},
  volume  = 14,
  number  = 1,
  pages   = {234--778},
  year    = 2004
}

@inproceedings{Alpher05,
  author    = {FirstName Alpher and FirstName Gamow},
  title     = {Can a computer frobnicate?},
  booktitle = CVPR,
  pages     = {234--778},
  year      = 2005
}


@inproceedings{Das_2019_ICCV,
  author    = {xxxx},
  title     = {xxxx},
  booktitle = {All the details of this paper are annonymized to comply with double-blind policy},
  month     = {xxx},
  year      = {2019}
}
@inproceedings{stein201350salades,
  title     = {Combining embedded accelerometers with computer vision for recognizing food preparation activities},
  author    = {Stein, Sebastian and McKenna, Stephen J},
  booktitle = {Proceedings of the 2013 ACM international joint conference on Pervasive and ubiquitous computing},
  pages     = {729--738},
  year      = {2013}
}
@inproceedings{Dai_2021_ICCV,
  author    = {Dai, Rui and Das, Srijan and Bremond, Francois},
  title     = {Learning an Augmented RGB Representation With Cross-Modal Knowledge Distillation for Action Detection},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  month     = {October},
  year      = {2021},
  pages     = {13053-13064}
}
@inproceedings{chen2019multi,
  title     = {Multi-label image recognition with graph convolutional networks},
  author    = {Chen, Zhao-Min and Wei, Xiu-Shen and Wang, Peng and Guo, Yanwen},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages     = {5177--5186},
  year      = {2019}
}

@inproceedings{gan2015devnet,
  title     = {Devnet: A deep event network for multimedia event detection and evidence recounting},
  author    = {Gan, Chuang and Wang, Naiyan and Yang, Yi and Yeung, Dit-Yan and Hauptmann, Alex G},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages     = {2568--2577},
  year      = {2015}
}

@inproceedings{Dai_2021_WACV,
  author    = {Dai, Rui and Das, Srijan and Minciullo, Luca and Garattoni, Lorenzo and Francesca, Gianpiero and Bremond, Francois},
  title     = {{PDAN: Pyramid Dilated Attention Network for Action Detection}},
  booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
  month     = {January},
  year      = {2021},
  pages     = {2970-2979}
}
@article{MLAD,
  title   = {Modeling Multi-Label Action Dependencies for Temporal Action Localization},
  author  = {Tirupattur, Praveen and Duarte, Kevin and Rawat, Yogesh and Shah, Mubarak},
  journal = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year    = {2021}
}
@article{AFNET,
  title     = {{AFNet: Temporal Locality-aware Network with Dual Structure for Accurate and Fast Action Detection}},
  author    = {Chen, Guang and Zhang, Can and Zou, Yuexian},
  journal   = {IEEE Transactions on Multimedia},
  year      = {2020},
  publisher = {IEEE}
}
@inproceedings{huang2020improving,
  title     = {Improving action segmentation via graph-based temporal reasoning},
  author    = {Huang, Yifei and Sugano, Yusuke and Sato, Yoichi},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages     = {14024--14034},
  year      = {2020}
}

@inproceedings{chen2019learning,
  title     = {Learning semantic-specific graph representation for multi-label image recognition},
  author    = {Chen, Tianshui and Xu, Muxin and Hui, Xiaolu and Wu, Hefeng and Lin, Liang},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages     = {522--531},
  year      = {2019}
}
@inproceedings{gtad,
  title     = {{G-TAD: Sub-graph localization for temporal action detection}},
  author    = {Xu, Mengmeng and Zhao, Chen and Rojas, David S and Thabet, Ali and Ghanem, Bernard},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages     = {10156--10165},
  year      = {2020}
}
@inproceedings{PGCN2019ICCV,
  author    = {Runhao Zeng and
               Wenbing Huang and
               Mingkui Tan and
               Yu Rong and
               Peilin Zhao and
               Junzhou Huang and
               Chuang Gan},
  title     = {Graph Convolutional Networks for Temporal Action Localization},
  booktitle = {ICCV},
  year      = {2019}
}
@article{cui2019traffic,
  title     = {Traffic graph convolutional recurrent neural network: A deep learning framework for network-scale traffic learning and forecasting},
  author    = {Cui, Zhiyong and Henrickson, Kristian and Ke, Ruimin and Wang, Yinhai},
  journal   = {IEEE Transactions on Intelligent Transportation Systems},
  volume    = {21},
  number    = {11},
  pages     = {4883--4894},
  year      = {2019},
  publisher = {IEEE}
}
@article{yu20193d,
  title   = {3d graph convolutional networks with temporal graphs: A spatial information free framework for traffic forecasting},
  author  = {Yu, Bing and Li, Mengzhang and Zhang, Jiyong and Zhu, Zhanxing},
  journal = {arXiv preprint arXiv:1903.00919},
  year    = {2019}
}
@inproceedings{liu2020disentangling,
  title     = {Disentangling and Unifying Graph Convolutions for Skeleton-Based Action Recognition},
  author    = {Liu, Ziyu and Zhang, Hongwen and Chen, Zhenghao and Wang, Zhiyong and Ouyang, Wanli},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages     = {143--152},
  year      = {2020}
}
@article{velivckovic2017graph,
  title   = {Graph attention networks},
  author  = {Veli{\v{c}}kovi{\'c}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Lio, Pietro and Bengio, Yoshua},
  journal = {arXiv preprint arXiv:1710.10903},
  year    = {2017}
}
@inproceedings{2sagcn2019cvpr,
  title     = {Two-stream adaptive graph convolutional networks for skeleton-based action recognition},
  author    = {Shi, Lei and Zhang, Yifan and Cheng, Jian and Lu, Hanqing},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages     = {12026--12035},
  year      = {2019}
}
@inproceedings{alwassel2018diagnosing,
  title     = {Diagnosing error in temporal action detectors},
  author    = {Alwassel, Humam and Heilbron, Fabian Caba and Escorcia, Victor and Ghanem, Bernard},
  booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
  pages     = {256--272},
  year      = {2018}
}
@article{zhou2020graph,
  title     = {Graph neural networks: A review of methods and applications},
  author    = {Zhou, Jie and Cui, Ganqu and Hu, Shengding and Zhang, Zhengyan and Yang, Cheng and Liu, Zhiyuan and Wang, Lifeng and Li, Changcheng and Sun, Maosong},
  journal   = {AI Open},
  volume    = {1},
  pages     = {57--81},
  year      = {2020},
  publisher = {Elsevier}
}
@inproceedings{dai2021ctrn,
  title     = {{CTRN: Class Temporal Relational Network For Action Detection}},
  author    = {Dai, Rui and Das, Srijan and Bremond, Francois F},
  booktitle = {{BMVC 2021 - The British Machine Vision Conference}},
  address   = {Virtual, United Kingdom},
  year      = {2021},
  month     = Nov
}
@article{dai2020toyota,
  title   = {{Toyota Smarthome Untrimmed: Real-World Untrimmed Videos for Activity Detection.}},
  author  = {Rui Dai and Srijan Das and Saurav Sharma and Luca Minciullo and Lorenzo Garattoni and Francois Bremond and Gianpiero Francesca},
  journal = {arXiv preprint arXiv:2010.14982},
  year    = {2020}
}
@inproceedings{li2018deeper,
  title     = {Deeper insights into graph convolutional networks for semi-supervised learning},
  author    = {Li, Qimai and Han, Zhichao and Wu, Xiao-Ming},
  booktitle = {Thirty-Second AAAI conference on artificial intelligence},
  year      = {2018}
}

@article{kipf2016semi,
  title   = {Semi-supervised classification with graph convolutional networks},
  author  = {Kipf, Thomas N and Welling, Max},
  journal = {arXiv preprint arXiv:1609.02907},
  year    = {2016}
}
@inproceedings{zhangtqn,
  title     = {Temporal Query Networks for Fine-grained Video Understanding},
  author    = {Chuhan Zhang and Ankush Gputa and Andrew Zisserman},
  booktitle = {Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2021}
}
@inproceedings{kazakos2021MTCN,
  author    = {Kazakos, Evangelos and Huh, Jaesung and Nagrani, Arsha and Zisserman, Andrew and Damen, Dima},
  booktitle = {British Machine Vision Conference (BMVC)},
  title     = {With a Little Help from my Temporal Context: Multimodal Egocentric Action Recognition},
  year      = {2021}
}
@inproceedings{dai2019self,
  title        = {Self-Attention Temporal Convolutional Network for Long-Term Daily Living Activity Detection},
  author       = {Dai, Rui and Minciullo, Luca and Garattoni, Lorenzo and Francesca, Gianpiero and Bremond, Fran{\c{c}}ois},
  booktitle    = {2019 16th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)},
  pages        = {1--7},
  year         = {2019},
  organization = {IEEE}
}
@inproceedings{stgcn,
  title     = {Spatial temporal graph convolutional networks for skeleton-based action recognition},
  author    = {Yan, Sijie and Xiong, Yuanjun and Lin, Dahua},
  booktitle = {Thirty-second AAAI conference on artificial intelligence},
  year      = {2018}
}
@inproceedings{gleason2019proposal,
  title        = {A proposal-based solution to spatio-temporal action detection in untrimmed videos},
  author       = {Gleason, Joshua and Ranjan, Rajeev and Schwarcz, Steven and Castillo, Carlos and Chen, Jun-Cheng and Chellappa, Rama},
  booktitle    = {2019 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  pages        = {141--150},
  year         = {2019},
  organization = {IEEE}
}
@inproceedings{gupta2019lvis,
  title     = {LVIS: A dataset for large vocabulary instance segmentation},
  author    = {Gupta, Agrim and Dollar, Piotr and Girshick, Ross},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages     = {5356--5364},
  year      = {2019}
}

@article{sigurdsson2018charades-ego,
  title   = {Charades-ego: A large-scale dataset of paired third and first person videos},
  author  = {Sigurdsson, Gunnar A and Gupta, Abhinav and Schmid, Cordelia and Farhadi, Ali and Alahari, Karteek},
  journal = {arXiv preprint arXiv:1804.09626},
  year    = {2018}
}
@inproceedings{hollywood2,
  author    = {Marcin Marsza{\l}ek and Ivan Laptev and Cordelia Schmid},
  title     = {Actions in Context},
  booktitle = {IEEE Conference on Computer Vision \& Pattern Recognition},
  year      = {2009}
}
@inproceedings{geta,
  title        = {Learning to recognize objects in egocentric activities},
  author       = {Fathi, Alireza and Ren, Xiaofeng and Rehg, James M},
  booktitle    = {CVPR 2011},
  pages        = {3281--3288},
  year         = {2011},
  organization = {IEEE}
}

@article{sigurdsson2018charades-ego,
  title   = {Charades-ego: A large-scale dataset of paired third and first person videos},
  author  = {Sigurdsson, Gunnar A and Gupta, Abhinav and Schmid, Cordelia and Farhadi, Ali and Alahari, Karteek},
  journal = {arXiv preprint arXiv:1804.09626},
  year    = {2018}
}
@inproceedings{ikea2,
  title        = {Human pose forecasting via deep markov models},
  author       = {Toyer, Sam and Cherian, Anoop and Han, Tengda and Gould, Stephen},
  booktitle    = {2017 International Conference on Digital Image Computing: Techniques and Applications (DICTA)},
  pages        = {1--8},
  year         = {2017},
  organization = {IEEE}
}
@inproceedings{youcook2,
  title     = {Towards automatic learning of procedures from web instructional videos},
  author    = {Zhou, Luowei and Xu, Chenliang and Corso, Jason J},
  booktitle = {Thirty-Second AAAI Conference on Artificial Intelligence},
  year      = {2018}
}
@inproceedings{tang2019coin,
  title     = {COIN: A large-scale dataset for comprehensive instructional video analysis},
  author    = {Tang, Yansong and Ding, Dajun and Rao, Yongming and Zheng, Yu and Zhang, Danyang and Zhao, Lili and Lu, Jiwen and Zhou, Jie},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages     = {1207--1216},
  year      = {2019}
}

@misc{VIRAT,
  howpublished = {\url{http://www.viratdata.org/}},
  note         = {Accessed Feb. 28th, 2020},
  title        = {VIRAT Video dataset},
  author       = {DARPA and Kitware}
}

@misc{ICVL,
  howpublished = {\url{http://vision.inha.ac.kr/}},
  note         = {Accessed Feb. 28th, 2020},
  title        = {ICVL dataset},
  author       = {Inha Univiersity}
}
 


@inproceedings{dahlia,
  title        = {The daily home life activity dataset: a high semantic activity dataset for online recognition},
  author       = {Vaquette, Geoffrey and Orcesi, Astrid and Lucat, Laurent and Achard, Catherine},
  booktitle    = {2017 12th IEEE International Conference on Automatic Face \& Gesture Recognition (FG 2017)},
  pages        = {497--504},
  year         = {2017},
  organization = {IEEE}
}


@inproceedings{svw,
  title        = {Sports videos in the wild (SVW): A video dataset for sports analysis},
  author       = {Safdarnejad, Seyed Morteza and Liu, Xiaoming and Udpa, Lalita and Andrus, Brooks and Wood, John and Craven, Dean},
  booktitle    = {2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)},
  volume       = {1},
  pages        = {1--7},
  year         = {2015},
  organization = {IEEE}
}
@inproceedings{coco,
  title     = {Microsoft COCO: Common Objects in Context},
  author    = {Tsung-Yi Lin and Michael Maire and Serge J. Belongie and Lubomir D. Bourdev and Ross B. Girshick and James Hays and Pietro Perona and Deva Ramanan and Piotr Doll{\'a}r and C. Lawrence Zitnick},
  booktitle = {ECCV},
  year      = {2014}
}
@article{neuro_image,
  title   = {Dissociating temporal attention from spatial attention and motor response preparation: A high-density EEG study},
  journal = {NeuroImage},
  volume  = {124},
  pages   = {947 - 957},
  year    = {2016},
  issn    = {1053-8119},
  doi     = {https://doi.org/10.1016/j.neuroimage.2015.09.051},
  url     = {http://www.sciencedirect.com/science/article/pii/S1053811915008733},
  author  = {Frédéric Faugeras and Lionel Naccache}
}
@article{orientation_in_time,
  author  = { Nobre, A. C. and  Miniussi, C. and  Wilding, E. L. and  Coull, J. T.},
  title   = {{Orienting attention in time: Modulation of brain potentials}},
  journal = {Brain},
  volume  = {122},
  number  = {8},
  pages   = {1507-1518},
  year    = {1999},
  month   = {08},
  issn    = {0006-8950},
  doi     = {10.1093/brain/122.8.1507},
  url     = {https://dx.doi.org/10.1093/brain/122.8.1507}
}
@inproceedings{CAD-60,
  author    = {Sung, Jaeyongand and Ponce, Colin and Selman, Bart and  Saxena, Ashutosh},
  title     = {Human Activity Detection from RGBD Images},
  booktitle = {\textit{AAAI} workshop},
  year      = {2011}
}
@inproceedings{CAD-120,
  author    = { Koppula, Hema Swetha and Gupta, Rudhir and Saxena, Ashutosh},
  title     = {Learning Human Activities and Object Affordances from RGB-D Videos},
  booktitle = {IJRR},
  year      = {2013}
}
@article{miech2019howto100m,
  title   = {HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips},
  author  = {Miech, Antoine and Zhukov, Dimitri and Alayrac, Jean-Baptiste and Tapaswi, Makarand and Laptev, Ivan and Sivic, Josef},
  journal = {arXiv preprint arXiv:1906.03327},
  year    = {2019}
}
@inproceedings{howto,
  title        = {{How2:} A Large-scale Dataset For Multimodal Language Understanding},
  author       = {Sanabria, Ramon and Caglayan, Ozan and Palaskar, Shruti and Elliott, Desmond and Barrault, Lo\"ic and Specia, Lucia and Metze, Florian},
  booktitle    = {Proceedings of the Workshop on Visually Grounded Interaction and Language (ViGIL)},
  year         = {2018},
  organization = {NeurIPS},
  url          = {http://arxiv.org/abs/1811.00347}
}

@article{konig2015ecological,
  title     = {Ecological assessment of autonomy in instrumental activities of daily living in dementia patients by the means of an automatic video monitoring system},
  author    = {K{\"o}nig, Alexandra and Crispim-Junior, Carlos Fernando and Covella, Alvaro Gomez Uria and Bremond, Francois and Derreumaux, Alexandre and Bensadoun, Gregory and David, Renaud and Verhey, Frans and Aalten, Pauline and Robert, Philippe},
  journal   = {Frontiers in aging neuroscience},
  volume    = {7},
  pages     = {98},
  year      = {2015},
  publisher = {Frontiers}
}
@inproceedings{dai2019tan,
  title        = {Tan: Temporal aggregation network for dense multi-label action recognition},
  author       = {Dai, Xiyang and Singh, Bharat and Ng, Joe Yue-Hei and Davis, Larry},
  booktitle    = {2019 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  pages        = {151--160},
  year         = {2019},
  organization = {IEEE}
}
@inproceedings{wsgn,
  author    = {Fernando, Basura and Tan, Cheston and Bilen, Hakan},
  title     = {Weakly Supervised Gaussian Networks for Action Detection},
  booktitle = {The IEEE Winter Conference on Applications of Computer Vision (WACV)},
  month     = {March},
  year      = {2020}
}
@inproceedings{rohrbach2014coherent,
  title        = {Coherent multi-sentence video description with variable level of detail},
  author       = {Rohrbach, Anna and Rohrbach, Marcus and Qiu, Wei and Friedrich, Annemarie and Pinkal, Manfred and Schiele, Bernt},
  booktitle    = {German conference on pattern recognition},
  pages        = {184--195},
  year         = {2014},
  organization = {Springer}
}
@article{long-tailed,
  author        = {Grant Van Horn and
                   Pietro Perona},
  title         = {The Devil is in the Tails: Fine-grained Classification in the Wild},
  journal       = {CoRR},
  volume        = {abs/1709.01450},
  year          = {2017},
  url           = {http://arxiv.org/abs/1709.01450},
  archiveprefix = {arXiv},
  eprint        = {1709.01450},
  timestamp     = {Mon, 13 Aug 2018 16:47:45 +0200},
  biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1709-01450},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}
@article{ghosh2018stacked_stgcn,
  title   = {Stacked spatio-temporal graph convolutional networks for action segmentation},
  author  = {Ghosh, Pallabi and Yao, Yi and Davis, Larry S and Divakaran, Ajay},
  journal = {arXiv preprint arXiv:1811.10575},
  year    = {2018}
}
@misc{MEVA,
  howpublished = {\url{http://mevadata.org/}},
  note         = {Accessed Feb. 23th, 2020},
  title        = {Kitware. The Multiview Extended Video with Activities (MEVA) dataset}
}
@article{youtube8M,
  title   = {Youtube-8m: A large-scale video classification benchmark},
  author  = {Abu-El-Haija, Sami and Kothari, Nisarg and Lee, Joonseok and Natsev, Paul and Toderici, George and Varadarajan, Balakrishnan and Vijayanarasimhan, Sudheendra},
  journal = {arXiv preprint arXiv:1609.08675},
  year    = {2016}
}
@inproceedings{hu2014learning,
  title        = {Learning latent structure for activity recognition},
  author       = {Hu, Ninghang and Englebienne, Gwenn and Lou, Zhongyu and Kr{\"o}se, Ben},
  booktitle    = {2014 IEEE International Conference on Robotics and Automation (ICRA)},
  pages        = {1048--1053},
  year         = {2014},
  organization = {IEEE}
}

@inproceedings{NGM,
  title     = {Neural graph matching networks for fewshot 3d action recognition},
  author    = {Guo, Michelle and Chou, Edward and Huang, De-An and Song, Shuran and Yeung, Serena and Fei-Fei, Li},
  booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
  pages     = {653--669},
  year      = {2018}
}
@article{LSTM,
  title     = {Long short-term memory},
  author    = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal   = {Neural computation},
  volume    = {9},
  number    = {8},
  pages     = {1735--1780},
  year      = {1997},
  publisher = {MIT Press}
}
@inproceedings{inception_googlenet,
  title     = {Going deeper with convolutions},
  author    = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages     = {1--9},
  year      = {2015}
}

@article{wang2021pvtv2,
  title   = {{PVTv2: Improved baselines with pyramid vision transformer}},
  author  = {Wang, Wenhai and Xie, Enze and Li, Xiang and Fan, Deng-Ping and Song, Kaitao and Liang, Ding and Lu, Tong and Luo, Ping and Shao, Ling},
  journal = {arXiv preprint arXiv:2106.13797},
  year    = {2021}
}
@article{xie2021segformer,
  title   = {SegFormer: Simple and efficient design for semantic segmentation with transformers},
  author  = {Xie, Enze and Wang, Wenhai and Yu, Zhiding and Anandkumar, Anima and Alvarez, Jose M and Luo, Ping},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {34},
  pages   = {12077--12090},
  year    = {2021}
}
@article{cheng2021per,
  title   = {Per-pixel classification is not all you need for semantic segmentation},
  author  = {Cheng, Bowen and Schwing, Alexander G and Kirillov, Alexander},
  journal = {arXiv preprint arXiv:2107.06278},
  year    = {2021}
}
@article{wang2021pyramid,
  title   = {Pyramid vision transformer: A versatile backbone for dense prediction without convolutions},
  author  = {Wang, Wenhai and Xie, Enze and Li, Xiang and Fan, Deng-Ping and Song, Kaitao and Liang, Ding and Lu, Tong and Luo, Ping and Shao, Ling},
  journal = {arXiv preprint arXiv:2102.12122},
  year    = {2021}
}
@article{zhou2021deepvit,
  title   = {Deepvit: Towards deeper vision transformer},
  author  = {Zhou, Daquan and Kang, Bingyi and Jin, Xiaojie and Yang, Linjie and Lian, Xiaochen and Jiang, Zihang and Hou, Qibin and Feng, Jiashi},
  journal = {arXiv preprint arXiv:2103.11886},
  year    = {2021}
}
@inproceedings{lin2017single,
  title        = {Single shot temporal action detection},
  author       = {Lin, Tianwei and Zhao, Xu and Shou, Zheng},
  booktitle    = {Proceedings of the 25th ACM international conference on Multimedia},
  pages        = {988--996},
  year         = {2017},
  organization = {ACM}
}
@misc{Authors14,
  author = {Authors},
  title  = {The frobnicatable foo filter},
  note   = {Face and Gesture  submission ID 324. Supplied as additional material {\tt fg324.pdf}},
  year   = 2014
}
@inproceedings{superevent,
  title     = {Learning latent super-events to detect multiple activities in videos},
  author    = {Piergiovanni, AJ and Ryoo, Michael S},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  year      = {2018}
}
@inproceedings{TGM2,
  title     = {Temporal Gaussian Mixture Layer for Videos},
  author    = {A. Piergiovanni and {M. S. Ryoo}},
  booktitle = {International Conference on Machine Learning (ICML)},
  year      = {2019}
}
@article{oord2016wavenet,
  title   = {Wavenet: A generative model for raw audio},
  author  = {Oord, Aaron van den and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
  journal = {arXiv preprint arXiv:1609.03499},
  year    = {2016}
}
@inproceedings{attention,
  title     = {Attention is all you need},
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle = {Advances in neural information processing systems},
  pages     = {5998--6008},
  year      = {2017}
}

@inproceedings{ADL,
  title        = {Detecting activities of daily living in first-person camera views},
  author       = {Pirsiavash, Hamed and Ramanan, Deva},
  booktitle    = {2012 IEEE conference on computer vision and pattern recognition},
  pages        = {2847--2854},
  year         = {2012},
  organization = {IEEE}
}
@inproceedings{Damen2018EPICKITCHENS,
  title     = {Scaling Egocentric Vision: The EPIC-KITCHENS Dataset},
  author    = {Damen, Dima and Doughty, Hazel and Farinella, Giovanni Maria  and Fidler, Sanja and Furnari, Antonino and Kazakos, Evangelos and Moltisanti, Davide and Munro, Jonathan 
               and Perrett, Toby and Price, Will and Wray, Michael},
  booktitle = {European Conference on Computer Vision (ECCV)},
  year      = {2018}
} 
@article{Damen2020RESCALING,
  title   = {Rescaling Egocentric Vision},
  author  = {Damen, Dima and Doughty, Hazel and Farinella, Giovanni Maria  and and Furnari, Antonino 
             and Ma, Jian and Kazakos, Evangelos and Moltisanti, Davide and Munro, Jonathan 
             and Perrett, Toby and Price, Will and Wray, Michael},
  journal = {CoRR},
  volume  = {abs/2006.13256},
  year    = {2020},
  ee      = {http://arxiv.org/abs/2006.13256}
}
@article{TGM1,
  title   = {Temporal Gaussian Mixture Layer for Videos},
  author  = {Piergiovanni, AJ and Ryoo, Michael S},
  journal = {International Conference on Machine Learning (ICML)},
  year    = {2019}
}
@article{ramachandran2019stand,
  title   = {Stand-Alone Self-Attention in Vision Models},
  author  = {Ramachandran, Prajit and Parmar, Niki and Vaswani, Ashish and Bello, Irwan and Levskaya, Anselm and Shlens, Jonathon},
  journal = {arXiv preprint arXiv:1906.05909},
  year    = {2019}
}
@article{pkummd,
  title   = {PKU-MMD: A large scale benchmark for continuous multi-modal human action understanding},
  author  = {Liu, Chunhui and Hu, Yueyu and Li, Yanghao and Song, Sijie and Liu, Jiaying},
  journal = {arXiv preprint arXiv:1703.07475},
  year    = {2017}
}
@inproceedings{jain201515,
  title     = {What do 15,000 object categories tell us about classifying and localizing actions?},
  author    = {Jain, Mihir and Van Gemert, Jan C and Snoek, Cees GM},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages     = {46--55},
  year      = {2015}
}
@inproceedings{lea2017temporal,
  title     = {Temporal convolutional networks for action segmentation and detection},
  author    = {Lea, Colin and Flynn, Michael D and Vidal, Rene and Reiter, Austin and Hager, Gregory D},
  booktitle = {proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages     = {156--165},
  year      = {2017}
}
@inproceedings{resnet,
  title     = {Deep residual learning for image recognition},
  author    = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages     = {770--778},
  year      = {2016}
}
@inproceedings{zhang2016real,
  title     = {Real-time action recognition with enhanced motion vector CNNs},
  author    = {Zhang, Bowen and Wang, Limin and Wang, Zhe and Qiao, Yu and Wang, Hanli},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages     = {2718--2726},
  year      = {2016}
}
@inproceedings{SSN,
  title     = {Temporal action detection with structured segment networks},
  author    = {Zhao, Yue and Xiong, Yuanjun and Wang, Limin and Wu, Zhirong and Tang, Xiaoou and Lin, Dahua},
  booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
  pages     = {2914--2923},
  year      = {2017}
}
@inproceedings{yuan2016temporal,
  title     = {Temporal action localization with pyramid of score distribution features},
  author    = {Yuan, Jun and Ni, Bingbing and Yang, Xiaokang and Kassim, Ashraf A},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages     = {3093--3102},
  year      = {2016}
}
@misc{Authors14b,
  author = {Authors},
  title  = {Frobnication tutorial},
  note   = {Supplied as additional material {\tt tr.pdf}},
  year   = 2014
}
@inproceedings{hussein2019timeception,
  title     = {Timeception for Complex Action Recognition},
  author    = {Hussein, Noureldien and Gavves, Efstratios and Smeulders, Arnold WM},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages     = {254--263},
  year      = {2019}
}
@article{multi-thumos,
  title     = {Every moment counts: Dense detailed labeling of actions in complex videos},
  author    = {Yeung, Serena and Russakovsky, Olga and Jin, Ning and Andriluka, Mykhaylo and Mori, Greg and Fei-Fei, Li},
  journal   = {International Journal of Computer Vision},
  volume    = {126},
  number    = {2-4},
  pages     = {375--389},
  year      = {2018},
  publisher = {Springer}
}
@article{Alpher02,
  author  = {FirstName Alpher},
  title   = {Frobnication},
  journal = {Journal of Foo},
  volume  = 12,
  number  = 1,
  pages   = {234--778},
  year    = 2002
}
@inproceedings{hara2018can,
  title     = {Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet?},
  author    = {Hara, Kensho and Kataoka, Hirokatsu and Satoh, Yutaka},
  booktitle = {Proceedings of the IEEE conference on Computer Vision and Pattern Recognition},
  pages     = {6546--6555},
  year      = {2018}
}
@inproceedings{hara2017learning,
  title     = {Learning spatio-temporal features with 3D residual networks for action recognition},
  author    = {Hara, Kensho and Kataoka, Hirokatsu and Satoh, Yutaka},
  booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
  pages     = {3154--3160},
  year      = {2017}
}

@article{Alpher03,
  author  = {FirstName Alpher and  FirstName Fotheringham-Smythe},
  title   = {Frobnication revisited},
  journal = {Journal of Foo},
  volume  = 13,
  number  = 1,
  pages   = {234--778},
  year    = 2003
}

@article{Alpher04,
  author  = {FirstName Alpher and FirstName Fotheringham-Smythe and FirstName Gamow},
  title   = {Can a machine frobnicate?},
  journal = {Journal of Foo},
  volume  = 14,
  number  = 1,
  pages   = {234--778},
  year    = 2004
}
@article{EPIC,
  author        = {Dima Damen and
                   Hazel Doughty and
                   Giovanni Maria Farinella and
                   Sanja Fidler and
                   Antonino Furnari and
                   Evangelos Kazakos and
                   Davide Moltisanti and
                   Jonathan Munro and
                   Toby Perrett and
                   Will Price and
                   Michael Wray},
  title         = {Scaling Egocentric Vision: The {EPIC-KITCHENS} Dataset},
  journal       = {CoRR},
  volume        = {abs/1804.02748},
  year          = {2018},
  url           = {http://arxiv.org/abs/1804.02748},
  archiveprefix = {arXiv},
  eprint        = {1804.02748},
  timestamp     = {Mon, 13 Aug 2018 16:47:18 +0200},
  biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1804-02748},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{something,
  author    = {Raghav Goyal and
               Samira Ebrahimi Kahou and
               Vincent Michalski and
               Joanna Materzynska and
               Susanne Westphal and
               Heuna Kim and
               Valentin Haenel and
               Ingo Fr{\"{u}}nd and
               Peter Yianilos and
               Moritz Mueller{-}Freitag and
               Florian Hoppe and
               Christian Thurau and
               Ingo Bax and
               Roland Memisevic},
  title     = {The "Something Something" Video Database for Learning and Evaluating
               Visual Common Sense},
  booktitle = {{IEEE} International Conference on Computer Vision, {ICCV} 2017, Venice,
               Italy, October 22-29, 2017},
  pages     = {5843--5851},
  publisher = {{IEEE} Computer Society},
  year      = {2017},
  url       = {https://doi.org/10.1109/ICCV.2017.622},
  doi       = {10.1109/ICCV.2017.622},
  timestamp = {Thu, 23 Mar 2023 23:57:44 +0100},
  biburl    = {https://dblp.org/rec/conf/iccv/GoyalKMMWKHFYMH17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{Alexnet,
  title     = {Imagenet classification with deep convolutional neural networks},
  author    = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle = {Advances in neural information processing systems},
  pages     = {1097--1105},
  year      = {2012}
}

@article{simonyan2014very,
  title   = {Very deep convolutional networks for large-scale image recognition},
  author  = {Simonyan, Karen and Zisserman, Andrew},
  journal = {arXiv preprint arXiv:1409.1556},
  year    = {2014}
}


@inproceedings{slow_fast,
  author    = {Christoph Feichtenhofer and
               Haoqi Fan and
               Jitendra Malik and
               Kaiming He},
  title     = {SlowFast Networks for Video Recognition},
  booktitle = {2019 {IEEE/CVF} International Conference on Computer Vision, {ICCV}
               2019, Seoul, Korea (South), October 27 - November 2, 2019},
  pages     = {6201--6210},
  publisher = {{IEEE}},
  year      = {2019},
  url       = {https://doi.org/10.1109/ICCV.2019.00630},
  doi       = {10.1109/ICCV.2019.00630},
  timestamp = {Thu, 05 Mar 2020 13:43:22 +0100},
  biburl    = {https://dblp.org/rec/conf/iccv/Feichtenhofer0M19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{video_transformer_network,
  author        = {Rohit Girdhar and
                   Jo{\~{a}}o Carreira and
                   Carl Doersch and
                   Andrew Zisserman},
  title         = {Video Action Transformer Network},
  journal       = {CoRR},
  volume        = {abs/1812.02707},
  year          = {2018},
  url           = {http://arxiv.org/abs/1812.02707},
  archiveprefix = {arXiv},
  eprint        = {1812.02707},
  timestamp     = {Tue, 01 Jan 2019 15:01:25 +0100},
  biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1812-02707},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}
@article{lcrnet_new,
  title     = {{LCR-Net++: Multi-person 2D and 3D Pose Detection in Natural Images}},
  author    = {Rogez, Gr\'egory and Weinzaepfel, Philippe and Schmid, Cordelia},
  journal   = {{IEEE Transactions on Pattern Analysis and Machine Intelligence}},
  year      = {2019},
  publisher = {IEEE}
}
@article{ssd,
  title     = {{SSD: Single Shot MultiBox Detector}},
  author    = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
  journal   = {{ECCV 2016}},
  year      = {2019},
  publisher = {IEEE}
}
@inproceedings{deep-adaptiveTP,
  author    = {Song, Sibo and Cheung, Ngai-Man and Chandrasekhar, Vijay and Mandal, Bappaditya},
  title     = {Deep Adaptive Temporal Pooling for Activity Recognition},
  booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
  series    = {MM '18},
  year      = {2018},
  isbn      = {978-1-4503-5665-7},
  location  = {Seoul, Republic of Korea},
  pages     = {1829--1837},
  numpages  = {9},
  url       = {http://doi.acm.org/10.1145/3240508.3240713},
  doi       = {10.1145/3240508.3240713},
  acmid     = {3240713},
  publisher = {ACM},
  address   = {New York, NY, USA},
  keywords  = {adaptive temporal pooling, human activity recognition}
} 
@inproceedings{lin2019tsm,
  title     = {TSM: Temporal Shift Module for Efficient Video Understanding},
  author    = {Lin, Ji and Gan, Chuang and Han, Song},
  booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
  year      = {2019}
} 
@inproceedings{sigurdsson2017actions,
  title     = {What actions are needed for understanding human actions in videos?},
  author    = {Sigurdsson, Gunnar A and Russakovsky, Olga and Gupta, Abhinav},
  booktitle = {Proceedings of the IEEE international conference on computer vision},
  pages     = {2137--2146},
  year      = {2017}
}
@article{graves2005framewise,
  title     = {Framewise phoneme classification with bidirectional LSTM and other neural network architectures},
  author    = {Graves, Alex and Schmidhuber, J{\"u}rgen},
  journal   = {Neural networks},
  volume    = {18},
  number    = {5-6},
  pages     = {602--610},
  year      = {2005},
  publisher = {Elsevier}
}
[download]
@inproceedings{TSN,
  author    = {Limin Wang and Yuanjun Xiong and Zhe Wang
               and Yu Qiao and Dahua Lin and Xiaoou Tang and Luc {Val Gool}},
  title     = {Temporal Segment Networks: Towards Good Practices for Deep Action Recognition},
  booktitle = {ECCV},
  year      = {2016}
}
@article{ltc,
  title   = {Long-term Temporal Convolutions for Action Recognition},
  author  = {Varol, G{\"u}l and Laptev, Ivan and Schmid, Cordelia},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year    = {2018},
  volume  = {40},
  number  = {6},
  pages   = {1510--1517},
  doi     = {10.1109/TPAMI.2017.2712608}
}
@article{DescribingVB,
  title   = {Describing Videos by Exploiting Temporal Structure},
  author  = {Li Yao and Atousa Torabi and Kyunghyun Cho and Nicolas Ballas and Christopher Joseph Pal and Hugo Larochelle and Aaron C. Courville},
  journal = {2015 IEEE International Conference on Computer Vision (ICCV)},
  year    = {2015},
  pages   = {4507-4515}
}
@article{hogg,
  title    = {Model-based vision: a program to see a walking person},
  journal  = {Image and Vision Computing},
  volume   = {1},
  number   = {1},
  pages    = {5 - 20},
  year     = {1983},
  issn     = {0262-8856},
  doi      = {https://doi.org/10.1016/0262-8856(83)90003-3},
  url      = {http://www.sciencedirect.com/science/article/pii/0262885683900033},
  author   = {David Hogg},
  keywords = {vision, machine perception, WALKER model}
}
@article{recurrent_attention_image,
  title   = {Residual Attention Network for Image Classification},
  author  = {Wang, Fei and Jiang, Mengqing and Qian, Chen and Yang, Shuo and Li, Cheng and Zhang, Honggang and Wang, Xiaogang and Tang, Xiaoou},
  journal = {arXiv preprint arXiv:1704.06904},
  year    = {2017}
}
@article{multi-object-attention,
  title   = {Multiple Object Recognition with Visual Attention},
  author  = {Jimmy Ba and Volodymyr Mnih and Koray Kavukcuoglu},
  journal = {CoRR},
  year    = {2014},
  volume  = {abs/1412.7755}
}
@article{hard_attention_end,
  title   = {End-to-end Learning of Action Detection from Frame Glimpses in Videos},
  author  = {Yeung, Serena and Russakovsky, Olga and Mori, Greg and Fei-Fei, Li},
  journal = {arXiv preprint arXiv:1511.06984},
  year    = {2015}
}
@inproceedings{Recurrent_models,
  author    = {Mnih, Volodymyr and Heess, Nicolas and Graves, Alex and Kavukcuoglu, Koray},
  title     = {Recurrent Models of Visual Attention},
  booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
  series    = {NIPS'14},
  year      = {2014},
  location  = {Montreal, Canada},
  pages     = {2204--2212},
  numpages  = {9},
  url       = {http://dl.acm.org/citation.cfm?id=2969033.2969073},
  acmid     = {2969073},
  publisher = {MIT Press},
  address   = {Cambridge, MA, USA}
} 

@article{uniform_masking,
  author  = {Li, Xiang and Wang, Wenhai and Yang, Lingfeng and Yang, Jian},
  journal = {arXiv:2205.10063},
  title   = {Uniform Masking: Enabling MAE Pre-training for Pyramid-based Vision Transformers with Locality},
  year    = {2022}
}



@inproceedings{nktm,
  author    = {Hossein Rahmani and Ajmal Mian},
  booktitle = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Learning a non-linear knowledge transfer model for cross-view action recognition},
  year      = {2015},
  volume    = {},
  number    = {},
  pages     = {2458-2466},
  keywords  = {gesture recognition;image motion analysis;unsupervised learning;video cameras;video coding;nonlinear knowledge transfer model;cross-view action recognition;unseen views;unknown views;unsupervised learning;nonlinear model;canonical view;NKTM;deep network;weight decay;sparsity constraints;high-level virtual path;camera viewing directions;camera viewpoints;dense trajectories;mocap data;real video data;codebook;IXMAS datasets;N-UCLA datasets;Computational modeling;Training},
  doi       = {10.1109/CVPR.2015.7298860},
  issn      = {1063-6919},
  month     = {June}
}
@inproceedings{ncte,
  author    = {Ankur Gupta and Julieta Martinez and James J. Little and Robert J. Woodham},
  booktitle = {2014 IEEE Conference on Computer Vision and Pattern Recognition},
  title     = {3D Pose from Motion for Cross-View Action Recognition via Non-linear Circulant Temporal Encoding},
  year      = {2014},
  volume    = {},
  number    = {},
  pages     = {2601-2608},
  keywords  = {encoding;image matching;image motion analysis;image sequences;pose estimation;video signal processing;cross view action recognition;nonlinear circulant temporal encoding;transfer knowledge;unlabelled mocap data;3D pose sequence;multiple motion projection;closed form solution;IXMAS dataset unsupervised modality;Videos;Trajectory;Three-dimensional displays;Training;Databases;Kernel;Encoding},
  doi       = {10.1109/CVPR.2014.333},
  issn      = {1063-6919},
  month     = {June}
}
@inproceedings{hanklets,
  author    = {B. Li and O. I. Camps and M. Sznaier},
  booktitle = {2012 IEEE Conference on Computer Vision and Pattern Recognition},
  title     = {Cross-view activity recognition using Hankelets},
  year      = {2012},
  volume    = {},
  number    = {},
  pages     = {1362-1369},
  keywords  = {Hankel matrices;image classification;spatiotemporal phenomena;video signal processing;Hankelets;human activity recognition;visual surveillance;gaming interface;localized spatiotemporal features;testing data;training data;tracklets;robust cross-view activity recognition;classifier;IXMAS dataset;performance improvement;Trajectory;Vectors;Training;Noise measurement;Histograms;Testing;Cameras},
  doi       = {10.1109/CVPR.2012.6247822},
  issn      = {1063-6919},
  month     = {June}
}
@inproceedings{hpm,
  author    = {Hossein Rahmani and Ajmal Mian.},
  booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {3D Action Recognition from Novel Viewpoints},
  year      = {2016},
  volume    = {},
  number    = {},
  pages     = {1506-1515},
  keywords  = {convolution;feature extraction;Fourier analysis;image capture;image motion analysis;image representation;neural nets;pattern clustering;pose estimation;rendering (computer graphics);solid modelling;video signal processing;3D action recognition;viewpoints;human pose representation model;view-invariant high-level space;deep convolutional neural network;multiview training data;synthetic 3D human models;motion capture;rendering;CNN model;pose labels;pose clustering;real depth images;real depth videos;view-invariant feature extraction;spatio-temporal representation;group sparse Fourier temporal pyramid;Videos;Three-dimensional displays;Solid modeling;Data models;Skeleton;Feature extraction;Biological system modeling},
  doi       = {10.1109/CVPR.2016.167},
  issn      = {1063-6919},
  month     = {June}
}
@inproceedings{cvp,
  author    = {Z. Zhang and C. Wang and B. Xiao and W. Zhou and S. Liu and C. Shi},
  booktitle = {2013 IEEE Conference on Computer Vision and Pattern Recognition},
  title     = {Cross-View Action Recognition via a Continuous Virtual Path},
  year      = {2013},
  volume    = {},
  number    = {},
  pages     = {2690-2697},
  keywords  = {feature extraction;image classification;image motion analysis;video signal processing;cross-view action recognition;continuous virtual path;source view;target view;virtual view;linear transformation;action descriptor;infinite-dimensional feature;kernelized classifiers;IXMAS dataset;Kernel;Vectors;Training;Pattern recognition;Robustness;Feature extraction;Target recognition},
  doi       = {10.1109/CVPR.2013.347},
  issn      = {1063-6919},
  month     = {June}
}
@inproceedings{dvv,
  author    = {R. Li and T. Zickler},
  booktitle = {2012 IEEE Conference on Computer Vision and Pattern Recognition},
  title     = {Discriminative virtual views for cross-view action recognition},
  year      = {2012},
  volume    = {},
  number    = {},
  pages     = {2855-2862},
  keywords  = {object recognition;discriminative virtual views;cross-view action recognition;action descriptor linear transformation;transformation sequence;source view;target view partial labeling;action category;Training;Target recognition;Covariance matrix;Cameras;Feature extraction;Transforms;Vectors},
  doi       = {10.1109/CVPR.2012.6248011},
  issn      = {1063-6919},
  month     = {June}
}
@article{visualattention,
  title   = {Action Recognition using Visual Attention},
  author  = {Sharma, Shikhar and Kiros, Ryan and Salakhutdinov, Ruslan},
  journal = {arXiv preprint arXiv:1511.04119},
  year    = {2015}
} 
@inproceedings{nucla,
  author    = {Jiang Wang and Xiaohan Nie and Yin Xia and Ying Wu and Song-Chun Zhu},
  booktitle = {2014 IEEE Conference on Computer Vision and Pattern Recognition},
  title     = {Cross-View Action Modeling, Learning, and Recognition},
  year      = {2014},
  volume    = {},
  number    = {},
  pages     = {2649-2656},
  keywords  = {cameras;geometry;graph theory;image motion analysis;image recognition;image representation;video signal processing;cross-view action modeling;cross-view action learning;cross-view action recognition;video-based action recognition;multiview spatio-temporal AND-OR graph representation;MST-AOG representation;compositional model;hierarchical combinatorial structures;cross-view actions;geometry modeling;appearance modeling;motion variation modeling;3D human skeleton data;Kinect cameras;multiview action 3D dataset;action representation;2D videos;Three-dimensional displays;Joints;Training;Solid modeling;Training data;Pattern recognition},
  doi       = {10.1109/CVPR.2014.339},
  issn      = {1063-6919},
  month     = {June}
}
@article{joint_trajectory,
  title    = {Action recognition based on joint trajectory maps with convolutional neural networks},
  journal  = {Knowledge-Based Systems},
  volume   = {158},
  pages    = {43 - 53},
  year     = {2018},
  issn     = {0950-7051},
  doi      = {https://doi.org/10.1016/j.knosys.2018.05.029},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950705118302582},
  author   = {Pichao Wang and Wanqing Li and Chuankun Li and Yonghong Hou},
  keywords = {Action recognition, Trajectory, Color encoding, Convolutional neural network}
}
@inproceedings{global-context,
  author    = {J. Liu and G. Wang and P. Hu and L. Duan and A. C. Kot},
  booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Global Context-Aware Attention LSTM Networks for 3D Action Recognition},
  year      = {2017},
  volume    = {},
  number    = {},
  pages     = {3671-3680},
  keywords  = {feature extraction;image motion analysis;image recognition;image representation;image sequences;learning (artificial intelligence);recurrent neural nets;informative joints;action sequence;global contextual information;reliable attention representation;recurrent attention mechanism;GCA-LSTM network;attention performance;end-to-end network;network yields;Global Context-Aware Attention LSTM networks;Long Short-Term Memory networks;3D human action recognition;action analysis;irrelevant joints;strong attention capability;skeleton sequence;Three-dimensional displays;Skeleton;Reliability;Solid modeling;Logic gates;Hidden Markov models;Data models},
  doi       = {10.1109/CVPR.2017.391},
  issn      = {1063-6919},
  month     = {July}
}
@inproceedings{sta_lstm,
  title     = {An End-to-End Spatio-Temporal Attention Model for Human Action Recognition from Skeleton Data},
  author    = {Song, Sijie and Lan, Cuiling and Xing, Junliang and Zeng, Wenjun and Liu, Jiaying},
  booktitle = {AAAI Conference on Artificial Intelligence},
  year      = {2017},
  pages     = {4263--4270}
}
@inproceedings{valstm,
  author    = {Zhang, Pengfei and Lan, Cuiling and Xing, Junliang and Zeng, Wenjun and Xue, Jianru and Zheng, Nanning},
  title     = {View Adaptive Recurrent Neural Networks for High Performance Human Action Recognition From Skeleton Data},
  booktitle = {The IEEE International Conference on Computer Vision (ICCV)},
  month     = {Oct},
  year      = {2017}
}

@inproceedings{glimpse,
  author    = {Baradel, Fabien and Wolf, Christian and Mille, Julien and Taylor, Graham W.},
  title     = {Glimpse Clouds: Human Activity Recognition From Unstructured Feature Points},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  year      = {2018}
}
@inproceedings{skeletalquads,
  author    = {Evangelidis, G., and  Singh, G., and Horaud, R.},
  booktitle = {2014 22nd International Conference on Pattern Recognition},
  title     = {Skeletal Quads: Human Action Recognition Using Joint Quadruples},
  year      = {2014},
  volume    = {},
  number    = {},
  pages     = {4513-4518},
  keywords  = {feature extraction;Gaussian processes;image motion analysis;image recognition;image representation;support vector machines;skeletal quads;human action recognition;joint quadruples;human motion analysis;human skeleton structure extraction;depth images;local skeleton descriptor;similarity normalisation transform;compact 6D view-invariant skeletal feature;Fisher kernel representation;Gaussian mixture model;Fisher vector;multilevel representation;action description;linear SVM;MSRAction3D datasets;HDM05 datasets;Joints;Vectors;Accuracy;Kernel;Training;Three-dimensional displays},
  doi       = {10.1109/ICPR.2014.772},
  issn      = {1051-4651},
  month     = {Aug}
}
@inproceedings{liegroup,
  author    = {Vemulapalli, R., and Arrate, F., and Chellappa, R.},
  booktitle = {2014 IEEE Conference on Computer Vision and Pattern Recognition},
  title     = {Human Action Recognition by Representing 3D Skeletons as Points in a Lie Group},
  year      = {2014},
  volume    = {},
  number    = {},
  pages     = {588-595},
  keywords  = {curve fitting;estimation theory;Fourier analysis;gesture recognition;image motion analysis;Lie algebras;Lie groups;support vector machines;3D skeletons;Lie group;cost-effective depth sensors;real-time skeleton estimation algorithm;skeleton-based human action recognition;skeleton-based approach;joint location;joint angle;human skeleton;skeletal representation;3D geometric relationship;3D space;3D rigid body motion;special Euclidean group;curved manifold;action curves;Lie algebra;vector space;dynamic time warping;Fourier temporal pyramid representation;linear SVM;Joints;Three-dimensional displays;Algebra;Geometry;Hidden Markov models;Sensors;Action Recognition;Special Euclidean Group;Lie Groups},
  doi       = {10.1109/CVPR.2014.82},
  issn      = {1063-6919},
  month     = {June}
}

@inproceedings{C3D,
  author    = {Tran, Du and Bourdev, Lubomir and Fergus, Rob and Torresani, Lorenzo and Paluri, Manohar},
  title     = {Learning Spatiotemporal Features with 3D Convolutional Networks},
  booktitle = {Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV)},
  series    = {ICCV '15},
  year      = {2015},
  isbn      = {978-1-4673-8391-2},
  pages     = {4489--4497},
  numpages  = {9},
  url       = {http://dx.doi.org/10.1109/ICCV.2015.510},
  doi       = {10.1109/ICCV.2015.510},
  acmid     = {2919929},
  publisher = {IEEE Computer Society},
  address   = {Washington, DC, USA}
} 
@inproceedings{STA_hands,
  author    = {Baradel, Fabien and Wolf, Christian and Mille, Julien},
  booktitle = {2017 IEEE International Conference on Computer Vision Workshops (ICCVW)},
  title     = {Human Action Recognition: Pose-Based Attention Draws Focus to Hands},
  year      = {2017},
  volume    = {},
  number    = {},
  pages     = {604-613},
  keywords  = {image recognition;pose estimation;recurrent neural nets;NTU-RGB+D human action recognition dataset;human articulated pose;pose-based attention;recurrent neural network;soft-attention based mechanisms;spatial attention distributions;spatio-temporal attention;temporal attention distributions;Computer architecture;Computer vision;Feature extraction;Recurrent neural networks;Streaming media;Visualization},
  doi       = {10.1109/ICCVW.2017.77},
  issn      = {},
  month     = {Oct}
}
@article{ucf,
  author     = {Khurram Soomro and
                Amir Roshan Zamir and
                Mubarak Shah},
  title      = {{UCF101:} {A} Dataset of 101 Human Actions Classes From Videos in
                The Wild},
  journal    = {CoRR},
  volume     = {abs/1212.0402},
  year       = {2012},
  url        = {http://arxiv.org/abs/1212.0402},
  eprinttype = {arXiv},
  eprint     = {1212.0402},
  timestamp  = {Mon, 13 Aug 2018 16:47:45 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1212-0402.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}
@article{Johansson,
  author   = {Johansson, Gunnar},
  title    = {Visual perception of biological motion and a model for its analysis},
  journal  = {Perception {\&} Psychophysics},
  year     = {1973},
  month    = {Jun},
  day      = {01},
  volume   = {14},
  number   = {2},
  pages    = {201--211},
  abstract = {This paper reports the first phase of a research program on visual perception of motion patterns characteristic of living organisms in locomotion. Such motion patterns in animals and men are termed here as biological motion. They are characterized by a far higher degree of complexity than the patterns of simple mechanical motions usually studied in our laboratories. In everyday perceptions, the visual information from biological motion and from the corresponding figurative contour patterns (the shape of the body) are intermingled. A method for studying information from the motion pattern per se without interference with the form aspect was devised. In short, the motion of the living body was represented by a few bright spots describing the motions of the main joints. It is found that 10--12 such elements in adequate motion combinations in proximal stimulus evoke a compelling impression of human walking, running, dancing, etc. The kinetic-geometric model for visual vector analysis originally developed in the study of perception of motion combinations of the mechanical type was applied to these biological motion patterns. The validity of this model in the present context was experimentally tested and the results turned out to be highly positive.},
  issn     = {1532-5962},
  doi      = {10.3758/BF03212378},
  url      = {https://doi.org/10.3758/BF03212378}
}
@inproceedings{SBU-kinect,
  title        = {Two-person Interaction Detection Using Body-Pose Features and Multiple Instance Learning},
  author       = {Kiwon Yun and Jean Honorio and Debaleena Chattopadhyay and Tamara L. Berg and Dimitris Samaras},
  booktitle    = {Computer Vision and Pattern Recognition Workshops (CVPRW), 2012 IEEE Computer Society Conference on},
  year         = {2012},
  organization = {IEEE}
}

@article{adam_optimizer,
  author        = {Diederik P. Kingma and
                   Jimmy Ba},
  title         = {{Adam: {A} Method for Stochastic Optimization}},
  journal       = {CoRR},
  volume        = {abs/1412.6980},
  year          = {2014},
  url           = {http://arxiv.org/abs/1412.6980},
  archiveprefix = {arXiv},
  eprint        = {1412.6980},
  timestamp     = {Wed, 07 Jun 2017 14:40:52 +0200},
  biburl        = {http://dblp.org/rec/bib/journals/corr/KingmaB14},
  bibsource     = {dblp computer science bibliography, http://dblp.org}
}



@inproceedings{gradientclipping,
  author    = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
  title     = {Sequence to Sequence Learning with Neural Networks},
  booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
  series    = {NIPS'14},
  year      = {2014},
  location  = {Montreal, Canada},
  pages     = {3104--3112},
  numpages  = {9},
  url       = {http://dl.acm.org/citation.cfm?id=2969033.2969173},
  acmid     = {2969173},
  publisher = {MIT Press},
  address   = {Cambridge, MA, USA}
} 
@article{Dropout,
  author     = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  title      = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  journal    = {J. Mach. Learn. Res.},
  issue_date = {January 2014},
  volume     = {15},
  number     = {1},
  month      = jan,
  year       = {2014},
  issn       = {1532-4435},
  pages      = {1929--1958},
  numpages   = {30},
  url        = {http://dl.acm.org/citation.cfm?id=2627435.2670313},
  acmid      = {2670313},
  publisher  = {JMLR.org},
  keywords   = {deep learning, model combination, neural networks, regularization}
} 
@misc{tensorflow2015,
  title  = { {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
  url    = {https://www.tensorflow.org/},
  note   = {Software available from tensorflow.org},
  author = {
            Mart\'{\i}n~Abadi and
            others},
  year   = {2015}
}
@misc{chollet2015keras,
  title     = {Keras},
  author    = {Chollet, Fran\c{c}ois and others},
  year      = {2015},
  publisher = {GitHub}
}
@inproceedings{Range_sample,
  author    = {C. Lu and J. Jia and C. K. Tang},
  booktitle = {2014 IEEE Conference on Computer Vision and Pattern Recognition},
  title     = {Range-Sample Depth Feature for Action Recognition},
  year      = {2014},
  volume    = {},
  number    = {},
  pages     = {772-779},
  keywords  = {computer vision;gesture recognition;image motion analysis;learning (artificial intelligence);τ tests;action recognition;binary range-sample depth feature;standard learning algorithms;Accuracy;Hamming distance;Histograms;Joints;Robustness;Standards;Three-dimensional displays;Action Recognition;Binary Feature;Depth;Sampling},
  doi       = {10.1109/CVPR.2014.104},
  issn      = {1063-6919},
  month     = {June}
}
@article{JOULE-SVM,
  author   = {Jian-Fang Hu and Wei-Shi Zheng and Jianhuang Lai and Jianguo Zhang},
  journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title    = {Jointly Learning Heterogeneous Features for RGB-D Activity Recognition},
  year     = {2017},
  volume   = {39},
  number   = {11},
  pages    = {2186-2200},
  keywords = {image colour analysis;inference mechanisms;iterative methods;learning (artificial intelligence);optimisation;RGB-D activity benchmarking;RGB-D activity recognition;feature channels;feature-specific intermediate transforms;fusion learning;heterogeneous features;heterogeneous multitask learning;i-transforms;joint learning model;joint model;latent shared features;shared feature-specific components;simple inference model;three-step iterative optimization algorithm;Feature extraction;Image color analysis;Skeleton;Three-dimensional displays;Transforms;Visualization;Heterogeneous features learning;RGB-D activity recognition;action recognition},
  doi      = {10.1109/TPAMI.2016.2640292},
  issn     = {0162-8828},
  month    = {Nov}
}
@article{LSTM_basic,
  author     = {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
  title      = {Long Short-Term Memory},
  journal    = {Neural Comput.},
  issue_date = {November 15, 1997},
  volume     = {9},
  number     = {8},
  month      = nov,
  year       = {1997},
  issn       = {0899-7667},
  pages      = {1735--1780},
  numpages   = {46},
  url        = {http://dx.doi.org/10.1162/neco.1997.9.8.1735},
  doi        = {10.1162/neco.1997.9.8.1735},
  acmid      = {1246450},
  publisher  = {MIT Press},
  address    = {Cambridge, MA, USA}
} 
@inproceedings{gemetricfeaturesWACV2017,
  author    = {S. Zhang and X. Liu and J. Xiao},
  booktitle = {2017 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  title     = {On Geometric Features for Skeleton-Based Action Recognition Using Multilayer LSTM Networks},
  year      = {2017},
  volume    = {},
  number    = {},
  pages     = {148-157},
  keywords  = {geometry;gesture recognition;3-layer LSTM framework;RNN model enhancement;RNN-based approach;geometric features;geometric relational features;multilayer LSTM networks;skeleton-based action recognition;Computational modeling;Computer architecture;Logic gates;Neurons;Nonhomogeneous media;Skeleton;Three-dimensional displays},
  doi       = {10.1109/WACV.2017.24},
  issn      = {},
  month     = {March}
}
@inproceedings{lrcn,
  author    = {Donahue, Jeffrey and Anne Hendricks, Lisa and Guadarrama, Sergio and Rohrbach, Marcus and Venugopalan, Subhashini and Saenko, Kate and Darrell, Trevor},
  title     = {Long-Term Recurrent Convolutional Networks for Visual Recognition and Description},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  year      = {2015}
}
@inproceedings{1,
  title     = {Learning realistic human actions from movies},
  author    = {Ivan Laptev and Marcin Marszałek and Cordelia Schmid and Benjamin Rozenfeld},
  booktitle = {CVPR},
  year      = {2008}
}
@inproceedings{2,
  title     = {Recognizing human actions: A local SVM approach},
  author    = {Christian Schuldt and Ivan Laptev and Barbara Caputo},
  booktitle = {ICPR},
  year      = {2004}
}
@inproceedings{3,
  title     = {Action recognition with improved trajectories},
  author    = {Heng Wang and Cordelia Schmid},
  booktitle = {ICCV},
  year      = {2013}
}
@inproceedings{4,
  title     = {Space-time Interest Points},
  author    = {Ivan Laptev and Tony Lindeberg},
  booktitle = {ICCV},
  year      = {2003}
}
@inproceedings{5,
  title     = {Super Normal Vector for Activity Recognition Using Depth Sequences},
  author    = {Xiaodong Yang and Yingli Tian},
  booktitle = {CVPR},
  year      = {2014}
}
@inproceedings{7,
  title     = {Bilinear heterogeneous information machine for {RGB}-{D} action recognition},
  author    = {Kong, Yu and Fu, Yun},
  booktitle = {CVPR},
  year      = {2015}
}
@inproceedings{8a,
  title     = {Mining Actionlet Ensemble for Action Recognition with Depth Cameras},
  author    = {Wu, Ying},
  booktitle = {CVPR},
  year      = {2012}
}
@article{8b,
  title    = {Action {Recognition} {Using} {Rate}-{Invariant} {Analysis} of {Skeletal} {Shape} {Trajectories}},
  author   = {Amor, B.B. and Su, J. and Srivastava, A.},
  journal  = {PAMI},
  year     = {2016},
  month    = Jan,
  number   = {1},
  pages    = {1--13},
  volume   = {38},
  file     = {IEEE Xplore Abstract Record:/user/mkopersk/home/.mozilla/firefox/mb40m4tj.default/zotero/storage/AGZ2SA7J/articleDetails.html:text/html},
  keywords = {Action recognition, Depth sensors, Hidden Markov models, Manifold Trajectories, Measurement, Riemannian geometry, Shape, Skeletal data, Skeleton, Space vehicles, Three-dimensional displays, Trajectory}
}
@inproceedings{8c,
  title     = {A Decision Forest Based Feature Selection Framework for Action Recognition
               from RGB-Depth Cameras},
  author    = {Farhood Negin and
               Firat {\"{O}}zdemir and
               Ceyhun Burak Akg{\"{u}}l and
               Kamer Ali Y{\"{u}}ksel and
               Ayt{\"{u}}l Er{\c{c}}il},
  booktitle = {ICIAR},
  year      = {2013}
}
@inproceedings{8d,
  title     = {Rolling Rotations for Recognizing Human Actions from 3DSkeletal Data},
  author    = {Raviteja Vemulapalli and Rama Chellappa},
  booktitle = {CVPR},
  year      = {2016}
}
@inproceedings{9,
  title     = {Leveraging Hierarchial Parametric Networks for Skeletal Joints Based Action Segmentation and Recognition},
  author    = {Di Wu and Ling Shao},
  booktitle = {CVPR},
  year      = {2014}
}
@inproceedings{10,
  title     = {Imagenet classification with deep convolutional neural networks},
  author    = {Alex Krizhevsky and Ilya Sutskever and Geoffrey E. Hinton},
  booktitle = {NIPS},
  year      = {2012}
}
@inproceedings{11,
  title     = {Large-{Scale} {Video} {Classification} with {Convolutional} {Neural} {Networks}},
  author    = {Karpathy, Andrej and Toderici, George and Shetty, Sanketh and Leung, Thomas and Sukthankar, Rahul and Fei-Fei, Li},
  booktitle = {CVPR},
  year      = {2014}
}
@inproceedings{12,
  title     = {Action {Recognition} {With} {Trajectory}-{Pooled} {Deep}-{Convolutional} {Descriptors}},
  author    = {Wang, Limin and Qiao, Yu and Tang, Xiaoou},
  booktitle = {CVPR},
  year      = {2015}
}
@inproceedings{13,
  title     = {Two-stream convolutional networks for action recognition in videos},
  author    = {Karen Simonyan and Andrew Zisserman},
  booktitle = {NIPS},
  year      = {2014}
}
@inproceedings{14,
  title     = {Beyond short snippets: Deep networks for video classification},
  author    = {Joe Yue-Hei and Matthew Hausknecht and Sudheendra Vijayanarasimhan and Oriol Vinyals and Rajat Monga and George Toderici},
  booktitle = {CVPR},
  year      = {2015}
}
@inproceedings{15,
  title     = {Regularizing LSTM with 3D Human-Skeleton Sequences for Action Recognition},
  author    = {Behrooz Mahasseni and Sinisa Todorovic},
  booktitle = {CVPR},
  year      = {2016}
}
@inproceedings{16,
  title     = {Articulated people detection and pose estimation:Reshaping the future},
  author    = {Leonid Pishchulin  and Arjun Jain and Mykhaylo Andriluka and Thorsten Thormahlen and Bernt Schiele},
  booktitle = {CVPR},
  year      = {2012}
}
@inproceedings{17,
  title     = {Using k-poselets for detecting people and localizing their keypoints},
  author    = {Georgia Gkioxari and Bharath Hariharan and Ross Girshick and Jitendra Malik},
  booktitle = {CVPR},
  year      = {2014}
}
@inproceedings{18,
  title     = {Articulated part-based model for joint object detection and pose estimation},
  author    = {Min Sun and Silvio Savarese},
  booktitle = {ICCV},
  year      = {2011}
}
@misc{19,
  author = {Umar Iqbal and Juergen Gall},
  title  = {Multi-Person Pose Estimation with Local Joint-to-Person Associations},
  year   = {2016},
  eprint = {arXiv:1608.08526}
}
@inproceedings{20,
  title     = {Deepcut: Joint subset partition and labeling for multi person pose estimation},
  author    = {Leonid Pishchulin and Eldar Insafutdinov and Siyu Tang and Bjoern Andres and Mykhaylo Andriluka and Peter Gehler and Bernt Schiele},
  booktitle = {CVPR},
  year      = {2016}
}
@article{21,
  title   = {Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields},
  author  = {Zhe Cao and Tomas Simon and Shih-En Wei and Yaser Sheikh},
  journal = {arXiv preprint arXiv:1611.08050},
  year    = {2016}
}
@inproceedings{22,
  title     = {P-CNN: Pose-based CNN Features for Action Recognition},
  author    = {Guilhem Cheron and Ivan Laptev and Cordelia Schmid},
  booktitle = {ICCV},
  year      = {2015}
}  
@inproceedings{23,
  title     = {Recognizing Actions from Depth Cameras as Weakly Aligned Multi-part Bag-of-Poses},
  author    = {Seidenari, L. and Varano, V. and Berretti, S. and Del Bimbo, A. and Pala, P.},
  booktitle = {CVPRW},
  year      = {2013}
}
@inproceedings{24,
  title     = {HON4D: Histogram of oriented 4D normals for activity recognition from depth sequences},
  author    = {Omar Oreifej and Zicheng Liu},
  booktitle = {CVPR},
  year      = {2013}
}
@article{25,
  title   = {Evaluating spatiotemporal interest point features for depth-based action recognition },
  author  = {Yu Zhu and Wenbin Chen and Guodong Guo},
  journal = {Image and Vision Computing },
  year    = {2014},
  number  = {8},
  pages   = {453 - 464},
  volume  = {32},
  doi     = {http://dx.doi.org/10.1016/j.imavis.2014.04.005},
  issn    = {0262-8856},
  url     = {http://www.sciencedirect.com/science/article/pii/S0262885614000651}
}
@inproceedings{26,
  title     = {Multi-modal feature fusion for action recognition in RGB-D sequences},
  author    = {Shahroudy, A. and Gang Wang and Tian-Tsong Ng},
  booktitle = {ISCCSP},
  year      = {2014}
}


@inproceedings{pivit,
  author    = {Dominick Reilly and Srijan Das},
  title     = {Just Add $\pi$! Pose Induced Video Transformers for Understanding Activities of Daily Living},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  year      = {2024}
}
@article{spence2009crossmodal,
  title={Crossmodal processing [Editorial]},
  author={Spence, Charles and Senkowski, Daniel and R{\"o}der, Brigitte},
  journal={Experimental Brain Research},
  volume={198},
  number={2-3},
  pages={107--111},
  year={2009},
  doi={10.1007/s00221-009-1973-4},
  url={https://doi.org/10.1007/s00221-009-1973-4}
}
@article{negin2019unsupervised,
  title={An Unsupervised Framework for Online Spatiotemporal Detection of Activities of Daily Living by Hierarchical Activity Models},
  author={Negin, Fatemeh and Br{\'e}mond, Fran{\c{c}}ois},
  journal={Sensors (Basel)},
  volume={19},
  number={19},
  pages={4237},
  year={2019},
  publisher={Multidisciplinary Digital Publishing Institute},
  doi={10.3390/s19194237},
  note={Published 2019 Sep 29}
}

@InProceedings{ego4d,
    author    = {Grauman, Kristen and Westbury, Andrew and Byrne, Eugene and Chavis, Zachary and Furnari, Antonino and Girdhar, Rohit and Hamburger, Jackson and Jiang, Hao and Liu, Miao and Liu, Xingyu and Martin, Miguel and Nagarajan, Tushar and Radosavovic, Ilija and Ramakrishnan, Santhosh Kumar and Ryan, Fiona and Sharma, Jayant and Wray, Michael and Xu, Mengmeng and Xu, Eric Zhongcong and Zhao, Chen and Bansal, Siddhant and Batra, Dhruv and Cartillier, Vincent and Crane, Sean and Do, Tien and Doulaty, Morrie and Erapalli, Akshay and Feichtenhofer, Christoph and Fragomeni, Adriano and Fu, Qichen and Gebreselasie, Abrham and Gonz\'alez, Cristina and Hillis, James and Huang, Xuhua and Huang, Yifei and Jia, Wenqi and Khoo, Weslie and Kol\'a\v{r}, J\'achym and Kottur, Satwik and Kumar, Anurag and Landini, Federico and Li, Chao and Li, Yanghao and Li, Zhenqiang and Mangalam, Karttikeya and Modhugu, Raghava and Munro, Jonathan and Murrell, Tullie and Nishiyasu, Takumi and Price, Will and Ruiz, Paola and Ramazanova, Merey and Sari, Leda and Somasundaram, Kiran and Southerland, Audrey and Sugano, Yusuke and Tao, Ruijie and Vo, Minh and Wang, Yuchen and Wu, Xindi and Yagi, Takuma and Zhao, Ziwei and Zhu, Yunyi and Arbel\'aez, Pablo and Crandall, David and Damen, Dima and Farinella, Giovanni Maria and Fuegen, Christian and Ghanem, Bernard and Ithapu, Vamsi Krishna and Jawahar, C. V. and Joo, Hanbyul and Kitani, Kris and Li, Haizhou and Newcombe, Richard and Oliva, Aude and Park, Hyun Soo and Rehg, James M. and Sato, Yoichi and Shi, Jianbo and Shou, Mike Zheng and Torralba, Antonio and Torresani, Lorenzo and Yan, Mingfei and Malik, Jitendra},
    title     = {Ego4D: Around the World in 3,000 Hours of Egocentric Video},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {18995-19012}
}
@article{kevin2022egovlp,
  title={Egocentric Video-Language Pretraining},
  author={Lin, Kevin Qinghong and Wang, Alex Jinpeng and Soldan, Mattia and Wray, Michael and Yan, Rui and Xu, Eric Zhongcong and Gao, Difei and Tu, Rongcheng and Zhao, Wenzhe and Kong, Weijie and others},
  journal={arXiv preprint arXiv:2206.01670},
  year={2022}
}
@inproceedings{singh2022flava,
  author = {
    Amanpreet Singh and
    Ronghang Hu and
    Vedanuj Goswami and
    Guillaume Couairon and
    Wojciech Galuba and
    Marcus Rohrbach and
    Douwe Kiela
  },
  title = {
    {FLAVA:} {A} Foundational Language And 
    Vision Alignment Model
  },
  booktitle={CVPR},
  year={2022}
}
@inproceedings{kinect,
  author    = {Shotton, Jamie and Fitzgibbon, Andrew and Cook, Mat and Sharp, Toby and Finocchio, Mark and Moore, Richard and Kipman, Alex and Blake, Andrew},
  booktitle = {CVPR 2011},
  title     = {Real-time human pose recognition in parts from single depth images},
  year      = {2011},
  volume    = {},
  number    = {},
  pages     = {1297-1304},
  doi       = {10.1109/CVPR.2011.5995316}
}
@inproceedings{27,
  title     = {Spatio-temporal Depth Cuboid Similarity Feature for Activity Recognition Using Depth Camera},
  author    = {Lu Xia and Aggarwal, J.K.},
  booktitle = {CVPR},
  year      = {2013}
}
@inproceedings{29,
  title     = {Learning Discriminative Representations from RGB-D Video Data},
  author    = {Liu, Li and Shao, Ling},
  booktitle = {IJCAI},
  year      = {2013}
}
@article{35,
  title      = {Learning Human Activities and Object Affordances from RGB-D Videos},
  author     = {Koppula, Hema Swetha and Gupta, Rudhir and Saxena, Ashutosh},
  journal    = {Int. J. Rob. Res.},
  year       = {2013},
  month      = jul,
  number     = {8},
  pages      = {951--970},
  volume     = {32},
  acmid      = {2502928},
  address    = {Thousand Oaks, CA, USA},
  doi        = {10.1177/0278364913478446},
  issn       = {0278-3649},
  issue_date = {July 2013},
  keywords   = {3D perception, human activity detection, object affordance, personal robots, spatio-temporal context, supervised learning},
  numpages   = {20},
  publisher  = {Sage Publications, Inc.},
  url        = {http://dx.doi.org/10.1177/0278364913478446}
}
@inproceedings{36,
  title     = {Jointly learning heterogeneous features for {RGB}-{D} activity recognition},
  author    = {Hu Jian-Fang and Zheng Wei-Shi and Lai Jianhuang and Zhang JianGuo},
  booktitle = {CVPR},
  year      = {2015},
  doi       = {10.1109/CVPR.2015.7299172}
}
@inproceedings{33,
  title     = {Modeling Spatial Layout of Features for Real World Scenario RGB-D Action Recognition},
  author    = {Michal Koperski and Francois Bremond},
  booktitle = {AVSS},
  year      = {2016}
}
@inproceedings{38,
  title     = {High accuracy optical flow estimation based on a theory for warping},
  author    = {Thomas Brox and Andres Bruhn and Nils Papenberg and Joachim Weickert},
  booktitle = {ECCV},
  year      = {2004}
}
@inproceedings{39,
  title     = {Finding action tubes},
  author    = {Georgia Gkloxari and Jitendra Malik},
  booktitle = {CVPR},
  year      = {2015}
}
@inproceedings{40,
  title     = {Unstructured Human Activity Detection from RGBD Images},
  author    = {Jaeyong Sung and Colin Ponce and Bart Selman and Ashutosh Saxena},
  booktitle = {ICRA},
  year      = {2012}
}

@inproceedings{bilinski:hal-01054943,
  title       = {{Representing Visual Appearance by Video Brownian Covariance Descriptor for Human Action Recognition}},
  author      = {Bilinski, Piotr and Koperski, Michal and Bak, Slawomir and Bremond, Fran{\c c}ois},
  url         = {https://hal.inria.fr/hal-01054943},
  booktitle   = {{AVSS}},
  year        = {2014},
  keywords    = {computer visiosn ; action recognition},
  pdf         = {https://hal.inria.fr/hal-01054943/file/Bilinski-VideoBrownianCovariance-AVSS2014.pdf},
  hal_id      = {hal-01054943},
  hal_version = {v2}
}

@inproceedings{koperski:hal-01054949,
  title       = {{3D Trajectories for Action Recognition}},
  author      = {Koperski, Michal and Bilinski, Piotr and Bremond, Fran{\c c}ois},
  url         = {https://hal.inria.fr/hal-01054949},
  booktitle   = {{ICIP}},
  year        = {2014},
  keywords    = {computer vision ; action recognition},
  pdf         = {https://hal.inria.fr/hal-01054949/file/koperski-icip.pdf},
  hal_id      = {hal-01054949},
  hal_version = {v1}
}
@inproceedings{Srijan,
  title     = {Action Recognition based on a mixture of RGB and Depth based skeleton},
  author    = {Das, Srijan and Koperski, Michal and Bremond, Francois and Francesca, Gianpiero},
  booktitle = {AVSS},
  year      = {2017}
}
@inproceedings{41,
  title     = {A spatio-temporal descriptor based on ¨
               3d-gradients},
  author    = { Klaser, Alexander and Marszaek, Marcin  and  Schmid Cordelia},
  booktitle = {BMVC},
  year      = {2008}
}
@inproceedings{42,
  author    = {N. Dalal and B. Triggs},
  booktitle = {2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)},
  title     = {Histograms of oriented gradients for human detection},
  year      = {2005},
  volume    = {1},
  pages     = {886-893 vol. 1},
  keywords  = {feature extraction;gradient methods;object detection;object recognition;support vector machines;coarse spatial binning;contrast normalization;edge based descriptors;fine orientation binning;fine-scale gradients;gradient based descriptors;histograms of oriented gradients;human detection;linear SVM;overlapping descriptor;pedestrian database;robust visual object recognition;High performance computing;Histograms;Humans;Image databases;Image edge detection;Object detection;Object recognition;Robustness;Support vector machines;Testing},
  doi       = {10.1109/CVPR.2005.177},
  issn      = {1063-6919},
  month     = {June}
}


@inproceedings{43,
  author    = {I. Laptev and M. Marszalek and C. Schmid and B. Rozenfeld},
  booktitle = {2008 IEEE Conference on Computer Vision and Pattern Recognition},
  title     = {Learning realistic human actions from movies},
  year      = {2008},
  pages     = {1-8},
  keywords  = {cinematography;image classification;image retrieval;learning (artificial intelligence);support vector machines;video signal processing;automatic video annotation;human action retrieval;local space-time feature;movie script;multichannel nonlinear SVM;space-time pyramid;text-based classifier;video action classification;video realistic human action recognition;visual learning;Cameras;Clothing;Humans;Image recognition;Layout;Motion pictures;Object recognition;Robustness;Text categorization;Video sharing},
  doi       = {10.1109/CVPR.2008.4587756},
  issn      = {1063-6919},
  month     = {June}
}
@inproceedings{44,
  author    = {S. Zhang and X. Liu and J. Xiao},
  booktitle = {2017 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  title     = {On Geometric Features for Skeleton-Based Action Recognition Using Multilayer LSTM Networks},
  year      = {2017},
  pages     = {148-157},
  keywords  = {Computational modeling;Computer architecture;Logic gates;Neurons;Nonhomogeneous media;Skeleton;Three-dimensional displays},
  doi       = {10.1109/WACV.2017.24},
  month     = {March}
}

@inproceedings{45,
  author    = {Feichtenhofer, Christoph and Pinz, Axel and Zisserman, Andrew},
  title     = {Convolutional Two-Stream Network Fusion for Video Action Recognition},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  year      = {2016}
}

@article{chu2021conditional,
  title   = {Conditional positional encodings for vision transformers},
  author  = {Chu, Xiangxiang and Tian, Zhi and Zhang, Bo and Wang, Xinlong and Wei, Xiaolin and Xia, Huaxia and Shen, Chunhua},
  journal = {arXiv preprint arXiv:2102.10882},
  year    = {2021}
}

@article{zhu2020deformable,
  title   = {Deformable detr: Deformable transformers for end-to-end object detection},
  author  = {Zhu, Xizhou and Su, Weijie and Lu, Lewei and Li, Bin and Wang, Xiaogang and Dai, Jifeng},
  journal = {arXiv preprint arXiv:2010.04159},
  year    = {2020}
}
@article{MSRnew,
  author   = {A. Shahroudy and T. T. Ng and Y. Gong and G. Wang},
  journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title    = {Deep Multimodal Feature Analysis for Action Recognition in RGB+D Videos},
  year     = {2017},
  volume   = {PP},
  number   = {99},
  pages    = {1-1},
  keywords = {Correlation;Feature extraction;Robustness;Sensors;Skeleton;Three-dimensional displays;Videos;Action Recognition;Multimodal Analysis;RGB+D;Structured Sparsity},
  doi      = {10.1109/TPAMI.2017.2691321},
  issn     = {0162-8828},
  month    = {}
}

@article{tan2021relaxed,
  title   = {Relaxed transformer decoders for direct action proposal generation},
  author  = {Tan, Jing and Tang, Jiaqi and Wang, Limin and Wu, Gangshan},
  journal = {arXiv preprint arXiv:2102.01894},
  year    = {2021}
}

@article{Das_icip,
  author        = {{Das}, S. and {Koperski}, M. and {Bremond}, F. and {Francesca}, G.
                   },
  title         = {{A Fusion of Appearance based CNNs and Temporal evolution of Skeleton with LSTM for Daily Living Action Recognition}},
  journal       = {ArXiv e-prints},
  archiveprefix = {arXiv},
  eprint        = {1802.00421},
  primaryclass  = {cs.CV},
  keywords      = {Computer Science - Computer Vision and Pattern Recognition},
  year          = 2018,
  month         = feb,
  adsurl        = {http://adsabs.harvard.edu/abs/2018arXiv180200421D},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{HOF,
  title        = {Learning realistic human actions from movies},
  author       = {Laptev, Ivan and Marszalek, Marcin and Schmid, Cordelia and Rozenfeld, Benjamin},
  booktitle    = {Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on},
  pages        = {1--8},
  year         = {2008},
  organization = {IEEE}
}

@inproceedings{MBH,
  title        = {Human detection using oriented histograms of flow and appearance},
  author       = {Dalal, Navneet and Triggs, Bill and Schmid, Cordelia},
  booktitle    = {European conference on computer vision},
  pages        = {428--441},
  year         = {2006},
  organization = {Springer}
}

@inproceedings{HOG3D,
  title        = {A spatio-temporal descriptor based on 3d-gradients},
  author       = {Klaser, Alexander and Marsza{\l}ek, Marcin and Schmid, Cordelia},
  booktitle    = {BMVC 2008-19th British Machine Vision Conference},
  pages        = {275--1},
  year         = {2008},
  organization = {British Machine Vision Association}
}

@inproceedings{gridhog,
  title        = {Modeling spatial layout of features for real world scenario rgb-d action recognition},
  author       = {Koperski, Michal and Bremond, Francois},
  booktitle    = {Advanced Video and Signal Based Surveillance (AVSS), 2016 13th IEEE International Conference on},
  pages        = {44--50},
  year         = {2016},
  organization = {IEEE}
}

@inproceedings{transferring,
  title        = {Learning and transferring mid-level image representations using convolutional neural networks},
  author       = {Oquab, Maxime and Bottou, Leon and Laptev, Ivan and Sivic, Josef},
  booktitle    = {Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on},
  pages        = {1717--1724},
  year         = {2014},
  organization = {IEEE}
}

@inproceedings{IDT,
  author    = {Heng Wang and Cordelia Schmid},
  title     = {Action Recognition with Improved Trajectories},
  booktitle = {IEEE International Conference on Computer Vision},
  year      = {2013},
  address   = {Sydney, Australia},
  url       = {http://hal.inria.fr/hal-00873267}
}

@inproceedings{DT,
  author    = {Heng Wang and Alexander Kl{\"a}ser and Cordelia Schmid and Cheng-Lin Liu},
  title     = {{Action Recognition by Dense Trajectories}},
  booktitle = {IEEE Conference on Computer Vision \& Pattern Recognition},
  year      = {2011},
  month     = Jun,
  pages     = {3169-3176},
  address   = {Colorado Springs, United States},
  url       = {http://hal.inria.fr/inria-00583818/en}
}

@inproceedings{fischer,
  title        = {Improving the fisher kernel for large-scale image classification},
  author       = {Perronnin, Florent and S{\'a}nchez, Jorge and Mensink, Thomas},
  booktitle    = {European conference on computer vision},
  pages        = {143--156},
  year         = {2010},
  organization = {Springer}
}

@inproceedings{tdd,
  title     = {Action recognition with trajectory-pooled deep-convolutional descriptors},
  author    = {Wang, Limin and Qiao, Yu and Tang, Xiaoou},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages     = {4305--4314},
  year      = {2015}
}
@inproceedings{tslstm,
  title     = {Ensemble deep learning for skeleton-based action recognition using temporal sliding LSTM networks},
  author    = {Lee, Inwoong and Kim, Doyoung and Kang, Seoungyoon and Lee, Sanghoon},
  year      = {2017},
  booktitle = {Proceedings of the IEEE International Conference on Computer Vision}
}


@inproceedings{twostream,
  title     = {Two-stream convolutional networks for action recognition in videos},
  author    = {Simonyan, Karen and Zisserman, Andrew},
  booktitle = {Advances in neural information processing systems},
  pages     = {568--576},
  year      = {2014}
}

@inproceedings{twostreamfusion,
  title        = {Convolutional Two-Stream Network Fusion for Video Action Recognition},
  author       = {Feichtenhofer, Christoph and Pinz, Axel and Zisserman, Andrew},
  booktitle    = {Computer Vision and Pattern Recognition (CVPR), 2016 IEEE Conference on},
  pages        = {1933--1941},
  year         = {2016},
  organization = {IEEE}
}



@inproceedings{pseudo3d,
  title        = {Learning spatio-temporal representation with pseudo-3d residual networks},
  author       = {Qiu, Zhaofan and Yao, Ting and Mei, Tao},
  booktitle    = {2017 IEEE International Conference on Computer Vision (ICCV)},
  pages        = {5534--5542},
  year         = {2017},
  organization = {IEEE}
}
@inproceedings{Salient,
  author    = {L. Rybok and B. Schauerte and Z. Al-Halah and R. Stiefelhagen},
  booktitle = {IEEE Winter Conference on Applications of Computer Vision},
  title     = { Important stuff, everywhere! Activity recognition with salient proto-objects as context},
  year      = {2014},
  volume    = {},
  number    = {},
  pages     = {646-651},
  keywords  = {object detection;object recognition;action recognition framework;activity recognition;object detectors;object information;object-action relationships;saliency detection;salient proto-objects;training data;Accuracy;Context;Detectors;Image segmentation;Microwave theory and techniques;Training data;Videos},
  doi       = {10.1109/WACV.2014.6836041},
  issn      = {1550-5790},
  month     = {March}
}

@article{Lin2016,
  author   = {Lin, Liang
              and Wang, Keze
              and Zuo, Wangmeng
              and Wang, Meng
              and Luo, Jiebo
              and Zhang, Lei},
  title    = {A Deep Structured Model with Radius--Margin Bound for 3D Human Activity Recognition},
  journal  = {International Journal of Computer Vision},
  year     = {2016},
  month    = {Jun},
  day      = {01},
  volume   = {118},
  number   = {2},
  pages    = {256--273},
  abstract = {Understanding human activity is very challenging even with the recently developed 3D/depth sensors. To solve this problem, this work investigates a novel deep structured model, which adaptively decomposes an activity instance into temporal parts using the convolutional neural networks. Our model advances the traditional deep learning approaches in two aspects. First, we incorporate latent temporal structure into the deep model, accounting for large temporal variations of diverse human activities. In particular, we utilize the latent variables to decompose the input activity into a number of temporally segmented sub-activities, and accordingly feed them into the parts (i.e. sub-networks) of the deep architecture. Second, we incorporate a radius--margin bound as a regularization term into our deep model, which effectively improves the generalization performance for classification. For model training, we propose a principled learning algorithm that iteratively (i) discovers the optimal latent variables (i.e. the ways of activity decomposition) for all training instances, (ii) updates the classifiers based on the generated features, and (iii) updates the parameters of multi-layer neural networks. In the experiments, our approach is validated on several complex scenarios for human activity recognition and demonstrates superior performances over other state-of-the-art approaches.},
  issn     = {1573-1405},
  doi      = {10.1007/s11263-015-0876-z},
  url      = {https://doi.org/10.1007/s11263-015-0876-z}
}
@inproceedings{mstcn,
  title     = {Ms-tcn: Multi-stage temporal convolutional network for action segmentation},
  author    = {Farha, Yazan Abu and Gall, Jurgen},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages     = {3575--3584},
  year      = {2019}
}
@inproceedings{action_for_actions,
  author    = {Gunnar A. Sigurdsson and Olga Russakovsky and Abhinav Gupta},
  title     = {What Actions are Needed for Understanding Human Actions in Videos?},
  booktitle = {International Conference on Computer Vision (ICCV)},
  year      = {2017},
  pdf       = {http://arxiv.org/pdf/1708.02696.pdf},
  poster    = {sigurdsson2017actions_poster.pdf},
  code      = {https://github.com/gsig/actions-for-actions}
}
@inproceedings{qiu2018precise,
  title        = {Precise temporal action localization by evolving temporal proposals},
  author       = {Qiu, Haonan and Zheng, Yingbin and Ye, Hao and Lu, Yao and Wang, Feng and He, Liang},
  booktitle    = {Proceedings of the 2018 ACM on International Conference on Multimedia Retrieval},
  pages        = {388--396},
  year         = {2018},
  organization = {ACM}
}

@inproceedings{kahatapitiya2021coarse,
  title     = {{Coarse-Fine Networks for Temporal Activity Detection in Videos}},
  author    = {Kahatapitiya, Kumara and Ryoo, Michael S},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages     = {8385--8394},
  year      = {2021}
}

@inproceedings{nam2014large,
  title        = {Large-scale multi-label text classification—revisiting neural networks},
  author       = {Nam, Jinseok and Kim, Jungi and Menc{\'\i}a, Eneldo Loza and Gurevych, Iryna and F{\"u}rnkranz, Johannes},
  booktitle    = {Joint european conference on machine learning and knowledge discovery in databases},
  pages        = {437--452},
  year         = {2014},
  organization = {Springer}
}
@inproceedings{zhu2017efficient,
  title        = {Efficient action detection in untrimmed videos via multi-task learning},
  author       = {Zhu, Yi and Newsam, Shawn},
  booktitle    = {2017 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  pages        = {197--206},
  year         = {2017},
  organization = {IEEE}
}
@inproceedings{RC3d,
  title     = {R-c3d: Region convolutional 3d network for temporal activity detection},
  author    = {Xu, Huijuan and Das, Abir and Saenko, Kate},
  booktitle = {Proceedings of the IEEE international conference on computer vision},
  pages     = {5783--5792},
  year      = {2017}
}
@article{weinzaepfel2016daly,
  title   = {Human Action Localization with Sparse Spatial Supervision},
  author  = {Weinzaepfel, Philippe and Martin, Xavier and Schmid, Cordelia},
  journal = {arXiv preprint arXiv:1605.05197},
  year    = {2016}
}
@inproceedings{breakfast,
  title     = {The language of actions: Recovering the syntax and semantics of goal-directed human activities},
  author    = {Kuehne, Hilde and Arslan, Ali and Serre, Thomas},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages     = {780--787},
  year      = {2014}
}
@inproceedings{sigurdsson2017asynchronous,
  title     = {Asynchronous temporal fields for action recognition},
  author    = {Sigurdsson, Gunnar A and Divvala, Santosh and Farhadi, Ali and Gupta, Abhinav},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages     = {585--594},
  year      = {2017}
}
@inproceedings{caba2015activitynet,
  title     = {Activitynet: A large-scale video benchmark for human activity understanding},
  author    = {Caba Heilbron, Fabian and Escorcia, Victor and Ghanem, Bernard and Carlos Niebles, Juan},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages     = {961--970},
  year      = {2015}
}
@inproceedings{ye2015eventnet,
  title        = {Eventnet: A large scale structured concept library for complex event detection in video},
  author       = {Ye, Guangnan and Li, Yitong and Xu, Hongliang and Liu, Dong and Chang, Shih-Fu},
  booktitle    = {Proceedings of the 23rd ACM international conference on Multimedia},
  pages        = {471--480},
  year         = {2015},
  organization = {ACM}
}
@inproceedings{STS,
  author    = {Koppula, Hema S. and Saxena, Ashutosh},
  title     = {Learning Spatio-temporal Structure from RGB-D Videos for Human Activity Detection and Anticipation},
  booktitle = {Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28},
  series    = {ICML'13},
  year      = {2013},
  location  = {Atlanta, GA, USA},
  pages     = {III-792--III-800},
  url       = {http://dl.acm.org/citation.cfm?id=3042817.3043025},
  acmid     = {3043025},
  publisher = {JMLR.org}
} 

@inproceedings{lstm3d,
  title     = {Regularizing long short term memory with 3D human-skeleton sequences for action recognition},
  author    = {Mahasseni, Behrooz and Todorovic, Sinisa},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages     = {3054--3062},
  year      = {2016}
}

@inproceedings{R-HCRF,
  author    = {T. Liu and X. Wang and X. Dai and J. Luo},
  booktitle = {2016 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  title     = {Deep recursive and hierarchical conditional random fields for human action recognition},
  year      = {2016},
  volume    = {},
  number    = {},
  pages     = {1-9},
  keywords  = {approximation theory;image motion analysis;image recognition;image representation;statistical analysis;support vector machines;CAD-120 benchmark dataset;DR-HCRFs model;block-coordinate primal-dual Frank-Wolfe algorithm;complex action recognition scenario modelling;deep recursive and hierarchical conditional random field model;discriminative models;human action recognition;infinite-order temporal-dependencies;linear-chain CRFs;mean-field-like approximation;model inference;model marginal likelihood approximation;support vector machine framework;temporal sequential labeling;Computational modeling;Context;Context modeling;Graphical models;Hidden Markov models;Inference algorithms;Target recognition},
  doi       = {10.1109/WACV.2016.7477694},
  issn      = {},
  month     = {March}
}

@article{posebased,
  title   = {A hierarchical pose-based approach to complex action understanding using dictionaries of actionlets and motion poselets},
  author  = {Lillo, Ivan and Niebles, Juan Carlos and Soto, Alvaro},
  journal = {arXiv preprint arXiv:1606.04992},
  year    = {2016}
}
@article{liu2017enhanced,
  title     = {Enhanced skeleton visualization for view invariant human action recognition},
  author    = {Liu, Mengyuan and Liu, Hong and Chen, Chen},
  journal   = {Pattern Recognition},
  volume    = {68},
  pages     = {346--362},
  year      = {2017},
  publisher = {Elsevier}
}
@inproceedings{hog2,
  author    = {E. Ohn-Bar and M. M. Trivedi},
  booktitle = {2013 IEEE Conference on Computer Vision and Pattern Recognition Workshops},
  title     = {Joint Angles Similarities and HOG2 for Action Recognition},
  year      = {2013},
  volume    = {},
  number    = {},
  pages     = {465-470},
  keywords  = {feature extraction;human computer interaction;image classification;image colour analysis;object recognition;object tracking;support vector machines;2D array;HOG2;action recognition;bag-of-words scheme;color images;depth images;depth maps;histogram of oriented gradient algorithm;human body;human-computer interaction;joint angles similarities;linear SVM;multiclass classification scheme;pairwise affinities;skeleton tracking;spatio-temporal feature extraction;view-invariant joint angles features;Accuracy;Feature extraction;Histograms;Joints;Trajectory;Vectors},
  doi       = {10.1109/CVPRW.2013.76},
  issn      = {2160-7508},
  month     = {June}
}
@inproceedings{st-lstm,
  author    = {Liu, Jun
               and Shahroudy, Amir
               and Xu, Dong
               and Wang, Gang},
  editor    = {Leibe, Bastian
               and Matas, Jiri
               and Sebe, Nicu
               and Welling, Max},
  title     = {Spatio-Temporal LSTM with Trust Gates for 3D Human Action Recognition},
  booktitle = {Computer Vision -- ECCV 2016},
  year      = {2016},
  publisher = {Springer International Publishing},
  address   = {Cham},
  pages     = {816--833},
  abstract  = {3D action recognition -- analysis of human actions based on 3D skeleton data -- becomes popular recently due to its succinctness, robustness, and view-invariant representation. Recent attempts on this problem suggested to develop RNN-based learning methods to model the contextual dependency in the temporal domain. In this paper, we extend this idea to spatio-temporal domains to analyze the hidden sources of action-related information within the input data over both domains concurrently. Inspired by the graphical structure of the human skeleton, we further propose a more powerful tree-structure based traversal method. To handle the noise and occlusion in 3D skeleton data, we introduce new gating mechanism within LSTM to learn the reliability of the sequential input data and accordingly adjust its effect on updating the long-term context information stored in the memory cell. Our method achieves state-of-the-art performance on 4 challenging benchmark datasets for 3D human action analysis.},
  isbn      = {978-3-319-46487-9}
}

@article{ftp-ds,
  author   = {Jian-Fang Hu and Wei-Shi Zheng and Jianhuang Lai and Jianguo Zhang},
  journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title    = {Jointly Learning Heterogeneous Features for RGB-D Activity Recognition},
  year     = {2017},
  volume   = {39},
  number   = {11},
  pages    = {2186-2200},
  keywords = {image colour analysis;inference mechanisms;iterative methods;learning (artificial intelligence);optimisation;RGB-D activity benchmarking;RGB-D activity recognition;feature channels;feature-specific intermediate transforms;fusion learning;heterogeneous features;heterogeneous multitask learning;i-transforms;joint learning model;joint model;latent shared features;shared feature-specific components;simple inference model;three-step iterative optimization algorithm;Feature extraction;Image color analysis;Skeleton;Three-dimensional displays;Transforms;Visualization;Heterogeneous features learning;RGB-D activity recognition;action recognition},
  doi      = {10.1109/TPAMI.2016.2640292},
  issn     = {0162-8828},
  month    = {Nov}
}

@inproceedings{Hu_2017_CVPR,
  author    = {Hu, Peiyun and Ramanan, Deva},
  title     = {Finding Tiny Faces},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {July},
  year      = {2017}
}
@inproceedings{yan2018spatial,
  title     = {Spatial temporal graph convolutional networks for skeleton-based action recognition},
  author    = {Yan, Sijie and Xiong, Yuanjun and Lin, Dahua},
  booktitle = {Thirty-second AAAI conference on artificial intelligence},
  year      = {2018}
}
@inproceedings{shi2019two,
  title     = {Two-stream adaptive graph convolutional networks for skeleton-based action recognition},
  author    = {Shi, Lei and Zhang, Yifan and Cheng, Jian and Lu, Hanqing},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages     = {12026--12035},
  year      = {2019}
}
@inproceedings{de2018modeling,
  title        = {Modeling temporal structure with LSTM for online action detection},
  author       = {De Geest, Roeland and Tuytelaars, Tinne},
  booktitle    = {2018 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  pages        = {1549--1557},
  year         = {2018},
  organization = {IEEE}
}
@inproceedings{richard2017weakly,
  title     = {Weakly supervised action learning with rnn based fine-to-coarse modeling},
  author    = {Richard, Alexander and Kuehne, Hilde and Gall, Juergen},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages     = {754--763},
  year      = {2017}
}
@article{carrara2019lstm,
  title     = {LSTM-based real-time action detection and prediction in human motion streams},
  author    = {Carrara, Fabio and Elias, Petr and Sedmidubsky, Jan and Zezula, Pavel},
  journal   = {Multimedia Tools and Applications},
  volume    = {78},
  number    = {19},
  pages     = {27309--27331},
  year      = {2019},
  publisher = {Springer}
}
@inproceedings{du2015hierarchical,
  title     = {Hierarchical recurrent neural network for skeleton based action recognition},
  author    = {Du, Yong and Wang, Wei and Wang, Liang},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages     = {1110--1118},
  year      = {2015}
}
@inproceedings{imagenet_cvpr09,
  author    = {Jia Deng and Wei Dong and Richard Socher and Li-Jia Li and Kai Li and Li Fei-Fei},
  title     = {{ImageNet: A Large-Scale Hierarchical Image Database}},
  booktitle = {CVPR09},
  year      = {2009},
  bibsource = {http://www.image-net.org/papers/imagenet_cvpr09.bib}
}
        
@inproceedings{RGBD-HuDaAct,
  author    = {Bingbing Ni and Gang Wang and Pierre Moulin},
  title     = {{RGBD-HuDaAct: A color-depth video database for human daily activity recognition}},
  booktitle = {2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops)},
  month     = {Nov},
  year      = {2011}
}
@inproceedings{UWAmultiview2,
  author    = {Hossein Rahmani and Arif Mahmood and Du Huynh and Ajmal Mian},
  title     = {Histogram of oriented principal components for cross-view action
               recognition},
  booktitle = {TPAMI},
  year      = {2016}
}

@inproceedings{office_activity,
  author    = {Limin Wang and Yu Qiao and Xiaoou Tang},
  title     = {Action recognition and detection
               by combining motion and appearance features},
  booktitle = {THUMOS},
  year      = {2014}
}

@inproceedings{MSRDailyactivity3D,
  author    = {Jiang Wang and Zicheng Liu and Ying Wu and Junsong Yuan},
  title     = {{Mining Actionlet Ensemble for Action Recognition with Depth Cameras}},
  booktitle = {IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2012}
}
@inproceedings{ACT4,
  author    = {Zhongwei Cheng and Lei Qin and Yituo Ye and Qingming Huang and Qi Tian},
  title     = {{Human Daily Action Analysis with Multi-view and Color-Depth Data}},
  booktitle = {European Conference on Computer Vision(ECCV)},
  year      = {2012}
}
@article{pascal,
  author     = {Everingham, Mark and Gool, Luc and Williams, Christopher K. and Winn, John and Zisserman, Andrew},
  title      = {{The Pascal Visual Object Classes (VOC) Challenge}},
  journal    = {Int. J. Comput. Vision},
  issue_date = {June      2010},
  volume     = {88},
  number     = {2},
  month      = jun,
  year       = {2010},
  issn       = {0920-5691},
  pages      = {303--338},
  numpages   = {36},
  url        = {http://dx.doi.org/10.1007/s11263-009-0275-4},
  doi        = {10.1007/s11263-009-0275-4},
  acmid      = {1747104},
  publisher  = {Kluwer Academic Publishers},
  address    = {Hingham, MA, USA},
  keywords   = {Benchmark, Database, Object detection, Object recognition}
} 
@inproceedings{coco,
  title     = {{Microsoft COCO: Common Objects in Context}},
  author    = {Tsung-Yi Lin and Michael Maire and Serge J. Belongie and Lubomir D. Bourdev and Ross B. Girshick and James Hays and Pietro Perona and Deva Ramanan and Piotr Doll{\'a}r and C. Lawrence Zitnick},
  booktitle = {ECCV},
  year      = {2014}
}
@article{neuro_image,
  title   = {{Dissociating temporal attention from spatial attention and motor response preparation: A high-density EEG study}},
  journal = {NeuroImage},
  volume  = {124},
  pages   = {947 - 957},
  year    = {2016},
  issn    = {1053-8119},
  doi     = {https://doi.org/10.1016/j.neuroimage.2015.09.051},
  url     = {http://www.sciencedirect.com/science/article/pii/S1053811915008733},
  author  = {Frédéric Faugeras and Lionel Naccache}
}
@article{orientation_in_time,
  author  = { Nobre, A. C. and  Miniussi, C. and  Wilding, E. L. and  Coull, J. T.},
  title   = {{Orienting attention in time: Modulation of brain potentials}},
  journal = {Brain},
  volume  = {122},
  number  = {8},
  pages   = {1507-1518},
  year    = {1999},
  month   = {08},
  issn    = {0006-8950},
  doi     = {10.1093/brain/122.8.1507},
  url     = {https://dx.doi.org/10.1093/brain/122.8.1507}
}

@inproceedings{sigurdsson2017actions,
  title     = {What actions are needed for understanding human actions in videos?},
  author    = {Sigurdsson, Gunnar A and Russakovsky, Olga and Gupta, Abhinav},
  booktitle = {Proceedings of the IEEE international conference on computer vision},
  pages     = {2137--2146},
  year      = {2017}
}

@inproceedings{charades,
  author    = {Gunnar A. Sigurdsson and G{\"u}l Varol and Xiaolong Wang and Ali Farhadi and Ivan Laptev and Abhinav Gupta},
  title     = {{Hollywood in Homes: Crowdsourcing Data Collection for Activity Understanding}},
  booktitle = {European Conference on Computer Vision(ECCV)},
  year      = {2016},
  pdf       = {http://arxiv.org/pdf/1604.01753.pdf},
  web       = {https://allenai.org/plato/charades/}
}

@article{nonlocal,
  title   = {{Non-local Neural Networks}},
  author  = {Xiaolong Wang and Ross B. Girshick and Abhinav Gupta and Kaiming He},
  journal = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year    = {2018},
  pages   = {7794-7803}
}
@article{AVA,
  title   = {{AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Actions}},
  author  = {Chunhui Gu and Chen Sun and David A. Ross and Carl Vondrick and Caroline Pantofaru and Yeqing Li and Sudheendra Vijayanarasimhan and George Toderici and Susanna Ricco and Rahul Sukthankar and Cordelia Schmid and Jitendra Malik},
  journal = {Conference on Computer Vision and Pattern Recognition(CVPR)},
  year    = {2018}
}
@inproceedings{wu2020distribution,
  title        = {Distribution-balanced loss for multi-label classification in long-tailed datasets},
  author       = {Wu, Tong and Huang, Qingqiu and Liu, Ziwei and Wang, Yu and Lin, Dahua},
  booktitle    = {European Conference on Computer Vision},
  pages        = {162--178},
  year         = {2020},
  organization = {Springer}
}
@inproceedings{lin2017focal,
  title     = {{Focal loss for dense object detection}},
  author    = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Doll{\'a}r, Piotr},
  booktitle = {Proceedings of the IEEE international conference on computer vision},
  pages     = {2980--2988},
  year      = {2017}
}
@article{zhang2019study,
  title   = {{A study on action detection in the wild}},
  author  = {Zhang, Yubo and Tokmakov, Pavel and Hebert, Martial and Schmid, Cordelia},
  journal = {arXiv preprint arXiv:1904.12993},
  year    = {2019}
}
@article{xu2018similarity,
  title   = {{Similarity r-c3d for few-shot temporal activity detection}},
  author  = {Xu, Huijuan and Kang, Bingyi and Sun, Ximeng and Feng, Jiashi and Saenko, Kate and Darrell, Trevor},
  journal = {arXiv preprint arXiv:1812.10000},
  year    = {2018}
}
@article{almeida1997c1,
  title   = {C1. 2 Multilayer perceptrons},
  author  = {Almeida, Luis B},
  journal = {Handbook of Neural Computation C},
  volume  = {1},
  year    = {1997}
}
@inproceedings{pose_for_action,
  title     = {2d/3d pose estimation and action recognition using multitask deep learning},
  author    = {Luvizon, Diogo C and Picard, David and Tabia, Hedi},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages     = {5137--5146},
  year      = {2018}
}
@misc{THUMOS14,
  author       = {Jiang, Yu-Gang. and Liu, Jingen and Roshan Zamir, Amir and Toderici, George and Laptev,
                  Ivan and Shah, Mubarak and Sukthankar, Rahul},
  title        = {{THUMOS Challenge: Action Recognition with a Large Number of Classes}},
  howpublished = {\url{http://crcv.ucf.edu/THUMOS14/}},
  year         = {2014}
}

@misc{MPII_site,
  author       = {Rohrbach, Marcus and Rohrbach, Anna and Regneri, Michaela and Amin, Sikandar and Andriluka, Mykhaylo and Pinkal, Manfred and Schiele, Bernt},
  title        = {{MPII Cooking 2 website}},
  howpublished = {\url{https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/human-activity-recognition/mpii-cooking-2-dataset}},
  year         = {2014}
}   
   
   t
@article{mpii-cooking2,
  year      = {2015},
  journal   = {International Journal of Computer Vision},
  title     = {Recognizing Fine-Grained and Composite Activities Using Hand-Centric Features and Script Data},
  author    = {Rohrbach, Marcus and Rohrbach, Anna and Regneri, Michaela and Amin, Sikandar and Andriluka, Mykhaylo and Pinkal, Manfred and Schiele, Bernt},
  issn      = {0920-5691},
  doi       = {10.1007/s11263-015-0851-8},
  url       = {http://dx.doi.org/10.1007/s11263-015-0851-8},
  publisher = {Springer US},
  pages     = {1-28}
} 
@inproceedings{zhao2019hacs,
  title     = {{HACS: Human action clips and segments dataset for recognition and temporal localization}},
  author    = {Zhao, Hang and Torralba, Antonio and Torresani, Lorenzo and Yan, Zhicheng},
  booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
  pages     = {8668--8678},
  year      = {2019}
}

@article{konig2015ecological,
  title     = {{Ecological assessment of autonomy in instrumental activities of daily living in dementia patients by the means of an automatic video monitoring system}},
  author    = {K{\"o}nig, Alexandra and Crispim-Junior, Carlos Fernando and Covella, Alvaro Gomez Uria and Bremond, Francois and Derreumaux, Alexandre and Bensadoun, Gregory and David, Renaud and Verhey, Frans and Aalten, Pauline and Robert, Philippe},
  journal   = {Frontiers in aging neuroscience},
  volume    = {7},
  pages     = {98},
  year      = {2015},
  publisher = {Frontiers}
}

@misc{elan,
  howpublished = {\url{https://tla.mpi.nl/tools/tla-tools/elan/}},
  note         = {Accessed Oct. 30th, 2019},
  title        = {TLA software: Elan},
  author       = {Max Planck Institute}
}

@inproceedings{liu2018path,
  title     = {{Path aggregation network for instance segmentation}},
  author    = {Liu, Shu and Qi, Lu and Qin, Haifang and Shi, Jianping and Jia, Jiaya},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages     = {8759--8768},
  year      = {2018}
}

@article{zhou2019objects,
  title   = {{Objects as points}},
  author  = {Zhou, Xingyi and Wang, Dequan and Kr{\"a}henb{\"u}hl, Philipp},
  journal = {arXiv preprint arXiv:1904.07850},
  year    = {2019}
}

@inproceedings{dai2021attentional,
  title     = {{Attentional feature fusion}},
  author    = {Dai, Yimian and Gieseke, Fabian and Oehmcke, Stefan and Wu, Yiquan and Barnard, Kobus},
  booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages     = {3560--3569},
  year      = {2021}
}

@article{islam2020much,
  title   = {{How much position information do convolutional neural networks encode?}},
  author  = {Islam, Md Amirul and Jia, Sen and Bruce, Neil DB},
  journal = {arXiv preprint arXiv:2001.08248},
  year    = {2020}
}

@article{wu2021cvt,
  title   = {{CVT: Introducing convolutions to vision transformers}},
  author  = {Wu, Haiping and Xiao, Bin and Codella, Noel and Liu, Mengchen and Dai, Xiyang and Yuan, Lu and Zhang, Lei},
  journal = {arXiv preprint arXiv:2103.15808},
  year    = {2021}
}

@inproceedings{Ding_2018_CVPR,
  author    = {Ding, Li and Xu, Chenliang},
  title     = {{Weakly-Supervised Action Segmentation with Iterative Soft Boundary Assignment}},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2018}
}

@misc{VIRAT,
  howpublished = {\url{http://www.viratdata.org/}},
  note         = {Accessed Feb. 28th, 2019},
  title        = {VIRAT Video dataset},
  author       = {DARPA and Kitware}
}

@article{kinetics,
  title   = {{The kinetics human action video dataset}},
  author  = {Kay, Will and Carreira, Joao and Simonyan, Karen and Zhang, Brian and Hillier, Chloe and Vijayanarasimhan, Sudheendra and Viola, Fabio and Green, Tim and Back, Trevor and Natsev, Paul and others},
  journal = {arXiv preprint arXiv:1705.06950},
  year    = {2017}
}
@inproceedings{dahlia,
  title        = {{The daily home life activity dataset: a high semantic activity dataset for online recognition}},
  author       = {Vaquette, Geoffrey and Orcesi, Astrid and Lucat, Laurent and Achard, Catherine},
  booktitle    = {2017 12th IEEE International Conference on Automatic Face \& Gesture Recognition (FG 2017)},
  pages        = {497--504},
  year         = {2017},
  organization = {IEEE}
}
@inproceedings{kuehne2011hmdb,
  title        = {{HMDB: a large video database for human motion recognition}},
  author       = {Kuehne, Hildegard and Jhuang, Hueihan and Garrote, Est{\'\i}baliz and Poggio, Tomaso and Serre, Thomas},
  booktitle    = {2011 International Conference on Computer Vision},
  pages        = {2556--2563},
  year         = {2011},
  organization = {IEEE}
}

@inproceedings{svw,
  title        = {{Sports videos in the wild (SVW): A video dataset for sports analysis}},
  author       = {Safdarnejad, Seyed Morteza and Liu, Xiaoming and Udpa, Lalita and Andrus, Brooks and Wood, John and Craven, Dean},
  booktitle    = {2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)},
  volume       = {1},
  pages        = {1--7},
  year         = {2015},
  organization = {IEEE}
}

@article{kahatapitiya2021self,
  title   = {{Self-supervised Pretraining with Classification Labels for Temporal Activity Detection}},
  author  = {Kahatapitiya, Kumara and Ren, Zhou and Li, Haoxiang and Wu, Zhenyu and Ryoo, Michael S},
  journal = {arXiv preprint arXiv:2111.13675},
  year    = {2021}
}

@article{ryoo2021tokenlearner,
  title   = {{TokenLearner: Adaptive Space-Time Tokenization for Videos}},
  author  = {Ryoo, Michael and Piergiovanni, AJ and Arnab, Anurag and Dehghani, Mostafa and Angelova, Anelia},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {34},
  year    = {2021}
}

@article{kahatapitiya2021swat,
  title   = {{SWAT: Spatial Structure Within and Among Tokens}},
  author  = {Kahatapitiya, Kumara and Ryoo, Michael S},
  journal = {arXiv preprint arXiv:2111.13677},
  year    = {2021}
}

@misc{cdnet_med,
  doi       = {10.48550/ARXIV.2203.15078},
  url       = {https://arxiv.org/abs/2203.15078},
  author    = {Kapse, Saarthak and Das, Srijan and Prasanna, Prateek},
  keywords  = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {CD-Net: Histopathology Representation Learning using Pyramidal Context-Detail Network},
  publisher = {arXiv},
  year      = {2022},
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}
@inproceedings{efficient,
  author    = {Liu, Yahui and Sangineto, Enver and Bi, Wei and Sebe, Nicu and Lepri, Bruno and De Nadai, Marco},
  title     = {Efficient Training of Visual Transformers with Small Datasets},
  booktitle = {Conference on Neural Information Processing Systems (NeurIPS)},
  year      = {2021}
}
@article{vu2022handcrafted_med,
  title   = {Handcrafted Histological Transformer (H2T): Unsupervised Representation of Whole Slide Images},
  author  = {Vu, Quoc Dang and others},
  journal = {arXiv preprint arXiv:2202.07001},
  year    = {2022}
}
@inproceedings{timesformer,
  author    = {Gedas Bertasius and Heng Wang and Lorenzo Torresani},
  title     = {Is Space-Time Attention All You Need for Video Understanding?},
  booktitle = {Proceedings of the International Conference on Machine Learning (ICML)},
  month     = {July},
  year      = {2021}
}
@inproceedings{vivit,
  author    = {Arnab, Anurag and Dehghani, Mostafa and Heigold, Georg and Sun, Chen and Lu\v{c}i\'c, Mario and Schmid, Cordelia},
  title     = {ViViT: A Video Vision Transformer},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  month     = {October},
  year      = {2021},
  pages     = {6836-6846}
}
@inproceedings{tokenlearner,
  author    = {Ryoo, Michael and Piergiovanni, AJ and Arnab, Anurag and Dehghani, Mostafa and Angelova, Anelia},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
  pages     = {12786--12797},
  publisher = {Curran Associates, Inc.},
  title     = {TokenLearner: Adaptive Space-Time Tokenization for Videos},
  url       = {https://proceedings.neurips.cc/paper/2021/file/6a30e32e56fce5cf381895dfe6ca7b6f-Paper.pdf},
  volume    = {34},
  year      = {2021}
}

@inproceedings{t2t,
  title     = {Tokens-to-token vit: Training vision transformers from scratch on imagenet},
  author    = {Yuan, Li and Chen, Yunpeng and Wang, Tao and Yu, Weihao and Shi, Yujun and Jiang, Zi-Hang and Tay, Francis EH and Feng, Jiashi and Yan, Shuicheng},
  booktitle = {Proceedings of the IEEE/CVF international conference on computer vision},
  pages     = {558--567},
  year      = {2021}
}

@article{vpn++,
  author  = {Das, Srijan and Dai, Rui and Yang, Di and Bremond, Francois},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title   = {VPN++: Rethinking Video-Pose embeddings for understanding Activities of Daily Living},
  year    = {2021},
  volume  = {},
  number  = {},
  pages   = {1-1},
  doi     = {10.1109/TPAMI.2021.3127885}
}
  
@article{mayo,
  author  = {Coffey, Jordan D.
             and Christopherson, Laura A.
             and Glasgow, Amy E.
             and Pearson, Kristina K.
             and Brown, Julie K.
             and Gathje, Shelby R.
             and Sangaralingham, Lindsey R.
             and Carmona Porquera, Eva M.
             and Virk, Abinash
             and Orenstein, Robert
             and Speicher, Leigh L.
             and Bierle, Dennis M.
             and Ganesh, Ravindra
             and Cox, Debra L.
             and Blegen, R. Nicole
             and Haddad, Tufia C.},
  title   = {Implementation of a multisite, interdisciplinary remote patient monitoring program for ambulatory management of patients with COVID-19},
  journal = {npj Digital Medicine},
  year    = {2021},
  month   = {Aug},
  day     = {13},
  volume  = {4},
  number  = {1},
  pages   = {123},
  issn    = {2398-6352},
  doi     = {10.1038/s41746-021-00490-9},
  url     = {https://doi.org/10.1038/s41746-021-00490-9}
}


@inproceedings{DSMIL,
  title     = {Dual-stream multiple instance learning network for whole slide image classification with self-supervised contrastive learning},
  author    = {Li, Bin and others},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year      = {2021}
}
@article{gildenblat2019self_med,
  title   = {Self-supervised similarity learning for digital pathology},
  author  = {Gildenblat, Jacob and others},
  journal = {arXiv preprint arXiv:1905.08139},
  year    = {2019}
}
@article{selfpath_med,
  title     = {Self-path: Self-supervision for classification of pathology images with limited annotations},
  author    = {Koohbanani, Navid Alemi and others},
  journal   = {IEEE Transactions on Medical Imaging},
  year      = {2021},
  publisher = {IEEE}
}

@inproceedings{deit,
  title     = {Training data-efficient image transformers &amp; distillation through attention},
  author    = {Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and Jegou, Herve},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  pages     = {10347--10357},
  year      = {2021},
  editor    = {Meila, Marina and Zhang, Tong},
  volume    = {139},
  series    = {Proceedings of Machine Learning Research},
  month     = {18--24 Jul},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v139/touvron21a/touvron21a.pdf},
  url       = {https://proceedings.mlr.press/v139/touvron21a.html},
  abstract  = {Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. These high-performing vision transformers are pre-trained with hundreds of millions of images using a large infrastructure, thereby limiting their adoption. In this work, we produce competitive convolution-free transformers trained on ImageNet only using a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop) on ImageNet with no external data. We also introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention, typically from a convnet teacher. The learned transformers are competitive (85.2% top-1 acc.) with the state of the art on ImageNet, and similarly when transferred to other tasks. We will share our code and models.}
}


@inproceedings{swinver1,
  title     = {Swin transformer: Hierarchical vision transformer using shifted windows},
  author    = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle = CVPR,
  pages     = {10012--10022},
  year      = {2021}
}
@inproceedings{crossvit,
  title     = {Crossvit: Cross-attention Multi-scale Vision Transformer for Image Classification},
  author    = {Chen, Chun-Fu Richard and others},
  booktitle = ICCV,
  year      = {2021}
}
@inproceedings{tnt,
  title     = {Transformer in Transformer},
  author    = {Kai Han and An Xiao and Enhua Wu and Jianyuan Guo and Chunjing Xu and Yunhe Wang},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
  year      = {2021},
  url       = {https://openreview.net/forum?id=iFODavhthGZ}
}
@misc{WPA_2019,
  howpublished = {\url{https://www.un.org/en/development/desa/population/}},
  note         = {Accessed Aug. 28th, 2022},
  title        = {Department of Economic and Social Affairs
                  Population of United Nations, the World population aging report 2019}
}
@misc{Authors14,
  author = {Authors},
  title  = {The frobnicatable foo filter},
  note   = {{BMVC14} submission ID 324. Supplied as additional material {\tt bmvc14.pdf}},
  year   = 2014
}
@article{MaskedAutoencoders2021,
  author  = {Kaiming He and Xinlei Chen and Saining Xie and Yanghao Li and Piotr Doll{\'a}r and Ross Girshick},
  journal = {arXiv:2111.06377},
  title   = {Masked Autoencoders Are Scalable Vision Learners},
  year    = {2021}
}
@misc{Authors14b,
  author = {Authors},
  title  = {Frobnication tutorial},
  note   = {Supplied as additional material {\tt tr.pdf}},
  year   = 2014
}

@article{Alpher02,
  author  = {A. Alpher},
  title   = {Frobnication},
  journal = {Journal of Foo},
  volume  = 12,
  number  = 1,
  pages   = {234--778},
  year    = 2002
}

@article{Alpher03,
  author  = {A. Alpher and J.~P.~N. Fotheringham-Smythe},
  title   = {Frobnication revisited},
  journal = {Journal of Foo},
  volume  = 13,
  number  = 1,
  pages   = {234--778},
  year    = 2003
}


@inproceedings{li2021crossclr,
  title     = {3D Human Action Representation Learning via Cross-View Consistency Pursuit},
  author    = {Linguo, Li and Minsi, Wang and Bingbing, Ni and Hang, Wang and Jiancheng, Yang and Wenjun, Zhang},
  booktitle = {CVPR},
  year      = {2021}
}
@inproceedings{sebirenet,
  title     = {Unsupervised 3D Human Pose Representation with Viewpoint and Pose Disentanglement},
  author    = {Nie, Qiang and Liu, Ziwei and Liu, Yunhui},
  booktitle = {European Conference on Computer Vision (ECCV)},
  year      = {2020}
}
@inproceedings{mlb,
  author    = {Piergiovanni, AJ and Ryoo, Michael},
  title     = {Learning Multimodal Representations for Unseen Activities},
  booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
  month     = {March},
  year      = {2020}
}
@inproceedings{video_byol,
  author    = {Feichtenhofer, Christoph and Fan, Haoqi and Xiong, Bo and Girshick, Ross and He, Kaiming},
  booktitle = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {A Large-Scale Study on Unsupervised Spatiotemporal Representation Learning},
  year      = {2021},
  volume    = {},
  number    = {},
  pages     = {3298-3308},
  doi       = {10.1109/CVPR46437.2021.00331}
}

@inproceedings{ctr_gcn,
  title     = {Channel-wise Topology Refinement Graph Convolution for Skeleton-Based Action Recognition},
  author    = {Chen, Yuxin and Zhang, Ziqi and Yuan, Chunfeng and Li, Bing and Deng, Ying and Hu, Weiming},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages     = {13359--13368},
  year      = {2021}
}
@inproceedings{supportset_2021_ICCV,
  author    = {Dwibedi, Debidatta and Aytar, Yusuf and Tompson, Jonathan and Sermanet, Pierre and Zisserman, Andrew},
  title     = {With a Little Help From My Friends: Nearest-Neighbor Contrastive Learning of Visual Representations},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  month     = {October},
  year      = {2021},
  pages     = {9588-9597}
}
@inproceedings{NPL_2021_CVPR,
  author    = {Piergiovanni, AJ and Ryoo, Michael S.},
  title     = {Recognizing Actions in Videos From Unseen Viewpoints},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  year      = {2021},
  pages     = {4124-4132}
}
@article{yang2021unik,
  title   = {UNIK: A Unified Framework for Real-world Skeleton-based Action Recognition},
  author  = {Di Yang and Yaohui Wang and Antitza Dantcheva and Lorenzo Garattoni and Gianpiero Francesca and Francois Bremond},
  year    = {2021},
  journal = {arXiv preprint arXiv:2107.08580}
}
@inproceedings{skeleton_colorization_ICCV,
  author    = {Yang, Siyuan and Liu, Jun and Lu, Shijian and Er, Meng Hwa and Kot, Alex C.},
  title     = {Skeleton Cloud Colorization for Unsupervised 3D Action Representation Learning},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  month     = {October},
  year      = {2021},
  pages     = {13423-13433}
}
@inproceedings{Motion_decoder_,
  author    = {Li, Junnan and Wong, Yongkang and Zhao, Qi and Kankanhalli, Mohan S},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {Unsupervised Learning of View-invariant Action Representations},
  url       = {https://proceedings.neurips.cc/paper/2018/file/2f37d10131f2a483a8dd005b3d14b0d9-Paper.pdf},
  volume    = {31},
  year      = {2018}
}
@inproceedings{su2020predict,
  title     = {Predict \& cluster: Unsupervised skeleton based action recognition},
  author    = {Su, Kun and Liu, Xiulong and Shlizerman, Eli},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages     = {9631--9640},
  year      = {2020}
}
@article{ASCAL,
  title   = {Augmented Skeleton Based Contrastive Action Learning with Momentum LSTM for Unsupervised Action Recognition},
  journal = {Information Sciences},
  volume  = {569},
  pages   = {90-109},
  year    = {2021},
  doi     = {https://doi.org/10.1016/j.ins.2021.04.023},
  author  = {Haocong Rao and Shihao Xu and Xiping Hu and Jun Cheng and Bin Hu}
}
@inproceedings{MS2L,
  author    = {Lin, Lilang and Song, Sijie and Yang, Wenhan and Liu, Jiaying},
  title     = {MS2L: Multi-Task Self-Supervised Learning for Skeleton Based Action Recognition},
  year      = {2020},
  isbn      = {9781450379885},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3394171.3413548},
  doi       = {10.1145/3394171.3413548},
  booktitle = {Proceedings of the 28th ACM International Conference on Multimedia},
  pages     = {2490–2498},
  numpages  = {9},
  keywords  = {multi-task, action recognition, self-supervised learning},
  location  = {Seattle, WA, USA},
  series    = {MM '20}
}
@inproceedings{LongTGAN,
  title     = {Unsupervised Representation Learning with Long-Term Dynamics for Skeleton Based Action Recognition},
  author    = {Zheng, Nenggan and Wen, Jun and Liu, Risheng and and Long, Liangqu and Dai, Jianhua and Gong, Zhefeng },
  booktitle = {AAAI},
  pages     = {2644--2651},
  year      = {2018}
}
@inproceedings{jft_300m,
  author    = {Sun, Chen and Shrivastava, Abhinav and Singh, Saurabh and Gupta, Abhinav},
  booktitle = {ICCV},
  keywords  = {dblp},
  pages     = {843-852},
  publisher = {IEEE Computer Society},
  title     = {Revisiting Unreasonable Effectiveness of Data in Deep Learning Era.},
  year      = 2017
}



@inproceedings{lee2021imix,
  title     = {i-Mix: A Domain-Agnostic Strategy for Contrastive Representation Learning},
  author    = {Lee, Kibok and Zhu, Yian and Sohn, Kihyuk and Li, Chun-Liang and Shin, Jinwoo and Lee, Honglak},
  booktitle = {ICLR},
  year      = {2021}
}
@article{small_data_wacv,
  author     = {Seung Hoon Lee and
                Seunghyun Lee and
                Byung Cheol Song},
  title      = {Vision Transformer for Small-Size Datasets},
  journal    = {CoRR},
  volume     = {abs/2112.13492},
  year       = {2021},
  url        = {https://arxiv.org/abs/2112.13492},
  eprinttype = {arXiv},
  eprint     = {2112.13492},
  timestamp  = {Tue, 04 Jan 2022 15:59:27 +0100},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2112-13492.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}
@article{yun2020videomix,
  title   = {VideoMix: Rethinking Data Augmentation for Video Classification},
  author  = {Yun, Sangdoo and Oh, Seong Joon and Heo, Byeongho and Han, Dongyoon and Kim, Jinhyung},
  journal = {arXiv preprint arXiv:2012.03457},
  year    = {2020}
}
@inproceedings{yun2019cutmix,
  title     = {CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features},
  author    = {Yun, Sangdoo and Han, Dongyoon and Oh, Seong Joon and Chun, Sanghyuk and Choe, Junsuk and Yoo, Youngjoon},
  booktitle = {International Conference on Computer Vision (ICCV)},
  year      = {2019},
  pubstate  = {published},
  tppubtype = {inproceedings}
}

@inproceedings{cvt,
  title     = {Cvt: Introducing convolutions to vision transformers},
  author    = {Wu, Haiping and Xiao, Bin and Codella, Noel and Liu, Mengchen and Dai, Xiyang and Yuan, Lu and Zhang, Lei},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages     = {22--31},
  year      = {2021}
}

@inproceedings{simclr,
  title        = {A simple framework for contrastive learning of visual representations},
  author       = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  booktitle    = {International conference on machine learning},
  pages        = {1597--1607},
  year         = {2020},
  organization = {PMLR}
}

@inproceedings{moco,
  title     = {Momentum contrast for unsupervised visual representation learning},
  author    = {He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
  booktitle = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages     = {9729--9738},
  year      = {2020}
}

@article{byol,
  title   = {Bootstrap your own latent-a new approach to self-supervised learning},
  author  = {Grill, Jean-Bastien and Strub, Florian and Altch{\'e}, Florent and Tallec, Corentin and Richemond, Pierre and Buchatskaya, Elena and Doersch, Carl and Avila Pires, Bernardo and Guo, Zhaohan and Gheshlaghi Azar, Mohammad and others},
  journal = {Advances in neural information processing systems},
  volume  = {33},
  pages   = {21271--21284},
  year    = {2020}
}

@inproceedings{mae,
  title     = {Masked autoencoders are scalable vision learners},
  author    = {He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll{\'a}r, Piotr and Girshick, Ross},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages     = {16000--16009},
  year      = {2022}
}

@inproceedings{pmlr-v162-park22b,
  title     = {Blurs Behave Like Ensembles: Spatial Smoothings to Improve Accuracy, Uncertainty, and Robustness},
  author    = {Park, Namuk and Kim, Songkuk},
  booktitle = {Proceedings of the 39th International Conference on Machine Learning},
  pages     = {17390--17419},
  year      = {2022},
  editor    = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume    = {162},
  series    = {Proceedings of Machine Learning Research},
  month     = {17--23 Jul},
  publisher = {PMLR},
  pdf       = {https://proceedings.mlr.press/v162/park22b/park22b.pdf},
  url       = {https://proceedings.mlr.press/v162/park22b.html},
  abstract  = {Neural network ensembles, such as Bayesian neural networks (BNNs), have shown success in the areas of uncertainty estimation and robustness. However, a crucial challenge prohibits their use in practice. BNNs require a large number of predictions to produce reliable results, leading to a significant increase in computational cost. To alleviate this issue, we propose spatial smoothing, a method that ensembles neighboring feature map points of convolutional neural networks. By simply adding a few blur layers to the models, we empirically show that spatial smoothing improves accuracy, uncertainty estimation, and robustness of BNNs across a whole range of ensemble sizes. In particular, BNNs incorporating spatial smoothing achieve high predictive performance merely with a handful of ensembles. Moreover, this method also can be applied to canonical deterministic neural networks to improve the performances. A number of evidences suggest that the improvements can be attributed to the stabilized feature maps and the smoothing of the loss landscape. In addition, we provide a fundamental explanation for prior works {—} namely, global average pooling, pre-activation, and ReLU6 {—} by addressing them as special cases of spatial smoothing. These not only enhance accuracy, but also improve uncertainty estimation and robustness by making the loss landscape smoother in the same manner as spatial smoothing. The code is available at https://github.com/xxxnell/spatial-smoothing.}
}
@article{zhang2018mixup,
  title   = {mixup: Beyond Empirical Risk Minimization},
  author  = {Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N. and Lopez-Paz, David },
  journal = {International Conference on Learning Representations},
  year    = {2018},
  url     = {https://openreview.net/forum?id=r1Ddp1-Rb}
}

@inproceedings{verma2021dacl,
  title     = {Towards Domain-Agnostic Contrastive Learning},
  author    = {Verma, Vikas and Luong, Minh-Thang and Kawaguchi, Kenji and Pham, Hieu and Le, Quoc V},
  booktitle = {International Conference on Machine Learning ()},
  year      = {2021}
}

@article{how_vt,
  title   = {How do vision transformers work?},
  author  = {Park, Namuk and Kim, Songkuk},
  journal = {arXiv preprint arXiv:2202.06709},
  year    = {2022}
}
@inproceedings{mvit2,
  title     = {MViTv2: Improved multiscale vision transformers for classification and detection},
  author    = {Li, Yanghao and Wu, Chao-Yuan and Fan, Haoqi and Mangalam, Karttikeya and Xiong, Bo and Malik, Jitendra and Feichtenhofer, Christoph},
  booktitle = {CVPR},
  year      = {2022}
}
@inproceedings{dai2022mstct,
  title     = {{MS-TCT: Multi-Scale Temporal ConvTransformer for Action Detection}},
  author    = {Dai, Rui and Das, Srijan and Kahatapitiya, Kumara and Ryoo, Michael and Bremond, Francois},
  booktitle = {CVPR},
  year      = {2022}
}
@inproceedings{mvit1,
  title     = {Multiscale vision transformers},
  author    = {Fan, Haoqi and Xiong, Bo and Mangalam, Karttikeya and Li, Yanghao and Yan, Zhicheng and Malik, Jitendra and Feichtenhofer, Christoph},
  booktitle = {ICCV},
  year      = {2021}
}
@inproceedings{dino,
  title     = {Emerging properties in self-supervised vision transformers},
  author    = {Caron, Mathilde and Touvron, Hugo and Misra, Ishan and J{\'e}gou, Herv{\'e} and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages     = {9650--9660},
  year      = {2021}
}
@inproceedings{heo2021pit,
  title     = {Rethinking Spatial Dimensions of Vision Transformers},
  author    = {Byeongho Heo and Sangdoo Yun and Dongyoon Han and Sanghyuk Chun and Junsuk Choe and Seong Joon Oh},
  booktitle = {International Conference on Computer Vision (ICCV)},
  year      = {2021}
}
@inproceedings{local,
  title        = {Locality guidance for improving vision transformers on tiny datasets},
  author       = {Li, Kehan and Yu, Runyi and Wang, Zhennan and Yuan, Li and Song, Guoli and Chen, Jie},
  booktitle    = {Computer Vision--ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23--27, 2022, Proceedings, Part XXIV},
  pages        = {110--127},
  year         = {2022},
  organization = {Springer}
}

@inproceedings{ssl_learn,
  title     = {What Do Self-Supervised Vision Transformers Learn?},
  author    = {Park, Namuk and Kim, Wonjae and Heo, Byeongho and Kim, Taekyung and Yun, Sangdoo},
  booktitle = {International Conference on Learning Representations}
}

@inproceedings{manifold_mixup,
  title     = {Manifold Mixup: Better Representations by Interpolating Hidden States},
  author    = {Verma, Vikas and Lamb, Alex and Beckham, Christopher and Najafi, Amir and Mitliagkas, Ioannis and Lopez-Paz, David and Bengio, Yoshua},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning},
  pages     = {6438--6447},
  year      = {2019},
  editor    = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume    = {97},
  series    = {Proceedings of Machine Learning Research},
  address   = {Long Beach, California, USA},
  month     = {09--15 Jun},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v97/verma19a/verma19a.pdf},
  url       = {http://proceedings.mlr.press/v97/verma19a.html}
}
@article{cutoff,
  title   = {A Simple but Tough-to-Beat Data Augmentation Approach for Natural Language Understanding and Generation},
  author  = {Shen, Dinghan and Zheng, Mingzhi and Shen, Yelong and Qu, Yanru and Chen, Weizhu},
  journal = {arXiv preprint arXiv:2009.13818},
  year    = {2020}
}
@inproceedings{speednet,
  author    = {Benaim, Sagie and Ephrat, Ariel and Lang, Oran and Mosseri, Inbar and Freeman, William T. and Rubinstein, Michael and Irani, Michal and Dekel, Tali},
  title     = {SpeedNet: Learning the Speediness in Videos},
  booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  year      = {2020}
}
@article{vcp,
  title   = {Video cloze procedure for self-supervised spatio-temporal learning},
  author  = {Luo, Dezhao and Liu, Chang and Zhou, Yu and Yang, Dongbao and Ma, Can and Ye, Qixiang and Wang, Weiping},
  journal = {arXiv preprint arXiv:2001.00294},
  year    = {2020}
}
@inproceedings{ss1,
  author    = {Xiaolong Wang and Allan Jabri and Alexei A. Efros},
  title     = {Learning Correspondence from the Cycle-Consistency of Time},
  booktitle = {CVPR},
  year      = {2019}
}
@inproceedings{ss2,
  author    = {Xiaolong Wang and Kaiming He and Abhinav Gupta},
  title     = {Transitive Invariance for Self-supervised Visual Representation Learning},
  booktitle = {ICCV},
  year      = {2017}
}
@inproceedings{ss3,
  title     = {"Tracking Emerges by Colorizing Videos"},
  author    = {Vondrick, Carl and Shrivastava, Abhinav and Fathi, Alireza and Guadarrama, Sergio and Murphy, Kevin},
  booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
  year      = {2018}
}
@article{buchler,
  author     = {Uta B{\"{u}}chler and
                Biagio Brattoli and
                Bj{\"{o}}rn Ommer},
  title      = {Improving Spatiotemporal Self-Supervision by Deep Reinforcement Learning},
  journal    = {CoRR},
  volume     = {abs/1807.11293},
  year       = {2018},
  url        = {http://arxiv.org/abs/1807.11293},
  eprinttype = {arXiv},
  eprint     = {1807.11293},
  timestamp  = {Mon, 13 Aug 2018 16:46:31 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1807-11293.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}
@inproceeding{OPN,
  author    = {Lee, Hsin-Ying and Huang, Jia-Bin and Singh, Maneesh Kumar and Yang, Ming-Hsuan},
  title     = {Unsupervised Representation Learning by Sorting Sequence},
  booktitle = {IEEE International Conference on Computer Vision},
  year      = {2017}
}
@inproceedings{jigsaw,
  author    = {Mehdi Noroozi and Paolo Favaro},
  title     = {Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles},
  booktitle = {ECCV},
  year      = {2016}
}
@article{brave,
  author     = {Adri{\`{a}} Recasens and
                Pauline Luc and
                Jean{-}Baptiste Alayrac and
                Luyu Wang and
                Florian Strub and
                Corentin Tallec and
                Mateusz Malinowski and
                Viorica Patraucean and
                Florent Altch{\'{e}} and
                Michal Valko and
                Jean{-}Bastien Grill and
                A{\"{a}}ron van den Oord and
                Andrew Zisserman},
  title      = {Broaden Your Views for Self-Supervised Video Learning},
  journal    = {CoRR},
  volume     = {abs/2103.16559},
  year       = {2021},
  url        = {https://arxiv.org/abs/2103.16559},
  eprinttype = {arXiv},
  eprint     = {2103.16559},
  timestamp  = {Wed, 07 Apr 2021 15:31:46 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2103-16559.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{vcop,
  title     = {Self-supervised Spatiotemporal Learning via Video Clip Order Prediction},
  author    = {Xu, Dejing and Xiao, Jun and Zhao, Zhou and Shao, Jian and Xie, Di and Zhuang, Yueting},
  booktitle = {Computer Vision and Pattern Recognition (CVPR)},
  year      = {2019}
}
@article{cvrl,
  author     = {Rui Qian and
                Tianjian Meng and
                Boqing Gong and
                Ming{-}Hsuan Yang and
                Huisheng Wang and
                Serge J. Belongie and
                Yin Cui},
  title      = {Spatiotemporal Contrastive Video Representation Learning},
  journal    = {CoRR},
  volume     = {abs/2008.03800},
  year       = {2020},
  url        = {https://arxiv.org/abs/2008.03800},
  eprinttype = {arXiv},
  eprint     = {2008.03800},
  timestamp  = {Fri, 14 Aug 2020 15:14:45 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2008-03800.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}
@misc{gdt,
  title     = {Multi-modal Self-Supervision from Generalized Data Transformations},
  author    = {Mandela Patrick and Yuki M. Asano and Polina Kuznetsova and Ruth Fong and João F. Henriques and Geoffrey Zweig and Andrea Vedaldi},
  year      = {2021},
  booktitle = {International Conference on Computer Vision (ICCV)}
}
@inproceedings{pan2021videomoco,
  title     = {Videomoco: Contrastive video representation learning with temporally adversarial examples},
  author    = {Pan, Tian and Song, Yibing and Yang, Tianyu and Jiang, Wenhao and Liu, Wei},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages     = {11205--11214},
  year      = {2021}
}
@inproceedings{coclr,
  author    = {Tengda Han and Weidi Xie and Andrew Zisserman},
  title     = {Self-supervised Co-training for Video Representation Learning},
  booktitle = {Neurips},
  year      = {2020}
}
@inproceedings{alwassel_2020_xdc,
  title     = {Self-Supervised Learning by Cross-Modal Audio-Video Clustering},
  author    = {Alwassel, Humam and Mahajan, Dhruv and Korbar, Bruno and 
               Torresani, Lorenzo and Ghanem, Bernard and Tran, Du},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year      = {2020}
}
@inproceedings{dpc,
  author    = {Tengda Han and Weidi Xie and Andrew Zisserman},
  title     = {Memory-augmented Dense Predictive Coding for Video Representation Learning},
  booktitle = {European Conference on Computer Vision},
  year      = {2020}
}
@article{cbt,
  author     = {Chen Sun and
                Fabien Baradel and
                Kevin Murphy and
                Cordelia Schmid},
  title      = {Contrastive Bidirectional Transformer for Temporal Representation
                Learning},
  journal    = {CoRR},
  volume     = {abs/1906.05743},
  year       = {2019},
  url        = {http://arxiv.org/abs/1906.05743},
  eprinttype = {arXiv},
  eprint     = {1906.05743},
  timestamp  = {Wed, 21 Oct 2020 08:21:25 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1906-05743.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{grad_cam,
  author    = {Ramprasaath R. {Selvaraju} and Michael {Cogswell} and Abhishek {Das} and Ramakrishna {Vedantam} and Devi {Parikh} and Dhruv {Batra}},
  booktitle = {2017 IEEE International Conference on Computer Vision (ICCV)},
  title     = {Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization},
  year      = {2017},
  volume    = {},
  number    = {},
  pages     = {618-626},
  doi       = {10.1109/ICCV.2017.74}
}

@inproceedings{contrastive_distillation,
  title     = {Contrastive Representation Distillation},
  author    = {Yonglong Tian and Dilip Krishnan and Phillip Isola},
  booktitle = {International Conference on Learning Representations},
  year      = {2020}
}
@inproceedings{miech20endtoend,
  title     = {{E}nd-to-{E}nd {L}earning of {V}isual {R}epresentations from {U}ncurated {I}nstructional {V}ideos},
  author    = {Miech, Antoine and Alayrac, Jean-Baptiste and Smaira, Lucas and Laptev, Ivan and Sivic, Josef and Zisserman, Andrew},
  booktitle = {CVPR},
  year      = {2020}
}
@inproceedings{out_of_time,
  author    = {Chung, Joon Son and Zisserman, Andrew},
  title     = {Out of time: automated lip sync in the wild},
  booktitle = {Workshop on Multi-view Lip-reading, ACCV},
  year      = {2016}
}
@article{cooperative_learning,
  author        = {Bruno Korbar and
                   Du Tran and
                   Lorenzo Torresani},
  title         = {Co-Training of Audio and Video Representations from Self-Supervised
                   Temporal Synchronization},
  journal       = {CoRR},
  volume        = {abs/1807.00230},
  year          = {2018},
  url           = {http://arxiv.org/abs/1807.00230},
  archiveprefix = {arXiv},
  eprint        = {1807.00230},
  timestamp     = {Mon, 13 Aug 2018 16:46:29 +0200},
  biburl        = {https://dblp.org/rec/journals/corr/abs-1807-00230.bib},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}
@misc{yang2020selective,
  title         = {Selective Spatio-Temporal Aggregation Based Pose Refinement System: Towards Understanding Human Activities in Real-World Videos},
  author        = {Di Yang and Rui Dai and Yaohui Wang and Rupayan Mallick and Luca Minciullo and Gianpiero Francesca and Francois Bremond},
  year          = {2020},
  eprint        = {2011.05358},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@article{objects_that_sounds,
  author        = {Relja Arandjelovic and
                   Andrew Zisserman},
  title         = {Objects that Sound},
  journal       = {CoRR},
  volume        = {abs/1712.06651},
  year          = {2017},
  url           = {http://arxiv.org/abs/1712.06651},
  archiveprefix = {arXiv},
  eprint        = {1712.06651},
  timestamp     = {Mon, 13 Aug 2018 16:46:22 +0200},
  biburl        = {https://dblp.org/rec/journals/corr/abs-1712-06651.bib},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}
@misc{owens2018audiovisual,
  title         = {Audio-Visual Scene Analysis with Self-Supervised Multisensory Features},
  author        = {Andrew Owens and Alexei A. Efros},
  year          = {2018},
  eprint        = {1804.03641},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@article{look_listen,
  author        = {Relja Arandjelovic and
                   Andrew Zisserman},
  title         = {Look, Listen and Learn},
  journal       = {CoRR},
  volume        = {abs/1705.08168},
  year          = {2017},
  url           = {http://arxiv.org/abs/1705.08168},
  archiveprefix = {arXiv},
  eprint        = {1705.08168},
  timestamp     = {Mon, 13 Aug 2018 16:48:46 +0200},
  biburl        = {https://dblp.org/rec/journals/corr/ArandjelovicZ17.bib},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}
@article{paying_more_attention,
  author        = {Sergey Zagoruyko and
                   Nikos Komodakis},
  title         = {Paying More Attention to Attention: Improving the Performance of Convolutional
                   Neural Networks via Attention Transfer},
  journal       = {CoRR},
  volume        = {abs/1612.03928},
  year          = {2016},
  url           = {http://arxiv.org/abs/1612.03928},
  archiveprefix = {arXiv},
  eprint        = {1612.03928},
  timestamp     = {Mon, 13 Aug 2018 16:47:29 +0200},
  biburl        = {https://dblp.org/rec/journals/corr/ZagoruykoK16a.bib},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{garcia_cross_modal_distillation,
  author    = {Garcia, Nuno C.
               and Morerio, Pietro
               and Murino, Vittorio},
  editor    = {Ferrari, Vittorio
               and Hebert, Martial
               and Sminchisescu, Cristian
               and Weiss, Yair},
  title     = {Modality Distillation with Multiple Stream Networks for Action Recognition},
  booktitle = {Computer Vision -- ECCV 2018},
  year      = {2018},
  publisher = {Springer International Publishing},
  address   = {Cham},
  pages     = {106--121},
  abstract  = {Diverse input data modalities can provide complementary cues for several tasks, usually leading to more robust algorithms and better performance. However, while a (training) dataset could be accurately designed to include a variety of sensory inputs, it is often the case that not all modalities are available in real life (testing) scenarios, where a model has to be deployed. This raises the challenge of how to learn robust representations leveraging multimodal data in the training stage, while considering limitations at test time, such as noisy or missing modalities. This paper presents a new approach for multimodal video action recognition, developed within the unified frameworks of distillation and privileged information, named generalized distillation. Particularly, we consider the case of learning representations from depth and RGB videos, while relying on RGB data only at test time. We propose a new approach to train an hallucination network that learns to distill depth features through multiplicative connections of spatiotemporal representations, leveraging soft labels and hard labels, as well as distance between feature maps. We report state-of-the-art results on video action classification on the largest multimodal dataset available for this task, the NTU RGB+D, as well as on the UWA3DII and Northwestern-UCLA.},
  isbn      = {978-3-030-01237-3}
}
@inproceedings{hoffman_adaptation,
  author    = {Judy {Hoffman} and Saurabh {Gupta} and Jian {Leong} and Sergio {Guadarrama} and Trevor {Darrell}},
  booktitle = {2016 IEEE International Conference on Robotics and Automation (ICRA)},
  title     = {Cross-modal adaptation for RGB-D detection},
  year      = {2016},
  volume    = {},
  number    = {},
  pages     = {5032-5039}
}
@inproceedings{hoffman,
  author    = {Judy {Hoffman} and Saurabh {Gupta} and Trevor {Darrell}},
  booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Learning with Side Information through Modality Hallucination},
  year      = {2016},
  volume    = {},
  number    = {},
  pages     = {826-834}
}
  
@inproceedings{soundnet,
  author    = {Aytar, Yusuf and Vondrick, Carl and Torralba, Antonio},
  title     = {SoundNet: Learning Sound Representations from Unlabeled Video},
  year      = {2016},
  isbn      = {9781510838819},
  publisher = {Curran Associates Inc.},
  address   = {Red Hook, NY, USA},
  abstract  = {We learn rich natural sound representations by capitalizing on large amounts of unlabeled sound data collected in the wild. We leverage the natural synchronization between vision and sound to learn an acoustic representation using two-million unlabeled videos. Unlabeled video has the advantage that it can be economically acquired at massive scales, yet contains useful signals about natural sound. We propose a student-teacher training procedure which transfers discriminative visual knowledge from well established visual recognition models into the sound modality using unlabeled video as a bridge. Our sound representation yields significant performance improvements over the state-of-the-art results on standard benchmarks for acoustic scene/object classification. Visualizations suggest some high-level semantics automatically emerge in the sound network, even though it is trained without ground truth labels.},
  booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
  pages     = {892–900},
  numpages  = {9},
  location  = {Barcelona, Spain},
  series    = {NIPS'16}
}
@inproceedings{dml,
  author    = {Y. {Zhang} and T. {Xiang} and T. M. {Hospedales} and H. {Lu}},
  booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  title     = {Deep Mutual Learning},
  year      = {2018},
  volume    = {},
  number    = {},
  pages     = {4320-4328}
}
@article{TV_L1,
  title   = {{TV-L1 Optical Flow Estimation}},
  author  = {Sánchez Pérez, Javier and Meinhardt-Llopis, Enric and Facciolo, Gabriele},
  journal = {{Image Processing On Line}},
  volume  = {3},
  pages   = {137--150},
  year    = {2013},
  doi     = {10.5201/ipol.2013.26}
}
@misc{dml_1,
  title         = {Knowledge Transfer via Dense Cross-Layer Mutual-Distillation},
  author        = {Anbang Yao and Dawei Sun},
  year          = {2020},
  eprint        = {2008.07816},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}

@inproceedings{wsgn,
  author    = {Fernando, Basura and Tan, Cheston and Bilen, Hakan},
  title     = {Weakly Supervised Gaussian Networks for Action Detection},
  booktitle = {The IEEE Winter Conference on Applications of Computer Vision (WACV)},
  month     = {March},
  year      = {2020}
}

@inproceedings{zhang2019weaklearning,
  title     = {Learning transferable self-attentive representations for action recognition in untrimmed videos with weak supervision},
  author    = {Zhang, Xiao-Yu and Shi, Haichao and Li, Changsheng and Zheng, Kai and Zhu, Xiaobin and Duan, Lixin},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume    = {33},
  pages     = {9227--9234},
  year      = {2019}
}

@inproceedings{mstcn,
  title     = {Ms-tcn: Multi-stage temporal convolutional network for action segmentation},
  author    = {Farha, Yazan Abu and Gall, Jurgen},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages     = {3575--3584},
  year      = {2019}
}
@article{adversial_loss,
  author        = {Hao{-}Wen Dong and
                   Yi{-}Hsuan Yang},
  title         = {Towards a Deeper Understanding of Adversarial Losses},
  journal       = {CoRR},
  volume        = {abs/1901.08753},
  year          = {2019},
  url           = {http://arxiv.org/abs/1901.08753},
  archiveprefix = {arXiv},
  eprint        = {1901.08753},
  timestamp     = {Sat, 02 Feb 2019 16:56:00 +0100},
  biburl        = {https://dblp.org/rec/journals/corr/abs-1901-08753.bib},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{lfb,
  author    = {Wu, Chao-Yuan and Feichtenhofer, Christoph and Fan, Haoqi and He, Kaiming and Krahenbuhl, Philipp and Girshick, Ross},
  title     = {Long-Term Feature Banks for Detailed Video Understanding},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  year      = {2019}
}

@inproceedings{CSN,
  author    = {Tran, Du and Wang, Heng and Torresani, Lorenzo and Feiszli, Matt},
  title     = {Video Classification With Channel-Separated Convolutional Networks},
  booktitle = {The IEEE International Conference on Computer Vision (ICCV)},
  month     = {October},
  year      = {2019}
}
@inproceedings{nas_multi,
  author    = {Perez-Rua, Juan-Manuel and Vielzeuf, Valentin and Pateux, Stephane and Baccouche, Moez and Jurie, Frederic},
  title     = {MFAS: Multimodal Fusion Architecture Search},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  year      = {2019}
}

@article{msaagcn,
  title     = {Skeleton-based action recognition with multi-stream adaptive graph convolutional networks},
  author    = {Shi, Lei and Zhang, Yifan and Cheng, Jian and Lu, Hanqing},
  journal   = {IEEE Transactions on Image Processing},
  volume    = {29},
  pages     = {9532--9545},
  year      = {2020},
  publisher = {IEEE}
}


@inproceedings{msg3d,
  title     = {Disentangling and Unifying Graph Convolutions for Skeleton-Based Action Recognition},
  author    = {Liu, Ziyu and Zhang, Hongwen and Chen, Zhenghao and Wang, Zhiyong and Ouyang, Wanli},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages     = {143--152},
  year      = {2020}
}
@article{Dai_2022_PAMI,
  author  = {Dai, Rui and Das, Srijan and Sharma, Saurav and Minciullo, Luca and Garattoni, Lorenzo and Bremond, Francois and Francesca, Gianpiero},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title   = {Toyota Smarthome Untrimmed: Real-World Untrimmed Videos for Activity Detection},
  year    = {2022},
  volume  = {},
  number  = {},
  pages   = {1-1},
  doi     = {10.1109/TPAMI.2022.3169976}
}
@inproceedings{viewclr,
  author    = {Srijan Das and
               Michael S. Ryoo},
  title     = {ViewCLR: Learning Self-supervised Video Representation for Unseen
               Viewpoints},
  booktitle = {{IEEE/CVF} Winter Conference on Applications of Computer Vision, {WACV}
               2023, Waikoloa, HI, USA, January 2-7, 2023},
  pages     = {5562--5572},
  publisher = {{IEEE}},
  year      = {2023},
  url       = {https://doi.org/10.1109/WACV56688.2023.00553},
  doi       = {10.1109/WACV56688.2023.00553},
  timestamp = {Sun, 12 Nov 2023 02:15:02 +0100},
  biburl    = {https://dblp.org/rec/conf/wacv/DasR23.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{stcmix,
  author     = {Srijan Das and
                Michael S. Ryoo},
  title      = {STC-mix: Space, Time, Channel mixing for Self-supervised Video Representation},
  journal    = {CoRR},
  volume     = {abs/2112.03906},
  year       = {2021},
  url        = {https://arxiv.org/abs/2112.03906},
  eprinttype = {arXiv},
  eprint     = {2112.03906},
  timestamp  = {Mon, 13 Dec 2021 17:51:48 +0100},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2112-03906.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}
 @inproceedings{zhang2020semantics,
  title     = {Semantics-Guided Neural Networks for Efficient Skeleton-Based Human Action Recognition},
  author    = {Zhang, Pengfei and Lan, Cuiling and Zeng, Wenjun and Xing, Junliang and Xue, Jianru and Zheng, Nanning},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  year      = {2020}
}
@inproceedings{wacv_temporal,
  author    = {Srijan {Das} and Monique {Thonnat} and Francois {Bremond}},
  booktitle = {2020 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  title     = {Looking deeper into Time for Activities of Daily Living Recognition},
  year      = {2020},
  volume    = {},
  number    = {},
  pages     = {487-496}
}
@inproceedings{weak_supervised_ICPR,
  author    = {Snehashis Majhi and Srijan Das and Francois Bremond and Ratnakar Dash and Pankaj Kumar Sa},
  title     = {Weakly-Supervised Joint Anomaly Detection and Classification},
  booktitle = {Anonymous conference (submitted)},
  month     = {April},
  year      = {2020}
}
@inproceedings{ADF_ECCV,
  author    = {Rui Dai and Srijan Das and Saurav Sharma and Luca Minciullo and  Lorenzo Garattoni and Francois Bremond and  Gianpiero Francesca},
  title     = {Activities De Facto: Real-World Untrimmed Videos for Activity Detection},
  booktitle = {Anonymous conference (submitted)},
  month     = {March},
  year      = {2020}
}
@inproceedings{Ryoo2020AssembleNet:,
  title     = {AssembleNet: Searching for Multi-Stream Neural Connectivity in Video Architectures},
  author    = {Michael S. Ryoo and AJ Piergiovanni and Mingxing Tan and Anelia Angelova},
  booktitle = {International Conference on Learning Representations},
  year      = {2020},
  url       = {https://openreview.net/forum?id=SJgMK64Ywr}
}
@inproceedings{howto100m,
  title     = {How{T}o100{M}: {L}earning a {T}ext-{V}ideo {E}mbedding by {W}atching {H}undred {M}illion {N}arrated {V}ideo {C}lips},
  author    = {Miech, Antoine and Zhukov, Dimitri and Alayrac, Jean-Baptiste and Tapaswi, Makarand and Laptev, Ivan and Sivic, Josef},
  booktitle = {ICCV},
  year      = {2019}
}
@inproceedings{evolving_losses,
  author    = {Piergiovanni, AJ and Angelova, Anelia and Ryoo, Michael S.},
  title     = {Evolving Losses for Unsupervised Video Representation Learning},
  booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  year      = {2020}
}
@inproceedings{videopose3d,
  title     = {3D human pose estimation in video with temporal convolutions and semi-supervised training},
  author    = {Pavllo, Dario and Feichtenhofer, Christoph and Grangier, David and Auli, Michael},
  booktitle = {Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2019}
}
@phdthesis{michal_thesis,
  title       = {{Human action recognition in videos with local representation}},
  author      = {Koperski, Michal},
  url         = {https://hal.inria.fr/tel-01648968},
  number      = {2017AZUR4096},
  school      = {{Universit{\'e} C{\^o}te d'Azur}},
  year        = {2017},
  month       = Nov,
  keywords    = {Computer vision ; Action recognition ; Machine learning ; Vision par ordinateur ; Reconnaissance d'actions},
  type        = {Theses},
  pdf         = {https://hal.inria.fr/tel-01648968/file/2017AZUR4096.pdf},
  hal_id      = {tel-01648968},
  hal_version = {v2}
}
@article{stlstm_extended,
  author  = {J. {Liu} and A. {Shahroudy} and D. {Xu} and A. C. {Kot} and G. {Wang}},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title   = {Skeleton-Based Action Recognition Using Spatio-Temporal LSTM Network with Trust Gates},
  year    = {2018},
  volume  = {40},
  number  = {12},
  pages   = {3007-3021}
}
@incollection{base_cnn,
  title     = {ImageNet Classification with Deep Convolutional Neural Networks},
  author    = {Alex Krizhevsky and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle = {Advances in Neural Information Processing Systems 25},
  editor    = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
  pages     = {1097--1105},
  year      = {2012},
  publisher = {Curran Associates, Inc.},
  url       = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf}
}

@misc{WPA_2015,
  howpublished = {\url{https://www.un.org/en/development/desa/population/}},
  note         = {Accessed Feb. 28th, 2020},
  title        = {Department of Economic and Social Affairs
                  Population of United Nations, the World population aging report}
}
@inproceedings{image_transformer,
  title     = {Image Transformer},
  author    = {Niki Parmar and Ashish Vaswani and Jakob Uszkoreit and Lukasz Kaiser and Noam Shazeer and Alexander Ku and Dustin Tran},
  booktitle = {},
  year      = {2018}
}
@article{stn,
  title   = {Spatial Transformer Networks},
  author  = {Max Jaderberg and Karen Simonyan and Andrew Zisserman and Koray Kavukcuoglu},
  journal = {ArXiv},
  year    = {2015},
  volume  = {abs/1506.02025}
}
@inproceedings{object_attention,
  author    = {F. {Wang} and M. {Jiang} and C. {Qian} and S. {Yang} and C. {Li} and H. {Zhang} and X. {Wang} and X. {Tang}},
  booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Residual Attention Network for Image Classification},
  year      = {2017},
  volume    = {},
  number    = {},
  pages     = {6450-6458},
  keywords  = {feature extraction;feedforward neural nets;image classification;learning (artificial intelligence);object recognition;attention residual;convolutional neural network;attention-aware features;deep residual attention networks;feed forward network architecture;image classification;CIFAR-10 datasets;CIFAR-100 datasets;ImageNet;object recognition performance;Neural networks;Training;Stacking;Logic gates;Noise measurement;Image color analysis},
  doi       = {10.1109/CVPR.2017.683},
  issn      = {1063-6919},
  month     = {July}
}

@article{smalldata,
  title   = {How to Train Vision Transformer on Small-scale Datasets?},
  author  = {Gani, Hanan and Naseer, Muzammal and Yaqub, Mohammad},
  journal = {arXiv preprint arXiv:2210.07240},
  year    = {2022}
}

@inproceedings{flowers,
  title        = {Automated flower classification over a large number of classes},
  author       = {Nilsback, Maria-Elena and Zisserman, Andrew},
  booktitle    = {2008 Sixth Indian Conference on Computer Vision, Graphics \& Image Processing},
  pages        = {722--729},
  year         = {2008},
  organization = {IEEE}
}

@article{chaoyang,
  title     = {Hard sample aware noise robust learning for histopathology image classification},
  author    = {Zhu, Chuang and Chen, Wenkai and Peng, Ting and Wang, Ying and Jin, Mulan},
  journal   = {IEEE Transactions on Medical Imaging},
  volume    = {41},
  number    = {4},
  pages     = {881--894},
  year      = {2021},
  publisher = {IEEE}
}

@article{medmnist,
  title     = {MedMNIST v2-A large-scale lightweight benchmark for 2D and 3D biomedical image classification},
  author    = {Yang, Jiancheng and Shi, Rui and Wei, Donglai and Liu, Zequan and Zhao, Lin and Ke, Bilian and Pfister, Hanspeter and Ni, Bingbing},
  journal   = {Scientific Data},
  volume    = {10},
  number    = {1},
  pages     = {41},
  year      = {2023},
  publisher = {Nature Publishing Group UK London}
}

@inproceedings{domainnet,
  title     = {Moment matching for multi-source domain adaptation},
  author    = {Peng, Xingchao and Bai, Qinxun and Xia, Xide and Huang, Zijun and Saenko, Kate and Wang, Bo},
  booktitle = {Proceedings of the IEEE/CVF international conference on computer vision},
  pages     = {1406--1415},
  year      = {2019}
}

@article{convmae,
  title   = {Convmae: Masked convolution meets masked autoencoders},
  author  = {Gao, Peng and Ma, Teli and Li, Hongsheng and Dai, Jifeng and Qiao, Yu},
  journal = {arXiv preprint arXiv:2205.03892},
  year    = {2022}
}

@inproceedings{simmim,
  title     = {Simmim: A simple framework for masked image modeling},
  author    = {Xie, Zhenda and Zhang, Zheng and Cao, Yue and Lin, Yutong and Bao, Jianmin and Yao, Zhuliang and Dai, Qi and Hu, Han},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages     = {9653--9663},
  year      = {2022}
}

@article{kipf2016semi,
  title   = {Semi-Supervised Classification with Graph Convolutional Networks},
  author  = {Kipf, Thomas N and Welling, Max},
  journal = {arXiv preprint arXiv:1609.02907},
  year    = {2016}
}
@article{howtolstm,
  title   = {How to Construct Deep Recurrent Neural Networks},
  author  = {Razvan Pascanu and Çaglar G{\"u}lçehre and Kyunghyun Cho and Yoshua Bengio},
  journal = {CoRR},
  year    = {2013},
  volume  = {abs/1312.6026}
}
@inproceedings{co_occurence,
  author    = {Zhu, Wentao and Lan, Cuiling and Xing, Junliang and Zeng, Wenjun and Li, Yanghao and Shen, Li and Xie, Xiaohui},
  title     = {Co-Occurrence Feature Learning for Skeleton Based Action Recognition Using Regularized Deep LSTM Networks},
  year      = {2016},
  publisher = {AAAI Press},
  booktitle = {Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence},
  pages     = {3697–3703},
  numpages  = {7},
  location  = {Phoenix, Arizona},
  series    = {AAAI’16}
}
@inproceedings{Dos,
  author    = {Veeriah, Vivek and Zhuang, Naifan and Qi, Guo-Jun},
  title     = {Differential Recurrent Neural Networks for Action Recognition},
  year      = {2015},
  isbn      = {9781467383912},
  publisher = {IEEE Computer Society},
  address   = {USA},
  url       = {https://doi.org/10.1109/ICCV.2015.460},
  doi       = {10.1109/ICCV.2015.460},
  booktitle = {Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV)},
  pages     = {4041–4049},
  numpages  = {9},
  series    = {ICCV ’15}
}
@inproceedings{HRNN,
  author    = { {Yong Du} and W. {Wang} and L. {Wang}},
  booktitle = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Hierarchical recurrent neural network for skeleton based action recognition},
  year      = {2015},
  volume    = {},
  number    = {},
  pages     = {1110-1118},
  keywords  = {feature extraction;image motion analysis;image representation;image sequences;perceptrons;recurrent neural nets;hierarchical recurrent neural network;end-to-end hierarchical RNN;skeleton based action recognition;subnet representation extraction;single-layer perceptron;deep RNN architectures;temporal sequences;Joints;Hidden Markov models;Recurrent neural networks;Computer architecture;Artificial neural networks;Neurons},
  doi       = {10.1109/CVPR.2015.7298714},
  issn      = {1063-6919},
  month     = {June}
}
@inproceedings{video_as_space_time,
  author    = {Xiaolong Wang and Abhinav Gupta},
  title     = {Videos as Space-Time Region Graphs},
  booktitle = {ECCV},
  year      = {2018}
}
@inproceedings{network_in_network,
  author    = {Min Lin and
               Qiang Chen and
               Shuicheng Yan},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Network In Network},
  booktitle = {2nd International Conference on Learning Representations, {ICLR} 2014,
               Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings},
  year      = {2014},
  url       = {http://arxiv.org/abs/1312.4400},
  timestamp = {Thu, 25 Jul 2019 14:35:26 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/LinCY13.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{cnn_lstm_6,
  author        = {Yunlong Bian and
                   Chuang Gan and
                   Xiao Liu and
                   Fu Li and
                   Xiang Long and
                   Yandong Li and
                   Heng Qi and
                   Jie Zhou and
                   Shilei Wen and
                   Yuanqing Lin},
  title         = {Revisiting the Effectiveness of Off-the-shelf Temporal Modeling Approaches
                   for Large-scale Video Classification},
  journal       = {CoRR},
  volume        = {abs/1708.03805},
  year          = {2017},
  url           = {http://arxiv.org/abs/1708.03805},
  archiveprefix = {arXiv},
  eprint        = {1708.03805},
  timestamp     = {Mon, 14 Oct 2019 17:53:36 +0200},
  biburl        = {https://dblp.org/rec/journals/corr/abs-1708-03805.bib},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{cnn_lstm_5,
  author    = {C. {Gan} and T. {Yao} and K. {Yang} and Y. {Yang} and T. {Mei}},
  booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {You Lead, We Exceed: Labor-Free Video Concept Learning by Jointly Exploiting Web Videos and Images},
  year      = {2016},
  volume    = {},
  number    = {},
  pages     = {923-932},
  keywords  = {image denoising;Internet;learning (artificial intelligence);neural nets;video retrieval;video signal processing;labor-free video concept learning;Web videos;Web images;noise-free training labels;data collection training;image retrieval;video retrieval;lead-exceed neural network;LENN;long short-term memory;LSTM;Conferences;Computer vision;Pattern recognition},
  doi       = {10.1109/CVPR.2016.106},
  issn      = {1063-6919},
  month     = {June}
}
@article{cnn_lstm_4,
  author        = {Zuxuan Wu and
                   Xi Wang and
                   Yu{-}Gang Jiang and
                   Hao Ye and
                   Xiangyang Xue},
  title         = {Modeling Spatial-Temporal Clues in a Hybrid Deep Learning Framework
                   for Video Classification},
  journal       = {CoRR},
  volume        = {abs/1504.01561},
  year          = {2015},
  url           = {http://arxiv.org/abs/1504.01561},
  archiveprefix = {arXiv},
  eprint        = {1504.01561},
  timestamp     = {Wed, 26 Jun 2019 08:42:53 +0200},
  biburl        = {https://dblp.org/rec/journals/corr/WuWJYX15.bib},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{cnn_lstm_3,
  author    = {Sun, Chen and Shetty, Sanketh and Sukthankar, Rahul and Nevatia, Ram},
  title     = {Temporal Localization of Fine-Grained Actions in Videos by Domain Transfer from Web Images},
  year      = {2015},
  isbn      = {9781450334594},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2733373.2806226},
  doi       = {10.1145/2733373.2806226},
  booktitle = {Proceedings of the 23rd ACM International Conference on Multimedia},
  pages     = {371–380},
  numpages  = {10},
  keywords  = {fine-grained action localization, domain transfer, lstm},
  location  = {Brisbane, Australia},
  series    = {MM ’15}
}
@article{cnn_lstm_1,
  author        = {Fu Li and
                   Chuang Gan and
                   Xiao Liu and
                   Yunlong Bian and
                   Xiang Long and
                   Yandong Li and
                   Zhichao Li and
                   Jie Zhou and
                   Shilei Wen},
  title         = {Temporal Modeling Approaches for Large-scale Youtube-8M Video Understanding},
  journal       = {CoRR},
  volume        = {abs/1707.04555},
  year          = {2017},
  url           = {http://arxiv.org/abs/1707.04555},
  archiveprefix = {arXiv},
  eprint        = {1707.04555},
  timestamp     = {Mon, 14 Oct 2019 17:53:36 +0200},
  biburl        = {https://dblp.org/rec/journals/corr/LiGLBLLLZW17.bib},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{cnn_lstm_2,
  author    = {Srivastava, Nitish and Mansimov, Elman and Salakhutdinov, Ruslan},
  title     = {Unsupervised Learning of Video Representations Using LSTMs},
  year      = {2015},
  publisher = {JMLR.org},
  booktitle = {Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37},
  pages     = {843–852},
  numpages  = {10},
  location  = {Lille, France},
  series    = {’15}
}

@inproceedings{KLT_tracker,
  author    = {Lucas, Bruce D. and Kanade, Takeo},
  title     = {An Iterative Image Registration Technique with an Application to Stereo Vision},
  year      = {1981},
  publisher = {Morgan Kaufmann Publishers Inc.},
  address   = {San Francisco, CA, USA},
  booktitle = {Proceedings of the 7th International Joint Conference on Artificial Intelligence - Volume 2},
  pages     = {674–679},
  numpages  = {6},
  location  = {Vancouver, BC, Canada},
  series    = {IJCAI’81}
}


@inproceedings{sun,
  author    = { {Ju Sun} and {Xiao Wu} and {Shuicheng Yan} and L. {Cheong} and T. {Chua} and {Jintao Li}},
  booktitle = {2009 IEEE Conference on Computer Vision and Pattern Recognition},
  title     = {Hierarchical spatio-temporal context modeling for action recognition},
  year      = {2009},
  volume    = {},
  number    = {},
  pages     = {2004-2011},
  doi       = {10.1109/CVPR.2009.5206721},
  issn      = {1063-6919},
  month     = {June}
}
@inproceedings{messing,
  author    = {R. {Messing} and C. {Pal} and H. {Kautz}},
  booktitle = {2009 IEEE 12th International Conference on Computer Vision},
  title     = {Activity recognition using the velocity histories of tracked keypoints},
  year      = {2009},
  volume    = {},
  number    = {},
  pages     = {104-111},
  keywords  = {image recognition;image sequences;human psychophysical performance;tracked keypoint velocity history;generative mixture model;local spatio-temporal features;KTH activity recognition dataset;high resolution video sequences;latent velocity model;History;Humans;Video sequences;Computer vision;Patient monitoring;Tracking;Cognition;Face recognition;Detectors;Computerized monitoring},
  doi       = {10.1109/ICCV.2009.5459154},
  issn      = {1550-5499},
  month     = {Sep.}
}
@inproceedings{trajectons,
  author    = {P. {Matikainen} and M. {Hebert} and R. {Sukthankar}},
  booktitle = {2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops},
  title     = {Trajectons: Action recognition through the motion analysis of tracked features},
  year      = {2009},
  volume    = {},
  number    = {},
  pages     = {514-521},
  keywords  = {image segmentation;image sequences;motion estimation;video retrieval;trajectons;action recognition;tracked features motion analysis;video defining feature;background-foreground segmentation;optical flow;hollywood actions dataset;video search task;YouTube videos;Motion analysis;Tracking;Optical noise;Image motion analysis;Image recognition;Image segmentation;Optical computing;Background noise;Trajectory;Histograms},
  doi       = {10.1109/ICCVW.2009.5457659},
  issn      = {null},
  month     = {Sep.}
}
@inproceedings{rapantzikos,
  author    = {K. {Rapantzikos} and Y. {Avrithis} and S. {Kollias}},
  booktitle = {2009 IEEE Conference on Computer Vision and Pattern Recognition},
  title     = {Dense saliency-based spatiotemporal feature points for action recognition},
  year      = {2009},
  volume    = {},
  number    = {},
  pages     = {1454-1461},
  keywords  = {image colour analysis;image motion analysis;image recognition;image representation;video signal processing;dense saliency;action recognition;spatiotemporal feature point detectors;video analysis;color;motion;multiscale volumetric representation;voxel level;saliency computation;global minimization process;volumetric constraints;spatial proximity;scale similarity;feature similarity;Hollywood human actions dataset;Spatiotemporal phenomena;Detectors;Computer vision;Motion detection;Entropy;Testing;Motion measurement;Density measurement;Color;Statistics},
  doi       = {10.1109/CVPR.2009.5206525},
  issn      = {1063-6919},
  month     = {June}
}
@inproceedings{bregonzio,
  author    = {M. {Bregonzio} and {Shaogang Gong} and {Tao Xiang}},
  booktitle = {2009 IEEE Conference on Computer Vision and Pattern Recognition},
  title     = {Recognising action as clouds of space-time interest points},
  year      = {2009},
  volume    = {},
  number    = {},
  pages     = {1948-1955},
  keywords  = {feature extraction;image motion analysis;image recognition;image representation;pattern clustering;spatiotemporal phenomena;action recognition;optimal space-time interest point extraction;bag-of-words representation;local space-time descriptor;spatio-temporal distribution;feature extraction;automatic feature selection;clustering algorithm;codebook construction;smooth motion;Clouds;Data mining;Shape;Clustering algorithms;Robustness;Computational efficiency;Cameras;Noise shaping;Power engineering and energy;Computer science},
  doi       = {10.1109/CVPR.2009.5206779},
  issn      = {1063-6919},
  month     = {June}
}
@inproceedings{dollar,
  author    = {Dollar, P. and Rabaud, V. and Cottrell, G. and Belongie, S.},
  title     = {Behavior Recognition via Sparse Spatio-Temporal Features},
  year      = {2005},
  isbn      = {0780394240},
  publisher = {IEEE Computer Society},
  address   = {USA},
  booktitle = {Proceedings of the 14th International Conference on Computer Communications and Networks},
  pages     = {65–72},
  numpages  = {8},
  series    = {ICCCN ’05}
}

@inproceedings{Harris88acombined,
  author    = {Chris Harris and Mike Stephens},
  title     = {A combined corner and edge detector},
  booktitle = {In Proc. of Fourth Alvey Vision Conference},
  year      = {1988},
  pages     = {147--151}
}
@inproceedings{action_mach,
  author    = {M. D. {Rodriguez} and J. {Ahmed} and M. {Shah}},
  booktitle = {2008 IEEE Conference on Computer Vision and Pattern Recognition},
  title     = {Action MACH a spatio-temporal Maximum Average Correlation Height filter for action recognition},
  year      = {2008},
  volume    = {},
  number    = {},
  pages     = {1-8},
  keywords  = {Fourier transforms;frequency-domain analysis;image recognition;action MACH;spatio-temporal maximum average correlation height filter;action recognition;template-based method;intra-class variability;3D spatiotemporal volume;frequency domain;Clifford Fourier transform;annotated human action datasets;Humans;Image motion analysis;Data analysis;Fourier transforms;Optical films;Optical filters;Computer vision;Spatiotemporal phenomena;Frequency domain analysis;Computational efficiency},
  doi       = {10.1109/CVPR.2008.4587727},
  issn      = {1063-6919},
  month     = {June}
}
@inproceedings{ke_1,
  author    = {Y. {Ke} and R. {Sukthankar} and M. {Hebert}},
  booktitle = {2007 IEEE Conference on Computer Vision and Pattern Recognition},
  title     = {Spatio-temporal Shape and Flow Correlation for Action Recognition},
  year      = {2007},
  volume    = {},
  number    = {},
  pages     = {1-8},
  keywords  = {correlation methods;feature extraction;gesture recognition;image classification;image segmentation;spatiotemporal phenomena;video signal processing;action recognition;spatiotemporal shape;flow-based correlation technique;video classification;video segmentation;object segmentation;feature extraction;tennis video;Shape;Cameras;Robustness;Image motion analysis;Image recognition;Humans;Computer science;Object segmentation;Object recognition;Image analysis},
  doi       = {10.1109/CVPR.2007.383512},
  issn      = {1063-6919},
  month     = {June}
}
@inproceedings{SelfSim_ShechtmanIrani07,
  author    = {Eli Shechtman and Michal Irani},
  title     = {Matching Local Self-Similarities across Images and Videos},
  booktitle = {IEEE Conference on Computer Vision and Pattern Recognition 2007 (CVPR'07)},
  month     = {June},
  year      = {2007},
  ee        = {www.wisdom.weizmann.ac.il/~vision/SelfSimilarities.html}
}
@article{silhou,
  author   = {A. F. {Bobick} and J. W. {Davis}},
  journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title    = {The recognition of human movement using temporal templates},
  year     = {2001},
  volume   = {23},
  number   = {3},
  pages    = {257-267},
  keywords = {computer vision;image sequences;image motion analysis;image reconstruction;human movement recognition;temporal templates;view-based approach;human movement representation;motion properties;aerobics exercises;recognition method;temporal segmentation;Image sequences;Image recognition;Computer Society;Computer vision;Cameras;Labeling;Image reconstruction;Humans;Pattern recognition;Pixel},
  doi      = {10.1109/34.910878},
  issn     = {1939-3539},
  month    = {March}
}
@inproceedings{bi_KL_3,
  author    = {Y. {Liu} and Y. {Guo} and E. M. {Bakker} and M. S. {Lew}},
  booktitle = {2017 IEEE International Conference on Computer Vision (ICCV)},
  title     = {Learning a Recurrent Residual Fusion Network for Multimodal Matching},
  year      = {2017},
  volume    = {},
  number    = {},
  pages     = {4127-4136},
  keywords  = {image matching;image representation;learning (artificial intelligence);text analysis;recurrent residual fusion network learning;vision matching;crossmodal gap;multimodal datasets;discriminative embedding space;textual representations;visual representations;feature enhancement component;matching network;intermediate recurrent outputs;fusion module;shared parameters;feature embeddings;residual learning;recurrent mechanism;RRF;recurrent residual fusion block;co-embedding space;modality-specific representations;multimodal matching;Feature extraction;Visualization;Correlation;Kernel;Neural networks;Bidirectional control},
  doi       = {10.1109/ICCV.2017.442},
  issn      = {2380-7504},
  month     = {Oct}
}
@inproceedings{bi_KL_1,
  title     = {Deep Cross-Modal Projection Learning for Image-Text Matching},
  author    = {Ying Zhang and Huchuan Lu},
  booktitle = {ECCV},
  year      = {2018}
}

@article{co_emmission,
  title   = {Quantifying the Carbon Emissions of Machine Learning},
  author  = {Lacoste, Alexandre and Luccioni, Alexandra and Schmidt, Victor and Dandres, Thomas},
  journal = {arXiv preprint arXiv:1910.09700},
  year    = {2019}
}
@inproceedings{bi_KL_2,
  author    = {L. {Wang} and Y. {Li} and S. {Lazebnik}},
  booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Learning Deep Structure-Preserving Image-Text Embeddings},
  year      = {2016},
  volume    = {},
  number    = {},
  pages     = {5005-5013},
  keywords  = {document image processing;learning (artificial intelligence);neural nets;deep structure-preserving image-text embeddings;two-branch neural network;linear projections;nonlinearities;cross-view ranking constraints;within-view neighborhood structure preservation constraints;metric learning;image-to-text retrieval;text-to-image retrieval;Flickr30K image-sentence datasets;MSCOCO image-sentence datasets;phrase localization;Flickr30K entities dataset;Training;Image color analysis;Bidirectional control;Visualization;Kernel;Machine learning;Computer vision},
  doi       = {10.1109/CVPR.2016.541},
  issn      = {1063-6919},
  month     = {June}
}
@inproceedings{uni_KL_1,
  author    = {Suyoun Kim and Michael Seltzer and Jinyu Li and Rui Zhao},
  title     = {Improved Training for Online End-to-end Speech Recognition Systems},
  year      = 2018,
  booktitle = {Proc. Interspeech 2018},
  pages     = {2913--2917},
  doi       = {10.21437/Interspeech.2018-2517},
  url       = {http://dx.doi.org/10.21437/Interspeech.2018-2517}
}
@misc{knowledge_distillation_hinton2015,
  title         = {Distilling the Knowledge in a Neural Network},
  author        = {Geoffrey Hinton and Oriol Vinyals and Jeff Dean},
  year          = {2015},
  eprint        = {1503.02531},
  archiveprefix = {arXiv},
  primaryclass  = {stat.ML}
}

@book{procustes,
  author    = {Krzanowski, W. J.},
  title     = {Principles of Multivariate Analysis: A User's Perspective},
  year      = {1988},
  isbn      = {0198522118},
  publisher = {Oxford University Press, Inc.},
  address   = {USA}
}
@article{clip_representation,
  author   = {Qiuhong {Ke} and Mohammed {Bennamoun} and Senjian {An} and Ferdous {Sohel} and Farid {Boussaid}},
  journal  = {IEEE Transactions on Image Processing},
  title    = {Learning Clip Representations for Skeleton-Based 3D Action Recognition},
  year     = {2018},
  volume   = {27},
  number   = {6},
  pages    = {2842-2855},
  keywords = {feature extraction;image motion analysis;image representation;image sequences;learning (artificial intelligence);recurrent neural nets;complex spatial structures;long-term temporal dynamics;skeleton joints;human skeleton;temporal information;clip representation;feature learning method;recurrent neural networks;skeleton sequence;spatial relationships;spatial structural information;skeleton-based 3D action recognition;hand-crafted features;multitask convolutional neural network;Skeleton;Hidden Markov models;Feature extraction;Three-dimensional displays;Task analysis;Computational modeling;Robustness;Clip representation;CNN;multi-task learning;3D action recognition},
  doi      = {10.1109/TIP.2018.2812099},
  issn     = {1941-0042},
  month    = {June}
}
@inproceedings{skel_gcn_3,
  author    = {C. {Si} and W. {Chen} and W. {Wang} and L. {Wang} and T. {Tan}},
  booktitle = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {An Attention Enhanced Graph Convolutional LSTM Network for Skeleton-Based Action Recognition},
  year      = {2019},
  volume    = {},
  number    = {},
  pages     = {1227-1236},
  keywords  = {convolutional neural nets;feature extraction;image motion analysis;image recognition;image representation;image sequences;learning (artificial intelligence);recurrent neural nets;skeleton-based action recognition;skeleton sequence;temporal features;discriminative spatial features;human action recognition;skeleton data;spatial configuration;temporal dynamics;spatial domains;temporal domains;temporal hierarchical architecture;temporal receptive fields;AGC-LSTM layer;discriminative spatial information;attention enhanced graph convolutional LSTM network;Action Recognition;Face;Gesture;and Body Pose ; Motion and Tracking; Video Analytics},
  doi       = {10.1109/CVPR.2019.00132},
  issn      = {1063-6919},
  month     = {June}
}
@inproceedings{rgb+pose_1,
  author    = {Amir {Shahroudy} and Gang {Wang} and Tian-Tsong {Ng}},
  booktitle = {2014 6th International Symposium on Communications, Control and Signal Processing (ISCCSP)},
  title     = {Multi-modal feature fusion for action recognition in RGB-D sequences},
  year      = {2014},
  volume    = {},
  number    = {},
  pages     = {1-4},
  keywords  = {image fusion;image recognition;image sequences;video signal processing;multimodal feature fusion;RGB-D sequences;Microsoft Kinect;RGB videos;depth sequences;skeleton information;action recognition techniques;feature extraction;multimodal information fusion;multiview structured sparsity learning;fuse atomic features;hierarchical bag-of-words feature fusion technique;Fuses;Joints;Feature extraction;Vectors;Videos;Dictionaries;Action Recognition;Kinect;Feature Fusion;Structured Sparsity},
  doi       = {10.1109/ISCCSP.2014.6877819},
  issn      = {null},
  month     = {May}
}
@inproceedings{rgb+pose_3,
  author    = {Hossein {Rahmani} and Mohammed {Bennamoun}},
  booktitle = {2017 IEEE International Conference on Computer Vision (ICCV)},
  title     = {Learning Action Recognition Model from Depth and Skeleton Videos},
  year      = {2017},
  volume    = {},
  number    = {},
  pages     = {5833-5842},
  keywords  = {human computer interaction;image motion analysis;image representation;image sensors;learning (artificial intelligence);object recognition;video signal processing;action recognition model learning;depth videos;skeleton videos;human action analysis;human-object interactions;NTU RGB+D;UWA3DII;intraclass variations;environmental objects;view-invariant body-part representation;end-to-end learning framework;shared view-invariant space;depth appearances;human body-part model;deep model;view-invariant representation;depth images;3D human skeleton data;human action recognition problem;depth sensors;benchmark human action recognition datasets;Skeleton;Videos;Biological system modeling;Three-dimensional displays;Sensors;Image recognition;Cameras},
  doi       = {10.1109/ICCV.2017.621},
  issn      = {2380-7504},
  month     = {Oct}
}
@inproceedings{rgb+pose_2,
  author    = {Guiyu {Liu} and Jiuchao {Qian} and Fei {Wen} and Xiaoguang {Zhu} and Rendong {Ying} and Peilin {Liu}},
  booktitle = {2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  title     = {Action Recognition Based on 3D Skeleton and RGB Frame Fusion},
  year      = {2019},
  volume    = {},
  number    = {},
  pages     = {258-264},
  keywords  = {},
  doi       = {10.1109/IROS40897.2019.8967570},
  issn      = {2153-0858},
  month     = {Nov}
}
@article{skel_CNN_2,
  title    = {Enhanced skeleton visualization for view invariant human action recognition},
  journal  = {Pattern Recognition},
  volume   = {68},
  pages    = {346 - 362},
  year     = {2017},
  issn     = {0031-3203},
  doi      = {https://doi.org/10.1016/j.patcog.2017.02.030},
  url      = {http://www.sciencedirect.com/science/article/pii/S0031320317300936},
  author   = {Mengyuan Liu and Hong Liu and Chen Chen},
  keywords = {Human action recognition, View invariant, Skeleton sequence},
  abstract = {Human action recognition based on skeletons has wide applications in human–computer interaction and intelligent surveillance. However, view variations and noisy data bring challenges to this task. What’s more, it remains a problem to effectively represent spatio-temporal skeleton sequences. To solve these problems in one goal, this work presents an enhanced skeleton visualization method for view invariant human action recognition. Our method consists of three stages. First, a sequence-based view invariant transform is developed to eliminate the effect of view variations on spatio-temporal locations of skeleton joints. Second, the transformed skeletons are visualized as a series of color images, which implicitly encode the spatio-temporal information of skeleton joints. Furthermore, visual and motion enhancement methods are applied on color images to enhance their local patterns. Third, a convolutional neural networks-based model is adopted to extract robust and discriminative features from color images. The final action class scores are generated by decision level fusion of deep features. Extensive experiments on four challenging datasets consistently demonstrate the superiority of our method.}
}
@inproceedings{skel_CNN_1,
  author    = {Du, Yong and Fu, Yun and Wang, Liang},
  booktitle = {2015 3rd IAPR Asian Conference on Pattern Recognition (ACPR)},
  title     = {Skeleton based action recognition with convolutional neural network},
  year      = {2015},
  volume    = {},
  number    = {},
  pages     = {579-583},
  doi       = {10.1109/ACPR.2015.7486569}
}


@article{tsne,
  added-at  = {2015-06-19T12:07:15.000+0200},
  author    = {van der Maaten, Laurens and Hinton, Geoffrey},
  biburl    = {https://www.bibsonomy.org/bibtex/28b9aebb404ad4a4c6a436ea413550b30/lopusz_kdd},
  interhash = {370ba8b9e1909b61880a6f47c93bcd49},
  intrahash = {8b9aebb404ad4a4c6a436ea413550b30},
  journal   = {Journal of Machine Learning Research},
  keywords  = {dimensionality_reduction tSNE visualization},
  pages     = {2579--2605},
  timestamp = {2015-08-19T15:19:11.000+0200},
  title     = {Visualizing Data using {t-SNE} },
  url       = {http://www.jmlr.org/papers/v9/vandermaaten08a.html},
  volume    = 9,
  year      = 2008
}
@inproceedings{skel_cnn_3,
  author    = {Qiuhong Ke and
               Mohammed Bennamoun and
               Senjian An and
               Ferdous Ahmed Sohel and
               Farid Boussa{\"{\i}}d},
  title     = {A New Representation of Skeleton Sequences for 3D Action Recognition},
  booktitle = {2017 {IEEE} Conference on Computer Vision and Pattern Recognition,
               {CVPR} 2017, Honolulu, HI, USA, July 21-26, 2017},
  pages     = {4570--4579},
  publisher = {{IEEE} Computer Society},
  year      = {2017},
  url       = {https://doi.org/10.1109/CVPR.2017.486},
  doi       = {10.1109/CVPR.2017.486},
  timestamp = {Sat, 19 Oct 2019 20:24:00 +0200},
  biburl    = {https://dblp.org/rec/conf/cvpr/KeBASB17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{scikit-learn,
  title   = {Scikit-learn: Machine Learning in {P}ython},
  author  = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
             and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
             and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
             Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  journal = {Journal of Machine Learning Research},
  volume  = {12},
  pages   = {2825--2830},
  year    = {2011}
}
@article{youtube8M,
  title   = {Youtube-8m: A large-scale video classification benchmark},
  author  = {Abu-El-Haija, Sami and Kothari, Nisarg and Lee, Joonseok and Natsev, Paul and Toderici, George and Varadarajan, Balakrishnan and Vijayanarasimhan, Sudheendra},
  journal = {arXiv preprint arXiv:1609.08675},
  year    = {2016}
}
@inproceedings{dance_with_flow,
  author    = {Zhao, Jiaojiao and Snoek, Cees G. M.},
  title     = {Dance With Flow: Two-In-One Stream Action Detection},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  year      = {2019}
}
@inproceedings{mars,
  title     = {{MARS: Motion-Augmented RGB Stream for Action Recognition}},
  author    = {Crasto, Nieves and Weinzaepfel, Philippe and Alahari, Karteek and Schmid, Cordelia},
  booktitle = {CVPR},
  year      = {2019}
}
@inproceedings{NetVLAD,
  author    = {Relja Arandjelovi\'c and Petr Gronat and Akihiko Torii and Tomas Pajdla and Josef Sivic},
  title     = {{NetVLAD}: {CNN} architecture for weakly supervised place recognition},
  booktitle = {IEEE Conference on Computer Vision and Pattern Recognition},
  year      = {2016}
}
@conference{untrimmed_basic,
  title     = {Temporal Activity Detection in Untrimmed Videos with Recurrent Neural Networks},
  booktitle = {1st NIPS Workshop on Large Scale Computer Vision Systems 2016},
  year      = {2016},
  month     = {12/2016},
  abstract  = {<p>This work proposes a simple pipeline to classify and temporally localize activities in untrimmed videos. Our system uses features from a 3D Convolutional Neural Network (C3D) as input to train a a recurrent neural network (RNN) that learns to classify video clips of 16 frames. After clip prediction, we post-process the output of the RNN to assign a single activity label to each video, and determine the temporal boundaries of the activity within the video. We show how our system can achieve competitive results in both tasks with a simple architecture. We evaluate our method in the ActivityNet Challenge 2016, achieving a 0.5874 mAP and a 0.2237 mAP in the classification and detection tasks, respectively. Our code and models are publicly available at:\&nbsp;<a href="https://imatge-upc.github.io/activitynet-2016-cvprw/">https://imatge-upc.github.io/activitynet-2016-cvprw/</a></p>},
  author    = {Montes, Alberto and Amaia Salvador and Pascual-deLaPuente, Santiago and Gir{\'o}-i-Nieto, X.}
}
@inproceedings{directed_graph,
  author    = {Shi, Lei and Zhang, Yifan and Cheng, Jian and Lu, Hanqing},
  title     = {Skeleton-Based Action Recognition With Directed Graph Neural Networks},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  year      = {2019}
}
@inproceedings{stgcn2018aaai,
  title     = {Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition},
  author    = {Sijie Yan and Yuanjun Xiong and Dahua Lin},
  booktitle = {AAAI},
  year      = {2018}
}
@inproceedings{deep-progressive,
  author    = {Yansong Tang and Yi Tian and Jiwen Lu and Peiyang Li and Jie Zhou},
  booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  title     = {Deep Progressive Reinforcement Learning for Skeleton-Based Action Recognition},
  year      = {2018},
  volume    = {},
  number    = {},
  pages     = {5323-5332},
  keywords  = {convolutional neural nets;graph theory;image capture;image motion analysis;image recognition;image representation;image sequences;learning (artificial intelligence);video signal processing;skeleton-based action recognition;deep progressive reinforcement learning method;skeleton-based videos;frame selection;graph-based structure;graph-based convolutional neural network;Joints;Videos;Biological system modeling;Three-dimensional displays;Computer vision},
  doi       = {10.1109/CVPR.2018.00558},
  issn      = {},
  month     = {June}
}
@article{timeception,
  author        = {Noureldien Hussein and
                   Efstratios Gavves and
                   Arnold W. M. Smeulders},
  title         = {Timeception for Complex Action Recognition},
  journal       = {CoRR},
  volume        = {abs/1812.01289},
  year          = {2018},
  url           = {http://arxiv.org/abs/1812.01289},
  archiveprefix = {arXiv},
  eprint        = {1812.01289},
  timestamp     = {Tue, 01 Jan 2019 15:01:25 +0100},
  biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1812-01289},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}
@article{text_video_embedding,
  author        = {Antoine Miech and
                   Ivan Laptev and
                   Josef Sivic},
  title         = {Learning a Text-Video Embedding from Incomplete and Heterogeneous
                   Data},
  journal       = {CoRR},
  volume        = {abs/1804.02516},
  year          = {2018},
  url           = {http://arxiv.org/abs/1804.02516},
  archiveprefix = {arXiv},
  eprint        = {1804.02516},
  timestamp     = {Mon, 13 Aug 2018 16:48:06 +0200},
  biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1804-02516},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{joint_modelling_captioning,
  author    = {Pan, Yingwei and Mei, Tao and Yao, Ting and Li, Houqiang and Rui, Yong},
  title     = {Jointly Modeling Embedding and Translation to Bridge Video and Language},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  year      = {2016}
}
@article{ntu120,
  title   = {NTU RGB+D 120: A Large-Scale Benchmark for 3D Human Activity Understanding},
  author  = {Liu, Jun and Shahroudy, Amir and Perez, Mauricio and Wang, Gang and Duan, Ling-Yu and Kot, Alex C.},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year    = {2019},
  doi     = {10.1109/TPAMI.2019.2916873}
}
@inproceedings{domain_adapt_1,
  author    = {Munro, Jonathan and Damen, Dima},
  title     = {{M}ulti-modal {D}omain {A}daptation for {F}ine-grained {A}ction {R}ecognition},
  booktitle = {Computer Vision and Pattern Recognition (CVPR)},
  year      = {2020}
}
@article{EPIC,
  author        = {Dima Damen and
                   Hazel Doughty and
                   Giovanni Maria Farinella and
                   Sanja Fidler and
                   Antonino Furnari and
                   Evangelos Kazakos and
                   Davide Moltisanti and
                   Jonathan Munro and
                   Toby Perrett and
                   Will Price and
                   Michael Wray},
  title         = {Scaling Egocentric Vision: The {EPIC-KITCHENS} Dataset},
  journal       = {CoRR},
  volume        = {abs/1804.02748},
  year          = {2018},
  url           = {http://arxiv.org/abs/1804.02748},
  archiveprefix = {arXiv},
  eprint        = {1804.02748},
  timestamp     = {Mon, 13 Aug 2018 16:47:18 +0200},
  biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1804-02748},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{Alexnet,
  title     = {Imagenet classification with deep convolutional neural networks},
  author    = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle = {Advances in neural information processing systems},
  pages     = {1097--1105},
  year      = {2012}
}

@article{simonyan2014very,
  title   = {Very deep convolutional networks for large-scale image recognition},
  author  = {Simonyan, Karen and Zisserman, Andrew},
  journal = {arXiv preprint arXiv:1409.1556},
  year    = {2014}
}

@article{video_transformer_network,
  author        = {Rohit Girdhar and
                   Jo{\~{a}}o Carreira and
                   Carl Doersch and
                   Andrew Zisserman},
  title         = {Video Action Transformer Network},
  journal       = {CoRR},
  volume        = {abs/1812.02707},
  year          = {2018},
  url           = {http://arxiv.org/abs/1812.02707},
  archiveprefix = {arXiv},
  eprint        = {1812.02707},
  timestamp     = {Tue, 01 Jan 2019 15:01:25 +0100},
  biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1812-02707},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}

@misc{ssd,
  added-at    = {2018-01-23T00:34:21.000+0100},
  author      = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
  biburl      = {https://www.bibsonomy.org/bibtex/217e9dc47586f63397871307f74f51453/roaur},
  description = {SSD: Single Shot MultiBox Detector},
  doi         = {10.1007/978-3-319-46448-0_2},
  interhash   = {3de0ab12eb5f85e1a99264c83b0d0016},
  intrahash   = {17e9dc47586f63397871307f74f51453},
  keywords    = {deep-learning ssd},
  note        = {cite arxiv:1512.02325Comment: ECCV 2016},
  timestamp   = {2018-01-23T00:34:21.000+0100},
  title       = {SSD: Single Shot MultiBox Detector},
  url         = {http://arxiv.org/abs/1512.02325},
  year        = 2015
}
@inproceedings{deep-adaptiveTP,
  author    = {Song, Sibo and Cheung, Ngai-Man and Chandrasekhar, Vijay and Mandal, Bappaditya},
  title     = {Deep Adaptive Temporal Pooling for Activity Recognition},
  booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
  series    = {MM '18},
  year      = {2018},
  isbn      = {978-1-4503-5665-7},
  location  = {Seoul, Republic of Korea},
  pages     = {1829--1837},
  numpages  = {9},
  url       = {http://doi.acm.org/10.1145/3240508.3240713},
  doi       = {10.1145/3240508.3240713},
  acmid     = {3240713},
  publisher = {ACM},
  address   = {New York, NY, USA},
  keywords  = {adaptive temporal pooling, human activity recognition}
} 
@article{context_gating,
  author        = {Antoine Miech and
                   Ivan Laptev and
                   Josef Sivic},
  title         = {Learnable pooling with Context Gating for video classification},
  journal       = {CoRR},
  volume        = {abs/1706.06905},
  year          = {2017},
  url           = {http://arxiv.org/abs/1706.06905},
  archiveprefix = {arXiv},
  eprint        = {1706.06905},
  timestamp     = {Mon, 13 Aug 2018 16:47:06 +0200},
  biburl        = {https://dblp.org/rec/journals/corr/MiechLS17.bib},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{TSN,
  author    = {Limin Wang and Yuanjun Xiong and Zhe Wang
               and Yu Qiao and Dahua Lin and Xiaoou Tang and Luc {Val Gool}},
  title     = {Temporal Segment Networks: Towards Good Practices for Deep Action Recognition},
  booktitle = {ECCV},
  year      = {2016}
}
@article{TSN_new,
  title   = {Temporal Segment Networks: Towards Good Practices for Deep Action Recognition},
  author  = {Limin Wang and Yuanjun Xiong and Zhe Wang and Yu Qiao and Dahua Lin and Xiaoou Tang and Luc Van Gool},
  journal = {ArXiv},
  year    = {2016},
  volume  = {abs/1608.00859}
}
@inproceedings{TRN,
  title     = {Temporal Relational Reasoning in Videos},
  author    = {Bolei Zhou and Alex Andonian and Antonio Torralba},
  booktitle = {ECCV},
  year      = {2017}
}
@article{ltc,
  title   = {Long-term Temporal Convolutions for Action Recognition},
  author  = {Varol, G{\"u}l and Laptev, Ivan and Schmid, Cordelia},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year    = {2018},
  volume  = {40},
  number  = {6},
  pages   = {1510--1517},
  doi     = {10.1109/TPAMI.2017.2712608}
}
@inproceedings{Baradel_BMVC,
  author    = {Baradel, Fabien and Wolf, Christian and Mille, Julien},
  title     = {Human Activity Recognition with Pose-driven Attention to RGB},
  booktitle = {The British Machine Vision Conference (BMVC)},
  month     = {September},
  year      = {2018}
}
@article{DescribingVB,
  title   = {Describing Videos by Exploiting Temporal Structure},
  author  = {Li Yao and Atousa Torabi and Kyunghyun Cho and Nicolas Ballas and Christopher Joseph Pal and Hugo Larochelle and Aaron C. Courville},
  journal = {2015 IEEE International Conference on Computer Vision (ICCV)},
  year    = {2015},
  pages   = {4507-4515}
}
@article{hogg,
  title    = {Model-based vision: a program to see a walking person},
  journal  = {Image and Vision Computing},
  volume   = {1},
  number   = {1},
  pages    = {5 - 20},
  year     = {1983},
  issn     = {0262-8856},
  doi      = {https://doi.org/10.1016/0262-8856(83)90003-3},
  url      = {http://www.sciencedirect.com/science/article/pii/0262885683900033},
  author   = {David Hogg},
  keywords = {vision, machine perception, WALKER model}
}
@article{recurrent_attention_image,
  title   = {Residual Attention Network for Image Classification},
  author  = {Wang, Fei and Jiang, Mengqing and Qian, Chen and Yang, Shuo and Li, Cheng and Zhang, Honggang and Wang, Xiaogang and Tang, Xiaoou},
  journal = {arXiv preprint arXiv:1704.06904},
  year    = {2017}
}
@article{multi-object-attention,
  title   = {Multiple Object Recognition with Visual Attention},
  author  = {Jimmy Ba and Volodymyr Mnih and Koray Kavukcuoglu},
  journal = {CoRR},
  year    = {2014},
  volume  = {abs/1412.7755}
}
@article{hard_attention_end,
  title   = {End-to-end Learning of Action Detection from Frame Glimpses in Videos},
  author  = {Yeung, Serena and Russakovsky, Olga and Mori, Greg and Fei-Fei, Li},
  journal = {arXiv preprint arXiv:1511.06984},
  year    = {2015}
}
@inproceedings{Recurrent_models,
  author    = {Mnih, Volodymyr and Heess, Nicolas and Graves, Alex and Kavukcuoglu, Koray},
  title     = {Recurrent Models of Visual Attention},
  booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
  series    = {NIPS'14},
  year      = {2014},
  location  = {Montreal, Canada},
  pages     = {2204--2212},
  numpages  = {9},
  url       = {http://dl.acm.org/citation.cfm?id=2969033.2969073},
  acmid     = {2969073},
  publisher = {MIT Press},
  address   = {Cambridge, MA, USA}
} 

@inproceedings{nktm,
  author    = {Hossein Rahmani and Ajmal Mian},
  booktitle = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Learning a non-linear knowledge transfer model for cross-view action recognition},
  year      = {2015},
  volume    = {},
  number    = {},
  pages     = {2458-2466},
  keywords  = {gesture recognition;image motion analysis;unsupervised learning;video cameras;video coding;nonlinear knowledge transfer model;cross-view action recognition;unseen views;unknown views;unsupervised learning;nonlinear model;canonical view;NKTM;deep network;weight decay;sparsity constraints;high-level virtual path;camera viewing directions;camera viewpoints;dense trajectories;mocap data;real video data;codebook;IXMAS datasets;N-UCLA datasets;Computational modeling;Training},
  doi       = {10.1109/CVPR.2015.7298860},
  issn      = {1063-6919},
  month     = {June}
}
@inproceedings{ncte,
  author    = {Ankur Gupta and Julieta Martinez and James J. Little and Robert J. Woodham},
  booktitle = {2014 IEEE Conference on Computer Vision and Pattern Recognition},
  title     = {3D Pose from Motion for Cross-View Action Recognition via Non-linear Circulant Temporal Encoding},
  year      = {2014},
  volume    = {},
  number    = {},
  pages     = {2601-2608},
  keywords  = {encoding;image matching;image motion analysis;image sequences;pose estimation;video signal processing;cross view action recognition;nonlinear circulant temporal encoding;transfer knowledge;unlabelled mocap data;3D pose sequence;multiple motion projection;closed form solution;IXMAS dataset unsupervised modality;Videos;Trajectory;Three-dimensional displays;Training;Databases;Kernel;Encoding},
  doi       = {10.1109/CVPR.2014.333},
  issn      = {1063-6919},
  month     = {June}
}
@inproceedings{hanklets,
  author    = {B. Li and O. I. Camps and M. Sznaier},
  booktitle = {2012 IEEE Conference on Computer Vision and Pattern Recognition},
  title     = {Cross-view activity recognition using Hankelets},
  year      = {2012},
  volume    = {},
  number    = {},
  pages     = {1362-1369},
  keywords  = {Hankel matrices;image classification;spatiotemporal phenomena;video signal processing;Hankelets;human activity recognition;visual surveillance;gaming interface;localized spatiotemporal features;testing data;training data;tracklets;robust cross-view activity recognition;classifier;IXMAS dataset;performance improvement;Trajectory;Vectors;Training;Noise measurement;Histograms;Testing;Cameras},
  doi       = {10.1109/CVPR.2012.6247822},
  issn      = {1063-6919},
  month     = {June}
}
@inproceedings{hpm,
  author    = {Hossein Rahmani and Ajmal Mian.},
  booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {3D Action Recognition from Novel Viewpoints},
  year      = {2016},
  volume    = {},
  number    = {},
  pages     = {1506-1515},
  keywords  = {convolution;feature extraction;Fourier analysis;image capture;image motion analysis;image representation;neural nets;pattern clustering;pose estimation;rendering (computer graphics);solid modelling;video signal processing;3D action recognition;viewpoints;human pose representation model;view-invariant high-level space;deep convolutional neural network;multiview training data;synthetic 3D human models;motion capture;rendering;CNN model;pose labels;pose clustering;real depth images;real depth videos;view-invariant feature extraction;spatio-temporal representation;group sparse Fourier temporal pyramid;Videos;Three-dimensional displays;Solid modeling;Data models;Skeleton;Feature extraction;Biological system modeling},
  doi       = {10.1109/CVPR.2016.167},
  issn      = {1063-6919},
  month     = {June}
}
@inproceedings{cvp,
  author    = {Z. Zhang and C. Wang and B. Xiao and W. Zhou and S. Liu and C. Shi},
  booktitle = {2013 IEEE Conference on Computer Vision and Pattern Recognition},
  title     = {Cross-View Action Recognition via a Continuous Virtual Path},
  year      = {2013},
  volume    = {},
  number    = {},
  pages     = {2690-2697},
  keywords  = {feature extraction;image classification;image motion analysis;video signal processing;cross-view action recognition;continuous virtual path;source view;target view;virtual view;linear transformation;action descriptor;infinite-dimensional feature;kernelized classifiers;IXMAS dataset;Kernel;Vectors;Training;Pattern recognition;Robustness;Feature extraction;Target recognition},
  doi       = {10.1109/CVPR.2013.347},
  issn      = {1063-6919},
  month     = {June}
}
@inproceedings{dvv,
  author    = {R. Li and T. Zickler},
  booktitle = {2012 IEEE Conference on Computer Vision and Pattern Recognition},
  title     = {Discriminative virtual views for cross-view action recognition},
  year      = {2012},
  volume    = {},
  number    = {},
  pages     = {2855-2862},
  keywords  = {object recognition;discriminative virtual views;cross-view action recognition;action descriptor linear transformation;transformation sequence;source view;target view partial labeling;action category;Training;Target recognition;Covariance matrix;Cameras;Feature extraction;Transforms;Vectors},
  doi       = {10.1109/CVPR.2012.6248011},
  issn      = {1063-6919},
  month     = {June}
}

@inproceedings{visualattention,
  title     = {Action Recognition using Visual Attention},
  author    = {Sharma, Shikhar and
               Kiros, Ryan and
               Salakhutdinov, Ruslan},
  booktitle = {International Conference on Learning Representations (ICLR) Workshop},
  month     = {May},
  year      = {2016},
  url       = {https://arxiv.org/abs/1511.04119}
}

@inproceedings{sta_lstm,
  title     = {An End-to-End Spatio-Temporal Attention Model for Human Action Recognition from Skeleton Data},
  author    = {Song, Sijie and Lan, Cuiling and Xing, Junliang and Zeng, Wenjun and Liu, Jiaying},
  booktitle = {AAAI Conference on Artificial Intelligence},
  year      = {2017},
  pages     = {4263--4270}
}
@inproceedings{valstm,
  author    = {Zhang, Pengfei and Lan, Cuiling and Xing, Junliang and Zeng, Wenjun and Xue, Jianru and Zheng, Nanning},
  title     = {View Adaptive Recurrent Neural Networks for High Performance Human Action Recognition From Skeleton Data},
  booktitle = {The IEEE International Conference on Computer Vision (ICCV)},
  month     = {Oct},
  year      = {2017}
}
@inproceedings{pem,
  author    = {Liu, Mengyuan and Yuan, Junsong},
  title     = {Recognizing Human Actions as the Evolution of Pose Estimation Maps},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  year      = {2018}
}

@inproceedings{skeletalquads,
  author    = {Evangelidis, G., and  Singh, G., and Horaud, R.},
  booktitle = {2014 22nd International Conference on Pattern Recognition},
  title     = {Skeletal Quads: Human Action Recognition Using Joint Quadruples},
  year      = {2014},
  volume    = {},
  number    = {},
  pages     = {4513-4518},
  keywords  = {feature extraction;Gaussian processes;image motion analysis;image recognition;image representation;support vector machines;skeletal quads;human action recognition;joint quadruples;human motion analysis;human skeleton structure extraction;depth images;local skeleton descriptor;similarity normalisation transform;compact 6D view-invariant skeletal feature;Fisher kernel representation;Gaussian mixture model;Fisher vector;multilevel representation;action description;linear SVM;MSRAction3D datasets;HDM05 datasets;Joints;Vectors;Accuracy;Kernel;Training;Three-dimensional displays},
  doi       = {10.1109/ICPR.2014.772},
  issn      = {1051-4651},
  month     = {Aug}
}
@inproceedings{liegroup,
  author    = {Vemulapalli, R., and Arrate, F., and Chellappa, R.},
  booktitle = {2014 IEEE Conference on Computer Vision and Pattern Recognition},
  title     = {Human Action Recognition by Representing 3D Skeletons as Points in a Lie Group},
  year      = {2014},
  volume    = {},
  number    = {},
  pages     = {588-595},
  keywords  = {curve fitting;estimation theory;Fourier analysis;gesture recognition;image motion analysis;Lie algebras;Lie groups;support vector machines;3D skeletons;Lie group;cost-effective depth sensors;real-time skeleton estimation algorithm;skeleton-based human action recognition;skeleton-based approach;joint location;joint angle;human skeleton;skeletal representation;3D geometric relationship;3D space;3D rigid body motion;special Euclidean group;curved manifold;action curves;Lie algebra;vector space;dynamic time warping;Fourier temporal pyramid representation;linear SVM;Joints;Three-dimensional displays;Algebra;Geometry;Hidden Markov models;Sensors;Action Recognition;Special Euclidean Group;Lie Groups},
  doi       = {10.1109/CVPR.2014.82},
  issn      = {1063-6919},
  month     = {June}
}
@inproceedings{can_spatio-temporal,
  title     = {Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet?},
  author    = {Hara, Kensho and Kataoka, Hirokatsu and Satoh, Yutaka},
  booktitle = {Proceedings of the IEEE conference on Computer Vision and Pattern Recognition},
  pages     = {6546--6555},
  year      = {2018}
}
@inproceedings{s3d,
  author    = {Saining Xie and
               Chen Sun and
               Jonathan Huang and
               Zhuowen Tu and
               Kevin Murphy},
  editor    = {Vittorio Ferrari and
               Martial Hebert and
               Cristian Sminchisescu and
               Yair Weiss},
  title     = {Rethinking Spatiotemporal Feature Learning: Speed-Accuracy Trade-offs
               in Video Classification},
  booktitle = {Computer Vision - {ECCV} 2018 - 15th European Conference, Munich,
               Germany, September 8-14, 2018, Proceedings, Part {XV}},
  series    = {Lecture Notes in Computer Science},
  volume    = {11219},
  pages     = {318--335},
  publisher = {Springer},
  year      = {2018},
  url       = {https://doi.org/10.1007/978-3-030-01267-0\_19},
  doi       = {10.1007/978-3-030-01267-0\_19},
  timestamp = {Tue, 14 May 2019 10:00:45 +0200},
  biburl    = {https://dblp.org/rec/conf/eccv/XieSHTM18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{Johansson,
  author   = {Johansson, Gunnar},
  title    = {Visual perception of biological motion and a model for its analysis},
  journal  = {Perception {\&} Psychophysics},
  year     = {1973},
  month    = {Jun},
  day      = {01},
  volume   = {14},
  number   = {2},
  pages    = {201--211},
  abstract = {This paper reports the first phase of a research program on visual perception of motion patterns characteristic of living organisms in locomotion. Such motion patterns in animals and men are termed here as biological motion. They are characterized by a far higher degree of complexity than the patterns of simple mechanical motions usually studied in our laboratories. In everyday perceptions, the visual information from biological motion and from the corresponding figurative contour patterns (the shape of the body) are intermingled. A method for studying information from the motion pattern per se without interference with the form aspect was devised. In short, the motion of the living body was represented by a few bright spots describing the motions of the main joints. It is found that 10--12 such elements in adequate motion combinations in proximal stimulus evoke a compelling impression of human walking, running, dancing, etc. The kinetic-geometric model for visual vector analysis originally developed in the study of perception of motion combinations of the mechanical type was applied to these biological motion patterns. The validity of this model in the present context was experimentally tested and the results turned out to be highly positive.},
  issn     = {1532-5962},
  doi      = {10.3758/BF03212378},
  url      = {https://doi.org/10.3758/BF03212378}
}
@inproceedings{SBU-kinect,
  title        = {Two-person Interaction Detection Using Body-Pose Features and Multiple Instance Learning},
  author       = {Kiwon Yun and Jean Honorio and Debaleena Chattopadhyay and Tamara L. Berg and Dimitris Samaras},
  booktitle    = {Computer Vision and Pattern Recognition Workshops (CVPRW), 2012 IEEE Computer Society Conference on},
  year         = {2012},
  organization = {IEEE}
}

@article{adam_optimizer,
  author        = {Diederik P. Kingma and
                   Jimmy Ba},
  title         = {Adam: {A} Method for Stochastic Optimization},
  journal       = {CoRR},
  volume        = {abs/1412.6980},
  year          = {2014},
  url           = {http://arxiv.org/abs/1412.6980},
  archiveprefix = {arXiv},
  eprint        = {1412.6980},
  timestamp     = {Wed, 07 Jun 2017 14:40:52 +0200},
  biburl        = {http://dblp.org/rec/bib/journals/corr/KingmaB14},
  bibsource     = {dblp computer science bibliography, http://dblp.org}
}

@inproceedings{gradientclipping,
  author    = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
  title     = {Sequence to Sequence Learning with Neural Networks},
  booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
  series    = {NIPS'14},
  year      = {2014},
  location  = {Montreal, Canada},
  pages     = {3104--3112},
  numpages  = {9},
  url       = {http://dl.acm.org/citation.cfm?id=2969033.2969173},
  acmid     = {2969173},
  publisher = {MIT Press},
  address   = {Cambridge, MA, USA}
} 
@article{Dropout,
  author     = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  title      = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  journal    = {J. Mach. Learn. Res.},
  issue_date = {January 2014},
  volume     = {15},
  number     = {1},
  month      = jan,
  year       = {2014},
  issn       = {1532-4435},
  pages      = {1929--1958},
  numpages   = {30},
  url        = {http://dl.acm.org/citation.cfm?id=2627435.2670313},
  acmid      = {2670313},
  publisher  = {JMLR.org},
  keywords   = {deep learning, model combination, neural networks, regularization}
} 
@misc{tensorflow2015,
  title  = { {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
  url    = {https://www.tensorflow.org/},
  note   = {Software available from tensorflow.org},
  author = {
            Mart\'{\i}n~Abadi and
            others},
  year   = {2015}
}
@misc{chollet2015keras,
  title     = {Keras},
  author    = {Chollet, Fran\c{c}ois and others},
  year      = {2015},
  publisher = {GitHub}
}
@inproceedings{Range_sample,
  author    = {C. Lu and J. Jia and C. K. Tang},
  booktitle = {2014 IEEE Conference on Computer Vision and Pattern Recognition},
  title     = {Range-Sample Depth Feature for Action Recognition},
  year      = {2014},
  volume    = {},
  number    = {},
  pages     = {772-779},
  keywords  = {computer vision;gesture recognition;image motion analysis;learning (artificial intelligence);τ tests;action recognition;binary range-sample depth feature;standard learning algorithms;Accuracy;Hamming distance;Histograms;Joints;Robustness;Standards;Three-dimensional displays;Action Recognition;Binary Feature;Depth;Sampling},
  doi       = {10.1109/CVPR.2014.104},
  issn      = {1063-6919},
  month     = {June}
}
@article{JOULE-SVM,
  author   = {Jian-Fang Hu and Wei-Shi Zheng and Jianhuang Lai and Jianguo Zhang},
  journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title    = {Jointly Learning Heterogeneous Features for RGB-D Activity Recognition},
  year     = {2017},
  volume   = {39},
  number   = {11},
  pages    = {2186-2200},
  keywords = {image colour analysis;inference mechanisms;iterative methods;learning (artificial intelligence);optimisation;RGB-D activity benchmarking;RGB-D activity recognition;feature channels;feature-specific intermediate transforms;fusion learning;heterogeneous features;heterogeneous multitask learning;i-transforms;joint learning model;joint model;latent shared features;shared feature-specific components;simple inference model;three-step iterative optimization algorithm;Feature extraction;Image color analysis;Skeleton;Three-dimensional displays;Transforms;Visualization;Heterogeneous features learning;RGB-D activity recognition;action recognition},
  doi      = {10.1109/TPAMI.2016.2640292},
  issn     = {0162-8828},
  month    = {Nov}
}
@article{LSTM_basic,
  author     = {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
  title      = {Long Short-Term Memory},
  journal    = {Neural Comput.},
  issue_date = {November 15, 1997},
  volume     = {9},
  number     = {8},
  month      = nov,
  year       = {1997},
  issn       = {0899-7667},
  pages      = {1735--1780},
  numpages   = {46},
  url        = {http://dx.doi.org/10.1162/neco.1997.9.8.1735},
  doi        = {10.1162/neco.1997.9.8.1735},
  acmid      = {1246450},
  publisher  = {MIT Press},
  address    = {Cambridge, MA, USA}
} 
@inproceedings{gemetricfeaturesWACV2017,
  author    = {Songyang Zhang and Xiaoming Liu and Jun Xiao},
  booktitle = {2017 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  title     = {On Geometric Features for Skeleton-Based Action Recognition Using Multilayer LSTM Networks},
  year      = {2017},
  volume    = {},
  number    = {},
  pages     = {148-157},
  keywords  = {geometry;gesture recognition;3-layer LSTM framework;RNN model enhancement;RNN-based approach;geometric features;geometric relational features;multilayer LSTM networks;skeleton-based action recognition;Computational modeling;Computer architecture;Logic gates;Neurons;Nonhomogeneous media;Skeleton;Three-dimensional displays},
  doi       = {10.1109/WACV.2017.24},
  issn      = {},
  month     = {March}
}
@inproceedings{lrcn,
  author    = {Donahue, Jeffrey and Anne Hendricks, Lisa and Guadarrama, Sergio and Rohrbach, Marcus and Venugopalan, Subhashini and Saenko, Kate and Darrell, Trevor},
  title     = {Long-Term Recurrent Convolutional Networks for Visual Recognition and Description},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  year      = {2015}
}
@inproceedings{1,
  title     = {Learning realistic human actions from movies},
  author    = {Ivan Laptev and Marcin Marszałek and Cordelia Schmid and Benjamin Rozenfeld},
  booktitle = {CVPR},
  year      = {2008}
}
@inproceedings{2,
  title     = {Recognizing human actions: A local SVM approach},
  author    = {Christian Schuldt and Ivan Laptev and Barbara Caputo},
  booktitle = {ICPR},
  year      = {2004}
}
@inproceedings{3,
  title     = {Action recognition with improved trajectories},
  author    = {Heng Wang and Cordelia Schmid},
  booktitle = {ICCV},
  year      = {2013}
}
@inproceedings{space-time_interest,
  title     = {Space-time Interest Points},
  author    = {Ivan Laptev and Tony Lindeberg},
  booktitle = {ICCV},
  year      = {2003}
}
@inproceedings{5,
  title     = {Super Normal Vector for Activity Recognition Using Depth Sequences},
  author    = {Xiaodong Yang and Yingli Tian},
  booktitle = {CVPR},
  year      = {2014}
}
@inproceedings{7,
  title     = {Bilinear heterogeneous information machine for {RGB}-{D} action recognition},
  author    = {Kong, Yu and Fu, Yun},
  booktitle = {CVPR},
  year      = {2015}
}
@inproceedings{8a,
  title     = {Mining Actionlet Ensemble for Action Recognition with Depth Cameras},
  author    = {Wu, Ying},
  booktitle = {CVPR},
  year      = {2012}
}
@article{8b,
  title    = {Action {Recognition} {Using} {Rate}-{Invariant} {Analysis} of {Skeletal} {Shape} {Trajectories}},
  author   = {Amor, B.B. and Su, J. and Srivastava, A.},
  journal  = {PAMI},
  year     = {2016},
  month    = Jan,
  number   = {1},
  pages    = {1--13},
  volume   = {38},
  file     = {IEEE Xplore Abstract Record:/user/mkopersk/home/.mozilla/firefox/mb40m4tj.default/zotero/storage/AGZ2SA7J/articleDetails.html:text/html},
  keywords = {Action recognition, Depth sensors, Hidden Markov models, Manifold Trajectories, Measurement, Riemannian geometry, Shape, Skeletal data, Skeleton, Space vehicles, Three-dimensional displays, Trajectory}
}
@inproceedings{8c,
  title     = {A Decision Forest Based Feature Selection Framework for Action Recognition
               from RGB-Depth Cameras},
  author    = {Farhood Negin and
               Firat {\"{O}}zdemir and
               Ceyhun Burak Akg{\"{u}}l and
               Kamer Ali Y{\"{u}}ksel and
               Ayt{\"{u}}l Er{\c{c}}il},
  booktitle = {ICIAR},
  year      = {2013}
}
@inproceedings{8d,
  title     = {Rolling Rotations for Recognizing Human Actions from 3DSkeletal Data},
  author    = {Raviteja Vemulapalli and Rama Chellappa},
  booktitle = {CVPR},
  year      = {2016}
}
@inproceedings{9,
  title     = {Leveraging Hierarchial Parametric Networks for Skeletal Joints Based Action Segmentation and Recognition},
  author    = {Di Wu and Ling Shao},
  booktitle = {CVPR},
  year      = {2014}
}
@inproceedings{10,
  title     = {Imagenet classification with deep convolutional neural networks},
  author    = {Alex Krizhevsky and Ilya Sutskever and Geoffrey E. Hinton},
  booktitle = {NIPS},
  year      = {2012}
}
@inproceedings{large-scale_video_classification,
  title     = {Large-{Scale} {Video} {Classification} with {Convolutional} {Neural} {Networks}},
  author    = {Karpathy, Andrej and Toderici, George and Shetty, Sanketh and Leung, Thomas and Sukthankar, Rahul and Fei-Fei, Li},
  booktitle = {CVPR},
  year      = {2014}
}
@inproceedings{tdd,
  title     = {Action {Recognition} {With} {Trajectory}-{Pooled} {Deep}-{Convolutional} {Descriptors}},
  author    = {Wang, Limin and Qiao, Yu and Tang, Xiaoou},
  booktitle = {CVPR},
  year      = {2015}
}

@inproceedings{short_snippets,
  title     = {Beyond short snippets: Deep networks for video classification},
  author    = {Joe Yue-Hei and Matthew Hausknecht and Sudheendra Vijayanarasimhan and Oriol Vinyals and Rajat Monga and George Toderici},
  booktitle = {CVPR},
  year      = {2015}
}
@inproceedings{15,
  title     = {Regularizing LSTM with 3D Human-Skeleton Sequences for Action Recognition},
  author    = {Behrooz Mahasseni and Sinisa Todorovic},
  booktitle = {CVPR},
  year      = {2016}
}
@inproceedings{16,
  title     = {Articulated people detection and pose estimation:Reshaping the future},
  author    = {Leonid Pishchulin  and Arjun Jain and Mykhaylo Andriluka and Thorsten Thormahlen and Bernt Schiele},
  booktitle = {CVPR},
  year      = {2012}
}
@inproceedings{17,
  title     = {Using k-poselets for detecting people and localizing their keypoints},
  author    = {Georgia Gkioxari and Bharath Hariharan and Ross Girshick and Jitendra Malik},
  booktitle = {CVPR},
  year      = {2014}
}
@inproceedings{18,
  title     = {Articulated part-based model for joint object detection and pose estimation},
  author    = {Min Sun and Silvio Savarese},
  booktitle = {ICCV},
  year      = {2011}
}
@misc{19,
  author = {Umar Iqbal and Juergen Gall},
  title  = {Multi-Person Pose Estimation with Local Joint-to-Person Associations},
  year   = {2016},
  eprint = {arXiv:1608.08526}
}
@inproceedings{20,
  title     = {Deepcut: Joint subset partition and labeling for multi person pose estimation},
  author    = {Leonid Pishchulin and Eldar Insafutdinov and Siyu Tang and Bjoern Andres and Mykhaylo Andriluka and Peter Gehler and Bernt Schiele},
  booktitle = {CVPR},
  year      = {2016}
}
@article{21,
  title   = {Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields},
  author  = {Zhe Cao and Tomas Simon and Shih-En Wei and Yaser Sheikh},
  journal = {arXiv preprint arXiv:1611.08050},
  year    = {2016}
}
@inproceedings{22,
  title     = {P-CNN: Pose-based CNN Features for Action Recognition},
  author    = {Guilhem Cheron and Ivan Laptev and Cordelia Schmid},
  booktitle = {ICCV},
  year      = {2015}
}  
@inproceedings{23,
  title     = {Recognizing Actions from Depth Cameras as Weakly Aligned Multi-part Bag-of-Poses},
  author    = {Seidenari, L. and Varano, V. and Berretti, S. and Del Bimbo, A. and Pala, P.},
  booktitle = {CVPRW},
  year      = {2013}
}
@inproceedings{24,
  title     = {HON4D: Histogram of oriented 4D normals for activity recognition from depth sequences},
  author    = {Omar Oreifej and Zicheng Liu},
  booktitle = {CVPR},
  year      = {2013}
}
@article{25,
  title   = {Evaluating spatiotemporal interest point features for depth-based action recognition },
  author  = {Yu Zhu and Wenbin Chen and Guodong Guo},
  journal = {Image and Vision Computing },
  year    = {2014},
  number  = {8},
  pages   = {453 - 464},
  volume  = {32},
  doi     = {http://dx.doi.org/10.1016/j.imavis.2014.04.005},
  issn    = {0262-8856},
  url     = {http://www.sciencedirect.com/science/article/pii/S0262885614000651}
}
@inproceedings{26,
  title     = {Multi-modal feature fusion for action recognition in RGB-D sequences},
  author    = {Amir Shahroudy and Gang Wang and Tian-Tsong Ng},
  booktitle = {ISCCSP},
  year      = {2014}
}
@inproceedings{27,
  title     = {Spatio-temporal Depth Cuboid Similarity Feature for Activity Recognition Using Depth Camera},
  author    = {Lu Xia and Aggarwal, J.K.},
  booktitle = {CVPR},
  year      = {2013}
}
@inproceedings{29,
  title     = {Learning Discriminative Representations from RGB-D Video Data},
  author    = {Liu, Li and Shao, Ling},
  booktitle = {IJCAI},
  year      = {2013}
}
@article{35,
  title      = {Learning Human Activities and Object Affordances from RGB-D Videos},
  author     = {Koppula, Hema Swetha and Gupta, Rudhir and Saxena, Ashutosh},
  journal    = {Int. J. Rob. Res.},
  year       = {2013},
  month      = jul,
  number     = {8},
  pages      = {951--970},
  volume     = {32},
  acmid      = {2502928},
  address    = {Thousand Oaks, CA, USA},
  doi        = {10.1177/0278364913478446},
  issn       = {0278-3649},
  issue_date = {July 2013},
  keywords   = {3D perception, human activity detection, object affordance, personal robots, spatio-temporal context, supervised learning},
  numpages   = {20},
  publisher  = {Sage Publications, Inc.},
  url        = {http://dx.doi.org/10.1177/0278364913478446}
}
@inproceedings{36,
  title     = {Jointly learning heterogeneous features for {RGB}-{D} activity recognition},
  author    = {Hu Jian-Fang and Zheng Wei-Shi and Lai Jianhuang and Zhang JianGuo},
  booktitle = {CVPR},
  year      = {2015},
  doi       = {10.1109/CVPR.2015.7299172}
}
@inproceedings{33,
  title     = {Modeling Spatial Layout of Features for Real World Scenario RGB-D Action Recognition},
  author    = {Michal Koperski and Francois Bremond},
  booktitle = {AVSS},
  year      = {2016}
}
@inproceedings{38,
  title     = {High accuracy optical flow estimation based on a theory for warping},
  author    = {Thomas Brox and Andres Bruhn and Nils Papenberg and Joachim Weickert},
  booktitle = {ECCV},
  year      = {2004}
}
@inproceedings{39,
  title     = {Finding action tubes},
  author    = {Georgia Gkloxari and Jitendra Malik},
  booktitle = {CVPR},
  year      = {2015}
}
@inproceedings{40,
  title     = {Unstructured Human Activity Detection from RGBD Images},
  author    = {Jaeyong Sung and Colin Ponce and Bart Selman and Ashutosh Saxena},
  booktitle = {ICRA},
  year      = {2012}
}

@inproceedings{bilinski:hal-01054943,
  title       = {{Representing Visual Appearance by Video Brownian Covariance Descriptor for Human Action Recognition}},
  author      = {Bilinski, Piotr and Koperski, Michal and Bak, Slawomir and Bremond, Fran{\c c}ois},
  url         = {https://hal.inria.fr/hal-01054943},
  booktitle   = {{AVSS}},
  year        = {2014},
  keywords    = {computer visiosn ; action recognition},
  pdf         = {https://hal.inria.fr/hal-01054943/file/Bilinski-VideoBrownianCovariance-AVSS2014.pdf},
  hal_id      = {hal-01054943},
  hal_version = {v2}
}

@inproceedings{koperski:hal-01054949,
  title       = {{3D Trajectories for Action Recognition}},
  author      = {Koperski, Michal and Bilinski, Piotr and Bremond, Fran{\c c}ois},
  url         = {https://hal.inria.fr/hal-01054949},
  booktitle   = {{ICIP}},
  year        = {2014},
  keywords    = {computer vision ; action recognition},
  pdf         = {https://hal.inria.fr/hal-01054949/file/koperski-icip.pdf},
  hal_id      = {hal-01054949},
  hal_version = {v1}
}
@inproceedings{Srijan,
  title     = {Action Recognition based on a mixture of RGB and Depth based skeleton},
  author    = {Das, Srijan and Koperski, Michal and Bremond, Francois and Francesca, Gianpiero},
  booktitle = {AVSS},
  year      = {2017}
}
@inproceedings{41,
  title     = {A spatio-temporal descriptor based on ¨
               3d-gradients},
  author    = { Klaser, Alexander and Marszaek, Marcin  and  Schmid Cordelia},
  booktitle = {BMVC},
  year      = {2008}
}
@inproceedings{42,
  author    = {N. Dalal and B. Triggs},
  booktitle = {2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)},
  title     = {Histograms of oriented gradients for human detection},
  year      = {2005},
  volume    = {1},
  pages     = {886-893 vol. 1},
  keywords  = {feature extraction;gradient methods;object detection;object recognition;support vector machines;coarse spatial binning;contrast normalization;edge based descriptors;fine orientation binning;fine-scale gradients;gradient based descriptors;histograms of oriented gradients;human detection;linear SVM;overlapping descriptor;pedestrian database;robust visual object recognition;High performance computing;Histograms;Humans;Image databases;Image edge detection;Object detection;Object recognition;Robustness;Support vector machines;Testing},
  doi       = {10.1109/CVPR.2005.177},
  issn      = {1063-6919},
  month     = {June}
}
@inproceedings{43,
  author    = {I. Laptev and M. Marszalek and C. Schmid and B. Rozenfeld},
  booktitle = {2008 IEEE Conference on Computer Vision and Pattern Recognition},
  title     = {Learning realistic human actions from movies},
  year      = {2008},
  pages     = {1-8},
  keywords  = {cinematography;image classification;image retrieval;learning (artificial intelligence);support vector machines;video signal processing;automatic video annotation;human action retrieval;local space-time feature;movie script;multichannel nonlinear SVM;space-time pyramid;text-based classifier;video action classification;video realistic human action recognition;visual learning;Cameras;Clothing;Humans;Image recognition;Layout;Motion pictures;Object recognition;Robustness;Text categorization;Video sharing},
  doi       = {10.1109/CVPR.2008.4587756},
  issn      = {1063-6919},
  month     = {June}
}
@inproceedings{44,
  author    = {S. Zhang and X. Liu and J. Xiao},
  booktitle = {2017 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  title     = {On Geometric Features for Skeleton-Based Action Recognition Using Multilayer LSTM Networks},
  year      = {2017},
  pages     = {148-157},
  keywords  = {Computational modeling;Computer architecture;Logic gates;Neurons;Nonhomogeneous media;Skeleton;Three-dimensional displays},
  doi       = {10.1109/WACV.2017.24},
  month     = {March}
}
@inproceedings{45,
  author    = {Feichtenhofer, Christoph and Pinz, Axel and Zisserman, Andrew},
  title     = {Convolutional Two-Stream Network Fusion for Video Action Recognition},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  year      = {2016}
}
@article{MSRnew,
  author   = {A. Shahroudy and T. T. Ng and Y. Gong and G. Wang},
  journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title    = {Deep Multimodal Feature Analysis for Action Recognition in RGB+D Videos},
  year     = {2017},
  volume   = {PP},
  number   = {99},
  pages    = {1-1},
  keywords = {Correlation;Feature extraction;Robustness;Sensors;Skeleton;Three-dimensional displays;Videos;Action Recognition;Multimodal Analysis;RGB+D;Structured Sparsity},
  doi      = {10.1109/TPAMI.2017.2691321},
  issn     = {0162-8828},
  month    = {}
}
@article{WACVrejected,
  title   = {Deep-Temporal LSTM for Daily Living Action Recognition},
  author  = {Srijan Das and Michal Koperski and François Br{\'e}mond and Gianpiero Francesca},
  journal = {2018 15th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)},
  year    = {2018},
  pages   = {1-6}
}
@article{srijan_MM,
  title        = {A New Hybrid Architecture for Human Activity Recognition from RGB-D Videos},
  author       = {Srijan Das and Monique Thonnat and kaustubh Sakhalkar and Michal Koperski and François Br{\'e}mond and Gianpiero Francesca},
  journal      = {MultiMedia Modeling. MMM 2019},
  year         = {2019},
  organization = {Springer}
}
@article{mutli-modal_harder,
  author        = {Weiyao Wang and
                   Du Tran and
                   Matt Feiszli},
  title         = {What Makes Training Multi-Modal Networks Hard?},
  journal       = {CoRR},
  volume        = {abs/1905.12681},
  year          = {2019},
  url           = {http://arxiv.org/abs/1905.12681},
  archiveprefix = {arXiv},
  eprint        = {1905.12681},
  timestamp     = {Mon, 03 Jun 2019 13:42:33 +0200},
  biburl        = {https://dblp.org/rec/journals/corr/abs-1905-12681.bib},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{HOF,
  title        = {Learning realistic human actions from movies},
  author       = {Laptev, Ivan and Marszalek, Marcin and Schmid, Cordelia and Rozenfeld, Benjamin},
  booktitle    = {Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on},
  pages        = {1--8},
  year         = {2008},
  organization = {IEEE}
}

@inproceedings{MBH,
  title        = {Human detection using oriented histograms of flow and appearance},
  author       = {Dalal, Navneet and Triggs, Bill and Schmid, Cordelia},
  booktitle    = {European conference on computer vision},
  pages        = {428--441},
  year         = {2006},
  organization = {Springer}
}

@inproceedings{HOG3D,
  title        = {A spatio-temporal descriptor based on 3d-gradients},
  author       = {Klaser, Alexander and Marsza{\l}ek, Marcin and Schmid, Cordelia},
  booktitle    = {BMVC 2008-19th British Machine Vision Conference},
  pages        = {275--1},
  year         = {2008},
  organization = {British Machine Vision Association}
}

@inproceedings{gridhog,
  title        = {Modeling spatial layout of features for real world scenario rgb-d action recognition},
  author       = {Koperski, Michal and Bremond, Francois},
  booktitle    = {Advanced Video and Signal Based Surveillance (AVSS), 2016 13th IEEE International Conference on},
  pages        = {44--50},
  year         = {2016},
  organization = {IEEE}
}

@inproceedings{transferring,
  title        = {Learning and transferring mid-level image representations using convolutional neural networks},
  author       = {Oquab, Maxime and Bottou, Leon and Laptev, Ivan and Sivic, Josef},
  booktitle    = {Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on},
  pages        = {1717--1724},
  year         = {2014},
  organization = {IEEE}
}

@inproceedings{IDT,
  author    = {Heng Wang and Cordelia Schmid},
  title     = {Action Recognition with Improved Trajectories},
  booktitle = {IEEE International Conference on Computer Vision},
  year      = {2013},
  address   = {Sydney, Australia},
  url       = {http://hal.inria.fr/hal-00873267}
}

@inproceedings{DT,
  author    = {Heng Wang and Alexander Kl{\"a}ser and Cordelia Schmid and Cheng-Lin Liu},
  title     = {{Action Recognition by Dense Trajectories}},
  booktitle = {IEEE Conference on Computer Vision \& Pattern Recognition},
  year      = {2011},
  month     = Jun,
  pages     = {3169-3176},
  address   = {Colorado Springs, United States},
  url       = {http://hal.inria.fr/inria-00583818/en}
}

@inproceedings{fischer,
  title        = {Improving the fisher kernel for large-scale image classification},
  author       = {Perronnin, Florent and S{\'a}nchez, Jorge and Mensink, Thomas},
  booktitle    = {European conference on computer vision},
  pages        = {143--156},
  year         = {2010},
  organization = {Springer}
}

@inproceedings{tslstm,
  title     = {Ensemble deep learning for skeleton-based action recognition using temporal sliding LSTM networks},
  author    = {Lee, Inwoong and Kim, Doyoung and Kang, Seoungyoon and Lee, Sanghoon},
  year      = {2017},
  booktitle = {Proceedings of the IEEE International Conference on Computer Vision}
}

@inproceedings{chainedcnn,
  title        = {Chained multi-stream networks exploiting pose, motion, and appearance for action classification and detection},
  author       = {Zolfaghari, Mohammadreza and Oliveira, Gabriel L and Sedaghat, Nima and Brox, Thomas},
  booktitle    = {Computer Vision (ICCV), 2017 IEEE International Conference on},
  pages        = {2923--2932},
  year         = {2017},
  organization = {IEEE}
}

@inproceedings{pseudo3d,
  title        = {Learning spatio-temporal representation with pseudo-3d residual networks},
  author       = {Qiu, Zhaofan and Yao, Ting and Mei, Tao},
  booktitle    = {2017 IEEE International Conference on Computer Vision (ICCV)},
  pages        = {5534--5542},
  year         = {2017},
  organization = {IEEE}
}
@inproceedings{Salient,
  author    = {L. Rybok and B. Schauerte and Z. Al-Halah and R. Stiefelhagen},
  booktitle = {IEEE Winter Conference on Applications of Computer Vision},
  title     = { Important stuff, everywhere! Activity recognition with salient proto-objects as context},
  year      = {2014},
  volume    = {},
  number    = {},
  pages     = {646-651},
  keywords  = {object detection;object recognition;action recognition framework;activity recognition;object detectors;object information;object-action relationships;saliency detection;salient proto-objects;training data;Accuracy;Context;Detectors;Image segmentation;Microwave theory and techniques;Training data;Videos},
  doi       = {10.1109/WACV.2014.6836041},
  issn      = {1550-5790},
  month     = {March}
}

@article{Lin2016,
  author   = {Lin, Liang
              and Wang, Keze
              and Zuo, Wangmeng
              and Wang, Meng
              and Luo, Jiebo
              and Zhang, Lei},
  title    = {A Deep Structured Model with Radius--Margin Bound for 3D Human Activity Recognition},
  journal  = {International Journal of Computer Vision},
  year     = {2016},
  month    = {Jun},
  day      = {01},
  volume   = {118},
  number   = {2},
  pages    = {256--273},
  abstract = {Understanding human activity is very challenging even with the recently developed 3D/depth sensors. To solve this problem, this work investigates a novel deep structured model, which adaptively decomposes an activity instance into temporal parts using the convolutional neural networks. Our model advances the traditional deep learning approaches in two aspects. First, we incorporate latent temporal structure into the deep model, accounting for large temporal variations of diverse human activities. In particular, we utilize the latent variables to decompose the input activity into a number of temporally segmented sub-activities, and accordingly feed them into the parts (i.e. sub-networks) of the deep architecture. Second, we incorporate a radius--margin bound as a regularization term into our deep model, which effectively improves the generalization performance for classification. For model training, we propose a principled learning algorithm that iteratively (i) discovers the optimal latent variables (i.e. the ways of activity decomposition) for all training instances, (ii) updates the classifiers based on the generated features, and (iii) updates the parameters of multi-layer neural networks. In the experiments, our approach is validated on several complex scenarios for human activity recognition and demonstrates superior performances over other state-of-the-art approaches.},
  issn     = {1573-1405},
  doi      = {10.1007/s11263-015-0876-z},
  url      = {https://doi.org/10.1007/s11263-015-0876-z}
}
@inproceedings{STS,
  author    = {Koppula, Hema S. and Saxena, Ashutosh},
  title     = {Learning Spatio-temporal Structure from RGB-D Videos for Human Activity Detection and Anticipation},
  booktitle = {Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28},
  series    = {ICML'13},
  year      = {2013},
  location  = {Atlanta, GA, USA},
  pages     = {III-792--III-800},
  url       = {http://dl.acm.org/citation.cfm?id=3042817.3043025},
  acmid     = {3043025},
  publisher = {JMLR.org}
} 

@inproceedings{lstm3d,
  title     = {Regularizing long short term memory with 3D human-skeleton sequences for action recognition},
  author    = {Mahasseni, Behrooz and Todorovic, Sinisa},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages     = {3054--3062},
  year      = {2016}
}

@inproceedings{R-HCRF,
  author    = {T. Liu and X. Wang and X. Dai and J. Luo},
  booktitle = {2016 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  title     = {Deep recursive and hierarchical conditional random fields for human action recognition},
  year      = {2016},
  volume    = {},
  number    = {},
  pages     = {1-9},
  keywords  = {approximation theory;image motion analysis;image recognition;image representation;statistical analysis;support vector machines;CAD-120 benchmark dataset;DR-HCRFs model;block-coordinate primal-dual Frank-Wolfe algorithm;complex action recognition scenario modelling;deep recursive and hierarchical conditional random field model;discriminative models;human action recognition;infinite-order temporal-dependencies;linear-chain CRFs;mean-field-like approximation;model inference;model marginal likelihood approximation;support vector machine framework;temporal sequential labeling;Computational modeling;Context;Context modeling;Graphical models;Hidden Markov models;Inference algorithms;Target recognition},
  doi       = {10.1109/WACV.2016.7477694},
  issn      = {},
  month     = {March}
}

@article{posebased,
  title   = {A hierarchical pose-based approach to complex action understanding using dictionaries of actionlets and motion poselets},
  author  = {Lillo, Ivan and Niebles, Juan Carlos and Soto, Alvaro},
  journal = {arXiv preprint arXiv:1606.04992},
  year    = {2016}
}
@article{liu2017enhanced,
  title     = {Enhanced skeleton visualization for view invariant human action recognition},
  author    = {Liu, Mengyuan and Liu, Hong and Chen, Chen},
  journal   = {Pattern Recognition},
  volume    = {68},
  pages     = {346--362},
  year      = {2017},
  publisher = {Elsevier}
}
@inproceedings{hog2,
  author    = {E. Ohn-Bar and M. M. Trivedi},
  booktitle = {2013 IEEE Conference on Computer Vision and Pattern Recognition Workshops},
  title     = {Joint Angles Similarities and HOG2 for Action Recognition},
  year      = {2013},
  volume    = {},
  number    = {},
  pages     = {465-470},
  doi       = {10.1109/CVPRW.2013.76},
  issn      = {2160-7508},
  month     = {June}
}

@inproceedings{st-lstm,
  author    = {Liu, Jun
               and Shahroudy, Amir
               and Xu, Dong
               and Wang, Gang},
  editor    = {Leibe, Bastian
               and Matas, Jiri
               and Sebe, Nicu
               and Welling, Max},
  title     = {Spatio-Temporal LSTM with Trust Gates for 3D Human Action Recognition},
  booktitle = {Computer Vision -- ECCV 2016},
  year      = {2016},
  publisher = {Springer International Publishing},
  address   = {Cham},
  pages     = {816--833},
  isbn      = {978-3-319-46487-9}
}

@article{ftp-ds,
  author  = {Jian-Fang Hu and Wei-Shi Zheng and Jianhuang Lai and Jianguo Zhang},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title   = {Jointly Learning Heterogeneous Features for RGB-D Activity Recognition},
  year    = {2017},
  volume  = {39},
  number  = {11},
  pages   = {2186-2200},
  doi     = {10.1109/TPAMI.2016.2640292},
  issn    = {0162-8828},
  month   = {Nov}
}

@article{luo2023valley,
  title={Valley: Video assistant with large language model enhanced ability},
  author={Luo, Ruipu and Zhao, Ziwang and Yang, Min and Dong, Junwei and Qiu, Minghui and Lu, Pengcheng and Wang, Tao and Wei, Zhongyu},
  journal={arXiv preprint arXiv:2306.07207},
  year={2023}
}

@inproceeding{sst1,
  author    = {Lee, Hsin-Ying and Huang, Jia-Bin and Singh, Maneesh Kumar and Yang, Ming-Hsuan},
  title     = {Unsupervised Representation Learning by Sorting Sequence},
  booktitle = {IEEE International Conference on Computer Vision},
  year      = {2017}
}

@inproceedings{sst2,
  author    = {Fernando, Basura and Bilen, Hakan and Gavves, Efstratios and Gould, Stephen},
  title     = {Self-Supervised Video Representation Learning With Odd-One-Out Networks},
  booktitle = {IEEE Conference on Computer Vision and Pattern Recognition},
  year      = {2017}
}

@inproceedings{sst3,
  author    = {Pickup, Lyndsey C. and Pan, Zheng and Wei, Donglai and Shih, YiChang and Zhang, Changshui and Zisserman, Andrew and Scholkopf, Bernhard and Freeman, William T.},
  booktitle = {2014 IEEE Conference on Computer Vision and Pattern Recognition},
  title     = {Seeing the Arrow of Time},
  year      = {2014},
  volume    = {},
  number    = {},
  pages     = {2043-2050},
  doi       = {10.1109/CVPR.2014.262}
}
@inproceedings{shufflelearn,
  title     = {{Shuffle and Learn: Unsupervised Learning using Temporal Order Verification}},
  author    = {Misra, Ishan and Zitnick, C. Lawrence and Hebert, Martial},
  booktitle = {ECCV},
  year      = {2016}
}
@inproceedings{infonce,
  title     = {Noise-contrastive estimation: A new estimation principle for unnormalized statistical models},
  author    = {Gutmann, Michael and Hyvärinen, Aapo},
  booktitle = {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  pages     = {297--304},
  year      = {2010},
  editor    = {Teh, Yee Whye and Titterington, Mike},
  volume    = {9},
  series    = {Proceedings of Machine Learning Research},
  address   = {Chia Laguna Resort, Sardinia, Italy},
  month     = {13--15 May},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf},
  url       = {https://proceedings.mlr.press/v9/gutmann10a.html}
}

@inproceedings{Hu_2017_CVPR,
  author    = {Hu, Peiyun and Ramanan, Deva},
  title     = {Finding Tiny Faces},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {July},
  year      = {2017}
}
@inproceedings{DSMIL,
  title     = {Dual-stream multiple instance learning network for whole slide image classification with self-supervised contrastive learning},
  author    = {Li, Bin and Li, Yin and Eliceiri, Kevin W},
  booktitle = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages     = {14318--14328},
  year      = {2021}
}
@inproceedings{hipt,
  title     = {Scaling Vision Transformers to Gigapixel Images via Hierarchical Self-Supervised Learning},
  author    = {Chen, Richard J and Chen, Chengkuan and Li, Yicong and Chen, Tiffany Y and Trister, Andrew D and Krishnan, Rahul G and Mahmood, Faisal},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages     = {16144--16155},
  year      = {2022}
}
@article{scorenet,
  title   = {Scorenet: Learning non-uniform attention and augmentation for transformer-based histopathological image classification},
  author  = {Stegm{\"u}ller, Thomas and Spahr, Antoine and Bozorgtabar, Behzad and Thiran, Jean-Philippe},
  journal = {arXiv preprint arXiv:2202.07570},
  year    = {2022}
}
@inproceedings{transpath,
  title        = {Transpath: Transformer-based self-supervised learning for histopathological image classification},
  author       = {Wang, Xiyue and Yang, Sen and Zhang, Jun and Wang, Minghui and Zhang, Jing and Huang, Junzhou and Yang, Wei and Han, Xiao},
  booktitle    = {International Conference on Medical Image Computing and Computer-Assisted Intervention},
  pages        = {186--195},
  year         = {2021},
  organization = {Springer}
}
@article{chen2022self,
  title   = {Self-supervised vision transformers learn visual concepts in histopathology},
  author  = {Chen, Richard J and Krishnan, Rahul G},
  journal = {arXiv preprint arXiv:2203.00585},
  year    = {2022}
}
 @article{laskin_srinivas2020curl,
  title   = {CURL: Contrastive Unsupervised Representations for Reinforcement Learning},
  author  = {Laskin, Michael and Srinivas, Aravind and Abbeel, Pieter},
  note    = {arXiv:2004.04136},
  journal = {Proceedings of the 37th International Conference on Machine 
             Learning, Vienna, Austria, PMLR 119},
  year    = {2020}
}       
@inproceedings{RGBD-HuDaAct,
  author    = {Bingbing Ni and Gang Wang and Pierre Moulin},
  title     = {RGBD-HuDaAct: A color-depth video database for human daily activity recognition},
  booktitle = {2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops)},
  month     = {Nov},
  year      = {2011}
}
@inproceedings{UWAmultiview2,
  author    = {Hossein Rahmani and Arif Mahmood and Du Huynh and Ajmal Mian},
  title     = {Histogram of oriented principal components for cross-view action
               recognition},
  booktitle = {TPAMI},
  year      = {2016}
}

@inproceedings{office_activity,
  author    = {Limin Wang and Yu Qiao and Xiaoou Tang},
  title     = {Action recognition and detection
               by combining motion and appearance features},
  booktitle = {THUMOS},
  year      = {2014}
}
@inproceedings{DML-smartactions,
  author    = {Amiri, S. Mohsen and Pourazad, Mahsa T. and Nasiopoulos, Panos and Leung, Victor C.M.},
  booktitle = {2013 IEEE 15th International Conference on e-Health Networking, Applications and Services (Healthcom 2013)},
  title     = {Non-intrusive human activity monitoring in a smart home environment},
  year      = {2013},
  volume    = {},
  number    = {},
  pages     = {606-610},
  doi       = {10.1109/HealthCom.2013.6720748}
}
@inproceedings{ACT4,
  author    = {Zhongwei Cheng and Lei Qin and Yituo Ye and Qingming Huang and Qi Tian},
  title     = {Human Daily Action Analysis with Multi-view and Color-Depth Data},
  booktitle = {European Conference on Computer Vision(ECCV)},
  year      = {2012}
}
@article{pascal,
  author     = {Everingham, Mark and Gool, Luc and Williams, Christopher K. and Winn, John and Zisserman, Andrew},
  title      = {The Pascal Visual Object Classes (VOC) Challenge},
  journal    = {Int. J. Comput. Vision},
  issue_date = {June      2010},
  volume     = {88},
  number     = {2},
  month      = jun,
  year       = {2010},
  issn       = {0920-5691},
  pages      = {303--338},
  numpages   = {36},
  url        = {http://dx.doi.org/10.1007/s11263-009-0275-4},
  doi        = {10.1007/s11263-009-0275-4},
  acmid      = {1747104},
  publisher  = {Kluwer Academic Publishers},
  address    = {Hingham, MA, USA},
  keywords   = {Benchmark, Database, Object detection, Object recognition}
} 
@inproceedings{coco,
  title     = {Microsoft COCO: Common Objects in Context},
  author    = {Tsung-Yi Lin and Michael Maire and Serge J. Belongie and Lubomir D. Bourdev and Ross B. Girshick and James Hays and Pietro Perona and Deva Ramanan and Piotr Doll{\'a}r and C. Lawrence Zitnick},
  booktitle = {ECCV},
  year      = {2014}
}
@article{neuro_image,
  title   = {Dissociating temporal attention from spatial attention and motor response preparation: A high-density EEG study},
  journal = {NeuroImage},
  volume  = {124},
  pages   = {947 - 957},
  year    = {2016},
  issn    = {1053-8119},
  doi     = {https://doi.org/10.1016/j.neuroimage.2015.09.051},
  url     = {http://www.sciencedirect.com/science/article/pii/S1053811915008733},
  author  = {Frédéric Faugeras and Lionel Naccache}
}
@article{orientation_in_time,
  author  = { Nobre, A. C. and  Miniussi, C. and  Wilding, E. L. and  Coull, J. T.},
  title   = {{Orienting attention in time: Modulation of brain potentials}},
  journal = {Brain},
  volume  = {122},
  number  = {8},
  pages   = {1507-1518},
  year    = {1999},
  month   = {08},
  issn    = {0006-8950},
  doi     = {10.1093/brain/122.8.1507},
  url     = {https://dx.doi.org/10.1093/brain/122.8.1507}
}
@article{tcga_luad,
  title   = {Radiology Data from The Cancer Genome Atlas Lung Adenocarcinoma [TCGA-LUAD] collection.},
  author  = {Albertina, B. and others},
  journal = {The Cancer Imaging Archive},
  year    = {2016}
}
@article{tcga_lusc,
  title   = {Radiology Data from The Cancer Genome Atlas Lung Squamous Cell Carcinoma [TCGA-LUSC] collection.},
  author  = {Kirk, S. and others},
  journal = {The Cancer Imaging Archive},
  year    = {2016}
}
@inproceedings{KrauseStarkDengFei-Fei_standford,
  title     = {3D Object Representations for Fine-Grained Categorization},
  booktitle = {4th International IEEE Workshop on  3D Representation and Recognition (3dRR-13)},
  year      = {2013},
  address   = {Sydney, Australia},
  author    = {Jonathan Krause and Michael Stark and Jia Deng and Li Fei-Fei}
}
@inproceedings{medmnistv1,
  title     = {MedMNIST Classification Decathlon: A Lightweight AutoML Benchmark for Medical Image Analysis},
  author    = {Yang, Jiancheng and Shi, Rui and Ni, Bingbing},
  booktitle = {IEEE 18th International Symposium on Biomedical Imaging (ISBI)},
  pages     = {191--195},
  year      = {2021}
}

@inproceedings{bert,
  author    = {Jacob Devlin and
               Ming{-}Wei Chang and
               Kenton Lee and
               Kristina Toutanova},
  editor    = {Jill Burstein and
               Christy Doran and
               Thamar Solorio},
  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
               Understanding},
  booktitle = {Proceedings of the 2019 Conference of the North American Chapter of
               the Association for Computational Linguistics: Human Language Technologies,
               {NAACL-HLT} 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long
               and Short Papers)},
  pages     = {4171--4186},
  publisher = {Association for Computational Linguistics},
  year      = {2019},
  url       = {https://doi.org/10.18653/v1/n19-1423},
  doi       = {10.18653/v1/n19-1423},
  timestamp = {Wed, 16 Mar 2022 23:55:36 +0100},
  biburl    = {https://dblp.org/rec/conf/naacl/DevlinCLT19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{videomae,
  title   = {VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training},
  author  = {Tong, Zhan and Song, Yibing and Wang, Jue and Wang, Limin},
  journal = {arXiv preprint arXiv:2203.12602},
  year    = {2022}
}
@article{cifar,
  title     = {Learning Multiple Layers of Features from Tiny Images},
  author    = {Krizhevsky, Alex and Hinton, Geoffrey and others},
  year      = {2009},
  publisher = {Citeseer}
}

@inproceedings{MicrosoftKinuct,
  author    = {Zhengyou Zhang},
  title     = {Microsoft Kinect Sensor and Its Effect},
  booktitle = {IEEE MultiMedia},
  volume    = {19},
  issue     = {2},
  month     = {April},
  year      = {2012}
}
@article{nonlocal,
  title   = {Non-local Neural Networks},
  author  = {Xiaolong Wang and Ross B. Girshick and Abhinav Gupta and Kaiming He},
  journal = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year    = {2018},
  pages   = {7794-7803}
}
@inproceedings{liu2017pku,
  author    = {Liu, Chunhui and Hu, Yueyu and Li, Yanghao and Song, Sijie and Liu, Jiaying},
  title     = {PKU-MMD: A Large Scale Benchmark for Skeleton-Based Human Action Understanding},
  booktitle = {Proceedings of the Workshop on Visual Analysis in Smart and Connected Communities},
  series    = {VSCC '17},
  year      = {2017},
  isbn      = {978-1-4503-5506-3},
  location  = {Mountain View, California, USA},
  pages     = {1--8},
  numpages  = {8},
  url       = {http://doi.acm.org/10.1145/3132734.3132739},
  doi       = {10.1145/3132734.3132739},
  acmid     = {3132739},
  publisher = {ACM},
  address   = {New York, NY, USA},
  keywords  = {action detection, skeleton-based action understanding, video analysis, video benchmark}
} 
@article{mpii-cooking2,
  year      = {2015},
  journal   = {International Journal of Computer Vision},
  title     = {Recognizing Fine-Grained and Composite Activities Using Hand-Centric Features and Script Data},
  author    = {Rohrbach, Marcus and Rohrbach, Anna and Regneri, Michaela and Amin, Sikandar and Andriluka, Mykhaylo and Pinkal, Manfred and Schiele, Bernt},
  issn      = {0920-5691},
  doi       = {10.1007/s11263-015-0851-8},
  url       = {http://dx.doi.org/10.1007/s11263-015-0851-8},
  publisher = {Springer US},
  pages     = {1-28}
} 


@misc{VIRAT,
  howpublished = {\url{http://www.viratdata.org/}},
  note         = {Accessed Feb. 28th, 2019},
  title        = {VIRAT Video dataset},
  author       = {DARPA and Kitware}
}
@inproceedings{dahlia,
  title        = {The daily home life activity dataset: a high semantic activity dataset for online recognition},
  author       = {Vaquette, Geoffrey and Orcesi, Astrid and Lucat, Laurent and Achard, Catherine},
  booktitle    = {2017 12th IEEE International Conference on Automatic Face \& Gesture Recognition (FG 2017)},
  pages        = {497--504},
  year         = {2017},
  organization = {IEEE}
}
@inproceedings{svw,
  title        = {Sports videos in the wild (SVW): A video dataset for sports analysis},
  author       = {Safdarnejad, Seyed Morteza and Liu, Xiaoming and Udpa, Lalita and Andrus, Brooks and Wood, John and Craven, Dean},
  booktitle    = {2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)},
  volume       = {1},
  pages        = {1--7},
  year         = {2015},
  organization = {IEEE}
}

@article{8016630,
  author   = {Zhen Qin and Christian R. Shelton},
  journal  = {IEEE Transactions on Image Processing},
  title    = {Event Detection in Continuous Video: An Inference in Point Process Approach},
  year     = {2017},
  volume   = {26},
  number   = {12},
  pages    = {5680-5691},
  keywords = {image segmentation;image sequences;inference mechanisms;piecewise constant techniques;video signal processing;real-world continuous video sequences;arbitrary-order nonMarkovian dependences;local visual ambiguities mitigation;event segmentation;event labeling;time-window free method;event stream;high-level semantic events;low-level video observations;point process model;piecewise-constant conditional intensity model;PCIM;low-level image observations;inference algorithm;video event detection;Streaming media;Event detection;Visualization;Inference algorithms;Labeling;Semantics;Image segmentation;Video event detection;event segmentation and labeling;video understanding;dependency modeling;video grammar;point process},
  doi      = {10.1109/TIP.2017.2745209},
  issn     = {},
  month    = {Dec}
}

@inproceedings{8026278,
  author    = { {Hongsong Wang} and {Liang Wang}},
  booktitle = {2017 IEEE International Conference on Multimedia Expo Workshops (ICMEW)},
  title     = {Learning robust representations using recurrent neural networks for skeleton based action classification and detection},
  year      = {2017},
  volume    = {},
  number    = {},
  pages     = {591-596},
  keywords  = {estimation theory;image classification;image colour analysis;learning (artificial intelligence);object detection;real-time systems;recurrent neural nets;learning;robust representations;recurrent neural networks;skeleton based action classification;skeleton based action detection;real-time skeleton estimation algorithms;RNN;temporal evolution;end-to-end architecture;starting point transformation layer;viewpoint transformation layer;spatial dropout layer;NTU RGB+D;PKU-MMD;Skeleton;Neural networks;Videos;Ions;Skeletons;deep RNN;starting points;viewpoints;spatial dropout},
  doi       = {10.1109/ICMEW.2017.8026278},
  issn      = {},
  month     = {July}
}

@inproceedings{Luo_2018_ECCV,
  author    = {Luo, Zelun and Hsieh, Jun-Ting and Jiang, Lu and Carlos Niebles, Juan and Fei-Fei, Li},
  title     = {Graph Distillation for Action Detection with Privileged Modalities},
  booktitle = {The European Conference on Computer Vision (ECCV)},
  month     = {September},
  year      = {2018}
} 

@inproceedings{collaborative,
  author    = {Guo, Qiushan and Wang, Xinjiang and Wu, Yichao and Yu, Zhipeng and Liang, Ding and Hu, Xiaolin and Luo, Ping},
  title     = {Online Knowledge Distillation via Collaborative Learning},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  year      = {2020}
}

@article{Alpher03,
  author  = {FirstName Alpher and  FirstName Fotheringham-Smythe},
  title   = {Frobnication revisited},
  journal = {Journal of Foo},
  volume  = 13,
  number  = 1,
  pages   = {234--778},
  year    = 2003
}

@article{Alpher04,
  author  = {FirstName Alpher and FirstName Fotheringham-Smythe and FirstName Gamow},
  title   = {Can a machine frobnicate?},
  journal = {Journal of Foo},
  volume  = 14,
  number  = 1,
  pages   = {234--778},
  year    = 2004
}

@inproceedings{Alpher05,
  author    = {FirstName Alpher and FirstName Gamow},
  title     = {Can a computer frobnicate?},
  booktitle = CVPR,
  pages     = {234--778},
  year      = 2005
}

@inproceedings{resnet-50,
  author    = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Deep Residual Learning for Image Recognition},
  year      = {2016},
  volume    = {},
  number    = {},
  pages     = {770-778},
  doi       = {10.1109/CVPR.2016.90}
}


@inproceedings{wangvrl3,
  title     = {VRL3: A Data-Driven Framework for Visual Deep Reinforcement Learning},
  author    = {Wang, Che and Luo, Xufang and Ross, Keith W and Li, Dongsheng},
  booktitle = NIPS,
  year      = 2022
}

@inproceedings{laskin2020curl,
  title        = {Curl: Contrastive unsupervised representations for reinforcement learning},
  author       = {Laskin, Michael and Srinivas, Aravind and Abbeel, Pieter},
  booktitle    = ICML,
  pages        = {5639--5650},
  year         = {2020},
  organization = {PMLR}
}

@inproceedings{yarats2021improving,
  title     = {Improving sample efficiency in model-free reinforcement learning from images},
  author    = {Yarats, Denis and Zhang, Amy and Kostrikov, Ilya and Amos, Brandon and Pineau, Joelle and Fergus, Rob},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume    = {35},
  number    = {12},
  pages     = {10674--10681},
  year      = {2021}
}

@inproceedings{schwarzerdata,
  title     = {Data-Efficient Reinforcement Learning with Self-Predictive Representations},
  author    = {Schwarzer, Max and Anand, Ankesh and Goel, Rishab and Hjelm, R Devon and Courville, Aaron and Bachman, Philip},
  booktitle = {International Conference on Learning Representations},
  year      = {2021}
}

@inproceedings{yumask,
  title     = {Mask-based Latent Reconstruction for Reinforcement Learning},
  author    = {Yu, Tao and Zhang, Zhizheng and Lan, Cuiling and Lu, Yan and Chen, Zhibo},
  booktitle = NIPS,
  year      = {2022}
}

@inproceedings{zhan2020framework,
  title     = {A framework for efficient robotic manipulation},
  author    = {Zhan, Albert and Zhao, Ruihan and Pinto, Lerrel and Abbeel, Pieter and Laskin, Michael},
  booktitle = {Deep RL Workshop NeurIPS 2021},
  year      = {2020}
}

@article{Radosavovic2022,
  title   = {Real-World Robot Learning with Masked Visual Pre-training},
  author  = {Ilija Radosavovic and Tete Xiao and Stephen James and Pieter Abbeel and Jitendra Malik and Trevor Darrell},
  year    = {2022},
  journal = {CoRL}
}

@inproceedings{lidoes,
  title     = {Does Self-supervised Learning Really Improve Reinforcement Learning from Pixels?},
  author    = {Li, Xiang and Shang, Jinghuan and Das, Srijan and Ryoo, Michael S},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = 2022
}


@inproceedings{svhn,
  title     = {Reading Digits in Natural Images with Unsupervised Feature Learning},
  author    = {Yuval Netzer and Tao Wang and Adam Coates and Alessandro Bissacco and Bo Wu and Andrew Y. Ng},
  year      = {2011},
  booktitle = {NIPS Workshop on Deep Learning and Unsupervised Feature Learning 2011}
}

@inproceedings{ranasinghe2022self,
  title     = {Self-supervised video transformer},
  author    = {Ranasinghe, Kanchana and Naseer, Muzammal and Khan, Salman and Khan, Fahad Shahbaz and Ryoo, Michael S},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages     = {2874--2884},
  year      = {2022}
}

@article{Selva2022video_transformer_survey,
  title   = {Video Transformers: A Survey},
  author  = {Javier Selva and Anders S. Johansen and Sergio Escalera and Kamal Nasrollahi and Thomas Baltzer Moeslund and Albert Clap'es},
  journal = {IEEE transactions on pattern analysis and machine intelligence},
  year    = {2022},
  volume  = {PP}
}

@inproceedings{szegedy2016inception,
  author    = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
  booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Rethinking the Inception Architecture for Computer Vision},
  year      = {2016},
  volume    = {},
  number    = {},
  pages     = {2818-2826},
  doi       = {10.1109/CVPR.2016.308}
}

@misc{carion2020detr,
  title         = {End-to-End Object Detection with Transformers},
  author        = {Nicolas Carion and Francisco Massa and Gabriel Synnaeve and Nicolas Usunier and Alexander Kirillov and Sergey Zagoruyko},
  year          = {2020},
  eprint        = {2005.12872},
  archiveprefix = {ECCV},
  primaryclass  = {cs.CV}
}
@article{strudel2021segmenter,
  title   = {Segmenter: Transformer for Semantic Segmentation},
  author  = {Robin Strudel and Ricardo Garcia Pinel and Ivan Laptev and Cordelia Schmid},
  journal = {2021 IEEE/CVF International Conference on Computer Vision (ICCV)},
  year    = {2021},
  pages   = {7242-7252}
}

@inproceedings{hachiuma2023unifiedskele,
  title     = {Unified keypoint-based action recognition framework via structured keypoint pooling},
  author    = {Hachiuma, Ryo and Sato, Fumiaki and Sekii, Taiki},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages     = {22962--22971},
  year      = {2023}
}

@inproceedings{Baradel2018HumanAR,
  title     = {Human Activity Recognition with Pose-driven Attention to RGB},
  author    = {Fabien Baradel and Christian Wolf and Julien Mille},
  booktitle = {British Machine Vision Conference},
  year      = {2018}
}

@inproceedings{Vyas2020MultiviewAR,
  title     = {Multi-view Action Recognition Using Cross-View Video Prediction},
  author    = {Shruti Vyas and Yogesh Singh Rawat and Mubarak Shah},
  booktitle = {European Conference on Computer Vision},
  year      = {2020},
  url       = {https://api.semanticscholar.org/CorpusID:220827170}
}

@inproceedings{PoseC3D_CVPR22,
  author    = {Duan, Haodong and Zhao, Yue and Chen, Kai and Lin, Dahua and Dai, Bo},
  booktitle = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Revisiting Skeleton-based Action Recognition},
  year      = {2022},
  volume    = {},
  number    = {},
  pages     = {2959-2968},
  doi       = {10.1109/CVPR52688.2022.00298}
}


@article{zhou2022hyperformer,
  title   = {Hypergraph Transformer for Skeleton-based Action Recognition},
  author  = {Zhou, Yuxuan and Cheng, Zhi-Qi and Li, Chao and Geng, Yifeng and Xie, Xuansong and Keuper, Margret},
  journal = {arXiv preprint arXiv:2211.09590},
  year    = {2022}
}

@article{Chen2021_CTRGCN_CVPR21,
  title   = {Channel-wise Topology Refinement Graph Convolution for Skeleton-Based Action Recognition},
  author  = {Yuxin Chen and Ziqi Zhang and Chunfen Yuan and Bing Li and Ying Deng and Weiming Hu},
  journal = {2021 IEEE/CVF International Conference on Computer Vision (ICCV)},
  year    = {2021},
  pages   = {13339-13348},
  url     = {https://api.semanticscholar.org/CorpusID:236428765}
}

@inproceedings{Hyung-Gun_InfoGCN_CVPR22,
  author    = {Chi, Hyung-Gun and Ha, Myoung Hoon and Chi, Seunggeun and Lee, Sang Wan and Huang, Qixing and Ramani, Karthik},
  booktitle = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {InfoGCN: Representation Learning for Human Skeleton-based Action Recognition},
  year      = {2022},
  volume    = {},
  number    = {},
  pages     = {20154-20164},
  doi       = {10.1109/CVPR52688.2022.01955}
}

@inproceedings{Ketul_ContrastiveMultiview_WACV23,
  author    = {Shah, Ketul and Shah, Anshul and Lau, Chun Pong and de Melo, Celso M. and Chellapp, Rama},
  booktitle = {2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
  title     = {Multi-View Action Recognition using Contrastive Learning},
  year      = {2023},
  volume    = {},
  number    = {},
  pages     = {3370-3380},
  doi       = {10.1109/WACV56688.2023.00338}
}


@article{Kim_3DDeformableAttnForActionRec_ICCV23,
  title   = {Cross-Modal Learning with 3D Deformable Attention for Action Recognition},
  author  = {Kim, Sangwon and Ahn, Dasom and Ko, Byoungchul},
  journal = {2023 IEEE/CVF International Conference on Computer Vision (ICCV)},
  year    = {2023}
}

@misc{layernormalization_2016,
  title         = {Layer Normalization},
  author        = {Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton},
  year          = {2016},
  eprint        = {1607.06450},
  archiveprefix = {arXiv},
  primaryclass  = {stat.ML}
}


@article{Johansson1973_motion-from-points,
  author    = {Johansson, Gunnar},
  title     = {Visual perception of biological motion and a model for its analysis},
  journal   = {Perception & Psychophysics},
  year      = {1973},
  volume    = {14},
  pages     = {201--211},
  doi       = {10.3758/BF03212378},
  url       = {https://doi.org/10.3758/BF03212378},
  issue     = {June},
  received  = {28 February 1972},
  accepted  = {28 February 1973},
  publisher = {Springer Nature}
}


@article{kinetics600,
  author     = {Jo{\~{a}}o Carreira and
                Eric Noland and
                Andras Banki{-}Horvath and
                Chloe Hillier and
                Andrew Zisserman},
  title      = {A Short Note about Kinetics-600},
  journal    = {CoRR},
  volume     = {abs/1808.01340},
  year       = {2018},
  url        = {http://arxiv.org/abs/1808.01340},
  eprinttype = {arXiv},
  eprint     = {1808.01340},
  timestamp  = {Sun, 02 Sep 2018 15:01:55 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1808-01340.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{actionrecognition_without_human_ECCVW16,
  title     = {Human Action Recognition Without Human},
  author    = {Yun He and Soma Shirakabe and Yutaka Satoh and Hirokatsu Kataoka},
  booktitle = {ECCV Workshops},
  year      = {2016},
  url       = {https://api.semanticscholar.org/CorpusID:3631570}
}

@inproceedings{dasWhereToFocus_pi3d_wacv2019,
  author    = {Das, Srijan and Chaudhary, Arpit and Bremond, Francois and Thonnat, Monique},
  booktitle = {2019 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  title     = {Where to Focus on for Human Action Recognition?},
  year      = {2019},
  volume    = {},
  number    = {},
  pages     = {71-80},
  doi       = {10.1109/WACV.2019.00015}
}

@inproceedings{3mformer_CVPR23,
  author    = {Wang, Lei and Koniusz, Piotr},
  title     = {3Mformer: Multi-Order Multi-Mode Transformer for Skeletal Action Recognition},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  year      = {2023},
  pages     = {5620-5631}
}

@inproceedings{potion_cvpr18,
  author    = {Choutas, Vasileios and Weinzaepfel, Philippe and Revaud, Jérôme and Schmid, Cordelia},
  title     = {PoTion: Pose MoTion Representation for Action Recognition},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  year      = {2018}
}

@inproceedings{pa3d_cvpr19,
  author    = {Yan, An and Wang, Yali and Li, Zhifeng and Qiao, Yu},
  booktitle = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {PA3D: Pose-Action 3D Machine for Video Recognition},
  year      = {2019},
  volume    = {},
  number    = {},
  pages     = {7914-7923},
  doi       = {10.1109/CVPR.2019.00811}
}

@article{Ahn2022_STARTransformerAR_WACV23,
  title   = {STAR-Transformer: A Spatio-temporal Cross Attention Transformer for Human Action Recognition},
  author  = {Dasom Ahn and Sangwon Kim and Hyun Wook Hong and ByoungChul Ko},
  journal = {2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
  year    = {2023},
  pages   = {3319-3328},
  url     = {https://api.semanticscholar.org/CorpusID:252907699}
}

@inproceedings{D3D_WACV20,
  author    = {Stroud, Jonathan C. and Ross, David A. and Sun, Chen and Deng, Jia and Sukthankar, Rahul},
  booktitle = {2020 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  title     = {D3D: Distilled 3D Networks for Video Action Recognition},
  year      = {2020},
  volume    = {},
  number    = {},
  pages     = {614-623},
  keywords  = {Three-dimensional displays;Integrated optics;Task analysis;Solid modeling;Optical fiber networks;Training;Kinetic theory},
  doi       = {10.1109/WACV45572.2020.9093274}
}

@inproceedings{dancewithflow_CVPR19,
  title     = {Dance with flow: Two-in-one stream action detection},
  author    = {Zhao, Jiaojiao and Snoek, Cees GM},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages     = {9935--9944},
  year      = {2019}
}

@inproceedings{vificlip,
  title     = {Finetuned CLIP models are efficient video learners},
  author    = {Rasheed, Hanoona and Khattak, Muhammad Uzair and Maaz, Muhammad and Khan, Salman and Khan, Fahad Shahbaz},
  booktitle = {The IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year      = {2023}
}

@article{XCLIP,
  title     = {Expanding Language-Image Pretrained Models for General Video Recognition},
  author    = {Ni, Bolin and Peng, Houwen and Chen, Minghao and Zhang, Songyang and Meng, Gaofeng and Fu, Jianlong and Xiang, Shiming and Ling, Haibin},
  booktitle = {European Conference on Computer Vision (ECCV)},
  year      = {2022}
}

@inproceedings{CLIP,
  title     = {Learning Transferable Visual Models From Natural Language Supervision},
  author    = {Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
  booktitle = {International Conference on Machine Learning},
  year      = {2021},
  url       = {https://api.semanticscholar.org/CorpusID:231591445}
}

@article{ActionCLIP,
  title   = {ActionCLIP: A New Paradigm for Video Action Recognition},
  author  = {Mengmeng Wang and Jiazheng Xing and Yong Liu},
  journal = {ArXiv},
  year    = {2021},
  volume  = {abs/2109.08472},
  url     = {https://api.semanticscholar.org/CorpusID:237563206}
}

@inproceedings{FROSTER,
  title     = {FROSTER: Frozen CLIP is a Strong Teacher for Open-Vocabulary Action Recognition},
  author    = {Xiaohu Huang and Hao Zhou and Kun Yao and Kai Han},
  booktitle = {International Conference on Learning Representations},
  year      = {2024}
}

@article{synse,
  title={Syntactically Guided Generative Embeddings for Zero-Shot Skeleton Action Recognition},
  author={Pranay Gupta and Divyanshu Sharma and Ravi Kiran Sarvadevabhatla},
  journal={2021 IEEE International Conference on Image Processing (ICIP)},
  year={2021},
  pages={439-443}
}

@inproceedings{smie,
  title     = {Zero-shot Skeleton-based Action Recognition via Mutual Information Estimation and Maximization},
  author    = {Zhou, Yujie and Qiang, Wenwen and Rao, Anyi and Lin, Ning and Su, Bing and Wang, Jiaqi},
  booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
  pages     = {5302--5310},
  year      = {2023}
}

@article{liu2023_STAN_CVPR23,
  title   = {Revisiting Temporal Modeling for CLIP-based Image-to-Video Knowledge Transferring},
  author  = {Liu, Ruyang and Huang, Jingjia and Li, Ge and Feng, Jiashi and Wu, Xinglong and Li, Thomas H},
  journal = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year    = {2023}
}

@misc{park2023_dualpath_CVPR,
  title   = {Dual-path Adaptation from Image to Video Transformers},
  author  = {Jungin Park and Jiyoung Lee and Kwanghoon Sohn},
  year    = {2023},
  journal = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}
}

@inproceedings{yang2023_AIM_ICLR,
  title     = {AIM: Adapting Image Models for Efficient Video Understanding},
  author    = {Taojiannan Yang and Yi Zhu and Yusheng Xie and Aston Zhang 
               and Chen Chen and Mu Li},
  booktitle = {International Conference on Learning Representations},
  year      = {2023},
  url       = {https://openreview.net/forum?id=CIoSZ_HKHS7}
}

@inproceedings{st_adapter,
  author    = {Pan, Junting and Lin, Ziyi and Zhu, Xiatian and Shao, Jing and Li, Hongsheng},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
  pages     = {26462--26477},
  publisher = {Curran Associates, Inc.},
  title     = {ST-Adapter: Parameter-Efficient Image-to-Video Transfer Learning},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2022/file/a92e9165b22d4456fc6d87236e04c266-Paper-Conference.pdf},
  volume    = {35},
  year      = {2022}
}

@article{chen_2022_adaptformer,
  title   = {AdaptFormer: Adapting Vision Transformers for Scalable Visual Recognition},
  author  = {Chen, Shoufa and Ge, Chongjian and Tong, Zhan and Wang, Jiangliu and Song, Yibing and Wang, Jue and Luo, Ping},
  journal = {Advances in Neural Information Processing Systems},
  year    = {2022}
}

@misc{gupta_crossmodaldistillation_cvpr16,
  title   = {Cross Modal Distillation for Supervision Transfer},
  author  = {Saurabh Gupta and Judy Hoffman and Jitendra Malik},
  year    = {2016},
  journal = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}
}

@misc{aytar2016soundnet,
  title   = {SoundNet: Learning Sound Representations from Unlabeled Video},
  author  = {Yusuf Aytar and Carl Vondrick and Antonio Torralba},
  year    = {2016},
  journal = {Advances in Neural Information Processing Systems}
}

@inproceedings{msfzssar,
  author    = {Li, Ming-Zhe and Jia, Zhen and Zhang, Zhang and Ma, Zhanyu and Wang, Liang},
  title     = {Multi-semantic Fusion Model For&nbsp;Generalized Zero-Shot Skeleton-Based Action Recognition},
  year      = {2023},
  isbn      = {978-3-031-46304-4},
  publisher = {Springer-Verlag},
  address   = {Berlin, Heidelberg},
  url       = {https://doi.org/10.1007/978-3-031-46305-1_6},
  doi       = {10.1007/978-3-031-46305-1_6},
  booktitle = {Image and Graphics: 12th International Conference, ICIG 2023, Nanjing, China, September 22–24, 2023, Proceedings, Part I},
  pages     = {68–80},
  numpages  = {13},
  keywords  = {Generalized Zero-Shot Learning, Skeleton-Based Action Recognition, Semantic Description, Generative Method},
  location  = {Nanjing, China}
}

@article{relationnet,
  title   = {Relational Network for Skeleton-Based Action Recognition},
  author  = {Wu Zheng and Lin Li and Zhaoxiang Zhang and Yan Huang and Liang Wang},
  journal = {2019 IEEE International Conference on Multimedia and Expo (ICME)},
  year    = {2018},
  pages   = {826-831},
  url     = {https://api.semanticscholar.org/CorpusID:119103433}
}

@article{mutualinfo,
  title   = {Zero-shot Skeleton-based Action Recognition via Mutual Information Estimation and Maximization},
  author  = {Yujie Zhou and Wenwen Qiang and Anyi Rao and Ning Lin and Bing Su and Jiaqi Wang},
  journal = {Proceedings of the 31st ACM International Conference on Multimedia},
  year    = {2023},
  url     = {https://api.semanticscholar.org/CorpusID:260704789}
}

@article{Wav2CLIP,
  title   = {Wav2CLIP: Learning Robust Audio Representations from Clip},
  author  = {Ho-Hsiang Wu and Prem Seetharaman and Kundan Kumar and Juan Pablo Bello},
  journal = {ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year    = {2021},
  pages   = {4563-4567},
  url     = {https://api.semanticscholar.org/CorpusID:239616434}
}

@article{Audioclip,
  title   = {Audioclip: Extending Clip to Image, Text and Audio},
  author  = {Andrey Guzhov and Federico Raue and J{\"o}rn Hees and Andreas R. Dengel},
  journal = {ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year    = {2021},
  pages   = {976-980},
  url     = {https://api.semanticscholar.org/CorpusID:235624127}
}

@inproceedings{clip4vla,
  title     = {Accommodating Audio Modality in CLIP for Multimodal Processing},
  author    = {Ludan Ruan and Anwen Hu and Yuqing Song and Liang Zhang and S. Zheng and Qin Jin},
  booktitle = {AAAI Conference on Artificial Intelligence},
  year      = {2023},
  url       = {https://api.semanticscholar.org/CorpusID:257496490}
}

@article {healthcare_worker_shortage,
	author = {Mathieu Boniol and Teena Kunjumen and Tapas Sadasivan Nair and Amani Siyam and James Campbell and Khassoum Diallo},
	title = {The global health workforce stock and distribution in 2020 and 2030: a threat to equity and {\textquoteleft}universal{\textquoteright} health coverage?},
	volume = {7},
	number = {6},
	elocation-id = {e009316},
	year = {2022},
	doi = {10.1136/bmjgh-2022-009316},
	publisher = {BMJ Specialist Journals},
	URL = {https://gh.bmj.com/content/7/6/e009316},
	eprint = {https://gh.bmj.com/content/7/6/e009316.full.pdf},
	journal = {BMJ Global Health}
}

@misc{mahmud_AVECLIP_WACV22,
      title={AVE-CLIP: AudioCLIP-based Multi-window Temporal Transformer for Audio Visual Event Localization}, 
      author={Tanvir Mahmud and Diana Marculescu},
      year={2022},
      eprint={2210.05060},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{star,
  title={Fine-Grained Side Information Guided Dual-Prompts for Zero-Shot Skeleton Action Recognition},
  author={Yang Chen and Jingcai Guo and Tian He and Ling Wang},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:269043086}
}

@article{register_viz,
  title={Vision Transformers Need Registers},
  author={Timoth'ee Darcet and Maxime Oquab and Julien Mairal and Piotr Bojanowski},
  journal={ArXiv},
  year={2023},
  volume={abs/2309.16588},
  url={https://api.semanticscholar.org/CorpusID:263134283}
}

@inproceedings{gap,
    title={Generative Action Description Prompts for Skeleton-based Action Recognition},
    author={Wangmeng, Xiang and Chao, Li and Yuxuan, Zhou and Biao, Wang and Lei, Zhang},
    booktitle={ICCV},
    year={2023}
}

@article{videollava,
  title={Video-LLaVA: Learning United Visual Representation by Alignment Before Projection},
  author={Lin, Bin and Zhu, Bin and Ye, Yang and Ning, Munan and Jin, Peng and Yuan, Li},
  journal={arXiv preprint arXiv:2311.10122},
  year={2023}
}

@article{videollama,
  title={Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding},
  author={Hang Zhang and Xin Li and Lidong Bing},
  journal={ArXiv},
  year={2023},
  volume={abs/2306.02858},
  url={https://api.semanticscholar.org/CorpusID:259075356}
}

@inproceedings{videochatgpt,
    title={Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models},
    author={Maaz, Muhammad and Rasheed, Hanoona and Khan, Salman and Khan, Fahad Shahbaz},
    booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL 2024)},
    year={2024}
}

@article{cogvlm,
  title={CogVLM: Visual Expert for Pretrained Language Models},
  author={Weihan Wang and Qingsong Lv and Wenmeng Yu and Wenyi Hong and Ji Qi and Yan Wang and Junhui Ji and Zhuoyi Yang and Lei Zhao and Xixuan Song and Jiazheng Xu and Bin Xu and Juanzi Li and Yuxiao Dong and Ming Ding and Jie Tang},
  journal={ArXiv},
  year={2023},
  volume={abs/2311.03079},
  url={https://api.semanticscholar.org/CorpusID:265034288}
}

@article{lavila,
  title={Learning Video Representations from Large Language Models},
  author={Yue Zhao and Ishan Misra and Philipp Krahenbuhl and Rohit Girdhar},
  journal={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2022},
  pages={6586-6597},
  url={https://api.semanticscholar.org/CorpusID:254408789}
}

@inproceedings{blip2,
  title={BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models},
  author={Junnan Li and Dongxu Li and Silvio Savarese and Steven C. H. Hoi},
  booktitle={International Conference on Machine Learning},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:256390509}
}
@article{llama,
  title={LLaMA: Open and Efficient Foundation Language Models},
  author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timoth{\'e}e Lacroix and Baptiste Rozi{\`e}re and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
  journal={ArXiv},
  year={2023},
  volume={abs/2302.13971},
  url={https://api.semanticscholar.org/CorpusID:257219404}
}

@article{gpt,
  title={Language Models are Few-Shot Learners},
  author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeff Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
  journal={ArXiv},
  year={2020},
  volume={abs/2005.14165},
  url={https://api.semanticscholar.org/CorpusID:218971783}
}

@inproceedings{
socratic,
title={Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language},
author={Andy Zeng and Maria Attarian and brian ichter and Krzysztof Marcin Choromanski and Adrian Wong and Stefan Welker and Federico Tombari and Aveek Purohit and Michael S Ryoo and Vikas Sindhwani and Johnny Lee and Vincent Vanhoucke and Pete Florence},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=G2Q2Mh3avow}
}

@misc{videochat,
      title={VideoChat: Chat-Centric Video Understanding}, 
      author={KunChang Li and Yinan He and Yi Wang and Yizhuo Li and Wenhai Wang and Ping Luo and Yali Wang and Limin Wang and Yu Qiao},
      year={2024},
      eprint={2305.06355},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{UniFormer,
  title={UniFormer: Unifying Convolution and Self-Attention for Visual Recognition},
  author={Kunchang Li and Yali Wang and Junhao Zhang and Peng Gao and Guanglu Song and Yu Liu and Hongsheng Li and Yu Jiao Qiao},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2022},
  volume={45},
  pages={12581-12600},
  url={https://api.semanticscholar.org/CorpusID:246240170}
}

@article{internvideo,
  title={Internvideo: General video foundation models via generative and discriminative learning},
  author={Wang, Yi and Li, Kunchang and Li, Yizhuo and He, Yinan and Huang, Bingkun and Zhao, Zhiyu and Zhang, Hongjie and Xu, Jilan and Liu, Yi and Wang, Zun and others},
  journal={arXiv preprint arXiv:2212.03191},
  year={2022}
}

@article{ChatVideo,
  title={ChatVideo: A Tracklet-centric Multimodal and Versatile Video Understanding System},
  author={Junke Wang and Dongdong Chen and Chong Luo and Xiyang Dai and Lu Yuan and Zuxuan Wu and Yu-Gang Jiang},
  journal={ArXiv},
  year={2023},
  volume={abs/2304.14407},
  url={https://api.semanticscholar.org/CorpusID:258352505}
}

@article{ChatCaptioner,
  title={Video ChatCaptioner: Towards Enriched Spatiotemporal Descriptions},
  author={Jun Chen and Deyao Zhu and Kilichbek Haydarov and Xiang Li and Mohamed Elhoseiny},
  journal={ArXiv},
  year={2023},
  volume={abs/2304.04227},
  url={https://api.semanticscholar.org/CorpusID:258048445}
}

@article{LongVLM,
  title={LongVLM: Efficient Long Video Understanding via Large Language Models},
  author={Yuetian Weng and Mingfei Han and Haoyu He and Xiaojun Chang and Bohan Zhuang},
  journal={ArXiv},
  year={2024},
  volume={abs/2404.03384},
  url={https://api.semanticscholar.org/CorpusID:268889590}
}

@article{llovi,
  title={A Simple LLM Framework for Long-Range Video Question-Answering},
  author={Ce Zhang and Taixi Lu and Md Mohaiminul Islam and Ziyang Wang and Shoubin Yu and Mohit Bansal and Gedas Bertasius},
  journal={ArXiv},
  year={2023},
  volume={abs/2312.17235},
  url={https://api.semanticscholar.org/CorpusID:266573523}
}

@article{timechat,
  title={TimeChat: A Time-sensitive Multimodal Large Language Model for Long Video Understanding},
  author={Ren, Shuhuai and Yao, Linli and Li, Shicheng and Sun, Xu and Hou, Lu},
  journal={arXiv preprint arXiv:2312.02051},
  year={2023}
}

@article{LanguageBind,
  title={LanguageBind: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment},
  author={Bin Zhu and Bin Lin and Munan Ning and Yang Yan and Jiaxi Cui and Hongfa Wang and Yatian Pang and Wenhao Jiang and Junwu Zhang and Zongwei Li and Wancai Zhang and Zhifeng Li and Wei Liu and Liejie Yuan},
  journal={ArXiv},
  year={2023},
  volume={abs/2310.01852},
  url={https://api.semanticscholar.org/CorpusID:263608698}
}

@article{OWLv2,
  title={Scaling open-vocabulary object detection},
  author={Minderer, Matthias and Gritsenko, Alexey and Houlsby, Neil},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{mementos,
  title={Mementos: A comprehensive benchmark for multimodal large language model reasoning over image sequences},
  author={Wang, Xiyao and Zhou, Yuhang and Liu, Xiaoyu and Lu, Hongjin and Xu, Yuancheng and He, Feihong and Yoon, Jaehong and Lu, Taixi and Bertasius, Gedas and Bansal, Mohit and others},
  journal={arXiv preprint arXiv:2401.10529},
  year={2024}
}

@InProceedings{flamingo,
  author       = "Jean-Baptiste Alayrac and Jeff Donahue and Pauline Luc and Antoine Miech and Iain Barr and Yana Hasson and Karel Lenc and Arthur Mensch and Katherine Millican and Malcolm Reynolds and Roman Ring and Eliza Rutherford and Serkan Cabi and Tengda Han and Zhitao Gong and Sina Samangooei and Marianne Monteiro and Jacob L. Menick and Sebastian Borgeaud and Andy Brock and Aida Nematzadeh and Sahand Sharifzadeh and Mikołaj Bińkowski and Ricardo Barreira and Oriol Vinyals and Andrew Zisserman and Karén Simonyan",
  title        = "Flamingo: a Visual Language Model for Few-Shot Learning",
  booktitle    = "Advances in Neural Information Processing Systems",
  editor       = "S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh",
  pages        = "23716--23736",
  publisher    = "Curran Associates, Inc.",
  url          = "https://proceedings.neurips.cc/paper_files/paper/2022/file/960a172bc7fbf0177ccccbb411a7d800-Paper-Conference.pdf",
  volume       = "35",
  year         = "2022"
}

@article{minigpt,
  title={Minigpt-4: Enhancing vision-language understanding with advanced large language models},
  author={Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed},
  journal={arXiv preprint arXiv:2304.10592},
  year={2023}
}

@article{mplugowl,
  title={mplug-owl: Modularization empowers large language models with multimodality},
  author={Ye, Qinghao and Xu, Haiyang and Xu, Guohai and Ye, Jiabo and Yan, Ming and Zhou, Yiyang and Wang, Junyang and Hu, Anwen and Shi, Pengcheng and Shi, Yaya and others},
  journal={arXiv preprint arXiv:2304.14178},
  year={2023}
}

@article{lora,
  title={LoRA: Low-Rank Adaptation of Large Language Models},
  author={J. Edward Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Weizhu Chen},
  journal={ArXiv},
  year={2021},
  volume={abs/2106.09685},
  url={https://api.semanticscholar.org/CorpusID:235458009}
}

@article{instructblip,
  title={Instructblip: Towards general-purpose vision-language models with instruction tuning},
  author={Dai, Wenliang and Li, Junnan and Li, Dongxu and Tiong, Anthony Meng Huat and Zhao, Junqi and Wang, Weisheng and Li, Boyang and Fung, Pascale N and Hoi, Steven},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{NEURIPS2023_9a6a435e,
 author = {Dai, Wenliang and Li, Junnan and LI, DONGXU and Tiong, Anthony and Zhao, Junqi and Wang, Weisheng and Li, Boyang and Fung, Pascale N and Hoi, Steven},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {49250--49267},
 publisher = {Curran Associates, Inc.},
 title = {InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/9a6a435e75419a836fe47ab6793623e6-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}

@article{pali,
  title={Pali: A jointly-scaled multilingual language-image model},
  author={Chen, Xi and Wang, Xiao and Changpinyo, Soravit and Piergiovanni, AJ and Padlewski, Piotr and Salz, Daniel and Goodman, Sebastian and Grycner, Adam and Mustafa, Basil and Beyer, Lucas and others},
  journal={arXiv preprint arXiv:2209.06794},
  year={2022}
}

@article{qwen,
  title={Qwen-vl: A frontier large vision-language model with versatile abilities},
  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2308.12966},
  year={2023}
}

@inproceedings{lemma,
author={Jia, Baoxiong and Chen, Yixin and Huang, Siyuan and Zhu, Yixin and Zhu, Song-Chun}, 
title={LEMMA: A Multiview Dataset for Learning Multi-agent Multi-view Activities}, 
booktitle={Proceedings of the European Conference on Computer Vision (ECCV)}, 
year={2020}
}

@article{mmbench,
  title={Mmbench: Is your multi-modal model an all-around player?},
  author={Liu, Yuan and Duan, Haodong and Zhang, Yuanhan and Li, Bo and Zhang, Songyang and Zhao, Wangbo and Yuan, Yike and Wang, Jiaqi and He, Conghui and Liu, Ziwei and others},
  journal={arXiv preprint arXiv:2307.06281},
  year={2023}
}

@article{mvbench,
  title={Mvbench: A comprehensive multi-modal video understanding benchmark},
  author={Li, Kunchang and Wang, Yali and He, Yinan and Li, Yizhuo and Wang, Yi and Liu, Yi and Wang, Zun and Xu, Jilan and Chen, Guo and Luo, Ping and others},
  journal={arXiv preprint arXiv:2311.17005},
  year={2023}
}

@article{palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={240},
  pages={1--113},
  year={2023}
}

@article{bloom,
  title={Bloom: A 176b-parameter open-access multilingual language model},
  author={Workshop, BigScience and Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c{c}}ois and others},
  journal={arXiv preprint arXiv:2211.05100},
  year={2022}
}

@InProceedings{webvid_data,
  author       = "Max Bain and Arsha Nagrani and G{\"u}l Varol and Andrew Zisserman",
  title        = "Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval",
  booktitle    = "IEEE International Conference on Computer Vision",
  year         = "2021",
}

@article{crossglg,
  title={CrossGLG: LLM Guides One-shot Skeleton-based 3D Action Recognition in a Cross-level Manner},
  author={Yan, Tingbing and Zeng, Wenzheng and Xiao, Yang and Tong, Xingyu and Tan, Bo and Fang, Zhiwen and Cao, Zhiguo and Zhou, Joey Tianyi},
  journal={arXiv preprint arXiv:2403.10082},
  year={2024}
}

@article{chatgpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{llama2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{zhang2024visually,
  title={Why are Visually-Grounded Language Models Bad at Image Classification?},
  author={Zhang, Yuhui and Unell, Alyssa and Wang, Xiaohan and Ghosh, Dhruba and Su, Yuchang and Schmidt, Ludwig and Yeung-Levy, Serena},
  journal={arXiv preprint arXiv:2405.18415},
  year={2024}
}

@article{wu2024frequency,
  title={Frequency Guidance Matters: Skeletal Action Recognition by Frequency-Aware Mixed Transformer},
  author={Wu, Wenhan and Zheng, Ce and Yang, Zihao and Chen, Chen and Das, Srijan and Lu, Aidong},
  journal={arXiv preprint arXiv:2407.12322},
  year={2024}
}

@inproceedings{imagebind,
  title={Imagebind: One embedding space to bind them all},
  author={Girdhar, Rohit and El-Nouby, Alaaeldin and Liu, Zhuang and Singh, Mannat and Alwala, Kalyan Vasudev and Joulin, Armand and Misra, Ishan},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={15180--15190},
  year={2023}
}

@article{qian2022_opticalflow_and_audioclip,
  title={Multimodal open-vocabulary video classification via pre-trained vision and language models},
  author={Qian, Rui and Li, Yeqing and Xu, Zheng and Yang, Ming-Hsuan and Belongie, Serge and Cui, Yin},
  journal={arXiv preprint arXiv:2207.07646},
  year={2022}
}

@inproceedings{cheng2020shiftgcn,  
  title     = {Skeleton-Based Action Recognition with Shift Graph Convolutional Network},  
  author    = {Ke Cheng and Yifan Zhang and Xiangyu He and Weihan Chen and Jian Cheng and Hanqing Lu},  
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},  
  year      = {2020},  
}

@misc{meta2024_llama3herdmodels,
      title={The Llama 3 Herd of Models}, 
      author={Meta},
      year={2024},
      eprint={2407.21783},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.21783}, 
}

@misc{zhang2023_speechgpt,
      title={SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities}, 
      author={Dong Zhang and Shimin Li and Xin Zhang and Jun Zhan and Pengyu Wang and Yaqian Zhou and Xipeng Qiu},
      year={2023},
      eprint={2305.11000},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@InProceedings{feng2024_chatpose,
    author = {Feng, Yao and Lin, Jing and Dwivedi, Sai Kumar and Sun, Yu and Patel, Priyanka and Black, Michael J.},
    title = {ChatPose: Chatting about 3D Human Pose},
    booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    year = {2024}
}

@article{su2023_pandagpt,
    title={PandaGPT: One Model To Instruction-Follow Them All},
    author={Su, Yixuan and Lan, Tian and Li, Huayang and Xu, Jialu and Wang, Yan and Cai, Deng},
    journal={arXiv preprint arXiv:2305.16355},
    year={2023}
  }

@article{lu2022_unifiedio,
  title={Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks},
  author={Lu, Jiasen and Clark, Christopher and Zellers, Rowan and Mottaghi, Roozbeh and Kembhavi, Aniruddha},
  journal={arXiv preprint arXiv:2206.08916},
  year={2022}
}

@article{ye2024_xvila,
  title={X-VILA: Cross-Modality Alignment for Large Language Model},
  author={Ye, Hanrong and Huang, De-An and Lu, Yao and Yu, Zhiding and Ping, Wei and Tao, Andrew and Kautz, Jan and Han, Song and Xu, Dan and Molchanov, Pavlo and others},
  journal={arXiv preprint arXiv:2405.19335},
  year={2024}
}

@article{eva_CLIP,
  title={Eva-clip: Improved training techniques for clip at scale},
  author={Sun, Quan and Fang, Yuxin and Wu, Ledell and Wang, Xinlong and Cao, Yue},
  journal={arXiv preprint arXiv:2303.15389},
  year={2023}
}

@article{llavidal2024,
          title={LLAVIDAL: A Large LAnguage VIsion Model for Daily Activities of Living}, 
          author={Dominick Reilly and Rajatsubhra Chakraborty and Arkaprava Sinha and Manish Kumar Govind and Pu Wang and Francois Bremond and Le Xue and Srijan Das},
          journal={arXiv},
          year={2024},
          volume={2406.09390}
        }