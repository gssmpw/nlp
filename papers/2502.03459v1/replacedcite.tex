\section{Related Work}
\noindent
\textbf{Multi-modal Knowledge Distillation.}
In standard knowledge distillation____, the knowledge of large-scale models (teachers) is transferred to smaller models (students), enabling students to replicate the teacherâ€™s predictions or feature representations and replace them during inference. Similarly, multi-modal knowledge distillation transfers the knowledge of a teacher model to a student model operating on a different modality. This approach is effective for tasks like action recognition, where complementary modalities exist that are not available or infeasible to compute at inference time (e.g., optical flow or human skeleton). In action recognition, the student modality is typically RGB, and the teacher modality is optical flow____ or audio____. However, these modalities are limited in modeling human motion and generalizing across varying viewpoints, limiting their adoption for ADL____. Human skeletons, by contrast, have emerged as the dominant modality to combine with RGB for ADL tasks____, as they effectively model human motion and generalize across viewpoints. While effective for understanding ADL, these previous approaches are not applicable to zero-shot tasks. In contrast, our proposed SKI-VLM and SKI-LVLM overcome this limitation by incorporating both modalities with language, enabling zero-shot tasks.\\
\textbf{Multi-modal VLMs for Action Recognition.}
Many approaches propose to extend the image-based CLIP to the video domain through fine-tuning CLIP to handle the temporal dimension of video data. Partially fine-tuned approaches
____
% ____
perform training with additional trainable temporal modules but leave CLIP's parameters frozen, while fully fine-tuned approaches____ perform a simple temporal pooling but update the CLIP parameters during training. These approaches only process RGB and ignore the rich multi-modal nature of videos. In response, some works attempt to incorporate additional modalities such as optical flow____ and audio____ into the CLIP embedding space. While these approaches all aim to introduce new modalities into CLIP, their methodologies vary. For example, ____ trains audio, visual, and language encoders using a tri-modal contrastive loss, while ____ contrastively aligns an audio encoder with a frozen CLIP visual encoder. Different from these works, we introduce the skeleton modality into the CLIP space to better address the challenges of ADL. Additionally we find, and experimentally validate, that previous alignment strategies are sub-optimal when considering the skeleton modality.

% \textbf{VLMs for Action Recognition.}
% Many approaches have been proposed to extend the image-based CLIP to the video domain through fine-tuning CLIP with additional temporal modules. These approaches can be broadly separated into two categories: partially fine-tuned and fully fine-tuned depending on whether the parameters of the CLIP model are updated during fine-tuning. Partially fine-tuned approaches____ freeze the pre-trained CLIP parameters and add trainable modules to learn temporal relationships in videos.
% For example, AIM____ introduces lightweight adapters between CLIPs vision encoder layers, and XCLIP____ introduces specialized transformers and video-specific prompting into the CLIP model.
% % For instance, XCLIP____ incorporates learnable prompts and inter-frame communication transformers to extract temporal features from frozen CLIP representations. 
% In contrast, fully-finetuned approaches____ update CLIP's parameters during training, as seen in ActionCLIP____ and ViFiCLIP____, which integrate temporal information through pooling of frame-level features.
% % One property of fully finetuned methods is that updating the CLIP parameters can lead to losses in generalization capabilities____. FROSTER____ mitigates this by distilling features between the base CLIP model and its video counterpart. \\

% \textbf{Optical flow} has been used alongside CLIP for action recognition, although prior work____ suggests it is less effective than 3D skeletons for Activities of Daily Living. \textbf{Audio Modality} approaches, such as AudioCLIP____ and CLIP4VLA____, align audio with image and text modalities, achieving strong results in tasks like audio captioning and classification. 
% Despite promising performance in other domains, the absence of a large-scale open-world 3D skeleton dataset poses a challenge for contrastive learning using skeleton data. In ADL, 3D skeletons are crucial due to the focus on human-centric actions, requiring SCD-like mechanisms for effective integration into the CLIP space. \\


% \textbf{Skeleton-based approaches} for zero shot action recognition____ leverage human skeleton sequences to improve model performance. SynSE____ enhances visual-semantic alignment through syntactic guidance, while SMIE____ uses mutual information maximization and a temporal constraint module to refine action differentiation. Two-stream approaches____ combine semantic understanding via LLMs with skeleton feature extraction, demonstrating the value of human movement dynamics in action recognition.    \\
% Inspired by the success of CLIP____ in the image domain, various CLIP-based models for video action recognition have emerged. These models aim to introduce spatio-temporal understanding into the CLIP representation space and can be classified as either partially fine-tuned or fully fine-tuned approaches. 
% Partially fine-tuned approaches____ freeze the pre-trained CLIP parameters and make use of lightweight parameterized adapters to learn the temporal relationship among frames in videos. 
% For example, X-CLIP____ introduces learnable prompting and inter-frame communication transformers to learn temporal relationships from the frozen CLIP representations. 
% On the other hand, fully fine-tuned approaches____ directly update CLIP's parameters during training. ActionCLIP____ introduces a pre-network prompt and post-network prompt to learn temporal features. ViFi-CLIP____ shows that training the CLIP encoders with a simple temporal pooling layer successfully learns useful representations for action recognition.
% These models learn rich temporal features in the CLIP representation space, but are shown to sacrifice generalization performance____. FROSTER____ addresses this by performing a feature distillation between the base CLIP model and their CLIP based video model. 
% % Until now, these approaches have only been applied to web video dataset and only utilize the RGB modality. In contrast, our proposed {\Large$\pi$}-CLIP is designed for ADL and makes use of both the RGB and human skeleton modalities.
% Other modalities, such as optical flow, audio etc., have been used in conjunction with CLIP for their respective downstream tasks. \\
% \textbf{Optical flow} is another modality that estimates motion of objects in subsequent frames of a video. It can be used in conjunction with video-based CLIP models for action recognition. However, in previous state of the art ____, it has been established that optical flow is a weaker modality than 3D skeletons for action recognition for ADL. \\
% \textbf{Audio Modality} approaches using CLIP ____ have shown promising performance in audio captioning, audio classification, as well as video retrieval. AudioCLIP ____ and CLIP4VLA ____ align the audio modality with that of image and text modalities. Additionally, CLIP4VLA performs intramodal contrastive learning with augmentations of audio inputs. It encodes both verbal and non-verbal heterogeneous information from audio. Wav2CLIP ____ on the other hand performs distillation with cross projection contrastive loss. 
% However, unlike the audio domain, there lacks a large-scale open-world 3D human skeleton dataset. This is a major downside for using skeletons for contrastive learning as it requires substantial amounts of data for effective performance. 
% Furthermore, Activities of Daily living focuses on human-centric actions where modelling the human anatomy is more important than modelling human audio. Hence, 3D human skeletons can play a crucial role in ADL and require SCD-like learning mechanism for its introduction in the CLIP embedding space. \\
\textbf{Zero-Shot Skeleton Action Recognition.}
Zero-shot skeleton-based models aim to enable action classification of unseen classes using only human skeleton sequences. SynSE____ introduces a syntactically guided approach, using part-of-speech tags to enhance the alignment between skeleton and language spaces. SMIE____ employs mutual information estimation and maximization to globally align the two spaces. CrossGLG____ uses LLM descriptions of actions and cross-attention to guide a skeleton encoder during training, but only uses the skeleton encoder at inference. Closest to our work is STAR____, which aligns a Shift-GCN____ skeleton encoder with a pre-trained transformer-based text encoder____ for zero-shot skeleton action recognition. STAR differs from our work in that it does not incorporate the RGB modality or investigate strategies to enhance the representations of VLMs and LVLMs.

% Many approaches leverage human skeleton sequences for Zero Shot Action Recognition____. SynSE____ introduces a syntactically guided approach, using part-of-speech tags to enhance the alignment between visual and semantic spaces, thereby improving compositional generalization capabilities. Meanwhile, SMIE____ employs mutual information estimation and maximization to globally align these spaces and incorporates a temporal constraint module to leverage the inherent dynamics of actions, thus enabling more accurate differentiation of similar actions over time. Some approaches such as____, consist of a two-stream approach, a semantic understanding branch to encode the semantic information of action classes using LLMs and their relation to the joints of the skeleton and a skeleton encoder branch to obtain skeleton features guided by the joint level features from the semantic branch. These skeleton-based approaches demonstrate innovative strategies to enhance Zero Shot Action Recognition by exploiting the dynamic and structural nuances of human movement.

\textbf{Multi-modal Large Language Models.}
Large language models, such as ChatGPT____ and LLaMA____, have exhibited remarkable capabilities in language understanding and generation tasks. These capabilities have inspired many works to extend LLMs to incorporate additional modalities, such as video____, audio____, and human pose____, typically through instruction tuning or the addition of modality-specific encoders with projection layers. Extending beyond this, other approaches show the possibility to incorporate a wide range of modalities into the LLM space
____. Unique 
% ____. Unique 
from these approaches, our SKI-LVLM targets ADL and aims to train using multiple modalities (vision, language and skeleton), but only use vision and language during inference.