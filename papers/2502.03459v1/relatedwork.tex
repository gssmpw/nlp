\section{Related Work}
\noindent
\textbf{Multi-modal Knowledge Distillation.}
In standard knowledge distillation~\cite{knowledge_distillation_hinton2015}, the knowledge of large-scale models (teachers) is transferred to smaller models (students), enabling students to replicate the teacherâ€™s predictions or feature representations and replace them during inference. Similarly, multi-modal knowledge distillation transfers the knowledge of a teacher model to a student model operating on a different modality. This approach is effective for tasks like action recognition, where complementary modalities exist that are not available or infeasible to compute at inference time (e.g., optical flow or human skeleton). In action recognition, the student modality is typically RGB, and the teacher modality is optical flow~\cite{gupta_crossmodaldistillation_cvpr16, mars} or audio~\cite{soundnet}. However, these modalities are limited in modeling human motion and generalizing across varying viewpoints, limiting their adoption for ADL~\cite{das2020vpn}. Human skeletons, by contrast, have emerged as the dominant modality to combine with RGB for ADL tasks~\cite{das2020vpn, vpn++, pivit}, as they effectively model human motion and generalize across viewpoints. While effective for understanding ADL, these previous approaches are not applicable to zero-shot tasks. In contrast, our proposed SKI-VLM and SKI-LVLM overcome this limitation by incorporating both modalities with language, enabling zero-shot tasks.\\
\textbf{Multi-modal VLMs for Action Recognition.}
Many approaches propose to extend the image-based CLIP to the video domain through fine-tuning CLIP to handle the temporal dimension of video data. Partially fine-tuned approaches
~\cite{yang2023_AIM_ICLR, XCLIP, st_adapter}
% ~\cite{yang2023_AIM_ICLR, XCLIP, chen_2022_adaptformer, st_adapter}
perform training with additional trainable temporal modules but leave CLIP's parameters frozen, while fully fine-tuned approaches~\cite{ActionCLIP, vificlip, FROSTER} perform a simple temporal pooling but update the CLIP parameters during training. These approaches only process RGB and ignore the rich multi-modal nature of videos. In response, some works attempt to incorporate additional modalities such as optical flow~\cite{qian2022_opticalflow_and_audioclip} and audio~\cite{Audioclip, clip4vla, Wav2CLIP} into the CLIP embedding space. While these approaches all aim to introduce new modalities into CLIP, their methodologies vary. For example, \cite{Audioclip} trains audio, visual, and language encoders using a tri-modal contrastive loss, while \cite{Wav2CLIP} contrastively aligns an audio encoder with a frozen CLIP visual encoder. Different from these works, we introduce the skeleton modality into the CLIP space to better address the challenges of ADL. Additionally we find, and experimentally validate, that previous alignment strategies are sub-optimal when considering the skeleton modality.

% \textbf{VLMs for Action Recognition.}
% Many approaches have been proposed to extend the image-based CLIP to the video domain through fine-tuning CLIP with additional temporal modules. These approaches can be broadly separated into two categories: partially fine-tuned and fully fine-tuned depending on whether the parameters of the CLIP model are updated during fine-tuning. Partially fine-tuned approaches~\cite{XCLIP, chen_2022_adaptformer, st_adapter, yang2023_AIM_ICLR, park2023_dualpath_CVPR} freeze the pre-trained CLIP parameters and add trainable modules to learn temporal relationships in videos.
% For example, AIM~\cite{yang2023_AIM_ICLR} introduces lightweight adapters between CLIPs vision encoder layers, and XCLIP~\cite{XCLIP} introduces specialized transformers and video-specific prompting into the CLIP model.
% % For instance, XCLIP~\cite{XCLIP} incorporates learnable prompts and inter-frame communication transformers to extract temporal features from frozen CLIP representations. 
% In contrast, fully-finetuned approaches~\cite{ActionCLIP, liu2023_STAN_CVPR23, vificlip, FROSTER} update CLIP's parameters during training, as seen in ActionCLIP~\cite{ActionCLIP} and ViFiCLIP~\cite{vificlip}, which integrate temporal information through pooling of frame-level features.
% % One property of fully finetuned methods is that updating the CLIP parameters can lead to losses in generalization capabilities~\cite{vificlip}. FROSTER~\cite{FROSTER} mitigates this by distilling features between the base CLIP model and its video counterpart. \\

% \textbf{Optical flow} has been used alongside CLIP for action recognition, although prior work~\cite{vpn++} suggests it is less effective than 3D skeletons for Activities of Daily Living. \textbf{Audio Modality} approaches, such as AudioCLIP~\cite{Audioclip} and CLIP4VLA~\cite{clip4vla}, align audio with image and text modalities, achieving strong results in tasks like audio captioning and classification. 
% Despite promising performance in other domains, the absence of a large-scale open-world 3D skeleton dataset poses a challenge for contrastive learning using skeleton data. In ADL, 3D skeletons are crucial due to the focus on human-centric actions, requiring SCD-like mechanisms for effective integration into the CLIP space. \\


% \textbf{Skeleton-based approaches} for zero shot action recognition~\cite{synse, smie, star} leverage human skeleton sequences to improve model performance. SynSE~\cite{synse} enhances visual-semantic alignment through syntactic guidance, while SMIE~\cite{smie} uses mutual information maximization and a temporal constraint module to refine action differentiation. Two-stream approaches~\cite{star, crossglg} combine semantic understanding via LLMs with skeleton feature extraction, demonstrating the value of human movement dynamics in action recognition.    \\
% Inspired by the success of CLIP~\cite{CLIP} in the image domain, various CLIP-based models for video action recognition have emerged. These models aim to introduce spatio-temporal understanding into the CLIP representation space and can be classified as either partially fine-tuned or fully fine-tuned approaches. 
% Partially fine-tuned approaches~\cite{XCLIP, chen_2022_adaptformer, st_adapter, yang2023_AIM_ICLR, park2023_dualpath_CVPR} freeze the pre-trained CLIP parameters and make use of lightweight parameterized adapters to learn the temporal relationship among frames in videos. 
% For example, X-CLIP~\cite{XCLIP} introduces learnable prompting and inter-frame communication transformers to learn temporal relationships from the frozen CLIP representations. 
% On the other hand, fully fine-tuned approaches~\cite{ActionCLIP, liu2023_STAN_CVPR23, vificlip, FROSTER} directly update CLIP's parameters during training. ActionCLIP~\cite{ActionCLIP} introduces a pre-network prompt and post-network prompt to learn temporal features. ViFi-CLIP~\cite{vificlip} shows that training the CLIP encoders with a simple temporal pooling layer successfully learns useful representations for action recognition.
% These models learn rich temporal features in the CLIP representation space, but are shown to sacrifice generalization performance~\cite{vificlip}. FROSTER~\cite{FROSTER} addresses this by performing a feature distillation between the base CLIP model and their CLIP based video model. 
% % Until now, these approaches have only been applied to web video dataset and only utilize the RGB modality. In contrast, our proposed {\Large$\pi$}-CLIP is designed for ADL and makes use of both the RGB and human skeleton modalities.
% Other modalities, such as optical flow, audio etc., have been used in conjunction with CLIP for their respective downstream tasks. \\
% \textbf{Optical flow} is another modality that estimates motion of objects in subsequent frames of a video. It can be used in conjunction with video-based CLIP models for action recognition. However, in previous state of the art \cite{vpn++}, it has been established that optical flow is a weaker modality than 3D skeletons for action recognition for ADL. \\
% \textbf{Audio Modality} approaches using CLIP \cite{Wav2CLIP, Audioclip, clip4vla} have shown promising performance in audio captioning, audio classification, as well as video retrieval. AudioCLIP \cite{Audioclip} and CLIP4VLA \cite{clip4vla} align the audio modality with that of image and text modalities. Additionally, CLIP4VLA performs intramodal contrastive learning with augmentations of audio inputs. It encodes both verbal and non-verbal heterogeneous information from audio. Wav2CLIP \cite{Wav2CLIP} on the other hand performs distillation with cross projection contrastive loss. 
% However, unlike the audio domain, there lacks a large-scale open-world 3D human skeleton dataset. This is a major downside for using skeletons for contrastive learning as it requires substantial amounts of data for effective performance. 
% Furthermore, Activities of Daily living focuses on human-centric actions where modelling the human anatomy is more important than modelling human audio. Hence, 3D human skeletons can play a crucial role in ADL and require SCD-like learning mechanism for its introduction in the CLIP embedding space. \\
\textbf{Zero-Shot Skeleton Action Recognition.}
Zero-shot skeleton-based models aim to enable action classification of unseen classes using only human skeleton sequences. SynSE~\cite{synse} introduces a syntactically guided approach, using part-of-speech tags to enhance the alignment between skeleton and language spaces. SMIE~\cite{smie} employs mutual information estimation and maximization to globally align the two spaces. CrossGLG~\cite{crossglg} uses LLM descriptions of actions and cross-attention to guide a skeleton encoder during training, but only uses the skeleton encoder at inference. Closest to our work is STAR~\cite{star}, which aligns a Shift-GCN~\cite{cheng2020shiftgcn} skeleton encoder with a pre-trained transformer-based text encoder~\cite{CLIP} for zero-shot skeleton action recognition. STAR differs from our work in that it does not incorporate the RGB modality or investigate strategies to enhance the representations of VLMs and LVLMs.

% Many approaches leverage human skeleton sequences for Zero Shot Action Recognition~\cite{synse, smie, star}. SynSE~\cite{synse} introduces a syntactically guided approach, using part-of-speech tags to enhance the alignment between visual and semantic spaces, thereby improving compositional generalization capabilities. Meanwhile, SMIE~\cite{smie} employs mutual information estimation and maximization to globally align these spaces and incorporates a temporal constraint module to leverage the inherent dynamics of actions, thus enabling more accurate differentiation of similar actions over time. Some approaches such as~\cite{star, crossglg}, consist of a two-stream approach, a semantic understanding branch to encode the semantic information of action classes using LLMs and their relation to the joints of the skeleton and a skeleton encoder branch to obtain skeleton features guided by the joint level features from the semantic branch. These skeleton-based approaches demonstrate innovative strategies to enhance Zero Shot Action Recognition by exploiting the dynamic and structural nuances of human movement.

\textbf{Multi-modal Large Language Models.}
Large language models, such as ChatGPT~\cite{chatgpt} and LLaMA~\cite{llama}, have exhibited remarkable capabilities in language understanding and generation tasks. These capabilities have inspired many works to extend LLMs to incorporate additional modalities, such as video~\cite{videollama, videollava, videochatgpt}, audio~\cite{zhang2023_speechgpt}, and human pose~\cite{feng2024_chatpose}, typically through instruction tuning or the addition of modality-specific encoders with projection layers. Extending beyond this, other approaches show the possibility to incorporate a wide range of modalities into the LLM space
~\cite{lu2022_unifiedio, su2023_pandagpt}. Unique 
% ~\cite{lu2022_unifiedio, su2023_pandagpt, ye2024_xvila}. Unique 
from these approaches, our SKI-LVLM targets ADL and aims to train using multiple modalities (vision, language and skeleton), but only use vision and language during inference.