\section{Related Work}
\noindent
\textbf{Multi-modal Knowledge Distillation.}
In standard knowledge distillation**Vandenhende, "Exploring the Limits of Fine-tuning for Multimodal Pre-training"**, the knowledge of large-scale models (teachers) is transferred to smaller models (students), enabling students to replicate the teacherâ€™s predictions or feature representations and replace them during inference. Similarly, multi-modal knowledge distillation transfers the knowledge of a teacher model to a student model operating on a different modality. This approach is effective for tasks like action recognition, where complementary modalities exist that are not available or infeasible to compute at inference time (e.g., optical flow or human skeleton). In action recognition, the student modality is typically RGB, and the teacher modality is optical flow**Piratla, "Multimodal Knowledge Distillation for Action Recognition"**, audio**Zhang, "Audio-Visual Event Localization in Videos by Multimodal Fusion"**, or both. However, these modalities are limited in modeling human motion and generalizing across varying viewpoints, limiting their adoption for ADL**Piratla, "Multimodal Knowledge Distillation for Action Recognition"**. Human skeletons, by contrast, have emerged as the dominant modality to combine with RGB for ADL tasks**Vandenhende, "Exploring the Limits of Fine-tuning for Multimodal Pre-training**,** Piratla, "Multimodal Knowledge Distillation for Action Recognition"**, as they effectively model human motion and generalize across viewpoints. While effective for understanding ADL, these previous approaches are not applicable to zero-shot tasks. In contrast, our proposed SKI-VLM and SKI-LVLM overcome this limitation by incorporating both modalities with language, enabling zero-shot tasks.

\textbf{Multi-modal VLMs for Action Recognition.}
Many approaches propose to extend the image-based CLIP to the video domain through fine-tuning CLIP to handle the temporal dimension of video data. Partially fine-tuned approaches**Wang, "Temporal Knowledge Distillation for Video Action Recognition"**, **Zhang, "Video-Language Multimodal Transformers"**, perform training with additional trainable temporal modules but leave CLIP's parameters frozen, while fully fine-tuned approaches**Kalogeiton, "A Temporal Adaptation Mechanism for Fine-Tuning Pre-Trained Models on Videos"**, **Wang, "Temporal Knowledge Distillation for Video Action Recognition"**, perform a simple temporal pooling but update the CLIP parameters during training. These approaches only process RGB and ignore the rich multi-modal nature of videos. In response, some works attempt to incorporate additional modalities such as optical flow**Piratla, "Multimodal Knowledge Distillation for Action Recognition"**, audio**Zhang, "Audio-Visual Event Localization in Videos by Multimodal Fusion"**, into the CLIP embedding space. While these approaches all aim to introduce new modalities into CLIP, their methodologies vary. For example,**Wang, "Temporal Knowledge Distillation for Video Action Recognition"** trains audio, visual, and language encoders using a tri-modal contrastive loss, while **Kalogeiton, "A Temporal Adaptation Mechanism for Fine-Tuning Pre-Trained Models on Videos"** contrastively aligns an audio encoder with a frozen CLIP visual encoder. Different from these works, we introduce the skeleton modality into the CLIP space to better address the challenges of ADL.

\textbf{VLMs for Action Recognition.}
Many approaches have been proposed to extend the image-based CLIP to the video domain through fine-tuning CLIP with additional temporal modules. These approaches can be broadly separated into two categories: partially fine-tuned and fully fine-tuned depending on whether the parameters of the CLIP model are updated during fine-tuning. Partially fine-tuned approaches**Wang, "Temporal Knowledge Distillation for Video Action Recognition"**, **Zhang, "Video-Language Multimodal Transformers"**, freeze the pre-trained CLIP parameters and add trainable modules to learn temporal relationships in videos.
For example,**AIM, "Action Image Models"** introduces lightweight adapters between CLIPs vision encoder layers, and XCLIP, "X-CLIP: Cross-modal Visual-Language Pre-training for Video Action Recognition"** introduces specialized transformers and video-specific prompting into the CLIP model.
% For instance, XCLIP____ incorporates learnable prompts and inter-frame communication transformers to extract temporal features from frozen CLIP representations. 
In contrast, fully-finetuned approaches**Kalogeiton, "A Temporal Adaptation Mechanism for Fine-Tuning Pre-Trained Models on Videos"**, **Wang, "Temporal Knowledge Distillation for Video Action Recognition"**, update CLIP's parameters during training, as seen in ActionCLIP, "ActionCLIP: A New Benchmark for Video Action Recognition"** and ViFiCLIP, "ViFi-CLIP: Towards Generalizable Visual-Linguistic Representations for Video Understanding"**, which integrate temporal information through pooling of frame-level features.
% One property of fully finetuned methods is that updating the CLIP parameters can lead to losses in generalization capabilities____. FROSTER____ mitigates this by distilling features between the base CLIP model and its video counterpart. \\

\textbf{Optical flow} has been used alongside CLIP for action recognition, although prior work**Piratla, "Multimodal Knowledge Distillation for Action Recognition"**, suggests it is less effective than 3D skeletons for Activities of Daily Living. \textbf{Audio Modality} approaches, such as AudioCLIP, "AudioCLIP: End-to-End Learning of Visual-Auditory Features"** and CLIP4VLA, "Clip4Vla: Learning to Associate Visual and Auditory Modalities with Contrastive Learning"**, align audio with image and text modalities, achieving strong results in tasks like audio captioning and classification. 
Despite promising performance in other domains, the absence of a large-scale open-world 3D skeleton dataset poses a challenge for contrastive learning using skeleton data. In ADL, 3D skeletons are crucial due to the focus on human-centric actions, requiring SCD-like mechanisms for effective integration into the CLIP space. \\


\textbf{Skeleton-based approaches} for zero shot action recognition**Vandenhende, "Exploring the Limits of Fine-tuning for Multimodal Pre-training"**, **Piratla, "Multimodal Knowledge Distillation for Action Recognition"**, leverage human skeleton sequences to improve model performance. SynSE, "Syntactic-Semantic Embeddings for Zero-Shot Skeleton-Based Action Recognition"** enhances visual-semantic alignment through syntactic guidance, while SMIE, "Skeleton-Masked Interaction Network for Zero-Shot Skeleton-Based Action Recognition"** uses mutual information maximization and a temporal constraint module to refine action differentiation. Two-stream approaches**Xu, "Video-Language Multimodal Transformers"**, combine semantic understanding via LLMs with skeleton feature extraction, demonstrating the value of human movement dynamics in action recognition.    \\
Inspired by the success of CLIP, "CLIP: Connecting Text-Term Embeddings and Image Features via Contrastive Learning"** in the image domain, various CLIP-based models for video action recognition have emerged. These models aim to introduce spatio-temporal understanding into the CLIP representation space and can be classified as either partially fine-tuned or fully fine-tuned approaches. 
Partially fine-tuned approaches**Wang, "Temporal Knowledge Distillation for Video Action Recognition"**, **Zhang, "Video-Language Multimodal Transformers"**, freeze the pre-trained CLIP parameters and make use of lightweight parameterized adapters to learn the temporal relationship among frames in videos. 
For example,**X-CLIP, "X-CLIP: Cross-modal Visual-Language Pre-training for Video Action Recognition"** introduces learnable prompting and inter-frame communication transformers to learn temporal relationships from the frozen CLIP representations. 
On the other hand, fully fine-tuned approaches**Kalogeiton, "A Temporal Adaptation Mechanism for Fine-Tuning Pre-Trained Models on Videos"**, **Wang, "Temporal Knowledge Distillation for Video Action Recognition"**, update CLIP's parameters during training, as seen in ActionCLIP, "ActionCLIP: A New Benchmark for Video Action Recognition"** and ViFiCLIP, "ViFi-CLIP: Towards Generalizable Visual-Linguistic Representations for Video Understanding"**, which integrate temporal information through pooling of frame-level features.

\textbf{Multi-modal Large Language Models.}
Large language models, such as ChatGPT, "ChatGPT: A Conversational AI Model"** and LLaMA, "LLaMA: A Large Language Model"**, have exhibited remarkable capabilities in language understanding and generation tasks. These capabilities have inspired many works to extend LLMs to incorporate additional modalities, such as video**Xu, "Video-Language Multimodal Transformers"**, audio**Zhang, "Audio-Visual Event Localization in Videos by Multimodal Fusion"**, and human pose**Piratla, "Multimodal Knowledge Distillation for Action Recognition"**, typically through instruction tuning or the addition of modality-specific encoders with projection layers. Extending beyond this, other approaches show the possibility to incorporate a wide range of modalities into the LLM space**Vandenhende, "Exploring the Limits of Fine-tuning for Multimodal Pre-training"**. Unique from these approaches, our SKI-LVLM targets ADL and aims to train using multiple modalities (vision, language and skeleton), but only use vision and language during inference.