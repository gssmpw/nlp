\section{Introduction} ~\label{Sec:Introduction}
Currently, UAV technology is approaching maturity and is sufficient to provide reliable assistance in fields such as agroforestry production management~\cite{jia2024maize,zheng2024robust}, emergency rescue~\cite{quero2025unmanned,wen2024route}, and security monitoring~\cite{zhu2025multiscale}. In the production of agricultural and forestry crops, UAV, by carrying multi-modal or high-resolution camera sensors, can quickly acquire image information of large-scale farmland and orchards, providing strong technical support for crop detection, yield estimation, and automated management in precision agriculture~\cite{ariza2024object}. Therefore, UAV technology is widely applied in the agricultural field. However, object detection combined with UAV technology faces many challenges, such as lighting changes, foliage occlusion, and the diversity of target scales~\cite{liang2025enhanced,du2023dsw,wu2024walnut}. These problems significantly increase the detection difficulty and limit the performance of existing algorithms in practical applications.

As a crop of great value~\cite{yang2025walnut}, the green walnut has a complex surface texture, a high color similarity to the background, and is often affected by the occlusion of branches, leaves, and lighting changes. These characteristics make it a research object with unique scientific challenges and engineering application value in agricultural object detection. In the future of the smart walnut industry, in automated applications such as aerial UAV picking robots, accurate object detection is not only the basis for crop positioning but also the core prerequisite for robot path planning, obstacle avoidance decision-making, and picking priority judgment. If the impact of environmental interference on the apparent characteristics of the target is ignored, the reliability of the detection algorithm will directly affect the efficiency and success rate of the robot's task execution. Therefore, data-driven automated management methods for walnut production will greatly need a large-scale dataset with higher-granularity target features.

Currently, most UAV-based object detection datasets are related to urban road environments, such as VisDrone~\cite{zhu2021detection} and UAVDT~\cite{du2018unmanned}, or datasets for maritime object detection, such as SeaDronesSee~\cite{varga2022seadronessee} and SDS-ODv2~\cite{kiefer20231st}. There are only relatively few open-source datasets for UAV-based object detection of agricultural and forestry crops. In addition, most of the object detection datasets related to agricultural crops are obtained by shooting with mobile phones or hand-held cameras, such as MinneApple~\cite{hani2020minneapple} and TomatoPlantfactoryDataset~\cite{wu2023dataset}. Therefore, this study aims to construct a large-scale walnut dataset obtained from UAV aerial photography and make it open-source worldwide.

Although UAV technology provides an efficient means of data collection for the object detection of agricultural and forestry crops, existing research mostly regards agricultural and forestry crops as a single category, ignoring the differences in apparent characteristics caused by environmental interference, such as backlight, frontal light, and occlusion. Although this simplification can improve the detection accuracy in the short term, it is difficult to meet the fine-grained perception requirements of picking robots for target states, thus severely restricting the development of their autonomous capabilities. Solving the detection problem of walnut fruits can provide a transferable technical paradigm for the automated management of other agricultural crops such as citrus and apples.

Therefore, to solve the above-mentioned series of current problems and meet the requirements of the smart walnut field, this study introduces the first large-scale UAV low-altitude remote-sensing green walnut object detection dataset - \textbf{WalnutData}. This dataset includes a total of 30,240 RGB images with a resolution of 1,024Ã—1,024 pixels, and the total number of annotated instances is as high as 706,208. It innovatively divides the targets into four environmental states, including A1 (being illuminated by frontal light and unoccluded), A2 (being backlit and unoccluded), B1 (being illuminated by frontal light and occluded), and B2 (being backlit and occluded), as shown in Fig.~\ref{fig:Examples of WalnutData Categories}. The main contributions of this research are as follows:
\begin{itemize}[left=0pt]
	\item  As far as we know, the WalnutData is the largest green walnut object detection dataset with annotation labels in the field of agricultural computer vision. This dataset refines the lighting and occlusion conditions of walnut fruits and has multiple categories. It can be used for the further development of object detectors in automated applications within the intelligent walnut production management.
	
	\item We used the WalnutData to conduct benchmark tests on the current mainstream one-stage detection algorithms such as DETR and the YOLO series, as well as two-stage detection algorithms like Fast R-CNN and Faster R-CNN. These algorithms can serve as the basis for future algorithm design.
\end{itemize}


