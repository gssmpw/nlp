\setlength{\tabcolsep}{10pt} 
\begin{table}
	\centering
	\caption{The Val benchmark evaluation results of one-stage object detection algorithms in the Ultralytics framework for WalnutData. The top two results in the evaluation are represented by bold and underline respectively. The top two in the mAP50 metric are YOLOv3 (95.1\%) and YOLOv3-SPP (94.9\%) respectively, and the top two in the mAP50:95 metric are YOLOv8x (72.7\%) and YOLOv10x (72.6\%) respectively.}
	\label{tab:Table_6}
	\begin{tabular}{lcccc}
		\toprule
		Method & Size & GFLOPs & mAP50  & mAP50:95  \\
		\midrule
		\multirow{3}{*}{YOLOv3~\cite{redmon2018yolov3}} 
		& - & 154.6 & \underline{94.9} & 71.6 \\
		& SPP & 155.4 & \textbf{95.1} & 71.4 \\
		& Tiny & 12.9 & 66.2 & 38.0 \\
		\midrule
		YOLOv4~\cite{bochkovskiy2020yolov4} & - & - & 57.3 & 31.3 \\
		\midrule
		\multirow{5}{*}{YOLOv5~\cite{jocher2022ultralytics}} 
		& n & 4.1 & 72.0 & 45.1 \\
		& s & 15.8 & 84.5 & 57.0 \\
		& m & 47.9 & 90.9 & 64.7 \\
		& l & 107.7 & 93.3 & 68.4 \\
		& x & 203.8 & 94.5 & 70.8 \\
		\midrule
		\multirow{4}{*}{YOLOv6~\cite{li2022yolov6}} 
		& n & - & 74.2 & 47.8 \\
		& s & - & 87.8 & 59.6 \\
		& m & - & 83.7 & 56.9 \\
		& l & - & 87.1 & 60.1 \\
		\midrule
		YOLOv7~\cite{wang2023yolov7} & - & - & 67.0 & 40.1 \\
		\midrule
		\multirow{5}{*}{YOLOv8~\cite{jocher2022ultralytics}} 
		& n & 8.1 & 75.2 & 49.2 \\
		& s & 28.4 & 86.2 & 59.7 \\
		& m & 78.7 & 92.2 & 68.0 \\
		& l & 164.8 & 93.6 & 70.6 \\
		& x & 257.4 & 94.6 & \textbf{72.7} \\
		\midrule
		\multirow{5}{*}{YOLOv9~\cite{wang2024yolov9}} 
		& t & - & 72.2 & 47.3 \\
		& s & - & 80.9 & 55.3 \\
		& m & - & 89.7 & 64.1 \\
		& c & - & 92.1 & 67.8 \\
		& e & - & 93.8 & 70.6 \\
		\midrule
		\multirow{6}{*}{YOLOv10~\cite{wang2025yolov10}} 
		& n & 8.2 & 75.9 & 49.6 \\
		& s & 24.5 & 86.3 & 59.9 \\
		& m & 63.4 & 89.1 & 63.4 \\
		& b & 98.0 & 90.9 & 65.9 \\
		& l & 126.3 & 92.0 & 67.6 \\
		& x & 169.8 & 94.4 & \underline{72.6} \\
		\midrule
		\multirow{5}{*}{YOLOv11~\cite{khanam2024yolov11}} 
		& n & 6.3 & 74.6 & 48.5 \\
		& s & 21.3 & 84.7 & 58.7 \\
		& m & 67.7 & 91.3 & 66.7 \\
		& l & 86.6 & 91.7 & 68.0 \\
		& x & 194.4 & 94.0 & 71.7 \\
		\bottomrule
	\end{tabular}
\end{table}

\setlength{\tabcolsep}{4.5pt} 
\begin{table}
	\centering
	\caption{The Val benchmark evaluation results of one-stage object detection algorithms in the mmdetection framework for WalnutData. The top two results in the evaluation are represented by bold and underline respectively. The top two in the AP50:95 metric are YOLOX-x (54.9\%) and YOLOX-l (51.6\%) respectively, the top two in the AP50 metric are YOLOX-x (82.7\%) and YOLOX-l (79.1\%) respectively, and the top two in the AP75 metric are YOLOX-x (67.5\%) and YOLOX-l (61.7\%) respectively.}
	\label{tab:Table_7}
	\begin{tabular}{lccccccc}
		\toprule
		Method           &  Size  &  Backbone  &  AP  & AP50 & AP75 &  \\ \midrule
		\multirow{3}{*}{YOLOX~\cite{ge2021yolox}} &   s    & CSPDarknet & 45.4 & 72.8 & 52.9 &  \\
		&   l    & CSPDarknet & \underline{51.6} & \underline{79.1} & \underline{62.7} &  \\
		&   x    & CSPDarknet & \textbf{54.9} & \textbf{82.7} & \textbf{67.5} &  \\ \midrule
		DETR~\cite{carion2020end}          &   -    &  ResNet50  & 14.1 & 34.7 & 8.0  &  \\
		Deformable DETR~\cite{zhu2020deformable}     &   -    &  ResNet50  & 49.2 & 76.8 & 59.7 &  \\
		DINO~\cite{zhang2022dino}         & 4scale &  ResNet50  & 50.3 & 77.0 & 61.7 &  \\
		Conditional DETR~\cite{meng2021conditional}    &   -    &  ResNet50  & 37.9 & 65.5 & 40.9 &  \\ \bottomrule
	\end{tabular}
\end{table}

\setlength{\tabcolsep}{12pt} 

\begin{table*}
	\centering
	\caption{The Val benchmark evaluation results of the one-stage object detection algorithm under the Ultralytics framework for each category in the WalnutData.  The top two results in the evaluation are represented by bold and underline respectively. In terms of the mAP50 metric, YOLOv3-SPP demonstrates the best detection performance under conditions such as occlusion and backlighting. For the mAP50:95 metric, YOLOv8x shows higher accuracy in the B1 and B2 categories, reaching 73.1\% and 68.8\%, respectively. In the case of no occlusion, regardless of front lighting or backlighting, the difference between YOLOv8x and the first place (YOLOv10x) is only 0.1\%.}
	\label{tab:Table_8}
	\begin{tabular}{lccccccccc}
		\toprule
		
		\multirow{2}{*}{Method} & \multirow{2}{*}{Size} & \multicolumn{4}{c}{mAP50} & \multicolumn{4}{c}{mAP50:95} \\
		
	\cmidrule(lr){3-6} \cmidrule(lr){6-10}
		& & A1 & B1 & B2 & A2 & A1 & B1 & B2 & A2 \\
		\midrule
		\multirow{3}{*}{YOLOv3~\cite{redmon2018yolov3}}
		& - & 96.4 & \underline{96.0} & \underline{94.6} & 92.7 & 75.3 & 71.9 & \underline{68.6} & 70.7 \\
		& SPP & 96.4 & \textbf{96.1} & \textbf{94.7} & \textbf{93.0} & 74.9 & 71.5 & 68.3 & 70.9 \\
		& Tiny & 73.9 & 71.7 & 64.9 & 54.1 & 46.9 & 39.2 & 33.0 & 33.0 \\
		\midrule
		YOLOv4~\cite{bochkovskiy2020yolov4} & - & 74.0 & 54.7 & 49.8 & 40.6 & 44.2 & 32.7 & 23.7 & 24.6 \\
		\midrule
		\multirow{5}{*}{YOLOv5~\cite{jocher2022ultralytics}}
		& n & 80.5 & 78.1 & 68.9 & 60.4 & 54.5 & 47.2 & 39.1 & 39.6 \\
		& s & 89.0 & 87.9 & 84.3 & 76.9 & 63.9 & 58.2 & 52.7 & 53.4 \\
		& m & 93.1 & 92.8 & 90.9 & 86.6 & 69.6 & 65.2 & 61.2 & 62.8 \\
		& l & 94.9 & 94.7 & 93.4 & 90.1 & 72.5 & 68.8 & 65.5 & 66.9 \\
		& x & 95.9 & 95.8 & 94.3 & 91.9 & 74.6 & 71.2 & 68.1 & 69.5 \\
		\midrule
		YOLOv7~\cite{wang2023yolov7} & - & 80.1 & 74.0 & 61.6 & 52.3 & 51.1 & 41.6 & 33.9 & 33.8 \\
		\midrule
		\multirow{5}{*}{YOLOv8~\cite{jocher2022ultralytics}}
		& n & 85.2 & 80.9 & 69.6 & 65.2 & 59.5 & 51.1 & 41.9 & 44.2 \\
		& s & 92.2 & 89.4 & 82.1 & 81.1 & 67.3 & 60.8 & 53.3 & 57.3 \\
		& m & 95.7 & 93.9 & 89.7 & 89.3 & 73.7 & 68.6 & 62.8 & 66.8 \\
		& l & 96.2 & 95.0 & 91.8 & 91.5 & 75.4 & 71.1 & 66.3 & 69.6 \\
		& x & \textbf{96.8} & 95.7 & 93.2 & \underline{92.9} & \underline{77.0} & \textbf{73.1} & \textbf{68.8} & \underline{71.8} \\
		\midrule
		\multirow{5}{*}{YOLOv9~\cite{wang2024yolov9}} 
		& t & 82.7 & 78.5 & 66.6 & 61.0 & 57.7 & 50.0 & 40.1 & 41.4 \\
		& s & 88.4 & 85.5 & 76.8 & 72.9 & 64.1 & 57.3 & 48.7 & 51.0 \\
		& m & 94.3 & 92.0 & 86.5 & 85.8 & 70.9 & 64.8 & 58.3 & 62.3 \\
		& c & 95.8 & 93.8 & 89.6 & 89.2 & 73.7 & 68.4 & 62.6 & 66.4 \\
		& e & \textbf{96.8} & 95.1 & 91.3 & 92.0 & 75.8 & 71.1 & 65.7 & 70.0 \\
		\midrule
		\multirow{6}{*}{YOLOv10~\cite{wang2025yolov10}} 
		& n & 85.2 & 80.8 & 70.6 & 67.0 & 59.3 & 51.1 & 42.4 & 45.4 \\
		& s & 92.1 & 89.2 & 82.5 & 81.6 & 67.5 & 60.6 & 53.6 & 58.0 \\
		& m & 94.0 & 91.6 & 85.8 & 85.1 & 70.1 & 64.2 & 57.6 & 61.7 \\
		& b & 94.8 & 93.0 & 88.1 & 87.8 & 71.7 & 66.5 & 60.7 & 64.6 \\
		& l & 95.5 & 93.6 & 89.3 & 89.4 & 73.0 & 67.9 & 62.6 & 66.7 \\
		& x & \underline{96.7} & 95.5 & 92.6 & 92.6 & \textbf{77.1} & \underline{72.8} & 68.5 & \textbf{71.9} \\
		\midrule
		\multirow{5}{*}{YOLOv11~\cite{khanam2024yolov11}} 
		& n & 84.3 & 79.9 & 69.1 & 65.0 & 58.5 & 50.4 & 41.3 & 43.7 \\
		& s & 91.3 & 88.0 & 80.5 & 79.2 & 66.7 & 59.8 & 52.0 & 56.0 \\
		& m & 95.0 & 93.1 & 88.7 & 88.5 & 72.5 & 67.3 & 61.4 & 65.8 \\
		& l & 95.2 & 93.3 & 89.4 & 88.9 & 73.5 & 68.5 & 63.2 & 66.8 \\
		& x & 96.5 & 95.2 & 92.2 & 91.9 & 76.3 & 72.2 & 67.5 & 70.8 \\
		\bottomrule
	\end{tabular}
\end{table*}


\begin{table*}
	\centering
	\caption{The benchmark evaluation results for small, medium, and large object detection on the WalnutData Val using a one-stage object detection algorithm under the mmdetection framework.  The top two results in the evaluation are represented by bold and underline respectively. YOLOX-x demonstrates the best performance for small and medium-sized object detection, with YOLOX-l ranking second for small objects and Deformable DETR ranking second for medium-sized objects. Additionally, Deformable DETR is more sensitive in detecting large objects, with an accuracy that exceeds the second-place method (DINO) by 7.8\%. The corresponding AR metric also places it in second place (72.5\%).}
	\label{tab:Table_9}
	\setlength{\tabcolsep}{8pt}
	\begin{tabular}{lccccccc}
		\toprule
		Method & Size & AP-small & AP-medium & AP-large & AR-small & AR-medium & AR-large \\
		\midrule
		\multirow{3}{*}{YOLOX~\cite{ge2021yolox}} & s & 44.1 & 46.8 & 37.3 & 64.1 & 64.3 & 45.9 \\
		& l & \underline{50.1} & 53.3 & 44.5 & 67.0 & 67.8 & 55.6 \\
		& x & \textbf{53.5} & \textbf{56.3} & 52.7 & \textbf{68.4} & 68.4 & 58.4 \\
		\midrule
		DETR~\cite{carion2020end} & - & 10.3 & 18.6 & 32.4 & 24.5 & 39.6 & 46.7 \\
		Deformable DETR~\cite{zhu2020deformable} & - & 43.7 & \underline{55.4} & \textbf{64.7} & 59.4 & \underline{69.5} & \underline{72.5} \\
		DINO~\cite{zhang2022dino} & 4scale & 47.5 & 53.5 & \underline{56.9} & \underline{68.1} & \textbf{72.9} & \textbf{72.8} \\
		Conditional DETR~\cite{meng2021conditional} & - & 32.3 & 44.5 & 56.1 & 49.5 & 63.7 & 68.0 \\
		\bottomrule
	\end{tabular}
\end{table*}

\setlength{\tabcolsep}{9pt} 
\begin{table*}
	\centering
	\caption{The benchmark evaluation results for the two-stage object detection algorithms on WalnutData Val. The top two results in the evaluation are represented by bold and underline respectively. In the evaluation results, Cascade R-CNN (ResNet101) demonstrates impressive performance, ranking first in all metrics, with the exception of large object detection, where it ranks second (74.5\%).}
	\label{tab:Table_10}
	\begin{tabular}{lcccccccc}
		\toprule
		Method & Backbone &   AP  & AP50  & AP75 & AP-small  & AP-medium  & AP-large  \\ 
		\midrule
		Fast R-CNN~\cite{girshick2015fast} & ResNet50 &   22.9 & 35.9 & 26.7 & 15.7 & 31.4 & 54.5 \\ 
		Faster R-CNN~\cite{ren2015faster} & ResNet50 &   51.5 & 79.7 & 62.2 & 45.6 & 58.2 & 69.7 \\ 
		Cascade R-CNN~\cite{cai2018cascade} & ResNet50 &  56.0 & 83.7 & 68.4 & 50.3 & 62.4 & 72.6 \\ 
		Grid R-CNN~\cite{lu2019grid} & ResNet50 &   53.8 & 80.4 & 66.1 & 48.2 & 60.2 & 71.9 \\ 
		TridentNet~\cite{li2019scale} & ResNet50 &   53.4 & 80.8 & 64.2 & 48.9 & 58.6 & 69.9 \\ 
		Double head R-CNN~\cite{wu2020rethinking} & ResNet50 &  55.2 & \underline{84.5} & 67.2 & 50.1 & 61.1 & 69.6 \\ 
		Sparse R-CNN~\cite{sun2021sparse} & ResNet50 &   45.3 & 68.8 & 55.8 & 40.6 & 50.9 & 53.8 \\ 
		\midrule
		Fast R-CNN~\cite{girshick2015fast} & ResNet101 &   24.5 & 37.7 & 28.9 & 16.8 & 33.7 & 55.7 \\ 
		Faster R-CNN~\cite{ren2015faster} & ResNet101 & 56.3 & 84.1 & 68.9 & 49.8 & 63.7 & 72.1 \\ 
		Cascade R-CNN~\cite{cai2018cascade} & ResNet101 &  \textbf{58.9} & \textbf{85.9} & \textbf{72.5} & \textbf{52.7} & \textbf{65.7} & \underline{74.5} \\ 
		Grid R-CNN~\cite{lu2019grid} & ResNet101 &  \underline{57.5} & 83.1 & \underline{70.9} & \underline{51.2} & \underline{64.9} & \textbf{75.3} \\ 
		Sparse R-CNN~\cite{sun2021sparse} & ResNet101 & 46.9 & 71.2 & 57.7 & 42.1 & 52.5 & 59.9 \\ 
		\bottomrule
	\end{tabular}
\end{table*}

\section{Experimental Evaluation}
We evaluated some of the more popular object detection models in recent years on WalnutData, and implemented one-stage and two-stage object detection algorithms using the ultralytics framework~\cite{jocher2022ultralytics} and the mmdetection framework~\cite{MMDetection_Contributors_OpenMMLab_Detection_Toolbox_2018} respectively. The one-stage object detection algorithms include YOLOv3~\cite{redmon2018yolov3}, YOLOv4~\cite{bochkovskiy2020yolov4}, YOLOv5~\cite{jocher2022ultralytics}, DETR~\cite{carion2020end}, etc.; the two-stage object detection algorithms include Fast R-CNN~\cite{girshick2015fast}, Faster R-CNN~\cite{ren2015faster}, TridentNet~\cite{li2019scale}, etc. In the following content, the evaluation results of each algorithm on instances of various categories and sizes in WalnutData will be announced. All the experiments of the models in this study were carried out on servers equipped with 8 RTX 3090 GPUs or A800 GPUs, and the hyperparameters of the baseline models all used the default parameter values. In addition, the evaluation results of these baseline models will be provided as benchmark values of WalnutData for researchers as a reference.  

\subsection{Baselines of One-Stage Object Detection Algorithms}
We conducted benchmark evaluations on WalnutData using YOLOv3~\cite{redmon2018yolov3}, YOLOv4~\cite{bochkovskiy2020yolov4}, YOLOv5~\cite{jocher2022ultralytics}, YOLOv6~\cite{li2022yolov6,wang2023yolov7,jocher2022ultralytics,wang2024yolov9,wang2025yolov10} to YOLOv11~\cite{khanam2024yolov11}, as well as YOLOX~\cite{ge2021yolox}, DETR~\cite{carion2020end}, Deformable DETR~\cite{zhu2020deformable}, DINO~\cite{zhang2022dino}, and Conditional DETR~\cite{meng2021conditional}. From the evaluation results on the validation set of WalnutData (Table~\ref{tab:Table_6}), it can be seen that YOLOv3 (154.6) and YOLOv3-SPP (155.4) with relatively high GFLOPs rank first and second in terms of the mAP50 metric. However, in terms of the mAP50:95 metric, YOLOv8x (72.7\%) and YOLOv10x (72.6\%) have better detection performance. Under the mmdetection framework, we used AP50:95, AP50, and AP75 as evaluation metrics. Among the evaluation metric results of these models (Table~\ref{tab:Table_7}), YOLOX shows the strongest performance.

In Table~\ref{tab:Table_8} and Table~\ref{tab:Table_9}, we will present the detection accuracies of these one-stage object detection algorithms for each category and for small, medium, and large-sized targets. In the evaluation results of the benchmark algorithms in Table~\ref{tab:Table_8}, YOLOv3-SPP performs the best in terms of the mAP50 metric. In addition, in the evaluation results of the mAP50:95 metric, YOLOv8x achieves 77.0\%, 73.1\%, 68.8\%, and 71.8\% for the A1, B1, B2, and A2 categories respectively, and its overall strength is the highest among other algorithms. Moreover, as can be seen from Table~\ref{tab:Table_9}, in the detection of small and medium-sized targets, YOLOX-x has the highest AP values, reaching 53.5\% and 56.3\% respectively, followed by YOLOX-l (50.1\%, 53.3\%) and Deformable DETR (43.7\%, 55.4\%). In the detection of large-sized targets, compared with other algorithms, Deformable DETR has a huge advantage in detection performance, and the corresponding AR metric (72.5\%) also ranks second.

\subsection{Baselines of two-stage object detection algorithms}
This study uses several popular two-stage object detection algorithms in recent years: Fast R-CNN~\cite{girshick2015fast}, Faster R-CNN~\cite{ren2015faster}, Cascade R-CNN~\cite{cai2018cascade}, Grid R-CNN~\cite{lu2019grid}, TridentNet~\cite{li2019scale}, Double Head R-CNN~\cite{wu2020rethinking}, and Sparse R-CNN~\cite{sun2021sparse}, and evaluates these algorithms on WalnutData. As shown in Table~\ref{tab:Table_10}, in the evaluation of two-stage object detection algorithms, Cascade R-CNN ranks first in overall performance, with the best detection results for small objects (52.7\%) and medium-sized objects (65.7\%). Grid R-CNN ranks second, with slightly lower detection performance for small and medium-sized objects compared to Cascade R-CNN, achieving 51.2\% and 64.9\%, respectively. However, Grid R-CNN outperforms Cascade R-CNN in detecting large objects.

