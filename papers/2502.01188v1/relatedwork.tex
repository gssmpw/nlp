\section{Related work}
This section discusses the most recent research in algorithmic discrimination, as well as the application of decision trees in uplift modeling. It also highlights how our proposed work connects these two domains. 

\textbf{Algorithmic discrimination :} 
\label{lit-review-disc}
Traditional pre-processing based techniques aim to produce “balanced” datasets that can be fed into any machine learning model. These techniques can be categorized as relabelling, resampling, and data transformation techniques. In relabelling, only the target values of a few training samples are changed \citep{kamiran2009classifying, luong2011k}. Resampling involves duplicating or dropping specific samples, or assigning weights, as in the reweighing method proposed by \citet{kamiran2012data}. Data transformation techniques manipulate the training features and their labels to generate new training sets. \citet{calmon2017optimized} propose an `optimized pre-processing algorithm' to minimize discrimination while controlling the distortion and utility in the resulting transformed data. However, the optimization programs can be computationally very expensive to solve. Disparate impact remover \citep{feldman2015certifying} is another data transformation technique, focused on satisfying the disparate impact fairness criteria, also known as the 80\% rule \citep{usgovernment2011}. It applies a repair procedure that removes the relationships between protected attribute and other input features. Recently, pre-processing approaches based on adversarial learning have also been developed \citep{madras2018learning}. However, these techniques are complex, require more computation power, and are difficult to interpret in terms of the resulting representations. In contrast, our proposed approach is focused on identification and relabeling of discriminatory subgroups in the data by building interpretable decision trees and is thus more intuitive.

In the domain of discrimination aware data mining, many in-processing approaches are also proposed for bias mitigation. A recent study analyzed fairness in in-processing algorithms \citep{wan2023processing} and divided these mitigation approaches into two categories: explicit and implicit mitigation methods. Explicit mitigation refers to modification of the objective function by either adding a regularization term \cite{jiang2020wasserstein, aghaei2019learning, agarwal2019fair} or by constraint optimization \citep{Saxena2024, garcia2021maxmin, lahoti2020fairness, garg2019counterfactual}. Implicit methods aim to improve latent representations by learning adversarial \citep{sweeney2020reducing, zhang2018mitigating}, disentangled \citep{kim2021counterfactual, park2021learning} and contrastive \citep{cheng2020fairfil, zhou2021contrastive} representations. Discrimination aware post-processing techniques, on the other hand, modify the output of a standard learning algorithm by using some non-discriminatory constraints \citep{kamiran2012icdm, hardt2016equality, Lohia2019, awasthi2020equalized}.

\textbf{Discrimination-aware decision trees:} Decision trees are a widely used supervised machine learning algorithm for classification and regression tasks. They have a hierarchical structure with branches, nodes, and leaves. The algorithm splits the dataset at each internal node based on decision rules from data attributes, leading to leaf nodes with the final classification or regression outcomes.

In traditional decision trees, each split aims to maximize information gain, resulting in branches with more homogeneous subsets of data, thereby improving prediction accuracy. However, in discrimination-aware decision trees, the objective is to balance accuracy with fairness. This involves a multi-objective optimization at each split, ensuring that outcomes are equitable across sensitive groups. The concept was first introduced by \citet{kamiran2010discrimination}, who modified the splitting criteria and applied a leaf relabeling approach to achieve non-discriminatory outcomes.
Recent approaches in constructing discrimination aware decision trees are more focused on modifying existing splitting criterion to account for fairness besides maximizing predictive performances. For example, \citet{raff2018fair} uses the difference method, originally proposed by \cite{kamiran2010discrimination}, to calculate information gain in CART decision Trees. Similarly, work by \citet{aghaei2019learning} added fairness constraints to the loss function of CART trees using Mixed-Integer Programming (MIP) model. The work of \cite{valdivia2020fair} involves optimizing multiple objective functions of decision trees using genetic algorithms. Work by \cite{zhang2020feat} propose modified hoeffding trees for maintaining fairness in streaming data. A more recent study introduced FFTree \citep{castelnovo2022fftree}, an in-processing algorithm that uses decision trees to select multiple fairness criteria and sensitive attributes at each split. The goal of FFTree is to select best split where information gain is optimal with respect to both fairness criteria and sensitive attributes. However, because of their flexibility in picking multiple fairness criteria and sensitive features, FFTree may suffer greater losses in accuracy than typical discrimination-aware decision trees.  

\textbf{Uplift modeling :} 
Most work in the domain of machine learning and Causal Inference (CI) has been focused on structural causal modeling i.e. learning causal graphs and counterfactuals. Another goal of CI is to measure the effect of potential cause (e.g. event, policy, treatment) on some outcome. Uplift modeling is associated with this potential outcomes framework of CI modeling.
The major focus of uplift modeling is to measure the effect of an action or a treatment (e.g. marketing campaign) on customer outcome \citep{gutierrez2017causal}. The term “uplift” specifically refers to estimating the differences between the buying behavior of customers given the promotion (treatment) and those without it (control) \citep{ jaskowski2012uplift}. Current uplift modeling techniques can be distributed in three categories: tree based uplift models and its ensembles, SVM based models and generative deep learning methods. While generative deep learning models are more recently introduced, we will continue our discussion on tree based uplift models since they are the main focus of our approach. Most of the existing tree based uplift modeling strategies used modified splitting criteria for measuring treatment effect \citep{gaoutboost2023}. For example, uplift incremental value modeling \citep{hansotia2002incremental} and uplift t-statistics tree \citep{Su2009subgroup} tends to maximize the splitting criteria by measuring conditional average treatment effect between left and right child node of a tree. However, work by \citet{rzepakowski2010decision} tries to maximize the difference of average outcome between control and treatment groups within a single decision tree node.

To the best of our knowledge, FairUDT is the first paper to study uplift modeling in terms of discrimination identification. A similar approach in the literature is proposed \citep{he2020inherent}, where causal trees are used for the discovery of discriminatory subgroups and the tree splitting criteria is based on measuring Heterogeneous Treatment Effect (HTE). A limitation of using causal trees is that it requires consistency estimates i.e. balanced number of favored and deprived individuals in the leaf nodes \citep{Athey7353}. This is practically impossible in bias quantification since deprived individuals are present in minorities mostly. Our approach, on contrary, deals with measuring differences in probability distribution between favored and deprived groups and does not require any such assumption.

 \textbf{Discrimination identification using uplift modeling :} Discrimination can be defined as the {\it class probabilities difference} among favored and deprived distributions for the groups exhibiting the same characteristics or features. This notion of discrimination identification is aligned with the idea of uplift modeling where \textbf{incremental impact} for treatment group was differentiated from control group \citep{rzepakowski2010decision}. In a marketing campaign, given a treatment and control set of users, uplift is measured as the difference between response rates for the two sets. Making use of the same analogy for a conventional discriminatory setting, the \textbf{disparate impact} between the favored and deprived groups can be measured by considering the difference between the acceptance and rejection rates among two groups. For example, given similar qualifications among members of both groups applying for the same job, how much is one group preferred over other.
 

 \textbf{Interpretable Decision Trees :} In the field of machine learning, interpretability is defined as the ability to explain models and their outputs in a manner that is comprehensible to humans. However, quantitative notions of interpretability are largely absent from the machine learning literature, complicating the comparison of models without human intervention \cite{aghaei2023optimalfair}. One such measure is sparsity, which refers to the simplicity of models within the same class. In the context of decision trees, \citet{rudin2022interpretable} defines sparsity as the number of leaves in the tree, where trees with fewer leaves are considered sparser and therefore, more desirable. Another important measure is simulatability \cite{Scarpato2024}, i.e. the ease of the humans to interpret part or all of the model decisions. Shallow trees are known to be most simulatable as the if-else decision rules generated by these trees are easily understandable by humans. Some studies \cite{molnar2022, carvalho2019machine} suggested that the depth of the decision tree can also be taken as a complexity indicator, where more depth means more complex model.