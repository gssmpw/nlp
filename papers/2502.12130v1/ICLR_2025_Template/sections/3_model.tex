\section{Model}
\begin{figure*}[t]
  \centering
  \includegraphics[width=\textwidth]{figures/framework_fig2.pdf}
   \caption{
   The pipeline of our \Model framework. We first generate an initial task instruction using LLMs with in-context learning and sample trajectories aligned with the initial language instructions in the environment. Next, we use the LLM to summarize the sampled trajectories and generate refined task instructions that better match these trajectories. We then modify specific actions within the trajectories to perform new actions in the environment, collecting negative trajectories in the process. Using the refined task instructions, along with both positive and negative trajectories, we train a lightweight reward model to distinguish between matching and non-matching trajectories. The learned reward model can then collaborate with various LLM agents to improve task planning.
   }
   \label{fig:pipeline}
\end{figure*}

In this section, we provide a detailed introduction to our framework, autonomous Agents from automatic Reward Modeling And Planning (\Model). The framework includes automated reward data generation in section~\ref{sec:data}, reward model design in section~\ref{sec:model}, and planning algorithms in section~\ref{sec:plan}.

\subsection{Background}
The planning tasks for LLM agents can be typically formulated as a Partially Observable Markov Decision Process (POMDP): $(\mathcal{X}, \mathcal{S}, \mathcal{A}, \mathcal{O}, \mathcal{T})$, where:
\begin{itemize}
    \item $\mathcal{X}$ is the set of text instructions;
    \item $\mathcal{S}$ is the set of environment states;
    \item $\mathcal{A}$ is the set of available actions at each state;
    \item $\mathcal{O}$ represents the observations available to the agents, including text descriptions and visual information about the environment in our setting;
    \item $\mathcal{T}: \mathcal{S} \times \mathcal{A} \rightarrow \mathcal{S}$ is the transition function of states after taking actions, which is given by the environment in our settings. 
\end{itemize}

Given a task instruction $\mathit{x} \in \mathcal{X}$ and the initial environment state $\mathit{s_0} \in \mathcal{S}$, planning tasks require the LLM agents to propose a sequence of actions ${\{a_n\}_{n=1}^{N}}$ that aim to complete the given task, where $a_n \in \mathcal{A}$ represents the action taken at time step $n$, and $N$ is the total number of actions executed in a trajectory.
Following the $n$-th action, the environment transitions to state $\mathit{s_{n}}$, and the agent receives a new observation $\mathit{o_{n}}$. Based on the accumulated state and action histories, the task evaluator determines whether the task is completed.

An important component of our framework is the learned reward model $\mathcal{R}$, which estimates whether a trajectory $h$ has successfully addressed the task:
\begin{equation}
    r = \mathcal{R}(\mathit{x}, h),
\end{equation}
where $h = \{\{a_n\}_{n=1}^N, \{o_n\}_{n=0}^{N}\}$, $\{a_n\}_{n=1}^N$ are the actions taken in the trajectory, $\{o_n\}_{n=0}^{N}$ are the corresponding environment observations, and $r$ is the predicted reward from the reward model.
By integrating this reward model with LLM agents, we can enhance their performance across various environments using different planning algorithms.

\subsection{ Automatic Reward Data Generation.}
\label{sec:data}
To train a reward model capable of estimating the reward value of history trajectories, we first need to collect a set of training language instructions $\{x_m\}_{m=1}^M$, where $M$ represents the number of instruction goals. Each instruction corresponds to a set of positive trajectories $\{h_m^+\}_{m=1}^M$ that match the instruction goals and a set of negative trajectories $\{h_m^-\}_{m=1}^M$ that fail to meet the task requirements. This process typically involves human annotators and is time-consuming and labor-intensive~\citep{christiano2017deep,rafailov2024direct}. As shown in Fig.~\ref{fig:instruction_generation_sciworld} of the Appendix. we automate data collection by using Large Language Model (LLM) agents to navigate environments and summarize the navigation goals without human labels.

\noindent\textbf{Instruction Synthesis.} The first step in data generation is to propose a task instruction for a given observation. We achieve this using the in-context learning capabilities of LLMs. The prompt for instruction generation is shown in Fig.~\ref{fig:instruction_refinement_sciworld} of the Appendix. Specifically, we provide some few-shot examples in context along with the observation of an environment state to an LLM, asking it to summarize the observation and propose instruction goals. In this way, we collect a set of synthesized language instructions $\{x_m^{raw}\}_{m=1}^M$, where $M$ represents the total number of synthesized instructions.

\noindent\textbf{Trajectory Collection.} Given the synthesized instructions $x_m^{raw}$ and the environment, an LLM-based agent is instructed to take actions and navigate the environment to generate diverse trajectories $\{x_m^{raw}, h_m\}_{m=0}^M$ aimed at accomplishing the task instructions. Here, $h_m$ represents the $m$-th history trajectory, which consists of $N$ actions $\{a_n\}_{n=1}^N$ and $N+1$ environment observations $\{o_n\}_{n=0}^N$.
Due to the limited capabilities of current LLMs, the generated trajectories $h_m$ may not always align well with the synthesized task instructions $x_m$. To address this, we ask the LLM to summarize the completed trajectory $h_m$ and propose a refined goal $x_m^r$. This process results in a set of synthesized demonstrations $\{x_m^r, h_m\}_{m=0}^{M_r}$, where $M_r$ is the number of refined task instructions.

\noindent\textbf{Pairwise Data Construction.} 
To train a reward model capable of distinguishing between good and poor trajectories, we also need trajectories that do not satisfy the task instructions. To create these, we sample additional trajectories that differ from $\{x_m^r, h_m\}$ and do not meet the task requirements by modifying actions in $h_m$ and generating corresponding negative trajectories $\{h_m^-\}$. For clarity, we refer to the refined successful trajectories as $\{x_m, h_m^+\}$ and the unsuccessful ones as $\{x_m, h_m^-\}$. These paired data will be used to train the reward model described in Section~\ref{sec:model}, allowing it to estimate the reward value of any given trajectory in the environment.

\subsection{ Reward Model Design.} 
\label{sec:model}
\noindent\textbf{Reward Model Architectures.}
Theoretically, we can adopt any vision-language model that can take a sequence of visual and text inputs as the backbone for the proposed reward model. In our implementation, we use the recent VILA model~\citep{lin2023vila} as the backbone for reward modeling since it has carefully maintained open-source code, shows strong performance on standard vision-language benchmarks like~\citep{fu2023mme,balanced_vqa_v2,hudson2018gqa}, and support multiple image input. 

The goal of the reward model is to predict a reward score to estimate whether the given trajectory $(x_m, h_m)$  has satisfied the task instruction or not, which is different from the original goal of VILA models that generate a series of text tokens to respond to the task query. To handle this problem, we additionally add a fully-connected layer for the model, which linearly maps the hidden state of the last layer into a scalar value. 

\noindent\textbf{Optimazation Target.}
Given the pairwise data that is automatically synthesized from the environments in Section~\ref{sec:data}, we optimize the reward model by distinguishing the good trajectories $(x_m, h^+_m)$ from bad ones $(x_m, h^-_m)$. Following standard works of reinforcement learning from human feedback~\citep{bradley1952rank,sun2023salmon,sun2023aligning}, we treat the optimization problem of the reward model as a binary classification problem and adopt a cross-entropy loss. Formally, we have 
\begin{equation}
    \mathcal{L(\theta)} = -\mathbf{E}_{(x_m,h_m^+,h_m^-)}[\log\sigma(\mathcal{R}_\theta(x_m, h_m^+)-\mathcal{R}_\theta(x_m, h_m^-))],
\end{equation}
where $\sigma$ is the sigmoid function and $\theta$ are the learnable parameters in the reward model $\mathcal{R}$.
By optimizing this target, the reward model is trained to give higher value scores to the trajectories that are closer to the goal described in the task instruction. 

\subsection{ Planning with Large Vision-Langauge Reward Model.}
After getting the reward model to estimate how well a sampled trajectory match the given task instruction, we are able to combine it with different planning algorithms to improve LLM agents' performance. Here, we summarize the typical algorithms we can adopt in this paper.

\noindent\textbf{Best of N.} This is a simple algorithm that we can adopt the learned reward model to improve the LLM agents' performances. We first prompt the LLM agent to generate $n$ different trajectories independently and choose the one with the highest predicted reward score as the prediction for evaluation. Note that this simple method is previously used in natural language generation~\citep{zhang2024improving} and we adopt it in the context of agent tasks to study the effectiveness of the reward model for agent tasks.

\noindent\textbf{Reflexion.} Reflexion~\citep{shinn2024reflexion} is a planning framework that enables large language models (LLMs) to learn from trial-and-error without additional fine-tuning. Instead of updating model weights, Reflexion agents use verbal feedback derived from task outcomes. This feedback is converted into reflective summaries and stored in an episodic memory buffer, which informs future decisions. Reflexion supports various feedback types and improves performance across decision-making, coding, and reasoning tasks by providing linguistic reinforcement that mimics human self-reflection and learning. %This approach yields significant gains over baseline methods in several benchmarks.

\noindent\textbf{MCTS.} 
We also consider tree search-based planning algorithms like Monte Carlo Tree Search (MCTS)~\citep{coulom2006efficient,silver2017mastering} to find the optimal policy. 
There is a tree structure constructed by the algorithm, where each node represents a state and each edge signifies an action.
Beginning at the initial state of the root node, the algorithm navigates the state space to identify action and state trajectories with high rewards, as predicted by our learned reward model. 

The algorithm tracks 1) the frequency of visits to each node and 2) a value function that records the maximum predicted reward obtained from taking action ${a}$ in state ${s}$.
MCTS would visit and expand nodes with either higher values (as they lead to high predicted reward trajectory) or with smaller visit numbers (as they are under-explored).
We provide more details in the implementation details and the appendix section.


\label{sec:plan}