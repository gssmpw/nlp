\section{Related Work}

\paragraph{LLMs for Agent tasks.}

Our research is related to deploying large language models (LLMs) as agents for decision-making tasks in interactive environments~\citep{liu2023agentbench,zhou2023webarena,shridhar2020alfred,toyama2021androidenv}. Earlier works, such as~\citep{yao2023webshopscalablerealworldweb}, fine-tuned models like BERT~\citep{devlin2019bertpretrainingdeepbidirectional} for decision-making in simplified environments, such as online shopping or mobile phone manipulation. With the advent of large language models~\citep{brown2020languagemodelsfewshotlearners,openai2024gpt4technicalreport}, it became feasible to perform decision-making tasks through zero-shot or few-shot in-context learning. To better assess the capabilities of LLMs as agents, several models have been developed~\citep{deng2024mind2web,xiong2024watch,hong2023cogagent,yan2023gpt}. Most approaches~\citep{zheng2024seeact,deng2024mind2web} provide the agent with observation and action history, and the language model predicts the next action via in-context learning. Additionally, some methods~\citep{zhang2023building,li2023camel,song2024trial} attempt to distill trajectories from state-of-the-art language models to train more effective policy models. In contrast, our paper introduces a novel framework that automatically learns a reward model from LLM agent navigation, using it to guide the agents in making more effective plans.

\textbf{LLM Planning.} Our paper is also related to planning with large language models. Early researchers~\citep{brown2020languagemodelsfewshotlearners} often prompted large language models to directly perform agent tasks. Later, \citet{yao2022react} proposed ReAct, which combined LLMs for action prediction with chain-of-thought prompting~\citep{wei2022chain}. Several other works~\citep{yao2023treethoughtsdeliberateproblem,hao2023reasoning,zhao2023large,qiao2024agentplanningworldknowledge} have focused on enhancing multi-step reasoning capabilities by integrating LLMs with tree search methods. Our model differs from these previous studies in several significant ways. First, rather than solely focusing on text generation tasks, our pipeline addresses multi-step action planning tasks in interactive environments, where we must consider not only historical input but also multimodal feedback from the environment. Additionally, our pipeline involves automatic learning of the reward model from the environment without relying on human-annotated data, whereas previous works rely on prompting-based frameworks that require large commercial LLMs like GPT-4~\citep{openai2024gpt4technicalreport} to learn action prediction. Furthermore, \Model supports a variety of planning algorithms beyond tree search.

\textbf{Learning from AI Feedback.} In contrast to prior work on LLM planning, our approach also draws on recent advances in learning from AI feedback~\citep{bai2022constitutional,lee2023rlaif,yuan2024self,sharma2024critical,pan2024autonomous,koh2024tree}. These studies initially prompt state-of-the-art large language models to generate text responses that adhere to predefined principles and then potentially fine-tune the LLMs with reinforcement learning. Like previous studies, we also prompt large language models to generate synthetic data. However, unlike them, we focus not on fine-tuning a better generative model but on developing a classification model that evaluates how well action trajectories fulfill the intended instructions. This approach is simpler, requires no reliance on state-of-the-art LLMs, and is more efficient. We also demonstrate that our learned reward model can integrate with various LLMs and planning algorithms, consistently improving their performance.

\textbf{Inference-Time Scaling.} ~\citet{snell2024scaling} validates the efficacy of inference-time scaling for language models. Based on inference-time scaling, various methods have been proposed, such as random sampling~\citep{wang2022self} and tree-search methods~\citep{hao2023reasoning, zhang2024accessing, guan2025rstar}. Concurrently, several works have also leveraged inference-time scaling to improve the performance of agentic tasks. ~\citet{koh2024tree} adopts a training-free approach, employing MCTS to enhance policy model performance during inference and prompting the LLM to return the reward. ~\citet{gu2024your} introduces a novel speculative reasoning approach to bypass irreversible actions by leveraging LLMs or VLMs. It also employs tree search to improve performance and prompts an LLM to output rewards. ~\citet{yu2024exact} proposes Reflective-MCTS to perform tree search and fine-tune the GPT model, leading to improvements in ~\citet{koh2024visualwebarena}. ~\citet{putta2024agent} also utilizes MCTS to enhance performance on web-based tasks such as ~\citet{yao2023webshopscalablerealworldweb} and real-world booking environments. ~\cite{lin2025qlass} utilizes the stepwise reward to give effective intermediate guidance across different agentic tasks. Our work differs from previous efforts in two key aspects: (1) Broader Application Domain. Unlike prior studies that primarily focus on tasks from a single domain, our method demonstrates strong generalizability across web agents, mathematical reasoning, and scientific discovery domains, further proving its effectiveness. (2) Flexible and Effective Reward Modeling. Instead of simply prompting an LLM as a reward model, we finetune a small scale VLM~\citep{lin2023vila} to evaluate input trajectories. %Our reward scores range continuously between 0 and 1, in contrast to existing methods that rely on discrete scoring (e.g., 0 and 1, or 0, 0.5, and 1) through direct LLM prompting.

% Concurrently, several works have also leveraged inference-time scaling to improve the performance of agentic tasks. ~\citet{pan2024autonomous} demonstrates that LLMs and VLMs, such as the GPT series, can function as evaluators or reward models to provide guidance for fine-tuning or reflection, thereby enhancing digital agents. This lays the groundwork for subsequent studies that directly prompt LLMs as reward models. ~\citet{koh2024tree} adopts a training-free approach, employing MCTS to enhance policy model performance during inference. However, it is limited to web environments~\citep{koh2024visualwebarena}. Moreover, its value function relies on prompting an LLM, which is less effective than our proposed method. We validate our approach through ablation studies, demonstrating that our fine-tuned reward model is more effective. ~\citet{gu2024your} introduces a novel speculative reasoning approach to bypass irreversible actions, such as purchasing a product, by leveraging LLMs or VLMs. It also employs tree search to improve performance, but it remains restricted to the web domain~\citep{koh2024visualwebarena, deng2024mind2web}. Additionally, it lacks reward modeling and instead prompts an LLM to output rewards. ~\citet{yu2024exact} proposes Reflective-MCTS to perform tree search and fine-tune the GPT model, leading to improvements in ~\citep{koh2024visualwebarena}. However, this work focuses solely on a single web agent task, and its reward modeling is derived from multi-agent debate, differing from our more effective and efficient reward modeling approach. ~\citet{putta2024agent} also utilizes MCTS to enhance performance, but it is limited to web-based tasks such as ~\citep{yao2023webshopscalablerealworldweb} and real-world booking environments.