\section{Introduction}

Developing AI agents capable of perceiving environments, understanding instructions, and acting to accomplish a wide range of tasks in interactive settings~\citep{1087032} have many real-world applications, including virtual human assistants~\citep{reed2022generalistagent,casheekar2024contemporary}, business process management~\citep{kirchdorfer2024agentsimulator}, and robotic process automation~\citep{rana2023sayplan,saycan2022arxiv,dipalo2023unifiedagentfoundationmodels}.

The recent advent of large generative models has revolutionized numerous applications, such as question answering~\citep{rajpurkar2016squad100000questionsmachine}, text summarization~\citep{NIPS2015_afdec700}, and multi-modal understanding~\citep{chen2015microsoftcococaptionsdata,balanced_vqa_v2,yu2016modelingcontextreferringexpressions}. However, while these models excel in text comprehension and generation tasks, their performance in decision-making scenariosâ€”such as online shopping and scientific reasoning falls relative short of human capabilities.
This disparity likely stems from the nature of the training data. Large generative models are typically pre-trained on readily available image and text corpora from the internet. In contrast, trajectory data for agent tasks, which require multi-step interaction with the environment, is more challenging to collect and does not naturally occur on the internet.
Furthermore, current state-of-the-art commercial Language Learning Models (LLMs), such as GPT-4V~\citep{openai2024gpt4technicalreport} and Gemini~\citep{reid2024gemini}, often provide only limited APIs for general users. This restriction renders it either infeasible or cost-prohibitive to fine-tune these models for specific agent tasks, further impeding progress in this field.

\begin{figure*}[t]
  \centering
  \includegraphics[width=\textwidth]{figures/teaser.pdf}
   \caption{
   In Fig.~\ref{fig:teaser}~(a), we show that it is difficult for LLM agents to generate multi-step plans in an interactive environment to achieve the instruction goal. However, it is relatively easy for an LLM to learn a reward model that can evaluate whether the trajectories meet the task instructions, as shown in Fig.~\ref{fig:teaser}~(b). In Fig.~\ref{fig:teaser}~(c), we show that a learned reward model can be used to guide the default policy models to improve action planning.}
   \label{fig:teaser}
\end{figure*}


Previous studies have explored the development of autonomous agents for decision-making tasks using large language models (LLMs). Early research~\citep{yao2023webshopscalablerealworldweb,zheng2024seeact,deng2024mind2web} utilized instruction prompts with few-shot examples to direct LLMs in handling various agent tasks. These methods do not require task-specific fine-tuning but have shown limited performance on benchmarks requiring interaction with environments and precise action prediction. A different research direction involves collecting human preference data~\citep{hong2023cogagent} or distilling trajectory data from advanced commercial LLM APIs~\citep{zeng2023agenttuning,deng2024mind2web} and fine-tuning smaller open-source LLMs to create new policy models for agent tasks. However, this distillation process relies on advanced pre-trained agent models for trajectory data extraction, which are often unavailable, expensive, or subject to commercial restrictions. For instance, data from models such as GPT-4 or Gemini cannot be used for commercial purposes.

A fundamental premise of our approach is that, in most agent applications, evaluation is easier than generation~\citep{karp1975computational,naor1996evaluation}. As illustrated in Fig.~\ref{fig:teaser}~(a), \textit{\textbf{generating}} a correct multi-step solution to navigate to the target page is challenging since it needs to predict multiple actions and interact with the environment. However, it is relatively simple to \textit{\textbf{evaluate}} whether the output action trajectories and environment states meet the provided intent to find a \textit{"vans sport canvas fashion sneaker"}. Building on this premise, we suggest that developing a reward model is more feasible than creating a policy model for agent tasks. With an effective reward model, it becomes possible to guide LLMs in planning tasks both effectively and efficiently. For instance, as depicted in Fig.~\ref{fig:teaser}~(c), by integrating the reward model with an LLM-based agent and the Monte Carlo Tree Search (MCTS) algorithm~\citep{silver2017mastering,coulom2006efficient}, we can simulate and evaluate the future states of agent tasks, thereby making better decisions for subsequent actions. This approach is analogous to mental simulation~\citep{hegarty2004mechanical,lake2017building} in cognitive science, where humans envision the outcomes of potential actions to make better decisions in problem-solving.%~\zf{need update fig description.}

While reward models can assist LLM agents in planning, developing these reward models presents significant challenges. Some prior studies have utilized powerful commercial LLM APIs as evaluators for tasks~\citep{kwon2023rewarddesignlanguagemodels}. Although these approaches have demonstrated effectiveness in certain applications, they rely on state-of-the-art LLM models for evaluation, which are often expensive and difficult to scale. In this paper, we introduce an automated method to learn multi-modal reward models without relying on state-of-the-art LLMs for guidance. Furthermore, previous work has not considered integrating the learned reward models with various planning algorithms for problem-solving.

The process of learning the reward model involves three steps. Initially, we utilize an LLM-based agent (e.g.,~\cite{dubey2024llama3herdmodels}) to navigate in the environments, aiming to achieve a randomly proposed intent while collecting extensive action trajectory demonstrations. Subsequently, the LLM model examines the collected trajectories and proposes a refined intent that the sampled trajectories actually accomplish. Additionally, we prompt the LLM to generate negative trajectories that fail to achieve the intended task. Finally, based on the synthetic data (intents, positive trajectories, and negative trajectories) collected, we train a customized reward model using widely adopted vision-language models such as VILA~\citep{lin2023vila} to evaluate whether the user's intent has been fulfilled by the action trajectories. With this automatic reward model, we enhance the performance of LLM-based agents in conjunction with various planning algorithms such as best of n, reflexion, and MCTS.

In summary, we introduce a novel framework \Model (autonomous Agents from automatic Reward Modeling And Planning) for LLM-based agents incorporating an automatic reward model that evaluates task completion, analogous to mental simulation in human cognition. This framework offers several advantages: (1) Effectiveness: It enhances the performance of various LLM agents across different tasks. (2) Flexibility: It eliminates the need for fine-tuning the LLMs themselves and allows for optimization of custom reward targets during inference, enabling more controllable generation.
(3) Practicality: The training of the automatic reward model does not rely on labor-intensive labeling or state-of-the-art commercial LLMs, making it more feasible and widely applicable.