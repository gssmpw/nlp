\section{Conclusion}
We propose a framework, \Model, for large language model (LLM) agents to manage tasks that require multi-step decision-making and environmental feedback, such as online shopping or scientific reasoning. This framework allows LLM-based agents to enhance task planning by autonomously learning a reward model from the environment, without the need for human labeling. The method utilizes pre-trained LLM agents to generate diverse action trajectories within an environment, which are then evaluated by a separate LLM based on the task's intent. These evaluations help train a reward model that strengthens the agents' decision-making capabilities. The framework enhances the performance of LLM agents in addressing complex tasks and mitigates issues related to data scarcity and API limitations. Its effectiveness is demonstrated across various benchmarks, representing a significant advancement in the development of AI agents for real-world, multi-step problem-solving.