\section{Appendix}
In this section, we provide supplementary material for the main paper.
\subsection{Experiments on ALFWorld and AgentClinic.}
\label{sec:exprenv}
We extend our experiment on ALFWorld~\citep{ALFWorld20}, a classic environment for House-Holding, where the agent must accomplish tasks in physical house-holding environments, like “Put a pan on the dining table”. Following the setup of AgentBench~\citep{liu2023agentbench} for LLM evaluation, we test the model on the dev and std split, using the default success rate as the evaluation metric. 
Specifically, we used LLaMa-3.1-70B to generate around 1600 pairs of positive and negative samples with our data generation pipeline. Then we train a reward model with these synthesized data. We evaluate our ARMAP framework on ALFWorld using various planning algorithms, including Reflexion and Best-of-N, which we refer to as ARMAP-R and ARMAP-B, respectively. Additionally, we compare our approach with two baseline methods that do not incorporate reward model guidance: Sampling and Greedy. The results are shown below. As shown in Table~\ref{tab:alfworld}, our model still performs well in this challenging environment, which contains diverse scenes and long-horizon planning tasks.
\input{tables/alfworld} 


We also extended our experiments to ClinicalAgent~\citep{schmidgall2024agentclinic}, an environment designed for medical decision-making tasks. ClinicalAgent evaluates models on their ability to interpret clinical scenarios and make accurate, high-stakes decisions. Results of ClinicalAgent are provided in Table~\ref{tab:clinic}, further supporting the versatility of ARMAP in domains requiring precise reasoning.
\input{tables/clinicalagent} 

\subsection{Ablation Study.}
\label{sec:expabl}

\textbf{Dependence on Quality of Synthetic Data from Various LLMs}. We choose ScienceWorld and conduct experiments to study the effectiveness of different reward models. As shown in Table~\ref{tab:dataquality}, the left column represents the results of using LLaMA-8B greedy directly and the Best of N results of LLaMA-8B with the reward model trained by the data generated from LLaMA-70B, LLaMA-8B, Mistral-7B, and Phi-3.8B, respectively. Greedy is our baseline result and it can be observed that using the reward model leads to better experimental outcomes. 
Among all the results, LLaMA-70B achieves the best performance. Compared to the other three models, LLaMA-70B has the largest scale and is naturally the most capable model. LLaMA-8B and Mistral-7B have a similar number of parameters, and in the ScienceWorld task, Mistral-7B performs better than LLaMA-8B. Phi-3.8B is the smallest of these models, yet it still achieved very good results. Notably, compared to the larger-scale LLaMA-8B and Mistral-7B, Phi-3.8B still scored better. These results indicate that our method exhibits good robustness when faced with LLMs of different scales and capabilities. Even with the smallest model, our method can still achieve good results. From these experimental outcomes, it is clear that our method does not overly rely on the capabilities of language models. In other words, our method is highly efficient and robust.
\input{tables/LLMquality} 


\textbf{Reward Modeling Target.} To further investigate the optimization target of the reward model, we conduct experiments to compare the performance of pairwise comparison and binary classification as learning methods for the reward model. Specifically, in the classification setting: each input pair is treated as a positive and a negative example. The model is trained to predict a score of 1 for positive examples and 0 for negative examples.
The comparative results are shown in Table~\ref{tab:rewardtarget}. Across all settings, pairwise comparison consistently outperforms binary classification. This confirms that pairwise comparison captures nuanced preferences more effectively than binary classification, leading to better reward modeling and overall task performance.
\input{tables/rewardtarget} 

\textbf{Computational Efficiency Analysis.}
We further study the data demands of the reward modelings. We show the performance of using different amounts of training data. In Table~\ref{tab:Efficiency1} and Table~\ref{tab:Efficiency2}, we selected ScienceWorld and used ARMAP-B as the experimental setting. In the leftmost column, we listed the different LLMs used in our study. In the first row, we introduced VILA-3B, VILA-13B, and LLaVA-13B, to compare the impact of different sizes and types of reward models on the final outcomes. In the last two columns, we trained the reward models using 1/5 and 1/25 of the original training dataset size, respectively, to assess how varying amounts of training data affect our method. (1) As seen, the effectiveness of our method continues to improve with increasing reward model sizes. However, in the experiments with LLaMA-8B and Phi-3.8B, despite using more potent reward models, there was no improvement in results. We believe that in the processes of planning and reasoning, the capability of the policy model still plays a dominant role. If the policy model is more robust, and concurrently, if we enhance the capability of the reward model, we can continuously achieve better results. (2) We also observe that the performance of LLaVA-13B is not as good as VILA-13B. We attribute this to VILA being an improved version of LLaVA, and it utilizes an interleaved image-text dataset in its training, which better aids the model in perceiving, understanding, and handling multimodal information. Hence, VILA outperforms LLaVA. (3) From the Table~\ref{tab:Efficiency1} and Table~\ref{tab:Efficiency2}, it is evident that regardless of whether the data is seen or unseen, increasing the model size improves the final experimental results. If we use the results of the VILA-3B model as a benchmark and compare it with the two settings, 1/5 data and 1/25 data, it is clear that increasing the training data enhances the outcomes. Conversely, even when using extremely limited data amounts like 1/5 or 1/25 of the original dataset, we can still achieve a capable model, and the performance does not dramatically decrease.

These results demonstrate that our method can still yield good results in a low-resource environment. In other words, our approach does not rely on large volumes of data and the strong capability of large models; it is succinct and efficient, capable of performing well in extremely low-resource settings.
\input{tables/ComputationalEfficiency}

\textbf{Ablation on Visual Input}. 
We also train a new reward model without visual information. As shown in Table~\ref{tab:visualinfo}, we can see that, in different settings, the reward model with visual information performs better than the model without visual information, which shows the importance of visual context in the Webshop task.
\input{tables/visualinfo}

\textbf{Overhead in Data Synthesis.}
We calculate the tokens we have used for task instruction generation and trajectory exploration. We summarize these overheads in Table~\ref{tab:overhead}. To provide a more intuitive comparison, we first calculated the average tokens per sample for these different tasks. We found that although Game of 24 overall consumes the most tokens, the average number of tokens spent per Game of 24 sample is relatively the least. In contrast, Webshop has the fewest total samples but the highest average number of tokens spent per sample. ScienceWorld falls in between these two. The reason Webshop has a higher average number of tokens compared to Game of 24 is that the environment required for Webshop is more complex, involving more diverse elements and possibilities.
\input{tables/overhead}

\textbf{Proprietary Models as Training Data Generators and Policy Models.}
In the main content, we mainly consider using open-source models to act as training data generators and policy models. In order to investigate the upper bounds of our proposed method, we also conduct some experiments by using powerful proprietary models. However, to serve as the training data generator, closed-source models have several drawbacks, including high costs, limited commercial access, and lack of reproducibility. In contrast, our approach achieves strong results without relying on closed-source models. Given the expense associated with API-based models like GPT-4o for generating training datasets, we have opted not to pursue this method for now.

For API-based proprietary models serving as policy models, the high cost of GPT-4o and API access rate limitations prompted us to focus our experiments primarily on ALFWorld. Specifically, we used GPT-4o-2024-08-06 to sample five trajectories each on ALFWorld’s Dev and Std sets, then conducted experiments using our automatic reward model. As shown in Table~\ref{tab:api_model},  our reward model is able to help the powerful GPT-4o gain better performance, demonstrating the effectiveness of our framework.
\input{tables/api_model}

\subsection{Implementation Details.}
\label{sec:llmapi}
\paragraph{Large Pretrain Model Setup.} We serve a diverse set of open-source LLM APIs to evaluate the effectiveness of the proposed pipeline. We list all the open-source models and their weights on huggingface in table~\ref{tab:llmapi}. All these models can be easily setup and reproduced with the VLLM libarary~\citep{kwon2023efficient}. We prove the effectiveness of our \Model framework across different LLM APIs.
\begin{table}[ht]
    \centering
    \begin{tabular}{l|l}
    \toprule
         Acronym & Model description and weight on huggingface websites \\
         \midrule
         Llama70B & https://huggingface.co/hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4 \\
         Llama8B  & https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct \\
         Mistral7B & https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3  \\
         Phi3.8B & https://huggingface.co/microsoft/Phi-3.5-mini-instruct \\ 
    \midrule
        VILA3B & https://huggingface.co/Efficient-Large-Model/VILA1.5-3b \\
    \bottomrule
    \end{tabular}
    \caption{Agent models, the reward model, and their associated description on huggingface websites.}
    \label{tab:llmapi}
\end{table}
\paragraph{Environment Setup.} We build our environments based on the environment setup of the previous works~\citep{liu2023agentbench,song2024trial,yao2023treethoughtsdeliberateproblem,ALFWorld20,schmidgall2024agentclinic}. For Webshop and ALFWorld environment, we start these docker environments from AgentBench~\citep{liu2023agentbench} and implement different planning algorithms, Reflexion, Best-of-N and MCTS on it. Similarly, we build our ScienceWorld, Game of 24 and AgentClinic environments from ~\cite{song2024trial}, ~\cite{yao2023treethoughtsdeliberateproblem} and~\cite{schmidgall2024agentclinic}, respectively. 
\paragraph{Planning Algorithm Details.} We compare the performance of different planning algorithms by limiting their maximum explored trajectory number. We set the maximum number to be 10 on Webshop and ScieneWorld in consideration of effectiveness and efficiency. We set the maximum number to be 100 on Game of 24 following the setup of ~\cite{yao2023treethoughtsdeliberateproblem}. In Webshop, ScienceWorld, ALFWorld and AgentClinic benchmarks, we only consider the top 10 available actions suggested by the LLM agent at each state to reduce search space. We also set a trajectory's maximal action number length to 10 for simplicity. 

For Reflexion, we set the maximum trial number to be 10 for all tasks. For different tasks and models, we set the threshold of Reflexion separately. During the iteration process, if the reward of the current trail's trajectory exceeds the threshold, the iteration will stop, and the current trail will be taken as the result. If the maximum number of trials is reached, the last trial will be taken as the result in Webshop and Game of 24, while the first trial will be taken as the result in ScienceWorld.
\paragraph{Data Generation.} In total, we generate 2,436, 4,064 and 37,885 pairs of data for Webshop, ScienceWorld and Game of 24, respectively. Sampled synthesized data sample can be seen in Fig.~\ref{fig:train_data_webshop}, Fig.~\ref{fig:train_data_sciworld} and Fig.~\ref{fig:train_data_game24}. We provide the sampled prompt we use for data generation from Fig.~\ref{fig:instruction_generation_sciworld} to Fig.~\ref{fig:neg_trajectories_synthesis_sciworld}. In Fig.~\ref{fig:instruction_generation_sciworld}, we show an example how we prompt the LLM to generate language instruction for ScienceWorld. In Fig.~\ref{fig:instruction_refinement_sciworld}, we show how we refine the language instruction to refine the instruction goal. In Fig.~\ref{fig:pos_trajectories_synthesis_sciworld} and Fig.~\ref{fig:neg_trajectories_synthesis_sciworld}, we show the prompt how the LLM agent synthesizes positive and negative trajectories, respectively. 

\newpage

\begin{tcolorbox}[breakable,title=Training Data Example for Webshop]
\textcolor{my_blue!50}{\textbf{Task Instruction:} I need 2 long-lasting bluetooth radio speaker units for a soundbar with 4ohm impedance and 6w power, and price lower than 50.00 dollars.}\\

\textcolor{my_green!50}{\textbf{Positive Example:} }\\
\textcolor{my_purple}{\textbf{Thought:} ...}\\
\textcolor{my_purple}{\textbf{Action:} ...}\\
\textcolor{my_blue!50}{\textbf{Current Screenshot:} ...}\\
\textcolor{my_blue!50}{\textbf{Observation:} ...}\\
\textcolor{my_purple}{\textbf{Thought:} The option B09STMXYR5 matches the required characteristics. I should select it.}\\
\textcolor{my_purple}{\textbf{Action:} click[B09STMXYR5]}\\
\textcolor{my_blue!50}{\textbf{Current Screenshot:} \begin{center}\includegraphics[width=0.7\textwidth]{figures/webshop_training_pos.pdf}\end{center}}
\textcolor{my_blue!50}{\textbf{Observation:} Instruction: [SEP] i need 2 long-lasting bluetooth radio speaker units for a soundbar with 4ohm impedance and 6w power, and price lower than 50.00 dollars [SEP] Back to Search [SEP] < Prev [SEP] JUSTQIJUN 2pcs 1.5 Inch Bluetooth Radio Speaker Unit 4 Ohm 6W Sound Bar Horn 18 Core 45mm Music Portable Loudspeaker Rubber Edge Compatible with Altavoces Bookshelf Speakers (Color : 40mm 4 ohm 6W) [SEP] Price: \$28.36 [SEP] Rating: N.A. [SEP] Description [SEP] Features [SEP] Reviews [SEP] Buy Now}\\
\textcolor{my_purple}{\textbf{Thought:} I should click on the 'Buy Now' button to proceed with the purchase.}\\
\textcolor{my_purple}{\textbf{Action:} click[buy now]}\\

\textcolor{my_green!50}{\textbf{Negative Example:} }\\
\textcolor{my_purple}{\textbf{Thought:} ...}\\
\textcolor{my_purple}{\textbf{Action:} ...}\\
\textcolor{my_blue!50}{\textbf{Current Screenshot:} ...}\\
\textcolor{my_blue!50}{\textbf{Observation:} ...}\\
\textcolor{my_purple}{\textbf{Thought:} I think I should click on the product I need, which is B09SWKXBY5, since it matches the requirements of 4ohm impedance and 6w power and the price is lower than 50.00 dollars.}\\
\textcolor{my_purple}{\textbf{Action:} click[B09SWKXBY5]}\\
\textcolor{my_blue!50}{\textbf{Current Screenshot:} \begin{center}\includegraphics[width=0.7\textwidth]{figures/webshop_training_neg.pdf}\end{center}}
\textcolor{my_blue!50}{\textbf{Observation:} Instruction: [SEP] i need 2 long-lasting bluetooth radio speaker units for a soundbar with 4ohm impedance and 6w power, and price lower than 50.00 dollars [SEP] Back to Search [SEP] < Prev [SEP] color [SEP] 4 ohm 10w [SEP] 4 ohm 15w [SEP] 4 ohm 20w [SEP] JUSTQIJUN 2pcs Full Range 2 Inch Speaker 15W DIY Soundbar Boombox}
\textcolor{my_blue!50}{Unit Portable Radio 10W 20w 4 Ohm Speaker HiFi Bluetooth Speakers 55mm Bookshelf Speakers (Color : 4 ohm 20W) [SEP] Price: \$42.66 [SEP] Rating: N.A. [SEP] Description [SEP] Features [SEP] Reviews [SEP] Buy Now}\\
\textcolor{my_purple}{\textbf{Thought:} I think I should click on the 'Buy Now' button to purchase the product.}\\
\textcolor{my_purple}{\textbf{Action:} click[buy now]}
\end{tcolorbox}
\begin{figure}[!htp]
    \centering
    \vspace{-16pt}
    \caption{
    Training Data Example for Webshop.
    }
    \label{fig:train_data_webshop}
\end{figure}


\begin{tcolorbox}[breakable,title=Training Data Example for ScienceWorld]
\textcolor{my_blue!50}{\textbf{Task Instruction:} You are a helpful assistant to do some scientific experiments in an environment. In the environment, there are several rooms: kitchen, foundry, workshop, bathroom, outside, living room, bedroom, greenhouse, art studio, hallway
You should explore the environment and find the items you need to complete the experiment. You can teleport to any room in one step. All containers in the environment have already been opened, you can directly get items from the containers. \\  
The available actions are: \\
open OBJ: open a container \\
close OBJ: close a container \\
... \\
wait1: task no action for a step \\
Task Description:
Your task is to find a non-living thing in the environment. Focus on the jug in the greenhouse. When you are done, move the jug to the green box in the kitchen.}\\

\textcolor{my_green!50}{\textbf{Positive Example:} }\\
\textcolor{my_purple}{\textbf{Action:} look around}\\
\textcolor{my_blue!50}{\textbf{Observation:}This room is called the hallway. In it, you see:\\
        the agent, a substance called air, a picture\\
You also see:\\
        A door to the art studio (that is open), A door to the bedroom (that is open), A door to the greenhouse (that is open), A door to the kitchen (that is open), A door to the living room (that is open), A door to the workshop (that is open)}\\
\textcolor{my_purple}{\textbf{Action:} teleport to the greenhouse}\\
\textcolor{my_blue!50}{\textbf{Observation:} You teleport to the greenhouse. }\\
\textcolor{my_purple}{\textbf{Action:} look around}\\
\textcolor{my_blue!50}{\textbf{Observation:} This room is called the greenhouse. In it, you see:\\
        the agent, a substance called air, a adult bee, a adult bee, a adult bee, a adult bee, a bee hive. The bee hive door is open. In the bee hive is: a adult bee. a flower pot 2 (containing a peach tree in the adult stage, soil), a flower pot 4 (containing a substance called water), a flower pot 5 (containing a substance called water), a flower pot 6 (containing a banana tree in the adult stage, soil), a flower pot 7 (containing a apple tree in the adult stage, soil), a jug (containing nothing), a shovel, a sink, which is turned off. In the sink is: nothing.\\
You also see:\\
        A door to the hallway (that is open), A door to the outside (that is open) }\\
\textcolor{my_purple}{\textbf{Action:} focus on the jug}\\
\textcolor{my_blue!50}{\textbf{Observation:} You focus on the jug. }\\

\textcolor{my_green!50}{\textbf{Negative Example:} }\\
\textcolor{my_purple}{\textbf{Action:} look around}\\
\textcolor{my_blue!50}{\textbf{Observation:}This room is called the hallway. In it, you see:\\
        the agent, a substance called air, a picture\\
You also see:\\
        A door to the art studio (that is open), A door to the bedroom (that is open), A door to the greenhouse (that is open), A door to the kitchen (that is open), A door to the living room (that is open), A door to the workshop (that is open)}\\
\textcolor{my_purple}{\textbf{Action:} teleport to the greenhouse}\\
\textcolor{my_blue!50}{\textbf{Observation:} You teleport to the greenhouse. }\\
\textcolor{my_purple}{\textbf{Action:} connect painting to kitchen}\\
\textcolor{my_blue!50}{\textbf{Observation:} terminal 1 on painting is now connected to terminal 1 on kitchen }\\
\textcolor{my_purple}{\textbf{Action:} connect agent to bathtub}\\
\textcolor{my_blue!50}{\textbf{Observation:} No known action matches that input. }\\

\end{tcolorbox}
\begin{figure}[!htp]
    \centering
    \vspace{-8pt}
    \caption{
    Training Data Example for ScienceWorld.
    }
    \label{fig:train_data_sciworld}
\end{figure}


\begin{tcolorbox}[breakable,title=Training Data Example for Game of 24]
\textcolor{my_blue!50}{\textbf{Task Instruction:} You are a start agent and generate data for Game24. Game24 requires users to use numbers and basic arithmetic operations (+ - * /) to obtain 24.\\
You task is to generate a new input (4 digital number) for Game 24.\\
1. each new input number should be in the range of 1 to 13.\\
2. People can use numbers and basic arithmetic operations (+ - * /) to obtain 24. At each step, people are only allowed to choose two of the remaining numbers to obtain a new number.\\
Here are the few-shot examples.\\
3. since there is only four number input and the intermediate steps should only be three.\\
Input: 12 10 8 4}\\

\textcolor{my_green!50}{\textbf{Positive Example:} }\\
\textcolor{my_purple}{\textbf{Steps:} 10 - 8 = 2 (left: 2 4 12)\\
12 / 2 = 6 (left: 4 6)\\
6 * 4 = 24 (left: 24)}\\
\textcolor{my_purple}{\textbf{Answer:} (12 / (10 - 8)) * 4 = 24}\\

\textcolor{my_green!50}{\textbf{Negative Example:} }\\
\textcolor{my_purple}{\textbf{Steps:} 10 - 12 = -2 (left: -2 4 8)\\
8 / 4 = 2 (left: -2 2)\\
-2 * 2 = 4 (left: 4)}\\
\textcolor{my_purple}{\textbf{Answer:} ((10 - 12) * (8 / 4)) = 4}\\

\end{tcolorbox}
\begin{figure}[!htp]
    \centering
    \vspace{-8pt}
    \caption{
    Training Data Example for Game of 24.
    }
    \label{fig:train_data_game24}
\end{figure}


\begin{tcolorbox}[breakable,title=Instruction Generation Prompt for ScienceWorld]
\textcolor{my_blue!50}{\textbf{Task Instruction:} You are a helpful assistant to do some scientific experiments in an environment. \\
\\
In the environment, there are several rooms: kitchen, foundry, workshop, bathroom, outside, living room, bedroom, greenhouse, art studio, and hallway. \\
\\
The available actions are: \\
open OBJ: open a container \\
… \\
\\
You will be given a dialogue between you (assistant) and a human user. You need to generate the task description after understanding the dialogue given to you. In order to help you better generate the task description, I will give you an example below. \\
\\
In this example, a human user gives a Task Description at first. Then you work as an assistant and talk with user to finish the task step by step using the available actions above. Thought and Action are your reasoning and the actions you take at each step. Observation is provided by the human user; it is an observation of the surrounding environment after you have executed the action. In the end, you need to restore the Task Description based on the }
\textcolor{my_blue!50}{whole dialogue.
}\\

\textcolor{my_green!50}{\textbf{Example:} }\\
\textcolor{my_purple}{
\textbf{User} \\
Task Description \\
\textbf{Assistant} \\
}
\textcolor{my_purple}{
Thought: I need to find materials to create green paint, which are likely to be found in an art studio. \\
Action: look around \\
\textbf{User} \\
Observation: This room is called the bedroom. In it, you see:  \\
... \\
User \\
Observation: Blue and yellow paint mix to produce green paint. \\
Assistant \\
\\
Thought: The task is to create green paint and then focus on it. Now that the green paint is made, I should concentrate on the final product. \\
Action: focus on green paint \\
\\
Task Description: Your task is to use chemistry to create green paint. When you are done, focus on the green paint.\\
}\\
\textcolor{my_blue!50}{Here is the task you need to do:\\
... \\
\\
You should generate the Task Description following the format like examples below and generate it based on the dialogue above without any other words.\\
Examples:\\
Your task is to boil ice cream. For compounds without a boiling point, combusting the substance is also acceptable. First, focus on the substance. Then, take actions that will cause it to change its state of matter.\\
Your task is to use chemistry to create violet paint. When you are done, focus on the violet paint.\\
Your task is to find a(n) living thing. First, focus on the thing. Then, move it to the red box in the bathroom.\\
\\
\textbf{Task Description:} \\
}\\
\end{tcolorbox}
\begin{figure}[!htp]
    \centering
    \vspace{-8pt}
    \caption{
    Instruction Generation Prompt for ScienceWorld.
    }
    \label{fig:instruction_generation_sciworld}
\end{figure}

\begin{tcolorbox}[breakable,title=Instruction Refinement Prompt for ScienceWorld]
\textcolor{my_blue!50}{\textbf{Task Instruction:} You are a helpful assistant to do some scientific experiments in an environment. \\
In the environment, there are several rooms: kitchen, foundry, workshop, bathroom, outside, living room, bedroom, greenhouse, art studio, and hallway. \\
The available actions are: \\
open OBJ: open a container \\
… \\
\\
You will be given a task description and a corresponding trajectory. The task description concludes what you have done in this trajectory. You need to elaborate this description based on this environment by adding more details. \\
}\\

\textcolor{my_green!50}{\textbf{Example:} }\\
\textcolor{my_purple}{
\textbf{Task Description:} Your task is to grow an apple. You can find seeds in the kitchen. You should focus on the grown apple. \\
\textbf{Corresponding Trajectory:} \\
\textbf{look around} \\
This room is called the hallway. In it, you see: \\
...\\
\textbf{open door to kitchen}\\
The door is already open.\\
\textbf{go to kitchen}\\
You move to the kitchen.\\
...\\
\\
\textbf{Refined Task Description:} Your task is to grow an apple. This will require growing several plants, and them being crosspollinated to produce fruit. Seeds can be found in the kitchen. To complete the task, focus on the grown apple.
}\\
\\
\textcolor{my_blue!50}{Here is the task description you need to refine, and the corresponding trajectory is also provided:\\
...\\
\\
\textbf{Refined Task Description:} \\
}\\
\end{tcolorbox}
\begin{figure}[!htp]
    \centering
    \vspace{-8pt}
    \caption{
    Instruction Refinement Prompt for ScienceWorld.
    }
    \label{fig:instruction_refinement_sciworld}
\end{figure}

\begin{tcolorbox}[breakable,title=Positive Trajectory Synthesis Prompt for ScienceWorld]
\textcolor{my_blue!50}{\textbf{Task Instruction:} You are a helpful assistant to do some scientific experiments in an environment. \\
\\
In the environment, there are several rooms: kitchen, foundry, workshop, bathroom, outside, living room, bedroom, greenhouse, art studio, and hallway. \\
\\
The available actions are: \\
open OBJ: open a container \\
… \\
\\
Based on this environment, you need to randomly propose a Task Description, which concludes what you have done in this environment.\\
\\
Here are some examples:\\
Your task is to use chemistry to create green paint. When you are done, focus on the green paint.\\
Your task is to determine whether tall plant height is a dominant or recessive trait in the pea plant. If the trait is dominant, focus on the red box. If the trait is recessive, focus on the green box.\\
…
\\
Once you obtain the Task Description, you need to navigate through the environment to complete the instruction and generate a trajectory.\\
}\\
\textcolor{my_green!50}{\textbf{Example:} }\\
\textcolor{my_purple}{
\textbf{Task Description:} Your task is to use chemistry to create green paint. When you are done, focus on the green paint.\\
}\\
\textcolor{my_purple}{
\textbf{Trajectory:}\\
\textbf{Thought:} I need to find materials to create green paint, which are likely to be found in an art studio.\\
\textbf{Action:} look around\\
…\\
}\\
\textcolor{my_blue!50}{\textbf{Generated Trajectory: }
}\\
\end{tcolorbox}
\begin{figure}[!htp]
    \centering
    \vspace{-8pt}
    \caption{
    Positive Trajectories Synthesis Prompt for ScienceWorld.
    }
    \label{fig:pos_trajectories_synthesis_sciworld}
\end{figure}

\begin{tcolorbox}[breakable,title=Negative Trajectory Synthesis Prompt for ScienceWorld]
\textcolor{my_blue!50}{\textbf{Task Instruction:} You are a helpful assistant to do some scientific experiments in an environment. \\
\\
In the environment, there are several rooms: kitchen, foundry, workshop, bathroom, outside, living room, bedroom, greenhouse, art studio, and hallway. \\
\\
The available actions are: \\
open OBJ: open a container \\
… \\
\\
You will be given a task description and a corresponding trajectory. Based on them, you need to generate a negative sample that is similar to the correct trajectory but different from it. The generated trajectory should not meet all requirements of the task description. Moreover, the generated trajectory should satisfy all requirements of the environment.\\
}\\

\textcolor{my_green!50}{\textbf{Example:} }\\
\textcolor{my_purple}{
\textbf{Task Description:} Your task is to focus on the life stages of the apple plant, starting from earliest to latest. The plants are located outside.\\
\\
\textbf{Positive Trajectory:}\\
\textbf{look around}\\
This room is called the hallway. In it, you see: \\
…\\
\textbf{open door to outside}\\
The door is already open\\
…\\
\\
\textbf{Negative Trajectory:}\\
\textbf{look around}\\
This room is called the hallway. In it, you see: \\
…\\
\textbf{open door to kitchen}\\
The door is already open.\\
\\
\textbf{go to kitchen}\\
You move to the kitchen.\\
…\\
}\\
\textcolor{my_blue!50}{Here is the task you need to do:\\
... \\
\textbf{Negative Trajectory: }
}
\end{tcolorbox}
\begin{figure}[!htp]
    \centering
    \vspace{-8pt}
    \caption{
    Negative Trajectories Synthesis Prompt for ScienceWorld.
    }
    \label{fig:neg_trajectories_synthesis_sciworld}
\end{figure}


\paragraph{Reward Model Training Details.} 
The detailed hyperparameters we use for reward model during training and inference are shown in Table~\ref{tab:hyperparameters}. We employ
identical hyperparameters for reward models of different environments. For Webshop, we use checkpoint of 1100 steps in \Model-B, and checkpoint of 1200 steps in \Model-R and \Model-M. 
\begin{table}[ht]
    \centering
    \renewcommand\arraystretch{1.1}
    
    \scalebox{1.}{
    \begin{tabular}{c|c|c|c}
        \toprule
        \textbf{Name} & \textbf{ScienceWorld} & \textbf{Webshop} & \textbf{Game of 24}\\
        \midrule
        lora r & \multicolumn{3}{c}{64}\\
        lora alpha & \multicolumn{3}{c}{16}\\
        lora dropout & \multicolumn{3}{c}{0.0}\\
        lora target modules & \multicolumn{3}{c}{q\_proj, k\_proj, v\_proj, o\_proj, gate\_proj, up\_proj, down\_proj}\\
        epochs & 10 & 3 & 10\\
        batch size & 8 & 1 & 4 \\
        batch size per device & 1 & 1 & 1\\
        gradient accumulation steps & 16 & 4 & 16\\
        learning rate & 1e-5 & 2e-5 & 1e-5\\
        warmup ratio & 0.2 & 0.1 & 0.25\\
        checkpoint steps & 160 & 1100, 1200 & 1500\\
        temperature & 0.0 & 0.0 & 0.0 \\
        \bottomrule
    \end{tabular}
    }
    \caption{Detailed hyperparameters used in reward model.}
    \label{tab:hyperparameters}
\end{table}

\paragraph{Implementation Details of Ablation baselines.} For SFT, we use all positive examples from the reward model training as the training data. The training objective is to enable the model to predict the output of the LLM in the positive examples. 

For using few-shot prompting to guide the LLMs to predict the reward of historical trajectories, we use the following format of the few-shot prompt:
\begin{tcolorbox}[breakable,title=Few-shot Prompt for LLMs Directly Serving as ScienceWorld Reward Model]
\textcolor{my_blue!50}{\textbf{Task Instruction:} You are an autonomous intelligent agent tasked with evaluating the trajectories of the past experience. 
You will be given the history of a past experience in which you were placed in an environment and given a task to complete. These tasks will be accomplished through the use of specific actions.
Now you are trying to evaluate the performance on a past task. You will be given the objective of the task, the history of interaction including the observations you had and the actions you issued, and the status of the task. Your goal is to think about the strategy and provided path to produce a score ranging from 0 to 1 to measure whether the objective of the task has been reached. \\
\\
Here are 2 examples:\\
\\
\textcolor{my_green!50}{\textbf{Example1:} }\\
\\
\textbf{Human:} You are a helpful assistant to do some scientific experiment in an environment. In the environment, there are several rooms: kitchen, foundry, workshop, bathroom, outside, living room, bedroom, greenhouse, art studio, hallway. You should explore the environment and find the items you need to complete the experiment. You can teleport to any room in one step. All containers in the environment have already been opened, you can  }
\textcolor{my_blue!50}{directly get items from the containers. The available actions are: open OBJ: open a container, close OBJ: close a container, activate OBJ: activate a device, deactivate OBJ: deactivate a device, connect OBJ to OBJ: connect electrical components, disconnect OBJ: disconnect electrical components, use OBJ [on OBJ]: use a device/item, look around: describe the }
\textcolor{my_blue!50}{current room, examine OBJ: describe an object in detail, look at OBJ: describe a container's contents, read OBJ: read a note or book, move OBJ to OBJ: move an object to a container, pick up OBJ: move an object to the inventory, pour OBJ into OBJ: pour a liquid into a container, mix OBJ: chemically mix a container, teleport to LOC: teleport to a specific room, focus on OBJ: signal intent on a task object, wait: task no action for 10 steps, wait1: task no action for a step\\}
\textcolor{my_blue!50}{\textbf{Task Description:} Your task is to find a(n) non-living thing. First, focus on the thing. Then, move it to the purple box in the living room. Please only output one Thought and one Action. The available actions are: open OBJ: open a container, close OBJ: close a container, activate  OBJ: activate a device, deactivate OBJ: deactivate a device, connect OBJ to OBJ: connect electrical components, disconnect OBJ: disconnect electrical components, use OBJ [on OBJ]: use a device/item, look around: describe the current room, examine OBJ: describe an object in detail, look at OBJ: describe a container's contents, read OBJ: read a note or book, move OBJ to OBJ: move an object to a container, pick up OBJ: move an object to the inventory, pour OBJ into OBJ: pour a liquid into a container, mix OBJ: chemically mix a container, teleport to LOC: teleport to a specific room, focus on OBJ: signal intent on a task object, wait: task no action for 10 steps, wait1: task no action for a step. Please strictly follow the format of the actions.
% Now is your turn:
}\\
\textcolor{my_purple}{
\textbf{Assistant: }
Thought: I need to find a non-living thing, which could be an object in any room, so I should start by looking around the current room. Action: look around
}\\
\textcolor{my_blue!50}{
\textbf{Human: }Observation: This room is called the bathroom. In it, you see: the agent, a substance called air, a bathtub, which is turned off. In the bathtub is: nothing. a drawing, a glass cup (containing nothing), a sink, which is turned off. In the sink is: nothing. a toilet. In the toilet is: A drain, which is open, a substance called water. You also see: A door to the kitchen (that is open). Please only output one Thought and one Action. The available actions are: open OBJ: open a container, close OBJ: close a container, activate OBJ: activate a device, deactivate OBJ: deactivate a device, connect OBJ to OBJ: connect electrical components, disconnect OBJ: disconnect electrical components, use OBJ [on OBJ]: use a device/item, look around: describe the current room, examine OBJ: describe an object in detail, look at OBJ: describe a container's contents, read OBJ: read a note or book, move OBJ to OBJ: move an object to a container, pick up OBJ: move an object to the inventory, pour OBJ into OBJ: pour a liquid into a container, mix OBJ: chemically mix a container, teleport to LOC: teleport to a specific room, focus on OBJ: signal intent on a task object, wait: task no action for 10 steps, wait1: task no action for a step. Please strictly follow the format of the actions.
}\\
\textcolor{my_purple}{
\textbf{Assistant: }
Thought: The glass cup in the bathroom is a non-living thing, so I should focus on it. Action: focus on glass cup.
}\\
... \\
\textcolor{my_purple}{
\textbf{Assistant: }
Thought: I've found the purple box, and now I need to move the glass cup into it. Action: move glass cup to purple box.
}\\
\textcolor{my_blue!50}{
\textbf{Human:} Observation: No known action matches that input.
}\\
\textcolor{my_blue!50}{
\textbf{Human:} Please evaluate whether you complete the 'Task Description' or not.
}\\
Following your definitions, the task completion score is <0.750>\\
\\
\textcolor{my_green!50}{\textbf{Example2:} ...}\\
\\
\textcolor{my_blue!50}{Now is your turn:\\
}
\end{tcolorbox}
\begin{figure}[!htp]
    \centering
    \vspace{-8pt}
    \caption{
    Few-shot Prompt for LLMs Directly Serving as ScienceWorld Reward Model.
    }
    \label{fig:fewshot_sciworld}
\end{figure}

\subsection{More Qualitative Results.}
We show more qualitative results of our \Model in Fig.~\ref{fig:qualitative_vis_sci} and Fig.~\ref{fig:qualitative_vis_game24}. Based on the examples, we can see that the automatic reward models in our pipeline can distinguish good trajectories from bad ones and guide LLMs to generate better trajectories to finish the tasks.

\begin{tcolorbox}[breakable,title=Qualitative Visualization for ScienceWorld]
\textcolor{my_blue!50}{\textbf{Task Description:} Task Description: Your task is to find a(n) non-living thing. First, focus on the thing. Then, move it to the orange box in the living room.}\\
\textcolor{my_green!50}{\textbf{Trajectory A:} }\\
\textcolor{my_blue!50}{\textbf{Observation:}
You teleport to the living room.}\\
\textcolor{my_purple}{\textbf{Thought:} Now that I'm in the living room, I should look around to find a non-living thing that I can move to the orange box.  \textbf{Action:} look around}\\
\textcolor{my_blue!50}{\textbf{Observation:} This room is called the living room. In it, you see: \\ the agent, a substance called air, a chair. On the chair is: nothing. a couch. On the couch is: a white pillow. a finger painting a orange box (containing nothing) a table. On the table is: nothing. \\You also see: A door to the hallway (that is open). }\\
\textcolor{my_purple}{\textbf{Thought:} I see several objects in the living room, but I need to find a non-living thing that I can move to the orange box.   \textbf{Action}: examine white pillow}\\
ARMAP \textcolor{red}{\textbf{Aborted}}\\

\textcolor{my_green!50}{\textbf{Trajectory B:} }\\
\textcolor{my_blue!50}{\textbf{Observation:} This room is called the workshop. In it, you see: \\the agent, a substance called air, a table. On the table is: a battery, a blue wire, a orange light bulb, which is off, a red light bulb, which is off, a red wire, a switch, which is off, a yellow light bulb, which is off, a yellow wire. a ultra low temperature freezer. The ultra low temperature freezer door is open. In the ultra low temperature freezer is: nothing. You also see:\\A door to the hallway (that is open) }\\
\textcolor{my_purple}{\textbf{Thought: }I see several objects in the workshop that could be considered non-living things, such as the battery, blue wire, or light bulbs. I should focus on one of these objects to complete the task.  \textbf{Action:} focus on battery}\\
\textcolor{my_blue!50}{\textbf{Observation:}You focus on the battery.}\\
\textcolor{my_purple}{\textbf{Thought:} Now that I've focused on the battery, I need to move it to the orange box in the living room. \textbf{Action:} teleport to living room}\\
\textcolor{my_blue!50}{\textbf{Observation:}You teleport to the living room.}\\
\textcolor{my_purple}{\textbf{Thought:} Now that I'm in the living room, I should look around to find the orange box and move the battery into it.  \textbf{Action:} look around}\\
\textcolor{my_blue!50}{\textbf{Observation:} This room is called the living room. In it, you see:\\ the agent, a substance called air, a chair. On the chair is: nothing. a couch. On the couch is: a white pillow. a finger painting a orange box (containing nothing) a table. On the table is: nothing. \\You also see: A door to the hallway (that is open) }\\
\textcolor{my_purple}{\textbf{Thought:} I've found the orange box in the living room, and now I can move the battery into it. \textbf{Action:} move battery to orange box}\\
ARMAP \textcolor{darkgreen}{\textbf{Selected}}\\
\end{tcolorbox}
\begin{figure}[!htp]
    \centering
    \vspace{-8pt}
    \caption{
    Qualitative Visualization for ScienceWorld. Based on the comparison of the two trajectories, it is evident that the final action of trajectory B brings the resulting state closer to the task objective. Therefore, our model identifies trajectory B as the superior option.
    }
    \label{fig:qualitative_vis_sci}
\end{figure}



\begin{tcolorbox}[breakable,title=Qualitative Visualization for Game of 24]
\begin{minipage}[t]{0.5\textwidth}
\textcolor{my_blue!50}{\textbf{Input A:} 2 5 8 11}\\
\textcolor{my_green!50}{\textbf{Trajectory A1:} }\\
\textcolor{my_purple}{\textbf{Steps:} 8 * 2 = 16 (left: 5 11 16)\\11 + 5 = 16 (left: 16 16)\\16 + 16 = 32 (not 24, not valid)\\11 - 5 = 6 (left: 2 6 8)\\8 / 2 = 4 (left: 4 6)\\4 * 6 = 24 (left: 24)\\\textbf{Answer:} (8 / 2) * (11 - 5) = 24}\\
ARMAP \textcolor{darkgreen}{\textbf{Selected}}\\

\textcolor{my_green!50}{\textbf{Trajectory A2:} }\\
\textcolor{my_purple}{\textbf{Steps:}\\8 - 5 = 3 (left: 2 3 11)\\11 - 3 = 8 (left: 2 8)\\2 * 8 = 16 (left: 16)\\16 + 8 = 24 (left: 24)\\ \textbf{Answer:} (((8 - 5) - 8) * 2) + 8 = 24}\\
ARMAP \textcolor{red}{\textbf{Aborted}}\\
\end{minipage}
\begin{minipage}[t]{0.5\textwidth}
\textcolor{my_blue!50}{\textbf{Input B:} 2 3 6 9}\\
\textcolor{my_green!50}{\textbf{Trajectory B1:} }\\
\textcolor{my_purple}{\textbf{Steps:} 9 - 3 = 6 (left: 2 6 6)\\6 + 6 = 12 (left: 2 12)\\2 * 12 = 24 (left: 24)\\ \textbf{Answer: }2 * (6 + (9 - 3)) = 24}\\
ARMAP \textcolor{darkgreen}{\textbf{Selected}}\\
\\
\\
\\
\\
\textcolor{my_green!50}{\textbf{Trajectory B2:} }\\
\textcolor{my_purple}{\textbf{Steps:}\\9 - 3 = 6 (left: 2 6 6)\\6 - 2 = 4 (left: 4 6)\\6 * 4 = 24 (left: 24)\\ \textbf{Answer:} (6 * (9 - 3)) = 24}\\
ARMAP \textcolor{red}{\textbf{Aborted}}\\
\end{minipage}
\end{tcolorbox}

\begin{figure}[!htp]
    \centering
    \vspace{-8pt}
    \caption{
    Qualitative Visualization for Game of 24. Trajectory A and Trajectory B correspond to input A and input B respectively. Results show that our \Model  can accurately pick out the correct trajectory.
    }
    \label{fig:qualitative_vis_game24}
\end{figure}


\subsection{Failure Case Analysis}
\label{sec:failure}
In this section, we investigate the common failure cases of our framework, aiming to provide data points and insights for future research.

The most common error occurs when there are multiple restrictions in the instruction, the reward model overlooks some of these key conditions. A representative example is illustrated in Fig.~\ref{fig:failure_webshop1}, the model focuses on price and size but ignores the details about 'Fluoride' hidden in the product description.


Another common failure mode occurs when commonsense knowledge is involved. As demonstrated in Fig.~\ref{fig:failure_webshop2}, the agent was tasked with buying a blackout shade but failed to choose both the color and the size. While, in everyday life, size is generally more important, the reward model prioritized color instead.
In Fig.~\ref{fig:failure_science1}, the reward model cannot assess the lifespan of dragonflies and chipmunks because it lacks the necessary biological knowledge.
\begin{figure}[!htp]
    \centering
    \includegraphics[width=1\textwidth]{figures/failure1.pdf}  
    \caption{
    \textbf{Failure Example from Webshop}. The reward model ignores certain key conditions in the task instruction.
    }
    \label{fig:failure_webshop1}
\end{figure}
\begin{figure}[!htp]
    \centering
    \includegraphics[width=1\textwidth]{figures/failure2.pdf}  
    \caption{
    \textbf{Failure Example from Webshop}. The reward model misjudged the importance of different conditions, such as the size and color in this case. 
    }
    \label{fig:failure_webshop2}
\end{figure}
\begin{figure}[!htp]
    \centering
    \includegraphics[width=1\textwidth]{figures/failure3.pdf}  
    \caption{
    \textbf{Failure Example from ScienceWorld}. Reward models have problems when commonsense knowledge is involved, such as the inability to determine the shortest life span.
    }
    \label{fig:failure_science1}
\end{figure}


\textbf{Discussion.}
The analysis of failure modes highlights the significant potential of our framework. To improve its performance, we propose two possible strategies for improvements in reward modeling:
\textbf{(a)} Constructing Data with Focus on Complex and Detailed Conditions: enhancing the dataset to include scenarios with higher complexity and more nuanced conditions will help the framework better handle intricate situations and edge cases.
\textbf{(b)} Intervening in Reward Scoring with External Knowledge: incorporating external knowledge by combining a prompted Large Language Model with the trained reward model. This approach allows the LLM's generalized knowledge to calibrate the reward scores in a controllable manner, improving the overall accuracy and robustness of the reward model.
