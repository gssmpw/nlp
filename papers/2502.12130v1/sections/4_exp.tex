\section{Experiments}
In this section, we conduct a series of experiments to demonstrate the effectiveness of the proposed framework for agent tasks. First, we evaluate the framework's performance on standard agent benchmarks~\citep{yao2023webshopscalablerealworldweb,scienceworld2022,yao2023treethoughtsdeliberateproblem}, detailed in Section~\ref{exp:effect}. Next, we show how customizing the reward target during inference allows us to generate more tailored action plans, as described in Section~\ref{exp:control}. Finally, we conduct ablation studies in Section~\ref{exp:abs}. Before delving into the experimental results, we provide an overview of our experimental setup.

\subsection{Experimental Setup}
\paragraph{Environments.} We evaluate the \Model framework in three different environments: 

\vspace{-2mm}
\begin{itemize}

\item \textbf{Webshop} is a well-known environment for online shopping~\citep{yao2023webshopscalablerealworldweb}, where the agent must search for and select products on the website to obtain a final result. Following the setup of AgentBench~\citep{liu2023agentbench} for LLM evaluation, we test the model on the validation split, using the default matching reward as the evaluation metric.
    \item \textbf{ScienceWorld}~\citep{scienceworld2022} is an interactive benchmark designed for embodied science experiments. It places agents in a simulated text-based environment where they must perform elementary science experiments by navigating the environment, manipulating objects, and observing outcomes. The aim is to assess whether AI models can apply scientific knowledge, rather than merely retrieve or assemble information. We evaluate the framework on both seen and unseen splits.
    \item \textbf{Game of 24} is a mathematical game where the agent is given four numbers and must use arithmetic operations (addition, subtraction, multiplication, and division) to make the number 24. For instance, given the input '3, 5, 7, 11,' one possible solution is '$(7 - 3) * (11 - 5) = 24$'. Following~\cite{yao2023treethoughtsdeliberateproblem}, we selected 100 challenging puzzles, specifically those indexed from 901 to 1,000, and the performance metric is the success rate across these puzzles. As shown in Fig.~\ref{fig:train_data_game24} of the Appendix, we use the chain-of-thought prompting technique, prompting the LLM agents to output intermediate steps followed by the final answer. Each step of the solution is considered an action.
\end{itemize}

\paragraph{LLM Setup.}
Our framework requires LLM models to act as agents, generating synthetic task instructions from the environment along with few-shot examples in the prompt context. We also deploy agents to perform these synthetic tasks in the environment, collecting diverse trajectories for further analysis. In this paper, we primarily use the Llama3-70b-instruct model~\citep{dubey2024llama3herdmodels} to synthesize training data for the automatic reward models, as it is open-source, easy to deploy locally, and delivers robust performance. We avoid state-of-the-art commercial models like GPT-4 or Gemini due to their high costs and the complexity of reproducing results caused by frequent model updates, making them less suitable for our research objectives.

To evaluate the performance of various LLM agents, we serve a representative set of LLM APIs locally, balancing model diversity with affordable serving costs. We identify the LLMs by their model family and size. Specifically, these are Llama70B, Llama8B, Mistral7B, and Phi3.8B. We note that these open-source model families are frequently updated, and we provide the current model links in the Appendix~\ref{sec:llmapi}. All models can be easily set up using the vLLM library~\citep{kwon2023efficient} and a single H100 GPU.

\paragraph{Baselines.} 
We implement our \Model framework using different planning algorithms, including Reflexion, Best-of-N, and MCTS, which we denote as \textbf{\Model-R}, \textbf{\Model-B}, and \textbf{\Model-M}, respectively. We limit the maximum number of trajectories our \Model can explore to 10 in the ScienceWorld and Webshop environments to systematically evaluate the pipeline's effectiveness across different LLM agent backbones. We also compare the model with two baselines that do not use reward model guidance: \textbf{Sampling} and \textbf{Greedy}. For the {Game of 24} environment, we follow the setup of a previous study~\citep{yao2023treethoughtsdeliberateproblem} and set the maximum number of explored trajectories to 100. For \textbf{Sampling}, we set the model temperature to 1 and sample action trajectories using chain-of-thought prompting~\citep{wei2023chainofthoughtpromptingelicitsreasoning}. For \textbf{Greedy}, we set the temperature to 0, generating the action sequence with the highest probability. Further implementation details are provided in the Appendix. We will release all the code, model, and data for easy reproduction upon acceptance.

\subsection{Effectiveness for Reward Planning.}
\label{exp:effect}
In this section, we investigate the effectiveness of the framework across different language models~\citep{dubey2024llama3herdmodels,jiang2023mistral7b,abdin2024phi3technicalreporthighly} and various planning algorithms. The results are shown in Table~\ref{tab:comparison}. Based on the table, we have the following observations. First, our proposed pipeline is effective, as it consistently outperforms the \textbf{Sampling} and \textbf{Greedy} baselines across different planning algorithms. Additionally, we observe that the average improvement is more significant on weaker models, such as Phi~\citep{abdin2024phi3technicalreporthighly} and Mistral-7B~\citep{jiang2023mistral7b}, compared to stronger models like Llama3-1-70B~\citep{dubey2024llama3herdmodels}. We believe this is because weaker models explore more low-reward trajectories, providing greater opportunities for the reward model to improve performance.

Among the three planning algorithms, MCTS performs the best on average, likely due to its superior mechanisms for identifying higher-reward trajectories and searching less-explored trajectories. We also notice that Reflexion performs the worst on weaker models like Mistral7B and Phi3.8B. We suspect this is because Reflexion was designed for ChatGPT-family-based agents and requires the LLM agent to possess strong capabilities for learning from trial and error.
Finally, we present qualitative results of different methods in Fig.~\ref{fig:vis_webshop}, where it is clear that our \Model generates better trajectories than the baselines, aided by the guidance of automatic reward models.
In Appendix~\ref{sec:failure}, 
we analyze several failure cases, offer more detailed insights into the limitations of the current approach, and suggest potential improvements in reward modeling.
\input{tables/compare}

\begin{figure}[t]  
   \centering
   \includegraphics[width=1\textwidth]{figures/visualization.pdf}  
   \caption{Two qualitative results of the Webshop task. The figure shows two examples utilizing the advantages of our \Model framework and we are able to correct errors made by existing methods. In the top example, when the search results do not meet the requirements, our \Model method leverages the advantage of the tree structure to backtrack and search again, thereby retrieving the appropriate target item. In contrast, existing methods fail to backtrack when the target item is not found. In the bottom example, by using the \Model to evaluate different states in the environment, our method is able to select the color that offers a higher reward and better meets the requirements when choosing between size and color, rather than mistakenly selecting the wrong size. These two examples sufficiently demonstrate the advantages of our method compared to traditional approaches.}
   \label{fig:vis_webshop}  
\end{figure}

\subsection{Controllable Generation.}
\label{exp:control}
Another benefit of our \Model pipeline is that we can customize our reward targets during inference, allowing us to generate more controllable action sequences, rather than solely maximizing the predicted rewards. Agent fine-tuning methods~\citep{li2023camel,zeng2023agenttuning} find it challenging to achieve this goal since agent behaviors are typically fixed during inference. 
\input{tables/control}
We conducted experiments in the Webshop environment to evaluate the impact of customizable reward targets. In addition to the original objective of maximizing the predicted reward $\mathcal{R}(x,h)$, we defined two additional optimization targets. First, we aimed to minimize the number of actions in the trajectory history, defining the reward target as $\mathcal{R}(x,h) - \text{NumberOfAction}(h)$. Second, we sought to minimize the price of the target product, with a customized target of $\mathcal{R}(x,h) - \text{PriceOfProduct}(h)$. 
Table~\ref{tab:control} presents the results. By applying a length penalty on the reward target for \Model-M, we reduced the average action length from 4.5 to 4 and the average product price from 97.9 to 69.0, while maintaining comparable performance on the default matching reward. Similar performance was observed for \Model-B. Additionally, we provide a qualitative example in Fig.~\ref{fig:control}. From this example, we can see that our customized reward target successfully guided the LLM agent to purchase products with fewer action steps while still finding the target product.

\begin{figure}[t]  
   \centering
   \includegraphics[width=1\textwidth]{figures/control.pdf}  
   \caption{A typical example of customized reward target for shorter trajectory generation. On the left, we show the default greedy decoding generates a long trajectory without finding the target product. In the middle, we show our default reward can guide the LLM agent to generate a correct but long trajectory. On the right, we show our framework with a customized reward target for shorter trajectories, which finds a correct and short trajectory for the target product.}  
   \label{fig:control}  
\end{figure}

\input{tables/ablation}
\subsection{Ablation studies.}
\label{exp:abs}
We conduct ablation studies to investigate the effectiveness of the framework. Specifically, we aim to answer the following questions: 
\textbf{Q1.} Can we train a policy model with fully supervised learning to handle multi-step tasks from the synthesized trajectory data? 
\textbf{Q2.} Can a large, general language model be used as the reward model to perform guidance without automatic reward learning?

We conducted experiments using the ScienceWorld benchmark, and the results are shown in Table~\ref{tab:ablation}. When comparing our pipeline to the SFT model trained using our reward backbone VILA3B, we observed that although the policy model trained through fully supervised learning performed reasonably well (18.6), it still lagged behind the performance of our planning framework (28.3). This suggests that learning a policy model is more challenging than learning a reward model, highlighting the effectiveness of our proposed \Model pipeline (answering \textbf{Q1}).

Next, we replaced our smaller 3B reward model with a much larger language model, Llama3-1-70B, and used few-shot prompting to predict the reward of the extracted trajectories. We found that this larger model also improved performance compared to the default greedy model, demonstrating the effectiveness of our planning framework. However, it still performed worse than our pipeline using automatic reward learning, despite the Llama3-1-70B being about 20 times larger, further showcasing the efficiency and effectiveness of our approach (answering \textbf{Q2}).



We provide additional ablation experiments in the Appendix~\ref{sec:expabl}, including the data quality from various LLMs, reward modeling target and computational efficiency.