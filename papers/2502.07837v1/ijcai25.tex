%%%% ijcai25.tex

\typeout{IJCAIâ€“25 Formatting Instructions }

% These are the instructions for authors for IJCAI-25.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in

% The file ijcai25.sty is a copy from ijcai22.sty
% The file ijcai22.sty is NOT the same as previous years'
\usepackage{ijcai25}

% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[switch]{lineno}
\usepackage{multirow}
\usepackage{amssymb}

% Comment out this line in the camera-ready submission
%\linenumbers

\urlstyle{same}

% the following package is optional:
%\usepackage{latexsym}

% See https://www.overleaf.com/learn/latex/theorems_and_proofs
% for a nice explanation of how to define new theorems, but keep
% in mind that the amsthm package is already included in this
% template and that you must *not* alter the styling.
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.


% PDF Info Is REQUIRED.

% Please leave this \pdfinfo block untouched both for the submission and
% Camera Ready Copy. Do not include Title and Author information in the pdfinfo section
\pdfinfo{
/TemplateVersion (IJCAI.2025.0)
}

\title{RoboBERT: An End-to-end Multimodal Robotic Manipulation Model}


% Single author syntax
% \author{
%     anonymous submissions
%     % Author Name
%     % \affiliations
%     % Affiliation
%     % \emails
%     % email@example.com
% }

% Multiple author syntax (remove the single-author syntax above and the \iffalse ... \fi here)
%\iffalse
\author{
Sicheng Wang$^1$ \and
Jianhua Shan$^2$ \and
Jianwei Zhang$^3$\and
Haozhang Gao$^4$ \and
Hailiang Han$^4$ \and
Yipeng Chen$^4$ \and
Kang Wei$^4$ \and
Chengkun Zhang$^1$ \and
Kairos Wong$^1$ \and
Jie Zhao$^2$ \and
Lei Zhao$^2$ \and
Bin Fang$^5$ \\
% \and
% Second Author$^2$\and
% Third Author$^{2,3}$\And
% Fourth Author$^4$\\
\affiliations
$^1$Westlake University\\
$^2$Anhui Technology University\\
$^3$Hamburg University\\
$^4$Phibotnacci Inc.\\
$^5$Beijing Universuty of Post and Telecommunication\\
\emails
sw5425@nyu.edu,
2931@ahut.edu.cn,
jianwei.zhang@uni-hamburg.de,
kapuowen@gmail.com,
hailiang.han@phibotnacci.com,
yipeng.chen@phibotnacci.com,
kang.wei@phibotnacci.com,
chkunzhang@gmail.com,
kairos.wong@outlook.com,
jha06039@gmail.com,
zhaoleo2023@gmail.com,
fangbin1120@bupt.edu.cn
}
%\fi

\begin{document}

\maketitle

\begin{abstract}
    Embodied intelligence integrates multiple modalities, enabling agents to understand images, language, and actions simultaneously. However, existing models always depend on additional datasets or extensive pre-training to maximize performance improvements, consuming abundant training time and expensive hardware cost. To tackle this issue, we present RoboBERT, a novel end-to-end robotic manipulation model integrated with a unique training strategy. This model utilizes a CNN-based diffusion policy, enhancing and stabilizing the effectiveness of this model by separating training processes for different modalities. It also underscores the importance of data augmentation, verifying various techniques to significantly boost performance. Unlike models that depend on extra data or large foundation models, RoboBERT achieves a highly competitive success rate while using only language-labeled expert demonstrations and maintaining a relatively smaller model size. Specifically, RoboBERT achieves an average length of 4.52 on the CALVIN benchmark for \(ABCD \rightarrow D\) task, setting a new state-of-the-art (SOTA) record. Furthermore, when tested on a real robot, the model demonstrates superior performance, achieving a higher success rate than other methods trained with the same data. We propose that these concepts and methodologies of RoboBERT demonstrate extensive versatility and compatibility, contributing significantly to the development of lightweight multimodal robotic models. The code can be accessed on https://github.com/PeterWangsicheng/RoboBERT
    \footnote{Sicheng Wang and Jianhua Shan are co-author of this work. Bin Fang is corresponding author.}
\end{abstract}

\section{Introduction}

With the development of multimodal language models, existing artificial intelligence can not only understand human language and perceive the surrounding environment, but also generate action sequences to interact with the environment, forming embodied intelligence. Existing work relies on large pre-trained models or additional datasets, aiming to use a large amount of data to enable agents to fully generalize in different scenarios, and indeed significant progress has been made. 

For example, GR-1\cite{A34} pre-trains a GPT-style autoregressive model using datasets from large-scale video-language tasks and then fine-tunes it on robotic tasks, achieving SOTA results on benchmarks. RoboFlamingo\cite{A5} utilizes large visual language models as base models and then modifies the task heads to enable them to predict action sequences, significantly improving the performance of the policy compared to training from scratch. RT-2\cite{A7} makes improvements to large language models by adding additional tokens for action types and then conducts joint training on language, image, and action tasks, enabling the model to not only complete tasks but also demonstrate reasoning abilities. Unified-IO2\cite{A39}, as a large multi-task model, undergoes comprehensive training in aspects such as language understanding, image generation, and music composition, endowing it with a certain degree of action generation ability as well.

However, at present, restricted by the scarcity of action modality datasets and the heterogeneity of robots, it is extremely difficult to collect action data on the same scale as language datasets on the Internet for high-intensity pre-training following the training approach similar to that of large language models. Even if we are capable of collecting a large amount of data, training on these data will also consume a significant amount of computing resources and time. Therefore, how to improve the performance of policies as much as possible through excellent network design and full utilization of limited datasets is of great significance, as it can not only relieve the pressure of data collection but also reduce the training cost.

To achieve this goal, our work proposes a new training paradigm, including a lightweight multimodal fusion network that can understand various forms of observations and uses a diffusion model to generate actions. Moreover, it highly emphasizes the significant gain of data augmentation in end-to-end training. Without relying on pretraining and additional data, it still outperforms previous work. The contributions are summarized as follows: (1) A lightweight end-to-end robotic manipulation model adapted to multiple modalities including actions, images, and language is designed. It does not rely on large pre-trained models or extra dataset, greatly reducing the training cost. (2) During the training process, the method of data augmentation like add noise, affine transformation, mixup, etc. are particularly emphasized, and it outperforms other similar models without relying on additional data. (3) In addition to being tested in the challenging Calvin virtual environment and get the SOTA comparing to methods using language labeled trajectories, the effectiveness of this model has also been verified on real robots, successfully completing a variety of tasks.

\section{Related Works}

\subsection{End-to-end Robotic Model}
End-to-end robotic control refers to the process where explicit pose estimation, grasping planning, and action planning are not required, and actions are predicted directly through learning from robot observations\cite{A1}. Since it is not constrained by specific features extracting flow, theoretically, the model can make full use of original observation and learn across a broad range of tasks, and exhibit good adaptability and generalization. 

For instance, the MVP\cite{A1} method employs the MAE\cite{A2} encoder as its backbone, driving the downstream reinforcement learning process, and its feasibility has been validated in real-world settings\cite{A3}. The SHF\cite{A4} model fuses multimodal inputs using a self-attention structure to directly serve policy learning with inputs like images, tactile, and sound. Cliport\cite{A6} utilizes the Clip model to encode image observations and language instructions, combining with a novel double-flow architecture, enabling the agent to follow natural language instructions for only picking-and-placing. The RoboBert is a typical end-to-end robotic model, which allows it to utilize the input completely and execute various skills.

\subsection{Imitation Learning}
Imitation learning involves using human expert demonstration data as samples, where inputs are task-related metrics, including target locations, distances, or even image observations, and outputs are human actions. By supervised learning, the model aims to closely mimic the output of human experts under similar inputs. The simplest form of imitation learning is behavioral cloning (BC), which directly establishes a mapping from observations to actions. Subsequently, researchers have proposed implicit behavioral cloning (IBC)\cite{A28} based on energy modeling, and diffusion policy\cite{A29} models based on vector fields, among others. Compared to another robotic policy learning approach, reinforcement learning, imitation learning does not require long-term interaction and trial-and-error with the environment but relies on a large number of high-quality expert demonstrations. It also faces the issue of distribution shift, where deviations in actions cause deviations in observations, ultimately leading observations to deviate from the dataset, resulting in performance degradation. The RoboBert will use diffusion policy to imitate the expert action, achieving the desired performance.

\subsection{Multimodal Fusion: Projection, Query, Fine-Tuning}
Humans receive multiple forms of sensory inputs, with significant differences between those modalities. Recent multimodal language models have demonstrated the excellent performance of transformers in modalities fusion. According to comprehensive reviews, current large models involve three primary modality fusion approaches: projection-based, query-based, and fine-tuning. Taking mainstream models\cite{A14}\cite{A14-2} as examples, projection-based work includes LLAVA \cite{A15}, which linearly maps the image modality into tokens for embedding into a large language model. BILP-2\cite{A16} and Flamingo\cite{A17} employ query-based modality fusion methods, using a preceiver resampler and cross-attention to fuse modalities. Fine-tuning methods, using lightweight structures like adapters and introducing trainable prompts combined with image-encoded information, align other tokens to understand images, as seen in LLaMA-Adapter\cite{A22}. RoboBERT  will adopt the modality fusion method from openFlamingo, integrating vision, language and action by cross attention and preceiver resampler.

\section{Method}

\subsection{RoboBert}
In this section, we will introduce the problem addressed by this project and the model used to solve it, named RoboBert, because our model utilizes a Bert encoder for processing language. We also describe the training methods tailored to the characteristics of this model.

\subsection{Problem Definition}
This work implements an end-to-end, language-conditioned robotic operation agent, where the agent model $\mathbf{M}$ should be able to accept natural language inputs $\mathbf{L} \in \mathbb{R}^l$ and raw observation sequences $\mathbf{V} \in \mathbb{R}^{t \times c \times h \times w}$, integrating these two different modality inputs. The model adjusts its internal parameters \( \theta \) through a designated training method to make its output action \( \hat{\mathbf{A}} \in \mathbb{R}^a \) closely approximate the expected action \( \mathbf{A} \in \mathbb{R}^a \). This can be expressed in the formula as:
\[
\theta = \arg\min_{\theta} \left( M_{\theta}(\mathbf{L}, \mathbf{V}) - \mathbf{A} \right) = \arg\min_{\theta} \left( \hat{\mathbf{A}} - \mathbf{A} \right)
\]
To address such problems, mainstream models \cite{A24}\cite{A25}\cite{A27}\cite{A33}\cite{A39} typically divide the process into three steps. The first step is feature extraction, where raw inputs are encoded, denoted as $M_{{ext-}\theta}$. The second step is the fusion of observation encodings, denoted as $M_{{fusion-}\theta}$. The third step is policy generation, denoted as $M_{{policy-}\theta}$, which provides the probability distribution of corresponding actions by analyzing the fused results from the second step. Therefore, the model can be represented as:
\[
M_{\theta}(\mathbf{L}, \mathbf{V}) = 
\]
\[
M_{{policy-}\theta}\left(M_{{fusion-}\theta}\left(M_{{ext-L}\theta}(\mathbf{L}), M_{{ext-V}\theta}(\mathbf{V})\right)\right)
\]
Each module performs a specific function, and is interconnected, with their performance mutually influencing each other. Designing and training these modules to maintain optimal performance is the focus of this work.

\subsection{Model Structure}
This section will introduce the structure of roboBert.

\subsubsection{Overview}

\begin{figure*}[t] 
\centering 
\includegraphics[scale=0.5]{f1.png}
\caption{(a) The structure of roboBert is mainly composed of language connectors, modality fusioner, and diffusion heads, which perform synonymous sentence understanding, modal mixing, and policy generation, respectively. The last layer of the ViT is unfreezed during training to adapt to the task.(b) The working flow of policy is to first take the observations of the last 1-2 frames, predict the actions for a long period of frames, and output the actions for the near future. After the output is completed.Take observations again, and repeat the cycle above.} 
\label{1}
\end{figure*}

Similar to other approaches, this model consists of three components: the feature extractor $M_{{ext-}\theta}$, the modality fusion $M_{{fusion-}\theta}$, and the action head $M_{{policy-}\theta}$. The internal implementation is shown in Fig.1, with detailed explanations provided in the subsequent sections.

\subsubsection{Feature Extractor $M_{{ext-}\theta}$}
Because the model just needs to comprehend the simple linguistic instruction, without the need for complex task reasoning, we opted for a relatively lightweight language model BERT, often used for synonymous sentence inference. It converts the instruction input from flexible strings into a matrix of tokens. To better adapt BERT's language encoding to the subsequent model stages, we added a ``Language Connector'', a perceiver resampler-based fine-tuning head, to further process the linguistic information and extract task-related features.

For image modality inputs, we employ the CLIP model's Vision Transformer (ViT)\cite{A37}, which is trained via a text-image contrastive methodl, which has forming the align of natural languages and image observations. The pertrained CLIP will provide a very good initialization for training of RoboBERT.

\subsubsection{Modality Fusion $M_{{fusion-}\theta}$}
After preprocessing by the feature extractor, the various modality inputs are transformed into corresponding tokens. The model then need to integrate data from diverse sources and formats. It employs a structure of transformer decoder without casual mask, which is also utilized by the OpenFlamingo multimodal large model, where language serves as the query and observations act as keys and values. This fusion is facilitated through several layers of cross-attention and self-attention. The result of this fusion is a set of latent semantic tokens, which are then compressed together through a max-pooling operation to represent the current multimodal observation.

\subsubsection{Action Head $M_{{policy-}\theta}$}
We employ a CNN-based diffusion model\cite{A29} for the action head. This diffusion policy model takes a noise vector and the latent from the modality fusion module \( \mathbf{M}_{{fusion-}\mathbf{\theta}} \) from recent timesteps as inputs. They serve to generate seeds and provide denoising conditions respectively. Under the constraints of the multimodal observation representation produced by \( \mathbf{M}_{{fusion-}\mathbf{\theta}} \), the action head iteratively transforms the pure noise seed into a sequence of action vectors, predicting actions over a future interval. The most recent predictions are selected as the final action outputs, which are illustrated in Figure 1(b).

\subsection{Training Method}


\subsubsection{Training Target}
Our model is an end-to-end system that does not involve target, pose detection or path plan; all task-relevant information is entirely stored in original images and extracted solely by the model itself. We focus only on the inputs and outputs of the model. Therefore, we employ the Behavioral Cloning (BC) algorithm for predicting actions from observations, ensuring that our agent mimics expert demonstrations under identical observations. The weight update formula is given by:
\[
\Delta \theta = \alpha \frac{\partial L}{\partial (\mathbf{L}, \mathbf{V})} = \alpha \frac{\partial (\hat{\mathbf{A}} - \mathbf{A})^2}{\partial (\mathbf{L}, \mathbf{V})} = \alpha \frac{\partial (M_{\theta}(\mathbf{L}, \mathbf{V}) - \mathbf{A})^2}{\partial (\mathbf{L}, \mathbf{V})}
\]

However, the action head inside the model actually predicts the noise from polluted expert action, not predicts the action directly. \(M_{policy-\theta} \) will predict the Gaussian noise with the variance \( \epsilon^k \)corresponding to the denoising iteration \(k\) with condition from \( M_{fusion-\theta}\), finally recovering the unpolluted sample \(\mathbf{A}^0\). The formula will be transformed into,

\[
\Delta \theta = \alpha \frac{\partial (M_{policy-\theta}(\mathbf{A}^0 + \epsilon^k, k | M_{fusion-\theta}(\cdot)) - \epsilon^k)^2}{\partial (\mathbf{L}, \mathbf{V})}
\]

\subsubsection{Two-stage Training}

\begin{figure}[t] 
\centering 
\includegraphics[scale=0.21]{two-stage.png}
\caption{The illustration of two-stage training. Red and blue blocks represent the activate and freezed modules (a) The first stage training, predicting the corresponding output according to the stable and simple linguistic labels. (b) The second stage training, unfreeze all the parts and train on the natural languages}
\label{1}
\end{figure}

Additionally, the model has two tasks: language understanding and action generation. We believe that direct training with varied natural language inputs can destabilize the training due to unfixed label values (synonymous sentences). This could increase the training burden as the model needs to understand synonymous sentences and learn policy simultaneously. Hence, training is split into two stages; the first involves training with a single, consistent sentence, termed `standard language', to ensure stable label values and allow the model to focus on learning policy. The trainable parameters and training language for this step are:
\[
\theta \in \{M_{{ext-L}\theta}, M_{{fusion-}\theta}, M_{{policy-}\theta}\}
\]
\[
\mathbf{L} \in \{\text{standard language}\}
\]

Note that for accelerating the training and protecting the pertaining of the clip encoder in the first stage, the vision encoder, which always consumes abundant calculations during training, will be frozen except the last layer.

Upon completing the first stage, the model has the ability to align the fixed instructions and their corresponding actions. The second stage training will be applied to the model and natural and diverse language labels will be injected into the model. By this way, the natural language will align the action very fast and well based on the foundation of the standard language. Because the initial loss in the second-stage actually is marginal, the risk of pertaining being damaged is limited. Thus, all the parameters will be unfrozen including the vision encoder to further fine-tune the action generation.

The trainable parameters and training language for this phase are:

\[
\theta \in \{M_{{ext-L}\theta}, M_{{ext-V}\theta},M_{{fusion-}\theta}, M_{{policy-}\theta}\}
\]
\[
\mathbf{L} \in \{\text{standard language},\text{natural language}\}
\]

With the standard language and policy aligned in the first stage and the natural language aligned with the standard language in the second stage, the natural language and policy are naturally aligned. The two-stage training is shown in Figure 2 (a)(b).

\subsubsection{Data Augmentation}

\begin{figure}[t] 
\centering 
\includegraphics[scale=0.28]{augmentation.png}
\caption{The illustration of data augmentations. (a)From left to right, it shows the original, polluted by salt-and-pepper noise, translation, color jitter images (b) It shows the new data generated by mixing two RGB observations and corresponding action vectors. }
\label{1}
\end{figure}

Furthermore, since our model operates end-to-end, it directly processes images as observations. These images often contain a significant amount of redundant information, which means that even if the images are degraded, the critical information necessary for robot operation should remain intact. For example, when image is polluted by noise, the model should can capture critical information for operation from the unpolluted part or entire image. We emphasize the importance of the data augmentation and apply some augmentation techniques in this work.

\paragraph{Salt-and-pepper Noise}
Salt-and-pepper noise is a form of impulse noise. It randomly presents as white (salt) and black (pepper) pixels in an image, just like scattered grains of salt and pepper. Introducing this kind of noise can force the model focus on the meaning of pixels as a entire body, not rely on the specific pixel. The effect is demonstrated in Figure 3(a) second graph.

\paragraph{Affine Transformation}
The creatures don not rely on the stable or specific perspective when performing the various task. The observation might undergo translation, rotation, stretch, scaling and distortion because the uncertainty of the body position. However, the human being and other animals can understand the relative relationship of different stuffs. 
By introducing affine transformation, the image can be applied the augmentation mentioned above to imitate the physical varying of the perspective, helping the model comprehend the physical information from the RGB. The effect is demonstrated in Figure 3(a) fourth graph.

\paragraph{Color Jitter}
One of the important information of image giving us comes from the high-frequency components, for example, edge, contour, shape, texture. The low-frequency property like color, illumination, temperature should not inference the the determination of the model severally. Inspired by this notion, and drawing from the training strategies used in YOLO\cite{A35}, color jitter is used. For color insensitive operation, the jitter in HSV space will help the model concentrate on the shape of the objects and generalize on the color. Specifically, for CALVIN benchmark, the color-related tasks always involve the simple pure color like pink, red and blue. Thus, we apply a mask to protect the corresponding hue, and just changing the color in the environment. The effect is demonstrated in Figure 3(a) third graph.

\paragraph{Robotic Mixup}
Mixup \cite{A43} is a very popular augmentation of the machine learning, widely absorbed by diverse tasks like computer vision, natural language processing, audio recognition, etc. It functions by creating new, synthetic training examples through the combination of pairs of data points. For instance, when dealing with image data, mixup blends two different images together, along with linearly interpolating their corresponding labels. By doing so, it significantly increases the diversity within the training dataset. It enriches the diversity of the data and so does the generalization. The similar technique is also applied here on the robotic manipulation.

Specifically, we will randomly select two samples and generate a new sample by calculating the weighted average of inputs like RGB, language tokens, etc. and labels, namely actions. The following formula shows the process; $x$ and $y$ are inputs and labels respectively. 
\[
x_{mix}=\lambda x_{0} + (1-\lambda)x_{1}
\]
\[
y_{mix}=\lambda y_{0} + (1-\lambda)y_{1}
\]

The weight $\lambda$ will be sampled from beta distribution of $\alpha=0.4$, shown as
\[\lambda \sim \text{Beta}(0.4, 0.4)\]

The effect is demonstrated in Figure 3(b). The mixup observation demonstrates a transparent effect for both two images, the language embedding and action vector will be weighted averaged in the same way.


\section{Experiment}

\subsection{Experimental Setup}

\subsubsection{Simulation Environment and Dataset}
We use the Calvin dataset\cite{A20}, which provides a desktop simulation environment based on PyBullet, designed with a series of robotic tasks and evaluation procedures. The Calvin contains 23 categories tasks, including lift, push, rotate, move, etc. operations and these operations are required to be accomplished sequentially, introducing a lot of uncertainties and randomness. Thus, it is a very challenging benchmark. The dataset includes a large number of expert demonstrations, divided into several subsets. We will use the \(ABCD \rightarrow D\) and \(ABC \rightarrow D\) subset. Additionally, only the expert demonstrations in the data include natural language descriptions of actions are used during training. According to the protocols of the environment, all the tests include 1000 groups consisting of 5 subtasks with designate natural language instruction.

For verifying the effectiveness of RoboBERT in the real physical environment, we also conduct the real robot experiment. We use the REALMAN RM65B arm with 6 degrees of freedom. We design a series of tasks to evaluate the performance of RoboBERT in individual tasks and instruction-following tasks. For equipping the model the capability resolving these tasks, 25-30 trajectories are collected by manual teleoperation for each task, including static and gripper camera RGBs and action increment. For languages label, besides given by human, GPT also is used to generate various expressions for identical task.

\subsubsection{Training Configuration}
For hardware during training, we utilized two RTX 3090s with 24GB each. When we use distributed training across two RTX 3090 24GB GPUs on dataset \(ABCD \rightarrow D\), completing ten training cycles in the first-stage training, each lasting about 40 minutes; completing five training cycles in the second-stage training, each lasting about 90 minutes.

Compared to other models that utilize large pre-trained frameworks, our training costs were significantly reduced.

\begin{table*}[h]
\centering
\begin{tabular}{@{}llcccccccc@{}}
\toprule
\textbf{Model Name} & \textbf{Observation}  &\textbf{Pertrain} & \textbf{Parameters(T)}  & \textbf{Task 1} & \textbf{Task 2} & \textbf{Task 3} & \textbf{Task 4} & \textbf{Task 5} & \textbf{Avg. Length} \\ 
\midrule

HULC&	S+G	&N	&100M &88.9\%	&73.3\%	&58.7\%	&47.5\%	&38.3\%	&$3.06\pm0.00$ \\
RoboFlamingo&	S+G	&Y &1000M	&96.4\%	 &89.6\%	&82.4\%	&74.0\%	&66.0\%	&$4.08\pm0.00$ \\
DeeR&	S+G	&Y &1000M	&98.2\%	&90.2\%	 &82.1\%	&75.9\%	&67.0\%	&$4.13\pm0.00$ \\
GR-1&	S+G+P	&Y	&130M &94.9\%	 &89.6\%	&84.4\%	&78.9\%	&73.1\%	&$4.21\pm0.00$ \\
MoDE(w. per.)&	S+G	&Y	&436M &97.1\%	&92.5\%	 &87.9\%	&83.5\%	&77.9\%	&$4.39\pm0.04$ \\
\textbf{RoboBERT(Ours)} &	S+G	&N &208M	&\textbf{98.8\%}	&\textbf{95.2\%}	 &\textbf{91.1\%}	&\textbf{86.5\%}	&\textbf{80.9\%}	&\textbf{$4.52\pm0.03$} \\

\bottomrule
\end{tabular}
\caption{CALVIN Performance Comparison on \(ABCD \rightarrow D\)}
\label{tab:robotic_models}
\end{table*}

\begin{table*}[h]
\centering
\begin{tabular}{@{}llcccccccc@{}}
\toprule
\textbf{Model Name} & \textbf{Observation}  &\textbf{Pertrain} & \textbf{Parameters(T)}  & \textbf{Task 1} & \textbf{Task 2} & \textbf{Task 3} & \textbf{Task 4} & \textbf{Task 5} & \textbf{Avg. Length} \\ 
\midrule

RoboFlamingo&	S+G	&Y &1000M	&82.4\%	 &61.9\%	&46.6\%	&33.1\%	&23.5\%	&$2.47\pm0.00$ \\
DeeR&	S+G	&Y &1000M	&86.2\%	&70.1\%	 &51.8\%	&41.5\%	&30.4\%	&$2.82\pm0.00$ \\
GR-1&	S+G+P	&Y	&130M &85.4\%	 &71.2\%	&59.6\%	&49.7\%	&40.1\%	&$3.06\pm0.00$ \\
3D Diffuser Actor&	S+G+P+C	&N	&200M &92.2\%	 &78.7\%	&63.9\%	&51.2\%	&41.2\%	&$3.27\pm0.04$ \\
MoDE(w/o per.)&	S+G	&N	&307M &91.5\%	&79.2\%	 &67.3\%	&55.8\%	&45.3\%	&$3.39\pm0.03$ \\

\textbf{RoboBERT(Ours)} &	S+G	&N &208M	&\textbf{95.3\%}	&\textbf{85.7\%}	 &\textbf{75.4\%}	&\textbf{66.3\%}	&\textbf{56.2\%}	&\textbf{$3.79\pm0.03$} \\

\bottomrule
\end{tabular}
\caption{CALVIN Performance Comparison on \(ABC \rightarrow D\)}
\label{tab:robotic_models}
\end{table*}

\begin{table*}[t]
\centering
\begin{tabular}{@{}ccccccccc@{}}
\hline
\multirow{2}{*}{ \textbf{Model Name}} &  \multicolumn{4}{c}{ \textbf{Sequential tasks}} & & \multicolumn{3}{c}{ \textbf{Indivdual tasks}} \\
\cline{2-5} \cline{7-9}
& Trans. D. & Trans. C. & Open D. & Close D. & & Stack Cube & Trans. P. & Open Door\\
\hline
MT-ACT & 72\% & 68\% & 73\% & 80\% &  & 72\% & 73\% & 78\%\\
RT-1 & 61\%& 56\%& 64\%& 72\% &  & 65\% & 60\% & 72\% \\
\textbf{RoboBERT(Ours)} & \textbf{86\%} & \textbf{87\%} & \textbf{80\%} & \textbf{92\%} &  & \textbf{90\%} & \textbf{80\%} & \textbf{82\%}\\
\hline

\end{tabular}
\caption{Comparison of Different Training Methods for Real Robot Experiments}
\end{table*}



\begin{table}[t]
\centering
\begin{tabular}{@{}llcccccc@{}}
\toprule
\textbf{Training Method} & \textbf{Dataset} & \textbf{Avg. Length} \\ 
\midrule
NL. directly & \(ABCD \rightarrow D\) & $3.39\pm0.05$ \\
Two-stage &\(ABCD \rightarrow D\)	& $4.52\pm0.03$ \\
NL. directly & \(ABC \rightarrow D\) & $1.25\pm0.04$ \\
Two-stage &\(ABC \rightarrow D\)	& $3.79\pm0.03$ \\

\bottomrule
\end{tabular}
\caption{Comparison of Different Training Methods}
\label{tab:training_comparison}
\end{table}

\begin{table}[t]
\centering
\begin{tabular}{@{}llcccccc@{}}
\toprule
\textbf{Augmentation} & \textbf{Avg. Length} & \textbf{Increment} \\ 
\midrule
No Augmentation & $3.00\pm0.02$ & $+0.00$ \\
Salt-and-Pepper & $3.22\pm0.03$	& $+0.22$ \\
Affine Transformation & $2.75\pm0.03$ & $-0.25$ \\
Color Jitter & $3.65\pm0.02$	& $+0.65$ \\
Robotic Mixup & $3.23\pm0.04$	& $+0.23$ \\
Combining All & $3.52\pm0.02$	& $+0.52$ \\
Combining All w/o Aff. & $3.79\pm0.03$	& $+0.79$ \\


\bottomrule
\end{tabular}
\caption{Comparison of Different Augmentation}
\label{tab:training_comparison}
\end{table}

\subsection{Baseline}
We compared our model against five baseline models: HULC\cite{A32}, GR-1\cite{A34}, DeeR\cite{A40}, MoDE\cite{A41}, 3D diffuser Actor\cite{A42} and RoboFlamingo\cite{A5}. HULC utilizes hierarchical task representations combined with a VAE encoder to fuse multimodal information, which is used for underlying policy learning. GR-1 leverages a pre-trained model on large-scale video tasks, subsequently duplicating weights for fine-tuning on robotic tasks. RoboFlamingo utilizes a large multimodal language model as a modality fusion, using the fused latent vectors for imitation learning. DeeR-VLA introduces a multi-exit architecture in MLLMs and develops algorithms to set early-termination criteria based on demands like computational cost and GPU memory usage, which reduce the calculation. MoDE uses a diffusion policy that uses a mixture of experts transformer with a noise-conditioned routing strategy and expert caching mechanism to enhance performance and efficiency in imitation learning. 3D diffuser Actor uses a 3D denoising transformer to fuse information from 3D visual scenes, language instructions and proprioception to predict robot pose trajectory.

\subsection{Result of Real Robot Experiments}

\subsubsection{Model Comparison}

Here we compare the performance of previous most advanced model in the table 1 and 2. In those two tables, observation means the modalities it use: S, G, P and C represents static camera, gripper camera, proprioception and camera parameters respectively. Pretrain means if the model use the large pertrain or extra data: Y, N for yes and no. Parameters(T) means the approximate trainable parameters.

\paragraph{Imitation performance}
As observed in Table 1 showing the successful rates given the dataset \(ABCD \rightarrow D\), our model demonstrates superior performance compared to all the models. Most pervious best models like GR-series, RoboFlamingo or MoDE all rely on remained unlabeled data in Calvin or extra open-source robotic data, usually taking more restore space and training cost. Their ablation experiments also prove that the significant performance loss will occur without the large-scale pertaining. Some models like GR-1 utilize more modalities like proprioception and data (2.7TB) but still have weaker successful rates than ours. Another notable advantage of roboBERT is amount of weights, which is also very competitive to other advanced models like MoDE, DeeR and RoboFlamingo. The later two methods actually are renovation of large foundations model, taking billions of parameters.

Overall, our model has making full use of the limited dataset by powerful data augmentation, mitigating the dependency of external data, showing the incredible improvement in this benchmark.

\paragraph{Zero-shot Experiment}
The table 2 shows that the performance when facing a unfamiliar D which does not appear in training dataset ABC, that is \(ABC \rightarrow D\). The model must use the experience learned from different environment to adapt on the novel environment. The result shows the similar ranking for our model on \(ABC \rightarrow D\). Comparing to the model utilizing the extra dataset or more observations input like proprioception and depth, our method still present very outstanding adaptivity and generalization, also exceeding these methods. Although our method do not have the better performance comparing to MoDE with extra data, our successful rate is higher under the fair situation that using the same size of dataset.

\subsubsection{Ablation Studies}

\paragraph{Two-stage Training Method}
To verify that the two-stage training method indeed enhances training effectiveness, we conduct the experiments that using natural languages directly (NL. directly) and two-stage training methods (Two-stage) on two dataset respectively.

The result is in Table 3. It observed that using the diverse natural languages and corresponding actions to train directly will largely limit the performance. It is very hard for model to understand the matching relationship for two modalities simultaneously. However, with the assistance of first-stage training, a very good weight initialization has formed to distinguish the policy for stable, simple instruction. The difficulty of comprehending complicated expression has mitigated significantly based on correct optimization direction provide by previous training of easier tasks. 


\paragraph{Data Augmentation}
To evaluate the effectiveness of data augmentation, we conducted comparative experiments on the dataset \(ABC \rightarrow D\) for different kinds of data augmentation. In the experiment, the salt-and-pepper noise (SNR = 0.95), random translation (amplitude = 15\%), Color Jitter (amplitude for HSV = 0.4), robotic mixup (\(\alpha = 0.4\)) and their combination is tested. The speific intensity for data augmentation like SNR, amplitude and others are hyperparameters which is set by trials empirically and take the best one.

As it shows, not all the augmentation will give the positive influence for our methods. The random translation, theoretically give the diverse perspective of observation, but reducing the successful rate. The similar phenomenon is also observed test for \(ABCD \rightarrow D\) and real robot experiments. We suggest that the change of the perspective may introduce the ambiguity for space position. For example, when the cube is located in the right of the gripper camera, the model may not know if the cube is moved to right due to the augmentation or it is on the right really. Although the gripper in the image can be a good reference to eliminate the ambiguity, it may be hard for model to comprehend it.

The color jitter gives the largest boost to the performance. The difference of expert demonstrations and test environment comes from the layout and color. The color jitter can largely eliminate the influence of the latter one, explicitly prompting the model that operations are unrelated to the color but shape, except the color-sensitive ones (which is also applicable by protecting the specific color). It is very useful to assist the model overcome the disturbance of different illumination, cameras setups, shadows and algorithm calibrations in reality.

Both salt-and-pepper noise and robotic mixup give the similar and moderate gain towards the performance, suggesting that even simple and easy-implemented augmentation can improve the end-to-end robotic operations. For verifying their joint effectiveness, we apply all the augmentations mentioned above and conduct the test, labelled as Combining All, and all augmentations expelling the Affine Transformation giving the negative contribution, labelled as Combining All w/o Aff. The result shows that altogether improvement is undoubtedly higher than single ones but not simply additions of each increment. The result also proves that Affine Transformation do not promote the performance further. 

\subsubsection{Real Robot Experiment}


\begin{figure}
    \centering
    \includegraphics[scale=0.35]{4.png}
    \caption{Some examples for real robot experiments. From left to right, sequential table tasks, moving pen to pen holder and stacking cubes}
    \label{fig:enter-label}
\end{figure}
\paragraph{Individual Tasks}

For evaluating if our method can accomplish specified task for different position configurations. We have designed some tasks, stacking cubes, recovering pen to penholder and open the cabinet door, denoted by Stack Cube, Trans. P. and Open Door respectively. The manipulated objects are positioned or initial pose of the robotic arm are set randomly. Besides, the test environment is not completely identical with that in dataset like different background objects. Just like Table 5 shows, in comparison with other popular language-conditioned robot model, RT-1\cite{A45} and MT-ACT \cite{A44}, our methods show better performance. However, it is also observed that end-to-end model usually is a little sensitive to some larger interference like strange objects in the center of the view, the model shows   ``confusion" or ``hesitation" towards these changes and successful rate will decline.

\paragraph{Sequential Tasks}
For evaluating if our method can follow the natural language instruction to complete long-sequence tasks, we also design the sequential tasks just like Calvin benchmark. Besides transferring objects from a random position, it also has the operation of articulation object like drawer. During the test, the next action is based on the result of last action, producing larger uncertainties and difficulties. The result in table 5 also shows the large potential of our method dealing with long-term tasks, exceeding other popular methods. However, it also faces some drawbacks. The model itself cannot determine if the task completes successfully, the switch is conducted by human currently. If we not do so, it will produce confusion and even performing other actions not belonging to corresponding language instructions. For example, it will automatically perform open the drawer after closing door without the instruction. This phenomenon also occurs in other methods. It should be improved in the future study by introducing a decision level to guide the low-level policy.

\section{Conclusion}
This paper have proposed a language-conditioned robotic multimodal operations model, featuring a novel modality-separated multiple stage training methods and data augmentation technique. The simulated and real experiments both show its competitive performance with a very light-weight structure and limited training data. It will bring some beneficial inspirations to other end-to-end robotic model designs.


% %% The file named.bst is a bibliography style file for BibTeX 0.99c
\bibliographystyle{named}
\bibliography{ijcai25}

\end{document}
