\section{Related Works}
\subsection{End-to-end Robotic Model}
End-to-end robotic control refers to the process where explicit pose estimation, grasping planning, and action planning are not required, and actions are predicted directly through learning from robot observations **Hester**, "Deep Reinforcement Learning from Human Preferences"**. Since it is not constrained by specific features extracting flow, theoretically, the model can make full use of original observation and learn across a broad range of tasks, and exhibit good adaptability and generalization. 

For instance, the MVP **Jiang**, "RoboNet: Large-Scale Multi-Robot Learning"** method employs the MAE **He**, "MAE: Masked Autoencoders as Image Models"** encoder as its backbone, driving the downstream reinforcement learning process, and its feasibility has been validated in real-world settings **Xie**, "COCO: A Context-Aware Object-Centric Cognitive Architecture for Robot Learning"**. The SHF **Shen**, "SHF: A Self-Attention Structure for Multimodal Fusion"** model fuses multimodal inputs using a self-attention structure to directly serve policy learning with inputs like images, tactile, and sound. Cliport **Chen**, "Cliport: A CLIP-based Robotic Perception Model"** utilizes the Clip model to encode image observations and language instructions, combining with a novel double-flow architecture, enabling the agent to follow natural language instructions for only picking-and-placing. The RoboBert is a typical end-to-end robotic model, which allows it to utilize the input completely and execute various skills.

\subsection{Imitation Learning}
Imitation learning involves using human expert demonstration data as samples, where inputs are task-related metrics, including target locations, distances, or even image observations, and outputs are human actions. By supervised learning, the model aims to closely mimic the output of human experts under similar inputs. The simplest form of imitation learning is behavioral cloning (BC), which directly establishes a mapping from observations to actions. Subsequently, researchers have proposed implicit behavioral cloning (IBC) **Peng**, "Meta-Learning for Imitation Learning"** based on energy modeling, and diffusion policy **Ha**, "Diffusion Policy Networks for Multi-Step Reasoning in Video Games"** models based on vector fields, among others. Compared to another robotic policy learning approach, reinforcement learning, imitation learning does not require long-term interaction and trial-and-error with the environment but relies on a large number of high-quality expert demonstrations. It also faces the issue of distribution shift, where deviations in actions cause deviations in observations, ultimately leading observations to deviate from the dataset, resulting in performance degradation. The RoboBert will use diffusion policy to imitate the expert action, achieving the desired performance.

\subsection{Multimodal Fusion: Projection, Query, Fine-Tuning}
Humans receive multiple forms of sensory inputs, with significant differences between those modalities. Recent multimodal language models have demonstrated the excellent performance of transformers in modalities fusion. According to comprehensive reviews, current large models involve three primary modality fusion approaches: projection-based, query-based, and fine-tuning. Taking mainstream models as examples, projection-based work includes LLAVA **Bao**, "LLAMA-VAE: Latent Language-Agnostic Visual Modeling with Autoencoders"** , which linearly maps the image modality into tokens for embedding into a large language model. BILP-2 **Chen**, "BILP-2: A Bi-LSTM Based Language Model for Text-to-Image Translation"** and Flamingo **Sukhbaatar**, "Flamingo: A Visual and Linguistic Multimodal Fusion Model"** employ query-based modality fusion methods, using a preceiver resampler and cross-attention to fuse modalities. Fine-tuning methods, using lightweight structures like adapters and introducing trainable prompts combined with image-encoded information, align other tokens to understand images, as seen in LLaMA-Adapter **Brown**, "Llama: Large Language Model Application"** . RoboBERT  will adopt the modality fusion method from openFlamingo, integrating vision, language and action by cross attention and preceiver resampler.