\section{Related Works}
\subsection{End-to-end Robotic Model}
End-to-end robotic control refers to the process where explicit pose estimation, grasping planning, and action planning are not required, and actions are predicted directly through learning from robot observations\cite{A1}. Since it is not constrained by specific features extracting flow, theoretically, the model can make full use of original observation and learn across a broad range of tasks, and exhibit good adaptability and generalization. 

For instance, the MVP\cite{A1} method employs the MAE\cite{A2} encoder as its backbone, driving the downstream reinforcement learning process, and its feasibility has been validated in real-world settings\cite{A3}. The SHF\cite{A4} model fuses multimodal inputs using a self-attention structure to directly serve policy learning with inputs like images, tactile, and sound. Cliport\cite{A6} utilizes the Clip model to encode image observations and language instructions, combining with a novel double-flow architecture, enabling the agent to follow natural language instructions for only picking-and-placing. The RoboBert is a typical end-to-end robotic model, which allows it to utilize the input completely and execute various skills.

\subsection{Imitation Learning}
Imitation learning involves using human expert demonstration data as samples, where inputs are task-related metrics, including target locations, distances, or even image observations, and outputs are human actions. By supervised learning, the model aims to closely mimic the output of human experts under similar inputs. The simplest form of imitation learning is behavioral cloning (BC), which directly establishes a mapping from observations to actions. Subsequently, researchers have proposed implicit behavioral cloning (IBC)\cite{A28} based on energy modeling, and diffusion policy\cite{A29} models based on vector fields, among others. Compared to another robotic policy learning approach, reinforcement learning, imitation learning does not require long-term interaction and trial-and-error with the environment but relies on a large number of high-quality expert demonstrations. It also faces the issue of distribution shift, where deviations in actions cause deviations in observations, ultimately leading observations to deviate from the dataset, resulting in performance degradation. The RoboBert will use diffusion policy to imitate the expert action, achieving the desired performance.

\subsection{Multimodal Fusion: Projection, Query, Fine-Tuning}
Humans receive multiple forms of sensory inputs, with significant differences between those modalities. Recent multimodal language models have demonstrated the excellent performance of transformers in modalities fusion. According to comprehensive reviews, current large models involve three primary modality fusion approaches: projection-based, query-based, and fine-tuning. Taking mainstream models\cite{A14}\cite{A14-2} as examples, projection-based work includes LLAVA \cite{A15}, which linearly maps the image modality into tokens for embedding into a large language model. BILP-2\cite{A16} and Flamingo\cite{A17} employ query-based modality fusion methods, using a preceiver resampler and cross-attention to fuse modalities. Fine-tuning methods, using lightweight structures like adapters and introducing trainable prompts combined with image-encoded information, align other tokens to understand images, as seen in LLaMA-Adapter\cite{A22}. RoboBERT  will adopt the modality fusion method from openFlamingo, integrating vision, language and action by cross attention and preceiver resampler.