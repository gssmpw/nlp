\section{Related work}
Modern machine learning tools have become indispensable in almost all industry and research fields.
As these tools find more and more applications, awareness of their limitations has simultaneously spread.
Chief among these is the lack of interpretability inherent to many of the most potent ML methods.
Understanding why ML models produce a specific prediction instead of something else poses obstacles in their adoption in, e.g., high-trust applications \cite{dosilovic2018survey}.
Such opaque methods are colloquially called black-box or closed-box methods, in contrast with white- or open-box methods, characterised by their interpretability to humans.
Examples of closed-box methods include random forests and deep neural networks, whereas, e.g., statistical models like linear regression models are often considered open-box methods.
As a result, the field of XAI has grown substantially in the last decade.
This section describes several post-hoc local explanation and model aggregation methods to give the reader context for the {\sc ExplainReduce} procedure.

XAI methods generally take a dataset and a closed-box function as inputs and produce an explanation that describes the relationship between the inputs and outputs of the closed-box model.
One commonly used approach is to produce post-hoc explanations using local (open-box) surrogate models.
In this approach, given a closed-box method $f: \mathcal{X} \rightarrow \mathcal{Y}, f(\bm{X}) = \hat{\bm{y}}$, a local surrogate model $g$ is another function that replicates the behaviour of the closed-box method in some region of the input space.
Mathematically, given a loss measure $\ell$ and a point in the input space $\bm{x}_i$, we might consider models $g$ such that $\ell(f(\bm{x}'_i), g(\bm{x}'_i)) \leq \varepsilon \; \forall \bm{x}'_i \in \{\bm{x} \in \mathcal{X} | D(\bm{x}'_i, \bm{x}_i) \leq \delta \}$ as local surrogate models.

Many methods have been proposed to produce local surrogate models as explanations.
Theoretically, the simplest way to produce a local surrogate model would be to calculate the gradient of the closed-box function.
This XAI method is often called {\sc vanillagrad} in the literature.
However, in practice, the gradients of machine learning models can be very noisy, as demonstrated by the effectiveness of adversarial attacks that exploit small perturbations in neural networks \cite{szegedy2014intriguing}.
On the other hand, many commonly used machine learning models, such as random forests, do not have well-defined gradients.
Hence, more involved approaches are warranted. 

{\sc smoothgrad} \cite{smilkov2017smoothgrad} attempts to solve the gradient noise problem in {\sc vanillagrad} by averaging over gradients sampled from the vicinity of the point under explanation.
Although the original paper only applies {\sc smoothgrad} to classification, the method can easily be extended to regression. 

Moving away from directly analysing the gradient, {\sc lime} \cite{ribeiro2016} and {\sc shap} \cite{lundberg2017unified} are perhaps the most widely known examples of practical local explanation generation methods.
In this paper, we focus on the {\sc kernel-shap} variant, which combines linear {\sc lime} with {\sc shap}.
Both {\sc lime} and {\sc kernel-shap} produce explanations in the form of additive linear models ($\hat{y} = \phi^T \bm{x}$).
Given an item $\bm{x}_i$, the methods sample novel items $\bm{x}'_i \in \mathcal{X}'$ in the neighbourhood of the first item, use the closed-box model to predict the labels for the novel items and find the linear model that best fits them \cite{lundberg2017unified},
\begin{equation}
    \hat{\phi} = \arg \min\nolimits_{\phi} \sum\nolimits_{\bm{x}'_i \in \mathcal{X}'} [f(\bm{x}'_i) - \phi^T \bm{x}'_i]^2 \pi_x(\bm{x}'_i) + \Omega(\phi),
\end{equation}
where $\pi_x$ represents a distance measure and $\Omega(\phi)$ is a regularisation term.
The difference between the methods lies in the choice of distance measure, which defines the notion of neighbourhood for $\bm{x}_i$; {\sc lime} most often uses either $L^2$ or cosine distance, whereas {\sc kernel-shap} utilizes results in game theory \cite{slack2021reliable}.

In addition to a procedure for generating local explanations, the authors of {\sc lime} also propose a method which constructs a global explanation by selecting a set of items whose explanations best capture the global behaviour.
They term this procedure \emph{submodular pick algorithm}.
The procedure first generates a local explanation for each item within the dataset. 
Then, a global feature importance score is calculated as the sum of the square roots of the feature attributions aggregated across all local explanations.
If we only pick items according to the presence of the most important features, there is a danger of ending up with many similar explanations.
Hence, \emph{submodular pick algorithm} encourages diversity by framing the problem as a weighted covering problem, balancing feature importance and representativeness such that the user sees a diverse set of explanations.

{\sc smoothgrad}, {\sc lime} and {\sc shap} are based on sampling novel items, which, while simple to implement, introduces a unique set of challenges.
First, formulating a reliable data generation process or sampling scheme for all possible datasets is difficult, if not impossible \cite{guidotti2018survey, laugel2018defining}.
For example, images generated by adding random noise to the pixel values rarely resemble natural images.
Second, randomly generating new items might produce items that cannot occur naturally due to, for example, violating the laws of physics.
{\sc slisemap} \cite{bjorklund2023slisemap} and its variant, {\sc slipmap} \cite{bjorklund2024SLIPMAP}, produce both a low-dimensional embedding for visualization and a local model for all training items without sampling any new points.
{\sc slisemap} finds both the embedding and local models by optimising a loss function consisting of two parts: an embedding term, where items with similar local explanations attract each other while repelling dissimilar ones and a local loss term for the explanation of each item:
\begin{equation}
    \min_{g_i} \mathcal{L}_i = \sum\nolimits_{i=1}^n \frac{\exp(-D(\bm{z}_i, \bm{z}_j))}{\sum_{k=1}^n \exp(-D(\bm{z}_k, \bm{z}_j))} \ell(g_i(\bm{x}_j), \bm{y}_j) + \Omega(g_i)
\end{equation}
where $D(\cdot, \cdot)$ is the euclidean distance in the embedding, $g_i$ represents the local model, $\ell$ is the local loss function, and $\Omega$ again denotes regularisation term(s).
In {\sc slisemap}, each item is fitted to its local models; in the {\sc slipmap} variant, the number of local models is fixed to some $p$, usually much less than the number of data items $n$, and the training items are mapped to one of the $p$ local models.

Another large class of explanations which deserves mention is the so-called case-based or example-based XAI methods, which use representative samples in the training set to explain novel ones \cite{agnar1994casebased, kim2016examples}.
One such method is using prototypes, which present the user with the most similar ``prototype items'' as an explanation, such as showing images of birds with similar plumage as a basis for classification. 

A shared property of post-hoc local surrogate models is the lack of uniqueness; for a given item, many local surrogates may exist with similar performance.
The phenomenon is documented for {\sc slisemap} in \cite{bjorklund2023slisemap} and implied for other methods based on the results in \cite{dombrowski2019explanations}, as well as for many publications aimed at fixing the inherent instability of {\sc lime} \cite{shankar2019alime, zafar2019dlime, zhao2021baylime}.
We argue that the existence of such alternative explanations is an inherent feature of using local surrogate models.
Intuitively, we can imagine the $n$-surface of a complex closed-box function and consider local surrogates as planes with dimensionality $n-1$.
There are many ways to orient a local surrogate on the curved surface of the closed-box function while retaining reasonable local fidelity.
Interestingly, the existence of alternative explanations implies that there may be surrogates which perform well for many items in the data distribution.
Therefore, we might be able to reduce a large set of local models to a small set of widely applicable surrogates, providing a global explanation of the model's behaviour.

The method proposed in the previous paragraph falls under \emph{model aggregation}.
The submodular pick algorithm mentioned when discussing {\sc lime} is an example of a model aggregation method.
Other methods include Global Aggregations of Local Explanations ({\sc gale}) \cite{vanderlinden2019global}, {\sc GLocalX} \cite{setzu2021glocalx}, and an integer programming-based approach introduced by Li et al. \cite{li2022optimal}.
{\sc gale} offers alternative ways to calculate feature importance for the submodular pick algorithm, as the authors argue that the way these importance values are calculated in the original \cite{ribeiro2016} is only applicable to a limited number of scenarios.
Furthermore, they show how the choice of the best-performing importance value definition is task-dependent.
In {\sc GLocalX} \cite{setzu2021glocalx}, the authors propose a method to merge rule-based explanations to find a global explanation.
In the programming-based approach (\cite{li2022optimal}), the authors take a similar approach to the one proposed in this paper.
They also attempt to find a representative subset of local models, and formulate model aggregation as an optimisation problem with fidelity and coverage constraints.
However, their work has some limitations.
First, their method relies on the definition of applicability radii for the local models, i.e., radii within which the explanation holds.
Second, the framework only functions in classification tasks.
Third, to satisfy the optimisation constraints, the framework requires the inclusion of tens of local models into the aggregation, limiting the interpretability of the aggregation as a global model.
Finally, they only tested their framework with random forest models and two datasets.
Because the model aggregation methods described above cannot be directly applied to an arbitrary set of local explanations, in this paper we opt to measure the performance of {\sc ExplainReduce} against the full set of local explanations instead.