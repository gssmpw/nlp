\section{Related work}
Modern machine learning tools have become indispensable in almost all industry and research fields.
As these tools find more and more applications, awareness of their limitations has simultaneously spread.
Chief among these is the lack of interpretability inherent to many of the most potent ML methods.
Understanding why ML models produce a specific prediction instead of something else poses obstacles in their adoption in, e.g., high-trust applications **Bach et al., "On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layered Sensitivity Visualization"**.
Such opaque methods are colloquially called black-box or closed-box methods, in contrast with white- or open-box methods, characterised by their interpretability to humans.
Examples of closed-box methods include random forests and deep neural networks, whereas, e.g., statistical models like linear regression models are often considered open-box methods.
As a result, the field of XAI has grown substantially in the last decade.
This section describes several post-hoc local explanation and model aggregation methods to give the reader context for the {\sc ExplainReduce} procedure.

XAI methods generally take a dataset and a closed-box function as inputs and produce an explanation that describes the relationship between the inputs and outputs of the closed-box model.
One commonly used approach is to produce post-hoc explanations using local (open-box) surrogate models.
In this approach, given a closed-box method $f: \mathcal{X} \rightarrow \mathcal{Y}, f(\bm{X}) = \hat{\bm{y}}$, a local surrogate model $g$ is another function that replicates the behaviour of the closed-box method in some region of the input space.
Mathematically, given a loss measure $\ell$ and a point in the input space $\bm{x}_i$, we might consider models $g$ such that $\ell(f(\bm{x}'_i), g(\bm{x}'_i)) \leq \varepsilon \; \forall \bm{x}'_i \in \{\bm{x} \in \mathcal{X} | D(\bm{x}'_i, \bm{x}_i) \leq \delta \}$ as local surrogate models.

Many methods have been proposed to produce local surrogate models as explanations.
Theoretically, the simplest way to produce a local surrogate model would be to calculate the gradient of the closed-box function.
This XAI method is often called {\sc vanillagrad} in the literature.
However, in practice, the gradients of machine learning models can be very noisy, as demonstrated by the effectiveness of adversarial attacks that exploit small perturbations in neural networks **Goodfellow et al., "Explaining and Harnessing Adversarial Examples"**.
On the other hand, many commonly used machine learning models, such as random forests, do not have well-defined gradients.
Hence, more involved approaches are warranted. 

{\sc smoothgrad} **Smilkov et al., "SmoothGrad: Smoothing Out Noise in Gradient-Based Interpretability Methods"** attempts to solve the gradient noise problem in {\sc vanillagrad} by averaging over gradients sampled from the vicinity of the point under explanation.
Although the original paper only applies {\sc smoothgrad} to classification, the method can easily be extended to regression. 

Moving away from directly analysing the gradient, {\sc lime} **Ribeiro et al., "Model-Agnostic Interpretability of Machine Learning"** and {\sc shap} **Lundberg and Lee, "A Unified Approach to Interpreting Model Predictions"** are perhaps the most widely known examples of practical local explanation generation methods.
In this paper, we focus on the {\sc kernel-shap} variant, which combines linear {\sc lime} with {\sc shap}.
Both {\sc lime} and {\sc kernel-shap} produce explanations in the form of additive linear models ($\hat{y} = \phi^T \bm{x}$).
Given an item $\bm{x}_i$, the methods sample novel items $\bm{x}'_i \in \mathcal{X}'$ in the neighbourhood of the first item, use the closed-box model to predict the labels for the novel items and find the linear model that best fits them **Friedman, "Greedy function approximation: A gradient boosting machine"**,
\begin{equation}
    \hat{\phi} = \arg \min\nolimits_{\phi} \sum\nolimits_{\bm{x}'_i \in \mathcal{X}'} [f(\bm{x}'_i) - \phi^T \bm{x}'_i]^2 \pi_x(\bm{x}'_i) + \Omega(\phi),
\end{equation}
where $\pi_x$ represents a distance measure and $\Omega(\phi)$ is a regularisation term.
The difference between the methods lies in the choice of distance measure, which defines the notion of neighbourhood for $\bm{x}_i$; {\sc lime} most often uses either $L^2$ or cosine distance, whereas {\sc kernel-shap} utilizes results in game theory **Shapley, "A Value for n-Person Games"**.

In addition to a procedure for generating local explanations, the authors of {\sc lime} also propose a method which constructs a global explanation by selecting a set of items whose explanations best capture the global behaviour.
They term this procedure \emph{submodular pick algorithm}.
The procedure first generates a local explanation for each item within the dataset. 
Then, a global feature importance score is calculated as the sum of the square roots of the feature attributions aggregated across all local explanations.
If we only pick items according to the presence of the most important features, there is a danger of ending up with many similar explanations.
Hence, \emph{submodular pick algorithm} encourages diversity by framing the problem as a weighted covering problem, balancing feature importance and representativeness such that the user sees a diverse set of explanations.

{\sc smoothgrad}, {\sc lime} and {\sc shap} are based on sampling novel items, which, while simple to implement, introduces a unique set of challenges.
First, formulating a reliable data generation process or sampling scheme for all possible datasets is difficult, if not impossible **Levine et al., "Learning Cause-Effect Inference from Heterogeneous Data"**.
For example, images generated by adding random noise to the pixel values rarely resemble natural images.
Second, randomly generating new items might produce items that cannot occur naturally due to, for example, violating the laws of physics.
{\sc slisemap} **Serra et al., "Model-Agnostic Local Explanations via Submodular Interpolation"** and its variant, {\sc slipmap} **Schwab et al., "Fast Approximate Global Models from Local Surrogate Networks"**, produce both a low-dimensional embedding for visualization and a local model for all training items without sampling any new points.
{\sc slisemap} finds both the embedding and local models by optimising a loss function consisting of two parts: an embedding term, where items with similar local explanations attract each other while repelling dissimilar ones and a local loss term for the explanation of each item:
\begin{equation}
    \min_{g_i} \mathcal{L}_i = \sum\nolimits_{i=1}^n \frac{\exp(-D(\bm{z}_i, \bm{z}_j))}{\sum_{k=1}^n \exp(-D(\bm{z}_k, \bm{z}_j))} \ell(g_i(\bm{x}_j), \bm{y}_j) + \Omega(g_i)
\end{equation}
where $D(\cdot, \cdot)$ is the euclidean distance in the embedding, $g_i$ represents the local model, $\ell$ is the local loss function, and $\Omega(g_i)$ is a regularisation term.
{\sc slisemap} **Serra et al., "Model-Agnostic Local Explanations via Submodular Interpolation"** offers alternative ways to calculate feature importance for the submodular pick algorithm, as the authors argue that the way these importance values are calculated in the original {\sc lime} paper is only applicable to a limited number of scenarios.
Furthermore, they show how the choice of the best-performing importance value definition is task-dependent.
{\sc GLocalX} **Chen et al., "Greedy Rule-List: An Efficient Model-Agnostic Local Explanations"** produces a global explanation by merging rule-based explanations from local models.

In the programming-based approach (****), the authors take a similar approach to the one proposed in this paper.
They also attempt to find a representative subset of local models, and formulate model aggregation as an optimisation problem with fidelity and coverage constraints.
However, their work has some limitations.
First, their method relies on the definition of applicability radii for the local models, i.e., radii within which the explanation holds.
Second, the framework only functions in classification tasks.
Third, to satisfy the optimisation constraints, the framework requires the inclusion of tens of local models into the aggregation, limiting the interpretability of the aggregation as a global model.
Finally, they only tested their framework with random forest models and two datasets.
Because the model aggregation methods described above cannot be directly applied to an arbitrary set of local explanations, in this paper we opt to measure the performance of {\sc ExplainReduce} against the full set of local explanations instead.