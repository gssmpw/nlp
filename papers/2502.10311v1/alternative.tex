






 
\documentclass[pdflatex,sn-mathphys-num]{sn-jnl}%


\usepackage{graphicx}%
\usepackage{multirow}%
\usepackage{amsmath,amssymb,amsfonts}%
\usepackage{amsthm}%
\usepackage{mathrsfs}%
\usepackage[title]{appendix}%
\usepackage{xcolor}%
\usepackage{textcomp}%
\usepackage{manyfoot}%
\usepackage{booktabs}%
\usepackage{algorithm}%
\usepackage{algpseudocode}%
\usepackage{listings}%
\usepackage{lmodern}
\usepackage{bm}
\usepackage{placeins}



\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\theoremstyle{definition}
\newtheorem{prob}{Problem}

\begin{document}

\title[Summarising local explanations via proxies]{{\sc ExplainReduce}: Summarising local explanations via proxies}



\author*[1]{\fnm{Lauri} \sur{Sepp\"al\"ainen}}\email{lauri.seppalainen@helsinki.fi}

\author[1]{\fnm{Mudong} \sur{Guo}}\email{mudong.guo@helsinki.fi}

\author[1]{\fnm{Kai} \sur{Puolam\"aki}}\email{kai.puolamaki@helsinki.fi}

\affil[1]{%
\orgname{University of Helsinki}, \orgaddress{\street{P.O. Box 64}, \city{Helsinki}, \postcode{00014}, \country{Finland}}}



\abstract{Most commonly used non-linear machine learning methods are closed-box models, uninterpretable to humans. The field of explainable artificial intelligence (XAI) aims to develop tools to examine the inner workings of these closed boxes. An often-used model-agnostic approach to XAI involves using simple models as local approximations to produce so-called local explanations; examples of this approach include {\sc lime},  {\sc shap}, and {\sc slisemap}. This paper shows how a large set of local explanations can be reduced to a small ``proxy set'' of simple models, which can act as a generative global explanation. This reduction procedure, {\sc ExplainReduce}, can be formulated as an optimisation problem and approximated efficiently using greedy heuristics.}



\keywords{Explainable artificial intelligence, XAI, local explanations, interpretability, machine learning}



\maketitle

\section{Introduction}\label{sec:intro}
Explainable artificial intelligence (XAI) aims to elucidate the inner workings of ``closed-box'' machine learning (ML) models: models that are not readily interpretable to humans.
As machine learning has found applications in almost all fields, the need for interpretability has likewise led to the use of XAI in medicine \cite{band2023medical}, manufacturing \cite{peng2022industrial} and atmospheric chemistry \cite{seppalainen2023using}, among many other domains.
In the past two decades, many different XAI methods have been developed to meet the diverse requirements \cite{guidotti2018survey}.
These methods produce explanations, i.e., distillations of a closed-box model's decision patterns.
An ideal XAI method should be model-agnostic -- applicable to a wide range of model types -- and its explanations would be succinct, easily interpretable by the intended user, and stable.
Additionally, such explanations would be global, allowing the user to comprehend the entire mechanism of the model.
However, producing global explanations is often a challenge.
For example, if a model approximates a complex function, describing its behaviour may require describing the complex function itself, thereby defeating the purpose of interpretability.

A common approach to producing model-agnostic explanations is to relax the requirement for explaining the model globally and instead focus on local behaviour \cite{guidotti2018survey}.
Assuming a degree of smoothness, approximating the closed-box function in a small neighbourhood is often feasible using simple, interpretable functions, such as sparse linear models, decision trees, or decision rules.

We argue that these local explanations are inherently unstable.
We initially observed this phenomenon with {\sc slisemap} \cite{bjorklund2023slisemap}, where we noted that most items could be accurately approximated with several different local models.
In the same vein, take two commonly used local explanation methods, {\sc lime} \cite{ribeiro2016} and {\sc shap} \cite{lundberg2017unified}.
Both methods estimate feature importance locally by averaging predictions from the closed-box model for items sampled near the one being explained.
Such explanations can be interpreted as linear approximations for the gradient of the closed-box model.
It is well known that the loss landscapes of, e.g., deep neural network models, are not smooth.
Therefore, it can be conjectured that the predictions from closed-box can be likewise unstable.
Indeed, research has shown that local explanations of neural networks can be manipulated due to the geometry of the closed-box function \cite{dombrowski2019explanations}.
Furthermore, numerous variants of {\sc lime} which aim to increase stability (such as \cite{shankar2019alime, zafar2019dlime}) indicate that instability of {\sc lime} is a key issue to address.
We hypothesise that this instability is a property of all local explanation methods that use simple models to approximate a complex function.
Even in the theoretically ideal case, where a complex model is defined by an exact mathematical formula and the local explanations take the form of gradients, there can be points (e.g., sharp peaks) where the gradient is ill-defined, as in Fig. \ref{fig:pyramid_example}.
In practice, especially when working with noisy data and averaging over random samples, the ambiguity is exacerbated; one item may have several nearly equally viable explanations.
At the same time, some local explanations may accurately approximate many items.
This suggests that if we generate a large set of local explanations, we may be able to find a smaller subset thereof, which could effectively replace the full set without sacrificing accuracy.
As an added bonus, while the local explanations may be unstable, this subset of explanations may be more stable, as observed in \cite{seppalainen2023using}.

This paper introduces a procedure, coined {\sc ExplainReduce}, that can reduce large sets of local explanations to a small subset of so-called proxy models.
This small proxy set can be used as a global explanation for the closed-box model, among other possible applications.
Fig. \ref{fig:pyramid_example} provides insight into the process.
In the left panel, we show noisy samples (blue dots) from a closed-box function (marked with a solid blue line).
The middle panel shows a set of local explanations (as blue and orange dashed lines) produced by the XAI method {\sc slisemap} \cite{bjorklund2023slisemap}, with one explanation per data item.
The right panel shows a reduction from this large set of local models to two (solid lines), maximising the coverage of items within an error tolerance (shaded area).
The procedure offers a trade-off between interpretability, coverage, and fidelity of the local explanation model by finding a minimal subset of local models that can still approximate the closed-box model for most points with reasonable accuracy.

We start with an overview of potential applications of local explanation subsets and introduce related research.
We then define the problem of reducing a large set of local models to a proxy set as an optimisation problem.
We also outline the {\sc ExplainReduce} algorithm and our performance metrics.
In the results section, we first show how a small proxy set provides a global explanation for the closed-box model with simulated data and a practical example.
Second, we find that a proxy set of modest size attains an adherence to the closed-box model on unseen data comparable to, or even better than, the complete set of explanations.
We continue to show that we can find a good proxy set even when starting from a limited set of initial explanations.
We then demonstrate how greedy approximation algorithms can efficiently solve the problem of finding the proxy set.
The code for the procedure and to recreate each of the experiments presented in this paper can be found at \url{https://github.com/edahelsinki/explainreduce}.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/pyramid_example.pdf}
    \caption{A simple example of the idea behind {\sc ExplainReduce}. A closed-box model (left) can have many local explanations (middle). We can reduce the size of the local explanation set to get a global explanation consisting of two simple models (right).}
    \label{fig:pyramid_example}
\end{figure}

\section{Applications for reduced local explanation sets}

\emph{Global explanations}:
Producing succinct global explanations for complex closed-box functions remains a challenge.
Given a local explanation method, a naive approach would be simply producing a local explanation for each item in a given training set.
However, this leaves the user with $n$ local explanations without guaranteeing global interpretability.
In practical settings, many of these local explanations are also likely to be similar and thus redundant.
Summarising the large set of local explanations with a reduced set of just a few local models would be much more palatable to the user as a global explanation.

\noindent\emph{Interpretable replacement for the closed-box model}:
Assuming that the method used to produce local explanations is faithful, i.e., able to accurately predict the behaviour of the closed-box function locally, the set of all local explanations can replace the closed-box model with little loss to accuracy.
However, a large set of (e.g., $500$) local models can hardly be called interpretable.
If there is sufficient redundancy in the explanations, selecting a small number of representative models yields a global explanation and provides an interpretable surrogate for the closed-box model.

\noindent\emph{Exploratory data analysis and XAI method comparison}:
Studying how the set of local explanations can be reduced can offer interesting insights about the data and XAI method used.
If, for example, different subsets of the data are consistently associated with very similar proxy sets, it implies that the closed-box model can be replaced with a reasonable loss of accuracy by a small set of simple functions.
Similarly, comparing the reduced proxy sets with individual explanations from different XAI methods could provide new insights into the explanation generation mechanisms in various settings.

\noindent\emph{Outlier detection}:
Given a novel observation, we can study how well the local models in the reduced set can predict it compared to similar items in the training dataset.
If the novel item is better explained by a different model than the one used for other similar items, or if no explanation in the reduced set accurately captures the relationship, the novel item could be considered an outlier.

\section{Related work}


Modern machine learning tools have become indispensable in almost all industry and research fields.
As these tools find more and more applications, awareness of their limitations has simultaneously spread.
Chief among these is the lack of interpretability inherent to many of the most potent ML methods.
Understanding why ML models produce a specific prediction instead of something else poses obstacles in their adoption in, e.g., high-trust applications \cite{dosilovic2018survey}.
Such opaque methods are colloquially called black-box or closed-box methods, in contrast with white- or open-box methods, characterised by their interpretability to humans.
Examples of closed-box methods include random forests and deep neural networks, whereas, e.g., statistical models like linear regression models are often considered open-box methods.
As a result, the field of XAI has grown substantially in the last decade.
This section describes several post-hoc local explanation and model aggregation methods to give the reader context for the {\sc ExplainReduce} procedure.

XAI methods generally take a dataset and a closed-box function as inputs and produce an explanation that describes the relationship between the inputs and outputs of the closed-box model.
One commonly used approach is to produce post-hoc explanations using local (open-box) surrogate models.
In this approach, given a closed-box method $f: \mathcal{X} \rightarrow \mathcal{Y}, f(\bm{X}) = \hat{\bm{y}}$, a local surrogate model $g$ is another function that replicates the behaviour of the closed-box method in some region of the input space.
Mathematically, given a loss measure $\ell$ and a point in the input space $\bm{x}_i$, we might consider models $g$ such that $\ell(f(\bm{x}'_i), g(\bm{x}'_i)) \leq \varepsilon \; \forall \bm{x}'_i \in \{\bm{x} \in \mathcal{X} | D(\bm{x}'_i, \bm{x}_i) \leq \delta \}$ as local surrogate models.

Many methods have been proposed to produce local surrogate models as explanations.
Theoretically, the simplest way to produce a local surrogate model would be to calculate the gradient of the closed-box function.
This XAI method is often called {\sc vanillagrad} in the literature.
However, in practice, the gradients of machine learning models can be very noisy, as demonstrated by the effectiveness of adversarial attacks that exploit small perturbations in neural networks \cite{szegedy2014intriguing}.
On the other hand, many commonly used machine learning models, such as random forests, do not have well-defined gradients.
Hence, more involved approaches are warranted. 

{\sc smoothgrad} \cite{smilkov2017smoothgrad} attempts to solve the gradient noise problem in {\sc vanillagrad} by averaging over gradients sampled from the vicinity of the point under explanation.
Although the original paper only applies {\sc smoothgrad} to classification, the method can easily be extended to regression. 

Moving away from directly analysing the gradient, {\sc lime} \cite{ribeiro2016} and {\sc shap} \cite{lundberg2017unified} are perhaps the most widely known examples of practical local explanation generation methods.
In this paper, we focus on the {\sc kernel-shap} variant, which combines linear {\sc lime} with {\sc shap}.
Both {\sc lime} and {\sc kernel-shap} produce explanations in the form of additive linear models ($\hat{y} = \phi^T \bm{x}$).
Given an item $\bm{x}_i$, the methods sample novel items $\bm{x}'_i \in \mathcal{X}'$ in the neighbourhood of the first item, use the closed-box model to predict the labels for the novel items and find the linear model that best fits them \cite{lundberg2017unified},
\begin{equation}
    \hat{\phi} = \arg \min\nolimits_{\phi} \sum\nolimits_{\bm{x}'_i \in \mathcal{X}'} [f(\bm{x}'_i) - \phi^T \bm{x}'_i]^2 \pi_x(\bm{x}'_i) + \Omega(\phi),
\end{equation}
where $\pi_x$ represents a distance measure and $\Omega(\phi)$ is a regularisation term.
The difference between the methods lies in the choice of distance measure, which defines the notion of neighbourhood for $\bm{x}_i$; {\sc lime} most often uses either $L^2$ or cosine distance, whereas {\sc kernel-shap} utilizes results in game theory \cite{slack2021reliable}.

In addition to a procedure for generating local explanations, the authors of {\sc lime} also propose a method which constructs a global explanation by selecting a set of items whose explanations best capture the global behaviour.
They term this procedure \emph{submodular pick algorithm}.
The procedure first generates a local explanation for each item within the dataset. 
Then, a global feature importance score is calculated as the sum of the square roots of the feature attributions aggregated across all local explanations.
If we only pick items according to the presence of the most important features, there is a danger of ending up with many similar explanations.
Hence, \emph{submodular pick algorithm} encourages diversity by framing the problem as a weighted covering problem, balancing feature importance and representativeness such that the user sees a diverse set of explanations.

{\sc smoothgrad}, {\sc lime} and {\sc shap} are based on sampling novel items, which, while simple to implement, introduces a unique set of challenges.
First, formulating a reliable data generation process or sampling scheme for all possible datasets is difficult, if not impossible \cite{guidotti2018survey, laugel2018defining}.
For example, images generated by adding random noise to the pixel values rarely resemble natural images.
Second, randomly generating new items might produce items that cannot occur naturally due to, for example, violating the laws of physics.
{\sc slisemap} \cite{bjorklund2023slisemap} and its variant, {\sc slipmap} \cite{bjorklund2024SLIPMAP}, produce both a low-dimensional embedding for visualization and a local model for all training items without sampling any new points.
{\sc slisemap} finds both the embedding and local models by optimising a loss function consisting of two parts: an embedding term, where items with similar local explanations attract each other while repelling dissimilar ones and a local loss term for the explanation of each item:
\begin{equation}
    \min_{g_i} \mathcal{L}_i = \sum\nolimits_{i=1}^n \frac{\exp(-D(\bm{z}_i, \bm{z}_j))}{\sum_{k=1}^n \exp(-D(\bm{z}_k, \bm{z}_j))} \ell(g_i(\bm{x}_j), \bm{y}_j) + \Omega(g_i)
\end{equation}
where $D(\cdot, \cdot)$ is the euclidean distance in the embedding, $g_i$ represents the local model, $\ell$ is the local loss function, and $\Omega$ again denotes regularisation term(s).
In {\sc slisemap}, each item is fitted to its local models; in the {\sc slipmap} variant, the number of local models is fixed to some $p$, usually much less than the number of data items $n$, and the training items are mapped to one of the $p$ local models.

Another large class of explanations which deserves mention is the so-called case-based or example-based XAI methods, which use representative samples in the training set to explain novel ones \cite{agnar1994casebased, kim2016examples}.
One such method is using prototypes, which present the user with the most similar ``prototype items'' as an explanation, such as showing images of birds with similar plumage as a basis for classification. 

A shared property of post-hoc local surrogate models is the lack of uniqueness; for a given item, many local surrogates may exist with similar performance.
The phenomenon is documented for {\sc slisemap} in \cite{bjorklund2023slisemap} and implied for other methods based on the results in \cite{dombrowski2019explanations}, as well as for many publications aimed at fixing the inherent instability of {\sc lime} \cite{shankar2019alime, zafar2019dlime, zhao2021baylime}.
We argue that the existence of such alternative explanations is an inherent feature of using local surrogate models.
Intuitively, we can imagine the $n$-surface of a complex closed-box function and consider local surrogates as planes with dimensionality $n-1$.
There are many ways to orient a local surrogate on the curved surface of the closed-box function while retaining reasonable local fidelity.
Interestingly, the existence of alternative explanations implies that there may be surrogates which perform well for many items in the data distribution.
Therefore, we might be able to reduce a large set of local models to a small set of widely applicable surrogates, providing a global explanation of the model's behaviour.

The method proposed in the previous paragraph falls under \emph{model aggregation}.
The submodular pick algorithm mentioned when discussing {\sc lime} is an example of a model aggregation method.
Other methods include Global Aggregations of Local Explanations ({\sc gale}) \cite{vanderlinden2019global}, {\sc GLocalX} \cite{setzu2021glocalx}, and an integer programming-based approach introduced by Li et al. \cite{li2022optimal}.
{\sc gale} offers alternative ways to calculate feature importance for the submodular pick algorithm, as the authors argue that the way these importance values are calculated in the original \cite{ribeiro2016} is only applicable to a limited number of scenarios.
Furthermore, they show how the choice of the best-performing importance value definition is task-dependent.
In {\sc GLocalX} \cite{setzu2021glocalx}, the authors propose a method to merge rule-based explanations to find a global explanation.
In the programming-based approach (\cite{li2022optimal}), the authors take a similar approach to the one proposed in this paper.
They also attempt to find a representative subset of local models, and formulate model aggregation as an optimisation problem with fidelity and coverage constraints.
However, their work has some limitations.
First, their method relies on the definition of applicability radii for the local models, i.e., radii within which the explanation holds.
Second, the framework only functions in classification tasks.
Third, to satisfy the optimisation constraints, the framework requires the inclusion of tens of local models into the aggregation, limiting the interpretability of the aggregation as a global model.
Finally, they only tested their framework with random forest models and two datasets.
Because the model aggregation methods described above cannot be directly applied to an arbitrary set of local explanations, in this paper we opt to measure the performance of {\sc ExplainReduce} against the full set of local explanations instead.



\section{Methods}

In this section, we describe the idea and implementation of {\sc ExplainReduce}.
Assuming that many items in a dataset can have many alternative explanations of similar performance, a proper subset of explanations can accurately model most of the items in the dataset. 
We refer to this smaller set as a set of ``proxy models''.
Thus, the method combines aspects of local surrogate explanations with prototype items; instead of representative data items, we use representative local surrogate models to summarise global model performance.

The {\sc ExplainReduce} procedure works as follows: after training a closed-box model, we generate a large set of local explanations for the closed-box and then find a covering subset of the local models, which acts as a global explanation.
In this section, we first define the problem of finding the subset of local models and then move on to cover the reduction methods and algorithms that generate these proxy sets.
We also introduce the quality metrics used to evaluate the performance of reduced sets.

\subsection{Problem definition}
\label{sec:problem}
A dataset $\mathcal{D} = \{(\bm{x}_1, \bm{y}_1) , \ldots, (\bm{x}_n, \bm{y}_n)\}$ consists $n$ of data items (covariates) $\bm{x}_i \in \mathcal{X}$ and labels (responses) $\bm{y}_i \in \mathcal{Y}$. We use ${\bm X}$ denote a matrix of $n$ rows such that ${\bm X}_{i\cdot}=\bm{x}_i$.
If we have access to a trained supervised learning algorithm $f(\bm{x}_i) = \hat{\bm{y}}_i$, we can instead replace the true labels $\bm{y}_i$ with the predictions from the learning algorithm $\hat{\bm{y}}_i$.
A local explanation for a data item $(\bm{x}_i, \bm{y}_i)$ is a simple model $g(\bm{x}_i) = \Tilde{\bm{y}}_i$  which locally approximates either the connection between the data items and the labels or the behaviour of the closed-box functions. 
In the previous section, we gave multiple examples of generating such explanations.
Assume we have generated a large set of such local explanations $\bm{G} = \{g_j\mid j\in[m]=\{1,\ldots,m\}\}$ and a mapping from data items to models $\Phi:{\mathcal{X}}\mapsto[m]$ using one of these methods.
Assume also that these local models can be identified by a set of $p$ real parameters (such as coefficients of linear models), which we will denote with $\bm{B}\in{\mathbb{R}}^{m\times p}$.
We define a loss function $\ell:\mathcal{Y} \times \mathcal{Y} \rightarrow \mathbf{R}_{\geq 0}$ and a loss matrix $\bm{L}\in{\mathbb{R}}_{\ge 0}^{m\times n}$, where individual items are defined as $\bm{L}_{ij} = \ell(g_i(\bm{x}_j), \bm{y}_j)$.
A straightforward example of a mapping $\Phi$ would then be chosen from the local models, the one with the lowest loss for each item: $\Phi(\bm{x}) = \arg\min\nolimits_{i\in[m]}{\ell(g_i(\bm{x}_j), \bm{y}_j)}$. 

Finally, we assume that the large set $\bm{G}$ contains a reasonable approximation for each item in the dataset ${\cal D}$: for all $i \in [n]$ and for a given $\varepsilon\in{\mathbb{R}}_{>0}$ there exists $j\in[m]$ such that $\ell (g_j(\bm{x}_i), \bm{y}_i) \leq \varepsilon$.
This can be achieved by, e.g., learning a local explanation for each item in ${\cal D}$.

We are interested in how many items can be explained by a given set of local surrogates to a satisfactory degree.
To measure this, we use coverage $C$, defined as the proportion of data items which can be explained sufficiently by at least one local model in a subset ${\bm{S}} \subseteq[m]$.
Mathematically, given a loss threshold $\varepsilon\in{\mathbb{R}}_{>0}$, the coverage can be calculated as
\begin{equation}
    C({\bm{S}}, \varepsilon) = (1/n) \left| \{j\in [n]\mid\min\nolimits_{i\in{\bm{S}}}{} \ell(g_i(\bm{x}_j), \bm{y}_j) \leq \varepsilon \; \} \right|.
\end{equation}
A $c$-covering subset of local models ${\bm{S}} \subseteq [m]$ is a set for which $C({\bm{S}}, \varepsilon) \geq c$. 


Next, we define three computational problems to address the task described earlier. Later, we will introduce algorithms to solve each of the problems.

The first formulation attempts to minimise the number of items for which a satisfactory local explanation is not included in the subset ${\bm{S}}_c$.
\begin{prob} ({\sc maximum coverage})\label{prob:1}
Given $k$ and $\varepsilon\in{\mathbb{R}}_{>0}$, find a subset ${\bm{S}}_c$ of cardinality $k$ that maximises coverage, or
\begin{equation}
{\bm{S}}_c=\arg\max\nolimits_{{\bm{S}}\subseteq [m]_k} C({\bm{S}}, \varepsilon), 
\end{equation}
where we have used $[m]_k=\{{\bm S}\subseteq[m]\mid\left|{\bm S}\right|=k\}$ to denote the subsets of cardinality $k$.
\end{prob}



The second definition attempts to capture a subset of explanations that can be used as a proxy model with a small average loss.
\begin{prob} ({\sc minimum loss})\label{prob:2}
Given $k$, find a subset ${\bm{S}}_c$ with cardinality $k$ such the average loss when picking the lowest loss model from ${\bm{S}}_c$ is minimised, or
\begin{equation}
{\bm{S}}_c=\arg\min\nolimits_{{\bm{S}}\in[m]_k}{\left(
\frac 1n\sum\nolimits_{j=1}^n{
\min\nolimits_{i\in{\bm{S}}}{\ell(g_i({\bf x}_j), {\bf y}_j)}
}
\right)}.
\end{equation}
\end{prob}


The final formulation is a combination of problems \ref{prob:1} and \ref{prob:2}.
\begin{prob} ({\sc coverage-constrained minimum loss})\label{prob:3}
Given $k$, $\varepsilon\in{\mathbb{R}}_{>0}$, and minimum coverage $c\in (0, 1]$, find a $c$-covering  ${\bm{S}}_c$ with cardinality $k$ such the average loss when picking the lowest loss model from ${\bm{S}}_c$ is minimised, or
\begin{equation}
 {\bm{S}}_c=\arg\min\nolimits_{{\bm{S}}\in[m]_{k,c}}{\left(
\frac 1n\sum\nolimits_{j=1}^n{
\min\nolimits_{i\in{\bm{S}}}{\ell(g_i({\bf x}_j), {\bf y}_j)}
}
\right)},
\end{equation}
where $[m]_{k,c}=\{{\bm S}\in[m]_k\mid C({\bm{S}},\varepsilon)\ge c\}$ are the $c$-coverings of cardinality $k$.
\end{prob}

\subsection{General procedure}\label{sec:procedure}

The {\sc ExplainReduce} algorithm is outlined in Algorithm \ref{alg:main}. 
Given a dataset $\mathcal{D}$, an explanation method $\text{Exp}$ that generates a set of $m$ local explanations (a special case being one local explanation for each training data point, in which case $m=n$) and a reduction algorithm explained later in Sect. \ref{sec:reduction_algs}, we first use the explanation method to generate $m\le n$ explanations for $m$ items sampled without replacement from $\mathcal{D}$.
If a closed-box function is provided, we replace the original labels in $\mathcal{D}$ with predictions from $f$.
We then apply the reduction algorithm to the generated set of local explanations $[m]$ and receive the proxy set ${\bm{S}}_c \subseteq [m]$.
Finally, for each sampled data item, we pick the local explanation in the proxy set ${\bm{S}}_c$ with minimal loss, mapping the items and the proxies.


\begin{algorithm}
\caption{{\sc ExplainReduce} Procedure to find a subset of explanations.}
\label{alg:main}
\hspace*{\algorithmicindent} \textbf{Input:} $\mathcal{D} \gets \{(\bm{x}_i, \hat{\bm{y}}_i) | i \in [n]\}$: dataset; ${\bm G}$: the set of $m$ local explanations; $\textrm{reduce}$: method to find ${\bm S}_c$, parametrised optionally by $\varepsilon$, $c$, or $k$, see Sect. \ref{sec:reduction_algs}. \\
\hspace*{\algorithmicindent} \textbf{Output:} ${\bm S}_c$: reduced set of explanations; $\text{map}$: $\text{map}[i]: [n]\mapsto{\bm S}_c$, mapping from the local dataset $[n]$ to explanations in ${\bm S}_c$.
\begin{algorithmic}
    \Procedure{ExplainReduce}{}    
    \State $\bm{S}_c \gets \textrm{reduce}(\bm{G}, \varepsilon, c, k)$ \Comment{$\textrm{reduce}$ is defined in Sect. \ref{sec:reduction_algs}}
    \State $\text{map} \gets \{\}$ \Comment{a mapping between items in $\mathcal{D}$ and the local models}
    \For{$i \in [n]$}
       \State $\text{map}[i] \gets \arg\min_{j\in{\bm S}_c}{\ell(g_j(\bm{x}_i), \bm{y}_i)}$
    \EndFor \\
    \Return ${\bm S}_c,\; \text{map}$\\
    \EndProcedure
\end{algorithmic}
\end{algorithm}
 
\subsection{Reduction algorithms}
\label{sec:reduction_algs}

In this section, we briefly overview the practical implementations of the reduction algorithms (function ${\textrm{reduce}}$ in Alg. \ref{alg:main}) used to solve the problems outlined in the previous section.

{\sc max coverage}: 
Problem 1 is a variant of the NP-complete partial set covering problem, sometimes called {\sc max $k$-cover}.
The equivalence is obvious if we consider an item $(\bm{x}_j, \bm{y}_j) \in \mathcal{D}$ covered by model $g_i$ if $\ell(g_i(\bm{x}_j), \bm{y}_j) \leq \varepsilon$.
We solve Prob. \ref{prob:1} exactly using integer programming (implemented by the {\sc pulp} Python library \cite{mitchell2024pulp}) and approximately using a greedy algorithm.
In the greedy approach, given fixed $k$, we iteratively pick local models $g \in \bm{G}$ such that the marginal increase in coverage is maximised with each iteration until $k$ models have been chosen.
Our problem is submodular, as adding each new model to the subset cannot decrease the coverage.
It has been shown that in this case, the greedy algorithm has a guaranteed lower bound to achieve coverage at least $1 - \left((k - 1)/k\right)^k$ times the optimal solution \cite{nemhauser1978analysis}.

Notably, this approximation ratio only applies to the original set of data items and their associated local explanations.
If we apply the proxy sets to novel data, we should use standard machine learning tools --- such as a separate validation set --- to ensure that the model performs appropriately.

{\sc min loss}:
Problem \ref{prob:2} is an example of a supermodular minimisation problem.
Let $f(\bm{S}) = \left(\frac 1n\sum_{j=1}^n{\min_{i\in{\bm{S}}}{\ell(g_i({\bf x}_j),{\bf y}_j)}} \right)$ and $\bm{A}, \bm{B}: \; \bm{A} \subset \bm{B} \subset \bm{G}$ be subsets of local models.
Additionally, let $v$ be a local model not contained in $\bm{B}$.
Clearly, the decrease in loss by adding $v$ to the larger set $\bm{B}$ must be, at most, as great as adding the same model $v$ to $\bm{A}$.
In other words,
\begin{equation}
    f(\bm{A} \cup \{v\}) - f(\bm{A}) \leq f(\bm{B} \cup \{v\}) - f(\bm{B}) \quad \forall \bm{A} \subset \bm{B} \subset \bm{G}, \; v \notin \bm{B},
\end{equation}
which is the definition of supermodularity \cite{mccormick2005submodular}.

Supermodular minimisation problems are known to be NP-hard.
Hence, we only use a greedy ascent algorithm to solve Problem \ref{prob:2}, as finding an exact solution is computationally expensive due to the continuous nature of loss.
In the worst case, the search for the optimal subset would require $\binom{m}{k}$ comparisons.
Like above, we iteratively pick local models with the best possible decrease in marginal loss until we reach $k$ chosen local models.
In the general case, a multiplicative approximation ratio for a supermodular minimisation problem may not exist due to the optimal solution having a value $f(\bm{S}^*) = 0$, while the greedy algorithm may converge to a solution with non-zero loss.
However, in our case, finding a subset of models with exact zero loss for all items is unlikely.
In \cite{ilev2001approximation}, the author derives a curvature-based approximation ratio for a greedy descent (worst out) algorithm.
Based on this analysis, authors of \cite{bounia2023approximating} derive an approximation ratio for greedy ascent for probabilistic decision trees.
Neither of these approaches is directly applicable to our setting, and hence, we cannot give a closed-form approximation ratio.
However, as we show in Section \ref{sec:greedy}, the empirical approximation ratios for the greedy ascent algorithm are reasonable.
Moreover, we show how the greedy approximation performs nearly equally, if not better, on unseen data compared to the exact solution for our datasets.

{\sc const min loss}:
We solve Problem 3 again with a greedy ascent algorithm, i.e., iteratively picking the best model to include in the subset ${\bm S}_c$ until the maximum coverage has been met.
The only difference to {\sc min loss} algorithm is how the models are scored.
If the coverage constraint has been met, we simply follow the procedure in {\sc min loss} and select the model with the best possible marginal loss decrease.
Otherwise, we divide the marginal loss decrease $\Delta \ell_i$ we would get by including model $i$ in the proxy set with the marginal coverage increase of including the same model $\Delta c_i$.
This reduction method can be implemented with a hard constraint by having the algorithm throw an exception if the coverage is not met or with a soft constraint where the resulting proxy set is returned regardless of satisfying the constraint.
In this paper, we opt to use the softly constrained variant exclusively.
We can see that in the case where the coverage constraint is not met, Problem 3 with this scoring scheme is also a supermodular minimisation problem and equivalent to Problem 2 with a different optimisation objective, namely the coverage-normalised loss.
Similar to Problem 2, we cannot give an exact approximation ratio for this algorithm, but the empirical results in Section \ref{sec:greedy} suggest good performance.

{\sc clustering}:
Above, we have treated the problem as a set covering problem.
We can also approach the problem from the unsupervised clustering perspective: can we cluster the training data to find a proxy set?
Given the number of clusters $k$, we can cluster the training data items $\bm{X}$, the local model parameters $\bm{B}$, or the training loss matrix $\bm{L}$ using some clustering algorithm.
In this paper, we use {\sc k-means} with Euclidean distance for $\bm{X}$ and $\bm{L}$, and cosine distance for the local model parameters $\bm{B}$.
After performing the clustering, we pick the local model closest to the cluster centroid for each cluster to form the proxy set.

\subsection{Performance measures}

\bmhead{Fidelity} Fidelity \cite{guidotti2018survey} measures the adherence of a surrogate model to the complex closed-box function.
Given a closed-box function $f$, we fidelity is the loss between the closed-box model prediction $\hat{\bm{y}} = f(\bm{x})$ and the surrogate model prediction:
\begin{equation}
    \textrm{fidelity} = \frac{1}{n} \sum\nolimits_{i=1}^{n} \ell(g_i(\bm{x}_i), \hat{y}_i),
    \label{eq:fidelity}
\end{equation}
where $g_i$ is the relevant local model either from the full set of local explanations or the proxy model set.

\bmhead{Instability} Instability \cite{guidotti2018survey} (sometimes also referred to as \emph{stability}, despite lower values denoting better performance) measures how much a slight change in the input changes the explanation.
In practice, we model the slight change by measuring the loss of a given local model $g_i$ associated with item $\bm{x}_i$ with its $\kappa$ nearest neighbours:
\begin{equation}
    \textrm{instability} = \frac{1}{n} \sum\nolimits_{i=1}^{n} \frac{1}{\kappa} \sum\nolimits_{j \in \textrm{NN}_\kappa(i)} \ell(g_i(\bm{x}_j), y_j). 
    \label{eq:instability}
\end{equation}
This paper uses a fixed number of $\kappa=5$ nearest neighbours when we report instability values ($\left|\textrm{NN}_5(i)\right|=5$).

\section{Experiments}

In this section, we demonstrate the performance of the proxy sets produced via different XAI methods and reduction strategies.
We experiment on a variety of different datasets, consisting of both classification and regression tasks.
For each of the datasets used, we train a closed-box model and produce the initial set of local explanations using that model.
A brief description of the datasets can be found in Table \ref{tab:datasets}, and further details in Appendix \ref{a:datasets}.

\begin{table}[b]
    \centering
\begin{tabular}{l|cccc}
Dataset Name    & Size        & Task           & Closed-box model  & Citation                                    \\ \hline
Synthetic       & 5000 × 11   & Regression     & Random Forest     & \cite{bjorklund2023slisemap}                \\
Air Quality     & 7355 × 12   & Regression     & Random Forest     & \cite{oikarinenDetectingVirtualConcept2021} \\
Life Expectancy & 2938 × 22   & Regression     & Neural Network    & \cite{rajarshi2017life}                     \\
Vehicle         & 2059 × 12   & Regression     & SVR               & \cite{birla2022vehicles}                    \\
Gas Turbine     & 36733 × 9   & Regression     & Adaboost          & \cite{2019gas}                              \\
QM9             & 133766 × 27 & Regression     & Neural Network    & \cite{ramakrishnan2014Quantum}              \\
Jets            & 266421 × 7  & Classification & Random Forest     & \cite{CMS:opendata}                         \\
HIGGS           & 100000 × 28 & Classification & Gradient Boosting & \cite{whiteson2014higgs}                    \\ \hline
\end{tabular}
    \caption{Summary of datasets used in experiments.}
    \label{tab:datasets}
\end{table}

In the experiments, we measure fidelity mostly with test data, i.e., data not used in generating the full set of local explanations.
This metric better captures the adherence of the explainer to the closed-box model than calculating the same value on the data used to generate the local explanations.

The local explanation methods discussed in this paper are generally not generative, that is, there is no obvious mapping from a new covariate ${\bm{x}}$ to an explanation model in ${\bm{S}}_c$.
Therefore, to use either the full explanation sets or the proxies to produce predictions, we need to generate another mapping between novel items and local models in the explanation set.
The simplest of such mappings is to use the distance in the data space: $\Phi_{\textrm{new}}( \bm{x}) = \arg\min\nolimits_{i\in{\bm{S}}_c} ||\bm{x}_i - \bm{x}||^p, \; i \in (1, ..., n)$, where $||a||^p$ is the $p$-norm.
This new mapping is used for unlabelled items only and is in addition to the loss-minimising mapping used in the reduction process to find an initial mapping with the ``training'' data (data used to generate the local explanations) described in Section \ref{sec:procedure}.

\subsection{Case studies}
We begin our examination by demonstrating the {\sc ExplainReduce} procedure with two case studies to give the reader a better intuition of the procedure and its possible usage.

\subsubsection{Synthetic data}

To give a simple example of the procedure, we apply it to a synthetic dataset.
The dataset, which is described in detail in Appendix \ref{a:datasets}, is generated by producing $k=4$ clusters in the input space and generating a random, different linear model for each cluster.
We then generate labels by applying a local model to the data items based on their cluster ID and adding Gaussian noise.
The dataset is then randomly split into a training set and a test set, and we train a {\sc smoothgrad} explainer on the training data.
For reduction, we use a greedy {\sc max coverage} algorithm, where $\varepsilon$ is defined as the 10th percentile of the loss matrix $\bm{L}$.

In Fig. \ref{fig:case_PCA}, we show a PCA of the items in the test set on the left, coloured with the ground truth cluster labels, and how the test items are mapped to proxy models on the right.
Overall, we can see that most items get mapped to the correct proxy model for that particular cluster.
The small impurity in the clusters stems both from the Gaussian noise and the approximative nature of the greedy coverage-maximising algorithm.
Furthermore, as Fig. \ref{fig:case_radar} shows, the reduced proxy models (red) correspond well with the ground truth models (blue).
The proxy set thus serves well as a generative global explanation for the dataset.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/case_PCA.pdf}
    \caption{Ground PCA of a synthetic test dataset (left) and the same dataset where colours correspond to reduced model indices, based on {\sc smoothgrad} local models and reduced using a greedy coverage-maximising algorithm (right). We can see that the reduction is able to faithfully approximate the ground truth clustering.}
    \label{fig:case_PCA}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/case_radar.pdf}
    \caption{Radar plots showing ground truth local model parameters (blue) and corresponding proxy model parameters (red). The reduction method is able to find very similar surrogate models to the ground truth.}
    \label{fig:case_radar}
\end{figure}

\subsubsection{Particle jet classification}

In a previous work \cite{seppalainen2023using}, we analysed a dataset containing simulated LHC proton-proton collisions using {\sc slisemap}.
These collisions can create either quarks or gluons, which decay into cascades of stable particles called jets.
These jets can then be detected, and we can train the classifier to distinguish between jets created by quarks and gluons based on the jet's properties.

We first trained a random forest model on the data and then applied {\sc slisemap} to find local explanations.
We clustered the {\sc slisemap} local explanations to 5 clusters and analysed the average models for each cluster, and found them to adhere well to physical theory.
When we generate 500 {\sc slisemap} local explanations on the same dataset and apply the {\sc const min loss} reduction algorithm with $k=4$ proxies, we find the proxy set depicted in Fig. \ref{fig:jets_example}.
The left panel shows a swarm plot where each item is depicted on a horizontal line based on the random forest-predicted probability of the jet corresponding to a gluon jet and coloured based on which proxy model is associated with the item.
The right panel shows the coefficients of the proxy models, which are regularised logistic regression models.
We find that the proxies are similar to the cluster mean models shown in the previous publication and show similar adherence to the underlying quantum chromodynamic theory \cite{cms2013performance}.
For example, wider jets (high {\sc jetGirth} and {\sc QG\_axis2}) are generally more gluon-like, and therefore these parameters are essential in classifying the jets.
Incidentally, proxy 3 (red) is associated with the most quark- and gluon-like jets and has high positive coefficients for both parameters.
Similarly, proxy 0 (blue) has negative coefficients for momentum ({\sc jetPt}) and {\sc QG\_ptD}, which measures the degree to which the total momentum parallels the jet.
Both of these features indicate a more quark-like jet.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/jets_example.pdf}
    \caption{Example of the {\sc ExplainReduce} procedure on a particle jet classification task. The dataset consists of LHC proton-proton collision particle jets that are created by decaying gluons or quarks. We train a random forest classifier on the dataset and use {\sc slisemap} to generate 500 explanations, which are then reduced to $k=4$ proxies using the {\sc const min loss} reduction algorithm. The left panel shows a swarm plot of the 500 items sorted horizontally based on the predicted probability of corresponding to a gluon; the y-axis has no significance. The right panel shows the coefficients of the logistic regression proxy models, which match the underlying physical theory.}
    \label{fig:jets_example}
\end{figure}

\subsection{Fidelity of proxy sets}

The interpretability of a set of local models as a global explanation is directly related to the size of that set.
As explained in a previous section, local explanation models often need to balance interpretability and granularity: a large set of local models may more easily cover a larger portion of the closed-box function, but the increased number of models makes the result less understandable.
In Fig. \ref{fig:fidelity_k}, we show the test fidelity (as defined in Eq. \eqref{eq:fidelity}) as a function of the size of the reduced explanation set for a selection of datasets and local explanation algorithms.
The fidelity is calculated with respect to the closed-box predicted labels, with a fidelity of 0 representing perfect adherence to the closed-box model.
As the figure shows, a set as small as $k=5$ proxies can reach or even surpass the fidelity of a set of $500$ local explanations for all three selected datasets\footnote{For full coverage on all of the seven datasets, we refer to Appendix \ref{a:fidelity_k_full}.}.
We find that optimisation-based approaches perform particularly well, especially the two greedy loss-minimising algorithms.
The clustering-based reduction methods seem to work almost equally well, as does a random selection of local models, although optimisation-based approaches consistently outperform the latter.
This implies that for proxy set generation methods that require choosing $k$ \emph{a priori}, performance is not very sensitive to the choice of $k$.
Therefore, it is enough for a user of {\sc ExplainReduce} to specify a reasonable value of $k$ for a faithful and interpretable global surrogate.
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/fidelity_k.pdf}
    \caption{Test set fidelity of explanations as a function of proxy set size $k$. The rows show different datasets, and the columns show different XAI generation methods. The different line colours and styles denote the reduction strategy, and the black horizontal line shows the performance of the full explanation set. The loss-minimising reduction methods consistently reach a fidelity comparable or even better than the full explanation set.}
    \label{fig:fidelity_k}
\end{figure}

If a few proxies are enough to find a faithful representation, how many local explanations do we need to generate to find a good proxy set?
The procedure may be infeasible if a very large set of initial local explanations is required.
For example, {\sc slisemap} scales as $\mathcal{O}(n^2m)$, where $n = |\mathcal{D}|$ and $m$ is the number of features \cite{bjorklund2023slisemap}, which makes generating thousands of explanations expensive.

To study the generalisation performance of the proxy sets, we artificially limit the set of all local models $\bm{G}$ via random sampling before reduction.
That is to say, we generate local approximations for each item in a randomly sampled subset of points in the training set ${\cal D}$.
We then calculate the test fidelity of the proxy set trained on the limited universe as above and compare that to the performance of the full explanation set.
As Fig. \ref{fig:fidelity_n} shows, the proxy sets generalize well even starting from a limited set of local models\footnote{Full version shown in Appendix \ref{a:fidelity_n_full}.}.
The coloured lines represent the various reduction algorithms, while the thick dashed black line denotes the performance of the entire set of local explanations.
The loss-minimising greedy algorithms again achieve the best performance across the studied datasets and explanation methods studied, often matching and sometimes outperforming the full explanation set.
On the other hand, the clustering-based approaches fall behind here, as in the previous section.
These results imply that the procedure of {\sc ExplainReduce} does not require training local models for each item; instead, a randomly sampled subset of models suffices to find a good proxy set.
Based on these results, we set the default number of subsamples to $n=500$ in this paper unless otherwise stated.
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/fidelity_n.pdf}
    \caption{Test set fidelity of explanations as a function of local model subsample size $n=|\mathcal{D}|$. Different datasets are depicted as rows with the XAI methods used to produce the initial local explanations shown as rows. The coloured lines depict the test fidelity of the proxy sets produced with different reduction algorithms, with the thick black line denoting the performance of the entire local explanation set.}
    \label{fig:fidelity_n}
\end{figure}

We also experimented with the impact of the hyperparameters $\varepsilon$, the loss threshold, and $c$, the minimum coverage constraint, and found the reduction algorithms to have similar fidelity performance across a wide range of these hyperparameter values.
We refer to Appendix \ref{a:cov_eps_sensitivity}
for detailed results.

\subsection{Coverage and stability of the proxy sets}

It should be no surprise that methods directly optimising for loss also show the best fidelity results.
However, an ideal global explanation method should be able to accurately explain most, if not all, of the items of interest.
Additionally, explanations are expected to be locally consistent, meaning that similar data items should generally have similar explanations. 
In Fig. \ref{fig:coverage_stability}, we show the evolution of training coverage and instability of reduction methods as a function of the proxy set size $k$ with respect to the closed-box predicted labels\footnote{Full versions shown in Appendices \ref{a:coverage_full} and \ref{a:stability_full} for coverage and stability, respectively.}.
The results follow a similar pattern to the previous sections: a small number of proxies is enough to achieve high coverage and low instability, with optimisation-based approaches outperforming clustering-based alternatives.
Unsurprisingly, the coverage-optimising reduction methods achieve the best coverage with the smallest number of proxies.
However, the loss-minimising algorithms perform surprisingly well even compared to the coverage-optimising variants.
Notably, the balanced approach -- the {\sc const min loss} reduction algorithm -- almost matches the reduction algorithms directly optimising coverage.

Regarding instability, the loss-minimising reduction methods have an edge over the clustering- and coverage-based approaches, but the distinction between methods is not pronounced.
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/coverage_stability_k_small.pdf}
    \caption{Coverage and instability of various reduction methods as a function of $k$ on the Gas Turbine dataset. The optimisation-based methods outperform the clustering-based approaches within both metrics.}
    \label{fig:coverage_stability}
\end{figure}

Combining the results from the fidelity, coverage, and instability experiments, the greedy loss-minimising approach with a soft coverage constraint appears to offer the best balance in performance across all three metrics.
The proxy set produced by this reduction method consistently achieves similar, if not better, performance than the full explanation set, even with a few proxies.
Based on these experiments, the various clustering-based approaches do not offer performance gains based on any of the metrics used.
While these approaches may produce more interpretable proxy sets by inherently accommodating the structures found in the data, they struggle to generate sufficiently covering proxy sets and lag behind optimisation-based approaches in both fidelity and instability.

\begin{table}[t]
    \centering
    \include{greedy_comparison}
    \caption{Comparison of the training fidelity and coverage, alongside the observed approximation ratios (A. ratio), between the analytical and approximate optimisation algorithms when applied to $n=100$ explanations produced with {\sc lime} and with $k=5$ proxies. G. Max $c$ corresponds to the greedy {\sc max coverage} algorithm, while G. Min Loss and G. Min Loss (min $c$) correspond to the greedy {\sc min loss} and {\sc const min loss} algorithms, respectively.}
    \label{tab:greedy}
\end{table}
\subsection{Performance of greedy algorithms}
\label{sec:greedy}

In section \ref{sec:reduction_algs}, we discussed the performance of greedy approximation algorithms used to solve Problems \ref{prob:1}--\ref{prob:3}.
As we have seen in previous sections, the performance of the greedy and analytical coverage-optimising algorithms is almost equal as both a function of the proxy set size and the initial local explanation set size.
As mentioned in the section above, solving for the exact loss-minimising proxy set is a computationally challenging problem which requires $\binom{m}{k}$ comparisons in the worst case.
Hence, the analysis in this section was limited to $n=m=100, k=5$ for computational tractability.
Table \ref{tab:greedy} shows how the greedy approximation algorithms perform comparably to the analytically optimal variants.
For the {\sc max coverage} algorithm, experimental approximation ratios are substantially better than the worst-case bound described in Sect. \ref{sec:reduction_algs}.
Furthermore, as we saw in the previous sections, while on the train set the greedy heuristics do not reach the optimal solution fidelity-wise, on the test the differences between the exact solution and the greedy heuristics vanish.
The {\sc const min loss} algorithm especially shows poorer performance compared to the greedy {\sc min loss} algorithm but generalises better on the test set while achieving good coverage performance as well.
Overall, we can conclude that the greedy approximations offer a computationally feasible approach to generating proxy sets.






\section{Discussion}

Using local surrogate models as explanations is a common approach for model-agnostic XAI methods.
In this paper, we have shown that the {\sc ExplainReduce} procedure can find a post-hoc global explanation consisting of a small subset of simple models when provided a modest set of pre-trained local explanations.
The procedure is agnostic to both the underlying closed-box model and the XAI method, assuming that the XAI method can function as a generative (predictive) model.
These proxy sets achieve comparable fidelity, stability, and coverage to the full explanation set while remaining succinct enough to be easily interpretable by humans.
The loss-minimising reduction method with a soft coverage constraint especially offers a balance of fidelity, coverage, and stability across different datasets.
Furthermore, we have shown how we can use greedy algorithms to find these proxy sets with minimal computational overhead with nearly equal performance to analytically optimal counterparts.
The methods are not sensitive to hyperparameters, allowing for easy adoption.

As with any work, there are shortcomings.
The presented implementation for the procedure simply finds a set of proxies that is (approximately) optimal from a global loss or coverage perspective.
In other words, there is no consideration of the spatial distribution of the data with respect to which proxy was associated with each item.
To deepen the insight provided by the method, an interesting idea would be to combine the reduction process with manifold learning techniques, enabling visualisation alongside the proxy set, similar to {\sc slisemap} and {\sc slipmap}.
For example, we might try to find an embedding where data items associated with a particular proxy would form continuous regions, significantly increasing interpretability and augmenting the method's capabilities as an exploratory data analysis tool.

Another interesting way to leverage the set of applicable local models is uncertainty quantification.
Instead of trying to find local models applicable to many data items, we could instead turn our attention to all applicable models for a given item.
This set of applicable models could be considered as samples from the global distribution of explanations for that particular item.
Thus, we could use such sets (alongside the produced explanation) as a way to find, e.g., confidence intervals for explanations produced by a wide variety of XAI methods.

\bmhead{Acknowledgements}

We thank the Research Council of Finland for funding (decisions 364226, VILMA Centre for Excellence) and the Finnish Computing Competence Infrastructure (FCCI) for supporting this project with computational resources.

\bibliography{alternative2024}%

\newpage

\appendix

\section{Dataset introduction}
\label{a:datasets}
Below, we describe each of the datasets used in the experiments in the paper.

\noindent\textit{Synthetic} \cite{bjorklund2023slisemap} is a synthetic regression dataset with $N$ data points, $M$ features, $k$ clusters, and cluster spread $s$.
Each cluster is associated with a linear regression model and a centroid.
Coefficient vectors $\beta_j \in \mathbb{R}^{M+1}$ and centroids $c_j \in \mathbb{R}^M$ are sampled from normal distributions, with repeated samplings if either the coefficients or the centroids are too similar between the clusters.
Each data point is assigned to a cluster $j_i$, with features $x_i$ sampled around $c_{j_i}$ and standardized.
The target variable is $y_i = x_i^\top \beta_{j_i} + \epsilon_i$, where $\epsilon_i$ is Gaussian noise with standard deviation $\sigma_e$.
In the experiment section, unless otherwise stated, we set the number of data points to $N = 5000$, the number of features to $M = 11$, the number of clusters to $k = 5$, and the standard deviation of Gaussian noise to $\sigma_e = 2.0$.
As the closed-box model, we use a random forest regressor.

\vspace{0.2cm}

\noindent\textit{Air Quality} \cite{oikarinenDetectingVirtualConcept2021} contains 7355 hourly observations of 12 different air quality measurements.
One of the measured qualities is chosen as the label, and the other values are used a covariates.
With this dataset, we use a random forest regressor as the closed-box model.

\vspace{0.2cm}

\noindent\textit{Life Expectancy} \cite{rajarshi2017life} comprising 2,938 instances of 22 distinct health-related measurements, this dataset spans the years 2000 to 2015 across 193 countries.
The expected lifespan from birth in years is used as the dependent variable, while other features act as covariates.
The closed-box model we use with this dataset is a neural network.

\vspace{0.2cm}

\noindent\textit{Vehicle} \cite{birla2022vehicles} contains 2059 instances of 12 different car-related features, with the target being the resale value of the instance.
We use a support vector regressor as the closed-box model with the Vehicles dataset.

\vspace{0.2cm}

\noindent\textit{Gas Turbine} \cite{2019gas} is a regression dataset with 36,733 instances of 9 sensor measurements on a gas turbine to study gas emissions.
With this dataset, we used a Adaboost regression as the closed-box model.

\vspace{0.2cm}

\noindent\textit{QM9} \cite{ramakrishnan2014Quantum} is a regression dataset comprising 133,766 small organic molecules. Features are created with the Mordred molecular description calculator \cite{moriwaki2018mordred}.
The QM9 closed-box model is a neural network.

\vspace{0.2cm}

\noindent\textit{HIGGS} \cite{whiteson2014higgs} is a two-class classification dataset consisting of signal processes that produce Higgs bosons or are background.
The dataset contains nearly $100000$ instances with 28 features.
We used a gradient boosting classifier with this dataset as the closed-box model.

\vspace{0.2cm}

\noindent\textit{Jets} \cite{CMS:opendata} contains simulated LHC proton-proton collisions.
The collisions produce quarks and gluons that decay into cascades of stable particles called jets.
The classification task is to distinguish between jets generated by quarks and gluons. 
The dataset has $266421$ instances with 7 features.
The closed-box model we used with this dataset is a random forest classifier.

\clearpage  %

\section{Fidelity of the proxy sets as a function of proxy number}\label{a:fidelity_k_full}
Figure \ref{fig:fidelity_k_full} shows the complete fidelity plots as a function of $k$. Each row corresponds to a different dataset, and each column represents a local model generation method. A general trend emerges, indicating improved fidelity with increasing $k$.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/fidelity_k_full.pdf}
    \caption{Fidelity of the reduction algorithms as a function of the proxy set size $k$. The black dashed line indicates the performance of the full set of explanations.}
    \label{fig:fidelity_k_full}
\end{figure}
\FloatBarrier  %
\clearpage  %

\section{Fidelity of the proxy sets as a function of subsample size}\label{a:fidelity_n_full}
Figure \ref{fig:fidelity_n_full} shows the complete fidelity plots as a function of subsample size. Each row corresponds to a different dataset, and each column represents a local model generation method. A general trend emerges, indicating improved fidelity with increasing subsample size.
\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/fidelity_n_full.pdf}
    \caption{Fidelity of the reduction algorithms as a function of subsample size. The black dashed line indicates the performance of the full set of local explanations.}
    \label{fig:fidelity_n_full}
\end{figure}

\FloatBarrier  %
\clearpage  %

\section{Coverage of the proxy sets as a function of proxy number}\label{a:coverage_full}
Figure \ref{fig:coverage_full} shows the complete results of the coverages of the proxy sets. Each row corresponds to a different dataset, and each column represents a local model generation method. A general trend emerges, indicating broader coverage with increasing $k$.
\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/coverage_full.pdf}
    \caption{Coverage of the reduction algorithms as a function of proxy set size $k$.}
    \label{fig:coverage_full}
\end{figure}

\FloatBarrier  %
\clearpage  %

\section{Stability of the proxy sets as a function of proxy number}\label{a:stability_full}
Figure \ref{fig:stability_full} shows the complete results of the stabilities of the proxy sets. Each row corresponds to a different dataset, and each column represents a local model generation method. A general trend emerges, indicating more stable performance with increasing $k$.
\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/stability_full.pdf}
    \caption{Instability of the reduction algorithms as a function of the proxy set size $k$.}
    \label{fig:stability_full}
\end{figure}

\FloatBarrier  %
\clearpage  %

\section{Full sensitivity analysis}\label{a:cov_eps_sensitivity}
 
Many of the reduction methods outlined in this paper require the user to select an error threshold $\varepsilon$, with some additionally requiring a minimum coverage constraint $c$.
Since $\varepsilon$ is a parameter in computing coverage, its value can significantly impact the performance of the {\sc ExplainReduce} procedure.
In this section, we study the sensitivity of the procedure to these hyperparameters.

As the error tolerance $\varepsilon$ is defined in comparison to the absolute loss, the exact value of the parameter is task- and data-dependent.
In this section, we set the error tolerance as a quantile of the training loss of the underlying closed-box function $f$, i.e., $\varepsilon = q(\bm{L}_{BB}, p)$, where $q$ denotes the percentile function, $\bm{L}_{BB, i} = \ell(f(\bm{x}_i), \bm{y}_i)$ and $p$ is the percentile value.

As Fig. \ref{fig:coverage_p_full} shows, the reduction methods that require these parameters produce results with nearly identical fidelity across a wide range of both $c$ and $\varepsilon$.
The test fidelity of the proxy set for all tested reduction methods exhibits low variance when the error tolerance is set anywhere between the 10th and 50th percentiles of the closed-box training loss values.
Additionally, the loss-minimising reduction method with soft coverage produces similar fidelity results across the entire tested range of minimum coverage values, though it often fails to meet the coverage constraint at higher values.
It can be concluded that the reduction methods are not sensitive with respect to these hyperparameters.
For simplicity, we suggest using $c = 0.8$ and $\varepsilon = q(\bm{L}_{BB}, 0.2)$ as default arguments.
When not stated otherwise, these are also the $c$ and $\varepsilon$ values used in other experiments in this paper.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{figures/coverage_p_sensitivity_full.pdf}
    \caption{Sensitivity of the reduction methods as a function of error tolerance and minimum coverage in case of the {\sc const min loss} algorithm.}
    \label{fig:coverage_p_full}
\end{figure}

\end{document}
