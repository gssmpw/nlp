\pdfoutput=1
\documentclass[10pt,journal,compsoc]{IEEEtran}

\ifCLASSOPTIONcompsoc
  \usepackage[nocompress]{cite}
\else
  \usepackage{cite}
  \usepackage{float}
\fi

\newtheorem{definition}{Definition} 
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
% \newtheorem{proof}{Proof}
\newtheorem{assumption}{Assumption}
\newtheorem{proposition}{Proposition}
\usepackage{booktabs}       % professional-quality tables
\usepackage{makecell}

\usepackage{xcolor}
\usepackage{color}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}

\usepackage{MnSymbol} % 美化公式的package

\usepackage{multirow}
% \usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{graphicx,subfig}
\usepackage{subcaption}
% \usepackage{subfigure}

\usepackage{url}
\usepackage{hyperref}

\newenvironment{yinchuanedit}{\color{cyan}}{ }
\newenvironment{yinchuancom}{\color{magenta}[Yinchuan]}{ }
\newenvironment{leoedit}{\color{blue}}{ }
\newenvironment{Haozhiedit}{\color{gray}}{ }
\newenvironment{xinyuedit}{\color{purple}}{ }
% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}
\definecolor{ccColor}{rgb}{0.788,0.278,0.216}
\newcommand{\cc}
[1]{\textcolor{ccColor}{[CC: #1]}}



\begin{document}
%

\title{Generative Models in Decision Making: A Survey}

\author{Yinchuan Li,~Xinyu Shao,~Jianping               Zhang,~Haozhi Wang,~Leo Maxime 
         Brunswic,Kaiwen Zhou,\\
        Jiqian Dong,~Kaiyang Guo,~Xiu Li,~Zhitang Chen,~Jun Wang,~Jianye Hao%
        \thanks{Yinchuan Li, Haozhi Wang, Leo Maxime Brunswic, Kaiwen Zhou, Jiqian Dong, Kaiyang Guo, Zhitang Chen and Jianye Hao are with the Huawei Noah’s Ark Lab, China (e-mail: liyinchuan@huawei.com; haojianye@huawei.com). (Corresponding author: Jianye Hao)}
        % (e-mail: 1155160329@link.cuhk.edu.hk).
        \thanks{Xinyu Shao is with Tsinghua Shenzhen International Graduate School, Tsinghua University, Shenzhen 518055, China, and is also with the Huawei Noah’s Ark Lab, Shenzhen 518129, China.}
        \thanks{Jianping Zhang is with The Chinese University of Hong Kong, Hong Kong.}
        % (e-mail: shaoxy23@mails.tsinghua.edu.cn).
        \thanks{Xiu Li is with Tsinghua Shenzhen International Graduate School, Tsinghua University, Shenzhen 518055, China.}
        % (e-mail: li.xiu@sz.tsinghua.edu.cn).
        \thanks{Jun Wang is with Department of Computer Science, University College London, WC1E 6BT London, UK.}%
        }

\IEEEtitleabstractindextext{

\begin{abstract}

In recent years, the exceptional performance of generative models in generative tasks has sparked great interest in their integration into decision-making processes. Due to their ability to handle complex data distributions and their strong model capacity, generative models can be effectively incorporated into decision-making systems by generating trajectories that guide agents toward high-reward state-action regions or intermediate sub-goals. This survey offers an in-depth overview of how generative models are applied to decision-making tasks. We categorize seven core types of generative models: energy-based models, generative adversarial networks, variational autoencoders, normalizing flows, diffusion models, generative flow networks, and autoregressive models. In terms of their applications, we divide their functions into three main roles: controllers, modelers and optimizers, and discuss how each role contributes to decision-making. Additionally, we explore how these models are applied in five key real-world decision-making situations. In conclusion, we highlight the strengths and limitations of existing approaches and suggest three key directions for advancing next-generation generative directive models: enhanced performance algorithms, large-scale generalized decision-making models, and self-evolving adaptive models. A curated list of papers categorized by our taxonomy is available at https://github.com/xyshao23/Awesome-Generative-Models-for-Decision-Making-Taxonomy. 

\end{abstract}

\begin{IEEEkeywords}
Generative Models, Decision Making, Generative Decision Making
\end{IEEEkeywords}}


\maketitle

\IEEEdisplaynontitleabstractindextext

\IEEEpeerreviewmaketitle

\IEEEraisesectionheading{\section{Introduction}\label{sec:introduction}}
\IEEEPARstart{G}{enerative} models have seen rapid advancements, from DALL-E \cite{ramesh2021zero} (for image generation) and GPT-3 \cite{brown2020language} (for text generation) to recent iterations such as DALL-E 3 \cite{betker2023improving}, ChatGPT, and GPT-4 \cite{openai2023gpt}. Despite their exceptional performance in content generation, applying them to decision-making poses several challenges, including:
1) How to learn policies through interaction with the environment, rather than simply mimicking expert behavior?
2) How to generate new policies based on learned behaviors, transitioning from policy learning to policy generation?
3) How to develop a robust decision-making model that can adapt to various environments with minimal tuning efforts?
4) How to build multi-step reasoning and long-term evolution capabilities of strategies?
These challenges highlight the necessity for generative models to go beyond simple data generation.

In practice, decision-making is often framed as sequential decision-making, where each decision influences future actions. Classical methods like Dynamic Programming (DP) and Reinforcement Learning (RL) optimize policies based on observed rewards and state transitions in Markov Decision Processes (MDPs). However, they often rely on trial-and-error or predefined states, which limits exploration. Additionally, they can be computationally expensive and require retraining when applied to new environments. In contrast, generative models aim to capture data distributions and generate new samples, which enhances the exploration of diverse decision paths. This makes them particularly valuable in complex environments where traditional methods may fall short. However, in simpler, well-defined environments, classical approaches remain effective.

Building on the advantages highlighted above, recent years have seen substantial research efforts focused on advancing generative models and applying them to decision-making. Fig. \ref{fig:papernumbers} illustrates the research trends in generative models and their applications in decision-making, further underscoring the significance of these methods in addressing such challenges. However, a thorough review summarizing past work and identifying new research directions is still lacking. This gap motivates us to provide this survey.

The survey presents three major contributions: 1) We propose a comprehensive and detailed taxonomy for classifying generative decision-making methods, identifying seven types of models and categorizing their roles into controller, modeler, and optimizer. 2) We review the practical and diverse uses of generative models in decision-making, focusing on robot control, structural generation, games, autonomous driving, and optimization tasks. 3) We summarize the key advantages and limitations of existing work and discuss future perspectives for developing high-performance generative models in decision-making.
    
The organization of the rest of this survey is as follows (refer to Fig. \ref{fig:overview} for a general outline): Section \ref{sec:sec2} serves as a preliminary by introducing sequential decision-making formulation and provides the basics. Specifically, we offer a detailed introduction to seven types of generative models and compare their performances with traditional approaches. Section \ref{sec:sec3} presents the proposed taxonomy for categorizing generative decision-making methods. In Section \ref{sec:sec4}, we review and analyze existing literature according to the introduced taxonomy. Section \ref{sec:sec5} showcases practical applications of generative models in decision-making. Finally, Section \ref{sec:sec6} discusses the future directions of generative models in decision-making, and we conclude in Section \ref{sec:sec7}.

\begin{figure}[t]
\centering
\includegraphics[width=0.95\columnwidth]{figures/0121_Fig1v3.png}
\caption{Research trends in generative models and their applications in decision-making (2000-2024). The bars represent the rough average annual number of papers, sourced from google scholar. The search includes titles associated with seven classic types of generative models and their applications in five real-world scenarios we details in Fig. \ref{fig:applications}.}
\label{fig:papernumbers}
\end{figure}

\begin{figure*}
\centering
\includegraphics[width=\textwidth]{figures/overview.pdf} % Use .pdf extension
\caption{An overview of the survey. Specific sections are distinguished by different colors. Best viewed in color.}
\label{fig:overview}
\end{figure*}

\section{Preliminaries}
\label{sec:sec2}

\subsection{Sequential Decision Making}
Sequential decision making involves step-by-step decisions where each choice depends on prior outcomes. The agent observes the current state, selects an action based on its policy, and the environment updates the state while providing rewards. The goal is to maximize accumulated rewards.

If the agent can fully observe the environment, it is modeled as a Markov Decision Process (MDP) \cite{bellman1957markovian}, which is defined as a tuple $\mathcal{M}=(\mathcal{S}, \mathcal{A}, \mathcal{R}, P, \rho_0, \gamma, H)$. Here, $\mathcal{S}$ is the state space, and $s \in \mathcal{S}$ contains all the information perceived by the agent from the environment. $\mathcal{A}$ represents the action space, encompassing all possible actions $a$ that the agent can execute when interacting with the environment. $r \in \mathcal{R}: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow \mathbb{R}$ denotes the reward function, which corresponds to the transition pair $(s,a,s')$. 
$P: \mathcal{S} \times \mathcal{A} \rightarrow \Delta(\mathcal{S})$ refers to the transition function. When the action $a$ is applied to the state $s$, $P(s'|s,a)$ generates a distribution over states. $\rho_0 \in \Delta(\mathcal{S})$ represents the initial state distribution. The discount factor $\gamma \in [0,1]$ quantifies the long-term value of the current action, while $H$ denotes the horizon.

If observations are limited, we can define the Partially Observed Markov Decision Process (POMDP) \cite{krishnamurthy2016partially} as a tuple $\mathcal{M}=(\mathcal{S}, \mathcal{A}, \mathcal{O}, \mathcal{R}, P, \rho_0, E, \gamma, H)$. Here, $\mathcal{O}$ denotes the observation space, and each observation $o_t \in \mathcal{O}$ is obtained by the emission function $E(o_t|s_t)$.

The goal of sequential decision making is to learn a policy $\pi(a|s)$ or $\pi(a|o)$ by optimizing the expected reward:
\begin{equation}\label{Object}
    J(\pi) = \mathbb{E}_{\tau \sim p_\pi(\tau)}\left[\sum_{t=0}^H \gamma^t r\left(s_t, a_t\right)\right],
\end{equation}
where $p_{\pi}(\tau)$ is the distribution over trajectories $\tau$ induced by policy $\pi$:
\begin{equation}
    p_\pi(\tau)=\rho_0\left(s_0\right) \prod_{t=0}^H \pi\left(a_t | s_t\right) P\left(s_{t+1} | s_t, a_t\right).
\end{equation}

\subsection{Related Methods}
In this section, we introduce several traditional methods for solving sequential decision-making problems.
% \subsubsection{Imitation Learning}
% Imitation learning aims to optimize a policy $\pi_{\theta}(s)$ such that the action distribution is close to that of an expert policy $\pi^{*}$ for any state $s$, the objective function is given by
% \begin{equation}
%     \theta = \arg\min_{\theta} \mathbb{E}_{s\sim p(s|\pi_{\theta})}L\left(\pi^{*}(s)-\pi_{\theta}(s)\right),
% \end{equation}
% where $p(s|\pi_{\theta})$ denotes the state distribution induced by $\pi_{\theta}$, and $L$ is a metric function.

\subsubsection{Dynamic Programming}
Dynamic Programming (DP) is a method for solving sequential decision-making problems by breaking them into simpler subproblems. It is effective for problems with a known model and optimal substructure, such as Markov Decision Processes (MDPs). The goal in DP is to find an optimal policy $\pi^*(s)$ that maximizes the expected cumulative reward, achieved by solving the Bellman equations.

The Bellman equation for the value function $V(s)$ of a state $s$ is given by:
\begin{equation}
    V(s) = \max_a \left( R(s, a) + \gamma \sum_{s'} P(s'|s, a)V(s') \right),
\end{equation}
where $R(s, a)$ is the immediate reward for taking action $a$ in state $s$, $\gamma$ is the discount factor, and $P(s'|s, a)$ is the transition probability to the next state $s'$.

Alternatively, for the action-value function $Q(s, a)$, the Bellman equation is:
\begin{equation}
    Q(s, a) = R(s, a) + \gamma \sum_{s'} P(s'|s, a) \max_{a'} Q(s', a'),
\end{equation}
where $Q(s, a)$ represents the expected return from taking action $a$ in state $s$.
% \subsubsection{Search based Methods}
% Policy search focuses on exploring the policy space directly, rather than computing the value function explicitly. For a given policy $\pi$, the expected discounted return is given by
% \begin{equation}
%     \mathcal{U}(\pi) = \sum_s \rho_0(s)\mathcal{U}^\pi(s).
% \end{equation}

% With large state space, $\mathcal{U}(\pi)$ could be approximated by sampling trajectories consists of $(\mathcal{S},\mathcal{A},\mathcal{R})$ pairs and $ \mathcal{U}(\pi)$ can be reformulated as \cite{kochenderfer2022algorithms}:
% \begin{equation}
%     \mathcal{U}(\pi) = \mathbb{E}_\tau[R(\tau)]= \int p_\pi(\tau)R(\tau)d\tau,
% \end{equation}
% where $R(\tau)$ denotes the discounted return related to $\tau$.
% Monte Carlo policy evaluation entails estimating the expected utility of the policy $\pi$ by performing numerous rollouts starting from $s_0\sim \rho_0(s)$.
% % states sampled from an initial state distribution.
% Local search starts with a feasible solution and moves to a neighbor with better utility until convergence, e.g., Hooke-Jeeves method \cite{hooke1961direct}. Simulated annealing \cite{kirkpatrick1983optimization} allows occasional moves to worse solutions to approximate the global optimum. Genetic algorithms evaluate different simulations, recombine them, and guide the population toward the global optimum.

% \subsubsection{Planning \& Optimization}
% Planning is an optimization approach that relies on a predefined model to guide the search. Let $U(s)$ denote the action space for each state $s$, the set of all actions that could be applied from $s$. For distinct $s, s^\prime \in \mathcal{S}$, $U(s)$ and $U(s^\prime)$ are not necessarily disjoint. As part of the planning problem, a set $\mathcal{S}_G \subset \mathcal{S}$ of goal states and the initial state $s_I$ are defined.

% Let $\pi_K$ denote a $K$-step~plan, which is a sequence of $(a_1,a_2,\cdots,a_K)$ of $K$ actions. Let $F$ denote the final stage where $F= K+1$.
% The cost functional is defined as:
% \begin{equation}
%     L(\pi_K) = \sum_{k=1}^N l(s_k,a_k) + l_F(s_F),
% \end{equation}
% where $l(s_k,a_k)$ is the cost term yielding a real value for every $s_k\in\mathcal{S}$ and $a_k \in U(s_k)$. The final term is defined $l_F(s_F) = 0$ if $s_f\in \mathcal{S}_G$ and $l_F(s_F) = \infty$ otherwise.

\subsubsection{Reinforcement Learning}
The field of RL contains various methods that cater to different decision-making tasks, but all standard RL algorithms generally follow the same learning principles. In particular, the agent interacts the current MDP $\mathcal{M}$ using some sort of behavior policy, which can be the present policy $\pi(a|s)$ or mix it with a random policy. Then the agent can receive the subsequent state $s_{t+1}$ along with the reward function $r_t$. After repeating this process for many steps, the agent can use the collected samples $\{s,a,r,s'\}$ to update the policy.


\textbf{Value-based Methods.} 
One classic approach is to directly estimate the value function of the state of state-action so that a near-optimal policy can be obtained. 
Define the state-action value as
\begin{equation}
Q^\pi\left(s_t, a_t\right) =\mathbb{E}_{\tau \sim p_\pi\left(\tau | s_t, a_t\right)}\left[\sum_{t^{\prime}=t}^H \gamma^{t^{\prime}-t} r\left(s_t, a_t\right)\right]
\end{equation}
and the state value $V^{\pi}(s_t)$ as
\begin{equation}\label{dp}
    \begin{aligned}
V^\pi\left(s_t\right) & =\mathbb{E}_{a_t \sim \pi\left(a_t |s_t\right)}\left[Q^\pi\left(s_t, a_t\right)\right].
\end{aligned}
\end{equation}
Then the recursive representation of $Q^{\pi}(s_t,a_t)$ can be derived as \cite{tao2021data}:
\begin{equation}\label{dp}
Q^\pi\left(s_t, a_t\right) = r\left(s_t,a_t\right)+\gamma \mathbb{E}_{s_{t+1} \sim P(s_t,a_t)}\left[V^\pi\left(s_{t+1}\right)\right],
\end{equation}
which can also be expressed as the Bellman operator $\mathcal{B}^{\pi}$, i.e., $Q^{\pi} = \mathcal{B}^{\pi}Q^{\pi}$.
$\mathcal{B}^{\pi}$ has the unique fixed point obtained by repeating iterations $Q^{\pi}_{k+1} = \mathcal{B}^{\pi}Q^{\pi}_{k}$ \cite{sutton1998introduction}. Based on this property, we can derive the modern value iteration (VI) algorithms.
Q-learning is a common VI method that expresses the policy as $\pi(a_t|s_t)=\delta(a_t=\arg\max_{a_t}Q(s_t,a_t))$ with Dirac function $\delta(\cdot)$. 
The optimal Q-function can be approximated by substituting this policy into \eqref{dp}.
To derive a learning algorithm, we can define a parametric Q-function estimator $Q_{\phi}(s,a)$, which can be optimized by minimizing the difference between LHS and RHS in Bellman equation, such as fitted Q-learning \cite{ernst2005tree}. With the help of neural networks, existing deep Q-learning methods can achieve more accurate value-function estimation by minimizing the Bellman error objective \cite{fu2019diagnosing}.

\textbf{Policy Gradients.}
Another classic approach is to directly estimate the gradient of \eqref{Object}. We define policy $\pi_{\theta}(a|s)$ parameterized by $\theta$, which can be a neural network and output the logits of action $a$. Then, We can then write the gradient of \eqref{Object} with respect to $\theta$ as:
\begin{equation}
    \nabla_\theta J\left(\pi_\theta\right)=\mathbb{E}_{\tau \sim p_{\pi_\theta}(\tau)}\left[\sum_{t=0}^H \gamma^t \nabla_\theta \log \pi_\theta\left(a_t | s_t\right)\hat{A}(s_t,a_t)\right], \nonumber
\end{equation}
where $\hat{A}(s_t,a_t)$ is the return estimator,
written by
\begin{equation}
    \hat{A}(s_t,a_t) = \sum_{t^{\prime}=t}^H \gamma^{t^{\prime}-t} r\left(s_{t^{\prime}}, a_{t^{\prime}}\right)-b\left(s_t\right),
\end{equation}
which can be calculated with Monte Carlo samples \cite{sutton1999policy}. 
$b(s_t)$ denotes the baseline, which can be approximated as the mean return of the collected trajectories or by the value function $V(s_t)$ \cite{degris2012model}.

True Region Policy Optimization (TRPO) \cite{schulman2015trust} and Proximal Policy Optimization (PPO) \cite{schulman2017proximal} stabilize training by introducing KL regularization and a clipped surrogate objective to limit policy updates at each iteration. These methods have been successful in areas such as robot control~\cite{kuba2021trust}, games~\cite{yu2022surprising}, and generative model training~\cite{radford2018improving}.

\textbf{Actor-Critic Methods.} 
Actor-critic methods combine policy gradients and value-based methods by using a parameterized policy and value function. The value function helps better estimate $\hat{A}(s_t,a_t)$ for policy gradients. 

Unlike Q-learning, actor-critic methods focuses on optimizing the Q-function for the current policy $\pi_{\theta}$ rather than learning the optimal Q-function. Variants include on-policy and off-policy algorithms~\cite{lillicrap2015continuous}. Deep Deterministic Policy Gradient (DDPG) introduces a deterministic policy for continuous control problems~\cite{lillicrap2015continuous}, while Soft Actor-Critic introduces entropy regularization to enhance exploration~\cite{haarnoja2018soft}.

\textbf{Model-based RL.}
For model-based RL, the first step is to learn the environment model $P(\cdot|s,a)$ based on historical trajectories. A common approach is to minimize the KL divergence between the learned model $\hat{P}(\cdot|s,a)$ and the true dynamics $P(\cdot|s,a)$ as \cite{chua2018deep}: 
\begin{equation}
\begin{split}
& \min _\theta \mathbb{E}_{(s, a) \sim \rho_{\pi}}\left[D_{\mathrm{KL}}\left(P(\cdot \mid s, a), \hat{P}_\theta(\cdot \mid s, a)\right)\right],
\end{split}
\end{equation}
where $\theta$ denotes the learned model's parameters.

When the model is ready, the agent develops plans to improve its strategy to interact with the modeled world. Traditional methods for integrating planning into MBRL mainly include model predictive control, Monte Carlo tree search and Dyna-style methods \cite{sutton2012introduction}. Dyna-style methods utilize the learned model to generate more experience and perform RL on the model-augmented dataset, which has been the main method \cite{feinberg2018model}. These methods also use value function estimation and policy gradients to refine the policy. 

\subsection{Generative Models}
Generative models ~\cite{gozalo2023survey,gozalo2023chatgpt,jo2023promise,brynjolfsson2023generative,ackley1985learning,goodfellow2020generative,zhu2017unpaired,sohl2015deep} are an crucial subfield of artificial intelligence, aimed at generating unseen data samples based on the underlying distribution of existing datasets. In this section, we explore generative models along three dimensions: sample quality, diversity, and computational efficiency \cite{verine2024quality,li2024balancing}. These dimensions are crucial for understanding generative model performance in decision-making, as they directly affect the accuracy, robustness, and applicability of generated outputs. 

Balancing sample quality, diversity, and computational efficiency presents a challenge for generative models. For example, Diffusion Models and Normalizing Flows offer strong sample diversity and stability but require high computational resources, limiting their suitability for real-time decision-making applications \cite{ho2020denoising,salimans2016improved,kingma2018glow}. In contrast, models like VAEs and GANs provide faster training and better efficiency but may struggle with maintaining sample diversity, potentially leading to overly similar or overfitted outputs \cite{alemi2016deep,kingma2013auto,goodfellow2020generative,radford2015unsupervised}. Drawing from the studies \cite{oussidi2018deep,dhariwal2021diffusion,kingma2013auto,salimans2017pixelcnn++,girin2020dynamical,finn2016connection,abdollahzadeh2023survey,bengio2021gflownet,betzalel2022study,papamakarios2021normalizing}, we roughly compare the performance of these seven generative models across sample quality, diversity, and efficiency, as shown in Fig.~\ref{fig:generative model}.
        
\begin{figure}[t]
\centering
\includegraphics[width=0.95\columnwidth]{figures/generative_comparison.pdf}
\caption{Comparison of seven generative models in decision-making: training stability, generation diversity, and computational efficiency. Larger bubbles represent higher computational efficiency, with different models indicated by distinct colors. Best viewed in color.}
\label{fig:generative model}
\end{figure}

\textbf{Energy Based Models~(EBMs).} 
An EBM \cite{belanger2017deep} assigns an energy function $E_\theta(x)$ to each data $x$ \cite{xu2019learning}, which is the unnormalized log probability of the data to measure the compatibility of $x$ with the model. The density of $x$ is 
% given by
\begin{equation}
    p_\theta(x)=\frac{1}{Z_\theta}\exp(-E_\theta(x)),
\end{equation}
where $Z_\theta$ denotes the normalizing constant, which also known as partition function:
\begin{equation}
    Z_\theta=\int \exp(-E_\theta(x))dx.
\end{equation}
The model is trained to reduce the energy of data point from the training set while increasing the energy of other, possibly unrealistic, data points \cite{sutton2012introduction}. The earliest EBM is the Boltzmann machine~\cite{ackley1985learning}, which is a stochastic version of the Hopfield network~\cite{hopfield2007hopfield}.
Since the energy function has no restrictions, it can be parameterized freely and can model any high-dimensional complex data \cite{suhail2021energy, lyngso2000rna} like images~\cite{ngiam2011learning} and natural languages~\cite{mikolov2013distributed}. However, the intractable normalizing constant poses training and sampling challenges. Typical solutions include 1)~MCMC sampling~\cite{parisi1981correlation}; 2)~score matching~\cite{hyvarinen2005estimation} and 3)~noise contrastive estimation~\cite{gutmann2010noise}.

\textbf{Generative Adversarial Networks~(GANs).} 
GANs~\cite{goodfellow2020generative} are well-known due to their distinctive architecture which involves a generator $G$ that creates synthetic samples and a discriminator $D$ that evaluates them by distinguishing between real and generated data. The target loss of a typical GAN is given by

\begin{equation}\label{GAN}
    \begin{aligned}
L(\theta, \phi) &=
\min _\theta \max _\phi E_{x \sim p_{\text{data}}}[\log D(x ; \phi)]\\
&~~~~~~~~~~~~~~~~~~+E_{\hat{x} \sim G(\theta)}[\log (1-D(\hat{x} ; \phi))] \,.
\end{aligned}
\end{equation}
While GAN can generate realistic contents in image synthesis~\cite{goodfellow2020generative}, image style transfer~\cite{zhu2017unpaired}, and behavior~\cite{ho2016generative}, it suffers from training instability and mode collapse. Several variants of GANs have be proposed to solve the problem, like Wasserstein GANs~\cite{arjovsky2017wasserstein,gulrajani2017improved}, CycleGANs~\cite{zhu2017unpaired}, and Progressive GANs~\cite{karras2017progressive}, which use different loss functions, architectures, or training techniques.

\textbf{Variational Autoencoders~(VAEs).} VAEs \cite{kingma2013auto} are probabilistic models that learn to represent input data in a compressed latent space, from which new samples can be generated. The objective of VAEs is to maximize the evidence lower bound~(ELBO):
\begin{equation}\label{VAE}
    \begin{aligned}
L(\phi, \theta)=\mathrm{E}_{z \sim p(z \mid x ; \phi)}[\log p(x \mid z ; \theta)]-\mathrm{KL}(q(z) \| p(z)).
\end{aligned}
\end{equation}
In \eqref{VAE}, the first term represents the likelihood of the observed data $x$ given latent variable $z$, and the expectation is taken over the distribution of $z$. The KL term measures the divergence between the variational distribution $q(z)$, which approximates the posterior distribution of $z$, and the prior distribution $p(z)$. 

VAEs are used for generating and augmenting new data (e.g., images, videos) \cite{kulkarni2015deep,liu2018image,deshpande2017learning}, data compression, and anomaly detection \cite{higgins2016beta,hu2023glso}. However, they may suffer from low generation quality and the independence assumption between latent variables \cite{higgins2016beta}. Extensions like conditional VAEs \cite{sohn2015learning} and Gaussian mixture VAEs \cite{dilokthanakul2016deep} have been proposed to address these issues. VAEs are also combined with techniques such as adversarial training \cite{mescheder2017adversarial,kim2021conditional} and normalizing flowss \cite{silvestri2022deterministic} for improved performance, and research on continual learning and hyperparameter tuning (e.g., in $\beta$-VAE) \cite{bae2022multi} is ongoing.

% Researchers propose to combine VAEs with other techniques, including adversarial training~\cite{mescheder2017adversarial,kim2021conditional} and normalizing flows~\cite{silvestri2022deterministic} for improving disentanglement. Besides, ~\cite{ye2022continual} and ~\cite{li2022continual} study the continual learning of VAE representations and ~\cite{bae2022multi} trains a response function of the hyperparameter $\beta$ in $\beta-VAE$ to trade off the reconstruction error and the KL divergence.

\textbf{Normalizing Flows (NFs).} 
Flow-based models \cite{rezende2015variational} map simple distributions, such as a Gaussian distribution, to a more complex distribution capable of representing data. Normalizing Flows \cite{oord2018parallel,van2016conditional} apply invertible transformations to maintain a tractable density function, as in \eqref{flow}:
\begin{align}\label{flow}
    &\left.\left.x=f_K\left(f_{K-1}\left(\ldots f_0\left(z_0\right)\right)\right) \ldots\right)\right) \text { with } z_0 \sim N(0, I) \,, \nonumber \\
        &f_\theta(z)=z+u h\left(w^T z+b\right) \,. 
\end{align}

Normalizing Flows excel in capturing complex probabilities and are used for tasks like sample generation and density estimation, though challenges such as restricted network structure and the rendered limited expressiveness remain. The field continues to evolve with advancements~\cite{NIPS2000_3c947bc2,tabak2010density,tabak2013family,kobyzev2020normalizing}, including methods like NICE~\cite{dinh2014nice}, Real-NVP~\cite{dinh2016density}, Glow~\cite{kingma2018glow}, autoregressive flows~\cite{kingma2016improved,papamakarios2017masked,huang2018neural}, and Neural ODEs~\cite{lu2018beyond,chen2018neural,grathwohl2018ffjord} to enhance efficiency and expressiveness.
% From limited pre-neural attempts \cite{NIPS2000_3c947bc2,tabak2010density,tabak2013family}, specific flow transformations \cite{kobyzev2020normalizing} are proposed to reduce the computational cost, such as NICE~\cite{dinh2014nice}, Real-NVP~\cite{dinh2016density}, and Glow~\cite{kingma2018glow}. Researchers also use autoregressive flows~\cite{kingma2016improved,papamakarios2017masked,huang2018neural} to accelerate the sampling process. Many Normalizing Flow variants also leverage the techniques of Neural ODE \cite{lu2018beyond,chen2018neural,grathwohl2018ffjord} and optimal transport \cite{finlay2020train,rozen2021moser}.

\textbf{Diffusion Models (DMs).} 
Diffusion models, introduced by Sohl-Dickstein et al. in 2015 \cite{sohl2015deep}, generate samples by progressively adding noise to data and then recovering the original data through a reverse diffusion process during sampling (as shown in Fig. \ref{fig:diffusion model}). 
% Diffusion models have outperformed GANs in generating high-quality and diverse outputs, especially in image synthesis tasks \cite{song2019generative}. This has led to growing interest in their application to complex decision-making, particularly in high-dimensional tasks where both diversity and quality are critical.
\begin{figure}[ht]
\centering
\includegraphics[width=0.85\columnwidth]{figures/diffusion.png}
\caption{A diagrammatic depiction of diffusion models and one denoising step is illustrated \cite{yang2023diffusion}.}
\label{fig:diffusion model}
\end{figure}

The key components of a diffusion model are the forward and the reverse process. In the forward phase, Gaussian noise is progressively introduced to the data. At time step $t$, the data point $\mathbf{x}_{t-1}$ is transformed into a noisy version $\mathbf{x}_t$ according to Equation \ref{17}:
\begin{equation}\label{17}
    q\left(\mathbf{x}_t \mid \mathbf{x}_{t-1}\right) = \mathcal{N}\left(\mathbf{x}_t; \sqrt{1-\beta_t} \mathbf{x}_{t-1}, \beta_t \mathbf{I}\right).
\end{equation}

Here, $\beta_t$ denotes a schedule of noise variance, which governs how quickly the noise is introduced at step $t$. The forward process effectively destroys the original data structure, making it increasingly difficult to recover the original input as the process progresses.

The sampling process works by reversing the diffusion (or noise addition) steps. The model iteratively refines a noisy sample to recover the original data by learning a reverse diffusion process:
\begin{equation}
    p_\theta\left(\mathbf{x}_{t-1} \mid \mathbf{x}_t\right) \quad \text{for} \quad t = T, T-1, \dots, 1.
\end{equation}

The reverse process, which involves multiple time steps, can be computationally expensive. However, it enables the generation of high-quality samples and has shown to outperform GANs in visual fidelity and diversity, particularly in image generation tasks \cite{song2019generative}.

Diffusion models are also well-suited for complex decision-making tasks, particularly in high-dimensional spaces. They can generate diverse solutions, making them ideal for applications in reinforcement learning (RL), robotics, and automated planning. For instance, in robotic control, diffusion models can generate motion sequences that explore alternative strategies for better performance in dynamic environments. Similarly, in autonomous driving, they can generate realistic trajectories while accounting for environmental uncertainties.

However, the slow sampling process limits scalability for high-dimensional tasks. Consistency models~\cite{song2023consistency} offer a potential solution with fast, one-step generation.

\textbf{GFlowNets/CFlowNets.}
GFlowNets is a new type of generative model, which \cite{bengio2021flow,bengio2021gflownet} are initially defined on finite directed acyclic graphs $G=(S,E)$ with given initial and final states $s_0,s_f\in S$ and given reward function $R(s\rightarrow s_f)$; an edge flow is simply a map $F:E\rightarrow \mathbb R_+$. Fig.\ref{fig:gflownet} shows the illustration of the structure of GFlowNets. From such an edge flow, one can define a Markov chain $(p_t)$ on $G$ setting $\mathbb P(p_{t+1}=s'|p_t=s) = \frac{F(s\rightarrow s')}{\sum_{s\rightarrow s''}F(s\rightarrow s'')}$.
The edge flow $F$ is trained so that for all state $s\in S\setminus\{s_0,s_f\}$ 
$$\sum_{s'\rightarrow s} F(s'\rightarrow s)=\sum_{s\rightarrow s'} F(s\rightarrow s'),\quad F(s\rightarrow s_f) = R(s\rightarrow s_f).$$
The first equality is the flow-matching constraint, the second is the reward constraint.
Assuming the reward $R$ is non-negative and not identically zero, those two constraints ensures that the Markov chains reaches $s_f$ at some finite time $\tau$ and that the last non-terminal position $p_{\tau-1}$ of the Markov chain follows the distribution obtained by normalizing $R$ i.e. $\mathbb P(p_{\tau-1}=s) = \frac{R(s \rightarrow s_f)}{\sum_{s'\in S} R(s'\rightarrow s_f)}$. 
The framework has been extended beyond graphs and acyclicity \cite{li2023cflownets,brunswic2024theory,lahlou2023theory}, with variation on losses and regularizations \cite{malkin2022trajectory,pan2023better,brunswic2024theory}.

\begin{figure}[ht]
\centering
\includegraphics[width=0.85\columnwidth]{figures/gflownet.pdf}
\caption{Illustration of the structure of a Generative Flow Network, as a pointed DAG over states $s$, with particles flowing along edges to represent the flow function \cite{bengio2021gflownet}.}
\label{fig:gflownet}
\end{figure}

\textbf{Autoregressive Models (AMs).} 
Autoregressive models are widely used in natural language processing and generative modeling. They are central to sequence-to-sequence models, like the Transformer \cite{vaswani2017attention}, which generate output sequences based on input or context.
These models predict each output element sequentially, conditioned on the preceding ones, as shown in \eqref{AM}:

\begin{equation}\label{AM}
p(y_1, y_2, \dots, y_T) = \prod_{t=1}^{T} p(y_t \mid y_1, y_2, \dots, y_{t-1}).
\end{equation}

At each time step \(t\), the model predicts the next element \(y_t\) based on the context provided by the preceding elements.

A popular method for autoregressive generation is using autoregressive language models, such as GPT \cite{radford2019language}, which have proven successful in NLP tasks like text generation, machine translation, and dialogue systems.

In AMs, the input at each step combines the previous elements and a context vector, typically obtained from an encoder that processes the input sequence and generates a fixed-size representation for the decoder.

During training, AMs are optimized using Maximum Likelihood Estimation (MLE) \cite{child2019generating}, aiming to maximize the likelihood of generating the target sequence given the input or context. This involves calculating the probability of each target element based on preceding elements and updating model parameters to improve predictions \cite{yang2019xlnet}.

Overall, AMs are a powerful tool for sequence generation tasks, particularly in NLP \cite{devlin2018bert}, significantly advancing generative modeling and remaining an active research area.

\subsection{Difference with previous approaches}

Reinforcement learning (RL) and generative models are distinct in their objectives and learning processes \cite{agarwal2020model}, as shown in Fig. \ref{fig:difference}. RL focuses on teaching an agent to learn an optimal policy through trial and error, maximizing cumulative rewards by interacting with the environment. The agent refines its strategy based on the feedback it receives from taking actions and observing the resulting states.

Generative models, on the other hand, aim to learn the underlying data distribution of a given dataset \cite{tirinzoni2020sequential}. They generate new samples that resemble the training data, capturing the inherent patterns and structures.

The learning processes also differ. In RL, the agent explores the environment, takes actions, and receives rewards, continuously refining its policy to make more effective decisions. In contrast, generative models learn from a dataset and estimate the data's probability distribution, with examples being VAEs and GANs.

Additionally, RL and generative models differ in their interaction with the environment \cite{li2020breaking}. In RL, the agent interacts with the environment through trial and error, learning from feedback without generating new data. In contrast, generative models, once trained, actively generate new data points that resemble the original data \cite{wang2021quantum}.

\begin{figure}[ht]
\centering
\includegraphics[width=0.85\columnwidth]{figures/difference.png}
\caption{A comparative framework: traditional and generative decision-making (MDP-based vs. Data-Driven).}
\label{fig:difference}
\end{figure}

In summary, RL focuses on learning optimal actions to achieve specific goals \cite{azar2012sample}, while generative models aim to model data distributions and generate similar samples. Both are vital in machine learning but tackle different challenges.

\begin{table*}
    \centering
    \begin{tabular}{c|c|c|c|c|c}
    \hline
     Model & Family & Function & Structure & Expertise & Application \\
      \hline
      
soft Q-learning \cite{haarnoja2017reinforcement} & EBM & Controller & EBM & Online RL & Robot Control\\
EBIL \cite{liu2021energy}& EBM & Controller & EBM & Imitation Learning & Robot Control \\

BM \cite{ackley1985learning}& EBM & Modeler & EBM & Generation & Optimization / Structural Generation \\
DEBMs\cite{haarnoja2017reinforcement} & EBM & Modeler & EBM & Online RL & Robot Control / Optimization\\
SGMs \cite{song2019generative}& EBM & Modeler & EBM & Generation & Structural Generation \\

SO-EBM \cite{kong2022end}& EBM & Optimizer & EBM & Others & Optimization \\
pcEBM\cite{tagasovska2022pareto} & EBM & Optimizer & EBM & Generation & Structural Generation / Optimization \\
CF-EBM \cite{zhao2020learning}& EBM & Optimizer & EBM & Generation & Structural Generation \\

GAIL \cite{ho2016generative}& GAN & Controller & GAN &Imitation Learning & Robot Control\\
InfoGAIL \cite{li2017infogail} & GAN & Controller & WGAN & Imitation Learning & Robot Control \\
MGAIL \cite{baram2017end} & GAN & Controller & GAN & Imitation Learning & Robot Control\\
FAGIL \cite{geiger2022fail}& GAN & Controller & WGAN &Imitation Learning & Robot Control\\
WGAIL \cite{wang2021learning}& GAN & Controller & GAN & Imitation Learning & Robot Control \\
IC-GAIL \cite{wu2019imitation}& GAN & Controller & GAN & Imitation Learning & Robot Control \\
AIRL \cite{fu2018learning}& GAN & Controller & GAN & Imitation Learning & Robot Control \\
AugAIRL \cite{wang2021decision}& GAN & Controller & GAN & Imitation Learning & Robot Control \\
WAIL \cite{xiao2019wasserstein}& GAN & Controller & WGAN & Imitation Learning & Robot Control \\
MAIRL \cite{sun2021adversarial}& GAN & Controller & GAN & Imitation Learning & Robot Control\\

EGAN \cite{huang2017enhanced}& GAN & Modeler & GAN & Online RL & Structural Generation\\
S2P \cite{cho2022s2p} & GAN & Modeler & GAN & Offline RL& Robot Control\\

He et al. \cite{he2020evolutionary}& GAN & Optimizer & GAN & Generation  & Optimization / Games \\
DCGANs \cite{sim2021gans} & GAN & Optimizer & DCGAN & Generation & Optimization \\
C-GANs \cite{kalehbasti2021augmenting} & GAN & Optimizer & C-GAN & Generation & Optimization \\

GTI \cite{mandlekar2020learning}& VAE & Controller & CVAE & Imitation Learning & Robot Control\\
HULC \cite{mees2022matters}& VAE & Controller & seq2seq CVAE & Imitation Learning& Robot Control\\
Play-LMP \cite{lynch2020learning}& VAE & Controller & seq2seq CVAE & Imitation Learning & Robot Control\\
TACO-RL \cite{rosete2023latent}& VAE & Controller & seq2seq CVAE &Imitation Learning & Robot Control\\
OPAL \cite{ajay2020opal}& VAE & Controller & $\beta$ VAE & Imitation Learning& Structural Generation\\
MaskDP \cite{liu2022masked}& VAE & Controller & MAE & Offline RL & Robot Control\\
Han \& Kim \cite{han2022selective}& VAE & Modeler & VAE & Offline RL & Structural Generation\\
CageBO \cite{xing2023bayesian} & VAE & Optimizer & CVAE & Others & Optimization\\
CVAE-Opt \cite{hottung2021learning} & VAE & Optimizer & CVAE & Others & Optimization\\

NF Policy \cite{ward2019improving}& NF & Controller & Coupling Flow & Offline RL & Robot Control\\
CNF \cite{akimov2022let}& NF & Controller & Autoregressive Flow & Offline RL & Autonomous Driving\\
Guided Flows \cite{zheng2023guided} & NF & Controller & Continuous Flow & Offline RL / Generation & Optimization / Games \\

NICE \cite{dinh2014nice}& NF & Modeler & Coupling Flow & Generation & Optimization \\
Rezende et al \cite{rezende2015variational}& NF & Modeler & NF & Generation & Optimization \\

Gabrié et al. \cite{gabrie2022adaptive}& NF & Optimizer & NF & Generation & Optimization \\

Pearce et al. \cite{pearce2022imitating}& DM & Controller & DDPM & Imitation Learning & Robot Control / Games \\
Diffuser \cite{janner2022planning}& DM & Controller & DDPM & Offline RL & Structural Generation \\
Decision Diffuser \cite{ajay2023is}& DM & Controller & DDPM & Offline RL & Structural Generation  \\
Decision Stacks \cite{zhao2024decision}& DM & Controller & DDPM & Offline RL & Structural Generation \\
Diffusion-QL \cite{wang2023diffusion}& DM & Controller & DDPM & Offline RL & Robot Control\\
Diffusion Policy \cite{chi2023diffusion}& DM & Controller & DDPM & Robotics & Robot Control\\
SfBC \cite{chen2023offline}& DM & Controller & DDPM & Offline RL & Structural Generation\\
UniPi \cite{du2024learning}& DM & Controller & DDPM & Offline RL & Robot Control\\
AdaptDiffuser \cite{liang2023adaptdiffuser}& DM & Controller & DDPM & Robotics& Robot Control\\
DIPO \cite{yang2023policy}& DM & Controller & DDPM & Online RL & Robot Control\\
MTDiff \cite{he2024diffusion} & DM & Modeler & DDPM & Offline RL & Robot Control\\
GenAug \cite{chen2023genaug}& DM & Modeler& LDM & Robotics & Robot Control\\
SynthER \cite{lu2024synthetic}& DM & Modeler & EDM & Offline \& Online RL & Robot Control\\
DDOM \cite{krishnamoorthy2023diffusion}& DM & Optimizer & DDPM & Others & Optimization \\
Li et. al. \cite{li2024diffusion} & DM & Optimizer & DDIM & Others & Optimization\\
DiffOPT \cite{kong2024diffusion}& DM & Optimizer & DDIM & Others & Optimization\\

GFlowNets \cite{bengio2021flow, bengio2021gflownet}& GFN & Controller & GFN & Offline \& Online RL & Structural Generation\\
Pan et al. \cite{pan2023stochastic} & GFN & Controller & GFN & Offline RL & Structural Generation\\
GAFlowNets \cite{pan2022generative}& GFN & Controller & GFN & Offline RL & Structural Generation\\
AFlowNets \cite{jiralerspong2023expected}& GFN & Controller & GFN & Offline RL & Structural Generation / Games\\
Brunswic et al. \cite{brunswic2024theory} & GFN & Controller & GFN & Offline RL & Structural Generation\\
CFlowNets \cite{li2023cflownets}& CFN & Controller & CFN & Online RL & Structural Generation\\

Zhang et al. \cite{zhang2022unifying}& GFN & Modeler & GFN & Generation & Optimization \\
Zhang et al. \cite{zhang2022generative} & GFN & Modeler & GFN & Generation & Optimization\\

GFACS \cite{kim2024ant} & GFN & Optimizer & GFN & Generation & Optimization\\
MOGFNs \cite{jain2023multi} & GFN & Optimizer & CFN & Generation & Optimization\\
 
Decision Transformer \cite{chen2021decision}& AM & Controller & Decoder Only & Offline RL & Robot Control\\
Trajectory Transformer\cite{janner2021offline}& AM & Controller & Decoder Only & Offline RL & Robot Control \\
Online DT \cite{zheng2022online}& AM & Controller & Decoder Only & Online RL & Autonomous Driving\\
GATO \cite{reed2022generalist}& AM & Controller & Decoder Only & Offline RL & Robot Control / Games \\
Multi-Game DT \cite{lee2022multigame}& AM &Controller & Decoder Only & Offline RL & Games\\
PEDA \cite{zhu2023scaling}& AM & Controller & Decoder Only & Offline RL & Robot Control\\
BooT \cite{wang2022bootstrapped}& AM & Controller & Decoder Only & Offline RL & Structural Generation\\
ALPINE \cite{wang2024alpine} & AM & Modeler & Decoder Only & Online RL & Optimization \\
ARP \cite{korenkevych2019autoregressive} & AM & Modeler & Decoder Only & Online RL & Games \\
BONET \cite{mashkaria2023generative}& AM & Optimizer & Decoder Only & Others & Optimization\\
TNP \cite{nguyen2022transformer}& AM & Optimizer & Encoder Decoder & Others & Optimization\\


\hline
    \end{tabular}
    \caption{Generative models in decision making}
    \label{tab:my_label}
\end{table*}

\section{Taxonomy}
\label{sec:sec3}
In this section, we state our taxonomy to group the generative approaches for solving sequential decision making problems. We categorize the methodologies into five key dimensions, outlined as follows. This taxonomy is further illustrated in Table \ref{tab:my_label}.

\textbf{Family.} 
The first dimension is the family of the generative models. We represent the family of the approach by the acronym of the category of the generative models. To be more specific, Energy Based Models (EBMs), Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), Normalizing Flow (NFs), Diffusion Models (DMs), GFlowNets (GFNs), and Autoregressive Models (AMs).

\textbf{Function.} 
The second dimension functions on the role of generative models in sequential decision making. Because they can model a diverse set of distributions, generative models can serve various roles in addressing sequential decision-making problems. We basically classify the functions into three categories.

\begin{itemize}
\item Controller. Generative models can serve as policy models to guide the decision-making process by modeling high-reward trajectories or observation-action pairs. They generate candidate actions based on current observations, refining these candidates to achieve optimal decisions. In this context, sampling from learned distributions is a key step that informs the decision-making process.
\item Modeler. Generative models are capable of capturing the underlying patterns of the dataset and generating new data resembling the original's distribution. By producing samples that capture the data's structure, they support tasks like data augmentation, privacy preservation, and simulation, providing rich inputs for subsequent optimization or decision-making.
\item Optimizer. Generative models can explore high-dimensional spaces and optimize solutions by iteratively refining sampled candidates. They generate and evaluate samples to navigate towards better solutions, helping to optimize decision-making processes or high-reward outcomes. 
\end{itemize}

\textbf{Structure.} 
The third dimension illustrates the basic model structure of generative models. Below is the structure for each category of generative models.

\begin{itemize}
    \item EBM: EBM 
    \item GAN: GAN / WGAN / DCGAN / C-GAN. 
    \item VAE: VAE / CVAE / seq2seq CVAE / $\beta$ VAE / MAE.
    \item NF: NF / Coupling Flow / Autoregressive Flow / Continuous Flow. 
    \item DM: DDPM / DDIM / LDM / EDM. 
    \item GFN: GFN / CFN. 
    \item AM: Decoder Only / Encoder Decoder. 
\end{itemize}

\textbf{Expertise.} 
The fourth dimension represents the expertise areas of the generative models in the sequential decision making. The expertise area includes Imitation Learning, online RL, offline RL, Robotics, Generation and Others. 

\textbf{Application.} 
The last dimension highlights several representative applications where generative models have shown particular promise, including robot control, autonomous driving, gaming, structural generation, and optimization tasks.  


\section{Survey: Methodology}
\label{sec:sec4}

This section delves into the three core functions of generative models and organizes the existing literature based on the methodology taxonomy we propose.

\subsection{Generative Models as Controller}

A controller aims to find a policy that modifies the system's inputs to attain the desired behavior~\cite{hazan2020nonstochastic,chen2021black}. Generative models excel at creating control policies by modeling decision-making and generating diverse strategies. They learn from expert trajectories, capturing complex data distributions to extract optimal policies.

\subsubsection{EBMs as Controller}

EBMs serve as controllers by solving the Inverse Reinforcement Learning (IRL) problems, where the goal is to derive a control policy from expert demonstrations. In this context, the EBMs learn the underlying reward structure of the expert's behavior.
EBIL \cite{liu2021energy} refines the IRL process by estimating the expert's energy with score matching, using it as a surrogate reward function to learn the policy through RL algorithms. Similarly, in \cite{haarnoja2017reinforcement}, EBMs model policies in RL by incorporating an energy function that defines the desirability of actions in a given state. The goal is to minimize energy, guiding the agent toward optimal policies. Unlike traditional RL methods, this approach does not rely on predefined rewards but instead uses an energy-based framework for more flexible and scalable policy learning.

\subsubsection{GANs as Controller}

Generative Adversarial Imitation Learning (GAIL) \cite{ho2016generative} combines imitation learning with GAN to directly extract the policy from the data. It seeks to find a saddle point $(\pi, D)$, representing a pair consisting of a policy $\pi$ and a discriminator $D$, by optimizing the following expression:
\begin{equation}
    \mathop{\mathbb{E}}_{\pi} [ \log(D(s,a)) ] + \mathop{\mathbb{E}}_{\pi_E} [ \log(1-D(s,a)) ] - \lambda H(\pi),
\end{equation}
where $(s, a)$ denotes the state-action pair, $\pi$ is the policy, $\pi_E$ represents the expert policy and $H$ is a regularizer. 

Several extensions have been proposed to address GAIL’s limitations. InfoGAIL \cite{li2017infogail} extends GAIL by unsupervisedly learning the structure of expert demonstrations, allowing it to imitate complex behaviors and extract interpretable representations. FAGIL \cite{geiger2022fail}, another extension, adds a safety layer to ensure policy safety while enabling closed-form computation of densities and gradients, combining adversarial training with worst-case safety guarantees for both safety and performance.

\begin{figure}[ht]
\centering
\includegraphics[width=0.85\columnwidth]{figures/AugAIRL.png}
\caption{The augmented AIRL learning framework uses the discriminator's weights as the reward function \cite{wang2021decision}.}
\label{fig:AugAIRL}
\end{figure}

Other related approaches include Adversarial Inverse Reinforcement Learning (AIRL) \cite{fu2018learning}, which frames policy learning as an adversarial reward learning problem for diverse environments. AugAIRL \cite{wang2021decision} enhances AIRL by incorporating semantic rewards, while WAIL \cite{xiao2019wasserstein} connects inverse reinforcement learning with optimal transport for large-scale applications. Model-based AIRL (MAIRL) \cite{sun2021adversarial} improves policy optimization by using a self-attention dynamics model for differentiable computation, and SC-AIRL \cite{xiang2024sc} reduces exploration by breaking long-horizon tasks into subtasks with shared rewards and critics.

\subsubsection{VAEs as Controller}
   
VAEs serve as controllers by learning compact, structured latent spaces that encode high-dimensional environments and decision-making processes, which are directly used to generate control actions.

For example, Play-LMP \cite{lynch2020learning} uses a self-supervised method to learn control from play behaviors, allowing autonomous policy generation. TACO-RL \cite{rosete2023latent} extends this by adopting a hierarchical model for long-horizon policies from high-dimensional camera observations. OPAL \cite{ajay2020opal} applies VAE for offline learning of primitive actions in imitation tasks. GTI \cite{mandlekar2020learning} uses VAE in a two-stage imitation learning algorithm to improve generalization in complex environments. HULC \cite{mees2022matters} enhances performance by integrating hierarchical decomposition in robot control with a multimodal transformer encoder and discrete latent plans.

MaskDP \cite{liu2022masked} introduces masked autoencoding to enhance decision-making models by learning latent spaces in a scalable, generalizable way, improving performance in high-dimensional tasks. This approach, similar to VAEs, generates control policies through efficient latent representations while incorporating masking for better task generalization.

\subsubsection{Normalizing Flows as Controller}

Normalizing flows are gaining attention for their ability to learn complex probability distributions and perform efficient density estimation. While mainly used in generative modeling, they are also appliable to decision-making control tasks \cite{sohn2015learning, dinh2016density}. In this context, normalizing flows can model and approximate the decision space underlying \cite{papamakarios2021normalizing, kobyzev2020normalizing}.

A common application of normalizing flows for controllers is behavioral cloning, where the target distribution $\mu$ represents the behavior to replicate. By training a reversed normalizing flow, where $\nu$ is a complex distribution with defined density and $\mu$ is a simpler distribution like a normal distribution, the training process becomes feasible.

Normalizing Flow Policy (NF Policy) \cite{ward2019improving} integrates Normalizing Flows with the Soft Actor-Critic (SAC) framework, enabling more expressive policy learning. Conservative Normalizing Flows (CNF) \cite{akimov2022let} uses NFs as a conservative action encoder for learning policies in latent action spaces. The action encoder is first pre-trained on an offline dataset, followed by policy learning through reinforcement learning. Guided Flows \cite{zheng2023guided} generates plans in offline reinforcement learning by training Continuous Normalizing Flows with classifier-free guidance to regress vector fields.

\subsubsection{Diffusion Models as Controller}

Diffusion models, known for their iterative refinement and controlled noise perturbation processes, have shown to be effective in sequential decision-making tasks. These models excel in transforming noisy inputs into structured outputs, making them well-suited as controllers.

There are two main diffusion-based frameworks for decision-making. The first generates subsequent actions based on historical observations, capturing temporal dependencies in sequential tasks \cite{pearce2023imitating}. The second models the entire trajectory of the decision-making process, capturing the broader decision path over time.

Diffusion models can represent the distribution of observation-action pairs as a policy. For example, Diffusion Q-learning (Diffusion-QL) \cite{wang2023diffusion} uses a conditional diffusion model as its policy. It learns an action-value function and optimizes actions by minimizing the following loss function: 
\begin{equation}
    L(\theta) = \mathbb{E}_{i, \epsilon, (s, a) \sim D}[|| \epsilon - \epsilon_{\theta}(a, s, i) ||^2].
\end{equation}

SfBC \cite{chen2023offline} breaks the policy into two components: a generative behavior model to capture the data distribution and an action evaluation model to assess action quality. DIPO \cite{yang2023policy} applies a diffusion policy for model-free, online reinforcement learning in complex environments.

Diffusion models can also describe the joint distribution of trajectories. Diffuser \cite{janner2022planning} optimizes trajectories through iterative refinement with a diffusion probabilistic model. In this framework, a trajectory is represented as $\mathbf{\tau} =  \begin{bmatrix} s_0 & s_1 & \cdots & s_T\\ a_0 & a_1 & \cdots & a_T \\ \end{bmatrix} $, where $T$ denotes the time horizon. During training, Diffuser minimizes a diffusion loss to optimize the trajectory generation process:
\begin{equation}
    L(\theta) = \mathbb{E}_{i, \epsilon, \mathbf{\tau}^{0}}[|| \epsilon - \epsilon_{\theta}(\tau^{i}, i) ||^2].
\end{equation}

Diffuser uses classifier-guided sampling and image inpainting for planning strategies. Decision Diffuser \cite{ajay2023is} treats decision-making as conditional generative modeling, using a return-conditioned diffusion model (see Fig. \ref{fig:decisiondiffuser}). This approach allows for incorporating constraints and skills into the generated behaviors. AdaptDiffuser \cite{liang2023adaptdiffuser} uses reward gradients and a discriminator to guide synthetic expert data generation for goal-conditioned tasks, further finetuning the diffusion model.

Diffusion models are applied in diverse decision-making contexts. Decision Stacks \cite{zhao2024decision} deconstructs goal-conditioned policies into three generative modules that simulate the evolution of observations, rewards, and actions in parallel. Diffusion Policy \cite{chi2023diffusion} simulates robotic policies by converting visuomotor policies into a denoising diffusion process conditioned on inputs. UniPi \cite{du2024learning} frames sequential decision-making as a video generation task conditioned on text, where control actions are derived from a text-encoded goal. Pearce et al. \cite{pearce2022imitating} uses diffusion models to imitate human behavior, abstracting the distribution over the joint action space.

\begin{figure}[H]
\centering
\includegraphics[width=0.85\columnwidth]{figures/DecisionDiffuser.pdf}
\caption{Planning with decision diffuser via generative modeling and inverse dynamics \cite{ajay2023is}.}
\label{fig:decisiondiffuser}
\end{figure}

\subsubsection{GFlowNets as Controller}

GFlowNet (GFN) is a powerful tool for sequential sampling, particularly in Markov Decision Processes (MDPs). It learns a stochastic policy where the probability of a terminal state $s_f$ is proportional to a reward function $R(s_f)$. GFlowNet is typically applied to discrete MDPs, but it is not limited to acyclic structures and can also be used in other types of MDPs, including those with more general graph structures.


GFlowNet \cite{bengio2021flow, bengio2021gflownet} is suitable for scenarios with multiple trajectories leading to the same terminal state. It transforms these trajectories into a flow and uses flow consistency equations as the learning objective, similar to the Bellman equations in Temporal Difference (TD) methods. A global minimum of this objective results in a policy that samples from the desired distribution, improving performance and diversity. Stochastic GFlowNet \cite{pan2023stochastic} extends GFlowNet to stochastic environments, capturing environmental randomness. GAFlowNets \cite{pan2022generative} adds intermediate rewards to address exploration challenges in sparse reward environments. EFlowNets \cite{jiralerspong2023expected} further adapts GFlowNets for such environments. Brunswic et al. \cite{brunswic2024theory} generalize GFlowNets to measurable spaces, including continuous state spaces and cycles.


\begin{figure}[H]
\centering
\includegraphics[width=0.85\columnwidth]{figures/cflownet.pdf}
\caption{Overall framework of CFlowNets~\cite{li2023cflownets}: environment interaction, flow estimation, and training.}
\label{fig:cflownet}
\end{figure}

CFlowNet \cite{li2023cflownets} advances GFlowNet by extending its use to continuous control tasks (see Fig. \ref{fig:cflownet}), making it applicable to real-world scenarios. It introduces a unified framework combining action selection, flow approximation, and optimization objectives. Experimental results show CFlowNets outperform traditional reinforcement learning methods, especially in exploration. Continuous GFlowNet \cite{lahlou2023theory} further develops the theoretical framework for continuous control.

\subsubsection{Autoregressive Models as Controller}

Autoregressive models are effective in decision-making tasks by generating outputs sequentially based on past observations. Decision Transformers (DTs) \cite{chen2021decision} reformulate RL as a conditional sequence modeling problem. They use a causally masked Transformer to generate optimal actions that maximize expected rewards, conditioned on past states, actions, and rewards. The Online Decision Transformer \cite{zheng2022online} combines offline pretraining with online fine-tuning, using sequence-level entropy regularizers and autoregressive objectives. DADT \cite{kim2022dynamics} enhances generalization by predicting the next state through representation learning. Trajectory Transformer \cite{janner2021offline} extends this idea by modeling reinforcement learning as sequence modeling over entire trajectories. It uses a Transformer to model trajectory distributions and employs beam search for planning to generate high-reward sequences. BooT \cite{wang2022bootstrapped} further enhances this by generating self-supervised offline data to improve model training.

Pareto-Efficient Decision Agents (PEDA) \cite{zhu2023scaling} extends Decision Transformers \cite{chen2021decision} and RvS \cite{emmons2021rvs} for multi-objective reinforcement learning, introducing a preference- and return-conditioned policy for efficient multi-objective handling. With the rise of large-scale language models \cite{brown2020language}, Transformer-based controllers, such as GATO \cite{reed2022generalist}, have emerged as versatile agents capable of handling multi-modal, multi-task, and multi-embodiment scenarios. RT-1 \cite{brohan2022rt} builds on this by training a scalable model on real-world tasks, demonstrating performance improvements with larger data and model sizes.

\begin{figure}[H]
\centering
\includegraphics[width=0.85\columnwidth]{figures/decision_transformer.pdf}
\caption{Decision transformer architecture\cite{chen2021decision}. }
\label{fig:decisiontransformer}
\end{figure}

\subsection{Generative Models as Modeler}

Unlike methods like S4RL \cite{sinha2022s4rl} or RAD \cite{laskin2020reinforcement}, which perturb inputs to maintain consistency, generative models focus on learning the underlying patterns within data to generate new data that mirrors the original distribution \cite{ashuach2023multivi}.

As modelers, generative models analyze input data to capture its inherent structures and relationships \cite{thomas2023integrating}. They then use this understanding to generate new data, such as images or text, that resemble the original dataset. For example, DALL·E \cite{ramesh2021zero} create images with similar style and content to the training data. Beyond image and text generation, they are also useful for synthetic data generation in tasks like data augmentation, privacy protection, and simulations.

\subsubsection{EBMs as Modeler}

EBMs model data using an energy function that captures its underlying structure, without relying on explicit probability distributions. Instead, they describe data through an energy landscape, and once trained, they generate new data by minimizing the energy, aligning generated samples with the learned patterns of the original dataset.

Boltzmann Machines (BMs) \cite{ackley1985learning} use contrastive divergence to generate samples that match the training data's distribution. DEBMs \cite{haarnoja2017reinforcement} extend this by learning a distribution over state-action pairs, allowing reinforcement learning agents to generate and select optimal actions. Score-Based Generative Models (SGMs) \cite{song2019generative} use score matching to learn gradients of the data distribution and generate samples by refining noisy data towards the target distribution.

\subsubsection{GANs as Modeler}

GANs function as models by learning the underlying data distributions through their unique adversarial framework. The generator captures the patterns in the data and uses this knowledge to generate synthetic samples, while the discriminator helps refine these samples by assessing their realism. Over time, this process allows the generator to better model the original data distribution, making it a powerful tool to generate new realistic data that reflect the learned structure of the dataset.

For instance, EGAN \cite{huang2017enhanced} models the relationship between states and actions, enabling the pre-training of agents and accelerating their learning. Similarly, State2Pixel (S2P) \cite{cho2022s2p} generates pixel images from the agent’s state, bridging the gap between state space and image domains in reinforcement learning. S2P also enables virtual exploration of the latent image distribution through model-based state transitions. Thus, GANs excel as modelers by capturing complex data relationships and generating data that mirrors the learned patterns.

\subsubsection{VAEs as Modeler}

VAEs function as modelers by learning a probabilistic mapping from the data space to a latent space, where they capture the underlying variations and structure of the data. Through this process, VAEs represent complex data distributions in a more compact and continuous latent space.

For example, Han \& Kim \cite{han2022selective} use VAEs to model the subspaces of a dataset. The VAE encoder transforms data points into the latent space, and the decoder generates new data by sampling from this learned latent space and mapping it back to the original data space. This capability to model and generate data based on learned distributions allows VAEs to capture the essential features of the data while producing new, similar instances.

\subsubsection{Normalizing Flows as Modeler}

Normalizing Flows (NFs) are powerful modelers for complex, high-dimensional data distributions, using invertible transformations to capture intricate data structures. This flexibility makes them well-suited for tasks such as density estimation, variational inference, and posterior distribution modeling and so on.

For instance, NICE \cite{dinh2014nice} uses a series of invertible transformations, specifically coupling layers, to model high-dimensional data distributions. This approach allows it to transform data into a simpler latent space, where the distribution is easier to model, while maintaining computational efficiency by ensuring the Jacobian determinant is simple to calculate. Rezende and Mohamed \cite{rezende2015variational} extends NF's use in variational inference to model posterior distributions, enabling efficient Bayesian inference. Müller et al. \cite{muller2019neural} introduces piecewise-polynomial coupling transformations, improving NF's capacity to model complex distributions.

\subsubsection{Diffusion Models as Modeler}

Diffusion models generate high-quality data by iteratively denoising random noise into structured data, enabling them to capture complex data distributions and reverse the noise-corruption process, making them effective modelers.

\begin{figure}[H]
\centering
\includegraphics[width=0.85\columnwidth]{figures/MTDiff.pdf}
\caption{Overall architecture of $MTD_{IFF}$. Different colors represent different tasks \cite{he2024diffusion}.}
\label{fig:MTDiff}
\end{figure}

ROSIE \cite{yu2023scaling} applies text-to-image diffusion models to generate valid data for robotic learning, inpainting objects for manipulation and backgrounds with text guidance. GenAug \cite{chen2023genaug} enhances real-world demonstration data by augmenting a small image-action dataset in tabletop pick-and-place tasks. SynthER \cite{lu2024synthetic} upsamples an agent’s experience to train RL agents in both proprioceptive and pixel-based environments. MTDiff \cite{he2024diffusion} introduces a multi-task offline data model using a single diffusion model with Transformer backbones and prompt learning to synthesize data in multitask scenarios.
%$MTD_{IFF}$

\subsubsection{GFlowNets as Modeler}

GFlowNets model complex data distributions by sampling from reward-defined distributions, making them effective for tasks like data augmentation and simulation.

In \cite{zhang2022unifying}, GFlowNets treat sampling as a decision-making process, using Markovian trajectories to model data distributions, unifying various generative models for efficient training and inference. In \cite{zhang2022generative}, energy-based GFlowNets (EB-GFNs) combine GFlowNets with energy functions to generate samples from energy-defined distributions, enhancing probabilistic modeling for high-dimensional discrete data. \cite{zhang2024improving} applies GFlowNets to text-to-image generation, using the Diffusion Alignment with GFlowNet (DAG) algorithm to refine diffusion models and generate high-reward images, addressing alignment issues.

\subsubsection{Autoregressive Models as Modeler}
Autoregressive models (AMs) have been applied to reinforcement learning (RL) and planning, where their ability to model sequential dependencies proves particularly useful. As modelers, AMs learn to predict each future state or action based on the previous ones, capturing complex sequential patterns that are crucial for decision-making tasks. 

For example, ALPINE \cite{wang2024alpine} extends this capability by enabling agents to learn sequential policies, allowing long-term decision making by predicting future actions from past ones, which is especially effective for complex planning tasks where the order of actions matters. Similarly, ARP \cite{korenkevych2019autoregressive} applies AMs in RL to enhance exploration. By generating exploration policies, ARP helps agents predict diverse action trajectories, improving their ability to explore and learn in environments with sparse rewards, thus addressing the exploration-exploitation tradeoff.

\subsection{Generative Models as Optimizer}

Generative models have increasingly been applied to optimization problems by treating them as sampling tasks. This approach enables models to learn data representations that directly optimize objective functions, making them powerful tools for complex optimization \cite{qiao2019defending}. These models can capture complex data distributions and generate efficient, scalable solutions \cite{lattimore2020learning}. As optimizers, generative models offer new perspectives, often outperforming traditional methods in various domains.

\subsubsection{EBMs as Optimizer}

Energy-Based Models (EBMs) can function as effective optimizers by refining candidate solutions toward optimal configurations. Using techniques like contrastive divergence~\cite{hinton2002training} or stochastic gradient descent, EBMs minimize the energy function, offering more flexibility than traditional methods that rely on fixed rules. Methods like MCMC or Langevin dynamics refine solutions iteratively, helping EBMs navigate the solution space efficiently.

EBMs have demonstrated their strength in various optimization tasks. For example, SO-EBM \cite{kong2022end} directly parameterizes optimization problems with differentiable energy functions, making it more accurate and efficient than traditional methods that rely on KKT conditions. Similarly, pcEBM \cite{tagasovska2022pareto}, applied in protein sequence optimization, optimizes multiple objectives by learning a multi-objective energy function, allowing it to simultaneously balance structural stability and sequence diversity. 

\subsubsection{GANs as Optimizer}

GANs have become effective optimizers, tackling high-dimensional and complex problems. Their ability to generate diverse, high-quality solutions allows them to efficiently explore large solution spaces. GANs excel in non-convex optimization, where gradient-based methods struggle, by generating realistic solutions that meet multiple constraints.

% He et al. \cite{he2020evolutionary} demonstrated GANs’ ability to improve multi-objective evolutionary algorithms by enhancing solution diversity and optimality. Sim et al. \cite{sim2021gans} applied GANs in topology optimization, efficiently navigating design spaces. Kalehbasti et al. \cite{kalehbasti2021augmenting} combined GANs with genetic algorithms to accelerate optimization, even with limited data, highlighting GANs' versatility across different tasks.

He et al. \cite{he2020evolutionary} showed how GANs improve multi-objective evolutionary algorithms by generating high-quality offspring, enhancing both diversity and optimality, even with limited data. Sim et al. \cite{sim2021gans} applied GANs and DCGANs in topology optimization for structural design, using K-means clustering to select optimized solutions, demonstrating GANs’ ability to navigate complex design spaces. This highlights their flexibility in engineering and design optimization. Kalehbasti et al. \cite{kalehbasti2021augmenting} combined Conditional GANs (C-GANs) with genetic algorithms to solve high-dimensional optimization problems. By augmenting genetic algorithm solutions with C-GANs, they accelerated the search for optimal solutions, even without large amounts of training data, showcasing GANs' growing role as powerful optimizers in diverse tasks.

\subsubsection{VAEs as Optimizer}

VAEs are effective optimizers due to their ability to learn compact probabilistic representations, which allow efficient exploration of complex decision spaces. By mapping high-dimensional inputs to a simpler latent space, VAEs enhance optimization tasks by reducing computational complexity and improving solution quality.

\begin{figure}[H]
\centering
\includegraphics[width=0.85\columnwidth]{figures/CVAE.png}
\caption{The overall training process of CVAE-Opt (Left) and the iterative search process (Right)\cite{hottung2021learning}.}
\label{fig:CVAE}
\end{figure}

For example, CageBO \cite{xing2023bayesian} uses a conditional VAE to optimize decision-making by mapping decisions to a reduced latent space, allowing for more efficient exploration. This approach improves optimization by generating high-quality solutions and reducing computational costs. Similarly, Conditional VAEs (CVAEs) are applied to complex tasks like routing problems \cite{hottung2021learning}, where they refine the search for feasible solutions by conditioning on problem-specific constraints, improving solution quality in a reduced space (see Fig. \ref{fig:CVAE}).

\subsubsection{Normalizing Flows as Optimizer}

NFs are powerful optimizers due to their ability to transform simple distributions into complex ones, matching the desired solution distribution. This transformation allows NFs to generate high-quality solutions efficiently with fewer iterations compared to traditional methods. Being differentiable, NF can integrate seamlessly with gradient-based optimization for smooth, end-to-end optimization.

For example, Gabrié et al. \cite{gabrie2022adaptive} introduced an adaptive MCMC method that enhances traditional MCMC sampling by incorporating NF-driven non-local transitions. This approach accelerates optimization by efficiently guiding the search for optimal solutions, improving both performance and convergence speed.

\subsubsection{Diffusion Models as Optimizer}

Diffusion models are emerging as effective optimizers, particularly suited for black-box optimization tasks where the objective function is complex or difficult to compute. Their iterative denoising process gradually refines solutions, enabling efficient exploration of solution spaces.

In traditional optimization, gradient-based methods often struggle in high-dimensional and non-convex spaces. In contrast, diffusion models excel by gradually refining solutions, effectively navigating complex problem spaces.

For example, Denoising Diffusion Optimization Models (DDOM) \cite{krishnamoorthy2023diffusion} tackle offline black-box optimization by using reverse diffusion to model complex objective functions, refining solutions through multiple steps. Similarly, Li et al. \cite{li2024diffusion} introduced a reward-directed conditional diffusion model that focuses on promising solution areas by conditioning on predicted rewards, further improving optimization efficiency.

\begin{figure}[H]
\centering
\includegraphics[width=0.85\columnwidth]{figures/alg_demo.pdf}
\caption{Black-box optimization with reward-directed conditional diffusion models \cite{li2024diffusion}.}
\label{fig:optimizer}
\end{figure}

Another approach, DiffOPT \cite{kong2024diffusion}, combines a guided diffusion process with Langevin dynamics to optimize solutions, ensuring both efficient exploration and precise refinement for complex tasks.

\subsubsection{GFlowNets as Optimizer}

GFlowNets have emerged as powerful optimizers that navigate complex solution spaces by generating solutions step-by-step through a learned flow. Their flow-based structure allows for diverse solution generation while maintaining high sample efficiency, crucial for expensive evaluations. GFlowNets perform well with both on-policy and off-policy data, boosting their effectiveness in tasks like combinatorial and multi-objective optimization.

For example, GFlowNets have been applied to combinatorial optimization (CO) problems, often NP-hard, where they effectively find high-quality solutions by sampling from the solution space \cite{zhang2024let}. Additionally, the extension to multi-objective optimization with MOGFNs \cite{jain2023multi} enables the generation of diverse Pareto-optimal solutions, handling trade-offs between conflicting objectives.

A hybrid approach, Generative Flow Ant Colony Sampler (GFACS) \cite{kim2024ant}, combines GFlowNets with Ant Colony Optimization (ACO), further improving efficiency in combinatorial optimization by incorporating off-policy training and local search strategies.

\subsubsection{Autoregressive Models as Optimizer}

Autoregressive models are effective optimizers due to their sequential structure, which captures dependencies between data points, allowing for iterative solution refinement. This ability helps them handle uncertainty and improve optimization performance across various tasks. Their sequential nature enables them to generate or adjust solutions gradually, making them suitable for complex, high-dimensional solution spaces.

For example, Transformer Neural Processes (TNPs) \cite{nguyen2022transformer} use a transformer-based autoregressive model within a meta-learning framework to optimize performance in uncertain tasks with variable inputs. By combining transformers’ strengths in sequence modeling with Neural Processes for uncertainty estimation, TNPs refine predictions and learn optimal strategies iteratively. (see in Fig. \ref{fig:TNPs})

Similarly, BONET \cite{mashkaria2023generative} introduces a generative framework using autoregressive models to pretrain optimization on offline datasets. The transformer-based model learns to optimize task performance in real-world settings without task-specific supervision, enabling adaptation to a wide range of optimization problems.

\begin{figure}[ht]
\centering
\includegraphics[width=0.85\columnwidth]{figures/TNPs.png}
\caption{Illustration of the TNP-A architecture \cite{nguyen2022transformer}.}
\label{fig:TNPs}
\end{figure}

\section{Applications}
\label{sec:sec5}
There are lots of applications of generative models in decision making scenarios. We consider five typical applications including: robot control, autonomous driving, games, structural generation, and optimization.

\subsection{Robotic Control}

Robotic control refers to the process of commanding a robot to execute specific tasks or actions \cite{duan2016benchmarking, levine2016end, bailo2018optimal, cartea2018algorithmic, wang2020continuous}. This can be achieved through a variety of methods, including manual control by a human operator, pre-programmed instructions, or autonomous decision-making using sensors and machine learning algorithms.

Policy generation in robotics refers to creating the decision-making framework that guides a robot’s actions \cite{hofer2021sim2real, chenunderstanding, hu2022provable}.  It entails defining objectives or goals for the robot while incorporating constraints and limitations to ensure safe and efficient behavior.

Effective policy generation for robots requires consideration of various factors, such as the robot's capabilities, limitations, operating environment, and task requirements. This often involves designing sophisticated algorithms and decision-making models capable of real-time adaptation to dynamic conditions.

Robot control generally encompasses several key aspects, including robot manipulation \cite{xu2019adversarially}, trajectory generation \cite{xu2019learning, zhang2023diffusion, malkin2022trajectory}, and locomotion \cite{margolis2024rapid}. Generative models play a pivotal role in either directly controlling robots or generating synthetic data to enhance the training of more effective control policies.

\begin{figure*}
\centering
\includegraphics[width=\textwidth]{figures/barchart.png}
\caption{The bar chart illustrates the continuous growth of generative models in decision-making across various applications, as indicated in the legend. The methods highlighted in the figure represent the
seminal works of each period.}
\label{fig:applications}
\end{figure*}

\subsection{Structural Generation}
Generative models are widely used in graph-based tasks, such as graph generation \cite{li2018multi}, completion \cite{grover2019graphite}, and classification \cite{zhong2005generative}. These models learn the structure of training graphs and generate new ones with similar characteristics, which has applications in fields like molecule design, protein interaction modeling, and architecture optimization.

One key example is the use of Generative Flow Networks (GFlowNets) for structure generation. Jain et al. \cite{jain2022biological} introduced an active learning framework with GFlowNets to generate diverse candidate solutions for drug discovery. Kim et al. \cite{kim2023local} enhanced exploration by incorporating a local search strategy, combining forward and backward policies to improve solution quality.

Recent work integrating reinforcement learning (RL) with natural language processing (NLP) has further advanced structural generation for decision-making. Xin et al. \cite{xin2022rethinking} proposes Prompt-Based Reinforcement Learning (PRL) to optimize decision-making tasks by using state-reward pairs as prompts. Deng et al. \cite{deng2022rlprompt} introduces RLPROMPT, an RL-based method for prompt optimization in pre-trained language models. Zhang et al. \cite{zhang2023tempera} extends this with TEMPERA, designing optimal prompts for zero-shot and few-shot learning.

These advances demonstrate how RL-enhanced generative models improve decision-making in NLP, and their potential extends to areas like robotics and planning.

\subsection{Games}
Games AI is a key research area aimed at developing AI systems that can perform at human-level across various gameplay types \cite{shao2019survey}. Games offer complex, controlled environments with challenges ranging from strategic planning to real-time decision-making, making them ideal benchmarks for AI development.

For single-player games, generative models have been increasingly applied~\cite{li2024unbounded}. The Multi-Game Decision Transformer \cite{lee2022multigame} utilizes a transformer-based model to efficiently handle various game scenarios, highlighting the flexibility of generative approaches for diverse gaming tasks. Another generalist agent, GATO \cite{reed2022generalist}, is capable of playing a variety of games, demonstrating the potential for generative models to adapt across multiple player environments.

In multiplayer games~\cite{omirgaliyev2024simulating, park2023generative,jiralerspong2023expected,park2023generative}, Adversarial Flow Network (AFlowNet) \cite{jiralerspong2023expected} extends EFlowNets to adversarial two-player zero-sum games. On the other hand, Generative Agents \cite{park2023generative} use generative models to simulate human-like behaviors and interactions among multiple agents in dynamic virtual environments, enhancing the complexity and depth of multiplayer experiences.

\subsection{Autonomous Driving}

Generative models have shown great potential in autonomous driving, particularly in driving control \cite{chen2018parallel, wang2021decision, si2019agen}, object detection \cite{marathe2023wedge}, and scene understanding \cite{arnelid2019recurrent}. They are especially effective in driving decision-making, generating complex policies from data. Huang et al. \cite{huang2021learning} proposed a hybrid framework that combines neural decision-making with classical pipelines, enabling dynamic decisions while adhering to physical constraints.

A major challenge in autonomous driving is the lack of diverse training data. Generative models address this by producing synthetic data to simulate real-world traffic scenarios. For example, TrafficGen \cite{feng2023trafficgen} uses an autoregressive model to generate long-term driving trajectories, helping overcome data scarcity, especially in edge cases.

Generative models also improve sensor data processing, enhancing object detection and tracking from LiDAR and camera inputs \cite{lateef2021saliency}, by creating 3D environment representations for more accurate real-time decisions.

In scene understanding and path planning, these models generate semantic maps and predict object movements for safe navigation. PlanCP \cite{sun2024conformal} enhances this by using Conformal Prediction (CP) to quantify uncertainty and assess policy robustness across scenarios.

Despite challenges like ensuring data quality and adapting to dynamic real-world conditions, generative models are essential for autonomous driving systems, offering scalable, safe, and robust solutions \cite{ghosh2016sad}.

\subsection{Optimization}
Generative models show significant potential in three optimization settings: black-box optimization, neural architecture search (NAS), and scheduling optimization.

Neural Combinatorial Optimization (NCO) leverages neural networks to learn solution distributions for combinatorial problems \cite{bengio2021machine, zhang2023survey}. This includes autoregressive \cite{khalil2017learning,kool2018attention,kwon2020pomo} and non-autoregressive solvers \cite{joshi2019efficient, fu2021generalize, qiu2022dimes} based on transformer architectures. GFlowNets \cite{zhang2023let} and methods like DIFUSCO \cite{sun2024difusco} apply graph-based diffusion models to solve NP-complete problems.

In black-box optimization, generative models optimize task-specific embeddings through prompt tuning \cite{lester2021power,ding2021openprompt,liu2022p,vu2021spot}, improving model performance. RL-based methods like RLPROMPT \cite{deng2022rlprompt} and TEMPERA \cite{zhang2023tempera} further refine prompt design, enhancing tasks such as classification and few-shot learning.

In NAS, generative models optimize neural network designs to maximize performance. GFlowOut \cite{liu2023gflowout} uses GFlowNets to optimize architectures by learning posterior distributions of dropout masks. GFlowNets are also applied in scheduling optimization, such as compiler scheduling \cite{zhang2023robust}, balancing the diversity and quality of proposed solutions in complex environments.

These optimization settings highlight the versatility of generative models in tackling diverse challenges, offering exciting prospects for further exploration and development.

\section{Perspective: Next Generation Generative Decision Making Models}
\label{sec:sec6}
Despite numerous successes, advancing next-generation generative decision-making models remains a critical challenge. In this section, we highlight three pivotal perspectives to enhance the generative models: high-performance algorithms, large-scale generalized decision-making models, and self-evolving and adaptive generative models. 

\subsection{High performance algorithms}
The growing use of generative models in decision-making highlights an increasing demand for high-performance algorithms capable of addressing the complexities of real-world applications. Future research should focus on improving scalability, efficiency, and adaptability to dynamic environments. Two promising directions are Normalizing Flows and GFlowNets, each with unique strengths and challenges. Normalizing flows excel in modeling uncertainties and supporting decision-making in high-dimensional environments. However, challenges remain, such as designing effective objective functions, improving efficiency in high-dimensional spaces, and reducing training costs. Future work should explore novel architectures and optimization strategies to overcome these limitations. 

GFlowNets offer a probabilistic approach to modeling unnormalized distributions, ideal for generating structured objects like graphs. Unlike VAEs and GANs, which approximate data distributions, GFlowNets sample according to a predefined reward function, ensuring the generated samples follow the desired reward distribution.

By addressing these challenges, both frameworks can advance generative decision-making, enabling more efficient, scalable, and adaptable systems for real-world applications.

\subsection{Large-scale Generalized Decision Making Models}

As artificial intelligence advances, developing large generative models for decision-making becomes essential to handle complex reasoning, integrate diverse knowledge, and offer robust solutions in dynamic, uncertain environments.

Current generative models are often task-specific, which limits their ability to generalize. To address this, there is a need for large-scale, generalized generative decision models that can adapt to diverse, real-world environments.

For instance, web-scale diffusion models for robotics \cite{kapelyukh2023dall,yu2023scaling,gupta2025pre} represent a significant step toward achieving this generalization. These models leverage massive datasets and scalable architectures to learn intricate patterns across diverse domains, making them suitable for generating solutions in complex decision-making tasks. Moreover, vision-language-action (VLA) models \cite{kim2024openvla,black2024pi0,brohan2023rt,li2024cogact} push this generalization further by integrating multimodal data, enabling them to interpret, reason, and act across diverse scenarios. These examples underline the transformative potential of generative models in building adaptive, large-scale decision-making frameworks.

Besides, generative models should be capable of producing diverse solutions to handle ambiguous or multi-objective tasks. This diversity allows them to account for uncertainties, offer alternative strategies, and support robust decision-making, critical for applications in dynamic domains like autonomous systems and optimization.

\subsection{Self-Evolving and Adaptive Generative Models}

The third perspective highlights the importance of self-evolving capabilities in generative models to address dynamic decision-making environments. While current models are often static, real-world scenarios require systems that can continuously adapt, learn in real time, and transfer knowledge across domains.

A crucial aspect of adaptive models is continual learning, enabling them to evolve as new data arrives without forgetting previously learned information. Techniques like incremental training, memory replay, and regularization help mitigate catastrophic forgetting, allowing models to stay accurate and relevant as data distributions change. This adaptability is essential for applications where data patterns shift frequently.

Real-time feedback further enhances adaptability, enabling models to adjust strategies based on immediate outcomes or user interactions. This feature is critical for tasks like autonomous navigation or adaptive recommendation systems, where rapid responses are necessary to maintain high performance.

Cross-domain adaptability also plays a key role, as decision-making often spans multiple areas with distinct challenges. Models that can transfer knowledge across domains can tackle new problems with minimal additional training. Methods like transfer learning and meta-learning help models identify and apply shared structures, making them versatile across various tasks.

Incorporating continual learning, real-time feedback, and cross-domain adaptability will enable generative models to better align with real-world challenges, paving the way for more robust, versatile decision-making systems that can thrive in complex, evolving environments.

\section{Conclusion}
\label{sec:sec7}

This survey systematically examines the applications of generative models in decision-making tasks. We begin by revisiting the problem formulation and comparing traditional and generative model approaches. We then present a taxonomy classifying generative models into seven categories based on their model families, functionalities, structures, and advantages. Next, we explore their core principles and real-world applications. We also outline promising future directions, emphasizing the desired characteristics of next-generation decision-making models. Leveraging their ability to model complex data distributions and generate innovative strategies, we argue that generative models hold transformative potential in decision-making. By synthesizing recent advancements and highlighting key research trajectories, this survey aims to bridge gaps and lay the groundwork for the continued evolution of generative models in decision-making.
    
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,refs}

\end{document}


