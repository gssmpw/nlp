\section{Related Work}
\label{sec:related_work}
Typically, an implicit neural representation (INR) $f_\Theta: \mathcal{E} \to \bb K$ approximates a function \( f : \mathcal{E} \to \bb K \) (with $\bb K = \bb R$ or $\bb C$, and \(\mathcal{E} \subset \mathbb{R}^d\)) from its sampling $\cl D = \{(\bs x_i, y_i = f(\bs x_i))\}_{i=1}^N$ on a set of $N$ points $\{\bs x_i\}_{i=1}^N \subset \cl E$. This is done by \emph{training} the set of parameters $\Theta$ on $\cl D$ so that $f_\Theta$ interpolates $f$, \ie $f_\Theta(\bs x_i) \approx y_i$. An INR combines a positional encoding (PE) composed of $n$ neurons
\begin{equation}
\label{eq:PE-Euclidean-INR}
\psi_{\sin}(\bs x) = \Big(\sin(\bs \omega_i^\top \bs x + b_i)\Big)_{i=1}^n,    
\end{equation}
which extracts $n$ Fourier features from the input coordinates \(\bs{x} \in \mathcal{E}\), with a multilayer perceptron (MLP) whose activations are typically ReLU or sine wave.  Frequencies $\bs \omega_i$ and bias $b_i$ in the PE can be either considered as trainable weights 
or generated randomly, leading then to the so-called Random Fourier Features~\cite{Rahimi2007RandomFF}.
This periodic positional encoding layer of the INR network is key for accurate representations of a signal~\cite{tancik_fourier_2020} as it enables the representation of high-frequency signal features. 
The final architecture of an INR reads
\begin{equation}
\label{eq:INR}
f_{\Theta}(\bs x) := \bs{W}^{(Q)} (\phi^{(Q-1)}\circ \cdots \circ \phi^{(1)})\left(\psi(\bs x)\right),
\end{equation}
with the PE $\psi = \psi_{\sin}$, $\phi^{(q)}(\cdot) := \sigma^{(q)} (\bs{W}^{(q)} \cdot + \bs{b}^{(q)})$, $q=1,\ldots,Q-1$, and where each layer \( q \) is characterized by a weight matrix \( \bs{W}^{(q)} \in \bb{R}^{d_{q} \times d_{q-1}} \), a bias vector \( \bs{b}^{(q)} \in \bb{R}^{d_q} \), and an activation function \( \sigma^{(q)}: \bb{R} \rightarrow \bb{R} \) applied element-wise. 

Moreover, these architectures benefit from a comprehensive understanding of their underlying spectral properties~\cite{yuce2022structureddictionaryperspectiveimplicit}, namely that the frequency support of the initial positional encoding entirely governs the expressive capacity of the model. 
In addition, assuming the activation function admits a Taylor series expansion, 
each layer in the network expands the frequency spectrum by the order of the activation functionâ€™s expansion. For networks employing sinusoidal activation functions (\eg SIRENs \cite{sitzmann2020implicitneuralrepresentationsperiodic}) this expansion is theoretically infinite due to the infinite Taylor series, while in practice most of the coefficients of the polynomial expansion decay at a factorial rate so that some terms become negligible up to some order.   


As they do not consider the spherical geometry, conventional INRs, such as SIRENs, can generate singular spherical functions prone to sampling artifacts, particularly at the poles. In \cite{russwurm2024locationencoding}, the authors addressed these limitations by introducing SPH-SIREN; a SIREN network integrating spherical harmonics (SH) $Y_{\ell m}(\theta,\varphi)$ with order $\ell \geq 0$ and moment $|m|\leq \ell$ \cite{courant62} into its PE, \ie they set the PE $\psi = \psi_{\rm SH}$ in~\eqref{eq:INR} with
\begin{equation*}
\bs x(\theta, \varphi)\in \bb S^2 \mapsto \psi_{\rm SH}(\bs x) = \{ {Y}_{\ell m}(\theta, \varphi) : |m|\leq \ell\}_{\ell=0}^{L_0}.
\end{equation*}
Our investigation revealed that, for orders above $L_0>25$, accurate closed-form expressions of SH in SPH-SIREN become impractical, while possible recursive estimation methods require high-precision arithmetic, limiting scalability.