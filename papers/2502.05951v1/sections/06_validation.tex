\section{Validation}
\label{sec:validation}
%include la validazione dei risultati del modello, la generazione del dataset con analisi delle sue carattertistiche e considerazioni sulla bontà
The evaluation involved a series of tests designed to assess the model’s performance in classifying emails and identifying phishing features. The tests were structured to incrementally refine the model’s prompt and configuration presented in Section~\ref{sec:lisa} and iteratively analyze how changes affected outcomes. Key metrics such as the number of false positives, number of false negatives, Precision, Recall, and F1-score were computed to measure performance quantitatively.

\begin{itemize}
    \item \textbf{False Positives (FP)}: The number of safe emails incorrectly classified as phishing. This metric reflects instances where the model raises unnecessary alarms, causing inconvenience or mistrust.
    \item \textbf{False Negatives (FN)}: The number of phishing emails incorrectly classified as safe. This is critical as it represents missed detections, allowing potential threats to go unnoticed.
    \item \textbf{Precision}: The proportion of correctly identified phishing emails out of all emails the model classified as phishing. It measures the model's accuracy in predicting phishing emails and is calculated as: \[
    \text{Precision} = \frac{TP}{TP + FP}
    \]
    \item \textbf{Recall}: The proportion of correctly identified phishing emails out of all actual phishing emails. It assesses the model's ability to detect all phishing attempts and is calculated as:     \[
    \text{Recall} = \frac{TP}{TP + FN}.
    \]
    \item \textbf{F1-Score}: The harmonic mean of precision and recall, providing a single metric that balances false positives and false negatives. It is calculated as:     \[
    \text{F1-Score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}.
    \]
\end{itemize}

\subsection{Validation Datasets}
\label{sec:valdataset}

To evaluate the model's effectiveness in detecting phishing emails, we required a comprehensive and diverse dataset of phishing emails that accurately reflect real-world phishing strategies. This dataset was meticulously crafted to encompass various topics and persuasion techniques, ensuring heterogeneity and coverage of most phishing scenarios.
\\
To create high-quality phishing emails, we utilized WormGPT V3.0~\cite{b34} since it is an advanced language model fine-tuned for crafting phishing emails (among other supported tasks). To guide the generation of phishing emails that embody the collected semantic features (both in isolation or in combination with others), we have utilized the following prompt:\\ \\
``\textit{Pretend to be a hacker planning a phishing campaign. Your goal is to generate one very detailed phishing email about this topic: \textless TOPIC\textgreater. You have to use fake American real names for the sender and recipient (example: John Smith, Sarah Thompson, Michael Garcia, Daniel Davis, James Wilson, etc) and you have to invent a phishing link URL to include in this email.''
\\ \\
The email must be characterized by the following persuasion technique:\\
\textless FEATURE \textgreater
} \\ \\
With this methodology, a total of 420 phishing emails have been generated, with 20 emails dedicated to each of the 21 identified Cyri semantic features. The emails were crafted to cover a wide array of topics relevant to each feature, enhancing the dataset’s heterogeneity. Each generated phishing email underwent a meticulous manual review by two experts to ensure that each email effectively embodied the specified semantic feature convincingly and eventually manually modified to make them more fitting to the targeted feature(s). Moreover, several additional characteristics for effective phishing attack generation, present in research papers that studied phishing characteristics, were taken into consideration such as Credibility \cite{b25}, Compatibility \cite{b25}, Personalization \cite{b35}, Contextual Relevance \cite{b35}  and Knowledge \cite{b10}, Reputation Exploitation \cite{b10}, Commitment and Consistency \cite{b20} and Liking \cite{b20}.

A meticulous process was undertaken to obtain accurate ground truth for identifying phishing features within these emails. The process involved leveraging ChatGPT-4o, supplemented by manual review and additions, to identify the specific features present in each phishing email.
\\ \\
To balance the dataset with an equal proportion of legitimate e-mails, and in the absence of an appropriate public dataset for them, a safe emails dataset composed of 420 legitimate emails was generated using ChatGPT-4. Prompts were crafted to create authentic, legitimate emails covering various topics, including:
\begin{itemize}
    \item Business Emails: Meeting requests, project updates, performance reviews, team announcements;
    \item Marketing Emails: Product launches, seasonal sales, newsletters;
    \item Personal Emails: Friendly catch-ups, event invitations, thank-you notes, congratulations messages, holiday greetings.
\end{itemize}

The two datasets were then merged into the final one, composed of 840 e-mails, heterogeneous in semantics and tactics, that will be made freely available as a public resource (the generation and curation process allow this step without incurring loss of privacy issues).

\subsection{Validation Results}
\label{sec:quantitativevalidation}

\subsubsection{Validating LLM choice}
In our comprehensive evaluation, we systematically tested the performance of the LLaMA 3.1 8B model to confirm its usage inside the LISA component. We used progressively refined prompts to assess their ability to detect phishing emails accurately.  \\ \\
In the initial phase of our evaluation, we utilized a straightforward prompt that asked whether an email was phishing or safe without providing any semantic features or detailed descriptions to guide the model's reasoning. This test aimed to establish a baseline for the model's inherent ability to classify emails based solely on its pre-trained knowledge and without additional context. The model's performance in this baseline test revealed moderate limitations (see Table~\ref{tab:test1}). There were 80 false positives, where legitimate safe emails were incorrectly classified as phishing, and 70 false negatives, where phishing emails were mistakenly identified as safe. This indicates that the model struggled to accurately differentiate between phishing and safe emails without explicit guidance. No evaluation of semantic features was possible in this case, as the problem was formulated as one of binary classification. No matter what, it confirmed our choice, dictated by security and privacy reasons, that even a small model like LLaMA 3.1 8B was a good base to build on our approach.

\begin{table}[ht]
\centering
\caption{Test 1: Classification Performance Metrics}
\label{tab:test1}
\begin{tabular}{lcccc}
\toprule
& \textbf{Precision} & \textbf{Recall} & \textbf{F1-score} & \textbf{Support} \\
\midrule
\textbf{Safe}       & 0.83 & 0.81 & 0.82 & 420 \\
\textbf{Phishing}   & 0.81 & 0.83 & 0.82 & 420 \\
\midrule
\textbf{Accuracy}   & & & 0.82 & 840 \\
\textbf{Macro Avg}  & 0.82 & 0.82 & 0.82 & 840 \\
\textbf{Weighted Avg} & 0.82 & 0.82 & 0.82 & 840 \\
\bottomrule
\end{tabular}
\end{table}


\subsubsection{Validating LISA phishing detection}
\label{sec:vallisa}

In the second test, we tried to enhance the model’s performance by incorporating semantic features of phishing emails into the prompt (see Section~\ref{sec:lisa} for details).
Results are visible in Table~\ref{tab:test2}. The number of false positives increased to 33,3\% (140), indicating that more safe emails were incorrectly classified as phishing. Conversely, the false negatives decreased to 9,5\% (40), showing an improvement in the model's ability to detect phishing emails. By introducing semantic features, the model's phishing recall improved slightly. However, this improvement in recall came at the expense of phishing precision, as evidenced by the increase in false positives. The model began over-identifying phishing characteristics in safe emails, leading to more legitimate emails being incorrectly flagged. This trade-off indicates that while the model became more sensitive to phishing indicators, it lacked the ability to adequately distinguish these features in the context of safe emails, highlighting the need for a more balanced approach.

\begin{table}[ht]
\centering
\caption{Test 2: Classification Performance Metrics}
\label{tab:test2}
\begin{tabular}{lcccc}
\toprule
& \textbf{Precision} & \textbf{Recall} & \textbf{F1-score} & \textbf{Support} \\
\midrule
\textbf{Safe}       & 0.875 & 0.667 & 0.757 & 420 \\
\textbf{Phishing}   & 0.731 & 0.905 & 0.809 & 420 \\
\midrule
\textbf{Accuracy}   & & & 0.786 & 840 \\
\textbf{Macro Avg}  & 0.803 & 0.786 & 0.783 & 840 \\
\textbf{Weighted Avg} & 0.803 & 0.786 & 0.783 & 840 \\
\bottomrule
\end{tabular}
\end{table}

Building on the previous tests, the third evaluation introduced weighted semantic features to the prompt. We assigned initial weights to each feature to reflect their importance in identifying phishing emails. Additionally, we included definitions of both phishing and safe emails and provided one example of each to guide the model exploiting 1-shot learning into the existing Chain-of-Tought approach. 
This test evidenced very good results in terms of recall (see Table~\ref{tab:test3}), with only two false negatives, indicating it almost perfectly identified all phishing emails. However, the false positives increased substantially to 42,9\% (180), resulting in many safe emails being incorrectly classified as phishing. The significant drop in precision and the high number of false positives indicated a problem with overfitting to the phishing characteristics. This imbalance suggested that the weighting scheme needed refinement to improve precision without compromising recall. 

\begin{table}[ht]
\centering
\caption{Test 3: Classification Performance Metrics}
\label{tab:test3}
\begin{tabular}{lcccc}
\toprule
& \textbf{Precision} & \textbf{Recall} & \textbf{F1-score} & \textbf{Support} \\
\midrule
\textbf{Safe}       & 0.992 & 0.571 & 0.725 & 420 \\
\textbf{Phishing}   & 0.699 & 0.995 & 0.822 & 420 \\
\midrule
\textbf{Accuracy}   & & & 0.783 & 840 \\
\textbf{Macro Avg}  & 0.846 & 0.783 & 0.773 & 840 \\
\textbf{Weighted Avg} & 0.846 & 0.783 & 0.773 & 840 \\
\bottomrule
\end{tabular}
\end{table}

In the final evaluation, we implemented a comprehensive and optimized prompt corresponding to the one utilized by the Cyri system. 
%but without considering the external APIs, sender information, and user's contact list.
To address the issues identified in the previous test, we adjusted the weights assigned to the features, aligning them more appropriately with their actual importance in phishing detection. We also provided multiple examples (3-shot learning) of safe emails to enhance the model's understanding of legitimate email patterns, having identified FP as the most problematic case. Finally, we enhanced the prompt to encourage the model to utilize its inherent reasoning abilities alongside all the detailed information provided, following public heuristics on how to make a prompt-based strategy more effective. This approach allowed the model to perform a more comprehensive analysis by ensuring that the model’s general understanding and language comprehension are utilized initially, potentially capturing nuances that this particular feature-based analysis might miss. 
These changes produced significant improvements in performance metrics. There were 13 false positives and 27 false negatives.

\begin{table}[ht]
\centering
\caption{Test 4: Classification Performance Metrics}
\label{tab:updated-classification-metrics}
\begin{tabular}{lcccc}
\toprule
& \textbf{Precision} & \textbf{Recall} & \textbf{F1-score} & \textbf{Support} \\
\midrule
\textbf{Safe}       & 0.938 & 0.969 & 0.953 & 420 \\
\textbf{Phishing}   & 0.968 & 0.936 & 0.952 & 420 \\
\midrule
\textbf{Accuracy}   & & & 0.952 & 840 \\
\textbf{Macro Avg}  & 0.953 & 0.952 & 0.952 & 840 \\
\textbf{Weighted Avg} & 0.953 & 0.952 & 0.952 & 840 \\
\bottomrule
\end{tabular}
\end{table} 

\subsubsection{Validating LISA phishing semantic features detection}
\label{sec:valsemfeat}

An in-depth validation was also conducted to evaluate the LISA's ability to identify specific phishing features by comparing the list of features found by LISA with the ground-truth list of features previously curated using ChatGPT-4o and manual annotations. The features were categorized based on the percentage of correct identifications into three classes of accuracy and sorted by decreasing accuracy. These results are visible in Figure~\ref{fig:testFeatures}.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.48\textwidth]{figures/testFeatures.PNG}
  \caption{Final configuration for semantic features detection accuracy}
  \label{fig:testFeatures}
\end{figure}

High accuracy rates were observed for critical semantic features fundamental to phishing detection, such as Unsolicited Requests for Personal Information/Financial Transactions, Urgency (Scarcity), Authority/Impersonation of Trusted Entities, Call to Action, and Exclusivity.
In those cases, we rarely foresee doubts from human users (expert or non-expert) about the presence of these features in a suspicious e-mail, but taking advantage of their identification as a severe factor for not trusting the message.
\\
Medium accuracy rates were noted for features like Appeal to Empathy/Altruism, Motivational Language, Assurance of Security, Undesirable Consequences, Curiosity/Vagueness/Mystery, and Sense of Surprise/Confusion.
For these cases, severity is lower, and contextual factors may help a non-expert user assess the genuine or malicious nature of the messages. No matter what, Cyri alerts them, helping the human user confirm or deny their harmful nature through visual inspection and conversation.
\\
Features with lower accuracy rates included Appeal to Values, Social Validation/Social Proof, False Dilemma, Reinforcement of Positive Behavior, and Reciprocation.
We found this result coherent with a situation where these characteristics may also happen in genuine email or more subtle tentative. These areas need to be improved from an automatic detection point of view, and this represents a current limitation of our approach that needs further investigation. Possible mitigations may be providing visual alerts in the absence of these characteristics or providing for these characteristics the information on the degree of confidence of LISA for consideration and further analysis. 

\subsubsection{Overall results discussion}
\label{sec:overallvalidation}

The final model demonstrated a significant improvement in both precision and recall, achieving a high level of accuracy. The balanced weighting of features, comprehensive definitions, and multiple examples contributed to reducing both false positives and false negatives. The feature identification analysis revealed that the model was highly effective in detecting critical phishing features. High accuracy rates for key features affirm the model's effectiveness in accurately identifying phishing emails. Features with lower accuracy rates were less crucial for phishing detection, and their misidentification did not substantially impact the overall performance. However, these features might benefit from well-structured fine-tuning to enhance the model's comprehensiveness and explanatory capabilities.
