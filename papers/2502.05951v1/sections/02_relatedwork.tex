\section{Related Work}
\label{sec:rel}
Phishing detection is a critical area of cybersecurity research due to the increasing sophistication of phishing attacks and their significant impact on individuals and organizations.\\
One of the earliest and most straightforward methods for phishing detection involves the use of blacklists-based and rule-based approaches~\cite{b2}. The limitations of these traditional phishing detection methods are that they depend on known patterns, signatures, or previously identified malicious entities, making them inadequate for detecting new or evolving threats that do not match existing criteria \cite{b13}.
To overcome these limitations, researchers have increasingly turned to Machine Learning (ML)~\cite{b12, b15, b16, b18} and, more recently, Deep Learning (DL)~\cite{b14, b40} techniques.\\
Machine learning methods involve algorithms that classify emails, URLs, or websites as phishing or legitimate based on extracted features. Deep Learning Approaches involves training neural networks capable of automatic feature extraction from raw and unstructured data. These approaches aim to learn patterns from data, enabling the detection of previously unseen phishing attempts by generalizing from known examples.\\
Although many ML and DL approaches achieve strong performance, they primarily provide mostly statistical detection outcomes without explaining the reasoning behind them in a human-comprehensible form and focus on technical characteristics of the phishing attempts, struggling to identify the semantic and contextual subtleties of more complex phishing attempts~\cite{b24}. Understanding the reasoning behind their predictions is difficult, which can hinder trust and adoption, especially in critical security applications. The lack of transparency poses challenges in validating model decisions, diagnosing errors, and complying with regulatory requirements that may mandate explainability in automated decision-making systems.
Finally, in all these cases, the provided explanation is targeted at only expert users, neglecting most of the technology-unsavvy population, as Cyri does.

Very recently, Large Language Models (LLMs) have emerged as powerful tools in the field of Natural Language Processing (NLP)  due to their remarkable ability to comprehend and generate human-like text~\cite{b38}. LLMs can comprehend context, summarize complex texts, answer questions, and engage in conversations that are contextually appropriate. Researchers have begun to explore how LLMs can enhance the identification of phishing emails by leveraging their advanced natural language processing abilities.
While these models offer numerous benefits in various fields, several studies highlight that as LLMs become more accessible, cybercriminals can produce high-quality phishing content at scale~\cite{b24, b25, b26, b35} \c. This implies that incorporating LLMs into detection strategies is crucial to keep pace with the evolving threats. In particular, their advanced language understanding could enable the detection of sophisticated phishing attempts that traditional methods might miss.\\
Greco et al.~\cite{b24} address the growing concern that cybercriminals can efficiently leverage LLMs to craft more convincing phishing emails. The primary objective of Greco et al.â€™s study was to evaluate the effectiveness of traditional ML models in detecting phishing emails crafted by LLMs. By examining whether ML classifiers can distinguish between human-written and LLM-generated phishing emails, the researchers aimed to understand the limitations of current detection approaches and highlight the need for advanced solutions to address this emerging threat, justifying the study of solutions like Cyri. Greco et al. found that while the ML models could achieve moderate accuracy, they struggled to distinguish between human-generated and LLM-generated phishing emails reliably. The research underscores the urgent need to explore new approaches, possibly involving LLMs themselves, to detect sophisticated phishing emails generated by AI.
Moreover, it highlights the need to incorporate human-centric solutions, which means enhancing user interfaces and warnings to better engage users and support them in recognizing phishing attempts, as Cyri does. \\ 
Li et al. approach~\cite{b28} involves constructing a multimodal knowledge graph that includes textual and visual information about legitimate entities. The LLM is used
to extract and interpret references within the email content, while the knowledge graph provides a factual basis for verifying the authenticity of these references. However, maintaining an up-to-date knowledge graph of legitimate entities, especially across diverse industries, would be resource-intensive. Additionally, the proposed study does not provide a system that can be implemented for use in real conditions.
Koide et al. proposed ChatSpamDetector~\cite{b27}, which utilizes LLMs like ChatGPT to analyze email content and provide detailed reasoning for its classifications. However, sending sensitive email data to third-party servers for analysis could raise significant privacy concerns.\\
The most similar contribution to Cyri is a very recent preprint (October 2024) by Desolda et al.~\cite{b39}, which proposes a tool based on OpenAI's GPT-4o to detect phishing emails and generate explanation messages to users about why a specific email is dangerous. While sharing with Cyri the main goal of supporting the human user in managing phishing, there are some key differences with our contribution: first, it focuses on chatGPT-based models, which can be accessed only online, raising privacy concerns when handling sensitive data such as email content. Second, the explanations generated are general and more concerned with the communication levels (through the four levels of warning provided) than with a systematic analysis and detection of phishing features that can be explored and analyzed directly in the email text by expert and non-expert users. Finally, they do not provide a fully local system that is easy to incorporate into existing email clients and accounts like Cyri does.
\\
Despite the advancements in leveraging Large Language Models (LLMs) for phishing detection \cite{b27, b28}, existing solutions exhibit significant limitations related to user engagement and data privacy. Firstly, many of these approaches do not adequately consider the user experience, relying heavily on traditional warning dialogs to alert users to potential threats. Studies have consistently demonstrated that such warning dialogs are often ineffective in alerting users to phishing threats due to two primary issues: a lack of user understanding of the risks involved~\cite{b22} and the phenomenon known as the habituation effect~\cite{b23}. The habituation effect occurs when users are repeatedly exposed to the same visual stimulus, such as generic phishing warnings, leading them to gradually ignore these alerts over time. This desensitization diminishes the effectiveness of security measures, as users may overlook critical warnings and fail to take appropriate action.\\
Secondly, some solutions transmit personal and potentially confidential information over the internet, exposing users to risks associated with data breaches and unauthorized access. 

To address these challenges, we introduce Cyri, an AI-powered conversational assistant that prioritizes both user engagement and data privacy in its design. Cyri employs a user-centered interface that moves beyond generic warning dialogs by providing clear, contextualized explanations of potential phishing threats within the user's email communications. By offering detailed analyses and actionable advice, Cyri enhances user understanding of the risks and encourages proactive behavior in managing suspicious emails. This approach mitigates the habituation effect by engaging users with dynamic and informative content, thereby maintaining their attention and responsiveness to security alerts. 
%Moreover, Cyri ensures data privacy by performing all email analyses locally on the user's machine. This design choice eliminates the need to send sensitive information to external servers, significantly reducing the risk of data exposure and aligning with best practices in data protection.



