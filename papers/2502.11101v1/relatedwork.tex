\section{Related Work}
Recent works in RAG have explored methods to efficiently process and incorporate large contexts. PCW~\cite{Ratner2023ParallelContextWindows} splits few-shot examples or documents into several windows processed in parallel, thereby reducing positional encoding overhead and removing cross-attention between windows. While performance tends to decrease when using more than $3$ windows, \textbf{\textit{CacheFocus}} demonstrates robust performance even with contexts longer than those covered by $3$ windows.

Various modifications to the attention mechanism have been proposed to address limitations in context relevance and distribution. For example, Structured Prompting~\cite{Hao2022structuredpromptingScalingInContext} scales attention values by $1/M$ (where $M$ is the number of windows), although it requires specific in-context learning examples. Similarly, APE~\cite{yang2025apeFasterandLongerContextAugmented} investigates additional scaling factors and temperature reduction, but it does not consider cache positioning based on the semantic relevance between query and document caches. SparseRAG~\cite{Zhu2024AcceleratingInferenceofRetrievalAugmentedGeneration} infers relevance scores to prune less pertinent documents, which requires an additional training process. XL3M~\cite{Wang2024XL3MaTrainingfreeFrameworkforLLMLength} and Superposition Prompting~\cite{Merth2024SuperpositionPromptingImprovingandAccelerating} adopt strategies that split and filter long inputs using probabilistic measures and Bayesian inference, respectively.

In contrast to these approaches~\cite{yang2025apeFasterandLongerContextAugmented, Zhu2024AcceleratingInferenceofRetrievalAugmentedGeneration, Wang2024XL3MaTrainingfreeFrameworkforLLMLength, Merth2024SuperpositionPromptingImprovingandAccelerating}, \textbf{\textit{CacheFocus}} not only repositions caches based on the semantic relevance between query and document caches but also aggregates attention scores in a layer-wise manner, thereby capturing multi-level contextual information and mitigating noise from individual layers, which could lead to improved stability and performance.