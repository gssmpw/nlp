\begin{table}[ht]
    \centering
        \scalebox{0.8}{\begin{tabular}{ll|cc|c}
        \hline
        \textbf{Length (\#doc)} & \textbf{Model} & \textbf{Prefill} & \textbf{Decode} & \textbf{Total}\\
        \hline \hline
                & naive     & 0.174 & 3.268 & 3.442 \\
        2K~(10)  & w/o cache & 0.652 & 3.066 & 3.718 \\
                & w/ cache  & 0.073 & 3.053 & 3.126 \\
                & w/ prune  & \textbf{0.075} & \textbf{2.995} & \textbf{3.070} \\
\hline          & naive     & 0.454 & 4.157 & 4.611 \\
        4K~(20)  & w/o cache & 1.260 & 3.594 & 4.854 \\
                & w/ cache  & 0.107 & 3.566 & 3.673 \\
                & w/ prune  & \textbf{0.095} & \textbf{3.067} & \textbf{3.162} \\
\hline          & naive     & 1.953 & 6.490 & 8.443 \\
        8K~(40)  & w/o cache & 2.549 & 4.764 & 7.313 \\
                & w/ cache  & 0.199 & 4.761 & 4.960 \\
                & w/ prune  & \textbf{0.154} & \textbf{3.476} & \textbf{3.630} \\
        \hline
    \end{tabular}}
    \caption{A table of $100$-token generation time analysis for the \llamasevenb~under varying settings. This indicates prefill, decode and total time for na\"ive without and with cache, and with pruning at input lengths of $2$K, $4$K, and $8$K.}
    \label{tab:cache_time}
\end{table}