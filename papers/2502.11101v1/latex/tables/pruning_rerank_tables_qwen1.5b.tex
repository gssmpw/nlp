\begin{table*}[ht]
    \centering
    \scalebox{0.7}{
    \begin{tabular}{l c | cccc | cccc}
    \toprule
         & & \multicolumn{4}{c|}{\textbf{DPR-based}} & \multicolumn{4}{c}{\textbf{BM25-based}} \\
    \textbf{Method} & \textbf{Layer} & \textbf{R@5} & \textbf{R@10} & \textbf{R@20} & \textbf{R@50} & \textbf{R@5} & \textbf{R@10} & \textbf{R@20} & \textbf{R@50} \\
    \midrule
    \midrule
    \textbf{(Baselines)} & & & & & & & & & \\
    DPR & -- & \textbf{0.7075} & 0.7925 & 0.8575 & \textbf{0.9125} & -- & -- & -- & -- \\
    BM25 & -- & -- & -- & -- & -- & 0.2500 & 0.3488 & 0.4387 & 0.5337 \\
    \midrule
    \textbf{(Pruning Methods)} & & & & & & & & & \\
    Pruning & 4  & 0.6987 & 0.7875 & 0.8488 & 0.9050 & 0.2537 & 0.3425 & 0.4200 & 0.5350 \\
    Pruning & 16 & 0.6937 & 0.7913 & \textbf{0.8600} & \textbf{0.9125} & 0.2700 & 0.3650 & 0.4688 & \textbf{0.6225} \\
    Pruning & 28 & 0.6925 & 0.7850 & 0.8550 & \textbf{0.9125} & 0.2712 & 0.3713 & 0.4850 & \textbf{0.6225} \\
    \midrule
    Pruning + Dynamic Positional Allocation & 4  & 0.6987 & 0.7875 & 0.8488 & 0.9050 & 0.2537 & 0.3425 & 0.4200 & 0.5350 \\
    Pruning + Dynamic Positional Allocation & 16 & 0.6937 & \textbf{0.7925} & 0.8575 & \textbf{0.9187} & 0.2700 & 0.3675 & 0.4713 & 0.5925 \\
    Pruning + Dynamic Positional Allocation & 28 & 0.6937 & 0.7887 & 0.8562 & \textbf{0.9187} & \textbf{0.2800} & 0.3762 & \textbf{0.4913} & 0.5925 \\
    \midrule
    Pruning + Attention-Guided Allocation & 4  & 0.6987 & 0.7875 & 0.8488 & 0.9050 & 0.2537 & 0.3425 & 0.4200 & 0.5350 \\
    Pruning + Attention-Guided Allocation & 16 & 0.6875 & 0.7863 & 0.8500 & \textbf{0.9137} & 0.2625 & 0.3600 & 0.4487 & 0.6025 \\
    Pruning + Attention-Guided Allocation & 28 & 0.6887 & 0.7837 & 0.8512 & \textbf{0.9137} & 0.2737 & \textbf{0.3862} & 0.4900 & 0.6025 \\
    \bottomrule
    \end{tabular}
    }
    \caption{The retrieval performances for DPR-based and BM25-based retrieval under different methods (\S\ref{sec:layer_adaptive_cache_pruning}, \S\ref{sec:adaptive_positional_allocation_strategy}) and layers ($4$, $16$, $28$) on \qwenoneb. Baseline rows (DPR, BM25) indicate retrieval performance without pruning.}
    \label{tab:unified_retrieval_results}
\end{table*}


% \begin{table}[ht]
%     \centering
%         \scalebox{0.7}{\begin{tabular}{lc|cccc}
%         \toprule
%         \textbf{method} & \textbf{layer} & \textbf{R@5} & \textbf{R@10} & \textbf{R@20} & \textbf{R@50} \\
%         \midrule 
%         \midrule
%              & DPR & 0.7075 & 0.7925 & 0.8575 & 0.9125 \\
%         \midrule
%         % Pruning & 4   & \textbf{0.6987} & 0.7875 & 0.8488 & 0.9050 \\
%         %      & 16  & 0.6937 & 0.7913 & \textbf{0.8600} & \textbf{0.9125} \\
%         %      & 28  & 0.6925 & 0.7850 & 0.8550 & \textbf{0.9125} \\
%         % \midrule
%         Pruning & 4   & \textbf{0.6987} & 0.7875 & 0.8488 & 0.9050 \\
%               & 16  & 0.6937 & \textbf{0.7913} & \textbf{0.8600} & \textbf{0.9125} \\
%               & 28  & 0.6925 & 0.7850 & 0.8550 & \textbf{0.9125} \\
%         \midrule
%         Pruning + Dynamic Positional Allocation & 4   & \textbf{0.6987} & 0.7875 & 0.8488 & 0.9050 \\
%              & 16  & 0.6937 & \textbf{0.7925} & \textbf{0.8575} & \textbf{0.9187} \\
%              & 28  & 0.6937 & 0.7887 & 0.8562 & \textbf{0.9187} \\
%         \midrule
%         Pruning + Attention-Guided Allocation & 4   & \textbf{0.6987} & \textbf{0.7875} & 0.8488 & 0.9050 \\
%              & 16  & 0.6875 & 0.7863 & 0.8500 & \textbf{0.9137} \\
%              & 28  & 0.6887 & 0.7837 & \textbf{0.8512} & \textbf{0.9137} \\
%         \bottomrule
%     \end{tabular}}
%     \caption{DPR 기반 실험 설정에 따른 재순위화 성능. Qwen2-1.5B}
%     \label{tab:rerank_800_dpr}
% \end{table}

% \begin{table}[ht]
%     \centering
%         \scalebox{0.7}{\begin{tabular}{lc|cccc}
%         \toprule
%         \textbf{method} & \textbf{layer} & \textbf{R@5} & \textbf{R@10} & \textbf{R@20} & \textbf{R@50} \\
%         \midrule 
%         \midrule
%              & BM25 & 0.2500 & 0.3488 & 0.4387 & 0.5337 \\
%         \midrule
%         Pruning & 4   & 0.2537 & 0.3425 & 0.4200 & 0.5350 \\
%              & 16  & 0.2700 & 0.3650 & 0.4688 & \textbf{0.6225} \\
%              & 28  & \textbf{0.2712} & \textbf{0.3713} & \textbf{0.4850} & \textbf{0.6225} \\
%         \midrule
%         Pruning + Dynamic Positional Allocation & 4   & 0.2537 & 0.3425 & 0.4200 & 0.5350 \\
%                      & 16  & 0.2700 & 0.3675 & 0.4713 & \textbf{0.5925} \\
%                      & 28  & \textbf{0.2800} & \textbf{0.3762} & \textbf{0.4913} & \textbf{0.5925} \\
%         \midrule
%         Pruning + Attention-Guided Allocation & 4   & 0.2537 & 0.3425 & 0.4200 & 0.5350 \\
%                     & 16  & 0.2625 & 0.3600 & 0.4487 & \textbf{0.6025} \\
%                     & 28  & \textbf{0.2737} & \textbf{0.3862} & \textbf{0.4900} & \textbf{0.6025} \\
%         \bottomrule
%     \end{tabular}}
%     \caption{BM25 기반 실험 설정에 따른 재순위화 성능 Qwen2-1.5B}
%     \label{tab:rerank_800_bm25}
% \end{table}