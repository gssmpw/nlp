% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@inproceedings{
zhu2025acceleratingInferenceof,
title={Accelerating Inference of Retrieval-Augmented Generation via Sparse Context Selection},
author={Yun Zhu and Jia-Chen Gu and Caitlin Sikora and Ho Ko and Yinxiao Liu and Chu-Cheng Lin and Lei Shu and Liangchen Luo and Lei Meng and Bang Liu and Jindong Chen},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=HE6pJoNnFp}
}


@inproceedings{yen-etal-2024-long,
    title = "Long-Context Language Modeling with Parallel Context Encoding",
    author = "Yen, Howard  and
      Gao, Tianyu  and
      Chen, Danqi",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.142/",
    doi = "10.18653/v1/2024.acl-long.142",
    pages = "2588--2610",
    abstract = "Extending large language models (LLMs) to process longer inputs is crucial for numerous applications. However, the considerable computational cost of transformers, coupled with limited generalization of positional encoding, restricts the size of their context window. We introduce Cross-Attention to Parallel Encodings (CAPE), a framework that can be applied to any existing decoder-only LLMs for context expansion. CAPE leverages a small encoder to process a long input chunk by chunk and enables the frozen decoder to cross-attend to the additional contexts. CAPE is efficient, generalizable, and versatile: trained with 8K-token documents, CAPE extends the context window of LLaMA-2 to 128K tokens, offering $10\times$ of the throughput with only 1/6 of the memory. CAPE yields strong performance on language modeling and in-context learning. CAPE also excels in retrieval-augmented applications, while existing long-context models degenerate with retrieved contexts. We further introduce a CAPE variant that can extend the context window of instruction-tuned models with only unlabeled data, and showcase its effectiveness on LLaMA-2-Chat, leading to a strong instruction-following model that can leverage very long context on downstream tasks."
}

@misc{sun2024blockattentionefficientrag,
      title={Block-Attention for Efficient RAG}, 
      author={East Sun and Yan Wang and Lan Tian},
      year={2024},
      eprint={2409.15355},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2409.15355}, 
}
@misc{li2024focusllmpreciseunderstandinglong,
      title={FocusLLM: Precise Understanding of Long Context by Dynamic Condensing}, 
      author={Zhenyu Li and Yike Zhang and Tengyu Pan and Yutao Sun and Zhichao Duan and Junjie Fang and Rong Han and Zixuan Wang and Jianyong Wang},
      year={2024},
      eprint={2408.11745},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2408.11745}, 
}

@inproceedings{Vaswani2017selfattention,
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, \L{}ukasz and Polosukhin, Illia},
title = {Attention is all you need},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {6000â€“6010},
numpages = {11},
location = {Long Beach, California, USA},
series = {NIPS'17}
}

@misc{beltagy2020longformerlongdocumenttransformer,
      title={Longformer: The Long-Document Transformer}, 
      author={Iz Beltagy and Matthew E. Peters and Arman Cohan},
      year={2020},
      eprint={2004.05150},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2004.05150}, 
}

@misc{su2023roformerenhancedtransformerrotary,
      title={RoFormer: Enhanced Transformer with Rotary Position Embedding}, 
      author={Jianlin Su and Yu Lu and Shengfeng Pan and Ahmed Murtadha and Bo Wen and Yunfeng Liu},
      year={2023},
      eprint={2104.09864},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2104.09864}, 
}

@article{Press2021trainshorttestlong,
  title={Train short, test long: Attention with linear biases enables input length extrapolation},
  author={Press, Ofir and Smith, Noah A and Lewis, Mike},
  journal={arXiv preprint arXiv:2108.12409},
  year={2021}
}

@article{Tan2023inputlengthshortening,
  title={Input-length-shortening and text generation via attention values},
  author={Tan, Ne{\c{s}}et {\"O}zkan and Peng, Alex Yuxuan and Bensemann, Joshua and Bao, Qiming and Hartill, Tim and Gahegan, Mark and Witbrock, Michael},
  journal={arXiv preprint arXiv:2303.07585},
  year={2023}
}

@article{lu2024encodeonceanddecodeinparallel,
  title={Encode Once and Decode in Parallel: Efficient Transformer Decoding},
  author={Lu, Bo-Ru and Haduong, Nikita and Lin, Chien-Yu and Cheng, Hao and Smith, Noah A and Ostendorf, Mari},
  journal={arXiv preprint arXiv:2403.13112},
  year={2024}
}

@article{hu2024longrecipeRecipeforEfficient,
  title={LongRecipe: Recipe for Efficient Long Context Generalization in Large Language Models},
  author={Hu, Zhiyuan and Liu, Yuliang and Zhao, Jinman and Wang, Suyuchen and Wang, Yan and Shen, Wei and Gu, Qing and Luu, Anh Tuan and Ng, See-Kiong and Jiang, Zhiwei and others},
  journal={arXiv preprint arXiv:2409.00509},
  year={2024}
}

@article{Zhu2023poseEfficientcontextwindow,
  title={Pose: Efficient context window extension of llms via positional skip-wise training},
  author={Zhu, Dawei and Yang, Nan and Wang, Liang and Song, Yifan and Wu, Wenhao and Wei, Furu and Li, Sujian},
  journal={arXiv preprint arXiv:2309.10400},
  year={2023}
}

@article{Li2024extendingContextWindow,
  title={Extending Context Window in Large Language Models with Segmented Base Adjustment for Rotary Position Embeddings},
  author={Li, Rongsheng and Xu, Jin and Cao, Zhixiong and Zheng, Hai-Tao and Kim, Hong-Gee},
  journal={Applied Sciences},
  volume={14},
  number={7},
  pages={3076},
  year={2024},
  publisher={MDPI}
}

@article{Chen2023clexContinuouslengthextrapolation,
  title={Clex: Continuous length extrapolation for large language models},
  author={Chen, Guanzheng and Li, Xin and Meng, Zaiqiao and Liang, Shangsong and Bing, Lidong},
  journal={arXiv preprint arXiv:2310.16450},
  year={2023}
}

@article{Ding2024longropeExtendingllmcontextwindow,
  title={Longrope: Extending llm context window beyond 2 million tokens},
  author={Ding, Yiran and Zhang, Li Lyna and Zhang, Chengruidong and Xu, Yuanyuan and Shang, Ning and Xu, Jiahang and Yang, Fan and Yang, Mao},
  journal={arXiv preprint arXiv:2402.13753},
  year={2024}
}

@article{Wang2024ResonanceropeImprovingcontextlength,
  title={Resonance rope: Improving context length generalization of large language models},
  author={Wang, Suyuchen and Kobyzev, Ivan and Lu, Peng and Rezagholizadeh, Mehdi and Liu, Bang},
  journal={arXiv preprint arXiv:2403.00071},
  year={2024}
}

@article{Peng2023yarnEfficientcontextwindow,
  title={Yarn: Efficient context window extension of large language models},
  author={Peng, Bowen and Quesnelle, Jeffrey and Fan, Honglu and Shippole, Enrico},
  journal={arXiv preprint arXiv:2309.00071},
  year={2023}
}

@article{Levy2024Sametaskmoretokenstheimpact,
  title={Same task, more tokens: the impact of input length on the reasoning performance of large language models},
  author={Levy, Mosh and Jacoby, Alon and Goldberg, Yoav},
  journal={arXiv preprint arXiv:2402.14848},
  year={2024}
}

@article{Huang2024ASurveyonRetrievalAugmentedTextGeneration,
  title={A Survey on Retrieval-Augmented Text Generation for Large Language Models},
  author={Huang, Yizheng and Huang, Jimmy},
  journal={arXiv preprint arXiv:2404.10981},
  year={2024}
}

@article{Gao2023Retrievalaugmentedgeneration,
  title={Retrieval-augmented generation for large language models: A survey},
  author={Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Haofen},
  journal={arXiv preprint arXiv:2312.10997},
  year={2023}
}

@article{Hadi2024Largelanguagemodelsacomprehensivesurvey,
  title={Large language models: a comprehensive survey of its applications, challenges, limitations, and future prospects},
  author={Hadi, Muhammad Usman and Al Tashi, Qasem and Shah, Abbas and Qureshi, Rizwan and Muneer, Amgad and Irfan, Muhammad and Zafar, Anas and Shaikh, Muhammad Bilal and Akhtar, Naveed and Wu, Jia and others},
  journal={Authorea Preprints},
  year={2024},
  publisher={Authorea}
}

@article{Minaee2024Largelanguagemodelsasurvey,
  title={Large language models: A survey},
  author={Minaee, Shervin and Mikolov, Tomas and Nikzad, Narjes and Chenaghlu, Meysam and Socher, Richard and Amatriain, Xavier and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2402.06196},
  year={2024}
}


@article{Veeramachaneni2025LargeLanguageModelsAComprehensiveSurveyonArchitectures,
  title={Large Language Models: A Comprehensive Survey on Architectures, Applications, and Challenges},
  author={Veeramachaneni, Vinod},
  journal={Advanced Innovations in Computer Programming Languages},
  volume={7},
  number={1},
  pages={20--39},
  year={2025}
}


@article{Touvron2023llama2Openfoundation,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}


@article{Kwiatkowski2019Naturalquestionsabenchmark,
  title={Natural questions: a benchmark for question answering research},
  author={Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and others},
  journal={Transactions of the Association for Computational Linguistics},
  volume={7},
  pages={453--466},
  year={2019},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~â€¦}
}

@article{liu2024LostintheMiddle,
    title = "Lost in the Middle: How Language Models Use Long Contexts",
    author = "Liu, Nelson F.  and
      Lin, Kevin  and
      Hewitt, John  and
      Paranjape, Ashwin  and
      Bevilacqua, Michele  and
      Petroni, Fabio  and
      Liang, Percy",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "12",
    year = "2024",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2024.tacl-1.9/",
    doi = "10.1162/tacl_a_00638",
    pages = "157--173",
    abstract = "While recent language models have the ability to take long contexts as input, relatively little is known about how well they use longer context. We analyze the performance of language models on two tasks that require identifying relevant information in their input contexts: multi-document question answering and key-value retrieval. We find that performance can degrade significantly when changing the position of relevant information, indicating that current language models do not robustly make use of information in long input contexts. In particular, we observe that performance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts, even for explicitly long-context models. Our analysis provides a better understanding of how language models use their input context and provides new evaluation protocols for future long-context language models."
}

@inproceedings{
wei2022finetunedLanguageModels,
title={Finetuned Language Models are Zero-Shot Learners},
author={Jason Wei and Maarten Bosma and Vincent Zhao and Kelvin Guu and Adams Wei Yu and Brian Lester and Nan Du and Andrew M. Dai and Quoc V Le},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=gEZrGCozdqR}
}

@misc{hoffmann2022trainingcomputeoptimallargelanguage,
      title={Training Compute-Optimal Large Language Models}, 
      author={Jordan Hoffmann and Sebastian Borgeaud and Arthur Mensch and Elena Buchatskaya and Trevor Cai and Eliza Rutherford and Diego de Las Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark and Tom Hennigan and Eric Noland and Katie Millican and George van den Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero and Karen Simonyan and Erich Elsen and Jack W. Rae and Oriol Vinyals and Laurent Sifre},
      year={2022},
      eprint={2203.15556},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2203.15556}, 
}

@misc{Ratner2023ParallelContextWindows,
      title={Parallel Context Windows for Large Language Models}, 
      author={Nir Ratner and Yoav Levine and Yonatan Belinkov and Ori Ram and Inbal Magar and Omri Abend and Ehud Karpas and Amnon Shashua and Kevin Leyton-Brown and Yoav Shoham},
      year={2023},
      eprint={2212.10947},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2212.10947}, 
}


@misc{Xiao2024EfficientStreamingLanguageModels,
      title={Efficient Streaming Language Models with Attention Sinks}, 
      author={Guangxuan Xiao and Yuandong Tian and Beidi Chen and Song Han and Mike Lewis},
      year={2024},
      eprint={2309.17453},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.17453}, 
}

@misc{Hao2022structuredpromptingScalingInContext,
      title={Structured Prompting: Scaling In-Context Learning to 1,000 Examples}, 
      author={Yaru Hao and Yutao Sun and Li Dong and Zhixiong Han and Yuxian Gu and Furu Wei},
      year={2022},
      eprint={2212.06713},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2212.06713}, 
}


@inproceedings{
yang2025apeFasterandLongerContextAugmented,
title={{APE}: Faster and Longer Context-Augmented Generation via Adaptive Parallel Encoding},
author={Xinyu Yang and Tianqi Chen and Beidi Chen},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=yUC8pU508S}
}

@misc{Zhu2024AcceleratingInferenceofRetrievalAugmentedGeneration,
      title={Accelerating Inference of Retrieval-Augmented Generation via Sparse Context Selection}, 
      author={Yun Zhu and Jia-Chen Gu and Caitlin Sikora and Ho Ko and Yinxiao Liu and Chu-Cheng Lin and Lei Shu and Liangchen Luo and Lei Meng and Bang Liu and Jindong Chen},
      year={2024},
      eprint={2405.16178},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.16178}, 
}

@misc{Wang2024XL3MaTrainingfreeFrameworkforLLMLength,
      title={XL3M: A Training-free Framework for LLM Length Extension Based on Segment-wise Inference}, 
      author={Shengnan Wang and Youhui Bai and Lin Zhang and Pingyi Zhou and Shixiong Zhao and Gong Zhang and Sen Wang and Renhai Chen and Hua Xu and Hongwei Sun},
      year={2024},
      eprint={2405.17755},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.17755}, 
}

@misc{Merth2024SuperpositionPromptingImprovingandAccelerating,
      title={Superposition Prompting: Improving and Accelerating Retrieval-Augmented Generation}, 
      author={Thomas Merth and Qichen Fu and Mohammad Rastegari and Mahyar Najibi},
      year={2024},
      eprint={2404.06910},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.06910}, 
}

@misc{Li2022DecoupledContextProcessing,
      title={Decoupled Context Processing for Context Augmented Language Modeling}, 
      author={Zonglin Li and Ruiqi Guo and Sanjiv Kumar},
      year={2022},
      eprint={2210.05758},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2210.05758}, 
}

@misc{Izacard2021LeveragingPassageRetrievalwithGenerativeModels,
      title={Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering}, 
      author={Gautier Izacard and Edouard Grave},
      year={2021},
      eprint={2007.01282},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2007.01282}, 
}



@misc{Joshi2017TriviaQAaLargeScaleDistantlySupervisedChallenge,
      title={TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension}, 
      author={Mandar Joshi and Eunsol Choi and Daniel S. Weld and Luke Zettlemoyer},
      year={2017},
      eprint={1705.03551},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1705.03551}, 
}

@INPROCEEDINGS{Lin2021PyseriniaPythonToolkit,
   author = "Jimmy Lin and Xueguang Ma and Sheng-Chieh Lin and Jheng-Hong Yang and Ronak Pradeep and Rodrigo Nogueira",
   title = "{Pyserini}: A {Python} Toolkit for Reproducible Information Retrieval Research with Sparse and Dense Representations",
   booktitle = "Proceedings of the 44th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2021)",
   year = 2021,
   pages = "2356--2362",
}

@misc{Karpukhin2020DensePassageRetrieval,
      title={Dense Passage Retrieval for Open-Domain Question Answering}, 
      author={Vladimir Karpukhin and Barlas OÄŸuz and Sewon Min and Patrick Lewis and Ledell Wu and Sergey Edunov and Danqi Chen and Wen-tau Yih},
      year={2020},
      eprint={2004.04906},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2004.04906}, 
}

@misc{Yang2024qwen2TechnicalReport,
      title={Qwen2 Technical Report}, 
      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jianxin Yang and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Xuejing Liu and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhifang Guo and Zhihao Fan},
      year={2024},
      eprint={2407.10671},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.10671}, 
}
















