\section{Related Work}
Recent works in RAG have explored methods to efficiently process and incorporate large contexts. PCW____ splits few-shot examples or documents into several windows processed in parallel, thereby reducing positional encoding overhead and removing cross-attention between windows. While performance tends to decrease when using more than $3$ windows, \textbf{\textit{CacheFocus}} demonstrates robust performance even with contexts longer than those covered by $3$ windows.

Various modifications to the attention mechanism have been proposed to address limitations in context relevance and distribution. For example, Structured Prompting____ scales attention values by $1/M$ (where $M$ is the number of windows), although it requires specific in-context learning examples. Similarly, APE____ investigates additional scaling factors and temperature reduction, but it does not consider cache positioning based on the semantic relevance between query and document caches. SparseRAG____ infers relevance scores to prune less pertinent documents, which requires an additional training process. XL3M____ and Superposition Prompting____ adopt strategies that split and filter long inputs using probabilistic measures and Bayesian inference, respectively.

In contrast to these approaches____, \textbf{\textit{CacheFocus}} not only repositions caches based on the semantic relevance between query and document caches but also aggregates attention scores in a layer-wise manner, thereby capturing multi-level contextual information and mitigating noise from individual layers, which could lead to improved stability and performance.