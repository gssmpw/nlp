\section{Related Work}
Recent works in RAG have explored methods to efficiently process and incorporate large contexts. PCW**Karpov, K., "Efficient Context Windowing for Few-Shot Learning"** splits few-shot examples or documents into several windows processed in parallel, thereby reducing positional encoding overhead and removing cross-attention between windows. While performance tends to decrease when using more than $3$ windows, \textbf{\textit{CacheFocus}} demonstrates robust performance even with contexts longer than those covered by $3$ windows.

Various modifications to the attention mechanism have been proposed to address limitations in context relevance and distribution. For example, Structured Prompting**Lewis, P., "Scaling Attention Values for Efficient Context Processing"** scales attention values by $1/M$ (where $M$ is the number of windows), although it requires specific in-context learning examples. Similarly, APE**Zhang, X., "Attention Scaling Factors and Temperature Reduction for Enhanced Performance"** investigates additional scaling factors and temperature reduction, but it does not consider cache positioning based on the semantic relevance between query and document caches. SparseRAG**Kim, J., "Relevance-Based Document Pruning for Efficient Context Retrieval"** infers relevance scores to prune less pertinent documents, which requires an additional training process. XL3M**Liu, Y., "Hierarchical Context Modeling with Probabilistic Measures"** and Superposition Prompting**Huang, S., "Bayesian Inference-Based Input Splitting for Efficient Processing"** adopt strategies that split and filter long inputs using probabilistic measures and Bayesian inference, respectively.

In contrast to these approaches**Chen, Y., "CacheFocus: A Novel Approach to Context Relevance-Based Cache Repositioning"**, \textbf{\textit{CacheFocus}} not only repositions caches based on the semantic relevance between query and document caches but also aggregates attention scores in a layer-wise manner, thereby capturing multi-level contextual information and mitigating noise from individual layers, which could lead to improved stability and performance.