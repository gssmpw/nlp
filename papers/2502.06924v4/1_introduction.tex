\section{Introduction}\label{sec_intro}
SSMs, a classical framework for modeling dynamic systems through first-order differential equations, have gained prominence in machine learning for their efficiency in handling sequential data. Unlike transformers, which rely on quadratic-complexity attention mechanisms, SSMs achieve linear or near-linear scalability with sequence length by leveraging principles from convolutional and recurrent neural networks~\cite{s4}. This makes SSMs highly suitable for long-sequence tasks~\cite{mambaextend} like natural language processing, computer vision, and medicine, where they match transformers' modeling capabilities with significantly lower computational overhead. Their efficiency is particularly critical for edge applications, such as personal assistants and real-time transcription, where SSMs enable transformative AI with reduced resource consumption and improved energy efficiency.
Among SSM-based architectures, Mamba~\cite{mamba} and Mamba-2~\cite{mamba2} are the most prominent.
% Mamba introduces hardware-optimized algorithms, such as cyclic computation using scans, for improved performance on modern accelerators. Mamba-2 advances this further with the structured state-space duality (SSD) framework, connecting SSMs to attention mechanisms and enabling the reuse of transformer optimization techniques~\cite{mamba360}. These innovations position SSMs as strong candidates for replacing transformers in resource-constrained environments.
Mamba introduces algorithmic innovations, such as selective scan, which efficiently processes sequential data by selectively updating only relevant states. Mamba-2 further refines this approach with the structured state-space duality (SSD) framework, which connects SSMs with attention mechanisms, enabling the reuse of transformer optimization techniques~\cite{mamba360}. These algorithmic advancements position SSMs as strong candidates for replacing transformers in resource-constrained environments.
\textcolor{black}{\textit{While specialized accelerators exist for emerging networks, designing a new hardware is costly and impractical for every model. Instead, \textbf{XAMBA} repurposes NPUs for SSMs, enabling efficient deployment on existing hardware.}}
% \textcolor{blue}{While specialized accelerators have been proposed for emerging neural networks, designing new hardware is time-intensive, costly, and impractical for every model. Instead, optimizing models for existing NPUs offers a scalable and efficient solution. This work, \textbf{XAMBA}, for the first time, bridges this gap by enabling and optimizing SSMs on NPUs, demonstrating how existing hardware can be repurposed for advanced SSMs.}
% This ensures faster integration, reduces deployment friction, and eliminates the need for new hardware designs, making it a practical step forward.}
The efficiency of SSMs makes them ideal for deployment on NPUs, specialized accelerators optimized for data-parallel operations like matrix multiplication. Modern edge processors, such as AI PCs from Intel, Qualcomm, and AMD, integrate NPUs alongside CPUs and GPUs to support diverse AI workloads. NPUs, designed for high throughput and energy efficiency, consist of two key components: (1) \textit{Multiply-and-accumulate (MAC) Processing Units (MPUs)}~\cite{flexnn}, which handle parallelized matrix operations, and (2) \textit{Digital Signal Processors (DSPs)}, which execute sequential tasks like non-linear activations and cumulative summations. This architecture makes NPUs ideal for continuous, resource-intensive workloads such as personal assistants, real-time transcription, and contextual search.
Prior work has demonstrated the benefits of deploying transformer-based models (\textit{e.g.}, Llama, Phi) on NPUs, achieving significant improvements in inference latency and energy efficiency~\cite{llm_npu}. However, transformers' quadratic complexity limits their scalability for long sequences. SSMs, with their linear complexity, offer a natural fit for NPUs, enabling even greater performance and energy efficiency. Despite this, deploying SSMs on NPUs presents unique challenges. Their sequential computations (\textit{e.g}., CumSum, Swish, Softplus) and specialized operators misalign with NPUs' data-parallel architecture, leading to inefficient DSP execution, increased memory traffic, and underutilized parallel units. While prior work has optimized recurrent models like LSTMs on NPUs~\cite{rnn_npu} and proposed specialized accelerators like MARCA for SSMs~\cite{marca}, \textit{no prior work addresses the challenges of deploying SSMs on COTS SOTA NPUs.}
As shown in Figure~\ref{fig:motivation_exec_lat_brkdwn}, SSMs face significant bottlenecks on NPUs. For Mamba, activation functions like Swish and Softplus~\cite{openvino_ops} dominate execution time due to inefficient DSP execution. In Mamba-2, CumSum and ReduceSum emerge as critical bottlenecks, exacerbated by poor memory reuse and increased off-chip traffic. These challenges underscore the need for specialized optimizations to fully leverage SSMs on NPUs.
To address these challenges, we propose \textbf{XAMBA}, \textit{the first work to enable and optimize SSMs on COTS NPUs.} XAMBA follows a systematic 3-step methodology: (1) enabling SSMs on NPUs, (2) optimizing performance to meet target KPI requirements, and (3) trading accuracy for additional performance gains. 
% (1) enabling SSMs on NPUs, (2) optimizing performance to meet target Key Performance Indicator (KPI) requirements, and (3) trading accuracy for additional performance gains



% By bridging the gap between SSMs and NPUs, XAMBA achieves superior performance and energy efficiency, making SSMs viable for real-world applications on resource-constrained devices.

\begin{figure}[t!]
\begin{center}
\includegraphics[width=\columnwidth]{Figures/Baseline_latency_breakdown_2.pdf}
\end{center}
% \caption{Execution bottlenecks for Mamba and Mamba-2 models on Intel\textregistered\ Core\texttrademark\ Ultra Series 2~\cite{lnl} NPU. Mamba is dominated by sequential DSP execution of Swish (SiLU) and Softplus, while Mamba-2 faces bottlenecks from CumSum and ReduceSum operations. Additional details on the architectures and per-block operators of Mamba and Mamba-2 are presented in Fig.~\ref{fig:mamba_vs_mamba2}.}
\caption{Execution bottlenecks for Mamba and Mamba-2 on Intel\textregistered\ Core\texttrademark\ Ultra Series 2 NPU. Mamba is limited by sequential DSP execution of Swish (a.k.a. SiLU) and SoftPlus, while Mamba-2 faces CumSum and ReduceSum bottlenecks. Refer ~\ref{subsec:mamba1_vs_2} for models' architectural details.}
\label{fig:motivation_exec_lat_brkdwn}
\end{figure}



\begin{itemize}
    \item \textbf{Optimizing SSM Performance on NPUs}: After enabling SSMs on NPUs, XAMBA introduces \textbf{CumBA} and \textbf{ReduBA} to address key bottlenecks in SSM execution on NPUs. CumBA replaces sequential CumSum operations with matrix multiplication (MatMul) using a precomputed mask, leveraging the high-frequency MPUs for faster execution. ReduBA computes ReduceSum via matrix-vector multiplication (MVM) with a precomputed vector mask, further reducing latency. Both techniques enhance memory efficiency by increasing data reuse and reducing SRAM access, while CumBA leverages sparsity in its mask for additional optimizations using Zero Value Compression (ZVC) and compute skip.

    \item \textbf{Trading Accuracy for Performance Gains}: XAMBA introduces \textbf{ActiBA}, which maps computationally expensive activation functions (e.g., Swish, Softplus) onto the NPU's Piecewise Linear Unit (PLU) using Configurable Lookup Table (C-LUT) during the drain phase of the previous layer. ActiBA uses piecewise linear approximations to implement these functions with negligible accuracy loss, avoiding sequential DSP execution and reducing memory access overhead.

    \item XAMBA achieves significant latency reductions for Mamba and Mamba-2 on an Intel\textregistered\ Core\texttrademark\ Ultra Series 2 NPU. CumBA reduces execution latency by 2.7$\times$, ReduBA by 1.2$\times$, and ActiBA by up to 2.6$\times$ compared to baseline unoptimized implementation.
    % These optimizations make SSMs viable for real-world deployment on resource-constrained devices.
\end{itemize}

% ############ Version-1 ###
% State-Space Models (SSMs) are a classical mathematical framework for modeling dynamic systems through first-order differential equations. Recently, SSMs have gained prominence in machine learning for their effectiveness in handling sequential data, offering linear or near-linear scalability with sequence length. Unlike transformers, which rely on quadratic-complexity attention mechanisms, SSMs achieve computational efficiency by leveraging principles from convolutional and recurrent neural networks. This makes SSMs particularly suitable for long-sequence tasks, such as natural language processing, computer vision, and medicine, where they demonstrate comparable modeling capabilities to transformers while maintaining significantly lower computational overhead.
% Among SSM-based architectures, Mamba~\cite{mamba} and Mamba-2~\cite{mamba2} stand out as the most prominent models. Mamba introduces hardware-optimized algorithms, such as cyclic computation using scans, to achieve significant performance improvements on modern accelerators. Mamba-2 further advances this by proposing the structured state-space duality (SSD) framework, connecting SSMs with attention mechanisms and enabling the reuse of optimization techniques initially developed for transformers. These innovations make SSMs promising candidates for replacing transformers in resource-constrained environments.

% The efficient design of SSMs motivates their deployment on Neural Processing Units (NPUs), specialized hardware accelerators optimized for data-parallel operations like matrix multiplication. Modern edge processors, such as AI PCs from Intel, Qualcomm, and AMD, integrate heterogeneous computing units, including CPUs, GPUs, and NPUs, to efficiently support diverse AI workloads. NPUs are tailored for deep learning applications, delivering high throughput and energy efficiency compared to traditional CPUs and GPUs. They typically consist of two key components: (1) \textit{Data Processing Units (DPUs)}~\cite{flexnn}, which handle parallelized matrix operations fundamental to neural network computations, and (2) \textit{Digital Signal Processors (DSPs)}, which execute sequential tasks like non-linear activation functions and cumulative summations. This dual architecture makes NPUs ideal for continuous and resource-intensive AI workloads, such as personal assistants, real-time transcription, and contextual search.
% Prior work has demonstrated the advantages of deploying transformer-based language models (e.g., Llama, Phi) on NPUs, achieving significant gains in inference latency and energy efficiency~\cite{llm_npu}. However, transformers' quadratic complexity limits their scalability for long-sequence tasks. SSMs, with their linear complexity, offer a natural fit for NPUs, enabling even greater performance and energy efficiency.
% Despite these advantages, deploying SSMs on NPUs presents unique challenges. SSMs involve sequential computations (e.g., CumSum, Swish, Softplus) and specialized operators that misalign with NPUs' data-parallel architecture. This leads to inefficiencies in DSP execution, increased memory traffic, and underutilization of parallel units. While prior work has explored optimizing recurrent models like LSTMs on NPUs~\cite{rnn_npu}, and specialized accelerators like MARCA have been proposed for SSMs~\cite{marca}, no prior work addresses the challenges of deploying SSMs on commercial off-the-shelf (COTS) NPUs.
% % Additionally, hardware synthesis approaches for SSMs~\cite{ssm_fpga} do not fully leverage NPUs' parallel processing capabilities, leaving significant performance gains untapped.
% As illustrated in Fig.~\ref{fig:motivation_exec_lat_brkdwn}, the primary bottlenecks for SSMs on NPUs stem from their sequential computations. For Mamba, activation functions like Swish and Softplus dominate execution time, as they are executed sequentially on DSPs. In Mamba-2, CumSum and ReduceSum emerge as critical bottlenecks, further exacerbated by inefficient memory reuse and increased off-chip memory traffic. These challenges highlight the need for specialized optimizations to fully leverage SSMs' potential on NPUs.

% To address these challenges, we propose \textbf{XAMBA}, \textit{the first work to enable and optimize SSMs on COTS NPUs}. Our approach follows a systematic three-step methodology: (1) enabling SSMs on NPUs, (2) optimizing performance, and (3) trading accuracy for further performance gains. By bridging the gap between SSMs and NPUs, XAMBA achieves superior performance and energy efficiency, making SSMs viable for real-world applications on resource-constrained devices.

% State-Space Models (SSMs) are a classical mathematical framework used to model dynamic systems through first-order differential equations. Originally adopted in fields like control systems, signal processing, and circuit design, SSMs have recently gained prominence in machine learning (ML) for their effectiveness in handling sequential data. Modern SSM-based ML architectures excel in modeling long sequences efficiently, offering faster training and inference compared to transformers ~\cite{mamba}. Unlike transformers, which rely on attention mechanisms with quadratic computational complexity for key-value operations, SSMs achieve linear or near-linear scalability with sequence length. They do this by leveraging principles from convolutional and recurrent neural networks. This makes SSMs computationally efficient and suitable for processing long sequences with reduced overhead. Mamba, a leading SSM-based model, demonstrates modeling capabilities comparable to transformers while maintaining linear time complexity. It incorporates a reparameterization mechanism to retain relevant information and discard irrelevant data efficiently. Additionally, Mamba employs hardware-optimized algorithms, such as cyclic computation using scans, to achieve significant performance improvements on modern accelerators. Building on Mamba, Mamba-2 ~\cite{mamba2} introduces the structured state-space duality (SSD) framework, connecting SSMs with attention mechanisms and enabling the reuse of optimization techniques initially developed for transformers. This evolution has made SSMs, particularly Mamba and Mamba-2, promising candidates for replacing transformers in applications like natural language processing, computer vision, and medicine. Their efficient design and scalability make them an attractive backbone for modern sequence modeling tasks.

% These advantages make SSMs particularly well-suited for execution on Neural Processing Units (NPUs), where their lightweight computational and memory requirements align with the hardware's strengths. NPUs are specialized hardware accelerators designed to efficiently execute deep learning workloads. Integrated into client PCs, NPUs are optimized for data-parallel computations like matrix multiplication, a fundamental operation in most neural networks. These accelerators consist of matrix multiplication units, often called Data Processing Units (DPUs), which handle the bulk of neural network operations. To support non-linear activations and sequential, non-data-parallel tasks, NPUs also include Digital Signal Processors (DSPs) or vector processors optimized for these computations. NPUs are tailored for deep learning applications, delivering high performance per watt compared to CPUs and GPUs, making them ideal for low-power, high-throughput tasks. Applications of sequence/language modeling on client PCs include personal assistants, real-time transcription, language translation, document summarization, and contextual search. These tasks demand continuous and efficient background processing, which benefits from the power-efficient and high-throughput capabilities of NPUs. Previous works have demonstrated the advantages of deploying transformer-based language models (Llama, Phi, etc.) on NPUs, showing significant gains in inference latency and energy efficiency. However, transformers are inherently resource-intensive due to their quadratic complexity, especially for long-sequence tasks. SSMs, such as Mamba and Mamba-2, are designed as efficient alternatives to transformers, offering similar modeling capabilities with linear complexity. This efficiency makes SSMs a natural fit for NPUs. By enabling and optimizing SSMs on NPUs, we can leverage their reduced computational overhead and memory requirements to achieve even greater performance and energy efficiency than transformers. Moreover, as drop-in replacements for transformers, SSMs can directly enhance applications previously dominated by LLMs while scaling better to resource-constrained environments. Enabling SSMs on NPUs represents a crucial step toward advancing the deployment of efficient, high-performance neural network architectures for real-world applications on client devices.

% Deploying SSMs on NPUs faces challenges due to their sequential computations (e.g., CumSum, Swish, Softplus) and specialized operators, which misalign with NPUs' data-parallel architecture. This leads to inefficiencies in DSP execution, increased memory traffic, and underutilization of parallel units, necessitating optimized memory and computation strategies for effective deployment.
% Despite the challenges in mapping SSMs like Mamba and Mamba-2 onto NPUs—such as mismatched kernel optimizations, sequential computation bottlenecks, and memory inefficiencies—no prior work has addressed these issues on commercial off-the-shelf (COTS) NPUs. While specialized accelerators have been proposed for emerging neural networks, designing new hardware is time-intensive, costly, and impractical for every model. Instead, optimizing general-purpose NPUs to handle diverse workloads offers a scalable and efficient solution. This work XAMBA, for the first time, bridges this gap by enabling and optimizing SSMs on Intel NPUs ~\cite{lnl} ~\cite{mtl}, demonstrating how existing hardware can be repurposed for advanced SSMs. This ensures faster integration, reduces deployment friction, and eliminates the need for new hardware designs, making it a practical step forward.

% To address these challenges, we propose the following:

% \begin{enumerate}
%     \item By leveraging the fast matrix multiplication capability of high-frequency data-parallel DPUs, XAMBA introduces \textbf{CumBA}, which achieves cumulative sum (CumSum) with matrix multiplication (MatMul) using a mask precomputed during compile time. This approach solves the challenge of high execution time caused by sequential CumSum operations on NPUs.

%     \item To address the second major bottleneck of Mamba-2, sequential DSP execution for reduction operations on NPUs, XAMBA introduces \textbf{ReduBA}, which computes reduce sum (ReduceSum) using matrix-vector multiplication (MVM) with a vector mask precomputed during compile time. By utilizing the high-frequency, data-parallel processing capability of DPUs, ReduBA reduces the end-to-end inference latency.

%     \item By utilizing the Post Processing Element (PPE) and Software Programmable Lookup Tables (Spr-LUTs) in the drain path of the DPU, this patent introduces \textbf{ActiBA}, a technique that maps computationally expensive activation functions such as Swish (SiLU) and Softplus onto specialized hardware during the drain of the previous layer's output. This approach addresses the challenge of sequential execution bottlenecks on NPUs, effectively reducing latency for Mamba-based models.

%     \item In addition to accelerating computation, XAMBA techniques address the memory-bounded nature of State-Space Model (SSM) execution by reducing local SRAM memory access and increasing data reuse, thereby enhancing effective memory bandwidth. CumBA improves data reuse by mapping it to the DPU consisting of local register files (that increases input and output data reuse) and eliminates redundant memory reads/writes to SRAM compared to DSP-based execution. ReduBA, utilizing matrix-vector multiplication (MVM), reuses the reduce sum mask across all operations, significantly lowering memory traffic and further increasing memory bandwidth. ActiBA, performed during the drain phase of the previous layer, avoids storing and reloading intermediate outputs from memory, effectively reducing memory access overhead.

%     \item Leveraging the sparsity in the CumSum mask (in CumBA), which is a lower triangular binary matrix with $\sim$50\% zeros, XAMBA introduces opportunities for further memory and compute optimization. By applying Zero Value Compression (ZVC), the storage and data transfer requirements for the CumSum mask are significantly reduced. Additionally, by utilizing the NPU’s support for compute skipping through sparsity bitmaps, unnecessary computations for zero values are avoided, resulting in accelerated execution and reduced memory traffic. This approach enhances overall efficiency by minimizing storage usage and maximizing computational throughput.

%     \item XAMBA introduces a unified framework for mapping SSMs like Mamba and Mamba-2 onto NPUs, requiring no hardware changes. Its implementation requires only a compiler change to implement CumBA and ReduBA, while for ActiBA, the lookup table inside the Spr-LUT needs to be programmed. This ensures optimized utilization of NPU resources and addresses the inefficiencies of blind out-of-the-box deployments.

%     \item By implementing CumBA, ReduBA, and ActiBA, this patent achieves significant performance improvements for Mamba-2 and Mamba on Intel NPUs. CumBA results in a 1.8$\times$ reduction in execution latency, ReduBA achieves a 1.1$\times$ improvement, and ActiBA delivers up to 2.6$\times$ reduction in execution latency compared to the initial out-of-the-box mapping.
% \end{enumerate}


% % \begin{figure}[h]
% % \begin{center}
% % %\framebox[4.0in]{$\;$}
% % \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
% % \end{center}
% % \caption{Sample figure caption.}
% % \end{figure}

