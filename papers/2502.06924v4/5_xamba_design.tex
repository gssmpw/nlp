\section{XAMBA Design Methodology}\label{sec_xamba_design}

\begin{figure}[t!]
\begin{center}
\includegraphics[width=\columnwidth]{Figures/CumBA_ReduBA_ActiBA.pdf}
\end{center}
\caption{XAMBA:
(a) NPU architecture
(b) Sequential CumSum and ReduceSum computation on a DSP.
(c) CumBA and ReduBA masks for optimized computations.
(d) Sequential execution of activation functions (Swish/SiLU and SoftPlus) on DSP.
(e) ActiBA: Efficient execution of SoftPlus and Swish activations using C-LUT in PLU.}\label{fig:xamba}
\end{figure}


Before detailing the design methodology, it is essential first to understand the underlying system architecture (refer Figure~\ref{fig:xamba}). We consider an output-stationary MPU architecture (as shown in Figure~\ref{fig:xamba}(a)) inspired by ~\cite{flexnn}. 
% The core component is the Data Processing Unit (DPU), an $M\times M$ grid of Versatile Processing Elements (VPEs). Each VPE comprises an $N\times N$ array of MAC Processing Elements (MPEs) designed for efficient Multiply-and-Accumulate (MAC) operations. This DPU is well-suited for operations like matrix multiplication, which are fundamental to many neural network computations.
% The architecture includes local SRAM for storing activations and weights, a tensor distribution network for data flow to and from the DPU, and control logic for managing computation, accumulation, and output extraction. MAC operations, integral to DNNs, calculate dot products of weights and activations to produce output feature maps. Each MPE leverages a local data path with register files, multipliers, and accumulators to perform these tasks. Additionally, a Digital Signal Processor (DSP) handles non-linear activation functions, complementing the data-parallel DPU. 
Although our case study considers an output-stationary MPU architecture, the proposed techniques are generic and can be applied to other NPUs without loss of generality. \textbf{Step-1:} The \textit{first step} of XAMBA is to enable SSMs on NPUs. Since NPUs generally support static input shapes, we use a prefill model with a fixed number of input tokens to generate the hidden states, applying padding for smaller inputs. For subsequent token generation, we employ a separate model that uses the cached hidden states to ensure efficiency. 
% This approach ensures compatibility with NPU constraints while maintaining performance.


\subsection{Step-2: Optimizing SSM Performance to meet target KPI requirements}

\textbf{CumBA:}  
As highlighted in Figure~\ref{fig:motivation_exec_lat_brkdwn}, one of the major bottlenecks in executing Mamba-2 on NPUs is the CumSum operation.
\textcolor{black}{A deeper analysis reveals that Mamba-2 contains three CumSum operations per block (Figure ~\ref{fig:mamba_vs_mamba2}), but the primary bottleneck, denoted as $CumSum_b$, accounts for more than 99.9\% of the total CumSum execution time. This bottleneck arises in step-1 of the SSD framework of Mamba-2 (Listing 1 of ~\cite{mamba2}).  
Within SSD, the input sequence is split into blocks, and step 1 computes the intra-chunk output, assuming an initial state of zero. $CumSum_b$, appearing at the start of this step, computes semi-separable (SS) matrices essential for modeling long-range dependencies across input segments.
% Within SSD, the input sequence is partitioned into multiple blocks, and step 1 involves determining the intra-chunk output. This step computes the contribution of each chunk independently, assuming an initial state of zero. $CumSum_b$ corresponds to the efficient computation of semi-separable (SS) matrices at the beginning of step 1, which are fundamental for modeling long-range dependencies across different input segments.  
The bottleneck stems from the large matrix dimensions associated with $CumSum_b$: In Mamba-2 130M, $CumSum_b$ operates on a $256 \times 256$ matrix, whereas the other CumSum operations in the model involve significantly smaller dimensions ($256$ and $2 \times 2$). }
% The sequential nature of CumSum, combined with these large dimensions, exacerbates execution latency, making it a critical performance limiter.}
% \textcolor{red}{A deeper analysis reveals that Mamba-2 contains three CumSum operations per block (Fig.~\ref{fig:mamba_vs_mamba2}), but the bottleneck ($CumSum_b$)—accounting for more than 99.9\% of the total CumSum execution time—is the one in the step-1 of the Structured State Space Duality (SSD) framework of Mamba-2~\cite{mamba2}. Within SSD, the input sequence is split into multiple blocks, and the first step is to determine the intra-chunk output, which computes the contribution of each chunk independently, assuming an initial state of zero. $CumSum_b$ corresponds to the efficient computation of semi-separable (SS) matrices, essential for modeling long-range dependencies across different input segments. The bottleneck arises due to the large matrix dimensions involved with $CumSum_b$: For Mamba-2 130M $CumSum_b$ operates on a $256\times 256$ matrix, whereas the other CumSums in the model have significantly smaller dimensions ($256$ and $2\times 2$).}
% Given that CumSum is inherently sequential, executing it on the DSP leads to excessive latency, as it requires n-width vector adder.}
Figure~\ref{fig:xamba}(b) depicts that executing CumSum on NPUs leads to high latency due to its \textit{sequential nature} on the DSP. Given an input tensor $\mathbf{X} \in \mathbb{R}^{m \times n}$, the standard CumSum operation along the row dimension is expressed as $\mathbf{C}_{i,j} = \sum_{k=1}^{i} \mathbf{X}_{k,j}$ for all $i \in [1, m]$, $j \in [1, n]$. This requires $m$ sequential cycles, assuming the DSP has an $n$-width vector adder. CumSum is processed in smaller chunks for higher-dimensional tensors exceeding register file capacity, requiring frequent on-chip SRAM memory transfers. This increases latency, memory traffic, and bandwidth consumption, leading to inefficient data reuse and performance degradation.
% Additionally, for large tensors, intermediate results must be stored in off-chip memory, increasing memory traffic.

To address these inefficiencies, XAMBA introduces \textbf{CumBA}, which transforms CumSum into a MatMul, leveraging the parallel processing capabilities of the NPU’s MPU. Specifically, CumBA precomputes (at compile-time) a lower triangular mask $\mathbf{M}_{\text{CumBA}}$ (Figure~\ref{fig:xamba}(c)), where $\mathbf{M}_{\text{CumBA}}(i, j) = 1$ if $j \leq i$ and $0$ otherwise. This enables CumSum to be computed as $\mathbf{C} = \mathbf{M}_{\text{CumBA}} \cdot \mathbf{X}$. By remapping CumSum to matrix multiplication, CumBA enables parallel execution, leveraging the MPU’s high-frequency MAC array to perform multiple operations simultaneously. It improves data reuse by utilizing the MPU’s larger local register files, reducing redundant memory reads/writes to SRAM compared to DSP-based execution. The MPU processes MatMul in a tiled manner, further improving data reuse and minimizing costly on-and-off-chip memory transfers. 
% By addressing both computational and memory inefficiencies of sequential CumSum, CumBA significantly enhances performance and resource utilization on NPUs.
% By utilizing the high-frequency MAC array in the DPU, CumBA enables parallel computation, reduces memory traffic, and minimizes SRAM accesses.

\textit{CumBA memory savings using Zero Value Compression (ZVC):}
XAMBA applies ZVC~\cite{zvc} to compress the CumBA mask, a lower triangular binary matrix with $\sim$50\% zeros, significantly reducing storage and memory traffic (Figure~\ref{fig:cumba_zvc}). This compression leads to substantial memory savings, as only non-zero elements are stored. Additionally, modern NPUs utilize sparsity bitmaps to skip zero-value computations, further improving execution speed and energy efficiency. While CumBA benefits from ZVC-driven optimizations, the weights in Mamba and Mamba-2 exhibit minimal inherent sparsity, limiting acceleration gains from sparsity-aware execution. As future work, we plan to explore structured sparsity techniques in SSMs to further enhance NPU efficiency.
% The $\sim$50\% zeros in the CumSum mask of CumBA, represented as a lower triangular binary matrix, enable efficient memory and compute optimization. As shown in Figure~\ref{fig:cumba_zvc}, ZVC~\cite{zvc} reduces storage and memory traffic by storing only non-zero elements. \textcolor{black}{Modern NPUs leverage sparsity bitmaps to skip zero-value computations, enhancing speed and energy efficiency. While CumBA benefits from this, Mamba weights show minimal inherent sparsity. Future work will explore injecting sparsity in SSMs for further acceleration.}
% \textcolor{blue}{Modern NPUs already support unstructured sparsity acceleration using sparsity bitmaps, allowing zero-value computations to be skipped, further improving execution speed and energy efficiency. While CumBA naturally benefits from this sparsity-aware execution, we observe that the weights of Mamba models exhibit negligible inherent sparsity. As part of future work, we plan to explore techniques for injecting sparsity in SSMs to further accelerate computing on NPUs.}
% The NPU’s sparsity-aware compute logic, using sparsity bitmaps, skips zero-value computations, further accelerating execution and improving energy efficiency. 
% As shown in Figure~\ref{fig:cumba_zvc}, storing only non-zero elements significantly reduces memory footprint, while the NPU's sparsity-aware compute logic skips redundant operations, further improving efficiency.
% Both Lunar Lake~\cite{lnl} and Meteor Lake~\cite{mtl} architectures support these capabilities, making them ideal for handling such sparse workloads.

% The $\sim$50\% zeros in the CumSum mask of CumBA, represented as a lower triangular binary matrix, provide a significant opportunity for memory and compute optimization. As shown in Figure~\ref{fig:cumba_zvc}, ZVC compresses the mask by storing only non-zero elements, which reduces both storage requirements and memory traffic. Additionally, the NPU’s sparsity-aware compute logic, using sparsity bitmaps, skips computations for zero values, further accelerating execution and improving energy efficiency. Both Lunar Lake~\cite{lnl} and Meteor Lake~\cite{mtl} architectures support ZVC and sparsity-aware compute, making them well-suited for such workloads. Figure~\ref{fig:cumba_zvc} illustrates this process, where the ZVC-compressed mask is stored in memory and the NPU bypasses unnecessary operations, leveraging both weight and activation sparsity to enhance computational throughput and reduce energy consumption.
% The $\sim$50\% sparsity in $\mathbf{M}_{\text{CumBA}}$ allows for \textbf{Zero Value Compression (ZVC)}~\cite{zvc}, reducing storage and memory traffic. As shown in Figure~\ref{fig:cumba_zvc}, storing only non-zero elements significantly reduces memory footprint, while the NPU's sparsity-aware compute logic skips redundant operations, further improving efficiency.

\textbf{ReduBA:} 
As illustrated in Figure~\ref{fig:motivation_exec_lat_brkdwn}, another significant bottleneck in the execution of Mamba-2 on NPUs is the ReduceSum operation.
\textcolor{black}{A detailed analysis shows that these bottlenecks originate from the reduction sum operations present in every step of the SSD framework in Mamba-2 (Listing 1 of ~\cite{mamba2}).}
Similar to CumSum, the ReduceSum operation suffers from high latency due to sequential DSP execution, as illustrated in Figure~\ref{fig:xamba}(b). Given an input matrix $\mathbf{X} \in \mathbb{R}^{m \times n}$, the ReduceSum along the row dimension is defined as $\mathbf{R}_{j} = \sum_{i=1}^{m} \mathbf{X}_{i,j} = \mathbf{C}_{m,j}$ for all $j \in [1, n]$.  

To mitigate this, XAMBA introduces \textbf{ReduBA}, which reformulates ReduceSum as a matrix-vector multiplication (MVM) using a precomputed vector mask $\mathbf{M}_{\text{ReduBA}}$ (Figure~\ref{fig:xamba}(c)), where $\mathbf{M}_{\text{ReduBA}}(i) = 1$ for all $i$. The ReduceSum operation is then computed as $\mathbf{R} = \mathbf{M}_{\text{ReduBA}} \cdot \mathbf{X}$. 
ReduBA improves upon CumBA by reusing the ReduBA vector mask $\mathbf{M}_{\text{ReduBA}}$ across all operations, reducing memory traffic. Unlike CumBA's matrix-matrix operations, where each computation fetches a new part of the mask, ReduBA’s matrix-vector multiplication applies the same mask repeatedly, minimizing memory accesses and optimizing bandwidth. ReduBA also leverages multiple MAC units in the MPU and a tiled computation strategy, further enhancing data reuse and reducing on-chip memory accesses, resulting in lower latency and improved memory efficiency for ReduceSum operations in Mamba-2 on NPUs.
% ReduBA improves upon CumBA by reusing the ReduBA vector mask $\mathbf{M}_{\text{ReduBA}}$ across all operations, which leads to significant reductions in memory traffic. Unlike the matrix-matrix operations in CumBA, where each computation requires fetching a new mask section, ReduBA's matrix-vector multiplication allows the mask to be applied repeatedly without reloading it. This reuse minimizes memory accesses and maximizes the use of available memory bandwidth. Furthermore, ReduBA takes advantage of multiple MAC units within the DPU and employs a tiled computation strategy, which further enhances data reuse and reduces the number of on-chip memory accesses. These optimizations collectively reduce latency and improve memory efficiency, significantly accelerating ReduceSum operations in Mamba-2 on NPUs.
% Unlike CumBA, ReduBA achieves superior memory savings by reusing $\mathbf{M}_{\text{ReduBA}}$ across all computations, significantly reducing memory traffic and optimizing bandwidth utilization.

\begin{figure}[t!]
\begin{center}
\includegraphics[width=\columnwidth]{Figures/CumBA_ZVC_1_row.pdf}
\end{center}
\caption{CumBA: Enhancing memory, bandwidth, and compute efficiency by exploiting CumBA mask sparsity and two-sided sparsity acceleration in the NPU datapath.}\label{fig:cumba_zvc}
\end{figure}

\subsection{Step-3: Trading Accuracy for Additional Performance Gains}

\textbf{ActiBA:}  
As illustrated in Figure~\ref{fig:motivation_exec_lat_brkdwn}, two of the most significant bottlenecks in Mamba’s execution on NPUs are the Swish (a.k.a. SiLU) and Softplus activation functions. They introduce significant execution overhead when processed sequentially on the DSP, as depicted in Figure~\ref{fig:xamba}(d). 
The SiLU function is defined as $\text{SiLU}(x) = x \cdot \sigma(x)$ with $\sigma(x) = \frac{1}{1 + e^{-x}}$, while Softplus is given by $\text{Softplus}(x) = \frac{1}{\beta} \log (1 + e^{\beta x})$. Both functions exhibit nonlinearity near the origin but transition to linear behavior elsewhere, making them suitable for approximation using piecewise linear functions.
% These functions are defined as $\text{SiLU}(x) = x \cdot \sigma(x)$ with $\sigma(x) = \frac{1}{1 + e^{-x}}$ and $\text{Softplus}(x) = \frac{1}{\beta} \log (1 + e^{\beta x})$. These functions are highly nonlinear near the origin but exhibit linear behavior elsewhere, enabling approximation using piecewise linear functions.

XAMBA introduces \textbf{ActiBA}, which leverages the Piecewise Linear Unit (PLU) found in modern NPUs to efficiently map the Swish and Softplus activation functions. The PLU, located within the Arithmetic Unit (AU) in the MPU's drain path, integrates a C-LUT, as shown in Figure~\ref{fig:xamba}(e).
% XAMBA introduces \textbf{ActiBA}, which efficiently maps the Swish and Softplus activation functions to the NPU’s PLU located within the Arithmetic Unit (AU) in the MPU's drain path. The PLU integrates C-LUT, as shown in Figure~\ref{fig:xamba}(e). 
The C-LUT stores precomputed slopes and intercepts for linear segments, enabling the approximation $f(x) \approx m_k x + c_k$ over intervals $[x_k, x_{k+1}]$. During runtime, the activation function is evaluated directly using the C-LUT, avoiding sequential DSP execution and significantly reducing latency. ActiBA also utilizes \textit{vertical fusion} by performing activation computations during the drain phase, which eliminates the need for storing and reloading intermediate outputs. This reduces memory access overhead and optimizes memory bandwidth usage. The simple linear computations, integrated into the data drain process, further minimize execution latency. By addressing both computational and memory inefficiencies, ActiBA drastically lowers end-to-end latency for Mamba-based models with \textit{negligible loss in quality}~\cite{dse_act}. Increasing the number of linear segments in the non-linear section of the activation functions can further reduce this loss without significantly impacting performance~\cite{flex_sfu}.

% XAMBA introduces \textbf{ActiBA}, which maps Swish and Softplus to the NPU’s Software Programmable Lookup Tables (Spr-LUTs), as illustrated in Figure~\ref{fig:xamba}(e). The LUT stores precomputed slopes and intercepts for linear segments, approximating $f(x) \approx m_k x + c_k$ for $x \in [x_k, x_{k+1}]$. At runtime, the function is evaluated using the LUT, eliminating sequential DSP execution and significantly reducing latency. Additionally, ActiBA leverages vertical fusion, performing activation calculations during the drain phase of preceding layers, minimizing memory overhead and improving dataflow efficiency.







% Version-1: with commented equations
% \subsection{Optimizing SSM Performance on NPUs}

% \textbf{CumBA: Optimizing CumSum Execution}
% As shown in Figure~\ref{fig:xamba}(b), executing cumulative sum (CumSum) on NPUs leads to high latency due to its sequential nature on the DSP. Given an input tensor $\mathbf{X} \in \mathbb{R}^{m \times n}$, the standard CumSum operation along the row dimension is expressed as $\mathbf{C}_{i,j} = \sum_{k=1}^{i} \mathbf{X}_{k,j}$ for all $i \in [1, m]$, $j \in [1, n]$.
% % $\mathbf{Y}{i,j} = \sum{k=1}^{i} \mathbf{X}_{k,j}$ for all $i \in [1, m]$, $j \in [1, n]$:
% % \begin{equation}
% %     \mathbf{Y}_{i,j} = \sum_{k=1}^{i} \mathbf{X}_{k,j}, \quad \forall i \in [1, m], \; j \in [1, n].
% % \end{equation}
% This requires $m$ sequential cycles for each column assuming the DSP has a n-width vector adder. Additionally, for large tensors, intermediate results must be stored in off-chip memory, increasing memory traffic.
% To address these inefficiencies, XAMBA introduces \textbf{CumBA}, which transforms CumSum into a matrix multiplication (MatMul), leveraging the parallel processing capabilities of the NPU’s DPU. Specifically, CumBA precomputes (at compile-time) a lower triangular $\mathbf{M}_{\text{CumBA}}$ (Figure~\ref{fig:xamba}(c)), where $\mathbf{M}_{\text{CumBA}}(i, j) = 1$ if $j \leq i$ and $0$ otherwise.
% % \begin{equation}
% %     \mathbf{M}_{\text{CumBA}} = 
% %     \begin{bmatrix}
% %         1 & 0 & 0 & \cdots & 0 \\
% %         1 & 1 & 0 & \cdots & 0 \\
% %         1 & 1 & 1 & \cdots & 0 \\
% %         \vdots & \vdots & \vdots & \ddots & \vdots \\
% %         1 & 1 & 1 & \cdots & 1
% %     \end{bmatrix} \in \mathbb{R}^{m \times m},
% % \end{equation}
% This enables CumSum to be computed as $\mathbf{C} = \mathbf{M}_{\text{CumBA}} \cdot \mathbf{X}$.
% % \begin{equation}
% %     \mathbf{Y} = \mathbf{M}_{\text{CumBA}} \cdot \mathbf{X}.
% % \end{equation}
% By utilizing the high-frequency MAC array in the DPU, CumBA enables parallel computation, reduces memory traffic, and minimizes SRAM accesses.

% \textbf{Memory Savings with Zero Value Compression (ZVC):}
% The $\sim$50\% sparsity in $\mathbf{M}_{\text{CumBA}}$ allows for \textbf{Zero Value Compression (ZVC)}~\cite{zvc}, reducing storage and memory traffic. As shown in Figure~\ref{fig:cumba_zvc}, storing only non-zero elements significantly reduces memory footprint, while the NPU's sparsity-aware compute logic skips redundant operations, further improving efficiency.

% \textbf{ReduBA: Addressing Sequential Execution Bottlenecks in ReduceSum}
% Similar to CumSum, the ReduceSum operation suffers from high latency due to sequential DSP execution, as illustrated in Figure~\ref{fig:xamba}(b). Given an input matrix $\mathbf{X} \in \mathbb{R}^{m \times n}$, the ReduceSum along the row dimension is defined as $\mathbf{R}_{j} = \sum_{i=1}^{m} \mathbf{X}_{i,j} = \mathbf{C}_{m,j}$ for all $j \in [1, n]$.
% % \begin{equation}
% %     \mathbf{Y}_{j} = \sum_{i=1}^{m} \mathbf{X}_{i,j}, \quad \forall j \in [1, n].
% % \end{equation}
% To mitigate this, XAMBA introduces \textbf{ReduBA}, which reformulates ReduceSum as a matrix-vector multiplication (MVM) using a precomputed vector mask $\mathbf{M}_{\text{ReduBA}}$ (Figure~\ref{fig:xamba}(c)), where $\mathbf{M}_{\text{ReduBA}}(i) = 1$ for all $i$.
% % \begin{equation}
% %     \mathbf{M}_{\text{ReduBA}} = 
% %     \begin{bmatrix}
% %         1 & 1 & 1 & \cdots & 1
% %     \end{bmatrix}^T \in \mathbb{R}^{m \times 1}.
% % \end{equation}
% The ReduceSum operation is then computed as $\mathbf{R} = \mathbf{M}_{\text{ReduBA}}^T \cdot \mathbf{X}$.
% % \begin{equation}
% %     \mathbf{Y} = \mathbf{M}_{\text{ReduBA}}^T \cdot \mathbf{X}.
% % \end{equation}
% Unlike CumBA, ReduBA achieves superior memory savings by reusing $\mathbf{M}_{\text{ReduBA}}$ across all computations, significantly reducing memory traffic and optimizing bandwidth utilization.

% \subsection{Trading Accuracy for Performance Gains}
% % \subsubsection{ActiBA: Efficient Activation Function Mapping}
% Activation functions such as Swish (SiLU) and Softplus introduce significant execution overhead when processed sequentially on the DSP, as depicted in Figure~\ref{fig:xamba}(d). 
% These functions are defined as $\text{SiLU}(x) = x \cdot \sigma(x)$ with $\sigma(x) = \frac{1}{1 + e^{-x}}$ and $\text{Softplus}(x) = \frac{1}{\beta} \log (1 + e^{\beta x})$.
% % Their standard formulations are:
% % \begin{equation}
% %     \text{SiLU}(x) = x \cdot \sigma(x), \quad \sigma(x) = \frac{1}{1 + e^{-x}},
% % \end{equation}
% % \begin{equation}
% %     \text{Softplus}(x) = \frac{1}{\beta} \log (1 + e^{\beta x}).
% % \end{equation}
% These functions are highly nonlinear near the origin but exhibit linear behavior elsewhere, enabling approximation using piecewise linear functions. XAMBA introduces \textbf{ActiBA}, which maps Swish and Softplus to the NPU’s Software Programmable Lookup Tables (Spr-LUTs), as illustrated in Figure~\ref{fig:xamba}(e). The LUT stores precomputed slopes and intercepts for linear segments, approximating $f(x) \approx m_k x + c_k$ for $x \in [x_k, x_{k+1}]$.
% % The LUT stores precomputed slopes and intercepts for piecewise segments:
% % \begin{equation}
% %     f(x) \approx a_k x + b_k, \quad \text{for } x \in [x_k, x_{k+1}].
% % \end{equation}
% At runtime, the function is evaluated using the LUT, eliminating sequential DSP execution and significantly reducing latency. Additionally, ActiBA leverages vertical fusion, performing activation calculations during the drain phase of preceding layers, minimizing memory overhead and improving dataflow efficiency.

% \subsection{Summary}
% XAMBA’s optimizations—CumBA for CumSum, ReduBA for ReduceSum, and ActiBA for activations—effectively mitigate sequential execution inefficiencies in SSMs on NPUs. By leveraging data-parallel execution, memory optimizations, and hardware-aware activation approximations, XAMBA significantly improves performance and efficiency for SSM workloads on edge AI devices.




% % XAMBA introduces an end-to-end methodology (as shown in Figure~\ref{fig:xamba_e2e}) for deploying pre-trained SSMs on a Neural Processing Unit (NPU), such as Intel’s NPU, without requiring retraining or hardware modifications. The approach ensures efficient execution while maintaining model performance by leveraging optimized software and compiler techniques. Key contributions include mapping CumSum and ReduceSum operations as matrix multiplications on the DPU during model compilation, as outlined by CumBA and ReduBA, and programming the Spr-LUT within the PPE to support SiLU and SoftPlus activation functions through ActiBA. This methodology maximizes compatibility and performance, enabling the efficient execution of SSMs on existing NPU hardware. Table~\ref{tab:xamba_techniques} summarizes the novel XAMBA techniques and the areas they target to improve, including compute acceleration, memory bandwidth enhancement, and data transfer reduction.

% % \begin{figure}[t!]
% % \begin{center}
% % \includegraphics[width=0.6\columnwidth]{Figures/XAMBA_e2e.pdf}
% % \end{center}
% % \caption{XAMBA end-to-end methodology highlighting the key steps}\label{fig:xamba_e2e}
% % \end{figure}


% % \begin{table}[h!]
% % \centering
% % \caption{Overview of XAMBA techniques and their corresponding optimization targets, including compute acceleration, memory bandwidth improvement, and data transfer reduction.}
% % \label{tab:xamba_techniques}
% % \begin{tabular}{|l|p{10cm}|}
% % \hline
% % \textbf{XAMBA Technique} & \textbf{Targeted Area} \\ \hline
% % CumBA & Accelerates compute with matrix multiplication for CumSum and boosts memory bandwidth by improving data reuse and reducing redundant memory accesses. \\ \hline
% % ReduBA & Enhances compute with matrix-vector multiplication for ReduceSum and reduces memory traffic by reusing the reduce sum mask. \\ \hline
% % ActiBA & Speeds up compute by offloading activation functions to specialized hardware and reduces memory overhead by avoiding intermediate output storage. \\ \hline
% % \end{tabular}
% % \end{table}



% \subsection{CumBA: Optimizing CumSum Execution on NPUs Using Matrix Multiplication}

% As highlighted in Figure~\ref{fig:motivation_exec_lat_brkdwn}, one of the major bottlenecks in executing Mamba-2 on NPUs is the cumulative sum (CumSum) operation, which suffers from high latency due to its sequential execution on the DSP. Figure~\ref{fig:cumba} provides a detailed view of this process, where CumSum is performed on a matrix of shape $m \times n$, with the summation occurring along the $m$-axis. Given that the DSP is equipped with an $n$-width vector adder, the output for each column is computed sequentially over $m$ cycles. For higher-dimensional tensors, the CumSum operation must be broken down into smaller chunks and processed sequentially, further exacerbating the latency. This approach also causes a significant increase in memory traffic and inefficient data reuse, particularly for tensors whose dimensions exceed the local SRAM capacity, as intermediate results must be written back and forth to off-chip memory.

% To overcome these inefficiencies, XAMBA introduces CumBA, a novel technique that remaps the CumSum operation to a matrix multiplication (MatMul) utilizing the data-parallel processing capabilities of the NPU’s Data Processing Unit (DPU). The DPU, an array of high-frequency Multiply-and-Accumulate (MAC) processing elements, is designed to handle matrix operations with significantly greater parallelism and efficiency compared to the sequential DSP. As shown in Figure~\ref{fig:cumba}, CumBA precomputes a mask during compile time with a shifting and saturating pattern of 1’s tailored to the CumSum operation. This mask enables the transformation of CumSum into a MatMul by reshaping the input tensor as needed.

% This remapping ensures the computation is performed in parallel, leveraging the DPU’s ability to execute multiple operations simultaneously. Additionally, CumBA improves data reuse through DPU-based stencils and eliminates redundant memory reads and writes to SRAM, addressing the inefficiencies of DSP-based execution. The DPU processes MatMul in a tiled manner, further enhancing data reuse within local register files and minimizing costly on-chip SRAM accesses. The result is an accurate output with significantly reduced execution latency. By tackling both computational and memory inefficiencies of sequential CumSum, CumBA achieves substantial improvements in performance and resource utilization on NPUs.


% % \begin{figure}[t!]
% % \begin{center}
% % \includegraphics[width=\columnwidth]{Figures/CumSum_as_MatMul.pdf}
% % \end{center}
% % \caption{CumBA: remapping the CumSum operation to matrix multiplication (MatMul) for efficient execution on NPUs}\label{fig:cumba}
% % \end{figure}


% The $\sim$50\% zeros in the CumSum mask in CumBA, represented as a lower triangular binary matrix, present an opportunity for significant memory and compute optimizations. As shown in Figure~\ref{fig:cumba_zvc}, all elements above the triangular are zero, making the mask highly sparse. By employing Zero Value Compression (ZVC), the storage requirements for the mask are greatly reduced, as only the non-zero elements are stored. This compression also minimizes memory traffic by reducing the volume of data transferred between memory and processing units. Furthermore, the NPU’s support for compute skipping using sparsity bitmaps enables additional acceleration by bypassing computations for zero values. Both Lunar Lake~\cite{lnl} and Meteor Lake~\cite{mtl} architectures support ZVC and sparse compute capabilities, making them well-suited for efficiently handling such workloads. Figure~\ref{fig:cumba_zvc} illustrates this process, showcasing the ZVC-compressed mask stored in memory and the NPU datapath with 2-sided sparsity acceleration logic. The sparsity bitmap allows the NPU to efficiently skip unnecessary operations, leveraging both weight and activation sparsity. This dual approach not only reduces memory usage but also decreases compute operations, accelerating the execution of CumSum and further enhancing energy efficiency for SSMs.




% \subsection{ReduBA: Addressing Sequential Execution Bottlenecks for ReduceSum on NPUs}
% As illustrated in Figure~\ref{fig:motivation_exec_lat_brkdwn}, another significant bottleneck in the execution of Mamba-2 on NPUs is the ReduceSum operation. This inefficiency stems from its sequential execution on the DSP. Figure~\ref{fig:reduba} shows the execution flow of ReduceSum for a tensor of shape $m \times n$ processed along the $m$-axis. The DSP, equipped with an $n$-width vector adder, produces the output over $m$ cycles, as depicted in the figure. For tensors with higher dimensions, the ReduceSum operation is divided into smaller workloads, further increasing execution time. Additionally, for tensors with shapes exceeding the vector width of the DSP, multiple intermediate results must be written to memory, leading to high memory traffic and inefficient local SRAM utilization.

% To mitigate this latency and memory inefficiency, XAMBA introduces ReduBA. As shown in Figure~\ref{fig:reduba}, ReduBA leverages the data-parallel architecture of the Data Processing Unit (DPU) to compute ReduceSum as a matrix-vector multiplication (MVM). DPUs, which consist of arrays of Multiply-and-Accumulate (MAC) processing elements, operate at a higher frequency and support parallel computation more effectively than DSPs. In ReduBA, a vector mask is precomputed at compile time for the ReduceSum operation. This mask captures the reduction pattern, allowing the ReduceSum operation to be reformulated as an MVM. The input tensor is reshaped as required, and the operation is then mapped to the DPU.

% By leveraging matrix-vector multiplication (MVM), ReduBA achieves superior data reuse compared to the matrix-matrix operations in CumBA. Specifically, the reduce sum mask is reused across all operations, significantly reducing memory traffic and effectively increasing the available memory bandwidth. Additionally, ReduBA utilizes multiple MAC units within the DPU and employs a tiled computation strategy, further enhancing data reuse and minimizing on-chip memory accesses. These optimizations collectively result in reduced latency and optimized memory usage for ReduceSum operations, significantly improving the execution efficiency of Mamba-2 on NPUs.


% % \begin{figure}[t!]
% % \begin{center}
% % \includegraphics[width=\columnwidth]{Figures/ReduceSum_as_MatMul.pdf}
% % \end{center}
% % \caption{ReduBA: transforming the ReduceSum operation into matrix-vector multiplication (MVM) to leverage the DPU's parallel processing capabilities}\label{fig:reduba}
% % \end{figure}


% \subsection{ActiBA: Efficient Mapping of Activation Functions Using Software Programmable Lookup Tables}
% As illustrated in Figure~\ref{fig:motivation_exec_lat_brkdwn}, two of the most significant bottlenecks in Mamba’s execution on NPUs are the Swish (SiLU) and Softplus activation functions. Figure~\ref{fig:actiba} demonstrates how these activation functions are processed in a sequential loop on the DSP, where each assembly instruction exhibits varying execution times based on complexity. This sequential processing results in high latency, contributing significantly to overall inefficiencies. 

% \[
% \text{SiLU}(x) = x \cdot \left( \frac{1}{1 + e^{-x}} \right), \quad \text{Softplus}(x) = \frac{1}{\beta} \log \left(1 + e^{\beta x}\right)
% \]

% However, the Swish and Softplus activation functions exhibit linear behavior over most of their domain, except for regions near the origin. This property enables their approximation using piecewise linear functions with minimal computational overhead. To address these bottlenecks, XAMBA introduces ActiBA, which leverages piecewise linear approximation to compute these functions efficiently. ActiBA uses more linear segments near the origin, where the functions are highly nonlinear, and fewer segments farther from the origin, where they become nearly linear.

% Most modern NPUs, including the Intel NPUs~\cite{lnl, mtl} considered in this work, incorporate Software Programmable Lookup Tables (Spr-LUTs) as part of the Data Processing Unit (DPU). Spr-LUTs are specialized hardware designed to approximate nonlinear activation functions directly on the main compute pipeline of the NPU. Unlike traditional approaches that compute these activations on a separate DSP, Spr-LUTs offer a significant performance advantage by avoiding additional communication overhead and exploiting the higher clock frequencies of DPUs.

% In ActiBA, the slopes and intercepts of the piecewise linear segments for Swish and Softplus are precomputed and programmed into the look-up table within the Spr-LUT during compile time (as shown in Figure~\ref{fig:actiba}). At runtime, as the output of the preceding layer is drained from the DPU, it directly passes through the Arithmetic Unit (AU), also known as the Post Processing Element (PPE). During this drain phase, the activation computations are performed in a fused manner on the DPU, leveraging the precomputed slopes and intercepts stored in the Spr-LUT. This approach eliminates the need to offload activation functions to the DSP, thereby avoiding the latency and inefficiencies associated with sequential DSP execution.

% Furthermore, since ActiBA performs activation computations during the drain phase, it eliminates the need to store the intermediate outputs of the preceding layer in memory and subsequently reload them for activation processing (also known as vertical fusion). This significantly reduces memory access overhead, improves memory bandwidth utilization, and enhances overall dataflow efficiency. As these operations are simple linear computations integrated into the data drain process, the execution latency is further minimized. By addressing both computational and memory inefficiencies, ActiBA achieves a substantial reduction in end-to-end latency for Mamba-based models without compromising accuracy.

% % \begin{figure}[t!]
% % \begin{center}
% % \includegraphics[width=\columnwidth]{Figures/SiLU_SoftPlus_SptLUT.pdf}
% % \end{center}
% % \caption{ActiBA: optimize the Swish and Softplus activation functions, using piecewise linear approximations and leveraging Spr-LUTs for efficient computation on NPUs}\label{fig:actiba}
% % \end{figure}


