\section{Prior Arts}\label{sec_prior_art}
Several efforts have been made to enable and optimize State Space Models (SSMs) for execution on hardware accelerators like GPUs. These works have made progress in improving the computational efficiency of state-space models for sequence modeling. However, they face significant limitations, which are addressed by the proposed XAMBA approach. Yang et al. (2019)~\cite{ssm_gpu} presented a GPU-optimized framework for accelerating state-space models, focusing on efficiently handling sequential data by optimizing memory transfers and reducing overheads. While this work demonstrated improved throughput, it still relied on GPU-centric architectures, which are not ideal for the memory and throughput challenges of resource-constrained edge devices. Moreover, the solution did not fully explore the specific needs of SSMs in terms of memory management and sequential computation, which are crucial for NPUs. This gap is significant because NPUs are optimized for high-throughput, parallelized computation, which requires more specialized mapping techniques for sequential models like SSMs. Wang et al. (2018)~\cite{rnn_npu} explored deploying LSTM models on NPUs, proposing custom kernels and memory optimization strategies that reduce off-chip memory access and improve data locality. Their work demonstrated improvements in the execution of recurrent models on NPUs, focusing on reducing latency and optimizing memory usage. However, their focus was primarily on traditional recurrent networks (LSTMs) rather than state-space models. The sequential nature of SSMs and the unique dataflow patterns they require for efficient execution on NPUs were not fully addressed. This is a limitation because SSMs, like Mamba, require novel kernel designs and optimizations specific to their compact representations and sequential computations. Without such optimizations, NPUs cannot fully exploit the advantages of SSMs, leading to suboptimal performance. Kiamarzi et al. (2021)~\cite{ssm_fpga} proposed a systematic approach for implementing state-space models in register transfer level (RTL), with a focus on neural network implementation. Their method can be applied to linear/nonlinear and time-varying/time-invariant systems, offering a framework for hardware synthesis of state-space equations. While this approach provides a foundation for hardware implementation, it does not specifically address the unique challenges of deploying SSMs on NPUs, such as dataflow alignment and memory optimization. Additionally, the approach may not fully leverage the parallel processing capabilities of NPUs, which are crucial for achieving high performance in resource-constrained environments. Behrouz et al. (2024)~\cite{mambamixer} introduced MambaMixer, an efficient selective state-space model with dual token and channel selection. This architecture aims to enhance the performance of SSMs by selectively mixing information across tokens and channels. While MambaMixer offers improvements in model efficiency and performance, it does not specifically address the hardware optimization aspects required for efficient execution on NPUs. The integration of such models into hardware accelerators necessitates specialized kernel designs and memory management strategies to fully exploit the capabilities of NPUs. Kiamarzi et al. (2024)~\cite{marca} proposed MARCA, a reconfigurable accelerator architecture designed to address the incompatibilities between Tensor Core-based architectures and Mamba's compute patterns. MARCA introduces a reduction-alternative PE array architecture, reusable nonlinear function units, and advanced buffer management strategies to address the bottlenecks of Mamba computations. While MARCA offers significant performance and energy efficiency improvements for SSMs on high-performance hardware, it does not directly address the unique kernel optimizations and memory alignment strategies critical for NPUs. Additionally, its focus on reconfigurable architecture may introduce overheads unsuitable for resource-constrained edge devices. These limitations highlight the need for more specialized approaches for deploying SSMs on NPUs. These prior works have made valuable contributions to optimizing sequential models for hardware accelerators. However, none of them directly target the unique characteristics of SSMs, especially on NPUs. The proposed XAMBA approach addresses these limitations by introducing novel kernel mappings, optimizing dataflow for sequential computations, and applying efficient memory management techniques specifically for SSMs. By leveraging the full potential of NPUs, XAMBA achieves superior performance and energy efficiency for state-space models, addressing challenges that past work has left unresolved.