\section{Experimental Methodology}\label{sec_expt_meth}
\textbf{Networks and Datasets:}  
Experiments use pretrained state-space models from HuggingFace, specifically \texttt{mamba-130m-hf} and \texttt{mamba2-130m-hf}, with fixed input tokens of 4.
\textbf{Preprocessing and Conversion:}  
The models are converted from PyTorch to ONNX. They are then converted to OpenVINO~\cite{openvino} IR files (compressing weights to FP16 precision) and compiled into a binary using the OpenVINO NPU compiler. CumBA and ReduBA optimizations are applied during conversion, and ActiBA is emulated by replacing activation functions with ReLU.
% \textbf{Baseline:}  
% Comparisons are made between original models (no optimizations) and optimized models using CumBA, ReduBA, and ActiBA.
\textbf{Platform:}  
Experiments run on an Intel\textregistered\ Core\texttrademark\ Ultra Series 2~\cite{lnl} platform (ASUS Zenbook S 14, 16GB RAM, 256V NPU).
\textbf{Performance Evaluation:}  
Models are evaluated using OpenVINO’s \texttt{benchmark\_app} tool, focusing on inference latency, with optimizations (CumBA, ReduBA, ActiBA) affecting NPU execution efficiency.
\textit{All results were collected using public frameworks (OpenVINO, PyTorch) and are replicable via the provided code (see abstract).}
% \textbf{Results and Replication:}  
% Results, collected via OpenVINO, HWINFO, and PyTorch, are available for replication with optimized models (link in abstract).



% \textbf{Networks and Datasets:}
% The experiments are conducted using pretrained state-space models (SSMs) available from HuggingFace, specifically the models \texttt{mamba-130m-hf} and \texttt{mamba2-130m-hf}. These models are designed to work with a specific input shape (4) and precision (FP16), which are defined before deployment on the NPU.
% \textbf{Preprocessing and Conversion:}
% The models are first exported from PyTorch to ONNX format, and during this conversion, weight compression is applied to reduce the model size. Following this, the models are converted into OpenVINO IR (Intermediate Representation) files, which consist of \texttt{Model.xml} and \texttt{Model.bin}. These IR files are then compiled into a hardware-optimized model blob using the NPU compiler and NPU UMD driver. During the compilation step, optimizations such as CumBA and ReduBA are applied to the models.
% To simulate the effect of ActiBA, the activation functions in the models are replaced with ReLU, and the performance is measured accordingly.
% \textbf{Baseline:}
% The baseline for comparison includes the original models without any optimizations, as well as models using the standard activation functions (before applying ActiBA). The performance is compared between these baseline models and the optimized models that leverage CumBA, ReduBA, and ActiBA.
% \textbf{Platform:}
% Experiments were conducted on an Intel\textregistered\ Core\texttrademark\ Ultra Series 2~\cite{lnl} platform, specifically an ASUS Zenbook S 14 with 16GB RAM and a 256V NPU. The NPU is used for deploying the models and evaluating their performance.
% \textbf{Performance Evaluation:}
% The performance of the models is evaluated using OpenVINO’s \texttt{benchmark\_app} tool, which configures performance hints and input/output precision. The evaluation focuses on latency and throughput, with specific attention given to how the optimizations (CumBA, ReduBA, and ActiBA) impact the execution efficiency on the NPU.
% \textbf{Results and Replication:}
% All results presented are collected using public frameworks, including OpenVINO, HWINFO, and PyTorch. These results can be replicated by using the optimized models provided at the link (referenced in the abstract).

% Fig.~\ref{fig:xamba_e2e} illustrates the experimental workflow for deploying pretrained state-space models (SSMs), such as those from HuggingFace/Transformers, on an NPU (Neural Processing Unit) using OpenVINO. The process begins with the pretrained PyTorch models, where the input shape and precision (e.g., FP16) are defined. These models are converted into OpenVINO IR (Intermediate Representation) by exporting from PyTorch to ONNX format and then compressing the weights during the conversion process. 
% The OpenVINO IR files, represented as \texttt{Model.xml} and \texttt{Model.bin}, are compiled into a hardware-optimized model blob using the NPU compiler and NPU UMD driver. During this step, CumBA and ReduBA are applied for efficient execution. Specialized hardware features, such as Spr-LUT, are leveraged for piecewise linear execution of computationally expensive activation functions like Softplus and SiLU, ensuring faster inference. 
% The compiled model blob is loaded into the OpenVINO Inference Engine, which interfaces directly with the NPU for deployment.
