\newpage
\section{Appendix}

\begin{figure}[b!]
\begin{center}
\includegraphics[width=0.65\columnwidth]{Figures/Mamba_vs_Mamba_2.pdf}
\end{center}
\caption{Mamba~\cite{mamba} vs. Mamba-2~\cite{mamba2} showcasing structural differences and operational trade-offs. Mamba-2 simplifies the design but suffers from decreased performance on Intel\textregistered\ AI PCs~\cite{lnl}~\cite{mtl} due to less hardware-efficient operations.}\label{fig:mamba_vs_mamba2}
\end{figure}

\subsection{Mamba vs. Mamba-2}\label{subsec:mamba1_vs_2}
This section presents a comparative analysis of the Mamba~\cite{mamba} and Mamba-2~\cite{mamba2} architectures, focusing on their structural and operational distinctions. Fig.~\ref{fig:mamba_vs_mamba2} compares the architectures and operations of the Mamba and Mamba-2 blocks, highlighting their structural and computational differences. Mamba-1 focuses on leveraging structured state-space models (SSMs) for long-range sequence modeling through memory initialization using a high-order polynomial projection operator (HiPPO)~\cite{hippo}, a selection mechanism, and hardware-aware computing. It employs a sequential processing structure where linear projections are applied to the input sequence and state-space parameters $[A, B, C]$ in stages, with selective scanning through the SSM unit to map inputs $X$ to outputs $Y$. The Mamba-1 block relies on skip connections to reuse features and mitigate performance degradation during training. Mamba-2 simplifies this design by introducing the structured state-space duality (SSD) framework, which connects SSM and attention mechanisms. Instead of sequential linear projections, the SSD layer processes $[X, A, B, C]$ simultaneously using a single projection, akin to generating $Q, K, V$ in standard attention mechanisms. This reduces computational overhead and accelerates processing. Additionally, Mamba-2 adds a normalization layer after the skip connection to improve training stability. 

Fig.~\ref{fig:mamba_vs_mamba2} also highlights the operator~\cite{openvino_ops} differences between Mamba and Mamba-2, emphasizing the trade-offs in performance on Intel\textregistered\ AI PCs~\cite{lnl}~\cite{mtl}. Mamba-2 introduces new operators like \texttt{CumSum} and \texttt{ReduceSum}, while reducing gather operations (from 18 to 7). However, computationally expensive operations such as \texttt{power} and \texttt{sqrt} increase, and data-parallel operations critical for DPUs, like \texttt{MatMul} (reduced from 8 to 2) and \texttt{Add} (from 11 to 10), are decreased. These changes negatively impact Mamba-2's performance on Intel\textregistered\ AI PCs, as it shifts away from hardware-optimized operations. Despite structural simplifications through its SSD framework, Mamba-2 performs worse than Mamba on AI PCs due to increased reliance on less hardware-efficient operators.


% \subsection{Mamba vs. Mamba-2}
% Fig.~\ref{fig:mamba_vs_mamba2} compares the architectures and operations of the Mamba~\cite{mamba} and Mamba-2~\cite{mamba2} blocks, highlighting their structural and computational differences. Mamba-1 focuses on leveraging structured state-space models (SSMs) for long-range sequence modeling through memory initialization using a high-order polynomial projection operator (HiPPO)~\cite{hippo}, a selection mechanism, and hardware-aware computing. It employs a sequential processing structure where linear projections are applied to the input sequence and state-space parameters $[A, B, C]$ in stages, with selective scanning through the SSM unit to map inputs $X$ to outputs $Y$. The Mamba-1 block relies on skip connections to reuse features and mitigate performance degradation during training. Mamba-2~\cite{mamba2} simplifies this design by introducing the structured state-space duality (SSD) framework, which connects SSM and attention mechanisms. Instead of sequential linear projections, the SSD layer processes $[X, A, B, C]$ simultaneously using a single projection, akin to generating $Q, K, V$ in standard attention mechanisms. This reduces computational overhead and accelerates processing. Additionally, Mamba-2 adds a normalization layer after the skip connection to improve training stability. 

% Fig.~\ref{fig:mamba_vs_mamba2} also highlights the operational differences between Mamba and Mamba-2, emphasizing the trade-offs in performance on Intel AI PCs~\cite{lnl}~\cite{mtl}. Mamba-2 introduces new operators like \texttt{CumSum} and \texttt{ReduceSum}, while reducing gather operations (from 18 to 7). However, computationally expensive operations such as \texttt{power} and \texttt{sqrt} increase, and data-parallel operations critical for DPUs, like \texttt{MatMul} (reduced from 8 to 2) and \texttt{Add} (from 11 to 10), are decreased. These changes negatively impact Mamba-2's performance on Intel AI PCs, as it shifts away from hardware-optimized operations. Despite structural simplifications through its SSD framework, Mamba-2 performs worse than Mamba on AI PCs due to increased reliance on less hardware-efficient operators.