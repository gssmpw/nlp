\section{Results}\label{sec_results}
\begin{figure}[t!]
\begin{center}
\includegraphics[width=\columnwidth]{Plots/Results_2.pdf}
\end{center}
\caption{Latency reduction for Mamba and Mamba-2 130M models on Intel\textregistered\ Core\texttrademark\ Ultra Series 2~\cite{lnl} NPU with XAMBA optimizations (CumBA, ReduBA, and ActiBA).}\label{plot:result}
\end{figure}



% \begin{table}[t]
% \centering
% \scriptsize % Smaller font size for compactness
% \setlength{\tabcolsep}{3pt} % Reduce column padding
% \begin{tabular}{@{}l l S[table-format=2.2] S[table-format=2.2] S[table-format=2.1] S[table-format=2.1] S[table-format=2.1] S[table-format=2.1] S[table-format=2.1] S[table-format=2.1] S[table-format=2.1] S[table-format=2.1]@{}}
% \toprule
% \textbf{Model} & \textbf{Token} & \textbf{\shortstack{PILE\\PPL $\downarrow$}} & \textbf{\shortstack{LAMBADA\\PPL $\downarrow$}} & \textbf{\shortstack{LAMBADA\\ACC $\uparrow$}} & \textbf{\shortstack{Hella-\\Swag $\uparrow$}} & \textbf{\shortstack{PIQA\\ACC $\uparrow$}} & \textbf{\shortstack{ARC-E\\ACC $\uparrow$}} & \textbf{\shortstack{ARC-C\\ACC $\uparrow$}} & \textbf{\shortstack{Winogrande\\ACC $\uparrow$}} & \textbf{\shortstack{Open-\\bookQA $\uparrow$}} & \textbf{\shortstack{Avg\\ACC $\uparrow$}} \\
% \midrule
% Hybrid H3-130M & GPT2 & {---} & 89.48 & 25.8 & 31.7 & 64.2 & 44.4 & \textbf{24.2} & 50.6 & 27.0 & 38.2 \\
% Pythia-160M & NeoX & 29.64 & 38.10 & 33.0 & 30.2 & 61.4 & 43.2 & 24.1 & 51.9 & 29.2 & 39.0 \\
% Lambda-130M & NeoX & 10.56 & \textbf{16.07} & \textbf{44.3} & 35.2 & 64.5 & \textbf{48.0} & \textbf{24.2} & 51.9 & 28.8 & 42.4 \\
% \textbf{Lambda-2-130M} & NeoX & \textbf{10.48} & 16.86 & 43.9 & \textbf{35.3} & \textbf{64.9} & 47.4 & \textbf{24.2} & \textbf{52.1} & \textbf{30.6} & \textbf{42.6} \\
% \bottomrule
% \end{tabular}
% \caption{\small Model performance comparison across benchmarks. Arrows indicate direction of better performance. Bold indicates best in column.}
% \label{tab:model_comparison}
% \end{table}



% \begin{table}[t]
% \caption{\small \textcolor{red}{Quality of XAMBA model variants across benchmarks. Arrows indicate direction of better performance. Groups are separated by model size.}}
% \label{tab:mamba_results}
% \centering
% \scriptsize
% \setlength{\tabcolsep}{3.5pt}
% \begin{tabular}{@{}l S[table-format=1.2] *{6}{S[table-format=2.1]} S[table-format=2.1]@{}}
% \toprule
% \textbf{Model} & \textbf{\shortstack{LAMBADA\\PPL $\downarrow$}} & \textbf{\shortstack{HellaSwag\\ACC $\uparrow$}} & \textbf{\shortstack{PIQA\\ACC $\uparrow$}} & \textbf{\shortstack{ARC-E\\ACC $\uparrow$}} & \textbf{\shortstack{ARC-C\\ACC $\uparrow$}} & \textbf{\shortstack{OpenbookQA\\ACC $\uparrow$}} & \textbf{\shortstack{Winogrande\\ACC $\uparrow$}} & \textbf{\shortstack{Avg\\ACC $\uparrow$}} \\
% \midrule
% Mamba-130M & 16.07 & 35.3 & 64.5 & 48.0 & 24.2 & 28.4 & 51.9 & 42.0 \\
% Mamba-130M-PLU SiLU \& Softplus & {---} & 31.8 & 59.2 & 47.1 & 25.3 & 28.2 & 52.3 & 40.7 \\
% Mamba-130M-PLU SiLU & 16.06 & 35.2 & 64.6 & 47.9 & 24.2 & 28.4 & 51.9 & 42.0 \\
% Mamba-130M-PLU Softplus & {---} & 31.8 & 59.3 & 47.1 & 25.3 & 28.2 & 52.3 & 40.7 \\
% \midrule
% Mamba-370M & 8.14 & 46.5 & 69.5 & 55.0 & 27.9 & 30.8 & 52.1 & 47.0 \\
% Mamba-370M-PLU SiLU & 8.14 & 46.5 & 69.5 & 55.1 & 28.0 & 30.8 & 52.1 & 47.0 \\
% Mamba-370M-PLU Softplus & 8.13 & 46.5 & 69.4 & 54.9 & 28.0 & 30.8 & 52.0 & 46.9 \\
% Mamba-370M-PLU SiLU \& Softplus & 8.13 & 46.5 & 69.5 & 54.9 & 28.0 & 30.8 & 52.0 & 46.9 \\
% \midrule
% Mamba-790M & 6.01 & 55.1 & 72.1 & 61.2 & 29.4 & 34.2 & 56.5 & 51.4 \\
% Mamba-790M-PLU SiLU & 6.02 & 55.1 & 72.1 & 61.2 & 29.4 & 34.2 & 56.4 & 51.4 \\
% Mamba-790M-PLU Softplus & 6.01 & 55.1 & 72.2 & 61.2 & 29.4 & 34.0 & 56.4 & 51.4 \\
% Mamba-790M-PLU SiLU \& Softplus & 6.02 & 55.1 & 72.1 & 61.2 & 29.4 & 34.0 & 56.4 & 51.4 \\
% \midrule
% Mamba-1.4B & 5.04 & 59.1 & 74.2 & 65.5 & 32.9 & 36.4 & 57.3 & 54.2 \\
% Mamba-1.4B-PLU SiLU & 5.04 & 59.1 & 74.2 & 65.5 & 32.9 & 36.4 & 57.3 & 54.2 \\
% Mamba-1.4B-PLU Softplus & 5.04 & 59.1 & 74.2 & 65.6 & 32.8 & 36.4 & 57.1 & 54.2 \\
% Mamba-1.4B-PLU SiLU \& Softplus & 5.04 & 59.1 & 74.2 & 65.5 & 32.6 & 36.4 & 57.2 & 54.1 \\
% \midrule
% Mamba-2.8B & 4.23 & 66.2 & 75.2 & 69.7 & 36.3 & 39.6 & 60.0 & 57.8 \\
% Mamba-2.8B-PLU SiLU & 4.23 & 66.1 & 75.2 & 69.6 & 36.3 & 39.6 & 60.0 & 57.8 \\
% Mamba-2.8B-PLU Softplus & 4.23 & 66.2 & 75.2 & 69.7 & 36.3 & 39.4 & 60.1 & 57.8 \\
% Mamba-2.8B-PLU SiLU \& Softplus & 4.23 & 66.2 & 75.2 & 69.6 & 36.3 & 39.6 & 60.1 & 57.8 \\
% \bottomrule
% \end{tabular}
% \end{table}




\begin{table}[t]
\caption{\small \textcolor{black}{Quality of XAMBA model variants across benchmarks. Arrows indicate direction of better performance. Groups are separated by model size.}}
\label{tab:mamba_results}
\centering
\scriptsize
\setlength{\tabcolsep}{3.5pt}
\begin{tabular}{@{}l S[table-format=1.2] *{6}{S[table-format=2.1]} S[table-format=2.1]@{}}
\toprule
\textbf{Model} & \textbf{\shortstack{LAMBADA\\PPL $\downarrow$}} & \textbf{\shortstack{HellaSwag\\ACC $\uparrow$}} & \textbf{\shortstack{PIQA\\ACC $\uparrow$}} & \textbf{\shortstack{ARC-E\\ACC $\uparrow$}} & \textbf{\shortstack{ARC-C\\ACC $\uparrow$}} & \textbf{\shortstack{OpenbookQA\\ACC $\uparrow$}} & \textbf{\shortstack{Winogrande\\ACC $\uparrow$}} & \textbf{\shortstack{AVG.\\ACC $\uparrow$}} \\
\midrule
Mamba-130M & 16.07 & 24.23 & 35.25 & 51.85 & 47.98 & 28.40 & 64.47 & 42.03 \\
Mamba-130M-PLU SiLU \& Softplus & {---} & 25.34 & 31.83 & 52.33 & 47.14 & 28.20 & 59.19 & 40.67 \\
Mamba2-130M & 16.79 & 24.15 & 35.27 & 52.80 & 47.39 & 30.60 & 64.91 & 42.52 \\
Mamba2-130M-PLU SiLU \& SoftPlus & 16.78 & 24.15 & 35.27 & 52.80 & 47.43 & 30.60 & 64.91 & 42.53 \\
\midrule
Mamba-370M & 8.14 & 27.90 & 46.48 & 52.09 & 54.97 & 30.80 & 69.48 & 46.95 \\
Mamba-370M-PLU SiLU \& Softplus & 8.13 & 27.99 & 46.49 & 52.01 & 54.92 & 30.80 & 69.48 & 46.95 \\
Mamba2-370M & 7.98 & 26.71 & 46.94 & 53.67 & 54.84 & 32.40 & 70.51 & 47.51 \\
Mamba2-370M-PLU SiLU \& SoftPlus & 7.98 & 26.71 & 46.94 & 53.83 & 54.84 & 32.40 & 70.51 & 47.54 \\
\midrule
Mamba-790M & 6.01 & 29.35 & 55.07 & 56.51 & 61.24 & 34.20 & 72.14 & 51.42 \\
Mamba-790M-PLU SiLU \& Softplus & 6.02 & 29.44 & 55.06 & 56.35 & 61.24 & 34.00 & 72.14 & 51.37 \\
Mamba2-780M & 5.85 & 28.50 & 54.91 & 56.91 & 60.98 & 36.20 & 72.03 & 51.59 \\
Mamba2-780M-PLU SiLU \& SoftPlus & 5.85 & 28.50 & 54.92 & 56.91 & 61.03 & 36.20 & 72.03 & 51.60 \\
\midrule
Mamba-1.4B & 5.04 & 32.94 & 59.11 & 57.30 & 65.53 & 36.40 & 74.16 & 54.24 \\
Mamba-1.4B-PLU SiLU \& Softplus & 5.04 & 32.59 & 59.07 & 57.22 & 65.45 & 36.40 & 74.16 & 54.15 \\
Mmaba2-1.3B & 5.02 & 33.11 & 59.94 & 58.25 & 64.14 & 37.80 & 73.23 & 54.41 \\
Mmaba2-1.3B-PLU SiLU \& SoftPlus & 5.02 & 33.19 & 59.93 & 58.33 & 64.14 & 37.80 & 73.23 & 54.44 \\
\midrule
Mamba-2.8B & 4.23 & 36.26 & 66.16 & 59.98 & 69.65 & 39.60 & 75.24 & 57.82 \\
Mamba-2.8B-PLU SiLU \& Softplus & 4.23 & 36.26 & 66.16 & 60.14 & 69.61 & 39.60 & 75.24 & 57.84 \\
Mmaba2-2.7B & 4.09 & 36.26 & 66.58 & 61.72 & 69.57 & 38.80 & 76.39 & 58.22 \\
Mmaba2-2.7B-PLU SiLU \& SoftPlus & 4.09 & 36.26 & 66.60 & 61.80 & 69.61 & 38.80 & 76.39 & 58.24 \\
\bottomrule
\end{tabular}
\end{table}



We evaluate XAMBA's effectiveness in reducing inference latency for Mamba-based models.
Figure~\ref{plot:result}(a) shows the average latency of a single-block Mamba-2 130M model on the Intel\textregistered\ Core\texttrademark\ Ultra Series 2 NPU. The CumBA technique reduces latency by $2.7 \times$ over the baseline, while ReduBA provides a $1.2 \times$ reduction. \textit{Reduced inference latency directly translates to improved Tokens/second.} Combining CumBA and ReduBA yields a $4.8 \times$ reduction, demonstrating the performance gains of XAMBA.
Figure~\ref{plot:result}(b) presents the normalized inference latency breakdown for the Mamba-2 130M model, comparing the baseline with the optimized CumBA technique. In the baseline, CumSum operations contribute over 50\% of the total latency. CumBA reduces this by transforming CumSum into matrix multiplication using a precomputed mask, achieving a $2.7 \times$ reduction in latency.
Figure~\ref{plot:result}(c) shows the first inference latency breakdown for the Mamba 130M model with ActiBA optimizations. Using PLU for SoftPlus in a piecewise linear fashion reduces latency by $1.2 \times$. Further reductions ($1.8 \times$) are achieved with SiLU, leading to a total $2.6 \times$ latency reduction with negligible quality loss when both SoftPlus and SiLU are mapped to PLU.
% Table~\ref{tab:mamba_results} demonstrates minimal quality tradeoffs for ActiBA: for mamba-130M average accuracy drops merely from 42.0\% to 40.7\% for the PLU SiLU+Softplus variant. This pattern holds for larger models, with Mamba-1.4B showing just a 0.1\% decrease (54.2\% $\to$ 54.1\%) and Mamba-2.8B maintaining identical 57.8\% accuracy post-optimization. The maximal 1.3\% accuracy reduction (for 130M) demonstrates favorable quality-efficiency tradeoffs across all scales.
Table~\ref{tab:mamba_results} demonstrates that ActiBA's hardware-friendly approximations introduce only negligible quality tradeoffs. For the original Mamba architecture, the largest accuracy drop occurs with the 130M model (42.03\% $\to$ 40.67\%, a 1.36\% reduction), while larger models like the 1.4B version show remarkably stable performance (54.24\% $\to$ 54.15\%, just 0.09\% difference). The Mamba2 variants exhibit even greater robustness to approximations---the 130M model's accuracy remains virtually unchanged (42.52\% $\to$ 42.53\%), and interestingly, the 2.7B model shows a slight improvement (58.22\% $\to$ 58.24\%). These results collectively confirm that ActiBA's optimizations successfully maintain model quality, with the maximum observed degradation being well under 1.5\% even for the smallest model, while most variants experience changes of less than 0.1\%.
We set a KPI target of 50 Tokens/s, following MobileLLM-125M~\cite{mobilellm} (a comparable LLM), to ensure sufficient client-side responsiveness. With ActiBA optimizations, the Mamba-130M model's decoding performance increases from 100 Tokens/s to 260 Tokens/s, surpassing the 50 Tokens/s KPI target.
XAMBA’s optimizations, demonstrated on the 130M models, extend to larger models and inputs having similar bottlenecks, enabling comparable or greater speed-ups. While performance gains vary by model size and workload, the core principles hold. \textit{Ongoing work will further explore scalability and optimization for larger models.}
% These results highlight ActiBA’s effectiveness in improving activation function efficiency.
% The proposed techniques, demonstrated on the 130M model, are generalizable to larger models and input sizes, offering potential performance gains. \textit{This work is ongoing, and further experiments will explore scalability and performance in larger models.}


% While we have demonstrated the effectiveness of the proposed techniques using the 130M model, it is important to note that the same bottlenecks are present in larger models and with larger input sizes. The proposed techniques are applicable to these models as well, offering potential performance gains. The speed-up may vary depending on model size and input, but the improvements in latency and throughput, such as Tokens per second, can be similarly optimized. This work is still a work in progress, and we will continue to explore these concepts and their scalability in future experiments.


% Fig.~\ref{plot:result_cumba_reduba} presents the average latency results of a single-block Mamba-2 130M model mapped on the Intel Lunar Lake NPU. The results demonstrate the effectiveness of the proposed XAMBA optimizations. The CumBA technique achieves a $1.8 \times$ reduction in latency over the baseline by efficiently optimizing cumulative summation. ReduBA provides a $1.1 \times$ reduction over the baseline. 
% When combined, CumBA and ReduBA yield a synergistic improvement, resulting in a $2.3 \times$ latency reduction compared to the baseline. These results highlight the significant performance benefits of XAMBA on Intel NPUs, enabling faster inference.

% % \subsection{Normalized First Inference Latency Breakdown for Mamba-2 130M Model}
% Fig.~\ref{plot:result_cumba_brkdwn} illustrates the normalized first inference latency breakdown for the single block Mamba-2 130M model across various operators, comparing the baseline approach to the optimized CumBA technique. In the baseline, cumulative sum (CumSum) operations dominate, contributing to over 50\% of the total latency. This significant bottleneck arises from the sequential nature of CumSum operations, which are inefficiently mapped on NPUs. 
% By introducing CumBA, which transforms CumSum into matrix multiplication (MatMul) using a precomputed mask at compile time, the execution time of this critical operation is drastically reduced. As a result, CumBA achieves an overall $1.85 \times$ reduction in inference latency over the baseline.

% % \subsection{Normalized First Inference Latency Breakdown for Mamba 130M Model on Intel Lunar Lake NPU}
% Fig.~\ref{plot:result_actiba} illustrates the normalized first inference latency breakdown of the Mamba 130M model on Intel Lunar Lake NPU, demonstrating the impact of ActiBA optimizations. In the baseline configuration, significant latency is contributed by activation functions like SoftPlus and SiLU (Swish). 
% By implementing SoftPlus in a piecewise linear fashion using Spr-LUTs within the PPE, ActiBA achieves a $1.2 \times$ reduction in overall latency. Further optimization is observed when SiLU is mapped to the Spr-LUTs, leading to a $1.8 \times$ latency reduction. Finally, when both SoftPlus and SiLU are implemented in the Spr-LUTs, the cumulative benefits result in a $2.6 \times$ latency reduction, showcasing the effectiveness of ActiBA in addressing activation bottlenecks and improving the overall efficiency of Mamba-based models.


% \begin{figure}[t!]
% \begin{center}
% \includegraphics[width=0.7\columnwidth]{Plots/Mamba_2_Avg_Latency.pdf}
% \end{center}
% \caption{Latency reduction for Mamba-2 130M model on Intel Lunar Lake~\cite{lnl} NPU with XAMBA optimizations (CumBA and ReduBA).}\label{plot:result_cumba_reduba}
% \end{figure}

% \begin{figure}[t!]
% \begin{center}
% \includegraphics[width=0.7\columnwidth]{Plots/CumBA_breakdown.pdf}
% \end{center}
% \caption{Latency breakdown for Mamba-2 130M model, showing improved performance with the CumBA optimization.}\label{plot:result_cumba_brkdwn}
% \end{figure}



% \begin{figure}[t!]
% \begin{center}
% \includegraphics[width=0.7\columnwidth]{Plots/ActiBA_result.pdf}
% \end{center}
% \caption{Latency reduction for Mamba 130M model with ActiBA optimizations}\label{plot:result_actiba}
% \end{figure}