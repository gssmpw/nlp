\section{Background}\label{sec_background}
\subsection{Comparative Analysis of Sequential Data Models: RNNs, Transformers, and Mamba}
Recurrent Neural Networks (RNNs) have been widely used for sequential data processing due to their ability to retain internal memory. At each time step, an RNN processes a vector alongside the hidden state of the previous time step, enabling it to capture temporal dependencies. However, RNNs struggle with complex dependencies and suffer from slow training times, limiting their scalability and efficiency~\cite{rnn_limitations}. Transformers, in contrast, use self-attention mechanisms to capture global dependencies across input sequences. While transformers excel at modeling long-range dependencies and are highly parallelizable, they suffer from quadratic scaling, making them inefficient for long sequences~\cite{transformer_limitations}. 

State-Space Models (SSMs)~\cite{mamba2} offer an alternative approach, characterized by their linear structure and associative properties. Unlike RNNs, which are nonlinear, SSMs efficiently model sequential data using linear state equations. They capture temporal dependencies using a discretized model, typically employing a Zero-Order Hold (ZOH) approach to convert continuous parameters into discrete intervals. This linearity allows SSMs to perform inference more efficiently than transformers and RNNs, especially for long sequences. Mamba~\cite{mamba}, a leading SSM-based model, provides linear-time efficiency, adaptive memory compression, and significantly faster inferenceâ€”up to 5$\times$ faster than transformers. This makes Mamba particularly suitable for modern AI workloads, addressing the limitations of traditional models.

\subsection{Performance and Efficiency of Mamba Compared to Transformers}
Mamba offers significant advantages over transformer-based models in computational efficiency and task performance. It achieves higher token throughput across all batch sizes. For example, with a batch size of 16, Mamba 1.4B processes 1089 tokens/second, outperforming transformer models of similar sizes, such as Transformer 1.3B (323 tokens/second) and Transformer 6.9B (120 tokens/second)~\cite{mamba_performance}. Additionally, Mamba scales efficiently to larger batch sizes, while transformers often encounter out-of-memory (OOM) issues. 

In terms of task performance, Mamba maintains high accuracy across benchmarks. The Mamba-2.8B model achieves an average accuracy of 63.3\%, with key benchmarks including LAMBADA (69.2\%), HellaSwag (66.1\%), and PIQA (75.2\%)~\cite{mamba_benchmarks}. Mamba's efficiency stems from its avoidance of the quadratic complexity of attention mechanisms, enabling it to handle long-range dependencies with lower resource requirements. This makes Mamba ideal for low-power, latency-critical environments where energy efficiency and high throughput are essential.

\subsection{Detailed Architecture of the Mamba Block}
The Mamba block is a fundamental component of the Mamba model. It begins by processing the input sequence, which is normalized using RMS normalization to stabilize training. A skip connection bypasses the block, enabling residual learning. The normalized input undergoes transformations through projection layers and a convolutional layer, followed by the SiLU activation function to introduce non-linearity. 

At the core of the block is the selective state-space model (SSM), which operates on sequences using state equations. The state equation $h_k = A h_{k-1} + B x_k$ updates the hidden state by modeling temporal dependencies through the matrix $A$ and incorporating new input $x_k$ via matrix $B$. The output equation $y_k = C h_k$ maps the hidden state to the output using the matrix $C$, with optional augmentation by another learnable matrix $D$. The output from the selective SSM block is combined with the original input using element-wise operations, followed by another RMS normalization. Finally, the output is passed through a task-specific linear layer and a softmax function to generate predictions. This block is repeated $n$ times in the overall Mamba model, forming a modular and scalable architecture for efficient sequence modeling~\cite{mamba_architecture}.