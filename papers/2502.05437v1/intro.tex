\section{Introduction}
The total variation distance (TV-distance) is a widely used metric for quantifying the difference between two distributions. For two discrete distributions \(\mu\) and \(\nu\) defined over the sample space \(\Omega\), the TV-distance is given by  
\[
\DTV{\mu}{\nu} \defeq \frac{1}{2}\sum_{\sigma \in \Omega} \abs{\mu(\sigma) - \nu(\sigma)} = \max_{A \subseteq \Omega} \tp{\mu(A) - \nu(A)}.
\]

Alternatively, the TV-distance can be characterized as the minimum probability of \(X \neq Y\), where \(X \sim \mu\) and \(Y \sim \nu\) form a \emph{coupling} of the two distributions. These different characterizations provide a rich set of tools for analyzing the TV-distance. It is also closely related to other measures of distance between distributions, such as the Wasserstein distance and the KL-divergence~\cite{mitzenmacher2017probability}. Consequently, the TV-distance is a fundamental quantity in many applications, including randomized algorithms, statistical physics, and machine learning.


The problem of \emph{computing} the TV-distance between two distributions has garnered considerable attention in machine learning and theoretical computer science. 
A straightforward approach is to compute the TV-distance directly from its definition. This algorithm runs in \(O(|\Omega|)\) time, assuming access to the probability mass at every point in \(\Omega\). However, the problem becomes particularly interesting when the distributions have succinct representation, as the sample space \(\Omega\) can be exponentially large relative to the size of the input.
In fact, the problem is intractable for many classes of distributions. For instance, ~\cite{sahai2003complete} showed that when two distributions are specified by circuits that sample from them, deciding whether their TV-distance is small or large is complete for SZK (statistical zero-knowledge). More recently, Bhattacharyya, Gayen, Meel, Myrisiotis, Pavan, and Vinodchandran~\cite{0001GMMPV23} proved that even for pairs of product distributions, the exact computation of their TV-distance is \textbf{\#P}-complete.

%
%Recent works~\cite{0001GMM0V24,BGMMPV24ICLR} also show that the problem can be intractable for certain graphical models.
%

On the algorithmic side, the problem of \emph{approximating} the TV-distance between two distributions with \(\epsilon\)-relative error was first studied in~\cite{0001GMMPV23}, where the authors proposed an approximation algorithm for a restricted class of product distributions. Subsequently, Feng, Guo, Jerrum, and Wang~\cite{FGJW23} developed a simple FPRAS that works for general product distributions. Later, a deterministic FPTAS was also introduced for the same task~\cite{FengLL24}.
Beyond product distributions, however, the understanding of approximating the TV-distance remains very limited. The only progress was made in~\cite{0001GMM0V24}, which showed that for two Bayesian networks on DAGs, an FPRAS for the TV-distance exists if \emph{exact} probabilistic inference can be performed in polynomial time. As a consequence, their algorithm can be applied when the underlying DAG has bounded treewidth. 
However, the requirement of exact inference is a strong assumption. For many natural graphical models without the bounded treewidth property, exact inference is itself a \(\textbf{\#P}\)-complete problem.



In this paper, we further investigate the problem of approximating the TV-distance for spin systems, which are a fundamental class of undirected graphical models. Canonical examples include the hardcore model and the Ising model. Our contributions can be summarized as:

\begin{itemize}
    \item We give a new algorithm to reduce the TV-distance approximation to sampling and approximate counting. 
    As a result, our algorithm applies to a broad class of hardcore and Ising models, even if the underlying graph has unbounded treewidth.
    \item We analyze the computational complexity of approximating the TV-distance between two \emph{marginal distributions} of spin systems. We show that this problem is \(\textbf{\#P}\)-hard, even in parameter regimes where both sampling and approximate counting are tractable.
\end{itemize}

%
%












\subsection{Approximating the TV-distance between two Gibbs distributions}
Let $G = (V,E)$ be a graph. A spin system $\mathbb{S}$ (a.k.a. Markov random field) defines a distribution over $\{-1,+1\}^V$ (denoted by $\{\pm\}^V$ in short) in the following way. It defines a weight function $w=w_{\mathbb{S}}:= \{\pm\}^V \to \mathbb{R}_{\geq 0}$ that assigns each configuration $\sigma \in \{\pm\}^V$ a weight $w(\sigma)$. The spin system $\mathbb{S}$ induces a Gibbs distribution $\mu = \mu_{\mathbb{S}}$ over $\{\pm\}^V$ such that
\begin{align*}
 \forall \sigma \in \{\pm\}^V,\quad   \mu_{}(\sigma) \defeq \frac{w(\sigma)}{Z}, \quad \text{where } Z = Z_{\mathbb{S}} \defeq \sum_{\tau \in \{\pm\}^V}w_{}(\tau) \text{ is the \emph{partition function}.}
\end{align*}
The weight function $w$ of a spin system is a product of factors associated with each vertex and edge on graph $G$, which can be computed exactly and efficiently.
For a broad class of spin systems, 
the following sampling and approximate counting oracles with $\TS_G(\epsilon),\TC_G(\epsilon) = \mathrm{poly}({n}/{\epsilon})$ exist, where $n$ denotes the number of vertices in graph $G$. 
\begin{definition}[sampling and approximate counting oracles]\label{def:oracle}
Let $\mathbb{S}$ be a spin system on graph $G$ with Gibbs distribution $\mu$ and partition function $Z$.
Let $\TS_G,\TC_G:(0,1) \to \mathbb{N}$ be two non-increasing cost functions.
\begin{itemize}
    \item We say $\mathbb{S}$ admits a sampling oracle with cost function $\TS_G$ if given any $0 <\epsilon<1$, it returns a random sample $X \in \{\pm\}^V$ in time $\TS_G(\epsilon)$ with $\DTV{X}{\mu} \leq \epsilon$.
    \item We say $\mathbb{S}$ admits an approx. counting oracle with cost function $\TC_G$ if given any $0 <\epsilon<1$, it returns a random number $\hat{Z}$ in time $\TC_G(\epsilon)$ with $\Pr{(1-\epsilon)Z\leq\hat{Z}\leq (1+\epsilon)Z}\geq 0.99$.
\end{itemize}
%
\end{definition}

Given two spin systems $\mathbb{S}^\mu$ and $\mathbb{S}^\nu$ on the same graph $G$ that defines two Gibbs distributions $\mu$ and $\nu$ respectively and an error bound $0 <\epsilon < 1$, we study the following problem.
\begin{center}
\emph{Given the assess to sampling and approximate counting oracles for both $\mathbb{S}^\mu$ and $\mathbb{S}^\nu$, can we efficiently approximate the TV-distance $\DTV{\mu}{\nu}$ within relative error }$(1\pm \epsilon)$?
\end{center}


In this paper,
we focuses on the following two canonical and extensively studied spin systems. 
\begin{itemize}
    \item \textbf{Hardcore model}: Let $G = (V,E)$ be a graph. Let $\lambda = (\lambda_v)_{v \in V} \in \mathbb{R}_{\geq 0}^V$ be external fields. A configuration $\sigma \in \{\pm\}^V$ is said to be an independent set if $S_\sigma = \{v \in V \mid \sigma_v = +1\}$ forms an independent set in graph $G$. 
The hardcore model is specified by the pair $(G,\lambda)$, which defines the weight function $w$ such that
\begin{align*}
   \forall \sigma \in \{\pm\}^V, \quad w(\sigma) \defeq \begin{cases}
   \prod_{v \in V:\sigma_v = +1}\lambda_v &\text{if $\sigma$ is an independent set};\\
   0 &\text{otherwise}.
   \end{cases}
\end{align*}
    \item \textbf{Ising model}: Let $G = (V,E)$ be a graph. Let $J \in \mathbb{R}^{V \times V}$ be a \emph{symmetric interaction matrix} such that $J_{uv} \neq 0$ only if $\{u,v\} \in E$. Let $h \in (\mathbb{R} \cup \{\pm \infty\})^V$ be the \emph{external field}. Define \emph{Hamiltonian} function
  \begin{align*}
  \forall \sigma \in \{\pm\},\quad   H(\sigma) \defeq \sum_{\{u,v\} \in E} J_{uv}\sigma_u\sigma_v + \sum_{v \in V}\sigma_v h_v = \frac{1}{2}\langle \sigma, J\sigma \rangle + \langle \sigma, h \rangle.
  \end{align*}
The weight of a configuration $\sigma$ in Ising model is defined by $w(\sigma) \defeq \exp(H(\sigma))$.
\end{itemize}
The problem of approximating the TV-distance can be formalized as follows.
\begin{problem}[Hardcore TV-distance approximation]\label{label:prob-hardcore} The problem is defined as follows.
%The problem of approximating the total variation distance between two hardcore models are defined as follows.
\begin{itemize}
    \item \emph{Input}: two hardcore models $(G,\lambda^\mu)$ and $(G,\lambda^\nu)$ defined on the same graph $G = (V,E)$, which specifies two Gibbs distributions $\mu$ and $\nu$ respectively, and an error bound $0<\epsilon<1$.
    \item \emph{Output}: a number $\hat{d}$ such that $(1-\epsilon) \DTV{\mu}{\nu} \leq \hat{d} \leq (1 + \epsilon)\DTV{\mu}{\nu}$.% with probability at least $\frac{2}{3}$.
\end{itemize}
\end{problem}

\begin{problem}[Ising TV-distance approximation]\label{label:prob-Ising} 
The problem is defined as follows.
\begin{itemize}
    \item \emph{Input}: two Ising models $(G,J^\mu,h^\mu)$ and $(G,J^\nu,h^\nu)$ defined on the same graph $G = (V,E)$, which specifies two Gibbs distributions $\mu$ and $\nu$ respectively, and an error bound $0<\epsilon<1$.
    \item \emph{Output}: a number $\hat{d}$ such that $(1-\epsilon) \DTV{\mu}{\nu} \leq \hat{d} \leq (1 + \epsilon)\DTV{\mu}{\nu}$.
    %\begin{align}\label{eq:2/3}
    %    \Pr{ } \geq \frac{2}{3}.
    %\end{align}
\end{itemize}
\end{problem}




We are interested in the FPRAS (fully polynomial randomized approximation scheme), which solves the above problems with probability at least $2/3$\footnote{The success probability can be boosted to $1-\delta$ by independently running the algorithm for $O(\log \frac{1}{\delta})$ times and taking the median of the output.} in time $\mathrm{poly}({n}/{\epsilon})$.
%We will explore the problem on two extensively studied spin systems: the hardcore model and the Ising model.

\subsubsection{General results for hardcore and Ising models}

We need a marginal lower bound condition for general results. %We introduce some notations.
%Let $\mu$ over $\{\pm\}^V$ be a Gibbs distribution on variable set $V$. %For any vertex $v \in V$ and subset $S \subseteq V$, let $\mu_v$ and $\mu_S$ denote the marginal distribution on $v$ and $S$ projected from $\mu$ respectively. 
For any subset $\Lambda \subseteq V$, any feasible partial configuration $\sigma \in \{\pm\}^\Lambda$, let $\mu^\sigma$ denote the distribution $\mu$ conditional on $\sigma$. 
For any $v \in V$, let $\mu^\sigma_v$ denote the marginal distribution on $v$ projected from  $\mu^\sigma$.
%Specifically, $\mu^\sigma$ is a distribution over $\{\pm\}^V$ such that the configuration on $\Lambda$ is fixed as $\sigma$ and other \emph{free} variables in $V \setminus \Lambda$ follows the conditional distribution. 

\begin{condition}[marginal lower bound]\label{def:marlow}
Let $0 < b < 1$ be a parameter. We say a distribution $\mu$ over $\{\pm\}^V$ is $b$-marginally bounded if
for any  feasible partial configuration $\sigma \in \{\pm\}^\Lambda$ on a subset $\Lambda \subseteq V$, any $v \in V$, any $c \in \{\pm\}$, it holds that $\mu_v^\sigma(c) \geq b$ if $\mu_v^\sigma (c) > 0$.
\end{condition}
The marginal lower bound condition is a natural condition for spin systems. Hardcore and Ising models with constant marginal lower bound were extensively studied in sampling and approximate counting~\cite{jerrum2003counting,Wei06,Sly10,CLV21}. %The marginal lower bound always holds if the graph has bounded degree $\Delta =O(1)$ and all parameters of spin systems are constants. 
%Moreover, for Ising models satisfying the uniqueness condition (see \Cref{cond:Ising}), the marginal lower bound only requires that for any $v \in V$, $|h_v| = O(1)$ or $\infty$\footnote{We allow $|h_v| = \infty$ because \Cref{def:marlow} only considers the lower bound for spins $c \in \{\pm\}$ with $\mu^\sigma_v(c) > 0$.}.

\begin{theorem}[general result]\label{thm:Ising-1}
Let $0 < b < 1$ be a constant.
There exists a randomized algorithm that solves 
\Cref{label:prob-hardcore} and \Cref{label:prob-Ising} with probability at least $\frac{2}{3}$ in time 
\begin{align*}
     O_b\tp{\frac{N^2}{\epsilon^2} \TS_G \tp{\mathrm{poly}(b) \cdot \frac{\epsilon^2}{ N^2}} + \TC_G\tp{\mathrm{poly}(b) \cdot \frac{\epsilon}{ N}}},
\end{align*}
 if two input spin systems are both $b$-marginally bounded and both admit sampling and approximate counting oracles with cost functions $\TS_G(\cdot),\TC_G(\cdot)$, 
 where $O_b(\cdot)$ hides a factor depending only on $b$,  $N = n = |V|$ for hardcore model, and $N = n + m = |V| + |E|$ for Ising model.
 %where for hardcore model, $C_b = (\frac{1}{b})^{O(\frac{1}{b}\ln \frac{1}{b})}$ and $N = n = |V|$; and for Ising model, $C_b = O(\frac{1}{b^3})$ and $N = n + m = |V| + |E|$.
 %$c_b \in (0,1)$ is a constant depending only on $b$; 
%\begin{itemize}
%    \item  and $C_b = (\frac{1}{b})^{O(\frac{1}{b}\ln\frac{1}{b})}$; 
%    \item for Ising model, $N = n + m = |V| + |E|$ and $C_b = \mathrm{poly}(\frac{1}{b})$.
%\end{itemize}
 %and $O_b(\cdot)$ hides a constant depending only on $b$.
\end{theorem}

For the hardcore and Ising models with a constant marginal lower bound, the theorem provides the first polynomial-time reduction from approximating the TV-distance to sampling and approximate counting. Moreover, the above theorem is a simplified version, and our technique yields a stronger result that also applies to some hardcore and Ising models with a smaller marginal lower bound $b = o(1)$. See \Cref{remark:b}\ifthenelse{\boolean{conf}}{ in the Appendix}{} or a more detailed discussion.


%For the hardcore and Ising models with a marginal lower bound \( b \) such that \( C_b \leq \mathrm{poly}(n) \), for instance, when \( b = \Omega(1) \) 
%For the hardcore and Ising models with a constant marginal lower bound $b = \Omega(1)$, the theorem provides a polynomial-time reduction from approximating the TV-distance to sampling and approximate counting. %The specific values of $C_b$ for hardcore and Ising models are given in \Cref{sec:proof-main} \ifthenelse{\boolean{conf}}{of the Appendix}{}. 
 


Consider a hardcore model  $(G,\lambda)$. Let $\Delta \geq 3$ denote the maximum degree of $G$. The hardcore model is said to satisfy the \emph{uniqueness condition} with a constant gap $0 < \eta < 1$ if
\begin{align}\label{eq:cond-hardcore}
    \forall v \in V, \quad \lambda_v \leq (1-\eta)\lambda_c(\Delta), \quad \text{ where } \lambda_c(\Delta) \defeq \frac{(\Delta-1)^{\Delta - 1}}{(\Delta-2)^\Delta} \approx \frac{e}{\Delta}.
\end{align}
Previous works~\cite{CFYZ22,CE22,SVV09} proved that for hardcore model satisfying the uniqueness condition in~\eqref{eq:cond-hardcore}, it admits $\text{poly}(\frac{n}{\epsilon})$ time sampling and approximate counting oracles. %with $\TS_G(\epsilon) = \tilde{O}_\eta(\Delta n)$ and approximate counting oracle with $\TC_G(\epsilon) = \tilde{O}_\eta(\frac{\Delta n^2}{\epsilon^2})$~\cite{SVV09}, where $\tilde{O}_{\eta}(\cdot)$ hides a constant factor $C(\eta)$ and a $\mathrm{polylog}(\frac{n}{\epsilon})$ factor. 
%Previous works~\cite{CFYZ22,CE22,SVV09} proved that for hardcore model satisfying the uniqueness condition in~\eqref{eq:cond-hardcore}, it admits $\text{poly}(\frac{n}{\epsilon})$ time sampling oracle with $\TS_G(\epsilon) = \tilde{O}_\eta(\Delta n)$ and approximate counting oracle with $\TC_G(\epsilon) = \tilde{O}_\eta(\frac{\Delta n^2}{\epsilon^2})$~\cite{SVV09}, where $\tilde{O}_{\eta}(\cdot)$ hides a constant factor $C(\eta)$ and a $\mathrm{polylog}(\frac{n}{\epsilon})$ factor.
%Specifically, the sampling oracle is implemented by running standard \emph{Glauber dynamics} for $O_{\eta}(n \log \frac{n}{\epsilon})$ steps, where every step costs $O(\Delta)$ time, and the approximate counting oracle is implemented by counting-to-sampling reduction in~\cite{SVV09}. 
To obtain a $\text{poly}(\frac{n}{\epsilon})$ time algorithm for TV-distance estimation, one way is to consider hardcore models in the uniqueness regime with constant marginal lower bound $b =\Omega(1)$, which requires that $\Delta = O(1)$ and for all $v \in V$, either $\Omega(1) \leq \lambda_v \leq (1-\eta)\lambda_c(\Delta)$ or $\lambda_v = 0$\footnote{We allow $\lambda_v = 0$ because \Cref{def:marlow} only considers the lower bound for spins $c \in \{\pm\}$ with $\mu^\sigma_v(c) > 0$.}.


For Ising models $\mathbb{S} = (G,J,h)$, the following conditions are well-known.
\begin{condition}\label{cond:Ising}
$\mathbb{S}$ satisfies \emph{one of} the following conditions:
\begin{itemize}
    \item Spectral condition: $\lambda_{\max}(J) - \lambda_{\min}(J) \leq 1 - \eta$ for some constant $\eta > 0$, where  $\lambda_{\max}(J)$ and $\lambda_{\min}(J)$ denote the max and min eigenvalues of $J$ respectively.
    \item Ferromagnetic interaction with consistent field condition: $J_{uv} \geq 0$ for all edge $\{u,v\} \in E$ and $h_v \geq 0$ for all $v \in V$. 
    \item Anti-ferromagnetic interaction at or within the uniqueness threshold: $J_{uv}=\beta\leq 0$ for all $\{u,v\} \in E$ and $\exp(2\beta) \geq \frac{\Delta-2}{\Delta} $, where $\Delta$ is the maximum degree of $G$.
\end{itemize}
\end{condition}

Previous works \cite{AJKPV22, FengGW23,JS93,CCYZ24,JVV86,SVV09} gave $\mathrm{poly}(\frac{n}{\epsilon})$ time sampling and approximate counting oracles for Ising models satisfying \Cref{cond:Ising}.

We have the following corollary for hardcore and Ising models from \Cref{thm:Ising-1}.


%We have the following corollary for hardcore and Ising model.

\begin{corollary}\label{Cor:Ising}
Let $0 < \eta < 1$ and $\Delta \geq 3$ be two constants.
There exists an FPRAS for \Cref{label:prob-hardcore} if two input hardcore models are defined on graph $G$ with maximum degree $\Delta$ and all external fields satisfy that $\Omega(1) \leq \lambda^\pi_v \leq (1 - \eta)\lambda_c(\Delta)$ or $\lambda^\pi_v = 0$ for all $v \in V$ and $\pi \in \{\mu,\nu\}$.

There exists an FPRAS for \Cref{label:prob-Ising} if two input Ising models both satisfy \Cref{cond:Ising} and the marginal lower bound in \Cref{def:marlow} with $b = \Omega(1)$.
\end{corollary}

%\Cref{Cor:Ising} presents the first algorithm for approximating the total variation distance for a broad class of spin systems. Moreover, for the Ising model, our proof provides a stronger result that gives a polynomial-time reduction when \( b \geq \frac{1}{\mathrm{poly}(n)} \) (see \Cref{remark:b}\ifthenelse{\boolean{conf}}{ in the Appendix}{}). The Ising result in \Cref{Cor:Ising} can be improved to allow $b \geq \frac{1}{\mathrm{poly}(n)}$.
 



The condition required by \Cref{Cor:Ising} is arguably significant conditions for TV-distance estimation.
Let $\Delta \geq 3$ be a constant.
For the hardcore model beyond the uniqueness regime such that $\lambda_v > \lambda_c(\Delta)$ for all $v \in V$, polynomial-time sampling and approximate counting oracles do not exist unless $\textbf{NP}=\textbf{RP}$~\cite{Sly10}. Moreover, the technique in~\cite{BGMMPV24ICLR} can show that unless $\textbf{NP}=\textbf{RP}$, there is no FPRAS for \Cref{label:prob-hardcore} if input hardcore models are beyond the uniqueness condition. For the completeness, we give a simplified proof in~\Cref{app:proof}. We can show that this hardness result holds even for \emph{additive-error} approximation.


To see the significance of \Cref{cond:Ising}, consider the following family of Ising models.
Let $\mathbb{S}^\mu = (G,J,h^\mu)$ and $\mathbb{S}^\nu = (G,J,h^\nu)$ be two Ising models defined on the same graph $G$ and have the same interaction matrix $J$.
Assume that $G$ has constant maximum degree $\Delta$; 
for all edges $\{u,v\}$ in $G$,
$J_{uv} = \beta < 0$ is a unified negative constant; 
and for all vertices $v \in V$, $h^\mu_v$ and $h^\nu_v$ can take values from set $\{\pm\infty,0\}$. This family of Ising models has a constant marginal lower bound. We have the following two results:
\begin{itemize}
    \item if $\exp(2\beta) \geq \frac{\Delta-2}{\Delta}$, then FPRAS for TV-distance exists by \Cref{Cor:Ising};
    \item if $\exp(2\beta) < \frac{\Delta-2}{\Delta}$, there is no FPRAS for TV-distance unless $\textbf{NP}=\textbf{RP}$, the hardness result holds even for approximating the TV-distance with \emph{additive} error.
\end{itemize}
Again, the hardness result can be proved by the technique in \cite{BGMMPV24ICLR}. We give a simple proof in \Cref{app:proof} for the completeness.


\subsubsection{Improved results for hardcore models in the uniqueness regime} 

\Cref{Cor:Ising} works for hardcore model in the uniqueness regime. However, it additionally requires a marginal lower bound $b = \Omega(1)$.
We give the following improved algorithm that removes the marginal lower bound requirement, thus it works for the \emph{whole} uniqueness regime.
\begin{theorem}[improved algorithm for the whole uniqueness regime]\label{thm:hardcore-1}
 There exists a randomized algorithm that solves \Cref{label:prob-hardcore} with probability at least $2/3$ in time $\tilde{O}_\eta(\frac{\Delta n^7}{\epsilon^{5/2}})$ if two input hardcore models both satisfy the uniqueness condition with a constant gap $\eta > 0$, where $n$ is the number of vertices and $\tilde{O}_\eta(\cdot)$ hides a constant factor depending on $\eta$ and a $\mathrm{polylog}(\frac{n}{\epsilon})$ factor.
\end{theorem}


%Compared to \Cref{Cor:Ising},
%the algorithm in \Cref{thm:hardcore-1} addresses general hardcore models in the whole uniqueness regime.
Next, let us further assume that the hardcore model has a \emph{constant} marginal lower bound $b = \Omega(1)$. 
%In this case, $\Delta = O(1)$ and for all $v \in V$ and $\pi \in \{\mu,\nu\}$, either $\Omega(1) \leq \lambda^\pi_v \leq (1 - \eta)\lambda_c(\Delta)$ or $\lambda^\pi_v = 0$.
%This class of hardcore models is extensively studied in sampling and approximate counting~\cite{CLV21,Sly10,Wei06}.
\Cref{thm:Ising-1} and \Cref{Cor:Ising} give an FPRAS in time 
\begin{align}\label{eq:time-gen}
O_b\tp{\frac{N^2}{\epsilon^2} \TS_G \tp{\mathrm{poly}(b) \cdot \frac{\epsilon^2}{ N^2}} + \TC_G\tp{\mathrm{poly}(b) \cdot \frac{\epsilon}{ N}}} = \tilde{O} \tp{\frac{n^4}{\epsilon^2}}.  
\end{align}
The equation holds because $\TS_G(\delta) = O(\Delta n\log\frac{n}{\delta})$~\cite{CFYZ22,CE22}, $\TC_G(\delta) = \tilde{O}(\frac{\Delta n^2}{\delta^2})$~\cite{SVV09}, and $\Delta = O(1)$ due to the constant marginal lower bound assumption. For these hardcore models, we can also give a faster algorithm than the general results in \Cref{thm:Ising-1}.
Compared to the running time in~\eqref{eq:time-gen}, the following improved algorithm reduces a factor of $n$ in the running time.  


\begin{theorem}[faster algorithm further assuming constant marginal lower bound]\label{thm:hardcore-2}
Let $0 < \eta < 1$ and $\Delta \geq 3$ be two constants.
There exists an FPRAS in time $\tilde{O}(\frac{n^3}{\epsilon^2})$ for \Cref{label:prob-hardcore} if two input hardcore models are defined on graph $G$ with maximum degree $\Delta$ and all external fields satisfy that $\Omega(1) \leq \lambda^\pi_v \leq (1 - \eta)\lambda_c(\Delta)$ or $\lambda^\pi_v = 0$ for all $v \in V$ and $\pi \in \{\mu,\nu\}$.
\end{theorem}





%We have the following corollary. %from \Cref{thm:hardcore-1}.

%\begin{corollary}
%There exists an FPRAS with running time $\tilde{O}(\frac{\Delta n^3}{\epsilon^2})$ for \Cref{label:prob-hardcore} if two input hardcore models both satisfy the uniqueness condition with a constant gap. 
%\end{corollary}

%Very recently, polynomial-time sampling and approximate counting oracles were discovered for hardcore model at the uniqueness threshold: $\lambda_v = \lambda_c(\Delta)$ for all $v \in V$~\cite{CCYZ24}. Combining it with \Cref{thm:hardcore-1} also implies polynomial-time algorithms for approximating TV-distance.









    


  
  %The Ising model is specified by a tuple $(G,J,h)$. It defines a Gibbs distribution $\mu$ over $\{\pm\}^V$ such that
  %\begin{align*}
  %    \mu(\sigma) \defeq \frac{w(\sigma)}{Z},\quad \text{where } Z \defeq \sum_{\tau \in \{\pm\}^V} w(\tau)) \text{ is partition function.}
  %\end{align*}

%We study the problem  of estimating the total variation distance between two Ising models.

%The success probability $\frac{2}{3}$ in~\eqref{eq:2/3} can be replace with any constant greater than $\frac{1}{2}$. It is standard to boost the  success probability from $\frac{2}{3}$ to $1 - \zeta$ by independently running the algorithm for $O(\log \frac{1}{\zeta})$ times and taking the median of the output.






%\begin{definition}[conditional sampler]
%Let $\mu$ be an Ising model on graph $G$.
%Let $T_G:(0,1) \to \mathbb{N}$ be a cost function.
%We say $\mu$ admits a  conditional sampler $\+S_\mu$ with cost function $T_G$ if given any feasible partial configuration $\sigma \in \{\pm\}^\Lambda$ on a subset $\Lambda \subseteq V$ and any error bound $0 < \epsilon < 1$, $\+S_\mu$ returns a random sample $X \sim \mu^\sigma$ in time $T_G(\epsilon)$ such that 
%$\DTV{X}{\mu^\sigma} \leq \epsilon.$
%\end{definition}

%For typical Ising models (e.g, Ising model in the uniqueness regime), the standard sampling algorithm is \emph{Glauber dynamics}. The mixing time of Glauber dynamics is $O(n \log \frac{n}{\epsilon})$ and each transition costs $O(\Delta)$ time, where $\Delta$ is the maximum degree of the graph $G$. The cost function for conditional sampler satisfies $T_G(\epsilon) = O(\Delta n \log \frac{n}{\epsilon})$. 

%We have the following abstract results assuming the conditional sampler.



 %Consider a Ising model $\mathbb{S}$ specified by $(G,J,h)$, where $G=(V,E)$ is a graph with $n$ vertices and maximum degree $\Delta$. 
%Let $\lambda_{\max}(J)$ and $\lambda_{\min}(J)$ denote the max and min eigenvalues of $J$ respectively. %$\Vert J \Vert_{\max} = \max_{ij}|J_{i,j}|$ and $ \Vert h \Vert_{\infty} = \max_i |h_i| $.
%Consider the following two conditions for Ising model.



%\begin{condition}\label{cond:1}
%$\Vert J \Vert_{\max}$, $\Vert h \Vert_{\infty}$, and $\Delta$ are all $O(1)$ and one of the following two conditions~holds
%\begin{itemize}
 %   \item Spectral condition: $\lambda_{\max}(J) - \lambda_{\min}(J) \leq 1 - \eta$ for some constant $\eta > 0$;
%    \item Ferromagnetic interaction with consistent field condition: $J_{uv} \geq 0$ for all edge $\{u,v\} \in E$ and $h_v \geq 0$ for all $v \in V$. 
%\end{itemize}
%\end{condition}
%\begin{condition}\label{cond:2}
%$\Vert h \Vert_{\infty} = O(1)$  and one of the following two conditions holds 
%\begin{itemize}
%    \item Dobrushin condition: $\Vert J\Vert_{\infty \to \infty} = \max_{i}\sum_j |J_{i,j}| \leq 1 - \eta$ for some constant $\eta > 0$;
%    \item Anti-ferromagnetic interaction at or within the uniqueness threshold: $J_{uv}=\beta\leq 0$ for all $\{u,v\} \in E$ and $\exp(2\beta) \geq \frac{\Delta-2}{\Delta} $.
%\end{itemize}
%\end{condition}



%We summarize the results as follows.
%\begin{proposition}[\text{\cite{AJKPV22,Hayes06,CCYZ24,FengGW23,JVV86}}]\label{prop:1}
%Let $0 < \eta < 1$ be a constant. The Ising models admits sampling and counting oracles with $\TS_G(\epsilon)= C_\eta \cdot\mathrm{poly}(\frac{n}{\epsilon})$, counting oracle with $\TC_G(\epsilon)= C_\eta \cdot \mathrm{poly}(\frac{n}{\epsilon})$, and a constant marginal  lower bound $b = O(1)$, where $C_\eta$ is a constant depending only on $\eta$, if one of the following conditions holds.
%\begin{itemize}
%    \item Spectral condition $\Vert J\Vert_{\infty} \leq 1 - \eta$. Additionally, $\Vert J\Vert_{\max},\Vert h \Vert_{\infty},\Delta$ are all $O(1)$;
%    \item Dobrushin condition $\Vert J\Vert_{\infty} \leq 1 - \eta$.
%    Additionally, $\Vert h \Vert_\infty =O(1)$;
%    \item Anti-ferro interaction with uniqueness condition: $J_{uv}=\beta\leq 0$ for all $\{u,v\} \in E$ and $e^{2\beta} \geq \frac{\Delta-2}{\Delta} $. Additionally, $\Vert h\Vert_{\infty} = O(1)$.
%    \item Ferro interaction with consistent field condition $J_{uv} \geq 0$ for all edge $\{u,v\} \in E$ and $h_v \geq 0$ for all $v \in V$. 
 %   Additionally,  $\Vert J\Vert_{\max},\Vert h \Vert_{\infty},\Delta$ are all $O(1)$
%\end{itemize}
%\end{proposition}

%\begin{proposition}[ferro-Ising with consistent field \text{\cite{JS93,FengGW23}}]\label{prop:2}
%For $J_{uv} \geq 0$ for all edge $\{u,v\} \in E$, $h_v \geq 0$ for all $v \in V$ and $\Vert J\Vert_{\max}=O(1),\Vert h \Vert_{\infty}=O(1),\Delta = O(1)$, then it admits sampling and counting oracles with $\TS_G(\epsilon) = \mathrm{poly}(\frac{n}{\epsilon})$ and $\TC_G(\epsilon) = \mathrm{poly}(\frac{n}{\epsilon})$ respectively and a constant marginal lower bound $b = O(1)$.
%\end{proposition}

%The sampling algorithm in \Cref{prop:1} is the standard Glauber dynamics and the sampling algorithm in \Cref{prop:2} is Jerrum-Sinclair~\cite{JS93} for ferromagnetic Ising models. 
%Combining the known results with \Cref{thm:Ising-1}, we have the following corollary. 
%\todo{point to pf of $b=O(1)$}





\subsection{Approximating the TV-distance between two marginal distributions}
One natural extension is to approximate the TV-distance between two marginal distributions on a subset of vertices.
%However, we show that this problem is hard even if two input distributions both admit sampling and approximate counting oracles. 
We use the hardcore model as an example to state our results on this problem. The same results can be extended to Ising model using a similar reduction.

\begin{problem}[$k$-marginal TV-distance approximation]\label{label:prob-mar} 
Let $k:\mathbb{N} \to \mathbb{N}$ be a function. 
\begin{itemize}
    \item \emph{Input}: two hardcore models $(G,\lambda^\mu)$ and $(G,\lambda^\nu)$ defined on the same graph $G = (V,E)$, which specifies two Gibbs distributions $\mu$ and $\nu$ respectively, a subset $S \subseteq V$ such that $|S| = k(n)$, where $n = |V|$, and an error bound $\epsilon > 0$.
    \item \emph{Output}: a number $\hat{d}$ such that $ \frac{\DTV{\mu_S}{\nu_S}}{1+\epsilon} \leq \hat{d} \leq (1 + \epsilon)\DTV{\mu_S}{\nu_S}$, where $\mu_S$ and $\nu_S$ are marginal distributions on $S$ projected from $\mu$ and $\nu$ respectively.
\end{itemize}
\end{problem}
In particular, if the function $k(n) = n$, then \Cref{label:prob-mar} is the same as \Cref{label:prob-hardcore}.

We show that the problem is hard when $k(n) = 1$. In this case, the problem is to approximate the TV-distance between two marginal distributions at a single vertex. The hardness result holds even if two input hardcore models are in the uniqueness regime, where both sampling and approximate counting are intractable in polynomial time.
\begin{theorem}\label{thm:one-vertex}
Let $k(n) = 1$ for all $n \in \mathbb{N}$ be a constant function.  The $k$-marginal TV-distance approximation is \textbf{\#P}-Hard when two input hardcore models both satisfy the uniqueness condition, the hardness result holds even if $\epsilon = \mathrm{poly}(n)$, where  $n$ is number of vertices in $G$.
\end{theorem}

The above theorem is for marginal distributions at one vertex. One can simply lift the result to the marginal distributions on a set of vertices.
%as long as $k(n)$ is a natural function and $n - k(n) \geq \mathrm{poly}(n)$. 
In particular, we have the following corollary.

\begin{corollary}\label{thm:many-vertex}
Let $0 < \alpha < 1$ be a constant and $k(n) = n - \lceil n^\alpha \rceil$. The $k$-marginal TV-distance approximation is \textbf{\#P}-Hard when two input hardcore models both satisfy the uniqueness condition, the hardness result holds even if $\epsilon = \mathrm{poly}(n)$, where  $n$ is number of vertices in $G$.      
\end{corollary}

The proofs of the hardness results are given in \Cref{sec:hard}\ifthenelse{\boolean{conf}}{ of the Appendix}{}.
The proof constructs a Turing reduction that exactly counts the number $Z$ of independent sets in graphs with a maximum degree of 3, a problem known to be \textbf{\#P}-complete~\cite{DyerG00}. Specifically, we show that if one can efficiently solve the problem stated in \Cref{thm:one-vertex}, then it is possible to efficiently estimate the probability that a vertex $v$ is included in a uniformly random independent set, with an \emph{exponentially} small relative error of $4^{-n}$. This, in turn, solves the \emph{exact} counting problem by using the self-reducibility~\cite{JVV86} property of the hardcore model and the fact that $Z \leq 2^n$.

%The hardness results are established in \Cref{sec:hard}. 
Now, let us compare our hardness results with that in \cite{BGMMPV24ICLR}. The hardness results in \cite{BGMMPV24ICLR} are for approximating the TV-distance between two \emph{entire Gibbs distributions}. In contrast, our hardness results are for approximating the TV-distance between two \emph{marginal distributions}. The hardness results in \cite{BGMMPV24ICLR} are \textbf{NP}-hard results. It considered the Gibbs distributions in a parameter regime where sampling and approximate counting are intractable. In contrast, our hardness results are \textbf{\#P}-hard results. Our hardness results holds even if sampling and approximate counting can both be solved in polynomial time.


Finally, consider approximating the TV-distance between two \emph{marginal distributions} with \emph{additive error} $\epsilon$. We show that this relaxed problem admits FPRAS if two input hardcore models both satisfy the uniqueness condition. 

\begin{theorem}\label{thm:many-vertex-alg}
There exists a randomized algorithm such that given two hardcore distributions $\mu$ and $\nu$ on the same graph $G=(V,E)$ with $n = |V|$, any subset $S \subseteq V$, and any $0 < \epsilon < 1$, if $\mu$ and $\nu$ both satisfy~\eqref{eq:cond-hardcore}, it returns a random number $\hat{d}$ in time $\frac{\Delta n^2}{\epsilon^4} \cdot \mathrm{polylog}(\frac{n}{\epsilon})$ such that \[\Pr{\big|\hat{d} - \DTV{\mu_S}{\nu_S}\big|\leq \epsilon} \geq \frac{2}{3}.\]
\end{theorem}

\Cref{thm:many-vertex-alg} together with two hardness results give a clear separation between the computational complexity for approximating the TV-distance with relative and additive error.


\subsection{Related work} 

There are a series of works on checking whether two given distributions are identical (e.g. \cite{CortesMR07,DoyenHR08,KieferMOWW11,BGMMPV24ICLR}), which can be viewed as the decision version of computing the TV-distance, i.e., checking whether it is zero. 



There are also a long line of works (e.g., see a survey in \cite{Canonne15}) studying the identical testing problem in access model, where the algorithm can only access the set of random samples from the distributions.
This setting is different from our setting, where we assume that all the parameters of the spin systems are given as the input to the algorithm.


A series of works~\cite{0001GMV20,ChenK14,Kiefer18,CanonneR14} studied the algorithm and the hardness of approximating the TV-distance with additive error, which is an easier problem than the relative-error approximation.


\cite{devroye2018total,ArbasAL23,kontorovich2024tensorization,kontorovich2024sharp} found \emph{closed-form formulas}, which approximates the TV-distance between two high-dimensional distributions with a \emph{fixed} relative error.
We study \emph{algorithms} achieving an \emph{arbitrary} $\epsilon$-relative error approximation for spin systems.


%\section{Technique overview}

\section{Algorithm overview}
In this section, we give an overview of our algorithm.
Let $G=(V,E)$ be a graph. 
Let $\mathbb{S}^\mu$ and $\mathbb{S}^\nu$ be two spin systems (either two hardcore models or two Ising models) defined on the same graph $G$.
Let $\mu$ and $\nu$ denote Gibbs distributions of $\mathbb{S}^\mu$ and $\mathbb{S}^\nu$ respectively.

We first introduce the distance between parameters of two spin systems. For a vector $a \in \mathbb{R}^V$, denote $\Vert a \Vert_\infty = \max_{v \in V}|a_v|$. For a matrix $A \in  \mathbb{R}^{V \times V}$, denote $\Vert A \Vert_{\max} = \max_{u,v \in V}|A_{uv}|$.
\begin{definition}[parameter distance]\label{def:dis}
The parameter distance $\dis(\mathbb{S}^\mu,\mathbb{S}^\nu)$ between $\mathbb{S}^\mu$ and $\mathbb{S}^\mu$, which is denote by  $\dis(\mu,\nu)$ for simplicity, is defined by  
\begin{itemize}
    \item Hardcore model: for two hardcore models $\mathbb{S}^\mu = (G,\lambda^\mu)$ and  $\mathbb{S}^\nu = (G,\lambda^\nu)$, \[\dis(\mu,\nu) \defeq \Vert \lambda^\mu - \lambda^\nu \Vert_{\infty}.\]
    \item Soft-Ising model:  for two soft-Ising models $\mathbb{S}^\mu = (G,J^\mu,h^\mu)$ and $\mathbb{S}^\nu = (G,J^\nu,h^\nu)$, \[\dis(\mu,\nu) \defeq \max\left\{ \Vert J^\mu - J^\nu \Vert_{\max}, \max_v \frac{|h^\mu(v)-h^\nu(v)|}{\deg_v+1} \right\},\]
    where $\deg_v$ is the degree of $v$ in graph $G$.
\end{itemize}
\end{definition}

The above parameter distance can be computed easily.
The following lemma gives the relation between parameter distance $\dis(\mu,\nu)$ and the total variation distance $\DTV{\mu}{\nu}$. 
%The lemma works for the \emph{soft-Ising model} and hardcore model. 
An Ising model $(G=(V,E),J,h)$ is said to be \emph{soft} if $h \in \mathbb{R}^V$ (instead of $h \in (\mathbb{R} \cup \{\pm \infty\})^V$).
%A hardcore model $(G,\lambda)$ is said to be \emph{soft} if $\lambda_v > 0$ (instead of $\lambda_v \geq 0$) for all $v \in V$.
We now focus on soft-Ising model in this overview. 
We will show how to reduce general Ising models to soft models in \Cref{sec:proof-main}\ifthenelse{\boolean{conf}}{ of the Appendix.}{.}
\begin{lemma}[TV-distance lower bound]\label{lem:TV-lower}
It holds that $\DTV{\mu}{\nu} \geq \CC\cdot \dis(\mu,\nu)$ such that
\begin{itemize}
    \item Hardcore model: if both $\mu$ and $\nu$ satisfy the uniqueness condition in~\eqref{eq:cond-hardcore}, $\CC = \frac{1}{5000}$.
    \item Hardcore model: if both $\mu$ and $\nu$ are $b$-marginally bounded, $\CC = b^3$.
    \item Soft-Ising model: if both $\mu$ and $\nu$ are $b$-marginally bounded, $\CC = \frac{b^2}{2}$.
\end{itemize}
\end{lemma}
 

By \Cref{lem:TV-lower}, if $\dis(\mu,\nu)$ is large, the TV-distance $\DTV{\mu}{\nu}$ is also large. Let $n = |V|$ and $m = |E|$ denote the numbers of vertices and edges in $G$. Define the a threshold $\theta = \frac{c_b}{\text{poly}(n)}$ for the parameter distance for soft-Ising and hardcore models, where $c_b$ is some parameter depending only on the marginal lower bound $b$\footnote{The value of $b$ is not given in the input, but we can compute $b$ efficiently (see \Cref{lem:alg-b}).}. The specific value of $\theta$ can be found in \Cref{sec:proof-main}\ifthenelse{\boolean{conf}}{ of the Appendix}{}.
%\begin{align}\label{eq:theta}
%     \Thre = \Thre_G(\mu,\nu) \defeq \begin{cases}
%         10^{-10}\frac{\epsilon^{1/4}}{n^{5/2}} &\text{if $\mu$ and $\nu$ are hardcore models},\\
%         \frac{1}{2(n+3m)} &\text{if $\mu$ and $\nu$ are soft-Ising models}.
%     \end{cases}
%\end{align}
%\todo{WF: I will fix parameter later}
%We may write $\Thre_G(\mu,\nu)$ as $\theta$ when $G,\mu,\nu$ are clear from the context.
Our algorithm first compute $\dis(\mu,\nu)$ in time $O(m+n)$, and then compares $\dis(\mu,\nu)$ to the threshold $\Thre$. The algorithm considers the following two cases.
\begin{itemize}
    \item Case $\dis(\mu,\nu) \geq \Thre$: By \Cref{lem:TV-lower}, the TV-distance $\DTV{\mu}{\nu} = \CC \dis(\mu,\nu) \geq \Omega_b(\frac{1}{\mathrm{poly}(n)})$ is large. In this case, the task of approximating $\DTV{\mu}{\nu}$ with relative error is the same (up to $O_b(\mathrm{poly}(n))$ running time) as the task of approximating $\DTV{\mu}{\nu}$ with \emph{additive} error. We give a general FPRAS that achieves the additive-error approximation assuming polynomial-time sampling and approximate counting oracles for both $\mu$ and $\nu$.
    \item Case $\dis(\mu,\nu) < \Thre$: The algorithm for this case is our main technical contribution.  Let $w_{\mu}(\cdot)$ and $w_\nu(\cdot)$ denote the weight functions for spin systems $\mathbb{S}^\mu$ and $\mathbb{S}^\nu$ respectively.
    By the definition of parameter distance, we know that for any $\sigma \in \{\pm\}^V$, $w_\mu(\sigma) \approx w_\nu(\sigma)$. 
    We will utilize this property to design efficient approximation algorithm for TV-distance. 
\end{itemize}
In \Cref{sec:add-alg} and \Cref{sec:mul-alg}, we will explain the main ideas of our algorithms for the above two cases.
The formal description of the algorithms are given in Sections \ref{sec:add} and \ref{sec:alg-main}\ifthenelse{\boolean{conf}}{ of the Appendix}{}.

\subsection{Warm-up: additive-error approximation algorithm}\label{sec:add-alg}
Let us first consider the easy case.
This case is solved by improving the algorithm in~\cite{0001GMV20}.
By the definition of total variation distance, we can write
\begin{align}\label{eq:TV-1}
  \DTV{\mu}{\nu} &= \sum_{\sigma \in \{\pm\}^V:\mu(\sigma)>\nu(\sigma)}|\mu(\sigma)-\nu(\sigma)| = \sum_{\sigma \in \{\pm\}^V} \mu(\sigma) \max \left( 0, 1 - \frac{\nu(\sigma)}{\mu(\sigma)} \right).
\end{align}
Define the random variable $X \defeq \max \left( 0, 1 - \frac{\nu(\sigma)}{\mu(\sigma)} \right)$, where $\sigma \sim \mu$. Note that $0 \leq X \leq 1$. We have $\E[]{X} = \DTV{\mu}{\nu}$ and $\Var[]{X} \leq 1$. Additive-error approximation can be achieved if random samples of the variable $X$ can be efficiently generated.



However, since we can only access sampling and approximate counting oracles, we cannot compute $\mu(\sigma)$ and $\nu(\sigma)$ exactly. Instead, our algorithm uses an alternative estimator $\hat{X}$ to approximate the random variable $X$.
Let $Z_\mu, w_{\mu}(\cdot)$ and $Z_\nu, w_\nu(\cdot)$ denote the partition functions and weight functions of $\mathbb{S}^\mu$ and $\mathbb{S}^\nu$, respectively. We first call the approximate counting oracle to obtain $\hat{Z}_\mu$ and $\hat{Z}_\nu$, which approximate $Z_\mu$ and $Z_\nu$ with a relative error of $O(\epsilon)$. The estimator $\hat{X}$ is then defined by the following process:

\begin{itemize}
    \item Call sampling oracle to generate approximate sample $\sigma$ from $\mu$;
    \item $\hat{X} = \max\left(0, 1 - \frac{\hat{\nu}(\sigma)}{\hat{\mu}(\sigma)}\right)$, where $\hat{\nu}(\sigma) = w_\nu(\sigma)/\hat{Z}_\nu$ and $\hat{\mu}(\sigma) = w_\mu(\sigma)/\hat{Z}_\mu$.
\end{itemize}
The error of $\hat{X}$ arises from the errors in the approximate sample $\sigma$ and the probabilities $\hat{\nu}(\cdot)$ and $\hat{\mu}(\cdot)$. To illustrate the main idea of the algorithm, let us ignore the sampling error and assume $\sigma \sim \mu$. 
Note that the true value $\frac{\nu(\sigma)}{\mu(\sigma)} = \frac{w_\nu(\sigma)}{w_{\mu}(\sigma)} \cdot \frac{Z_{\mu}}{Z_\nu}$. 
By the assumption of approximate counting oracle, $\frac{\hat{\nu}(\sigma)}{\hat{\mu}(\sigma)} \geq (1-O(\epsilon))\frac{\nu(\sigma)}{\mu(\sigma)}$, which implies $\mathbf{E}[\hat{X}] \leq \mathbf{E}_{\sigma \sim \mu}[\max(0,1-(1-O(\epsilon))\frac{\nu(\sigma)}{\mu(\sigma)})]$, which is at most $\mathbf{E}_{\sigma \sim \mu}[\max(0,1-\frac{\nu(\sigma)}{\mu(\sigma)})] +O(\epsilon)\mathbf{E}_{\sigma \sim \mu} \frac{\nu(\sigma)}{\mu(\sigma)} = \E[]{X} +O(\epsilon)$.  A similar analysis gives the lower bound $\mathbf{E}[\hat X] \geq \E[]{X} - O(\epsilon)$. Since  $0 \leq \hat{X} \leq 1$ so the variance is at most 1, this achieves the additive error approximation of the TV-distance. 

The algorithm in \Cref{thm:many-vertex-alg} estimates the TV-distance between two marginal distributions with additive error, which follows a similar approach. The key difference is that it requires approximating the marginal probabilities $\nu_S(\sigma)$ and $\mu_S(\sigma)$ for $\sigma \in \{\pm\}^S$, where $S \subseteq V$. For the hardcore model within the uniqueness regime, we have not only an efficient approximate counting oracle but also efficient oracles for approximating the conditional partition function $Z^\sigma = \sum_{\tau \in \{\pm\}^V: \tau_S = \sigma} w(\tau)$, which give the efficient approximation of $\mu_S(\sigma)$ and $\nu_S(\sigma)$.


Additive-error approximation algorithms were also studied in~\cite{0001GMV20}, where a similar algorithm was proposed. Their approach assumes that the values of $\mu(\sigma)$ and $\nu(\sigma)$ can be computed exactly, allowing the use of the estimator $X$. In contrast, our algorithm demonstrates that approximated values of $\mu(\sigma)$ and $\nu(\sigma)$ suffice. Consequently, our additive-error algorithm~(see \Cref{thm:Approximate-Gibbs} for a formal statement) improves upon some results in~\cite{0001GMV20}. For instance, while \cite{0001GMV20} applies to ferromagnetic Ising models on bounded tree-width graphs with consistent external fields, our result removes the bounded tree-width restriction and applies to general graphs (see \Cref{corollary:apps}).



\subsection{Approximation algorithm for instances with small parameter distance}\label{sec:mul-alg}
%\todo{WM: I will rewrite this part later}
Consider the case where $\dis(\mu, \nu) < \theta$. The total variation distance $\DTV{\mu}{\nu}$ can be very small (e.g., $\exp(-\Omega(n))$). We cannot use additive-error approximation algorithm to efficiently achieve a relative-error approximation. The main challenge lies in the term $1 - {\nu(\sigma)}/{\mu(\sigma)}$ in~\eqref{eq:TV-1}. With approximate counting oracles, we can approximate ${\nu(\sigma)}/{\mu(\sigma)}$ with relative error, but there is no guarantee of relative accuracy for $1 - {\nu(\sigma)}/{\mu(\sigma)}$. Due to this difficulty, previous works mainly studied graphical models on bounded treewidth graphs~\cite{0001GMM0V24} and product distributions~\cite{FGJW23}, where ${\nu(\sigma)}$ and ${\mu(\sigma)}$ can be computed \emph{exactly}.

We overcome this challenge by designing an alternative estimator with good concentration property. 
We use the hardcore model as an example to illustrate the main idea. 
Assume both $\mu$ and $\nu$ are hardcore models $(G,\lambda^\mu)$ and $(G,\lambda^\nu)$ satisfying uniqueness condition in~\eqref{eq:cond-hardcore}.
For the simplicity of the overview, let us  further assume that for any for $v \in V$, $0 < \lambda_v^\mu \leq \lambda_v^\nu$. In words, $\nu$ is obtained by increasing some external fields of $\mu$ by a small amount. We will deal with the general case in later technical sections.

\paragraph{Basic case without small external fields} 
Let us start with a simple case where $\lambda^\mu_v = \Theta(\frac{1}{\Delta})$ for all $v \in V$. The uniqueness condition guarantees that $\lambda^\mu_v = O(\frac{1}{\Delta})$. The assumption additionally requires that every $\lambda^\mu_v$ cannot be too small. By the definitions of total variation distance and Gibbs distributions, we can compute
\begin{align}\label{eq:TV-2}
     \DTV{\mu}{\nu} &= \frac{1}{2}\sum_{\sigma \in \{\pm\}^V}\mu(\sigma)\left|1-\frac{\nu(\sigma)}{\mu(\sigma)}\right| %= \frac{1}{2}\sum_{\sigma \in \{\pm\}^V}\mu(\sigma)\left|1-\frac{w_\nu(\sigma)Z_{\mu}}{w_\mu(\sigma)Z_{\nu}}\right|\notag 
     = \frac{Z_\mu}{2Z_\nu}\cdot \sum_{\sigma \in \{\pm\}^V}\mu(\sigma)\left| \frac{Z_{\nu}}{Z_{\mu}} -\frac{w_\nu(\sigma)}{w_\mu(\sigma)}\right|.
\end{align}
The first term $\frac{Z_\mu}{2Z_\nu}$ can be approximated with relative error by approximate counting oracle. For the second term, we consider the following random variable, which also appears in the previous work of approximate counting~\cite{SVV09},
\begin{align*}
 W \defeq \frac{w_{\nu}(\sigma)}{w_{\mu}(\sigma)}, \text{ where } \sigma \sim \mu.
\end{align*}
Note that $\E[]{W} = \frac{Z_\nu}{Z_\mu}$. By~\eqref{eq:TV-2}, our task is reduced to estimate the value of $\E[]{\vert \E[]{W} - W \vert}$. We are in the case that $ 0\leq \lambda^\nu_v-\lambda^\mu_v \leq \dis(\mu,\nu) < \theta = \frac{1}{\mathrm{poly}(n)}$, so that for any $\sigma \in \{\pm\}^V$, $\frac{w_\mu(\sigma)}{w_\nu(\sigma)} \approx 1$.
We will utilize this property to show that $W$ has a good concentration property. Formally, 
%The key property we use is that since $\dis(\mu,\nu) < \theta$, then for any , 
%$W$ has a very good concentration property.  
 %To avoid the discussion of some technique details, consider a simple case when $\lambda^\mu_v = \Theta(\frac{1}{\Delta})$ and $\lambda_v^\mu \leq \lambda_v^\nu$ for all $v \in V$ in this overview.
%Since $\dis(\mu,\nu) = \Vert \lambda^\nu-\lambda            ^\mu \Vert_\infty < \theta = \frac{1}{\Delta n}$, we can bound the value of the ratio as follows 
\begin{align}\label{eq:w-bound}
 1\leq \frac{w_\nu(\sigma)}{w_{\mu}(\sigma)} &\leq  \prod_{v \in V: \sigma_v = +1} \frac{\lambda_v^\nu}{\lambda^\mu_v} \leq \prod_{v \in V: \sigma_v = +1}\left(1 + \frac{\dis(\mu,\nu)}{\lambda^\mu_v}\right) \notag\\
 (\star)\quad&\leq 1 + O \left( \sum_{v \in V: \sigma_v = +1} \frac{\dis(\mu,\nu)}{\lambda^\mu_v} \right) \leq 1 + O(n\Delta) \cdot \dis(\mu,\nu).
\end{align}
In inequality $(\star)$, we use the fact that $\lambda^\mu_v = \Omega(\frac{1}{\Delta})$ so that $\frac{\dis(\mu,\nu)}{\lambda^\mu_v} = O(\Delta \theta)$, 
since we choose $\theta = \frac{1}{\text{poly}(n)}$ small enough, then $ \frac{\dis(\mu,\nu)}{\lambda^\mu_v} \ll \frac{1}{n}$ and the inequality can be verified as follows
\begin{align*}
\prod_{v \in V: \sigma_v = +1}\left(1 + \frac{\dis(\mu,\nu)}{\lambda^\mu_v}\right) \leq \exp \underbrace{ \tp{\sum_{v \in V: \sigma_v = +1}\frac{\dis(\mu,\nu)}{\lambda^\mu_v}}}_{\ll n \cdot (1/n) = o(1)}   = 1 + O \left( \sum_{v \in V: \sigma_v = +1} \frac{\dis(\mu,\nu)}{\lambda^\mu_v} \right).
\end{align*}
By~\Cref{lem:TV-lower}, $ \DTV{\mu}{\nu} = \Omega(\dis(\mu,\nu))$. This implies $W$ enjoys a very good concentration property such that
\begin{align}\label{eq:w-var}
    1 \leq W \leq 1 + O(n\Delta) \cdot \DTV{\mu}{\nu} \quad \implies \quad  \Var[]{W} \leq O(n^2\Delta^2) \cdot (\DTV{\mu}{\nu})^2.
\end{align}
%The above variance bound is obtained by a simple analysis.
Now, our algorithm can be outline as follows.
\begin{tcolorbox}[colback=lightgray!20, colframe=lightgray!18, coltitle=black, title={}]
    \begin{itemize}
        \item Call approximate counting oracles to obtain $\hat{Z}_\mu$ and $\hat{Z}_\nu$ with relative error $O({\epsilon})$.
        \item Draw $T=\mathrm{poly}(n/\epsilon)$ samples $W_1,\dots, W_T$ from $W$ independently.
        \item Compute $\bar{W}=\frac{1}{T}\sum_{i=1}^T W_i$. \hfill \texttt{(approximate  $\E[]{W}$)}
        \item Compute $\bar{E}=\frac{1}{T}\sum_{i=1}^{T} |W_i-\bar{W}|$. \hfill \texttt{(approximate $\E[]{|W-\E[]{W}|}$)}
        \item Return $\hat{d}=\frac{\hat{Z}_\mu}{2\hat{Z}_\nu}\bar{E}$.
        \end{itemize}
    \end{tcolorbox}
Using the variance bound in~\eqref{eq:w-var}, we can prove that with high probability, $\bar{E}$ approximates $\E[]{|W-\E[]{W}|}$ with an additive error of $O(\epsilon) \cdot \DTV{\mu}{\nu}$.
By~\eqref{eq:w-bound}, we can also verify that $\frac{Z_\mu}{Z_\nu} = O(1)$. Hence, we can bound the error as follows
\begin{align*}
   \hat{d} &\leq (1+O(\epsilon))\frac{Z_\nu}{2Z_\mu}\cdot\left( \E[]{|W-\E[]{W}|} + O(\epsilon) \cdot \DTV{\mu}{\nu} \right)%\\
   %&\leq (1+O(\epsilon))\DTV{\mu}{\nu} + (1+O(\epsilon))\frac{Z_\nu}{2Z_\mu} \cdot O(\epsilon) \cdot \DTV{\mu}{\nu}\\
   \leq (1+O(\epsilon))\DTV{\mu}{\nu}.
\end{align*}
A similar analysis gives the lower bound $\hat{d} \geq (1-O(\epsilon))\DTV{\mu}{\nu}$. This achieves the relative-error approximation of the TV-distance.
The above technique can be generalized to Ising models and hardcore models with a marginal lower bound.

%Carefully readers may notices that the random variables $W_i$ is correlated with the random variable $\bar{W}$ in our algorithm. As a consequence, the expectation of $\bar{E}$ may not be $\E[]{W-\E[]{W}|}$. However, our analysis only requires $\bar{E}$ to approximate $\E[]{|W-\E[]{W}|}$ with certain small error. We do not require $\bar{E}$ to be an unbiased estimator of $\E[]{|W-\E[]{W}|}$.
%By drawing \(\mathrm{poly}(n / \epsilon)\) independent samples of \(W\), we can first estimate \(\E[]{W}\) and then estimate \(\E[]{|\E[]{W} - W|}\), both with an additive error of \(O(\epsilon) \cdot \DTV{\mu}{\nu}\). The complete algorithm is outlined as follows:
%\begin{itemize}
%    \item estimate the first term in~\eqref{eq:TV-2} with a relative error of \(O(\epsilon)\) using the approximate counting oracles for both $\mu$ and $\nu$.
%    \item estimate the second term in~\eqref{eq:TV-2} with an additive error of \(O(\epsilon) \cdot \DTV{\mu}{\nu}\) using $\mathrm{poly}(n/\epsilon)$ independent samples of \(W\).
%\end{itemize}
%Finally, note that the first term \(\frac{Z_\mu}{2Z_\nu} = O(1)\) due to~\eqref{eq:w-bound}. By combining the estimators for the two terms, we approximate the value of \(\DTV{\nu}{\mu}\) with a relative error of \(\epsilon\).

\paragraph{General case containing small external fields} 
For the general case, there may exist vertex $v \in V$ such that the external field $\lambda^\mu_v \ll \dis(\mu,\nu)$. In this case, the inequality $(\star)$ in~\eqref{eq:w-bound} may not hold. In fact, it will cause a fundamental problem to the above algorithm. Consider the case that $\lambda^\mu_v = \exp(-n)$ and $\lambda^\nu_v = \lambda^\mu_v + D$ for all $v \in V$, where $D > 0$. If we draw polynomial number of samples $\sigma \sim \mu$, typically, every sample $\sigma$ corresponds to the empty set, i.e., $\sigma_u = -1$ for all $ u \in V$. Hence, with high probability, all $W_i$ in the above algorithm are $\frac{w_\nu(\emptyset)}{w_\mu(\emptyset)} = 1$ and the algorithm will return $\hat{d} = 0$. However, the true TV-distance $\DTV{\mu}{\nu} > 0$ is positive. 

Let us first consider a special case such that for all $v \in V$, $\lambda^\mu_v < \kappa$ and $\lambda^\nu_v < \kappa$, where $\kappa =  \mathrm{poly}(\frac{\epsilon}{n}) \ll \frac{1}{n}$ is very small. 
%
The total variation distance is the sum of $\frac{1}{2}|\mu(\sigma)-\nu(\sigma)|$ for all independent sets $\sigma \in \{\pm\}^V$.  
In this special case, all external fields are tiny, so that large independent sets appear with very low probability.
Let $\Vert \sigma\Vert_+$ be the number of $+1$ in $\sigma$.
We can show that there is a constant $t = O(1)$ depending on $\kappa$ such that
\begin{align*}
    \frac{1}{2}\sum_{\sigma \in \{\pm\}^V: \Vert \sigma\Vert_+ \geq t+1} |\mu(\sigma)-\nu(\sigma)| \leq O(\epsilon) \cdot \DTV{\mu}{\nu}.
\end{align*} 
In words, to approximate the total variation distance, we only need to consider the independent sets with size at most $t$.
Now, we define a distribution $\mu'$ as the distribution $\mu$ restricted on the independent sets with size at most $t$.
Similarly, we can define $\nu'$ from $\nu$. 
Since $t$ is a constant, we can enumerate all independent sets with size at most $t$ and compute the total variation distance between $\mu'$ and $\nu'$.
We can show that $\DTV{\mu'}{\nu'}$ approximate $\DTV{\mu}{\nu}$ with $\epsilon$ relative error.

For the most general case, we divide the vertices into two groups. The big group $B$ contains all vertices $v \in V$ such that $\min\{\lambda^\mu_v,\lambda^\nu_v\} > \kappa$. The small group $S$ contains all vertices $v \in V$ such that $\min\{\lambda^\mu_v,\lambda^\nu_v\} \leq \kappa$ for a small $\kappa = \mathrm{poly}(\frac{\epsilon}{n})$. The total variation distance is
\begin{align*}
    \DTV{\mu}{\nu} &= \frac{1}{2}\sum_{\sigma \in \{\pm\}^V} \left|\mu(\sigma)-{\nu(\sigma)}\right| = \frac{1}{2}\sum_{x \in \{\pm\}^B}\sum_{y \in \{\pm\}^S} \left|\mu_B(x)\mu_S^x(y)-\nu_B(x)\nu_S^x(y)\right|\\
    &= \sum_{x \in \{\pm\}^B}\mu_B(x) \cdot  \underbrace{\frac{1}{2}\sum_{y \in \{\pm\}^S} \left|\frac{\nu_B(x)}{\mu_B(x)}\nu_S^x(y)-\mu_S^x(y)\right|}_{f(x)}.
\end{align*}
In a high level, our algorithm will draw independent samples $x \sim \mu_B$ from the marginal distribution and approximately compute the value of $f(x)$. The algorithm finally outputs the average value of $f(x)$ over all samples. 
\ifthenelse{\boolean{conf}}{We need to deal with the following two main technical challenging.}{To make the idea work, we need to deal with the following two main technical challenging.}
\begin{itemize}
    \item We need to bound the variance of $f(x)$ where $x \sim \mu_B$ to show that polynomial number of samples are sufficient for approximation. To achieve a good bound, we use the \emph{Poincar\'e inequality} for hardcore model in uniqueness regime~\cite{ChenFYZ21} to control the variance.
    \item Given a sample $x \sim \mu_B$, we also need to approximately compute the value of $f(x)$. We need to (1) approximate two distributions $\mu_S^x$ and $\nu_S^x$ over $\{\pm\}^S$; and (2) approximate the ratio $\frac{\nu_B(x)}{\mu_B(x)}$. For the first task, note that both $\mu_S^x$ and $\nu_S^x$ are hardcore distributions on the induced subgraph $G[S]$ such that for all $v \in S$, the external fields are small. Hence, we can approximate them using distributions over independent sets of size at most $t = O(1)$. A similar idea will also be use for the second task: to estimate marginal probabilities $\mu_B(x)$ and $\nu_B(x)$, we also need to consider the total weight of all independent sets $I$ in $G[S]$ such that $I \cup \{v \in B \mid x_v = +1\}$ forms a independent set in $G$. Again, we show that the approximation algorithm only needs to consider all such $I$ with $|I| \leq t = O(1)$.
\end{itemize}
All the technical details for general case are given in \Cref{sec:var-main}\ifthenelse{\boolean{conf}}{ of the Appendix}{}.




\ifthenelse{\boolean{conf}}{\paragraph{Organization of the Appendix}}{\paragraph{Organization of the paper}}
\Cref{lem:TV-lower} is proved in \Cref{sec:lower}. The additive error approximation algorithm is given in \Cref{sec:add}. The algorithm for instances with small parameter distance in given in \Cref{sec:alg-main}. In \Cref{sec:proof-main}, we put all the pieces together to prove all algorithmic results. 
Our hardness results on approximating TV-distance for marginal distributions is proved in \Cref{sec:hard}.

%\ifthenelse{\boolean{conf}}{\Cref{sec:hard} is for the proof of our hardness results.}{Finally, \Cref{sec:hard} is for the proof of hardness results.}
