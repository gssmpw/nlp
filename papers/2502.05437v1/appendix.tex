\section{NP-hardness of approximating the TV-distance} \label{app:proof}
In this section, we prove the hardness results of approximating the TV-distance between two Gibbs distributions beyond the uniqueness threshold. 
The proof is based on the technique developed in~\cite{BGMMPV24ICLR}.
We define the following instance family for TV-distance approximation.
\begin{problem}\label{prob:hardcore-tv}
Let $\Delta \geq 3$ and $\lambda > \lambda_c(\Delta) = \frac{(\Delta-1)^{\Delta-1}}{(\Delta-2)^{\Delta}}$ be two constants.
\begin{itemize}
    \item \emph{Input}: two hardcore models $(G,\lambda^\mu)$ and $(G,\lambda^\nu)$ defined on the same graph $G = (V,E)$ with maximum degree at most $\Delta$, which specifies two Gibbs distributions $\mu$ and $\nu$ respectively, and an error bound $0<\epsilon<1$. 
    There exists a vertex $v^* \in V$ such that 
    the external fields $\lambda^\mu_v = \lambda^\nu_v = \lambda$ for all $v \neq v^*$, $\lambda^\nu_{v^*} = \infty$, and $\lambda^\mu_{v^*} = \lambda$.
    \item \emph{Output}: a number $\hat{d}$ such that $|\DTV{\mu}{\nu} - \hat{d}| \leq \epsilon$.
\end{itemize}
\end{problem}

The two input hardcore models are not in the uniqueness regime because $\lambda > \lambda_c(\Delta)$.  The output only requires to approximate the TV-distance up to an additive error of $\epsilon$, which is weaker requirement than the relative-error approximation. The hardness result for this problem implies the hardness of relative-error approximation.

\begin{theorem} \label{thm:hardcore-tv}
There is no FPRAS for \Cref{prob:hardcore-tv} unless $\textbf{NP}=\textbf{RP}$.
\end{theorem}

To prove \Cref{thm:hardcore-tv}, we need the following lemma, which can be abstracted from the proofs in~\cite{BGMMPV24ICLR}.

\ifthenelse{\boolean{conf}}{\begin{lemma}[\cite{BGMMPV24ICLR}]}{\begin{lemma}[\cite{BGMMPV24ICLR}]}
\label{lem:hardcore-tv-lemma}
Let $\mu$ be a distribution over $\{\pm\}^V$.
Let $v \in V$ and $c \in \{\pm\}$ with $\mu_v(c) > 0$.
Let $\mu^{vc}$ be the distribution over $\{\pm\}^{V}$ obtained from $\mu$ by conditioning on $v$ taking value $c$. Then, $ \DTV{\mu}{\mu^{vc}} = \mu_v(-c)$.
\end{lemma}
\begin{proof}
For any $\sigma \in \{\pm\}^V$ with $\sigma_v = c$, we have $\mu^{vc}(\sigma) \geq \mu(\sigma)$. For any $\tau \in \{\pm\}^V$ with $\tau_v =  -c$, we have $\mu^{vc}(\tau) = 0 \leq \mu(\tau)$. Therefore, $\DTV{\mu}{\mu^{vc}} = \sum_{\tau \in \{\pm\}^V:\tau_v = -c}\mu(\tau) = \mu_v(-c)$.
\end{proof}

\begin{proof}\ifthenelse{\boolean{conf}}{\textbf{of \Cref{thm:hardcore-tv}}}{[Proof of \Cref{thm:hardcore-tv}]}
Let $\Delta \geq 3$ and $\lambda > \lambda_c(\Delta) = \frac{(\Delta-1)^{\Delta-1}}{(\Delta-2)^{\Delta}}$ be two constants.
By the hardness results in~\cite{SlyS12,GalanisSV16}, unless $\textbf{NP}=\textbf{RP}$, there is no FPRAS for approximating the partition function of the hardcode model $\mathbb{S}=(G,(\lambda_v)_{v \in V})$ with $\epsilon$-relative error, where $\lambda_v = \lambda$ for all $v \in V$ and $G$ has the maximum degree $\Delta$. By the standard counting-to-sampling reduction~\cite{JVV86}, approximating the partition function $Z_{\mathbb{S}}$ is equivalent to approximating the probability of $\pi(\sigma^\emptyset)$, where $\pi$ is the Gibbs distribution of $\mathbb{S}$ and $\sigma^\emptyset_v = -1$ for all $v \in V$. In other words, $\sigma^\emptyset$ corresponds to the empty set.
Let us number all the vertices in $V$ as $\{1,2,\cdots,n\}$. To approximate $\pi(\sigma^\emptyset)$, we need to approximate the probability $p_i \defeq \pi_i(-1 \mid \forall j < i, j \text{ takes value } -1)$  with relative error $O(\frac{\epsilon}{n})$. This probability is the same as the marginal probability of $i$ in the induced subgraph $G[V\setminus\{1,2,\cdots,i-1\}]$. We show how to approximate $p_1 = \pi_1(-1)$. We let $\mathbb{S}^\mu  = \mathbb{S}$ and define $\mathbb{S}^\nu$ for $\mathbb{S}$ by changing $\lambda^\nu_1$ to $\infty$. 
By \Cref{lem:hardcore-tv-lemma}, we have $\DTV{\mu}{\nu} = \mu_1(-1) = \pi_1(-1)$. It is easy to verify that $\pi_1(-1) \geq \frac{1}{1+\lambda} = \Omega(1)$ has a constant lower bound. Hence, if we can solve \Cref{prob:hardcore-tv} with $O(\epsilon/n)$-additive error, we can approximate $\pi_1(-1)$ with $O(\epsilon/n)$-relative error. The same argument can be applied to other probabilities by considering the instances in induced subgraphs. Hence, if \Cref{prob:hardcore-tv} admits an FPRAS, then there is an FPRAS for approximating the partition function of $\mathbb{S}$.
\end{proof}


For the Ising model, one can also verify the instance family stated after \Cref{Cor:Ising} is a hard instance family for approximating the TV-distance with additive error. The proof is similar to the one for the hardcore model. 
Let $\Delta \geq 3$ be a constant and $\beta < 0$ be a constant with $\exp(2\beta) < \frac{\Delta-2}{\Delta}$.
Let $\mathbb{S} = (G,J,h)$ such that $J_{uv} = \beta$ for all $\{u,v\} \in E$ and $h_v = 0$ for all $v \in V$. 
The starting point is the \text{NP}-hardness of approximating the partition function of $\mathbb{S}$~\cite{SlyS12,GalanisSV16}. 
Then one can go through the same reduction to estimate $p_i \defeq \pi_i(-1 \mid \forall j < i, j \text{ takes value } -1)$. We can construct $\mathbb{S}^\mu = (G,J,h^\mu)$ and $\mathbb{S}^\nu = (G,J,h^\nu)$ such that for all $j < i$, $h^\mu_j = h^\nu_j = -\infty$, for all $k > i$, $h^\mu_k = h^\nu_k = 0$, and $h^\mu_i = 0,h^\nu_i = \infty$. By \Cref{lem:hardcore-tv-lemma}, we have $\DTV{\mu}{\nu} = p_i$. One can verify $p_i = \Omega(1)$ so that $O(\epsilon/n)$-additive error approximation of $\DTV{\mu}{\nu}$ is equivalent to $O(\epsilon/n)$-relative error approximation of $p_i$. The hardness result follows from the same argument as in the proof of \Cref{thm:hardcore-tv}.


\section{Poincar\'e inequality for marginal distribution}\label{app:poin}
The Poincar\'e inequality for hardcore model in the uniqueness regime is established in~\cite{ChenFYZ21}. The paper considers the hardcore model $(G,\lambda)$ such that all $\lambda_v$ are the same. By verifying the technical condition in\ifthenelse{\boolean{conf}}{~\cite[Theorem 1.9]{ChenFYZ21}}{~\cite[Theorem 1.9]{ChenFYZ21}}, the following Poincar\'e inequality also holds for hardcore model with different $\lambda_v$ satisfying~\eqref{eq:cond-hardcore}. For any function $g: \Omega \to \mathbb{R}$, where $\Omega$ is the support of the distribution, we have
\begin{align*}
\Var[\mu]{g} \leq C_\eta \sum_{v \in V} \sum_{\sigma \in \Omega_{V -v}} \mu_{V - v}(\sigma) \Var[\mu^\sigma]{g}.
\end{align*}
Let $g': \Omega_B \to \mathbb{R}$ be an arbitrary function. 
Define $g(x) = g'(x_B)$ for all $x \in \Omega$. It holds that $\E[\mu]{g} =\E[\mu_B]{g'}$ and $\E[\mu]{g^2} = \E[\mu_B]{(g')^2}$. We have
\begin{align*}
    \Var[\mu_B]{g'} &= \Var[\mu]{g}  \leq C_\eta \sum_{v \in V} \sum_{\sigma \in \Omega_{V -v}} \mu_{V - v}(\sigma) \Var[\mu^\sigma]{g}\\
\text{($g(\sigma)$ depends on $\sigma_B$)}\quad    &= C_\eta \sum_{v \in B}  \sum_{\sigma \in \Omega_{V -v}} \mu_{V - v}(\sigma) \Var[\mu^\sigma]{g}.
\end{align*}
Fix a partial assignment $\tau \in \Omega_{B-v}$. Note that $\E[\mu^\tau]{g} = \E[\mu^\tau_B]{g'}$ and  $\E[\mu^\tau]{g^2} = \E[\mu^\tau_B]{(g')^2}$. We have $\Var[\mu^\tau]{g} = \Var[\mu^\tau_B]{g'}$. Let $X \sim \mu^\tau$ and $Y = g(X)$. By the law of total variance,
\begin{align*}
  \Var[\mu^\tau_B]{g'} &= \Var[\mu^\tau]{g} = \Var[]{Y}\\
   &= \E[]{\Var[]{Y \mid X_{V-v}}} + \Var[]{\E[]{Y \mid X_{V-v}}}\\
   &\geq \E[]{\Var[]{Y \mid X_{V-v}}}\\
   &= \sum_{\sigma \in \Omega^\tau_{V-v}} \mu^\tau_{V-v}(\sigma) \Var[\mu^\sigma]{g},
\end{align*}
where $\Omega^\tau_{V-v} \subseteq \{\pm\}^{V-v}$ is the support of $\mu^\tau_{V-v}$.
Combining with the above inequality, we have
\begin{align*}
    \Var[\mu_B]{g'} &\leq C_\eta \sum_{v \in B}  \sum_{\tau \in \Omega_{B -v}} \mu_{B-v}(\tau) \sum_{\sigma \in \Omega^\tau_{V-v}} \mu^\tau_{V-v}(\sigma) \Var[\mu^\sigma]{g}\\ &\leq C_\eta \sum_{v \in B}  \sum_{\tau \in \Omega_{B -v}} \mu_{B-v}(\tau) \Var[\mu^\tau_B]{g'}.
\end{align*}
This proves the Poincar\'e inequality for the marginal distribution.

Alternatively, one can also use the fact the the Poincar\'e inequality is equivalent to the decay of $\chi^2$-divergence in the down walk of Glauber dynamics. The results follows from the data processing inequality. See \ifthenelse{\boolean{conf}}{~\cite[Section 6.1]{FGJW23}}{~\cite[Section 6.1]{FGJW23}} for more details.