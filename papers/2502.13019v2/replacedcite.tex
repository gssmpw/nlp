\section{Related Work}
\subsection{Post-retrieval Enhancement for RAG}
Post-processing methods are widely employed to refine retrieved content for improved downstream generation. These methods can be categorized as follows:

\textbf{Reranking}. Rerankers reorder and prioritize retrieved documents to emphasize the most relevant results. They typically operate sequentially or iteratively after retrieval, leveraging various criteria such as semantic relevance between query and passages ____, connections among documents ____, the majority of reader predictions ____, and utility for generation ____. Rerankers are usually based on cross-encoder (\emph{e.g} BGE ____, Mixedbread ____), multi-vector models (\emph{e.g} ColBert ____). Recent works explore using LMs as rerankers (\emph{e.g}, RankT5 ____, RankZephyr ____, RankGPT ____).  

\textbf{Filtering}. Filtering is the process of selectively removing irrelevant, redundant, or low-quality information from retrieved documents to enhance their usefulness. Filtering can be performed at different granularity levels such as passage-level ____, sentence-level ____ or finer token-level ____.  % self-RAG filters out irrelevant passages by self-reflection mechanism
\begin{table}[H]
\caption{\textmd{QA performance with zero-shot setting. PopQA $\to$ NQ represents the model trained on PopQA is applied to NQ.}}
  \centering
  % \resizebox{\linewidth}{!}{
    \begin{tabular}{ccc}
          \toprule
          \textbf{Dataset} & \textbf{Model $\to$ Dataset}  & \textbf{Performance} \\ \hline
     \multirow{3}[3]{*}{NQ} & NQ $\to$ NQ & 0.4413 \\
     & PopQA $\to$ PopQA & 0.4682 \\
     & PopQA $\to$ NQ & 0.4352 \\
    \midrule
    \multirow{3}[3]{*}{HotpotQA} & HotpotQA $\to$ HotpotQA & 0.6775 \\
     & 2WQA $\to$ 2WQA & 0.6384 \\
     & 2WQA $\to$ HotpotQA & 0.6344 \\
    \bottomrule
    \end{tabular}
  \label{tab:zero-shot}%
\end{table}%
 \textbf{Post verification and correction}. Some studies incorporate post evaluation ____, fact-checking ____ or attribution ____ mechanisms to further solidify the accuracy and reliability of the retrieved documents and responses. For example,  ____ uses a retrieval evaluator accessing the relevance of the retrieved documents and estimating a correctness confidence degree, then self-correct the retrieved results. RETA-LLM ____ includes a fact checking module to verify whether there exist factual errors in the generated answers. by using a natural language understanding (NLU) model. CHAIN-OF-NOTE (CON) ____ accesses relevance of each retrieved document to the query by generating sequential reading notes. These notes are integrated during final answer generation. 
 
 \textbf{Summarizing} or \textbf{compressing}. Summarization and compression of retrieved content aim to create concise, faithful contexts that enhance the efficiency and effectiveness of downstream generation ____. Several methods have been proposed to achieve this. ____ proposed dual-encoder compressors for extractive and abstractive to eliminate irrelevant context and summarize upon the remaining content. Selective-Context ____ employs a small LM  compute the self-information of each lexical uni and then drop the less informative content. LongLLMLingua ____ proposes coarse-grained compression via document-level perplexity and fine-grained compression of remaining text based on token perplexity. 
 
\subsection{Reinforcement Learning for Large Language Models}
Reinforcement learning for Language Models (RL4LM) has emerged as a transformative technique to further enhance LLMs' performance during the post-training process ____. Traditional RL4LM usually involves a reward model, for example using PPO ____ to update the policy model (\emph{e.g} InstructGPT ____, GPT-4 ____). Some RL4LM such as Direct Preference Optimization (DPO) ____
and Reward-aware Preference Optimization (RPO) ____ get rid of reward model to provide more stable and computationally efficient solutions (\emph{e.g} Qwen 2 ____ and  Nemotron-4 ____). Common goals of RL4LM include improving performance of downstream NLP tasks ____, minimizing data and resource dependencies ____, aligning model outputs with user intent, values and goals ____, and adhering to responsible AI principles ____. Human feedback can be integrated into the framework by constructing preference datasets, which are then used to fine-tune both the policy and reward models (also termed as Reinforcement Learning from Human Feedback (RLHF)) ____. Some studies also explore RL4LM without human feedback ____ or replaced with AI feedback ____ by distillation from LLMs ____, prompting LLMs as reward functions ____, and self-rewarding ____, or using performance-based metrics such as fluency or coherence ____, and task-specific constraints over the distribution of language ____. In the specific domain of RAG, RRAML ____ employs RL to train an efficient retriever to search for relevant information in in arbitrarily large databases. PRCA ____ applies RL to further fine-tune the context extracted by a model pre-trained on domain-specific abstractive summarization tasks to optimize the reward for the generator. BIDER ____ adopts RL to bridge the inconsistency between the retriever and generator.