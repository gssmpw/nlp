\section{Related Work}
\subsection{Post-retrieval Enhancement for RAG}
Post-processing methods are widely employed to refine retrieved content for improved downstream generation. These methods can be categorized as follows:

\textbf{Reranking}. Rerankers reorder and prioritize retrieved documents to emphasize the most relevant results. They typically operate sequentially or iteratively after retrieval, leveraging various criteria such as semantic relevance between query and passages \cite{glass-etal-2022-re2g, hofstatter2023fid}, connections among documents \cite{dong2024don}, the majority of reader predictions \cite{mao-etal-2021-reader}, and utility for generation \cite{ma-etal-2023-large}. Rerankers are usually based on cross-encoder (\emph{e.g} BGE \cite{bgeembedding}, Mixedbread \cite{li2023angle}), multi-vector models (\emph{e.g} ColBert \cite{khattab2020colbert, santhanam-etal-2022-colbertv2}). Recent works explore using LMs as rerankers (\emph{e.g}, RankT5 \cite{zhuang2023rankt5}, RankZephyr \cite{pradeep2023rankzephyr}, RankGPT \cite{sun-etal-2023-chatgpt}).  

\textbf{Filtering}. Filtering is the process of selectively removing irrelevant, redundant, or low-quality information from retrieved documents to enhance their usefulness. Filtering can be performed at different granularity levels such as passage-level \cite{asai-etal-2022-evidentiality, yoran2023making, asai2023self}, sentence-level \cite{wang2023learning, hwang-etal-2024-dslr} or finer token-level \cite{jin-etal-2024-bider, anderson-etal-2022-lingua, jiang-etal-2024-longllmlingua}.  % self-RAG filters out irrelevant passages by self-reflection mechanism
\begin{table}[H]
\caption{\textmd{QA performance with zero-shot setting. PopQA $\to$ NQ represents the model trained on PopQA is applied to NQ.}}
  \centering
  % \resizebox{\linewidth}{!}{
    \begin{tabular}{ccc}
          \toprule
          \textbf{Dataset} & \textbf{Model $\to$ Dataset}  & \textbf{Performance} \\ \hline
     \multirow{3}[3]{*}{NQ} & NQ $\to$ NQ & 0.4413 \\
     & PopQA $\to$ PopQA & 0.4682 \\
     & PopQA $\to$ NQ & 0.4352 \\
    \midrule
    \multirow{3}[3]{*}{HotpotQA} & HotpotQA $\to$ HotpotQA & 0.6775 \\
     & 2WQA $\to$ 2WQA & 0.6384 \\
     & 2WQA $\to$ HotpotQA & 0.6344 \\
    \bottomrule
    \end{tabular}
  \label{tab:zero-shot}%
\end{table}%
 \textbf{Post verification and correction}. Some studies incorporate post evaluation \cite{yan2024corrective}, fact-checking \cite{liu2306reta} or attribution \cite{gao-etal-2023-rarr} mechanisms to further solidify the accuracy and reliability of the retrieved documents and responses. For example,  \cite{yan2024corrective} uses a retrieval evaluator accessing the relevance of the retrieved documents and estimating a correctness confidence degree, then self-correct the retrieved results. RETA-LLM \cite{liu2306reta} includes a fact checking module to verify whether there exist factual errors in the generated answers. by using a natural language understanding (NLU) model. CHAIN-OF-NOTE (CON) \cite{yu-etal-2024-chain} accesses relevance of each retrieved document to the query by generating sequential reading notes. These notes are integrated during final answer generation. 
 
 \textbf{Summarizing} or \textbf{compressing}. Summarization and compression of retrieved content aim to create concise, faithful contexts that enhance the efficiency and effectiveness of downstream generation \cite{xu2024recomp}. Several methods have been proposed to achieve this. \cite{xu2024recomp} proposed dual-encoder compressors for extractive and abstractive to eliminate irrelevant context and summarize upon the remaining content. Selective-Context \cite{li-etal-2023-compressing} employs a small LM  compute the self-information of each lexical uni and then drop the less informative content. LongLLMLingua \cite{jiang-etal-2024-longllmlingua} proposes coarse-grained compression via document-level perplexity and fine-grained compression of remaining text based on token perplexity. 
 
\subsection{Reinforcement Learning for Large Language Models}
Reinforcement learning for Language Models (RL4LM) has emerged as a transformative technique to further enhance LLMs' performance during the post-training process \cite{cao2024survey, pternea2024rl}. Traditional RL4LM usually involves a reward model, for example using PPO \cite{schulman2015high} to update the policy model (\emph{e.g} InstructGPT \cite{ouyang2022training}, GPT-4 \cite{achiam2023gpt}). Some RL4LM such as Direct Preference Optimization (DPO) \cite{rafailov2024direct}
and Reward-aware Preference Optimization (RPO) \cite{adler2024nemotron} get rid of reward model to provide more stable and computationally efficient solutions (\emph{e.g} Qwen 2 \cite{chu2024qwen2} and  Nemotron-4 \cite{adler2024nemotron}). Common goals of RL4LM include improving performance of downstream NLP tasks \cite{deng-etal-2022-rlprompt, ghalandari-etal-2022-efficient, ramamurthy2022reinforcement}, minimizing data and resource dependencies \cite{zhang2022tempera}, aligning model outputs with user intent, values and goals \cite{ouyang2022training}, and adhering to responsible AI principles \cite{bai2022training, bai2022constitutional}. Human feedback can be integrated into the framework by constructing preference datasets, which are then used to fine-tune both the policy and reward models (also termed as Reinforcement Learning from Human Feedback (RLHF)) \cite{bai2022training, ouyang2022training, hu2023aligning}. Some studies also explore RL4LM without human feedback \cite{rafailov2024direct} or replaced with AI feedback \cite{bai2022constitutional, yuan2024self} by distillation from LLMs \cite{cui2023ultrafeedback, park-etal-2024-offsetbias}, prompting LLMs as reward functions \cite{kwon2023reward, lee2023rlaif, zhang2024generative}, and self-rewarding \cite{yuan2024self}, or using performance-based metrics such as fluency or coherence \cite{ghalandari-etal-2022-efficient}, and task-specific constraints over the distribution of language \cite{ramamurthy2022reinforcement, wu2024beta}. In the specific domain of RAG, RRAML \cite{bacciu2023rraml} employs RL to train an efficient retriever to search for relevant information in in arbitrarily large databases. PRCA \cite{yang-etal-2023-prca} applies RL to further fine-tune the context extracted by a model pre-trained on domain-specific abstractive summarization tasks to optimize the reward for the generator. BIDER \cite{jin-etal-2024-bider} adopts RL to bridge the inconsistency between the retriever and generator.