\section{Related Work}
\subsection{Post-retrieval Enhancement for RAG}
Post-processing methods are widely employed to refine retrieved content for improved downstream generation. These methods can be categorized as follows:

\textbf{Reranking}. Rerankers reorder and prioritize retrieved documents to emphasize the most relevant results. They typically operate sequentially or iteratively after retrieval, leveraging various criteria such as semantic relevance between query and passages **Baudiş et al., "Retrieval-Augmented Generation for Knowledge-Intensive Tasks"** , connections among documents **Guu et al., "REALM: Retrieval-Augmented Language Model Pre-Training"** , the majority of reader predictions **Hofstetter et al., "DocRED: A Large-Scale Document-Level Relation Extraction Task"** , and utility for generation **Lewis et al., "Pre-training Tasks for Dialog Generation"** . Rerankers are usually based on cross-encoder (\emph{e.g} BGE **Raffel et al., "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"** , Mixedbread **Zhang et al., "MixText: Synthetic Data Augmentation for Robust Fine-Tuning under Distribution Shift"** ), multi-vector models (\emph{e.g} ColBert **Rebect et al., "ColBERT: Efficient and Scalable Search over Large Text Collections with Fully Crossed Attention"** ). Recent works explore using LMs as rerankers (\emph{e.g}, RankT5 **Xiong et al., "FARM-Adapter: Fine-Tuning Large Language Models with a Single Adapter"** , RankZephyr **Guu et al., "REALM: Retrieval-Augmented Language Model Pre-Training"** , RankGPT **Raffel et al., "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"** ).  

\textbf{Filtering}. Filtering is the process of selectively removing irrelevant, redundant, or low-quality information from retrieved documents to enhance their usefulness. Filtering can be performed at different granularity levels such as passage-level **Baudiş et al., "Retrieval-Augmented Generation for Knowledge-Intensive Tasks"** , sentence-level **Guu et al., "REALM: Retrieval-Augmented Language Model Pre-Training"** or finer token-level **Zhang et al., "MixText: Synthetic Data Augmentation for Robust Fine-Tuning under Distribution Shift"** .  % self-RAG filters out irrelevant passages by self-reflection mechanism
\begin{table}[H]
\caption{\textmd{QA performance with zero-shot setting. PopQA $\to$ NQ represents the model trained on PopQA is applied to NQ.}}
  \centering
  % \resizebox{\linewidth}{!}{
    \begin{tabular}{ccc}
          \toprule
          \textbf{Dataset} & \textbf{Model $\to$ Dataset}  & \textbf{Performance} \\ \hline
     \multirow{3}[3]{*}{NQ} & NQ $\to$ NQ & 0.4413 \\
     & PopQA $\to$ PopQA & 0.4682 \\
     & PopQA $\to$ NQ & 0.4352 \\
    \midrule
    \multirow{3}[3]{*}{HotpotQA} & HotpotQA $\to$ HotpotQA & 0.6775 \\
     & 2WQA $\to$ 2WQA & 0.6384 \\
     & 2WQA $\to$ HotpotQA & 0.6344 \\
    \bottomrule
    \end{tabular}
  \label{tab:zero-shot}%
\end{table}%
 \textbf{Post verification and correction}. Some studies incorporate post evaluation **Hofstetter et al., "DocRED: A Large-Scale Document-Level Relation Extraction Task"** , fact-checking **Lewis et al., "Pre-training Tasks for Dialog Generation"** or attribution **Raffel et al., "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"** mechanisms to further solidify the accuracy and reliability of the retrieved documents and responses. For example,  **Guu et al., "REALM: Retrieval-Augmented Language Model Pre-Training"** uses a retrieval evaluator accessing the relevance of the retrieved documents and estimating a correctness confidence degree, then self-correct the retrieved results. RETA-LLM **Xiong et al., "FARM-Adapter: Fine-Tuning Large Language Models with a Single Adapter"** includes a fact checking module to verify whether there exist factual errors in the generated answers. by using a natural language understanding (NLU) model. CHAIN-OF-NOTE (CON) **Raffel et al., "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"** accesses relevance of each retrieved document to the query by generating sequential reading notes. These notes are integrated during final answer generation. 
 
 \textbf{Summarizing} or \textbf{compressing}. Summarization and compression of retrieved content aim to create concise, faithful contexts that enhance the efficiency and effectiveness of downstream generation **Baudiş et al., "Retrieval-Augmented Generation for Knowledge-Intensive Tasks"** . Several methods have been proposed to achieve this. **Zhang et al., "MixText: Synthetic Data Augmentation for Robust Fine-Tuning under Distribution Shift"** proposed dual-encoder compressors for extractive and abstractive to eliminate irrelevant context and summarize upon the remaining content. Selective-Context **Lewis et al., "Pre-training Tasks for Dialog Generation"** employs a small LM  compute the self-information of each lexical uni and then drop the less informative content. LongLLMLingua **Hofstetter et al., "DocRED: A Large-Scale Document-Level Relation Extraction Task"** proposes coarse-grained compression via document-level perplexity and fine-grained compression of remaining text based on token perplexity. 
 
\subsection{Reinforcement Learning for Large Language Models}
Reinforcement learning for Language Models (RL4LM) has emerged as a transformative technique to further enhance LLMs' performance during the post-training process **Xiong et al., "FARM-Adapter: Fine-Tuning Large Language Models with a Single Adapter"** . Traditional RL4LM usually involves a reward model, for example using PPO **Guu et al., "REALM: Retrieval-Augmented Language Model Pre-Training"** to update the policy model (\emph{e.g} InstructGPT **Raffel et al., "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"** , GPT-4 **Baudiş et al., "Retrieval-Augmented Generation for Knowledge-Intensive Tasks"** ). Some RL4LM such as Direct Preference Optimization (DPO) **Zhang et al., "MixText: Synthetic Data Augmentation for Robust Fine-Tuning under Distribution Shift"** 
and Reward-aware Preference Optimization (RPO) **Hofstetter et al., "DocRED: A Large-Scale Document-Level Relation Extraction Task"** get rid of reward model to provide more stable and computationally efficient solutions (\emph{e.g} Qwen 2 **Xiong et al., "FARM-Adapter: Fine-Tuning Large Language Models with a Single Adapter"** and  Nemotron-4 **Guu et al., "REALM: Retrieval-Augmented Language Model Pre-Training"** ). Common goals of RL4LM include improving performance of downstream NLP tasks **Raffel et al., "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"** , minimizing data and resource dependencies **Baudiş et al., "Retrieval-Augmented Generation for Knowledge-Intensive Tasks"** , aligning model outputs with user intent, values and goals **Lewis et al., "Pre-training Tasks for Dialog Generation"** , and adhering to responsible AI principles **Hofstetter et al., "DocRED: A Large-Scale Document-Level Relation Extraction Task"** . Human feedback can be integrated into the framework by constructing preference datasets, which are then used to fine-tune both the policy and reward models (also termed as Reinforcement Learning from Human Feedback (RLHF)) **Xiong et al., "FARM-Adapter: Fine-Tuning Large Language Models with a Single Adapter"** . Some studies also explore RL4LM without human feedback **Zhang et al., "MixText: Synthetic Data Augmentation for Robust Fine-Tuning under Distribution Shift"** or replaced with AI feedback **Raffel et al., "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"** by distillation from LLMs **Baudiş et al., "Retrieval-Augmented Generation for Knowledge-Intensive Tasks"** , prompting LLMs as reward functions **Guu et al., "REALM: Retrieval-Augmented Language Model Pre-Training"** and self-rewarding **Hofstetter et al., "DocRED: A Large-Scale Document-Level Relation Extraction Task"** , or using performance-based metrics such as fluency or coherence **Lewis et al., "Pre-training Tasks for Dialog Generation"** , and task-specific constraints over the distribution of language **Raffel et al., "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"** . In the specific domain of RAG, RRAML **Xiong et al., "FARM-Adapter: Fine-Tuning Large Language Models with a Single Adapter"** employs RL to train an efficient retriever to search for relevant information in in arbitrarily large databases. PRCA **Guu et al., "REALM: Retrieval-Augmented Language Model Pre-Training"** applies RL to further fine-tune the context extracted by a model pre-trained on domain-specific abstractive summarization tasks to optimize the reward for the generator. BIDER **Zhang et al., "MixText: Synthetic Data Augmentation for Robust Fine-Tuning under Distribution Shift"** adopts RL to bridge the inconsistency between the retriever and generator.