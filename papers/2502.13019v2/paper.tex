%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\PassOptionsToPackage{prologue,dvipsnames}{xcolor}
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{naturalnames}{hyperref}
\pagestyle{plain} % removes running headers
\documentclass[sigconf,natbib=true,anonymous=false]{acmart}
\settopmatter{printacmref=false} % Removes citation information below abstract
\renewcommand\footnotetextcopyrightpermission[1]{} % removes footnote with conference information in first column
\pagestyle{plain} % removes running headers
\usepackage{xspace} %  
%\usepackage{xcolor}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{svg}
\usepackage{float}
\usepackage[dvipsnames]{xcolor}
\usepackage{underscore}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage[most]{tcolorbox} % define prompt box
\newtcolorbox[list inside=prompt,auto counter]{prompt}[1][]{
    colbacktitle=black!60,
    coltitle=white,
    fontupper=\footnotesize,
    boxsep=5pt,
    left=0pt,
    right=0pt,
    top=0pt,
    bottom=0pt,
    boxrule=1pt,
    #1,
}


\newcommand\ie{ie\@ifnextchar.{}{.\@}}
\newcommand\eg{eg\@ifnextchar.{}{.\@}}
\newcommand\etc{etc\@ifnextchar.{}{.\@}}
\def\oreo{\emph{\textbf{\textit{Oreo}}} }
%%\def\oreo{\emph{\textbf{\textit{Oreo}}} }
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{none}

\usepackage{ragged2e}
\usepackage{booktabs, makecell, multirow}
\usepackage{amsthm}
\usepackage{siunitx}
\usepackage{colortbl}
\usepackage{pifont}
\usepackage{makecell}
\usepackage{subfloat}
\usepackage{caption}
\usepackage{xcolor}
\usepackage{subcaption}
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

 
%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
%\title{Enhancing RAG by Transforming Retrieved Context}
\title{\includegraphics[height=20pt,width=20pt]{oreo3.png} Oreo: A Plug-in C\underline{o}ntext \underline{Re}c\underline{o}nstructor to Enhance Retrieval-Augmented Generation }
%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.

\author{Sha Li}
\affiliation{%
  \institution{Virginia Tech}
  \city{Blacksburg}
  \country{USA}}
\email{shal@vt.edu}

\author{Naren Ramakrishnan}
\affiliation{%
  \institution{Virginia Tech}
   \city{Arlington}
   \country{USA}}
\email{naren@vt.edu}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Li et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}

Despite the remarkable capabilities of Large Language Models (LLMs) in various NLP tasks, they remain vulnerable to hallucinations due to their limited parametric knowledge and lack of domain-specific expertise. Retrieval-Augmented Generation (RAG) addresses this challenge by incorporating external document retrieval to augment the knowledge base of LLMs. In this approach, RAG retrieves document chunks from an external corpus in response to a query, which are then used as context for the downstream language model to generate an answer. However, these retrieved knowledge sources often include irrelevant or erroneous information, undermining the effectiveness of RAG in downstream tasks. To overcome this limitation, we introduce a compact, efficient, and plugable module designed to refine external knowledge sources before feeding them to the generator. The module reconstructs retrieved content by extracting the most relevant and supportive information and reorganizing it into a concise, query-specific format. Through a three-stage training paradigm—comprising supervised fine-tuning, contrastive multi-task learning, and reinforcement learning-based alignment—it prioritizes critical knowledge and aligns it with the generator’s preferences. This method enables LLMs to produce outputs that are more accurate, reliable, and contextually appropriate.

\end{abstract}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Retrieval Augmented Generation, Prompt Optimization, Contrastive learning}


\settopmatter{printfolios=true} 
\maketitle
\pagestyle{plain}
\section{Introduction}
\label{intro}
Large language models (LLMs) have demonstrated extraordinary versatility in various natural language processing (NLP) tasks, subsuming pipelines that were originally tailormade for each task. Despite being trained on massive text corpora, LLMs still face memory-related challenges such as out-of-date and out-of-domain knowledge, and they occasionally hallucinate non-factual or non-sensical content~\cite{zhou21aclfindings, maynez-etal-2020-faithfulness}. To enhance the accuracy and reliability of LLM-generated outputs, retrieval-augmented generation (RAG) has emerged as a promising solution for knowledge-intensive tasks \cite{zhu2023large, gao2023retrieval, li2022survey} (\emph{e.g} open-domain question and answering).
RAG systems typically follow a \textit{``retrieve-then-generate"} paradigm \cite{shao-etal-2023-enhancing}, where a \textit{retriever} identifies relevant information from an external corpus and uses this information to augment
context in constituting the input to a generative model (\emph{i.e}, \textit {generator}), thus yielding an improved answer. 
\begin{figure}[H]
\begin{center}
  \includegraphics[scale=0.55]{images/introimage.pdf}
  \end{center}
  \caption{\textmd{An example illustrating {\color{red}{vanilla RAG}} versus {\color{ForestGreen}{RAG with \oreo}} highlights the impact of redundant and scattered information within the retrieved document chunks. In the vanilla RAG setup, even the retrieved chunks contain contextually relevant information and the correct answer, but the presence of distractions misguides the downstream LM to generate incorrect answers. In contrast, \oreo effectively captures the essential evidence and reconstructs the context, enabling the LM to generate accurate and correct responses.}}
  \Description{introimage}
  \label{fig:intro}
  \vspace{-1.1em}
\end{figure}
Despite its promise, a vanilla RAG system usually comes with shortcomings that can hinder its its effectiveness. One major issue is semantic dissonance between the user query, the retriever, and the generator. This occurs when the retrieved documents, while semantically or contextually related to the topic, fail to directly address the query, leading to suboptimal answers~\cite{cuconasu2024power, wu2024easily}. Another challenge pertains to the presence of noise, i.e., misleading, redundant, distracting, or even erroneous information within the retrieved documents. Such noise can misguide the generator, resulting in inaccurate or incoherent answers~\cite{sun2023contrastive, shi2023large}. For complex tasks that necessitate reasoning across multiple documents, the generator often struggles to correlate dependencies and relationships between them \cite{behnamghader2022can}, leading to reasoning errors.  For example, as illustrated in Figure \ref{fig:intro}, the correct answer is derived from two different document chunks, but the generator in the vanilla RAG pipeline fails to capture their connections due to the presence of redundant and distracting information. Additionally, RAG systems are prone to the ``lost-in-the-middle''\cite{liu2023lost} dilemma, where LMs exhibit a tendency to prioritize information presented at the beginning and end of an input sequence, while paying less attention to the middle. Finally, the lack of joint optimization between the retriever and the generator exacerbates issues such as \textit{knowledge inconsistencies} \cite{wang-etal-2023-causal} or \textit{knowledge conflicts} \cite{xu-etal-2024-knowledge-conflicts}, which prevent the generator from producing accurate and contextually appropriate responses as the retrieved knowledge fails to adequately support the generation.

To address these challenges, many solutions have been proposed in prior research. Techniques such as query decomposition \cite{chan2024rq, kim-etal-2023-tree}, query rewriting \cite{wang-etal-2023-query2doc,tan2024small, chan2024rq, ma-etal-2023-query}, and query expansion \cite{lei-etal-2024-corpus} aim to improve retriever performance by refining or enriching the input queries. Some studies have integrated rerankers \cite{yoran2023making, yu2023chain, nogueira2019multi} into retrieval systems,  which reorder and prioritize the most relevant documents to ensure that the most pertinent information is provided to the generator. These works attempt to optimize the context on the passage level and largely ensure relevance with the query, but they still face challenges in maintaining comprehensive attention to the nuanced, finer-grained details of query-specific information. Further advancements have been made in noise and redundancy exclusion. For example, filters based on lexical and information-theoretic approaches have been developed to identify and preserve useful content while directly eliminating less relevant information \cite{wang2023learning, li-etal-2023-compressing, jiang-etal-2024-longllmlingua}. Summarization techniques  \cite{xu2024recomp} have been developed to synthesize and condense query-focused information from retrieved documents, leveraging extraction or abstraction methods. Compression techniques \cite{chevalier-etal-2023-adapting, cao-etal-2024-retaining} extend this functionality by generating summary vectors that encode essential information for downstream tasks. While these methods improve efficiency, they do not align the retriever and generator in a manner that guarantees effective collaboration, which often result in knowledge gaps and consequently incorrect or suboptimal generation. From a training perspective, concurrent \cite{guu2020retrieval, lin2023ra, zamani2024stochastic, izacard2022few} or asynchronous \cite{zhang-etal-2024-arl2, shi-etal-2024-replug} training of retrievers and generators is a widely adopted strategy to improve their interaction and collaboration \cite{guu2020retrieval, borgeaud2022improving}. Although such techniques foster synergistic improvements, they can be computationally expensive and often require large amounts of annotated data to achieve optimal results.

Therefore, in this work, we introduce \oreo, a c\underline{O}ntext \underline{RE}c\underline{O}nstructor designed to enhance the performance of RAG systems on knowledge-intensive tasks by \textit{optimizing the quality of context} and \textit{mitigating knowledge inconsistencies}. \oreo is implemented in a plug-and-play manner, functioning as an intermediary module between the retriever and the generator. It receives original document chunks from the retriever and produces refined context tailored for the generator. Instead of merely extracting critical tokens from the chunks, \oreo reorganizes them and generates condensed query-aware summaries. Additionally, \oreo synergizes the reconstructed context with the generator’s behavior of knowledge acquisition, ultimately leading to more accurate and contextually relevant answers. To equip \oreo with robust capabilities for extracting supporting evidence from lengthy contexts and maximizing the accuracy of query responses, we adopt a three-stage fine-tuning paradigm: \textit{Stage 1 - supervised fine-tuning (SFT)}. \oreo is trained to acquire information extraction capabilities by leveraging annotated datasets and knowledge distilled from LLMs. \textit{Stage 2 - contrastive learning multitask learning (CML)}. \oreo enhances its generalizability and identification of self-errors during context reconstruction. \textit{Stage 3 - reinforcement learning (RL)}. The outputs of \oreo are further optimized and aligned with the knowledge demand of downstream generator. In this stage, \oreo learns by leveraging generator's feedback. To minimize training costs, \oreo utilizes a relatively lightweight language model as its backbone, and keeps the retriever and generator frozen as black-box components. This design ensures modularity and efficiency.

Our key contributions are:
\begin{enumerate}
    \item We propose enhancing the RAG by introducing a "retrieve-reconstruct-then-generate" paradigm, offering a novel perspective on refining retrieved content for improved integration of external knowledge in RAG. \oreo
    overcomes the lack of
    contextual integration among fragmented chunks in vanilla RAG
    by extracting
    subtle relations from scattered facts, 
and transforming redundant context into a concise context.
\item \oreo is a plug-and-play module, inherently modular, generalizable, flexible and robust, powered by a three-stage training scheme comprising supervised fine-tuning, contrastive multi-task learning and reinforcement learning.
This enables seamless integration with arbitrary retrievers, generators, and off-the-shelf RAG systems. 
\item We demonstrate \oreo's 
improved performance and
reduced token length for
both single-hop QA tasks (PopQA \cite{mallen-etal-2023-trust}, NaturalQuestion (NQ) \cite{kwiatkowski2019natural}, TriviaQA (TQA) \cite{JoshiTriviaQA2017}, and multi-hop QA tasks (HopotQA \cite{yang-etal-2018-hotpotqa}, 2WikiMultiHopQA \cite{ho-etal-2020-constructing}). Overall, \oreo improves an average of 6.87\% performance while reducing the input token length for generator by 12.87x.
\end{enumerate}

%Revisiting the motivational example in Figure \ref{fig:intro}, the lack of  systems often results in disjointed responses or hallucinations. In contrast, \oreo overcomes these limitations by extracting query-specific evidence, capturing , finally  guiding the generator to produce accurate answers. By design, \oreo offers several key advantages. First, \textit{producing condensed and refined context.} \oreo significantly shortens the length of retrieved context by extracting, abstracting, and reorganizing supporting evidence into concise and query-aware context. This enables the generator to focus on the most critical information, improving response quality. Second, \textit{plug-and-play flexibility}. The modular nature of \oreo makes it highly adaptable, allowing seamless integration with arbitrary retrievers, generators, and off-the-shelf RAG systems. This flexibility enables broad applicability across diverse RAG configurations without significant adjustments. Third, \textit{lightweight and efficient}. \oreo is lightweight and trainable, enhancing system performance while maintaining computational feasibility.


%\textbf{Contributions. } (i) (ii)  (iii) We demonstrate improved performance and reduced token length for factual short-form QA tasks. 

\section{Methodology}
\label{method}
We begin with a quick primer on general retrieval-augmented generation (RAG) and formulate our problem (\S\ref{problem}), followed by an overview of the proposed method (\S\ref{met_overview}) and details of each step (\S\ref{data_coll}, \S\ref{sft}, \S\ref{cl} and \S\ref{rl}). 

\subsection{Problem Formulation}
\label{problem}
A typical RAG system comprises of two primary components that work in tandem: the retriever $\mathcal{R}$ identifies and retrieves top-\textit{k} document chunks $\mathcal{D}=\{d_{1}, d_{2},...,d_{k}\}$ from an external knowledge base based on their relevance to a given query \textit{q}; the generator $\mathcal{G}$ then produces the final answer for \textit{q} by conditioning on the combination of $\mathcal{D}$ and query \textit{q}, formally expressed as $y=\mathcal{G}(\mathcal{D}, q)$. However, the performance of such a general pipeline is compromised by reasons we indicated in \S\ref{intro}. Therefore, we propose \oreo to reconstruct context by extracting the most supportive evidence from $\mathcal{D}$, and producing a concise, query-aware context $\mathcal{C}$ that aligns with the knowledge acquisition mechanics and preference of $\mathcal{G}$. An ideal $\mathcal{C}$ should be produced after \oreo thoroughly understands $\mathcal{D}$, identifies essential entities and facts, establishes their relations, and retains only the necessary information for $\mathcal{G}$ to effectively answer \textit{q}. This process goes beyond plain information extraction, as it involves organizing and synthesizing content into a coherent and query-specific context. Therefore, we formulate the context reconstruction task as a text generation problem.

\subsection{Method Overview}
\label{met_overview}
Our method extends the standard RAG paradigm from ``retrieve-then-generate'' to ``retrieve-reconstruct-then-generate''. Specifically, we train a text generation model $\mathcal{M}_{\theta}$, parameterized by $\theta$ to map the retrieved documents $\mathcal{D}$ into a refined context \textit{c} that enables the downstream generator $\mathcal{G}$ to produce more accurate answers for an input query: $c=f_{\mathcal{M}_{\theta}}(\mathcal{D}, q)$. The training of $\mathcal{M}_{\theta}$ involves three stages: 1. $\mathcal{M}_{\theta}$ is trained to learn the transformation from original documents to refined context using annotated datasets (\S\ref{sft}). 2. Self-generated samples are incorporated to enhance the model's ability to recognize and correct its own errors, thereby improving robustness and generalization (\S\ref{cl}). 3. The reconstructed context is aligned with the generator’s knowledge acquisition process by incorporating feedback from $\mathcal{G} 
  $(\S\ref{rl}). However, obtaining an annotated dataset with refined context for SFT is challenging. Drawing inspiration from prior work \cite{balachandran-etal-2022-correcting}, we replace human annotation with advanced LLMs to generate high-quality synthetic oracle training data (\S\ref{data_coll}). An overview of the framework is depicted in Figure \ref{fig:overview}.
\begin{figure}[htbp!]
\begin{center}
  \includegraphics[scale=0.52]{images/overview.pdf}
  \end{center}
  \caption{\textmd{The framework of \oreo. (a) outlines the process of data collection and curation (\textbf{top}). (b) demonstrates the three-stage training, which comprises the supervised fine-tuning (SFT), contrastive multi-task learning (CML) and reinforcement learning (RL) alignment (\textbf{middle}). (c) illustrates the application of \oreo, comparing against the vanilla RAG (\textbf{bottom}).}}
  \Description{framework}
  \label{fig:overview}
    \vspace{-0.5em}
\end{figure}
 

\subsection{Data Collection and Curation}
\label{data_coll}
\textbf{Data collection}. To train \oreo during the SFT stage, an annotated dataset containing context with the most supportive evidence from retrieved document chunks is crucial. Such context should be query-specific, answer-aware, grounded in retrieved chunks, and structured as a rationale chain capable of deriving the correct answer. However, such datasets are not readily available, and manually annotating evidence for each query is time-consuming and labor-intensive. Fortunately, contemporary LLMs have exhibited impressive instruction learning capabilities to extract useful information \cite{dagdelen2024structured} and generate high-quality reasoning steps \cite{wei2022chain} even in few-shot settings \cite{brown2020language}. In this work, we elicit such capability from more advanced LLMs to our relatively smaller model \oreo, through generating a high-quality reasoning dataset using LLMs and utilizing it as "gold context" to train \oreo. Specifically, given a query and corresponding retrieved document chunks, we first prompt Llama-3-8B-Instruct \cite{touvron2023llama} to extract key entities and events from $\mathcal{D}$, and generate detailed rationales to answer the query. Since we prioritize the information extraction capability of \oreo during the SFT stage, to ensure reliability and minimize hallucinations, we construct the gold training dataset  solely from query-document pairs where the ground-truth answer is explicitly present within the retrieved chunks.

\textbf{Bootstrapping}. For queries where the generated reasoning fails to include the ground truth (despite it being present in the retrieved chunks), we bootstrap Llama3 by providing the correct answer and iteratively reprompting it to perform generation. Such an iterative process allows Llama3 to reason backwards and learn to generate rationale chains that support the correct answer. This bootstrapping is inspired by \cite{zelikman2022star, wei2024instructrag}. The prompts and demonstrations used for gold context generation and boostrapping are deatiled in Appendix.

\textbf{Data curation}. Accurate extraction of supporting evidence and reasoning from query to answer is essential for training $\mathcal{M_{\theta}}$. To eliminate hallucination and ensure the quality of learning, we conduct data curation by applying the following rules. \textit{1. Ground Truth Alignment.} We retain query-context pairs where the generated context from Llama-3 contains ground truth answers. \textit{2. Entity and Event Consistency. } We extract sets of entities and events from both the original documents and the Llama3-generated context. Instances are kept only if the entities and events extracted from the generated context ($\mathcal{E}_{gen}$) are a subset of those present in the original documents ($\mathcal{E}_{ori}$). By following these steps, the refined context generated by Llama-3 is treated as "gold context" for training $\mathcal{M_{\theta}}$.


\subsection{Supervised Fine-tuning}
\label{sft}
With the curated dataset constructed in \S\ref{data_coll}, we employ supervised fine-tuning (SFT) to elicit the ability of extracting and reasoning from LLM to \oreo. Specifically, given a curated training dataset \ $\mathcal{T}=\{{x}_{i}, c_{i}\}^{N}_{i=1}$, where $x_{i}$ is the combination of query $q_{i}$, the associated retrieved document chunks $\mathcal{D}_{i}$ and task instructions. The goal of SFT is to train a sequential model $\mathcal{M}_{\theta}$ to generate target context conditioned on the $x$, and preceding tokens $c_{<t}$ of the context. The model minimizes the negative log-likelihood over the gold context, as defined by the following loss function:
\begin{equation}
\begin{split}
\label{eq:sftloss}
\mathcal{L}_{SFT}= &\mathbb{E}_{(x, c)\sim \mathcal{T}}[-log \ p_{\mathcal{M}_{\theta}}(c|x)] \\
 = & - \sum_{t=1}^{L}log\ p_{\mathcal{M}_{\theta}}(c_{t}|x, c_{< t})
\end{split}
\end{equation}
where \textit{p} represents probability distribution of generation by the model $\mathcal{M}_{\theta}$. 

\subsection{Contrastive Multitask Learning}
\label{cl}
The SFT in \S\ref{sft} serves as the initial step in equipping \oreo with the capability to reconstruct context. By emulating the behavior of an LLM, SFT enables \oreo to extract critical entities, events, and facts from $\mathcal{D}$, capture subtle relationships and organize them into coherent reasoning paths. This process ensures that the reconstructed context effectively supports the generation of accurate and complete answers for queries. However, autoregressive models trained solely on ground truth data often demonstrate suboptimal generalization performance. To address this issue, our goal is broader: we seek to empower \oreo to identify its own errors and integrate sequence-level supervised signals, which are critical for enhancing conditional text generation into training, thus improving its generalization. To achieve this goal, we introduce contrastive learning as a complementary step following SFT.

\textbf{Construct contrastive samples. }Inspired by \cite{an2022cont}, in addition to using in-batch instances, we gather contrastive samples from \oreo's own predictions. Specifically, we obtain the model’s top-\textit{n} recent predictions via beam search, rank and label them as positive and negative pairs ($c^{+}, c^{-}$) in descending order of sequence-level similarity with the gold context $\mathcal{c}$, using the ROUGE metric to measure the similarity.

\textbf{Margin-based pairwise loss. } To guide the learning process, we employ a pairwise margin-based loss that encourages \oreo to bring positive candidates closer semantically to the retrieved document chunks $\mathcal{D}$ while distancing negative ones. This ensures that the positive candidates generated by \oreo capture the essential and grounded information from $\mathcal{D}$ with the guidance of gold context, while discarding irrelevant information. The pairwise loss function is combined with the negative log-likelihood loss from SFT, forming a multi-task learning process. The final loss function is expressed as:
\begin{equation}
\begin{split}
\label{clloss}
    \mathcal{L}_{CL}=& \sum \ max \{0, cos(E_\mathcal{D},E_c^-)  -cos(E_\mathcal{D},E_c^+) \\
    & + \eta *(rank_{c^-}-rank_{c^+})\}  \\
    & + \alpha \mathcal{L}_{SFT}
\end{split}
\end{equation}
where $E$ denotes the vector representations, $\eta$ is the hyperparameter and $rank_{c^+/c^-}$ denotes the ranking position of the candidates respectively, meaning that the contrastive pair with a larger ranking gap should have a larger margin \cite{an2022cont, zhong-etal-2020-extractive}. 

\subsection{Reinforcement Learning Alignment}
\label{rl}
The supervised fine-tuning and contrastive multitask learning stages equip \oreo with the ability to capture critical evidence and retain supportive information from retrieved content. However, knowledge inconsistencies among the retriever $\mathcal{R}$, \oreo and the generator $\mathcal{G}$ persist due to their independent optimization processes. Additionally, training \oreo with keeping $\mathcal{G}$ as a black-box precludes gradient back-propagation from $\mathcal{G}$ to update \oreo. To address these challenges, we incorporate reinforcement learning (RL) into \oreo's training pipeline following the above training stages. This step enables \oreo learn from labeled ground truth of downstream tasks by aligning their output with the needs of $\mathcal{G}$ to produce correct answers. Specifically, we model $\mathcal{G}$ as a reward model and leverage the discrepancy between $\mathcal{G}$'s generated output and ground truth as reward signals. Proximal Policy Optimization (PPO) \cite{ouyang2022training, Stiennon2020LearningTS} is employed to optimize \oreo in this alignment stage. 

\textbf{Policy formulation and optimization}. In this step, $\mathcal{M}_{\theta}$ serves as the policy $\pi_{\theta}$. It takes the reconstructed context $\hat{c}$ from prior training steps and returns a new $\hat{c}^{'}$, optimized by feedback from $\mathcal{G}$. The action space consists of all tokens in the corpus. At each step, the parameterized policy $\pi_{\theta_{t}}$ selects an action $a_{t}$ in a given state $s_{t}$ to maximize the discounted long-term reward $\mathbb{E}_{\pi_{\theta_{t}}}[\sum_{t-0}^{T}\gamma^{t}\mathcal{R}(s_{t}, a_{t})]$. Specifically, the action $a_{t}$ is predicting the next token, and state $s_{t}$ is the sequence of all preceding tokens. The objective function is:
\begin{equation}
\begin{split}
\label{ppoclip}
\mathcal{L}_{RL} = &\mathbb{E}[min(r_{t}(\theta)\cdot A_{t}, clip(r_{t}(\theta), 1-\epsilon, 1+\epsilon)\cdot A_{t})] \\
& - \beta(V(s_{t})-R_{t})^2
\end{split}
\end{equation}
where $r_{\theta}=\frac{\pi_{\theta}(a_{t}|s_{t})}{\pi_{\theta_{old}}(a_{t}|s_{t})}$ is the ratio of the updated policy $\pi_{\theta}$ to previous policy $\pi_{\theta_{old}}$. PPO ensures stable and efficient updates by clipping policy ratios, preventing excessively large changes that could destabilize training. The parameter $\epsilon$ defines how much the new policy can deviate from the old policy. $A_{t}$ is the advantage function, measures
whether or not the action is better or worse than the policy’s old behavior, estimated using Generalized Advantage Estimation (GAE) \cite{schulman2015high}: $A_{t}=\sum_{l=0}^{L}(\gamma\lambda)^{l}(R_{t+l}+\gamma V(s_{t+l-1})-V(s_{t+l}))$ where $\gamma$ and $\lambda$ are discount factors. $V(s_{t})$ is a critic network estimating the value of state $s_{t}$. $R_{t}$ is the estimated reward at time $t$. $\beta(V(s_{t})-R_{t})^2$ weighted by $\beta$ minimizes the discrepency between estimated and true values. 

\textbf{Reward estimation}. The downstream generator $\mathcal{G}$ serves as a reward model, the generation of \oreo by policy $\pi_{\theta_{t}}$ is passed to $\mathcal{G}$ with query $q$ to generate the answer $y$. When the end of sentence (\emph{e.g} <EOS>) token is generated, the corresponding reward $R_{t}$ is obtained by comparing the generated answer $y$ with ground truth answer $y_{gold}$, which is measured by the ROUGE score $R_{t}=ROUGE(y, y_{gold})$. However, $\mathcal{G}$ generates answers only after completing all tokens, but (\ref{ppoclip}) updates every action step.  To address this, we incorporate a token-level weighting mechanism \cite{yang-etal-2023-prca}. Considering that a token $t$ with higher generation probability deemed more critical by the current policy. Consequently, the token's contribution to the final reward is proportionally adjusted. We estimate the reward at each step $t$ using the formulation: 
\begin{equation}
\label{rewd}
    R_{t}=ROUGE(y, y_{gold})*log\_softmax(e^{\pi_{\theta}(a_{t}|s_{t})})
\end{equation}
Since the rewards estimated by (\ref{rewd}) are sequence-level and sparse, following \cite{wu2021recursively} we regularize the reward function using a token-level KL penalty to prevent the model from deviating too far from the initialized LM. The final regularized reward estimation is:
\begin{equation}
\label{regrewd}
    \hat{R_{t}}=R_{t}-\delta KL(\pi_{\theta_{t}}(a_{t}|s_{t})||\pi_{0}(a_{t}|s_{t}))
\end{equation}

\begin{table*}[!htb]
\caption{\textmd{Dataset statistics, retrievers and evaluation metrics. }}
\label{table:stat}
  \vspace{-.1em}
\begin{tabular}{ c|cccccc } 
 \hline
 \textbf{Dataset} & \textbf{\# Train (k)} & \textbf{\# Test (k)} & \textbf{Retriever} & \textbf{Precision@\textit{5}} & \textbf{Task} & \textbf{Metric} \\ 
  \hline 
PopQA & 6.5 & 1.4 & Contriver & 0.287 & Extractive single-hop QA & EM \\ 
 NaturalQuestions & 28.3 & 3.6 & DPR & 0.33 & Extractive single-hop QA & EM\\ 
 TriviaQA & 30.1 & 11.3 & Contriver & 0.43 & Extractive single-hop QA & EM\\ 
HotpotQA & 20.7 & 5.6 & Contriver & 0.137 & Abstractive multi-hop QA & F1 \\
2WikiMultiHopQA & 20.7 & 12.6 & BM25 & 0.07 & Abstractive multi-hop QA & F1\\
 \hline
\end{tabular}
\end{table*}

\section{Experiments}
We evaluate \oreo across five open-domain question-answering (ODQA) tasks, comparing its performance against several baselines. Our experiments holistically assess the quality of reconstructed context by \oreo from three key aspects: \textbf{efficiency, effectiveness,} and \textbf{robustness}, using a range of metrics designed to measure the performance of downstream tasks. Our primary emphasis is on short-term factual QA tasks, where the answers to queries are typically concise and consist of only a few tokens. These tasks require LMs to identify and extract precise evidence from retrieved documents while avoiding irrelevant or distracting information. Factual QA tasks are particularly sensitive to context quality, making them an ideal benchmark for evaluating the performance of \oreo. In this section, we provide details of tasks and datasets (\S\ref{sec:dst}), baselines (\S\ref{sec:basline}) and experiment setup (\S\ref{sec:expset}). 

\subsection{Datasets and Tasks}
\label{sec:dst}
\textbf{Single-hop question answering. } We employ PopQA (PQA) \cite{mallen-etal-2023-trust}, NaturalQuestions (NQ) \cite{kwiatkowski2019natural}, and TriviaQA (TQA) \cite{JoshiTriviaQA2017} to experiment with the single-hop open-domain QA task. Each sample in these datasets has a question and annotated short extractive answers. For PopQA, we utilize the long-tail subset, which comprises 1,399 queries involving rare entities with fewer than 100 monthly Wikipedia page views \cite{asai2023self}. 

\textbf{Multi-hop question answering. }We also test \oreo on more complex QA scenarios, specifically using multi-hop datasets HopotQA (HQA) \cite{yang-etal-2018-hotpotqa} and 2WikiMultiHopQA (2WQA) \cite{ho-etal-2020-constructing}. Each question in these datasets requires reasoning over multiple articles. 

\textbf{External knowledge source. }For all experiments, we use the Wikipedia dump \cite{karpukhin-etal-2020-dense} as the external knowledge source.  

\textbf{Evaluation metrics. }Following previous study \cite{wang2023learning}, we assess extractive QA performance (PQA, NQ, and TQA) using the Exact Match (EM) metric, while abstractive QA performance (HQA and 2WQA) is measured using unigram F1.

Table \ref{table:stat} provides detailed statistics for each dataset, including the number of samples in the training set after curation, test set size, the specific retriever used, and evaluation metrics. Besides, we use the precision\textit{@k} as an approximation of retrievers' performance. Precision\textit{@k} is defined as the ratio of chunks that contain the among all retrieved chunks \textit{k} for each query.  
\vspace{-0.7em}
\subsection{Baselines}
\label{sec:basline}
For comparison, we focus on evaluating how effectively \oreo enhances vanilla RAG systems treating both the retriever and generator as black-box components, acknowledging that they may be imperfect and not allowed to be fine-tuned. We compare the performance of downstream tasks using five configurations: \textbf{(1) Query only}. The answer generation is performed by using only the query without incorporating any retrieved context. This mostly relies on the internal knowledge of LMs. \textbf{(2) Original full content}. The context for answer generation is the sequential concatenation of all retrieved document chunks. This setup uses raw, unprocessed retrieval results. 
\textbf{(3) Passage-level filtering. } In this setting, only the most relevant chunks are selected as context. Specifically, the chunk that is best-ranked and contains the ground-truth answer is chosen for each query. If no ground-truth answer is found, the top-ranked chunk, as determined by the retriever, is used instead. For single-hop tasks, only one passage is selected, while for multi-hop tasks, two passages are used.
\textbf{(4) Extraction and compression}. We employ three information extraction and compression methods to extract informative sentences and derive summaries from retrieved documents. Specifically, we use conditional cross-mutual information (CXMI) \cite{fernandes-etal-2021-measuring} to train an LM to learn to filter out redundant context by following \cite{wang2023learning}. CXMI quantifies how much a sentence contribute to the correct answer. For compression, we employ Selective-Context \cite{li2023unlocking} and LLMLingua \cite{jiang-etal-2023-llmlingua} to compress the context. Selective-Context filters out unimportance context based on self-information while LLMLingua is based on perplexity. 
\textbf{(5) reconstructed context by using \oreo}. 

\subsection{Experiment setup}
\label{sec:expset}
\textbf{Retriever}.
To retrieve top-\textit{k} document chunks for each query ($k=5$ unless otherwise specified), we employ a range of off-the-shelf retrievers, including  Contriver \cite{lei-etal-2023-unsupervised}, DPR \cite{karpukhin-etal-2020-dense} and BM25 \cite{robertson1994some}. The choice of multiple retrievers ensures that the robustness of \oreo is tested against various retrieval mechanisms, each with different strengths and weaknesses. Additionally, we extend our experiments to include retrieval of the top-\textit{10} document chunks for 2WQA to examine whether \oreo's performance is sensitive to context length. 

\textbf{Downstream generator}. 
We access how do the contexts generated by different methods described in \S\ref{sec:basline} affect the downstream generator by evaluating the performance of QA tasks. Specifically, we use FLAN-T5 \cite{chung2024scaling} and OPT-IML \cite{iyer2022opt} as the downstream generator. (Note that, \oreo operates as an independent module, making it compatible with various retrievers, generators, and other existing RAG frameworks. )


\textbf{Model and training}. We employ T5-small \cite{raffel2020exploring} as the backbone model for \oreo, though it is applicable to any encoder-decoder and auto-regressive models such as LLaMA. \oreo is implemented based on Transformer library \cite{wolf-etal-2020-transformers}, with RL implementation built upon the open-sourced package RL4LM \cite{ramamurthy2022reinforcement}. For CML, we allow a maximum of 12 contrastive samples generated by \oreo and set beam size as 8. Unless otherwise specified, \oreo is trained for 5 epochs during SFT and 3 epochs for CML stages, with batch size 4/8/16 based on the dataset size, and using a learning rate of $5e-5$. Detailed parameter settings are listed in Table \ref{table:params}. 

\textbf{Inference}. During inference, we limit \oreo to generate a maximum of 300 tokens for all datasets.

\begin{table}[H]
\vspace{-1em}
\centering
\caption{\textmd{Parameter settings for experiments. Parameters without being specified are set to their default values as defined by the development package. }}
\begin{tabular}{c|c} 
 \hline
 Parameter & Value \\  
 \hline
    $\eta$ (CML) & 0.01\\
    $\alpha$ (CML) & 0.5\\
    $\epsilon$ (RL) & 0.2 \\
    $\gamma$, $\lambda$(RL) & 0.95 \\
    Top-\textit{k} (RL) & 4 \\
    Top-\textit{p} (RL)& 0.95 \\
 \hline
\end{tabular}
\label{table:params}
\end{table}
\section{Results and Analysis}
We seek to address the following questions through our experiments:
\begin{enumerate}
\item How does \oreo perform in RAG pipeline for QA tasks compared to alternative context configurations as listed in \S\ref{sec:basline}? (\S \ref{sec:overall})
\item To what extent does \oreo reduce input token length while improving QA performance? (\S\ref{sec:effect})
\item How sensitive is \oreo to noisy context and chunk order perturbations? (\S\ref{sec:robust})
\item How does \oreo perform on unseen datasets? (\S\ref{sec:zeroshot})
\end{enumerate}

\subsection{Effectiveness Evaluation}
\begin{table*}[!ht]
\centering
\footnotesize
    \caption{
    \textmd{Average performance of \oreo on five QA tasks by using Flan-T5 and OPT-IML as downstream generators, compared with baselines. }
    \label{tab:average}} 
\vspace{-1.1em}
\begin{tabular}{lccccccc}
\toprule
 & No Retrieval & Full Content & Passage-level & CXMI & Selective-Context & LLMLingua & \oreo (Ours) \\ 
Task &  & & & & & & \\
\midrule
\multicolumn{8}{c}{\textit{Flan-T5 as the downstream generator}} \\
Single-hop QA & 0.1088 & 0.3662 & 0.3394 & 0.4016 & 0.2713 & 0.3491 & 0.4451 \\
Multi-hop QA & 0.4485 & 0.5671 & 0.5398 & 0.603 & 0.5297 & 0.5745 & 0.658\\
\midrule
\multicolumn{8}{c}{\textit{OPT-IML as the downstream generator}} \\
Single-hop QA & 0.125 & 0.2300 & 0.2698 & 0.2714 & 0.1696 & 0.1726 & 0.3616\\
Multi-hop QA &0.4416 & 0.334 & 0.5866 & 0.4626 & 0.346 & 0.4363 & 0.6542 \\
\bottomrule
\end{tabular}
\vspace{-1.1em}
\end{table*}

\label{sec:overall}
\textbf{Overall Performance}. Table \ref{tab:average} reports the average performance across single-hop and multi-hop QA tasks using Flan-T5 and OPT-IML as downstream generators, with contexts obtained through various methods. The results demonstrate that \oreo consistently outperforms all baseline approaches, achieving the highest performance on both single-hop (EM) and multi-hop (F1) tasks. Notably, Flan-T5 generally delivers superior performance compared to OPT-IML.

\textbf{Comparison against context configurations.} Figure \ref{fig:metrics} presents the performance of different setups across five datasets (with Flan-T5 as downstream generator): using query-only inputs (without retrieval), original full content, passage-level filtering, \oreo without RL, and \oreo with RL. The results demonstrate that \oreo consistently outperforms the use of full content and passage-level filtering across all datasets. For single-hop QA tasks, \oreo achieves notable improvements in EM scores, with gains of 8.8\%, 23.1\%, and 37.5\% on the PopQA, NQ, and TriviaQA datasets compared with using original full context respectively. The relatively smaller improvement on PopQA can be attributed to the nature of its queries, which involve rare entities. In the case of more complex multi-hop QA tasks, \oreo achieves F1 score improvements of 12.7\% on HotpotQA and 19.8\% on 2WQA. These improvements are comparatively less pronounced than those seen in single-hop tasks. This discrepancy likely stems from the increased task complexity inherent in multi-hop QA. The additional challenge of ensuring coherence in abstractive multi-hop reasoning from fragmented chunks underscores the potential for further optimization in \oreo's handling of such tasks. The experiments conducted on the 2WQA dataset using top-\textit{5} and top-\textit{10} retrieved document chunks demonstrate \oreo's flexibility in handling varying input lengths. The improved performance with the top-\textit{10} chunks arises from the increased likelihood of covering more passages that contain the ground-truth answer.

Overall, these results reveals \oreo's capability to capture essential information and filter out distracting content from retrieved document chunks, leading to improved performance in downstream factual QA tasks. The modest improvements achieved with reinforcement learning further emphasize its value in addressing knowledge inconsistencies between the retriever and generator. 

\begin{figure}[!htb]
\vspace{-1em}
\begin{center}
  \includegraphics[width=0.495\textwidth]{images/metrics.pdf}
  \end{center}
  \caption{\textmd{Performance on five datasets by using {\color{RoyalBlue}{query without retrieval}}, {\color{Bittersweet}{original full concatenation of chunks}}, {\color{ForestGreen}{passage-level filtering}}, {\color{Red}{context generated by \oreo without RL}}, and {\color{Purple}{context generated by \oreo with RL}}. 2WQA_\textit{k} represents retrieving top-\textit{k} documents for the 2WikiMultiHopQA dataset. The downstream generator is Flan-T5.} }
  \Description{perform}
  \label{fig:metrics}
  \vspace{-1.1em}
\end{figure}

\textbf{Comparison against baselines. }
% singlehop and multihop - which type of task was improved more, has a less pronounced impact on
We compare the context quality generated by \oreo with that of selected extraction and compression methods. Table \ref{tab:main} summarizes the performance and token counts across five datasets by employing Flan-T5 as the downstream generator. From the table, it is evident that \oreo consistently achieves the best performance across all five datasets, with improvements ranging from 2.15\% to 8.75\% over the second-best methods. Notably, \oreo demonstrates more significant improvements on extractive single-hop tasks compared to multi-hop ones. This could due to the strict limitation of the output token count to 300. While this constraint is effective for single-hop tasks, where answers typically explicitly contained in the retrieved chunks, but it may lead to information loss for multi-hop QA tasks, which often require synthesizing evidence scattered across multiple chunks, demanding more lenient token caps to avoid truncation of reasoning chains. In addition to achieving superior performance, \oreo significantly reduces the context length provided to the downstream generator while maintaining or even enhancing task accuracy. This demonstrates that \oreo is effective at extracting and synthesizing the most relevant content from the retrieved context, thereby supporting more accurate generation with fewer tokens. Figure \ref{fig:methods} illustrates the performance by using OPT-IML as the generator. Consistent with results of using Flan-T5, \oreo brings +0.0211 EM (PopQA), +0.0473 EM (NQ), + 0.0349 EM (TriviaQA), +0.0856 F1 (HotpotQA) and + 0.0495 F1 (2WQA) improvements compared with the second best baseline method. 

Regarding extraction and compression baselines, the variation in performance reflects the differing strengths and limitations of these methods in addressing specific tasks. As reported in Table \ref{tab:main}, the CXMI-guided model demonstrates performance closest to \oreo across most datasets among all baselines. In contrast, Selective-Context exhibits the lowest performance. This underperformance likely arises from its reliance on the self-information of lexical units (\emph{e.g} tokens, phrases, or sentences), which overlooks the interdependence between compressed content and fails to create a cohesive context for downstream tasks. LLMLingua, which is based on perplexity, also faces challenges due to its lack of contextual modeling, as it does not explicitly consider contextual information during content selection. Furthermore, both LLMLingua and Selective-Context are not specifically designed for RAG scenarios. As a result, these methods lack query awareness, which limits their ability to tailor extracted context to the unique demands of QA tasks with RAG. 

\begin{table*}
  \vspace{-1.1em}
    \caption{\textmd{Summary of QA task performance and token counts using context derived from different methods. Flan-T5 is the generator. \textbf{Bold} numbers indicate the best performance among all methods, \textit{italics} text denotes the second-best performance. The values in parentheses indicate the percentage improvement of the best-performing method over the second-best method. All datasets are tested with top-\textit{5} retrieved chunks. }}
  \centering
   \resizebox{\linewidth}{!}
   {
    \begin{tabular}{lcccccccccc}
    \toprule
    \multirow{2}[3]{*}{\textbf{Methods}} & \multicolumn{2}{c}{\textbf{PopQA}} & \multicolumn{2}{c}{\textbf{NaturalQuestions}} & \multicolumn{2}{c}{\textbf{TriviaQA}} & \multicolumn{2}{c}{\textbf{HotPotQA}} & \multicolumn{2}{c}{\textbf{2WikiMultihopQA}} \\
\cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}\cmidrule(lr){8-9}\cmidrule(lr){10-11}         & \textbf{EM} & \textbf{\# tokens} & \textbf{EM} & \textbf{\# tokens} & \textbf{EM} & \textbf{\# tokens} & \textbf{F1} & \textbf{\# tokens} & \textbf{F1} & \textbf{\# tokens} \\
    \midrule
    No Retrieval      & 0.1320                     & 30           & 0.0558                    & 39           & 0.1387                    & 31           & 0.4599                     & 47           & 0.4371                     & 35           \\
Full content      & \textit{0.4305}            & 1689         & 0.3584                    & 1636         & 0.3097                    & 1676         & 0.6014                     & 1707         & 0.5328                     & 1786         \\
CXMI              & 0.4202                     & 340          & 0.3917                    & 329          & \textit{0.3929}           & 354          & \textit{0.6409}            & 351          & 0.565                      & 305          \\
Selective Context & 0.1445                     & 199          & 0.3981                    & 193          & 0.2712                    & 203          & 0.5588                     & 214          & \textit{0.6106}            & 158          \\
LLMLingua         & 0.2702                     & 497          & \textit{0.4125}           & 491          & 0.3647                    & 520          & 0.5584                     & 527          & 0.5905                     & 394          \\
Passage           & 0.4150                     & 131          & 0.2506                    & 183          & 0.3526                    & 203          & 0.5280                     & 190          & 0.5515                     & 205 \\
\textbf{\oreo (Ours) }             & \textbf{0.4682 (+ 8.75\%)} & \textbf{108} & \textbf{0.4413 (+6.98\%)} & \textbf{134} & \textbf{0.4257 (+8.35\%)} & \textbf{130} & \textbf{0.6775 (+ 5.71\%)} & \textbf{272} & \textbf{0.6384 (+ 2.15\%)} & \textbf{103} \\
    \bottomrule
    \end{tabular}
    }
  \label{tab:main}
\end{table*}

\begin{figure*}[!htb]
\begin{center}
  \includegraphics[width=0.9\textwidth]{images/methods.pdf}
  \end{center}
  \caption{\textmd{Performance comparison against baselines by using OPT-IML as the generator. The meaning and method of each bar: Passage - passage-level filtering, CXMI - conditional cross-mutual information guided filter, Selective - Selective Context, Fulll - original full content, No - query without retrieval, Llmlingua - LLMLingua. }}
  \Description{perform}
  \label{fig:methods}
\end{figure*}
\begin{figure}[H]
\vspace{-1.em}
\begin{center}
  \includegraphics[scale=0.76]{images/token_vs_perf.pdf}
  \end{center}
 % \vspace{-1em}
  \caption{\textmd{Comparison of number of input tokens for generator and QA performance between different types of context. }}
  \Description{token_vs_perf}
  \label{fig:token_perf}
  \vspace{-1.em}
\end{figure}
\subsection{Efficiency Evaluation}
\label{sec:effect}
We evaluate the efficiency of \oreo from two aspects: the reduction in the input context length provided to the generator and its impact on QA task performance. Figure \ref{fig:token_perf} provides an overview of the number of tokens sent to the downstream generator when using query-only inputs, full content,and context reconstructed by \oreo. \oreo achieves a remarkable 84\% to 94\% reduction in token counts while delivering a performance improvement ranging from 8.76\% to 37.46\% compared to using the original full content. The gains are particularly notable in extractive QA tasks (\emph{e.g} NQ, TriviaQA) as shown from the steeper improvement in Figure \ref{fig:token_perf} from right endpoints to peaks. The high compression rate and improved performance demonstrates \oreo’s capability to effectively condense the retrieved context by preserving only the most critical evidence required for accurate answer generation. This also indicates the context reconstructed by \oreo is highly utilized by the generator. 

\subsection{Robustness Evaluation}
We evaluate \oreo's robustness from two aspects: its sensitivity to irrelevant or distracting information (noise robustness), and its ability to handle arbitrary rankings of retrieved chunks (order robustness).

\textbf{Noise robustness. }
We evaluate the robustness of \oreo in handling noise within the retrieved documents, focusing specifically on extractive QA tasks. In this evaluation, we retain a single chunk that explicitly contains the ground-truth answer and introduce four irrelevant documents to simulate a noisy retrieval scenario. This setup examines \oreo's effectiveness in filtering out distractions content and identifying query-specific information to generate accurate responses. Figure \ref{fig:noise} depicts the performance degradation as irrelevant chunks are added to the context. Compared to directly concatenating all retrieved chunks as context, context reconstructed by \oreo demonstrates a smaller decrease in EM scores and a slower rate of decline, as evidenced by a less steep slope.
\begin{figure}[hb]
\vspace{-2em}
\begin{center}
  \includegraphics[scale=0.75]{images/noise_exp.pdf}
  \end{center}
  \caption{\textmd{Performance declines as irrelevant chunks are introduced into the retrieved chunk set. }}
  \Description{npose}
  \label{fig:noise}
  \vspace{-1em}
\end{figure}


\textbf{Order robustness. } 
\label{sec:robust}
We evaluate the robustness of \oreo to variations in the order of retrieved documents by shuffling the top-\textit{5} retrieved results and comparing its performance against the original document order. The results for five datasets are presented in Table \ref{tab:shuffle}. From the table we can see that, \oreo consistently maintain the performance on five datasets (with $\pm 0.003$ to $\pm 0.027$). This highlights that \oreo is order- or position-agnostic. Even the retrieved chunks are suboptimally ranked or presented in an arbitrary order, \oreo can still effectively capture and synthesize essential information as long as the evidence exists in the chunks. This capability is largely attributed to \oreo's inherent reordering feature during context reconstruction, enabling it to function as an implicit reranker. Such robustness is particularly valuable for mitigating the "lost-in-the-middle" \cite{liu2023lost} phenomenon, where the order of relevant information may influence the downstream generator's performance. 


\subsection{Generalizability Evaluation}
\label{sec:zeroshot}
To evaluate the cross-dataset generalizability of \oreo, we assess its transferring capability by applying models trained on one dataset to tasks in a different dataset without any fine-tuning. This approach tests \oreo's ability to generalize its context reconstruction and synthesis capabilities to unseen query distributions. Specifically, we examine performance when using a model trained on PopQA to generate answers for NQ  and a model trained on 2QWA to process HQA queries. The results, presented in Table \ref{tab:zero-shot}, reveal that \oreo achieves competitive performance in zero-shot setting. For example, the model trained on PopQA achieves a score of 0.4352 when applied to NQ, only slightly lower than the performance of being specific trained (\emph{i.e} 0.4413 and 0.4682). Similarly, using the 2WQA-trained model on HQA yields a score of 0.6344, closely matching the intra-dataset score of 0.6384. These findings demonstrate \oreo's ability to generalize its context reconstruction to similar QA types effectively, even under query distribution shifts. Its strong performance across datasets highlights its robustness and adaptability, making it a promising solution for open-domain QA tasks that require flexibility in handling diverse knowledge sources and query structures.

\begin{table}[H]
\caption{\textmd{QA performance when shuffling the retrieved documents. }}
  \centering
    \begin{tabular}{lcc}
    \toprule
     \textbf{Dataset} & \textbf{w/o shuffle} & \textbf{w/ shuffle}\\
    \midrule
    PopQA  & 0.468 & 0.441 \\
    NaturalQuestions  & 0.441 & 0.425 \\
    HotpotQA  & 0.426 & 0.429 \\
    TriviaQA  & 0.678 & 0.668 \\
    2WikiMultihopQA  & 0.638 & 0.614 \\
    \bottomrule
    \end{tabular}
  \label{tab:shuffle}%
\end{table}%


\section{Related Work}
\subsection{Post-retrieval Enhancement for RAG}
Post-processing methods are widely employed to refine retrieved content for improved downstream generation. These methods can be categorized as follows:

\textbf{Reranking}. Rerankers reorder and prioritize retrieved documents to emphasize the most relevant results. They typically operate sequentially or iteratively after retrieval, leveraging various criteria such as semantic relevance between query and passages \cite{glass-etal-2022-re2g, hofstatter2023fid}, connections among documents \cite{dong2024don}, the majority of reader predictions \cite{mao-etal-2021-reader}, and utility for generation \cite{ma-etal-2023-large}. Rerankers are usually based on cross-encoder (\emph{e.g} BGE \cite{bgeembedding}, Mixedbread \cite{li2023angle}), multi-vector models (\emph{e.g} ColBert \cite{khattab2020colbert, santhanam-etal-2022-colbertv2}). Recent works explore using LMs as rerankers (\emph{e.g}, RankT5 \cite{zhuang2023rankt5}, RankZephyr \cite{pradeep2023rankzephyr}, RankGPT \cite{sun-etal-2023-chatgpt}).  

\textbf{Filtering}. Filtering is the process of selectively removing irrelevant, redundant, or low-quality information from retrieved documents to enhance their usefulness. Filtering can be performed at different granularity levels such as passage-level \cite{asai-etal-2022-evidentiality, yoran2023making, asai2023self}, sentence-level \cite{wang2023learning, hwang-etal-2024-dslr} or finer token-level \cite{jin-etal-2024-bider, anderson-etal-2022-lingua, jiang-etal-2024-longllmlingua}.  % self-RAG filters out irrelevant passages by self-reflection mechanism
\begin{table}[H]
\caption{\textmd{QA performance with zero-shot setting. PopQA $\to$ NQ represents the model trained on PopQA is applied to NQ.}}
  \centering
  % \resizebox{\linewidth}{!}{
    \begin{tabular}{ccc}
          \toprule
          \textbf{Dataset} & \textbf{Model $\to$ Dataset}  & \textbf{Performance} \\ \hline
     \multirow{3}[3]{*}{NQ} & NQ $\to$ NQ & 0.4413 \\
     & PopQA $\to$ PopQA & 0.4682 \\
     & PopQA $\to$ NQ & 0.4352 \\
    \midrule
    \multirow{3}[3]{*}{HotpotQA} & HotpotQA $\to$ HotpotQA & 0.6775 \\
     & 2WQA $\to$ 2WQA & 0.6384 \\
     & 2WQA $\to$ HotpotQA & 0.6344 \\
    \bottomrule
    \end{tabular}
  \label{tab:zero-shot}%
\end{table}%
 \textbf{Post verification and correction}. Some studies incorporate post evaluation \cite{yan2024corrective}, fact-checking \cite{liu2306reta} or attribution \cite{gao-etal-2023-rarr} mechanisms to further solidify the accuracy and reliability of the retrieved documents and responses. For example,  \cite{yan2024corrective} uses a retrieval evaluator accessing the relevance of the retrieved documents and estimating a correctness confidence degree, then self-correct the retrieved results. RETA-LLM \cite{liu2306reta} includes a fact checking module to verify whether there exist factual errors in the generated answers. by using a natural language understanding (NLU) model. CHAIN-OF-NOTE (CON) \cite{yu-etal-2024-chain} accesses relevance of each retrieved document to the query by generating sequential reading notes. These notes are integrated during final answer generation. 
 
 \textbf{Summarizing} or \textbf{compressing}. Summarization and compression of retrieved content aim to create concise, faithful contexts that enhance the efficiency and effectiveness of downstream generation \cite{xu2024recomp}. Several methods have been proposed to achieve this. \cite{xu2024recomp} proposed dual-encoder compressors for extractive and abstractive to eliminate irrelevant context and summarize upon the remaining content. Selective-Context \cite{li-etal-2023-compressing} employs a small LM  compute the self-information of each lexical uni and then drop the less informative content. LongLLMLingua \cite{jiang-etal-2024-longllmlingua} proposes coarse-grained compression via document-level perplexity and fine-grained compression of remaining text based on token perplexity. 
 
\subsection{Reinforcement Learning for Large Language Models}
Reinforcement learning for Language Models (RL4LM) has emerged as a transformative technique to further enhance LLMs' performance during the post-training process \cite{cao2024survey, pternea2024rl}. Traditional RL4LM usually involves a reward model, for example using PPO \cite{schulman2015high} to update the policy model (\emph{e.g} InstructGPT \cite{ouyang2022training}, GPT-4 \cite{achiam2023gpt}). Some RL4LM such as Direct Preference Optimization (DPO) \cite{rafailov2024direct}
and Reward-aware Preference Optimization (RPO) \cite{adler2024nemotron} get rid of reward model to provide more stable and computationally efficient solutions (\emph{e.g} Qwen 2 \cite{chu2024qwen2} and  Nemotron-4 \cite{adler2024nemotron}). Common goals of RL4LM include improving performance of downstream NLP tasks \cite{deng-etal-2022-rlprompt, ghalandari-etal-2022-efficient, ramamurthy2022reinforcement}, minimizing data and resource dependencies \cite{zhang2022tempera}, aligning model outputs with user intent, values and goals \cite{ouyang2022training}, and adhering to responsible AI principles \cite{bai2022training, bai2022constitutional}. Human feedback can be integrated into the framework by constructing preference datasets, which are then used to fine-tune both the policy and reward models (also termed as Reinforcement Learning from Human Feedback (RLHF)) \cite{bai2022training, ouyang2022training, hu2023aligning}. Some studies also explore RL4LM without human feedback \cite{rafailov2024direct} or replaced with AI feedback \cite{bai2022constitutional, yuan2024self} by distillation from LLMs \cite{cui2023ultrafeedback, park-etal-2024-offsetbias}, prompting LLMs as reward functions \cite{kwon2023reward, lee2023rlaif, zhang2024generative}, and self-rewarding \cite{yuan2024self}, or using performance-based metrics such as fluency or coherence \cite{ghalandari-etal-2022-efficient}, and task-specific constraints over the distribution of language \cite{ramamurthy2022reinforcement, wu2024beta}. In the specific domain of RAG, RRAML \cite{bacciu2023rraml} employs RL to train an efficient retriever to search for relevant information in in arbitrarily large databases. PRCA \cite{yang-etal-2023-prca} applies RL to further fine-tune the context extracted by a model pre-trained on domain-specific abstractive summarization tasks to optimize the reward for the generator. BIDER \cite{jin-etal-2024-bider} adopts RL to bridge the inconsistency between the retriever and generator. 

\section{Conclusion}
We have presented \oreo - a lightweight and pluggable module designed to enhance the performance of RAG systems by reconstructing retrieved document chunks and mitigating the potential knowledge inconsistencies between the retriever and generator. Upon receiving document chunks from the retriever, \oreo efficiently filters out irrelevant, redundant, and distracting content, transforming them into a concise and query-supportive context. These reconstructed contexts effectively guide the generator toward producing accurate answers for open-domain QA tasks. Notably, \oreo can be seamlessly integrated with arbitrary retrievers, generators, or other RAG components without requiring significant adjustments or modifications. Experimental results demonstrate \oreo's effectiveness in downstream tasks, its efficiency in compressing context while improving performance, and its robustness in handling noisy and imperfectly ranked document chunks. 

\textbf{Limitations and future works. } 
Despite \oreo's demonstrated effectiveness in short-form factual QA tasks, certain limitations remain. First, to achieve a high compression rate, \oreo enforces a maximum token count during context synthesis. While effective for most factual QA tasks, this constraint may inadvertently exclude vital information, particularly in complex tasks that require multi-hop reasoning or synthesizing extensive, interconnected evidence. Future work should investigate the trade-off between token counts and different tasks, and explore dynamic token allocation strategies that adapt to the query type (\emph{e.g} multi-hop QA, long-form QA) and consider latency requirements during inference. Second, while \oreo has shown resilience to perturbed document orders and noise-injected context, it has not been thoroughly tested in adversarial environments, such as those involving misinformation or conflicting evidence in retrieved documents. Future research should consider evaluating and improving \oreo's robustness in such challenging scenarios. Third, the current evaluation of \oreo relies heavily on the performance of downstream tasks, which serve as indirect assessments and may introduce bias. Developing advanced evaluation schemes (\emph{e.g} LLM-as-a-jedge \cite{gu2024survey}), will be essential for providing a more direct and reliable assessment of the quality of \oreo's reconstructions. Addressing these challenges could unlock the full potential of \oreo in more diverse and demanding real-world applications. 
%\section{Acknowledgments}
%\iffalse 
\section{Appendices}
\label{app:prompts}
\begin{table}[!htb]
\caption{Prompt template used for data collection.\label{tab:data_gen}}
\vspace{-1.2em}
\begin{prompt}[title={Prompt}, label=prompt:data_gen]
{\bf Input:}
Your task is to decompose the question, extract and abstract supporting information from the context to answer the question. Your output should mention all entities involved in the question, supporting sentences and rationals to all sub-questions from the context. If the conetxt doesn't provide information to answer the question, output '[UNKNOWN]'. Output the <Output> part only.\\
Example1:\\
    <Question>: Where was the director of film The Fascist born? \\
    <Context>: \{Retrieved document chunks\} \\
    <Output>: Luciano Salce, the director of the satirical film The Fascist, was born on September 25, 1922, in Rome, Italy. Salce was an Italian filmmaker, actor, and screenwriter known for his ability to blend comedy with social and political critique. \\
    Example2: \\
    <Question>: what is the number 1 sport in the usa? \\
    <Context>: \{Retrieved document chunks\} \\
    <Output>: American football is the most popular sport in the United States followed by basketball, baseball, and soccer. \\
    Example3:  \\
    <Question>: What was the first English monastery to be sacked by the Norsemen?\\
    <Context>: \{Retrieved document chunks\} \\
    <Output>: Vikings attacked the monastery at Lindisfarne on June 8, 793, which is the first recorded Viking raid on an English monastery.\\
    Example4:\\
    <Question>: Kate Philips played which wife of Henry VIII in 'Wolf Hall'? \\
    <Context>: \{Retrieved document chunks\} \\
    <Output>: Kate Phillips played Abigail Williams in "The Crucible" at the West Yorkshire Playhouse, and then went on to film her scenes for the BBC's adaptation of "Wolf Hall" in which she played Jane Seymour, Henry VIII's third wife. \\
    Example5:\\
    <Question>: Lokomotiv Yaroslavl was the team founded in 2011 after the plane crash near which airport? \\
    <Context>: \{Retrieved document chunks\} \\
    <Output>: Lokomotiv Yaroslavl Hockey Club Lokomotiv, also known as Lokomotiv Yaroslavl, is a Russian professional ice hockey team. On 7 September 2011, nearly the entire team perished in the Lokomotiv Yaroslavl plane crash. The aircraft ran off the runway before lifting off, struck a tower mast, caught fire and crashed from the end of the runway of Tunoshna Airport on the Volga River bank. \\

\{Question\} \\ 
\{Retrieved document chunks\} \\
{\bf Output:} \{Output\}
\end{prompt}
\vspace{0.2em}
\end{table} 
\begin{table}[H]
\caption{Bootstraping Prompt Template \label{tab:boost_gen}}
\vspace{-1.5em}
\begin{prompt}[title={Prompt}, label=prompt:boost_gen]
{\bf Input:}
You are given a question, a set of document chunks, a correct answer, extract evidences and supporting information from the chunks and generate rationales how these information derive the correct answer.
\\
Example1:\\
    <Question>: What nationality were social anthropologists Alfred Gell and Edmund Leach? \\
    <Chunks>: \{Retrieved document chunks\} \\
    <Correct answer>: British. \\
    <Output>: Both Alfred Gell and Edmund Leach were British. They were educated and primarily worked within the United Kingdom's academic framework. Their national and professional affiliations firmly establish their British nationality. \\
    Example2: \\
    <Question>: Crucible is a geodemography computer system created by a company that has stores in how many countries? \\
    <Chunks>: \{Retrieved document chunks\} \\
    <Correct answer>: 12. \\
    <Output>: Crucible is a geodemography computer system created by Tesco, a multinational grocery and general merchandise retailer. Tesco has stores in 12 countries as of recent data, so 12 is the answer. \\
    Example3: \\
    <Question>: What word is in both the genre of Muhammed Suiçmez's band and the genre of Dave Meniketti's band? \\
    <Chunks>: \{Retrieved document chunks\} \\
    <Correct answer>: Metal. \\
    <Output>: Necrophagist is known for its death metal style. Y\&T is often classified under the broader category of heavy metal. So the answer is mental. \\
    
\{Question\} \\ 
\{Retrieved document chunks\} \\
\{Correct answer\} \\
{\bf Output:} \{output\}
\end{prompt}
\vspace{-2em}
\end{table} 

%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.

\newpage
%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{paper}
\end{document}
\endinput
%%

