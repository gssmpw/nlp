\section{Introduction}
Generative imitation-based policies are an increasingly powerful way to learn low-level robot behaviors from multimodal\footnote{Here, multimodality only refers to the training data's action distribution.} expert demonstrations~\citep{chi2024diffusionpolicy, fu2024mobile,zhao2023learningfinegrainedbimanualmanipulation}. 
Despite their impressive ability to learn diverse behaviors directly from high-dimensional observations, these policies still degrade in nuanced and unexpected ways at runtime.  
For example, consider the robot in the left of Figure~\ref{fig:front-fig} that must pick up a mug from the table. 
At training time, the generative policy learns a distribution over useful interaction modes such as grasping the cup by different parts (e.g., handle, lip and interior, etc.) shown in wrist camera photo in Figure~\ref{fig:front-fig}.

However, at runtime, the policy exhibits a range of degradations, from complete task failures (such as the robot knocking down the cup during grasping, shown in 
the center of Figure~\ref{fig:front-fig}), to inappropriate behaviors that are misaligned with the deployment context or preferences of an end-user (such as the robot placing its gripper inside of a cup of water when serving a guest shown in the middle of Figure~\ref{fig:front-fig}). 
While a common mitigation strategy involves re-training the policy via more demonstrations~\citep{shafiullah2024supervised} or interventions~\citep{ross2010efficient, liumulti},  runtime failures are not always an indication that the base policy is fundamentally incapable of producing the desired behavior. 
In fact, the base policy may already contain the ``right'' behavior mode within its distribution (e.g., grasping the cup by the handle), but due to putting too much probability mass on an undesired mode, the robot does not reliably choose the correct action plan at runtime. 

Runtime policy steering \citep{nakamoto2024steering,wang2024inference} offers an elegant solution to this mode-selection problem. 
By using an external \textit{verifier}
to select candidate plans proposed by an imperfect generative policy, the robot's behavior can be ``steered'' at runtime without any need for re-training.
Despite the initial successes demonstrated by the policy steering paradigm, the question still remains of how to fully unlock autonomous policy steering in the open world when the robot's environment, task context, and base policy's performance are constantly changing.

Policy steering can be approached via the stochastic model-predictive control framework of modern control theory, which decomposes the optimal action selection (i.e. \textit{generation}) problem of runtime policy steering into \textit{(a)} \textit{predicting} the outcomes of a given action plan and \textit{(b)} \textit{verifying} how well they align with user intent. However, this approach is only feasible when a physics-based, low-dimensional dynamics model is available for outcome prediction and a well-defined reward function can be specified for verification. In open-world environments, both of these requirements are challenging to fulfill due to the complexity of dynamics modeling and the difficulty of hand-crafting rewards to evaluate nuanced task requirements \citep{hadfield2017inverse}.

Our core idea is to leverage world models, which are well-suited for predicting action outcomes in open world settings, and VLMs, which have great potential as verifiers due to their commonsense reasoning abilities, to develop a divide-and-conquer approach to open-world policy steering. However, doing so naively is challenging as world models and VLMs operate on fundamentally different representations of reality.


To address this concern, we propose  \textbf{FOREWARN}: \textbf{F}iltering \textbf{O}ptions via \textbf{RE}presenting \textbf{W}orld-model \textbf{A}ction \textbf{R}ollouts via \textbf{N}arration.
To predict challenging action outcomes (e.g., interaction dynamics of a manipulator and a deformable bag), we use state-of-the-art world models
\citep{liumulti,wu2023daydreamer} to predict lower-dimensional latent state representations from high-dimensional observation-action data (shown in orange in the center of Figure~\ref{fig:front-fig}). 
To critique behavior outcomes under nuanced task specifications (e.g., ``Serve the cup of water to the guest''), we leverage vision-language models (VLMs)~\cite{dubey2024llama,openai2024gpt4technicalreport} as our open-world verifiers (shown in green in the center of Figure~\ref{fig:front-fig}).
Importantly, we demonstrate that \textit{aligning} the VLM to reason directly about the predicted latent states from the world model enables it to understand fine-grained outcomes that it cannot directly predict zero-shot nor understand from image reconstructions. 
Ultimately, this alignment step enables our ``VLM-in-the-loop'' policy steering approach to interpret action plans as behavior narrations and select high-quality plans by reasoning over those narrations even under novel task descriptions
(shown on the right of Figure~\ref{fig:front-fig}). 


We evaluate \textbf{FOREWARN} on a suite of robotic manipulation tasks, demonstrating how it can robustly filter proposed action plans to match user intent and task goals even when faced with variations not seen during training. In summary, our main contributions are:
\begin{itemize}
    \item Formalizing runtime policy steering a stochastic model-predictive control problem, revealing the \textit{generation-verification gap} \citep{Godel, cook2023complexity} and where state-of-the-art models have maximal potential to shine. 
    \item A latent space alignment strategy that enables a VLM to more reliably verify action plan outcomes predicted by a low-level, action-conditioned world model. 
    \item A novel, fully-autonomous policy steering framework that improves a base generative imitation-based policy by over $30\%$, even in novel task descriptions. 
    \item Extensive experiments in hardware showing that our latent-aligned VLM approach outperforms (by $\sim40\%$) altnerative VLM approaches that do \textit{not} decouple the prediction of outcomes from verification. 
\end{itemize}

