\section{Related Work}

\para{Generative Imitation-Based Policies}
With the rise of large-scale open-source datasets of expert demonstrations~\citep{droid, rt12022arxiv,  rt22023arxiv,contributors2024agibotworldrepo, fang2023rh20t, open_x_embodiment_rt_x_2023}, imitation learning (IL) has become a popular way to learn low-level robot control policies from data. 
In particular, recent advances in generative modeling have unlocked policy architectures that can model diverse, multi-modal behaviors directly from high-dimensional observations~\citep{chi2024diffusionpolicy,leebehavior,reuss2024multimodal}. 
At the same time, generative IL policies still exhibit nuanced, hard-to-anticipate performance degradations during deployment time.  
These degradations range from complete task failures (e.g., inability to grasp a cup, knocking it down, or dropping it~\citep{nakamoto2024steering}) potentially due to distribution shifts or visual distractors~\citep{hancock2024run,vincent2024generalizable}, to inappropriate behaviors that are misaligned with the deployment context or an end-user's preferences (e.g., placing the gripper inside of a cup filled with water during grasping)~\citep{agiaunpacking}. 
In this work, our goal is to leverage the diverse low-level behavior distribution that the base policy has learned, but prevent these nuanced performance degradations at runtime via our novel policy steering method. 


\para{Failure Detection, Monitoring \& Prediction}
The handling of generative policy failures can be grouped into three broad categories: \textit{posthoc} detection, \textit{runtime} monitoring, and failure \textit{prediction}. 
\textit{Posthoc} approaches identify and explain failures present in offline robot datasets, and have recently leveraged Vision Language Models (VLMs) to accelerate this process via video captioning, highlighting critical data frames, and providing human-interpretable summaries of failures ~\citep{duanaha, guan2024task,liu2023reflect, wangcan}. 
In contrast, \textit{runtime} monitoring aims to detect failures as they happen during robot deployment.
To quickly identify nuanced failures, recent methods propose a ``fast and slow'' approach: a fast online detector flags unusual situations (e.g., binary anomaly classifier), while a slower VLM-based reasoner provides a deeper understanding of the event and if it is a relevant failure~\citep{agiaunpacking,SinhaElhafsiEtAl2024}. 
Although these strategies can effectively identify failures, they fundamentally require the robot to start failing for the runtime monitor to activate. 
The final category, failure \textit{prediction} methods, anticipate failures before they occur and unlock the potential for preemptive correction of the base policy. 
Here, existing approaches~\citep{kambara2025futuresuccesspredictionopenvocabulary,liu2023modelbased,liumulti} often rely on out-of-distribution (OOD) detection in a latent space or dense human labels to train a binary classifier that distinguishes failures from successes. 
In this work, we contribute to the \textit{predictive} category of methods. Our method anticipates future outcomes of the policy's actions via a latent world model, and reasons about the outcomes via a VLM that is aligned with the predicted latent states. 

\para{Policy Steering}
A traditional method to improve a base IL policy is to fine-tune it with additional intervention data~\citep{liumulti} or recovery behaviors~\citep{shafiullah2024supervised}. 
However, recently, runtime policy steering has become an attractive alternative to improving a generalist IL policy \cite{nakamoto2024steering, wang2024inference} without needing any additional and expensive demonstration data.  
Runtime policy steering assumes that the base policy is capable of generating the correct behaviors, but fails to select them reliably. 
Here, an external verifier can be used to re-rank (i.e., ``steer'') the generations towards ones with good outcomes. 
Previous methods have explored humans-in-the-loop~\citep{wang2024inference} or a $Q$-function learned from very large offline datasets labeled with sparse rewards. ~\citep{nakamoto2024steering} as the verifier. In our framework, by using a VLM-in-the-loop as our verifier, we can perform policy steering autonomously after finetuning VLM on a small dataset and provide human-like interpretable guidance.


    









  
  

     







    







      

