\section{Problem Formulation}
\label{sec:problem-formulation}

We formalize the general problem of policy steering as a stochastic model-predictive control problem over the set of action plans proposed by a base action generation model. 

\input{sections/figure_forewarn_framework}

\para{Setup \& Notation} 
Let the robot's pre-trained multimodal imitative action generation model \citep{chi2024diffusionpolicy,leebehavior} be denoted by  $\pi(\acttraj_t\mid \obs_t)$. We will often refer to this model as the robot's ``base policy'' throughout this paper with which it performs a task for an end-user. 
The robot’s observations $\obs \in \obsSpace := \imgSpace \times \proprioSpace$ combine RGB image data $\img \in \imgSpace$ and proprioceptive states $\proprio \in \proprioSpace$ (e.g., end-effector pose, gripper state), and $\acttraj_t := \action_{t:t+\thor}$ denotes a robot's $\thor$ step action plan, with each action in the sequence specifying end-effector positions and rotations. Similarly, $\obstraj_t := \obs_{t:t+\thor}$ is an observation sequence. In the real world, after observing some $\obs_t$, generating some action plan $\acttraj_t \sim \pi(\cdot \mid \obs_t)$ and executing it open-loop, the robot observes a sequence of observations $\obstraj_t \sim \mathcal{P}(\cdot| \obs_t, \acttraj_t)$.


\para{Problem} Given the robot's current observation $\obs_t$ at timestep $t$ and $K$ \textit{i.i.d.} samples from the base policy, $\{\acttraj^i_t\}^K_{i=1} \sim \pi(\acttraj_t\mid \obs_t)$, the \textit{policy steering} problem seeks to return the action plan $\acttraj^{\star}_t$ which optimizes the following objective:  
\begin{align}
    \acttraj^{\star}_t = \arg\max_{\acttraj_t \in \{\acttraj^i_t\}^K_{i=1}} \mathbb{E}_{\obstraj_t\sim \mathcal{P}(\cdot \mid \obs_t, \acttraj_t) } \Big[R(\obstraj_t;\lang )\Big].
    \label{eq:policy-steering-oc-problem}
\end{align} 
Fundamentally, solving the \textit{(behavior) generation} problem specified in Eq.\ref{eq:policy-steering-oc-problem} requires two abilities: \textit{prediction} and \textit{verification}. 
The \textit{prediction} problem can be clearly seen in the expectation of Eq.\ref{eq:policy-steering-oc-problem}, wherein we have to forward simulate the outcomes of any action plan $\acttraj^i_t$ and ``imagine'' the potential future observations, $\obstraj_t \sim \mathcal{P}(\obstraj_t \mid \obs_t, \acttraj^i_t)$. 
The \textit{verification} problem lies inside of the expectation and with the reward function $R(\obstraj_t; \lang)$. 
Here, $\lang \in \langSpace$ is the task description, represented via a language description (e.g., ``Serve the cup of water to the guest.''), and $R$ verifies how well or how poorly the future observations align with the behavior specified by $\ell$.

While Eq.\ref{eq:policy-steering-oc-problem} characterizes the underlying policy steering problem, it is extremely challenging to solve end-to-end because of the coupled prediction and verification steps. 
This is where we seek to leverage vision-language models in-the-loop to obtain a practical solution with the potential to generalize to new environments and hard-to-model steering criteria.
Initially, it may be tempting use the VLM directly as a black-box solver of Eq.\ref{eq:policy-steering-oc-problem} (i.e. to solve the overarching behavior generation problem) by simply passing it the $K$ action plan options, the current observation $\obs_t$, and the task description $\lang$, and having it directly return $\acttraj^{\star}_t$. 
However, low-level action data is beyond the training distribution of current VLMs, which primarily focus on high-level semantic understanding and not embodied control~\citep{hu2023toward}. 
Alternatively, we could fine-tune the VLM to directly select action plans via labeled observation-action-samples datasets (labeled with optimal action trajectories). 
However, this strategy is sample-inefficient, requiring extensive embodied rollouts and human annotations to generate labels. 
Instead, we propose tackling the problem in Eq.\ref{eq:policy-steering-oc-problem} in a way that leverages the unique strengths of a VLM. 
We hypothesize that the right place to put VLMs into the loop is specifically in the \textit{verification} step above, as it leverages the model’s strong reasoning abilities \textit{given predicted outcomes}.
Then, to complement the VLM, we propose that \textit{prediction} should be handled by an embodied world model that can reason about hard-to-model outcomes directly from low-level actions.
It is well known that verification is significantly easier than generation for many problems \citep{cook2023complexity}, boding well for the sample-efficiency of our modular approach. 










    









