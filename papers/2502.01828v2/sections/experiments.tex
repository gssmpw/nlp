\section{Experiments}
\label{sec:experiments}
In this section, we instantiate several real-world manipulation tasks to study our method. 
We first investigate how effectively we can translate low-level actions into high-level behavior descriptions (Sec.~\ref{sec:behavior}).%
Then we evaluate the closed-loop policy steering performance as well as our method's robustness to novel task descriptions, $\lang$ (Sec.~\ref{sec:steering}). 


\para{Real Robot Setup} We use a Franka Emika robotic manipulator equipped with a 3D printed gripper from~\citep{chi2024universal} in our experiments. 
The base generative policy controls the low-level robot actions (which are the robot's end-effector pose and gripper opening) controlled at 15Hz.
The robot uses RGB images from a wrist-mounted camera and a third-person camera mounted in front of the robot. See App.~\ref{sec:appendix_robot_setup} for more details.


    
\para{Manipulation Tasks} We consider two real-world robot manipulation tasks that exhibit underlying multi-modal behaviors, hard-to-model outcomes, and nuanced failures. 
In the \textbf{Cup} task, the robot must grasp a cup placed in front of it and in the \textbf{Bag} task, the robot must pick up a bag of chips from the table. 
Fundamentally, both tasks can be accomplished in diverse ways: for example, for the \textbf{Cup} task, the robot can pick up the cup by the handle or by placing its fingers inside the cup by grasping the lip; for the \textbf{Bag} task, the robot can grab the bag by any edge, or by squeezing the center. 
Furthermore, the \textbf{Bag} task has more challenging dynamics and potential outcomes because the bag is deformable. 
We use this task to study how our framework performs when faced with harder-to-predict interaction outcomes and nuanced failures (e.g., crushing the chips inside the bag). 
In the top row of Figure~\ref{fig:behavior_qualitative}, we visualize the camera observations of the robot interacting with these items for each task.





  
\para{Base Policy} For our multi-modal imitative action generation model, $\pi(\acttraj_t \mid \obs_t)$, we use a Diffusion Policy~\citep{chi2024diffusionpolicy} trained on 100 teleoperated demonstrations per task. 
The policy takes inputs from wrist and third-person cameras, along with proprioceptive states, to predict a distribution over $\thor$-step action plan, where $\thor = 64$. Please see App.~\ref{sec:appendix_base_policy} for more details. %

\para{World Model Training}We collected 230 real-world trajectories per task, including both successful and failed rollouts from the base policy, along with additional 100 demonstrations used in base policy, for training and evaluating the world model (300 for training and 30 for testing).
The world model is trained to predict $\thor=64$ future latent states 
given the current $\obs_t$ and an action plan $\acttraj_t$. %

\para{VLM Fine-tuning} We construct our VQA dataset for fine-tuning from the same offline dataset, $\mathbb{D}_{\text{WM}}$, used to train the world model. 
For each $\thor$-step trajectory snippet  $ \{(\obs_\tau^j, \action_\tau^j\}_{\tau=t}^{t+\thor}$ from the dataset, the encoder $\enc$ processes the initial observation $\obs_t^j$ at timestep $t$, and the forward dynamics model $\dyn$ predicts latent states $\latent^j_{t:t+\thor}$ based on the action plan $\action^j_{t:t+\thor}$. We note that in our specific implementation, although $\dyn$ is a stochastic dynamics function, we use only the most likely prediction as the outcome associated with the action plan. To avoid ``semantically repetitive'' latent states between adjacent low-level actions, we downsample the T-step latent states to T/4.  
These downsampled latent states, along with the task description $\ell$, are provided as input to the VLM, and we manually annotate the corresponding behavior narrations, $\behavior \in \langSpace_{b}$, for the associated observations $\obs_{t:t+\thor}^j$.
We fine-tune the model using the Low-Rank Adaptation (LoRA) technique~\citep{hu2022lora}, keeping both the encoder $\enc$ and the latent dynamics model $\dyn$ frozen during the fine-tuning process.

\para{VLM-In-the-Loop Policy Steering} When deploying \ours for run-time policy steering, we begin by sampling 100 action plans from the base policy and aggregating them into $K=6$ modes using the non-maximum suppression (NMS) scheme from \cite{seff2023motionlm}. These 6 aggregated action plans are then passed to \ours for interpretation and evaluation. 
For each candidate action plan, we use only the most likely future latent state predictions from $\dyn$ as input to the VLM for reasoning.

\input{sections/table_behavior_translation_visual}








\subsection{From Action Rollouts to Behavior Narration}\label{sec:behavior}

As discussed in Sec.~\ref{sec:forethought}, if we want to use the VLM as an open world verifier $\rewvlm_\psi(\cdot; \lang)$, we need to enable the model to understand the underlying textual representation of low-level action outcomes.
In this section, we study if our our latent-aligned VLM can accurately describe the outcomes of low-level actions. 
We also compare our approach with several baselines to investigate the advantages of using an explicit world model for predicting action outcomes and decoding a robot’s action plans into behavior narrations. 





\para{Baselines} We compare our approach, \ours, against four baselines (more implementation details provided in App.~\ref{sec:appendix_additional_baselines}). 
(1) \oursoracle, is an upper-bound on our method's performance assuming that we had access to \textit{ground-truth} future observations (instead of relying on the latent dynamics $\dyn$ to predict future outcomes). This method uses the encoder $\enc$ on ground-truth future observations to get privileged (posterior) future latent states $\latent_{t:t+\thor}$ as input for the VLM. %
(2) \vlmact, which directly fine-tunes the original Llama-3.2-11B-Vision-Instruct model to generate behavior narrations end-to-end from the current observation $\obs_t$ and an action plan $\action_{t:t+\thor}$ (represented as text), without explicitly predicting outcomes. %
(3) \vlmimg, which utilizes the \textit{decoded} world model's predictions (i.e., the predicted future visual observations) given a robot’s planned actions. We use GPT-4o~\citep{openai2024gpt4technicalreport} to process the predicted visual observations and generate behavior narrations in a zero-shot manner.
(4) \vlmimgoracle, which is similar to \vlmimg but is an upper bound on this method by using ground-truth visual observations instead of predicted ones.

\para{Metrics} %
We adopt the metrics from~\citep{duanaha} to evaluate the alignment between predicted behavior narrations and ground-truth narrations:
(1) \textbf{LLM Score}: A similarity score (ranging from $0$ to $1$) determined by the GPT-4o model.
(2) \textbf{GT Accuracy}: A binary score ($0$ or $1$) indicating whether the predictions match the ground-truth narrations, as determined by a human labeler (in this case, the authors). For further details on the motivation behind using these metrics for evaluation, please refer to App.~\ref{sec:appendix_ablations}.




\input{sections/table_behavior_translation_eval}

\para{Results: On the Value of Explicit Action Outcome Prediction} Table~\ref{tab:vlm-ablation_all} presents the \textbf{GT Accuracy} and \textbf{LLM Score} for our approach and each baseline. The results are averaged across 30 test rollouts for each task.
The results show that \vlmact performs poorly, achieving less than $50\%$ \textbf{GT Accuracy} across all tasks. This underperformance is due to its inability to interpret low-level actions without the grounding provided by a world model’s future outcome predictions. 
In contrast, \ours, which leverages an explicit world model, outperforms \vlmact by over $50\%$ on every task, despite both being fine-tuned on the same dataset. These results demonstrate that decoupling the VLM’s burden of predicting action outcomes enables the model to produce more accurate outcome narrations than directly training the VLM to both predict outcomes and generate narrations end-to-end.



\para{Results: On the Value of VLM Fine-Tuning}
Interestingly, despite being among the most advanced VLMs, both \vlmimg and \vlmimgoracle struggle to accurately interpret robot behaviors directly from visual observations, even with access to ground-truth observations. As shown in Table~\ref{tab:vlm-ablation_all}, these methods fall behind \ours by at least $30\%$ in \textbf{GT Accuracy} and $16\%$ in \textbf{LLM Score}. These results show that existing state-of-the-art VLMs struggle to decode fine-grained motion details from video observations, underscoring the importance of fine-tuning for improved performance in such tasks.


\para{Results: Qualitative Examples}
In \autoref{fig:behavior_qualitative}, we visualize behavior narrations generated by our approach and the baselines. \ours consistently produces more accurate outcome narrations, effectively capturing nuanced motion details. In contrast, the baselines often hallucinate or fail to capture critical contact details between the gripper and objects.












 

\begin{figure*}[t!]
    \centering
    \includegraphics[width=1\linewidth]{figs/figure_policy_steering.pdf}
    \caption{\textbf{Policy Steering.} On the left, we visualize the imagined $\thor$-step rollouts decoded from the world model for the three out of six action plans sampled from the base policy. On the right, we show the corresponding behavior narrations generated from our finetuned VLM $\mathcal{T}^{\vlm}_\psi$ and the VLM's reasoning $\rewvlm_\psi(\cdot; \lang)$ about the outcomes based on the task description $\lang$ and behavior narrations to select the best action plan to execute. The bottom row shows the real-world execution of the selected behavior from the perspective of the wrist-camera.}
    \label{fig:policy-steering}
\end{figure*}



\subsection{Policy Steering for Open-World Alignment}
\label{sec:steering}


The results from the previous section demonstrate our approach's effectiveness in decoding predicted latent states into nuanced behavior narrations by explicitly using a world model for outcome prediction.
In this section, we compare our approach against several baselines to evaluate its system-level policy steering performance and robustness under \textbf{novel} task descriptions and specifications. Our experiments focus on evaluating the impact of leveraging the VLM as both an interpreter and evaluator of predicted action outcomes.



\para{Baselines} 
We first keep the VLM’s role as the verifier unchanged and ablate the effect of using an explicit world model to predict action outcomes. Specifically, we compare \ours to \vlmact (described in \autoref{sec:behavior}), which directly fine-tunes the original Llama model to predict action outcome narrations without utilizing a world model. Similar to our approach, \vlmact then queries the VLM to select the best motion plan based on the task description $l$.
Next, we keep the world model unchanged and ablate the effect of alignment: mapping latent states generated by the world model to their underlying textual representation that the VLM can easily reason about.

We compare our approach against two more baselines: (3) \vlmdynlatentcat, which also leverages a world model for outcome predictions and a VLM for action plan selection. However, instead of decoding the predicted outcomes into text representations and leveraging the VLM's open-world knowledge for policy steering, it directly fine-tunes the VLM to predict a set of indices of the successful candidate action plans and randomly selects one index from the set to execute the corresponding action plan.
(4) \classdynlatent, which is similar to \vlmdynlatentcat, but instead of relying on a VLM, it directly takes the predicted latent embeddings $\hat{\latent}_{t:t+\thor}$ as input and trains a transformer-based binary classifier (commonly used in prior failure-prediction work \citep{liumulti}) to predict success or failure for each candidate action plan and randomly selects one predicted to succeed.









\para{Metrics} We evaluate each policy steering method using the \textbf{Success Rate}. For each method, we conduct 10 trials with randomly initialized task configurations and report the average success rate across these trials. A trial is considered successful if the robot successfully completes the task while aligning with the end-user’s preferences.



\input{sections/table_policy_steering}
\para{Results: Policy Steering Performance} 
We first evaluate our approach and the baselines under task descriptions $\ell$ that fall within the training distribution.
Table~\ref{tab:policy_steering_all} shows the success rates of the base robot policy (without policy steering), our approach (\ours), and the baselines for each task.
Our results demonstrate that \ours can effectively steer the policy towards safe and aligned behavior modes by leverging the VLM as an interpreter and evaluator of predicted latent action outcomes.

Interestingly, \classdynlatent achieves comparable performance to our approach in both tasks under this setting while \vlmdynlatentcat fails to match \ours in more complicated \textbf{Bag} task. \vlmact has similar performance as base policy because the behavior narrations, directly generated from low-level action sequences, fail to capture accurate motion details and thus provide no useful signal for VLM to steer the policy.    Next, we assess their generalization abilities under novel task descriptions to evaluate the robustness of our approach. 




\para{Results: Robustness to Novel Task Descriptions} 
We evaluate the success rate of each approach when the task description is altered to introduce novel scenarios.
Specifically, in the \textbf{Cup} task, we modify the original task description from \textit{“Please grasp a cup from the table and serve the cup of water to the guest”} to a novel description: \textit{“Please grasp a cup from the table, but note that the handle is covered with oil.”} This change makes behaviors where the robot grasps the cup by the handle unsafe, while grasping the cup by the rim becomes the desired behavior.
In the \textbf{Bag} task, we modify the original task description from \textit{“Please pick up a bag of chips from the table and minimize the contact region to avoid crushing contents inside”} to a novel description: \textit{“Please pick up a bag of chips from the table and maximize stability without dropping the bag.”} This change makes behaviors where the robot picks up the bag by the corner less preferred, as the bag may slip from the gripper, while squeezing the middle of the bag to secure it becomes the desired behavior.

Interestingly, while both \vlmdynlatentcat and \classdynlatent improve task success rates when task descriptions fall within the training distribution, their performance deteriorates significantly with novel task descriptions. This indicates that the VLM struggles to reason directly about predicted action outcomes from the world model’s latent states and essentially degrades to a traditional end-to-end model. To fully leverage the VLM’s open-world reasoning capabilities for generalized policy steering, it is essential to enable the model to interpret predicted action outcomes through textual representations.


\para{Results: Qualitative Examples}
In \autoref{fig:policy-steering}, we present examples of runtime policy steering using our approach and the baselines. \ours consistently demonstrates superior performance by selecting motion plans that align with task descriptions and user preferences, even in scenarios with novel task specifications. In contrast, the baselines either fail to interpret action outcomes effectively, resulting in unsafe behaviors, or experience severe performance degradation in novel task specifications.

\para{Results: Policy Steering Speed}
Our system queries the VLM twice to first generate behavior narrations and then select the best action plan. The overall inference time is 3.7 seconds among which the generation of behavior narrations for 6 candidate action plans takes 1.3 seconds. In comparison, \vlmact takes 22.0 seconds in total for VLM inference and behavior narration runs 19.7 seconds, much slower than \ours, partially due to the usage of token per patch in images rather than a single token per state (image) like \ours. Additionally, our world model and VLM communicate directly in latent space, avoiding image decoding from world model and encoding from VLM, which could further speeds up inference.
\begin{table}[ht]
    \centering
        \renewcommand{\arraystretch}{1.5}
     \setlength{\tabcolsep}{2pt}
    \begin{tabular}{c|c|c|c|c}
    \hline
         \multirow{2}{*}{Method} &  \multicolumn{4}{c}{Time (Seconds) $\downarrow$}\\
         \cline{2-5}
         
         &World Model Prediction & Behavior Narration& Evaluation & Total \\
         \hline
        \ours &  0.1 &  1.3 & \multirow{2}{*}{2.3} & 3.7\\
        \vlmact  & - & 19.7 & & 22.0\\
        \hline
    \end{tabular}
    \caption{\textbf{Inference time for the Policy Steering System.} Inference time for each component in the system (averaged across 3 runs) shows that \ours greatly reduces the time to generate behavior narrations from our modified VLM compared to \vlmact. }
    \label{tab:inference time}
\end{table}

\para{Supplementary Experiments \& Analyses} To provide an in-depth analysis of our approach, we performed additional studies on the impact of each component on the overall system and a detailed comparison with baselines. 
We also showcase an additional application of our system as a runtime monitor which returns if a single action plan is good or bad, opening up potential future directions for soliciting supervisor assistance. 
All these experiments can be found in App.~\ref{sec:appendix_sup_exp_ana}.










































       

       
   
       









    

    








    



    








    

    


