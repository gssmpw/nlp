
\section{Our Approach: FOREWARN}
\label{sec:method}
Our key idea is to adopt a divide-and-conquer strategy, explicitly decoupling the VLM's burden of predicting action outcomes from evaluation during policy steering. 
Specifically, we take advantage of recent advances in latent dynamics models \citep{liumulti,wu2023daydreamer} which can learn lower-dimensional latent state representations from high-dimensional observation-action data collected on a robot. 
By passing possible action generations to the latent dynamics model, we can efficiently predict diverse future outcomes that would hard to model otherwise. 
Importantly, even though the world model learns a latent state that is an approximate sufficient statistic of the dynamics, one still needs to teach the VLM to evaluate outcomes \textit{in the latent representation of the world model}, rather than from the raw image observations, as we will address in detail below.

At the highest level, our mathematical formulation of model-predictive ``VLM-in-the-loop'' policy steering is: 
\begin{equation}
    \begin{aligned}
    \acttraj^{\star}_t &= \arg\max_{\acttraj_t \in \{\acttraj^i_t\}^K_{i=1}} \mathbb{E}_{\latenttraj_t \sim \dyn(\latent_t, \acttraj_t)} \Big[R^\vlm_\psi\big(\latenttraj_t; \lang \big) \Big], \\ 
    & \text{s.t.} \quad\quad \latent_t = \enc(\obs_t),\\ 
\end{aligned}
\label{eq:vlm_mpc_latent}
\end{equation}
where $\latent_{t+1} \sim \dyn(\latent_t, \action_t)$ is a probabilistic latent dynamics model, $\latenttraj_{t}:=\latent_{t:t+\thor}$, $\enc: \obsSpace \rightarrow \latentSpace$ is an observation encoder that maps the robot’s current observations $\obs_t$ into a latent space $\latentSpace$, and $R^
\vlm_\psi(\latenttraj_t; \lang)$ represents an implicit reward function embedded in our %
VLM (parameterized by $\psi$) which evaluates the predicted outcomes given the task description. Both $f_\phi$ and $\enc$ are part of our world model and parameterized by $\phi$. In short, to realize our idealized policy steering objective \eqref{eq:policy-steering-oc-problem}, we approximate the expectation over future outcomes with world model rollouts and leverage a fine-tuned VLM as a latent-outcome-conditioned verifier, leading to \eqref{eq:vlm_mpc_latent}.
We call our overall policy steering approach \textbf{FOREWARN}: \textbf{F}iltering \textbf{O}ptions via \textbf{RE}presenting \textbf{W}orld-model \textbf{A}ction \textbf{R}ollouts as \textbf{N}arration, visualize it in \autoref{fig:front-fig}, and now discuss the details.















\subsection{\textit{Foresight}: 
Predicting Outcomes via Latent World Models} 
\label{sec:foresight}


In this work, we adopt the Dreamerv3 world model ~\citep{hafner2023mastering} as our mechanism for predicting future outcomes of action plans. 
This world model (visualized on the left in Fig.~\ref{fig:overview_fig}) is pre-trained via an offline dataset that contains trajectories of the robot's observations and actions: $\mathbb{D}_{\text{WM}} = \{\{ (\obs_\tau^j, \action_\tau^j)\}_{\tau=0}^{H-1}\}_{j=1}^M$, where $M$ is the total number of trajectories with each trajectory representing the robot's outcome $\obs_{0:H}^j$ induced by an action plan $\action_{0:H}^j$, and $H$ denotes the length of these trajectories. 
The training data consists of both successful and failed rollouts from the base policy $\pi(\acttraj \mid \obs)$ and additional demonstration data. This allows the world model to accurately predict the outcomes of both good and bad actions plans, a sufficient condition for successful behavior generation \citep{ren2024hybrid}.


The world model training loss incentivizes the latent state to be informative for high-quality image decoding as well as highly predictive of the next latent state given an action (see \cite{hafner2023mastering} and Appendix~\ref{sec:appendix_world_model} for more details). 
After training, the world model is frozen, and we utilize the trained $\enc$ and $\dyn$ throughout the rest of our policy steering algorithm. 


\subsection{\textit{Forethought}: Latent-Text Alignment for Outcome Reasoning and Policy Steering}
\label{sec:forethought}

Once the world model encoder $\enc$ learns an effective low-dimensional latent state representation $\latent_t$ and the latent dynamics model $\dyn$ learns to predict future latent states conditioned on the robot’s actions, we can shift our focus to evaluating the imagined outcomes of any candidate low-level action plan. 
We hypothesize that the VLM’s open-world reasoning capabilities could allow it to be an effective verifier here, enabling better decision-making downstream.

However, before we can unlock the potential of the VLM as a verifier, we need to first enable the VLM to reason directly about the predicted latent states (rather than in terms of raw image observations). We propose approaching this problem as a \textit{latent-text alignment} problem: by mapping latent states generated by the world model to a textual representation, we can enable the VLM to more easily evaluate plans due to its strong natural language understanding abilities. 
We then further frame this alignment problem as a \textit{visual question-answering (VQA)} task, where we prompt the VLM to narrate the real-world behavior of the robot conditioned on a sequence of latent states produced by the world model (visualized on the right of \autoref{fig:overview_fig}). We now discuss this process in detail.





\para{VLM Backbone}
\label{para:vlm model}
We use the Llama-3.2-11B-Vision-Instruct model~\citep{dubey2024llama} as our VLM backbone. The original Llama model %
utilizes a Vision Transformer (ViT) to tokenize visual inputs into latent tokens, which are then processed by the LLM backbone to generate text outputs.
In our setting, we adapt the Llama model by replacing its observation (image) tokenization module with our world model’s encoder $\enc$ and latent dynamics model $\dyn$. To align the latent dynamics embedding space with the text embedding space, we introduce a linear layer as an adapter. 
Specifically, $\enc$ encodes the current robot observation, and the forward dynamics model $\dyn$ predicts future latent states based on a candidate action plan (left side of \autoref{fig:overview_fig}). These latent states are passed through the linear layer to produce a sequence of latent tokens, which serve as input to the LLM backbone (right side of \autoref{fig:overview_fig}).






\para{VLM Finetuning} Our objective is to enable the VLM to narrate the real-world behavior of the robot, denoted as $\behavior^i$, based on a sequence of generated latent states, $\latent^i_{t:t+\thor}$ from the world model. These behavior narrations highlight fine-grained motion details (e.g., ``the robot grasps the cup by the handle") rather than the high-level semantics of the motion (e.g., ``the robot grasps the cup"). This design encourages the model to capture nuanced behaviors and identify potential failures in the robot’s predicted outcomes. We construct a VQA dataset to finetune the VLM to achieve this objective (described in Sec.~\ref{sec:experiments} and App.~\ref{sec:appendix_vlm}). 


















\para{VLM-In-the-Loop Policy Steering} 
Our fine-tuned VLM can now describe the outcomes of candidate action plan in natural language narration. However, our ultimate goal is to query the VLM to identify the best action sequence. To accomplish this, we propose querying the same VLM again (i.e. using it as a verifier), leveraging its open-world reasoning capabilities and natural language understanding to select the best action plan.
Mathematically, policy steering can be expressed as:
\begin{equation}
\begin{aligned}
    \acttraj^{\star}_t &= \arg\max_{\acttraj_t \in \{\acttraj^i_t\}^K_{i=1}} \mathbb{E}_{\latenttraj_t \sim \dyn(\latent_t, \acttraj_t)} \Big[\rewvlm_\psi \big(\mathcal{T}^{\vlm}_\psi(\latenttraj_t; \lang); \lang \big) \Big], \\  
    & \text{s.t.} \quad\quad \latent_t = \enc(\obs_t).
   \label{eq:vlm_reward}
    \end{aligned}
\end{equation}
where $\mathcal{T}^{\vlm}_\psi : \latentSpace^\thor \rightarrow \mathcal{L}_{b}$ denotes the inference process that translates a candidate action plan into the associated robot behavior narration. Instead of instructing the VLM to explicitly assign the reward for each predicted action plan outcome, we leverage its implicit knowledge about the reward and directly query the VLM for the best action plan among the $K$ candidates, conditioned on the task description $l$ (see the example in right side of \autoref{fig:front-fig}), which can be seen as a form of multiple-choice question answering (MCQA).

In summary, by integrating the predictions from the world model with the VLM’s open-vocabulary behavior narration generation and commonsense reasoning, \textit{FOREWARN} guides the robot towards action plans that are aligned with the deployment context and task goals and enables the robot to proactively prevent failures.  













