\subsection{Algorithm \& Implementation Details}

\subsubsection{Base Policy} \hfill 

\label{sec:appendix_base_policy} We use Diffusion Policy~\cite{chi2024diffusionpolicy} as our robot action generation model due to its strong ability to capture multimodal and complex robot behaviors.
 
 \begin{figure}[ht]
     \centering
     \includegraphics[width=\linewidth]{figs/demonstration.pdf}
     \caption{\textbf{Multimodality in Demonstration Datasets.}}
     \label{fig:demonstration_rollout}
 \end{figure}

\para{Training dataset} For each task, we collect 100 multimodal demonstrations and train the policy on an NVIDIA A6000 GPU following the procedure in~\citep{chi2024diffusionpolicy}.  In \textbf{Cup Task},the training dataset consists of 50 demonstrations where the robot grasps the cup by the rim while touching the inner surface, and 50 demonstrations where the robot grasps it by the handle. In \textbf{Bag Task}, the training dataset consists of 50 demonstrations where the robot grasps the middle part of the bag and 50 demonstrations where it grasps the top edge without crushing the chips. Figure~\ref{fig:demonstration_rollout} presents multimodal demonstrations for each task, and Table~\ref{tab:base_policy} details the hyperparameters used for training. 
\begin{table}[ht]
    \centering
    \begin{tabular}{c|c}
        Hyperparameter & Value  \\
        \hline
         State Normalization & Yes \\
         Action Normalization & Yes \\
         Image Chunk & 2  \\
         Image Size & 256 \\
         State Dimension & 8 \\
         Action Dimension & 10 \\
         Action Execution Horizon (Bag) & 140 \\
         Action Execution Horizon (Cup) & 120 \\
          Prediction Horizon (Bag) & 120 \\
         Prediction Horizon (Cup) & 140 \\
         Batch Size & 100 \\
         Training Epochs &  600 \\
         Learning Rate &  1e-5\\
         Number of Worker& 16 \\
         Train Diffusion Step & 100 \\
         Inference Diffusion Iteration & 16 \\
         
         \hline
   
    \end{tabular}
    \caption{\textbf{Base Policy Hyperparameters.}}
    \label{tab:base_policy}
\end{table}

\para{Observation and action spaces} The robot’s policy receives inputs from a wrist-mounted camera, a third-person camera, and proprioceptive states—including the end-effector position, orientation quaternion relative to the base, and gripper opening—to predict actions that control the end-effector’s absolute pose and gripper opening.


\para{Action plan aggregation} During policy steering, we aggregate 100 sampled action plans into 6 modes. We represent each action trajectory as a T $\times$ 7 array, where T is the trajectory length, and each action consists of the gripper’s position (3D coordinates) and orientation (quaternion with four components). To cluster these trajectories into 6 modes, we apply Time Series K-Means with Dynamic Time Warping (DTW) as the distance metric. This allows us to group similar motion patterns while accounting for temporal variations in trajectory execution. The average trajectory within each cluster is used as the aggregated action plan. In the subsequent stages of our system, these 6 aggregated action plans serve as candidate options for policy steering selection.
\\

\subsubsection{World Model}\hfill 

\label{sec:appendix_world_model}
\para{Motivation} The effectiveness of world models has been demonstrated across various embodied domains~\citep{liumulti,wu2023daydreamer}. In our problem setting, it provides several key advantages: 1) it grounds low-level actions, difficult for a VLM to interpret—by predicting future image observations from action plans, bridging the gap between low-level actions and visual observations; 2) it compresses information into latent states that not only retain essential details for high-quality image decoding but also effectively predict the next latent state given an action. 

\para{Architecture} We use DreamerV3, a state-of-the-art recurrent world model from~\citep{hafner2023mastering}. Our world model, denoted as $\mathcal{W}_\phi = (\enc, \dyn, \dec)$, consists of three key components: an encoder network $\enc$, a recurrent dynamics model $\dyn$ that operates over a stochastic continuous latent space, and a decoder network $\dec$ that projects latent embeddings back into observations. Below, we provide a detailed definition of each module:
\begin{align}
\label{eq:encoder}
\latent_\tau &:= 
\begin{cases} 
\enc(\obs_\tau^i), & \quad \tau = t. \\ 
\enc(\obs_\tau^i, \latent_{\tau-1}^i, \action_{\tau-1}^i), & \quad t < \tau < t + T .
\end{cases} \\
\label{eq:dyancmis}
\hat{\latent}_{\tau+1}^i &\sim
\begin{cases} 
\dyn(\latent_\tau^i, \action_\tau^i), & \quad \tau = t. \\ 
\dyn(\hat{\latent}_\tau^i, \action_\tau^i), & \quad t < \tau < t + T. 
\end{cases} \\
\label{eq:decoder}
\hat{o}^i_\tau &:= \dec(\hat{\latent}_\tau^i), \quad t \leq \tau < t + T .
\end{align}

\begin{table}[ht]
    \centering
    \begin{tabular}{c|c}
        Hyperparameter & Value  \\
        \hline
         Observation Normalization & Yes \\
         Action Normalization & Yes \\
         Image Size & (64, 64) \\
         Batch Length & 64 \\
         Batch Size & 16\\
         Training Step (Cup) & 100000\\
        Training Step (Bag) & 150000\\
        Hidden State $h$ Dimension & 512 \\
        Stochastic Representation $z$ Dimension & 32 \\
        Dynamics Loss Ratio $\alpha_{dyn}$& 0.5 \\
        Representation Loss Ratio $\alpha_{rep}$ & 0.1 \\
        Reconstruction Loss Ratio $\alpha_{pred}$ & 1.0 \\
        CNN Encoder Depth & 32\\
        CNN Encoder Kernel Size & 4\\
        \hline
        
    \end{tabular}
    \caption{\textbf{World Model Hyperparameters.}}
    \label{tab:world_model}
\end{table}
\para{Training} The world model $\mathcal{W}_\phi$ is pretrained using an offline dataset of policy's rollouts and demonstrations $\mathbb{D}_{\text{WM}} = \{\{ (\obs_\tau^j, \action_\tau^j, \obs_{\tau+1}^j)\}_{\tau=0}^{H-1}\}_{j=1}^M$. Apart from 100 demonstrations, we collect 230 rollouts for each task, including both the success and failure experiences of the generative policy $\pi$ deployed on the real robot. 
\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{figs/Llama-structure.pdf}
    \caption{\textbf{VLM Architecture}. On the left is detailed architecture \ours, an modified VLM with an explicit world model. On the right is original Llama-3.2-Vision-Instruct model, which is used as our baseline \vlmact. This is an elaborated version of Fig.~\ref{fig:overview_fig}}
    \label{fig:llama_structure}
\end{figure}

Since offline datasets lack reward signals, we omit the reward loss. Instead, the pretraining of the world model is supervised using a modified loss function:  $\mathcal{L}_\mathcal{W} = \alpha_{dyn} \times \mathcal{L}_{dyn} + \alpha_{rep} \times \mathcal{L}_{rep} + \alpha_{pred} \times \mathcal{L}_{pred} $. Here, the dynamic loss $ \mathcal{L}_{dyn}$ incentivizes accurate forward predictions in the latent space while $\mathcal{L}_{rep}$ ensures that latent states are informative for reconstructing observations and learning good representation. Finally, the prediction loss $ \mathcal{L}_{pred}$ minimizes the error between decoded observations of the predicted latent states and the ground-truth observations. The exact loss calculation is shown in ~\citep{hafner2023mastering}. The hyperparameters used for training world model is shown in Table~\ref{tab:world_model}.


During training, the decoder $\dec$ reconstructs observations from the latent space to compute the prediction loss and the visualization helps us select the best world model. 
\begin{table}[ht]
    \centering
  \setlength{\tabcolsep}{5pt}
    \renewcommand{\arraystretch}{1.5}
    \begin{tabular}{c|c}
        Hyperparameter & Value  \\
        \hline
        LoRA (rank) & 8 \\
        LoRA (dropout) & 0.05  \\
        LoRA (alpha) & 32 \\
        Precision & bfloat16 \\
        Batch Size & 10 \\
        Learning Rate & 1e-4 \\
        Epoch (Cup) & 10 \\
        Epoch (Bag) & 15 \\
        \multirow{2}{*}{Finetuned Layers} & ["down\textunderscore proj", "up\textunderscore proj", "o\textunderscore proj", "k\textunderscore proj", \\
        & "q\textunderscore proj",  "v\textunderscore proj", "gate\textunderscore proj", "linear\textunderscore proj"]\\
        \hline
    \end{tabular}
    \caption{\textbf{VLM Hyperparameters of \ours}}
    \label{tab:vlm_hyperparameter}
\end{table}

\para{Deployment} 
Because we cannot reset the real-world environment to the exact same state, each action sequence $\acttraj_t^i$
  yields only a single observation sample $\obstraj^i_t$. As a result, the expectation in Eq.~\ref{eq:vlm_reward} is approximated in practice. During finetuning, we make the dynamics model $f_\phi$ deterministic by taking the mean of its predicted distribution. Although the model provides a 64-step prediction horizon, we downsample these future latent states to 16 to reduce redundancy from minimal changes across adjacent steps. Each latent state is formed by concatenating the hidden state and the stochastic representation, and we keep the world model frozen throughout VLM finetuning and deployment.
\\
\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{figs/prompt_narration.pdf}
    \caption{\textbf{Prompt Template for Behavior Narration in \ours.}}
    \label{fig:prompt_behavior}
\end{figure}

\subsubsection{Vision Language Model}\hfill 


\label{sec:appendix_vlm}

\para{Architecture} We use Llama-3.2-11B-Vision-Instruct model as our VLM backbone. We modify the original Llama Model to incorporate the explicit world model to predict outcomes of the action plans first and then use VLM to reason about the latent states to generate behavior narrations. The modified architecture is visualized in Fig.~\ref{fig:llama_structure}.
Specifically, we replace the original vision encoder (ViT) of Llama with our world model's encoder as well as dynamics model,  and project latent states as text tokens. We use one single linear layer to project latent tokens to text tokens.



\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{figs/prompt_steering.pdf}
    \caption{\textbf{Prompt Template for Policy Steering.}}
    \label{fig:prompt_steering}
\end{figure}

\para{Finetuning} We adopt LoRA to finetune our modified model, loading the base weights from Llama-3.2-Vision-Instruct and randomly initializing the new linear projection layer. This approach updates only $0.2664\%$ of the original model’s parameters. The finetuning hyperparameters are listed in Table~\ref{tab:vlm_hyperparameter}, and we select the model with the lowest evaluation loss for deployment.
\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{figs/prompt_monitoring.pdf}
    \caption{\textbf{Prompt Template for Failure Monitoring.}}
    \label{fig:prompt_monitoring}
\end{figure}

 

\para{Prompt}
The modified VLM is finetuned to generate behavior narration with the prompt template in Fig.~\ref{fig:prompt_behavior}, explaining the predicted latent states from the world model. 

In policy steering. we sample and aggregate action sequences into 6 modes to query VLM for the best choice. Each mode's behavior narration replaces Action Plan in the prompt template in Fig.~\ref{fig:prompt_steering}. 

Another potential usage of our system is to evaluate and monitor different policies' performances under specific task description before execution. To showcase our system can be a reliable failure monitor across different tasks, we conduct additional experiments in App.~\ref{sec:appendix_sup_exp_ana}. We list the prompts used for failure monitor in Fig.~\ref{fig:prompt_monitoring}. 


Different tasks have different task descriptions, and specifications (i.e., contexts), which are put in the prompt for policy monitoring and policy steering can be found in Sec.~\ref{sec:experiments}
 
\begin{figure}[h!]
        \centering
        \includegraphics[width=\linewidth]{figs/prompt_vlmact.pdf}
        \caption{\textbf{Prompt Template for Behavior Narration for \vlmact in Cup Task}}
        \label{fig:prompt_vlmact}
    \end{figure}
\begin{table}[h!]
    \centering 
    \begin{tabular}{c|c}
   Hyperparameter & Value  \\
        \hline
        LoRA (rank) & 8 \\
        LoRA (dropout) & 0.05  \\
        LoRA (alpha) & 32 \\
        Quantization & 4bit \\
        Batch Size & 1 \\
        Learning Rate & 1e-5 \\
        Epoch (Cup) & 10 \\
        Epoch (Bag) & 15 \\
        \multirow{2}{*}{Finetuned Layers} & ["down\textunderscore proj", "up\textunderscore proj", "o\textunderscore proj", "k\textunderscore proj", \\
        & "q\textunderscore proj",  "v\textunderscore proj", "gate\textunderscore proj"]\\
        \hline
    \end{tabular}
    \caption{VLM Hyperparameters of \vlmact.}
    \label{tab:vlmact_hyperparameter}
    \end{table}
 
    
\subsubsection{Additional Details of Baselines}\hfill
\label{sec:appendix_additional_baselines}

    \para{\vlmact} This baseline is an ablated version of \ours without the explicit world model. It uses the original Llama-3.2-11B-Vision-Instruct model as shown in right part of Fig.~\ref{fig:llama_structure} and finetuned with the same labels as in \textbf{VQA Dataset}. The action plans and states are prompted as text and image observations are concatenated together and processed as image tokens. Details of the prompt are available in Fig.~\ref{fig:prompt_vlmact}. Since VLM cannot directly interpret the low-level action control, we give some privileged information in the prompt, marked as red in the prompt template, to help it generate behavior narration. However, this baseline still struggles to generate accurate behavior narration as shown in Sec.~\ref{sec:behavior}, despite being finetuned on the same dataset. For policy monitoring and policy steering, \vlmact uses the same prompt as \ours. Hyperparamers used for finetuning are shown in Table.~\ref{tab:vlmact_hyperparameter}
   
 
    \begin{figure}[h!]
        \centering
        \includegraphics[width=\linewidth]{figs/prompt_gpt4o.pdf}
        \caption{\textbf{Prompt Template for Behavior Narration for GPT-4o in Cup Task.}}
        \label{fig:prompt_gpt4o}
    \end{figure}
       \begin{table}[h!]
    \centering 
    \begin{tabular}{c|c}
   Hyperparameter & Value  \\
        \hline
        Embedding Dimension & 64 \\
        Number of Head & 1 \\
        Attention Dropout & 0.05\\
        Embedding Dropout & 0.05 \\
        Block Output Dropout & 0.05 \\
        Context Length & 16 \\
        Sinusoidal Embedding & True \\
        Learning Rate & 1e-4\\
        Gradient Clip & $(-\infty , 100]$ \\
        Epochs & 30 \\
        
        \hline
    \end{tabular}
    \caption{Hyperparameters of \classdynlatent.}
    \label{tab:classifier}
    \end{table}
    \para{\vlmimg \& \vlmimgoracle} We further evaluate an advanced VLM (GPT-4o) to interpret fine-grained motion details from a sequence of 16 images, either reconstructed from predicted latent states or recorded from actual execution. GPT-4o runs at the default temperature. As shown by the red text in Fig.~\ref{fig:prompt_gpt4o}, GPT-4o still struggles to comprehend subtle motion details, even when given privileged information to guide its focus.
   
 
    
    \para{\classdynlatent} For policy steering and monitoring, we adopt a causal transformer-based binary classifier~\citep{liumulti} to directly predict success or failure from future latent states. Table~\ref{tab:classifier} summarizes its hyperparameters. We manually label each rollout as success or failure under the first task description to train the classifier.
    
    \para{\vlmdynlatentbin}  This end-to-end baseline also predicts a binary success/failure label from future latent states, but employs the same modified VLM architecture as \ours, which provides a larger capacity than the transformer-based classifier. It is trained on the same dataset as \classdynlatent, with the prompt template shown in Fig.~\ref{fig:prompt_vlmdynlatentbin}.

    \begin{figure}[h!]
        \centering
        \includegraphics[width=\linewidth]{figs/prompt_vlmdynlatentbin.pdf}
        \caption{\textbf{Prompt Template for Policy Monitoring for \vlmdynlatentbin.}}
        \label{fig:prompt_vlmdynlatentbin}
    \end{figure}
            \begin{figure}[h!]
        \centering
        \includegraphics[width=\linewidth]{figs/prompt_vlmlatentdyncat.pdf}
        \caption{\textbf{Prompt Template for Policy Steering for \vlmdynlatentcat.}}
        \label{fig:prompt_vlmdynlatentcat}
    \end{figure}
    \para{\vlmdynlatentcat} 
Similarly, we develop an end-to-end approach that directly predicts a valid set of action plans for policy steering. The modified VLM is finetuned to output which action‐plan indexes are valid under the current task description. Its input includes a text prompt and six sequences of predicted future latent states (Fig.~\ref{fig:prompt_vlmdynlatentcat}), each corresponding to one candidate action plan.
    


\subsection{Experiment}
\subsubsection{Real Robot Setup}\hfill 
\label{sec:appendix_robot_setup}
Fig.~\ref{fig:robot_setup} demonstrates the setup of our real-world experiments. We employ two cameras, a RealSense D435 camera on the Franka hand and a Zed mini 2i camera placed in front of the robot. In order to increase the contact region and compilancy, we replace the original Franka gripper finger with 3D printed gripper finger from~\citep{chi2024universal}.
\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{figs/real_robot_setup.pdf}
    \caption{\textbf{Real Robot Setup.} The visualization of the real robot environment and the positions of the cameras. }
    \label{fig:robot_setup}
\end{figure}
\begin{table*}[ht]
    \centering
    \setlength{\tabcolsep}{2pt}
    \renewcommand{\arraystretch}{1.5}
    \begin{tabular}{c|ccc|ccc|ccc|ccc|ccc|ccc}
    \hline
    \multirow{3}{*}{Method} 
    & \multicolumn{6}{c|}{Cup} & \multicolumn{6}{c|}{Bag} & \multicolumn{6}{c}{Average} \\
    \cline{2-19}
    & \multicolumn{3}{c|}{Training} & \multicolumn{3}{c|}{Unseen} & \multicolumn{3}{c|}{Training} & \multicolumn{3}{c|}{Unseen} & \multicolumn{3}{c|}{Training} & \multicolumn{3}{c}{Unseen} \\
    \cline{2-19}
    & Acc$\uparrow$& TPR$\uparrow$& TNR$\uparrow$& Acc$\uparrow$& TPR$\uparrow$ & TNR$\uparrow$ & Acc$\uparrow$ & TPR$\uparrow$ & TNR$\uparrow$ & Acc$\uparrow$ & TPR$\uparrow$ & TNR$\uparrow$ & Acc$\uparrow$ & TPR$\uparrow$ & TNR$\uparrow$ & Acc$\uparrow$ & TPR$\uparrow$  & TNR$\uparrow$ \\
    \hline
    \ours (Ours)             & \textbf{0.90} & \textbf{0.80} & \textbf{1.00} & \textbf{0.75} & \textbf{0.70} & 0.80  & 0.75 & 0.71 & 0.77 & \textbf{0.75} & \textbf{0.75} & 0.75 & 0.83 & 0.76 & 0.89 & \textbf{0.75} & \textbf{0.73} & 0.78 \\
    \vlmdynlatentbin   & 0.85 & 0.77 & 0.91 & 0.15 & 0.11 & 0.27 & 0.75 & \textbf{0.86} & 0.69 & 0.40 & 0.38 & 0.42 & 0.80 & \textbf{0.82} & 0.80 & 0.28 & 0.25 & 0.35 \\
    \classdynlatent    & \textbf{0.90} & \textbf{0.80} & \textbf{1.00} & 0.10 & 0.10 & 0.10 & \textbf{0.80} & 0.71 & 0.85 & 0.35 & 0.13 & 0.50 & \textbf{0.85} & 0.76 & \textbf{0.93} & 0.23 & 0.12 & 0.30 \\
    \vlmact            & 0.65 & 0.75 & 0.58 & 0.50 & 0.10 & \textbf{0.90} & 0.60 & 0.00 & \textbf{0.92} & 0.65 & 0.13 & \textbf{1.00} & 0.63 & 0.38 & 0.75 & 0.58 & 0.12 & \textbf{0.95} \\
    \hline
    \end{tabular}
    \caption{\textbf{Policy Monitoring.}
    The reported result in the table is averaged over 20 trajectories. \ours, \vlmdynlatentbin and \classdynlatent perform similarly well in training task description while \vlmact has poor performance. In unseen task description, \ours is the only method that maintains similar performance as training task description.}
    \label{tab:policy_evaluation_all}
\end{table*}

   \begin{figure*}[h!]
       \centering
       \includegraphics[width=\linewidth]{figs/policy_monitoring.pdf}
       \caption{\textbf{Policy Monitoring.} In the top (pink) and medium (green) row, the robot imagines correctly about the robot behaviors but only \ours describes the behavior correctly and generates correct monitoring results with adequate explanations. In the bottom row (blue), all of the methods distinguish failure correctly from success.}
       \label{fig:policy_monitoring_more}
   \end{figure*}

\begin{table*}[h!]
        \centering
        \setlength{\tabcolsep}{3pt}
        \renewcommand{\arraystretch}{1.5}
        
        \begin{tabular}{c|ccc | ccc |ccc |ccc|ccc | ccc}
        \hline
            \multirow{3}{*}{Method} & \multicolumn{18}{c} {Accuracy $\uparrow$}\\
            \cline{2-19}
             & \multicolumn{6}{c|}{Independent}&  \multicolumn{6}{c|}{Training Task Description } & \multicolumn{6}{c}{Novel Task Description } \\
             \cline{2-19}
            & \multicolumn{3}{c}{World Model}& \multicolumn{3}{c|}{Behavior Narration} & \multicolumn{3}{c|}{Reasoning} & \multicolumn{3}{c|}{Overall System} & \multicolumn{3}{c|}{Reasoning} & \multicolumn{3}{c}{Overall System }\\
            \cline{2-19}
            & Cup & Bag & Average & Cup & Bag & Average &Cup & Bag & Average &Cup & Bag & Average &Cup & Bag & Average &Cup & Bag & Average \\
            \hline 
            \ours & \multirow{3}{*}{0.80} &\multirow{3}{*}{0.75}  & \multirow{3}{*}{0.78} &  \textbf{0.90}  & \textbf{0.70} & \textbf{0.80} & \textbf{1.00} & 0.85 & 0.93 & \textbf{0.90} & 0.75 & 0.83  & \textbf{0.90} & 0.85 & 0.88 & \textbf{0.75} & \textbf{0.75} & \textbf{0.75}\\
            \vlmdynlatentbin & &  &   & -& -& - & 0.95 & 0.80 & 0.88 & 0.85 & 0.75 & 0.80 & 0.20 & 0.45 & 0.33  & 0.15 & 0.40 & 0.28 \\
            \classdynlatent &  &  &  & -& -& - & \textbf{1.00}  & 0.90 & \textbf{0.95} & \textbf{0.90} &\textbf{ 0.80 }& \textbf{0.85} & 0.05 & 0.25 & 0.15 & 0.10 & 0.35 & 0.23\\
            \vlmact &- & - & -&0.35 & 0.35 & 0.35 & 0.95 & \textbf{0.95} & \textbf{0.95} & 0.65 & 0.60 & 0.63 & \textbf{0.90}  & \textbf{1.00} & \textbf{0.95} &0.50 & 0.65 & 0.58\\
            \hline
        \end{tabular}
        \caption{\textbf{Performance Breakdown} for each component in our pipeline across four methods. \ours have high accuracy across all components while \vlmact has very low accuracy in narration, leading the poor performance of the overall system. both \classdynlatent and \vlmdynlatentbin performs well in training task description and drop sharply in novel task scenarios. }
        \label{tab:policy_breakdown_all}
    \end{table*}


\subsubsection{Supplementary Experiments \& Analyses}\hfill 
\label{sec:appendix_sup_exp_ana}



\para{FOREWARN as a VLM-in-the-loop Failure Monitoring System} In this section, we present another application of our approach—preemptive failure monitoring—based on the behavior narrations generated in Sec.~\ref{sec:behavior}. Similarly, our modified VLM first decodes the predicted latent states from the world model, as behavior narrations $\hat{\behavior}$. Then it reasons about behavior narrations under task description $\lang$ and decides whether the future action plan, translated as $\hat{\behavior}$, is a failure. As both quantitative and qualitative results demonstrate,  \ours is a reliable and versatile failure monitoring framework across diverse task.

\para{Baselines} We consider three methods as our baselines, which preemptively predict the outcome of the action plan before the execution. 1) \vlmdynlatentbin takes the predicted latent states from the world model and task description $\lang$ as input to the VLM, directly generating binary output to indicate success or failure without the intermediate step of behavior narration; 2) \classdynlatent takes the predicted latent states as input and trains a transformer-based binary classifier to generate binary output with the same dataset as \vlmdynlatentbin; 3) \vlmact uses generated behavior narrations in Sec.~\ref{sec:behavior} and queries the VLM again to decide if the behavior is a success or failure within the context of the task description $\lang$. These baselines are equivalent to those in Sec.~\ref{sec:steering}.

\para{Metrics}
To evaluate the overall performance of different methods as preemptive failure monitors, we report the standard detection metrics from prior work~\citep{agiaunpacking} including \textit{Accuracy (ACC)}, \textit{True Positive Rate (TPR)}, \textit{True Negative Rate (TNR)}.

\para{Results: Failure Monitoring Performance} Both \vlmdynlatentbin and \classdynlatent perform well when evaluation task description is the same as training. However, their performances drop sharply in novel task description, indicating poor generalization of end-to-end model even though they have the same model capacity as our method.  In contrast, \ours consistently attains high accuracy, higher than $75\%$, with balanced TPR and TNR across all tasks given different task descriptions, demonstrating its reliability and flexibility as a failure monitor.

The generalization capability of our method comes from decoupling the problem of failure monitoring as behavior narration and outcome evaluation. 

This is also demonstrated by \vlmact, which has the same intermediate step and shows balanced performance in both task descriptions, but \vlmact often misclassifies behaviors in \textit{Bag Task} as failures, resulting in low TPR and lower overall accuracy than \ours.

 In Fig.~\ref{fig:policy_monitoring_more}, we demonstrate monitoring results for all three different behaviors qualitatively. Across all three modes of behaviors, \ours consistently generates accurate descriptions as well as correct monitoring results. \vlmact is biased to generate failure narrations across all three modes, leading to wrong monitoring results. \classdynlatent and \vlmdynlatentbin completely do not understand different action plans within different task descriptions. They generate the same monitoring results for totally different descriptions, contradictory to the actual execution.

\para{Component-level Analysis for the System}
We analyze each component in our method as well as all the baselines in (Table~\ref{tab:policy_breakdown_all}). 
\ours shows high accuracy across all the components while the poor performance of \vlmact is from the low-quality of narration directly generated from low-level action sequences. After finetuning, both methods preserve the strong reasoning capability of the VLM. However, \classdynlatent and \vlmdynlatentbin has a huge performance drop in novel task scenarios because they are overfitting to the specific task description in the training.
\\

\subsubsection{Metric Ablations}\hfill 
\label{sec:appendix_ablations}

\para{Metrics for Behavior Narration}
We investigate four common text-generation metrics proposed in prior work~\citep{duanaha}: \textit{Cosine Similarity}, \textit{ROUGE-L}, \textit{LLM Fuzzy Matching}, and \textit{Binary Success Rate}. To assess each metric’s correlation with ground-truth labels, we sample 16 narrations for each of three behaviors in the \textbf{Cup Task} (grasping by handle, grasping by rim, and grasp failures), yielding 360 intra-category and 768 inter-category comparisons. As shown in Figures.~\ref{fig:rouge_score},\ref{fig:cosine_score}, and\ref{fig:llm_score}, it is difficult for Cosine Similarity and ROUGE-L to cleanly distinguish narrations from the same category versus different categories, whereas \textit{LLM Fuzzy Matching} with GPT-4o can easily separate them. This discrepancy arises because the narrations share similar high-level semantics (e.g., “grasping the cup”) and differ only in fine-grained details (e.g., grasp location). Consequently, we adopt \textbf{LLM Score} and \textbf{Ground-Truth Accuracy} (manual matching) as our final metrics for evaluating generated behavior.
   \begin{figure}[h!]
   \vspace{-0.3cm}
    \centering
    \includegraphics[width=0.8\linewidth]{figs/rouge_similarity_score.pdf}
    \caption{\textbf{Distribution of ROUGE-L Score} shows that intra-category scores are overlapped with inter-category ones.}
    \label{fig:rouge_score}
    \vspace{-0.3cm}
\end{figure}
\begin{figure}[h!]
\vspace{-0.5cm}
    \centering
    \includegraphics[width=0.8\linewidth]{figs/cosine_similiarty_score.pdf}
    \caption{\textbf{Distribution of Cosine Similarity Score} shows that intra-category scores are overlapped with inter-category ones.}
    \label{fig:cosine_score}
    \vspace{-0.4cm}
\end{figure}
\begin{figure}[h!]
    \centering
    \vspace{-0.3cm}
    \includegraphics[width=0.8\linewidth]{figs/gpt4-o-mini-behavior-score.pdf}
    \caption{\textbf{Distribution of LLM Score} shows inter-category  and inter-category scores can be roughly separated at 0.7.}
    \label{fig:llm_score}
\end{figure}




