  \begin{table}[ht]
   \tiny
        \centering
        \setlength{\tabcolsep}{5pt}

        \renewcommand{\arraystretch}{1.6}
        \begin{tabular}{c|ccc|ccc}
     
        \hline
       
           \multirow{2}{*}{} & \multicolumn{3}{c|}{\textbf{GT Accuracy} $\uparrow$ } & \multicolumn{3}{c}{\textbf{LLM Score} $\uparrow$}\\
           \cline{2-7}
        & \textbf{Cup} & \textbf{Bag} & Average & \textbf{Cup} & \textbf{Bag} & Average\\
            \hline
            \oursoracle & 0.92$\pm$0.02 & 0.78$\pm$0.02 & 0.85$\pm$0.02 &  0.87$\pm$0.01 & 0.76$\pm$0.02 & 0.82$\pm$0.02 \\
            \hline
            \ours (Ours) & \textbf{0.89}$\pm$\textbf{0.02} & \textbf{0.76}$ \pm $\textbf{0.03} & \textbf{0.83}$\pm$\textbf{0.03} & \textbf{0.84}$\pm$\textbf{0.01} & \textbf{0.71}$\pm$\textbf{ 0.03}&\textbf{0.78}$\pm$\textbf{0.02} \\
            \vlmact & 0.37$\pm$0.03 & 0.34$\pm$0.05 & 0.36$\pm$0.04 & 0.52$\pm$0.06 & 0.50$\pm$0.08 & 0.51$\pm$0.07\\
            
            \vlmimgoracle & 0.58$\pm$0.05 & 0.43$\pm$0.03 & 0.51$\pm$0.04 & 0.66$\pm$0.02 & 0.64$\pm$0.02 & 0.65$\pm$0.02\\
            \vlmimg &  0.34$\pm$0.04 & 0.27$\pm$0.07& 0.31$\pm$0.06 & 0.56$\pm$0.02 & 0.56$\pm $0.04 & 0.56$\pm$0.03\\ 
            \hline 
        \end{tabular}
        \caption{\textbf{Alignment Between Predicted Behavior Narrations and Ground-Truth Narrations.} \ours outperforms all baselines across both tasks and achieves performance comparable to \oursoracle, which has access to ground-truth action outcomes and represents the upper bound for our approach.  We use 30 rollouts to evaluate the performance. For \ours, \oursoracle and \vlmact, the mean and standard deviation are reported by running 3 seeds for the finetuning experiments while \vlmimg and \vlmimgoracle, report 3 queries of GPT-4o. }
        \label{tab:vlm-ablation_all} 
        \vspace{-0.3cm}
    \end{table}
    

