\documentclass[conference]{IEEEtran}
\usepackage{times}
\usepackage{tabularx,ragged2e}
\usepackage[numbers]{natbib}
\usepackage{multicol}
\usepackage[bookmarks=true]{hyperref}
\usepackage{multirow}
\usepackage{array}
\usepackage{textgreek}
\newcolumntype{C}{>{\arraybackslash}X}
\input{notation_macros}
\pdfminorversion=4  

\begin{document}

\title{From Foresight to Forethought: VLM-In-the-Loop Policy Steering via Latent Alignment}

\author{Yilin Wu$^{1}$, Ran Tian$^{2}$, Gokul Swamy$^{1}$, Andrea Bajcsy$^{1}$\\
$^{1}$Carnegie Mellon University $^{2}$UC Berkeley

}





\makeatletter
\let\@oldmaketitle\@maketitle%
\renewcommand{\@maketitle}{\@oldmaketitle%
\setcounter{figure}{0} %
\centering
\includegraphics[width=1.0\textwidth]{figs/framework_figure.pdf}
\captionof{figure}{We present \textbf{FOREWARN}, an VLM-in-the-loop policy steering algorithm for multi-modal generative robot policies. 
Our key idea is to decouple the VLM's burden of predicting action outcomes from evaluation. 
By predicting action outcomes with a pre-trained latent dynamics model and aligning a VLM to reason about these latent states in text, FOREWARN can select action plans at runtime that are most appropriate for new task contexts and user needs. 
}
\label{fig:front-fig}
\vspace{-0.3in}
\bigskip}
\makeatother
\maketitle



\begin{abstract}
While generative robot policies have demonstrated significant potential in learning complex, multimodal behaviors from demonstrations, they still exhibit diverse failures at deployment-time. 
Policy steering offers an elegant solution to reducing the chance of failure by using an external verifier to select from low-level actions proposed by an imperfect generative policy. 
Here, one might hope to use a Vision Language Model (VLM) as a verifier, leveraging its open-world reasoning capabilities. 
However, off-the-shelf VLMs struggle to understand the consequences of low-level robot actions as they are represented fundamentally differently than the text and images the VLM was trained on. 
In response, we propose FOREWARN,
a novel framework to unlock the potential of VLMs 
as open-vocabulary verifiers for runtime policy steering. 
Our key idea is to decouple the VLM's burden of predicting action outcomes (\textit{foresight}) from evaluation (\textit{forethought}). 
For foresight, we leverage a latent world model to imagine future latent states given diverse low-level action plans. 
For forethought, we align the VLM with these predicted latent states to 
reason about the consequences of actions in its native representation---natural language---and effectively filter proposed plans. 
We validate our framework across diverse robotic manipulation tasks, demonstrating its ability to bridge representational gaps and provide robust, generalizable policy steering. Videos can be found on the project website: ~\href{https://yilin-wu98.github.io/forewarn/}{https://yilin-wu98.github.io/forewarn/}.
\end{abstract}

\IEEEpeerreviewmaketitle

\input{sections/intro-v2}
\input{sections/related-work}
\input{sections/problem}
\input{sections/method}
\input{sections/experiments}
\input{sections/limitation}
\input{sections/conclusion}


\bibliographystyle{plainnat}
\bibliography{bibliography}
\clearpage
\appendix
\input{sections/appendix}

\end{document}
