\input{figures/llm_issues}

\section{Fixing \mcqa Can Help Us Fix LLMs} \label{section:models}

Fixing issues in \mcqa's format (\cref{section:format}) and datasets (\cref{section:dataset}) will not just improve evaluation quality; they can also improve our understanding of LLM weaknesses.
This section outlines three persistent
issues of LLMs in \mcqa (Figure~\ref{fig:llm_issue})---robustness~(\cref{subsection:robustness}), bias
(\cref{subsection:bias}), and explanations
(\cref{subsection:explanations})---and how our prior solutions can better address or evaluate them.


\subsection{New Prompts Lead to New \mcqa Scores} \label{subsection:robustness}
% \subsection{LLMs Lack Robustness in \mcqa} \label{subsection:robustness}

% On specific prompts, LLMs score highly in~\mcqa, but currently crumble
% when prompts change, sensitive to: choice
% symbols~\cite{alzahrani2024benchmarks}, choice
% order~\cite{pezeshkpour2023large, Zheng2023LargeLM}, and
% phrasing~\cite{holtzman-etal-2021-surface, Wiegreffe2023IncreasingPM}.
% This brittleness weakens \mcqa leaderboard reproducibility, as varied
% setups yield conflicting rankings~\cite{gu2024olmes}

On specific prompts, LLMs score highly in~\mcqa, but currently crumble
when prompts change, sensitive to: choice
symbols \cite{alzahrani2024benchmarks}, choice
ordering \cite{Zheng2023LargeLM}, and
phrasing \cite{Wiegreffe2023IncreasingPM}.
This brittleness degrades \mcqa leaderboard reproducibility, as varied setups produce conflicting rankings~\cite{gu2024olmes}


LLM robustness varies by task setup.
%
In \mcqa, early LLMs with poor instruction
following \cite{zhang2022opt} used probability-based \textit{scoring},
while instruction-tuning enabled LLMs to \textit{generate}
answers \cite{longpre2023flan}; while logically equivalent, these give varying answers \cite{lyu2024beyond}. 
Probability scoring looks to be more at fault, more sensitive to prompts \cite{wang2024look}, doubting if LLMs can aid decision-making tasks that need accurate confidence scores \cite{liu2024dellma}.
To track this progress in LLMs, researchers can use \mcqa scoring protocols that measure calibration~(\cref{subsubsection:shortcut_scoring}).

As accuracy often \textit{drops} post-perturbation \cite{zhou-etal-2024-revisiting}, these failures in generalization could indicate dataset leakage (\cref{subsection:test_set_leakage}) or over-reliance on biases (discussed in \cref{subsection:bias}).
Another proposed explanation is symbol binding error: LLMs ``know'' the right answer but cannot link it to the correct choice \cite{Wiegreffe2024AnswerAA, xue2024strengthened}.
These failures weaken \mcqa's ability to test knowledge, obscured by memorization and symbol binding.\footnote{Such evaluations have downstream use (e.g. privacy, interpretability), but do not truly measure knowledge as intended.}~Our proposed solutions---like curating live MCQs for data leakage (\cref{subsection:test_set_leakage}) and generative \mcqa formats to expose knowledge gaps without needing symbol binding (\cref{section:fixing_format})---can more reliably assess knowledge.

% \jbgcomment{If short on space, some of these could be move to appendix}

\subsection{LLMs are Biased \mcqa Test-Takers} \label{subsection:bias}

Like most \abr{nlp} tasks \cite{chu2024fairness}, LLMs have \mcqa biases, grouped into two types.
%
% \jbgcomment{can we be a little more precise than ``favor''?}
%
The first are \mcqa-specific biases---picking answers based on symbols \cite{Zheng2023LargeLM}, positions \cite{Li2024CanMQ,
wei-etal-2024-unveiling}, or phrases like ``none of the
above'' \cite{xu2024llms, wang2025llms} rather than MCQ content.
This degrades robustness
(\cref{subsection:robustness}), masking model knowledge.
%
We believe they likely stem from shortcuts (\cref{subsection:artifacts});
%
LLMs tuned on data where choices with patterns are often
correct will exhibit these biases~\cite{pacchiardi2024leaving}.
%
Our safeguards---uniform design, contrast sets,
and ``cheating'' (\cref{subsubsection:shortcut_dataset})---can
reduce these biases, and more work in shortcuts may be key
to quash them.

The second bias type stems from general training, like cultural/linguistic bias \cite{myung2024blend, li2023land};
LLMs often err on non-Western cultural MCQs \cite{Acquaye2024SusuBO, azime2024proverbeval} and non-English MCQs \cite{son2024kmmlu, li2023cmmlu}.
MCQs are popular for testing bias \cite{guo2023evaluating}, but if they use subjective commonsense \cite{seo-etal-2024-kocommongen}, they are subpar (\cref{subsection:best_answer}).
Thus, we advise writing MCQs with rubrics to limit ambiguity and correct distractors (\cref{subsection:quality}),~ensuring bias is more objectively tested.
If subjectivity persists, evaluators should consider ways to handle it like our~proposed Explanation MCQA (\cref{subsection:justified_mcqa}) or calibration scoring (\cref{subsubsection:shortcut_scoring}), aiding bias testing and for \emcqa, better matching how bias presents downstream \cite{seshadri2025does}.

\subsection{LLMs Struggle to ``Explain Their Work''} \label{subsection:explanations}

% LLMs can easily pick MCQ answers~(\cref{subsection:saturation}), but often give \textbf{unfaithful} explanations---failing to mirror their true reasoning---whether via chain-of-thought \cite{lyu2023faithful, lanham2023measuring, turpin2024language} or self-explanations \cite{agarwal2024faithfulness, kim2024can, madsen2024self}.
% However, they are convincing, surpassing crowdworkers in perceived quality \cite{mishra-etal-2024-characterizing} and misleading humans when incorrect \cite{si2023large}, likely due to LLM alignment protocols that optimize user preferences instead of accuracy \cite{wen2024language}.

Even if LLMs get the right answer to an~MCQ, they may justify their selection \textbf{unfaithfully}---failing to mirror their true reasoning---whether via chain-of-thought \cite{lyu2023faithful, turpin2024language} or self-explanations \cite{kim2024can, madsen2024self}.
However, they are convincing, besting crowdworkers in judged quality \cite{mishra-etal-2024-characterizing} and misleading users when wrong \cite{si2023large}, likely as LLMs are fine-tuned to optimize on user preferences over accuracy \cite{wen2024language}.

LLM explanation flaws are even clearer after logical consistency checks.
\citet{kawabata2023evaluating} show LLMs often inaccurately explain answers to subquestions in reading comprehension, even when answering the higher-level question correctly.
Similarly, \citet{balepur-etal-2024-easy} find LLMs struggle to reason why distractors are wrong.
These issues likely stem from broader LLM logical inconsistencies \cite{liu2024aligning, varshney2024investigating}.

Explanations are popular LLM use cases~(\cref{subsection:use_cases}), highlighting the need for improved \mcqa formats such as Explanation MCQA (\cref{subsection:justified_mcqa}) which explicitly test explanation skills.
Further, LLMs' logically inconsistent explanations offer a path toward harder datasets (\cref{subsection:saturation}); tools like MIRT (\cref{subsection:irt}) can identify logical error types (negation, decomposition) that elude LLMs, while adversarial collection can curate MCQs to excise these errors while staying easy for humans (\cref{subsection:hitl}).
In all, LLMs' poor explanation abilities give an opportunity to design harder \mcqa evaluations better aligned with user needs.




% \subsection{Shortcuts}

% Word level shortcuts in MCQA \cite{pacchiardi2024leaving}

% BERT can do MRC without the passage, random keywords distract BERT, perturbations change accuracy \cite{si2019does}

% More MRC shortcuts \cite{Yu2020CounterfactualVC}

% Quantifying the learnability of shortcuts \cite{Shinoda2022WhichSS}

% MRC doesnt need the input passage \cite{raina2023analyzing}

% MRC doesnt need the input passage, neither do humans \cite{liusie2022world} 

% Easy to cheat on MCQA versions of multi-hop datasets \cite{chen2019understanding}


% \subsection{Future Directions}

% Interpretability via contrastive edits \cite{ross2020explaining}

% Interp via circuits \cite{lieberum2023does}

% Interpreting biases \cite{shen2022understanding}

% AI Safety --- some MCQA tasks are hard for humans, we can study models in relation to helping humans \cite{parrish2022single, parrish2022two, wen2024language}
