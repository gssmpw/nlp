\section{Questioning Multiple Choice Questions}

Multiple choice question answering (\mcqa) is~the standard for large language model (LLM) evaluations, prized for simplicity and
similarity to human testing \cite{robinson2023leveraging}.
%
When designing new benchmarks, \mcqa seems easy to~implement
\cite{guo2023evaluating}, and when selecting new LLMs to use, \mcqa
leaderboards inform our decisions \cite{open-llm-leaderboard-v2}.
%
If you want to build a popular dataset, prove your LLM is smart,
or even publish a position paper, it is hard to avoid~\mcqa.

Standardized testing groups have long explored ways to
better use \mcqa for student testing \cite{angoff1971college}.
But despite years of use in \abr{nlp}~\cite{turney2003combining}, few have asked: 1) should \mcqa be a standard model evaluation format; and 2) are its datasets well-designed?
This position paper argues: \textbf{Evaluating LLMs with \mcqa has flaws in both its inherent format and dataset construction.}
We state our case in three points (Figure~\ref{fig:intro}).

\input{figures/intro}

We first argue \mcqa is not the best standardized format for LLM evaluations, showing its goal of ``pick the
best answer'' cannot optimally test generation or subjectivity
(\cref{subsection:best_answer}), misaligns with LLM use cases
(\cref{subsection:use_cases}), and poorly tests knowledge
(\cref{subsection:testing_what}).
Drawing from education, we advocate two \textit{generative}
improvements to \mcqa's format for future exploration: 1) providing
short, constructed-response answers without using choices
(\cref{subsection:constructed_response}); and 2) evaluating
explanations for model answers (\cref{subsection:justified_mcqa}).
%
These formats capture generation or subjectivity, match LLM use cases,
and improve knowledge testing, all while mostly preserving \mcqa's
simple scoring.


Next, we argue even when \mcqa is a useful format, its datasets suffer from: dataset leakage
(\cref{subsection:test_set_leakage}), unanswerable \mcq{}s~(\cref{subsection:quality}), shortcuts (\cref{subsection:artifacts}),
and saturation (\cref{subsection:saturation}), degrading \mcqa's utility.
%
To enhance \abr{nlp} dataset design, we offer solutions for each issue based on best practices in human
testing, like rubrics to flag \mcq errors (\cref{subsection:quality}),
metrics~to curb guessing from shortcuts
(\cref{subsubsection:shortcut_scoring}), and Item Response Theory
\cite{baker2001basics} to cull shoddy MCQs and make the ones left
more challenging~(\cref{subsection:irt}).

Lastly, we show many errors of LLMs in \mcqa directly relate to
\mcqa's flaws (\cref{section:models}).
%
These issues, like brittleness to perturbations
(\cref{subsection:robustness}), bias toward certain options, cultures,
and languages (\cref{subsection:bias}), and generating
unfaithful explanations (\cref{subsection:explanations}), can all be
better measured or addressed with our proposed improvements to \mcqa's
format and datasets.


Many promising improvements to \mcqa draw from education, a field dedicated to effective assessment
\cite{haladyna2002review}, but these practices are rarely used in \abr{nlp}.
Adopting them demands more effort---\mcqa{} is popular
as it seems simple---but this effort is worth it to improve
evaluations.
%
To encourage researchers to take on these challenges, we conclude with
guidelines for designing meaningful evaluations whether or not you use
\mcqa{}~(\cref{section:conclusion}).

