\section{Limitations} \label{section:limitations}

Inspired by \citet{saxon2024benchmarks}, we organize our limitations section as potential counterarguments:



\paragraph{I Don't Work on \mcqa:}

Our approach to \mcqa can apply to all tasks;
it is important to question if your format effectively evaluates your intended ability.
Educators have long-studied the best formats for different tasks, but we are not using these insights to guide our benchmark design.
As an example beyond \mcqa, in human math assessments, students are often required to ``show their work'' to verify understanding and diagnose misconceptions \cite{choy2016snapshots}. However, math datasets like GSM-8k \cite{cobbe2021training} often ignore intermediate computations in their metrics.
Similarly, our dataset quality issues are universal; it is always important to ensure datasets are not contaminated or saturated, as well as free from errors and shortcuts.
Thus, we advise all researchers---regardless of task or domain---to consult education work to see if they can improve their evaluations' efficacy.

\paragraph{Other modalities, languages, etc. are different:}
Our critiques of \mcqa's rigid goals (\cref{subsection:best_answer}), misalignment with user needs (\cref{subsection:use_cases}), and failure to fully test knowledge (\cref{subsection:testing_what}), along with our proposed generative task alternatives (\cref{section:fixing_format}), are applicable regardless of modality and language.
Further, the \mcqa dataset quality concerns (\cref{section:dataset}) we discuss are still relevant to these domains;
for example, in some multi-modal QA datasets, models can answer questions without using the input image \cite{goyal2017making}, indicating shortcuts exist (\cref{subsection:artifacts}).

\paragraph{Why not abandon \mcqa?}

Indeed, other formats have grown in popularity, such as prompting LLMs on typical user queries and using annotators or models to judge model responses \cite{chiang2024chatbot, lin2024wildbench}.
These efforts are exciting and can directly proxy LLM use cases, but are difficult to scale for every domain we currently use \mcqa for, their subjective scoring lacks reproducibility, and these metrics are easy to game \cite{zheng2025cheating}.
In contrast, \mcqa and our proposed generative formats include scoring using ``pick the best answer'', forming a more efficient and objective metric.
Thus, we should aim to advance both of these threads for more reliable evaluations.

\paragraph{This is Way Too Much Work:}

We have proposed many directions for future research, but our objective is not to have every \mcqa dataset designer engage with each of these efforts.
We hope that by pointing out these issues in \mcqa, dataset designers will start to consider how using \mcqa will affect their datasets' reliability in the long term, and researchers will further study ways to improve \mcqa evaluation.
Even adopting just one of our proposals could greatly enhance the quality and effectiveness of \mcqa datasets.
Over time, these small, incremental improvements across the evaluation community will drive meaningful progress.

\section{Ethical Considerations}

Flawed evaluations can mislead both researchers and users; researchers may misinterpret model abilities due to quality issues in datasets, while users may struggle to identify the best models for their needs.
This paper outlines several potential solutions to mitigate these risks in \mcqa, ensuring more reliable evaluations for researchers and users.

%Overall, our findings provide broader insights to improve general evaluation practices beyond text-only \mcqa.

\section*{Acknowledgments}

We would like to thank the \abr{clip} lab at the University of Maryland and our external collaborators for their feedback.
In particular, we appreciate Dang Nguyen, Paiheng Xu, and Shi Feng for general discussions and feedback; Yu Hou, Dayeon Ki, and Connor Baumler for discussions of biases; Maharshi Gor for feedback on IRT; Yoo Yeon Sung for feedback on IRT and adversarial dataset collection; Matthew Shu for feedback on best practices in education; Atrey Desai for helping us implement MCQ checklist evaluation; and Naina Balepur for helping us brainstorm our excellent title.
We also sincerely appreciate Michael Saxon for feedback on our entire paper.
This material is based upon work supported by the National Science Foundation under Grant No. \abr{iis}-2403436 (Boyd-Graber), \abr{iis}-2339746 (Rudinger), and \abr{dge}-2236417 (Balepur).
Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.

% Hope, Zoey, Connor (Bias)
% Maharshi (IRT)
% YY (Adversarial, IRT)
% Maharshi (IRT)
% Naina (Title)
% Matthew (Education)
% Michael (full review)
% Andrew Lam
% Yi Ting