\subsection{LLMs Inevitably Ace \mcqa Datasets} \label{subsection:saturation}

Even if we fix all of these issues, MCQs become too easy over
time (i.e. saturated), no longer tracking LLM progress
\cite{li2024crowdsourced}.
%
To still use ``easy'' \mcqa datasets, we need to
make them harder for models.
%
% \jbgcomment{this sentence could be more direct: ``propose'' and ``study'' are filler words in academic writing}
Below, we show how understanding which MCQs
are hard (\cref{subsection:irt}) and helping users author hard, interpretable MCQs (\cref{subsection:hitl}) delay saturation.


\input{figures/contrast}

\subsubsection{IRT Reveals Challenging MCQs} \label{subsection:irt}

When LLMs excel in \mcqa datasets, some MCQs remain hard; finding
them and why they are hard informs data design
\cite{sugawara2018makes, Sugawara2022WhatMR}.
One MCQ difficulty metric is success rate (\success)---the number of models
answering correctly \cite{gupta2024improving}---but \success omits
\textit{which} models succeed.
MCQs solved by just the worst or best model have equally low success rates, but the former suggests MCQ errors (\cref{subsection:quality})---as a weaker model besting all others is rare---while the latter matches our expectation. 
As \success conflates these cases, it cannot separate flawed MCQs from those discerning model~ability.

% \jbgcomment{I think the better argument that IRT is better than SR is
%   that bad questions can have a low success rate: you want to
%   eliminate questions with errors, ambiguities, etc.  And leave the
%   discriminative questions.  You already introduced why you want
%   discriminative questions, so you can just back point to that.  I'd
%   talk more about the failure modes of SR, which can highlight how the
%   opposite of those are things IRT can find.}

Item Response Theory (IRT), a tool used in education \cite{lord2008statistical}, is a stronger way to find harder MCQs.
%
While success rate treats all models equally, IRT learns the skill of all models which then estimates each MCQ's difficulty (how hard it is) and discriminability (how well it discerns between weak/strong models).
IRT can then~filter high-difficulty and high-discriminability MCQs as harder subsets \cite{polo2024tinybenchmarks}, detect saturation if all MCQs have low difficulty and discriminability \cite{vania2021comparing}, and omit erroneous MCQs with negative discriminability \cite{rodriguez-etal-2021-evaluation}.

% rodriguez-etal-2021-evaluation

While IRT-based filtering finds harder MCQs, it does not give new MCQs, limiting its long-term use.
However, we can extend IRT to multi-dimensional IRT \cite[MIRT]{reckase200618} to capture \textit{many} latent skills, offering more insights into model abilities; by interpreting these skill dimensions, we can pinpoint model issues that inform future data efforts (\cref{subsection:hitl}).
\citet{gor2024great} reveal LLM errors in abductive reasoning via a variant of MIRT---an issue confirmed by abduction research \cite{del-fishel-2023-true, DBLP:conf/iclp/NguyenGTSS23}.
MIRT could similarly find difficult \mcqa topics, distractor patterns, or reasoning types for models \cite{benedetto-etal-2021-application}.

% \cite{balepur2024reverse}

% \jbgcomment{Density of self-cites a little high here (for the group, not just you)}

Overall, IRT can find which MCQs are hard but also why, informing future data collection efforts.

% \jbgcomment{Section title doesn't cover adversarial questions that well.  Perhaps a transition of the form: while engaged authors are used to understanding what trips up humans, they may need interface / computational support to trip up models\dots

% That could transition to adversarial data.}

\input{figures/adv}

\subsubsection{A Good MCQ is Hard \textit{and} Interpretable} \label{subsection:hitl}

A popular way to write harder MCQs is requiring obscure knowledge
(Figure~\ref{fig:adv}, left), sourcing from experts
\cite{rein2023gpqa, phan2025hle} and global competitions
\cite{fang2024mathodyssey}.
%
These challenge models \textit{and} humans, which is useful for \abr{ai} safety work in scalable oversight \cite{bowman2022measuring}.
However, this makes them uninterpretable for non-experts diagnosing model errors, especially when studying model rationales (\cref{subsection:explanations}).
If LLMs~err on obscure MCQs, it is hard for non-experts to find if errors are from faulty reasoning, misunderstandings, or knowledge gaps \cite{anderson2025phdknowledgerequiredreasoning}.

While authors know which MCQs elude humans, writing ones that surface model errors while staying human-interpretable needs support.
This is \textbf{adversarial} data collection's goal \cite{kiela-etal-2021-dynabench}---building UIs to help authors write examples hard for models but easy for humans (Figure~\ref{fig:adv}, right).
Rather than using niche knowledge, authors must write MCQs with spurious patterns, misleading distractors (\cref{subsection:quality}), or reasoning traps that trick LLMs but not humans \cite{xu2023llm}.
As a result, these MCQs better expose robustness (\cref{subsection:robustness}) and logical reasoning (\cref{subsection:explanations}) errors, less clouded by knowledge.

Adversarial MCQs are useful, but finding users to write high-quality ones is tough.
Gamification---making the task fun---helps, shown by successful adversarial commonsense \cite{talmor2022commonsenseqa}, QA \cite{wallace2019trick}, and fact-checking \cite{eisenschlos2021fool} datasets.
\citet{wallace-etal-2022-analyzing} use gamification to build adversarial NLI data and show it has \textit{long-term} difficulty, delaying saturation. 
% \citet{sung2024your} recently use gamification with IRT (\cref{subsection:irt}) to build an adversarial QA dataset with the highest gap in human and model accuracy in the past five years.
For even more engagement, researchers can stir users to author MCQs exposing shocking failure cases, inspired by jailbreaking, where provocative outputs are naturally fun to elicit \cite{10.1145/3656156.3665432}.
%\cite{schulhoff2023ignore}.

% \jbgcomment{DADC would be better for submission (the barrage of self-cites can return for camera ready, so don't delete, jsut comment)}

Obscure and adversarial MCQs can both make \mcqa harder: the former
tests niche knowledge, while the latter better finds reasoning or
consistency failures that are uneclipsed by knowledge gaps.
