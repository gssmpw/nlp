
% \jbgcomment{I think it's better for filenames to be named so that they sort correctly}

\subsection{LLMs Peek at \mcqa Answer Keys} \label{subsection:test_set_leakage}

To build an \mcqa dataset, we first need sources to write or collect
MCQs.
%
But as many sources end up being leaked\footnote{gpt-3 has seen
45\% of RACE's test set \cite{sainz2023nlp}.} in LLM training data
\cite{magar2022data}, such MCQs may confuse~generalization abilities for
memorization \cite{lewis2020question}.
%
Private test sets \cite{sap2019socialiqa} and de-contamination
\cite{zhou2023don} help, but LLMs tuned on newer data can overlap with
(1), and opacity in LLM data \cite{soldaini2024dolma} blocks (2).
% \jbgcomment{Patrick Lewis's paper on NQ overlap is probably worth citing here}

An ambitious solution to test set leakage is live MCQs that update over time to stay unseen \cite{white2024livebench}, like how educators rewrite exams to limit cheating.
In fact, trivia \cite{jennings2007brainiac}~and standardized testing groups often write new questions, making them ideal partners.
To aid both~parties, researchers could offer these groups tools for tutoring \cite{li-etal-2024-eden}, MCQ validation \cite{yu-etal-2024-automating-true}, or answer scoring \cite{yang-etal-2020-enhancing}.

Test set leakage would be easier to fix if model designers released
training data, but as we all know, most do not.
%
While unfair researchers must aid this effort, we hope they view it as
a difficult, impactful research problem in evaluation and
generalization.
