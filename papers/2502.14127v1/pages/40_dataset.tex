\section{\mcqa Datasets are Flawed but Fixable} \label{section:dataset}

\mcqa is not always the best format (\cref{section:format}), but we still need high-quality MCQs for comprehension/validation, along with tasks like LLM-as-a-judge \cite{gu2024olmes} and re-ranking \cite{ma-etal-2023-large} which require comparing answers.
Also, our generative \mcqa formats (\cref{section:fixing_format}) still use MCQs as inputs.

However, like most \abr{nlp} tasks, \mcqa datasets have quality issues that impede their utility: leakage
(\cref{subsection:test_set_leakage}), unanswerability
(\cref{subsection:quality}), shortcuts (\cref{subsection:artifacts}),
and saturation (\cref{subsection:saturation}).
We show how educators'~solutions to these issues can guide \abr{nlp} dataset design.



\input{pages/41_leakage}


\input{pages/42_unanswerable}

\input{pages/43_shortcuts}

\input{pages/44_saturation}
