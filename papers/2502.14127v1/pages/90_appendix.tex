\section{Appendix}

\subsection{Initial Paper Selection Process} \label{appendix:paper_selection}

To identify relevant papers for our initial reading list, we follow PRISMA \cite{page2021prisma}, a systematic methodology for paper review.
We start by curating 25 keywords related to \mcqa evaluation:

\begin{itemize}[noitemsep, topsep=0pt]
    \item multiplechoice
    \item multiple-choice
    \item multiplechoicequestionanswering
    \item multiple-choice question-answering
    \item multiple choice
    \item multiplce choice question answering
    \item multiple choice evaluation
    \item multiple choice benchmarks
    \item multiple choice benchmarking
    \item multiple choice reasoning
    \item multiple choice limitations
    \item multiple choice weaknesses
    \item multiple choice issues
    \item multiple choice large language models
    \item multiple choice llms
    \item mcqa
    \item mcqa evaluation
    \item mcqa benchmarks
    \item mcqa benchmarking
    \item mcqa reasoning
    \item mcqa large language models
    \item mcqa llms
    \item mcqa limitations
    \item mcqa weaknesses
    \item mcqa issues
\end{itemize}


We use these keywords to search ArXiv, Semantic Scholar, and ACL Anthology, resulting in 1476 total papers and 1250 unique papers.
To help automate the filtering process, we follow \citet{schulhoff2024prompt} and use \texttt{gpt-4o} to classify irrelevant papers.
The LLM labels if a paper is ``highly relevant'', ``somewhat relevant'', ``neutral'', ``somewhat irrelevant'', or ``highly irrelevant'' by its abstract and title (Prompt~\ref{prompt:paper_clf}).
We only keep ``highly relevant'', ``somewhat relevant'', or ``neutral'' papers.
We validate the classifier on 200 sampled papers, achieving 92\% recall.
This filtered 42\% of papers.

Post-filtering, we manually screen the remaining 734 papers, excluding 612 studies that only introduce new MCQA benchmarks without providing new findings on model evaluation or focus exclusively on multi-modal MCQA.
While we mainly discuss text-only MCQA, many findings are also applicable to multi-modal settings (\cref{section:limitations}).
In total, we used 122 papers to form the initial reading list of this survey, which helped us form our initial arguments.
While writing our arguments, we searched for more papers to supplement each of the points we discussed, often from education research.


\subsection{Prompts for Examples} \label{appendix:prompts}

On the next page, we provide the prompts used to produce the LLM outputs for all of our figures.

\subsection{Additional Related Works}

There are several works that expose LLM issues in \mcqa (\cref{section:models}), many of which came out around the same time.
Due to space constraints, we are unable to include all of them in the main body of the paper. To ensure they are still recognized, we cite these works here.
There are several works showing LLM robustness issues in \mcqa, studying shuffling option order and formatting perturbations \cite{zong2023fool, pezeshkpour2023large, Ranaldi2023HANSAY, Zheng2023LargeLM, Li2024AnchoredAU, gupta2024changing, alzahrani2024benchmarks, long2024llms, lyu2024beyond, tsvilodub2024predictions, khatun2024study}.
Similarly, there are many works showing that LLMs provide unfaithful explanations in \mcqa \cite{agarwal2024faithfulness, kim2024can, madsen2024self, lyu2023faithful, lanham2023measuring, turpin2024language}.

\clearpage
\input{appendix/prompts}