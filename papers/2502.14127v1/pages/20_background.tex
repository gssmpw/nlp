% \jbgcomment{Worth a footnote to cover ``all of the above'' as hinted in the title?}

\section{Background: A Brief History of \mcqa} \label{section:background}

A multiple-choice question (MCQ) is a question~$q$ and set of
choices $\mathcal{C}$.\footnote{Some choices can link to many other choices (e.g. ``All of the above''), but these are discouraged \cite{haladyna2002review}.}
%
One choice $a \in \mathcal{C}$ is the~gold answer, while others
are plausible-sounding but~incorrect distractors $\mathcal{D} = \mathcal{C}\setminus\{a\}$ meant to test misunderstandings.\footnote{Extractive QA (e.g. SQuAD) and classification (e.g. NLI) can have a finite set of choices, but they are fixed (i.e. labels) or have non-misleading distractors, so these tasks are not~\mcqa.}
%
\mcqa's simple goal---picking the best answer $a$---is popular for LLM evaluation, but it has flaws.
%
Before naming them, we first review its history in
human testing (\cref{subsection:eval_humans}) and
\abr{nlp}~(\cref{subsection:eval_machines}).

%\subsection{Evaluating \textit{Humans} with \mcqa}
\subsection{Why \mcqa{} is the Standard for Humans}
\label{subsection:eval_humans}

The \mcqa format originated in 1914 with Frederick Kelley's Kansas
Silent Reading Test \cite{kelly1916kansas}, proposed as an efficient
measure of student reading comprehension \cite{monroe1917report}.
%
Soon after, \mcqa was attempted at scale, notably with Robert Yerkes’
Army Alpha and Beta tests~\cite{yerkes1918psychology} in 1917 to assess
U.S. Army intelligence.
%
An initial bottleneck in \mcqa was the manual effort required for
scoring, which researchers like Benjamin Wood and Reynold Johnson tackled by designing automated grading systems
\cite{brennan1971ibm, woodcolumbia} with \abr{ibm}.

Automatic scoring eventually enabled \mcqa's popularity in
primary/secondary education~\cite{butler2018multiple}, college
admissions \cite{daneman2001using}, language proficiency
\cite{jamieson2000toefl}, and even everyday tasks like compliance
training \cite{puhakainen2010improving} or driver's permit exams
\cite{beanland2013there}.
%
% \jbgcomment{We should be careful here, as
%   the first College Board tests were essays, and they went to full
%   \mcqa{} later.}
%
Parallel to this, education researchers began exploring the best practices for writing high-quality MCQs \cite{morrison2001writing, campbell2011write}, crafting difficult distractors \cite{pho2015distractor, gierl2017developing}, and designing test settings \cite{rakes2008open, shute2013comparison}.

Despite \mcqa's simplicity and popularity, organizations still
critically assess its use in standardized testing.  In the United
States, the SAT removes unsound MCQ
types,\footnote{https://blog.prepscholar.com/sat-analogies-and-comparisons-why-removed-what-replaced-them} and France's
Baccalauréat uses long essay tasks over
\mcqa.\footnote{https://www.education.gouv.fr/reussir-au-lycee/le-baccalaureat-general-10457} We argue
LLM evaluation needs similar scrutiny and should draw from education to
refine MCQA's format and data.


% While once popular, \mcqa's dominance in standardized testing is wavering.
% University of California no longer considers SAT/ACT scores for admissions,\footnote{https://admission.universityofcalifornia.edu/} and many graduate programs have dropped the GRE,\footnote{https://grenotrequired.com/} citing limited predictive validity.
% As institutions reconsider \mcqa evaluation, we argue similar scrutiny is needed for LLM evaluation.

\subsection{How \mcqa Became Popular for LLMs} \label{subsection:eval_machines}

\abr{nlp} first used MCQs from human exams; solving these with models that used external sources was considered part of an ``\abr{ai} grand challenge'' \cite{reddy1988foundations}, as it required semantic \cite{turney2003combining, veale2004wordnet} and factual understanding \cite{6587172, 10.1145/2509558.2509565}.
Other early MCQs such as COPA~\cite{roemmele2011choice} and Winograd \cite{levesque2012winograd} tested commonsense reasoning over events and ambiguity in premises.
Soon after, \citet{richardson2013mctest} designed MCTest for machine reading comprehension (MRC) via fiction text and MCQs.
All tasks challenged models, but most \mcqa work studied MRC \cite{lai-etal-2017-race}.

With the advent of larger, neural LMs \cite{devlin2018bert}, \mcqa needed to become harder.
Researchers expanded MRC to test numerical reasoning \cite{dua2019drop} and uncertainty \cite{rogers2020getting}, and successfully scaled existing commonsense MCQs \cite{sakaguchi2021winogrande}.
New MCQs that tested LM pre-training knowledge also grew popular, often using commonsense in daily~tasks \cite{talmor2018commonsenseqa, bisk2020piqa} or science exams \cite{mihaylov2018can, clark2018think}.

As LLMs improved in generation \cite{brown2020language}, \mcqa evaluation changed;
models usually scored \mcqa choices independently, but~\citet{robinson2023leveraging} showed prompting LLMs with the question and \textit{all} choices was easy to score and matched human testing.
It soon became standard to test LLMs with MCQs; companies used~the task to parade their models \cite{achiam2023gpt}, some equating it to intelligence \cite{anthropic2024introducing}.
This industry adoption incentivized researchers to write more MCQs across topics \cite{rein2023gpqa}, languages, and modalities \cite{zhang2023m3exam}.

% with MCQs being used to test LLMs in long-context~MRC \cite{Pang2021QuALITYQA} and understanding of various subjects \cite{hendrycks2020measuring}, languages \cite{hardalov2020exams}, and modalities \cite{yang2022avqa}.

Recent work critiques LLM evaluations in general, discussing reproducibility issues \cite{laskar-etal-2024-systematic}, how it should be a distinct discipline \cite{chang2024survey}, and its failure to predict deployment settings \cite{saxon2024benchmarks}.
%
We similarly argue that while \mcqa is simple and popular, the task has flaws in its format and datasets, many of which can be fixed using insights from education research.

% \jbgcomment{Last sentence could tie the section together better}
