\input{figures/formats}

\section{\underline{Generative} \mcqa Tasks are Promising} \label{section:fixing_format}

\mcqa is flawed, so how should its role change?
Standardized LLM evaluations must 1) proxy LLM use cases; and 2) test skills for (1).
\mcqa is unfit for (1), so~evaluations need more tasks matching LLM needs (\cref{subsection:use_cases}).
Writing/explanation tasks are hard to score \cite{charney1984validity}, but it is still odd they are often omitted, as they are typical needs~(\cref{subsection:use_cases}).

This limits \mcqa to (2), but it is best for comprehension or validating objective facts (\cref{subsection:best_answer}), not generation, subjectivity, or knowledge (\cref{subsection:testing_what}).
Validation/comprehension are valuable, which is why we discuss \mcqa datasets in \cref{section:dataset}, but we need~better formats for the other skills.
Thus, we give two generative versions of \mcqa to better test LLMs (Figure~\ref{fig:format}):
Constructed Response (\cref{subsection:constructed_response}), answering sans choices, and Explanation \mcqa (\cref{subsection:justified_mcqa}), justifying predictions.
We ensure the formats only slightly increase evaluation complexity, mostly preserving \mcqa's simplicity of scoring. 
We now~describe the formats and future work to realize them.

\subsection{Constructed Response Questions} \label{subsection:constructed_response}

Having LLMs solve MCQs without choices, called \textbf{Constructed Response (CR)} in education \cite{livingston2009constructed} or short-form QA in \abr{nlp} \cite{krishna-etal-2021-hurdles}, is one better format.
CRQs test answer \textit{generation} unlike MCQs (\cref{subsection:best_answer}), better mirroring LLM needs (\cref{subsection:use_cases}), and it is easier to write CRQs testing all skills in Bloom's Taxonomy \cite{krathwohl2002revision}, so they better expose knowledge gaps (\cref{subsection:testing_what}).
Thus, students find CRQs harder than MCQs~\cite{hancock1994cognitive}, which can also delay saturation (\cref{subsection:saturation}).

Instead of writing CRQs to replace our vast existing \mcqa data, a promising solution is to convert MCQs into CRQs by omitting choices and tasking LLMs to give a \textbf{short-form} answer $\hat{a}$ for question $q$, comparing it to gold answer $a \in \mathcal{C}$ \cite{bhakthavatsalam2021think}.
\citet{myrzakhan2024open}~show two hurdles in this: finding MCQs to convert\footnote{\;``Which of these best...'' MCQs require using all choices.} and scoring $\hat{a}$ with $a$. 
Recent efforts in flagging MCQ errors (\cref{subsection:quality}) and judging short-form answer correctness \cite{li-etal-2024-pedants} show we can realistically overcome these challenges.
Thus, we believe merging this work with best practices for creating~CRQs \cite{snow2012construct} can successfully implement the task.

% Reliable MCQ to CRQ conversion would thus require combining advancements in MCQ generation \cite{lee2024math} and automatic scoring \cite{li-etal-2024-pedants} with best practices for writing and evaluating CRQs~\cite{snow2012construct}.

\subsection{Explanation Multiple Choice Questions} \label{subsection:justified_mcqa}

Constructed Response is promising (\cref{subsection:constructed_response}), but using one short answer is unfit for subjectivity \cite{lin2020differentiable} and conflicts user preferences for long outputs \cite{zheng2024judging}.
We thus propose~\textbf{Explanation \mcqa (\emcqa)} as another \mcqa alternative from education \cite{lau2011guessing}: for a question $q$ and choices $\mathcal{C}$, models give an answer $\hat{a} \in \mathcal{C}$ and explanation $\mathcal{E}$ for why $\hat{a}$ is right.
This format tests generation (\cref{subsection:best_answer}), matches the use case of explanations (\cref{subsection:use_cases}), and has shown to test more knowledge levels over \mcqa \cite{lee2011validating}.

%This format is also supported by education research, as asking for justifications can expose student knowledge gaps and heighten task difficulty \cite{tamir1990justifying, koretsky2016written}.

We envision \emcqa being treated like reasoning tasks \cite{cobbe2021training}, checking if LLMs pick $a$ like \mcqa's simple scoring, but also studying $\mathcal{E}$.
If models select $a$ but justify it poorly, it exposes knowledge gaps like in student assessments \cite{jonassen2010arguing}, and when models give strong explanations for wrong answers, it enables partial credit for subjective tasks \cite{lau2011guessing}.

\emcqa has many benefits, but needs metrics to score ``good'' explanations over many facets \cite{xu2023critical}, like factuality to curb hallucinations \cite{min2023factscore}, plausibility for convincingness \cite{liu2023vera}, and faithfulness to verify $\mathcal{E}$ supports $a$ \cite{lanham2023measuring}.
We believe these goals could be achieved by combining ongoing efforts in building verifiers for LLM reasoning \cite{NEURIPS2023_72393bd4} with educational best practices for grading justifications \cite{jonassen2010arguing}, yielding reliable metrics that realize \emcqa's potential.

% \subsection{So... How Should I Evaluate My LLM?} \label{subsection:choosing_task_formats}

% As we have flagged many issues with \mcqa, \textit{how should we evaluate our LLMs?}
% For specific use cases, the ideal approach is to curate tasks that closely mirror deployment settings \cite{saxon2024benchmarks}.
% However, we often need general-purpose evaluations, tradeoffs arise between alignment with real-world use cases and evaluation cost and scalability. Human or LLM-based ratings align closely with downstream tasks but are costly and subjective. In contrast, \mcqa offers efficient and objective scoring but sacrifices generative alignment and real-world applicability. Our proposed alternatives, Constructed Response Questions and Justified \mcqa, offer a middle ground: they retain \mcqa’s simplicity and scalability while better reflecting LLMs’ generative capabilities. These formats could serve as practical, scalable solutions for evaluating general model capabilities and bridging gaps in existing benchmarks.

% We also don't mean to use our proposed formats to eliminate \mcqa.
% The ability of models to discriminate and pick high-quality answers via \mcqa formats is still valid and worth testing, especially as LLMs become more practically used for things like LLM-as-a-judge and (something else).
% This is why, in the next section, we discuss ways to improve current \mcqa datasets, which can also improve our alternative task formats that rely on \mcqa data. Everyone wins





% Testing how LLM calibration in MCQA impacts human confidence \cite{steyvers2024calibration}

% Conformal prediction to assess confidence \cite{kumar2023conformal}

% Answer-level calibration improves MCQA reliability and finds some data biases \cite{kumar2022answer}

% Uncertainty datasets/benchmark \cite{raina-gales-2022-answer, wang2024ubench}

% Prompt engineering can improve calibration \cite{ma2023prompt}
