\section{\mcqa is Flawed as a Standard Format} \label{section:format}

\mcqa's format is simple for student assessment, but educators find
tradeoffs: \mcqa may not fully capture learning~\cite{simkin2005multiple} or student success~\cite{moneta2017limitations}.
%
This section uses these insights to argue \mcqa should not be the standard format to evaluate LLMs, showing its rigid goal
(\cref{subsection:best_answer}), failure to capture LLM use
cases (\cref{subsection:use_cases}), and limited knowledge testing~(\cref{subsection:testing_what}).

\input{figures/subjective}

\subsection{``\underline{Pick} the \textit{Best} Answer'' is Too Rigid} \label{subsection:best_answer}

One of \mcqa's key issues is its rigid goal: pick the best answer from a set of choices.
While easy to score, both designs---the use of
1) one gold answer; and 2) input choices---limit \mcqa's applicability.

First, one gold answer hinders \mcqa's use for subjectivity
\cite{finetti1965methods}.
Still, we use MCQs for commonsense \cite{bisk2020piqa}, morals \cite{yu-etal-2024-cmoraleval}, and culture (\cref{subsection:bias}), where many~choices~can be subjectively right (Fig~\ref{fig:subjective}).
\citet{Palta2024PlausiblyPQ} find users rate distractors in commonsense MCQs as~the most plausible choice >20\% of times.
Thus, extra care is needed to write MCQs for subjective tasks.

Second, picking from choices means MCQs test \textit{validation}, useful in tasks
like LLM-as-a-judge or re-ranking which must compare answers
\cite{gu2024survey}, but inhibiting tasks like writing and coding that
require \textit{generation} \cite{celikyilmaz2020evaluation}.
%
One may argue \mcqa proxies generation (if you pick good answers, you generate good ones), but LLMs~lack validation/generation consistency
\cite{li2024benchmarking, west2023generative}.
%
Validation/generation are separate skills, so \mcqa is poor for
testing generation.

In all, \mcqa best tests LLMs in objective validation, struggling with subjectivity and generation.

\subsection{Users Rarely Ask LLMs to Solve MCQs} \label{subsection:use_cases}

Many leaderboards aim to rank LLMs by their overall abilities, helping users select the best model for their needs \cite{xia2024top}.
Hence, they should adopt tasks that mirror the popularity of user needs.

\mcqa is over-represented versus how LLMs are used; 32\% of the tasks in HELM
\cite{perlitz2023efficient}, 71\% in GPT-4's card \cite{achiam2023gpt}, and 79\% in OpenLLM (Big Bench has 21 \mcqa tasks) \cite{open-llm-leaderboard-v2} are \mcqa.
%
In contrast, \citet{ouyang2023shifted} find in ShareGPT's dataset of ChatGPT queries that nearly all user queries
ask for free-form text; we estimate just 7.2\% are validation (4.3\% evaluation and 2.9\% comparison).
%
Similarly, WildChat notes just 6.3\% of their LLM queries are factual QA \cite{zhao2024wildchat}.\footnote{It is not explicitly stated if these are even validation tasks, so this is another \textit{upper} bound of validation task prevalence.}
Thus, >90\% of queries are likely generative tasks (code, writing, or explanations), which MCQs struggle to test~(\cref{subsection:best_answer}).

% % \jbgcomment{I haven't checked this,
%   but if I remember it correctly, it is predictive, just less
%   predictive than other features.  This is still useful (if I'm
%   remembering correctly) for the story: grad admissions got rid of
%   multiple choice but kept the generative parts.}

Informative evaluation suites must reflect LLM use cases.
%
This is precisely why \mcqa exams are waning in graduate admissions
criteria: they cannot fully predict graduate school
success~\cite{sampson2001gre}. 
%
Similarly, over-representing \mcqa in evaluations obscures which LLMs best aid users.

\subsection{\mcqa Does Not Fully Test Knowledge} \label{subsection:testing_what}

While \mcqa fails to match user needs, we assume the format may test basic skills for such needs, justifying its usage.
%
\mcqa is meant to test knowledge \cite{moss2001multiple}, and with input texts, comprehension \cite{farr1990description}.
%
However, research in education reveals \mcqa may be suboptimal for these goals.

 MCQs mainly assess the basic knowledge levels in Bloom's Taxonomy of educational goals \cite{krathwohl2002revision}: recalling, understanding, and applying knowledge \cite{simkin2005multiple, shin-etal-2024-generation};
it is hard to write MCQs for the higher levels requiring reasoning \cite{stupans2006multiple, palmer2007assessment, lin2012can}: analyzing, evaluating, and creating knowledge.
As evidence, students can solve MCQs without full understanding, exposed in free-response answers \cite{mckenna2019multiple}.
%
MCQs with passages generally test comprehension, but some doubt this;
\citet{ozuru2013comparing} find \mcqa scores correlate with prior knowledge of the passage, overestimating true comprehension.

% In all, \mcqa may reliably test students' comprehension, but not full knowledge assessments.
We believe these same insights can apply to \abr{nlp}: \mcqa may be apt for
comprehension, but rewards LLMs for basic recall versus in-depth
knowledge.
