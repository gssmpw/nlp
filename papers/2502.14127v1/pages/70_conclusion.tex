\section{Call to Action: Benchmarking 101} \label{section:conclusion}

% \jbgcomment{I think this ``call to action'' could be a little stronger and more specific.}

If you want to make the best benchmark ever, where do you begin?
First, define the ability you want to test and decide if \mcqa is the right format (\cref{section:format}).
If the ability matches a downstream task (e.g. coding), just use that task \cite{saxon2024benchmarks}.
If the ability is fundamental (e.g. knowledge), consult education research to weigh any alternative formats (e.g. \cref{section:fixing_format}).

If \mcqa is the best format, find a data source~to curb leakage---one with fresh content (\cref{subsection:test_set_leakage}).
When curating MCQs from your source,~follow educators' rubrics to ensure answerability (\cref{subsection:quality}), and release the rubric as a data card to record errors \cite{pushkarna2022data}.
Consistent design choices will limit shortcuts (\cref{subsection:artifacts}), and providing a contrast set could help researchers check if their models over-rely on shortcuts \cite{Gardner2020EvaluatingML}.
As another safeguard, your benchmark can use calibration scoring beyond accuracy to discourage guessing~(\cref{subsubsection:shortcut_scoring}).

Post-release, models will hill-climb and saturate your data over time (\cref{subsection:saturation}).
If you want to delay~saturation, you may restart with an obscure knowledge source, but if you want your data to better diagnose errors, aim for interpretability.
Use IRT (\cref{subsection:irt}) to find which of your MCQs are hard and why, then design an engaging, adversarial dataset collection protocol (\cref{subsection:hitl}) guided by these insights, yielding a new dataset hard for models but easy for humans.

By using even some of educators' insights, we can refine the utility of \mcqa---or any task.
This approach takes more effort than the simple \mcqa practices that initially attracted researchers, but if we do not address the flaws of \mcqa, \textbf{what~model abilities can our \mcqa benchmarks even test?}
