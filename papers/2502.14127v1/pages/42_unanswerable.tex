\subsection{Some MCQs Have No Correct Answer} \label{subsection:quality}

Once a source is found (\cref{subsection:test_set_leakage}), researchers collect~or write MCQs, but errors often arise rendering them unanswerable, like mislabeling \cite{explained2023smart}, multiple correct choices \cite{Palta2024PlausiblyPQ}, ambiguity \cite{gema2024we}, missing contexts \cite{wang2024mmlu}, and grammar errors \cite{chen2023hellaswag}.

Educators write MCQs with rigorous protocols, and we must meet similar standards in \abr{nlp} \cite{boyd2019question}; we should use educators' rubrics (Figure~\ref{fig:checklist}) for writing and validating MCQs \cite{haladyna1989taxonomy}.
Such guidelines also specifically exist for distractors \cite{haladyna2002review}---the part of MCQs that discern testees' skills---ensuring they are truly wrong, shortcut-proof (\cref{subsection:artifacts}), and not too easy to rule out (\cref{subsection:saturation}).
Beyond MCQ writing, rubrics can form~data cards \cite{pushkarna2022data} to help researchers record errors in their data and how~they~fixed them.

Recent work in LLM checklist evaluation \cite{cook2024ticking}, 
MCQ metrics \cite{moon2022evaluating},~and MCQ generation \cite{sileo2023generating} show parts~of this may be
automatable (Figure~\ref{fig:checklist}).
For instance, \citet{wang2024mmlu} fix errors in MMLU by using LLMs to detect issues and
write new choices.~LLM judges are not always reliable \cite{xu-etal-2024-pride}, so
human-\abr{ai} collaboration, like model-assisted
refinement~\cite{shankar2024validates} and task
routing~\cite{miranda2024hybrid}, may be more promising.
%
Errors will arise in MCQ writing, but educators' rubrics can help
find and fix them, ensuring answerability.

\input{figures/checklist}