\subsection{Implementation Details}
% We use RAFT-Stereo \cite{lipson2021raft} to compute depth from the Trisect underwater stereo camera sensor. 
We treat the depth noise as static across frames and fix it as $\sigma_{uv}^2 = 0.001$ m.
We downsample input images by a factor of 4 to improve the run time of the uncertainty computation.
We use GPyTorch \cite{gardner2018gpytorch} to train the SVGP with a learning rate of $1e-3$ and $500$ inducing points over 100 epochs.
We use TS-Grasp \cite{player_real-time_2023} for the grasp selection algorithm in real-world and simulation experiments.
Throughout all experiments, we set the weight factor hyperparameter used in Eq. \eqref{eq:grasp_weighing} to $\nu = 5$.

The depth measurements at each pose are converted to point clouds to construct a fused mesh at 3 mm resolution using voxblox \cite{oleynikova2017voxblox}.
The output mesh is sampled to obtain a fused pointcloud, filtered by removing statistical outliers outside a threshold standard deviation ratio of $\sigma_\text{thresh}=0.01$, using Open3D \cite{Zhou2018}.
The filtered pointcloud is input into TSGrasp \cite{player_real-time_2023} and PUGS to obtain the final grasp weight confidences.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/sideways_kettlebell_new.pdf}
    \caption{Selected results from simulation experiments with partial reconstructions of the kettlebell (first row) and noisy partial reconstructions of the coffee mug (second row). The first column shows the 3D object; the second column shows the grasps proposed by TSGrasp \cite{player_real-time_2023}, and the third column shows the grasps after the adjustment made by PUGS.
    The first row shows the partially reconstructed kettlebell and the ability of PUGS to recover from an incorrect grasp pose prediction where TSGrasp attempts to grasp from the edge of the partial reconstruction and PUGS leads to a more reliable grasp.
    The second row shows a noisy reconstruction of the coffee mug, and a corrected grasp prediction by PUGS.}
    \label{fig:sideways_fail}
\end{figure}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.85\textwidth]{figures/alternative_qualitative_results.pdf}
    \caption{Results from data collected from the test tank. 
    The columns represent separate logs collected in the test tank.
    Each log contains images of partial views from different angles of the kettlebell.
    The first and second columns are from the same log, but the first column shows the pointcloud without the aggressive filtering.
    Here, we show a qualitative comparison of the proposed grasp poses from PUGS (green) and TSGrasp (red) \cite{player_real-time_2023}.
    The reconstructions are colored by the weighted confidence output of PUGS and overlaid on a representative 3D model of the object for easy visualization.
    Higher confidence regions are colored in red, and lower confidence regions are colored in blue.
    % It is worth noting that it is not the exact model and occlusion of the reconstruction occurs especially for the \textbf{\texttt{Front}} and \textbf{\texttt{Sideways} }experiments.
    }
    \label{fig:qualitative_results}
    \vspace{-4mm}
\end{figure*}


\subsection{Simulation Setup}
% We validate and collect quantitative data of the proposed method in a simulated environment.
We run experiments on a kettlebell and a coffee mug, shown in Fig. \ref{fig:qualitative_results}. 
For the simulation, we take a 3D model of the object and simulate object-centric camera poses.
We use Blender \cite{blender} to obtain the RGB and depth images.
We mask out the object in the rendered images to ensure that the grasp detection reasons only over the object.
% The segmentation mask for the object is applied by masking out the rays that intersect with the object.

Each RGB and depth image, camera pose, and mask are saved into a \texttt{rosbag} \cite{ros}, which we use to play back the simulated setup in a Gazebo \cite{gazebo} environment containing a Bravo manipulator \cite{noauthor_underwater_nodate} model.

We evaluate three types of reconstruction for each object: complete, partial, and noisy partial.
We set the depth noise for the noisy reconstruction to $\sigma_{uv}^2=0.01$ m and $\sigma_{uv}^2=0.001$ m for the other experiments.
When running the experiments, we note that the grasp pose proposals from PUGS and TSGrasp \cite{player_real-time_2023} are not filtered based on collision with the surrounding simulation environment, as the points input into the models are only points on the object.
To mitigate this, we transform the object's pose to make the grasp collision-free and kinematically feasible. 
If neither of these is achievable, the experiment is repeated.

\subsection{Real World Setup}
Real-world testing uses the underWater Arm-Vehicle Emulator (WAVE) at UW-APL \cite{rosette_wave_nodate}.  
It includes a Reach Robotics Bravo 7 manipulator \cite{noauthor_underwater_nodate} and a Trisect tri-focal underwater stereo sensor \cite{trisect_web_site} on a four-degree-of-freedom moving platform.
We test four scenarios of the kettlebell shown on right side in Fig. \ref{fig:hh101}, which also shows the test setup.
The platform was swept through various trajectories to allow the collection of multiple views of the target object.
We collect images of the kettlebell in challenging perceptual and grasping configurations: tilted towards the camera, lying flat on the ground, facing forward, and rotated sideways.
Images from the tests are shown on the bottom row of Fig. \ref{fig:qualitative_results}. 

We use RAFT-Stereo \cite{lipson2021raft} to compute depth from the stereo camera sensor and a fine-tuned Detectron2 \cite{wu2019detectron2} model to produce segmentation masks of the object.
The depth images and the left image from the stereo images are input into RTABMap \cite{labbe_rtabmap_2019} to estimate camera poses.
RTABMap produces poses with covariance estimates only at keyframes.
To increase the density of the reconstruction, we additionally use the poses between the keyframes and take the covariance from the most recent keyframe pose to use as the covariance for the pose estimates.