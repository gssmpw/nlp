The complete pipeline for PUGS is shown in Fig. \ref{fig:overview}.
We take in rectified stereo image pairs as input and estimate camera pose and depth measurements within our SLAM pipeline.
We quantify the uncertainties associated with depth and pose estimates, and integrate them to compute occupancy uncertainty. % given measurements.
We use occupancy uncertainty as a weighting factor for confidence output in grasp selection.

% In this work, we propose a framework to use perceptual uncertainty inherent in multi-view stereo methods for improving underwater grasp selection.
% We present a formulation for constructing an occupancy field informed from the uncertainty inherent in sensor measurements and pose estimated (Sec. \ref{ss:observational_unc}), which we call the observational uncertainty.
% We then use this occupancy field as a prior to determine the variance associated with occupancy (Sec. \ref{ss:pred_unc}), which we call the predictive uncertainty.
% We finally show how the predictive and observational uncertainty can be fused (Sec. \ref{ss:unc_fusion}) and be used for grasp selection (Sec. \ref{ss:grasp_selection})

% we study uncertainty representation in 3D reconstruction, specifically how uncertainty modeling from multi-view stereo can be used for downstream robotic tasks such as grasp selection. 
% For this, we first consider the uncertainties that can be modeled from multi-view stereo, how they can be formulated and integrated into a 3D reconstruction method, and how they are later used to weight grasp points in various domains, primarily underwater manipulation tasks.

We consider a scenario in which we have an object of interest to grasp and a set of camera views in which this object is visible. 
Going forward, we assume we have the following systems: a vision-based SLAM system for pose estimation~\cite{labbe_rtabmap_2019}, a semantic segmentation model for segmenting the object of interest in the image~\cite{wu2019detectron2}, and depth estimates computed from the calibrated stereo rig~\cite{lipson2021raft}.
We describe how, given the measurements and systems in place, an uncertainty representation for grasping can be formulated and used for reliable robotic manipulation in adversarial environments such as underwater environments.

\subsection{Multi-view Uncertainty in 3D Representations}
We aim to model the uncertainty associated with occupancy based on observations from a multi-view camera system.
For this, we consider two primary modes of uncertainty that play a role in reconstruction and grasp selection: observational uncertainty and predictive uncertainty. 

\subsubsection{Notation and Formal Definitions}
We use bold variables $\boldsymbol{x}$ to represent a vector, and non-bolded variables $x$ to represent scalar values.
Random variables and vectors are notated as $\widehat{x}$.

From the given set-up, we compute depth images $\mathbf{D} = \{\widehat{D}^i\}_{i=1}^N$ from each stereo image pair. 
We make the modeling assumption that each pixel $\widehat{d}_{\text{uv}}^i$ in the depth image $\widehat{D}^i$ can be modeled as a Gaussian random variable $\widehat{d}_{\text{uv}}^i \sim \mathcal{N}(\mu_\text{uv}^{(i)}, \sigma^{2(i)}_\text{uv})$, where $\mu_\text{uv}^{(i)}$ is the depth measurement at pixel $(u,v)$ for frame $i$ and $\sigma^{2(i)}_{\text{uv}}$ is the associated measurement noise.
The noise $\sigma^{2(i)}_{\text{uv}}$ can be obtained from either the sensor specifications or the model used to compute the depth from the stereo images. 

From the vSLAM system, we obtain a set of poses $\mathbf{C} = \{\widehat{C}^i\}_{i=1}^N$ synchronized with each stereo pair RGB image and depth image.
We assume that the estimated poses are modeled as Gaussian random variables such that $\widehat{C}_i \sim \mathcal{N}(\boldsymbol{\mu}_C^i, \mathbf{\Sigma}_C^i)$, where $\boldsymbol{\mu}_C \in SE(3)$ is the estimated pose of the camera at frame $i$ and $\mathbf{\Sigma_C^i}$ is the associated pose covariance.

\subsection{Observational Uncertainty Representation}\label{ss:observational_unc}
Observational uncertainty is modeled using the measurement noise and the pose uncertainty.
Based on these estimates, these two factors are propagated into a fused occupancy field (FOF) to model areas that are likely to be occupied.

\subsubsection{Measurement Noise Propagation} \label{sss:mnp}
Since each measurement used to compute occupancy has inherent uncertainty, we aim to model how this uncertainty can be estimated and used for occupancy computation.
In our setup, we consider occupancy to be measured by the depth estimate.
We backproject pixel $(u,v)$ into the camera frame as $\widehat{\mathbf{p}}_c \sim \mathcal{N}(\boldsymbol{\mu}_{\mathbf{p}_c}, \mathbf{\Sigma}_{\mathbf{p}_c})$ using the uncertain depth measurement $\widehat{d}_{uv}^i$. The mean $\boldsymbol{\mu}_{\mathbf{p}_c} \in \mathbb{R}^3$ is the position in the camera frame and $\mathbf{\Sigma}_{\mathbf{p}_c}$ the positional covariance. 
The backprojection model is shown in Eq. \eqref{eq:unproj_mean_cov}, where $\left(f_x, f_y, c_x, c_y\right)$ are the pinhole camera intrinsic parameters.
To compute the covariance, we take the first-order linearization of the backprojection model with respect to the depth variable and propagate the covariance originating from the depth variance into the camera frame using the Jacobian $\boldsymbol{J}_d$ in Eq. \eqref{eq:jacobian}.
\begin{gather}
    \boldsymbol{\mu}_{\mathbf{p}_c} = \mu_\text{uv}^{(i)}\begin{bmatrix}\frac{u-c_x}{f_x} \\ \frac{v-c_y}{f_y} \\ 1\end{bmatrix}, \ \  
    \mathbf{\Sigma}_{\mathbf{p}_c} = \boldsymbol{J}_d \sigma_\text{uv}^{2(i)} \boldsymbol{J}_d^T \label{eq:unproj_mean_cov} \\
    \boldsymbol{J}_d = \frac{\partial{\mathbf{p}_c}}{\partial d_\text{uv}} = \begin{bmatrix}\frac{u-c_x}{f_x} & \frac{v-c_y}{f_y} & 1\end{bmatrix}^T \label{eq:jacobian}
\end{gather}
% The positional distribution $\widehat{\mathbf{p}}_c$ is incorporated into the FOF to model how noisy depth measurements contribute to uncertainty in computed occupancy.

\subsubsection{Pose Uncertainty Propagation} \label{sss:pup}
The estimated poses are inherently uncertain due to accumulating errors and the absence of absolute position measurements.
The uncertainty of poses when measurements are made contributes to uncertainty in computed occupancy.
To model this, we use a pose uncertainty propagation mechanism.

We place $\widehat{\mathbf{p}}_c$ from the camera into the world frame as $\widehat{\mathbf{p}}_w \sim \mathcal{N}(\boldsymbol{\mu}_{\mathbf{p}_w}, \mathbf{\Sigma}_{\mathbf{p}_w})$ using the pose estimate $\widehat{C}^i$ as shown Eq. \eqref{eq:cam_to_world}. 
The mean $\boldsymbol{\mu}_{\mathbf{p}_w}$ is the position in the world coordinate frame and $\mathbf{\Sigma}_{\mathbf{p}_w}$ the positional covariance.
Here, $\mathbf{\Sigma}_{C_t}^i$ is the covariance associated with the translation, $\text{R}_c^w \in SO(3)$ is the rotation and $\text{t}_c^w \in \mathbb{R}^3$ is the translation component of $\widehat{C}^i$.
\begin{gather}
    \boldsymbol{\mu}_{\mathbf{p}_w} = \text{R}_c^w \boldsymbol{\mu}_{\mathbf{p}_c} + \text{t}_c^w,\ \ \mathbf{\Sigma}_{\mathbf{p}_w} = \mathbf{\Sigma}_{\mathbf{p}_c} + \mathbf{\Sigma}_{C_t}^i \label{eq:cam_to_world}
\end{gather}

\subsubsection{Fused Occupancy Field} \label{sss:FOFc}
Through the FOF, we aim to represent the occupancy field based on the measurements and their uncertainties.
We assume that the probability distribution of $\widehat{\mathbf{p}}_w$ can be used as an informative representation of the occupancy density in the 3D space in which it exists.

For each of the $\mathbf{N}$ points in the world frame $\widehat{\mathbf{p}}_w^{(i)}$, we take a weighted sum of the probability distribution functions to obtain a Gaussian Mixture Model (GMM) representation of the occupancy field, as shown in Eq. \eqref{eq:gmm}.
This modeling choice for the occupancy field allows us to maintain a continuous and generative representation \cite{gmm_perception}, which we can then query to compute the occupancy of new measurements.
\begin{gather}
    \text{OF}(\mathbf{p}) = \frac{1}{\mathbf{N}}\sum_{j=1}^{\mathbf{N}} \mathcal{N}(\mathbf{p} \lvert \boldsymbol{\mu}_{\mathbf{p}_w}^{(j)}, \mathbf{\Sigma}_{\mathbf{p}_w}^{(j)}) \label{eq:gmm}
\end{gather}

Although Eq. \eqref{eq:gmm} is sufficient for representing the occupancy field, it neglects the capability to fuse measurements that share information.
% When we query a point, the covariance associated with its position in 3D reduces if multiple measurements of that region exist.
To address this, we add an extra measurement fusion mechanism.
For a new measurement $\mathbf{p}_z \in \mathbb{R}^3$, we find the $K$ nearest Gaussians $\widehat{\mathbf{P}}_K = \{\widehat{\mathbf{p}}_w^i\}_{i=1}^K$. 
We then perform a weighted Bayesian fusion \cite{barfoot_state_2017} to compute the resulting Gaussian distribution of $\mathbf{p}_z$, as shown in Eq. \eqref{eq:bayesian_fusion}.
Each weight for the $K$ nearby Gaussians is determined by the responsibility of Gaussian $\widehat{\mathbf{p}}_w^i$ for $\mathbf{p}_z$, shown in Eq. \eqref{eq:responsibility}, where $\pi_k=\frac{1}{K}$.
\begin{gather}
    \mathbf{\Sigma}_{\mathbf{p}_z} = \left(\sum_{k=1}^K\gamma_k(\mathbf{p}_z)\mathbf{\Sigma}_k^{-1}\right)^{-1} \label{eq:bayesian_fusion}\\
    \gamma_k(\mathbf{x}) = \frac{\pi_k \mathcal{N}(\mathbf{x} | \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)}{\sum_{j=1}^{K} \pi_j \mathcal{N}(\mathbf{x} | \boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j)},\ \ \sum_{j=1}^K\pi_j = 1 \label{eq:responsibility}
\end{gather}
When we take the query point as the mean of each unprojected Gaussian and compute the occupancy field as shown in Eq. \eqref{eq:gmm}, we end up with a fused occupancy field (FOF).
This modeling allows us to represent the positional uncertainty of a new observation given past observations and the occupancy of a region based on depth estimates.
\subsection{Predictive Uncertainty Representation} \label{ss:pred_unc}
We model the predictive uncertainty to capture phenomena such as the frequency at which a certain region has been observed.
Intuitively, the aim is to obtain a representation that yields lower variance in regions with higher observation frequencies.
To model this, we used a stochastic variational Gaussian process (SVGP) \cite{hensman_scalable_nodate} to regress on the FOF, as the FOF provides an informative representation of occupancy density.
We select SVGPs over standard Gaussian Processes as SVGPs have been shown to be capable of running real-time, dense probabilistic mapping \cite{torroba_fully-probabilistic_2022}.

By performing a probabilistic regression on the FOF, we can obtain the variance associated with the occupancy at each point in space, allowing us to quantify the uncertainty of the occupancy.

% \subsubsection{Stochastic Variational Gaussian Process Primer}
% \textbf{Prioritizing for later. The notation for later sections is} $\text{SVGP}(p) = \{\boldsymbol{\mu}_\text{pred}, \boldsymbol{\sigma}_\text{pred}^2\}$

\subsubsection{Predictive Occupancy Uncertainty Estimation}
To train the SVGP, we use the FOF, which is constructed from each of the backprojected depth points $\mathbf{P}_w = \{\mathbf{p}_w^i\}_{i=1}^{\mathbf{N}}$.
After the training has converged, we query the SVGP model with a set of $Q$ query points $\mathbf{P}_Z = \{\mathbf{p}_Z^i\}_{i=1}^Q$. 
\begin{gather}
    \text{SVGP}(\mathbf{P}_Z) = \{\mu_Z^{(i)}, \sigma_Z^{2(i)}\}_{i=1}^Q \label{eq:gp} \\
    \sigma_{\text{pred}}^2 = \frac{\sigma_Z^{2(i)}}{1 + \lvert\mu_Z^{(i)}\rvert} \label{eq:pred_uncertainty}
\end{gather}

\noindent In Eq. \eqref{eq:gp}, mean $\mu_Z^{(i)}$ represents the occupancy density of point $\mathbf{p}_Z^{(i)}$, while $\sigma_Z^{2(i)}$ represents the variance of the occuancy density.
We use the two outputs in order to obtain the final predictive uncertainty as shown in Eq. \eqref{eq:pred_uncertainty}, which scales the variance by the density of the point $\mathbf{p}_Z^{(i)}$.

We elect to follow this modeling choice as the variance of the SVGP regression is dictated by the spatial frequency on the input domain rather than the regressed function value.
Through Eq. \eqref{eq:pred_uncertainty}, we aim to mitigate the fact that predictive variance should be dictated not just by spatial frequency but also by occupancy density.


\subsection{Uncertainty Fusion}\label{ss:unc_fusion}
In Sec. \ref{ss:observational_unc}, we describe how to obtain positional uncertainty of points in the world frame from depth measurements.
The trained SVGP models the predictive occupancy uncertainty at these points.
We propose a fusion method between these two representations to create a final \textbf{occupancy uncertainty} representation.

We take inspiration from Torroba et al. \cite{torroba_fully-probabilistic_2022} in performing this fusion through cubature integration and sigma point propagation.
Cubature integration allows us to use the shape of the covariance associated with the positional uncertainty.
To perform cubature integration, we determine integration points, $\mathbf{p}_n$, and mean and variance integration weights, $\boldsymbol{w}_\mu, \boldsymbol{w}_{\sigma^2}$, respectively.
The number of integration points is determined by the dimension $d$ of the space in which integration occurs ($d=3$ for 3D), and the integration points and weights are determined using the cubature spread parameters $\left(\alpha, \beta, \kappa\right)$, as shown in Eqs. \eqref{eq:lambda}-\eqref{eq:cubature}.



\begin{gather}
 \lambda = \alpha^2\left(d + \kappa\right)-d, \ \ \hfill \boldsymbol{p}_n^{(i)} = u^{(i)}\sqrt{d+\lambda} \label{eq:lambda}\\
    u = \begin{Bmatrix}
       \begin{bmatrix}
           0 \\ 0 \\  0
       \end{bmatrix}, & \begin{bmatrix}
           \pm1 \\ 0 \\  0
       \end{bmatrix}, & \begin{bmatrix}
           0 \\ \pm1 \\  0
       \end{bmatrix},
       & \cdots
    \end{Bmatrix},\ \vert u \rvert = 2d+1 \\
    \boldsymbol{w}^{(i)}_\mu = \begin{cases}
        \frac{\lambda}{d + \lambda},& \text{i}=0 \\
        \frac{\lambda}{2(d + \lambda)}, & \text{i}=1,2,\dots,2d
    \end{cases} \\
    \boldsymbol{w}^{(i)}_{\sigma^2} = \begin{cases}
        \frac{\lambda}{d + \lambda} + \left(1 - \alpha^2 + \beta\right),& \text{i}=0 \\
        \frac{\lambda}{2(d + \lambda)}, & \text{i}=1,2,\dots,2d
    \end{cases}
    \label{eq:cubature}    
\end{gather}

The integration points $\{\boldsymbol{p}_n^{(i)}\}_{i=1}^{2d+1}$ are used to query the trained SVGP to obtain the occupancy mean and predictive variance at each point.
We then perform cubature integration, as shown in Alg. \ref{alg:fusion}.
The output of this integration yields the final occupancy variance, $\sigma_{occ}^2$, for a query point, $p_z$.
\begin{table*}[b!]
\centering
\caption{Grasp success evaluation in the simulation environment. We run $N=5$ experiments for each test setup and report the percent grasp success by counting the number of successful attempts against the total number of attempts.}
\label{tab:quant_res_counting}
\begin{tabular}{c|l|cc|cc|cc}
 &
   &
  \multicolumn{2}{c|}{Grasp Traversal Success $\uparrow$} &
  \multicolumn{2}{c|}{Gripper Closed Onto Object $\uparrow$} &
  \multicolumn{2}{c}{Goal Pose Reached $\uparrow$} \\ \hline \hline
Method &
  Reconstruction Type &
  Kettlebell &
  Coffee Mug &
  Kettlebell &
  Coffee Mug &
  Kettlebell &
  Coffee Mug \\ \hline
TSGrasp \cite{player_real-time_2023} & Partial & 0\%          & 40\%          & 0\%          & 0\%          & 0\%          & 0\%          \\
PUGS (ours)                          & Partial & \textbf{60\%} & \textbf{60\%} & \textbf{60\%} & \textbf{40\%} & \textbf{60\%} & \textbf{40\%} \\ \hline
TSGrasp \cite{player_real-time_2023} & Noisy Partial            & 0\%          & 0\%          & 0\%          & 0\%          & 0\%          & 0\%          \\
PUGS (ours)                          & Noisy Partial            & \textbf{40\%}          & \textbf{80\%}            & 0\%          & \textbf{60\%}          & 0\%          & \textbf{60\%}          \\ \hline
TSGrasp \cite{player_real-time_2023} & Complete    & \textbf{100\%} & 60\%          & \textbf{100\%} & \textbf{80\%} & \textbf{100\%} & \textbf{60\%} \\
PUGS (ours)                          & Complete    & \textbf{100\%} & \textbf{80\%} & 60\%          & 60\%          & 60\%          & 20\%         
\end{tabular}
\end{table*}

\begin{algorithm}
\caption{Uncertainty and Predictive Uncertainty Fusion}
\begin{algorithmic}[1]
\Require Query point $p_z$
\State $\mathbf{P}_k \gets \texttt{KDTree}(\mathbf{P}_w).\texttt{query}(p_z)$
\State $\mathbf{\Sigma}_{\mathbf{p}_z} \gets \texttt{bayesianFusion}(p_z, \mathbf{P}_k)$ \Comment{Eq. \eqref{eq:bayesian_fusion}}
\State $\mathbf{p}_n, \boldsymbol{w}_\mu, \boldsymbol{w}_{\sigma^2} \gets \texttt{cubature}(p_z, \mathbf{\Sigma}_{\mathbf{p}_z})$ \Comment{Eqs. \eqref{eq:lambda}-\eqref{eq:cubature}}
\State $\boldsymbol{\mu}_n, \boldsymbol{\sigma}_\text{pred}^2 \gets \texttt{SVGP}\left(\mathbf{p}_n\right)$ \Comment{Sec. \ref{ss:pred_unc}}
\State $\mu_\text{occ} \gets \sum_i \boldsymbol{w}_\mu^{(i)} \boldsymbol{\mu}_n^{(i)}$
\State $\sigma^2_\text{occ} \gets \sum_i \boldsymbol{w}_{\sigma^2}^{(i)} \boldsymbol{\sigma}_\text{pred}^{2(i)}$
\State \Return $\{\mu_\text{occ}, \sigma_\text{occ}^2\}$
\end{algorithmic}
\label{alg:fusion}
\end{algorithm}


% \begin{algorithm}
% \caption{Uncertainty and Predictive Uncertainty Fusion}\label{alg:cpf}
% \begin{algorithmic}
% \Procedure{CPF}{$\vec{\gamma}$}
% \State $\mat{R}_c \gets \{\vec{r}_{c,0}, \cdots, \vec{r}_{c,N_{clp}} \} \in \mathcal{S}_R$ \Comment{Init particles}
% \State $N_{cs} \gets $ Set Number of Iterations
% \For{$i \gets 0; \; i < N_{cs}; \; i++$}
%     \State $\mat{R}_c \gets $ Noise-Model$(\mat{R}_c)$
%     \For{$\vec{r}_c$ in $\mat{R}_c$}
%         \State $p(\vec{\gamma} | \vec{r}_c) \gets \text{QP}(\vec{\gamma} | \vec{r}_c)$
%     \EndFor
%     \State $\mat{R}_c \gets $ Low-Var-Resample($\mat{R}_c, \vec{p}(\vec{\gamma} | \mat{R}_c)$)
% \EndFor
% \State \textbf{return} $\mat{R}_c$
% \EndProcedure
% \end{algorithmic}
% \end{algorithm}

\subsection{Fusing Occupancy Uncertainty with Grasp Selection}\label{ss:grasp_selection}
For the grasp selection, we use the computed occupancy uncertainty with the outputs of a pre-trained grasp selection framework, such as TSGrasp \cite{player_real-time_2023}.
This allows us to balance the geometric reasoning inherent in the grasp selection method with the uncertainty over the constructed geometry.
This weighing is shown in Eq. \eqref{eq:grasp_weighing}, where $\left(\sigma_\text{occ}^2\right)_G$ is the occupancy variance for each point in $\mathbf{p}_G$, $GS\left(\cdot\right)$ a grasp selection method that outputs grasp confidences given a point, and $\nu$ the scaling parameter to control how much we want the uncertainty to be weighted.
\begin{gather}
    \tilde{\mathbf{c}}_G = \frac{GS\left(\mathbf{p}_G\right)}{\left(\sigma_\text{occ}^2\right)_G^\nu} \label{eq:grasp_weighing}
\end{gather}

The grasp pose is selected to be the pose associated with the highest confidence computed in Eq. \eqref{eq:grasp_weighing}.

% We take the query points from the fused map output from a volumetric fusion method, such as voxblox \cite{oleynikova2017voxblox} to evaluate the grasp selection.
% The proposed grasp pose is the one that corresponds to the highest weighted confidence $\tilde{\mathbf{c}}_G$, which is taken from the output of TSGrasp \cite{player_real-time_2023}.
