\section{\projecttitle{}}
%\subsection{}
\label{sec:design}


%\dimitra{reminder: ONE paragraph -> one point, MANY points under the same par -> subparag}


%\dimitra{do an OVERVIEW as a subsection that serves two things: 1) the link between 4 \& 5 and 2) give the intuition}

\subsection{Overview}
%\projecttitle{} provides a general transformation of CFT protocols to target performance under Byzantine settings. 
\dimitra{SOS FIX ME EVERYWHERE: 5.1, 5.3 and 5.5.!}
Figure~\ref{fig:overview} shows the overview of the \projecttitle{} system.  We base our design on a distributed setting of TEEs that implement a (distributed) trusted computing base (TCB) and shield the execution of unmodified CFT protocols against Byzantine failures. \projecttitle{}'s TCB contains the protocol's code along with some metadata which, among others, include the configuration, the view id, the status of the replica, etc. (Table~\ref{tab:meta}). Replicas receive secrets (e.g., encryption keys) and configuration in a secure manner at initialization ($\S$~\ref{subsec:attestation}).

Further, \projecttitle{} builds a networking library to achieve low-latency communication between replicas ($\S$~\ref{subsec:networkin}). Our library shields the communication against Byzantine actors by guaranteeing the non-equivocation and transferable authentication across the network. \projecttitle{} guarantees both properties by layering the non-equivocation and authentication layers on top of the direct I/O library. We present an overview of the properties and the mechanism ($\S$~\ref{subsec:req}), the detailed description ($\S$~\ref{subsec:networkin}), and the correctness analysis ($\S$~\ref{sec:theory}).


%layer ensures that replicas will not miss any past messages. The authentication layer ensures integrity and authenticity for the network messages; replicas can .

%which shield the network stack against Byzantine errors. 

Lastly, \projecttitle{} builds a KV store which increases the trust to individual nodes allowing for local reads ($\S$~\ref{subsec:KV}). In reality, our KV store achieves two goals; first we guarantee trust to individual replicas to serve reads locally and secondly we limit the TCB size optimizing the enclave memory usage. As shown in Figure~\ref{fig:overview}, \projecttitle{} keeps bulk data (values) in the host memory and stores only minimal data (keys + metadata) in the enclave area. The metadata, i.g., hash of the value, timestamps, etc. are kept along with keys in the TEE for verifying the integrity of the unprotected values.

\subsection{Requirements for Transformation: An Overview}
\label{subsec:req}
We next discuss how \projecttitle{} satisfies the requirements for the tansformation~\cite{clement2012}.

%Prior work~\cite{clement2012} proves theoretically that non-equivocation
%and transferable authentication are the lower bounds to replace the bound $f<n/3$ by $f<n/2$ (where $f$ is the number of faulty replicas and $n$ the total number of replicas) for a reliable broadcast in Byzantine settings.  

\noindent{}{\bf{\underline{Property 1:}}} The transferable authentication property refers to the authenticity of a received message, requiring that a replica must be able to verify that the supposed sender indeed had sent the message. The authentication is transferable if the sender can be verified even if the message is forwarded by a replica other than the original sender. 

\noindent{}{\bf{Mechanism 1:}} \projecttitle{}'s authentication layer relies on cryptographic attestation primitives to ensure the property. We calculate the hash of the message's payload which we encrypt with a secret key along with some metadata. Only trusted nodes that have the key can decrypt the message, which is ensured by remote attestation~\cite{Parno2010}.% \pramod{cite remote attestation paper}

\noindent{}{\bf{\underline{Property 2:}}} The non-equivocation property guarantees that replicas cannot \emph{accept} conflicting statements for the same request. That implies that \projecttitle{} must detect attacks where adversaries try to compromise the protocol by sending invalid requests or by re-sending valid, but stale, requests (\emph{replay attacks}). 

\noindent{}{\bf{Mechanism 2:}} \projecttitle{}'s non-equivocation layer assigns sequence ids to network messages to guarantee that replicas cannot miss past messages (or double-execute them). In addition, the authentication layer guarantees that invalid requests that are not signed with the secret key are detected.






\subsection{Normal Operation: DIMITRA: NEEDS CHANGE GIVEN THE FIGURE}
\label{sec:normal_operation}

\begin{figure*}
    \begin{center}
        %\includegraphics[width=0.8\textwidth]{figs/recipe_overview.pdf}
        %\includegraphics[width=0.38\textwidth]{figs/raft_recipe.pdf}
        \includegraphics[width=\textwidth]{figs/recipe_protocol_overview3.pdf}
        %\vspace{-4pt}
        \caption{Example of R-Raft execution.}
        \label{fig:raft_example}
        %\vspace{-24pt}
    \end{center}
\end{figure*}



\if 0
\dimitra{\begin{itemize}
    \item clients are running requests through a \texttt{PUT}/\texttt{GET} API. 
    \item describe the clients' requirements (message format, ids, term/view/leader identifier etc.)
    \item a \projecttitle{}'s coordinator receives the request: 
    \begin{itemize}
        \item verify integrity (and/or confidentiality)
        \item execute the underlying protocol implementation (tell how put/get operations are executed, when the application is considered committed, tell about the uncommitted queues, etc.). 
        \item update metadata (client table, commit idx, last seen message idx, etc.) and app state
        \item cleanup history if required
        \item reply to the client (reply format, etc.)
    \end{itemize}
\end{itemize}
}
\fi

\if 0
\begin{itemize}
    \item the configuration (i.e., the encryption keys for network and application state and the IPs of the nodes along with their replica number).
%    \item the metadata index for networking (i.e., a node stores for each replica the latest processed message index and a queue with the on-hold message identifiers).
    \item the clients table which stores the most recent operation seen from a client (identified by an id).
    \item the term and the leader identifiers in the case of a leader-based underlying protocol.
    \item the operation status of the replica (i.e., \texttt{Normal}, \texttt{Recovering}, \texttt{View\_change}).
    \item the leases table (i.e., access leases for local reads and/or other nodes' leases).
    \item an index store for the application's state that holds metadata for the integrity and/security checks.
\end{itemize}
\fi




\begin{table}[t]
%\small
\fontsize{9}{10}\selectfont 

%\fontsize{8}{10}\selectfont 

\begin{center}
\begin{tabular}{ |l|c| }
 \hline
 \multirow{1}{*}{The configuration} & Encryption keys, IPs, etc.\\ \hline
 \multirow{1}{*}{The clients table} & \multirow{1}{*}{Last req id seen from a client.}\\ \hline
 The view (term) & \multirow{2}{*}{For leader-based CFT protocols.}\\ and leader id & \\ \hline
 Replicas operation &  \texttt{Normal}, \texttt{Recovering}, \texttt{ViewChange}.\\ \hline
 %\multirow{1}{*}{The leases table} & \multirow{1}{*}{Access leases for local reads and/or other nodes' leases.} \\ 
 \multirow{2}{*}{The (hybrid) KV store} & Stores the metadata in the TEE\\
 &  and bulk data in host memory.\\ \hline
\end{tabular}
\end{center}
%\vspace{-10pt}
\caption{\projecttitle{}'s metadata in the TEE.}\label{tab:meta}
%\vspace{-18pt}
\end{table}


Clients in \projecttitle{} execute requests through a \texttt{PUT}/\texttt{GET} API by sending the request to the coordinator node. Figure~\ref{fig:raft_example} shows as an example a \projecttitle{} implementation of Raft (R-Raft). The leader initializes the underlying CFT protocol to execute the request \circled{2}---\circled{3a},\circled{3b}---\circled{4}---\circled{5} and, upon completion, replies to clients \circled{6a}, \circled{6b}. Next we discuss the \projecttitle{} abstraction under Normal operation. %\manos{some repetition with the system model (\$3) now}

 %In leaderless protocols, coordinator nodes are selected randomly. In leader-based protocols, all non-leader replicas deny to process requests and reply back to clients with their view of the current active leader. The client then can forward the request to the active leader.


%\begin{itemize}[noitemsep]
  %\setlength\itemsep{10em}
    \noindent{}{\bf{\underline{\#1}}:} Clients send to the coordinator their request of the form $<(metadata, req\_data)$, $h_{c}>_{\sigma_{c}}$. The req\_data is the request's associated data and the metadata might include among others the client id, the leader's and term's ids (known to the client), and the request id. The request is sent over the network along with the message's calculated signed hash $h_{c}$. We use the symbol $\sigma_{c}$ to denote that the message is signed with a key $c$.
    %Clients send to a node (from now on the request's \emph{coordinator node} or leader), their preferred request that is of the form <$\texttt{CLIENT\_REQ(cid, lid, tid, rid, rdata)}$, $h_{c}$> where \texttt{cid} is the client id, \texttt{lid} and \texttt{tid} is the leader's and term's ids known to the client, \texttt{rid} is the request id which gets monotonically incremented per client and \texttt{rdata} is the serialized request with its arguments. The request is sent over the network along with the message's calculated signed hash $h_{c}$.
    
    \noindent{}{\bf{\underline{\#2}}:} Nodes receive and process a request as long as they are authorized to do so. Essentially, requests that have a wrong view of the term and the leader or have already processed (their id is known to the coordinator node) are dropped.
%    \dimitra{MAKE THE PAR shorter in all paper -> make it as substeps}
    
    \noindent{}{\bf{\underline{\#3}}:} Upon the reception of a client's request: 
    
    {\bf{\#3.1}} The coordinator node verifies the integrity and authenticity of the message using \projecttitle{}'s authentication layer. The node also verifies the metadata, e.g., the message is not valid if the term and the leader (if any) known to the client are not correct. The coordinator updates the client table with the latest processed request for each client. 
    
    {\bf{\#3.2}} Next, the coordinator initializes the protocol for that request. In our example, the Raft leader receives the request \circled{2} and passes through Raft's replication phase by multi-casting the \texttt{PUT} request to the majority \circled{3a}---\circled{3b}. Once the majority acknowledges the request (red arrows), the leader initiates the second round of the protocol instructing the majority to commit the update. The execution returns back to the client after the majority acks the commit \circled{5}---\circled{6a}. 
    
    {\bf{\#3.3}} The messages exchanged between replicas are of the form $<(metadata, req\_data)$, $h_{r}>_{\sigma_{cq}}$. The metadata include a per-request unique tuple which contains the view, the communication channel id and a sequencer id for that channel ($<$$view, cq, cnt$$>$). The sequencer id is increased monotonically for every new request sent over the $cq$ (non-equivocation).
    %\dimitra{here: explain the counter}
    %The messages $m$ that are sent between replicas are of the form $<\texttt{CFT\_REQ(cid, lid, tid, rid, rdata), <view, cq, cnt>}$, $h_{m}>_{\sigma_{c}}$. \dimitra{here: explain the counter}
    
    \noindent{}{\bf{\underline{\#4}}:} When a replica receives a message from another replica:
    
    {\bf{\#4.1}} It verifies validity and integrity of the message (authentication-layer). Only replicas in Normal operation mode are authorized to execute the protocol---if the replica's state differs, it might refuse to process the request.% (e.g, Recovering-state replicas do not execute the read path).
    
    {\bf{\#4.2}} The replica verifies the message counter ($msg\_cnt$) through the non-equivocation layer to see if it is consistent with its local counter ($cnt_{cq}$). If the current $msg\_cnt$ equals ($cnt_{cq}$+$1$), the replica executes the request immediately, increases the $commit_{idx}$ and acknowledges to the sender node. The replica also updates the client table. In case the message counter refers to a ``future'' message ($msg\_cnt$ $>$ $cnt_{cq}$+$1$), the replica might queue the request in the protected area. Periodically, \projecttitle{} replicas apply the queued requests that are eligible for execution and notify coordinators accordingly.
    
    \noindent{}{\bf{\underline{\#5}}:} After the protocol's execution for a request, the coordinator marks the request as committed and notifies the client (In our example \circled{6a} and \circled{6b}).
    %\noindent{}{\bf{\underline{\#5}}:} A request coordinator after the protocol's execution for that request updates the state of the request as committed and notifies the client (In our example \circled{6a} and \circled{6b}.).
    
    \noindent{}{\bf{\underline{\#6}}:} Replicas, if required, rely on timeouts and heartbeat messages to verify they are part of the configuration. %In case of untrusted environments, \projecttitle{} models timeouts with trusted leases for TEEs~\cite{t-lease}.
%\end{itemize}





\subsection{View Change}
\if 0
\dimitra{\begin{itemize}
    \item view change is ''leader election`` for us!
    \item Describe how recipe identifies leader failures
    \item Leader election depends on the specific protocol but you must say here how you ensure at most one leader at a time
\end{itemize}
}
\fi

While decentralized protocols remain always available as long as a majority of nodes is part of the membership, the leader-based protocols do not make any progress if the leader goes down. To remain available after the leader crashes, the followers need to closely monitor the leader (e.g., heartbeat messages in inactive periods) and, in case it is unreachable, to \emph{elect} a new one---in PBFT parlance to do a \emph{view change}.

%Usually, the leader sends requests which also notify followers about the leader's activity. However, due to inactivity, the leader might remain idle and in this case it will send pings. 

%The follower nodes rely on timeouts to identify leader failures. Specifically, if a timeout expires without communication (and, probably, after some retries), the replicas need to carry out a leader election protocol to switch to a new leader---similarly, as a view change to switch to a new primary in PBFT. 

\projecttitle{} assigns a leader with a term and the node identifier. The term identifier can be seen as an \emph{epoch}, a monotonically increased number that uniquely identifies the current view of the system. To continue serving requests after a leader election, all replicas of the membership need to confirm the new leader along with the new term. Since a leader needs to be acknowledged by the majority of the nodes to operate, it follows that the latest term will survive in at least one node, which ensures the monotonic increments of the term.

\myparagraph{Correctness} The correctness condition for leader elections is that every committed operation must survive into the new leader election in the order selected for it at the time it was executed. \projecttitle{} does not make further assumptions for leader election, we guarantee that the replicas are trusted ($\S$~\ref{sec:theory}). Protocols can rely on their election algorithms. %Additionally, if the client receives no reply to a request, it resents the request; this way all clients eventually will learn about the new elected leader, and also prompts the new leader/primary to send it the reply.

\myparagraph{Failure detection} Some CFT protocols~\cite{raft, chain-replication} often require a trusted timer source to detect failures. We build \projecttitle{} on top of Intel SGX that is no longer supporting secure timers~\cite{monotoniccounterssgx, sgxtrustedtime}. The OS-timers and software-based implementations of a clock cannot be trusted. %Similarly, any software-based implementations or simulations of a clock within the TEE can be compromised by a Byzantine OS.%---for example, the OS can continuously schedules out the timer thread.\\
Unfortunately, the lack of a trusted timer complicates failure detection for protocols that primarily rely on accurate time-outs---for example, leader election in Raft. \projecttitle{} builds on top of \scone{} that implements a trusted lease mechanism~\cite{t-lease}. Our mechanism supports all the  properties of classical leases~\cite{10.1145/74850.74870} in untrusted environments that is the building block for trusted timeouts, failure detectors~\cite{222603}, leader election~\cite{815321}, etc.

While \projecttitle{}'s overcomes this problem, we believe that this is no longer a real limitation as the state-of-the-art TEEs~\cite{intelTDX, amd-sev} offer trusted timer sources~\cite{intelTSC, amdTSC}. %\projecttitle{}'s lease mechanism supports all the properties of classical leases~\cite{10.1145/74850.74870} while it offers accuracy, performance and correctness in untrusted environments. The lease duration of the holder is always a superset of the granter under the presence of a powerful attacker that can manipulate the clocks, the cpu frequency, etc.
%We argue that this is no longer a limitation as the state-of-the-art TEEs~\cite{intelTDX, amd-sev} offer trusted timer sources~\cite{intelTSC, amdTSC}.


\subsection{Recovery}

As nodes fail, new or recovered nodes need to be added to continue operating at peak performance. To add a new node, the membership needs to be reliably updated, following that all other live replicas are notified of the new node’s
intention to join the replica group. Once all the replicas acknowledge this notification, the new node starts operating as a shadow replica that participates as a follower for  the writes but does not serve any client requests. Additionally, it might read chunks from other replicas to fetch
the latest state and reconstruct the application state similarly to~\cite{10.1145/2815400.2815425, 10.1145/2043556.2043560}. Overall a new joining node follows the next steps:

%\begin{itemize}
   \noindent{}{\bf{\underline{\#1}}:} A recovering node needs first to be attested before any secrets and membership information are shared. Particularly, upon a node recovery or join, before the control passes to the CFT protocol, the node sends a join request to a designated node notifying it about its willingness to join the cluster. %\footnote{While we were implementing \projecttitle{}, we assumed static IP addresses where nodes are listening for join requests. However, this is not a restriction. The developers can implement there own membership management service.}
   
   \noindent{}{\bf{\underline{\#2}}:} The node (challenger-node) that receives a join request from a new node requests a remote attestation to verify the trustworthiness of the TEE and the loaded code ($\S$~\ref{subsec:attestation}). 
   
    \noindent{}{\bf{\underline{\#3}}:} After a successful attestation, the challenger-node shares the network encryption keys, etc., and the configuration of the membership by replying back to the join-request. Once the new joiner acknowledges that it received the configuration, the replicas are notified about the new membership. 
    %\footnote{\projecttitle{} does not make any assumption of who is in change of distributing the configuration. $\S$~\ref{subsec:attestation} explains how \projecttitle{} implements configuration distribution. However, the developer can build its own configuration and attestation service building on top of \projecttitle{}'s TEE-assisted primitives.}
    
    % and waits for \texttt{ACK} before proceeding. This handshake verifies that the joiner node indeed sent the request. The other replicas about the new recovering node. The configuration might be the nodes' IPs or the IP of a management service that knows about the system membership. %Joiner nodes and CAS communication is performed over a TLS network channel.
    %\item[3.]  and communicates with a node of the membership with a request <\texttt{JOIN\_REQ(uid)$_s$}>, \texttt{uid} is a randomly generated unique id from that node.
    %\item[4.] The recovering node marks its state as \texttt{RECOVERING} The node that receives a join request from a new node sends back a <\texttt{VERIFY\_ID\_REQ(uid)$_s$}> and waits for \texttt{ACK} before proceeding. This handshake verifies that the joiner node indeed sent the request. The other replicas about the new recovering node.
    
    \noindent{}{\bf{\underline{\#4}}:} The new node marks its state as Recovering and act as a shadow replica fetching the state of the system. If the CFT protocol allows it, joiner nodes can participate in writes while recovering. Once the replica is fully synchronized with the current state of the system, it changes its state to Normal. The replica can now execute the entire protocol. % After reading the entire applications state, the shadow replica is up-to-date and transitions to operational state, whereby it is able to serve client requests.
    %and updates the replicas (or the management service) about the membership change
%\end{itemize}

\myparagraph{Persistent storage for recovery} \projecttitle{} does not support secure recovery from persistent storage. Persistent data are susceptible to rollback attacks where the adversaries can replace the data with stale, yet valid, versions of it. Nevertheless, developers are able to support recovery from persistent (local) logs and checkpoints (if necessary) at the cost of relying on external trusted services for rollback violations detection, similarly to other systems~\cite{engraft, treaty, rote, speicher-fast}.