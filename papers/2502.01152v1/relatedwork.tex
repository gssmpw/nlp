\section{Related work}
\noindent \textbf{Backdoor Attacks. }In backdoor attacks \cite{gu2019badnets}, an attacker injects a specific pattern, known as \textit{trigger}, into a portion of the training data and assigns these samples a target label. The resulting backdoored model performs normally on clean data but misclassifies inputs with the trigger to the target label. Most backdoor attack techniques \cite{gu2019badnets, chen2017targeted, nguyen2020input, nguyen2021wanet, wang2022bppattack, wu2023attacks} are designed for the visual domain, among which classic examples include BadNets \cite{gu2019badnets} and Blended \cite{chen2017targeted}. In the audio domain, Ultrasonic attack \cite{koffas2022can} is a representative method for automatic speech recognition tasks, where the attacker uses an ultrasonic signal as the backdoor trigger. To enable attacks in a physical scenario, naturally occurring sounds are chosen as triggers in DABA \cite{liu2022opportunistic}. Various audio-specific methods \cite{koffas2023going, cai2024towards} have also been devised to increase the stealthiness of attacks. Recently, a stealthy attack FlowMur \cite{lan2024flowmur} was introduced, where it trains a model to generate triggers while ensuring consistency between the target label and ground truth.

\noindent \textbf{Backdoor Defenses. }According to \cite{yan2023backdoor}, backdoor defenses can be categorized into data-level and model-level approaches. Data-level defenses aim to identify and remove poisoned data from the dataset, while model-level defenses attempt to mitigate backdoored effect in a well-trained backdoored model using a small amount of clean data. In the audio domain, existing backdoor defenses are all adaptations from the visual domain and are primarily data-level \cite{gao2019strip, ma2022beatrix}. FP \cite{liu2018fine} is the only adapted model-level defense for audio-backdoored models, which prunes neurons with low activation on clean data and then fine-tunes the pruned model. However, FP fails to effectively defend against most audio backdoor attacks. In this work, we address this issue by proposing a gradient-regularized fine-tuning technique from the model-level perspective, which is the first specialized defense for the audio-backdoored models. 

% Some audio processing techniques for handling noise signals, such as Noise Suppression \cite{hu2020dccrn}, and DeepFake \cite{hamza2022deepfake}, also have the potential for use as data-level backdoor defenses \cite{yan2023backdoor}.

% , based on the observation that neurons exhibit distinct activation levels for clean versus poisoned data