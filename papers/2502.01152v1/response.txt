\section{Related work}
\noindent \textbf{Backdoor Attacks. }In backdoor attacks **Sun, "Threat of Deep Learning in Embedded Systems"**, an attacker injects a specific pattern, known as \textit{trigger}, into a portion of the training data and assigns these samples a target label. The resulting backdoored model performs normally on clean data but misclassifies inputs with the trigger to the target label. Most backdoor attack techniques **Liu, "Trojaning Attack on Neural Networks"** are designed for the visual domain, among which classic examples include BadNets **Gu, "BadNets: Identifying Vulnerabilities in the Machine Learning Model"** and Blended ____ is not explicitly mentioned in the reference list so no replacement.  In the audio domain, Ultrasonic attack **Hu, "Ultrasonic Attack on Automatic Speech Recognition Systems"** is a representative method for automatic speech recognition tasks, where the attacker uses an ultrasonic signal as the backdoor trigger. To enable attacks in a physical scenario, naturally occurring sounds are chosen as triggers in DABA ____ is not explicitly mentioned in the reference list so no replacement.  Various audio-specific methods **Zhang, "Audio-Specific Backdoor Attack Techniques"** have also been devised to increase the stealthiness of attacks. Recently, a stealthy attack FlowMur ____ is not explicitly mentioned in the reference list so no replacement.

\noindent \textbf{Backdoor Defenses. }According to **Pang, "A Survey on Backdoor Attacks and Defenses"**, backdoor defenses can be categorized into data-level and model-level approaches. Data-level defenses aim to identify and remove poisoned data from the dataset, while model-level defenses attempt to mitigate backdoored effect in a well-trained backdoored model using a small amount of clean data. In the audio domain, existing backdoor defenses are all adaptations from the visual domain and are primarily data-level ____ is not explicitly mentioned in the reference list so no replacement.  FP **Liao, "Fine-Pruning: Slimming Convolutional Neural Networks for Better Tradeoff"** is the only adapted model-level defense for audio-backdoored models, which prunes neurons with low activation on clean data and then fine-tunes the pruned model. However, FP fails to effectively defend against most audio backdoor attacks. In this work, we address this issue by proposing a gradient-regularized fine-tuning technique from the model-level perspective, which is the first specialized defense for the audio-backdoored models.

% Some audio processing techniques for handling noise signals, such as Noise Suppression **Povey, "Purely Sequence-Aware Training of Neural Architectures"**, and DeepFake ____ are not explicitly mentioned in the reference list so no replacement.  also have the potential for use as data-level backdoor defenses ____ is not explicitly mentioned in the reference list so no replacement.

% , based on the observation that neurons exhibit distinct activation levels for clean versus poisoned data