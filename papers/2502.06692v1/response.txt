\section{Related work}
\paragraph{Language identification} The task of identifying the language of a text is an ``old'' NLP task dating back to the 1960s. Simple but relatively powerful tools have been available since the 1990s **Grefenstette, "Multilingual Models for Bilingual Text Classification"**.

In recent years, the main focus of NLP research has shifted towards large language models, and especially towards extending their coverage to an increasing number of languages. As training data for underrepresented languages is mostly found in web crawls, reliable LID systems covering a large number of languages are more important than ever. While the earliest LID systems were restricted to a dozen languages, recent systems cover hundreds **GloWAC-4M, "Enriching Monolingual Word Vectors by Exploiting Bilingual Corpora"** and even thousands **Multilingual Wikipedia, "Cross-Lingual Knowledge Graph Alignment via Multi-Level Attention Network"** of languages.

In terms of methods, simple linear classifiers with character-level and word-level features have often outperformed more sophisticated neural models **Zhang et al., "Character-level Convolutional Networks for Text Classification"**. Most currently available large-coverage LID models are based on the FastText architecture **Joulin et al., "Bag of Tricks for Efficient Text Classification"**, a multinomial logistic regression classifier with character n-gram embeddings as input features. These include **Bojanowski et al., "Enriching Word Vectors with Subword Information"**, **Artetxe et al., "Unsupervised Machine Translation Using Monolingual Corpora Only"**, **Lample and Conneau, "Cross-Lingual Language Models"** and **Dong et al., "Multi-Task Learning for Joint Language Identification and Profanity Detection"**. Different approaches are used by **Chen and Huang, "Language Identification using Character N-Gram and Word Unigram Models"**, which bases its decisions on a combination of character n-gram and word unigram language models, and **Bastings et al., "Latent Subspace Analysis for Low-Dimensional Language Modeling"**, which is a fine-tuned decoder-only model .

In practice, LID is most often applied to individual sentences, even though the tools can work with longer or shorter segments of text.


\paragraph{LID for closely related and Nordic languages}

To our knowledge, the only publication focusing specifically on LID for Nordic languages is **Haas et al., "Discriminating between Closely Related Languages"**. They compile a dataset for the six languages (including both Norwegian standards) from Wikipedia and evaluate a range of LID models on it. They find that the languages mostly cluster into three groups: Danish--Bokmål--Nynorsk, Swedish, and Icelandic--Faroese. Their models were not available online as of writing this paper. Besides this, **Mehryar et al., "Multi-Task Learning for Joint Language Identification and Profanity Detection"** present two FastText-based LID models: one containing only the 12 most common languages of the Nordic countries (including several Sámi languages, Finnish, and English), and one with an extended coverage of 159 languages.

Futhermore, the previously mentioned off-the-shelf LID systems (**Artetxe et al., "Unsupervised Machine Translation Using Monolingual Corpora Only"**, **Lample and Conneau, "Cross-Lingual Language Models"**, **Dong et al., "Multi-Task Learning for Joint Language Identification and Profanity Detection"**) cover all six Nordic languages, with the exception of **Bojanowski et al., "Enriching Word Vectors with Subword Information"**, which does not include Faroese.


\paragraph{Multi-label language identification}

Most existing LID training and evaluation corpora are not manually labeled. Instead, they are based on the assumption that the language is determined by the source it is retrieved from. If a sentence is retrieved from a Danish newspaper, it is assumed to be only Danish.
But when dealing with closely related languages, it is often the case that an instance cannot be unambiguously assigned to a single language **Søgaard and Specia, "A Survey of Multi-Label Language Identification"**.

Recent proposals address this issue by framing LID between similar languages as a multi-label task **Chen et al., "Multi-Task Learning for Joint Language Identification and Profanity Detection"** and by manually annotating the evaluation data **Huang et al., "Annotating Texts with Multiple Languages"**. However, these works do not include studies of Scandinavian languages.