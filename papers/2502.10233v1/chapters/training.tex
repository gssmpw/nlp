\label{sec:training}
During training, the objective is to adjust the parameters $\theta$ of the policy $p_\theta$ to maximize the reward $R(\bm{a}, x)$ for any given problem instance $x$. Formally, we can cast the optimization problem as follows:
%
\begin{align}
\label{eq:training_problem_simple_ar}
    \theta^{*}=\underset{\theta}{\text{argmax}} 
    \Big[
    \mathbb{E}_{x \sim P(x)}\big[\mathbb{E}_{\bm{a} \sim p_\theta(\bm{a}|x)}R(\bm{a},x)\big] 
    \Big].
\end{align}
%

Due to the absence of large datasets containing the optimal solutions $\bm{a}$ for CO problem instances $x$, several Reinforcement Learning techniques have been developed to train neural CO solvers \cite{koolBuyREINFORCESamples2019,kwonPOMOPolicyOptimization2021,kim2022sym}. However, recently, self-supervised approaches have emerged in the realm of neural combinatorial optimization and already achieve state-of-the-art results on some CO problems \cite{pirnay2024selfimprovement,SelfJSP}. 

A major advantage over REINFORCE-based learning is that single actions instead of entire trajectories can serve as training examples. While REINFORCE prohibits a re-encoding of the problem state after each decoding step due to the accumulation of gradient information during training, the use of single actions or sub-trajectories in self-supervised learning allows for stepwise encoding \cite{pirnay2024selfimprovement}. 
While this might impose unnecessary computational cost for static problems like the TSP, it is a strong benefit for a highly dynamic problem like the MSPRP, where after each step the demand, supply and capacity change. Therefore, in this work, we adopt the self-improvement approach described in \cite{pirnay2024selfimprovement}. This method samples $\alpha \gg 1$ candidate solutions for an instance $x$ from the current best-known policy $p_{\theta^\ast}$ and selects the best one, $\bm{a}^\ast \coloneq \text{argmax} \{R(\bm{a}^1, x), \ldots, R(\bm{a}^\alpha, x)\}$, as a training example. Then, cross-entropy loss $\mathcal{L}_{\text{CE}} = - \sum_{t=1}^T \log \, p_\theta (a^\ast_t | s_t)$ is used to train the model on these pseudo-optimal solutions. The refined model is used in the next iteration to generate new candidate solutions, leading to progressively better training examples as training advances.\footnote{A detailed description of the algorithm is given in \Cref{algo:learning}}

In order to apply self-improvement to learn the parameters of MAHAM, we first revise the autoregressive policy of \Cref{eq:autoregressive_encoding_decoding} and extend it with the components introduced by our MAHAM architecture: 

\begin{align}
    p(\bm{a}|x) = \prod_{t=1}^T f_\theta(H|s_t) \cdot \prod_{d=\{\mathcal{V}, \mathcal{P}\}}  g^d_\theta(\logits_d|s_t, H) \cdot \prod_{\agentidx=1}^\numagents \psi(a_{dt}^{m} | \logits_d, \bm{a}_{dt}^{1:m-1}),
\end{align}
where the encoder is factorized over the action sub-spaces $|\mathcal{V}|$ and $|\mathcal{P}|$, which both use the same encoder embeddings, and the decoder produces logits $\logits_d$ only once for all $M$ agents, allowing MAHAM to efficiently model dependencies in multi-agent decision-making. Resulting from this, we train the model with cross entropy loss via gradient descent using the following definition of the gradients:
\begin{align}
    \nabla_\theta \mathcal{L} = - \sum_{t=1}^T \sum_{d=\{\mathcal{V}, \mathcal{P}\}} \sum_{\agentidx=1}^\numagents \nabla_\theta \log \, p_\theta(a_{dt}^{m} | \bm{a}_{dt}^{1:m-1}, s_t)
\end{align}
