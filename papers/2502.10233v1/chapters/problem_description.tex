
This work focuses on a min-max variant of the MSPRP with split orders and split deliveries covered in \cite{luttmann2024neural}. The split orders assumption allows items of an order to be picked
within different tours and split deliveries relaxes the assumption that the demand
for an SKU must be satisfied by a single picker tour \cite{xie2021introducing}. A tour is defined by the storage locations visited between two successive visits to a packing station $h \in \mathcal{V}^{\mathrm{D}}$, where picked items are unloaded and commissioned. During a tour, no more than $\kappa$ units can be picked. Further, due to the mixed-shelves storage policy each shelf may consist of multiple storage locations or compartments storing units of different SKUs. Also, the mixed-shelves storage policy allows each SKU $p$ to be retrieved from multiple storage locations $i \in \SetOfLocationsIncludePickItem{p}$. 

The goal of the min-max MSPRP is to pick all $d_p$ demanded units of all requested SKUs $p \in \SetOfSKUs$ and returning them to a packing station $h \in \SetOfPackingStations$ while minimizing the maximum travel distance among the individual pickers $m = 1,...,M$, henceforth also called agents. 
Note that in order to compare our proposed method against baselines \cite{luttmann2024neural}, \cite{liu20242d}, and \cite{son2024equity}, we assume that the number of agents is equal to the number of tours required to collect all demanded items given the picker capacity $\kappa$ (i.e. $\numagents = \left \lceil \frac{\sum_p d_p}{\kappa} \right \rceil$). We provide the mathematical model for the min-max MSPRP in \Cref{appendix:notation}.



\subsection{Markov Decision Process Formulation}
\label{sec:mdp}
\newcommand{\setOfNodesMDP}{\mathcal{V}}
The min-max MSPRP can be modeled as a cooperative Multi-Agent Markov Decision Process (MMDP) with $\numagents$ agents sharing a common reward. An MMDP is defined as $(\mathcal{S}, \SetOfAgents, \{\mathcal{A}_i\}, \Gamma, R)$, where $\mathcal{S}$ and $\SetOfAgents$ are finite sets of states and agents, respectively. Each agent $m$ selects actions from $\mathcal{A}_m$, with the joint action space denoted as $\bm{\mathcal{A}}$. The transition function $\Gamma$ determines state changes based on actions, and $R$ is the shared reward function.  

MMDPs involve sequential decision-making, where agents select and execute actions at each step until a terminal state $s_T$ is reached. The min-max MSPRP is framed as an MMDP, with pickers (agents) visiting warehouse shelves to fulfill SKU demands. A shared $\theta$-parameterized policy determines the next location and SKU to pick. This chapter formally defines the min-max MSPRP as an MMDP, specifying its state, action space, transition rule, and reward function.


\vspace{-4.5mm}
\subsubsection{State.} 
The state $s_t$ of the min-max MSPRP at step $t$ can be represented as a heterogeneous graph $\mathcal{G}=(\setOfNodesMDP,\SetOfSKUs, \SetOfAgents, E_{t})$ with pickers, warehouse locations and SKUs posing different types of nodes in the graph. 
The set of warehouse locations $\setOfNodesMDP$ is the collection of all packing stations $\SetOfPackingStations$ and shelves $\mathcal{V}^\text{R}$. 
%Locations can be represented by the Cartesian coordinates $\mathbf{x}^{\mathcal{V}}_i \in \mathbb{R}^2$. 
The state of an SKU $p \in \SetOfSKUs$ is defined by its remaining demand $d_{pt}$ at step $t$. 
Moreover, edges with weights $E_t$ connect shelf and SKU nodes, specifying the storage quantity $e_{vpt}$ of an item $p$ in the respective shelf $v$ at time $t$. 
Lastly, the state of the pickers $m \in \SetOfAgents$ is defined by their current location $v^m_{t}$, remaining capacity $\kappa^m_{t}$ and the length of their current tour $\tau^m_{1:t}=(v_1^m,\ldots,v_t^m)$, denoted as $dist(\tau^m_{1:t})$.
\vspace{-4.5mm}
\subsubsection{Action.} 
A single agent action $a^m_{t}$ is a tuple $(v,p)$ specifying the next shelf to visit as well as the SKU to pick for agent $m$. Given $s_t$, visiting shelf $v$ is a feasible action if it stores items of at least one SKU currently in demand. Furthermore, given the picking location $v$, the picker may only select an SKU for picking that is both still in demand and available in the current shelf. Note, that the quantity of picked items will be determined heuristically by the transition function $\Gamma$ in order to decrease the complexity of the action space and facilitate policy learning. 

The packing station can always be visited by a picker to unload picked items and thus to restore the capacity. When a picker's capacity is exhausted, visiting the packing station is the only possible action. Moreover, to facilitate agent coordination, a picker may always choose to stay at its current location in order to give other pickers precedence. This way, a hesitant picker may wait and evaluate what the other pickers are doing, before making the next move. 
\vspace{-4.5mm}
\subsubsection{Transition.} 
Given the joint actions $\bm{a}_t = (a_t^1,\ldots,a_t^m)$ of all agents, the transition function $\Gamma(s_t, \bm{a}_t)$  deterministically transits to $s_{t+1}$. The new state consists of the updated agent locations $\bm{v}_{t} = (v^1_{t}, \ldots, v^\numagents_{t})$ and agent tours $\tau^m_{1:t} = \tau^m_{1:t-1} \cup{\{ v^m_{t} \}}$.
To update the remaining demand, supply and picker capacities, the pick quantity $y_t^m$ must be determined. Given the pick locations, SKUs and a permutation $\Omega$ over pickers, we iteratively determine the pick quantity as the minimum of the remaining demand of the selected SKU $p$, the storage quantity at the agent's new location $v$ as well as the agent's remaining capacity:
%
\begin{align}
    y^\curragent{k}_t = \text{min} (\kappa_t^\curragent{k}, \, d_{pt} - \sum_{j=1}^{k-1} y_t^\curragent{j} \cdot \mathbbm{1}_{p^{\curragent{j}}_t = p}, \, e_{vpt} ),
\end{align}
%
where no two agents may select the same shelf-SKU combination in the same decoding step, for which reason the supply $e_{vpt}$ will not be altered by preceding agents. 
%If the above mechanism determines that an agent cannot pick any item from the chosen SKU because its predecessors already retrieved all demanded units, we leave the respective agent in its current position without a picking job, similar to the fallback action defined in \cite{berto2024parco}. 
%Note that permutation $\Omega$ can have a significant influence on the solution quality. We provide our model with a mechanism to learn this permutation, effectively enabling it to generate high quality solutions.
Given the pick quantities $y_t^m$, the transition function updates the demand $d_{pt+1} = d_{pt} - \sum_{m=1}^\numagents y^m_{t} \cdot \mathbbm{1}_{p^m_t = p}$, the supply $e_{vpt+1} = e_{vpt} - \sum_{m=1}^\numagents y^m_{t} \cdot \mathbbm{1}_{v^m_{t} = v; p^m_t = p}$ and the remaining picker capacity $\kappa^m_{t+1} = \kappa^m_{t} - y^m_{t}$. 
%NOTE dont need this condition in this problem since we assume the existance of one agent per tour
%When an agent chooses to return to the packing station, its capacity will be restored to the initial payload $\kappa$.
The problem instance $x$ is solved once the demand for every SKU is met and all pickers have returned to the packing station they were starting from. A feasible solution to $x$, reaching the terminal state $s_T$ in $T$ construction steps, will be denoted as $\bm{a} \coloneqq (\bm{a}_1, \ldots, \bm{a}_{T})$. 
%We use $\tau^m \coloneqq \tau^m_{1:T}$ as a shorthand to denote the full tour of an agent and $\bm{\tau}=(\tau^1, \ldots, \tau^\numagents)$ as a feasible solution.
\vspace{-4.5mm}
\subsubsection{Reward.}
The MMDP formulation of the min-max MSPRP has a sparse reward function, which is only defined for a complete solution $\bm{a}$. We define the reward $R(\bm{a}, x)$ as the negative of the maximum travel distance of any picker, i.e. $R(\bm{a},x) = - \text{max}_{m \in \SetOfAgents} \, dist(\tau^m_{1:T})$, and the goal of our approach is to maximize it.
