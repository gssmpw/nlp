\clearpage

\newpage
\appendix
\onecolumn
\renewcommand{\paragraph}[1]{\bigskip\noindent\textbf{#1}\quad} % Makes \paragraph bold


\section{Formal Definition of the MSPRP}
\label{appendix:notation}

The following mathematical model describes the min-max MSPRP cover in this work and table \ref{tab:notation} summarizes the notation used to define the model.

\begin{align}
  \label{eq:obj_fn}
  \textbf{min} \qquad \qquad \qquad \quad Z &= \underset{b \in \NumTours}{\text{max}} \sum_{(i,j) \in \mathcal{E}} D_{ij} \cdot x_{ijb} \\
    \label{eq:flow_cnstr}
    \textbf{s.t.} \qquad \quad \; \; \sum_{(i,j) \in \mathcal{E}} x_{ijb} &=  \sum_{(j,i) \in \mathcal{E}} x_{jib} &&\forall \, i \in \SetOfAllNodes, b \in \NumTours \\
    \label{eq:no_mult_visits}
    \sum_{(i,j) \in \mathcal{E}} x_{ijb} &\leq 1 &&\forall \, i \in \SetOfAllNodes, \, b \in \NumTours \\
    \label{eq:take_only_if_visit}
    BigM \cdot \sum_{i \in \SetOfAllNodes} x_{ijb} &\geq y_{jb} && \forall \, j \in \SetOfStorageLocations , \,b \in \NumTours \\
    \label{eq:include_depot}
    \sum_{h \in \SetOfPackingStations} \sum_{j \in \SetOfStorageLocations} x_{hjb} &= 1 && \forall \, b \in \NumTours \\
     \label{eq:subtour}
    \sum_{i\in S} \sum_{j \in S} x_{ijb} &\leq |S| -1 && \forall \, b \in \NumTours, \,  S \subset \SetOfStorageLocations, |S| \geq 2 \\
    \label{eq:capacity}
     \sum_{i \in \SetOfStorageLocations} y_{ib} &\leq \kappa && \forall b \in \NumTours\\
    \label{eq:meet_demand}
    \sum_{i \in \SetOfLocationsIncludePickItem{p}} \sum_{b \in \NumTours} y_{ib} &= d_p && \forall \, p \in \SetOfSKUs \\
    \label{eq:no_exceed_sup}
    \sum_{b \in \NumTours} y_{ib} &\leq n_{i} && \forall \, i \in \SetOfStorageLocations \\
    \label{eq:def_x}
     x_{ijb} &\in \{0,1\} && \forall \, (i,j) \in \mathcal{E},\, b \in \NumTours \\
    \label{eq:def_y}
     y_{ib} &\geq 0 && \forall \,  i \in \SetOfStorageLocations, \, b \in \NumTours
\end{align}

%\input{tables/notation}

The objective function (\ref{eq:obj_fn}) aims to minimize the maximum distance traveled by any picker. Constraints (\ref{eq:flow_cnstr}) ensure that every storage location visited during a pickerâ€™s tour is also exited. Constraints (\ref{eq:no_mult_visits}) prevent storage locations from being visited multiple times within a single tour, though multiple visits are allowed across tours if the picker's capacity is insufficient to fulfill the demand in one trip. However, revisiting the same storage location within a single tour is inefficient and therefore disallowed.
Using the Big-M formulation in (\ref{eq:take_only_if_visit}), we ensure that items can only be picked from storage locations included in the respective tour. Since no more than $\kappa$ items can be picked in one tour, setting $BigM=\kappa$ is sufficient.
To guarantee that each tour begins and ends at a packing station, constraints (\ref{eq:include_depot}) enforce that a packing station is exited exactly once per tour. Combined with the network flow constraints (\ref{eq:flow_cnstr}), this ensures that every tour returns to the packing station it initially departed from. Additionally, subtour elimination constraints (\ref{eq:subtour}) ensure that all visited storage locations are connected within a tour.
Constraints (\ref{eq:capacity}) prevent the picker's capacity from being exceeded, while constraints (\ref{eq:meet_demand}) ensure that all customer orders are fulfilled. To avoid exceeding the available stock of items in any storage location during order picking, constraints (\ref{eq:no_exceed_sup}) are enforced. Finally, constraints (\ref{eq:def_x}) and (\ref{eq:def_y}) define the domains of the decision variables $x$ and $y$.
\input{tables/notation}

\section{Baselines}
\label{appendix:baselines}

\paragraph{Gurobi \cite{gurobi}.}
We implement the mathematical model described in \Cref{appendix:notation} in the exact solver Gurobi \cite{gurobi} and provide a time budget of 600 and 3600 seconds per test instance. We run the Gurobi solver with activated multi-threading on a machine equipped with two Intel Xeon E5-2690 v4 processors, totaling 28 physical cores and 56 logical threads.

\paragraph{Greedy.}
Due to the absence of heuristics developed for the min-max MSPRP, we develop a greedy heuristic as a simple baseline. The heuristic constructs solutions sequentially by assigning each agent logits for selecting a shelf, weighted inversely by its distance from the agent's current position. Similarly, SKUs are chosen with logits proportional to the number of units an agent could potentially pick. Given the logits, the same sequential action selection as described in \Cref{alg:seq-action-selection} is used to generate actions for all agent. Being a stochastic heuristic, we use it to generate 100 different solutions for each test instance and select the best one. 


\paragraph{Hierarchical Attention Model \cite{luttmann2024neural}.}
The Hierarchical Attention Model (HAM) introduces the idea of a hierarchical decoder to generate actions over the decomposed action space of the MDP formulation of the MSPRP. Although HAM was developed to solve the min-sum MSPRP, creating $\NumTours$ one after another, it can be used to solve the min-max MSPRP as well thanks to our assumption, that there are exactly as many pickers as there are tours. In this work, HAM is trained like all other models on the min-max-based reward defined in \Cref{sec:mdp} using the learning method outlined in \Cref{sec:training}.


\paragraph{2d-Ptr \cite{liu20242d}.}
The 2D Array Pointer network (2d-Ptr) addresses the heterogeneous capacitated vehicle routing problem (HCVRP) by using a dual-encoder setup to map vehicles and customer nodes effectively. This approach facilitates dynamic, real-time decision-making for route optimization. Its decoder employs a 2D array pointer for action selection, prioritizing actions over vehicles. The 2d-Ptr can be adapted to solve the min-max MSPRP by using the 2D pointer hierarchically to select shelves and SKUs and by using pickers instead of vehicles.

\paragraph{Equity Transformer (ET) \cite{son2024equity}.}
The Equity-Transformer (ET) approach  \cite{son2024equity} addresses min-max routing problems by employing a sequential planning approach with sequence generators like the Transformer. It focuses on equitable workload distribution among multiple agents, applying this strategy to challenges like the min-max multi-agent traveling salesman and pickup and delivery problems. In our experiments, we modify the agent context in the decoder to the MSPRP setting


\paragraph{PARCO \cite{berto2024parco}.}
PARCO is a recent NCO framework for solving multi-agent CO problems. It uses a multi-pointer mechanism paired with a conflict handler to generate solutions for multiple agents in parallel. It is a versatile framework, which has been applied to different routing and scheduling problems. 






\section{Model And Training Configuration}

In the following, we detail the model and training parameters as well as the parameters for generating the training / test data. Besides that, to ensure proper reproducibility we provide all training details in our publicly available GitHub repository as configuration files. 


\subsection{Instance Generation}
\label{appendix:instance}
For training and evaluating MAHAM and the baselines described above, we use the same instance generation scheme described in \cite{luttmann2024neural}, who generate instances for three warehouse types that differ in the number of available shelves. They generate instances with 10, 25 and 40 shelves referred to as MSPRP10, MSPRP25 and MSPRP40, respectively. While the number of shelves is fixed, the number of demanded SKUs is altered for each warehouse type.

We randomly select the $|\SetOfStorageLocations|$ storage locations from all $|\SetOfSKUs| \times |\mathcal{V}^{\mathrm{R}}|$ possible SKU-shelf combinations and sample the supply from a discrete uniform distribution with mean $\Bar{n}_i$. Likewise, the demand for each SKU is sampled from a discrete uniform distribution with mean $\Bar{d}_p$. Lastly, we clip the demand of an SKU by the warehouse's total supply for it in order to ensure the feasibility of all generated instances. 
Table \ref{tab:instances} summarizes the parameters of the different instances. 

\input{tables/instance_gen_luttmann}




\subsection{Network Hyperparameters}
\label{appendix:network}
To ensure valid and meaningful experiments, the hyperparameters are identical for all models. The size of the embeddings is set to 256 and the number of heads for multi-head attention mechanisms is set to 8. All models use $L=4$ encoder layers, GELU activation functions \cite{hendrycks2016gaussian}, and Layer Normalization \cite{ba2016layernormalization}. To map the different entities of the MSPRP into embedding space, all models use the same features outlined in \Cref{tab:features}.

\input{tables/features}

\subsection{Training Hyperparameters}
\label{appendix:training}
We train all models using the self-improvement method described by \cite{pirnay2024selfimprovement}. To ensure consistency, we use identical hyperparameters and training environments for all neural baselines described in \Cref{appendix:baselines}. All models are trained on a single NVIDIA A100 GPU with 40GB of VRAM. Training spans 50 epochs, with each epoch generating $N=5,000$ independent instances. For each instance, $\alpha=100$ candidate solutions are sampled from the reference-policy $\pi_{\text{best}}$, and the best solution is added to the training dataset.
After all instances are solved by the reference policy $\pi_{\text{best}}$, we draw training samples in mini-batches of $B=2.000$ and determine the cross-entropy loss for the pseudo-optimal actions with respect to the target-policy $\pi$. Adam optimizer with a learning rate of 0.0001 is used to update the parameters of the target-policy, and the trainer class from the RL4CO \cite{berto2023rl4co} library is used to guide the learning process.  

The validation dataset consists of 10,000 independently generated instances per epoch. If the target policy outperforms the reference policy on the validation set, the reference policy is updated, and the training dataset is reset. \Cref{algo:learning} provides a detailed breakdown of these steps.
% Define a light gray color for comments
\definecolor{lightgray}{gray}{0.5}

\begin{algorithm}[h]
   \caption{Self-improvement training for neural CO}
   \label{algo:learning}
   \begin{algorithmic}[1]
      \REQUIRE $\mathcal{X}$: distribution over problem instances; $f_*$: objective function
      \REQUIRE $N$: number of instances to sample in each epoch
      \REQUIRE $\alpha$: number of sequences to sample for each instance
      \REQUIRE $\textsc{Validation} \sim \mathcal{X}$: validation dataset
      \STATE Randomly initialize policy $\pi_\theta$
      \STATE $\pi_{\text{best}} \gets \pi_\theta$
      \STATE $\textsc{Dataset} \gets \emptyset$
      \FOR{epoch}
          \STATE Sample set of $n$ problem instances $\textsc{Instances} \sim \mathcal{X}$
          \FOR{each $x \in \textsc{Instances}$}
              
              \STATE \textcolor{lightgray}{// Sample set of $m$ feasible solutions}
              \STATE $A := \{\bm a_{1:T}^{(1)}, \dots, \bm a_{1:T}^{(m)}\} \sim \pi_{\text{best}}$
              \STATE \textcolor{lightgray}{// Add best solution to training dataset}
              \STATE $\textsc{Dataset} \gets \textsc{Dataset} \cup \{(x, \arg\max_{\bm a_{1:T} \in A} f_x(\bm a_{1:T}))\}$ 
          \ENDFOR
          \FOR{batch}
              \STATE \textcolor{lightgray}{// Sample $B$ instances and partial solutions from \textsc{Dataset}}
              \STATE $\{(x_j, \bm{a}^{(j)}_{1:d_j})\}_{j=1}^B \sim \textsc{Dataset}, \qquad \{d_j\}_{j=1}^B \sim \mathcal{U}(1, T-1)$ 
              \STATE \textcolor{lightgray}{// Minimize batch-wise cross entropy loss}
              \STATE $\mathcal{L}_\theta = - \frac{1}{B}\sum_{j=1}^B \log \pi_\theta \left( a^{(j)}_{d_{j+1}} | \bm a^{(j)}_{1:d_j} \right)$
          \ENDFOR
          \IF{greedy performance of $\pi_\theta$ on \textsc{Validation} better than $\pi_{\text{best}}$}
              \STATE \textcolor{lightgray}{// update best policy}
              \STATE $\pi_{\text{best}} \gets \pi_\theta$ 
              \STATE \textcolor{lightgray}{// Empty Training Dataset}
              \STATE $\textsc{Dataset} \gets \emptyset$ 
          \ENDIF
      \ENDFOR
   \end{algorithmic}
\end{algorithm}


