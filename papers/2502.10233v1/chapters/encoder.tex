\subsubsection{Problem Encoder.}
\label{sec:encoder}



As defined earlier, the min-max MSPRP can be represented as a heterogeneous graph with agents, packing stations, shelves and SKUs posing different node types. We follow \cite{luttmann2024neural} and first project these different node-types from their distinct feature spaces into a mutual embedding space of dimensionality $\embdim$ using type-specific transformations $W_{\phi_i}$ for node $i$ of type $\phi_i$. The features used to represent agents, stations, shelves and SKUs in the features space are listed in \Cref{tab:features} in \Cref{appendix:network}.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/maham.drawio.pdf}
    \vspace{-5mm}
    \caption{Overview of the MAHAM Architecture}
    \vspace{-5mm}
    \label{fig:maham}
\end{figure*}


Also similar to \cite{luttmann2024neural}, we use several layers of self- and cross-attention between location and SKU nodes. To this end, we treat packing stations as shelves that store zero units for each SKU and concatenate their initial embeddings to those of the shelf nodes, yielding $H^{0}_\mathcal{V} = [H^{0}_{\SetOfPackingStations} || H^{0}_{\SetOfShelves}]$. Likewise, the initial SKU embeddings are denoted as $H^{0}_\mathcal{P}$. While self-attention is applied independently to shelf and SKU embeddings following the Transformer architecture \cite{vaswaniAttentionAllYou2017a}, cross-attention allows shelves and SKUs to influence each other’s embeddings. Consequently, shelf embeddings encode information about the SKUs they store, and SKU embeddings reflect their placement within the storage area -- an essential property for hierarchical action selection. To perform cross-attention we compute a single matrix of attention scores $A$ using shelf embeddings as queries $Q$ and SKU embeddings as keys $K$. This contrasts with the MatNet \cite{kwonMatrixEncodingNetworks2021a} and HAM \cite{luttmann2024neural} architectures, which compute separate attention scores for each node type—once as queries and once as keys. Formally we perform:
\begin{align}
    \label{eq:matnet_dot_score}
    A &= \frac{QK^\top}{\sqrt{d_k}}, \qquad Q = W^Q H_{\mathcal{V}}^{l-1}, \qquad  K = W^K H_{\mathcal{P}}^{l-1}
\end{align}
where $W^Q$ and $W^K \in \mathbb{R}^{d_k \times \embdim}$ are weight matrices learned per attention head\footnote{For succinctness, we omit the layer and head enumeration} and $d_k$ is the per-head embedding dimension. The resulting attention scores $A\in \mathbb{R}^{|\mathcal{V}| \times |\mathcal{P}|}$ can be interpreted as the (learned) influence of an SKU $p$ on the embedding of location $v$. 
Similar to MatNet \cite{kwonMatrixEncodingNetworks2021a} we fuse these learned attention scores with the supply-matrix $E \in \mathbb{R}^{|\mathcal{V}| \times |\mathcal{P}|}$, which specifies how many units of SKU $p$ are stored in location $v$. To this end, we concatenate the attention score and the matrix of storage quantities and feed the resulting score vector through a multi-layer perceptron $\text{MLP}: \mathbb{R}^{|\mathcal{V}| \times |\mathcal{P}| \times 2} \rightarrow \mathbb{R}^{|\mathcal{V}| \times |\mathcal{P}|}$, with a single hidden layer comprising of $\embdim$ units and GELU activation function \cite{hendrycks2016gaussian}. Further, we pass the transpose of the attention scores and of the supply matrix $A^\top, \, E^\top \in \mathbb{R}^{|\mathcal{P}| \times |\mathcal{V}|}$ through a second MLP to obtain the influence $A_{\mathcal{P} \rightarrow \mathcal{V}}$ of locations $v$ on the SKU embeddings $H_{\mathcal{P}}$:
%
\begin{align}
    \label{eq:matnet_mixed_score}
     A_{\mathcal{V} \rightarrow \mathcal{P}} = \mathrm{MLP}_{\mathcal{V}} \big ([A || E ] \big ), \quad  A_{\mathcal{P} \rightarrow \mathcal{V}} = \mathrm{MLP}_{\mathcal{P}} \big ([A^\top || E^\top ] \big ),
\end{align}
%
By avoiding to compute the (computationally expensive) attention scores twice, once to generate shelf embeddings and once for the SKU embeddings, our implementation of the cross-attention mechanism leverages parameter sharing, improving both efficiency and generalization performance, as demonstrated in \Cref{sec:exp}. 
The resulting attention scores are then used to compute the embeddings for the nodes of the respective type: 
%
\begin{align}
     H_{\mathcal{V}}^\prime &= \text{softmax}(A_{\mathcal{V} \rightarrow \mathcal{P}})V_\mathcal{P}, \quad V_\mathcal{P} = W^{V}_\mathcal{P} H_{\mathcal{P}}^{l-1} \\
     H_{\mathcal{P}}^\prime &= \text{softmax}(A_{\mathcal{P} \rightarrow \mathcal{V}})V_\mathcal{V}, \quad V_\mathcal{V} = W^{V}_\mathcal{V} H_{\mathcal{V}}^{l-1}
\end{align}
%
As in \cite{vaswaniAttentionAllYou2017a}, $H_{\mathcal{V}}^\prime$ and $H_{\mathcal{P}}^\prime$  are then augmented through skip connections, layer normalization, and a feed-forward network, yielding the location and SKU embeddings $H_{\mathcal{V}}^l$ and $H_{\mathcal{P}}^l$, respectively, of the current layer $l$. 
%
% \begin{figure*}[!htb]
%     \centering
%     \includegraphics[width=0.8\textwidth]{figures/cntxt_encoder.drawio.pdf}
%     \vspace{-4mm}
%     \caption{Agent Context Encoder}
%     \label{fig:cntxt}
% \end{figure*}



\setlength{\intextsep}{10pt}%
\begin{wrapfigure}{r}{0.6\textwidth}
  \begin{center}
    \vspace{-20pt}
    \includegraphics[width=0.6\textwidth]{figures/cntxt_encoder.drawio.pdf}
    \vspace{-35pt}
  \end{center}
  \caption{Agent Context Encoder}
  \label{fig:cntxt}
\end{wrapfigure}

\subsubsection{Agent Encoder.}
To account for multiple agents, we introduce an Agent Context Encoder, as illustrated in \Cref{fig:cntxt}, into our MAHAM architecture. This encoder leverages the embeddings $H_\mathcal{V}$ and $H_\mathcal{P}$ from the problem encoder, along with the current state $s_t$, to generate embeddings for each picker.  
To facilitate informed decision-making at each decoding step, the agent embeddings incorporate three key types of information. First, spatial information of pickers is captured by using the embedding of a picker's current location. 
Further, the remaining capacity and the length of an agent's current tour are included in the agent encoder, helping the model to determine whether to continue the tour or send the picker to a packing station.
Lastly, the total demand across all SKUs and the average-pooled SKU embeddings provide insights into the remaining workload. 
Each context feature is first projected into a shared embedding space of dimensionality $\embdim$. The resulting embeddings are then concatenated and passed through an MLP, ensuring that the final representation is mapped back to the original embedding space.

Since coordination between pickers is critical in the min-max MSPRP, we add a Multi-Head-Self-Attention (MHSA) layer \cite{vaswaniAttentionAllYou2017a} at the end of the Agent Context Encoder, which enables message passing between agents. As in \cite{vaswaniAttentionAllYou2017a}, we add a positional encoding to the agent embeddings before they enter the MHSA layer. However, given the absence of a natural ordering of pickers, we employ a Ranking-based Position Encoding, where pickers are ranked in descending order of their remaining capacity. This allows the encoder to better prioritize agents based on their current workload, which is crucial for the sequential action selection that will be described in \Cref{sec:seq_action_selection}. We denote the final agent embeddings as $H_\SetOfAgents$ and the set of all embeddings as $H=(H_\mathcal{V},H_\mathcal{P},H_\SetOfAgents)$.

