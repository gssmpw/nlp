


Given the embeddings for warehouse locations, SKUs, and agents from the encoder, the decoder determines the next location to visit by the pickers as well as the SKUs to be picked there. In contrast to other architectures like \cite{son2024equity}, our approach generates trajectories for all $\numagents$ simultaneously. This way, the agents can coordinate and balance the workload. If, for example, in step $t$ the tour of picker $m=1$ is much longer than that of picker $m=2$, the agents can coordinate that $m=2$ picks an SKU that is only available in far away shelves. This kind of coordination is not possible in purely autoregressive settings, where agent trajectories are constructed one after another. 

For our parallel and hierarchical decoding scheme, we adopt the hierarchical decoder architecture from \cite{luttmann2024neural} to sample actions specifying the next locations $v^{\agentidx}_{t}$ to visit and the SKU $p^{\agentidx}_{t}$  to pick by agents $\agentidx=1,\ldots \numagents$. To this end, we define two decoders $\shelfPolicy: \mathcal{S} \rightarrow \mathcal{V}$ and $\SKUPolicy: \mathcal{S} \rightarrow \mathcal{P}$ for action subspaces $\setOfNodesMDP$ and $\SetOfSKUs$, respectively. Moreover, we define a partial transition function $s_t^\prime = \Gamma_\circ(s_t, \bm{v}_t)$, generating an intermediate state $s_t^\prime$ with updated location information. The decoders can then be used in a hierarchical manner to generate the joint probability of a shelf-SKU pair according to the chain rule of probability \cite{luttmann2024neural}:
%
\begin{align}
    \label{eq:multi-agent-policy}
    g_\theta (\bm{a}_t \, | \, s_t, H) = \shelfPolicy (\bm{v}_t \, | \,  s_t, H) \cdot \SKUPolicy (\bm{p}_t \, | \, s_t^\prime, H),
\end{align}
%
where $\bm{v}_t$ and $\bm{p}_t$ are the joint agent actions for the shelf and SKU sub-action spaces, respectively. MAHAM models the joint agent actions $\bm{a}_{dt}$ for each sub-action space $d$ and the corresponding decoder $g_\theta^d$ as an autoregressive sequence generation process, similar to \Cref{eq:autoregressive_encoding_decoding}:
%
\begin{align}
\label{eq:maham}
p_\theta(\bm{a}_{dt} \, | s_t, \, H) &= g^d_\theta(\logits_d \, | \, s_t, H) \cdot \prod_{i=1}^{M} \psi \left( a^{\PermOverL_{i}}_{dt} | \logits_d, \bm{a}^{\PermOverL_{1:i-1}}_{dt} \right)
\end{align}
%
where $\logits_d \in \mathbb{R}^{\numagents \times |\mathcal{A}_d|}$ are the unnormalized log-probabilities (henceforth logits) over the joint action space generated by sub-policy $g_\theta^d$ and $\PermOverL$ is a permutation over agents given $\logits_d$. Further, $\psi$ is a stochastic function, which autoregressively selects actions based on the logits $L$ as well as the action sequence $\bm{a}^{\PermOverL_{1:i-1}}_{dt}$ of preceding agents.
%, i.e. $\bm{a}^{\PermOverL_{1:i-1}}_{dt}=(a^{\PermOverL_{1}}_{dt},\ldots,a^{\PermOverL_{i-1}}_{dt})$.
Note, that while the policy $p_\theta$ itself acts autoregressively according to the sequential action selection strategy $\psi$, \Cref{eq:maham} factors out the computationally expensive calculation of the log-probabilities, which is done in parallel for all agents, allowing an effective and efficient agent coordination and ranking. Therefore, both the shelf and SKU decoder are modifications of the AM decoder proposed by \cite{kool2018attention}, which uses the cross-attention mechanism to generate unnormalized log-probabilities $\bm{l} \in \mathbb{R^{|\mathcal{A}|}}$. However, in contrast to \cite{kool2018attention}, who use a single context vector as query, our architecture uses the agent embeddings $H_{\SetOfAgents} \in \mathbb{R}^{\numagents \times D}$ in the cross-attention mechanism: 
%
\begin{align}
    \label{eq:attn_dec}
    Q_d &= \text{Attn}(H_{\SetOfAgents} W^Q, \, H_d W^K, \, H_d W^V) \\
    \logits_d &= C \cdot tanh \left ( \frac{Q_d K_d^\top}{\sqrt{\embdim}}  \right )
\end{align}
%
where $C$ is a scale parameter, $K_d$ is a projection of the embeddings $H_d$ belonging to the sub-action space $d$ of the decoder. In the following we will show how we can use the logits of the joint action space $\logits_d \in \mathbb{R}^{\numagents \times |\mathcal{A}_d|}$ to generate feasible actions $\bm{a}_{dt}=(a^{1}_{dt},\ldots,a^{\numagents}_{dt})$ for all agents for the current sub-action space $d$.


\subsection{Sequential Action Selection from Joint Logit-Space}
\label{sec:seq_action_selection}


Given the logits of the joint action space $\logits_d \in \mathbb{R}^{\numagents \times |\mathcal{A}_d|}$ for subspace $d$, we iteratively select actions for each agent using common decoding strategies, such as greedy selection or sampling.
%\footnote{Note that greedy selection can be turned on in \Cref{alg:seq-action-selection} by setting $\beta$ close to 0.} 
To ensure feasibility, joint agent actions are initialized with a set of feasible default actions. For picker locations, the default action is to remain at the current position. Additionally, we introduce a dummy SKU that serves as the default SKU action and can be always selected. 
%This dummy SKU is also the only option available for the SKU decoder when an agent is at a packing station.

After each agent received a default action, we iteratively refine the agent actions based on the logits $\logits$. Therefore, we first mask infeasible actions in $\logits$ by setting their values to negative infinity. The masked logits are then converted into a single probability distribution over both agents and actions, rather than creating separate distributions per agent. This approach allows the policy to implicitly learn the ranking $\PermOverL$ by assigning higher logits to agents that should act first. As a result, agents with greater confidence select actions earlier, while less confident agents act later. This step is crucial, as an agent's action can constrain the action space of others, and the order of selection directly affects the picking quantity $y^m$, as detailed in \Cref{sec:mdp}.

Given the probability distribution over the joint action space, a single action (i.e. agent-action combination) $a^m$ is drawn. As a consequence, all actions of the chosen agent $m$ are marked as infeasible. Further, more actions can be masked based on the actions taken so far using a sub-action specific masking function $\xi_d$. 
This way, we avoid that multiple agents select the same shelf- and SKU-combination as required per our MMDP formulation in \Cref{sec:mdp}.
%For shelf-selection, we choose to mask all shelves that have already been selected in the current step by other agents. While not strictly necessary, as described in the MMDP formulation in \Cref{sec:mdp}, we found this restriction to lead to better results and faster convergence. For SKUs on the other hand, we mask SKUs whose demand has already been satisfied by preceding agents. 
In the next iteration, the logits are computed with the updated action mask and the process repeats until no more actions can be selected (i.e., when all actions are marked infeasible). \Cref{alg:seq-action-selection} formally describes this process. 


\definecolor{lightgray}{gray}{0.5}

\begin{algorithm}[t]
\caption{Sequential Action Selection from Joint Logit-Space}
\label{alg:seq-action-selection}
\begin{spacing}{1.0}  % Increase line spacing inside the algorithm
\begin{algorithmic}[1]
\REQUIRE Logits $\logits \in \mathbb{R}^{\numagents \times |\mathcal{A}_d|}$, Binary Action Mask $\mathbf{M} \in \mathbb{R}^{\numagents \times |\mathcal{A}_d|}$, Mask Update Function $\xi_d$, Temperature $\beta$, default action $\bm{a}^\prime$ (e.g. $\bm{a}^\prime \equiv \bm{a}_{t-1}$)
\ENSURE Feasible agent actions $\bm{a} \in \mathbb{N}^\numagents$
\STATE $\bm{a} \gets \bm{a}^\prime$ \hfill \textcolor{lightgray}{\COMMENT{Initialize agent actions}}
%\STATE $\mathcal{M} \gets \{1,\ldots,\numagents\}$ \hfill \textcolor{lightgray}{\COMMENT{Initialize set of all agents}}
\WHILE{not all elements in \( \textbf{M} \) are 1}
    \STATE $\logits^\prime \gets \logits -  \textbf{M} * \infty$ \hfill \textcolor{lightgray}{\COMMENT{Mask infeasible actions}}
    \vspace{1mm}
    \STATE $P_{\agentidx a} \gets \frac{\text{exp}(\logits^\prime_{ma} / \beta)}{\sum_{i\in \SetOfAgents} \sum_{j\in \mathcal{A}_d} \exp(\logits^\prime_{ij} / \beta)} \quad \forall \, \agentidx \in \SetOfAgents, a \in \mathcal{A}_d$ \hfill \textcolor{lightgray}{\COMMENT{Normalize}} \vspace{1mm}
    \STATE $a^\agentidx \sim \text{Categorical}(P)$ \hfill \textcolor{lightgray}{\COMMENT{Sample a single agent's action}}
    \STATE $\bm{a}[\agentidx] = a^\agentidx$ \hfill \textcolor{lightgray}{\COMMENT{Update vector of actions}}
    \STATE \( \textbf{M}[\agentidx, :] = 1 \) \hfill  \textcolor{lightgray}{\COMMENT{Mark all actions of agent \( \agentidx \) infeasible}}
    \STATE $\textbf{M} \gets \xi_d(\textbf{M}, \bm{a})$ \hfill \textcolor{lightgray}{\COMMENT{Update Mask according to subspace specific logic}}
    %\STATE $\mathcal{M} \gets \mathcal{M} \setminus \agentidx$ \hfill \textcolor{lightgray}{\COMMENT{Remove agent from set of remaining agents}}
\ENDWHILE
\end{algorithmic}
\end{spacing}
\end{algorithm}