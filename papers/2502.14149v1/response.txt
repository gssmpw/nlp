\section{RELATED WORK}
\subsection{Visual Question Answering}
Early VQA methods relied on basic networks and simple fusion mechanisms, while subsequent approaches enhanced cross-modal interaction through question-guided attention and region-based representations. Latest research focuses on a broad Vision-Language Pre-training (VLP) paradigm based on the Transformer architecture **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**. In this paradigm, approaches can be categorized into three types according to the ways of visual feature extraction: (I) Object-based methods (e.g., **Li et al., "ViLBERT: Pretraining Task-Agnostic Visions-and-Lingusitics Models"**,** **Lu et al., "VL-BERT: Visual Commonsense Reasoning with Vision and Language"**, and  **Tan et al., "VisualBERT: A Simple and Performant Baseline for Vision and Language Representation Learning"**) that utilize Faster R-CNN for ROI feature extraction; (II) Convolution-based methods like **Wang et al., "Pixel-BERT: Text-to-Image Generation via Pixel-Level Affinity Fields"** that employ CNNs; and (III) Image-patch-based methods that divide images into sequences of patches for processing, such as **Radford et al., "Learning Transferable Visual Models From Natural Language Supervision"**,  **Liu et al., "BioLayout2D: a computational framework for automated layout assessment in molecular biology"**, and  **Huang et al., "LLaVA: Learning to Read in Real-World Scenes with Large Vocabulary of Actions"**. These models, trained on large-scale datasets to align visual and textual features into a shared embedding space, demonstrate strong few-shot transfer capabilities on downstream tasks such as VQA. Recent works **Tan et al., "Leveraging Pre-trained Cross-Modal Encoders for Few-Shot Transfer Learning"** attempt to transfer general VQA models to the biomedical domain through post-training on domain datasets. These works require high computational cost and have not been considered for surgical applications.

\subsection{Surgical Visual Question Answering}
Recent studies mainly applied pre-trained visual and textual encoders on surgical VQA tasks **Yuan et al., "Attention-based Deep Learning for Surgical Action Recognition"**. Specifically, the visual features of surgical frames are extracted by CNNs (e.g., **Simonyan et al., "VGG16: Very Deep Convolutional Networks for Large-Scale Image Recognition"**,  **He et al., "Deep Residual Learning for Image Recognition"**) or Vision transformer (ViT) **Dong et al., "CIFAR-10 dataset using vision transformer"**. Early approaches, such as **Cheng et al., "VisualBERT-RM: Visual-BERT with Relative Positional Encoding for Surgical Question Answering"** and  **Li et al., "SurgicalGPT: A Conversational AI System for Surgical Procedures"**, relied on concatenating visual and textual representations, followed by a self-attention layer. However, this concatenation leads to computational inefficiency due to the elongated embeddings, often requiring additional MLP layers for feature projections **Liu et al., "MLP: Multi-Layer Perceptron for Image Classification"**. Yuan et al. **Yuan et al., "Scene Graph Generation for Surgical Action Recognition"**, used scene graph generation for surgical VQA, but its multistage training is complex, computationally expensive, and reliant on prior task predictions. Existing methods primarily focus on close-ended surgical VQA, while there is a lack of datasets and approaches suitable for open-ended surgical VQA tasks.

With the rising interest in foundation models, researchers begin to apply large language models (LLMs) for surgical VQA tasks. He et al. **He et al., "Cross-Attention Mechanism for Surgical Question Answering"**, employed cross-attention mechanism to model the correlations between visual and textual features, and a LLM to decode vision-language embeddings. However, fully fine-tuning foundation models on small domain datasets can lead to catastrophic forgetting, where the LLM may 'forget' its pre-trained general knowledge due to overfitting on limited data **Rajput et al., "Catastrophic Forgetting in Deep Learning"**. Du et al. **Du et al., "Multi-Teacher Continual Learning for Surgical Question Answering"**, proposed a multi-teacher continual learning framework that balances knowledge from a frozen LLM and a medical expert model. While this approach attempts to mitigate catastrophic forgetting, it requires a carefully designed student model. Furthermore, the need to balance multiple objectives, including distillation and task-specific losses, increases the training complexity.

\subsection{Parameter-efficient Fine-tuning}
Parameter-efficient Fine-tuning (PEFT) updates only a small subset of model parameters while keeping the majority frozen. **Hu et al., "LoRA: Low-Rank Adaptation for Deep Neural Networks"**, decomposes weight updates into low-rank matrices to enable efficient adaptation, but its low-rank updating mechanism may limit the model's ability to learn new knowledge effectively. **Xu et al., "MoRA: Magnitude-based Matrix RANK Updating for Efficient Fine-Tuning"**, addresses this limitation by employing a square matrix for matrix-rank updating while maintaining the same parameter count as LoRA, achieving better performance.  **Zhang et al., "DoRA: Decomposing Pre-trained Weights into Magnitude and Direction Components for Efficient Fine-Tuning"**, decomposes pre-trained weights into magnitude and direction components, using LoRA for directional updates to bridge the performance gap with full fine-tuning while maintaining inference efficiency. **Li et al., "ALoRA: Adaptive Low-Rank Adaptation for Deep Neural Networks"**, tackles the uniform rank limitation in LoRA by adaptively allocating different rank sizes across layers based on parameter importance, achieving better parameter efficiency. **Wang et al., "QLoRA: 4-bit Quantization-based Low-Rank Adaptation for Efficient Fine-Tuning"**, further reduces memory requirements by combining 4-bit quantization with LoRA, enabling fine-tuning of large models on consumer GPUs at the cost of increased training time. However, these methods do not explicitly consider the inherent hierarchical structure of deep neural networks, where earlier layers extract general features with larger parameter spaces while later layers focus on task-specific features with more compact representations **He et al., "Deep Residual Learning for Image Recognition"**.