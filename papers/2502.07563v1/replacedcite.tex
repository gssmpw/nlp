\section{Related Work}
\subsubsection{Linear Sequence Modeling}

\textbf{Linear Attention.} Vanilla linear attention____ introduces the use of kernel methods as a replacement for the $\operatorname{Softmax}$ attention____, thereby reducing the computational complexity to linear in sequence length. Following this, several variants of linear attention have been proposed. TransNormerLLM____ proposes Lightning Attention, a refined linear attention mechanism that accelerates processing by optimizing IO interactions. Lightning Attention-2____ further realizes the theoretical advantages of linear attention by separately handling inter- and intra-block computations. RetNet____ introduces a retention mechanism that combines recurrence with attention, benefiting from both parallel training and linear inference. Gated Linear Attention (GLA)____ incorporates a data-independent gating mechanism into the linear attention framework, and presents an efficient algorithm for training. DeltaNet____ and its parallelized version____ use a delta rule-like update to enhance linear attention performance in long-context scenarios. Finally, Gated Slot Attention (GSA)____, inspired by GLA, introduces a gated linear attention mechanism with bounded-memory slot control to further improve efficiency.

\textbf{State Space Modeling.} The SSM serves as a powerful framework for representing the behavior of sequences within dynamic systems, and it has shown considerable promise in the realm of linear sequence modeling. Mamba ____ incorporates a mechanism for selecting states, thereby facilitating the scaling of linear sequence lengths. This architecture has been further enhanced in Mamba-2 ____, where the introduction of the state space duality (SSD) framework optimizes its performance.

\textbf{Linear RNN.} Traditional RNNs face significant challenges in handling long-context sequence modeling, primarily due to their inherent sequence dependency during training, which prevents them from fully capitalizing on scaling laws____. To address these limitations, RWKV____ was introduced as a linear RNN-based large language model that aims to efficiently manage long-term dependencies. Additionally, HGRN____ highlights the critical role of data-dependent decay mechanisms in enhancing linear RNN performance, demonstrating how adjustments to decay parameters can improve learning in long-context tasks. An enhanced version, HGRN2____, expands on this approach by incorporating a state expansion mechanism that utilizes outer product operations, which allows for greater scalability and improved modeling capabilities over extended sequences. Both RWKV and HGRN series seek to overcome weaknesses of RNNs for efficient long-sequence modeling.

% \vspace{-2mm}
\subsubsection{Sequence Parallelism}
% \vspace{-2mm}
SP ____ is a distributed technology designed for training language models more efficiently, which is implemented by dividing a long sequence into multiple shorter subsequences and processing these subsequences in parallel on multiple computing devices. Existing SP methods ____ whose parallelism degree cannot exceed the number of attention heads, which limits their scalability. Ring Attention ____ is proposed to address high memory cost in long sequence modeling by distributing subsequences across different devices and overlapping the communication of KV blocks. LASP ____ proposes a new linear attention-tailored SP strategy based on GPU friendly implementation by utilizing a P2P ring-style communication strategy, but still lacks of optimizations for hybrid model architecture.