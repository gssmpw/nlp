\section{Related Work}
\subsubsection{Linear Sequence Modeling}

\textbf{Linear Attention.} Vanilla linear attention~\citep{katharopoulos2020transformers} introduces the use of kernel methods as a replacement for the $\operatorname{Softmax}$ attention~\citep{vaswani2017attention}, thereby reducing the computational complexity to linear in sequence length. Following this, several variants of linear attention have been proposed. TransNormerLLM~\citep{qin2023scaling,qin2023transnormerllm} proposes Lightning Attention, a refined linear attention mechanism that accelerates processing by optimizing IO interactions. Lightning Attention-2~\citep{qin2024lightning} further realizes the theoretical advantages of linear attention by separately handling inter- and intra-block computations. RetNet~\citep{sun2023retentive} introduces a retention mechanism that combines recurrence with attention, benefiting from both parallel training and linear inference. Gated Linear Attention (GLA)~\citep{yang2023gated} incorporates a data-independent gating mechanism into the linear attention framework, and presents an efficient algorithm for training. DeltaNet~\citep{Schlag2021LinearTA} and its parallelized version~\citep{yang2024parallelizing} use a delta rule-like update to enhance linear attention performance in long-context scenarios. Finally, Gated Slot Attention (GSA)~\citep{zhang2024gsa}, inspired by GLA, introduces a gated linear attention mechanism with bounded-memory slot control to further improve efficiency.

\textbf{State Space Modeling.} The SSM serves as a powerful framework for representing the behavior of sequences within dynamic systems, and it has shown considerable promise in the realm of linear sequence modeling. Mamba \citep{gu2023mamba} incorporates a mechanism for selecting states, thereby facilitating the scaling of linear sequence lengths. This architecture has been further enhanced in Mamba-2 \citep{dao2024transformers}, where the introduction of the state space duality (SSD) framework optimizes its performance.

\textbf{Linear RNN.} Traditional RNNs face significant challenges in handling long-context sequence modeling, primarily due to their inherent sequence dependency during training, which prevents them from fully capitalizing on scaling laws~\citep{sun2023retentive}. To address these limitations, RWKV~\citep{peng-etal-2023-rwkv,peng2024eagle} was introduced as a linear RNN-based large language model that aims to efficiently manage long-term dependencies. Additionally, HGRN~\citep{qin2024hierarchically} highlights the critical role of data-dependent decay mechanisms in enhancing linear RNN performance, demonstrating how adjustments to decay parameters can improve learning in long-context tasks. An enhanced version, HGRN2~\citep{qin2024hgrn2}, expands on this approach by incorporating a state expansion mechanism that utilizes outer product operations, which allows for greater scalability and improved modeling capabilities over extended sequences. Both RWKV and HGRN series seek to overcome weaknesses of RNNs for efficient long-sequence modeling.

% \vspace{-2mm}
\subsubsection{Sequence Parallelism}
% \vspace{-2mm}
SP \citep{li2022sequence} is a distributed technology designed for training language models more efficiently, which is implemented by dividing a long sequence into multiple shorter subsequences and processing these subsequences in parallel on multiple computing devices. Existing SP methods \citep{korthikanti2022reducing,jacobs2023deepspeed} whose parallelism degree cannot exceed the number of attention heads, which limits their scalability. Ring Attention \citep{liu2023ring} is proposed to address high memory cost in long sequence modeling by distributing subsequences across different devices and overlapping the communication of KV blocks. LASP \citep{sun2024linear} proposes a new linear attention-tailored SP strategy based on GPU friendly implementation by utilizing a P2P ring-style communication strategy, but still lacks of optimizations for hybrid model architecture.