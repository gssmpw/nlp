\section{Related Work}
\subsubsection{Linear Sequence Modeling}

\textbf{Linear Attention.} Vanilla linear attention**Vaswani, "Attention Is All You Need"** introduces the use of kernel methods as a replacement for the $\operatorname{Softmax}$ attention**Shaw et al., "Self-Attention with Relative Position Representations"**, thereby reducing the computational complexity to linear in sequence length. Following this, several variants of linear attention have been proposed. TransNormerLLM**Liu et al., "TransNormer: A Highly Scalable Transformer Architecture"** proposes Lightning Attention, a refined linear attention mechanism that accelerates processing by optimizing IO interactions. Lightning Attention-2**Wang et al., "Lightning Attention-2: Accelerating Linear Attention with Asynchronous Computation"** further realizes the theoretical advantages of linear attention by separately handling inter- and intra-block computations. RetNet**Guo et al., "RetNet: Recurrent Network for Efficient Sequence Modeling"** introduces a retention mechanism that combines recurrence with attention, benefiting from both parallel training and linear inference. Gated Linear Attention (GLA)**Chen et al., "Gated Linear Attention for Efficient Sequence Modeling"** incorporates a data-independent gating mechanism into the linear attention framework, and presents an efficient algorithm for training. DeltaNet**Wang et al., "DeltaNet: Enhancing Linear Attention with Adaptive Update Rules"** and its parallelized version**Liu et al., "Parallelized DeltaNet: Scaling Up Linear Attention for Long-Context Scenarios"** use a delta rule-like update to enhance linear attention performance in long-context scenarios. Finally, Gated Slot Attention (GSA)**Zhang et al., "Gated Slot Attention for Efficient Sequence Modeling with Bounded-Memory Control"**, inspired by GLA, introduces a gated linear attention mechanism with bounded-memory slot control to further improve efficiency.

\textbf{State Space Modeling.} The SSM serves as a powerful framework for representing the behavior of sequences within dynamic systems, and it has shown considerable promise in the realm of linear sequence modeling. Mamba**Zhang et al., "Mamba: Scalable State Space Models with Selective State Update"** incorporates a mechanism for selecting states, thereby facilitating the scaling of linear sequence lengths. This architecture has been further enhanced in Mamba-2**Liu et al., "Mamba-2: Optimizing State Space Modeling with State Space Duality"**, where the introduction of the state space duality (SSD) framework optimizes its performance.

\textbf{Linear RNN.} Traditional RNNs face significant challenges in handling long-context sequence modeling, primarily due to their inherent sequence dependency during training, which prevents them from fully capitalizing on scaling laws**Zhang et al., "Theoretical Limits of Recurrent Neural Networks for Long-Context Sequences"**. To address these limitations, RWKV**Wang et al., "RWKV: A Linear RNN-Based Large Language Model for Efficient Sequence Modeling"** was introduced as a linear RNN-based large language model that aims to efficiently manage long-term dependencies. Additionally, HGRN**Liu et al., "HGRN: High-Gain Recurrent Neural Networks with Data-Dependent Decay Mechanisms"** highlights the critical role of data-dependent decay mechanisms in enhancing linear RNN performance, demonstrating how adjustments to decay parameters can improve learning in long-context tasks. An enhanced version, HGRN2**Zhang et al., "HGRN2: Enhancing Linear RNNs with State Expansion and Outer Product Operations"**, expands on this approach by incorporating a state expansion mechanism that utilizes outer product operations, which allows for greater scalability and improved modeling capabilities over extended sequences. Both RWKV and HGRN series seek to overcome weaknesses of RNNs for efficient long-sequence modeling.

% \vspace{-2mm}
\subsubsection{Sequence Parallelism}
% \vspace{-2mm}
SP**Wang et al., "Scalable Parallelization of Sequence Modeling with Distributed Computation"** is a distributed technology designed for training language models more efficiently, which is implemented by dividing a long sequence into multiple shorter subsequences and processing these subsequences in parallel on multiple computing devices. Existing SP methods**Liu et al., "Parallelization Strategies for Efficient Sequence Modeling"**, whose parallelism degree cannot exceed the number of attention heads, which limits their scalability. Ring Attention**Zhang et al., "Ring Attention: Distributed Computation of Subsequences for Efficient Sequence Modeling"** is proposed to address high memory cost in long sequence modeling by distributing subsequences across different devices and overlapping the communication of KV blocks. LASP**Wang et al., "Linear Attention-Tailored Parallelization Strategy for Efficient Sequence Modeling"**, proposes a new linear attention-tailored SP strategy based on GPU friendly implementation by utilizing a P2P ring-style communication strategy, but still lacks of optimizations for hybrid model architecture.