@inproceedings{Schlag2021LinearTA,
  title={Linear Transformers Are Secretly Fast Weight Programmers},
  author={Imanol Schlag and Kazuki Irie and J{\"u}rgen Schmidhuber},
  booktitle={International Conference on Machine Learning},
  year={2021}
}

@article{dao2024transformers,
  title={Transformers are {SSM}s: Generalized models and efficient algorithms through structured state space duality},
  author={Dao, Tri and Gu, Albert},
  journal={arXiv preprint arXiv:2405.21060},
  year={2024}
}

@article{gu2023mamba,
  title={Mamba: Linear-time sequence modeling with selective state spaces},
  author={Gu, Albert and Dao, Tri},
  journal={arXiv preprint arXiv:2312.00752},
  year={2023}
}

@misc{jacobs2023deepspeed,
      title={DeepSpeed {U}lysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models}, 
      author={Sam Ade Jacobs and Masahiro Tanaka and Chengming Zhang and Minjia Zhang and Shuaiwen Leon Song and Samyam Rajbhandari and Yuxiong He},
      year={2023},
      eprint={2309.14509},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{katharopoulos2020transformers,
  title={Transformers are {RNN}s: Fast autoregressive transformers with linear attention},
  author={Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Fran{\c{c}}ois},
  booktitle={International Conference on Machine Learning},
  pages={5156--5165},
  year={2020},
  organization={PMLR}
}

@misc{korthikanti2022reducing,
      title={Reducing Activation Recomputation in Large Transformer Models}, 
      author={Vijay Korthikanti and Jared Casper and Sangkug Lym and Lawrence McAfee and Michael Andersch and Mohammad Shoeybi and Bryan Catanzaro},
      year={2022},
      eprint={2205.05198},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{li2022sequence,
      title={Sequence Parallelism: Long Sequence Training from System Perspective}, 
      author={Shenggui Li and Fuzhao Xue and Chaitanya Baranwal and Yongbin Li and Yang You},
      year={2022},
      eprint={2105.13120},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{liu2023ring,
      title={Ring Attention with Blockwise Transformers for Near-Infinite Context}, 
      author={Hao Liu and Matei Zaharia and Pieter Abbeel},
      year={2023},
      eprint={2310.01889},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{peng-etal-2023-rwkv,
    title = "{RWKV}: Reinventing {RNN}s for the Transformer Era",
    author = "Peng, Bo  and
      Alcaide, Eric  and
      Anthony, Quentin  and
      Albalak, Alon  and
      Arcadinho, Samuel  and
      Biderman, Stella  and
      Cao, Huanqi  and
      Cheng, Xin  and
      Chung, Michael  and
      Derczynski, Leon  and
      Du, Xingjian  and
      Grella, Matteo  and
      Gv, Kranthi  and
      He, Xuzheng  and
      Hou, Haowen  and
      Kazienko, Przemyslaw  and
      Kocon, Jan  and
      Kong, Jiaming  and
      Koptyra, Bart{\l}omiej  and
      Lau, Hayden  and
      Lin, Jiaju  and
      Mantri, Krishna Sri Ipsit  and
      Mom, Ferdinand  and
      Saito, Atsushi  and
      Song, Guangyu  and
      Tang, Xiangru  and
      Wind, Johan  and
      Wo{\'z}niak, Stanis{\l}aw  and
      Zhang, Zhenyuan  and
      Zhou, Qinghua  and
      Zhu, Jian  and
      Zhu, Rui-Jie",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.936",
    doi = "10.18653/v1/2023.findings-emnlp.936",
    pages = "14048--14077",
    abstract = "Transformers have revolutionized almost all natural language processing (NLP) tasks but suffer from memory and computational complexity that scales quadratically with sequence length. In contrast, recurrent neural networks (RNNs) exhibit linear scaling in memory and computational requirements but struggle to match the same performance as Transformers due to limitations in parallelization and scalability. We propose a novel model architecture, Receptance Weighted Key Value (RWKV), that combines the efficient parallelizable training of transformers with the efficient inference of RNNs. Our approach leverages a linear attention mechanism and allows us to formulate the model as either a Transformer or an RNN, thus parallelizing computations during training and maintains constant computational and memory complexity during inference. We scale our models as large as 14 billion parameters, by far the largest dense RNN ever trained, and find RWKV performs on par with similarly sized Transformers, suggesting future work can leverage this architecture to create more efficient models. This work presents a significant step towards reconciling trade-offs between computational efficiency and model performance in sequence processing tasks.",
}

@article{peng2024eagle,
  title={Eagle and {F}inch: {RWKV} with matrix-valued states and dynamic recurrence},
  author={Peng, Bo and Goldstein, Daniel and Anthony, Quentin and Albalak, Alon and Alcaide, Eric and Biderman, Stella and Cheah, Eugene and Ferdinan, Teddy and Hou, Haowen and Kazienko, Przemys{\l}aw and others},
  journal={arXiv preprint arXiv:2404.05892},
  year={2024}
}

@article{qin2023scaling,
  title={Scaling transnormer to 175 billion parameters},
  author={Qin, Zhen and Li, Dong and Sun, Weigao and Sun, Weixuan and Shen, Xuyang and Han, Xiaodong and Wei, Yunshen and Lv, Baohong and Yuan, Fei and Luo, Xiao and others},
  journal={arXiv preprint arXiv:2307.14995},
  year={2023}
}

@article{qin2023transnormerllm,
  title={{TransNormerLLM}: A faster and better large language model with improved transnormer},
  author={Qin, Zhen and Li, Dong and Sun, Weigao and Sun, Weixuan and Shen, Xuyang and Han, Xiaodong and Wei, Yunshen and Lv, Baohong and Luo, Xiao and Qiao, Yu and others},
  year={2023}
}

@article{qin2024hgrn2,
  title={{HGRN2}: Gated linear rnns with state expansion},
  author={Qin, Zhen and Yang, Songlin and Sun, Weixuan and Shen, Xuyang and Li, Dong and Sun, Weigao and Zhong, Yiran},
  journal={arXiv preprint arXiv:2404.07904},
  year={2024}
}

@article{qin2024hierarchically,
  title={Hierarchically gated recurrent neural network for sequence modeling},
  author={Qin, Zhen and Yang, Songlin and Zhong, Yiran},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{qin2024lightning,
  title={Lightning {A}ttention-2: A free lunch for handling unlimited sequence lengths in large language models},
  author={Qin, Zhen and Sun, Weigao and Li, Dong and Shen, Xuyang and Sun, Weixuan and Zhong, Yiran},
  journal={arXiv preprint arXiv:2401.04658},
  year={2024}
}

@article{sun2023retentive,
  title={Retentive network: A successor to transformer for large language models},
  author={Sun, Yutao and Dong, Li and Huang, Shaohan and Ma, Shuming and Xia, Yuqing and Xue, Jilong and Wang, Jianyong and Wei, Furu},
  journal={arXiv preprint arXiv:2307.08621},
  year={2023}
}

@article{sun2024linear,
  title={Linear Attention Sequence Parallelism},
  author={Sun, Weigao and Qin, Zhen and Li, Dong and Shen, Xuyang and Qiao, Yu and Zhong, Yiran},
  journal={arXiv preprint arXiv:2404.02882},
  year={2024}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{yang2023gated,
  title={Gated linear attention transformers with hardware-efficient training},
  author={Yang, Songlin and Wang, Bailin and Shen, Yikang and Panda, Rameswar and Kim, Yoon},
  journal={arXiv preprint arXiv:2312.06635},
  year={2023}
}

@article{yang2024parallelizing,
  title={Parallelizing Linear Transformers with the Delta Rule over Sequence Length},
  author={Yang, Songlin and Wang, Bailin and Zhang, Yu and Shen, Yikang and Kim, Yoon},
  journal={arXiv preprint arXiv:2406.06484},
  year={2024}
}

@article{zhang2024gsa,
  title={Gated Slot Attention for Efficient Linear-Time Sequence Modeling},
  author={Yu Zhang and Songlin Yang and Ruijie Zhu and Yue Zhang and Leyang Cui and Yiqiao Wang and Bolun Wang and Freda Shi, Bailin Wang, Wei Bi and Peng Zhou and Guohong Fu},
  journal={arXiv preprint arXiv:2409.07146},
  year={2024}
}

