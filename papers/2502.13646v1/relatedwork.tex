\section{Related Work}
\subsection{In-Context Learning}
    It was discovered that pre-trained LLMs have remarkable capabilities in adapting to new tasks by providing a related context or several demonstrations alongside the test input~\citep{gpt3,dong-etal-2024-survey,luo2024incontext}, which is typically referred to as the in-context learning ability of LLMs. However, it's evident that the selection and order of demonstrations can largely affect the final performance~\cite{liu-etal-2022-makes,lu-etal-2022-fantastically}.
    % Apart from the empirical discovery of ICL, researchers also make an effort to find a proper theoretical explanation behind this paradigm. \citet{dai-etal-2023-gpt} first bridges the theoretical gap between in-context learning and gradient descent. Subsequent works~\citep{pmlr-v202-von-oswald23a, deutch-etal-2024-context} further demonstrate how transformers learn in context by implicitly performing gradient descent.

    \subsection{Demonstration Selection in ICL}
     While early corpus-level methods relied on a fixed set of demonstrations~\cite{gpt3,shin-etal-2020-autoprompt,gao-etal-2021-making,jiang-etal-2021-know,sorensen-etal-2022-information}, recent studies emphasize dynamically selecting the most suitable demonstrations for each test input~\cite{luo2024incontext}, which can be categorized into two groups: data-dependent strategies and self-adaptive strategies.

    \begin{figure*}[h]
        \centering
        \includegraphics[width=0.93\textwidth]{figs/main_fig.pdf}
        \caption{The main framework of \textbf{D.Va}. We first retrieve the nearest demonstration as the validation example and a demonstration candidate set of size $K-1$. Then use our proposed metric to re-rank all the candidates and concatenate the top $n$ candidates as the final context at the inference stage.}
        \label{fig:main}
    \end{figure*}

    As for data-dependent strategies, previous work always relies on the textual or semantical similarity between the test input and the demonstrations to select the most suitable demonstrations, namely retrieval-based ICL (Ret-ICL). In this circumstance, BM25~\citep{bm25} and Sentence-BERT~\citep{reimers-gurevych-2019-sentence} are commonly used to retrieve the most similar demonstrations for each test input. Besides, many researchers also focus on extracting high-quality training data and further optimizing the ability of retrievers~\cite{ceil,li-etal-2023-unified,luo2023dricldemonstrationretrievedincontextlearning,wang-etal-2024-learning}.

    In the realm of self-adaptive strategies, \citet{wu-etal-2023-self} pioneered this area by introducing a self-adaptive method for selecting effective demonstrations for classification tasks.
    Subsequently, \citet{peng-etal-2024-revisiting} leveraged the language models' understanding of test inputs together with candidate examples to identify demonstrations that effectively minimize the perplexity of the test inputs.
    % We further list the difference between our proposed method and previous self-adaptive methods in Table~\ref{tab:comp}.

    % \begin{table}[h]
    %   \centering
    %   \resizebox{\columnwidth}{!}{%
    %   \begin{tabular}{@{}c|ccc@{}}
    %   \toprule
    %   \textbf{Method} & \textbf{Selection Granularity} & \textbf{Labels-free} & \textbf{Train-valid splitting} \\ \midrule
    %   \textbf{MDL}    & organization-level & \xmark  & \xmark \\
    %   \textbf{ConE}   & demonstration-level     & \cmark  & \xmark \\
    %   \textbf{D.Va} & demonstration-level     & \cmark  & \cmark \\ \bottomrule
    %   \end{tabular}%
    %   }
    %   \caption{Comparison between previous self-adaptive methods and ours.}
    %   \label{tab:comp}
    % \end{table}