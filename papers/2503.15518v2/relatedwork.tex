\section{Related Work}
\subsection{Enhancing Robot Lifelikeness with Personality}%Endowing Robots with Personality for Enhanced Lifelikeness}

Designing robots with personality and memory has been shown to significantly influence user modeling in human-robot interactions. This suggests that a robot’s personality traits can influence how it remembers and interprets user interactions, thereby enhancing personalization and making future interactions more natural \cite{personality2024adaptability}. The Big Five traits is the most widely used multidimensional framework for Personality on Robots (POR) \cite{personality2020review}. They are a widely recognized framework in psychology that categorizes human personality into five core dimensions: Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism \cite{fivefactor1987validation, bigfive1992background}. We can effectively define robot's behavior and personality based on these trait. This demonstrates significant potential for authentic and lifelike interaction with human users.

Traditionally, robot personality has been expressed using explicit methods such as visual appearance, language, vocal features, movement, facial expressions, haptics, interaction patterns, and proxemics \cite{personality2020review}. Previous work \cite{personality2016behavior} highlighted that interaction authenticity and user trust can be enhanced by embedding personality traits into robots’ decision-making processes and ensuring consistent and predictable behaviors. Recent advancements have explored POR in the context of large language models (LLMs). For example, \cite{nardelli2024personality} introduces a framework that integrates personality traits to enhance emotional intelligence, enabling agents to exhibit consistent, contextually appropriate responses. This shows humans can accurately perceive and distinguish the robot’s personality. Collectively, these works have explored and validated the implementation and effects of incorporating robot personality, paving the way for integrating this with other domains, such as affective computing and adaptive systems, to achieve more natural human-robot interactions. 

\subsection{Developing Emotional Intelligence and Agility in Robots}

In addition to personality, emotional intelligence is another key component in enhancing human-robot interaction and establishment of a genuine and deep human-robot relationship \cite{stock2022survey, spezialetti2020emotion, emotion2022review, wellbeing2022review}. Many existing works attempted to make robots affectionate for companionship applications \cite{abdollahi2022artificial, pet2021emotion} and beyond \cite{stock2022survey, wang2023emotional, gorur2023fabric}. These applications typically encompass emotion recognition, empathetic response generation, and adaptive interaction, aiming at providing emotional support, fostering a sense of companionship, and enhancing user well-being \cite{abdollahi2022artificial, pet2021emotion}. Demonstrated by \cite{gorur2023fabric}, emotional agility allows collaborative robots to dynamically adapt behavior based on emotional context to offer more intuitive, cooperative, and efficient collaboration in shared workspaces.

The work in \cite{emotion2022review} reviews the implementation of emotional intelligence on robots, classifying techniques into 3 categories: sensing, computing, and acting. Sensing refers to recognizing human emotion using inputs including facial expression, gesture, and voice, and is commonly achieved with visual and acoustic feature extraction or deep learning techniques \cite{emotion2022review}. Computing and acting aspects of emotional intelligence refer to the processes by which the agents assess, understand, and respond to emotional stimuli to guide their behavior and decision-making adaptively \cite{emotion2022review}. Several works were proposed to address the development of computational frameworks and models for enabling agents to exhibit emotional intelligence \cite{jain2015emia, taverner2022genia3, ghafurian2021improving, wang2023emotional, zhao2024both}. The study \cite{jain2015emia} integrates appraisal theories of emotions and fuzzy logic to enable intelligent agents to elicit, regulate, and transition emotions continuously. By incorporating both regular affective and empathic appraisal processes, \cite{taverner2022genia3} proposes a framework that allows agents to select plans based on analyzed events, their own affective states, and personalities, while distinguishing between self and others. The work in  \cite{ghafurian2021improving} examines how virtual agents designed with appraisal theories can generate interactions perceived as more human-like, thus enhancing user enjoyment and cooperation in social dilemmas. As widely employed on intelligent agents, the appraisal theory facilitates the agents with emotion elicitation, regulation, and transition by modeling emotional responses based on the individual evaluation of events \cite{roseman2001appraisal, ellsworth2003appraisal}.

While capable of achieving basic emotional recognition and response generation, these discrete methods still rely on predefined emotion elicitation models and domain-specific designs, which are inherently limited in dimensionality and adaptability. These approaches struggle to capture the complex, non-deterministic nature of human sentiment. For instance, interpreting a scenario where a person is frowning while giving a thumbs-up — indicating sarcasm in a specific context — requires an understanding of nuanced emotional and cultural cues that current systems find challenging to achieve. Recently, LLMs' emotional intelligence has been assessed based on their ability to recognize, interpret, and understand human emotions, revealing that certain models exhibit human-level emotional understanding \cite{wang2023emotional}, while \cite{zhao2024both} explored methods to improve the LLMs' ability to generate emotionally nuanced responses.


\subsection{Adaptation \&  Memory}

Memory has been proven to be critical for robots and virtual agents as it enables personalized and adaptive interactions by recalling past experiences and user preferences. This capability enhances user engagement by making interactions coherent and meaningful, sustaining interest beyond the initial novelty. Additionally, memory improves social presence, allowing robots to deliver context-aware, consistent responses that make interactions feel more natural and human-like \cite{kasap2012building}. While many have incorporated memory into emotion models for intelligent agents \cite{mcduff2012affectaura, kazemifard2014emotion, el2000flame}, its implementation into robots \cite{kang2024nadine} remains limited. The action space sufficiently differentiates virtual agents from robots who interact directly with the physical world. Incorporating memory into robotic systems enables them to capture and reflect richer information, enhancing their ability to interact with real-world environments. 

Traditionally, episodic memory systems \cite{episodic2010,episodic2013,episodic2018} and associative memory models \cite{associative2014, associative2018} were the two most popular and prominently used approaches for robot memory modeling. Episodic memory system stores specific events or interactions, associating them with temporal and emotional context \cite{episodic_foundation1, episodic_foundation2}. Robots can then recall these episodes to inform future actions. Associative memory models create direct links between stimuli and responses by forming patterns of associations, such as connecting facial expressions or tone of voice with emotional states or actions \cite{associative_foundation, associative_foundation_machinelearning}.

However, associative memory models have limited adaptability in complex scenarios due to the inability to recall specific past events or integrate contextual information, while episodic memory systems struggle to synthesize knowledge across episodes or derive broader insights. LLMs address these challenges by efficiently processing vast data to extract, synthesize, and generalize semantic knowledge, as demonstrated by \cite{park2023generative}. LLM-powered agents with memory, reflection, and planning can exhibit realistic interactions and emergent social dynamics in virtual environments, effectively simulating the complexity and nuance of fine-grained human mentality through context-aware, evolving interactions, lifelike behavior, and coherent decision-making \cite{park2023generative}. Furthering the application of LLMs in the memory system, \cite{zhong2024memorybank} introduces a mechanism to equip LLMs with long-term memory capabilities, enabling them to recall relevant information from past interactions and provide more coherent, contextually appropriate responses in extended conversations. Atlhough LLM-powered memory systems have a significant potential in robotics, limited literature has developed such system. We believe that an LLM-driven social robot equipped with human-like memory will allow it to navigate and adapt to nuanced contexts with greater depth and flexibility\cite{kang2024nadine}.

\begin{figure}[!h]
    \centering
    \includegraphics[width=1\linewidth]{figures/overview.png}
    \caption{Overview of the framework. Human influences (red) shape the robot’s memory, which is periodically updated and stored. Robot influences (blue) drive decision-making and external actions. Mentality processing (purple) integrates personality, memory, and appraisal theory to generate emotions and select contextually appropriate actions.}
    \label{fig:overview}
\end{figure}