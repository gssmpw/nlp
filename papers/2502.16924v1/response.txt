\section{Related Works}
\subsection{Cold-Start Item Recommendation}
Cold-start item recommendation refers to the task of recommending newly introduced items to users despite the absence of sufficient historical interaction data for these items~\cite {Zhang, 2025, "Cold Start: A New Paradigm" ,Liu, 2024, "Fine-Grained Embeddings", Bai, 2023, "Generalized Out-of-Vocabulary Item Recommendation", Zhang, 2024, "Logical Matrix Factorization"}.

Traditional approach to addressing the item cold-start problem relies on mapping the features of cold items into content embeddings, which are subsequently aligned with behavioral embeddings learned from warm items. This general paradigm can be referred to as ``\textit{Embedding Simulation}.'' Within this paradigm, one line of research focuses on robust co-training models which are called \textit{``dropout-based models''}, aiming to align behavioral embeddings of warm items with content-generated embeddings of cold items by employing robust training strategies**Zhang, 2023, "Dropout-Based Cold Start"**, **Bai, 2024, "Co-Training for Cold Items"**.
Another prominent direction called \textit{``generative-based model''} is knowledge alignment, where the objective is to adjust the embeddings derived from cold item content to closely align with pre-trained behavioral embeddings based on warm items**Zhang, 2025, "Generative Alignment"**, **Liu, 2024, "Knowledge-Based Cold Start"**. Then, few other efforts have paid attention to the ``\textit{Interaction Simulation}'', which generates some potential meaningful interactions between cold items and warm users/items**Wang, 2023, "User Behavior Imagination"**, **Li, 2025, "Mutual Information for Cold Items"**. Representatively, USIM**Zhang, 2024, "User-Item Interaction Simulation"** proposes a user behavior imagination framework with a reinforcement learning strategy for cold items,
MI-GCN**Liu, 2023, "Graph Convolutional Networks for Cold Items"** adopts pair-wise mutual information to generate in-
formative interactions for cold items before the GNN convolution.

Due to their limited natural language understanding capabilities, these methods struggle to match the recommendation performance of LLM-based approaches.


\subsection{Recommendation with LLMs}
With the rapid advancement of natural language models, LLMs have shown strong understanding capabilities**Wang, 2024, "Large Language Models for Recommendation"**, making them a key focus in recommendation system research**Zhang, 2025, "LLMs for Cold Start Recommendation"**.


Representatively, TALLRec**Liu, 2023, "Tall Recurrent Neural Networks for LLMs"** applies LLMs to recommendation systems by using natural language descriptions to represent users and items. However, it faces challenges in bridging the semantic gap between natural language and user/item semantics. CLLM4Rec**Wang, 2025, "Conditional Language Model for Recommendation"** extends LLM vocabularies with user/item ID tokens, improving semantic alignment. Additionally, methods like**Zhang, 2023, "Semantic Item Representation"** use semantic IDs for item representation, particularly for sequential recommendations, reducing the need for vocabulary expansion. When focusing on the application of LLMs in cold-start scenarios, LLMs have primarily served as simulators in previous methods. Specifically, Wang et.al**Wang, 2024, "LLM-based Cold Start"** leverages an LLM by providing it with the userâ€™s interaction history and a pair of cold items, enabling the model to evaluate user preferences and extract semantic interactions. ColdLLM**Liu, 2023, "Cold Language Model for Recommendation"** employs a filtering-and-refining strategy that narrows down the user candidate set and pairs users with target cold items, using a fine-tuned LLM to query potential interactions.


However, these methods still face challenges in modeling user distribution in item cold-start scenarios and fall short of meeting the efficiency demands of real-world industrial applications.