\section{Related Works}
\subsection{Cold-Start Item Recommendation}
Cold-start item recommendation refers to the task of recommending newly introduced items to users despite the absence of sufficient historical interaction data for these items~\cite {zhang2025cold,liu2024fine,bai2023gorec,zhang2024logical}. 

Traditional approach to addressing the item cold-start problem relies on mapping the features of cold items into content embeddings, which are subsequently aligned with behavioral embeddings learned from warm items. This general paradigm can be referred to as ``\textit{Embedding Simulation}.'' Within this paradigm, one line of research focuses on robust co-training models which are called \textit{``dropout-based models''}, aiming to align behavioral embeddings of warm items with content-generated embeddings of cold items by employing robust training strategies~\cite{volkovs2017dropoutnet,zhu2020heater,wei2021clcrec,shi2019dropoutmethods1,xu2022dropoutmethods2}.
Another prominent direction called \textit{``generative-based model''} is knowledge alignment, where the objective is to adjust the embeddings derived from cold item content to closely align with pre-trained behavioral embeddings based on warm items~\cite{van2013deepmusic,pan2019metaemb,chen2022gar,huang2023aldi}. Then, few other efforts have paid attention to the ``\textit{Interaction Simulation}'', which generates some potential meaningful interactions between cold items and warm users/items~\cite{wang2024mutual,liu2023ucc,liu2024fine}. Representatively, USIM~\cite{liu2024fine} proposes a user behavior imagination framework with a reinforcement learning strategy for cold items,
MI-GCN~\cite{wang2024mutual} adopts pair-wise mutual information to generate in-
formative interactions for cold items before the GNN convolution.

Due to their limited natural language understanding capabilities, these methods struggle to match the recommendation performance of LLM-based approaches.


\subsection{Recommendation with LLMs}
With the rapid advancement of natural language models, LLMs have shown strong understanding capabilities~\cite{wu2024survey,zhang2025survey,hong2024knowledge,hong2024next,chen2024entity}, making them a key focus in recommendation system research~\cite{li2023large,zhao2024recommender,chang2024survey}.


Representatively, TALLRec~\cite{bao2023tallrec} applies LLMs to recommendation systems by using natural language descriptions to represent users and items. However, it faces challenges in bridging the semantic gap between natural language and user/item semantics. CLLM4Rec~\cite{zhu2024collaborative} extends LLM vocabularies with user/item ID tokens, improving semantic alignment. Additionally, methods like~\cite{rajput2023recommender, jin2023language, zheng2024adapting} use semantic IDs for item representation, particularly for sequential recommendations, reducing the need for vocabulary expansion. When focusing on the application of LLMs in cold-start scenarios, LLMs have primarily served as simulators in previous methods. Specifically, Wang et.al~\cite{wang2024large} leverages an LLM by providing it with the userâ€™s interaction history and a pair of cold items, enabling the model to evaluate user preferences and extract semantic interactions. ColdLLM~\cite{huang2024large} employs a filtering-and-refining strategy that narrows down the user candidate set and pairs users with target cold items, using a fine-tuned LLM to query potential interactions.


However, these methods still face challenges in modeling user distribution in item cold-start scenarios and fall short of meeting the efficiency demands of real-world industrial applications.