\documentclass{article}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{booktabs}
\usepackage{hyperref}
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage[accepted]{icml2025}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage[capitalize,noabbrev]{cleveref}
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\usepackage[textsize=tiny]{todonotes}
\icmltitlerunning{On the Impact of the Utility in Semivalue-based Data Valuation}

\begin{document}

\twocolumn[
\icmltitle{On the Impact of the Utility in Semivalue-based Data Valuation}
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Mélissa Tamine}{Criteo,Inria}
\icmlauthor{Benjamin Heymann}{Criteo}
\icmlauthor{Patrick Loiseau}{Inria}
\icmlauthor{Maxime Vono}{Criteo}
\end{icmlauthorlist}

\icmlaffiliation{Criteo}{Criteo AI Lab, Paris, France}
\icmlaffiliation{Inria}{Inria, Fairplay joint team, Palaiseau, France}

\icmlcorrespondingauthor{Mélissa Tamine}{m.tamine@criteo.com}

\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]
\printAffiliationsAndNotice{}

\begin{abstract}
Semivalue-based data valuation in machine learning (ML) quantifies the contribution of individual data points to a downstream ML task by leveraging principles from cooperative game theory and the notion of utility. While this framework has been used in practice for assessing data quality, our experiments reveal inconsistent valuation outcomes across different utilities, albeit all related to ML performance. Beyond raising concerns about the reliability of data valuation, this inconsistency is challenging to interpret, as it stems from the complex interaction of the utility with data points and semivalue weights, which has barely been studied in prior work. In this paper, we take a first step toward clarifying the utility impact on semivalue-based data valuation. Specifically, we provide geometric interpretations of this impact for a broad family of classification utilities, which includes the accuracy and the arithmetic mean. We introduce the notion of \textit{spatial signatures}: given a semivalue, data points can be embedded into a two-dimensional space, and utility functions map to the dual of this space. 
This geometric perspective separates the influence of the dataset and semivalue from that of the utility, providing a theoretical explanation for the experimentally observed sensitivity of valuation outcomes to the utility choice.
\end{abstract}
\section{Introduction}
\label{sec:introduction}

Supervised machine learning (ML) relies on data, but real-world datasets often suffer from noise and biases as they are collected from multiple sources and are subject to measurement and annotation errors \cite{errors1}. Such variability can impact learning outcomes, highlighting the need for systematic methods to evaluate data quality. In response, \textit{data valuation} has emerged as a growing research field that aims to quantify individual data points' contribution to a learning task, helping to identify informative samples and mitigate the impact of low-quality data. A popular way to tackle the data valuation problem is to adopt a cooperative game-theoretic viewpoint, where each data point is modeled as a player in a coalitional game, and the usefulness of any data subset is measured by a \textit{utility function}. This approach leverages game theory solution concepts called \textit{semivalues} \cite{semivalues}, which input data and utility to assign an importance score to each data point \cite{datashapley, betashapley, databanzhaf, jia2023, jia2020}.
When computing semivalues, the utility function is typically selected as a performance metric, such as the accuracy in classification or the mean squared error in regression. However, this choice is inherently unconstrained—any function mapping data subsets to real values can serve as a utility as long as a higher utility reflects better performance. This flexibility raises a fundamental and legitimate question: \textit{to what extent does the choice of utility impact data valuation outcomes?}

Despite the widespread use of semivalue-based data valuation, there is a limited theoretical understanding of how and why the choice of utility function influences valuation outcomes. In practice, utility functions are often chosen for convenience, typically as standard ML performance metrics \cite{datashapley}, rather than being grounded in theoretical principles. However, \citet{rethinkingdatashapley} demonstrated that for a particular semivalue-based method, certain utility choices can lead to valuation outcomes no better than random importance assignment when no specific constraints are imposed. This finding underscores a fundamental issue: the flexibility in utility selection introduces variability in data valuation, potentially leading to inconsistent or misleading conclusions. This question's lack of theoretical grounding is particularly concerning in high-stakes decision-making scenarios such as healthcare, where data valuation informs critical tasks \cite{pandl2021, bloch2021, zheng2024}. Practitioners risk making unreliable decisions that undermine model performance and interpretability without a clear understanding of how utility functions shape valuation outcomes. Our study aims to fill in this gap, providing insights to better understand data valuation and its practical applications.

Our contributions can be summarized as follows:
\begin{enumerate}
\item \textbf{Empirical evidence of data valuation outcomes variability across utility functions.} Our experiments reveal that the agreement between two utility functions in assessing data importance varies unpredictably across datasets and semivalues. For a given dataset and semivalue, two utilities may produce similar rankings of data points, while for another pair, they may diverge entirely. This lack of a systematic pattern suggests that a utility’s impact on data valuation is not solely determined by its intrinsic properties but rather by its interaction with the dataset and the semivalue.
\item \textbf{A geometric interpretation of a utility interaction with data and semivalue.} We propose a geometric framework for a class of binary classification utilities to better understand this interaction. We introduce the concept of \textit{spatial signatures}, which correspond to an embedding of the dataset into a two-dimensional space induced by the semivalue. We show that the utility functions we consider map to the dual of this space, enabling data values to be visualized as projections onto directions defined by the utility. This geometric perspective provides a structured way to understand which datasets and semivalues lead to robust data valuations across utilities and which lead to variable and inconsistent valuations. In particular, it explains why, in our experiments, utility functions influence data valuation inconsistently across different datasets and semivalues.
\end{enumerate}

\noindent \textbf{Related work.} Game-theoretic approaches to data valuation have gained traction in recent years due to their formal justification through axioms. The Shapley value \cite{shapley, datashapley}, in particular, has been widely adopted as a data valuation method because it uniquely satisfies four key axioms: linearity, dummy player, symmetry, and efficiency. Alternative approaches have emerged by relaxing some of these axioms. By omitting the efficiency requirement, one obtains the semivalue framework \cite{semivalues}. Examples of value notions within this class include LOO (Leave-One-Out) \cite{loo}, Beta Shapley \cite{betashapley}, and Data Banzhaf \cite{databanzhaf}. Furthermore, relaxing the linearity axiom leads to the Least Core, an alternative concept from the cooperative game theory proposed by \cite{leastcore} for data valuation. The Least Core determines an optimal profit allocation where each coalition $S$ receives the minimum required subsidy to prevent any participant from defecting from the grand coalition $\mathcal{D}$. The Distributional Shapley Value \cite{ghorbani2020, kwon21} is an extension of Data Shapley designed to assess data contributions based on an underlying data distribution rather than a fixed dataset.
Beyond cooperative game theory, several non-game-theoretic data valuation methods have been explored. An overview is provided by \cite{sim2022}, and some of them are benchmarked by \cite{opendataval}. 

\noindent \textbf{Notation.}  We set $\mathbb{N}^{*} = \mathbb{N} \setminus \{0\}$. For $n \in \mathbb{N}^{*}$, we denote $[n]:=\{1, . ., n\}$. For a dataset $\mathcal{D}$, we denote as $\lvert \mathcal{D} \rvert$ its cardinality and as $2^{\mathcal{D}}$ its powerset, i.e., the set of all possible subsets of $\mathcal{D}$, including the empty set $\emptyset$ and $\mathcal{D}$ itself. For $d \in \mathbb{N}^*$, we denote $\mathcal{X} \subseteq \mathbb{R}^d$ and $\mathcal{Y} \subseteq \mathbb{R}$ an input space and an output space, respectively.

\section{Background}
\label{sec:background}

\subsection{Semivalue-based data valuation set-up}
\label{subsec:semivalues}
The data valuation problem involves a dataset of interest $\mathcal{D} = \{z_i=(x_i, y_i)\}_{i \in [n]}$, where for any $i \in [n]$  each $x_i \in \mathcal{X}$ is a feature vector and $y_i \in \mathcal{Y}$ is the corresponding label. We focus on binary classification, where $\mathcal{Y} = \{0,1\}$. 
Data valuation aims to assign a scalar score to each data point in $\mathcal{D}$, quantifying its contribution to a downstream ML task. These scores will be referred to as \textit{data values}.

\noindent \textbf{Utility functions.} Most data valuation methods rely on \textit{utility functions} to compute data values. A utility is a set function $u : 2^{\mathcal{D}} \to \mathbb{R}$ that maps any subset $S$ of the training set $\mathcal{D}$ to a score indicating its usefulness for performing the considered ML task. Formally, this can be expressed as $u(S) = \textsc{perf}(\mathcal{A}(S))$, where $\mathcal{A}$ is a learning algorithm that takes a subset $S$ as input and returns a trained model, and $\textsc{perf}$ is a metric function used to evaluate the model’s performance. For classification tasks, $\textsc{perf}$ can be chosen, for instance, as the accuracy evaluated on a hold-out test set $\mathcal{D}_{\text{test}}$. There are, however, many other choices of performance function, which lead to different utility functions---this is precisely the focus of our study. For convenience, we interchangeably refer to the utility $u$ and the performance metric $\textsc{perf}$ as $u$ inherently depends on $\textsc{perf}$. 

\noindent \textbf{Semivalues.} The most popular data valuation methods assign a value score to each data point in $\mathcal{D}$ using solution concepts from cooperative game theory, known as semivalues \cite{semivalues}. The collection of data valuation methods that fall under this category is referred to as \textit{semivalue-based data valuation}. These methods rely on the notion of \textit{marginal contribution}. 
Formally, for any $i,j \in [n]$, let $\mathcal{D}_j^{\backslash z_i}$ denote the set of all subsets of $\mathcal{D}$ of size $j-1$ that exclude $z_i$. Then, the marginal contribution of $z_i$ with respect to other $j-1$ samples is defined as
\begin{align*}
    \Delta_j(z_i; u) := \frac{1}{\binom{n-1}{j-1}} \sum_{S \subseteq \mathcal{D}_j^{\backslash z_i}} u\left(S \cup \{z_i\}\right) - u(S) \ .
\end{align*}
The marginal contribution $\Delta_j(z_i; u)$ considers all
possible subsets $S \in \mathcal{D}_j^{\backslash z_i}$ with the same cardinality $j-1$
and measures the average changes of $u$ when datum of interest $z_i$ is removed from $S \cup \{z_i\}$.

Each semivalue-based method is characterized by a weight vector $\omega := (\omega_1, \hdots, \omega_n)$ and assigns a score $\phi(z_i; \omega, u)$ to each data point $z_i \in \mathcal{D}$ by computing a weighted average of its marginal contributions $\{\Delta_j(z_i;u)\}_{j \in [n]}$. Specifically,
\begin{align}
    \phi(z_i; \omega, u) := \sum_{j=1}^{n} \omega_j \Delta_j(z_i; u) \ .
\end{align}
Below, we define the weights for three commonly used semivalue-based methods \cite{opendataval} and illustrate them in Figure \ref{fig:weights}. Their differences in weighting schemes have geometric implications discussed in Section \ref{sec:geometric-interpretation}.
\begin{definition}
\textit{Data Shapley} \cite{datashapley} is derived from the \textit{Shapley value} \cite{shapley}, a solution concept from cooperative game theory that fairly allocates the total gains generated by a coalition of players based on their contributions. In the context of data valuation, Data Shapley takes a simple average of all the marginal contributions. Its weight vector $\omega_{\text{shap}}$ is 
\begin{align*}
    \omega_{\text{shap}} = \left(\frac{1}{n}, \hdots, \frac{1}{n}\right)
\end{align*}
\end{definition}
\begin{definition}
\textit{$(\alpha, \beta)$-Beta Shapley} \cite{betashapley} extends Data Shapley by introducing tunable parameters $(\alpha, \beta) \in \mathbb{R}^2$, which controls the emphasis placed on marginal contributions from smaller or larger subsets. The corresponding weight vector $\omega_{\text{beta}}$ = $(\omega_{\text{beta},j})_{j \in [n]}$ has
\begin{align*}
    \omega_{\text{beta}, j} = \binom{n-1}{j-1} \cdot \frac{\text{Beta}(j+\beta-1, n-j+\alpha)}{\text{Beta}(\alpha, \beta)}
\end{align*}
for all $j$, where $\text{Beta}(\alpha, \beta) = \Gamma(\alpha)\Gamma(\beta)/\Gamma(\alpha + \beta)$ and $\Gamma$ is the Gamma function. 
\end{definition}

\begin{definition}
\textit{Data Banzhaf} \cite{databanzhaf} is derived from the \textit{Banzhaf value} \cite{banzhaf}, a cooperative game theory concept originally introduced to measure a player's influence in weighted voting systems. Data Banzhaf weight's vector $\omega_{\text{banzhaf}} = (\omega_{\text{banzhaf},j})_{j\in[n]}$ is defined as
\begin{align*}
    \omega_{\text{banzhaf}, j} = \binom{n-1}{j-1} \cdot \frac{1}{2^{n-1}}
\end{align*}
\end{definition}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/weights.png}
    \caption{Comparison of Data Shapley, $(4,1)$-Beta Shapley, and Data Banzhaf weighting schemes for $n = 100$.}
    \label{fig:weights}
    \vskip -0.1in
\end{figure}

These semivalue-based methods satisfy fundamental axioms that ensure desirable properties in data valuation. We formally state these axioms in the following.
Let $\phi(., \omega; .)$ be a semivalue-based data valuation method defined by a weight vector $\omega$ and let $u$ and $v$ be utility functions. Then, $\phi$ satisfies the following axioms:
\begin{enumerate}
    \item Dummy. If $u(S \cup \{z_i\}) = u(S) + c$ for all $S \subseteq \mathcal{D} \backslash \{z_i\}$ and some $c \in \mathbb{R}$, then $\phi(z_i; \omega, u) = c$.
    \item Symmetry. If $u(S \cup \{z_j\}) = u(S\cup \{z_j\})$ for all $S \subseteq \mathcal{D} \backslash \{z_i, z_j\}$, then $\phi(z_i; \omega, u) = \phi(z_j; \omega, u)$.
    \item Linearity. For any $\alpha_1, \alpha_2 \in \mathbb{R}$, $\phi(z_i; \omega, \alpha_1 u + \alpha_2 v) = \alpha_1 \phi(z_i; \omega, u) + \alpha_2 \phi(z_i; \omega, v)$.
\end{enumerate}

While all semivalues satisfy the above axioms, Data Shapley uniquely also guarantees \textit{efficiency}: $\sum_{z \in \mathcal{D}} \phi(z, \omega, u) = u(\mathcal{D})$.

\subsection{Applications of semivalue-based methods}
In practice, semivalue-based methods are mostly applied to perform \textit{data cleaning} or \textit{data subset selection} \cite{tang2021, pandl2021, bloch2021, zheng2024}. Both tasks involve ranking data points according to their assigned values.

\textbf{Data cleaning}. Data cleaning aims to improve dataset quality by identifying and removing noisy or low-quality data points. Since semivalue-based methods quantify each point’s contribution to a downstream task, low-valued points are natural candidates for removal. Specifically, a common approach is to remove points that fall into the set $\mathcal{N}_{\tau}$, defined as the subset of data points with the lowest values \cite{datashapley}. Formally, $\mathcal{N}_{\tau} = \{z_i \in \mathcal{D} \mid \phi(z_i; u, \omega) \leq \tau\}$, where $\tau$ is a threshold determined through domain knowledge or empirical evaluation.

\textbf{Data subset selection}. Data subset selection involves choosing the optimal training set from available samples to maximize final model performance. Since semivalues measure data quality, prioritizing data points with the highest values is a natural approach. Consequently, a common practice in the literature is selecting, given a size budget $k$, the subset $\mathcal{S}^{(k)}_{\phi(u,\omega)}$ of data points with top-$k$ data values, i.e., $\mathcal{S}^{(k)}_{\phi(u,\omega)} = \arg\max_{\mathcal{S} \subseteq \mathcal{D}, |\mathcal{S}| = k} \sum_{z_i \in \mathcal{S}} \phi(z_i;u,\omega)$ \cite{rethinkingdatashapley}.
 
\section{Variability of data valuations across utility functions: an experimental investigation}
\label{sec:experimental-investigation}
Although utility functions are central in semivalue-based data valuation methods, their impact on data valuation outcomes has received little attention. \citet{rethinkingdatashapley} is the only work that theoretically explores this question, focusing specifically on how the choice of utility affects the reliability of Data Shapley for data subset selection. In this section, we broaden this scope by experimentally studying the influence of various utility functions across multiple semivalue-based methods. To conduct this investigation, we propose an application-agnostic metric based on data values \textit{ranking}, enabling a broader perspective beyond one specific application. 
\subsection{An application-agnostic metric based on rank correlation to compare utility impact} 
\label{sec:rank-correlation}
Most data valuation applications depend on the relative ranking of data points based on their assigned values. Data cleaning prioritizes identifying low-ranked points, while data subset selection focuses on points with the highest values. Since the rankings inherently determine the outcome of these applications, if rankings induced by different utility functions are highly similar, it suggests that the utilities are aligned in their ability to prioritize data points for a given application. Therefore, \textit{rank correlation} appears as an intuitive and reasonable metric for evaluating whether utility functions produce consistent data valuation outcomes. 

Formally, given a dataset $\mathcal{D}$ and a semivalue characterized by weight vector $\omega$, we compare the impact of two utilities $u$ and $v$ on data valuation outcomes by computing the rank correlation between $\{\phi(z_i;u,\omega)\}_{i \in [n]}$ and $\{\phi(z_i;v,\omega)\}_{i \in [n]}$. Several measures of rank correlation exist to evaluate the similarity between two rankings. One of the most widely used is the \textit{Kendall rank correlation coefficient},\footnote{Along with the \textit{Spearman rank correlation} \cite{spearman}.} which quantifies the agreement between two rankings by comparing the relative order of all pairs of elements.

Kendall rank correlation coefficient \cite{kendall} measures the ordinal association between two sets of values $\{\phi_i^u\}_{i \in [n]}$
and $\{\phi_i^v\}_{i \in [n]}$ assigned to $n$ elements. For any pair of indices $(i,j)$ where $i < j$, the pair is \textit{concordant} if the relative order of $\phi_i^u$ and $\phi_j^u$ matches the relative order of $\phi_i^v$ and $\phi_j^v$ and is \textit{discordant} otherwise. Let $C$ and $D$ denote the number of concordant and discordant pairs, respectively. The Kendall rank coefficient $\tau(u, v)$ is defined as $\tau(u, v) = \frac{C - D}{\binom{n}{2}}$, and can be equivalently expressed as
\vspace{-0.3cm}
\begin{align}
    \label{def:kendall-tau}
    \tau(u, v) = \frac{1}{\binom{n}{2}}\sum_{i < j} \operatorname{sgn}[(\phi_i^u - \phi_j^u)\cdot (\phi_i^v - \phi_j^v)] \ ,
\end{align}
and $\tau(u,v)=1$ indicates perfect alignment between $\{\phi_i^u\}_{i \in [n]}$ and $\{\phi_i^v\}_{i \in [n]}$ while $\tau(u,v)=-1$ traduces perfect disagreement and $\tau(u,v)=0$ an absence of correlation.

While similar data value rankings suggest alignment in data valuation applications, the converse is not always true. Low-rank correlation does not necessarily imply misalignment. We derive an analytical example in Appendix \ref{subsec:remark-rank-interpretation}, which shows that two utility functions can exhibit low-rank correlation yet consistently separate high-importance from low-importance points, demonstrating aligned behavior despite ranking differences. This is why, in cases of low-rank correlation, we complement the rank correlation measure with an intersection analysis, evaluating the overlap between bottom-ranked subsets of data values to better assess utility alignment in performing data valuation applications.

\subsection{Experimental evidence of ranking variability}
We perform systematic rank correlation computations on various datasets and semivalue-based methods to assess the ranking variability of data values induced by different utility functions.

\noindent \textbf{Experimental setup.} 
Rank correlation computations are performed on several publicly available binary classification datasets widely used in the literature \cite{datashapley, databanzhaf, betashapley, opendataval}. Table \ref{tab:datasets} lists these datasets along with their sources. We compute data values using three semivalue-based methods: Data Shapley, $(4,1)$-Beta Shapley,\footnote{The authors in \cite{betashapley} identify 
$(\alpha,\beta)=(16,1)$ as best suited for data valuation applications, but we use $(\alpha,\beta)=(4,1)$ as it performs best in the benchmark proposed by \cite{opendataval}.} and Data Banzhaf. Given a dataset $\mathcal{D}$ and a semivalue $\omega$, we evaluate the impact of three commonly used classification utilities: the \textit{accuracy} (\textsc{acc}), the \textit{recall} (\textsc{rec}), and the \textit{arithmetic mean} (\textsc{am}).
Specifically, for each utility function pair $(u,v)$, we compute the Kendall rank correlation $\tau(u,v)$ to quantify ranking consistency.  
We extend these experiments to additional classification utilities and reproduce them with the Spearman rank correlation for completeness. The corresponding results are provided in Appendix \ref{sec:additional-experiments}.
\vspace{3mm}
\begin{remark}
\label{remark:limitation}
Computing $\tau(u,v)$ for each utility pair $(u,v)$ requires obtaining the data values sets $\{\phi(z, \omega, u)\}_{z \in \mathcal{D}}$ and $\{\phi(z, \omega, v)\}_{z \in \mathcal{D}}$. This computation involves a learning algorithm $\mathcal{A}$, and a test dataset $\mathcal{D}_{\text{test}}$ to evaluate the utility on different subsets $S \subseteq \mathcal{D}$, and an approximation method as the exact computation of semivalues is infeasible for large datasets \cite{datashapley, jia2023, dushapley}. To ensure that any observed differences in both sets' rankings arise solely from the choice of the utility and not from these other sources of variability, we propose a systematic methodology in Appendix \ref{subsec:systematic-methodology} that eliminates extraneous perturbations. The results reveal qualitatively similar insights, reinforcing the observation that the impact of utility functions on data valuation rankings is not solely dictated by their intrinsic properties but also by their interaction with the dataset and the semivalue. Full details are available in Appendices \ref{sec:extended-utilities} and \ref{sec:spearman-correlation}.
\end{remark}

\noindent \textbf{Results analysis.} The experimental results in Table~\ref{tab:table-rank-correlation} reveal that utility pairs (\textsc{acc}-\textsc{am}, \textsc{acc}-\textsc{rec}, and \textsc{rec}-\textsc{am}) exhibit no systematic agreement or disagreement across datasets and semivalues. In some cases, utility functions produce highly similar rankings, while their rank correlation is markedly low in others.

For example, in the Breast dataset, \textsc{acc}-\textsc{am} exhibits strong agreement across all semivalues, with Kendall correlations of $0.98$, $0.98$, and $0.99$ for Shapley, $(4,1)$-Beta Shapley, and Banzhaf, respectively. However, this same utility pair shows significantly weaker agreement for the \textsc{credit} dataset, with Kendall correlations dropping to $0.51$, $0.58$, and $0.07$, highlighting a sharp dataset-dependent divergence.

Moreover, substantial differences emerge within a dataset depending on the chosen semivalue. For the \textsc{cpu} dataset, the correlation between \textsc{acc}-\textsc{rec} is relatively high under Shapley and $(4,1)$-Beta Shapley (Kendall rank correlations of $0.78$ and $0.79$, respectively), yet it vanishes entirely under Banzhaf (Kendall coefficient $= 0.01$).

In addition, following Remark \ref{remark:limitation}, we complement the rank correlation computations with an intersection-based analysis in Appendix \ref{sec:intersection-analysis} for datasets, semivalues, and utility pairs exhibiting low Kendall rank correlation coefficient. This analysis assesses whether differences in value rankings indicate misalignment in performing data valuation applications. The results show no cases where low-rank correlation preserves alignment, suggesting that low-rank correlation effectively reflects misalignment for these datasets, semivalues, and utility pairs.

All those results suggest that a utility function’s influence on data valuation rankings is not an intrinsic property of the utility itself but rather emerges from its interaction with both the dataset and the semivalue. The same utility pair can yield highly similar rankings in one context yet diverge entirely in another, indicating that the way a utility assigns value to data points is shaped by how it interacts with the dataset's structure and how the semivalue aggregates marginal contributions. This reinforces the idea that utility-driven data valuation cannot be understood in isolation—its effects are context-dependent, varying with both the dataset characteristics and the weighting mechanism imposed by the semivalue.

This utility-dataset-semivalue dependency remains under-explored in prior work, resulting in a limited understanding of what semivalue-based methods truly capture. This raises concerns, as data valuation methods are intended to enhance the interpretability of dataset quality. Addressing this gap requires a deeper exploration of the interplay between utilities, datasets, and semivalue weights to ensure semivalue-based data valuation delivers its promise.

\begin{table*}
\vskip 0.15in
\begin{center}
\begin{scriptsize} 
\begin{sc}
\begin{tabular}{l|ccc|ccc|ccc}
\hline
\abovespace
\belowspace
dataset & \multicolumn{3}{c|}{shapley} & \multicolumn{3}{c|}{$(4,1)$-beta Shapley} & \multicolumn{3}{c}{banzhaf} \\
\abovespace\belowspace
& \textsc{acc}-\textsc{rec} & \textsc{acc}-\textsc{am} & \textsc{rec}-\textsc{am} & \textsc{acc}-\textsc{rec} & \textsc{acc}-\textsc{am} & \textsc{rec}-\textsc{am} & \textsc{acc}-\textsc{rec} & \textsc{acc}-\textsc{am} & \textsc{rec}-\textsc{am} \\
\hline
\abovespace
breast & 0.93 (0.01) & 0.98 (0.01) & 0.92 (0.01) & 0.94 (0.01) & 0.98 (0.01) & 0.92 (0.01) & 0.82 (0.03) & 0.99 (0.01) & 0.81 (0.03) \\
titanic & 0.42 (0.03) & 0.77 (0.01) & 0.64 (0.02) & 0.46 (0.02) & 0.81 (0.01) & 0.65 (0.01) & -0.25 (0.04) & 0.77 (0.02) & -0.05 (0.05) \\
credit & 0.31 (0.01) & 0.51 (0.01) & 0.79 (0.01) & 0.35 (0.01) & 0.58 (0.01) & 0.76 (0.01) & -0.31 (0.01) & 0.07 (0.01) & 0.60 (0.02) \\
heart & 0.52 (0.02) & 0.98 (0.01) & 0.50 (0.02) & 0.61 (0.01) & 0.98 (0.01) & 0.59 (0.02) & 0.19 (0.02) & 0.98 (0.01) & 0.18 (0.02) \\
wind & 0.77 (0.01) & 0.98 (0.01) & 0.75 (0.01) & 0.79 (0.01) & 0.98(0.01) & 0.79 (0.01) & 0.08 (0.03) & 0.98 (0.01) & 0.07 (0.03) \\
cpu & 0.78 (0.01) & 0.92 (0.01) & 0.86 (0.01) & 0.79 (0.01) & 0.93 (0.01) & 0.86 (0.01) & 0.01 (0.03) & 0.75 (0.02) & 0.19 (0.04) \\
2dplanes & 0.31 (0.02) & 0.99 (0.01) & 0.31 (0.02) & 0.33 (0.02) & 0.99 (0.01) & 0.33 (0.02) & 0.37 (0.01) & 0.99 (0.01) & 0.37 (0.01) \\
\belowspace
pol & 0.56 (0.01) & 0.73 (0.01) & 0.29 (0.01) & 0.56 (0.01) & 0.79 (0.01) & 0.34 (0.01) & 0.67 (0.01) & 0.69 (0.01) & 0.36 (0.01) \\
\hline
\end{tabular}
\end{sc}
\end{scriptsize}
\end{center}
\caption{Kendall rank correlations between different utility function pairs (Accuracy-Recall, Accuracy-Arithmetic Mean, and Recall-Arithmetic Mean) across multiple datasets and three semivalues: Shapley, $(4,1)$-Beta Shapley, and Banzhaf. Each semivalue is approximated $5$ times using Monte Carlo sampling, and Kendall rank correlations are computed for each run. The reported values are the mean across the $5$ runs, while the standard errors (in parenthesis) are derived from the standard deviation of these $5$ estimates.}
\label{tab:table-rank-correlation}
\vskip -0.1in
\end{table*}
\section{Explaining ranking variability through a geometric interpretation of the utility-dataset-semivalue interplay}
\label{sec:geometric-interpretation}
Motivated by the experimental results from Section \ref{sec:experimental-investigation}, we aim to understand the interplay between utilities, datasets, and semivalues in order to explain ranking variability. We focus on a family of classification utilities that includes accuracy, recall, and arithmetic mean, for which we derive \textit{geometric interpretations} of ranking diversity. This geometric perspective provides a framework to explain the results in Table \ref{tab:table-rank-correlation}.

\subsection{A subclass of \textit{linear fractional performance measures}}
This section introduces the specific family of utility functions we consider in our analysis. 
We build on the framework of \textit{linear fractional performance measures} \cite{koyejo2014}, which generalizes various classification metrics, including the F-score and misclassification risk.

These measures express classifier performance as a ratio of affine functions of classification probabilities. Formally, given a training dataset $S \in (\mathcal{X} \times \{0,1\})^n$, a test dataset $\mathcal{D}_{\text{test}} = \{(x_j, y_j)\}_{j \in [m]} \in (\mathcal{X} \times \{0,1\})^m$ and a learning algorithm $\mathcal{A}$, we denote $g_{S} = \mathcal{A}(S)$ a classifier trained on $S$ which maps input features to predicted labels, i.e., $g_{S} : \mathcal{X} \rightarrow \{0,1\}$. A linear fractional performance measure $u$ evaluates the performance of $g_{S}$ on $\mathcal{D}_{\text{test}}$ as
\begin{align*}
    u(S) = 
    \frac{
        c_0 + c_1 \lambda(S, \mathcal{D_{\text{test}}}) 
        + c_2 \gamma(S, \mathcal{D_{\text{test}}})
    }{
        d_0 + d_1 \lambda(S, \mathcal{D_{\text{test}}}) 
        + d_2 \gamma(S, \mathcal{D_{\text{test}}}) 
    } \ ,
\end{align*}
where $(c_0, c_1, c_2, d_0, d_1, d_2) \in \mathbb{R}^6$ determines the structure of $u$ while $\lambda(S, \mathcal{D_{\text{test}}}) = \widehat{\mathbb{P}}_{\mathcal{D}_{\text {test }}}\left(g_{S}\left(x\right)=1, y=1\right) = \frac{1}{m} \sum_{j=1}^m \mathbf{1}\left[g_{S}\left(x_j\right)=1, y_j=1\right]$ is the empirical probability of true positives and $\gamma(S, \mathcal{D_{\text{test}}}) =\widehat{\mathbb{P}}_{\mathcal{D}_{\text {test }}}\left(g_{S}\left(x\right)=1\right) =\frac{1}{m} \sum_{j=1}^m \mathbf{1}\left[g_{S}\left(x_j\right)=1\right] $ is the empirical probability of positive predictions. When the test dataset $\mathcal{D}_{\text{test}}$ is clear from context, we use the shorthand 
$\lambda(S)$ to denote $\lambda(S, \mathcal{D}_{\text{test}})$ and $\gamma(S)$ to denote $\gamma(S, \mathcal{D}_{\text{test}})$. 

Our analysis focuses on a specific subclass of utilities within the framework of linear fractional performance measures. These utilities are characterized by a constant denominator, i.e., $d_1 = d_2 = 0$ and $d_0 \neq 0$, which simplifies their formulation to
\begin{align}
    u(S) = \frac{1}{d_0} \left[c_0 + c_1 \lambda(S) + c_2 \gamma (S)\right] \ .
    \label{eq:utility-class}
\end{align}
We refer to this subclass as the $(\lambda,\gamma)-$\textit{linear utility class}, denoted by $\mathcal{U}_{\lambda, \gamma}$, since each utility function in this class is a linear transformation of the classification statistics $\lambda$ and $\gamma$. Accuracy, recall, and arithmetic mean, used in our experiments (Section~\ref{sec:experimental-investigation}), belong to this subclass. Their formulation follows Eq.~\eqref{eq:utility-class}, with coefficients $(c_0, c_1, c_2, d_0)$ given in Table~\ref{tab:utility-class}.
We chose to restrict our analysis to this class of utilities, which already encompasses popular classification performance metrics, in order to be able to provide theoretical results matching empirical evidence.
\begin{table}[t]
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{lcc}
\toprule
utility & $(c_0, c_1, c_2)$ & $d_0$ \\
\midrule
accuracy (acc)  & $(1-\pi, 2, -1)$ & $1$ \\
recall (rec)   & $(0, 1, 0)$ & $\pi$ \\
arithmetic mean (am) & $\left(\frac{1}{2}, \frac{1}{2\pi} + \frac{1}{2(1-\pi)}, \frac{-1}{2(1-\pi)}\right)$ & $1$ \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\caption{Examples of utilities which can be represented by Eq.~\eqref{eq:utility-class}. For more examples, see \citet{choi2010}. We set \(\pi = \frac{1}{m} \sum_{j = 1}^m \mathbf{1}[y_j = 1]\), the proportion of positive labels in \(\mathcal{D}_{\text{test}}\).}
\label{tab:utility-class}
\vskip -0.1in
\end{table}

\subsection{Geometric characterization of dataset, semivalue, and utilty}

This section examines the geometric properties of utility functions in $
\mathcal{U}_{\lambda, \gamma}$ in relation to the dataset $
\mathcal{D}$ and the semivalue $\omega$. 
A fundamental property of the utility class $\mathcal{U}_{\lambda, \gamma}$ is that, for any $u \in \mathcal{U}_{\lambda, \gamma}$, the data value $\phi(z_i; \omega, u)$ can be represented as a linear combination of $\phi(z_i; \omega, \lambda)$ and  $\phi(z_i; \omega, \gamma)$, as stated formally in the following proposition.

\begin{proposition}
\label{prop:linear-decomposition}
(Linear decomposition of data values for $(\lambda, \gamma)$-linear utilities) Let $\mathcal{D} = \{z_i\}_{i \in [n]}$ be a dataset and $\omega$ a semivalue weight vector. For any utility function $u \in \mathcal{U}_{\lambda, \gamma}$ characterized by coefficients $(c_0, c_1, c_2, d_0) \in \mathbb{R}^4$, the data value assigned to $z_i$ can be decomposed as
\begin{align}
    \label{eq:phi-linear}
    \phi(z_i; \omega, u) = \frac{c_1}{d_0} \phi(z_i; \omega, \lambda) + \frac{c_2}{d_0} \phi(z_i; \omega, \gamma) \ .
\end{align}
\end{proposition}
The decomposition in \cref{prop:linear-decomposition} suggests that the influence of the dataset and semivalue is fully captured by the vector $\mathbf{e_i} = (\phi(z_i; \omega, \lambda), \phi(z_i; \omega, \gamma))$ which embeds each data point $z_i \in \mathcal{D}$ into a two-dimensional space $\mathcal{P}_{\omega}$ induced by the semivalue $\omega$.
While the embedding $\mathbf{e_i}$ depends on $\lambda$ and $\gamma$, the spatial structure of $\mathcal{D}$ in $\mathcal{P}_{\omega}$ is independent of the particular choice of $u$ within $\mathcal{U}_{\lambda, \gamma}$ as this choice is uniquely determined by the coefficients $(c_0, c_1, c_2)$ and $d_0$.

\begin{definition} (\textit{Spatial signature}) We define the spatial signature of $\mathcal{D}$ in $\mathcal{P}_{\omega}$ as the collection of all embedded data points $\mathbf{e} = (\mathbf{e_i})_{i \in [n]} = \big(\phi(z_i; \omega, \lambda), \phi(z_i; \omega, \gamma) \big)_{i \in [n]}$. 


\end{definition}
Similarly, utility functions in $\mathcal{U}_{\lambda, \gamma}$ can be characterized by the vector $\mathbf{u} = (c_1/d_0, c_2/d_0)$, which belongs to space $\mathcal{U}^{*} \subset \mathbb{R}^2$ (cf. \eqref{eq:utility-class} and \eqref{eq:phi-linear}). This gives the following result:
\begin{theorem} 
\label{theorem:dual-space-isomorphism}
Let $\mathcal{P}_{\omega} \subset \mathbb{R}^2$ denote the space where each data point $z_i \in \mathcal{D}$ is embedded as $\mathbf{e_i} = \big(\phi(z_i; \omega, \lambda), \phi(z_i; \omega, \gamma)\big)$. Let $\mathcal{U}^{*} \subset \mathbb{R}^2$ denote the space of linear utilities, where each utility function $\mathbf{u} \in \mathcal{U}^{*}$ is represented as $\mathbf{u} = \left(\frac{c_1}{d_0}, \frac{c_2}{d_0} \right)$. Then, $\mathcal{U}^{*}$ is isomorphic to the dual space $\mathcal{P}_{\omega}^*$ of $\mathcal{P}_{\omega}$. 
\end{theorem}
\cref{theorem:dual-space-isomorphism} establishes a correspondence between utility functions and linear functionals over the data embedding space. In other words, each data value $\phi(z_i; \omega, u)$ results from applying a linear transformation parameterized by $\mathbf{u}$ to the embedded data representation $\mathbf{e}_i$. This directly leads to the following geometric interpretation.
\begin{corollary}
The data value $\phi(z_i; \omega, u)$ can be expressed as the scalar product $\phi(z_i; \omega, u) = \mathbf{u} \cdot \mathbf{e}_i = \mathbf{u}(c_1, c_2, d_0) \cdot \mathbf{e}_i(\omega)$.
This identifies $\phi(z_i; \omega, u)$ as the projection of $\mathbf{e_i}$ onto the utility direction $\mathbf{u}$, explicitly separating the contribution of $u$ and $\mathcal{D}, \omega$.
\end{corollary}

\noindent \textbf{Utility directions and the unit sphere.} Building on this geometric framework, we aim to understand how distinct utilities in $\mathcal{U}^{*}$ induce similar or divergent rankings of data values. Since rankings depend only on relative orderings and not absolute values, the space of all distinct utilities (in terms of ranking) corresponds to the set of possible directions in $\mathcal{U}^{*}$, which can be naturally identified with the unit sphere. \cref{prop:unit-sphere} formalizes this observation. 
\begin{proposition}
\label{prop:unit-sphere}
(\textit{Unit sphere representation of distinct utilities}) Let $\mathbf{u} \in \mathcal{U}^{*}$. The set of utilities that share the same direction as $\mathbf{u}$ is in bijection with the unit sphere $\mathcal{S}^1 \subset \mathcal{U}^*$ and is uniquely represented by $\tilde{\mathbf{u}} = \mathbf{u}/\|\mathbf{u}\| = (c_1, c_2)/\sqrt{c_1^2 + c_2^2}$.
\end{proposition}
We visually illustrate the geometric concepts discussed so far in Figure \ref{fig:geometric-visualization} for a specific dataset. 
\begin{figure*}[t]
    \centering
    \subfloat[Spatial signature under $\omega_{\text{shap}}$]{%
        \includegraphics[width=0.25\textwidth]{figures/geometry_wind_shapley.png}
    }
    \hfill
    \subfloat[Spatial signature under $\omega_{\text{beta}}$]{%
        \includegraphics[width=0.25\textwidth]{figures/geometry_wind_beta.png}
    }
    \hfill
    \subfloat[Spatial signature under $\omega_{\text{banz}}$]{%
        \includegraphics[width=0.25\textwidth]{figures/geometry_wind_banzhaf.png}
    }
    
    \caption{These three figures illustrate the spatial signatures of the \textsc{wind} dataset under three semivalues: $\omega_{\text{shap}}, \omega_{\text{beta}}, \omega_{\text{banz}}$. The unit circle represents the set of distinct utilities in terms of ranking, with markers indicating the normalized utility vectors $\tilde{\mathbf{u}}_{\text{acc}}, \tilde{\mathbf{u}}_{\text{rec}}$, and $\tilde{\mathbf{u}}_{\text{am}}$ for the three utility functions used in our experiments, namely the accuracy, the recall, and the arithmetic mean that belong to $\mathcal{U}_{\lambda, \gamma}$.}
    \label{fig:geometric-visualization}
    \vskip -0.1in
\end{figure*}


\subsection{Insights on ranking diversity for extreme cases of spatial signatures.}
The spatial signature of $\mathcal{D}$ in $\mathcal{P}_{\omega}$ plays a crucial role in shaping the rankings induced by different utility functions. In particular, we analyze two extreme cases: (a) $\mathbf{e}$ is in \textit{general position} (\cref{def:general-position}), (b) $\mathbf{e}$ is \textit{collinear} (\cref{def:collinearity}).
\begin{definition}
\label{def:general-position} (\textit{General position})
A spatial signature $\mathbf{e} = (\mathbf{e}_i)_{i \in [n]} \subset \mathcal{P}_{\omega}$ is in general position if:
\begin{enumerate}
    \item For all distinct $i,j,k$, there does not exist a line $L \subset \mathcal{P}_{\omega}$ such that $\mathbf{e}_i, \mathbf{e}_j, \mathbf{e}_k \in L$.
    \item For all distinct $i,j$, there is no scalar $k>0$ such that $\mathbf{e}_i = k \mathbf{e}_j$.
\end{enumerate}
\end{definition}
\begin{definition}
\textit{(Collinearity)}
\label{def:collinearity}
A spatial signature $\mathbf{e} = (\mathbf{e}_i)_{i \in [n]} \subset \mathcal{P}_{\omega}$ is collinear if there exists $\mathbf{w} \in \mathcal{P}_{\omega}$ and scalar $k_i \in \mathbb{R}$ such that $\mathbf{e}_i = k_i \mathbf{w}$ for all $i \in [n]$.
\end{definition}
We establish \cref{theorem:ranking-diversity}, which quantifies the impact of these spatial configurations on ranking diversity.
\begin{theorem}
\label{theorem:ranking-diversity}
Let $\mathbf{e} = (\mathbf{e}_i)_{i \in [n]}$ be a spatial signature in $\mathcal{P}_{\omega}$. Define the ranking regions as the connected components of the unit sphere $\mathcal{S}^1$ where the linear utilities $\tilde{\mathbf{u}} \in \mathcal{S}^1$ induce identical rankings on $\mathcal{D}$. Then,
\begin{enumerate}
    \item if $\mathbf{e}$ is in general position, the number of distinct ranking regions is maximal and equal to $R_{\text{gen}}(n) = 2 \times \binom{n}{2}$.
    \item if $\mathbf{e}$ is collinear, the number of distinct ranking regions is minimal as it collapses to $R_{\text{col}}(n) = 2$.
\end{enumerate}
\end{theorem}
To illustrate this result, Figure~\ref{fig:ranking-regions} presents these two extreme cases for a dataset of three points. 
\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.65\linewidth]{figures/polar_cases.png}\\[-5mm]
    \caption{Ranking regions induced by linear utilities on the unit circle $\mathcal{S}^1$ for collinear (left) and general-position (right) spatial signatures of three data points. In the collinear case, only two ranking regions exist—rankings flip when $\tilde{\mathbf{u}}$ crosses the direction orthogonal to $\mathbf{w} = (1,0)$. In the general-position case, six distinct ranking regions emerge. The red crosses correspond to the sets $\mathcal{H}_{ij} = \left\{\tilde{\mathbf{u}} \in \mathcal{S}^1 \mid \tilde{\mathbf{u}} \cdot (\mathbf{e}_i - \mathbf{e}_j) = 0 \right\}$, which are the utility points for which two data points $\mathbf{e}_i$ and $\mathbf{e}_j$ have equal utility values, leading to ranking transitions. The black dashed lines partition $\mathcal{S}^1$ into ranking regions by connecting pairs of antipodal points $\mathcal{H}_{ij}$.}
    \label{fig:ranking-regions}
\end{figure}
  
For a spatial signature $\mathbf{e}$ that is neither collinear nor in general position, let $\sigma_1 > \sigma_2 > 0$ be the singular values of $\mathbf{e}$ and $\rho = \sigma_1 / \sigma_2$. The number of ranking regions $R(n)$ satisfies $2 < R(n) < 2\binom{n}{2}$ and $R(n)$ decreases as $\rho$ increases. As $\rho \to 1^+$ (balanced variance), $R(n) \to 2\binom{n}{2}$. As $\rho \to +\infty$ (collinearity), $R(n) \to 2$. The transition is governed by the alignment of the line crossing the circle at antipodal pairs $\mathcal{H}_{i j}=\left\{\tilde{\mathbf{u}} \in \mathcal{S}^1 \mid \tilde{\mathbf{u}} \cdot\left(\mathbf{e}_i-\mathbf{e}_j\right)=0\right\} \subset S^1$ with the principal eigenvector of $\mathbf{e}$'s covariance matrix, which dominates as $\rho$ grows.  

\subsection{Explaining experimental results from Section \ref{sec:experimental-investigation}}  
From the spatial signatures of the \textsc{wind} dataset in Figure~\ref{fig:geometric-visualization}, we observe an almost collinear structure in \(\mathcal{P}_{\omega_{\text{shap}}}\), \(\mathcal{P}_{\omega_{\text{beta}}}\), and \(\mathcal{P}_{\omega_{\text{banz}}}\). This near-alignment indicates that the embedded points \(\mathbf{e}_i\) predominantly lie along the principal eigenvector \(\mathbf{w}_1^{\omega}\) of each spatial signature’s covariance matrix. For analytical simplicity, we treat these signatures as fully collinear.  

By \cref{theorem:ranking-diversity}, collinearity collapses the unit circle \(\mathcal{S}^1\) into \textbf{two ranking regions}, separated by the antipodal points orthogonal to $\mathbf{w}_1^{\omega}$. These antipodal points, defined as $H_{\omega} = \left\{\tilde{\mathbf{u}} \in \mathcal{S}^1 \mid \tilde{\mathbf{u}} \cdot \mathbf{w}_1^{\omega} = 0 \right\}$,
partition $\mathcal{S}^1$ into hemispheres where $\tilde{\mathbf{u}} \cdot \mathbf{w}_1^{\omega} > 0$ or $\tilde{\mathbf{u}} \cdot \mathbf{w}_1^{\omega} < 0$. Utilities within the same hemisphere induce identical rankings, yielding perfect correlation.  Figure~\ref{fig:kendall-tau-abacus} (left) shows that in our experiments: under $\omega_{\text{shap}}$ and $\omega_{\text{beta}}$, the utilities $\tilde{\mathbf{u}}_{\text{acc}}$, $\tilde{\mathbf{u}}_{\text{rec}}$, and $\tilde{\mathbf{u}}_{\text{am}}$ lie in the \textit{same hemisphere}, explaining their strong correlation.  Under $\omega_{\text{banz}}$, $\tilde{\mathbf{u}}_{\text{rec}}$ aligns \textit{exactly} with the boundary point $H_{\omega}$, satisfying $\tilde{\mathbf{u}}_{\text{rec}} \cdot \mathbf{e}_i = 0 \quad \forall i$ which orthogonalizes it to the collinear direction $\mathbf{w}_1^{\omega}$. This degeneracy equalizes utility values across all points, producing tied rankings and the observed weak correlation with $\tilde{\mathbf{u}}_{\text{acc}}$ and $\tilde{\mathbf{u}}_{\text{am}}$. Figures~\ref{fig:geometric-visualization-breast}, \ref{fig:geometric-visualization-titanic}, \ref{fig:geometric-visualization-credit}, \ref{fig:geometric-visualization-heart}, \ref{fig:geometric-visualization-cpu}, \ref{fig:geometric-visualization-2dplanes} and \ref{fig:geometric-visualization-pol} in Appendix \ref{sec:more-spatial-signatures} present similar visualizations for other experimental datasets. We observe that Banzhaf consistently produces nearly collinear spatial signatures across all datasets, leading to reduced ranking diversity as described in \cref{theorem:ranking-diversity}. In contrast, Shapley and Beta Shapley exhibit more dataset-dependent structures, sometimes aligning with Banzhaf but often showing greater dispersion, resulting in higher ranking variability.

To further illustrate ranking diversity in this setting, particularly transitions across ranking regions, we present Figure \ref{fig:kendall-tau-abacus}, which depicts the evolution of the Kendall rank correlation along a convex combination path of utility functions in $\mathcal{S}^1$. This path is specifically designed to cross one of the two antipodals points in $H_{\omega}$ that define the ranking regions, encompassing the three key utilities we analyzed $\mathbf{\tilde{u}}_{\text{acc}}, \mathbf{\tilde{u}}_{\text{rec}}, \mathbf{\tilde{u}}_{\text{am}}$. The resulting visualization captures how rank correlation fluctuates as the utility function moves along this path. To formally derive this figure, we rely on the following proposition.
\begin{proposition}
\label{prop:kendall-rank-corr}
Let $\mathcal{D}$ be a dataset, $\omega$ a semivalue and let $u, v \in \mathcal{U}_{\lambda,\gamma}$ be two utilities respectively characterized by $(c_0, c_1, c_2, d_0) \in \mathbb{R}^4$ and $(c_0^{\prime}, c_1^{\prime}, c_2^{\prime}, d_0^{\prime}) \in \mathbb{R}^4$. The Kendall rank correlation between the data values sets $\{\phi(z, \omega, u)\}_{z \in \mathcal{D}}$ and $\{\phi(z, \omega, v)\}_{z \in \mathcal{D}}$ denoted as $\tau(u, v)$ is defined as\\[-7mm]
\begin{align*}
    \tau(u,v) &= \frac{1}{\binom{n}{2}} \sum_{i < j} \operatorname{sgn}\Big[\frac{1}{d_0d_0'}[
        c_1c_1^{\prime} D_{ij}^2(\omega, \lambda) \\
        &\quad + (c_1c_2^{\prime} + c_1^{\prime}c_2)D_{ij}(\omega,\lambda)D_{ij}(\omega,\gamma) \\
        &\quad + c_2c_2^{\prime}D_{ij}^2(\omega,\gamma)]
    \Big] \\[-5mm]
\end{align*}
where $\operatorname{sgn}(x) = 1$ if $x > 0$, $0$ if $x = 0$, and $-1$ if $x < 0$ and for any utility $h$, $D_{ij}(\omega, h) = \phi(z_i;\omega,h) - \phi(z_j;\omega,h)$.
\end{proposition}
\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/ss+kendall.png}\\[-5mm]
    \caption{(Left) The spatial signature of the dataset is shown along with the unit circle of utilities $\mathcal{S}^1$, with the two ranking regions induced by different utility functions. The dashed curve represents a \textit{convex path} of utility functions. The points corresponding to accuracy (\textsc{ACC}), recall (\textsc{REC}), and arithmetic mean (\textsc{AM}) utilities are marked. (Right) Kendall rank correlation between rankings induced by pairs of utility functions along the convex path as a function of parameters $p$ and $q$. The color scale encodes the rank correlation, with blue indicating a negative correlation and red indicating a positive correlation. Pairs \textsc{ACC}-\textsc{REC}, \textsc{ACC}-\textsc{AM} and \textsc{AM}-\textsc{REC} are represented as crosses of different colors.}
    \label{fig:kendall-tau-abacus}
\end{figure}


\section{Conclusion}
This work advances the understanding of how utility functions impact semivalue-based data valuation. Our experiments show that the way a utility influences data value rankings is not solely dictated by its intrinsic properties but rather by its specific interaction with the dataset and the semivalue weights. We explain this interaction by introducing a geometric framework that clarifies empirical results. 
\noindent \textbf{Limitations.} Our study focuses on a specific subclass of classification utility functions, limiting the generality of our theoretical findings. While this restriction enables a deeper analytical understanding, it remains unclear how our results extend to broader classes of utility functions. However, we have extended our experimental analysis to other utility functions beyond this subclass in Appendix \ref{sec:additional-experiments}, and the empirical conclusions remain consistent. An extension of our theoretical framework to more general utility functions is a key direction for future research.



\section*{Broader impact statement}

This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here.

\bibliography{arxiv}
\bibliographystyle{icml2025}
\newpage
\appendix
\onecolumn
\clearpage
\section{Additional definitions}
\begin{definition}
\textit{(Spearman rank correlation)}. The Spearman rank correlation coefficient $\rho_S$ measures the monotonic relationship between two sets of values $\{\phi_i^u\}_{i \in [n]}$ and $\{\phi_i^v\}_{i \in [n]}$ assigned to $n$ elements. Let $\operatorname{rg}(\phi_i^u)$ and $\operatorname{rg}(\phi_i^v)$ denote the ranks of $\phi_i^u$ and $\phi_i^v$ within their respective sets. The Spearman coefficient $\rho_S(u, v)$ is defined as the Pearson correlation coefficient between the ranked values:
\begin{align*}
\rho(u, v) = \frac{\operatorname{Cov}\left(\operatorname{rg}(\phi^u), \operatorname{rg}(\phi^v)\right)}{\sigma_{\operatorname{rg}(\phi^u)} \cdot \sigma_{\operatorname{rg}(\phi^v)}}   
\end{align*}
where $\operatorname{Cov}$ denotes covariance, $\sigma_{\operatorname{rg}(\phi^u)}$ and $\sigma_{\operatorname{rg}(\phi^v)}$ are the standard deviations of the ranks of $\{\phi_i^u\}$ and $\{\phi_i^v\}$, respectively.

If there are no tied ranks, $\rho_S(u, v)$ simplifies to:
\begin{align*}
\rho_S(u, v) = 1 - \frac{6 \sum_{i=1}^n d_i^2}{n(n^2 - 1)},  
\end{align*}
where $d_i = \operatorname{rg}(\phi_i^u) - \operatorname{rg}(\phi_i^v)$.  

The coefficient $\rho_S(u, v)$ satisfies $-1 \leq \rho_S(u, v) \leq 1$, where $\rho_S(u, v) = 1$ indicates perfect monotonic agreement between rankings (ranks increase identically), $\rho_S(u, v) = -1$ indicate perfect monotonic disagreement (ranks are inversely ordered) and $\rho_S(u, v) = 0$ indicates no monotonic association.
\end{definition}
We consider a binary classification task, where each sample belongs to one of two classes $\in \{0, 1\}$. Given a dataset of size $n$, we define the following classification utility functions:

\begin{definition}
\textit{(Accuracy)}.  
Accuracy measures the proportion of correctly classified instances and is defined as:
\begin{align*}
\text{Accuracy} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{TN} + \text{FP} + \text{FN}},  
\end{align*}
where TP (true positives), TN (true negatives), FP (false positives), and FN (false negatives) denote the number of samples correctly or incorrectly classified with respect to the positive and negative classes.
\end{definition}

\begin{definition}
\textit{(Recall)}.  
Also known as true positive rate (TPR), recall quantifies the model’s ability to correctly identify positive instances. It is given by:
\begin{align*}
\text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}, 
\end{align*}
\end{definition}

\begin{definition}
\textit{(Arithmetic mean)}.  
\begin{align*}
    \text{AM} = \frac{1}{2} (\text{TPR} + \text{TNR})
\end{align*}
\end{definition}
\begin{definition}
\textit{(F1-score)}.  
The F1-score provides a balance between precision and recall by computing their harmonic mean. It is defined as:
\begin{align*}
\text{F1} = 2 \cdot \frac{P \cdot R}{P + R},  
\end{align*}
where precision $P$ is given by:
\begin{align*}
P = \frac{\text{TP}}{\text{TP} + \text{FP}}
\end{align*}
\end{definition}

\begin{definition}
\textit{(Negative log loss)}.  
Negative log loss (NLL) evaluates the confidence of probabilistic predictions. Given a dataset of size $n$, where each sample has a true label $y_i \in \{0,1\}$ and a predicted probability $p_i \in [0,1] $ for class 1, NLL is defined as
\begin{align*}
\text{NLL} = -\frac{1}{n} \sum_{i=1}^n \left[ y_i \log p_i + (1 - y_i) \log(1 - p_i) \right]. 
\end{align*}
\end{definition}
\clearpage
\section{Proofs \& complementary analytical results}
\label{sec:proofs}
\subsection{Proofs of propositions}
\begin{proposition}
(Restate of \cref{prop:linear-decomposition}) Let $\mathcal{D} = \{z_i\}_{i \in [n]}$ be a dataset and $\omega$ a semivalue weight vector. For any utility function $u \in \mathcal{U}_{\lambda, \gamma}$ characterized by coefficients $(c_0, c_1, c_2, d_0) \in \mathbb{R}^4$, the data value assigned to $z_i$ can be decomposed as
\begin{align*}
    \phi(z_i; \omega, u) = \frac{c_1}{d_0} \phi(z_i; \omega, \lambda) + \frac{c_2}{d_0} \phi(z_i; \omega, \gamma)
\end{align*}
\end{proposition}
\begin{proof}
By assumption, the utility function $u$ satisfies the decomposition $u(S) = \frac{c_0 + c_1 \lambda(S) + c_2 \gamma(S)}{d_0}, \quad \forall S \subseteq \mathcal{D}$. Thus, for all $z_i \in \mathcal{D}$, $\phi(z_i; \omega, u) = \phi\left(z_i; \omega, \frac{c_0 + c_1 \lambda + c_2 \gamma}{d_0} \right)$. Using the linearity axiom of semivalues defined in \ref{subsec:semivalues}, we get $\phi(z_i; \omega, u) = \frac{1}{d_0} \left( c_0 \phi(z_i; \omega, 1) + c_1 \phi(z_i; \omega, \lambda) + c_2 \phi(z_i; \omega, \gamma) \right)$.
Since $\phi(z_i; \omega, 1) = 0$, we obtain
\begin{align*}
\phi(z_i; \omega, u) = \frac{c_1}{d_0} \phi(z_i; \omega, \lambda) + \frac{c_2}{d_0} \phi(z_i; \omega, \gamma).
\end{align*}
\end{proof}
\begin{proposition}
(Restate of \cref{prop:unit-sphere}) Let $\mathbf{u} \in \mathcal{U}^{*}$. The set of utilities that share the same direction as $\mathbf{u}$ is in bijection with the unit sphere $\mathcal{S}^1 \subset \mathcal{U}^*$ and is uniquely represented by: 
\begin{align*}
    \tilde{\mathbf{u}} = \frac{\mathbf{u}}{\|\mathbf{u}\|} = \frac{\left(c_1, c_2\right)}{\sqrt{c_1^2 + c_2^2}}
\end{align*}
\end{proposition}
\begin{proof}
We proceed in two steps: first, proving surjectivity, then proving injectivity.
\begin{itemize}
    \item[--] \textit{Surjectivity.} Every $\mathbf{u} \in \mathcal{U}^*$ can be normalized to $\tilde{\mathbf{u}}=\frac{\mathbf{u}}{\|\mathbf{u}\|} \in \mathcal{S}^1$ with the ranking induced by $\mathbf{u}$ identical to that induced by $\tilde{\mathbf{u}}$.
    \item[--] \textit{Injectivity.} Suppose $\tilde{\mathbf{u}}_1, \tilde{\mathbf{u}}_2 \in \mathcal{S}^1$ induce the same ranking. Then $\tilde{\mathbf{u}}_1=\tilde{\mathbf{u}}_2$.
    In fact, if $\tilde{\mathbf{u}}_1 \neq \tilde{\mathbf{u}}_2$, there exists a point $\mathbf{e}_i$ such that $\tilde{\mathbf{u}}_1 \cdot \mathbf{e}_i>0$ but $\tilde{\mathbf{u}}_2 \cdot \mathbf{e}_i<0$, violating identical rankings. 
\end{itemize}
Thus, the map $\mathbf{u} \mapsto \tilde{\mathbf{u}}$ sends each directionally equivalent utility to a unique point on $\mathcal{S}^1$.
\end{proof}
\begin{proposition}
(Restate of \cref{prop:kendall-rank-corr}) Let $\mathcal{D}$ be a dataset, $\omega$ a semivalue and let $u, v \in \mathcal{U}_{\lambda,\gamma}$ be two utilities respectively characterized by $(c_0, c_1, c_2, d_0) \in \mathbb{R}^4$ and $(c_0^{\prime}, c_1^{\prime}, c_2^{\prime}, d_0^{\prime}) \in \mathbb{R}^4$. The Kendall rank correlation between the data values sets $\{\phi(z, \omega, u)\}_{z \in \mathcal{D}}$ and $\{\phi(z, \omega, v)\}_{z \in \mathcal{D}}$ denoted as $\tau(u, v)$ is defined as
\begin{align*}
    \tau(u,v) &= \frac{1}{\binom{n}{2}} \sum_{i < j} \operatorname{sgn} \Big[\frac{1}{d_0d_0^{\prime}}[
        c_1c_1^{\prime} D_{ij^2}(\omega, \lambda) + (c_1c_2^{\prime} + c_1^{\prime}c_2)D_{ij}(\omega,\lambda)D_{ij}(\omega,\gamma) + c_2c_2^{\prime}D_{ij}^2(\omega,\gamma)]
    \Big]
\end{align*}
where $\operatorname{sgn}(x) = 1$ if $x > 0$, $0$ if $x = 0$, and $-1$ if $x < 0$ and for any utility $h$, $D_{ij}(\omega, h) = \phi(z_i;\omega,h) - \phi(z_j;\omega,h)$.
\end{proposition}
\begin{proof}
From \cref{prop:linear-decomposition}, we have for all $i \in [n]$,
\begin{align*}
    \phi(z_i;\omega, u) = \frac{c_1}{d_0} \phi(z_i; \omega, \lambda) + \frac{c_2}{d_0} \phi(z_i; \omega, \gamma).
\end{align*}
\begin{align*}
    \phi(z_i;\omega, v) = \frac{c_1'}{d_0'} \phi(z_i; \omega, \lambda) + \frac{c_2'}{d_0'} \phi(z_i; \omega, \gamma).
\end{align*}
For any pair $(i,j)$,
\begin{align*}
    D_{ij}(\omega,u) = \phi(z_i;\omega,u) -  \phi(z_j;\omega,u) = \frac{c_1}{d_0} D_{ij}(\omega,\lambda) + \frac{c_2}{d_0} D_{ij}(\omega,\gamma).
\end{align*}
\begin{align*}
    D_{ij}(\omega,v) = \phi(z_i;\omega,v) -  \phi(z_j;\omega,v) = \frac{c_1^{\prime}}{d_0^{\prime}} D_{ij}(\omega,\lambda) + \frac{c_2^{\prime}}{d_0^{\prime}} D_{ij}(\omega,\gamma).
\end{align*}
By \cref{def:kendall-tau},
\begin{align}
    \tau(u,v) = \frac{1}{\binom{n}{2}} \sum_{i < j} \operatorname{sgn}[D_{ij}(\omega,u)D_{ij}(\omega,v)]
\end{align}
with $D_{ij}(\omega,u)D_{ij}(\omega,v) = \frac{1}{d_0d_0^{\prime}}[
        c_1c_1^{\prime} D_{ij}^2 (\omega, \lambda) + (c_1c_2^{\prime} + c_1^{\prime}c_2)D_{ij}(\omega,\lambda)D_{ij}(\omega,\gamma) + c_2c_2^{\prime}D_{ij}^2(\omega,\gamma)]$.
\end{proof}
\subsection{Proofs of theorems}
\begin{theorem} 
(Restate of \cref{theorem:dual-space-isomorphism}) Let $\mathcal{P}_{\omega} \subset \mathbb{R}^2$ denote the space where each data point $z_i \in \mathcal{D}$ is embedded as $\mathbf{e_i} = \big(\phi(z_i; \omega, \lambda), \phi(z_i; \omega, \gamma)\big)$. Let $\mathcal{U}^{*} \subset \mathbb{R}^2$ denote the space of linear utilities, where each utility function $\mathbf{u} \in \mathcal{U}^{*}$ is represented as $\mathbf{u} = \left(\frac{c_1}{d_0}, \frac{c_2}{d_0} \right)$. Then, $\mathcal{U}^{*}$ is isomorphic to the dual space $\mathcal{P}_{\omega}^*$ of $\mathcal{P}_{\omega}$.
\end{theorem}
\begin{proof}
We define the map $\Psi : \mathcal{U}^* \rightarrow \mathcal{P}_{\omega}^{*}$ by 
\begin{align*}
    \Psi(\mathbf{u})(\mathbf{e}_i) = \mathbf{u} \cdot \mathbf{e_i} = \frac{c_1}{d_0} \phi(z_i; \omega, \lambda) + \frac{c_2}{d_0} \phi(z_i;\omega,\gamma)
\end{align*}
where $\mathbf{u} = \left(\frac{c_1}{d_0}, \frac{c_2}{d_0}\right)$. This map is linear by the linearity of the dot product.
\begin{itemize}
    \item[--] \textit{Injectivity of $\Psi$}. Suppose $\Psi(\mathbf{u}) = \Psi(\mathbf{v})$. Then, for all $\mathbf{e_i} \in \mathcal{P}_{\omega}$, 
    \begin{align*}
        \mathbf{u} \cdot \mathbf{e}_i = \mathbf{v} \cdot \mathbf{e}_i
    \end{align*}
    Since $\mathcal{P}_{\omega}$ spans $\mathbb{R}^2$ ($\lambda$ and $\gamma$ are linearly independent utilities), this implies $\mathbf{u} = \mathbf{v}$. Thus, $\Psi$ is injective. 
    \item[--] \textit{Surjectivity of $\Psi$}. Any linear functional $f \in \mathcal{P}^{*}$ can be written as
    \begin{align*}
        f = \alpha \mathbf{e}^{\lambda} + \beta \mathbf{e}^{\gamma} \quad \text{for } \alpha, \beta \in \mathbb{R}
    \end{align*}
    where $(\mathbf{e}^{\lambda}, \mathbf{e}^{\gamma})$ is the dual basis. Setting $\alpha = \frac{c_1}{d_0}$ and $\beta = \frac{c_2}{d_0}$, we recover $\mathbf{u} = \left(\frac{c_1}{d_0}, \frac{c_2}{d_0}\right) \in \mathcal{U}^{*}$ such that $f = \Psi(\mathbf{u})$. Thus, $\Psi$ is subjective.
\end{itemize}
Since $\Psi$ is a linear bijection (both injective and surjective), it is an isomorphism.
\end{proof}
\begin{theorem} 
(Restate of \cref{theorem:ranking-diversity}) Let $\mathbf{e} = (\mathbf{e}_i)_{i \in [n]}$ be a spatial signature in $\mathcal{P}_{\omega}$. Define the ranking regions as the connected components of the unit sphere $\mathcal{S}^1$ where the linear utilities $\tilde{\mathbf{u}} \in \mathcal{S}^1$ induce identical rankings on $\mathcal{D}$. Then,
\begin{enumerate}
    \item if $\mathbf{e}$ is in general position, the number of distinct ranking regions is maximal equals to $R_{\text{gen}}(n) = 2 \times \binom{n}{2}$.
    \item if $\mathbf{e}$ is collinear, the number of distinct ranking regions is minimal as it collapses to $R_{\text{col}}(n) = 2$.
\end{enumerate}
\end{theorem}
\begin{proof}
When analyzing the ranking regions induced by linear utilities on $\mathcal{S}^1$, we consider how the set of inequalities
$\tilde{\mathbf{u}} \cdot \mathbf{e}_i > \tilde{\mathbf{u}} \cdot \mathbf{e}_j$ partitions $\mathcal{S}^1$. These partitions are determined by each set $H_{ij} = \left\{ \tilde{\mathbf{u}} \in \mathcal{S}^1 \mid \tilde{\mathbf{u}} \cdot (\mathbf{e}_i - \mathbf{e}_j) = 0 \right\}$,
which consists of the points on $\mathcal{S}^1$
where the ranking order of $\mathbf{e}_i$ and $\mathbf{e}_j$ flips.
\begin{enumerate}
\item If $\mathbf{e}$ is in general position, then for every pair $(i,j)$, the differences $\mathbf{e}_i - \mathbf{e}_j$ are pairwise non-collinear (since in general position, there is no collinear triples or radial pairs). This implies that for each pair $(i,j)$, the equation $\tilde{\mathbf{u}} \cdot (\mathbf{e}_i - \mathbf{e}_j) = 0$ defines a unique line through the origin in $\mathbb{R}^2$, which intersects $\mathcal{S}^1$
at exactly two distinct antipodal points. These two points partition $\mathcal{S}^1$ into two connected regions, corresponding to the two possible rankings between $\mathbf{e}_i$
and $\mathbf{e}_j$. Since there are $\binom{n}{2}$ such pairs and each contributes two distinct partitioning points, the total number of connected ranking regions is $R_{\text{gen}}(n) = 2 \times \binom{n}{2}$.
\item If $\mathbf{e}$ is collinear, all embedded points satisfy $\mathbf{e}_i = k_i \mathbf{w}$ for $\mathbf{w} \in \mathcal{P}_{\omega}$ and scalars $k_i \in \mathbb{R}$. The differences satisfy $\mathbf{e}_i - \mathbf{e}_j = (k_i - k_j) \mathbf{w}$, which means that all differences are collinear with $\mathbf{w}$. Consequently, the separating sets $H_{ij}$
are all aligned with the unique perpendicular direction $\mathbf{w}^{\perp}$ , which is a single line through the origin in $\mathbb{R}^2$, intersecting $\mathcal{S}^1$ at exactly two antipodal points.
Since all pairwise ranking reversals occur along this single separating direction, all ranking regions collapse into just two distinct regions $R_{\text{col}}(n) = 2$.
These two regions correspond to the cases $\tilde{\mathbf{u}} \cdot \mathbf{w} > 0$ and $\tilde{\mathbf{u}} \cdot \mathbf{w} < 0$, meaning that all rankings in each half-space are identical.
\end{enumerate}
\end{proof}
\subsection{Remark on the interpretation of low-rank correlations}
\label{subsec:remark-rank-interpretation}
Caution is essential when interpreting rank correlations from different utility functions. Poor rank correlations do not necessarily imply inconsistency in the assigned values for a data valuation application; rather, the values generated using different utility functions can still be equally useful for this task. This may occur when data points are clustered into groups of similar values for both utility functions, while the rankings within those groups differ depending on the utility function used. Figure \ref{fig:illustrative-example-spearman-real-data} and the analytical example above illustrate this idea.
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/rank_remark/spearman_rank_remark_real_data.png}
    \caption{Illustration of Shapley values computed with accuracy and negative log loss as utility functions on the open-source dataset \textsc{wind} which has been split into a training set with 10 points (50\% of which have noisy labels due to label flipping) and a test set with 200 points. The five lowest-valued points (highlighted with green circles) are observed to be associated with noisy points (red) across both utility functions. This figure demonstrates that poor rank correlations (Spearman correlation of 0.176) between different utility functions do not necessarily imply inconsistency in the data valuation task. Despite the low correlation, the five lowest points consistently belong to the same group, indicating that the points are correctly grouped into clusters of similar values.}
    \label{fig:illustrative-example-spearman-real-data}
\end{figure}

\noindent \textbf{Analytical illustration of low-rank correlation while clustering effect.} Suppose we have a training dataset $\mathcal{D}=\{z_i\}_{i \in [n]}$ partitioned as $\mathcal{D} = \mathcal{D}_{\text{high}} \cup \mathcal{D}_{\text{low}}$ where $\mathcal{D}_{\text{high}}$ contains $n_{\text{high}} = k\cdot m$ points and $\mathcal{D}_{\text{low}}$ contains $n_{\text{high}} = m$ points and. Here, $k$ is a positive constant representing the ratio of the sizes of the two subsets, so $n=(k+1)\cdot m$. Without loss of generality, we assume the indices in $\mathcal{D}$ are structured such that points in $\mathcal{D}_{\text{high}}$ have indices $1$ through 
$k \cdot m$ and those in $\mathcal{D}_{\text{low}}$ have indices $k \cdot (m+1)$ through $n$. Let $U: 2^{\mathcal{D}} \to \mathbb{R}^{+}$ be a function defined on any subset $S \subseteq \mathcal{D}$ such as $U(S) = \frac{\lvert \{z_i \in S: z_i \in \mathcal{D}_{\text{high}}\}\rvert}{C_{\omega}}$ where $C_{\omega} = \sum_{S \subseteq \mathcal{D} \backslash \{z\}} \omega(S) $, $z \in \mathcal{D}$.\footnote{Since the value of $C_{\omega}$ depends only on the size of $\mathcal{D} \backslash \{z\}$ (which has $n - 1$ elements if $\mathcal{D}$ has $n$ elements), and not on the specific point $z$ being excluded, $C_{\omega}$ remains the same regardless of the choice of $z$.} We define two utility functions $ U_{+}$ and $U_{-}$, as follows:
$$
U_{+}(S) = U(S) + \sum_{z_i \in S} i \cdot \varepsilon \quad \text{and} \quad U_{-}(S) = U(S) - \sum_{z_i \in S} i \cdot \varepsilon $$
where $\varepsilon > 0$. We assume $\varepsilon$ to be sufficiently small to satisfy $C_{\omega} \cdot k \cdot m \cdot \varepsilon < 1$.
Let $\varphi(z_i; \omega, V)$ denote the semivalue characterized by $\omega$ for each point $z_i$ under a utility function $V$. Then, under this setting, we have:
\begin{itemize}
\item[(i)] \textit{Cluster separation}: For any $z_i \in \mathcal{D}_{\text{high}}$ and $z_j \in \mathcal{D}_{\text{low}}$,
$$
\varphi(z_i; \omega, U_{+}) > \varphi(z_j; \omega, U_{+}) \quad \text{and} \quad \varphi(z_i; \omega, U_{-}) > \varphi(z_j; \omega, U_{-})$$
\item[(ii)] \textit{Low Spearman rank correlation}: The Spearman rank correlation between both sets of values $\{\varphi(z_i; \omega, U_{+})\}_{z_i \in \mathcal{D}}$ and $\{\varphi(z_i; \omega, U_{-})\}_{z_i \in \mathcal{D}}$ denoted as $\rho_{s}(k,m)$ equals:
$$
\rho_{s}(k,m) = 1-\frac{6\left(\sum_{j=1}^{k \cdot m}(2 j-k \cdot m-1)^2+\sum_{j=1}^m(2 j-m-1)^2\right)}{(k+1) \cdot m\left(((k+1) \cdot m)^2-1\right)}
$$
and for $m$ fixed, $\exists \ k^{*}$ such that $\rho_{s}(k^{*},m) \approx 0$.
\end{itemize}
\begin{proof}
    \begin{itemize}
        \item[(i)] \textit{Cluster separation}: For any $z_i \in \mathcal{D}_{\text{high}}$ and $z_j \in \mathcal{D}_{\text{low}}$, we have by definition of $U$,
        \begin{align*}
            \varphi(z_i; \omega, U) &= \sum_{S \subseteq \mathcal{D} \backslash \{z_i\}} \omega(S)[U(S \cup \{z_i\}) - U(S)] = \sum_{S \subseteq \mathcal{D} \backslash \{z_i\}} \omega(S) \cdot \frac{1}{C_{\omega}} = 1 \\
            \varphi(z_j; \omega, U) &= \sum_{S \subseteq \mathcal{D} \backslash \{z_j\}} \omega(S)[U(S \cup \{z_j\}) - U(S)] = 0.
        \end{align*}
        Then, we use the linearity of the semivalue to express the following:
        \begin{align*}
            \varphi(z_i; \omega, U_{+}) &= \varphi(z_i; \omega, U) + C_{\omega} \cdot i \cdot \varepsilon  = 1 + C_{\omega} \cdot i \cdot \varepsilon \\
            \varphi(z_j; \omega, U_{+}) &= \varphi(z_j; \omega, U) + C_{\omega} \cdot j \cdot \varepsilon  = C_{\omega} \cdot j \cdot \varepsilon
        \end{align*}
        Similarly, for $U_{-}$, we have:
        \begin{align*}
            \varphi(z_i; \omega, U_{-}) &= \varphi(z_i; \omega, U) - C_{\omega} \cdot i \cdot \varepsilon  = 1 - C_{\omega} \cdot i \cdot \varepsilon \\
            \varphi(z_j; \omega, U_{-}) &= \varphi(z_j; \omega, U) - C_{\omega} \cdot j \cdot \varepsilon  = - C_{\omega} \cdot j \cdot \varepsilon
        \end{align*}
        Since $0 < C_{\omega} \cdot j \cdot \varepsilon < 1$ and $0 < C_{\omega} \cdot i \cdot \varepsilon < 1$, we conclude that:
        $$
        \varphi(z_i; \omega, U_{+}) < \varphi(z_j; \omega, U_{+}) \quad \text{and} \quad \varphi(z_i; \omega, U_{-}) < \varphi(z_j; \omega, U_{-}).
        $$
        \item[(ii)] \textit{Low Spearman rank correlation}: The Spearman rank correlation $\rho_s$ is defined as:
        $$
        \rho_s = 1 - \frac{6 \sum_{z_i \in \mathcal{D}}^n d_i^2}{n(n^2 - 1)},
        $$
        where $d_i = \text{rank}[\varphi(z_i; \omega, U_{+})] - \text{rank}[\varphi(z_i; \omega, U_{-})]$ is the difference in ranks for each data point $z_i$, and $n = (k + 1) \cdot m$ is the total number of points in the dataset. 
        
        For $\mathcal{D}_{\text{high}}$, based on the values computed in (i), the ranks of $\{\varphi(z_i; \omega, U_{+})\}_{z_i \in \mathcal{D}_{\text{high}}}$ are in descending order from $k \cdot m$ to 1, and the ranks of $\{\varphi(z_i; \omega, U_{-})\}_{z_i \in \mathcal{D}_{\text{high}}}$ are in ascending order from 1 to $k \cdot m$. Therefore, for each $z_i \in \mathcal{D}_{\text{high}}$, the rank difference $d_i$ is:
        $$
        d_i = \text{rank}[\varphi(z_i; \omega, U_{-})] - \text{rank}[\varphi(z_i; \omega, U_{+})] = i - (k \cdot m - i + 1) = 2i - k \cdot m - 1.
        $$
        For $\mathcal{D}_{\text{low}}$, based on the values computed in (i), the ranks of $\{\varphi(z_i; \omega, U_{+})\}_{z_i \in \mathcal{D}_{\text{low}}}$ are from $k \cdot m + 1$ to $(k + 1) \cdot m$, and the ranks of $\{\varphi(z_i; \omega, U_{-})\}_{z_i \in \mathcal{D}_{\text{low}}}$ are in descending order from $n = (k + 1) \cdot m$ to $k \cdot m + 1$. Therefore, for each $z_i \in \mathcal{D}_{\text{low}}$, the rank difference $d_i$ is:
        $$
        d_i = \text{rank}[\varphi(z_i; \omega, U_{-})] - \text{rank}[\varphi(z_i; \omega, U_{+})] = (k \cdot m + i) - \left( (k + 1) \cdot m - i + 1 \right) = 2i - m - 1.
        $$
        Hence, the total sum of squared rank differences is:
        $$
        \sum_{z_i \in \mathcal{D}} d_i^2 = \sum_{z_i \in \mathcal{D}_{\text{high}}} d_i^2 + \sum_{z_i \in \mathcal{D}_{\text{low}}} d_i^2 = 
        \sum_{i=1}^{n_{\text{high}}} (2i - k \cdot m - 1)^2 + \sum_{i=1}^{n_{\text{low}}} (2i - m - 1)^2 = \sum_{i=1}^{k \cdot m} (2i - k \cdot m - 1)^2 + \sum_{i=1}^{m} (2i - m - 1)^2.
        $$
        Substituting this into the formula for $\rho_s$, we obtain the Spearman rank correlation:
        $$
        \rho_s(k, m) = 1 - \frac{6 \left( \sum_{i=1}^{k \cdot m} (2i - k \cdot m - 1)^2 + \sum_{i=1}^{m} (2i - m - 1)^2 \right)}{(k + 1) \cdot m \left( ((k + 1) \cdot m)^2 - 1 \right)}.
        $$
        To illustrate the fact that for $m$ fixed, $\exists \ k^{*}$ such that $\rho_{s}(k^{*},m) \approx 0$, we calculate $\rho_s(4, 2)$ as an example. For $k = 4$ and $m = 2$, we have $n_{\text{high}} = 4 \cdot 2 = 8$ points in $\mathcal{D}_{\text{high}}$ and $n_{\text{low}} = 2$ points in $\mathcal{D}_{\text{low}}$, with a total dataset size of $n = (4 + 1) \cdot 2 = 10$ points. Substituting these values into the expression for $\rho_s$, we get:
        $$
        \rho_s(4, 2) = 1 - \frac{6 \left( \sum_{i=1}^{8} (2i - 8 - 1)^2 + \sum_{i=1}^{2} (2i - 2 - 1)^2 \right)}{5 \cdot 2 \cdot ((5 \cdot 2)^2 - 1)} \approx -0.03
        $$
        This yields a Spearman rank correlation near zero, supporting our claim of low-rank correlation between the values $\{\varphi(z_i; \omega, U_{+})\}_{i \in \mathcal{D}}$ and $\{\varphi(z_i; \omega, U_{-})\}_{i \in \mathcal{D}}$ for this particular choice of $k$ and $m$. Furthermore, to generalize this observation, we plot $\rho_s(k, m)$ for various values of $m$ and $k$ (see Figure \ref{fig:spearman_plot}). As shown in the figure, for each fixed $m$, choosing $k = 4$ leads to a Spearman rank correlation close to zero.
    \begin{figure}[ht]
        \centering
        \includegraphics[width=0.7\textwidth]{figures/rank_remark/spearman_plot.png}
        \caption{Spearman rank correlation $\rho_s(k, m)$ as a function of $k$ for different values of $m$.}
        \label{fig:spearman_plot}
    \end{figure}
    \end{itemize}
\end{proof}
\clearpage
\section{Additional settings \& experiments}
\label{sec:supplementary-experiments}
\subsection{Datasets}
\label{appendix:datasets}
Table \ref{tab:datasets} provides an overview of the datasets used in Section \ref{sec:experimental-investigation}. These datasets are widely utilized in the data valuation literature \cite{datashapley, betashapley, jia2023, databanzhaf, opendataval}. Due to the computational cost associated with repeated model retraining in our experiments, we select a subset of $100$ instances for training and $50$ instances for testing from each dataset. 
\begin{table}[h]
\centering
\begin{tabular}{@{}cccc@{}}
\toprule
\textbf{Dataset} &  \textbf{Source}                        \\ \midrule
Breast & \url{https://www.openml.org/d/13}\\
Titanic & \url{https://www.openml.org/d/40945} \\
Credit & \cite{creditpaper}\\
Heart & \url{https://www.openml.org/d/43398} \\
Wind              &\url{https://www.openml.org/d/847}   \\
CPU               &\url{https://www.openml.org/d/761}   \\
2DPlanes          &\url{https://www.openml.org/d/727}    \\
Pol               &\url{https://www.openml.org/d/722}   \\ \bottomrule
\end{tabular}
\caption{A summary of datasets used in experiments from Section \ref{sec:experimental-investigation}.}
\label{tab:datasets}
\end{table}
\subsection{Systematic methodology for isolating utility effects in semivalue approximation}  
\label{subsec:systematic-methodology}
For a given dataset $\mathcal{D}$, we propose a systematic methodology to isolate the effect of utility functions on data valuation rankings by controlling for extraneous variability. This is achieved through two key principles:  
\begin{enumerate}
    \item A \textit{fixed learning context} \(\mathcal{L} = (\mathcal{A}, \mathcal{D}_{\text{test}})\), ensuring consistency in model training and evaluation.
    \item \textit{Consistent semivalue approximations} across utility functions to maintain comparability.
\end{enumerate}  

\subsubsection{Fixed learning context $\mathcal{L}$}  

As outlined in Section \ref{sec:background}, a utility function $u$ is defined as:  
\begin{align*}
    u(S) = \textsc{perf}(\mathcal{A}(S), \mathcal{D}_{\text{test}}),
\end{align*}  
where $\mathcal{A}$ is a learning algorithm that trains a model on a subset $S$, and \textsc{perf} evaluates the model on a test set $\mathcal{D}_{\text{test}}$. The learning algorithm $\mathcal{A}$ specifies the model class, objective function, optimization procedure, and hyperparameters (e.g., learning rate, weight initialization). We define the \textit{learning context} as $\mathcal{L} = (\mathcal{A}, \mathcal{D}_{\text{test}})$.  

By fixing $\mathcal{L}$, the only varying factor is the performance metric $\textsc{perf}$. Changing $\textsc{perf}$ (e.g., accuracy vs. recall) induces different utility functions $u$ and $v$, while all other elements remain unchanged. Consequently, variations in data valuation rankings—quantified via Kendall’s $\tau(u, v)$—are exclusively attributable to differences in the chosen performance metrics.  

\subsubsection{Accounting for approximation variability in semivalue computation}  
The above reasoning holds if \textit{exact} semivalues are computed. However, in practice, semivalues are estimated using permutation sampling techniques, which introduce stochastic variability. This variability can affect data valuation rankings, making it difficult to attribute observed differences solely to the choice of utility function.  

To control for this, we propose \textit{aligned sampling} in addition to fixing the learning context $\mathcal{L}$. The fixed learning context ensures that all model training and evaluation factors remain constant, while aligned sampling guarantees that the same set of permutations is used across utility functions, eliminating variability introduced by the stochastic nature of semivalue approximations.  

\noindent \textbf{Fixed set of permutations.} Let $\mathcal{P}=\left\{\pi_1, \pi_2, \ldots, \pi_m\right\}$ denote a fixed set of $m$ random permutations of the data points in $\mathcal{D}$. The sampler applies this exact set of permutations across multiple utilities $\left\{u_1, u_2, \ldots, u_q\right\}$ such that $u_k(\cdot)= \textsc{perf}_k[(\mathcal{A})(\cdot), \mathcal{D}_{\text{test}}]$ with fixed $\mathcal{L} = (\mathcal{A}, \mathcal{D}_{\text{test}})$ for all $k \in [q]$.

For a given performance metric $\textsc{perf}_k$ and the set of permutations $\mathcal{P}$, the sampler estimates the marginal contributions $\{\hat{\Delta}_j\left(z_i ; u_k\right)\}_{j=1}^{n}$ for each data point $z_i \in \mathcal{D}$ with respect to the utility $u_k$ such as
$$
\hat{\Delta}_j\left(z_i ; u_k\right) := \frac{1}{m} \sum_{s=1}^m \left(u_k\left(S_j^{\pi_s} \cup \{z_i\}\right) - u_k\left(S_j^{\pi_s}\right)\right),
$$
where $m$ is the number of permutations used, $\pi_s$ denotes the $s$-th permutation and $S_j^{\pi_s}$ represents the subset of data points of size $j - 1$ that precedes  $z_i$ in the order defined by permutation $\pi_s$. 

Using the same permutation set $\mathcal{P}$ across performance metrics $u_1, u_2, \ldots, u_q$ ensures consistency in the sampling process. The sampler minimizes random variability by maintaining a fixed order of permutations, allowing each configuration to be evaluated in a stable and controlled framework. This setup provides a uniform basis for data valuation without introducing noise from randomly varying coalition structures.

\noindent \textbf{Determining $m$: convergence and maximum number of permutations.}
The number of permutations $m$ used in the marginal contribution estimator is determined based on both a maximum limit and a convergence criterion applied across all configurations. Specifically:
$$
m = \max\left(m_{\text{min}}, \min \left( m_{\text{max}}, m_{\text{conv}} \right)\right),$$
where: $m_{\text{min}}$ is a predefined minimum number of permutations to avoid starting convergence checks prematurely, $m_{\text{max}}$ is a predefined maximum number of permutations set to control computational feasibility, $m_{\text{conv}}$ is the smallest number of permutations required for the Gelman-Rubin (GR) \cite{gelmanrubin} statistic to converge across all utility functions $u_1, \hdots, u_q$. The use of the Gelman-Rubin statistic as a convergence criterion follows established practices in the literature \cite{opendataval, betashapley}.

Since $m$ is required to be consistent across all performance metrics, the GR statistic must indicate convergence for each $u$ before the sampling process stops. This means that if the GR statistic has not converged for even one utility, the computation of marginal contributions continues across all utilities.

For each data point $z_i$, the GR statistic $R_i$ is computed for every $100$ permutation across all utilities. The sampling process halts when the maximum GR statistic across all data points and all utilities falls below a threshold (e.g., $1.05$), indicating convergence:
$$
\max_{k} \max_{i=1, \dots, n} R_i^{k} < \text{threshold}.
$$
In this framework, the GR statistic, $ R_i^{k}$, is used to assess the convergence of marginal contribution estimates for each data point $z_i$ across multiple chains of sampled permutations under each utility $u_k$. The GR statistic evaluates the agreement between chains by comparing the variability within each chain to the variability across the chains, with convergence indicated when $R_i^{k}$ approaches 1. Specifically, to compute the GR statistic for each data point $z_i$ under $u_k$, we determine: 
\begin{enumerate}
    \item The within-chain variance $W_i^{k}$ which captures the variability of marginal contributions for $z_i$ within each chain. Specifically, if there are $c$ independent chains, $ W_i^{k}$ is calculated as the average of the sample variances within each chain:
    $$
    W_i^{k} = \frac{1}{C} \sum_{c=1}^C s_{i,c}^2,
    $$
    where $s_{i,c}^2$ is the sample variance of marginal contributions for $z_i$ within chain $c$. This term reflects the dispersion of estimates within each individual chain,
    \item And the between-chain variance $B_i^{k}$ which measures the variability between the mean marginal contributions across the chains. It indicates how much the chain means differ from each other. The between-chain variance is defined as:
    $$
    B_i^{k} = \frac{n}{C - 1} \sum_{c=1}^C \left(\bar{\Delta}_{c}(z_i; u_k) - \bar{\Delta}(z_i; u_k) \right)^2,
    $$
    where $\bar{\Delta}_{c}(z_i; u_k)$ is the mean marginal contribution for $z_i$ in chain $c$, and $\bar{\Delta}(z_i; u_k)$ is the overall mean across all chains:
    $$
    \bar{\Delta}(z_i; u_k) = \frac{1}{C} \sum_{c=1}^C \bar{\Delta}_{c}(z_i; u_k).
    $$
    The term $B_i^{k}$ quantifies the extent of disagreement among the chain means.
\end{enumerate}
Combining both $W_i^{k}$ and $ B_i^{k}$, the GR statistic $ R_i^{k}$ for data point $z_i$ under utility $u_k$ is defined as:
$$
   R_i^{k} = \sqrt{\frac{(n - 1)}{n} + \frac{B_i^{k}}{W_i^{k} \cdot n}},
$$
where $n$ is the number of samples per chain.

\noindent \textbf{Intra-permutation truncation.}
Building on existing literature \cite{datashapley, opendataval}, we improve computational efficiency by implementing an intra-permutation truncation criterion that restricts coalition growth once contributions stabilize. Given a random permutation $\pi_s \in \mathcal{P}$, the marginal contribution for each data point $z_{\pi_{s,j}}$ (the $j$-th point in the permutation $\pi_s$ is calculated incrementally as the coalition size $j$ increases from $1$ up to $n$. However, instead of expanding the coalition size through all $n$ elements, the algorithm stops increasing $j$ when the marginal contributions become stable based on a relative change threshold.

For each step $l \in [n]$ within a permutation, the relative change $V_l^{k}$ in the utility $u_k$ is calculated as:
$$
V_l^{k} := \frac{\left| u_k\left(\{z_{\pi_{s,j}}\}_{j=1}^l \cup \{z_{\pi_{s,l+1}}\}\right) - u_k\left(\{z_{\pi_{s,j}}\}_{j=1}^l\right) \right|}{u_k\left(\{z_{\pi_{s,j}}\}_{j=1}^l\right)}.
$$
where $\{z_{\pi_{s,j}}\}_{j=1}^l$ represents the coalition formed by the first $l$ data points in $\pi_s$. This measures the relative change in the utility $u_k$ when adding the next data point to the coalition. The truncation criterion stops increasing the coalition size at the smallest value $j$ satisfying the following condition:
$$
j^{*} = \arg \min \left\{ j \in [n] : \left| \{ l \leq j : V_l \leq 10^{-8} \} \right| \geq 10 \right\}.
$$
This means that the coalition size $j^{*}$ is fixed at the smallest $j$ for which there are at least $10$ prior values of $V_l$ (for $l \leq j$ that are smaller than a threshold of $10^{-8}$). This condition ensures that the utility $u_k$ has stabilized, indicating convergence within the permutation.
This intra-permutation truncation reduces computational overhead by avoiding unnecessary calculations once marginal contributions stabilize, thus improving efficiency.

\noindent \textbf{Aggregating marginal contributions for semivalues estimation.}
Once the marginal contributions have been estimated consistently across all permutations and configurations, these contributions are aggregated to compute various semi-values, such as the Shapley, Banzhaf, and Beta Shapley values. Each semivalue method applies a specific weighting scheme to the marginal contributions to reflect the intended measure of data point importance.

For a data point $z_i$ under utility $u_k$, the semivalue $ \hat{\varphi}(z_i;\omega, u_k)$ is computed by applying a weighting function $\omega$ to the marginal contributions across coalition sizes:
$$
\hat{\varphi}(z_i; \omega, u_k) = \sum_{j=1}^n \omega_j \, \hat{\Delta}_j(z_i; u_k),
$$
where $\hat{\Delta}_j(z_i; u_k) $ is the estimated marginal contribution for coalition size $j$, and $\omega_j$ is the weight assigned to coalition size $j$ in the semivalue calculation.

\subsection{Learning algorithm $\mathcal{A}$}  
This section details the fixed learning algorithm $\mathcal{A}$ used throughout all our experiments. 

Specifically, we use logistic regression as the model, trained with the L-BFGS optimization algorithm. The loss function is set to Binary Cross-Entropy (BCE), and $\ell_2$-regularization is applied with a $\lambda = 1.0$ coefficient. Model weights are initialized from a fixed normal distribution, and the learning rate is fixed at $1.0$. 

\subsection{Decision threshold calibration} 
We calibrate the decision threshold based on the proportion of positive labels in the training set. Instead of using a fixed threshold (e.g., 0.5), we adapt the threshold dynamically to match the expected class distribution. The calibration process works as follows: first, it computes the target proportion of positive labels in the training set. Then, it sorts the predicted probabilities in ascending order and selects the threshold at the position corresponding to the fraction of negative samples. This ensures that the proportion of instances classified as positive matches the empirical distribution observed in the training set, leading to a more dataset-adaptive classification decision.
\clearpage
\subsection{Additional experiments for Section \ref{sec:experimental-investigation}}
\label{sec:additional-experiments}
\subsubsection{Extended experiments with additional classification utilities}
\label{sec:extended-utilities}
To further explore the impact of utility function selection on data valuation rankings, we extend our experiments by replacing recall (\textsc{rec}) and arithmetic mean (\textsc{am}) with F1-score (\textsc{f1}) and negative log-loss (\textsc{nll}). While accuracy (\textsc{acc}) remains a common baseline, these additional utility functions introduce distinct mathematical and statistical properties.

The F1-score is a \textit{linear fractional measure}, as introduced in Section \ref{sec:geometric-interpretation}. While it shares similarities with linear performance measures such as accuracy, it remains fundamentally different, as accuracy corresponds to the special subset of this class for which $d_1 = d_2 = 0$. On the other hand, negative log-loss relies on \textit{probabilistic predictions} rather than discrete classification decisions, introducing a continuous performance evaluation criterion that accounts for model confidence. We incorporate these utility functions to examine whether their different mathematical structures affect data valuation rankings.

The results in Table~\ref{tab:table-extended-rank-correlation} show the Kendall rank correlations computed across these alternative utility functions for various datasets and three different semivalue-based data valuation methods: Data Shapley, $(4,1)$-Beta Shapley, and Data Banzhaf.

\begin{table*}[h!]
\vskip 0.15in
\begin{center}
\begin{scriptsize} 
\begin{sc}
\begin{tabular}{l|ccc|ccc|ccc}
\hline
\abovespace
\belowspace
dataset & \multicolumn{3}{c|}{shapley} & \multicolumn{3}{c|}{$(4,1)$-beta Shapley} & \multicolumn{3}{c}{banzhaf} \\
\abovespace\belowspace
& \textsc{acc}-\textsc{f1} & \textsc{acc}-\textsc{nll} & \textsc{f1}-\textsc{nll} & \textsc{acc}-\textsc{f1} & \textsc{acc}-\textsc{nll} & \textsc{f1}-\textsc{nll} & \textsc{acc}-\textsc{f1} & \textsc{acc}-\textsc{nll} & \textsc{f1}-\textsc{nll} \\
\hline
\abovespace
breast & 0.98 (0.01) & -0.59 (0.02) & -0.6 (0.02) & 0.98 (0.01) & -0.65 (0.01) & -0.66 (0.01) & 0.98 (0.01) & 0.18 (0.01) & 0.18 (0.01) \\
titanic & 0.63 (0.01) & -0.53 (0.01) & 0.54 (0.01) & 0.65 (0.01) & -0.60 (0.01) & -0.61 (0.01) & 0.26 (0.02) & 0.14 (0.02) & -0.07 (0.01) \\
credit & 0.30 (0.01) & -0.59 (0.02) & -0.43 (0.01) & 0.39 (0.01) & -0.66 (0.01) & -0.49 (0.01) & 0.05 (0.03) & 0.38 (0.01) & 0.28 (0.03) \\
heart & 0.80 (0.01) & -0.04 (0.02) & 0.01 (0.02) & 0.87 (0.01) & -0.20 (0.02) & -0.17 (0.03) & 0.80 (0.01) & -0.07 (0.01) & -0.05 (0.01) \\
wind & 0.44 (0.01) & 0.67 (0.02) & 0.69 (0.01) & 0.55 (0.02) & 0.74 (0.02) & 0.73 (0.01) & -0.17 (0.01) & 0.26 (0.01) & 0.44 (0.01) \\
cpu & 0.84 (0.01) & 0.55 (0.01) & 0.68 (0.01) & 0.87 (0.01) & 0.59 (0.01) & 0.69 (0.01) & -0.32 (0.01) & -0.53 (0.01) & 0.52 (0.01) \\
2dplanes & 0.23 (0.02) & 0.22 (0.02) & 0.98 (0.01) & 0.41 (0.01) & 0.41 (0.01) & 0.98 (0.01) & -0.05 (0.05) & -0.03 (0.01) & 0.18 (0.01) \\
\belowspace
pol & 0.58 (0.01) & 0.58 (0.01) & 0.79 (0.01) & 0.78 (0.01) & 0.74 (0.01) & 0.81 (0.01) & -0.29 (0.02) & -0.01 (0.02) & 0.13 (0.02) \\
\hline
\end{tabular}
\end{sc}
\end{scriptsize}
\end{center}
\caption{Kendall rank correlations with standard errors in parentheses between different utility function pairs (Accuracy-F1, Accuracy-Negative Log-Loss, and F1-Negative Log-Loss) across multiple datasets and three semivalues: Shapley, $(4,1)$-Beta Shapley, and Banzhaf. Each semivalue is approximated $5$ times using Monte Carlo sampling, and Kendall rank correlations are computed for each run. The reported values correspond to the mean correlation across the $5$ runs, while the standard errors are derived from the standard deviation of these $5$ estimates.}
\label{tab:table-extended-rank-correlation}
\vskip -0.1in
\end{table*}
\subsubsection{Spearman rank correlation results}
\label{sec:spearman-correlation}
We replicate all rank correlation computations using the Spearman rank correlation coefficient for completeness. The results presented in this section confirm that the trends observed with Kendall rank correlation persist under Spearman rank correlation.

Tables \ref{tab:table-rank-correlation-spearman} and \ref{tab:table-extended-rank-correlation-spearman} report the Spearman rank correlation coefficients for the same experimental settings as in Tables \ref{tab:table-rank-correlation} and \ref{tab:table-extended-rank-correlation}, respectively, ensuring consistency in the evaluation across different ranking measures.`

\begin{table*}[h!]
\vskip 0.15in
\begin{center}
\begin{scriptsize} 
\begin{sc}
\begin{tabular}{l|ccc|ccc|ccc}
\hline
\abovespace
\belowspace
dataset & \multicolumn{3}{c|}{shapley} & \multicolumn{3}{c|}{$(4,1)$-beta Shapley} & \multicolumn{3}{c}{banzhaf} \\
\abovespace\belowspace
& \textsc{acc}-\textsc{rec} & \textsc{acc}-\textsc{am} & \textsc{rec}-\textsc{am} & \textsc{acc}-\textsc{rec} & \textsc{acc}-\textsc{am} & \textsc{rec}-\textsc{am} & \textsc{acc}-\textsc{rec} & \textsc{acc}-\textsc{am} & \textsc{rec}-\textsc{am} \\
\hline
\abovespace
breast & 0.99 (0.01) & 0.99 (0.01) & 0.99 (0.01) & 0.99 (0.01) & 0.99 (0.01) & 0.99 (0.01) & 0.90 (0.02) & 0.99 (0.01) & 0.89 (0.03) \\
titanic & 0.56 (0.03) & 0.91 (0.01) & 0.82 (0.01) & 0.62 (0.03) & 0.93 (0.01) & 0.84 (0.01) & -0.37 (0.05) & 0.91 (0.02) & -0.08 (0.08) \\
credit & 0.46 (0.01) & 0.67 (0.01) & 0.94 (0.01) & 0.50 (0.01) & 0.75 (0.01) & 0.92 (0.01) & -0.45 (0.01) & 0.09 (0.02) & 0.79 (0.01) \\
heart & 0.71 (0.02) & 0.99 (0.01) & 0.69 (0.02) & 0.80 (0.02) & 0.99 (0.01) & 0.78 (0.02) & 0.29 (0.02) & 0.99 (0.01) (0.01) & 0.27 (0.02) \\
wind & 0.92 (0.01) & 0.99 (0.01) & 0.91 (0.01) & 0.93 (0.01) & 0.99 (0.01) & 0.92 (0.01) & 0.12 (0.04) & 0.99 (0.01) & 0.11 (0.04) \\
cpu & 0.93 (0.01) & 0.99 (0.01) & 0.97 (0.01) & 0.93 (0.01) & 0.99 (0.01) & 0.97 (0.01) & 0.01 (0.03) & 0.90 (0.01) & 0.27 (0.05) \\
2dplanes & 0.44 (0.03) & 0.99 (0.01) & 0.44 (0.03) & 0.47 (0.03) & 0.99 (0.01) & 0.47 (0.03) & 0.52 (0.01) & 0.99 (0.01) & 0.52 (0.01) \\
\belowspace
pol & 0.75 (0.01) & 0.90 (0.01) & 0.42 (0.01) & 0.74 (0.01) & 0.93 (0.01) & 0.48 (0.02) & 0.85 (0.01) & 0.87 (0.01) & 0.52 (0.01) \\
\hline
\end{tabular}
\end{sc}
\end{scriptsize}
\end{center}
\caption{Spearman rank correlations with standard errors in parentheses between different utility function pairs (Accuracy-Recall, Accuracy-Arithmetic Mean, and Recall-Arithmetic Mean) across multiple datasets and three semivalues: Shapley, $(4,1)$-Beta Shapley, and Banzhaf. Each semivalue is approximated $5$ times using Monte Carlo sampling, and Spearman rank correlations are computed for each run. The reported values correspond to the mean correlation across the $5$ runs, while the standard errors are derived from the standard deviation of these $5$ estimates.}
\label{tab:table-rank-correlation-spearman}
\vskip -0.1in
\end{table*}

\begin{table*}[h!]
\vskip 0.15in
\begin{center}
\begin{scriptsize} 
\begin{sc}
\begin{tabular}{l|ccc|ccc|ccc}
\hline
\abovespace
\belowspace
dataset & \multicolumn{3}{c|}{shapley} & \multicolumn{3}{c|}{$(4,1)$-beta Shapley} & \multicolumn{3}{c}{banzhaf} \\
\abovespace\belowspace
& \textsc{acc}-\textsc{f1} & \textsc{acc}-\textsc{nll} & \textsc{f1}-\textsc{nll} & \textsc{acc}-\textsc{f1} & \textsc{acc}-\textsc{nll} & \textsc{f1}-\textsc{nll} & \textsc{acc}-\textsc{f1} & \textsc{acc}-\textsc{nll} & \textsc{f1}-\textsc{nll} \\
\hline
\abovespace
breast & 0.99 (0.01) & -0.76 (0.02) & -0.78 (0.02) & 0.99 (0.01) & -0.82 (0.01) & -0.83 (0.01) & 0.99 (0.01) & 0.22 (0.01) & 0.23 (0.01) \\
titanic & 0.81 (0.01) & -0.71 (0.01) & 0.74 (0.01) & 0.82 (0.01) & -0.79 (0.01) & -0.80 (0.01) & 0.35 (0.02) & 0.18 (0.02) & -0.20 (0.01) \\
credit & 0.44 (0.02) & -0.76 (0.02) & -0.61 (0.02) & 0.55 (0.02) & -0.83 (0.01) & -0.68 (0.02) & 0.06 (0.05) & 0.53 (0.01) & 0.40 (0.03) \\
heart & 0.94 (0.01) & -0.04 (0.02) & 0.03 (0.03) & 0.97 (0.01) & -0.28 (0.04) & -0.23 (0.04) & 0.95 (0.01) & -0.10 (0.02) & -0.08 (0.02) \\
wind & 0.60 (0.02) & 0.84 (0.01) & 0.86 (0.01) & 0.72 (0.02) & 0.90 (0.01) & 0.89 (0.01) & -0.23 (0.02) & 0.34 (0.01) & 0.62 (0.01) \\
cpu & 0.95 (0.01) & 0.73 (0.01) & 0.85 (0.01) & 0.97 (0.01) & 0.77 (0.01) & 0.86 (0.01) & -0.43 (0.01) & -0.71 (0.01) & 0.70 (0.01) \\
2dplanes & 0.33 (0.02) & 0.33 (0.02) & 0.99 (0.01) & 0.58 (0.01) & 0.58 (0.01) & 0.99 (0.01) & -0.08 (0.07) & -0.04 (0.02) & 0.24 (0.05) \\
\belowspace
pol & 0.77 (0.01) & 0.77 (0.01) & 0.92 (0.01) & 0.93 (0.01) & 0.90 (0.01) & 0.93 (0.01) & -0.41 (0.03) & -0.01 (0.03) & 0.21 (0.02) \\
\hline
\end{tabular}
\end{sc}
\end{scriptsize}
\end{center}
\caption{Spearman rank correlations with standard errors in parentheses between different utility function pairs (Accuracy-F1, Accuracy-Negative Log-Loss, and F1-Negative Log-Loss) across multiple datasets and three semivalues: Shapley, $(4,1)$-Beta Shapley, and Banzhaf. Each semivalue is approximated $5$ times using Monte Carlo sampling, and Spearman rank correlations are computed for each run. The reported values correspond to the mean correlation across the $5$ runs, while the standard errors are derived from the standard deviation of these $5$ estimates.}
\label{tab:table-extended-rank-correlation-spearman}
\vskip -0.1in
\end{table*}

\subsubsection{Intersection analysis}
\label{sec:intersection-analysis}
While rank correlations provide a measure of the consistency in the ordering of data values across different utility functions, they may not fully capture the grouping behavior that is often relevant in data valuation applications. As noted in \ref{subsec:remark-rank-interpretation}, a low-rank correlation between utility functions does not necessarily imply an inconsistent assigned importance of data points.

To better understand how changes in the utility function impact data point valuations, we conduct an \textit{intersection analysis}. We examine the overlap in the bottom-$n$\% of data points ranked by different utility functions to identify whether the least valuable points—according to various utility metrics—are consistently grouped together, even if their exact rankings differ. Figures \ref{fig:intersection-analysis-breast}, \ref{fig:intersection-analysis-titanic}, \ref{fig:intersection-analysis-credit}, \ref{fig:intersection-analysis-heart}, \ref{fig:intersection-analysis-wind}, \ref{fig:intersection-analysis-cpu}, \ref{fig:intersection-analysis-2dplanes}, and \ref{fig:intersection-analysis-pol} illustrate the results.
\begin{figure}[ht]
    \centering
    \begin{minipage}{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/intersection/breast_shapley.png}
        \caption*{(a) \textsc{breast} - Shapley}
    \end{minipage}
    
    \vspace{0.5cm}

    \begin{minipage}{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/intersection/breast_beta.png}
        \caption*{(b) \textsc{breast} - (4,1)-Beta Shapley}
    \end{minipage}
    
    \vspace{0.5cm}

    \begin{minipage}{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/intersection/breast_banzhaf.png}
        \caption*{(c) \textsc{brest} - Banzhaf}
    \end{minipage}

    \caption{Intersection of bottom-$n$\% ranked data points for the \textsc{breast} dataset using different utility functions (accuracy, recall, and arithmetic mean) across semi-value methods. The plot includes a theoretical random baseline, computed using a formula that derives the expected intersection when selecting bottom-$n$\% points from a random permutation. This provides a reference for evaluating how much the observed intersections deviate from purely random rankings.}
    \label{fig:intersection-analysis-breast}
\end{figure}
\begin{figure}[ht]
    \centering
    \begin{minipage}{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/intersection/titanic_shapley.png}
        \caption*{(a) \textsc{titanic}- Shapley}
    \end{minipage}
    
    \vspace{0.5cm}

    \begin{minipage}{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/intersection/titanic_beta.png}
        \caption*{(b) \textsc{titanic}- (4,1)-Beta Shapley}
    \end{minipage}
    
    \vspace{0.5cm}

    \begin{minipage}{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/intersection/titanic_banzhaf.png}
        \caption*{(c) \textsc{titanic} - Banzhaf}
    \end{minipage}

    \caption{Intersection of bottom-$n$\% ranked data points for the \textsc{titanic} dataset using different utility functions (accuracy, recall, and arithmetic mean) across semi-value methods. The plot includes a theoretical random baseline, computed using a formula that derives the expected intersection when selecting bottom-$n$\% points from a random permutation. This provides a reference for evaluating how much the observed intersections deviate from purely random rankings.}
    \label{fig:intersection-analysis-titanic}
\end{figure}
\begin{figure}[ht]
    \centering
    \begin{minipage}{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/intersection/credit_shapley.png}
        \caption*{(a) \textsc{credit} - Shapley}
    \end{minipage}
    
    \vspace{0.5cm}

    \begin{minipage}{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/intersection/credit_beta.png}
        \caption*{(b) \textsc{credit} - (4,1)-Beta Shapley}
    \end{minipage}
    
    \vspace{0.5cm}

    \begin{minipage}{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/intersection/credit_banzhaf.png}
        \caption*{(c) \textsc{credit} - Banzhaf}
    \end{minipage}

    \caption{Intersection of bottom-$n$\% ranked data points for the \textsc{credit} dataset using different utility functions (accuracy, recall, and arithmetic mean) across semi-value methods. The plot includes a theoretical random baseline, computed using a formula that derives the expected intersection when selecting bottom-$n$\% points from a random permutation. This provides a reference for evaluating how much the observed intersections deviate from purely random rankings.}
    \label{fig:intersection-analysis-credit}
\end{figure}
\begin{figure}[ht]
    \centering
    \begin{minipage}{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/intersection/heart_shapley.png}
        \caption*{(a) \textsc{heart} - Shapley}
    \end{minipage}
    
    \vspace{0.5cm}

    \begin{minipage}{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/intersection/heart_beta.png}
        \caption*{(b) \textsc{heart} - (4,1)-Beta Shapley}
    \end{minipage}
    
    \vspace{0.5cm}

    \begin{minipage}{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/intersection/heart_banzahf.png}
        \caption*{(c) \textsc{heart} - Banzhaf}
    \end{minipage}

    \caption{Intersection of bottom-$n$\% ranked data points for the \textsc{heart} dataset using different utility functions (accuracy, recall, and arithmetic mean) across semi-value methods. The plot includes a theoretical random baseline, computed using a formula that derives the expected intersection when selecting bottom-$n$\% points from a random permutation. This provides a reference for evaluating how much the observed intersections deviate from purely random rankings.}
    \label{fig:intersection-analysis-heart}
\end{figure}
\begin{figure}[ht]
    \centering
    \begin{minipage}{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/intersection/wind_shapley.png}
        \caption*{(a) \textsc{wind} - Shapley}
    \end{minipage}
    
    \vspace{0.5cm}

    \begin{minipage}{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/intersection/wind_beta.png}
        \caption*{(b) \textsc{wind} - (4,1)-Beta Shapley}
    \end{minipage}
    
    \vspace{0.5cm}

    \begin{minipage}{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/intersection/wind_banzhaf.png}
        \caption*{(c) \textsc{wind} - Banzhaf}
    \end{minipage}

    \caption{Intersection of bottom-$n$\% ranked data points for the \textsc{wind} dataset using different utility functions (accuracy, recall, and arithmetic mean) across semi-value methods. The plot includes a theoretical random baseline, computed using a formula that derives the expected intersection when selecting bottom-$n$\% points from a random permutation. This provides a reference for evaluating how much the observed intersections deviate from purely random rankings.}
    \label{fig:intersection-analysis-wind}
\end{figure}
\begin{figure}[ht]
    \centering
    \begin{minipage}{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/intersection/cpu_shapley.png}
        \caption*{(a) \textsc{cpu} - Shapley}
    \end{minipage}
    
    \vspace{0.5cm}

    \begin{minipage}{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/intersection/cpu_beta.png}
        \caption*{(b) \textsc{cpu}- (4,1)-Beta Shapley}
    \end{minipage}
    
    \vspace{0.5cm}

    \begin{minipage}{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/intersection/cpu_banzhaf.png}
        \caption*{(c) \textsc{cpu} - Banzhaf}
    \end{minipage}

    \caption{Intersection of bottom-$n$\% ranked data points for the \textsc{cpu} dataset using different utility functions (accuracy, recall, and arithmetic mean) across semi-value methods. The plot includes a theoretical random baseline, computed using a formula that derives the expected intersection when selecting bottom-$n$\% points from a random permutation. This provides a reference for evaluating how much the observed intersections deviate from purely random rankings.}
    \label{fig:intersection-analysis-cpu}
\end{figure}
\begin{figure}[ht]
    \centering
    \begin{minipage}{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/intersection/2dplanes_shapley.png}
        \caption*{(a) \textsc{2dplanes} - Shapley}
    \end{minipage}
    
    \vspace{0.5cm}

    \begin{minipage}{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/intersection/2dplanes_beta.png}
        \caption*{(b) \textsc{2dplanes} - (4,1)-Beta Shapley}
    \end{minipage}
    
    \vspace{0.5cm}

    \begin{minipage}{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/intersection/2dplanes_banzhaf.png}
        \caption*{(c) \textsc{2dplanes} - Banzhaf}
    \end{minipage}

    \caption{Intersection of bottom-$n$\% ranked data points for the \textsc{2dplanes} dataset using different utility functions (accuracy, recall, and arithmetic mean) across semi-value methods. The plot includes a theoretical random baseline, computed using a formula that derives the expected intersection when selecting bottom-$n$\% points from a random permutation. This provides a reference for evaluating how much the observed intersections deviate from purely random rankings.}
    \label{fig:intersection-analysis-2dplanes}
\end{figure}
\begin{figure}[ht]
    \centering
    \begin{minipage}{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/intersection/pol_shapley.png}
        \caption*{(a) \textsc{pol} - Shapley}
    \end{minipage}
    
    \vspace{0.5cm}

    \begin{minipage}{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/intersection/pol_beta.png}
        \caption*{(b) \textsc{pol} - (4,1)-Beta Shapley}
    \end{minipage}
    
    \vspace{0.5cm}

    \begin{minipage}{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/intersection/pol_beta.png}
        \caption*{(c) \textsc{pol} - Banzhaf}
    \end{minipage}

    \caption{Intersection of bottom-$n$\% ranked data points for the \textsc{pol} dataset using different utility functions (accuracy, recall, and arithmetic mean) across semi-value methods. The plot includes a theoretical random baseline, computed using a formula that derives the expected intersection when selecting bottom-$n$\% points from a random permutation. This provides a reference for evaluating how much the observed intersections deviate from purely random rankings.}
    \label{fig:intersection-analysis-pol}
\end{figure}
\clearpage
\subsection{Additional figures for Section \ref{sec:geometric-interpretation}}
\label{sec:more-spatial-signatures}
This section contains all figures illustrating the spatial signatures across different datasets and semivalues.
\begin{figure*}[ht!]
    \centering
    \subfloat[Spatial signature under $\omega_{\text{shap}}$]{%
        \includegraphics[width=0.32\textwidth]{figures/geometry_breast_shapley.png}
    }
    \hfill
    \subfloat[Spatial signature under $\omega_{\text{beta}}$]{%
        \includegraphics[width=0.32\textwidth]{figures/geometry_breast_beta.png}
    }
    \hfill
    \subfloat[Spatial signature under $\omega_{\text{banz}}$]{%
        \includegraphics[width=0.327\textwidth]{figures/geometry_breast_banzhaf.png}
    }
    
    \caption{These three figures illustrate the spatial signatures of the \textsc{breast} dataset under three semivalues: $\omega_{\text{shap}}, \omega_{\text{beta}}, \omega_{\text{banz}}$. The unit circle represents the set of distinct utilities in terms of ranking, with markers indicating the normalized utility vectors $\tilde{\mathbf{u}}_{\text{acc}}, \tilde{\mathbf{u}}_{\text{rec}}$, and $\tilde{\mathbf{u}}_{\text{am}}$ for the three utility functions used in our experiments, namely the accuracy, the recall, and the arithmetic mean that belong to $\mathcal{U}_{\lambda, \gamma}$.}
    \label{fig:geometric-visualization-breast}
    \vskip -0.1in
\end{figure*}
\begin{figure*}[ht!]
    \centering
    \subfloat[Spatial signature under $\omega_{\text{shap}}$]{%
        \includegraphics[width=0.32\textwidth]{figures/geometry_titanic_shapley.png}
    }
    \hfill
    \subfloat[Spatial signature under $\omega_{\text{beta}}$]{%
        \includegraphics[width=0.32\textwidth]{figures/geometry_titanic_beta.png}
    }
    \hfill
    \subfloat[Spatial signature under $\omega_{\text{banz}}$]{%
        \includegraphics[width=0.327\textwidth]{figures/geometry_titanic_banzhaf.png}
    }
    
    \caption{These three figures illustrate the spatial signatures of the \textsc{titanic} dataset under three semivalues: $\omega_{\text{shap}}, \omega_{\text{beta}}, \omega_{\text{banz}}$. The unit circle represents the set of distinct utilities in terms of ranking, with markers indicating the normalized utility vectors $\tilde{\mathbf{u}}_{\text{acc}}, \tilde{\mathbf{u}}_{\text{rec}}$, and $\tilde{\mathbf{u}}_{\text{am}}$ for the three utility functions used in our experiments, namely the accuracy, the recall, and the arithmetic mean that belong to $\mathcal{U}_{\lambda, \gamma}$.}
    \label{fig:geometric-visualization-titanic}
    \vskip -0.1in
\end{figure*}
\begin{figure*}[ht!]
    \centering
    \subfloat[Spatial signature under $\omega_{\text{shap}}$]{%
        \includegraphics[width=0.32\textwidth]{figures/geometry_credit_shapley.png}
    }
    \hfill
    \subfloat[Spatial signature under $\omega_{\text{beta}}$]{%
        \includegraphics[width=0.32\textwidth]{figures/geometry_credit_beta.png}
    }
    \hfill
    \subfloat[Spatial signature under $\omega_{\text{banz}}$]{%
        \includegraphics[width=0.327\textwidth]{figures/geometry_credit_banzhaf.png}
    }
    
    \caption{These three figures illustrate the spatial signatures of the \textsc{credit} dataset under three semivalues: $\omega_{\text{shap}}, \omega_{\text{beta}}, \omega_{\text{banz}}$. The unit circle represents the set of distinct utilities in terms of ranking, with markers indicating the normalized utility vectors $\tilde{\mathbf{u}}_{\text{acc}}, \tilde{\mathbf{u}}_{\text{rec}}$, and $\tilde{\mathbf{u}}_{\text{am}}$ for the three utility functions used in our experiments, namely the accuracy, the recall, and the arithmetic mean that belong to $\mathcal{U}_{\lambda, \gamma}$.}
    \label{fig:geometric-visualization-credit}
    \vskip -0.1in
\end{figure*}
\begin{figure*}[ht!]
    \centering
    \subfloat[Spatial signature under $\omega_{\text{shap}}$]{%
        \includegraphics[width=0.32\textwidth]{figures/geometry_heart_shapley.png}
    }
    \hfill
    \subfloat[Spatial signature under $\omega_{\text{beta}}$]{%
        \includegraphics[width=0.32\textwidth]{figures/geometry_heart_beta.png}
    }
    \hfill
    \subfloat[Spatial signature under $\omega_{\text{banz}}$]{%
        \includegraphics[width=0.327\textwidth]{figures/geometry_heart_banzhaf.png}
    }
    
    \caption{These three figures illustrate the spatial signatures of the \textsc{heart} dataset under three semivalues: $\omega_{\text{shap}}, \omega_{\text{beta}}, \omega_{\text{banz}}$. The unit circle represents the set of distinct utilities in terms of ranking, with markers indicating the normalized utility vectors $\tilde{\mathbf{u}}_{\text{acc}}, \tilde{\mathbf{u}}_{\text{rec}}$, and $\tilde{\mathbf{u}}_{\text{am}}$ for the three utility functions used in our experiments, namely the accuracy, the recall, and the arithmetic mean that belong to $\mathcal{U}_{\lambda, \gamma}$.}
    \label{fig:geometric-visualization-heart}
    \vskip -0.1in
\end{figure*}
\begin{figure*}[ht!]
    \centering
    \subfloat[Spatial signature under $\omega_{\text{shap}}$]{%
        \includegraphics[width=0.32\textwidth]{figures/geometry_cpu_shapley.png}
    }
    \hfill
    \subfloat[Spatial signature under $\omega_{\text{beta}}$]{%
        \includegraphics[width=0.32\textwidth]{figures/geometry_cpu_beta.png}
    }
    \hfill
    \subfloat[Spatial signature under $\omega_{\text{banz}}$]{%
        \includegraphics[width=0.327\textwidth]{figures/geometry_cpu_banzhaf.png}
    }
    
    \caption{These three figures illustrate the spatial signatures of the \textsc{cpu} dataset under three semivalues: $\omega_{\text{shap}}, \omega_{\text{beta}}, \omega_{\text{banz}}$. The unit circle represents the set of distinct utilities in terms of ranking, with markers indicating the normalized utility vectors $\tilde{\mathbf{u}}_{\text{acc}}, \tilde{\mathbf{u}}_{\text{rec}}$, and $\tilde{\mathbf{u}}_{\text{am}}$ for the three utility functions used in our experiments, namely the accuracy, the recall, and the arithmetic mean that belong to $\mathcal{U}_{\lambda, \gamma}$.}
    \label{fig:geometric-visualization-cpu}
    \vskip -0.1in
\end{figure*}
\begin{figure*}[ht!]
    \centering
    \subfloat[Spatial signature under $\omega_{\text{shap}}$]{%
        \includegraphics[width=0.32\textwidth]{figures/geometry_2dplanes_shapley.png}
    }
    \hfill
    \subfloat[Spatial signature under $\omega_{\text{beta}}$]{%
        \includegraphics[width=0.32\textwidth]{figures/geometry_2dplanes_beta.png}
    }
    \hfill
    \subfloat[Spatial signature under $\omega_{\text{banz}}$]{%
        \includegraphics[width=0.327\textwidth]{figures/geometry_2dplanes_banzhaf.png}
    }
    
    \caption{These three figures illustrate the spatial signatures of the \textsc{2dplanes} dataset under three semivalues: $\omega_{\text{shap}}, \omega_{\text{beta}}, \omega_{\text{banz}}$. The unit circle represents the set of distinct utilities in terms of ranking, with markers indicating the normalized utility vectors $\tilde{\mathbf{u}}_{\text{acc}}, \tilde{\mathbf{u}}_{\text{rec}}$, and $\tilde{\mathbf{u}}_{\text{am}}$ for the three utility functions used in our experiments, namely the accuracy, the recall, and the arithmetic mean that belong to $\mathcal{U}_{\lambda, \gamma}$.}
    \label{fig:geometric-visualization-2dplanes}
    \vskip -0.1in
\end{figure*}
\begin{figure*}[ht!]
    \centering
    \subfloat[Spatial signature under $\omega_{\text{shap}}$]{%
        \includegraphics[width=0.32\textwidth]{figures/geometry_pol_shapley.png}
    }
    \hfill
    \subfloat[Spatial signature under $\omega_{\text{beta}}$]{%
        \includegraphics[width=0.32\textwidth]{figures/geometry_pol_beta.png}
    }
    \hfill
    \subfloat[Spatial signature under $\omega_{\text{banz}}$]{%
        \includegraphics[width=0.327\textwidth]{figures/geometry_pol_banzhaf.png}
    }
    
    \caption{These three figures illustrate the spatial signatures of the \textsc{pol} dataset under three semivalues: $\omega_{\text{shap}}, \omega_{\text{beta}}, \omega_{\text{banz}}$. The unit circle represents the set of distinct utilities in terms of ranking, with markers indicating the normalized utility vectors $\tilde{\mathbf{u}}_{\text{acc}}, \tilde{\mathbf{u}}_{\text{rec}}$, and $\tilde{\mathbf{u}}_{\text{am}}$ for the three utility functions used in our experiments, namely the accuracy, the recall, and the arithmetic mean that belong to $\mathcal{U}_{\lambda, \gamma}$.}
    \label{fig:geometric-visualization-pol}
    \vskip -0.1in
\end{figure*}

\end{document}
