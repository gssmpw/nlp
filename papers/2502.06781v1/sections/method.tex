% !TEX root = ../neurips_2024.tex

\section{Methods}
\label{sec: methods}
To obtain a deeper understanding of the challenges when applying reinforcement learning (RL) for solving math word problems, we first analyze the formulation of RL and the intrinsic properties of underlying binary feedback environments (\S\ref{subsec:preliminary}), and establish a theoretical foundation for our optimization framework about how to learn from positive samples (\S\ref{subsec:learn_from_positive}) and failure trials (\S\ref{subsec:learn_from_negative}). To further conquer the learning ambiguity brought by outcome rewards to the partially correct long reasoning chains, we adopt a new strategy to estimate the importance of tokens for learning (\S\ref{subsec:tokenlevel_rm}).

\subsection{Preliminary}\label{subsec:preliminary}
When adopting a large language model (LLM) for mathematic reasoning, the input to the LLM policy is a textual math problem that prompts the LLM to output a multi-step reasoning trajectory consisting of multiple tokens as actions. During RL training, common practices~\cite{shao2024deepseekmath,yuan2023scaling} conduct sampling on the LLM to produce multiple reasoning trajectories, assign binary feedback (0/1 reward) based solely on the correctness of their final answer, and perform corresponding policy optimization using the sampled trajectories with reward.

\textbf{Policy Optimization.} Consider a Markov Decision Process (MDP) defined by the tuple \(( \mathcal{S}, \mathcal{A}, P, r, \gamma )\), where \(\mathcal{S}\) is a finite state space (\eg, contextual steps in mathematical reasoning), \(\mathcal{A}\) is the action space (\ie the token space of LLMs), \(P(s'|s, a)\) specifies the state transition dynamics, \(r: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}\) is the reward function, and \(\gamma \in [0,1)\) denotes the discount factor.  

In this section, we focus on KL-regularized policy optimization, which maximizes the expected cumulative returns while regularizing the policy \(\pi_\theta(\cdot|s)\) toward a reference policy \(\pi_0(\cdot|s)\). The objective function is formulated as:  

\begin{align}\label{eq: klpg}
J(\theta) \triangleq \mathbb{E}_{s \sim \rho_0, a \sim \pi_\theta(\cdot|s)}\left[ Q^{\pi_\theta}(s, a) \right] - \alpha \cdot \mathbb{E}_{s \sim \rho_0}\left[ D_{\text{KL}}\left(\pi_\theta(\cdot|s) \| \pi_0(\cdot|s)\right)\right]
\end{align}

with the state-action value function \(Q^{\pi}(s,a) = \mathbb{E}_{\pi}\left[\sum_{k=0}^\infty \gamma^k r(s_{t+k}, a_{t+k}) \mid s_t = s, a_t = a\right]\) under vanilla policy \(\pi\). This objective admits a closed-form solution for optimal policy \(\pi^*\):  
\begin{align}\label{eq: solution}
    \pi^*(a|s) = \frac{\pi_0(a|s) \exp\left( Q^{\pi}(s,a)/\alpha \right)}{Z(s)},
\end{align}

where \(Z(s) = \mathbb{E}_{a \sim \pi_0(\cdot|s)}\left[\exp\left( Q^{\pi}(s,a)/\alpha \right)\right]\) is the partition function that ensures normalization.  

\textbf{Best-of-N (BoN) Sampling.} As a common and efficient strategy to sample multiple reasoning trajectories from LLMs, Best-of-\(N\) sampling selects the trajectory with maximal reward among \(n\) independent rollouts from \(\pi_0\) to enhance policy performance. Formally, given candidate actions \(\{a^{(i)}\}_{i=1}^n \sim \pi_0(\cdot|s)\), the chosen action is \(a^* = \arg\max_{a^{(i)}} Q(s,a^{(i)})\). This strategy effectively leverages the exploration-exploitation trade-off through parallel sampling~\cite{gao2023scaling, go2023compositional}.  

\textbf{Binary Feedback under Outcome Supervision.}
Though a reasoning trajectory usually contains multiple reasoning steps with thousands of tokens, there lacks an efficient approach to automatically label the correctness of each token or reasoning step in math reasoning tasks.
Thus, a practical way is to parse the final answer from the reasoning trajectory~\cite{deepseekr1, lambert2024t}, evaluate its correctness based on rules or models, and then provide an outcome reward at the end of the trajectory as below:
\begin{align}\label{eq:orm}
    R(s_t) = \begin{cases} 
1 & \text{if \(t\) is the end step and the answer is correct} \\
0 & \text{otherwise},
\end{cases} 
\end{align}
which intrinsically treats the correct trajectory equally for learning. Moreover, the reward signal is severely sparse when compared to the thousands of tokens and does not provide any signal of progress or correctness for intermediate steps.
The resulting reward distribution of trajectories is also different from that of the dense reward function constructed through preference pairs in traditional RL for large language models~\cite{ouyang2022training}, which induces a more appropriate optimization framework for mathematical reasoning tasks, discussed in the next section.


\subsection{Learning from Positive Samples}\label{subsec:learn_from_positive}
Building upon the reward equivalence principle stated in Eq. \ref{eq:orm}, we first formalize a key probabilistic characteristic of BoN sampling:

\begin{lemma}\label{lemma:3.1}
Let $\pi(\theta, s)$ be a distribution over parameters $\theta$ and trajectory $s$, where each $s$ is associated with a binary reward $R(s) \in \{0, 1\}$. Define $p \triangleq \mathbb{E}_{s \sim \pi(\theta,\cdot)}[R(s) = 1] > 0$. Consider the BoN sampling:
$n = n_0 \to \infty$ and sample $\{s_1, s_2, \dots, s_n\}$ i.i.d. from $\pi_\theta$.
BoN selects $s^*$ uniformly from the subset with $R(s_i) = 1$. We have that,
The probability of selecting $s^*$ is converge to $\frac{\pi(\theta, s)}{p}$, which is independent of $n$.
\end{lemma}

The proof follows directly from the union law of BoN sampling ($\text{BoN}_{n+m} = \text{BoN}_2(\text{BoN}_m, \text{BoN}_n)$) and the trivial distinguishability of $0-1$ rewards. This result reveals that for problems with attainable positive responses, we are using a BoN generator with an arbitrary sampling budget to construct the positive training samples. 

To quantify the distributional divergence induced by BoN sampling, prior work~\cite{hilton2022measuring, scheurer2023training, coste2023reward} has analyzed the KL divergence between the BoN distribution $\pi_{\text{BoN}}$ and the original policy $\pi$. For continuous trajectory spaces $\mathcal{S}$, the BoN distribution admits the explicit form:
\begin{align}
\label{eq: pibon}
\pi_{\text{BoN}}(s) = n \cdot \left[P(s)\right]^{n-1} \cdot \pi(s),
\end{align}
where $P(s)$ denotes the cumulative distribution function (CDF) associated with $\pi(s)$. The corresponding KL divergence is given by
\[
\text{KL}(\pi_{\text{BoN}} \parallel \pi) = \log n - \frac{n-1}{n}.
\]

The KL divergence expression possesses two crucial properties, as $n \to \infty$, the $\text{KL}(\pi_{\text{BoN}}\parallel \pi)$ is strictly increasing in $n$ for $n \geq 1$, and converge to $\log n \to \infty$, covering the entire positive real axis.
This implies that for any prescribed KL divergence constraint $\epsilon > 0$, there exists a BoN parameterization for approximating the optimal policy, and can be simply sampled with the minimal $n(\epsilon)$ satisfying that
\[
n(\epsilon) = \arg\min_{n} \mathbb{E}_{s \sim \pi_{\text{BoN}}}[-R(s)].
\]

BoNBoN~\cite{gui2024bonbon} empirically shows that BoN sampling achieves the optimal win rate under fixed KL constraint by exhaustive search over the positive support. Therefore, the behavior cloning on BoN-selected positive samples directly learns the analytic solution to Eq. \ref{eq: klpg}. Intuitively, since every correct answer is preferred identically in the outcome-supervised sense, we only need to sample until we get a positive example, whose generating probability distribution will be the same as randomly picking from arbitrarily large numbers of samples. 


Based on the theoretical understanding established, we formulate the first component of the learning objective in \methodname{} by incorporating KL-constrained max-likelihood-objective over positive examples obtained through sampling:

\[
\mathcal{L}_1(\theta) = \underbrace{\mathbb{E}_{s \sim \mathcal{D}^+} \left[ -\log \pi_\theta(s) \right]}_{\text{Positive example alignment}} + \beta \underbrace{\text{KL}(\pi_\theta \parallel \pi_{\text{old}})}_{\text{Policy constraint}},
\]
where $\mathcal{D}^+$ denotes the set of positive trajectories selected via BoN sampling from RL rollouts. 


\subsection{Learning from Negative Samples}\label{subsec:learn_from_negative}
As established in Section \ref{subsec:learn_from_positive}, direct behavioral cloning on positive responses can effectively recover the policy distribution. BOND \cite{sessa2024bond} proposes estimating Jeffreys divergence \cite{jeffreys1946invariant} for the BoN strategy to train with both positive and negative samples, and demonstrates that signals from unsuccessful trajectories provide critical information about decision boundaries and failure modes.

In this section, we will discuss the relationship between the BoN (Best-of-N) distribution and the optimization objective defined in Eq. \ref{eq: klpg}, then elucidate the necessity of reward reshaping when training with negative samples.
Notably, while Eq. \ref{eq: pibon} shares structural similarities with Eq. \ref{eq: solution}, its application to mathematical reasoning tasks with binary feedback requires reformulation. Specifically, the transformed BoN distribution can be expressed as

\begin{align}\label{eq. pbon}
\pi_{\text{bon}}(s) = \pi(s) \left[ R(s) \cdot \frac{1 - \left(1 - p\right)^n}{p} + \left(1 - R(s)\right) \cdot \left(1 - p\right)^{n-1} \right],
\end{align}

which reveals fundamental differences between the BoN distribution and the original sampling distribution. Consider a scenario where two correct and two incorrect solutions are sampled, yielding an empirical accuracy of 50\%. However, the probability of selecting negative samples under Best-of-4 becomes $(0.5)^4 = 6.25\%$, significantly lower than the original distribution. This discrepancy necessitates reward shaping to maintain consistency between our optimization target and the expected return under the BoN distribution.

Building on BoN-RLB's \cite{chow2024inference} application of the log-likelihood trick for BoN-aware policy gradients, we analyze the reward shaping technic for negative samples to maintain gradient consistency with Section \ref{subsec:learn_from_positive}. With expectation return $p$ follow the definition in Lemma \ref{lemma:3.1}. The policy gradient under the BoN distribution can be derived as

\begin{equation}
\begin{aligned}
\nabla_\theta J_{\text{bon}} &= \mathbb{E}_{s \sim \pi_{\text{bon}}(\cdot)}\left[R(s) \nabla_\theta \log \pi_{\text{bon}}(s)\right] \\
&= \mathbb{E}_{s \sim \pi_{\text{bon}}(\cdot)}\left[R(s) \nabla_\theta\left( I_{D_+}(s)\log \pi(s)\frac{1 - (1-p)^n}{p} + I_{D_-}(s)\log \pi(s)(1-p)^{n-1} \right)\right],
\end{aligned}
\end{equation}

where $I_{D_+}(s)$ and $I_{D_-}(s)$ denote indicator functions for positive and negative sample sets respectively. Notably, these indicators are independent of policy parameters $\theta$. Given $\mathbb{E}_{s \sim \pi_{\text{bon}}}[I_{D_+}(s)] = 1-(1-p)^n$, we derive the gradient components as

\begin{align*}
    &\mathbb{E}_{s \sim \pi_{\text{bon}}}\left[\nabla_\theta \left( I_{D_+}(s) \log \pi(s)\frac{1 - (1-p)^n}{p}\right)\right] = n(1-p)^{n-1} \mathbb{E}_{s \sim \pi, s\in D_+}\left[\nabla_\theta \log \pi(s)\right].
\end{align*}
Similarly, we also have
\begin{align*}
\mathbb{E}_{s \sim \pi_{\text{bon}}}\left[\nabla_\theta \left( I_{D_-}(s) \log \pi(s)(1-p)^{n-1}\right)\right] = n(1-p)^n \mathbb{E}_{s \sim \pi, s\in D_-}\left[\nabla_\theta \log \pi(s)\right].
\end{align*}

This derivation reveals that when assigning unit reward ($R(s)=1$) to positive samples, gradient consistency requires reshaping negative sample rewards to $R^\star(s) \triangleq (1-p)R(s)$. Based on this reward shaping, we can construct policy optimization both on positive and negative samples for optimal policy. 
 
 
To obtain the parameter $1-p$ which can be linked to the Monte-Carlo (MC) advantage estimation, we can simply estimate that probability by calculating the expected accuracy on the sample space by counting a small number of responses.
In this paper we apply a similar setting to RLOO \cite{fukunaga1989leave}, namely $R_{RLOO}(s) = R(s) - \frac{1}{N-1} \sum_{s^\star \neq s} R(s^\star)$ for unbiased mean reward
and train with policy gradient. The second part of our OREAL objective is then formulated as below:
\[
\mathcal{L}_{2}(\theta) = \mathbb{E}_{s \sim S_-} \left[  F(1-p) \cdot\log \frac{\pi_\theta(s)}{\pi_{old}(s)} \right]  + \beta \text{KL}(\pi_\theta \parallel \pi_{\text{old}}),
\]
where $p = \mathbb{P}_{\theta \sim \pi}[R(\theta) = 1]$, $S_-$ is the failed subset generated by policy model, and $F$ represents the preprocessing for advantage scores to serve as a generalized form, for example, $F(1-p) \triangleq \frac{r_i-mean(\{r_i...r_n\})}{std(\{r_i...r_n\})}$ in the recent GRPO \cite{shao2024deepseekmath} algorithm, where $mean(\{r_i...r_n\}) \to p$ when $n \to \infty$. 


\subsection{Dealing with Long Reasoning Chains}\label{subsec:tokenlevel_rm}
In the previous discussion, we introduced the adaptation of binary reward training in response space. However, since the outcome supervision only provides feedback at the sequence level, this modeling essentially reduces to a contextual bandit without internal reward modeling within the MDP. A common counterexample is PPO, which utilizes a separate critic model to estimate the value function. However, such a solution appears to be expensive and complex, which has induced numerous explorations on how to stabilize the PPO training. 

Things become slightly different in mathematical reasoning, where the model can spontaneously revise omissions in intermediate steps to obtain the correct final answer. Therefore, outcome supervision is preferred, and the value function is more meant to be a simple credit assignment to determine how much the process step contributes to the outcome reward. With efficiency and performance trade-off considerations, we choose to use some low-cost alternatives for sequence-level reweighting.


Taking into account the deterministic dynamics in mathematical reasoning (\(s_{t+1} = f(s_t, a_t)\)), the state-action function $Q^\pi(s_{<t}, \pi(s_t))$ simplifies to the cumulative discounted reward of policy \(\pi\):  
\begin{align}\label{eq: vandq}
    Q^\pi(s_{<t}, \pi(s_t)) = V^\pi(s_{\leq t}) =  \sum_{k=0}^{T-t} \gamma^k r(s_{t+k} | s_{<t}).
\end{align}

Since intermediate rewards are not provided in mathematical reasoning tasks, we define an advantage function based solely on outcome feedback:
\begin{equation}
A(s_{\leq t}) = V^\pi(s_{\leq t+1}) - V^\pi(s_{\leq t}).
\end{equation}
This formulation treats \(A(s_{\leq t})\) as a token-wise credit assignment mechanism, estimating each token's contribution toward the final outcome.

For a pair of responses $y_1$ and $y_2$ to the same query, their initial values coincide $V_0^1 = V_0^2$. The win rate between them then satisfies:
\begin{equation}
\begin{aligned}\label{eq: winrate}
p(y_1>y_2)&=
\sigma(r(y_1) - r(y_2)) \\&= \sigma\left( \left(V_0^1 + \sum_{t=0}^T \gamma^t A_{y_1}^t\right) - \left(V_0^2 + \sum_{t=0}^T \gamma^t A_{y_2}^t\right) \right) \\
&= \sigma\left( \sum_{t=0}^T \gamma^t \left(A_{y_1}^t - A_{y_2}^t\right) \right).
\end{aligned}
\end{equation}

Equation \ref{eq: winrate} indicates that for any function family $\mathcal{A} = \{A(s_{\leq t})\}$ , a cumulative reward function through sequence aggregation can be constructed to model rewards:

\[
r^*(s) \triangleq \sum_{t=0}^{T} \gamma^t A(s_{\leq t}),
\]

which is trainable via preference pairs $\{(y_w, y_l)\}$ by fitting the outcome feedback. The learned $A(s_{\leq t})$ serves as a weighting function for credit assignment, which is used to reweight the original training loss, emphasizing critical reasoning steps or errors. An analogous implementations is $ r 2 Q^* $ \cite{rafailov2024r, xia2024inverse} by defining $A = \log \frac{\pi(y_i)}{\pi_{\text{ref}}(y_i)}$, PRIME \cite{cui2025process} then apply this formulation to improve performance of RLOO. In our work, following the practice from \cite{cobbe2021training}, we directly train a token-level reward function $w(s_{\leq t})$ satisfying
\[
\frac{1}{T}\sum_{t=0}^{T} w(s_{\leq t}) = r(s),
\]
without constraining KL-divergence to reference model in reward model training. These sequential rewards can serve as a proxy for the contribution of thinking steps to the result accuracy. Assuming a pair of prefix-consistent correct and incorrect samples, due to the causal inference nature of the token-level reward model, the preference optimization for these samples will only function on the steps that have different contents, which induces higher credits on the core reasoning step that affects the final result. We further discuss the training details of this model and analyze the visualization of its token-wise scoring effects later in Section \ref{subsec: rl} and Appendix \ref{sec: vis_token}.

In practice, we decompose the output weight $w(s)$ for positive and negative samples and clip on the positive axis to prevent reversing the direction of the optimized gradient, denoted as $\omega^+$ and $\omega^-$:

\begin{align}
\omega^+ = \max(2 \sigma(w) - 1, 0), \omega^- = \max(1 - 2\sigma(w), 0).
\end{align}

Giving input query $d$, the overall loss is as follows:


\begin{equation}
\begin{aligned}\label{eq: overall loss}
\mathcal{L}_{\text{total}}(d) \triangleq &\mathbb{E}_{s \sim S} \left[ \sum_{t=0}^T \left( -\omega_{s\leq t}^+\log \pi_{\theta}(s_{\leq t}|d)I_{D_+}(s) +  \eta 
 \ \omega_{s\leq t}^-\log \frac{\pi_\theta(s_{\leq t}|d)}{\pi_{old}(s_{\leq t}|d)}I_{D_-}(s) \right) \right] \\ &+ \beta \text{KL}(\pi_\theta(\cdot|d) \parallel \pi_{\text{old}}(\cdot|d)),
\end{aligned}
\end{equation}

where $\eta$ represents the balancing weights for positive and negative losses.