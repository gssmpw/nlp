% !TEX root = ../neurips_2024.tex

\section{Implementation}

\subsection{Policy Initialization}

We utilize Qwen2.5-7B and Qwen2.5-32B~\cite{yang2024qwen2} as the base model. 
Initially, we fine-tune the base models using long chain-of-thought data obtained through rejection sampling~\cite{yuan2023scaling}. 
This rejection sampling fine-tuned (RFT)~\cite{yuan2023scaling} models then serve as the initialization for the policy model in our RL framework.
We also explore to use of DeepSeek-R1-Distill-Qwen-7B~\cite{deepseekr1} as the initial policy model and perform \methodname{} on it and discuss the influence of different initial policy models in Section~\ref{subsec: policy model analysis}.
The training data for the RFT models consists of in-house datasets supported by OpenDataLab~\cite{opendatalab} and open-source datasets including Numina~\cite{li2024numinamath} and the training set of MATH~\cite{hendrycks2021measuring}.

\subsection{Reinforcement Learning}
\label{subsec: rl}

\noindent\textbf{Data Preparation.} During the on-policy RL process, we utilize questions from Numina, MATH training sets, and historical AMC/AIME (without AIME2024) competitions. For each question, we independently sample 16 trajectories from the RFT models. The correctness of each trajectory is then averaged to estimate the correctness rate of each query. To increase the difficulty of training queries, only questions with correctness rates between 0 and 0.8 are retained for further training.

\noindent\textbf{Outcome Reward Signal.} We employ the Qwen2.5-72B-Instruct~\cite{yang2024qwen2} as a generative verifier, in conjunction with a rule-based verifier, to evaluate the correctness of the model's outputs and provide binary rewards. This combination enhances the robustness of correctness assessment, mitigating issues related to the false negative of the rule-based verifier.

\noindent\textbf{Training Token-level Reward Model.} For the token-level reward model, we directly use the binary outcome rewards provided by the verifier and optimize using the cross-entropy loss:  

\begin{equation}
\mathcal{L}_{\text{CE}} = - \mathbb{E}_{(s, r) \sim \mathcal{D}} \left[ r \log p(s) + (1 - r) \log (1 - p(s)) \right],
\label{eq: ce}
\end{equation}

where $s$ represents the sampled trajectory, $ r \in \{0,1\}$  is the binary outcome reward from the verifier, and $p(s) = \sigma(\frac{1}{T}\sum_{t}^{T} w(s_{t}))$ denotes the predicted probability of correctness by the token-level reward model $w$.

To further analyze the behavior of the token-level reward model, we visualize its output distribution $w(s_{t})$ during the on-policy RL training process (see Appendix \ref{sec: vis_token}). In this training paradigm, $w(s_{t})$ assigns token-wise importance scores across the chain-of-thought reasoning process, capturing each token's contribution to the final correctness of the generated response. Consequently, this allows us to leverage $w(s_{t})$ for importance sampling during the optimization process, enabling a more principled selection of informative tokens. 

\noindent\textbf{Training Algorithm.} 
The loss function for the policy model follows the formulation described in Section \ref{sec: methods}. 
The complete RL training procedure is described in Algorithm \ref{alg:oreal}.

\input{algo/code}

\noindent\textbf{Hyperparameters.} 
The policy model is initialized from the RFT model. Similarly, the token-level reward model is also initialized with the same weights, but its output layer is replaced with a linear layer that produces a one-dimensional scalar. The weights of this layer are initialized to zero to ensure unbiased importance sampling weight at the start of training.  

During training iterations, each batch consists of 64 questions, with 16 rollouts per question. The max length of each rollout trajectory is set to 16384 tokens. Then the correctness of each response is averaged to calculate the pass rate, and questions with an overall pass rate of 0 or 1 are discarded. For the remaining trajectories, we retain only one correct response and one incorrect response per question, ensuring a balanced distribution of positive and negative samples for token-level reward model training. 

For optimization, the policy model is trained with a learning rate of $5e{-7}$, while the token-level reward model is trained with a learning rate of $2e{-6}$. The latter undergoes a 10-step warm-up phase before training begins. Both models employ a cosine annealing learning rate schedule, decaying to $1/5$ of the initial learning rate over time. We optimize both models using the AdamW optimizer. The total number of training steps is 80, with evaluation conducted every 10 steps. The KL coefficient $\beta$ is set to 0.01. We select the best-performing model determined by evaluation metrics.


\subsection{Skill-based Enhancement}
\label{subsec:skill-enhance}

During the RL training procedure, we observe that the model consistently struggles with certain types of questions, particularly those involving specific knowledge and skill areas, such as trigonometric constant transformations, probability statistics, series transformations, \etc. We believe this is caused by the insufficient learning of the base model on these concepts in the Pre-training or RFT stages.

To address this problem, we implement a skill-based enhancement approach, using the MATH dataset to reduce the high cost of skill annotation. Specifically, we annotate each question in the training set with its corresponding core skill. For questions that the model repeatedly fails to answer correctly during the RL phase, we perform data augmentation by including similar questions from the training set that share the same skill. These augmented questions are then added to the training data during the RFT stage to help the model better internalize these skills.

\section{Experiment}
\label{sec:experiment}



\subsection{Evaluation Setup}

\noindent\textbf{Baseline.}
We conduct evaluations against several baselines, including GPT-4o-0513~\cite{gpt4o}, Claude-Sonnet-3.5-1022~\cite{claude35sonnet}, OpenAI-o1-mini, OpenAI-o1-preview~\cite{openai2024learning}, Qwen2.5-Instrust-7B, Qwen2.5-Math-Instrust-7B, Qwen2.5-Instrust-32B~\cite{yang2024qwen2}, QwQ-32B-Preview~\cite{qwq-32b-preview}, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-32B~\cite{deepseekr1}, SimpleRL~\cite{zeng2025simplerl}, PRIME~\cite{cui2025process}, rStarMath~\cite{guan2025rstar}.
For part of the baseline, we directly use the results from their report, which we mark with *.

\noindent\textbf{Benchmark.}
We use some well-established mathematical datasets for evaluation, including MATH-500~\cite{hendrycks2021measuring}, AIME2024~\cite{AIME2024}, AIME2025 (Part1)~\cite{AIME2024}, LiveMathBench~\cite{liu2024your}, and OlympiadBench~\cite{he2024olympiadbench}.

\noindent\textbf{Metrics.}
We use pass@1 as the metric for evaluation under the zero-shot chain-of-thought setting and use greedy decoding for each sample to assess correctness using OpenCompass~\cite{2023opencompass}.

\input{./tables/main_result}

\subsection{Overall Results}

Tabel~\ref{tab: main_res} shows the results of the comprehensive evaluation, highlighting the performance of our proposed models across different parameter scales. 
Notably, at the 7B scale, \methodname{}-7B achieves a remarkable pass@1 accuracy of 91.0 on the MATH-500 and 59.9 on OlympiadBench. 
To the best of our knowledge, this is the first time a model of this size has reached such a high level of accuracy using RL instead of distillation. 
This performance not only establishes a new milestone for RL-based methods but also surpasses significantly larger models, including QwQ-32B-Preview and OpenAI-o1-mini, demonstrating the effectiveness of our approach.
Furthermore, after applying \methodname{} on the previous best 7B model, DeepSeek-R1-Distill-Qwen-7B, the resulting model, OREAL-DSR1-Distill-Qwen-7B, obtains 94.0 and 66.1 pass@1 accuracy on MATH-500 and OlympiadBench, respectively, setting new records among all 7B models. This result verifies the effectiveness of \methodname{} even when faced with strong initial policies.

For 32B models, \methodname{}-32B achieves a groundbreaking pass@1 accuracy of 95.0 on MATH-500, 46.7 on AIME2025-I, 74.8 on LiveMathBench, and 72.4 on OlympiadBench, setting a new state-of-the-art among all previously reported models.
These results underscore the advantages of our methodology, including its scalability for training superior mathematical reasoning models across different model sizes.

Compared to the most competitive baseline, DeepSeek-R1-Distill-Qwen series, \methodname{}-32B demonstrates a clear advantage, whereas \methodname{}-7B lags slightly behind than the distilled 7B model, despite being trained on the same dataset as \methodname{}-32B. We attribute this discrepancy to the different affinities of the base models for the post-training data. Qwen-7B and Qwen-32B may exhibit varying degrees of knowledge gaps due to model sizes and pre-training settings. Our training data appears to better complement the existing knowledge of Qwen-32B, while it may be less effective in bridging gaps for Qwen-7B.

In addition, \methodname{}-DSR1-Distill-Qwen-7B improves the MATH-500 score from 92.8 to 94.0 and also achieves gains on LiveMathBench and OlympiadBench.
However, its performance on the AIME benchmark series is comparatively weaker. 
We observe the same disadvantages of \methodname{}-32B and \methodname{}-7B, whose AIME2024 scores are relatively lower than the best scores.
Since the overall performance verifies the effectiveness of the \methodname{} algorithm, we attribute the reason to the deficiency (\eg, in response quality, query difficulty, and quantity) of RFT data and RL training queries for obtaining high performance in the domain of AIME and leave it for the future work.


\subsection{Ablation Study}

\input{./tables/ablation}

To verify the effectiveness of each component described in Section~\ref{sec: methods}, we progressively add the proposed component based on the 7B model and compare the evaluation results on MATH-500, starting from REINFORCE~\cite{sutton1999policy} as baseline.

As shown in Tabel~\ref{tab: ablation}, we add each component step by step, where ``Reward Shaping'' represents $L_2$ introduced in Section~\ref{subsec:learn_from_negative}, ``Behavior Cloning'' represents $L_1$ introduced in Section~\ref{subsec:learn_from_positive}, ``Importance Shaping'' represents $L_{total}$ introduced in Section~\ref{subsec:tokenlevel_rm}.
The gradual addition of the modules steadily increases the Pass@1 scores of the 7B model on MATH-500, proving the effectiveness of our method.
Ultimately, the policy model is raised from an initial score of 84.8 to 91.0.

We also report average pass@1 accuracy across all benchmarks during the training process with different RL settings. 
As shown in Figure~\ref{fig: test acc}, the REINFORCE training process is unstable, which can be mitigated by ``Reward Shaping''.
``Behavioral Cloning'' for positive samples can speed up convergence and show better performance early in training.
Although the performance growth of ``Importance Sampling'' is relatively slow in the early stage of training, it ultimately obtains the best results.


\subsection{Analysis of Initial Policy Models}
\label{subsec: policy model analysis}

\input{./tables/analysis_policy}

We further analyze \methodname{} by adopting it to several different initial policy models, as shown in Table~\ref{tab: base_model}. \methodname{} consistently improves the performance of each initial policy model, including our own trained model and the strong distilled model~\cite{deepseekr1}, on MATH-500, LiveMathBench, and OlympiadBench, except slight fluctuations on AIME2024 and AIME2025 part1 when the performance of initial policy models are already high (\eg, DeepSeek-R1-Distill-Qwen-7B), which demonstrates the generality of \methodname{}.

After adding skill-based enhancement data (introduced in Section~\ref{subsec:skill-enhance}), there is a significant rise in MATH-500 scores for the initial policy model (row 1 and row 3) and the corresponding RL-trained model (row 2 and row 4). 
Since our enhancement is performed primarily for the MATH-500, this verifies the effectiveness of the skill-based enhancement approach. In addition, the performance of the model after RL is strongly correlated with the capabilities of the initial policy model itself. 
The stronger the initial policy model, the higher the performance that RL can deliver, indicating the importance of policy initialization.
