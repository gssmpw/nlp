% !TEX root = ../neurips_2024.tex

\section{Related Work}

\noindent\textbf{Stimulate Reasoning using Chain of Thought.}
In mathematical reasoning tasks, Chain of Thought (CoT)~\cite{wei2022chain} is recognized as a crucial technique to enhance the reasoning ability of large language models (LLMs), which can be implemented through few-shot examples~\cite{wei2022chain} or zero-shot prompt engineering~\cite{kojima2022large}. 
Self-consistency (SC)~\cite{wang2022self} is further proposed to generate and voting through multiple CoTs. 
In addition to simple CoTs, various search methods have been explored that simultaneously consider multiple potential CoTs, such as Tree-of-Thought (ToT)~\cite{yao2024tree} and Graph-of-Thought (GoT)~\cite{besta2024graph}, which extend the idea to tree or graph structure, offering more flexibility in developing CoTs and backtracking.
However, these methods mainly stimulate the reasoning capability of LLMs by prompts without parameter updates, these inference-time techniques do not fundamentally improve the underlying ability of LLMs.

\noindent\textbf{Reasoning Enhancement by Supervised Fine-tuning.}
To let the LLMs essentially acquire the reasoning abilities, many studies~\cite {ying2024internlm, yu2023metamath, liu2024augmenting, li2023query, liu2023goat, yue2023mammoth} have explored synthesizing high-quality data to conduct supervised fine-tuning (SFT) on LLMs. 
But this method heavily relies on high-quality training data and a existing high-performing model~\cite{lightman2023let}. 
As a result, many existing works~\cite{huang2024o1, Slow_Thinking_with_LLMs_2} have turned to distilling knowledge from powerful, large-scale models to synthesize data, yielding good results. 
However, distilled-based methods receive the limitations of the teacher model.
One criticism of SFT is its limited generalization capability~\cite{chu2025sft}. 
Some studies argue that SFT merely transforms the model into a knowledge retriever, rather than an actual reasoner~\cite{kambhampati2024can}.


\noindent\textbf{Reinforcement Learning for LLM.}
Compared to SFT, reinforcement learning (RL) offers better generalization and is therefore considered a more fundamental training aproach~\cite{chu2025sft}. 
Previous attempts applying RL for LLMs mainly target aligning the LLM to human preferences~\cite{ouyang2022training}.
Later, some works~\cite{ying2024internlm, shao2024deepseekmath, luo2023wizardmath, lightman2023let} has attempted to use it to enhance the model's reasoning and obtained promising results.
Recently, the advent of the o1 family of models~\cite{openai2024learning} and a series of o1-like works~\cite{deepseekr1, zeng2025simplerl, cui2025process, guan2025rstar} make the importance of large-scale RL for inference became more apparent. 
Currently, the mainstream approach to RL involves using outcome reward signals~\cite{deepseekr1, zeng2025simplerl, kazemnejad2024vineppo} and there are different views in the community on how to use that reward signal.
ReST$^{EM}$~\cite{singh2023beyond} and RFT~\cite{yuan2023scaling} simply select the positive samples based on the binary signal and only use them for behavior cloning.
GRPO~\cite{shao2024deepseekmath}, RLOO~\cite{fukunaga1989leave, ahmadian2024back}, REINFORCE~\cite{sutton1999policy}, use both positive and negative samples for policy updating, but facing the challenges of sparse reward in long sequence.
PPO~\cite{schulman2017proximal} makes the preference modeling on sequence-level.
Different from them, to explore the limit of outcome reward, \methodname{} presents a unified framework, grounded in the unique characteristics of mathematical reasoning tasks.