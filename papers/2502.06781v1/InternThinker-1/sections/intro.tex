% !TEX root = ../neurips_2024.tex

\section{Introduction}
\label{sec: intro}

Solving complex problems with reasoning capability forms one of the cornerstones of human cognition - a cognitive ability that artificial general intelligence must ultimately master~\cite{xu2025towards, zhong2024evaluation}. Among various problem domains, the mathematical problem emerges as a particularly compelling experimental paradigm for AI research~\cite{liu2023mathematical, matzakos2023learning, ying2024internlm, shao2024deepseekmath}, owing to its relatively well-defined structure and availability of precise binary correctness feedback based on the verifiable final answers.

Recent advances in large language models (LLMs) have achieved remarkable progress in mathematical reasoning by the chain-of-thought technics~\cite{wei2022chain, wang2022self, kojima2022large}, 
in which the LLMs are elicited to produce a series of intermediate reasoning steps before providing the final answers to the problem.
However, as most of the capable models (\eg, the o-series models by OpenAI~\cite{openai2024learning}) are developed by proprietary companies, there is no clear pathway to develop state-of-the-art reasoning models. Some recent work shows that distillation~\cite{huang2024o1, Slow_Thinking_with_LLMs_2} is sufficient to obtain high performance given the accessibility to existing best or near best AI models, reinforcement learning (RL) is believed to be a more fundamental approach and has exhibited potential~\cite{deepseekr1} to advance beyond the intelligence boundary of current AI models, using the most capable open-source foundation models (DeepSeek-V3-base~\cite{liu2024deepseek}, \textit{inter alia}).

However, fundamental challenges of sparse reward in RL persist and are even exacerbated in mathematical reasoning tasks that mainly rely on the chain of thought technics with LLMs~\cite{wei2022chain}: the evaluation of intermediate reasoning steps is labor intensive~\cite{lightman2023let} and its accurate automation approach is still under-explored, thus, the only reliable reward is based on the outcome (correctness of final answer), which is inherently binary and sparse when faced with more than 2000 tokens in the long reasoning trajectories~\cite{deepseekr1, team2025kimi}. Existing approaches have attempted to estimate the advantages or values of reasoning steps by search~\cite{wang2024math, kazemnejad2024vineppo} or value function-based credit assignment~\cite{schulman2017proximal, cui2025process}, yet, their performance remains unsatisfactory in comparison with the distilled models~\cite{deepseekr1}.

This paper aims to conquer the above challenges and proposes a simple framework, termed \methodname{}, to push the limit of \textbf{O}utcome \textbf{RE}w\textbf{A}rd-based reinforcement \textbf{L}earning for mathematical reasoning tasks. \methodname{} is grounded in the unique characteristics of mathematical reasoning tasks that binary outcome feedback creates an environment where all positive trajectories are equally valid. We first establish that behavior cloning on BoN-sampled positive trajectories is sufficient to achieve KL-regularized optimality, which emerges from the analysis that the positive trajectory from BoN sampling converges to a distribution independent of the sample number. For learning on negative samples, \methodname{} reveals the necessity of reward shaping to maintain consistent gradient estimation between sampling and target distributions. Such a mechanism compensates for BoN's under-sampling of negative gradients, and enables difficulty-adaptive optimization over both successful and failed trajectories.

Another intrinsic property of mathematical reasoning tasks is the partial correctness in long reasoning chains, which further imposes the learning difficulty of sparse rewards when only a binary outcome reward is available at each iteration of RL training.
Thus, \methodname{} adopts a lightweight credit assignment scheme through a token-level reward model trained using outcome rewards. This mechanism automatically estimates step-wise importance weights by decomposing trajectory advantages, enabling focused learning of critical reasoning steps or errors.
The integration of these components yields a theoretically sound framework that effectively bridges the gap between sparse binary feedback and dense policy optimization requirements for mathematical reasoning tasks.

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{./figures/main_fig.pdf}
    \caption{Overall performance between \methodname{}-32B and some competitive baselines.}
    \vspace{-16pt}
    \label{fig: main_fig}
\end{figure}

Extensive experimental results show that \methodname{} effectively improves the mathematical reasoning capability of LLMs.
At the 7B parameter scale, to the best of our knowledge, \methodname{}-7B is the first to obtain the pass@1 accuracy on MATH-500~\cite{hendrycks2021measuring} to 91.0 using RL instead of distillation, which even exceeds QwQ-32B-Preview~\cite{qwq-32b-preview} and o1-mini~\cite{openai2024learning}.
OREAL also improves DeepSeek-R1-Distilled-Qwen-7B from 92.8 to 94.0 pass@1 accuracy, being on par with the previous best 32B models.
For the 32B model, \methodname{}-32B outperforms all previous models (Figure~\ref{fig: main_fig}), both distilled and RL-based, obtaining new state-of-the-art results with 95.0 pass@1 accuracy on MATH-500.
