% !TEX root = ../neurips_2024.tex

\section{Conclusion and Future Work}

This paper aims to explore the limit of \textbf{O}utcome \textbf{RE}w\textbf{A}rd-based reinforcement \textbf{L}earning for mathematical reasoning tasks, and proposes a unified policy optimization framework, termed \methodname{}, grounded in three key insights: 1) Behavior cloning on positive trajectories from Best-of-n (BoN) sampling is both necessary and sufficient for optimal policy learning under binary feedback;
2) Accordingly, a reward-shaping mechanism should be further introduced to transform the reward function derived from the optimal policy;
3) An efficient token-level credit assignment scheme can be achieved through trajectory advantage decomposition without relying on additional value networks.
Together, these components form a theoretically grounded, general, and scalable approach for mathematical reasoning tasks.
With \methodname{}, we are the first to improve the performance of a 7B model on the MATH-500 accuracy to 91 using the RL method instead of distillation, which even surpasses OpenAI-o1-mini and QwQ-32B-Preview. Even when taking the previous best 7B model, DeepSeek-R1-Distill-Qwen-7B, as initial policy, \methodname{} can improve it to 94 pass@1 accuracy on MATH-500, being even on par with the previous best 32B models.
\methodname{}-32B also obtains new state-of-the-art results among the 32B model on MATH-500, LiveMathBench, and OlympiadBench.

Along with the experimental observations presented in this paper, we also find two factors that are crucial for the success of scalable RL for mathematical reasoning tasks, which become the primary focus of our future work.
First, the initial policy model should be as free of knowledge deficiencies as possible, as this serves as the foundation for further improvement during the RL stage. A strong starting point ensures that RL can effectively and efficiently incentivize the underlying capability of LLMs obtained through pre-training or supervised fine-tuning. Towards this goal, it is a practical way to conduct distillation or data synthesis with DeepSeek-R1 or DeepSeek-V3, which is not explored in this work as it is orthogonal to our investigation.
Second, the quality of the data used in the RL phase must be diverse and sufficient in terms of difficulty, quantity, and scope. A well-balanced dataset enables the model to reach its full potential by exposing it to a broad range of challenges and learning opportunities. Thus, we believe it is still valuable to make efforts in the pre-training and post-training data construction process.
