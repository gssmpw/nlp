\begin{algorithm}[htbp]
\caption{The OREAL Reinforcement Learning Algorithm}
\begin{algorithmic}[1]
\STATE \textbf{Inputs:} Question set $\mathcal{D}$, policy model $\pi_\theta$, token-level reward model $w_\theta$, number of iterations $N$, batch size $B$, number of rollouts per question $K$.
\STATE Initialize policy $\pi_0$ and token-level reward model $w_0$ with $\pi_{\text{sft}}$.
\FOR{$i = 0, \dots , N$}
    \STATE Sample a batch of questions $\mathcal{D}_i \subseteq \mathcal{D}$ of size $B$.
    \STATE For $x \in \mathcal{D}_i$, generate $K$ policy samples: $Y = \{y_1, \dots, y_K\}$ where $y_k \sim \pi_i(x)$
    \STATE Obtain binary rewards $ \{r_1, \dots, r_K\}$ from verifier.
    \STATE Compute correctness rate: $p = \frac{1}{K} \sum_{k=1}^{K} r_k$ for reward shaping.
    \STATE Filter out questions with $0 < p < 1$ to avoid trivial cases.
    \STATE Select one correct $y^+$ and one incorrect sample $y^-$ for each question to avoid imbalance between positive and negative samples.
    \STATE Compute token-level importance sampling weights of each token with $w_i$.
    \STATE Use Eq \eqref{eq: ce} to update $w_i$.
    \STATE Update $\pi_i$ with Eq \eqref{eq: overall loss}
\ENDFOR
\STATE \textbf{Return:} The optimized policy model $\pi^*$.
\end{algorithmic}
\label{alg:oreal}
\end{algorithm}
