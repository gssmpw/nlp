\subsection{Tasks and datasets}
We evaluate our method \scope on a total of 6 datasets, spanning multiple domains and difficulties, where generating grounded context is a crucial requirement. We first run experiments on four data-to-text generation datasets.
\textbf{ToTTo} \citep{totto} is an English dataset with Wikipedia tables where specific cells are highlighted, paired with a sentence describing those cells.
%All examples in this dataset were post-edited in multiple steps to ensure that the targets are fully faithful to the input information. 
\textbf{WebNLG 2020 (English)} \citep{webnlg2020} is an English dataset composed of pairs of knowledge graphs and text crawled from DBpedia. 
%This is follow-up work from the WebNLG 2017 challenge \citep{webnlg2017}.
\textbf{E2E} \citep{e2e_cleaned} is an English benchmark dataset that verbalizes key-value attribute pairs in the restaurant domain. 
%The cleaned version has filtered out examples with hallucinations and outputs that do not fully cover all input attributes.
\textbf{FeTaQA} \citep{fetaqa} is an English table question answering dataset with tables from Wikipedia, paired with corresponding questions, answers, and supporting table cells.

We further evaluate the methods on three summarization datasets.
\textbf{XSum} \citep{xsum} contains BBC articles from 2010 to 2017, along with their summaries, each consisting of one highly abstractive sentence.
\textbf{SAMSum} \citep{samsum} is a dataset of messenger conversations about daily-life topics, annotated with short summaries.
\textbf{PubMed} \citep{summ-medical} is a collection of medical scientific articles where the goal is to summarize the conclusions of the authors based on the description of a medical experiment.

Although our primary focus is on domain-specific tasks, \Cref{app:scope-alpaca} shows the results of applying \scope to a generalist model fine-tuned with instructions on the Alpaca dataset \citep{alpaca}.

\subsection{Metrics}

% It has been shown that references of summarization and data-to-text generation can contain inaccuracies \citep{RebuffelRSSCG22,faithfulness-summarization}. Moreover, two summaries can contain different facts and both be faithful. To avoid all these issues, we focus our analysis on metrics that evaluate faithfulness with respect to the input context.

%Given the limitations of BLEU and ROUGE metrics, we focus our evaluation of faithfulness on metrics that evaluate the generation with respect to the input context. In addition to reference-based metrics, we also evaluate each method against standard fine-tuning for Llama.

We present in what follows the different metrics used for each task. Having in mind the limitations of BLEU and ROUGE metrics (resp. used for data-to-text generation and summarization as standard metrics for each task) and regarding our research objectives, we focus on faithfulness metrics that evaluate the generation with respect to the input context. 
%In addition to sample-based metrics, we also evaluate each method against standard fine-tuning for Llama.


\paragraph{BLEU (Data-to-text).} Traditional metric to assess the similarity between the generated text and given gold references. In the context of data-to-text generation, it has shown limitations especially when reference text diverges from the input data \citep{parent}.


\paragraph{PARENT Recall (PAR, Data-to-text).} \citep{parent} Noted PAR. A standard n-gram based faithfulness proxy metric for data-to-text introduced to address the limitations of BLEU. It assesses how well the candidate text replicates relevant entities from the data by measuring its n-gram recall against entities in the structured input. Unlike BLEU, PARENT Recall directly compares to the structured input, making it a more suitable measure of faithfulness.

 
\paragraph{NLI Score (NLI. Data-to-text).} Proposed by \citet{nli-d2t}, this metric adapts NLI models to data-to-text. It first computes the entailment probabilities of atomic input facts extracted from the structured data by the candidate text, characterizing \textit{omissions}. A second score measures \textit{hallucinations} by computing the entailment probability of the generated text by the sum of all the facts in the input data. The resulting NLI score is the minimum of all the entailment probabilities, assessing the overall faithfulness of the generated text.



\paragraph{ROUGE-L (R-L, Summarization).} \citep{lin-2004-rouge} Traditional n-gram overlap summarization metrics between the generated and the gold reference. Similarly to BLEU, it has known limitations \citep{fabbri-etal-2021-summeval} regarding faithfulness evaluation.

\paragraph{AlignScore (AL,  Summarization).} \citep{alignscore} A recent state-of-the-art entailment metrics. It measures the information alignment between the summary and the source article on a 0-1 scale, using a RoBERTa model \citep{roberta} trained on a unified set of entailment tasks.

\paragraph{QuestEval (Summarization).} \citep{scialom-etal-2021-questeval} A reference-free evaluation metric for summarization. It assesses the semantic alignment between the source article and the generated summary by generating and answering questions about their content. QuestEval uses a question generation and answering pipeline, leveraging a pre-trained language model, to compute a similarity score between the information in the source and the summary.

\paragraph{FactCC (Summarization).} \citep{kryscinski-etal-2020-evaluating} A factual consistency metric for summarization. It evaluates the factual alignment between the summary and the source article on a binary scale. FactCC relies on a fine-tuned BERT model, trained specifically to detect factual consistency through synthetic data generated by introducing factual errors into summaries.
% This metric employs NLI models that determine whether one sentence entails another or if they are in contradiction. In the context of data-to-text generation, \textit{omissions} occur when a fact from structured data is not entailed by the prediction. The generated text exhibits \textit{hallucinations} if it is not entailed by the sum of all the facts in the data. In both scenarios, the entailment probabilities are computed using the NLI model. The NLI Score is the minimum of all of them.

\paragraph{GPT-4 preference (Both tasks).} Previous work  \citep{gpt-gilardi,gpt-chiang} have shown that powerful LLMs, like GPT-4 can serve as effective proxies for human evaluation. To provide a scalable human-like assessment of the generations' faithfulness, we use GPT-4 for pairwise preference evaluation. Given a an input and two texts, the model is asked which sample is more faithful to the input data.  This metric yields win, loss, and tie rates against the standard fine-tuning baseline. Details regarding GPT-4 preference evaluation can be found in \Cref{app:gpt-eval}.


%We focus on assessing the faithfulness of the model to the input data. We use the variant of PARENT \citep{parent} that only considers the input context and computes the n-gram recall against the structured data. For NLI, we follow the methodology in \cite{nli-d2t} where the NLI score is given by the minimum of all the entailment probabilities. For reference, we report the traditional SacreBLEU \citep{sacrebleu}. Finally, we use GPT-4 as a proxy for human evaluation thanks to its correlation with human judgments. More details about the prompts used can be found in \red{Appendix}.

%\subsection{Datasets}
%We conduct our experiments on four data-to-text datasets:
%\textbf{ToTTo.} \citep{totto} An English table-to-text dataset, where each example is composed of a table from Wikipedia with highlighted cells that is paired with a sentence that describes these cells. 
%All examples in this dataset were post-edited in multiple steps to ensure that the targets are fully faithful to the input information. 
%\textbf{WebNLG 2020 (English).} \citep{webnlg2020}  a dataset composed of pairs of knowledge graphs and text crawled from DBpedia. 
%This is follow-up work from the WebNLG 2017 challenge \citep{webnlg2017}.
%\textbf{E2E.} \citep{e2e_cleaned} an English benchmark dataset that verbalizes a set of key-value attribute pairs in the restaurant domain. 
%The cleaned version has filtered out examples with hallucinations and outputs that do not fully cover all input attributes.
%\textbf{FeTaQA.} \citep{fetaqa} A dataset consisting of tables sourced from Wikipedia, along with corresponding questions, answers, and supporting table cells.

\subsection{Models and baselines}
We experiment on \textsc{Llama2-7B} \citep{llama2} and \textsc{Mistral-7B} \citep{mistral}, two recent LLMs. 
For all baselines, hyperparameters were carefully determined from a grid-search following recommendations in reference articles, using NLI Score for data-to-text generation and AlignScore for summarization as objectives. All training details and hyperparameters can be found in \Cref{app:experiments}.

\paragraph{Supervised fine-tuning (\sft).} This is the standard fine-tuning approach where the pre-trained model $\plm$ is optimized using MLE on the \emph{full} training dataset $\dataset$. We train for 3 epochs and choose the model according to NLI Score for data-to-text generation and AlignScore for summarization.

\paragraph{PMI decoding (\pmi). } \citep{pmi} \pmi reduces hallucinations by penalizing "ungrounded tokens" when next-token entropy is high, adjusting probabilities using a context-less model with hyperparameters \( \lambda \) and \( \tau \).

\paragraph{Context-aware decoding (\cad).} \citep{cad}  Similar to \pmi, \cad downweights probabilities using a context-less model, with an adjustment factor controlled by \( \alpha \).

\paragraph{Critic-driven decoding (\critic).} \citep{critic-driven} \critic improves generation by using, for each dataset, a model trained to differentiate context-supported tokens. It factors the model probability and generates samples based on a score combining the token probability and the classifier's context likelihood, adjusted by \( \lambda \).


\paragraph{\cliff.} \citep{cao-wang-2021-cliff} CLIFF is a training method that leverages a contrastive learning framework, where more positive samples are generated through a back-translation method, while negative samples are created using Named Entity Recognition (NER) models and different mask-and-generate methods. We choose the \textsc{MaskRel} baseline, which demonstrate strong overall results in the original paper. Initially designed for encoder-decoder models, we reimplemented the method for decoder-only architectures.

\paragraph{\scope (ours).} Models trained following \scope framework. For the experiments, we tune the noise level $\alpha$  by selecting the value that yields the highest NLI Score or AlignScore on the validation set. As detailed in \Cref{sec:analysis}, we restrict our search of $\alpha$ to the $[0.4, 0.6]$ interval, which corresponds to a zone where the BLEU/ROUGE scores does not decrease significantly. The selected value for each dataset and model can be found in \Cref{tab:scope-hyperparameter}. 

\cliff and \scope are methods that present a training method, while \cad and \pmi modifies the decoding process. \critic trains a model to modify the decoding process. We highlight that all baselines have been trained on the same amount of annotated samples, since decoding methods are applied to a fully fine-tuned model.

\section{Results}
\label{sec:results}
\begin{table*}[t]
    \centering
    \resizebox{\textwidth}{!}{\input{tables/results}}
    \caption{Performance comparison on the test set of ToTTo, E2E, FeTaQA, and WebNLG. Note that the missing BLEU results are due to the absence of gold references in the test set of ToTTo. $^*$ denotes faithfulness scores statistically significantly higher than the \sft baseline.}
    \label{tab:results}
    \vspace{-0.4cm}
\end{table*}

\begin{table*}[t]
    \centering
    \resizebox{\textwidth}{!}{\input{tables/summ_results}}
    \caption{Performance comparison on the test set of SAMSum, XSum and PubMed. $^*$ denotes faithfulness scores statistically significantly higher than the \sft baseline.}
    \label{tab:summ_results}
    \vspace{-0.7cm}
\end{table*}
We now present the results of \scope and of the baselines on the data-to-text generation and text summarization tasks introduced above.
%\red{commentaire des expes: (i) echec des methodes autres qui n'ameliorent pas SFT, (ii) notre methode marche beyond suoervisé parce que (i) training, (ii) preferences car discriminative SFT est limité , (iii) analyse. Note that all the current baselines have been developed and evaluated up to now on smaller LLM models.}


%\paragraph{Selecting $\alpha$.} We select the best $\alpha$ for \scope for each dataset using the model that gives the best NLI score. However, since NLI does not penalize non-fluent behavior that appear when $\alpha$ is too low, we select ranges of $\alpha$ on which values of BLEU do not decrease significantly. In practice, we consider only value of $\alpha \geq 0.4$.



\paragraph{\scope improves faithfulness over all tasks and domains.} 
%According to faithfulness metrics, training models using \scope gives a significant improvement in faithfulness metrics compared to standard fine-tuning,  \Cref{tab:results,tab:summ_results}.
According to automatic faithfulness metric, training with \scope gives consistent and significant improvement in faithfulness compared to standard fine-tuning, as presented in \Cref{tab:results,tab:summ_results}.
For data-to-text generation (\Cref{tab:results}), training models with \scope show significant improvements over standard fine-tuning, with an increase of up to 8.2 and 5.5 points PARENT and NLI Score respectively. For text summarization (\Cref{tab:summ_results}), \scope demonstrates an increase of up to 8.8 points in AlignScore.
On most datasets, \scope scores slightly lower BLEU and ROUGE scores than other baselines, especially on the abstractive XSum dataset.  Previous work \citep{goyal2022zeroshotnews} highlighted the saturation of summarization benchmarks and the limitations of reference-based metrics like BLEU and ROUGE in evaluating the summarization capabilities of recent LLMs. Given the high faithfulness scores achieved by \scope on both tasks, we suggest that this decrease in BLEU and ROUGE may indicate \scope's tendency to deviate from standard fine-tuning and to disfavor irrelevant generation.



\paragraph{Baselines present mixed results on faithfulness metrics.} 
Summarization-focused baselines (\cad, \pmi, \cliff) show an overall increase in AlignScore on SAMSum, XSum and PubMed (\Cref{tab:summ_results}). However, the improvements on XSum remain marginal compared to \scope's results. For data-to-text generation, all baselines show minimal to no faithfulness improvement over \sft (\Cref{tab:results}).
Depending on the methods, we identified two reasons that could explain these mixed results. First, \cliff, \critic, and \pmi were originally designed for smaller encoder-decoder models. We suspect that differences in architecture and the number of parameters in larger, more recent LLMs may limit their effectiveness. Secondly, \cad, \pmi, \cliff were mainly designed for general summarization tasks, we suspect that for data-to-text generation, which require further adaptation, these methods may fall short.

%The \critic baseline, specifically developed for data-to-text generation, also shows minimal gains in faithfulness.

% This may be due to the fact that methods like \cliff, \critic, and \pmi were originally designed for smaller encoder-decoder models. We suspect that differences in architecture and the number of parameters in larger, more recent LLMs may limit their effectiveness. Additionally, applying these baselines to data-to-text tasks shows minimal to no faithfulness improvement over \sft (\Cref{tab:results}). We suspect that for domain-specific tasks like data-to-text generation, which require further adaptation, existing faithfulness-enhanced methods for summarization may fall short.

\paragraph{GPT4-as-a-judge evaluation.} To further assess their performances, all methods applied to \textsc{Llama-2-7B} were compared to standard fine-tuning, with GPT-4 used as the evaluator. Results are presented in \Cref{tab:gpt_results,tab:gpt_summ_results}. Across all datasets, \scope consistently shows a much higher win rate than other methods, confirming its efficiency in improving faithfulness. For the baselines, especially in data-to-text generation tasks, we observe a noticeable high tie rate. This indicates that a significant proportion of the samples are considered equivalent in quality to the standard fine-tuning samples. Consequently, it suggests that these methods have not adequately addressed the faithfulness issues related to fine-tuning.




%\subsection{Ineffectiveness of faithfulness enhanced approaches}
% \paragraph{Current faithfulness enhanced baselines fail to show improvements }
% Using \textsc{Llama-2-7b} and \textsc{Mistral-7b}, we evaluate the baselines on all the datasets and present the results in \Cref{tab:results,tab:summ_results}. A first finding is that, compared to standard supervised fine-tuning, these methods offer minimal or no improvement in both PARENT and NLI (around $\pm 0.2\%)$ for data-to-text. This is further confirmed by the GPT-4 evaluation (see \Cref{tab:gpt_results}), where the \textbf{very high tie rates against \sft} prove their ineffectiveness in enhancing faithfulness. For summarization, 

% \paragraph{How does \scope perform on data-to-text generation and text summarization?}
% Across all datasets and for both \textsc{Llama-2-7b} and \textsc{Mistral-7b}, \scope consistently outperforms the self-enhanced baselines and \sft by a margin up to 6 points and 9 points for \textsc{Llama-2-7b} and \textsc{Mistral-7b}, respectively on PARENT, and up to 5 points and 7 points for \textsc{Llama-2-7b} and \textsc{Mistral-7b}, respectively on NLI Score, see \Cref{tab:results}. When judged by GPT-4, our approach exhibits a preference win rate against SFT that is 5 to 10 times higher than the ones of the other baselines, showcasing the effectiveness of our self-supervised framework.
% Moreover, this comes at a small cost of fluency, as assessed by the BLEU scores of our models which are only slightly below the ones of \sft. Qualitative examples of winning samples are provided in \Cref{app:win_samples}.

\begin{table*}[h!]
    \centering
    \resizebox{\textwidth}{!}{\input{tables/gpt_eval_results}}
    \caption{GPT-4 preference results of \cad, \pmi, \critic, \cliff and \scope versus \sft with \textsc{Llama-2-7b} on ToTTo, E2E, FeTaQA and WebNLG. Results with $^*$ are statistically significantly higher than all other baselines.}
    \label{tab:gpt_results}
    \vspace{-0.7cm}
\end{table*}

\begin{table*}[h!]
    \centering
    \small
    \resizebox{0.8\textwidth}{!}{\input{tables/gpt_eval_summ_results}}
    \caption{GPT-4 preference results of \cad, \pmi, \critic, \cliff and \scope versus \sft with \textsc{Llama-2-7b} on SAMSum, XSum and PubMed. Results with $^*$ are statistically significantly higher than all other baselines.}
    \vspace{-0.5cm}
    \label{tab:gpt_summ_results}
\end{table*}

\begin{wraptable}{r}{0.45\textwidth}
%\begin{table}[r]{0.4\textwidth}
\centering
\footnotesize
\input{tables/human_eval}
\caption{Human preference results of  \scope versus \sft on ToTTo test set with \textsc{Llama-2}.}
\label{tab:human_eval}
\vspace{-0.3cm}
\end{wraptable}
\paragraph{Further validation through human evaluation.}
%\hil{TODO: Expliquer la subtilité de la fidélité de SCOPE vs SFT et réinsister sur l'aspect fiable et non ambigu de l'évaluation humaine pour les tâches de data-to-text.}

%\hil{The input data in data-to-text generation is inherently concise and unambiguous, with only the necessary entities and their relationships being presented in a structured format. This clarity makes it easy to determine exactly what needs to be translated into fluent text. This allows us to qualitatively assess the faithfulness of a generation with respect to a given structured data}. 
In addition to using automatic faithfulness metrics and GPT-4 preference judgments, we conduct human evaluations to comprehensively assess the quality of \scope generations. We distribute different sets of 25 ToTTo samples to 5 annotators, totaling 125 samples. Each sample includes a table, one generation from \scope and one from \sft, using \textsc{Llama-2-7b}. Annotators are tasked with rating which of the two descriptions is more faithful to the table. They are asked to put the emphasis on \emph{faithfulness exclusively}, meaning that although a generation may contain factually correct details, these additions are deemed less desirable than a generation that strictly relies on the information provided in the table. Full experimental details are described in \Cref{app:human-eval}.
The results are presented in \Cref{tab:human_eval}.
The descriptions generated by \scope are preferred twice as often as those by the associated \sft. The results closely match those obtained with GPT4-as-a-judge, further validating the soundness of our approach. We present some samples of \scope and \sft generations in \Cref{app:win_samples}.



% \paragraph{PMI decoding (\pmi).} \citet{pmi} suggest that hallucinations occur when the model is "unsure" about the next token prediction, indicated by a high entropy in the next token distribution. To address this, they propose reducing the likelihood of "ungrounded tokens", which is approximated using a language model that does not have access to the context $c$, whenever the entropy of $\psft$ is high. The samples are generated by maximizing the following score:
% \begin{equation*}
%     \pmi(y_t \given \ylt, c)  = \ \log \psft(y_t \given y_{<t}, c)
%     - \lambda \cdot \mathbf{1}_{\bigl\{ H\mleft(\psft(\cdot \mid y_{<t}, c)\mright) \geq \tau \bigr\}} \cdot \log \plm(y_t \mid y_{<t}),
% \end{equation*}
% where \( H \) denotes the entropy of the distribution, and \( \lambda \) and \( \tau \) are hyperparameters.


% \paragraph{Context-aware decoding (\cad).} \citet{cad} propose a similar approach. The key difference is that the downweighting of probabilities is applied at each step, irrespective of the entropy:
% \begin{equation*}
%     \cad(y_t \given \ylt, c) =\ (1 + \alpha) \log\psft(y_t \given \ylt, c) - \alpha \log\plm(y_t\given \ylt).
% \end{equation*}
 
% \paragraph{Critic-driven decoding (\critic).}  \citep{critic-driven}. This method also aims to correct the generation during decoding. For this, they train an auxiliary classification model to distinguish between tokens supported by the context and the others, over all pairs $(c, y) \in \dataset$. More formally, they consider the factorization  $\pmodel(y_t \given \ylt, c) \propto p(c \given y_{\leq t}) \psft(y_t \given \ylt, c)$ where $p(c \given y_{\leq t})$ is approximated by the classifier. Finally, samples are generated using the following score:
% \begin{equation*}
%         \critic(y_t \given \ylt, c) = \log \psft(y_t \given \ylt, c)+\lambda \log p(c \given y_{\leq t}).
% \end{equation*}
