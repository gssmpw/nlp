% \section{Challenges of current faithfulness enhanced methods in data-to-text}
% \label{sec:d2t}
% Data-to-text poses a significant difficulty for generalist LLMs which have been mostly pre-trained on plain text. When prompted with a linearized structured input, they are either too verbose or produce a degenerated output due to the peculiar input formatting \citep{hard_d2t}. Because this, generalist pre-trained LLMs lag behind their supervised counterparts on data-to-text.

% \paragraph{Faithfulness evaluation in data-to-text.} Measuring faithfulness automatically is not straightforward. Traditional data-to-text generation evaluation often relies on comparing the generated output to a reference text, typically measured using the BLEU score \citep{papineni-bleu}. Additionally, data-to-text corpora are often collected automatically, leading to divergences between the reference text and the actual input data. This highlights BLEU's limitations in data-to-text, since no direct comparison to input data is actually performed. To address these issues, evaluation methods that take into account the input data have been proposed. \citet{parent} introduce PARENT, which computes the recall of n-gram overlap between the entities in the data and the candidate text. \citet{nli-d2t} develop an entailment metric using Natural Language Inference (NLI) models, where the generated text is compared directly to a simple verbalization of the data. The gold-standard still remains the human or human-like evaluation, conducted with powerful generalist LLMs. These metrics form the core focus of our work.

% \paragraph{Faithfulness self-enhancement.}
% We position our work with respect to existing self-supervised approaches.
% \citet{cad, pmi} propose a method to downweight the probability of tokens that are not grounded in the input context.
% For this, they alter only the decoding process without additional training, by using an auxiliary LM without access to the input context. The generation is guided by a new decoding objective in the form of:
% \begin{equation}
%     \log p(y_{t}) \propto \log p_\theta(y_{t} \given y_{<t}, c) - \lambda \log p_{\theta^\prime}(y_t \given y_{<t}).
% \label{eq:faith}
% \end{equation}
% The intuition behind this approach is that by downweighting the token logits that are highly scored by a model lacking access to the context $c$, the method should instead favor tokens that are grounded in $c$. Although these methods have shown great results in many conditional text generation tasks such as summarization or QA, they have not yet been applied to structured knowledge tasks like data-to-text. According to our assessment, they bring little or no improvement compared to the fine-tuned versions (see \Cref{tab:results}). We believe this to be peculiar to data-to-text tasks:
% \begin{itemize}[leftmargin=*] \setlength{\itemsep}{0em}
%     \item These methods have been primarily designed and tested for summarization and QA tasks, where both the context and the output consist of fluent text. As a result, the fine-tuned model's distribution does not diverge significantly from the initial language model distribution, used for logits penalization.  In such cases, methods such as in \Cref{eq:faith} can penalize non-grounded tokens.
%     \item For data-to-text, the models' distribution is more tightly constrained by the context, which encapsulates atomic facts. The expected output text is also much shorter and more straightforward than in other generative tasks. This difference leads to significant divergence between fine-tuned models and pre-trained models, resulting in minimal improvements from these methods.
% \end{itemize}

% We propose to tackle this issue by going one step forward and directly optimizing the model for faithfulness using a self-supervised preference framework. %Rather than using a pre-trained LLM during the decoding stage, we leverage it to construct non-grounded inputs that are more meaningful for the fine-tuned models.
% %On data-to-text, without further fine-tuning, these methods fall short compared to the supervised fine-tuned version. Furthermore, applying them to the SFT model does not seem to bring improvements as the unconditional distribution $p_\theta(\cdot \mid r)$ is not being optimized during the SFT phase while $p_\theta(\cdot \mid r, \boldsymbol{c})$ is, resulting in vanilla decoding since the first term in \Cref{eq:pmi} is dominant. This suggests the need to explore alternatives to supervised fine-tuning that guide the model to better integrate context.



