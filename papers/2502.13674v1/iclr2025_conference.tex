\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference, times}


% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.

\usepackage{hyperref}
\usepackage{url}

% Standard package includes
\usepackage{latexsym}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{tabularx}
\usepackage{soul}
\usepackage{wrapfig}

\newcommand{\redhl}[1]{\sethlcolor{red!40}\hl{#1}}
\newcommand{\yellowhl}[1]{\sethlcolor{yellow}\hl{#1}}
\newcommand{\greenhl}[1]{\sethlcolor{blue!20}\hl{#1}}
\newcommand{\blue}[1]{\protect\textcolor{blue}{#1}}
% command for ICLR change tracking
% Define a switch for highlighting
\newif\ifhil

% Define the highlight command
\newcommand{\hil}[1]{\ifhil\textcolor{blue}{#1}\else#1\fi}

% Command to turn on highlighting
\newcommand{\highlighton}{\hiltrue}

% Command to turn off highlighting
\newcommand{\highlightoff}{\hilfalse}



\input{settings.tex}

\input{math_commands.tex}

\title{\textsc{Scope}: A Self-supervised Framework for Improving Faithfulness in Conditional Text Generation}
% Author information can be set in various styles:
% For several authors from the same institution:


\author{{\noindent
Song Duong$^{*,1,4}$} \quad {\bf Florian Le Bronnec$^{*,1,2}$} \quad
{\bf Alexandre Allauzen$^{2}$} \quad {\bf Vincent Guigue$^{3}$} \\ {\bf Alberto Lumbreras$^{4}$} \quad {\bf Laure Soulier$^{1}$} \quad {\bf Patrick Gallinari$^{1,4}$} \\
{\small $^1$Sorbonne Université, CNRS, ISIR, F-75005 Paris, France} \\ 
\small$^2$Miles Team, LAMSADE, Université Paris-Dauphine, Université PSL, CNRS, 75016 Paris, France \\ 
\small$^3$AgroParisTech, UMR MIA-PS, Palaiseau, France \\
\small$^4$Criteo AI Lab, Paris, France \vspace{-0.3cm}
}


\iclrfinalcopy
%\lhead{Published as a conference paper at ICLR 2025}
\begin{document}
\highlighton
%\highlightoff
\maketitle
\def\thefootnote{*}\footnotetext{Equal contribution. Corresponding authors: s.duong@criteo.com, florian.le-bronnec@dauphine.psl.eu}
\begin{abstract}

    % Large Language Models (LLMs), when used for conditional text generation, often prioritize their internal knowledge over the provided contextual input, leading to the generation of hallucinations i.e. information that is unfaithful or not grounded in the context. This issue arises in typical conditional text generation tasks, such as summarization and data-to-text, where the goal is to produce fluent text based on structured contextual input. Current approaches, whether relying on fine-tuning or modifying the decoding process of pre-trained models, struggle to provide faithful answers, often adding information not present in the context or generating errors.  We introduce \scope (Self-supervised Context Preference), a novel self-supervised approach for generating a training dataset of unfaithful samples, and leverage preference training using this dataset to boost the faithfulness of pre-trained models. Experiments with recent LLMs demonstrate that our approach significantly improves faithfulness, outperforming existing self-supervised solutions in metrics that measure hallucination and entailment.

    %In typical conditional text generation tasks, Large Language Models (LLMs) are expected to generate responses that are grounded in the input context. This issue is critical in typical conditional text generation tasks, such as summarization and data-to-text, where the goal is to produce fluent text based on structured contextual input.  However, when generating texts, LLMs rely on statistical patterns learned from training data, which can interfere with the model’s ability to stay faithful to the input, leading to the generation of ungrounded information. We build upon this observation and propose a novel self-supervised method for generating a training set of unfaithful samples. Using this dataset, we perform preference training to improve the model’s adherence to the input context. Our approach leads to significantly more grounded text generation, outperforming existing self-supervised techniques in faithfulness, as evaluated through automatic metrics, LLM-based assessments, and human evaluations.

    % sinon on ne parle pas de internal knowledge dans l'abstract ?
    Large Language Models (LLMs), when used for conditional text generation, often produce hallucinations, i.e., information that is unfaithful or not grounded in the input context. This issue arises in typical conditional text generation tasks, such as text summarization and data-to-text generation, where the goal is to produce fluent text based on contextual input. When fine-tuned on specific domains, LLMs struggle to provide faithful answers to a given context, often adding information or generating errors. One underlying cause of this issue is that LLMs rely on statistical patterns learned from their training data. This reliance can interfere with the model’s ability to stay faithful to a provided context, leading to the generation of ungrounded information. We build upon this observation and introduce a novel self-supervised method for generating a training set of unfaithful samples. We then refine the model using a training process that encourages the generation of grounded outputs over unfaithful ones, drawing on preference-based training. Our approach leads to significantly more grounded text generation, outperforming existing self-supervised techniques in faithfulness, as evaluated through automatic metrics, LLM-based assessments, and human evaluations. Code is available at \url{https://github.com/sngdng/scope-faithfulness}.



    %\red{j'ai reecrit un abstract -



\end{abstract}



\section{Introduction}
\label{sec:intro}
\input{0_introduction_1}

%\input{legacy_writing}

\section{Related work}
\label{sec:rel_work}
\input{1_related_work}

\input{2_background}

\label{sec:cafet}
\input{3_method}

\section{Experiments}
\label{sec:exp}
\input{4_experiments}

\section{Analysis of \scope}
\label{sec:analysis}
\input{5_analysis}

\section{Conclusion}

%After demonstrating that current solutions to improve faithfulness fail in the context of data-to-text applications, we introduce a training method that exploits preference learning by generating a preference dataset in a self-supervised manner for training. This approach consistently improves the faithfulness of generated answers across various data-to-text tasks, significantly outperforming existing solutions as assessed by relevant automatic faithfulness metrics and evaluations leveraging GPT-4 and human judges. The key contributions lie in the automatic and self-supervised construction of the preference dataset tailored for the model and in the associated framework that enables preference learning. We provide an analysis of the main factors underlying the successful deployment of the method, with its behavior illustrated quantitatively and qualitatively on typical samples.

Faithfulness hallucinations are a common issue in standard fine-tuned LLMs, and existing methods developed to mitigate these hallucinations yield mixed results with recent LLM models. In contrast, we demonstrate that employing a two-stage method, distinct from standard fine-tuning, effectively addresses typical challenges. Our key contributions include the automatic and self-supervised construction of a preference dataset tailored for the model, along with a framework that enables preference learning. Notably, our approach, \scope, consistently enhances the faithfulness of generated responses across various data-to-text and summarization tasks, significantly outperforming existing solutions as assessed by relevant automatic faithfulness metrics, evaluations using GPT-4 and human judges. We provide an analysis of the main factors contributing to the successful deployment of this method, illustrating its performance quantitatively and qualitatively with typical samples.


\section{Limitations}
\label{sec:limitations}
\input{6_limitations}
\section{Acknowledgments}
\label{sec:acknowledgments}

This work has been partly funded through project ACDC ANR-21-CE23-0007.
This project was provided with computing AI and storage resources by GENCI at IDRIS thanks to the grants 20XX-AD011014053R2, 20XX-A0151014638, 20XX-A0171014638 and 20XX-A0151014627 on the supercomputer Jean Zay's V100/A100 partition.
\newpage
%Compared to pure decoding based methods that operate on pre-trained LLMs, the performance increase in our model is achieved at the cost of retraining the LLM. Although the tasks on which the method has been evaluated correspond to classical benchmarks in data-to-text, their complexity remains limited. This limitation comes from both the structured data involved and the nature of the queries made on these data sets.
%For computational capabilities reasons, we restricted ourselves to two models of 7B parameters, which we believe to be representative of typical recent LLMs. Experiments on other models could be conducted to assess the generalization of the method.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\appendix
\input{appendix}

\end{document}
