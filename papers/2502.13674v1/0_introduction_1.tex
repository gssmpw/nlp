

Large Language Models (LLMs) are widely used for generating fluent and coherent text completions based on input contexts \citep{brown2020language}. These models generate completions by leveraging the statistical patterns encoded in their parameters, which are learned from extensive training data. While these parameters provide the model with a broad knowledge of various topics, they can also cause interference. This occurs when the model combines information provided in the input context with general patterns from its training data, potentially leading to inaccuracies. More generally, irrelevant content generated by a LLM is commonly referred to as \textit{hallucinations} \citep{RebuffelRSSCG22,faithfulness-summarization}.
To mitigate these hallucinations, two primary dimensions are considered: \textbf{factuality} and \textbf{faithfulness} \citep{surveyhallucinationllm}. Factuality refers to whether the model's generated information aligns with external, real-world knowledge and is typically evaluated against a reference dataset or established knowledge. Faithfulness, on the other hand, evaluates how accurately the generated content reflects the information provided in the input context. A model may produce factual but unfaithful content if, while true with respect to world knowledge, it distorts important details from the input or adds extra information (see \Cref{tab:faithful_factful_medical}). This is particularly crucial in fields where accurate information transfer is essential. For instance, in medical transcription, the text output must accurately reflect the content of the medical record without introducing any distortions \citep{cawsey1997naturallanguagegenerationhealthcare}. 




\begin{table}[h]
\small
    \centering


    \textbf{Patient Data (Input)}:
    
    \begin{tabular}{ c c c c c }
        \hline
        \textbf{Age} & \textbf{Sex} & \textbf{Symptoms} & \textbf{Diagnosis} & \textbf{Treatment} \\ \hline
        45 & Male & Persistent cough & Pneumonia & Antibiotics \\ \hline
    \end{tabular}

    \vspace{0.5cm} % Space between input and output examples

    \textbf{Output Examples}:
    \begin{tabular}{c c l }
        \hline
        \textbf{Faithful} & \textbf{Factful} & \textbf{Output} \\ \hline
        No                & No               & \redhl{21 y.o. female} with a \redhl{headache} due to a \redhl{migraine} is given antibiotics. \\ 
        No                & Yes              & 45 y.o. male with a cough due to pneumonia is given \redhl{amoxicillin}. \\
        %Yes               & No               & 45 y.o. male with a cough has the flu and is told to rest.  \\
        Yes               & Yes              & 45 y.o. male with a cough due to pneumonia is given antibiotics. \\ \hline
    \end{tabular}
    \caption{An example of faithful and factful combinations in LLM for data-to-text generation in a medical context. Unfaithful spans are highlighted in \redhl{red}. While amoxicillin is a common antibiotic prescription for pneumonia, the name of the antibiotics is not the mentioned in the table.}
    \label{tab:faithful_factful_medical}
    \vspace{-0.6cm}
\end{table}

%We therefore focus on \emph{faithfulness hallucinations} in text generation conditioned on an input text. Examples of such conditional text generation tasks include summarization, translation, data-to-text generation, conversation, or question answering. 

% Summarization, translation, data-to-text generation, conversation, or question answering are examples of tasks where the model should remain grounded in the input text, without generating \emph{faithfulness hallucinations}.

In this paper, we focus on the generation of \textbf{faithful} responses grounded in a self-contained input context. A major challenge concerning faithfulness is the difficulty of annotating data and there is no standard way to determine if a text is faithful to an input context. As a result, annotation is typically performed by humans \citep{goyal2021annotating,kryscinski-etal-2020-evaluating}. However, this approach is costly, not scalable, and the resulting annotations might not transfer to other domains. To circumvent the lack of annotated data, some unsupervised methods have been proposed. A first line of research consists of leveraging a contrastive loss on hidden representations \citep{zhao-etal-2020-reducing,kryscinski-etal-2019-neural}.  These methods have demonstrated improvements on small models (around 500 million parameters), but they have not yet been benchmarked on recent LLMs. Our evaluations indicate that their effectiveness does not appear to extend to these larger models.
Another direction consists of altering the decoding process of pre-trained models \citep{cad, pmi}. While these methods work well on generalist text datasets, we found that on more domain specific tasks where a heavy fine-tuning is required such as data-to-text generation, these methods struggle to improve over standard fine-tuning of models (see \Cref{sec:results}).
 
%Our experiments show that applying these methods to tasks like data-to-text generation or text summarization does not significantly improve the faithfulness of recent LLMs. 

Acknowledging these limitations, we propose a novel fine-tuning framework, tailored for recent LLMs. Drawing inspiration from recent work \citep{dpo}, propose a method tailored for recent LLMs that teaches a model to disfavors ungrounded generation. Unlike typical preference-tuning which involves human annotation of model-generated outputs, we aim for a self-supervised process to generate a dataset of \textit{prefered} and \emph{dispreferred} samples. Here, in the context of faithfulness, the goal is to teach the model to prefer the context-grounded reference labels over unfaithful ones that present hallucinations. A challenge then lies in the generation of representative unfaithful examples that convey effective learning signals. These examples should closely resemble target sentences while exhibiting realistic hallucinations. In conditional text generation tasks, hallucinations occur when the model's internal knowledge improperly influences the generation process \citep{faithfulness-summarization}. Building on this observation, we propose an original procedure for automatically generating realistic examples. This generation process is fully unsupervised and does not require external resources. 
%Finally, these unfaithful samples will serve as negative examples in the preference-tuning phase, while context-grounded references will act as positive examples.
We apply our method to six datasets across various domains for data-to-text generation and text summarization. Data-to-text generation \citep{lin-d2t} involves converting structured data like tables into coherent language, while summarization condenses longer texts while preserving key information. Faithfulness is essential for both tasks to ensure the generated text accurately reflects the input data. To summarize, in this paper:

\begin{itemize}[leftmargin=*]
    \item We introduce \scope, a new method that leverages ideas from preference training by using a self-supervised generated dataset. In this approach, the model is trained to favor reference labels over carefully generated unfaithful samples.
    \item We empirically show that our approach significantly enhances the faithfulness of text generated by fine-tuned LLMs, surpassing current faithfulness-enhanced methods for conditional text generation.
    \item We bring new insights on the behavior of preference-tuning by analyzing its sensitivity to the effect of negative samples.
\end{itemize}


Our experiments reveal that training using \scope achieves up to a 14\% improvement in faithfulness metrics over existing methods, according to automatic evaluation metrics. Furthermore, evaluations by both GPT-4 and human judges indicate that the generations with \scope are substantially more faithful, with an improved preference win rate against the supervised fine-tuned model that is in average 2.1 times higher than the baselines.

%Inspired by preference-tuning \citep{dpo}, we propose a method tailored for recent LLMs that teaches a model to prefer the context-grounded reference labels over unfaithful ones that present hallucinations. Typically, preference-tuning involves human annotation of model-generated samples, followed by training the model to learn these preferences. In this work, we aim for a self-supervised process that requires no human intervention.
%%A challenge then lies in the generation of representative unfaithful examples that convey effective learning signals. These examples should closely resemble target sentences while exhibiting realistic hallucinations. In conditional text generation tasks, hallucinations occur when the model's internal knowledge improperly influences the generation process \citep{faithfulness-summarization}. Building on this observation, we propose an original procedure for automatically generating realistic examples. These examples will be used as a training set for our preference-tuning framework. This generation process is fully unsupervised and does not require external resources, contrarily to typical preference-tuning which involves human annotations of model-generated samples.

% Our experiments show that applying these methods to tasks like data-to-text or summarization does not significantly improve the faithfulness of recent LLMs. For conditional text generation tasks, hallucinations arise when the model's internal knowledge leaks inappropriately in the generation process \citep{faithfulness-summarization}. We draw inspiration from this observation to teach a model to prefer context grounded responses over unfaithful ones that present hallucinations. For that we make use of preference tuning\cite{}. The latter usually works by letting a human annotate samples generating by a model according to its preference and train the model to learn this preference. Here we want a self supervised process without human intervention.
% A challenge then lies in the generation of representative unfaithful examples that convey effective learning signals. They should at the same time be close to The target sentences while exhibiting realistic hallucinations. We propose an original procedure for generating realistic examples automatically. These will be used as a training et for our preference tuning framework. This generating process is fully unsupervised and does not require using external resources.
%Leveraging this observation, we simulate this phenomenon explicitly when training our models, using a new noisy decoding method. In this method, the model is randomly prompted to generate text spans based only on its internal knowledge, using input contexts from the original training set. We then train the model to disprefer these unfaithful samples compared to the reference ones.


%We apply our method on six datasets covering different domains for the tasks of data-to-text generation and text summarization. Data-to-text \citep{lin-d2t} involves transforming structured data, such as tables or charts, into coherent natural language descriptions, while text summarization focuses on condensing longer texts into shorter versions while preserving key information. Faithfulness is crucial for both tasks to ensure that the generated text accurately reflects the input data, maintaining the integrity of the information conveyed. 





% Therefore, in this setting generative tasks when the input context is more easily quantifiable are particularly interesting.

% Discuter des tâches que l'on considère resume et data2text et why.\\





%\begin{figure}[t]
%    \centering
%    \includegraphics[width=0.6\columnwidth]{images/sample.pdf}
%    \caption{Qualitative example  of \scope compared to the supervised fine-tuned baseline model (\sft) with \textsc{Llama2-7b}, taken from the E2E dataset. Data-to-text proposes an appealing framework for easily quantifying the extent to which a given prediction is faithful to the input structured data.}
%    \label{fig:sample}
%\end{figure}



% Inspired by the Preference Optimization framework \citep{dpo}, we craft unfaithful samples in a self-supervised manner without relying on any kind of external intervention. The model is then optimized to prefer the reference label over the self-generated unfaithful one. We achieve this by perturbing the model's prediction with an LLM that has no access to the input context, simulating potential hallucinations that arise from the impact of the model's internal knowledge on the generation process.
% To summarize, in this paper:

% \begin{itemize}[leftmargin=*] \setlength{\itemsep}{0em}
%     \item We demonstrate that generic faithfulness-enhanced approaches do not significantly improve faithfulness in data-to-text tasks, using recent LLMs such as \textsc{Llama-2-7b} \citep{llama2} and \textsc{Mistral} \citep{mistral}.
%     \item We introduce \scope, a method that leverages ideas from preference training by using a self-supervised, generated preference dataset. In this approach, the model is trained to favor reference labels over carefully crafted noisy samples.
%     \item We empirically show that our approach significantly enhances the faithfulness of text generated by fine-tuned LLMs, surpassing current faithfulness-enhanced methods for data-to-text.
% \end{itemize}

%In natural language generation, summarization, translation, data-to-text generation, conversation, or question answering are examples of tasks where models are expected to generate responses without adding external information nor modifying the input data (see \Cref{fig:sample}).


%\begin{figure}[h]
%    \centering
%    \caption{A toy example of Faithful and Factful combinations in LLM Responses. Since the prompts contradict the internal knowledge, a factful and faithful generation is impossible and the models has to choose between the internal knowledge, the prompt, or neither.}
%    \label{fig:faithful_factful_example}
%
%    \textbf{Prompt:} \textit{ICLR stands for International Conference on Literature and Religion. What's the main topic of ICLR?}

%    \vspace{0.5cm} % Space between the prompt and the table
%
%    \begin{tabular}{c c l}
%    \hline
%        \textbf{Faithful} & \textbf{Factful} & \textbf{Output} \\ \hline \hline
%        No                & No               & Music \\
%        No                & Yes              & Machine Learning \\
%        Yes               & No               & Literature and Religion \\ 
%        Yes               & Yes              & - \\ \hline
%    \end{tabular}
%\end{figure}

% By increasing the number of parameters and expanding the pre-training corpus, Large Language Models (LLMs) now exhibit exceptional fluency in generating text from prompts or instructions \citep{brown2020language}. However, this versatility comes with some drawbacks. When generating text, LLMs rely on two potentially conflicting sources of evidence: the input context and the internal parametric knowledge acquired during pre-training. This conflict can lead to hallucinations, where the generated content includes \textit{unfaithful} information, i.e., information not supported by the input context or that contradicts this context \cite{cad, zhou-etal-2023-context}. \hil{With the ever-increasing costs of fine-tuning models on new data, retrieval-augmented generation (RAG) based models enable updating the model's world knowledge through new retrieved contexts. However, this requires the model to be faithful to the given context. We focus on the latter and aims to improve the faithfulness of the model. Note that we make the distinction between \textit{faithfulness} and \textit{factuality}. While both these notions tackle hallucinations, we purposely put the emphasis on improving faithfulness at the potential expense of factuality. This occurs when the given context contradicts the model's internal knowledge or when pre-training data become obsolete compared to new retrieved data.} 
%
% In this paper, \hil{we focus on two text generation tasks that require leveraging a given context: the typical summarization task and data-to-text.} The latter involves converting structured data—encoded, for example, in the form of tables or knowledge graphs—into natural language. Originating in the 1980s \citep{kukich-d2t, mckeown-d2t}, this task has witnessed a recent surge of interest, motivated by the widespread use of structured data in organizations and the potential of LLMs \citep{unifiedskg, tablellama}. Data-to-text offers an interesting playground for analyzing these conflicting influences, and to develop strategies for removing hallucinations and generating faithful text. It presents unique challenges compared to other natural language processing tasks. First, it is crucial to ensure that the output is strictly based on the input context. For example, when given a table, the description must accurately reflect the information within the table, ensuring all details are directly inferred from it without adding any new content. Second, in comparison to tasks such as summarization or open question answering (QA), the format of data-to-text makes it easier to objectively quantify the extent to which a given model fulfills the task objective, see \Cref{fig:sample}. An additional specificity stems from the structured format of the contextual input which makes it less natural to interpret than plain text for pre-trained LLMs \citep{hallucination_survey, d2t-challenges}.
%
%
% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.6\columnwidth]{images/sample.pdf}
%     \caption{Qualitative example  of \scope compared to the supervised fine-tuned baseline model (\sft) with \textsc{Llama2-7b}, taken from the E2E dataset. \hil{Data-to-text proposes an appealing framework to qualitatively assess the faithfulness of generated samples to the input data.}}
%     \label{fig:sample}
% \end{figure}
%
% Thus the structured information involved in data-to-text and the constraints imposed by the task that require the model to generate fluent sentences while accurately reproducing table information make generalist pre-trained LLMs unsuitable for this task.
% Therefore, the standard approach for data-to-text consists of fine-tuning pre-trained models over data-to-text pairs \cite{hard_d2t}. However, faithfulness issues most often persist even after supervised fine-tuning, with the content in the generated text still diverging from that in the input structures \citep{RebuffelRSSCG22}.
%
%
% Since faithfulness issues might be hard to fully characterize or identify, unsupervised approaches are particularly appealing. An approach consists of making use of a generalist pre-trained LLM and to operate only at the decoding level \citet{pmi,cad,critic-driven}. The logits of the generated tokens are altered by downweighting "ungrounded" candidates. However, we found out that for current LLMs, these methods fail to provide significant improvements if any, over the fine-tuned version of the LLM on data-to-text datasets. In this paper, we argue and demonstrate empirically that only modifying the decoding method is not sufficient to improve over already strong fine-tuned models on data-to-text.
%
% Inspired by the Preference Optimization framework \citep{dpo}, we craft unfaithful samples in a self-supervised manner without relying on any kind of external intervention. The model is then optimized to prefer the reference label over the self-generated unfaithful one. We achieve this by perturbing the model's prediction with an LLM that has no access to the input context, simulating potential hallucinations that arise from the impact of the model's internal knowledge on the generation process.
% To summarize, in this paper:
%
% \begin{itemize}[leftmargin=*] \setlength{\itemsep}{0em}
%     \item We demonstrate that generic faithfulness-enhanced approaches do not significantly improve faithfulness in data-to-text tasks, using recent LLMs such as \textsc{Llama-2-7b} \citep{llama2} and \textsc{Mistral} \citep{mistral}.
%     \item We introduce \scope, a method that leverages ideas from preference training by using a self-supervised, generated preference dataset. In this approach, the model is trained to favor reference labels over carefully crafted noisy samples.
%     \item We empirically show that our approach significantly enhances the faithfulness of text generated by fine-tuned LLMs, surpassing current faithfulness-enhanced methods for data-to-text.
% \end{itemize}
%
% Our experiments reveal that \scope achieves up to a \textit{14\%} improvement in faithfulness metrics over existing methods. Furthermore, evaluations by both GPT-4 and human judges indicate that the generations with \scope are substantially more faithful, with an improved preference win rate against the supervised fine-tuned model that is \textit{5 to 10 times} higher than the baselines.



 % Large Language Models (LLMs) are today the standard approach for generating fluent and coherent completions when given prompts. When they generate these completions, LLMs are also influenced by their internal parametric knowledge acquired during training. Parametric knowledge gives the model broad topic understanding, but it can also cause interference. This occurs when the model blends input information with general knowledge, often resulting in hallucinations in the generated text. Hallucinations areusually characterized in terms of two related but different concepts: factuality and faithfulness.


% Hallucinations can be evaluated using two related but distinct criteria: factuality, which refers to whether the generated information is factually accurate, and faithfulness, which assesses whether the generated content accurately reflects the input prompt.


% In this work, we distingh between \textbf{factuality} and \textbf{faithfulness}. The former refers to whether the information generated by the model is factually accurate or true based on external, real-world knowledge. Factuality is typically evaluated against a reference dataset or established knowledge. Faithfulness, on the other hand, refers to whether the generated content accurately reflects a source information or input context, such as a given document.  A model may generate content that is factually correct but unfaithful if it distorts key details from the input.

% In this work, we focus on \emph{faithfulness hallucinations} and consider text generation conditioned on an input text. Typical  examples of conditional text generation include summarization, translation, data-to-text, conversation or question answering. Faithfulness is a critical issue in many application domains. For example the transcription of medical records in plain text should reflect exactly the record content.
% A major challenge concerning faithfulness hallucinations is their detection and annotation. There is no standard way to determine if a text is faithful to an input context. Supervised approaches such as fine-tuning pre-trained models \cite{hard_d2t} require annotated data, and most often faithfulness issues persist even after supervised fine-tuning, with the content in the generated text still diverging from that in the input structures \citep{RebuffelRSSCG22}. This makes unsupervised methods especially appealing for hallucinations reduction, since they do not require any external intervention.
%  An approach consists of making use of a generalist pre-trained LLM and to operate only at the decoding level \citet{pmi,cad,critic-driven}. The logits of the generated tokens are altered by downweighting "ungrounded" candidates. However, we found out that for current LLMs, these methods fail to provide significant improvements if any, over the fine-tuned version of the LLM on data-to-text datasets. In this paper, we argue and demonstrate empirically that only modifying the decoding method is not sufficient to improve over already strong fine-tuned models on data-to-text.