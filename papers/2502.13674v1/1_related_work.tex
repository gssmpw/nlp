% \paragraph{Data-to-Text.} \citep{kukich-d2t, mckeown-d2t} is the task of converting structured data into fluent text. These structured data may correspond to tables \citep{totto}, meaning representations \citep{e2e}, relational graphs \citep{webnlg2017}, etc.
% %This complex format poses a significant challenge to LLMs pre-trained on plain text. 
% Recent approaches to data-to-text typically involve training end-to-end models with encoder-decoder architectures \citep{wiseman-etal-2017-challenges, gardent2017creating,RebuffelSSG20,RebuffelSSG20-ECIR,RebuffelRSSCG22}. Notably, using large pre-trained encoder-decoder models \citep{t5} has significantly improved performance by framing data-to-text as a text-to-text task \citep{kale-rastogi-2020-text, duong23a}. More recently, large pre-trained decoder-only models \citep{llama2} have shown strong performance and become the de facto approach for text generation, now being applied to data-to-text \citep{tablellama}. Despite these advancements, LLMs still struggle with hallucinations, and data-to-text generation is no exception.
This section reviews methods aimed at improving the faithfulness of LLMs to input contexts. We focus exclusively on approaches designed to ensure the generated content remains grounded in the provided information, excluding techniques related to factuality or external knowledge alignment.

\paragraph{Faithfulness enhancement.} Several methods have been used for improving faithfulness of text summarization. A first line of work consist in using external tools to retrieve key entities or facts form the source document and use these as weak labels during training \citep{zhang-etal-2022-improving-faithfulness}. \citet{faitful-improv} identify key entities using a Question-Answering system and modify the architecture of an encoder-decoder model to put more cross-attention weight on these entities. \citet{zhu-etal-2021-enhancing} propose to improve the faithfulness of summaries by extracting a knowledge graph from the input texts and embed it in the model cross-attention using a graph-transformer. Another line of work focuses on post-training improvements by bootstrapping model-generated outputs ranked by quality \citep{slic,brio,slic-nli}.
% \citet{zhang-etal-2022-improving-faithfulness} forces , \citet{faitful-improv} introduce a Question-Answering system enhanced encoder-decoder architecture, where the cross-attention in the decoder is directed towards key entities. \citet{zhu-etal-2021-enhancing} propose to improve the faithfulness of summaries by extracting a knowledge graph from the input texts and embed it in the model cross-attention using a graph-transformer.
Regarding data-to-text generation, \citet{RebuffelRSSCG22} propose a custom model architecture to reduce the effect of loosely aligned datasets, using token-level annotations and a multi-branch decoder model. The closest work to ours is from \citep{cao-wang-2021-cliff} which proposes a contrastive learning approach where synthetic samples are constructed using different tools like Named Entity Recognition (NER) models and back-translation.
%These approaches have been primarily designed and evaluated for text summarization. 
These approaches address specific forms of unfaithfulness and rely heavily on external tools such as NER or QA models, and are especially tailored for text summarization, while we target a more general focus. More recently, simpler methods that leverage only a pre-trained model have been proposed for summarization. \citet{cad,pmi} downweight the probabilities of tokens that are not grounded in the input context, using an auxiliary LM without access to the input context.
\citet{critic-driven} train a self-supervised classification model to detect hallucinations and guide the decoding process.  \cite{confident-decoding} propose a method to estimate the decoder's confidence by analyzing cross-attention weights, encouraging greater focus on the source during generation. Our method focuses on a decoder-only architecture and uses a single model, providing a streamlined and efficient approach specifically tailored for general conditional text generation tasks without the need for complex external tools.

\paragraph{Faithfulness evaluation.} Measuring faithfulness automatically is not straightforward. Traditional conditional text generation evaluation often relies on comparing the generated output to a reference text, typically measured using n-gram based metrics such as BLEU \citep{papineni-bleu} or ROUGE \citep{lin-2004-rouge}. However, reference-based metrics limitations are well known to correlate poorly with faithfulness \citep{fabbri-etal-2021-summeval,gabriel-etal-2021-go}. Both for summarization and data-to-text generation, new metrics evaluating the generation exclusively against the input context have been proposed, using QA models \citep{rebuffel-etal-2021-data,scialom-etal-2021-questeval} or entity-matching metrics \citep{nan-etal-2021-entity}. In this work, we evaluate primarily our models using recent NLI-related metrics \citep{alignscore, nli-d2t}, and LLM-as-a-judge, focusing on faithfulness \citep{gpt-chiang,gpt-gilardi}. For data-to-text generation, we also report the PARENT metric \citep{parent}, which computes n-gram overlap against elements of the source table cells.

%Additionally, corpora are often collected automatically, leading to divergences between the reference text and the actual input data. , since no direct comparison to the actual input source is actually performed. To address these issues, evaluation methods that take into account the input data have been proposed. \citet{parent} introduce PARENT, which computes the recall of n-gram overlap between the entities in the data and the candidate text. \citet{nli-d2t} develop an entailment metric using Natural Language Inference (NLI) models, where the generated text is compared directly to a simple verbalization of the data. The gold-standard still remains the human or human-like evaluation, conducted with powerful generalist LLMs. These metrics form the core focus of our work.

\paragraph{Preference tuning.} Recent instruction-tuned LLMs are often further refined through "human-feedback alignment" \citep{oaif}. These methods utilize human-crafted preference datasets, consisting of pairs of preferred and dispreferred texts $(\ywin, \ylose)$, typically obtained by collecting human feedback and ranking responses via voting. Recent work \citep{spin} uses the model's previous predictions in a self-play manner to iteratively improve the performance of chat-based models. Whether through an auxiliary preference model \citep{rlhf} or by directly tuning the models on the pairs \citep{dpo}, these approaches have demonstrated remarkable results in chat-based models. Our method leverages a preference framework without the need for human intervention and is specifically tailored for models trained on conditional text generation tasks.

% However, it remains unclear on what values the models are being aligned. Some works have shown that these methods can effectively alter the model's behaviour to the extent that they become useless and refuse to answer to any requests. In this work, we follow a preference fine-tuning scheme but tailored for input-aware tasks like data-to-text.

