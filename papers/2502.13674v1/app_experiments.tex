\section{Experiments}
\label{app:experiments}

\paragraph{Implementation details.} Our code is based on Pytorch \citep{pytorch} and Huggingface \cite{huggingface-transformers}. Experiments were ran on NVidia 80GB A100 GPUs. BLEU is computed using the SacreBLEU \citep{sacrebleu} implementation. For NLI metric we use the model available at \url{https://huggingface.co/cross-encoder/nli-deberta-v3-large}.
% \subsection{Training dynamics}
% We plot on the training dynamics for three values of alpha for E2E, FeTaQa, WebNLG on \Cref{fig:train_alpha_e2e,fig:train_alpha_fetaqa} respectively. We observe the same pattern than the one described in \Cref{sec:alpha-effect}.

% \subsection{Effect of $\alpha$ on BLEU}
% \Cref{fig:bleu_alpha} shows that the fluency grows with $\alpha$, acknowledging the effect observed during training.

\subsection{Baselines}\label{app:baselines}
For each baseline we choose the best hyper-parameters by conducting a grid-search. We initially conducted the search over ranges disclosed in original publications and refined based on our own experiments.

\subsubsection{Context-aware Decoding} \Cref{tab:cad-hyperparameter} shows the best hyperparameters for \cad method. In the original paper, it is recommended to select \( \alpha \) between 0 and 1, with 0.5 being a suitable choice.
\begin{table}[ht]
\centering

\begin{tabular}{lccccccc}
    & \textbf{ToTTo} & \textbf{FeTaQA} & \textbf{WebNLG} & \textbf{E2E} & \textbf{SAMSum} & \textbf{XSum} & \textbf{PubMed} \\
    & $\alpha$ & $\alpha$ & $\alpha$ & $\alpha$ & $\alpha$ & $\alpha$ & $\alpha$ \\
    \midrule
    \textsc{Llama-2-7B} & 0.01 & 0.05 & 0.03 & 0.03 & 0.05 & 0.40 & 0.40 \\
    \textsc{Llama-13B}   & 0.01 & 0.01 & 0.07 & 0.01 & 0.3 & 0.10 & 0.40 \\
    \textsc{Mistral-7B}  & 0.01 & 0.09 & 0.01 & 0.04 & 0.04 & 0.30 & 0.20 \\
    \midrule
\end{tabular}

\caption{Best Context-Aware Decoding (\cad) $\alpha$ hyperparameter.}
\label{tab:cad-hyperparameter}
\end{table}

\subsubsection{Pointwise Mutual Information}
\Cref{tab:pmi-hyperparameter} shows best hyperparameters for \pmi method.
\begin{table}[h!]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lccccccc}
    & \textbf{ToTTo} & \textbf{FeTaQA} & \textbf{WebNLG} & \textbf{E2E} & \textbf{SAMSum} & \textbf{XSum} & \textbf{PubMed} \\
    & $(\lambda, \tau)$ & $(\lambda, \tau)$ & $(\lambda, \tau)$ & $(\lambda, \tau)$ & $(\lambda, \tau)$ & $(\lambda, \tau)$ & $(\lambda, \tau)$ \\
    \midrule
    \textsc{Llama-2-7B} & (0.07, 3.25) & (0.07, 3.25) & (0.05, 3.5) & (0.06, 3.25) & (0.20, 3.25) & (0.15, 3.25) & (0.20, 3.25) \\
    \textsc{Llama-13B}   & (0.07, 3.25) & (0.05, 3.25) & (0.05, 3.25) & (0.05, 3.40) & (0.15, 3.25) & (0.10, 3.75) & (0.15, 3.25) \\
    \textsc{Mistral-7B}  & (0.06, 3.5)  & (0.07, 3.25) & (0.05, 3.25) & (0.09, 3.25) & (0.05, 3.25) & (0.20, 3.25) & (0.15, 3.25) \\
    \midrule
\end{tabular}

}
\caption{Best PMI Decoding (\pmi) $(\lambda, \tau)$ hyperparameters.}
\label{tab:pmi-hyperparameter}
\end{table}


\subsubsection{Critic-driven Decoding}
For the classifier, we replace the original XLM-RoBERTa-base \citep{xlm-roberta} with a stronger DebertaV3-large \citep{deberta} model allowing for much larger contexts, since the linearized data did not fit in the context-window of XLM-RoBERTa-base. In our experiments, we trained a classifier on each dataset using the method \textit{"base with full sentences"} reported to give the highest NLI score on WebNLG dataset in the original publication.
\Cref{tab:critic-hyperparameter} shows the best hyperparameters for the method.

\begin{table}[h!]
\centering
%\resizebox{\columnwidth}{!}{%
\begin{tabular}{lccccccc}
    & \textbf{ToTTo} & \textbf{FeTaQA} & \textbf{WebNLG} & \textbf{E2E} & \textbf{SAMSum} & \textbf{XSum} & \textbf{PubMed} \\
    & $\lambda$ & $\lambda$ & $\lambda$ & $\lambda$ & $\lambda$ & $\lambda$ & $\lambda$ \\
    \midrule
    \textsc{Llama-2-7B} & 0.02 & 0.03 & 0.01 & 0.01 & 0.07 & 0.25 & 0.25 \\
    \textsc{Llama-13B}   & 0.03 & 0.07 & 0.01 & 0.06 & 0.25 & 0.10 & 0.75 \\
    \textsc{Mistral-7B}  & 0.05 & 0.01 & 0.01 & 0.10 & 0.05 & 0.75 & 0.50 \\
    \midrule
\end{tabular}

%}
\caption{Best Critic-driven Decoding (\critic) $\lambda$ hyperparameter.}
\label{tab:critic-hyperparameter}
\end{table}

\subsection{Hyperparameters}

\paragraph{\scope $\alpha$.} Selected value of $\alpha$ for \scope for each dataset are presented in \Cref{tab:scope-hyperparameter}.
\begin{table}[h!]
\centering

\begin{tabular}{lccccccc}
    & \textbf{ToTTo} & \textbf{FeTaQA} & \textbf{WebNLG} & \textbf{E2E} & \textbf{SAMSum} & \textbf{XSum} & \textbf{PubMed} \\
    & $\alpha$ & $\alpha$ & $\alpha$ & $\alpha$ & $\alpha$ & $\alpha$ & $\alpha$ \\
    \midrule
    \textsc{Llama-2-7B} & 0.5 & 0.5 & 0.5 & 0.6 & 0.5 & 0.5 & 0.4 \\
    Llama-2-13B & 0.4 & 0.4 & 0.4 & 0.5 & 0.5 & 0.5 & 0.4 \\
    \textsc{Mistral-7B} & 0.5 & 0.5 & 0.5 & 0.6 & 0.6 & 0.5 & 0.4 \\
    \midrule
\end{tabular}


\caption{Best \scope value of $\alpha$ for \textsc{Llama-2-7b} and \textsc{Mistral-7b} on ToTTo, FeTaQA, WebNLG, and E2E.}
\label{tab:scope-hyperparameter}
\end{table}

\paragraph{Full \sft training.}
\begin{itemize}
    \item \textsc{Llama-2-7b.} The \sft version of \textsc{Llama-2-7b} where fine-tuned using a batch size of 16, a learning rate of $2\times 10^{-5}$, using a linear scheduler with a warm-up ratio of 0.1 on all datasets. The model is optimized with Adam optimizer.
    \item \textsc{Mistral-7b.} We used a batch size of 16, a learning rate of $2\times 10^{-6}$ using a linear scheduler with a warm-up ratio of 0.1 on all datasets. The model is optimized with Adam optimizer.
\end{itemize}

\paragraph{\scope training.}
\begin{itemize}
    \item \textbf{Training }$\bm{p_{\theta_0}}$\textbf{ on }$\bm{\dataset_1}$. For training the fine-tuned version of each model on the split $\dataset_1$, we used the exact same setting than for the full \sft training described above, except that we only performed one epoch for \textsc{Llama-2-7b} and two epochs for \textsc{Mistral-7b}.
    \item \textbf{Preference tuning.} Regarding the hyperparameter of \Cref{eq:dpo}, we set $\beta = 0.1$ for all models and datasets.
\end{itemize}

\subsection{Fine-tuning on half datasets}
\label{app:half_ft}
When fine-tuned on half the samples, we observe experimentally that the models have very close performances to the model fine-tuned on the full train set, see \Cref{tab:sft-05-d2t,tab:sft-05-summ}. The models fine-tuned on half the samples are therefore a strong initialization for the subsequent stages of the method.
\label{app:sft-05}
\input{tables/sft_05}

\subsection{Ablation by varying the dataset proportions used in the first phase of fine-tuning}
Based on the observations in \Cref{app:half_ft}, we chose to use 50\% of the data for the first phase of fine-tuning given the considered datasets and tasks. Here, we present an ablation study on ToTTo. In this study, we fine-tuned a model on 25\% (resp. 75\%) of the dataset and preference-tuned on the remaining 75\% (resp. 25\%) with noisy samples. Results on the validation set, are shown in the table below. On automatic faithfulness metrics (NLI and PARENT), all splits yield comparable results, though a bit higher with a split of 50/50.
\begin{table}[h]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{First phase trained on} &\textbf{NLI} & \textbf{PARENT} \\
\midrule
25\% &  49.57 & 86.08 \\
50\% &  50.64 & 86.34 \\
75\% & 49.07 & 84.10 \\
\bottomrule
\end{tabular}
\caption{NLI and PARENT scores on the validation set of ToTTo when varying the proportion used in the first phase of fine-tuning and using the remaining split for the second phase of preference tuning.}
\label{tab:ablation_ft}
\end{table}


\subsection{Preference loss}
We chose to use DPO \citep{dpo} for its seminal work and its widespread usage.  But our self-supervised framework has no dependency with DPO and should also work with other preference tuning approaches. We tested with ORPO \citep{orpo} and observed very similar results to DPO, see \Cref{tab:orpo-results}.
\label{app:orpo}

\begin{table}[h]
    \centering
    \begin{tabular}{lcc}
        \textbf{Method} & \textbf{NLI} & \textbf{PARENT} \\
        \hline
        \sft & 46.0 & 80.2 \\
        \scope with \textsc{Dpo} loss & 49.9 & 84.2 \\
        \scope with \textsc{Orpo} loss & 49.3 & 85.9 \\
        \hline
    \end{tabular}
    \caption{Results on the validation set of ToTTo with different preference optimization losses applied to \textsc{Llama-2-7b}.}
    \label{tab:orpo-results}
\end{table}


\subsection{Ablation on the value of $\beta$ in preference-tuning stage}
\Cref{tab:beta-totto-xsum} presents faithfulness metrics as we change the value of $\beta$ in the preference-tuning phase of \scope. In the original DPO paper \citep{dpo}, authors use a value $\beta=0.1$ which we found to also work well for \scope.
\begin{table}[h!]
\small
\centering

\begin{tabular}{lcccc}
    & \multicolumn{2}{c}{\textbf{ToTTo}} & \multicolumn{2}{c}{\textbf{XSum}} \\
    \cmidrule(lr){2-3} \cmidrule(lr){4-5}
    \textbf{\(\beta\)} & \textbf{PARENT} & \textbf{NLI} & \textbf{ROUGE-L} & \textbf{AlignScore} \\
    \midrule
    0.05 & 83.54 & 48.31 & 29.51 & 65.16 \\
    0.1  & \textbf{85.39} & \textbf{49.21} & 30.66 & \textbf{65.37} \\
    1    & 81.98 & 46.24 & 33.80 & 59.30 \\
    5    & 81.04 & 45.80 & \textbf{33.84} & 57.45 \\
    \bottomrule
\end{tabular}
\caption{The effect of different \(\beta\) values on performance for ToTTo and XSum tasks.}
\label{tab:beta-totto-xsum}
\end{table}


\subsection{\scope on instruction-tuned models}
\label{app:scope-alpaca}


We intentionally focused on a task-specific setup, targeting use cases where specialized models are most applicable. However, to explore \scope's performance in a general-purpose context, we conducted additional experiments. Specifically, we fine-tuned a Llama-2-7b model on the Alpaca instruction dataset and compared it to a model fine-tuned using the \scope pipeline. Both models were evaluated on our initial tasks, including data-to-text and summarization. As shown in \Cref{tab:alpaca-totto-xsum}, \scope continues to demonstrate consistent gains in faithfulness according to our metrics. However, these improvements are smaller than those observed for domain-specific models, suggesting that \scope is particularly effective in specialized contexts.
\begin{table}[h!]
\small
\centering
\begin{tabular}{lcccc}
    & \multicolumn{2}{c}{\textbf{ToTTo}} & \multicolumn{2}{c}{\textbf{XSum}} \\
    \cmidrule(lr){2-3} \cmidrule(lr){4-5}
    & \textbf{NLI} & \textbf{PARENT} & \textbf{AlignScore} & \textbf{Rouge-L} \\
    \midrule
    \textsc{Model} \\
    \midrule
    \sft   & 35.89 & 66.97 & 84.70 & \textbf{19.46} \\
    \scope & \textbf{37.81} & \textbf{68.69} & \textbf{86.59} & 16.97 \\
    \bottomrule
    \end{tabular}
\caption{On context-intensive tasks, \scope applied to generalist instruction-tuned models improves the faithfulness of the generation.}
\label{tab:alpaca-totto-xsum}


\end{table}

To ensure that these gains in faithfulness do not compromise reasoning capabilities, we benchmarked both models on tasks from the OpenLLM Leaderboard. The results indicate similar overall performance for both models, with \scope outperforming supervised fine-tuning (SFT) on tasks such as TruthfulQA, WinoGrande, and HellaSwag-tasks that require strong context comprehension rather than general knowledge. These results, presented in the appendix, reinforce our contributions. Nonetheless, a more comprehensive exploration of \scope's advantages in broader setups is left for future work.



\begin{table}[ht]
\centering
\small
\begin{tabular}{lccccc|c}
         & \textbf{ARC} & \textbf{HellaSwag} & \textbf{MMLU} & \textbf{TruthfulQA} & \textbf{Winogrande} & \textbf{Avg} \\
\midrule
\sft   & \textbf{47.61}             & 56.50                   & \textbf{41.06}      & 30.72                            & 70.96   & 49.37                 \\
\scope & 47.27                      & \textbf{57.07}           & 39.75               & \textbf{31.95}                   & \textbf{71.98}      & \textbf{49.60}       \\
\bottomrule
\end{tabular}
\caption{Performance comparison between \sft and \scope on various tasks. Scores are percentages, and the best result for each task is highlighted in bold. Metrics: Accuracy is used for ARC, HellaSwag, MMLU, and Winogrande, while BLEU-Acc is used for TruthfulQA to evaluate faithfulness to the reference responses.}
\label{tab:alpaca_sft_vs_scope}
\end{table}

