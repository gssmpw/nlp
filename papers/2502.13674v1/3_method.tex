\section{Method}

% We present here \scope, our method to tackle hallucinations in fine-tuned models. Building uppon previous work highlighting the insufficiency of standard fine-tuning to prevent faithfulness hallucinations \citep{faithfulness-summarization,cao-wang-2021-cliff}. We propose instead a novel two-stages approach to teach the model to be more faithful, going beyond standard fine-tuning. First, the model is initialized through standard fine-tuning. Then the model is further optimized through a preference tuning step guided by synthetic samples.
% Let $\dataset = \{(c_i, y_i)\}_{i=1}^N$ be an aligned dataset made of pairs of contexts and target texts.

We introduce \scope, a novel approach designed to address hallucinations by overcoming the limitations of standard fine-tuning \citep{faithfulness-summarization,cao-wang-2021-cliff}. Unlike traditional methods, our two-stage process aims to enhance the model's faithfulness. In the first stage, we perform standard fine-tuning to initialize the model. In the second stage, we apply preference tuning, where the model is further optimized using synthetic samples that guide it toward generating more faithful outputs. An illustration of the method is presented on \Cref{fig:method}.

\subsection{Training phase}
Let $\dataset = {(c_i, y_i)}_{i=1}^N$ be an aligned dataset of context-target pairs used for training.
\paragraph{Fine-tuning.} For the first stage, our goal is to get an initial version of a fine-tuned model. We start from a pre-trained model $\plm$. To better leverage training samples, we propose for this part to train $\plm$ only on the first half $\dataset_1$ of the samples of \dataset. We keep the second part $\dataset_2$ of the dataset for the next step. %Let $\dataset_1$ and $\dataset_2$ be the first and second half of $\dataset$ respectively.
We denote by $p_{\theta_0}$ the model fine-tuned from $\plm$ on $\dataset_1$ using cross-entropy. Given the strong sample efficiency of recent LLMs, we empirically found that for the datasets used, fine-tuning on only half of the samples was sufficient to achieve a strong initialization for the subsequent stage, see \Cref{app:sft-05}.


\begin{figure}[t]
     \centering
     \includegraphics[width=0.6\columnwidth]{images/method_horizontal.pdf}
     \caption{\scope training framework. A pre-trained model $\plm$ is first fine-tuned on a subset $\dataset_1$ of $\dataset$  and produces a model $p_{\theta_0}$. A mixture of $\plm$ and $p_{\theta_0}$ is then used to generate a synthetic preference dataset, which finally serves for preference fine-tuning.}
     \label{fig:method}
\end{figure}

\paragraph{Preference-tuning.} The second phase involves contrastive learning. Training will be conducted on $\dataset_2$, the second half of the samples.
We augment $\dataset_2$ with artificial unfaithful samples to get a dataset $\dataset_2 = \{c, y_i, y_i^-)\}_{i=1}^N$. Our complete process to generate these samples is described in \Cref{sec:noisy-generation}. For each annotated target $y$, we have a corresponding noisy $\ylose$ which contains unfaithful patterns.

While other baselines propose to use a custom contrastive loss often based on embeddings similarity, we propose optimizing the model to prefer $y$ over $\ylose$ by leveraging the recent framework of preference tuning \citep{dpo}, with the following loss:
\begin{equation}
    \mathcal{L}_\theta = -\mathbb{E}_{(c, y, \ylose) \sim \dataset_2} \bigg[\log \sigma\bigg(\beta \log \frac{p_\theta(y \mid c)}{p_{\theta_0}(y \mid c)}
    -\beta \log \frac{p_\theta(\ylose \mid c)}{p_{\theta_0}(\ylose \mid c)}\bigg)\bigg],
    \label{eq:dpo}
\end{equation}
where $\sigma$ is the sigmoid function and $\beta$ is a scalar hyperparameter that quantifies how much $p_\theta$ deviates from $p_{\theta_0}$. 
Intuitively, minimizing $\mathcal{L}_\theta$ w.r.t. $\theta$ amounts to increasing the gap between the likelihood of generating grounded responses $y$ and non-grounded ones $\ylose$. More details about the training dynamics can be found in \Cref{sec:alpha-effect}. Additionally, we experimented with an alternative preference loss in \Cref{app:orpo} and observed similar behavior.
% \subsection{Training algorithm}
% \label{trainingalgo}

% Let  $\widetilde{\dataset}_2$ be the dataset of triples $(c, \ywin, \ylose)$ obtained from the noisy generation procedure described before, with $\ywin$ the corresponding target and $\ylose$ the noisy generated sample. We optimize the model $\pmodel$ to prefer $\ywin$ over the noisy $\ylose$ using DPO \citep{dpo} through the following loss:
% \begin{equation}
%     \mathcal{L}_\theta = -\mathbb{E}_{(c, \ywin, \ylose) \sim \widetilde{\dataset}_2} \bigg[\log \sigma\bigg(\beta \log \frac{p_\theta(\ywin \mid c)}{\psft^1(\ywin \mid c)}
%     -\beta \log \frac{p_\theta(\ylose \mid c)}{\psft^1(\ylose \mid c)}\bigg)\bigg],
%     \label{eq:dpo}
% \end{equation}
% where $\sigma$ is the sigmoid function and $\beta$ is a scalar hyperparameter that quantifies how much $p_\theta$ deviates from $\psft^1$. 
% Intuitively, minimizing $\mathcal{L}_\theta$ w.r.t. $\theta$ amounts to increasing the gap between the likelihood of generating grounded responses $\ywin$ and non-grounded ones $\ylose$. More details about the training dynamics can be found in \Cref{sec:alpha-effect}.


% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.4\columnwidth]{images/method.pdf}
%     \caption{Our proposed method. A pre-trained model $\plm$ is first fine-tuned on a subset $\dataset_1$ of $\dataset$  and produces a model $\psft^1$ (step 1). A mixture of $\plm$ and $\psft^1$ is used to generate a synthetic preference dataset (step 2), which serves for preference fine-tuning (step 3).}
%     \label{fig:method}
% \end{figure}



% This section presents our method, \scope. We explain how we generate unfaithful samples and how we leverage them to improve the model's ability to produce contextually grounded text.

% \paragraph{Notations.}
%  Let $\dataset = \{(c_i, y_i)\}_{i=1}^N$ be a dataset of input structured contexts (tables, knowledge graphs, etc.) and their associated texts. $\plm$ is a pre-trained language model. $\psft^1$ is a supervised fine-tuned version of $\plm$, trained on a subset $\dataset_1$ of $\dataset$. $\pmodel$ denotes the faithfulness-enhanced model to be trained by our algorithm, initialized at $\psft^1$. $\pdata$ is the true underlying distribution of the data.

%  \paragraph{Intuition.}
% Fine-tuning with a maximum likelihood estimation (MLE) objective approximates $\pdata$. However, the resulting model still generates unfaithful content: it produces samples $\yhat \sim \pmodel(\cdot \given c)$, with some tokens not grounded in the context $c$. The generated content may, for example, include information not present in the context or that contradicts the context.
% To address this issue, we propose enhancing a pre-trained model using a preference framework that exploits a dataset that is generated automatically. The method proceeds in two steps. First, we generate a preference dataset of triples $\{(c, \ywin, \ylose)\}$ where $c$ is a structured input context (e.g., a table), $\ywin$ is an associated transcription from the training set and $\ylose$ is an automatically generated unfaithful transcription of $c$. We then guide the model $\pmodel$ to prefer generating faithful samples $\ywin$ from $\dataset$ over the unfaithful ones $\ylose$.




\subsection{Unfaithful dataset generation}
\label{sec:noisy-generation}

In this section, we present our method to generate unfaithful samples. Contrarily to other methods that rely on external tool, such a named entity recognition or number entity recognition, we propose an easier and more general method.
When a LLM generates ungrounded spans of text, it is often caused by an interference between the context and the learned statistical patterns acquired during training. 
A \emph{convincing unfaithful sample} generated by a LLM should satisfy at least two desiderata: \textbf{(i)} attain the same level of fluency than the target LLM, and \textbf{(ii)} being more or less consistent with the input while containing one or several spans of text not grounded in the input context.
An ideal method would be to run our initial fine-tuned model $p_{\theta_0}$ and find among the samples the ones that are unfaithful.  But as discussed in \Cref{sec:intro}, accurately detecting unfaithful samples automatically is a difficult problem. Instead, we propose a simple unsupervised method to simulate the creation of noisy samples. Our strategy is to "force" the model to leak its internal statistical knowledge in the generation by adopting a noisy decoding method using two models simultaneously.
%//  But given the lack of trustworthy unfaithful samples classifier, we have to simulate the creation of noisy samples. Instead, we propose to "force" the model to leak its internal statistical knowledge in the generation by adopting a noisy decoding method using two models simultaneously.
\begin{itemize}[leftmargin=*]
    \item The main model is $p_{\theta_0}$, the initially fine-tuned model on half the dataset. This model generates samples conditionally to the input context, $y \sim p_{\theta_0}(\cdot \given c)$. It is supposed to generate text that is grounded in the input context, but can still contain inaccuracies due to its shortened training.
    \item The second model is $\plm$, the pre-trained counterpart of $p_{\theta_0}$. This model won't be given access to the input context and will simply sample from its context-free distribution $y \sim \plm(\cdot)$, generating general patterns that it has learned.
\end{itemize}

Both distributions are \emph{de facto} fluent, but used individually might not be enough to teach anything during preference tuning. $\plm$ samples will obviously not be challenging enough, while $p_{\theta_0}$ samples won't contain enough hallucination patterns.
Instead we propose to combine both during the decoding process. 
 We generate these noisy samples token by token by sampling mainly from the grounded $p_{\theta_0}(\cdot \given c)$ and randomly from the non-grounded $\plm$. This method introduces fluent but non-grounded artifacts, exhibiting both intrinsic errors, i.e., generated outputs that contradict the data, and extrinsic hallucinations, i.e., generated outputs that cannot be inferred from the data alone (see \Cref{tab:noisy_s}). Refer to \Cref{alg:preferencedatasetgen} for the complete details of the algorithm.




% Let $(c, y)$ be an aligned data-to-text pair from $\dataset$. In the following, we denote by $\ywin$ and $\ylose$ the target $y$ and a noisy version of $y$, respectively. To generate the noisy dataset, we proceed as follows:

% \paragraph{1. Split.} For fair comparison with the baselines in terms of the number of samples seen, we split our dataset into two equal parts: $\dataset_1$ and $\dataset_2$. $\dataset_1$ is used to fine-tune the pre-trained model, $\plm$ for the data-to-text task resulting in the fine-tuned model $\psft^1$. $\dataset_2$ is  used to generate a preference dataset  $\widetilde{\dataset}_2$ and for subsequent training (see \Cref{trainingalgo}).

% \paragraph{2. Initial fine-tuning.} We first fine-tune a pre-trained model $\plm$ on $\dataset_1$ to get $\psft^1$. Given the strong capabilities of recent LLMs, when fine-tuned on one half of the data, $\psft^1$ has converged to a good approximation of $\pdata$.


% \paragraph{3. Noisy generation.} We construct the noisy unfaithful dataset as a mixture of $\psft^1(\cdot \given c)$ and $\plm(\cdot)$, using context data $c$ from $\dataset_2$.  We generate these noisy samples token by token by sampling either from the grounded $\psft^1(\cdot \given c)$ or from the non-grounded $\plm(\cdot)$, as described in \Cref{alg:preferencedatasetgen}. This method introduces fluent but non-grounded artifacts, exhibiting both intrinsic errors, i.e., generated outputs that contradict the data, and extrinsic hallucinations, i.e., generated outputs that cannot be inferred from the data alone (see \Cref{tab:noisy-samples}).

\begin{algorithm}[ht]
\small
\caption{$\mathtt{noisy\_generation}(c, p_\mathrm{LM}, p_{\theta_0})$}
\label{alg:preferencedatasetgen}
\SetKwInOut{Input}{Input}
\Input{$c$ an input context, $\plm$ the pre-trained model, $p_{\theta_0}$ the fine-tuned model on $\dataset_1$.}

\For{token decoding step $t > 0$}{

\begin{enumerate}[leftmargin=*, rightmargin=1.5em]\setlength{\itemsep}{0em}
    \item Sample: $\alpha_t \sim \text{Bernoulli}(\alpha)$ ($\alpha_t \in \{0, 1\}$).
    \item Sample:
    \begin{equation}
    \ylose_t \sim \ (1 - \alpha_t) p_{\theta_0}(\cdot \given \ylose_{<t}, c) +\alpha_t\plm(\cdot \given \ylose_{<t})
    \label{eq:noise}
    \end{equation}

\end{enumerate}}
\BlankLine
\Return $\ylose$;
\end{algorithm}



The mixture is parameterized by $\alpha$, which tunes the noise level within the samples. $\alpha = 0$ corresponds to the fine-tuned model $p_{\theta_0}$ and $\alpha = 1$ corresponds to the unconditional model $\plm$.  
This parameter actually plays an important role:  the noisy $\ylose$ should contain divergences from the context but still be close enough to the true $y$ to provide a meaningful learning signal. This is a sensible step for preference learning, as illustrated later in the experiments (\Cref{sec:alpha-effect}).


Our detailed pipeline is described in \Cref{alg:method}. 
Existing preference tuning methods usually depend on offline preference data gathered from various sources and ranked through voting. In contrast, the originality of our approach lies in its ability to automatically generate unfaithful responses, simulating potential hallucinations from the model's internal state \emph{without requiring supervision}. This distinguishes it from traditional preference training, which typically involves human intervention.
% Existing preference tuning methods typically rely on offline preference data collected from various sources and ranked via voting. In contrast, \hil{the originality of our approach lies in its ability to automatically generate unfaithful responses, simulating potential hallucinations from the model's internal state without the need for supervision. This differs from traditional preference training, which typically relies on human intervention}.


% \begin{algorithm}[ht]
% \caption{\scope (Self-supervised Context Preference).}
% \label{alg:method}
% \SetKwInOut{Input}{Input}
% \Input{$\dataset$ the training data and $p_\mathrm{LM}$ the pre-trained model.}
% \BlankLine
% \tcp{1. Split the train data}
% $\dataset_1, \dataset_2 \leftarrow$ Split $\dataset$ into two halves

% \BlankLine
% \tcp{2. Initial fine-tuning}
% $\psft^1 \leftarrow$ Fine-tune $p_\mathrm{LM}$ on $\dataset_1$

% \BlankLine
% \tcp{3. Noisy generation}
% $\widetilde{\dataset}_2 \leftarrow \{\}$
% \BlankLine
% \For{$(c, \ywin)$ in $\dataset_2$}{
%     $\ylose \leftarrow \mathtt{noisy\_generation}(c, p_\mathrm{LM}, \psft^1)$
%     \BlankLine
%     Append $(c, \ywin, \ylose)$ to $\widetilde{\dataset}_2$
% }


% \BlankLine
% \tcp{4. Preference fine-tuning by optimizing \Cref{eq:dpo}}

% $p_\theta  \leftarrow$ Preference fine-tune $\psft^1$ over $\widetilde{\dataset}_2$, using $\ywin$ as the preferred label and $\ylose$ as the negative example

% \BlankLine
% \Return $p_\theta$;
% \end{algorithm}



\begin{algorithm}[ht]
\small
\caption{\scope (Self-supervised Context Preference).}
\label{alg:method}
\SetKwInOut{Input}{Input}
\Input{$\dataset$ the training data and $\plm$ the pre-trained model.}
\BlankLine
\tcp{ Split the train data}
$\dataset_1, \dataset_2 \leftarrow$ Split $\dataset$ into two halves

\BlankLine
\tcp{1. Initial fine-tuning}
$p_{\theta_0} \leftarrow$ Fine-tune $p_\mathrm{LM}$ on $\dataset_1$

\BlankLine
\tcp{2. Noisy generation}
$\widetilde{\dataset}_2 \leftarrow \{\}$
\BlankLine
\For{$(c, y)$ in $\dataset_2$}{
    $\ylose \leftarrow \mathtt{noisy\_generation}(c,\plm, p_{\theta_0})$
    \BlankLine
    Append $(c, y, \ylose)$ to $\widetilde{\dataset}_2$
}


\BlankLine
\tcp{3. Preference fine-tuning by optimizing \Cref{eq:dpo}}

$p_\theta  \leftarrow$ Preference fine-tune $p_{\theta_0}$ over $\widetilde{\dataset}_2$, using $y$ as the preferred label and $\ylose$ as the negative example

\BlankLine
\Return $p_\theta$;
\end{algorithm}
