\section{Related Work}
\label{sec:related_work}
\myparagraph{Moment Retrieval.} 
The task of video moment retrieval was first introduced in**Zhou, "Unsupervised Learning for Visual Question Answering"**, with the aim of expanding the action localisation task**Lin et al., "Action Recognition using a Deep Neural Network"** to the open vocabulary setting. Moment retrieval methods require cross-domain interactions between video and text in order to determine the correspondences between them. Approaches have historically been divided largely between proposal-based**Lin, "Proposal-Free Moment Retrieval by Predicting Relevant Moments"** and proposal-free methods**Liu et al., "Proposal-Free Moment Retrieval via Graph Neural Networks"**.

Recent works have been designed to jointly perform moment retrieval and highlight detection**Li et al., "Moment Retrieval with Highlight Detection"**, being trained with both \textbf{moment start/end times} and \textbf{saliency scores} which are the ground truths for the highlight detection task. Certain datasets such as QVHighlights**Zhu et al., "QVHighlights: A Dataset of High-Quality Moment Annotations for Video Highlight Detection"** contain both human-annotated moments and saliency scores. Datasets which do not have human-annotated saliency scores instead use pseudo-saliency scores from the moment start/end times, with a non-zero score within ground truth moments.

\myparagraph{Moment Retrieval Methods.}
Moment-DETR**Wang et al., "Moment-DETR: A Transformer-based Framework for Moment Retrieval"** is one such method that jointly trains on highlight detection and has served as a base for many recent moment retrieval methods**Kim et al., "Improving Moment Retrieval with Moment-DETR"**. %Many of them focus on altering the base Moment-DETR methodology to produce improved results.
This approach takes inspiration from DEtection TRansformer (DETR)**Carion et al., "End-to-end Object Detection with Transformers"** methods in object detection, viewing moment retrieval as a direct set prediction problem, generating moment candidates and associated scores with which to rank them in an end-to-end manner. It concatenates text and video features as input and passes them through a transformer encoder-decoder. Trainable positional embeddings known as \textit{moment queries} are inputted to the decoder to generate candidate moment predictions. It has separate prediction heads for the moment span; the score of each proposed moment; and the saliency scores. While Moment-DETR uses foreground/background labels during training to supervise the moment score predictions, these are not designed to be used during evaluation for determining positive/negative queries.
Some approaches based on this framework have included the addition of extra prior information to initialise the queries**Gong et al., "Initializing Queries with Extra Prior Information"** or the addition of an extra modality**Kim et al., "Adding an Extra Modality to Moment-DETR"**. Others have focused on improving the video-text interactions in the model**Wang et al., "Improving Video-Text Interactions in Moment-DETR"**.%We will describe a subset of these models in more detail.

UniVTG**Li et al., "UniVTG: A Unified Framework for Three Video Tasks"** jointly trains for three tasks: moment retrieval, highlight detection, and video summarisation from datasets which do not contain a training signal for all three. It utilises a large pre-training scheme and alters the Moment-DETR architecture by removing the decoder and estimating an indicator score and predicted span for every feature clip in the video. This replaces the moment queries used within Moment-DETR.  
Furthermore, saliency scores are produced purely through feed-forward layers and attentive pooling over text features, without being passed through the transformer encoder.

QD-DETR**Zhu et al., "QD-DETR: A Framework for Moment Retrieval with Cross-Attention"** alters the encoder to contain cross-attention layers to ensure that the visual features are being properly attended to by the text features. It makes use of negative queries during training, shuffling video-sentence pairs across the dataset. This forces the saliency score predictions to be more indicative of the relevance of the sentence to the video clips.
However, saliency score predictions are still not used for the video moment retrieval task.

CG-DETR**Kim et al., "CG-DETR: A Framework for Moment Retrieval with Clip-Word Correlation"** employs a clip-word correlation learner and uses dummy tokens concatenated with the text query tokens. These dummy tokens are designed to attend to sections of the video that are not represented by the query, thus essentially representing the query-excluded meaning. This helps to prevent irrelevant video clips from being represented by the text query.

Despite progress in moment retrieval performance, \emph{all methods assume queries at inference are always positive}. 
Models are not evaluated with an input text query that is irrelevant to the video. 
Additionally, models are not equipped to handle such negative queries.

\myparagraph{Video Corpus Moment Retrieval.}
Video Corpus Moment Retrieval (VCMR)**Wang et al., "Video Corpus Moment Retrieval"** is another related task. This expands the search from a single video to a corpus of videos. While this returns only a single video segment from a set of videos, essentially serving as a rejection of the other videos, VCMR works under the assumption that the query is present in exactly one video in the corpus. There is no active scheme to determine negatives. Currently, VCMR methods cannot determine if a query is \emph{not} present in any videos in the corpus, similar to the current state of video moment retrieval. Therefore this task formulation also does not allow for negative rejection in video moment retrieval.

\myparagraph{Negative Rejection in Other Tasks.} 
Modelling uncertainty of predictions, and discarding decisions with low certainty has been integrated into classification tasks**Mittal et al., "Uncertainty-based Classification"**.
Our approach differs from these because the model is actively predicting whether the query is present in the video, rather than stating that it is unsure about the prediction.

Recently, the topic of negative rejection has been highlighted**Xu et al., "Negative Rejection in LLMs with Retrieval Augmented Generation"** in LLMs with Retrieval Augmented Generation, whereby information is extracted from retrieved documents to aid in responding to input queries. Negative rejection is crucial in cases where the required information is not present in the retrieved documents, as otherwise the hallucination of incorrect information can occur. It has been shown that currently these LLMs are not robust to negative rejection**Wang et al., "Robustness of LLMs with Retrieval Augmented Generation"**. Our findings for the video moment retrieval task parallels this study, where current models are hallucinating moments in videos when negative queries are provided. Just as it will be important to deal with this issue in LLMs in order to improve reliability, it will also be important to achieve negative rejection in video moment retrieval.