\section{Related Work}
\label{sec:relwork}

Several surveys have explored EEG-based brain-computer interfaces (BCIs) and deep learning techniques for neural decoding. ____ provides a broad overview of EEG-based BCI applications, discussing signal processing methodologies and traditional machine learning techniques but lacks an in-depth analysis of generative models. ____ and ____ review deep learning techniques, particularly CNNs and RNNs, for EEG classification, emotion recognition, and cognitive state monitoring, yet they focus more on supervised learning approaches rather than self-supervised and generative modeling. More recently, ____ examined EEG-to-text decoding but primarily covered classification-based approaches without exploring the role of large language models (LLMs) or multimodal integration with vision-based EEG applications. Additionally, ____ discusses self-supervised learning (SSL) techniques for EEG but does not emphasize their applicability to generative tasks, such as EEG-to-image or EEG-to-text generation. Our survey specifically examines the intersection of EEG and generative AI, focusing on recent approches for EEG-to-text and EEG-to-image translation. While previous studies focus on EEG feature extraction and classification, our work highlights how self-supervised learning, contrastive learning, and multimodal alignment improve EEG-based generation.

\input{dataset_table}


\tikzstyle{my-box1}=[
    rectangle,
    draw=gray,
    rounded corners,
    text opacity=1,
    minimum height=1.5em,
    minimum width=5em,
    inner sep=5pt,
    align=center,
    fill=green!10,
    fill opacity=.5,
    font=\small,
]

\tikzstyle{my-box2}=[
    rectangle,
    draw=gray,
    rounded corners,
    text opacity=1,
    minimum height=1.5em,
    minimum width=5em,
    inner sep=5pt,
    align=center,
    fill=orange!10,
    fill opacity=.5,
    font=\small,
]
\tikzstyle{my-box3}=[
    rectangle,
    draw=gray,
    rounded corners,
    text opacity=1,
    minimum height=1.5em,
    minimum width=5em,
    inner sep=5pt,
    align=center,
    fill=blue!10,
    fill opacity=.5,
    font=\small,
]
    
\begin{figure*}[t]
    \centering
    \resizebox{\textwidth}{!}{
        \begin{tikzpicture}[node distance=0.2cm]
            \node[my-box1] (box1) at (0,0) {
                \arrayrulecolor{lightgray}
                \begin{tabular}{|m{0.15\linewidth}|m{1.3\linewidth}|} \hline \textbf{EEG-Text}&\textbf{Studies and main use cases}\\ \hline CNNs & • Generate medical reports ____ • Translate user's active intent to text represented by morse code ____ • Folded ensemble technique to minimize the computational complexity and solve all the class imbalance issues to enhance the accuracy of text generation ____\\ \hline LSTMs&• Translate user's active intent to text represented by morse code ____ •  Folded ensemble technique to minimize the computational complexity and solve all the class imbalance issues to enhance the accuracy of text generation ____\\ \hline 
     LLMs& • EEG-to-text seq-to-seq decoding and zero-shot sentence sentiment classification on natural reading tasks ____ • Improving accuracy of open-vocabulary EEG-to-text decoding ____ • Bridge the semantic gap between EEG and Text ____ • Open vocabulary EEG decoding incorporating a subject-dependent representation learning module ____ • Ensuring cross-modal semantic consistency between EEG and Text ____ • Capture global and local contextual information and long-term dependencies • ____ • Use visual stimuli rather than text circumvent the complexities of language processing ____\\\hline 
     
     Transformer&• EEG-to-text seq-to-seq decoding and zero-shot sentence sentiment classification on natural reading tasks ____ • Open vocabulary EEG decoding incorporating a subject-dependent representation learning module ____ • Ensuring cross-modal semantic consistency between EEG and Text ____ • Improving accuracy of open-vocabulary EEG-to-text decoding ____ • Bridge the semantic gap between EEG and Text using LLMs ____ • Recalibrates subject-dependent EEG representation to the semantic-dependent EEG representation ____ • Open vocabulary EEG-to-Text translation tasks  with or without word -level markers ____ • Decoding EEG Speech Perception with Transformers and VAE-based Data Augmentation ____\\\hline
     
     Gated Recurrent Units& • Open vocabulary EEG decoding incorporating a subject-dependent representation learning module ____ • Capture global and local contextual information and long-term dependencies. 
 ____ \\\hline
     Recurrent Neural Networks& • Translate active intention into text format based on Morse code ____ \\\hline
     Attention Mechanism& • Generate medical reports ____ • Capture global and local contextual information and long-term dependencies ____ \\ \hline 
     Contrastive Learning&  • Recalibrates subject-dependent EEG representation to the semantic-dependent EEG representation ____  • Ensuring cross-modal semantic consistency between EEG and Text ____  • Bridge the semantic gap between EEG and Text using LLMs ____ \\ \hline 
     Masked Signal Modeling&• Improving accuracy of open-vocabulary EEG-to-text decoding ____ • Ensuring cross-modal semantic consistency between EEG and Text ____ • Bridge the semantic gap between EEG and Text using LLMs ____ \\ \hline \end{tabular}
            };
            
            \node[my-box2, below=of box1] (box2) {
                \arrayrulecolor{lightgray}
                \begin{tabular}{|m{0.15\linewidth}|m{1.3\linewidth}|} \hline \textbf{EEG-Image}&\textbf{Studies and main use cases}\\ \hline CNNs & • Semi-supervised cross-modal image generation ____ • Visual Saliency and Image Reconstruction from EEG Signals ____ • Map EEG signals to the visual saliency maps corresponding to each image ____ • Demonstrate the generalizability of feature extraction pipeline across three different datasets ____ • Visual Decoding and Reconstruction via EEG Embeddings with Guided Diffusion ____ • Reconstructing images using the same semantics as the corresponding EEG ____ • Zero-shot framework to project neural signals from different sources into the shared subspace ____ \\ \hline 
                
                LSTMs& • Extracting visual class discriminative information from EEG data ____ • Framework for synthesizing the images using small-size EEG datasets ____ • Image reconstruction using generative adversarial and deep fuzzy neural network ____ • Demonstrate the generalizability of feature extraction pipeline across three different datasets ____ \\\hline
                
     Generative Adversarial Networks (GANs)& • Extracting visual class discriminative information from EEG data ____ • Map EEG signals to the visual saliency maps corresponding to each image ____ ____ • Generates images along with producing class-specific EEG encoding as a latent representation ____  • Visual Decoding and Reconstruction via EEG Embeddings with Guided Diffusion ____ • Framework for synthesizing the images using small-size EEG datasets ____ • Image reconstruction using generative adversarial and deep fuzzy neural network ____ \\\hline
     
      Variational Autoencoder& • Generating high-quality images directly from brain EEG signals, without the need to translate thoughts into text ____ • Photorealistic Reconstruction of Visual Texture From EEG Signals ____ • Extracting visual class discriminative information from EEG data ____\\\hline
      
     Diffusion Models& • Zero-shot framework to project neural signals from different sources into the shared subspace ____ • Generating high-quality images directly from brain EEG signals, without the need to translate thoughts into text ____ • Reconstructing images using the same semantics as the corresponding EEG ____ • Multi-level perceptual information decoding to draw multi grained outputs from given EEG ____ \\\hline
     
     Attention Mechanism& • Generates images along with producing class-specific EEG encoding as a latent representation ____ • Self-supervised framework to decode natural images for object recognition ____ • Visual Decoding and Reconstruction via EEG Embeddings with Guided Diffusion ____ \\\hline
     
     Contrastive Learning& • Framework for synthesizing the images using small-size EEG datasets ____ • Multi-level perceptual information decoding to draw multi grained outputs from given EEG ____ • Self-supervised framework to decode natural images for object recognition ____ • Generating perceptual and cognitive contents using EEG data ____ • Self-supervised framework to decode natural images for object recognition ____ \\\hline
     
     Masked Signal Modeling& • Generating high-quality images directly from brain EEG signals, without the need to translate thoughts into text ____ \\\hline
     \end{tabular}
            };



            
            \node[my-box3, below=of box2] (box3) {
                \arrayrulecolor{lightgray}
                \begin{tabular}{|m{0.15\linewidth}|m{1.3\linewidth}|} \hline \textbf{EEG-Audio/Speech/Music}  &\textbf{Studies and main use cases}\\ \hline 
                CNNs & • Show how acoustic features are related to EEG signals recorded during speech perception and production ____ • Reconstructing music stimuli to be perceived and identified independently 
 ____ \\\hline
                
     Transfomer& • Investigate the potential to reconstruct speech from EEG signals, including the corresponding speaker's characteristics ____ • Utilizing deep features from EEG data for emotional music composition ____\\\hline
     
     Gated Recurrent Units& • Show how acoustic features are related to EEG signals recorded during speech perception and production ____ • Convert EEG of imagined speech into user's own voice ____ • EEG-based Talking-face Generation ____ \\\hline
     
     Generative Adversarial Networks (GANs)& • Convert EEG of imagined speech into user's own voice ____ • EEG-based Talking-face Generation ____ \\\hline
     
     Automatic Speech Recognition&• Convert EEG of imagined speech into user's own voice ____ • EEG-based Talking-face Generation ____\\\hline
     
     Latent Diffusion Models&• Reconstructing naturalistic music from EEG without need for manual pre-processing and channel selection ____\\\hline
     
     Attention Module& • Show how acoustic features are related to EEG signals recorded during speech perception and production ____ \\\hline \end{tabular}
            };

            
    \end{tikzpicture}
    }
    \caption{Techniques and References of Surveyed Studies for EEG to Text, Image and Beyond}
    \label{fig:techniques}
\end{figure*}


% \begin{table*}[t]
% \centering
% \tiny % smaller font size
% \renewcommand{\arraystretch}{0.8} % reduce row height
% \begin{tabular}{|l|p{3cm}|p{5cm}|p{6cm}|}  % Added new column for description
% \hline
% \multicolumn{4}{|c|}{\textbf{Evaluation Metrics for Text, Image, and Sound/Speech Tasks}} \\ \hline
% \textbf{Category}     & \textbf{Metric}                            & \textbf{Studies}                                                                                                   & \textbf{Description}  \\ \hline
% \multirow{11}{*}{Text} & BLEU                                      & biswal2019eegtotext, wang2022open, feng2023aligning, duan2023dewave, liu2024eeg2text, wang2024enhancing, amrani2024deep, chen2025decoding & Measures the similarity between the generated text and a reference text.\\ \cline{2-4} 
%                        & ROUGE                                     & wang2022open, feng2023aligning, duan2023dewave, wang2024enhancing, amrani2024deep, chen2025decoding, mishra2024thought2text & Measures recall and overlap between generated and reference text.\\ \cline{2-4} 
%                        & METEOR                                    & biswal2019eegtotext, mishra2024thought2text, chen2025decoding                                                     & Similar to BLEU but incorporates synonyms and stemming. \\ \cline{2-4} 
%                        & CIDEr                                     & biswal2019eegtotext                                                                                                & Focuses on evaluating the quality of image captions.\\ \cline{2-4} 
%                        & Inception Score                           & kavasidis2017brain2image, li2020semi, singh2023eeg2image, singh2024learning                                      & Measures the quality of generated images by comparing their classification probability.\\ \cline{2-4} 
%                        & F1-Score                                  & wang2022open, zeng2023dm, ahmadieh2024visual                                                                     & Measures the balance between precision and recall.\\ \cline{2-4} 
%                        & Precision                                 & ahmadieh2024visual                                                                                               & Measures the accuracy of positive predictions.\\ \cline{2-4} 
%                        & Recall                                    & ahmadieh2024visual                                                                                               & Measures the proportion of actual positives that were correctly identified.\\ \cline{2-4} 
%                        & Accuracy                                  & zeng2023dm, ahmadieh2024visual, lee2023towards, jiang2024eeg, mizuno2024investigation, ramirez2022eeg2mel        & Measures the overall correctness of the predictions.\\ \cline{2-4} 
%                        & WER (Word Error Rate)                     & mizuno2024investigation                                                                                          & Measures the error in speech-to-text tasks.\\ \cline{2-4} 
%                        & BERTScore                                 & amrani2024deep, mizuno2024investigation                                                                          & Measures the contextual similarity between generated and reference text.\\ \hline
% \multirow{5}{*}{Image} & SSIM (Structural Similarity Index)         & khaleghi2022visual, shimizu2022improving, zeng2023dm, sugimoto2024image, park2024towards                         & Assesses the perceptual quality and structural similarity between images.\\ \cline{2-4} 
%                        & Inception Score                           & kavasidis2017brain2image, li2020semi, singh2023eeg2image, singh2024learning                                      & Assesses the quality of generated images via a pre-trained classifier.\\ \cline{2-4} 
%                        & PSNR (Peak Signal-to-Noise Ratio)         & ramirez2022eeg2mel                                                                                              & Measures the quality of the reconstruction by comparing the signal-to-noise ratio.\\ \cline{2-4} 
%                        & SSI (Structural Similarity Index)         & ramirez2022eeg2mel                                                                                              & Evaluates the structural similarity of images in a perceptual sense.\\ \cline{2-4} 
%                        & Cosine Similarity                         & ikegawa2024text                                                                                                  & Measures the similarity of two vectors using cosine distance.\\ \hline
% \multirow{7}{*}{Sound/Speech} & Mel Cepstral Distortion (MCD)        & krishna2021advancing, park2024towards                                                                            & Measures the difference between the mel-cepstral coefficients of two audio signals.\\ \cline{2-4} 
%                        & RMSE (Root Mean Square Error)            & krishna2021advancing, park2024towards, lee2023towards                                                           & Measures the error between predicted and actual values.\\ \cline{2-4} 
%                        & FAD (Frechet Audio Distance)             & postolache2024naturalistic                                                                                        & Quantifies the quality of generated audio by comparing distributions.\\ \cline{2-4} 
%                        & CER (Character Error Rate)               & lee2023towards, mizuno2024investigation                                                                           & Measures the error rate in character-based speech-to-text tasks.\\ \cline{2-4} 
%                        & WER (Word Error Rate)                    & mizuno2024investigation, park2024towards                                                                         & Measures the accuracy of word transcription in speech-to-text tasks.\\ \cline{2-4} 
%                        & BertScore                                 & mizuno2024investigation                                                                                          & Contextual similarity between generated and reference speech.\\ \cline{2-4} 
%                        & Pearson Coefficient                       & postolache2024naturalistic                                                                                        & Measures the linear relationship between two variables.\\ \cline{2-4} 
%                        & MSE (Mean Squared Error)                 & postolache2024naturalistic                                                                                        & Measures the squared error between predicted and actual values.\\ \hline
% \end{tabular}
% \caption{Evaluation Metrics for Text, Image, and Sound/Speech Tasks with Descriptions}
% \end{table*}



% \tikzstyle{my-box1}=[
%     rectangle,
%     draw=gray,
%     rounded corners,
%     text opacity=1,
%     minimum height=1.5em,
%     minimum width=5em,
%     inner sep=5pt,
%     align=center,
%     fill=green!10,
%     fill opacity=.5,
%     font=\scriptsize,
% ]

% \tikzstyle{my-box2}=[
%     rectangle,
%     draw=gray,
%     rounded corners,
%     text opacity=1,
%     minimum height=1.5em,
%     minimum width=5em,
%     inner sep=5pt,
%     align=center,
%     fill=orange!10,
%     fill opacity=.5,
%     font=\scriptsize,
% ]
% \tikzstyle{my-box3}=[
%     rectangle,
%     draw=gray,
%     rounded corners,
%     text opacity=1,
%     minimum height=1.5em,
%     minimum width=5em,
%     inner sep=5pt,
%     align=center,
%     fill=blue!10,
%     fill opacity=.5,
%     font=\scriptsize,
% ]
    
% \begin{figure*}[t]
%     \centering
%     \resizebox{\textwidth}{!}{
%         \begin{tikzpicture}[node distance=0.2cm]
%             \node[my-box1] (box1) at (0,0) {
%                 \arrayrulecolor{lightgray}
%                 \begin{tabular}{|m{0.20\linewidth}|>{\raggedright\arraybackslash}p{0.7\linewidth}|m{0.7\linewidth}|} \hline \textbf{Metrics} (EEG-Text)& \textbf{Description}&\textbf{Studies}\\ \hline METEOR&  Metric that evaluates translation quality based on precision, recall, synonymy, and word order.	&____, ____\\ \hline BLEU& A metric for evaluating text generation quality by comparing n-grams in machine-generated text with reference text.	&____, ____, ____, ____, ____, ____\\ \hline 
%      CIDEr&  Measures the consensus between generated captions and reference captions in image captioning tasks.	&____\\\hline 
     
%      ROUGE& Measures recall-based metrics for n-grams, word sequences, and word pairs.	&____, ____, ____, ____, ____, ____, ____\\\hline
     
%      BERTScore	&  Uses pre-trained BERT embeddings to compute similarity between reference and generated text.	&____, ____, ____\\\hline \end{tabular}
%             };
            
     %            \node[my-box2, below=of box1] (box2) {
     %                \arrayrulecolor{lightgray}
     %                \begin{tabular}{|m{0.2\linewidth}|m{1.2\linewidth}|} \hline \textbf{Technologies} (EEG-Image)&\textbf{Studies and main use cases}\\ \hline CNNs & • Semi-supervised cross-modal image generation ____ • Visual Saliency and Image Reconstruction from EEG Signals ____ • Map EEG signals to the visual saliency maps corresponding to each image ____ • Demonstrate the generalizability of feature extraction pipeline across three different datasets ____ • Visual Decoding and Reconstruction via EEG Embeddings with Guided Diffusion ____ • Reconstructing images using the same semantics as the corresponding EEG ____ • Zero-shot framework to project neural signals from different sources into the shared subspace ____ \\ \hline 
                    
     %                LSTMs& • Extracting visual class discriminative information from EEG data ____ • Framework for synthesizing the images using small-size EEG datasets ____ • Image reconstruction using generative adversarial and deep fuzzy neural network ____ • Demonstrate the generalizability of feature extraction pipeline across three different datasets ____ \\\hline
                    
     %     Generative Adversarial Networks (GANs)& • Extracting visual class discriminative information from EEG data ____ • Map EEG signals to the visual saliency maps corresponding to each image ____ ____ • Generates images along with producing class-specific EEG encoding as a latent representation ____  • Visual Decoding and Reconstruction via EEG Embeddings with Guided Diffusion ____ • Framework for synthesizing the images using small-size EEG datasets ____ • Image reconstruction using generative adversarial and deep fuzzy neural network ____ \\\hline
         
     %      Variational Autoencoder& • Generating high-quality images directly from brain EEG signals, without the need to translate thoughts into text ____ • Photorealistic Reconstruction of Visual Texture From EEG Signals ____ • Extracting visual class discriminative information from EEG data ____\\\hline
          
     %     Diffusion Models& • Zero-shot framework to project neural signals from different sources into the shared subspace ____ • Generating high-quality images directly from brain EEG signals, without the need to translate thoughts into text ____ • Reconstructing images using the same semantics as the corresponding EEG ____ • Multi-level perceptual information decoding to draw multi grained outputs from given EEG ____ \\\hline
         
     %     Attention Mechanism& • Generates images along with producing class-specific EEG encoding as a latent representation ____ • Self-supervised framework to decode natural images for object recognition ____ • Visual Decoding and Reconstruction via EEG Embeddings with Guided Diffusion ____ \\\hline
         
     %     Contrastive Learning& • Framework for synthesizing the images using small-size EEG datasets ____ • Multi-level perceptual information decoding to draw multi grained outputs from given EEG ____ • Self-supervised framework to decode natural images for object recognition ____ • Generating perceptual and cognitive contents using EEG data ____ • Self-supervised framework to decode natural images for object recognition ____ \\\hline
         
     %     Masked Signal Modeling& • Generating high-quality images directly from brain EEG signals, without the need to translate thoughts into text ____ \\\hline
     %     \end{tabular}
     %            };
    
    
    
                
     %            \node[my-box3, below=of box2] (box3) {
     %                \arrayrulecolor{lightgray}
     %                \begin{tabular}{|m{0.2\linewidth}|m{1.2\linewidth}|} \hline \textbf{Technologies} (EEG-Audio/Speech/Music)  &\textbf{Studies and main use cases}\\ \hline 
     %                CNNs & • Show how acoustic features are related to EEG signals recorded during speech perception and production ____ • Reconstructing music stimuli to be perceived and identified independently 
     % ____ \\\hline
                    
     %     Transfomer& • Investigate the potential to reconstruct speech from EEG signals, including the corresponding speaker's characteristics ____ • Utilizing deep features from EEG data for emotional music composition ____\\\hline
         
     %     Gated Recurrent Units& • Show how acoustic features are related to EEG signals recorded during speech perception and production ____ • Convert EEG of imagined speech into user's own voice ____ • EEG-based Talking-face Generation ____ \\\hline
         
     %     Generative Adversarial Networks (GANs)& • Convert EEG of imagined speech into user's own voice ____ • EEG-based Talking-face Generation ____ \\\hline
         
     %     Automatic Speech Recognition&• Convert EEG of imagined speech into user's own voice ____ • EEG-based Talking-face Generation ____\\\hline
         
     %     Latent Diffusion Models&• Reconstructing naturalistic music from EEG without need for manual pre-processing and channel selection ____\\\hline
         
     %     Attention Module& • Show how acoustic features are related to EEG signals recorded during speech perception and production ____ \\\hline \end{tabular}
     %            };
    
            
%     \end{tikzpicture}
%     }
%     \caption{Technologies and Corresponding References in Surveyed Studies}
% \end{figure*}