\section{EEG-to-Sound/Speech Generation}

We review studies focused on EEG-based generation of sound, speech, voice or music and cover use cases, concerns, techniques, and EEG feature encoding methods for generating Sound or Speech from from EEG.

\subsection{Use Cases and Addressed Concerns}   

EEG-based generation has been explored in various fields beyond image reconstruction, particularly in audio and speech-related applications. These include speech synthesis \cite{krishna2021advancing, lee2023speech}, music decoding and reconstruction \cite{ramirez2022eeg2mel, postolache2024naturalistic}, emotive music generation \cite{jiang2024eeg}, voice reconstruction \cite{lee2023towards}, talking-face generation \cite{park2024towards}, and speech recovery \cite{mizuno2024investigation}. While some studies focus on decoding audio signals for listening tasks in speech or music perception \cite{krishna2021advancing, ramirez2022eeg2mel, park2024towards, mizuno2024investigation, postolache2024naturalistic, jiang2024eeg}, others also investigate speaking tasks and imagined speech \cite{krishna2021advancing, lee2023towards, lee2023speech}.

For more naturalistic communication, \citet{lee2023towards} converts EEG signals recorded during imagined speech into the userâ€™s own voice, aiming for personalized speech synthesis. Similarly, \citet{park2024towards} synthesizes speech from EEG along with generating a talking face with lip-sync. Furthermore, these studies tackle issues such as generating fragmented or abstract outputs \cite{park2024towards}, challenges of synthesizing complete speech from EEG \cite{mizuno2024investigation}, being restricted to simpler music with limited timbres \cite{postolache2024naturalistic}, and the absence of a standardized vocabulary for aligning EEG and audio data \cite{jiang2024eeg}.

\subsection{Techniques Used Across Studies}

\textbf{Convolutional Neural Network (CNN)}-based deep learning models have been used in studies \cite{krishna2021advancing, ramirez2022eeg2mel} to generate audio waveforms from EEG input. \citet{krishna2021advancing} explores speech synthesis for both speaking and listening tasks, using a deep learning architecture with temporal convolution layers, 1D layer, and a time-distributed layer to generate audio waveforms directly. Similarly, \citet{ramirez2022eeg2mel} reconstructs music stimuli using sequential CNN regressors.

\citet{lee2023towards} propose NeuroTalk framework for voice reconstruction from imagined speech. The framework uses a generator based on \textbf{GRUs} to capture sequential EEG information, which outputs a mel-spectrogram. Mel-spectrogram is then converted into a waveform using a \textbf{HiFi-GAN vocoder} \cite{kong2020hifi}, and the resulting waveform is transcribed into text using an \textbf{Automatic Speech Recognition (ASR)} system based on HuBERT \cite{hsu2021hubert}, a self-supervised speech representation learning method. \citet{park2024towards} uses NeuroTalk framework to synthesize audible speech and integrates it with a personalized talking face using \textbf{Wave2Lip} \cite{prajwal2020lip} and Apple API-based avatar generator that accurately lip-sync to the synthesized speech.


\textbf{Transformers and Latent Diffusion Models} have been used to reconstruct speech \cite{mizuno2024investigation} and music \cite{postolache2024naturalistic, jiang2024eeg}. \citet{jiang2024eeg} employs a Transformer model for emotive music generation, while \citet{postolache2024naturalistic} decodes naturalistic music from EEG using a ControlNet adapter \cite{zhang2023adding} to guide AudioLDM2 \cite{liu2024audioldm}, a pre-trained diffusion model, improving control over the generated music.


\subsection{EEG Feature Encoding Techniques}

For speech, voice, and music decoding or generation from EEG, EEG signals are either transformed into intermediate representations, such as mel-spectrograms, or decoded into acoustic and articulatory features\cite{krishna2021advancing}, or EEG temporal features \cite{jiang2024eeg} are utilized. Mel-spectrograms are especially useful, as they offer a shared representational state for both neural signals and audio, enabling more efficient translation between the two modalities.

\citet{krishna2021advancing} incorporates an attention model to predict \textbf{articulatory features} and another attention-regression model to convert these predicted features into \textbf{acoustic features}. Similarly, \citet{jiang2024eeg} extracts EEG tokens through a multi-step process which includes DBSCAN clustering algorithm to derive \textbf{EEG temporal features}. These features are eventually transformed \textbf{EEG positional encoding EEG features} using positional encoding, which are used to form EEG tokens.

In studies using \textbf{mel-spectrograms} as intermediate representations, \citet{ramirez2022eeg2mel} employs a sequential CNN-based regressor to directly map EEG input to time-aligned music spectra. \citet{lee2023speech} seeks to adapt spoken EEG to the subspace of imagined EEG using Common Spatial Pattern (CSP) filters trained on imagined EEG, aiming to generate a user's voice from imagined speech. These CSP filters extract temporal oscillation patterns, minimizing distribution differences between spoken and imagined EEG. Similarly, \citet{postolache2024naturalistic} applies this technique while temporally aligning users' voices with brain signals, using triggers to mark onset intervals and clearly distinguish actual utterance intervals in continuous brain signals.

\subsection{Evaluation Metrics}

EEG-to-speech generation is evaluated using quantitative and qualitative metrics, based on its time-series structure, which also enables its representation as mel-spectrograms. \textit{Mel Cepstral Distortion (MCD)} and \textit{Root Mean Square Error (RMSE)} measure similarity between reconstructed and original speech signals \cite{krishna2021advancing, park2024towards}, while \textit{Structural Similarity Index (SSI)} and \textit{Peak Signal-to-Noise Ratio (PSNR)} assess spectrogram quality \cite{ramirez2022eeg2mel}. Linguistic accuracy is evaluated using Word Error Rate (WER), Character Error Rate (CER), and BERTScore \cite{mizuno2024investigation}, and perceptual quality is quantified with \textit{Frechet Audio Distance (FAD)} \cite{postolache2024naturalistic}. Additional metrics include \textit{Hits@k} for search relevance \cite{jiang2024eeg} and \textit{Mean Opinion Score (MOS)} for subjective quality assessment \cite{lee2023towards}.






