\section{EEG-to-Image Generation}
\label{sec:bci_cv}

This section explores regenerating images from visually evoked brain signals via EEG. It covers use cases, concerns addressed, techniques employed, and EEG feature encoding methods for image generation used by surveyed studies.

\subsection{Use Cases and Addressed Concerns}

Surveyed studies address key challenges like low signal-to-noise ratio of EEG signals \cite{bai2306dreamdiffusion, lan2023seeing, zeng2023dm}, limited information and individual differences in EEG signals \cite{bai2306dreamdiffusion}, lower performance on natural object images compared to digits and characters \cite{mishra2023neurogan} and small dataset sizes \cite{singh2023eeg2image}. Additionally, some efforts explore alternatives to supervised learning \cite{li2020semi, song2023decoding}, since it demands large amount of data. \citet{song2023decoding} addresses concerns regarding convolution layers applied separately along temporal and spatial dimensions, which disrupts the correlation between channels and hinders the spatial properties of brain activity. Overall, these approaches aim to enhance the training, performance, and interpretation of brain data \cite{li2024visual}.

\citet{kavasidis2017brain2image}, \citet{song2023decoding} and \citet{mishra2023neurogan} extract class-specific EEG encodings that contain discriminative information to improve image generation quality, while \cite{nemrodovneural} focus on utilizing spatiotemporal EEG information to determine the neural correlates of facial identity representations and \cite{khaleghi2022visual} map EEG signals to visual saliency maps corresponding to each image. Other Common strategies include projecting neural signals into a shared subspace with image embeddings \cite{shimizu2022improving}, generating class-specific EEG encodings as latent representations \cite{mishra2023neurogan}, and decoding multi-level perceptual information from EEG signals to produce multi-grained outputs \cite{lan2023seeing}.

Additionally, research efforts focus on enhancing the generalizability of feature extraction pipelines across datasets \cite{singh2024learning}, evaluating the performance of different channels \cite{sugimoto2024image}, and incorporating attention modules to highlight the significance of each channel or frequency band \cite{li2024visual}.

\subsection{Techniques Used Across Studies}

Various computer vision generative models are employed to reconstruct    images from EEG signals. These include \textbf{Variational Autoencoders} \cite{kavasidis2017brain2image, wakita2021photorealistic}, \textbf{Generative Adversarial Networks (GANs) }\cite{kavasidis2017brain2image, khaleghi2022visual, mishra2023neurogan, singh2024learning, li2024visual}, and conditional GANs \cite{singh2023eeg2image, ahmadieh2024visual}. \textbf{Diffusion models}, including prior diffusion models that refine EEG embeddings into image priors \cite{shimizu2022improving}, as well as pre-trained diffusion models such as Stable Diffusion \cite{bai2306dreamdiffusion}, are also commonly used. Additionally, diffusion modules based on U-net architecture have been used in \cite{zeng2023dm, lan2023seeing} to further enhance EEG-to-image reconstruction.

\textbf{Contrastive learning} is another popular approach to align multimodal embeddings, employed in studies \cite{singh2023eeg2image, lan2023seeing, song2023decoding, sugimoto2024image} to obtain discriminative features from EEG signals and align the two modalities by constraining their cosine similarity \cite{song2023decoding}. Furthermore, \textbf{attention mechanisms} are integrated into various models \cite{mishra2023neurogan, song2023decoding, li2024visual} to enhance image quality, capture spatial correlations that reflect brain activity inferred from EEG data, and determine the relative importance of individual EEG channels.


\subsection{EEG Feature Encoding Techniques}

In EEG-to-image reconstruction, the process typically begins with an encoder identifying the latent feature space of EEG signals, followed by a decoder that converts these features into an image. Long Short-Term Memory (LSTM)-based architectures are widely used due to their effectiveness in capturing \textbf{temporal dependencies} in EEG signals. \citet{kavasidis2017brain2image} employs an LSTM network to generate a compact and class-discriminative feature vector, which is also used for object recognition. Similarly, \cite{singh2023eeg2image} integrates LSTM with a triplet-loss-based contrastive learning approach to enhance \textbf{feature discrimination}. \citet{singh2024learning} extends this approach by incorporating both CNN and LSTM architectures trained under EEG label supervision with triplet loss, further improving discriminative feature learning. Additionally, \cite{ahmadieh2024visual} uses LSTM to extract EEG features across two dimensions (EEG channels and signal duration) and \textbf{enhances feature generation} through various regression methods, including polynomial regression, neural network regression, and type-1 and type-2 fuzzy regression.

Several studies also leverage convolutional architectures to capture \textbf{spatial dependencies} in EEG. \citet{li2020semi} uses a three-layer feedforward neural network to project EEG signals into semantic features. \citet{wakita2021photorealistic} adopts a 1D convolutional encoder-decoder as part of a multimodal variational autoencoder (VAE) to obtain mean and variance vectors for EEG signal representation. \citet{mishra2023neurogan} uses a convolutional encoder-decoder framework enhanced with an attention module to focus more on channels with important features instead of using all the features with equal weights. Similarly, \citet{sugimoto2024image} implements EEGNet \cite{lawhern2018eegnet}, a compact convolutional network, as an EEG encoder, while \citet{li2024visual} uses Sinc-EEGNet \cite{bria2021sinc}, an architecture incorporating a sinc-based convolution layer, depth-wise convolution, and separable convolution to extract EEG features. It also integrates an attention mechanism to identify the \textbf{most relevant frequency bands and channels} for signal-based classification.

Graph-based techniques have been explored for EEG feature extraction. \citet{khaleghi2022visual} constructs functional graph connectivity-based embeddings from EEG signals, which are then processed using a Geometric Deep Network (GDN) to derive feature vectors. \citet{song2023decoding} integrates temporal-spatial convolution with plug-and-play spatial modules, leveraging self-attention and graph attention mechanisms to extract EEG features more effectively.

To integrate temporal and spatial feature extraction mechanisms to improve EEG-based image reconstruction, \citet{zeng2023dm} develops a framework inspired by EEGChannelNet \cite{palazzo2020decoding} and ResNet-18, combining spatial, temporal, and temporal-spatial blocks with a multi-kernel residual block. \citet{shimizu2022improving} uses a time-series-inspired architecture with a channel-wise transformed encoder and temporal-spatial convolution to extract \textbf{rich latent EEG representations}.

Self-supervised learning and contrastive learning have been applied to enhance EEG feature extraction. \citet{bai2306dreamdiffusion} uses masked signal modeling, where EEG tokens are partially masked, and a 1D convolutional layer transforms all tokens into embeddings. A Masked Autoencoder (MAE) predicts the missing tokens, refining the learned representations. \citet{lan2023seeing} employs contrastive learning to extract pixel-level semantics from EEG signals while generating a \textbf{saliency map of silhouette information} using GANs. It also aligns CLIP embeddings for image captions with an EEG sample-level encoder through a specialized loss function.

\subsection{Evaluation Metrics} 

EEG-to-image generation often begins with object classification to ensure extracted EEG features contain useful class-discriminative information. Metrics like \textit{top-k accuracy} are commonly used \cite{shimizu2022improving, lan2023seeing, song2023decoding}, along with qualitative visual analysis and quantitative evaluations. Key quantitative metrics include \textit{Inception Score (IS)} \cite{salimans2016improved}, used by \cite{kavasidis2017brain2image, li2020semi, bai2306dreamdiffusion, singh2023eeg2image} which measures the quality of images, \textit{Frechet Inception Distance (FID)} for measuring realism \cite{bai2306dreamdiffusion, singh2024learning, ahmadieh2024visual}, and saliency metrics such as \textit{Structural Similarity Index (SSIM)} for assessing perceptual fidelity \cite{khaleghi2022visual, shimizu2022improving, bai2306dreamdiffusion, ahmadieh2024visual, sugimoto2024image}. Other useful metrics are PixCorr (Pixel-wise Correlation) \cite{shimizu2022improving}, \textit{Kernel Inception Distance (KID)} \cite{singh2024learning}, \textit{LPIPS (Learned Perceptual Image Patch Similarity)} \cite{bai2306dreamdiffusion} and \textit{Diversity Score} \cite{mishra2023neurogan}.

