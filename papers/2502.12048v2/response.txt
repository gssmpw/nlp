\begin{figure*}[t]
    \centering
    \resizebox{\textwidth}{!}{
        \begin{tikzpicture}[node distance=0.2cm]
            \node[my-box1] (box1) at (0,0) {
                \arrayrulecolor{lightgray}
                \begin{tabular}{|m{0.20\linewidth}|>{\raggedright\arraybackslash}p{0.7\linewidth}|m{0.7\linewidth}|} \hline \textbf{Metrics} (EEG-Text)& \textbf{Description}&\textbf{Studies}\\ \hline METEOR&  Metric that evaluates translation quality based on precision, recall, synonymy, and word order.	&[1], [2]\\ \hline BLEU& A metric for evaluating text generation quality by comparing n-grams in machine-generated text with reference text.	&[3], [4], [5], [6], [7], [8]\\ \hline 
        CIDEr&  Measures the consensus between generated captions and reference captions in image captioning tasks.	&[9]\\\hline 
     
        ROUGE& Measures recall-based metrics for n-grams, word sequences, and word pairs.	&[10], [11], [12], [13], [14], [15], [16]\\ \hline
     
        BERTScore	&  Uses pre-trained BERT embeddings to compute similarity between reference and generated text.	&[17], [18], [19]\\\hline \end{tabular}
            };
            
     %            \node[my-box2, below=of box1] (box2) {
     %                \arrayrulecolor{lightgray}
     %                \begin{tabular}{|m{0.2\linewidth}|m{1.2\linewidth}|} \hline \textbf{Technologies} (EEG-Image)&\textbf{Studies and main use cases}\\ \hline CNNs & • Semi-supervised cross-modal image generation ____ • Visual Saliency and Image Reconstruction from EEG Signals ____ • Map EEG signals to the visual saliency maps corresponding to each image ____ • Demonstrate the generalizability of feature extraction pipeline across three different datasets ____ • Visual Decoding and Reconstruction via EEG Embeddings with Guided Diffusion ____ • Reconstructing images using the same semantics as the corresponding EEG ____ • Zero-shot framework to project neural signals from different sources into the shared subspace ____ \\ \hline 
                    
     %                LSTMs& • Extracting visual class discriminative information from EEG data ____ • Framework for synthesizing the images using small-size EEG datasets ____ • Image reconstruction using generative adversarial and deep fuzzy neural network ____ • Demonstrate the generalizability of feature extraction pipeline across three different datasets ____ \\\hline
                    
     %     Generative Adversarial Networks (GANs)& • Extracting visual class discriminative information from EEG data ____ • Map EEG signals to the visual saliency maps corresponding to each image ____ ____ • Generates images along with producing class-specific EEG encoding as a latent representation ____  • Visual Decoding and Reconstruction via EEG Embeddings with Guided Diffusion ____ • Framework for synthesizing the images using small-size EEG datasets ____ • Image reconstruction using generative adversarial and deep fuzzy neural network ____ \\\hline
         
     %      Variational Autoencoder& • Generating high-quality images directly from brain EEG signals, without the need to translate thoughts into text ____ • Photorealistic Reconstruction of Visual Texture From EEG Signals ____ • Extracting visual class discriminative information from EEG data ____\\\hline
          
     %     Diffusion Models& • Zero-shot framework to project neural signals from different sources into the shared subspace ____ • Generating high-quality images directly from brain EEG signals, without the need to translate thoughts into text ____ • Reconstructing images using the same semantics as the corresponding EEG ____ • Multi-level perceptual information decoding to draw multi grained outputs from given EEG ____ \\\hline
         
     %     Attention Mechanism& • Generates images along with producing class-specific EEG encoding as a latent representation ____ • Self-supervised framework to decode natural images for object recognition ____ • Visual Decoding and Reconstruction via EEG Embeddings with Guided Diffusion ____ \\\hline
         
     %     Contrastive Learning& • Framework for synthesizing the images using small-size EEG datasets ____ • Multi-level perceptual information decoding to draw multi grained outputs from given EEG ____ • Self-supervised framework to decode natural images for object recognition ____ • Generating perceptual and cognitive contents using EEG data ____ • Self-supervised framework to decode natural images for object recognition ____ \\\hline
         
     %     Masked Signal Modeling& • Generating high-quality images directly from brain EEG signals, without the need to translate thoughts into text ____ \\\hline
            \node[my-box2, below=of box1] (box2) {
                \arrayrulecolor{lightgray}
                \begin{tabular}{|m{0.20\linewidth}|>{\raggedright\arraybackslash}p{0.7\linewidth}|} \hline \textbf{Technologies} (EEG-Audio/Speech/Music)  &\textbf{Studies and main use cases}\\ \hline 
                CNNs & • Show how acoustic features are related to EEG signals recorded during speech perception and production ____ • Reconstructing music stimuli to be perceived and identified independently 
% ____ \\\hline
                    
                Transfomer& • Investigate the potential to reconstruct speech from EEG signals, including the corresponding speaker's characteristics ____ • Utilizing deep features from EEG data for emotional music composition ____\\\hline
         
                Gated Recurrent Units& • Show how acoustic features are related to EEG signals recorded during speech perception and production ____ • Convert EEG of imagined speech into user's own voice ____ • EEG-based Talking-face Generation ____ \\\hline
         
                Generative Adversarial Networks (GANs)& • Convert EEG of imagined speech into user's own voice ____ • EEG-based Talking-face Generation ____ \\\hline
         
                Automatic Speech Recognition&• Convert EEG of imagined speech into user's own voice ____ • EEG-based Talking-face Generation ____\\\hline
         
                Latent Diffusion Models&• Reconstructing naturalistic music from EEG without need for manual pre-processing and channel selection ____\\\hline
         
                Attention Module& • Show how acoustic features are related to EEG signals recorded during speech perception and production ____ \\\hline \end{tabular}
            };
    
            
%     \end{tikzpicture}
    }
    \caption{Technologies and Corresponding References in Surveyed Studies}
\end{figure*}