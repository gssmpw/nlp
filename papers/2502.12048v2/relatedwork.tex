\section{Related Work}
\label{sec:relwork}

Several surveys have explored EEG-based brain-computer interfaces (BCIs) and deep learning techniques for neural decoding. \cite{chen2022toward} provides a broad overview of EEG-based BCI applications, discussing signal processing methodologies and traditional machine learning techniques but lacks an in-depth analysis of generative models. \cite{gong2021deep} and \cite{weng2024self} review deep learning techniques, particularly CNNs and RNNs, for EEG classification, emotion recognition, and cognitive state monitoring, yet they focus more on supervised learning approaches rather than self-supervised and generative modeling. More recently, \cite{murad2024unveiling} examined EEG-to-text decoding but primarily covered classification-based approaches without exploring the role of large language models (LLMs) or multimodal integration with vision-based EEG applications. Additionally, \cite{sun2023survey} discusses self-supervised learning (SSL) techniques for EEG but does not emphasize their applicability to generative tasks, such as EEG-to-image or EEG-to-text generation. Our survey specifically examines the intersection of EEG and generative AI, focusing on recent approches for EEG-to-text and EEG-to-image translation. While previous studies focus on EEG feature extraction and classification, our work highlights how self-supervised learning, contrastive learning, and multimodal alignment improve EEG-based generation.

\input{dataset_table}


\tikzstyle{my-box1}=[
    rectangle,
    draw=gray,
    rounded corners,
    text opacity=1,
    minimum height=1.5em,
    minimum width=5em,
    inner sep=5pt,
    align=center,
    fill=green!10,
    fill opacity=.5,
    font=\small,
]

\tikzstyle{my-box2}=[
    rectangle,
    draw=gray,
    rounded corners,
    text opacity=1,
    minimum height=1.5em,
    minimum width=5em,
    inner sep=5pt,
    align=center,
    fill=orange!10,
    fill opacity=.5,
    font=\small,
]
\tikzstyle{my-box3}=[
    rectangle,
    draw=gray,
    rounded corners,
    text opacity=1,
    minimum height=1.5em,
    minimum width=5em,
    inner sep=5pt,
    align=center,
    fill=blue!10,
    fill opacity=.5,
    font=\small,
]
    
\begin{figure*}[t]
    \centering
    \resizebox{\textwidth}{!}{
        \begin{tikzpicture}[node distance=0.2cm]
            \node[my-box1] (box1) at (0,0) {
                \arrayrulecolor{lightgray}
                \begin{tabular}{|m{0.15\linewidth}|m{1.3\linewidth}|} \hline \textbf{EEG-Text}&\textbf{Studies and main use cases}\\ \hline CNNs & • Generate medical reports \cite{biswal2019eegtotext} • Translate user's active intent to text represented by morse code \cite{srivastava2020think2type} • Folded ensemble technique to minimize the computational complexity and solve all the class imbalance issues to enhance the accuracy of text generation \cite{rathod2024folded}\\ \hline LSTMs&• Translate user's active intent to text represented by morse code \cite{srivastava2020think2type} •  Folded ensemble technique to minimize the computational complexity and solve all the class imbalance issues to enhance the accuracy of text generation \cite{rathod2024folded}\\ \hline 
     LLMs& • EEG-to-text seq-to-seq decoding and zero-shot sentence sentiment classification on natural reading tasks \cite{wang2022open} • Improving accuracy of open-vocabulary EEG-to-text decoding \cite{liu2024eeg2text} • Bridge the semantic gap between EEG and Text \cite{wang2024enhancing} • Open vocabulary EEG decoding incorporating a subject-dependent representation learning module \cite{amrani2024deep} • Ensuring cross-modal semantic consistency between EEG and Text \cite{tao2024see} • Capture global and local contextual information and long-term dependencies • \cite{chen2025decoding} • Use visual stimuli rather than text circumvent the complexities of language processing \cite{mishra2024thought2text}\\\hline 
     
     Transformer&• EEG-to-text seq-to-seq decoding and zero-shot sentence sentiment classification on natural reading tasks \cite{wang2022open} • Open vocabulary EEG decoding incorporating a subject-dependent representation learning module \cite{amrani2024deep} • Ensuring cross-modal semantic consistency between EEG and Text \cite{tao2024see} • Improving accuracy of open-vocabulary EEG-to-text decoding \cite{liu2024eeg2text} • Bridge the semantic gap between EEG and Text using LLMs \cite{wang2024enhancing} • Recalibrates subject-dependent EEG representation to the semantic-dependent EEG representation \cite{feng2023aligning} • Open vocabulary EEG-to-Text translation tasks  with or without word -level markers \cite{duan2023dewave} • Decoding EEG Speech Perception with Transformers and VAE-based Data Augmentation \cite{yu2025decoding}\\\hline
     
     Gated Recurrent Units& • Open vocabulary EEG decoding incorporating a subject-dependent representation learning module \cite{amrani2024deep} • Capture global and local contextual information and long-term dependencies. 
 \cite{chen2025decoding} \\\hline
     Recurrent Neural Networks& • Translate active intention into text format based on Morse code \citet{yang2023thoughts} \\\hline
     Attention Mechanism& • Generate medical reports \cite{biswal2019eegtotext} • Capture global and local contextual information and long-term dependencies \cite{chen2025decoding} \\ \hline 
     Contrastive Learning&  • Recalibrates subject-dependent EEG representation to the semantic-dependent EEG representation \citet{feng2023aligning}  • Ensuring cross-modal semantic consistency between EEG and Text \cite{tao2024see}  • Bridge the semantic gap between EEG and Text using LLMs \cite{wang2024enhancing} \\ \hline 
     Masked Signal Modeling&• Improving accuracy of open-vocabulary EEG-to-text decoding \cite{liu2024eeg2text} • Ensuring cross-modal semantic consistency between EEG and Text \cite{tao2024see} • Bridge the semantic gap between EEG and Text using LLMs \cite{wang2024enhancing} \\ \hline \end{tabular}
            };
            
            \node[my-box2, below=of box1] (box2) {
                \arrayrulecolor{lightgray}
                \begin{tabular}{|m{0.15\linewidth}|m{1.3\linewidth}|} \hline \textbf{EEG-Image}&\textbf{Studies and main use cases}\\ \hline CNNs & • Semi-supervised cross-modal image generation \cite{li2020semi} • Visual Saliency and Image Reconstruction from EEG Signals \cite{khaleghi2022visual} • Map EEG signals to the visual saliency maps corresponding to each image \cite{song2023decoding} • Demonstrate the generalizability of feature extraction pipeline across three different datasets \cite{singh2024learning} • Visual Decoding and Reconstruction via EEG Embeddings with Guided Diffusion \cite{li2024visual} • Reconstructing images using the same semantics as the corresponding EEG \cite{zeng2023dm} • Zero-shot framework to project neural signals from different sources into the shared subspace \cite{shimizu2022improving} \\ \hline 
                
                LSTMs& • Extracting visual class discriminative information from EEG data \cite{kavasidis2017brain2image} • Framework for synthesizing the images using small-size EEG datasets \cite{singh2023eeg2image} • Image reconstruction using generative adversarial and deep fuzzy neural network \cite{ahmadieh2024visual} • Demonstrate the generalizability of feature extraction pipeline across three different datasets \cite{singh2024learning} \\\hline
                
     Generative Adversarial Networks (GANs)& • Extracting visual class discriminative information from EEG data \cite{kavasidis2017brain2image} • Map EEG signals to the visual saliency maps corresponding to each image \cite{khaleghi2022visual} \cite{mishra2023neurogan} • Generates images along with producing class-specific EEG encoding as a latent representation \cite{singh2024learning}  • Visual Decoding and Reconstruction via EEG Embeddings with Guided Diffusion \cite{li2024visual} • Framework for synthesizing the images using small-size EEG datasets \cite{singh2023eeg2image} • Image reconstruction using generative adversarial and deep fuzzy neural network \cite{ahmadieh2024visual} \\\hline
     
      Variational Autoencoder& • Generating high-quality images directly from brain EEG signals, without the need to translate thoughts into text \cite{bai2306dreamdiffusion} • Photorealistic Reconstruction of Visual Texture From EEG Signals \cite{wakita2021photorealistic} • Extracting visual class discriminative information from EEG data \cite{kavasidis2017brain2image}\\\hline
      
     Diffusion Models& • Zero-shot framework to project neural signals from different sources into the shared subspace \cite{shimizu2022improving} • Generating high-quality images directly from brain EEG signals, without the need to translate thoughts into text \cite{bai2306dreamdiffusion} • Reconstructing images using the same semantics as the corresponding EEG \cite{zeng2023dm} • Multi-level perceptual information decoding to draw multi grained outputs from given EEG \cite{lan2023seeing} \\\hline
     
     Attention Mechanism& • Generates images along with producing class-specific EEG encoding as a latent representation \cite{mishra2023neurogan} • Self-supervised framework to decode natural images for object recognition \cite{song2023decoding} • Visual Decoding and Reconstruction via EEG Embeddings with Guided Diffusion \cite{li2024visual} \\\hline
     
     Contrastive Learning& • Framework for synthesizing the images using small-size EEG datasets \cite{singh2023eeg2image} • Multi-level perceptual information decoding to draw multi grained outputs from given EEG \cite{lan2023seeing} • Self-supervised framework to decode natural images for object recognition \cite{song2023decoding} • Generating perceptual and cognitive contents using EEG data \cite{sugimoto2024image} • Self-supervised framework to decode natural images for object recognition \cite{song2023decoding} \\\hline
     
     Masked Signal Modeling& • Generating high-quality images directly from brain EEG signals, without the need to translate thoughts into text \cite{bai2306dreamdiffusion} \\\hline
     \end{tabular}
            };



            
            \node[my-box3, below=of box2] (box3) {
                \arrayrulecolor{lightgray}
                \begin{tabular}{|m{0.15\linewidth}|m{1.3\linewidth}|} \hline \textbf{EEG-Audio/Speech/Music}  &\textbf{Studies and main use cases}\\ \hline 
                CNNs & • Show how acoustic features are related to EEG signals recorded during speech perception and production \cite{krishna2021advancing} • Reconstructing music stimuli to be perceived and identified independently 
 \cite{ramirez2022eeg2mel} \\\hline
                
     Transfomer& • Investigate the potential to reconstruct speech from EEG signals, including the corresponding speaker's characteristics \cite{mizuno2024investigation} • Utilizing deep features from EEG data for emotional music composition \cite{jiang2024eeg}\\\hline
     
     Gated Recurrent Units& • Show how acoustic features are related to EEG signals recorded during speech perception and production \cite{krishna2021advancing} • Convert EEG of imagined speech into user's own voice \cite{lee2023towards} • EEG-based Talking-face Generation \cite{park2024towards} \\\hline
     
     Generative Adversarial Networks (GANs)& • Convert EEG of imagined speech into user's own voice \cite{lee2023towards} • EEG-based Talking-face Generation \cite{park2024towards} \\\hline
     
     Automatic Speech Recognition&• Convert EEG of imagined speech into user's own voice \cite{lee2023towards} • EEG-based Talking-face Generation \cite{park2024towards}\\\hline
     
     Latent Diffusion Models&• Reconstructing naturalistic music from EEG without need for manual pre-processing and channel selection \cite{postolache2024naturalistic}\\\hline
     
     Attention Module& • Show how acoustic features are related to EEG signals recorded during speech perception and production \cite{krishna2021advancing} \\\hline \end{tabular}
            };

            
    \end{tikzpicture}
    }
    \caption{Techniques and References of Surveyed Studies for EEG to Text, Image and Beyond}
    \label{fig:techniques}
\end{figure*}


% \begin{table*}[t]
% \centering
% \tiny % smaller font size
% \renewcommand{\arraystretch}{0.8} % reduce row height
% \begin{tabular}{|l|p{3cm}|p{5cm}|p{6cm}|}  % Added new column for description
% \hline
% \multicolumn{4}{|c|}{\textbf{Evaluation Metrics for Text, Image, and Sound/Speech Tasks}} \\ \hline
% \textbf{Category}     & \textbf{Metric}                            & \textbf{Studies}                                                                                                   & \textbf{Description}  \\ \hline
% \multirow{11}{*}{Text} & BLEU                                      & biswal2019eegtotext, wang2022open, feng2023aligning, duan2023dewave, liu2024eeg2text, wang2024enhancing, amrani2024deep, chen2025decoding & Measures the similarity between the generated text and a reference text.\\ \cline{2-4} 
%                        & ROUGE                                     & wang2022open, feng2023aligning, duan2023dewave, wang2024enhancing, amrani2024deep, chen2025decoding, mishra2024thought2text & Measures recall and overlap between generated and reference text.\\ \cline{2-4} 
%                        & METEOR                                    & biswal2019eegtotext, mishra2024thought2text, chen2025decoding                                                     & Similar to BLEU but incorporates synonyms and stemming. \\ \cline{2-4} 
%                        & CIDEr                                     & biswal2019eegtotext                                                                                                & Focuses on evaluating the quality of image captions.\\ \cline{2-4} 
%                        & Inception Score                           & kavasidis2017brain2image, li2020semi, singh2023eeg2image, singh2024learning                                      & Measures the quality of generated images by comparing their classification probability.\\ \cline{2-4} 
%                        & F1-Score                                  & wang2022open, zeng2023dm, ahmadieh2024visual                                                                     & Measures the balance between precision and recall.\\ \cline{2-4} 
%                        & Precision                                 & ahmadieh2024visual                                                                                               & Measures the accuracy of positive predictions.\\ \cline{2-4} 
%                        & Recall                                    & ahmadieh2024visual                                                                                               & Measures the proportion of actual positives that were correctly identified.\\ \cline{2-4} 
%                        & Accuracy                                  & zeng2023dm, ahmadieh2024visual, lee2023towards, jiang2024eeg, mizuno2024investigation, ramirez2022eeg2mel        & Measures the overall correctness of the predictions.\\ \cline{2-4} 
%                        & WER (Word Error Rate)                     & mizuno2024investigation                                                                                          & Measures the error in speech-to-text tasks.\\ \cline{2-4} 
%                        & BERTScore                                 & amrani2024deep, mizuno2024investigation                                                                          & Measures the contextual similarity between generated and reference text.\\ \hline
% \multirow{5}{*}{Image} & SSIM (Structural Similarity Index)         & khaleghi2022visual, shimizu2022improving, zeng2023dm, sugimoto2024image, park2024towards                         & Assesses the perceptual quality and structural similarity between images.\\ \cline{2-4} 
%                        & Inception Score                           & kavasidis2017brain2image, li2020semi, singh2023eeg2image, singh2024learning                                      & Assesses the quality of generated images via a pre-trained classifier.\\ \cline{2-4} 
%                        & PSNR (Peak Signal-to-Noise Ratio)         & ramirez2022eeg2mel                                                                                              & Measures the quality of the reconstruction by comparing the signal-to-noise ratio.\\ \cline{2-4} 
%                        & SSI (Structural Similarity Index)         & ramirez2022eeg2mel                                                                                              & Evaluates the structural similarity of images in a perceptual sense.\\ \cline{2-4} 
%                        & Cosine Similarity                         & ikegawa2024text                                                                                                  & Measures the similarity of two vectors using cosine distance.\\ \hline
% \multirow{7}{*}{Sound/Speech} & Mel Cepstral Distortion (MCD)        & krishna2021advancing, park2024towards                                                                            & Measures the difference between the mel-cepstral coefficients of two audio signals.\\ \cline{2-4} 
%                        & RMSE (Root Mean Square Error)            & krishna2021advancing, park2024towards, lee2023towards                                                           & Measures the error between predicted and actual values.\\ \cline{2-4} 
%                        & FAD (Frechet Audio Distance)             & postolache2024naturalistic                                                                                        & Quantifies the quality of generated audio by comparing distributions.\\ \cline{2-4} 
%                        & CER (Character Error Rate)               & lee2023towards, mizuno2024investigation                                                                           & Measures the error rate in character-based speech-to-text tasks.\\ \cline{2-4} 
%                        & WER (Word Error Rate)                    & mizuno2024investigation, park2024towards                                                                         & Measures the accuracy of word transcription in speech-to-text tasks.\\ \cline{2-4} 
%                        & BertScore                                 & mizuno2024investigation                                                                                          & Contextual similarity between generated and reference speech.\\ \cline{2-4} 
%                        & Pearson Coefficient                       & postolache2024naturalistic                                                                                        & Measures the linear relationship between two variables.\\ \cline{2-4} 
%                        & MSE (Mean Squared Error)                 & postolache2024naturalistic                                                                                        & Measures the squared error between predicted and actual values.\\ \hline
% \end{tabular}
% \caption{Evaluation Metrics for Text, Image, and Sound/Speech Tasks with Descriptions}
% \end{table*}



% \tikzstyle{my-box1}=[
%     rectangle,
%     draw=gray,
%     rounded corners,
%     text opacity=1,
%     minimum height=1.5em,
%     minimum width=5em,
%     inner sep=5pt,
%     align=center,
%     fill=green!10,
%     fill opacity=.5,
%     font=\scriptsize,
% ]

% \tikzstyle{my-box2}=[
%     rectangle,
%     draw=gray,
%     rounded corners,
%     text opacity=1,
%     minimum height=1.5em,
%     minimum width=5em,
%     inner sep=5pt,
%     align=center,
%     fill=orange!10,
%     fill opacity=.5,
%     font=\scriptsize,
% ]
% \tikzstyle{my-box3}=[
%     rectangle,
%     draw=gray,
%     rounded corners,
%     text opacity=1,
%     minimum height=1.5em,
%     minimum width=5em,
%     inner sep=5pt,
%     align=center,
%     fill=blue!10,
%     fill opacity=.5,
%     font=\scriptsize,
% ]
    
% \begin{figure*}[t]
%     \centering
%     \resizebox{\textwidth}{!}{
%         \begin{tikzpicture}[node distance=0.2cm]
%             \node[my-box1] (box1) at (0,0) {
%                 \arrayrulecolor{lightgray}
%                 \begin{tabular}{|m{0.20\linewidth}|>{\raggedright\arraybackslash}p{0.7\linewidth}|m{0.7\linewidth}|} \hline \textbf{Metrics} (EEG-Text)& \textbf{Description}&\textbf{Studies}\\ \hline METEOR&  Metric that evaluates translation quality based on precision, recall, synonymy, and word order.	&\cite{feng2023aligning}, \cite{chen2025decoding}\\ \hline BLEU& A metric for evaluating text generation quality by comparing n-grams in machine-generated text with reference text.	&\cite{feng2023aligning}, \cite{wang2024enhancing}, \cite{amrani2024deep}, \cite{mishra2024thought2text}, \cite{chen2025decoding}, \cite{yu2025decoding}\\ \hline 
%      CIDEr&  Measures the consensus between generated captions and reference captions in image captioning tasks.	&\cite{feng2023aligning}\\\hline 
     
%      ROUGE& Measures recall-based metrics for n-grams, word sequences, and word pairs.	&\cite{wang2022open}, \cite{feng2023aligning}, \cite{liu2024eeg2text}, \cite{amrani2024deep}, \cite{tao2024see}, \cite{mishra2024thought2text}, \cite{duan2023dewave}\\\hline
     
%      BERTScore	&  Uses pre-trained BERT embeddings to compute similarity between reference and generated text.	&\cite{wang2024enhancing}, \cite{amrani2024deep}, \cite{mishra2024thought2text}\\\hline \end{tabular}
%             };
            
     %            \node[my-box2, below=of box1] (box2) {
     %                \arrayrulecolor{lightgray}
     %                \begin{tabular}{|m{0.2\linewidth}|m{1.2\linewidth}|} \hline \textbf{Technologies} (EEG-Image)&\textbf{Studies and main use cases}\\ \hline CNNs & • Semi-supervised cross-modal image generation \cite{li2020semi} • Visual Saliency and Image Reconstruction from EEG Signals \cite{khaleghi2022visual} • Map EEG signals to the visual saliency maps corresponding to each image \cite{song2023decoding} • Demonstrate the generalizability of feature extraction pipeline across three different datasets \cite{singh2024learning} • Visual Decoding and Reconstruction via EEG Embeddings with Guided Diffusion \cite{li2024visual} • Reconstructing images using the same semantics as the corresponding EEG \cite{zeng2023dm} • Zero-shot framework to project neural signals from different sources into the shared subspace \cite{shimizu2022improving} \\ \hline 
                    
     %                LSTMs& • Extracting visual class discriminative information from EEG data \cite{kavasidis2017brain2image} • Framework for synthesizing the images using small-size EEG datasets \cite{singh2023eeg2image} • Image reconstruction using generative adversarial and deep fuzzy neural network \cite{ahmadieh2024visual} • Demonstrate the generalizability of feature extraction pipeline across three different datasets \cite{singh2024learning} \\\hline
                    
     %     Generative Adversarial Networks (GANs)& • Extracting visual class discriminative information from EEG data \cite{kavasidis2017brain2image} • Map EEG signals to the visual saliency maps corresponding to each image \cite{khaleghi2022visual} \cite{mishra2023neurogan} • Generates images along with producing class-specific EEG encoding as a latent representation \cite{singh2024learning}  • Visual Decoding and Reconstruction via EEG Embeddings with Guided Diffusion \cite{li2024visual} • Framework for synthesizing the images using small-size EEG datasets \cite{singh2023eeg2image} • Image reconstruction using generative adversarial and deep fuzzy neural network \cite{ahmadieh2024visual} \\\hline
         
     %      Variational Autoencoder& • Generating high-quality images directly from brain EEG signals, without the need to translate thoughts into text \cite{bai2306dreamdiffusion} • Photorealistic Reconstruction of Visual Texture From EEG Signals \cite{wakita2021photorealistic} • Extracting visual class discriminative information from EEG data \cite{kavasidis2017brain2image}\\\hline
          
     %     Diffusion Models& • Zero-shot framework to project neural signals from different sources into the shared subspace \cite{shimizu2022improving} • Generating high-quality images directly from brain EEG signals, without the need to translate thoughts into text \cite{bai2306dreamdiffusion} • Reconstructing images using the same semantics as the corresponding EEG \cite{zeng2023dm} • Multi-level perceptual information decoding to draw multi grained outputs from given EEG \cite{lan2023seeing} \\\hline
         
     %     Attention Mechanism& • Generates images along with producing class-specific EEG encoding as a latent representation \cite{mishra2023neurogan} • Self-supervised framework to decode natural images for object recognition \cite{song2023decoding} • Visual Decoding and Reconstruction via EEG Embeddings with Guided Diffusion \cite{li2024visual} \\\hline
         
     %     Contrastive Learning& • Framework for synthesizing the images using small-size EEG datasets \cite{singh2023eeg2image} • Multi-level perceptual information decoding to draw multi grained outputs from given EEG \cite{lan2023seeing} • Self-supervised framework to decode natural images for object recognition \cite{song2023decoding} • Generating perceptual and cognitive contents using EEG data \cite{sugimoto2024image} • Self-supervised framework to decode natural images for object recognition \cite{song2023decoding} \\\hline
         
     %     Masked Signal Modeling& • Generating high-quality images directly from brain EEG signals, without the need to translate thoughts into text \cite{bai2306dreamdiffusion} \\\hline
     %     \end{tabular}
     %            };
    
    
    
                
     %            \node[my-box3, below=of box2] (box3) {
     %                \arrayrulecolor{lightgray}
     %                \begin{tabular}{|m{0.2\linewidth}|m{1.2\linewidth}|} \hline \textbf{Technologies} (EEG-Audio/Speech/Music)  &\textbf{Studies and main use cases}\\ \hline 
     %                CNNs & • Show how acoustic features are related to EEG signals recorded during speech perception and production \cite{krishna2021advancing} • Reconstructing music stimuli to be perceived and identified independently 
     % \cite{ramirez2022eeg2mel} \\\hline
                    
     %     Transfomer& • Investigate the potential to reconstruct speech from EEG signals, including the corresponding speaker's characteristics \cite{mizuno2024investigation} • Utilizing deep features from EEG data for emotional music composition \cite{jiang2024eeg}\\\hline
         
     %     Gated Recurrent Units& • Show how acoustic features are related to EEG signals recorded during speech perception and production \cite{krishna2021advancing} • Convert EEG of imagined speech into user's own voice \cite{lee2023towards} • EEG-based Talking-face Generation \cite{park2024towards} \\\hline
         
     %     Generative Adversarial Networks (GANs)& • Convert EEG of imagined speech into user's own voice \cite{lee2023towards} • EEG-based Talking-face Generation \cite{park2024towards} \\\hline
         
     %     Automatic Speech Recognition&• Convert EEG of imagined speech into user's own voice \cite{lee2023towards} • EEG-based Talking-face Generation \cite{park2024towards}\\\hline
         
     %     Latent Diffusion Models&• Reconstructing naturalistic music from EEG without need for manual pre-processing and channel selection \cite{postolache2024naturalistic}\\\hline
         
     %     Attention Module& • Show how acoustic features are related to EEG signals recorded during speech perception and production \cite{krishna2021advancing} \\\hline \end{tabular}
     %            };
    
            
%     \end{tikzpicture}
%     }
%     \caption{Technologies and Corresponding References in Surveyed Studies}
% \end{figure*}