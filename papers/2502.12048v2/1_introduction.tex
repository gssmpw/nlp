\section{Introduction \& Motivation}
\label{sec:intro}
The convergence of Brain-Computer Interfaces (BCIs) and Generative Artificial Intelligence (GenAI) is transforming human-computer interaction by enabling direct brain-to-device communication. These advancements have enabled applications in assistive communication for individuals with disabilities, cognitive neuroscience, mental health assessment, augmented reality (AR)/virtual reality (VR), and neural art generation. Electroencephalography (EEG), a widely used non-invasive neural recording technique, \textbf{enables both passive and active Brain-Computer Interfaces (BCIs) and holds potential for applications in real-time adaptive human-computer interaction} \cite{zander2010enhancing, wolpaw2010brain}. Recent advancements in deep learning and generative models have significantly improved the decoding of EEG signals, enabling the translation of neural activity into text, images, and speech. Specifically, Generative AI, including Generative Adversarial Networks (GANs) \cite{goodfellow2014generative} and Transformers \cite{vaswani2017attention}, has significantly advanced brain decoding, facilitating visual reconstruction, language generation, and speech synthesis \cite{bai2306dreamdiffusion, srivastava2020think2type, lee2023towards}. GANs improve cross-subject classification and EEG data augmentation \cite{song2021common}, while Transformer-based architectures and multimodal deep learning frameworks \cite{liu2024eeg2text,wang2022open} enhance EEG-to-text translation and semantic decoding \cite{ali2024get}, pushing the boundaries of brain-signal interpretation.

\begin{figure*}[t]
     \centering
    {\includegraphics[width=0.85\textwidth]{images/flow.png}}
   \caption{General Steps from EEG Data Gathering to Stimuli Reconstruction (Image, Text, or Sound)}
	\label{fig:overall}
\end{figure*}


In light of recent breakthroughs in \emph{Generative AI}, this survey provides a scope review of recent advancements in EEG-based generative AI, with a focus on two primary  directions. The first explores \emph{how brain signals can be used to generate or reconstruct visual stimuli}, utilizing models such as GANs and Diffusion Models to decode perceptual representations. The second investigates the application of \emph{deep learning for EEG-to-text translation}, where recurrent neural network and Transformers \cite{vaswani2017attention} based language models, and contrastive learning techniques play a crucial role in learning linguistic representation. The survey also examines emerging trends in \emph{speech decoding from EEG signals and multimodal integration considerations} surrounding the use of generative AI for brain signal interpretation. Through this, we hope to provide a structured understanding of EEG-based generative AI to researchers and practitioners, offering insights to drive innovation in neural decoding, assistive technology, and brain-computer interaction.

Before proceeding, we would like to highlight that Figure~\ref{fig:overall} provides a high-level overview of the EEG-to-stimuli generation pipeline, illustrating the key stages from neural data acquisition to the generation of text, images, or audio. Additionally, Table~\ref{tab:datasets} summarizes key datasets that incorporate EEG and other modalities, serving as a valuable resource for researchers exploring multimodal neural decoding. Finally, Figure~\ref{fig:techniques} centralizes a detailed overview of the techniques and approaches covered in this survey, contextualizing their purpose and applications in EEG-based generative modeling.