\section{Conclusion}
We presented \textbf{FFTNet}, a novel approach that overcomes the inherent limitations of self-attention through adaptive spectral filtering. Our method transforms inputs into the frequency domain, leveraging Fourier theory to ensure energy preservation and efficient global mixing. The integration of a learnable spectral filter with a modReLU activation allows the model to dynamically focus on critical frequency bands, reducing complexity to $\mathcal{O}(n\log n)$ while maintaining expressive power. Extensive evaluations on LRA and ImageNet confirm that FFTNet not only achieves competitive accuracy but also significantly improves computational efficiency compared to both fixed Fourier approaches and standard self-attention. These results underscore the potential of merging rigorous theoretical foundations with adaptive learning strategies for scalable sequence modeling. Future avenues of research include incorporating our FFT-based design into state-of-the-art language modeling and large language model (LLM) incorporation.