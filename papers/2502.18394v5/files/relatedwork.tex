\section{Related Work}
\label{sec:related_work}

In this section, we review existing methods aimed at improving the efficiency of sequence models. We first discuss the complexity issues inherent in self-attention (Section~\ref{subsec:self_attention_complexity}), then highlight Fourier-based approaches (Section~\ref{subsec:fourier_based_mixing}) and other approximation techniques (Section~\ref{subsec:linear_sparse}). We then examine orthogonal matrix decomposition methods (Section~\ref{subsec:orthogonal_decompositions}), and finally position our adaptive spectral filtering method within this landscape (Section~\ref{subsec:adaptive_spectral_filtering_context}).

\subsection{Self-Attention Complexity}
\label{subsec:self_attention_complexity}
The original Transformer architecture~\cite{vaswani2017attention} uses pairwise dot-product attention, incurring a computational and memory cost of \(\mathcal{O}(n^2)\), where \(n\) is the sequence length. As \(n\) grows, this quadratic complexity quickly becomes infeasible for long sequences in tasks such as language modeling and long-context document understanding.

\subsection{Fourier-Based Mixing}
\label{subsec:fourier_based_mixing}
Fourier-based approaches leverage the Fast Fourier Transform (FFT)~\cite{cooley1965algorithm} to achieve more efficient global mixing of tokens. FNet~\cite{leethorp2022fnetmixingtokensfourier}, for example, replaces the self-attention sublayer with a fixed Fourier transform, drastically lowering computational overhead. However, the use of a static transform limits its capacity to adapt to varying inputs or highlight task-specific frequency components.

Beyond language and vision tasks, Fourier-based methods have also proven effective in modeling physical processes governed by partial differential equations. Fourier Neural Operator (FNO)~\cite{li2021fourierneuraloperatorparametric} demonstrates that neural architectures parameterized in the Fourier domain can learn solution operators for parametric PDEs, achieving resolution-invariant performance and fast inference. Although primarily designed for PDE modeling and simulation, FNO nonetheless highlights the broader potential of leveraging Fourier representations for efficient token mixing in deep learning applications.

In another line of work, Xu \emph{et al.}~\cite{xu2020learningfrequencydomain} propose learning-based frequency selection methods for large-scale vision tasks, showing that CNNs can be trained directly on partial frequency components while retaining high accuracy. Their approach offers a path to reducing data transmission overhead and computational costs without sacrificing performance.

\subsection{Linear, Sparse, and Low-Rank Approximations}
\label{subsec:linear_sparse}
Beyond Fourier methods, several alternative strategies aim to reduce the cost of self-attention. Performer~\cite{choromanski2021rethinking} and linear transformer variants~\cite{katharopoulos2020transformers} approximate the softmax attention matrix to achieve linear or near-linear complexity. Meanwhile, Reformer~\cite{kitaev2020reformer}, Linformer~\cite{wang2020linformer}, and BigBird~\cite{zaheer2020bigbird} employ sparse or low-rank approximations, extending the effective context length without paying the full quadratic price. Other approaches like Synthesizer~\cite{tay2020synthesizer} and MLP-Mixer~\cite{tolstikhin2021mlpmixer} avoid explicit token-pair interactions, replacing them with fixed or learned mixing operations.

\subsection{Convolution-Based Approaches}
\label{subsec:convolution_based_approaches}
Another line of work replaces or augments self-attention with convolutional modules to capture long-range dependencies. Methods such as Hyena~\cite{poli2023hyenahierarchylargerconvolutional} introduce a hierarchical, convolution-based framework that can scale to long input sequences while maintaining the expressive capacity of attention. Similarly, lightweight and dynamic convolutions~\cite{wu2019pay} have been proposed to reduce the cost associated with attention heads, learning per-feature or per-time-step kernels to approximate global interactions. Although these methods can exhibit favorable scaling compared to vanilla self-attention, they often rely on carefully designed convolutional architectures to handle very long contexts.

\subsection{Orthogonal Matrix Decomposition Methods}
\label{subsec:orthogonal_decompositions}
Orthogonal (or unitary) transformations provide a powerful avenue for stable and efficient sequence modeling. A key advantage of orthogonal decompositions is their norm-preserving property, which can mitigate issues such as vanishing or exploding gradients~\cite{wisdom2016full}. In the context of RNNs, unitary or orthonormal recurrent weights have been shown to preserve long-term dependencies while keeping representations stable~\cite{arjovsky2016unitary,lezcano2019cheap}. From another perspective, the discrete Fourier transform (DFT) itself is an orthonormal transformation (up to scaling) that can mix tokens globally without explicit pairwise attention.

The FFT-based approaches discussed above can be viewed as a special class of such orthonormal transforms, where the matrix is structured by the DFT. More general orthogonal transformations---whether learned or hand-crafted---have also been proposed to reduce complexity or enhance stability in modern architectures. These include fast variants of orthonormal transforms, often parameterized in ways that ensure orthogonality is preserved throughout training~\cite{lezcano2019cheap}. Within Transformers, adopting orthogonal or unitary blocks has been explored to stabilize training and capture global structure, although these methods may not always achieve the same \(\mathcal{O}(n \log n)\) cost as the FFT. Nonetheless, they highlight a broad paradigm wherein structured or parameterized orthonormal decompositions serve as efficient global mixing mechanisms.

Recently, there has also been significant progress in state space modeling approaches for sequence tasks, exemplified by S4 and its extensions. More streamlined variants such as the Simplified State Space layer (S5)~\cite{smith2023simplifiedstatespacelayers} have demonstrated that parameterizing the state space system with a diagonal transition matrix can enable efficient parallel scans over the sequence dimension while preserving high performance. These developments underscore the ongoing push to find alternatives to self-attention that can handle long sequence lengths without sacrificing expressiveness.

\subsection{Adaptive Spectral Filtering in Context}
\label{subsec:adaptive_spectral_filtering_context}
Our work diverges from both fixed Fourier-based schemes and the various attention approximations by incorporating a \emph{learnable} filter in the frequency domain. This adaptive mechanism leverages the theoretical underpinnings of FFT-based transformations---including energy preservation via Parseval's theorem---while permitting dynamic reweighting of salient frequency bands. Thus, our method maintains an \(\mathcal{O}(n \log n)\) complexity yet provides richer expressivity than fixed spectral mixing approaches. In contrast to purely approximate or sparse attention mechanisms, adaptive spectral filtering offers a direct and theoretically grounded route to capture long-range dependencies efficiently.

Notably, our approach also differentiates itself from recent work such as \cite{guibas2022adaptivefourierneuraloperators}, which introduces Adaptive Fourier Neural Operators (AFNO) for token mixing. While AFNO employs a block-diagonal structure for channel mixing and adaptive weight sharing, our method takes a distinct approach. Specifically, we compute a global context vector that directly modulates the Fourier coefficients via learnable scaling and bias parameters, followed by a nonlinear modReLU activation. This design not only preserves the computational advantages of FFT-based mixing but also enhances expressivity by dynamically emphasizing salient frequency bands based on the input. As such, our adaptive spectral filtering strategy offers a complementary alternative to AFNO, merging efficiency with adaptive, context-dependent feature modulation.

Overall, while approaches such as FNet, Performer, and sparse transformers demonstrate that either fixed or approximate token mixing can reduce computational overhead, our adaptive spectral filtering strategy uniquely merges the efficiency of the FFT with a learnable, input-dependent spectral filter. This provides a compelling combination of scalability and adaptability, which is crucial for complex sequence modeling tasks.

