\section{Experiments}
\label{sec:experiments}

% One simple way to reduce spacing before/after subsections is to manually insert negative vertical space
% immediately after each \subsection command:
% \vspace{-2mm}
% Adjust the -2mm to whatever reduction you prefer.

We present a comprehensive evaluation of our proposed approach, \textbf{FFTNet}, comparing it against both \textbf{FNet} and traditional Transformers based on self-attention. Our experiments cover the Long Range Arena (LRA) benchmark~\cite{tay2021long} and ImageNet classification. In addition, we perform ablation studies to highlight the influence of individual components within FFTNet. We compare against the ``standard'' variants of the ViT~\cite{dosovitskiy2021imageworth16x16words} and transformer~\cite{vaswani2017attention}, so as to form an ``apples-to-apples'' comparison. 

\subsection{Training Protocol}
\vspace{-2mm}
All models are trained using the AdamW optimizer with a cosine decay schedule, employing an initial warmup phase for the learning rate. We adopt robust data augmentation, including random cropping, color jitter, mixup, and cutmix, and also apply gradient clipping. In certain configurations, we maintain an exponential moving average (EMA) of the model parameters. Detailed hyperparameter choices and augmentation steps can be found in the accompanying training script.

\subsection{Long Range Arena (LRA) Benchmark}
\vspace{-2mm}
We evaluate on six LRA tasks: \emph{ListOps}, \emph{Text}, \emph{Retrieval}, \emph{Image}, \emph{Pathfinder}, and \emph{Path-X}. Accuracy results for each task, as well as the mean accuracy across all tasks, appear in Table~\ref{tab:lra}. 
Here, the \textbf{standard Transformer} serves as the baseline for computing improvements/drops. 
``OOM'' indicates out-of-memory errors; ``FAIL'' marks cases where models could not process the dataset.

\begin{table}[!htbp]
    \centering
    \begin{adjustbox}{width=\textwidth}
    \begin{threeparttable}
    \caption{Accuracy (\%) on the Long Range Arena (LRA) benchmark tasks. Arrows indicate the performance difference relative to the \textbf{standard Transformer}.}
    \label{tab:lra}
    \rowcolors{2}{gray!10}{white}
    \begin{tabular}{l|cccccc|c}
        \toprule
        \textbf{Model} & \textbf{ListOps} & \textbf{Text} & \textbf{Retrieval} & \textbf{Image} & \textbf{Pathfinder} & \textbf{Path-X} & \textbf{Avg.} \\
        \midrule
        Transformer & 36.06 & 61.54 & 59.67 & 41.51 & 80.38 & OOM & 55.83 \\
        FNet &
            35.33\drop{0.7} & 
            65.11\imp{3.6} & 
            59.61\drop{0.1} & 
            38.67\drop{2.8} & 
            77.80\drop{2.6} & 
            FAIL & 
            55.32\drop{0.5} \\
        \textbf{FFTNet (No-Windowing)} &
            37.65\imp{1.6} &
            66.01\imp{4.5} &
            60.21\imp{0.5} &
            42.02\imp{0.5} &
            80.71\imp{0.3} &
            83.25 & 
            58.31\imp{2.5} \\
        \textbf{FFTNet (With Windowing)} &
            38.02\imp{2.0} &
            66.25\imp{4.7} &
            60.64\imp{1.0} &
            42.45\imp{0.9} &
            80.99\imp{0.6} &
            83.64 &
            \textbf{58.83}\imp{3.0} \\
        \bottomrule
    \end{tabular}
    \end{threeparttable}
    \end{adjustbox}
\end{table}

As shown, even the \textbf{FFTNet (No-Windowing)} version outperforms both baselines (Transformer and FNet) on most tasks. Adding local windowing (STFT) further improves the accuracy slightly, achieving the highest average score.

\subsection{ImageNet Classification}
We further assess our FFTNetViT variants on ImageNet, comparing them against standard ViT baselines. Table~\ref{tab:imagenet} provides FLOPs (in GFLOPs), Top-1 accuracy, and Top-5 accuracy for Base, Large, and Huge model sizes. Although FFTNetViT typically uses fewer FLOPs, the \emph{No-Windowing} variant is slightly worse than the standard ViT. By contrast, adding local windowing (STFT) brings a small but consistent accuracy boost, surpassing the ViT baseline.

\begin{table}[!htbp]
    \centering
    \begin{adjustbox}{width=\textwidth}
    \begin{threeparttable}
    \caption{Comparison on ImageNet for Base, Large, and Huge variants. FLOPs are in GFLOPs. 
    ``No-Windowing'' vs. ``With Windowing'' are two configurations of \textbf{FFTNetViT}.}
    \label{tab:imagenet}
    \rowcolors{2}{gray!10}{white}
    \scriptsize
    \begin{tabular}{lccccccccc}
        \toprule
         & \multicolumn{3}{c}{\textbf{FFTNetViT (No-Windowing)}} & \multicolumn{3}{c}{\textbf{FFTNetViT (With Windowing)}} & \multicolumn{3}{c}{\textbf{ViT}} \\
        \cmidrule(lr){2-4}\cmidrule(lr){5-7}\cmidrule(lr){8-10}
         \textbf{Variant} & FLOPs & Top-1 (\%) & Top-5 (\%) & FLOPs & Top-1 (\%) & Top-5 (\%) & FLOPs & Top-1 (\%) & Top-5 (\%)\\
        \midrule
        Base  & 22.64  & 79.2\drop{0.2} & 94.7\drop{0.1} 
              & 22.64  & 79.8\imp{0.4}  & 95.0\imp{0.2}  
              & 36.65  & 79.4 & 94.8 \\
        Large & 79.92  & 81.5\drop{0.3} & 95.9\drop{0.1} 
              & 79.92  & 82.3\imp{0.5}  & 96.3\imp{0.3}  
              & 127.18 & 81.8 & 96.0 \\
        Huge  & 166.14 & 82.7\drop{0.2} & 96.5\drop{0.1}
              & 166.14 & 83.4\imp{0.5}  & 96.9\imp{0.3}
              & 261.39 & 82.9 & 96.6 \\
        \bottomrule
    \end{tabular}
    \end{threeparttable}
    \end{adjustbox}
\end{table}

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.85\textwidth]{imgs/combined_latency_comparison.pdf}
    \caption{Latency comparison (No-Windowing) of FFTNetViT vs.\ standard ViT for various batch sizes on ImageNet. FFTNetViT typically exhibits better scaling properties.}
    \label{fig:latency}
\end{figure}

\textbf{Throughput and Speedup Analysis (No-Windowing).}
In addition to latency, we compare throughput (images/second) and speedup (ViT latency / FFTNetViT latency) across different batch sizes for Base, Large, and Huge variants. Figure~\ref{fig:throughput-speedup} displays these two plots side by side, illustrating: \\
\ballnumber{1}\textbf{ Throughput:} FFTNetViT variants consistently process more images per second than standard ViT, and the gap widens at larger batch sizes. 
\ballnumber{2}\textbf{ Speedup:} The ratio of ViT latency to FFTNetViT latency is generally above 1, confirming that FFT-based variants are faster. This advantage becomes more pronounced as the batch size increases.

\begin{figure}[!htbp]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{imgs/combined_throughput_comparison.pdf}
        \caption{Throughput (images/second) for FFTNetViT vs.\ standard ViT.}
        \label{fig:throughput}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{imgs/combined_speedup_comparison.pdf}
        \caption{Speedup (ViT latency / FFTNetViT latency) for FFTNetViT vs.\ standard ViT.}
        \label{fig:speedup}
    \end{subfigure}
    \caption{Comparison of throughput (left) and speedup (right) across various batch sizes (No-Windowing) for Base, Large, and Huge variants of FFTNetViT vs.\ standard ViT.}
    \label{fig:throughput-speedup}
\end{figure}

\subsection{Ablation Studies}
\vspace{-2mm}
Ablation studies on ImageNet (Base variant, No-Windowing) were conducted to isolate key FFTNet components. Four configurations are examined. The first is the \emph{full} FFTNet model (baseline in Table~\ref{tab:ablation}), which includes spectral gating, the adaptive module, and FFT-based mixing. The other rows each omit or replace one key component, showing how it affects Top-1 accuracy.

\begin{table}[!htbp]
    \centering
    \begin{adjustbox}{width=0.98\textwidth}
    \begin{threeparttable}
    \caption{Ablation experiments on ImageNet (Base variant, No-Windowing). The \textbf{FFTNet (full)} row is the baseline. Arrows show accuracy drops relative to it.}
    \label{tab:ablation}
    \rowcolors{2}{gray!10}{white}
    \begin{tabular}{l|c|l}
        \toprule
        \textbf{Variant} & \textbf{Top-1 Acc (\%)} & \textbf{Observation} \\
        \midrule
        FFTNet (full)                            & 79.2 & Baseline: spectral gating, adaptive module, FFT-based filtering \\
        Without spectral gating                   & 77.8\drop{1.4} & Omitting gating causes a moderate performance decline. \\
        Without adaptive module                   & 77.2\drop{2.0} & Disabling the adaptive module degrades accuracy further. \\
        Replace FFT with convolution             & 75.4\drop{3.8} & Using a standard convolution causes the largest drop. \\
        \bottomrule
    \end{tabular}
    \end{threeparttable}
    \end{adjustbox}
\end{table}

All components contribute to performance, with substituting the FFT layer for a convolution causing the largest accuracy drop.
