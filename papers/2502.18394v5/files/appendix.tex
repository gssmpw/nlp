\section{Appendix}
\subsection{GPU Utilization and Scaling Analysis}
To further illustrate the performance characteristics of our approach, we show additional plots capturing GPU utilization, FLOPs achieved, latency, and throughput under different configurations. Specifically, the next figures highlight how increasing data size (e.g., batch size or input resolution) can better saturate the GPU, leading to improved scaling until the model becomes bandwidth-constrained, after which performance gains plateau.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.75\textwidth]{imgs/achieved_gflops_subplots.pdf}
    \caption{Achieved GFLOPs under varying input sizes/batch configurations. Larger data sizes utilize the GPU more effectively and improve model scaling. As the lines flatten, the GPU is becoming bandwidth-limited.}
    \label{fig:achieved_gflops}
\end{figure}

\begin{figure}[!htbp]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{imgs/ddp_latency_32.pdf}
        \caption{DDP Latency (32).}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{imgs/ddp_latency_224.pdf}
        \caption{DDP Latency (224).}
    \end{subfigure}
    \caption{DDP Latency for two different input sizes. As input size increases, the GPU is better utilized and the model scales more efficiently. The flattening of the line indicates bandwidth is becoming the limiting factor.}
    \label{fig:ddp_latency}
\end{figure}

\begin{figure}[!htbp]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{imgs/ddp_throughput_32.pdf}
        \caption{DDP Throughput (32).}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{imgs/ddp_throughput_224.pdf}
        \caption{DDP Throughput (224).}
    \end{subfigure}
    \caption{DDP Throughput at two input sizes. Larger data sizes lead to better GPU utilization, but once bandwidth constraints kick in, scaling gains flatten.}
    \label{fig:ddp_throughput}
\end{figure}

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.75\textwidth]{imgs/speedup_comparison_subplots.pdf}
    \caption{Speedup comparison subplots. When the data size is smaller, the GPU is underutilized. As the data size grows, speedup increases until bandwidth constraints force the line to flatten.}
    \label{fig:speedup_subplots}
\end{figure}

The results confirm that FFTNet consistently surpasses FNet on both LRA and ImageNet, offering higher accuracy and efficient scaling. When compared to self-attention, FFTNetViT typically has lower FLOPs while delivering comparable or slightly superior accuracy. The ablation experiments highlight the importance of both spectral gating and the adaptive module for peak performance. Additionally, as the data size increases, the GPU is utilized more efficiently, leading to better throughput and speedup. Once the lines begin to flatten, it indicates that the GPU has become bandwidth-constrained, limiting further scaling gains.
