\section{Introduction}
\label{sec:introduction}
Conventional self-attention mechanisms capture global interactions through explicit token-pair computations, resulting in quadratic complexity \(\mathcal{O}(n^2)\). This high cost becomes prohibitive for tasks involving lengthy sequences, such as long-context language modeling or large-scale image processing.

In this paper, we introduce an adaptive spectral filtering framework that leverages the Fast Fourier Transform (FFT) to perform global token mixing with \(\mathcal{O}(n\log n)\) efficiency. By converting the input sequence into the frequency domain, we harness orthogonal frequency components to naturally capture long-range dependencies. This frequency-domain representation not only reduces computational overhead but also preserves the original signal's energy due to Parseval's theorem.

A distinguishing feature of our approach is a learnable spectral filter that modulates the Fourier coefficients based on a global context vector. This adaptive mechanism selects which frequency bands are most relevant, allowing the model to dynamically focus on critical components of the input. Additionally, introducing nonlinear activations to both the real and imaginary parts of the signal extends the representational capacity beyond linear transformations, enabling the model to capture more complex interactions.

Overall, our adaptive spectral filtering framework unifies the efficiency of FFT-based methods with a powerful, input-dependent spectral filter and nonlinear feature modulation. This combination offers a computationally efficient and expressive alternative to traditional self-attention, promising robust handling of long-range dependencies in various sequence modeling tasks.
