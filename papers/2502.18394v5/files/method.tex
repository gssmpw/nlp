\section{Adaptive Spectral Filtering Method}
\label{sec:adaptive_spectral_filtering}

We present an \emph{adaptive spectral filtering} framework that avoids explicit pairwise interactions while retaining global mixing capabilities. Rather than computing dot-product self-attention, this approach applies the discrete Fourier transform (DFT) to capture long-range dependencies in \(\mathcal{O}(n \log n)\) time. By adaptively reweighting the resulting frequency components and then applying an inverse transform, the method achieves an effective balance of expressive power and computational efficiency, backed by solid theoretical foundations.

\subsection{Motivation}
Traditional attention mechanisms incur quadratic cost in the sequence length \(n\) when computing pairwise token interactions. As \(n\) increases, this quickly becomes expensive. By contrast, the DFT naturally encodes global interactions in \(\mathcal{O}(n \log n)\) time, since it decomposes the token sequence into orthogonal frequency components. To increase representational power, we introduce a learnable filter in the frequency domain that emphasizes or attenuates particular frequency bands based on a global context.

\paragraph{Why Local Windowing (or Wavelets)?}
While the global DFT branch is excellent for capturing large-scale dependencies, it can blur fine-grained or short-range signals. \emph{Local windowing (STFT) is optional, but it can help} preserve the high-resolution structure within shorter segments. In fact, \emph{wavelet transforms} can also be used here instead of or alongside STFT, offering a multi-resolution perspective that naturally adapts to local signal changes. When used, merging these \emph{local} (STFT or wavelets) and \emph{global} representations yields a model capable of both wide-ranging context and precise local detail. If purely global context suffices, one may omit local windowing or wavelet transforms altogether.

\subsection{Method Description}
Let \(\mathbf{X} \in \mathbb{R}^{n \times d}\) be an input sequence of length \(n\) and embedding dimension \(d\). Our method integrates both \emph{global} and \emph{local} spectral representations to capture wide-ranging dependencies and fine-grained context. The approach proceeds in five main steps:

\ballnumber{1}\textbf{ Global Fourier Transform.}\\
Apply the discrete Fourier transform (FFT) \emph{across the entire sequence}:
\[
\mathbf{F}_{\mathrm{global}} \;=\; \operatorname{FFT}(\mathbf{X}) \;\in\; \mathbb{C}^{n \times d},
\]
which captures long-range dependencies without explicit pairwise comparisons.

\ballnumber{2 }\textbf{ Local Windowing (STFT) or Wavelets. \emph{(Optional)}}\\
In parallel, and \emph{optionally}, perform a short-time Fourier transform (STFT) \emph{or} a wavelet transform for local context:
\begin{enumerate}
    \item \textbf{For STFT:} Partition the input \(\mathbf{X}\) into overlapping windows of length \(w\). Let each window be denoted by \(\mathbf{X}^{(m)} \in \mathbb{R}^{w \times d}\), where \(m\) indexes the window.
    \item Apply a window function (e.g., Hann or Hamming) to each segment.
    \item Compute the FFT for each window to obtain local frequency components:
    \[
      \mathbf{F}_{\mathrm{local}}^{(m)} = \operatorname{FFT}\bigl(\mathbf{X}^{(m)}\bigr).
    \]
    \item \textbf{Alternatively (Wavelets):} Decompose each local segment using a chosen wavelet family (e.g.\ Daubechies) to capture time-varying frequency information at multiple scales.
\end{enumerate}
These local spectra or wavelet coefficients emphasize short-range patterns that might be blurred in a purely global transform. If the application does not require high-resolution local detail, this step can be omitted.

\ballnumber{3}\textbf{ Merging Global and Local Representations.}\\
To \emph{maximize information flow} from both branches, we propose an \emph{isometric fusion} approach in the frequency (or transform) domain. First, concatenate the global and local representations (spectra or wavelet coefficients) along their feature dimension:\footnote{If multiple local windows exist, they may be pooled or aggregated (e.g.\ average or max) before concatenation, or one can apply a deeper fusion network to combine them.}
\[
\hat{\mathbf{F}} \;=\; \bigl[\mathbf{F}_{\mathrm{global}} \;\;\|\;\; \mathbf{F}_{\mathrm{local}}\bigr] 
\;\in\; \mathbb{C}^{n \times (2d)},
\]
where \(\mathbf{F}_{\mathrm{local}}\) can be a representative fusion of all local windows (or wavelet scales).\footnote{If retaining every window or wavelet scale separately, this dimension becomes \(n \times (d + \sum_m d_{\mathrm{local}}^{(m)})\).}

Next, apply a learnable matrix 
\(\mathbf{U} \in \mathbb{C}^{(2d) \times d}\) 
that fuses the concatenated features into a unified spectral representation:
\[
\mathbf{F} \;=\; \hat{\mathbf{F}} \,\mathbf{U}.
\]
In the simplest form, \(\mathbf{U}\) can be unconstrained and learned end-to-end. However, to \emph{preserve} (and thus maximize) the signal energy from both branches, \(\mathbf{U}\) may be constrained to be \emph{isometric} (or approximately isometric), satisfying \(\mathbf{U}^* \mathbf{U} = \mathbf{I}\). This ensures that no information is lost during the fusion step, effectively providing a richer merge than a simple gating or element-wise sum.

\paragraph{Gating Instead of Concatenation (When Resources Are Limited).}
If memory or computational overhead from concatenation is a concern, one may \emph{gate} the local and global transforms instead. For example, one can learn a scalar or vector gate \(\sigma(\mathbf{c})\) (where \(\mathbf{c}\) is a global context) that modulates:
\[
\hat{\mathbf{F}} \;=\; \sigma(\mathbf{c}) \,\odot\, \mathbf{F}_{\mathrm{global}} \;+\; \bigl(1-\sigma(\mathbf{c})\bigr) \,\odot\, \mathbf{F}_{\mathrm{local}}.
\]
In this case, the fusion dimension is unchanged and can be more lightweight, albeit at the cost of discarding some of the richer information that the concatenation-based approach retains.

\ballnumber{4}\textbf{ Adaptive Spectral Filtering.}\\
Compute a global context vector
\[
\mathbf{c} \;=\; \frac{1}{n} \sum_{i=1}^{n} \mathbf{X}_i
\]
and feed it into a small MLP to generate scaling and bias parameters, \(\Delta \mathbf{s}\) and \(\Delta \mathbf{b}\), for each frequency bin and attention head. These parameters modify a base filter and bias via
\[
\mathbf{W} \;=\; \mathbf{W}_{\mathrm{base}} \odot (1 + \Delta \mathbf{s}), 
\quad
\mathbf{b} \;=\; \mathbf{b}_{\mathrm{base}} + \Delta \mathbf{b},
\]
reweighting the complex coefficients:
\[
\tilde{\mathbf{F}} \;=\; \mathbf{F} \odot \mathbf{W} + \mathbf{b}.
\]
Here, \(\mathbf{F}\) includes both local and global frequency (or wavelet) information from the isometric fusion step (or the gated mixture).

\ballnumber{5}\textbf{ Nonlinear Activation (modReLU) and Inverse FFT.}\\
To enrich the representation, we apply \emph{modReLU} \cite{arjovsky2016unitary} on each complex coefficient \(z = re^{i\theta}\). This preserves the phase while applying a threshold to the magnitude:
\[
\operatorname{modReLU}(z) 
\;=\;
\begin{cases}
(r + b)\,e^{i\theta}, & \text{if } r + b > 0,\\[3pt]
0, & \text{otherwise}.
\end{cases}
\]
Finally, if using FFT-based local features, we map back to the token domain via the inverse FFT:
\[
\mathbf{Y} \;=\; \operatorname{IFFT}\bigl(\tilde{\mathbf{F}}\bigr) \;\in\; \mathbb{R}^{n \times d}.
\]
For wavelet-based features, one may similarly invert them if a full reconstruction in the token domain is desired. The resulting output is a globally mixed representation that also incorporates local spectral (or wavelet) features (if local transforms were used).

\subsection{Efficient Mixing}
Using the FFT to encode global interactions has several advantages. It performs global mixing in \(\mathcal{O}(n \log n)\) time, contrasting with the \(\mathcal{O}(n^2)\) cost of self-attention. Meanwhile, local STFT or wavelets capture short-range structure that may be lost in a purely global transform. By \emph{isometrically} merging (or gating) local and global frequency components, we fuse high-resolution local context with a global view, all in roughly \(\mathcal{O}(n \log n)\) time.

The adaptive filter serves as an implicit frequency-domain mask, conditioned on global context rather than explicit pairwise comparisons. Nonlinear activation in the complex domain (modReLU) further enriches representational capacity by capturing higher-order structure. Parseval's theorem and the unitary properties of the DFT ensure that this transformation preserves both energy and inner-product structure.

\subsection{Parseval's Theorem}
Parseval's theorem asserts that the total energy of a signal remains constant under the Fourier transform, aside from a constant scaling factor. For an input sequence \(\mathbf{X} \in \mathbb{R}^{n \times d}\) and its Fourier transform \(\mathbf{F} = \operatorname{FFT}(\mathbf{X})\), the theorem states:
\[
\|\mathbf{X}\|_2^2 \;=\; \frac{1}{n}\|\mathbf{F}\|_2^2.
\]
This energy preservation is crucial for our method as it guarantees that the adaptive filtering and subsequent nonlinear operations do not inadvertently distort the input signal's inherent information.

\subsection{Orthogonal Decomposition and Unitary Property of the DFT}
The DFT can be represented by a matrix \(\mathbf{F}_n \in \mathbb{C}^{n \times n}\) with entries given by 
\[
[\mathbf{F}_n]_{j,k} \;=\; e^{-2\pi i (j-1)(k-1)/n}.
\]
This matrix is unitary up to a scaling factor, satisfying:
\[
\mathbf{F}_n^* \mathbf{F}_n \;=\; n\,\mathbf{I},
\]
where \(\mathbf{F}_n^*\) is the conjugate transpose and \(\mathbf{I}\) is the identity matrix. This property implies that the transformation preserves inner products:
\[
\langle \mathbf{X}_i, \mathbf{X}_j \rangle \;=\; \frac{1}{n}\,\langle \mathbf{F}_i, \mathbf{F}_j \rangle.
\]
Thus, every token in the input contributes to every frequency component, enabling effective global mixing akin to self-attention but at a significantly reduced computational cost.

\subsection{Proof of Computational Complexity}
\label{sec:proof_complexity}

\paragraph{Global FFT and Inverse FFT.}
For \(\mathbf{X} \in \mathbb{R}^{n \times d}\), the full-sequence FFT and IFFT each cost \(\mathcal{O}(n \log n)\) per channel, leading to \(\mathcal{O}(d\,n \log n)\) overall.

\paragraph{Local Windowing (STFT) or Wavelets. \emph{(Optional)}}
If each STFT window has length \(w\) and we assume an overlap of \(\alpha w\) tokens, the number of windows is approximately \(\frac{n}{(1-\alpha)w}\). Each windowed FFT thus costs \(\mathcal{O}(w \log w)\) for each of the \(d\) channels. The total cost is:
\[
\mathcal{O}\!\Bigl(\tfrac{n}{(1-\alpha)w} \;d\; w \log w\Bigr).
\]
For wavelets, the complexity is usually \(\mathcal{O}(n)\) or \(\mathcal{O}(n \log n)\) depending on the chosen algorithm and wavelet family, often comparable to STFT approaches. These steps are optional and can be skipped if local structure is not critical for the task.

\paragraph{Isometric Fusion and Adaptive Filtering.}
Merging local and global representations with an isometric (or unconstrained) matrix \(\mathbf{U}\) is an \(\mathcal{O}(n \, d^2)\) step when viewed as a matrix multiplication in \(\mathbb{C}^{n \times (2d)}\). This is generally modest compared to the \(\mathcal{O}(d\,n \log n)\) term for large \(n\). Adaptive filtering, which includes the MLP for context-based parameter generation, similarly contributes \(\mathcal{O}(d\,n)\).

\paragraph{Overall Complexity.}
Because \(\mathcal{O}(d\,n \log n)\) typically dominates the cost of element-wise operations for large \(n\), the total complexity remains \(\mathcal{O}(d\,n \log n)\). For practical configurations, this effectively behaves as \(\mathcal{O}(n \log n)\).

\begin{figure}
    \centering
    \includegraphics[width=0.92\textwidth]{imgs/tik.pdf}
    \caption{Overall Process of FFTNet with Local Windowing (STFT or wavelets, optional) and Global FFT, followed by isometric fusion (or gating) in the frequency/transform domain.}
    \label{fig:fft}
\end{figure}

\subsection{Comparison with Convolution-Based Methods}
Although a sufficiently large convolution kernel can span the entire sequence, it generally lacks the adaptability of self-attention unless the kernel itself is dynamically generated. Our approach bridges this gap by using a data-dependent, learnable filter in the frequency domain, guided by global context and applied through modReLU. Because frequency-domain multiplication corresponds to convolution in the token domain, the model can realize global, context-aware mixing more flexibly than standard convolutional methods. Local windowing or wavelet transforms further refine the representation without resorting to large kernel sizes.

\subsection{Comparison with FNet}
FNet \cite{leethorp2022fnetmixingtokensfourier} similarly uses the DFT for token mixing but employs no learnable parameters in the mixing step. In contrast, our method includes a context-dependent, adaptive filter and a nonlinear activation that both enhance expressivity and allow emphasis on specific frequency components. Further, we incorporate a local STFT or wavelet pathway to capture fine-grained interactions. Despite these adaptive steps, the overall complexity remains \(\mathcal{O}(n \log n)\), dominated by the FFT itself.

\subsubsection{Mitigating Complexity in the Adaptive MLP}
For very long sequences, generating frequency-bin parameters can still be demanding. Possible solutions include \ballnumber{1} subsampling or pooling the frequency representation before the MLP, \ballnumber{2} sharing parameters among adjacent frequency bins, and \ballnumber{3} applying low-rank factorization to reduce dimensionality. These techniques help maintain a reasonable balance between the expressive power of the adaptive filter and computational cost.

\bigskip

\noindent
\textbf{Conclusion.} 
Adaptive spectral filtering integrates frequency-domain global mixing, data-driven filtering, and nonlinear activation into a unified framework. By adding an \emph{optional} local transform path (STFT or wavelets) and merging (or gating) it with the global FFT via an isometric fusion, the model captures both short-range and long-range interactions with \(\mathcal{O}(n \log n)\) efficiency. Parseval’s theorem and the orthogonal properties of the DFT ensure stable transformations, enabling many benefits of self-attention at a reduced computational cost. Full implementation details are available at: 
\url{https://github.com/jacobfa/fft}.
