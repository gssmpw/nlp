\section{Small Model Learnability Gap}
\label{sec: empirical}
In this section, we fine-tune student models using different CoT data. 
We then reveal the small model learnability gap given the performance of fine-tuned models.

\subsection{Experiment Setup}

\paragraph{Datasets.}
We use the 7,500 prompt set of MATH \citep{hendrycks2021measuringmathematicalproblemsolving}. This dataset encompasses seven math topics such as advanced calculus, geometry, and linear algebra. 





\paragraph{Student models.}
Our study considers ten student models from the Qwen \citep{qwen2.5} and Llama \citep{llama32,llama31} model families of varying sizes. These models include the Instruct version of \texttt{Qwen2.5-0.5B}, \texttt{Qwen2.5-1.5B}, \texttt{Qwen2.5-3B}, \texttt{Qwen2.5-7B}, \texttt{Qwen2.5-14B}, and \texttt{Qwen2.5-32B}, and the Instruct version of \texttt{Llama3.2-1B}, \texttt{Llama3.2-3B}, \texttt{Llama3.1-8B}, and \texttt{Llama3.3-70B}. A comprehensive overview of the student models is presented in Table \ref{tab:models_overview} of Appendix \ref{appendix:More on Experimental Setups}. 


\paragraph{Teacher models.} 


To compare long CoT with short CoT, we use \texttt{QwQ-32B-Preview} \citep{QwenTeam2024b} to generate long CoT sequences and \texttt{Qwen2.5-32B-Instruct} as the response generator for short CoT. 
Within each model family, we designate the larger scale model as the large teacher and the smaller scale model as the small teacher. 
This includes \texttt{Qwen2.5-72B-Instruct} vs \texttt{Qwen2.5-\allowdisplaybreaks3B-Instruct},~\texttt{Llama3.1-70B-Instruct} vs~\texttt{Llama3.1-8B-Instruct},~and \texttt{Gemma2-27B-it} vs \texttt{Gemma2-9B-it}.



\begin{figure*}[!t]
    \centering
    \includegraphics[width=\textwidth]{figs/long_cot_combined_model_performance.pdf}
    \caption{Long CoT Gap ($\Delta_{Long}=P_{Long} - P_{Short}$) of student models with different models sizes for (a) Qwen family (b) Llama family. For teacher models, \texttt{QwQ-32B-Preview} is chosen to generate long CoT responses, while \texttt{Qwen2.5-32B-Instruct} is chosen to generate short CoT responses. Negative (Positive) $\Delta_{Long}$ indicates that long CoT is worse (better) than short CoT. Our results demonstrate that short CoT is better for smaller student models (indicated by $\Delta_{Long}$ < 0), while long CoT is better for larger student models (indicated by $\Delta_{Long}$ > 0).}
    % \vspace{-1em}
    \label{fig:combined_model_performance}
\end{figure*}
\begin{figure*}[!t]
    \centering
    \includegraphics[width=\textwidth]{figs/lmp_combined_model_performance.pdf}
    \caption{Large model CoT Gap ($\Delta_{Large}=P_{Large} - P_{Small}$) of student models with different models sizes for (a) Qwen family (b) Llama family. For teacher models, \texttt{Qwen2.5-72B-Instruct} is chosen as the large teacher to generate responses, while \texttt{Qwen2.5-3B-Instruct} is chosen as the small teacher to generate responses. Negative (positive) $\Delta_{Large}$ indicates that large teacher CoT is worse (better) than small teacher CoT. Our results demonstrate that small teacher CoT is better for smaller student models (indicated by $\Delta_{Large}$ < 0), while large model CoT is better for larger student models (indicated by $\Delta_{Large}$ > 0).}
    \label{fig:lmp_combined_model_performance}
\end{figure*}

\paragraph{Evaluation Benchmarks.} 


We evaluate the reasoning capability of fine-tuned student models on a set of commonly used benchmarks, including MATH \citep{hendrycks2021measuringmathematicalproblemsolving}, GSM8K \citep{cobbe2021trainingverifierssolvemath}, AMC 2023, AIME 2024, and the English math subset of OlympiadBench \citep{he2024olympiadbenchchallengingbenchmarkpromoting}. These benchmarks span a wide range of challenge levels, from elementary mathematics to advanced competition problems. 
We define the student model performance as the average score on five benchmarks. Unless otherwise specified, all fine-tuned models are evaluated in a zero-shot setting using greedy decoding. We set the maximum generation tokens as 16k. Please see Appendix \ref{appendix:More on Experimental Setups} for detailed experimental setup.

We define the following performance scores:
\begin{itemize}
    \item \(P_{Long}\): Performance score of a student model fine-tuned on long CoT data.
    \item \(P_{Short}\): Performance score of a student model fine-tuned on short CoT data.
    \item \(P_{Large}\): Performance score of a student model fine-tuned on CoT from a larger teacher.
    \item \(P_{Small}\): Performance score of a student model fine-tuned on CoT from a smaller teacher.
\end{itemize}




\paragraph{Training Setup.} 
Teacher models generate responses by rejection sampling 
\citep{dong2023raftrewardrankedfinetuning, gulcehre2023reinforcedselftrainingrestlanguage, tong2024dartmathdifficultyawarerejectiontuning, yuan2023scalingrelationshiplearningmathematical, yue2023mammothbuildingmathgeneralist, zelikman2022starbootstrappingreasoningreasoning}
By default, teacher models employ greedy decoding.  
By combining the math problem instructions with corresponding solutions generated by teacher models, we construct problem-solution pairs to fine-tune student models. 
We train the models using the LLaMA-Factory framework \citep{zheng2024llamafactory}. 
For student models of scale less than 14B, we use full-parameter SFT and implement a cosine learning rate schedule with a maximum learning rate of $10^{-5}$ to fine-tune student models for two epochs \citep{touvron2023llama}. 
For student models larger than 14B, we adopt LoRA fine-tuning with a learning rate of $10^{-4}$ for two epochs. Detailed hyperparameters and information about the experimental platform are provided in Appendix \ref{appendix:More on Experimental Setups}.





\subsection{Long CoT Gap}
This section evaluates the reasoning capabilities of student models fine-tuned over long CoT data and short CoT data. We quantify the performance difference between long and short CoT data using \emph{long CoT gap} \(\Delta_{Long}\), defined as:
\begin{equation*}
\Delta_{Long} = P_{Long} - P_{Short}.
\end{equation*}

Figure \ref{fig:combined_model_performance} provides a comprehensive overview of the long CoT gap $\Delta_{Long}$ across different student models. 
The detailed benchmark scores on MATH, GSM8K, AIME, AMC, and OlympiadBench are deferred to Table \ref{tab:full_performance_lg} in Appendix \ref{appendix: More Experiments}.
We report the following key takeaways. 

\begin{AIbox}{Takeaway 1: Long CoT Gap}
Small student models tend to benefit more from short CoT, while large student models gain greater advantages from long CoT.

\end{AIbox}

We observe that long CoT is more effective for larger models, consistently leading to improved performance across most math benchmarks. 
For example, the student model \texttt{Qwen2.5-32B-Instruct} improves about 15 points across all math metrics on average. 

However, long CoT data is not effective for smaller models, yielding significantly less improvement compared to short CoT. On the MATH and AMC benchmarks, student model \texttt{Qwen2.5-1.5B-Instruct}  performs over 10 points lower when fine-tuned with long CoT data. This shows that smaller models may not be able to effectively learn and utilize the long CoT paradigm. 
Please see more attribution analysis in Section \ref{More Analysis Results}.



\input{tables/long_cot_compare}
\input{tables/lmp_compare}





\subsection{Large Teacher CoT Gap}
We investigate how effective small models may learn from large teacher and small teachers.
We define a \emph{large teacher CoT gap} as:
\[
\Delta_{Large} = P_{Large} - P_{Small}.
\]

Figure \ref{fig:lmp_combined_model_performance} provides a comprehensive comparison of the $\Delta_{Large}$ incurred by all student models. 
The detailed benchmark scores of MATH, GSM8K, AIME, AMC and OlympiadBench are deferred to Table \ref{tab:lmp-full_comparison} in Appendix \ref{appendix: More Experiments}. More experimental results of different teacher models, including \texttt{Llama3.1-70B} vs \texttt{Llama3.1-8B} and \texttt{Gemma2-27B} vs \texttt{Gemma2-9B} are in Table \ref{tab:lmp_comparison2} of Appendix \ref{appendix: More Experiments}.

We observe that larger student models learn effectively from large teacher CoT. 
For example, \texttt{Qwen2.5-7B-Instruct} and \texttt{Qwen2.5-32B-Instruct} student models improve over 5 points on average, with \texttt{Qwen2.5-32B-Instruct} achieving more than a 15 point increase on the AIMC benchmark. 
However, smaller models do not learn effectively from large teacher models such as \texttt{Qwen2.5-72B-Instruct}. 
Instead, small teacher models such as \texttt{Qwen2.5-3B-Instruct} may serve as better teacher models for small student models.
For instance, the performance of \texttt{Qwen2.5-0.5B-Instruct} degrades by more than 10 points on the AMC benchmark.


Note that prior studies \citep{kim2024evaluatinglanguagemodelssynthetic} also demonstrated that stronger models are not necessarily stronger teachers, 
emphasizing response generator and teacher-side factors. Our work differs in that we attribute this phenomenon primarily to the size of the student model.


\begin{AIbox}{\makecell{Takeaway 2: Large Teacher CoT Gap}}
Small student models tend to learn better from small teachers, while large student models benefit more from large teachers.
\end{AIbox}



\input{tables/mix_distilation_qwen}

\subsection{Analysis of Small Model Learnability Gap}
\label{More Analysis Results}


\paragraph{Domain knowledge affects learnability gap.}

We observe that math expert models, in spite of small model size,  exhibit a smaller learnability gap for both long CoT and large teacher CoT data compared to general models in Figure \ref{fig:math_expert_vs_general}. 
Specifically, we compare the learnability gaps between the student models \texttt{Qwen2.5-Math-1.5B-Instruct} and \texttt{Qwen2.5-1.5B-Instruct}. Our findings show that the long CoT gap of the small math expert model is significantly smaller than that of general small models. 
Furthermore, the performance improvement of \texttt{Qwen2.5-Math-1.5B} when fined-tuned with large teacher CoT exceeds that of \texttt{Qwen2.5-1.5B}, suggesting that math expert models benefit more substantially from large teacher CoT. We conjecture that a key factor leading to the small model learnability gap is the \textit{limited in-domain knowledge of small student models}.
We summarize this observation in the following takeaway.

\begin{AIbox}{\makecell{Takeaway 3: Effect of Domain Knowledge}} Limited domain knowledge of small models may hinder their learning from strong reasoning teachers.  \end{AIbox}

\begin{figure*}[!t]
    \centering
    \includegraphics[width=\textwidth]{figs/math_expert_vs_general.pdf}
    \caption{Math expert models usually have a less significant Learnability Gap than the general models. 
    A positive Gap means long CoT or large teacher CoT is better while negative means worse. This indicates that the math expert model could more easily learn from long CoT data or large teacher CoT. }
    \label{fig:math_expert_vs_general}
\end{figure*}



\paragraph{Base models exhibit a more significant learnability gap.}
We observe that base models generally exhibit a more significant learnability gap than Instruct models in Figure \ref{fig:Base_vs_Instruct_Gap}. 
This suggests that it is more challenging for small base models to effectively learn from long CoT data or large teacher CoT.


\begin{AIbox}{Takeaway 4: Base vs Instruct}
Small base models experience more significant learnability gap than Instruct models.
\end{AIbox}
\begin{figure*}[!t]
    \centering
    \includegraphics[width=\textwidth]{figs/Base_vs_Instruct_Gap.pdf}
    \caption{Base models generally exhibit a more significant learnability gap than Instruct models. A positive gap indicates that long CoT data or large teacher CoT enhance performance, whereas a negative gap suggests they have the opposite effect. This implies that it is more challenging for small base models to effectively learn from long CoT data or large teacher CoT.}
    % \vspace{-1em}
    \label{fig:Base_vs_Instruct_Gap}
\end{figure*}





\paragraph{Speaking styles shift.}
We adopt the method from  \citep{lin2023unlockingspellbasellms} to evaluate the rank shift of each token before and after fine-tuning on long CoT and Large teacher CoT data. This allows us to compare the token distribution shifts induced by the fine-tuning process. We then annotate the tokens that exhibit the largest rank shifts as the most shifted tokens. Our analysis reveals that these tokens are predominantly associated with expressive and stylistic elements, such as “wait”, “But”, and “Let”. Please see Appendix \ref{Examples of Speaking Style Shift} for more details.


\begin{AIbox}{\makecell{Takeaway 5: Speaking Styles Shift}} 
Long CoT and large teacher CoT primarily shift the student model's  distribution of tokens associated with speaking styles. 
\end{AIbox}




\section{Mix Distillation: Bridge Small Model Learnability Gap}
This section presents our Mix Distillation approach to bridge the small model learnability gap.
\subsection{Mix Distillation}
We propose \textit{Mix Distillation} to address the learnability gap observed in small models. This approach blends easier-to-learn data with more challenging data for small models, thereby leveraging the strengths of both. 

Our insight is that small models tend to perform better on data that closely matches their inherent distribution (such as short CoT or small teacher CoT), while they struggle with data that exhibits greater distribution shifts. The token distribution of the mixed long CoT and large teacher CoT data may become closer to that of small models' inherent distribution, thereby enabling them to learn more effectively from challenging datasets. 

We propose Mix-Long, which combines long CoT and short CoT data with a weight of long CoT $\alpha$ and short CoT $1-\alpha$. Similarly, we proposed Mix-Large, which combines large teacher CoT with a weight of $\alpha$ and small teacher CoT with a weight of $1-\alpha$.


\subsection{Experiment Results}

We use Qwen2.5-3B-Instruct as the student model and MATH (7.5k) as the training set. We distill different teacher models to generate responses as the baseline. They include \texttt{QwQ-32B} (long CoT), \texttt{Qwen2.5-32B} (short CoT), \texttt{Qwen2.5-72B} (large teacher CoT), \texttt{Qwen2.5-3B} (small teacher CoT). We add \texttt{Deepseek-R1-32B} \citep{DeepSeekAI2025DeepseekR1} as the teacher model to generate another set of long CoT data as baseline. We set $\alpha=0.2$ in both configurations of Mix-Long and Mix-Large.


Experimental results demonstrate that both Mix-Long and Mix-Large surpass baselines in most evaluation metrics. We show that the small student model could achieve improved performance by Mix Distillation compared to training on a single dataset. For instance, \texttt{Qwen2.5-3B-Instruct} improves by more than 8 points on MATH and AMC using Mix-Long, compared to direct training on long CoT data. It also shows a more than 7-point gain on MATH, AIME and AMC for \texttt{Qwen2.5-3B-Instruct} by Mix-Large compared with training on large teacher CoT data. This implies that it is easier for small student models to learn from datasets generated by Mix Distillation.

\begin{AIbox}{\makecell{Takeaway 6: Mix Distillation Bridges Gap}}
By mixing long CoT data (resp. large teacher CoTs) and short CoT data (resp. small teacher CoT), the small student model could achieve better performance compared to training on either data alone.
\end{AIbox}


\begin{figure}[!t]
    \centering
    \includegraphics[width=0.35\textwidth]{figs/lg_lmp_mix_weight_ablation.pdf}
    \caption{The average performance varies with the mix weight of long CoT or large teacher CoT data. \texttt{Qwen2.5-3B-Instruct} is chosen as the student model. At a weight of 0.2, mix distillation achieves the highest average performance.}
    % \vspace{-1em}
    \label{fig:lg_lmp_mix_weight_ablation}
\end{figure}







\begin{figure}[!t]
    \centering
    \includegraphics[width=0.5\textwidth]{figs/mix_long.png}
    \caption{Case Study of Mix-Long. Models fine-tuned on long CoT tended to overthink, while those trained on short CoT produced incorrect answers. In contrast, Mix-Long, incorporating branching elements (e.g., “Alternatively”), achieved a balanced reasoning process and arrived at the correct answer.}
    \label{fig:balanced CoT}
\end{figure}

Figure \ref{fig:lg_lmp_mix_weight_ablation} shows the average performance when taking different mix weight $\alpha$ of long CoT data or large teacher CoT. We choose \texttt{Qwen2.5-3B-Instruct} as the student model and find that a weight $\alpha$ of 0.2 achieves the highest average performance across five benchmarks for both Mix-Long and Mix-Large. 

Interestingly, we find that after mixing long CoT and short CoT data, the small student model’s output incorporates characteristics of long CoT, such as a branching process, while maintaining a reduced token length and avoiding overly elaborate thinking. This is illustrated in Figure \ref{fig:balanced CoT}. We observed that the small student model fine-tuned on long CoT data becomes overwhelmed by repeated thoughts and fails to stop, whereas the model fine-tuned on short CoT data produces incorrect answers. In contrast, our proposed Mix-Long, which incorporates branching elements (e.g., the use of “Alternatively”), delivers the correct answer. Additionally, the average token lengths of responses generated by long CoT, short CoT, and Mix-Long are 3384.7, 575.7, and 1248.9, respectively. We suggest that mixing long CoT and short CoT data is a practical approach to achieving a balanced CoT length, thereby enhancing the reasoning capabilities of small student models.
