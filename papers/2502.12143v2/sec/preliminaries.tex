\section{Preliminaries}

\subsection{Notation}
Let $x = (x_1, x_2, \dots, x_n)$ represent an input sequence (e.g., a prompt), and $y = (y_1, y_2, \dots, y_m)$ be the corresponding output sequence.
We consider a LLM parameterized by $\theta$, which predicts the next token following a conditional distribution 
$\pi_\theta \bigl(y_t|x, y_{1:t-1}\bigr)$. We denote by $\text{CoT}(y) \subseteq y$ the subset of tokens in the generated output that encodes a \emph{chain-of-thought}, often serving as a reasoning trace or explanatory sequence.

Throughout this work, we use the term \textbf{short CoT}, to describe concise reasoning paths to arrive at solutions \citep{min2024imitateexploreselfimprovereproduction,yeo2025demystifyinglongchainofthoughtreasoning} and \textbf{long CoT} to describe an extended reasoning sequence that is not only longer but also demonstrates more complex reflective thoughts \citep{QwenTeam2024b, yeo2025demystifyinglongchainofthoughtreasoning}. Additionally, we use the term \textbf{large teacher CoT} to refer to the reasoning trace generated by a larger teacher model, and the term \textbf{small teacher CoT} for the reasoning steps produced by a smaller teacher model.
Please see Appendix \ref{app:example} for more examples.




\subsection{Supervised Fine-Tuning (SFT)} 

Supervised fine-tuning (SFT) is widely adopted to enhance reasoning capabilities of LLMs on a dataset $\mathcal{D} = \{(x^i, y^i)\}_{i=1}^N$, where $y^i$ can be short CoT, long CoT, strong model CoT or weak model CoT sequences.
The SFT process updates the parameters $\theta$ of a language model by minimization the negative log-likelihood loss over the instruction dataset $\mathcal{D}$. 
