\section{Introduction}





Large language models (LLMs) \citep{anthropic2023claude, brown2020languagemodelsfewshotlearners, openai2023gpt4,touvron2023llamaopenefficientfoundation} have demonstrated remarkable performance in complex reasoning tasks, enabling advancements in mathematical problem-solving, logical inference, and structured decision-making \citep{cobbe2021trainingverifierssolvemath, shao2024deepseekmathpushinglimitsmathematical, yang2024qwen25mathtechnicalreportmathematical}. A key advancement in
improving LLM complex reasoning capability is the chain-of-thought (CoT) prompting. This technique decomposes complex problems into intermediate reasoning steps, enhancing both performance and interpretability. \citep{wei2023chainofthoughtpromptingelicitsreasoning}.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figs/teaser.pdf}

    \caption{Small student models ($\leq$3B parameters) do not consistently benefit from long CoT reasoning or distillation from large teacher models. Instead, they perform better when fine-tuned on shorter CoT reasoning or distilled from smaller teachers, which better matches their intrinsic learning capacity. We term this phenomenon the \textit{Small Model Learnability Gap}.}
    \label{fig:main}
\end{figure}

However, the high computational cost of LLMs hinders their deployment on resource-constrained devices, motivating the development of smaller models that offer similar capabilities at reduced cost. A widely adopted strategy to achieve this is distillation \citep{agarwal2024onpolicydistillationlanguagemodels, hinton2015distillingknowledgeneuralnetwork, kim2024promptkddistillingstudentfriendlyknowledge}, where CoT sequences generated by a strong teacher model are used to fine-tune a weaker student model.
Naturally, one might expect that distilling CoT sequences from stronger models would consistently improve small models' complex reasoning capabilities \citep{agarwal2024onpolicydistillationlanguagemodels, deepseekai2024deepseekv3technicalreport, min2024imitateexploreselfimprovereproduction, tunstall2023zephyrdirectdistillationlm}.  



However, we reveal an interesting phenomenon, which we term the \textit{Small Model Learnability Gap} (Fig. \ref{fig:main}): small models do not consistently benefit from the complex reasoning sequences provided by strong teachers, such as long CoT reasoning or distillation from large models. In our experiments, we observe that when small models are exposed to long and intricate reasoning traces, they struggle to internalize the multi-step logic due to their constrained ability. Instead, small models perform better when fine-tuned on \textit{shorter, simpler reasoning chains} that align more closely with their intrinsic learning capacity. This suggests that small models struggle to process overly elaborate reasoning traces or adapt to the distribution shifts introduced by stronger teachers, ultimately limiting their ability to generalize effectively.




To address the challenge described above, we propose \textit{Mix Distillation}, a simple yet effective approach that balances reasoning complexity by blending different types of reasoning traces. Specifically, our method comprises two configurations: (1) \textit{Mix-Long} – A combination of long and short CoT examples, ensuring that small models are exposed to both detailed and concise reasoning steps. (2) \textit{Mix-Large} – A mixture of responses from both larger and smaller models, allowing small models to learn from reasoning chains that are better suited to their capacity.  

Our experiments demonstrate that \textit{Mix Distillation} consistently improves small model reasoning performance compared to standard distillation. 
For instance, \texttt{Qwen2.5-3B-Instruct} improves by more than 8 points on MATH and AMC using Mix-Long, compared to direct training on long CoT data.
\texttt{Qwen2.5-3B-Instruct} gains more than 7 points on MATH, AIME and AMC using Mix-Large compared with training on large teacher CoT data.

These findings highlight a fundamental limitation of direct strong model distillation and emphasize the importance of \textit{adapting reasoning complexity} for effective knowledge transfer. By carefully designing distillation strategies, we provide new insights into overcoming the constraints of small model learning, making them more effective at reasoning-intensive tasks.  


