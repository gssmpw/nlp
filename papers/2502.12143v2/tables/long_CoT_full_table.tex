
\begin{table*}[htbp]
  \centering
  \resizebox{1\textwidth}{!}{%
  \begin{tabular}{l*{5}{ccc}c}
    \toprule
    & \multicolumn{3}{c}{MATH} 
    & \multicolumn{3}{c}{GSM8K} 
    & \multicolumn{3}{c}{AIME} 
    & \multicolumn{3}{c}{AMC} 
    & \multicolumn{3}{c}{Olympiad} 
    & \multicolumn{1}{c}{Average $\Delta_{\rm Long}$} \\
    \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10} \cmidrule(lr){11-13} \cmidrule(lr){14-16}
    Model 
    & $P_{\rm Long}$ & $P_{\rm Short}$ & $\Delta_{\rm Long}$ 
    & $P_{\rm Long}$ & $P_{\rm Short}$ & $\Delta_{\rm Long}$ 
    & $P_{\rm Long}$ & $P_{\rm Short}$ & $\Delta_{\rm Long}$ 
    & $P_{\rm Long}$ & $P_{\rm Short}$ & $\Delta_{\rm Long}$ 
    & $P_{\rm Long}$ & $P_{\rm Short}$ & $\Delta_{\rm Long}$ 
    & \\ 
    \midrule

    Llama-3.2-1B  & 28.6  & 33.4  & \cellcolor{red!32} -4.78  & 42.3  & 49.2  & \cellcolor{red!46} -6.90  & 0.00  & 0.00  & 0.00  & 2.50  & 7.50  & \cellcolor{red!33} -5.00  & 5.48  & 7.40  & \cellcolor{red!13} -1.92  & \cellcolor{red!25} -3.72 \\
    Llama-3.2-3B  & 48.7  & 50.9  & \cellcolor{red!14} -2.14  & 75.1  & 77.5  & \cellcolor{red!16} -2.42  & 3.33  & 3.33  & \cellcolor{white} 0.00  & 17.5  & 15.0  & \cellcolor{green!17} 2.50  & 17.6  & 18.7  & \cellcolor{red!7} -1.04  & \cellcolor{red!4} -0.619 \\
    Llama-3.1-8B  & 50.0  & 44.6  & \cellcolor{green!36} 5.36   & 81.4  & 75.5  & \cellcolor{green!39} 5.84   & 0.00  & 0.00  & \cellcolor{white} 0.00   & 27.5  & 22.5  & \cellcolor{green!33} 5.00   & 17.3  & 14.8  & \cellcolor{green!17} 2.52   & \cellcolor{green!25} 3.74 \\
    Llama-3.3-70B & 75.3  & 74.9  & \cellcolor{green!2} 0.340   & 92.7  & 91.2  & \cellcolor{green!10} 1.44   & 26.7  & 13.3  & \cellcolor{green!89} 13.3   & 55.0  & 52.5  & \cellcolor{green!17} 2.50   & 41.3  & 39.7  & \cellcolor{green!11} 1.63   & \cellcolor{green!26} 3.85 \\
    \midrule
    Qwen2.5-0.5B  & 23.0  & 31.5  & \cellcolor{red!56} -8.44  & 39.5  & 45.3  & \cellcolor{red!39} -5.84  & 0.00  & 0.00  & \cellcolor{white} 0.00  & 7.50  & 15.0  & \cellcolor{red!50} -7.50  & 4.00  & 5.93  & \cellcolor{red!13} -1.93  & \cellcolor{red!32} -4.74 \\
    Qwen2.5-1.5B  & 41.6  & 52.3  & \cellcolor{red!71} -10.7  & 63.8  & 71.7  & \cellcolor{red!53} -7.89  & 0.00  & 0.00  & \cellcolor{white} 0.00  & 17.5  & 27.5  & \cellcolor{red!67} -10.0  & 12.3  & 19.4  & \cellcolor{red!47} -7.11  & \cellcolor{red!48} -7.13 \\
    Qwen2.5-3B   & 56.2  & 61.0  & \cellcolor{red!32} -4.84  & 80.0  & 82.0  & \cellcolor{red!13} -1.98  & 3.33  & 10.0  & \cellcolor{red!44} -6.67  & 37.5  & 37.5  & \cellcolor{white} 0.00  & 24.4  & 26.4  & \cellcolor{red!13} -1.93  & \cellcolor{red!21} -3.08 \\
    Qwen2.5-7B   & 68.2  & 67.8  & \cellcolor{green!3} 0.460    & 86.2  & 85.7  & \cellcolor{green!4} 0.560    & 13.3  & 6.67  & \cellcolor{green!44} 6.67    & 40.0  & 40.0  & \cellcolor{white} 0.00   & 36.6  & 35.7  & \cellcolor{green!6} 0.889    & \cellcolor{green!11} 1.72 \\
    Qwen2.5-14B  & 78.3  & 76.2  & \cellcolor{green!14} 2.04   & 93.3  & 92.5  & \cellcolor{green!5} 0.760    & 20.0  & 6.67  & \cellcolor{green!89} 13.3   & 60.0  & 55.0  & \cellcolor{green!33} 5.00   & 44.4  & 40.9  & \cellcolor{green!24} 3.56   & \cellcolor{green!33} 4.94 \\
    Qwen2.5-32B  & 84.8  & 82.3  & \cellcolor{green!16} 2.44   & 94.9  & 94.3  & \cellcolor{green!4} 0.610    & 40.0  & 10.0  & \cellcolor{green!100} 30.0  & 85.0  & 62.5  & \cellcolor{green!100} 22.5  & 60.4  & 47.3  & \cellcolor{green!88} 13.2  & \cellcolor{green!91} 13.7 \\

    \bottomrule
  \end{tabular}
  }


\caption{This table summarizes the performance of models in Llama and Qwen families fine-tuned with long CoT and short CoT data. They are evaluated on MATH, GSM8K, AIME, AMC, and OlympiadBench. \texttt{QwQ-32B-Preview} is chosen to generate long CoT and awhile \texttt{Qwen-2.5-32B-Instruct} is chosen to generate short CoT. We observe that small student models tend to benefit more from short CoT, while large student models gain greater advantages from long CoT.}
\label{tab:full_performance_lg}
\end{table*}
