\section{Detailed Experimental Setups}
\label{appendix:More on Experimental Setups}

\input{tables/response_generator}




\subsection{Models}
Table \ref{tab:models_overview} presents a comprehensive overview of student and teacher models used in our paper. 




\subsection{Training Setup}
\label{appx:training-setup}
Our model training is conducted using LLaMA-Factory \citep{zheng2024llamafactory}, on a server with four NVIDIA A100-SXM4-80GB GPUs, an AMD EPYC 7763 64-Core Processor, and 512 GB of RAM. We use full parameter fine-tuning on student models less than 14B parameters. 
When the student model is larger than 14B, we use LoRA fine-tuning \cite{hu2021loralowrankadaptationlarge}. 
Table \ref{tab: training-hyperparameters} and Table \ref{tab: training-lora-hyperparameters} list hyper-parameters for full parameter fine-tuning and LoRA fine-tuning respectively. 


\begin{table}[!h]
\small
\centering
\resizebox{0.8\columnwidth}{!}{
\begin{tabular}{ll}
\toprule
\textbf{Hyper-parameter} & \textbf{Value} \\ \midrule
Learning Rate & $1 \times 10^{-5}$ \\
Number of Epochs & $2$ \\
Number of Devices & $4$ \\
Per-device Batch Size & $2$ \\
Optimizer & \texttt{Adamw} \\
Learning Rate Scheduler & \texttt{cosine} \\
Max Sequence Length  & $16384$ \\ \bottomrule
\end{tabular}
}
\caption{This table shows the hyper-parameters for full parameter fine-tuning.}
\label{tab: training-hyperparameters}
\end{table}


\begin{table}[!h]
\small
\centering
\resizebox{0.8\columnwidth}{!}{
\begin{tabular}{ll}
\toprule
\textbf{Hyper-parameter} & \textbf{Value} \\ \midrule
Learning Rate & $1 \times 10^{-4}$ \\
Number of Epochs & $2$ \\
Number of Devices & $4$ \\
Per-device Batch Size & $1$ \\
Lora Target & \texttt{full} \\
Learning Rate Scheduler & \texttt{cosine} \\
Warmup Ratio & $0.03$ \\
Max Sequence Length  & $16384$ \\ \bottomrule
\end{tabular}
}
\caption{This table shows the hyper-parameters for LoRA fine-tuning.}
\label{tab: training-lora-hyperparameters}
\end{table}



Teacher models generate responses by rejection sampling \citep{zelikman2022starbootstrappingreasoningreasoning,tong2024dartmathdifficultyawarerejectiontuning,yue2023mammothbuildingmathgeneralist,singh2024humandatascalingselftraining,gulcehre2023reinforcedselftrainingrestlanguage,yuan2023scalingrelationshiplearningmathematical,dong2023raftrewardrankedfinetuning}. The prompt used to make teacher models generate responses to the math questions is shown as follows.
By default, teacher models employ greedy decoding. 
By combining the math problem instructions with corresponding solutions generated by teacher models, we construct problem-solution pairs to fine-tune student models. 
We perform pairwise comparisons of solutions generated by different teacher models and filter out problem-solution pairs that are correct for both models to fine-tune student models.

\begin{figure}[htbp]
    \centering
\begin{tcolorbox}[title=Prompt, promptstyle]
\lstset{
    basicstyle=\normalfont\sffamily\footnotesize,
    breaklines=true,
    frame=none,
    columns=fullflexible,
}
Solve the following math problem. Present the final answer in the format: Final Answer: $\boxed{\{\texttt{your answer}\}}$

Problem: \{problem\}

Answer:
\end{tcolorbox}
    \label{fig: train_sampling_prompt}
\end{figure}





\subsection{Evaluation Setup} 


We evaluate the reasoning capability of fine-tuned student models on a set of commonly used benchmarks, including MATH \citep{hendrycks2021measuringmathematicalproblemsolving}, GSM8K \citep{cobbe2021trainingverifierssolvemath}, AMC 2023, AIME 2024, and the English math subset of OlympiadBench \citep{he2024olympiadbenchchallengingbenchmarkpromoting}. 

Unless otherwise specified, all fine-tuned models are evaluated in a zero-shot setting using greedy decoding. We set the maximum generation tokens as 16k. The evaluation prompt is shown below. 


\begin{figure}[htbp]
    \centering
\begin{tcolorbox}[title=Prompt, promptstyle]
\lstset{
    basicstyle=\normalfont\sffamily\footnotesize,
    breaklines=true,
    frame=none,
    columns=fullflexible,
}
Solve the following math problem and present the final answer in the format: Final Answer: $\boxed{\{\texttt{your answer}\}}$

Problem: \{problem\}

Answer:
\end{tcolorbox}
    \label{fig: evaluation_prompt}
\end{figure}


After extracting the final answer of the evaluated model, we first employ exact matching to determine the correctness of the answer. If the answer is incorrect, we use Qwen-32B-Instruct as a judge to compare the extracted final answers against that of the ground truth. The prompt is shown below.


\begin{figure}[htbp]
    \centering
\begin{tcolorbox}[title=Prompt, promptstyle]
\lstset{
    basicstyle=\normalfont\sffamily\footnotesize,
    breaklines=true,
    frame=none,
    columns=fullflexible,
}
Given a math problem, its correct final answer, and the model's generated final answer, determine if the model's answer is correct. Respond with 'True' if the it is correct and 'False' if it is incorrect. 

Problem: \texttt{\{problem\}}

Correct Final Answer: \texttt{\{ground truth\}}

Model's Generated Final Answer: \texttt{\{resp answer\}}

Your Judgement:
\end{tcolorbox}
    \label{fig: score_prompt}
\end{figure}





            



