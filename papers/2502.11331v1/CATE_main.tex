
% \documentclass[onepage,11pt,a4paper,pdftex]{article}
\RequirePackage{fix-cm}
\documentclass[12pt,a4paper,pdftex,onepage]{article}
\usepackage[fontsize=11pt]{scrextend}
\makeatletter
\newcommand*\showfontsize{\f@size{} point}
\makeatother
% \usepackage{blfootnote} 
\setlength{\parskip}{0.5em}
\usepackage{setspace}
\usepackage{tcolorbox}
\usepackage{todonotes}
% \usepackage{charter}
\usepackage{stackrel}
\usepackage[
top    = 2cm,
bottom = 2cm,
left   = 2.5cm,
right  = 2.5cm]{geometry}
\setlength\parindent{0pt}
\renewcommand{\baselinestretch}{1.2}
% \usepackage[comma,sort]{natbib}
%Fonts________
% \usepackage{cmbright}
\usepackage[T1]{fontenc}
% \renewcommand*\familydefault{\sfdefault} %% Only if the base font of the document is to be sans serif
% \include{macros}
\usepackage{SJ_Definitions}
\usepackage{authblk,natbib}
\usepackage[flushleft]{threeparttable} % note below tables
\bibliographystyle{apalike}
\setcitestyle{authoryear}
\usepackage{authblk}
\hypersetup{
colorlinks = true,
citecolor={blue},
citebordercolor={green},
linkcolor={red},
linkbordercolor={cyan}
}
\hypersetup{linktocpage}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
% \newtheorem{remark}{\bf Remark}
\title{\LARGE Transfer Learning of CATE with Kernel Ridge Regression}
\renewcommand\thefootnote{} % Remove numbering

\author[1]{Seok-Jin Kim}
\author[2]{Hongjie Liu}
\author[3]{Molei Liu}
\author[4]{Kaizheng Wang}
\affil[1]{Department of IEOR, Columbia University}
\affil[2]{Department of Statistics, Purdue University}
\affil[3]{Department of Biostatistics, Columbia Mailman School of Public Health}
\affil[4]{Department of IEOR and Data Science Institute, Columbia University}

\renewcommand\thefootnote{\arabic{footnote}} % Restore numbering
\setcounter{footnote}{0} % Reset the footnote counter
\newcommand{\inn}{\operatorname{in}}
\newcommand{\Sighat}{\widehat{{\bSigma}}}
\newcommand{\op}{\operatorname{op}}
\newcommand{\sig}{\sigma}
\newcommand{\Sbar}{\mathbf{S}}
\newcommand{\Shat}{\widehat{\mathbf{S}}}
\newcommand{\final}{\operatorname{final}}
\newcommand{\ratio}{\beta_{\delta}}
\newcommand{\de}{\mathrm{d}}
\newcommand{\Sigmatreated}{\bSigma_{1}}
\newcommand{\Sigmacontrol}{\bSigma_{0}}
\newcommand{\event}{\mathscr{E}}


% \renewcommand\Affilfont{\itshape\footnotesize} 
\renewcommand\Authands{ and }
% Apply exact font size (custom)
\renewcommand\Affilfont{\fontsize{9}{12}\selectfont} % Set font size to 10pt with 12pt line spacing
\date{\today}
\begin{document}
\maketitle	
\renewcommand*\contentsname{\begin{center}
\huge{Contents}
\end{center}}

\begin{abstract}
\noindent The proliferation of data has sparked significant interest in leveraging findings from one study to estimate treatment effects in a different target population without direct outcome observations. However, the transfer learning process is frequently hindered by substantial covariate shift and limited overlap between (i) the source and target populations, as well as (ii) the treatment and control groups within the source. We propose a novel method for overlap-adaptive transfer learning of conditional average treatment effect (CATE) using kernel ridge regression (KRR). Our approach involves partitioning the labeled source data into two subsets. The first one is used to train candidate CATE models based on regression adjustment and pseudo-outcomes. An optimal model is then selected using the second subset and unlabeled target data, employing another pseudo-outcome-based strategy. We provide a theoretical justification for our method through sharp non-asymptotic MSE bounds, highlighting its adaptivity to both weak overlaps and the complexity of CATE function. Extensive numerical studies confirm that our method achieves superior finite-sample efficiency and adaptability. We conclude by demonstrating the effectiveness of our approach using a 401(k) eligibility dataset.
\end{abstract}

\noindent{\bf Keywords:} Data integration; Conditional average treatment effect (CATE); Covariate shift; Weak overlap; Model selection; Pseudo-outcomes.




\section{Introduction}



\subsection{Background}

Predicting the conditional average treatment effect (CATE) given the covariates of an individual enables more precise and personalized decision-making in various application fields \citep{kent2018personalized,dube2023personalized}. Meanwhile, there have emerged increasing needs in transporting one particular causal study to infer the effect of interests on a different population without observations of the targeted treatment and outcome. For example, for a new drug studied on some source cohort, one may be interested in generalizing its treatment effect to another target cohort and comparing it with some existing treatment appearing in the target sample \citep{signorovitch2012matching}. Also, there is a great interest in transferring the results in randomized controlled trials (RCT) conducted on some less representative subjects to more general real-world observational populations \citep{colnet2024causal}. 

In these data integration setups, covariate shift adaptation plays an important role in adjusting for the potential bias caused by the heterogeneity between the source and target \citep{pan2009survey}. For this purpose, the most frequently used strategy is to match the source sample with the target through importance weighting (IW) \citep{huang2007correcting,liu2023augmented}. However, the classic IW method tends to result in low effective sample sizes \citep{signorovitch2010comparative} when the covariate distribution of the target population deviates from the source excessively. Such a weak overlap between the source and target is a common issue in observational studies on which one could barely design or control the data collection mechanism.  For example, consider a source sample with its age normally ranging between 30 and 65 and a target with the mean age 62 \citep{ishak2015simulation}. In this case, matching the two samples on age could introduce extremely high variance to the IW function and cause inefficiency. Moreover, a similar weak overlapping issue often occurs between the non-completely-randomly assigned treated and control groups on a single source, referred as the violation of the positivity assumption \citep{cole2008constructing}. These challenges can impede effective transfer learning of the causal effects and models, especially when the outcome models are highly complex.

In this paper, we aim at addressing the aforementioned two folds of weak overlap in the transfer learning setup where the treatment and outcome are only observed on some source sample but not on the target. In particular, we are interested in the transfer learning of CATE through kernel ridge regression (KRR) within the framework of reproducing kernel Hilbert space (RKHS) response functions. 



\subsection{Related Literature}

With the increasing interests in personalized decision making in various fields, CATE has caught great attention in recent literature. For example, \cite{kunzel2019metalearners} proposed a meta-learner for CATE that allows the use of machine learning (ML) and adapts to the structural properties of the targeted treatment effect function. \cite{kennedy2020towards} extended the double machine learning (DML) framework \citep{chernozhukov2018double} for estimation of CATE with general ML methods and showed that their estimator achieves the rate-double-robustness on the nuisance models under the structural model assumptions like sparsity or smoothness. \cite{kennedy2022minimax} further studied the minimax property of the semiparametric estimation of CATE. In addition, \cite{lan2024causal} developed a model selection approach for CATE using doubly robust Q-aggregation. Despite the recent advances, there still lack approaches for the transfer learning of CATE targeting some population with drastic covariate shift from the source causal study. 


Related to this methodological gap, \cite{hartman2015sample} proposed a calibration weighting (CW) approach to adjust for the covariate shift between RCTs and real-world data sets (RWDs) and generalize the RCT for inference of the average treatment effect (ATE) on more general populations. \cite{dahabreh2019generalizing} established a causal framework with identification conditions in a similar setup and developed an augmented inverse probability weighting estimator for the ATE on the target population. \cite{lee2023improving} proposed an augmented CW (ACW) method for transfer learning of ATE that is less prone to misspecification of the calibration models. More close to our CATE problem, \cite{wu2023transfer} developed a method to generalize the individual treatment rules for RCTs to RWDs. Nevertheless, all these methods are developed under the classic semiparametric inference framework with the positivity and strong overlap assumption \citep{rosenbaum1983central} commonly imposed. To our best knowledge, none of existing work on this track can adapt to the unknown degree of covariate shift between the source and target and stay effective under weak overlap.


To address such unknown and poor overlap issues in transfer learning, \cite{ma2023optimally} proposed a reweighted KRR method for the adaptive and efficient estimation of RKHS outcome models that weights the source samples with truncated density ratio functions. Recently, \cite{wang2023pseudo} developed a pseudo-labeling transfer learning approach that achieves the optimal model selection for KRR on some target population with strong and unknown covariate shift to the source data. In broader contexts, we also notice related work such as \cite{mou2023kernel} studying the optimal kernel-based treatment effect estimation in the absence of the strong overlap or positivity assumption on the treatment regime, as well as \cite{jin2022policy} proposing a pessimistic policy learning approach for the optimal treatment rule in a similar scenario. Nevertheless, none of existing work can fully adapt to our setup with more complicated data structure involving two folds of distributional shift between (i) the source and target populations, and (ii) the treatment and control groups on the source, as well as (iii) the unknown function complexity of the CATE and nuisance outcome models. 


\subsection{Our Contribution}

We propose a novel approach for the transfer learning of {\bf C}ATE with {\bf O}verlap-adaptive {\bf KE}rnel ridge regression (\texttt{COKE}). It first splits the source data and uses one subset to derive candidate regression adjustment (RA) learners for CATE, through KRR with various regularization parameters. Then it fits another KRR with small regularization on the holdout source data to impute the counterfactual outcomes on the unlabeled target data. Finally, the best candidate RA learner is selected using the target data with pseudo-outcomes.

Through theoretical analyses, we establish the rate-optimality of our regression adjustment and model selection strategies. Consequently, \texttt{COKE} simultaneously achieves the adaptivity to (i) unknown and weak overlap between the source and target distributions; (ii) unknown and weak overlap between the treated and control groups on the source; (iii) unknown complexity (e.g., smoothness and size) of the CATE function, as well as (iv) rate robustness to errors in the highly complex nuisance outcome models. For (i) and (ii), we notice that recent work like \cite{wang2023pseudo} and \cite{mou2023kernel} can only achieve one of them in simpler setups. Simultaneously realizing both of them and achieving sharp error rates are technically more involving due to the complication of the two-fold missing data structure in our setup. For (iii) and (iv), \texttt{COKE} is shown to maintain a higher-order and milder dependency on the complexity of the nuisance models compared to the targeted CATE function. This is in a similar sense to the rate-double-robustness established in recent semiparametric literature \citep[e.g.]{kennedy2020towards} using the doubly robust or DML construction. However, unlike DML, (iii) and (iv) are realized by us without requiring the common positivity or strong overlap assumptions \citep{rosenbaum1983central}. 


In both theoretical and numerical studies, \texttt{COKE} is demonstrated to consistently outperform existing strategies like the separate regression and DML estimation, under various settings on the degree of distributional overlap and the complexity of the models. In addition, we illustrate the utility and superiority of \texttt{COKE} in a real-world example about transfer learning for the CATE of 401(k) eligibility between two populations with different marriage status that are subject to significant covariate shift. 




\subsection{Notations}
We define \( [n] := \{1, 2, \dots, n\} \).  
For an operator \( A \) in a Hilbert space \( \HH \), we define the operator norm as \( \|A\|_{\op} \), and for \( \theta \in \HH \), we use the Hilbert norm as \( \|\theta\|_{\HH} \).  
We denote the identity operator as \( \Ib \), and the outer product operator of \( x, y \in \HH \) as \( x \otimes y \).  
We use the notation \( \cO() \) or \( \lesssim \) to hide constants, and \( \tilde{\cO}() \) to hide constants and logarithmic terms.  
Whenever additional factors are hidden, we explicitly note them.  
We use the notation \( a \asymp b \) when \( a \lesssim b \) and \( b \lesssim a \).  
For the inner product in the Hilbert space \( \HH \), we use \( \langle a, b \rangle_\HH \); however, when the context is clear, we use \( a^\top b \) or \( b^\top a \) for simplicity and to improve readability.



\section{Problem Setup}\label{section; problem setup}

\subsection{Treatment Regime and Covariate Shift}\label{sec-setup-1}
We begin by introducing the treatment regime and the concept of the conditional average treatment effect (CATE) under covariate shift.  

Suppose we have \(n\) i.i.d.\ samples from a \emph{source} distribution, denoted by \(\cD = \{(z_i, a_i, y_i)\}_{i=1}^n\), where \(z_i \) are covariates in space \(\cZ\), \(a_i \in \{0,1\}\) are treatments, and \(y_i \in \RR\) are responses.  
For the \emph{target} distribution, which is our primary focus, assume that there are \(n_\cT\) i.i.d.\ samples \(\{z_{0i}, a_{0i}, y_{0i}\}_{i=1}^{n_\cT}\), where \(z_{0i} \in \cZ\), \(a_{0i} \in \{0,1\}\), and \(y_{0i} \in \RR\).  
However, we consider the case in which \(\{(a_{0i}, y_{0i})\}_{i=1}^{n_\cT}\) are \emph{unobserved}; therefore, we only observe the i.i.d.\ covariates \(\cD_{\cT} := \{z_{0i}\}_{i=1}^{n_\cT}\). 

The distributions of \(z_i\) and \(z_{0i}\) may differ, giving rise to the \emph{covariate shift} problem.  
We write \(z_i \sim \cQ_{\mathcal{S}}\) for source covariates and \(z_{0i} \sim \cQ_{\mathcal{T}}\) for target covariates.  
In addition, we assume the treatment assignment follows \(a_i \mid z_i \sim \mathrm{Bernoulli}(\pi(z_i))\), where \(\pi: \cZ \to [0,1]\) is the propensity score function.  
Instead of the commonly used positivity condition, which requires \(\pi\) to be bounded away from \(\{0,1\}\), we adopt a weaker assumption that allows singular cases.  
The precise definition of this weaker condition is deferred to Section~\labelcref{subsection; weak overlap}.  
We also do not assume any model for \(\pi\).

We denote the joint distribution of \((z_i, a_i, y_i)\) by \(\cQ_\cS^\star\) and that of \((z_{0i}, a_{0i}, y_{0i})\) by \(\cQ_\cT^\star\).  
Consider a function space \(\cF\).  
We assume there exist two elements \(f_0^\star\) and \(f_1^\star \in \cF\) such that
\begin{align*}
f_{1}^\star(z) &= \EE_{(z,a,y)\sim \cQ_\cS^\star}\bigl[y \mid a=1, z\bigr] 
= \EE_{(z,a,y)\sim \cQ_\cT^\star}\bigl[y \mid a=1, z\bigr],\\
f_{0}^\star(z) &= \EE_{(z,a,y)\sim \cQ_\cS^\star}\bigl[y \mid a=0, z\bigr]
= \EE_{(z,a,y)\sim \cQ_\cT^\star}\bigl[y \mid a=0, z\bigr].
\end{align*}
Thus, the response functions for the treated and controlled data are the same in both source and target populations, whereas the distributions of the covariates differ.

Our primary goal is to estimate the CATE,
\[
h^\star(z) := f_1^\star(z) - f_0^\star(z),
\]
and we measure the quality of any estimator \(h \in \cF\) via its mean squared error (MSE) under the target distribution:
\[
\cE_{\cT}(h) = \EE_{z \sim \cQ_\cT}|h(z) - h^\star(z)|^2.
\]
We additionally define the noise variables \(\varepsilon_i := y_i - f^\star_{a_i}(z_i)\) for \(i \in [n]\).  
By construction, these satisfy \(\EE[\varepsilon_i \mid z_i,a_i] = 0\).

Next, we focus on the setting in which \(\cF\) is a reproducing kernel Hilbert space (RKHS) induced by a symmetric and positive semi-definite kernel \(K(\cdot,\cdot): \cZ \times \cZ \to \RR\) \citep{paulsen2016introduction}.  
By the Moore--Aronszajn Theorem \citep{aronszajn1950theory}, there exists a Hilbert space \(\HH\) and an associated feature mapping \(\phi: \cZ \to \HH\) such that \(\langle \phi(z), \phi(w)\rangle_{\HH} = K(z,w)\).  
We then define
\[
\mathcal{F} = \bigl\{
f_\theta: \cZ \to \RR 
\mid 
f_\theta(z) = \langle \phi(z), \theta \rangle_{\HH}
\text{ for some } \theta \in \HH 
\bigr\}.
\]
This class includes Sobolev and Besov spaces as special cases \citep{wainwright2019high,fischer2020sobolev}.  
Moreover, \(\cF\) is isomorphic to \(\HH\), and we denote its norm by \(\|\cdot\|_{\cF}\).  
Finally, assume that the kernel \(K\) is bounded, i.e., $\sup_{z \in \cZ} K(z,z) \leq \xi$
for some constant \(\xi > 0\).  
This assumption is common in kernel ridge regression (KRR) analyses. 


\subsection{Kernel Ridge Regression}\label{section; preliminaries kernel ridge regression}

In this section, we briefly explain the method and properties of KRR in the setting where we have $N$ covariates and corresponding response data.  
Suppose we observe data $\{(u_i, r_i)\}_{i=1}^N$, with \(u_i \in \cZ\) and \(r_i \in \RR\).

A KRR estimator aims to solve the following penalized least-squares problem and produce an estimator \(\hat{f}\):
\begin{align}\label{equation; KRR program}
\hat{f}=  \arg \min_{f \in \cF} 
\bigg\{
\frac{1}{N}\sum_{i=1}^N (r_i - f(u_i))^2 + \lambda \| f\|_{\cF}^2
\bigg\}
.
\end{align}
We refer to \(\lambda>0\) as the ridge regularizer.  
Because the solution depends only on the kernel evaluations \(\{K(u_i, u_j)\}\), one can efficiently compute the estimator by solving an equivalent finite-dimensional quadratic program \citep{wainwright2019high}.  

A primary challenge in KRR is selecting the regularizer \(\lambda\), as there is an inherent bias--variance trade-off.  
Thus, finding an appropriate value for \(\lambda\) is critical.  
We provide additional details and the closed-form solution of KRR in Appendix~\labelcref{section; groundwork}.


Before presenting our main algorithm, we describe a naive approach for CATE estimation, referred to as \emph{separate regression}.  
First, split the data \(\cD\) into the treated group \(\cD(a=1)\) and the control group \(\cD(a=0)\), where
\[
\cD(a=1) := \{(z_i, a_i, y_i) \in \cD \mid a_i =1 \}, 
\quad
\cD(a=0) := \{(z_i, a_i, y_i) \in \cD \mid a_i =0 \}.
\]
We select two regularization parameters \(\lambda_0\) and \(\lambda_1\), then estimate each \(f_k^\star\) by applying KRR to \(\cD(a=k)\) with regularizer \(\lambda_k\). 
This yields \(\hat{f}_0\) and \(\hat{f}_1\).  
Finally, we define 
\[
\hat{h}_{\operatorname{sep}} = \hat{f}_1 - \hat{f}_0
\]
as the CATE estimator.  
This plug-in procedure is appealing for its simplicity: one can independently choose \(\lambda_0\) and \(\lambda_1\) to minimize the estimation errors of \(\hat{f}_0\) and \(\hat{f}_1\).  However, because our main interest lies in the contrast function \(h^\star = f^\star_1 - f^\star_0\), there is no direct rule of thumb for selecting \(\lambda_0\) and \(\lambda_1\) specifically to optimize the estimation of \(h^\star\).  

We end this section by mentioning the challenges posed by covariate shift in kernel ridge regression. 
Even though the response model is the same for both source and target data, if there is a covariate shift of unknown degree, choosing an effective regularizer becomes both extremely important and difficult. Consequently, extensive research on covariate shift in KRR has been actively pursued \citep{ma2023optimally,wang2023pseudo,chen2024high,PDT24}.



\section{Methodology}\label{section; methodology}

In this section, we present our transfer learning methodology for CATE estimation. It consists of three key steps: 
(1) splitting the data, 
(2) training candidate models via regression adjustment learner (RA learner), 
and (3) selecting the best model.

To begin, we choose positive integers $\{ n_j \}_{j=1}^3$ such that $n = n_1 + n_2 + n_3$, and randomly split the source data $\cD$ into three subsets $\{\cD_j\}_{j=1}^3$ of sizes $\{ n_j \}_{j=1}^3$.  
We denote each $\cD_j$ as $\{ (z_{ji}, a_{ji}, y_{ji}) \}_{i=1}^{n_j}$.  
We will use $\cD_1 \cup \cD_2$ to train candidate models, and then use $\cD_3 \cup \cD_{\cT}$ for model selection.  
For simplicity, we assume below that $n$ is a multiple of 3 and set $n_1 = n_2 = n_3 = n/3$.

\subsection{RA Learner}

We now describe the \emph{Regression Adjustment (RA)} learner. 
Our goal is to estimate the contrast (CATE), $h^{\star}$.  
However, due to the treatment structure, some outcomes are missing, which prevents us from directly targeting the CATE through regression.
Instead, we generate pseudo-outcomes and perform regression on them.
This idea underlies the RA learner:
\begin{enumerate}
\item Estimate imputation models on $\mathcal{D}_1$.
\item Generate pseudo-outcomes on $\mathcal{D}_2$.
\item Run KRR on the pseudo-outcomes in \(\mathcal{D}_2\) to learn \(h^\star\).
\end{enumerate}
See \cite{kunzel2019metalearners,kennedy2020towards,curth2021nonparametric} for related approaches.

Our RA learner (Algorithm~\labelcref{algorithm; RA learner}) takes as input the datasets \(\cD_1\), \(\cD_2\), and a tuple of three regularizers \(\bm{\lambda }  = (\lambda_{1,0}, \lambda_{1,1}, \lambda_2) \in \RR^3\) with \(\lambda_{1,0}, \lambda_{1,1}, \lambda_2 >0\).  
First, we estimate nuisance functions by performing KRR separately on \(\cD_1\) with regularizers \(\lambda_{1,0}\) and \(\lambda_{1,1}\), obtaining estimators \(\hat{f}_0\) and \(\hat{f}_1\).  
Specifically:
\begin{align}\label{equation; RA learner algorithm}
\hat{f}_0 &:= \arg\min_{f \in \cF} \bigg\{ \frac{1}{n_1} \sum_{i=1}^{n_1} (y_{1i} -f(z_{1i}))^2 \one(a_{1i}=0) + \lambda_{1,0} \| f\|_{\cF}^2 \bigg\}, \\
\hat{f}_1 &:= \arg \min_{f \in \cF} \bigg\{\frac{1}{n_1} \sum_{i=1}^{n_1} (y_{1i} -f(z_{1i}))^2\one(a_{1i}=1)  + \lambda_{1,1} \| f\|_{\cF}^2\bigg\}. 
\label{equation; RA learner algorithm 2}
\end{align}
We then form pseudo-outcomes \(\{m_{2i}\}_{i=1}^{n_2}\) on $\cD_2$ by 
\begin{align*}
m_{2i} := 
\begin{cases}
y_{2i} - \hat{f}_0(z_{2i}), & \text{if } a_{2i} = 1,\\[6pt]
\hat{f}_1(z_{2i}) - y_{2i}, & \text{if } a_{2i} = 0.
\end{cases}
\end{align*}
Next, we run KRR with regularizer \(\lambda_2\) on these pseudo-outcomes and return the CATE estimator \(\hat{h}_{\bm{\lambda}}\):
\begin{align*}
\hat{h}_{\bm{\lambda}} := \arg \min_{h \in \cF} \bigg\{ \frac{1}{n_2} \sum_{i=1}^{n_2} (m_{2i}-h(z_{2i}))^2  + \lambda_2 \| h\|_\cF^2 \bigg\}.
\end{align*}
The pseudocode of this procedure is given below.

\begin{algorithm}[H]
\caption{RA Learner}
\label{algorithm; RA learner}
\begin{algorithmic}
\Require  Split datasets \(\cD_1, \cD_2\), regularizers  \(\bm{\lambda }  = (\lambda_{1,0}, \lambda_{1,1}, \lambda_2) \in \RR^3\) where \(\lambda_{1,0}, \lambda_{1,1}, \lambda_2 >0 \).
\State Using \(\cD_1\), run KRR to get nuisance estimators \(\hat{f}_0, \hat{f}_1\) by solving \eqref{equation; RA learner algorithm} and \eqref{equation; RA learner algorithm 2}.
\State Using \(\cD_2\), set the pseudo-outcome \(m_{2i} := (y_{2i} - \hat{f}_0(z_{2i}) )\bm{1}(a_{2i}=1) + (\hat{f}_1(z_{2i}) - y_{2i} )\bm{1}(a_{2i}=0)\) for all \(i \in [n_2]\).
\State On \(\cD_2\), apply KRR using the independent variables \(\{z_{2i}\}_{i=1}^{n_2}\) and responses \(\{m_{2i}\}_{i=1}^{n_2}\) with regularizer \(\lambda_2 > 0\), and get estimator \(\hat{h}_{\bm{\lambda}}\).
\Ensure \(\hat{h}_{\bm{\lambda}}\).

\end{algorithmic}
\end{algorithm}

Recall that \emph{separate regression} estimates two nuisance functions independently and outputs $\hat{f}_1 - \hat{f}_0$ as the CATE estimator.  
By contrast, Algorithm~\labelcref{algorithm; RA learner} fits an additional regression on the pseudo-outcomes, which, as we will see, brings notable benefits.

\subsection{Model Selection}
We now present our key procedure for generating (pseudo) test outcomes and performing model selection using them.  
Since \(\cD_\cT\) contains only target covariates, our model selection algorithm (Algorithm~\labelcref{algorithm; model selection}) generates test outcomes and selects the best model with respect to these test outcomes.
The inputs are the dataset \(\cD_3\), the target covariates \(\cD_\cT\), and a set of candidate CATE estimators denoted by \(\Hcal_0 := \{\hat{h}_1, \dots, \hat{h}_L\}\) for some \(L > 0\).

First, we set two regularizers \(\tilde{\lambda}_0, \tilde{\lambda}_1 = \frac{\xi \log n}{n}\) (where \(\xi\) is the upper bound of the kernel assumed in Section \labelcref{sec-setup-1}).  
Next, we run KRR on \(\cD_3\) with these regularizers to obtain \(\tilde{f}_0\) and \(\tilde{f}_1\), via:
\begin{align}\label{equation; test outcome algorithm}
\tilde{f}_0 &:= \arg\min_{f \in \cF} \bigg\{\frac{1}{n_3} \sum_{i=1}^{n_3} (y_{3i} -f(z_{3i}))^2 \one(a_{3i}=0) + \tilde{\lambda}_{0} \| f\|_{\cF}^2 \bigg\}, \\
\tilde{f}_1 &:= \arg \min_{f \in \cF} \bigg\{ \frac{1}{n_3} \sum_{i=1}^{n_3} (y_{3i} -f(z_{3i}))^2\one(a_{3i}=1)  + \tilde{\lambda}_{1} \| f\|_{\cF}^2 \bigg\}. 
\label{equation; test outcome algorithm 2}
\end{align}
Then, we define $\tilde{h} = \tilde{f}_1 - \tilde{f}_0$ and use $\tilde{h}$ to generate test outcomes for \(\cD_\cT\).  
We then select the final model from \(\Hcal_0\) by minimizing the squared loss over these test outcomes.  
Algorithm~\labelcref{algorithm; model selection} provides the detailed steps.

\begin{algorithm}[H]
\caption{Model Selection}
\label{algorithm; model selection}
\begin{algorithmic}
\Require  Dataset \(\cD_3, \cD_\cT\); set of CATE estimators $\Hcal_0 = \{ \hat{h}_1, \dots, \hat{h}_L\}$ where \(\hat{h}_i \in \cF\) for \(i \in [L]\).
\State Run KRR to obtain $\tilde{f}_0, \tilde{f}_1$ by solving \eqref{equation; test outcome algorithm} and \eqref{equation; test outcome algorithm 2}.
\State Define $\tilde{h} = \tilde{f}_1 -\tilde{f}_0$, and form the test outcomes for \(\cD_\cT\) as \(\{\tilde{h}(z_{0i})\}_{i=1}^{n_{\cT}}\).
\State For each $h \in \Hcal_0$, compute
\[
L(h) = \frac{1}{n_\cT} \sum_{i=1}^{n_\cT} \bigl(\tilde{h}(z_{0i}) -h(z_{0i})\bigr)^2.
\]
Choose the final model $\hat{h}_{\final} = \arg \min_{h \in \Hcal_0} L(h)$.
\Ensure \(\hat{h}_{\final}\).
\end{algorithmic}
\end{algorithm}

Next, we combine Algorithm~\labelcref{algorithm; RA learner} (the RA learner) and Algorithm~\labelcref{algorithm; model selection} to present our main algorithm, which is described in the following section.


\subsection{The Final Procedure}

We are now ready present our main algorithm, Transfer learning of {\bf C}ATE with {\bf O}verlap-adaptive {\bf KE}rnel ridge regression (COKE), in Algorithm~\labelcref{algorithm; main}. It first uses an RA learner to generate multiple candidate CATE estimators, and then employs Algorithm~\labelcref{algorithm; model selection} to select the final model.

The algorithm takes as input the labeled source data \(\cD\) and unlabeled target data \(\cD_\cT\).  
First, we set \(\lambda_{1,0}, \lambda_{1,1} = \frac{\xi\log n}{n}\) and construct the grid \(\Lambda_2 =\Bigl\{ \frac{\xi \log n}{n}, \frac{2 \xi 
\log n }{n}, \dots, \frac{2^q  \xi \log n}{n} \Bigr\}\) for \(q = \lceil \log n \rceil\).  
This serves as the grid for the regularizer \(\lambda_2\).  
Next, we define \(\bm{\Lambda} := \{ \lambda_{1,0}\} \times \{\lambda_{1,1}\} \times \Lambda_2\), and apply the RA learner (Algorithm~\labelcref{algorithm; RA learner}) for every \(\bm{\lambda} = (\lambda_{1,0}, \lambda_{1,1}, \lambda_2) \in \bm{\Lambda}\), using the splits \(\cD_1, \cD_2\).  
This produces a set of candidate CATE estimators, \(\cH = \{\hat{h}_{\bm{\lambda}} \mid \bm{\lambda} \in \bm{\Lambda}\}\).  
We then feed \(\cH\) and the remaining data \(\cD_3, \cD_\cT\) into Algorithm~\labelcref{algorithm; model selection} to obtain our final model.  
The pseudocode is summarized below.

\begin{algorithm}[H]
\caption{\texttt{COKE}: Transfer learning of {\bf C}ATE with {\bf O}verlap-adaptive {\bf KE}rnel ridge regression}
\label{algorithm; main}
\begin{algorithmic}
\Require Dataset $\cD, \cD_{\cT}$.
\State Set grid \(\Lambda_2 = \Bigl\{ \frac{\xi \log n}{n}, \frac{2 \xi \log n}{n}, \dots, \frac{2^q \xi \log n}{n} \Bigr\}\) where $q = \lceil 2\log n \rceil$, and set $\lambda_{1,0}, \lambda_{1,1} = \frac{\xi\log n}{n}$.
\State Split source data $\cD$ into $\cD_{1}, \cD_{2}, \cD_{3}$.
\State Define \(\bm{\Lambda} := \{ \lambda_{1,0}\} \times \{\lambda_{1,1}\} \times \Lambda_2\). For each \( \bm{\lambda} \in \bm{\Lambda}\), run Algorithm~\labelcref{algorithm; RA learner} on \(\cD_1, \cD_2\) to obtain $\hat{h}_{\bm{\lambda}}$.
\State Set the collection of candidates as $\cH = \{\hat{h}_{\bm{\lambda}} \mid \bm{\lambda} \in \bm{\Lambda}\}$.
\State Run Algorithm~\labelcref{algorithm; model selection} with input \(\cD_3, \cD_\cT, \cH\), yielding the final model \(\hat{h}_{\final}\).
\Ensure $\hat{h}_{\final}$.
\end{algorithmic}
\end{algorithm}

Again, Algorithm~\labelcref{algorithm; main} outputs the best CATE estimator among the various estimators generated by the RA learner through model selection, and later we discuss how this demonstrates great adaptivity.


\section{Theoretical Results}\label{section; main results}

In this section, we present our theoretical results, including the final model's MSE bounds, and outline the assumptions that underlie these results. 
We begin by describing the standard assumptions, then introduce our novel weak overlap framework.

\subsection{Standard Assumptions}

We first present 3 assumptions, which are widely used in causal inference and nonparametric regression.
We will discuss them in detail after listing them below.

\begin{assumption}[Consistency and unconfoundedness]\label{assumption; consistency and unconfoundedness}
An individual $i$ has two potential outcomes \(y_i(1), y_i(0)\) under treatment and control, respectively.
\begin{itemize}
\item Consistency: If an individual $i$ is assigned treatment $a_i \in \{0, 1\}$, we observe \(y_i = y_i(a_i)\). 
\item Unconfoundedness: There are no unobserved confounders, i.e., $( y_i(0), y_i(1) )$ are independent of $a_i$ given $z_i$.    
\end{itemize}
\end{assumption}

\begin{assumption}[Sub-Gaussian noise]\label{assumption; subGaussian noise}
Conditioned on \(\{z_i,a_i\}_{i=1}^{n}\), the noise variables \((\varepsilon_1, \dots, \varepsilon_n)\) form a sub-Gaussian vector with proxy \(\sigma\).
For simplicity, we assume \(\sigma\) is bounded by some universal constant.
\end{assumption}


\begin{assumption}[Polynomial eigenvalue decay]\label{Assumption; eigenvalue decay}\label{assumption; boundedness}
We assume that \(\bSigma_\cT\)'s ordered eigenvalues \(\mu_1 \geq \mu_2 \geq \dots\) satisfy \(\mu_j \lesssim j^{-2\ell}\) for some \(\ell > \frac{1}{2}\).
We set \(\alpha = \frac{2\ell}{1+2\ell}\).
Also, the kernel is bounded as \(\sup_{z \in \Zcal}K(z,z) \leq \xi\) for some constant \(\xi>0\).  
For simplicity, we assume \(\xi\) is bounded by some universal constant.
\end{assumption}

The first two assumptions are widely used in the CATE literature \citep{kunzel2019metalearners,curth2021nonparametric,kennedy2020towards,kennedy2022minimax}. Assumption~\labelcref{Assumption; eigenvalue decay} is widely utilized in the literature on kernel ridge regression (KRR).
It is well known that the Sobolev space \(H^k(\cZ)\) and Besov spaces satisfy this assumption \citep{wainwright2019high,fischer2020sobolev}.  
In general, for \(H^k(\cZ)\) with \(k,d \in \NN\), the parameter \(\ell\) is given by \(\ell = \frac{k}{d}\).  
Therefore, both Sobolev kernels and Matérn kernels satisfy this condition.
Moreover, the neural tangent kernel (NTK) \citep{li2024eigenvalue,bietti2020deep}, corresponding to over-parameterized neural networks, is also known to exhibit a polynomially decaying spectrum.
Boundedness of the kernel is also widely assumed in KRR literature \citep{wainwright2019high,fischer2020sobolev}. 


\subsection{Weak Overlap}\label{subsection; weak overlap}

We now explain \emph{weak overlap}, one of our core assumptions. 
Our setting involves two types of distributional shifts: one between treated covariates and control covariates, and another between source and target covariates. 
We propose and assume the concept of weak overlap for both shifts, and we discuss how it is weaker than the commonly used overlap condition.

Existing works on causal inference usually assume \emph{positivity} or \emph{strong overlap}, which requires the propensity score be bounded away from 0 and 1. We aim to relax this assumption, allowing the propensity score to take singular values of 0 or 1. Below, we present our \emph{weak treatment overlap} assumption.

To facilitate analysis, we define two second-moment operators associated with the treated and control groups:
\[
\Sigmatreated= \EE_{(z,a,y) \sim \cQ^\star_S}[\phi(z) \otimes \phi(z) \one(a =1)], 
\quad  
\Sigmacontrol := \EE_{(z,a,y) \sim \cQ^\star_S}[\phi(z) \otimes \phi(z) \one(a=0)].
\]
Also, we define the expected second moments of source and target covariates as 
\[
\bSigma  = \EE_{z \sim \cQ_\cS}[\phi(z) \otimes \phi(z)], 
\quad  
\bSigma_\cT = \EE_{z \sim \cQ_\cT} [\phi(z)\otimes \phi(z)].
\]
See Appendix~\labelcref{section; groundwork} for more details.

\begin{assumption}[Weak overlap]\label{assumption; weak treatment overlap}\label{assumption; overlap source target}  
There exist two constants \(R,B \geq 1\) satisfying the following:
\begin{itemize}
\item Treatment overlap:
\[
\Sigmacontrol  \preceq R\Bigl(\Sigmatreated + \frac{\xi}{n} \Ib\Bigr),
\quad
\Sigmatreated \preceq R\Bigl(\Sigmacontrol + \frac{\xi}{n} \Ib\Bigr).
\]
\item Source-target overlap:
\[
\bSigma_{\cT} \preceq B \Bigl(\bSigma + \frac{\xi}{n} \Ib\Bigr).
\]
\end{itemize}
\end{assumption}

We will show that the treatment overlap assumption can be implied by the commonly-used positivity condition, and allows for singular propensity scores. To the best of our knowledge, this is the first relaxation of positivity condition in nonparametric CATE estimation. 
Furthermore, we treat the overlap parameter \(R\) as an important quantity and explicitly specify its dependency in subsequent results. 
This assumption offers a significant relaxation, representing a major contribution on its own.
The source-target overlap condition is widely used in the covariate shift setting for KRR \citep{ma2023optimally,wang2023pseudo}. The parameter \(B\) controls the amount of covariate shift. When the source and target have the same distribution, we have $B = 1$.


\begin{remark}
For the propensity score, a substantial body of literature assumes strong overlap, often requiring that \(\pi(z)\) is bounded away from 0 and 1 \citep{wager2024causal}. 
To the best of our knowledge, nonparametric CATE estimation methods universally assume strong positivity \citep{kennedy2020towards,kennedy2022minimax,curth2021nonparametric,gao2020minimax}. 
For other topics such as estimating ATE, several works have explored relaxing the positivity assumption \citep{mou2023kernel,ma2022testing,ma2020robust}.
\end{remark}

Next, we provide examples illustrating Assumptions~\labelcref{assumption; weak treatment overlap,assumption; overlap source target}.
The following two examples illustrate that our assumption is well satisfied under the classical strong overlap conditions.


\begin{example}[$R$: Bounded propensity score]\label{example; positivity}
If \(\pi(z) \in [\kappa,1-\kappa]\) for some \(\kappa>0\), then the first part of Assumption~\labelcref{assumption; weak treatment overlap} holds with \(R \leq \frac{1}{\kappa}\). 
\end{example}

\begin{example}[$B$: Bounded source-target density ratio]
If the density of the source \(f_\cS(z)\) and target \(f_\cT(z)\) has an upper-bounded ratio
\[
\frac{f_\cT(z)}{f_\cS(z)} \leq B'
\]
for some \(B'>0\), then the second part of Assumption~\labelcref{assumption; overlap source target} holds with \(B = B'\).
\end{example}

Next, we show that our weak overlap assumption also holds under a singular propensity score, highlighting its powerful relaxation. 
In the following example, the density ratio between the treated and control covariates is unbounded. 
This scenario has not been explored in causal inference studies using KRR. 
We claim to be the first to generalize the concept of overlap in this way.

\begin{example}[$R$: Singular propensity score] \label{example; singular propensity 1}
Consider the propensity score \(\pi(z) = z\).
For \(\cF = H^1([0,1])\) with bounded density source covariates, the first part of Assumption~\labelcref{assumption; weak treatment overlap} holds with \(R \asymp n^{\frac{1}{3}}\).
\end{example}

Once again, we emphasize that for \(\pi(z)=z\), the density ratio between the treated and control covariates is unbounded. 
Finally, we present an example of \(B\) under a singular source-target density ratio. 
This example is studied in \citet{wang2023pseudo}.

\begin{example}[$B$: Dirac target distribution]\label{example; dirac target}
Consider a scenario where \(\cQ_\cT\) is a Dirac point measure at some point \(z_0\). 
For \(\cF = H^1([0,1])\) with bounded density source covariates, the second part of Assumption~\labelcref{assumption; overlap source target} holds with \(B \asymp n^{\frac{1}{2}}\).
\end{example}

Proofs for the examples above are presented in Appendix~\labelcref{section; proof weak overlap exampls}.

\subsection{Results Overview}\label{subsection; summary of results}
In this section, we summarize our main result. 
We show that our MSE bound adapts to all the statistical components arising in CATE estimation, which is one of our primary contributions.\footnote{Another core contribution is our relaxation through weak overlap.} 
Also, for sufficient large \(n\), our MSE bound matches the lower bound.
We list the statistical components involved in CATE estimation as follows:
\begin{enumerate}
\item Eigenvalue decay rate \(\ell\), and function structural complexities \(\|h^\star \|_{\cF}\), \(\|f_1^\star \|_{\cF}\), \(\|f_0^\star \|_{\cF}\). 
These are determined by the interaction among the target distribution, the kernel, and the response functions.
\item Source sample size \(n\), two overlaps \(B,R\), and target sample size \(n_\cT\).
\end{enumerate}

To investigate the optimal performance, consider the following (simplified) imaginary scenario:
\begin{itemize}
\item We observe both potential outcomes \(y_i(0)\) and \(y_i(1)\) for all \(i \in [n]\).
\item There is no covariate shift.
\end{itemize}
In this case, the problem reduces to classical KRR, and the known state-of-the-art MSE bound of KRR is \citep{wainwright2019high,fischer2020sobolev}
\begin{equation}\label{equation; imaginary MSE}
\tilde{\cO}\bigl(n^{-\alpha} \|h^\star\|_{\cF}^{2(1-\alpha)} \bigr).
\end{equation}
For Sobolev kernels, it is also known that this rate is minimax optimal \citep{green2021minimax}.

However, our setting is more challenging due to:
\begin{itemize}
\item[\textbf{(C1)}] Missing outcomes (only one of \(y_i(0)\), \(y_i(1)\) is observed);
\item[\textbf{(C2)}] Two distribution shifts with weak overlaps (Assumption~\labelcref{assumption; weak treatment overlap,assumption; overlap source target}).
\end{itemize}

The first challenge (C1) arises because missing outcomes prevent direct regression on the CATE function \(h^\star\), making it difficult to adapt to \(\|h^\star\|_\cF\). 
The second challenge (C2) limits the usage of the entire source sample \(n\). 
Hence, the \emph{effective} sample size is determined by weak overlap, which influences the learning rate.
We define our effective sample size by accounting for these two overlaps. 
\begin{definition}[Effective sample size]
We define the effective sample size under the two overlaps (Assumption~\labelcref{assumption; weak treatment overlap,assumption; overlap source target}) as 
\[
n_{\operatorname{eff}}:= \frac{n}{BR}.
\]
\end{definition}
Instead of \(n\), the effective sample size \(n_{\operatorname{eff}}\) plays a central role in the MSE bound.
Later in Section~\labelcref{subsection; lower bound}, we prove that effective sample size is necessary and that it aligns with the lower bound.

We now present our main result regarding the MSE bound.
Let us set \(\max(\|f_0^\star \|_\cF,\|f_1^\star \|_\cF) = M\). 

\begin{theorem}[MSE bound of final model]\label{theorem; main theorem}
Suppose that we run \texttt{COKE} under Assumptions~\labelcref{assumption; boundedness,assumption; consistency and unconfoundedness,assumption; subGaussian noise,assumption; overlap source target,assumption; weak treatment overlap,Assumption; eigenvalue decay}. 
We further assume that \(n > BR\) and that \(\| h^\star\|_{\cF}\) is bounded by some universal constant. 
We do not impose any bound on \(\|f_0^\star\|_\cF\) or \(\|f_1^\star\|_\cF\) and set \(\max\bigl(\|f_0^\star\|_\cF,\|f_1^\star\|_\cF\bigr) = M\). 
Then, with probability at least \(1-n^{-10}\), the MSE of our final model satisfies:
\[
\cE_\cT(\hat{h}_{\final}) 
\lesssim n_{\operatorname{eff}}^{-\alpha} \norm{h^\star}^{2(1-\alpha)}_\cF + M^2 \Bigl(\frac{1}{n_{\operatorname{eff}}}+ \frac{R}{n_\cT}\Bigr).
\]
Here, \(\lesssim\) hides absolute constants, \(\sigma,\xi\), and logarithmic factors.
\end{theorem}

The first (leading) term in the bound matches the sharp rate \eqref{equation; imaginary MSE} when replacing \(n\) with the effective sample size \(n_{\operatorname{eff}}\). 
Moreover, this term alone matches the lower bound, which we present in Section~\labelcref{subsection; lower bound}.
The second term decreases faster than the first term as \(n\) increases, and it can be dominated by the first term whenever \(n\) is sufficiently large.
Thus, our result can be viewed as an oracle inequality with \emph{adaptivity}, and the second term is a negligible overhead. 

Even if the source and target share the same response function, unknown distribution shifts (overlaps) require a careful choice of regularizer for optimality. 
Our problem involves two such shifts, making it even more challenging. 
The significance of our result lies in showing that our approach effectively adapts to both unknown shifts, which is a major contribution.
The scenario without covariate shift, in which the performance of the CATE for the source distribution is considered, is one of our special cases.
In this case, by simply setting $B=1$, both the upper and lower bounds are easily obtained (Theorem~\ref{theorem; main theorem} and Theorem~\ref{theorem; lower bound}), and the leading term of the upper bound still matches that of the lower bound.

Notably, to our knowledge, no existing result demonstrates that the naive plug-in approach
(i.e., separate regression) can adapt to the function’s structural complexity \(\|h^\star\|_\cF\).
Consequently, it remains unknown whether this method can match the lower bound.
Furthermore, the assumption of an upper bound on $\|h^\star\|_{\cF}$ is imposed solely for simplicity, and the same result holds even if it takes on a reasonably large value (for details, see Appendix~\ref{subsection; proof main corollary}).
The proof is deferred to Appendix~\labelcref{section; proof main theorem} and consists of two parts: 
(i) analyzing the RA learner, presented in the next section, and 
(ii) analyzing model selection in Section~\labelcref{subsection; model selection}.




We summarize our theoretical contributions below:
\begin{itemize}
\item Our MSE bound for the final model successfully adapts to the lower bound with negligible cost. 
Our results are sharp with respect to \(n\), \(B\), \(R\), and the structural complexity \(\|h^\star\|_{\cF}\).
\item Our algorithm works under a weak overlap condition, which allows for singular propensity scores or a singular source–target density ratio. 
This is the first relaxation of the positivity for nonparametric CATE estimation. 
Furthermore, our model selection procedure adapts to the unknown degrees of overlap, \(B\) and \(R\).
\end{itemize}


\subsection{MSE Bound of RA Learner}

Next, we examine the MSE bound of the estimator from the RA learner (Algorithm~\labelcref{algorithm; RA learner}). 
It requires a triple of regularizers \(\bm{\lambda} = (\lambda_{1,0}, \lambda_{1,1}, \lambda_2)\) with \(\lambda_{1,0}, \lambda_{1,1}, \lambda_2 >0\). 
Below, we present a theorem on the MSE bound of the RA learner with regularizers \(\bm{\lambda}\). 
Before that, we define an operator:
\[
\Sbar_\lambda := (\bSigma + \lambda \Ib)^{-\frac{1}{2}} \bSigma_\cT (\bSigma + \lambda \Ib)^{-\frac{1}{2}}, 
\quad \text{for any } \lambda > 0.
\]

For \(\bm{\lambda} = (\lambda_{1,0}, \lambda_{1,1}, \lambda_2)\), we define
\[
\Rcr(\bm{\lambda}) 
:= 
R\|\Sbar_{\lambda_2}\|_{\op}\bigl(\lambda_{1,1}\|f^\star_1\|_{\cF}^2 + \lambda_{1,0}\|f^\star_0\|_{\cF}^2\bigr)
+
\lambda_{2}\|\Sbar_{\lambda_2}\|_{\op}\|h^\star\|_{\cF}^2
+
\sig^2 \frac{R\operatorname{Tr}(\Sbar_{\lambda_2})}{n}\log n.
\]
The next theorem shows the MSE bound of \(\hat{h}_{\bm{\lambda}}\) from Algorithm~\labelcref{algorithm; RA learner}.

\begin{theorem}[MSE bound of RA learner estimators]\label{theorem; MSE bound RA learner}
Assume we run Algorithm~\labelcref{algorithm; RA learner} with the regularizers \(\bm{\lambda} = (\lambda_{1,0}, \lambda_{1,1}, \lambda_2)\) and the dataset \(\cD_1, \cD_2\).
Under the same setup of Theorem~\ref{theorem; main theorem}, the MSE of \(\hat{h}_{\bm{\lambda}}\) is bounded by
\[
\cE_\cT(\hat{h}_{\bm{\lambda}}) \lesssim \Rcr(\bm{\lambda})
\]
with probability at least \(1-2n^{-11}\).
Here, \(\lesssim\) hides absolute constants.
\end{theorem}

Its proof is deferred to Appendix~\labelcref{section; proof RA learner}. 
This theorem provides the upper bound of MSE with deterministic form.
Recall that we generate multiple CATE estimators \(\cH := \{\hat{h}_{\bm{\lambda}} \mid \bm{\lambda}\in \bm{\Lambda}\}\) in Algorithm~\labelcref{algorithm; RA learner}.
The following corollary states the best possible MSE bound among these candidates.

\begin{corollary}[Optimal MSE bound among candidates]\label{corollary; optimal MSE bound}
Assume that we run \texttt{COKE} under the same setup as in Theorem~\ref{theorem; main theorem}. Then, with probability at least \(1-2n^{-11}\), the following holds:
\[
\inf_{\bm{\lambda} \in \bm{\Lambda}} \cE_\cT(\hat{h}_{\bm{\lambda}}) 
\lesssim  
\Bigl(\frac{BR}{n}\Bigr)^{\alpha}\|h^\star\|_{\cF}^{2(1-\alpha)}(\log n)^{\alpha}
+\frac{BR}{n} M^2\log n.
\]
Here, the symbol \(\lesssim\) hides absolute constants and the parameters \(\sigma\) and \(\xi\).



\end{corollary}

Its proof is deferred to Appendix~\labelcref{section; proof RA learner}. 
We observe that the optimal MSE over the grid of regularizers adapts to the lower bound, as will be discussed in Theorem~\ref{theorem; lower bound}.
Again, the assumption of an upper bound on $\|h^\star\|_{\cF}$ is imposed solely for simplicity. The same result holds even if $\|h^\star\|_{\cF}$ takes on a reasonably large value; details are provided in the proof. A natural question is how to choose the regularizer $\bm{\lambda}$ that minimizes $\cE_\cT(\hat{h}_{\bm{\lambda}})$, i.e.,
\(
\arg \min_{\bm{\lambda}\in\bm{\Lambda}} \cE_\cT(\hat{h}_{\bm{\lambda}})\).
The next section provides an oracle inequality for our model selection procedure.

\subsection{Model Selection and Oracle Inequalities}\label{subsection; model selection}
We now present the oracle inequalities for our model selection procedure (Algorithm~\labelcref{algorithm; model selection}), which establish the MSE bound for the final selected model. 
Define a key quantity
\[
\Ocr := \bigl(\xi M^2 + \sigma^2\bigr)\Bigl(\frac{BR}{n} + \frac{R}{n_{\cT}}\Bigr)\log(n n_\cT).
\]
This plays a important role in model selection, appearing as an additive term in the oracle inequalities.

For any estimator \(\hat{h}\), define the in-sample MSE
\[
\cE_\cT^{\inn}(\hat{h}) := \frac{1}{n_\cT} \sum_{i=1}^{n_\cT} |\hat{h}(x_{0i}) - h^\star(x_{0i})|^2,
\]
which is the mean squared error over the target covariates in \(\cD_\cT\). 
We first present the oracle inequality for in-sample MSE:

\begin{proposition}[Oracle inequality for in-sample MSE]\label{proposition; in-sample MSE oracle}
Under Assumptions~\labelcref{assumption; boundedness,assumption; consistency and unconfoundedness,assumption; subGaussian noise,assumption; overlap source target,assumption; weak treatment overlap,Assumption; eigenvalue decay}, with probability at least \(1 - 2n^{-11}\), the following holds:
\[
\cE_{\cT}^{\inn}(\hat{h}_{\operatorname{final}}) 
\lesssim 
\min_{\hat{h} \in \cH} \cE_{\cT}^{\inn}(\hat{h}) 
+ 
\Ocr,
\]
where \(\lesssim\) hides absolute constants.
\end{proposition}

We now obtain the oracle inequality for MSE:
\begin{proposition}[Oracle inequality for MSE]\label{proposition; oracle inequality MSE}
Under Assumptions~\labelcref{assumption; boundedness,assumption; consistency and unconfoundedness,assumption; subGaussian noise,assumption; overlap source target,assumption; weak treatment overlap,Assumption; eigenvalue decay}, with probability at least \(1 - 3n^{-11}\), the following holds:
\[
\cE_{\cT}(\hat{h}_{\operatorname{final}}) 
\lesssim 
\min_{\hat{h} \in \cH} \cE_{\cT}(\hat{h}) 
+ 
\Ocr,
\]
where \(\lesssim\) hides absolute constants.
\end{proposition}

Combining this result with Corollary~\labelcref{corollary; optimal MSE bound}, we obtain Theorem~\labelcref{theorem; main theorem}. Compared to Corollary~\labelcref{corollary; optimal MSE bound}, the above oracle inequality contains the additional term \(\Ocr\), reflecting the cost of model selection. 
However, since \(\Ocr = \tilde{\cO}\bigl(\frac{1}{n} + \frac{1}{n_\cT}\bigr)\), it remains a negligible bound relative to the leading term in Theorem~\labelcref{theorem; main theorem}.
Hence, our algorithm achieves strong adaptivity, attaining near-optimal performance with model selection, as stated in Theorem~\labelcref{theorem; main theorem}.
The proofs of both propositions are presented in Appendix~\labelcref{section; proofs oracle inequalities}.


\subsection{Lower Bound Results}\label{subsection; lower bound}
Finally, we present results related to the lower bound. 
We argue that the first (leading) term in Theorem~\labelcref{theorem; main theorem} matches the lower bound, so our result is an oracle inequality that attains adaptivity.
First, we define an \((R,B)\)-bounded instance.


\begin{definition}[\((R,B)\)-bounded instance]
For the distribution of source covariates \(\cQ_\cS\), target covariates \(\cQ_\cT\), and source propensity score \(\pi(\cdot)\), we say they form a \((R,B)\)-bounded instance if the propensity score satisfies \(\frac{1}{R} \leq \pi(\cdot)\leq 1-\frac{1}{R}\) and the density ratio \(\frac{\de \cQ_\cT}{\de \cQ_\cS}\) is bounded by \(B\).
\end{definition}

We can see that every \((R,B)\)-bounded instance satisfies the weak overlap (Assumption~\labelcref{assumption; overlap source target}). 
Now, we present our result for the lower bound.
For simplicity, we next present our lower bound results for \(\ell=1\), where \(\ell\) was defined as the eigenvalue decay rate in
Assumption~\labelcref{Assumption; eigenvalue decay}. 
However, our argument can easily be extended to general \(\ell>\frac{1}{2}\).

\begin{theorem}[Lower bound]\label{theorem; lower bound}
For any \(W,R,B>0\), there exists a triple \((\cQ_\cS, \cQ_\cT, \pi)\) of \((R,B)\)-bounded instance and a kernel \(K(\cdot, \cdot)\) with \(1\)-polynomial eigenvalue decay such that
\[
\inf_{\hat{h}} \sup_{\substack{(f_0^\star, f_1^\star, h^\star): \|h^\star\|_\cF \leq W}} 
\EE\bigl[\cE_\cT(\hat{h})\bigr] 
\gtrsim
\Bigl(\frac{BR}{n}\Bigr)^{\frac{2}{3}}W^{\frac{2}{3}}.
\]
\end{theorem}

This result shows that the first term in Theorem~\labelcref{theorem; main theorem} indeed matches the lower bound. In particular, introducing the effective sample size is essential, and our main result demonstrates adaptivity.
The proof is deferred to Appendix~\labelcref{section; lower bound proof}.


\section{Simulation Studies}\label{sec:simu}

\subsection{Setups and benchmarks}

We conduct extensive simulation studies to evaluate the finite-sample performance of \texttt{COKE} and compare it with existing approaches. The simulation code and methods used in this study are publicly available on GitHub at \url{https://github.com/hongjiel/COKE}. For data generation, we set the source sample size $n$ and the target (unlabeled) sample size $n_{\cT}$ such that $n_{\cT}=n/4$. To generate the covariates $z$, we let $\mathcal{U}^+$ and $\mathcal{U}^-$ respectively denote uniform distributions over $(0,\pi)$ and $(-\pi,0)$. On the source $\cS$, we independently generate $z_{j}\sim \frac{S_B^{1/q}}{S_B^{1/q} + 1}\mathcal{U}^-+ \frac{1}{S_B^{1/q} + 1}\mathcal{U}^+$ for $j=1,\ldots,q$ and $z_{q+1}, \ldots, z_{p} \stackrel{\text{i.i.d.}}{\sim} \text{Uniform}(-\pi,\pi)$. On the target, we independently generate $z_{j}\sim \frac{1}{S_B^{1/q} + 1}\mathcal{U}^-+\frac{S_B^{1/q}}{S_B^{1/q} + 1}\mathcal{U}^+$ for $j=1,\ldots,q$ and $z_{q+1}, \ldots, z_{p} \stackrel{\text{i.i.d.}}{\sim} \text{Uniform}(-\pi,\pi)$. Here, we fix $p = 4$ and $q<p$ is the number of covariates subject to distributional shift between the source and target. Also, the hyper-parameter $S_B$ controls the degree of covariate shift between the source and target, with a larger $S_B$ resulting in a weaker overlap.


Then we set the propensity score as $\pi(z) = \mathrm{expit}(S_R\sum_{j=1}^4z_j/8)$ and generate  $a_i \mid z_i \sim \mathrm{Bernoulli}(\pi(z_i))$, where $S_R$ controls the overlap between the treatment and control groups on the source. We set the outcome models as
\[
f_a^\star(z) = c \cdot q^{-1}\sum_{i=1}^q\left[2 \left( \left| z_i \right| - \frac{\pi}{4} \right) \bold{1}\left( \left| z_i \right| \geq \frac{\pi}{2} \right) + \left| z_i \right| \bold{1}\left( \left| z_i \right| < \frac{\pi}{2} \right) \right]
+ (a - \frac{1}{2}) \cdot q^{-1}\sum_{i=1}^q\sin z_i,
\]
for $a = 0, 1$, which yields the true CATE function $h^\star(z) = q^{-1}\sum_{i=1}^q\sin z_i$. Note that $f_0^\star(z)$ and $f_1^\star(z)$ are less smooth and more complex than $h^\star(z)$ due to their absolute value terms of $z_i$. Then we generated $y_i \mid a_i, z_i \sim N(f_{a_i}^\star(z_i), 0.25)$. Here, $c$ controls the complexity of the nuisance model $f_a^\star$ relative to the CATE function $h^\star$. We consider simulation settings with varying hyper-parameters including the sample size $n_{\cT}=n/4$, the dimension of covariate shift $q$, the degree of covariate shift $S_B$, the degree of non-overlap between treatment and control $S_R$, and the complexity of the nuisance model $c$. In specific, we first set $q=1$, $S_B = 10$, $S_R = 2$, $c = 1$, and $n_{\cT}=n/4=\lceil 350\sqrt{S_B}+60S_R+25 \rceil$. Then we vary each single parameter among $S_B$, $S_R$ and $c$ separately with the remaining parameters fixed and $n_{\cT}=n/4$ changing accordingly. We also vary the sample size $n_{\cT}=n/4$ with others fixed, and vary $S_B$ under the setting $q=2$. In each setting, we evaluate the mean squared error of an estimator $\hat{h}(z)$ using the mean square error (MSE): $\EE_{z \sim \cQ_\cT}\{\hat{h}(z) - h^\star(z)\}^2$ averaged over 350 repetitions.


We consider four methods in our studies including \texttt{COKE} and three benchmark methods--separate regression (\texttt{SR}), DR-Learner for CATE (\texttt{DR-CATE}) \citep{kennedy2020towards}, and the ACW estimator tailored for CATE estimation (\texttt{ACW-CATE}), which is motivated by \cite{lee2023improving}. For fair comparison, KRR is used for all regression tasks in these methods. The \texttt{ACW-CATE} method extends the DML approach of \cite{kennedy2020towards} by incorporating a density ratio model to address covariate shift and form the efficient influence function for the CATE on the target sample in our setup. Implementation details of the benchmark methods are provided in Appendix \labelcref{sec:app:bench}.


For the implementation of KRR, we use the Matérn kernel:
\[
K(\boldsymbol z,\boldsymbol w) = \frac{4}{\sqrt{\pi} \rho} \exp\left(-\frac{2\sqrt{2} \|\boldsymbol{z} - \boldsymbol{w}\|_2}{\rho}\right),
\]
where $\rho$ is the scale parameter set as $5$. For \texttt{COKE}, we set the tuning parameters in Algorithm \labelcref{algorithm; main} as $\lambda_{1,0}=\lambda_{1,1} = \frac{1}{5n}$ and ${\bm \Lambda} = \{\frac{2^k}{5n} : k = 0, 1, \ldots, \lceil\log_2(5n)\rceil\}$. Note that the output $\hat{h}_{\final}$ in Algorithm \labelcref{algorithm; main} actually comes from the KRR on $\cD_{2}$ while $\cD_{1}$ and $\cD_{3}$ are respectively used for nuisance model estimation and model selection. To utilize the data more effectively, we introduce a cross-fitting (CF) version of that is obtained by (1) splitting the source data $\cD$ into $\cD_{1},\cD_{2},\cD_{3}$; (2) implementing the training procedures in Algorithm \labelcref{algorithm; main} three times separately on the permutations $\{\cD_{1},\cD_{2},\cD_{3}\}$, $\{\cD_{3},\cD_{1},\cD_{2}\}$, and $\{\cD_{2},\cD_{3},\cD_{1}\}$; (3) averaging the three estimators resulted from (2) as the final output. 
Due to the simple average form in Step (3), all our convergence results in Section \labelcref{section; main results} on the original data-splitting version of \texttt{COKE} in Algorithm \labelcref{algorithm; main} can be easily extended to this CF version. 
Proof for CF version is presented in Appendix~\labelcref{subsection; proofs for cross fitting}.
To ensure fair comparison, the CF strategy is also used to implement the three benchmark methods and this is actually recommended by existing work like \cite{chernozhukov2018double} and \cite{kennedy2020towards}.



\subsection{Results}



As shown in Figure \labelcref{fig:matern}, \texttt{COKE} consistently outperforms all the benchmark methods across all scenarios. When $S_B$ increases, there are severer covariate shift and weaker overlap between the source and target. In this scenario with $n_{\cT}=n/4=\lceil 350\sqrt{S_B}+60S_R+25 \rceil$, the performance of \texttt{COKE} remains stable and better than other methods. For example, when $S_B = 25$, the relative efficiency of \texttt{COKE} compared to \texttt{DR-CATE} is $3.56$. Unlike \texttt{COKE}, the mean squared errors of \texttt{DR-CATE} and \texttt{ACW-CATE} grow significantly with $S_B$ even under an increasing sample size $n$ proportional to $\sqrt{S_B}$. This demonstrate the superior robustness of \texttt{COKE} to the weak overlap issue, compared to typical semiparametric (DML) approaches. 


\begin{figure}[htbp!]
\centering
\includegraphics[width=16cm]{simulation_results/matern_results.pdf}
\caption{Performance of \texttt{COKE}, \texttt{ACW-CATE}, \texttt{DR-CATE} and \texttt{SR} across varying simulation settings. Panels show the average MSE as a function of: (i) $S_B$ (degree of covariate shift between source and target) for $q = 1$, (ii) $S_R$ (degree of shift between treatment and control groups), (iii) $c$ (complexity of outcome models relative to the CATE), (iv) $S_B$ for $q = 2$ (weak overlap on two-dimensional covariates), and (v) $n_{\cT}=n/4$.}
\label{fig:matern}
\end{figure}

As $S_R$ increases, reflecting severer non-overlap between treated and control groups, \texttt{COKE} also maintains better performances over the benchmarks. For example, when $S_R = 5$, the relative efficiency of \texttt{COKE} compared to \texttt{SR} is 1.60, and is even higher when compared with \texttt{DR-CATE} and \texttt{ACW-CATE}. This shows \texttt{COKE}’s effectiveness in handling the weak overlap between the treated and control groups. As $c$ increases, there arises higher complexity of the outcome models compared to the CATE, \texttt{COKE} shows consistently smaller estimation error compared to the benchmarks across different $c$. For example, when $c = 2.5$, the relative efficiency of \texttt{COKE} compared to any other benchmark is larger than $2$. Moreover, the improvement of \texttt{COKE} over \texttt{SR} becomes more significant as $c$ gets larger, which demonstrates \texttt{COKE}'s better adaptivity to complex outcome regression functions and is consistent with our theoretical results in Section~\labelcref{subsection; summary of results}.

We also consider a different setup with $q = 2$, including two covariates subject to weak overlap between the source and targets and making the outcome and CATE models more complex. As shown in the fourth panel of Figure \labelcref{fig:matern}, \texttt{COKE} again displays consistently lower mean squared errors compared to \texttt{ACW-CATE}, \texttt{DR-CATE}, and \texttt{SR} across various values of $S_B$ when $q=2$. Additionally, we vary the sample sizes of the target and source data while keeping their ratio constant at $n / n_{\cT} = 4$ and present the results in the last panel of Figure \labelcref{fig:matern}. As the sample sizes increase, \texttt{COKE} exhibits a similar rate of risk reduction as other methods, and maintains the smallest estimation error among all methods. Finally, we compare the cross-fitting version of \texttt{COKE} with the original Algorithm \labelcref{algorithm; main} as detailed in Appendix \labelcref{sec:app:simu}. In our setup with $q=1$ and varying $S_B$, the CF version displays around $10$--$15\%$ lower risk than the original data-splitting version, which is a noticeable improvement. 





\section{Real-World Example}

The impact of the 401(k) program has been extensively studied \citep[e.g.,]{abadie2003semiparametric}. Unlike other plans like Individual Retirement Accounts (IRAs), 401(k) eligibility is solely determined by employers. As a result, unobserved individual savings preferences are unlikely to significantly affect eligibility for 401(k) plans. Nonetheless, factors like job choice, income, and age may still confound causal analyses of the 401(k) program. To address this, \citep{abadie2003semiparametric} and \citep{chernozhukov2018double} suggested adjusting for specific covariates related to job selection to treat 401(k) eligibility as exogenous. Examining the average effect of 401(k) eligibility on the overall net financial assets (NFA) is a key question addressed in previous studies such as \cite{abadie2003semiparametric} and \cite{chernozhukov2018double}. However, quantifying the CATE of 401(k) eligibility for individualized policy evaluation is an important yet overlooked problem.


We aim at learning the individualized treatment effect of 401(k) eligibility $a_i$ on the NFA outcome $y_i$ using the data set from the 1991 Survey of Income and Program Participation. We include $7$ adjustment and effect modifying covariates in $z_i$ including {\em age, income, family size, education years, benefit pension status, participation in an IRA plan, and home ownership}. We consider a transfer learning setup with $n=5997$ source samples including all subjects in the original data set with their marital status being {\em married} and $n_{\cT}=3918$ {\em not married} subjects as the target data, with only their $z_i$ used for training and their treatment and outcome information used for validation and evaluation. Through some preliminary analyses, we examine the severity of covariate shift between the source and target. In specific, we fit the logistic regression to obtain $\widehat{\omega}(z)$ as an estimate of the density ratio of the covariates $z$ between the source and target. In Figure \labelcref{fig:densityratio}, we plot the histograms of $\log_{10}\{\widehat{\omega}(z)\}$ separately on the source and target samples. The source sample has a mean of $-0.889$ and a standard deviation of $0.956$ for $\log_{10}\{\widehat{\omega}(z)\}$, whereas the target sample has a mean of $0.732$ with a standard deviation of $0.748$. The effective sample size of the source sample is $399.01$. Through this plot, one can severe strong covariate shift and weak overlap between the source and target. For example, at the mode of $\log_{10}\{\widehat{\omega}(z)\}$ on the target sample with a density larger than $0.6$, the source sample has a density less than $0.02$. Also, on the left tail of $\log_{10}\{\widehat{\omega}(z)\}$ on the source sample, the target data shows nearly zero density.

\begin{figure}[htbp!]
\centering
\includegraphics[width=8cm]{simulation_results/density_ratio.pdf}
\caption{Empirical distribution of the logarithms of the estimated density ratio (using the logistic regression) between the source and target. 
}
\label{fig:densityratio}
\end{figure}


Consistent with Section \labelcref{sec:simu}, we include \texttt{SR}, \texttt{DR-CATE}, and \texttt{ACW-CATE} as the benchmark methods for comparison. All methods are implemented with cross-fitting. To evaluate the predictive performance on CATE, the ideal label is the counterfactual $y_{0i}(1)-y_{0i}(0)$ that is unobservable in real-world data. To approximate $y_{0i}(1)-y_{0i}(0)$, we leverage the observations of the treatment $a_{0i}$ and outcome $y_{0i}$ on the target sample to derive the efficient score (influence function) for the validation of CATE estimators as 
\[
\hat{s}_{0i}=\frac{a_{0i}-\hat \pi_{\cT}(z_{0i})}{\hat \pi_{\cT}(z_{0i})\{1-\hat \pi_{\cT}(z_{0i})\}}\{y_{0i}-\hat f_{a_{0i},{\cT}}(z_{0i})\}+\hat f_{1,{\cT}}(z_{0i})-\hat f_{0,{\cT}}(z_{0i}),
\]
where $\hat \pi_{\cT}$, $\hat f_{0,{\cT}}$, and $\hat f_{1,{\cT}}$ are machine learners for the nuisance functions $\mathbb{P}_{\cQ_\cT^\star}[a=1\mid z]$, $\EE_{\cQ_\cT^\star}[y \mid a=0, z]$ and $\EE_{\cQ_\cT^\star}[y \mid a=1, z]$. Then we evaluate the Spearman and Pearson correlation coefficients between the score $\hat{s}_{0i}$ and the CATE estimator $\hat{h}(z_{0i})$. This evaluation strategy is based on the main result of \cite{kennedy2020towards} that regressing $\hat{s}_{0i}$ against $z_{0i}$ result in DML estimation for the CATE. To obtain the nuisance models $\hat \pi_{\cT}$, $\hat f_{0,{\cT}}$, and $\hat f_{1,{\cT}}$, we consider two options including generalized linear regression (GLR) and random forest (RF). In a similar spirit, one could also leverage the R-learner \citep{nie2021quasi} to construct the approximation for $y_{0i}(1)-y_{0i}(0)$. We find that this produces similar results as our primarily used $\hat{s}_{0i}$. As the methods involve random data-splitting, we replicate the analysis 200 times and report the average performance. 



\begin{table}[htb!]
\centering
\begin{tabular}{|c|cccc|}
\hline Metrics & \texttt{COKE} & \texttt{SR} & \texttt{DR-CATE} & \texttt{ACW-CATE} \\
\hline Spearman Cor with $\hat{s}_{0i}$ & 0.175 & 0.113 & 0.110 & 0.121 \\
\hline Pearson Cor with $\hat{s}_{0i}$ & 0.038  & 0.025 & 0.011 & 0.027   \\ 
\hline
\end{tabular}
\caption{\label{tab:res:401:crossfit} Spearman and Pearson correlation coefficients between the CATE efficient score $\hat{s}_{0i}$ and the CATE estimators obtained by cross-fitting. The nuisance models are obtained using generalized linear regression.}
\end{table}

In Table \labelcref{tab:res:401:crossfit}, we present the Spearman and Pearson correlation coefficients between the CATE predictors and the validation score $\hat{s}_{0i}$ on the target sample, with the nuisance models in $\hat{s}_{0i}$ estimated by GLR and the CATE predictors obtained using cross-fitting. Among all methods, \texttt{COKE} achieves the highest degree of concordance with $\hat{s}_{0i}$. In specific, \texttt{COKE} attains more than 45\% higher Spearman correlation with $\hat{s}_{0i}$ as well as more than 40\% higher Pearson correlation compared to all other methods. In addition, both correlation coefficients of \texttt{COKE} are significantly larger than zero, with the $p$-value for Spearman correlation being $<10^{-15}$ and that for Pearson being $0.009$. As a sensitivity analysis, we also present two more performance metric tables in Appendix \labelcref{sec:app:real:results} with one having the nuisance models estimated by RF and another having all the CATE estimators constructed without cross-fitting. They show similar results as Table \labelcref{tab:res:401:crossfit}, in which \texttt{COKE} still achieves the best performance among all under comparison.








\section{Discussion}

We propose a novel methodology and a theoretical framework for CATE estimation under covariate shift.
We present an optimal MSE bound and provide an oracle inequality that effectively adapts to this bound.
In particular, adapting to the Hilbert norm of the CATE function poses a significant challenge due to missing data, but our methodology facilitates this adaptation.
Furthermore, we introduce the concept of \textit{weak overlap}, a relaxation of the traditional overlap (or positivity) assumption, and establish performance guarantees under this condition.
We also discuss how a singular propensity score may still satisfy our weak overlap condition.
Our result adapts to both the unknown degree of weak overlaps and the specified optimal MSE bound.
As a future direction, it would be valuable to investigate whether the benefits of the imputation method persist when using general regression methods (such as neural networks) beyond KRR.
Additionally, applying kernel ridge regression and a model selection procedure to other complex causal models—such as dynamic treatment regimes or individualized treatment rules—could be a promising avenue of further research.

\section*{Acknowledgement}
Seok-Jin Kim and Kaizheng Wang’s research is supported by an NSF grant DMS-2210907, and
a startup grant and a Data Science Institute seed grant SF-181 at Columbia University.



\clearpage
\newpage
\bibliography{ref}

\appendix

%\setcounter{lemma}{0}
%\setcounter{assume}{0}
\setcounter{table}{0}
\setcounter{figure}{0}
%\setcounter{remark}{0}
%\setcounter{theorem}{0}
%\setcounter{algorithm}{0}
\setcounter{equation}{0}


%\renewcommand{\thelemma}{A\arabic{lemma}}
%\renewcommand{\theassume}{A\arabic{assume}}
%\renewcommand{\thetheorem}{A\arabic{theorem}}
\renewcommand{\thefigure}{A\arabic{figure}}
\renewcommand{\thetable}{A\arabic{table}}
%\renewcommand{\theremark}{A\arabic{remark}}
%\renewcommand{\thealgorithm}{A\arabic{algorithm}}
\renewcommand{\theequation}{A\arabic{equation}}


\clearpage
\newpage

\tableofcontents





\clearpage
\newpage
\section{Preparations: Linear Model in RKHS and Notations}\label{section; groundwork}
In this section, we formulate our RKHS responses as a linear model in Hilbert space and lay the groundwork for the proofs.  
Next, we observe that the KRR estimator can be expressed in a form similar to linear regression, using the language of RKHS.  

\subsection{Linear Model via RKHS Mapping}
Under the same treatment regime as in Section~\labelcref{section; problem setup}, we reformulate it in the language of Hilbertian elements.  
Using the properties of RKHS, we can construct a linear model through feature mapping.  
Recall that we defined the Hilbert space induced by the kernel \( K(\cdot,\cdot) \) as \( \HH \) in Section~\labelcref{section; problem setup}.  
The two spaces \( \cF \) and \( \HH \) are isomorphic; hence, there exists a bijection that preserves the metric.  
For any element \( \theta \in \HH \), there exists \( f_\theta \in \cF \) such that \( \langle \theta, \phi(x) \rangle_{\HH} = f_\theta(x) \).  
Conversely, any function \( f \in \cF \) can be represented by a Hilbertian element \( \theta(f) \in \HH \) where \( \langle \theta(f), \phi(x) \rangle_{\HH} = f(x) \).  

For the Hilbert space \( \HH \), we define the Hilbert norm as \( \| \cdot \|_{\HH} \).  
Since the two spaces \( \cF \) and \( \HH \) are isomorphic, we have \( \| \theta \|_\HH = \| f_\theta \|_\cF \) and \( \| f \|_\cF = \| \theta(f) \|_\HH \).  
With slight abuse of notation, we write \( \langle x, y \rangle_{\HH} := x^\top y = y^\top x \) for any \( x, y \in \HH \) when the context is clear.  
Similarly, we write \( xy^\top := x \otimes y \) when the context is clear.  

We define the RKHS covariates as
\begin{align*}
x_i := \phi(z_i) \quad \forall i \in [n], \quad x_{0i} := \phi(z_{0i}) \quad \forall i \in [n_\cT].
\end{align*}
By setting $\theta(f_0^\star) = \theta_0^\star$ and $\theta(f_1^\star) = \theta_1^\star$ for some $\theta_0^\star, \theta_1^\star \in \HH$, we can reformulate our RKHS responses as the following linear model:
\begin{align*}
\EE[y_i \mid x_i,a_i=0] = x_i^\top \theta^\star_0, \quad \EE[y_i \mid x_i,a_i=1] = x_i^\top \theta^\star_1.
\end{align*}
Then, \(f_1^\star, f_0^\star \in \cF\) in Section~\labelcref{section; problem setup} correspond to \(\theta_0^\star, \theta_1^\star \in \HH\), respectively, and they have the same Hilbert norm, where $\|f_0^\star \|_\cF = \| \theta_0^\star \|_\HH$ and $\|f_1^\star\|_\cF = \| \theta_1^\star \|_\HH$.

Then, naturally, we define the Hilbertian element of the CATE function as $\theta(h^\star) = \theta_1^\star - \theta_0^\star$, and we define 
\begin{align*}
\eta^\star := \theta_1^\star - \theta_0^\star.
\end{align*}
Our main goal is to estimate $\eta^\star$.
Recall that we defined $(z_i,a_i,y_i) \sim \cQ^\star_\cS$. 
By setting $x_i = \phi(z_i)$, we define the distribution of the source with the RKHS covariates as 
\begin{align*}
(x_i,a_i,y_i) \sim \cP^\star_\cS.
\end{align*}
In addition, we define the distributions of the RKHS covariates for the source and target as $x_i \sim \cP_\cS$ and \(x_{0i} \sim \cP_{\cT}\), respectively.
Then, our main object of interest, CATE, can be formulated as 
\begin{align*}
&\eta^\star = \theta^\star_1 - \theta_0^\star \\
&x^\top \eta^\star :=\EE_{(x,a,y)\sim \cP_\cS^\star} [y \mid x, a=1] - \EE_{(x,a,y)\sim \cP_\cS^\star} [y \mid x, a=0].
\end{align*}
Our goal is to predict CATE, $\eta^\star$, by minimizing the target MSE.
We redefine expected second-order moments in the language of RKHS covariates. 
Then, we have $\Sigmatreated= \EE_{(x,a,y) \sim \cP^\star_S}[x \otimes x \one(a=1)]$, $\Sigmacontrol := \EE_{(x,a,y) \sim \cP^\star_S}[x \otimes x \one(a=0)]$, and $\bSigma = \EE_{(x,a,y) \sim \cP^\star_S}[x \otimes x]$.
For target RKHS covariates, we define \( \bSigma_\cT = \EE_{x \sim \cP_\cT} [x \otimes x] \).
We emphasize again that we only receive covariates for the target distribution.

\subsection{Closed Form of KRR Estimator}
Recall the KRR setup described in Section~\labelcref{section; preliminaries kernel ridge regression}. 
We define RKHS covariates as $v_i := \phi(u_i)$ for all $i \in [N]$ and define the \emph{design operator} of \(\{v_1, \dots, v_N \}\) as $\Vb: \HH \to \RR^N$, which satisfies for all \(\theta \in \HH\):
\begin{align*}
\Vb \theta = (v_1^\top \theta, v_2^\top \theta, \dots, v_N^\top \theta)^\top.
\end{align*}
Similarly, we define the adjoint of \(\Vb\), denoted as \(\Vb^\top: \RR^N \to \HH \), as the operator such that for all \(\ab =(a_1, \dots, a_N) \in \RR^N\),
\begin{align*}
\Vb^\top \ab= \sum_{i=1}^N a_i v_i \in \HH.
\end{align*}
We define \(\rb =(r_1, \dots, r_N)^\top \).
It is known that the solution of the KRR program \eqref{equation; KRR program} in Section~\labelcref{section; preliminaries kernel ridge regression}, denoted by $\hat{f}$, satisfies
\(\hat{f}(u) = \phi(u)^\top \hat{\theta}\), where 
\begin{align*}
\hat{\theta} =(\Vb^\top \Vb + N\lambda \Ib)^{-1} \Vb^\top \rb.
\end{align*}
This is exactly the same form as the ridge estimator in Euclidean linear regression, but expressed in terms of Hilbert space operators.

\subsection{Notations for Proofs}\label{subsection; notations}
We define the notations for the proofs and also review and summarize the already defined notation.

\paragraph*{Notations for second moments and design operators}
\begin{itemize}
\item Define the expected second-order moments as $\Sigmatreated= \EE_{(x,a,y) \sim \cP^\star_S}[x \otimes x \one(a=1)]$, $\Sigmacontrol := \EE_{(x,a,y) \sim \cP^\star_S}[x \otimes x \one(a=0)]$, and $\bSigma = \EE_{(x,a,y) \sim \cP^\star_S}[x \otimes x]$.  

\item Define the target second-order moment as $\bSigma_\cT = \EE_{x \sim \cP_\cT} [x \otimes x]$.

\item For $j =1,2,3$ and $t= 0,1$, define $\cD_{j}(a=t)$ as 
\(\cD_j(a=t ) :=  \{(z_{ji}, a_{ji}, y_{ji}) \in \cD_j \mid a_{ji} = t\}\).

\item Define the \textbf{empirical second-order moments} as follows: \\
$\Sighat_2 := \frac{1}{n_2}\sum_{\cD_2} x_{2i}x_{2i}^\top$, \\
$\Sighat_{2,0} = \frac{1}{n_2}\sum_{\cD_2(a=0)} x_{2i}x_{2i}^\top$, \\
$\Sighat_{2,1} = \frac{1}{n_{2}}\sum_{\cD_2(a=1)} x_{2i}x_{2i}^\top$, \\
$\Sighat_{1,0} = \frac{1}{n_{1}}\sum_{\cD_{1}(a=0)} x_{1i}x_{1i}^\top$, \\
$\Sighat_{1,1} = \frac{1}{n_{1}}\sum_{\cD_{1}(a=1)} x_{1i}x_{1i}^\top$,
\\
$\Sighat_{3,0} =\frac{1}{n_{3}}\sum_{\cD_{3}(a=0)} x_{3i}x_{3i}^\top$, and \\
$\Sighat_{3,1} = \frac{1}{n_{3}}\sum_{\cD_{3}(a=1)} x_{3i}x_{3i}^\top$.

For the summations, for instance, $\sum_{\cD_2(a=0)} x_{2i}x_{2i}^\top$ denotes the summation of RKHS covariates in the dataset $\cD_2(a=0)$.

\item Define the design matrix (operator) of RKHS covariates for dataset $\cD_j(a=t)$ as $\Xb_{j,t}$ for $t=1,0$ and \(j \in \{1,2,3 \}\).
Also, define the design operator of dataset \(\cD_j\) as \(\Xb_j\).
The definition of the design operator is the same as in Appendix~\labelcref{section; groundwork}; please refer to that section.

\item For target covariates, define the design operator of \(\{x_{0i}\}_{i=1}^{n_\Tcal}\) as $\Xb_\cT$ similarly.

\item Define \(M := \max( \|\theta_0^\star \|_\HH, \|\theta_1^\star \|_\HH) =\max( \|f_0^\star \|_\cF, \|f_1^\star \|_\cF)\).

\item For $\lambda>0$, define $\Sbar_\lambda := (\bSigma + \lambda \Ib)^{-\frac{1}{2}} \bSigma_{\cT} (\bSigma + \lambda \Ib)^{-\frac{1}{2}}$ and $\Shat_\lambda := (\bSigma + \lambda \Ib)^{-\frac{1}{2}} \Sighat_{\cT} (\bSigma + \lambda \Ib)^{-\frac{1}{2}}$.
\end{itemize}

\paragraph{Other key notations}
\begin{itemize}
\item With slight abuse of notation, we write \(\langle x,y\rangle_{\HH} := x^\top y = y^\top x\) and $xy^\top := x \otimes y$ for any \(x, y \in \HH\) when the context is clear. 

\item We denote the number of samples in each dataset $\cD_j(a=t)$ as \(n_{j,t}\) for \(j \in \{1,2,3\}\) and \(t \in \{0,1\}\), and the number of samples in \(\cD_j\) as \(n_j\).
Note that $n_1 = n_{1,0} + n_{1,1}$, $n_2 = n_{2,0} + n_{2,1}$, and $n_3 = n_{3,0} + n_{3,1}$.

\item Define $\tilde{\theta}_1, \tilde{\theta}_0$ as the corresponding Hilbertian elements of $\tilde{f}_0, \tilde{f}_1$ in Algorithm~\labelcref{algorithm; model selection} and define $\tilde{\eta} := \tilde{\theta}_1 - \tilde{\theta}_0$. 

\item Define $\cH$ as the set of candidate estimators defined in our algorithm.

\item Define \(\yb_{j,t}, \bm{\varepsilon}_{j,t}\) as the response vector and noise vector of dataset \(\cD_j(a=t)\).
Also, define \(\yb_{j}, \bm{\varepsilon}_{j}\) as the response vector and noise vector of dataset \(\cD_{j}\).
These are vector versions of \(y_i\) and \(\varepsilon_i\) contained in \(\cD_j(a=t)\) and \(\cD_j\).


\item For \(\ell\)-polynomial decay eigenvalues (Assumption~\labelcref{Assumption; eigenvalue decay}), we define \(\alpha = \frac{2\ell}{1+2\ell}\).

\item We define the noises of dataset $\cD_j$ as $\{\varepsilon_{ji}\}_{i=1}^{n_j}$ for \(j=1,2,3\).
\end{itemize}

\subsection{Proof Workflow}
In Appendix~\labelcref{section; good events and second moments}, we first define the good event $\event$, which ensures sufficient concentration for the empirical second moments.  
Roughly speaking, under the event $\event$, every empirical second moment  
$\Sighat_{j,t}$ for $j = 1, 2, 3$ and $t = 0, 1$ is sufficiently concentrated around $\bSigma_{t}$.  
The rigorous definition can be found in Appendix~\labelcref{section; good events and second moments}.  
We will establish that $\PP[\event] \geq 1 - n^{-11}$ in the next section, and the rest of the analysis is carried out under the event $\event$.  

Our goal is to prove the main result, Theorem~\labelcref{theorem; main theorem}. The workflow of the proof is as follows:  
\begin{enumerate}  
\item Under the event $\event$, we first bound the MSE of the estimator $\hat{h}_{\bm{\lambda}}$ for a fixed $\bm{\lambda}$.  
This result establishes Theorem~\labelcref{theorem; MSE bound RA learner} (presented in Appendix~\labelcref{section; proof RA learner}).  

\item Using Theorem~\labelcref{theorem; MSE bound RA learner}, we derive Corollary~\labelcref{corollary; optimal MSE bound} (presented in Appendix~\labelcref{section; proof RA learner}).  

\item Under the event $\event$, we prove the oracle inequality for the in-sample MSE in Proposition~\labelcref{proposition; in-sample MSE oracle} (presented in Appendix~\labelcref{section; proofs oracle inequalities}).  

\item Using the result of Proposition~\labelcref{proposition; in-sample MSE oracle}, we establish the oracle inequality for the MSE in Proposition~\labelcref{proposition; oracle inequality MSE} (presented in Appendix~\labelcref{section; proofs oracle inequalities}).  

\item Finally, by combining Proposition~\labelcref{proposition; oracle inequality MSE}, Theorem~\labelcref{theorem; MSE bound RA learner}, and Corollary~\labelcref{corollary; optimal MSE bound}, we prove the main result, Theorem~\labelcref{theorem; main theorem} (presented in Appendix~\labelcref{section; proof main theorem}).  
\end{enumerate}  




\section{Second-moment Concentrations and Good Event $\event$}\label{section; good events and second moments}
In this section, we analyze the concentration of second moments and define a good event named $\event$. Additionally, under $\event$, we present important results that are widely used in proofs: Lemma~\labelcref{lemma; moment ratio in E1} and Corollary~\labelcref{corollary; application of Lemma second moment ratio}.

\subsection{Good Event: Sufficient Second Moment Concentrations}

Here, we present the concentration of the empirical second-order moment operators and define the good event \(\event\). For the definition of empirical second moments, please refer to Appendix~\labelcref{subsection; notations}. We investigate several concentration bounds using Lemma~\labelcref{lemma; trace class concentration bounded}. By applying Lemma~\labelcref{lemma; trace class concentration bounded} multiple times, we establish the existence of an absolute constant \(c_0\) that satisfies the following concentration inequalities.

\underline{Concentration 1:} \quad 

With probability at least $1 - \frac{n^{-11}}{8}$, for any $\mu \geq \frac{c_0 \xi \log n}{n}$, the following inequalities hold:
\begin{align*}
\frac{1}{2}(\Sighat_{2,1} + \mu \mathbf{I}) \preceq \Sigmatreated + \mu \mathbf{I} \preceq 2(\Sighat_{2,1} + \mu \mathbf{I}).
\end{align*}

Similarly, with the same probability and for the same $\mu$, we have:
\begin{align*}
\frac{1}{2}(\Sighat_{2,0} + \mu \mathbf{I}) \preceq \Sigmacontrol + \mu \mathbf{I} \preceq 2(\Sighat_{2,0} + \mu \mathbf{I}),
\end{align*}
and
\begin{align*}
\frac{1}{2}(\Sighat_{2} + \mu \mathbf{I}) \preceq \bSigma + \mu \mathbf{I} \preceq 2(\Sighat_{2} + \mu \mathbf{I}).
\end{align*}

\underline{Concentration 2:} \quad 

Similarly, for $\cD_1$, with probability at least $1 - \frac{2}{8}n^{-11}$ and for any $\mu \geq \frac{c_0 \xi \log n}{n}$, the following inequalities hold:
\begin{align*}
\frac{1}{2}(\Sighat_{1,1} + \mu \mathbf{I}) \preceq \Sigmatreated + \mu \mathbf{I} \preceq 2(\Sighat_{1,1} + \mu \mathbf{I}),
\end{align*}
and
\begin{align*}
\frac{1}{2}(\Sighat_{1,0} + \mu \mathbf{I}) \preceq \Sigmacontrol + \mu \mathbf{I} \preceq 2(\Sighat_{1,0} + \mu \mathbf{I}).
\end{align*}

\underline{Concentration 3:} \quad 

For any $\mu \geq \frac{c_0 \xi \log n}{n}$, with probability at least $1 - \frac{2}{8}n^{-11}$, the following inequalities hold:
\begin{align*}
\frac{1}{2}(\Sighat_{3,1} + \mu \mathbf{I}) \preceq \Sigmatreated + \mu \mathbf{I} \preceq 2(\Sighat_{3,1} + \mu \mathbf{I}),
\end{align*}
\begin{align*}
\frac{1}{2}(\Sighat_{3,0} + \mu \mathbf{I}) \preceq \Sigmacontrol + \mu \mathbf{I} \preceq 2(\Sighat_{3,0} + \mu \mathbf{I}).
\end{align*}

Next, we present similar second-moment concentration bounds for the target data.

\underline{Concentration 4:} \quad 

For any $\mu' \geq c_0\frac{\xi (\log n_\cT + \log n)}{n_\cT}$, with probability at least $1 - \frac{n^{-11}}{8}$, the following inequality holds:
\begin{align*}
\frac{1}{2}(\Sighat_{\cT} + \mu' \mathbf{I}) \preceq \bSigma_{\cT} + \mu' \mathbf{I} \preceq 2(\Sighat_{\cT} + \mu' \mathbf{I}).
\end{align*}

\begin{definition}[Good event]
We define the good event $\event$ as the event in which Concentrations 1, 2, 3, and 4 hold for all $\mu \geq c_0\frac{\xi\log n}{n}$ and $\mu' \geq c_0\frac{\xi (\log n_\cT + \log n)}{n_\cT}$. Then, we have
\begin{align*}
\PP[\event] \geq 1 - n^{-11}
\end{align*}
by the previous observations.
\end{definition}

\subsection{Second Moment Ratio Bounds under the Event \(\event\)}
Next, we present important and useful properties under the event $\event$. Note that our regularizers for nuisance estimation are given by $\lambda_{1,0}, \lambda_{1,1} = \frac{\xi \log n}{n}$. Accordingly, our analysis is conducted within the range defined by these regularizers.

\begin{lemma}[Second moment concentrations in \(\event\)]\label{lemma; moment ratio in E1}
Under the event \(\event\), for any \(\lambda \geq \frac{\xi \log n}{n}\), \(j \in \{1,2,3\}\), and \(t \in \{0,1\}\), the following inequalities hold for some absolute constant \(c_1>0\):
\begin{align*}
\frac{1}{c_1}(\Sighat_{j,t} + \lambda \Ib) \preceq \bSigma_{t} + \lambda \Ib \preceq c_1 (\Sighat_{j,t} + \lambda \Ib),
\end{align*}
and
\begin{align*}
\frac{1}{c_1}(\Sighat_{2} + \lambda \Ib) \preceq \bSigma + \lambda \Ib \preceq c_1 (\Sighat_{2} + \lambda \Ib).
\end{align*}
Additionally, we have the following properties:
\begin{align*}
\frac{1}{c_2 R} (\Sighat_{1,1} + \lambda \Ib) \preceq \Sighat_{2,0} + \lambda \Ib \preceq c_2 R (\Sighat_{1,1} + \lambda \Ib),
\end{align*}
and
\begin{align*}
\frac{1}{c_2 R} (\Sighat_{1,0} + \lambda \Ib) \preceq \Sighat_{2,1} + \lambda \Ib \preceq c_2 R (\Sighat_{1,0} + \lambda \Ib)
\end{align*}
for some absolute constant $c_2>0$.
\end{lemma}

\begin{proof}
Let \(\mu = \frac{c_0 \xi \log n}{n}\). 

First, we prove the first and second inequalities. Observe that these inequalities hold directly for when \(\lambda \geq \mu\). 
For \(\lambda < \mu\), note that \(\frac{\mu}{\lambda} \leq c_0\). For \(j \in \{1,2,3\}\), under the event \(\event\),
\begin{align*}
\bSigma + \lambda \Ib &\succeq (\bSigma + \mu \Ib) \frac{\lambda}{\mu} \\
&\succeq (\Sighat_2 + \mu \Ib) \frac{\lambda}{2\mu} \\
&\succeq \frac{1}{2c_0} (\Sighat_2 + \mu \Ib) \\
&\succeq \frac{1}{2c_0} (\Sighat_2 + \lambda \Ib).
\end{align*}
Similarly,
\begin{align*}
\Sighat_2 + \lambda \Ib &\succeq \frac{\lambda}{\mu} (\Sighat_2 + \mu \Ib) \\
&\succeq \frac{1}{c_0} (\Sighat_2 + \mu \Ib) \\
&\succeq \frac{1}{2c_0} (\bSigma + \mu \Ib) \\
&\succeq \frac{1}{2c_0} (\bSigma + \lambda \Ib).
\end{align*}
Since \(c_0\) is an absolute constant, we have
\begin{align*}
\frac{1}{c_1}(\Sighat_{2} + \lambda \Ib) \preceq \bSigma + \lambda \Ib \preceq c_1 (\Sighat_{2} + \lambda \Ib)
\end{align*}
for some absolute constant \(c_1 > 0\).

Similarly, for all \(j \in \{1,2,3\}\) and \(t \in \{0,1\}\),
\begin{align*}
\frac{1}{c_1}(\Sighat_{j,t} + \lambda \Ib) \preceq \bSigma_{t} + \lambda \Ib \preceq c_1 (\Sighat_{j,t} + \lambda \Ib).
\end{align*}

Next, we prove the third and fourth inequalities. Using the above observations and Assumption~\labelcref{assumption; weak treatment overlap}, we have
\begin{align*}
\Sighat_{2,0} + \lambda \Ib &\preceq c_1 (\Sigmacontrol + \lambda \Ib) \\
&\preceq c_1 (R\Sigmatreated + R\frac{\xi}{n} \Ib + \lambda \Ib) \quad \text{(by Assumption~\ref{assumption; weak treatment overlap})}\\
&\preceq 2c_1 R (\Sigmatreated + \lambda \Ib)  \\
&\preceq 2c_1^2 R (\Sighat_{1,1} + \lambda \Ib) \quad (\text{by the first inequality}),
\end{align*}
and
\begin{align*}
\Sighat_{1,1} + \lambda \Ib &\preceq c_1 (\Sigmatreated + \lambda \Ib) \quad (\text{by the first inequality}) \\
&\preceq c_1 (R\Sigmacontrol + R\frac{\xi}{n} \Ib + \lambda \Ib) \quad \text{(by Assumption~\ref{assumption; weak treatment overlap})} \\
&\preceq 2c_1 R (\Sigmacontrol + \lambda \Ib) \\
&\preceq 2c_1^2 R (\Sighat_{2,0} + \lambda \Ib).
\end{align*}
The same reasoning applies to the fourth inequality, proving the lemma.
\end{proof}

Next, we state our key lemma for matrix calculations under the good event \(\event\).

\begin{corollary}[Second moment ratio upper bounds]\label{corollary; application of Lemma second moment ratio}
Under the event $\event$, the following inequalities hold for any $\lambda \geq \frac{\xi \log n}{n}$ for some absolute constant \(c>0\):
\begin{align*}
(\Sighat_{1,1} + \lambda \Ib)^{-\frac{1}{2}} \Sighat_{2,0} (\Sighat_{1,1} + \lambda \Ib)^{-\frac{1}{2}} &\preceq c R \Ib, \\
(\Sighat_{1,0} + \lambda \Ib)^{-\frac{1}{2}} \Sighat_{2,1} (\Sighat_{1,0} + \lambda \Ib)^{-\frac{1}{2}} &\preceq c R \Ib, \\
\Sighat_{2,1}^{\frac{1}{2}} (\Sighat_{1,0} + \lambda \Ib)^{-\frac{1}{2}} \Sighat_{2,1}^{\frac{1}{2}} &\preceq c R \Ib, \\
\Sighat_{2,0}^{\frac{1}{2}} (\Sighat_{1,1} + \lambda \Ib)^{-\frac{1}{2}} \Sighat_{2,0}^{\frac{1}{2}} &\preceq c R \Ib.
\end{align*}
Additionally, we have:
\begin{align*}
(\widehat{\bSigma}_{2} + \lambda \Ib)^{-\frac{1}{2}} \bSigma_\cT (\widehat{\bSigma}_{2} + \lambda \Ib)^{-\frac{1}{2}} &\preceq c B \Ib, \\
\bSigma_\cT^{\frac{1}{2}} (\widehat{\bSigma}_{2} + \lambda \Ib)^{-1} \bSigma_\cT^{\frac{1}{2}} &\preceq c B \Ib.
\end{align*}
\end{corollary}

\begin{proof}
First, consider the first inequality:
\begin{align*}
(\Sighat_{1,1} + \lambda \Ib)^{-\frac{1}{2}} \Sighat_{2,0} (\Sighat_{1,1} + \lambda \Ib)^{-\frac{1}{2}} 
&\preceq (\Sighat_{1,1} + \lambda \Ib)^{-\frac{1}{2}} (\Sighat_{2,0} + \lambda \Ib) (\Sighat_{1,1} + \lambda \Ib)^{-\frac{1}{2}} \\
&\preceq c_2 R (\Sighat_{1,1} + \lambda \Ib)^{-\frac{1}{2}} (\Sighat_{1,1} + \lambda \Ib) (\Sighat_{1,1} + \lambda \Ib)^{-\frac{1}{2}} \\
&\preceq c_2 R \Ib,
\end{align*}
where we used Lemma~\labelcref{lemma; moment ratio in E1} for the second line.
The second inequality can be proved similarly.

For the third inequality:
\begin{align*}
\Sighat_{2,1}^{\frac{1}{2}} (\Sighat_{1,0} + \lambda \Ib)^{-\frac{1}{2}} \Sighat_{2,1}^{\frac{1}{2}} 
&\preceq c_2 R \Sighat_{2,1}^{\frac{1}{2}} (\Sighat_{2,1} + \lambda \Ib)^{-\frac{1}{2}} \Sighat_{2,1}^{\frac{1}{2}} \\
&\preceq c_2 R \Ib,
\end{align*}
where we used Lemma~\labelcref{lemma; moment ratio in E1} and Lemma~\labelcref{lemma; matrix inverse inequality} for the first line.
The fourth inequality is proved similarly.

For the fifth inequality:
\begin{align*}
(\widehat{\bSigma}_{2} + \lambda \Ib)^{-\frac{1}{2}} \bSigma_\cT (\widehat{\bSigma}_{2} + \lambda \Ib)^{-\frac{1}{2}}  
&\stackrel{(i)}{\preceq} (\widehat{\bSigma}_{2} + \lambda \Ib)^{-\frac{1}{2}} B (\bSigma + \lambda \Ib) (\widehat{\bSigma}_{2} + \lambda \Ib)^{-\frac{1}{2}} \\
&\stackrel{(ii)}{\preceq} c_1 (\widehat{\bSigma}_{2} + \lambda \Ib)^{-\frac{1}{2}} B (\Sighat_2 + \lambda \Ib) (\widehat{\bSigma}_{2} + \lambda \Ib)^{-\frac{1}{2}} \\
&= c_1 B \Ib,
\end{align*}
where in step (i) we used Assumption~\labelcref{assumption; overlap source target}, and in step (ii) we applied Lemma~\labelcref{lemma; moment ratio in E1}.
The sixth inequality holds similarly.

\end{proof}


We finalize the section by presenting useful lemma, that gives relation between \(\Sigmacontrol, \Sigmatreated, \bSigma\) under the Assumption~\labelcref{assumption; weak treatment overlap}.
\begin{lemma}\label{lemma; second moment relation under weak overlap}
Under the Assumption~\labelcref{assumption; weak treatment overlap}, for any \(\lambda \geq \frac{\xi \log n}{n}\),
\begin{align*}
\Sigmatreated +\lambda \Ib \preceq c R (\Sigmacontrol +\lambda \Ib) \\
\Sigmacontrol +\lambda \Ib \preceq c R (\Sigmatreated +\lambda \Ib).
\end{align*}
and 
\begin{align*}
\bSigma +\lambda \Ib \preceq c R (\Sigmacontrol +\lambda \Ib) \\
\bSigma +\lambda \Ib \preceq c R (\Sigmatreated +\lambda \Ib).
\end{align*}
holds for some absolute constant \(c>0\).
\end{lemma}

\begin{proof}
For the first inequality, observe that 
\begin{align*}
\Sigmatreated +\lambda \Ib &\preceq R(\Sigmacontrol+ \frac{\xi}{n} \Ib) + \lambda \Ib \quad \text{(By Assumption~\labelcref{assumption; weak treatment overlap})} \\
&\preceq R(\Sigmacontrol+ \lambda  \Ib) + \lambda \Ib \\
&\preceq 2R(\Sigmacontrol+ \lambda  \Ib).
\end{align*} 
The second inequality can be proved similarly.
We next prove the third inequality.
By using the established first inequality, we get
\begin{align*}
\bSigma +\lambda \Ib &= \Sigmacontrol + \Sigmatreated + \lambda \Ib \\
&\preceq \Sigmacontrol + 2R(\Sigmacontrol +\lambda \Ib)  \\
&\preceq 3R(\Sigmacontrol +\lambda \Ib).
\end{align*}
The fourth one can be proved similarly.
\end{proof}



\section{Proofs for Theorem~\labelcref{theorem; MSE bound RA learner} and Corollary~\labelcref{corollary; optimal MSE bound}} \label{section; proof RA learner}
\noindent
This entire section is dedicated to proving Theorem~\labelcref{theorem; MSE bound RA learner} and Corollary~\labelcref{corollary; optimal MSE bound}.
Throughout, we work within the good event \(\event\), which is defined in Appendix~\labelcref{section; good events and second moments}.
As proved in Appendix~\labelcref{section; good events and second moments}, \(\PP[\event] \geq 1 - n^{-11}\).

\subsection{Hilbertian Formulation of RA Learner}\label{subsection; Hilbertian formulation RA learner}
\noindent
We begin the proof by reformulating the RA learner in the language of Hilbertian elements, as established in Appendix~\labelcref{section; groundwork}. 
For the estimator \(\hat{h}_{\bm{\lambda}}\) obtained from the RA learner with regularizers \(\bm{\lambda} = (\lambda_{1,0}, \lambda_{1,1}, \lambda_2)\), let \(\hat{\eta}_{\bm{\lambda}}\) be its corresponding Hilbert space element.

The RA learner performs nuisance estimation in the first stage.
Let \(\hat{\theta}_0\) and \(\hat{\theta}_1\) be the Hilbertian elements corresponding to \(\hat{f}_0\) and \(\hat{f}_1\), defined as 
\begin{align*}
&\hat{\theta}_1 := (\Xb_{1,1}\Xb_{1,1}^\top + n_{1} \lambda_{1,1}\Ib)^{-1} \Xb_{1,1}^\top \yb_{1,1},\\
&\hat{\theta}_0 := (\Xb_{1,0}\Xb_{1,0}^\top + n_{1} \lambda_{1,0}\Ib)^{-1} \Xb_{1,0}^\top \yb_{1,0}.
\end{align*}
Recall that the pseudo-outcome in Algorithm~\labelcref{algorithm; RA learner} is defined as 
\[
m_{2i} := (y_{2i} - x_{2i}^\top \hat{\theta}_0)\indicator(a_{2i}=1) + (x_{2i}^\top \hat{\theta}_1 - y_{2i}) \indicator(a_{2i}=0).
\]
With a slight abuse of notation, set the target MSE of any CATE estimator \(\hat{\eta}\) as 
\[
\Ecal_{\cT}(\hat{\eta}) = \EE_{x \sim \cP_\cT} \bigl|x^\top(\hat{\eta}-\eta^\star)\bigr|^2 
= \|\hat{\eta} - \eta^\star \|_{\bSigma_\cT}^2.
\]
Using the RKHS formulation, we can restate Theorem~\labelcref{theorem; MSE bound RA learner} as:
\[
\cE_\cT(\hat{\eta}_{\bm{\lambda}}) 
\lesssim R \|\Sbar_{\lambda_2} \|_{\op}
\bigl(\lambda_{1,1}\norm{\theta^\star_1}_{\HH}^2 + \lambda_{1,0}\norm{\theta^\star_0}_{\HH}^2\bigr)
+ \lambda_2 \|\Sbar_{\lambda_2} \|_{\op} \|\eta^{\star}\|_{\HH}^2
+\sig^2  \frac{ R\operatorname{Tr}\bigl(\Sbar_{\lambda_2}\bigr)}{n}\log n,
\]
which holds for all \(\lambda_{1,0}, \lambda_{1,1}, \lambda_{2} >0\).
The equivalent statement of Corollary~\labelcref{corollary; optimal MSE bound} is:
\[
\min_{\bm{\lambda} \in \bm{\Lambda}} \cE_\cT(\hat{\eta}_{\bm{\lambda}}) 
\lesssim 
\Bigl(\frac{BR}{n}\Bigr)^{\alpha}
\norm{\eta^\star}_{\HH}^{2(1-\alpha)}
(\log n)^\alpha
+ \frac{\xi BR}{n}\max\bigl(\|\theta_{0}^\star\|_{\HH}, \|\theta_{1}^\star\|_{\HH}\bigr)^2 \log n.
\]
We will prove both results in this section.
In what follows, we analyze the MSE of \(\hat{\eta}_{\bm{\lambda}}\).

\subsection{Decomposition of MSE}\label{subsection; MSE decomposition}
\noindent
By the definition of the RA learner, we can write the estimator in closed form as
\begin{align*}
\hat{\eta}_{\bm{\lambda}} 
&= (\Sighat_2 + \lambda_2 \mathbf{I})^{-1} \frac{1}{n_2} \Bigl(\sum_{\cD_2(a=1)}x_{2i} \bigl(y_{2i} - x_{2i}^\top \hat{\theta}_0\bigr) +\sum_{\cD_2(a=0)} x_{2i}\bigl( x_{2i}^\top \hat{\theta}_1 - y_{2i}\bigr)\Bigr)\\
&=(\Sighat_2 + \lambda_2 \mathbf{I})^{-1}  \frac{1}{n_2}\Bigl(\sum_{\cD_2(a=1)} x_{2i}\bigl(y_{2i} - x_{2i}^\top {\theta}^\star_0 \bigr) + x_{2i}x_{2i}^\top\bigl({\theta}^\star_0-\hat{\theta}_0\bigr)\\
&\quad +\sum_{\cD_2(a=0)} x_{2i}\bigl( x_{2i}^\top {\theta}^\star_1 -y_{2i}\bigr) - x_{2i}x_{2i}^\top\bigl({\theta}^\star_1-\hat{\theta}_1\bigr) \Bigr) \\
&= (\Sighat_2 + \lambda_2 \mathbf{I})^{-1}  \frac{1}{n_2}\Bigl(\sum_{\cD_2(a=1)} x_{2i}\bigl(x_{2i}^\top \theta_1^\star + \varepsilon_{2i} - x_{2i}^\top {\theta}^\star_0\bigr) +x_{2i}x_{2i}^\top\bigl({\theta}^\star_0-\hat{\theta}_0\bigr)\\
&\quad+\sum_{\cD_2(a=0)} x_{2i}\bigl( x_{2i}^\top {\theta}^\star_1 -x_{2i}^\top \theta_0^\star - \varepsilon_{2i}\bigr)
+x_{2i}x_{2i}^\top\bigl(\hat{\theta}_1-{\theta}^\star_1\bigr)\Bigr) \\
&= (\Sighat_2 + \lambda_2 \mathbf{I})^{-1}  \frac{1}{n_2}\Bigl(\sum_{i=1}^{n_2}x_{2i}x_{2i}^\top\bigl(\theta_1^\star-\theta_0^\star\bigr) 
+
\sum_{\cD_2(a=1)} \bigl(x_{2i}\varepsilon_{2i} +x_{2i}x_{2i}^\top\bigl({\theta}^\star_0-\hat{\theta}_0\bigr)\bigr) \\
&\quad +\sum_{\cD_2(a=0)}  \bigl(- x_{2i}\varepsilon_{2i}+x_{2i}x_{2i}^\top\bigl(\hat{\theta}_1-{\theta}^\star_1\bigr)\bigr)\Bigr)\\
&= (\Sighat_2 + \lambda_2 \mathbf{I})^{-1} \frac{1}{n_2}\Bigl(n_2 \Sighat_2 \eta^\star + 
\sum_{\cD_2(a=1)} \bigl(x_{2i}\varepsilon_{2i} +x_{2i}x_{2i}^\top\bigl({\theta}^\star_0-\hat{\theta}_0\bigr)\bigr) \\
&\quad +\sum_{\cD_2(a=0)} \bigl(- x_{2i}\varepsilon_{2i}+x_{2i}x_{2i}^\top\bigl(\hat{\theta}_1-{\theta}^\star_1\bigr)\bigr)\Bigr).
\end{align*}
Then, the difference \(\hat{\eta}_{\bm{\lambda}}-\eta^\star\) is
\begin{align*}
\hat{\eta}_{\bm{\lambda}} -\eta^\star 
&= (\Sighat_2 + \lambda_2 \mathbf{I})^{-1} \frac{1}{n_2}\Bigl(\sum_{\cD_2(a=1)} \bigl(x_{2i}\varepsilon_{2i} +x_{2i}x_{2i}^\top\bigl({\theta}^\star_0-\hat{\theta}_0\bigr)\bigr)\\
&\quad +\sum_{\cD_2(a=0)} \bigl(-x_{2i}\varepsilon_{2i}+x_{2i}x_{2i}^\top\bigl(\hat{\theta}_1-{\theta}^\star_1\bigr)\bigr) - n_2\lambda_2 \eta^\star \Bigr) \\
&= (\Sighat_2 + \lambda_2 \mathbf{I})^{-1} \Bigl( \frac{1}{n_2} \bigl(\sum_{\cD_2(a=1)}x_{2i} \varepsilon_{2i}+\sum_{\cD_2(a=0)} -x_{2i}\varepsilon_{2i}\bigr) -\lambda_2 \eta^\star\Bigr)\\
&\quad + (\Sighat_2 + \lambda_2 \mathbf{I})^{-1} \Bigl( \frac{1}{n_2}\bigl( n_2\Sighat_{2,1}\bigl({\theta}^\star_0-\hat{\theta}_0\bigr)
+ n_2\Sighat_{2,0} \bigl(\hat{\theta}_1-{\theta}^\star_1\bigr)\bigr)\Bigr) \\
&= (\Sighat_2 + \lambda_2 \mathbf{I})^{-1} \Bigl( \frac{1}{n_2} \bigl(\sum_{\cD_2(a=1)}x_{2i} \varepsilon_{2i}+\sum_{\cD_2(a=0)} -x_{2i}\varepsilon_{2i}\bigr) -\lambda_2 \eta^\star\Bigr)\\
&\quad + (\Sighat_2 + \lambda_2 \mathbf{I})^{-1} \Bigl( \Sighat_{2,1}\bigl({\theta}^\star_0-\hat{\theta}_0\bigr)
+ \Sighat_{2,0} \bigl(\hat{\theta}_1-{\theta}^\star_1\bigr)\Bigr).
\end{align*}
To control the MSE, define each term as
\begin{align*}
\Vcr &= \norm{(\Sighat_2 + \lambda_2 \mathbf{I})^{-1}\Bigl( \frac{1}{n_2} \bigl(\sum_{\cD_2(a=1)}x_{2i} \varepsilon_{2i}+\sum_{\cD_2(a=0)} -x_{2i}\varepsilon_{2i}\bigr)\Bigr)}_{\bSigma_{\cT}},\\
\Bcr &= \norm{(\Sighat_2 + \lambda_2 \mathbf{I})^{-1}\lambda_2 \eta^\star}_{\bSigma_{\cT}},\\
\Pcr_0 &= \norm{(\Sighat_2 + \lambda_2 \mathbf{I})^{-1} \Sighat_{2,1}\bigl(\hat{\theta}_0-{\theta}^\star_0\bigr)}_{\bSigma_{\cT}},\\
\Pcr_1 &= \norm{(\Sighat_2 + \lambda_2 \mathbf{I})^{-1} \Sighat_{2,0} \bigl(\hat{\theta}_1-{\theta}^\star_1\bigr)}_{\bSigma_{\cT}}.
\end{align*}
Then, the MSE can be decomposed as
\[
\norm{\hat{\eta}_{\bm{\lambda}} -\eta^\star}_{\bSigma_{\cT}} \leq \mathscr{V}+\mathscr{B}+ \mathscr{P}_0 +  \mathscr{P}_1.
\]
In the following sections, we derive bounds for each term.

\subsection{Bounding \(\mathscr{V}\) and \(\mathscr{B}\)}
\noindent
We can control \(\Vcr^2 + \Bcr^2\) using techniques from \citet{wang2023pseudo,ma2023optimally}, along with choosing \(\lambda_2 \asymp n_2^{-\alpha}\).
We first present the upper bound of \(\Vcr^2 + \Bcr^2\).
We introduce \(\delta_1 >0\) as an additional probability parameter under \(\event\), which can be arbitrary. 
In the end, we set \(\delta_1 = \frac{n^{-11}}{|\cH|}\).

\begin{lemma}\label{lemma; RA learner bound 1}
Set any \(\delta_1>0\).
Under the good event \(\event\), with probability at least \(1- \delta_1/3\), we have
\[
\Vcr^2 + \Bcr^2 \lesssim  \lambda_2\|\Sbar_{\lambda_2}  \|_{\op}\bigl\|\eta^{\star}\bigr\|_{\HH}^2+\sig^2 \frac{ \operatorname{Tr}\bigl(\Sbar_{\lambda_2}\bigr) \log (1 / \delta_1)}{n_2}.
\]
Here, \(\lesssim\) hides absolute constants.
\end{lemma}

\begin{proof}
The noise vectors \(\varepsilon_{2i}(-1)^{a_{2i}+1}, i =1,2,\dots,n\) are mean-zero and sub-Gaussian given \(\{(x_i,a_i) \}_{i=1}^n\). 
Hence, \(\Vcr^2 + \Bcr^2\) can be viewed as the MSE of a general KRR problem, allowing us to apply prior results.
Below is a brief argument. 

For the variance term, by applying the Hanson-Wright inequality (Lemma~\labelcref{lemma; quadratic martingale}), we obtain
\begin{align*}
\Vcr^2 
&\lesssim \sigma^2 \frac{1}{n_2^2}\Tr\Bigl( \Xb_2 \bigl(\Sighat_2 + \lambda_2 \Ib\bigr)^{-1} \bSigma_\cT \bigl(\Sighat_2 + \lambda_2 \Ib\bigr)^{-1} \Xb_2^\top \Bigr) \log\Bigl(\frac{1}{\delta_1}\Bigr)\\
&\lesssim \sigma^2 \frac{1}{n_2}\Tr\Bigl( \bigl(\Sighat_2 + \lambda_2 \Ib\bigr)^{-1} \bSigma_\cT \bigl(\Sighat_2 + \lambda_2 \Ib\bigr)^{-1}\Sighat_2\Bigr)\log\Bigl(\frac{1}{\delta_1}\Bigr)\\
&\lesssim \sigma^2 \frac{1}{n_2}\Tr\Bigl( \bigl(\Sighat_2 + \lambda_2 \Ib\bigr)^{-\frac{1}{2}} \bSigma_\cT \bigl(\Sighat_2 + \lambda_2 \Ib\bigr)^{-\frac{1}{2}} 
\bigl(\Sighat_2 + \lambda_2 \Ib\bigr)^{-\frac{1}{2}} \Sighat_2 \bigl(\Sighat_2 + \lambda_2 \Ib\bigr)^{-\frac{1}{2}}\Bigr) \log\Bigl(\frac{1}{\delta_1}\Bigr)\\
&\stackrel{\text{(i)}}{\lesssim} \sigma^2 \frac{1}{n_2}\Tr\Bigl( \bigl(\Sighat_2 + \lambda_2 \Ib\bigr)^{-\frac{1}{2}} \bSigma_\cT \bigl(\Sighat_2 + \lambda_2 \Ib\bigr)^{-\frac{1}{2}}\Bigr)\log\Bigl(\frac{1}{\delta_1}\Bigr)\\
&\lesssim \sigma^2 \frac{1}{n_2}\Tr\Bigl(\bSigma_\cT \bigl(\Sighat_2 + \lambda_2 \Ib\bigr)^{-1}\Bigr)\log\Bigl(\frac{1}{\delta_1}\Bigr)\\
& \stackrel{\text{(ii)}}{\lesssim} \sigma^2  \frac{1}{n_2}\Tr\Bigl(\bSigma_\cT \bigl(\bSigma + \lambda_2 \Ib\bigr)^{-1}\Bigr)\log\Bigl(\frac{1}{\delta_1}\Bigr)
=\sigma^2\frac{1}{n_2}\Tr\bigl(\Sbb_{\lambda_2}\bigr)\log\Bigl(\frac{1}{\delta_1}\Bigr)
\end{align*}
with probability at least \(1-\frac{\delta_1}{3}\), where we used 
Lemma~\labelcref{lemma; trace simple inequality} in (i), 
and inequality (ii) follows from 
Lemma~\labelcref{lemma; moment ratio in E1,lemma; matrix inverse inequality}.

For the bias term, we have
\begin{align*}
\Bcr^2 
&\lesssim  \lambda_2\Bigl\| \bSigma_\cT^{\frac{1}{2}} \bigl(\Sighat_2 + \lambda_2 \Ib\bigr)^{-\frac{1}{2}}\Bigr\|_{\operatorname{op}} 
\Bigl\|\bigl(\Sighat_2 + \lambda_2 \Ib\bigr)^{-\frac{1}{2}} \eta^\star \Bigr\|_\HH \\
&\lesssim  \Bigl\| \bSigma_\cT^{\frac{1}{2}} \bigl(\Sighat_2 + \lambda_2 \Ib\bigr)^{-\frac{1}{2}}\Bigr\|_{\operatorname{op}} 
\lambda_2^{\frac{1}{2}} \|\eta^\star \|_{\HH} \\
&\lesssim \lambda_2^{\frac{1}{2}}  \|\Sbb_{\lambda_2}\|_{\op} \|\eta^\star\|_{\HH},
\end{align*}
where we used Corollary~\labelcref{corollary; application of Lemma second moment ratio} in the third line.
\end{proof}

\subsection{Bounding \(\mathscr{P}_0\) and \(\mathscr{P}_1\)}
\noindent
The terms \(\Pcr_0\) and \(\Pcr_1\) represent propagated errors from the nuisance estimators \(\hat{\theta}_0\) and \(\hat{\theta}_1\).

Noting that
\begin{align*}
\bSigma_{\cT}^{\frac{1}{2}} (\Sighat_2 + \lambda_2 \mathbf{I})^{-1} \Sighat_{2,1}\bigl({\theta}^\star_0-\hat{\theta}_0\bigr)
&= \bSigma_{\cT}^{\frac{1}{2}} (\Sighat_2 + \lambda_2 \mathbf{I})^{-1} \Sighat_{2,1}\bigl(\Xb_{1,0}^\top \Xb_{1,0}+n_{1}\lambda_{1,0} \Ib\bigr)
^{-1} \bigl(-\Xb_{1,0}^\top \bm{\varepsilon}_{1,0} + n_{1}\lambda_{1,0}\theta_0^\star\bigr) \\
&=  \bSigma_{\cT}^{\frac{1}{2}} (\Sighat_2 + \lambda_2 \mathbf{I})^{-1}  \Sighat_{2,1} \bigl(\Sighat_{1,0}+\lambda_{1,0} \mathbf{I}\bigr)
^{-1} \Bigl(-\frac{1}{n_1}\Xb_{1,0}^\top \bm{\varepsilon}_{1,0} + \lambda_{1,0}\theta_0^\star\Bigr),
\end{align*}
we can write
\[
\Pcr_0 \leq \Vcr_0 + \Bcr_0,
\]
where 
\begin{align*}
\Vcr_0 &:= \Bigl\|   (\Sighat_2 + \lambda_2 \mathbf{I})^{-1}  \Sighat_{2,1} \bigl(\Sighat_{1,0}+\lambda_{1,0} \mathbf{I}\bigr)
^{-1} \frac{1}{n_1}\Xb_{1,0}^\top \bm{\varepsilon}_{1,0} \Bigr\|_{\bSigma_{\cT}},\\
\Bcr_0 &:= \Bigl\| (\Sighat_2 + \lambda_2 \mathbf{I})^{-1}  \Sighat_{2,1} \bigl(\Sighat_{1,0}+\lambda_{1,0} \mathbf{I}\bigr)
^{-1} \lambda_{1,0}\theta_0^\star\Bigr\|_{\bSigma_{\cT}}.
\end{align*}


\begin{lemma}[Propagated bias bound]\label{lemma; bounding B_0}
Under the good event \(\event\),
\[
\mathscr{B}_0^2 \lesssim R  \|\Sbar_{\lambda_2} \|_{\op}\lambda_{1,0}\norm{\theta^\star_0}_{\HH}^2.
\]
Here, \(\lesssim\) hides absolute constants.
\end{lemma}

\begin{proof}
By direct calculation,
\begin{align*}
\mathscr{B}_0 
&\leq \Bigl\|\bSigma_\cT^{\frac{1}{2}}\bigl(\Sighat_{2}+ \lambda_2 \Ib\bigr)^{-\frac{1}{2}} \Bigr\|_{\op} 
\Bigl\|\bigl(\Sighat_{2}+ \lambda_2 \Ib\bigr)^{-\frac{1}{2}} \Sighat_{2,1}^{\frac{1}{2}}\Bigr\|_{\op} 
\Bigl\|\Sighat_{2,1}^{\frac{1}{2}} \bigl(\Sighat_{1,0}+\lambda_{1,0} \mathbf{I}\bigr)
^{-\frac{1}{2}} \Bigr\|_{\op} 
\sqrt{\lambda_{1,0}}\|\theta_0^\star\|_{\HH}.
\end{align*}
First, by Lemma~\labelcref{lemma; moment ratio in E1} and Lemma~\labelcref{lemma; matrix inverse inequality}, 
\[
\bigl\| \bSigma_{\cT}^{\frac{1}{2}} (\Sighat_2 + \lambda_2 \mathbf{I})^{-1}\bSigma_{\cT}^{\frac{1}{2}}   \bigr\|_{\op} 
\lesssim  \bigl\| \bSigma_{\cT}^{\frac{1}{2}} \bigl(\bSigma + \lambda_2 \mathbf{I}\bigr)^{-1}\bSigma_{\cT}^{\frac{1}{2}} \bigr\|_{\op} 
=\|\Sbb_{\lambda_2} \|_{\op}.
\]
Next, by Lemma~\labelcref{lemma; matrix inverse inequality} and the definition of \(\Sighat_{2,1}\),
\[
\Sighat_{2,1}^{\frac{1}{2}} \bigl(\Sighat_2 + \lambda_2 \mathbf{I}\bigr)^{-1}  \Sighat_{2,1}^{\frac{1}{2}} 
\preceq  \Sighat_{2,1}^{\frac{1}{2}}  \bigl(\Sighat_{2,1} + \lambda_2 \mathbf{I}\bigr)^{-1}  \Sighat_{2,1}^{\frac{1}{2}}  
\preceq \Ib.
\]
Lastly, by Corollary~\labelcref{corollary; application of Lemma second moment ratio},
\[
\bigl(\Sighat_{1,0}+\lambda_{1,0} \mathbf{I}\bigr)
^{-\frac{1}{2}}\Sighat_{2,1} \bigl(\Sighat_{1,0}+\lambda_{1,0} \mathbf{I}\bigr)
^{-\frac{1}{2}}  \preceq  cR \Ib
\]
for some absolute constant \(c>0\).
Putting these together yields
\begin{align*}
\mathscr{B}_0 
&\lesssim \bigl\|\Sbar_{\lambda_2} \bigr\|_{\op}^{\frac{1}{2}} \times 1 \times \sqrt{R}\sqrt{\lambda_{1,0}}\|\theta_0^\star\|_{\HH} 
\lesssim \lambda_{1,0}^{\frac{1}{2}}\bigl\|\Sbar_{\lambda_2} \bigr\|_{\op}^{\frac{1}{2}} \sqrt{R}\|\theta_0^\star\|_{\HH}.
\end{align*}
\end{proof}


\begin{lemma}[Propagated variance bound]\label{lemma; bounding V_0}
Under the good event \(\event\), with probability at least \(1-\frac{\delta_1}{3}\), we have 
\[
\mathscr{V}_0^2 \lesssim \sig^2 \frac{R}{n_1}\Tr\bigl(\Sbar_{\lambda_2}\bigr)\log\Bigl(\frac{1}{\delta_1}\Bigr).
\]
Here, \(\lesssim\) hides absolute constants.
\end{lemma}

\begin{proof}
By the Hanson-Wright inequality (Lemma~\labelcref{lemma; quadratic martingale}), with probability \(1-\frac{\delta_1}{3}\):
\begin{align*}
\mathscr{V}_0^2 
&\lesssim 
{\small \frac{\sig^2 \log \bigl(\frac{3}{\delta_1}\bigr)}{n_1^2}\Tr\Bigl(\Xb_{1,0}\bigl(\Sighat_{1,0}+\lambda_{1,0} \mathbf{I}\bigr)^{-1}\Sighat_{2,1} (\Sighat_2 + \lambda_2 \mathbf{I}\bigr)^{-1}\bSigma_{\cT}\bigl(\Sighat_2 + \lambda_2 \mathbf{I}\bigr)^{-1}\Sighat_{2,1} \bigl(\Sighat_{1,0}+\lambda_{1,0} \mathbf{I}\bigr)^{-1}\Xb_{1,0}^\top \Bigr) }\\
&\lesssim \frac{\sig^2 \log \bigl(\frac{1}{\delta_1}\bigr)}{n_1}\Tr\Bigl(\bigl(\Sighat_{1,0}+\lambda_{1,0} \mathbf{I}\bigr)^{-1}\Sighat_{2,1} (\Sighat_2 + \lambda_2 \mathbf{I}\bigr)^{-1}\bSigma_{\cT}\bigl(\Sighat_2 + \lambda_2 \mathbf{I}\bigr)^{-1}\Sighat_{2,1} \bigl(\Sighat_{1,0}+\lambda_{1,0} \mathbf{I}\bigr)^{-1} \Sighat_{1,0}\Bigr) \\
&= \frac{\sig^2}{n_1} \Tr\Bigl( (\Sighat_2 + \lambda_2 \mathbf{I}\bigr)^{-1}\bSigma_{\cT}\bigl(\Sighat_2 + \lambda_2 \mathbf{I}\bigr)^{-1}\Sighat_{2,1} \bigl(\Sighat_{1,0}+\lambda_{1,0} \mathbf{I}\bigr)^{-1} \Sighat_{1,0} \bigl(\Sighat_{1,0}+\lambda_{1,0} \mathbf{I}\bigr)^{-1}\Sighat_{2,1}\Bigr) \log \bigl(\frac{1}{\delta_1}\bigr)\\
&\stackrel{\text{(i)}}{\lesssim} \frac{\sig^2}{n_1}\Tr\Bigl( (\Sighat_2 + \lambda_2 \mathbf{I}\bigr)^{-1}\bSigma_{\cT}\bigl(\Sighat_2 + \lambda_2 \mathbf{I}\bigr)^{-1}\Sighat_{2,1}\bigl(\Sighat_{1,0}+\lambda_{1,0} \mathbf{I}\bigr)^{-1}\Sighat_{2,1}\Bigr)\log \bigl(\frac{1}{\delta_1}\bigr) \quad (\text{Lemma~\labelcref{lemma; trace simple inequality}})\\
&\stackrel{\text{(ii)}}{\lesssim} \frac{\sig^2}{n_1}\Tr\Bigl( (\Sighat_2 + \lambda_2 \mathbf{I}\bigr)^{-1}\bSigma_{\cT}\bigl(\Sighat_2 + \lambda_2 \mathbf{I}\bigr)^{-1} R\Sighat_{2,1} \Bigr)\log \bigl(\frac{1}{\delta_1}\bigr) \quad (\text{Corollary~\labelcref{corollary; application of Lemma second moment ratio}})\\
&\lesssim \frac{\sig^2R}{n_1}\Tr\Bigl((\Sighat_2 + \lambda_2 \mathbf{I}\bigr)^{-\frac{1}{2}}\bSigma_{\cT}(\Sighat_2 + \lambda_2 \mathbf{I}\bigr)^{-\frac{1}{2}}(\Sighat_2 + \lambda_2 \mathbf{I}\bigr)^{-\frac{1}{2}}\Sighat_{2,1}(\Sighat_2 + \lambda_2 \mathbf{I}\bigr)^{-\frac{1}{2}}\Bigr) \log \bigl(\frac{1}{\delta_1}\bigr)\\
&\stackrel{\text{(iii)}}{\lesssim} \frac{\sig^2R}{n_1}\Tr\Bigl((\Sighat_2 + \lambda_2 \mathbf{I}\bigr)^{-\frac{1}{2}}\bSigma_{\cT}(\Sighat_2 + \lambda_2 \mathbf{I}\bigr)^{-\frac{1}{2}}\Bigr)\log \bigl(\frac{1}{\delta_1}\bigr) \quad (\because \Sighat_2 \succeq \Sighat_{2,1}, \text{ and Lemma~\labelcref{lemma; trace simple inequality}}) \\
&\lesssim  \frac{\sig^2R}{n_1}\Tr\Bigl(\bSigma_{\cT}(\Sighat_2 + \lambda_2 \mathbf{I}\bigr)^{-1}\Bigr) \log \bigl(\frac{1}{\delta_1}\bigr)\\
&\stackrel{\text{(iv)}}{\lesssim} \frac{\sig^2R}{n_1}\Tr\Bigl(\bSigma_{\cT}\bigl(\bSigma + \lambda_2 \mathbf{I}\bigr)^{-1}\Bigr) \log \bigl(\frac{1}{\delta_1}\bigr) \quad (\text{Lemma~\labelcref{lemma; moment ratio in E1,lemma; trace simple inequality}})  \\
&\lesssim  \frac{\sig^2R}{n_1}\Tr\bigl(\Sbar_{\lambda_2}\bigr)\log \bigl(\frac{1}{\delta_1}\bigr).
\end{align*}
In (i) and (iii), we used Lemma~\labelcref{lemma; trace simple inequality}, 
in (ii), we applied Corollary~\labelcref{corollary; application of Lemma second moment ratio} and Lemma~\labelcref{lemma; trace simple inequality}, 
and in (iv), we used Lemma~\labelcref{lemma; moment ratio in E1,lemma; matrix inverse inequality}.
\end{proof}

Combining Lemmas~\labelcref{lemma; bounding B_0} and \labelcref{lemma; bounding V_0}, we obtain:

\begin{corollary}\label{corollary; RA learner bound 2}
Under \(\event\), with probability at least \(1-\frac{\delta_1}{3}\),
\[
\Pcr_0^2 \lesssim R\|\Sbar_{\lambda_2} \|_{\op}\lambda_{1,0}\norm{\theta^\star_0}_{\HH}^2 + \sig^2R \frac{1}{n_1}\Tr\bigl(\Sbar_{\lambda_2}\bigr)\log\Bigl(\frac{1}{\delta_1}\Bigr).
\]
Here, \(\lesssim\) hides absolute constants.
\end{corollary}

Analogously, we also obtain:

\begin{corollary}\label{corollary; RA learner bound 3}
Under \(\event\), with probability at least \(1-\frac{\delta_1}{3}\),
\[
\Pcr_1^2  \lesssim R\|\Sbar_{\lambda_2} \|_{\op}\lambda_{1,1}\norm{\theta^\star_1}_{\HH}^2 
+\sig^2 R \frac{1}{n_1}\Tr\bigl(\Sbar_{\lambda_2}\bigr)\log \Bigl(\frac{1}{\delta_1}\Bigr).
\]
Here, \(\lesssim\) hides absolute constants.
\end{corollary}

\subsection{Proof of Theorem~\labelcref{theorem; MSE bound RA learner}}
\noindent
Recalling that \(n_1 \asymp n_2 \asymp n_3 \asymp n\), combining Lemma~\labelcref{lemma; RA learner bound 1} with Corollaries~\labelcref{corollary; RA learner bound 2} and \ref{corollary; RA learner bound 3} shows that under \(\event\), with probability at least \(1-\delta_1\),
\begin{align*}
\cE_{\cT}(\hat{\eta}_{\bm{\lambda}}) 
&=\bigl\|(\hat{\eta}_{\bm{\lambda}} -\eta^\star)\bigr\|_{\bSigma_{\cT}}^2 
\leq 4\bigl(\Pcr_0^2 + \Pcr_1^2 + \Vcr^2 + \Bcr^2 \bigr)\\
&\lesssim R\|\Sbar_{\lambda_2} \|_{\op}\Bigl(\lambda_{1,0}\norm{\theta_0^\star}_\HH^2+\lambda_{1,1}\norm{\theta_1^\star}_\HH^2\Bigr)+\sig^2\frac{R}{n}\Tr\bigl(\Sbar_{\lambda_2}\bigr)\log \Bigl(\frac{1}{\delta_1}\Bigr) 
+ \lambda_2\|\Sbar_{\lambda_2} \|_{\op}\norm{\eta^\star}^2_\HH.
\end{align*}
Because \(\abs{\cH} \leq \log n\), we set \(\delta_1 =\frac{n^{-11}}{|\cH|}\). Then, with probability at least \(1-2n^{-11}\), the following holds for \emph{all} \(\bm{\lambda} \in \cH\):
\begin{align*}
\cE_{\cT}(\hat{\eta}_{\bm{\lambda}}) 
&\lesssim  \sig^2\frac{R}{n}\Tr\bigl(\Sbar_{\lambda_2}\bigr)\log n 
+\lambda_2 \|\Sbar_{\lambda_2} \|_{\op}\norm{\eta^\star}^2_\HH 
+R\|\Sbar_{\lambda_2} \|_{\op} M(\lambda_{1,0} + \lambda_{1,1}).
\end{align*}
Thus, with probability at least \(1-2n^{-11}\), we have for all estimators \(\hat{\eta}_{\bm{\lambda}} \in \cH\):
\[
\cE_{\cT}(\hat{\eta}_{\bm{\lambda}}) 
\lesssim \sig^2\frac{R}{n}\Tr\bigl(\Sbar_{\lambda_2}\bigr)\log n 
+\lambda_2 \|\Sbar_{\lambda_2} \|_{\op}\norm{\eta^\star}^2_\HH 
+R\|\Sbar_{\lambda_2} \|_{\op}M(\lambda_{1,0} + \lambda_{1,1}),
\]
which completes the proof.

\subsection{Proof of Corollary~\labelcref{corollary; optimal MSE bound}}\label{subsection; proof main corollary}
\noindent
We prove that under the relaxed assumption---that is, when 
\(\|h^\star\|_{\mathcal{F}}\) is bounded by a sufficiently large value---the following inequality holds:
\[
\|h^\star\|_{\mathcal{F}} \lesssim R \left(\frac{n}{B \log n}\right)^{\frac{1}{2\ell}} = \widetilde{\Omega}\Bigl(R \cdot n_{\operatorname{eff}}^{\frac{1}{2\ell}}\Bigr).
\]
Under Assumption~\labelcref{assumption; overlap source target}, we have \(\|\Sbar_{\lambda_2} \|_{\op} \leq B\).
Additionally, recall that \(\lambda_{1,0}, \lambda_{1,1} = \frac{\xi \log n}{n}\).
Hence, the MSE bound simplifies to
\begin{align*}
\cE_{\cT}(\hat{\eta}_{\bm{\lambda}})
&\lesssim \sig^2\frac{R}{n}\Tr\bigl(\Sbar_{\lambda_2}\bigr)\log n 
+\lambda_2B \norm{\eta^\star}^2_\HH 
+\frac{BR\xi\log n}{n}M^2.
\end{align*}
By hiding dependencies on \(\sigma\) and \(\xi\) (which we regard as universal constants), we obtain
\[
\cE_{\cT}(\hat{\eta}_{\bm{\lambda}})
\lesssim \frac{R}{n}\Tr\bigl(\Sbar_{\lambda_2}\bigr)\log n 
+  \lambda_2 B \norm{\eta^\star}^2_\HH 
+\frac{BR\log n}{n}M^2.
\]
We consider two cases:

\paragraph{Case 1:} \(\frac{R\log n}{n^2}\lesssim\|\eta^\star\|^2_\cF.\)
By applying Lemma~\labelcref{lemma; optimal trade-off lambda} and Corollary~\labelcref{corollary; optimal MSE in grid} with \(h = \frac{R}{n}\log n\), we deduce that the value of \(\lambda^\star\) in Corollary~\ref{corollary; optimal MSE in grid} satisfies
\[
\lambda^\star \asymp \Bigl(\frac{R\log n}{n}\Bigr)^{\alpha}B^{-(1-\alpha)} \|\eta^\star \|^{-2\alpha}.
\]
We can check that $\lambda^\star$ lies in our grid $\Lambda_2$, i.e., 
\[
\frac{\xi \log n}{n} \leq \lambda^\star \leq \frac{\xi n \log n}{2}.
\]
Hence, by Lemma~\labelcref{lemma; optimal trade-off lambda} and Corollary~\labelcref{corollary; optimal MSE in grid},
\[
\inf_{\bm{\lambda} \in \bm{\Lambda}}  \cE_{\cT}(\hat{\eta}_{\bm{\lambda}}) 
\lesssim \Bigl(\frac{BR}{n}\Bigr)^\alpha\norm{\eta^\star}^{2(1-\alpha)}_\cF (\log n)^\alpha 
+\frac{BR\log n}{n}M^2.
\]

\paragraph{Case 2:} \(\frac{R\log n}{n^2} \gtrsim\|\eta^\star\|^2_\cF.\)
In this scenario, there exists \(\lambda_2' \in [1,2] \cap \Lambda_2\), and we obtain
\begin{align*}
\inf_{\bm{\lambda} \in \bm{\Lambda}} \cE_{\cT}(\hat{\eta}_{\bm{\lambda}})  
&\lesssim \frac{R}{n}\Tr\bigl(\Sbar_{\lambda_2'}\bigr)\log n 
+  \lambda_2' B \norm{\eta^\star}^2_\HH 
+\frac{BR\log n}{n}M^2 \\ 
&\lesssim \frac{R}{n}\Tr\bigl(\Sbar_{1}\bigr)\log n 
+  2 B \norm{\eta^\star}^2_\HH 
+\frac{BR\log n}{n}M^2 \\  
&\lesssim \frac{BR\log n}{n}M^2,
\end{align*}
where, in the last inequality, we used
\begin{align*}
    \Tr(\Sbb_{1}) \stackrel{(i)}{\lesssim} \sum_{j=1}^\infty \frac{B\mu_j}{\mu_j + 1} \lesssim B \sum_{j \ge 1} \mu_j \lesssim B,
\end{align*}
and (i) follows from the argument in the proof of Lemma~\ref{lemma; optimal trade-off lambda}.


\section{Proofs for Proposition~\labelcref{proposition; oracle inequality MSE,proposition; in-sample MSE oracle}}\label{section; proofs oracle inequalities}


\subsection{Guideline for Proofs}

Using the split data $\cD_3$, we generate test outcomes and perform model selection using them.
The following part describes how we generate the test outcomes in the language of RKHS covariates in Algorithm~\labelcref{algorithm; model selection}.

\begin{enumerate}\label{equation; model selection nuisance estimator}
\item We calculate the nuisance estimator as 
\begin{align*}
&\tilde{\theta}_1 = (\Xb_{3,1}^\top \Xb_{3,1}+n_3\tilde{\lambda}_1\Ib)^{-1}\Xb_{3,1}^\top \yb_{3,1}\\
&\tilde{\theta}_0 = (\Xb_{3,0}^\top \Xb_{3,0}+n_3\tilde{\lambda}_0\Ib)^{-1}\Xb_{3,0}^\top \yb_{3,0}.
\end{align*}
where $\tilde{\theta}_0, \tilde{\theta}_1$ are the corresponding Hilbertian elements of $\tilde{f}_0$ and $\tilde{f}_1$.
\item We define \(\tilde{\eta}:= \tilde{\theta}_1 - \tilde{\theta}_0\), which is corresponding to $\tilde{h}$.
\item Generate the pseudo test outcomes \(\{ x_{0i}^\top \tilde{\eta}\}_{i=1}^{n_{\cT}}\) and perform model selection.
\end{enumerate}

In this section, we first present several norm bounds related to test outcomes $\{\tilde{h}(x_{0i})\}_{i=1}^{n_{\cT}}$. 
Next, using these constructed norm bounds, we apply Lemma~\labelcref{lemma; loss model selection} and obtain an oracle inequality for in-sample MSE.
To investigate the difference between in-sample MSE and MSE, we aim to use Lemma~\labelcref{lemma; key lemma in-sample population}. 
For this, we first bound several norms of the estimators $\hat{\eta}_{\bm{\lambda}}$. 
With these norm bounds, we finally apply Lemma~\labelcref{lemma; key lemma in-sample population} and derive Proposition~\labelcref{proposition; oracle inequality MSE}. 



\subsection{Norm Bounds for Test Outcomes}
To establish an in-sample MSE oracle inequality, we aim to apply Lemma~\ref{lemma; loss model selection}, which requires some preparation. To this end, for our test-outcome parameter \(\tilde{\eta} = \tilde{\theta}_1 - \tilde{\theta}_0\), we bound several related \(\psi_2\) and Hilbert norms.
We define \(\EE[\tilde{\eta}]\) as the expectation taken with the noise variables $\varepsilon_1, \dots \varepsilon_n$.
Recall that we have defined $\Shat_\lambda := (\bSigma + \lambda \Ib)^{-\frac{1}{2}} \Sighat_{\cT} (\bSigma + \lambda \Ib)^{-\frac{1}{2}}$ for any \(\lambda >0\).

\begin{lemma}\label{lemma; test outcome norm preparation}
Under the good event $\event$, the test outcomes satisfy 
\begin{align*}
\frac{1}{n_\cT} ||\Xb_\cT^\top (\EE[\tilde{\eta}] -\eta^\star) ||^2_2 &\lesssim 
R\tilde{\lambda}_1 \norm{\theta_1^\star}_{\HH}^2 \norm{\Shat_{\tilde{\lambda}_1}}_{\op} + R  \tilde{\lambda}_0 \norm{\theta_0^\star}_{\HH}^2 \norm{\Shat_{\tilde{\lambda}_0}}_{\op}\\
\|\Xb_\cT^\top (\tilde{\eta} -\EE[\tilde{\eta}]) \|^2_{\psi_2} &\lesssim \sig^2  R\frac{n_{\cT}}{n_3}(\norm{\Shat_{\tilde{\lambda}_1}}_{\op} + \norm{\Shat_{\tilde{\lambda}_0}}_{\op})\\
\norm{\Shat_{\tilde{\lambda}_1}}_{\op} &\lesssim \norm{\Sbar_{\tilde{\lambda}_1}}_{\op} + \frac{n_3 \log (n_\cT n)}{n_{\cT} \log n}\\
\norm{\Shat_{\tilde{\lambda}_0}}_{\op} &\lesssim \norm{\Sbar_{\tilde{\lambda}_0}}_{\op} + \frac{n_3 \log (n_\cT n)}{n_{\cT} \log n}.
\end{align*}
Here, \(\lesssim\) hides absolute constants.
\end{lemma}

\begin{remark}
The first term is related to $\cE_{\cT}^{\inn}(\EE[\tilde{\eta}])$ in Lemma~\labelcref{lemma; loss model selection}, and the second term is related to the variance term of Lemma~\labelcref{lemma; loss model selection}.
By applying Lemma~\labelcref{lemma; loss model selection}, we first establish the oracle inequality for in-sample MSE.
\end{remark}

\begin{proof}
For any norm $ \| \cdot \|$, we get the decomposed upper bound as
\begin{align*}
\|\Xb_\cT^\top (\EE[\tilde{\eta}] -\eta^\star) \| &=  \|\Xb_\cT^\top (\EE[\tilde{\theta_1}] -\theta_1^\star) - \Xb_\cT^\top (\EE[\tilde{\theta}_0] -\theta_0^\star)  \|\\
&\leq \|\Xb_\cT^\top (\EE[\tilde{\theta_1}] -\theta_1^\star) \|  +\| \Xb_\cT^\top (\EE[\tilde{\theta}_0] -\theta_0^\star)  \|.
\end{align*}

We aim to prove the first inequality.
Observe that
\begin{align*}
\|\Xb_\cT^\top (\EE[\tilde{\theta_1}] -\theta_1^\star) \|_{2} &=  \|\Xb_\cT^\top ( \Sighat_{3,1} +\tilde{\lambda}_{1} \Ib)^{-1} \tilde{\lambda}_{1} \theta_1^\star  \|_{2}\\
&\leq \tilde{\lambda}_{1} \|\Xb_\cT^\top (\Sighat_{3,1} + \tilde{\lambda}_{1} \Ib)^{-\frac{1}{2}} \|_{\op} \| ( \Sighat_{3,1} + \tilde{\lambda}_{1} \Ib)^{-\frac{1}{2}} \theta_1^\star  \|_{\HH} \\
&\leq \tilde{\lambda}_{1}^{\frac{1}{2}} \|\theta_1^\star  \|_{\HH} \|\Xb_\cT^\top (\Sighat_{3,1} + \tilde{\lambda}_{1} \Ib)^{-\frac{1}{2}} \|_{\op}.
\end{align*}
Observe that
\begin{align*}
\|\Xb_\cT^\top (\Sighat_{3,1}+\tilde{\lambda}_{1} \Ib)^{-\frac{1}{2}} \|^2_{\op}  &= n_{\cT}  \|( \Sighat_{3,1}+\tilde{\lambda}_{1} \Ib)^{-\frac{1}{2}} \Sighat_{\cT} (\Sighat_{3,1}+\tilde{\lambda}_{1} \Ib)^{-\frac{1}{2}} \|_{\op}\\
&\lesssim n_{\cT}  \| \Sighat_{\cT}^{\frac{1}{2}} ( \Sighat_{3,1}+\tilde{\lambda}_{1} \Ib)^{-1}  \Sighat_{\cT}^{\frac{1}{2}} \|_{\op} \\
&\stackrel{{(i)}}{\lesssim}n_{\cT}  \| \Sighat_{\cT}^{\frac{1}{2}} (\Sigmatreated+\tilde{\lambda}_{1}\Ib)^{-1}  \Sighat_{\cT}^{\frac{1}{2}} \|_{\op} \\
&\stackrel{{(ii)}}{\lesssim} R n_{\cT}  \| \Sighat_{\cT}^{\frac{1}{2}} (\bSigma+\tilde{\lambda}_{1}\Ib)^{-1}  \Sighat_{\cT}^{\frac{1}{2}} \|_{\op} \\
&= Rn_{\cT}  \|{\Shat}_{\tilde{\lambda_1}} \|_{\op} .
\end{align*}
where (i) follows from Lemma~\labelcref{lemma; moment ratio in E1}, and (ii) follows from Lemma~\labelcref{lemma; second moment relation under weak overlap}.
Thus we get 
\begin{align*}
\frac{1}{\sqrt{n_{\cT}}}\|\Xb_\cT^\top (\EE[\tilde{\theta_1}] -\theta_1^\star) \|_{2} 
&\lesssim (R \tilde{\lambda}_{1} \|{\Shat}_{\tilde{\lambda_1}} \|_{\op} )^{\frac{1}{2}}\|\theta_1^\star  \|_{\HH} .
\end{align*}
We can similarly obtain a bound for 
\(
\frac{1}{\sqrt{n_{\cT}}}\|\Xb_\cT^\top (\EE[\tilde{\theta_0}] - \theta_0^\star)\|_{2},
\)
and these observations prove the first inequality.


To prove the second inequality, see that
\begin{align*}
&\quad \|\Xb_\cT^\top (\EE[\tilde{\eta}] -\hat{\eta}) \|^2_{\psi_2} \\
&\lesssim \sig^2 \|\Xb_\cT^\top (\Sighat_{3, 1} + {\tilde{\lambda}}_{1} \Ib )^{-1} \frac{1}{n_3 }\Xb_{3,1}  \|^2_{\op} + \sig^2 \|\Xb_\cT^\top (\Sighat_{3, 0} + {\tilde{\lambda}}_{0} \Ib )^{-1} \frac{1}{n_3 }\Xb_{3,0}  \|^2_{\op}\\
&=  \sig^2 \frac{n_{\cT}}{n_3}\| \frac{1}{n_{\cT}} \Xb_\cT^\top(\Sighat_{3, 1} + {\tilde{\lambda}}_{1} \Ib  )^{-1} \Sighat_{3,1}(\Sighat_{3, 1} + {\tilde{\lambda}}_{1} \Ib  )^{-1} \Xb_\cT \|_{\op} \\
&\quad+ \sig^2\frac{n_{\cT}}{n_3}\| \frac{1}{n_{\cT}} \Xb_\cT^\top(\Sighat_{3, 0} + {\tilde{\lambda}}_{0} \Ib  )^{-1} \Sighat_{3,0}(\Sighat_{3, 0} + {\tilde{\lambda}}_{0} \Ib  )^{-1} \Xb_\cT \|_{\op}\\
&\leq \sig^2 \frac{n_{\cT}}{n_3}\| \frac{1}{n_{\cT}} \Xb_\cT^\top(\Sighat_{3, 1} + {\tilde{\lambda}}_{1} \Ib  )^{-1}  \Xb_\cT  \|_{\op} +\sig^2\frac{n_{\cT}}{n_3}\| \frac{1}{n_{\cT}} \Xb_\cT^\top(\Sighat_{3, 0} + {\tilde{\lambda}}_{0} \Ib  )^{-1}  \Xb_\cT  \|_{\op}\\
&\stackrel{(i)}{\lesssim} \sig^2 \frac{n_{\cT}}{n_3} R\|\frac{1}{n_{\cT}}  \Xb_\cT^\top(\bSigma + {\tilde{\lambda}}_{1} \Ib  )^{-1}  \Xb_\cT  \|_{\op}+\sig^2  \frac{n_{\cT}}{n_3} R\|\frac{1}{n_{\cT}}  \Xb_\cT^\top(\bSigma + {\tilde{\lambda}}_{0} \Ib  )^{-1}  \Xb_\cT  \|_{\op} \quad (\text{By Lemma~\labelcref{lemma; moment ratio in E1,lemma; second moment relation under weak overlap}}) \\
&\lesssim \sig^2\frac{n_{\cT}}{n_3} R \|\frac{1}{n_{\cT}} (\bSigma + {\tilde{\lambda}}_{1} \Ib  )^{-\frac{1}{2}}  \Xb_\cT \Xb_\cT^\top(\bSigma + {\tilde{\lambda}}_{1} \Ib  )^{-\frac{1}{2}} \|_{\op} \\
&\quad +\sig^2 \frac{n_{\cT}}{n_3} R\|\frac{1}{n_{\cT}} (\bSigma + {\tilde{\lambda}}_{0} \Ib  )^{-\frac{1}{2}}  \Xb_\cT \Xb_\cT^\top(\bSigma + {\tilde{\lambda}}_{0} \Ib  )^{-\frac{1}{2}} \|_{\op}\\
&= \sig^2\frac{n_{\cT}}{n_3} R  \|{\Shat}_{\tilde{\lambda}_1}  \|_{\op}+\sig^2\frac{n_{\cT}}{n_3} R  \|{\Shat}_{\tilde{\lambda}_0}  \|_{\op} 
\end{align*}
holds, where for inequality $(i)$, we used Lemma~\labelcref{lemma; moment ratio in E1,lemma; second moment relation under weak overlap}.

Next, we prove the third inequality. 
Note that under the good event \(\event\),
\begin{align*}
\norm{\Shat_{\tilde{\lambda}_1}}_{\op} &= \norm{ (\bSigma+ \tilde{\lambda}_1 \Ib)^{-\frac{1}{2}} {\Sighat}_\cT (\bSigma+ \tilde{\lambda}_1 \Ib)^{-\frac{1}{2}}  }_{\op} \\
&\stackrel{(i)}{\lesssim }\norm{ (\bSigma+ \tilde{\lambda}_1 \Ib)^{-\frac{1}{2}} (\bSigma_\cT +\frac{\xi}{n_{\cT}}\log(n n_\cT)) (\bSigma+ \tilde{\lambda}_1 \Ib)^{-\frac{1}{2}}  }_{\op} \\
&\lesssim \norm{\Sbar_{\tilde{\lambda}_1}}_{\op} + \|(\bSigma+ \tilde{\lambda}_1 \Ib)^{-\frac{1}{2}} \times \left(\frac{\xi}{n_{\cT}}\log(n n_\cT)\right)\Ib  \times (\bSigma+ \tilde{\lambda}_1 \Ib)^{-\frac{1}{2}}\|_{\op} \\
&\lesssim  \norm{\Sbar_{\tilde{\lambda}_1}}_{\op} + \frac{n_3}{n_{\cT}} \frac{\log (n_\cT n)}{\log n}.
\end{align*}
where $(i)$ holds by the definition of $\event$.
Hence, under \(\event\),
\begin{align*}
&\norm{\Shat_{\tilde{\lambda}_1}}_{\op} \lesssim \norm{\Sbar_{\tilde{\lambda}_1}}_{\op} + \frac{n_3}{n_{\cT}} \frac{\log (n_\cT n)}{\log n} 
\end{align*}
holds, and the fourth one can be proved in the same way.

\end{proof}


\begin{corollary}[Norm bounds for test outcomes]\label{corollary; test label norm bounds}
Under the event $\event$, the following holds:
\begin{align*}
\|\Xb_\cT^\top (\tilde{\eta}-\EE[\tilde{\eta}]) \|^2_{\psi_2} &\lesssim \sig^2 \frac{n_{\cT}}{n_3} BR +\sig^2R \frac{\log (nn_\cT)}{\log n} \\
\|\frac{1}{\sqrt{n_{\cT}}}\Xb_\cT^\top (\EE[\tilde{\eta}] -\eta^\star) \|^2_{2}  &\lesssim \xi RM^2 \log n \Bigl(\frac{B}{n} +\frac{1}{n_{\cT}}\frac{\log (nn_\cT)}{\log n}\Bigr).
\end{align*}
Here, \(\lesssim\) hides absolute constants.
\end{corollary}

\begin{proof}
By Lemma~\labelcref{lemma; test outcome norm preparation}, we have
\begin{align*}
\|\Xb_\cT^\top (\tilde{\eta}-\EE[\tilde{\eta}]) \|^2_{\psi_2} & \lesssim \sig^2  R\frac{n_{\cT}}{n_3}(\norm{\Shat_{\tilde{\lambda}_1}}_{\op} + \norm{\Shat_{\tilde{\lambda}_0}}_{\op}) \\
&\lesssim    \sig^2  \frac{n_{\cT}}{n_3} R \|\Sbar_{\tilde{\lambda}_1}  \|_{\op}+\sig^2  \frac{n_{\cT}}{n_3} R \|\Sbar_{\tilde{\lambda}_0}  \|_{\op} + \sig^2 R  \frac{\log (nn_\cT)}{\log n} \\
&\lesssim \sig^2 \frac{n_{\cT}}{n_3} BR +\sig^2R \frac{\log (nn_\cT)}{\log n} \\
&= \sig^2 R  \Bigl(\frac{n_\cT}{n}B + \frac{\log (nn_\cT)}{\log n}\Bigr).
\end{align*}

For the second inequality, by Lemma~\labelcref{lemma; test outcome norm preparation}, we have
\begin{align*}
&\|\frac{1}{\sqrt{n_{\cT}}}\Xb_\cT^\top (\EE[\tilde{\eta}] -\eta^\star) \|^2_{2} \\
&\lesssim  R  \tilde{\lambda}_1 \norm{\theta_1^\star}_{\HH}^2 \norm{\Shat_{\tilde{\lambda}_1}}_{\op} +  R  \tilde{\lambda}_0 \norm{\theta_0^\star}_{\HH}^2 \norm{\Shat_{\tilde{\lambda}_0}}_{\op}\\
&\lesssim  R \tilde{\lambda}_1 \norm{\theta_1^\star}_{\HH}^2 \norm{\Sbar_{\tilde{\lambda}_1}}_{\op} 
+  R  \tilde{\lambda}_0 \norm{\theta_0^\star}_{\HH}^2 \norm{\Sbar_{\tilde{\lambda}_0}}_{\op} 
+   R \tilde{\lambda}_1 \norm{\theta_1^\star}_{\HH}^2\frac{n_3}{n_{\cT}} \frac{\log (nn_\cT)}{\log n}+  R \tilde{\lambda}_0 \norm{\theta_0^\star}_{\HH}^2 \frac{n_3}{n_{\cT}}\frac{\log (nn_\cT)}{\log n}\\
&\leq BR  \tilde{\lambda}_1 \norm{\theta_1^\star}_{\HH}^2  + BR \tilde{\lambda}_0 \norm{\theta_0^\star}_{\HH}^2  +  R \tilde{\lambda}_1n_3 \norm{\theta_1^\star}_{\HH}^2\frac{1}{n_{\cT}}\frac{\log (nn_\cT)}{\log n}+  R \tilde{\lambda}_0 n_3 \norm{\theta_0^\star}_{\HH}^2 \frac{1}{n_{\cT}}\frac{\log (nn_\cT)}{\log n} \\
&\lesssim BR \frac{\xi\log n }{n} \max\bigl(\norm{\theta_0^\star}_{\HH},\norm{\theta_1^\star}_{\HH}\bigr)^2 +R \max\bigl(\norm{\theta_0^\star}_{\HH},\norm{\theta_1^\star}_{\HH}\bigr)^2\frac{\xi \log n}{n_{\cT}}\frac{\log (nn_\cT)}{\log n} \\
&\lesssim \xi RM^2  \Bigl(\frac{B}{n}\log n +\frac{\log (nn_\cT)}{n_{\cT}}\Bigr).
\end{align*}    
\end{proof}


\subsection{Proof of Proposition~\labelcref{proposition; in-sample MSE oracle}}

\begin{proof}
We aim to apply Lemma~\labelcref{lemma; loss model selection} to $\tilde{h}$, under the event $\Ecr$ and given $\cD_1, \cD_2$.
Under the good event \(\event\), we first bound $ \cE_\cT^{\inn}(\EE[\tilde{\eta}])$.
By using Corollary~\labelcref{corollary; test label norm bounds}, we have
\begin{align*}
\cE_\cT^{\inn}(\EE[\tilde{\eta}])= \Bigl\|\frac{1}{\sqrt{n_{\cT}}}\Xb_\cT^\top (\EE[\tilde{\eta}] -\eta^\star)\Bigr\|_{2} \lesssim\xi RM^2  \Bigl(\frac{B}{n}\log n +\frac{\log (nn_\cT)}{n_{\cT}}\Bigr).
\end{align*}

Next, we bound \(V^2\) of Lemma~\labelcref{lemma; loss model selection} under the event \(\event\).
Using the results of Corollary~\labelcref{corollary; test label norm bounds}, we get
\begin{align*}
V^2 := \|\Xb_\cT^\top (\tilde{\eta}- \EE[\tilde{\eta}]) \|_{\psi_2} &\lesssim 
\sig^2 R \Bigl(\frac{n_\cT}{n}B + \frac{\log (nn_\cT)}{\log n}\Bigr).
\end{align*}
Since our choice of \(\tilde{\lambda}_1, \tilde{\lambda}_0 \asymp \frac{\xi \log n}{n}\), by applying Lemma~\labelcref{lemma; loss model selection}, we get the desired in-sample MSE oracle inequality:
\begin{align*}
&\cE_{\cT}^{\inn}(\hat{\eta}_{\operatorname{final}}) \\
&\leq \min_{\hat{\eta}_{\bm{\lambda}} \in \cH} \cE_{\cT}^{\inn}(\hat{\eta}_{\bm{\lambda}})
+\xi RM^2  \Bigl(\frac{B}{n}\log n +\frac{\log (nn_\cT)}{n_{\cT}}\Bigr)
+ \sig^2 (\log n)  R  \Bigl(\frac{1}{n}B + \frac{1}{n_\cT}\frac{\log (nn_\cT)}{\log n} \Bigr)
\\
&\leq \min_{\hat{\eta}_{\bm{\lambda}} \in \cH} \cE_{\cT}^{\inn}(\hat{\eta}_{\bm{\lambda}})
+\xi RM^2  \Bigl(\frac{B}{n}\log n +\frac{\log (nn_\cT)}{n_{\cT}}\Bigr)
+ \sig^2  R  \Bigl(\frac{B}{n}\log n + \frac{\log (nn_\cT)}{n_\cT}  \Bigr)
\\
&\leq \min_{\hat{\eta}_{\bm{\lambda}} \in \cH} \cE_{\cT}^{\inn}(\hat{\eta}_{\bm{\lambda}})
+R(\xi M^2 + \sig^2 ) \Bigl(\frac{B}{n}\log n +\frac{\log (nn_\cT)}{n_{\cT}}\Bigr).
\end{align*} 
This holds with probability $1-n^{-11}$ under the event $\Ecr$.
Since $R(\xi M^2 + \sig^2 ) \Bigl(\frac{B}{n}\log n +\frac{\log (nn_\cT)}{n_{\cT}}\Bigr) \lesssim \Ocr$, we get the desired result.
\end{proof}



\subsection{Norm Bounds for RA Learner Estimator}\label{subsection; norm bounds RA learner estimator}
Next, we prepare to prove Proposition~\labelcref{proposition; oracle inequality MSE}.
Pick any fixed $\hat{\eta}_{\bm{\lambda}} \in \cH$. 
To apply Lemma~\labelcref{lemma; key lemma in-sample population}, we investigate several norm bounds for that estimator.
Recall that, in Appendix~\labelcref{section; proof RA learner}, we proved that the estimation error can be decomposed as
\begin{align*}
{\hat{\eta}_{\bm{\lambda}} -\eta } &= (\Sighat_2 + \lambda_2 \mathbf{I})^{-1} \Bigl( \frac{1}{n_2} \bigl(\sum_{a_{2i}=1}x_{2i} \varepsilon_{2i}+\sum_{a_{2i}=0} -x_{2i}\varepsilon_{2i}\bigr) -\lambda_2 \eta^\star\Bigr)\\
&\quad + (\Sighat_2 + \lambda_2 \mathbf{I})^{-1} \Bigl( \Sighat_{2,1}({\theta}^\star_0-\hat{\theta}_0)
+ \Sighat_{2,0} (\hat{\theta}_1-{\theta}^\star_1)  \Bigr).
\end{align*}

We define \(\bar{\eta}_{\bm{\lambda}} = \EE[\hat{\eta}_{\bm{\lambda}}],\bar{\theta}_1 = \EE[\hat{\theta}_1],\bar{\theta}_0 = \EE[\hat{\theta}_0] \) where the expectations are taken in the noise variables \(\{\varepsilon_i\}_{i=1}^n\).

\begin{lemma}\label{lemma; bias H bound}
The following holds under the event $\event$:
\begin{align*}
\norm{\bar{\eta}_{\bm{\lambda}} - \eta^\star}_{\HH} \lesssim \sqrt{R} \| \theta_0^\star\|_{\HH}+  \sqrt{R} \| \theta_1^\star\|_{\HH} + \|\eta^\star \|_{\HH}.
\end{align*}
Here, \(\lesssim\) hides absolute constants.
\end{lemma}

\begin{proof}
By the error decomposition established in Appendix~\labelcref{section; proof RA learner}, it can be written as
\begin{align*}
\norm{\bar{\eta}_{\bm{\lambda}} - \eta^\star}_{\HH} &\leq \|(\Sighat_2 + \lambda_2 \mathbf{I})^{-1} \Sighat_{2,1}(\bar{\theta}_0 -\theta_0^\star) \|_{\HH} + \| (\Sighat_2 + \lambda_2 \mathbf{I})^{-1} \Sighat_{2,0}(\bar{\theta}_1 -\theta_1^\star) \|_{\HH} \\
&\quad+ \|(\Sighat_2 + \lambda_2 \mathbf{I})^{-1}\lambda_2 \eta^\star  \|_{\HH}.
\end{align*}
In the proof, we only bound the term $I_1 := \|(\Sighat_2 + \lambda_2 \mathbf{I})^{-1} \Sighat_{2,1}(\bar{\theta}_0 -\theta_0^\star) \|_{\HH}$; the other term $ \| (\Sighat_2 + \lambda_2 \mathbf{I})^{-1} \Sighat_{2,0}(\bar{\theta}_1 -\theta_1^\star) \|_{\HH} $ can be bounded similarly.
Observe
\begin{align*}
I_1 &\leq \| (\Sighat_2 + \lambda_2 \mathbf{I})^{-1}  \Sighat_{2,1} (\Sighat_{1,0}+\lambda_{1,0} \mathbf{I})^{-1} \lambda_{1,0}\theta_0^\star\|_{\HH} \\
&\leq \lambda_{1,0} \| (\Sighat_2 + \lambda_2 \mathbf{I})^{-1}  \Sighat_{2,1} (\Sighat_{1,0}+\lambda_{1,0} \mathbf{I})^{-\frac{1}{2}}\|_{\op} \|(\Sighat_{1,0}+\lambda_{1,0} \mathbf{I})^{-\frac{1}{2}}\theta_0^\star \|_{\HH}.
\end{align*}    
We first examine 
\begin{align*}
\| (\Sighat_2 + \lambda_2 \mathbf{I})^{-1}  \Sighat_{2,1} (\Sighat_{1,0}+\lambda_{1,0} \mathbf{I})^{-\frac{1}{2}}\|^2_{\op}  
&= \| (\Sighat_2 + \lambda_2 \mathbf{I})^{-1}  \Sighat_{2,1} (\Sighat_{1,0}+\lambda_{1,0} \mathbf{I})^{-1} \Sighat_{2,1}(\Sighat_2 + \lambda_2 \mathbf{I})^{-1}\|_{\op} \\
&\stackrel{(i)}{\lesssim} R \| (\Sighat_2 + \lambda_2 \mathbf{I})^{-1}  \Sighat_{2,1}(\Sighat_2 + \lambda_2 \mathbf{I})^{-1}\|_{\op} \\
&\stackrel{(ii)}{\leq} R  \| (\Sighat_2 + \lambda_2 \mathbf{I})^{-1}  \Sighat_{2}(\Sighat_2 + \lambda_2 \mathbf{I})^{-1}\|_{\op} \\
&\lesssim R  \| (\Sighat_2 + \lambda_2 \mathbf{I})^{-1}\|_{\op} \\
&\lesssim R \frac{1}{\lambda_2},
\end{align*}
where we applied Corollary~\labelcref{corollary; application of Lemma second moment ratio} for (i), and (ii) holds by $\Sighat_{2,1} \preceq \Sighat_2$.
Hence we get 
\begin{align*}
I_1 \leq  \sqrt{\lambda_{1,0}} \sqrt{\frac{R}{\lambda_2}} \|\theta_0^\star \|_{\HH} \leq \sqrt{R}\|\theta_0^\star \|_{\HH},
\end{align*}
since our algorithm forces \(\lambda_2 \geq \lambda_{1,0}, \lambda_{1,1}\).
For the term \(\|(\Sighat_2 + \lambda_2 \mathbf{I})^{-1}\lambda_2 \eta^\star  \|_{\HH}\), we can bound it using 
\begin{align*}
\|(\Sighat_2 + \lambda_2 \mathbf{I})^{-1}\lambda_2\|_{\op} &\leq 1
\end{align*}
and thus \(\|(\Sighat_2 + \lambda_2 \mathbf{I})^{-1}\lambda_2 \eta^\star  \|_{\HH} \leq \|\eta^\star \|_{\HH}\).
\end{proof}


We next bound the \(\psi_2\) norm $\norm{\hat{\eta}_{\bm{\lambda}}-\bar{\eta}_{\bm{\lambda}}}_{\psi_2}$.

\begin{lemma}\label{lemma; variance psi2 bound}
The following holds under the event $\event$:
\begin{align*}
\norm{\hat{\eta}_{\bm{\lambda}}-\bar{\eta}_{\bm{\lambda}}}_{\psi_2} \lesssim \sigma\frac{ \sqrt{R}}{\sqrt{n_1 \lambda_2}} + \sigma\frac{1}{\sqrt{n_2\lambda_2}} \lesssim \sigma \frac{\sqrt{R}}{ \sqrt{\xi \log n}}.
\end{align*}   
Here, \(\lesssim\) hides absolute constants.
\end{lemma}

\begin{proof}
By the error decomposition established in Appendix~\labelcref{section; proof RA learner}, it can be written as
\begin{align*}
\norm{ \hat{\eta}_{\bm{\lambda}} -\bar{\eta}_{\bm{\lambda}} }_{\psi_2} &= \|  (\Sighat_2 + \lambda_2 \mathbf{I})^{-1} \Sighat_{2,1}(\hat{\theta}_0 -\bar{\theta}_0) \|_{\psi_2} + \| (\Sighat_2 + \lambda_2 \mathbf{I})^{-1} \Sighat_{2,0}(\hat{\theta}_1 -\bar{\theta}_1) \|_{\psi_2} \\
&\quad+ \| (\Sighat_2 + \lambda_2 \mathbf{I})^{-1} \frac{1}{n_2}\bigl(  \sum_{a_{2i}=1}x_{2i}\varepsilon_{2i}+\sum_{a_{2i}=0} (- x_{2i}\varepsilon_{2i}) \bigr) \|_{\psi_2}.
\end{align*}
In the proof, we only bound $I_2 = \|  (\Sighat_2 + \lambda_2 \mathbf{I})^{-1} \Sighat_{2,1}(\hat{\theta}_0 -\bar{\theta}_0) \|_{\psi_2}$; the other term $\| (\Sighat_2 + \lambda_2 \mathbf{I})^{-1} \Sighat_{2,0}(\hat{\theta}_1 -\bar{\theta}_1) \|_{\psi_2}$ can be bounded similarly.
Observe that 
\begin{align*}
I_2&= \Bigl\|   (\Sighat_2 + \lambda_2 \mathbf{I})^{-1}  \Sighat_{2,1} (\Sighat_{1,0}+\lambda_{1,0} \mathbf{I})
^{-1} \frac{1}{n_1} \Xb_{1,0} \bm{\varepsilon}_{1,0}\Bigr\|_{\psi_2}.   
\end{align*}
We interpret this as the operator map of $\bm{\varepsilon}_{1,0}$, and we bound the operator norm of the Hilbertian operator $(\Sighat_2 + \lambda_2 \mathbf{I})^{-1}  \Sighat_{2,1} (\Sighat_{1,0}+\lambda_{1,0} \mathbf{I})^{-1} \frac{1}{n_1} \Xb_{1,0}$. 
Define its operator norm as $A$, 
\[
A := \Bigl\|(\Sighat_2 + \lambda_2 \mathbf{I})^{-1}  \Sighat_{2,1} (\Sighat_{1,0}+\lambda_{1,0} \mathbf{I})^{-1} \frac{1}{n_1} \Xb_{1,0}\Bigr\|_{\op}.
\]
Observe that 
\begin{align*}
A^2 & \leq \frac{1}{n_1} \norm{(\Sighat_2 + \lambda_2 \mathbf{I})^{-1}  \Sighat_{2,1} (\Sighat_{1,0}+\lambda_{1,0} \mathbf{I})^{-1} \Sighat_{1,0}(\Sighat_{1,0}+\lambda_{1,0} \mathbf{I})^{-1} \Sighat_{2,1}  (\Sighat_2 + \lambda_2 \mathbf{I})^{-1}  }_{\op}  \\
&\lesssim \frac{1}{n_1} \norm{(\Sighat_2 + \lambda_2 \mathbf{I})^{-1}  \Sighat_{2,1} (\Sighat_{1,0}+\lambda_{1,0} \mathbf{I})^{-1}  \Sighat_{2,1} (\Sighat_2 + \lambda_2 \mathbf{I})^{-1} }_{\op} \\
&\stackrel{(i)}{\lesssim } R  \frac{1}{n_1} \norm{(\Sighat_2 + \lambda_2 \mathbf{I})^{-1}  \Sighat_{2,1}   (\Sighat_2 + \lambda_2 \mathbf{I})^{-1}  }_{\op}\\
&\stackrel{(ii)}{\lesssim } R  \frac{1}{n_1} \norm{(\Sighat_2 + \lambda_2 \mathbf{I})^{-1}  \Sighat_{2}   (\Sighat_2 + \lambda_2 \mathbf{I})^{-1}  }_{\op} \\
&\lesssim  R  \frac{1}{n_1} \norm{(\Sighat_2 + \lambda_2 \mathbf{I})^{-1}   }_{\op}\\
&\lesssim R  \frac{1}{n_1 \lambda_2},
\end{align*}
where in the third line (i), we used Corollary~\labelcref{corollary; application of Lemma second moment ratio}, and (ii) holds by the relation $\Sighat_{2,1} \preceq \Sighat_2$.
Thus we get
\begin{align*}
A \lesssim \frac{ \sqrt{R}}{\sqrt{n_1 \lambda_2}},
\end{align*}    
and hence
\begin{align*}
I_2 \lesssim \sig \frac{ \sqrt{R}}{\sqrt{n_1 \lambda_2}}.
\end{align*}
For the other term \(\Bigl\| (\Sighat_2 + \lambda_2 \mathbf{I})^{-1} \frac{1}{n_2}\Bigl(  \sum_{a_{2i}=1}x_{2i}\varepsilon_{2i}+\sum_{a_{2i}=0} (- x_{2i}\varepsilon_{2i}) \Bigr) \Bigr\|_{\psi_2}\), we can interpret it as 
\begin{align*}
(\Sighat_2 + \lambda_2 \mathbf{I})^{-1} \frac{1}{n_2} \Xb_2 \bm{\varepsilon}'_2 ,
\end{align*}
where \(\bm{\varepsilon}_2' \) is a vectorized form of \(\varepsilon_{2i}(-1)^{a_{2i}+1}\).
Under Assumption~\labelcref{assumption; consistency and unconfoundedness}, \(\varepsilon_{2i}(-1)^{a_{2i}+1}\) is a mean-zero and sub-Gaussian noise given \(\{(x_i,a_i)\}_{i=1}^n\).
Hence, by the same argument, we bound the operator norm of 
\begin{align*}
\Bigl\| (\Sighat_2 + \lambda_2 \mathbf{I})^{-1} \frac{1}{n_2} \Xb_2  \Bigr\|^2_{\op} &\leq \frac{1}{n_2 } \bigl\|(\Sighat_2 + \lambda_2 \mathbf{I})^{-1}\Sighat_2(\Sighat_2 + \lambda_2 \mathbf{I})^{-1} \bigr\|_{\op} \\
&\lesssim \frac{1}{n_2\lambda_2},
\end{align*}
and this completes the proof.
\end{proof}


Lastly, we aim to bound the Hilbert norm of $\hat{\eta}_{\bm{\lambda}}- \bar{\eta}$.

\begin{lemma}\label{lemma; variance hilbert norm bound}
Under the good event $\event$, for any \(0 < \rho <1\), 
\begin{align*}
\| \hat{\eta}_{\bm{\lambda}}- \bar{\eta}  \|^2_{\HH} \lesssim \sig^2 Rn\frac{\log(\frac{1}{\rho})}{\xi \log n}
\end{align*}
holds with probability at least \(1-\rho\).
Here, \(\lesssim\) hides absolute constants.
\end{lemma}

\begin{proof}
By the error decomposition established in Appendix~\labelcref{section; proof RA learner}, we have
\begin{align*}
\norm{ \hat{\eta}_{\bm{\lambda}} -\bar{\eta}_{\bm{\lambda}} }_{\HH} &= \|  (\Sighat_2 + \lambda_2 \mathbf{I})^{-1} \Sighat_{2,1}(\hat{\theta}_0 -\bar{\theta}_0) \|_{\HH} + \| (\Sighat_2 + \lambda_2 \mathbf{I})^{-1} \Sighat_{2,0}(\hat{\theta}_1 -\bar{\theta}_1) \|_{\HH} \\
&\quad+ \| (\Sighat_2 + \lambda_2 \mathbf{I})^{-1} \frac{1}{n_2}\Bigl(  \sum_{a_{2i}=1}x_{2i}\varepsilon_{2i}+\sum_{a_{2i}=0} (- x_{2i}\varepsilon_{2i}) \Bigr) \|_{\HH}.
\end{align*}
In the proof, we only bound $I_3 = \norm{(\Sighat_2 + \lambda_2 \mathbf{I})^{-1} \Sighat_{2,1}(\hat{\theta}_0-\Bar{\theta}_0)}_{\HH}$; the other terms can be bounded similarly.
Since
\begin{align*}
I_3&= \Bigl\|   (\Sighat_2 + \lambda_2 \mathbf{I})^{-1}  \Sighat_{2,1} (\Sighat_{1,0}+\lambda_{1,0} \mathbf{I})
^{-1} \frac{1}{n_1} \Xb_{1,0} \bm{\varepsilon}_{1,0}\Bigr\|^2_{\HH},
\end{align*}
we interpret this as the quadratic form of a martingale, and we apply the Hanson--Wright inequality, Lemma~\labelcref{lemma; quadratic martingale}.
Under the event \(\event\), with probability at least $1-\frac{\rho}{3}$, 
\begin{align*}
I_3&\leq   \Bigl\|   (\Sighat_2 + \lambda_2 \mathbf{I})^{-1}  \Sighat_{2,1} (\Sighat_{1,0}+\lambda_{1,0} \mathbf{I})
^{-1} \frac{1}{n_1} \Xb_{1,0} \bm{\varepsilon}_{1,0}\Bigr\|^2_\HH\\
&\stackrel{(i)}{\lesssim }\frac{\sig^2}{n_1}\log\Bigl(\frac{1}{\rho}\Bigr)\Tr\Bigl((\Sighat_2 + \lambda_2 \mathbf{I})^{-1}  \Sighat_{2,1} (\Sighat_{1,0}+\lambda_{1,0} \mathbf{I})
^{-1} \frac{1}{n_1} \Xb_{1,0}\Xb_{1,0}^\top (\Sighat_{1,0}+\lambda_{1,0} \mathbf{I})
^{-1}  \Sighat_{2,1} (\Sighat_2 + \lambda_2 \mathbf{I})^{-1} \Bigr)\\
&\lesssim \sig^2\frac{1}{n_1}\log\Bigl(\frac{1}{\rho}\Bigr)\Tr\Bigl((\Sighat_2 + \lambda_2 \mathbf{I})^{-1}  \Sighat_{2,1} (\Sighat_{1,0}+\lambda_{1,0} \mathbf{I})
^{-1} \Sighat_{1,0}(\Sighat_{1,0}+\lambda_{1,0} \mathbf{I})
^{-1}  \Sighat_{2,1} (\Sighat_2 + \lambda_2 \mathbf{I})^{-1} \Bigr)\\
&\lesssim \sig^2 \frac{1}{n_1}\log\Bigl(\frac{1}{\rho}\Bigr)\Tr\Bigl((\Sighat_2 + \lambda_2 \mathbf{I})^{-1}  \Sighat_{2,1} (\Sighat_{1,0}+\lambda_{1,0} \mathbf{I})^{-1}   \Sighat_{2,1} (\Sighat_2 + \lambda_2 \mathbf{I})^{-1} \Bigr)\\
&\stackrel{(ii)}{\lesssim} \sig^2 \frac{R}{n_1}\log\Bigl(\frac{1}{\rho}\Bigr)\Tr\Bigl((\Sighat_2 + \lambda_2 \mathbf{I})^{-1}  \Sighat_{2,1} (\Sighat_2 + \lambda_2 \mathbf{I})^{-1} \Bigr)  \quad (\text{By Corollary~\labelcref{corollary; application of Lemma second moment ratio}}) \\
&\lesssim \sig^2 \frac{R}{n_1}\log\Bigl(\frac{1}{\rho}\Bigr)\Tr\Bigl((\Sighat_2 + \lambda_2 \mathbf{I})^{-\frac{1}{2}}  \Sighat_{2,1} (\Sighat_2 + \lambda_2 \mathbf{I})^{-\frac{1}{2}} (\Sighat_2 + \lambda_2 \mathbf{I})^{-1}\Bigr)\\
&\stackrel{(iii)}{\lesssim} \sig^2   \frac{R}{n_1}\log\Bigl(\frac{1}{\rho}\Bigr)\Tr\Bigl((\Sighat_2 + \lambda_2 \mathbf{I})^{-\frac{1}{2}}  \Sighat_{2} (\Sighat_2 + \lambda_2 \mathbf{I})^{-\frac{1}{2}} (\Sighat_2 + \lambda_2 \mathbf{I})^{-1}\Bigr) \quad (\text{By Lemma~\labelcref{lemma; trace simple inequality}, }\widehat{\bSigma}_{2,1}\preceq \widehat{\bSigma}_2)\\
&\stackrel{(iv)}{\lesssim} \sig^2   \frac{R}{n_1}\log\Bigl(\frac{1}{\rho}\Bigr)\Tr\Bigl((\Sighat_2 + \lambda_2 \mathbf{I})^{-1} \Bigr) \quad (\text{By Lemma~\labelcref{lemma; trace simple inequality}}) \\
&\stackrel{(v)}{\lesssim} \sig^2   \frac{R}{n_1} \frac{n_2}{\lambda_2}\log\Bigl(\frac{1}{\rho}\Bigr) \quad (\text{since $\Sighat_2$ has rank at most $n_2$}) \\
&\lesssim \sig^2   Rn\frac{\log(\frac{1}{\rho})}{\xi \log n}.
\end{align*}
where for (i), we used the Hanson--Wright inequality 
(Lemma~\labelcref{lemma; quadratic martingale}), 
and for (ii), we used 
Corollary~\labelcref{corollary; application of Lemma second moment ratio}.
For (iii) and (iv), we used 
Lemma~\labelcref{lemma; trace simple inequality}, 
and for (v), we used the fact that \(\Sighat_2\) has rank at most \(n_2\).

\end{proof}

Using the previously established 
Lemmas~\labelcref{lemma; variance hilbert norm bound,lemma; variance psi2 bound,lemma; bias H bound}, 
we obtain the following corollary directly.


\begin{corollary}\label{corollary; Hilbert norm bound of estimators}
Under the event \(\event\), for any \(0<\rho <1\), 
\begin{align*}
\|\hat{\eta}_{\bm{\lambda}}-\eta^\star \|_{\HH} \lesssim \sigma  \sqrt{Rn\frac{\log(\frac{1}{\rho})}{\xi \log n}}+  \sqrt{R}\| \theta_0^\star\|_{\HH}+ \sqrt{R}\| \theta_1^\star\|_{\HH} + \|\eta^\star \|_{\HH}
\end{align*}
holds with probability at least \(1-\rho\).
Here, \(\lesssim\) hides absolute constants.
\end{corollary}

\paragraph{Summary. }\label{summary; summary norm bound}
We summarize the established norm bounds.

\underline{1. Bias and variance norm bounds:}\quad
Under the event \(\event\), 
\begin{align*}
&\norm{\hat{\eta}_{\bm{\lambda}}-{\bar{\eta}}_{\bm{\lambda}}}_{\psi_2} \lesssim \sigma\frac{\sqrt{R} }{\sqrt{n_1 \lambda_2}} + \sigma\frac{1}{\sqrt{n_2\lambda_2}} \lesssim \frac{\sigma  \sqrt{R}}{\sqrt{\xi \log n}},  \\
&\norm{\bar{\eta}_{\bm{\lambda}}-{{\eta^\star}}}_{\HH} \lesssim \sqrt{R}  \|\theta_0^\star\|_{\HH}+\sqrt{R} \|\theta_1^\star\|_{\HH}+\|\eta^\star\|_{\HH}.
\end{align*}

\underline{2. Hilbert norm bound of estimation error:}\quad
Under the event \(\event\), with probability at least \(1-n^{-24}\),
\begin{align*}
\|\hat{\eta}_{\bm{\lambda}}-\eta^\star \|_{\HH} &\lesssim \sigma \sqrt{\frac{Rn  }{\xi}}+\sqrt{R} \| \theta_0^\star\|_{\HH}+\sqrt{R}  \| \theta_1^\star\|_{\HH} +\| \eta^\star\|_{\HH}.
\end{align*}


\subsection{Proof of Proposition~\labelcref{proposition; oracle inequality MSE}}
Note that the in-sample MSE of the target estimator \(\hat{\eta}_{\bm{\lambda}}\) is \((\hat{\eta}_{\bm{\lambda}}-\eta^\star)\Sighat_{\cT} (\hat{\eta}_{\bm{\lambda}}-\eta^\star)\), and the MSE is \((\hat{\eta}_{\bm{\lambda}}-\eta^\star)\bSigma_{\cT} (\hat{\eta}_{\bm{\lambda}}-\eta^\star)\).
Under the event \(\event\), we aim to apply Lemma~\labelcref{lemma; key lemma in-sample population} with $\varepsilon = n^{-24}$. 
By our norm bounds for RA learner estimators established in Appendix~\labelcref{summary; summary norm bound} (Lemma~\labelcref{lemma; variance hilbert norm bound,lemma; bias H bound,lemma; variance psi2 bound} and Corollary~\labelcref{corollary; Hilbert norm bound of estimators}), for any estimator $\hat{\eta}_{\bm{\lambda}} \in \cH$, we get 
\begin{align*}
\PP_{x_{0i} \sim \cP_\cT, \hat
{\eta}_{\bm{\lambda}}}\Bigl[ \bigl|x_{0i}^\top (\hat{\eta}_{\bm{\lambda}}-\eta^\star)\bigr| \lesssim 
\sqrt{R}\Bigl(M \sqrt{\xi} + \frac{\sigma }{\sqrt{\log n}}\Bigr) 
, \norm{\hat{\eta}_{\bm{\lambda}}-\eta^\star}_{\HH} \lesssim \sqrt{R}   M + \sigma  \sqrt{\frac{Rn }{\xi}}  \Bigr] \geq 1- n^{-24}.
\end{align*}   
Then, we apply Lemma~\labelcref{lemma; key lemma in-sample population} by setting its parameters as 
\[
\varepsilon = n^{-24}, \quad
\gamma = \frac{1}{2}, \quad
r \asymp \sqrt{R}\Bigl(M \sqrt{\xi} + \frac{\sig}{\sqrt{\log n}}\Bigr), \quad
K \asymp \Bigl(\sqrt{R}  M + \sigma \sqrt{\frac{Rn}{\xi}}\Bigr), \quad
\eta = \frac{n^{-11}}{2|\cH|}.
\]
As a result, for any estimator $\hat{\eta}_{\bm{\lambda}} \in \cH$, we get 
\begin{align*}
\cE^{\inn}_\cT(\hat{\eta}_{\bm{\lambda}}) \lesssim  \cE_\cT(\hat{\eta}_{\bm{\lambda}}) + \frac{r^2}{n_{\cT}}\log n ,
\end{align*}
and for some constant \(c>0\), we have
\begin{align*}
\frac{1}{2}\cE_\cT(\hat{\eta}_{\bm{\lambda}}) - c\frac{r^2}{n_{\cT}}\log n - c\frac{\xi(R M^2 +R\sig^2)}{n^5} \leq \cE_\cT^{\inn}(\hat{\eta}_{\bm{\lambda}}).
\end{align*}
This holds with probability at least \(1- n^{-11}\) for some constant \(c>0\).

We define 
\[
\Ocr' :=  \frac{1}{n_\cT}R \bigl(\sig^2 + M^2 \xi \log n\bigr)+\frac{\xi\bigl(R M^2 +R\sig^2\bigr)}{n^5}.
\]
Then we get the following with probability at least \(1-n^{-11}\) for all \(\hat{\eta}_{\bm{\lambda}} \in \cH\):
\begin{align*}
&\cE_{\cT}^{\inn}(\hat{\eta}_{\bm{\lambda}}) \lesssim \cE_\cT(\hat{\eta}_{\bm{\lambda}}) +\Ocr',\\
&\cE_{\cT}(\hat{\eta}_{\bm{\lambda}}) \lesssim  \cE_{\cT}^{\inn}(\hat{\eta}_{\bm{\lambda}}) +\Ocr'.
\end{align*}

Recall that we have defined
\[
\Ocr =R \bigl(\xi M^2 + \sig^2 \bigr) \Bigl(\frac{B}{n} +\frac{1}{n_{\cT}}\Bigr)\log (n_\cT n).
\]
We can easily check that \(\Ocr \gtrsim \Ocr'\).
In conclusion, for any estimator $\hat{\eta}_{\bm{\lambda}}$, we have 
\begin{align*}
\cE_\cT^{\inn}(\hat{\eta}_{\bm{\lambda}}) \lesssim \cE_\cT(\hat{\eta}_{\bm{\lambda}}) +\Ocr
\end{align*}
and 
\begin{align*}
\cE_\cT(\hat{\eta}_{\bm{\lambda}}) \lesssim  \cE_\cT^{\inn}(\hat{\eta}_{\bm{\lambda}}) +\Ocr  
\end{align*}
with probability $1-n^{-11}$ under the event $\event$.
By combining this with Proposition~\labelcref{proposition; in-sample MSE oracle}, we get the desired result directly.

\hfill \BlackBox






\section{Proof of Theorem~\labelcref{theorem; main theorem}}\label{section; proof main theorem}

Using Corollary~\labelcref{corollary; optimal MSE bound}, 
we have
\[
\min_{\hat{\eta}_{\bm{\lambda}} \in \cH} \cE_{\cT}(\hat{\eta}_{\bm{\lambda}})
\lesssim
\Bigl(\frac{BR}{n}\Bigr)^\alpha \|\eta^\star\|_\HH^{2(1-\alpha)} (\log n)^\alpha
+ \frac{BR}{n}M^2 \log n,
\]
which holds with probability at least \(1 - 2n^{-11}\).

Then, combining this with 
Proposition~\labelcref{proposition; oracle inequality MSE,proposition; in-sample MSE oracle}, we obtain
\[
\cE_\cT(\hat{h}_{\operatorname{final}}) 
\lesssim  
\Bigl(\frac{BR}{n}\Bigr)^\alpha \|\eta^\star\|_\HH^{2(1-\alpha)} (\log n)^\alpha
+ \frac{BR}{n}M^2 \log n 
+ \Ocr,
\]
with probability at least \(1 - 6n^{-11}\).

Finally, for \(n > 6\), Theorem~\labelcref{theorem; main theorem} follows.


\hfill \BlackBox


\section{Proof of Theorem~\labelcref{theorem; lower bound} (Lower Bound)}\label{section; lower bound proof}

\begin{proof}
Let's consider the following kernel \(K(\cdot,\cdot)\) and the distributions \(\cQ_\cS, \cQ_\cT\), and \(\pi(\cdot)\).
\begin{itemize}
\item The support of source covariates is \(\cZ = [-1,1]\).
\item By \citet{liu2023adaptation,zhang2023optimality}, for any bounded domain \(\Omega\) with a Lipschitz boundary, there is a kernel that generates \(H^1(\Omega)\).  
Let the kernel of the Sobolev space \(H^1([0,1])\) be \(K_1(\cdot, \cdot)\) and the kernel of the Sobolev space \(H^1([-1,0))\) be \(K_2(\cdot,\cdot)\).
\item Define our kernel as:
\[
K(z,w) = \begin{cases}
K_1(z,w)&\quad \text{when \(z,w \geq 0\)} \\
K_{2}(z,w) &\quad \text{when \(z,w <0\)}\\
0 &\quad \text{else}\\
\end{cases}
\]
\item \(\pi(z) \equiv \frac{1}{R}\).
\item \(\cQ_\cS = \begin{cases}
\operatorname{Unif}([0,1]) &\quad \text{with probability \(\frac{1}{B}\)} \\
\operatorname{Unif}([-1,0]) &\quad \text{with probability \(1- \frac{1}{B}\)}.
\end{cases}\)
\item \(\cQ_\cT = \operatorname{Unif}([0,1])\)
\item \(f_0^\star \equiv C_0 = (const) \).
\item Also, we consider the easier case: \blue{\(C_0\) is known.}
This additional information makes the problem easier, hence it only decreases the lower bound. Without loss of generality, we set \(C_0=0\), but it can be an arbitrary value.
\end{itemize}

\paragraph{Step 1. }
We first investigate the function space that the kernel \(K(\cdot,\cdot)\) generates.
We can easily check that the kernel \(K(\cdot, \cdot)\) is symmetric and positive semidefinite.
Set the function space of the RKHS of kernel \(K\) as \(\cF\).
Also, for any \(w>0\), set \(\cF(w) := \{f \in \cF \mid \| f \|_\cF <w\}\).
Then we have
\begin{align}\label{equation; kernel for lower bound}
\cF(w) = \{&f = f_-(z)\one(z< 0) + f_+(z)\one(z \geq 0) \text{ where } f_+ \in H^1([0,1]), f_- \in H^1([-1,0)), \nonumber \\ 
&
\text{ and } 
\|f_- \|^2_{H^1([-1,0))} +  \|f_+ \|_{H^1([0,1])}^2 \leq  w^2\}.
\end{align}
We define two function spaces 
\begin{align*}
&  \cF_+(w) := \{ f \mid  f \in H^1([0,1]),  \|f\|_{H^1([0,1])}^2 \leq w^2\},\\
&  \cF_-(w) := \{ f \mid f \in H^1([-1,0)),   \|f\|_{H^1([-1,0))}^2 \leq w^2\}.
\end{align*}
We form a subspace of \(\cF\), say \(\cF_{\operatorname{sub}}(w) \subset \cF(w)\), as 
\begin{align*}
\cF_{\operatorname{sub}}(w) := \{f = f_-(z)\one(z < 0) + f_+(z)\one(z \geq 0)  \text{ where } f_+ \in \cF_+(\frac{w}{\sqrt{2}}), f_- \in \cF_-(\frac{w}{\sqrt{2}}) \}.
\end{align*}
Hence we get the relation 
\begin{align*}
\inf_{\hat{h}} \sup_{(f_0^\star, f_1^\star, h^\star )\text{ with } \|h^\star\|_\cF \leq W} \EE[\cE_\cT(\hat{h})] \geq   \inf_{\hat{h}} \sup_{(f_0^\star, f_1^\star, h^\star )\text{ with } h^\star \in \cF_{\operatorname{sub}}(W)} \EE[\cE_\cT(\hat{h})].
\end{align*}
We highlight that for \(\cF_{\operatorname{sub}}(w)\), the combination of \(f_+\) and \(f_-\) can be arbitrary, unless \(f_+ \in \cF_+(\frac{w}{\sqrt{2}})\) and \(f_- \in \cF_-(\frac{w}{\sqrt{2}})\).

\paragraph{Step 2. }
Recall that we know \(f_0^\star \equiv C_0\) and we also know the value of \(C_0\) (we simply assumed it is zero).
Hence, the controlled samples are meaningless, and only the treated data (\(\cD_1:=\{(z_i,a_i, y_i) \in \cD, a_i=1 \}\)) is informative for learning.
Since \(\pi(z) \equiv \frac{1}{R}\), \blue{the distribution of informative samples has likelihood \(\frac{1}{R} \operatorname{Unif}([0,1])\)}.
Also, since \(f_0^\star\) is known, the situation is equivalent to a general regression problem with those informative samples.

\paragraph{Step 3. }
We decompose \(\cD_1\) as \(\cD_{1, +} := \{(z_i, a_i, y_i), z_i \geq 0, a_i=1\} \) and \(\cD_{1,-} := \{(z_i, a_i, y_i), z_i < 0, a_i=1\} \).
Since \(h^\star \in \cF_{\operatorname{sub}}(W)\), set \(h^\star = h_+^\star \one(z \geq 0) + h_-^\star \one(z<0)\) for \(h^\star_+ \in \cF_+(W/\sqrt{2})\) and \(h^\star_- \in \cF_-(W/\sqrt{2})\).
This can be viewed as two totally separate and independent regression problems:
\begin{enumerate}
\item Data \(\cD_{1,+}\) from response model \(h^\star_+ \in \cF_+(W/\sqrt{2})\).
\item Data \(\cD_{1,-}\) from response model \(h^\star_- \in \cF_-(W/\sqrt{2})\).
\end{enumerate}
These two problems are separate, and since our target distribution is uniform on \([0,1]\), we only need to estimate \(h^\star_+\).
Hence only data \(\cD_{1,+}\) is informative, and \(\cD_{1,-}\) is meaningless. Data from a different response model does not have value at all.
Therefore, the likelihood of informative covariates (in \(\cD_{1,+}\)) is \(\frac{1}{BR} \times \operatorname{Unif}([0,1])\) among the total \(n\) samples.

\paragraph{Step 4. }
For sufficiently large \(n\), with probability at least \(1- \frac{1}{n^2}\), the sample size of \(\cD_{1,+}\) is smaller than \(\frac{2n}{BR}\), i.e., \(|\cD_{1,+}| \leq \frac{2n}{BR}\).
Also, given the event that the sample size of \(\cD_{1,+}\) is fixed, covariates in \(\cD_{1,+}\) follow the uniform distribution.
Then, by applying the known result on the lower bound of Sobolev space \citep{green2021minimax}, we have 
\begin{align*}
\inf_{\hat{h}} \sup_{f_0^\star \equiv C_0, f_1^\star, h^\star \text{ such that } h^\star \in \cF_{\operatorname{sub}}(W)} \EE[\cE_\cT(\hat{h})] \gtrsim \Bigl(\frac{BR}{n}\Bigr)^{\alpha} W^{2(1-\alpha)}
\end{align*}
for \(\alpha = \frac{2}{3}\).

\end{proof}



\section{Proofs for Examples in Section~\labelcref{subsection; weak overlap}}
\label{section; proof weak overlap exampls}

\subsection{Proof of Example~\labelcref{example; positivity}}
Let the probability distribution function of the source covariates \(z_i\) be \(F_{\cQ_\cS}(\cdot)\).
By the definition of \(\Sigmatreated\) and \(\Sigmacontrol\), we have
\begin{align}\label{equation; integral form of moments}
\Sigmatreated 
&= \int_{\cZ} \bigl(\phi(z) \otimes \phi(z)\bigr) \pi(z)\mathrm{d}F_{\cQ_\cS}(z), 
\quad 
\Sigmacontrol  
= \int_{\cZ} \bigl(\phi(z) \otimes \phi(z)\bigr) \bigl(1-\pi(z)\bigr)\mathrm{d}F_{\cQ_\cS}(z).
\end{align}
Hence, when \(\kappa \leq \pi(z) \leq 1-\kappa\), the following holds:
\begin{align*}
\Sigmatreated \preceq \frac{1}{\kappa} \Sigmacontrol 
\quad\text{and}\quad    
\Sigmacontrol  \preceq \frac{1}{\kappa} \Sigmatreated.
\end{align*}

\subsection{Proof of Example~\labelcref{example; singular propensity 1}}\label{section; proof weak overlap exampls}

\begin{proof}
We simply set \(\xi \geq 1\). 
We prove this for the general \(H^k([0,1])\), where \(k \in \NN\).
We aim to find \(R\) such that, for all \(g \in H^k([0,1])\),
\begin{align*}
\theta(g)^\top \Sigmatreated \theta(g) &\leq R \theta(g)^\top \Sigmacontrol \theta(g) 
+ \frac{R}{n}\|g\|_{H^k([0,1])}^2, 
\\
\theta(g)^\top \Sigmacontrol \theta(g) &\leq R \theta(g)^\top \Sigmatreated \theta(g) 
+ \frac{R}{n}\|g\|_{H^k([0,1])}^2,
\end{align*}
where \(\theta(g)\) is the Hilbertian element of \(g\), as defined in Appendix~\labelcref{section; good events and second moments}.
First, we prove the case when the source distribution is uniform on \([0,1]\).
Since the density is bounded above and below by some constant, it is straightforward to extend the argument to more general cases.

These conditions are equivalent to 
\begin{align*}
&\int_0^1 (1-z)g(z)^2 \de z 
\leq R \int_0^1 zg(z)^2 \de z 
+ \frac{R}{n}\|g\|_{H^k([0,1])}^2,  
\\
&\int_0^1 zg(z)^2 \de z 
\leq R \int_0^1 (1-z)g(z)^2 \de z  
+ \frac{R}{n}\|g\|_{H^k([0,1])}^2.
\end{align*}
The second inequality is equivalent to
\begin{align*}
&\int_0^1  z g(z)^2 \de z 
\leq R \int_0^1 (1-z)g(z)^2 \de z  
+ \frac{R}{n}\|g\|_{H^k}^2 
\\
&\Leftrightarrow   
\int_0^1  \bigl(1-u\bigr) g(1-u)^2 \de u 
\leq R \int_0^1 ug(1-u)^2 \de u  
+ \frac{R}{n}\|\tilde{g}\|_{H^k}^2  \quad \bigl(\text{by setting } g(1-u) = \tilde{g}(u)\bigr).
\\
&\Leftrightarrow  
\int_0^1  \bigl(1-u\bigr)\tilde{g}(u)^2 \de u 
\leq R \int_0^1 u\tilde{g}(u)^2 \de u  
+ \frac{R}{n}\|\tilde{g}\|_{H^k}^2.
\end{align*}
Hence, it suffices to find \(R\) that satisfies, for all \(g\) with \(\|g\|_{H^k}=1\),
\begin{align*}
\int_0^1 g(z)^2 \de z 
\leq (R+1)\int_0^1 zg(z)^2 \de z  
+ \frac{R}{n}.
\end{align*}
Using Claim~\labelcref{claim; ex 3} below, we see that the inequalities hold for all \(R\) satisfying
\begin{align*}
R +1 \geq \frac{c}{r}, 
\quad  
\frac{R}{n} \geq cr^{2k}
\end{align*}
for some \(0< r <\frac{1}{2}\) and some constant \(c>0\).
By choosing \(r \asymp n^{-\frac{1}{2k+1}}\), we see that Assumption~\labelcref{assumption; weak treatment overlap} holds with \(R \asymp n^{\frac{1}{2k+1}}\).    
\end{proof}


\begin{claim}\label{claim; ex 3}
For any \(g\) with \(\|g\|_{H^k([0,1])} =1\) and any \(0<r < \frac{1}{2}\),
\begin{align*}
\frac{c}{r} \int_0^1 zg^2(z) \de z  
+  cr^{2k}
\geq \int_0^1 g^2(z)\de z
\end{align*}
holds for some constant \(c>0\) that depends only on \(k\).
\end{claim}

\begin{proof}
Let \(c_1\) be the constant from Lemma~\labelcref{lemma; polynomial integration} and set \(c = c_1 +1\). 
Then,
\begin{align*}
\frac{c}{r} \int_0^1 zg^2(z) \de z  +  cr^{2k} 
&\geq \Bigl(c_1  \frac{1}{r}+\frac{1}{r}\Bigr) 
\Bigl(\int_0^r zg^2(z)\de z + \int_r^1   zg^2(z)\de z \Bigr) 
+  c_1r^{2k}
\\
&\geq \int_r^1 g^2(z)\de z  +c_1 \int_r^1 g^2(z)\de z 
+  c_1r^{2k}  
\\
&\geq \int_0^1  g^2(z)\de z 
\quad \text{(by Lemma~\labelcref{lemma; polynomial integration})}.
\end{align*}
\end{proof}


\begin{lemma}\label{lemma; polynomial integration}
For any \(0<r<\frac{1}{2}\) and \(g \in H^k([0,1])\) with \(\|g\|_{H^k([0,1])}=1\), we have 
\begin{align*}
\int_{0}^r g(z)^2 \de z \leq  c_1 \int_r^1 g(z)^2 \de z +  c_1r^{2k}
\end{align*}
for some constant \(c_1\) that depends only on \(k\).
\end{lemma}

\begin{proof}
By Sobolev embedding and Morrey's theorem \citep{evans2022partial}, there exist a polynomial \(p\) of degree \(k-1\) and a function \(\varepsilon\) such that for all \(0 < t < 2r\),
\begin{align*}
g(t) = p(t) + \varepsilon(t),
\end{align*}
where \(|\varepsilon(t)| \leq c_2 |t-r|^{k-\frac{1}{2}}\) for some constant \(c_2>0\) that depends only on \(k\). 
(Indeed, it is \(\frac{1}{(k-1)!}\) times a constant from Morrey's theorem.)

\paragraph{Goal.} 
We want to show
\begin{align*}
\|g \|_{L^2([0,r])}^2 
\leq c_1 \|g \|_{L^2([r,2r])}^2 
+c_1r^{2k}.
\end{align*} 
First, observe that
\begin{align}
\| g\|^2_{L^2([0,r])} =\| p + \varepsilon\|^2_{L^2([0,r])}  
&\leq 2\| p\|_{L_2([0,r])}^2 + 2\|  \varepsilon\|^2_{L^2([0,r])} 
\nonumber \\
&\leq 2\| p\|_{L_2([0,r])}^2 +2 rc_2^2 r^{2k-1}. 
\label{equation; singular propensity 1}
\end{align}
Next, we see that
\begin{align*}
\| g\|^2_{L^2([r,2r])} 
=\|p+\varepsilon \|^2_{L^2([r,2r])} 
&= \|p\|^2_{L^2([r,2r])} 
+ \|\varepsilon\|^2_{L^2([r,2r])} 
-2\langle p, \varepsilon \rangle_{L^2([r,2r])}
\\
&\geq \|p\|^2_{L^2([r,2r])} 
+ \|\varepsilon\|_{L^2([r,2r])}^2  
- 2\|p\|_{L^2([r,2r])}\|\varepsilon\|_{L^2([r,2r])}.
\end{align*}
Hence,
\begin{align*}
&\|g \|^2_{L^2([r,2r])} 
+ \|\varepsilon \|^2_{L^2([r,2r])} 
\\
&\geq \frac{1}{2}\|p\|^2_{L^2([r,2r])}  
+ \frac{1}{2}\|p\|^2_{L^2([r,2r])} 
+2\|\varepsilon \|^2_{L^2([r,2r])} 
- 2\|p\|_{L^2([r,2r])}\|\varepsilon\|_{L^2([r,2r])}
\\
&\geq \frac{1}{2}\|p\|^2_{L^2([r,2r])}.
\end{align*}
Set \(\tilde{c} =4\max\bigl(c_2^2, c_3^k\bigr)\), where \(c_3\) is the constant from Lemma~\labelcref{lemma; polynomial integral ratio}.
By combining Lemma~\labelcref{lemma; polynomial integral ratio}, we obtain
\begin{align*}
&\tilde{c} 
\bigl(\|g \|^2_{L^2([r,2r])} 
+ \|\varepsilon \|^2_{L^2([r,2r])}\bigr) 
+ \tilde{c}r^{2k} 
\\
&\geq 4c_3^k 
\bigl(\|p+\varepsilon \|^2_{L^2([r,2r])} 
+ \|\varepsilon \|^2_{L^2([r,2r])}\bigr) 
+ 4c_2^2r^{2k} 
\\
&\geq 2c_3^k \|p\|^2_{L^2([r,2r])}
+ 2rc_2^2r^{2k-1}
\\
&\stackrel{(i)}{\geq} 2\|p\|^2_{L^2([0,r])} 
+2rc_2^2r^{2k-1} 
\\
&\stackrel{(ii)}{\geq}  \| p + \varepsilon\|^2_{L^2([0,r])},
\end{align*}
where (i) follows from Lemma~\labelcref{lemma; polynomial integral ratio} and (ii) follows from \eqref{equation; singular propensity 1}.
Thus,
\begin{align*}
\tilde{c}  \|g\|^2_{L^2([r,2r])}  
+ (\tilde{c} c_2^2 + \tilde{c})r^{2k} 
\geq \| g\|^2_{L^2([0,r])}.
\end{align*}
Setting \(c_1 := \tilde{c}c_2^2 + \tilde{c}\) completes the proof.
\end{proof}

\begin{lemma}\label{lemma; polynomial integral ratio}
For any polynomial \(p(x)\) of degree \(\beta\) and any \(0<r<1\), we have
\begin{align*}
\int_{-r}^0 p(x)^2 \de x 
\leq c_3^\beta \int_0^r p(x)^2 \de x
\end{align*}
for some absolute constant \(c_3>0\).
\end{lemma}

\begin{proof}
Let \(p(x)= a_0 + a_1 x + \dots + a_\beta x^\beta\).
Denote the maximum and minimum eigenvalues of 
\(\EE_{x \sim \operatorname{Unif}(0,1)}\bigl[(1,x,\dots,x^\beta)(1,x,\dots,x^\beta)^\top\bigr]\) 
by \(M_\beta\) and \(m_\beta\), respectively. Then,
\begin{align*}
\int_0^r p(x)^2 \de x 
&= r \int_0^1 p(rx)^2 \de x 
\geq  rm_\beta 
\|(a_0, ra_1, \dots, r^\beta a_\beta) \|_2^2.
\end{align*}
Next,
\begin{align*}
\int_{-r}^0 p(x)^2 \de x 
&= r \int_{-1}^0 p(rx)^2 \de x 
= r \int_0^1 p(-rx)^2 \de x 
\\
&\leq  rM_\beta 
\bigl\|(a_0, -ra_1, r^2 a_2, \dots, r^\beta a_\beta(-1)^\beta)\bigr\|_2^2 
\\
&= rM_\beta 
\|(a_0, ra_1, r^2 a_2, \dots, r^\beta a_\beta)\|_2^2.
\end{align*}
By Lemma~\labelcref{lemma; key constant LPR}, \(\frac{M_\beta}{m_\beta} \leq c_3^\beta\) for some absolute constant \(c_3>0\). 
Hence the result follows immediately.
\end{proof}



\section{Proofs for Cross Fitting Algorithm}\label{subsection; proofs for cross fitting}

With a slight modification of our analysis, we can obtain the same rate of results for the CF algorithm.
We briefly provide a proof sketch of the CF algorithm's performance analysis.
We set \(\hat{\eta}_{\final}^{(1)}, \hat{\eta}_{\final}^{(2)}, \hat{\eta}_{\final}^{(3)}\) as the final estimators for three dataset permutations.
Next, we define the average estimator as
\[
\hat{\eta}_{\final} := \frac{1}{3} \bigl(\hat{\eta}_{\final}^{(1)} 
+\hat{\eta}_{\final}^{(2)} +\hat{\eta}_{\final}^{(3)}\bigr).
\]

By Theorem~\labelcref{theorem; main theorem}, with probability at least \(1-n^{-10}\), each estimator \(\hat{\eta}_{\operatorname{final}}^{(j)}\) for \(j =1,2,3\) achieves the same MSE bound as in Theorem~\labelcref{theorem; main theorem}.
Since
\begin{align*}
\|\hat{\eta}_{\final} -\eta^\star \|_{\bSigma_\cT} 
&\leq  \frac{1}{3} \sum_{j=1}^3 \|\hat{\eta}_{\final}^{(j)} -\eta^\star \|_{\bSigma_\cT} 
\leq \max_{j \in \{1,2,3\}}\cE_\cT(\hat{\eta}_{\final}^{(j)}),
\end{align*}
with probability at least \(1-3n^{-10}\), we have 
\begin{align*}
\max_{j \in \{1,2,3\}}\cE_\cT(\hat{\eta}_{\final}^{(j)} )
&\lesssim  n_{\operatorname{eff}}^{-\alpha} \norm{h^\star}^{2(1-\alpha)}_\cF 
+ M^2 \Bigl(\frac{1}{n_{\operatorname{eff}}}+ \frac{R}{n_\cT}\Bigr),
\end{align*}
which is the same rate as in Theorem~\labelcref{theorem; main theorem}.

\hfill \BlackBox


\section{Technical Lemmas}

\subsection{Lemmas for Bias-variance Trade-off}

Next, we present our key lemmas regarding the optimal choice of \(\lambda\) under the given polynomial decay eigenvalue spectrum. 
Under Assumption~\labelcref{Assumption; eigenvalue decay}, we define the effective dimension \(\db(\lambda) = \inf \{j \mid \mu_j < \lambda \}\), and the following inequality
\begin{align*}
\frac{\sum_{\mu_j < \lambda} \mu_j}{\lambda} \leq c \db(\lambda)
\end{align*}
holds for some constant \(c > 0\). Also, we have \(\db(\lambda) \lesssim \lambda^{-\frac{1}{2\ell}}\).

\begin{lemma}[Performance of optimal regularizer]\label{lemma; optimal trade-off lambda}
Let \(h > 0\) be an arbitrary positive constant and let \(\eta \in \HH\) with \(\lambda_2 = h^{\alpha}B^{-(1-\alpha)}\|\eta\|_{\HH}^{-2\alpha} \ge \frac{\xi}{n}\). Then the following holds:
\begin{align*}
B \lambda_2 \norm{\eta}^2_\HH + h \Tr(\Sbar_{\lambda_2}) \lesssim \norm{\eta}^{2(1-\alpha)}_\HH\, h^{\alpha} B^{\alpha}.
\end{align*}
\end{lemma}

\begin{proof}
Under Assumption~\ref{assumption; overlap source target}, we see that 
\begin{align*}
2(B \bSigma + B \lambda_2 \Ib) \succeq \bSigma_\cT + B \lambda_2 \Ib,
\end{align*}
and hence
\begin{align*}
\bSigma + \lambda_2 \Ib \succeq \frac{1}{2}\Bigl(\frac{1}{B} \bSigma_\cT + \lambda_2 \Ib\Bigr).
\end{align*}
Therefore, we have
\begin{align*}
\Tr( \Sbar_{\lambda_2} ) &= \Tr((\bSigma + \lambda_2 \Ib)^{-1} \bSigma_\cT)\\
&\lesssim \Tr\Bigl(\bSigma_\cT \Bigl(\frac{1}{B} \bSigma_\cT + \lambda_2 \Ib\Bigr)^{-1}\Bigr)\\
&\leq \sum_{j=1}^{\infty} \frac{B \mu_j}{\mu_j + B \lambda_2}.
\end{align*}  
Then the left-hand side of the statement is bounded by 
\begin{align*}
B \lambda_2 \norm{\eta}_\HH^2 + h \sum_{j=1}^{\infty} \frac{B \mu_j}{\mu_j + B \lambda_2} 
&\lesssim B\Bigl(\lambda_2 \norm{\eta}^2_\HH + h\, \db(\lambda_2 B)\Bigr)\\
&\lesssim B\Bigl(\lambda_2 \norm{\eta}^2_\HH + h\, (\lambda_2 B)^{-\frac{1}{2\ell}}\Bigr)\\
&\lesssim B\Bigl(\norm{\eta}^2_\HH\, \lambda_2 + h\, B^{-\frac{1}{2\ell}} \lambda_2^{-\frac{1}{2\ell}}\Bigr)\\
&\lesssim B\Bigl(\norm{\eta}^2_\HH\Bigr)^{\frac{1}{2\ell+1}} h^{\frac{2\ell}{2\ell+1}} B^{-\frac{1}{2\ell+1}} 
\quad \text{($\because$ evaluate at \(\lambda_2 = h^{\alpha}B^{-(1-\alpha)}\|\eta\|_{\HH}^{-2\alpha}\))}\\
&= \Bigl(\norm{\eta}^2_\HH\Bigr)^{\frac{1}{2\ell+1}} h^{\frac{2\ell}{2\ell+1}} B^{\frac{2\ell}{2\ell+1}}\\
&= \Bigl(\norm{\eta}^2_\HH\Bigr)^{1-\alpha} h^{\alpha} B^{\alpha},
\end{align*}
where we used \(\db(\lambda_2) \lesssim \lambda_2^{-\frac{1}{2\ell}}\) under Assumption~\labelcref{Assumption; eigenvalue decay}.
\end{proof}

\begin{corollary}[Performance of optimal regularizer in grid]\label{corollary; optimal MSE in grid}
Under the same setup as in Lemma~\labelcref{lemma; optimal trade-off lambda}, set \(\lambda^\star = h^{\alpha}B^{-(1-\alpha)}\|\eta\|_{\HH}^{-2\alpha}\). Then, for any \(\lambda > 0\) with \(\lambda^\star \leq \lambda \leq 2\lambda^\star\),
\begin{align*}
B \lambda \norm{\eta}^2_\HH + h \Tr(\Sbar_{\lambda}) \lesssim \Bigl(\norm{\eta}_\HH^2\Bigr)^{1-\alpha} h^{\alpha} B^{\alpha}.
\end{align*}
\end{corollary}

\begin{proof}
Observe that 
\begin{align*}
B \lambda \norm{\eta}^2_\HH + h \Tr(\Sbar_{\lambda})
&\leq B\Bigl( \lambda \norm{\eta}^2_\HH + h\, (\lambda B)^{-\frac{1}{2\ell}}\Bigr) \\
&\leq B\Bigl(2g\, \lambda^\star \norm{\eta}^2_\HH + h\, (\lambda^\star B)^{-\frac{1}{2\ell}}\Bigr) \\
&\leq 2B\Bigl(g\, \lambda^\star \norm{\eta}^2_\HH + h\, (\lambda^\star B)^{-\frac{1}{2\ell}}\Bigr) \\
&\lesssim \Bigl(\norm{\eta}_\HH^2\Bigr)^{1-\alpha} h^{\alpha} B^{\alpha}.
\end{align*}
\end{proof}


\subsection{Concentration Inequalities}

First, we present the key lemmas of the trace class from \citet*{wang2023pseudo}.
\begin{lemma}[Lemma E.1 from \citealt{wang2023pseudo}]\label{lemma; quadratic martingale}
Suppose that $\boldsymbol{x} \in \mathbb{R}^d$ is a zero-mean random vector with $\|\boldsymbol{x}\|_{\psi_2} \leq 1$. There exists a universal constant $C>0$ such that for any symmetric and positive semi-definite matrix $\boldsymbol{\Sigma} \in \mathbb{R}^{d \times d}$,
\begin{align*}
\mathbb{P}\left(\boldsymbol{x}^{\top} \boldsymbol{\bSigma} \boldsymbol{x} \leq C \operatorname{Tr}(\boldsymbol{\bSigma}) t\right) \geq 1-e^{-r(\boldsymbol{\bSigma}) t}, \quad \forall t \geq 1 .
\end{align*}

Here $r(\boldsymbol{\bSigma})=\operatorname{Tr}(\boldsymbol{\bSigma}) /\|\boldsymbol{\bSigma}\|_2$ is the effective rank of $\boldsymbol{\bSigma}$.
\end{lemma}
\begin{lemma}[Corollary E.1 from \citealt{wang2023pseudo}]\label{lemma; trace class concentration bounded}
Let $\left\{\boldsymbol{x}_i\right\}_{i=1}^n$ be i.i.d. random elements in a separable Hilbert space $\mathbb{H}$ with $\boldsymbol{\bSigma}=$ $\mathbb{E}\left(\boldsymbol{x}_i \otimes \boldsymbol{x}_i\right)$ being trace class. Define $\hat{\boldsymbol{\bSigma}}=\frac{1}{n} \sum_{i=1}^n \boldsymbol{x}_i \otimes \boldsymbol{x}_i$. Choose any constant $\gamma \in(0,1)$ and define an event $\mathcal{A}=\{(1-\gamma)(\boldsymbol{\bSigma}+\lambda \boldsymbol{I}) \preceq \hat{\boldsymbol{\bSigma}}+\lambda \boldsymbol{I} \preceq(1+\gamma)(\boldsymbol{\bSigma}+\lambda \boldsymbol{I})\}$.
1. If $\left\|\boldsymbol{x}_i\right\|_{\mathbb{H}} \leq M$ holds almost surely for some constant $M$, then there exists a constant $C \geq 1$ determined by $\gamma$ such that $\mathbb{P}(\mathcal{A}) \geq 1-\delta$ holds so long as $\delta \in(0,1 / 14]$ and $\lambda \geq \frac{C \xi \log (n / \delta)}{n}$.    
\end{lemma}


\subsection{Lemmas for Model Selection}

\begin{lemma}[Theorem 5.2 from \citealt{wang2023pseudo}]\label{lemma; loss model selection}
Let $\left\{\boldsymbol{z}_i\right\}_{i=1}^n$ be deterministic elements in a set $\mathcal{Z} ; g^{\star}$ and $\left\{g_j\right\}_{j=1}^m$ be deterministic functions in $\mathcal{Z} ; \widetilde{g}$ be a random function on $\mathcal{Z}$. Define
\begin{align*}
\mathcal{L}(g)=\frac{1}{n} \sum_{i=1}^n\left|g\left(\boldsymbol{z}_i\right)-g^{\star}\left(\boldsymbol{z}_i\right)\right|^2
\end{align*}
for any function $g$ on $\mathcal{Z}$. Assume that the random vector $\widetilde{\boldsymbol{y}}=\left(\widetilde{g}\left(\boldsymbol{z}_1\right), \widetilde{g}\left(\boldsymbol{z}_2\right), \cdots, \widetilde{g}\left(\boldsymbol{z}_n\right)\right)^{\top}$ satisfies $\|\widetilde{\boldsymbol{y}}-\mathbb{E} \widetilde{\boldsymbol{y}}\|_{\psi_2} \leq V<\infty$. Choose any
\begin{align*}
\widehat{j} \in \underset{j \in[m]}{\operatorname{argmin}}\left\{\frac{1}{n} \sum_{i=1}^n\left|g_j\left(\boldsymbol{z}_i\right)-\widetilde{g}\left(\boldsymbol{z}_i\right)\right|^2\right\} .
\end{align*}

There exists a universal constant $C$ such that for any $\delta \in(0,1]$, with probability at least $1-\delta$ we have
\begin{align*}
\mathcal{L}\left(g_{\tilde{j}}\right) \leq \inf _{\gamma>0}\left\{(1+\gamma) \min _{j \in[m]} \mathcal{L}\left(g_j\right)+C\left(1+\gamma^{-1}\right)\left(\mathcal{L}(\mathbb{E} \widetilde{g})+\frac{V^2 \log (m / \delta)}{n}\right)\right\} .
\end{align*}

Consequently,
\begin{align*}
\mathbb{E} \mathcal{L}\left(g_{\tilde{j}}\right) \leq \inf _{\gamma>0}\left\{(1+\gamma) \min _{j \in[m]} \mathcal{L}\left(g_j\right)+C\left(1+\gamma^{-1}\right)\left(\mathcal{L}(\mathbb{E} \widetilde{g})+\frac{V^2(1+\log m)}{n}\right)\right\}.
\end{align*}    
\end{lemma}



\begin{lemma}[Lemma E.5 from \citealt{wang2023pseudo}]\label{lemma; key lemma in-sample population}
Let $\mathbb{H}$ be a separable Hilbert space with inner product $\langle\cdot, \cdot\rangle$ and norm $\|\cdot\|_{\mathbb{H}} ;\left\{\boldsymbol{z}_i\right\}_{i=1}^n$ be i.i.d. samples from a distribution over $\mathbb{H}$ such that $\boldsymbol{S}=\mathbb{E}\left(\boldsymbol{z}_i \otimes \boldsymbol{z}_i\right)$ is trace class; $\boldsymbol{w} \in \mathbb{H}$ be random and independent of $\left\{\boldsymbol{z}_i\right\}_{i=1}^n$. Define $\widehat{\boldsymbol{S}}=\frac{1}{n} \sum_{i=1}^n \boldsymbol{z}_i \otimes \boldsymbol{z}_i$. We have the following results.
1. Suppose that $\mathbb{E}\left\|\boldsymbol{z}_1\right\|_{\mathbb{H}}^4<\infty$ and the inequality
\begin{align*}
\mathbb{P}\left(\left|\left\langle\boldsymbol{z}_1, \boldsymbol{w}\right\rangle\right| \leq r \text { and }\|\boldsymbol{w}\|_{\mathbb{H}} \leq K\right) \geq 1-\varepsilon
\end{align*}
holds for some deterministic $r>0, K>0$ and $\varepsilon \in(0,1)$. Then, for any $\gamma \in(0,3 / 4]$ and $\eta \in(0,1)$, we have
\begin{align*}
\begin{aligned}
& \mathbb{P}\left(\langle\boldsymbol{w}, \widehat{\boldsymbol{S}} \boldsymbol{w}\rangle \leq(1+\gamma)\langle\boldsymbol{w}, \boldsymbol{S} \boldsymbol{w}\rangle+\frac{r^2 \log (1 / \eta)}{\gamma n}\right) \geq 1-\eta-n \varepsilon \\
& \mathbb{P}\left(\langle\boldsymbol{w}, \widehat{\boldsymbol{S}} \boldsymbol{w}\rangle \geq(1-\gamma)\langle\boldsymbol{w}, \boldsymbol{S} \boldsymbol{w}\rangle-\frac{r^2 \log (1 / \eta)}{\gamma n}-K^2 \varepsilon^{1 / 4} \sqrt{\mathbb{E}\|\boldsymbol{z}\|_{\mathbb{H}}^4}\right) \geq 1-\eta-(n+1) \sqrt{\varepsilon}.
\end{aligned}
\end{align*}

\end{lemma}

\subsection{Other Lemmas}
\begin{lemma}\label{lemma; matrix inverse inequality}\label{lemma; psd inverse inequality}
For any psd trace class $A \succeq 0$, the following holds: 
\begin{align*}
A^{\frac{1}{2}}(A+\lambda I)^{-1} A^{\frac{1}{2}} \preceq I
\end{align*}
Also if two invertible trace class $A,B$ satisfies $A \succeq B$, then $A^{-1} \preceq B^{-1}$.
\end{lemma}

\begin{proof}
We do eigendecomposition for $A$ and we get $A = P \Lambda P^\top$. 
Then, 
\begin{align*}
A^{\frac{1}{2}}(A+\lambda I)^{-1} A^{\frac{1}{2}} &= P \Lambda^{\frac{1}{2}} P^\top P (\Lambda + \lambda I)^{-1} P^\top  P \Lambda^{\frac{1}{2}} P^\top  \\
&= P \Lambda^{\frac{1}{2}}(\Lambda + \lambda I)^{-1}\Lambda^{\frac{1}{2}} P^\top \\
&\preceq PP^\top = I.
\end{align*}
For the second statement firstly get $I \preceq B^{-1 / 2} A B^{-1 / 2}$. 
The eigenvalues of the latter symmetric operator are thus $\geq 1$. Its inverse $B^{1 / 2} A^{-1} B^{1 / 2}$ has eigenvalues $\leq 1$, that is $B^{1 / 2} A^{-1} B^{1 / 2} \preceq I$. This gives $z^T B^{1 / 2} A^{-1} B^{1 / 2} z \leq\|z\|^2$ for $z \in \HH$. 
Setting $w=B^{1 / 2} z$, this writes $w^T A^{-1} w \leq z^T B^{-1} z$, that leads $A^{-1} \preceq B^{-1}$.  
\end{proof}

\begin{lemma}\label{lemma; trace simple inequality}
For any psd operators \(A,B,C\) with \(B \preceq C\), 
\begin{align*}
\Tr(AB) \leq \Tr(AC).
\end{align*}    
\end{lemma}

\begin{proof}
Observe that 
\begin{align*}
\Tr(AB) = \Tr(A^{\frac{1}{2}} BA^{\frac{1}{2}}) \leq \Tr(A^{\frac{1}{2}} CA^{\frac{1}{2}}) = \Tr(AC).
\end{align*}
\end{proof}

\begin{lemma}[Key constant of local polynomial regression]\label{lemma; key constant LPR}
For any \(k \in \NN\), the matrix
\[
\EE_{x \sim \operatorname{Unif}[0,1]}[(1,x, \dots, x^k)(1,x, \dots, x^k)^\top]
\]
has condition number (i.e., the ratio of the maximum eigenvalue to the minimum eigenvalue) \(\cO\!\left(\frac{(1+\sqrt{2})^{4k}}{\sqrt{k}}\right)\), and hence its minimum eigenvalue is lower bounded by \(\frac{\sqrt{k}}{c^k}\).
\end{lemma}

\begin{proof}
Note that \(\EE_{x \sim \operatorname{Unif}[0,1]}[(1,x, \dots, x^k)(1,x, \dots, x^k)^\top]\) is a Hilbert matrix, and this follows from well-known results in \citet{choi1983tricks}.
\end{proof}
    

\section{Supplements for Numerical and Real-world Studies}\label{sec:app:numerical}

\subsection{Benchmark methods}\label{sec:app:bench}

To ensure fair comparisons, we adopt cross-fitting on all the following three benchmark methods that involve data-splitting setups. 

\paragraph{Separate regression (\texttt{SR}).} We randomly divide the source data into two parts, $\mathcal{D}_1$ and $\mathcal{D}_1'$. For $f_1^\star(z)$ and $f_0^\star(z)$, we fit candidate models using the subsets of $\mathcal{D}_1$ with $a = 1$ and $a = 0$, respectively. We then fit imputation models for $f_1^\star(z)$ and $f_0^\star(z)$ using $\widetilde\lambda_0= \widetilde\lambda_1= \frac{1}{5n}$ (the same as $\lambda_{1,0},\lambda_{1,1}$ in \texttt{COKE}) with the subsets of $\mathcal{D}_1'$ with $a = 1$ and $a = 0$, respectively. The model selection for each $f_a^\star(x)$ follows the pseudo-labeling technique described in \cite{wang2023pseudo}, where pseudo-labels are generated for all unlabeled target data and used to for model selection over KRR estimators with their tuning parameters ranging in ${\bm \Lambda}$. In our numerical studies, this range of tuning parameters is set to be the same as \texttt{COKE}, i.e., ${\bm \Lambda} = \{\frac{2^k}{5n} : k = 0, 1, \ldots, \lceil\log_2(5n)\rceil\}$. The final model is computed as $\hat h(z) = \hat f_1(z) - \hat f_0(z)$.

\paragraph{\texttt{DR-CATE}.} We implement the two-stage DR-Learner, as described in \cite{kennedy2020towards}, using kernel ridge regression for all regression tasks. The source dataset is randomly divided into two subsets, $\mathcal{D}_1$ and $\mathcal{D}_1'$. Using $\mathcal{D}_1$, we estimate the propensity score $\pi(z)$ through logistic regression, and we also estimate the outcome regression functions $f_a^\star(z)$ (for $a = 0, 1$) using kernel ridge regression with model selection performed through hold-out validation.

Let $d_i = (z_i, a_i, y_i)$. We define the pseudo-outcome as $$\widehat\varphi(d)=\frac{a-\hat \pi(z)}{\hat \pi(z)(1-\hat \pi(z))}(y-\hat f_a(z))+\hat f_1(z)-\hat f_0(z).$$

Next, we regress the pseudo-outcome $\widehat\varphi(d)$ on $z$ using $\mathcal{D}_1'$ through kernel ridge regression with hold-out validation. We divide $\mathcal{D}_1'$ into two halves: training set $\mathcal{D}_2$ and validation set $\mathcal{D}_3$. Then we fit KRR for $\widehat\varphi(d_i)$ against $z_i$ on $\mathcal{D}_2$, with the candidate set of penalization parameters being the same as \texttt{COKE}, i.e., ${\bm \Lambda} = \{\frac{2^k}{5n} : k = 0, 1, \ldots, \lceil\log_2(5n)\rceil\}$ and select the best estimator using $\mathcal{D}_3$.



\subsubsection{\texttt{ACW-CATE}}

We implement the two-stage ACW estimator. The source dataset is randomly divided into two subsets, $\mathcal{D}_1$ and $\mathcal{D}_1'$. Using $\mathcal{D}_1$, we estimate the propensity score and outcome regression functions as described previously in \texttt{DR-CATE}. We also estimate the density ratio $w(z) = p_\mathcal{T}(z)/p_\mathcal{S}(z)$. Define $S$ such that $S = 0$ indicates data from the source, and $S = 1$ indicates data from the target population. First, we estimate $\mathbb{P}(S = 1 \mid z)$ using both $\mathcal{D}_1$ and target data through logistic regression. The density ratio estimate is then given by:

\[
\widehat\omega(z) = \frac{n_\mathcal{S} \widehat{\mathbb{P}}(S = 1 \mid z)}{n_\mathcal{T} \widehat{\mathbb{P}}(S = 0 \mid z)}.
\]

We define the pseudo-outcome $\widehat\varphi$ as
$$\begin{aligned}\widehat\varphi(d)=&\ (1-S)\cdot\frac{n_2+n_\mathcal{T}}{n_2}\widehat\omega(z)\left\{\frac{a-\hat \pi(z)}{\hat \pi(z)(1-\hat \pi(z))}(y-\hat f_a(z))\right\}\\&\ +S\cdot\frac{n_2+n_\mathcal{T}}{n_\mathcal{T}}\left\{\hat f_1(z)-\hat f_0(z)\right\}.\end{aligned}$$

Next, we use kernel ridge regression to regress the pseudo-outcome $\widehat\varphi$ on $z$ using the other half of the source data $\mathcal{D}_1'$ combined with all target data $\mathcal{D}_\mathcal{T}$. We divide these data into two halves for hold-out validation: the training set $\mathcal{D}_2$, containing half of $\mathcal{D}_1'$ and half of $\mathcal{D}_\mathcal{T}$, and the validation set $\mathcal{D}_3$. Then we fit KRR for $\widehat\varphi(d_i)$ against $z_i$ on $\mathcal{D}_2$, with the candidate set of penalization parameters being the same as \texttt{COKE}, i.e., ${\bm \Lambda} = \{\frac{2^k}{5n} : k = 0, 1, \ldots, \lceil\log_2(5n)\rceil\}$ and select the best estimator using $\mathcal{D}_3$.


\subsection{Supplementary results}

\subsubsection{Simulation}\label{sec:app:simu}

Under the default setting with $q = 1$, we compare the performance of \texttt{COKE} with and without cross-fitting across varying $S_B$, as shown in Figure \labelcref{fig:CF}. Cross-fitting improves estimation accuracy by leveraging an additional estimate obtained by swapping the roles of $\mathcal{D}_1$ and $\mathcal{D} \setminus \mathcal{D}_1$, and averaging the two estimates. The mean squared error for the cross-fitted version is consistently lower across all $S_B$ values, showing its effectiveness in reducing estimation error. For example, at $S_B = 20$, cross-fitting reduces the mean squared error by approximately 13\%.


\begin{figure}[ht]
\centering
\includegraphics[width=8cm]{simulation_results/changeB_cross.pdf}
\caption{Comparison of mean squared error for \texttt{COKE} with and without cross-fitting under the default setting with $q = 1$ across varying $S_B$ (degree of covariate shift between source and target). Cross-fitting reduces estimation error consistently by approximately 13\%.}
\label{fig:CF}
\end{figure}


\subsubsection{Real example}\label{sec:app:real:results}

\begin{table}[htbp]
\centering
\begin{tabular}{|c|cccc|}
\hline Metrics & \texttt{COKE} & \texttt{SR} & \texttt{DR-CATE} & \texttt{ACW-CATE} \\
\hline Spearman Cor with $\hat{s}$ & 0.166 & 0.113 & 0.112 & 0.104 \\
\hline Pearson Cor with $\hat{s}$ & 0.036 & 0.025 & 0.015 & 0.023  \\ 
\hline
\end{tabular}
\caption{\label{tab:res:401:datasplit} Spearman and Pearson correlation coefficients between the score $\hat{s}_{0i}$ and the CATE estimators obtained by data-fitting in our real-world example. The nuisance models are obtained using generalized linear regression.}
\end{table}


\begin{table}[htbp]
\centering
\begin{tabular}{|c|cccc|}
\hline Metrics & \texttt{COKE} & \texttt{SR} & \texttt{DR-CATE} & \texttt{ACW-CATE} \\
\hline Spearman Cor with $\hat{s}$ & 0.126 & 0.101 & 0.078 & 0.105 \\
\hline Pearson Cor with $\hat{s}$ & 0.063 & 0.044 & 0.023 & 0.040   \\ 
\hline
\end{tabular}
\caption{\label{tab:res:401:cross:rf} Spearman and Pearson correlation coefficients between the score $\hat{s}_{0i}$ and the CATE estimators obtained by cross-fitting in our real-world example. The nuisance models are obtained using random forest.}
\end{table}




\end{document}