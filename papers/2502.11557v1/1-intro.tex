\section{Introduction}
\label{sec:intro}

Graphs have been increasingly adopted to capture the relationships among entities in various domains, including social media, biological networks, communication networks, collaboration networks  etc. As a result, graph data analysis has gained great attention in the recent years. 
%
One of the most fundamental problems in graph analysis is \emph{maximum common subgraph search}, which is widely used to measure the similarity of two graphs~\cite{mcgregor1982backtrack,mccreesh2016clique,vismara2008finding,zhoustrengthened,liu2020learning,liu2023hybrid,mccreesh2017partitioning,choi2012efficient,rutgers2010approximate,xiao2009generative,zanfir2018deep,bai2021glsearch}. 
%
More precisely, a common subgraph between two graphs $Q$ and $G$ refers to a subgraph  that  appears in both $Q$ and $G$. 
% Conceptually, it is defined by a pair of induced subgraphs (i.e., an induced subgraph $q$ of $Q$ and an induced subgraph $g$ of $G$) and a bijection mapping (i.e., $\phi: V_q\rightarrow V_{g}$ from vertices in $q$ to vertices in $g$) such that $q$ and $g$ are \emph{isomorphic} to each other under the bijection $\phi$, which we denote by $\langle q,g,\phi \rangle$. 
\laks{Conceptually, it is defined by {\chengB a pair of subgraphs, $q = (V_q, E_q)$ of $Q$ and  $g = (V_g, E_g)$ of $G$, and a bijection mapping $\phi: V_q\rightarrow V_{g}$} such that $q$ and $g$ are \emph{isomorphic}  under $\phi$, which we denote by $\langle q,g,\phi \rangle$.
Given two graphs $Q$ and $G$, the problem asks for the common subgraph $\langle q,g,\phi \rangle$ with the maximum number of vertices in $q$ (equiv. $g$).} 

The maximum common subgraph search problem 
{\cheng has many applications} 
% is significant 
across various disciplines, 
{\chengB 
{\cheng and} 
has been widely studied~\cite{abu2014maximum,levi1973note,krissinel2004common,suters2005new,mcgregor1982backtrack,mccreesh2016clique,vismara2008finding,zhoustrengthened,liu2020learning,liu2023hybrid,mccreesh2017partitioning,choi2012efficient,rutgers2010approximate,xiao2009generative,zanfir2018deep,bai2021glsearch}}. 
% from both the theoretical 
% % interest
% {\cheng perspective}
% ~\cite{abu2014maximum,levi1973note,krissinel2004common,suters2005new} and the practical perspective~\cite{mcgregor1982backtrack,mccreesh2016clique,vismara2008finding,zhoustrengthened,liu2020learning,liu2023hybrid,mccreesh2017partitioning,choi2012efficient,rutgers2010approximate,xiao2009generative,zanfir2018deep,bai2021glsearch} in the past. 
To be specific, it offers an operator for evaluating the similarity between graphs in graph database systems~\cite{yan2005substructure} and thus 
% {\chengD It} 
has found a wide range of real applications, including cheminformatics~\cite{antelo2020maximum,schmidt2020disconnected}, communication networks~\cite{nirmala2016vertex}, software analysis~\cite{park2011deriving,sun2021effective},  biochemistry~\cite{bonnici2013subgraph,larsen2017cytomcs,ehrlich2011maximum}, and image segmentation~\cite{hati2016image}. 
% As an example of drug discovery and analysis, it would help
{\YuiRR For example, the similarity between two molecules 
% $Q$ and $G$ 
is calculated based on the maximum common subgraph between 
% $Q$ and $G$
{\chengD them}
% %, i.e., $Sim(Q,G)=(|V_q|+|E_q|)/((|V_Q|+|V_G|)\times (|E_Q|+|E_G|))$
~\cite{ehrlich2011maximum}.}
{\chengC Therefore, in drug discovery and analysis, it is used}
to quickly identify a small group of compounds with similar substructures (which tend to {\YuiR preserve} similar properties) for further analysis, 
% which reduces 
{\chengC so as to reduce}
the manual labor and shorten the cycle of discovery~\cite{ehrlich2011maximum}.
%
Besides, the maximum common subgraph search problem generalizes the well-studied \emph{subgraph matching} problem~\cite{bhattarai2019ceci,ullmann1976algorithm,sun2020rapidmatch,sun2020subgraph,shang2008taming,kim2023fast,han2013turboiso,han2019efficient,cordella2004sub,bi2016efficient,arai2023gup,jin2023circinus,sun2023efficient}. 
%%%%%%%%%%%%%%%%%%%%%%%%% 
\eat{ 
In specific, given two graphs $Q$ and $G$, subgraph matching aims to find from one graph (say, data graph $G$) all embeddings of the other (say, query graph $Q$), i.e., subgraphs of $G$ that are isomorphic to a query graph $Q$. 
Here, an embedding of $Q$ in $G$ is a common subgraph (i.e., a pair of subgraph $p$ and $q$) satisfying the constraint $q=Q$.
%
{\cheng Note that the embedding of $Q$ in $G$ (if it exists) corresponds to the maximum common subgraph between $G$ and $Q$.}
%
We note that subgraph matching is too restrictive in some real applications due to the data quality issues and/or potential requirements of the fuzzy search (e.g., no result will be returned if such an embedding does not exist). 
%
% Motivated by all the above, 
{\cheng Therefore,}
we focus on finding the maximum common subgraph between two graphs in this paper.} 
%%%%%%%%%%%%%%%%%%%%%%%%% 
\laks{More specifically, given a data graph $G$ and a query graph $Q$, subgraph matching seeks to find if there is an embedding\footnote{Not to be confused with graph embeddings.} from $Q$ to $G$, where an embedding means a 1-1 function from the nodes of $Q$ to those of $G$ which preserves edges, i.e., the embedding is a witness that $Q$ is isomorphic to a subgraph of $G$. Embedding or subgraph matching is a strong requirement. In real applications data quality is a real concern \cite{chiang2007coverage,balasundaram2011clique} and thus an embedding from $Q$ to $G$ may fail to exist, with subgraph matching yielding no results. In such circumstances, finding a maximum common subgraph is a graceful relaxation of the problem which may still yield useful results. {\YuiRR If the largest common subgraph is very similar to $Q$, it can serve as an approximation to the embedding of $Q$.} 
Given this, in this paper, we focus on finding the maximum common subgraph between two given graphs. }


\smallskip
\noindent\textbf{Challenges and {\chengC existing} methods}. Detecting the maximum common subgraph is quite  challenging as noted in the literature. Specifically, it is NP-hard~\cite{lewis1983michael} and is shown to be {\YuiR at least as hard to approximate as the maximum clique problem: it does not admit any $r$-approximate algorithm that runs in polynomial time (unless $P=NP$), for any $r\geq 1$~\cite{kann1992approximability}.}
%
%to be hard to approximate: it does not admit any $O(n^{\epsilon})$-approximate algorithm that runs in polynomial time (unless $P=NP$), \laks{for any}  $\epsilon>0$, \laks{where} $n$ is {\YuiR the number of vertices in $Q$ and $G$}~\cite{kann1992approximability}. 
%
{\Yui Existing works for solving the problem are of either theoretical or practical interest. On the one hand, some algorithms are designed 
% from the theoretical perspective
{\chengB to improve theoretical time complexity}
~\cite{abu2014maximum,levi1973note,krissinel2004common,suters2005new}. {\revision Assume that $Q$ has the number of vertices no larger than $G$, i.e., $|V_Q|\leq |V_G|$.} They have gradually improved the worst-case time complexity from $O^*(1.19^{|V_Q||V_G|})$~\cite{levi1973note} to $O^*(|V_Q|^{(|V_G|+1)})$~\cite{krissinel2004common}, and to $O^*((|V_Q|+1)^{|V_G|})$~\cite{suters2005new}, 
% which is the \emph{theoretical} state-of-the-art to the best of our knowledge. 
{\chengB which is \laks{the} best-known worst-case time complexity for the problem.} Here, $O^*$ suppresses polynomials.
However, these algorithms are of theoretical interest only and not efficient in practice. {\YuiRR This is mainly because their theoretical results rely on some sophisticated data structures while maintaining them introduces a huge amount of time and/or memory overhead in practice, e.g., the intermediate graph structure built from $Q$ and $G$ has $|V_Q|\times |V_G|$ vertices and could be very large if $G$ is large (e.g., with million nodes).}
%
On the other hand, quite a few algorithms have been developed {\YuiR towards improving the practical performance}~\cite{levi1973note,mcgregor1982backtrack,mccreesh2016clique,vismara2008finding,zhoustrengthened,liu2020learning,liu2023hybrid,mccreesh2017partitioning}. They are all backtracking (also known as branch-and-bound) algorithms, among which the recent works~\cite{zhoustrengthened,liu2020learning,liu2023hybrid} are based on a newly proposed backtracking framework called \texttt{McSplit}~\cite{mccreesh2017partitioning}. \texttt{McSplit} recursively partitions the search space (i.e., the set of possible common subgraphs) to multiple sub-spaces via a process of branching. Each sub-space corresponds to a branch. 
%
%The redundant computation during the backtracking refers to the time costs for exploring those branches that do not hold the maximum common subgraph to be returned. 
%
Furthermore, \texttt{McSplit} uses the 
% upper bounds of each branch (i.e., 
{\chengB upper bound on the size of common subgraphs that could be found within a branch} for reducing  redundant computations \laks{associated with exploring those branches that do not \laks{lead to finding}  the maximum common subgraph.} 
% The rationale is to 
{\chengB Specifically, it prunes} those branches that have upper bounds no larger than the size of the largest common subgraph seen so far. However, \textit{these algorithms provide no {\chengE non-trivial} theoretical guarantee on the worst-case time complexity.}
%and (2) still suffer from efficiency issues in practice.
}

{
We note that maximum common subgraph is closely related to a well-known graph similarity measure, namely graph edit distance (GED)~\cite{bunke1997relation}. The GED between two graphs $Q$ and $G$ is the  cost of the  least-cost edit path, i.e., a sequence of edit operations that transform $Q$ to $G$. Under a special cost function  which does not charge for edge insertion/deletion and charges an infinite cost for node/edge substitution, the GED computation has been shown to be equivalent to finding the maximum common subgraph~\cite{bunke1997relation}. However, recent state-of-the-art approaches~\cite{chen2019efficient,gouda2016csi_ged,piao2023computing,chang2020speeding,kim2023efficient} for computing GED \eat{cannot be used for providing an efficient solution to our problem since they study computing GED under} assume  different cost functions where the above equivalence does not hold. \eat{In specific, the cost function used in~\cite{bunke1997relation} ignores the costs for edge deletion/insertion and takes the costs for vertex/edge substitution as infinity, while those used in recent algorithms} Specifically, these cost functions do not ignore edge deletion/insertion costs and usually assign the same positive finite costs to various edit operators. Consequently, state of the art algorithms for GED cannot lead to efficient solutions for computing the maximum common subgraph. 

}   



\smallskip
\noindent\textbf{Our method}. In this paper, we develop an efficient backtracking algorithm called \texttt{RRSplit}, which \laks{leverages} newly-designed reductions and new upper bounds for decreasing the redundant computations. 
% In particular, with the proposed reductions, 
{\revision {\chengE With} $|V_Q|\leq |V_G|$,} \texttt{RRSplit} achieves a worst-case time complexity of $O^*((|V_G|+1)^{|V_Q|})$, \laks{matching} 
% , 
% to our best knowledge, the state-of-the-art
{\chengB the best-known worst-case time complexity for the problem}~\cite{suters2005new}. 
We note that this theoretical result is remarkable since (1) the algorithm {\chengD with the best-known time complexity  in~\cite{suters2005new}} is of theoretical interest only and is not {\cheng practically} efficient and (2) {\chengD the algorithms with the best-known practical performance, i.e., \texttt{McSplit}~\cite{mccreesh2017partitioning}  and its variants,} do not have any theoretical guarantee on the worst-case time complexity.
%
{\chengB Specifically}, \texttt{RRSplit} combines the following two kinds of reductions: (i) {\chengB vertex-equivalence-based and (ii) maximality-based}, and a  vertex-equivalence-based upper bound \laks{that we establish}. %We remark that the proposed reductions and upper bound are orthogonal to the existing upper bound techniques.

Vertex-equivalence-based reductions reduce the redundant computations induced by \emph{common subgraph isomorphism (cs-isomorphism)}. 
%
{\YuiR
{\chengD Given} two common subgraphs $\langle q,g,\phi \rangle$ and $\langle q',g',\phi' \rangle$ of graphs $G$ and $Q$, they are said to be \textit{common subgraph isomorphic} (\textit{cs-isomorphic} for short) if and only if $q$ is isomorphic to $q'$ (or equivalently, $g$ is isomorphic to $g'$). All cs-isomorphic common subgraphs evidently share the same structural information, and exploring all of them is clearly redundant.
%Consider a common subgraph $\langle q,g,\phi \rangle$ {\cheng (where $q$ is an induced subgraph of $Q$ and $g$ is an induced subgraph of $G$)} and a subgraph $q'$ of $Q$ that is isomorphic to $q$ under bijection $f: V_{q'}\rightarrow V_q$. We observe that $q'$ is isomorphic to $g$ under bijection $\phi \circ f$ (since graph isomorphism is an equivalence relation) and thus $\langle q',g, \phi \circ f\rangle$ is a common subgraph. Clearly, $\langle q,g,\phi \rangle$ and $\langle q',g, \phi \circ f\rangle$ carry the same structural information (i.e., $q$, $q'$ and $g$ are isomorphic), for which they are said to be self-{\cheng isomorphic}. Therefore, if $\langle q,g,\phi \rangle$ is found during the backtracking, redundant computations will occur when exploring those branches that hold all common subgraphs self-isomorphic to $\langle q,g,\phi \rangle$. 
% Unfortunately
%{\cheng Furthermore}, given a common subgraph $\langle q,g,\phi \rangle$, the number of its cs-isomorphic common subgraphs $\langle q',g',\phi' \rangle$ {\kaixin could be} \emph{exponentially large}, i.e., $O(|V_Q|^{|V_q|}\cdot |V_q|!)$. \eat{This is because (1) there could be $O(|V_Q|^{|V_q|})$ different subgraphs of $Q$ with the number of vertices \eat{inside} equal to $|V_q|$ and (2) for each subgraph $q'$, there could be $O(|V_q|!)$ isomorphic bijections between $q$ and $q'$.}
%
{\kaixin To reduce this redundancy, we take two sufficient conditions into consideration {\chengD when deciding whether we can prune a branch}. Specifically, } for {\chengC any} common subgraph $\langle q,g,\phi \rangle$ to be found in a branch, if there exists another that is cs-isomorphic to $\langle q,g,\phi \rangle$ (Condition 1) and has been found before (Condition 2), we can safely {\chengC prune the branch.}
% ignore the common subgraph $\langle q,g,\phi \rangle$. 
To facilitate the reduction, 
we will leverage the \emph{vertex equivalence} property \cite{nguyen2019applications} and an \emph{auxiliary data structure} for verifying Condition 1 and Condition 2, respectively {\chengC (details in Section~\ref{subsec:VE-reduction})}.}
%Motivated by this, vertex-equivalence-based reductions aim to prune those formed sub-branches that have all common subgraphs inside self-isomorphic to the one found before, which are designed based on the \emph{structural equivalence} among vertices.  


Maximality-based reductions capture the redundant computations induced by \emph{maximality}. Specifically, a common subgraph $\langle q,g,\phi\rangle$ is maximal if and only if there does not exist any other common subgraph $\langle q',g',\phi' \rangle$ such that $q$ and $g$ are subgraphs of $q'$ and $g'$, respectively. Therefore, the maximum common subgraph is a maximal common subgraph, and those branches that hold only non-maximal ones {\chengD would} incur redundant computations. To reduce them,  we observe one necessary condition for a branch to hold the largest common subgraphs {\chengC (details  in Section~\ref{subsec:maximality-reduction})}. As a result, we can safely prune those branches that \laks{violate} the condition.

{\YuiR
Furthermore, we leverage the vertex equivalence property to derive a new vertex-equivalence-based upper bound, which is tighter than the existing one~\cite{mccreesh2017partitioning} and thus can help to prune more branches 
{\chengC (details in Section~\ref{subsec:upper-bound}).}
% (note that a branch can be pruned if its upper bound is no larger than the largest common subgraph seen so far).
}



\smallskip
\noindent\textbf{Contributions}. We make the following  contributions.
\begin{itemize}[leftmargin=*]
    \item We introduce vertex-equivalence-based reductions for reducing the redundant computation induced by cs-isomorphism (Section~\ref{subsec:VE-reduction}). We further propose  maximality-based reductions for pruning those branches that hold non-maximal common {\chengE subgraphs} only  (Section~\ref{subsec:maximality-reduction}). {\YuiR Finally, we develop a new vertex-equivalence-based upper bound for pruning more branches  (Section~\ref{subsec:upper-bound}}).
    
    \item We propose a new backtracking algorithm called \texttt{RRSplit}, which is based on the newly-designed reductions. It has a worst-case time complexity\eat{of $O^*((|V_G|+1)^{|V_Q|})$, {\chengB which is the same as}} \laks{that matches} the best-known time complexity (of the algorithms that are of theoretical interest only) (Section~\ref{subsec:summary}). 
    
    \item We conduct an extensive empirical evaluation on {\chengE several} benchmark graph collections. Our experiments reveal that our algorithm \texttt{RRSplit} runs several orders of {\cheng magnitude} faster than the state-of-the-art \texttt{McSplitDAL} on the majority of the tested input instances (Section~\ref{sec:exp}). 
\end{itemize}

Section~\ref{sec:preli} provides a formal statement of the problem studied. Section~\ref{sec:sota} reviews the existing framework \texttt{McSplit} and its state-of-the-art variant \texttt{McSplitDAL}. In Section~\ref{sec:related} we discuss related work and conclude the paper in Section~\ref{sec:conclusion}. {
\YuiR Due to space limits, we omit some proofs and sketch others, and all proofs can be found in the 
\ifx \CR\undefined
Appendix. 
\else
technical report~\cite{TR}. 
\fi
}
