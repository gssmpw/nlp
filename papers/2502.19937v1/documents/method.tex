
\vspace{-0.5em}
\section{Method}
\vspace{-0.5em}

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figures/framework.pdf}
    \vspace{-1.5em}
    \caption{Illustration of the proposed framework. We use reference masks to separate reference images into foreground and background and CLIP Image encoder $\phi$ to extract both regions into embeddings. The background embeddings first go through the recovery transformer $\varphi$ to recover detailed information, then concatenated with foreground embeddings as final K and V inputs for split cross-attention. Similar to Eq \ref{split-attention}, the compose operation is a spatial piece-wise function employed to separate foreground and background.}
    \label{framework}
\vspace{-1em}
\end{figure*}


Inspired by our observation of real-world animation production, we propose an image-referenced sketch colorization framework. As is shown in Figure \ref{framework}, it leverages a sketch image $\bm{X_s} \in \mathbb{R}^{w_s \times h_s \times 1}$, a reference image $\bm{X_r} \in \mathbb{R}^{w_r \times h_r \times c}$ and a foreground mask $\bm{X_m} \in \mathbb{R}^{w_s \times h_s \times 1}$ as inputs, and returns the colorized result $\bm{Y} \in \mathbb{R}^{w_s \times h_s \times c}$, with $w$, $h$ and $c$ representing the width, height and channel of the images. All components of the framework are based on the animation production workflow with the following design:

%Firstly, to mimic the behavior that animators use character designs as the reference to colorize the sketches, we use a pre-trained vision transformer (ViT) to extract image embeddings and inject them into diffusion backbones 
Firstly, to mimic the animator's colorizing sketches with character designs as references, we utilize a pre-trained vision transformer (ViT) to extract image embeddings as reference information. The embeddings are later injected into the diffusion backbone with split cross-attention layers. Secondly, to integrate the sketches into the framework, we adopt a multi-layer sketch encoder to inject sketch information into the latent layers of the diffusion backbone as spatial guidance. Thirdly, based on the behavior of animators separately colorizing the foreground and background of the sketch, we propose a novel split cross-attention mechanism that uses spatial masks to separate the trainable LoRA modules corresponding to the keys and values for foreground and background for training. A switchable LoRA mechanism is then applied during inference for different application scenarios with different colorization modes. The implementation details are described in the supplementary materials.%separately colorize the foreground and the background.


%\vspace{-0.5em}
\subsection{Pretrain of the Diffusion Backbone}
%\vspace{-0.5em}
The backbone of the proposed framework consists of a pre-trained VAE, a sketch encoder, a denoising U-Net, and a pre-trained Vision Transformer (ViT) functioning as the image encoder from OpenCLIP-H \cite{RadfordKHRGASAM21,openclip,openclip-2,schuhmann2022laionb}. %The denoising U-Net is initialized using Waifu Diffusion v1.4 \cite{waifudiffusion}.
We denote sketch images, reference images, and ground truth as $s$, $r$, and $y$, respectively. The VAE encoder, U-Net, and ViT are represented by $\mathcal{E}$, $\theta$, and $\phi$, respectively. The timestep $t$ starts from $T-1$ and goes to $0$, where $T$ is the diffusion steps, set to 1000. The training objective of the diffusion model is to denoise the intermediate noisy image $z_t$ via noise prediction:

%The denoising U-Net and sketch encoder are pre-trained using the diffusion loss, which is formulated as
\vspace{-1em}
\begin{equation}
    \mathcal{L}(\theta)=\mathbb{E}_{\mathcal{E}(y),\epsilon,t,s,r}[\|\epsilon-\epsilon_{\theta}(z_{t},t,s,\phi(r))\|^{2}_{2}].
\end{equation}
\vspace{-1em}

%\textcolor{red}{The major difference between ours and ColorizeDiffusion \cite{yan2024colorizediffusion} in the pre-training stage is that, as the spatial entanglement is solved by the proposed method, we utilized a dynamic reference drop rate and slightly extended the training duration to pursue better image quality and similarity with references. We initialized VAE and U-Net with WaifuDiffusion \cite{waifudiffusion} and trained networks with the reference drop rate decreasing from 80\% to 50\% as training progresses to avoid the distribution issue \cite{yan2024colorizediffusion}. VAE, U-Net, and sketch encoder were then frozen during the training for the proposed method, as illustrated in Figure \ref{framework}.}

To pre-train the diffusion backbone, We initialize the VAE and U-Net with WaifuDiffusion \cite{waifudiffusion} and train networks with a dynamic reference drop rate decreasing from 80\% to 50\% as training progresses to avoid the distribution shift mentioned by \cite{yan2024colorizediffusion}. VAE, U-Net, and sketch encoder are then frozen during the training of the full framework. %as illustrated in Figure \ref{framework}.

%\vspace{-0.5em}
\subsection{Color Reference Extraction}
%\vspace{-0.5em}

Following the real-world animation production workflow, we utilize images as color references for sketch colorization. The commonly used ViT-based image encoder networks have two kinds of output embeddings: the CLS embeddings $\bm{E_{cls}} \in \mathbb{R}^{bs \times 1 \times 1024}$ and the local embeddings $\bm{E_{local}} \in \mathbb{R}^{bs \times 256 \times 1024}$, where $bs$ represent the batch size. The CLS embeddings are projected to CLIP embedding space for image-text contrastive learning, with spatial information compressed and connected to text-level notions, and are employed as color or style references by previous image-guided methods \cite{ip-adapter, instantstyle}. %, and can be inferred from zero-shot text-based interpolation using DALL-E-2 \cite{abs-2204-06125}. 
ColorizeDiffusion \cite{yan2024colorizediffusion}, on the contrary, reveals that local embeddings also express text-level semantics, indicating that they express more details regarding textures, strokes, and styles, enabling the network to generate better reference-based results, especially for transferring detailed textures and strokes. Therefore, the proposed method follows \cite{yan2024colorizediffusion} to adopt local tokens as color reference inputs for the framework.

However, the excessive spatial information of image semantics and compositions contained in local embeddings leads to frequent occurrences of artifacts such as overflow of color regions and unexpected objects outside sketches. Illustrated in Figure \ref{entanglement}, such artifacts widely exist in frameworks with image references \cite{ip-adapter,instantstyle,t2i-adapter}. To eliminate this problem, %which is called spatial entanglement in previous literature \cite{yan2024colorizediffusion}, 
we follow the real-world workflow to explicitly separate the foreground and background with spatial masks during colorization and describe the detail in \ref{split_cross_attention} and \ref{switchable_LoRA}.

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{figures/inference.pdf}
    \vspace{-2em}
    \caption{Based on the LoRA weights, the proposed method can merge the foreground and background features in one forward pass and switch between three inference modes. We denote the dimension of pre-trained weights as CH. The rank of foreground LoRA is fixed at 16, while the rank of background LoRA is 0.5*CH.}
    \label{mode}
    \vspace{-1.5em}
\end{figure}


\subsection{Split Cross-Attention}
\label{split_cross_attention}

In anime images, the foreground regions and background regions differ distinctively in color distribution, color block sizes, tones, and textures. Thus, the colorization of foreground and background is separated into two independent steps in the animation production workflow. Following this scheme, we propose a novel split cross-attention mechanism to substitute the cross-attention layers in the diffusion backbone to separately process foreground and background regions with different parameters in a single forward pass.

%There are two sets of trainable parameters in the proposed method: a six-layer recovery transformer $\varphi$ and foreground/background LoRA weights $W_{f}, W_{b}$ inside cross-attention modules, which include query weights $W_{f}^{q}, W_{b}^{q}$, key weights $W_{f}^{k}, W_{b}^{k}$, and value weights $W_{f}^{v}$ and $W_{b}^{v}$ for foreground and background QKV projection, respectively. An open-sourced segmentation tool for animation images \cite{anime-segmentation} is used to automatically extract the foreground mask $m_{s}$ and $m_{r}$ of sketches and reference images, respectively. Regions with pixel values larger than thresholds $ts_{s}$ and $ts_{r}$ are considered as foreground, otherwise background. 


A split cross-attention layer consists of two groups of trainable LoRA weights % $W_{f}$ that contains query weights $W_{f}^{q}$, key weights $W_{f}^{k}$ and value weights $W_{f}^{v}$ for the foreground QKV projection and $W_{b}$ that contains query weights $W_{b}^{q}$, key weights $W_{b}^{k}$ and value weights $W_{b}^{v}$ for the background respectively.  
$W_{f}^{t}$ and $W_{b}^{t}$, which include query weights $W_{f}^{q}$ and $W_{b}^{q}$, key weights $W_{f}^{k}$ and $W_{b}^{k}$, and value weights $W_{f}^{v}$ and $W_{b}^{v}$ for foreground and background QKV projection respectively. An open-sourced animation image segmentation tool \cite{anime-segmentation} is used to automatically extract the foreground mask $m_{s}$ and $m_{r}$ of sketches and reference images. Regions with pixel values larger than thresholds $ts_{s}$ and $ts_{r}$ are considered as foreground, otherwise background. For foreground LoRAs, we set the ranks as 16; for background LoRA, the rank is formulated as $r=0.5*min(D_{q}, D_{kv})$, where $D_{q}$ and $D_{kv}$ are dimensions of queries and keys/values for the corresponding cross-attention layers.

As foreground and background regions of animation images feature different textures, color distribution, and color tones, injecting the embeddings of foreground and background directly into split cross-attention results in deterioration in structure preservation, synthesis quality, and stylization. %Therefore, we train a group of lightweight LoRAs and a transformer to recover this loss. 
Therefore, we further add a trainable recovery transformer $\varphi$ to process the background embeddings and facilitate better integration of the foreground and background reference information into the diffusion backbone.%reference information into the diffusion backbone.


We define query inputs (forward features) as $\bm{z}_{f}$, $\bm{z}_{b}$, key and value inputs (reference embeddings) as $\bm{e}$, $\bm{e}_{b}$, attention outputs as $\bm{y}$ in the following sections, where the index $f$ and $b$ indicate foreground and background respectively. Specifically, $\bm{e}$ denotes the reference embeddings extracted from the whole reference image $r$, formulated as $\bm{e}=\phi(\bm{r})$; and $\bm{e}_{b}=\varphi(\phi(\bm{r}_{b}))$, where $\bm{r}_{b}$ is the background region of the reference image. During training, the proposed split cross-attention can be formulated as follows:

\vspace{-1em}
\begin{equation}
\begin{small}
    \bm{y} = \begin{cases}
    \mathbf{Softmax}(\frac{(\hat{W}_{f}^{q}\bm{z}_{f})\cdot(\hat{W}_{f}^{k}\bm{e})}{d})(\hat{W}_{f}^{v}\bm{e}) & \text{if $\bm{m_{s}} > ts_{s}$}\\
    \mathbf{Softmax}(\frac{(\hat{W}_{b}^{q}\bm{z}_{b})\cdot(\hat{W}_{b}^{k}\bm{e}_{b})}{d})(\hat{W}_{b}^{v}\bm{e}_{b}) & \text{if $\bm{m_{s}}\leq ts_{s}$}
    \end{cases}
    \label{split-attention}
\end{small}
\end{equation}
\vspace{-0.5em}


where $\hat{W}_{f}^{t} = W^{t} + W_{f}^{t}$, and $W^{t}$ represents the pre-trained weights, which remain frozen during training. Similarly, $\hat{W}_{b}^{t}$ follows the same approach. 

%Foreground and background LoRAs have different ranks. For foreground LoRAs, we set their ranks to 16, while for background LoRA, the rank is formulated as $r=0.5*min(D_{q}, D_{kv})$, where $D_{q}$ and $D_{kv}$ are the channel size of query and key/value at corresponding cross-attention, respectively.


\subsection{Switchable inference mode}
\label{switchable_LoRA}

The application scenarios and the sketch-reference combinations in the real world may be complicated. Also, naively separating the foreground and background regions for colorization degrades the quality of background synthesis, especially when reference images have severe semantic mismatches with the sketches or have complicated backgrounds. Therefore, we design three different inference modes: \textit{Vanilla}, \textit{Bg2Fig}, and \textit{Fig2Fig} for different scenarios based on the weights and reference inputs used for KV calculation. We visualize all inference modes in Figure \ref{mode}.
%Therefore, we design different inference modes depending on the application scenario. There are three inference modes based on the weights and reference inputs used for KV calculation: vanilla mode, figure-based mode, and background-based mode. In the following sections, we denote them as \textit{Vanilla}, \textit{Bg2Fig}, \textit{Fig2Fig}, respectively. Only \textit{Fig2Fig} mode is mask controlled, which is illustrated in Figure \ref{mode}.

\textit{Vanilla} mode %is the baseline mode that 
only utilizes the pre-trained weights for cross-attention modules, with $W_{f}^{t}$, $W_{b}^{t}$ and recovery transformers deactivated. It's suitable for most scenarios but suffers from spatial entanglement when references are figure-only images.% It utilizes the pre-trained weights for cross-attention modules, so its results are similar to ColorizeDiffusion \cite{yan2024colorizediffusion}.

\textit{Bg2Fig} mode activates only the foreground-related LoRA weights $W_{f}^{t}$ during inference and is used when reference images are figure images with complicated backgrounds. This mode outperforms \textit{Vanilla} mode in character colorization and \textit{Fig2Fig} mode in background generation as its foreground weights are further optimized by LoRAs. 

\textit{Fig2Fig} is designed for figure-to-figure colorization, where reference images are figures with simple background composition. This mode activates both LoRA weights $W_{f}^{t}$, $W_{b}^{t}$, and the recovery transformer and uses spatial masks to separate foreground/background embeddings for calculating query and key/value inputs. % with the behavior same as during the training. 
It effectively eliminates the spatial entanglement in reference-based sketch colorization. 