\section{Related Work}

\subsection{Latent Diffusion Models}

Diffusion Probabilistic Models \cite{HoJA20,0011SKKEP21} are a class of latent variable models inspired by nonequilibrium thermodynamics \cite{Sohl-DicksteinW15} and have achieved great success in image synthesis and editing. Compared to Generative Adversarial Networks (GANs)\cite{GoodfellowPMXWOCB14,KarrasLA19,KarrasLAHLA20,ChoiCKH0C18,ChoiUYH20}, Diffusion Models excel at generating highly realistic images with various contexts and able to be controlled by text prompts. However, the autoregressive denoising process of diffusion models, typically computed with a U-Net \cite{RonnebergerFB15} or a Diffusion Transformer (DiT) \cite{DiT,pixart}, incurs substantial computational costs. 

To address this limitation, Stable Diffusion (SD) \cite{RombachBLEO22,sdxl} as a class of Latent Diffusion Models (LDMs) was proposed, where a two-stage synthesis mechanism was adopted to enable diffusion/denoising process to be performed on a highly compressed latent space with a pair of pre-trained Variational Autoencoder (VAE), so as to significantly reduce computational costs. Concurrently, different researches of diffusion samplers have been conducted and proved to be effective in accelerating the denoising process \cite{SongME21,0011SKKEP21,0011ZB0L022,abs-2211-01095}. We adopt SD as our neural backbone, utilize the DPM++ solver \cite{abs-2211-01095,0011SKKEP21,KarrasAAL22} as the default sampler, and employ classifier-free guidance \cite{DhariwalN21,abs-2207-12598} to strengthen the reference-based performance.
 

\subsection{Image Prompted Diffusion Models}

 Currently, most diffusion models for image synthesis tasks are based on text prompts \cite{RombachBLEO22,sdxl, DiT,pixart}. However, there are tasks where text prompts can not provide enough information to precisely guide the image synthesis and editing, such as image-to-image translation \cite{KwonY23}, style transfer \cite{instantstyle, zhang2023inversion}, colorization \cite{animediffusion, yan2024colorizediffusion} and image composition \cite{zhang2023controlcom, kim2023reference}, and thus images are also used as prompts to provide reference information. The reference information extracted from prompt images varies from tasks: style transfer tasks adopt the textures and colors from reference images, image composition tasks focus more on the object-related information, and sketch colorization requires all the above.
 
There are two common practices to combine image prompts with diffusion models. Given image embedding vectors extracted by pre-trained feature extraction networks, existing methods either train an adapter module to inject the reference embedding vector into the backbone \cite{ip-adapter,t2i-adapter} or directly inject reference information into the backbone with attention layers \cite{hu2023animateanyone}. However, both of them \cite{ip-adapter,t2i-adapter,hu2023animateanyone} may introduce loss or mismatch of structure when inputs are not well paired, resulting in performance deterioration. In the sketch colorization task, specifically, these adapters provide conflicting spatial information from references and lead to unacceptable artifacts, as illustrated in Figure \ref{entanglement}.

 %The common practice to combine image prompts with diffusion models is to train a adapter module to inject image embedding vectors of reference images extracted by pre-trained feature extraction networks into the diffusion backbones.


\subsection{Sketch Colorization}

Sketch colorization has been a long-standing topic in computer vision. Interactive optimization-based method \cite{SykoraDC09} was employed for the task, and deep-learning-based methods \cite{ZhangLW0L18, KimJPY19, li2022eliminating, animediffusion, yan2024colorizediffusion} later became the mainstream due to the ability to synthesize high-quality and high-resolution images. There are three main technical solutions for deep learning methods: text-prompted sketch colorization \cite{KimJPY19,yan-cgf,controlnet-iccv}, user-guided sketch colorization \cite{ZhangLW0L18,s2pv5} and image reference sketch colorization \cite{li2022eliminating,yan-cgf,animediffusion}. User-guided methods can precisely colorize given sketches with detailed guidance from users, but the manual labor needed makes them unsuitable to be integrated into an automatic workflow. Text-prompted methods have received great popularity over recent years due to the development of Text-to-Image diffusion models, but it is challenging to precisely control colors, textures, and styles using text prompts. %which is also a defect of user-guided methods. 
Image-referenced methods also benefit from the development of diffusion models, along with relevant works that enable image control \cite{controlnet-iccv,controllllite,controlnet-v11,t2i-adapter,ip-adapter}. However, the mismatch of reference images and sketches still results in severe deterioration. ColorizeDiffusion \cite{yan2024colorizediffusion} achieved notable progress in the colorization quality, yet it still suffers from spatial entanglement, which is shown in Figure \ref{entanglement}. In this paper, we base our method on the production pipeline of animation studios to use image references to guide colorization. We separate the foreground and background with an innovative switchable LoRA to improve colorization performance and prevent artifacts.

