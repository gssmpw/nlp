\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{figures/ablation.pdf}
    \vspace{-1.5em}
    \caption{%Split cross-attention with mask guidance effectively solves spatial entanglement but degrades the color saturation and background generation. We employ switchable LoRAs to address this deterioration.
    %Results of ablation study. The baseline model causes severe spatial entanglement; adding split cross-attention reduces artifacts, and the proposed full pipeline can synthesize high-quality colorization results free from artifacts.
    Results of the ablation study. The baseline model demonstrates significant spatial entanglement; incorporating split cross-attention reduces artifacts, the trainable LoRAs improve color saturation and details, and the proposed complete pipeline produces high-quality results free of artifacts. Zoom in for details.}
    \label{ablation}
    \vspace{-1em}
\end{figure}

\section{Experiment}
\subsection{Implementation details}
%Our training dataset contains over 4.8 million triples of (sketch, color, mask) images with various contents, and our validation set consists of 52,000 triples. All validation data are excluded from the training set. The original images are from Danbooru2021 \cite{danbooru2021}. We extracted sketches by jointly using \cite{sketchKeras} and \cite{xiang2022adversarial}.
We used Danbooru2021 dataset \cite{danbooru2021} to train and validate the proposed method. The training set contains over 4.8 million triples of (sketch, color, mask) images with various contents, and the validation set consists of 52,000 triples, with all the data excluded from the training set. Sketches were extracted by jointly using \cite{sketchKeras} and \cite{xiang2022adversarial}.
We first trained the denoising U-Net and sketch encoder using the dataset for 6 epochs and then froze the backbone and trained the recovery transformer and switchable LORAs on the dataset for 3 epochs. The training was conducted on 4x H100 (94GB) using Deepspeed ZeRO2 \cite{deepspeed} and the AdamW optimizer \cite{KingmaB14, LoshchilovH19} with the learning rate set to $0.0001$ and betas set to $(0.9, 0.999)$. Following \cite{yan2024colorizediffusion}, we dropped at least 50\% of the reference inputs in all training.

\subsection{Ablation study}
\noindent\textbf{Split cross-attention.} The proposed method aims to address spatial entanglement by simulating the animation workflow. To demonstrate the effectiveness of this workflow, we set up three frameworks: 1) baseline model without split cross attention, trainable LoRAs, and recovery transformer, 2) baseline model with split cross attention but no trainable LoRAs and recovery transformer, and 3) the proposed full framework. %1) The proposed method; 2) Split cross-attention without trainable LoRAs and recovery transformer; and 3) No split cross-attention.

We show the qualitative comparison in Figure \ref{ablation} to validate the effectiveness of the proposed modules.%split cross-attention mechanism and switchable LoRAs. 
The baseline model causes severe spatial entanglement in generating additional figures in (a) and undesired clothes in (b). The application of split cross-attention mitigates the spatial entanglement but still causes artifacts and degrades the color saturation and details of the results. Collaborating split cross attention with trainable LoRAs improves the quality of results and further improves the background, but still suffers from artifacts. The proposed full framework enhanced by recovery transformers effectively eliminates spatial entanglement and synthesizes colorization results that have clear boundaries and rich details and textures, and loyally preserves the color distribution of reference images.

\noindent\textbf{Inference modes.} We illustrate the differences between inference modes and their use cases in Figure \ref{ablation2}. \textit{Fig2Fig} mode is fully mask-guided, enabling it to eliminate spatial entanglement shown in (a). However, it is less suitable for inpainting character sketches, as the region without the mask guide may suffer from the lack of reference information. \textit{Bg2Fig} performs similarly to the \textit{Vanilla} mode. With the help of foreground LoRA weight, the results demonstrate clearer segmentation and better stroke quality. All modes perform well for landscape sketches, which contain more structural detail than figure sketches, making them easier to colorize.

%\noindent\textbf{Color Reference Extraction.} 
We show the comparison of local embeddings and CLS embeddings for \noindent\textbf{color reference extraction} in the supplementary materials.

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{figures/ablation2.pdf}
    \vspace{-1.5em}
    \caption{Colorization results with three different inference modes. \textit{Fig2Fig} mode performs better in eliminating spatial entanglement, while \textit{Bg2Fig} and \textit{Vanilla} mode can generate vivid backgrounds and inpainting results.}
    \label{ablation2}
    \vspace{-1em}
\end{figure}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.98\linewidth]{figures/comparison.pdf}
     \vspace{-1em}
    \caption{Qualitative comparisons between our proposed method and existing methods show that our results are visually more appealing than those of image-prompt adapters \cite{ip-adapter, instantstyle, t2i-adapter} and the GAN-based method \cite{yan-cgf}. Compared to ColorizeDiffusion \cite{yan2024colorizediffusion}, the proposed framework eliminates spatial entanglement and improves overall quality.}
    \label{qualitative}
     \vspace{-1em}
\end{figure*}



\subsection{Comparison with baseline}

We compare our method with six existing reference-based sketch colorization methods \cite{yan-cgf,animediffusion,yan2024colorizediffusion} to demonstrate the superiority of the proposed framework.\\

\vspace{-0.5em}
\noindent\textbf{Baseline introduction.} Two baseline methods are the combination of SD \cite{RombachBLEO22}, ControlNet \cite{controlnet-iccv,controlnet-v11}, and IP-Adatper \cite{ip-adapter}. They adopt different cross-attention scales during denoising, labeled as \textit{IP-Adapter} and \textit{InstantStyle}, respectively. \textit{IP-adapter} baseline generates results with normal cross-attention scales, while \textit{InstantStyle} generates results using the ``style transfer'' weight type, which is claimed to prevent composition transfer by setting specific cross-attention scales to 0 according to \cite{instantstyle}. Following the official document of \cite{controlnet-v11}, we adopted Anything v3 \cite{anything}, a community-developed model, as the SD backbone for IP-Adapter-H and ControlNet\_lineart\_anime. All these models are officially implemented and claimed to be effective for anime-style image generation. The \textit{T2I-Adapter} baseline simply replaces IP-Adapter with T2I-Adapter-Style \cite{t2i-adapter,t2i-adapter-code}. Specifically, we introduced quality-related prompts and textual inversion, such as ``masterpiece'' and ``easynegative'' \cite{easynegative}, for T2I-model baselines to improve their image quality.\\

\vspace{-0.5em}
\noindent\textbf{Qualitative comparison.} We show the qualitative comparison of our proposed method and existing methods in Figure \ref{qualitative}, where most of the existing methods suffer from spatial entanglement in various cases. %We first evaluate the qualitative performance. As can be observed in Figure \ref{qualitative}, most baseline methods suffer from spatial entanglement in various cases. 
In rows (a)-(d), existing methods (2)-(6) failed to distinguish foreground and background regions and, therefore, synthesized artifacts in the background. Such artifacts become more obvious when reference images have complicated backgrounds in rows (e) and (f), where IP-Adapter and InstantStyle mixed reference composition with the sketch composition, making the generated results visually messy. The GAN-based method (7) successfully generated results with clear backgrounds, but the color preservation, texture transformation, and detail qualities are much poorer than diffusion-based methods. 

%The other comparison is given in Figure \ref{qualitative2} to show the advantage of adopting local tokens instead of using the CLS embedding and text embeddings as the medium to introduce image prompts, where baseline methods even failed to colorize the character depending on the color scheme of the landscape image in row (c). By adopting local tokens, the proposed method can transfer styles, textures, and strokes from references to sketches without requiring references to have corresponding semantics and identities.

We also show the comparison of our proposed method and adapter-based methods on semantically non-relevant sketch-reference pairs in Figure \ref{qualitative2}. Both IP-Adapter and T2I-Adapter fail to separate foreground from background and colorize the whole image with the same tone and texture. 
Our proposed method, on the contrary, generated results with clear region boundaries, rich texture and details, and visually pleasant bright color, with the help of the proposed spatial aware split cross-attention mechanism and switchable LoRA. The qualitative comparisons demonstrate that our proposed method is effective for arbitrary input sketch-reference pairs and achieves high-quality colorization in various use scenarios. 


\begin{table*}[t]
    \centering
    \caption{Quantitative comparison between the proposed model and baseline methods. We calculated 50K FID, 5K PSNR, 5K MS-SSIM, and 5K CLIP cosine similarity of image embeddings in this experiment. \dag: FID evaluation randomly selected color images as references, making it close to real-application scenarios. \ddag: Ground truth color images were deformed to obtain semantically paired and spatially similar references for evaluations.}
    \vspace{-0.5em}
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
        \hline
        \multicolumn{5}{|c|}{Method} & {\dag 50K-FID $\downarrow$} & {\ddag PSNR$\uparrow$} & {\ddag MS-SSIM$\uparrow$} & {\ddag CLIP similarity$\uparrow$}\\
        \hline
        \multicolumn{5}{|c|}{Ours} & \textbf{6.8327} & 28.9144 & \textbf{0.6002} & \textbf{0.8829} \\
        \hline
        \multicolumn{5}{|c|}{\textit{ColorizeDiffusion} \cite{yan2024colorizediffusion}} & 9.5276 & 28.7384 & 0.5913 & 0.8775 \\
        \hline
	\multicolumn{5}{|c|}{\textit{IP-Adapter} \cite{controlnet-iccv,ip-adapter,anything}} & 38.9184 & 28.6767& 0.5478 & 0.8672 \\
        \hline
        \multicolumn{5}{|c|}{\textit{InstantStyle} \cite{controlnet-iccv,ip-adapter,instantstyle,anything}} & 40.8144 & 28.1090 & 0.4459 & 0.8042 \\
	\hline
        \multicolumn{5}{|c|}{\textit{T2I-Adapter} \cite{controlnet-iccv,t2i-adapter,anything}} & 41.1569 & 28.1275 & 0.3243 & 0.7180 \\
        \hline
        \multicolumn{5}{|c|}{\textit{AnimeDiffusion}\cite{animediffusion}} &  61.5999 & 27.8454 & 0.3185 & 0.7319\\
	\hline
        \multicolumn{5}{|c|}{Yan et al. \cite{yan-cgf}} & 27.0147 & \textbf{29.2483} & 0.5253 & 0.7634 \\
	\hline
	\end{tabular}
    \label{quantitative}
    \vspace{-1em}
\end{table*}


\noindent\textbf{Quantitative comparison.} Fr√©chet Inception Distance (FID) \cite{HeuselRUNH17} is a widely used quantitative creteria to evaluate the %quality of generated images objectively. 
performance of image synthesis tasks. It calculates the perceptual distance of %distributions between real and fake data 
two distributions without requiring them to be semantically and spatially paired. %This property allows us to simulate real-world application scenarios in our evaluation and, therefore, the most important metrics in the quantitative comparison. 
We conduct a quantitative evaluation measured by FID on the entire validation set which includes 52k+ (sketch, reference) image pairs. Sketches are colorized with randomly selected reference images, ensuring each batch has semantically and spatially mismatched inputs.

Multi-scale structural similarity index measure (MS-SSIM), peak signal-to-noise ratio (PSNR), and CLIP score \cite{RadfordKHRGASAM21,openclip} assess the similarity between processed images and given ground truth. It requires the guiding reference to be aligned with the ground truth when applied to image-referenced sketch colorization. To fulfill this, we selected 5000 color images as ground truth, extracted the sketches from and used thin plate spline (TPS) transformation to spatially distort them as color references to build a test set of 5000 triples of sketch, reference, and ground truth, and evaluate all the 7 methods with MS-SSIM, PSNR and CLIP score on this dataset.  %As the sketches were extracted from ground truth color images, references are spatially similar to the sketch inputs, making their evaluations more likely to be image-guided reconstruction instead of real-application scenarios of reference-based sketch colorization. Thus, they only went through 5k triples of (sketch, reference, color) images. We used thin plate spline (TPS) transformation to deform ground truth color images as references in both evaluations.

We show the results of the quantitative comparison in Table \ref{quantitative}, where the proposed method significantly outperforms in FID, MM-SSIM, and CLIP similarity due to better texture and color synthesis. GAN-base method \cite{yan-cgf} achieved the best score in PSNR, with the proposed method ranked number 2. This is because the limited generation ability of \cite{yan-cgf} prevents it from generating complicated backgrounds and also hinders it from synthesizing bright colors and rich details of the figures. The close-to-average results make it advantageous to the calculation of PSNR \cite{blau2018perception}. %making the semantically results similar to ground truth. 
 %because it less generated artifacts due to its insufficient generation ability for such diverse inputs, observable in \ref{qualitative}. \\


\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{figures/comparison2.pdf}
    \vspace{-1em}
    \caption{Colorization results for semantically non-relevant sketch-reference pairs. Adapter-based methods fail to generate clear borders and high-quality textures, while our proposed method synthesizes visually satisfying results.}
    \label{qualitative2}
    \vspace{-1em}
\end{figure}




\noindent\textbf{User study.}
%We invited \textcolor{red}{t.b.d} participants to answer \textcolor{red}{t.b.d} questions based on 30 groups of generated results. 
The quality of sketch colorization is highly subjective and easily influenced by personal preference, we thus demonstrate how different individuals evaluate the proposed method and existing methods through an user study. 40 participants are invited to select the best results with two criteria: the overall colorization quality and the preservation of geometric structure of the sketches. We prepare 25 image sets and show each participant 16 image sets for evaluation.  
%Within each group, participants were asked to choose the result that exhibited the best stylization quality while effectively preserving the identity of the original person.
For each image set, participants are presented the colorization results of our proposed method as well as those generated by six existing methods. 

The results of the user study are presented in Figure \ref{userstudy}, where our proposed method has received the most numbers of preferences among all the methods presented. To further validate the comparison, we applied the Kruskal-Wallis test as a statistical method. The results clearly demonstrate that the proposed method significantly outperforms all the existing methods in terms of user preference with a significance level of p \textless 0.01. All the images shown in the user study are included in the supplementary materials.


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/user_study.pdf}
    \vspace{-1em}
    \caption{Results of user study. Our method is preferred across all shown methods in overall quality and geometric preservation.}
    \label{userstudy}
    \vspace{-1em}
\end{figure}