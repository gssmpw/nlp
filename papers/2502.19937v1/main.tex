\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage{cvpr}     
\usepackage{times}
\usepackage{color}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
 \usepackage{mathrsfs}
\usepackage{booktabs}
\usepackage{comment}

\usepackage{bm} 
\usepackage{algorithm}
\usepackage{tabularx}
\usepackage{algpseudocode}
\usepackage{subfloat}
\usepackage{verbatim}
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,            %链接颜色
    linkcolor=blue,             %内部链接
    filecolor=magenta,          %本地文档
    urlcolor=cyan,              %网址链接
    pdftitle={Overleaf Example},
    pdfpagemode=FullScreen,
    }
    

\def\cvprPaperID{1921}
\def\confName{CVPR}
\def\confYear{2025}


\begin{document}

%\title{Improve Image-Guided Sketch Colorization via Mimicking \\ Animation Creation Workflow}

\title{Image Referenced Sketch Colorization Based on Animation Creation Workflow}

\vspace{-1em}
\author{
*Dingkun Yan\textsuperscript{1}
\qquad
*Xinrui Wang\textsuperscript{2}
\\
Zhuoru Li\textsuperscript{3}
\qquad
Suguru Saito\textsuperscript{1}
\qquad
Yusuke Iwasawa\textsuperscript{2}
\qquad
Yutaka Matsuo\textsuperscript{2}
\qquad
Jiaxian Guo\textsuperscript{2}
\\
\textsuperscript{1}Institute of Science Tokyo
\qquad
\textsuperscript{2}The University of Tokyo
\qquad
\textsuperscript{3}Project HAT}
\vspace{-1em}
\twocolumn[{%
    \renewcommand\twocolumn[1][]{#1}%
    \maketitle
    \begin{center}
        \centering
        \includegraphics[width=0.98\linewidth]{figures/teaser.pdf}
        \vspace{-0.5em}
        \captionof{figure}{Given reference images, our proposed method automatically synthesizes high-quality sketch colorization results that loyally match the reference color distribution and are free from artifacts.} %Our method focuses on colorizing sketch images using reference images, especially anime-style images. Given a reference color image in fine-art styles, the proposed method can colorize sketches based on colors and textures extracted from the reference.}
        \label{teaserfigure}
        \vspace{-0.5em}
    \end{center}%
    }]
\maketitle


\def\thefootnote{*}\footnotetext{Represent equal contribution to this work}\def\thefootnote{\arabic{footnote}}


\setcounter{figure}{2}
\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/pipeline.pdf}
    \vspace{-2em}
    \caption{Illustration of colorization workflow in professional animation studios. A: character designers design characters as references. B: Senior animators draw the sketches for the key frames. C: animators colorize the figures in the sketches according to the character designs, and D: animators colorize the background of the sketches and merge foreground and background into finished frames.}
    \label{pipeline}
    \vspace{-1em}
\end{figure*}
\setcounter{figure}{1}

\begin{abstract}
\vspace{-1em}
% Our approach leverages the sketch as a spatial reference and an RGB image as a color reference. Using spatial masks, we separately extract foreground and background information from the reference image. We introduce a split cross-attention mechanism where LoRA (Low-Rank Adaptation) modules are trained separately to control the embeddings of foreground and background for keys and values in cross-attention. This design allows the diffusion model to integrate information from foreground and background independently, preventing interference and eliminating the need to fine-tune model parameters. During inference, we implement a switchable LoRA mechanism controlled by soft foreground and background masks, providing precise control over the colorization process.Extensive qualitative and quantitative experiments on benchmark colorization datasets, along with user studies, demonstrate that our approach significantly outperforms existing methods, particularly in generating artifact-free colorization results using geometric mismatch reference. Ablation studies further confirm the effectiveness of each component in our framework.

%Diffusion models have been widely applied to sketch colorization tasks to accelerate animation production due to their successes achieved on image synthesis. However, problems still exist that hints based methods requires manual assists and exemplar based methods result in severe artifacts. 

% We present a framework to colorize sketches from reference images. In professional animation studios, the sketch colorization workflow consists of the following key-steps: (1) character designers design the characters. (2) senior animators draw the sketch of each frame. (3) Animators colorize figures in the frame based on the character design. (4) Animators colorize the background and compose the whole frame. Motivated by this workflow, %we design a diffusion framework with components corresponding to each step: the framework takes a sketch as spatial reference and a RGB image as color reference. A novel split cross-attention together with spatial masks are applied to separate the foreground and background injected to diffusion models during training. During inference, a switchable LoRA mechanism is employed to control the embeddings injected to cross attention layers and change modes between foreground and background without tuning the model parameters. The proposed method  achieves precise control with image prompts and synthesize colorization results free from artifacts. 

%Reference-based sketch colorization is critical for applications such as animation and digital illustration production, where maintaining consistent colorization across frames is essential. 
Sketch colorization plays an important role in animation and digital illustration production tasks. %However, existing methods often suffer from spatial mismatches between reference images and sketches, causing unintended transfer of spatial structures and leading to distorted colorized outputs.
However, existing methods still meet problems in that text-guided methods fail to provide accurate color and style reference, hint-guided methods still involve manual operation, and image-referenced methods are prone to cause artifacts. To address these limitations, we propose a diffusion-based framework inspired by real-world animation production workflows. % which is consisted of the following key-steps: (1) character designers design the characters. (2) senior animators draw the sketch of each frame. (3) Animators colorize figures in the frame based on the character design. (4) Animators colorize the background and compose the whole frame. 
Our approach leverages the sketch as the spatial guidance and an RGB image as the color reference, and separately extracts foreground and background from the reference image with spatial masks. Particularly, we introduce a split cross-attention mechanism with LoRA (Low-Rank Adaptation) modules. They are trained separately with foreground and background regions to control the corresponding embeddings for keys and values in cross-attention. This design allows the diffusion model to integrate information from foreground and background independently, preventing interference and eliminating the spatial artifacts. During inference, we design switchable inference modes %assisted by spatial masks, that provides different inference modes 
for diverse use scenarios by changing modules activated in the framework. Extensive qualitative and quantitative experiments, along with user studies, demonstrate our advantages over existing methods in generating high-qualigy artifact-free results with geometric mismatched references. Ablation studies further confirm the effectiveness of each component. Codes are available at \url{https://github.com/tellurion-kanata/colorizeDiffusion}.
%\newpage

%Despite the powerful image generation capabilities of diffusion models, existing diffusion based sketch colorization methods are facing problems in controllability: texts prompts are naturally defective in passing accurate information of spatial layout and richer clues on colors, textures and tones to the backbone model, while image prompts usually exhibit different spatial structures and semantics to sketches, causing various artifacts and severe degrades on the colorization results. To address these issues, we present a novel sketch colorization method to improve the controllability of image-prompted sketch colorization. Specifically, we apply spatial control on different regions with adaptive spatial masks, and proposed switchable LoRA to control the embeddings injected to cross attention layers, and can change modes between foreground and background without tuning the model parameters. The proposed method  achieves precise control with image prompts and synthesize colorization results free from artifacts. Extensive qualitative and quantitative experiments, together with user studies, demonstrate our advantages over existing methods. Ablation studies further confirm the effectiveness of each component.


\end{abstract}



\input{documents/introduction}
\input{documents/related}
\input{documents/method}
\input{documents/experiment}
\input{documents/conclusion}


\newpage

{\small
\bibliographystyle{ieee_fullname}
\bibliography{main}
}

\end{document}



