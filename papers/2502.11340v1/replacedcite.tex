\section{Related Works}
The field of time series forecasting has seen significant evolution over the decades: shifting from classical mathematical tools ____ and statistical techniques like ARIMA ____ to more recent deep learning approaches such as recurrent neural networks ____ and long-short term memory models ____. Notably, in recent years, transformers ____ have demonstrated particularly promising performance on sequence modeling tasks, especially in natural language processing. Interestingly, studies have revealed that even simple linear layers can outperform complex transformer-based models in both performance and efficiency for time series forecasting ____.

% Making the subsections shorter so the related work can contain more paper
\textbf{Inverted Dimension.}
In investigating why transformers underperform in time series forecasting, ____ argues that the direct application of transformers that embed all variates is undesirable. This embedding compresses variates with distinct physical meanings and inconsistent measurement at each time step to a single token, erasing the important multivariate correlations. To address this limitation, the authors propose inverting the dimension of time and variates in the data while preserving the core mechanisms of the transformer. Many subsequent studies ____ build upon this paradigm, achieving improvements in both performance and efficiency. %This innovative approach effectively unlocks the potential of transformers for time series forecasting.
% Building on this innovative paradigm, subsequent works further refined the approach by replacing the Transformer with Mamba-based models, a promising alternative that offers linear training complexity.

\textbf{Patchification.}
Patchification of inverted data transforms the time series of each variate into a multivariate time sequence where the patches are stacked to construct an additional dimension. While patchification facilitates the capture of temporal dependencies by introducing an inductive bias aligned with the localized nature of time series data, it also overlooks the between-variate correlations due to the additional dimension: existing approaches, such as SST ____ and PatchTST ____, treat each variate independently. Despite their strong performance, these methods lack any form of inter-variate communication. 
% Taking this out for now, maybe we can use this in 4.2? 
%Other methods like MOIRAI flatten all variables into a single sequence before patchification, but the extended sequence length, particularly in high-dimension, in addition to the quadratic complexity of the transformer, imposes a heavy computation burden.

\textbf{Mixture of Experts.}
The mixture of experts method receives increasing attention in sequence modeling after the release of Mamba ____. Combining the linear complexity of Mamba and the strong performance of transformers could lead to efficient and accurate sequence models. For instance, Jamba ____ employs a layerwise stacking of Mamba and attention layers, achieving superior performance in natural language processing compared to either component individually. For time series forecasting, SST ____ utilizes Mamba to capture global patterns with prolonged patch lengths, while leveraging transformer to learn local details with shorter patch lengths. However, global and local patches are processed separately through each expert before their output embeddings are concatenated. Such inadequate communication between global and local features limits the integration results, restricting the model's ability to fully exploit each expert's complementary strength. 

\begin{figure*}[t]
    \centering
    \includegraphics[width=1\textwidth]{./images/structure.pdf} % Change to your image file
    \caption{Overview of the proposed architecture S2TX. Different variables (in different colors) of the time series are patched into global and local patches. The global patches are processed by the global model, which outputs the global context that is used to compute the key and value matrices during cross-attention with the local model. Skip connections and normalization layers are omitted for clarity of presentation.}
    \label{fig:structure}
\end{figure*}
%\textbf{Our Approach}
%\HM{Maybe we can take this out, seems redundant.}
%As we clarified the different aspects of multivariate time series forecasting, the goal is clear: with S2TX, we develop an architecture that combines the advantages of all three approaches. S2TX is a low memory architecture that maintains the learning of multivariate correlation while enabling global-local feature interplay through a cross-attentional mixture of experts.