\section{Proof of Proposition 3.1}
\label{sec: appendix proof prop 1}
\noindent\textbf{Proposition 3.1} 
\textit{
    (Incorporation of diffusion process into VAE)\textbf{.}
    By introducing a sequence of hidden variables $\mathbf{x}_{1:T}$, under the setup of conditional diffusion models where the Markov Chain assumption is employed on the forward process $q(\mathbf{x}_{1:T}|\mathbf{x}_0)\coloneq\prod_{t=1}^Tq(\mathbf{x}_t|\mathbf{x}_{t-1})$ and the reverse process $p_\theta(\mathbf{x}_{0:T}|\mathbf{y})\coloneq p(\mathbf{x}_T)\prod_{t=1}^Tp_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t,\mathbf{y})$ parameterized by a DDPM $\theta$, and assuming that $\mathbf{x}_T=\boldsymbol{\epsilon}$ (i.e., the latent noise of DDPM samples from the VAE latent distribution), we have the lower bound on $\log p(\mathbf{x}_0|\mathbf{y},\boldsymbol{\epsilon})$ in the reconstruction term of the VAE as:
    \begin{equation*}
        \log p_\theta(\mathbf{x}_0|\mathbf{y},\boldsymbol{\epsilon})\geq\mathbb{E}_{q(\mathbf{x}_{1:T}|\mathbf{x}_0)}\left[\log\frac{p_\theta({\mathbf{x}_{0:T}}|\mathbf{y})}{q(\mathbf{x}_{1:T}|\mathbf{x}_0)}\right],
    \end{equation*}
    which is the ELBO of the conditional DDPM. 
}

\vspace{0.25cm}

\textit{Proof:}

We lower bound the conditional data log-likelihood $\log p_\theta(\mathbf{x}_0|\mathbf{y},\boldsymbol{\epsilon})$ by utilizing a sequence of hidden latent representations $\mathbf{x}_{1:T}$ and the approximate variational distribution $q(\mathbf{x}_{1:T}|\mathbf{x}_0)$:
\begin{equation}
\begin{aligned}
    \log p_\theta(\mathbf{x}_0|\mathbf{y},\boldsymbol{\epsilon})=&\log\int p_\theta(\mathbf{x}_{0:T}|\mathbf{y},\boldsymbol{\epsilon})d\mathbf{x}_{1:T} \\
    =&\log\int \frac{p_\theta(\mathbf{x}_{0:T}|\mathbf{y},\boldsymbol{\epsilon})q(\mathbf{x}_{1:T}|\mathbf{x}_0)}{q(\mathbf{x}_{1:T}|\mathbf{x}_0)}d\mathbf{x}_{1:T} \\
    =&\log\mathbb{E}_{q(\mathbf{x}_{1:T}|\mathbf{x}_0)}\left[\frac{p_\theta({\mathbf{x}_{0:T}}|\mathbf{y},\boldsymbol{\epsilon})}{q(\mathbf{x}_{1:T}|\mathbf{x}_0)}\right] \\
    \geq&\mathbb{E}_{q(\mathbf{x}_{1:T}|\mathbf{x}_0)}\left[\log\frac{p_\theta({\mathbf{x}_{0:T}}|\mathbf{y},\boldsymbol{\epsilon})}{q(\mathbf{x}_{1:T}|\mathbf{x}_0)}\right],
\label{eq: cddpm lower bound appendix}
\end{aligned}
\end{equation}
where the inequality is obtained by applying Jensen’s inequality.

The reverse diffusion process incorporating $\boldsymbol{\epsilon}$ is given as:
\begin{equation}
\begin{aligned}
    p_\theta(\mathbf{x}_{0:T}|\mathbf{y},\boldsymbol{\epsilon})\coloneq & p(\mathbf{x}_T)\prod_{t=1}^Tp_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t,\mathbf{y},\boldsymbol{\epsilon}) \\
    =&p(\mathbf{x}_T)\prod_{t=1}^Tp_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t,\mathbf{y},\mathbf{x}_T) \\
    =&p(\mathbf{x}_T)\prod_{t=1}^Tp_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t,\mathbf{y})\eqcolon p_\theta(\mathbf{x}_{0:T}|\mathbf{y}),
\end{aligned}
\end{equation}
by using the assumption that the diffusion prior samples from the VAE latent space, i.e., $\mathbf{x}_T=\boldsymbol{\epsilon}$, and utilizing the Markov Chain property on the reverse transition probability to remove the $\mathbf{x}_T$ as a condition. This indicates that:
\begin{equation}
    \mathbb{E}_{q(\mathbf{x}_{1:T}|\mathbf{x}_0)}\left[\log\frac{p_\theta({\mathbf{x}_{0:T}}|\mathbf{y},\boldsymbol{\epsilon})}{q(\mathbf{x}_{1:T}|\mathbf{x}_0)}\right]=\mathbb{E}_{q(\mathbf{x}_{1:T}|\mathbf{x}_0)}\left[\log\frac{p_\theta({\mathbf{x}_{0:T}}|\mathbf{y})}{q(\mathbf{x}_{1:T}|\mathbf{x}_0)}\right].
\label{eq: cddpm lower bound appendix 2} 
\end{equation}

Together, (\ref{eq: cddpm lower bound appendix}) and (\ref{eq: cddpm lower bound appendix 2}) lead to the result in Proposition \ref{proposition: 1}.



\section{Derivation of Proposition 3.2}
\label{sec: appendix proof prop 2}
\noindent\textbf{Proposition 3.2} (RestoreGrad)\textbf{.}\textit{
Assume the prior and posterior distributions are both zero-mean Gaussian, parameterized as $p_{\psi}(\boldsymbol{\epsilon}|\mathbf{y})=\mathcal{N}(\boldsymbol{\epsilon};\mathbf{0},\boldsymbol{\Sigma}_{\text{prior}}(\mathbf{y};\psi))$ and $q_{\phi}(\boldsymbol{\epsilon}|\mathbf{x}_0, \mathbf{y})=\mathcal{N}(\boldsymbol{\epsilon};\mathbf{0},\boldsymbol{\Sigma}_{\text{post}}(\mathbf{x}_0,\mathbf{y};\phi))$, respectively, where the covariances are estimated by the Prior Net $\psi$ (taking $\mathbf{y}$ as input) and Posterior Net $\phi$ (taking both $\mathbf{x}_0$ and $\mathbf{y}$ as input). Let us simply use $\boldsymbol{\Sigma}_{\text{prior}}$ and $\boldsymbol{\Sigma}_{\text{post}}$ hereafter to refer to $\boldsymbol{\Sigma}_{\text{prior}}(\mathbf{y};\psi)$ and $\boldsymbol{\Sigma}_{\text{post}}(\mathbf{x}_0,\mathbf{y};\phi)$ for concise notation. 
Then, with the direct sampling property in the forward path $\mathbf{x}_t=\sqrt{\bar{\alpha}_t}\mathbf{x}_0+\sqrt{1-\bar{\alpha}_t}\boldsymbol{\epsilon}$ at arbitrary timestep $t$ where $\boldsymbol{\epsilon}\sim q_{\phi}(\boldsymbol{\epsilon}|\mathbf{x}_0,\mathbf{y})$, and assuming the reverse process has the same covariance as the true forward process posterior conditioned on $\mathbf{x}_0$, by utilizing the conditional DDPM $\boldsymbol{\epsilon}_\theta(\mathbf{x}_t,\mathbf{y},t)$ as the noise estimator of the true noise $\boldsymbol{\epsilon}$, we have the modified ELBO associated with (\ref{eq: vae elbo}): 
\begin{equation*}
\begin{aligned}
    -ELBO  =& \underbrace{\frac{\bar{\alpha}_T}{2}\mathbb{E}_{\mathbf{x}_0}\norm{\mathbf{x}_0}^2_{\boldsymbol{\Sigma}^{-1}_{\text{post}}}+\frac{1}{2}\log\abs{\boldsymbol{\Sigma}_{\text{post}}}}_{\text{Latent Regularization (LR) terms}}
    +\underbrace{\sum_{t=1}^{T}\gamma_t\mathbb{E}_{(\mathbf{x}_0,\mathbf{y}),\boldsymbol{\epsilon}\sim \mathcal{N}(\mathbf{0},\boldsymbol{\Sigma}_{\text{post}})}\norm{\boldsymbol{\epsilon}-\boldsymbol{\epsilon}_\theta(\mathbf{x}_t,\mathbf{y},t)}^2_{\boldsymbol{\Sigma}^{-1}_{\text{post}}}}_{\text{Denoising Matching (DM) terms}} \\
    &+\underbrace{\frac{1}{2}\bigr(\log\frac{\abs{\boldsymbol{\Sigma}_{\text{prior}}}}{\abs{\boldsymbol{\Sigma}_{\text{post}}}}+\text{tr}(\boldsymbol{\Sigma}_{\text{prior}}^{-1}\boldsymbol{\Sigma}_{\text{post}})\bigr)}_{\text{Prior Matching (PM) terms}} + \, \text{C},
\end{aligned}
\end{equation*}
where $\gamma_t= \begin{cases} \frac{\beta_t^2}{2\sigma_t^2\alpha_t(1-\bar{\alpha}_t)}, & t>1 \\ \frac{1}{2\alpha_1}, & t=1\end{cases}$ are weighting factors, $\norm{\mathbf{x}}^2_{\boldsymbol{\Sigma}^{-1}}=\mathbf{x}^T\boldsymbol{\Sigma}^{-1}\mathbf{x}$, $\sigma_t^2=\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha_t}}\beta_t$ and $C$ is some constant not depending on learnable parameters $\theta$, $\phi$, $\psi$.some constant not depending on the learnable parameters $\theta$, $\phi$, and $\psi$.
}

\vspace{0.25cm}

\textit{Derivation:}

Recall our proposed lower bound in (\ref{eq: vae elbo}) to incorporate the conditional DDPM into the VAE framework is given as:
\begin{equation}
    \mathbb{E}_{q_{\phi}(\boldsymbol{\epsilon}|\mathbf{x}_0,\mathbf{y})}\Biggr[\underbrace{\mathbb{E}_{q(\mathbf{x}_{1:T}|\mathbf{x}_0)}\left[\frac{p_{\theta}(\mathbf{x}_{0:T}|\mathbf{y})}{q(\mathbf{x}_{1:T}|\mathbf{x}_0)}\right]}_{-\mathcal{L}(\theta,\phi)}\Biggr] - D_{\text{KL}}\bigr(q_{\phi}(\boldsymbol{\epsilon}|\mathbf{x}_0,\mathbf{y}) || p_{\psi}(\boldsymbol{\epsilon}|\mathbf{y})\bigr).
\label{eq: rg elbo appendix}
\end{equation}

Note that as assumed in standard DDPMs, the forward diffusion process gradually corrupts the data distribution into the prior distribution, which can be achieved by carefully designing the variance schedule for the forward pass, i.e., $\{\beta_t\}_{t=1}^T$, such that $\mathbf{x}_T\to\boldsymbol{\epsilon}$ (as a result of $\bar{\alpha}_T\to0$). More specifically, the 
$q(\mathbf{x}_T|\mathbf{x}_0)$ of the forward diffusion process converges in distribution to the approximate posterior $q_{\phi}(\boldsymbol{\epsilon}|\mathbf{x}_0,\mathbf{y})$ from the posterior encoder $\phi$. Then, the term $\mathcal{L}(\theta,\phi)$ in (\ref{eq: rg elbo appendix}) suggests training a conditional diffusion model $\theta$ to reverse the diffusion trajectory from the estimated distribution of $\boldsymbol{\epsilon}$ given by the posterior encoder $\phi$ back to the target data distribution of $\mathbf{x}_0$. 

The $-$ELBO can be shown to be expanded as \citep{luo2022understanding,sohl2015deep,ho2020denoising}:
\begin{equation}
\begin{aligned}
    \mathcal{L}(\theta,\phi)\coloneq&-\mathbb{E}_{q(\mathbf{x}_{1:T}|\mathbf{x}_0)}\left[\log\frac{p_\theta({\mathbf{x}_{0:T}}|\mathbf{y})}{q(\mathbf{x}_{1:T}|\mathbf{x}_0)}\right] \\
    =&-\mathbb{E}_{q(\mathbf{x}_{1:T}|\mathbf{x}_0)}\left[\log\frac{p(\mathbf{x}_T)\prod_{t=1}^Tp_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t,\mathbf{y})}{\prod_{t=1}^T q(\mathbf{x}_t|\mathbf{x}_{t-1})}\right] \\
    =&-\mathbb{E}_{q(\mathbf{x}_{1:T}|\mathbf{x}_0)}\left[\log\frac{p(\mathbf{x}_T)p_\theta(\mathbf{x}_0|\mathbf{x}_1,\mathbf{y})\prod_{t=2}^T p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t,\mathbf{y})}{q(\mathbf{x}_1|\mathbf{x}_0)\prod_{t=2}^T q(\mathbf{x}_t|\mathbf{x}_{t-1})}\right] \\
    =&-\mathbb{E}_{q(\mathbf{x}_{1:T}|\mathbf{x}_0)}\left[\log p_\theta(\mathbf{x}_0|\mathbf{x}_1,\mathbf{y})\right] -\mathbb{E}_{q(\mathbf{x}_{1:T}|\mathbf{x}_0)}\left[\log\frac{p(\mathbf{x}_T)}{q(\mathbf{x}_T|\mathbf{x}_0)}\right] 
    -\sum_{t=2}^T\mathbb{E}_{q(\mathbf{x}_{1:T}|\mathbf{x}_0)}\left[\log\frac{p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t,\mathbf{y})}{q(\mathbf{x}_{t-1}|\mathbf{x}_t,\mathbf{x}_0)}\right] \\
    =&-\mathbb{E}_{q(\mathbf{x}_1|\mathbf{x}_0)}\left[\log p_\theta(\mathbf{x}_0|\mathbf{x}_1,\mathbf{y})\right] -\mathbb{E}_{q(\mathbf{x}_T|\mathbf{x}_0)}\left[\log\frac{p(\mathbf{x}_T)}{q(\mathbf{x}_T|\mathbf{x}_0)}\right] 
    -\sum_{t=2}^T\mathbb{E}_{q(\mathbf{x}_t,\mathbf{x}_{t-1}|\mathbf{x}_0)}\left[\log\frac{p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t,\mathbf{y})}{q(\mathbf{x}_{t-1}|\mathbf{x}_t,\mathbf{x}_0)}\right] \\
    =&\mathcal{L}_0+\mathcal{L}_T+\sum_{t=2}^T\mathcal{L}_{t-1},
\label{eq: ddpm elbo appendix}
\end{aligned}
\end{equation}
where
\begin{align}
    \mathcal{L}_0&\coloneq -\mathbb{E}_{q(\mathbf{x}_1|\mathbf{x}_0)}\left[\log p_\theta(\mathbf{x}_0|\mathbf{x}_1,\mathbf{y})\right], \\
    \mathcal{L}_{t-1}&\coloneq \mathbb{E}_{q(\mathbf{x}_t|\mathbf{x}_0)}\left[\mathcal{D}_{\text{KL}}\left(q(\mathbf{x}_{t-1}|\mathbf{x}_t,\mathbf{x}_0)||p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t,\mathbf{y})\right)\right], \\    
    \mathcal{L}_T&\coloneq \mathcal{D}_{\text{KL}}\left(q(\mathbf{x}_T|\mathbf{x}_0)||p(\mathbf{x}_T)\right).
\label{eq: ddpm elbo appendix next}
\end{align}

According to \citet{lee2021priorgrad}, the terms of the loss function for training the noise estimator network $\theta$ of the conditional DDPM for an arbitrary $\boldsymbol{\epsilon}\sim\mathcal{N}(\mathbf{0},\boldsymbol{\Sigma})$ can be explicitly written as: 

\begin{equation}
\begin{aligned}
    \mathcal{L}_0=&\frac{1}{2}\log\left((2\pi\beta_1)^d\abs{\boldsymbol{\Sigma}}\right)+\frac{1}{2\alpha_1}
    \mathbb{E}_{\mathbf{x}_0,\boldsymbol{\epsilon}\sim\mathcal{N}(\mathbf{0},\boldsymbol{\Sigma})}
    \norm{\boldsymbol{\epsilon}-\boldsymbol{\epsilon}_\theta(\mathbf{x}_1,\mathbf{y},1)}^2_{\boldsymbol{\Sigma}^{-1}}, \\\\
    \mathcal{L}_{t-1}=&\frac{\beta_t}{2\alpha_t(1-\bar{\alpha}_{t-1})}\mathbb{E}_{\mathbf{x}_0,\boldsymbol{\epsilon}\sim\mathcal{N}(\mathbf{0},\boldsymbol{\Sigma})}
    \norm{\boldsymbol{\epsilon}-\boldsymbol{\epsilon}_\theta(\mathbf{x}_t,\mathbf{y},t)}^2_{\boldsymbol{\Sigma}^{-1}} \\
    =&\frac{\beta_t^2}{2\sigma_t^2\alpha_t(1-\bar{\alpha}_t)}\mathbb{E}_{\mathbf{x}_0,\boldsymbol{\epsilon}\sim\mathcal{N}(\mathbf{0},\boldsymbol{\Sigma})}
    \norm{\boldsymbol{\epsilon}-\boldsymbol{\epsilon}_\theta(\mathbf{x}_t,\mathbf{y},t)}^2_{\boldsymbol{\Sigma}^{-1}}, \\\\
    \mathcal{L}_T=&\frac{\bar{\alpha}_T}{2}\mathbb{E}_{\mathbf{x}_0}\norm{\mathbf{x}_0}^2_{\boldsymbol{\Sigma}^{-1}}-\frac{d}{2}(\bar{\alpha}_T+\log(1-\bar{\alpha}_T)),
\label{eq: loss terms priorgrad}
\end{aligned}
\end{equation}
with $\bar{\alpha}_t\coloneq\prod_{i=1}^t\alpha_i$ and $\alpha_t\coloneq 1-\beta_t$ for $t=1,\dots,T$ where $\{\beta_t\}_{t=1}^{T}$ is the noise variance schedule as a hyperparameter, $d$ is the parameter freedom and $\sigma_t^2=\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha_t}}\beta_t$.

In our case, we have assumed modeling of the posterior distribution where the $\boldsymbol{\epsilon}$ is sampled from as the zero-mean Gaussian $\boldsymbol{\epsilon}\sim\mathcal{N}(\mathbf{0},\boldsymbol{\Sigma}_{\text{post}})$ where the covariance $\boldsymbol{\Sigma}_{\text{post}}\coloneq\boldsymbol{\Sigma}_{\text{post}}(\mathbf{x}_0,\mathbf{y};\phi)$ is estimated by the Posterior Net $\phi$, taking both the ground truth data $\mathbf{x}_0$ and the conditioner $\mathbf{y}$ as input.
By directly plugging in $\boldsymbol{\Sigma}=\boldsymbol{\Sigma}_{\text{post}}$ for each term in (\ref{eq: loss terms priorgrad}), we obtain:
\begin{equation}
    \mathcal{L}(\theta,\phi)=\frac{\bar{\alpha}_T}{2}\mathbb{E}_{\mathbf{x}_0}\norm{\mathbf{x}_0}^2_{\boldsymbol{\Sigma}^{-1}_{\text{post}}}+\frac{1}{2}\log\abs{\boldsymbol{\Sigma}_{\text{post}}} 
    +\sum_{t=1}^{T}\gamma_t\mathbb{E}_{(\mathbf{x}_0,\mathbf{y}),\boldsymbol{\epsilon}\sim\mathcal{N}(\mathbf{0},\boldsymbol{\Sigma}_{\text{post}})}\norm{\boldsymbol{\epsilon}-\boldsymbol{\epsilon}_\theta(\underbrace{\sqrt{\bar{\alpha}_t}\mathbf{x}_0+\sqrt{1-\bar{\alpha}_t}\boldsymbol{\epsilon}}_{\mathbf{x}_t},\mathbf{y},t)}^2_{\boldsymbol{\Sigma}^{-1}_{\text{post}}}+\textit{C},
\label{eq: prop 2 ddpm}
\end{equation}
where
\begin{equation*}
    \gamma_t= 
        \begin{cases}
        \frac{\beta_t^2}{2\sigma_t^2\alpha_t(1-\bar{\alpha}_t)}, & t>1 \\
        \frac{1}{2\alpha_1}, & t=1
        \end{cases}
\end{equation*}
and $C$ is some constant not depending on the learnable parameters.

For the prior matching term in (\ref{eq: rg elbo appendix}), we can utilize the analytic form of the KL divergence between two Gaussians which leads to:
\begin{equation}
    D_{\text{KL}}\bigr(q_{\phi}(\boldsymbol{\epsilon}|\mathbf{x}_0,\mathbf{y}) || p_{\psi}(\boldsymbol{\epsilon}|\mathbf{y})\bigr)=\frac{1}{2}\bigr(\log\frac{\abs{\boldsymbol{\Sigma}_{\text{prior}}}}{\abs{\boldsymbol{\Sigma}_{\text{post}}}}+\text{tr}(\boldsymbol{\Sigma}_{\text{prior}}^{-1}\boldsymbol{\Sigma}_{\text{post}})\bigr),
\label{eq: prop 2 kl div}
\end{equation}
where the covariances $\boldsymbol{\Sigma}_{\text{prior}}\coloneq\boldsymbol{\Sigma}_{\text{prior}}(\mathbf{y};\psi)$ and $\boldsymbol{\Sigma}_{\text{post}}\coloneq\boldsymbol{\Sigma}_{\text{post}}(\mathbf{x}_0,\mathbf{y};\phi)$.

Combining (\ref{eq: prop 2 ddpm}) and (\ref{eq: prop 2 kl div}), we have obtained the $-$ELBO of Proposition \ref{proposition: 2}.



\section{Implementation Details}
\label{sec: appendix exp}

\subsection{Algorithms}
\label{sec: algorithms}
\begin{minipage}{0.53\textwidth}
\begin{algorithm}[H]
    \footnotesize
    \caption{Training of RestoreGrad}
    \For{$i=0,1,2...,N_{\text{iter}}$}
    {
        Sample $(\mathbf{x}_0,\mathbf{y})\sim q_{\text{data}}(\mathbf{x}_0,\mathbf{y})$ \\
        $\boldsymbol{\Sigma}_{\text{prior}}\gets$Prior Net($\mathbf{y};\psi$) \\
        $\boldsymbol{\Sigma}_{\text{post}}\gets$Posterior Net($\mathbf{x}_0,\mathbf{y};\phi$) \\
        Sample $\boldsymbol{\epsilon}\sim \mathcal{N}(\mathbf{0},\boldsymbol{\Sigma}_{\text{post}})$ and $t\sim \mathcal{U}(\{1,\dots,T\})$ \\
        $\mathbf{x}_t=\sqrt{\bar{\alpha}_t}\mathbf{x}_0+\sqrt{1-\bar{\alpha}_t}\boldsymbol{\epsilon}$ \\
        $\mathcal{L}_{\text{LR}}=\bar{\alpha}_T||\mathbf{x}_0||^2_{\boldsymbol{\Sigma}^{-1}_{\text{post}}}+\log\abs{\boldsymbol{\Sigma}_{\text{post}}}$ \\
        $\mathcal{L}_{\text{DM}}=||\boldsymbol{\epsilon}-\boldsymbol{\epsilon}_\theta(\mathbf{x}_t,\mathbf{y},t)||^2_{\boldsymbol{\Sigma}^{-1}_{\text{post}}}$ \\
        $\mathcal{L}_{\text{PM}}=\log\frac{\abs{\boldsymbol{\Sigma}_{\text{prior}}}}{\abs{\boldsymbol{\Sigma}_{\text{post}}}}+\text{tr}(\boldsymbol{\Sigma}_{\text{prior}}^{-1}\boldsymbol{\Sigma}_{\text{post}})$ \\
        Update $\theta,\psi,\phi$ with $\nabla_{\theta,\psi,\phi} \,\, \eta\mathcal{L}_{\text{LR}}+\mathcal{L}_{\text{DM}}+\lambda\mathcal{L}_{\text{PM}}$ 
    }
\label{algo: training}
\end{algorithm}
\end{minipage}
\hfill
\begin{minipage}{0.44\textwidth}
\begin{algorithm}[H]
    \footnotesize
    \caption{Sampling of RestoreGrad}
    $\boldsymbol{\Sigma}_{\text{prior}}\gets$Prior Net($\mathbf{y};\psi$) \\
    Sample $\mathbf{x}_T\sim \mathcal{N}(\mathbf{0},\boldsymbol{\Sigma}_{\text{prior}})$ \\
    \For{$t=T,T-1,...,1$}
    {
        \eIf {$t>0$}{Sample $\boldsymbol{\epsilon}\sim\mathcal{N}(\mathbf{0},\boldsymbol{\Sigma}_{\text{prior}})$}{$\boldsymbol{\epsilon}=0$}
        $\mathbf{x}_{t-1}=\frac{1}{\sqrt{\alpha_t}}\left(\mathbf{x}_t-\frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}}_t}\boldsymbol{\epsilon}_{\theta}(\mathbf{x}_t,\mathbf{y},t)\right)+\sigma_t\boldsymbol{\epsilon}$
        
    }
    \Return $\mathbf{x}_0$
\label{algo: sampling}
\end{algorithm}
\end{minipage}


\subsection{Experiments on Speech Enhancement (SE)}
\subsubsection{Dataset}
We used the VoiceBank+DEMAND dataset \citep{valentini2016investigating} with the same experimental setup as in previous work \citep{pascual2017segan,phan2020improving,strauss2021flow,lu2022conditional} to perform a direct comparison. The clean speech and noise recordings were provided from the VoiceBank corpus \citep{veaux2013voice} and the Diverse Environments Multichannel Acoustic Noise Database (DEMAND) \citep{thiemann2013diverse}, respectively, each recorded with sampling rate of 48kHz. Noisy speech inputs used for training were composed by mixing the two datasets with four signal-to-noise ratio (SNR) settings from \{0, 5, 10, 15\} dB, using 10 types of noise (2 artificially generated + 8 real recorded from the DEMAND dataset) and 28 speakers from the Voice Bank corpus. The test set inputs were made with four SNR settings different from the training set, i.e., \{2.5, 7.5, 12.5, 17.5\} dB, using the remaining 5 noise types from DEMAND and 2 speakers from the VoiceBank corpus. There are totally 11527 utterances for training and
824 for testing. Note that the speaker and noise classes were uniquely selected for the training and test sets. The dataset is publicly available at: \url{https://datashare.ed.ac.uk/handle/10283/2826}. In our experiments, the audio steams were resampled to 16kHz sampling rate.

\subsubsection{Model Architecture}
\noindent\textbf{Baseline DDPM-based SE Model:} The baseline SE model considered in this work, i.e., CDiffuSE \citep{lu2022conditional}, performs enhancement in the time domain. We utilized the CDiffuSE base model, which has approximately 4.28M learnable parameters, from the implementation at: \url{https://github.com/neillu23/CDiffuSE}. The model is implemented based on DiffWave \citep{kong2020diffwave}, a versatile diffusion probabilistic model for conditional and unconditional waveform generation. The basic model structure of CDiffuSE is similar to that of DiffWave. However, since the target task is SE, CDiffuSE uses the noisy spectral features as the conditioner, rather than the clean Mel-spectral features used in DiffWave utilized for vocoders. After the reverse process is completed, the enhanced waveform further combine the observed noisy signal $\mathbf{y}$ with the ratio 0.2 to recover the high frequency speech in the final enhanced waveform, as suggested in \citet{abd2008speech,defossez2020real}. 

\noindent\textbf{PriorGrad:} We implemented the PriorGrad \citep{lee2021priorgrad} on top of the CDiffuSE model by using a data-dependent prior $\mathcal{N}(\mathbf{0},\boldsymbol{\Sigma}_{\text{y}})$, where $\boldsymbol{\Sigma}_{\text{y}}$ is the covariance of the prior distribution computed based on using the mel-spectrogram of the noisy input $\mathbf{y}$. Following the application to vocoder in \citet{lee2021priorgrad}, we leveraged a normalized frame-level energy of the mel-spectrogram for acquiring data-dependent prior, exploiting the fact that the spectral energy contains an exact correlation to the waveform variance (by Parseval’s theorem \citep{stoica2005spectral}). More specifically, we computed the frame-level energy by taking the square root of the sum of $\exp(\mathbf{Y})$ over the frequency axis for each time frame, where $\mathbf{Y}$ is the mel-spectrogram of the noisy input $\mathbf{y}$ from the training data. We then normalized the frame-level energy to a range of $(0, 1]$ to acquire the data-dependent diagonal variance $\boldsymbol{\Sigma}_{Y}$. Then we upsampled $\boldsymbol{\Sigma}_{Y}$ in the frame level to $\boldsymbol{\Sigma}_{y}$ in the waveform-level using the given hop length of computing the mel-spectrogram. We imposed the minimum standard deviation of the prior to 0.1 through clipping to ensure numerical stability during training, as suggested in \citet{lee2021priorgrad}.

\noindent\textbf{Prior Net and Posterior Net for RestoreGrad:} The additional encoder modules for the RestoreGrad adopt the ResNet-20 architect \citep{he2016deep} using the implementation from: \url{https://github.com/akamaster/pytorch_resnet_cifar10}. We suitably modified the original 2-D convolutions in ResNet-20 to 1-D convolutions for waveform processing. The modified ResNet-20 model has only 93K learnable parameters (only 2\% of the size of CDiffuSE model). The Prior Net takes the noisy speech waveform $\mathbf{y}$ as input, while the Posterior Net takes both the clean and noisy waveforms, $\mathbf{x}_0$ and $\mathbf{y}$, as input, which are concatenated along the channel dimension. We employed the exponential nonlinearity at the network output for estimating the variances of the prior and posterior distributions.

\subsubsection{Optimization and Inference}
We used the same configurations of CDiffuSE (Base) \citep{lu2022conditional} for optimizing all the models, where the batch size was 16, the Adam optimizer was used with a learning rate of $2\times 10^{-4}$, and the diffusion steps $T=50$ with linearly spaced $\beta_t\in[10^{-4}, 0.035]$. For RestoreGrad, we imposed the minimum standard deviation $\sigma_{\text{min}}=0.1$ by adding it to the output of the Prior Net and Posterior Net to ensure stability during training. The fast sampling scheme in \citet{kong2020diffwave} was used in the reverse processes with $S=6$ and the inference schedule $\beta^{\text{infer}}_t=[10^{-4}, 10^{-3}, 0.01, 0.05, 0.2, 0.35]$. The models were trained on one NVIDIA Tesla V100 GPU of 32 GB CUDA memory and finished training for 96 epochs in 1 day.

\subsubsection{Evaluation Metrics}
\noindent\textbf{PESQ:} a speech quality measure using the wide-band version recommended in ITU-T P.862.2 \citep{itu862}. It basically models the mean opinion scores (MOS) that cover a scale from 1 (bad) to 5 (excellent). We used the Python-based PESQ implementation from: \url{https://github.com/ludlows/python-pesq}.

\noindent\textbf{SI-SNR:} a variant of the conventional SNR measure taking into account the scale-invariance of audio signals. The SI-SDR is a more robust and meaningful metric than the traditional SNR for measuring speech quality. A higher SI-SNR score indicates better perceptual speech quality. We adopted the SI-SNR implementation from: \url{https://lightning.ai/docs/torchmetrics/stable/audio/scale_invariant_signal_noise_ratio.html}.

\noindent\textbf{SSNR:} an SNR measure, instead of working on the whole signal, that calculates the average of the SNR values of short segments (segment length $=30$ msec, $75\%$ overlap, $\text{SNR}_{\text{min}}=-10$ dB, $\text{SNR}_{\text{max}}=35$ dB). We use the Python-based SSNR implementation from: \url{https://github.com/schmiph2/pysepm}.

\noindent\textbf{CSIG:} The mean opinion score (MOS) prediction of the signal distortion (from 1 to 5, the higher the better) \citep{hu2007evaluation}. We used the implementation from: \url{https://github.com/schmiph2/pysepm}.

\noindent\textbf{CBAK:} MOS prediction of the intrusiveness of background noises (from 1 to 5, the higher the better) \citep{hu2007evaluation}. We used the implementation from: \url{https://github.com/schmiph2/pysepm}.

\noindent\textbf{COVL:} MOS prediction of the overall effect (from 1 to 5, the higher the better) \citep{hu2007evaluation}. We used the implementation from: \url{https://github.com/schmiph2/pysepm}.


\subsection{Experiments on Image Restoration (IR)}

\subsubsection{Datasets}
We used three standard benchmark image restoration datasets considering adverse weather conditions of snow, heavy rain with haze, and raindrops on the camera sensor, following \citet{ozdenizci2023restoring}. 

\noindent\textbf{Snow100K \citep{liu2018desnownet}:} a dataset for evaluation of image desnowing models. We used the test set for evaluation, which consist of 50,000 samples. The images are split into approximately equal sizes of three Snow100K-S/M/L sub-test sets (16,611/16,588/16,801), indicating the synthetic snow strength imposed via snowflake sizes (light/mid/heavy). The dataset can be downloaded from: \url{https://sites.google.com/view/yunfuliu/desnownet}.

\noindent\textbf{Outdoor-Rain \citep{li2019heavy}:} a dataset of simultaneous rain and
fog which exploits a physics-based generative model to simulate not only dense synthetic rain streaks, but also incorporating more realistic scene views, constructing an inverse
problem of simultaneous image deraining and dehazing. We used the test set, denoted in \citet{li2019heavy} as Test1, which is of size 750 for quantitative evaluations. The dataset can be accessed at: \url{https://github.com/liruoteng/HeavyRainRemoval}.

\noindent\textbf{RainDrop \citep{qian2018attentive}:} a dataset of images with raindrops
introducing artifacts on the camera sensor and obstructing the view. It consists of 861 training images with synthetic raindrops, and a test set of 58 images dedicated for quantitative evaluations, denoted in \citet{qian2018attentive} as RainDrop-A. The dataset is provided at: \url{https://github.com/rui1996/DeRaindrop}.

In addition, we also used the composite dataset for multi-weather IR model training:

\noindent\textbf{AllWeather \citep{valanarasu2022transweather}:} is a dataset of 18,069 samples composed of subsets of training images from the training sets of the three datasets above, in order to create a balanced training set across three weather conditions with a similar approach to \citet{li2020all}. The dataset is publicly available at: \url{https://github.com/jeya-maria-jose/TransWeather}.

\subsubsection{Model Architecture}
\label{appendix: model arch of ir for weather}
\noindent\textbf{Baseline DDPM-based IR Models:} The baseline IR models considered in this work, i.e., the RainDropDiff and WeatherDiff from \citet{ozdenizci2023restoring}, perform patch-based diffusive restoration of the images. The models perform diffusion process at the patch level, where overlapping $p\times p$ patches are taken as input. When sampling, all $p\times p$ patches extracted from the image with a hop size $r$ are processed by the DDPM model, utilizing the mean estimated noise based sampling updates for the overlapping pixels to synthesize the clean image. In this work, we considered $p=64$ and $r=16$, which correspond to the RainDropDiff$_{64}$and WeatherDiff$_{64}$ models (with 110M and 82 M learnable parameters, respectively) provided by the authors at: \url{https://github.com/IGITUGraz/WeatherDiffusion}.

\noindent\textbf{Prior Net and Posterior Net for RestoreGrad:} The additional encoder modules for the RestoreGrad adopt the ResNet-20 architect \citep{he2016deep} using the implementation from: \url{https://github.com/akamaster/pytorch_resnet_cifar10}. The ResNet-20 model has 0.27M learnable parameters, which is less than 0.3\% of the size of RainDropDiff and WeatherDiff. The Prior Net takes the noisy image $\mathbf{y}$ as input, while the Posterior Net takes both the clean and noisy images, $\mathbf{x}_0$ and $\mathbf{y}$, as input, which are concatenated along the channel dimension. We employed the exponential nonlinearity at the network output for estimating the variances of the prior and posterior distributions.

\subsubsection{Optimization and Inference}
\label{appendix: opt and infer of ir for weather}
We used the same configurations of \citet{ozdenizci2023restoring} for optimizing all the models, except the batch size was 4 instead of 16 due to GPU memory limitation. The Adam optimizer with a fixed learning rate of $2\times 10^{-5}$ was used for training models without weight decay, and an exponential moving average with a weight of 0.999 was applied during parameter updates. The number of diffusion steps was $T=1000$ and the noise schedule was $\beta_t\in[10^{-4}, 0.02]$, linearly spaced. For inference, we used $S=10$ sampling timesteps for each model that we trained on our own. We did not use the deterministic implicit sampling scheme as in \citet{ozdenizci2023restoring} for our RestoreGrad-based DDPM models as we found using the normal stochastic sampling scheme actually works better. The models were trained on 2 NVIDIA Tesla V100 GPU of 32 GB CUDA memory and finished training for 9,261 epochs on the RainDrop dataset in 12 days and 887 epochs on the AllWeather dataset in 21 days.

\subsubsection{Evaluation Metrics}
\noindent\textbf{PSNR:} a non-linear full-reference metric that compares the pixel values of the original reference image to the values of the degraded image based on the mean squared error \citep{huynh2008scope}. A higher PSNR indicates better reconstruction quality of images in terms of distortion. PSNR can be calculated for the different color spaces. We followed \citet{ozdenizci2023restoring} to compute PSNR based on the luminance channel Y of the YCbCr color space. We used the implementation form \url{https://github.com/JingyunLiang/SwinIR} for PSNR calculation.

\noindent\textbf{SSIM:} a non-linear full-reference metric compares the luminance, contrast and structure of the original and degraded image \citep{wang2004image}. It provides a value from 0 to 1,  the closer the score is to 1, the more similar the degraded image is to the reference image. We followed \citet{ozdenizci2023restoring} to compute SSIM based on the luminance channel Y of the YCbCr color space. We used the implementation form \url{https://github.com/JingyunLiang/SwinIR} for SSIM calculation.

\textbf{Learned Perceptual Image Patch Similarity (LPIPS)} \citep{zhang2018unreasonable} and \textbf{Fréchet Inception Distance (FID)} \citep{heusel2017gans}: to provide better quantification of perceptual quality over the traditional distortion measures of PSNR and SSIM \citep{blau2018perception,freirich2021theory}. For the LPIPS we used the implementation from \url{https://github.com/richzhang/PerceptualSimilarity}, and for FID we used the  implementation from \url{https://github.com/chaofengc/IQA-PyTorch}. In both metrics, a lower score indicates better perceptual quality of the restored image. 


\subsection{Experiments on Generalization to OOD and Realistic Data}

\subsubsection{Datasets}
The additional datasets considered for experiments on realistic data for the IR task are:

\noindent\textbf{RainDS-Real \citep{qian2018attentive}:} is the raindrop removal test subset of the RainDS dataset presented in  \citet{qian2018attentive}. It consists of 98 real-world captured raindrop obstructed images. The dataset is publicly available at: \url{https://github.com/Songforrr/RainDS_CCN}.

\noindent\textbf{Snow100K-Real \citep{liu2018desnownet}:} is the subset of the Snow100K dataset \citep{liu2018desnownet} that consists of 1,329 realistic snowy images for testing real-world restoration cases. The dataset can be accessed at: \url{https://sites.google.com/view/yunfuliu/desnownet}.

The additional dataset considered for experiments on OOD data of the SE task is:

\noindent\textbf{CHiME-3 \citep{barker2017third}:} is a 6-channel microphone recording of talkers speaking in a noisy environment, sampled at 16 kHz. It consists of 7138 and 1320 simulated utterances for training and testing, respectively, which are generated by artificially mixing clean speech data with noisy backgrounds of four types, i.e. cafe, bus, street, and pedestrian area. In this paper, we only take the 5-th channel recordings for the experiments. The dataset information can be found at: \url{https://www.chimechallenge.org/challenges/chime3/data}.


\subsubsection{Evaluation Metrics}
The additional evaluation metric used in the corresponding section is:

\noindent\textbf{NIQE:} is a reference-free quality assessment of real-world restoration performance introduced by \citet{mittal2012making} which measures the naturalness of a given image without using any reference image. A lower NIQE score indicates better perceptual image quality. We used the NIQE implementation from: \url{https://github.com/chaofengc/IQA-PyTorch}.


\subsection{Applications to Other IR tasks}

\subsubsection{Datasets}

The datasets considered for experiments on image deblurring and super-resolution tasks are:

\noindent\textbf{RealBlur \citep{rim_2020_ECCV}:} a large-scale dataset of real-world blurred images and ground truth sharp images for learning and benchmarking single image deblurring methods. The images were captured both in the camera raw and JPEG formats, leading to two datasets: \textit{RealBlur-R} from the raw images and \textit{RealBlur-J} from the JPEG images. Each training set consists of 3,758 image pairs and each test set consists of 980 image pairs. The dataset can be downloaded from: \url{https://cg.postech.ac.kr/research/realblur/}.

\noindent\textbf{DIV2K \citep{Agustsson_2017_CVPR_Workshops,Timofte_2017_CVPR_Workshops}:} a dataset of 2K resolution high quality images collected from the Internet as part of the NTIRE 2017 super-resolution challenge. There are 800, 100, and 100 images for training, validation, and testing, respectively. The dataset provides $\times2$, $\times3$, and $\times4$ downscaled images with bicubic and unknown downgrading operations. The dataset can be downloaded from:\url{https://data.vision.ee.ethz.ch/cvl/DIV2K/}.


\subsubsection{Model Architecture}
The baseline conditional DDPM (cDDPM) implements the same architecture as the patch-based denoising diffusion model of WeatherDiff \citep{ozdenizci2023restoring}. The Prior Net and Posterior Net of RestoreGrad also adopt the same ResNet models as in the IR experiments under adverse weather conditions. For more details please refer to Appendix \ref{appendix: model arch of ir for weather}.

\subsubsection{Optimization and Inference}
The models were optimized and inferenced using the same configurations and settings as given in Appendix \ref{appendix: opt and infer of ir for weather} for the IR experiments under adverse weather conditions. The models were trained on 2 NVIDIA Tesla V100 GPU of 32 GB CUDA memory and finished training for 853 epochs on the RealBlur-\{R,J\} dataset each in 5 days and 2000 epochs on the DIV2K-\{$\times2$,$\times4$\} dataset each in 3 days.


\section{Additional Experimental Results}
\label{sec: additional results}

\subsection{Generalization to Out-of-Distribution (OOD) and Realistic Data}
We have so far evaluated the models on in-domain scenarios with synthetic noisy data where RestoreGrad has shown substantial improvements. A natural question is that if the demonstrated improvements have actually come at the expense of the model's generalizability to unseen or realistic data. To address the concern,  we evaluate the IR models on two additional datasets from \citet{quan2021removing,liu2018desnownet} that consist of real-world images, using the reference-free Natural Image Quality Evaluator (NIQE) metric \citep{mittal2012making} (a lower score indicates better quality). In Table \ref{table: ir sota comp realistic} we see that RestoreGrad is able to perform on par with or better than WeatherDiff and the non-generative TransWeather model. For OOD testing, we evaluate the SE models on the CHiME-3 dataset \citep{barker2017third} unseen during model training. Table \ref{table: se sota comp ood} compares RestoreGrad with CDiffuSE that was also trained for 96 epochs, DOSE \citep{tai2024dose}, and two discriminative SE models. We can see that RestoreGrad is able to perform equally well as the CDiffuSE while outperforming DOSE and other non-generative SE models (Demucs \citep{defossez2020real}, WaveCRN \citep{hsieh2020wavecrn}). The results in both tables show that RestoreGrad is capable of improving in-domain performance while maintaining desirable generalization capabilities of generative models.

\begin{table}[!t]
\centering
\begin{small}
\setlength{\tabcolsep}{5pt} % let TeX compute the intercolumn space
\caption{Evaluation on realistic image datasets of IR models trained on synthetic images of AllWeather training set.}
\vspace{0.2cm}
\label{table: ir sota comp realistic}
\centering
\resizebox{0.6\columnwidth}{!}{%
\begin{NiceTabular}{lccc}
\toprule 
 \multirow{2}{*}{Methods} & \multirow{2}{*}{Gen. } & RainDS-Real & Snow-Real \\
 \cmidrule(lr){3-3}
 \cmidrule(lr){4-4}
 &  & NIQE $\downarrow$ & NIQE $\downarrow$ \\
 \midrule
 TransWeather \citep{valanarasu2022transweather} & N & 4.005 & 3.161 \\
 \midrule
 WeatherDiff \citep{ozdenizci2023restoring} & Y & \underline{3.050} & \textbf{2.985} \\
 + RestoreGrad (ours) & Y & \textbf{2.556} & \underline{3.015}  \\
 \bottomrule
\end{NiceTabular}
}\\
\vspace{0.1cm}
\scriptsize
*Bold text for best and underlined text for second best values. The column ``Gen.'' indicates if the model is generative (Y) or not (N).
\end{small}
\end{table}

\begin{table}[!t]
\centering
\begin{small}
\setlength{\tabcolsep}{5pt} % let TeX compute the intercolumn space
\caption{Evaluation of SE models on CHiME-3 test set, where the models were trained on VoiceBank+DEMAND training set.}
\vspace{0.2cm}
\label{table: se sota comp ood}
\resizebox{0.65\columnwidth}{!}{%
\begin{NiceTabular}{lcccccc}
\toprule 
 Methods & Gen. & PESQ$\uparrow$ & CSIG$\uparrow$ & CBAK$\uparrow$ & COVL$\uparrow$ & SI-SNR$\uparrow$ \\
\midrule
 Unprocessed & - & 1.27 & 2.61 & 1.93 & 1.88 & 7.51 \\
 \midrule
 Demucs \citep{defossez2020real} & N & 1.38 & 2.50 & 2.08 & 1.88 & - \\
 WaveCRN \citep{hsieh2020wavecrn} & N & 1.43 & 2.53 & 2.03 & 1.91 & - \\
 DOSE \citep{tai2024dose} & Y &  1.52 & 2.71 & \textbf{2.15} & 2.06 & -\\
 \midrule
 CDiffuSE \citep{lu2022conditional} & Y & \textbf{1.55} & \underline{2.87} & 2.09 & \underline{2.15} & \underline{7.67}\\
 + RestoreGrad (ours) & Y &\underline{1.54} & \textbf{2.88} & \underline{2.14} & \textbf{2.16} & \textbf{8.45} \\
 \bottomrule
\end{NiceTabular}
}
\\
\vspace{0.1cm}
\scriptsize
*Bold text for best and underlined text for second best values. The column ``Gen.'' indicates if the model is generative (Y) or not (N).
\end{small}
\end{table}

\subsection{Additional Results on SE}
\label{sec: additional results se}

\noindent\textbf{Model Learning Performance in Terms of Other Metrics:}
In addition to the results evaluated by PESQ and COVL in Figure \ref{fig: training_curve}, we provide the learning curves in terms of the CSIG, CBAK, and SI-SNR metrics in Figure \ref{fig: training curves other}, to further support the advantages of RestoreGrad over the baseline DDPM and PriorGrad for improved training behavior and efficiency.

\begin{figure}[!th]
    \centering
    \includegraphics[width=0.85\linewidth]{Figs/training_curves_extra.pdf}
    \vspace{-0.2cm}
    \caption{Model learning performance in terms of CSIG, CBAK, and SI-SNR metrics. Improved training behavior of RestoreGrad over CDiffuSE and PriorGrad is observed among all metrics.} 
\label{fig: training curves other}
\end{figure}

\vspace{0.2cm}

\noindent\textbf{Performance with Using Different Numbers of Inference Steps:}
In Figure \ref{fig: tolerance to reduced interence sampling steps appendix}, we show how the trained diffusion models perform with respect to using different numbers of reverse steps for inference. Specifically, in each case of CDiffuSE, PriorGrad, and RestoreGrad, we trained the model for 96 epochs and then inferenced with $S\in\{3,4,5\}$ reverse steps to compare with the originally adopted $S=6$ steps in \citet{lu2022conditional}. We used $\beta^{\text{infer}}_t=[10^{-4}, 10^{-3}, 0.05, 0.2, 0.35]$ for $S=5$, $\beta^{\text{infer}}_t=[10^{-4}, 0.05, 0.2, 0.35]$ for $S=4$, and $\beta^{\text{infer}}_t=[0.05, 0.2, 0.35]$ for $S=3$. These choices were selected from the subsets of the original noise schedule for $S=6$, i.e., $\beta^{\text{infer}}_t=[10^{-4}, 10^{-3}, 0.01, 0.05, 0.2, 0.35]$, that resulted in best performance of the models. For the figure we can see that as $S$ becomes smaller, the baseline CDiffuSE degrades considerably, while PriorGrad shows certain resistance, and RestoreGrad manages to maintain the high performance. We present more comparison in Table \ref{table: tolerence to infer reduct extra} in terms of SI-SNR, CSIG, CBAK, and COVL metrics. The results further support that RestoreGrad is much more robust to the reduction in sampling steps, achieving the best quality scores in all the metrics over the baseline DDPM and PriorGrad across all sampling steps considered.

\begin{figure}[!th]
    \centering
    \includegraphics[width=0.85\linewidth]{Figs/infer_step_reduct_SE.pdf}
    \vspace{-0.25cm}
    \caption{Effect of using reduced numbers of sampling steps in inference on the SE performance, in terms of PESQ. RestoreGrad demstrates strongest endurance to the reduction in reverse sampling steps for inference.} 
\label{fig: tolerance to reduced interence sampling steps appendix}
\end{figure}

\begin{table}[!th]
\centering
\begin{small}
\setlength{\tabcolsep}{3pt} % let TeX compute the intercolumn space
\caption{Performance comparison of RestoreGrad with the baseline DDPM (CDiffuSE) and PriorGrad for using various numbers of sampling steps $S$ during inference.}
\vspace{0.25cm}
\label{table: tolerence to infer reduct extra}
\resizebox{0.9\columnwidth}{!}{%
\begin{NiceTabular}{lcccc|cccc|cccc|cccc}
\toprule 
 \multirow{2}{*}{Methods} & \multicolumn{4}{c}{SI-SNR $\uparrow$} & \multicolumn{4}{c}{CSIG $\uparrow$} & \multicolumn{4}{c}{CBAK $\uparrow$} & \multicolumn{4}{c}{COVL $\uparrow$}  \\
 \cmidrule(lr){2-5}
 \cmidrule(lr){6-9}
 \cmidrule(lr){10-13}
 \cmidrule(lr){14-17}
 & \multirow{1}{*}{$S$=6} & \multirow{1}{*}{$S$=5}  & \multirow{1}{*}{$S$=4}  & \multirow{1}{*}{$S$=3}  & \multirow{1}{*}{$S$=6} & \multirow{1}{*}{$S$=5}  & \multirow{1}{*}{$S$=4}  & \multirow{1}{*}{$S$=3}  & \multirow{1}{*}{$S$=6} & \multirow{1}{*}{$S$=5}  & \multirow{1}{*}{$S$=4}  & \multirow{1}{*}{$S$=3}  & \multirow{1}{*}{$S$=6} & \multirow{1}{*}{$S$=5}  & \multirow{1}{*}{$S$=4}  & \multirow{1}{*}{$S$=3} \\
 \midrule
 CDiffuSE \citep{lu2022conditional} & 11.84 & 11.46 & 11.32 & 11.28 & 3.52 & 3.44 & 3.15 & 3.13 & 2.76 & 2.72 & 2.64 & 2.63 & 2.89 & 2.82 & 2.60 & 2.58 \\
 + PriorGrad \citep{lee2021priorgrad} & 14.21 & 13.98 & 13.93 & 13.93 & 3.67 & 3.61 & 3.56 & 3.54 & 2.93 & 2.90 & 2.88 & 2.88 & 3.02 & 2.97 & 2.93 & 2.92 \\
 + RestoreGrad (ours) & \textbf{14.74} & \textbf{14.66} & \textbf{14.64} & \textbf{14.65} & \textbf{3.80} & \textbf{3.77} & \textbf{3.75} & \textbf{3.75} & \textbf{3.00} & \textbf{2.99} & \textbf{2.99} & \textbf{2.99} & \textbf{3.14} & \textbf{3.12} & \textbf{3.11} & \textbf{3.11} \\
 \bottomrule
\end{NiceTabular}
}
\\
\vspace{0.1cm}
*Best values are indicated with bold text.
\end{small}
\vspace{0.2cm}
\end{table}



\noindent\textbf{Visualizing the Learned Prior.} It would be interesting to see how the latent noise prior that has been learned by RestoreGrad looks like and how it compares with that of the PriorGrad. In Figure \ref{fig: learned noise prior} we present an example of a randomly chosen noisy speech waveform and the corresponding latent noise $\boldsymbol{\Sigma}_{y}=\mbox{diag}\{\boldsymbol{\sigma}^2_{y}\}$ of PriorGrad and that of RestoreGrad (with $(\eta, \lambda)=(0.1, 0.5)$ for (\ref{eq: elbo of restoregrad})). It can be seen that the variances of the pre-defined (PriorGrad) and learned (RestoreGrad) latent noise distributions are actually quite different, though both show the trend of following the variation of the conditioner signal level. This trend indicates that both latent distributions aim to better approximate the true signal distribution in a more informative manner for improved efficiency, as against the standard Gaussian prior used in the original DDPM. Note that in the RestoreGrad training, we have chosen a proper KL weight $\lambda$ so that the Prior Net distribution matches the Posterior Net distribution reasonably well without harming the reconstruction ability of the DDPM model. On the other hand, using a too large $\lambda$ might lead to a collapsed latent space as the optimization could have put too much emphasis on matching the prior and posterior distribution, discarding the contribution of the reconstruction loss term. In contrast, using a too small $\lambda$ might result in large discrepancy between the learned prior and posterior distributions, as also illustrated in Figure \ref{fig: learned noise prior}. 
Empirically, we found a naive choice of 1 works reasonably well and also for similar values, e.g., 0.5, 10, etc., as similarly observed in the VAE-type model of \citet{kohl2018probabilistic}.


\begin{figure}[!t]
    \centering
    \includegraphics[width=0.8\linewidth]{Figs/se_pg_rg_lambda.pdf}
    \caption{An example of learned latent distribution variances, $\boldsymbol{\Sigma}_{\text{prior}}=\mbox{diag}\{\boldsymbol{\sigma}^2_{\text{prior}}\}$ and $\boldsymbol{\Sigma}_{\text{post}}=\mbox{diag}\{\boldsymbol{\sigma}^2_{\text{post}}\}$ by RestoreGrad, and the effect of the KL weight $\lambda$ of the prior matching loss $\mathcal{L}_{\text{PM}}$ on the resulting latent distribution variances. The pre-computed variance of the handcrafted prior using PriorGrad is also presented for reference purposes.} 
\label{fig: learned noise prior}
\end{figure}


\noindent\textbf{Comparison to Existing Waveform-Domain Generative SE Models:} 
In Table \ref{table: se sota comp 2} we benchmark RestoreGrad with several generative SE approaches. Note that although RestoreGrad performs slightly inferior to DOSE, a recent SE model also based on DiffWave \citep{kong2020diffwave}, it was actually achieved with 4.6$\times$ fewer training epochs.

\begin{table}[!t]
\centering
\begin{small}
\setlength{\tabcolsep}{5pt} % let TeX compute the intercolumn space
\caption{Comparison with existing time-domain, generative SE models.}
\vspace{0.1cm}
\label{table: se sota comp 2}
\resizebox{0.5\linewidth}{!}{%
\begin{NiceTabular}{lcccc}
\toprule 
 Methods & PESQ$\uparrow$ & CSIG$\uparrow$ & CBAK$\uparrow$ & COVL$\uparrow$ \\%& SI-SNR($\uparrow$) \\
\midrule
 Unprocessed & 1.97 & 3.35 & 2.44 & 2.63 \\%& 8.46 \\
 \midrule
 SEGAN \citep{pascual2017segan} & 2.16 & 3.48 & 2.94 & 2.80 \\%& - \\
 DSEGAN \citep{phan2020improving} & 2.39 & 3.46 & \underline{3.11} & 2.90 \\%& - \\
 SE-Flow \citep{strauss2021flow} & 2.28 & 3.70 & 3.03 & 2.97 \\%& - \\
 DOSE \citep{tai2024dose} & \textbf{2.56} & \textbf{3.83} & \textbf{3.27} & \textbf{3.19} \\
 \midrule
 CDiffuSE \citep{lu2022conditional} & 2.44 & 3.66 & 2.83 & 3.03 \\%& - \\
 + RestoreGrad (ours) & \underline{2.51} & \underline{3.80} & 3.00 & \underline{3.14} \\%& \textbf{14.74}\\
 
 \bottomrule
\end{NiceTabular}
}
\\
\vspace{0.1cm}
\scriptsize
*Best values in bold and second best values underlined. 
\end{small}
\end{table}



\noindent\textbf{Evaluation Using Automatic Speech Recognition (ASR):} Following \citet{benitadiffar} who perform evaluation of diffusion-based speech generation using ASR, we evaluate the SE model as a front-end denoiser for ASR under noisy environments. To this end, we pre-process the noisy VoiceBand+DEMAND test data samples through the well-trained SE model and feed the denoised audio separately to two pre-trained ASR engines taken from the NVIDIA NeMo toolkit\footnote{https://github.com/NVIDIA/NeMo}: \textit{Conformer-transducer-large} \citep{gulati2020conformer} and \textit{Citrinet-1024} \citep{majumdar2021citrinet}. We report the word error rate (WER) and character error rate (CER) for each ASR engine outcome, where the lower WER / CER indicate better performance. The results are presented in Table \ref{table: asr} with all the SE models trained after 96 epochs, inferred using 6 steps. It is interesting to see that CDiffuSE and PriorGrad actually lead to worse performance than the unprocessed speech case for Citrinet ASR. Our RestoreGrad is able to achieve the lowest WER and CER for both ASR models, demonstrating its efficacy for enhancing machine learning capabilities under noisy environments.

\begin{table}[th!]
\centering
%\vspace{-0.2cm}
\begin{small}
\setlength{\tabcolsep}{2.5pt} % let TeX compute the intercolumn space
\caption{Following \citet{benitadiffar} who perform evaluation of diffusion-based speech generation using ASR, we evaluate SE models on two ASR engines (Conformer, Citrinet) for the VoiceBand+DEMAND test set. The results further confirm the superiority of RestoreGrad over the baseline and PriorGrad.}
\vskip 0.1in
\label{table: asr}
\begin{NiceTabular}{lcc}
\toprule
 \multirow{3}{*}{SE model} & \multicolumn{2}{c}{ASR: WER $\downarrow$ (\%) / CER $\downarrow$ (\%)}  \\
\cmidrule{2-3}
     & Conformer \citep{gulati2020conformer} & Citrinet \citep{majumdar2021citrinet} \\
\midrule
    Unprocessed & 6.62 / 6.15 & 8.69 / 6.86 \\
\midrule
    CDiffuSE \citep{lu2022conditional} & 6.55 / 6.01 & 9.77 / 7.41 \\
    + PriorGrad \citep{lee2021priorgrad} & 6.13 / 5.70 & 9.15 / 7.00 \\
    + RestoreGrad (Ours) & \textbf{5.07} / \textbf{5.27} & \textbf{8.15} / \textbf{6.51} \\
\bottomrule
\end{NiceTabular}
\\
\vspace{0.1cm}
*Best values are indicated with bold text.
\end{small}
\end{table}


\vspace{0.2cm}


\noindent\textbf{Enhanced Speech Examples:}
We present several audio examples in Figure \ref{fig: audio examples} to facilitate the comparison of the baseline DDPM and our RestoreGrad. It can be seen the RestoreGrad is able to recover a better speech signal closer to the target clean speech, which is also reflected by the higher PESQ scores obtained. 


\begin{figure}[!th]
    \centering
    \includegraphics[width=0.7\linewidth]{Figs/audio_examples.pdf}
    \caption{Enhanced speech examples of the baseline DDPM (CDiffuSE) and the proposed RestoreGrad for several noisy samples taken from the VoiceBank+DEMAND test set.}
\label{fig: audio examples}
\end{figure}


\subsection{Additional Results on IR}
\label{sec: additional results on IR appendix}

\noindent\textbf{Comparison to Existing IR Models on RainDrop Dataset:}
We compare our method with existing IR models including DuRN \citep{liu2019dual}, RaindropAttn \citep{quan2019deep}, AttentiveGAN \citep{qian2018attentive}, and IDT \citep{xiao2022image} in Table \ref{table: ir sota comp raindrop} on the RainDrop dataset \citep{qian2018attentive}, where the models were all trained and tested on the same training and test samples. The results of the compared models were taken from \citet{ozdenizci2023restoring}, where the RainDropDiff was trained for 37,042 epochs. \textit{Our RestoreGrad was only trained for 9,261 epochs (4$\times$ fewer than RainDropDiff), and has achieved the highest scores} (here we report $\text{mean}\pm \text{std}$ of RestoreGrad based on results of 10 independent samplings).

\begin{table}[!t]
\centering
\begin{small}
\setlength{\tabcolsep}{5pt} % let TeX compute the intercolumn space
\caption{Weather-specific (RainDrop dataset) model comparison.}
\vspace{0.1cm}
\label{table: ir sota comp raindrop}
\resizebox{0.6\linewidth}{!}{%
\begin{NiceTabular}{lcc}
\toprule 
 \multirow{2}{*}{Methods} & \multicolumn{2}{c}{RainDrop}  \\
 \cmidrule(lr){2-3}
 & \multirow{1}{*}{PSNR $\uparrow$} & \multirow{1}{*}{SSIM $\uparrow$} \\
 \midrule
 DuRN \citep{liu2019dual} &  31.24 & 0.9259 \\
 RaindropAttn \citep{quan2019deep} & 31.44 & 0.9263 \\
 AttentiveGAN \citep{qian2018attentive} & 31.59 & 0.9170 \\
 IDT \citep{xiao2022image} & 31.87 & 0.9313 \\
 \midrule
 RainDropDiff \citet{ozdenizci2023restoring} & \underline{32.29} & \underline{0.9422} \\
 + RestoreGrad (ours) & \textbf{32.69}$\pm$0.03 & \textbf{0.9441}$\pm$7e-5 \\
 \bottomrule
\end{NiceTabular}
}
\\
\vspace{0.1cm}
\footnotesize
*Best values in bold and second best values underlined. 
\end{small}
\end{table}

\noindent\textbf{Visualizing the Learned Prior:}
We visualize the learned prior distribution variances for a chosen image input with various $\eta$ values in Figure \ref{fig: learned prior vs eta image} since we are interested in the effect of this newly introduced hyperparameter.
We plot the results for the first channel of the image. The original contaminated image (i.e., the conditioner $\mathbf{y}$ to the DDPM model) is also presented for reference purposes. As expected for the latent space regularization effect, a large $\eta$ results in smaller variances as enforcing stronger regularization, while a small $\eta$ leads to larger variances, as observed in the plots. Moreover, the learned prior appears to preserve the structure of the image, indicating that it tends to learn a prior distribution that approximates the data distribution.

\begin{figure}[!th]
    \centering
    \includegraphics[width=0.7\linewidth]{Figs/ir_vs_eta.pdf}
    \vspace{-0.4cm}
    \caption{Visualization of learned prior distribution variances with various $\eta$ for a sample image taken from the RainDrop test set \citep{qian2018attentive}. Mind the magnitude color bar of each figure. We can see that a larger $\eta$ results in smaller variance of the prior distribution, while a smaller $\eta$ leads to larger variance.} 
\label{fig: learned prior vs eta image}
\end{figure}

\vspace{0.2cm}

\noindent\textbf{Restoration Performance vs. $\eta$ and $\lambda$:}
We also study the IR performance of the RestoreGrad models trained across various combinations of $\eta$ and $\lambda$ in Table \ref{table: ir eta lambda}, where the models were trained and tested on the RainDrop dataset. The results show that RestoreGrad works effectively for a wide range of $\eta$ and $\lambda$ values as outperforming the baseline DDPM model, RainDropDiff from \citet{ozdenizci2023restoring}, which utilizes the standard Gaussian prior for the diffusion process.

\begin{table}[!th]
\centering
\begin{small}
\setlength{\tabcolsep}{5pt} % let TeX compute the intercolumn space
\caption{RestoreGrad performance for various $\eta$ and $\lambda$, where the models were trained for 9,261 epochs and tested with $S=10$ sampling steps on the RaindDrop dataset \citep{qian2018attentive}. The baseline RainDropDiff model results reported in the original paper of \citet{ozdenizci2023restoring} (which was trained for 37,042 epochs, 4 times more than our RestoreGrad models) are also presented here for comparison purposes.}
\vspace{0.5cm}
\label{table: ir eta lambda}
%\resizebox{0.6\columnwidth}{!}{%
\begin{NiceTabular}{cllcc}
    \toprule 
    Model & $\eta$ & $\lambda$ & PSNR $\uparrow$ & SSIM $\uparrow$ \\
    \midrule
    \multirow{5}{*}{RestoreGrad (ours)}
     & $0.05$ & \multirow{5}{*}{$0.1$} & \textbf{32.55} & \textbf{0.9440} \\
     & $0.01$ & & \textbf{32.73} & \textbf{0.9448} \\
     & $0.005$ & & \textbf{32.69} & \textbf{0.9441} \\
     & $0.001$ & & \textbf{32.63} & 0.9404 \\
     & $0.0005$ & & \textbf{32.50} & 0.9405 \\
     \midrule
     \multirow{4}{*}{RestoreGrad (ours)}
     & \multirow{4}{*}{0.005} 
       & 10 & \textbf{32.74} & \textbf{0.9442} \\
      & & 1 & \textbf{32.72} & \textbf{0.9441} \\
      & & 0.1 & \textbf{32.69} & \textbf{0.9441} \\
      & & 0.01 & \textbf{32.41} & 0.9417 \\
      \midrule
      RainDropDiff \citep{ozdenizci2023restoring} & - & - & 32.29 & 0.9422 \\
     \bottomrule
    \end{NiceTabular}
%}
\\
\vspace{0.1cm}
*Values in bold text indicate better scores than the baseline ReainDropDiff model.
\end{small}
%\vspace{0.5cm}
\end{table}


\vspace{0.2cm}

\noindent\textbf{Experiments on Image Deblurring:}
We apply RestoreGrad to the baseline conditional DDPM (cDDPM) which implements the same architecture as the patch-based DDPM of \citet{ozdenizci2023restoring} used for weather degradations for deblurring. We trained the baseline cDDPM and RestoreGrad models and validated their performance on the RealBlur dataset \citep{rim_2020_ECCV}, a large-scale dataset of real-world blurred images captured both in the camera raw and JPEG formats, leading to two sub-datasets: \textit{RealBlur-R} from the raw images and \textit{RealBlur-J} from the JPEG images. Each training set consists of 3,758 image pairs and each test set consists of 980 image pairs. In Table \ref{table: ir sota comp deblur}, we present results of the baseline cDDPM and RestoreGrad models trained after 853 epochs. We also include scores of two existing models, SRN-DeblurNet \citep{tao2018scale} and DeblurGAN-v2 \citep{kupyn2019deblurgan}, which performed similarly to the baseline cDDPM (taken from results by \citet{rim_2020_ECCV}), as references for comparison. We can see that, except for LPIPS and FID on RealBlur-J, RestoreGrad is able to achieve improved scores than the baseline cDDPM, and outperform the compared methods.


\begin{table}[!th]
\centering
\begin{small}{
\setlength{\tabcolsep}{5pt} % let TeX compute the intercolumn space
\caption{Image deblurring of realistic blurred images.}
\vspace{0.1cm}
\label{table: ir sota comp deblur}
\resizebox{0.7\columnwidth}{!}{%
\begin{NiceTabular}{lcccccccc}
\toprule 
 \multirow{2}{*}{Methods} & \multicolumn{4}{c}{RealBlur-J} & \multicolumn{4}{c}{RealBlur-R}  \\
 \cmidrule(lr){2-5}
 \cmidrule(lr){6-9}
 & \multirow{1}{*}{PSNR$\uparrow$} & \multirow{1}{*}{SSIM$\uparrow$} 
 & \multirow{1}{*}{LPIPS$\downarrow$} & \multirow{1}{*}{FID$\downarrow$} 
 & \multirow{1}{*}{PSNR$\uparrow$} & \multirow{1}{*}{SSIM$\uparrow$} & \multirow{1}{*}{LPIPS$\downarrow$} & \multirow{1}{*}{FID$\downarrow$} \\
 \midrule
 SRN-DeblurNet & \underline{31.38} & \underline{0.9091} & - & - & \underline{38.65} & 0.9652 & - & - \\
 DeblurGAN-v2 & 29.69 & 0.8703 & - & - & 36.44 & 0.9347 & - & - \\
 \midrule
 Baseline cDDPM & 30.69 & 0.9043 & \textbf{0.220} & \textbf{15.17} & 37.71 & \underline{0.9777} & \underline{0.126} & \underline{14.46} \\
 + RestoreGrad (ours) & \textbf{31.51} & \textbf{0.9095} & \underline{0.224} & \underline{15.53} & \textbf{38.78} & \textbf{0.9796} & \textbf{0.122} & \textbf{13.61} \\
 \bottomrule
\end{NiceTabular}
}
\\
*Bold text for best and underlined text for second best values.
}
\end{small}
\end{table}

\vspace{0.2cm}

\noindent\textbf{Experiments on Image Super-Resolution:}
We further study the benefits of RestoreGrad over the baseline conditional DDPM (cDDPM) model on image super-resolution tasks with the DIV2K dataset \citep{Agustsson_2017_CVPR_Workshops,Timofte_2017_CVPR_Workshops}. We compare RestoreGrad with the baseline cDDPM model (the same architecture of the patch-based DDPM of WeatherDiff \citep{ozdenizci2023restoring}) for $\times 2$ and $\times 4$ downscale factor subsets (with bicubic downgrading operators). There are 800 images for training and 100 images for validation in each subset. For both subsets, we trained a baseline cDDPM and the RestoreGrad models for 2000 epochs on the training set and evaluated their performance on the corresponding validation set. The results are presented in Table \ref{table: ir sota comp sr}, where we can see that except for the LPIPS metric, RestoreGrad is more beneficial then the baseline cDDPM in terms of achieving better scores in the other three metrics.

\begin{table}[!th]
\centering
\begin{small}{
\setlength{\tabcolsep}{5pt} % let TeX compute the intercolumn space
\caption{Comparison of baseline conditional DDPM (cDDPM) and the RestoreGrad on image super-resolution tasks.}
\vspace{0.1cm}
\label{table: ir sota comp sr}
\resizebox{0.7\columnwidth}{!}{%
\begin{NiceTabular}{lcccccccc}
\toprule 
 \multirow{2}{*}{Methods} & \multicolumn{4}{c}{DIV2K $\times$2} & \multicolumn{4}{c}{DIV2K $\times$4}  \\
 \cmidrule(lr){2-5}
 \cmidrule(lr){6-9}
 & \multirow{1}{*}{PSNR $\uparrow$} & \multirow{1}{*}{SSIM $\uparrow$} & \multirow{1}{*}{LPIPS $\downarrow$} & \multirow{1}{*}{FID $\downarrow$} & \multirow{1}{*}{PSNR $\uparrow$} & \multirow{1}{*}{SSIM $\uparrow$} & \multirow{1}{*}{LPIPS $\downarrow$} & \multirow{1}{*}{FID $\downarrow$} \\
 \midrule
 Baseline cDDPM & 27.40 & 0.9291 & \textbf{0.127} & 7.577 & 25.18 & 0.8064 & \textbf{0.269} & 7.849 \\
 + RestoreGrad (ours) & \textbf{27.56} & \textbf{0.9341} & 0.136 & \textbf{7.547} & \textbf{25.56} & \textbf{0.8228} & 0.290 & \textbf{7.839} \\
 \bottomrule
\end{NiceTabular}
}
}
\\
\vspace{0.1cm}
*Better values are indicated with bold text.
\end{small}
\end{table}


\vspace{0.2cm}

\noindent\textbf{More Image Restoration Examples:}
We provide more examples in Figures \ref{fig: ir example randropdiff}, \ref{fig: ir example snow 2}, \ref{fig: ir example rainfog},  \ref{fig: ir example raindrop}, \ref{fig: ir example raindrop 2} for comparing our RestoreGrad with the baseline DDPM approaches (i.e., RainDropDiff, WeatherDiff) of \citet{ozdenizci2023restoring}. The restored images of WeatherDiff were obtained by using the trained model weights provided by \citet{ozdenizci2023restoring} at \url{https://github.com/IGITUGraz/WeatherDiffusion}. We also provide examples of image deblurring in Figures \ref{fig: ir example blur j} and \ref{fig: ir example blur j 2}.

\vspace{0.2cm}

\begin{figure}[!hp]
    \centering
    \includegraphics[width=0.85\linewidth]{Figs/ir_example.pdf}
    \caption{Restored images by RainDropDiff \citep{ozdenizci2023restoring} and RestoreGrad (ours) for a test sample from the RainDrop dataset.} 
\label{fig: ir example randropdiff}
\end{figure}

\vspace{0.5cm}

\begin{figure}[!tp]
    \centering
    \includegraphics[width=0.85\linewidth]{Figs/ir_example_snow_2.pdf}
    \caption{Image restoration examples using a test image taken from the Snow100K-L test set.} 
\label{fig: ir example snow 2}
\end{figure}

\vspace{0.5cm}

\begin{figure}[!tp]
    \centering
    \includegraphics[width=0.85\linewidth]{Figs/ir_example_rainfog.pdf}
    \caption{Image restoration examples using a test image taken from the Outdoor-Rain test set.} 
\label{fig: ir example rainfog}
\end{figure}

\vspace{0.5cm}

\begin{figure}[!tp]
    \centering
    \includegraphics[width=0.85\linewidth]{Figs/ir_example_raindrop.pdf}
    \caption{Image restoration examples using a test image taken from the RainDrop test set.} 
\label{fig: ir example raindrop}
\end{figure}

\vspace{0.5cm}

\begin{figure}[!tp]
    \centering
    \includegraphics[width=0.85\linewidth]{Figs/ir_example_raindrop_2.pdf}
    \caption{Image restoration examples using a test image taken from the RainDrop test set.} 
\label{fig: ir example raindrop 2}
\end{figure}

\vspace{0.5cm}

\begin{figure}[!tp]
    \centering
    \includegraphics[width=0.85\linewidth]{Figs/ir_example_blur_J_2.pdf}
    \caption{Image deblurring examples using a test image taken from the RealBlur test set.} 
\label{fig: ir example blur j}
\end{figure}

\vspace{0.5cm}

\begin{figure}[!tp]
    \centering
    \includegraphics[width=0.85\linewidth]{Figs/ir_example_blur_J.pdf}
    \caption{Image deblurring examples using a test image taken from the RealBlur test set.} 
\label{fig: ir example blur j 2}
\end{figure}

