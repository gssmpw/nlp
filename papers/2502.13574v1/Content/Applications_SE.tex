\section{Experiments}
\label{sec: exp}

\subsection{Application to Speech Enhancement (SE)}

\subsubsection{Experimental Setup}

\noindent\textbf{Dataset:} We validate performance on the benchmark SE dataset \textit{VoiceBank+DEMAND} \citep{valentini2016investigating}, consisting of clean speech clips collected from the VoiceBank corpus \citep{veaux2013voice}, mixed with ten types of noise profiles from the DEMAND database \citep{thiemann2013diverse}. Specifically, the training utterances from VoiceBank are artificially contaminated with the noise samples from DEMAND at 0, 5, 10, and 15 dB signal-to-noise ratio (SNR) levels, amounting to 11,572 utterances. The testing utterances are mixed with different noise samples at 2.5, 7.5, 12.5, and 17.5 dB  SNR levels, amounting to 824 utterances. 

\noindent\textbf{Evaluation Metrics:} We consider: \textbf{PESQ:} Perceptual Evaluation of Speech Quality \citep{itu862}. \textbf{SI-SNR:} Scale-Invariant SNR \citep{le2019sdr}. \textbf{SSNR}: Segmental SNR \cite{hu2007evaluation}. \textbf{CSIG, CBAK, COVL:} Mean-opinion-score predictors of signal distortion, background-noise intrusiveness, and overall signal quality, respectively \citep{hu2007evaluation}.

\noindent\textbf{Models:} The following models are compared:
\begin{itemize}[leftmargin=*]
\vspace{-0.15cm}
    \item \textbf{Baseline DDPM}: We adopt the CDiffuSE (Base) model from \citet{lu2022conditional}, which is based on DiffWave \citep{kong2020diffwave} with 4.28M learnable parameters. 
    \item \textbf{PriorGrad}: We implement the PriorGrad \citep{lee2021priorgrad} on top of CDiffuSE by changing the prior distribution from $\mathcal{N}(\mathbf{0},\mathbf{I})$ to $\mathcal{N}(\mathbf{0},\boldsymbol{\Sigma}_{\text{y}})$, where $\boldsymbol{\Sigma}_{\text{y}}$ is the covariance of the data-dependent prior computed based on the conditioner $\mathbf{y}$, using the rule-based estimation approach for the application to vocoder in \citet{lee2021priorgrad}.
    \item \textbf{RestoreGrad}: We incorporate Prior Net and Posterior Net on top of CDiffuSE. Both modules adopt the ResNet-20 architect \citep{he2016deep} suitably modified to 1-D convolutions for waveform processing, each has only 93K learnable parameters (only 2\% of the CDiffuSE model).
\end{itemize}

\noindent\textbf{Configurations:} 
We adopted the basic configurations same as in \citet{lu2022conditional}. The waveforms were processed at 16kHz sampling rate. The number of forward diffusion steps was $T=50$. The variance schedule was $\beta_t\in[10^{-4}, 0.035]$, linearly spaced. The batch size was 16. The fast sampling scheme in \citet{kong2020diffwave} was used in the reverse processes with $S=6$ steps to reduce inference complexity. The inference variance schedule was $\beta^{\text{infer}}_t=[10^{-4}, 10^{-3}, 0.01, 0.05, 0.2, 0.35]$. Adam optimizer \citep{kingma2014adam} was utilized with a learning rate of $2\times 10^{-4}$. We set $\eta=0.1$ and $\lambda=0.5$ for (\ref{eq: elbo of restoregrad}). The models were trained on one NVIDIA Tesla V100 GPU (32 GB CUDA memory) and finished 96 epochs in 1 day.

\subsubsection{Results}

\noindent\textbf{Improved Model Convergence:} 
As shown in Figure \ref{fig: training_curve} (test set performance), RestoreGrad shows better convergence behavior over PriorGrad (handcrafted prior) and CDiffuSE (standard Gaussian prior). For example, \textit{PriorGrad reaches 2.4 in PESQ at 96 epochs, whereas RestoreGrad reaches it in (roughly) 10 epochs, indicating a 10$\times$ speed-up.} The results suggest that jointly learning the prior distribution can be beneficial for conditional DDPMs.

\noindent\textbf{Robustness to Reduced Number of Reverse Steps in Inference:} 
RestoreGrad can potentially reduce the inference complexity. In Figure \ref{fig: tolerance to reduced interence sampling steps}, we show how the trained diffusion models tolerate reduction in the number of inference steps. In each model, we trained the network for 96 epochs and then inferenced with $S=3$ reverse steps to compare with the originally adopted $S=6$ steps in \citet{lu2022conditional}. The noise schedule for $S=3$ was $\beta^{\text{infer}}_t=[0.05, 0.2, 0.35]$, a subset of the $S=6$ schedule that resulted in best performance. We can see that the baseline DDPM is most sensitive to the step reduction, while PriorGrad shows certain tolerance as leveraging a closer-to-data prior distribution. \textit{Finally, RestoreGrad barely degrades with reduced sampling steps, echoing that a better prior has been obtained as it recovers higher fidelity signal even in fewer reverse steps}. 

\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{Figs/tolerance_reduce_infer_two_metrics.pdf}
    \vspace{-0.85cm}
    \caption{Robustness to the reduction in reverse sampling time steps for inference.} 
\label{fig: tolerance to reduced interence sampling steps}
\vspace{-0.41cm}
\end{figure}

\noindent\textbf{Effect of $\eta$:} 
An important factor in our prior learning scheme is the regularization weight $\eta$ for $\mathcal{L}_{\text{LR}}$ of the training loss. An appropriate value of $\eta$ should be large enough to properly regularize the learned latent space for avoiding instability, while not adversely affecting signal reconstruction performance. It is thus interesting to see how the performance varies with the choice of $\eta$. \textit{Empirically, we found the overall SE performance not to be very sensitive to the value of $\eta$ across a wide range,} as shown in Figure \ref{fig: eta effect}: roughly in the range of $[10^{-2}, 10]$ of the $\eta$ value we see that RestoreGrad gives better results over both PriorGrad and CDiffuSE.

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.8\linewidth]{Figs/eta_effect.pdf}
    \vspace{-0.3cm}
    \caption{Effect of latent regularization weight $\eta$ for $\mathcal{L}_{\text{LR}}$ on SE. } 
\label{fig: eta effect}
\vspace{-0.4cm}
\end{figure}

\noindent\textbf{Comparison to Fully-Trained CDiffuSE:} 
We present in Table \ref{table: se sota comp 1} more detailed comparison of RestoreGrad with the baseline CDiffuSE. Here, the scores of CDiffuSE were directly taken from the results reported in \citet{lu2022conditional} where the model has been fully trained for 445 epochs. For PriorGrad and RestoreGrad we report the mean$\pm$std computed based on results of 10 independent samplings. \textit{We can see that with RestoreGrad applied, the SE model can achieve better performance over the baseline CDiffuSE by only training for 96 epochs (4.6 times lesser than the baseline) in all the metrics}. In addition, halving the number of reverse steps in inference still maintains better performance than the fully-trained CDiffuSE and also the PriorGrad. 

\noindent\textbf{Signal Quality and Encoder Size Trade-Offs / Complexity Analysis:} We further present results using three different model sizes (24K, 93K, 370K) for the Prior and Posterior Nets (i.e., encoders) in Table \ref{table: se quality complexity trade-off}, along with latency and GPU memory usage. The results clearly show that the restored speech quality improves with an increasing encoder size. \textit{This indicates that there is a trade-off between the restoration signal quality and encoder model complexity}. Notably, \textit{the latency and memory usage of the encoder modules are relatively small compared to the DDPM processing (decoding)}, suggesting that RestoreGrad is capable of achieving improved performance without incurring considerable increase in complexity compared to the adopted DDPM model.

\textbf{Posterior Net Helps:} Finally, we validate the benefits brought by employing Posterior Net in the training phase by comparing with the RestoreGrad models trained without Posterior Net as (\ref{eq: elbo of restoregrad no post net}) for some $\eta$. For fairness, all models were trained with 96 epochs, inferred with 6 steps. In Table \ref{table: se no post net}, we observe that \textit{RestoreGrad achieves better results with Posterior Net than without it, indicating the benefits of being informed of the target $\mathbf{x}_0$ by utilizing the Posterior Net}. We also observe that without regularizing the latent space (i.e., with $\eta=0$) it could lead to training divergence.

\begin{table}[!t]
\centering
\begin{small}
\setlength{\tabcolsep}{2pt} % let TeX compute the intercolumn space
\caption{Comparison with the fully-trained CDiffuSE model performance reported in \citet{lu2022conditional}.}
\vspace{0.15cm}
\label{table: se sota comp 1}
\resizebox{1\linewidth}{!}{%
\begin{NiceTabular}{lccccccc}
\toprule 
 \multirow{2}{*}{Methods} & \multirow{1}{*}{\# train} & \multirow{1}{*}{\# infer} & \multirow{2}{*}{PESQ $\uparrow$} & \multirow{2}{*}{CSIG $\uparrow$} & \multirow{2}{*}{CBAK $\uparrow$}& \multirow{2}{*}{COVL $\uparrow$} & \multirow{2}{*}{SI-SNR $\uparrow$} \\
 & \multirow{1}{*}{epochs} & \multirow{1}{*}{steps} & & & & & \\
 \midrule
 CDiffuSE & 445 & 6 & 2.44 & 3.66 & 2.83 & 3.03 & - \\
 \midrule
 + PriorGrad & 96 & 6 & 2.42$\pm$3e-3 & 3.67$\pm$2e-3 & 2.93$\pm$1e-3 & 3.03$\pm$2e-3 & 14.21$\pm$2e-3 \\
 \midrule
 \multirow{1}{*}{+ RestoreGrad}  & \multirow{2}{*}{96} & 6 & \textbf{2.51}$\pm$6e-4 & \textbf{3.80}$\pm$4e-4 & \textbf{3.00}$\pm$3e-4 & \textbf{3.14}$\pm$5e-4 & \textbf{14.74}$\pm$3e-4 \\
 \multirow{1}{*}{\quad\quad(ours)} &  & 3 & \underline{2.50}$\pm$3e-4 & \underline{3.75}$\pm$2e-4 & \underline{2.99}$\pm$2e-4 & \underline{3.11}$\pm$3e-4 & \underline{14.65}$\pm$2e-4 \\
 \bottomrule
\end{NiceTabular}
}
\\
\vspace{0.1cm}
\scriptsize
*Bold text for best and underlined text for second best values.
\end{small}
\vspace{-0.25cm}
\end{table}

\begin{table}[!t]
\centering
\begin{small}
\setlength{\tabcolsep}{1.5pt} % let TeX compute the intercolumn space
\caption{SE comparison of RestoreGrad models using encoder modules of different sizes and the corresponding latency and GPU memory usage (measured on one NVIDIA Tesla V100 GPU).}
\vspace{0.15cm}
\label{table: se quality complexity trade-off}
\resizebox{\linewidth}{!}{%
\begin{NiceTabular}{lcccccccc}
\toprule
  \multirow{2}{*}{Encoder size} & \multirow{2}{*}{PESQ$\uparrow$} & \multirow{2}{*}{COVL$\uparrow$} & \multirow{2}{*}{SSNR$\uparrow$} & \multirow{2}{*}{SI-SNR$\uparrow$} & \multicolumn{2}{c}{Proc. Time (msec)} & \multicolumn{2}{c}{Memory} \\
  \cmidrule(lr){6-7}
  \cmidrule(lr){8-9} 
  \multirow{1}{*}{(\# params)} & & & & & \multirow{1}{*}{DDPM} & \multirow{1}{*}{Encoder} & \multirow{1}{*}{DDPM} & \multirow{1}{*}{Encoder} \\
\midrule
    Tiny (24K) & 2.48 & 3.11  & 5.10 & 13.74 & 229.5 & 4.5 & 74.4M & 4.8M \\
    Base (93K) & 2.51 & 3.14  & 5.92 &  14.74 & 230.1 & 5.1 & 74.5M & 7.7M \\
    Large (370K) & 2.54 & 3.16 & 6.15 & 15.01 & 231.7 & 6.0 & 74.7M & 13.6M\\
\bottomrule
\end{NiceTabular}
}
\end{small}
\vspace{-0.25cm}
\end{table}

\begin{table}[!t]
\centering
\begin{small}
\setlength{\tabcolsep}{1.2pt} % let TeX compute the intercolumn space
\caption{Performance of RestoreGrad models trained with and without using Posterior Net.}
\vspace{0.1cm}
\label{table: se no post net}
\resizebox{0.98\linewidth}{!}{%
\begin{NiceTabular}{lcccc}
\toprule
  SE models & PESQ$\uparrow$ & COVL$\uparrow$ & SSNR$\uparrow$ & SI-SNR$\uparrow$ \\
\midrule
    CDiffuSE (trained for 96 epochs) & 2.32 & 2.89 & 3.94 & 11.84 \\
    + PriorGrad & 2.42 & 3.03 & 5.53 & 14.21 \\
    + RestoreGrad & \textbf{2.51} & \textbf{3.14} & \textbf{5.92} & \textbf{14.74} \\
\midrule
    + RestoreGrad w/o Posterior Net ($\eta=0$) & --- & training & diverged & --- \\
    + RestoreGrad w/o Posterior Net ($\eta=0.01$) & 2.47 & 3.08 & 4.96 &11.22 \\
    + RestoreGrad w/o Posterior Net ($\eta=1$) & 2.48 & 3.12 & 5.11 & 13.29 \\
\bottomrule
\end{NiceTabular}
}
\\
\vspace{0.1cm}
\scriptsize
*Best values in bold.
\end{small}
\vspace{-0.25cm}
\end{table}
