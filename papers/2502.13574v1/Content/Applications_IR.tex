\subsection{Application to Image Restoration (IR)}
\label{sec: ir on weathers}

\subsubsection{Experimental Setup}

\noindent\textbf{Dataset:} Following \citet{ozdenizci2023restoring}, we consider the IR task of recovering clean images from their degraded versions contaminated by synthesized noises corresponding to different weather conditions. Two datasets are considered, where one is a weather-specific dataset called \textit{RainDrop} \citep{qian2018attentive} and the other is a multi-weather dataset named \textit{AllWeather} \citep{valanarasu2022transweather}. The RainDrop dataset consists of images captured with raindrops on the camera sensor which obstruct the view. It has 861 training images with synthetic raindrops, and a test set of 58 images dedicated for quantitative evaluations. The AllWeather dataset is a curated training dataset from \citet{valanarasu2022transweather}, which has 18,069 samples composed of subsets of training images from Snow100K \citep{liu2018desnownet}, Outdoor-Rain \citep{li2019heavy} and RainDrop \citep{qian2018attentive}, in order to create a balanced training set across three weather conditions. 

\noindent\textbf{Evaluation Metrics:} Quantitative evaluations of restored images are performed via Peak Signal-to-Noise Ratio (\textbf{PSNR}) \citep{huynh2008scope}, Structural SIMilarity (\textbf{SSIM}) \citep{wang2004image}, %based on the luminance channel Y of the YCbCr color space following \citet{ozdenizci2023restoring}, 
Learned Perceptual Image Patch Similarity (\textbf{LPIPS}) \citep{zhang2018unreasonable}, and Fr√©chet Inception Distance (\textbf{FID}) \citep{heusel2017gans}.

\noindent\textbf{Models:} The following IR models are compared:
\begin{itemize}[leftmargin=*]
\vspace{-0.15cm}
    \item \textbf{Baseline DDPMs}: We consider the $\text{RainDropDiff}_{64}$ and $\text{WeatherDiff}_{64}$ in \citet{ozdenizci2023restoring} trained on the RainDrop and AllWeather datasets, respectively, as baseline DDPMs. Our work is based on the implementation provided by \citet{ozdenizci2023restoring}. 
    \vspace{-0.45cm}
    \item \textbf{RestoreGrad}: We incorporate the encoder modules, Prior Net and Posterior Net, on top of the baseline DDPM. Both encoder modules adopt the ResNet-20 architect \citep{he2016deep} with only 0.27M learnable parameters, significantly smaller ($<0.3\%$) than the baseline DDPM model.
\end{itemize}

\noindent\textbf{Configurations:} 
We used Adam optimizer with a learning rate of $2\times 10^{-5}$. An exponential moving average with a weight of 0.999 was applied. We used $T=1000$ and linear noise schedule $\beta_t\in[10^{-4}, 0.02]$, same as \citet{ozdenizci2023restoring}. A batch size of 4 was used. The models were trained on two NVIDIA Tesla V100 GPUs of 32 GB CUDA memory and finished training for 9,261 epochs on the RainDrop dataset in 12 days and 887 epochs on the AllWeather dataset in 21 days.

\subsubsection{Results}

\noindent\textbf{Model Convergence:} As presented in Figure \ref{fig: training_curve} (test set performance), RestoreGrad demonstrates faster convergence and better restored image quality over the baseline DDPM (RainDropDiff). For example, \textit{RainDropDiff reaches 0.143 in LPIPS at 9.2k epochs, while RestoreGrad reaches it in 1.8k epochs only, indicating a 5$\times$ speed-up} due to the effectiveness of the prior learning scheme.

\noindent\textbf{Comparison with Existing IR Models:}
We compare our method on multi-weather cases in Table \ref{table: ir sota comp allweather} with All-in-One \citep{li2020all} and TransWeather \citep{valanarasu2022transweather}, where the models were trained on the AllWeather dataset and tested on the three weather-specific test sets. The numbers of the compared models were taken from \citet{ozdenizci2023restoring}, where the WeatherDiff was trained for 1,775 epochs and inferenced with $S=25$ steps. \textit{Our RestoreGrad was trained for only 887 epochs (2$\times$ fewer than WeatherDiff) and inferenced with $S=10$ steps to already achieve the best performance in almost all test schemes}. 

\noindent\textbf{IR Example:} Figure \ref{fig: ir example} presents examples of restored images by the models. It can be seen that RestoreGrad is able to better recover the original image, \textit{especially in regions of the blue and red boxes where the baseline WeatherDiff fails to remove the snow obstructions}. The higher PSNR and SSIM scores of RestoreGrad also reflect the improvements.

\begin{table*}[!t]
\vspace{-0.1cm}
\centering
\begin{small}
\setlength{\tabcolsep}{1pt} % let TeX compute the intercolumn space
\caption{Comparison with existing IR models. The multi-weather (MW) models were trained on the AllWeather training set \citep{valanarasu2022transweather} and tested on three different weather types: Snow100K-L \citep{liu2018desnownet}, Outdoor-Rain \citep{li2019heavy}, and RainDrop \citep{qian2018attentive}. Several weather-specific (WS) models that were trained on individual weather types are also presented for reference.}
\vspace{0.1cm}
\label{table: ir sota comp allweather}
\resizebox{\linewidth}{!}{%
\begin{NiceTabular}{l|lcc||lcc||lcc}
\toprule 
 \multirow{2}{*}{Type} & \multirow{2}{*}{Methods} & \multicolumn{2}{c||}{Snow100K-L} & \multirow{2}{*}{Methods} & \multicolumn{2}{c||}{Outdoor-Rain} & \multirow{2}{*}{Methods} & \multicolumn{2}{c}{RainDrop}  \\
 \cmidrule(lr){3-4}
 \cmidrule(lr){6-7}
 \cmidrule(lr){9-10}
 & & \multirow{1}{*}{PSNR $\uparrow$} & \multirow{1}{*}{SSIM $\uparrow$} & & \multirow{1}{*}{PSNR $\uparrow$} & \multirow{1}{*}{SSIM $\uparrow$} & & \multirow{1}{*}{PSNR $\uparrow$} & \multirow{1}{*}{SSIM $\uparrow$} \\
 \midrule
 \multirow{4}{*}{WS} & RESCAN \citep{li2018recurrent} &  26.08 & 0.8108 & HRGAN \citep{li2019heavy} & 21.56 &   0.8550 & RaindropAttn \citep{quan2019deep} & 31.44 &  0.9263 \\
 & DesnowNet \citep{liu2018desnownet} &   27.17 & 0.8983 & PCNet \citep{jiang2021rain} &  26.19 &  0.9015 & AttentiveGAN \citep{qian2018attentive} & 31.59 & 0.9170 \\
 & DDMSNet \citep{zhang2021deep} &   28.85 & 0.8772 & MPRNet \citep{zamir2021multi} &  28.03 & 0.9192 & IDT \citep{xiao2022image} & \underline{31.87} &  0.9313 \\
 & SnowDiff \citep{ozdenizci2023restoring} &  \underline{30.43} &  \underline{0.9145} & RainHazeDiff \citep{ozdenizci2023restoring} &  28.38 & \underline{0.9320} & RainDropDiff \citep{ozdenizci2023restoring} & \textbf{32.29}&  \textbf{0.9422} \\
 \midrule
 \multirow{4}{*}{MW} & All-in-One \citep{li2020all} &  28.33 & 0.8820 & All-in-One \citep{li2020all} & 24.71 &  0.8980 & All-in-One \citep{li2020all} & 31.12 & 0.9268 \\
 & TransWeather \citep{valanarasu2022transweather} &  29.31 & 0.8879 & TransWeather \citep{valanarasu2022transweather} & 28.83 & 0.9000 & TransWeather \citep{valanarasu2022transweather} & 30.17 & 0.9157 \\
   \cmidrule(lr){2-4}
   \cmidrule(lr){5-7}
   \cmidrule(lr){8-10}
   &  WeatherDiff \citep{ozdenizci2023restoring} & 30.09 & 0.9041 & WeatherDiff \citep{ozdenizci2023restoring} & \underline{29.64} & 0.9312 & WeatherDiff \citep{ozdenizci2023restoring} & 30.71 & 0.9312 \\
    &  + RestoreGrad (ours) & \textbf{30.82} & \textbf{0.9159} & + RestoreGrad (ours) & \textbf{30.83} & \textbf{0.9411} & + RestoreGrad (ours) & 31.78 & \underline{0.9394} \\
 \bottomrule
\end{NiceTabular}
}
\\
\vspace{0.1cm}
\scriptsize
*Bold text for best and underlined text for second best values.
\end{small}
\vspace{-0.15cm}
\end{table*}

\begin{figure*}[!tp]
    \centering
    \includegraphics[width=0.85\linewidth]{Figs/ir_example_snow.pdf}
    \vspace{-0.25cm}
    \caption{Image restoration examples using a test image taken from the Snow100K-L test set. We provide more examples, including other degradations (\textbf{desnowing}, \textbf{deraining}, \textbf{raindrop removal} and \textbf{deblurring}) in Appendix \ref{sec: additional results on IR appendix}.} 
\label{fig: ir example}
\vspace{-0.2cm}
\end{figure*}

\noindent\textbf{Other IR Tasks:}
Our method demonstrates the advantages of adopting learnable priors also in image \textit{deblurring} and \textit{super-resolution} tasks, suggesting its \textit{generality}. We refer the reader to Appendix \ref{sec: additional results on IR appendix} for the results and discussion.