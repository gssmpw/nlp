\section{Proposed Method: Integrating DDPM and VAE for Learnable Diffusion Prior}

We start with the conditional VAE \citep{sohn2015learning} formulation to maximize the conditional data log-likelihood, $\log p(\mathbf{x}_0|\mathbf{y})=\log\int p(\mathbf{x}_0,\boldsymbol{\epsilon}|\mathbf{y}) d\boldsymbol{\epsilon}$, where $\boldsymbol{\epsilon}$ is an introduced latent variable. To avoid intractable integral, in VAEs an ELBO is utilized as the surrogate objective by introducing an approximate posterior $q(\boldsymbol{\epsilon}|\mathbf{x}_0,\mathbf{y})$ \citep{harvey2022conditional}:
\begin{equation}
\begin{aligned}
    \log p(\mathbf{x}_0|\mathbf{y})\geq&\underbrace{\mathbb{E}_{q(\boldsymbol{\epsilon}|\mathbf{x}_0,\mathbf{y})}\left[\log p(\mathbf{x}_0|\mathbf{y},\boldsymbol{\epsilon})\right]}_{\text{reconstruction term}} \\ 
    &- \underbrace{D_{\text{KL}}\left(q(\boldsymbol{\epsilon}|\mathbf{x}_0,\mathbf{y}) || p(\boldsymbol{\epsilon}|\mathbf{y})\right)}_{\text{prior matching term}}.
\label{eq: vae original elbo}
\end{aligned}
\end{equation}
The \textit{reconstruction} and \textit{prior matching} terms are typically realized by an encoder-decoder architecture with $\boldsymbol{\epsilon}$ being the bottleneck representation sampled from the latent distribution. VAEs generally benefit from learnable latent spaces for good modeling efficiency. However, their generation capabilities often lag behind DDPMs that employ an iterative, more sophisticated decoding (reconstruction) process.

In this work, our aim is to embrace the best of both worlds, i.e., \textit{remarkable generation ability (DDPM) and modeling efficiency (VAE) to achieve improved output signal quality and training/sampling efficiency simultaneously.} To this end, we introduce the following lower bound:

\vspace{0.2cm}

\begin{proposition}[Incorporation of diffusion process into VAE]
    By introducing a sequence of hidden variables $\mathbf{x}_{1:T}$, under the setup of conditional diffusion models where the Markov Chain assumption is employed on the forward process $q(\mathbf{x}_{1:T}|\mathbf{x}_0)\coloneq\prod_{t=1}^Tq(\mathbf{x}_t|\mathbf{x}_{t-1})$ and the reverse process $p_\theta(\mathbf{x}_{0:T}|\mathbf{y})\coloneq p(\mathbf{x}_T)\prod_{t=1}^Tp_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t,\mathbf{y})$ parameterized by a DDPM $\theta$, and assuming that $\mathbf{x}_T=\boldsymbol{\epsilon}$ (i.e., the latent noise of DDPM samples from the VAE latent distribution), we have the lower bound on $\log p(\mathbf{x}_0|\mathbf{y},\boldsymbol{\epsilon})$ in the reconstruction term of the VAE as:
    \begin{equation}
        \log p_\theta(\mathbf{x}_0|\mathbf{y},\boldsymbol{\epsilon})\geq\mathbb{E}_{q(\mathbf{x}_{1:T}|\mathbf{x}_0)}\left[\log\frac{p_\theta({\mathbf{x}_{0:T}}|\mathbf{y})}{q(\mathbf{x}_{1:T}|\mathbf{x}_0)}\right],
    \label{eq: cddpm lower bound}
    \end{equation}
    which is the ELBO of the conditional DDPM. 
\label{proposition: 1}
\end{proposition}
The proof (based on Markov Chain property) is provided in Appendix \ref{sec: appendix proof prop 1}. Proposition \ref{proposition: 1} suggests a seamless integration of the DDPM into the VAE framework as the decoder module for improved generation capabilities. 

Having incorporated the DDPM as the decoder, we now discuss the encoder part (the prior matching term in (\ref{eq: vae original elbo})) of the framework. A straightforward design could be using a network $\psi$ (\textit{Prior Net}) to parameterize the prior distribution as $p_\psi(\boldsymbol{\epsilon}|\mathbf{y})$, while assuming the posterior to be a fixed form of distribution like the standard Gaussian. However, this may in turn discard any useful information from between $\mathbf{x}_0$ and $\mathbf{y}$. To take advantage of the adequate correlation of $\mathbf{x}_0$ and $\mathbf{y}$ as in signal restoration applications, we propose to also parameterize the posterior distribution with another network $\phi$ (\textit{Posterior Net}), to incorporate richer information about the target signal distribution into the learning of the prior. 
Together with (\ref{eq: cddpm lower bound}), we obtain the \textbf{new lower bound} of the conditional data log-likelihood:
\begin{equation}
\begin{aligned}
    \log p(\mathbf{x}_0|\mathbf{y})\geq&\mathbb{E}_{q_\phi(\boldsymbol{\epsilon}|\mathbf{x}_0,\mathbf{y})}\Biggr[\underbrace{\mathbb{E}_{q(\mathbf{x}_{1:T}|\mathbf{x}_0)}\left[\log\frac{p_{\theta}(\mathbf{x}_{0:T}|\mathbf{y})}{q(\mathbf{x}_{1:T}|\mathbf{x}_0)}\right]}_{\text{conditional DDPM}}\Biggr] \\
    &- D_{\text{KL}}\bigr(\underbrace{q_{\phi}(\boldsymbol{\epsilon}|\mathbf{x}_0,\mathbf{y})}_{\text{Posterior Net}} || \underbrace{p_{\psi}(\boldsymbol{\epsilon}|\mathbf{y})}_{\text{Prior Net}}\bigr).
\label{eq: vae elbo}
\end{aligned}
\end{equation}
The two-encoder design is inspired by \citet{kohl2018probabilistic} for image segmentation with traditional U-Nets. Here, we adopt the idea in the context of DDPM for signal restoration. Note that the Posterior Net is exclusively used in training to help the Prior Net learn a more informative prior.

Based on (\ref{eq: vae elbo}), we introduce the modified ELBO for the training objective of RestoreGrad:

\begin{figure*}[!t]  
    \centerline{\includegraphics[width=0.99\linewidth]{Figs/restoregrad.pdf}}
    \vspace{-0.5cm}
    \caption{Proposed RestoreGrad. During training, the conditional DDPM $\theta$, Prior Net $\psi$, and Posterior Net $\phi$ are jointly optimized by (\ref{eq: elbo of restoregrad}). During inference, the DDPM $\theta$ samples the latent noise $\boldsymbol{\epsilon}$ from the jointly learned prior distribution to synthesize the clean signal. (We summarize the algorithm details in Appendix \ref{sec: algorithms}.)} 
\label{fig: restoregrad}
\vspace{-0.25cm}
\end{figure*} 

\vspace{0.2cm}

\begin{proposition}[RestoreGrad]
    Assume the prior and posterior distributions are both zero-mean Gaussian, parameterized as $p_{\psi}(\boldsymbol{\epsilon}|\mathbf{y})=\mathcal{N}(\boldsymbol{\epsilon};\mathbf{0},\boldsymbol{\Sigma}_{\text{prior}}(\mathbf{y};\psi))$ and $q_{\phi}(\boldsymbol{\epsilon}|\mathbf{x}_0, \mathbf{y})=\mathcal{N}(\boldsymbol{\epsilon};\mathbf{0},\boldsymbol{\Sigma}_{\text{post}}(\mathbf{x}_0,\mathbf{y};\phi))$, respectively, where the covariances are estimated by the Prior Net $\psi$ (taking $\mathbf{y}$ as input) and Posterior Net $\phi$ (taking both $\mathbf{x}_0$ and $\mathbf{y}$ as input). Let us simply use $\boldsymbol{\Sigma}_{\text{prior}}$ and $\boldsymbol{\Sigma}_{\text{post}}$ hereafter to refer to $\boldsymbol{\Sigma}_{\text{prior}}(\mathbf{y};\psi)$ and $\boldsymbol{\Sigma}_{\text{post}}(\mathbf{x}_0,\mathbf{y};\phi)$ for concise notation. 
    Then, with the direct sampling property in the forward path $\mathbf{x}_t=\sqrt{\bar{\alpha}_t}\mathbf{x}_0+\sqrt{1-\bar{\alpha}_t}\boldsymbol{\epsilon}$ at arbitrary timestep $t$ where $\boldsymbol{\epsilon}\sim q_{\phi}(\boldsymbol{\epsilon}|\mathbf{x}_0,\mathbf{y})$, and assuming the reverse process has the same covariance as the true forward process posterior conditioned on $\mathbf{x}_0$, by utilizing the conditional DDPM $\boldsymbol{\epsilon}_\theta(\mathbf{x}_t,\mathbf{y},t)$ as the noise estimator of the true noise $\boldsymbol{\epsilon}$, we have the modified ELBO associated with (\ref{eq: vae elbo}): 
    \begin{equation}
    \begin{aligned}
        -ELBO & = \underbrace{\frac{\bar{\alpha}_T}{2}\mathbb{E}_{\mathbf{x}_0}\norm{\mathbf{x}_0}^2_{\boldsymbol{\Sigma}^{-1}_{\text{post}}}+\frac{1}{2}\log\abs{\boldsymbol{\Sigma}_{\text{post}}}}_{\text{Latent Regularization (LR) terms}} \\
        +&\underbrace{\sum_{t=1}^{T}\gamma_t\mathbb{E}_{(\mathbf{x}_0,\mathbf{y}),\boldsymbol{\epsilon}\sim \mathcal{N}(\mathbf{0},\boldsymbol{\Sigma}_{\text{post}})}\norm{\boldsymbol{\epsilon}-\boldsymbol{\epsilon}_\theta(\mathbf{x}_t,\mathbf{y},t)}^2_{\boldsymbol{\Sigma}^{-1}_{\text{post}}}}_{\text{Denoising Matching (DM) terms}} \\
        +&\underbrace{\frac{1}{2}\bigr(\log\frac{\abs{\boldsymbol{\Sigma}_{\text{prior}}}}{\abs{\boldsymbol{\Sigma}_{\text{post}}}}+\text{tr}(\boldsymbol{\Sigma}_{\text{prior}}^{-1}\boldsymbol{\Sigma}_{\text{post}})\bigr)}_{\text{Prior Matching (PM) terms}} + \, \text{C},
    \end{aligned}
    \label{eq: elbo for restoregrad}
    \end{equation}
    where $\gamma_t= \begin{cases} \frac{\beta_t^2}{2\sigma_t^2\alpha_t(1-\bar{\alpha}_t)}, & t>1 \\ \frac{1}{2\alpha_1}, & t=1\end{cases}$ are weighting factors, $\norm{\mathbf{x}}^2_{\boldsymbol{\Sigma}^{-1}}=\mathbf{x}^T\boldsymbol{\Sigma}^{-1}\mathbf{x}$, $\sigma_t^2=\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha_t}}\beta_t$ and $C$ is some constant not depending on learnable parameters $\theta$, $\phi$, $\psi$.
\label{proposition: 2}
\end{proposition}

The derivation (see Appendix \ref{sec: appendix proof prop 1}) is based on combining the conditional VAE and the results in \citet{lee2021priorgrad}. \textit{Notably, we join the conditional DDPM with the posterior/prior encoders and optimize all modules at once, by connecting the DDPM prior space with the latent space estimated by the encoders as in Proposition \ref{proposition: 1}.}
To this end, the sampling $\boldsymbol{\epsilon}\sim q_{\phi}(\boldsymbol{\epsilon}|\mathbf{x}_0, \mathbf{y})$ is performed by the standard reparameterization trick as in VAEs, unlocking end-to-end training via gradient descent on the obtained loss terms:
\begin{itemize}[leftmargin=*]
\vspace{-0.15cm}
    \item \underline{\textit{Latent Regularization (LR)} terms}: help learn a reasonable latent space; e.g., minimizing $\log\abs{\boldsymbol{\Sigma}_{\text{post}}}$ avoids $\boldsymbol{\Sigma}_{\text{post}}$ from becoming arbitrary large due to the presence of its inverse in the weighted norms.
    \vspace{-0.1cm}
    \item \underline{\textit{Denoising Matching (DM)} terms}: responsible for training the DDPM to predict the prior noise.
    \vspace{-0.1cm}
    \item \underline{\textit{Prior Matching (PM)} terms}: obtain a desirable latent space by aligning the prior and posterior distributions.
\end{itemize}

\noindent\textbf{Training of RestoreGrad:} 
With the the conditional DDPM $\theta$, Prior Net $\psi$, and Posterior Net $\phi$ defined in Proposition \ref{proposition: 2}, we are ready to perform optimization on learning the model parameters of $\theta,\psi,\phi$ based on the modified ELBO. The RestoreGrad framework jointly trains the three neural network modules by minimizing (\ref{eq: elbo for restoregrad}) as depicted in Figure \ref{fig: restoregrad}. Following existing DDPM literature, we approximate the objective by dropping the weighting constant $\gamma_t$ of the DM terms, leading to the simplified loss:
\begin{equation}
\begin{small}
\begin{aligned}
    \min_{\theta,\phi,\psi} \,\,\, \eta&\bigr(\underbrace{\bar{\alpha}_T\norm{\mathbf{x}_0}^2_{\boldsymbol{\Sigma}^{-1}_{\text{post}}}+\log\abs{\boldsymbol{\Sigma}_{\text{post}}}}_{\mathcal{L}_{\text{LR}}}\bigr)+\underbrace{\norm{\boldsymbol{\epsilon}-\boldsymbol{\epsilon}_\theta(\mathbf{x}_t,\mathbf{y},t)}^2_{\boldsymbol{\Sigma}^{-1}_{\text{post}}}}_{\mathcal{L}_{\text{DM}}} \\
    +&\lambda \underbrace{\bigr(\log\frac{\abs{\boldsymbol{\Sigma}_{\text{prior}}}}{\abs{\boldsymbol{\Sigma}_{\text{post}}}}+\text{tr}(\boldsymbol{\Sigma}_{\text{prior}}^{-1}\boldsymbol{\Sigma}_{\text{post}})\bigr)}_{\mathcal{L}_{\text{PM}}},
\end{aligned}
\end{small}
\label{eq: elbo of restoregrad}
\end{equation}
where we approximate the expectations by randomly sampling $(\mathbf{x}_0,\mathbf{y})\sim q_{\text{data}}(\mathbf{x}_0,\mathbf{y})$ and $\boldsymbol{\epsilon}\sim \mathcal{N}(\mathbf{0},\boldsymbol{\Sigma}_{\text{post}})$, and the summation by sampling $t\sim \mathcal{U}(\{1,\dots,T\})$ (exploiting the independency due to Markov assumption \citep{nichol2021improved}) in each training iteration. We also introduce $\eta>0$ for the LR terms and $\lambda>0$ for PM terms, to exert flexible control of the learned latent space.

\noindent\textbf{Sampling of RestoreGrad:} 
In applications that RestoreGrad is mainly concerned with, the target signal $\mathbf{x}_0$ is not available in inference time. As in Figure \ref{fig: restoregrad}, the conditional DDPM then samples $\boldsymbol{\epsilon}\sim p_{\psi}(\boldsymbol{\epsilon}|\mathbf{y})=\mathcal{N}(\mathbf{0},\boldsymbol{\Sigma}_{\text{prior}})$ from the Prior Net instead; the Posterior Net is no longer needed.

\noindent\textbf{Leveraging Both Target and Conditioner Signal Information for Prior Learning:} 
In the training stage of RestoreGrad, the latent code $\boldsymbol{\epsilon}$ samples from the posterior $q_\phi(\boldsymbol{\epsilon}|\mathbf{x}_0,\mathbf{y})$ which exploits both the ground truth signal $\mathbf{x}_0$ and conditioner $\mathbf{y}$. It is thus more advantageous than existing works on adaptive priors (e.g., PriorGrad \citep{lee2021priorgrad}) that solely utilize the conditioner $\mathbf{y}$. To observe the benefits brought by the posterior information, we can make the comparison with a variant of RestoreGrad where the Posterior Net is excluded during training, that is,
\begin{equation}
\begin{small}
    \min_{\theta,\psi} \,\,\, \eta\bigr(\bar{\alpha}_T\norm{\mathbf{x}_0}^2_{\boldsymbol{\Sigma}^{-1}_{\text{prior}}}+\log\abs{\boldsymbol{\Sigma}_{\text{prior}}}\bigr)+\norm{\boldsymbol{\epsilon}-\boldsymbol{\epsilon}_\theta(\mathbf{x}_t,\mathbf{y},t)}^2_{\boldsymbol{\Sigma}^{-1}_{\text{prior}}},
\label{eq: elbo of restoregrad no post net}
\end{small}
\end{equation}
which basically removes the Posterior Net $\phi$ and only trains the Prior Net $\psi$ and DDPM $\theta$. Interestingly, our results presented later show that RestoreGrad indeed performs better with Posterior Net than without having it in model training.

