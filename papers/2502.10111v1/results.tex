
In this section, we analyze the experimental results in Table~\ref{tab:results_general}. All the other results are reported in Appendix~\ref{subsec:graphc} and \ref{subsec:nodec}.
%in comparison with other explainability methods across different datasets and graph neural network architectures.
The evaluation considers the five key metrics mentioned above: \textit{Validity}, \textit{Fidelity}, \textit{Distribution Distance}, \textit{Node Sparsity}, and \textit{Edge Sparsity}. 

%The results presented in Table~\ref{tab:results_general} provide a comprehensive overview of the effectiveness of each method.

The first observation that stands out is the consistently high \textit{validity} of COMBINEX, which maintains a perfect score of $1$ across all datasets and architectures. This indicates that COMBINEX can easily generate counterfactual explanations regardless for the $\alpha$ scheduling policy. In contrast, other methods exhibit much lower validity, often falling below $0.5$. Turning our attention to \textit{fidelity}, which measures how closely the generated counterfactuals align with the decision boundary, we see that COMBINEX performs remarkably well. While methods such as CFF occasionally achieve slightly higher fidelity in specific settings, they often suffer from reduced validity or increased sparsity. COMBINEX consistently ranks among the best-performing methods in fidelity, reinforcing its ability to generate explanations that are not only valid but also faithful to the underlying model.

\textit{Distribution Distance} is another important metric, as lower values indicate that the generated counterfactuals remain within the natural data distribution. Although EGO and CF-GNNExplainer achieve competitive results in some cases, their poor validity makes these results less meaningful. COMBINEX, while not always achieving the lowest distribution distance, maintains a strong balance by ensuring both validity and fidelity remain high. This balance is critical in real-world applications, where counterfactuals must not only be feasible but also realistic. When considering \textit{sparsity}, both in terms of nodes and edges, COMBINEX once again demonstrates its superiority generating explanations with minimal perturbations. 
A deeper comparison with baseline methods reveals further insights. Random Features, for instance, occasionally achieves high validity, but its counterfactuals are highly unrealistic, as indicated by their excessively high distribution distance. 
%This suggests that while it can generate plausible counterfactuals from a model perspective, these explanations do not align well with the actual data distribution, making them less useful in practice.
Random Edges, instead, tends to perform poorly in fidelity and sparsity, demonstrating that randomly modifying graph structures does not produce meaningful counterfactual explanations. Among structured methods, CF-GNNExplainer exhibits relatively low distribution distance and reasonable edge sparsity, but its lower validity and fidelity scores limit its overall usefulness. CFF, on the other hand, achieves the highest fidelity in some cases, but at the cost of poor validity and increased sparsity. This indicates that while CFF can produce highly faithful explanations, they are often unrealistic or overly complex.

EGO, while getting a low distribution distance in some instances, suffers from extremely low validity scores.

Overall, we attribute these outstanding results to our method's ability to optimally balance node feature and edge perturbations, leading to superior counterfactual explanations.
%(e.g., $0.015$ on AIDS with GCNConv). This suggests that although its counterfactuals may resemble the original data distribution, they fail to provide actionable or meaningful explanations, making them impractical for real-world applications.

%When considering the trade-offs between validity, fidelity, distribution distance, and sparsity, COMBINEX, has a good trade-off between .

%While some methods excel in a single metric, they do so at the expense of others. EGO minimizes distribution distance but sacrifices validity, while CFF achieves high fidelity yet struggles with validity and sparsity.
% While some competing methods might achieve stronger performance in a single metric, they inevitably sacrifice performance in others. For example, EGO achieves a lower distribution distance but at the cost of near-zero validity. Similarly, CFF attains high fidelity but struggles with validity and sparsity. This reinforces the conclusion that COMBINEX is not just competitive but optimal in a Pareto sense, as it provides the best overall balance across all key evaluation metrics.
