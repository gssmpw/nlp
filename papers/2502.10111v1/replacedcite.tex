\section{Related Work}
\label{sec:related}
Recently, counterfactual explanations have become popular for explaining black-box models like GNNs. In general, many different approaches address the problem of generating a counterfactual example given a factual instance and a predictive model.

A number of techniques have been developed to explain prediction for node classification tasks. CF-GNNExplainer ____ is a perturbation-based method that uses stochastic optimization to find a counterfactual graph that changes the model's classification of a given node. The optimization process trains a perturbation matrix that modifies the graph structure, eliminating edges until the prediction changes. 
In RCE-GNN ____, the authors model the decision logic of a GNN using multiple decision regions. A set of linear decision boundaries of the GNN induces each region. The linear decision boundaries of the decision region capture the common decision logic on all the graph instances inside the decision region. Exploring the common decision logic encoded in the linear boundaries, it is possible to produce counterfactual samples. Using the linear boundaries of the decision region, they build a loss function that is used to train a neural network that generates a counterfactual explanation for an oracle, ensuring that the counterfactual sample lies in the decision region.
GNN-MOeXP ____ is a multi-objective factual-based explanation method for GNN node classification tasks. GNN-MOExp imposes counterfactual relevance to its factual explanation subgraphs. It looks for a subgraph in the original instance that optimizes factual and counterfactual features. GNN-MOExp comes with several limitations that limit the expressiveness of the produced counterfactual: (1) the factual subgraphs are required to be acyclic, and (2) the explanation size is specified a priori.
The authors in CFF ____ build an optimization framework to get GNN explanations. The framework integrates counterfactual and factual reasoning objectives: the counterfactual objective maintains edges relevant to the explanation, while the factual objective ensures that the extracted explanation contains sufficient information. 
The UNR-Explainer ____  instead generates counterfactual (CF) explanations for unsupervised node representation learning by identifying subgraphs whose perturbation significantly alters a nodeâ€™s \textit{top-k} nearest neighbors in the embedding space.
Generative AI models like autoencoders and diffusion models have been widely used to generate counterfactual samples to explain oracle. To this day, there are several generative-based counterfactual explainers. CLEAR ____ is a generative VAE-based counterfactual explainer that uses variational autoencoders to generate counterfactual explanations on graphs for graph-level prediction models. Another approach, like D4Explainer ____, instead uses discrete diffusion models to generate counterfactual graphs by means of a discrete diffusion process on the adjacency matrix.

The method presented in this work differs from existing approaches as it is the first unified framework that 
%finds the optimal counterfactual explanations for GNN model  by striking the optimal 
balance edge and node feature perturbations.
\vspace{-1mm}