\documentclass[sigconf]{acmart}

\usepackage{ulem}
\usepackage{color}
\usepackage{dsfont}
\usepackage{amsmath}
\usepackage{rotating}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{float}
\usepackage{amsthm}
\usepackage{bbm}
\usepackage[ruled,linesnumbered]{algorithm2e}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{remark}[theorem]{Remark}
\setcounter{page}{1}
\DeclareMathOperator*{\argmin}{arg\,min}
\RestyleAlgo{ruled}


%% \BibTeX command to typeset BibTeX logo in the docs
%\AtBeginDocument{%
%  \providecommand\BibTeX{{%
%    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
%\setcopyright{acmlicensed}
%\copyrightyear{2018}
%\acmYear{2018}
%\acmDOI{XXXXXXX.XXXXXXX}
%% These commands are for a PROCEEDINGS abstract or paper.
%\acmConference[Conference acronym 'XX]{Make sure to enter the correct  conference title from your rights confirmation email}{June 03--05,  2018}{Woodstock, NY}

%\acmISBN{978-1-4503-XXXX-X/2018/06}



\begin{document}

\title[COMBINEX]{COMBINEX: A Unified Counterfactual Explainer for Graph Neural Networks via Node Feature and Structural Perturbations}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Flavio Giorgi}
%\authornote{Both authors contributed equally to this research.}
\email{giorgi@di.uniroma1.it}
\affiliation{%
  \institution{Sapienza University of Rome\\Department of Computer Science}
  \city{Rome}
  %\state{Ohio}
  \country{Italy}
}
%\orcid{1234-5678-9012}
\author{Fabrizio Silvestri}
%\authornotemark[1]
\email{fsilvestri@diag.uniroma1.it}
\affiliation{%
  \institution{Sapienza University of Rome\\Department of Computer, Control and Management Engineerin}
  \city{Rome}
  %\state{Ohio}
  \country{Italy}
}

\author{Gabriele Tolomei}
\email{tolomei@di.uniroma1.it}
\affiliation{%
  \institution{Sapienza University of Rome\\Department of Computer Science}
  \city{Rome}
  %\state{Ohio}
  \country{Italy}
}


\begin{abstract}
Counterfactual explanations have emerged as a powerful tool to unveil the opaque decision-making processes of graph neural networks (GNNs). However, existing techniques primarily focus on edge modifications, often overlooking the crucial role of node feature perturbations in shaping model predictions. To address this limitation, we propose COMBINEX, a novel GNN explainer that generates counterfactual explanations for both node and graph classification tasks. Unlike prior methods, which treat structural and feature-based changes independently, COMBINEX optimally balances modifications to edges and node features by jointly optimizing these perturbations. This unified approach ensures minimal yet effective changes required to flip a model's prediction, resulting in realistic and interpretable counterfactuals. Additionally, COMBINEX seamlessly handles both continuous and discrete node features, enhancing its versatility across diverse datasets and GNN architectures. Extensive experiments on real-world datasets and various GNN architectures demonstrate the effectiveness and robustness of our approach over existing baselines.
%Graph Neural Networks (GNNs) have experienced a significant surge in adoption across various domains, including recommender systems and biology, demonstrating their efficacy in addressing a wide range of classification and regression tasks. As GNNs become increasingly integral in these applications, the need for explainability methods to interpret and understand their decisions has become paramount. Existing approaches primarily focus on subgraph extraction or counterfactual explanations via edge modifications, often overlooking the role of node feature perturbations.  

%In this paper, we propose a new GNN explainer -- called COMBINEX -- that generates explanations for both node and graph classification tasks by optimizing minimal perturbations to both edges and node features thereby capturing a more comprehensive view of counterfactual explanations. 
%In this paper, we introduce COMBINEX, a novel GNN explainer that not only generates counterfactual explanations for both node and graph classification tasks but also optimally balances modifications between edges and node features. Unlike previous methods that independently perturb structural or feature-based information, COMBINEX jointly optimizes both types of perturbations, ensuring the minimal and most effective changes needed to alter a model’s prediction. This principled approach leads to counterfactuals that are both realistic and interpretable.
%Moreover, our explainer is designed to accommodate both continuous and discrete node features, making it highly versatile. Extensive experiments on real-world datasets and various GNN architectures demonstrate the effectiveness and robustness of our approach.
%Furthermore, we observe an intriguing phenomenon: the quality of generated explanations tends to correlate with the level of training of the underlying predictive model, particularly in the case of perturbation-based optimization. Specifically, the more the model overfits the data, the easier it becomes to generate a counterfactual explanation. This finding advocates for a trade-off between the generalizability and explainability of a model.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10010147.10010257</concept_id>
       <concept_desc>Computing methodologies~Machine learning</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10010147.10010257.10010293.10010294</concept_id>
       <concept_desc>Computing methodologies~Neural networks</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10010147.10010178</concept_id>
       <concept_desc>Computing methodologies~Artificial intelligence</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Machine learning}
\ccsdesc[500]{Computing methodologies~Neural networks}
\ccsdesc[500]{Computing methodologies~Artificial intelligence}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Explainable AI, Counterfactual explanations, Graph explanations, GNN explanations}
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.


%\received{20 February 2007}
%\received[revised]{12 March 2009}
%\received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
Recent breakthroughs in deep learning have propelled significant advancements in artificial intelligence (AI) systems across a wide array of scientific and non-scientific fields. From engineering to social sciences, many areas of human knowledge have greatly benefited from these innovations. However, despite the excitement surrounding the potential of deep learning, growing concerns about the explainability and interpretability of these complex models persist among both the public and researchers.
Moreover, explainability is not only crucial for end-users but also for regulators and policymakers. For instance, the European Union has been actively working on regulations such as the Artificial Intelligence Act (AI Act)~\cite{european2023artificial}, which includes provisions for the ``\textit{right to explanation}''. This regulation requires that individuals have the right to obtain an explanation of decisions made by automated AI systems. 

In response to these needs, considerable efforts have been made to establish the foundations of Explainable Artificial Intelligence \cite{dovsilovic2018explainable,angelov2021explainable} (XAI). 
Among the various XAI techniques proposed, \textit{counterfactual explanations} (CFEs) have emerged as one of the most promising methods for explaining model predictions~\cite{stepin2021survey}. The primary goal of a CFE is to elucidate a model's prediction for a given instance by identifying minimal changes to the input features that would change the model's output. 
Therefore, CFEs are designed to answer ``what if'' questions, helping users comprehend the inner logic of a complex model in the form: ``\textit{If A had been different, B would \textbf{not} have occurred}''. This capability is particularly valuable in sensitive domains such as finance, healthcare, and justice, where understanding the reasoning behind a model's prediction is essential for building trust and ensuring accountability.
Moreover, this approach not only aids in understanding the model's decision-making process but also provides \textit{actionable} insights for users.


% CFEs are particularly valuable because they answer ``what if'' questions, helping users comprehend how slight modifications to input features can lead to different outcomes. This is especially useful in sensitive applications such as finance, healthcare, and criminal justice, where understanding the reasoning behind a model's prediction is essential for trust and accountability.

Existing CFE methods have played a crucial role in interpreting and validating predictions from various machine learning models~\cite{ribeiro2016should,lundberg2017unified,tolomei2017interpretable,chen2022relax}. Recently, however, there has been a growing need to extend these techniques to accommodate diverse data types and model architectures. Among these, Graph Neural Networks (GNNs) have emerged as particularly effective for tasks involving graph-structured data, such as node classification and link prediction. GNNs have delivered significant benefits across multiple sectors; for instance, in the financial industry, they underpin (semi-)automated fraud detection systems~\cite{10.1145/3589334.3645673}. Furthermore, advancements in GNNs have broadened their applicability to fields like chemistry and biology. In Wong et al.~\cite{Wong2023DiscoveryOA}, for example, GNNs were employed to predict the chemical properties of molecules, thereby bypassing the need for costly and time-intensive experimental screenings of large chemical libraries. Due to the unique characteristics of GNNs, developing methods to explain their predictions is, therefore, critical. 

% In this study, we generalize the approach presented by Lucic et al.~\cite{lucic2022cf} (CF-GNNExplainer) by extending it to incorporate node feature perturbations along with edge modifications. 
Inspired by CF-GNNExplainer~\cite{lucic2022cf}, we propose a unified framework, COMBINEX, to find the optimal counterfactual explanations for GNN models.
Unlike prior methods, which treat structural and feature-based changes independently, COMBINEX balances modifications to edges and node features by jointly optimizing these perturbations (see Figure~\ref{fig:sparse}).
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{imgs/Frame_1.pdf}
    \caption{Comparison of two approaches for generating counterfactual explanations in Graph Neural Networks (GNNs): CF-GNNExplainer vs. COMBINEX. CF-GNNExplainer (\textit{top}) modifies the graph structure by perturbing the adjacency matrix through edge removal. COMBINEX (\textit{bottom}) takes a unified approach, balancing both node feature and structural perturbations to find optimal counterfactual explanations.}
    % \caption{Two possible approaches for generating counterfactual explanations in Graph Neural Networks (GNNs. 1) The adjacency matrix perturbation-based approach (top), where the graph's structure is altered by removing edges to generate counterfactual instances (e.g., CF-GNNExplainer). \textit{The bottom row} represents the \textbf{COMBINEX} approach.}
    \label{fig:sparse}
\end{figure}
Through extensive experiments, we show that modifying node features improves standard quality metrics for explanations
across many different datasets. Additionally, we find that the model training degree has a direct impact on the performance of some of the explainers we evaluated. Our contribution can be summarized as follows:
\begin{itemize}
  \item We propose \textbf{COMBINEX}, a novel counterfactual explainer for Graph Neural Networks (GNNs) that generates explanations by perturbing both node features and graph structure, introducing a \textbf{stochastic optimization framework} that efficiently finds minimal perturbations required to alter model predictions while preserving interpretability.
    \item We conduct extensive experiments on multiple real-world datasets, demonstrating that COMBINEX outperforms existing counterfactual explainers in terms of \textit{validity}, \textit{fidelity}, and \textit{sparsity}.
    \item We provide the source code of our method at the following GitHub repository: \url{https://github.com/flaat/COMBINEX}.
    %\item We analyze the \textbf{relationship between explainability and model training}, revealing that models with higher training accuracy (overfitting) tend to produce easier-to-generate counterfactual explanations.
\end{itemize}
The remainder of this paper is structured as follows. In Section\ref{sec:related}, we discuss related work. Section~\ref{sec:formulation} covers background concepts. We introduce our proposed method (COMBINEX) in Section~\ref{sec:method}, which we extensively validate in Section~\ref{sec:experiments}. Section~\ref{sec:feasibility} discusses the feasibility of our method. Finally, we draw the conclusions in Section~\ref{sec:conclusion}.


\section{Related Work}
\label{sec:related}
Recently, counterfactual explanations have become popular for explaining black-box models like GNNs. In general, many different approaches address the problem of generating a counterfactual example given a factual instance and a predictive model.

A number of techniques have been developed to explain prediction for node classification tasks. CF-GNNExplainer \cite{lucic2022cf} is a perturbation-based method that uses stochastic optimization to find a counterfactual graph that changes the model's classification of a given node. The optimization process trains a perturbation matrix that modifies the graph structure, eliminating edges until the prediction changes. 
In RCE-GNN \cite{bajaj2021robust}, the authors model the decision logic of a GNN using multiple decision regions. A set of linear decision boundaries of the GNN induces each region. The linear decision boundaries of the decision region capture the common decision logic on all the graph instances inside the decision region. Exploring the common decision logic encoded in the linear boundaries, it is possible to produce counterfactual samples. Using the linear boundaries of the decision region, they build a loss function that is used to train a neural network that generates a counterfactual explanation for an oracle, ensuring that the counterfactual sample lies in the decision region.
GNN-MOeXP \cite{liu2021multi} is a multi-objective factual-based explanation method for GNN node classification tasks. GNN-MOExp imposes counterfactual relevance to its factual explanation subgraphs. It looks for a subgraph in the original instance that optimizes factual and counterfactual features. GNN-MOExp comes with several limitations that limit the expressiveness of the produced counterfactual: (1) the factual subgraphs are required to be acyclic, and (2) the explanation size is specified a priori.
The authors in CFF \cite{tan2022learning} build an optimization framework to get GNN explanations. The framework integrates counterfactual and factual reasoning objectives: the counterfactual objective maintains edges relevant to the explanation, while the factual objective ensures that the extracted explanation contains sufficient information. 
The UNR-Explainer \cite{kangunr}  instead generates counterfactual (CF) explanations for unsupervised node representation learning by identifying subgraphs whose perturbation significantly alters a node’s \textit{top-k} nearest neighbors in the embedding space.
Generative AI models like autoencoders and diffusion models have been widely used to generate counterfactual samples to explain oracle. To this day, there are several generative-based counterfactual explainers. CLEAR \cite{ma2022clear} is a generative VAE-based counterfactual explainer that uses variational autoencoders to generate counterfactual explanations on graphs for graph-level prediction models. Another approach, like D4Explainer \cite{chen2023d4explainer}, instead uses discrete diffusion models to generate counterfactual graphs by means of a discrete diffusion process on the adjacency matrix.

The method presented in this work differs from existing approaches as it is the first unified framework that 
%finds the optimal counterfactual explanations for GNN model  by striking the optimal 
balance edge and node feature perturbations.
\vspace{-1mm}
\section{Background}\label{sec:formulation}

%\subsection{Graph Neural Networks}

%Graph Neural Networks (GNNs) are a class of neural networks that generalize deep learning models to work on graph-structured data. Graphs are defined as $G = (V, E)$, where $V$ is a set of nodes (vertices) and $E$ is a set of edges (links between nodes). GNNs are designed to capture the structural and relational information in graphs, enabling tasks such as node classification, link prediction, and graph classification \cite{wu2022graph}.

%The core mechanism of GNNs is the message-passing or neighborhood aggregation scheme. At each layer $l$ of a GNN, the hidden representation $\mathbf{h}_v^{(l)}$ of a node $v$ is updated by aggregating the hidden representations of its neighboring nodes $N(v)$ and combining this with its own representation. The general update rule can be expressed as:

%\[
%\mathbf{h}_v^{(l+1)} = \sigma \left( \text{AGGREGATE}\left( \left\{ \mathbf{h}_u^{(l)} : u \in N(v) \right\} \right), \mathbf{h}_v^{(l)} \right).
%\]

%Here, $\text{AGGREGATE}(\cdot)$ is a function that combines the features of node $v$'s neighbors (e.g., sum, mean, or attention), and $\sigma(\cdot)$ is an activation function, such as ReLU. This update mechanism allows each node to iteratively gather information from its neighborhood, progressively expanding its receptive field.

%One common GNN architecture is the Graph Convolutional Network (GCN), which uses a convolution-like operation to aggregate features. The layer-wise propagation rule in GCNs is defined as:

%\[
%\mathbf{H}^{(l+1)} = \sigma \left( \hat{\mathbf{D}}^{-\frac{1}{2}} \hat{\mathbf{A}} \hat{\mathbf{D}}^{-\frac{1}{2}} \mathbf{H}^{(l)} \mathbf{W}^{(l)} \right),
%\]
%where $\hat{\mathbf{A}} = \mathbf{A} + \mathbf{I}$ is the adjacency matrix with added self-loops, $\hat{\mathbf{D}}$ is the degree matrix, $\mathbf{H}^{(l)}$ is the node feature matrix at layer $l$, and $\mathbf{W}^{(l)}$ is the trainable weight matrix. This operation effectively smooths node features over the graph, enabling information propagation across the graph structure.

\noindent \textbf{\textit{Graph Neural Networks.}}
Graph Neural Networks (GNNs) extend deep learning to graph-structured data. 
Formally, let $G=(V,E)$ be a graph with $n$ nodes $V$, and $m$ edges $E$.
The structure of $G$ is encoded by its adjacency matrix $\mathbf{A} \in \lbrace 0,1 \rbrace^{n \times n}$, where $\mathbf{A}_{i,j}$ = 1 iff $(i, j) \in E$. 
Moreover, we assume there exists a feature matrix $\mathbf{X} \in \mathbb{R}^{n \times k}$, which associates features to nodes of $G$.
Generally speaking, a GNN $g$ learns a hidden representation of nodes in the graph (i.e., a node embedding). Such representation is, in turn, used for downstream tasks of interest like node classification, link prediction, and graph classification \cite{wu2022graph}.
GNNs generate node embeddings through a process known as message passing, where a node's features are iteratively updated based on the features of its neighbors.
Formally, let $\mathbf{h}^l_u$ denote the embedding of node $u\in V$ at the $l$-th layer of $g$ and $\mathcal{N}_u$ the set of $u$'s neighbors. The updated node embedding for $u$ at layer $l+1$ is then computed as follows.
\[
\mathbf{h}_u^{(l+1)} = g(\mathbf{A}^{(l+1)}_u,\mathbf{X}^{(l+1)}_u) =\sigma \left( \text{AGG} \left( \left\{ \mathbf{h}_v^{(l)} : v \in \mathcal{N}(u) \right\} \right), \mathbf{h}_u^{(l)} \right),
\]
where: $\mathbf{A}^{l+1}_v$ and $\mathbf{X}^l_v$ are the adjacency matrix and the feature matrix of the subgraph $G^l_u$ of $G$, induced by $u$ and its $l$-hop neighbors; \(\text{AGG}(\cdot)\) is a function combining neighbor embeddings (e.g., sum, mean, or attention); \(\sigma(\cdot)\) is an activation function like ReLU. %; $\boldsymbol{\theta}$ are the trainable parameters of $g$.
%A common GNN variant, the Graph Convolutional Network (GCN), performs feature aggregation using:

%\[
%\mathbf{H}^{(l+1)} = \sigma \left( \hat{\mathbf{D}}^{-\frac{1}{2}} \hat{\mathbf{A}} \hat{\mathbf{D}}^{-\frac{1}{2}} \mathbf{H}^{(l)} \mathbf{W}^{(l)} \right),
%\]

%where \(\hat{\mathbf{A}} = \mathbf{A} + \mathbf{I}\) is the adjacency matrix with self-loops, \(\hat{\mathbf{D}}\) is the degree matrix, \(\mathbf{H}^{(l)}\) is the feature matrix, and \(\mathbf{W}^{(l)}\) is a trainable weight matrix. This operation enables feature propagation and smooths node representations across the graph.


\noindent \textbf{\textit{Counterfactual Explanations.}}
The general formulation of the counterfactual explanation problem in a classification task can be expressed as the following optimization problem. Given an input sample $\boldsymbol{x}$ and a classifier $f$ -- hereinafter referred to as \textit{oracle} -- the objective is to find a \textit{counterfactual sample} $\boldsymbol{x'}$ such that $f(\boldsymbol{x}) \neq f(\boldsymbol{x'})$ while minimizing the distance  $d(x, x')$. 
Formally:
\begin{equation}
\label{eq:cf-train}
\begin{aligned}
\boldsymbol{x'} = \argmin_{\boldsymbol{\widetilde{x}}}  &~d(\boldsymbol{x}, \boldsymbol{\widetilde{x}})\\
\text{ s.t.: } &f(\boldsymbol{x}) \neq f(\boldsymbol{\widetilde{x}}).
\end{aligned}
\end{equation}
The function $d$ enforces similarity between the counterfactual sample $\boldsymbol{x'}$ and the original factual sample $\boldsymbol{x}$.
Note that the formulation above is general enough and also encompasses a \textit{targeted} version of the counterfactual explanation problem, where the objective is not only to ensure $f(\boldsymbol{x}) \neq f(\boldsymbol{\widetilde{x}})$ but also to enforce a specific target prediction, i.e., $f(\boldsymbol{\widetilde{x}}) = y_t$.



\section{Proposed Method}
\label{sec:method}
\subsection{Problem Formulation}
Tackling the counterfactual explanation problem outlined in (\ref{eq:cf-train}) within the context of GNNs -- where the input instance is a graph -- introduces unique challenges.
To formalize this, we follow the notation used by Romero et al.~\cite{Prado_Romero_2023}, replacing the original input instance $\boldsymbol{x}$ with the graph $G(V, E)$ and the counterfactual sample $\boldsymbol{x'}$ as $G'(V', E')$.
Since $G$ and $G'$ can be represented by their adjacency and node feature matrices, respectively, the (targeted) counterfactual explanation problem for GNNs can be rewritten as follows:
\begin{equation}
\label{eq:cf-gnn-train}
\begin{aligned}
\mathbf{A'},\mathbf{X'} = \argmin_{\mathbf{\widetilde{A}},\mathbf{\widetilde{X}}}  &~\Big\{d_{\text{topology}}(\mathbf{A}, \mathbf{\widetilde{A}}) +  d_{\text{features}}(\mathbf{X}, \mathbf{\widetilde{X}})\Big\}\\
\text{ s.t.: } &f(g(\mathbf{A}, \mathbf{X})) \neq f(g(\mathbf{\widetilde{A}}, \mathbf{\widetilde{X}})) = y_t,
\end{aligned}
\end{equation}
where $d_{\text{topology}}$ captures the structural distance between the original graph $G$ and its counterfactual $G'$, $d_{\text{features}}$ measures the magnitude of node feature perturbation between $G$ and $G'$, $f$ is a classifier representing a downstream graph-related task, and $y_t$ denotes the desired target prediction for the counterfactual example.
Note that the input to the classifier $f$ can be any representation produced by the GNN $g$. For instance, if $f$ operates on a single node embedding, the task corresponds to node classification. If $f$ processes the entire graph representation, it performs a graph classification task. Finally, if $f$ takes two node embeddings as input, it may determing whether the nodes are connected by a link or not, i.e., edge classification.


The counterfactual optimization problem in (\ref{eq:cf-gnn-train}) can be directly translated into a loss function ($\mathcal{L}_{total}$) as follows:
\begin{equation}
\label{eq:loss}
\mathcal{L}_{total} = \mathcal{L}_{CF} + (1 - \alpha) \mathcal{L}_E + \alpha \mathcal{L}_X, 
\end{equation}
where: $\mathcal{L}_{CF}$ penalizes when the counterfactual goal is \textit{not} met, i.e., when the modified input instance does \textit{not} lead to an actual classification change; $\mathcal{L}_{E}$ that enforces minimal structural perturbations of the input graph; $\mathcal{L}_{X}$ that tries to control the magnitude of node feature perturbations. 

Since we focus on classification tasks only, the first term ($\mathcal{L}_{CF}$) can be expressed as $\eta\mathcal{L}_{CE}$, where $\mathcal{L}_{CE}$ is the cross-entropy loss between the original and the counterfactual predictions and  $\eta$ is an indicator variable ensuring this term is considered only when the classification change has not yet occurred.  The graph structural loss ($\mathcal{L}_E$) is defined as the sum of absolute differences between the original adjacency matrix and the perturbed one. The node feature loss ($\mathcal{L}_X$) is composed of two terms: L1 loss for discrete features, ensuring minimal modifications while preserving categorical interpretability and Mean Squared Error (MSE) for continuous features, penalizing large deviations while allowing smooth gradient-based optimization.
Finally, $\alpha$ is an adjustable trade-off hyperparameter that regulates the balance between feature and structural modifications. 
This \textit{joint} formulation ensures that both types of perturbations remain minimal while still enforcing the desired classification change.
The impact of $\alpha$ will be further analyzed in Section~\ref{subsec:alpha}.

To optimize the loss function defined in (\ref{eq:loss}) using gradient-based methods, we introduce two differentiable perturbation matrices. The first, the node feature perturbation matrix $\mathbf{P}$, is responsible for modifying node features, while the second, the edge perturbation matrix 
$\mathbf{EP}$, governs changes in the graph topology by modifying edge values.

Given that both structural and node feature data in graphs often consist of discrete values, we adopt the approach proposed by Lucic et al.~\cite{lucic2022cf} to preserve differentiability. Specifically, we apply a 
$\tanh$-based transformation to discrete node features and a sigmoid activation to edge perturbations. This ensures that modifications remain within valid bounds while allowing gradients to propagate effectively during optimization. The differentiable versions of these matrices are used to update model parameters via backpropagation, whereas their corresponding thresholded (non-differentiable) versions are ultimately employed to generate the final counterfactual examples.

%\textbf{Con questa aggiunta mi sembra molto più chiaro. Secondo me manca l'ultimo pezzo, ossia come possiamo utilizzare metodi di ottimizzazione gradient-based per risolvere (3). In parte lo diciamo per MSE e sicuramente ne parli in modo tecnico nell'implementazione. Una frase/paragrafo alla fine la direi... Qualcosa tipo: In order to solve the objective in (3) using gradient-based optimization methods, we ...}

%To address the counterfactual explanation problem defined in Section \ref{sec:formulation}, we generate a counterfactual example $G'$ starting from an instance $G$ using the  algorithm. The idea behind is to use gradient-based optimization to find a perturbation that changes the sample's classification. In order to overcome efficency problems due to previous methods (Lucic et al. \cite{lucic2022cf}) we used a trick involving edge weights (see Section \ref{}). Moreover, we apply the same idea used to perturb the edges to perturb the discrete node features. Anyway, our method can efficently perturb both nodes features (discrete or continuos) and edges. One of the biggest advantages concerns the flexibility of our approach in blocking certain features, and on imposing min-max ranges to avoid Out-Of-Distribution samples. 
% To address the counterfactual explanation problem formulated in Section~\ref{sec:formulation}, we propose a method that generates a counterfactual example $G'$ from a given instance $G$ using gradient-based optimization. Unlike previous approaches that suffer from efficiency limitations (see, e.g., Lucic et al. \cite{lucic2022cf}), our method incorporates a novel strategy based on edge weights to streamline the perturbation process (see Section~\ref{subsec:edgesparse}). Moreover, our work allows discrete and continuous node features perturbations. Furthermore, a significant advantage of our method is its inherent flexibility: it permits the selective blocking of specific features and the imposition of min-max constraints to avoid generating samples having features with highly unlikely values.

%%%%%%%%%%%%%%




%The process begin with  the matrix $P^0$ and $EP^0$ to perturb respectively the nodes features and the edges are  filled with zeros and ones to maintain the original sets of attributes and edges. Given the oracle $\phi$ we fix all its weights and use $P^{(t)}$ and $EP^{(t)}$ to change the nodes features and edges matrix. At each iteration we check if the prediction changed. To preserve differentiability with discrete values, we used the same trick used in Lucic et al., namely, we first build a real-valued perturbation matrix to which we apply the sigmoid or the tanh function to train the perturbation.

%\noindent Furthermore, we define the three-term loss function below:
%\begin{equation}\label{eq:loss}
%    \mathcal{L} = \eta * \mathcal{L}_{CE} +  (1 - \alpha) * \mathcal{L}_{E} + \alpha *\mathcal{L}_{X}
%\end{equation}

%Here, $\mathcal{L}_{CE}$ is the cross entropy (CE) loss between the oracle prediction for the perturbed graph and a target class, . 

%The terms $\mathcal{L}_{E}$ and $\mathcal{L}_{X}$, instead, enforce the closeness between the factual and counterfactual samples. The $\alpha$ represents a weighting factor that balances the importance of the edge loss $\mathcal{L}_{E}$ and the feature loss $\mathcal{L}_{X}$ during the optimization process. $\eta$ instead turn off the first term of the loss function when the sample generated using the perturbed matrices traverses the decision boundary, leaving only the second and third part of the loss that lead to perturbation minimization.

%To minimize the loss function defined in Equation \ref{eq:loss}, we set up the optimization problem below and use Adam optimizer to solve the objective below:
%\begin{equation}\label{eq:optim}
%    \argmin_{\mathbf{V}_{pert}}  \eta * \underbrace{\ \text{CE}(\Phi_{\theta}(G'), c_{cf})}_{\mathcal{L}_{CE}} + (1 - \alpha) * \underbrace{\left \lVert \mathbf{V}_{x} - \mathbf{V}_{pert}\right \rVert_1}_{\mathcal{L}_{E}} + \alpha * \underbrace{\left \lVert \mathbf{V}_{x} - \mathbf{V}_{pert}\right \rVert_1}_{\mathcal{L}_{X}}. 
%\end{equation}

%In Algorithm~\ref{alg:one}, we present the pseudocode of the proposed procedure.

%The discrete features are first updated using the perturbation (\textbf{lines 7--8}), followed by the computation of the continuous features (\textbf{line 8}). The discrete features are obtained by summing the original node features $X$ with the Hadamard product of the scaled perturbation and the discrete feature mask $M_d$. Similarly, the continuous features are computed by applying the same process but without any transformation on the perturbation. Both results are then passed through a clamping function to ensure that feature values remain within a predefined range. The final perturbed feature matrix $X_p$ is obtained by summing $X_d$ and $X_c$ (\textbf{line 10}).

%In \textbf{line 11}, the differentiable edge matrix is computed, followed by the model’s prediction for the perturbed graph in \textbf{line 12}. Next, in \textbf{lines 13--15}, the non-differentiable matrices for both node features and edges are obtained, and the non-differentiable prediction is computed. The function \textit{to\_int} takes as input a continuous matrix and returns its discretized version by rounding values to the nearest integer.

%In particular, \textbf{line 16} determines the value of $\eta$, which is used to selectively enable or disable the cross-entropy loss computed in \textbf{line 17}. In \textbf{line 18}, the \textbf{edge distance loss} is computed to minimize perturbations applied to the graph structure. From \textbf{lines 19--21}, the \textbf{feature loss} is calculated separately for discrete and continuous features, employing the L1 loss for the discrete case and the Mean Squared Error (MSE) for the continuous case. The function invoked in \textbf{line 22} (Algorithm~\ref{alg:get_alpha}) returns the value of the parameter $\alpha$, which regulates the balance between edge and feature modifications. Different policies for determining $\alpha$ were explored and will be discussed later.


%In Algorithm \ref{alg:perturbation} the perturbation mechanism is shown for both the nodes features and the edges. First of all the discrete features are updated using the perturbation (\textbf{lines 1--2}), followed by the computation of the continuous features (\textbf{line 3}). The discrete features are obtained by summing the original node features $X$ with the Hadamard product of the scaled perturbation and the discrete feature mask $M_d$. Similarly, the continuous features are computed by applying the same process but without any transformation on the perturbation. Both results are then passed through a clamping function to ensure that feature values remain within a predefined range. The final perturbed feature matrix $X_p$ is obtained by summing $X_d$ and $X_c$ (\textbf{line 10}).

%We need a differentiable and a non-differentiable matrix for both edges and features to preserve differentiability due to the presence of discrete values in both the metrices. The differentiable matrices $X_p$ and $E_p$ are going to be used to get a differentiable output from the model to compute the loss. The non differentiable matrices $E^{nd}_p$ and $X^{nd}_p$ instead are used to get the \textit{official} prediction. Note that the graph components we are going to use to get the counterfactual sample at the end of the process are the \textit{non-differentiable} ones.

%The algorithm returns both the differentiable and non-differentiable versions of the edge and node feature matrices, enabling compatibility with both gradient-based optimization and discrete counterfactual reasoning.


%In \textbf{line 11}, the differentiable edge matrix is computed, followed by the model’s prediction for the perturbed graph in \textbf{line 12}. Next, in \textbf{lines 13--15}, the non-differentiable matrices for both node features and edges are obtained, and the non-differentiable prediction is computed. The function \textit{to\_int} takes as input a continuous matrix and returns its discretized version by rounding values to the nearest integer.

% \begin{remark}[Time Complexity of COMBINEX]

%%%%%%%%%%%%%%%
\subsection{The COMBINEX Algorithm}
In this section, we detail our proposed counterfactual explanation algorithm (Algorithm \ref{alg:one}), which solves the objective defined in (\ref{eq:loss}) via gradient-based optimization.


The algorithm takes as input a graph $G(V, E)$, a pre-trained GNN model $g$ with fixed parameters, an integer $k$ representing the maximum number of optimization epochs, a target class $y_t$, a vector $\mathbf{M}_d$ serving as a mask for discrete features, and a learning rate $\gamma$. 

The procedure begins by initializing the perturbation matrices for node features and edges, denoted as $\mathbf{P}^0$ and $\mathbf{EP}^0$, respectively. It then computes the initial prediction $y$ for the input graph $G(V, E)$ and sets the epoch counter to 1 (lines 1--2). Subsequently, the algorithm extracts the edge matrix and node feature matrix from the original graph (lines 3--5) and initializes the variable that will store the counterfactual sample. Additionally, the continuous feature mask is derived from the discrete feature mask $\mathbf{M}_d$, which is used to selectively enable or disable specific features during computation.

The optimization process begins in line 6. In line 7, we get both the edge and node perturbations using the function \FuncSty{get\_pert} (see Algorithm \ref{alg:perturbation}). 
At line 10, the algorithm computes the loss function using the \FuncSty{get\_loss}
function (Algorithm \ref{alg:loss}).
Finally, from lines 12--20, the total loss is computed, and the perturbation matrices are updated to minimize it. If the model's prediction changes with the newly perturbed node features and edges while achieving the lowest loss observed thus far, the algorithm identifies a valid counterfactual sample.
Below, we describe the algorithms used to compute feature and edge perturbations as well as the loss function that drives the optimization process at the core of COMBINEX. 

\input{algorithms/algorithm}
\smallskip
\noindent \textbf{\textit{Features and Edge Perturbation.}} 
Algorithm~\ref{alg:perturbation} outlines the perturbation mechanism applied to both node features and edges. The process begins by computing a scaled perturbation vector $\mathbf{P}^{(t)}_s$, to perturb the discrete node features, using a $\tanh$-based transformation, ensuring that the values remain within the predefined feature bounds (line 1). Next, discrete features are updated by applying the Hadamard product between the scaled perturbation and the discrete feature mask $\mathbf{M}_d$, followed by summation with the original node features $\mathbf{X}$, and subsequently clamped within the feature range (line 2). Similarly, continuous features are updated without any transformation on the perturbation and are clamped accordingly (line 3). The final perturbed node feature matrix is obtained by summing the contributions of discrete and continuous feature updates, yielding $\mathbf{X}_p = \mathbf{X}_d + \mathbf{X}_c$ (line 4). 
\input{algorithms/pert.tex}
For edge perturbations, the differentiable edge matrix $\mathbf{E}_p$ is computed by applying a sigmoid activation $\sigma$ to the edge perturbation vector $\mathbf{EP}^{(t)}$, ensuring that the resulting values are in the range $(0,1)$ (line 5). The non-differentiable edge matrix $\mathbf{E}^{nd}_p$ is obtained by thresholding the sigmoid output at $0.5$, enforcing a hard binary decision on edge presence (line 6). Similarly, the non-differentiable node feature matrix $\mathbf{X}^{nd}_p$ is computed by converting the discrete feature updates into integer values and applying clamping to maintain the predefined feature bounds (line 7).
We employ both differentiable and non-differentiable versions of the matrices for edges and features in order to maintain gradient flow during optimization while ultimately obtaining the final discrete prediction. Specifically, the differentiable matrices $\mathbf{X}_p$ and $\mathbf{E}_p$ are used to produce a continuous output from the model, which is essential for computing the loss via backpropagation. In contrast, the non-differentiable matrices $\mathbf{X}^{nd}_p$ and $\mathbf{E}^{nd}_p$ are derived by thresholding $\mathbf{EP}^{(t)}$ and $\mathbf{P}^{(t)}_s$ (see Algorithm \ref{alg:perturbation}, lines 6-7), thereby yielding the definitive prediction. It is important to note that the final counterfactual sample is constructed from these non-differentiable components, ensuring that the discrete nature of the graph is preserved in the final output.
%\smallskip
\noindent \textbf{\textit{Loss Function.}} The loss function is computed in Algorithm  \ref{alg:loss}: line 1 computes the indicator variable $\eta$, which determines whether the cross-entropy loss $\mathcal{L}_{CE}$ should be applied (line 2). This ensures that the classification loss is only considered when the counterfactual prediction does not yet match the counterfactual target class $y_t$. The \textit{edge distance loss} $\mathcal{L}_E$ is then computed in line 3 by summing the absolute differences between the sigmoid-transformed edge perturbation values and $1$, encouraging minimal modifications to the graph structure. 

From lines 4--6, the \textit{node feature loss} $\mathcal{L}_X$ is computed separately for discrete and continuous features. The discrete feature loss $\mathcal{L}_d$ is defined using the L1 loss between the masked original feature values and their perturbed versions, while the continuous feature loss $\mathcal{L}_c$ is computed using the Mean Squared Error (MSE). These two losses are then combined to form the total feature loss $\mathcal{L}_X = \mathcal{L}_d + \mathcal{L}_c$.

Next, in line 7, the function $\FuncSty{get\_alpha}$ (see Appendix \ref{alg:get_alpha}) determines the value of the weighting parameter $\alpha$, which regulates the trade-off between edge and feature modifications. To enhance the flexibility of the framework, we introduce the ability to select from various \textit{scheduling policies}, allowing $\alpha$ to dynamically evolve over training epochs. This adaptive approach ensures that the model progressively refines its focus on structural and feature perturbations, leading to a more effective optimization process and improving the quality of the generated counterfactual explanations. Finally, in line 8, the total loss $\mathcal{L}_{total}$ is computed as specified in (\ref{eq:loss}). This formulation ensures that perturbations remain minimal while enforcing the desired classification change. 

\vspace{-2.5mm}
\subsection{Computational Complexity Analysis}
The time complexity of the COMBINEX algorithm is primarily determined by its iterative optimization process over $k$ training epochs. 
Let $n$ and $m$ denote the number of nodes and edges in the graph, respectively. Moreover, let $f$ the number of node features. 
Therefore, each iteration consists of:\\ 
\noindent (1) \textbf{Perturbation Step:} $O(nf + m)$ for updating node features and edge perturbations;\\ 
\noindent (2) \textbf{GNN Forward Pass:} $O(L(n + m) f)$, where $L$ is the number of GNN layers;\\
\noindent (3) \textbf{Loss Computation:} $O(nf + m)$ for feature and edge losses;\\
\noindent (4) \textbf{Gradient Update:} $O(nf + m)$ for updating perturbation vectors.
%Aggregating these components, the total complexity per iteration is:
%\begin{equation}
%    O(F_{\phi}(n, m, f)) + O(nf + m) = O(L(n + m) f + nf + m).
%\end{equation}

Overall, the worst-case complexity is, therefore, $\mathcal{O}(k n f + k m f)$. This implies that the algorithm scales \textit{linearly} with the number of nodes, edges, features, and training epochs.
%\end{remark}


\section{Experiments}\label{sec:experiments}
Below, we outline our experimental setup and evaluation methodology to assess the effectiveness of our proposed COMBINEX approach for generating counterfactual explanations for GNNs. 
%The source code for our experiments is available at: \url{https://anonymous.4open.science/r/fjhAB41jnkq123/}.

\noindent \textbf{\textit{Tasks, Datasets, and Models.}} Our method is tested on two tasks -- \textit{node classification} and \textit{graph classification} -- using a diverse collection of real-world datasets to ensure robustness and generalizability across domains such as citation networks, web page classification, and social network analysis (see Tables~\ref{tab:datasets} and \ref{tab:datasets_graph}). Specifically, the dataset collection includes the Planetoid datasets (CiteSeer, Cora, and PubMed)
%, where documents are represented by binary word vectors or TF-IDF features; 
, the WebKB datasets, 
%which consist of webpages from various university computer science departments with bag-of-words features and manual categorization; 
the Attributed datasets (Wiki and Facebook), 
%comprising graphs where nodes represent web pages or users and edges denote hyperlinks or social connections; 
the Biological datasets (AIDS, Enzymes, and Proteins), originally designed for graph classification and here adapted also for node classification; the COIL-DEL dataset,
%, which contains graphs corresponding to 2D images from COIL-100 with nodes representing superpixels; 
and the Miscellaneous category, which includes the Karate and Actor datasets.

We experiment with three GNN models: Graph Convolutional Network (GCN) \cite{kipf2016semi}, Chebyshev Network (ChebNet) \cite{he2022convolutional}, and GraphConv Networks \cite{morris2019weisfeiler}.

\noindent \textbf{\textit{Baselines.}} We evaluate our approach against a set of baselines that span naive strategies (random-edges, random-features, and ego-graph) and state-of-the-art techniques (CF-GNNExplainer \cite{lucic2022cf}, CounterFactual and Factual Explainer (CFF) \cite{tan2022learning}, and UNR \cite{kangunr}). 
Baselines such as random-edges and random-features perturb the graph's adjacency and feature matrices by applying random modifications, whereas the ego-graph method extracts a subgraph centered on a given node. 
%CF-GNNExplainer employs a perturbation-based strategy to learn a binary mask that ``sparsifies'' the adjacency matrix, with our implementation further exploiting edge weights for efficiency. CFF leverages causal inference to balance factual and counterfactual reasoning, and UNR uses a reward-guided search to identify subgraphs whose perturbation significantly alters a node's nearest neighbors in the embedding space. 

\noindent \textbf{\textit{Evaluation Measures.}} To assess the quality of generated counterfactuals, we consider several key metrics. \textit{Validity} is an indicator function that returns $1$ if the counterfactual successfully alters the model's prediction compared to the original instance, as defined in~\cite{guidotti2019factual, guidotti2022counterfactual}. \textit{Fidelity} measures how well the counterfactual explanation aligns with the oracle’s decisions, following~\cite{Prado_Romero_2023}. \textit{Node/Edge sparsities} are computed as the ratio of modified features/edges between the factual and counterfactual graphs, respectively, ensuring minimal perturbations. Finally, \textit{Distribution Distance} is quantified using the $L_2$ distance between a graph's embedded representation and the dataset's mean embedding, capturing how much the counterfactual deviates from the original average data distribution.

\noindent \textbf{\textit{Settings.}}
The experiments were conducted on two machines, each equipped with an Nvidia GTX 4090 GPU, 64 GB of RAM, and an AMD Ryzen 9 7900 processor. To ensure robustness, we employed 4-fold cross-validation. Additionally, we performed an extensive hyperparameter tuning process to identify the optimal parameter configurations for our method, which were set as follows: learning rate of $0.1$, and $500$ training epochs. For the baseline models, we used the same hyperparameters specified in the original papers.  

\noindent \textbf{\textit{Scheduling Policy $\pmb{\alpha}$.}} We explored several $\alpha$ scheduling policies to control the trade-off between edge and feature modifications in our loss function. These different strategies result in multiple variants of our COMBINEX method. Specifically, we consider the following distinct approaches. The \textit{linear} policy (COMBINEX$_{\textit{lin}}$) lets $\alpha$ decrease linearly from $1.0$ to $0.0$ over the course of training. The \textit{exponential} policy (COMBINEX$_{\textit{exp}}$) sets $\alpha = e^{-\frac{epoch}{\delta}}$, leading to an exponential decay. The \textit{sinusoidal} policy (COMBINEX$_{\textit{cos}}$) modulates $\alpha$ according to $\alpha = 0.5 \times \left(1 + \cos\left(\pi \times \frac{epoch}{epochs_{max}}\right)\right)$, resulting in a cosine-shaped decay. The \textit{dynamic} policy (COMBINEX$_{\textit{dyn}}$) adjusts $\alpha$ based on the relative magnitudes of the edge loss and node feature loss, setting $\alpha=0$ when the edge loss dominates and $\alpha=1$ otherwise. Finally, if no policy is specified (COMBINEX$_{\textit{def}}$), $\alpha$ is fixed at a constant, default value $\alpha_{default}$. We also evaluated a variant where $\alpha$ is fixed at $1$, meaning only node features are perturbed. We refer to this approach as COMBINEX$_{\textit{feat}}$.

For a comprehensive list of parameters and settings, we refer the reader to Appendix \ref{subsec:params} and the GitHub repository.

%\section{Experiments}
%\label{sec:experiments}
%In this section, we present the experimental setup and evaluation methodology used to assess the effectiveness of our proposed approach. To demonstrate its versatility, we tested our method on two different tasks: node classification and graph classification. We employed real-world datasets to ensure robustness and generalizability across different domains. The datasets selected for evaluation include well-known benchmarks in graph-based machine learning, such as citation networks, web page classification, and social network analysis.

%We also implemented a variety of baseline methods to compare against our approach. These baselines include both naive strategies and state-of-the-art techniques for generating counterfactual explanations in graph neural networks. We evaluated the performance of all methods using several key metrics, such as validity, fidelity, and sparsity, to measure the quality and effectiveness of the generated counterfactual examples. The code used for the experiments is available at \url{https://anonymous.4open.science/r/fjhAB41jnkq123/}.
% Node datasets description
\input{tables/datasets/node/node}

% Graph datasets description
\input{tables/datasets/graph/graph}


%We conducted extensive experiments on a wide range of real-world datasets (summarized in Tables~\ref{tab:datasets} and \ref{tab:datasets_graph}) to evaluate our approach. Our dataset collection comprises several categories: the Planetoid datasets include citation networks (CiteSeer, Cora, and PubMed) where documents are represented by binary word vectors or TF-IDF features; the WebKB datasets represent webpages from various university computer science departments with bag-of-words features and manual classification into five categories; the Attributed datasets (Wiki and Facebook) consist of graphs where nodes represent web pages or users and edges denote hyperlinks or social connections, respectively; the Biological datasets include molecular and protein structure graphs from AIDS, Enzymes, and Proteins, originally designed for graph classification but adapted here for node classification; the COIL-DEL dataset contains graphs corresponding to 2D images from COIL-100, with nodes representing superpixels and edges indicating spatial relationships; and finally, the Miscellaneous category encompasses the Karate and Actor datasets, where the former is a small social network and the latter represents co-occurrence relations among actors with node features derived from associated keywords.



%In our study, we compared three GNN models: the Graph Convolutional Network (GCN) \cite{kipf2016semi}, Chebyshev Network (ChebNet) \cite{he2022convolutional}, and GraphConv Networks \cite{morris2019weisfeiler}. GCN simplifies spectral convolutions by using only the first two Chebyshev polynomials, effectively capturing local structures with low computational cost. ChebNet extends this idea by using Chebyshev polynomials up to the K-th order, which increases expressiveness at the expense of higher computational complexity. In contrast, the GraphConv layer learns adaptive aggregation functions for neighbors, offering a flexible balance between efficiency and expressiveness that can better handle heterogeneous or noisy graph data.


%To evaluate our proposed approach, we compared it against a comprehensive set of baseline methods. Specifically, we implemented three naive baselines—random-edges, random-features, and ego-graph—alongside three state-of-the-art techniques: CF-GNNExplainer \cite{lucic2022cf}, CounterFactual and Factual Explainer (CFF) \cite{tan2022learning}, and UNR \cite{kangunr}. The random-edges baseline perturbs the graph's adjacency matrix by applying a randomly generated modification to each entry and then thresholding the result to determine edge presence. Similarly, the random-features method applies a random perturbation to the node feature matrix to produce modified features. The ego-graph approach extracts a subgraph centered on a given node along with its immediate neighbors, thereby focusing on local connectivity patterns. CF-GNNExplainer is a perturbation-based counterfactual explainer that learns a binary mask to sparsify the adjacency matrix, thereby altering the model's prediction, with our implementation further exploiting edge weights to enhance efficiency. CFF leverages ideas from causal inference to formulate an optimization problem that balances factual and counterfactual reasoning, while UNR generates counterfactual explanations for unsupervised node representations by identifying subgraphs whose perturbations significantly affect a node's nearest neighbors in the embedding space via a reward-guided search strategy. These baselines provide a robust framework for assessing the quality of counterfactual explanations generated by our COMBINEX solution.



%In our evaluation, we quantify the quality of generated counterfactuals using a set of key metrics. \textbf{Validity} is measured as an indicator function that returns 1 when the counterfactual changes the model’s prediction relative to the original instance, and 0 otherwise, following the definitions in \cite{guidotti2019factual, guidotti2022counterfactual}. \textit{Fidelity}, as described in \cite{Prado_Romero_2023}, assesses how well the explanation aligns with the oracle’s decisions by comparing the predictions on the factual and counterfactual samples relative to the true label, yielding values of 1, 0, or -1. To capture the sparsity of the modifications, we compute \textbf{node sparsity} as the ratio of altered node features between the factual and counterfactual graphs, and \textbf{edge sparsity} as the ratio of modified edges determined by differences in their adjacency matrices. Additionally, we introduce a \textbf{distribution distance} metric, which calculates the \(L_2\) distance between the embedded representation of a graph—obtained via the oracle—and the overall mean embedding of the dataset. Together, these metrics provide a comprehensive framework for evaluating both the structural and behavioral fidelity of counterfactual explanations.

%Summarizing, Section \ref{sec:dataset} provides a comprehensive description of the datasets used in our experiments, while Section \ref{sec:baselines} discusses the baseline methods against which our proposed approach is compared. Section \ref{sec:metrics} defines the key metrics used to evaluate counterfactual generation methods, Section \ref{sec:sett} outlines the experimental settings, and finally, Section \ref{sec:results} presents the main experimental results and analysis, discussing the implications of our findings in both node and graph classification tasks. Due to the large amount of data collected during the evaluation we encourage the reader to see Appendix \ref{subsec:graphc} and \ref{subsec:nodec} to assess the extensive evaluation of our proposed approach.

%\subsection{Baselines}\label{sec:baselines}
%To evaluate the performance of our solution, we implemented five different baseline methods. Specifically, we selected three naive approaches alongside two state-of-the-art techniques such as CF-GNNExplainer\cite{lucic2022cf}, CounterFactual and Factual Explainer (CFF)\cite{tan2022learning} and UNR\cite{kangunr}. The naive approaches include \textit{random-edges}, \textit{random-features}, and \textit{ego-graph}.
%\subsubsection{Random-edges} This method takes an adjacency matrix $\mathbf{A}$ and applies a randomly generated perturbation matrix $\widetilde{\mathbf{P}}_A$. Each element $p_A^{ij}$ in the perturbation matrix $\widetilde{\mathbf{P}}_A$ is independently sampled from a uniform distribution $U(0, 1)$ and then thresholded using the function described in Equation \ref{eq:thresh}. The modified adjacency matrix is obtained by computing the Hadamard product between the original matrix and the perturbed one, namely: $\widetilde{\mathbf{A}} = \mathbf{A} \odot T(\widetilde{\mathbf{P}}_A)$.
%\subsubsection{Random-features} Similarly to the random-edges baseline, this method takes a nodes features matrix $\mathbf{V}_x$ and applies a randomly generated perturbation matrix $\widetilde{\mathbf{P}}_x$ drawn from a uniform distribution $U(0, 1)$ and then it applies the function in Equation \ref{eq:thresh}, resulting in a modified nodes features matrix $\widetilde{\mathbf{V}}_x = \mathbf{V} \odot T(\widetilde{\mathbf{P}}_x)$. 

%\subsubsection{Ego-graph}
%An ego graph is a subgraph that centers on a particular node called the "ego" and includes the node along with its immediate neighbors, known as "alters," and the edges between them. This localized view highlights the direct connections of the ego within the larger network.
%\subsubsection{CFF} takes insights into counterfactual and factual reasoning from causal inference theory to solve learning and evaluation problems in explainable GNNs. They propose a model-agnostic framework by formulating an optimization problem based on both casual perspectives. To do that, they set up an optimization problem to find the masks needed to remove edges or features. They define a contrastive loss that takes into account factual and counterfactual reasoning. The goal is to find a simple but robust explanation. 
%The contrastive loss to minimize is defined as: 

%\begin{equation*}
%\begin{split}
%\operatorname{minimize}\left\|\mathbf{M}_{k}^{*}\right\|_{1}+\left\|\mathbf{F}_{k}^{*}\right\|_{1}+\lambda\left(\alpha L_{f}+(1-\alpha) L_{c}\right) 
%\end{split}
%\end{equation*}


%\noindent where $L_f$ and $L_c$ describe respectively the factual and the counterfactual explanation strength. The terms $\mathbf{M}_{k}^{*}$ and $\mathbf{F}_{k}^{*}$ on the other hand, denote the masks.

%\subsubsection{CF-GNNExplainer} is a perturbation-based counterfactual explainer. It defines $\bar{\mathbf{A}}_{v} = \mathbf{P} \odot \mathbf{A}_v$, where $\mathbf{P}$ is a binary perturbation matrix that sparsifies $\mathbf{A}_v$. The goal is to find $\mathbf{P}$ for a given node $v$ such that $\Phi(\mathbf{A}_v, x) \neq \Phi(\mathbf{P} \odot \mathbf{A}_v, x)$. To find $\mathbf{P}$, CF-GNNExplainer exploits a technique to train sparse neural networks to zero out entries in the adjacency matrix (i.e., removing edges). This results in the deletion of the edge between node $i$ and node $j$. In order to make this explainer more efficient and faster we slightly modified the core implementation. In particular we used the possibility to exploit the edge weights to turn off edges without using the dense adjacency matrix.


%\subsubsection{UNR}
%UNR-Explainer generates counterfactual (CF) explanations for unsupervised node representation learning by identifying subgraphs whose perturbation significantly alters a node’s \textit{top-k} nearest neighbors in the embedding space. A counterfactual explanation modifies the graph to ensure a change in the top-k nearest neighbors of a node. The importance of a subgraph is measured by how much this modification affects the embedding structure.
%To identify the most relevant subgraph, UNR-Explainer employs Monte Carlo Tree Search (MCTS), leveraging a reward-based exploration strategy. 
%The search is guided by:
%\begin{equation}
%    UCB(N_i, a) = Q(N_i, a) + \lambda \frac{\sqrt{\ln(C(N_0,a))}}{C(N_i,a)}.
%\end{equation}
%A restart mechanism improves exploration, ensuring diverse candidate explanations.

%\subsection{Evaluation Metrics}\label{sec:metrics}
%In the context of counterfactual example generation, it is essential to have a set of metrics to evaluate the effectiveness and quality of the generated counterfactuals. These metrics help in assessing various aspects of the counterfactuals, such as their validity, fidelity, and sparsity, among others. In this section, we will discuss the key metrics used for evaluating counterfactual example generation, their definitions, and how they are computed.\begin{itemize}
    %\item \textit{\textbf{Validity} (Correctness)}: as defined in \cite{guidotti2019factual, guidotti2022counterfactual} indicates whether the explainer is capable of producing a valid counterfactual explanation (i.e., the example has a different classification from the original instance). More formally, given the original instance $x$, the produced example $ x'$, and oracle $\Phi$. Summing up, the validity is an indicator function $\Omega(x, x') = \mathds{1}[\Phi(x) \neq \Phi(x')]$.
    
    %\item \textit{\textbf{Fidelity}}: as defined in \cite{Prado_Romero_2023} measures how faithful the explanations are to the oracle considering their correctness. Given the input $x$, its true label $y_x$ , and its counterfactual $x'$, fidelity is defined as $\Psi(x, x') = \chi(x) - \mathds{1}[\Phi(x' ) = y_x ]$ where $\chi(x)$ is an indicator function that describes the oracle's accuracy, namely, $\mathds{1}[\Phi(x ) = y_x ]$. Notice that $\Psi(x, x' )$ can assume three values. A value of 1 implies that both the explainer and oracle are working correctly. 
    %It is straightforward to confirm that $\chi(x)$ must output 1 (i.e., $\Phi(x) = y_x$) while the indicator should yield 0 (i.e., $\Phi(x') \neq y_x$). Values of 0 or -1 signal an issue with either the explainer or the oracle. However, it is not possible to determine whether the fault lies with the explainer or the oracle. This reflects a limitation of fidelity, as it evaluates correctness based on the true label $y_x$ rather than the prediction $\Phi(x)$.

    %\item \textit{\textbf{Fidelity}}: As defined in \cite{Prado_Romero_2023}, fidelity measures how accurately explanations align with the oracle’s decisions. Given an input $x$ with true label $y_x$ and its counterfactual $x'$, it is defined as $\Psi(x, x') = \chi(x) - \mathds{1}[\Phi(x' ) = y_x]$, where $\chi(x) = \mathds{1}[\Phi(x) = y_x]$ represents the oracle’s accuracy. Fidelity takes values in $\{1, 0, -1\}$: a value of 1 indicates both the oracle and explainer perform correctly, while 0 or -1 suggest an inconsistency, though it remains unclear whether the issue stems from the oracle or the explainer. This limitation arises because fidelity evaluates correctness based on the true label rather than the model’s prediction.    
    %\item \textit{\textbf{Node Sparsity}}: this metric measures the sparsity of node attributes in a counterfactual graph compared to a factual graph. It quantifies how many node attributes have been modified between the factual and counterfactual node attribute matrices. The number of modified attributes is computed as follows:
    %$\Delta_V = \sum (\mathbf{V}_{x} \neq \mathbf{V}_{pert})$ where $\mathbf{V}_{x}$ and $\mathbf{V}_{pert}$ are the node attribute matrices of the factual and counterfactual graphs, respectively. The node sparsity is then computed as the ratio of modified attributes to the total number of attributes in the factual graph.

   % \item \textit{\textbf{Edge Sparsity}}: This metric quantifies the proportion of edges modified between the factual and counterfactual graphs by comparing their adjacency matrices. The total count of differing entries is summed, and for undirected graphs, divided by 2. It is computed as $\Delta_A =\sum (\mathbf{A} \neq \widetilde{\mathbf{A}})$, where $\mathbf{A}$ and $\widetilde{\mathbf{A}}$ are the factual and counterfactual adjacency matrices. Edge sparsity is the ratio of modified edges to the total edges in the factual graph.
    %\item \textit{\textbf{Edge Sparsity}}: this metric measures the sparsity of edges in a counterfactual graph compared to a factual graph. It quantifies how many edges have been modified between the factual and counterfactual adjacency matrices. The number of modified edges is computed by comparing the adjacency matrices of the factual and counterfactual graphs. The total count of differing entries (edges) is summed, and in the case of undirected graphs, it is divided by 2. We can compute this metric as follows: $\Delta_A =\sum (\mathbf{A} \neq \widetilde{\mathbf{A}})$ where $\mathbf{A}$ and $\widetilde{\mathbf{A}}$ are the adjacency matrices of the factual and counterfactual graphs, respectively. The edge sparsity is then calculated as the ratio of the number of modified edges to the total number of edges in the factual graph.


    %\item \textit{\textbf{Distribution Distance}}: To better understand how counterfactual samples are distributed within the original data, we defined a cluster-like distance measure. Given a graph $ G = (V, E) $, we compute the embedded graph representation for $G$ using the oracle $\Phi$ as follows: $G_{h} = mean(\Phi(G))$. Then, we calculate the $L_2$ distance between $G_{h}$ and the distribution mean value $\mu_{G}$, namely, $d_v = \| \mathbf{G}_h - \mathbf{\mu}_G \|_2$. The centroid value $\mu_{G}$ is computed as in Equation \ref{eq:mu}:
    %\begin{equation}\label{eq:mu}
    %    \mu_{G} = \frac{1}{|D|}\sum_{G \in D} mean(\Phi(G)).
   % \end{equation}
%\end{itemize}

% \subsection{Settings}\label{sec:sett}
% The experiments were conducted on two machines, each equipped with an Nvidia GTX 4090 GPU, 64 GB of RAM, and an AMD Ryzen 9 7900 processor. We employed 4-fold cross-validation to ensure the robustness of our results. Additionally, an extensive hyperparameter tuning process was carried out to identify the optimal parameter configurations for our method, including learning rate: 0.1, beta: 0.5, number of epochs: 500. For the baseline models, we adhered to the hyperparameters specified in the original papers to ensure a fair comparison and consistency with previously reported results. To get a complete overview of all the parameters and settings, we refer the reader to the code on GitHub and go to Appendix \ref{}

\newcommand{\btable}[3]{
    \begin{table*}[h!]
        \begin{center}
            \caption{#2\label{#3}}
            \begin{tabular}{#1}
    }

\newcommand{\etable}{
    \end{tabular}
    \end{center}
    \end{table*}
}

%Usage: \mc{number of columns spanned}{major column heading}
\newcommand{\mc}[2]{\multicolumn{#1}{c}{#2}}

\btable{l|ccccc}{Results for the CiteSeer dataset (node classification task) and the AIDS dataset (graph classification task).}{tab:results_general} \hline\hline
\mc{1}{\textbf{Explainer}} 
& \mc{1}{\textbf{Validity} $\uparrow$} 
& \mc{1}{\textbf{Fidelity} $\uparrow$} 
& \mc{1}{\textbf{Distribution Distance} $\downarrow$} 
& \mc{1}{\textbf{Node Sparsity} $\downarrow$} 
& \mc{1}{\textbf{Edge Sparsity} $\downarrow$} 
\\ \hline % column 4 blank, for spacing\hline

%\textbf{Explainers} 
%& \textit{mean($\pm$std)}
%& \textit{mean($\pm$std)}
%& \textit{mean($\pm$std)}
%& \textit{mean($\pm$std)}
%& \textit{mean($\pm$std)} \\ \hline

 & \mc{5}{\textbf{Dataset: CiteSeer -  Model: GCNConv -  Task: Node Classification}} \\ \hline


COMBINEX$_{\textit{def}}$ &$\mathbf{1.000 (\pm 0.000)}$ & $\mathbf{0.755 (\pm 0.007)}$ & $5.620 (\pm 0.212)$ & $\uline{0.031 (\pm 0.002)}$ & $\mathbf{0.000 (\pm 0.000)}$ \\
COMBINEX$_{\textit{feat}}$ &$\mathbf{1.000 (\pm 0.000)}$ & $\mathbf{0.755 (\pm 0.007)}$ & $2.464 (\pm 0.002)$ & $\mathbf{0.001 (\pm 0.000)}$ & $n.d.(\pm n.d.)$ \\
COMBINEX$_{\textit{dyn}}$ &$\mathbf{1.000 (\pm 0.000)}$ & $0.750 (\pm 0.000)$ & $10.997 (\pm 0.201)$ & $0.095 (\pm 0.002)$ & $\mathbf{0.000 (\pm 0.000)}$ \\
COMBINEX$_{\textit{exp}}$ &$\mathbf{1.000 (\pm 0.000)}$ & $\mathbf{0.755 (\pm 0.007)}$ & $27.121 (\pm 0.111)$ & $0.313 (\pm 0.002)$ & $\mathbf{0.000 (\pm 0.000)}$ \\
COMBINEX$_{\textit{lin}}$ &$\mathbf{1.000 (\pm 0.000)}$ & $\mathbf{0.755 (\pm 0.007)}$ & $11.231 (\pm 0.300)$ & $0.090 (\pm 0.004)$ & $\mathbf{0.000 (\pm 0.000)}$ \\
COMBINEX$_{\textit{sin}}$ &$\mathbf{1.000 (\pm 0.000)}$ & $\uline{0.752 (\pm 0.003)}$ & $11.557 (\pm 0.304)$ & $0.094 (\pm 0.004)$ & $\mathbf{0.000 (\pm 0.000)}$ \\
EGO &$0.012 (\pm 0.003)$ & $0.250 (\pm 0.500)$ & $1.995 (\pm 0.206)$ & $n.d.(\pm n.d.)$ & $0.937 (\pm 0.003)$ \\
Random Edges &$0.123 (\pm 0.009)$ & $0.476 (\pm 0.099)$ & $\mathbf{1.716 (\pm 0.089)}$ & $n.d.(\pm n.d.)$ & $0.393 (\pm 0.014)$ \\
Random Features &$\uline{0.514 (\pm 0.065)}$ & ${0.721 (\pm 0.024)}$ & $32.926 (\pm 0.291)$ & $\uline{0.489 (\pm 0.000)}$ & $n.d.(\pm n.d.)$ \\
CFF &$0.010 (\pm 0.004)$ & $0.125 (\pm 0.629)$ & $2.325 (\pm 1.323)$ & $n.d.(\pm n.d.)$ & $0.594 (\pm 0.194)$ \\
CF-GNNExplainer &$0.108 (\pm 0.014)$ & $0.534 (\pm 0.103)$ & $\uline{1.783 (\pm 0.134)}$ & $n.d.(\pm n.d.)$ & $\uline{0.070 (\pm 0.022)}$ \\
UNR &$0.047 (\pm 0.012)$ & $0.202 (\pm 0.162)$ & $2.389 (\pm 0.336)$ & $n.d.(\pm n.d.)$ & $0.186 (\pm 0.068)$ \\ \hline

 & \mc{5}{\textbf{Dataset: AIDS -  Model: GCNConv -  Task: Graph Classification}} \\ \hline


COMBINEX$_{\textit{def}}$ &$\mathbf{1.000 (\pm 0.000)}$ & $\uline{0.517 (\pm 0.007)}$ & $3.665 (\pm 0.493)$ & $\mathbf{0.087 (\pm 0.009)}$ & $\mathbf{0.004 (\pm 0.003)}$ \\
EGO &$0.015 (\pm 0.008)$ & $\mathbf{0.562 (\pm 0.315)}$ & $\mathbf{2.301 (\pm 0.101)}$ & $n.d.(\pm n.d.)$ & $0.892 (\pm 0.025)$ \\
Random Edges &$\uline{0.458 (\pm 0.003)}$ & $-0.033 (\pm 0.014)$ & $\uline{2.566 (\pm 0.012)}$ & $n.d.(\pm n.d.)$ & $0.292 (\pm 0.004)$ \\
Random Features &$\mathbf{1.000 (\pm 0.000)}$ & $0.513 (\pm 0.008)$ & $24.123 (\pm 1.771)$ & $\uline{0.521 (\pm 0.022)}$ & $n.d.(\pm n.d.)$ \\
CFF &$n.d.(\pm n.d.)$ & $n.d.(\pm n.d.)$ & $n.d.(\pm n.d.)$ & $n.d.(\pm n.d.)$ & $n.d.(\pm n.d.)$ \\
CF-GNNExplainer &$\uline{0.458 (\pm 0.019)}$ & $-0.034 (\pm 0.034)$ & $2.618 (\pm 0.022)$ & $n.d.(\pm n.d.)$ & $\uline{0.076 (\pm 0.005)}$ \\

\hline


 & \mc{5}{\textbf{Dataset: CiteSeer - Model: ChebConv - Task: Node Classification} } \\ \hline


COMBINEX$_{\textit{def}}$ &$\mathbf{1.000 (\pm 0.000)}$ & $\uline{0.753 (\pm 0.022)}$ & $\uline{4.165 (\pm 0.215)}$ & $\mathbf{0.021 (\pm 0.004)}$ & $\mathbf{0.000 (\pm 0.000)}$ \\

EGO &$0.000 (\pm 0.000)$ & $n.d.(\pm n.d.)$ & $n.d.(\pm n.d.)$ & $n.d.(\pm n.d.)$ & $n.d.(\pm n.d.)$ \\
Random Edges &$0.000 (\pm 0.000)$ & $n.d.(\pm n.d.)$ & $n.d.(\pm n.d.)$ & $n.d.(\pm n.d.)$ & $n.d.(\pm n.d.)$ \\
Random Features &$\uline{0.675 (\pm 0.114)}$ & $0.742 (\pm 0.030)$ & $32.451 (\pm 0.242)$ & $\uline{0.491 (\pm 0.001)}$ & $n.d.(\pm n.d.)$ \\
CFF &$0.210 (\pm 0.019)$ & $\mathbf{0.805 (\pm 0.126)}$ & $\mathbf{2.126 (\pm 0.176)}$ & $n.d.(\pm n.d.)$ & $\uline{0.615 (\pm 0.047)}$ \\
CF-GNNExplainer &$0.000 (\pm 0.000)$ & $n.d.(\pm n.d.)$ & $n.d.(\pm n.d.)$ & $n.d.(\pm n.d.)$ & $n.d.(\pm n.d.)$ \\
UNR &$0.000 (\pm 0.000)$ & $n.d.(\pm n.d.)$ & $n.d.(\pm n.d.)$ & $n.d.(\pm n.d.)$ & $n.d.(\pm n.d.)$ \\
\hline
 & \mc{5}{\textbf{Dataset: AIDS - Model: ChebConv - Task: Graph Classification}} \\ \hline
COMBINEX$_{\textit{def}}$ &$\mathbf{1.000 (\pm 0.000)}$ & $\mathbf{0.513 (\pm 0.007)}$ & $4.772 (\pm 0.475)$ & $\mathbf{0.079 (\pm 0.002)}$ & $\mathbf{0.000 (\pm 0.000)}$ \\
EGO &$0.000 (\pm 0.000)$ & $n.d.(\pm n.d.)$ & $n.d.(\pm n.d.)$ & $n.d.(\pm n.d.)$ & $n.d.(\pm n.d.)$ \\
Random Edges &$0.240 (\pm 0.000)$ & $\uline{-0.944 (\pm 0.000)}$ & $2.759 (\pm 0.015)$ & $n.d.(\pm n.d.)$ & $\uline{0.237 (\pm 0.007)}$ \\
Random Features &$\mathbf{1.000 (\pm 0.000)}$ & $\mathbf{0.513 (\pm 0.007)}$ & $24.364 (\pm 1.838)$ & $\uline{0.520 (\pm 0.015)}$ & $n.d.(\pm n.d.)$ \\
CFF &$0.002 (\pm 0.003)$ & $-1.000 (\pm 0.000)$ & $\mathbf{2.525 (\pm 0.000)}$ & $n.d.(\pm n.d.)$ & $0.565 (\pm 0.000)$ \\
CF-GNNExplainer &$\uline{0.241 (\pm 0.002)}$ & $-0.945 (\pm 0.001)$ & $\uline{2.755 (\pm 0.018)}$ & $n.d.(\pm n.d.)$ & $\mathbf{0.000 (\pm 0.000)}$ \\
\hline

 & \mc{5}{\textbf{Dataset: CiteSeer - Model: GraphConv - Task: Node Classification}} \\ \hline


COMBINEX$_{\textit{def}}$ &$\mathbf{1.000 (\pm 0.000)}$ & $\uline{0.792 (\pm 0.010)}$ & $7.153 (\pm 0.668)$ & $\mathbf{0.055 (\pm 0.006)}$ & $\mathbf{0.000 (\pm 0.001)}$ \\

EGO &$0.005 (\pm 0.007)$ & $0.750 (\pm 0.354)$ & $\mathbf{1.232 (\pm 0.294)}$ & $n.d.(\pm n.d.)$ & $0.956 (\pm 0.055)$ \\
Random Edges &$0.076 (\pm 0.006)$ & $0.475 (\pm 0.204)$ & $1.454 (\pm 0.080)$ & $n.d.(\pm n.d.)$ & $0.440 (\pm 0.020)$ \\
Random Features &$\uline{0.481 (\pm 0.098)}$ & $0.725 (\pm 0.039)$ & $32.458 (\pm 0.231)$ & $\uline{0.491 (\pm 0.001)}$ & $n.d.(\pm n.d.)$ \\
CFF &$0.165 (\pm 0.028)$ & $\mathbf{0.867 (\pm 0.112)}$ & $2.316 (\pm 0.157)$ & $n.d.(\pm n.d.)$ & $0.570 (\pm 0.041)$ \\
CF-GNNExplainer &$0.071 (\pm 0.014)$ & $0.579 (\pm 0.053)$ & $\uline{1.353 (\pm 0.190)}$ & $n.d.(\pm n.d.)$ & $\uline{0.042 (\pm 0.031)}$ \\
UNR &$0.017 (\pm 0.007)$ & $0.375 (\pm 0.479)$ & $1.854 (\pm 0.196)$ & $n.d.(\pm n.d.)$ & $0.144 (\pm 0.068)$ \\
\hline

 & \mc{5}{\textbf{Dataset: AIDS - Model: GraphConv - Task: Graph Classification}} \\ \hline


COMBINEX$_{\textit{def}}$ &$\mathbf{1.000 (\pm 0.000)}$ & $\mathbf{0.503 (\pm 0.013)}$ & $3.767 (\pm 0.252)$ & $\mathbf{0.096 (\pm 0.006)}$ & $\mathbf{0.001 (\pm 0.000)}$ \\
EGO &$0.122 (\pm 0.011)$ & $0.426 (\pm 0.020)$ & $\mathbf{1.805 (\pm 0.072)}$ & $n.d.(\pm n.d.)$ & $0.917 (\pm 0.002)$ \\
Random Edges &$0.290 (\pm 0.018)$ & $-0.576 (\pm 0.047)$ & $\uline{2.615 (\pm 0.049)}$ & $n.d.(\pm n.d.)$ & $0.259 (\pm 0.010)$ \\
Random Features &$\uline{0.815 (\pm 0.282)}$ & $\uline{0.459 (\pm 0.114)}$ & $27.592 (\pm 1.728)$ & $\uline{0.517 (\pm 0.012)}$ & $n.d.(\pm n.d.)$ \\
CFF &$0.008 (\pm 0.006)$ & $-1.000 (\pm 0.000)$ & $10.201 (\pm 2.868)$ & $n.d.(\pm n.d.)$ & $0.278 (\pm 0.293)$ \\
CF-GNNExplainer &$0.298 (\pm 0.010)$ & $-0.621 (\pm 0.048)$ & $2.619 (\pm 0.031)$ & $n.d.(\pm n.d.)$ & $\uline{0.025 (\pm 0.002)}$ \\ \hline



\etable

\subsection{Results}\label{sec:results}

\input{results}
%This section discusses the key findings that emerged from our experiments, using the simplest variant of our method, COMBINEX$_{\textit{def}}$.
%The complete set of experimental results are reported in Appendix~\ref{subsec:graphc} and \ref{subsec:nodec}. Table \ref{tab:results_general} presents the results obtained using a model with GCNConv layers on two distinct datasets. The first dataset, Citeseer, is used for a node classification task, while the second, AIDS, is employed for a graph classification task.

%Focusing first on the Citeseer results, our COMBINEX Cons. variant clearly stands out by achieving perfect validity (1.000 $\pm$ 0.000) and strong fidelity (0.755 $\pm$ 0.007), accompanied by very low node sparsity (0.031 $\pm$ 0.002) and zero edge sparsity. These metrics indicate that the explanations produced are both highly faithful to the oracle's decisions and extremely concise. In contrast, CF-GNNExplainer returns very low validity (0.108 $\pm$ 0.014) and exhibits a higher distribution distance (1.783 $\pm$ 0.134, underlined as the second best), while CFF achieves almost negligible validity (0.010 $\pm$ 0.004) and very low fidelity (0.125 $\pm$ 0.629). Although Random Features reach high validity (0.514 $\pm$ 0.065) and fidelity (0.721 $\pm$ 0.024), their explanation distribution is poorly aligned with the oracle, as demonstrated by a very high distribution distance (32.926 $\pm$ 0.291). Overall, these comparisons underscore that COMBINEX Cons. offers a well-balanced solution, prioritizing both interpretability and faithfulness.

%Turning to the AIDS dataset, the performance trends are analogous. COMBINEX Cons. again achieves perfect validity (1.000 $\pm$ 0.000) with competitive fidelity (0.855 $\pm$ 0.003) and excellent sparsity measures (node sparsity of 0.018 $\pm$ 0.001 and zero edge sparsity). Although CF-GNNExplainer and CFF deliver lower distribution distances (0.598 $\pm$ 0.039 and 0.482 $\pm$ 0.146, respectively), their validity scores are substantially lower (e.g., CF-GNNExplainer has 0.165 $\pm$ 0.022), and CFF even reports a negative fidelity (–0.556 $\pm$ 0.770), which is undesirable. Furthermore, while EGO and Random Edges exhibit moderate performance in some metrics, they do not match the balanced performance of COMBINEX Cons. Notably, Random Features, despite reaching high fidelity (0.947 $\pm$ 0.005) and low node sparsity (0.492 $\pm$ 0.000), suffer from an excessively high distribution distance (18.795 $\pm$ 0.313), indicating that their explanations capture extraneous or spurious information.

%Overall, the results in Table~\ref{tab:results_general} demonstrate that our COMBINEX approach consistently delivers explanations with perfect validity and highly sparse representations, while maintaining competitive fidelity and a reasonable distribution distance. This balance is critical for ensuring that explanations are both faithful to the underlying model (with GCNConv layers achieving oracle accuracies of 0.97 on Citeseer and 0.99 on AIDS) and interpretable for end users. Compared to alternative explainers, COMBINEX provides a more reliable and concise summary of the model’s decision process across both node and graph classification tasks.





\subsection{The Impact of the Scheduling Policy $\alpha$}
\label{subsec:alpha}
The results with different $\alpha$ values reported in Table~\ref{tab:results_general} are shown only for a single dataset, model, and task due to space constraints. For the full results, refer to the Appendix~\ref{subsec:graphc} and \ref{subsec:nodec}. 

Our experiments (see Table~\ref{tab:results_general}) demonstrate that the choice of $\alpha$ in the COMBINEX framework influences performance metrics. The constant policy (\textit{def}), which maintains a fixed value for $\alpha$ throughout the optimization process, achieves the lowest distribution distance (5.620) and one of the lowest node sparsity values (0.031), indicating a more controlled perturbation that preserves the original graph structure. Similarly, the feature-only variant (\textit{feat}) achieves a comparably high fidelity (0.755) while minimizing node sparsity (0.001), suggesting that altering only node features without modifying edges leads to minimal changes while maintaining counterfactual validity.

Conversely, the exponential policy (\textit{exp}) leads to the highest distribution distance ($27.121$) and a notable increase in node sparsity ($0.313$), reflecting the aggressive perturbations caused by the rapidly decaying $\alpha$. In contrast, the dynamic, linear, and sinusoidal policies achieve intermediate distribution distances ($10.997$–$11.557$) and maintain node sparsity around $0.090$–$0.095$, suggesting a more balanced trade-off between modification extent and stability.

These findings, along with the others presented in Appendix~\ref{subsec:graphc} and \ref{subsec:nodec} highlight that the impact of $\alpha$ is highly dependent on both the scheduling policy, the oracle model, and the dataset characteristics. The optimal choice of $\alpha$ should therefore be carefully tuned based on the complexity of the graph data and the interpretability objectives of the counterfactual explanations.

%Our experiments (see Table~\ref{tab:results_general}) demonstrate that the choice of $\alpha$ in the COMBINEX framework can influences performance metrics. When using the constant policy (\textit{def}), which maintains a fixed value for $\alpha$ throughout the optimization process, we observe lower distribution distances and reduced node sparsity. In contrast, the feature-only variant (\textit{feat}), which perturbs only node features while leaving edge structures unaltered, achieves similar fidelity but exhibits slightly different sparsity characteristics. Moreover, the exponential policy (\textit{exp}) results in significantly higher distribution distances and increased node sparsity, indicating that a more aggressive decay in $\alpha$ leads to larger perturbations. The dynamic, linear, and sinusoidal policies yield intermediate outcomes, generally balancing the extent of modifications between the two extremes. 
%It is important to note that the impact of $\alpha$ depends not only on the scheduling policy but also on the model architecture and the dataset; different graph structures and learning tasks may favor distinct $\alpha$ strategies. Overall, these findings underscore that while the constant and feature-only approaches tend to provide a more favorable trade-off between achieving the desired counterfactual change and preserving the original graph integrity, the optimal choice of $\alpha$ scheduling should be tailored to the specific model and dataset at hand.



%Results are presented in Tables   \ref{tab:results_planetoid}, \ref{tab:webkb}, \ref{tab:catalog}, \ref{tab:misc}. Each table presents a comparative analysis of various techniques across different datasets, focusing on six key metrics: Validity, Fidelity, Distribution Distance, Node Sparsity, Edge Sparsity, and Model Accuracy. It is important to notice that some techniques have not defined metrics, for instance, in  and Random Features the metric Edge Sparsity is not defined because the adjacency matrix does not change using these algorithms. Anyway, to be meticulous we decided to include every metric helping the reader understanding different aspects of the performance evaluation.

%In Table \ref{tab:results_planetoid}, we present the results for the Planetoid datasets: our approach has higher Validity and Fidelity in every dataset by a large margin, namely, in CiteSeer using  we got a $112\%$ improvement in Validity and a $31\%$ improvement in Fidelity if compared to the second best technique. We got also a better Node Sparsity ($33\%$ improvement) if compared to the nodes features modification technique. Regarding the distribution distance our approach is the second best after EGO. Cora and PubMed dataset sharply confirm the improvement we got in CiteSeer dataset: Validity got respectively a $294\%$ and a $62\%$ improvement in Cora and PubMed if compared to the second best approach. We also got a minor improvement for Fidelity in Cora (about $6\%$), additionally, our approach is the best one also in PubMed in terms of Fidelity, however, the difference between our approach and the second-best one is negligible, approximately $1\%$. Our approach had also better Node Sparsity, both in Cora and PubMed, decreasing it to one third of the baseline value in the former case up to one sitxh in the latter case.

%We got similar results for the WebKB datasets (Table \ref{tab:webkb}): on average, we observed an improvement very close to 50\% in Validity with our approach if compared to the second best. Regarding Fidelity, except for the Wisconsin dataset, our approach always maintained the highest Fidelity with a 14\% and 30\% improvement respectively in Texas and Cornell, along with the smallest Node Sparsity in every case. The distribution distance instead results to be lower using other algorithms for these datasets.  

%For the Attributed datasets, we also got similar results: in the Wiki dataset, we got a 234\% increase in Validity if compared to the second-best algorithm. The same is true also for the BlogCatalog dataset, which has a comparable increase in Validity. In the Facebook dataset instead, Validity is generally lower compared to all the other datasets, but our approach got a 24\% increase over the second-best result. Our approach has consistently better results also regarding Fidelity and Node Sparsity.  has also a low Distribution Distance in the Facebook dataset but not in the Wiki and BlogCatalog datasets. Results for the Attributed datasets are visible In Table \ref{tab:catalog}.

%Finally, we tested our approach using Karate and Actor datasets. Our algorithm has the highest Validity among all the other algorithms, even though the difference is not as pronounced as in the other tests: for the Karate dataset, we got a 21\% improvement, instead for the Actor dataset, we got a 2\% improvement w.r.t. the best baseline. We also got similar results for Node Sparsity and, at the end, we got the second-best result for the Karate dataset and the best one for the Actor dataset in Fidelity. Concerning Distribution Distance our approach is respectively the second and third best result.\\

%In conclusion, our comprehensive analysis demonstrates that , our proposed approach, consistently outperforms existing techniques across various datasets. Specifically, our method shows substantial improvements in Validity and Fidelity, which are critical metrics for model performance. Our approach achieved significant gains for the Planetoid datasets, with improvements up to 294\% in Validity and notable reductions in Node Sparsity, indicating a more efficient model representation.

%Similarly, in the WebKB datasets, our method enhanced Validity by nearly 50\% on average and maintained the highest Fidelity in most cases, along with the smallest Node Sparsity. Although other algorithms performed better in terms of Distribution Distance, our approach's overall performance remained superior.

%For the Attributed datasets, the results further validate the robustness of our approach, with increases in Validity and consistent improvements in Fidelity and Node Sparsity. Lastly, the Karate and Actor datasets also confirmed the efficacy of our method, achieving the highest Validity and competitive results in Fidelity and Distribution Distance.

%These findings underline the effectiveness of our approach in improving key performance metrics across diverse datasets, highlighting its potential for broader applications in graph-based learning tasks. Future work will focus on further refining our method and exploring its applicability to additional datasets and real-world scenarios.

%The results presented in Table~\ref{tab:results_general} compare multiple explainers on the CiteSeer, Cora, and PubMed datasets across five key evaluation metrics: \textit{Validity}, \textit{Fidelity}, \textit{Distribution Distance}, \textit{Node Sparsity}, and \textit{Edge Sparsity}. These metrics provide a comprehensive assessment of the explainers' ability to generate counterfactual explanations that are both reliable and interpretable.

%The first aspect to consider is the \textit{Validity} of the generated counterfactuals, which measures how often the generated examples successfully alter the model’s decision. COMBINEX Consistently achieves the highest \textit{Validity} across all datasets, reaching $1$ for CiteSeer and Cora, and $0.75$ for PubMed. These results suggest that COMBINEX reliably finds counterfactual instances that successfully change the prediction of the model. In contrast, other methods exhibit significantly lower \textit{Validity} scores, indicating that they fail to produce effective counterfactuals in many cases.

%The \textit{Fidelity} metric assesses whether the counterfactual examples remain faithful to the model’s decision boundary. COMBINEX also performs well in this aspect, with a fidelity of $0.755$ on CiteSeer and $0.855$ on Cora, ranking among the best methods. However, on PubMed, while achieving strong results ($0.754$), it is outperformed by UNR, which attains the highest possible value ($1.000$). This suggests that while COMBINEX effectively finds valid counterfactuals, there are instances where its generated counterfactuals do not align perfectly with the model’s decision-making process. This could indicate that COMBINEX's optimization procedure prioritizes finding minimal perturbations over strictly adhering to the model’s original behavior.


%A crucial aspect of counterfactual explanations is ensuring that they remain close to the original data distribution. The \textit{Distribution Distance} metric quantifies this aspect, where lower values are preferable. Although COMBINEX achieves superior \textit{Validity}, it does not always produce the most distributionally plausible counterfactuals. Notably, Random Features produces the worst results in this regard, particularly on CiteSeer and Cora, where its values are significantly larger than those of other explainers (e.g., $32.926$ on CiteSeer and $18.795$ on Cora). This suggests that randomly altering features without a structured approach generates unrealistic counterfactuals that deviate significantly from real-world data points.

%In contrast, CF-GNNExplainer and CFF achieve lower distribution distances, with CFF achieving the best result on Cora ($0.482$) and PubMed ($0.056$). However, these methods struggle with \textit{Validity}, meaning that although they find counterfactuals closer to the data distribution, they often fail to alter the model’s prediction. This suggests a potential trade-off between producing counterfactuals that remain close to real data and those that effectively manipulate the model’s decision boundary.

%Sparsity is a critical factor in interpretability, as it determines how many features or edges must be modified to generate a counterfactual. COMBINEX consistently produces the sparsest counterfactuals in terms of both \textit{Node Sparsity} and \textit{Edge Sparsity}, achieving near-zero modifications in all cases. This suggests that COMBINEX finds minimal yet effective perturbations, making its counterfactuals highly interpretable. For instance, in the CiteSeer dataset, COMBINEX achieves an \textit{Edge Sparsity} of $0.000$, while other methods, such as EGO and Random Edges, require significantly larger modifications ($0.937$ and $0.393$, respectively). This highlights COMBINEX's ability to generate explanations that require minimal structural changes to the graph.


%One possible explanation for COMBINEX's superior performance is its ability to balance perturbation optimization across both node features and edges, rather than focusing solely on one aspect. Unlike CF-GNNExplainer, which primarily modifies edges, or CFF, which focuses on node features, COMBINEX systematically optimizes both, ensuring a holistic approach to counterfactual generation.

%Another hypothesis is that COMBINEX's strong performance across multiple datasets suggests that it generalizes better across different graph structures. The consistent \textit{Validity} and high \textit{Fidelity} values indicate that its optimization process effectively adapts to varying graph topologies, preventing overfitting to specific datasets.

%Finally, the discrepancies observed in \textit{Distribution Distance} suggest that while COMBINEX effectively modifies the minimum necessary features, it may occasionally generate counterfactuals that slightly deviate from natural data distributions. This could be an area for future refinement, ensuring that counterfactuals remain as realistic as possible while maintaining high validity.


%Usage: \btable{table specs}{caption}{reference label}

\section{Feasibility of our Method}\label{sec:feasibility}

In this section, we discuss the feasibility of our method in comparison with the baselines. Our edge sparsification process uses the adjacency matrix perturbation approach introduced by Lucic et al.~\cite{lucic2022cf}, while addressing scalability challenges inherent to their method. Specifically, Lucic et al. represent edges using a sparse matrix format, which significantly limits the applicability of their algorithm to graphs containing more than 30--35 nodes. 
In Table \ref{tab:citeseer_time_memory}, for example, we compare the execution time and the memory needed for each explainer on the Citeseer dataset. 
\begin{table}[h]
\centering
\begin{tabular}{lcc}
\hline
\textbf{Explainer} & \textbf{Time (s) ($\pm$ std)} & \textbf{Memory (MB) ($\pm$ std)} \\
\hline
COMBINEX & $7.665 (\pm 0.245)$ & $1583.035 (\pm 97.627)$ \\
CF-GNNExplainer & $45.576 (\pm 1.306)$ & $2045.919 (\pm 88.096)$ \\
Random Features & $1.287 (\pm 0.025)$ & $1387.031 (\pm 151.101)$ \\
Random Edges & $1.350 (\pm 0.017)$ & $1359.386 (\pm 144.630)$ \\
EGO & $ 0.011 (\pm 0.002)$ & $1022.579 (\pm 14.825)$ \\
CFF & $ 8.936 (\pm 0.204)$ & $1469.579 (\pm 64.606)$ \\
UNR & $ 0.266 (\pm 0.327)$ & $ 1221.638 (\pm 99.061)$ \\
\hline
\end{tabular}
\caption{Execution time and memory usage for different explainers on the Citeseer dataset.}
\label{tab:citeseer_time_memory}
\end{table}
To overcome this limitation, we leverage a novel strategy that exploits the edge weight vector, which can be seamlessly integrated into various graph convolutional layers, such as \textit{ChebConv}, \textit{GCNConv}, and \textit{GraphConv}. Given a graph $ G(V, E) $ with $ |V| = n $, our approach introduces a perturbation vector $ \mathbf{EP}^{n\times 1} $ that effectively cancels out edges by feeding it into the GNN. The effectiveness of our edge weight sparsification technique is evident when analyzing the execution time results in Table \ref{tab:citeseer_time_memory}. Notably, CF-GNNExplainer requires $45.576$ seconds on average to generate explanations, whereas COMBINEX completes the same process in just $7.665$ seconds. This substantial improvement in runtime efficiency highlights the scalability advantage introduced by our sparsification technique. By reducing the computational overhead associated with handling edge deletions, COMBINEX maintains high explainability performance while significantly lowering execution time. 
%This result suggests that our method not only preserves interpretability but also enables practical application in larger graphs, where computational constraints often become a bottleneck.

We formally demonstrate that this technique is equivalent to performing edge deletion using the full adjacency matrix. For \textit{GCNConv}, this equivalence is trivial and follows directly from Theorem~\ref{theorem:gcn}. However, for \textit{ChebConv}, we establish that when the filter size is set to 1, edge nullification via the edge weight vector remains consistent with the behavior observed in \textit{GCNConv}. Conversely, when the filter size exceeds 1, such equivalence cannot be guaranteed (see Appendix~\ref{theorem:cheb}). The proof for GraphConv can be found in Appendix \ref{theorem:graph}.


%In this section, we discuss the feasibility of our method in comparison with the baselines.
%Our edge sparsification process uses the adjacency matrix perturbation approach introduced by Lucic et al.~\cite{lucic2022cf}, while addressing scalability challenges inherent to their method. Specifically, Lucic et al. represent edges using a sparse matrix format, which significantly limits the applicability of their algorithm to graphs containing more than 25--30 nodes due to computational constraints. In Table \ref{tab:citeseer_time_memory} for example we compared the execution time and the memory needed for each explainer on the Citeseer dataset. To overcome this limitation, we leverage a novel strategy that exploits the edge weight vector, which can be seamlessly integrated into various graph convolutional layers, such as \textit{ChebConv}, \textit{GCNConv}, and \textit{GraphConv}. Given a graph $ G(V, E) $ with $ |V| = n $, our approach introduces a perturbation vector $ \mathbf{EP}^{1\times n} $ that effectively cancels out edges by feeding it into the GNN.

%We formally demonstrate that this technique is equivalent to performing edge deletion using the full adjacency matrix. For \textit{GCNConv}, this equivalence is trivial and follows directly from Theorem~\ref{theorem:gcn}. However, for \textit{ChebConv}, we establish that when the filter size is set to 1, edge nullification via the edge weight vector remains consistent with the behavior observed in \textit{GCNConv}. Conversely, when the filter size exceeds 1, such equivalence cannot be guaranteed (see Appendix~\ref{theorem:cheb}). The proof for GraphConv can be found in Appendix \ref{theorem:graph}.


%%%%%%%%%%%%%

\begin{theorem} \label{theorem:gcn}[Equivalence of Edge Weight Nullification and Adjacency Matrix Edge Removal in GCNs]
Let $ G = (V, E) $ be a graph with $ n $ nodes and $ m $ edges. Let $ \mathbf{X} \in \mathbb{R}^{n \times d} $ be the node feature matrix, where each node has a feature vector of dimension $ d $. Consider a \textit{GCNConv} layer parameterized by a weight matrix $ \mathbf{W} \in \mathbb{R}^{d \times d'} $. Setting an edge weight to zero in the GCNConv’s edge weight vector is equivalent to removing the corresponding edge in the adjacency matrix representation.
\end{theorem}

\begin{proof}
We represent the edges of the graph using an edge index matrix $ \mathbf{E} \in \mathbb{R}^{2 \times m} $, where each column corresponds to an edge with source and target node indices. Let $ \mathbf{EP} \in \mathbb{R}^{1\times m} $ be the vector of edge weights corresponding to the edges in $ \mathbf{E} $. To include self-loops, we update the edge index matrix to $ \mathbf{E}' $ and the edge weight vector to $ \mathbf{EP}' $. Define the degree matrix $ \mathbf{D} \in \mathbb{R}^{n \times n} $ where $D_{ii} = \sum_{j} A_{ij},$ with $ A_{ij} $ is the adjacency matrix element corresponding to edge $ (i, j) $. The normalized adjacency matrix incorporating edge weights is computed as: $\mathbf{\tilde{A}} = \mathbf{D}^{-1/2} \mathbf{A} \mathbf{D}^{-1/2}.$ The output feature matrix at the GCNConv layer is given by: $ \mathbf{H} = \sigma\left( \mathbf{\tilde{A}} \mathbf{X} \mathbf{W} \right),$ where $ \sigma $ is an activation function. Alternatively, the node-wise update rule for node $ i $ can be expressed as:
\begin{equation}\label{eq:mp}
\mathbf{h}^{\prime}_i = \sigma \left( \mathbf{W}^{\top} \sum_{j \in
\mathcal{N}(i) \cup \{ i \}} \frac{e_{j,i}}{\sqrt{\hat{d}_j \hat{d}_i}} \mathbf{x}_j \right).
\end{equation}

\noindent{\textbf{\textit{Equivalence Analysis:}}}
Setting the $ k $-th edge weight $ e_k = 0 $ in $ \mathbf{EP}' $ removes the contribution of the corresponding edge $ (i, j) $ in the message-passing process (Equation \ref{eq:mp}). Since edge weights scale the aggregated messages, setting $ e_k = 0 $ nullifies the corresponding contribution.
Explicitly removing edge $ (i, j) $ from the adjacency matrix sets $ A_{ij} = 0 $, ensuring that node $ j $ no longer contributes to the feature update of node $ i $. To formalize the equivalence, let:
\begin{itemize}
    \item $ \mathbf{A}' $ be the adjacency matrix after removing edge $ (i, j) $.
    \item $ \mathbf{EP}'' $ be the edge weight vector where $ e_k = 0 $ for the corresponding edge.
    \item  $ \mathbf{\tilde{A}}' $ be the normalized adjacency matrix computed from $ \mathbf{A}' $.
    \item $ \mathbf{\tilde{A}}_{\text{ew}} $ be the normalized adjacency matrix computed using the edge weight vector $ \mathbf{e}'' $.
\end{itemize}

Since both methods eliminate the contribution of edge $ (i, j) $, we obtain $\mathbf{\tilde{A}}' = \mathbf{\tilde{A}}_{\text{ew}}$. Thus, the resulting feature updates remain identical:
\begin{equation*}
  \mathbf{H}' = \sigma ( \mathbf{\tilde{A}}' \mathbf{X} \mathbf{W} ) = \sigma (\mathbf{\tilde{A}}_{\text{ew}} \mathbf{X} \mathbf{W}).  
\end{equation*}


This confirms that setting an edge weight to zero is mathematically equivalent to removing the corresponding edge in the adjacency matrix.
\end{proof}

\section{Conclusion and Future Work}
\label{sec:conclusion}

In this work, we introduced COMBINEX, a unified counterfactual explainer for Graph Neural Networks (GNNs) that integrates both node feature and structural perturbations. Through extensive experiments across various datasets, tasks, and architectures, we demonstrated that COMBINEX effectively balances key evaluation metrics, ensuring high validity while minimizing modifications to the graph structure and node features to maintain realism.

We also proposed a novel edge weight sparsification technique, which significantly improves computational efficiency without compromising explainability. Our comparative analysis showed that COMBINEX operates more efficiently and with lower computational costs than existing methods following a similar approach, such as CF-GNNExplainer. Additionally, we explored different scheduling policies for balancing node and edge perturbations, further highlighting the flexibility and generalizability of COMBINEX across diverse scenarios.

In summary, COMBINEX represents a state-of-the-art counterfactual explanation framework for GNNs, offering a comprehensive and computationally efficient approach that aligns with real-world interpretability requirements. 

Future work will focus on extending our approach to broader graph-based tasks, including link prediction, as well as adapting the counterfactual framework to dynamic and heterogeneous graph structures.

% In summary, COMBINEX establishes a new benchmark for counterfactual explanations in GNNs, offering a comprehensive, efficient, and interpretable solution suitable for real-world applications. 
% Future work will focus on extending our approach to broader graph-based tasks, including link prediction, as well as adapting the counterfactual framework to dynamic and heterogeneous graph structures.

% In this work, we introduced COMBINEX, a unified counterfactual explainer for Graph Neural Networks (GNNs) that effectively combines both node feature and structural perturbations. Through extensive experimentation across multiple datasets, tasks, and graph architectures, we demonstrated that COMBINEX successfully balances key evaluation metrics, maintaining high validity while also delivering minimal and realistic modifications to the graph structure and node features.
%By ensuring a stable trade-off between fidelity, sparsity, and distribution distance, COMBINEX provides a robust and interpretable solution for counterfactual explanations in GNNs, making it well-suited for a variety of real-world applications.

% Additionally, we proposed a novel edge weight sparsification technique, which significantly enhances computational efficiency without sacrificing explainability. Our comparative analysis showed that COMBINEX operates more efficiently while using fewer computational resources if compared to techniques based on the same approach (CF-GNNExplainer). 
%This demonstrates that our sparsification approach not only reduces computational overhead but also enables practical applicability to larger graphs where traditional methods struggle with scalability.

% Beyond computational efficiency, our study also explored the impact of different scheduling policies for balancing node and edge perturbations. The flexibility of COMBINEX in accommodating various perturbation strategies further reinforces its generalizability across different datasets and tasks.

% In summary, COMBINEX represents a state-of-the-art counterfactual explanation framework for GNNs, offering a comprehensive and computationally efficient approach that aligns with real-world interpretability requirements. Future research directions include extending our method to broader graph-based tasks such as link prediction and extending the counterfactual framework to dynamic and heterogeneous graph structures. 
%Furthermore, exploring the integration of generative models to enhance the realism of counterfactual samples remains an exciting avenue for future work.




%Overall, the results demonstrate that COMBINEX Cons. is the most effective explainer, striking a balance between \textit{Validity}, \textit{Fidelity}, and \textit{Sparsity}. While some methods achieve better scores in individual metrics, none provide the same level of consistency across all datasets. The ability of COMBINEX to generate counterfactuals with minimal modifications while maintaining high fidelity and validity makes it the preferred choice for explainability in graph neural networks.








%\begin{figure}[htbp]

%     \centering
%    \includegraphics[width=0.98\linewidth]{imgs/Validity_and_Accuracy_vs_epochs.png}
%    \caption{The plot has on the y axis the validity and on the x axis there are the number of epochs on the Cora Dataset. The values has been obtained using a k-fold validation with k=6.}
%    \label{fig:validity_epochs}

%\end{figure}


%During the experiments, we noticed that the oracle's degree of training affects metrics such as Validity (Figure \ref{fig:validity_epochs}). In particular, in our proposed approach (), the higher the oracle's training accuracy, the higher the validity; namely, it seems that it gets easier to find counterfactual examples. We observed a similar trend for CF-GNNExplainer, although it is less evident. This phenomenon can be explained by how the decision boundary changes during training, as stated in Shamir et al.\cite{shamir2022dimpled}, allowing the counterfactual methods to find a counterfactual sample more easily. We belive that this phenomenon should be better investigated in the future.

%\section{Conclusion}

%We developed a novel counterfactual generation method for graph neural networks () that leverages optimization of node feature perturbations. To rigorously evaluate this approach, we implemented five baseline methods and tested the models on 11 real-world datasets. Our proposed method demonstrated superior validity and fidelity across nearly all datasets, highlighting the effectiveness of node feature perturbation in identifying counterfactual examples within graph neural networks. Additionally, the perturbation magnitude required to generate counterfactuals is minimal, ensuring that the resulting examples remain close to the original data points, thereby enhancing human interpretability

%For future research, we aim to explore the applicability of our technique in additional tasks, such as graph classification and edge prediction, to assess its broader effectiveness. Another promising avenue of investigation lies in examining the relationship between validity, fidelity, and the level of oracle training. We believe that a more rigorous analysis of this relationship could provide valuable insights into the underlying mechanisms of counterfactual generation, potentially leading to a deeper understanding of its behavior and improving its interpretability and performance across different scenarios.


%\section{Acknowledgments}



\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base.bib}
\

%%
%% If your work has an appendix, this is the place to put it.
\appendix

\section{Appendix}

\subsection{Edge Nullification Theorem for ChebConv}
\begin{theorem}\label{theorem:cheb}
In a Chebyshev Convolutional Network (ChebConv), setting an edge weight to zero is equivalent to removing the corresponding edge from the graph's adjacency matrix for $K=1$. However, for $K>1$, this equivalence does not necessarily hold.
\end{theorem}

\begin{proof}

The ChebConv layer applies a Chebyshev polynomial filter to the graph Laplacian. The output feature matrix $\mathbf{H}$ is computed as:
\begin{equation}
    \mathbf{H} = \sum_{k=0}^{K-1} \mathbf{T}_k(\tilde{\mathbf{L}}) \mathbf{X} \mathbf{\Theta}_k,
\end{equation}
where: $K$ is the Chebyshev filter size, $\mathbf{T}_k(\tilde{\mathbf{L}})$ is the Chebyshev polynomial of order $k$ evaluated at the scaled Laplacian $\tilde{\mathbf{L}}$, $\mathbf{X}$ is the input node feature matrix, $\mathbf{\Theta}_k$ is the learnable weight matrix for the $k$-th order. The scaled and normalized Laplacian $\tilde{\mathbf{L}}$ is defined as:
\begin{equation}
    \tilde{\mathbf{L}} = \frac{2\mathbf{L}}{\lambda_{\max}} - \mathbf{I},
\end{equation}
where $\mathbf{L} = \mathbf{D} - \mathbf{A}$ is the unnormalized Laplacian, $\mathbf{D}$ is the degree matrix, $\mathbf{A}$ is the adjacency matrix, and $\lambda_{\max}$ is the largest eigenvalue of $\mathbf{L}$. The Chebyshev polynomials are computed recursively as:
\begin{align}
    \mathbf{T}_0(\tilde{\mathbf{L}}) &= \mathbf{I}, \\
    \mathbf{T}_1(\tilde{\mathbf{L}}) &= \tilde{\mathbf{L}}, \\
    \mathbf{T}_k(\tilde{\mathbf{L}}) &= 2\tilde{\mathbf{L}} \mathbf{T}_{k-1}(\tilde{\mathbf{L}}) - \mathbf{T}_{k-2}(\tilde{\mathbf{L}}), \quad k \geq 2.
\end{align}

\textbf{Case $K=1$:}
For $K=1$, ChebConv simplifies to a first-order approximation similar to GCN. Setting an edge weight $e_{ij}$ to zero directly removes its contribution in the message passing, making it equivalent to removing the edge. It is important to notice that $e_{ij}$ is included within the adjacency matrix $\mathbf{A}$.

\textbf{Case $K>1$:}
For $K>1$, ChebConv introduces dependencies on multiple-hop neighbors due to higher-order polynomial terms. Even if an edge weight $e_{ij}$ is set to zero, information may still propagate through alternative paths in $\mathbf{T}_k(\tilde{\mathbf{L}})$. Specifically, for $K=2$:
\begin{equation}
    \mathbf{T}_2(\tilde{\mathbf{L}}) = 2\tilde{\mathbf{L}}^2 - \mathbf{I}.
\end{equation}
This squared term allows second-order neighbors to contribute, preventing a strict equivalence between weight nullification and edge removal.

Thus, for $K=1$, the equivalence holds, but for $K>1$, setting an edge weight to zero does not necessarily remove all contributions from that edge in ChebConv.
\end{proof}


\subsection{Edge Nullification Theorem for GraphConv}\label{theorem:graph}

\begin{theorem}
Let $ G = (V, E) $ be a graph with $ n $ nodes and $ m $ edges. Let $ \mathbf{X} \in \mathbb{R}^{n \times d} $ denote the node feature matrix, where each node has a feature vector of dimension $ d $. Consider a Graph Convolutional Network (GraphConv) layer parameterized by a weight matrix $ \mathbf{W} \in \mathbb{R}^{d \times d'} $. Setting an edge weight to zero in the GraphConv's edge weight vector is equivalent to removing the corresponding edge in the adjacency matrix representation.
\end{theorem}

\begin{proof}
The GraphConv layer follows the message-passing framework:
\begin{equation}
    \mathbf{H} = \sigma \left( (\mathbf{D}^{-1} \mathbf{A}) \mathbf{X} \mathbf{W} + \mathbf{X} \mathbf{W} \right),
\end{equation}
where:
- $ \mathbf{A} $ is the adjacency matrix (including self-loops).
- $ \mathbf{D} $ is the degree matrix with $ D_{ii} = \sum_{j} A_{ij} $.
- $ \sigma $ is an activation function.
- $ \mathbf{W} $ is the weight matrix.
- $ \mathbf{X} $ is the input node feature matrix.
- The term $ \mathbf{D}^{-1} \mathbf{A} $ represents row-normalized message aggregation.

Expanding the node-wise update, the representation for node $ i $ is:
\begin{equation}
    \mathbf{h}^{\prime}_i = \sigma \left( \mathbf{W}^{\top} \sum_{j \in \mathcal{N}(i) \cup \{ i \}} \frac{e_{j,i}}{d_j} \mathbf{x}_j + \mathbf{W}^{\top} \mathbf{x}_i \right).
\end{equation}

Setting the edge weight $ e_{i,j} = 0 $ removes the contribution of $ \mathbf{x}_j $ from the summation, effectively eliminating the influence of node $ j $ on node $ i $. On the other hand, removing edge $ (i, j) $ from the adjacency matrix $ \mathbf{A} $ sets $ A_{ij} = 0 $, which in turn removes $ x_j $ from the summation in the message-passing step. Since both approaches lead to the same feature update, the equivalence holds:
\begin{equation}
    \mathbf{H}' = \sigma \left( (\mathbf{D}'^{-1} \mathbf{A}') \mathbf{X} \mathbf{W} + \mathbf{X} \mathbf{W} \right) = \sigma \left( (\mathbf{D}^{-1} \mathbf{A}'') \mathbf{X} \mathbf{W} + \mathbf{X} \mathbf{W} \right).
\end{equation}

Thus, setting $ e_k = 0 $ in the edge weight vector produces the same feature transformation as removing the edge $ (i, j) $ from $ \mathbf{A} $, proving the claim.
\end{proof}

\subsection{Parameters}\label{subsec:params}


The experimental configurations for our models were set as follows. For the ChebConv model, we employed three hidden layers each with 64 units, along with a dropout rate of 0.5 to mitigate overfitting, and set the Chebyshev filter order \(K\) to 1. In contrast, the GCNConv model was configured with three hidden layers, each containing 128 units, and a dropout of 0.5. For the GraphConv model, the architecture comprised three hidden layers with 64 units each and also utilized a dropout rate of 0.5. 


\subsection{Functions}
Algorithm~\ref{alg:get_alpha} presents our \emph{Alpha Scheduling Function}, which determines the value of $\alpha$—a weighting parameter—during training based on the current epoch, loss values, and a specified scheduling policy. The function accepts as inputs the current epoch number, the edge loss $\mathcal{L}_E$, the node loss $\mathcal{L}_X$, a default value $\alpha_{default}$, the scheduling policy ($policy$), a decay rate $\delta$, and the maximum number of epochs $epochs_{max}$. Depending on the selected policy, the function computes $\alpha$ according to one of several strategies:

\begin{itemize}
    \item \textbf{Linear:} When $policy = linear$, $\alpha$ decreases linearly with the epoch number. This is computed as $\alpha = \max(0.0, 1.0 - \frac{epoch}{epochs_{max}})$, ensuring that $\alpha$ gradually decays from 1 to 0 over the course of training.
    \item \textbf{Exponential:} For $policy = exponential$, an exponential decay is applied: $\alpha = \max(0.0, e^{-epoch / \delta})$. The decay rate $\delta$ controls how fast $\alpha$ decays, allowing for rapid reduction at early epochs if desired.
    \item \textbf{Sinusoidal:} When $policy = sinusoidal$, the function uses a cosine-based schedule: $\alpha = \max(0.0, 0.5 \times (1 + \cos(\pi \times \frac{epoch}{epochs_{max}})))$. This policy produces a periodic decay that may help in scenarios where a smooth cyclic modulation of $\alpha$ is beneficial.
    \item \textbf{Dynamic:} If $policy = dynamic$, the scheduling is determined by comparing the edge loss and the node loss. Specifically, if $\mathcal{L}_E > \mathcal{L}_X$, then $\alpha$ is set to 0.0; otherwise, it is set to 1.0. This policy allows the training process to adaptively prioritize either edge or node information based on the relative magnitude of their losses.
    \item \textbf{Default:} In all other cases, the function returns a pre-specified default value $\alpha_{default}$.
\end{itemize}

This flexible scheduling mechanism is crucial for balancing different loss components during training, and its design allows for easy experimentation with various decay strategies. By incorporating both fixed (linear, exponential, sinusoidal) and adaptive (dynamic) policies, the function ensures that $\alpha$ can be tuned to optimize the trade-off between edge and node losses under different training conditions.


% get_alpha algorithm
\input{algorithms/get_alpha}


\subsection{Datasets}\label{sec:dataset}

\subsubsection{Planetoid}  
We use three citation network datasets: CiteSeer, Cora, and PubMed. CiteSeer contains 3,312 scientific publications across six classes, with a citation network of 4,732 links. Each document is represented by a binary word vector from a 3,703-word dictionary. Cora includes 2,708 publications classified into seven categories, with 5,429 citation links and binary word vectors from a 1,433-word dictionary. PubMed comprises 19,717 diabetes-related publications, categorized into three classes, with 44,338 citation links. Each document is represented using TF-IDF word vectors from a 500-word dictionary.
\subsubsection{WebKB} The WebKB datasets represent webpages collected from computer science departments of various universities. Our work uses three datasets: Cornell, Texas, and Wisconsin. Each dataset is a graph where nodes represent web pages, and edges are hyperlinks between them. Node features are the bag-of-words representation of web pages. The web pages are manually classified into the five categories, student, project, course, staff, and faculty.

\subsubsection{Attributed} The Attributed category contains three datasets: Wiki, and Facebook. The Wiki dataset comprises web pages, nodes, and edges representing hyperlinks between them. Node features represent several informative nouns on the Wikipedia pages. The Facebook dataset instead is a graph representing relations between users. In particular, the dataset contains profile and network data from 10 ego-networks, consisting of 193 circles and 4,039 users.

\subsubsection{Biological} The AIDS dataset contains 2,000 molecular graphs from the AIDS Antiviral Screen Database, used to study anti-HIV activity. The Enzymes dataset includes 600 protein structures classified into six enzyme classes, with nodes representing secondary structure elements and edges indicating their interactions. The Proteins dataset consists of 1,113 protein structures labeled as enzymes or non-enzymes, where nodes are amino acids, and edges connect those within 6 angstroms. Originally designed for graph classification, we adapted these datasets for node classification tasks.

\subsubsection{COIL-DEL} The COIL-DEL dataset contains 3,900 graphs, each representing a 2D image from COIL-100. Each graph corresponds to one of 100 objects, with 39 images per object captured from different angles. Nodes represent superpixels with 2D feature vectors, and edges denote spatial relationships. On average, graphs have 21.54 nodes and 54.24 edges. This dataset is used for graph-based machine learning in object recognition and image classification.

\subsubsection{Miscellaneous}
This category encompasses two different datasets: Karate and Actors. The Karate dataset contains 34 nodes connected by 156 (undirected and unweighted) edges. Every node is labeled by one of four classes obtained via modularity-based clustering.\\
In the Actor dataset, instead, each node corresponds to an actor, and the edge between two nodes denotes co-occurrence on the same Wikipedia page. Node features correspond to keywords in Wikipedia pages associated with the actors. The task is to classify the nodes into five categories.

\subsection{Graph Classification Results}\label{subsec:graphc}


The results are presented in Tables~\ref{tab:graph_cheb}, \ref{tab:graph_graph}, and \ref{tab:graph_gcn}.
COMBINEX consistently attains the best validity scores across all datasets and configurations, whereas baseline methods such as CF-GNNExplainer and EGO often exhibit significantly lower validity scores. 
While fidelity varies depending on the specific scheduling policy, COMBINEX remains competitive and, in many cases, outperforms traditional methods. The results suggest that COMBINEX generates explanations that remain faithful to the original model's decision boundary while introducing minimal but effective perturbations. One of the key advantages of COMBINEX is its ability to maintain a reasonable distribution distance. Unlike some baselines that introduce drastic changes leading to unrealistic counterfactuals, COMBINEX ensures that the generated explanations remain close to the original data distribution, enhancing their interpretability. 
Both node and edge sparsity are crucial for producing interpretable counterfactuals. COMBINEX effectively minimizes modifications, preserving the underlying graph structure while ensuring that only necessary changes are introduced. This makes it a more controlled and interpretable approach compared to methods that introduce excessive perturbations.
Traditional methods often exhibit trade-offs between different metrics, struggling to balance validity, fidelity, and sparsity simultaneously. EGO, for instance, sometimes achieves competitive distribution distances but at the cost of poor validity and high edge modifications. CF-GNNExplainer, on the other hand, frequently underperforms in validity and fidelity, limiting its reliability. Random perturbation-based approaches lead to large distribution shifts, making the generated counterfactuals less meaningful.





\input{tables/models/ChebConv/graph_classification}

\input{tables/models/GraphConv/graph_classification}

\input{tables/models/GCN/graph_classification}

\subsection{Node Classification Results}\label{subsec:nodec}


\subsubsection{Miscellaneous datasets}

The results reported in Tables~\ref{tab:misc_chebconv}, \ref{tab:misc_gcn_conv}, and \ref{tab:misc_graph_conv} demonstrate the effectiveness of our COMBINEX approach across different oracles (ChebConv, GCNConv, and GraphConv) on two distinct datasets: Karate and Actor. In Table~\ref{tab:misc_chebconv}, which employs ChebConv layers, the COMBINEX variants consistently achieve near-perfect Validity on the Karate dataset, indicating that the generated explanations capture the essential substructures of this relatively small and well-defined network. Furthermore, these variants yield competitive Fidelity values and exhibit a notably low Distribution Distance, while maintaining extremely sparse explanations, particularly in terms of edge sparsity.

When considering the results from Table~\ref{tab:misc_gcn_conv} using GCNConv layers, a similar trend is observed on the Karate dataset, with COMBINEX variants again showing high Validity and low sparsity. However, the slight differences in Fidelity and Distribution Distance between the ChebConv and GCNConv settings illustrate that the specific convolutional layer influences how the oracle’s decisions are reflected in the explanations. 

The Actor dataset, on the other hand, represents a more complex and noisy social network, where the oracle accuracy is lower (0.65) compared to Karate. Despite this increased complexity, COMBINEX still maintains robust performance: the explanations continue to achieve high Validity and relatively low Distribution Distance, although Fidelity and sparsity metrics tend to be less optimal than those observed on the Karate dataset. This variation likely reflects the inherent differences in network structure and the level of noise between the two datasets. In the Actor dataset, the explanations must account for more diverse and overlapping communities, which can challenge the generation of both highly faithful and extremely sparse explanations.

Overall, the empirical findings across these datasets and oracles highlight several advantages of our COMBINEX approach. Notably, COMBINEX consistently produces highly valid explanations that accurately capture the underlying graph structures, achieves competitive fidelity while closely matching the distribution of the oracle outputs, and delivers concise explanations through enhanced sparsity. These strengths are evident across different convolutional settings—whether using ChebConv, GCNConv, or GraphConv layers—thus confirming the versatility and robustness of COMBINEX even when applied to both simple (Karate) and complex (Actor) network datasets.


% Miscellaneous GCN
\input{tables/models/GCN/miscellaneous}

% Miscellaneous GraphConv
\input{tables/models/GraphConv/miscellaneous}

% Miscellaneous ChebConv
\input{tables/models/ChebConv/miscellaneous}

\subsubsection{Planetoid datasets}

Tables~\ref{tab:planetoid_chebconv}, \ref{tab:planetoid_gcn}, and \ref{tab:planetoid_graphconv} collectively report the performance of various explainers on the Planetoid datasets— PubMed, Cora, and Citeseer—under three different oracle settings: ChebConv, GCNConv, and GraphConv, respectively. A careful examination of these results reveals important insights into the behavior of our COMBINEX approach compared to other methods, as well as differences across datasets and convolutional operators.

Starting with the ChebConv-based results in Table~\ref{tab:planetoid_chebconv}, our COMBINEX variants (Feat., Cons., Dyn., Exp., Lin., and Sin.) consistently achieve strong performance. On PubMed, the COMBINEX variants attain moderate validity and fidelity with an impressively low distribution distance and node sparsity near 0.106. Although the Random Features baseline shows very high validityand fidelity, its distribution distance is significantly higher, indicating that while it captures certain aspects of the oracle’s behavior, its explanations tend to be overly dense and less faithful in distribution. On Cora, the clear structure of the dataset is exploited by COMBINEX Feat., which achieves nearly perfect validity and extremely low node sparsity, alongside competitive fidelity (0.771) and a relatively low distribution distance (with a second-best value of 0.744). Citeseer, being more complex and noisy, naturally yields higher distribution distances; yet, COMBINEX variants still deliver perfect validity and strong fidelity, with COMBINEX Cons. notably achieving nearly zero edge sparsity.

Turning to the GCNConv results in Table~\ref{tab:planetoid_gcn}, we observe that the overall trends are consistent with those seen under ChebConv. On PubMed, COMBINEX variants again strike a favorable balance, with validity and fidelity scores around 0.66–0.76 and very low distribution distances. In this setting, while the Random Features method achieves similarly high validity, its elevated distribution distance persists as a drawback. On Cora and Citeseer, the COMBINEX methods continue to outperform or match baselines: for example, on Cora, COMBINEX Feat. reaches perfect validity with negligible node sparsity, and on Citeseer, variants such as COMBINEX Cons. and COMBINEX Dyn. deliver flawless validity and very low edge sparsity, even though the distribution distances remain higher due to the dataset’s intrinsic complexity.

Finally, the GraphConv results shown in Table~\ref{tab:planetoid_graphconv} further confirm the robustness of our COMBINEX approach. With GraphConv oracles, COMBINEX variants maintain high validity (often close to or at 1.000) and competitive fidelity, accompanied by low distribution distances and sparse explanations. On PubMed, COMBINEX Feat. achieves a validity of approximately 0.910 with a distribution distance of only 0.103, while on Cora and Citeseer, the COMBINEX methods continue to outperform alternative explainers such as CF-GNNExplainer and CFF. Notably, the consistent performance across GraphConv, as with the ChebConv and GCNConv settings, highlights the versatility of COMBINEX in adapting to different convolutional operators.

Across all three tables, a clear picture emerges COMBINEX not only delivers high validity and fidelity but does so while keeping distribution distances low and explanations sparse. Moreover, the differences across datasets are also instructive. PubMed, with its more heterogeneous structure, leads to somewhat lower validity and higher distribution distances compared to the more structured Cora, whereas Citeseer’s inherent complexity results in increased distribution distances even as validity remains perfect.

In summary, whether using ChebConv, GCNConv, or GraphConv as the oracle, our COMBINEX approach consistently provides balanced, interpretable, and faithful explanations. The approach is robust across different datasets and convolutional models, achieving high validity and fidelity while minimizing both distribution distance and sparsity—qualities that are essential for effective explanation in graph neural networks.


% Planetoid GCN
\input{tables/models/GCN/planetoid}

% Planetoid GraphConv
\input{tables/models/GraphConv/planetoid}

% Planetoid ChebConv
\input{tables/models/ChebConv/planetoid}


\subsubsection{WebKb datasets}

The results are presented in Tables \ref{tab:webkb_gcn}, \ref{tab:webkb_graphconv}, and \ref{tab:webkb_cheb}.

The experimental results on the WebKB datasets—Texas, Cornell, and Wisconsin—demonstrate the effectiveness of COMBINEX in generating counterfactual explanations while maintaining high validity across different settings.
For the Wisconsin dataset, almost all variants of COMBINEX achieve perfect validity, ensuring that the generated counterfactuals adhere to the oracle’s classification boundaries. Among them, the feature-only variant (COMBINEX$_{\textit{feat}}$) achieves the lowest node sparsity, suggesting that perturbing only node features results in minimal changes while still preserving explainability. However, fidelity remains relatively low across all COMBINEX variants, indicating that further optimization may be needed to ensure better alignment with the model's decision boundary. The exponential scheduling policy (COMBINEX$_{\textit{exp}}$) introduces the largest distribution distance, highlighting that more aggressive perturbations lead to greater deviation from the original data distribution. Notably, results vary slightly across models, with GraphConv achieving higher fidelity than GCN, while ChebConv provides a more stable trade-off between sparsity and fidelity.
In the Texas dataset, COMBINEX continues to exhibit strong validity across all configurations. The feature-only variant again achieves the lowest node sparsity, reinforcing its ability to produce concise explanations with minimal modifications. However, compared to other datasets, fidelity values are lower, suggesting that the graph structure may play a significant role in the interpretability of counterfactual explanations. Notably, the exponential scheduling policy results in a sharp increase in distribution distance, emphasizing that a more aggressive decay in the perturbation parameter leads to excessive divergence from the original data. Differences between models indicate that GraphConv achieves the best overall fidelity, while GCN and ChebConv yield similar results in terms of validity but diverge in sparsity control.

For the Cornell dataset, COMBINEX maintains its perfect validity across all configurations, confirming its robustness in different graph structures. The feature-only and default scheduling policies achieve the best balance between fidelity and distribution distance, ensuring both faithful explanations and reasonable proximity to the original data. The dynamic, linear, and sinusoidal policies introduce slightly higher perturbations, resulting in larger distribution distances and node sparsity values. As in previous datasets, the exponential scheduling policy significantly increases distribution distance, further underscoring the importance of selecting an appropriate scheduling strategy to balance counterfactual realism and interpretability. The model-specific results reveal that ChebConv provides the most stable performance across all COMBINEX configurations, while GCN exhibits greater variance in fidelity scores.

Across all three datasets, COMBINEX demonstrates consistent validity and competitive fidelity while maintaining low node sparsity in its feature-only and default configurations. The choice of scheduling policy has a noticeable impact on the trade-off between fidelity, sparsity, and distribution distance, highlighting the need for dataset-specific tuning. Furthermore, the results indicate that different graph neural network architectures influence explainability outcomes, with GraphConv generally achieving better fidelity, ChebConv offering a balanced approach, and GCN showing greater variability across datasets. These results confirm that COMBINEX is a reliable counterfactual explainer capable of adapting to different graph structures and model architectures while maintaining interpretability and computational efficiency.


% Webkb GCN
\input{tables/models/GCN/webkb}

% Webkb GraphConv
\input{tables/models/GraphConv/webkb}

% Webkb ChebConv
\input{tables/models/ChebConv/webkb}

%Usage: \mc{number of columns spanned}{major column heading}


\subsubsection{Attributed datasets}

In this section we comment on the results obtained on the Attributed datasets (Wiki and Facebook) using different alpha scheduling policies within our COMBINEX framework. Tables~\ref{tab:attributed_graph_conv}, \ref{tab:attributed_gcn}, and \ref{tab:attributed_cheb} show that COMBINEX consistently outperforms traditional baselines by achieving high validity and fidelity while maintaining low sparsity, although the trade-off with distribution distance varies depending on the specific alpha policy. 

On the Wiki dataset, for instance, when using GraphConv layers (Table~\ref{tab:attributed_graph_conv}), the COMBINEX Feat. variant attains a high validity and moderate fidelity, coupled with very low node sparsity; however, the distribution distance is considerably high, indicating that while the explanations are faithful in terms of structure, the overall activation distribution deviates substantially from that of the oracle. In contrast, COMBINEX Cons. and COMBINEX Dyn. slightly reduce the validity and fidelity (to around 0.130–0.162 and 0.535–0.569, respectively) but incur even higher distribution distances or only marginal improvements in sparsity. Notably, the Exp policy (COMBINEX Exp.) leads to the highest distribution distance, suggesting that an overly aggressive exponential decay may deteriorate the overall quality of the explanation. Similar trends are observed in the results obtained with GCN and ChebConv oracles, where the Feat. variant typically yields the best balance between validity, fidelity, and sparsity.

On the Facebook dataset, the performance of COMBINEX improves markedly. Under the GraphConv setting, COMBINEX Feat. achieves a validity of 0.690 and fidelity of 0.762, with a very low node sparsity and a moderate distribution distance. The other alpha policies (Cons., Dyn., Exp., Lin., and Sin.) yield slightly lower validity and fidelity, with differences in distribution distance and sparsity that are less pronounced compared to Wiki. In particular, the Cons. variant on Facebook shows a good trade-off with a distribution distance of 4.070 and slightly higher node and edge sparsity, while the Dyn., Exp., Lin., and Sin. variants achieve similar results with only minor differences. Baselines such as EGO and Random Edges consistently perform poorly on both datasets, and Random Features and UNR are either not able to find counterfactuals or yield extreme values, confirming that our COMBINEX approach is superior in producing balanced and interpretable explanations.

Overall, these results confirm that our COMBINEX solution, with appropriate alpha scheduling (particularly the Feat. and Cons. variants), consistently delivers high-quality explanations across attributed datasets, regardless of the convolutional operator used. The experiments illustrate that while the choice of alpha policy influences the trade-off between fidelity, distribution distance, and sparsity, COMBINEX remains robust and effective in both Wiki and Facebook scenarios.

% Attributed GCN
\input{tables/models/GCN/attributed}

% Attributed GraphConv
\input{tables/models/GraphConv/attributed}

% Attributed ChebConv
\input{tables/models/ChebConv/attributed}


% Time and Memory
%\input{tables/time_memory/time}

\subsubsection{Biological datasets}
Below a comment on Tables \ref{tab:bio_graph_conv}, \ref{tab:bio_gcn_conv}, \ref{tab:bio_cheb_conv}.
For the AIDS dataset, COMBINEX variants consistently achieved the highest validity scores, while also achieving one of the lowest edge sparsity values.
Notably, COMBINEX$_{\textit{def}}$ and COMBINEX$_{\textit{lin}}$ exhibited a better balance between validity and sparsity, with \\
COMBINEX$_{\textit{lin}}$ producing the most compact counterfactual explanations. This suggests that a structured, linear decay of perturbations maintains a more stable trade-off in preserving graph integrity.
In comparison, other explainers such as EGO, Random Features, and CF-GNNExplainer performed significantly worse, struggling with either validity, fidelity, or sparsity. The lowest distribution distance was achieved by UNR (4.134), but at the cost of much lower validity (0.237), reinforcing that COMBINEX consistently finds counterfactuals that are both valid and meaningful.

The performance of COMBINEX on the Enzymes dataset follows a similar trend, where different scheduling strategies lead to variations in performance. The COMBINEX$_{\textit{exp}}$ approach achieved the best validity while maintaining one of the highest fidelities. However, this came at the cost of a high distribution distance, indicating that these perturbations were more aggressive.Interestingly, COMBINEX$_{\textit{lin}}$ and COMBINEX$_{\textit{sin}}$ continued to exhibit balanced trade-offs, achieving sparse counterfactuals with low edge sparsity values while keeping validity relatively high. This suggests that more structured perturbation schedules (linear and sinusoidal) prevent unnecessary modifications while maintaining valid counterfactuals. When comparing to other explainers, EGO once again struggled, and Random Features produced high validity but at the cost of an extremely high distribution distance, meaning the counterfactuals were highly unrealistic. UNR achieved the lowest distribution distance but suffered from poor validity.

The results on the Proteins dataset highlight a notable performance gap between different variants of COMBINEX. COMBINEX$_{\textit{exp}}$ achieved the best validity and highest fidelity. However, its distribution distance was significantly higher, implying that the changes introduced were more substantial. The structured perturbation strategies, COMBINEX$_{\textit{lin}}$ and COMBINEX$_{\textit{sin}}$, also produced highly valid counterfactuals while maintaining low edge sparsity. COMBINEX$_{\textit{lin}}$, in particular, showed the lowest node sparsity while keeping validity high, suggesting that linear perturbation schedules can effectively preserve the original graph structure. Other explainers, such as EGO and CFF, struggled significantly, with validity scores below. The CF-GNNExplainer performed particularly poorly, failing to generate meaningful counterfactuals in many cases.


\input{tables/models/GCN/biological}

\input{tables/models/GraphConv/biological}

\input{tables/models/ChebConv/biological}


\end{document}
