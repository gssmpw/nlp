\section{Related Work}
\subsection{Knowledge Distillation}
Knowledge distillation is a technique in which a student model is usually trained by mimicking the outputs of a more powerful teacher model.
However, in a semi-supervised learning context, knowledge distillation can also be used to train same-size or larger student models that outperform their teachers by pseudo-labeling large amounts of unlabeled data \cite{bib:billion-scale, bib:naive-student, bib:noisy-student, bib:mean-teacher, bib:pp-liteseg, bib:meta-pseudo-labels, bib:data-distillation}.
Some studies even show that pre-training a model on its own pseudo-labels can produce better models than those pre-trained on another hand-labeled dataset \cite{bib:rethinking-pretraining, bib:depth-anything}.

In Naive Student \cite{bib:naive-student}, for example, the authors use a Cityscapes-trained model to pseudo-label abundant unlabeled images of the same dataset with semantic, instance, and panoptic segmentation labels.
The newly initiated student model then trains on all data (pseudo and ground truth).
The entire process is iterated through multiple times, whereby for each iteration, the student model becomes the teacher model for the next iteration.
In a similar work, Noisy Student \cite{bib:noisy-student}, a classification model, is used to generate soft pseudo-labels for 300M images.
During the subsequent student training, noise is injected into the model, improving generalizability and overall classification accuracy.

The above works focus on utilizing large amounts of unlabeled data.
Another approach is to use weakly annotated data, e.g. in the form of object bounding boxes \cite{bib:weakly-and-semi-supervised-learning, bib:boosting-ssss, bib:bbam}, rough segmentation maps \cite{bib:urban-scene-coarse-annotation} or image-level annotations \cite{bib:weakly-and-semi-supervised-learning, bib:boosting-ssss, bib:image-level-weak-supervision}.
Our work fits somewhere in between these two paradigms, since it uses pseudo-labels that were enhanced by dataset priors (also a form of weak supervision) but is otherwise fully compatible with unlabeled data.

\subsection{Label Space Unification}

Label Space Unification for semantic segmentation is typically approached from three angles: at the model architecture level \cite{bib:automated-label-unification, bib:multi-head-semseg, bib:cross-dataset-learning, bib:universal-ssss, bib:heterogeneous-street-datasets, bib:multi-task-domain-learning}, the loss level \cite{bib:universal-image-concepts, bib:partial-label-losses, bib:multi-domain-semseg, bib:heterogeneous-street-datasets, bib:multi-task-domain-learning}, and the data level \cite{bib:mseg, bib:unifying-off-road}.

Modifying the architecture to accommodate different label spaces typically involves adding prediction heads for each taxonomy \cite{bib:heterogeneous-street-datasets, bib:multi-head-semseg, bib:universal-ssss, bib:multi-task-domain-learning}, or extra layers that learn the taxonomy between the label spaces \cite{bib:automated-label-unification}.
However, besides increasing the overall model size and complexity, elaborate model architectures have limited practical use and can be incompatible with existing architectures.

Moreover, past works have proposed methods whereby only prediction errors within the dataset-specific label space are penalized by the loss function, leaving the model architecture largely unchanged:
Bevandi\'{c} et al. \cite{bib:universal-image-concepts, bib:multi-domain-semseg} introduced a variation of the negative log-likelihood loss that is designed to handle partial labels, i.e., labels that represent a set of possible classes in a hierarchical taxonomy.
They do this by aggregating the probabilities of universal classes corresponding to each dataset-specific label and computing the log-sum over these probabilities.
Similarly, in \cite{bib:heterogeneous-datasets} the softmax output is re-mapped from the fine-granular universal label space to a dataset-specific one before applying the standard cross-entropy loss.
Fourure et al. \cite{bib:multi-task-domain-learning} avoid the the label space unification step by letting the model predict over all classes of all datasets within the softmax layer.
Via their proposed "selective cross-entropy" loss function, only logits belonging to the dataset of the input image have a non-zero gradient in the backpropagation step.
However, such methods of naive label space concatenation bring with them significant overhead in train and prediction time since the dimension of the softmax output grows with each class.
Modifying only the loss function has the advantage of reduced data preparation.
However, this approach often requires heavy modifications of the training pipeline and not all models may be compatible with the proposed loss function.

Finally, a straightforward but time-consuming approach to dataset unification is via manipulating the data itself.
MSeg \cite{bib:mseg} unifies 7 datasets under a fine-granular taxonomy by manual labeling.
In a similar work \cite{bib:unifying-off-road}, the authors unify several off-road datasets by using an ontology-based framework that maps differing label schemes to a shared taxonomy. 
Both approaches lack an automated labeling scheme, thus necessitating manual re-labeling to a source taxonomy or moving classes to the highest level hierarchy that is shared across all datasets, resulting in a loss of granularity.

Our approach, due to its simplicity, does not require any modification to architecture or loss function and can be easily built on top of existing training pipelines.