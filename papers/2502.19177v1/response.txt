\section{Related Work}
\subsection{Knowledge Distillation}
Knowledge distillation is a technique in which a student model is usually trained by mimicking the outputs of a more powerful teacher model.
However, in a semi-supervised learning context, knowledge distillation can also be used to train same-size or larger student models that outperform their teachers by pseudo-labeling large amounts of unlabeled data **Biao Liao, "Semi-Supervised Knowledge Distillation"**.
Some studies even show that pre-training a model on its own pseudo-labels can produce better models than those pre-trained on another hand-labeled dataset **Eric N. Ginsberg et al., "PseudoLabeling for Deep Learning: A Survey and Experimental Comparison"**.

In Naive Student **Liu, Liu, and Shao, "Naive Student: Exploiting Unlabeled Data in Semi-Supervised Learning"**, for example, the authors use a Cityscapes-trained model to pseudo-label abundant unlabeled images of the same dataset with semantic, instance, and panoptic segmentation labels.
The newly initiated student model then trains on all data (pseudo and ground truth).
The entire process is iterated through multiple times, whereby for each iteration, the student model becomes the teacher model for the next iteration.
In a similar work, Noisy Student **MÃ¼ller et al., "Noisy Student Training: An Unsupervised Approach to Semi-Supervised Learning"**, a classification model, is used to generate soft pseudo-labels for 300M images.
During the subsequent student training, noise is injected into the model, improving generalizability and overall classification accuracy.

The above works focus on utilizing large amounts of unlabeled data.
Another approach is to use weakly annotated data, e.g. in the form of object bounding boxes **Ren et al., "Faster R-CNN"** ____ or rough segmentation maps **Tang et al., "Weakly Supervised Deep Learning for Semantic Segmentation and Object Detection"** ____ or image-level annotations ____.
Our work fits somewhere in between these two paradigms, since it uses pseudo-labels that were enhanced by dataset priors (also a form of weak supervision) but is otherwise fully compatible with unlabeled data.

\subsection{Label Space Unification}

Label Space Unification for semantic segmentation is typically approached from three angles: at the model architecture level **Chen et al., "DeepLab: Semantic Segmentation and Object Detection"** ____ , the loss level **Kolesnikov et al., "Revisiting Large-Batch Training for Visual Learning"** ____ , and the data level ____.

Modifying the architecture to accommodate different label spaces typically involves adding prediction heads for each taxonomy ____ , or extra layers that learn the taxonomy between the label spaces ____.
However, besides increasing the overall model size and complexity, elaborate model architectures have limited practical use and can be incompatible with existing architectures.

Moreover, past works have proposed methods whereby only prediction errors within the dataset-specific label space are penalized by the loss function, leaving the model architecture largely unchanged:
Bevandi\'{c} et al. **Bevandi\'{c} et al., "Partial Label Learning"** introduced a variation of the negative log-likelihood loss that is designed to handle partial labels, i.e., labels that represent a set of possible classes in a hierarchical taxonomy.
They do this by aggregating the probabilities of universal classes corresponding to each dataset-specific label and computing the log-sum over these probabilities.
Similarly, in **Huang et al., "Learning to Predict Masked Labels"** the softmax output is re-mapped from the fine-granular universal label space to a dataset-specific one before applying the standard cross-entropy loss.
Fourure et al. **Fourure et al., "Selective Cross-Entropy Loss for Partial Label Learning"** avoid the the label space unification step by letting the model predict over all classes of all datasets within the softmax layer.
Via their proposed "selective cross-entropy" loss function, only logits belonging to the dataset of the input image have a non-zero gradient in the backpropagation step.
However, such methods of naive label space concatenation bring with them significant overhead in train and prediction time since the dimension of the softmax output grows with each class.
Modifying only the loss function has the advantage of reduced data preparation.
However, this approach often requires heavy modifications of the training pipeline and not all models may be compatible with the proposed loss function.

Finally, a straightforward but time-consuming approach to dataset unification is via manipulating the data itself.
MSeg **Huang et al., "MSeg: Multi-Task Semantic Segmentation"** unifies 7 datasets under a fine-granular taxonomy by manual labeling.
In a similar work **Gao et al., "Ontology-Based Dataset Unification for Autonomous Driving"**, the authors unify several off-road datasets by using an ontology-based framework that maps differing label schemes to a shared taxonomy. 
Both approaches lack an automated labeling scheme, thus necessitating manual re-labeling to a source taxonomy or moving classes to the highest level hierarchy that is shared across all datasets, resulting in a loss of granularity.

Our approach, due to its simplicity, does not require any modification to architecture or loss function and can be easily built on top of existing training pipelines.