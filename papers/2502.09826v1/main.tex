%===============================================================================
% $Id: ifacconf.tex 19 2011-10-27 09:32:13Z jpuente $
% Template for IFAC meeting papers
% Copyright (c) 2007-2008 International Federation of Automatic Control
%===============================================================================
\documentclass{ifacconf}
% \usepackage{setspace}
% \doublespacing

\usepackage{graphicx}      % include this line if your document contains figures
\usepackage[square,numbers]{natbib} % For numerical citations
\usepackage{tabularx}
\usepackage{tikz}
\usetikzlibrary{positioning, arrows, patterns} % Add any other libraries you need
\usepackage{pgfplots}
\pgfplotsset{compat=1.18} % Set the version for compatibility, adjust if needed
\usepackage{float}

%===============================================================================

\usepackage{amsmath}
%\usepackage[table]{xcolor}% http://ctan.org/pkg/xcolor
%\usepackage{multirow}
\usepackage{amsfonts,bm} % Math packages
\usepackage{framed} % Framing content 
\usepackage{nomencl}
\usepackage{float}
\usepackage{optidef}
\usepackage{mathtools}
%\usepackage{url}
\usepackage[hyphens]{url}
%\usepackage{hyperref}
\DeclareMathSymbol{\sm}{\mathbin}{AMSa}{"39} %short minus
% \usepackage{newclude}
% \makenomenclature
\newcommand{\ind}[1]{{_{\mathrm{#1}}}} %  Alex
\newcommand{\vect}[1]{\ensuremath{\boldsymbol{\mathrm{#1}}}} % Alex
\DeclarePairedDelimiter\abs{\lvert}{\rvert}%Alex
\DeclarePairedDelimiter\norm{\lVert}{\rVert}%Alex
% Add these spacing controls to your preamble
\setlength{\textfloatsep}{10pt}    % Space between floats and text
\setlength{\floatsep}{10pt}        % Space between consecutive floats
\setlength{\intextsep}{10pt}       % Space between in-text floats and text
\usepackage{tabularx}
\usepackage{array}

\pagestyle{plain}
%--------------------------------
\newcommand{\nox}{$\mathrm{NO_{x}}$}
\newcommand{\cotw}{$\mathrm{CO_{2}}$}


\nomenclature{ML}{Machine Learning}
\nomenclature{MPC}{Model Predictive Control}
\nomenclature{LSTM}{Long-short term memory}
\nomenclature{ICE}{Internal Combustion Engine}
\nomenclature{PPO}{Proximal Policy Optimisation}
\nomenclature{TD3}{Twin Delayed Deep Deterministic Policy Gradients}
\nomenclature{NMPC}{Nonlinear Model Predictive Control}
\nomenclature{LMPC}{Linear Model Predictive Control}
\nomenclature{ARX}{Autoregressive with Extra Input}
\nomenclature{SOI}{Start Of Injection}
\nomenclature{FQ}{Fuel Quantity}
\nomenclature{DPM}{Detailed Physical Model}
\nomenclature{SVM}{Support Vector Machine}
\nomenclature{NOx}{Nitrogen Oxide}
\nomenclature{RNN}{Recurrent Neural Network}
\nomenclature{VGT}{Variable-Geometry Turbocharger }
\nomenclature{OCP}{Optimal Control Problem}
\nomenclature{SQP}{Sequential Quadratic Programming}
\nomenclature{ESM}{Engine Simulation Model}

% \newacronym{PPO}{PPO}{Proximal Policy Optimization}
% \makenomenclature

\sloppy
\begin{document}
\begin{frontmatter}

\title{Safe Reinforcement Learning-based Control for  Hydrogen Diesel Dual-Fuel Engines}
% Title, preferably not more than 10 words.
%{Safe Model-based offline Learning based Control for  Hydrogen Diesel Dual-Fuel Engines}

\thanks{Corresponding Author: Vasu Sharma -(e-mail: sharma\_v@mmp.rwth-aachen.de).}


% \author[First]{Armin Norouzi}
% \author[First]{Saeid Shahpouri}
% \author[First]{Mahdi Shahbakhti}
% \author[First]{Charles Robert Koch}
% \address[First]{Mechanical Engineering Department,~University~of~Alberta, Edmonton, Canada}


\author[2]{Vasu Sharma} %113371
\author[2]{Alexander Winkler} %137059
\author[1]{Armin Norouzi} %135760
\author[2]{Jakob Andert}
\author[1]{David Gordon} %129281
\author[3]{Hongsheng Guo}
\address[1]{Department of Mechanical Engineering, University of Alberta, Edmonton, Canada}
\address[2]{Teaching and Research Area Mechatronics in Mobile Propulsion, RWTH Aachen University, Aachen, Germany}
\address[3]{National Research Council, Canada}

This work has been submitted to IFAC for possible publication
\begin{abstract}
The urgent energy transition requirements towards a sustainable future stretch across various industries and are a significant challenge facing humanity. Hydrogen promises a clean, carbon-free future, with the opportunity to integrate with existing solutions in the transportation sector. However, adding hydrogen to existing technologies such as diesel engines requires additional modeling effort. Reinforcement Learning (RL) enables interactive data-driven learning that eliminates the need for mathematical modeling. The algorithms, however, may not be real-time capable and need large amounts of data to work in practice. This paper presents a novel approach which uses offline model learning with RL to demonstrate safe control of a 4.5$\,$L Hydrogen Diesel Dual-Fuel (H2DF) engine. The controllers are demonstrated to be constraint compliant and can leverage a novel state-augmentation approach for sample-efficient learning. The offline policy is subsequently experimentally validated on the real engine where the control algorithm is executed on a Raspberry Pi controller and requires 6 times less computation time compared to online Model Predictive Control (MPC) optimization.
\end{abstract}

\begin{keyword}
Hydrogen Diesel Engines, Reinforcement Learning, Machine Learning, Proximal Policy Optimization, Twin Delayed Deep Deterministic Policy Gradients, Deep Learning
\end{keyword}
\end{frontmatter}
%===============================================================================



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
In the coming decades, global warming could result in a mean global temperature rise of 1.5$^\circ\text{C}$ from preindustrial levels, which would not only have a seismic impact on our way of life, but also jeopardize sensitive ecosystems worldwide~\citep{IntergovernmentalPanelonClimateChange.2022}. Despite major advances, the transportation sector continues to be a significant contributor towards green house gas emissions, with estimated contributions from 16 to 23 percent ~\citep{Crippa.2023}. Within the transportation sector, the heavy-freight industry disproportionately contributes to roughly 35 percent of the total carbon dioxide~(\cotw) emissions, despite having a total vehicle footprint of less than 8 percent through buses and trucks \citep{TrackingCleanEnergyProgress2023IEA.2023}.

Owing to their high thermal efficiency and low maintenance, diesel engines have been a cornerstone of the heavy-duty transportation sector. However, due to ever stringent emission regulations worldwide, a cleaner alternative has been sought by the sector. Hydrogen promises a carbon-free alternative which can be used
in conjunction with electrification to offer a smoother energy transition for the
transportation sector. When combined with conventional diesel engines, hydrogen replaces diesel from the engine in the combustion energy share, thus promoting lower particulate and~\cotw~emissions. However, complex combustion characteristics can limit the hydrogen fraction and cause abnormal combustion at higher loads, and therefore require complex modeling and calibration. Additionally, higher operating temperatures can lead to increased nitrogen oxide (NOx) formation \citep{Tsujimura.2017}. 

  
With the prospect to synthesize controllers directly from data, Reinforcement Learning (RL) offers an alternative approach. In the last few years, RL has been used to control the variable geometry turbocharger (VGT) vane position along with the injection timing of a diesel engine, to maximize the engine break torque \citep{Malikopoulos.2009}, transient boost pressure tracking by controlling the vane position of a VGT-equipped diesel engine \citep{Hu.2019} and fuel consumption minimization among other use cases \citep{Egan.2023}. Virtualized development platforms have also been shown to be effective in saving online training efforts \citep{Koch.2023b, Koch.2023, Badalian.2024}.


However, results from the current literature utilize high-fidelity simulation training models, which may not always be available or, when available, are not real-time capable. This also leads to challenges when bringing the learned policies in the simulated environment to the real world \citep{Li.2024}. 
Moreover, despite the success in simulated environments, only a few multi-objective real-world use cases can be found in the literature \citep{Norouzi.2023}. The control actions are also often low-dimensional and build on existing engine applications instead of controlling the entire combustion process itself.


This paper aims to tackle the aforementioned challenges by combining state-of-the-art deep learning models to enable sample-efficient reinforcement learning for a hydrogen diesel engine. The methodology is demonstrated to work for both on-policy and off-policy agents which are trained in a simulated environment. The trained policies are then shown to control the actual process. To the best of the authors' knowledge, this paper presents the first study to use this technique for real-time control of a hydrogen diesel engine.

Based on the current literature, this paper has the following contributions:

\begin{enumerate}
    \item Deep learning-based offline model learning for RL design
        \begin{enumerate}
    \item A Gated Recurrent Unit~(GRU)-based encoder-decoder model is developed to capture and represent the engine dynamics and emissions 

        \end{enumerate}
    \item Offline reinforcement learning based control 
        \begin{enumerate}
    \item RL agents are trained on the learned model to minimize engine-out emissions and fuel consumption while maintaining the same output load tracking performance.
    \item Developed model-based offline-RL controller is demonstrated to reduce the computational time of optimization
    \end{enumerate}
\end{enumerate}

   
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Methodology}

In this paper, a four-step methodology combining deep neural networks (DNN) and RL to control a hydrogen diesel engine is presented. An overview of the methodology is depicted in Fig.~\ref{fig:offlinemodeling}. Step 1 focuses on system identification using random actuations applied to the engine, and recording the outputs. Then, a GRU-based encoder-decoder style model is learned to represent the plant dynamics through deep learning. This is subsequently used in conjunction with a reward function and state augmentations to learn load tracking and constraint-compliant control through offline model-based reinforcement learning. The effects of state feedback are studied on training and performance for Proximal Policy Optimization (PPO) and Twin Delayed Deep Deterministic Policy Gradients (TD3) agents. Finally, the learned neural network policies are executed on the real engine using a Raspberry Pi controller.


\begin{figure}
    \centering
    \includegraphics[width = 0.46\textwidth]{Figures/Vasu_Plan2-Page-1.drawio.jpg}
    \caption{Modeling and controller design procedure based on deep reinforcement learning }
    \label{fig:offlinemodeling}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{GRU Encoder-Decoder based DNN Engine Model}
\subsection{Experimental Setup Details}
One cylinder of a Cummins QSB 4.5$\,$L 4-cylinder (EPA Offroad Tier 3 certified) engine is modified for port-injected hydrogen to allow for H2DF operation. Hydrogen is injected into the intake runner at the beginning of the intake stroke. Valve timing, combustion chamber geometry, and diesel injectors are stock. Diesel rail pressure is kept at a constant 970$\,$bar for all tests. The production engine control unit (ECU) is replaced with a dSPACE MABX~2 (MABX) and RapidPro power electronics. The MABX contains a Field Programmable Gate Array (FPGA) which is used for real-time calculation of the combustion parameters used in this work, including Maximum Pressure Rise Rate (MPRR) and Indicated Mean Effective Pressure~(IMEP). The MABX allows for fully flexible control of the diesel injection, hydrogen injection, and diesel pressure. The MABX is then connected to a Raspberry Pi 400 where the developed RL control strategies are run. More details outlining the experimental setup can be found in~\citep{Gordon.2022}.
 
\subsection{Offline Model Learning}

To model the engine dynamics, performance, and emissions, a DNN model with six fully connected layers and one recurrent unit layer was developed as shown in Fig.~\ref{fig:DNNmodel}.
\begin{figure}
    \centering
    \includegraphics[trim = 10 370 40 5, clip, width = 0.45\textwidth]{Figures/StructureOfModell_H2DF_MPC.drawio.pdf}
    \caption{Deep Neural Network Engine Plant Model. GRU: gated recurrent unit, DOI: duration of injection, P2M: pre to main injections, SOI: start of injection, IMEP: indicated mean effective pressure, MPRR: maximum pressure rise rate.}
    \label{fig:DNNmodel}
\end{figure}
The model structure has proven to be effective in similar works in \citep{Gordon.2022, Norouzi.2023, Gordon.2024}. 
The recurrent units within the DNN are GRUs, which can be described by
\begin{subequations}\label{eq:GRU_equ}
   \begin{align}   
    z(t) &= \sigma\left(W_{h,z}^\top h(t-1) + W_{u,z}^\top u(t) + b_{z}\right), \\
    r(t) &= \sigma\left(W_{h,r}^\top h(t-1) + W_{u,r}^\top u(t) + b_{r}\right) ,\\
    \tilde{h}(t) &= \tanh\left(W_{u,\tilde{h}}^T u(t) + W_{h,\tilde{h}}^\top (r(t) \odot h(t-1)) + b_{\tilde{h}}\right),\\
    h(t) &= (1-z(t)) \odot h(t-1) + z(t) \odot \tilde{h}(t).
   \end{align}
\end{subequations}
where $u(t)$ is the input vector to the unit, $z(t)$ and $r(t)$ are the update gate vector and
reset gate vector, respectively. $h(t)$ is the output vector of the unit, $\tilde{h}(t)$ is the candidate activation vector, while $W$ and $b$
are the parameter weights and biases to be learned during training. GRU offers the advantage of achieving similar performance in time-series prediction as Long Short-Term Memory units, while using fewer learnable parameters and requiring fewer operations, making them more efficient during training and execution.
\subsubsection{Plant Training: }As shown in Fig.~\ref{fig:DNNmodel}, the inputs $u(t)$ and outputs $y(t)$ of the engine plant model can be denoted as 
\begin{align}
u(t) &=
\begin{bmatrix}
u_{\text{DOI,fuel}}(t) & u_{\text{P2M}}(t) & u_{\text{SOI,fuel}}(t) \\ 
u_{\text{DOI,H2}}(t) & y_{\text{IMEP}}(t - 1)
\end{bmatrix}^T, \notag \\
y(t) &=
\begin{bmatrix}
y_{\text{IMEP}}(t) & y_{\text{NOx}}(t) & y_{\text{Soot}}(t) & y_{\text{MPRR}}(t)
\end{bmatrix}^T.
\end{align}
The duration of injection (DOI) of the two fuels, diesel (Main) and hydrogen (H2), the time between the pre and the main diesel injection (P2M), and the starting crank angle degree (CAD) of the main diesel injection (SOI) are the control variables of the engine and the DNN model inputs. The engine and DNN model outputs are the IMEP which represents the engine load, nitrogen oxide emissions \nox, particulate emissions(Soot) and MPRR. 

A dataset for the DNN training of 85,000 consecutive cycles was collected from the engine utilizing a pseudo-random binary input generator to train the DNN. The training parameters are shown in Tab.~\ref{tab:TrainingSettings}, with an 80/15/5 split between training, validation, and unseen test data.
\subsubsection{Results: }The resulting accuracy of the DNN model can be considered sufficient, with the Root Mean Squared Percentage Error (RMSPE) being under 4.6\% for all four prediction outputs on the unseen test dataset. Fig.~\ref{fig:NNTestResults} shows the prediction on the unseen test dataset time-series.
\begin{table}[htbp]
    \centering
    \renewcommand{\arraystretch}{1.3}
    \caption{Offline plant learning hyperparameters}
    \begin{tabular*}{0.45\textwidth}{l @{\extracolsep{\fill}} @{\hspace{1em}} | c}
        \hline
        \hline
        \textbf{Parameter} & \textbf{Value} \\
        \hline
        \hline
        Maximum Epochs              & 1000 \\
        Batch Size                  & 512 \\
        Gradient Threshold          & 1 \\
        L2 Regularization           & 0.01 \\
        Learning Rate Decay         & Piecewise at 250 Epochs \\
        Initial Learning Rate       & 0.0005 \\
        Learning Rate Drop Factor   & 0.75 \\
        Validation Patience         & 3 \\
        \hline
    \end{tabular*}
    \label{tab:TrainingSettings}
\end{table}
\begin{figure}[htbp]
\centering
\includegraphics[ width = 0.48\textwidth]{Figures/Training_Results_Test.pdf}
\caption{GRU-based network outputs on the test set}
\label{fig:NNTestResults}
\end{figure}

\section{Offline Reinforcement Learning}
\subsection{Reward Modeling with Additive Noise}
The reward formulation term (Eq. \ref{eq:RewardForm}) consists of multiple 
penalty terms, namely: control input minimization term \( r \), output deviation term \( q \), reward staging term \(R^{-}\), constraint violation penalty term \( S \). The reward is calculated by adding all penalty terms and taking the negative absolute value (Eq.~\ref{eq:RewardForm}). The formulation is designed for multi-objective optimization, as commonly seen in model predictive control (MPC)~\citep{Gordon.2022}.

The output deviation term rewards the agent for strict tracking to the required IMEP load reference, while minimizing the emissions (\nox~and Soot) and ensuring engine durability (MPRR). In contrast, the control minimization term penalizes the agent for larger control actions during tracking. The reward staging term (Eq. \ref{eq:rewardstaging}) exponentially rewards the agent as it matches the tracking target, where the extra gradient information aims to mitigate the diminishing gradients from the least-square term ($q_1$).
\begin{equation}
\scalebox{0.8}{$  % Adjust scaling factor (0.9) as needed
\sum_{t=0}^{N} R_t = - \left( 
\begin{aligned}
& \alpha \cdot\sum_{t=0}^{N} \Bigg( \|
 \underbrace{\left( r_{\text{IMEP}_t} - y_{\text{IMEP}_t} \right)^2}_{q_1 \text{ Load Tracking}} +
 \underbrace{\left( y_{\text{MPRR}_t} \right)^2}_{q_2 \text{ Combustion Noise}} \\
& \quad + 
 \underbrace{\left( y_{\text{NOx}_t} \right)^2 + \left( y_{\text{Soot}_t} \right)^2}_{q_3 ~\text{Emissions Minimization}} \Bigg\|_Q \Bigg) \\
& + \beta \cdot \sum_{t=0}^{N}  \Bigg( \|
 \underbrace{\left( u_{\text{H2DOI}_{\text{main}, t}} \right)^2 + \left( u_{\text{DOI}_{\text{Main}, t}} \right)^2}_{r~\text{Control ...}} \\
& \quad + \underbrace{\left( u_{\text{P2M}_t} \right)^2 + \left( u_{\text{SOI}_{\text{Main}, t}} \right)^2}_{\text{...Effort Minimization}} \Bigg\|_{\tilde{R}} \Bigg) \\
& + \sum_{t=0}^{N} \Bigg( 
\underbrace{\|R^{-}_{t}\|}_{R^{-} \text{ Reward Staging}} + \zeta\cdot\underbrace{\|W_t\|}_{W \text{ Constraint Violation}}
\Bigg)
\end{aligned}
\right)$}
\label{eq:RewardForm}
\end{equation}
\begin{equation}
R^{-}_{t} = -0.0025 \times 10^{\left\lfloor 5 - \log_{10}(\delta \text{IMEP}_t) \right\rfloor},
\label{eq:rewardstaging}
\end{equation}
where $\delta \text{IMEP}_t$ refers to $\left | r_{\text{IMEP}_t} - y_{\text{IMEP}_t}\right|$. Finally, for the agent to discover a safe operating region in the output space, the constraint violation term $S$ signals safety violations caused by the agent's actions, via large penalties $W_t$. Safety bounds are pre-selected based on process knowledge and any violations outside the safe polytope result in an $L_1$-style  penalty. The formulation allows soft-safety to be directly included in the optimization objective of the problem and builds on previous work \citep{Norouzi.2023}. The resultant penalty polytope is visualized in Fig.~\ref{fig:ConstraintViolationPenalty3D} for two arbitrary output domains.

% \begin{figure}[H]
%     \centering
%     \begin{tikzpicture}
%         \begin{axis}[
%             width=6cm, height=6cm, % Size of the plot
%             grid=both, % Show grid
%             grid style={black, opacity=0.2}, % Grid line color and opacity
%             xmin=0, xmax=35, % Limits for x-axis (Output Domain 1)
%             ymin=0, ymax=35, % Limits for y-axis (Output Domain 2)
%             zmin=0, zmax=100, % Limits for z-axis (Penalty)
%             axis background/.style={fill=white}, % Background color
%             xlabel={Output Domain 1}, 
%             ylabel={Output Domain 2}, 
%             zlabel={Penalty ($W_t$)}, % Labels for axes
%             tick style={color=black}, % Tick mark color
%             tick label style={black}, % Color of axis labels
%             axis line style={black}, % Axis line color
%             y label style={sloped}, % Slanted label for Output Domain 2
%             ytick={0,10,20,30}, % Y-ticks (Output Domain 2)
%             xtick={0,10,20,30}, % X-ticks (Output Domain 1)
%             ztick={0,25,50,75,100}, % Z-ticks (Penalty)
%             colormap/viridis, % Apply a colormap for the surface
%             colorbar, % Include colorbar on the side to show penalty values
%             colorbar style={
%                 title={Penalty $W_t$},
%                 ytick={0,50,100}, % Define ticks on the colorbar
%             },
%             every axis plot/.append style={ultra thick}, % Line style
%             legend style={at={(0.5,1)}, anchor=north west}, % Position legend outside
%             legend cell align={left}
%         ]

%         % Create a square region with zero penalty
%         \addplot3 [
%             fill=blue, opacity=0.7, % Use blue color for zero penalty region
%             domain=10:25, % Adjusted range for Output Domain 1
%             y domain=10:25, % Adjusted range for Output Domain 2
%             samples=16 % Integer increments from 10 to 25 (16 samples total)
%         ]
%         {0}; 

%         % Create a surface representing linear increase in penalty outside the square region
%         \addplot3 [
%             mesh, % Use mesh option to avoid sampling issues
%             fill=orange, opacity=0.5, % Orange for penalty surface
%             domain=0:35, % Adjusted range for Output Domain 1
%             y domain=0:35,
%             samples=36, % Integer increments from 0 to 35 (36 samples total)
%         ]
%         {
%             abs(4 * (max(0, 10 - x)) + 4 * (max(0, x - 25))) + % Distance in x-direction
%             abs(4 * (max(0, 10 - y)) + 4 * (max(0, y - 25)))   % Distance in y-direction
%         };
%         %\addlegendentry{Penalty Surface}
        
%         \end{axis}
%     \end{tikzpicture}
%     \caption{Constraint violation penalty visualized over multiple output domains}
%     \label{fig:ConstraintViolationPenalty3D}
% \end{figure}

\begin{figure}
    \centering
    \includegraphics[width = 0.46\textwidth]{Figures/Screenshot_ConstraintVoilation_.jpg}
    \caption{Constraint violation penalty visualized over multiple output domains.}
    \label{fig:ConstraintViolationPenalty3D}
\end{figure}

The terms in \( r \) and \( q \) are weighted with weight matrices \( \tilde{R} \) and \( Q \). Moreover, three constants $\alpha, \beta, \zeta$ are needed for relative weighting of the terms between \( r \), \( q \) and \( S \), thus allowing large penalties for constraint violations and output deviations.

\subsection{State Augmentation with Perturbed Rewards}

The trained plant model is a deterministic mapping ($\text{f}_{\text{GRU-NN}}$) from the controls to the output space. While it matches the real process closely in simulations, the experimentally measured signals could still be noisy. Additive Gaussian noise is thus added to the outputs during training to better represent the noisy signals from the actual process. This also perturbs the reward signals, which may help the optimizer from getting stuck in local minima.

The applied RL controls ($u_{RL}$) result in an internal state ($\hat{h}_{\text{GRU}}(t)$) change for the DNN model-based environment. These updated states can also be accessed by propagating the estimated outputs of the network (Eq. \ref{eq:stateaugmentation}). This lookahead during learning informs the RL agents about the current system state. Based on the accuracy of the model and the reward formulation, the agent can choose to use or reject this information with gradient updates. The state feedback also introduces a multi-step temporal feature into the observation model.
    \begin{equation}
\begin{aligned}
    \text{f}_{\text{GRU-NN}}(u_{RL}(t), \tilde{y}_{\text{IMEP}}(t-1)) \to \hat{y}(t), \\
    \hat{y}(t)= \left[ \hat{y}_{\text{IMEP}}(t), \hat{y}_{\text{NOx}}(t), \hat{y}_{\text{Soot}}(t), \hat{y}_{\text{MPRR}}(t) \right]^T, \\
    \tilde{y}(t) \gets \hat{y}(t) + \epsilon, \quad \epsilon \sim \mathcal{N}(0, \tilde{\sigma}), \\
    \text{f}_{\text{GRU-NN}}(u_{RL}(t), \tilde{y}_{\text{IMEP}}(t-1)) \to \hat{h}_{\text{GRU}}(t).
\end{aligned}
\label{eq:stateaugmentation}
\end{equation}


\subsection{RL Training}
The RL agent observes the environment states $o(t)$, including the outputs from the last applied controls, the current reference load, and the augmented plant states (Eq.~\ref{eq:ot}, Eq. \ref{eq:otdefinitions}) to decide on the current actions $u_{\text{RL}}$. Thus $o(t)$ can be defined as:
\begin{align}
o(t) &= 
\begin{bmatrix} 
\tilde{y}(t\!-\!1) & {\Delta \tilde y}(t\!-\!1) & r_{\text{load}} & {h}_{\text{GRU}}(t) 
\end{bmatrix}^T
\label{eq:ot}
\end{align}
\begin{align}
\begin{split}
    \tilde{y}(t\!-\!1) &= \begin{bmatrix} y_{\text{IMEP}}(t\!-\!1) & y_{\text{NOx}}(t\!-\!1) & y_{\text{MPRR}}(t\!-\!1) \end{bmatrix}^T, \\
    {\Delta \tilde y}(t\!-\!1) &= \begin{bmatrix} \Delta y_{\text{IMEP}}(t\!-\!1) & \Delta y_{\text{NOx}}(t\!-\!1) \end{bmatrix}^T, \\
    r_{\text{load}} &= \begin{bmatrix} r_{\text{IMEP,ref}}(t) & r_{\text{IMEP,ref}}(t\!-\!1) & r_{\text{IMEP,Err}}(t\!-\!1) \end{bmatrix}^T, \\
    {h}_{\text{GRU}}(t) &= \hat{h}_{\text{GRU}}(t).
\end{split}
\label{eq:otdefinitions}
\end{align}
\begin{equation}
u_\text{RL}(t) = 
\begin{bmatrix}
u_{\text{DOI,fuel}}(t) & u_{\text{P2M}}(t) & u_{\text{SOI,fuel}}(t) & u_{\text{DOI,H2}}(t)\end{bmatrix}^T.
\end{equation}
Using the previously elaborated reward model, off-policy and on-policy algorithms are trained in an episodic setting. Random trajectories are generated per episode for a total episodic length of 625 time-steps. Twin Delayed Deep Deterministic Policy Gradients (TD3) is an off-policy, actor critic-style RL algorithm that uses randomly sampled experiences from an experience buffer to train a deterministic policy \citep{Fujimoto.2018}. For improved stability, delayed target updates with policy smoothing and two critic networks are used. Proximal Policy Optimization (PPO) is an on-policy, advantage actor critic algorithm that uses a clipped surrogate objective \citep{Schulman.2017}. Both of these are used in this work. The relevant training parameters are included in Tab.~\ref{tab:OfflineTrainingHyperparams}.
\begin{table}[htbp]
\centering
\renewcommand{\arraystretch}{1.3}
\caption{RL training hyperparameters}
\begin{tabular*}{0.45\textwidth}{l @{\extracolsep{\fill}} @{\hspace{2em}} | c}
\hline \hline
\textbf{Parameter} & \textbf{Value} \\
\hline \hline
Observation Space Size &            16 \\
Action Space Size &          4 \\
Training Stop Threshold &       9 \\
Optimizer &         Adam \\
Learning Rate &         0.0001 \\
Gradient Threshold &        2 \\
Discount Factor &            0.99 \\
Experience Buffer Size &            100,000 \\
Sample Time &           0.08 \\
\hline
\end{tabular*}
\label{tab:OfflineTrainingHyperparams}
\end{table}
The training plots presented in Fig.~\ref{fig:Td3vsPPO} highlight the sample efficiency of the state augmented off-policy method. Both state augmented cases not only reach higher reward values, but also do so much faster, compared to their non state augmented counterparts.
\begin{figure}[htbp]
% Adjust spacing before the figure
\centering
\includegraphics[width=0.45\textwidth]{Figures/Combined_OffPolicyVsOnPolicy.pdf}
\caption{RL training for various agents}
\label{fig:Td3vsPPO}
\end{figure}
\subsection{RL Validation}
The trained RL agents are simulated over a random virtual trajectory with a network in the loop simulation. The agents observe the reference IMEP load to decide on control actions. To demonstrate the robustness of the proposed method, the validation trajectory is approximately 8 times longer than training trajectories with multiple load changes and ramps. The results are included in Fig.~\ref{fig:ValResults}. 
\begin{figure}[htbp]
\centering
\includegraphics[width=0.49\textwidth]{Figures/ACC2025viewcombined.pdf}
\caption{Constraint compliant load tracking: PPO and TD3}
\label{fig:ValResults}
\end{figure}
Both methods demonstrate excellent tracking behavior, highlighting good credit assignment and generalization of the agents. Noteworthy is the difference in control action chosen by the different agents through the various training constraints. The choice of fuel is particularly of interest, where the PPO agent is able to learn to deploy more hydrogen for the same output load, whereas the TD3 agent prefers to use more diesel, since it has a higher NOx tolerance during training. 

Another key point is the difference in behavior of the SOI. While the TD3 agent chooses to almost replicate the requested trajectory, the PPO agent does not really change the SOI trajectory much. This behavior suggests that the engine is not particularly sensitive to SOI changes.

\section{Real-Time Implementation}
The neural network policies from the trained agents are  validated on the actual engine using an ARM Cortex-A72  64-bit Raspberry Pi 400. MATLAB's Raspberry Pi interface, Coder, and the Deep Learning toolbox are used to generate executable C code. The  Pi 400 receives the current engine states from the MABX, along with the desired IMEP via User Datagram Protocol (UDP) and evaluates the policy network to get control actions. Based on the performance of various networks, a cascaded policy selection controller is developed. The total turnaround time is 1$\,$ms which is 6 times faster than a comparative GRU-based MPC. The validation results are included in Fig.~\ref{fig:RealTimeimplementation}.
\begin{figure}[htbp]
\centering
\includegraphics[ width = 0.49\textwidth]{Figures/Tracking_Results.pdf}
\caption{Real-time policy validation results on the actual engine using a neural cascaded controller}
\label{fig:RealTimeimplementation}
\end{figure}
\section{Summary and Conclusions}
This paper presents the integration of reinforcement learning with offline plant learning for controller implementation for a hydrogen diesel engine application. First, a deep learning-based Gated Recurrent Unit (GRU) encoder-decoder model is trained based on real engine data. The learned plant model is then used to train off-policy (TD3) and on-policy (PPO) agents, with a constraint aware reward model and state augmentation. The trained policies are then executed on a Raspberry Pi to control the real engine, which demonstrates a significant reduction in computation time.

The paper presents a novel end-to-end data-driven approach to control a real process. The modular approach to plant learning and control can leverage the latest developments in deep learning and reinforcement learning in the future. The developed toolchain does not need any special hardware for real-time execution of the learned policies. Real-time capable online fine-tuning could further improve the presented results. The method could also be applied to carbon-free fuels for further emission reduction.

\section*{Acknowledgments}
The author(s) disclosed receipt of the following financial support for the research, authorship, and/or publication of this article: The research was carried out as part of the NRC-MITACS-RWTH Globalink Research Award Program, which is funded by the National Research Council Canada (NRC), MITACS, and Rheinisch-Westf√§lische Technische Hochschule (RWTH) Aachen University.
\bibliography{Bib_MA}

% \appendix

\end{document}
