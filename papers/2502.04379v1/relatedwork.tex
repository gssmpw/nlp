\section{Related Work}
\label{sec:background}

This study investigates the capacity of LMMs to accurately annotate subjectively-defined aspects of gameplay. We leverage the existing \emph{knowledge-priors} of these algorithms, without fine-tuning or using complex retrieval augmented strategies. We thus hypothesise that the algorithm's prior knowledge is sufficient to approximate the ground truth of engagement (as provided via human feedback) in a set of gameplay scenarios. This section covers related work in affect modelling using LLMs, the use of LLMs in games, and it ends with a focus on modelling aspects of players and their games.

\subsection{LLMs for Affect Modelling}

Given the resounding success of LLMs in several domains, several recent research efforts naturally focus on their direct application in affect detection tasks. The vast majority of research on LLMs related to human affect have focused on predicting manifestations of affect from text as this plays to the strengths of their architecture. Unsurprisingly, sentiment analysis has been the most common research application of LLMs in affective computing and has given us some impressive results already \cite{mao2022biases}. Indicatively, Broekens et al. \cite{broekens2023fine} highlighted how \emph{GPT-3.5} can accurately perform sentiment analysis on the ANET corpus \cite{bradley2007affective} for valence, arousal and dominance. Similarly, MÃ¼ller et al. \cite{muller2024recognizing} used fine-tuned \emph{Llama2-7b} \cite{touvron2023llama} and \emph{Gemma} \cite{team2024gemma} models to classify shame in the \emph{DEEP} corpus \cite{schneeberger2023deep}, achieving $84\%$ accuracy. Whilst LLMs have been extensively tested for sentiment analysis on existing text-based corpora, research on using LLMs as predictors of experience by observing multimodal content such as games remains unexplored. 

Despite their promise, some critical challenges have emerged when working with pre-trained LLMs for prediction tasks such as affect modelling. A recent study by Chochlakis et al. \cite{chochlakis2024strong} has found that LLMs struggle to perform meaningful in-context learning from new examples and remain fixed to their knowledge priors, with larger models exaggerating this issue. This problem is even more pressing in closed-source models such as \emph{GPT-4o} because researchers lack important details which can help them assess the level of data contamination. Balloccu et al. \cite{balloccu2024leak} conducted a study across $255$ academic papers and found that LLMs have been exposed to a significant number of samples from existing ML benchmarks, potentially painting a misleading picture about their predictive performance in such tasks. 
%
While the dataset we use in this paper covers a novel domain, it is possible that some of the videos in the \emph{GameVibe} dataset have been exposed to some of the models we use. However, because the dataset was published after the models used here\footnote{\emph{LLaVA 1.6} was published on 18 July 2023; \emph{GPT-4o} was originally released on 13 May 2024; the \emph{GameVibe} dataset was published on 17 June 2024.}, we are confident that the engagement prediction task specifically does not suffer from any significant data contamination.

Beyond contamination, we also have to face the inherent biases encoded in LLMs. Mao et al. \cite{mao2022biases} have conducted a study on such biases in \emph{BERT}-like models \cite{devlin2018bert} on affective computing tasks. 
% 
In our study we use what Mao et al. call ``coarse-grain'' tasks---a binary decision with symmetrical labels (here \emph{increase} and \emph{decrease} of engagement). When evaluating these types of tasks, LLMs have been shown to exhibit less bias \cite{mao2022biases} than on ``fine-grained'' tasks with multiple asymmetrical labels. This gives us confidence on the feasibility of our task---which is formulated as a binary classification problem. 

Amin et al. \cite{amin2024wide} have also conducted a study on the capabilities of \emph{GPT} \cite{openai2023gpt4} on affective computing tasks. They have put forth a comprehensive series of experiments which included a similar pairwise preference classification task for engagement prediction to what we use in this paper. They showed that when it comes to subjective tasks with a high potential for disagreement between annotators, out-of-box LLMs, such as \emph{GPT} struggle compared to architectures leveraging specialized supervised networks. In those experiments---focusing  on a simple one-shot prompting strategy on text input---\emph{GPT} barely surpassed the baseline. In contrast \cite{amin2024wide}, we investigate multimodal, chain-of-thought, and few-shot strategies in visual-based engagement prediction tasks across multiple games, analysing where LLMs either struggle or flourish compared to baseline approaches.

\subsection{LLMs in Games}

The recent developments in LLM methods and technology 
brought unprecedented wide adoption of AI across multiple domains including law \cite{lai2024large}, healthcare \cite{nazi2024large}, and education \cite{moore2023empowering}. Advancements in transformer architectures \cite{vaswani2017attention}, coupled with a rapid increase in dataset and parameter sizes \cite{kaplan2020scaling} led to a new wave of algorithms with previously unseen capabilities to generate high-quality text. Starting with Bidirectional Encoder Representations from Transformers (BERT) \cite{devlin2018bert} but eventually popularized with the release of Generative Pre-trained Transformers (GPT) \cite{radford2019language,floridi2020gpt,openai2023gpt4}, LLMs have largely been characterized as transformer-based models, using large amounts of parameters (in the 100 millions and billions), built on large amounts of data, generating text in an autoregressive manner---that is predicting future tokens based on prior data. More recently, LLMs have been expanded to handle new modalities beyond text, such as audio and images \cite{touvron2023llama}, making them a candidate for applications using multimodal content such as gameplay videos.

In the context of games, LLMs have been used to create game-playing agents \cite{tsai2023can,hu2024survey}, commentators \cite{ranella2023towards} game analytics \cite{wang2024player2vec, ravsajski2024behave}, AI directors and game masters \cite{you2024dungeons, zhu2023calypso}, content generators \cite{sudhakaran2024mariogpt}, and design assistants \cite{gallotta2024llmaker}. 
% 
Beyond the academic setting, we are seeing considerable interest from industrial players as well, such as NVIDIA's recent ACE small language models \footnote{\url{https://developer.nvidia.com/ace}} for autonomously generating the behaviour and animation of NPCs. 
% 
Gallotta et al. \cite{gallotta2024large} offer a recent and thorough overview on how LLMs can be utilised in games. In their roadmap, they identify player modelling as one of the most promising, yet unexplored avenues for future research into LLMs and games. Whilst affect modelling research has demonstrated that LLMs can be effective predictors in tasks such as sentiment analysis \cite{mao2022biases}, they are yet to be widely evaluated to modelling player experience in the context of games. 

\subsection{Player Affect Modelling}

Player modelling is an active field within AI and games research \cite{yannakakis2018artificial} with a particular focus on methods that capture emotional and behavioural aspects of gameplay such as engagement \cite{melhart2020moment}, toxicity \cite{canossa2021honor} and motivation \cite{melhart2019your}. Traditionally, the field has focused heavily on data aggregation \cite{el2016game} and pattern discovery \cite{makarovych2018like, melhart2019your} of playing behaviours, but there has been a recent shift towards moment-to-moment predictive models of players 
\cite{makantasis2021privileged,melhart2020moment,melhart2021towards,booth2024people,barthet2024gamevibe}. The prevalent strategy of such modelling methods relies on the availability of continuous annotation traces, which are generally processed as interval data \cite{yannakakis2018ordinal}. This allows for the treatment of the labelled data as absolute ratings such as player engagement levels or classes such as low and high game intensity \cite{booth2024people,makantasis2021privileged}.

In contrast to the traditional way of treating annotations as absolute ratings, here we view player modelling as an ordinal learning paradigm aiming to maximize the reliability and validity of our predictive models \cite{yannakakis2018ordinal,yannakakis2017ordinal}. We task LLMs to label \emph{increases} or \emph{decreases} of engagement across frames of a game instead of asking them to provide ratings of engagement per frame. The ordinal representation of subjective notions such as engagement is supported both by theories of human psychology and cognition \cite{helson1964current,solomon1974opponent} and by a growing body of research in neuroscience \cite{damasio1996somatic} and affective computing \cite{yannakakis2015grounding,lotfian2016practical,yannakakis2018ordinal,melhart2020study,melhart2021towards,barthet2024gamevibe} among other disciplines. Importantly, we employ LLMs and we test their ability to model game engagement as viewed through gameplay videos. 


\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{graphics/Pagan.png}
\label{fig:pagan}
\caption{Example clip from \emph{GameVibe} showcasing the annotation interface using PAGAN and the RankTrace annotation tool for collecting unbounded, time continuous signals in real-time.}
\end{figure}