%% bare_jrnl.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% see http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/

\documentclass[journal]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[journal]{../sty/IEEEtran}


% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
\usepackage{url}
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.

%Remove before camera-ready
% \usepackage{comment}
% \usepackage{todonotes}
\usepackage{xcolor}

% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.


% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex


% *** MATH PACKAGES ***
%
\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath


% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx


% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.


% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig


% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix


%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.


% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.


% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
%\title{Towards Continuous Engagement Prediction in Games Using Large Language Models}
\title{Can Large Language Models Capture \\Video Game Engagement?}

%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%

% \author{
%     \IEEEauthorblockN{Anonymous}
%     \\
%     \IEEEauthorblockA{\emph{Institute}\\
%     Country \\
%     email@address.com}}% <-

% REMOVED FOR ANONYM
\author{
    \IEEEauthorblockN{David Melhart, Matthew Barthet, and Georgios N. Yannakakis, \emph{IEEE Fellow}}
    \\
    \IEEEauthorblockA{\emph{Institute of Digital Games, University of Malta}\\
    Msida, Malta \\
    david.melhart@um.edu.mt, matthew.barthet@um.edu.mt, georgios.yannakakis@um.edu.mt}}% <-this % stops a space
% \thanks{...}% <-this % stops a space

% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.


% The paper headers
% \markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2015}%
% {Shell \MakeLowercase{\emph{et al.}}: Bare Demo of IEEEtran.cls for IEEE Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.


% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.


% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}


% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\begin{abstract}
Can out-of-the-box pretrained Large Language Models (LLMs) detect human affect successfully when observing a video?
% 
To address this question, for the first time, we evaluate comprehensively the capacity of popular LLMs to annotate and successfully predict continuous affect annotations of videos when prompted by a sequence of text and video frames in a multimodal fashion. Particularly in this paper, we test LLMs' ability to correctly label changes of in-game engagement in 80 minutes of annotated videogame footage from 20 first-person shooter games of the \emph{GameVibe} corpus. We run over 2,400 experiments to investigate the impact of LLM architecture, model size, input modality, prompting strategy, and ground truth processing method on engagement prediction. Our findings suggest that while LLMs rightfully claim human-like performance across multiple domains, they generally fall behind capturing continuous experience annotations provided by humans. We examine some of the underlying causes for the relatively poor overall performance, highlight the cases where LLMs exceed expectations, and draw a roadmap for the further exploration of automated emotion labelling via LLMs.
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Large language models, affective computing, player modelling, engagement
\end{IEEEkeywords}

% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle


\section{Introduction}
The use of autoregressive modelling and large pretrained models such as Large Language Models (LLMs) is currently dominating AI research. LLMs have demonstrated unprecedented advances in language translation, code generation, problem solving, and AI-based assistance among many other downstream tasks \cite{radford2019language}. Given their versatility and efficiency compared to earlier autoregressive models, one might even argue that the current capabilities of LLMs are endless as long as a problem and its corresponding solution(s) are represented as text.
% 
Meanwhile, the recent applications of LLMs within affective computing largely consider text-based affect modelling tasks such as LLM-based sentiment analysis \cite{zhang2023sentiment, broekens2023fine, zhang2024affective}.
% 
The automatic labelling of affect based on time-continuous visual input remains largely unexplored \cite{zhang2024affective}, however, as the handful of studies available rely on still images \cite{lian2024gpt, yang2024mm}.

Motivated by the aforementioned lack of studies this paper introduces the first comprehensive evaluation of LLMs tasked to predict time-continuous affect labels from videos. In this initial evaluation we let LLMs observe gameplay videos as we prompt them with textual information of what they observe, and ask them to label the viewer engagement on those videos. We chose games as the domain of our study since they can act as rich elicitors of emotions and can offer a wide range of dynamic scenes and stimuli, varying from intense player actions to less intense game-world exploration.
% 
Even though LLMs have been used in a series of diverse tasks within the domain of videogames---both in academic studies \cite{gallotta2024large,yannakakis2018artificial} and industrial applications such as \emph{AI Dungeon} (Latitude, 2019), \emph{AI People} (GoodAI, 2025) and \emph{Infinite Craft}\footnote{https://neal.fun/infinite-craft/}---the capacity of these foundation models as predictors of player experience has not been investigated yet.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{graphics/Games_Screenshot.png}
\caption{Clips in the \emph{GameVibe} Dataset. List of game titles: (1) \emph{Apex Legends}; (2)\emph{ Blitz Brigade}; (3) \emph{Borderlands} 3; (4) \emph{Corridor 7}; (5) \emph{Counter Strike 1.6}; (6) \emph{CS:GO - Dust2}; (7) \emph{CS:GO - Office}; (8) \emph{Doom}; (9) \emph{Insurgency}; (10) \emph{Far Cry}; (11) \emph{Fortnite}; (12) \emph{Heretic}; (13) \emph{Medal of Honor 2010}; (14) \emph{Overwatch 2}; (15) \emph{PUBG}; (16) \emph{Medal of Honor 1999}; (17) \emph{Team Fortress 2}; (18) \emph{Void Bastards}; (19) \emph{HROT}; (20) \emph{Wolfram}.}\label{fig:gamevibe}
\end{figure}

\begin{figure*}[t]
\centering
\includegraphics[width=1\linewidth]{graphics/PROCESS.pdf}
\caption{Overview of the evaluation experiments presented in this study. Independently of experimental setting, the downstream task is engagement prediction formulated as a binary preference.
We use a combination of text prompts and/or video frames as input and task the LLMs to label engagement. To evaluate the models, we compare the generated labels to the ground truth labels from the annotated \emph{GameVibe} corpus (see Section \ref{sec:data}).
All LLMs are prompted with a Chain-of-Thought (CoT) strategy. In the \emph{Text Input} setup, the input for the downstream task is text descriptions (see Section \ref{sec:methods:text}) whereas in the \emph{Multimodal} settings, the input contains both images and text prompts (see Section \ref{sec:methods:visual}). In the \emph{few-shot} experiments we generate reasoning examples based on ground truth evaluations. The examples are given to the LLM in addition to the base CoT prompt and the images (see Section \ref{sec:methods:prompts}). In all experimental settings we generate a \emph{description}, \emph{comparison}, \emph{reasoning}, and a \emph{decision} relating to an increase or decrease in engagement. We parse these outputs to derive the final binary engagement evaluation. $^\ast$\emph{Descriptions} are only generated in the \emph{Multimodal Input} settings.}\label{fig:overview}
\end{figure*}

We employ LLMs as autonomous player experience annotators and present a thorough evaluation of their capacity to predict player experience in one-shot and few-shot fashions. Specifically, we compare state of the art foundation models from the \emph{LLaVA} and \emph{GPT} families against human annotated data of player engagement of the \emph{GameVibe} dataset \cite{barthet2024gamevibe} (see Fig.~\ref{fig:gamevibe}). The dataset contains continuous engagement labels of gameplay videos across a variety of first-person shooter (FPS) games. We present selected results out of $2,440$ experimental settings in which we vary and test LLM model types, model sizes, prompting strategies, input types, and ground truth processing methods. 
% 
Figure~\ref{fig:overview} shows a high level overview of our experimental setup followed in this study. We focus on videogame footage---one might come across on game streaming services such as \emph{Twitch}\footnote{\url{https://www.twitch.tv}}---as input and \emph{viewer engagement} as output. 

%Key findings 
The novelty of this paper is two-fold. First, we investigate the capacity of LLMs to accurately label affect in a time-continuous manner using videos as affect elicitors. Second, we present the first large set of evaluation experiments that lays the groundworks for LLM-based player experience prediction. Our experiments show the feasibility of leveraging LLMs for engagement prediction particularly on popular games with a rich online presence (such as ApexLegends, 2019). Our key findings suggest that a) text-based summarisation of frames and direct multimodal prompting do not impact LLM performance; b) LLM performance is largely dependent on the elicitor (i.e. different games in this study); c) the multimodal few-shot prompting strategy is the one that improves LLM performance the most;
% 
and d) scale matters. Specifically, the best results obtained are when we employ the \emph{GPT-4o} model and we feed it with a few positive and negative multimodal examples of increasing or decreasing engagement (few-shot prompting). While this approach yields an average accuracy of $6\%$ over the baseline across games, the \emph{GPT-4o} model is able to improve the baseline performance by up to $47\%$ in certain games.

%Paper structure
The paper is structured as follows. Section \ref{sec:background} presents related work on LLMs for affect modelling, uses of LLMs in games, and player modelling. Section \ref{sec:data} briefly presents the \emph{GameVibe} dataset and the data preprocessing. Section \ref{sec:methods} discusses our approach, presenting the models used and the different prompting strategies we employed. Section \ref{sec:results} presents the key results obtained, including a sensitivity analysis and hyperparameter tuning, a comparison between different input modalities, results of few-shot experiments, and a qualitative analysis on the most and least successful models. The paper ends with a brief discussion on possible avenues for future research (see Section \ref{sec:discussion}) and our key conclusions (see Section \ref{sec:conclusions}). 

\section{Related Work}\label{sec:background}

This study investigates the capacity of LMMs to accurately annotate subjectively-defined aspects of gameplay. We leverage the existing \emph{knowledge-priors} of these algorithms, without fine-tuning or using complex retrieval augmented strategies. We thus hypothesise that the algorithm's prior knowledge is sufficient to approximate the ground truth of engagement (as provided via human feedback) in a set of gameplay scenarios. This section covers related work in affect modelling using LLMs, the use of LLMs in games, and it ends with a focus on modelling aspects of players and their games.

\subsection{LLMs for Affect Modelling}

Given the resounding success of LLMs in several domains, several recent research efforts naturally focus on their direct application in affect detection tasks. The vast majority of research on LLMs related to human affect have focused on predicting manifestations of affect from text as this plays to the strengths of their architecture. Unsurprisingly, sentiment analysis has been the most common research application of LLMs in affective computing and has given us some impressive results already \cite{mao2022biases}. Indicatively, Broekens et al. \cite{broekens2023fine} highlighted how \emph{GPT-3.5} can accurately perform sentiment analysis on the ANET corpus \cite{bradley2007affective} for valence, arousal and dominance. Similarly, Müller et al. \cite{muller2024recognizing} used fine-tuned \emph{Llama2-7b} \cite{touvron2023llama} and \emph{Gemma} \cite{team2024gemma} models to classify shame in the \emph{DEEP} corpus \cite{schneeberger2023deep}, achieving $84\%$ accuracy. Whilst LLMs have been extensively tested for sentiment analysis on existing text-based corpora, research on using LLMs as predictors of experience by observing multimodal content such as games remains unexplored. 

Despite their promise, some critical challenges have emerged when working with pre-trained LLMs for prediction tasks such as affect modelling. A recent study by Chochlakis et al. \cite{chochlakis2024strong} has found that LLMs struggle to perform meaningful in-context learning from new examples and remain fixed to their knowledge priors, with larger models exaggerating this issue. This problem is even more pressing in closed-source models such as \emph{GPT-4o} because researchers lack important details which can help them assess the level of data contamination. Balloccu et al. \cite{balloccu2024leak} conducted a study across $255$ academic papers and found that LLMs have been exposed to a significant number of samples from existing ML benchmarks, potentially painting a misleading picture about their predictive performance in such tasks. 
%
While the dataset we use in this paper covers a novel domain, it is possible that some of the videos in the \emph{GameVibe} dataset have been exposed to some of the models we use. However, because the dataset was published after the models used here\footnote{\emph{LLaVA 1.6} was published on 18 July 2023; \emph{GPT-4o} was originally released on 13 May 2024; the \emph{GameVibe} dataset was published on 17 June 2024.}, we are confident that the engagement prediction task specifically does not suffer from any significant data contamination.

Beyond contamination, we also have to face the inherent biases encoded in LLMs. Mao et al. \cite{mao2022biases} have conducted a study on such biases in \emph{BERT}-like models \cite{devlin2018bert} on affective computing tasks. 
% 
In our study we use what Mao et al. call ``coarse-grain'' tasks---a binary decision with symmetrical labels (here \emph{increase} and \emph{decrease} of engagement). When evaluating these types of tasks, LLMs have been shown to exhibit less bias \cite{mao2022biases} than on ``fine-grained'' tasks with multiple asymmetrical labels. This gives us confidence on the feasibility of our task---which is formulated as a binary classification problem. 

Amin et al. \cite{amin2024wide} have also conducted a study on the capabilities of \emph{GPT} \cite{openai2023gpt4} on affective computing tasks. They have put forth a comprehensive series of experiments which included a similar pairwise preference classification task for engagement prediction to what we use in this paper. They showed that when it comes to subjective tasks with a high potential for disagreement between annotators, out-of-box LLMs, such as \emph{GPT} struggle compared to architectures leveraging specialized supervised networks. In those experiments---focusing  on a simple one-shot prompting strategy on text input---\emph{GPT} barely surpassed the baseline. In contrast \cite{amin2024wide}, we investigate multimodal, chain-of-thought, and few-shot strategies in visual-based engagement prediction tasks across multiple games, analysing where LLMs either struggle or flourish compared to baseline approaches.

\subsection{LLMs in Games}

The recent developments in LLM methods and technology 
brought unprecedented wide adoption of AI across multiple domains including law \cite{lai2024large}, healthcare \cite{nazi2024large}, and education \cite{moore2023empowering}. Advancements in transformer architectures \cite{vaswani2017attention}, coupled with a rapid increase in dataset and parameter sizes \cite{kaplan2020scaling} led to a new wave of algorithms with previously unseen capabilities to generate high-quality text. Starting with Bidirectional Encoder Representations from Transformers (BERT) \cite{devlin2018bert} but eventually popularized with the release of Generative Pre-trained Transformers (GPT) \cite{radford2019language,floridi2020gpt,openai2023gpt4}, LLMs have largely been characterized as transformer-based models, using large amounts of parameters (in the 100 millions and billions), built on large amounts of data, generating text in an autoregressive manner---that is predicting future tokens based on prior data. More recently, LLMs have been expanded to handle new modalities beyond text, such as audio and images \cite{touvron2023llama}, making them a candidate for applications using multimodal content such as gameplay videos.

In the context of games, LLMs have been used to create game-playing agents \cite{tsai2023can,hu2024survey}, commentators \cite{ranella2023towards} game analytics \cite{wang2024player2vec, ravsajski2024behave}, AI directors and game masters \cite{you2024dungeons, zhu2023calypso}, content generators \cite{sudhakaran2024mariogpt}, and design assistants \cite{gallotta2024llmaker}. 
% 
Beyond the academic setting, we are seeing considerable interest from industrial players as well, such as NVIDIA's recent ACE small language models \footnote{\url{https://developer.nvidia.com/ace}} for autonomously generating the behaviour and animation of NPCs. 
% 
Gallotta et al. \cite{gallotta2024large} offer a recent and thorough overview on how LLMs can be utilised in games. In their roadmap, they identify player modelling as one of the most promising, yet unexplored avenues for future research into LLMs and games. Whilst affect modelling research has demonstrated that LLMs can be effective predictors in tasks such as sentiment analysis \cite{mao2022biases}, they are yet to be widely evaluated to modelling player experience in the context of games. 

\subsection{Player Affect Modelling}

Player modelling is an active field within AI and games research \cite{yannakakis2018artificial} with a particular focus on methods that capture emotional and behavioural aspects of gameplay such as engagement \cite{melhart2020moment}, toxicity \cite{canossa2021honor} and motivation \cite{melhart2019your}. Traditionally, the field has focused heavily on data aggregation \cite{el2016game} and pattern discovery \cite{makarovych2018like, melhart2019your} of playing behaviours, but there has been a recent shift towards moment-to-moment predictive models of players 
\cite{makantasis2021privileged,melhart2020moment,melhart2021towards,booth2024people,barthet2024gamevibe}. The prevalent strategy of such modelling methods relies on the availability of continuous annotation traces, which are generally processed as interval data \cite{yannakakis2018ordinal}. This allows for the treatment of the labelled data as absolute ratings such as player engagement levels or classes such as low and high game intensity \cite{booth2024people,makantasis2021privileged}.

In contrast to the traditional way of treating annotations as absolute ratings, here we view player modelling as an ordinal learning paradigm aiming to maximize the reliability and validity of our predictive models \cite{yannakakis2018ordinal,yannakakis2017ordinal}. We task LLMs to label \emph{increases} or \emph{decreases} of engagement across frames of a game instead of asking them to provide ratings of engagement per frame. The ordinal representation of subjective notions such as engagement is supported both by theories of human psychology and cognition \cite{helson1964current,solomon1974opponent} and by a growing body of research in neuroscience \cite{damasio1996somatic} and affective computing \cite{yannakakis2015grounding,lotfian2016practical,yannakakis2018ordinal,melhart2020study,melhart2021towards,barthet2024gamevibe} among other disciplines. Importantly, we employ LLMs and we test their ability to model game engagement as viewed through gameplay videos. 


\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{graphics/Pagan.png}
\label{fig:pagan}
\caption{Example clip from \emph{GameVibe} showcasing the annotation interface using PAGAN and the RankTrace annotation tool for collecting unbounded, time continuous signals in real-time.}
\end{figure}

\section{The GameVibe Corpus}\label{sec:data}

This section gives a general overview of the \emph{GameVibe} corpus used throughout all experiments presented in this paper followed by an outline of the preprocessing approach we adopted for the engagement labels in this study. While the dataset is introduced thoroughly in \cite{barthet2024gamevibe} in this section we highlight the main aspects of the dataset that are relevant to our experiments here.

\begin{table}[!tb] 
    \caption{Core properties of the original Gamevibe corpus and the processed version (GameVibe-LLM) used in this study}
    \centering
    \begin{tabular}{|l|l|l|} 
        \hline
        Properties & GameVibe & GameVibe-LLM \\ \hline\hline
        Annotators & 20 & 20 \\ \hline
        Number of videos & 120 videos & 80 videos \\ \hline
        Video database size & 120 minutes & 80 minutes \\\hline
        Number of games & 30 games & 20 games \\ \hline
        Gameplay video duration & 1 minute each & 1 minute each\\ \hline
        Annotation  type & Interval signal & Discrete ordinal \\ \hline
        Modalities & Visual, audio & Visual \\ \hline
        %\hline
    \end{tabular}
    \label{tab:summary}
\end{table}

\subsection{Corpus Overview}
The \emph{GameVibe} corpus \cite{barthet2024gamevibe} consists of a set of 120 audiovisual clips and human annotations for engagement as viewers of first-person shooter games. This corpus presents a significant challenge for affect modelling research as its stimuli encompass a wide variety of graphical styles (e.g. photorealistic, retro) and game modes (e.g. deathmatch, battle royale). Table \ref{tab:summary} contains a basic summary of the properties of this corpus and processed version we use for this study. 

\emph{GameVibe} is organized into 4 sessions of 30 unique video clips of 1 minute each, with each video in a session annotated by the same set of 5  human annotators. The video clips were selected to contain a maximum of 15 seconds of non-gameplay content such as pause menus and cut scenes, and were sampled at 30 hertz with a resolution of $1280\times720$ for modern titles and $541\times650$ for older titles. Annotations were collected using the PAGAN annotation platform \cite{melhart2019pagan} and the RankTrace annotation tool \cite{lopes2017ranktrace} (see Fig. \ref{fig:pagan}), with the videos presented to participants in random order to minimize habituation and ordering effects. In RankTrace, participants are exposed to stimuli and annotate in real-time by scrolling up or down on a mouse wheel in an unbounded manner to indicate increases and decreases of their labelled state, in this case viewer engagement. Participants of \emph{GameVibe} were given the following definition of engagement prior to starting their annotation task: \begin{quote}
    \emph{A high level of engagement is associated with a feeling of tension, excitement, and readiness. A low level of engagement is associated with boredom, low interest, and disassociation with the game.}
\end{quote}

After a qualitative analysis of the dataset, we select 20 games from the \emph{GameVibe} corpus to form \emph{GameVibe-LLM} (see Table \ref{tab:summary}). We discard 10 games that feature third-person segments, mix footage of menus and gameplay, have large mobile UI overlay, or include poor footage. We select one out of four sessions randomly for generating few-shot examples in the final experiments and we test performance on the remaining 3 sessions. To be able to fairly compare the performance of different setups, we exclude the selected session from the remaining of the experiments.

\subsection{Engagement Data Pre-Processing}\label{sec:methods:preprocessing}

Our data preprocessing method closely follows common practices in affective computing and methods introduced in previous studies with \emph{GameVibe} \cite{pinitas2024-dt}. Thus, each annotation trace was resampled into three-second non-overlapping time windows using simple averaging. The videos were sampled at a similar rate to align the stimuli to the engagement traces provided by the participants. These traces were then processed into discrete ordinal signals by comparing pairs of consecutive time windows to determine whether engagement increased (1), decreased (-1) or remained stable (0) between the two time windows. 

Based on preliminary experiments and findings from earlier studies \cite{pinitas2024across, pinitas2024-dt}, we solely focus on the data points with changes in engagement. We thus removed any data where engagement remained stable and fix the time-window between frames at $3$ seconds. We select frames from the first minute of gameplay and extract $20$ videos per session. We discard the first comparison in each session (frame 0 to frame 1) because the very first frame of the videos lack necessary context for the viewer to provide meaningful a rating. This means we have $18$ comparisons per video. We select $3$ sessions from each game and after removing uncertain evaluations, we end up with around $2,000$ comparisons (around $33$ per video).

\section{Methodology}\label{sec:methods}

In this section we detail our chosen algorithms and the different prompting strategies we employ throughout our experiments. In the presented studies we evaluate the capacity of LLMs to correctly evaluate changes of engagement in gameplay videos. In particular we picked \emph{LLaVA} and \emph{GPT-4o} as our base LLMs under investigation (see Section \ref{sec:methods:algorithms}). In all reported experiments the downstream task of the employed LLM is to label a change in engagement (\emph{increase} or \emph{decrease}) given two consecutive frames of a video. We evaluate the algorithm's performance against the human labelled engagement data of \emph{GameVibe} that we treat as our ground truth. 

To explore how different experimental setups affect LLM engagement predictability, we ran experiments both with \emph{Multimodal} and \emph{Text Input}. Figure~\ref{fig:overview} illustrates the overall strategy and the different experimental setting employed. In the \emph{Multimodal Input} setting, the input for the algorithm is one or two images accompanied by a text-prompt describing the task. We detail the format of the multimodal input in Section \ref{sec:methods:visual}. In the \emph{Text Input} setting, instead, we provide text-based descriptions of two video frames as part of the text prompt. We describe the format of the text input in Section \ref{sec:methods:text}. Finally, we also study few-shot prompting, using multimodal input and we detail this process in Section \ref{sec:methods:prompts} along with our general prompting strategy.

\subsection{Employed LLMs}\label{sec:methods:algorithms}

As mentioned earlier, we employ the \emph{Large Language and Vision Assistant} (LLaVA) \cite{liu2024visual} and the \emph{Generative Pre-trained Transformer} (GPT) models for all reported experiments. This section outlines the reasons we select these two LMMs and details the specific algorithmic properties we used for each model. 

\subsubsection{LLaVA}

\emph{LLaVA} \cite{liu2023improvedllava, liu2024visual} is an ensemble model connecting a vision encoder with an LLM. \emph{LLaVA} uses \emph{Contrastive Language–Image Pre-training} (CLIP) \cite{radford2021learning} as a vision encoder and \emph{Vicuna} \cite{chiang2023vicuna} as a language decoder. To train \emph{LLaVA}, Liu et al. leveraged \emph{GPT4} to generate data on instruction following examples and trained their framework end-to-end to fuse vision and language input. The result is a robust model which is able to output text-descriptions and solve reasoning tasks based on image and text prompts combined. We have selected \emph{LLaVA} because a) it is an open-source model with multimodal capabilities; and b) it is easily deployed in local environments. We run experiments with the $7$ billion (7b), $13$ billion (13b), and $34$ billion (34b) parameter version of the algorithm using the \emph{Ollama} API\footnote{\url{https://ollama.com/}}.

\subsubsection{GPT-4o}

\emph{GPT4} is, at the time of writing, the most recent of a series of \emph{Generative Pre-trained Transformer} (GPT) models developed by \emph{OpenAI}. \emph{GPT4} is a closed source model. While a technical report about \emph{GPT4} has been published \cite{openai2023gpt4}, the exact architecture and training data is unknown. What is known is that \emph{GPT4} uses a transformer architecture for both vision and language tasks, relies on \emph{reinforcement learning from human feedback} and makes use of \emph{rule-based reward models} based on hidden policy models and human-written rubrics to steer the algorithm in a direction that is considered ``safe`` by \emph{OpenAI}. In this paper we use the \emph{GPT-4o (Omni) 2024-08-06} model variant. At the time of writing this is considered the flagship model of \emph{OpenAI}. Unlike previous iterations, \emph{GPT-4o} is trained end-to-end to incorporate text, audio, image, and video in both its input and output space \cite{openai2024gpt4o}. We have selected this model because it is one of the most popular \cite{sergeyuk2025using}, state-of-art, closed-source LLMs as an alternative to the open-source \emph{LLaVA}. We leverage the Open AI API\footnote{\url{https://platform.openai.com/}} for all reported experiments with \emph{GPT-4o}. 

\subsection{Multimodal Input} \label{sec:methods:visual}

In our experiments with \emph{Multimodal Input}, we feed the models with both visual input and a corresponding text prompt. To provide the visual input we first extract single frames from \emph{GameVibe} videos at a given interval. Then each frame is cropped to a square and downscaled to a fixed size. Particularly, in our experiments using one image we downscale our images to $336\times336$ pixels to be able to achieve the highest resolution input possible when combining two images in LLaVA models.\footnote{LLaVA models support $672\times672$, $336\times1344$, $1344\times336$ resolutions.\\More information: \url{https://ollama.com/library/llava}} In our early experiments with \emph{Multimodal Input}, we use a single image as the model's input due to a limitation of the \emph{LLaVA} models, which can only consider one image at a time. To circumvent this limitation we \emph{stitch} the two video frames together vertically (i.e. a top and a bottom image), leaving a white band of $50$ pixels between them. We call this experimental setting \emph{Multimodal Input - 1 Image (Stitched)}. This type of image stitching performs well on \emph{LLaVA} models compared to other approaches---such as concatenating the visual tokens \cite{meng2024mmiu}. For consistency we follow the same processing method with our \emph{GPT-4o} models when it comes to experiments using a single image. We show an example of this prompting strategy and the output it produces in the Appendix (see Fig.~\ref{annex:multimodal}).

In experiments involving few-shot prompting, we use two separate images per prompt. This experimental setting, named \emph{Multimodal Input - 2 Images}, is only applicable to \emph{GPT-4o}. This choice is partly informed by the aforementioned technical limitation of \emph{LLaVA} since the few-shot experiments require multiple prompts with multiple images to be chained together. Since these experiments run exclusively on \emph{GPT-4o} models we downscale images to $512\times512$ pixels in an effort to exploit the larger input space of \emph{GPT} vision models\footnote{GPT vision models process images in $512\times512$ pixel tiles with a maximum image size of $2048\times768$.\\More information: \url{https://platform.openai.com/docs/guides/vision}}.

\subsection{Text Input} \label{sec:methods:text}

In our experiments using \emph{Text Input}, we feed the models with text descriptions of two video frames as part of the prompt. We obtain these descriptions using the same LLM we use to generate the engagement evaluation. Similarly to the \emph{Multimodal Input - 1 Image} setup, we downsample the obtained video frames to $336\times336$ pixels. Contrary to the previous setup, here we use these images one-by-one and generate descriptions in two different ways. We call these \emph{Basic} and \emph{Advanced Descriptions} based on the amount of context given to the model. For the former, we instruct the model to give a brief description, capturing only essential details without subjective commentary based on the setting and layout, enemies, and player action. For obtaining \emph{Advanced Descriptions}, we instruct the model to also take player engagement into account and generate a description that captures how it might engage the player or viewer. We illustrate this process in the Appendix; see Figs.~\ref{annex:text-basic} and \ref{annex:text-advanced} respectively. For the engagement prediction task, we feed these descriptions to the models in pairs as part of their text prompt. We show an example of this prompting strategy and the output it produces in the Appendix (see Fig.~\ref{annex:text-prediction}).

\subsection{Prompting Methods}\label{sec:methods:prompts}

All prompting strategies we use for the engagement evaluation task follow a \emph{Chain-of-Thought} (CoT) paradigm \cite{wei2022chain,zhang2023multimodal}. We ask the models to provide a \emph{comparison} between the given input frames, \emph{reasoning} its analysis of engagement, and finally offering a one-word \emph{decision} (i.e., engagement increase or decrease). Additionally, for the \emph{Multimodal Input} experiments we also generate a \emph{description} of the visual input before the \emph{comparison}. In the \emph{Multimodal Input - 1 Image} and \emph{Text Input} experiments the decision is to pick the most engaging frame (see Fig.~\ref{annex:multimodal} in the Appendix). In the \emph{Multimodal Input - 2 Images} experiments, instead, we refine the prompt and ask the model to explicitly output \emph{increasing} and \emph{decreasing} labels. We instruct the model to output its answers in a \emph{JSON} format, which we parse and extract the final \emph{decision} from; see also Fig.~\ref{annex:oneshot} in the Appendix.

For our few-shot experiments in the \emph{Multimodal Input - 2 Images} setup, we generate artificial \emph{reasoning} samples for a positive and negative example for each task. We use the same CoT prompt for this process as for the one-shot \emph{Multimodal Input - 2 Images} experiments. We will call this prompt ``CoT prompt'' in the remainder of this section. To generate these samples we take the following steps (see also \emph{Multimodal Input - 2 Images, Few-Shot} in the middle of Fig~\ref{fig:overview}):
\begin{enumerate}
\item We take a random example from the same game as presented in the task from an unseen session. 
\item We use the same CoT prompt as for the final engagement evaluation task but modify the prompt leaving only the correct option for the \emph{decision}. 
\item We amend the prompt with the correct evaluation based on the ground truth (see \emph{Ground Truth Engagement} on Fig.~\ref{fig:overview}).
\item We add a \emph{Reasoning Prompt} to instruct the model to provide \emph{reasoning} for the ground truth evaluation.
\end{enumerate}
% David: This now follows the main Figure much better
 
By removing incorrect options but using the same CoT prompt when generating positive and negative examples, we ensure that the algorithm's output is formatted the same way as for the downstream task, including the \emph{description}, \emph{comparison}, \emph{reasoning}, and \emph{decision}. We use these outputs to construct an artificial history of positive and negative examples, which are added to the final prompt for the engagement evaluation task. For this final step we provide the CoT prompt with the example images as a question, and the example output as an answer; then finally we provide a set of unseen images with the CoT prompt and instruct the LLM to evaluate engagement the same way it would for a one-shot experiment. Figures~\ref{annex:fewshot-examples} and \ref{annex:fewshot-prediction} in the Appendix detail the process starting from example generation all the way to engagement prediction.

\section{Results}\label{sec:results}

\begin{figure}[t]
\centering
\includegraphics[width=1\linewidth]{graphics/delta_time.pdf}
\caption{Application of the \emph{temporal shift} ($\Delta t$) hyperparameter to the ground truth. The top red bar (\emph{Vision Input}) shows an example of individual frames extracted from the gameplay video at a 3-second interval. The bottom green bar (\emph{Ground Truth}) shows a $\Delta t$ of $-2$ seconds, which means that each window aggregates information $2$ seconds before and $1$ second after the corresponding video frame.}\label{fig:delta_time}
\end{figure}

\begin{figure*}[!ht]
\centering
\includegraphics[width=1\linewidth]{graphics/results/sensitivity.PDF}
\caption{Sensitivity analysis across hyperparameters $\Delta t$ and $\theta$. The table presents $\Delta A$ values (relative gain in accuracy). $\Delta t$ is the relative shift of the time window to the frame, and $\theta$ is the binary threshold for the split criterion (i.e., increasing or decreasing engagement). The last column shows average $\Delta A$ across all games.}\label{fig:groundtruth_acc}
\end{figure*}

This section presents the main results of the experiments performed as follows. In Section 
\ref{sec:results:setup} we outline the setup of the experiments reported and in Section \ref{sec:results:prelim} we discuss our exploratory findings. In Section \ref{sec:results:modalities} we examine LLM performance across different input modalities for the engagement evaluation task. Section \ref{sec:results:fewshot} presents the results of our few-shot prompting experiments, and finally Section \ref{sec:results:analysis} takes qualitative lens in our attempt to explain and justify our core findings.

\subsection{Experimental Setup}
\label{sec:results:setup}

We compare the engagement labels generated by LLMs to an engagement ground truth calculated from 3-second time windows of \emph{GameVibe} annotation traces as outlined in Section \ref{sec:methods:preprocessing}. We introduce and vary two hyperparameters in this process: 
\begin{enumerate}
    \item A temporal shift compared to the observed video frame ($\Delta t$). This is similar to what the literature often refers to as \emph{input lag} \cite{melhart2021towards}. While this correction is generally used to account for reaction time, here we use it to control the temporal difference between the observed frames and the ground truth (see Fig. \ref{fig:delta_time}).
    \item A preference threshold ($\theta$), taking values between $0$ and $1$, that determines whether a difference between the ground truth value of two consecutive time windows is considered a \emph{change} (increase or decrease) in engagement; e.g. $\theta=0.05$ considers windows which have a difference of more than $5\%$ when evaluating engagement change.
\end{enumerate}

We formulate the downstream task of LLMs as binary classification, and ask our models to predict the \emph{increase} or \emph{decrease} of perceived engagement between two frames of consecutive time-windows. We discard predictions which could not be interpreted when either for the following occurs: a) the algorithm predicts no change in engagement, b) the LLM generates outputs we could not parse, or c) the model is not able to provide an output.\footnote{Good examples of these cases were \emph{LLaVA} models providing verbose answers instead of picking one of the provided options for their final answer, e.g.: ``It depends on personal preference. If one prefers an immersive experience similar to the player's perspective, the upper picture might be considered more engaging. On the other hand, if one values breadth and variety in game views, the lower image could be seen as a more engaging alternative.'' instead of simply ``top'' or ``bottom'' when picking which image is more engaging; and \emph{GPT-4o} refusing to provide analysis, e.g.: ``I'm unable to analyse the content of these images. If you can describe the frames, I can help evaluate the change in engagement''.} 

We define our baseline performance as equivalent to always predicting the majority class of a given game session. It is worth noting that the baseline differs widely across games and varies to a lesser degree based on the hyperparameter values selected; the lowest baseline across game sessions, on average, is $52\%$ in \emph{Far Cry} (2004), \emph{Blitz Brigade} (2013) and \emph{Fortnite} (2018) whereas the highest is $78\%$ in \emph{Counter Strike 1.6} (2003), followed by \emph{Heretic} (1994) with $73\%$, and \emph{Corridor 7} (1994) with $70\%$. Given this level of discrepancy among baselines, and to be able to meaningfully compare performance different instructions \cite{ajith2023instructeval, yang2024mm}, we report accuracy gain over the corresponding baseline---instead of the accuracy values per se---as follows:

\begin{equation}\label{eq:relative_improvement}
    \Delta A = \frac{A_{LLM}-A_{b}}{A_{b}}
\end{equation}

\noindent where $\Delta A$ is the relative gain in accuracy; $A_{LLM}$ is the accuracy of the given LLM; and $A_{b}$ is the baseline accuracy given by the majority class. We use the $\Delta A$ measure of performance in all reported experiments in this paper. 

\subsection{Sensitivity Analysis} \label{sec:results:prelim}

We experiment with the \emph{temporal shift} $\Delta t \in \{0, -0.5, -1, -1.5, -2, -2.5, -3\}$ and \emph{preference threshold} $\theta \in \{0, 0.01, 0.05, 0.1\}$ parameters---introduced in the previous section---using the $7$, $13$, and $34$ billion parameter version of \emph{LLaVA}, and \emph{GPT-4o}. The combinations of these parameters, however, result in $112$ experimental setups for each game. Due to space considerations we only present the best performing subset of these hyperparameters ($\Delta t \in \{-1, -2\}$ and $\theta \in \{0.01, 0.05\}$). 
% 
We run these experiments with the \emph{Multimodal Input - 1 Image} strategy as described in Section \ref{sec:methods:visual}. We chose this setup for the initial parameter tuning because this is the most straightforward setup involving only one image and one text prompt.

Figure~\ref{fig:groundtruth_acc} presents the $\Delta A$ performance across two $\Delta t$ and $\theta$ values. We can observe that larger $\Delta t$ and $\theta$ values tend to yield higher performance; it also appears that the model size and architecture have a higher impact on $\Delta A$. While \emph{LLaVA-7b} and \emph{LLaVA-34b} consistently perform significantly worse than the baseline---measured with \emph{Student's t-Test} at significance level $\alpha < 0.05$ corrected with the \emph{Bonferroni} method, accounting for repeated measurements---\emph{GPT-4o} shows performance comparable to the baseline. Interestingly, \emph{LLaVA-13b} outperforms the larger \emph{LLaVA} model and is not significantly worse than the baseline performance.

The best performing hyperparameter set is $\Delta t=-2$ and $\theta=0.05$ both in terms of average and single-game performance.
% 
The best performances are as follows: \emph{LLaVa-34b} improves the baseline by $37\%$ on \emph{Blitz Brigade}; and \emph{GPT-4o} by $33\%$, $29\%$, and $26\%$ on \emph{Doom} (1993), \emph{Wolfram} (2012), and \emph{Blitz Brigade}, respectively. 
% 
Interestingly, we can see comparable performances with other models and configurations on single games. The most indicative of these is the \emph{LLaVA-7b} model reaching $37\%$ higher performance than the baseline on \emph{Wolfram} with $\Delta t=-1$ and $\theta=0.01$. The average performance of the aforementioned setup, however, is lower that the performance of models tuned to $\Delta t=-2$ and $\theta=0.05$. This indicates that the models are sensitive to the games themselves and can't perform uniformly well across the whole dataset.
% 
Two striking examples are \emph{LLaVA-13b}, consistently outperforming every other model on \emph{Void Bastards} (2019) and \emph{LLaVA-7b}, consistently underperforming on \emph{CS:GO - Dust2} (2012). 

Some games are easier to predict than others, regardless of experimental setup. For example, \emph{Wolfram}, \emph{Blitz Brigade}, and \emph{PUBG} are constantly listed within the top performing games in terms of $\Delta A$, whereas \emph{Heretic}, \emph{Counter Strike 1.6}, \emph{Overwatch 2} (2022), and \emph{HROT} (2023) yield among the lowest $\Delta A$. 
% 
It is important to note that games where engagement changes are predicted well by LLMs tend to have lower baselines (i.e. \emph{Wolfram}: $57\%$; \emph{Blitz Brigade} $52\%$; \emph{PUBG}: $60\%$) whereas games where engagement is not predicted as well tend to have high baselines (i.e. \emph{Heretic}: $73\%$; \emph{Counter Strike 1.6}: $78\%$; \emph{Overwatch 2}: $69\%$; and \emph{HROT}: $61\%$). This indicates that engagement prediction is easier in game videos that feature more dynamic gameplay footage and a more uniform distribution of increasing vs. decreasing engagement labels.

Considering the overall performance of LLM engagement prediction across games, we fix our parameters for processing the ground truth at $\Delta t=-2s$ and $\theta=0.05$ for the remaining experiments presented in this paper. As we observed high levels of performance across different LLM models, we continue our investigations experimenting with both \emph{LLaVA} and \emph{GPT-4o} models.

\subsection{Text-based Engagement Prediction}\label{sec:results:modalities}

\begin{figure*}[t]
\centering
\includegraphics[width=1\linewidth]{graphics/results/text.PDF} 
\caption{$\Delta A$ (relative gain in accuracy) values across games of models fed with \emph{Text Input} compared to the best LLMs using \emph{Multimodal Input - 1 Image} presented in Section~\ref{sec:results:prelim}. Experiments using \emph{Text Input} are based on text-descriptions of frames only. Experiments using \emph{Multimodal Input} use both images and text prompts. The rightmost column shows $\Delta A$ values averaged across all games.}\label{fig:results_text}
\end{figure*}

In this section we examine the impact of text-based vs. multimodal prompting strategies on LLM performance. While in the former case we provide solely a text prompt to the model, in the latter case we feed both a text prompt and a corresponding image. Because the performance of LLMs can be affected even by small prompt variations \cite{mao2022biases}, we experiment with both \emph{Basic} and \emph{Advanced} prompts. The prompting procedure for the text-based experiments are detailed in Section \ref{sec:methods:text}.
% 
Figure~\ref{fig:results_text} shows the $\Delta A$ performance of \emph{Text Input} experiments compared to the best \emph{Multimodal Input - 1 Image} models discussed in the previous section.
 
In this section our analysis focuses on the \emph{Text Input} compared to the \emph{Multimodal - 1 Image (Stitched)} results presented in the previous section. This focus on text allows us to compare the \emph{Text Input} method to a simple multimodal approach across different models. Our hypothesis is that the strategy of generating text-descriptions of frames first and then using these descriptions as part of the \emph{Text Input} will improve model performance, because it essentially encodes the images in terms of action and player involvement. We thus assume that using this type of \emph{Text Input} will present a better representation by discarding surface-level differences between frames and emphasising the structural differences.

Overall, we can note that \emph{LLaVA-34b} models perform significantly worse than the baseline across all modalities (\emph{Multimodal} and \emph{Text}) except when the text-only input is combined with \emph{Advanced Descriptions}, but the performance still remains on the lower end of the spectrum. \emph{LLaVA-13b} models yield performance values that are significantly below the baseline interdependently of the description setup. Finally, \emph{LLaVA-7b} underperforms significantly on the multimodal task.
% 
It is somewhat surprising that while the larger \emph{LLaVA} models generally perform better on multimodal tasks, the smallest model (\emph{7b}) marginally outperforms the other two larger models of the \emph{LLaVA} family when fed with text-only input. We hypothesise that this is due to the larger models' stronger tendency to fall into what Chochlakis et al. \cite{chochlakis2024strong} call ``gravity wells of knowledge priors''. This hypothesis is reinforced when we look at the best performing \emph{LLaVA} models of Fig. \ref{fig:results_text}. The better performing LLMs are usually fed with \emph{Basic} instead of \emph{Advanced Descriptions}. The added context seems to confuse the LLM or fails to orient the models to make accurate predictions. The same issue doesn't seem to affect the \emph{GPT-4o} model which performs consistently close to the baseline and better than the \emph{LLaVA} family overall. 
% 
While the \emph{GPT-4o} model performs marginally better on the text-input task using the \emph{Advanced Descriptions}, the biggest improvement can be observed with \emph{Basic Descriptions} on \emph{Doom} with a $39\%$ relative gain in accuracy.

With regards to the different prompting strategies we observe no significant difference in performance between \emph{Basic} and \emph{Advanced Descriptions} for \emph{Text Input}, among the models tested. While some prompting techniques appear to help certain models to perform well in certain games, there is no apparent overarching pattern we can analyse. It also seems that any performance outliers can mostly be explained through the particularities of the data and the chosen algorithm. 
% 
Some indicative examples of this observation are the games \emph{CS:GO - Dust2}, \emph{CS:GO - Office}, and \emph{Doom} where the discrepancy between the best and worst performing models is the largest. Conversely \emph{HROT}, \emph{Apex Legends}, and \emph{Medal of Honor 2010} have the least amount of performance variation across models and prompting strategies. It is worth noting that the models are only successful in predicting \emph{Apex Legends}---with \emph{GPT-4o} reaching $31\%$ $\Delta A$ using \emph{Text Input - Basic Description}. 
% 
In general, \emph{LLaVA} models appear to be more sensitive than \emph{GPT} models to the input modality and prompting strategy, often fluctuating in performance between different experimental setups. While we can observe a similar pattern with \emph{GPT} models there are particular games in which such models yield decent relative gains in accuracy across different experimental setups, such as \emph{Doom} ($25\%$-$39\%$), \emph{Wolfram} ($25\%$-$37\%$), and \emph{Apex Legends} (2019) ($6\%$-$31\%$).
% 
While in some cases \emph{LLaVA} models show comparable performance to \emph{GPT-4o} (e.g. \emph{LLaVA-7b} on \emph{Text Input - Basic Description} on \emph{Wolfram} and \emph{Void Bastards}---$23\%$ and $15\%$ respectively; and \emph{LLaVA-34b} outperforming \emph{GPT-4o} on \emph{Blitz Brigade} with \emph{Multimodal Input} at $37\%$), there is no game where any of these models perform consistently better than the baseline regardless of input modality and prompting strategy. 
% 
Once again the games where the models perform consistently worse than the baseline are \emph{Heretic} ($-34\%$ on average), \emph{Counter Strike 1.6} ($-32\%$ on average), and \emph{Overwatch 2} ($-26\%$ on average). In some extreme cases in both the \emph{Text Input} and \emph{Multimodal} experiments, models consistently predict the wrong label, leading to drastic drops in performance. Representative examples of this are \emph{LLaVA-7b} on \emph{CS:GO - Dust2} ($-60\%$); \emph{LLaVA-34b} on \emph{Counter Strike 1.6} ($-57\%$); and \emph{LLaVA-13b} on \emph{Heretic} ($-54\%$). As mentioned in the previous section, these games have higher baselines, pointing towards a less dynamic gameplay footage. 

We started this section with a hypothesis that using text-descriptions of frames would improve predictive capacity of LLMs compared against multimodal inputs. We believed this would be the case because the generation of text descriptions would act as a type of game-agnostic encoding, putting more emphasis on the layout and action of frames. The results presented here indicate that this is not the case. In general, obtained results show no significant differences between the \emph{Text Input} and \emph{Multimodal Input - 1 Image} setups. Generating text-descriptions first and using text-only input cannot provide a better encoding than simpler multimodal approaches for this task.
% 
The main reason behind this probably lies within how these models handle vision input. While \emph{LLaVA} relies on \emph{CLIP} for image encoding, \emph{GPT-4o} uses a custom multimodal end-to-end architecture. 
% 
Because both models were trained for to encode images and text into a shared embedding space \cite{radford2021learning} the extra ``image to text'' step is unnecessary.

\subsection{Multi-Image One-Shot and Few-Shot Prompting} \label{sec:results:fewshot}

\begin{figure*}[t]
\centering
\includegraphics[width=1\linewidth]{graphics/results/fewshot.PDF} 
\caption{$\Delta A$ values of \emph{GPT-4o} models across games and across different experimental settings. The bottom two rows depict the $\Delta A$ values of the \emph{Multimodal Input - 2 Image, One-Shot} and \emph{Few-Shot} experiments. The rightmost column shows $\Delta A$ values averaged across all games.}\label{fig:results_fewshot}
\end{figure*}

In this section we present experiments using \emph{Multimodal Input - 2 Image}, \emph{One-Shot} and \emph{Few-Shot} strategies (see Fig.~\ref{fig:overview} and Section~\ref{sec:methods} for more details on these approaches). 
% 
In these experiments we opt to employ the \emph{GPT-4o} model only; the reason for doing so is two-fold. First, \emph{GPT} models have been observed to be more consistent and perform better across all games in experiments presented in the previous sections. Second, models of the \emph{LLaVA} family are limited in how they can process images as input. As mentioned in Section \ref{sec:methods:visual} \emph{LLaVA} models can only take single images in their input space, while \emph{GPT-4o} uses a tile-based input tokenizer that is able to handle multiple images.

Figure~\ref{fig:results_fewshot} presents the results of our \emph{Multimodal Input - 2 Image}, \emph{One-Shot} and \emph{Few-Shot} experiments compared to the best overall \emph{Text Input} and \emph{Multimodal Input - 1 Image} results obtained using \emph{GPT-4o}. We can see that the best overall performance is achieved when using \emph{Few-Shot} prompting.
% 
While the relative improvement over the baseline is not significant across all games, there is a clear pattern of improvement compared to other models. Comparing results between Fig.~\ref{fig:results_text} and Fig.~\ref{fig:results_fewshot}, can see that \emph{GPT-4o} models significantly outperform \emph{LLaVA} models on several experimental setups. These setups include \emph{LLaVA-7b} on the \emph{Multimodal Input - 1 Image (Stitched)} task; \emph{LLaVA-13b} models on both \emph{Text Input} setups; and \emph{LLaVA-34b} on the \emph{Text Input - Basic Description} and \emph{Multimodal Input - 1 Image (Stitched)} tasks. 
% 
While there is no significant difference between one-shot and few-shot prompting, the latter strategy improves the performance in $13$ out of $20$ experimental settings. We note only $6$ out of $20$ settings where the introduction of few-shot prompting decreased the performance. Our findings are aligned with results reported in the literature \cite{wei2022chain, min2022rethinking, zhang2023multimodal} suggesting that a few-show, multimodal, chain-of-thought prompting method can significantly improve LLM performance. However even with this performance boost, the observed models barely surpass the majority baseline, on average, across games. 

Looking at the best and worst performances of \emph{GPT-4o} across games we observe a familiar pattern. Once again, the games whose engagement is easier to predict are \emph{Wolfram} ($38\%$), \emph{Apex Legends} ($27\%$), and \emph{Doom} ($23\%$) when we look at the average performance across both the \emph{GPT-4o One-Shot} and \emph{Few-Shot} settings.
% 
Similarly, the games where the LLM models performed worst on average are \emph{Counter Strike 1.6} ($-40\%$), \emph{Corridor 7} ($-25\%$), and \emph{Heretic} ($-17\%$). These findings are in line with our previous experiments.

\subsection{Qualitative Analysis}\label{sec:results:analysis}

In this section we outline the reasons for the observed poor performance of the tested LLMs and analyse why certain games are easier to predict.
% 
For our analysis we are looking at the highest performing model, the \emph{GPT-4o} with \emph{Multimodal Input - 2 Images} using \emph{Few-Shot} prompting. 
% 
Employing this model we list $5$ games where the $\Delta A$ exceeds $25\%$: \emph{Doom}, \emph{Wolfram}, \emph{PlayerUnknown's Battlegrounds (PUBG)} (2018), \emph{Apex Legends}, and \emph{Borderlands 3} (2019). Conversely, the five games, where the performance was well-below the baseline are as follows: \emph{Corridor 7}, \emph{Heretic}, \emph{Counter Strike 1.6}, \emph{Medal of Honor} (2010), and \emph{HROT}; see Fig.~\ref{fig:top_games}.

\begin{figure}[t]
\centering
\includegraphics[width=1\linewidth]{graphics/top_games.png}
\caption{The 5 best and worst performing games in terms of $\Delta A$ (relative gain in accuracy) using \emph{Multimodal Input - 2 Images} with \emph{GPT-4o Few-Shot} prompting. Best games from left to right: a) \emph{Doom}, b) \emph{Wolfram}, c) \emph{PlayerUnknown's Battlegrounds (PUBG)}, d) \emph{Apex Legends}, and e) \emph{Borderlands 3}. Worst games from left to right: f) \emph{Corridor 7}, g) \emph{Heretic}, h) \emph{Counter Strike 1.6}, i) \emph{Medal of Honor 2010}, j) \emph{HROT}}\label{fig:top_games}
\end{figure}

A qualitative analysis of the games where LLMs perform best (vs. those where they perform worst) reveals some possible underlying reasons that could influence these models. The five games where LLMs perform best are fast paced, with short bursts of action separated by similarly short navigation sequences. The game scenes are well-lit or stylized in a way that is easy to read. In contrast, the five games where LLMs fail to assign engagement labels feature repetitive sections of navigation with limited gameplaying action such as shooting, reloading, collecting items, or dodging fire. These games also tend to feature dark backgrounds and enemies with silhouettes that are difficult to distinguish, or they take place in drab environments where the ground, background, and often non-player characters blend together.
% 
\begin{figure}[t]
\centering
\includegraphics[width=1\linewidth]{graphics/counter_strike.png}
\caption{Similar frames from \emph{Counter Strike }variants: \emph{Counter Strike 1.6}, \emph{CS:GO - Dust2}, and \emph{CS:GO - Office} (left to right)}\label{fig:counter_strike}
\end{figure}
% 
A representative example that highlights these performance differences are the \emph{Counter Strike} game variants existent in the dataset; see Fig.~\ref{fig:counter_strike}. Compared to the best performance of the multimodal few-shot \emph{GPT-4o} on \emph{Counter Strike 1.6} ($33\%$ worse than baseline), the same model on \emph{CS:GO - Dust2} has a performance comparable to baseline levels. Even though these two games use essentially the same level, the visuals of \emph{CS:GO - Dust2} are much clearer; in \emph{Counter Strike 1.6} the background and foreground are harder to separate visually.
% 
In \emph{CS:GO - Office}---where the visuals are arguably even more readable---the model showcases much higher predictive capacity (i.e. $19\%$ higher than the baseline).

Another way to explain the fluctuation in LLM performance is the familiarity of the model with the games per se. We observe that more popular games
% 
(such as \emph{Counter Strike}, \emph{Apex Legends}, and \emph{PUBG}, with a peak viewership\footnote{Peak viewership refers to the historically highest number of concurrent viewers watching a stream. It is indicative of the maximum audience size.} of $1,914,861$, $674,070$, and $597,663$, respectively on Twitch\footnote{\label{twitchtracker}Numbers retrieved from \url{https://twitchtracker.com/}, January 2025.} yield generally better engagement predictions compared to less popular games 
% 
(such as \emph{HROT}, \emph{Heretic}, and \emph{Corridor 7}, with a peak viewership of $24,721$, $2,280$, and $195$ on Twitch\footref{twitchtracker}),
% 
although we should be careful with naive over-generalizations from these findings. For one, \emph{Counter Strike 1.6} is a variant of a very popular game with a peak viewership of $125,378$ in itself, but the models struggle with correctly evaluating the change in engagement---at least in the \emph{GameVibe} dataset. While it is possible that the training data of \emph{GPT-4o} contains images from more popular games, attempting to verify this by reconstructing parts of the \emph{GPT-4o} training data is out of scope of this paper.

\section{Discussion}\label{sec:discussion}

The evaluation experiments presented in this paper are the first of its kind for LLM-based engagement prediction in games. While collectively we tried $2,440$ combinations of experimental settings---varying the LLM model type, model size, prompting strategy, input type, and ground truth processing---there are still many aspects that we did not explore in this initial study. We argue, however, that we set out to lay ground works for future research by approaching the problem of automating gameplay annotation in a relatively straightforward way. While, for instance, we experimented with several out-of-box LLM models and prompting strategies, we kept the granularity of the vision input constant which potentially poses a core limitation to this initial study. Since we sample the videos in question at a 3-second interval, the model loses a lot of information between these frames. Although we briefly experimented with different time intervals (i.e. between 1 and 5 seconds), simply increasing the sampling rate did not yield a performance increase. It is likely, however, that by either providing more frames per query or using video input directly would lead to a significant performance improvement that remains to be tested in future studies. These investigations were purposefully left out of the scope of the current study, mainly because (at the time of writing) there were no widely available video models which could have have fit into the experimental protocol presented here.

While video input could feed more information to the LLM, the context of the query could also be augmented, and then provided to the LLM, thereby improving its predictive capacity. By implementing a memory mechanism \cite{zhang2024survey}, for instance, we could potentially store and recall the temporal context of the play session, providing richer information to the model. Similarly, we could provide more context on the necessary domain knowledge for the task by implementing \emph{retrieval-augmented generation} \cite{lewis2020retrieval}, where we could feed more information on the game, play session, or downstream task similarly to how we have been providing positive and negative examples to the model in our few-shot examples. We plan to pursue these avenues in our future studies in our effort to further investigate how more contextual information impacts the performance of LLMs towards fully autonomous engagement annotation.

Generating subjective labels is a relatively open field with a lot of unanswered questions. Naturally, the exploration should be extended into other datasets, involving games---also beyond first-person shooters---and other media as well. While this study focuses on engagement, there are other subjective aspects of both player and viewer experience that could be evaluated further. A natural step forward would be to make use of a diverse set of affective corpora, focusing, for instance, on affect prediction across videogame datasets \cite{melhart2022again}, but also architectural spaces \cite{xylakis2024affect} and movie corpora \cite{girard2023dynamos}.
% 
As discussed above, the current evaluation of LLMs---even though it was multimodal---considered a predetermined number of modalities: text and images. As we move forward and more multimodal architectures become widely adopted, the research into utilizing LLMs for autonomous affect annotation could encompass different modalities from images, through video, to audio. When it comes to interactive mediums such as games, user behavioural data could also be included \cite{ravsajski2024behave} providing a richer context to the models.

\section{Conclusion}\label{sec:conclusions}

This paper explored a novel application of LLMs for autonomously annotating the continuous experience of viewers when consuming videos of first-person shooter video games from the \emph{GameVibe} corpus. We conducted an in-depth analysis comparing multiple foundation models, including Open-AI's \emph{GPT} and the \emph{LLaVa} model families, and evaluated their performance across different input modalities (i.e. multimodal, text-based) and prompting strategies (i.e. one-shot, few-shot). Our findings confirm that model size and prompting strategy have a critical impact on model performance. The LLMs presented here demonstrate promising capabilities on certain game elicitors---although their overall performance only marginally surpasses the baseline. Perhaps unsurprisingly, the games where LLMs are successful predicting the continuos change in engagement are popular games with easy-to-read graphical styles and concise gameplay. The gap in performance on these games compared to more challenging elicitors shows that while LLMs have potential, there is still a long road ahead towards automated continuous affect labelling using using these type of foundation models.

As LLMs continue to scale and evolve, we believe their ability to capture subjective experiences will drastically improve---especially when incorporating richer multimodal inputs such as video, audio, and physiological signals. The annotation capacities of such LLM-based foundation affect models extends well beyond the domain of games to video-based general affect modelling, and human-computer interaction at large. Based on the results presented here, we believe future work could leverage LLMs as flexible and scalable annotators in a wide variety of dynamic and real-world settings.

% if have a single appendix:
% \appendix[Proof of the Zonklar Equations]
% or
\newpage
\appendix  % for no appendix heading
\begin{figure}[ht]
\centering
\includegraphics[width=1\linewidth]{graphics/appendix/multimodal1.pdf}
\caption{Multimodal Input - 1-Image (Stitched) input and output example using \emph{GPT-4o}.}\label{annex:multimodal}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=1\linewidth]{graphics/appendix/text-description-basic.pdf}
\caption{Text Input - Basic Description image-to-text description generation using \emph{GPT-4o}.}\label{annex:text-basic}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=1\linewidth]{graphics/appendix/text-description-complex.pdf}
\caption{Text Input - Advanced Description image-to-text description generation using \emph{GPT-4o}.}\label{annex:text-advanced}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=1\linewidth]{graphics/appendix/text-input-final.pdf}
\caption{Text Input - Engagement Prediction using \emph{GPT-4o}. $<$description A$>$ and $<$description B$>$ are replaced with generated text descriptions.}\label{annex:text-prediction}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=1\linewidth]{graphics/appendix/multimodal2.pdf}
\caption{Multimodal Input - 2-Image One-Shot input and output example using \emph{GPT-4o}.}\label{annex:oneshot}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=1\linewidth]{graphics/appendix/example.pdf}
\caption{Multimodal Input - 2-Image Few-Shot Example Reasoning Generation input and output example using \emph{GPT-4o}. The generated output is used as an artificial example in the Few-Shot experiments.}\label{annex:fewshot-examples}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=1\linewidth]{graphics/appendix/fewshot.pdf}
\caption{Multimodal Input - 2-Image Few-Shot Engagement Prediction input and output example using \emph{GPT-4o}. The Base and Task Prompts are identical to the ones described in Figure~\ref{annex:oneshot}. Example Outputs are generated as shown in Figure~\ref{annex:fewshot-examples}.}\label{annex:fewshot-prediction}
\end{figure}


% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%


% \appendices
% \section{Proof of the First Zonklar Equation}
% Appendix one text goes here.

% % you can choose not to have a title for an appendix
% % if you want by leaving the argument blank
% \section{}
% Appendix two text goes here.


% use section* for acknowledgment
% \section*{Acknowledgment}
% The authors would like to thank...


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi

\clearpage

% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section
% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
\bibliography{bibtex/paper}

% \end{thebibliography}

% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

% REMOVED FOR ANONYM
\begin{IEEEbiography}[{\includegraphics[width=0.9in,clip,trim={40px 0 40px 0},keepaspectratio]{graphics/authors/melhart.jpg}}]
{David Melhart} is a Postdoctoral Fellow at the Institute of Digital Games, University of Malta. He received his Ph.D. from the University of Malta, focusing on user research, player modeling, and affective computing in games. His research explores automated annotation, user modelling in videogames, and ethical AI. He has contributed to various academic and industry events, serving as Communication Chair (\emph{FDG} 2020, 2022; \emph{DiGRA} 2025), Workshop and Panels Chair (\emph{FDG} 2023), and Workshop Organizer (\emph{CHI-Play Workshop on Ethics and Transparency in Game Data} 2024). He is one of the main organizers of the \emph{International Summer School on Artificial Intelligence and Games} (2018–2025). He has been a Review Editor of \emph{Frontiers in Human-Media Interaction}, Guest Associate Editor of the \emph{Frontiers in Virtual Reality and Human Behaviour} also serves as an \emph{Editorial Assistant} for the \emph{IEEE Transactions on Games}.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=0.9in,clip,trim={170px 0 170px 0},keepaspectratio]{graphics/authors/barthet.jpg}}]
{Matthew Barthet}
received a bachelors of science degree in computer science, and a masters of science degree in digital games from the University of Malta in 2019 and 2021, respectively. He is currently a PhD candidate at the University of Malta researching training reinforcement learning agents in affective computing applications. His other research interests include procedural content generation, game artificial intelligence, and computational creativity. 
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=0.9in,clip,trim={80px 60px 70px 20px},keepaspectratio]{graphics/authors/yannakis.png}}]
{Georgios N. Yannakakis} is a Professor and Director of the Institute of Digital Games, University of Malta (UM) and a co-founder of modl.ai. He received the PhD degree in Informatics from the University of Edinburgh in 2006. Prior to joining UM, in 2012 he was an Associate Professor at the Center for Computer Games Research at the IT University of Copenhagen. He does research at the crossroads of artificial intelligence, affective computing, games and computational creativity. He has published more than 300 papers in the aforementioned fields and his work has been cited broadly. His research has been supported by numerous national and European grants (including a Marie Skłodowska-Curie Fellowship) and has appeared in \emph{Science Magazine} and \emph{New Scientist} among other venues. He is currently the Editor in Chief of the \emph{IEEE Transactions on Games}, an Associate Editor of the \emph{IEEE Transactions on Evolutionary Computation}, and used to be Associate Editor of the \emph{IEEE Transactions on Affective Computing} and the {IEEE Transactions on Computational Intelligence and AI in Games} journals. He has been the General Chair of key conferences in the area of game artificial intelligence (IEEE CIG 2010) and games research (FDG 2013, 2020). Among the several rewards he has received for his papers he is the recipient of the \emph{IEEE Transactions on Affective Computing Most Influential Paper Award} and the \emph{IEEE Transactions on Games Outstanding Paper Award}. Georgios is and IEEE Fellow.
\end{IEEEbiography}

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}

% that's all folks
\end{document}
