\section{Related Work}
\label{sec:related work}

\subsection{Jailbreak Attacks on LLMs}
Initially, researchers reveal that adversaries could launch attacks by manually constructing out-of-distribution (OOD) samples~\citep{DBLP:conf/emnlp/LiGFXHMS23, DBLP:conf/ccs/ShenC0SZ24}.
Building on these observations, several white-box attack methods are proposed~\citep{ARCA, DBLP:conf/icml/JonesDRS23, gao2024denial}. 
Compared to white-box attacks, black-box attacks assume that adversaries   adjust their prompt strategies only based on the model's responses~\citep{PsySafe, DBLP:journals/corr/abs-2311-03348, DBLP:conf/iclr/LiuXCX24, DBLP:journals/corr/abs-2410-10700, cipherchat, Multilingual}. 

% Attacks leveraging long-tail encoded distributions are particularly effective when the target LLM's safety fine-tuning fails to generalize to domains requiring corresponding capabilities. 
Recently, some black-box are proposed to use code to encrypt malicious inputs to build long-tail encoded distributions. 
CodeAttack~\citep{codeattack} embeds malicious queries within data structures (e.g., stacks and queues) to bypass safety alignments designed for prompts written in natural languages. 
Codechameleon~\citep{codechameleon} encrypts malicious prompts using custom program functions, transforming them into code completion tasks.
Unlike these methods, QueryAttack does not rely on the syntax of programming languages for encryption. Instead, it only requires certain keywords or expressions from the programming language. This means that QueryAttack can be applied not only using programming languages but also to any non-natural language that the target LLM can understand but has not been well aligned during the safety alignment phase. Moreover, even without the need for output encryption, QueryAttack can still effectively attack target LLMs.

\subsection{Safety Alignment for Defending Jailbreak}
% To defend against jailbreak attacks, researchers have developed various mitigation strategies to guide large language models in rejecting malicious prompts of potential adversaries.
Reinforcement Learning from Human Feedback (RLHF)~\citep{RLHF} is one of the most widely used defense mechanisms. For instance, recent works such as~\citep{DBLP:conf/emnlp/MehrabiGDHGZCG024,Self-Alignment} explore the effectiveness of alignment during pre-training in defending against malicious queries, CoT reasoning~\citep{o1}, as well as in-context learning~\citep{DBLP:journals/corr/abs-2310-06387, DBLP:conf/acl/RenG0LZQL24}.  
These methods often rely on natural language inputs collected from red teams, which can lead to generalization issues when faced with non-natural language or other OOD inputs. 

Beyond the training process, some approaches focus on input and output safeguards, such as input perturbation~\citep{paraphrase}, safe decoding~\citep{SafeDecoding}, and jailbreak detection~\citep{SmoothLLM, SelfDefense, DBLP:journals/corr/abs-2309-00614, gao2024embedding}. These methods can effectively reduce the attack success rate of jailbreak attacks. 
However, their effectiveness depends heavily on the quality of malicious data used for training and incurs significant additional overhead during deployment, which may affect user experience.

% The latest CoT reasoning-enhanced models (e.g., O1~\citep{o1}) employ an embedded chain-of-thought mechanism to enforce step-by-step reasoning about the intent of queries and reflection on their compliance with policies during the response generation process. This approach does not rely on text in natural language from the training phase and simultaneously improves the model's understanding of both queries and policy guidelines.