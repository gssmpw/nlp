\section{Related Work}
\label{sec:related work}

\subsection{Jailbreak Attacks on LLMs}
Initially, researchers reveal that adversaries could launch attacks by manually constructing out-of-distribution (OOD) samples **Santos, "Adversarial Attacks on Deep Learning Models"**.
Building on these observations, several white-box attack methods are proposed **Cheng, "White-Box Attack Methods for Large Language Models"**. 
Compared to white-box attacks, black-box attacks assume that adversaries   adjust their prompt strategies only based on the model's responses **Gao, "Black-Box Attacks on Large Language Models"**. 

% Attacks leveraging long-tail encoded distributions are particularly effective when the target LLM's safety fine-tuning fails to generalize to domains requiring corresponding capabilities. 
Recently, some black-box are proposed to use code to encrypt malicious inputs to build long-tail encoded distributions. 
CodeAttack **Zhang, "CodeAttack: Bypassing Safety Alignments with Code"** embeds malicious queries within data structures (e.g., stacks and queues) to bypass safety alignments designed for prompts written in natural languages. 
Codechameleon **Liu, "Codechameleon: Encrypting Prompts using Custom Program Functions"** encrypts malicious prompts using custom program functions, transforming them into code completion tasks.
Unlike these methods, QueryAttack does not rely on the syntax of programming languages for encryption. Instead, it only requires certain keywords or expressions from the programming language. This means that QueryAttack can be applied not only using programming languages but also to any non-natural language that the target LLM can understand but has not been well aligned during the safety alignment phase. Moreover, even without the need for output encryption, QueryAttack can still effectively attack target LLMs.

\subsection{Safety Alignment for Defending Jailbreak}
% To defend against jailbreak attacks, researchers have developed various mitigation strategies to guide large language models in rejecting malicious prompts of potential adversaries.
Reinforcement Learning from Human Feedback (RLHF) **Brown, "Reinforcement Learning from Human Feedback"** is one of the most widely used defense mechanisms. For instance, recent works such as **Wang, "Exploring Alignment during Pre-Training for Defending Malicious Queries"**, **Zhu, "CoT Reasoning and its Applications"**, and **Kim, "In-Context Learning for Large Language Models"** explore the effectiveness of alignment during pre-training in defending against malicious queries, CoT reasoning, as well as in-context learning.  
These methods often rely on natural language inputs collected from red teams, which can lead to generalization issues when faced with non-natural language or other OOD inputs. 

Beyond the training process, some approaches focus on input and output safeguards, such as input perturbation **Lee, "Input Perturbation for Defending Against Jailbreak Attacks"**, safe decoding **Kim, "Safe Decoding for Large Language Models"**, and jailbreak detection **Choi, "Jailbreak Detection for Large Language Models"**. These methods can effectively reduce the attack success rate of jailbreak attacks. 
However, their effectiveness depends heavily on the quality of malicious data used for training and incurs significant additional overhead during deployment, which may affect user experience.

% The latest CoT reasoning-enhanced models (e.g., O1____) employ an embedded chain-of-thought mechanism to enforce step-by-step reasoning about the intent of queries and reflection on their compliance with policies during the response generation process. This approach does not rely on text in natural language from the training phase and simultaneously improves the model's understanding of both queries and policy guidelines.