% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{multirow} 
\usepackage{enumitem}
\usepackage{subfigure}
\definecolor{ForestGreen}{RGB}{34,139,34}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

% \title{Making Them a Malicious Database — Jailbreaking Aligned Large Language Model Using Query Code}

\title{Making Them a Malicious Database: Exploiting Query Code to Jailbreak Aligned Large Language Models}

% Author information can be set in various styles:t
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}



\author{
  \normalfont
  Qingsong Zou$^{1,3}$\thanks{Equal contribution.} \quad
  Jingyu Xiao$^{2}$\footnotemark[1] \quad
  Qing Li$^{3}$\thanks{Corresponding author.} \quad 
  Zhi Yan$^{4}$ 
  \quad 
  Yuhang Wang$^{5}$ \\
  Li Xu$^{6}$ 
  \quad Wenxuan Wang$^{2}$ 
  \quad Kuofeng Gao$^{1}$ 
  \quad Ruoyu Li$^{7}$ 
  \quad Yong Jiang$^{1}$ \\
  $^1$Tsinghua Shenzhen International Graduate School \quad $^2$The Chinese University of Hong Kong \\
  \quad $^3$Pengcheng Laboratory \quad $^4$Jilin University
  \quad $^5$Southwest University \\
  $^6$University of Electronic Science and Technology of China \quad $^7$Shenzhen University
}

% \author{anonymous}

\begin{document}
\maketitle
\begin{abstract}
Recent advances in large language models (LLMs) have demonstrated remarkable potential in the field of natural language processing. Unfortunately, LLMs face significant security and ethical risks. 
Although techniques such as safety alignment are developed for defense, prior researches reveal the possibility of bypassing such defenses through well-designed jailbreak attacks. 
In this paper, we propose QueryAttack, a novel framework to examine the generalizability of safety alignment. By treating LLMs as knowledge databases, we translate malicious queries in natural language into structured non-natural query language to bypass the safety alignment mechanisms of LLMs. 
We conduct extensive experiments on mainstream LLMs, and the results show that QueryAttack not only can achieve high attack success rates (ASRs), but also can jailbreak various defense methods. Furthermore, we tailor a defense method against QueryAttack, which can reduce ASR by up to 64\% on GPT-4-1106.
Our code is available at \url{https://github.com/horizonsinzqs/QueryAttack}.

{\textcolor{red}{WARNING: THIS PAPER CONTAINS UNSAFE MODEL RESPONSES.}}
\end{abstract}

\section{Introduction}
\label{sec:introduction}

Large language models (LLMs) such as OpenAI’s GPT series~\citep{gptseries} and Meta’s Llama series~\citep{llamaseries} demonstrate remarkable generative potential across various domains~\citep{DBLP:journals/corr/abs-2411-03292, DBLP:journals/corr/abs-2304-05332, DBLP:conf/naacl/HeLGJZLJYDC24, gao2024inducing}. 
However, the immense amounts of data used for training LLMs contain massive information, enabling them to learn unscreened knowledge, including those may evidently violate ethical and moral standards~\citep{DBLP:conf/emnlp/LiGFXHMS23, artprompt, cipherchat, bai2024badclip}. 
Therefore, a critical responsibility of service providers is to prevent these models from supplying harmful information to potentially adversaries. 
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{intro.pdf}
    \caption{Existing methods reveal a well-established phenomenon: malicious queries, when well encrypted, can bypass the security defenses of LLMs, leading them to generate encrypted harmful outputs.
    % Adversaries can obtain the desired harmful content in natural language by decryption. 
    Unlike these methods, QueryAttack achieves jailbreak by translating malicious inputs into structured non-natural query languages and is capable of directly inducing LLMs to generate harmful content in natural language without the need for a de-translation step.}
    \label{fig:intro}
\end{figure}

To align the responses of LLMs with human ethics and preferences, numerous techniques are employed during the training process of LLMs to regulate their outputs to human queries.
For example, supervised fine-tuning~\citep{DBLP:conf/iclr/WeiBZGYLDDL22, DBLP:conf/nips/Ouyang0JAWMZASR22}, reinforcement learning from human feedback~\citep{Self-Alignment, DBLP:conf/emnlp/MehrabiGDHGZCG024}, red teaming~\citep{redteam}, and the constitutional AI~\citep{Constitutional} approach are proposed to enhance the safety of LLMs.
% These efforts enhance the ability of LLMs to identify and defend against malicious queries. 
Unfortunately, a significant limitation of these methods is their reliance on malicious natural language samples from the alignment stage to train the model to recognize malicious queries and ensure the generation of safe outputs.
This dependency leaves room for adversaries to develop jailbreak methods using non-natural language as input. 


Specifically, CipherChat~\citep{cipherchat} uses encryption methods such as the Caesar cipher to translate harmful queries into encrypted text.  
ArtPrompt~\citep{artprompt} replaces sensitive terms with ASCII-style encoding.  
~\citet{Multilingual} convert sensitive contents into low-resource languages. The essence of these methods lies in inducing the model to generate encrypted outputs, which are then decrypted to harmful text in natural language format. 
However, they typically require the model to possess knowledge of encryption to understand the prompts or place high requirements on the model's ability to generate encrypted content. As a result, their attack effectiveness is limited.
Therefore, developing an effective and efficient jailbreak attack
method remains a critical challenge.

We observe that, the essence of these jailbreak attacks lies in defining a customized encryption method and then using the language encrypted by this method to interact with the target LLMs, thereby bypassing their defense mechanisms.
Inspired by prior work, we find that LLM’s defensive mechanisms are not sensitive to structured, non-natural query languages. 
For example, by treating the target LLM as a knowledge database, when using structured query language (SQL) to request malicious knowledge (as shown in Figure~\ref{fig:intro}) the target LLM not only identifies the intent of the request well but also does not trigger the defense mechanisms. Instead, the target LLM responds to the entire prompt in natural language normally.

From this new perspective, we propose an attack that first uses structured non-natural query languages to jailbreak LLMs, named QueryAttack. Specifically, we break down QueryAttack into three main components:

\noindent
1). Extracting three key components from the original query: the requested content, the modifier of the content, and the high-level category to which the content belongs (potential sources where the content can be found).

\noindent
2). Filling the query components into predefined query templates (e.g., SQL templates) to generate a structured non-natural query.

\noindent
3). Applying in-context learning to help the target LLM understand the natural semantics of the template and prompting the target LLM using the structured non-natural query.

These three steps define a query task, analogous to querying data from a database using SQL. The additional cost introduced by this process is limited to translating the malicious query into the specified format, which can be easily adapted to any query based on natural languages.
Given that programming languages are widely present in the training data of LLMs and that these models exhibit excellent semantic understanding of programming languages~\citep{OpenAI23, gptseries, Anthropic23, llamaseries}, we naturally employ programming syntax to construct the query templates. 

% We design a total of nine query formats written in commonly used programming language styles to cover a wide range of typical programming languages. 
We test QueryAttack on AdvBench~\citep{GCG} across both well-known open-source and closed-source LLMs. The experimental results show that QueryAttack effectively bypasses their security defenses, achieving state-of-the-art attack success rates (ASRs). Besides, we provide a visual analysis of QueryAttack’s success and propose a tailored defense method against QueryAttack. 

Our contributions can be summarized as follows:
\begin{itemize}
[topsep=0pt,itemsep=0pt,parsep=0pt,partopsep=0pt,leftmargin=10pt]
    \item We are the first to observe that the defense mechanisms of LLMs are not sensitive to structured non-natural query languages and propose QueryAttack, a novel jailbreak framework based on this observation.
    % QueryAttack only requires the translation of harmful quries without requiring a de-translation step. 
    \item Our evaluation on mainstream LLMs demonstrates that QueryAttack successfully bypasses their security mechanisms and achieves state-of-the-art average attack success rate.
    \item We propose a tailored defense approach to mitigate QueryAttack, and experiments show that it can effectively help LLMs reduce the attack success rate.
\end{itemize}

% The rest of our paper is organized as follows. In Section~\ref{sec:relatedwork}, we summarize related work on jailbreak attacks and defense mechanisms for large language models. Section~\ref{sec:methodlogy} provides a detailed explanation of the proposed jailbreak attack framework, followed by extensive experimental results presented in Section~\ref{sec:experiment}. The conclusion is provided in Section~\ref{sec:conclusion}.

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{QueryAttack.pdf}
    \caption{The overview of QureyAttack. QueryAttack executes a three-step process to jailbreak the target LLM: 
    1). Extracting three key query components from the original query.
    2). Filling the query template to get a query code.
    3). Applying in-context learning to help the target LLM understand the natural semantics of the template and prompting the target LLM using the query code.}
    \label{fig:overview}
\end{figure*}

\section{Background}
\label{sec:background}
Large language models (LLMs) have demonstrated remarkable generative potential across various fields. However, they are still vulnerable to jailbreak attacks. Jailbreak attacks against LLMs typically involve crafting carefully designed inputs to prompt models to generate and output harmful response, such as instructions that blatantly violate human ethics or the disclosure of sensitive information. 
Since natural language samples are widely used as safety alignment data during the training phase of LLMs~\citep{gpt4}, potential malicious users or adversaries can bypass the defense mechanisms of these models by designing prompts based on non-natural language distributions. 

% Initially, researchers reveal that adversaries could launch attacks by manually constructing out-of-distribution (OOD) samples~\citep{DBLP:conf/emnlp/LiGFXHMS23, DBLP:conf/ccs/ShenC0SZ24}.
% Building on these observations, several white-box attack methods are proposed~\citep{GCG, ARCA, DBLP:conf/icml/JonesDRS23, gao2024denial}. These attacks leverage access to the model's architecture and weights of parameters to generate adversarial prompts based on gradients. 
% Compared to white-box attacks, black-box attacks assume that adversaries   adjust their prompt strategies only based on the model's responses~\citep{PsySafe, DBLP:journals/corr/abs-2311-03348, DBLP:conf/iclr/LiuXCX24, DBLP:journals/corr/abs-2410-10700, HEA, cipherchat, Multilingual}. 
Attacks leveraging long-tail encoded distributions are particularly effective when the target LLM's safety fine-tuning fails to generalize to domains requiring corresponding capabilities. 
For example, by replacing sensitive contents with Base64~\citep{Jailbroken}, ciphertext~\citep{cipherchat}, or low-resource languages~\citep{Multilingual}, such attacks induce mismatched generalization in the target LLMs. 

Despite the development of numerous defense methods by researchers to mitigate jailbreaking attacks, such as supervised fine-tuning~\citep{DBLP:conf/iclr/WeiBZGYLDDL22, DBLP:conf/nips/Ouyang0JAWMZASR22} and reinforcement learning from human feedback~\citep{Self-Alignment, DBLP:conf/emnlp/MehrabiGDHGZCG024}.
However, recent work has shown that they are not able to completely defend against zero-day jailbreaking attacks~\citep{HEA, DBLP:conf/coling/WuG0025}. Therefore, it is necessary to continue studying the attack vector of LLMs and provide insights for developing new defenses.

% Initially, researchers reveal that adversaries could launch attacks by manually constructing out-of-distribution (OOD) samples~\citep{DBLP:conf/emnlp/LiGFXHMS23, DBLP:conf/ccs/ShenC0SZ24}. However, this kind of approaches relies heavily on extensive human effort, incurs significant costs on launching attacks, or it is difficult to adapt to different models or models trained on different datasets. 
% Building on these observations, several attack methods based on white-box assumptions are proposed~\citep{GCG, ARCA, DBLP:conf/icml/JonesDRS23, gao2024denial}.
% These attacks leverage access to the model's architecture and weights of parameters to generate adversarial prompts based on gradients. While effective, these methods are likely to fail when applied to closed-source models under black-box scenarios.

% Compared to white-box attacks, black-box attacks have gradually garnered significant attention due to their minimal requirement for model-specific knowledge. In general, black-box attacks assume that adversaries can only adjust their prompt strategies based on the model's responses~\citep{Jailbroken}. 
% Typical black-box attack methods, such as scenario setting-up or role-playing~\citep{PsySafe, DBLP:journals/corr/abs-2311-03348, DBLP:conf/iclr/LiuXCX24, DBLP:journals/corr/abs-2410-10700, HEA}, work by prompting the LLMs into dangerous hypothetical contexts, thereby triggering conflicts with the value alignment mechanisms of these models. 
% Another approach aims to use pre-trained models to adversarially generate harmful prompts by leveraging feedback from the target models as feedback~\citep{DBLP:journals/corr/abs-2310-08419}.

% Attacks leveraging long-tail encoded distributions are particularly effective when the target LLM's safety fine-tuning fails to generalize to domains requiring corresponding capabilities. 
% By replacing sensitive contents with Base64~\citep{Jailbroken}, ciphertext~\citep{cipherchat}, or low-resource languages~\citep{Multilingual}, such attacks induce mismatched generalization in the target LLMs. 
% Notably, some recent work has focused on crafting adversarial inputs using programming languages. For instance, ~\citep{codeattack} embed malicious queries within data structures (e.g., stacks and queues) to bypass safety alignments designed for prompts written in natural languages.
% ~\citep{codechameleon} encrypt malicious prompts using custom functions, transforming them into code completion tasks. 
% The essence of these methods lies in inducing the model to generate encrypted outputs, which are then decrypted to harmful text in natural language format. 
% %%?
% However, these methods typically require the model to possess knowledge of encryption to understand the prompts or place high requirements on the model's ability to generate encrypted content.


\section{Methodology}
\label{sec:methodlogy}

The core idea of QueryAttack is to use structured non-natural query languages to carry out jailbreak attacks. As shown in Figure~\ref{fig:overview}, QueryAttack defines a content query task using three components to induce harmful outputs from the target LLM:
1). Query Components Extraction, which extracts key query components from the original query written in natural language.
2). Query Template Filling, which uses the extracted query components to fill the query template and get query code.
3). ICL-based Query Understanding, which applies in-context learning to enable the target LLM to understand the natural semantics of the template and generate responses with the user’s desired content.

% \subsection{Observation}
% \label{sec:observation}
% As discussed in Section~\ref{sec:introduction}, we aim to demonstrate that the current generalization capabilities of safety aligned LLMs are insufficient to defend against zero-day attack samples constructed by adversaries using custom OOD languages. 
% To promote understanding of target models, many jailbreak attack methods employ few-shots or many-shots to help the model learn the user-defined language~\citep{cipherchat, artprompt}. However, custom languages can be highly diverse. The effectiveness of such attacks depends significantly on the model's ability to understand the new language. 

% In light of this, some attacks use common but non-natural languages as templates to encrypt malicious queries, achieving high ASR even under zero-shot conditions~\citep{DBLP:journals/corr/abs-2411-12762, codeattack}. These languages often exist within the training samples of LLMs and are well understood by the models. However, they are not fully explored during the safety alignment process, making it possible for the models to generate malicious responses based on harmful prompts written by these languages.
% Programming languages clearly align with the common characteristics of such non-natural languages. Therefore, in this paper, we use them as examples to construct our query template.

% Given the excellent generative capabilities of LLMs, we leverage them to construct translators that converts text in natural language into the our custom query templates. Few-shot learning is employed to guide the model to understand the query intents defined by the templates and generate responses in natural language. 
% For those models with stronger comprehension capabilities (e.g., GPT-O1~\citep{o1}), we employ zero-shot learning. Figure~\ref{fig:overview} provides an overview of QueryAttack.


\subsection{Query Components Extraction}
\label{sec:Query Translator}
We first identify the components to be extracted from a natural language query (referred to as query components).
Taking using Structured Query Language (SQL) as an example, the preliminary task for converting a natural language query into SQL code is to extract the key components of the query sentence. 
We observe that, for a given query, the two most relevant components to the query’s semantics are: the content to be queried and the description of that content. These two components can naturally be filled into the ``\textit{SELECT}'' and ``\textit{WHERE NAME=}'' sections of an SQL query. To ensure the completeness of the SQL query and to guide the target LLM’s response, we add an additional component: the category to which the queried content belongs (which can be understood as potential sources where the content may be found). This component can then be filled into the ``\textit{FROM}'' section of an SQL query.

Using these three extracted query components, a clear query can be defined, regardless of the language used. For example, the three key query components of the natural language query ``\textbf{Tell me the method of crafting a bomb}'' are \textbf{\{ content: `crafting method', modifiers: `bomb', category: `crafting catalog'. \}}.
Therefore, the first step of QueryAttack is to extract the following three query components from the natural language query:

\noindent
(1) the query content (content);

\noindent
(2) the modifiers of the query content (modifiers);

\noindent
(3) the higher-level benign category to which the query content belongs (category).

% After defining the templates, the primary task is to translate natural language prompts into the template format. Extracting the three query components from the text in natural language is the most critical step in this process. 

The query components extraction task can be performed manually by humans or automated using natural language processing tools such as NLTK~\citep{nltk}. Given the strong text-processing capabilities of LLMs, we choose GPT-4-1106~\citep{gptseries} to perform the component extraction. 
We design specialized prompts to guide the GPT-4-1106 in components extraction. This prompt is aimed at ensuring the model focuses on the text-processing task rather than the malicious content, and thereby prevents the translation request from being rejected by the model. 
To achieve this, we employ in-context learning to enable the model to understand the component extraction task and standardize its output. The details of the prompt are provided in the Appendix~\ref{app:component}.


\subsection{Query Template Filling}
\label{sec:Query Template}
\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{template.pdf}
    \caption{Templates written in common programming language styles.}
    \label{fig:template}
\end{figure}

% To make QueryAttack adaptable to various non-natural languages, we construct templates in different non-natural languages based on the three query components. 
After extracting the query components, we obtain the key semantics of a query. Therefore, we can rewrite the query into any non-natural language based on them. To automate the attack, we can predefine query templates for non-natural languages, allowing us to simply fill them with the query components to generate query in non-natural language.

One observation is that non-natural languages commonly present in the training data of LLMs are more likely to be understood by LLMs. Since programming languages are widely present in the training data of LLMs, we choose them as examples to demonstrate the effectiveness of QueryAttack.
Among many programming languages, two that are naturally associated with requesting content are SQL and Uniform Resource Locators (URLs). SQL is designed to query content from databases using standardized syntax. Similarly, the format of the Hypertext Transfer Protocol Secure (HTTPS), which uses URLs to fetch web resources from servers, follows a similar process. 
For example, in SQL, a malicious query written in natural language, such as ``\textbf{Tell me the method of crafting a bomb}'', can be rewritten using its three query components as ``\textbf{SELECT 'crafting method' FROM 'crafting catalog' WHERE NAME = 'bomb'}''.

Inspired by this, although other programming languages are not specifically designed for querying resources, their syntax contains similar keywords or expressions that can express similar query intents, such as ``\textit{print}'', ``\textit{input}'' and ``\textit{return}''.
It means once the three query components are identified, this query template can be adapted to other programming languages.
Specifically, using the query components, we define query templates for nine common programming languages (C, C++, C\#, Python, Java, Javascript, Go, URL, SQL), as shown in Figure~\ref{fig:template}.

As a conclusion, when using QueryAttack to jailbreak the LLMs, the second step is to fill the extracted three key query components into the corresponding language’s query template and obtain the query code, as shown in Figure~\ref{fig:overview}.

\subsection{Query Understanding}
\label{sec:In-context Learning}

The purpose of the query learning is to guide the model in understanding the intent behind the query codes and then generate a natural language response. To help the LLMs understand the content, we first establish the context of the conversation. 
By describing the three query components, we guide the model in building a mapping from the query code to the natural language, and define the conversation within an educational context.

Few-shot learning is then used to reinforce the model's understanding of the query and guide it on how to respond to these prompts using natural language. Some text in natural languages, which contain multiple queries, may require several query codes to help define the query. Therefore, we provide both short and long examples.

For models with strong understanding of programming languages, we can skip this process and use zero-shot to launch the attack.  
Finally, we provide detailed guidance in natural language to respond to the query queries. The aim of this process is to have the model answer the relevant knowledge as thoroughly as possible, rather than focusing on understanding and explaining the natural semantics of the prompts. In Appendix~\ref{app:Attack Examples of QueryAttack}, we provide the complete prompt of this part.

Through the above steps, we enable the target LLM to understand the intent of the query code and generate responses in natural language according to the query. The adversary then uses the query code obtained in the second step to launch the attack and obtain the desired malicious knowledge.


\section{Experiments}
\label{sec:experiment}

\subsection{Experimental Setup}
\label{sec:experimental setup}

\begin{sloppypar}
\textbf{Victim Models.}
We test QueryAttack on 13 mainstream large language models: GPT-3.5 (gpt-3.5-turbo)~\citep{OpenAI23}, GPT-4-1106 (gpt-4-1106-preview)~\citep{gptseries}, GPT-4o~\citep{gpt-4o}, O1 (gpt-o1)~\citep{o1}, Deepseek (deepseek-chat)~\citep{deepseek}, Gemini-flash (gemini-1.5-flash), Gemini-pro (gemini-1.5-pro)~\citep{gemini1.5}, Llama-3.1-8B (meta-llama-3.1-8B-instruct), Llama-3.1-70B (meta-llama-3.1-70B-instruct), Llama-3.2-1B (meta-llama-3.2-1B-instruct), Llama-3.2-3B (meta-llama-3.2-3B-instruct), Llama-3.2-11B (meta-llama-3.2-11B-vision-instruct) and Llama-3.3-70B (meta-llama-3.3-70B-instruct)~\citep{llama3.1, llama3.2}.
To maintain the reproducibility of the results, we set all the temperature to 0.
\end{sloppypar}

\textbf{Datasets.}
We used AdvBench~\citep{GCG} as the dataset for our experiments. This is a harmful behavior dataset that contains 520 different harmful query instances written in natural language. For some experiments, we use a subset of AdvBench which contains 50 representative, non-repetitive harmful instructions refined in~\citep{artprompt}. We will specify this at the beginning of these parts where the subset is used.

\begin{table*}[h]
\centering
\resizebox{0.98\textwidth}{!}{
\begin{tabular}{c|cccccc}
\hline
                & GPT-4-1106     & GPT-4o*          & Llama-3.1-8B*     & Llama-3.3-70B*    & Gemini-pro*     & Gemini-flash*   \\ \hline
PAIR            & - / -          & 3.16 / 45.38 \% & 3.06 / 35.38\% & 3.24 / 47.30\% & 1.92 / 22.31\% & 1.92 / 18.27\% \\
TAP             & - / -          & 3.24 / 51.34\%  & 2.97 / 31.34\% & 3.71 / 55.38\% & 2.83 / 24.23\% & 3.01 / 33.27\% \\
CipherChat      & - / 19\%       & 1.94 / 16.34\%  & 1.76 / 0 \%    & 2.40 / 4.23\%  & 2.22 / 3.27\%  & 2.12 / 5.38\%  \\
CodeAttack      & - / 81\%       & - / 89\%        & - / -          & - / -          & - / 2\%        & - / -          \\
HEA             & - / -          & 4.42 / 90.38\%  & \underline{4.67} / \textbf{95.38\%} & 3.58 / 68.27\% & 4.21 / 82.38\% & 4.64 / 100\%   \\ \hline
Ours (Top 1)    & 4.65 / \underline{82.18\%} & \underline{4.72} / \underline{90.58\%}  & 4.04 / 65.78\% & \underline{3.98} / \underline{68.77\%}             & \underline{4.71} / \underline{85.63\%} & \underline{4.93} / \underline{95.59\%} \\
Ours (Ensemble) & 4.75 / \textbf{93.80\%} & \textbf{4.85} / \textbf{96.35\%}  & \textbf{4.83} / \underline{88.89\%} & \textbf{4.11} / \textbf{73.56\%}             & \textbf{4.91} / \textbf{95.40\%} & \textbf{4.99} / \textbf{99.62\%} \\ \hline
\end{tabular}}
\caption{Average HS / ASR of baselines and QueryAttack on the AdvBench. QueryAttack can breach the safety guardrails of mainstream LLMs, including GPT, Llama and Gemini series and achieves SoTA ASR. *:~\citet{HEA} report that they ``consider responses of LLMs with a score higher than 4 as successful attacks'' and we present their original results. When calculating the ASR for QueryAttack, we follow~\citet{codeattack} to count instances where HS = 5.}
\label{tab:baseline}
\end{table*}

\textbf{Baselines.} We select five representative jailbreak techniques as baselines:

\noindent
\textit{PAIR}~\citep{PAIR}: a black-box attack that uses an adversarial LLM to automatically generate adversarial inputs for the target LLM. 

\noindent
\textit{CipherChat}~\citep{cipherchat}: a black-box attack that converts inputs into ciphers for jailbreaking.

\noindent
\textit{CodeAttack}~\citep{codeattack}: a method that leverages code to embed natural language inputs into data structures to generate adversarial inputs.

\noindent
\textit{TAP}~\citep{TAP}: A black-box jailbreak attack via iterative refinement of attack prompts.

\noindent
\textit{HEA}~\citep{HEA}: A black-box method that embeds malicious queries within a scenario containing positive prompts and deceives the LLM into jailbreaking during subsequent malicious queries.
% \begin{enumerate}
%     \item GCG~\citep{GCG}: a white-box attack that generates adversarial inputs through gradient-based discrete optimization.  
%     \item AutoDAN~\citep{AutoDAN}: a black-box attack that employs a genetic algorithm to iteratively optimize adversarial examples. 
%     \item ARCA~\citep{ARCA}: a white-box attack that uses discrete optimization to automatically find adversarial inputs.
%     \item PAIR~\citep{PAIR}: a black-box attack that uses an adversarial LLM to automatically generate adversarial inputs for the target LLM. 
%     \item CipherChat~\citep{cipherchat}: a black-box attack that converts inputs into ciphers to jailbreak the target LLMs.
%     \item CodeAttack~\citep{codeattack}: a method that leverages programming code to embed natural language inputs into data structures to generate adversarial inputs.
% \end{enumerate}

\textbf{Evaluation and Metrics.}
We focus on three main metrics for evaluation.

\textbf{Harmfulness score (HS).} Our evaluation follows the work of~\citep{gpt4-judge}, utilizing the powerful evaluation capabilities of GPT-4 to assign harmfulness scores to the responses generated by the target LLM. The harmfulness score ranges from 1 to 5. To improve the accuracy of GPT-4 Judge, we remove educational statements or disclaimers that may appear in the responses to extract the harmful content, as shown in~\ref{app:removal}. The extracted content is then input into GPT-4 Judge for evaluation. Detailed settings can be found in the Appendix~\ref{app:Experimental details}. 

\textbf{Attack success rate (ASR).} Based on HS, we calculate the ASR, the percentage of harmful responses in the case of adversarial queries, which can be calculated by Equation~\ref{eq:asr}.  
\begin{equation}
A S R=\frac{\# \text { of responses with } H S=5}{\# \text { of responses }} .
\label{eq:asr}
\end{equation}

\textbf{Refuse rate (RR).} 
The third evaluation metric is the refuse rate (RR). We follow the setup of~\citep{GCG} and use Dict-Judge to assess the number of refusal responses, as shown in Equation~\ref{eq:rr}.
\begin{equation}
\label{eq:rr}
R R=\frac{\text { \# of queries that are refused by LLM }}{\# \text { of queries }} .
\end{equation}

% In the comparison with baselines, we primarily consider ASR as the evaluation metric. In other experiments, we also report RR and HS to further supplement the evaluation. 

We present the experimental results from CodeAttack~\citep{codeattack} and~\citep{HEA}, as they use the same benchmark as ours and also employ the GPT-4 Judge~\citep{gpt4-judge} method to evaluate their attacks. Therefore, we use their results as baseline for comparisons with QueryAttack. 
Note that~\citet{HEA} consider an attack successful when the HS is ``higher than 4'', whereas we follow~\citet{codeattack} to define success only when HS equals 5. 
% However, we can still compare results using the average HS.

\subsection{Results}
\label{sec:results}
\textbf{QueryAttack achieves SoTA ASR.} 
Table~\ref{tab:baseline} presents the average HS and the ASR of QueryAttack and several baselines on AdvBench~\citep{GCG}. We demonstrate two configurations of QueryAttack. In the first configuration, denoted as \textit{Top 1}, QueryAttack uses the programming language style with the highest ASR to construct the query template. In the second configuration, denoted as \textit{Ensemble}, no restrictions are placed on the programming language styles. 
From this table, we can observe that, despite specialized safety alignment training in these latest LLMs, QueryAttack successfully bypassed their defenses, inducing responses that violate policies or human values. 
For GPT-4-1106, the \textit{Top 1} configuration achieves a 82.18\% ASR and the \textit{Ensemble} configuration achieves 93.80\%. In contrast, the baselines which perform best are only able to bypass GPT-4-1106's safeguards in up to 81\% of cases. 
The same trend is observed in other models. Except for Llama-3.1-8B, QueryAttack’s ASR is lower than HEA. However, under the \textit{Ensemble} configuration, the average HS remains higher than that of HEA. 
% This demonstrates the effectiveness of QueryAttack.
% However, when attacking GPT-3.5, the \textit{Top 1} ASR was lower. This is because GPT-3.5 has relatively weaker comprehension of query texts in coding styles, causing some of the generated responses to deviate from the original intent of the malicious queries.

\textbf{QueryAttack remains effective when facing reasoning-enhanced models.}
For cost considerations, we test QueryAttack’s effectiveness against the O1 model using the subset of AdvBench containing 50 samples. Under the \textit{Ensemble} configurations, QueryAttack achieves an average HS of 3.66 and an ASR of 50\%. These results indicate that CoT Reasoning-enhanced models may have the potential to defend against QueryAttack, but QueryAttack still maintains a considerable ASR.

\subsection{Ablation and Analysis}
\label{sec:abalation}

% \subsubsection{Impact of different language styles.}
% \label{sec:differentLanguage}

% \subsubsection{Embedding Similarity of Natural Language Malicious Queries and Query Templates}
% \label{}

\textbf{Languages that differ more from natural language is likely to increase QueryAttack’s ASR.}
% We create nine query templates in different programming language styles: Python, Java, JavaScript, C++, C, C\#, SQL, URL, and Go.
Figure~\ref{fig:language} shows the average HS and RR obtained by attacking GPT-4-1106, Llama-3.1-70B, Gemini-flash and DeepSeek with templates of different language styles. 
On GPT-4-1106, Gemini-flash and DeepSeek, different language styles do not show significant variations in average HS. A noticeable decrease is observed when attacking Llama-3.1-70B with URL and SQL-style templates, where a higher RR leads to a lower HS. 
This may be because these two template styles closely resemble the structure of natural language, making them more likely to trigger existing defenses.
% Thus, using languages that differ more from natural language is likely to increase QueryAttack’s ASR. 
Despite this, under the \textit{Top 1} configuration, the ASR on Llama-3.1-70B still reachs 76.3\%, while the \textit{Ensemble} configuration achieves 92.9\%.

\begin{figure}[h!]
\centering %图片全局居中
\subfigure{
\includegraphics[width=0.48\linewidth]{GPT-4-1106_language.pdf}}
\subfigure{
\includegraphics[width=0.48\linewidth]{Llama-3.1-70B_language.pdf}}

\subfigure{
\includegraphics[width=0.48\linewidth]{DeepSeek_language.pdf}}
\subfigure{
\includegraphics[width=0.48\linewidth]{Gemini-flash_language.pdf}}
\caption{Performance of different language styles.}
\label{fig:language}
\end{figure}

\begin{table*}[h]
\centering
\resizebox{0.98\textwidth}{!}{
\begin{tabular}{c|cc|cc|cc|cc}
\hline
\multirow{2}{*}{} & \multicolumn{2}{c|}{Gemini-flash} & \multicolumn{2}{c|}{GPT-4-1106} & \multicolumn{2}{c|}{GPT-3.5} & \multicolumn{2}{c}{Llama-3.1-8B} \\ \cline{2-9} 
                  & ASR             & HS             & ASR            & HS            & ASR           & HS          & ASR             & HS             \\ \hline
Paraphrase        & 94\% ${\color{ForestGreen}\downarrow 6\%}$                & 4.88 ${\color{ForestGreen}\downarrow 0.12}$               & 72\% ${\color{ForestGreen}\downarrow 20\%}$               & 4.16 ${\color{ForestGreen}\downarrow 0.76}$              & 68\% ${\color{ForestGreen}\downarrow 14\%}$              & 4.56 ${\color{ForestGreen}\downarrow 0.14}$            & 90\% ${\color{red}\uparrow 2\%}$                & 4.86 ${\color{red}\uparrow 0.14}$               \\
Rand-insert       & 100\% ${\color{red}- 0\%}$                & 5.00 ${\color{red}- 0.00}$               & 86\% ${\color{ForestGreen}\downarrow 6\%}$               & 4.76 ${\color{ForestGreen}\downarrow 0.16}$              & 66\% ${\color{ForestGreen}\downarrow 16\%}$              & 4.48 ${\color{ForestGreen}\downarrow 0.22}$            & 72\% ${\color{ForestGreen}\downarrow 14\%}$                & 4.46 ${\color{ForestGreen}\downarrow 0.26}$               \\
Rand-swap         & 100\% ${\color{red}- 0\%}$                & 5.00 ${\color{red}- 0.00}$               & 94\% ${\color{red}\uparrow 2\%}$               & 4.88 ${\color{ForestGreen}\downarrow 0.04}$              & \textbf{54\% ${\color{ForestGreen}\downarrow 28\%}$}              & \textbf{4.12 ${\color{ForestGreen}\downarrow 0.58}$}            & 70\% ${\color{ForestGreen}\downarrow 16\%}$                & 4.50 ${\color{ForestGreen}\downarrow 0.22}$               \\
Rand-patch        & 100\% ${\color{red}- 0\%}$                & 5.00 ${\color{red}- 0.00}$               & 94\% ${\color{red}\uparrow 2\%}$               & 4.90 ${\color{ForestGreen}\downarrow 0.02}$              & 64\% ${\color{ForestGreen}\downarrow 18\%}$              & 4.48 ${\color{ForestGreen}\downarrow 0.22}$            & 72\% ${\color{ForestGreen}\downarrow 14\%}$                & 4.50 ${\color{ForestGreen}\downarrow 0.22}$               \\
Ours              & \textbf{36\% ${\color{ForestGreen}\downarrow 74\%}$}                & \textbf{3.56 ${\color{ForestGreen}\downarrow 1.44}$}               & \textbf{28\% ${\color{ForestGreen}\downarrow 64\%}$ }               & \textbf{3.10 ${\color{ForestGreen}\downarrow 1.82}$}              & 76\% ${\color{ForestGreen}\downarrow 6\%}$              & 4.68 ${\color{ForestGreen}\downarrow 0.02}$            & \textbf{34\% ${\color{ForestGreen}\downarrow 52\%}$}                & \textbf{3.38 ${\color{ForestGreen}\downarrow 1.34}$}               \\ \hline
\end{tabular}}
\caption{QueryAttack’s average ASR / HS against defense baselines. The differences between the defense and no defense are indicated by arrows.}
\label{tab:countermeasure}
\end{table*}

\textbf{Larger models do not provide better defense against QueryAttack.}
Table~\ref{tab:parasize} presents the ASRs of QueryAttack on models with different parameter sizes from the latest Llama-3.1 and Llama-3.2 series under \textit{Ensemble} configuration. 
On Llama-3.2 series, although the ASR slightly decreases for the 1B parameter models compared to larger models, the ASR does not show a decline as the parameter size continues to grow. 
% This can be observed in the results from the Llama-3.2 series. 
Specifically, as the parameter size increases from 8B to 70B, QueryAttack’s ASR on the Llama-3.1 series models rises from 88.89\% to 92.91\%.
This means increase in model parameter size does not show a positive correlation with the effectiveness of defending against QueryAttack. 
Without targeted safety alignment, bigger models may even have a higher risk of being attacked due to their stronger understanding ability of new language. Therefore, exploring more robust defense methods is necessary.
% A similar trend can also be seen in results from other previous studies~\citep{codeattack, cipherchat, codechameleon}. 
\begin{table}[h]
\centering
\resizebox{0.48\textwidth}{!}{
\begin{tabular}{c|ccccc}
\hline
    & 3.1-8B & 3.1-70B & 3.2-1B & 3.2-3B & 3.2-11B \\ \hline
ASR & 88.89\%           & 92.91\%            & 94.83\%           & 81.61\%           & 88.89\%            \\
RR  & 2.87\%           & 4.41\%            &   3.45\%         & 2.11\%           & 3.26\%            \\ \hline
\end{tabular}}
\caption{QueryAttack’s attack success rate and refusal rate on Llama 3.1 and 3.2 series models with different parameter sizes, under the \textit{Ensemble} configuration.}
\label{tab:parasize}
\end{table}

\textbf{Attention score distribution of natural language malicious queries and QueryAttack.}
We employ contrastive input erasure (CIE) \cite{yin2022interpreting} to quantify attention distribution. CIE evaluates how input tokens influence the model's preference between an expected token (ET) and an unexpected token (UT). Since LLMs typically begin their refusal responses with ``Sorry'' and their acceptance responses with ``Sure'', we set the ET to ``Sure'' and the UT to ``Sorry''.  An illustrative example is shown in the Figure~\ref{fig:example}, where blue tokens plays the role in rejecting query, whereas red tokens have the opposite effect. When directly questioned, the LLM precisely focuses its attention on the token ``make a bomb'', significantly contributing to its likelihood of denying a malicious query. In contrast, when applying query attack, LLMs mainly allocate attention to the term ``method'' and ``WHERE NAME = '', leading to a higher possibility to jailbreak.



% We utilize contrastive input erasure \cite{yin2022interpreting} (CIE) to represent the attention scores. CIE is a metric which can be calculated by a LLM, a query, an expected token (ET), and an unexpected token(UT). CIE measures how each token in the query contributes to the LLM generating the next token as the ET, rather than the UT. Since LLMs typically begin their refusal responses with ``Sorry'' and their acceptance responses with ``Sure'', we set the ET to ``Sure''
% and the UT to ``Sorry''.  





% PromptRobust \cite{zhu2023promptrobust} demonstrates that LLMs' jailbreak resistance improves when attention focuses on prompt keywords, whereas attention dispersion to peripheral tokens increases vulnerability by prioritizing superficial requirements over malicious intent detection. 




% correctly deducing a ‘Negative’ sentiment.
% the LLM’s attention is on ``method'', ``WHERE NAME = '', leading to a higher possibility to jailbreak.




\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{example.pdf}
    \caption{The attention score of natural language malicious query and QueryAttack.}
    \label{fig:example}
\end{figure}


\subsection{Discussion about Countermeasures}
\label{sec:countermeasures}
We consider two baseline defenses and design a tailored defense method against QueryAttack as follows. Detailed settings for these defense methods can be found in the Appendix~\ref{app:defense}.
% \begin{enumerate}
%     \item Paraphrase~\citep{paraphrase}: A defense method that reduces the ASRs by reconstructing inputs while preserving natural semantics.
%     \item Rand-Insert, Rand-Swap, and Rand-Patch~\citep{SmoothLLM}: A defense method against adversarial prompts by perturbing the inputs in different ways.
%     % \item Retokenization~\citep{retokenization}: A defense that also reconstruct inputs by dropping a certain percentage of the merges from the BPE tokenizer.
%     \item Cross-lingual Alignment Prompting based defense: A method that utilizes cross-lingual chain-of-thought prompting to generate reasoning paths, leveraging cross-language knowledge to improve zero-shot CoT reasoning across languages~\citep{Cross-lingual}.
% \end{enumerate}

\noindent
\textit{Paraphrase}~\citep{paraphrase}: A defense method that reduces the ASRs by reconstructing inputs while preserving natural semantics.

\noindent
\textit{Rand-Insert, Rand-Swap, and Rand-Patch}~\citep{SmoothLLM}: A defense method against adversarial prompts by perturbing the inputs in different ways.
% \item Retokenization~\citep{retokenization}: A defense that also reconstruct inputs by dropping a certain percentage of the merges from the BPE tokenizer.

\noindent
\textit{Cross-lingual Alignment Prompting based defense (Ours)}: 
~\citep{Cross-lingual} propose a method that uses cross-lingual chain-of-thought (CoT) prompting to generate reasoning paths, improving zero-shot CoT reasoning across languages. 
Although this approach is not originally designed for defense, we find that such CoT reasoning can help LLMs recognize cross-lingual malicious intent. 
Our insight is that, the success of QueryAttack relies on the target model’s ability to accurately interpret custom language templates, meaning the model should also be capable of identifying the intent of them and translating them into natural language. 
Once translated into natural language, malicious prompts are more likely to be filtered by existing safety alignment defenses. This indicates that a well-executed translate-then-reason CoT process can effectively defend against QueryAttack-like jailbreak attacks. 
Based on this, we design a defense method using cross-lingual chain-of-thought prompting. 
The complete chain of thought is: we first requires the target model to identify the Query Content, Key Object, and Content Source of the input, then describe the query in natural language. The target model then responds to this natural language query, thereby activating existing safety alignment mechanisms for defense. The detailed prompt can be found in the Appendix~\ref{app:defense}.

We test the effectiveness of these countermeasures against QueryAttack on four models: Gemini-flash, GPT-3.5, Llama-3.1-8B, and GPT-4-1106. For cost considerations, we use the subset of AdvBench refined in~\citep{artprompt} for evaluation in this section and report the results of the \textit{Ensemble} configuration using Python, C++, and SQL styles. 

Table~\ref{tab:countermeasure} presents the average HS / ASR of QueryAttack and the extent to which these defenses reduce average HS / ASR. The results show that QueryAttack is robust to baseline defenses. In the worst case, QueryAttack still achieves an average ASR of 63\% on GPT-3.5.
SmoothLLM and Paraphrase assume that adversarial tokens may be embedded in malicious prompts, yet they do not show significant defensive effects against QueryAttack. In some cases, QueryAttack’s effectiveness is even enhanced. For example, Paraphrase increase the HS for Llama-3.1-8B by 0.14 and the ASR by 2\%.

In contrast, our tailored defense based on cross-lingual alignment prompting effectively reduces the ASR of QueryAttack across all models, with the exception of a small reduction in GPT-3.5. However, on the other three models, our defense achieves an average ASR reduction of 63\% and an average HS reduction of 1.53, demonstrating the effectiveness of our defense.

% On the O1 model, QueryAttack achieves a lower ASR than on other models, further supporting this opinion.


\section{Related Work}
\label{sec:related work}

\subsection{Jailbreak Attacks on LLMs}
Initially, researchers reveal that adversaries could launch attacks by manually constructing out-of-distribution (OOD) samples~\citep{DBLP:conf/emnlp/LiGFXHMS23, DBLP:conf/ccs/ShenC0SZ24}.
Building on these observations, several white-box attack methods are proposed~\citep{ARCA, DBLP:conf/icml/JonesDRS23, gao2024denial}. 
Compared to white-box attacks, black-box attacks assume that adversaries   adjust their prompt strategies only based on the model's responses~\citep{PsySafe, DBLP:journals/corr/abs-2311-03348, DBLP:conf/iclr/LiuXCX24, DBLP:journals/corr/abs-2410-10700, cipherchat, Multilingual}. 

% Attacks leveraging long-tail encoded distributions are particularly effective when the target LLM's safety fine-tuning fails to generalize to domains requiring corresponding capabilities. 
Recently, some black-box are proposed to use code to encrypt malicious inputs to build long-tail encoded distributions. 
CodeAttack~\citep{codeattack} embeds malicious queries within data structures (e.g., stacks and queues) to bypass safety alignments designed for prompts written in natural languages. 
Codechameleon~\citep{codechameleon} encrypts malicious prompts using custom program functions, transforming them into code completion tasks.
Unlike these methods, QueryAttack does not rely on the syntax of programming languages for encryption. Instead, it only requires certain keywords or expressions from the programming language. This means that QueryAttack can be applied not only using programming languages but also to any non-natural language that the target LLM can understand but has not been well aligned during the safety alignment phase. Moreover, even without the need for output encryption, QueryAttack can still effectively attack target LLMs.

\subsection{Safety Alignment for Defending Jailbreak}
% To defend against jailbreak attacks, researchers have developed various mitigation strategies to guide large language models in rejecting malicious prompts of potential adversaries.
Reinforcement Learning from Human Feedback (RLHF)~\citep{RLHF} is one of the most widely used defense mechanisms. For instance, recent works such as~\citep{DBLP:conf/emnlp/MehrabiGDHGZCG024,Self-Alignment} explore the effectiveness of alignment during pre-training in defending against malicious queries, CoT reasoning~\citep{o1}, as well as in-context learning~\citep{DBLP:journals/corr/abs-2310-06387, DBLP:conf/acl/RenG0LZQL24}.  
These methods often rely on natural language inputs collected from red teams, which can lead to generalization issues when faced with non-natural language or other OOD inputs. 

Beyond the training process, some approaches focus on input and output safeguards, such as input perturbation~\citep{paraphrase}, safe decoding~\citep{SafeDecoding}, and jailbreak detection~\citep{SmoothLLM, SelfDefense, DBLP:journals/corr/abs-2309-00614, gao2024embedding}. These methods can effectively reduce the attack success rate of jailbreak attacks. 
However, their effectiveness depends heavily on the quality of malicious data used for training and incurs significant additional overhead during deployment, which may affect user experience.

% The latest CoT reasoning-enhanced models (e.g., O1~\citep{o1}) employ an embedded chain-of-thought mechanism to enforce step-by-step reasoning about the intent of queries and reflection on their compliance with policies during the response generation process. This approach does not rely on text in natural language from the training phase and simultaneously improves the model's understanding of both queries and policy guidelines.

\section{Conclusion}
\label{sec:conclusion}
% In this paper, we investigate the generalization challenges faced by large language models with safety alignment when encountering out-of-distribution malicious structured non-natural query language. 
% Building on previous encrypt-decrypt-based jailbreak methods, we further demonstrate that even when the target LLM is not required to generate encrypted response and instead directly produces natural language responses, a number of mainstream LLMs remain ineffective in mitigating harmful content generation.

In this paper, we investigate the generalization challenges faced by large language models with safety alignment when encountering out-of-distribution malicious structured non-natural query language.
Specifically, we introduce QueryAttack, a novel jailbreak attack framework. QueryAttack extracts three query components from a query in natural language, fill them into query templates of various styles, and leverages the obtained query code to bypass the target LLM’s safety alignment. 
Although QueryAttack does not encrypt the outputs, our extensive evaluation shows that it still effectively bypasses the defenses of mainstream LLMs and can withstand common defense methods.
Besides, to defend against QueryAttack, we develop a tailored defense strategy. Experimental results demonstrate that it effectively enhances the ability of mainstream models to resist such attacks.
% Therefore, we strongly advocate for further research into developing more robust safety alignment techniques.


\section*{Limitations}
\label{sec:limitaion}
The limitation of our study is the insufficient discussion of related defense mechanisms. As mentioned in the related work (Section~\ref{sec:related work}), research on jailbreak attack defenses has explored various potential countermeasures. However, we are unable to cover all of them. A more comprehensive investigation of defense methods would provide deeper insights into the generalizability of QueryAttack and help identify more effective mitigation techniques against such attacks.


\section*{Ethics Statement}
\label{sec:ethics_statement}
Please note that all experiments are conducted within controlled laboratory environments. We do not support the application of our QueryAttack in real-world scenarios. The primary objective of our work is to raise awareness about the security concerns related to the safety of LLMs. We aim to highlight the potential vulnerabilities about QueryAttack and encourage practitioners to prioritize the development of robust security measures and trustworthy deployment practices.


\bibliography{acl_latex}

\appendix


\section{Attack Examples of QueryAttack}
\label{app:Attack Examples of QueryAttack}

\subsection{Success Examples}
\label{app:success example}
Figure~\ref{fig:successexample} illustrates a simplified attack process of QueryAttack. The adversary first utilizes query learning in context. Then, the translated query is used to launch the attack.
\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{successexample.pdf}
    \caption{An attack example of QueryAttack using a C++ style template.}
    \label{fig:successexample}
\end{figure}

\subsection{Prompts of Query Learning}
\label{app:In-context Learning}
Figure~\ref{fig:zero-shot} and Figure~\ref{fig:few-shot} present the prompts used for few-shot and zero-shot learning in SQL style. For zero-shot learning, we establish an educational context to encourage the model to generate more detailed information related to the risky knowledge.
\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{zero-shot.pdf}
    \caption{The zero-shot prompts for Query Understanding (SQL style).}
    \label{fig:zero-shot}
\end{figure}
\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{few-shot.pdf}
    \caption{The few-shot prompts for Query Understanding (C++ style).}
    \label{fig:few-shot}
\end{figure}

\clearpage

\section{Experimental Details}
\label{app:Experimental details}
\subsection{Human Evaluation on GPT-4 Judge}
\label{app:gpt4judge}
To confirm the effectiveness of GPT evaluation, we conduct an experiment with human evaluators to assess the responses of LLMs.
We randomly select 100 outputs from GPT-4-1106 under the CodeAttack. Three authors independently classify these outputs as either successful or failed attacks based on OpenAI’s policy, and the final results are determined by majority vote. 
The manual verification results show a 93\% agreement with the GPT-4 based evaluation (which also uses OpenAI’s policy as the benchmark). The result demonstrates the effectiveness of GPT-4 based evaluation.

\subsection{Defense Settings}
\label{app:defense}
\textbf{Paraphrase}. We follow the setting of~\citep{artprompt} and use the GPT-3.5 model and the exact same prompt to complete the paraphrase task. The paraphrased translated template is then used as a replacement to launch the attack in the last step~\ref{fig:overview}.

\textbf{Rand-Insert, Rand-Swap, and Rand-Patch}. We follow the defense and evaluation setting of~\citep{SmoothLLM}, but choose the hyperparameter settings that give the best defensive performance reported in their paper.

\textbf{Cross-lingual alignment prompting based defense}. Figure~\ref{fig:defense} shows the cross-lingual alignment prompts we use for each attack sample. These prompts guide the target model to analyze the intent of the input query and rewrite it as text in natural language. Then, the target model is asked to generate response according to the query described in the text in natural language.
\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{defense.pdf}
    \caption{Prompt for cross-lingual alignment based defense.}
    \label{fig:defense}
\end{figure}

\subsection{Ethic-related Disclaimer Removal}
\label{app:removal}
Figure~\ref{fig:removal} shows a malicious response generated by the O1 model~\citep{o1} using zero-shot prompt~\ref{fig:zero-shot}. The response fully complies with the adversary’s query (\textbf{Develop a program that can exploit vulnerabilities in IoT devices and take control of them}). 
Besides, it provides concise mitigation measures as required and includes brief ethic-related disclaimers in the the response (highlighted in purple). 
However, these measures are clearly ineffective, as the model has already provided the detailed information queried by the adversary. 
Moreover, offering mitigation strategies may further enhance the adversary’s malicious knowledge to develop stronger attack vectors. Therefore, it should be considered a successful attack.

Nevertheless‌, we observe that these disclaimers and mitigation measures could influence the evaluation of GPT-4 Judge~\citep{gpt4-judge}, making it more likely to assess the response as compliant with OpenAI’s policies. This could result in the harm score dropping from 5 to 4 (or even lower). Therefore, before submitting responses for GPT-4 Judge, we remove the initial disclaimer, the mitigation / education part and subsequent content to extract the harmful content.
\begin{figure*}[h]
    \centering
    \includegraphics[width=\linewidth]{removal.pdf}
    \caption{An example of a malicious response generated by GPT-O1~\citep{o1}. The response is clearly malicious, containing a lot of harmful knowledge, but includes concise mitigation measures as required and provides educational disclaimers at the beginning and end (highlighted in purple).}
    \label{fig:removal}
\end{figure*}


\section{Prompts for Extracting Query Components}
\label{app:component}
We use GPT-4-1106~\citep{gptseries} to perform the component extraction task, employing few-shot learning in the process. To ensure the LLM remains focused on the natural language processing task, we refine the task details and require the model to simultaneously assess the risk level of the extracted components, as shown in Figure~\ref{fig:component}. 
The risk level of the components should not be low to prevent the model’s defense mechanisms from replacing malicious phrases with benign ones, which could affect translation quality. 
% To reduce costs, adversaries may also choose alternative LLMs for this task.
\begin{figure*}[h!]
    \centering
    \includegraphics[width=\linewidth]{component.pdf}
    \caption{The prompts for extracting query components from text in natural language using GPT-4-1106.}
    \label{fig:component}
\end{figure*}



\end{document}
