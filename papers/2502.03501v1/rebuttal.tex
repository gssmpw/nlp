\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage[rebuttal]{cvpr}

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage[table]{xcolor}
\usepackage{tabularx}
%\usepackage{geometry}
%\geometry{a4paper，bottom=1cm}


% Import additional packages in the preamble file, before hyperref
\input{preamble}

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\definecolor{cvprblue}{rgb}{0.21,0.49,0.74}
\usepackage[pagebackref,breaklinks,colorlinks,allcolors=cvprblue]{hyperref}

% If you wish to avoid re-using figure, table, and equation numbers from
% the main paper, please uncomment the following and change the numbers
% appropriately.
%\setcounter{figure}{2}
%\setcounter{table}{1}
%\setcounter{equation}{2}

% If you wish to avoid re-using reference numbers from the main paper,
% please uncomment the following and change the counter value to the
% number of references you have in the main paper (here, 100).
%\makeatletter
%\apptocmd{\thebibliography}{\global\c@NAT@ctr 100\relax}{}{}
%\makeatother

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\paperID{16541} % *** Enter the Paper ID here
\def\confName{CVPR}
\def\confYear{2025}
\renewcommand{\thetable}{\Alph{table}} 
\renewcommand{\baselinestretch}{0.97}

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
% \title{\LaTeX\ Guidelines for Author Response}  % **** Enter the paper title here

% \maketitle
\thispagestyle{empty}
\appendix

To ALL Reviewers,we very much appreciate your precious time and great efforts to provide us with thoughtful \& constructive feedback. In view of space,we primarily address all the major weaknesses (W).

%%%%%%%%% BODY TEXT - ENTER YOUR RESPONSE BELOW
% \section{To LL65}

\textcolor{blue}{{\bf To LL65:}} {\bf W1:} Using MedSAM (box), we generated low-quality masks (Avg. Dice=82.4\% to the ground truth), with a typical example is visualized in \cref{fig:fig1} A.(c). As shown in \cref{tab:tab1}, Our model remains robust when using low-quality masks with minimal drop in avg dice while outperforming \(2^{nd}\)-best model of AutoSAM. As varying quality of pre-annotation masks may happen in real-world clinical practice, we analyzed our model performance under two additional lower-quality masks using empty masks and masks generated by SAM (box) (Dice=0\% \& 66.5\%, respectively, compared to ground-truth, as visualized in \cref{fig:fig1} A.(d-e)). Results of \cref{fig:fig1} A.(f) show a gradual improvement as masks quality increases, with significant drop occurring only under 0\% Dice Mask. Detailed results and visualizations of each dataset will be in the Suppl. Material.

% -----------------------------------------------------------------
\begin{table}[t]
\centering
\fontsize{8}{10}\selectfont
\setlength{\abovecaptionskip}{-2pt}
\captionsetup{font=footnotesize}
\caption{Performance under high- and low-quality pre-annotated masks.}
\begin{tabularx}{0.99\columnwidth}{p{1.5cm}|p{2.1cm}|p{2.1cm}|c}
\toprule
Method & REFUGE2-Disc & REFUGE2-Cup & Avg \\ \hline
Ours (high) & 88.2 & 85.6 & 86.9 \\
Ours (low) & 87.7 \footnotesize \textcolor{red}{(-0.5)} & \cellcolor{orange!30}85.3 \footnotesize \textcolor{red}{(-0.3)} & \cellcolor{orange!30}86.5 \footnotesize \textcolor{red}{(-0.4)} \\ \hline
AutoSAM & \cellcolor{orange!30}87.9 & 84.6 & 86.3 \\
\bottomrule
\end{tabularx}
\label{tab:tab1}
\vspace{-10.5pt}
\end{table}
% -----------------------------------------------------------------
%-------------------------------------------------------------------------
 \begin{figure}
  \centering
  \setlength{\abovecaptionskip}{0pt}
   \includegraphics[width=1\linewidth]{Rebuttal_Figure/Combine.pdf}
   \captionsetup{font=footnotesize}
   \caption{Visualization for W1\&2 for Reviewer 1. In A.(b-e), \% indicates the Dice between pre-annotated masks and ground truth.}
   \label{fig:fig1}
   \vspace{-23pt}
\end{figure}

{\bf W2:} Within each visualization in \cref{fig:fig1} B, the model selectively emphasizes areas of the support pair that are highly relevant to the target image (fetal head), demonstrating its alignment with our design intention described in lines 109-111 of the main text. Furthermore, compared to a random support pair (low relative), the model focuses more on the yellow-arrowed areas in the high relative support pair.

{\bf W3:} We appreciate the reviewer’s recognition of our potential. \cref{tab:tab3} shows our superiority over the \(2^{nd}\)-best model (AutoSAM) on Synapse CT dataset.

\textcolor{blue}{{\bf To ATzZ:}} We greatly appreciate your very mindful and professional comments. {\bf W1:} {\bf 1.} Pre-trained weights of Vision Mamba were not used. {\bf 2.} Equation 3 is correct; we will rectify in \textcolor{orange}{Fig. 3}. {\bf 3.} We sincerely apologize for the omission of citations and will add it in the final version. We would like to clarify that our contributions lie in designing a strategy for generating prompts from non-target data and developing the CSM and CCM to enhance interaction, rather than stating the novelty of Equation 6. {\bf 4.} We replaced the Cross-Attention with L2-Attention, causing a drop from 84.4\% to 81.9\%, likely due to \(\mathbf{E}_{\text{ctx}}\) in CCM included user intents, unlike CSM's pure image features where Euclidean distance-based L2-Attention is sufficient.

{\bf W2:} {\bf 1.} We determined based on the training approach outlined in their public code. E.g., It is stated that AutoSAM is limited in supporting single-object segmentation, requiring at least 5 separate training runs to segment 5 different objects. In contrast, our model enables multi-object and cross-modality segmentation. We train separate models for different modalities to achieve optimal performance at low cost (only 16 samples). {\bf 2.} We will provide details of \textcolor{orange}{Fig. 7} in Suppl. Material. {\bf 3.} In main text (lines 221-222), we define \(N\) as the number of objects and \(K\) as the number of support pairs. Thus, \(N\) is fixed for each dataset, and \(K\) corresponds to \(G\) in \textcolor{orange}{Fig. 7}. We will unify \(K\) and \(G\) in the final version. {\bf 4.} We randomly sampled 16 patient-level data for training, leaving the remainder as the test set. {\bf 5.} Image and video tasks were trained separately. For the 3 image datasets, we trained a single model on STARE and REFUGE2 and a separate model on the remaining dataset. {\bf 6.} Both training samples and support sets were selected randomly. {\bf 7.} We apologize for the mistake in our writing and would like to clarify that our model is not based on Med-SA. We will revise the wording to: "Without these two modules, our model’s performance drops significantly."

{\bf W3:} Rigorous suggestion. Upon further consideration, we agree that this statement is more appropriate for the discussion section. We will remove it in the final version.

\begin{table}[t]
\centering
\fontsize{8}{10}\selectfont
\setlength{\abovecaptionskip}{-2pt}
\captionsetup{font=footnotesize}
\caption{Model performance on CT. The $\bf best$ results are bolded.}
\begin{tabular}{c|c|c|c|c}
\toprule
Method & Spleen &  Liver & Stomach & Average \\ \hline
AutoSAM & 68.8 & 88.1 & 52.7 & 69.9 \\ \hline
Ours & {\bf 81.3} & {\bf 93.8} & {\bf 69.1} & {\bf 81.4} \\
\bottomrule
\end{tabular}
\label{tab:tab3}
\vspace{-13pt}
\end{table}
% -----------------------------------------------------------------
% -----------------------------------------------------------------
\begin{table}[t]
\centering
\setlength{\abovecaptionskip}{-2pt}
\fontsize{8}{10}\selectfont
\captionsetup{font=footnotesize}
\caption{Performance of MedSAM\&SAM with conventional\&our prompts}
\begin{tabularx}{1\columnwidth}{p{2.1cm}|X|X|X}
\toprule
Method & REFUGE2-Disc &  REFUGE2-Cup & STARE-Vessel \\ \hline
SAM (box) & 54.2 & 71.6 & 20.5 \\ 
MedSAM (box) & 87.2 & 81.3 & 20.1 \\ \hline
SAM (our) & 88.2 \footnotesize \textcolor{green}{(+34)} & {\bf 85.6} \footnotesize \textcolor{green}{(+14)} & {\bf 78.9} \footnotesize \textcolor{green}{(+58.4)} \\ 
MedSAM (our) & {\bf 94.3} \footnotesize \textcolor{green}{(+7.1)} & 83.8  \footnotesize \textcolor{green}{(+2.5)} & 59.4 \footnotesize \textcolor{green}{(+39.3)} \\
\bottomrule
\end{tabularx}

\label{tab:tab5}%tab:compare_image
\vspace{-23pt}
\end{table}

\textcolor{blue}{{\bf To ZPpt:}} {\bf W1:} We would like to clarify that we evaluated SAM and MedicalSAM (BOTH with conventional prompt). The superior performance of MedicalSAM is likely due to its training on a massive medical dataset. Yet, when SAM uses our advanced prompt with only 16 samples, it achieves comparable results as MedicalSAM. Our advanced prompt also enhances MedicalSAM (\cref{tab:tab5}); as it appears beyond the study scope, we did not emphasize this in main text.

{\bf W2:} We would like to clarify that our research is motivated by urgent needs of real-world medical scenarios, therefore natural images is not the primary study focus.

{\bf W3:} We would like to clarify that our method differs significantly from VRP-SAM in both techniques and motivations. In our method, we address the challenges of automated prompting and improved interactions as recognized by Reviewers (LL65, ATzZ), we devised proxy-prompt and a novel three-step strategy to attain this goal. In contrast, the mechanism and point of focus in VRP-SAM are different, we will discuss this and cite this work in final version.


%%%%%%%%% REFERENCES
% {
%     \small
%     \bibliographystyle{ieeenat_fullname}
%     \bibliography{main}
% }

\end{document}