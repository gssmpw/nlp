\section{Introduction}
\label{sec:intro}

Under the paradigm shifts in Large-scale Vision Models (LVMs), Segment Anything Model (SAM)~\cite{kirillov2023segment} and SAM 2~\cite{ravi2024sam} have been introduced as generalized foundation models for segmenting and tracking any objects on image and video data, respectively. These models provide a certain degree of interactive segmentation capacity as users can segment any target object(s) according to their needs by using a single model in-one-go. Such capabilities are achieved by leveraging the concept of prompts~\cite{wang2023review}, such as points, boxes, or masks, waiving the traditional demand for massive manually annotations. Instead, users are only required to input prompts directly on target images or video frames.

\begin{figure}[t]
  \centering
  \setlength{\abovecaptionskip}{3pt}
   \includegraphics[width=0.99\linewidth]{Figure_Table/Figure_1.pdf}
   \caption{Illustration of comparison without/with PP in (a-b) SAM 2 using real-time ultrasound frames of 1 subject; and (c-d) SAM using a fundus retina dataset of 3 subjects.}
   \label{fig:Figure1}
   \vspace{-15pt}
\end{figure}

Notwithstanding, widespread clinical adoptions of these LVMs have been substantially impeded by the soaring medical demands for ``automated prompting'' and ``high-level human-model interactions'' when it comes to downstream medical tasks, particularly real-time imaging-guided interventions. The current prompting strategies are sub-optimal for two key reasons. First, medical image/video data entails an overwhelmingly huge variations in terms of complexity of the target object(s) to be segmented; segmenting such structures (\eg vessels) using existing prompts can be practically challenging. As illustrated in \cref{fig:Figure1}(c), for instance, manually inputting either box prompt (Case 1) or point prompt (Case 3) generates poor results; while adopting mask prompt (Case 2) performs well, it is highly tedious, exhaustive and knowledge-demanding for precise mask prompt formation. Second, users are required to input prompt for every single target image/video frame, which is a manual trial-and-error process, tedious, and not user-friendly. Clinical burden becomes exceedingly prominent when segmenting intricated structures \cref{fig:Figure1}(c) and/or massive datasets, especially in resource-limited clinics. Therefore, there is a pressing demand for automated prompt generation to accommodate various clinical needs.

\begin{figure}
  \centering
  \setlength{\abovecaptionskip}{3pt}
   \includegraphics[width=0.99\linewidth]{Figure_Table/Figure_2.pdf}
   \caption{Schematic differences of traditional prompt encoder and our proposed PPG in SAM 2 (a-b) and SAM (c-d).}
   \label{fig:Figure2}
   \vspace{-15pt}
\end{figure}

Apart from this, the existing SAM-based models~\cite{shaharabany2023autosam, zhang2023customized, wu2023self} are inadequate to support the growing demand of high-level human-model interaction to accommodate multifarious clinical goals and high disparity in preferences of clinical users. These models segment target object(s) via training on specific single/multiple object(s). Yet, they do not adapt well to changes in the user’s preferences. For instance, in fundus imaging, optic dis/cup weights more for glaucoma detection~\cite{thakur2018survey}, while vessels are prioritized for assessing retinal vascular occlusion~\cite{sekhar2008automated}. Even though these objects may appear within the same image, switching segmentation task from the optic disc/cup to vessels necessitates retraining the model. In real-world clinics where tasks are greatly diverse, creating separate models for individual tasks is practically challenging and computational demanding. Therefore, it is imperative to reinforce human-model interaction capacity by allowing users to flexibly adjust prompts for satisfying various clinical demands without the need for model retraining. For example, in intrapartum ultrasound as illuminated in \cref{fig:Figure1}(a), these properties would provide high flexibility for users to segment fetal head (FH) alone or FH\&pubic symphysis (PS) at any time points \(\{t_{1}, t_{2}\}\) to achieve measurement of fetal head rotation or fetal position~\cite{ghi2018isuog}.

Confronted with these, we propose a novel Proxy Prompt (PP), which can be automatically generated from a ``non-target'' data (i.e., image/video frame of subjects other than the one under examination, such as from retrospective datasets) with a pre-annotated mask. This PP strategy is distinct from the existing prompting methods where prompting can only be made on ``target'' data, in a manual manner. As illustrated in \cref{fig:Figure1}(d), only one annotated image is required in using PP, tremendously streamlining workflow by waiving the prerequisite of providing separated prompts for every image/frame. Moreover, clinicians can freely switch segmentation tasks by adjusting the support-pair input anytime during examination without model retraining nor adopting different models, as shown in \cref{fig:Figure1}(b). Working in tandem with PP, we innovated a Proxy Prompt Generator (PPG) to reform SAM and SAM 2 for image and video data, respectively. Compared to SAM and SAM 2 in \cref{fig:Figure2} (a\&c), we employed high dimensional embedding from the PPG as prompts in \cref{fig:Figure2} (b\&d).

The core design of PPG lies in the novel Contextual Selective Module (CSM) and Contextual Colorization Module (CCM), which are dedicatedly configurated for automated prompting and high-level human-model interactions. First, CSM is introduced to enable adaptive selection of the most representative contextual information from ``non-target'' data for the ``target'' data, achieving cross-data guidance. Besides, CSM contains Vision Mamba, Bridge Unit, and Selective Map, implementing a 3-step selection process: (i) input-driven, (ii) object-guided, and (iii) target image/frame relevance selection to support both cross-video tasks and cross-image prompting. 

Second, CCM is devised to reinforce human-model interaction, enabling the model to interpret diverse user needs as indicated by different masks (\eg single/multiple objects). This aim is achieved by leveraging dual-reverse cross-attention to enhance the representation of contextual embedding. Finally, the PP, effectively capturing specific object features, is generated.
Such PPG-based strategy presents a simple yet efficient architecture to enhance clinical-friendliness of SAM and SAM 2, even in few-shot settings. Furthermore, with all original parameters frozen, our design can function as a flexible, plug-and-play module and can continuously adapt to the ever-evolving LVMs beyond SAM and SAM 2.

We conducted extensive experiments across several popular image and video datasets, validating the superior performance and stability of our proposed approach. Our main contributions are outlined below:

1. We propose a novel PP to enhance user-friendliness and precision of SAM and SAM 2 by equipping them with the capacity of automated prompting and high-level human-model interaction.

2. We devise CSM for adaptive selection of the most representative contextual information from “non-target” data to guide segmentation on “target” data, enabling effective cross-image/video prompting, waiving the need to execute prompting for every single target data, and minimizing experience-/expertise-derived variability in prompt quality.

3. We configurate CCM for enhancing the expressiveness of contextual embeddings to interpret and accommodate diverse clinical demands and preference of end users, thereby reinforcing model-human interactions.

4. Extensive experiments show that our model achieves state-of-the-art (SOTA) performance and is comparable to traditional segmentation models trained on full data volumes, even with only 16 image-mask pairs for SAM and SAM 2 training. Moreover, our strategy is of high potential to adapt to the iterative evolution of LVMs for medical tasks in the future.
