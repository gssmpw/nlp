%-------------------------------------------------------------------------
 \begin{figure*}
  \centering
  \setlength{\abovecaptionskip}{3pt}
   \includegraphics[width=0.9\linewidth]{Figure_Table/Figure_3.pdf}
   \caption{Designed Proxy Prompt Generator for both SAM 2 and SAM. Our key designed focus on the Contextual Selective Module and the Contextual Colorization Module. The Encoder and Decoder refer to the original structures, which are frozen.}
   \label{fig:Figure3}
   \vspace{-15pt}
\end{figure*}
%-------------------------------------------------------------------------
\section{Method}
\label{sec:method}

Our proposed PPG, compatible with SAM and SAM 2, is illustrated in \cref{fig:Figure3}. Utilizing support image-mask pairs, this network not only enables auto-segmentation on target images or videos but also builds human-model interaction based on user-provided mask. For a target \(\mathbf{X}\), we define a support set \({S} = \{ {s}_i \}_{i=1}^N\) to assist in the segmentation of \(N\) objects within the target \(\mathbf{X}\). Each \({s}_i\) comprises \(K\) image and mask pairs denoted as \({s}_i = \{ (\mathbf{I}^j, \mathbf{M}_i^j) \}_{j=1}^K\), where each image \(\mathbf{I}^j \in \mathbb{R}^{3 \times H^0 \times W^0}\) and the corresponding mask \(\mathbf{M}_i^j \in \mathbb{R}^{H^0 \times W^0}\). First, the target \(\mathbf{X}\) and support images \(\{ \mathbf{I}^j \}_{j=1}^K\) are processed by a shared image encoder, with LoRA~\cite{hu2021lora} applied to adapt with medical tasks, resulting in \(\mathbf{F}_x \in \mathbb{R}^{ C \times H \times W}\) and \(\mathbf{F}_{\text{sup}} \in \mathbb{R}^{ K \times C \times H \times W}\). In parallel, our Contextual Selective Module (CSM) processes the support pairs through a three-layer selection mechanism to identify the most valuable contextual information for target segmentation, thereby enabling cross-frame prompting (\cref{sec:Selective}). Secondly, the contextual information \(\mathbf{E}_{\text{ctx}}\) is refined within the Contextual Colorization Module (CCM), ``coloring'' target features with user specifications via dual-reverse cross-attention to achieve human-model interaction (\cref{sec:Colorization}). Finally, this refined high-level embedding—enriched with user-defined information and object-specific target feature—serves as a high-dimensional prompt for SAM or SAM 2 (\cref{sec:SAM 2andSAM}). In summary, our method offers three main characteristics: cross-frame guidance, human-model interaction, and high-dimensional prompting via CSM and CCM. In the following sections, we detail each module step-by-step.

%-------------------------------------------------------------------------
\subsection{Contextual Selective Module}
\label{sec:Selective}
We leverage Vision Mamba~\cite{zhu2024vision}’s selective input capabilities, establish communication between different objects, and design a filtering mechanism to refine the contextual embedding. As illustrated in \cref{fig:Figure3}, support pairs \(\{ {s}_i \}_{i=1}^N\) are initially processed by the Vision Mamba encoder to extract feature \(\mathbf{V}\) with input-adaptive parameters, marking the first selective step. Secondly, within the designed Bridge Unit, we enable inter-object communication across the concatenated features, thereby selecting information from object-level to form \(\mathbf{A}_{\text{agg}}\). Thirdly, features (\(\mathbf{F}_{\text{x}}\)) and (\(\mathbf{F}_{\text{sup}}\)) are used to compute a \(\mathbf{Selective}\) map. Each row in this map representing a given patch from a support image will positively influence a specific patch in \( \mathbf{X} \). This selective map subsequently filters the aggregated information \(\mathbf{A}_{\text{agg}}\) to derive the most valuable information as the contextual embedding \(\mathbf{E}_{\text{ctx}}\). The following sections will detail each component of this pipeline.

%-------------------------------------------------------------------------
{\bf First Selection Step: Vision Mamba.} We use the encoder of Vision Mamba (Vim) ~\cite{zhu2024vision} as the contextual encoder, due to its natural alignment with our needs through its input-dependent selection mechanism. This mechanism enables parameters (i.e., \( \mathbf{\overline{A}} \), \( \mathbf{\overline{B}} \) and \(\mathbf{C}\))  influencing interactions within the image to adapt as functions of the input, allowing features to be extracted based on the input itself. Specifically, Vim first flattens the input 2-D image into patches and form the token sequence \( x \), which is progressively transformed into the output token sequence \( y \) according to the following formula, ultimately being encoded by us into the feature matrix.
\begin{equation}
  h_{t} = \mathbf{\overline{A}}h_{t-1} + \mathbf{\overline{B}}x_{t}
\end{equation}
\begin{equation}
  y_{t} = \mathbf{C}h_{t}
\end{equation}
where \( t \) represents the timestep and \( h \) denotes the latent state. The parameters \( \mathbf{\overline{A}} \), \( \mathbf{\overline{B}} \) and \(\mathbf{C}\) are generated depending on the input \( x \), rather than being input-invariant, thus allowing us to selectively encode the input image as our first step in the selection strategy. For more details, refer to~\cite{zhu2024vision}.

Given the support set \({S} = \{ {s}_i \}_{i=1}^N\), we fix a value \(K\) such that each \({s}_i = \{ (\mathbf{I}^j, \mathbf{M}_i^j) \}_{j=1}^K\) provides \(K\) support pairs \((\mathbf{I}^j, \mathbf{M}_i^j)\). For the \(j\)-th pair \(\{ (\mathbf{I}^j, \mathbf{M}_i^j) \}_{i=1}^N\) among the \(N\) support subset \({s}_i\), each masks \(\mathbf{M}_i^j\) correspond to one same image \(\mathbf{I}^j\). As a result, \(\mathbf{I}^j\) is concatenated with \(\mathbf{M}_i^j\) and fed into the contextual encoder, resulting in \(N\) feature matrices \(\mathbf{V} \in \mathbb{R}^{K \times C^{v} \times H \times W}\), where \(C^{v}\) indicates the channel of \(\mathbf{V}\). The process of extracting the feature matrix \(\mathbf{V}\) can be breifly represented by the following equation:
\begin{equation}
\mathbf{V} = \text{Vision\,Mamba\,Encoder}(\text{concat}(\mathbf{I}^j, \mathbf{M}_i^j))
\end{equation}

%-------------------------------------------------------------------------
{\bf Second Selection Step: Bridge Unit.} Subsequently, in the Bridge Unit, we initially duplicate \(N\) times the obtained feature \(\mathbf{F}_{\text{sup}}\) and concat with \(\mathbf{V}\) of each object along the channel dimension to further aggregate the features. Most importantly, we employ convolutional block attention module(CBAM)~\cite{woo2018cbam} along the concatenated channel dimension to facilitate implicit inter-target communication, allowing the model to select key features across multiple objects, thereby enhancing cross-target contextual understanding. Additionally, two ResBlocks ~\cite{he2016deep} are used to prevent further feature dimension expansion. After the final block, the feature is flattened to form \(\mathbf{A}_{agg}\). This process is summarized as follows:

\begin{equation}
\mathbf{F}_{\text{cat}} = \text{concat}(\mathbf{F}_{\text{sup}}, \mathbf{V})
\end{equation}
\begin{equation}
\mathbf{A}_{agg} = \text{ResBlock}_2(\text{CBAM}(\text{ResBlock}_1(\mathbf{F}_{\text{cat}})))
\end{equation}
where  \(\mathbf{F}_{\text{cat}} \in \mathbb{R}^{K \times (N \times (C^{v} + C)) \times H \times W}\) and \(\mathbf{A}_{agg} \in \mathbb{R}^{N \times C \times (K \times H \times  W)}\) represents the aggregated features of the support set.

%-------------------------------------------------------------------------
{\bf Third Selection Step: Selective Map.} In order to select the key information for the target features, we compute the selective map based on \(\mathbf{F}_x\) and \(\mathbf{F}_{\text{sup}}\). First, \(\mathbf{F}_x\) and \(\mathbf{F}_{\text{sup}}\) are flattened into \(\mathbf{\hat{F}}_x \in \mathbb{R}^{ C \times (H \times W)}\) and \(\mathbf{\hat{F}}_{\text{sup}} \in \mathbb{R}^{ C \times (K  \times H \times W)}\), respectively. Then, map is calculated through matrix multiplication, and the calculation is performed using the following equation:
\begin{equation}
\mathbf{Selective} = \frac{2 \cdot (\mathbf{\hat{F}}_{\text{sup}}^T \cdot \mathbf{\hat{F}}_x) - \mathbf{\hat{F}}_{\text{sup}}^2}{\sqrt{C}} 
\end{equation}
where \(C\) is the channel dimension of \(\mathbf{\hat{F}}_{\text{sup}}\) and \( \mathbf{Selective}  \in \mathbb{R}^{ (K  \times H \times W) \times (H \times W)}\). Afterward, the computed \(\mathbf{Selective}\) is normalized using the softmax to ensure that the contribution values conform to a probability distribution. The resultant information matrix can be represented as follows:
\begin{equation}
\mathbf{E}_{\text{ctx}} = \mathbf{A}_{agg} \cdot Softmax(\mathbf{Selective})
\end{equation}
where \(\mathbf{E}_{\text{ctx}}\) serves as the contextual (ctx) embedding of the key information from the support set, and \(\mathbf{E}_{\text{ctx}} \in \mathbb{R}^{N \times C \times (H \times W)}\).

%-------------------------------------------------------------------------
\subsection{Contextual Colorization Module}
\label{sec:Colorization}
The CCM is proposed to interpret user intent from the contextual embedding derived from the support set, enabling human-model interaction. Conceptually, this process is similar to ``colorizing'' the target image based on the support mask. Unlike the CSM, which focuses on selecting contextually representative information tailored to the target image, this module emphasizes dynamically refining target features on the ground of the contextual embedding via cross-attention, thereby facilitating a deeper understanding of the user’s segmentation intent. Before entering the module, \(\mathbf{\hat{F}}_{\text{x}}\) is duplicated \(N\) times to align its dimensions with \(\mathbf{E}_{\text{ctx}}\).

The CCM consists of four identical blocks, one of which is detailed in \cref{fig:Figure3}. In each block, \(\mathbf{\hat{F}}_{\text{x}}\) and \(\mathbf{E}_{\text{ctx}}\) are each passed through a learnable projection layer to reduce their dimensions to \(\mathbf{\hat{F}}_{\text{x}} \in \mathbb{R}^{N \times C \times (H \times W / 2)}\) and \(\mathbf{E}_{\text{ctx}} \in \mathbb{R}^{N \times C \times (H \times W / 2)}\), respectively. Subsequently, \(\mathbf{E}_{\text{ctx}}\) is integrated into the target image features to guide the model in identifying specific regions expected to be ``colored''. This integration occurs by adding cross-attention-processed information back into \(\mathbf{\hat{F}}_{\text{x}}\), followed by a feed-forward layer (FFN)~\cite{vaswani2017attention} for contextual reasoning, yielding \(\mathbf{\hat{F}}_{\text{x\_next}}\), which then serves as the \textbf{next} target feature input for the next block. This process is represented as follows:

\begin{equation}
\mathbf{\hat{F}^{'}}_{\text{x}} = \text{Add\&Norm}(\text{Cross Attention}(\mathbf{\hat{F}}_{\text{x}}, \mathbf{E}_{\text{ctx}}))
\end{equation}
\begin{equation}
\mathbf{\hat{F}}_{\text{x\_next}} = \text{Add\&Norm}(\text{FFN}(\mathbf{\hat{F}^{'}}_{\text{x}}))
\end{equation}
Subsequently, \(\mathbf{E}_{\text{ctx}}\) reads from the updated target image features \(\mathbf{\hat{F}}_{\text{x\_next}}\) through a reversed cross-attention layer to pinpoint features essential for matching object information in the contextual embedding. In the followed FFN, the context embedding, equipped with object-specific feature representation, undergoes further enhancement to strengthen its segmentation guidance capability. The resulting \(\mathbf{E}_{\text{ctx\_next}}\) then serves as the \textbf{next} context embedding for the next block. This process can be expressed as follows:

\begin{equation}
\mathbf{E}^{'}_{\text{ctx}} = \text{Add\&Norm}(\text{Cross Attention}(\mathbf{E}_{\text{ctx}}, \mathbf{\hat{F}}_{\text{x\_next}}))
\end{equation}
\begin{equation}
\mathbf{E}_{\text{ctx\_next}} = \text{Add\&Norm}(\text{FFN}(\mathbf{E}^{'}_{\text{ctx}}))
\end{equation}
After passing through the final block, the iteratively refined context embedding fully comprehends the user-defined segmentation intent from the support set, as well as the corresponding image features derived from the target image. This enhanced context embedding subsequently serves as \(N\) prompts \(\mathbf{P}\) for the SAM or SAM 2 model, where \(\mathbf{P} \in \mathbb{R}^{N \times (H \times W / 16) \times C}\).

%-------------------------------------------------------------------------
\subsection{Loss Function}
\label{sec:SAM 2andSAM}
For SAM, the image features \(\mathbf{F}_x\) extracted from the target image \(\mathbf{X}\) is fed into the Decoder with the prompt \(\mathbf{P}\) to generate the final output, represented by the following equation:
\begin{equation}
\text{Output} = \text{Decoder}(\mathbf{F}_x, \mathbf{P})
\end{equation}
where \(\text{Output} \in \mathbb{R}^{N \times H^0 \times W^0}\). For SAM 2, our approach directly inputs the high-dimensional vector \(\mathbf{P}\) into memory attention as prompt embeddings, thus \(\mathbf{P}\) also can be considered as memory extracted across video frames. For loss settings, we simply follow the design of SAM ~\cite{kirillov2023segment} to supervise the mask prediction with a Dice loss ~\cite{milletari2016v}, using the following formula:
\begin{equation}
\mathcal{L} = 1 - \frac{2 \sum_{i=1}^{D} p_i g_i}{\sum_{i=1}^{D} p_i + \sum_{i=1}^{D} g_i}
\end{equation}
where \(p_i\) and \(g_i\) represent the probability of pixel \(i\) in the predicted mask and the label of pixel \(i\) in the ground truth mask, respectively. The number of pixels involved in the computation of the Dice loss is denoted by \(D\).
