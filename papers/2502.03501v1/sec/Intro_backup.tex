\section{Introduction}
\label{sec:intro}

To improve the efficiency of manual segmentation in healthcare, models tailored for specific targets or specific medical imaging modalities are often developed to enable automatic segmentation as the current paradigm~\cite{rayed2024deep}. Instead of training specific models for each task, a universal model for image and video segmentation is clearly preferable. Segment Anything(SAM)~\cite{kirillov2023segment} and SAM2~\cite{ravi2024sam}, as large-scale vision models (LVM), introduce a new paradigm through the concept of prompts. SAM and SAM2, trained on vast natural image datasets, utilize prompts such as points, boxes, or masks on target images or frames, thereby achieving interactive segmentation capability across various tasks.

\begin{figure}[t]
  \centering
   \includegraphics[width=0.99\linewidth]{Figure_Table/Figure_1.pdf}
   \caption{Comparison of SAM2 and SAM with Proxy Prompt Enhancement. 'User-defined N' indicates the number of objects the user expect to segment. All segmentation of cases in (c-d) are shown in blue, except for case 3 in (c) is shown in highlight.
   }
   \label{fig:Figure1}
\end{figure}


However, when applying SAM and SAM2 to downstream medical image and video segmentation tasks, several challenges still hinder widespread clinical adoption. First, medical tasks have a high demand for the automation of prompt generation, while SAM and SAM2 require users to manually generate prompts for each target image or frame. Clinicians often work with vast amounts of raw data~\cite{azad2024medical}, especially in high-demand with resource-limited regions, sometimes thousands of images, making even a single-point prompt per image imposes a tedious workload on medical staff. Moreover, segmenting intricate structures, such as vessels (as shown in \Cref{fig:Figure1}(c)), requires precise and multiple prompts to avoid ambiguity and ensure accurate segmentation, increasing the burden of manual prompting. Some other SAM-based works~\cite{shaharabany2023autosam, zhang2023customized, wu2023self} also put efforts into enabling automated medical segmentation by training models either on object-specific datasets or to simultaneously segment all objects without allowing user-adjustments. However, these approaches overlook the second challenges: they sacrifice human-model interaction, which remains crucial given multifarious clinical goals and the preferences of different medical experts. For instance, in fundus imaging, where vessels, the optic disc, and optic cup overlap, the optic disc/cup is prioritized for doctor A in glaucoma detection~\cite{thakur2018survey}, while vessels are crucial for doctor B in assessing diabetic retinopathy~\cite{sekhar2008automated}. Clinicians need the flexibility to switch segmentation tasks by adjusting prompts, rather than retraining models for each task. Otherwise, creating multiple models for various applications becomes time-consuming and memory-intensive, and will be the prime culprit to limit the expansion of such models into real-world clinical settings. These two challenges are more prominent in real-time clinical procedures, such as intrapartum ultrasound illustrated in \Cref{fig:Figure1}(a), where both automation and human-machine interaction are crucial for segmenting fetal head (FH) or FH\&pubic symphysis (PS) at \(\{t_{1}, t_{2}\}\) to achieve measurement of fetal head rotation or fetal station~\cite{ghi2018isuog}.

\begin{figure}
  \centering
   \includegraphics[width=0.99\linewidth]{Figure_Table/Figure_2.pdf}
   \caption{Comparison of SAM2 and SAM, with and without the proxy prompt generator, for both video and image tasks. (a-b) for SAM2 while (c-d) for SAM.}
   \label{fig:Figure2}
\end{figure}

To address these issues, we propose a Proxy Prompt (PP), which can be automatically generated from a non-target data (frame in another patient’s video vs another image) with a pre-annotated mask to guide SAM or SAM2 auto-segmenting on target-data, as illustrated in \Cref{fig:Figure1}(b \& d). For segmenting multiple images as shown in (\Cref{fig:Figure1}(d)), only one annotated image is required for PP, making it much easier for clinicians to provide than prompts for each individual image. Additionally, clinicians can freely switch segmentation tasks by adjusting the support pair input at any moment, as shown in \Cref{fig:Figure1}(b). The auto-generated prompt further prevents instability in segmentation results caused by manual prompts, which may be of low quality when hastily provided during urgent surgeries. Working in tandem with the proxy prompt concept, we innovated a Proxy Prompt Generator(PPG) for clinical adoption of the emerging SAM and SAM2 for image and real-time video data. As shown in \Cref{fig:Figure2}(a), SAM2’s prompt requires a specific point or mask in any frame of the target video, while our approach leverages a support image-mask pair from another video to automatically guide segmentation in the target video, as illustrated in \Cref{fig:Figure2}(b). The process is similar for image tasks. Compared to the SAM approach shown in \Cref{fig:Figure2}(c), we used high dimensional embedding from the PPG as prompts shown in \Cref{fig:Figure2}(d).

The core of our PPG lies in Contextual Selective Module(CSM) and Contextual Colorization Module(CCM), which are designed for full automation and high-level human-machine interaction. First, to enable frames with different contextual information from two videos can achieve cross-video guidance, we introduced the CSM that adaptively selects the most representative contextual information from the non-target frame for the target frame. The CSM contains Vision Mamba~\cite{zhu2024vision}, Bridge Unit, and Selective Map, implementing a 3-step selection process of input-driven selection, object-guided selection, and target-frame relevance selection to support both cross-video tasks and cross-image prompting(\Cref{fig:Figure2}(d)). Second, to preserve model-human interactions, we configured the CCM enhancing the expressiveness of contextual embedding, thus enabling the model to interpret diverse user needs as indicated by different masks (\eg single-object or multi-object). By leveraging dual-reverse cross-attention, we ensure the enhanced embedding effectively captures the specific object feature through iterative interactions between the target feature and contextual information from distinct objects. Our architecture is simple yet efficient, leveraging Proxy Prompt Generator to enhance clinical-friendliness of both SAM and SAM2, even in few-shot settings. Furthermore, with all original parameters frozen, our design can function as a flexible, plug-and-play module and can continuously adapting to the ever-evolving LVM.

We conducted extensive experiments across several commonly used image and video datasets, validating the superior performance and stability of our approach. Our main contributions are as follows:

1. We propose a novel Proxy Prompt (PP) to boost interactive large vision models (LVMs), such as SAM and SAM2, enabling them to meet the need for interactive and automatic general models in downstream medical segmentation tasks. PP with SAM reduces prompt burden and minimizes ambiguity, especially for intricate structures, further enhancing the user-friendliness of SAM2 in scenarios like surgeries and interventions.

2. We propose a Contextual Selective Module that adaptively selects the most representative contextual information from non-target frames to guide target frames, enabling cross-video and cross-image prompting.

3. We propose a Contextual Colorization Module that enhances the expressiveness of contextual embeddings to interpret diverse user needs, thereby preserving model-human interactions.

4. Extensive experiments show that our model achieves state-of-the-art (SOTA) performance and comparable to traditional segmentation models trained on full data volumes, even with only 16 image-mask pairs for SAM1/2 training. Moreover, our strategy has the potential to adapt to the iterative evolution of LVMs for medical tasks in the future.


\section{Introduction}
\label{sec:intro}

Under the paradigm shifts in Large-scale Vision Models (LVMs), Segment Anything Model (SAM)~\cite{kirillov2023segment} and SAM 2~\cite{ravi2024sam} have been introduced as a generalized foundation model for segmenting and tracking any objects on imaging and video data, respectively. These models provide a certain degree of interactive segmentation capacity as users can segment any target object(s) according to their needs by using a single model in-one-go. Such capabilities are achieved by leveraging the concept of prompts~\cite{wang2023review}, such as points, boxes, or masks, waiving the traditional demand for massive manually annotations. Instead, users are only required to input prompts directly on target images or video frames.

\begin{figure}[t]
  \centering
  \setlength{\abovecaptionskip}{3pt}
   \includegraphics[width=0.99\linewidth]{Figure_Table/Figure_1.pdf}
   \caption{Illustration of enhancement effects of the proposed Proxy Prompt in (a-b) SAM 2 using real-time ultrasound frames of 1 subject, where `User-defined N' in (b) refers to number of target objects to be segmented ; and (c-d) SAM using a fundus retina dataset of 3 subjects, where all the generated segmentation are indicated in blue, except for case 3 in (c).}
   \label{fig:Figure1}
   \vspace{-15pt}
\end{figure}

Notwithstanding, widespread clinical adoptions of these LVMs have been substantially impeded by the soaring medical demands for ``automated prompting'' and ``high-level human-model interactions'' when it comes to downstream medical tasks, particularly real-time imaging-guided interventions. Regarding automated prompting, the current prompting strategies are sub-optimal for two key reasons. First, medical image and video data entail an overwhelmingly huge variations in terms of complexity of the target object(s) to be segmented; segmenting such structures (\eg vessels) using existing prompts can be practically challenging. As illustrated in \Cref{fig:Figure1}(c), for instance, manually inputting either box prompt (Case 1) or point prompt (Case 3) generates poor results; while adopting mask prompt (Case 2) performs well, the generation of such mask prompt is highly time-consuming and knowledge-demanding. Second, users required to give prompt for every single target image/video frame, which is a manual trial-and-error process, insufficient and tedious. The clinical burden  become exceedingly prominent when segmenting intricated structures \Cref{fig:Figure1}(c) and/or massive datasets, especially in resource-limited clinics. Therefore, there is a pressing demand for automated prompt generation to accommodate various clinical needs.

\begin{figure}
  \centering
  \setlength{\abovecaptionskip}{3pt}
   \includegraphics[width=0.99\linewidth]{Figure_Table/Figure_2.pdf}
   \caption{A diagram showcasing schematic differences of traditional prompt encoder and our proposed PPG in SAM2 (a-b) and SAM (c-d).}
   \label{fig:Figure2}
   \vspace{-15pt}
\end{figure}

Apart from this, the existing SAM- and SAM 2-driven models~\cite{shaharabany2023autosam, zhang2023customized, wu2023self} are inadequate to support the growing demand of high-level human-model interaction to accommodate the  disparity in preferences. These models segment target object(s) via training on specific objects, whether for single-object or multi-object segmentation tasks. However, they do not adapt well to changes in the user’s preferences. For instance, in fundus imaging, optic dis/cup weights more for glaucoma detection~\cite{thakur2018survey}, while vessels are prioritized for assessing retinal vascular occlusion~\cite{sekhar2008automated}. Even though these objects may appear within the same image, switching the segmentation focus from the optic disc/cup to vessels necessitates retraining the model. In real-word clinical settings where tasks are greatly diverse, creating separate models to each of tasks is practically challenging and computational resources demanding. Therefore, it is imperative to reinforce the capacity of human-model interaction by allowing users to flexibly adjust prompts for satisfying various clinical demands without the need for model retraining. All things considered, a automated prompting strategy and high-level interaction are crucial in clinical segmentation tasks, especially in real-time clinical procedures, such as image-guided surgeries. For example, in intrapartum ultrasound as illuminated in \Cref{fig:Figure1}(a), these properties would provide high flexibility for users to segment fetal head (FH) alone or FH\&pubic symphysis (PS) at any time points \(\{t_{1}, t_{2}\}\) to achieve measurement of fetal head rotation or fetal position~\cite{ghi2018isuog}.

To address these issues, we propose a novel Proxy Prompt (PP), which can be automatically generated from a ``non-target'' data (i.e., image / video frame of subjects other than the one under examination, such as from retrospective datasets) with a pre-annotated mask. This feature of PP strategy is distinct from the existing prompting methods where prompting can only be made on ``target'' data, in a manual manner. As illustrated in \Cref{fig:Figure1}(c), only one annotated image is required in using PP, tremendously streamline the workflow by waiving the prerequisite of providing separated prompts for every image/frame. Moreover, clinicians can freely switch segmentation tasks by adjusting the support-pair input at any moment during examination without model retraining or adopting different trained models, as shown in \Cref{fig:Figure1}(d). 
Working in tandem with the PP concept, we innovated a Proxy Prompt Generator (PPG) for adoption of SAM and SAM 2 for image and real-time video data, respectively. Compared to the SAM and SAM2 \Cref{fig:Figure2}(a\&c), we employed high dimensional embedding from the PPG as prompts, as shown in \Cref{fig:Figure2}(b\&d).

The core design of our PPG lies in the novel Contextual Selective Module (CSM) and Contextual Colorization Module (CCM), which are dedicatedly configurated for automated prompting and high-level human-model interactions. First, CSM is introduced to enable adaptive selection of the most representative contextual information from the ``non-target'' data for the ``target'' data, for the sake of achieving cross-data guidance. Besides, CSM contains Vision Mamba, Bridge Unit, and Selective Map, implementing a 3-step selection process of input-driven selection, object-guided selection, and target-frame relevance selection to support both cross-video tasks and cross-image prompting (\Cref{fig:Figure2}(b\&d)). Second, CCM is devised to reinforce human-model interaction by enhancing the expressiveness of contextual embedding, thus enabling the model to interpret diverse user needs as indicated by different masks (\eg single-object or multiple objects). By leveraging dual-reverse cross-attention, we ensure the enhanced embedding effectively captures specific object features through iterative interactions between the target feature and contextual information from distinct objects. In short, we present a simple yet efficient architecture by leveraging PPG to enhance clinical-friendliness of both SAM and SAM 2, even in few-shot settings. Furthermore, with all original parameters frozen, our design can function as a flexible, plug-and-play module and can continuously adapt to the ever-evolving LVMs.

We conducted extensive experiments across several popular image and video datasets, validating the superior performance and stability of our proposed approach. Our main contributions are outlined below:

1. We propose a novel PP to boost interactive LVMs for SAM and SAM 2, equipping them with the capacity of automated prompting and high-level human-model interaction. Notably, PP-empowered SAM 2 with PP greatly strengthen user-friendliness, particularly in real-time image-guided surgeries and interventions, and PP-enhanced SAM tremendously alleviates prompting burdens of users and minimizes ambiguity, especially for intricate structures.

2. We devise CSM for adaptive selection of the most representative contextual information from “non-target” data to guide “target” data, enabling effective cross-video and cross-image prompting, waiving the need for users to execute prompting for every single target data, and minimizing the experience-/expertise-derived variability in prompt quality. 

3. We configurate CCM for enhancing the expressiveness of contextual embeddings to interpret and accommodate diverse clinical demands and preference from end users, thereby reinforcing model-human interactions.

4. Extensive experiments show that our model achieves state-of-the-art (SOTA) performance and comparable to traditional segmentation models trained on full data volumes, even with only 16 image-mask pairs for SAM 1/2 training. Moreover, our strategy is of high potential to adapt to the iterative evolution of LVMs for medical tasks in the future.
