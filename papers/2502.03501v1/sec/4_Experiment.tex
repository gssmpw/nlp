%-------------------------------------------------------------------------
\begin{figure*}[t]
  \centering
  \setlength{\abovecaptionskip}{3pt}
  % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}} 
   \includegraphics[width=0.99\linewidth]{Figure_Table/bar_graph}
   \caption{Comparison on the video dataset using five different support pairs, showing the average results for two objects. Results show that our method achieves SOTA performance over existing models and demonstrates stability with a maximum fluctuation of only 1.0\%.}
   \label{fig:bar_graph}
   \vspace{-15pt}
\end{figure*}
%-------------------------------------------------------------------------
%-------------------------------------------------------------------------
\begin{figure*}[t]
  \centering
  \setlength{\abovecaptionskip}{3pt}
   \includegraphics[width=0.99\linewidth]{Figure_Table/compare_video}
   \caption{Comparison on one representative video. The top left corner of each subplot displays the Dice scores for the segmentation of the public symphysis(PS\_DSC) and fetal head (FH\_DSC).}
   \label{fig:compare_video}
   \vspace{-15pt}
\end{figure*}
%-------------------------------------------------------------------------
%-------------------------------------------------------------------------
\section{Experiment}
\label{sec:experiment}

\subsection{Datasets}
We conducted extensive experiments on four widely used publicly available datasets, including REFUGE2~\cite{fang2022refuge2}, STARE~\cite{hoover2000locating}, FH-PS-AOP(FPA)~\cite{jieyun_2023_7851339}, and JNU-IFM~\cite{LU2022107904}. Among these, STARE~\cite{hoover2000locating} contains only one single target, while the other datasets contain multiple segmentation targets. REFUGE2~\cite{fang2022refuge2}, STARE~\cite{hoover2000locating} and FPA~\cite{jieyun_2023_7851339} are image datasets evaluated in the image segmentation task, whereas JNU-IFM~\cite{LU2022107904} is a video dataset assessed in the video task. Further introduction to the datasets can be found in the supplementary materials.

%-------------------------------------------------------------------------
\subsection{Implementation Details}
The ``ViT-B” SAM model was used and initialized with pre-trained weights. Due to the limited training data, we freeze the pre-trained parameters of SAM to avoid overfitting. We applied LoRA~\cite{hu2021lora} on original encoder and decoder for efficient fine-tuning on medical datasets. The patch size and embedding dimension of ViM~\cite{zhu2024vision} encoder were set to 16 and 192, respectively. Model optimization was performed using the Stochastic Gradient Descent (SGD) optimizer, with a momentum of 0.9, a learning rate of 0.01, and a weight decay of 0.0005.
Data augmentation was applied to enrich the training process under limited data conditions. This included random variations in image brightness, rotation angles, and shear angles. Specifically, subtle adjustments were made to image saturation, brightness, and contrast within a range of 0.1 to simulate different settings and image gains. The maximum random rotation angle was set to 15 degrees, and the shear angle was limited to 10 degrees. For each training iteration, a consistent augmentation was applied to all images and masks in the support set.

%-------------------------------------------------------------------------
\subsection{Comparison with SOTA on Video Dataset}
%-------------------------------------------------------------------------

In this present work, we conducted a comprehensive comparison on the JNU-IFM dataset across four models: MedSAM 2, SAM 2-box, SAM 2-point, and Ours. MedSAM 2 and Ours utilize the image-mask pair to achieve automatic prompting in real-time scenarios, experiments settings can be found in the supplementary materials. Consequently, the simplest prompt in real-time is provided by Ours and MedSAM 2, followed by SAM-point with only one click, and finally SAM-box. Although mask prompts have superior performance based on the most complex information, they are practically overwhelming by a single operator to implement in real-time scenarios, thus serves as our ``Upper''.

We conducted five experiments on the same test set across the four models to evaluate their performance and stability. We randomly excluded five videos from five patients and paired the first frame's image and mask as five candidate support pairs. In each experiment, we altered different support pair as prompt to assess stability.

%-------------------------------------------------------------------------
\definecolor{gray}{rgb}{0.5, 0.5, 0.5} % haylee check
\begin{table*}[t]

\scriptsize
  \centering
  \begin{tabular}{@{}p{3.2cm}p{1cm}ccccccc@{}}
    \toprule
    \multirow{2}{*}{Method} & \multirow{2}{*}{\parbox{3cm}{\raggedright Training \\ times}} & \multicolumn{3}{c}{Color fundus photography} & \multicolumn{2}{c}{Ultrasound} & \multirow{2}{*}{Average} \\
    \cmidrule(lr){3-5} \cmidrule(l){6-7}
                            & & REFUGE2-Disc & REFUGE2-Cup & STARE-Vessel & FPA-PS & FPA-FH \\
    \midrule
    Upper$^{*}$ & 5 & 93.7 & 83.5 & 65.5 & 82.1 & 91.7 & 83.3\\
    \midrule
    SAM$^{\text{TF}}$ (1-point) & - & 39.1 & 33.5 & 16.3 & 18.2 & 34.4 & 28.3\\
    SAM$^{\text{TF}}$ (box) & - & 54.2 & 71.6 & 20.5 & 67.0 & {\bf 88.0} & 60.3\\
    SAM$^{\text{TF}}$ (everything) & - & 48.8 & 40.0 & 20.5 & 25.6 & 35.4 & 34.1\\
    MedSAM$^{\text{TF}}$ (box) & - & 87.2 & 81.3 & 20.1 & \textcolor{gray}{\bf 97.1} & \textcolor{gray}{\bf 97.5} & 74.5\\
    \midrule
    Med-SA (1-point) & 3 & {85.6} & {83.0} & {45.4} & {71.3} & {81.4} &{73.3}\\
    Med-SA (box) & 3 & {86.8} & {82.6} & {37.1} & {71.8} & {83.1} & {72.3}\\
    SAMed & 3 & {86.9} & {84.3} & {15.8} & {66.3} & \underline{87.0} & {68.1}\\
    AutoSAM & 5 & \underline{87.9} & \underline{84.6} & \underline{73.8} & \underline{78.0} & 79.1 & \underline{80.7}\\
    \midrule
    Ours & 2 & {\bf 88.2} & {\bf 85.6} & {\bf 78.9} & {\bf 81.5} & {\bf 88.0} & {\bf 84.4}\\
    \bottomrule
  \end{tabular}
  \caption{Comparison of our model with SAM-based SOTAs in Dice Score (\%), including train-free models (denoted as $^{\text{TF}}$), efficient fine-tuned models, and models trained on full data (denoted as $Upper^{*}$). ‘Training times’ records minimum number of trained models required to segment 5 objects, with dash(–) denoting inapplicability. \textcolor{gray}{Gray} indicates the data were used in model pre-training. The $\bf best$ and $\underline{\text{second-best}}$ results (excluding Upper) are bolded and underlined, respectively, showcasing our SOTA performance among various models under same few-shot settings.}
  \label{tab:compare_image}
  \vspace{-10pt}
\end{table*}
%-------------------------------------------------------------------------

%-------------------------------------------------------------------------
\begin{figure*}
  \centering
  \setlength{\abovecaptionskip}{3pt}
   \includegraphics[width=0.9\linewidth]{Figure_Table/Compare_Image.pdf}
   \caption{Visualization comparison results of nine models across five objects.
   }
   \label{fig:compare_Image}
   \vspace{-15pt}
\end{figure*}
%-------------------------------------------------------------------------

{\bf The quantitative comparison results are shown in \cref{fig:bar_graph}.} From the averaged performance across five experiments, it can be observed that our method improves the dice score by 17.5\% compared to MedSAM 2, which uses a similarly simple prompt strategy. Even when compared to SAM 2-box, which employs the most demanding prompt strategy, our method still achieves a 2.3\% improvement and is comparable to the ``Upper''. 
In terms of stability, MedSAM 2 is significantly affected by variations in the support pairs across the five trials. This results in a maximum dice score fluctuation of 29.1\% for MedSAM 2, whereas our method maintains a far smaller fluctuation of only 1.0\%.

{\bf The qualitative comparison results are shown in \cref{fig:compare_video}.} Support pair 1 and 2 were chosen to demonstrate the stability of MedSAM 2 and our model under different prompts. Due to the blurred FH boundary in this case, both SAM 2-box and Ours (as well as the ``Upper'') exhibited slight over-segment in FH segmentation. However, while SAM 2-box over-segmented the FH region to the left in \(Frame_{t}\), our method constrains this expansion. Although MedSAM 2 showed a notable drop in PS segmentation performance with support pair 1, our method maintained consistent results.

%-------------------------------------------------------------------------
\subsection{Comparison with SOTA on Image Dataset}
%-------------------------------------------------------------------------
% {\bf Comparison on Image Dataset}
We extensively evaluated diverse comparison methods, including train-free foundation models, models efficiently fine-tuned on the specific-dataset, and traditional segmentation models. Experiment settings of prompts are in the supplementary materials. For train-free models such as SAM and MedSAM, we tested their performance with different prompt. Since MedSAM have been pre-trained on a large medical dataset, we evaluated its performance on both the pre-trained FPA dataset and the unseen REFUGE2 dataset. For models requiring training, including Med-SA, SAMed, AutoSAM, and ours, we trained each model on the same 16 images until full convergence to evaluate their performance in few-shot settings. For traditional segmentation models, we used dataset-specific SOTA methods, including BEAL~\cite{wang2019boundary} for REFUGE2, nnUnet~\cite{isensee2021nnu} for STARE, and Segnet~\cite{badrinarayanan2017segnet} for FPA. The results of the three models, as claimed in their papers, were obtained under full data training settings and thus were treated as the ``Upper''.

{\bf The quantitative comparison results are presented in \cref{tab:compare_image}.} MedSAM and AutoSAM showed strong performance in both train-free and few-shot settings, probably due to pre-training on large medical datasets or object-specific training. However, MedSAM's performance dropped significantly on unseen datasets and struggled with intricate structure such as vessel in the STARE dataset. AutoSAM, on the other hand, underperformed on ultrasound datasets, possibly due to the challenges such as ultrasound imaging-associated speckle noise. Furthermore, AutoSAM requires training five models for five objects, whereas our method only requires training two models for two modalities with human-model interaction. This reduction in retraining enhances practicality in real-world clinics, where tasks are more diverse. Our method achieved SOTA performance under limited conditions, even comparable with traditional SOTAs trained on the full dataset.

{\bf The qualitative comparison results are presented in \cref{fig:compare_Image}.} For the REFUGE2 dataset, SAMed and AutoSAM performed relatively well, nevertheless both showed issues with missing regions in the boundaries for certain objects. On the vascular dataset STARE, MedSAM with box prompt exhibited ambiguity, confusing the objects to be segmented. For PS in the FPA dataset, most models tended to over-segment, with SAM-point, SAM-box, Med-SA, and AutoSAM extending the PS segmentation beyond the object’s lower right boundary, while SAM-everything and SAMed less segment in the lower left edge. For FH, most models tended to over-segment, while SAM-box and SAMed exhibited less segmentation at the upper left edge. This may be caused by blurring of the object boundary due to presence of speckle noise in the ultrasound dataset. Across all datasets, our method consistently produced superior and stable results.

%-------------------------------------------------------------------------
\subsection{Ablation Studies}
%-------------------------------------------------------------------------
\begin{figure}[t]
  \centering
  \setlength{\abovecaptionskip}{3pt}
   \includegraphics[width=0.99\columnwidth]{Figure_Table/ablation.pdf}
   \caption{The boxes show performance for each \(M\) with varying \(G\), while in the rightmost plots (\(G < M-1\)), higher peaks refer to better performance and narrower ranges reflect improved stability. Optimum is achieved when both \(M\) and \(G\) are maximized.}
   \label{fig:ablation}
   \vspace{-15pt}
\end{figure}
%-------------------------------------------------------------------------

For each dataset, 16 samples were excluded to ensure a consistent test set across all experiments. To evaluate the impact of training set size \( M \), we defined \(\mathbf{Train\; set} = \{ (\mathbf{I}_{i}, \mathbf{M}_{i}) \}_{i=1}^M\) with \( M \in \{2, 4, 8, 16\} \), setting the number of support pairs to \( M-1 \) during training. For a given \( M \), we conducted repeated experiments by randomly selecting multiple independent and diverse \(\mathbf{Support\; set} = \{ (\mathbf{I}_{i}, \mathbf{M}_{i}) \}_{i=1}^G\) for each inference support size \( G \in \{1, 2, 4, 8\} \). Under different values of \( M \) and \( G \), the averaged dice scores across three datasets and five objects are illuminated in \cref{fig:ablation}. 

The figure shows a positive correlation between segmentation quality and \( M \). For each \( M \), the dice score monotonically increases with \( G \) until \( G \) exceeds \( M-1 \), after which a slight decline may occur. Furthermore, the ridge plot reveals that when \( G=1 \), the distribution of dice scores is more dispersed due to the highly diverse \(\mathbf{Support\; set}\). However, as \( G \) increases, the variance in model predictions with various support pairs greatly decreases. For instance, in the top-right plot based on \( M=16 \), the results of repeated experiments are more concentrated with sharper shape when \( G=8 \)(teal region) compared to \( G=1 \)(charcoal purple region), indicating that the model is less affected by the quality of individual support pairs. In summary, both training dataset size \( M \) and inference support size \( G \) influence model performance, with \( M \) having a notably greater impact. The dice score stabilizes at its highest value when both \( M \) and \( G \) are maximized, reflecting the optimal performance and stability of our model.

Experiments validating the effectiveness of the two modules are provided in the supplementary materials.