\section{Related Work}
\label{sec:related}

 %-------------------------------------------------------------------------
\subsection{Adapting SAM to Medical Image Segmentation}
 Given the domain gap between natural and medical image datasets, various works~\cite{ma2024segment, wu2023medical, zhang2023customized, shaharabany2023autosam} have studied the application of SAM to medical image segmentation.  Medical SAM (MedSAM)~\cite{ma2024segment} fine-tunes the decoder of SAM using 1,570,263 medical image-mask pairs with bounding box prompts to adapt to medical tasks. Medical SAM Adapter (Med-SA)~\cite{wu2023medical} efficiently fine-tunes SAM using adapter with point and box prompts. SAM Medical (SAMed) model~\cite{zhang2023customized} efficiently fine-tunes the image encoder of SAM using another technique---low-rank-based (LoRA) strategy~\cite{hu2021lora}. AutoSAM~\cite{shaharabany2023autosam} utilizes the gradients provided by a frozen SAM to train a new encoder, thus automatically extracts prompts from the image itself to achieve automatic segmentation. However, these SAM-based works on medical data can be roughly categorized into two types: one inherits the prompt design of SAM but without the automatic capability (MedSAM and Med-SA); the other can automatically segment objects but sacrifices the human-model interaction (SAMed and AutoSAM).

 %-------------------------------------------------------------------------
\subsection{SAM without Manual Given Prompt}
In addition to SAMed and AutoSAM, many other works also focus on enhancing the automatic prompting capability of SAM to improve user-friendliness. Self-prompting SAM~\cite{wu2023self} fine-tune a self-prompt unit to first provide coarse segmentations, from which they extract point/box prompts for SAM to obtain the final results. Personalization approach for SAM (PerSAM) ~\cite{zhang2023personalize} identify the most similar point between the reference and test image as the prompt for SAM. Evidential prompt generation method (EviPrompt)~\cite{xu2023eviprompt} and ProtoSAM~\cite{Ayzenberg2024ProtoSAMOM} both adapt the idea in PerSAM into medical image domain. EviPrompt fits the point prompts according to image similarity, while ProtoSAM ultilizes reference image/mask pair to obtain a coarse segmentation of target image, then extracts point or box as prompts required by SAM.
However, these methods are still limited in simulating prompts such as points or boxes, which restricts their capability in the vessel-like branching structures due to the ambiguous instruction~\cite{ma2024segment}. In contrast, our PP emphasizes the high-level embeddings as prompts, thereby guiding the model with deeper-level information to precisely segment such intricate objects.

%-------------------------------------------------------------------------
\subsection{Vision Backbone Based on Mamba}
To extract contextual information for guiding model segmentation, the recently designed Mamba can serve as a potential choice for building vision backbone. Based on the state space model (SSM)~\cite{gu2023modeling}, Mamba~\cite{gu2023mamba} boosts the development of SSM from the key aspects of ``structure''. In terms of structure, it breaks the input-invariant feature of the conventional SSM layer and constructs an input-dependent SSM layer, enabling it to focus on the effective information in the input. Various studies~\cite{ma2024fer,ju2024vm,zheng2024fd,ma2024u,zhu2024vision} have migrated Mamba to the vision domain. Facial Expression Recognition-YOLO-Mamba (FER-YOLO-Mamba)~\cite{ma2024fer} combines Mamba with attention to construct a dual-branch structure for facial expression detection. Vision Mamba-Denoising Diffusion Probabilistic Model (VM-DDPM)~\cite{ju2024vm} introduces Mamba in the medical image synthesis domain, utilizing an SSM-CNN hybrid structure within the diffusion model. Vision Mamba (Vim) solely relies on the SSM to construct a vision backbone, selectively capturing key information in the input-dependent manner, making it highly suitable for handling high-resolution inputs~\cite{zhu2024vision}, which is common in medical tasks. 
