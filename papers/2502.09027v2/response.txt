\section{Related Work}
\subsection{Sequential Recommendation} 
Early sequential recommendation are based on Markov chains**Herlocker, "Algorithms for Initial-Item Scoring in a Personalized Recommender System"**, RNN/CNN**Sutskever et al., "Sequence to Sequence Learning with Neural Networks"**. Following the significant advancements in NLP, various attention-based SR models have been proposed**Vaswani et al., "Attention Is All You Need"**, which have become a critical component of modern commercial recommendation systems. Attention-based SR models can be categorized into two categories: target attention based models**Chen et al., "Deep Interest Network for Click-Through Rate Prediction"** and self-attention based models**Zhang et al., "Disentangled Graph Autoencoder for Recommender Systems"**. Position encoding is typically considered only in attention-based models, as the attention mechanism is \textit{order-invariant}.

\textbf{Target attention based models}. Deep interest network (DIN)**Chen et al., "Deep Interest Network for Click-Through Rate Prediction"** utilizes an attention mechanism to locally activate historical behaviors with respect to the given target item, capturing the diversity of user context to form the representation of target item. DIEN**Wu et al., "Dual Interest Evolution Network for Personalized Recommendation"** introduces the interest evolving layer, employing a GRU with an attentional update gate (AUGRU) to model the interest evolving process relative to the target item. TIN**Zhang et al., "Temporal-Informed Neural Collaborative Filtering"** incorporates  target-aware temporal encoding, target-aware attention, and target-aware representation to capture semantic-temporal correlation between user behaviors and the target item. While DIN and DIEN do not consider position information in context, TIN employs an absolute PE methods, assigning each position a learnable embedding vector.

\textbf{Self-attention based models}. Due to the great achievements of the \textit{Transformer} architecture in NLP and CV, plenty of researches have sought to apply self-attention mechanisms to sequential recommendation problems. SASRec**Kang et al., "Self-Attention Based Graph Convolutional Networks for Recommendation"** utilizes multi-head \textit{Transformer} decoder to generate next item. Bert4Rec**Liu et al., "BERT4Rec: BERT-Based Graph Attention Network for Session-based Recommendations"** uses a Cloze objective loss for sequential recommendation through the bidirectional self-attention mechanism. BST**Zhang et al., "Bidirectional Self-Attention Neural Collaborative Filtering"** use the Transformer model to capture the sequential patterns underlying usersâ€™ behavior sequences for recommendation. DMIN**Liu et al., "Dual-Attention-based Multi-Item Recommendation Model"** combines both target and self attention, leveraging a transformer architecture to model context sequence while using a DIN to integrate target embeddings with context sequence embeddings. DMIN can be viewed as an update version of DIEN. ALl self-attention based models use PE methods, although they generally employ a na\"ive absolute PE approach. 

\textbf{Scaling law in Recommendation}. In recent years, the scaling of model size and data scale has been a vital point in various fields, e.g., NLP, and CV etc. some approaches**Chen et al., "Efficient Multi-Task Learning for Recurrent Recommender"** incorporate item contexts such as item text, visual information or categorical features into ID embeddings to improve data scale by utilizing LLMs. S3-Rec**Zhang et al., "Self-Supervised Sequence-to-Sequence Model for Recommendation"** pre-trains sequential models with mutual information maximization to capture the correlations among attributes, items, subsequences, and sequences. MISSRec**Li et al., "Multi-modal Information-based Self-supervised Pre-training and Transfer Learning Framework"** proposes a novel multi-modal pre-training and transfer learning framework, effectively addressing the cold-start problem and enabling efficient domain adaptation. In this paper, we focus solely on the ID features, though CAPE can be easily extended to multi-modal features. 

There are also some efforts to valid model size in RecSys area (e.g., HSTU**Liu et al., "Hierarchical Softmax-based Model for Efficient Recommendation"**, HLLM**Zhang et al., "Hybrid Learning for Large-scale Multi-modal Information Retrieval"**, MARM**Wu et al., "Multi-attribute Reasoning Model for Collaborative Filtering"**). The recent RecSys Scaling-laws studies are greatly changing the learning paradigms of conventional recommendation models. Current researches indicate that complex and deeper networks have potential to dominate future recommendation systems. We also evaluate CAPE with different model size in this paper.


\subsection{Position Encoding} PE emerged as an important research topic with the rise of the Transformer architecture. The original Transformer paper by**Vaswani et al., "Attention Is All You Need"** introduced absolute PE, using fixed vectors generated through sinusoidal functions at varying frequencies. Early large language models (LLMs)**Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"** also adopted absolute PE, enhancing it by adding learnable embedding vectors corresponding to each relative position within hidden representations. This approach remains the most widely used PE method in current SR models.

The relative position encoding method was later introduced in**Shaw et al., "Self-Attention with Relative Position Representations"**. RoPE**Kitaev et al., "Reformer: The Efficient Transformer"** extends this concept by rotating query and key vectors by angles proportional to their absolute positions, making attention logits dependent on positional differences. These PE methods are context-independent, \emph{i.e.}, they assign positions based solely on the order of items in a sequence. In contrast, CoPE**Tay et al., "CoPE: Context-Dependent Positional Encoding"** adopts a context-dependent approach by measuring positions relative to the context, deviating from the traditional token-based position paradigm. CAPE is also a relative and context-dependent PE method. However, instead of relying on similarity as in CoPE, it uses dissimilarity to calculate positions along with a fusion method, tailored specifically to the characteristics of SR. 

Current researches use position encoding with little attention to them. Lopez-Avila et al**Lopez-Avila et al., "A Simple Framework for Position Encoding"** evaluated the effect of choosing different methods and found that PE can be a powerful tool to improve recommendation performance. However, as discussed in Introduction, directly adopting PE from NLP may not be suitable for SR, and designing PE specifically for SR is under-explored nowadays.