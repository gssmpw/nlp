\section{Related Work}
\subsection{Sequential Recommendation} 
Early sequential recommendation are based on Markov chains____, RNN/CNN____. Following the significant advancements in NLP, various attention-based SR models have been proposed____, which have become a critical component of modern commercial recommendation systems. Attention-based SR models can be categorized into two categories: target attention based models____ and self-attention based models____. Position encoding is typically considered only in attention-based models, as the attention mechanism is \textit{order-invariant}.

\textbf{Target attention based models}. Deep interest network (DIN)____ utilizes an attention mechanism to locally activate historical behaviors with respect to the given target item, capturing the diversity of user context to form the representation of target item. DIEN____ introduces the interest evolving layer, employing a GRU with an attentional update gate (AUGRU) to model the interest evolving process relative to the target item. TIN____ incorporates  target-aware temporal encoding, target-aware attention, and target-aware representation to capture semantic-temporal correlation between user behaviors and the target item. While DIN and DIEN do not consider position information in context, TIN employs an absolute PE methods, assigning each position a learnable embedding vector.

\textbf{Self-attention based models}. Due to the great achievements of the \textit{Transformer} architecture in NLP and CV, plenty of researches have sought to apply self-attention mechanisms to sequential recommendation problems. SASRec____ utilizes multi-head \textit{Transformer} decoder to generate next item. Bert4Rec____ uses a Cloze objective loss for sequential recommendation through the bidirectional self-attention mechanism. BST____ use the Transformer model to capture the sequential patterns underlying usersâ€™ behavior sequences for recommendation. DMIN____ combines both target and self attention, leveraging a transformer architecture to model context sequence while using a DIN to integrate target embeddings with context sequence embeddings. DMIN can be viewed as an update version of DIEN. ALl self-attention based models use PE methods, although they generally employ a na\"ive absolute PE approach. 

\textbf{Scaling law in Recommendation}. In recent years, the scaling of model size and data scale has been a vital point in various fields, e.g., NLP, and CV etc. some approaches____ incorporate item contexts such as item text, visual information or categorical features into ID embeddings to improve data scale by utilizing LLMs. S3-Rec____ pre-trains sequential models with mutual information maximization to capture the correlations among attributes, items, subsequences, and sequences. MISSRec____ proposes a novel multi-modal pre-training and transfer learning framework, effectively addressing the cold-start problem and enabling efficient domain adaptation. In this paper, we focus solely on the ID features, though CAPE can be easily extended to multi-modal features. 

There are also some efforts to valid model size in RecSys area (e.g., HSTU____, HLLM____, MARM____). The recent RecSys Scaling-laws studies are greatly changing the learning paradigms of conventional recommendation models. Current researches indicate that complex and deeper networks have potential to dominate future recommendation systems. We also evaluate CAPE with different model size in this paper.


\subsection{Position Encoding} PE emerged as an important research topic with the rise of the Transformer architecture. The original Transformer paper by____ introduced absolute PE, using fixed vectors generated through sinusoidal functions at varying frequencies. Early large language models (LLMs)____ also adopted absolute PE, enhancing it by adding learnable embedding vectors corresponding to each relative position within hidden representations. This approach remains the most widely used PE method in current SR models.

The relative position encoding method was later introduced in____. RoPE____ extends this concept by rotating query and key vectors by angles proportional to their absolute positions, making attention logits dependent on positional differences. These PE methods are context-independent, \emph{i.e.}, they assign positions based solely on the order of items in a sequence. In contrast, CoPE____ adopts a context-dependent approach by measuring positions relative to the context, deviating from the traditional token-based position paradigm. CAPE is also a relative and context-dependent PE method. However, instead of relying on similarity as in CoPE, it uses dissimilarity to calculate positions along with a fusion method, tailored specifically to the characteristics of SR. 

Current researches use position encoding with little attention to them. Lopez-Avila et al____ evaluated the effect of choosing different methods and found that PE can be a powerful tool to improve recommendation performance. However, as discussed in Introduction, directly adopting PE from NLP may not be suitable for SR, and designing PE specifically for SR is under-explored nowadays.