%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{caption}
\usepackage{stix}


% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage lineli.quanzheng@mgh.harvard.edu and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
\usepackage{multirow}
\newcommand{\add}[1] {\textcolor{black}{#1}} 
\newcommand{\blue}[1] {\textcolor{black}{#1}} 

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Distribution-aware Fairness Learning in Medical Image Segmentation From A Control-Theoretic Perspective}

\begin{document}

\twocolumn[
\icmltitle{Distribution-aware Fairness Learning in Medical Image Segmentation \\ From A Control-Theoretic Perspective}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}
\icmlsetsymbol{corres}{$\dagger$}

\begin{icmlauthorlist}
\icmlauthor{Yujin Oh}{equal,mgh}
\icmlauthor{Pengfei Jin}{equal,mgh}
\icmlauthor{Sangjoon Park}{equal,ycc}
\icmlauthor{Sekeun Kim}{mgh}
\icmlauthor{Siyeop Yoon
}{mgh}
\icmlauthor{Kyungsang Kim}{mgh}
\icmlauthor{Jin Sung Kim}{ycc}
\icmlauthor{Xiang Li}{corres,mgh}
\icmlauthor{Quanzheng Li}{corres,mgh}
\end{icmlauthorlist}

\icmlaffiliation{mgh}{Center for Advanced Medical Computing and Analysis (CAMCA), Department of Radiology, Massachusetts General Hospital (MGH) and Harvard Medical School, MA 02114, USA}
\icmlaffiliation{ycc}{Department of Radiation Oncology, Yonsei University College of Medicine, Yonsei University, Seoul 03772, Republic of Korea}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Xiang Li}{xli60@mgh.harvard.edu}
\icmlcorrespondingauthor{Quanzheng Li}{li.quanzheng2@mgh.harvard.edu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document

%\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Ensuring fairness in medical image segmentation is critical due to biases in imbalanced clinical data acquisition caused by demographic attributes (e.g., age, sex, race) and clinical factors (e.g., disease severity). To address these challenges, we introduce Distribution-aware Mixture of Experts (dMoE), inspired by optimal control theory. We provide a comprehensive analysis of its underlying mechanisms and clarify dMoE's role in adapting to heterogeneous distributions in medical image segmentation. Furthermore, we integrate dMoE into multiple network architectures, demonstrating its broad applicability across diverse medical image analysis tasks. By incorporating demographic and clinical factors, dMoE achieves state-of-the-art performance on two 2D benchmark datasets and a 3D in-house dataset. Our results highlight the effectiveness of dMoE in mitigating biases from imbalanced distributions, offering a promising approach to bridging control theory and medical image segmentation within fairness learning paradigms. The source code will be made available.
\end{abstract}



\section{Introduction}

% \begin{itemize}
% \item Paper Submissions Open Jan 09 '25 11:59 PM UTC		
% \item Abstract Due	Jan 23 '25 (Anywhere on Earth)
% \item \add{(CAMCA Due)	Jan 25 '25 (Saturday)}	
% \item Full Paper Due	Jan 30 '25 11:59 AM UTC
% \end{itemize}


\begin{figure}[t]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=1\columnwidth]{fig/intro.pdf}}
\caption{The influence of clinical data distribution on medical image segmentation and the role of dMoE as a distribution-aware control mechanism to address inequity challenges. Transparent blue lines within the violin plots connect the most densely populated regions of each attribute, visually representing overall equity.}
\label{fig_intro}
\end{center}
\vskip -0.2in
\end{figure}

The inherent imbalance and ill-posed nature of data collected in clinical practice, impacted by demographic attributes such as gender and race, {as well as clinical factors, e.g., disease severity}, emphasize the need for fairness-aware learning approaches in medical data analysis \cite{ktena2024generative, jin2024fairmedfm}. Deep neural networks, due to their data-driven optimization processes, often overfit to prevalent patterns while failing to adequately learn from underrepresented ones, leading to potentially biased predictions and weakening fairness across demographic subgroups, as shown in \cref{fig_intro}. Although advanced fairness training strategies have been actively developed \cite{tian2025fairdomain, tianfairseg, li2024fairdiff}, distributional approaches remain underexplored—despite the fact that {clinical and demographic factors play a significant role in many clinical decision-making \cite{9308226, theodore2023clinical}}. 
% \blue{When diagnosing patients, clinicians implicitly group individuals based on attributes such as age, gender, and race, leveraging prior experience to comprehensively evaluate the subgroup characteristics alongside the individual’s unique condition.} 
\blue{Clinicians consider demographic factors such as age, gender, and race alongside a patient’s unique condition to assess both individual and subgroup characteristics. This process not only informs clinical factors like disease stage and decision-making but also implicitly shapes region/population-specific practice patterns based on cumulative experience and local demographics. However, current fairness learning approaches primarily focus on explicit factors such as demographic attributes but neglect implicit/contextual factors such as disease progression patterns or severity, despite the fact that these factors can be potential influence by region/population characteristics.} %Thus, there is a need for a comprehensive solution that ensures fairness by incorporating both demographic and pathological factors.}

% This process highlights a critical limitation of existing fairness learning approaches, which primarily focus on demographic attributes while neglecting pathological information, such as disease severity and progression stage, which is crucial for clinical decision-making. Moreover, a comprehensive solution addressing fairness across both demographic and pathologic dimensions remains absent.


Building on these distributional insights and the need for broader applicability across diverse attributes, we propose Distribution-aware Mixture of Experts (dMoE), a framework that adaptively incorporates individual characteristics and distributional patterns into deep neural network training. This architecture is grounded in an in-depth analysis of the Mixture of Experts (MoE) framework \cite{shazeer2017outrageously} with the adaptation of an optimal control perspective system. {By analyzing the structural parallels between MoE and traditional control systems \cite{aastrom2021feedback}, we reinterpret MoE as a feedback control mechanism. We further enhance its gating mechanism to incorporate distributional information as a mode-switching control for adaptive parameter selection, resulting in dMoE (\cref{fig_main}). This approach ensures robust and fair performance across diverse demographic and clinical subgroups.}

Moreover, to ensure broad applicability, we integrate dMoE with various network architectures, including transformers \cite{vaswani2017attention} and convolutional neural networks (CNNs), demonstrating the generalizability of its architecture and making the framework well-suited for a wide range of medical image analysis tasks. We validate the effectiveness of dMoE based on the segmentation task on multiple clinical datasets with diverse segmentation masks for diagnosis and treatment planning tasks. %These datasets include public scanning laser ophthalmoscopy (SLO) images, dermatology images, and in-house computed tomography (CT) scans. The evaluated tasks include optic cup and neuroretinal rim segmentation, lesion segmentation, and radiotherapy target segmentation. 
Experimental results demonstrate that dMoE not only advances state-of-the-art (SOTA) fairness learning approaches but also presents a way to incorporate distributional attributes to provide robust and equitable diagnosis and clinical decision-making across diverse demographic and clinical attributes. %\add{The advancements achieved through our distribution-aware fairness learning framework enhance the adaptability of clinical AI, promoting reliable and unbiased decision-making across diverse healthcare settings.}

Our core contributions are summarized as follows:
\begin{itemize}
\item {We reinterpret MoE from the perspective of feedback control mechanism and extend it into dMoE for fairness learning by incorporating distributional awareness.}
\item dMoE operates seamlessly within transformers and CNNs, enabling its use in 2D and 3D segmentation tasks across demographic and \blue{clinical} attributes. 
\item Extensive medical image segmentation experiments for diagnosis and treatment planning demonstrate dMoE’s robustness, demonstrating its effectiveness in mitigating biases from imbalanced medical data distributions.
\end{itemize}

\medskip



\section{Related Work}

\subsection{Fairness Learning in Medical Image Segmentation}
Fairness-oriented medical image segmentation datasets with demographic information \cite{tschandl2018ham10000, tianfairseg} have enabled research on bias mitigation, driving the development of robust fairness training strategies. Advanced generative approaches \cite{li2024fairdiff, ktena2024generative} aim to generate diverse samples from skewed distributions to mitigate bias. However, challenges remain in addressing the computational demands and achieving high-quality sample generation for high-dimensional medical images, such as 3D whole-body computed tomography (CT), for use in augmentation datasets. On the other hand, fairness-focused loss function modifications, such as distributionally robust optimization (DRO) \cite{sagawa2019distributionally} and fair error-bound scaling (FEBS) \cite{tianfairseg}, have been proposed to integrate fairness considerations directly into the optimization process. While partially effective, these methods are vulnerable to the data distribution within the training batch, limiting their applicability in 3D medical image segmentation, where large batch sizes are constrained. 
%Furthermore, existing benchmarks and studies primarily focus on demographic attributes \cite{tian2025fairdomain, jin2024fairmedfm}, often overlooking critical \blue{clinical} factors such as tumor progression and metastasis, which are essential for medical image analysis. In this study, we propose a general method that can be broadly applied to mitigate bias arising from both demographic and \blue{clinical} factors. 
\blue{
Furthermore, existing benchmarks and studies primarily focus on demographic attributes \cite{tian2025fairdomain, jin2024fairmedfm}, often overlooking critical clinical factors, such as tumor progression and metastasis, which contribute to regional variations in clinical practice patterns due to inherent biases. In this study, we propose a general method to mitigate bias arising from both demographic and clinical factors.}
%, including those shaping regional practice differences.}
 
\subsection{Mixture of Expert in Multi-distribution Learning}
Recent advances in the MoE framework \cite{shazeer2017outrageously} have demonstrated remarkable potential for adapting AI models to diverse data distributions, particularly within continual learning paradigms. MoE enhances robustness and adaptability when confronted with previously unseen data patterns \cite{rypesc2024divide, yu2024boosting}. In the medical domain, MoE has been effectively extended to address challenges such as multimodal integration \cite{jiang2024m4oe}, scanning modality heterogeneity \cite{zhang2024foundation}, and catastrophic forgetting issues in continual learning \cite{chen2024low, wang2024sam}, unifying these approaches into a cohesive framework that enhances performance. However, theoretical insights into how MoE facilitates the adaptation of disparate distributions to a target distribution remain limited. {This study clarifies the underlying mechanism of MoE as dynamic parameter selection,  and integrates environmental attributes into its gating mechanism, enabling distributional adaptation for medical image segmentation. }

\subsection{Training Neural Networks as Optimal Control}
Neural networks, especially those with shortcut connections, perform complex transformations through successive modifications of a hidden state. These networks can be conceptualized as undergoing a continuous dynamic process, which can be described using ordinary differential equations (ODEs) \cite{weinan2017proposal, lu2018beyond}. Training these networks resembles solving an optimal control problem, where the objective is to adjust the network parameters, i.e., weights, to minimize a loss function \cite{chen2018neural, sun2024layer}. %{This approach is analogous to determining the optimal control path from an input  to its segmented result.}

{Fixed-architecture feedforward neural networks can be regarded as operating under a non-feedback control. Employing a consistent strategy, however, can be restrictive in dynamic environments \cite{aastrom1995adaptive}.}
To address this limitation, feedback control mechanisms \cite{doyle2013feedback, aastrom2021feedback} offer an alternative by enabling continuous monitoring and adjustment based on the system's outputs and desired targets.
Furthermore, mode-switching control \cite{yamaguchi1996mode, boskovic2000multi} introduces additional flexibility by enabling the policy to alternate between multiple optimized operational modes in response to external inputs. This capacity is essential for managing complex systems across different conditions and offers a robust response to diverse operational challenges \cite{yu2017sliding}. %In the context of image segmentation, it is similarly beneficial to adopt varying strategies based on the type of image, which motivates our method.
Likewise, in the context of image segmentation, adopting varying strategies— such as leveraging distributional attributes of the input—proves beneficial and serves as the motivation for our method.





\begin{figure*}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=2\columnwidth]{fig/main.pdf}}
\caption{(a) Schematic of the dMoE segmentation network for fairness learning, and (b) its interpretation through a control system.}
\label{fig_main}
\end{center}
\vskip -0.2in
\end{figure*}





\section{Method}

In the methods section, we provide theoretical insights and the motivation for proposing the distribution-aware MoE as a solution for fairness learning. We begin by revisiting MoE in \cref{method_moe} and then expand it into dMoE in \cref{method_dmoe}. After defining dMoE, in \cref{method_oc}, we further elaborate on the principles of optimal control and demonstrate how dMoE can be formulated as a mode-switching optimal control problem, by providing conceptual connections between them, as illustrated in \cref{fig_main}(a) and \cref{fig_main}(b) (bottom).

\subsection{Revisiting MoE} \label{method_moe}
The MoE framework \cite{shazeer2017outrageously}, which serves as the backbone structure for our proposed dMoE, originally leverages sparse gating to achieve computational efficiency while allowing for large model capacity. The output of the MoE layer is defined as follows:
\begin{equation}
y = \sum_{i=1}^{n} G(x)_i E_i(x),
\end{equation}
where $n$ is the total number of experts, $E_i(x)$ represents the output of the $i$-th expert network for the input $x$, and $G(x)$ is the output of the gating network, a sparse $n$-dimensional vector that determines which experts are activated. The MoE approach fundamentally relies on sparsity, where the gating network engages only a limited subset of experts for each input, substantially lowering computational overhead by excluding inactive experts from processing. The gating function calculates $G(x)$ using methods such as \textit{Noisy Top-K Gating}, which prioritizes the most relevant $k$ experts while nullifying contributions from the remaining $n-k$. Both the gating network and the experts are optimized simultaneously through backpropagation, ensuring seamless integration and fine-tuning of the MoE layer. This design facilitates a significant expansion of model capacity without proportionally increasing computational demands, making it particularly well-suited for large-scale tasks like language modeling and machine translation.



\subsection{Distribution-aware MoE}   \label{method_dmoe}
Now, we explain our proposed Distribution-aware Mixture of Experts (dMoE), as illustrated in \cref{fig_main}(a). We further provide detailed network architecture in \cref{appen_arch}. Unlike traditional sparse MoE, our proposed dMoE module integrates multiple distribution-wise router networks $ G^{attr} $ and a set of $ n $ expert modules, consisting of shallow multi-layer perceptron (MLP) neural networks, defined as $ E = \{ \textit{Expert}_1, \textit{Expert}_2, \ldots, \textit{Expert}_n \} $. We start by patching intermediate image embeddings from $ l $-th layer block output, represented as $ {h}_l \in \mathbb{R}^{H_l W_l D_l \times Ch_l} $, into patches $ \tilde{h}_l \in \mathbb{R}^{N_l \times Ch_l} $, where the total number of patches is $ N_l = H_l W_l D_l $. Here, $ H_l $, $ W_l $, $ D_l $, and $ Ch_l $ correspond to the height, width, depth, and channel dimensions of the intermediate image embeddings, respectively. For the transformer, which already has a predefined dimension, we omit this process.
Given these {patched} embeddings $ \tilde{h}_l $ and an attribute flag $ attr $, the activated router $ G^{attr} $ identifies the top-$ k $ experts. The final output is computed as a weighted sum of the outputs of these selected experts:
\begin{align}
\bar{h}_l =  \tilde{h}_l + \sum_{i=1}^{k} {G^{attr}_{i}(\tilde{h}_l)} \cdot {E_i(\tilde{h}_l)},
\label{eq_exeprt}
\end{align}
where $G^{attr}()$ outputs a weight matrix prioritizing each expert’s contribution in a center-specific manner. The resulting weighted output is combined with ${h_l}$, representing the shared path, to yield the final unpatched dMoE image embedding $\bar{h_l} \in \mathbb{R}^{H_l W_l D_l \times Ch_l}$. The router network $G^{attr}$ computes sparse weights $H$ using Gaussian noise, as follows:
\begin{equation} \label{G1}
G^{attr}(x) = \text{Softmax}(\text{KeepTop-$k$}(H(x), k)),
\end{equation}
\begin{equation} \label{G2}
H(x)_i = (x ^\top \cdot W)_i + \text{Normal()} \cdot \text{Softplus}((x ^\top \cdot W^{\text{noise}})_{i}),
\end{equation}
\begin{equation} \label{G3}
\text{KeepTop-$k$}(v, k)_i = 
\begin{cases} 
v_i & \text{if } v_i \text{ is in top } k \text{ elements of } v, \\
-\infty & \text{otherwise.}
\end{cases}
\end{equation}
where, $W$ and $W^{\text{noise}}$ are trainable weight matrices, $\text{Normal}() \sim N(0,1)$, $\text{KeepTop-$k$}(\cdot)$ retrains only the top-$k$ expert contributions, and $\text{Softmax}(\cdot)$ function normalizes the selected weights. 
% The dMoE modules are shared on a layer-wise basis when the dimensionality is identical.
Following the dMoE modules, the embeddings $\bar{h_l}$ are passed to the decoder, which predicts the final output, $\hat{y}$. The network is optimized using the segmentation loss: 
\begin{align}
\min_{\mathcal{M}} \mathcal{L}(\hat{y}, y) = -\mathbb{E}_{x\sim P_X} \left[y_i \log p(\hat{y}_i)\right], 
\label{losses1}
\end{align}
\noindent where $\mathcal{M}$ represents any 2D-to-3D neural network architecture equipped with our proposed dMoE module, $y \in \mathbb{R}^{H W D}$ is the ground-truth label, and the predicted output $\hat{y} \in \mathbb{R}^{H W D}$, which is computed as follows:
\begin{align}
\hat{y} = \mathcal{M}(x, attr),\, 
\label{network_output}
\end{align}
\noindent where $x$ is the input image  and  $attr$ is  the attribute flag.  


\subsection{Interpreting dMoE Through Optimal Control} \label{method_oc}
In this section, we begin by providing a brief overview of training deep neural networks from the perspectives of dynamical systems and optimal control. Next, we demonstrate that for a given feedforward architecture as a non-feedback control, MoE can be interpreted as state feedback control. Finally, we show how dMoE relates to the mode-switching control variant of optimal control.

Neural networks create complex transformations by applying a sequence of changes to a hidden state. These transformations can be represented mathematically as follows:
\begin{equation}\label{Res}
    h_{l+1}=h_l+f(h_l,\theta_l).
\end{equation}
This equation illustrates iterative updates similar to an Euler discretization of a continuous transformation. The dynamics of these transformations are governed by an ODE, parameterized by a neural network, which is a continuous representation of the hidden units' dynamics:
\begin{equation}
    d h_{t}=f(h_t,u_t)dt.
\end{equation}
In practical applications, such as training neural networks with a dataset labeled $\{x, y\}$, the goal is to minimize the objective function through an optimization process:
\begin{equation}
\begin{split}
    \arg\min_{\{\theta_l\}}&\{\mathbb{E}_{\{x,y\}} L(\hat{y},y)|\\ & h_{l+1}=h_l+f(h_l,\theta_l),h_0=x,\hat{y}=h_L\}.
\end{split}
\end{equation}
This optimization problem can be seen as the discretization of a terminal control problem, which is governed by the neural ODE:
\begin{equation} \label{open}
\begin{split}
    \arg\min_{\{u^t\}}&\{\mathbb{E}_{\{x,y\}} L(\hat{y},y)|\\
    &dh_{t}=f(h_t,u_t),h_0=x,\hat{y}=h_L\}.
\end{split}    
\end{equation}
Here, the MoE framework can be considered an approximation of feedback control, also known as closed-loop control, as illustrated in \cref{fig_main}(b) (middle). 
The feedback control ensures optimal operational outcomes by leveraging real-time state information, $h_t$ to adjust control inputs dynamically. This is expressed mathematically as:
\begin{equation}
    d h_{t}=f(h_t,u_t(h_t))dt.
\end{equation}
Given the complexity of optimizing policy function through neural networks, kernel methods \cite{hofmann2008kernel} are often employed for parameterization:
\begin{equation}
   u_t(h_t)\approx \sum_i K(h_t,h_t^i)u^i_t(h^i_t),
\end{equation}
where $K$ is a kernel function, $h_i^t$ are anchor points, and $u^i_t(h^i_t)$ are fixed values.
%This enables the optimization functions $u$ to optimization fixed parameters $\theta^i_t = u^i_t(h^i_t)$:
This re-parameterization facilitates the shift in optimization focus from the function $u$ itself to the fixed parameters $\theta^i_t = u^i_t(h^i_t)$, enhancing the manageability and efficiency of the optimization process:
\begin{equation}
f(h_t,u_t(h_t))\approx f(h_t, \sum_i K(h_t,h^i_t)\theta^i_t).
\end{equation}
We consider the following kernel function 
\begin{equation} \label{kernel}
K(h_t, h_t^i) = \frac{\exp \langle \phi(h_t), \phi(h_t^i) \rangle}{\sum_i \exp \langle \phi(h_t), \phi(h_t^i) \rangle}.
\end{equation}
\textcolor{black}{To facilitate the learning of the kernel function $\phi$ we employ a reparameterization technique where $\langle \phi(h_t), \phi(h_t^i) \rangle=\langle h_t, \phi^*\phi(h_t^i) \rangle= (h_t^\top\cdot W)_i$. Here, the transformed parameters \(\phi^* \phi(h_t^i)\) are represented by the \(i\)-th column of \(W\), with \(\phi^*\) denoting the transpose of \(\phi\).} Due to considerations of computational complexity and robustness, we have incorporated Noisy Top-K Gating, corresponding to Eqs.~\ref{G1}, \ref{G2}, and \ref{G3}. Specifically, if the function \( f \) is linear with respect to its second argument and can be interpreted as the strategy of an expert, where each expert \( E_i \) acts as a controller with fixed parameters \( \theta_i \), then the following equation holds:
\begin{equation}
\begin{split}
   f(h_t, \sum_i K(h_t,h_t^i)\theta^i_t) &= \sum_i K(h_t,h^i_t) f(h_t, \theta^i_t) \\
   &= \sum_i G_i(h_t) E_i(h_t).
\end{split}
\end{equation}
In the field of optimal control, the methodology of choosing or switching between different control strategies based on varying environmental conditions is commonly referred to as mode-switching control or multi-modal control. This approach involves selecting the most suitable control law not only depending on the state $h_t$ but also on system parameters $attr$, aiming to optimize overall system performance.

In particular, we consider a special case by introducing a switching logic dependent on the external environmental conditions denoted by \( attr \). The dynamics of the system are described by the following equation:
\begin{equation}
    dh_t = f(h_t, u_t), \quad u_t = \kappa_{s(attr)}(h_t),
\end{equation}
where \( \kappa \) is a control strategy mapping that selects the most appropriate control input based on the current state and environment. If the system features several potential control modes, \( \kappa \) may comprise multiple sub-strategies, \( \kappa_1, \kappa_2, \ldots, \kappa_m \), each applicable under specific conditions. The logic for switching modes is governed by the switching function \( s(attr) \).

To prevent overfitting and to share knowledge learned from images of different distributions, we utilize a shared expert \( E_i \) across different flags \(attr\). \textcolor{black}{For each attribute, a distinct \( G \) is trained, implying that for different \( \text{attr} \), we adopt different functions \( \phi \) in Eq. \ref{kernel}, analogous to using different matrices \( W \) in Eq. \ref{G2}.} This results in the following control scheme:
\begin{align}
 \sum_{i=1}^{k} G^{attr}_i(h_t) \cdot E_i(h_t).
\end{align}
Considering that the neural network's layerwise structure represents a discrete form of the control, incorporating Patchify leads to Eq. \ref{eq_exeprt}. 
Therefore, dMoE can be interpreted as attribute-wise mode-switching control variant of optimal control for fairness learning, as reflected in the structural resemblance illustrated in \cref{fig_main}(a) and (b) (bottom).




\section{Experimental Results}



\subsection{Datasets}


To demonstrate the effectiveness of our proposed dMoE framework as an optimal control approach, we conduct extensive experiments on two benchmark datasets and an in-house dataset. For each dataset, we visualize the data distribution for each attribute in \cref{fig_intro} and provide further details of the training and test datasets in \cref{appen_data}. For the 2D segmentation experiments, we utilize two datasets: 1) Harvard-FairSeg \cite{tianfairseg} and 2) HAM10000 \cite{tschandl2018ham10000}. For the 3D segmentation experiments with \blue{clinical} attributes, we utilize \blue{our} in-house radiotherapy target dataset for prostate cancer patients. 

\paragraph{Harvard-FairSeg} is a scanning laser ophthalmoscopy (SLO) fundus image dataset comprising 10,000 samples with pixel-wise optic cup and outer neuroretinal rim segmentation masks for diagnosing glaucoma. It includes six key demographic attributes—age, gender, race, ethnicity, language preference, and marital status—enabling comprehensive studies of fairness. In this study, we focus on the race attribute, i.e. race $\in$ \{Black, Asian, White\}, as individuals from minor attribute subgroups face a risk of developing glaucoma compared to other groups, yet the segmentation accuracy is often lowest for this demographic \cite{tianfairseg}. Fairness and segmentation performance are evaluated on the test benchmark, which consists of 2,000 samples.

\paragraph{HAM10000} is a dermatology image dataset comprising 10,015 2D RGB samples with binary segmentation masks for diagnosing skin lesions. It includes demographic attributes such as sex and age, enabling targeted analysis of distributional disparities. In this study, we focus on the age attribute, as younger and older populations are underrepresented in the dataset. For age categorization, patients are divided into four groups at 20-year intervals, with the test benchmark consisting of 1,061 samples.

\paragraph{Radiotherapy Target Dataset} comprises pelvic CT scans of prostate cancer patients, accompanied by clinical target volume (CTV) segmentation masks for radiotherapy planning in radiation oncology. The dataset includes \blue{clinical factors}, such as tumor staging and histopathological findings. In this study, we focus on the Tumor (T)-stage attribute, i.e. T-stage $\in$ \{T1, T2, T3, T4\}, as the dataset exhibits an imbalance in population distribution. \blue{For example, radiotherapy can be applied across all T-stages in prostate cancer, but its aim may vary depending on clinical practices. In certain regions, radiotherapy alone is used as definitive treatment for both early and advanced stages without surgery. In contrast, other regions favor radiotherapy alone for early stages, while combining surgery with adjuvant radiotherapy for advanced stages, reflecting %regional 
institutional variations in treatment patterns.} To address biases arising from the imbalanced T-stage distribution when training deep neural networks for radiotherapy target segmentation, we utilize a training dataset comprising 721 primary prostate cancer patients from Yonsei Cancer Center, Seoul, South Korea, and validate network performance using an independent test set of 132 primary prostate cancer patients from Yongin Severance Hospital, Yongin, South Korea. The data collected for this study has been ethically approved by the IRB of the Department of Radiation Oncology at Yonsei Cancer Center and the Department of Radiation Oncology at Yongin Severance Hospital (IRB numbers 4-2023-0179 and 9-2023-0161). %The requirement for informed consent is waived because the study is retrospective.


\subsection{Implementation Details}

For all experiments, we set the dMoE module hyperparameters with Top-$k$ as 2 and the number of experts $n$ as 8. Each expert layer consists of a standard MLP with two linear layers, a ReLU activation, and a dropout layer. For 2D segmentation tasks, we use TransUNet \cite{chen2021transunet} as the backbone with the standard ViT-B architecture. The network is trained following the setup used in previous studies. All the input images are center-cropped and resized into 2D patches of size 224 × 224 pixels with a batch size of 42. The network is trained with a learning rate of 0.01 for 300 epochs on the Harvard-FairSeg dataset and 100 epochs on HAM10000, following the benchmark setting. The best performance is selected from checkpoints saved at 100-epoch intervals. For 3D radiotherapy target segmentation task, we adopt the 3D Residual U-Net \cite{cciccek20163d} as the backbone architecture, since it is reported as an effective architecture for radiotherapy target segmentation \cite{oh2023llm}. The network is trained using randomly cropped 3D patches of size 384 × 384 × 128 voxels and a batch size of 4. During evaluation, the entire 3D CT volumes are processed using a sliding window approach. The training is conducted with a learning rate of $5 \times 10^{-5}$ over 100 epochs and early stopping based on the validation dataset. The additional computational costs introduced to each backbone are compared in \cref{appen_computation}.  
We implement the networks using PyTorch \cite{paszke2019pytorch} in Python with CUDA 11.8 The AdamW \cite{loshchilov2017decoupled} optimizer with exponential learning rate decay is used for all experiments. For 2D segmentation tasks, we use a single NVIDIA A100 80GB GPU, while for the 3D segmentation task, we use a single NVIDIA RTX A6000 48GB GPU.

\subsection{Baseline Method and Evaluation Metrics}
\label{metric}

We evaluate our method against baseline approaches for fairness learning. On the 2D Harvard-FairSeg dataset, we compare our approach with reported metrics from adversarial (ADV) training \cite{madras2018learning}, distributionally robust optimization (DRO) \cite{sagawa2019distributionally}, fair-error-bound scaling (FEBS) \cite{tianfairseg}, and generative model-based augmentation (FairDiff). We implement our method under the same training conditions as the benchmark methods. For further experiments on the 2D HAM10000 dataset and the 3D  radiotherapy target dataset, we compare our method with FEBS among SOTA methods. We exclude the generative model-based approach, FairDiff, as it requires additional image generation and evaluation tailored to 2D and 3D datasets, which makes it suboptimal for direct comparison. Moreover, we use the default MoE as an additional baseline, replacing our proposed dMoE modules by default MoE modules without distribution-aware router.  We implement all methods under the same training conditions as our method. To evaluate performance, we employ the Dice Similarity Coefficient, Intersection over Union (IoU), as well as {equity-scaled segmentation performance (ESSP) metrics for both Dice and IoU—denoted as ES-Dice and ES-IoU—following  \cite{tianfairseg}:}
\begin{equation}
\text{ESSP} = \frac{I(\{(\hat{y}, y)\})}{1 + \Delta}, \\
\end{equation}
where,
\begin{equation}
\Delta = \sum_{attr \in {A}} \left| I(\{(\hat{y}, y)\}) - I(\{(\hat{y}, a, y) \mid a = attr\}) \right|,
\end{equation}
{where $I \in \{\text{Dice}, \text{IoU}\}$ and $A$ represents the set of all demographic groups. While ESSP evaluates fairness in performance across diverse demographic subgroups, it has limitations in capturing substantial performance gains for specific subgroups, particularly in the 3D radiotherapy target segmentation task.} We complement the limitation of ESSP metrics by providing quantitative analysis with violin plots, which offer a more detailed visualization of equity and performance distribution across diverse subgroups.


\begin{table*}[h!]
\begin{center}
\caption{Comparison on 2D Harvard-FairSeg dataset with {\bf{race}} as the distribution attribute.}\label{tab_optic}
\vskip 0.15in
\resizebox{0.73\linewidth}{!}{
\begin{tabular}{lccccccccccc}
\toprule
% \multirow{2}{*}{Distribution} & 
\multirow{2}{*}{Method} & \multicolumn{4}{c}{All (n=2000)} & \multicolumn{2}{c}{Asian (n=169)} & \multicolumn{2}{c}{Black (n=299)} & \multicolumn{2}{c}{White (n=1532)} \\
  % \cmidrule(l){3-6}  \cmidrule(lr){7-8}  \cmidrule(lr){9-10} \cmidrule(lr){11-12} 
  \cmidrule(l){2-5}  \cmidrule(lr){6-7}  \cmidrule(lr){8-9} \cmidrule(lr){10-11} 
 % &
 & ES-Dice  & Dice  & ES-IoU  & IoU  & Dice  & IoU  & Dice  & IoU  & Dice  & IoU  \\

\midrule
 % \multirow{16}{*}{\shortstack[l]{{Benchmark}}} 
% &
{Rim Segmentation} &  \\
   % \cmidrule(l){2-2} \cmidrule(l){3-6}  \cmidrule(lr){7-8}  \cmidrule(lr){9-10} \cmidrule(lr){11-12} 
   \cmidrule(l){1-1} \cmidrule(l){2-5}  \cmidrule(lr){6-7}  \cmidrule(lr){8-9} \cmidrule(lr){10-11} 
% & 
TransUNet \cite{chen2021transunet} & 0.703 & 0.793 & 0.585 & 0.671 & 0.746 & 0.616 & 0.731 & 0.599 & 0.811 & 0.691 \\
% & 
+ ADV \cite{madras2018learning}  & 0.700 & 0.791 & 0.583 & 0.668 & 0.741 & 0.612 & 0.729 & 0.598 & 0.809 & 0.689\\
% & 
+ DRO \cite{sagawa2019distributionally} & 0.700 & 0.790 & 0.581 & 0.667 & 0.747 & 0.618 & 0.723 & 0.590 & 0.808 & 0.689 \\
% & 
+ FEBS \cite{tianfairseg} & 0.705 & 0.795 & 0.587 & 0.673 & 0.748 & 0.619 & 0.733 & 0.602 & 0.813 & 0.694 \\
% &
+ FairDiff \cite{li2024fairdiff} &  0.716 & {0.800} & 0.596 &  {0.680} & {0.757} & {0.628} & 0.743 & 0.611 & {0.817} & {0.699}\\
% & 
+ MoE & 0.732 & 0.804 & 0.614 & 0.686 & 0.760 & 0.635 & 0.763 & 0.635 & 0.817 & 0.701 \\
% & 
+ dMoE & \bf0.743 & \bf0.813 & \bf0.627 & \bf0.698 & \bf0.769 & \bf0.645 & \bf0.776 & \bf0.652 & \bf0.825 & \bf0.713 \\

% \cmidrule(l){2-2} \cmidrule(l){3-6}  \cmidrule(lr){7-8}  \cmidrule(lr){9-10} \cmidrule(lr){11-12} 
\cmidrule(l){1-1} \cmidrule(l){2-5}  \cmidrule(lr){6-7}  \cmidrule(lr){8-9} \cmidrule(lr){10-11} 
% &
{Cup Segmentation} &  \\
% \cmidrule(l){2-2} \cmidrule(l){3-6}  \cmidrule(lr){7-8}  \cmidrule(lr){9-10} \cmidrule(lr){11-12} 
\cmidrule(l){1-1} \cmidrule(l){2-5}  \cmidrule(lr){6-7}  \cmidrule(lr){8-9} \cmidrule(lr){10-11} 
% &
TransUNet \cite{chen2021transunet} & 0.828 & 0.848 & 0.730 & 0.753 & 0.827 & 0.728 & {0.849}  & {0.758} & 0.850 & 0.755 \\
% &
+ ADV \cite{madras2018learning} & 0.826 & 0.841 & 0.727 & 0.743 & 0.825 & 0.726 &0.842  & 0.748 & 0.843 & 0.744\\
% &
+ DRO \cite{sagawa2019distributionally} & 0.820 & 0.844 & 0.725 & 0.748 & 0.820 & 0.723 & 0.847   &  0.753 & 0.846 & 0.750 \\
% &
+ FEBS \cite{tianfairseg} & 0.825 & 0.846 & 0.727 & 0.750 & 0.825 & 0.725 & {0.848} & {0.755} & 0.848 & 0.751 \\
% &
+ FairDiff \cite{li2024fairdiff} &  0.825 & 0.848 & 0.736 & 0.753 & 0.832 & 0.735 & 0.848 & 0.757 & 0.850 & 0.754\\
% & 
+ MoE &  \bf0.834 & 0.854 & 0.745 & 0.762 &  \bf0.845 &  \bf0.757 & 0.842 & 0.748 & 0.857 & 0.765 \\
% &
+ dMoE &  \bf0.834 & \bf0.862 & \bf0.748 &\bf 0.773 & 0.844 & 0.755 & \bf0.851 & \bf0.761  & \bf0.866 & \bf0.777\\


\bottomrule

\multicolumn{10}{l}{{{$\textit{Note.}$ {ES-Dice and ES-IoU assess fairness by measuring performance consistency across entire attribute subgroups.}}}}

\end{tabular}
}
\end{center}
\vskip -0.1in
\end{table*}





\begin{table*}[h!]
\begin{center}
\caption{Comparison on 2D HAM10000 dataset for skin lesion segmentation with {\bf{age}} as the distribution attribute.}\label{tab_skin}
\vskip 0.15in
\resizebox{0.89\linewidth}{!}{
\begin{tabular}{lcccccccccccccc}
\toprule
\multirow{3}{*}{Method}  & \multicolumn{4}{c}{All} & \multicolumn{2}{c}{Age $\geq$ 80}  & \multicolumn{2}{c}{Age $\geq$ 60 } & \multicolumn{2}{c}{Age $\geq$ 40} &  \multicolumn{2}{c}{Age $\geq$ 20}  & \multicolumn{2}{c}{Age $<$ 20}  \\

& \multicolumn{4}{c}{ (n=1061)}  & \multicolumn{2}{c}{(n=121)} & \multicolumn{2}{c}{(n=469)}  & \multicolumn{2}{c}{(n=328)} & \multicolumn{2}{c}{(n=120)} & \multicolumn{2}{c}{(n=24)} \\

 \cmidrule(l){2-5}  \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11} \cmidrule(lr){12-13}  \cmidrule(lr){14-15} 
 
 & ES-Dice  & Dice  & ES-IoU  & IoU  & Dice   & IoU  & Dice   & IoU  & Dice   & IoU  & Dice   & IoU  & Dice   & IoU  \\
 \midrule
 
% SAM \cite{kirillov2023segment} & 0.828 & 0.836 & 0.845 & 0.827  \\
% SAMed \cite{samed} & 0.909 & 0.909 & 0.910 & 0.908 \\
% + InD \cite{wang2020towards} & 0.882 & 0.889 & 0.896 & 0.882\\
% + Resampling \cite{zong2022medfair} & 0.914& 0.915 & 0.916 & 0.914 \\
% + FEBS \cite{tianfairseg} & 0.916 & 0.916 & 0.916 & 0.916 \\
% + dMoE (Ours) & \\
% \cmidrule(l){1-1}  \cmidrule(l){2-3}  \cmidrule(lr){4-4}  \cmidrule(lr){5-5} 
TransUNet \cite{chen2021transunet} & 0.831 & 0.876 & 0.735 & 0.824  & 0.862 & 0.787 & 0.868 & 0.809 & 0.888 & 0.846 & 0.895 & 0.857 & 0.875 & 0.839    \\
+ FEBS \cite{tianfairseg} & 0.787 & 0.858 & 0.679 & 0.798 & 0.831 & 0.747 & 0.844 & 0.774  & 0.884 & 0.837 & 0.871 & 0.827 & 0.869 & 0.830   \\
+ MoE & 0.836 & 0.882 & 0.742 & 0.833 & \bf0.864 & \bf0.794 & 0.875 & 0.820 & 0.889 & \bf0.851 & \bf0.904 & \bf0.869 & \bf0.882 & \bf0.850   \\
+ dMoE & \bf0.841 & \bf0.884 & \bf0.749 & \bf0.834 & \bf0.864 & 0.791 & \bf0.881 & \bf0.824 & \bf0.890 & 0.850  & 0.901 & 0.866 & 0.880 & 0.846   \\


\bottomrule
\end{tabular}
}
\vskip -0.1in
\end{center}
\end{table*}



\begin{table*}[h!]
\begin{center}
\caption{Comparison on 3D radiotherapy target segmentation with {\bf{tumor stage}} as the distribution attribute.}\label{tab_3d}
\vskip 0.15in
\resizebox{0.81 \linewidth}{!}{
\begin{tabular}{lcccccccccccc}
\toprule
%  \multirow{2}{*}{Method} & \multicolumn{4}{c}{All  (n=182)} & \multicolumn{2}{c}{T1 (n=5)} & \multicolumn{2}{c}{T2 (n=62)} & \multicolumn{2}{c}{T3 (n=107)}  & \multicolumn{2}{c}{T4 (n=8)} \\
%  \cmidrule(l){2-5}  \cmidrule(lr){6-7}  \cmidrule(lr){8-9} \cmidrule(lr){10-11} \cmidrule(lr){12-13} 
%  & ES-Dice  & Dice  & ES-IoU  & IoU  & Dice   & IoU  & Dice   & IoU  & Dice   & IoU  & Dice   & IoU  \\
% \midrule
% \multicolumn{13}{l}{{Internal Test }}  \\
%  \cmidrule(l){1-1}  \cmidrule(l){2-5}  \cmidrule(lr){6-7}  \cmidrule(lr){8-9} \cmidrule(lr){10-11} \cmidrule(lr){12-13} 
% 3D ResUNet \cite{cciccek20163d} & \bf{0.645} & \bf{0.734} & \bf{0.530} & \bf{0.610} & {0.640} & {0.501} & \bf{0.727} & \bf{0.604} & {0.745} & {0.621} & {0.708} & {0.585} \\
% + FEBS \cite{tianfairseg} & {0.595} & {0.722} & {0.476} & {0.593} & {0.560} & {0.407} & {0.705} & {0.576} & {0.738} & {0.610} & {0.741} & {0.619}\\ 
% + MoE &  {0.607} & {0.720} & {0.496} & {0.594} & {0.578} & {0.438} & {0.702} & {0.575} & {0.738} & {0.613} & {0.712} & {0.589} \\
% + dMoE &  \bf{0.645} & {0.733} & \bf{0.530} & {0.609} & \bf{0.801} & \bf{0.677} & {0.702} & {0.574} & \bf{0.747} & \bf{0.624} & \bf{0.757} & \bf{0.641}  \\
% % + dMoE/Decoder & \bf{0.695} & \bf{0.737} & \bf{0.555} & \bf{0.612} & \bf{0.750} & \bf{0.645} & \bf{0.706} & \bf{0.573} & \bf{0.752} & \bf{0.629} & \bf{0.739} & \bf{0.625} \\
% % + dMoe (Skip) & \bf{0.681} & \bf{0.735} & \bf{0.551} & \bf{0.610} & \bf{0.754} & \bf{0.631} & \bf{0.708} & \bf{0.576} & \bf{0.745} & \bf{0.623} & \bf{0.758} & \bf{0.649}\\
% % \cmidrule(l){1-1}  \cmidrule(l){2-5}  \cmidrule(lr){6-7}  \cmidrule(lr){8-9} \cmidrule(lr){10-11} \cmidrule(lr){12-13} 
% % 3D LLMSeg$^\dagger$ \cite{oh2023llm} & {0.657} & {0.749} & {0.530} & {0.625} & {0.856} & {0.751} & {0.730} & {0.602} & {0.755} & {0.630} & {0.758} & {0.649} \\
% % + FEBS \cite{tianfairseg} & \\ 
% % + MoE & {0.695} & {0.740} & {0.580} & {0.616} & {0.733} & {0.621} & {0.715} & {0.588} & {0.756} & {0.633} & {0.724} & {0.605}  \\
% % + dMoE & \bf{0.672} & \bf{0.739} & \bf{0.546} & \bf{0.615} & \bf{0.782} & \bf{0.668} & \bf{0.704} & \bf{0.575} & \bf{0.757} & \bf{0.634} & \bf{0.744} & \bf{0.628}\\

% \midrule
\multirow{2}{*}{Method} & \multicolumn{4}{c}{All  (n=132)} & \multicolumn{2}{c}{T1 (n=1)} & \multicolumn{2}{c}{T2 (n=55)} & \multicolumn{2}{c}{T3 (n=64)}  & \multicolumn{2}{c}{T4 (n=12)} \\
 \cmidrule(l){2-5}  \cmidrule(lr){6-7}  \cmidrule(lr){8-9} \cmidrule(lr){10-11} \cmidrule(lr){12-13} 
 & ES-Dice  & Dice  & ES-IoU  & IoU  & Dice   & IoU  & Dice   & IoU  & Dice   & IoU  & Dice   & IoU  \\
\midrule
% \multicolumn{13}{l}{{External Test}}  \\
 \cmidrule(l){1-1}  \cmidrule(l){2-5}  \cmidrule(lr){6-7}  \cmidrule(lr){8-9} \cmidrule(lr){10-11} \cmidrule(lr){12-13} 
3D ResUNet \cite{cciccek20163d} & {0.583} & {0.671} & {0.443} & {0.521} & {0.659} & {0.491} & \bf{0.635} & \bf{0.485} & {0.712} & {0.564} & {0.609} & {0.455}\\
+ FEBS \cite{tianfairseg} &{0.549} & {0.652} & {0.413} & {0.496} & {0.593} & {0.421} & {0.587} & {0.430} & {0.707} & {0.554} & {0.660} & {0.499}\\ 
+ MoE  &  \bf{0.584} & {0.687} & \bf{0.444} & {0.540} & {0.658} & {0.490} & {0.611} & {0.459} & {0.754} & {0.614} & {0.682} & {0.528} \\
+ dMoE &  {0.546} & \bf{0.711} & {0.424} & \bf{0.569} & \bf{0.828} & \bf{0.707} & {0.634} & {0.482} & \bf{0.765} & \bf{0.630} & \bf{0.767} & \bf{0.626} \\

%  \cmidrule(l){1-1}  \cmidrule(l){2-5}  \cmidrule(lr){6-7}  \cmidrule(lr){8-9} \cmidrule(lr){10-11} \cmidrule(lr){12-13} 
% 3D LLMSeg$^\dagger$ \cite{oh2023llm} & {0.628} & {0.714} & {0.496} & {0.571} & {0.818} & {0.693} & {0.709} & {0.570} & {0.721} & {0.575} & {0.692} & {0.545}  \\
% + FEBS \cite{tianfairseg} & \\ 
% + MoE & {0.628} & {0.705} & {0.500} & {0.562} & {0.769} & {0.624} & {0.692} & {0.553} & {0.720} & {0.575} & {0.675} & {0.524} \\
% + dMoE & \bf{0.653} & \bf{0.732} & \bf{0.527} & \bf{0.591} & \bf{0.786} & \bf{0.648} & \bf{0.698} & \bf{0.557} & \bf{0.759} & \bf{0.620} & \bf{0.736} & \bf{0.589} \\

\bottomrule

% \multicolumn{13}{l}{{{$^\dagger$LLMSeg is a multimodal segmentation framework, which processes image inputs as well as both demographic and pathological information}}}\\
% \multicolumn{13}{l}{{{as additional text inputs, e.g., $<$Gleason Grade$>$ 7 (3+4) $<$TN-stage$>$ {{pT2c}}, N0 $<$Metastasis$>$ - $<$Age$>$ 64 $<$Prostate Specific Antigen$>$ 4.5}}}
\end{tabular}
}
\end{center}
\vskip -0.1in
\end{table*}





\begin{figure*}[!h]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=1.82\columnwidth]{fig/result.pdf}}
\caption{(a) Violin plots depicts attribute-wise segmentation performance. {Transparent blue lines within the plots connect the most densely populated regions for each attribute, visually representing overall equity.} (b) Qualitative comparison  across different subgroups. }


\label{fig_result}
\end{center}
\vskip -0.2in
\end{figure*}




\subsection{Results}

\subsubsection{2D Segmentation with Demographic Attributes}

We first evaluate dMoE for 2D segmentation on benchmark datasets. For the Harvard-FairSeg neuroretinal rim and optic nerve segmentation tasks, our method consistently achieves SOTA performance in terms of ES-Dice and ES-IoU, surpassing previous fairness learning approaches, as shown in \cref{tab_optic}. Specifically, our method demonstrates superior performance for the minor attribute subgroup (Black), achieving a Dice score of 0.776 for rim segmentation, compared to FairDiff (0.743) and FEBS (0.733). Similarly, for optic cup segmentation, our method significantly improves performance for the Asian attribute subgroup, achieving a Dice score of 0.844 compared to FairDiff (0.832) and FEBS (0.825). Although MoE, which removes the distribution-aware gating network from our proposed method, achieves comparable performance (0.845), dMoE outperforms MoE across other subgroups for both rim and cup segmentation. Furthermore, the shared structure within dMoe module across subgroups enables synergistic effects, maintaining or even improving performance for the major attribute subgroup (White). For the HAM10000 skin lesion segmentation task, our method also demonstrates SOTA performance in ES-Dice and ES-IoU, with scores of 0.841 and 0.749, respectively, outperforming MoE (0.836 and 0.742) and FEBS (0.787 and 0.679), as shown in \cref{tab_skin}. Specifically, loss function modification methods, such as FEBS, exhibit inferior performance across attribute subgroups compared to the baseline method. This suggests that when the training dataset for each subgroup is insufficient, these methods may sacrifice overall performance. MoE shows promising results for minor attribute subgroups, dMoE further enhances equity across all subgroups. %, demonstrating greater robustness and adaptability.



\subsubsection{3D Radiotherapy Target Segmentation with \blue{Clinical Attributes}}

We further evaluate dMoE for 3D radiotherapy target segmentation, using T-stage as a distributional attribute. \cref{tab_3d} summarizes the segmentation performance. \add{Despite the test set being acquired from a different hospital with a distinct data distribution, dMoE demonstrates promising generalization performance.} The strength of dMoE is particularly evident in underrepresented subgroups, such as T1 and T4 where the frequency is significantly lower than that of T2 and T3, by resulting the most notable performance improvements in these subgroups. 
% Specifically, on the internal test set, dMoE achieved a Dice score of 0.801 for T1, surpassing the baseline 3D ResUNet (0.640), FEBS (0.560), and MoE (0.578). Similarly, for T4, dMoE improved the Dice score to 0.757, outperforming MoE (0.712). While FEBS demonstrated competitive performance in the T4 subgroup, it struggled with subgroups that have extremely low representation, such as T1.}
%One of dMoE’s key strengths is its ability to enhance segmentation performance for minor attribute subgroups, \blue{particularly the underrepresented T1 and T4 stages.} On the internal test set, dMoE achieved a Dice score of 0.776 for T1, outperforming the baseline 3D ResUNet (0.640) as well as other methods, including FEBS (0.560) and MoE (0.587). Similarly, for the T4 stage, dMoE significantly improved the Dice score to 0.756, exceeding the performance of MoE (0.712). Although FEBS exhibited competitive performance for the T4 subgroup, it struggled with extremely low-representation subgroups, such as T1. 
Specifically, dMoE demonstrates robust and consistent performance, particularly excelling in the T4 subgroup with a Dice score of 0.767, significantly outperforming FEBS (0.660) and MoE (0.683). Moreover, despite the limited sample size of the T1 subgroup, dMoE achieves superior performance. This consistent improvement demonstrates dMoE’s generalizability and robustness, even when applied to datasets from different centers with varying data distributions. 

{However, as noted in \cref{metric}, ESSP metrics have limitations in capturing significant performance gains for minor attribute subgroups. To address this, we further illustrate violin plots in \cref{fig_result}(a). Comparison on violin plots, with equity measures indicated by transparent blue lines, confirms dMoE’s ability to maintain equity across attribute-wise subgroups.} Qualitative results for each subgroup are presented in \cref{fig_result}(b). As observed in the ground truth labels, the radiotherapy target volume tends to expand with the progression of the tumor's clinical stage. This trend is distinctly captured by dMoE, which demonstrates superior adaptability to distribution-specific characteristics, thereby achieving notable fairness in performance. 





\subsubsection{Ablation Studies}

We further conduct additional analyses to explore the effects of various configurations of the dMoE module. Specifically, we vary the placement of the dMoE module within the encoder and decoder and investigate whether its parameters should be shared or separated across layers to determine the optimal architecture. Detailed ablation study results are provided in \cref{appen_ablation}. %Notably, the impact of parameter sharing differs between the backbone network architectures. For the Transformer architecture, sharing dMoE parameters across layers enhances performance due to consistent dimensionality among layers. Conversely, for CNNs, layer-wise dimensional discrepancies made parameter sharing suboptimal. Thus, a layer-wise dMoE configuration shall be adopted.


\section{Conclusion, Limitations \& Future Work}
{Adopting an optimal control perspective, we design distribution-aware Mixture of Experts (dMoE) architectures to address data imbalance issues. By modeling distribution as an external factor influencing control, we integrate dMoE into medical image segmentation for more effective handling of imbalanced clinical datasets. }
\blue{Given the nature of clinical practice, which accounts for distributional characteristics, our proposed algorithm offers a promising solution to data imbalance and provides a robust framework for clinical decision-making that aligns with clinicians' perspectives.} 
\add{Notably, ensuring distributional equity is essential for real-world clinical AI deployment, where training and deployment dataset distributions often differ. Our distribution-aware dMoE holds promise in adapting trained models to unknown distributions, thereby improving the success of clinical AI integration across diverse hospitals.}

However, several limitations remain for this study:

First, while we explore diverse attributes across three datasets, the performance improvement trends vary depending on the task and dataset characteristics. Moreover, the optimal configuration of the dMoE module varies between the two different architectures. Future research will focus on uncovering the underlying characteristics of attribute-wise subgroups within each dataset to identify the factors driving these variations and will aim to develop a more generalized module capable of delivering task-agnostic performance improvements.

Second, our focus on a single attribute per task limits the exploration of combined attribute imbalances. For instance, while patients with T2-stage cancer may be prevalent, further subgroup imbalances, such as age or metastasis status, could exist. Addressing this issue may require integrating multiple attributes within a hierarchical dMoE framework, which we plan to explore in future studies.

Last, this study adopts an optimal control perspective to design efficient deep neural network architectures, leveraging back-propagation for parameter optimization. Future research will explore how advanced numerical methods from optimal control theory can inspire novel optimization algorithms specifically tailored to enhance fairness learning.

Despite these limitations, we believe our distribution-aware approach represents a step forward in advancing fairness learning for diverse data-imbalanced clinical scenarios.

%\section*{Accessibility}
%Authors are kindly asked to make their submissions as accessible as possible for everyone including people with disabilities and sensory or neurological differences.
%Tips of how to achieve this and what to pay attention to will be provided on the conference website \url{http://icml.cc/}.
%
%\section*{Software and Data}
%
%If a paper is accepted, we strongly encourage the publication of software and data with the
%camera-ready version of the paper whenever appropriate. This can be
%done by including a URL in the camera-ready copy. However, \textbf{do not}
%include URLs that reveal your institution or identity in your
%submission for review. Instead, provide an anonymous URL or upload
%the material as ``Supplementary Material'' into the OpenReview reviewing
%system. Note that reviewers are not required to look at this material
%when writing their review.


% % Acknowledgements should only appear in the accepted version.
% \section*{Acknowledgements}

% \textbf{Do not} include acknowledgements in the initial version of
% the paper submitted for blind review.

% If a paper is accepted, the final camera-ready version can (and
% usually should) include acknowledgements.  Such acknowledgements
% should be placed at the end of the section, in an unnumbered section
% that does not count towards the paper page limit. Typically, this will 
% include thanks to reviewers who gave useful comments, to colleagues 
% who contributed to the ideas, and to funding agencies and corporate 
% sponsors that provided financial support.

% \section*{Ethical Statement}


%\newpage

\section*{Impact Statement}
This paper aims to advance fairness in medical image segmentation by addressing biases arising from imbalanced clinical data distributions. Our work demonstrates robustness across diverse datasets and network architectures, contributing to equitable and reliable AI-driven healthcare applications.
 
\nocite{langley00}

\bibliography{refs}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
% \setcounter{table}{0}  % Resets table numbering
% \setcounter{figure}{0} % Resets figure numbering

% Disable numbering for tables on this specific page


\section{Appendix}

\subsection{Detailed Network Architecture.}
\label{appen_arch}

We provide the network architecture for a better understanding of the dMoE modules location within each Transformer-based and CNN-based architecture in \cref{table_transunet} and \cref{table_3dresunet}, respectively. %Additionally, we present a detailed design of the dMoE in \cref{tab_dmoe}.

\begin{table*}[h!]
\begin{center}
\caption{dMoE within Transformer (TransUNet).}\label{table_transunet}
\vskip 0.15in
\resizebox{0.5 \linewidth}{!}{
\begin{tabular}{ccccc}
\toprule
\multirow{2}{*}{{Module}} &   \multirow{2}{*}{Layer Block} &   \multirow{2}{*}{Resample} & \multirow{2}{*}{dMoE}  & Data dimension \\ 
& & &  & (C $\times$ H $\times$ W)  \\

\cmidrule(l){1-1} \cmidrule(l){2-2} \cmidrule(l){3-3} \cmidrule(l){4-4} \cmidrule(l){5-5}

\multirow{2}{*}{In} & - & - &  - & $Ch_{in}$ $\times$ 224 $\times$ 224 \\
& Conv & - & - & 1 $\times$  14 $\times$ 14 \\
\cmidrule(l){1-1} \cmidrule(l){2-2} \cmidrule(l){3-3} \cmidrule(l){4-4} \cmidrule(l){5-5}

\multirow{5}{*}{{Encoder} } & $\text{AttentionBlock}_{1}$ & - &  & 768 $\times$ (14 $\times$ 14) \\
& $\text{AttentionBlock}_{2}$ & - &  & 768 $\times$ (14 $\times$ 14)  \\
& $\vdots$  & -  & dMoE & $\vdots$ \\
& $\text{AttentionBlock}_{11}$ & - &  & 768 $\times$ (14 $\times$ 14)   \\
& $\text{AttentionBlock}_{12}$ & - &  & 768 $\times$ (14 $\times$ 14)   \\
\cmidrule(l){1-1} \cmidrule(l){2-2} \cmidrule(l){3-3} \cmidrule(l){4-4} \cmidrule(l){5-5}

\multirow{4}{*}{Decoder} & $\text{UpResBlock}_{4}$ & Up & - & 256  $\times$ 28 $\times$ 28 \\
& $\text{UpResBlock}_{3}$ &  Up & - & 128  $\times$ 56 $\times$ 56\\
& $\text{UpResBlock}_{2}$ &  Up & - & 64 $\times$ 112 $\times$ 112 \\
& $\text{UpResBlock}_{1}$ &  Up & - & 16 $\times$ 224 $\times$ 224\\
\cmidrule(l){1-1} \cmidrule(l){2-2} \cmidrule(l){3-3} \cmidrule(l){4-4} \cmidrule(l){5-5}

\multirow{1}{*}{Out } &  $\text{Conv}$ &  -   &  - & $Ch_{out}$  $\times$ 224 $\times$ 224 \\
\bottomrule
\end{tabular}
}
\end{center}
\vskip -0.1in
\end{table*}



\begin{table*}[h!]
\begin{center}
\caption{dMoE within 3D CNN (3D ResUNet).}\label{table_3dresunet}
\vskip 0.15in
\resizebox{0.6 \linewidth}{!}{
\begin{tabular}{cccccc}
\toprule
\multirow{2}{*}{{Module}} & \multirow{2}{*}{Layer Block}   &  \multirow{2}{*}{Resample} &  \multirow{2}{*}{dMoE} & Skip- & Data dimension \\ 
& & & & Connection & (C $\times$ H $\times$ W $\times$ D)  \\
\cmidrule(l){1-1} \cmidrule(l){2-2} \cmidrule(l){3-3} \cmidrule(l){4-4} \cmidrule(l){5-5} \cmidrule(l){6-6}

{In} & Conv & - & - & - & $Ch_{in}$ $\times $ 384 $ \times $ 384 $ \times $ 128 \\
\cmidrule(l){1-1} \cmidrule(l){2-2} \cmidrule(l){3-3} \cmidrule(l){4-4} \cmidrule(l){5-5} \cmidrule(l){6-6}

\multirow{5}{*}{{Encoder}}& $\text{ResBlock}_{1}$ & Down &  $\text{dMoE}_{1}$ & \multicolumn{1}{l}{$\hspace{6mm}\linefeed$}  & 48 $\times $ 192 $ \times $ 192 $ \times $ 64  \\
& $\text{ResBlock}_{2}$ & Down & $\text{dMoE}_{2}$ & \multicolumn{1}{l}{$\hspace{4mm}\linefeed$} & 48 $\times $ 96 $ \times $ 96 $ \times $ 32  \\
& $\text{ResBlock}_{3}$ & Down & $\text{dMoE}_{3}$ &  \multicolumn{1}{l}{$\hspace{2mm}\linefeed$} & 96 $\times $ 48 $ \times $ 48 $ \times $ 16 \\
& $\text{ResBlock}_{4}$ & Down & $\text{dMoE}_{4}$ &   \multicolumn{1}{l}{$\linefeed$} & 192 $\times $ 24 $ \times $ 24 $ \times $ 8 \\
& $\text{ResBlock}_{5}$ & Down & $\text{dMoE}_{5}$ &   \multicolumn{1}{l}{$\hspace{2mm}$} & 384 $\times $ 12 $ \times $ 12 $ \times $ 4 \\
\cmidrule(l){1-1} \cmidrule(l){2-2} \cmidrule(l){3-3} \cmidrule(l){4-4} \cmidrule(l){5-5} \cmidrule(l){6-6}


\multirow{4}{*}{Decoder } & $\text{UpResBlock}_{4}$ & Up & - & \multicolumn{1}{l}{$\carriagereturn$} & 192 $\times $ 24 $ \times $ 24 $ \times $ 8  \\
& $\text{UpResBlock}_{3}$ & Up & - & \multicolumn{1}{l}{$\hspace{1.8mm}\carriagereturn$} & 96 $\times $ 48 $ \times $ 48 $ \times $ 16\\
& $\text{UpResBlock}_{2}$ & Up & - & \multicolumn{1}{l}{$\hspace{3.6mm}\carriagereturn$} & 48 $\times $ 96 $ \times $ 96 $ \times $ 32\\
& $\text{UpResBlock}_{1}$ & Up & - & \multicolumn{1}{l}{$\hspace{5.4mm}\carriagereturn$} & 48 $\times $ 192 $ \times $ 192 $ \times $ 64 \\
\cmidrule(l){1-1} \cmidrule(l){2-2} \cmidrule(l){3-3} \cmidrule(l){4-4} \cmidrule(l){5-5} \cmidrule(l){6-6}

\multirow{1}{*}{Out } & TransposeConv & Up & - & - & $Ch_{out}$ $\times $ 384 $ \times $ 384 $ \times $ 128\\

\bottomrule
\end{tabular}
}
\end{center}
\vskip -0.1in
\end{table*}




% \begin{table*}[h!]
% \begin{center}
% \caption{dMoE architecture.}\label{tab_dmoe}
% \vskip 0.15in
% \resizebox{0.6 \linewidth}{!}{
% \begin{tabular}{ccccc}
% \toprule
% \multirow{2}{*}{{Module}} &   \multirow{2}{*}{Layer Block} & \multirow{2}{*}{Top-K Gating} &   \multirow{2}{*}{Expert ($ E $)}  & Data dimension \\ 
% & & &  & (C $\times$ H $\times$ W $\times$ S)  \\

% \cmidrule(l){1-1} \cmidrule(l){2-2} \cmidrule(l){3-3} \cmidrule(l){4-4} \cmidrule(l){5-5}

% \multirow{2}{*}{In} &  &  &   & $Ch_l \times H_l \times W_l \times D_l$ \\

% &  Patchify &  &  & $Ch_l \times (H_l \times W_l \times D_l)$ \\
% \cmidrule(l){1-1} \cmidrule(l){2-2} \cmidrule(l){3-3} \cmidrule(l){4-4} \cmidrule(l){5-5}

% \multirow{4}{*}{dMoE}  & $\text{Linear}_{W}$&  &  & $n \times (H_l \times W_l \times D_l)$   \\
% & $\text{Linear}_{W_{}noise}$ & & & $n \times (H_l \times W_l \times D_l)$  \\
% & $\text{SoftMax}$ &  &  &  $n \times (H_l \times W_l \times D_l)$   \\
% & Top-K Gating &  &  & $K \times (H_l \times W_l \times D_l)$   \\
% & & Top-1 &   & $Ch_l \times (H_l \times W_l \times D_l)$    \\
% & & Top-2 & MLP & $Ch_l \times (H_l \times W_l \times D_l)$    \\
% \cmidrule(l){1-1} \cmidrule(l){2-2} \cmidrule(l){3-3} \cmidrule(l){4-4} \cmidrule(l){5-5}
 
% \multirow{1}{*}{Out}  & Unpatchify & & & $Ch_l \times H_l \times W_l \times D_l$ \\

% \bottomrule
% \end{tabular}
% }
% \end{center}
% \vskip -0.1in
% \end{table*}




\subsection{Dataset Details.}
\label{appen_data}

We further provide the trainset and testset for each dataset, along with the attribute subgroup-wise data distribution and percentiles for the trainset in \cref{tab_data}.

\begin{table*}[h!]
\begin{center}
\caption{Detailed distribution of data across attribute subgroups.}\label{tab_data}
\vskip 0.15in
\resizebox{0.8 \linewidth}{!}{
\begin{tabular}{lccccccccccccccccc}
\toprule
& \multicolumn{4}{l}{\bf{Harvard-FairSeg}}  & \multicolumn{6}{l}{\bf{HAM10000}} & \multicolumn{5}{l}{\bf{Radiotherapy Target Dataset}}  \\
\cmidrule(l){2-5} \cmidrule(l){6-11} \cmidrule(l){12-16}

 \multirow{2}{*}{Dataset} & \multirow{2}{*}{Total} & \multicolumn{3}{c}{Attribute (Race)} & \multirow{2}{*}{Total} & \multicolumn{5}{c}{Attribute (Age)} & \multirow{2}{*}{Total} & \multicolumn{4}{c}{Attribute (T-stage)} \\
 &  & Asian  & Black & White &  & { $\geq$ 80}  & {  $\geq$ 60 } & {  $\geq$ 40} &  {  $\geq$ 20}  & { $<$ 20} &    & T1 & T2 & T3 & T4 \\
\cmidrule(l){1-1}  \cmidrule(l){2-2} \cmidrule(l){3-5} \cmidrule(l){6-6} \cmidrule(l){7-11} \cmidrule(l){12-12} \cmidrule(l){13-16}
% \midrule

 \multirow{1}{*}{Trainset}  & 7945 & 750 & 1174 & 6021 & 8137 &  191&	1324&	3693 &	2356 &	573 & 721  & 26 & 227 & 425 &  43\\
 (\%) & (100) & (9) & (15) & (76) & (100)  & (2) & (16) & (45) & (31) & (7)  & (100) & (4) & (31) & (59) & (6) \\

\cmidrule(l){1-1}  \cmidrule(l){2-2} \cmidrule(l){3-5} \cmidrule(l){6-6} \cmidrule(l){7-11} \cmidrule(l){12-12} \cmidrule(l){13-16}

 \multirow{1}{*}{Testset}   & 2000 & 169 & 299 & 1532 & 1061 & 121 & 469 & 328 & 120 & 24 & 132 & 1 & 55 & 64 & 12  \\
 % \multirow{1}{*}{Testset}   & 2000 & 169 & 299 & 1532 & 1061 & 121 & 469 & 328 & 120 & 24 & 182 & 5 & 62 & 107 & 8 \\
% & (\%) & \\

% \cmidrule(l){1-1}  \cmidrule(l){2-2} \cmidrule(l){3-5} \cmidrule(l){6-6} \cmidrule(l){7-11} \cmidrule(l){12-12} \cmidrule(l){13-16}

 % \multirow{1}{*}{External Testset} &  - &  - &  - &  - &  - &  - &  - &  - &  - &  - & 132 & 1 & 55 & 64 & 12 \\
\bottomrule
\end{tabular}
}
\end{center}
\vskip -0.1in
\end{table*}



\subsection{Comparison on Computations.}
\label{appen_computation}
We compare the computational complexity of incorporating the MoE or dMoE module into the backbone in \cref{tab_compute}.

\begin{table}[!h]
\caption{Computational complexity comparison.}\label{tab_compute}
\centering
\vskip 0.15in
\resizebox{0.55\linewidth}{!}{
\begin{tabular}{lcccccc}
\toprule
& \bf{TransUNet} & {+MoE} & {+dMoE} & \bf{3D ResUNet} & {+MoE} & {+dMoE} \\    
\cmidrule(l){1-1} \cmidrule(l){2-4}  \cmidrule(l){5-7} 
Input & \multicolumn{3}{c}{224 W $\times$ 224 H} & \multicolumn{3}{c}{384 W $\times$ 384 H $\times$ 128 D} \\
\cmidrule(l){1-1} \cmidrule(l){2-4}  \cmidrule(l){5-7} 
GFlops & 45.84 & 90.28 & 90.28 & 1542.36 & 1761.30 & 1761.30\\
Params (M) & 91.67 & 129.46 & 129.51 & 13.28 & 26 & 26.05\\
\bottomrule 
\end{tabular}
}
\label{tile}
\end{table}


\subsection{Ablation Study Results.}
\label{appen_ablation}

We perform ablation studies to examine the effects of various configurations of the dMoE module. Specifically, we investigate performance changes based on \textbf{1) dMoE Location} within the network, by inserting the dMoE module at different layers within the encoder, the decoder, or both for comparison. We also examine whether \textbf{2) dMoE Parameters} should be shared or separated across layers to determine the optimal architecture. In the case of parameter sharing for CNNs, we incorporate additional linear layers before and after the dMoE module to match the channel dimensions across different layer blocks. For performing experiments, we use the HAM10000 dataset for Transformer-based architecture (TransUNet), while for CNN-based architecture (3D ResUNet), we utilize a radiotherapy target segmentation task, by reporting performance using the ES-Dice and Dice score as the metric.

As shown in \cref{tab_abl_2d} and \cref{tab_abl_3d}, inserting the dMoE module within the encoder, decoder, or both yields comparable performance for both architectures, particularly for minority groups (The subgroup with Age $<$ 20 for HAM10000 dataset and the T4-stage subgroup for radiotherapy target dataset). Therefore, we retain the dMoE module within the encoder only to optimize network training efficiency. In the Transformer-based architecture, sharing dMoE parameters across layers enhances performance due to consistent dimensionality between layers. Conversely, in CNN-based architecture, differences in layer-wise dimensionality reduce the effectiveness of parameter sharing. This necessitates adding dimension-matching linear layers before and after the dMoE modules; however, performance remains comparable. As a result, we adopt a layer-wise dMoE configuration specifically tailored for CNNs.

% \subsubsection{Ablation Studies on Transformer Architecture.}
% \label{appen_ablation_Trans}

% To analyse

\begin{table*}[h!]
\begin{center}
\caption{Ablation study on HAM10000 datsaset for Transformer-based architecture.}\label{tab_abl_2d}
\vskip 0.15in
\resizebox{0.9 \linewidth}{!}{
\begin{tabular}{lcccccccccc}
\toprule
\multirow{3}{*}{Method}  & & & \multicolumn{2}{c}{All} &  { Age $\geq$ 80}  & { Age $\geq$ 60 } & { Age $\geq$ 40} &  { Age $\geq$ 20}  & {Age $<$ 20}  \\
& & &  \multicolumn{2}{c}{ (n=1061)}  & \multicolumn{1}{c}{(n=121)} & 
 \multicolumn{1}{c}{(n=469)}  & \multicolumn{1}{c}{(n=328)} & \multicolumn{1}{c}{(n=120)} & \multicolumn{1}{c}{(n=24)} \\

  \cmidrule(l){4-5}  \cmidrule(lr){6-6}  \cmidrule(lr){7-7} \cmidrule(lr){8-8} \cmidrule(lr){9-9} \cmidrule(lr){10-10} 

 & & & ES-Dice  & Dice  &  Dice    & Dice    & Dice    & Dice    & Dice  \\
 
\midrule
 \multirow{4}{*}{1) dMoE Location} & Encoder & Decoder \\
 \cmidrule(l){2-3}  \cmidrule(l){4-5}  \cmidrule(lr){6-6}  \cmidrule(lr){7-7} \cmidrule(lr){8-8} \cmidrule(lr){9-9}  \cmidrule(lr){10-10}
& \checkmark (Ours) &  &  0.841 & 0.884 & 0.864 & 0.881 & 0.890 & 0.901  & 0.880 \\
&  &  \checkmark  &  0.840 & 0.881 & 0.871 & 0.872 & 0.892 & 0.899 & 0.879 \\
&  \checkmark  &  \checkmark &  0.842 & 0.885 & 0.873 & 0.877 & 0.893 & 0.905 & 0.882 \\


\midrule
 \multirow{3}{*}{2) dMoE Parameters} &  Layer-Wise & Shared \\
 \cmidrule(l){2-3}  \cmidrule(l){4-5}  \cmidrule(lr){6-6}  \cmidrule(lr){7-7} \cmidrule(lr){8-8} \cmidrule(lr){9-9}  \cmidrule(lr){10-10}
& \checkmark &  &  0.812 & 0.881 & 0.870 & 0.875 & 0.893 & 0.890 & 0.834     \\
&    &  \checkmark (Ours) & 0.841 & 0.884     & 0.864 & 0.881 & 0.890 & 0.901 & 0.880 \\
% &   &    &   \\

\bottomrule
\end{tabular}
}
\end{center}
\vskip -0.1in
\end{table*}




% \subsubsection{Ablation Studies on CNN Architecture.}
% \label{appen_ablation_CNN}


\begin{table*}[h!]
\begin{center}
\caption{Ablation study on radiotherapy target segmentation for CNN-based architecture.}\label{tab_abl_3d}
\vskip 0.15in
\resizebox{0.7\linewidth}{!}{
\begin{tabular}{lcccccccc}
\toprule
 \multirow{3}{*}{Method} & & & \multicolumn{2}{c}{All} & \multicolumn{1}{c}{T1} & \multicolumn{1}{c}{T2} & \multicolumn{1}{c}{T3 }  & \multicolumn{1}{c}{T4} \\
& & & \multicolumn{2}{c}{(n=132)} & \multicolumn{1}{c}{(n=1)} & \multicolumn{1}{c}{(n=55)} & \multicolumn{1}{c}{(n=64)}  & \multicolumn{1}{c}{(12)} \\
 \cmidrule(l){4-5}  \cmidrule(lr){6-6}  \cmidrule(lr){7-7} \cmidrule(lr){8-8} \cmidrule(lr){9-9} 
 & & & ES-Dice  & Dice  &  Dice    & Dice    & Dice    & Dice    \\
 
\midrule
 \multirow{4}{*}{1) dMoE Location} & Encoder & Decoder \\
 \cmidrule(l){2-3}  \cmidrule(l){4-5}  \cmidrule(lr){6-6}  \cmidrule(lr){7-7} \cmidrule(lr){8-8} \cmidrule(lr){9-9} 
& \checkmark (Ours)&  & {0.546} & {0.711} & {0.828} & {0.634} & {0.765}  & {0.767}    \\
&  &  \checkmark  &   {0.569} & {0.704} & {0.798}  & {0.644}  & {0.747} & {0.746}   \\
&  \checkmark  &  \checkmark  &  {0.543} & {0.701} & {0.840} & {0.635} & {0.747} & {0.741} \\



\midrule
 \multirow{3}{*}{2) dMoE Parameters} &  Layer-Wise & Shared \\
 \cmidrule(l){2-3}  \cmidrule(l){4-5}  \cmidrule(lr){6-6}  \cmidrule(lr){7-7} \cmidrule(lr){8-8} \cmidrule(lr){9-9} 
& \checkmark (Ours) &  & {0.546} & {0.711} & {0.828} & {0.634}& {0.765}  & {0.767}   \\
&    &  \checkmark  &  {0.546} & {0.710} & {0.806} &{0.626} &{0.769} & {0.769}  \\
% &   &    &   \\

\bottomrule
\end{tabular}
}
\end{center}
\vskip -0.1in
\end{table*}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
