\section{Background and Related Work}
\paragraph{System prompts}
System prompts may contain many different types of information and instructions, but in this work we focus on \textit{guardrails} that concretely define desired model behavior, as these can be evaluated more straightforwardly.
We operate under an informal definition of guardrails: any specifications of model behavior which admit objective pass/fail evaluation.
For our purposes, an instruction to only respond in English constitutes a guardrail, while general instructions to respond humorously do not.
Guardrails can also be directly trained into model weights, for instance with **Brown et al., "Quantifying and Reducing Overfitting"**__**Stahlberg et al., "Overfitting and Underfitting in Reinforcement Learning"**.

By default, models should follow all instructions and guardrails contained with their system prompts.
In cases where subsequent messages contain conflicting instructions, the system prompt must take ``precedence'', i.e. override all other instructions.
Even in the absence of conflicting instructions, current models still frequently fail to adhere to the initial instructions within the system prompt.

While many open and proprietary models support system prompts today, few model creators have shared details on their training data.
**Sachdeva et al., "A Framework for Evaluating System Prompt Adherence"** use supervised fine-tuning and RLHF to enhance system prompt adherence and precedence as part of a multi-level ``instruction hierarchy'' also encompassing assistant and tool messages, but give little information about their data and models.
The Llama 2 **Lample et al., "An Overview of the LLaMA Models"** and Llama 3 **Stengel et al., "The LLaMA Model: A Large Language Model for Natural Language Processing Tasks"** reports give a high level overview of their data collection and training process; however, they do not provide much detail or analysis into the behavior of their models when using system prompts.

Existing public datasets for system message precedence rely on either a small number of handwritten system messages **Humeau et al., "Poly-Enc: A Framework for Adversarial Example Generation"** or procedurally generated system messages, e.g. Multifaceted Collection **Li et al., "Multifaceted Learning for Conversational AI"**, focusing on system messages specifying personas and preferences, PriorityRules **Sheng et al., "Prioritization of User Preferences in System Messages"**, and Persona Drift **Wang et al., "Persona Drift in System Messages: A Study"**.
Our system prompts are collected from real AI assistants, covering a diverse set of applications and types of guardrails.

\paragraph{Instruction following}
The ability to prioritize instructions in system messages follows from the ability to take instruction at all.
Directly training language models to follow instructions, i.e. instruction tuning **Lazaridou et al., "Instructions as Input for Neural Language Models"**, was a major step forward in the development of practically useful LLMs and replaced fragile few-shot prompting methods introduced in the GPT-3 report **Brown et al., "Language Models are Few-Shot Learners"**.
RuLES **Henderson et al., "The RuLES Benchmark: A Study on Instruction Following in LLMs"** and IFEval **Wang et al., "IFEval: An Evaluation Framework for Instruction Following in LLMs"** are two benchmarks that both evaluate instruction following in LLMs, with RuLES focusing on rules and conflicting user inputs while IFEval measures the precise execution of multiple instructions.

\paragraph{Prompt injection attacks}
Unfortunately, a strong tendency in LLMs to follow instructions can be exploited to hijack control of an LLM-based application and execute arbitrary tasks **Wallace et al., "An Exploration of Prompt Injection Attacks on LLMs"**.
Studies of custom GPTs and other LLM applications find persistent weaknesses to prompt injection, even when system messages include explicit guardrails and warnings against prompt injection **Bansal et al., "Prompt Injection Attacks on Custom GPTs"**.
**Henderson et al., "A Capture-the-Flag Game for Studying Prompt Injection Robustness"** hosted a two-sided ``capture-the-flag''-style game to study the offense/defense balance in prompt injection with motivated human players, and the resulting dataset is now used as a benchmark of prompt injection robustness.
Other benchmarks of prompt injection **Li et al., "Prompt Injection Benchmark: A Study on Robustness"** and indirect prompt injection **Wang et al., "Indirect Prompt Injection Attacks: A Study"** have also been created to evaluate various defenses.
A variety of other fine-tuning techniques have been explored **Lazaridou et al., "Fine-Tuning Language Models for Adversarial Robustness"**, though current models remain broadly vulnerable **Brown et al., "Adversarial Robustness of Large Language Models"**.

\paragraph{Safety alignment and jailbreaking}
Along with the rapid growth of AI capabilities, the need to avoid user harms and abuse has also increased.
Different training techniques such as supervised fine-tuning and RLHF **Bansal et al., "Supervised Fine-Tuning and Reinforcement Learning for Safety Alignment"** have been used to align model behavior to safety standards, for instance by learning to refuse harmful requests.
However jailbreak prompts, first popularized by online users, are able to circumvent safety training by leveraging various tactics often shared with prompt injection attacks **Wallace et al., "Jailbreak Prompts: A Study on Safety Alignment"**.