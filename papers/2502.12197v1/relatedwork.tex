\section{Background and Related Work}
\paragraph{System prompts}
System prompts may contain many different types of information and instructions, but in this work we focus on \textit{guardrails} that concretely define desired model behavior, as these can be evaluated more straightforwardly.
We operate under an informal definition of guardrails: any specifications of model behavior which admit objective pass/fail evaluation.
For our purposes, an instruction to only respond in English constitutes a guardrail, while general instructions to respond humorously do not.
Guardrails can also be directly trained into model weights, for instance with RLHF~\citep{Bai2022-bz}.

By default, models should follow all instructions and guardrails contained with their system prompts.
In cases where subsequent messages contain conflicting instructions, the system prompt must take ``precedence'', i.e. override all other instructions.
Even in the absence of conflicting instructions, current models still frequently fail to adhere to the initial instructions within the system prompt.

While many open and proprietary models support system prompts today, few model creators have shared details on their training data.
\citet{wallaceInstructionHierarchyTraining2024} use supervised fine-tuning and RLHF to enhance system prompt adherence and precedence as part of a multi-level ``instruction hierarchy'' also encompassing assistant and tool messages, but give little information about their data and models.
The Llama 2~\citep{touvronLlamaOpenFoundation2023a} and Llama 3~\citep{dubey2024llama} reports give a high level overview of their data collection and training process; however, they do not provide much detail or analysis into the behavior of their models when using system prompts.

Existing public datasets for system message precedence rely on either a small number of handwritten system messages~\citep{mukherjee2023orca} or procedurally generated system messages, e.g. Multifaceted Collection~\citep{leeAligningThousandsPreferences2024} focusing on system messages specifying personas and preferences, PriorityRules~\citep{luSoFAShieldedFly2024}, and Persona Drift~\citep{liMeasuringControllingInstruction2024}.
Our system prompts are collected from real AI assistants, covering a diverse set of applications and types of guardrails.

\paragraph{Instruction following}
The ability to prioritize instructions in system messages follows from the ability to take instruction at all.
Directly training language models to follow instructions, i.e. instruction tuning~\citep{wei2021finetuned, khashabi2020unifiedqa, weller2020learning, mishra2021cross, sanh2021multitask, Ouyang2022-wv}, was a major step forward in the development of practically useful LLMs and replaced fragile few-shot prompting methods introduced in the GPT-3 report~\citep{brown2020language}.
\textsc{RuLES}~\citep{muCanLLMsFollow2024} and IFEval~\citep{zhou2023instruction} are two benchmarks that both evaluate instruction following in LLMs, with \textsc{RuLES} focusing on rules and conflicting user inputs while IFEval measures the precise execution of multiple instructions.

\paragraph{Prompt injection attacks}
Unfortunately, a strong tendency in LLMs to follow instructions can be exploited to hijack control of an LLM-based application and execute arbitrary tasks~\citep{Branch2022-ck, perezIgnorePreviousPrompt2022, greshakeNotWhatYouve2023}.
Studies of custom GPTs and other LLM applications find persistent weaknesses to prompt injection, even when system messages include explicit guardrails and warnings against prompt injection~\citep{yuAssessingPromptInjection2024, liuPromptInjectionAttack2024}.
\citet{toyer2023tensor} hosted a two-sided ``capture-the-flag''-style game to study the offense/defense balance in prompt injection with motivated human players, and the resulting dataset is now used as a benchmark of prompt injection robustness.
Other benchmarks of prompt injection~\citep{schulhoffIgnoreThisTitle2024, li2024evaluating} and indirect prompt injection~\citep{yi2023benchmarking} have also been created to evaluate various defenses.
A variety of other fine-tuning techniques have been explored~\citep{chenStruQDefendingPrompt2024, piet2024jatmo, yi2023benchmarking, wallaceInstructionHierarchyTraining2024, wu2024instructionalsegmentembeddingimproving}, though current models remain broadly vulnerable~\citep{rehbergerBreakingInstructionHierarchy2024}.

\paragraph{Safety alignment and jailbreaking}
Along with the rapid growth of AI capabilities, the need to avoid user harms and abuse has also increased.
Different training techniques such as supervised fine-tuning and RLHF~\citep{Bai2022-bz, Bai2022-zx, Ouyang2022-wv, glaese2022improving, achiam2023gpt} have been used to align model behavior to safety standards, for instance by learning to refuse harmful requests.
However jailbreak prompts, first popularized by online users, are able to circumvent safety training by leveraging various tactics often shared with prompt injection attacks~\citep{Kang2023-qa, Zou2023-lc, Wei2023-be, mazeika2024harmbench}.