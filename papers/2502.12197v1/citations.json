[
  {
    "index": 0,
    "papers": [
      {
        "key": "Bai2022-bz",
        "author": "Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell,\nAmanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and\nFort, Stanislav and Ganguli, Deep and Henighan, Tom and\nJoseph, Nicholas and Kadavath, Saurav and Kernion, Jackson\nand Conerly, Tom and El-Showk, Sheer and Elhage, Nelson and\nHatfield-Dodds, Zac and Hernandez, Danny and Hume, Tristan\nand Johnston, Scott and Kravec, Shauna and Lovitt, Liane and\nNanda, Neel and Olsson, Catherine and Amodei, Dario and\nBrown, Tom and Clark, Jack and McCandlish, Sam and Olah,\nChris and Mann, Ben and Kaplan, Jared",
        "title": "{Training a Helpful and Harmless Assistant with Reinforcement\nLearning from Human Feedback}"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "wallaceInstructionHierarchyTraining2024",
        "author": "Wallace, Eric and Xiao, Kai and Leike, Reimar and Weng, Lilian and Heidecke, Johannes and Beutel, Alex",
        "title": "The {{Instruction Hierarchy}}: {{Training LLMs}} to {{Prioritize Privileged Instructions}}"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "touvronLlamaOpenFoundation2023a",
        "author": "Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and Ferrer, Cristian Canton and Chen, Moya and Cucurull, Guillem and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu, Wenyin and Fuller, Brian and Gao, Cynthia and Goswami, Vedanuj and Goyal, Naman and Hartshorn, Anthony and Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas, Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann, Isabel and Korenev, Artem and Koura, Punit Singh and Lachaux, Marie-Anne and Lavril, Thibaut and Lee, Jenya and Liskovich, Diana and Lu, Yinghai and Mao, Yuning and Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and Molybog, Igor and Nie, Yixin and Poulton, Andrew and Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and Schelten, Alan and Silva, Ruan and Smith, Eric Michael and Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang, Binh and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang, Yuchen and Fan, Angela and Kambadur, Melanie and Narang, Sharan and Rodriguez, Aurelien and Stojnic, Robert and Edunov, Sergey and Scialom, Thomas",
        "title": "Llama 2: {{Open Foundation}} and {{Fine-Tuned Chat Models}}"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "dubey2024llama",
        "author": "Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others",
        "title": "The llama 3 herd of models"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "mukherjee2023orca",
        "author": "Mukherjee, Subhabrata and Mitra, Arindam and Jawahar, Ganesh and Agarwal, Sahaj and Palangi, Hamid and Awadallah, Ahmed",
        "title": "Orca: Progressive learning from complex explanation traces of gpt-4"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "leeAligningThousandsPreferences2024",
        "author": "Lee, Seongyun and Park, Sue Hyun and Kim, Seungone and Seo, Minjoon",
        "title": "Aligning to {{Thousands}} of {{Preferences}} via {{System Message Generalization}}"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "luSoFAShieldedFly2024",
        "author": "Lu, Xinyu and Yu, Bowen and Lu, Yaojie and Lin, Hongyu and Yu, Haiyang and Sun, Le and Han, Xianpei and Li, Yongbin",
        "title": "{{SoFA}}: {{Shielded On-the-fly Alignment}} via {{Priority Rule Following}}"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "liMeasuringControllingInstruction2024",
        "author": "Li, Kenneth and Liu, Tianle and Bashkansky, Naomi and Bau, David and Vi{\\'e}gas, Fernanda and Pfister, Hanspeter and Wattenberg, Martin",
        "title": "Measuring and {{Controlling Instruction}} ({{In}}){{Stability}} in {{Language Model Dialogs}}"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "wei2021finetuned",
        "author": "Wei, Jason and Bosma, Maarten and Zhao, Vincent Y and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V",
        "title": "Finetuned language models are zero-shot learners"
      },
      {
        "key": "khashabi2020unifiedqa",
        "author": "Khashabi, Daniel and Min, Sewon and Khot, Tushar and Sabharwal, Ashish and Tafjord, Oyvind and Clark, Peter and Hajishirzi, Hannaneh",
        "title": "Unifiedqa: Crossing format boundaries with a single qa system"
      },
      {
        "key": "weller2020learning",
        "author": "Weller, Orion and Lourie, Nicholas and Gardner, Matt and Peters, Matthew E",
        "title": "Learning from task descriptions"
      },
      {
        "key": "mishra2021cross",
        "author": "Mishra, Swaroop and Khashabi, Daniel and Baral, Chitta and Hajishirzi, Hannaneh",
        "title": "Cross-task generalization via natural language crowdsourcing instructions"
      },
      {
        "key": "sanh2021multitask",
        "author": "Sanh, Victor and Webson, Albert and Raffel, Colin and Bach, Stephen H and Sutawika, Lintang and Alyafeai, Zaid and Chaffin, Antoine and Stiegler, Arnaud and Scao, Teven Le and Raja, Arun and others",
        "title": "Multitask prompted training enables zero-shot task generalization"
      },
      {
        "key": "Ouyang2022-wv",
        "author": "Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo\nand Wainwright, Carroll L and Mishkin, Pamela and Zhang,\nChong and Agarwal, Sandhini and Slama, Katarina and Ray,\nAlex and Schulman, John and Hilton, Jacob and Kelton, Fraser\nand Miller, Luke and Simens, Maddie and Askell, Amanda and\nWelinder, Peter and Christiano, Paul and Leike, Jan and\nLowe, Ryan",
        "title": "{Training language models to follow instructions with human\nfeedback}"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "brown2020language",
        "author": "Brown, Tom B",
        "title": "Language models are few-shot learners"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "muCanLLMsFollow2024",
        "author": "Mu, Norman and Chen, Sarah and Wang, Zifan and Chen, Sizhe and Karamardian, David and Aljeraisy, Lulwa and Alomair, Basel and Hendrycks, Dan and Wagner, David",
        "title": "Can {{LLMs Follow Simple Rules}}?"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "zhou2023instruction",
        "author": "Zhou, Jeffrey and Lu, Tianjian and Mishra, Swaroop and Brahma, Siddhartha and Basu, Sujoy and Luan, Yi and Zhou, Denny and Hou, Le",
        "title": "Instruction-following evaluation for large language models"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "Branch2022-ck",
        "author": "Branch, Hezekiah J and Cefalu, Jonathan Rodriguez and\nMcHugh, Jeremy and Hujer, Leyla and Bahl, Aditya and del\nCastillo Iglesias, Daniel and Heichman, Ron and Darwishi,\nRamesh",
        "title": "{Evaluating the Susceptibility of Pre-Trained Language Models\nvia Handcrafted Adversarial Examples}"
      },
      {
        "key": "perezIgnorePreviousPrompt2022",
        "author": "Perez, F{\\'a}bio and Ribeiro, Ian",
        "title": "Ignore {{Previous Prompt}}: {{Attack Techniques For Language Models}}"
      },
      {
        "key": "greshakeNotWhatYouve2023",
        "author": "Greshake, Kai and Abdelnabi, Sahar and Mishra, Shailesh and Endres, Christoph and Holz, Thorsten and Fritz, Mario",
        "title": "Not What You've Signed up for: {{Compromising Real-World LLM-Integrated Applications}} with {{Indirect Prompt Injection}}"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "yuAssessingPromptInjection2024",
        "author": "Yu, Jiahao and Wu, Yuhang and Shu, Dong and Jin, Mingyu and Yang, Sabrina and Xing, Xinyu",
        "title": "Assessing {{Prompt Injection Risks}} in 200+ {{Custom GPTs}}"
      },
      {
        "key": "liuPromptInjectionAttack2024",
        "author": "Liu, Yi and Deng, Gelei and Li, Yuekang and Wang, Kailong and Wang, Zihao and Wang, Xiaofeng and Zhang, Tianwei and Liu, Yepang and Wang, Haoyu and Zheng, Yan and Liu, Yang",
        "title": "Prompt {{Injection}} Attack against {{LLM-integrated Applications}}"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "toyer2023tensor",
        "author": "Toyer, Sam and Watkins, Olivia and Mendes, Ethan Adrian and Svegliato, Justin and Bailey, Luke and Wang, Tiffany and Ong, Isaac and Elmaaroufi, Karim and Abbeel, Pieter and Darrell, Trevor and Ritter, Alan and Russell, Stuart",
        "title": "{Tensor Trust}: Interpretable Prompt Injection Attacks from an Online Game"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "schulhoffIgnoreThisTitle2024",
        "author": "Schulhoff, Sander and Pinto, Jeremy and Khan, Anaum and Bouchard, Louis-Fran\u00e7ois and Si, Chenglei and Anati, Svetlina and Tagliabue, Valen and Kost, Anson Liu and Carnahan, Christopher and Boyd-Graber, Jordan",
        "title": "Ignore {{This Title}} and {{HackAPrompt}}: {{Exposing Systemic Vulnerabilities}} of {{LLMs}} through a {{Global Scale Prompt Hacking Competition}}"
      },
      {
        "key": "li2024evaluating",
        "author": "Li, Zekun and Peng, Baolin and He, Pengcheng and Yan, Xifeng",
        "title": "Evaluating the instruction-following robustness of large language models to prompt injection"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "yi2023benchmarking",
        "author": "Yi, Jingwei and Xie, Yueqi and Zhu, Bin and Kiciman, Emre and Sun, Guangzhong and Xie, Xing and Wu, Fangzhao",
        "title": "Benchmarking and defending against indirect prompt injection attacks on large language models"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "chenStruQDefendingPrompt2024",
        "author": "Chen, Sizhe and Piet, Julien and Sitawarin, Chawin and Wagner, David",
        "title": "{{StruQ}}: {{Defending Against Prompt Injection}} with {{Structured Queries}}"
      },
      {
        "key": "piet2024jatmo",
        "author": "Piet, Julien and Alrashed, Maha and Sitawarin, Chawin and Chen, Sizhe and Wei, Zeming and Sun, Elizabeth and Alomair, Basel and Wagner, David",
        "title": "Jatmo: Prompt injection defense by task-specific finetuning"
      },
      {
        "key": "yi2023benchmarking",
        "author": "Yi, Jingwei and Xie, Yueqi and Zhu, Bin and Kiciman, Emre and Sun, Guangzhong and Xie, Xing and Wu, Fangzhao",
        "title": "Benchmarking and defending against indirect prompt injection attacks on large language models"
      },
      {
        "key": "wallaceInstructionHierarchyTraining2024",
        "author": "Wallace, Eric and Xiao, Kai and Leike, Reimar and Weng, Lilian and Heidecke, Johannes and Beutel, Alex",
        "title": "The {{Instruction Hierarchy}}: {{Training LLMs}} to {{Prioritize Privileged Instructions}}"
      },
      {
        "key": "wu2024instructionalsegmentembeddingimproving",
        "author": "Tong Wu and Shujian Zhang and Kaiqiang Song and Silei Xu and Sanqiang Zhao and Ravi Agrawal and Sathish Reddy Indurthi and Chong Xiang and Prateek Mittal and Wenxuan Zhou",
        "title": "Instructional Segment Embedding: Improving LLM Safety with Instruction Hierarchy"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "rehbergerBreakingInstructionHierarchy2024",
        "author": "Rehberger, Johann",
        "title": "Breaking {{Instruction Hierarchy}} in {{OpenAI}}'s Gpt-4o-Mini {$\\cdot$} {{Embrace The Red}}"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "Bai2022-bz",
        "author": "Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell,\nAmanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and\nFort, Stanislav and Ganguli, Deep and Henighan, Tom and\nJoseph, Nicholas and Kadavath, Saurav and Kernion, Jackson\nand Conerly, Tom and El-Showk, Sheer and Elhage, Nelson and\nHatfield-Dodds, Zac and Hernandez, Danny and Hume, Tristan\nand Johnston, Scott and Kravec, Shauna and Lovitt, Liane and\nNanda, Neel and Olsson, Catherine and Amodei, Dario and\nBrown, Tom and Clark, Jack and McCandlish, Sam and Olah,\nChris and Mann, Ben and Kaplan, Jared",
        "title": "{Training a Helpful and Harmless Assistant with Reinforcement\nLearning from Human Feedback}"
      },
      {
        "key": "Bai2022-zx",
        "author": "Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and\nAskell, Amanda and Kernion, Jackson and Jones, Andy and\nChen, Anna and Goldie, Anna and Mirhoseini, Azalia and\nMcKinnon, Cameron and Chen, Carol and Olsson, Catherine and\nOlah, Christopher and Hernandez, Danny and Drain, Dawn and\nGanguli, Deep and Li, Dustin and Tran-Johnson, Eli and\nPerez, Ethan and Kerr, Jamie and Mueller, Jared and Ladish,\nJeffrey and Landau, Joshua and Ndousse, Kamal and Lukosuite,\nKamile and Lovitt, Liane and Sellitto, Michael and Elhage,\nNelson and Schiefer, Nicholas and Mercado, Noemi and\nDasSarma, Nova and Lasenby, Robert and Larson, Robin and\nRinger, Sam and Johnston, Scott and Kravec, Shauna and El\nShowk, Sheer and Fort, Stanislav and Lanham, Tamera and\nTelleen-Lawton, Timothy and Conerly, Tom and Henighan, Tom\nand Hume, Tristan and Bowman, Samuel R and Hatfield-Dodds,\nZac and Mann, Ben and Amodei, Dario and Joseph, Nicholas and\nMcCandlish, Sam and Brown, Tom and Kaplan, Jared",
        "title": "{Constitutional AI: Harmlessness from AI Feedback}"
      },
      {
        "key": "Ouyang2022-wv",
        "author": "Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo\nand Wainwright, Carroll L and Mishkin, Pamela and Zhang,\nChong and Agarwal, Sandhini and Slama, Katarina and Ray,\nAlex and Schulman, John and Hilton, Jacob and Kelton, Fraser\nand Miller, Luke and Simens, Maddie and Askell, Amanda and\nWelinder, Peter and Christiano, Paul and Leike, Jan and\nLowe, Ryan",
        "title": "{Training language models to follow instructions with human\nfeedback}"
      },
      {
        "key": "glaese2022improving",
        "author": "Glaese, Amelia and McAleese, Nat and Trebacz, Maja and Aslanides, John and Firoiu, Vlad and Ewalds, Timo and Rauh, Maribeth and Weidinger, Laura and Chadwick, Martin and Thacker, Phoebe and others",
        "title": "Improving alignment of dialogue agents via targeted human judgements"
      },
      {
        "key": "achiam2023gpt",
        "author": "Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others",
        "title": "Gpt-4 technical report"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "Kang2023-qa",
        "author": "Kang, Daniel and Li, Xuechen and Stoica, Ion and Guestrin,\nCarlos and Zaharia, Matei and Hashimoto, Tatsunori",
        "title": "{Exploiting Programmatic Behavior of LLMs: Dual-Use Through\nStandard Security Attacks}"
      },
      {
        "key": "Zou2023-lc",
        "author": "Zou, Andy and Wang, Zifan and Zico Kolter, J and Fredrikson,\nMatt",
        "title": "{Universal and Transferable Adversarial Attacks on Aligned\nLanguage Models}"
      },
      {
        "key": "Wei2023-be",
        "author": "Wei, Alexander and Haghtalab, Nika and Steinhardt, Jacob",
        "title": "{Jailbroken: How Does LLM Safety Training Fail?}"
      },
      {
        "key": "mazeika2024harmbench",
        "author": "Mazeika, Mantas and Phan, Long and Yin, Xuwang and Zou, Andy and Wang, Zifan and Mu, Norman and Sakhaee, Elham and Li, Nathaniel and Basart, Steven and Li, Bo and others",
        "title": "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal"
      }
    ]
  }
]