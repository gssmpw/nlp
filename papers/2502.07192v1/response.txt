\section{Related Work}
\subsection{Polychronous Oscillatory Networks}
The Ising model**Braunstein, "An Introduction to Models of Phase Transitions"** was discovered in the context of ferromagnetism in statistical mechanics. The Ising Machine leverages the natural tendency of such systems to minimize their energy, enabling it to efficiently find optimal solutions**Shor, "Approximation Algorithms for NP-Hard Problems"**. It has been widely applied to various combinatorial optimization problems. In an Ising Machine, each node can only exist in one of two possible states.
In this paper, we build upon the principles of CMOS oscillator networks and the Ising Machine to design a system capable of converging to the minimum of the Potts Hamiltonian, where each node can have multiple states. Polychronous Oscillator Networks on CMOS are inspired by spiking neurons, mimicking their interactions, and have been successfully applied to NP-hard problems such as Graph Coloring, as well as in designing quantum computers.
Inspired by neural computing**Hopfield, "Neural Networks and Physical Systems with Emergent Collective Computational Abilities"** and their implementation on hardware**Mead, "Analog VLSI and Neural Systems"**, we design a MIMO OscNet structure, where some oscillators serve as inputs and others as outputs. The hardware can autonomously perform inference, effectively enabling forward propagation. The primary goal of this paper is to design an appropriate learning strategy to complement this hardware, enabling its application in machine learning tasks.

\subsection{Hebbian Learning}
Biological systems learn through signal forward propagation and Hebbian learning**Hebb, "The Organization of Behavior"**. In essence, Hebbian theory states that an increase in synaptic efficacy arises from the repeated and persistent stimulation of a postsynaptic cell by a presynaptic cell**Stent, "A Physiological Approach to the Action of Drugs on the Nervous System at the Cellular Level"**.
Before birth, humans undergo initial learning of the visual system spontaneously. Retinal waves**Meister, "Spatiotemporal Patterns in the Thalamocortical Input to Cat Area 17"** are generated on the retina as input signals, and the connection weights between retinal cells and the LGN are updated based on Hebbian theory**Shatz, "Visual Experience and Cortical Development: A Critical Period for Functional Plasticity"**. 
Similar learning mechanisms, rooted in Hebbian learning**Hebb, "The Organization of Behavior"**, are also applied in principal component analysis (PCA)**Pearson, "On Lines and Planes of Closest Fit to Systems of Points in Space"**, sparse coding**Olshausen, "Emergence of Simple-Cell Receptive Field Properties by Learning a Sparse Code"**, reinforcement learning**Sutton, "Learning to Predict by the Method of Temporal Differences"** and unsupervised learning in neural networks**Hinton, "A Learning Algorithm for Continually Running Fully Recurrent Neural Networks"**. Using the Winner-Takes-All (WTA) strategy, network weights are updated efficiently.
In this paper, we model the early visual system development using Hebbian theory and OscNet to address the question: How do humans perceive a straight line as being straight in Sec.~\ref{sec:human_visual_sys}. We design a pipeline where forward propagation and Hebbian weight updating serve as the foundation for OscNet's unsupervised learning. This pipeline demonstrates its applicability not only in unsupervised learning tasks but also in general supervised machine learning tasks.