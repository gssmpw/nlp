\section{Related Work}
\subsection{Polychronous Oscillatory Networks}
The Ising model~\cite{ising_1,ising_2,ising_3,ising_4} was discovered in the context of ferromagnetism in statistical mechanics. The Ising Machine leverages the natural tendency of such systems to minimize their energy, enabling it to efficiently find optimal solutions~\cite{ising_solve_1,ising_solve_2,ising_solve_3,ising_solve_4}. It has been widely applied to various combinatorial optimization problems. In an Ising Machine, each node can only exist in one of two possible states.
In this paper, we build upon the principles of CMOS oscillator networks and the Ising Machine to design a system capable of converging to the minimum of the Potts Hamiltonian, where each node can have multiple states. Polychronous Oscillator Networks on CMOS are inspired by spiking neurons, mimicking their interactions, and have been successfully applied to NP-hard problems such as Graph Coloring, as well as in designing quantum computers.
Inspired by neural computing~\cite{neural_comp_1,neural_comp_2,neural_comp_3,neural_comp_4} and their implementation on hardware~\cite{circuit_learn_1,circuit_learn_2,circuit_learn_3}, we design a MIMO OscNet structure, where some oscillators serve as inputs and others as outputs. The hardware can autonomously perform inference, effectively enabling forward propagation. The primary goal of this paper is to design an appropriate learning strategy to complement this hardware, enabling its application in machine learning tasks.

\subsection{Hebbian Learning}
Biological systems learn through signal forward propagation and Hebbian learning~\cite{hebbian_bio_1,hebbian_bio_2}. In essence, Hebbian theory states that an increase in synaptic efficacy arises from the repeated and persistent stimulation of a postsynaptic cell by a presynaptic cell~\cite{hebbian_synaptic_1,hebbian_synaptic_2}.
Before birth, humans undergo initial learning of the visual system spontaneously. Retinal waves~\cite{retinal_wave_1,retinal_wave_2,retinal_wave_3,retinal_wave_4,retinal_wave_5,retinal_wave_6,retinal_wave_7} are generated on the retina as input signals, and the connection weights between retinal cells and the LGN are updated based on Hebbian theory~\cite{retina_grow_nips}. 
Similar learning mechanisms, rooted in Hebbian learning~\cite{hebbian_math_1}, are also applied in principal component analysis (PCA)~\cite{hebbian_pca}, sparse coding~\cite{hebbian_sparse_coding}, reinforcement learning~\cite{hebbian_control_1, hebbian_control_2} and unsupervised learning in neural networks~\cite{hebbian_unsupervised_1,hebbian_unsupervised_2,hebbian_unsupervised_3,hebbian_unsupervised_4, hebbian_unsupervised_5,hebbian_unsupervised_6}. Using the Winner-Takes-All (WTA) strategy, network weights are updated efficiently.
In this paper, we model the early visual system development using Hebbian theory and OscNet to address the question: How do humans perceive a straight line as being straight in Sec.~\ref{sec:human_visual_sys}. We design a pipeline where forward propagation and Hebbian weight updating serve as the foundation for OscNet's unsupervised learning. This pipeline demonstrates its applicability not only in unsupervised learning tasks but also in general supervised machine learning tasks.