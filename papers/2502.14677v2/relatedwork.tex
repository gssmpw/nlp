\section{Related Research}
There have been several approaches to generating synthetic clinical data for a number of languages and for different purposes. Broadly speaking, most prior works have either focused on maximizing the utility of the synthetic data, or on studying the privacy characteristics of synthetic corpora.

While most papers studying data synthesis contain some form of privacy analysis, other papers have this as their main focus. Several papers study how differentially private learning impacts the utility \cite{Yue-etal-2023,igamberdiev_dp-nmt_2024} and the privacy of the data \cite{Miranda-etal-2024}. While privacy is an important justification for synthesizing data, it is not the main focus of our paper.

The second main current in the literature explores how to optimize synthesis to create the best possible synthetic corpora. These papers synthesize data using locally domain-adapted LLMs \cite{ive2020generation,hiebel2023can}, or using instruction-tuned models \cite{kiefer2024instruction,Liu-etal-2025}. They show that synthesis of high-utility data is possible. However, fewer papers systematically examine the conditions required for success.

In our literature review, two studies stand out as particularly relevant to this study. \citet{libbi2021generating} synthesize a Dutch corpus for PII detection using a GPT-2 model \cite{radford_language_2019} domain-adapted using 1 million documents and add rule-based machine annotations. Our study follows the same overall process for synthesis, but uses much less data and more modern NLP techniques. \citet{Xu-etal-2023a} similarly create synthetic corpora and experiment with constraining the total amount of data used, but do so for the relation extraction task. In this paper, we focus on a different task -- NER for PII detection. Furthermore, in contrast to both studies, we systematically evaluate the impact of constraining data alternately for \textit{both} domain adaptation \textit{and} machine annotation, try two different model sizes, synthesize corpora of different sizes, and validate our results across two languages.