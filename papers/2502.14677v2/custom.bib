% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").


@inproceedings{Lin-2004,
	address = {Barcelona, Spain},
	title = {{ROUGE}: {A} {Package} for {Automatic} {Evaluation} of {Summaries}},
	shorttitle = {{ROUGE}},
	url = {https://aclanthology.org/W04-1013/},
	urldate = {2025-02-11},
	booktitle = {Text {Summarization} {Branches} {Out}},
	publisher = {Association for Computational Linguistics},
	author = {Lin, Chin-Yew},
	month = jul,
	year = {2004},
	pages = {74--81},
}


@inproceedings{Bird-Loper-2004,
	address = {Barcelona, Spain},
	title = {{NLTK}: {The} {Natural} {Language} {Toolkit}},
	shorttitle = {{NLTK}},
	url = {https://aclanthology.org/P04-3031},
	urldate = {2024-04-05},
	booktitle = {Proceedings of the {ACL} {Interactive} {Poster} and {Demonstration} {Sessions}},
	publisher = {Association for Computational Linguistics},
	author = {Bird, Steven and Loper, Edward},
	month = jul,
	year = {2004},
	pages = {214--217}
}


@article{Liu-etal-2025,
	title = {Generating synthetic clinical text with local large language models to identify misdiagnosed limb fractures in radiology reports},
	volume = {159},
	issn = {0933-3657},
	url = {https://www.sciencedirect.com/science/article/pii/S0933365724002690},
	doi = {10.1016/j.artmed.2024.103027},
	abstract = {Large language models (LLMs) demonstrate impressive capabilities in generating human-like content and have much potential to improve the performance and efficiency of healthcare. An important application of LLMs is to generate synthetic clinical reports that could alleviate the burden of annotating and collecting real-world data in training AI models. Meanwhile, there could be concerns and limitations in using commercial LLMs to handle sensitive clinical data. In this study, we examined the use of open-source LLMs as an alternative to generate synthetic radiology reports to supplement real-world annotated data. We found LLMs hosted locally can achieve similar performance compared to ChatGPT and GPT-4 in augmenting training data for the downstream report classification task of identifying misdiagnosed fractures. We also examined the predictive value of using synthetic reports alone for training downstream models, where our best setting achieved more than 90 \% of the performance using real-world data. Overall, our findings show that open-source, local LLMs can be a favourable option for creating synthetic clinical reports for downstream tasks.},
	urldate = {2025-02-14},
	journal = {Artificial Intelligence in Medicine},
	author = {Liu, Jinghui and Koopman, Bevan and Brown, Nathan J. and Chu, Kevin and Nguyen, Anthony},
	month = jan,
	year = {2025},
	keywords = {Emergency department, Large language models, Local LLMs, Natural language processing, Radiology report, Synthetic data},
	pages = {103027},
}


@inproceedings{Miranda-etal-2024,
	title = {Evaluating {Privacy} {Risks} in {Synthetic} {Clinical} {Text} {Generation} in {Spanish}},
	url = {https://openreview.net/forum?id=0hxBH0dEKu},
	abstract = {Leveraging medical data for Deep Learning models holds great potential, but ensuring the protection of sensitive patient information is paramount in the clinical domain. A widely used approach to balance data utility and privacy is the generation of synthetic text with Large Language Models (LLMs) under the framework of differential privacy (DP). Techniques like Differentially Private Stochastic Gradient Descent (DP-SGD) are typically considered to provide privacy guarantees, but they rely on specific conditions. This research demonstrates how memorization in LLMs can deteriorate when these privacy safeguards are not fully met, increasing the risk of personal and sensitive information being leaked in synthetic clinical reports. Addressing these vulnerabilities could enhance the reliability of DP in protecting clinical text data while maintaining its utility.},
	language = {en},
	urldate = {2025-02-14},
	author = {Miranda, Luis and Dunstan, Jocelyn and Toro, Matías and Olmedo, Federico and Melo, Felix},
    booktitle={Latinx in AI @ NeurIPS 2024},
	month = oct,
	year = {2024},
}

@inproceedings{Yue-etal-2023,
	address = {Toronto, Canada},
	title = {Synthetic {Text} {Generation} with {Differential} {Privacy}: {A} {Simple} and {Practical} {Recipe}},
	shorttitle = {Synthetic {Text} {Generation} with {Differential} {Privacy}},
	url = {https://aclanthology.org/2023.acl-long.74},
	abstract = {Privacy concerns have attracted increasing attention in data-driven products due to the tendency of machine learning models to memorize sensitive training data. Generating synthetic versions of such data with a formal privacy guarantee, such as differential privacy (DP), provides a promising path to mitigating these privacy concerns, but previous approaches in this direction have typically failed to produce synthetic data of high quality. In this work, we show that a simple and practical recipe in the text domain is effective: simply fine-tuning a pretrained generative language model with DP enables the model to generate useful synthetic text with strong privacy protection. Through extensive empirical analyses on both benchmark and private customer data, we demonstrate that our method produces synthetic text that is competitive in terms of utility with its non-private counterpart, meanwhile providing strong protection against potential privacy leakages.},
	urldate = {2023-07-18},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Yue, Xiang and Inan, Huseyin and Li, Xuechen and Kumar, Girish and McAnallen, Julia and Shajari, Hoda and Sun, Huan and Levitan, David and Sim, Robert},
	month = jul,
	year = {2023},
	pages = {1321--1342},
}


@inproceedings{Xu-etal-2023a,
	address = {Toronto, Canada},
	title = {{S2ynRE}: {Two}-stage {Self}-training with {Synthetic} data for {Low}-resource {Relation} {Extraction}},
	shorttitle = {{S2ynRE}},
	url = {https://aclanthology.org/2023.acl-long.455/},
	doi = {10.18653/v1/2023.acl-long.455},
	abstract = {Current relation extraction methods suffer from the inadequacy of large-scale annotated data. While distant supervision alleviates the problem of data quantities, there still exists domain disparity in data qualities due to its reliance on domain-restrained knowledge bases. In this work, we propose S2ynRE, a framework of two-stage Self-training with Synthetic data for Relation Extraction.We first leverage the capability of large language models to adapt to the target domain and automatically synthesize large quantities of coherent, realistic training data. We then propose an accompanied two-stage self-training algorithm that iteratively and alternately learns from synthetic and golden data together. We conduct comprehensive experiments and detailed ablations on popular relation extraction datasets to demonstrate the effectiveness of the proposed framework.},
	urldate = {2025-02-15},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Xu, Benfeng and Wang, Quan and Lyu, Yajuan and Dai, Dai and Zhang, Yongdong and Mao, Zhendong},
	editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
	month = jul,
	year = {2023},
	pages = {8186--8207},
}



@inproceedings{Devlin-etal-2019,
	address = {Minneapolis, Minnesota},
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {https://aclanthology.org/N19-1423},
	doi = {10.18653/v1/N19-1423},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2021-10-26},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = jun,
	year = {2019},
	pages = {4171--4186},
}


@article{Malmsten-etal-2020,
	title = {Playing with {Words} at the {National} {Library} of {Sweden} -- {Making} a {Swedish} {BERT}},
	url = {http://arxiv.org/abs/2007.01658},
	abstract = {This paper introduces the Swedish BERT ("KB-BERT") developed by the KBLab for data-driven research at the National Library of Sweden (KB). Building on recent efforts to create transformer-based BERT models for languages other than English, we explain how we used KB's collections to create and train a new language-specific BERT model for Swedish. We also present the results of our model in comparison with existing models - chiefly that produced by the Swedish Public Employment Service, Arbetsf{\textbackslash}"ormedlingen, and Google's multilingual M-BERT - where we demonstrate that KB-BERT outperforms these in a range of NLP tasks from named entity recognition (NER) to part-of-speech tagging (POS). Our discussion highlights the difficulties that continue to exist given the lack of training data and testbeds for smaller languages like Swedish. We release our model for further exploration and research here: https://github.com/Kungbib/swedish-bert-models .},
	urldate = {2021-04-07},
	journal = {arXiv:2007.01658 [cs]},
	author = {Malmsten, Martin and Börjeson, Love and Haffenden, Chris},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.01658},
	keywords = {Computer Science - Computation and Language},
}



@inproceedings{Dalianis-etal-2015,
	title = {{HEALTH} {BANK} - {A} workbench for data science applications in healthcare},
	abstract = {The enormous amounts of data that are generated in the healthcare process and stored in electronic health record (EHR) systems are an underutilized resource that, with the use of data science applications, can be exploited to improve healthcare. To foster the development and use of data science applications in healthcare, there is a fundamental need for access to EHR data, which is typically not readily available to researchers and developers. A relatively rare exception is the large EHR database, the Stockholm EPR Corpus, comprising data from more than two million patients, that has been been made available to a limited group of researchers at Stockholm University. Here, we describe a number of data science applications that have been developed using this database, demonstrating the potential reuse of EHR data to support healthcare and public health activities, as well as facilitate medical research. However, in order to realize the full potential of this resource, it needs to be made available to a larger community of researchers, as well as to industry actors. To that end, we envision the provision of an infrastructure around this database called HEALTH BANK - the Swedish Health Record Research Bank. It will function both as a workbench for the development of data science applications and as a data exploration tool, allowing epidemiologists, pharmacologists and other medical researchers to generate and evaluate hypotheses. Aggregated data will be fed into a pipeline for open e-access, while non-aggregated data will be provided to researchers within an ethical permission framework. We believe that HEALTH BANK has the potential to promote a growing industry around the development of data science applications that will ultimately increase the efficiency and effectiveness of healthcare. Copyright © 2015 held by the authors. Copyright © 2015 for the individual papers by the papers' authors.},
	language = {English},
	booktitle = {{CEUR} {Workshop} {Proceedings}},
	publisher = {CEUR-WS},
	author = {Dalianis, Hercules and Henriksson, Aron and Kvist, Maria and Velupillai, Sumithra and Weegar, Rebecka},
	year = {2015},
}



@inproceedings{Holtzman-etal-2020,
	title = {The {Curious} {Case} of {Neural} {Text} {Degeneration}},
	url = {https://iclr.cc/virtual_2020/poster_rygGQyrFvH.html},
	abstract = {Current language generation systems either aim for high likelihood and devolve into generic repetition or miscalibrate their stochasticity—we provide evidence of both and propose a solution:...},
	language = {en},
	urldate = {2021-04-01},
	booktitle = {Proceedings of the {Eighth} {International} {Conference} on {Learning} {Representations}},
	author = {Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
	year = {2020},
}


@inproceedings{Kwon-etal-2023,
	address = {New York, NY, USA},
	series = {{SOSP} '23},
	title = {Efficient {Memory} {Management} for {Large} {Language} {Model} {Serving} with {PagedAttention}},
	isbn = {9798400702297},
	url = {https://dl.acm.org/doi/10.1145/3600006.3613165},
	doi = {10.1145/3600006.3613165},
	abstract = {High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time. However, existing systems struggle because the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. To address this problem, we propose PagedAttention, an attention algorithm inspired by the classical virtual memory and paging techniques in operating systems. On top of it, we build vLLM, an LLM serving system that achieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV cache within and across requests to further reduce memory usage. Our evaluations show that vLLM improves the throughput of popular LLMs by 2--4× with the same level of latency compared to the state-of-the-art systems, such as FasterTransformer and Orca. The improvement is more pronounced with longer sequences, larger models, and more complex decoding algorithms. vLLM's source code is publicly available at https://github.com/vllm-project/vllm.},
	urldate = {2025-02-03},
	booktitle = {Proceedings of the 29th {Symposium} on {Operating} {Systems} {Principles}},
	publisher = {Association for Computing Machinery},
	author = {Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion},
	year = {2023},
	pages = {611--626},
}


@inproceedings{Ekgren-etal-2024,
	address = {Torino, Italia},
	title = {{GPT}-{SW3}: {An} {Autoregressive} {Language} {Model} for the {Scandinavian} {Languages}},
	shorttitle = {{GPT}-{SW3}},
	url = {https://aclanthology.org/2024.lrec-main.695/},
	abstract = {This paper details the process of developing the first native large generative language model for the North Germanic languages, GPT-SW3. We cover all parts of the development process, from data collection and processing, training configuration and instruction finetuning, to evaluation, applications, and considerations for release strategies. We discuss pros and cons of developing large language models for smaller languages and in relatively peripheral regions of the globe, and we hope that this paper can serve as a guide and reference for other researchers that undertake the development of large generative models for smaller languages.},
	urldate = {2025-01-30},
	booktitle = {Proceedings of the 2024 {Joint} {International} {Conference} on {Computational} {Linguistics}, {Language} {Resources} and {Evaluation} ({LREC}-{COLING} 2024)},
	publisher = {ELRA and ICCL},
	author = {Ekgren, Ariel and Cuba Gyllensten, Amaru and Stollenwerk, Felix and Öhman, Joey and Isbister, Tim and Gogoulou, Evangelia and Carlsson, Fredrik and Casademont, Judit and Sahlgren, Magnus},
	editor = {Calzolari, Nicoletta and Kan, Min-Yen and Hoste, Veronique and Lenci, Alessandro and Sakti, Sakriani and Xue, Nianwen},
	month = may,
	year = {2024},
	pages = {7886--7900},
}


@misc{Scao-etal-2023,
	title = {{BLOOM}: {A} {176B}-{Parameter} {Open}-{Access} {Multilingual} {Language} {Model}},
	shorttitle = {{BLOOM}},
	url = {https://inria.hal.science/hal-03850124},
	abstract = {Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.},
	language = {en},
	urldate = {2025-01-30},
	author = {Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ilić, Suzana and Hesslow, Daniel and Castagné, Roman and Luccioni, Alexandra Sasha and Yvon, François and Gallé, Matthias and Tow, Jonathan and Rush, Alexander M. and Biderman, Stella and others},
	month = nov,
	year = {2023},
}


@inproceedings{DaDalt-etal-2024,
	address = {Torino, Italia},
	title = {{FLOR}: {On} the {Effectiveness} of {Language} {Adaptation}},
	shorttitle = {{FLOR}},
	url = {https://aclanthology.org/2024.lrec-main.650/},
	abstract = {Large language models have amply proven their great capabilities, both in downstream tasks and real-life settings. However, low- and mid-resource languages do not have access to the necessary means to train such models from scratch, and often have to rely on multilingual models despite being underrepresented in the training data. For the particular case of the Catalan language, we prove that continued pre-training with vocabulary adaptation is a better alternative to take the most out of already pre-trained models, even if these have not seen any Catalan data during their pre-training phase. We curate a 26B tokens corpus and use it to further pre-train BLOOM, giving rise to the FLOR models. We perform an extensive evaluation to assess the effectiveness of our method, obtaining consistent gains across Catalan and Spanish tasks. The models, training data, and evaluation framework are made freely available under permissive licenses.},
	urldate = {2025-01-30},
	booktitle = {Proceedings of the 2024 {Joint} {International} {Conference} on {Computational} {Linguistics}, {Language} {Resources} and {Evaluation} ({LREC}-{COLING} 2024)},
	publisher = {ELRA and ICCL},
	author = {Da Dalt, Severino and Llop, Joan and Baucells, Irene and Pamies, Marc and Xu, Yishi and Gonzalez-Agirre, Aitor and Villegas, Marta},
	editor = {Calzolari, Nicoletta and Kan, Min-Yen and Hoste, Veronique and Lenci, Alessandro and Sakti, Sakriani and Xue, Nianwen},
	month = may,
	year = {2024},
	pages = {7377--7388},
}


@inproceedings{Vakili-etal-2022,
	title = {Downstream {Task} {Performance} of {BERT} {Models} {Pre}-{Trained} {Using} {Automatically} {De}-{Identified} {Clinical} {Data}},
	copyright = {All rights reserved},
	url = {http://urn.kb.se/resolve?urn=urn:nbn:se:su:diva-207395},
	abstract = {DiVA portal is a finding tool for research publications and student theses written at the following 50 universities and research institutions.},
	language = {eng},
	urldate = {2022-08-23},
	booktitle = {Proceedings of the 13th {Conference} on {Language} {Resources} and {Evaluation}},
	publisher = {European Language Resources Association},
	author = {Vakili, Thomas and Lamproudis, Anastasios and Henriksson, Aron and Dalianis, Hercules},
	year = {2022},
	pages = {4245--4252},
}


@article{Gutierrez-Fandino-etal-2022,
	title = {{MarIA}: {Spanish} {Language} {Models}},
	volume = {68},
	copyright = {Copyright (c) 2022 Procesamiento del Lenguaje Natural},
	issn = {1989-7553},
	shorttitle = {{MarIA}},
	url = {http://journal.sepln.org/sepln/ojs/ojs/index.php/pln/article/view/6405},
	abstract = {This work presents MarIA, a family of Spanish language models and associated resources made available to the industry and the research community. Currently, MarIA includes RoBERTa-base, RoBERTa-large, GPT2 and GPT2-large Spanish language models, which can arguably be presented as the largest and most proficient language models in Spanish. The models were pretrained using a massive corpus of 570GB of clean and deduplicated texts with 135 billion words extracted from the Spanish Web Archive crawled by the National Library of Spain between 2009 and 2019. We assessed the performance of the models with nine existing evaluation datasets and with a novel extractive Question Answering dataset created ex novo. Overall, MarIA models outperform the existing Spanish models across a variety of NLU tasks and training settings.},
	language = {es\_ES},
	number = {0},
	urldate = {2024-08-27},
	journal = {Procesamiento del Lenguaje Natural},
	author = {Gutiérrez-Fandiño, Asier and Armengol-Estapé, Jordi and Pàmies, Marc and Llop-Palao, Joan and Silveira-Ocampo, Joaquin and Carrino, Casimiro Pio and Armentano-Oller, Carme and Rodriguez-Penagos, Carlos and Gonzalez-Agirre, Aitor and Villegas, Marta},
	month = mar,
	year = {2022},
	note = {Number: 0},
	pages = {39--60},
}


@inproceedings{Marimon-etal-2019,
	title = {Automatic {De}-{Identiﬁcation} of {Medical} {Texts} in {Spanish}: the {MEDDOCAN} {Track}, {Corpus}, {Guidelines}, {Methods} and {Evaluation} of {Results}},
	abstract = {There is an increasing interest in exploiting the content of electronic health records by means of natural language processing and text-mining technologies, as they can result in resources for improving patient health/safety, aid in clinical decision making, facilitate drug repurposing or precision medicine. To share, re-distribute and make clinical narratives accessible for text mining research purposes, it is key to fulﬁll legal conditions and address restrictions related data protection and patient privacy. Thus, clinical records cannot be shared directly ”as is”. A necessary precondition for accessing clinical records outside of hospitals is their de-identiﬁcation or exhaustive removal/replacement of all mentioned privacy related protected health information phrases. Providing a proper evaluation scenario for automatic anonymization tools is key for approval of data redistribution. The construction of manually de-identiﬁed medical records is currently the main rate and cost-limiting step for secondary use applications of clinical data. This paper summarizes the settings, data and results of the ﬁrst shared track on anonymization of medical documents in Spanish, the MEDDOCAN (Medical Document Anonymization) track. This track relied on a carefully constructed synthetic corpus of clinical case documents, the MEDDOCAN corpus, following annotation guidelines for sensitive data based on the analysis of the EU General Data Protection Regulation. A total of 18 teams (from the 51 registrations) submitted 63 runs for ﬁrst sub-track 1 and 61 systems for the second sub-track. The top scoring systems were based on sophisticated deep learning approaches, representing strategies that can signiﬁcantly reduce time and costs associated to accessing textual data containing privacy-related sensitive information. The results of this track might help in lowering the clinical data access hurdle for Spanish language technology developers, showing also potentials for similar settings using data in other languages or from diﬀerent domains.},
    booktitle = {IberLEF @ SEPLN 2019},
	language = {en},
	author = {Marimon, Montserrat and Gonzalez-Agirre, Aitor and Intxaurrondo, Ander and Martin, Jose Antonio Lopez and Villegas, Marta},
	year = {2019},
}

@article{velupillai2009developing,
  title={{Developing a standard for de-identifying electronic patient records written in Swedish: precision, recall and F-measure in a manual and computerized annotation trial}},
  author={Velupillai, Sumithra and Dalianis, Hercules and Hassel, Martin and Nilsson, Gunnar H},
  journal={International journal of medical informatics},
  volume={78},
  number={12},
  pages={e19--e26},
  year={2009},
  publisher={Elsevier}
}

@inproceedings{dalianis2019pseudonymisation,
  title={{Pseudonymisation of Swedish electronic patient records using a rule-based approach}},
  author={Dalianis, Hercules},
  booktitle={Proceedings of the Workshop on NLP and Pseudonymisation},
  volume={166},
  pages={16--23},
  year={2019}
}

@article{libbi2021generating,
  title={Generating synthetic training data for supervised de-identification of electronic health records},
  author={Libbi, Claudia Alessandra and Trienes, Jan and Trieschnigg, Dolf and Seifert, Christin},
  journal={Future Internet},
  volume={13},
  number={5},
  pages={136},
  year={2021},
  publisher={MDPI}
}

@inproceedings{hiebel2023can,
  title={{Can synthetic text help clinical named entity recognition? a study of electronic health records in French}},
  author={Hiebel, Nicolas and Ferret, Olivier and Fort, Kar{\"e}n and N{\'e}v{\'e}ol, Aur{\'e}lie},
  booktitle={EACL The 17th Conference of the European Chapter of the Association for Computational Linguistics},
  year={2023}
}

@inproceedings{Dettmers-etal-2023,
	title = {{QLoRA}: {Efficient} {Finetuning} of {Quantized} {LLMs}},
	volume = {36},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/1feb87871436031bdc0f2beaa62a049b-Paper-Conference.pdf},
	booktitle = {Advances in neural information processing systems},
	publisher = {Curran Associates, Inc.},
	author = {Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
	editor = {Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
	year = {2023},
	pages = {10088--10115},
}

@article{ive2020generation,
  title={{Generation and evaluation of artificial mental health records for natural language processing}},
  author={Ive, Julia and Viani, Natalia and Kam, Joyce and Yin, Lucia and Verma, Somain and Puntis, Stephen and Cardinal, Rudolf N and Roberts, Angus and Stewart, Robert and Velupillai, Sumithra},
  journal={NPJ digital medicine},
  volume={3},
  number={1},
  pages={69},
  year={2020},
  publisher={Nature Publishing Group UK London}
}

@article{meystre2010automatic,
  title={Automatic de-identification of textual documents in the electronic health record: a review of recent research},
  author={Meystre, Stephane M and Friedlin, F Jeffrey and South, Brett R and Shen, Shuying and Samore, Matthew H},
  journal={BMC medical research methodology},
  volume={10},
  pages={1--16},
  year={2010},
  publisher={Springer}
}

@mathesis{kiefer2024instruction,
  author = {Kiefer, Lotta},
  title = {{Instruction-Tuning {LLaMA} for Synthetic Medical Note Generation: Bridging Data Privacy and Utility in Downstream Tasks}},
  school = {Saarland University},
  year = {2024},
  type = {Master's Thesis},
  address = {Saarbrücken, Germany},
  note = {Master's thesis, Saarland University}
}

@article{radford_language_2019,
    title = {Language models are unsupervised multitask learners},
    volume = {1},
    number = {8},
    journal = {OpenAI blog},
    author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
    year = {2019},
    pages = {9},
}

@article{liu_roberta_2019,
    title = {{RoBERTa}: {A} {Robustly} {Optimized} {BERT} {Pretraining} {Approach}},
    shorttitle = {{RoBERTa}},
    url = {http://arxiv.org/abs/1907.11692},
    abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
    urldate = {2021-04-08},
    journal = {arXiv:1907.11692 [cs]},
    author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
    month = jul,
    year = {2019},
    note = {arXiv: 1907.11692},
    keywords = {Computer Science - Computation and Language},
}
@article{pilan_text_2022,
    title = {The {Text} {Anonymization} {Benchmark} ({TAB}): {A} {Dedicated} {Corpus} and {Evaluation} {Framework} for {Text} {Anonymization}},
    volume = {48},
    issn = {0891-2017},
    shorttitle = {The {Text} {Anonymization} {Benchmark} ({TAB})},
    url = {https://doi.org/10.1162/coli_a_00458},
    doi = {10.1162/coli_a_00458},
    abstract = {We present a novel benchmark and associated evaluation metrics for assessing the performance of text anonymization methods. Text anonymization, defined as the task of editing a text document to prevent the disclosure of personal information, currently suffers from a shortage of privacy-oriented annotated text resources, making it difficult to properly evaluate the level of privacy protection offered by various anonymization methods. This paper presents TAB (Text Anonymization Benchmark), a new, open-source annotated corpus developed to address this shortage. The corpus comprises 1,268 English-language court cases from the European Court of Human Rights (ECHR) enriched with comprehensive annotations about the personal information appearing in each document, including their semantic category, identifier type, confidential attributes, and co-reference relations. Compared with previous work, the TAB corpus is designed to go beyond traditional de-identification (which is limited to the detection of predefined semantic categories), and explicitly marks which text spans ought to be masked in order to conceal the identity of the person to be protected.Along with presenting the corpus and its annotation layers, we also propose a set of evaluation metrics that are specifically tailored toward measuring the performance of text anonymization, both in terms of privacy protection and utility preservation. We illustrate the use of the benchmark and the proposed metrics by assessing the empirical performance of several baseline text anonymization models. The full corpus along with its privacy-oriented annotation guidelines, evaluation scripts, and baseline models are available on: https://github.com/NorskRegnesentral/text-anonymization-benchmark.},
    number = {4},
    urldate = {2023-03-05},
    journal = {Computational Linguistics},
    author = {Pilán, Ildikó and Lison, Pierre and Øvrelid, Lilja and Papadopoulou, Anthi and Sánchez, David and Batet, Montserrat},
    month = dec,
    year = {2022},
    pages = {1053--1101},
}
@inproceedings{brown_what_2022,
    address = {New York, NY, USA},
    series = {{FAccT} '22},
    title = {What {Does} it {Mean} for a {Language} {Model} to {Preserve} {Privacy}?},
    isbn = {978-1-4503-9352-2},
    url = {https://doi.org/10.1145/3531146.3534642},
    doi = {10.1145/3531146.3534642},
    abstract = {Natural language reflects our private lives and identities, making its privacy concerns as broad as those of real life. Language models lack the ability to understand the context and sensitivity of text, and tend to memorize phrases present in their training sets. An adversary can exploit this tendency to extract training data. Depending on the nature of the content and the context in which this data was collected, this could violate expectations of privacy. Thus, there is a growing interest in techniques for training language models that preserve privacy. In this paper, we discuss the mismatch between the narrow assumptions made by popular data protection techniques (data sanitization and differential privacy), and the broadness of natural language and of privacy as a social norm. We argue that existing protection methods cannot guarantee a generic and meaningful notion of privacy for language models. We conclude that language models should be trained on text data which was explicitly produced for public use.},
    urldate = {2022-12-18},
    booktitle = {2022 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
    publisher = {Association for Computing Machinery},
    author = {Brown, Hannah and Lee, Katherine and Mireshghallah, Fatemehsadat and Shokri, Reza and Tramèr, Florian},
    month = jun,
    year = {2022},
    keywords = {Data Sanitization, Differential Privacy, Natural Language Processing, Privacy},
    pages = {2280--2292},
}
@inproceedings{igamberdiev_dp-nmt_2024,
    address = {St. Julians, Malta},
    title = {{DP}-{NMT}: {Scalable} {Differentially} {Private} {Machine} {Translation}},
    shorttitle = {{DP}-{NMT}},
    url = {https://aclanthology.org/2024.eacl-demo.11/},
    abstract = {Neural machine translation (NMT) is a widely popular text generation task, yet there is a considerable research gap in the development of privacy-preserving NMT models, despite significant data privacy concerns for NMT systems. Differentially private stochastic gradient descent (DP-SGD) is a popular method for training machine learning models with concrete privacy guarantees; however, the implementation specifics of training a model with DP-SGD are not always clarified in existing models, with differing software libraries used and code bases not always being public, leading to reproducibility issues. To tackle this, we introduce DP-NMT, an open-source framework for carrying out research on privacy-preserving NMT with DP-SGD, bringing together numerous models, datasets, and evaluation metrics in one systematic software package. Our goal is to provide a platform for researchers to advance the development of privacy-preserving NMT systems, keeping the specific details of the DP-SGD algorithm transparent and intuitive to implement. We run a set of experiments on datasets from both general and privacy-related domains to demonstrate our framework in use. We make our framework publicly available and welcome feedback from the community.},
    urldate = {2025-02-15},
    booktitle = {Proceedings of the 18th {Conference} of the {European} {Chapter} of the {Association} for {Computational} {Linguistics}: {System} {Demonstrations}},
    publisher = {Association for Computational Linguistics},
    author = {Igamberdiev, Timour and Vu, Doan Nam Long and Kuennecke, Felix and Yu, Zhuo and Holmer, Jannik and Habernal, Ivan},
    editor = {Aletras, Nikolaos and De Clercq, Orphee},
    month = mar,
    year = {2024},
    pages = {94--105},
}

@inproceedings{Bridal-etal-2022,
	title = {Cross-{Clinic} {De}-{Identification} of {Swedish} {Electronic} {Health} {Records} : {Nuances} and {Caveats}},
	copyright = {All rights reserved},
	shorttitle = {Cross-{Clinic} {De}-{Identification} of {Swedish} {Electronic} {Health} {Records}},
	url = {http://urn.kb.se/resolve?urn=urn:nbn:se:su:diva-207392},
	language = {eng},
	urldate = {2022-08-23},
	booktitle = {Proceedings of the {Legal} and {Ethical} issues {Workshop} @{LREC2022}},
	publisher = {European Language Resources Association},
	author = {Bridal, Olle and Vakili, Thomas and Santini, Marina},
	year = {2022},
	pages = {49--52},
}