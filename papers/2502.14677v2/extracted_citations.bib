@article{Liu-etal-2025,
	title = {Generating synthetic clinical text with local large language models to identify misdiagnosed limb fractures in radiology reports},
	volume = {159},
	issn = {0933-3657},
	url = {https://www.sciencedirect.com/science/article/pii/S0933365724002690},
	doi = {10.1016/j.artmed.2024.103027},
	abstract = {Large language models (LLMs) demonstrate impressive capabilities in generating human-like content and have much potential to improve the performance and efficiency of healthcare. An important application of LLMs is to generate synthetic clinical reports that could alleviate the burden of annotating and collecting real-world data in training AI models. Meanwhile, there could be concerns and limitations in using commercial LLMs to handle sensitive clinical data. In this study, we examined the use of open-source LLMs as an alternative to generate synthetic radiology reports to supplement real-world annotated data. We found LLMs hosted locally can achieve similar performance compared to ChatGPT and GPT-4 in augmenting training data for the downstream report classification task of identifying misdiagnosed fractures. We also examined the predictive value of using synthetic reports alone for training downstream models, where our best setting achieved more than 90 \% of the performance using real-world data. Overall, our findings show that open-source, local LLMs can be a favourable option for creating synthetic clinical reports for downstream tasks.},
	urldate = {2025-02-14},
	journal = {Artificial Intelligence in Medicine},
	author = {Liu, Jinghui and Koopman, Bevan and Brown, Nathan J. and Chu, Kevin and Nguyen, Anthony},
	month = jan,
	year = {2025},
	keywords = {Emergency department, Large language models, Local LLMs, Natural language processing, Radiology report, Synthetic data},
	pages = {103027},
}

@inproceedings{Miranda-etal-2024,
	title = {Evaluating {Privacy} {Risks} in {Synthetic} {Clinical} {Text} {Generation} in {Spanish}},
	url = {https://openreview.net/forum?id=0hxBH0dEKu},
	abstract = {Leveraging medical data for Deep Learning models holds great potential, but ensuring the protection of sensitive patient information is paramount in the clinical domain. A widely used approach to balance data utility and privacy is the generation of synthetic text with Large Language Models (LLMs) under the framework of differential privacy (DP). Techniques like Differentially Private Stochastic Gradient Descent (DP-SGD) are typically considered to provide privacy guarantees, but they rely on specific conditions. This research demonstrates how memorization in LLMs can deteriorate when these privacy safeguards are not fully met, increasing the risk of personal and sensitive information being leaked in synthetic clinical reports. Addressing these vulnerabilities could enhance the reliability of DP in protecting clinical text data while maintaining its utility.},
	language = {en},
	urldate = {2025-02-14},
	author = {Miranda, Luis and Dunstan, Jocelyn and Toro, Matías and Olmedo, Federico and Melo, Felix}

@inproceedings{Xu-etal-2023a,
	address = {Toronto, Canada},
	title = {{S2ynRE}: {Two}-stage {Self}-training with {Synthetic} data for {Low}-resource {Relation} {Extraction}},
	shorttitle = {{S2ynRE}},
	url = {https://aclanthology.org/2023.acl-long.455/},
	doi = {10.18653/v1/2023.acl-long.455},
	abstract = {Current relation extraction methods suffer from the inadequacy of large-scale annotated data. While distant supervision alleviates the problem of data quantities, there still exists domain disparity in data qualities due to its reliance on domain-restrained knowledge bases. In this work, we propose S2ynRE, a framework of two-stage Self-training with Synthetic data for Relation Extraction.We first leverage the capability of large language models to adapt to the target domain and automatically synthesize large quantities of coherent, realistic training data. We then propose an accompanied two-stage self-training algorithm that iteratively and alternately learns from synthetic and golden data together. We conduct comprehensive experiments and detailed ablations on popular relation extraction datasets to demonstrate the effectiveness of the proposed framework.},
	urldate = {2025-02-15},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Xu, Benfeng and Wang, Quan and Lyu, Yajuan and Dai, Dai and Zhang, Yongdong and Mao, Zhendong},
	editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
	month = jul,
	year = {2023},
	pages = {8186--8207},
}

@inproceedings{Yue-etal-2023,
	address = {Toronto, Canada},
	title = {Synthetic {Text} {Generation} with {Differential} {Privacy}: {A} {Simple} and {Practical} {Recipe}},
	shorttitle = {Synthetic {Text} {Generation} with {Differential} {Privacy}},
	url = {https://aclanthology.org/2023.acl-long.74},
	abstract = {Privacy concerns have attracted increasing attention in data-driven products due to the tendency of machine learning models to memorize sensitive training data. Generating synthetic versions of such data with a formal privacy guarantee, such as differential privacy (DP), provides a promising path to mitigating these privacy concerns, but previous approaches in this direction have typically failed to produce synthetic data of high quality. In this work, we show that a simple and practical recipe in the text domain is effective: simply fine-tuning a pretrained generative language model with DP enables the model to generate useful synthetic text with strong privacy protection. Through extensive empirical analyses on both benchmark and private customer data, we demonstrate that our method produces synthetic text that is competitive in terms of utility with its non-private counterpart, meanwhile providing strong protection against potential privacy leakages.},
	urldate = {2023-07-18},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Yue, Xiang and Inan, Huseyin and Li, Xuechen and Kumar, Girish and McAnallen, Julia and Shajari, Hoda and Sun, Huan and Levitan, David and Sim, Robert},
	month = jul,
	year = {2023},
	pages = {1321--1342},
}

@inproceedings{hiebel2023can,
  title={{Can synthetic text help clinical named entity recognition? a study of electronic health records in French}},
  author={Hiebel, Nicolas and Ferret, Olivier and Fort, Kar{\"e}n and N{\'e}v{\'e}ol, Aur{\'e}lie},
  booktitle={EACL The 17th Conference of the European Chapter of the Association for Computational Linguistics},
  year={2023}
}

@inproceedings{igamberdiev_dp-nmt_2024,
    address = {St. Julians, Malta},
    title = {{DP}-{NMT}: {Scalable} {Differentially} {Private} {Machine} {Translation}},
    shorttitle = {{DP}-{NMT}},
    url = {https://aclanthology.org/2024.eacl-demo.11/},
    abstract = {Neural machine translation (NMT) is a widely popular text generation task, yet there is a considerable research gap in the development of privacy-preserving NMT models, despite significant data privacy concerns for NMT systems. Differentially private stochastic gradient descent (DP-SGD) is a popular method for training machine learning models with concrete privacy guarantees; however, the implementation specifics of training a model with DP-SGD are not always clarified in existing models, with differing software libraries used and code bases not always being public, leading to reproducibility issues. To tackle this, we introduce DP-NMT, an open-source framework for carrying out research on privacy-preserving NMT with DP-SGD, bringing together numerous models, datasets, and evaluation metrics in one systematic software package. Our goal is to provide a platform for researchers to advance the development of privacy-preserving NMT systems, keeping the specific details of the DP-SGD algorithm transparent and intuitive to implement. We run a set of experiments on datasets from both general and privacy-related domains to demonstrate our framework in use. We make our framework publicly available and welcome feedback from the community.},
    urldate = {2025-02-15},
    booktitle = {Proceedings of the 18th {Conference} of the {European} {Chapter} of the {Association} for {Computational} {Linguistics}: {System} {Demonstrations}},
    publisher = {Association for Computational Linguistics},
    author = {Igamberdiev, Timour and Vu, Doan Nam Long and Kuennecke, Felix and Yu, Zhuo and Holmer, Jannik and Habernal, Ivan},
    editor = {Aletras, Nikolaos and De Clercq, Orphee},
    month = mar,
    year = {2024},
    pages = {94--105},
}

@article{ive2020generation,
  title={{Generation and evaluation of artificial mental health records for natural language processing}},
  author={Ive, Julia and Viani, Natalia and Kam, Joyce and Yin, Lucia and Verma, Somain and Puntis, Stephen and Cardinal, Rudolf N and Roberts, Angus and Stewart, Robert and Velupillai, Sumithra},
  journal={NPJ digital medicine},
  volume={3},
  number={1},
  pages={69},
  year={2020},
  publisher={Nature Publishing Group UK London}
}

@mathesis{kiefer2024instruction,
  author = {Kiefer, Lotta},
  title = {{Instruction-Tuning {LLaMA} for Synthetic Medical Note Generation: Bridging Data Privacy and Utility in Downstream Tasks}},
  school = {Saarland University},
  year = {2024},
  type = {Master's Thesis},
  address = {Saarbrücken, Germany},
  note = {Master's thesis, Saarland University}
}

@article{libbi2021generating,
  title={Generating synthetic training data for supervised de-identification of electronic health records},
  author={Libbi, Claudia Alessandra and Trienes, Jan and Trieschnigg, Dolf and Seifert, Christin},
  journal={Future Internet},
  volume={13},
  number={5},
  pages={136},
  year={2021},
  publisher={MDPI}
}

@article{radford_language_2019,
    title = {Language models are unsupervised multitask learners},
    volume = {1},
    number = {8},
    journal = {OpenAI blog},
    author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
    year = {2019},
    pages = {9},
}

