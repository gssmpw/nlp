\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2024


% ready for submission
%\usepackage{neurips_2024}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
\usepackage[nonatbib, preprint]{neurips_2024}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2024}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2024}

\usepackage[numbers]{natbib}  % Force numerical citations

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{tikz}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{adjustbox} 
\usepackage{tcolorbox}
% \hypersetup{
%     colorlinks=true,
%     linkcolor=blue,
%     filecolor=magenta,      
%     urlcolor=blue
%     }

\definecolor{customcolor}{rgb}{0.675, 0.399, 0.373}
\hypersetup{
    colorlinks=true, % Enables colored links
    linkcolor=blue, % Internal links
    urlcolor=customcolor,  % External links (like the GitHub link)
    citecolor=blue  % Citation links
}



\usepackage{tcolorbox}
% \usepackage[finalizecache,cachedir=.]{minted}
% \tcbuselibrary{minted,breakable,xparse,skins}
\tcbuselibrary{listings,breakable,xparse,skins}  % Use listings instead of minted


% Updated background color to match second image
\definecolor{bg}{HTML}{1F1D2E}
% New colors for syntax highlighting
\definecolor{codetype}{HTML}{89DCEB}    % For Pipeline, Layer, etc.
\definecolor{codestring}{HTML}{A6E3A1}  % For strings like "gpt-4o"
\definecolor{linenumbers}{HTML}{9792A2}

\usepackage{fvextra}
\usepackage{inconsolata}  % Alternative monospaced font for pdfLaTeX
\renewcommand{\theFancyVerbLine}{%
  \textcolor{linenumbers}{\tiny\arabic{FancyVerbLine}}}

% Create a completely separate environment for code blocks
% \NewTCBListing{mintedbox}{O{}m!O{}}{%
%     reset,  % Reset all settings
%     breakable=true,
%     listing engine=minted,
%     listing only,
%     minted language=#2,
%     minted options={%
%         linenos,
%         escapeinside=||,
%         gobble=0,
%         breaklines=true,
%         breakafter=,,
%         fontsize=\fontsize{7}{8},
%         numbersep=12pt,
%         numbers=left,
%         frame=none,
%         style=one-dark,
%         #1
%     },
%     boxsep=0pt,
%     left skip=0pt,
%     right skip=0pt,
%     left=25pt,
%     right=0pt,
%     top=5pt,
%     bottom=5pt,
%     arc=4pt,
%     leftrule=0pt,
%     rightrule=0pt,
%     bottomrule=0pt,
%     toprule=0pt,
%     colback=bg,
%     colframe=bg,
%     enhanced,
%     overlay={%
%         \begin{tcbclipinterior}
%             \fill[bg] (frame.south west) rectangle ([xshift=20pt]frame.north west);
%         \end{tcbclipinterior}
%     },
%     #3
% }
\definecolor{codebg}{rgb}{0.12, 0.12, 0.15}  % Dark background
\definecolor{codetext}{rgb}{0.85, 0.85, 0.85}  % Light gray text
\definecolor{keywordcolor}{rgb}{0.98, 0.50, 0.30}  % Bright orange for keywords
\definecolor{stringcolor}{rgb}{0.60, 0.85, 0.50}  % Green for strings
\definecolor{commentcolor}{rgb}{0.55, 0.55, 0.60}  % Dim gray for comments
\definecolor{numbercolor}{rgb}{0.90, 0.80, 0.40}  % Yellow for numbers
\definecolor{operatorcolor}{rgb}{0.85, 0.60, 0.90}  % Purple for operators
\definecolor{triplequote}{rgb}{0.60, 0.85, 0.50}  % Green for triple-quoted strings
\definecolor{funcname}{rgb}{0.85, 0.60, 0.90}  % Purple for function and class names

% Define the Python listing style with explicit function/class highlighting
\lstdefinestyle{pythonstyle}{
    language=Python,
    backgroundcolor=\color{codebg},
    basicstyle=\fontsize{8}{8}\ttfamily\color{codetext},  % Main text color
    keywordstyle=\bfseries\color{keywordcolor},  % Built-in Python keywords (bold + orange)
    stringstyle=\color{stringcolor},  % Strings
    commentstyle=\itshape\color{commentcolor},  % Comments
    numberstyle=\color{numbercolor},  % Line numbers
    emphstyle=\color{funcname},  % Functions and class names (purple, NOT bold)
    emph={DiscreteScale, Pipeline, Layer, CoTUnit, JudgeUnit, WeightedSummedScoreExtractor, MeanVariancePoolUnit, MeanPoolUnit, ConversationalUnit, BooleanScale, prompt, extract, via, pin},  % Functions & classes in purple
    emphstyle={[2]\color{keywordcolor}},  % Custom keywords (orange, NOT bold)
    emph={[2]temperature, retries, repeat, role_name, inner, outer},  % Custom keywords
    literate={"""}
        {{\textcolor{triplequote}{\texttt{"""}}}}3  % Triple-quoted strings highlighted
        {'}{\textquotesingle}1
        {""}{\textquotedbl}1
        {>=}{{\textcolor{operatorcolor}{\texttt{>=}}}}2
        {<=}{{\textcolor{operatorcolor}{\texttt{<=}}}}2
        {==}{{\textcolor{operatorcolor}{\texttt{==}}}}2
        {=>}{{\textcolor{operatorcolor}{\texttt{=>}}}}2,
    breaklines=true,
    frame=none,
    showstringspaces=false,
    tabsize=4,
    upquote=true,
    columns=flexible,
    keepspaces=true
}


% Define a tcolorbox for Python with the updated style
\NewTCBListing{mintedbox}{O{}m!O{}}{%
    reset,  
    breakable=true,
    listing engine=listings,
    listing only,
    boxsep=0pt,
    left=10pt,
    right=10pt,
    top=5pt,
    bottom=5pt,
    arc=4pt,
    colback=codebg,
    colframe=codebg,
    enhanced,
    overlay={%
        \begin{tcbclipinterior}
            \fill[codebg] (frame.south west) rectangle (frame.north east);
        \end{tcbclipinterior}
    },
    listing options={%
        style=pythonstyle,  % Use our defined Python style
        #1
    }
}


% % Define a tcolorbox that uses the new Python style
% \NewTCBListing{mintedbox}{O{}m!O{}}{%
%     reset,  
%     breakable=true,
%     listing engine=listings,
%     listing only,
%     boxsep=0pt,
%     left=10pt,
%     right=10pt,
%     top=5pt,
%     bottom=5pt,
%     arc=4pt,
%     colback=codebg,
%     colframe=codebg,
%     enhanced,
%     overlay={%
%         \begin{tcbclipinterior}
%             \fill[codebg] (frame.south west) rectangle (frame.north east);
%         \end{tcbclipinterior}
%     },
%     listing options={%
%         style=pythonstyle,  % Use our defined Python style
%         #1
%     }
% }




% \newcommand{\textttpop}[1]{\texttt{\textcolor{blue}{#1}}}
% \newcommand{\textttpop}[1]{\texttt{\textcolor[rgb]{0.704,0.480,0.314}{#1}}}
% \newcommand{\textscpop}[1]{\textsc{\textcolor[rgb]{0.704,0.480,0.314}{#1}}}

\newcommand{\textttpop}[1]{\texttt{\textcolor{customcolor}{#1}}}
\newcommand{\textscpop}[1]{\textsc{\textcolor{customcolor}{#1}}}


\title{\textscpop{Verdict:} A Library for Scaling Judge-Time Compute}

\author{%
  Nimit Kalra\\
  Haize Labs\\
  New York City \\
  \texttt{nimit@haizelabs.com} \\
  \And
  Leonard Tang\\
  Haize Labs\\
  New York City \\
  \texttt{leonard@haizelabs.com}
}

\begin{document}

\maketitle

\begin{abstract}
  The use of LLMs as automated judges ("LLM-as-a-judge") is now widespread, yet standard judges suffer from a multitude of reliability issues. To address these challenges, we introduce \textscpop{Verdict}\footnote{Project page at \url{https://verdict.haizelabs.com/}.}, an open-source\footnote{Code available at \url{https://github.com/haizelabs/verdict}.} library for scaling \textcolor{customcolor}{\textit{judge-time compute}} to enhance the accuracy, reliability, and interpretability of automated evaluators. \textscpop{Verdict} leverages the composition of modular reasoning units---such as verification, debate, and aggregation---and increased inference-time compute to improve LLM judge quality. Across a variety of challenging tasks such as content moderation, fact-checking, and hallucination detection, \textscpop{Verdict} judges achieve state-of-the-art (SOTA) or near-SOTA performance, surpassing orders-of-magnitude larger fine-tuned judges, prompted judges, and reasoning models. Ultimately, we hope \textscpop{Verdict} serves as a useful framework for researchers and practitioners building scalable, interpretable, and reliable LLM-based evaluators.
\end{abstract}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.35\linewidth]{examples/Verdict-Logo.png}
    \label{fig:logo}
\end{figure}

\section{Building LLM Judges that Actually Work}

Automated evaluation using LLMs, a.k.a. ``LLMs-as-a-judge'', is a widely adopted practice for both developers and researchers building LLM-powered applications. However, LLM judges still face a variety of reliability issues, such as inconsistent output formats, missing or miscalibrated uncertainty quantification, biases towards superficial qualities such as answer positioning, style and tone, safety, numerical frequency and preferences, the type of underlying LLM being judged, and numerous other failure modes.

To mitigate these shortcomings, we developed \textscpop{Verdict}, a library for building compound LLM judge systems. \textscpop{Verdict} provides both the primitives (\textttpop{Units}) and execution framework for building such systems. Instead of a single LLM call to produce a judge result, \textscpop{Verdict} Judges combine multiple units of reasoning, verification, debate, and aggregation into a single judge system. When applied, these judge architectures leverage additional inference-time compute to yield impressive results on automatic evaluation of LLMs and LLM applications. 

\textscpop{Verdict}'s primary contributions are as follows:


\begin{center}
\begin{tcolorbox}[
    colback=customcolor!5!white, 
    colframe=customcolor!75!black, 
    % sharp corners=all,
    boxrule=1pt, 
    arc=3mm, % Increase the corner rounding
    width=0.9\textwidth, % Make the box narrower
    height=0.31\textheight,
    valign=center,
    halign=center,
    title style={font=\bfseries}, % Optional: Style the title if needed
]
\begin{enumerate}
    \item \textscpop{Verdict} provides a \textbf{unified interface} for a potpourri of prompting strategies, bias mitigation methods, architectures, and other principles grounded in frontier research. We support ideas from the disciplines of automated evaluation, scalable oversight, safety and content moderation, fact-checking, generative reward modeling, and more. 
    \item \textscpop{Verdict} introduces \textbf{powerful reasoning primitives and patterns} for automated evaluation, such as hierarchical reasoning verification and debate-aggregation.
    \item \textscpop{Verdict} is \textbf{fully composable}, allowing arbitrary reasoning patterns to be stacked into expressive and powerful architectures.
    \item \textscpop{Verdict} judges require \textit{minimal fitting} but achieve \textbf{SOTA or near-SOTA} performance on a wide variety of challenging automated evaluation evaluation tasks spanning safety moderation, factual and logical correctness, and hallucination detection.
\end{enumerate}
\end{tcolorbox}
\end{center}

\section{Prior Art in LLM Judging}
% Prompted  Judges
\subsection{Prompted Judges}
Prompted out-of-the-box LLMs are the earliest and most straightforward approach to automted evaluation. This method involves providing an LLM with specific instructions or criteria to assess outputs \cite{bai2024benchmarking, lin2023llm, zheng2023judging}. The effectiveness of prompted judges has been demonstrated in both pairwise comparison and single output scoring scenarios. One key advantage of prompted judges is their flexibility. They can be easily adapted to evaluate different aspects of LLM outputs, such as linguistic quality, content accuracy, and task-specific metrics. However, the reliability of prompted judges is often inhibited by factors such as prompt design, formatting, and in-context example selection. In the same way that underlying LLMs suffer from sensitivity, brittleness, and hallucinations, so too do prompted LLM judges \cite{shankar2024validates, thakur2024judging, wang2023large}.


% Fine-Tuned Judges 
\subsection{Fine-Tuned Judges}
One alternative to prompted judges is fine-tuned custom judge models. These fine-tuned judges alleviate the biases and shortcomings of prompted judges, and sometimes are also cheaper and smaller than prompted judges. Indeed, it is possible to produce fine-tuned judge models that are only 7B parameters with performance on-par with leading closed-source models \cite{kim2023prometheus, kim2024prometheus, zhu2023judgelm}. Oftentimes, custom judges are fine-tuned to improve performance on specific tasks like fact-checking and hallucination detection, rather than general preference modeling and evaluation \cite{tang-etal-2024-minicheck, wang2024halu}.

The emerging study of generative verifiers and reward models has also provided new insights for LLM judging. Unlike traditional discriminative verifiers, generative reward models utilize next-token prediction for training, allowing it to tap into the benefits of generative LLMs. Research has demonstrated that generative reward models outperform discriminative verifiers, DPO verifiers, and LLM-as-a-Judge approaches \cite{ankner2024critique, zhang2024generative}. On algorithmic and math reasoning tasks, generative reward models show a 16-40\% improvement in the number of problems solved using the Best-of-N method. 

While these models can often times appear strong, they have been shown that to overfit their training data distribution \cite{huang2024empirical, sphynx2024}.

% These models are trained on synthetic data produced by more capable models, and support both direct judging as well as pairwise judging. 

% Early Compound Judges
\subsection{Early Compound Judges}
Recent approaches have leveraged multiple LLM calls to enhance the accuracy, consistency, and robustness of evaluations. One such method is LLM debate, where multiple instances argue from different perspectives, presenting arguments and counterarguments. A final judge LLM analyzes these debates to provide a comprehensive and balanced conclusion \cite{chan2023chateval, chu2024pre, li2023prd}. This technique has contemporaneously gained traction in the scalable oversight community, particularly for improving the accuracy of less-capable judges \cite{kenton2024scalable, michael2023debate}.

Ensemble judging, which independently queries multiple LLM judges and aggregates their assessments, has also become widely adopted. Aggregation strategies range from simple majority voting to weighted averaging based on model confidence, as well as more sophisticated fusion techniques that account for the strengths of different models \cite{liang2024abseval, verga2024replacing}.

These early compound judge systems have demonstrated significant improvements over single-LLM methods. Smaller, weaker models can often be combined to rival or even surpass the performance of larger models. \textscpop{Verdict} unifies and builds upon these insights to yield even more reliable, accurate, and powerful compound judge systems.


% \textscpop{Verdict} Core Concepts
\section{\textscpop{Verdict} Core Concepts: Units and Orchestration}
At the heart of \textscpop{Verdict} are:
\begin{enumerate}
    \item Primitives for judging (\textttpop{Units}), and
    \item Methods for linking, orchestrating, and executing systems of \textttpop{Units}.
\end{enumerate}

% \section{\textscpop{Verdict} Primitives: Units}

\subsection{Basic Anatomy of a Unit}
A \textttpop{Unit} is the fundamental building block of a judge system. They are composed of the following:
\begin{itemize}
    \item \textbf{Prompt:} The instructions for how a \textttpop{Unit} should function.
    \item \textbf{Model:} The model that the \textttpop{Unit} calls to generate responses.
    \item \textbf{Scale:} The domain that generated values must be restricted to. For example, the \texttt{1--5} Likert Scale, or the \textttpop{Yes/No} Scale, or the \textttpop{Safe/Unsafe} Scale, or any other ordinal or categorical Scale.
    \item \textbf{Input Schema:} The values and types the \textttpop{Unit} can accept, either from a previous \textttpop{Unit} or from raw user-provided input.
    \item \textbf{Response Schema:} The values and types the \textttpop{Unit}'s model can generate.
    \item \textbf{Output Schema:} The values that are ultimately generated by the \textttpop{Unit}. This usually involves either postprocessing values produced by the Response Schema or updating a cumulative state from prior \textttpop{Units}. These values get passed to the subsequent \textttpop{Unit}.
\end{itemize}

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{Units.png}
    \caption{A few common \textttpop{Units} in \textscpop{Verdict}, segregated by functionality. \textttpop{JudgeUnits} produce a judge result (e.g. a float score or selected category); \textttpop{IntermediateUnits} produce reasoning as part of a broader \textscpop{Verdict} system; \textttpop{MapUnits} aggregate intermediate results into a final verdict.}
    \label{fig:layer-connections}
\end{figure}

The structure of a \textttpop{Unit} solves two major pain points of standard LLM judges:
\begin{enumerate}
    \item The output structure of the judge is \emph{predictable} even for small language models, governed in particular by the Scale, Response Schema, and Output Schema.
    \item Format and function are specified separately. The Prompt implements the evaluation logic---the function---of the \textttpop{Unit}, while the Scale and Schemas manage the format of the \textttpop{Unit}.
\end{enumerate}

In the context of a \textscpop{Verdict} system, \textttpop{Units} that are chained together are also automatically type-checked. The Output Schema of a \textttpop{Unit} must match the Input Schema of its subsequent \textttpop{Unit}. This allows for information to flow through a compound judge system in a stable and predictable fashion.

\subsection{Uncertainty Quantification with Extracting Logprobs}
While simple Scales like the \texttt{1--5} Likert Scale are straightforward to interpret and implement, they miss out on the rich information from a model's underlying probability distribution.

For more precise, calibrated, and powerful judging, \textttpop{Units} support accessing, managing, and aggregating the log probabilities of their corresponding Scale. For example, if using a \texttt{1--5} Likert Scale, we can access not just the token output (an integer 1 through 5), but the log probabilities across the full distribution---in this case, the log probability of each integer 1 through 5.

One can then \emph{extract} the log probability as a final score, or \emph{extract} a weighted sum (with potentially learned weights to further reduce bias) from the distribution. For simplicity, probabilities are not managed by default unless a \textttpop{Unit} invokes an Extractor.

\subsection{Common Units}
\textttpop{Units} are meant to be maximally flexible and amenable to any practitioner or researcher use case. Nonetheless, we implement the following \textttpop{Units} that appear frequently across use cases:

\paragraph{Judge Units:}
\begin{itemize}
    \item \textbf{JudgeUnit:} A judge that supports any discrete Scale, such as a Likert Scale, categorical Scale, or ordinal Scale. This is the standard LLM-as-a-judge.
    \item \textbf{PairwiseJudgeUnit:} A judge that takes as input two strings and outputs a preferred choice between the two strings. This may also effectively be used as a categorical judge. 
\end{itemize}

\paragraph{Intermediate Units:}
\begin{itemize}
    \item \textbf{CoTUnit:} Generates a Chain of Thought in the usual sense.
    \item \textbf{ConversationalUnit:} Takes as input an existing conversation (list of messages) and generates the next turn of the conversation. Generally used in tandem with other \textttpop{ConversationalUnits}.
    \item \textbf{DebateUnit:} A subclass of \textttpop{ConversationalUnit} meant for debate (arguing opposing stances) between other \textttpop{DebateUnits}. Standard \textttpop{ConversationalUnits} are more neutral in their discourse.
\end{itemize}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{Layer_Connections.png}
    \caption{A few common \textscpop{Verdict} patterns for stitching \textttpop{Units} together within and between \textttpop{Layers}.}
    \label{fig:layer-connections}
\end{figure}


\paragraph{Map/Aggregate Units:}
\begin{itemize}
    \item \textbf{MapUnit:} A generic way to aggregate outputs from a previous \textttpop{Layer's} \textttpop{Units} (and optionally across several \textttpop{Layers}).
    \item \textbf{MedianFilterUnit:} Takes the median of outputs from the previous layer of \textttpop{Units}.
    \item \textbf{MeanPoolUnit:} Takes the mean of outputs from the previous layer of \textttpop{Units}.
    \item \textbf{MaxPoolUnit:} Takes the max of outputs from the previous layer of \textttpop{Units}.
    \item \textbf{MeanVariancePoolUnit:} Takes the mean and variance of outputs from the previous layer of \textttpop{Units}.
\end{itemize}

\subsection{Stitching Units Together}
\textttpop{Units} on their own only re-implement existing LLM-judge methods, albeit in a more reliable and consistent fashion. Stitching them together, however, unlocks the full power of \textscpop{Verdict} and increased inference-time compute. Combining \textttpop{Units} with the right architecture priors can yield impressive results across a variety of evaluation, judging, and reward modeling tasks.

\textscpop{Verdict} draws inspiration from deep learning libraries like PyTorch vis-\`a-vis managing \textttpop{Unit} groups and interactions. The most fundamental organizational principle is that of a \textttpop{Layer}, which is a list of \textttpop{Units}. This is analogous to how neural network layers are a list of neurons.

A standard \textttpop{Layer} propagates information through a judge system in a \emph{feedforward} fashion. By default, subsequent \textttpop{Layer's} \textttpop{Units} receive the output of previous \textttpop{Layer's} \textttpop{Units} in a one-to-one fashion, and \textttpop{Units} within a \textttpop{Layer} are fully independent of one another. However, it is possible to customize \textttpop{Unit} behavior both \emph{within} a \textttpop{Layer} (using keyword \textttpop{inner=}) and \emph{between} current and subsequent \textttpop{Layers} (using keyword \textttpop{outer=}). Below are common instances of how \textttpop{Units} can be stitched together from \textttpop{Layer} to \textttpop{Layer}.



\section{Examples of Compound Judge Systems}
To illustrate the expressiveness and ease of \textscpop{Verdict}, we use it to implement various examples from the LLM Judge literature with just a few lines of code\footnote{These judge systems are also accessible via the \href{mailto:contact@haizelabs.com}{Haize Labs API}}.

% Debate Example
\subsection{Debate}
In the field of scalable oversight, \textit{debates} are often used to evaluate LLM reasoning, resolve conflicting perspectives, and identify the most robust and well-supported answers by having multiple models critique and counter-argue each other's outputs \cite{khan2024debating, michael2023debate}.

The debate method involves multiple LLMs (typically two) arguing different sides of a question or task, with a judge LLM or human evaluating the debate to determine the most convincing argument. Typically, the process is as follows:

\begin{enumerate}
    \item A question or task is presented to the debater LLMs.
    \item Each debater LLM provides an initial answer and supporting arguments.
    \item In subsequent rounds, debaters respond to each other's arguments, providing additional evidence or pointing out flaws in the opponent's reasoning.
    \item After a set number of rounds (often 3--5), the judge reviews the entire debate transcript.
    \item The judge determines which debater presented the most convincing argument and selects a winner.
\end{enumerate}

\textscpop{Verdict} implements this relatively complex process with a \textttpop{Pipeline} comprising \textttpop{ConversationalUnits}, a \textttpop{MapUnit}, and a \textttpop{JudgeUnit}. \textttpop{ConversationalUnits}  sequentially on top of each to execute several rounds of debate. The \textttpop{MapUnit} aggregates information from the debate history into a string format, and finally the \textttpop{JudgeUnit} performs the binary classification to determine which \textttpop{ConversationalUnits} prevails.

\begin{figure}[h!]
    \centering
    \begin{mintedbox}{python}
debate_prompt = """
    You are participating in a debate as the {unit.role_name}.
    Here is the debate transcript thus far:
    {input.conversation}
"""

Pipeline("Debate") \
    >> Layer(
        [
            ConversationalUnit(role_name="Proponent").prompt(debate_prompt),
            ConversationalUnit(role_name="Opponent").prompt(debate_prompt)
        ],
        repeat=3,
        inner="chain",
        outer="last"
    ) \
    >> JudgeUnit(BooleanScale()).prompt("""
        Evaluate the following debate transcript:
        {previous.conversation}
    """)
    \end{mintedbox}
    \caption{A \textscpop{Verdict} \textttpop{Pipeline} for the Debate protocol. Leveraging \textttpop{ConversationalUnits}, \textttpop{MapUnits}, and \textttpop{JudgeUnits} makes for a quick and easy implementation.}
    \label{fig:debate}
\end{figure}

% Jury Example
\subsection{Jury: Ensemble of Judges}
Instead of using a single large model---like \texttt{o1}---as a judge, one can instead use a Panel of LLM evaluators (PoLL) composed of multiple smaller models \cite{verga2024replacing}. The PoLL approach of ensembling smaller, diverse language models offers a more cost-effective (seven times less expensive than GPT-4), less biased, and potentially more accurate method for evaluating LLM outputs.

\begin{mintedbox}{python}
Pipeline("Jury") \
    >> Layer(
        [
        JudgeUnit(DiscreteScale([1, 5])).prompt(qa_judge_prompt).via(model)
            for model in [
                "gpt-4o-mini",
                "gpt-4o",
                "command-r",
                "command-r-plus",
                "claude-3-5-sonnet-20241022",
            ]
        ]
    ) \
    >> MeanPoolUnit()
    \end{mintedbox}

\begin{figure}[h!]
    \centering
    % \includegraphics[width=1\linewidth]{examples/Jury.png}
    \caption{A \textscpop{Verdict} \textttpop{Pipeline} implementation of Jury. Creating an ensemble of models is as simple as specifying their names in a list. Individual \textttpop{Unit} results are aggregated via the \textttpop{MeanPoolUnit}.}
    \label{fig:jury}
\end{figure}

\subsection{G-Eval}
% G-Eval \cite{liu2023g} is an automated evaluation framework for evaluating the quality of generated text in terms of accuracy, coherence, relevance, and factual alignment. With \textscpop{Verdict}, the core functionality of G-Eval can be expressed concisely, showcasing its ability to support advanced evaluation protocols with a simple, declarative structure.

G-Eval is an automated evaluation framework for natural language generation (NLG) tasks that uses LLMs with chain-of-thought (CoT) reasoning and a form-filling paradigm \cite{liu2023g}. It was developed to assess the quality of NLG outputs, such as text summaries and dialogue responses, with better human alignment than previous methods. Indeed, G-Eval with GPT-4 achieves a Spearman correlation of 0.514 with human evaluations on summarization tasks.

\textscpop{Verdict} can implement the G-Eval \textttpop{Pipeline} using a \textttpop{CoTUnit}, \textttpop{JudgeUnit}, and \textttpop{MeanVariancePoolUnit}. Critically, both here and for all pipelines implemented with \textscpop{Verdict}, the input and output schemas of each \textttpop{Unit} are validated and enforced throughout the \textttpop{Pipeline}. This allows the ML practitioner to focus on core judging logic without needing to worry about the brittleness of message passing between LLMs. 

\begin{figure}[h!]
    \centering
    % \includegraphics[width=1\linewidth]{examples/G-Eval.png}
    \begin{mintedbox}{python}
scale = DiscreteScale((1, 5))
pipeline = Pipeline("GEval") \
    >> Layer(
        CoTUnit().prompt(f"""
            ### Generate evaluation steps for the following task:
            {TASK}
            ### Evaluation Criteria:
            {criteria_name} ({scale}) - {criteria_description}
            ### Evaluation Steps:
        """).via("gpt-4o", retries=3, temperature=0.6).pin() \
        # .pin(): run CoTUnit once and pass shared result to JudgeUnit across all samples

        >> JudgeUnit(scale).prompt(f"""
            {TASK}
            ### Evaluation Criteria:
            {criteria_name} ({scale}) - {criteria_description}
            ### Evaluation Steps:
            {{previous.thinking}}
            ### Source Text:
            {{source.source}}
            ### Summary:
            {{source.summary}}
            ### Evaluation Form (scores ONLY):
            - {criteria_name}:
        """).extract(WeightedSummedScoreExtractor()).via("gpt-4o-mini", retries=3, temperature=0.0)
    , repeat=5) \
    >> MeanVariancePoolUnit("score")
    \end{mintedbox}
    \caption{A \textscpop{Verdict} \textttpop{Pipeline} implementation of G-Eval using a \textttpop{CoTUnit}, \textttpop{JudgeUnit}, and \textttpop{MeanVariancePoolUnit}. Structure is enforced via the \textttpop{Scale} property, model parameters are easily managed on each \textttpop{Unit}, and the prompt can be flexibly defined inline.}
    \label{fig:g-eval}
\end{figure}

% Strong Results and Broad Applicability
\section{Strong Results \& Broad Applicability}
\textscpop{Verdict} systems are \textbf{powerful}, \textbf{reliable}, and \textbf{versatile}. 

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{verdict-tradeoff.png}
    \caption{A simple Verdict judge beats out reasoning models like \texttt{o1} (+9.28\%) on the ExpertQA evaluation task while only requiring a fraction of the cost and latency.}
    \label{fig:tradeoff}
\end{figure}

On the tasks of content moderation, factual and logical validation, and hallucination detection, \textscpop{Verdict} systems achieve SOTA or near-SOTA results. This is possible simply by specifying a high-level architectural design and, critically, does not require any bespoke prompt, formatting, or hyperparameter tuning. 

% Safety Moderation
\subsection{Safety Moderation}
We first demonstrate the power of \textscpop{Verdict} on XSTest, a test suite containing challenging, borderline prompts that induce False Positive and False Negative safety classifications by LLMs. XSTest comprises:
\begin{itemize}
    \item 250 safe prompts across ten prompt types that well-calibrated models should not refuse to comply with.
    \item 200 unsafe prompts as contrasts, which models should typically refuse for most applications
\end{itemize}

XSTest includes diverse prompt types, such as homonyms and figurative language, to test various aspects of model behavior. Each prompt is crafted as a single English sentence in question format to simulate dialogue. XSTest helps identify exaggerated safety behaviors where models refuse to respond to safe prompts, evaluates model calibration by testing responses to both safe and unsafe prompts, and highlights systematic content moderation failure modes across leading LLMs. Table \ref{tab:xstest} shows the results of the \textscpop{Verdict} judge vis-à-vis other judges. Our judge is a system consisting of one \textttpop{CoTUnit}, two \textttpop{JudgeUnits}, and one \textttpop{MeanPoolUnit}.

\begin{table}[t]
\caption{Results for \textscpop{Verdict} Judge vs. Other Judges on XSTest. The reported \textscpop{Verdict} system is a pipeline of \textttpop{CoTUnit} + \textttpop{JudgeUnits} + \textttpop{MeanPoolUnit}.}
\label{tab:xstest}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lc}
\toprule
Judge Model & XSTest Score \\
\midrule
\textscpop{Verdict} $\Rightarrow$ \textttpop{CoTUnit} + \textttpop{JudgeUnits} + \textttpop{MeanPoolUnit} & \textbf{96.44\%} \\
o1 & 96.00\% \\
GPT-4o & 96.00\% \\
o1 Mini & 95.56\% \\
WildGuard & 95.11\% \\
o1 Preview & 94.89\% \\
GPT-4o Mini & 93.33\% \\
Llama-Guard-3-8B & 90.44\% \\
Claude 3.5 Sonnet & 87.33\% \\
Claude 3.5 Haiku & 84.44\% \\
Llama-Guard-2-8B & 83.56\% \\
Claude 3 Opus & 80.89\% \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

Notably, our \textscpop{Verdict} judge achieves SOTA on XSTest, eclipsing even the o1 family of models. 

% We show the prompts we used for non-\textscpop{Verdict} models in Appendix X.


% Factual and Logical Correctness
\subsection{Factual and Logical Validation}
\textscpop{Verdict} is also useful for evaluating the factual and logical correctness of LLMs. In Table \ref{tab:combined_results}, we showcase how \textscpop{Verdict} judges can achieve competitive performance on JudgeBench, a benchmark designed to assess LLM-based judges on complex responses spanning knowledge, reasoning, math, and coding. Each example in JudgeBench consists of a response pair with corresponding preference labels reflecting \textit{objective correctness}. This is unlike previous benchmarks that implicitly emphasize instruction-following and style preferences. JudgeBench is a challenging benchmark---many frontier models perform only marginally better than random guessing. However, a \textscpop{Verdict} architecture performs well, only second to the \texttt{o1} models.  

\begin{table}[h]
\caption{Results for \textscpop{Verdict} Judge vs. Other Judges on JudgeBench. Our \textscpop{Verdict} system consists of 4 \textttpop{ArgumentUnits} that argue for and against each response in a response pair, and a \textttpop{JudgeUnit} that determines the final \textscpop{Verdict} based on these arguments. All \textttpop{Units} use GPT-4o.}
\label{tab:combined_results}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lc}
\toprule
Model & Overall Score \\
\midrule
\textsc{o1-preview} & \textbf{75.43\%} \\
\textsc{o1-mini} & 65.71\% \\
Claude-3.5-Sonnet & 64.29\% \\
\textscpop{Verdict} $\Rightarrow$ \textttpop{ArgumentUnits} + \textttpop{JudgeUnit} & 63.55\% \\
Llama-3.1-405B-Instruct & 56.86\% \\
GPT-4o & 56.57\% \\
Llama-3.1-70B-Instruct & 52.29\% \\
GPT-4o-mini & 50.00\% \\
Gemini-1.5-pro & 47.14\% \\
Llama-3.1-8B-Instruct & 40.86\% \\
Gemini-1.5-flash & 39.71\% \\
Claude-3-Haiku & 33.14\% \\

\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

% \begin{table*}[t]
% \caption{Results for \textscpop{Verdict} Judge vs. Other Judges on JudgeBench, split by the GPT-4o and Claude-3.5 subset. Our \textscpop{Verdict} system in both splits consists of 4 \textttpop{ArgumentUnits} that argue for and against each response in a response pair, and a \textttpop{JudgeUnit} that determines the final \textscpop{Verdict} based on these arguments. All \textttpop{Units} use GPT-4o.}
% \label{tab:combined_results}
% \vskip 0.15in
% \centering
% \begin{minipage}{0.48\textwidth}
%     \centering
%     \textit{\small JudgeBench GPT-4o Subset} \\
%     \vskip 0.1in
%     \begin{small}
%     \begin{sc}
%     \begin{tabular}{lc}
%     \toprule
%     Judge Model & Accuracy \\
%     \midrule
%     \textscpop{Verdict}            &  66.86\% \\
%     o1-Preview          & \textbf{75.43\%}         \\
%     o1-Mini           & 71.85\%         \\
%     GPT-4o             & 56.57\%         \\
%     GPT-4o-Mini        & 50.00\%         \\
%     Claude-3.5-Sonnet  & 64.29\%         \\
%     Prometheus2-8x7b    & 40.29\%         \\
%     JudgeLM-33B-v1.0   & 35.71\%         \\
%     \bottomrule
%     \end{tabular}
%     \end{sc}
%     \end{small}
% \end{minipage}
% \hfill
% \begin{minipage}{0.48\textwidth}
%     \centering
%     \textit{\small JudgeBench Claude-3.5 Subset} \\
%     \vskip 0.1in
%     \begin{small}
%     \begin{sc}
%     \begin{tabular}{lc}
%     \toprule
%     Judge Model & Accuracy \\
%     \midrule
%     \textscpop{Verdict}            &  \textbf{59.26\%} \\
%     GPT-4o            & 51.85\% \\
%     Llama-3.1-70B     & 45.19\% \\
%     Claude-3.5 Sonnet & 44.81\% \\
%     Llama-3.1-8B      & 36.67\% \\
%     Claude-3 Haiku  & 32.22\% \\
%     \bottomrule
%     \end{tabular}
%     \end{sc}
%     \end{small}
% \end{minipage}
% \vskip -0.1in
% \end{table*}


% \begin{table}[t]
% \caption{Results for \textscpop{Verdict} Judge vs. Other Judges on the HaluEval benchmark. Our \textscpop{Verdict} system is a \textttpop{JudgeUnit} with explanation (using \texttt{GPT-4o-Mini}) followed by an 3-way \textttpop{PairwiseJudgeUnit} (using \texttt{GPT 4o Mini}) that picks the most convincing explanation. We \textbf{bold} the best overall model and \underline{underline} the best prompting-only Judge. The Lynx family of models is introduced by \cite{ravi2024lynx} and we report their experimental results denoted by \textsuperscript{*}; \texttt{Flow-Judge-v0.1} in \cite{flowjudge2024}}
% \label{tab:halueval}
% \vskip 0.15in
% \begin{center}
% \begin{small}
% \begin{sc}
% \begin{tabular}{lcccr}
% \toprule
% Judge Model & Accuracy \\
% \midrule
% \textscpop{Verdict}                            & 86.3\%            \\
% GPT-4o\textsuperscript{*}                             & \underline{87.9}\%             \\
% GPT-4o-Mini                        & 85.8\%           \\
% Claude 3 Sonnet\textsuperscript{*}                    & 84.5\%             \\
% Claude 3 Haiku\textsuperscript{*}                     & 68.9\%             \\
% Llama-3-Instruct-8B\textsuperscript{*}                & 83.1\%             \\
% Llama-3-Instruct-70B\textsuperscript{*}               & 87.0\%             \\
% Lynx-v1.1-70B\textsuperscript{*}                      & 88.4\%             \\
% Lynx-v1.1-8B\textsuperscript{*}                       & 85.7\%             \\
% Lynx-v2.0-8B                       & \textbf{89.0\%}    \\
% Flow-Judge-v0.1                    & 85.4\%             \\
% \bottomrule
% \end{tabular}
% \end{sc}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table}

% XSUM
\begin{table}[t]
\caption{Results for our \textscpop{Verdict} Judge vs. Other Judges on the LLM-AggreFact ExpertQA split. Results are taken directly from the LLM-AggreFact leaderboard. Our \textscpop{Verdict} Judge is an ensemble of three instances of a \textttpop{JudgeUnit} with explanation followed by a self-verifier \textttpop{JudgeUnit}. The results of each instance are aggregated via a \textttpop{MaxPoolUnit}.}
\label{tab:expertqa}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccr}
\toprule
Judge Model & Accuracy \\
\midrule
\textscpop{Verdict} (GPT-4o) $\Rightarrow$ \textttpop{JudgeUnit} + \textttpop{JudgeUnit} + \textttpop{MaxPoolUnit} &\textbf{ 79.17\%} \\
o1 & 69.91\% \\
\textscpop{Verdict} (GPT-4o-Mini) $\Rightarrow$ \textttpop{JudgeUnit} + \textttpop{JudgeUnit} + \textttpop{MaxPoolUnit} & 67.72\% \\
GPT-4o & 64.67\% \\
o3-mini & 63.88\% \\
Claude-3.5 Sonnet & 60.9\% \\
Mistral-Large 2 & 60.8\% \\
Qwen2.5-72B-Instruct & 60.1\% \\
QwQ-32B-Preview & 60.0\% \\
GPT-4o-2024-05-13 & 59.6\% \\
Bespoke-Minicheck-7B & 59.2\% \\
MiniCheck-Flan-T5-L & 59.0\% \\
Llama-3.1-405B-Instruct & 58.5\% \\
Llama-3.3-70B-Instruct & 58.3\% \\
Tülu-3-70B & 55.7\% \\


\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}
% \footnotetext{\texttt{Bespoke-Minicheck-7b} is introduced as part of the AggreFact benchmark \cite{tang-etal-2024-minicheck}}

% Hallucination Detection
\subsection{Hallucination Detection}
Verdict systems set a new state-of-the-art (SOTA) in hallucination detection for expert-curated QA. Table \ref{tab:expertqa} compares a \textscpop{Verdict} system---a triplet ensemble of \textttpop{JudgeUnit} models verified by another \textttpop{JudgeUnit} and aggregated with a \textttpop{MaxPoolUnit}---against other judges on the ExpertQA dataset. 

The ExpertQA dataset is designed to evaluate the factuality of AI-generated responses in domain-specific contexts. It consists of 2,177 expert-curated questions across medicine, law, history, and engineering. The dataset includes: 
\begin{itemize}
    \item Expert-written questions that reflect real-world professional scenarios.
    \item Model-generated answers, evaluated by experts for factuality, attribution, and reliability.
    \item Expert-revised answers, ensuring factual correctness and alignment with credible sources.
\end{itemize}

An answer that is deemed not factual by an expert is considered to be a hallucination. Notably, our \textscpop{Verdict} system using \texttt{GPT-4o} outperforms the SOTA judge (\texttt{GPT-4o}) by +14.5\%. The same \textscpop{Verdict} system, using a weaker backbone of \texttt{GPT-4o-mini}, still outperforms \texttt{GPT-4o} by +3.05\%.

% that of models specifically tuned for this particular task, with no manual tuning. On RAGTruth, we outperform all models excluding those specifically tuned for the task of hallucination detection.

\section{Interlude: \textscpop{Verdict} Judges as Verifiers}

Ostensibly, Verdict judges are used for offline evaluation, but practically speaking Verdict judges can be used anywhere to replace human feedback and verification. Naturally, they apply to at least the following scenarios:

\begin{enumerate}
    \item \textbf{Automated Evaluation} of AI Applications. Verdict judges enable tailored and automated evaluation of AI applications.
    \item Run-Time \textbf{Guardrails}. Verdict judges are guardrails that sit on top of AI applications running in production.
    \item \textbf{Test-Time Compute Scaling}. Verdict judges are verifiers that help rank, prune, and select candidates during test-time compute scaling.
    \item \textbf{Reward Modeling} \& Reinforcement Learning. Verdict judges provide signal in reinforcement learning --- particularly in settings where rewards are not easily verifiable.
\end{enumerate}


Given the recent innovation around scaling inference-time compute, reinforcement learning, and reasoning models, it is prudent to point out that \textscpop{Verdict} judges can indeed be leveraged as \textit{verifiers} in these settings. \textscpop{Verdict} is especially well-suited for verification, given that \textscpop{Verdict} judges are:
\begin{enumerate}
    \item More \textbf{general} than fine-tuned reward models. \textscpop{Verdict} judges readily apply across different tasks and domains, as seen by our experiments on safety moderation, checking for factual and logical correctness, and hallucination detection. 
    \item More \textbf{stable} and reliable compared to simple LLM judges. \textscpop{Verdict} judges beat out all simple LLM judges (and fine-tuned evaluators), barring the \texttt{o1} models on JudgeBench, on the three tasks presented here.
    \item Capable of generating \textbf{soft rewards}, unlike formal verifiers. This is critical for extending reasoning models beyond verifiable domains like mathematics and programming.
    \item Relatively \textbf{low-latency} and \textbf{cost-efficient} compared to similarly powerful judges, which is necessary for methods leveraging heavy inference-time compute.
\end{enumerate}
 
\section{Conclusion}
We introduce \textscpop{Verdict}: a modular, expressive, and flexible approach to automated evaluation of LLM outputs. By enabling the composition of diverse reasoning units---such as verification, debate, and aggregation---\textscpop{Verdict} enhances the robustness, interpretability, and accuracy of LLM judges.

\textscpop{Verdict} judges achieve SOTA or near-SOTA across a wide range of challenging evaluation tasks, including safety moderation, factual and logical verification, and hallucination detection.  Notably, \textscpop{Verdict}-based judges---without any bespoke customization---surpass both 1) models that are fine-tuned specifically for each evaluation task, as well as 2) orders-of-magnitude larger judge models. This highlights Verdict’s potential as an efficient and scalable alternative for AI evaluation.

Beyond the immediate demonstrated performance gains, Verdict serves as a unified framework for scaling judge-time compute. We hope Verdict will enable researchers and practitioners to develop reliable, accurate, and scalable AI evaluators to advance our field into the next era of major progress.

\section*{Acknowledgments}
We would like to thank Omar Khattab, Julian Michael, Jon Saad-Falcon, William Brown, Hamel Husain, Eugene Yan, and Shi Feng for their discussion and insights on scalable oversight protocols, automated evaluation, and generative reward models. We would also like to thank the gracious staff at Siena Bakehouse for fueling our work with delectable coffee and pastries.


\bibliographystyle{plain}
\bibliography{references}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \appendix

% \section{Appendix / supplemental material}


% Optionally include supplemental material (complete proofs, additional experiments and plots) in appendix.
% All such materials \textbf{SHOULD be included in the main submission.}




\end{document}