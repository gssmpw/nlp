\section{Related Work}
\paragraph{Long Text Generation}
Recent research on long text generation has primarily focused on enhancing model performance____. A common approach involves constructing large-scale instruction-following datasets tailored for long-text generation and employing various optimization strategies to improve the capabilities of LLMs. 
% However, existing evaluations predominantly assess instruction-following ability without systematically analyzing performance across different aspects.
Beyond direct model training, plan-based methods have gained traction for long-text generation. LongWriter____ demonstrates that synthetic datasets, generated using a structured planning approach with GPT-4o, can effectively enhance LLMs' ability to produce extended text. Similarly, ____ propose a framework for generating survey papers section by section, while ____ employ a similar strategy to generate entire scientific articles. 
These studies suggest that structured generation methods can improve coherence and control over long-text outputs.

\paragraph{Long Context Understanding}
A key challenge in long-text generation is ensuring that LLMs effectively comprehend and utilize long contexts. Research in this area has focused on enhancing models' long-context understanding while extending their input length, leveraging their strong in-context learning capabilities____. These efforts primarily target tasks such as reading comprehension, where models extract relevant information from lengthy inputs, as exemplified by benchmarks like LongICLBench____, $\infty$BENCH____, and LonGLE____. Despite these advancements, prior work has largely overlooked the challenge of generating coherent and contextually consistent long-form text beyond mere retrieval or summarization.

\paragraph{Long Text Evaluation}
Evaluating long-form text remains an open challenge. HelloBench____ attempts to address this by selecting long-text samples of general tasks and evaluating LLMs through using direct generation method. Most existing evaluation frameworks rely on LLM-based scoring, but their robustness and reliability remain debated. As an alternative, ____ propose a reward model specifically designed for long-text evaluation. 
% Interestingly, contrary to prior studies, our experiments suggest that LLM-based evaluation can be robust when applied appropriately (see \S\ref{sec: sec_experiments_results}).

Additionally, several datasets have been developed to support long-text evaluation. Suri____ employs a plan-based approach and backtranslation____ to generate instructional texts, though its focus is primarily on creative writing and blogs rather than academic content. In contrast, ____ construct a long-text dataset based on Wikipedia and CommonCrawl, prioritizing direct text generation over structured planning. These studies highlight the need for high-quality datasets and evaluation metrics that account for both plan-based and direct-generation methods, particularly in domains requiring structured and coherent long-form outputs.

% \paragraph{Long Text Generation}

% Most work on long text generation focuses on improving model performance____.
% Specifically, these studies involve constructing large-scale instruction-following datasets for long texts and enhancing the long-text generation capabilities of LLMs through various methods. However, evaluations of long-text generation capabilities have mainly been limited to assessing the modelâ€™s ability to follow instructions, and have not systematically examined performance across different dimensions. 
% In addition to training models to directly generate long text, plan-based methods are also becoming popular. LongWriter____ constructs a dataset of 6000 texts using plan-based method with GPT4o, demonstrating that synthetic data can efficiently improve LLMs' long text generation ability.
% ____ design a framework to automatically generate surveys section by section. ____ also generates whole scientific paper section by section.


% \paragraph{Long Context Understanding}
% In order to evaluate the quality of long documents, LLMs need to have a strong understanding of long context. Improving their long-context understanding while extending the input length has been a key focus in research particularly due to the exceptional in-context learning capabilities of LLMs ____. However, these studies primarily focus on leveraging LLMs' long-context capabilities for tasks such as reading comprehension, where information is extracted from the context, as exemplified by benchmarks like LongICLBench____, $\infty$BENCH____, and LonGLE____. Consequently, previous studies have largely overlooked the potential of continued writing based on long contexts.

% \paragraph{Long Text Evaluation}

% Evaluating generated long-form text remains a challenge. HelloBench____ attempts to address this by providing task-specific long-text samples with distinct evaluation criteria. Most approaches for assessing long text output involve LLM-based scoring. 
% % However, some studies have indicated that using LLMs for assessing long text is challenging and lacks robustness. 
% As a robust alternative, ____ introduce a reward model specifically designed for long-text evaluation. Contrary to past research, however, our experiments indicate that LLMs can be robust when used for assessing and scoring long text (see \S\ref{sec: sec_experiments_results}).  
% Multiple datasets have been created for the purpose of enabling long-text evaluation, such as Suri____, which employs a planning-based approach and backtranslation____ to generate instructions but focuses on creative writing and blogs rather than academic texts. In contrast, ____ constructs a long-text dataset from Wikipedia and CommonCrawl but prioritizes direct generation over planning-based methods. As a result, there is a need for high-quality datasets and metrics for the evaluation of long text generation that also take into account planning-based approaches.



\begin{figure*}[!tb]
    \centering
    \includegraphics[width=0.99\textwidth]{figures/main.pdf}
    \caption{The Framework of our Long Text Generation method. Part (a) is the Plan-based method and part (b) is the Long Text Evaluation method.}
    \label{fig:framework}
\end{figure*}



\begin{table*}[h]
\centering
\footnotesize
\resizebox{1.95\columnwidth}{!}{
\begin{tabular}{l cccc}
\toprule
\multirow{2}{*}{\textbf{Benchmarks}} & \multicolumn{4}{c}{\textbf{Characteristics}} \\
\cmidrule(lr){2-5}
 & \textbf{Real Data} & \textbf{Plan Based} & \textbf{Domain Specific} & \textbf{Section \& Document Level} \\
\midrule
LongReward & \xmark & \xmark & \xmark & \xmark \\
LongWriter & \xmark & \cmark & \xmark & \xmark \\
HelloBench & \cmark & \xmark & \cmark & \xmark \\
LongEval (Ours) & \cmark & \cmark & \cmark & \cmark \\
\bottomrule
\end{tabular}
}
\caption{Comparison of different long-text generation benchmarks.}
\label{tab:method_comparison}
\end{table*}