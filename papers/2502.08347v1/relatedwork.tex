\section{Related Works}
\subsection{Masked image modeling}
Driven by BERT~\citep{beit}, masked image modeling (MIM) aims to remove or corrupt portions of the visual input and learn to predict the corrupted ones~\citep{inpaint, simmim, mae, spark, highlevel, cae, peco}. These approaches have been studied to reveal their ability to learn local attention patterns~\citep{lowlevel} and demonstrate better transferability to downstream tasks, such as segmentation and detection~\citep{mae, simmim, cmae, spark, hyspark}. The most representative of these methods is Masked Autoencoders (MAE)~\citep{mae}, which achieves efficient pre-training by dropping masked tokens. Although MAE uses an asymmetric design for reconstruction, the decoder still plays a significant role in reconstruction, limiting the quality of representation learning~\citep{cae}. Despite recent efforts~\citep{cae, peco} to address this issue, they overlook the importance of different layer informative anatomical representation learning in visual pre-training tasks~\citep{spark, hyspark} and struggle to balance both efficiency and representational capability. 

Given the importance of enhancing localized anatomical representation learning, the core idea of our approach is integrating hierarchical encoder-driven reconstruction into the decoding process, reducing the decoder's workload in reconstruction to compel the encoder to assume a greater role in the reconstruction task.

\begin{figure*}[ht!]
    \centering
    \includegraphics[width=0.95\linewidth]{framework_hi_end_mae.pdf}
    \vspace{-1mm}
    \caption{\textbf{The overall framework of Hi-End-MAE}. The Encoder-driven Dense Decoding architecture uses encoder representations to guide the decoder bottom-up dense reconstruction. The encoder (blue) is a Vision Transformer (ViT), which only processes the visible patches (blue cube). The decoder (green) incorporates a cross-attention mechanism, feeding in a full set of token \textit{i.e.} visible token (grey cube) and learnable masked token (mosaic cube) to query the encoder representation (blue arrow) for encoder-driven reconstruction.}
    \label{fig:framework}
    \vspace{-4mm}
\end{figure*}


\subsection{Self-supervised learning for medical imaging.}
Due to the scarcity of labeled medical images, self-supervised learning for medical images is a promising task~\citep{survey}. Existing medical SSL methods are mainly based on contrastive learning, conducted with strong data augmentation \textit{e.g.}, rotate~\citep{swinunetr, rotate} and multi-view crops~\citep{prlv2, unimiss, voco, gvsl, vox2vec}. However, most of these learn modality-specific high-level semantic representations~\citep{highlevel}, which introduce strong biases in downstream tasks with different data distributions~\citep{sslmia, bias}. In contrast, introducing MIM methods in medical image pre-training~\citep{mim, mae3d, hyspark} presents a promising avenue for addressing the above challenges~\citep{survey, mim}. However, most of these methods rarely prioritize the learning of anatomical semantics and downstream adaptation, both of which are crucial for medical visual tasks.

In this paper, we propose a simple yet effective MIM method. Unlike previous works, we emphasize the importance of enhancing the encoder's localized anatomical representation learning and downstream adaptability. By leveraging the encoderâ€™s localized anatomical representations for dense decoding, our approach not only minimizes the role of the decoder in reconstruction, unleashing the potential of the encoder in medical visual learning, but also enables seamless adaptation of the encoder to downstream tasks.