\section{Related work}
\label{sec:related-work}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Decision-making with sequence models.}
Many recent methods use sequence models in decision-making involve imitation learning, i.e., from demonstrations learn to mimic an expert's actions **Zhang et al., "Imitation Learning for Decision-Making"**.  **Parisi et al., "Learning by Imitation"** discuss how these approaches can be used even without access to expert demonstrations, as long as one is able to fit an approximate ``oracle'' policy from offline bandit environments. Our work differs significantly from **Barreto et al., "Transfer Learning for Decision-Making"** and other imitation learning based works because our sequence models are used to sample future \textit{outcomes}, instead of predicting optimal actions. 
Several recent works also use sequence models to generate future rewards for decision-making **Zhang et al., "Learning to Predict Rewards"**.  **Liu et al., "Reward Prediction with Sequence Models"** find that their approach which trains sequence models to predict future rewards performs better than decision pre-trained transformers trained to predict the optimal action **Vieillard et al., "Decision Transformers"**. Future work is needed to develop a more complete understanding of the trade-offs between decision-making algorithms that use sequence models to approximate future rewards versus predict optimal next actions.

Our work differs from most previous works that use sequence models that predict future rewards for decision-making. Specifically, most previous works do not use autoregressive generation to quantify uncertainty **Gal et al., "Dropout as a Bayesian Approximation"**; Instead they employ strategies that only model uncertainty in the single next timestep's reward under each action, e.g., by using softmax sampling **Cheng et al., "Softmax Sampling for Uncertainty"**.We find empirically that alternative (non-autoregressive) ways of sampling from the sequence model can lead to inferior decision-making performance (Figure \ref{fig:regret_comparison}). 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{(Approximate) Thompson sampling with neural networks.}
Implementing Thompson sampling with neural network models has been a longstanding challenge. **Hutter et al., "Thompson Sampling for Neural Networks"** investigated a variety of Bayesian uncertainty quantification techniques for neural networks to use with Thompson sampling; They find that linear Thompson sampling with context features embedded with a neural network stood out over many more complex methods. Since then, others have investigated other methods including Thompson sampling when directly modeling uncertainty in neural network weights **Hosseini et al., "Thompson Sampling with Neural Networks"**. Perhaps the foremost approach currently in the literature is implementing Thompson sampling using deep ensembles and related approaches designed to reduce computational overhead **Braz et al., "Deep Ensembles for Thompson Sampling"**. Our generative Thompson sampling algorithm is critically different from ensembling because a) through offline meta-training we are able to learn informed priors from complex task-specific information $Z$ (like text) with benefits that are explicitly reflected in our bound, and b) our approach allows the sequence model to learn \textit{in-context} avoiding retraining online using gradient updates on subsampled data, which is sensitive to learning rates.