\section{Experiment details}
\label{app:experiments}

\subsection{Data generating process}
\label{app:dgp}
\paragraph{Synthetic setting}
We evaluate our method on a synthetic contexutal bandit setting. The task features $Z$ for a given bandit task consist of one feature per action, i.e. $Z=\{Z^{(a)}\}_{a\in \mathcal A_{\tau}}$, where only $Z^{(a)}$ affects the reward for action $a$.
For simplicity, $R(y)=y$. 
For task $\tau$, action $a$, and timestep $t$, with action features $Z^{(a)}_{\tau}$, context features $X_t$, and unknown coefficients $U^{(a)}:=(U^{(a)}_{\rm const}, U^{(a)}_{Z}, U^{(a)}_{X}, U^{(a)}_{\rm cross})$, let $\sigma(w):=(1+\exp(-w))^{-1}$, and define
\begin{align}
\label{eq:synthetic_dgp}
&W^{(a)}_t = U^{(a)}_{\rm const} + U^{(a)}_{Z} Z^{(a)}+U^{(a)}_{X} X_t + X_t^\top U^{(a)}_{\rm cross} Z^{(a)}\nonumber\\
&\quad\quad\textrm{ where }\;Y^{(a)}_t\sim \textrm{Bernoulli}(\sigma(W^{(a)}_t)) \; \textrm{ i.i.d.}
\end{align}
All of the random variables above are generated i.i.d. for each task $\tau$. All of the random variables indexed by action $a$ above are also generated i.i.d. across actions $a\in\mathcal A_{\tau}$. 

We generate each $Z^{(a)}\sim N(0_2,I_2)$ and $X_t\sim N(0_5,I_5)$ as multivariate Gaussians. The unobserved coefficients are also drawn as multivariate Gaussians: $U_{\rm const}^{(a)}\sim N(0,1)$, $U_Z^{(a)}\sim N(1_2,I_2 \cdot 0.25^2)$,
$U_X^{(a)}\sim N(1_5,I_5 \cdot 0.25^2)$. The last coefficient
$U^{(a)}_{\rm cross}$ is drawn as a random diagonal matrix, where the diagonal entries are each drawn independently as i.i.d. $N(1,0.25^2)$. 

Unless otherwise specified, the training dataset consists of 10k independently drawn actions, and the validation set also consists of 10k independently drawn actions. For bandit evaluation, sets of 10 actions are drawn independently for each bandit environment. 



\paragraph{Semi-synthetic setting}
We extend our synthetic experiment setting to a semi-synthetic news recommendation setting in which we use text headlines $Z^{(a)}$ for arm $a$, so that the sequence model requires feature learning. 
We define\begin{align}
\label{eq:semisynthetic_dgp}
&W^{(a)}_t = U^{(a)}_{\rm const} + U^{(a)}_{Z} \phi_Z (Z^{(a)})+U^{(a)}_{X} \phi_X( X_t) + \phi_X (X_t)^\top U^{(a)}_{\rm cross} \phi_Z( Z^{(a)})\nonumber\\
&\quad\quad\textrm{ where }\;Y^{(a)}_t\sim \textrm{Bernoulli}(\sigma(W^{(a)}_t)) \; \textrm{ i.i.d.}
\end{align}
This is similar to the synthetic setting in Equation~\eqref{eq:synthetic_dgp}, except that the data-generating process uses $\phi_X(X_t)$ and $\phi_Z(Z^{(a)})$ instead of $X_t,Z^{(a)}$, respectively, where $\phi_X$ and $\phi_Z$ are nonlinear. This increases the difficulty of the learning task for the sequence model. The rest of the data generation is the same, aside from $Z^{(a)}$ being text headlines and using $\phi_X(X_t)$ and $\phi_Z(Z^{(a)})$, is the same. 

More specifically, the headlines $Z^{(a)}$ are sampled randomly (without replacement) from the MIND large dataset  \cite{wu2020mind} (training split only). The headlines are split into training, validation, and bandit evaluation sets, where headlines are disjoint between these three datasets. The training and validation sets are used to train and perform model selection for sequence models, and the bandit evaluation set is solely for evaluating regret. We generate one draw of one action (i.e. $W_t^{(a)}$) for each headline.  Unless otherwise specified, the training set has 20k headlines, validation has 10k, and the bandit set is everything left over, which is about 74k headlines. 

Additionally, $\phi_Z( Z^{(a)})$ is a two-dimensional vector, where the first dimension is the probability output of a pre-trained binary \cite{hfsentiment} evaluated on $Z^{(a)}$, and the second dimension is the probability output of 
a binary pre-trained formality classifier on $Z^{(a)}$ \cite{hfformality} with outputs normalized to have mean 0 and variance 1. Both models were obtained from huggingface.com.
Next, $\phi_X(X_t)$ is as follows: as $X_t\in \mathbb R^5$ as defined in the synthetic setting, $\phi_X(X)_{t,1:4}= X_{t,1:4}\cdot  \textrm{sign}(X_{t,5})$, i.e. $\phi_X$ multiplies the first four dimensions of $X_t$ by the sign of the fifth dimension. 


\subsection{Offline training}
\subsubsection{Resampling historical data}
\label{app:pretrain_bootstrap}
It is uncommon in to have access to all potential outcomes for all actions in realistic scenarios. 
Instead, it is more common to have access to the outcome corresponding to the action that was taken. Under the assumption that the contexts $X_t$ are exchangeable, and that the actions chosen historically were chosen at random, then for each action $a$, we can consider the contexts $X_t$ for timesteps $t$ for which this action was taken, and the corresponding outcomes $Y^{(a)}_t$. 
We assume that we have 1000 such timesteps per action. 
During training, in every epoch, we sample without replacement from this set of $(X_t, Y_t^{(a)})$'s to form a sequence of length 500; the sequence model $p_\theta$ is then trained on such sequences of data. 



\subsubsection{Sequence model architecture}
\label{app:sequence_models}
\begin{figure}[h]
\centering
\includegraphics[width=0.6\linewidth]{figures/architecture7.png}    
\caption{Diagram of model architecture for $p_\theta$, for semisynthetic settings. In synthetic settings, the model architecture is the same, except that it does not include the DistilBERT~\cite{sanh2019distilbert} encoder to process text, or the $X$ MLP encoder. }
\label{fig:architecture}
\end{figure}

\paragraph{Synthetic setting} In the synthetic setting, the model architecture is as follows: the output of $p_\theta$ is a final MLP head on top of a vector that is the concatenation of $Z^{(a)},X_t,$ and the summary statistics of the history for action $a$. The final MLP head has 3 layers, each with width 100. 

For simplicity, in the synthetic setting, we use sequence $p_\theta$ models that summarize recent history with summary statistics as follows. The summary statistics are $(\mathbf{X}^\top \mathbf{X}+\epsilon I)^{-1}$ and $\mathbf{X}^\top \mathbf{Y}$, where $\mathbf{X}$ denotes a matrix where each row is a past observation of $\mathbf{X}$, and $\mathbf{Y}$ is a vector where each element the corresponding past observation of $Y$. The hyperparameter $\epsilon$ is a value that is tuned during training, and we set it to $\epsilon=1$. 


\paragraph{Semisynthetic setting} In the semisynthetic setting, $p_\theta$ is implemented to take as input the part of the task feature for one action $a$, $Z^{(a)}$, along with history for that action, and context $X_t$, to predict the next reward given $X_t$ and for action $a$. As displayed in Figure~\ref{fig:architecture}, 
the model architecture is as follows. We concatenate a DistilBert \citep{sanh2019distilbert} embedding of headline $Z^{(a)}$ with $X_t$, and also summary statistics of the history that take in $Y_{1:t-1}^{(a)}$, as well as an MLP embedding of $X_{1:t-1}$ (2 layers, width 100); the sufficient statistics described above are  repeated 100 times. Then, this concatenated vector is fed into the final MLP head (3 layers, width 100). Finally, the output of the MLP is fed through a sigmoid function to obtain a prediction for the probability that the next outcome is 1 (rather than 0). 
The other difference from the synthetic setting is that the summary statistics feed the history of $X_t$'s into a 2-layer, width 100 MLP before calculating summary statistics. 




\subsubsection{Additional sequence model training details}
\label{app:poolactions}
\paragraph{Synthetic setting} We train (and validate) on sequences of length 500, sampled with replacement from historical sequences of length 1000, for 100 epochs. The training set has a pool of 10,000 actions (except for Figure~\ref{fig:loss_vs_regret}), and the validation set also has pool of 10,000 actions. Tasks are sampled/constructed by independently selecting 10 actions from the pool. We optimize weights with the AdamW optimizer. We try learning rates $\{0.1,0.01,0.001\}$ and choose the learning rate with the lowest validation loss, which is 0.01. We set weight decay to 0.01. The batch size is 500. 

\paragraph{Semi-synthetic setting} We train (and validate) on sequences of length 500, sampled with replacement from historical sequences of length 1000, for 40 epochs. 
The training set has a pool of 20,000 actions. The validation set has a pool of 10,000 actions. Tasks are sampled/constructed by independently selecting 10 actions from the pool. Aagin, we optimize weights with the AdamW optimizer. We try learning rates $\{0.1,0.01,0.001\}$ and choose the learning rate and also the training epoch with the lowest validation loss; the learning rate chosen is 0.01. We set weight decay to 0.01. The batch size is 500. We do not fine-tune the DistilBERT encoder and leave those weights as-is. 

\subsection{Online learning}

\subsubsection{TS-Gen details}
\label{app:more_generation}

Here we describe additional details used to draw potential outcomes tables $\hat{ \tau}$ 
by using a sequence model $p_\theta$. 

First, the $\{X_t\}_{1:T}$ on which we evaluate the bandit algorithm are assumed to be known at the beginning of bandit evaluations, but not known before that (i.e. the sequence model is not trained on the exact data used to evaluate the bandit algorithm). When the potential outcomes table $\hat{ \tau}$ is generated, this $\{X_t\}_{1:T}$ is fixed. Variations of this algorithm can be made where the $\{X_t\}_{1:T}$ seen in the bandit setting are \emph{not} known ahead of time, but we use this setting for simplicity. 


Here we describe additional details used to fit $\pi^*\in \Pi$ on $\hat{\tau}$. 
For all model classes used, there is no train/test split. The policy $\pi^*$ is fit directly on $\hat{\tau}$. This is the case for both TS-Gen, and for ``oracles''.  
For logistic methods, we use the default logistic regression implementation from \texttt{scikit-learn} \cite{scikit-learn}. 
For tree-based methods, we use gradient-boosted forests with maximum depth set to 2, also from \texttt{scikit-learn} \cite{scikit-learn}. 




\subsubsection{Baseline bandit methods}
\label{app:baseline_bandit_methods}
The first three (Greedy, Epsilon-Greedy, and Softmax) are alternative ways to make decisions using an existing pre-trained sequence model $p_\theta$. The others (Linear Thompson Sampling, LinUCB) are contextual bandit methods that do not use $p_\theta$. 


\paragraph{Greedy}
We use a trained sequence model $p_\theta$ as in TS-Gen. 
In the online step, at time $t$, instead of using $p_\theta$ to generate the entire potential outcomes table $\hat{\tau}$, fit a policy $\pi^*$, and then evaluate this policy at the current context $X_t$, we just evaluate $p_\theta$ on the current task (action) features $Z^{(a)}$, history $\mathcal H_t$, and current context $X_t$. In our setting, this gives the probability that the corresponding outcome (and also reward, as $R(y)=y$) $Y_t^{(a)}$ is 1. We choose the arm with the largest such probability. 



\paragraph{Epsilon-Greedy}
The version of epsilon-greedy that we present here differs from the classical version of epsilon-greedy \citep{sutton2018reinforcement}. In our version, we choose according to the greedy algorithm (as described above, using the trained sequence model to obtain predicted rewards) with probability $1-\epsilon$, and choose a uniformly random action with probability $\epsilon$. We use $\epsilon=0.1$ in the experiments. 

\paragraph{Softmax}
This method is similar to PreDeToR-$\tau$ in \citet{mukherjee2024pretraining}. PreDeToR-$\tau$ also uses a trained sequence model to predict rewards, and chooses actions with probability 
according to softmax of the rewards for each arm, multiplied by a constant $\tau$. 
In other words, if $\hat {\textbf {r}}=\hat r_1,\ldots,\hat r_{\mathcal A_{\tau}}$ are the predicted rewards from a pre-trained model for actions $1,\ldots,\mathcal A_{\tau}$, then we choose an action with probability 
$\textrm{softmax}( \hat{\textbf r}/\tau)$. 
We set $\tau=0.05$ as in \citet{mukherjee2024pretraining}.

\paragraph{Linear Thompson Sampling}
For each arm $a\in\mathcal A_{\tau}$, outcomes are modeled as a linear function of $X$,
$$Y=X\beta + \epsilon,$$
where $\beta$ is modeled as a multivariate Gaussian prior with mean 0 and identity variance, and $\epsilon$ is modeled as a Gaussian prior with mean 0 and variance 1/4 (since the maximum variance of a Bernoulli is 1/4). 
After $t$ timesteps, we use the history $\mathcal H_t$ to calculate the posterior distributions for $\beta$ and $\epsilon$, for each arm $a$. 
Then, we do Thompson sampling: for each arm $a$, we sample once from the posteriors of $\beta$ and $\epsilon$, and calculate what $Y$ should be, given the current context $X_t$. We choose the arm with the largest such value. 
Note that unlike TS-Gen, linear Thompson sampling does not learn a rich and flexible prior based on task features $Z_\tau$. 

\paragraph{LinUCB}
We implement LinUCB-disjoint in \citep{li2010contextual}, on contexts $X_t$. We set $\alpha=0.1$ as it performs well in comparison to a small set of other values tried (\{0.1,1,2\}). 
In this particular setting, the task features are different for each action, there are few actions in each environment, and the arms are generated independently, so it is appropriate to exclude task features from the on-line modeling for LinUCB. 
Note that unlike TS-Gen, LinUCB does not learn a rich and flexible prior based on task features $Z_\tau$. 


\subsection{Sequence loss vs. regret under TS-Gen (Figure \ref{fig:loss_vs_regret})}
\label{app:seqloss}
We examine the relationship between sequence model loss $\ell(p_\theta)$ and regret of TS-Gen using $p_\theta$. Our Theorem \ref{thm:psarRegret} suggests that the lower the loss of a sequence model $p_\theta$ the lower the regret of TS-Gen using that sequence model $p_\theta$. We examine this by varying the amount of training tasks used to learn $p_\theta$ and thus obtain sequence models with different losses. We also compute the cumulative regret for TS-Gen using each respective sequence models. Indeed, in Figure~\ref{fig:loss_vs_regret}, models trained on more data tend to have lower sequence loss, which tend to have lower regret. 
\label{sec:loss_vs_regret}
\begin{figure}[h]
    \centering
\includegraphics[width=0.415\linewidth]{figures/loss_for_data_sizes.png}
\includegraphics[width=0.4\linewidth]{figures/regret_for_data_sizes.png}
\caption{\bo{Sequence loss vs. bandit regret:} 
We demonstrate the relationship between sequence loss and regret for TS-Gen by pre-training our sequence models offline on varying dataset sizes in the semisynthetic setting. As training dataset sizes are smaller, sequence loss (left) is higher (worse), and bandit regret (right) is higher (worse). ``Training rows'' refers to the number of actions used in the pool of actions to select from to form tasks (Appendix \ref{app:poolactions}).
\bo{(Left)}: Prediction loss by timestep. We plot an empirical estimate of the per-timestep (non-cumulative) loss from \eqref{eq:pop_loss} by evaluating our sequence models on an held-out validation set.
%On the horizontal axis is the number of previous observations in $\mathcal H_t$ for a given action to condition on. On the vertical axis is cross-entropy loss for $p_\theta$. If the true probability of the outcome is 1 is $q$ and the predicted probability is $p$, then we display $-q\log(p)-(1-q)\log(1-p)$. 
Error bars represent $\pm 1$ s.e. % averaged over 80,000 evaluation actions (10,000 from the validation set, plus 70,000 additional independently sampled actions). 
\bo{(Right)}: Cumulative regret for TS-Gen using the corresponding sequence models, with logistic policy class, and relative to the logistic ``oracle''. Error bars represent $\pm 1$ s.e. averaged over 500 re-drawn bandit environments.}
\label{fig:loss_vs_regret}
\end{figure}


\subsection{Policy class for TS-Gen (Figure \ref{fig:semisynthetic_policy_class_comparison})}
\label{sec:policy_class}
The choice of policy class affects both TS-Gen (for a fixed sequence model $p_\theta$), as well as the ``oracle''; See Figure \ref{fig:semisynthetic_policy_class_comparison}. In the semisynthetic setting, TS-Gen has moderately greater reward using a tree-based policy than a logistic policy. In contrast, the ``oracle'' using a tree-based policy is much better than the ``oracle'' using a logistic policy. 

\begin{figure}[h]
\centering\includegraphics[width=0.4\linewidth]{figures/semisynthetic_policy_class.png}
\centering\includegraphics[width=0.4\linewidth]{figures/semisynthetic_policy_class_zoomed.png}
\caption{Varying policy classes in the semisynthetic setting. The same experimental results are plotted on the left and the right. The plot on the right calculates regret relative to the logistic ``oracle'', while the left calculates regret relative to the tree-based ``oracle''. 
Error bars are $\pm 1$ s.e. across 500 bandit environments.}
\label{fig:semisynthetic_policy_class_comparison}
\end{figure}