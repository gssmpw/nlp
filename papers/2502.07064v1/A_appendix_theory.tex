
\section{Theory}

\subsection{Relationship to Thompson sampling with misspecified priors and lower bounds.}
\label{app:misspecifiedPriors}

In our per-period regret bound from Theorem \ref{thm:psarRegret}, the ``penalty'' for using a suboptimal sequence model $p_\theta$ does not vanish as $T$ grows. 
Since frequentist regret for Thompson sampling do not incorporate such non-vanishing terms, one might interpret this as indicating our result is not tight. This interpretation is significantly mistaken. Standard frequentist regret bounds for Thompson sampling \textit{critically} assume diffuse, non-informative priors \citep{agrawal2012analysis,agrawal2013thompson}, which ensure that each arm is explored sufficiently. It turns out that Thompson Sampling can be highly sensitive to misspecification in the prior, especially if under the prior the probability of the optimal action being the best is too low, so the algorithm has a high probability of under exploring the best action. Specifically, previous work has shown that the per period regret may be non-vanishing for a worst case environment and choice of prior \citep{liu2016prior,simchowitz2021bayesian}. Additionally, \citet{psar2024} show that for a multi-arm (non-contextual) bandit version of the generative Thompson sampling algorithm that the penalty for using an imperfect sequence model depends on $\sqrt{\ell(p_\theta) - \ell(p^*)}$ in a way that is in general unavoidable.



\subsection{Notation}
We first introduce some notation we use throughout this Appendix.
\begin{itemize}
    \item Recall that by definition 
    \begin{align*}
        \Delta \big( \psar(p_\theta ) \big) = \E_{\psar(p_\theta )} \left[  \frac{1}{T} \sum_{t=1}^{T} R \big( Y_t^{(\pi^*(X_t; \tau))} \big) - \frac{1}{T} \sum_{t=1}^{T} R \big( Y_{t}^{(A_t)} \big)\right].
    \end{align*}
    Throughout proof, we will omit the $\psar(p_\theta )$ subscript on the expectation, i.e., we use $\E \left[ \cdotspace \right] := \E_{\psar(p_\theta )} \left[ \cdotspace \right]$. 
    \item Additionally, throughout this proof we use $\E_t$ to denote expectations conditional on $\HH_t$ and $X_{1:T}$, i.e., we use 
    \begin{align}
        \label{eqn:EtDefinition}
        \E_t \left[ \cdotspace \right] = \E \left[ \cdotspace \mid \HH_t, X_{1:T} \right].
    \end{align}
    Note that this means $\E_1\left[ \cdotspace \right] = \E \left[ \cdotspace \mid \HH_1, X_{1:T} \right]= \E \left[ \cdotspace \mid Z, X_{1:T} \right]$.
    \item We use $H(Y)$ to denote the entropy of a discrete random variable $Y$, i.e., $H(Y) = \sum_y \PP(Y = y) \log \PP(Y = y) dy$. We also use $H_t(Y) = H(Y \mid \HH_t, X_{1:T})$ to denote the entropy of $Y$ conditional on $\HH_t$ and $X_{1:T}$; Note that is standard in information theory, $H_t(Y)$ \textit{is not} a random variable, rather, it marginalizes over $\HH_t$ and $X_{1:T}$:
    \begin{align*}
        H_t(Y) := 
        H(Y \mid \HH_t, X_{1:T}) = \E \left[ \sum_y \PP(Y = y \mid \HH_t, X_{1:T}) \log \PP(Y = y \mid \HH_t, X_{1:T}) dy \right];
    \end{align*}
    Above, the outer expectation marginalizes over the history $\HH_t$ and $X_{1:T}$.
    \item We also use $I(Z;Y)$ to denote the mutual information between some random variables $Z$ and $Y$, i.e., $I(Z;Y) = \int_z \int_y \PP(Z = z, Y=y) \log \frac{\PP(Z = z, Y=y)}{\PP(Z = z) \PP(Y=y)} dz dy$. We further use $I_t(Z; Y)$ to denote the mutual information between $Z$ and $Y$ conditional on $\HH_t$ and $X_{1:T}$ (where again we marginalize over $\HH_t$ and $X_{1:T}$), i.e.,
    \begin{align}
        &I_t(Z;Y) := I(Z;Y \mid \HH_t, X_{1:T}) \nonumber \\
        &= \E \left[ \int_z \int_y \PP(Z = z, Y=y \mid \HH_t, X_{1:T}) \log \frac{\PP(Z = z, Y=y\mid \HH_t, X_{1:T})}{\PP(Z = z\mid \HH_t, X_{1:T}) \PP(Y=y\mid \HH_t, X_{1:T})} dx dy \right];
        \label{eqn:mutualInfoDef}
    \end{align}
    Above, the outer expectation marginalizes over the history $\HH_t$ and $X_{1:T}$. 
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{VC Dimension}
\label{app:VCdim}

\begin{customlemma}{\ref{lemma:VC}}[VC dimension bound on entropy]
    For any binary\footnote{Note, VC-dimension is defined for binary functions.} action policy class $\Pi^*$, 
    \begin{align*}
        H \big( \piX \mid Z_{\tau}, X_{1:T} \big)
        \leq H \big( \piX \mid X_{1:T} \big)
        = O\big( \TN{VCdim}(\Pi^*) \log T \big).
    \end{align*}
\end{customlemma}

\begin{proof}
The first inequality $H \big( \piX \mid Z_{\tau}, X_{1:T} \big) \leq H \big( \piX \mid X_{1:T} \big)$ holds by the chain rule for entropy.

The second big O equality result holds by the Sauer-Shelah lemma \citep{sauer1972density,shelah1972combinatorial}. Specifically, by the Sauer-Shelah lemma if a function class has VC dimension $k$, then that function class can produce most $\sum_{i=0}^k {T\choose i} = O(T^{k})$ different labelings of any $T$ points. Thus, since a coarse upper bound on the entropy of a random variable is the log of the number of unique values that variable can take, we get that $H \big( \piX \mid X_{1:T} \big) \leq \log \sum_{i=0}^{\TN{VCdim}(\Pi^*)} {T\choose i} = O\big( \TN{VCdim}(\Pi^*) \log T \big)$.
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Lemma \ref{lemma:lossdecomp}: To minimize loss $p_\theta$ needs to approximate $p^*$.}
The next lemma is a standard result connecting the excess expected loss of a sequence model $p_{\theta}$ to its KL divergence from the true sequence model $p^*$. Recall, the expected loss of a sequence model $p_\theta$ is denoted $\ell(p_\theta)$, defined in \eqref{eq:pop_loss}. To (nearly) minimize loss, $p_\theta$ the learner needs to closely approximate the true sequence model $p^*$.
\begin{lemma}[Decomposing loss under $p_\theta$]
    \label{lemma:lossdecomp}
    Under Assumptions \ref{assump:contextualBandit} and \ref{assump:context}, for the loss $\ell$ as defined in \eqref{eq:pop_loss},
    \[
    \ell(p_{\theta}) = \ell(p^*) + |\MC{A}_{\tau}| \cdot \E \left[D_{\rm KL}\left( p^* \big( Y_{1:T}^{(a)} \mid Z_{\tau}, X_{1:T} \big) \; \Big\| \; p_{\theta} \big( Y_{1:T}^{(a)} \mid Z_{\tau}, X_{1:T} \big) \right) \right]. 
    \]
\end{lemma}
\begin{proof}
By the definition of the expected loss in  \eqref{eq:pop_loss},
\begin{align*}
    \ell(p_\theta) - \ell(p^*) &= \E \left[ - \sum_{a \in \MC{A}_{\tau}} \sum_{t=1}^T \log p_{\theta} \big( Y_t^{(a)} \mid Z_{\tau}, X_{1:t}, Y_{1:t-1}^{(a)} \big) \right] 
    - \left[ - \sum_{a \in \MC{A}_{\tau}} \sum_{t=1}^T \log p^* \big( Y_t^{(a)} \mid Z_{\tau}, X_{1:t}, Y_{1:t-1}^{(a)} \big) \right] \\
    &\underbrace{=}_{(a)} - |\MC{A}_{\tau}| \cdot \E \left[ \sum_{t=1}^T \left\{ \log p_{\theta} \big( Y_t^{(a)} \mid Z_{\tau}, X_{1:t}, Y_{1:t-1}^{(a)} \big) - \log p^* \big( Y_t^{(a)} \mid Z_{\tau}, X_{1:t}, Y_{1:t-1}^{(a)} \big) \right\} \right] \\
    &\underbrace{=}_{(b)} |\MC{A}_{\tau}| \cdot \sum_{t=1}^T \E \left[ \dkl{p^* \left( Y_t^{(a)} \mid Z_{\tau}, X_{1:t}, Y_{1:t-1}^{(a)} \right)}{p_\theta \left( Y_t^{(a)} \mid Z_{\tau}, X_{1:t}, Y_{1:t-1}^{(a)} \right)} \right] 
\end{align*}
\begin{align*}
    &\underbrace{=}_{(c)} |\MC{A}_{\tau}| \cdot \sum_{t=1}^T \E \left[ \dkl{p^* \left( Y_t^{(a)} \mid Z_{\tau}, X_{1:t}, Y_{1:t-1}^{(a)} \right)}{p_\theta \left( Y_t^{(a)} \mid Z_{\tau}, X_{1:t}, Y_{1:t-1}^{(a)} \right) } \right] \\
    &\quad + |\MC{A}_{\tau}| \cdot \sum_{t=1}^T \underbrace{ \E \left[ \dkl{p^* \left( X_t \mid Z_{\tau}, X_{1:t-1}, Y_{1:t-1}^{(a)} \right)}{p_\theta \left( X_t \mid Z_{\tau}, X_{1:t-1}, Y_{1:t-1}^{(a)} \right)} \right] }_{=0} \\
    &\underbrace{=}_{(d)} |\MC{A}_{\tau}| \cdot \E \left[ \dkl{p^* \big( Y_{1:T}^{(a)}, X_{1:T} \mid Z_{\tau} \big)}{p_\theta \big( Y_{1:T}^{(a)}, X_{1:T} \mid Z_{\tau} \big)} \right] \\
    &\underbrace{=}_{(e)} |\MC{A}_{\tau}| \cdot \E \left[ \dkl{p^* \left( Y_{1:T}^{(a)}\mid Z_{\tau}, X_{1:T} \right)}{p_\theta \left( Y_{1:T}^{(a)}, \mid Z_{\tau}, X_{1:T}  \right)} \right]
\end{align*}
Above, equality (a) uses Assumption \ref{assump:indepAction} (Independence across actions). Equality (b) uses the definition of KL divergence. Equality (c) uses Assumption \ref{assump:contextualBandit} (Contextual bandit environment) and Assumption \ref{assump:context} (Context distribution is known). Equality (d) uses the chain rule for KL divergence. Equality (e) holds by the chain rule for KL divergence and since $\E \left[ \dkl{p^* \left( X_{1:T}\mid Z_{\tau} \right)}{p_\theta \left( X_{1:T} \mid Z_{\tau} \right)} \right] = 0$ by Assumption \ref{assump:context} (Context distribution is known).
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Lemma \ref{lemma:AAbarKLNew}: Action selection under perfect vs. imperfect imputation models.} 
\begin{lemma}[KL Divergence in next action distribution]
\label{lemma:AAbarKLNew}
Under Assumption \ref{assump:context}, for any $t$,
\begin{align*}
    \E \big[ \dkl{\PP_t \left(\pi^*(X_t; \tau) = \cdot \right)}{\PP_t \left(A_t = \cdot \right)} \big]
    \leq |\MC{A}_{\tau}| \cdot \{ \ell(p_{\theta}) - \ell(p^*) \}.
\end{align*}
\end{lemma}

\begin{proof}
Note the following:
\begin{align*}
    &\E \big[ \dkl{\PP_t \left(\pi^*(X_t; \tau) = \cdot \right)}{\PP_t \left(A_t = \cdot \right)} \big] \\
    &\underbrace{\leq}_{(a)} \E \left[
    \dkl{\PP_{p^*} \left(\{Y_{1:T}^{(a)}\}_{a \in \MC{A}_{\tau}} \mid X_{1:T}, \HH_t \right)}{\PP_{p_{\theta}}\left( \{Y_{1:T}^{(a)}\}_{a \in \MC{A}_{\tau}} \mid X_{1:T}, \HH_t \right)} \right] \\
    &\underbrace{\leq}_{(b)} \E \left[
    \dkl{\PP_{p^*} \left(\{Y_{1:T}^{(a)}\}_{a \in \MC{A}_{\tau}} \mid X_{1:T}, Z_{\tau} \right)}{\PP_{p_{\theta}}\left( \{Y_{1:T}^{(a)}\}_{a \in \MC{A}_{\tau}} \mid X_{1:T}, Z_{\tau} \right)} \right] \\
    &\underbrace{\leq}_{(c)} |\MC{A}_{\tau}| \cdot \E \left[
    \dkl{\PP_{p^*} \left( Y_{1:T}^{(a)} \mid X_{1:T}, Z_{\tau} \right)}{\PP_{p_{\theta}}\left( Y_{1:T}^{(a)} \mid X_{1:T}, Z_{\tau} \right)} \right] 
    \underbrace{\leq}_{(d)} \left\{ \ell(p_{\theta}) - \ell(p^*) \right\}.
\end{align*}
Above,
\begin{itemize}
    \item Inequality (a) holds because $\pi^*(X_t; \tau)$ and $A_t$ are both are derived by applying the same function to the outcomes $\{Y_{1:T}^{(a)}\}_{a \in \MC{A}_{\tau}}$.
    \item Inequality (b) holds because by the chain rule for KL divergence, 
    \begin{align*}
        &\dkl{\PP_{p^*} \left(\{Y_{1:T}^{(a)}\}_{a \in \MC{A}_{\tau}} \mid X_{1:T}, Z_{\tau} \right)}{\PP_{p_{\theta}}\left( \{Y_{1:T}^{(a)}\}_{a \in \MC{A}_{\tau}} \mid X_{1:T}, Z_{\tau} \right)} \\
        &= \dkl{\PP_{p^*} \left(\{Y_{1:T}^{(a)}\}_{a \in \MC{A}_{\tau}} \mid X_{1:T}, \HH_t \right)}{\PP_{p_{\theta}}\left( \{Y_{1:T}^{(a)}\}_{a \in \MC{A}_{\tau}} \mid X_{1:T}, \HH_t \right)} \\
        &+ \dkl{\PP_{p^*} \left( \HH_t \mid X_{1:T}, Z_{\tau} \right)}{\PP_{p_{\theta}}\left( \HH_t \mid X_{1:T}, Z_{\tau} \right)},
    \end{align*}
    and the KL divergence is non-negative.
    \item Inequality (c) holds by Assumption \ref{assump:indepAction} (Independence across actions).
    \item Inequality (d) holds by Lemma \ref{lemma:lossdecomp} (Decomposing loss under $p_\theta$).
\end{itemize}
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Lemma \ref{lemma:mutualInfoNew}: Mutual information equivalency.} 
\begin{lemma}[Mutual information equivalency]
\label{lemma:mutualInfoNew}
\begin{multline*}
    I_t \big( \pi^*(X_t; \tau); ( Y_t^{(A_t)}, A_t ) \big) \\
    = \E \bigg[ \sum_{a, A \in \MC{A}_{\tau}} \PP_t \big( A_t = a \big) \PP_t \big( \pi^*(X_t ; \tau) = A \big) \cdot \dkl{ \PP_t \big( Y_t^{(a)} \mid \pi^*(X_t; \tau) = A \big) }{ \PP_t \big( Y_t^{(a)} \big) } \bigg]
\end{multline*}
\end{lemma}

\begin{proof}
Note that
\begin{align*}
    &I_t \big( \pi^*(X_t; \tau); ( Y_t^{(A_t)}, A_t ) \big)
    \underbrace{=}_{(a)} I_t \big( \pi^*(X_t; \tau); Y_t^{(A_t)} \mid A_t \big) \\
    &\underbrace{=}_{(b)} \E \bigg[ \sum_{a \in \MC{A}_{\tau}} \PP_t \big( A_t = a \big) I_t \big( \pi^*(X_t; \tau); Y_t^{(a)} \mid A_t = a \big) \bigg] \\
    &\underbrace{=}_{(c)} \E \bigg[ \sum_{a \in \MC{A}_{\tau}} \PP_t \big( A_t = a \big) I_t \big( \pi^*(X_t; \tau); Y_t^{(a)} \big) \bigg] \\
    &\underbrace{=}_{(d)} \E \bigg[ \sum_{a \in \MC{A}_{\tau}} \PP_t \big( A_t = a \big) \sum_{A \in \MC{A}_{\tau}} \PP_t \big( \pi^*(X_t; \tau) = A \big) \dkl{\PP_t \big( Y_t^{(a)} \mid \pi^*(X_t; \tau) = A \big)}{\PP_t \big( Y_t^{(a)}\big)} \bigg].
\end{align*}
Above, equality (a) holds since  $\pi^*(X_t; \tau)$ and $A_t$ are independent conditional on $\HH_{t}, X_{1:T}$. 
Equality (b) holds by the definition of conditional mutual information.
Equality (c) holds because $Y_t^{(a)}$ and $\pi^*(X_t; \tau)$ are independent of $A_t$ conditional on $\HH_{t}, X_{1:T}$.
Equality (d) holds by the KL divergence form of mutual information.
\end{proof}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Lemma \ref{lemma:boundingMI}: Bounding sum of mutual information terms.}
\begin{lemma}[Bounding sum of mutual information terms]
    \label{lemma:boundingMI}
    \[
    \sum_{t=1}^T I_t \left( \piX; (A_t, Y_t^{(A_t)}) \right)
    \leq H \big( \piX \mid Z_{\tau}, X_{1:T} \big). 
    \]
\end{lemma}
\begin{proof}
\begin{align*}
    \sum_{t=1}^T I_t \left( \piX; (A_t, Y_t^{(A_t)}) \right)
    &\underbrace{=}_{(i)} I_1 \big( \piX; ( Y_t^{(A_t)}, A_t )_{t=1}^T \big) \\
    &\underbrace{=}_{(ii)} H_1(\piX) - H_1(\piX \mid ( Y_t^{(A_t)}, A_t )_{t=1}^T) \\
    &\underbrace{\leq}_{(iii)} H_1(\piX) \underbrace{=}_{(iv} H \big( \piX \mid Z_{\tau}, X_{1:T} \big).
\end{align*}
Above, equality (i) holds by the chain rule for mutual information. Equality (ii) holds by the entropy formulation of mutual information. Equality (iii) holds since the entropy is always non-negative. Equality (iv) holds by the definition of $H_1$, which is the entropy conditional on $\HH_1 = \{ Z_{\tau} \}$ and $X_{1:T}$.
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proof of Theorem \ref{thm:psarRegret}}
\begin{customthm}{\ref{thm:psarRegret}}[TS-Gen regret bound]
We use $\ell$ as defined in \eqref{eq:pop_loss}. Under Assumptions \ref{assump:contextualBandit}, \ref{assump:indepAction}, and \ref{assump:context}, the regret of the TS-Gen algorithm is bounded as follows:
    \begin{align*}
      \Delta \big( \psar(p_\theta) \big)
      & \leq 
        \underbrace{ \sqrt{ \frac{ |\MC{A}_{\tau}| \cdot H \big( \piX \mid Z, X_{1:T} \big) }{2 T} } }_{\TN{Regret bound for Thompson sampling}} 
        + \underbrace{ \sqrt{ 2 \big\{ \ell(p_\theta) - \ell(p^*) \big\} } }_{\TN{Penalty for sub-optimal prediction}}.
    \end{align*}
\end{customthm}
Recall from \eqref{eqn:piXdef} that $\piX := \{ \pi^*(X_t; \tau) \}_{t=1}^T$.

\begin{proof}
Note that by the law of iterated expectations,
\begin{align*}
    \Delta(\psar) 
    = \E \left[ \frac{1}{T} \sum_{t=1}^T R(Y_{t}^{(\pi^*(X_t; \tau))}) - R(Y_{t}^{(A_t)}) \right]
    = \E \left[ \frac{1}{T} \sum_{t=1}^T \E_t \left[ R(Y_{t}^{(\pi^*(X_t; \tau))}) - R(Y_{t}^{(A_t)}) \right] \right].
\end{align*}
Consider the following for any $t \in [1 \colon T]$:
\begin{align*}
  &\E_t \left[ R(Y_{t}^{(\pi^*(X_t; \tau))}) - R(Y_{t}^{(A_t)}) \right] \\
  &= \sum_{a \in \MC{A}_{\tau}} \PP_t(\pi^*(X_t; \tau) = a) \cdot \E_t \big[ R \big( Y_t^{(a)} \big) \mid \pi^*(X_t; \tau) = a \big]
  + \sum_{a \in \MC{A}_{\tau}} \PP_t(A_t = a) \cdot \E_t \big[ R \big( Y_t^{(a)} \big) \mid A_t = a \big] \\
  &\underbrace{=}_{(i)} \sum_{a \in \MC{A}_{\tau}} \PP_t(\pi^*(X_t; \tau) = a) \cdot \E_t \big[ R \big( Y_t^{(a)} \big) \mid \pi^*(X_t; \tau) = a \big]
  + \sum_{a \in \MC{A}_{\tau}} \PP_t(A_t = a) \cdot \E_t \big[ R \big( Y_t^{(a)} \big) \big] \\
  &= \sum_{a \in \MC{A}_{\tau}} \sqrt{ \PP_t(\pi^*(X_t; \tau) = a) \PP_t(A_t = a) } \left( \E_t \big[ R \big( Y_t^{(a)} \big) \mid \pi^*(X_t; \tau) = a \big]
  - \E_t \big[ R \big( Y_t^{(a)} \big) \big] \right) \\
  &+ \sum_{a \in \MC{A}_{\tau}} \left( \sqrt{ \PP_t(\pi^*(X_t; \tau) = a) } - \sqrt{ \PP_t(A_t = a) } \right) \\
  &\qquad \qquad \qquad \left( \sqrt{ \PP_t(\pi^*(X_t; \tau) = a) } \E_t \big[ R \big( Y_t^{(a)} \big) \mid \pi^*(X_t; \tau) = a \big]
  + \sqrt{ \PP_t(A_t = a) } \E_t \big[ R \big( Y_t^{(a)} \big) \big] \right) \\
 &\underbrace{\leq}_{(ii)} \sqrt{ |\MC{A}_{\tau}| \sum_{a \in \MC{A}_{\tau}} \PP_t(\pi^*(X_t; \tau) = a) \PP_t(A_t = a) \left( \E_t \big[ R \big( Y_t^{(a)} \big) \mid \pi^*(X_t; \tau) = a \big]
  - \E_t \big[ R \big( Y_t^{(a)} \big) \big] \right)^2 } \\
  &\qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad + \sum_{a \in \MC{A}_{\tau}} \left| \PP_t(\pi^*(X_t; \tau) = a) - \PP_t(A_t = a) \right| \\
  &\underbrace{\leq}_{(iii)} \sqrt{ |\MC{A}_{\tau}| \sum_{a \in \MC{A}_{\tau}} \PP_t(A_t = a) \sum_{A \in \MC{A}_{\tau}} \PP_t(\pi^*(X_t; \tau) = A) \left( \E_t \big[ R \big( Y_t^{(a)} \big) \mid \pi^*(X_t; \tau) = A \big]
  - \E_t \big[ R \big( Y_t^{(a)} \big) \big] \right)^2 } \\
  &\qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad + \sqrt{ 2 \cdot \dkl{\PP_t\big(\pi^*(X_t; \tau) = \cdotspace\big)}{\PP_t \big(A_t = \cdotspace \big)} } \\
  &\underbrace{\leq}_{(iv)} \sqrt{ \frac{|\MC{A}_{\tau}|}{2} \sum_{a \in \MC{A}_{\tau}} \PP_t(A_t = a) \sum_{ A \in \MC{A}_{\tau}} \PP_t(\pi^*(X_t; \tau) = A) \cdot \dkl{ \PP_t \big( Y_t^{(a)} \mid \pi^*(X_t; \tau) = A \big) }{ \PP_t \big( Y_t^{(a)} \big) } } \\
  &\qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad + \sqrt{ 2 \cdot \dkl{\PP_t\big(\pi^*(X_t; \tau) = \cdotspace\big)}{\PP_t \big(A_t = \cdotspace \big)} }
\end{align*}
Above, equality (i) holds since conditional on $\HH_t$, the action $A_t$ and the outcome $Y_t^{(a)}$ are independent.
Inequality (ii) uses Cauchy-Schwartz inequality in the first term and uses that $R$ takes values in $[0,1]$ in the second term.
Inequality (iii) uses an elementary equality of summation in the first term, and Pinsker's inequality in the second term.
Inequality (iv) uses Fact 9 of \citet{russo2016information} (which uses Pinsker's inequality).

Using the above result, averaging over $t$ and taking an expectation, we get
\begin{align*}
    &\Delta(\psar) 
    = \E \left[ \frac{1}{T} \sum_{t=1}^T \E_t \left[ R(Y_{t}^{(\pi^*(X_t; \tau))}) - R(Y_{t}^{(A_t)}) \right] \right] \\
    &\leq \E \bigg[ \frac{1}{T} \sum_{t=1}^T \sqrt{ \frac{|\MC{A}_{\tau}|}{2} \sum_{A \in \MC{A}_{\tau}} \PP_t(A_t = A) \sum_{a \in \MC{A}_{\tau}} \PP_t(\pi^*(X_t; \tau) = a) \cdot \dkl{ \PP_t \big( Y_t^{(a)} \mid \pi^*(X_t; \tau) = a \big) }{ \PP_t \big( Y_t^{(A)} \big) } } \bigg] \\
    &\qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad + \E \bigg[ \frac{1}{T} \sum_{t=1}^T \sqrt{ 2 \cdot \dkl{\PP_t\big(\pi^*(X_t; \tau) = \cdotspace\big)}{\PP_t \big(A_t = \cdotspace \big)} } \bigg] \\
    &\underbrace{\leq}_{(i)} \sqrt{ \E \bigg[ \frac{1}{T} \sum_{t=1}^T  \frac{|\MC{A}_{\tau}|}{2} \sum_{A \in \MC{A}_{\tau}} \PP_t(A_t = A) \sum_{a \in \MC{A}_{\tau}} \PP_t(\pi^*(X_t; \tau) = a) \cdot \dkl{ \PP_t \big( Y_t^{(a)} \mid \pi^*(X_t; \tau) = a \big) }{ \PP_t \big( Y_t^{(A)} \big) } \bigg] } \\
    &\qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad + \sqrt{ \E \bigg[ \frac{1}{T} \sum_{t=1}^T 2 \cdot \dkl{\PP_t\big(\pi^*(X_t; \tau) = \cdotspace\big)}{\PP_t \big(A_t = \cdotspace \big)} \bigg] } \\
    &\underbrace{=}_{(ii)} \sqrt{ \frac{|\MC{A}_{\tau}|}{2} \cdot \frac{1}{T} \sum_{t=1}^T I_t\big ( \pi^*(X_t; \tau); ( Y_t^{(A_t)}, A_t ) \big) }
    + \sqrt{ \E \bigg[ \frac{1}{T} \sum_{t=1}^T 2 \cdot \dkl{\PP_t\big(\pi^*(X_t; \tau) = \cdotspace\big)}{\PP_t \big(A_t = \cdotspace \big)} \bigg] } \\
    &\underbrace{\leq}_{(iii)} \sqrt{ \frac{|\MC{A}_{\tau}|}{2} \cdot \frac{1}{T} \sum_{t=1}^T I_t\big ( \pi^*(X_t; \tau); ( Y_t^{(A_t)}, A_t ) \big) }
    + \sqrt{ 2 \{ \ell(p_{\theta}) - \ell(p^*) \} } \\
    &\underbrace{\leq}_{(iv)} \sqrt{ \frac{|\MC{A}_{\tau}|}{2} \cdot \frac{1}{T} \sum_{t=1}^T I_t\big ( \piX; ( Y_t^{(A_t)}, A_t ) \big) }
    + \sqrt{ 2 \{ \ell(p_{\theta}) - \ell(p^*) \} } \\
    &\underbrace{=}_{(v)} \sqrt{ \frac{|\MC{A}_{\tau}|}{2T}  I_1\big ( \piX; ( Y_t^{(A_t)}, A_t )_{t=1}^T \big) }
    + \sqrt{ 2 \{ \ell(p_{\theta}) - \ell(p^*) \} } \\
    &\underbrace{=}_{(vi)} \sqrt{ \frac{|\MC{A}_{\tau}|}{2T} \big\{ H_1(\piX) - H_1(\piX \mid ( Y_t^{(A_t)}, A_t )_{t=1}^T) \big\} }
    + \sqrt{ 2 \{ \ell(p_{\theta}) - \ell(p^*) \} } \\
    &\underbrace{\leq}_{(vii)} \sqrt{ \frac{|\MC{A}_{\tau}| \cdot H_1(\piX) }{2T} }
    + \sqrt{ 2 \{ \ell(p_{\theta}) - \ell(p^*) \} } \\
\end{align*}
Above, 
\begin{itemize}
    \item Inequality (i) uses Jensen's inequality twice.
    \item Equality (ii) uses Lemma \ref{lemma:mutualInfoNew} (Mutual information equivalency).
    \item Inequality (iii) uses Lemma \ref{lemma:AAbarKLNew} (KL Divergence in next action distribution).
    \item For inequality (iv), note that for any random variables $X_1, X_2, Y$ (where $X_1, X_2$ are discrete), by properties of mutual information and entropy,
    \begin{align*}
        I((X_1, X_2); Y) &= H(X_1, X_2) - H(X_1, X_2 \mid Y) \\
        &= H(X_1) - H(X_1 \mid Y) + H(X_2 \mid X_1) - H(X_2 \mid Y, X_1) \\
        &= I(X_1; Y) + I(X_2; Y \mid X_1)
    \end{align*}
    The above implies that $I((X_1, X_2); Y) \geq I(X_1; Y)$ since $I(X_2; Y \mid X_1) \geq 0$. Thus, since $\pi^*(X_t; \tau) \in \piX$ we have that
    \begin{align*}
        I_t \big( \piX; ( Y_t^{(A_t)}, A_t ) \big)
        \geq I_t\big ( \pi^*(X_t; \tau); ( Y_t^{(A_t)}, A_t ) \big).
    \end{align*}
    \item Equality (v) uses the chain rule for mutual information.
    \item Equality (vi) holds by the relationship between mutual information and entropy.
    \item Inequality (vii) holds since entropy is always nonnegative.
\end{itemize}
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proof of Proposition \ref{prop:psarRegretPerfect}}

\begin{customprop}{\ref{prop:psarRegretPerfect}}[Regret for Thompson sampling via generation with a perfect imputation model]
    Under Assumptions \ref{assump:contextualBandit} and \ref{assump:context}, Thompson sampling via generation (Algorithm \ref{alg:Thompson}) with the imputation model $p^*$ has regret that is bounded as follows:
    \begin{align*}
        \Delta( \mathbb{A}_{\rm{TS-Gen}}(p^*) ) \leq \sqrt{ \frac{ |\Aeval| \cdot H \big( \piX \mid Z_{\tau}, X_{1:T} \big) }{2 T} }.
    \end{align*}
\end{customprop}

\begin{proof}
This proposition holds as a direct corollary of Theorem \ref{thm:psarRegret}.
\end{proof}