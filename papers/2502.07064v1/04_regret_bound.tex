\section{Regret of generative Thompson sampling with an imperfect imputation model}
\label{sec:regret}
In this section, we present a generalization of the regret bound for the Thompson sampling via generation algorithm from Section \ref{sec:regret-perfect}. Our generalization is notable because the sequence model only affects the regret bound through its offline prediction loss, which means any sequence model class can be used---even sequence models that are not exactly exchangeable. Moreover, our result shows that the lower offline prediction loss of the sequence model $p_\theta$ translates into a better regret guarantee. Our result effectively \textit{reduces a difficult online decision-making problem to one of training an accurate sequence prediction model.} 

Specifically, our regret bound will depend on the following population-level version of the training loss from \eqref{eq:train_loss} (the expectation below averages over the task distribution $p^*$):
\begin{align}
    \ell(p_\theta) = - \E \bigg[ \sum_{a \in \MC{A}_{\tau}} \sum_{t=1}^T \log p_{\theta} \big( Y_t^{(a)} \mid Z_{\tau}, X_{1:t}, Y_{1:t-1}^{(a)} \big) \bigg].
    \label{eq:pop_loss}
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{theorem}[Regret bound for generative Thompson sampling with an imperfect imputation model]
    \label{thm:psarRegret}
    Let Assumptions \ref{assump:contextualBandit}, \ref{assump:indepAction}, and \ref{assump:context} hold. Under generative Thompson sampling (Algorithm \ref{alg:Thompson}) using $p_\theta$, $\psar(p_\theta)$, 
    \begin{align}
        \label{eqn:psarregret}
      \Delta \big( \psar(p_\theta) \big)
      & \leq 
        \underbrace{ \sqrt{ \frac{ |\MC{A}_{\tau}| \cdot H \big( \piX \mid Z_\tau, X_{1:T} \big) }{2 T} } }_{\TN{Regret bound for Thompson sampling}} 
        + \underbrace{ \sqrt{ 2 \big\{ \ell(p_\theta) - \ell(p^*) \big\} } }_{\TN{Penalty for sub-optimal prediction}}.
    \end{align}
\end{theorem}
Comparing the results of Theorem \ref{thm:psarRegret} and Proposition \ref{prop:psarRegretPerfect}, we can interpret the ``cost'' of using an imperfect imputation model $p_\theta$ in generative sequence modeling instead of $p^*$ as $\sqrt{ 2 \big\{ \ell(p_\theta) - \ell(p^*) \big\}}$.
In other words, the regret is penalized depending on how well $p_\theta$ approximates $p^*$. This penalty term, which does not vanish as $T$ grows, is unavoidable in bandits; see Appendix \ref{app:misspecifiedPriors} for further discussion. 
Also, we do not attempt to mathematically characterize when making the gap $\ell(p_\theta) - \ell(p^*)$ small via offline learning is possible, since it would involve stringent and unrealistic conditions derived from loose generalization bounds for neural networks. Model performance ``scaling-laws'' with the quantity of training data has been studied with great interest empirically \citep{henighan2020scaling}.


\paragraph{Novelty and proof approach.}
Note that Proposition \ref{prop:psarRegretPerfect} itself (a direct corollary of Theorem \ref{thm:psarRegret}) is novel due to the conditional entropy term we introduce, which quantifies the benefit of using prior information $Z$ and can be bounded using VC dimension (Section \ref{sec:regret-perfect}). 
What is particularly novel about Theorem \ref{thm:psarRegret} is that we are able to carry out our analysis even in cases where the imputation model $p_\theta$ is misspecified and does not correspond to any proper way of performing Bayesian inference. 


In our previous work \citep{psar2024}, we prove a regret bound with a similar prediction loss penalty for model misspecifcation for a generative Thompson sampling algorithm for a multi-armed, non-contextual bandit algorithm. Although they also use an information theoretic analysis, in their simpler setting without contexts, they do not need to introduce the concept of a general ``oracle'' policy fitting procedure and as a result do not provide an information theoretic analysis that extends to infinite policy classes as we do.
Moreover, we were not able to directly build on the proof approach used in \citet{psar2024} because they critically rely on the fact that under $p^*$, unobserved outcomes $Y$ are exchangeable given the history; However, for contextual bandits, unobserved outcomes $Y$ are not exchangeable conditional on observing their associated contexts $X$. 

\citet{wen2021predictions} also consider a non-contextual, multi-armed Thompson sampling algorithm that incorporates a generative outcome model. Our algorithm is not a generalization of theirs, since they require specifying a prior over latent environment parameters. Their regret bound allows for the generative model to be misspecified and requires a history-dependent KL divergence term to be small, which is different from our prediction loss penalty. Despite these differences, we carefully integrate bounding techniques from \citet{wen2021predictions} in proving our contextual bandit regret bounds. What is surprising is that their information theoretic bounding techniques developed for multi-armed, non-contextual bandits are relevant for contextual bandits with infinite policy classes, which has been of independent interest in the literature, even for correctly specified model settings \citep{neu2022lifting,infotheoreticNonstationary}.

