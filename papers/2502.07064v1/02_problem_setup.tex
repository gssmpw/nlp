\section{Problem formulation}
\label{sec:probFormulation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Meta contextual bandit problem}
We consider a meta contextual bandit problem where bandit tasks $\tau$ are sampled from an unknown task distribution $p^*$:
\begin{align}
    \tau \sim p^*.
    \label{eqn:taskDist}
\end{align}
Each bandit task $\tau$ consists of prior information $Z_{\tau}$, an action space $\MC{A}_{\tau}$, a sequence of context vectors $X_{1:T} = \{ X_1, \dots, X_T \}$, and a table of potential outcomes\footnote{We omit subscripting $X_t$ and $Y_t^{(a)}$ with $\tau$ to reduce clutter.} $\{ Y_{1:T}^{(a)} \}_{a \in \MC{A}_{\tau}} = \{ Y_1^{(a)}, \dots, Y_T^{(a)} \}_{a \in \MC{A}_{\tau}}$:
\begin{align*}
    \tau = \big\{ Z_{\tau}, X_{1:T}, \{ Y_1^{(a)}, \dots, Y_T^{(a)} \}_{a \in \MC{A}_{\tau}} \big\}.
\end{align*}
Informally, the agent's objective is to select actions to maximize the total expected reward for each encountered task. 
At the start of a task, the agent observes prior information $Z_{\tau}$. For each decision time $t \in [1 \colon T]$, the agent observes the context $X_t$, selects an action $A_t \in \MC{A}_{\tau}$, observes the outcome $Y_t = Y_t^{(A_t)}$, and computes the reward $R(Y_t)$, for a fixed, known function $R$ that takes values in $[0,1]$. We use $\HH_t$ to denote the history, which includes the current context:
\begin{align*}
    \HH_t = \left\{ Z_{\tau}, (X_1, A_1, Y_1),  \dots, (X_{t-1}, A_{t-1}, Y_{t-1}), X_t \right\}.
\end{align*}
The agent is able to learn both online \textit{within a single task} meaning over the $T$ total decision times, as well as meta-learn \textit{across different tasks} (e.g., learning how task prior information $Z_{\tau}$ may inform the distribution of $\{ Y_{1:T}^{(a)} \}_{a \in \MC{A}_{\tau}}$).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Offline data.}
We assume that the algorithm has access to training data from previous tasks, $\Dtrain = \{ \tau_1, \dots, \tau_n \}$, sampled according to \eqref{eqn:taskDist}. These previous bandit tasks can be used by the algorithm to meta-learn across tasks, e.g., learn about the distribution $p^*$ itself to improve decision-making quality. For simplicity, we present our algorithm assuming we have access to complete task datasets $\tau_i$, where all outcomes from all actions, $\{ Y_{1:T}^{(a)} \}_{a \in \MC{A}_{\tau}}$, are available. In Appendix~\ref{app:pretrain_bootstrap}, we discuss how by bootstrapping we can learn from partial data from each task (e.g., where $Y_t^{(a)}$ is only observed if $A_t = a$ was selected).

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/news-recommendation-context.png}
    \caption{News recommendation meta contextual bandit problem. This }
    \label{fig:news-recommendation}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Motivating example: News recommendation task.}
As depicted in \ref{fig:news-recommendation}, a motivating meta-contextual bandit problem is cold-start news recommendations.
Each day, a new set of articles $\MC{A}_{\tau}$ are released, which the agent recommends to users who arrive throughout the day. In contrast to \citet{li2010contextual}, our algorithm meta-learns across news recommendation tasks and uses the article text to improve cold-start decisions. We use $Z_{\tau} = (Z_{\tau}^{(a)})_{a \in \MC{A}_{\tau}}$ to denote the task-specific prior information, i.e., the news article text. The context variables $X_t$ consist of user-specific features and $Y_t$ are recommendation outcomes observed following the $t^{\rm{th}}$ decision. 

The modern challenge in this problem setting is that incorporating the news article text $Z_{\tau}$ can greatly improve the recommendation system's decisions, but a foundation model is needed to read process this high dimensional text and inform the decision-making of the agent. This motivates us in this work to i) make very minimal structural assumptions on the relationship between prior information $Z_{\tau}$, context features $X_t$, and outcomes $Y_t$, and ii) develop an algorithm that is amenable to incorporating foundation models. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Environment assumptions}
The defining quality of our problem formulation is that \textit{we do not make explicit assumptions about the distribution of outcomes $Y$ conditional on contexts $X$ and prior information $Z$}. It is common in the meta bandit literature to assume a known parametric model class that accurately captures the distribution $\{ Y_t^{(a)} \}_{a \in \MC{A}_{\tau}} \mid (X_t, Z_{\tau})$ \citep{kveton2021meta,wan2021metadata,cella2020meta,cella2022meta}; Typically, there is an unknown environment parameter that varies between tasks.
We instead allow this distribution to be general. Our algorithm's decision-making quality depends on how accurately the agent models this distribution, as well as the policy fitting procedure the algorithm designer chooses. Rather than relying on strong assumptions on the environment structure, we put the onus on the algorithm designer to i) learn a model that accurately captures the environment structure of the meta-bandit task at hand, and ii) choose a meaningful method for fitting a desired ``oracle'' policy, assuming access to a complete dataset. The motivation for this comes recognizing that such offline learning problems are routinely solved in practice in settings that extend well beyond theory. We focus on our theory on formal reductions to offline learning and policy selection problems, assuming offline learning can be done at scale.

\paragraph{(1) Stationary contextual bandit.} Assumption \ref{assump:contextualBandit} below captures two critical properties of (stationary) contextual bandit environments: 
(i) that contexts $X_{1:T}$ are \textit{exogenous} to (independent of) past outcomes and actions taken by the agent,
and (ii) that the context outcome tuples $(X_t, Y_t^{(a)})$ are permutation invariant over time. These assumptions are standard properties of contextual bandit type environments considered in the literature \citep{LattimoreSz19}.

\begin{assumption}[Contextual bandit environment]
\label{assump:contextualBandit}
For any task $\tau \sim p^*$, the distribution of the next context $X_t$ is independent of the previous outcomes, i.e., 
\begin{align*}
     X_t \indep \big\{ Y_{1:t-1}^{(a)} \big\}_{a \in \MC{A}_{\tau}} \mid \big( X_{1:t-1}, Z_{\tau} \big). 
\end{align*}
Additionally, for any action $a \in \MC{A}_{\tau}$, the tuples $(X_t, Y_t^{(a)})$ are exchangeable over time, i.e., for any permutation $\sigma$ over $T$ elements, the following are equal in distribution: 
\begin{align*}
    \big( X_t, Y_t^{(a)} \big)_{t \in [1 \colon T]} 
    \quad \overset{D}{=} \quad
    \big( X_{\sigma(t)}, Y_{\sigma(t)}^{(a)} \big)_{t \in [1 \colon T]}.
\end{align*}
\end{assumption}

%%%%%%%%%%%%%%%%%%%%%%
\paragraph{(2) Independence across actions.} 
We also assume that the outcomes $Y$ are independent across actions, conditional on $Z$ and $X$. This is a simplifying assumption, which allows us to safely model actions independently, and ignore dependencies between outcomes across actions.
\begin{assumption}[Independence across actions]
    Conditional on $Z_{\tau}, X_{1:T}$, outcomes $Y_{1:T}^{(a)}$ are independent across actions $a \in \MC{A}_{\tau}$.
    \label{assump:indepAction}
\end{assumption}

\paragraph{(3) Known context distribution.}
The final assumption we make is that the agent knows the distribution of contexts $X_{1:T}$. In practice, this is often not a strong assumption because contexts $X$ from previous tasks (which do not need to be associated with any outcomes $Y$) can be used to approximate this context distribution. In many applications, the foremost cost in data collection is that of acquiring ``labels'' associated with the unlabeled covariates $X$ \citep{settles2009active} and it is common to easily acquire large quantities of unlabeled covariate data \citep{zhou2021semi}. We do not focus on learning the context distribution in this work since existing unsupervised methods could be directly used here for covariate modeling \citep{jonsson1998automated}. Moreover, learning the distribution of passively observed stationary contexts (which are unaffected by the actions taken by the agent by Assumption \ref{assump:contextualBandit}) is less compelling from the perspective of designing active learning algorithms. 
\begin{assumption}[Context distribution is known]
    The conditional distribution of contexts, $X_{1:T} \mid Z_{\tau}$, is known.\label{assump:context}
\end{assumption}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Definition of regret}
\label{sec:regretDef}
\bo{Policy fitting.} 
We assume that the algorithm designer specifies a procedure for fitting a desired ``oracle'' policy given access to a complete bandit task dataset $\tau$. This policy fitting procedure outputs policies in a function class $\Pi^*$ where each $\pi^* \in \Pi^*$ defines a mapping from contexts $X_t$ to an action $a \in \MC{A}_{\tau}$ that does not vary over time. For notational simplicity, the policies in $\Pi^*$ are assumed to be non-stochastic. Note that we \textit{do not} require that this policy class is necessarily ``correct''. For a particular task $\tau$, we use $\pi^*(\cdotspace; \tau)$ to denote a ``best-fitting'' policy $\pi^* \in \Pi^*$, where the fitting criterion is defined by the algorithm designer. 
For a simple example, one could fit using a least squares criterion:
\begin{align*}
    \TN{argmin}_{\pi \in \Pi^*} ~ \frac{1}{T} \sum_{t=1}^T \left\{ R \big(Y_t^{(\pi(X_t))} \big) - \max_{a \in \MC{A}_{\tau}} R \big(Y_t^{(a)} \big) \right\}^2.
\end{align*}
One should think of $\pi^*(\cdotspace; \tau)$ the policy one \emph{would} implement if abundant task data, $\tau$, was available. This could involve fitting a model, adding prompt tokens to condition a language model, or maximizing hindsight performance. This policy fitting could also incorporate various desirable constraints on the policy, including resource or fairness constraints. We aim to attain performance competitive with this policy through efficient interactive learning, despite starting without any of the outcomes $Y$ from the task data. 


\paragraph{Regret.} 
We consider a best-in-class style regret objective, which is common in the contextual bandit literature \citep{foster2020open,foster2019model,langford2007epoch,agarwal2017corralling}. The objective of the agent $\mathbb{A}$ is to make decisions to minimize the per-period regret against $\pi^*(X_t; \tau)$: 
\begin{align}
    \label{eqn:regretContext}
    \hspace{-1mm}\Delta(\mathbb{A}) = \E \bigg[ \frac{1}{T} \sum_{t=1}^T \left\{ R \big(Y_{t}^{(\pi^*(X_t; \tau))} \big) - R \big( Y_{t}^{(A_t)} \big) \right\} \bigg].\hspace{-1mm}
\end{align}
The expectation above averages over tasks $\tau \sim p^*$ and any randomness in the algorithm used to select actions $A_{1:T}$. This can be interpreted as the long-run per-period regret if the algorithm was deployed across many tasks.

Note that increasing the complexity of the policy class $\Pi^*$ (in a VC dimension sense) can increase the average reward under the best-fitting policy, $\E \left[ \frac{1}{T} \sum_{t=1}^T R \big(Y_{t}^{(\pi^*(X_t; \tau))} \big)\right]$. However, this increased complexity also means that large sample sizes are required to learn $\pi^*(\cdotspace; \tau)$ accurately and will worsen our regret bound (see Section \ref{sec:regret}).

