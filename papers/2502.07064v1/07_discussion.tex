\section{Discussion}
We introduce a generative Thompson sampling algorithm for contextual bandits that is compatible with any generative model with low offline prediction loss and any policy fitting procedure. We prove a regret bound for our algorithm that allows for misspecification of the generative model, and also provides insights into information theoretic analyses for contextual bandits, which may be of independent interest.

Directions for future investigation include i) developing methods to guide how one might choose an appropriate policy class \citep{foster2020open}, ii) quantifying how much offline data is needed to train a high quality generative sequence model (which includes settings where the offline data is collected by a behavior policy), iii) investigating the impact of approximating the context distribution when it is unknown, iv) exploring if the generative approach to modelling uncertainty can be extended to more difficult decision-making settings, like Markov decision processes.