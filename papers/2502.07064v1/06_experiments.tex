\section{Experiments}
\label{sec:experiments}

\paragraph{Problem settings.} We consider two meta-bandit settings. In both, $T=500$, $|\MC{A}| = 10$, outcomes $Y$ are binary, and $R(y) = y$. Our \textsc{synthetic} setting uses a Bayesian logistic regression data generating process with a 2-dimensional task vector $Z$ and 5-dimensional contexts $X$. Our \textsc{semi-synthetic} setting is designed to mimic a cold-start news recommendation setting and uses the MIcrosoft News Dataset \citep{wu2020mind}; Here $Z$ consists of the article headlines, contexts $X$ are 5-dimensional user features, and $Y$ represents whether or not the user clicked on a recommended article. See Appendix \ref{app:dgp} for more details.

\paragraph{Bandit algorithms.}
The sequence model used in generative Thompson sampling (TS-Gen) is a simple multi-layer perceptron-based sequence model which takes as input the prior information $Z$, a summary statistic for the history, and the current context $X$, and outputs a distribution over $Y$. In the \textsc{semi-synthetic} setting, we additionally embed the article text $Z$ using DistilBERT \citep{sanh2019distilbert} before feeding it into our sequence model. For the policy class $\Pi^*$, we choose a simple logistic policy class for our \textsc{synthetic} setting and a XGBoost-like, tree based policy class for our \textsc{semi-synthetic} setting. We compute regret against the best fitting policy in each class for the respective setting. We further examine the choice of policy class in App. \ref{sec:policy_class}.

We compare to several baselines. Three baselines use the same sequence model $p_\theta$ as TS-Gen, but use alternative, non-autoregresive ways to select the next action: 1) \textsc{Greedy} deterministically selects the actions predicted by $p_\theta$ to have the greatest next reward. 2) \textsc{Epsilon-greedy} acts the same as \textsc{Greedy} with probability $0.9$ and with probability $0.1$ selects an action uniformly at random. 3) \textsc{Softmax} uses softmax/Boltzmann exploration \citep{cesa2017boltzmann}, which is akin to the PreDeTor-$\tau$ algorithm from \citet{predetor}. 
Finally, we also compare to linear Thompson sampling (TS-Linear) with an uninformative prior \citep{agrawal2013thompson} and LinUCB \citep{li2010contextual}.

\begin{figure*}[t]
\centering
\includegraphics[width=0.48\linewidth]{figures/synthetic_regret_0131.png}
\hfill
\includegraphics[width=0.48\linewidth]{figures/semisynthetic_regret_0131.png}
\caption{Cumulative regret for synthetic (left) and semisynthetic (right) settings, averaged over 500 bandit tasks each. 
TS-Gen performs well compared to other algorithms, including others that use the same sequence model $p_\theta$ (Greedy, Epsilon-Greedy, Softmax). Regret is against the best fitting policy in $\Pi^*$ (logistic for synthetic, and tree-based for semisynthetic). Error bars (barely visible) denote $\pm 1$ s.e. 
}
\label{fig:regret_comparison}
\end{figure*}

\paragraph{Results.} 
As shown in Figure \ref{fig:regret_comparison}, TS-Gen significantly outperforms other approaches in both settings. Our advantage over LinUCB and TS-Linear attributable to our meta-learning procedure and better use of prior information $Z$. Moreover, TS-Gen's superior performance compared to other algorithms that use the same $p_\theta$ sequence model to make decisions 
supports the benefit of autoregressive generative approach to uncertainty quantification and decision-making. Also, we find, as suggested by Theorem \ref{thm:psarRegret}, the lower the offline prediction loss of a sequence model, the lower the regret of TS-Gen with that sequence model (Appendix \ref{app:seqloss}).


