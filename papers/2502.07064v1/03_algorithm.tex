\section{Key conceptual idea: Missing data view of uncertainty quantification}
In this work, we view missing data as the source of the decision-maker's uncertainty.
This contrasts the classical approach of considering unknown model parameters as the source of uncertainty. As we will explore in the following sections, the missing data viewpoint is very amenable to modern deep learning methods, which can be used to train models that are able to impute missing data probabilistically in a calibrated fashion. 

\subsection{Posterior sampling via imputing missing data} 
To convey the missing data viewpoint, we first consider an idealized setting in which we have access to the meta task distribution $p^*$ (we discuss how to approximate $p^*$ in Section \ref{sec:ourAlg}). Using $p^*$ we can form an exact posterior sample for task outcomes $\tau = \big\{ Z_{\tau}, X_{1:T}, \{Y_{1:T}^{(a)}\} \big\}$ given the history $\HH_t$:
\begin{align}
    \label{eqn:taskPosteriorSample}
    \hat{\tau}_t \sim p^* \left( \tau \in \cdot \mid \HH_t \right).
\end{align}
Above we probabilistically generate values in $\tau$ that have not yet been observed in the history $\HH_t$; This consists of future contexts, future outcomes, and outcomes from previous timesteps for actions that were not selected.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/fitted-policy.png}
    \caption{At each decision time, the agent imputes missing outcomes and uses the imputed dataset to fit a policy to select actions.}
    \label{fig:fitted-policy}
\end{figure}

With this exact posterior sample of the task outcomes $\hat{\tau}_t$, we can use it to form posterior samples of any statistic computed using $\hat{\tau}_t$. 
In particular, we are interested in sampling from the posterior distribution of the fitted policy $\pi^*(\cdotspace; \tau)$, which can be computed by finding the fitted policy for the sampled task dataset $\hat{\tau}_t$, i.e., $\pi^*(\cdotspace; \hat{\tau}_t)$. 
Obtaining posterior samples of a best-fitting policy is a common subroutine used in Bayesian decision-making algorithms \cite{kaufmann2012bayesian, russo2018learning, ryzhov2012knowledge}. Thus, the posterior sampling via generation easily integrates with these existing contextual bandit algorithms. 

In this work, we focus on Thompson sampling \citep{russo2016information,thompson1933likelihood}, i.e., probability matching, which selects actions according to the posterior probability that they are optimal. Thompson sampling can be implemented with a single posterior sample per decision time. Specifically, at decision time $t$, after sampling $\hat{\tau}_t$ as in \eqref{eqn:taskPosteriorSample}, Thompson sampling fits the policy $\pi^*(\cdotspace; \hat{\tau}_t)$, and selects the action $A_t \gets \pi^*(X_t; \hat{\tau}_t)$. See Figure \ref{fig:fitted-policy} for a depiction.
Under this generative version of Thompson sampling, the polices in $\Pi^*$ that are optimal under some likely generation of $\hat{\tau}_t$ have a chance of being selected. Once no plausible sample of missing outcome $\hat{\tau}_t$ could result in an action being optimal, it is essentially written off. 
See Algorithm \ref{alg:Thompson} for pseudocode for Thompson sampling via generation.

\begin{algorithm}[h]
\caption{Thompson sampling via generation}
\label{alg:Thompson}
\begin{algorithmic}[1]
   \REQUIRE Imputation model $p$, actions $\MC{A}_{\tau}$, task input $Z_{\tau}$.
   \FOR{$t \in \{1, \dots, T\}$}
        \STATE Observe context $X_t$ and append it to $\HH_t$
        \STATE Generate / sample $\hat{\tau}_t \sim p( \tau \in \cdot \mid \HH_t)$
        \STATE Fit the policy $\pi^*(\cdotspace; \hat{\tau}_t)$ % where $\hat{\tau} \gets \{ Z_{\tau}, \hat{X}_{1:T}, \hat{\bs{Y}} \}$
        \STATE Select the action $A_t \gets \pi^*(X_t; \hat{\tau}_t)$
        \STATE Observe outcome $Y_t \gets Y_t^{(A_t)}$ from action $A_t$.
        \STATE Update history $\HH_{t+1} \gets \HH_t \cup \{ (X_t, A_t, Y_t)\}$ 
   \ENDFOR
\end{algorithmic}
\end{algorithm}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Regret: Thompson sampling via generation with $p^*$}
\label{sec:regret-perfect}
This section presents a regret bound for Algorithm \ref{alg:Thompson} with the perfect imputation model, $p^*$ from \eqref{eqn:taskDist}. Our work develops a novel analysis of contextual Thompson sampling, which is applicable to infinite policy classes $\Pi^*$ with finite VC dimension. Our VC dimension bound resembles those from adversarial bandits, but for the first time, we show we can derive this using an information theoretic analysis.

\paragraph{Notation.}
For any random variables $X, Y$ (for $Y$ discrete), we denote the conditional entropy of $Y$ given $X$ using $H(Y \mid X)$; Note $H(Y \mid X)$ is a constant, i.e., $H(Y \mid X) = \E \big[ \sum_y \PP( Y = y \mid X) \log \PP( Y = y \mid X) dy \big]$.
Additionally, we use the following notation to define the best fitting policy evaluated at contexts $X_{1:T}$:
\begin{align}
    \label{eqn:piXdef}
    \piX := \{ \pi^*(X_t; \tau) \}_{t=1}^T.
\end{align}

\paragraph{Regret bound.}
Proposition \ref{prop:psarRegretPerfect} states our regret bound for Algorithm \ref{alg:Thompson} when the imputation model is $p^*$ from \eqref{eqn:taskDist}.
\begin{proposition}[Regret for Thompson sampling via generation with $p^*$]
    \label{prop:psarRegretPerfect}
    Let Assumptions \ref{assump:contextualBandit} and \ref{assump:context} hold. Under Thompson sampling via generation (Algorithm \ref{alg:Thompson}) with the imputation model $p^*$, 
    denoted $\mathbb{A}_{\rm{TS-Gen}}(p^*)$, 
    \begin{align*}
        \Delta( \mathbb{A}_{\rm{TS-Gen}}(p^*) ) \leq \sqrt{ \frac{ |\Aeval| \cdot H \big( \piX \mid Z_{\tau}, X_{1:T} \big) }{2 T} }.
    \end{align*}
\end{proposition}
Proposition \ref{prop:psarRegretPerfect} follows as a direct corollary of our more general result (Theorem \ref{thm:psarRegret} in Section \ref{sec:regret}).  
What is notable in the regret bound from this result is that i) it quantifies the benefit of incorporating prior information $Z$, and ii) it automatically applies to infinite policy classes since it only depends on the entropy of the fitted policy evaluated at a finite number of contexts, $\piX := \{ \pi^*(X_t; \tau) \}_{t=1}^T$. 


\paragraph{Relation to other information theoretic regret bounds.} Our regret bound notably improves on prior work, which develop Bayesian regret bounds for contextual Thompson sampling. First, our bound can be applied broadly, while approaches like \citet{neu2022lifting} depends on the entropy of a latent environment parameter, which is only applicable to parametric contextual bandits. Second, the entropy term in our bound $H( \piX \mid Z_\tau, X_{1:T})$ is often notably smaller than those from other contextual Thompson sampling Bayesian regret bounds. Specifically, \citet{infotheoreticNonstationary} consider a contextual bandit setting where at decision-time $t$ the algorithm selects a policy to use to choose $A_t$, prior to observing the context $X_t$. Their regret depends on the entropy of the optimal policy function, which when translated to our setting is much larger than our entropy term:  $H( \{ \pi^*(X_t) \}_{t=1}^T \mid X_{1:T}, Z_\tau) < H ( \pi^* \mid Z_\tau)$; Intuitively, this is saying that the entropy of a policy evaluated at a finite number of contexts is less than the entropy of the entire policy function.
Finally, while many information-theoretic analyses for Thompson sampling that generalize beyond multi-armed bandits require arguments that discretize the latent parameter space \cite{dong2018information,gouverneur2024an,neu2022lifting,infotheoreticNonstationary} and utilize cover-number type arguments, our proof approach notably does not require any discretization.

\paragraph{Upper bounding the condition entropy using VC dimension.} We can construct a coarse upper bound for the entropy $H \big( \piX \mid Z_{\tau}, X_{1:T} \big)$ using the VC-dimension of the policy class $\Pi^*$. This bound is coarse because the VC-dimension is a worst case quantity that has to with the number of possible assignments of actions to contexts. In contrast, entropy reflects that based on the task distribution (learned from past tasks) and the information $Z$ (e.g. article texts), many assignments may be extremely unlikely to be optimal. We formalize VC dimension upper bound in Lemma \ref{lemma:VC} below, which holds by the Sauer-Shelah lemma \citep{sauer1972density,shelah1972combinatorial}.

\begin{lemma}[VC dimension bound on entropy]\label{lemma:VC}
    For any binary\footnote{Note, VC-dimension is defined for binary functions.} action policy class $\Pi^*$, 
    \begin{align*}
        H \big( \piX \mid Z_{\tau}, X_{1:T} \big)
        \leq H \big( \piX \mid X_{1:T} \big)
        = O\big( \TN{VCdim}(\Pi^*) \log T \big).
    \end{align*}
\end{lemma}

Using our coarser upper bound on the entropy, our regret bound (Proposition \ref{prop:psarRegretPerfect}) resembles adversarial regret bounds that depend on VC dimension, showing for the first time how such a result can be established through information theoretic arguments \citep{beygelzimer2011contextual}.
Additionally, our rate also resembles standard Bayesian regret bounds for linear contextual bandits \citep{russo2018learning}; When $\Pi^*$ is defined by a linear model of dimension $d$, then $\TN{VCdim}(\Pi^*) = d+1$, so $H \big( \piX \mid X_{1:T} \big) = O(d \log T)$. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Thompson sampling via generation under an imperfect imputation model}
\label{sec:ourAlg}
In the previous section, we introduced a generative version of Thompson sampling for contextual bandits under the assumption we have access to a perfect imputation model $p^*$. In this section, we discuss how to practically approximate such an algorithm. 
First, we pretrain an autoregressive sequence model to predict successive outcomes ($Y$'s) on historical data $\Dtrain$. Then, at decision time, recommendation decisions are made by imputing the missing outcomes in $\tau$ with by generating outcomes ($\hat{Y}'$s) autoregressively from the pretrained sequence model. The offline pretraining allows the algorithm to ``meta-learn'' a good model for imputing missing outcomes using data from previous tasks.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\linewidth]{figures/process-fig.png}
    \caption{Offline meta-learning and online decision-making across multiple tasks. Figure is slight modification of one from \citet{psar2024}.}
    \label{fig:process}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Step 1: Offline learning to predict masked outcomes.}
We train an autoregressive sequence model $p_{\theta}$, parameterized by $\theta \in \Theta$, to predict missing outcomes, conditioned on the task prior information $Z$, and limited previously observed context and outcome tuples $(X, Y)$. This enables us to generate hypothetical completions of the potential outcome table $\tau$ later in the online decision-making phase. Formally, this model specifies a probability $p_{\theta}(Y_{t}^{(a)} \mid Z, X_{1:T}, Y_{1 : t-1}^{(a)} )$ of observing outcome $Y_{t}^{(a)}$ from the next interaction conditioned on the current context $X_t$, prior information $Z$, and the previous contexts and outcomes $(X_{1:t-1}, Y_{1: t-1}^{(a)})$. These one-step conditional probabilities generate a probability distribution over missing task $\tau$ outcomes. 


Specifically, we use historical data $\Dtrain$ to minimize the following loss function, which can be optimized using stochastic gradient descent (Algorithm \ref{alg:offline}): 
\begin{align}
    \label{eq:train_loss}
    &- \frac{1}{|\Dtrain|} \hspace{-1mm} \sum_{\tau \in \Dtrain} \sum_{a \in \MC{A}_{\tau}} \sum_{t=1}^T \log p_{\theta} \big( Y_t^{(a)} \mid  Z_{\tau}, X_{1:t}, Y_{1:t-1}^{(a)} \big)
\end{align}
The offline learning procedure can be used with any sequence model class. The quality of the decision-making algorithm depends on the quality of the learned sequence model $p_\theta$; Our regret bounds (Section \ref{sec:regret}) formalize this.
\begin{algorithm}[h]
  \caption{Offline training of a sequence model}
  \label{alg:offline}
  \begin{algorithmic}[1]
    \REQUIRE Training data $\Dtrain$, model class $\{p_\theta\}_{\theta \in \Theta}$
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \WHILE{not converged}
        \STATE Sample a mini-batch of tasks $\MC{D}^{\TN{mini-batch}} \subset \Dtrain$
        \STATE Compute loss in \eqref{eq:train_loss} using tasks $\tau \in \MC{D}^{\TN{mini-batch}}$ \\(optionally permute the order of $(X_t, Y_t^{(a)})$ tuples)
        \STATE Backpropagate and take gradient step to update $p_\theta$
    \ENDWHILE
  \end{algorithmic}
\end{algorithm}


Optionally, when training to minimize the loss \eqref{eq:train_loss}, we can permute the order of the tuples $(X_t, Y_t^{(a)})$ over time, since they are exchangeable under Assumption \ref{assump:contextualBandit}. Learning an approximately exchangeable sequence model mirrors recent work on neural processes \citep{garnelo2018neural,jha2022neural,NguyenGr22,lee2023martingale} and prior-data fitted networks \citep{MullerHoArGrHu22}, connecting also to a long tradition in Bayesian statistics and information theory \cite{dawid1984present, barron1998minimum}. 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Step 2: Online decision-making using the learned sequence model.}
\label{sec:online}
After a sequence model $p_\theta$ is trained offline, it is deployed and used for online decision-making. No additional training of $p_\theta$ is needed. Instead the sequence model learns ``in-context'' by conditioning \citep{brown2020language}. We implement generative Thompson sampling (Algorithm \ref{alg:Thompson}) by using $p_\theta$ to autoregressively generate (impute) missing outcomes in $\tau$ for each candidate action $a \in \MC{A}_{\tau}$, forming $\tau_t$. In the generation procedure, we sample both missing outcomes $Y$ and future contexts $X$ (which is known by Assumption \ref{assump:context}).
Recall that generative Thompson sampling uses both the observed and generated outcomes, $\tau_t$, to fit a policy and selects the best action according to that policy. 
Good decision-making relies on the model $p_\theta$ matching the meta-bandit task distribution $p^*$ closely (see Section \ref{sec:regret}). 


\begin{figure*}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/generation-steps.png}
    \caption{Posterior sampling via autoregressive generation (Algorithm \ref{alg:posterior_sample}). }
    \label{fig:autoregressive-generation}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}[h]
  \caption{Posterior sampling via autoregressive generation}
  \label{alg:posterior_sample}
  \begin{algorithmic}[1]
    \REQUIRE Sequence model $p_{\theta}$, actions $\MC{A}_{\tau}$, history $\HH_t$
    \STATE For each $a \in \MC{A}_{\tau}$, construct $\MC{M}^{(a)}$ the set of timesteps $i \in [1 \colon T]$ for which $Y_i^{(a)}$ has not been observed in $\HH_t$
    \STATE For each $a \in \MC{A}_{\tau}$, construct ordering $\prec_{a}$ placing observed outcomes before unobserved ones
    \STATE $\hat{X}_{1:t+1} \gets X_{1:t+1}$ (observed contexts from $\HH_t$)
    \STATE Sample future contexts $\hat{X}_{t+1}, \dots, \hat{X}_T$ (Assump. \ref{assump:context})
    \FOR{$a \in \MC{A}_{\tau}$}
    \FOR{$i \in \{1,\ldots,T\}$ in order of $\prec_{a}$}        
        \IF{$i \not\in \MC{M}^{(a)}$} 
            \STATE $\hat{Y}_i^{(a)} \gets Y_i^{(a)}$
        \ELSE
            \STATE Sample $\hat{Y}_i^{(a)} \sim p_{\theta}(\cdotspace \mid Z, \{\hat{X}_j, \hat{Y}_j^{(a)} \}_{j \prec_{a} i}, \hat{X}_i )$
        \ENDIF
    \ENDFOR
    \ENDFOR
    \STATE \TN{\bo{Return:}} \, $\hat{\tau}_t \gets \big\{ Z_\tau, \hat{X}_{1:T}, \{ \hat{Y}_{1:T}^{(a)} \}_{a \in \MC{A}_{\tau}} \big\}$
  \end{algorithmic}
\end{algorithm}

We formally describe the posterior sampling via autoregressive generation procedure to form samples $\hat{\tau}_t$ in Algorithm~\ref{alg:posterior_sample}; This subroutine can be used in line 3 of Algorithm \ref{alg:Thompson} to implement generative Thompson sampling. In Algorithm~\ref{alg:posterior_sample}, we use $\mathcal{M}_a \subset \{1,\ldots,T\}$ to denote the timesteps $t$ for which $Y_t^{(a)}$ has not been observed. 
To clarify the order of generating outcomes in $\hat{\tau}_t$, we permute observed outcomes to always precede missing ones; this matches the ordering used for generation in Figure \ref{fig:autoregressive-generation}. 
Specifically, we use $\prec_a$ to denote this ordering for an action $a \in \MC{A}_{\tau}$; We use $i\prec_a j$ whenever either (a) $i < j$ or (b) $i \notin \mathcal{M}_a$, but $j \in \mathcal{M}_a$.






