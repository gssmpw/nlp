\section{Introduction}
Recent advances in machine learning have transformed our ability to develop high quality predictive and generative models for complex data. This work introduces a framework for developing decision-making algorithms, specifically for contextual bandit problems, that can take advantage of these machine learning advances. By design, we assume the algorithm developer is able to effectively apply these machine learning techniques (e.g., minimize a loss via gradient descent) and employ these methods as subroutines in our decision-making algorithm. Moreover, our theory formally connects the quality of effective (self-)supervised learning via loss minimization to the quality of decision-making.

% Redfining the primitive operations
Classically, contextual Thompson sampling algorithms form a parametric model of the environment and consider the decision-maker's uncertainty as arising from unknown latent parameters of that model \citep{russo2020tutorial}. In this classical perspective, the primitive operations that are assumed to be feasible (at least approximately) include i) the ability to specify an informative prior for the latent parameter using domain knowledge, ii) the ability to sample from the posterior distribution of the latent parameter, and iii) the ability to update the posterior distribution as more data is collected. 
Unfortunately, it is well known that all three of these primitive operations are non-trivial to perform with neural networks \citep{tran2020practical,goan2020bayesian}.

Building on our previous work \citep{psar2024} which focuses on multi-armed bandits without contexts, we view missing, but potentially observable, future outcomes as the source of the decision-maker's uncertainty. This perspective allows us to replace the primitive operations required in the classical view with new primitives that are more compatible with neural networks: i) the ability to effectively minimize an offline sequence prediction loss, ii) the ability to autoregressively generate from the optimized sequence model, and iii) the ability to fit a desired policy given access to a complete dataset (outcomes from all actions and decision-times). 


% Explain the algorithm and what makes this algorithm possible (meta)
In the missing data view of uncertainty, if we had a complete dataset, there is no uncertainty because we could simply use the entire dataset to fit a desired ``oracle'' policy to use to make decisions. Inspired by this idea, at each decision time our algorithm imputes all missing outcomes using a pretrained generative sequence model, fits a desired policy using the imputed complete dataset, and selects the best action according to the fitted policy. We show that this algorithm is a generative implementation of Thompson sampling \citep{russo2020tutorial}. Moreover, we demonstrate empirically that it is possible to learn an accurate generative model to impute missing outcomes using standard machine learning tools in \textit{meta-bandit} settings, where one encounters many distinct, but related bandit tasks. We use data from previous bandit tasks to train  a generative sequence model offline.

% Advantages of this method
We prove a state-of-the-art regret bound for our generative Thompson sampling algorithm with three key properties, which each have significant practical implications. First, the generative model used to impute missing outcomes only affects our bound through the offline sequence prediction of the model. This means that our theory is applicable to any sequence model architecture, and that the quality of the sequence model can be easily optimized for and evaluated via offline training and validation. Second, our bound is unique in that it applies to any procedure for fitting a desired ``oracle'' policy. This allows one to easily adapt Thompson sampling to decision-making problems with constraints, including resource and fairness constraints. Finally, our proof approach makes important improvements to previous information theoretic analyses, which may be broadly applicable: i) we accommodate infinite policy classes directly without discretization, and ii) our bound quantifies the benefit of prior information available from a task, such as side information regarding actions. 

