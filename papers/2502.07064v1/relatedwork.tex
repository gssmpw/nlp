\section{Related work}
\label{sec:related-work}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Decision-making with sequence models.}
Many recent methods use sequence models in decision-making involve imitation learning, i.e., from demonstrations learn to mimic an expert's actions \citep{decisionTransformer,janner2021offline,hussein2017imitation}. \citet{lee2023incontext} discuss how these approaches can be used even without access to expert demonstrations, as long as one is able to fit an approximate ``oracle'' policy from offline bandit environments. Our work differs significantly from \citet{lee2023incontext} and other imitation learning based works because our sequence models are used to sample future \textit{outcomes}, instead of predicting optimal actions. 
Several recent works also use sequence models to generate future rewards for decision-making \citep{mukherjee2024pretraining,NguyenGr22, MullerHoArGrHu22, GarneloRoMaRaSaShTeReEs18,liu2016prior}. \citet{mukherjee2024pretraining} find that their approach which trains sequence models to predict future rewards performs better than decision pre-trained transformers trained to predict the optimal action \citep{lee2023incontext}. Future work is needed to develop a more complete understanding of the trade-offs between decision-making algorithms that use sequence models to approximate future rewards versus predict optimal next actions.

Our work differs from most previous works that use sequence models that predict future rewards for decision-making. Specifically, most previous works do not use autoregressive generation to quantify uncertainty \citep{mukherjee2024pretraining,NguyenGr22, MullerHoArGrHu22, GarneloRoMaRaSaShTeReEs18}; Instead they employ strategies that only model uncertainty in the single next timestep's reward under each action, e.g., by using softmax sampling \citep{mukherjee2024pretraining}.We find empirically that alternative (non-autoregressive) ways of sampling from the sequence model can lead to inferior decision-making performance (Figure \ref{fig:regret_comparison}). 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{(Approximate) Thompson sampling with neural networks.}
Implementing Thompson sampling with neural network models has been a longstanding challenge. \citet{riquelme2018deep} investigated a variety of Bayesian uncertainty quantification techniques for neural networks to use with Thompson sampling; They find that linear Thompson sampling with context features embedded with a neural network stood out over many more complex methods. Since then, others have investigated other methods including Thompson sampling when directly modeling uncertainty in neural network weights \citep{zhang2020neural,local-uncertainty}. Perhaps the foremost approach currently in the literature is implementing Thompson sampling using deep ensembles and related approaches designed to reduce computational overhead \citep{ensembleSampling,lu2017ensemble,dwaracherla2020hypermodels,osband2023approximate,osband2015bootstrapped,osband2023approximate,li2024ensemble++,li2024hyperagent}. Our generative Thompson sampling algorithm is critically different from ensembling because a) through offline meta-training we are able to learn informed priors from complex task-specific information $Z$ (like text) with benefits that are explicitly reflected in our bound, and b) our approach allows the sequence model to learn \textit{in-context} avoiding retraining online using gradient updates on subsampled data, which is sensitive to learning rates.