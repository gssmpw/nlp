@article{SAC3_hallucination_detection_black_box_lms,
  title={SAC3: Reliable Hallucination Detection in Black-Box Language Models via Semantic-aware Cross-check Consistency},
  author={Zhang, Jiaxin and Li, Zhuohang and Das, Kamalika and Malin, Bradley and Kumar, Sricharan},
  journal={arXiv preprint arXiv:2311.01740},
  year={2023},
  url={https://arxiv.org/abs/2311.01740}
}

@article{agrawal2023language,
  title={Do Language Models Know When They're Hallucinating References?},
  author={Agrawal, Ayush and Suzgun, Mirac and Mackey, Lester and Kalai, Adam Tauman},
  journal={arXiv preprint arXiv:2305.18248},
  year={2023}
}

@article{asai2023self,
  title={Self-rag: Learning to retrieve, generate, and critique through self-reflection},
  author={Asai, Akari and Wu, Zeqiu and Wang, Yizhong and Sil, Avirup and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2310.11511},
  year={2023}
}

@article{calibrated_language_models_must_hallucinate,
  title={Calibrated Language Models Must Hallucinate},
  author={Kalai, Adam Tauman and Vempala, Santosh S.},
  journal={Proceedings of the 56th Annual ACM Symposium on Theory of Computing (STOC)},
  year={2024},
  url={https://arxiv.org/abs/2311.14648}
}

@inproceedings{duan2024shifting,
  title={Shifting attention to relevance: Towards the predictive uncertainty quantification of free-form large language models},
  author={Duan, Jinhao and Cheng, Hao and Wang, Shiqi and Zavalny, Alex and Wang, Chenan and Xu, Renjing and Kailkhura, Bhavya and Xu, Kaidi},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={5050--5063},
  year={2024}
}

@article{farquhar2024detecting,
  title={Detecting hallucinations in large language models using semantic entropy},
  author={Farquhar, Sebastian and Kossen, Jannik and Kuhn, Lorenz and Gal, Yarin},
  journal={Nature},
  volume={630},
  number={8017},
  pages={625--630},
  year={2024},
  publisher={Nature Publishing Group UK London}
}

@article{gao2022rarr,
  title={Rarr: Researching and revising what language models say, using language models},
  author={Gao, Luyu and Dai, Zhuyun and Pasupat, Panupong and Chen, Anthony and Chaganty, Arun Tejasvi and Fan, Yicheng and Zhao, Vincent Y and Lao, Ni and Lee, Hongrae and Juan, Da-Cheng and others},
  journal={arXiv preprint arXiv:2210.08726},
  year={2022}
}

@article{kadavath2022language,
  title={Language models (mostly) know what they know},
  author={Kadavath, Saurav and Conerly, Tom and Askell, Amanda and Henighan, Tom and Drain, Dawn and Perez, Ethan and Schiefer, Nicholas and Hatfield-Dodds, Zac and DasSarma, Nova and Tran-Johnson, Eli and others},
  journal={arXiv preprint arXiv:2207.05221},
  year={2022}
}

@article{kuhn2023semantic,
  title={Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation},
  author={Kuhn, Lorenz and Gal, Yarin and Farquhar, Sebastian},
  journal={arXiv preprint arXiv:2302.09664},
  year={2023}
}

@article{lee2022factuality,
  title={Factuality enhanced language models for open-ended text generation},
  author={Lee, Nayeon and Ping, Wei and Xu, Peng and Patwary, Mostofa and Fung, Pascale N and Shoeybi, Mohammad and Catanzaro, Bryan},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={34586--34599},
  year={2022}
}

@article{li2024inference,
  title={Inference-time intervention: Eliciting truthful answers from a language model},
  author={Li, Kenneth and Patel, Oam and Vi{\'e}gas, Fernanda and Pfister, Hanspeter and Wattenberg, Martin},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{lin2022teaching,
  title={Teaching models to express their uncertainty in words},
  author={Lin, Stephanie and Hilton, Jacob and Evans, Owain},
  journal={arXiv preprint arXiv:2205.14334},
  year={2022}
}

@article{lin2023generating,
  title={Generating with confidence: Uncertainty quantification for black-box large language models},
  author={Lin, Zhen and Trivedi, Shubhendu and Sun, Jimeng},
  journal={arXiv preprint arXiv:2305.19187},
  year={2023}
}

@article{manakul2023selfcheckgpt,
  title={Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models},
  author={Manakul, Potsawee and Liusie, Adian and Gales, Mark JF},
  journal={arXiv preprint arXiv:2303.08896},
  year={2023}
}

@article{mielke2022reducing,
  title={Reducing conversational agents’ overconfidence through linguistic calibration},
  author={Mielke, Sabrina J and Szlam, Arthur and Dinan, Emily and Boureau, Y-Lan},
  journal={Transactions of the Association for Computational Linguistics},
  volume={10},
  pages={857--872},
  year={2022},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@article{nikitin2024kernel,
  title={Kernel Language Entropy: Fine-grained Uncertainty Quantification for LLMs from Semantic Similarities},
  author={Nikitin, Alexander and Kossen, Jannik and Gal, Yarin and Marttinen, Pekka},
  journal={arXiv preprint arXiv:2405.20003},
  year={2024}
}

@article{tian2023fine,
  title={Fine-tuning language models for factuality},
  author={Tian, Katherine and Mitchell, Eric and Yao, Huaxiu and Manning, Christopher D and Finn, Chelsea},
  journal={arXiv preprint arXiv:2311.08401},
  year={2023}
}

@article{tian2023just,
  title={Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback},
  author={Tian, Katherine and Mitchell, Eric and Zhou, Allan and Sharma, Archit and Rafailov, Rafael and Yao, Huaxiu and Finn, Chelsea and Manning, Christopher D},
  journal={arXiv preprint arXiv:2305.14975},
  year={2023}
}

@article{varshney2023stitch,
  title={A stitch in time saves nine: Detecting and mitigating hallucinations of llms by validating low-confidence generation},
  author={Varshney, Neeraj and Yao, Wenlin and Zhang, Hongming and Chen, Jianshu and Yu, Dong},
  journal={arXiv preprint arXiv:2307.03987},
  year={2023}
}

@article{xiong2023can,
  title={Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms},
  author={Xiong, Miao and Hu, Zhiyuan and Lu, Xinyang and Li, Yifei and Fu, Jie and He, Junxian and Hooi, Bryan},
  journal={arXiv preprint arXiv:2306.13063},
  year={2023}
}

@article{yin2024characterizing,
  title={Characterizing truthfulness in large language model generations with local intrinsic dimension},
  author={Yin, Fan and Srinivasa, Jayanth and Chang, Kai-Wei},
  journal={arXiv preprint arXiv:2402.18048},
  year={2024}
}

@article{zou2023representation,
  title={Representation engineering: A top-down approach to ai transparency},
  author={Zou, Andy and Phan, Long and Chen, Sarah and Campbell, James and Guo, Phillip and Ren, Richard and Pan, Alexander and Yin, Xuwang and Mazeika, Mantas and Dombrowski, Ann-Kathrin and others},
  journal={arXiv preprint arXiv:2310.01405},
  year={2023}
}

