
\section{Experiments}\label{sec: exp}



\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.99\linewidth]{figures/two_stage/budget/auroc/squad.pdf}
    
    \includegraphics[width=0.99\linewidth]{figures/two_stage/budget/auroc/triviaqa.pdf}

    \includegraphics[width=0.99\linewidth]{figures/two_stage/budget/auroc/nq.pdf}
    \vspace{-.2cm}
    \caption{ We plot AUROC against the \emph{relative additional cost} for SQuAD (top), TriviaQA (middle), and Natural Questions (bottom). Solid curves represent results where the validation set consists of independent samples for the same questions, while dashed curves correspond to validation sets consisting of answers for different questions. The dotted horizontal line indicates the approximate ceiling performance using GNN. The curves are mostly convex, indicating that a small number of cross-model consistency checks contribute to most of the performance improvement. This demonstrates that our approach can achieve high performance with very low cost, which is further evidenced in Table \ref{tab: min_p}.
    %Key observations include: (1) \llamathreeseventy{}, the largest model considered, achieves the best results given a large computation budget; (2) \mixtral{} is the most budget-efficient, demonstrating sharp performance gains under small computation budgets, sometimes even on par with \llamathreeseventy{} with $p=1$; (3) the selection of thresholds transfers well to different questions, as evidenced by the small gap between the solid and dashed curves; and (4) results are very close to the approximate ceiling. 
    Results for AURAC are in Fig. \ref{fig: budget_aurac}. 
    \looseness=-1 }
    \label{fig: budget_auroc}
    \vspace{-.2cm}
\end{figure*}


%\subsection{Experimental setup}

\textbf{Datasets.} We consider datasets widely used in research on hallucination detection \cite{kuhn2023semantic,farquhar2024detecting,lin2023generating,nikitin2024kernel}. These datasets focus on reading comprehension and question-answering tasks, including TriviaQA \cite{2017arXivtriviaqa} for trivia knowledge, SQuAD \cite{rajpurkar-etal-2016-squad} for general knowledge, and Natural Questions \cite{47761}, derived from real user queries to Google Search. 

\textbf{Models.} We consider: \llamathreeseventy{}, \llamatwoseventy{} \llamatwothirteen{}, \mixtral{} and \merlinite{}, which results in 20 target-verifier combinations. Following prior works \cite{lin2023generating,kuhn2023semantic,nikitin2024kernel}, we use \texttt{deberta-v2-xlarge-mnli} as the entailment estimator and take the post-softmax probability corresponding to the `Entailment' label as the output (ranging from 0 to 1). \looseness=-1

\textbf{Evaluation.} We set $\tau=0.1, \tau'=1.0, m=10$.  To obtain the ground truth annotations for hallucination, we use GPT-4 as the judge. Performance is measured in terms of AUROC and AURAC as described in Section \ref{sec: exp}. Since this requires a validation set, we compare two scenarios: (1) the validation set consists of the same questions as the test set but with independently sampled random answers, and (2) the validation set consists of different questions from the same dataset. In both scenarios, the sizes of the validation and test sets are 400. First scenario represents an oracle setting, as the thresholds rely on the ground truth for the same questions. The second scenario reflects a practical setting where a separate set of questions is used to determine thresholds. We repeat the test with 5 random seeds and report the average result along with the standard deviation. 


% To effectively evaluate the performance of the algorithm, we propose the following method for calculating these metrics. Specifically, given a sample of questions and their ground-truth annotations, we partition the range of $X$ (i.e., [0, 1]) into $L$ small intervals. Within each interval, we select the thresholds that yield the highest $Y$. The quantiles corresponding to these thresholds are recorded, and the process can be expressed as searching for the following optimal quantiles, $r_1*$ and $r_2*$, for each $k = 0, 1, \dots, L-1$:
% \begin{align}
%     \nonumber
%     r_1*, r_2* = \arg\max_{r_1, r_2} Y\!\left( \tilde{\gD} , Q(r_1), Q(r_2) \right)  \\
%     \nonumber
%     s.t.~~ X\!\left(\tilde{\gD} , Q(r_1), Q(r_2) \right) \in  [\frac{k}{L}, \frac{k+1}{L} ).
% \end{align}
% After determining these optimal $r_1*$ and $r_2*$ values, we then evaluate our algorithm on a test set $\hat{\gD}$ using the corresponding thresholds.

\textbf{Estimation of computational overhead.} Given that Alg. \ref{alg: two_stage} focuses on budget awareness, it is important to consider both performance and budget in our evaluation. To quantify the additional computational cost introduced in Alg. \ref{alg: two_stage} compared to the case where only self-consistency is used, we define a metric called \emph{relative additional cost}:
$$
\frac{\text{FLOPs(Alg. \ref{alg: two_stage})$-$ FLOPs(only self-consistency checking)} }{\text{FLOPs(only self-consistency checking)}},
$$
which can be estimated as $ \frac{pN_{v}}{N_{t}}$ using the FLOP estimation formula from \cite{kaplan2020scaling} (detailed derivation Appx \ref{apdx: estimation}), Here, $N_{t}$ and $N_{v}$ represent the number of non-embedding parameters in the target and verifier models, respectively.  $p$ accounts for the fact that the verifier model is queried for only a fraction $p$ of the questions in Alg. \ref{alg: two_stage}. \looseness=-1

%\subsection{Results and discussion}



\textbf{Performance vs. cost.} In Figs. \ref{fig: budget_auroc} (AUROC) and \ref{fig: budget_aurac}  (AURAC) (in the supplement), we plot the detection performance against the estimated \emph{relative additional cost} when varying $p$. A general trend observed is that when the verifier model is stronger than the target model (e.g., when the target is \merlinite{} and the verifier is \llamathreeseventy{}), increasing the computational budget—by allowing more verifier calls controlled by a larger $p$—monotonically improves performance. However, with a weaker verifier (e.g., when the target is \llamathreeseventy{} and the verifier is \llamatwothirteen{}), we observe an increasing-decreasing trend, where an intermediate number of verifier calls achieves the best results. This aligns with the intuition from Fig. \ref{fig: weighted_avg}, where intermediate weights on self-consistency cross-model-consistency are optimal, indicating that even a weaker verifier can contribute meaningfully to detection when an appropriate balance is maintained.\looseness=-1

\begin{table}[!t]
    \centering
    \caption{ The minimum \( p \) (percentage of verifier calls) required to achieve $\alpha\%$ of the maximal AUROC gain (denoted as \( p(\alpha) \)) when \llamathreeseventy{} is the verifier, for different values of \( \alpha \). We report the average results across all target models on SQuAD (S), TriviaQA (T) and Natural Questions (N). The last column shows the maximal AUROC gain ($\Delta_{\text{max}}$).}
    \label{tab: min_p}
    \vspace{-.2cm}
    %\small
    \begin{tabular}{|c|c|c|c|c|c|}
    \hline
         & $p(70)$ & $p(80)$ & $p(90)$ & $p(95)$ & $\Delta_{\text{max}}$ \\
         \hline
        S & $48.5_{\pm 7}$ & $62.5_{\pm 11}$ & $83.0_{\pm 5}$ & $86.5_{\pm 5}$ & $0.1_{\pm 0.03}$ \\
        \hline
        T & $43.0_{\pm 3}$ & $53.0_{\pm 2}$ & $70.5_{\pm 8}$ & $78.5_{\pm 10}$ & $0.1_{\pm 0.02}$ \\
        \hline
        N & $39.5_{\pm 6}$ & $52.0_{\pm 10}$ & $65.0_{\pm 6}$ & $72.0_{\pm 6}$ & $0.07_{\pm 0.00}$ \\
        \hline
    \end{tabular}
    \vspace{-.4cm}
\end{table}

\textbf{Our approach can achieve high performance with very low computational cost.} The curves in Fig. \ref{fig: budget_auroc} are convex, indicating that a small number of cross-model consistency checks contributing to most of the improvement in performance. To further illustrate this, we examine the minimum \( p \) required to achieve different percentages of performance gains when using \llamathreeseventy{} as the verifier in Table \ref{tab: min_p}. Notably, compared to querying the verifier for all questions, we can reduce the cost by 13.5\%–28\% while retaining 95\% of the gain ($\Delta_{\max}$) in performance. 

\textbf{Selection of the verifier.} \llamathreeseventy{} (blue), consistently achieves the best results when the computational budget is large. However, \mixtral{} (purple), stands out for its exceptional performance with a very small cost. This efficiency can be attributed to its MoE based design—despite having 46.7B total parameters, it only uses 12.9B parameters per token.%\looseness=-1

%\ym{We need to mention here below that we are back to comment on Figure 6}

\textbf{Transferability of thresholds.} The gap between the scenarios where the validation set contains the same questions or different questions is overall small (dashed vs. solid lines, Fig \ref{fig: budget_auroc}), suggesting that the selection of thresholds transfers well to different questions, making the approach practical.


\textbf{Comparison with approximated ceiling performance.} {The gap between our approach and the ceiling performance  approximate using supervised learning with GCNs (described in Sec. \ref{sec: improvement_ceiling_cross}; horizontal dashed line, Fig \ref{fig: budget_auroc}) is generally small.} This indicates that our method effectively utilizes the verifier's information despite not always querying it.\looseness=-1
