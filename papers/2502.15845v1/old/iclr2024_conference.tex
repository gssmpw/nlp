
\documentclass{article} % For LaTeX2e
\usepackage{iclr2024_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors


\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{bbm}
\usepackage{mathrsfs} 

\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{graphbox}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{comment}


\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amssymb}

\usepackage{minted}

\usepackage{adjustbox}


\newcommand{\mself}{\mP_{\text{self}}}
\newcommand{\mcross}{\mP_{\text{cross}}}

\newcommand{\yx}[1]{{{\textcolor{red}{[Y: #1]}}}}



\title{Formatting Instructions for ICLR 2024 \\ Conference Submissions}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Antiquus S.~Hippocampus, Natalia Cerebro \& Amelie P. Amygdale \thanks{ Use footnote for providing further information
about author (webpage, alternative address)---\emph{not} for acknowledging
funding agencies.  Funding acknowledgements go at the end of the paper.} \\
Department of Computer Science\\
Cranberry-Lemon University\\
Pittsburgh, PA 15213, USA \\
\texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
\And
Ji Q. Ren \& Yevgeny LeNet \\
Department of Computational Neuroscience \\
University of the Witwatersrand \\
Joburg, South Africa \\
\texttt{\{robot,net\}@wits.ac.za} \\
\AND
Coauthor \\
Affiliation \\
Address \\
\texttt{email}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
The abstract paragraph should be indented 1/2~inch (3~picas) on both left and
right-hand margins. Use 10~point type, with a vertical spacing of 11~points.
The word \textsc{Abstract} must be centered, in small caps, and in point size 12. Two
line spaces precede the abstract. The abstract must be limited to one
paragraph.
\end{abstract}

\input{method}

\section{Opportunities for theory}
\begin{itemize}
\item Simple generalization bound on how much PFA/PD can shift from picking thresholds for the hatPFA/hatPD from a validation set of size $n$.
\item KL divergence between cross/self distributions - think what this may be useful for?
\item Intuition-building assumption set for distributions on semantic space, leading to propositions saying that the jump in entropy/mean entailment/whatever by going from self-only to self-cross is larger when at least one model is hallucinating. Perhaps saying that if $X\%$ of the mass is on the true semantic atom, then the entropy increase is bounded by something. 
\end{itemize}


\subsection{ROC generalization}
Setup: 
\begin{itemize}
    \item $n_{neg}$ i.i.d. samples from the non-hallucinating distribution and $n_{pos}$ i.i.d. samples from the hallucinating distribution. 
    \item Sets of thresholds $\mathcal{T}_1 = \{t^1_j\}$ and $\mathcal{T}_2 = \{t^2_k\}$ for stages 1 and 2 respectively. 
\end{itemize}
A classifier $C$ maps points to binary predictions. The $p_D(C)$ is the probability that positive samples are mapped to 1, and $p_{FA}(C)$ is the probability that negative samples are mapped to 1. Hence
\[
\hat{p}_D = \frac{1}{n_{neg}} \sum_{i} X^{neg}_i
\]
\[
\hat{p}_{FA} = \frac{1}{n_{pos}} \sum_{i} X^{pos}_i
\]
where $X^{neg}_i$, $X^{pos}_i$ are the labels of the positive and negative samples, respectively. These are each i.i.d. Bernoulli random variables. 

The Hoeffding inequality can be directly applied (see wikipedia):
\[
Pr(|\hat{p}_D - p_D| \geq \epsilon) \leq 2 \exp\left(-2{\epsilon^2}{n_{neg}}\right)
\]
and similarly for $p_{FA}$. Now, we want a uniform bound that holds simultaneously for all possible threshold combinations. The number of these are $|\mathcal{T}_1| |\mathcal{T}_2|$.

Define events $\mathcal{E}^D_{j,k}$ and $\mathcal{E}^{FA}_{j,k}$ to be the events that the estimated probabilities under the $j$th, $k$th threshold are within $\epsilon$, i.e.
\begin{align*}
\mathcal{E}^D_{j,k}&:= |\hat{p}_D(t^1_j,t^2_k) - p_D(t^1_j,t^2_k)| \leq \epsilon\\
\mathcal{E}^{FA}_{j,k}&:= |\hat{p}_{FA}(t^1_j,t^2_k) - p_{FA}(t^1_j,t^2_k)| \leq \epsilon.
\end{align*}
Let's use the union bound across the $j,k$, we don't need it across FA/D, since these are independent. 

\begin{align*}
&Pr\left(\cap_{j=1}^{|\mathcal{T}_1|}\cap_{k=1}^{|\mathcal{T}_2|} (\mathcal{E}^D_{j,k}\cap \mathcal{E}^{FA}_{j,k})\right) 
\geq \left(1 - 2|\mathcal{T}_1||\mathcal{T}_2| \exp\left(-{2\epsilon^2}{n_{neg}}\right)\right)\left(1 - 2|\mathcal{T}_1||\mathcal{T}_2| \exp\left(-{2\epsilon^2}{n_{pos}}\right)\right) \\
%\geq 1 - 2|\mathcal{T}_1| |\mathcal{T}_2|\left(\exp\left(-{2\epsilon^2}{n_{neg}}\right) + \exp\left(-{2\epsilon^2}{n_{pos}}\right)\right)\\
%&=1 - 2 \left(\exp\left(\log(|\mathcal{T}_1|) + \log(|\mathcal{T}_2|)-{2\epsilon^2}{n_{neg}}\right) + \exp\left(\log(|\mathcal{T}_1|) + \log(|\mathcal{T}_2|)-{2\epsilon^2}{n_{pos}}\right)\right)
\end{align*}
In other words, 
\[
\max_{j,k} \max(|\hat{p}_D(t^1_j,t^2_k) - p_D(t^1_j,t^2_k)|, |\hat{p}_{FA}(t^1_j,t^2_k) - p_{FA}(t^1_j,t^2_k)|) \leq \sqrt{\frac{\log(|\mathcal{T}_1|) + \log(|\mathcal{T}_2|)}{\min(n_{neg}, n_{pos})}},
\]
with probability at least $\left(1 - \frac{2}{|\mathcal{T}_1| |\mathcal{T}_2|}\right)^2$. Since this holds simultaneously for all threshold combinations, if we take the training-convex-hull area, the test AUC from using the frontier of that hull will be at most $2\epsilon $ smaller. 

For sample complexity, this means we need to have
\[
n_{neg}, n_{pos} = \Omega\left(\log(|\mathcal{T}_1|) + \log(|\mathcal{T}_2|)\right)
\]
to guarantee the test AUC is close to the train convex-hull-AUC. 


\bibliography{iclr2024_conference}
\bibliographystyle{iclr2024_conference}

\appendix

\end{document}
