\section{Related Work}
There are works that explore white-box detection methods, such as **Santos, "Unsupervised Detection of Adversarial Samples to Defend Against Attack"**, which require token-level logits, or **Carlini, "Towards Evaluating the Robustness of Neural Networks"**, which rely on intermediate representations. White-box methods are less suitable for certain scenarios, such as closed-source commercial APIs. In this work, we focus exclusively on black-box hallucination detection, where we do not have access to the internal workings of the LLM. In this scenario, the primary approach involves checking the consistency between multiple samples of the LLM's answers **Kassiano, "Black-Box Adversarial Attacks with Limited Queries"**. These works rely on sampling multiple answers to the same question from the LLM and using an NLI (Natural Language Inference) model to determine whether they are semantically equivalent. The NLI judgments are then processed in various ways to decide whether a hallucination has occurred. Details of these methods are discussed in Section \ref{sec:3}. **Jiang, "N-gram Based Method for Adversarial Attacks Detection"** also explore alternative methods for judging semantic equivalence, which are either less effective (e.g., n-gram) or computationally expensive (e.g., using an LLM). **Dong, "Adversarial Attacks and Defenses in Deep Learning Systems"** identify limitations in self-consistency-based methods and propose leveraging question perturbation and cross-model response consistency (comparing an LLM's responses with those from an additional verifier LLM) to improve performance. While their approach improves results, introducing a verifier model adds computational overhead. In this work, we systematically explore the possibility of achieving computational efficiency when combining self-consistency and cross-model consistency. Note that the question perturbation technique from **Xie, "Question Perturbation for Adversarial Attacks Detection"** is orthogonal to our approach and could potentially be incorporated to achieve better results. Another line of work involves directly asking LLMs to judge the uncertainty of their answers **Gal, "A Theoretically Grounded Method for Time Series Forecasting"**, which typically requires additional finetuning/calibration and does not fit within the black-box scenario. Without any modification to the original LLMs, their verbalized confidence is often inaccurate **Hendrycks, "Deep Anomaly Detection with Outlier Exposure"**. The inherent conflict between calibration and hallucination, as theoretically shown in **Guo, "On Calibration of Modern Neural Networks"**, further highlights the limitations of this approach. There are also works addressing hallucination mitigation, such as using RAG **Zellers, "Relevant Sentence Selection for Question Answering"**, inference-time intervention **Hendrycks, "Deep Anomaly Detection with Outlier Exposure"**, or fine-tuning **Wang, "Adversarial Training Methods for Semi-Supervised Text Classification"**, which is beyond the scope of this paper.