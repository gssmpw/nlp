
\section{Experimental Details}


\subsection{Estimation of computation cost}\label{apdx: estimation}

To compute the budget, we use the estimation formula derived in \cite{kaplan2020scaling}, Table 1, which states that the number of FLOPs per token is approximately $2N$ for models with sufficiently large dimensions, where $N$ represents the number of non-embedding parameters in the model. Then, for each question, the computation cost of cross-model consistency checking can be expressed as:
\begin{align}
    \label{eq: flops}
    ml_{q}2N_{verifier}  + m^2 l_{a}2N_{deberta},
\end{align}
where $l_{q}$ is the number of tokens in the question, and  $l_{a}$ is the number of tokens in each answer (assuming all $m$ answers share the same length), $N_{verifier},N_{deberta}$ are the number of parameters of the verifier model and the entailment estimator (i.e., \texttt{deberta-v2-xlarge-mnli}), respectively. This estimation remains challenging due to the variability in the lengths of questions and answers, which depend on each specific question and model. To simplify this, we consider the scaling limit, where—assuming no restrictions—the lengths of questions and answers scale to the respective context lengths of the models processing them (since tokens exceeding the context length are dropped). Notably, the context length of \texttt{deberta-v2-xlarge-mnli} is only 512, while the context lengths of verifier models range from 4,096 to 128,000. Using this, we compute the ratio of the second term to the first term in Equation \ref{eq: flops}, which is $ m\frac{l_{a}}{l_{q}}\frac{N_{deberta}}{N_{verifier}}$, in the limit where both lengths reach their maximum. We find that this ratio is no greater than 0.087 in the largest case in our settings, indicating a negligible contribution of the second term in the limit. Thus, we omit the second term to simplify the estimated additional computation per question to $ml_{q}2N_{verifier}$. Similarly, we can derive the estimation for the cost of self-consistency checking as $ml_{q}2N_{target}$. Based on this simplification, the metric \emph{additional relative cost} can be estimated as $ \frac{pN_{verifier}}{N_{target}}$, where  $p$ accounts for the fact that the verifier model is queried for only a fraction $p$ of the questions in Algorithm \ref{alg: two_stage}. 

\subsection{Additional Results}\label{apdx: additional_exp}

Figure \ref{fig: methods_vs_gcn_rac} shows the comparison between the performance of existing methods and the approximated ceiling performance in terms of AURAC. We observe a pattern consistent with that in Figure \ref{fig: methods_vs_gcn_roc}.


Figure \ref{fig: weighted_avg_aurac} shows the AURAC performance against $\lambda$ when using a weighted average of self-consistency and cross-consistency-based metrics, $(1-\lambda) \mpd(\mself) + \lambda \mpd(\mcross)$.

Figure \ref{fig: budget_aurac} shows the AUROC performance of the two-stage detection method under varying computational budgets.

\begin{figure*}[!t]
    \centering
    \subfigure[\llamatwothirteen  \label{}]{
        \includegraphics[width=0.3\textwidth]{figures/method_vs_gcn/squad/meta-llama_llama-2-13b-chat_aurac.pdf}
    }
    \subfigure[ \llamathreeseventy \label{}]{
        \includegraphics[width=0.3\textwidth]{figures/method_vs_gcn/squad/meta-llama_llama-3-70b-instruct_aurac.pdf}
    }
    % Third subfigure
    \subfigure[ \mixtral \label{}]{
        \includegraphics[width=0.3\textwidth]{figures/method_vs_gcn/squad/mistralai_mixtral-8x7b-instruct-v01_aurac.pdf}
    }

    \subfigure[\llamatwothirteen\label{}]{
        \includegraphics[width=0.3\textwidth]{figures/method_vs_gcn/triviaqa/meta-llama_llama-2-13b-chat_aurac.pdf}
    }
    \subfigure[\llamathreeseventy\label{}]{
        \includegraphics[width=0.3\textwidth]{figures/method_vs_gcn/triviaqa/meta-llama_llama-3-70b-instruct_aurac.pdf}
    }
    % Third subfigure
    \subfigure[\mixtral\label{}]{
        \includegraphics[width=0.3\textwidth]{figures/method_vs_gcn/triviaqa/mistralai_mixtral-8x7b-instruct-v01_aurac.pdf}
    }
    \caption{Comparison between the performance of existing methods and the approximated ceiling performance on the SQuAD ((a)–(c)) and TriviaQA ((d)–(f)) datasets. Here, performance is measured in terms of AURAC. We observe a pattern consistent with the discussion in Figure \ref{fig: methods_vs_gcn_roc}. \looseness=-1}
    \label{fig: methods_vs_gcn_rac}
\end{figure*}


\begin{figure}[!t]
\centering
\includegraphics[width=0.25\linewidth]{figures/weighted_avg/aurac/squad/meta-llama_llama-2-13b-chat/meta-llama_llama-3-70b-instruct/multiple.pdf}
\includegraphics[width=0.25\linewidth]{figures/weighted_avg/aurac/squad/meta-llama_llama-3-70b-instruct/meta-llama_llama-2-13b-chat/multiple.pdf}
\includegraphics[width=0.25\linewidth]{figures/weighted_avg/aurac/squad/mistralai_mixtral-8x7b-instruct-v01/meta-llama_llama-2-13b-chat/multiple.pdf}

\includegraphics[width=0.25\linewidth]{figures/weighted_avg/aurac/squad/meta-llama_llama-2-13b-chat/mistralai_mixtral-8x7b-instruct-v01/multiple.pdf}
\includegraphics[width=0.25\linewidth]{figures/weighted_avg/aurac/squad/meta-llama_llama-3-70b-instruct/mistralai_mixtral-8x7b-instruct-v01/multiple.pdf}
\includegraphics[width=0.25\linewidth]{figures/weighted_avg/aurac/squad/mistralai_mixtral-8x7b-instruct-v01/meta-llama_llama-3-70b-instruct/multiple.pdf}
\caption{A simple weighted average of self-consistency and cross-consistency-based metrics, $(1-\lambda) \mpd(\mself) + \lambda \mpd(\mcross)$, can achieve performance close to that of the oracle method.}
\label{fig: weighted_avg_aurac}
\end{figure}


\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.99\linewidth]{figures/two_stage/budget/aurac/squad.pdf}
    
    \includegraphics[width=0.99\linewidth]{figures/two_stage/budget/aurac/triviaqa.pdf}

    \includegraphics[width=0.99\linewidth]{figures/two_stage/budget/aurac/nq.pdf}
    \caption{ AURAC vs. relative additional cost for SQuAD (top), TriviaQA (middle), and Natural Questions (bottom).  }
    \label{fig: budget_aurac}
\end{figure*}
