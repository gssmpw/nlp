\section{Conclusion}

In this paper, we empirically analyzed consistency-based methods for hallucination detection. Based on this analysis, we introduced a budget-aware two-stage approach that leverages both self-consistency and cross-model consistency with a given verifier. Our method reduces computational cost by selectively querying the verifier. Extensive experiments demonstrate that it achieves strong performance with minimal computation, notably reducing computation by up to 28\% while retaining 95\% of the maximal performance gain.\looseness=-1

% The verifier is sparsely used, depending on the uncertainty of the self consistency. This leads us to a light weight detection method that maintains the performance of an oracle, for example using as a verifier ... uses both self and cross consistencies with a speedup up to \ym{$x \%$}. 


\section*{Acknowledgement}
This research was partially supported by the National Science Foundation CAREER Award 2146492.

% \section*{Impact Statement}
% Hallucination remains a problem in Large Language models, we introduce in this work a light weight method for hallucination detection. We see positive societal impacts of our methods. It is important to note that those detection methods may have false positives and false negatives and hence verification of outputs of the LLMs is still imperative.  
%This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.