%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
%%% ourpackages

%\usepackage[margin=1in]{geometry} % Adjust the page margins if necessary  % For adjusting margins
\input{symbols}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}


\usepackage{adjustbox} % check if we can use this package
\usepackage{subcaption}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

% further packages

\usepackage{xcolor}  
\usepackage[most]{tcolorbox} 
\usepackage{tabularx} 
\usepackage{xspace}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{hypothesis}{Hypothesis}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}
\usepackage{cleveref}
\usepackage{thmtools}
\usepackage{amsthm} 
\usepackage{multirow} % Required for \multirow  
\usepackage{svg}
\usepackage{siunitx}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Implicit Language Models are RNNs: Balancing Parallelization and Expressivity}

\begin{document}

\twocolumn[
%\icmltitle{Infinite Depth with Finite Compute: A scalable approach to lifting Transformer and SSM into RNN}
%\icmltitle{Infinite Depth with Finite Compute: Lifting SSMs and Transformers into RNNs}
\icmltitle{Implicit Language Models are RNNs: Balancing Parallelization and Expressivity}
% ideas: 
% Implicit State-Space Models are RNNs
% 

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Mark Sch√∂ne}{equal,tud,msr,intern}
\icmlauthor{Babak Rahmani}{equal,msr}
\icmlauthor{Heiner Kremer}{msr}
\icmlauthor{Fabian Falck}{msr}
\icmlauthor{Hitesh Ballani}{msr}
\icmlauthor{Jannes Gladrow}{msr}
% \icmlauthor{Firstname1 Lastname1}{equal,yyy}
% \icmlauthor{Firstname2 Lastname2}{equal,yyy,comp}
% \icmlauthor{Firstname3 Lastname3}{comp}
% \icmlauthor{Firstname4 Lastname4}{sch}
% \icmlauthor{Firstname5 Lastname5}{yyy}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
% %\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{tud}{Chair of Highly-Parallel VLSI Systems and Neuro-Microelectronics , TUD Dresden University of Technology, Dresden, Germany}
\icmlaffiliation{msr}{Microsoft Research, Cambridge, United Kingdom}
\icmlaffiliation{intern}{Parts of this project were conducted as an intern at Microsoft Research, Cambridge, United Kingdom}

\icmlcorrespondingauthor{Jannes Gladrow}{jannes.gladrow@microsoft.com}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document


\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution

\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.


\begin{abstract}
State-space models (SSMs) and transformers dominate the language modeling landscape. However, they are constrained to a lower computational complexity than classical recurrent neural networks (RNNs), limiting their expressivity. %, while the latter lack training parallelizability. 
In contrast, RNNs lack parallelization during training, raising fundamental questions about the trade off between parallelization and expressivity.
We propose \emph{implicit SSMs}, which iterate a transformation until convergence to a fixed point.
Theoretically, we show that implicit SSMs implement the non-linear state-transitions of RNNs.
Empirically, we find that only approximate fixed-point convergence suffices, enabling the design of a scalable training curriculum that largely retains parallelization, with full convergence required only for a small subset of tokens.
Our approach demonstrates superior state-tracking capabilities on regular languages, surpassing transformers and SSMs.
We further scale implicit SSMs to natural language reasoning tasks and pretraining of large-scale language models up to 1.3B parameters on 207B tokens--representing, to our knowledge, the largest implicit model trained to date. Notably, our implicit models outperform their explicit counterparts on standard benchmarks.

\end{abstract}

\section{Introduction}
\begin{figure}[!htb]
    \centering
    %\includegraphics[width=\linewidth]{figures/S5/s5-word-problem.pdf}
    \includegraphics[width=\linewidth]{figures/opening-figure.pdf}
    \caption{
        \textbf{Left:} Minimum layers required to solve the $S_5$ word problem, a theoretically hard formalization of state tracking, for different sequence lengths.
        %A single layer suffices for our implicit Mamba2, 
        %while Mamba2 requires linearly increasing number of layers as sequences become longer.
        \textbf{Right:} Length generalization for Mamba2 and our implicit Mamba2 trained on $L=32$ and extrapolated up to $L=128$.
        \textbf{Bottom:} Scaling of language models pretrained on $207$B tokens of the deduplicated \textsc{Pile}.
        %Our implicit models outperform their conventional counterparts, with a slight advantage for Llama over Mamba2.
    }
    \label{fig:opener}
\end{figure}

Transformers, despite their dominance on contemporary language benchmarks, exhibit fundamental limitations in computational expressiveness. Both theoretically and empirically, they cannot fully recognize regular languages~\cite{bhattamishra2020ability} or, equivalently, represent finite state machines (FSMs) ~\citep{merrill2022saturated}. This limitation is significant because FSMs form the backbone of many real-world state-tracking problems, including evaluating code, tracking object permutations (e.g., in games like chess or structured narratives), and modeling sequential dependencies in logic~\cite{li2021implicit}, location tracking~\cite{guan2023leveraging}, games~\cite{li2023emergent} and scientific applications such as protein generation, genetics, and chemistry~\cite{briand2023dna, chowdhury2022single, boiko2023autonomous}. This raises questions about the ability of transformers to maintain coherent world models based on transitions between states~\cite{vafa2024evaluating} and hence, their suitability for tasks requiring robust state-tracking. These shortcomings appear to stem from a fundamental trade-off between parallelizability at training time and the ability to track state~\cite{Merrill_Sabharwal_2023}.


Surprisingly, recently emerging state-space models (SSM), a class of \emph{linear} recurrent neural networks, are bound by the same trade-off: despite their seemingly sequential nature they cannot express some inherently sequential problems such as certain regular languages~\cite{merrill2024illusion}.
% by standard conjectures % (see \Cref{sec:Background} for details).
In contrast, non-linear recurrent neural networks (RNNs) 
are not bound by these restrictions on compute complexity and can track state~\cite{siegelmann1992,merrill2019sequential} but lack parallelizability at scale.
This raises the question:
\textit{How much sequential processing does one have to accept to solve the state tracking problem?}

Previous attempts to address these limitations in transformers have leveraged non-linear transitions through self-iteration in the depth dimension~\cite{dehghani2018universal, Banino_Balaguer_Blundell_2021}.
However, backpropagation through unrolled networks is computationally prohibitive at scale. 
Deep equilibrium (DEQ) models~\cite{Bai_Kolter_Koltun_2019}, in contrast, define a function \emph{implicitly} via the fixed-points of a neural network; their output is the result of self-iteration until convergence. 
Training such networks requires backpropagation solely at the fixed point, eliminating the need to traverse the iterative path and thereby decoupling memory usage from the depth of iterations.
Emerging hardware promising rapid computation of fixed-points of neural networks~\cite{photonics_roadmap} may tilt the hardware lottery~\cite{Hooker_2020} in favor of such implicit models, making this an opportune moment to explore their potential.

 
Our approach to balancing state tracking and parallelization relies on two key observations. 
First, we demonstrate that implicit models naturally adapt their compute load to the difficulty of the learning problem (see \Cref{fig:word-problem}\textbf{Left}).
At both training and test time, such models effectively interpolate between their parallelizable form, when all tokens in the sequence are resolvable, and RNNs, when there are no resolvable tokens. Further, we show theoretically that implicit models have indeed non-linear token-to-token transitions similar to RNNs. Second, based on the success of transformers on many practical language modeling problems, we hypothesize that natural language contains only a sparse set of tokens that cannot be resolved by transformers (and SSMs).
Such {non-solvable} transitions are critical for state tracking but remain intractable for the class of circuits representable by transformers and SSMs~\citep{merrill2022saturated,merrill2024illusion}.
Exploiting these properties, we devise implicit models that combine the expressive power of RNNs with the parallelizability of transformers and SSMs (see \Cref{fig:duality}).
In contrast to conventional transformers and SSMs, implicit models can track state, even out-of-distribution (see \Cref{fig:opener}\textbf{Right}).
In contrast to RNNs, these models permit a much larger degree of parallelization as the depth of self-iteration is much smaller than the sequence length (see \Cref{fig:word-problem}\textbf{Mid}).

\par\noindent
\textbf{Contributions.} 
\textbf{(a)} We propose implicit SSMs and show theoretically that they represent non-linear and non-diagonal state-to-state transitions similar to RNNs.
\textbf{(b)} We confirm empirically that implicit SSMs can solve the $S_5$ word problem, which conventional SSMs and transformers fail to solve.
\textbf{(c)} We show by constructing distributions with varying difficulty level over the word problem that implicit SSMs as well as transformers require much fewer non-parallelizable transitions to learn word problems than RNNs
\textbf{(d)} We demonstrate scalability of implicit models through a carefully chosen training curriculum that bounds the number of iterations, training implicit SSM and transformers up to 1.3B parameters on 207B tokens of the deduplicated \textsc{Pile} (\textsc{D-Pile})~\cite{gao2020pile}--- see \Cref{fig:opener}\textbf{Bottom}, the largest self-iterated model with dynamic halting condition to date, to the best of our knowledge.
\textbf{(e)} We highlight a set of properties of our pretrained implicit language models such as favorable length generalization, and path-independent auto-regressive generation.


\section{Background}
\label{sec:Background}


\subsection{State-Space Models}
\label{sec:State-space Models}
SSMs are linear recurrent models which produce an output $y_t \in \mathbb{R}^{d_\text{out}}$ given an input $x_t \in \mathbb{R}^{d_\text{in}}$ and a sequentially updated hidden state vector $h_t \in \mathbb{R}^n$ via the recurrence
\begin{align}
    h_t &= \Lambda(x_t) h_{t-1} + u(x_t) \label{eq:linear-ssm}\\
    y_t &= f(h_{t-1}, x_t), \label{eq:ssm-forward}
\end{align}
where $u$ and $f$ are possibly non-linear learned functions. % and $f_\theta$ is parametrized by $\theta$.
The learned matrix $\Lambda \in \mathbb{R}^{n \times n}$ is typically diagonal and can be constant \cite{gu2022efficiently, smith2023simplified} or an input-dependent matrix-valued function \citep{qin2023hierarchically, gu2023mamba, dao2024transformers}. 
A SSM combines a number of these blocks with non-linear feed-forward blocks. 
In contrast to non-linear RNNs, the linear state recurrence \eqref{eq:linear-ssm} allows for training parallelism along the sequence dimension,
and avoids the quadratic scaling of self-attention.

\subsection{Limitations of Transformers and SSMs}
\label{sec:limitations-transformer-ssm}
Efficient parallelization is one of the central features enabling transformers and SSMs to scale to large machine learning problems such as language modeling.
Parallel circuits, however, face fundamental trade-offs regarding the class of problems that they can address. 
In particular, transformers and SSMs theoretically fail to recognize certain regular languages, or equivalently, to simulate FSMs~\citep{merrill2022saturated,merrill2024illusion}.
Empirical studies have confirmed that neither of the models are capable of learning the algorithms constituting certain regular languages~\cite{bhattamishra2020ability,sarrof2024expressive}.
By contrast, the sequential nature of RNNs allows them to express all regular languages~\citep{merrill2019sequential}.
A detailed discussion is given in \Cref{sec:circuit-complexity}.

\subsection{Deep Equilibrium Models}
\label{sec:deq}
Most deep learning architectures \textit{explicitly} parametrize a function $x \mapsto y$ with a neural network.
Deep Equilibrium Models (DEQ), in contrast, define a function \textit{implicitly} via the fixed-points of 
an input-conditional neural network, i.e., 
\begin{align}
    z^\ast = F_\theta(z^\ast, x) \label{eq:deq},
\end{align}
where $z^\ast$ is identified with the prediction $y$.
Naively differentiating a loss function $\mathcal{L}(z^\ast)$ with respect to the model parameters $\theta$ generally requires a costly differentiation through the employed fixed-point solver. Instead, to allow for gradient computations with a constant memory footprint, 
DEQs utilize the Implicit Function Theorem:

Let $G_\theta(z, x) = z - F_\theta(z,x)$.
If the Jacobian $J_{G, z}$ of $G$ w.r.t.\ $z$ is non-singular in $z^\ast$,
then there exists an open set $U$ around $(x, \theta)$ and a unique function $\Phi$ on $U$ such that $\Phi(x,\theta) = z^\ast$ and $G(\Phi(\tilde{x}, \tilde{\theta}), \tilde{x}, \tilde{\theta}) = 0$ for all $(\tilde{x}, \tilde{\theta})\in U$. 
Furthermore, the derivative of $\Phi$ w.r.t. $\theta$ is given by
\begin{align}
    \frac{\partial \Phi}{\partial \theta} = - J_{G, z^\ast}^{-1} \frac{\partial F_\theta}{\partial \theta}.
    \label{eq:implicit-diff}
\end{align}
A range of methods have been proposed to efficiently compute $\frac{\partial \loss}{\partial \theta} = \frac{\partial \Phi}{\partial \theta} \frac{\partial \mathcal{L}}{\partial z^\ast}$ using Equation~\eqref{eq:implicit-diff} \citep{Bai_Kolter_Koltun_2019, geng2021training}.
%
Here, we employ the Phantom Gradient approach of \citet{geng2021training} (see in the Appendix~\Cref{fig:deq}). The method is based on solving a smoothed version of the fixed point equation \eqref{eq:deq} combined with a finite truncation of the von Neumann series of the Jacobian-vector-product in \eqref{eq:implicit-diff} given as
\begin{align}
    \widehat{\frac{\partial \Phi}{\partial \theta}} = \lambda \frac{\partial F_\theta}{\partial \theta} \Big{|}_{z^\ast} \sum_{i=0}^{k-1} \left( \lambda \frac{\partial F_\theta}{\partial z}\Big{|}_{z^\ast} + (1-\lambda) I \right)^i, \label{eq:phantom-gradient}
\end{align}
where a small smoothing parameter $\lambda \in (0,1]$ helps maintaining a small condition number at the cost of increased fixed-point iterations and the truncation length $k$ determines the accuracy of the approximation.

%
\section{Implicit Sequence Models}
\label{sec:implicit-models}
\subsection{Implicit State-space Models}
\label{sec:Implicit State-space Models}
\begin{figure*}[!htb]
    \centering
    \includegraphics[width=\linewidth]{figures/2025-01-30-duality.pdf}
    \caption{
        %Implicit sequence models admit two modes for unrolling the for-loops introduced in \Cref{sec:implicit-models}.
        \textbf{A:} The simultaneous mode self-iterates the entire sequence such that trajectories interact during convergence. It exploits the parallelism of the backbone model.
        \textbf{B:} The sequential mode iterates each token individually. Only converged hidden states or kv-caches are passed on. This mode is used for generation.
        \textbf{C:} Difference in perplexity between the two modes for our 1.3B implicit models.
    }
    \label{fig:duality}
\end{figure*}
The linear recurrence of SSMs shown in equation~\eqref{eq:linear-ssm} cannot resolve elaborate sequential problems~\cite{merrill2024illusion}.
Here, we propose to exploit self-iterations along the depth of neural networks to close the expressivity gap between SSMs and RNNs.
Following the DEQ paradigm (see \Cref{sec:deq}), we implicitly define a model via the fixed points of a SSM.
Introducing the iteration variable $z_t\step{s}\in\R{d_\text{out}}$ to equations~\eqref{eq:linear-ssm} and \eqref{eq:ssm-forward} yields the fixed point iteration
\begin{align}
    h_t\iter{s} &= \Lambda\func{z_t\iter{s-1}, x_t} h_{t-1}\iter{s} + u\func{z_t\iter{s-1}, x_t} \label{eq:implicit-ssm-recurrence}\\
    z_t\iter{s} &= f_\theta\func{z_t\iter{s - 1}, h_t\iter{s}, x_t} \,,
    \label{eq:implicit-ssm-block}
\end{align}
where ${z_t\step{0} = 0}$ for ${t=0,\dots,T}$ and ${h_0\step{s} = 0}$ for ${s=0,\dots,S}$ respectively.
The output $z\step{s}_t$ of $f_\theta$ is fed back to the self-iteration until (approximate) convergence to a fixed-point.
Note, how this adds a new dependency to the functions $\Lambda$ and $u$ in equation~\eqref{eq:implicit-ssm-recurrence} that is not present in equation~\eqref{eq:linear-ssm}.
Notably, this minor technical change leads to fundamental differences between explicit SSMs and the implicit SSM defined above.


Computing the output, as well as the gradient, of our implicit SSM requires to iterate the two loops defining equations~\eqref{eq:implicit-ssm-recurrence} and \eqref{eq:implicit-ssm-block}:
A loop $t=1,\dots,T$ over the sequence dimension, 
and a loop $s=0,\dots,S$ to find the fixed point.
The two loops give rise to two modes of evaluation visualized in \Cref{fig:duality}.
The \textit{simultaneous mode} simultaneously finds the fixed points for all $t$ (see \Cref{fig:duality}\textbf{A}),
and exploits parallelization strategies for SSMs~\cite{dao2024transformers}.
The \textit{sequential mode} resolves the $s$ and $t$ loops in the transpose order, 
and processes sequences sequentially just like classical SSMs or RNNs (see \Cref{fig:duality}\textbf{B}).
While the simultaneous mode allows for highly parallel training, the sequential mode enables efficient inference at constant memory, e.g. for language generation.
% No matter which mode is used to find the fixed points,
For both modes,
equation~\eqref{eq:implicit-ssm-recurrence} in the limit ${s\rightarrow\infty}$ reads
\begin{align}
        h_t\fp &= \Lambda\func{z_t\fp, x_t} h_{t-1}\fp + u\func{z_t\fp, x_t} \label{eq:implicit-ssm-fixed-points}\,,
\end{align}
where ${z_t\fp = \lim_{s\rightarrow\infty}z_t\step{s}}$ and ${h_t\fp = \lim_{s\rightarrow\infty}h_t\step{s}}$ denote the fixed points.
The fixed point $z_t\fp$ depends on $h_t\fp$, and hence by equation~\eqref{eq:implicit-ssm-block} on $h_{t-1}\fp$.
Notably, our self-iteration introduces a non-linear dependency to the originally linear recurrence~\eqref{eq:linear-ssm} via the % non-linear 
functions $\Lambda$ and $u$.
Thereby, our implicit SSM inherits one of the crucial properties of RNNs, as formalized next.
\iffalse
It is clear that $h_t\iter{s}$ depends on $h_t\iter{s-1}$ through the fixed point iteration and on $h_{t-1}\iter{s}$ via the linear recurrence.
Notably, the fixed point iteration of $z_t\iter{s}$ introduces an additional non-linear dependency of $h_t\iter{s}$ on $h_{t-1}\iter{s-1}$ via $z_t\iter{s-1}$, which we discuss in the following.
\fi

\begin{theorem}
    Consider an implicit SSM defined by equations~\eqref{eq:implicit-ssm-recurrence} and \eqref{eq:implicit-ssm-block}. Then the transition function 
    % $\xi: h_{t-1}\fp \mapsto h_t\fp$ 
    $h_{t-1}\fp \mapsto h_t\fp$ 
    defined by equation~\eqref{eq:implicit-ssm-fixed-points} is non-linear and non-diagonal, 
    i.e. each hidden state $h_t\fp$ is a non-linear function of the previous hidden state $h_{t-1}\fp$.
    Consequently, the state-to-state Jacobian is a non-diagonal operator.
    \label{thm:non-linear}
\end{theorem}

{\textit{Proof:} We refer the reader to \Cref{sec:proof-theorem} \qed}

As discussed in \Cref{sec:limitations-transformer-ssm}, non-linear RNNs surpass transformers and linear SSMs in terms of circuit complexity.
By the above construction, our implicit SSM appears to exhibit the favourable computational properties of RNNs, lifting the illusion of state in linear SSMs ~\cite{merrill2024illusion}.
Furthermore, the gradients of a fixed point iteration depend solely on the fixed point, and not on the path to the fixed point, by the implicit function theorem.
This suggests that both modes resolving the two for loops yield functionally equivalent fixed points.

These properties raise the following hypotheses, which we will investigate empirically in this work.
%
\begin{hypothesis}[Expressivity]
    Implicit SSMs can learn and express all regular languages. %the \nc complete $S_5$ word problem.%
\end{hypothesis}
\begin{hypothesis}[Parallelization]
    Implicit SSMs can be trained in simultaneous mode and evaluated in sequential mode without loss in performance.
\end{hypothesis}

\subsection{Implicit Transformers}
Similar to implicit SSMs, one can define an implicit transformer model~\citep{Bai_Kolter_Koltun_2019} as
\begin{align*}
    z^{(s)}_t = \operatorname{LN}(\operatorname{FFN}(\operatorname{LN}(\operatorname{Attn}(z^{(s-1)}_t W_{QKV} + x_t W_{\text{inp}})))),
\end{align*}
where $W_{QKV} \in \mathbb{R}^{d \times 3d}$ produces the $Q$, $K$, $V$ for the multi-head self-attention (Attn), FFN denotes a feed-forward block, LN stands for layer normalization, and $W_{\text{inp}} \in \mathbb{R}^{d \times 3d}$ is the input projection.
Conventional transformers, with their finite number of layers, cannot learn certain formal languages outside of the \tc circuit complexity class~\citep{merrill2022saturated, strobl2024formal}. 
However, chain of thought (CoT) models~\citep{wei2022chain} bypass this restriction by using an adaptive compute budget through recursive generation of intermediate tokens~\cite{merrillexpressive}. 
Implicit transformers~\cite{Bai_Kolter_Koltun_2019} utilize an adaptive compute budget differently, using fixed-point iterations that can be interpreted as sequences of latent thoughts~\citep{hao2024traininglargelanguagemodels}, undergoing non-linear updates similar to a non-linear RNN's hidden state. 
\section{Implicit SSMs Adapt to Hard Languages}
\label{sec:word-problem}
\paragraph{Implicit SSMs Lift the Illusion of State}
%\label{sec:s5-word-problem}
%
The Illusion of State~\cite{merrill2024illusion} reveals that SSMs cannot simulate arbitrary finite state machines.
A hard state tracking problem in the sense that all state tracking problems can be reduced to it is given by the \textit{word problem} for the symmetric group $S_5$~\cite{barrington1989nc1}.
The word problem for a monoid $\func{M, \circ}$ is to resolve arbitrary length products of the form ${\hat{m} = m_1\cdot m_2\circ \dots\circ m_k}$ for ${m_1, m_2, \dots, m_k\in M, k\in\mathbb{N}}$.
A comprehensive introduction to the word problem and our particular learning setting is provided in \Cref{sec:appendix-word-problem}.

We train a set of Mamba2 SSMs~\cite{dao2024transformers} to reproduce the results of \citet{merrill2024illusion}.
\Cref{fig:opener}\textbf{Left} highlights that Mamba2 requires more layers as the sequences get longer.
For example resolving sequences of \num{32} elements from $S_5$ requires a minimum of \num{16} layers.
Extending the result of \citet{merrill2024illusion}, \Cref{fig:opener}\textbf{Right} shows that the same Mamba2 model with 16 layers does not generalize beyond the training distribution when evaluated on sequences longer than \num{32} elements.
Our implicit Mamba2, however, can utilize additional self-iterations at test-time to resolve longer sequences of up to \num{128} elements.
This result establishes that implicit SSMs effectively learn to be RNNs. 
However, with naive unrolling in implicit SSMs, parallelization would still be challenging.
In the following, we show a subtle yet important result: Implicit SSMs can adapt to word problems of varying difficulty even when trained with bounded depth.
%
\paragraph{Languages with Sparse Non-Solvable Transitions}
SSMs excel in natural language processing tasks despite being theoretically constrained to the simple class of star-free formal languages~\cite{sarrof2024expressive}.
We conject that natural language is mostly composed of simple to comprehend tokens, while harder tokens appear only sparsely.
To study implicit models in a controlled learning environment closer to natural language than the $S_5$ word problem, we construct a word problem that mixes simple and hard examples.
Let $M = M^{a} \times G$ be a direct product of an aperiodic monoid $M^{a}$ and a non-solvable group $G$.
A sequence $m_0, \dots, m_T$ is sampled from $M$ with replacement.
To control the number of hard examples and simple examples, we define a family of distributions \distribution{p} over $M$ as follows.
An element $m^{a}_k\in M^{a}$ is sampled uniformly at each step $k$, representing the presence of simple examples.
On the other hand, we sample elements $g_k\in G\setminus \{e\}$ from $G$ without the identify transformation, each with probability $\frac{p}{\left|G\right|-1}$. 
The identity element $g_k=e\in G$ is sampled with probability $1-p$.
The resulting transformations $\func{m^{a}_k, g_k}$ are aperiodic at least when $g_k = e$, i.e. with probability $1-p$.
\paragraph{Interpolating between SSMs and RNNs}
%\label{sec:ssm-rnn-interpolation}
We will identify minimally sequential models that parallelize to a high degree and still capture all non-solvable transitions in a language.
Therefore, we apply our construction of a word problem above to mix tokens from simple languages with tokens from non-solvable hard languages.
This section studies a word problem over $M=M^{a}\times A5$, where $M^{a}$ is a simple aperiodic monoid with four elements and $A_5\subset S_5$ is the alternating group over 5 elements, the smallest non-solvable subgroup of $S_5$.
For details on the learning problem, we refer the reader to \Cref{sec:appendix-word-problem}.

We train Mamba2 and implicit Mamba2 models on a range of mixtures of simple and hard tokens between $p=0.0$ and $p=0.25$, 
and in the case of the implicit models with varying self-iteration depths at training time between \num{2} and \num{128}.
All training sequences sample $L=256$ tokens, and evaluation is conducted on the distribution \distribution{0.5}, where half of the tokens is hard.
The evaluation is hence an out-of-distribution (OOD) setting.
We report averaged results over 10 random seeds with boostrapped \SI{95}{\percent} confidence intervals as well as the best models per configuration.
None of the conventional models got OOD accuracies beyond random chance as shown in the right panel of \Cref{fig:word-problem}, hence we will focus our discussion on the implicit models in the following.
The left panel of \Cref{fig:word-problem} shows that implicit SSMs capture the underlying algorithm, as measured by out-of-distribution evaluation with $p=0.5$, even when trained on very few non-solvable tokens.
While a fraction of \SI{2}{\percent} hard tokens per sample ($p=0.02$) suffices for some configurations, reliable training can be observed from $p=0.1$ on.
\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figures/monoid-task/word-problem.pdf}
    \caption{
        All models were trained and evaluated on sequences of length $L=256$.
        The out-of-distribution (OOD) evaluation is conducted with $p=50\%$.
        % Confidence intervals (\SI{95}{\percent}) were calculated from 10 runs with bootstrapping.
        \textbf{Left:}
        Comparison of OOD accuracy for a range of training distributions with hard token probabilities $p$.
        \textbf{Mid:}
        Comparison of OOD accuracy for a range of self-iterations caps at training time, trained with $p=0.1$.
        \textbf{Right:}
        Comparison of implicit Mamba2, unrolled Mamba2, and Mamba2 trained with $p=0.1$. 
        Unrolled Mamba2 unrolls a single layer with full backpropagation, while implicit Mamba2 receives only \num{4} Phantom Gradient steps. 
        All models have a training depth of 16 (layers for Mamba2, self-iterations for implicit and unrolled).
        %The comparison shows that the implicit model with $4$ steps of Phantom Gradient succeeds over the unrolled model.
        }
    \label{fig:word-problem}
\end{figure*}


We are left with the question of how many self-iterations are required during training to learn the algorithm intrinsic to the word problem. 
To answer this we trained a range of models with $p=0.1$, setting a different upper bound on the number of self-iterations at training time.
The number of self-iterations at test time is unbounded and solely defined by the fixed point iteration.
The mid panel of \Cref{fig:word-problem} shows that a small amount of down to \num{8} self-iterations at training time suffices to generalize from the distribution \distribution{0.1} at training time to \distribution{0.5} at test time.
Interestingly, the number of test time self-iterations is quite similar for the models trained with different upper bounds on the training time self-iterations,
hinting that the models learned similar algorithms.
Note that the self-iterations required during training are significantly lower than the sequence length.
For comparison, a conventional RNN conducts $L=256$ non-parallelizable steps to solve the same problem, a factor of \num{32} larger than the \num{8} self-iterations required by our implicit Mamba2.
This comes at a cost: we need to self-iterate over every token.
However, each self-iteration can be parallelized across the sequence dimension by the parallelization of the base model.

In the right panel of \Cref{fig:word-problem}, we demonstrate that the phantom gradient is, in most cases, a more effective method for gradient computation than backpropagation through the entire sequence of unrolling steps. 
To evaluate this, we train three variants of the Mamba2 model: 
(1) an implicit Mamba2, which self-iterates and employs phantom gradients; 
(2) an unrolled Mamba2, which backpropagates through all unrolling steps; and 
(3) an explicit Mamba2, a conventional model. 
All models are trained on sequences of length $L=256$ sampled from \distribution{0.1}, with a depth constraint of 16 -- corresponding to 16 self-iterations for the implicit and unrolled models and 16 layers for the explicit model.
Our result shows that a constant number of backpropagation steps using the phantom gradient method is enough to learn complex non-solvable transitions and generalize to difficult distributions at test time. 
Since phantom gradients require a constant memory that is independent of the number of self-iteration steps, the training of larger language models appears feasible. 
%
\paragraph{CatbAbi: A benchmark requiring state tracking.}
\label{sec:catbabi-results}
To evaluate the state-tracking capabilities of SSMs on language tasks, we use the \textsc{CatbAbI} dataset \citep{schlag2020learning}, a modified version of the \textsc{BAbi} dataset \citep{weston2015towards}, consisting of 20 tasks within a 5M token corpus. These tasks, requiring various reasoning abilities like deduction, conference, or counting, involve short stories with embedded questions \citep{schlag2020learning}, and require state tracking in various degrees. We train our implicit SSM model, using Mamba2 as the core architecture, alongside the baseline Mamba2 model, both with up to three layers. Our findings show that the implicit Mamba2 model with a single layer outperforms its single-layer Mamba2 counterpart on most tasks. Additionally, more layers in the implicit model's backbone reduce the number of self-iteration steps needed to solve the tasks (see Appendix~\Cref{fig:catbabi-performance}a, \Cref{fig:catbabi-performance}b).
We furthermore evaluate the performance of the models for tasks sorted by increasing story length. We see how implicit models retain its performance as the lengths increases in~\Cref{fig:catbabi-performance}c at a slight increase in the number of iterations of the implicit models in~\Cref{fig:catbabi-performance}d.

\section{Implicit Large Language Models}
\label{sec:pretraining-deq}
We investigate whether implicit models can be effectively pretrained to function as language models. 
Motivated by the results of \Cref{sec:word-problem}, we implement a pretraining strategy for implicit models with two stages of bounded and free self-iterations.
Transformer (LLama) ~\citet{touvron2023llama} and SSM (Mamba2) ~\citep{dao2024transformers} architectures serve as the core backbones for our implicit models.
In the bounded stage, we train with four self-iterations and a single step of phantom gradient, which we refer to as the ($4+1$)-model.
The ($s+k$)-notation refers to $s$ gradient-tape-free self-iteration steps and $k$ phantom gradient steps. 
$k$ refers to~\Cref{eq:phantom-gradient}, see also~\Cref{fig:deq}.
The free stage starts from a checkpoint of the ($4+1$)-model and increases the number of self-iterations to \num{24}/\num{32} followed by four steps of phantom gradient.
We refer to these models as ($24 + 4$)/($32 + 4$)-models for Mamba2/Llama, respectively.
We employ four model sizes: 125M, 350M, 760M, and 1.3B. 
These models are pretrained in an autoregressive manner for next-token prediction across all sizes on the \textsc{D-Pile}~\citep{gao2020pile} dataset, which consists of 207B tokens.
For baselines, we use both Mamba2~\citep{dao2024transformers} and Llama ~\citep{Beck_P√∂ppel_Spanring_Auer_Prudnikova_Kopp_Klambauer_Brandstetter_Hochreiter} models previously trained on a corpus of 300B tokens.
Additionally, we reproduce Mamba2$^*$ 
and Llama$\dag$ as baselines trained with the same code and data as our implicit models. 
We evaluate the pretrained models on the test set of the \textsc{D-Pile}, examine their length extrapolation capabilities, and assess their common sense reasoning performance on downstream tasks. 
See \Cref{sup:Pretraining Details} for pretraining details.

\paragraph{Pretraining Results and Downstream Performance.}
We report in \Cref{tab:llm-metrics} the next-token perplexity performance of all models trained on the entire 207B token corpus using a test split of the \textsc{D-Pile}\footnote{The test split represents a random selection of 0.1 percent of the entire dataset. This size is in line with the proportion used for the \textsc{Pile}'s validation set \citep{gao2020pile}.}. We observe our implicit models consistently achieve a lower perplexity compared to their explicit counterparts---see also ~\Cref{fig:opener}\textbf{Bottom}. For details related to the dynamics of the implicit models on \textsc{D-pile}, refer to \Cref{tab:implicit-dynamics stats}. Additionally, we evaluate the models' performance on common sense reasoning tasks using the LM Evaluation Harness~\citep{eval-harness}. 
% correct below
The results show that implicit Mamba2  outperform the explicit Mamba2$^*$, which are pretrained on the same number of tokens, on most tasks. This difference becomes more pronounced as the size of the models increases, specifically with the 760M and 1.3B variants.
Compared to the original Mamba2 baseline, trained on 1.5 times more data, the implicit models do better on \textsc{HellaSwag}, \textsc{PIQA}, \textsc{Arc-E}, and are competitive in \textsc{Lambada} and \textsc{Arc-C}. Across all scales, the implicit Mamba2 models significantly outperform Mamba2 in the \textsc{HellaSwag} task, yet they underperform in \textsc{Winogrande} and \textsc{OpenbookQA}. 

It is also noteworthy that our implicit Llama models substantially outperform the baseline Llamas, including both the results reported in~\citep{Beck_P√∂ppel_Spanring_Auer_Prudnikova_Kopp_Klambauer_Brandstetter_Hochreiter} and the Llama$\dag$. This improvement is consistent across all tasks and model sizes. Strikingly, we note that our implicit Llama (32+4) 760M is competitive to the explicit Llama$\dag$ 1.3B.

\paragraph{Implicit-SSMs Demonstrating Length Extrapolation Capabilities}


\begin{figure}[h]  
  \centering  
  \begin{minipage}{0.22\textwidth}  
    \centering 
    \hspace{-1em}
    \includegraphics[width=1.45\textwidth]{figures/ppl_per_token_position.pdf} 
    % \includesvg[width=1.45\textwidth]{figures/ppl_per_token_position.svg}  
  \end{minipage}%  
  \hfill  
  \begin{minipage}{0.18\textwidth}  
    \centering  
    \hspace{-0.2em}
    \resizebox{1.21\textwidth}{!}{\input{tables/llm-1.3B-16k-test-ppl}}  
 
  \end{minipage}  
  \caption{
    Length extrapolation performance on the the test split of the \textsc{D-Pile} of the original 1.3B Mamba2, our Mamba2$^*$, and our implicit Mamba2 with (4+1) and (24+4) self-iterations.
    Shaded gray area shows the in-distribution length. 
    \textbf{Left}: Per token perplexities at different lengths. 
    \textbf{Right}: The average perplexity of tokens for a context length of \num{16384}.   
    }
  \label{fig:per-token-perplexity}  
\end{figure}   
All implicit models in our study were trained on sequences of 2048 tokens. To assess their capability for length extrapolation, we evaluated the implicit models on the test split of the \textsc{D-Pile}, which was packed with longer sequences consisting of 4096, 8192, and 16384 tokens. We compared these results with the baseline Mamba2 and Mamba2$^*$ in \Cref{fig:per-token-perplexity}, where the per-token perplexities are reported. For the average perplexity at 16384 and other lengths, refer to the table in \Cref{fig:per-token-perplexity} and \Cref{tab:all-lengths-ppls} in the Appendix. The implicit Mamba2 models maintain their perplexity as sequence length increases, whereas the baseline Mamba2 models exhibit an increase in perplexity with longer sequences.
%
\paragraph{Effective Duality between Simultaneous Mode and Sequential Mode}
Autoregressive generation, a core functionality of contemporary language models, for implicit models requires that the sequential mode introduced in \Cref{sec:implicit-models} and \Cref{fig:duality} is functionally equivalent to the simultaneous mode used for pretraining.
Effectively, the loops over $s$ and $t$ in \Cref{eq:implicit-ssm-recurrence} have to be interchangeable (also see \Cref{fig:exchange-for-loops}), which we empirically demonstrate  with our pretrained language models. Specifically, we utilize our 1.3B implicit Mamba2 (24+4) and Llama (32+4) models to compute next-token predictions on the \textsc{D-Pile} test split. The models are fed identical input tokens of length 2048 in batches of size 16 and predict outputs greedily in both simultaneous and sequential modes. We observe token match rates of \SI{97.6}{\percent} (on 3M tokens) between the outputs of the two modes for the implicit Mamba2, and \SI{97.7}{\percent} (on 330K tokens) for the implicit Llama. Examples of these model predictions are provided in Appendix \Cref{tab:simul_seq_examples}. The per-token perplexity differences in the predictions of the models are depicted in~\Cref{fig:duality}. 
To our knowledge, this is the first demonstration of sequential evaluation with self-iterated models at constant memory in the number of self-iterations, enabling auto-regressive generation for this class of models.

\input{tables/llm-metrics}
\section{Related Work}
\paragraph{Adaptive-Compute Time} The idea of an adaptive compute budget goes back to \cite{Schmidhuber_2012} who employ a halting neuron to delimit the computation on a particular input. \citet{Graves_2017} generalized the idea and regularised the halting condition to encourage the network to stop early.
They implemented an adaptive-depth RNN and demonstrated the network adjusting the compute budget based on the difficulty of instances in a parity-check task.
This idea was later applied to Transformers, resulting in ''Universal Transformers'' (UT)~\cite{dehghani2018universal}. UTs can either be unrolled to a fixed depth or augmented with a dynamic halting condition (DHC) per token.
UTs were later shown to exhibit improved scaling laws compared to standard transformers~\cite{Kaplan_McCandlish_Henighan_Brown_Chess_Child_Gray_Radford_Wu_Amodei_2020}.
PonderNet~\cite{Banino_Balaguer_Blundell_2021} introduced a principled probabilistic model for determining the halting condition. This approach improved on the UT on the \textsc{bAbi} benchmark.
Recently, a mixture-of-experts (MoE) variant of the UT (MoEUT) was presented~\cite{csordas2024moeut} with 1B parameters, seeking to improve the parameter-to-compute ratio of UTs.
The MoEUT is an unrolled model with fixed iterations and does not employ a DHC. While our models presented here are dense, they could, in principle, be turned into MoE. 
\citet{gatmiry2024can} show that looped linear transformers implement gradient-descent until convergence on the prediction loss defined by previous input-output examples in the context window.
\citet{lim2024parallelizing} take the opposite approach to our work:
Instead of augmenting SSMs or transformers, they propose an approach based on fixed-point iterations to enable parallel training of RNNs.
However, their method incurs cubic cost in terms of state size, limiting the method to smaller models. 

\paragraph{Reasoning and out-of-distribution generalization.}

The ability of looped models to generalize better to input lengths not seen during training is empirically well established: For example~\citet{Yang_Lee_Nowak_Papailiopoulos_2024} show this for looped transformers, while \citet{anil2022path} demonstrate length generalization for DEQs, particularly when they are path independent.
\citet{Du_Li_Tenenbaum_Mordatch} show that energy-based models trained to map energy-gradient-descent steps to algorithmic steps, can length generalize in summation, and complex algorithms such as shortest-path.
On the theoretical side,
The pioneering work of \citet{siegelmann1992} shows that iterated RNNs are Turing complete at infinite numerical precision. 
More recently, \citet{deletang2023neural} studied a number of sequence models and report that grouping tasks by their rung in the Chomsky hierarchy is predictive of models ability to length-generalize. 
While the works of Merrill \emph{et al}~ \cite{merrill2019sequential, merrill2020formal, Merrill_Sabharwal_2023, merrill2024illusion}, which we discuss in\Cref{sec:limitations-transformer-ssm}, showed that both transformers and SSMs are restricted to \tc; several studies sought to find more precise constraints.
\citet{Weiss_Goldberg_Yahav_2021} observe that programs written in a specific language (RASP) can be mapped to transformer models of sufficient capacity.
\citet{Zhou_Bradley_Littwin_Razin_Saremi_Susskind_Bengio_Nakkiran_2024} then showed that transformers tend to length-generalise if the underlying data-generating process can be expressed in RASP.
\citet{sarrof2024expressive} derived a similar refined constraint for SSMs and showed that they can precisely express star-free regular languages.
\citet{Grazzi_Siems_Franke_Zela_Hutter_Pontil_2024} demonstrate that SSMs can track state in simple problems, such as parity, when their (diagonal) recurrence matrix \(\Lambda\) in Equation~\eqref{eq:linear-ssm} permits negative eigenvalues.
Moreover, they illustrate that a variant of DeltaNet~\cite{Yang_Wang_Zhang_Shen_Kim} with (possibly) negative eigenvalues can solve the S5 problem when only swaps of two values are considered in the transition.
However, no variant of Mamba or DeltaNet was capable of learning S5 and achieving length generalization.
To tackle the parallelization-expressiveness trade-off, \citet{Beck_P√∂ppel_Spanring_Auer_Prudnikova_Kopp_Klambauer_Brandstetter_Hochreiter} propose two new LSTM-inspired layer architectures: the sLSTM and mLSTM layers.
While the latter is parallelizable, the former is not and intended to enable the whole model to recognize regular languages.
Finally, \citet{soulos2024recurrent} survey strategies for chunking input sequences with transformers, maintaining parallelizability within each chunk and using RNN-like transitions between chunks.
They find these architectures recognize regular languages for small chunk sizes with scaling remaining a challenge.


\section{Discussion and Conclusion}

This work demonstrates that models implicitly defined by a fixed point iteration can solve hard state tracking problems that resist the capabilities of transformers and SSMs.
We provide theoretical insight how implicit SSMs can deviate from pure diagonal and linear token-to-token transitions and effectively become an RNN in the limit.
When trained with a relatively small number of self-iterations,
our models seamlessly generalize from simpler to harder word problems (see \Cref{fig:word-problem}).
This property is of special interest in language modeling where 'hard' sequences are rare but might occur clustered in applications requiring state tracking.

Our extensive study of synthetic state tracking problems informs a pretraining schedule for large language models.
The implicit Llama and Mamba2 models improve over the baselines in many cases, and prove particularly beneficial on downstream tasks such as \textsc{HellaSwag} (see~\Cref{tab:llm-metrics}).
Performance on language modeling is typically primarily determined by parameter count which traditionally caused weight-shared models to underperform~\cite{tay2022scalinglawsvsmodel}. 
While implicit models lift the limitations of state-of-the-art language models,
self-iteration comes at a cost that only amortizes over the long tail of natural language.
However, emerging hardware that accelerates such self-iteration would alleviate this overhead~\citep{photonics_roadmap}.
Furthermore, as LLMs make more progress on reducing perplexity, they may eventually face tokens requiring RNN-like transitions.

Finally, given the recent rise of test-time compute~\citep{Snell_Lee_Xu_Kumar_2024} and latent-space reasoning~\citep{hao2024traininglargelanguagemodels}, models with adaptive depth per token deserve careful consideration as potential bridgeheads for such techniques as they natively offer adaptive depth and latent-space iteration.

% \section{Broader Impact}
% This paper presents work aimed at advancing the field of machine learning by developing sequential and language models that can track state and hence prove to be more powerful in reasoning. Regarding language modeling, there are numerous potential societal consequences to consider, such as the energy consumption required for the training of large models and the additional iterations that our implicit models impose (see~\Cref{tab:gpu-hours}) to enhance computational expressivity of language models. When developing training algorithms for our language modeling, computational efficiency was taken into account--- see~\Cref{sup:Pretraining Details}. Nevertheless, further research is essential to maintain the power of the proposed models while continuing to reduce computational costs.

\section{Acknowledgment} 
The authors of the paper would like to thank colleagues from the Analog Optical Computer (AOC) team at Microsoft Research Cambridge for their discussions and feedback during the project. Additionally, we acknowledge support from the Microsoft GCR team for providing the GPUs and prompt assistance in resolving issues faced during the training of large language models.

\bibliography{9_references}
\bibliographystyle{icml2025}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Algebraic Structure of Finite State Machines}
\label{sec:algebra}
This section provides a basic introduction to the word problem and it's relation to simulating finite state machines (FSMs).
We start with some results in the circuit complexity and then relate them to the properties of FSMs.
%
\subsection{Circuit Complexity}
\label{sec:circuit-complexity}
Efficient parallelization is one of the central features enabling transformers and SSMs to scale to large machine learning problems such as language modeling.
Parallel circuits, however, face fundamental trade-offs regarding the class of problems that they can address. 
Circuit complexity theory provides a framework to characterize the types of problems that parallel circuits can solve.
\tc is the class of circuits with constant depth and polynomial width composed of unbounded fan-in \AND-gates, \OR-gates, \NOT-gates and \MAJ-gates.
The second class of interest, \nc, is represented by logarithmic depth circuits with a polynomial number of bounded fan-in gates. 
From the perspective of formal languages, \nc is equivalent to the class of circuits recognizing the regular languages.
Since the unbounded fan-in gates allowed in \tc circuits can be constructed from log-depth circuits with bounded fan-in, it follows that $\text{TC}^0\subset\text{NC}^1$.
It is open if \tc is a proper subset of \nc, and we will discuss a regular language for which no \tc circuit construction is known.

Both transformers and SSMs can be simulated by \tc circuit families under mild assumptions~\cite{merrill2022saturated, merrill2024illusion}.
If \tc is a proper subset of \nc, the leading sequence models today cannot even recognize all regular languages.
Consequentially, they cannot execute arbitrary finite state machines (FSMs), a fundamental skill to execute tasks, or to form world models~\cite{vafa2024evaluating}.
Many empirical studies confirm these theoretical limitations of transformers and SSMs to learn regular languages~\cite{deletang2023neural, sarrof2024expressive, strobl2024formal}.
At the same time, recurrent neural networks are known to recognize regular languages~\cite{kleene1951representationof, elman1991distributed, merrill2020formal}, and to effectively implement internal FSMs to solve language problems~\cite{Omlin1996extraction}.
%
\subsection{Algebraic Concepts of Finite State Machines}
\paragraph{Monoids and Groups}
There is a tight relationship between finite state machines and algebraic concepts such as monoid and groups. 
We define the relevant concepts for our state tracking problem described in \Cref{sec:word-problem}
\begin{definition}[Monoid]
    A set $M$ and a binary operation $\circ: M\times M\longrightarrow M$ are called a \textit{monoid} $\func{M, \circ}$ if 
    \begin{enumerate}
        \item there exists an identity element $e\in M$ with $e\circ m = m\circ e = m$ for all $m\in M$
        \item the operation $\circ$ is associative, i.e. ${(m_1\circ m_2)\circ m_3 = m_1\circ (m_2\circ m_3)}$ for all $m_1, m_2, m_3 \in M$.
    \end{enumerate}
\end{definition}
Straight forward examples for monoids are natural, rational or real numbers with multiplication, or strings with string concatenation.
Since monoid are associative, we can simplify notation and write $m\circ m = m^2$, and so on for all powers $k\in\mathbb{N}$.
\begin{definition}[Aperiodic Monoid]
    A monoid $\func{M, \circ}$ is called \textit{aperiodic} if for all $m\in M$ there is a $k\in\mathbb{N}$ s.t. $m^k = m^{k+1}$.
\end{definition}
Monoid whose elements can be inverted have a particularly right structure.
\begin{definition}[Group]
    A \textit{group} $\func{G, \circ}$ is a monoid with the additional property that for every $g\in G$ there is $g^{-1}\in G$ s.t. ${g\circ g^{-1} = g^{-1}\circ g = e}$.
\end{definition}
Examples for groups are rational numbers with multiplication, or the orthogonal matrices with matrix multiplication.
Notably, permutations on a set of $k$ elements for $k\in\mathbb{N}$ form a group, called the \textit{symmetric group} $S_k$.

Our synthetic learning problem discussed in \Cref{sec:word-problem} will be constructed based on a classical problem in computer science.
\begin{definition}[Word Problem]
Let $M^\ast$ denote the set of all sequences over elements of $M$.
The \textit{Word Problem} on a monoid $\func{M, \circ}$ is defined by the function 
\begin{align}
    \text{WP}: M^*&\longrightarrow M \nonumber\\
    \text{WP}\func{m_0, m_1\dots,m_k}&\mapsto m_0\circ m_1 \circ \dots \circ m_k \,,
\end{align}
i.e. a word over $M$ is resolved by composition to a single element in $M$.
\end{definition}
The central question for our experiment will be which kinds of circuits can solve the word problem for arbitrary sequence lengths.
\begin{theorem}[\cite{barrington1989nc1}]
    The word problem for any fixed non-solvable group $G$ is complete for \nc under \ac reductions.
    \label{thm:barrington}
\end{theorem}
%
\paragraph{Algebra of FSMs}
Tracking the state of a system can be formalized as executing a finite state machine~\cite{merrill2024illusion}.
To characterize the limits of certain FSMs, we define a few formal concepts.
\begin{definition}[FSM]
    A \textit{finite state machine} (FSM) consists of a finite set of states $Q$, a finite set of input symbols $\Sigma$ called the alphabet, and a transition function ${\delta: Q\times \Sigma\longrightarrow Q}$.
\end{definition}
Given an initial state $q_0\in Q$ and a sequence of symbols $w=a_1a_2\dots a_k\in\Sigma^*$, a FSM transitions from the initial state into a final state.

Finite state machines naturally define a monoid.
%
\begin{definition}[Syntactic Monoid]
    For each symbol $a\in\Sigma$, define the function ${\delta_a: Q\longrightarrow Q}$.
    The transformation monoid $M$ generated by $\delta_a, a\in\Sigma$ and the composition of functions $\circ$, is called the \textit{syntactic monoid} $\func{M, \circ}$ of the finite state machine.
\end{definition}
%
The algebraic structure of $M$ is tightly coupled to the programs that the original FSM can execute.
Our investigation is based on the classical result stated in \Cref{thm:barrington}.
The simplest example of a non-solvable group is the permutation group of five elements $S_5$. 
A corollary from theorem~\ref{thm:barrington} is that the FSM whose syntactic monoid is $S_5$ is complete in \nc and hence in the class of regular languages.
We have thus identified a \textit{hard} state tracking problem: Permutations of five elements.

Another classical result tightly related to state-space models is
\begin{theorem}[\cite{SCHUTZENBERGER1965190}]
    Let $L$ be the regular language defined by a FSM, and let $M$ be the syntactic monoid of the same FSM.
    Then $L$ is a star-free language if an only if $M$ is aperiodic.
    \label{thm:schutzenberger}
\end{theorem}
It is intuitive that the word problem for finite aperiodic monoids is in \tc. 
The maximal depth of the circuit is driven by the number of elements of the monoid and it's maximal $k$ for the aperiodicity condition.
%
Empirical studies have shown that transformers and SSMs can simulate a range of regular languages~\cite{deletang2023neural,strobl2024formal}, but they struggle to learn the $S_5$ word problem in line with their characterization as \tc circuits~\cite{merrill2024illusion}.
SSMs can be further restricted to the star-free languages~\cite{sarrof2024expressive}, i.e. those with aperiodic syntactic monoid.
%
\section{Proof of Theorem 1}
\label{sec:proof-theorem}
% to get the same numbers as in maint ext (could be solved more nicely)
\setcounter{theorem}{0}
\begin{theorem}
    Consider an implicit SSM given by Equations \eqref{eq:implicit-ssm-block} and \eqref{eq:implicit-ssm-recurrence}. Then the transition function $h_{t-1}\fp \mapsto h_t\fp$ is non-linear and non-diagonal, 
    i.e. each hidden state $h_t\fp$ is a non-linear function of the previous hidden state $h_{t-1}\fp$.
    Consequently, the state-to-state Jacobian is a non-diagonal operator.
\end{theorem}
\begin{proof}
    We will apply the implicit function theorem for the function
    \begin{align}
        g\func{z, h, x, \theta} = z - f_\theta\func{z, h, x} \,.
    \end{align}
    If $g\func{z_t\fp, h_{t-1}\fp, x_t, \theta_0} = 0$ and $J_{g, z}$ is non-singular,
    then there exists an open set $U$ with $\func{h_{t-1}\fp, x, \theta_0}\in U$ and a differentiable function $\varphi$ on $U$ s.t. 
    \begin{align}
        g\func{\varphi\func{h, x, \theta}, h, x, \theta} = 0 \quad \forall \func{h, x, \theta}\in U \,.
    \end{align}
    The derivative of $\varphi$ at the fixed point is given by
    \begin{align}
        \partialdiff{\varphi}{h}{h_{t-1}\fp, x_t, \theta_0} = 
        - \func{I - \partialdiff{f_\theta}{z}{z_t\fp, h_{t-1}\fp, \theta_0}}^{-1}
        \partialdiff{f_\theta}{h}{z_t\fp, h_{t-1}\fp, \theta_0}
        %- \func{I - \frac{\partial f_\theta}{\partial z}}^{-1}\frac{\partial f_\theta}{\partial h}
    \end{align}
    Clearly, $\varphi$ is a non-linear function if $f_\theta$ is a non-linear function.

    Now, consider Equation~\ref{eq:implicit-ssm-recurrence} at the fixed point
    \begin{align}
            h_t\fp = \Lambda\func{z_t\fp, x_t} h_{t-1}\fp + u\func{z_t\fp, x_t} \,,
    \end{align}
    where $z_t\fp = \varphi\func{h_{t-1}\fp, x_t, \theta}$.
    If $\varphi$ is a non-linear function of $h_{t-1}\fp$, then $\ h_{t-1}\fp \mapsto h_t\fp$ is a non-linear function as well.
    Equipped with the derivative of $\varphi$, we can derive the state-to-state Jacobian of the implicit SSM as
    \begin{align}
        \partialdiff{h_t\fp}{h_{t-1}\fp}{h_{t-1}\fp, x_t, \theta}
        & = \Lambda\func{z_t\fp, x_t} \nonumber\\
        &+ \frac{\partial \Lambda}{\partial z_t\fp} \frac{\partial \varphi}{\partial h_{t-1}\fp}\mathrm{diag}\func{h_{t-1}\fp} \nonumber\\
        &+ \frac{\partial u}{\partial z_t\fp} \frac{\partial \varphi}{\partial h_{t-1}\fp} \,.
        \label{eq:implicit-ssm-jacobian}
    \end{align}
    This equation highlights the non-diagonal corrections to the diagonal Jacobian $\Lambda$ of the explicit state-space Equation~\eqref{eq:linear-ssm}.
\end{proof}
%
\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{figures/2025-01-28-deq.pdf}
    \caption{Fixed-point iteration and phantom gradients: A neural network is iterated until convergence in the forward pass. When employing the phantom gradient principle, only a fraction of the forward steps is however considered for the backward pass. }
    \label{fig:deq}
\end{figure}


\section{Additional Results}





\subsection{Inference Dynamics and Convergence of Implicit Language Models}

In Table \Cref{tab:implicit-dynamics stats}, we present the average number of steps that the implicit language models require to process a sequence length of 2048 from the test split of the \textsc{D-Pile} dataset during inference in simultaneous mode. Moreover, the table also shows the relative error difference of the solutions found by the language models. Notably, the (4+1) configurations achieve a fixed point, despite not being explicitly constrained to do so. Our observations further reveal that the implicit models trained with a full DEQ setup---(24+4) for Mamba2 and (32+4) for Llama---consistently reach fixed points within their training stop thresholds, which are $<0.05$ and below the inference step threshold of four times 24 and 32, respectively.

\input{tables/llm-steps}

\subsection{Length Extrapolation Capabilities in Pretrained Models}
We evaluated the ability of our models to extrapolate to longer sequence lengths and compared their performance with baseline models. All our in-house trained models, including the Mamba2 and Llama baselines, were initially trained on sequences of 2048 tokens and subsequently tested on sequences of 4096, 8192, and up to 16384 tokens. Table \ref{tab:all-lengths-ppls} presents the average perplexities across different model scales. We note that the original Mamba2 models, denoted as Mamba2$^*$, were trained on sequence lengths of 8192. Our observations indicate that in all instances, our implicit models, including the Mamba2 (4+1), and Mamba2 (24+4) as well as the Llama(32+4) configuration, maintain lower perplexities compared to the explicit Mamba2 and Llama respectively. We also found that the difference in perplexity between the longer and shorter sequences becomes more pronounced as the size of the models increases.

\input{tables/llm-ppl-length-extrapolation-all}

\subsection{Effective Duality between Simultaneous Mode and Sequential Mode in SSMs}

Empirically, we demonstrate that once implicit state-space models (SSMs) are trained in a simultaneous mode (parallelizable in token dimension), these trained models can be utilized in sequential mode. In sequential mode, self-iteration occurs for each token, thereby affirming the duality of the two modes. We employ the 1.3B Mamba2 (24+4) model and 1.3B Llama (32+4) model, trained on 207B tokens from the \textsc{D-Pile}, to evaluate the duality between the two modes using examples from the test split of the \textsc{D-Pile}. \Cref{tab:simul_seq_examples} presents some detokenized outputs of the model in both modes.


\input{tables/out_text}




\section{Experimental Details}
\subsection{The Word Problem}
\label{sec:appendix-word-problem}
\paragraph{Details of the Learning Problem}
To construct a learning problem for sequence models, we represent each element of the monoid $M$ as a token, and present a sequence $m_0, \dots, m_L$ of tokens to the model.
The ground truth at each position $k=1,\dots,L$ is the token representing the element ${m_0 \circ \dots \circ m_k}$.
We then calculate the mean cross entropy loss over the entire sequence, providing a learning signal at each step $k=1, \dots, L$.

State-space models can learn the word problem for aperiodic monoids~\cite{sarrof2024expressive}, but fail so solve it for non-solvable groups such as $S_5$~\cite{merrill2024illusion}.
We confirm in \Cref{fig:opener} that implicit state-space models can in fact learn the word problem for $S_5$.
We now want to \textit{interpolate between word problems for aperiodic and non-solvable monoids} to test how much signal our implicit state-space model defined in ~\Cref{sec:implicit-models} needs from the hard non-solvable group word problem to learn it.

Let $M = M^{a} \times G$ be a direct product of an aperiodic monoid $M^{a}$ and a non-solvable group $G$.
A sequence $m_0, \dots, m_T$ is sampled from $M$ with replacement.
To control the number of hard examples and simple examples, we define a family of distributions \distribution{p} over $M$ as follows.
An element $m^{a}_k\in M^{a}$ is sampled uniformly at each step $k$, representing the presence of simple examples.
On the other hand, we sample elements $g_k\in G\setminus \{e\}$ from $G$ without the identify transformation, each with probability $\frac{p}{\left|G\right|-1}$. 
The identity element $g_k=e\in G$ is sampled with probability $1-p$.
The resulting transformations $\func{m^{a}_k, g_k}$ are aperiodic at least when $g_k = e$, i.e. with probability $1-p$.

We'll call the tokens representing $\func{m, e}$ \textit{simple tokens} and the tokens representing $\func{m, g}$ with ${g\neq e}$ are called \textit{hard tokens}.
The names derive from the fact that SSMs can resolve the word problem if it is only composed from simple tokens. 
Any non-zero probability $p$ of sampling hard tokens renders the word problem unsolvable for fixed depth SSMs on arbitrarily long sequences.

Our construction of a distribution over a monoid allows us to test out-of-distribution generalization not only in terms of length generalization, the most common setting in the literature.
By changing $p$ between training time and test time, we construct training tasks and evaluation tasks with varying difficulty.
This effectively offers OOD evaluation with the same number of tokens, but different mixtures of easy and hard tokens.
While this property allows us to distil expressivity questions from length generalization properties, 
our construction is not limited to the same sequence length and could as well be used in the length generalization setup (see \Cref{fig:model-comparison}).
%
\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{figures/monoid-task/2025-01-27-word_problem_models.pdf}
    \caption{
        Comparison of implicit Mamba2, unrolled Mamba2, and Mamba2 for $p=0.05$.
        All models were trained and evaluated on sequences of length $L=256$.
        Unrolled Mamba2 refers to a single layer being unrolled multiple times with a full backpropagation trace, while the implicit Mamba2 receives only \num{4} steps of Phantom Gradient.
        The training time depth of all models is limited to $16$, i.e. $16$ layers for Mamba2, and $16$ self-iterations for the implicit and weight tied models.
        Implicit and unrolled models use unbounded test-time computation to converge to a fixed point.
        The comparison shows that the implicit model with $4$ steps of Phantom Gradient succeeds over the unrolled model.
        }
    \label{fig:model-comparison}
\end{figure}
%
\paragraph{Experimental Details}
Each data point in \Cref{fig:word-problem} and \Cref{fig:model-comparison} is based on \num{10} runs with different random seeds.
We report the best run, mean accuracy and a \SI{95}{\percent} confidence interval for each data point.
All word problem models were trained on sequences of length $L=256$, and a batch size of 512 on \SI{32}{\giga\byte} V100s.
The explicit models and self-iterated models with full backpropagation trace required gradient accumulation over two steps.
The learning rate is set to \num{0.001}.
We disable dropout and weight decay, which appears to harm learning on the word problem.
The self-iterations are stopped upon convergence, which we define as a relative difference between two consecutive states of \num{0.01} for \Cref{fig:word-problem} or \num{0.05} for \Cref{fig:model-comparison}.
We trained a number of standard Mamba2 models with the same number of runs for multiple numbers of layers.
These models struggle to capture the training distribution compared to self-iterated models, and none of them was able to generalize to a harder distribution or to longer sequences.
%
\subsection{CatbAbI}
\label{sec:details_catbabi}
The models, both implicit and explicit, comprise up to three layers of the Mamba2 architecture with an embedding dimension of 256, a state dimension of 16 (expansion factor 2), and a head dimension of 32. We trained the models using batch sizes of 128 and 256, and learning rates of 0.0001, 0.0005, 0.001, and 0.005. The models were trained for 15,000 steps, with the implicit model specifically trained for 5,000 steps in unrolling mode, utilizing 32 steps with normal gradient checkpointing, followed by 10,000 steps of self-iteration fixed-point search. The self-iteration included a stop threshold of 0.03 and a training and testing maximum number of steps 50 and 200 , respectively, and phantom gradient parameters of 6 steps with ($\lambda = 0.5$). Data were packed in sequences of length 200 tokens as per \cite{schlag2020learning}. Figures \ref{fig:catbabi-explicit_Valid acc_plot} and \ref{fig:catbabi-implicit_Valid acc_plot} illustrate the validation accuracy of the explicit and implicit Mamba2 models on the \textsc{CatbAbI}  dataset, respectively. Additionally, \Cref{fig:catbabi-implicit_Valid steps_plot} plots the number of iterations required for the implicit model to reach a fixed point on the validation set of the \textsc{CatbAbI} dataset.

\begin{figure}[h!]   
  \centering       
  \begin{adjustbox}{trim=0cm 0.0cm 0cm .8cm,clip}    
    \includegraphics[width=0.8\textwidth]{figures/catbabi/catbabi_explicit_Valid_acc_plot.png} 
  \end{adjustbox}    
  \caption{Hyperparameter sweeps for explicit Mamba2 models over batch sizes 128, 256, layers 1,2,3 and various learning rates for training on the \textsc{CatbAbI}  dataset.}    
  \label{fig:catbabi-explicit_Valid acc_plot}    
\end{figure} 

\begin{figure}[h!]   
  \centering       
  \begin{adjustbox}{trim=0cm 0.0cm 0cm 0.8cm,clip}    
    \includegraphics[width=0.8\textwidth]{figures/catbabi/catbabi_implicit_Valid_acc_plot.png}
  \end{adjustbox}    
  \caption{Hyperparameter sweeps for implicit Mamba2 models over batch sizes 128, 256, layers 1,2,3 and various learning rates for training on the \textsc{CatbAbI} dataset.}    
  \label{fig:catbabi-implicit_Valid acc_plot}    
\end{figure} 

\begin{figure}[h!]   
  \centering       
  \begin{adjustbox}{trim=0cm 0.0cm 0cm .8cm,clip}    
    \includegraphics[width=0.8\textwidth]{figures/catbabi/catbabi_implicit_Valid_steps_plot.png}
  \end{adjustbox}    
  \caption{Hyperparameter sweeps for implicit Mamba2 models over batch sizes 128, 256, layers 1,2,3 and various learning rates for training on the \textsc{CatbAbI}  dataset.}    
  \label{fig:catbabi-implicit_Valid steps_plot}    
\end{figure}
\begin{figure*}[h!]      
  \centering      
  % First minipage for the first figure      
  \begin{minipage}[b]{0.85\textwidth} 
  \includegraphics[width=\linewidth]{figures/catbabi/catbabi_combined.pdf}
    % \includesvg[width=\textwidth]{figures/catbabi/catbabi_combined.pdf}     
\caption{Small-scale reasoning advantages of implicit SSMs compared to explicit SSMs on the \textsc{CatbAbI}  dataset. (\textbf{a}) Task-specific performance comparison, measured in accuracy of the models in predicting answers to questions within a story, between the implicit Mamba2 and baseline Mamba2. The one-layer implicit Mamba2 model outperforms the one-layer explicit Mamba2 on most tasks. As the number of layers in the explicit Mamba2 increases, its performance approaches that of the implicit Mamba2. Adding more layers to the implicit Mamba2 benefits certain tasks, such as 'Basic induction' or 'Path finding,' where the implicit Mamba2 achieves the best performance. (\textbf{b}) The correlation between the number of self-iteration steps that the implicit Mamba2 takes to solve a task and the number of layers in the implicit model's backbone architecture, showing a decrease in the required steps with additional layers. (\textbf{c}) The implicit Mamba2 retains its performance as the story length increases, whereas the explicit Mamba2's performance declines. (\textbf{d}) The trend in the number of iterations needed by the implicit Mamba2 models as story length increases, indicating a modest rise in computational steps.}    
    \label{fig:catbabi-performance}    
  \end{minipage}      
\end{figure*} 

\subsection{Language Modeling}


\paragraph{Pretraining Details}
\label{sup:Pretraining Details}
We have trained a suite of Implicit SSM models with the core architecture of Mamba2 and Implicit Transformer models with the core of Llama3. For each implicit model, we have a corresponding weight-tied model that is also trained on the entire \textsc{D-Pile} dataset. We use the checkpoint from 80 percent of the way through training the weight-tied model to train the fully implicit model. We use four scales for the training of the models: 1.3B, 760M, 360M, and 130M. In all models, the LLM head weights are tied to the embedding weights. The implicit and weight-tied models have the same architecture as those of the Mamba2 and Llama models, except for an injection module, consisting of an MLP, which transforms the input into the domain of the mixer latent space. This module has a constant size equivalent to $2 \times d_{emb} + 2 \times d_{state} + n_{heads}$ in Mamba2, matching with $d_{in_{proj}}$, and $3 \times d_{emb}$ in Llama models, corresponding to the key, value, and queries, and is shared across all layers of the model. Details for each model is provided in Table \ref{tab:architecture-detail}. 


We followed the training recipe of Mamba2 and Llama models. In particular, we used a weight decay of 0.1, no bias for the LLM head, AdamW hyperparameters $\beta = (0.9, 0.95)$, RMSNorm instead of LayerNorm, and a linear warm-up step to the peak learning value, which is chosen as 5 times the value of the GPT-3 model. For the learning rate scheduler, we used a constant learning rate followed by a square root decay to a minimum value of $10^{-5}$ \cite{hagele2024scaling}. While this scheduling has also been shown to be compute-optimal \cite{hagele2024scaling} alongside the cosine scheduling, it allows us to use intermediate checkpoints during training more conveniently without considering how the new learning rate affects the training of implicit models. All models were trained with an effective batch size of 1M tokens and a training sequence length of 2048 tokens. 


\input{tables/pretraining_details}

\paragraph{Downstream Task Details} We evaluated our models as well as the baseline models on seven tasks: \textsc{Lambada\_openai} \cite{paperno2016lambada}, \textsc{Hellaswag} \cite{zellers2019hellaswag}, \textsc{Piqa} \cite{bisk2020piqa}, \textsc{Arc-easy} and \textsc{Arc-challenge} \cite{clark2018think}, \textsc{Winogrande} \cite{sakaguchi2021winogrande}, and \textsc{OpenbookQA} \cite{mihaylov2018can}. We used the model checkpoints on the 207B tokens of the \textsc{D-Pile}. For the evaluation of the downstream tasks, we utilized the LM Evaluation Harness package ~\citep{eval-harness} with the default specifications, i.e., a batch size of 256 and a sequence length of 2048. All models were evaluated using one H100 80GB GPU.


\paragraph{Curriculum-Based Training of Language Models}
\label{Curriculum-Based Training of Language Models}
The training of implicit models is achieved through a curriculum training framework that is divided into two main phases: bounded phase and the free phase. In the bounded phase, the models are subjected to four steps of self-iteration, followed by a singular phantom gradient step to store the activations necessary for backpropagation. This phase of training involves 80 percent of the dataset (the choice of this proportion and its influence on model performance are discussed below). Training then progresses to the free phase, wherein the model undergoes additional fixed point searches, capped at 24 self-iterations for SSMs and 32 for Transformers, and is followed by four phantom gradient iterations. A stopping criterion of $\epsilon=0.05$ is implemented during this second phase, allowing models to terminate the fixed point search once this threshold is met. For validation and testing on the \textsc{D-Pile} dataset, the limit on fixed-point iterations is set to four times the number used in the free phase of training. The learning rate reduction starts in phase two; for Transformers, this reduction begins immediately to prevent instability caused by their high spectral norm. Conversely, for SSMs, the learning rate cooldown starts after 90 percent of the overall training period has elapsed, due to their spectral norm being approximately one, which permits more substantial weight adjustments at a higher learning rate.


\paragraph{Duration of Bounded and Free Phases in Training Language Models}
\label{appendix-phase-one-percentage}
In our exploration of the optimal duration for bounded and free phases of training, we aimed to find a balance between computational efficiency and the necessary nonlinear transitions each token must undergo through self-iteration. We tested models trained with 70, 80, and 90 percent of the bounded phase duration before starting the full fixed-point search in the free phase---refer to Fig. \ref{fig:cooldown-ratio-deq-spec}a. For this evaluation, we used a 130M Mamba2 model. Based on the model's perplexity on the validation split of \textsc{D-Pile} (2M examples of length 2048), we observed that beyond a certain extent of free phase training, the model's performance plateaus or overfits. We determined that 20 to 10 percent of free phase training is optimal. Consequently, we applied 20 percent free phase training for the training of larger models. It is crucial to note that the 130M model, when trained with an effective batch size of 0.5M tokens, experienced overfitting. This overfitting was not present when we increased the batch size to 1M tokens, ---see Fig. \ref{fig:cooldown-ratio-deq-spec}b. Thus, we adopted a 1M token batch size for the training of models across all scales.

\paragraph{Specification of Fixed Point Solver for Language Model Training}
\label{Specification of Fixed Point Solver for Language Model Training}
Our experimentation with the fixed point solver in the free phase involved adjusting various parameters, including the maximum number of self-iterations for the fixed point search (16, 24, 32) and the number of gradient accumulation steps (2, 4). The 32 iterations have a smaller stop threshold of 0.02, whereas the 16 and 24 iterations have a stop threshold of 0.05-----see Fig. \ref{fig:cooldown-ratio-deq-spec}b. We used a 130M Mamba2 model for this evaluation with an effective batch size of 1M tokens. We measured the validation split perplexity during training. For the training of the implicit Transformer models, we used a maximum self-iteration cap of 32. This was due to the higher spectral norm of the Transformer models requiring more steps to reach a fixed point below the threshold.

\begin{figure*}[h!]      
  \centering      
  % First minipage for the first figure      
  \begin{minipage}[b]{\textwidth}

    \includegraphics[width=\textwidth]{figures/130M-cosine-constant-deq-spec-cooldown-ratio.pdf}
    % \includesvg[width=\textwidth]{figures/130M-cosine-constant-deq-spec-cooldown-ratio.svg}     
    \caption{\textbf{Left}: Impact of Bounded Phase (4+1) Training Duration on Model Performance: A comparison of perplexity on the validation split of the \textsc{D-Pile} obtained by 130M Mamba2 models trained with 70\%, 80\%, and 90\% bounded phase durations, with a batch size of 0.5M tokens. \textbf{Right}: Evaluation of Fixed Point Solver Specifications: The relationship between different maximum iterations (16, 24, 32) and gradient accumulation steps (2, 4) on the validation split perplexity of the \textsc{D-Pile} for the 130M Mamba2 model with a batch size of 1M tokens.}   
    \label{fig:cooldown-ratio-deq-spec}    
  \end{minipage}      
\end{figure*} 

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{figures/exchange-for-loops.pdf}
    \caption{
        The simultaneous mode exploits the parallelism of state-space models or transformers,
        while the sequential mode is well suited for language generation.
        State-space models can further utilize the sequential mode for processing with constant memory over any sequence length.
        The two modes emerge from exchanging the for loops over the two variables $t$ and $s$ in the DEQ iteration~\eqref{eq:implicit-ssm-recurrence}.
        We demonstrate in \Cref{sec:pretraining-deq}, and \Cref{fig:duality}, that 1.3B parameter language models trained with the simultaneous mode show negligible difference in perplexity when evaluated with the sequential mode.
        }
    \label{fig:exchange-for-loops}
\end{figure}

\paragraph{Resource Allocation for Training and Evaluating Large Language Models}
We trained our suite of models on a cluster with AMD Instinct MI300X GPUs. Each node within the cluster comprises 8 GPUs, and we employed distributed multi-node processing to train our models on up to 32 GPUs simultaneously. Table \ref{tab:gpu-hours} details the number of GPUs allocated for the training of each model, as well as the total GPU hours consumed by each. The evalution of models on downstream tasks was achieved on one 80GB Nvidia H100 GPU.

\input{tables/llm-gpu-hours}

\input{tables/catbabi-examples}


\clearpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}