%%% Looped transformers and results
@inproceedings{
gatmiry2024can,
title={Can Looped Transformers Learn to Implement Multi-step Gradient Descent for In-context Learning?},
author={Khashayar Gatmiry and Nikunj Saunshi and Sashank J. Reddi and Stefanie Jegelka and Sanjiv Kumar},
booktitle={Forty-first International Conference on Machine Learning},
year={2024},
url={https://openreview.net/forum?id=o8AaRKbP9K}
}


%%%% Universal Transformers and recurrent RNN
 @article{Schmidhuber_2012, title={Self-Delimiting Neural Networks}, url={http://arxiv.org/abs/1210.0118}, DOI={10.48550/arXiv.1210.0118}, abstractNote={Self-delimiting (SLIM) programs are a central concept of theoretical computer science, particularly algorithmic information & probability theory, and asymptotically optimal program search (AOPS). To apply AOPS to (possibly recurrent) neural networks (NNs), I introduce SLIM NNs. A typical SLIM NN is a general parallel-sequential computer. Its neurons have threshold activation functions. Its output neurons may affect the environment, which may respond with new inputs. During a computational episode, activations are spreading from input neurons through the SLIM NN until the computation activates a special halt neuron. Weights of the NN’s used connections deﬁne its program. Halting programs form a preﬁx code. An episode may never activate most neurons, and hence never even consider their outgoing connections. So we trace only neurons and connections used at least once. With such a trace, the reset of the initial NN state does not cost more than the latest program execution. This by itself may speed up traditional NN implementations. To efﬁciently change SLIM NN weights based on experience, any learning algorithm (LA) should ignore all unused weights. Since preﬁxes of SLIM programs inﬂuence their sufﬁxes (weight changes occurring early in an episode inﬂuence which weights are considered later), SLIM NN LAs should execute weight changes online during activation spreading. This can be achieved by applying AOPS to growing SLIM NNs. Since SLIM NNs select their own task-dependent effective size (=number of used free parameters), they have a built-in way of addressing overﬁtting, with the potential of effectively becoming small and slim whenever this is beneﬁcial. To efﬁciently teach a SLIM NN to solve many tasks, such as correctly classifying many different patterns, or solving many different robot control tasks, each connection keeps a list of tasks it is used for. The lists may be efﬁciently updated during training. To evaluate the overall effect of currently tested weight changes, a SLIM NN LA needs to re-test performance only on the efﬁciently computable union of tasks potentially affected by the current weight changes. Search spaces of many existing LAs (such as hill climbing and neuro-evolution) can be greatly reduced by obeying restrictions of SLIM NNs. Future SLIM NNs will be implemented on 3-dimensional brain-like multi-processor hardware. Their LAs will minimize task-speciﬁc total wire length of used connections, to encourage efﬁcient solutions of subtasks by subsets of neurons that are physically close. The novel class of SLIM NN LAs is currently being probed in ongoing experiments to be reported in separate papers.}, note={arXiv:1210.0118 [cs]}, number={arXiv:1210.0118}, publisher={arXiv}, author={Schmidhuber, Juergen}, year={2012}, month=sep, language={en} 
}

 @article{Snell_Lee_Xu_Kumar_2024, title={Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters}, url={http://arxiv.org/abs/2408.03314}, DOI={10.48550/arXiv.2408.03314}, abstractNote={Enabling LLMs to improve their outputs by using more test-time computation is a critical step towards building generally self-improving agents that can operate on open-ended natural language. In this paper, we study the scaling of inference-time computation in LLMs, with a focus on answering the question: if an LLM is allowed to use a fixed but non-trivial amount of inference-time compute, how much can it improve its performance on a challenging prompt? Answering this question has implications not only on the achievable performance of LLMs, but also on the future of LLM pretraining and how one should tradeoff inference-time and pre-training compute. Despite its importance, little research attempted to understand the scaling behaviors of various test-time inference methods. Moreover, current work largely provides negative results for a number of these strategies. In this work, we analyze two primary mechanisms to scale test-time computation: (1) searching against dense, process-based verifier reward models; and (2) updating the model’s distribution over a response adaptively, given the prompt at test time. We find that in both cases, the effectiveness of different approaches to scaling test-time compute critically varies depending on the difficulty of the prompt. This observation motivates applying a “compute-optimal” scaling strategy, which acts to most effectively allocate test-time compute adaptively per prompt. Using this compute-optimal strategy, we can improve the efficiency of test-time compute scaling by more than 4x compared to a best-of-N baseline. Additionally, in a FLOPs-matched evaluation, we find that on problems where a smaller base model attains somewhat non-trivial success rates, test-time compute can be used to outperform a 14x larger model.}, note={arXiv:2408.03314 [cs]}, number={arXiv:2408.03314}, publisher={arXiv}, author={Snell, Charlie and Lee, Jaehoon and Xu, Kelvin and Kumar, Aviral}, year={2024}, month=aug, language={en} }

@inproceedings{
csordas2024moeut,
title={Mo{EUT}: Mixture-of-Experts Universal Transformers},
author={R{\'o}bert Csord{\'a}s and Kazuki Irie and J{\"u}rgen Schmidhuber and Christopher Potts and Christopher D Manning},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=ZxVrkm7Bjl}
}

@article{Banino_Balaguer_Blundell_2021, title={PonderNet: Learning to Ponder}, url={http://arxiv.org/abs/2107.05407}, DOI={10.48550/arXiv.2107.05407}, abstractNote={In standard neural networks the amount of computation used grows with the size of the inputs, but not with the complexity of the problem being learnt. To overcome this limitation we introduce PonderNet, a new algorithm that learns to adapt the amount of computation based on the complexity of the problem at hand. PonderNet learns end-to-end the number of computational steps to achieve an eﬀective compromise between training prediction accuracy, computational cost and generalization. On a complex synthetic problem, PonderNet dramatically improves performance over previous adaptive computation methods and additionally succeeds at extrapolation tests where traditional neural networks fail. Also, our method matched the current state of the art results on a real world question and answering dataset, but using less compute. Finally, PonderNet reached state of the art results on a complex task designed to test the reasoning capabilities of neural networks.}, note={arXiv:2107.05407 [cs]}, number={arXiv:2107.05407}, publisher={arXiv}, author={Banino, Andrea and Balaguer, Jan and Blundell, Charles}, year={2021}, month=sep, language={en} 
}
@inproceedings{
tay2022scalinglawsvsmodel,
title={Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?},
author={Yi Tay and Mostafa Dehghani and Samira Abnar and Hyung Won Chung and William Fedus and Jinfeng Rao and Sharan Narang and Vinh Q. Tran and Dani Yogatama and Donald Metzler},
booktitle={The 2023 Conference on Empirical Methods in Natural Language Processing},
year={2023},
url={https://openreview.net/forum?id=E9dH0BP5VW}
}
 @article{Graves_2017, title={Adaptive Computation Time for Recurrent Neural Networks}, url={http://arxiv.org/abs/1603.08983}, DOI={10.48550/arXiv.1603.08983}, abstractNote={This paper introduces Adaptive Computation Time (ACT), an algorithm that allows recurrent neural networks to learn how many computational steps to take between receiving an input and emitting an output. ACT requires minimal changes to the network architecture, is deterministic and diﬀerentiable, and does not add any noise to the parameter gradients. Experimental results are provided for four synthetic problems: determining the parity of binary vectors, applying binary logic operations, adding integers, and sorting real numbers. Overall, performance is dramatically improved by the use of ACT, which successfully adapts the number of computational steps to the requirements of the problem. We also present character-level language modelling results on the Hutter prize Wikipedia dataset. In this case ACT does not yield large gains in performance; however it does provide intriguing insight into the structure of the data, with more computation allocated to harder-to-predict transitions, such as spaces between words and ends of sentences. This suggests that ACT or other adaptive computation methods could provide a generic method for inferring segment boundaries in sequence data.}, note={arXiv:1603.08983 [cs]}, number={arXiv:1603.08983}, publisher={arXiv}, author={Graves, Alex}, year={2017}, month=feb, language={en} }

 @article{Zhou_Bradley_Littwin_Razin_Saremi_Susskind_Bengio_Nakkiran_2024, 
  title={What algorithms can transformers learn? a study in length generalization},
  author={Zhou, Hattie and Bradley, Arwen and Littwin, Etai and Razin, Noam and Saremi, Omid and Susskind, Josh and Bengio, Samy and Nakkiran, Preetum},
  journal={arXiv preprint arXiv:2310.16028},
  year={2023}
}

 @article{Weiss_Goldberg_Yahav_2021, title={Thinking Like Transformers}, url={http://arxiv.org/abs/2106.06981}, DOI={10.48550/arXiv.2106.06981}, abstractNote={What is the computational model behind a Transformer? Where recurrent neural networks have direct parallels in ﬁnite state machines, allowing clear discussion and thought around architecture variants or trained models, Transformers have no such familiar parallel. In this paper we aim to change that, proposing a computational model for the transformer-encoder in the form of a programming language. We map the basic components of a transformer-encoder—attention and feed-forward computation—into simple primitives, around which we form a programming language: the Restricted Access Sequence Processing Language (RASP). We show how RASP can be used to program solutions to tasks that could conceivably be learned by a Transformer, and how a Transformer can be trained to mimic a RASP solution. In particular, we provide RASP programs for histograms, sorting, and Dyck-languages. We further use our model to relate their difﬁculty in terms of the number of required layers and attention heads: analyzing a RASP program implies a maximum number of heads and layers necessary to encode a task in a transformer. Finally, we see how insights gained from our abstraction might be used to explain phenomena seen in recent works.}, note={arXiv:2106.06981 [cs]}, number={arXiv:2106.06981}, publisher={arXiv}, author={Weiss, Gail and Goldberg, Yoav and Yahav, Eran}, year={2021}, month=jul, language={en} }


%%% Other
@inproceedings{
anonymous2024unlocking,
title={Unlocking State-Tracking in Linear {RNN}s Through Negative Eigenvalues},
author={Anonymous},
booktitle={Submitted to The Thirteenth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=UvTo3tVBk2},
note={under review}
}
@inproceedings{
sarrof2024expressive,
title={The Expressive Capacity of State Space Models: A Formal Language Perspective},
author={Yash Sarrof and Yana Veitsman and Michael Hahn},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=eV5YIrJPdy}
}
@article{gu2023mamba,
  title={Mamba: Linear-time sequence modeling with selective state spaces},
  author={Gu, Albert and Dao, Tri},
  journal={arXiv preprint arXiv:2312.00752},
  year={2023}
}
@inproceedings{
deletang2023neural,
title={Neural Networks and the Chomsky Hierarchy},
author={Gregoire Deletang and Anian Ruoss and Jordi Grau-Moya and Tim Genewein and Li Kevin Wenliang and Elliot Catt and Chris Cundy and Marcus Hutter and Shane Legg and Joel Veness and Pedro A Ortega},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=WbxHAzkeQcn}
}

@article{kleene1951representationof,
  title={Representation of Events in Nerve Nets and Finite Automata},
  author={Kleene, SC},
  journal={CE Shannon and J. McCarthy},
  year={1951}
}
@article{elman1991distributed,
  title={Distributed representations, simple recurrent networks, and grammatical structure},
  author={Elman, Jeffrey L},
  journal={Machine learning},
  volume={7},
  pages={195--225},
  year={1991},
  publisher={Springer}
}
@article{Omlin1996extraction,
title = {Extraction of rules from discrete-time recurrent neural networks},
journal = {Neural Networks},
volume = {9},
number = {1},
pages = {41-52},
year = {1996},
issn = {0893-6080},
doi = {https://doi.org/10.1016/0893-6080(95)00086-0},
url = {https://www.sciencedirect.com/science/article/pii/0893608095000860},
author = {Christian W. Omlin and C.Lee Giles},
keywords = {Recurrent neural networks, Grammatical inference, Regular languages, Deterministic finite-state automata, Rule extraction, Generalization performance, Model selection, Occam's razor},
}
@inproceedings{
dao2024transformers,
title={Transformers are {SSM}s: Generalized Models and Efficient Algorithms Through Structured State Space Duality},
author={Tri Dao and Albert Gu},
booktitle={Forty-first International Conference on Machine Learning},
year={2024},
url={https://openreview.net/forum?id=ztn8FCR1td}
}

@inproceedings{biderman2023pythia,
  title={Pythia: A suite for analyzing large language models across training and scaling},
  author={Biderman, Stella and Schoelkopf, Hailey and Anthony, Quentin Gregory and Bradley, Herbie and O’Brien, Kyle and Hallahan, Eric and Khan, Mohammad Aflah and Purohit, Shivanshu and Prashanth, USVSN Sai and Raff, Edward and others},
  booktitle={International Conference on Machine Learning},
  pages={2397--2430},
  year={2023},
  organization={PMLR}
}
@inproceedings{
sieber2024understanding,
title={Understanding the Differences in Foundation Models: Attention, State  Space Models, and Recurrent Neural Networks},
author={Jerome Sieber and Carmen Amo Alonso and Alexandre Didier and Melanie Zeilinger and Antonio Orvieto},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=iF7MnXnxRw}
}

@inproceedings{vaswani2017,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@inproceedings{
arora2024zoology,
title={Zoology: Measuring and Improving  Recall in Efficient Language Models},
author={Simran Arora and Sabri Eyuboglu and Aman Timalsina and Isys Johnson and Michael Poli and James Zou and Atri Rudra and Christopher Re},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=LY3ukUANko}
}
@inproceedings{
jelassi2024repeat,
title={Repeat After Me: Transformers are Better than State Space Models at Copying},
author={Samy Jelassi and David Brandfonbrener and Sham M. Kakade and eran malach},
booktitle={Forty-first International Conference on Machine Learning},
year={2024},
url={https://openreview.net/forum?id=duRRoGeoQT}
}
@inproceedings{
zucchet2024recurrent,
title={Recurrent neural networks: vanishing and exploding gradients are not the end of the story},
author={Nicolas Zucchet and Antonio Orvieto},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=46Jr4sgTWa}
}
@inproceedings{merrill2019sequential,
    title = "Sequential Neural Networks as Automata",
    author = "Merrill, William",
    editor = "Eisner, Jason  and
      Gall{\'e}, Matthias  and
      Heinz, Jeffrey  and
      Quattoni, Ariadna  and
      Rabusseau, Guillaume",
    booktitle = "Proceedings of the Workshop on Deep Learning and Formal Languages: Building Bridges",
    month = aug,
    year = "2019",
    address = "Florence",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-3901/",
    doi = "10.18653/v1/W19-3901",
    pages = "1--13",
}
@InProceedings{merrill2024illusion,
  title = 	 {The Illusion of State in State-Space Models},
  author =       {Merrill, William and Petty, Jackson and Sabharwal, Ashish},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {35492--35506},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/merrill24a/merrill24a.pdf},
  url = 	 {https://proceedings.mlr.press/v235/merrill24a.html},
  abstract = 	 {State-space models (SSMs) have emerged as a potential alternative architecture for building large language models (LLMs) compared to the previously ubiquitous transformer architecture. One theoretical weakness of transformers is that they cannot express certain kinds of sequential computation and state tracking (Merrill &amp; Sabharwal, 2023), which SSMs are explicitly designed to address via their close architectural similarity to recurrent neural networks (RNNs). <em>But do SSMs truly have an advantage (over transformers) in expressive power for state tracking?</em> Surprisingly, the answer is no. Our analysis reveals that the expressive power of SSMs is limited very similarly to transformers: SSMs cannot express computation outside the complexity class $\mathsf{TC}^0$. In particular, this means they cannot solve simple state-tracking problems like permutation composition. It follows that SSMs are provably unable to accurately track chess moves with certain notation, evaluate code, or track entities in a long narrative. To supplement our formal analysis, we report experiments showing that Mamba-style SSMs indeed struggle with state tracking. Thus, despite its recurrent formulation, the "state” in an SSM is an illusion: SSMs have similar expressiveness limitations to non-recurrent models like transformers, which may fundamentally limit their ability to solve real-world state-tracking problems.}
}
 @article{Merrill_Sabharwal_2023, title={The Parallelism Tradeoff: Limitations of Log-Precision Transformers}, volume={11}, rights={https://creativecommons.org/licenses/by/4.0/}, ISSN={2307-387X}, DOI={10.1162/tacl_a_00562}, abstractNote={Despite their omnipresence in modern NLP, characterizing the computational power of transformer neural nets remains an interesting open question. We prove that transformers whose arithmetic precision is logarithmic in the number of input tokens (and whose feedforward nets are computable using space linear in their input) can be simulated by constant-depth logspace-uniform threshold circuits. This provides insight on the power of transformers using known results in complexity theory. For example, if L = P (i.e., not all poly-time problems can be solved using logarithmic space), then transformers cannot even accurately solve linear equalities or check membership in an arbitrary context-free grammar with empty productions. Our result intuitively emerges from the transformer architecture’s high parallelizability. We thus speculatively introduce the idea of a fundamental parallelism tradeoff: any model architecture as parallelizable as the transformer will obey limitations similar to it. Since parallelism is key to training models at massive scale, this suggests a potential inherent weakness of the scaling paradigm.}, journal={Transactions of the Association for Computational Linguistics}, publisher={MIT Press}, author={Merrill, William and Sabharwal, Ashish}, year={2023}, month=jun, pages={531–545}, language={en} }

@article{barrington1989nc1,
title = {Bounded-width polynomial-size branching programs recognize exactly those languages in NC1},
journal = {Journal of Computer and System Sciences},
volume = {38},
number = {1},
pages = {150-164},
year = {1989},
issn = {0022-0000},
doi = {https://doi.org/10.1016/0022-0000(89)90037-8},
url = {https://www.sciencedirect.com/science/article/pii/0022000089900378},
author = {David A. Barrington},
}
@inproceedings{
schlag2020learning,
title={Learning Associative Inference Using Fast Weight Memory},
author={Imanol Schlag and Tsendsuren Munkhdalai and J{\"u}rgen Schmidhuber},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=TuK6agbdt27}
}
@article{weston2015towards,
  title={Towards ai-complete question answering: A set of prerequisite toy tasks},
  author={Weston, Jason and Bordes, Antoine and Chopra, Sumit and Rush, Alexander M and Van Merri{\"e}nboer, Bart and Joulin, Armand and Mikolov, Tomas},
  journal={arXiv preprint arXiv:1502.05698},
  year={2015}
}
@article{gao2020pile,
  title={The {P}ile: An 800GB Dataset of Diverse Text for Language Modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and Presser, Shawn and Leahy, Connor},
  journal={arXiv preprint arXiv:2101.00027},
  year={2020}
}
@misc{eval-harness,
  author       = {Gao, Leo and Tow, Jonathan and Abbasi, Baber and Biderman, Stella and Black, Sid and DiPofi, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and Le Noac'h, Alain and Li, Haonan and McDonell, Kyle and Muennighoff, Niklas and Ociepa, Chris and Phang, Jason and Reynolds, Laria and Schoelkopf, Hailey and Skowron, Aviya and Sutawika, Lintang and Tang, Eric and Thite, Anish and Wang, Ben and Wang, Kevin and Zou, Andy},
  title        = {A framework for few-shot language model evaluation},
  month        = 07,
  year         = 2024,
  publisher    = {Zenodo},
  version      = {v0.4.3},
  doi          = {10.5281/zenodo.12608602},
  url          = {https://zenodo.org/records/12608602}
}
@article{paperno2016lambada,
  title={The LAMBADA dataset: Word prediction requiring a broad discourse context},
  author={Paperno, Denis and Kruszewski, Germ{\'a}n and Lazaridou, Angeliki and Pham, Quan Ngoc and Bernardi, Raffaella and Pezzelle, Sandro and Baroni, Marco and Boleda, Gemma and Fern{\'a}ndez, Raquel},
  journal={arXiv preprint arXiv:1606.06031},
  year={2016}
}
@article{zellers2019hellaswag,
  title={Hellaswag: Can a machine really finish your sentence?},
  author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  journal={arXiv preprint arXiv:1905.07830},
  year={2019}
}
@inproceedings{bisk2020piqa,
  title={Piqa: Reasoning about physical commonsense in natural language},
  author={Bisk, Yonatan and Zellers, Rowan and Gao, Jianfeng and Choi, Yejin and others},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  number={05},
  pages={7432--7439},
  year={2020}
}
@article{clark2018think,
  title={Think you have solved question answering? try arc, the ai2 reasoning challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  journal={arXiv preprint arXiv:1803.05457},
  year={2018}
}
@article{sakaguchi2021winogrande,
  title={Winogrande: An adversarial winograd schema challenge at scale},
  author={Sakaguchi, Keisuke and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
  journal={Communications of the ACM},
  volume={64},
  number={9},
  pages={99--106},
  year={2021},
  publisher={ACM New York, NY, USA}
}
@article{mihaylov2018can,
  title={Can a suit of armor conduct electricity? a new dataset for open book question answering},
  author={Mihaylov, Todor and Clark, Peter and Khot, Tushar and Sabharwal, Ashish},
  journal={arXiv preprint arXiv:1809.02789},
  year={2018}
}
@article{SCHUTZENBERGER1965190,
title = {On finite monoids having only trivial subgroups},
journal = {Information and Control},
volume = {8},
number = {2},
pages = {190-194},
year = {1965},
issn = {0019-9958},
doi = {https://doi.org/10.1016/S0019-9958(65)90108-7},
url = {https://www.sciencedirect.com/science/article/pii/S0019995865901087},
author = {M.P. Schützenberger},
abstract = {An alternative definition is given for a family of subsets of a free monoid that has been considered by Trahtenbrot and by McNaughton.}
}

@article{strobl2024formal,
  title={What formal languages can transformers express? a survey},
  author={Strobl, Lena and Merrill, William and Weiss, Gail and Chiang, David and Angluin, Dana},
  journal={Transactions of the Association for Computational Linguistics},
  volume={12},
  pages={543--561},
  year={2024},
  publisher={MIT Press 255 Main Street, 9th Floor, Cambridge, Massachusetts 02142, USA~…}
}
@inproceedings{bhattamishra2020ability,
    title = "On the {A}bility and {L}imitations of {T}ransformers to {R}ecognize {F}ormal {L}anguages",
    author = "Bhattamishra, Satwik  and
      Ahuja, Kabir  and
      Goyal, Navin",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.576/",
    doi = "10.18653/v1/2020.emnlp-main.576",
    pages = "7096--7116",
    abstract = "Transformers have supplanted recurrent models in a large number of NLP tasks. However, the differences in their abilities to model different syntactic properties remain largely unknown. Past works suggest that LSTMs generalize very well on regular languages and have close connections with counter languages. In this work, we systematically study the ability of Transformers to model such languages as well as the role of its individual components in doing so. We first provide a construction of Transformers for a subclass of counter languages, including well-studied languages such as n-ary Boolean Expressions, Dyck-1, and its generalizations. In experiments, we find that Transformers do well on this subclass, and their learned mechanism strongly correlates with our construction. Perhaps surprisingly, in contrast to LSTMs, Transformers do well only on a subset of regular languages with degrading performance as we make languages more complex according to a well-known measure of complexity. Our analysis also provides insights on the role of self-attention mechanism in modeling certain behaviors and the influence of positional encoding schemes on the learning and generalization abilities of the model."
}

@inproceedings{merrillexpressive,
  title={The Expressive Power of Transformers with Chain of Thought},
  author={Merrill, William and Sabharwal, Ashish},
  booktitle={The Twelfth International Conference on Learning Representations}
}

@inproceedings{merrill2020formal,
  title={A Formal Hierarchy of RNN Architectures},
  author={Merrill, William and Weiss, Gail and Goldberg, Yoav and Schwartz, Roy and Smith, Noah A and Yahav, Eran},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={443--459},
  year={2020}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{merrill2022saturated,
  title={Saturated transformers are constant-depth threshold circuits},
  author={Merrill, William and Sabharwal, Ashish and Smith, Noah A},
  journal={Transactions of the Association for Computational Linguistics},
  volume={10},
  pages={843--856},
  year={2022},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}
@inproceedings{
gu2022efficiently,
title={Efficiently Modeling Long Sequences with Structured State Spaces},
author={Albert Gu and Karan Goel and Christopher Re},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=uYLFoz1vlAC}
}
@inproceedings{
smith2023simplified,
title={Simplified State Space Layers for Sequence Modeling},
author={Jimmy T.H. Smith and Andrew Warrington and Scott Linderman},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=Ai8Hw3AXqks}
}
@inproceedings{
qin2023hierarchically,
title={Hierarchically Gated Recurrent Neural Network for Sequence Modeling},
author={Zhen Qin and Songlin Yang and Yiran Zhong},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=P1TCHxJwLB}
}

@InProceedings{dauphin2017,
  title = 	 {Language Modeling with Gated Convolutional Networks},
  author =       {Yann N. Dauphin and Angela Fan and Michael Auli and David Grangier},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {933--941},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  url = 	 {https://proceedings.mlr.press/v70/dauphin17a.html},
}
@inproceedings{
Yang_Lee_Nowak_Papailiopoulos_2024,
title={Looped Transformers are Better at Learning Learning Algorithms},
author={Liu Yang and Kangwook Lee and Robert D Nowak and Dimitris Papailiopoulos},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=HHbRxoDTxE}
}
 @article{Bhattamishra_Ahuja_Goyal_2020, title={On the Ability and Limitations of Transformers to Recognize Formal Languages}, url={http://arxiv.org/abs/2009.11264}, DOI={10.48550/arXiv.2009.11264}, abstractNote={Transformers have supplanted recurrent models in a large number of NLP tasks. However, the differences in their abilities to model different syntactic properties remain largely unknown. Past works suggest that LSTMs generalize very well on regular languages and have close connections with counter languages. In this work, we systematically study the ability of Transformers to model such languages as well as the role of its individual components in doing so. We ﬁrst provide a construction of Transformers for a subclass of counter languages, including well-studied languages such as n-ary Boolean Expressions, Dyck-1, and its generalizations. In experiments, we ﬁnd that Transformers do well on this subclass, and their learned mechanism strongly correlates with our construction. Perhaps surprisingly, in contrast to LSTMs, Transformers do well only on a subset of regular languages with degrading performance as we make languages more complex according to a well-known measure of complexity. Our analysis also provides insights on the role of self-attention mechanism in modeling certain behaviors and the inﬂuence of positional encoding schemes on the learning and generalization abilities of the model.}, note={arXiv:2009.11264 [cs]}, number={arXiv:2009.11264}, publisher={arXiv}, author={Bhattamishra, Satwik and Ahuja, Kabir and Goyal, Navin}, year={2020}, month=oct, language={en} }

@misc{photonics_roadmap,
      title={Roadmap on Neuromorphic Photonics}, 
      author={Daniel Brunner and Bhavin J. Shastri and Mohammed A. Al Qadasi and H. Ballani and Sylvain Barbay and Stefano Biasi and Peter Bienstman and Simon Bilodeau and Wim Bogaerts and Fabian Böhm and G. Brennan and Sonia Buckley and Xinlun Cai and Marcello Calvanese Strinati and B. Canakci and Benoit Charbonnier and Mario Chemnitz and Yitong Chen and Stanley Cheung and Jeff Chiles and Suyeon Choi and Demetrios N. Christodoulides and Lukas Chrostowski and J. Chu and J. H. Clegg and D. Cletheroe and Claudio Conti and Qionghai Dai and Luigi Di Lauro and Nikolaos Panteleimon Diamantopoulos and Niyazi Ulas Dinc and Jacob Ewaniuk and Shanhui Fan and Lu Fang and Riccardo Franchi and Pedro Freire and Silvia Gentilini and Sylvain Gigan and Gian Luca Giorgi and C. Gkantsidis and J. Gladrow and Elena Goi and M. Goldmann and A. Grabulosa and Min Gu and Xianxin Guo and Matěj Hejda and F. Horst and Jih Liang Hsieh and Jianqi Hu and Juejun Hu and Chaoran Huang and Antonio Hurtado and Lina Jaurigue and K. P. Kalinin and Morteza Kamalian Kopae and D. J. Kelly and Mercedeh Khajavikhan and H. Kremer and Jeremie Laydevant and Joshua C. Lederman and Jongheon Lee and Daan Lenstra and Gordon H. Y. Li and Mo Li and Yuhang Li and Xing Lin and Zhongjin Lin and Mieszko Lis and Kathy Lüdge and Alessio Lugnan and Alessandro Lupo and A. I. Lvovsky and Egor Manuylovich and Alireza Marandi and Federico Marchesin and Serge Massar and Adam N. McCaughan and Peter L. McMahon and Miltiadis Moralis Pegios and Roberto Morandotti and Christophe Moser and David J. Moss and Avilash Mukherjee and Mahdi Nikdast and B. J. Offrein and Ilker Oguz and Bakhrom Oripov and G. O'Shea and Aydogan Ozcan and F. Parmigiani and Sudeep Pasricha and Fabio Pavanello and Lorenzo Pavesi and Nicola Peserico and L. Pickup and Davide Pierangeli and Nikos Pleros and Xavier Porte and Bryce A. Primavera and Paul Prucnal and Demetri Psaltis and Lukas Puts and Fei Qiao and B. Rahmani and Fabrice Raineri and Carlos A. Ríos Ocampo and Joshua Robertson and Bruno Romeira and Charles Roques Carmes and Nir Rotenberg and A. Rowstron and Steffen Schoenhardt and Russell L . T. Schwartz and Jeffrey M. Shainline and Sudip Shekhar and Anas Skalli and Mandar M. Sohoni and Volker J. Sorger and Miguel C. Soriano and James Spall and Ripalta Stabile and Birgit Stiller and Satoshi Sunada and Anastasios Tefas and Bassem Tossoun and Apostolos Tsakyridis and Sergei K. Turitsyn and Guy Van der Sande and Thomas Van Vaerenbergh and Daniele Veraldi and Guy Verschaffelt and E. A. Vlieg and Hao Wang and Tianyu Wang and Gordon Wetzstein and Logan G. Wright and Changming Wu and Chu Wu and Jiamin Wu and Fei Xia and Xingyuan Xu and Hangbo Yang and Weiming Yao and Mustafa Yildirim and S. J. Ben Yoo and Nathan Youngblood and Roberta Zambrini and Haiou Zhang and Weipeng Zhang},
      year={2025},
      eprint={2501.07917},
      archivePrefix={arXiv},
      primaryClass={cs.ET},
      url={https://arxiv.org/abs/2501.07917}, 
}

@article{anil2022path,
  title={Path independent equilibrium models can better exploit test-time computation},
  author={Anil, Cem and Pokle, Ashwini and Liang, Kaiqu and Treutlein, Johannes and Wu, Yuhuai and Bai, Shaojie and Kolter, J Zico and Grosse, Roger B},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={7796--7809},
  year={2022}
}

 @inproceedings{Du_Li_Tenenbaum_Mordatch,
  title={Learning iterative reasoning through energy minimization},
  author={Du, Yilun and Li, Shuang and Tenenbaum, Joshua and Mordatch, Igor},
  booktitle={International Conference on Machine Learning},
  pages={5570--5582},
  year={2022},
  organization={PMLR}
}


@article{siegelmann1991turing,
  title={Turing computability with neural nets},
  author={Siegelmann, Hava T and Sontag, Eduardo D},
  journal={Applied Mathematics Letters},
  volume={4},
  number={6},
  pages={77--80},
  year={1991},
  publisher={Elsevier}
}
@inproceedings{siegelmann1992,
author = {Siegelmann, Hava T. and Sontag, Eduardo D.},
title = {On the computational power of neural nets},
year = {1992},
isbn = {089791497X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130385.130432},
doi = {10.1145/130385.130432},
abstract = {This paper deals with finite networks which consist of interconnections of synchronously evolving processors. Each processor updates its state by applying a “sigmoidal” scalar nonlinearity to a linear combination of the previous states of all units. We prove that one may simulate all Turing Machines by rational nets. In particular, one can do this in linear time, and there is a net made up of about 1,000 processors which computes a universal partial-recursive function. Products (high order nets) are not required, contrary to what had been stated in the literature. Furthermore, we assert a similar theorem about non-deterministic Turing Machines. Consequences for undecidability and complexity issues about nets are discussed too.},
booktitle = {Proceedings of the Fifth Annual Workshop on Computational Learning Theory},
pages = {440–449},
numpages = {10},
location = {Pittsburgh, Pennsylvania, USA},
series = {COLT '92}
}
@article{lipton2015critical,
  title={A Critical Review of Recurrent Neural Networks for Sequence Learning},
  author={Lipton, Zachary Chase},
  journal={arXiv Preprint, CoRR, abs/1506.00019},
  year={2015}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, A},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}

@inproceedings{valmeekam2022large,
  title={Large language models still can't plan (a benchmark for LLMs on planning and reasoning about change)},
  author={Valmeekam, Karthik and Olmo, Alberto and Sreedharan, Sarath and Kambhampati, Subbarao},
  booktitle={NeurIPS 2022 Foundation Models for Decision Making Workshop},
  year={2022}
}



@article{jaech2024openai,
  title={Openai o1 system card},
  author={Jaech, Aaron and Kalai, Adam and Lerer, Adam and Richardson, Adam and El-Kishky, Ahmed and Low, Aiden and Helyar, Alec and Madry, Aleksander and Beutel, Alex and Carney, Alex and others},
  journal={arXiv preprint arXiv:2412.16720},
  year={2024}
}

%% advantages of looped models@article{Kaplan_McCandlish_Henighan_Brown_Chess_Child_Gray_Radford_Wu_Amodei_2020, title={Scaling Laws for Neural Language Models}, url={http://arxiv.org/abs/2001.08361}, DOI={10.48550/arXiv.2001.08361}, abstractNote={We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overﬁtting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a ﬁxed compute budget. Larger models are signiﬁcantly more sampleefﬁcient, such that optimally compute-efﬁcient training involves training very large models on a relatively modest amount of data and stopping signiﬁcantly before convergence.}, note={arXiv:2001.08361 [cs]}, number={arXiv:2001.08361}, publisher={arXiv}, author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario}, year={2020}, month=jan, language={en} }

@inproceedings{
soulos2024recurrent,
title={Recurrent Transformers Trade-off Parallelism for Length Generalization on Regular Languages},
author={Paul Soulos and Aleksandar Terzic and Michael Hersche and Abbas Rahimi},
booktitle={The First Workshop on System-2 Reasoning at Scale, NeurIPS'24},
year={2024},
url={https://openreview.net/forum?id=6PjZA4Jvge}
}

%% looped
@inproceedings{Bai_Kolter_Koltun_2019,
  author    = {Shaojie Bai and J. Zico Kolter and Vladlen Koltun},
  title     = {Deep Equilibrium Models},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year      = {2019},
}

% ponder net is already listed before

@inproceedings{
dehghani2018universal,
title={Universal Transformers},
author={Mostafa Dehghani and Stephan Gouws and Oriol Vinyals and Jakob Uszkoreit and Lukasz Kaiser},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=HyzdRiR9Y7},
}

%% hardware

 @article{Hooker_2020, title={The Hardware Lottery}, url={http://arxiv.org/abs/2009.06489}, DOI={10.48550/arXiv.2009.06489}, abstractNote={Hardware, systems and algorithms research communities have historically had diﬀerent incentive structures and ﬂuctuating motivation to engage with each other explicitly. This historical treatment is odd given that hardware and software have frequently determined which research ideas succeed (and fail). This essay introduces the term hardware lottery to describe when a research idea wins because it is suited to the available software and hardware and not because the idea is superior to alternative research directions. Examples from early computer science history illustrate how hardware lotteries can delay research progress by casting successful ideas as failures. These lessons are particularly salient given the advent of domain specialized hardware which make it increasingly costly to stray oﬀ of the beaten path of research ideas. This essay posits that the gains from progress in computing are likely to become even more uneven, with certain research directions moving into the fast-lane while progress on others is further obstructed.}, note={arXiv:2009.06489 [cs]}, number={arXiv:2009.06489}, publisher={arXiv}, author={Hooker, Sara}, year={2020}, month=sep, language={en} }

@misc{hao2024traininglargelanguagemodels,
      title={Training Large Language Models to Reason in a Continuous Latent Space}, 
      author={Shibo Hao and Sainbayar Sukhbaatar and DiJia Su and Xian Li and Zhiting Hu and Jason Weston and Yuandong Tian},
      year={2024},
      eprint={2412.06769},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.06769}, 
}
@inproceedings{
vafa2024evaluating,
title={Evaluating the World Model Implicit in a Generative Model},
author={Keyon Vafa and Justin Y. Chen and Ashesh Rambachan and Jon Kleinberg and Sendhil Mullainathan},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=aVK4JFpegy}
}
@inproceedings{
Beck_Pöppel_Spanring_Auer_Prudnikova_Kopp_Klambauer_Brandstetter_Hochreiter,
title={x{LSTM}: Extended Long Short-Term Memory},
author={Maximilian Beck and Korbinian P{\"o}ppel and Markus Spanring and Andreas Auer and Oleksandra Prudnikova and Michael K Kopp and G{\"u}nter Klambauer and Johannes Brandstetter and Sepp Hochreiter},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=ARAxPPIAhq}
}
@article{Grazzi_Siems_Franke_Zela_Hutter_Pontil_2024, title={Unlocking State-Tracking in Linear RNNs Through Negative Eigenvalues}, url={http://arxiv.org/abs/2411.12537}, DOI={10.48550/arXiv.2411.12537}, abstractNote={Linear Recurrent Neural Networks (LRNNs) such as Mamba, RWKV, GLA, mLSTM, and DeltaNet have emerged as efficient alternatives to Transformers in large language modeling, offering linear scaling with sequence length and improved training efficiency. However, LRNNs struggle to perform state-tracking which may impair performance in tasks such as code evaluation or tracking a chess game. Even parity, the simplest state-tracking task, which non-linear RNNs like LSTM handle effectively, cannot be solved by current LRNNs. Recently, Sarrof et al. (2024) demonstrated that the failure of LRNNs like Mamba to solve parity stems from restricting the value range of their diagonal state-transition matrices to [0, 1] and that incorporating negative values can resolve this issue. We extend this result to non-diagonal LRNNs, which have recently shown promise in models such as DeltaNet. We prove that finite precision LRNNs with state-transition matrices having only positive eigenvalues cannot solve parity, while complex eigenvalues are needed to count modulo 3. Notably, we also prove that LRNNs can learn any regular language when their state-transition matrices are products of identity minus vector outer product matrices, each with eigenvalues in the range [−1, 1]. Our empirical results confirm that extending the eigenvalue range of models like Mamba and DeltaNet to include negative values not only enables them to solve parity but consistently improves their performance on state-tracking tasks. Furthermore, pre-training LRNNs with an extended eigenvalue range for language modeling achieves comparable performance and stability while showing promise on code and math data. Our work enhances the expressivity of modern LRNNs, broadening their applicability without changing the cost of training or inference.}, note={arXiv:2411.12537 [cs]}, number={arXiv:2411.12537}, publisher={arXiv}, author={Grazzi, Riccardo and Siems, Julien and Franke, Jörg K. H. and Zela, Arber and Hutter, Frank and Pontil, Massimiliano}, year={2024}, month=dec, language={en} }

@inproceedings{
Yang_Wang_Zhang_Shen_Kim,
title={Parallelizing Linear Transformers with the Delta Rule over Sequence Length},
author={Songlin Yang and Bailin Wang and Yu Zhang and Yikang Shen and Yoon Kim},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=y8Rm4VNRPH}
}

@article{geng2021training,
  title={On training implicit models},
  author={Geng, Zhengyang and Zhang, Xin-Yu and Bai, Shaojie and Wang, Yisen and Lin, Zhouchen},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={24247--24260},
  year={2021}
}

@inproceedings{
lim2024parallelizing,
title={Parallelizing non-linear sequential models over the sequence length},
author={Yi Heng Lim and Qi Zhu and Joshua Selfridge and Muhammad Firmansyah Kasim},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=E34AlVLN0v}
}

 @article{Gonzalez_Warrington_Smith_Linderman_2025, title={Towards Scalable and Stable Parallelization of Nonlinear RNNs}, url={http://arxiv.org/abs/2407.19115}, DOI={10.48550/arXiv.2407.19115}, abstractNote={Transformers and linear state space models can be evaluated in parallel on modern hardware, but evaluating nonlinear RNNs appears to be an inherently sequential problem. Recently, however, Lim et al. [1] developed an approach called DEER, which evaluates nonlinear RNNs in parallel by posing the states as the solution to a fixed-point problem. They derived a parallel form of Newton’s method to solve the fixed-point problem and achieved significant speedups over sequential evaluation. However, the computational complexity of DEER is cubic in the state size, and the algorithm can suffer from numerical instability. We address these limitations with two novel contributions. To reduce the computational complexity, we apply quasi-Newton approximations and show they converge comparably to Newton, use less memory, and are faster. To stabilize DEER, we leverage a connection between the Levenberg-Marquardt algorithm and Kalman smoothing, which we call ELK. This connection allows us to stabilize Newton’s method while using efficient parallelized Kalman smoothing algorithms to retain performance. Through several experiments, we show that these innovations allow for parallel evaluation of nonlinear RNNs at larger scales and with greater stability.}, note={arXiv:2407.19115 [cs]}, number={arXiv:2407.19115}, publisher={arXiv}, author={Gonzalez, Xavier and Warrington, Andrew and Smith, Jimmy T. H. and Linderman, Scott W.}, year={2025}, month=jan, language={en} }
@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@inproceedings{li2021implicit,
  title={Implicit Representations of Meaning in Neural Language Models},
  author={Li, Belinda Z. and Nye, Maxwell and Andreas, Jacob},
  booktitle={Proceedings of the Association for Computational Linguistics and International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  pages={4744--4756},
  year={2021},
  organization={Association for Computational Linguistics},
  url={https://aclanthology.org/2021.acl-long.143/}
}

@inproceedings{guan2023leveraging,
  title={Leveraging Pre-trained Large Language Models to Construct and Utilize World Models for Model-based Task Planning},
  author={Guan, Lin and Valmeekam, Karthik and Sreedharan, Sarath and Kambhampati, Subbarao},
  booktitle={Advances in Neural Information Processing Systems},
  year={2023},
  url={https://arxiv.org/abs/2305.14909}
}

@inproceedings{li2023emergent,
  title={Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task},
  author={Kenneth Li and Aspen K. Hopkins and David Bau and Fernanda Vi{\'e}gas and Hanspeter Pfister and Martin Wattenberg},
  booktitle={International Conference on Learning Representations},
  year={2023},
  url={https://openreview.net/forum?id=DeG07_TcZvT}
}

@article{briand2023dna,
  title={DNA language models are powerful predictors of genome-wide variant effects},
  author={Briand, Thomas and Rombach, Clemens and Menden, Michael P. and Stegle, Oliver and Luecken, Malte D.},
  journal={Proceedings of the National Academy of Sciences},
  volume={120},
  number={43},
  pages={e2311219120},
  year={2023},
  publisher={National Academy of Sciences},
  url={https://www.pnas.org/doi/10.1073/pnas.2311219120}
}

@article{chowdhury2022single,
  title={Single-sequence protein structure prediction using a language model and deep learning},
  author={Chowdhury, Ratul and Bouatta, Nazim and Biswas, Surojit and Floristean, Alexandru and Kharkar, Arjun and Roy, Ron and Rochereau, Claire and Zhang, Jian and Church, George M and Sorger, Peter K and AlQuraishi, Mohammed},
  journal={Nature Biotechnology},
  volume={40},
  pages={1617--1623},
  year={2022},
  publisher={Nature Publishing Group},
  url={https://www.nature.com/articles/s41587-022-01432-w}
}

@article{boiko2023autonomous,
  title={Autonomous chemical research with large language models},
  author={Boiko, Daniil A. and MacKnight, Robert and Kline, Ben and Gomes, Gabriel N.},
  journal={Nature},
  volume={620},
  pages={547--552},
  year={2023},
  publisher={Nature Publishing Group},
  url={https://www.nature.com/articles/s41586-023-06792-0}
}

@article{hagele2024scaling,
  title={Scaling Laws and Compute-Optimal Training Beyond Fixed Training Durations},
  author={H{\"a}gele, Alexander and Bakouch, Elie and Kosson, Atli and Allal, Loubna Ben and Von Werra, Leandro and Jaggi, Martin},
  journal={arXiv preprint arXiv:2405.18392},
  year={2024}
}