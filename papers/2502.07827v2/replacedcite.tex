\section{Related Work}
\paragraph{Adaptive-Compute Time} The idea of an adaptive compute budget goes back to ____ who employ a halting neuron to delimit the computation on a particular input. ____ generalized the idea and regularised the halting condition to encourage the network to stop early.
They implemented an adaptive-depth RNN and demonstrated the network adjusting the compute budget based on the difficulty of instances in a parity-check task.
This idea was later applied to Transformers, resulting in ''Universal Transformers'' (UT)____. UTs can either be unrolled to a fixed depth or augmented with a dynamic halting condition (DHC) per token.
UTs were later shown to exhibit improved scaling laws compared to standard transformers____.
PonderNet____ introduced a principled probabilistic model for determining the halting condition. This approach improved on the UT on the \textsc{bAbi} benchmark.
Recently, a mixture-of-experts (MoE) variant of the UT (MoEUT) was presented____ with 1B parameters, seeking to improve the parameter-to-compute ratio of UTs.
The MoEUT is an unrolled model with fixed iterations and does not employ a DHC. While our models presented here are dense, they could, in principle, be turned into MoE. 
____ show that looped linear transformers implement gradient-descent until convergence on the prediction loss defined by previous input-output examples in the context window.
____ take the opposite approach to our work:
Instead of augmenting SSMs or transformers, they propose an approach based on fixed-point iterations to enable parallel training of RNNs.
However, their method incurs cubic cost in terms of state size, limiting the method to smaller models. 

\paragraph{Reasoning and out-of-distribution generalization.}

The ability of looped models to generalize better to input lengths not seen during training is empirically well established: For example____ show this for looped transformers, while ____ demonstrate length generalization for DEQs, particularly when they are path independent.
____ show that energy-based models trained to map energy-gradient-descent steps to algorithmic steps, can length generalize in summation, and complex algorithms such as shortest-path.
On the theoretical side,
The pioneering work of ____ shows that iterated RNNs are Turing complete at infinite numerical precision. 
More recently, ____ studied a number of sequence models and report that grouping tasks by their rung in the Chomsky hierarchy is predictive of models ability to length-generalize. 
While the works of Merrill \emph{et al}~ ____, which we discuss in\Cref{sec:limitations-transformer-ssm}, showed that both transformers and SSMs are restricted to \tc; several studies sought to find more precise constraints.
____ observe that programs written in a specific language (RASP) can be mapped to transformer models of sufficient capacity.
____ then showed that transformers tend to length-generalise if the underlying data-generating process can be expressed in RASP.
____ derived a similar refined constraint for SSMs and showed that they can precisely express star-free regular languages.
____ demonstrate that SSMs can track state in simple problems, such as parity, when their (diagonal) recurrence matrix \(\Lambda\) in Equation~\eqref{eq:linear-ssm} permits negative eigenvalues.
Moreover, they illustrate that a variant of DeltaNet____ with (possibly) negative eigenvalues can solve the S5 problem when only swaps of two values are considered in the transition.
However, no variant of Mamba or DeltaNet was capable of learning S5 and achieving length generalization.
To tackle the parallelization-expressiveness trade-off, ____ propose two new LSTM-inspired layer architectures: the sLSTM and mLSTM layers.
While the latter is parallelizable, the former is not and intended to enable the whole model to recognize regular languages.
Finally, ____ survey strategies for chunking input sequences with transformers, maintaining parallelizability within each chunk and using RNN-like transitions between chunks.
They find these architectures recognize regular languages for small chunk sizes with scaling remaining a challenge.