\begin{table*}[h!]     
\small  
    \centering
    \caption{GPU resource allocation and utilization for training large language models. The GPU counts employed and the total GPU hours expended for the training of each Mamba2 and Llama model variant across different parameter scales---130M, 350M, 760M, and 1.3B is listed. The models were trained using a cluster of AMD Instinct MI300X GPUs, with 8 GPUs per node, utilizing distributed multi-node processing with up to 32 GPUs in parallel.}  
    \resizebox{0.5\textwidth}{!}{%        
        \begin{tabular}{lcccc}   
            \toprule        
             & \textbf{Model} & \textbf{GPU Counts} & \textbf{Total GPU Hours}\\      
            \midrule  
%130M scale  
\multirow{6}{*}{\rotatebox{90}{130M}}&  Mamba2$^*$ & 8 &1620
\\
 & Mamba2 (4+1)  & 8 &3185.6
 \\    
 & Mamba2 (24+4)  & 8 & 2756.8
\\ 
 \cmidrule(l){2-4}
   & Llama$^\dag$  & 32 & 1146.56
\\
 & Llama (4 + 1) & 32 & 1027.2
 \\
& Llama (32+4) & 32 &  1561.6
\\
  
 \midrule    
%350M scale 
\multirow{6}{*}{\rotatebox{90}{350M}} & Mamba2$^*$ & 32 &1516\\
 & Mamba2 (4+1)  & 8 & 2427.2
\\ 
 & Mamba2 (24+4)  & 8 & 2168.8
\\     
  \cmidrule(l){2-4}
  & Llama$^\dag$  & 32 & 1324.8
\\
& Llama (4 + 1) & 8 & 2515.2
 \\
& Llama (32+4) & 8 &  2335.2
\\  
\midrule  
    %760M scale  
\multirow{6}{*}{\rotatebox{90}{760M}}&Mamba2$^*$ & 16 &1920
\\
 & Mamba2 (4+1)  & 16 & 3932.8
\\ 
 & Mamba2 (24+4)  & 16 & 3440
\\  
  \cmidrule(l){2-4}
  & Llama$^\dag$  & 16 & 4636.8
\\
& Llama (4 + 1) & 16 &7612.8
  \\
& Llama (32+4) & 32 & 7676.8
 \\     
\midrule  
    %1.3B scale  
\multirow{6}{*}{\rotatebox{90}{1.3B}} &Mamba2$^*$ & 32 & 3054.4
\\ 
 & Mamba2 (4+1)  & 32 &5820.8 \\ 
 & Mamba2 (24+4) & 32 & 5084.8

 \\  
  \cmidrule(l){2-4}
   & Llama$^\dag$  & 32 & 3084.8
\\
 & Llama (4 + 1) & 32 & 6057.6
 \\ 
& Llama (32+4) & 32 &  7225.6
\\
    \midrule     
&Sum & & 83132.16\\
            \bottomrule 
        \end{tabular}        
    }    
        
    \label{tab:gpu-hours}      
\end{table*}  