
  
\begin{table*}[t!]      
    \centering  
        
    \caption{
        Comparison of test set perplexity and downstream performance. 
        We compare our implicit models, which have 4 self-iteration steps and 1 phantom gradient step (denoted as 4+1), and those with 24/32 self-iteration steps and 4 phantom gradient steps (denoted as 24+4/32+4), with our baseline models Mamba2$^*$ and Llama$^\dag$. 
        These baseline models as well as the implicit models are trained on 207B tokens from the \textsc{D-Pile} dataset and range in size from 130M to 1.3B parameters. 
        For further comparison, we include the original Mamba2 ~\citep{dao2024transformers} (trained on 300B tokens of the \textsc{Pile}) and the Llama (trained on 300B tokens of the \textsc{SlimPajama}) from \citep{Beck_PÃ¶ppel_Spanring_Auer_Prudnikova_Kopp_Klambauer_Brandstetter_Hochreiter}. 
        % The first column lists the model types, the second column specifies the training dataset, the fourth column shows the next-token prediction perplexity on the D-Pile, and the subsequent columns detail performance on various downstream tasks. 
        The best performing model for each type is highlighted in bold, and the second-best is underlined. 
        % Across all sizes and the majority of tasks, the implicit Llama models outperform the others. 
        % For larger models with 760M and 1.3B parameters, the implicit Mamba2 models surpass the baseline Mamba2.
    } \label{tab:llm-metrics} 
    \resizebox{\textwidth}{!}{%      
        \begin{tabular}{lcccccccccccc} 
            \toprule      
             & \textbf{Model} & \textbf{Dataset/Tokens (B)} & \textbf{D-Pile} & \textbf{LAMBADA} & \textbf{LAMBADA} & \textbf{HellaSwag} & \textbf{PIQA} & \textbf{Arc-E} & \textbf{Arc-C} & \textbf{WinoGrande} & \textbf{OpenbookQA} & \textbf{Average} \\    
            & & & \textbf{ppl$\downarrow$} & \textbf{ppl$\downarrow$} & \textbf{acc$\uparrow$} & \textbf{acc$\uparrow$} & \textbf{acc$\uparrow$} & \textbf{acc$\uparrow$} & \textbf{acc$\uparrow$} & \textbf{acc$\uparrow$} & \textbf{acc$\uparrow$} &\textbf{acc$\uparrow$}\\    
            \midrule

            % 130M Group 
  \multirow{8}{*}{\rotatebox{90}{130M}} &
            Mamba2& Pile/300 & 13.72 & \textbf{16.83} & \textbf{0.4388} & 0.3525 & \underline{0.6496} &  \underline{0.4739} & \textbf{0.2423} & \textbf{0.5233} & \textbf{0.306} & \textbf{0.4266} \\  
            
            & Mamba2$^*$ & D-Pile/207 & \underline{13.05} & 18.51 & 0.4116 & 0.3527 & \textbf{0.6572} & \textbf{0.4815} & \underline{0.2372} & 0.5130 & \underline{0.300} & \underline{0.4219} \\  
            
            & Mamba2(4+1)-ours & D-Pile/207 & 13.76 & 18.58 & 0.4118 & \underline{0.3628} & 0.6485 & 0.4537 & 0.2287 & 0.5107 & 0.288 & 0.4149 \\  
            
            &Mamba2(24+4)-ours& D-Pile/207 & \textbf{12.86} & \underline{18.03} & \underline{0.4174} & \textbf{0.3673} & \underline{0.6496} & 0.4604 & \underline{0.2372} & \underline{0.5178} & 0.290 & 0.4200 \\ 
            
            \cmidrule(l){2-13}
            & Llama & SlimPajama/300 &- & 39.21 & 0.3154 & 0.3409 & \underline{0.6545} & 0.4533 & 0.2363 & 0.5067 &- & 0.4178 \\
            
            & Llama$^\dag$&D-Pile/207 & 12.77& 17.08 & 0.4297 & 0.3513 & 0.6540 & 0.4794 & \textbf{0.2440} & 0.5122 & 0.280 & 0.4215 \\   
            
            &Llama (4+1)-ours& D-Pile/207 & \underline{12.73} & \underline{15.54} & \underline{0.4518} & \underline{0.3706} & 0.6447 & \underline{0.4823} & \underline{0.2372} & \textbf{0.5391} & \underline{0.290} & \underline{0.4308} \\  

            &Llama (32+4)-ours& D-Pile/207 & \textbf{11.73} & \textbf{13.39} & \textbf{0.4801} & \textbf{0.3958} & \textbf{0.6676} & \textbf{0.4886} & 0.2355 & \underline{0.5304} & \textbf{0.298} & \textbf{0.4423} \\  



            \midrule
            % 350M Group  
            \multirow{8}{*}{\rotatebox{90}{350M}} & Mamba2 & Pile/300 &10.55 & \textbf{8.00} & \textbf{0.5593} & \underline{0.4692} & \textbf{0.7046} & 0.5476& 0.2671 & \textbf{0.5564} & \textbf{0.324} & \textbf{0.4897} \\  
            
            & Mamba2$^*$ & D-Pile/207 & 10.18 & 8.96 & 0.5333 & 0.4653 & 0.6942 & \textbf{0.5526} & \underline{0.2696} & 0.5320 & 0.306 & 0.4790 \\  
            
            &Mamba2(4+1)-ours &D-Pile/207 & \underline{10.02}& 8.79 & 0.5457 & 0.4684 & 0.6899 & 0.5358 & \textbf{0.2696} & 0.5162 & 0.308 & 0.4762 \\ 
            
           &Mamba2(24+4)-ours &D-Pile/207 & \textbf{9.70} & \underline{8.26} & \underline{0.5575} & \textbf{0.4792} & \underline{0.7040} & \underline{0.5484} & 0.2688 & \underline{0.5351} & \underline{0.316} & \underline{0.487} \\ 
           
           \cmidrule(l){2-13} 
           & Llama & SlimPajama/300 & - & 15.73 & 0.4419 & 0.4445 & 0.6915 & 0.5223 & \underline{0.2628} & 0.5359 & - & 0.4832 \\
           
           &Llama$^\dag$ & D-Pile/207 & 10.30 & 8.37 & 0.5624 & 0.4537 & 0.6844 & \underline{0.5476} & 0.2577 & 0.5541 & \underline{0.318} & 0.4826 \\  
           
            &Llama (4+1)-ours & D-Pile/207& \underline{9.66}& \textbf{7.03} & \underline{0.5898} & \underline{0.5030} & \underline{0.7024} & \textbf{0.5539} & 0.2611 & \underline{0.5572} & 0.314 & \underline{0.4973} \\  
            
            &Llama (32+4)-ours & D-Pile/207& \textbf{9.43}& \underline{7.04} & \textbf{0.5956} & \textbf{0.5114} & \textbf{0.7078} & 0.5244 & \textbf{0.2705} & \textbf{0.5722} & \textbf{0.320} & \textbf{0.5003} \\  
            
             \midrule
            % 760M Group  
           \multirow{8}{*}{\rotatebox{90}{760M}} & Mamba2 & Pile/300 &  9.23& \textbf{5.86} & \underline{0.6167} & 0.5492 & 0.7198 & \textbf{0.6103} & 0.2850 & \textbf{0.6030} & \textbf{0.362} & \underline{0.5351} \\
           
           & Mamba2$^*$ & D-Pile/207 & 8.98 & 6.24 & 0.6125 & 0.5418 & 0.7231 & 0.6044 & 0.2858 & \underline{0.5777} & \underline{0.338} & 0.5262 \\
           
            &Mamba2(4+1)-ours &D-Pile/207 & \underline{8.60} & 6.15 & 0.6117 & \underline{0.5569} & \underline{0.7296} & 0.6077 & \textbf{0.3140} & 0.5509 & 0.336 & 0.5295 \\
            
            &Mamba2(24+4)-ours &D-Pile/207 &\textbf{8.35} & \underline{5.90} & \textbf{0.6191} & \textbf{0.5698} & \textbf{0.7334} & \underline{0.6090} & \underline{0.3131} & 0.5730 & \underline{0.338} & \textbf{0.5365} \\  
            
            \cmidrule(l){2-13} 
          & Llama & SlimPajama/300 & - & 9.90 & 0.5141 & 0.5216 & 0.7095 & 0.5648 & 0.2875 & 0.5667 & - & 0.5274 \\ 
          
        &Llama$^\dag$ & D-Pile/207& 8.88& 5.77 & 0.6375 & 0.5448 & 0.7171& 0.5905 & 0.2816& \textbf{0.6054} & 0.338& 0.5307 \\  
            
        &Llama (4+1)-ours & D-Pile/207& \underline{8.27}& \underline{5.15} & \underline{0.6524} & \underline{0.5853} & \underline{0.7312} & \underline{0.6052} & \textbf{0.3097} & 0.5967 & \textbf{0.356} & \underline{0.5481} \\  

        &Llama (32+4)-ours & D-Pile/207 & \textbf{7.90} & \textbf{4.82} & \textbf{0.6703} & \textbf{0.5995} & \textbf{0.7416} & \textbf{0.6187} & \underline{0.3012} & \underline{0.5991} & \underline{0.344} & \textbf{0.5535} \\  
            \midrule  

            % 1.3B Group  
            \multirow{8}{*}{\rotatebox{90}{1.3B}} &
            
            Mamba2 & Pile/300 &8.40  & \underline{5.02} & \textbf{0.6559} & 0.5995 & 0.7378 & 0.6418 & \underline{0.3319} & \textbf{0.6117} & \textbf{0.378} & \textbf{0.5652} \\
            
            & Mamba2$^*$ & D-Pile/207 & 8.28 & 5.12 & 0.6456 & 0.5939 & \underline{0.741}6 & 0.6145 & 0.3123 & \underline{0.6117} & 0.352 & 0.5531 \\ 
            
            &Mamba2(4+1)-ours & D-Pile/207& \underline{7.97} & 5.21 & 0.6383 & \underline{0.6136} & \textbf{0.7437} & \textbf{0.6343} & 0.3302 & 0.5746 & \underline{0.354} & 0.5555 \\  
            
            &Mamba2(24+4)-ours & D-Pile/207& \textbf{7.70 }& \textbf{4.99} & \underline{0.6489} & \textbf{0.6267} & \underline{0.7416} & \underline{0.6423 }& \textbf{0.3336} & 0.5888 & 0.352 & \underline{0.5620} \\ 

            \cmidrule(l){2-13} 
            &Llama & SlimPajama/300 & - & 7.23 & 0.5744 & 0.5781 & 0.7312 & 0.6279 & 0.3174 & 0.5904 & - & 0.5699 \\  
            
            &Llama$^\dag$  & D-Pile/207 & 7.99  & 4.95 & 0.6569 & 0.5936 & 0.7432 & \underline{0.6385} & 0.3217 & 0.6062 & 0.352 & 0.5589 \\  
            
            &Llama (4+1)-ours &D-Pile/207 &\underline{7.66} & \underline{4.40} & \underline{0.6852} & \underline{0.6397} & \underline{0.7448} & 0.6338 & \underline{0.3396} & \textbf{0.6575} & \underline{0.360} & \underline{0.5801} \\ 
            
            & Llama (32+4)-ours & D-Pile/207 & \textbf{7.24} & \textbf{4.24} & \textbf{0.6901} & \textbf{0.6583} & \textbf{0.7465} & \textbf{0.6654} & \textbf{0.3601} & \underline{0.6401} & \textbf{0.364} & \textbf{0.5892} \\  
            \bottomrule      
        \end{tabular}
        }   
\end{table*}