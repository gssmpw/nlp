\begin{table*}[h!]     
\small  
    \centering        
        \caption{Inference dynamics and convergence characteristics of implicit language models across different scales. We show the performance of Mamba2 and Llama models at various parameter scales—130M, 350M, 760M, and 1.3B—when processing sequences of length 2048 from the test split of the Pile dataset. The average number of steps required for convergence during inference in simultaneous mode is detailed alongside the relative error difference of the solutions obtained by the models. Remarkably, the (4+1) configurations reach a fixed point organically, without explicit constraints enforcing this behavior. The table further highlights that implicit models employing a full DEQ setup—(24+4) for Mamba2 and (32+4) for Llama—demonstrate consistent convergence within their predefined stop thresholds, which are less than $0.05$. These thresholds are also below the designated inference step threshold of four times 24 and 32 for the respective models.}   
    \resizebox{0.5\textwidth}{!}{%        
        \begin{tabular}{lcccc}   
            \toprule        
             & \textbf{Model} & \textbf{D-Pile Perplexity} & \textbf{Inference Steps} & \textbf{Rel. Diff.}\\      
            \midrule  
%130M scale  
\multirow{4}{*}{\rotatebox{90}{130M}} & Mamba2(4+1)  & 13.76 & 14 & 0.035 \\      
& Mamba2(24+4)  & 12.86 & 62 & 0.036 \\
\cmidrule(l){2-5}
& Llama(4+1)  & 12.73 & 13 & 0.014 \\     
&  Llama(32+4)  & 11.73 & 53 & 0.033 \\    
 \midrule    
%350M scale  
\multirow{4}{*}{\rotatebox{90}{350M}} & Mamba2(4+1)  & 10.02 & 10 & 0.012 \\   
& Mamba2(24+4)  & 9.70 & 49 & 0.024 \\ 
\cmidrule(l){2-5}
& Llama(4+1)  & 9.66 & 12 & 0.015 \\  
&  Llama(32+4)  & 9.43 & 57 & 0.037 \\    
            \midrule  
    %760M scale  
\multirow{4}{*}{\rotatebox{90}{760M}} & Mamba2(4+1)  & 8.60 & 10 & 0.013 \\      
& Mamba2(24+4)  & 8.35 & 45 & 0.025 \\ 
\cmidrule(l){2-5}
& Llama(4+1)  & 8.27 & 12 & 0.014 \\      
&  Llama(32+4)  & 7.90 & 77 & 0.044 \\    
            \midrule  
    %1.3B scale  
\multirow{4}{*}{\rotatebox{90}{1.3B}} & Mamba2(4+1)  & 7.97 & 10 & 0.013 \\      
& Mamba2(24+4)  & 7.70 & 47 & 0.029 \\ 
\cmidrule(l){2-5}
& Llama(4+1)  & 7.66 & 13 & 0.015 \\      
&  Llama(32+4)  & 7.24 & 69 & 0.048 \\    
            \bottomrule        
        \end{tabular}        
    }    
   
    \label{tab:implicit-dynamics stats}      
\end{table*}  