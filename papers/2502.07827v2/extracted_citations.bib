@article{Banino_Balaguer_Blundell_2021, title={PonderNet: Learning to Ponder}, url={http://arxiv.org/abs/2107.05407}, DOI={10.48550/arXiv.2107.05407}, abstractNote={In standard neural networks the amount of computation used grows with the size of the inputs, but not with the complexity of the problem being learnt. To overcome this limitation we introduce PonderNet, a new algorithm that learns to adapt the amount of computation based on the complexity of the problem at hand. PonderNet learns end-to-end the number of computational steps to achieve an eﬀective compromise between training prediction accuracy, computational cost and generalization. On a complex synthetic problem, PonderNet dramatically improves performance over previous adaptive computation methods and additionally succeeds at extrapolation tests where traditional neural networks fail. Also, our method matched the current state of the art results on a real world question and answering dataset, but using less compute. Finally, PonderNet reached state of the art results on a complex task designed to test the reasoning capabilities of neural networks.}, note={arXiv:2107.05407 [cs]}, number={arXiv:2107.05407}, publisher={arXiv}, author={Banino, Andrea and Balaguer, Jan and Blundell, Charles}, year={2021}, month=sep, language={en} 
}

@inproceedings{Du_Li_Tenenbaum_Mordatch,
  title={Learning iterative reasoning through energy minimization},
  author={Du, Yilun and Li, Shuang and Tenenbaum, Joshua and Mordatch, Igor},
  booktitle={International Conference on Machine Learning},
  pages={5570--5582},
  year={2022},
  organization={PMLR}
}

@article{Graves_2017, title={Adaptive Computation Time for Recurrent Neural Networks}, url={http://arxiv.org/abs/1603.08983}, DOI={10.48550/arXiv.1603.08983}, abstractNote={This paper introduces Adaptive Computation Time (ACT), an algorithm that allows recurrent neural networks to learn how many computational steps to take between receiving an input and emitting an output. ACT requires minimal changes to the network architecture, is deterministic and diﬀerentiable, and does not add any noise to the parameter gradients. Experimental results are provided for four synthetic problems: determining the parity of binary vectors, applying binary logic operations, adding integers, and sorting real numbers. Overall, performance is dramatically improved by the use of ACT, which successfully adapts the number of computational steps to the requirements of the problem. We also present character-level language modelling results on the Hutter prize Wikipedia dataset. In this case ACT does not yield large gains in performance; however it does provide intriguing insight into the structure of the data, with more computation allocated to harder-to-predict transitions, such as spaces between words and ends of sentences. This suggests that ACT or other adaptive computation methods could provide a generic method for inferring segment boundaries in sequence data.}, note={arXiv:1603.08983 [cs]}, number={arXiv:1603.08983}, publisher={arXiv}, author={Graves, Alex}, year={2017}, month=feb, language={en} }

@article{Grazzi_Siems_Franke_Zela_Hutter_Pontil_2024, title={Unlocking State-Tracking in Linear RNNs Through Negative Eigenvalues}, url={http://arxiv.org/abs/2411.12537}, DOI={10.48550/arXiv.2411.12537}, abstractNote={Linear Recurrent Neural Networks (LRNNs) such as Mamba, RWKV, GLA, mLSTM, and DeltaNet have emerged as efficient alternatives to Transformers in large language modeling, offering linear scaling with sequence length and improved training efficiency. However, LRNNs struggle to perform state-tracking which may impair performance in tasks such as code evaluation or tracking a chess game. Even parity, the simplest state-tracking task, which non-linear RNNs like LSTM handle effectively, cannot be solved by current LRNNs. Recently, Sarrof et al. (2024) demonstrated that the failure of LRNNs like Mamba to solve parity stems from restricting the value range of their diagonal state-transition matrices to [0, 1] and that incorporating negative values can resolve this issue. We extend this result to non-diagonal LRNNs, which have recently shown promise in models such as DeltaNet. We prove that finite precision LRNNs with state-transition matrices having only positive eigenvalues cannot solve parity, while complex eigenvalues are needed to count modulo 3. Notably, we also prove that LRNNs can learn any regular language when their state-transition matrices are products of identity minus vector outer product matrices, each with eigenvalues in the range [−1, 1]. Our empirical results confirm that extending the eigenvalue range of models like Mamba and DeltaNet to include negative values not only enables them to solve parity but consistently improves their performance on state-tracking tasks. Furthermore, pre-training LRNNs with an extended eigenvalue range for language modeling achieves comparable performance and stability while showing promise on code and math data. Our work enhances the expressivity of modern LRNNs, broadening their applicability without changing the cost of training or inference.}, note={arXiv:2411.12537 [cs]}, number={arXiv:2411.12537}, publisher={arXiv}, author={Grazzi, Riccardo and Siems, Julien and Franke, Jörg K. H. and Zela, Arber and Hutter, Frank and Pontil, Massimiliano}, year={2024}, month=dec, language={en} }

@article{Kaplan_McCandlish_Henighan_Brown_Chess_Child_Gray_Radford_Wu_Amodei_2020, title={Scaling Laws for Neural Language Models}, url={http://arxiv.org/abs/2001.08361}, DOI={10.48550/arXiv.2001.08361}, abstractNote={We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overﬁtting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a ﬁxed compute budget. Larger models are signiﬁcantly more sampleefﬁcient, such that optimally compute-efﬁcient training involves training very large models on a relatively modest amount of data and stopping signiﬁcantly before convergence.}, note={arXiv:2001.08361 [cs]}, number={arXiv:2001.08361}, publisher={arXiv}, author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario}, year={2020}, month=jan, language={en} }

@article{Merrill_Sabharwal_2023, title={The Parallelism Tradeoff: Limitations of Log-Precision Transformers}, volume={11}, rights={https://creativecommons.org/licenses/by/4.0/}, ISSN={2307-387X}, DOI={10.1162/tacl_a_00562}, abstractNote={Despite their omnipresence in modern NLP, characterizing the computational power of transformer neural nets remains an interesting open question. We prove that transformers whose arithmetic precision is logarithmic in the number of input tokens (and whose feedforward nets are computable using space linear in their input) can be simulated by constant-depth logspace-uniform threshold circuits. This provides insight on the power of transformers using known results in complexity theory. For example, if L = P (i.e., not all poly-time problems can be solved using logarithmic space), then transformers cannot even accurately solve linear equalities or check membership in an arbitrary context-free grammar with empty productions. Our result intuitively emerges from the transformer architecture’s high parallelizability. We thus speculatively introduce the idea of a fundamental parallelism tradeoff: any model architecture as parallelizable as the transformer will obey limitations similar to it. Since parallelism is key to training models at massive scale, this suggests a potential inherent weakness of the scaling paradigm.}, journal={Transactions of the Association for Computational Linguistics}, publisher={MIT Press}, author={Merrill, William and Sabharwal, Ashish}, year={2023}, month=jun, pages={531–545}, language={en} }

@article{Schmidhuber_2012, title={Self-Delimiting Neural Networks}, url={http://arxiv.org/abs/1210.0118}, DOI={10.48550/arXiv.1210.0118}, abstractNote={Self-delimiting (SLIM) programs are a central concept of theoretical computer science, particularly algorithmic information & probability theory, and asymptotically optimal program search (AOPS). To apply AOPS to (possibly recurrent) neural networks (NNs), I introduce SLIM NNs. A typical SLIM NN is a general parallel-sequential computer. Its neurons have threshold activation functions. Its output neurons may affect the environment, which may respond with new inputs. During a computational episode, activations are spreading from input neurons through the SLIM NN until the computation activates a special halt neuron. Weights of the NN’s used connections deﬁne its program. Halting programs form a preﬁx code. An episode may never activate most neurons, and hence never even consider their outgoing connections. So we trace only neurons and connections used at least once. With such a trace, the reset of the initial NN state does not cost more than the latest program execution. This by itself may speed up traditional NN implementations. To efﬁciently change SLIM NN weights based on experience, any learning algorithm (LA) should ignore all unused weights. Since preﬁxes of SLIM programs inﬂuence their sufﬁxes (weight changes occurring early in an episode inﬂuence which weights are considered later), SLIM NN LAs should execute weight changes online during activation spreading. This can be achieved by applying AOPS to growing SLIM NNs. Since SLIM NNs select their own task-dependent effective size (=number of used free parameters), they have a built-in way of addressing overﬁtting, with the potential of effectively becoming small and slim whenever this is beneﬁcial. To efﬁciently teach a SLIM NN to solve many tasks, such as correctly classifying many different patterns, or solving many different robot control tasks, each connection keeps a list of tasks it is used for. The lists may be efﬁciently updated during training. To evaluate the overall effect of currently tested weight changes, a SLIM NN LA needs to re-test performance only on the efﬁciently computable union of tasks potentially affected by the current weight changes. Search spaces of many existing LAs (such as hill climbing and neuro-evolution) can be greatly reduced by obeying restrictions of SLIM NNs. Future SLIM NNs will be implemented on 3-dimensional brain-like multi-processor hardware. Their LAs will minimize task-speciﬁc total wire length of used connections, to encourage efﬁcient solutions of subtasks by subsets of neurons that are physically close. The novel class of SLIM NN LAs is currently being probed in ongoing experiments to be reported in separate papers.}, note={arXiv:1210.0118 [cs]}, number={arXiv:1210.0118}, publisher={arXiv}, author={Schmidhuber, Juergen}, year={2012}, month=sep, language={en} 
}

@article{Weiss_Goldberg_Yahav_2021, title={Thinking Like Transformers}, url={http://arxiv.org/abs/2106.06981}, DOI={10.48550/arXiv.2106.06981}, abstractNote={What is the computational model behind a Transformer? Where recurrent neural networks have direct parallels in ﬁnite state machines, allowing clear discussion and thought around architecture variants or trained models, Transformers have no such familiar parallel. In this paper we aim to change that, proposing a computational model for the transformer-encoder in the form of a programming language. We map the basic components of a transformer-encoder—attention and feed-forward computation—into simple primitives, around which we form a programming language: the Restricted Access Sequence Processing Language (RASP). We show how RASP can be used to program solutions to tasks that could conceivably be learned by a Transformer, and how a Transformer can be trained to mimic a RASP solution. In particular, we provide RASP programs for histograms, sorting, and Dyck-languages. We further use our model to relate their difﬁculty in terms of the number of required layers and attention heads: analyzing a RASP program implies a maximum number of heads and layers necessary to encode a task in a transformer. Finally, we see how insights gained from our abstraction might be used to explain phenomena seen in recent works.}, note={arXiv:2106.06981 [cs]}, number={arXiv:2106.06981}, publisher={arXiv}, author={Weiss, Gail and Goldberg, Yoav and Yahav, Eran}, year={2021}, month=jul, language={en} }

@article{Zhou_Bradley_Littwin_Razin_Saremi_Susskind_Bengio_Nakkiran_2024, 
  title={What algorithms can transformers learn? a study in length generalization},
  author={Zhou, Hattie and Bradley, Arwen and Littwin, Etai and Razin, Noam and Saremi, Omid and Susskind, Josh and Bengio, Samy and Nakkiran, Preetum},
  journal={arXiv preprint arXiv:2310.16028},
  year={2023}
}

@article{anil2022path,
  title={Path independent equilibrium models can better exploit test-time computation},
  author={Anil, Cem and Pokle, Ashwini and Liang, Kaiqu and Treutlein, Johannes and Wu, Yuhuai and Bai, Shaojie and Kolter, J Zico and Grosse, Roger B},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={7796--7809},
  year={2022}
}

@inproceedings{merrill2019sequential,
    title = "Sequential Neural Networks as Automata",
    author = "Merrill, William",
    editor = "Eisner, Jason  and
      Gall{\'e}, Matthias  and
      Heinz, Jeffrey  and
      Quattoni, Ariadna  and
      Rabusseau, Guillaume",
    booktitle = "Proceedings of the Workshop on Deep Learning and Formal Languages: Building Bridges",
    month = aug,
    year = "2019",
    address = "Florence",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-3901/",
    doi = "10.18653/v1/W19-3901",
    pages = "1--13",
}

@inproceedings{merrill2020formal,
  title={A Formal Hierarchy of RNN Architectures},
  author={Merrill, William and Weiss, Gail and Goldberg, Yoav and Schwartz, Roy and Smith, Noah A and Yahav, Eran},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={443--459},
  year={2020}
}

@InProceedings{merrill2024illusion,
  title = 	 {The Illusion of State in State-Space Models},
  author =       {Merrill, William and Petty, Jackson and Sabharwal, Ashish},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {35492--35506},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/merrill24a/merrill24a.pdf},
  url = 	 {https://proceedings.mlr.press/v235/merrill24a.html},
  abstract = 	 {State-space models (SSMs) have emerged as a potential alternative architecture for building large language models (LLMs) compared to the previously ubiquitous transformer architecture. One theoretical weakness of transformers is that they cannot express certain kinds of sequential computation and state tracking (Merrill &amp; Sabharwal, 2023), which SSMs are explicitly designed to address via their close architectural similarity to recurrent neural networks (RNNs). <em>But do SSMs truly have an advantage (over transformers) in expressive power for state tracking?</em> Surprisingly, the answer is no. Our analysis reveals that the expressive power of SSMs is limited very similarly to transformers: SSMs cannot express computation outside the complexity class $\mathsf{TC}^0$. In particular, this means they cannot solve simple state-tracking problems like permutation composition. It follows that SSMs are provably unable to accurately track chess moves with certain notation, evaluate code, or track entities in a long narrative. To supplement our formal analysis, we report experiments showing that Mamba-style SSMs indeed struggle with state tracking. Thus, despite its recurrent formulation, the "state” in an SSM is an illusion: SSMs have similar expressiveness limitations to non-recurrent models like transformers, which may fundamentally limit their ability to solve real-world state-tracking problems.}
}

@inproceedings{siegelmann1992,
author = {Siegelmann, Hava T. and Sontag, Eduardo D.},
title = {On the computational power of neural nets},
year = {1992},
isbn = {089791497X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130385.130432},
doi = {10.1145/130385.130432},
abstract = {This paper deals with finite networks which consist of interconnections of synchronously evolving processors. Each processor updates its state by applying a “sigmoidal” scalar nonlinearity to a linear combination of the previous states of all units. We prove that one may simulate all Turing Machines by rational nets. In particular, one can do this in linear time, and there is a net made up of about 1,000 processors which computes a universal partial-recursive function. Products (high order nets) are not required, contrary to what had been stated in the literature. Furthermore, we assert a similar theorem about non-deterministic Turing Machines. Consequences for undecidability and complexity issues about nets are discussed too.},
booktitle = {Proceedings of the Fifth Annual Workshop on Computational Learning Theory},
pages = {440–449},
numpages = {10},
location = {Pittsburgh, Pennsylvania, USA},
series = {COLT '92}
}

