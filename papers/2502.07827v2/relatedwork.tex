\section{Related Work}
\paragraph{Adaptive-Compute Time} The idea of an adaptive compute budget goes back to \cite{Schmidhuber_2012} who employ a halting neuron to delimit the computation on a particular input. \citet{Graves_2017} generalized the idea and regularised the halting condition to encourage the network to stop early.
They implemented an adaptive-depth RNN and demonstrated the network adjusting the compute budget based on the difficulty of instances in a parity-check task.
This idea was later applied to Transformers, resulting in ''Universal Transformers'' (UT)~\cite{dehghani2018universal}. UTs can either be unrolled to a fixed depth or augmented with a dynamic halting condition (DHC) per token.
UTs were later shown to exhibit improved scaling laws compared to standard transformers~\cite{Kaplan_McCandlish_Henighan_Brown_Chess_Child_Gray_Radford_Wu_Amodei_2020}.
PonderNet~\cite{Banino_Balaguer_Blundell_2021} introduced a principled probabilistic model for determining the halting condition. This approach improved on the UT on the \textsc{bAbi} benchmark.
Recently, a mixture-of-experts (MoE) variant of the UT (MoEUT) was presented~\cite{csordas2024moeut} with 1B parameters, seeking to improve the parameter-to-compute ratio of UTs.
The MoEUT is an unrolled model with fixed iterations and does not employ a DHC. While our models presented here are dense, they could, in principle, be turned into MoE. 
\citet{gatmiry2024can} show that looped linear transformers implement gradient-descent until convergence on the prediction loss defined by previous input-output examples in the context window.
\citet{lim2024parallelizing} take the opposite approach to our work:
Instead of augmenting SSMs or transformers, they propose an approach based on fixed-point iterations to enable parallel training of RNNs.
However, their method incurs cubic cost in terms of state size, limiting the method to smaller models. 

\paragraph{Reasoning and out-of-distribution generalization.}

The ability of looped models to generalize better to input lengths not seen during training is empirically well established: For example~\citet{Yang_Lee_Nowak_Papailiopoulos_2024} show this for looped transformers, while \citet{anil2022path} demonstrate length generalization for DEQs, particularly when they are path independent.
\citet{Du_Li_Tenenbaum_Mordatch} show that energy-based models trained to map energy-gradient-descent steps to algorithmic steps, can length generalize in summation, and complex algorithms such as shortest-path.
On the theoretical side,
The pioneering work of \citet{siegelmann1992} shows that iterated RNNs are Turing complete at infinite numerical precision. 
More recently, \citet{deletang2023neural} studied a number of sequence models and report that grouping tasks by their rung in the Chomsky hierarchy is predictive of models ability to length-generalize. 
While the works of Merrill \emph{et al}~ \cite{merrill2019sequential, merrill2020formal, Merrill_Sabharwal_2023, merrill2024illusion}, which we discuss in\Cref{sec:limitations-transformer-ssm}, showed that both transformers and SSMs are restricted to \tc; several studies sought to find more precise constraints.
\citet{Weiss_Goldberg_Yahav_2021} observe that programs written in a specific language (RASP) can be mapped to transformer models of sufficient capacity.
\citet{Zhou_Bradley_Littwin_Razin_Saremi_Susskind_Bengio_Nakkiran_2024} then showed that transformers tend to length-generalise if the underlying data-generating process can be expressed in RASP.
\citet{sarrof2024expressive} derived a similar refined constraint for SSMs and showed that they can precisely express star-free regular languages.
\citet{Grazzi_Siems_Franke_Zela_Hutter_Pontil_2024} demonstrate that SSMs can track state in simple problems, such as parity, when their (diagonal) recurrence matrix \(\Lambda\) in Equation~\eqref{eq:linear-ssm} permits negative eigenvalues.
Moreover, they illustrate that a variant of DeltaNet~\cite{Yang_Wang_Zhang_Shen_Kim} with (possibly) negative eigenvalues can solve the S5 problem when only swaps of two values are considered in the transition.
However, no variant of Mamba or DeltaNet was capable of learning S5 and achieving length generalization.
To tackle the parallelization-expressiveness trade-off, \citet{Beck_PÃ¶ppel_Spanring_Auer_Prudnikova_Kopp_Klambauer_Brandstetter_Hochreiter} propose two new LSTM-inspired layer architectures: the sLSTM and mLSTM layers.
While the latter is parallelizable, the former is not and intended to enable the whole model to recognize regular languages.
Finally, \citet{soulos2024recurrent} survey strategies for chunking input sequences with transformers, maintaining parallelizability within each chunk and using RNN-like transitions between chunks.
They find these architectures recognize regular languages for small chunk sizes with scaling remaining a challenge.