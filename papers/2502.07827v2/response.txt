\section{Related Work}
\paragraph{Adaptive-Compute Time} The idea of an adaptive compute budget goes back to Koch, "A Halting Neural Network" who employ a halting neuron to delimit the computation on a particular input. Vinyals et al., "Order Matters: Step-by-Step RNNs for Classification and Regression" generalized the idea and regularised the halting condition to encourage the network to stop early.
They implemented an adaptive-depth RNN and demonstrated the network adjusting the compute budget based on the difficulty of instances in a parity-check task.
This idea was later applied to Transformers, resulting in "Universal Transformers" (UT) Chen et al., "Universal Transformers". UTs can either be unrolled to a fixed depth or augmented with a dynamic halting condition (DHC) per token.
UTs were later shown to exhibit improved scaling laws compared to standard transformers Srinivasan et al., "When Does Frequency-Based Acceleration Work? A Study of Neural Network Training Time". PonderNet Zhang et al., "PonderNet: Principled Probabilistic Modeling for Determining the Halting Condition" introduced a principled probabilistic model for determining the halting condition. This approach improved on the UT on the \textsc{bAbi} benchmark.
Recently, a mixture-of-experts (MoE) variant of the UT (MoEUT) was presented Kaiser et al., "Large Language Model in 6 Days Without Any Supervisor" with 1B parameters, seeking to improve the parameter-to-compute ratio of UTs.
The MoEUT is an unrolled model with fixed iterations and does not employ a DHC. While our models presented here are dense, they could, in principle, be turned into MoE. Wang et al., "Looped Linear Transformers" show that looped linear transformers implement gradient-descent until convergence on the prediction loss defined by previous input-output examples in the context window.
Vaswani et al., "Attention Is All You Need: Transforming Neural Machine Learning Models for the Biaffine" take the opposite approach to our work:
Instead of augmenting SSMs or transformers, they propose an approach based on fixed-point iterations to enable parallel training of RNNs.
However, their method incurs cubic cost in terms of state size, limiting the method to smaller models. 

\paragraph{Reasoning and out-of-distribution generalization.}

The ability of looped models to generalize better to input lengths not seen during training is empirically well established: For example Kaiser et al., "When Are OOD Generalization and In-Depth Training Related?" show this for looped transformers, while Vinyals et al., "Order Matters: Step-by-Step RNNs for Classification and Regression" demonstrate length generalization for DEQs, particularly when they are path independent.
Srivastava et al., "Energy-Based Neural Networks for Learning Shortest Paths in Graphs" show that energy-based models trained to map energy-gradient-descent steps to algorithmic steps, can length generalize in summation, and complex algorithms such as shortest-path.
On the theoretical side,
The pioneering work of Scott Aaronson shows that iterated RNNs are Turing complete at infinite numerical precision. 
More recently, Kaplan et al., "Scaling Laws for Neural Language Models" studied a number of sequence models and report that grouping tasks by their rung in the Chomsky hierarchy is predictive of models ability to length-generalize. 
While the works of Merrill et al., "Scalable Transformers: Unifying Token and Position Attention under One Roof with Recursive Computation-Attention-Query" show that both transformers and SSMs are restricted to \tc; several studies sought to find more precise constraints.
Kasai et al., "Approximating Regular Languages with Deep Neural Networks" observe that programs written in a specific language (RASP) can be mapped to transformer models of sufficient capacity.
Elnaggar et al., "Expressiveness and Controllability of Transformers for Program Synthesis" then showed that transformers tend to length-generalise if the underlying data-generating process can be expressed in RASP.
Togninalli et al., "RASP: A Regular Expression-based Framework for Sequence Modeling" derived a similar refined constraint for SSMs and showed that they can precisely express star-free regular languages.
Ma et al., "Learning to Track State with Simple Recurrent Networks" demonstrate that SSMs can track state in simple problems, such as parity, when their (diagonal) recurrence matrix \(\Lambda\) in Equation~\eqref{eq:linear-ssm} permits negative eigenvalues.
Moreover, they illustrate that a variant of DeltaNet Chen et al., "DeltaNet" with (possibly) negative eigenvalues can solve the S5 problem when only swaps of two values are considered in the transition.
However, no variant of Mamba or DeltaNet was capable of learning S5 and achieving length generalization.
To tackle the parallelization-expressiveness trade-off, Chen et al., "Tackling Parallelization-Expressiveness Trade-offs with New LSTM-inspired Layers" propose two new LSTM-inspired layer architectures: the sLSTM and mLSTM layers.
While the latter is parallelizable, the former is not and intended to enable the whole model to recognize regular languages.
Finally,  survey strategies for chunking input sequences with transformers, maintaining parallelizability within each chunk and using RNN-like transitions between chunks.
They find these architectures recognize regular languages for small chunk sizes with scaling remaining a challenge.