\section{Related Work}
\medskip
\noindent \textbf{Homomorphic Compression.}
The concept of homomorphic compression, where arithmetic operations are performed directly on compressed data, has been explored in the context of gradient aggregation~\cite{thc2024li}, enabling a parameter server to aggregate compressed gradients without decompressing them. However, this method is limited to addition operations and is unsuitable for the matrix multiplications required in the attention computation procedure.

\medskip
\noindent \textbf{Disaggregated LLM inference.} Disaggregated LLM inference.~\cite{distserve, splitwise, mooncake, memserve, strati2024dejavukvcachestreamingfast} has gained popularity as it separates the computation-intensive prefill and memory-intensive decode stages into different instances, improving resource utilization.
Two works~\cite{distserve, splitwise, strati2024dejavukvcachestreamingfast} focus on disaggregated LLM inference systems. Other research~\cite{mooncake, memserve} focuses on KV cache sharing, where KV data generated during the prefill stage is stored and shared across requests to reduce computation time. However, transmitting KV values from the prefill instance to the decode instance can become a bottleneck, a challenge that intensifies with KV data sharing. Our work addresses this issue by introducing homomorphic quantization, eliminating significant KV dequantization overhead.

%these methods do not reduce the size of KV data to lower transmission and memory overhead, nor do they leverage low-precision computation to accelerate attention. Our work addresses these gaps by using homomorphic quantization without introducing the significant overhead caused by KV dequantization.

\medskip
\noindent\textbf{KV Quantization.}
Many methods use quantization to compress KV~\cite{zipcache, kang2024gear, kivi, cachegen, kvquant} by reducing the high-bit representation FP16 to lower-bit representations in order to reduce the size of KV data. %It reduces the number of bits required to store KV.
However, during attention computation, the quantized KV in the KV cache must first be dequantized to recover the original data, introducing significant decompression overhead. \sys, on the other hand, uses homomorphic quantization to avoid the KV dequantization overhead and performs attention computation on low-precision data to accelerate speed.

% Recent work TurboAttention~\cite{turboattention} on arXiv, also performs attention computation directly on quantized KV data; however, it focuses solely on accelerating computation and mitigating KV cache bottlenecks. In contrast, our work not only accelerates computation and mitigates KV cache bottlenecks but also focuses on minimizing KV communication overhead in disaggregated LLM inference by leveraging 2-bit quantization.
% Furthermore, TurboAttention adopts symmetric quantization to eliminate the approximation step in Eq.~\eqref{eq:hoq_matmul}, which in turn introduces higher quantization error~\cite{turboattention}.
% In contrast, our method utilizes stochastic quantization based on asymmetric quantization, resulting in lower quantization error. We store the sum value $\sum_z b_{zj}'$ for KV with minimal memory overhead, effectively reducing the approximation overhead.

Recent work TurboAttention~\cite{turboattention} on arXiv, also performs attention computation directly on quantized KV data; however, it focuses solely on accelerating computation and mitigating KV cache bottlenecks. In contrast, our work not only accelerates computation and mitigates KV cache bottlenecks but also focuses on minimizing KV communication overhead in disaggregated LLM inference by leveraging 2-bit quantization.
Furthermore, TurboAttention chooses symmetric quantization to reduce computation overhead, which has a lower accuracy than asymmetric quantization~\cite{turboattention}. \sys uses asymmetric quantization with summation elimination to increase accuracy. In addition, \sys eliminates the requantization overhead and reduce quantization error for the last block of $V$ during the decode stage.


% Furthermore, as indicated in~\cref{sec:implementation}, TurboAttention uses symmetric quantization that decreases accuracy to eliminate approximation\sh{change "approximation" to "dequantization"?}, while \sys uses asymmetric quantization with summation elimination to increase accuracy. In addition, \sys eliminates the requantization overhead and reduce quantization error for the last block of $V$ during the decode stage.

% Recent work TurboAttention~\cite{turboattention} on arXiv, also performs attention computation directly on quantized KV data; however, it focuses solely on accelerating computation and mitigating KV cache bottlenecks. In contrast, our work not only accelerates computation and mitigates KV cache bottlenecks but also focuses on minimizing KV communication overhead in disaggregated LLM inference by leveraging 2-bit quantization.
% Furthermore, as indicated in~\cref{sec:implementation}, TurboAttention uses symmetric quantization that decreases accuracy to eliminate approximation\sh{change "approximation" to "dequantization"?}, while \sys uses asymmetric quantization with summation elimination to increase accuracy. In addition, \sys eliminates the requantization overhead and reduce quantization error for the last block of $V$ during the decode stage.

% Recent work TurboAttention~\cite{turboattention} on arXiv, also performs attention computation directly on quantized KV data to accelerate attention.
% However, it adopts symmetric quantization to eliminate the approximation step in Eq.~\eqref{eq:hoq_matmul}, which in turn introduces higher quantization error~\cite{turboattention}.
% In contrast, our method utilizes stochastic quantization based on asymmetric quantization, resulting in lower quantization error. We store the sum value $\sum_z b_{zj}'$ for KV with minimal memory overhead, effectively reducing the approximation overhead.
% % While TurboAttention has an approximation step to convert quantized results into approximations of the real results, it does not store the summation term $\sum_z b_{zj}'$ for KV that is computed every decode iteration. As a result, it incurs recomputation overhead in the decode stage.
% Moreover, in each decode iteration, TurboAttention quantizes the last block of $V$ using INT8 before performing the quantized matrix multiplication, which introduces requantization overhead and accumulates quantization error as decoding progresses.
% In contrast, \sys stores the original FP16 data for the last block of $V$ with minimal memory overhead, thereby reducing quantization error and eliminating requantization overhead.

\medskip
\noindent\textbf{KV Eviction.}
Another solution for KV compression is KV eviction~\cite{h2o2023zhang, ge2023model, scissorhands2023liu, pyramidinfer, l2norm-kv, keyformer, dynamic-context-pruning, infinigen, zhang2024efficientsparseattentionneeds, jiang2024minference}, also known as KV pruning, which removes unimportant tokens' KV that have minimal impact on inference results, measured by attention scores.
Eviction-based methods and quantization-based methods are complementary: the former reduces the number of elements in the KV matrix, while the latter lowers the precision of those elements. These two approaches can be combined for enhanced effectiveness. While our work focuses on homomorphic quantization, we plan to explore how KV eviction can address the challenges in future work.
%\sh{think about whether KV eviction can be used to reduce data size in transmission. any problem if we use it? Either it is out of the scope or our future work \red{Do not see challenges here. Probably an implementation idea, but need to explore. Other quantization paper also mentions eviction and quantization are complementary in the dicsussion.}}


\DEL{
\noindent\textbf{TurboAttenion.}
Recent work TurboAttention~\cite{turboattention}, uploaded on arXiv, also performs attention computation directly on quantized KV data; however, it focuses solely on accelerating computation and mitigating KV cache bottlenecks. In contrast, our work not only accelerates computation and mitigates KV cache bottlenecks but also focuses on minimizing KV communication overhead in disaggregated LLM inference by leveraging 2-bit quantization.
Furthermore, as indicated in~\cref{sec:implementation}, TurboAttention uses symmetric quantization that decreases accuracy to eliminate approximation, while \sys uses asymmetric quantization with summation elimination to increase accuracy. \sys can also eliminate the requantization overhead and reduce quantization error for the last block of $V$ during the decode stage.
}

% \sh{can you move it to the end of "KV Quantization", and remove }