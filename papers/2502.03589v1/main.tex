\documentclass[sigconf]{acmart}

\usepackage{float}
\usepackage{subfigure}
% to be able to draw some self-contained figs
\usepackage{tikz}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage[ruled,linesnumbered]{algorithm2e}
\usepackage{soul}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{dutchcal}
\usepackage{pifont}
\usepackage{adjustbox}
%\usepackage[dvipsnames]{xcolor}

\usepackage{enumitem}

\usepackage{makecell}

% \usepackage{hyperref}

\newcommand{\DEL}[1]{\iffalse #1 \fi}



% inlined bib file
\usepackage{filecontents}

%\newif\ifcomm
%\commtrue
%%\commfalse
%\ifcomm
%\newcommand{\sh}[1]{\textcolor{blue}{[HS: #1]}}
%\newcommand{\minlan}[1]{\textcolor{blue}{[Minlan: #1]}}
%\newcommand{\red}[1]{\textcolor{red}{#1}}
%\newcommand{\zeyu}[1]{{\color{pink}[Zeyu: #1]}}
%\newcommand{\MM}[1]{{\color{Cyan}[MM: #1]}}
%\newcommand{\ran}[1]{{\color{green}[Ran: #1]}}
%\else
%\newcommand{\sh}[1]{}
%\newcommand{\minlan}[1]{}
%\newcommand{\red}[1]{}
%\newcommand{\zeyu}[1]{}
%\newcommand{\MM}[1]{}
%\newcommand{\ran}[1]{}
%\fi

\newlength{\oldintextsep}
\newlength{\oldcolumnsep}


\usepackage{cleveref}
\crefname{section}{§}{§§}
\Crefname{section}{§}{§§}
\crefformat{section}{§#2#1#3}


\usepackage{ifthenx}
%\newtheorem{obs}{\textbf{\textit{O\hspace{-2.8pt}}}}
%\newcommand{\obsref}[1]{\textbf{\textit{O\hspace{-2.6pt}}}~\ref{#1}}

% \newcommand*\circled[1]{\tikz[baseline=(char.base)]{
% 		\instance[shape=circle,draw,inner sep=0.5pt] (char) {#1};}}
\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
		\node[shape=circle,draw,inner sep=0.5pt] (char) {#1};}}

\newtheorem{obs}{\textbf{Observation}}
\usepackage[most]{tcolorbox}
\definecolor{light-gray}{gray}{0.95}
\tcolorboxenvironment{obs}{
   center,
   boxsep=0.1pt,
   top=1.0pt,
   bottom=1.0pt,
   left=6pt,
   right=6pt,
    width=\columnwidth,
    colframe=light-gray,
    colback=light-gray
}

\newtheorem{lem}{\textbf{Lemma}}
\usepackage[most]{tcolorbox}
\definecolor{light-gray}{gray}{0.95}


\newcommand{\approptoinn}[2]{\mathrel{\vcenter{
  \offinterlineskip\halign{\hfil$##$\cr
    #1\propto\cr\noalign{\kern2pt}#1\sim\cr\noalign{\kern-2pt}}}}}
\newcommand{\appropto}{\mathpalette\approptoinn\relax}


\def\DONE{{\color{red}DONE }}


\setlength{\tabcolsep}{2pt}
\usepackage{url}
\def\UrlBreaks{\do\/\do-\do_}
\newcommand{\squishlist}{
\begin{list}{$\bullet$}
  { \setlength{\itemsep}{0pt}
     \setlength{\parsep}{0pt}
     \setlength{\topsep}{0pt}
     \setlength{\partopsep}{0pt}
     \setlength{\leftmargin}{0em}
     \setlength{\labelwidth}{0em}
     \setlength{\labelsep}{0.2em} } }

\newcommand{\squishlisttwo}{
\begin{list}{$\bullet$}
  { \setlength{\itemsep}{0pt}
     \setlength{\parsep}{0pt}
    \setlength{\topsep}{0pt}
    \setlength{\partopsep}{0pt}
    \setlength{\leftmargin}{2em}
    \setlength{\labelwidth}{1.5em}
    \setlength{\labelsep}{0.5em} } }

\newcommand{\squishend}{
  \end{list}  }

\newlength{\defaultcolumnsep}
\setlength{\defaultcolumnsep}{\columnsep}

% \newcommand{\sys}{HoqKV }
\newcommand{\sys}{{HACK}\xspace}

% Copyright
\renewcommand\footnotetextcopyrightpermission[1]{} % removes footnote with conference info
\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}

\settopmatter{printacmref=false, printccs=false, printfolios=true}

% DOI
\acmDOI{}

% ISBN
\acmISBN{}

%Conference
\acmConference[]{arXiv}
\acmYear{2025}
%\copyrightyear{}

%% {} with no args suppresses printing of the price
\acmPrice{}


\begin{document}

% \tableofcontents


\title{\sys: Homomorphic Acceleration via Compression of the Key-Value Cache for Disaggregated LLM Inference}

%\titlenote{Produces the permission block, and copyright information}
%\subtitle{Extended Abstract}

\author{Zeyu Zhang}
\affiliation{
\institution{University of Virginia}
}
\author{Haiying Shen}
\affiliation{
\institution{University of Virginia}
}
\author{Shay Vargaftik}
\affiliation{
\institution{VMware Research}
}
\author{Ran Ben Basat}
\affiliation{
\institution{University College London}
}
\author{Michael Mitzenmacher}
\affiliation{
\institution{Harvard University}
}
\author{Minlan Yu}
\affiliation{
\institution{Harvard University}
}

% \author{Firstname Lastname}
% \authornote{Note}
% \orcid{1234-5678-9012}
% \affiliation{%
%   \institution{Affiliation}
%   \streetaddress{Address}
%   \city{City}
%   \state{State}
%   \postcode{Zipcode}
% }
% \email{email@domain.com}

% \author{
% {\rm Zeyu Zhang}\\
% University of Virginia
% \and
% {\rm Haiying Shen}\\
% University of Virginia
% }

% The default list of authors is too long for headers}
\renewcommand{\shortauthors}{}

\begin{abstract}
Disaggregated Large Language Model (LLM) inference has gained popularity as it separates the computation-intensive prefill stage from the memory-intensive decode stage, avoiding the prefill-decode interference and improving resource utilization. However, transmitting Key-Value (KV) data between the two stages can be a bottleneck, especially for long prompts.
Additionally, the computation time overhead for prefill and decode is key for optimizing Job Completion Time (JCT),
and KV data size can become prohibitive for long prompts and sequences.
%accounting for up to 42.2\% of the end-to-end time.
% This challenge is exacerbated in long-sequence applications such as document summarization and composition, where computation times increase, and GPU memory pressure intensifies.
Existing KV quantization methods can alleviate the transmission bottleneck and reduce memory requirements, but they introduce significant dequantization overhead, exacerbating the computation time.
% \ran{Is the `memory' bottleneck the same as the `memory access' bottleneck from the previous sentence? I thought the issue is the memory allocated and not accesses? We can just say `alleviate these bottlenecks' if you're referring to the same two as before.}
%reduce GPU memory pressure, and enhance decode performance. However, they introduce significant KV dequantization overhead,
%, accounting for up to 37.9\% of the end-to-end time,
%and fail to reduce the computational complexity of the prefill and decode stages.

We propose \underline{H}omomorphic \underline{A}cceleration via \underline{C}ompression of the \underline{K}V cache (\sys) for disaggregated LLM inference. \sys eliminates the heavy KV dequantization step, and directly performs computations on quantized KV data to approximate and reduce the cost of the expensive matrix-multiplication step.
%leveraging low-precision formats to accelerate computation and bypassing the KV dequantization process. Compared to the disaggregated LLM inference baseline,
Extensive trace-driven experiments show that \sys reduces JCT by up to 70.9\% compared to disaggregated LLM inference baseline and by up to 52.3\% compared to state-of-the-art KV quantization methods. %\sys achieves up to 52.3\% reduction in JCT
%, due to the improvement in prefill and decode time and the elimination of KV dequantization overhead.

\end{abstract}

\maketitle

\section{Introduction}\label{sec:intro}


%Ran and Shay's alternative first paragraph (old text commented out below):
LLM inference is the process of generating answers to incoming requests (prompts), and its optimization is crucial for reducing costs, enhancing scalability, lowering energy consumption, and enabling deployment on commodity GPUs.
LLM inference has two stages: prefill and decode. The prefill stage processes the prompt, incurring significant compute overhead that scales with both prompt length and model size. The decode stage generates one token at a time per prompt and is memory-intensive, since it must store Key-Value (KV) data that expands with the sequence length.

Traditionally, prefill and decode run on the same GPU, causing interference: the prefill stage’s compute load delays the decode stage, whereas the decode’s memory footprint reduces the supported number of concurrent prefill operations.
Although high-end GPUs offer sufficient memory and computational power for both stages, their high cost is a limiting factor.
Disaggregated LLM inference addresses this issue by splitting the stages between different compute-focused GPUs (e.g., A10G, V100, T4, L4) for prefill, and larger-memory GPUs (e.g., A100, H100) for decode~\cite{distserve, splitwise, mooncake, memserve, strati2024dejavukvcachestreamingfast, a10g-inference, infer-without-infer}. This separation eliminates performance interference and improves overall GPU utilization, making LLM inference more efficient and cost-effective.

%Old text:
%In recent years, the deployment of Large Language Models (LLMs) has evolved significantly, with disaggregated LLM inference emerging as a promising approach~\cite{distserve, splitwise, mooncake, memserve, strati2024dejavukvcachestreamingfast}. LLM inference consists of a prefill stage and a decode stage. The prefill stage, which processes input prompts, is computation-intensive, with its cost scaling significantly with both prompt length and model size. The decode stage generates one token for each request at a time, and is memory-intensive due to the need to store Key-Value (KV) data, which increases proportionally with the sequence length. Although high-end commercial GPUs offer substantial memory capacity and computational power, they are expensive. Unlike traditional methods that colocate the prefill and decode stages on the same instance, disaggregated LLM inference separates these stages across compute-intensive, small-memory GPUs (e.g., A10G, V100, T4, and L4) for prefill, and large-memory GPUs (e.g., A100 and H100) for decode~\cite{a10g-inference, infer-without-infer}. This separation eliminates interference between the two stages, and optimizes heterogeneous GPU resource utilization. Interference here means that prefill computation delays the decode stage, while the decode stage consumes substantial memory, limiting the number of requests prefill can process concurrently.

%\sh{did you check SplitWise paper or search online about large-memory but low-compute GPUs?}
%\zeyu{They do not mention high-memory CPU may have low-compute. They say given the lack
%of compute parallelism, the decode stage tends to be more memory bandwidth and capacity bound, despite state-of-the-art batching.}
%\zeyu{I checked online resources. The trend is the more memory, the more compute. Some articles say certain GPUs have low-compute than A100/H100 while having high memory, but they have 48G and A100/H100 have 80G memory.}

% \sh{I revised the above sentence, so the next sentence can be delected.}

% This separation aligns resource usage more effectively with the distinct requirements of each stage, thus allowing for improved scalability and performance, more flexible resource allocation, and reduced resource costs.
% \MM{This last sentence is not well explained;  how does this separation give these things?  I think you want some high-level explanation:  by separating the stages, one can choose hardware tailored for each stage, lowering the overall cost and improving perfomrmance/scalability.  This would lead into the next paragraph/sentence.}



% \begin{table*}[h]\vspace{-0in}
% \centering
% \small
% \begin{adjustbox}{max width=\linewidth}
% \begin{tabular}{|c||c|c|c|c|c|}
% \hline
%  & Reduce KV comm & Reduce compute & Reduce memory access latency for KV & Avoid KV dequant & High KV compression rate\\
% \hline
% Baseline & $\times$ & $\times$ & $\times$ & $\checkmark$ & $\times$ \\
% \hline
% KV quant & $\checkmark$ & $\times$ & $\checkmark$ & $\times$ & $\checkmark$ \\
% \hline
% FP4/6/8 & $\checkmark$ & Require hardware & $\checkmark$ & Require hardware & $\times$ \\
% \hline
% \sys & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ \\
% \hline
% \end{tabular}
% \end{adjustbox}
% \vspace{-0in}
% \caption{Features of different methods and our system \sys.\sh{\DONE \red{Words are too small if put tab in one column even the first row is put in 3 rows.} combine CacheGen and KVQunt to KV Quan. you need to add "Low-precision" in a row., change last column to "Avoid KV dequan."--always make x bad, add another column "Reduce memory demand". Use two row space for the first row to fit the table to one column.}}
% \vspace{0in}
% \label{tab:features}
% \end{table*}

% \ran{I suggest to avoid cutting words, here's an alternative table:}
\begin{table*}[h]\vspace{-0in}
\centering
%\small
\footnotesize
\begin{adjustbox}{max width=\linewidth}
\begin{tabular}{|c||c|c|c|c|c|}
\hline
\multirow{2}{*}{}
& \multirow{2}{*}{\shortstack{Reducing KV \\ communication}}
& \multirow{2}{*}{\shortstack{Reducing  compute}}
& \multirow{2}{*}{\shortstack{Reducing memory access \\ latency for KV}}
& \multirow{2}{*}{\shortstack{Avoiding KV \\ dequantization}}
& \multirow{2}{*}{\shortstack{Achieving high KV \\ compression rate}} \\
& & & & & \\
\hline\hline
Baseline & $\times$ & $\times$ & $\times$ & $\checkmark$ & $\times$ \\
\hline
KV quantization & $\checkmark$ & $\times$ & $\checkmark$ & $\times$ & $\checkmark$ \\
\hline
FP4/6/8 & $\checkmark$ & Require hardware & $\checkmark$ & Require hardware & $\times$ \\
\hline
\sys & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ \\
\hline
\end{tabular}
\end{adjustbox}
\vspace{-0in}
\caption{Features of different methods and our system \sys.}
\vspace{-0in}
\label{tab:features}
\end{table*}







% To reduce costs, LLM developers often adopt a hybrid strategy: GPUs with large memory capacities, such as the A100 and H100, are typically reserved for decode tasks, while more economical GPUs with smaller memory capacities, such as the A10G and V100, are utilized for prefill tasks~\cite{a10g-inference, infer-without-infer}.

The prefill GPU must send its KV data to the decode GPU over the network. However, inexpensive GPU instances from cloud providers often lack high-speed networking. For example, AWS's A10G, V100, T4, and L4 instances cost roughly 10–20 times less than A100 instances—which typically offer 400 Gbps bandwidth—but their network speeds are limited to 10–50 Gbps or lower~\cite{aws-gpu-instances}. Similarly, Tencent Cloud’s A100 instances are configured with only 5–50 Gbps bandwidth to cut costs~\cite{tencentcloud-a100}. As a result, KV transmission between the prefill and decode instances can become a bottleneck, impairing end-to-end performance. Our measurements (\cref{sec:motivation}) show that this overhead can account for up to 42.2\% of Job Completion Time (JCT). With the growing popularity of KV data sharing across requests~\cite{cachegen, mooncake} to reduce computation time in disaggregated LLM inference, the communication bottleneck is expected to worsen.

%In the future, as KV data sharing across requests~\cite{cachegen, mooncake}\sh{these 2 references indicate the disaggregated LLM inference scenario or not?} to reduce computation time in disaggregated LLM inference becomes popular, the communication bottleck will come even more severe.



Pipelining communication to overlap with prefill computation~\cite{splitwise} can reduce communication overhead. However, if communication time greatly exceeds prefill time, this approach loses effectiveness. Additionally, when the prefill instance lacks sufficient GPU memory across decode instances to store decode-specific data, it must temporarily transfer KV data to CPU memory~\cite{strati2024dejavukvcachestreamingfast}, rendering pipelining infeasible.



Computation can also become a bottleneck; in our measurements (\cref{sec:motivation}), prefill and decode times account for up to 45.6\% and 83.3\% of JCT, respectively. Moreover, during the decode stage, GPU memory is constrained by the large volume of cached KV data~\cite{distserve, splitwise}. Memory usage can reach up to 93.7\%. Memory access latency for KV data can consume up to 33.1\% of JCT.


These issues become more pronounced in long-prompt and long-sequence applications, such as book summarization, article writing, and coding, which have surged in popularity. In fact, commercial models like Gemini 1.5~\cite{gemini-1.5} now support a context window of up to 1M. %Compared to shorter sequences, we measured long ones incur 15.5--43.1$\times$ more KV communication time, 9.8--19.2$\times$ more computation time, and 18.9--34.7$\times$ higher memory access latency for KV data during decoding (\cref{sec:motivation}).
% The memory access latency for KV data during decode is approximately 9.8\%-30.5\% of JCT in the experiment.
% 78.1\%-90.5\%

% increase the KV communication time by \sh{??\%}, the computation time by \sh{??\%}, and the memory access latency \sh{??\%} (\cref{sec:motivation}).




\DEL{As the sequence length grows, which is 9.5\%-15.1\% of JCT for short-sequence datasets and 18.6\%-21.9\% of JCT for long-sequence datasets in our measurement (\cref{sec:motivation}).
The compute in the prefill and decode stages scales significantly with the sequence length~\cite{deepspeed-ulysses, ding2023longnet, loongserve}, leading to extended computation time.
Long sequences enlarge the size of KV data, further increasing memory access latency during decode, which is approximately 17.9\%-24.6\% of the decode time for short-sequence datasets and approximately 38.2\%-49.6\% of the decode time for long-sequence datasets in our measurement (\cref{sec:motivation}).
Therefore, as such long-sequence applications gain popularity, it becomes essential to reduce or eliminate the KV communication time, the computation time, the memory usage, and the KV dequantization overhead in disaggregated LLM inference.}





%As a result, reducing the network bottleneck, computation time and KV cache usage are critical requirements for disaggregated LLM inference, especially for long-prompt and long-sequence applications.

%\sh{\DONE revise the above: these problems generally exist but are more serious in long-sequence applications, You current description just indicate the long-sequence--need to correct, you may want to say "problem description, especially in long-sequence applications." Otherwise, your paper and exp. analsysis should be only on long-sequences.}

%\sh{\red{see below} in analysis and exp section, do we have long sequences? in sensitivity testing, did you vary sequence length?}
%\zeyu{the dataset we use are short and long-sequence datasets. We have them in analysis and exp. We already show the JCT and decomposition of JCT for different datasets. So I think we do not need to add such an exp in the sensitivity testing.}

%\sh{\red{Done, I distinguish them when I describe the results for different datasets in analysis and evaluation.} can you distinguish the results and say it can improve by ??\% for short sequences and by ??\% for long sequences.}





%\sh{\DONE do not forget to add motivation for reducing computation time. When we give applications, we need to give examples for long-prompt requests, which generate the communication bottleneck, and long-sequence requests, which generate long latency in decoding. Maybe you can say the decode stage is in less computation powerful GPUs, so we need to reduce the computation time in these GPUs.}

%as indicated in Observation~\ref{obs:kv_bottleneck}.
% \MM{I'm not sure where/what Observation 2 is, but that's an awkward phrasing and this seems out of place.}


% In industry, the most powerful GPUs are generally expensive and offer higher computational power and larger memory capacity. To minimize the cost of LLM inference, LLM developers currently adopt a hybrid strategy: deploying the decode stage on GPUs with high memory capacity, such as A100, while assigning the prefill stage to GPUs with smaller memory that are more affordable~\cite{a10g-inference, infer-without-infer}, such as on AWS V100 and A10G GPU instances. This approach leverages the strengths of each type of GPU to balance performance and cost-effectiveness, ensuring that the decode stage, which has intensive memory requirements, operates efficiently on premium hardware, while the prefill stage benefits from the economical use of less expensive resources.


% However, using cheap GPU instances for the prefill stage introduces additional challenges to disaggregated LLM inference. Affordable GPU instances available on cloud platforms, such as AWS A10G, T4, V100, and L4 GPU instances, typically lack high-speed networking capabilities and are instead limited to bandwidths of 50, 40, 25, 10, or single-digit Gbps. Consequently, the transmission of KV between the prefill and decode instances can become a bottleneck compared to the end-to-end time. While overlapping prefill computation and KV transmission can reduce the communication overhead to some extent, this approach has two key limitations. First, the decode instance must be determined in advance during prefill, and it must have sufficient GPU memory available to store the pre-received KV data. Second, overlapping is not always sufficient to fully hide KV transmission time under the prefill time on certain GPU instances. For instance, as described in \cref{sec:comm_bottleneck}, on V100 GPU instances, even with overlapping, KV transmission time accounts for approximately 25\% of the end-to-end time for long-sequence workloads.
% If no decode instance with adequate memory is available, KV data is usually transferred to a cloud storage server until a decode instance becomes available~\cite{mooncake, cachegen, memserve}. The decode instance then retrieves the KV data from the storage server before proceeding with the decode stage. Since cloud storage servers are typically equipped with network bandwidths in the range of several to tens of Gbps~\cite{cachegen}, this retrieval process further exacerbates the communication bottleneck. As described in \cref{sec:comm_bottleneck}, KV transmission time can occupy around 10\% to 39\% of the end-to-end time in such scenarios. Therefore, optimizing KV transmission is a critical issue for achieving efficient disaggregated LLM inference.









KV quantization methods, such as CacheGen~\cite{cachegen} and KVQuant~\cite{kvquant}, can alleviate the transmission bottleneck and reduce memory access latency during decoding. These approaches quantize the KV data after each iteration before storing it in the cache, then retrieve and dequantize all tokens' KV data in the next decode iteration. However, they introduce significant KV dequantization overhead.
When we employ CacheGen and KVQuant in the disaggregated LLM inference, the dequantization overhead can account for up to 37.9\% of JCT (\cref{sec:motivation}).


%because of the large amount of KV data to be dequantized, which is 17.2\%-20.9\% of JCT for short-sequence datasets and 24.5\%-30.4\% of JCT for long-sequence datasets in our measurement (\cref{sec:motivation}).

%as indicated in Observation~\ref{obs:dequant_overhead}.




% making it particularly high for long-sequence applications such as book summarization, article writing, and coding. Existing commercial models such as Gemini 1.5~\cite{gemini-1.5} can even support a context window length of 1M. As such long-sequence applications gain popularity, it becomes essential to reduce or eliminate this dequantization overhead\sh{all}.
% \red{Additionally, quantization methods reduce the peak GPU memory usage during decode by !! and decrease the percentage of decode time in JCT by !! due to the improved memory access for KV data in our measurement (\cref{sec:motivation}).\sh{this is for the advantage of quantization methods? If yes, remove it. this is not contribution or motivation.}}



%\sh{\DONE you need to move the above long-sequence application discussion here, and discuss all of these time overheads become serious in long-sequence applications together}



% As such long-sequence applications gain popularity, it becomes essential to reduce or eliminate this dequantization overhead\sh{all}.
% Additionally, for prefill and decode, the computational complexity scales significantly with the sequence length~\cite{deepspeed-ulysses, ding2023longnet, loongserve}, leading to extended computation time for long-sequence applications such as book summarization and article writing. Long sequences enlarge the size of KV data, further increasing memory access latency, which is up to approximately 29\% of the decode time in our measurement (\cref{sec:motivation}).




%\MM{I don't get this -- of course longer sequence length, if you mean input, takes longer -- again, like previous comment, it's not clear what you mean here.}
%especially as long-sequence workloads become increasingly popular, such as information retrieval~\cite{cocktailforir} and summarization tasks~\cite{arxiv-summarization}. Existing commercial models such as Gemini 1.5~\cite{gemini-1.5} can even support a context window length of 1M.



Lower precision Floating Point (FP) formats (e.g., FP4~\cite{fp4}, FP6~\cite{fp6}, and FP8~\cite{vllm-fp8}) can reduce KV size and accelerate computation when supported by hardware. They provide up to 73\% KV compression~\cite{open-compute}, compared to the 86\%~\cite{cachegen, kvquant} achieved by CacheGen and KVQuant through mixed-precision and 2-bit quantization. However, for FP4 and FP6, current GPUs require conversion to FP8 or FP16 for computation. On GPUs without FP8 support—such as pre-H100 architectures like the A100—FP8 must be converted to FP16, \mbox{introducing additional computational overhead.}
% \red{We focus on 2-bit quantization to minimize the KV communication overhead in disaggregated LLM inference.}\sh{you should remove this sentence.}
% \ran{\red{I changed it to 73\% now. Thank you.} I'm unclear about why FP8/6/4 is up to 75\% but CacheGen and KVQuant are 86\%. FP4 is actually 4.25 bits because every 32 floats have a shared E8M0 scale. (see here: https://www.opencompute.org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf) I assume you write CacheGen and KVQuant as 86\% and not 87.5\% because they use more than 2 bits due to overheads, but then we should also take the scale into consideration for FP4.}

%LLM developers have also explored using FP8~\cite{vllm-fp8}, FP6~\cite{fp6}, and FP4~\cite{fp4} to reduce KV size and accelerate computation. However, FP8 requires GPU to support FP8 computation to accelerate computation. For example, NVIDIA H100 supports FP8, but GPUs from architectures prior to H100, such as A100, do not. Without FP8 support, FP8 should be transformed to FP16 for computation, introducing overhead.
%FP6 and FP4 also require conversion to FP8 or FP16 for computation, introducing computational overhead.
%Besides it, FP8, FP6, and FP4 cannot achieve the approximately 85\% KV compression rate of CacheGen and KVQuant, resulting in relatively high KV transmission overhead.





% adopting FP8 for KV cache in disaggregated LLM inference involves a trade-off between hardware requirements, accuracy, and end-to-end performance.
% \MM{What follows is too detailed for the intro.  We want a 1-sentence summary -- FP8 is a bad idea because XXX, as we discuss in Section XXX. Don't need numbers here.}
% First, NVIDIA GPUs require at least an H100 to support FP8 matrix operations for accelerating computation. Given the high cost of H100 GPUs, they are typically used for decode instances, leaving the prefill stage without FP8 acceleration.
% Second, although the accuracy performance test in \cref{sec:acc_perf} shows that FP8 achieves approximately 1\% higher accuracy than CacheGen, KVQuant, and our proposed system in this paper, this improvement comes at the cost of reduced KV compression rate. CacheGen, KVQuant, and our proposed system achieve KV compression rates of approximately 85\%, whereas FP8 achieves only a 50\% compression rate. This can result in a higher KV transmission time and an increased memory access latency for KV data, compromising the end-to-end performance.
% The real-testbed-based simulation analysis in \cref{sec:fp8_for_kv} and \cref{sec:e2e_perf_test} shows that FP8 incurs KV transmission time of up to 37\% of the end-to-end time and has up to 48\% higher end-to-end time than our proposed system.
% FP8 can effectively improve end-to-end performance while maintaining high accuracy only when the hardware supports it, and sufficient bandwidth is available.







% \MM{This is our argument -- we need to make clear this is our argument, our contribution.  "We are that a key improvement to disaggregated LLM inference is to....
% Need to start making clear our contribution, can't just have the bullets at the end.
% }








\DEL{To harness the advantages of KV quantization while minimizing the overhead introduced by dequantization, it is critical to avoid dequantizing KV data in each decode iteration. Previous research on quantization~\cite{thc2024li} has proposed homomorphic quantization for gradient aggregation in machine learning training, enabling direct aggregation of quantized gradient data without the need for dequantization and re-quantization. Following this idea, we can also perform attention
% \MM{how about "we show in this paper that we can also...".  "Specifically, we derive am method applies minimal...."
% Need to empasize that applies minimal additional computation BUT THEN AVOIDS DEQUANTIZING;  probably bettet ro remove the additional computation phrasing and say it reduced computation! (because it avoids dequantizing...)
% }
computation directly on the low-precision quantized KV data, applying minimal additional computation to produce an approximation of the true output, thus eliminating KV dequantization overhead and improving the speed of attention computation.
}
% \MM{Emphasize our contribution more:
% While we make use of the previous idea of homomorphic quantization, the method used in \cite{} is restricted....
% Therefore, to overcome this limitation,....}


%Our system overcome this limitation.
%enables direct aggregation of quantized gradient data without the need for dequantization and re-quantization. Following this idea, we can also perform attention
%However, the homomorphic quantization method in~\cite{thc2024li} is restricted to matrix addition and not applicable to the matrix multiplications in attention.

Ideally, arithmetic operations should be executable directly on quantized KV data, eliminating dequantization overhead and accelerating computation through smaller data elements. To this end, we propose \underline{H}omomorphic \underline{A}cceleration via \underline{C}ompression of the \underline{K}V cache (\sys) for disaggregated LLM inference. \sys addresses the KV transmission bottleneck by enabling computation on quantized data while maintaining comparable inference accuracy and reducing memory constraints. Table~\ref{tab:features} summarizes the advantages of \sys over earlier approaches.


%\red{ lists the features of different methods and .}


\DEL{Previous homomorphic quantization techniques~\cite{thc2024li} for gradient aggregation in machine learning training are limited to matrix addition and cannot be applied to the matrix multiplications required in attention computation, while \sys is for this purpose. %\sys not only reduces communication overhead but also enhances computational efficiency and memory utilization.
}


%we propose a \underline{Ho}momorphic \underline{Quant}ization method tailored for KV-related matrix multiplications in attention (\sys)\sh{you need to change it to shorter term based on the Ran's suggestion below Abstract, and also change it in the Abstract} for disaggregated LLM inference.





In summary, our work has the following contributions:\vspace*{-1mm}
\begin{itemize}[leftmargin=1em]
    \item We propose a homomorphic  quantization method for matrix multiplication. It conducts KV-related matrix multiplication on quantized matrices without the need for dequantizing them and then applies an approximation to transform the quantized output into an approximation of the real output.
This approach reduces KV transmission latency, computation time, memory access latency, and memory demand, all while avoiding the costly overhead of KV dequantization.
    \item We further reduce the overhead of homomorphic quantization by storing data using a small amount of memory to eliminate redundant computations and the need to update the quantized data.


    \DEL{reducing the number of recomputation operations in approximation with a minimal cost of memory. Moreover, in the decode stage, we do not immediately quantize the newly generated token. Instead, we perform computation using its original values and wait until a sufficient number of new tokens have accumulated before quantizing them all at once. This approach eliminates the requantization overhead for recently generated tokens and reduces quantization error with a minimal memory overhead.}
    % \item We build our system on top of \sh{I am not clear about the relationship of vLLm and FlashAttention-2, does vLLm already ahve it, or you integrate FlashAttention-2 to vLLM}vLLM~\cite{vllm2023kwon}. We integrate \sys with the widely adopted memory-efficient attention backend, FlashAttention-2~\cite{flashattn2}, to leverage its advantages.
    \item We integrate \sys into FlashAttention-2~\cite{flashattn2} and build our system on vLLM~\cite{vllm2023kwon}. We conduct extensive experiments across various models, datasets, and GPU configurations. These experiments demonstrate that \sys can reduce JCT by up to 70.9\% and 52.3\% compared to the disaggregated LLM inference baseline and state-of-the-art KV quantization methods, respectively. We open-sourced the code of \sys~\cite{hkvq-code}.
\end{itemize}


% By leveraging homomorphic quantization, the inefficiencies associated with repetitive dequantization can be significantly reduced, paving the way for more efficient disaggregated LLM inference.





\section{Motivation}\label{sec:motivation}
% \minlan{some preamble here to summarize key ideas?}
% Disaggregated LLM inference experiences high communication, computation, and memory bottlenecks, especially for long prompts and sequences. State-of-the-art KV quantization works~\cite{cachegen, kvquant} can mitigate the communication and memory bottlenecks but introduce
We investiage the networking, computation, and memory bottlenecks in disaggregated LLM inference and demonstrate the limitations of current KV quantization methods in addressing these issues. The default experiment settings are detailed in~\cref{sec:exp_setup}.




%\section{Motivation}\label{sec:motivation}

\subsection{Bottlenecks in Disaggregated LLM~Inference}\label{sec:kv_bottleneck}
% \sh{let me know when you are done with this section, and I will modify}

% Recent work in LLM inference has advanced prefill-decode disaggregation~\cite{distserve, splitwise, mooncake, memserve, strati2024dejavukvcachestreamingfast}. Traditionally, LLM inference systems employ a monolithic architecture where a single instance handles both the prefill and decode stages. This presents significant limitations in terms of scalability and resource utilization \red{due to the execution interference between the computation-intensive prefill stage and the memory-intensive decode stage~\cite{distserve}.\sh{this is not the reason. This instead supports placing two stages together. you need to refer to that paper to understand and explain interference.}}
% \MM{\red{added explanation} WHY???? What is the problem that this solves.
% }
% \MM{I think in the intro or here you need to briefly say what "execution interference" is -- if one wants compute, and the other wants memory, how do they interfere?}
% Prefill-decode disaggregation addresses these challenges by separating prefill and decode onto different instances, allowing them to utilize different GPU resources independently. Disaggregation eliminates the interference between prefill and decode, enhances system scalability and efficiency, and reduces GPU resource costs.

% Specifically, while there are more powerful commercial GPUs that are equipped with both larger memory capacities and greater computational power, but they are naturally more expensive than alternatives. As a result, LLM developers typically reduce costs by using GPUs with large memory capacities, such as the A100 and H100, specifically for decode, while employing more affordable GPUs with smaller memory capacities, such as the A10G and V100, for prefill tasks~\cite{a10g-inference, infer-without-infer}. AWS offers many affordable GPU instances for inference that can be used for prefill, such as A10G, V100, T4, and L4 GPU instances~\cite{aws-gpu-instances}. However, these cheaper GPU instances typically do not have high-speed bandwidth, with speeds generally limited to 50, 40, 25, or 10 Gbps, or even lower~\cite{aws-gpu-instances}.
% \red{To reduce costs, this trend will continue. For example, Tencent Cloud limits the bandwidth of its A100 instances to 5-50 Gbps to offer lower prices~\cite{tencentcloud-a100}.}
% As a result, transmitting KV data between the prefill instance and the decode instance can become a bottleneck in disaggregated LLM inference.

% \sh{you can remove the above. It is duplicated with Intro}
The popularity of disaggregation has led to the adoption of cheaper GPU instances for prefill to reduce costs~\cite{distserve, splitwise, mooncake, memserve, strati2024dejavukvcachestreamingfast, a10g-inference, infer-without-infer}. However, this also exacerbates certain overheads.
%We conducted several measurements to show the bottlenecks in disaggregated LLM inference regarding communication, computation, and memory.

% \minlan{argue for the trend of disaggregation. the trend of using weaker machines for prefill? and then say the main drawback is network, memory, and compute overhead}




\begin{figure*}[h]
    \centering
    \subfigure[Varing GPU for prefill.\label{fig:comm_bottleneck_gpus}]
    {\includegraphics[width=0.245\linewidth,height=2.5cm]{fig/analysis/comm_bottleneck_gpus.pdf}}
    \subfigure[Varying model.\label{fig:comm_bottleneck_models}]
    {\includegraphics[width=0.245\linewidth,height=2.5cm]{fig/analysis/comm_bottleneck_models.pdf}}
    \subfigure[Varying dataset.\label{fig:comm_bottleneck_datasets}]
    {\includegraphics[width=0.245\linewidth,height=2.5cm]{fig/analysis/comm_bottleneck_datasets.pdf}}
    \subfigure[With pipelining.\label{fig:comm_bottleneck_overlapping}]
    {\includegraphics[width=0.245\linewidth,height=2.5cm]{fig/analysis/comm_bottleneck_overlapping.pdf}}

    \vspace{-0.2in}
    \caption{Bottlenecks in disaggregated LLM inference.}
    \label{fig:comm_bottleneck}
    \vspace{-0in}
\end{figure*}


% \noindent\textbf{Measurement results.}

% \minlan{change to:}
\noindent\textbf{Network and computation overhead.}
To demonstrate the network and computation overhead when using varying prefill instances, we tested Llama-3.1 using the Cocktail on A100, T4, A10G, L4, and V100. Fig.~\ref{fig:comm_bottleneck_gpus} shows the average prefill time ratio, average communication time ratio, and average decode time ratio. The average time ratio is calculated by $\frac{1}{N}\sum_{i=1}^N(\frac{\mbox{time}_i}{\mbox{JCT}_i})$, where $\mbox{time}_i$ and $\mbox{JCT}_i$ denote the time and JCT of request $i$, respectively.
A100, equipped with a 400 Gbps network, achieves a 3.7\% average communication time ratio, while other instances with 10–50 Gbps networks range from 19.1–23.5\%.
The average prefill time ratio and the average decode time ratio are 19.7\%-41.4\% and \mbox{43.1\%-82.5\%, respectively.}
% The memory access latency for KV data during decode is approximately 15.3\%-32.6\% of JCT in the experiment.
% 57.8\%-96.3\% of JCT.

Next, we tested the performance of different models using the Cocktail dataset. Since Falcon-180B has a limitation of 2K context window, it cannot process Cocktail. Therefore, we used another long-sequence dataset, arXiv~\cite{arxiv-summarization}, for Falcon 180B, and denote it as F-arXiv.
Fig.~\ref{fig:comm_bottleneck_models} shows the average time ratios. The average communication time ratio is 11.8\% for F-arXiv and 18.7\%-25.3\% for other models.
\DEL{21\%, 25.3\%, 18.7\%, 21.9\%, and 11.8\% for models M, P, Y, L, and F-arXiv, respectively.
For the four models using Cocktail, KV communication overhead is similarly high due to the longer sequence lengths in Cocktail. F-arXiv has a smaller KV communication overhead because of the shorter sequence lengths.}
The average prefill time ratio and the average decode time ratio are 17.6\%-45.6\% and 39.8\%-81.7\%, respectively.
% \minlan{no need to show all the numbers. But focus on your key ideas:

% - large overhead for specific device and model (so people understand how to read the figure)

% - impact of different devices

% - overhead is generic to different models. The writing should focus on overhead is high... not focus on you did all types of experiments

% }

% The memory access latency for KV data during decode is approximately 13.4\%-33.7\% of JCT in the experiment.
% 84.7\%-88.2\%


%because its context window length restricts the sequence length to within 2K.

% \minlan{change to:
% \noindent\textbf{Computation overhead.??}}

Fig.~\ref{fig:comm_bottleneck_datasets} presents the average time ratios for Llama-3.1 70B across various datasets on A10G prefill instances. The average communication time ratio, driven by input sequence length, varies from 9.5\% to 21.9\%. The average prefill and decode time ratios are 13.6\%-37.1\% and 54.8\%-83.3\%, respectively. The arXiv and Cocktail datasets, with long prompts and sequences, incur 15.5--43.1$\times$ higher KV communication time and 9.8--19.2$\times$ higher computation time compared to the shorter IMDb and HumanEval datasets.


% \minlan{change to:
% \noindent\textbf{Memory overhead.}}
% \minlan{Just say memory is high, and with the trend of models, we may no longer fit.... the gaol is not to talk about memory access latency... but argue memory will be too large to GPU so we need quantization. Compare memory absolute numbers with available GPU memory}

\noindent\textbf{Memory overhead.}
To illustrate the memory bottleneck, we measured peak GPU memory usage on decode instances for Llama-3.1 70B across different datasets. This metric represents the ratio of memory required for parameters, KV data, and activations to the total memory capacity, ranging from 65.3\% to 93.7\% (Table~\ref{tab:peak_gpu_mem}). We also measured GPU memory access time for loading KV data during decode, with an average ratio of 16.3\%–33.1\%.
%Long-sequence datasets have 18.9-34.7$\times$ longer memory access time compared to short-sequence datasets.

% \begin{table}[h]
% \centering
% \begin{adjustbox}{max width=\columnwidth}
% \begin{tabular}{ |c|c|c|c|  }
%  \hline
%  IMDb & arXiv & Cocktail & HumanEval \\
%  \hline
%  61.3\% & 77.1\% & 85.3\% & 64.9\% \\
%  \hline
% \end{tabular}
% \end{adjustbox}
%  \vspace{-0in}
% \caption{Peak GPU memory on decode instances with varying datasets.}\label{tab:peak_gpu_mem_datasets}
% \vspace{-0.25in}
% \end{table}
% Table~\ref{tab:peak_gpu_mem_datasets} lists the peak (i.e., maximum) GPU memory usage during the decode stage for different datasets when using Llama-3.1 70B and A10G prefill instances. %The peak GPU memory usage is the maximum GPU memory usage observed during the decode stage.
% Long-sequence datasets arXiv and Cocktail increase the GPU memory pressure. \sh{\red{Minlan asks to add peak memory usage.} which figure? if need space, remove this paragraph. you want to show the memory access time, not usage, 67.3\%-82.1\% usage cannot support that this is a problem}



\begin{obs}~\label{obs:kv_bottleneck}
In disaggregated LLM inference, KV transmission can contribute up to 42.2\% of JCT, with prefill and decode times reaching 45.6\% and 83.3\%, GPU memory usage up to 93.7\%, and KV memory access up to 33.1\%, creating significant bottlenecks.
\DEL{In the disaggregated LLM inference, KV transmission can contribute up to 42.2\% of JCT.
The prefill time and decode time can account for up to 45.6\% and up to 83.3\% of JCT, respectively, while GPU memory usage can reach up to 93.7\%, and memory access time for KV can be up to 33.1\% of JCT. Together, these factors can become significant bottlenecks.}
% \red{The prefill time and decode time account for 13.6\%-45.6\% and 39.8\%-83.3\% of JCT, respectively.}
\end{obs}
% 57.8\%-96.3\%

\DEL{\noindent\textbf{Pipelining for reducing communication overhead.}
One approach to reducing KV transmission overhead is pipelining, overlapping communication with prefill computation~\cite{splitwise}. However, pipelining has two limitations. First, it cannot fully hide communication within the prefill computation. %such as when communication time exceeds prefill time. For example, as shown in
In Fig.~\ref{fig:comm_bottleneck_gpus}, the communication time on V100 prefill instances is approximately twice the prefill time. Second, if the prefill instance does not pre-know the decode instance due to the insufficient GPU memory on all decode instances, it has to transfer KV data to CPU memory on the prefill instance until a decode instance has enough memory~\cite{strati2024dejavukvcachestreamingfast}, and thus pipelining cannot be realized\sh{\DONE add reference, this point also needs to be explained in Intro as we discussed.}.
%pipelining requires identifying the decode instance to use to transmit KV data in advance.
\sh{\DONE is the following the third point or the second point above? if the former, change two to three above, and add Third here. If the latter, make the connection above and below tight.}
% Decode is memory-intensive, and if no decode instance has sufficient memory\sh{in this case, usually how to deal with it?} to pre-store \sh{decode instance always need to store KV data. What the difference between pre-store and non-pre-store?} KV data, pipelining cannot be applied.
% \MM{Also you don't specifically need to know at the beginning of the prefill stage to pipeline, do you?  You'll have less benefit if you start somewhere in the middle, but so what?  So I've removed references to "beginning" of prefiil.}
% In this case, KV data generated on the prefill instance must be temporarily stored in the GPU memory. If the GPU memory is insufficient, the KV data will be stored in local or remote CPU memory~\cite{strati2024dejavukvcachestreamingfast}. When a decode instance becomes available, it retrieves the KV data, and the KV transmission bottleneck can then again negatively affect the decode stage.
}


\noindent\textbf{Pipelining for reducing communication overhead.}
As indicated in~\cref{sec:intro}, pipelining i) cannot mitigate communication overhead when communication time significantly exceeds prefill time, and ii) is infeasible when GPU memory is insufficient on all decode instances. In case ii), the prefill instance transfers the KV data to its CPU memory~\cite{strati2024dejavukvcachestreamingfast}.
We tested Llama-3.1 with Cocktail on various prefill instances with pipelining.   Fig.~\ref{fig:comm_bottleneck_overlapping} shows the average communication time ratio with different requests-per-second (RPS).
When RPS increases from 0.06 to 0.18, the average communication time ratio on V100 rises from 21.4\% to 39.2\%, which reflects case i) above. For A10G, T4, and L4, this ratio increases from 3.3\%-4.1\% to 18.7\%-23.5\%, which reflects case ii) above.
When RPS is 0.18, the decode instances have insufficient GPU memory. On A100 with high bandwidth, this ratio grows from 1.4\% to 3.7\%.
\DEL{For V100, the pipelining cannot reduce communication overhead because of its low bandwidth.
For A10G, T4, and L4, when RPS is 0.18, the decode instances have insufficient GPU memory to serve the new requests; hence, the prefill instances store the KV data in CPU memory, making the pipelining inapplicable.}
The results demonstrate the limitations of pipelining; it is primarily suitable for scenarios with low KV transmission overhead and light workloads.
In this paper, we focus on reducing KV communication overhead through quantization, as opposed to relying on pipelining.
% \sh{need to be concise here}
% Furthermore, if the prefill instance lacks prior knowledge of the decode instance due to insufficient GPU memory across all decode instances, it must temporarily transfer KV data to its CPU memory~\cite{strati2024dejavukvcachestreamingfast}, rendering pipelining infeasible.

% We implemented \minlan{not we implemented. but one way to address comm overhead is pipelining... explain what's pipelining. Say why it may not work well for load balancing. and then say even when you use it, there are still significant networking overhead} pipelining~\cite{splitwise} for the baseline and used the strategy of transferring KV data to CPU memory on the prefill instance when no decode instance has enough memory~\cite{strati2024dejavukvcachestreamingfast}.
% % \sh{\red{DONE} again, percentage/ratio: only choose one in paper writing and in figures. "ratio" is shorter. "Average" in Y can be "Avg' if need space. Call it "communication time ratio" for the average value. Clearly explain how it is calculated in the first place, and then just use this exact term. the same for other names.}




% making the pipeline inapplicable. This demonstrates the limitations of pipelining; it is primarily suitable for scenarios with low KV transmission overhead and light workloads.

%In the future, as KV data sharing across requests~\cite{cachegen, mooncake} for resuing KV data to reduce computation time in disaggregated LLM inference becomes popular, pipelining communication will become even less applicable for overlapping KV transmission and prefill computation \red{because the shared KV data is stored on remote storage servers and the communication between the storage server and decode instances can not be eliminated}\sh{\DONE explain why}.
% In this paper, we focus on reducing KV communication overhead through quantization, as opposed to relying on pipelining.

\subsection{Overhead for KV Quantization}
%Reducing the KV transmission overhead in disaggregated LLM inference involves minimizing the size of KV data as much as possible without significantly compromising accuracy.
Quantization methods can be used to reduce the KV transmission overhead and the memory overhead during decode. However, they must dequantize all tokens' KV values retrieved from the KV cache before computation in every decode iteration, which incurs substantial dequantization overhead.
% \minlan{Say high-level why kvquant may not work first}
%State-of-the-art works have proposed various quantization methods for KV compression, among which
CacheGen~\cite{cachegen} and KVQuant~\cite{kvquant} are representative of the state-of-the-art. CacheGen focuses on leveraging KV data’s
distributional properties to encode it into more compact bitstream representations. KVQuant, on the other hand, targets low-precision KV quantization with 2-bit precision. Both can achieve up to approximately 86\% KV compression rates with approximately 98\% of baseline accuracy.

As strawman methods,
we implemented CacheGen and KVQuant in the baseline disaggregated LLM inference. On the prefill instance, the methods quantize KV data using CacheGen or KVQuant and transmit the quantized KV data to the decode instance, which dequantizes the data back to FP16 before conducting the attention computation.

%

% \sh{\DONE in the final version, make sure each fig is on the same page the refers to it}

% \begin{table}[h]
% \centering
% \begin{adjustbox}{max width=\columnwidth}
% \begin{tabular}{ |c||c|c|c|c|c|  }
%  \hline
%  & A10G & V100 & T4 & L4 & A100 \\
%  \hline
%  Baseline & 82.1\% & 79.6\% & 84.6\% & 82.3\% & 85.9\% \\
%  \hline
%  CacheGen & 58.4\% & 57.5\% & 59.7\% & 58.8\% & 62.1\% \\
%  \hline
%  KVQuant & 55.9\% & 55.7\% & 56.2\% & 57.4\% & 59.8\% \\
%  \hline
% \end{tabular}
% \end{adjustbox}
%  \vspace{-0in}
% \caption{Peak GPU memory usage on decode instances when using different prefill instances.}\label{tab:peak_gpu_mem}
% \vspace{-0.25in}
% \end{table}

% \begin{table}[h]
% \centering
% \begin{adjustbox}{max width=\columnwidth}
% \begin{tabular}{ |c||c|c|c|c|  }
%  \hline
%  IMDb & arXiv & Cocktail & HumanEval \\
%  \hline
%  Baseline 61.3\% & 77.1\% & 85.3\% & 64.9\% \\
%  \hline
% \end{tabular}
% \end{adjustbox}
%  \vspace{-0in}
% \caption{Peak GPU memory on decode instances for different methods.}\label{tab:peak_gpu_mem_datasets_diff_methods}
% \vspace{-0.25in}
% \end{table}





\begin{figure}[h]
    \centering
    \subfigure[CacheGen.\label{fig:comm_bottleneck_gpus_cachegen}]
    {\includegraphics[width=0.495\linewidth,height=2.5cm]{fig/analysis/comm_bottleneck_gpus_cachegen.pdf}}
    \subfigure[KVQuant.\label{fig:comm_bottleneck_gpus_kvquant}]
    {\includegraphics[width=0.495\linewidth,height=2.5cm]{fig/analysis/comm_bottleneck_gpus_kvquant.pdf}}

    \vspace{-0.2in}
    \caption{Employing KV quantization across prefill instances.}
    \label{fig:comm_bottleneck_gpus_methods}
    \vspace{-0.2in}
\end{figure}

\begin{figure}[h]
    \centering
    \subfigure[CacheGen.\label{fig:comm_bottleneck_models_cachegen}]
    {\includegraphics[width=0.495\linewidth,height=2.5cm]{fig/analysis/comm_bottleneck_models_cachegen.pdf}}
    \subfigure[KVQuant.\label{fig:comm_bottleneck_models_kvquant}]
    {\includegraphics[width=0.495\linewidth,height=2.5cm]{fig/analysis/comm_bottleneck_models_kvquant.pdf}}

    \vspace{-0.2in}
    \caption{Employing KV quantization across models.}
    \label{fig:comm_bottleneck_models_methods}
    \vspace{-0in}
\end{figure}


\begin{figure}[h]
    \centering
    \subfigure[CacheGen.\label{fig:comm_bottleneck_datasets_cachegen}]
    {\includegraphics[width=0.495\linewidth,height=2.5cm]{fig/analysis/comm_bottleneck_datasets_cachegen.pdf}}
    \subfigure[KVQuant.\label{fig:comm_bottleneck_datasets_kvquant}]
    {\includegraphics[width=0.495\linewidth,height=2.5cm]{fig/analysis/comm_bottleneck_datasets_kvquant.pdf}}

    \vspace{-0.2in}
    \caption{Employing KV quantization across datasets.}
    \label{fig:comm_bottleneck_datasets_methods}
    \vspace{-0.15in}
\end{figure}

%We tested CacheGen and KVQuant using the same settings as for Fig.~\ref{fig:comm_bottleneck_gpus}, \ref{fig:comm_bottleneck_models},and~\ref{fig:comm_bottleneck_datasets}.
Fig.~\ref{fig:comm_bottleneck_gpus_methods}, \ref{fig:comm_bottleneck_models_methods}, and~\ref{fig:comm_bottleneck_datasets_methods} illustrate the average prefill time ratio, communication time ratio, dequantization time ratio, and decode time ratio with different prefill instances, models, and datasets, respectively.
Comparing Fig.~\ref{fig:comm_bottleneck_gpus_methods} with Fig.~\ref{fig:comm_bottleneck_gpus}, we observe that
for the A100, T4, A10G, L4, and V100 prefill instances, CacheGen and KVQuant reduce the average communication time ratio
by 3.13\%-3.25\%, 16.18\%-16.39\%, 18.5\%-18.84\%, 19.81\%-20.27\%, and 33.5\% and 34.1\%, respectively. %The average communication time ratio on V100 is around 8\% due to its low bandwidth, which is only 10 Gbps.
% Based on our experiment results, CacheGen and KVQuant reduce the average JCT by 21.2\%-24.1\% by decreasing memory access latency for KV data.
However, the average KV dequantization time ratios for other instances range from 26.4\% to 37.9\%.
%34.6\%-37.9\%, 25.3\%-25.5\%, 28.6\%-30.4\%, 26.4\%-28.1\%, and 24.4\%-28.7\%, respectively.
This demonstrates that the dequantization overhead introduced in each decode iteration negatively impacts the end-to-end performance.
% CacheGen and KVQuant reduce the average percentage\sh{\red{use percentage} percentage/ratio--choose one and always use this one in the whole paper} of decode time by 9\%-29\%.
% CacheGen and KVQuant reduce the average decode time by 41.3\%-46.8\%.
% This is because the reduced KV size improves the memory access speed of the KV data during the decode stage.
% Table~\ref{tab:peak_gpu_mem} shows the peak GPU memory usage on decode instances for Llama-3.1 with Cocktail. CacheGen and KVQuant reduce the peak GPU memory by 22.1\%-26.2\%.
% \sh{seems Table~\ref{tab:peak_gpu_mem} is not needed. quantization methods reduce mem usage is for sure, no new observation here}.





Comparing Fig.~\ref{fig:comm_bottleneck_models_methods} and Fig.~\ref{fig:comm_bottleneck_models}, we see that CacheGen and KVQuant reduce the average communication time ratio by 10.26\%-21.63\%.
% Based on our experiment results, CacheGen and KVQuant reduce the average JCT by 17.5\%-23.6\% by decreasing memory access latency for KV data.
However, for all models, they have an average dequantization time ratio of 18.2\%-30.8\%. Comparing Fig.~\ref{fig:comm_bottleneck_datasets_methods} and Fig~\ref{fig:comm_bottleneck_datasets}, we see that CacheGen and KVQuant reduce the average communication time ratio by 8.19\%-18.73\%.
% Based on our experiment results, CacheGen and KVQuant reduce the average JCT by 13.6\%-24.1\% by decreasing memory access latency for KV data.
However, %for IMDb, HumanEval, arXiv, and Cocktail,
they have average dequantization time ratios of
17.2\%-30.4\%.
%17.2\%-19.3\%, 19.2\%-20.9\%, 24.5\%-27.7\%, and 28.6\%-30.4\%, respectively.
Long-sequence datasets have 12.4-24.9$\times$ dequantization time compared to short-sequence datasets because more KV data is dequantized.
% CacheGen and KVQuant reduce the average decode time by 17.9\%-24.6\% for the short-sequence datasets IMDB and HumanEval and by 38.2-49.6\% for the long-sequence datasets arXiv and Cocktail.


% CacheGen and KVQuant reduce the average decode time by 31.2\%-48.1\%.
%The reasons are the same as explained for Fig.~\ref{fig:comm_bottleneck_gpus_methods}.


 % \sh{\red{I use percentage in the whole paper} name it "decode time ratio"} from 36\%-77\% in Fig.~\ref{fig:comm_bottleneck_models} to 33\%-65\% and reduce the average percentage of communication to 1.54\%-3.74\%. For all models, CacheGen and KVQuant have an average percentage of dequantization of 18.2\%-30.8\%. The reasons are the same as explained for Fig.~\ref{fig:comm_bottleneck_gpus_methods}.


Our measurement results from Llama-3.1 70B with different datasets indicate that CacheGen and KVQuant reduce the peak memory usage on decode instances by 15.7\%-19.6\% for short-sequence datasets and by 26.9\%-33.6\% for long-sequence datasets (Table~\ref{tab:peak_gpu_mem}). They reduce the average JCT by 13.6\%-24.1\% by decreasing memory access time for KV data.


The computation times are the same between the baseline and KV quantization methods because they all perform computation on the original FP16 data.

% \red{
% Although CacheGen and KVQuant can reduce KV transmission overhead and improve memory access speed for KV data during the decode stage, they introduce extra overhead for KV dequantization in every decode iteration. This expensive cost negatively impacts the decode efficiency and increases with the sequence length. In contrast to KV dequantization, quantizing KV for each token occurs only once during the entire end-to-end process. Therefore, quantization's proportion of the end-to-end time is very small, typically not exceeding 3\%, which is demonstrated in the detailed breakdown of JCT in Fig.~\ref{fig:e2e_decompose}. As a result, we focus on the dequantization overhead.
% }


% reduce the average percentage of decode\sh{use one term as the above} from 52\%-76\% in Fig.~\ref{fig:comm_bottleneck_datasets} to 40\%-66\% and reduce the average percentage of communication to 1.31\%-3.17\% \sh{do not use "to", use "by' and update the statistics. "to" cannot give a sense how much unless you give the original statistics}. For IMDb, HumanEval, arXiv, and Cocktail, CacheGen and KVQuant have average percentages of dequantization of 17.2\% and 19.3\%, 19.2\% and 20.9\%, 24.5\% and 27.7\%, and 28.6\% and 30.4\%, respectively\sh{use range}. The longer the sequence length, the higher the dequantization overhead because more KV data is dequantized in each decode iteration.


\begin{obs}~\label{obs:dequant_overhead}
Although KV quantization methods can effectively reduce the KV transmission overhead, memory usage, and KV memory access time in disaggregated LLM inference, they introduce additional KV dequantization overhead up to 37.9\% of JCT, which is even higher for long-sequences. %increases with the sequence length.
Additionally, they cannot reduce computation time.
% \sh{\DONE I added the last part. make sure you have results to support.}
\end{obs}

% Nowadays, long-sequence workloads, such as information retrieval~\cite{cocktailforir} and summarization tasks~\cite{arxiv}, are becoming increasingly popular. Commercial models like Gemini 1.5~\cite{gemini-1.5} already support a context window length of 1M.




\section{Low-Precision Floating Points}\label{sec:fp8_for_kv}

Low-precision FP4/6/8 can accelerate computation when hardware supports them.
However, NVIDIA GPUs with pre-H100 architectures do not support FP8 computation.
FP4/6 needs to be converted to FP8 or FP16 for computation, leading to conversion overhead. Furthermore, FP4/6/8 cannot achieve a high KV compression rate to minimize the communication overhead and KV memory access time.

Since none of the GPUs in the experiments support FP8 computation, we conducted a simulation test to measure the communication overhead and KV memory access time for FP4/6/8 using Llama-3.1 70B, Cocktail and different prefill instances.
We converted KV data from FP4/6/8 to FP16 before attention computation, and then manually halved the time spent in matrix multiplication in attention to simulate FP8 computation.
The simulation results show that FP4, FP6, and FP8 can have an average KV communication time ratio of up to 24.3\%, 32.3\%, and 37.5\%, respectively.
The average KV memory access time ratio is 10.7\%-19.4\%.
These simulation results indicate that FP4/FP6/FP8 cannot effectively minimize the communication and memory access time overhead due to the low KV compression rate.

% This highlights that FP8 still suffers from high KV transmission overhead. Compared to CacheGen and KVQuant, FP8 reduces the end-to-end time by only 7\%-11\%, as it still has high KV transmission overhead and a longer decode time caused by higher memory access latency for KV data during the decode stage. Furthermore, in the end-to-end performance evaluation in~\cref{sec:e2e_perf_test}, our FP8 simulation results show that its end-to-end time can be up to 48\% higher than our proposed system, again due to its high KV transmission overhead and decode time.





% \sh{FP8 testing should be combined into the previous subsection. It belongs to the same category of acheGen and
% KVQuant.}\ran{I don't actually know why this is in the Motivation section.}

% \sh{either you move it to the top and get the same observation, or add another observation here saying: Though low-precision methods coud avoid the dequantization step and reduce computation time, their compression rate is low, so are not applicable for cheap GPUs with limited bandwidth.-make sure your results support this.}

% \MM{\red{Changes marked in red below.} This should come before homomorphic.  Again, you need to make clear to the reader which are the comparison points/straw men, and which is our much better solution, in this section.}

% LLM developers have explored using FP8~\cite{vllm-fp8} to reduce KV size and accelerate computation. However, adopting FP8 for KV in disaggregated LLM inference involves trade-offs between hardware requirements, accuracy, and end-to-end performance \sh{this should not be the main claim. It has the same drawbacks as CacheGen and
% KVQuant, and it has lower compression ratio or not so effective in reducing data size. Additionally, you can briefly use one sentence to say it is not supported by the hardwared.}.
% First, NVIDIA GPUs require at least an H100 to support FP8 computation. \ran{The term `at least an H100' is not clear. }Given the high cost of H100 GPUs, they are typically used for decode instances, leaving the prefill stage without FP8 acceleration.
% \red{Second, although Table~\ref{tab:accuracy_perf} shows that FP8 achieves approximately 1\% higher accuracy compared to CacheGen, KVQuant, and our proposed system\sh{our proposed system is \sys, if yes, never mention our proposed system in this section. If no, indicate what the system is. There is no point to compare FP8 and CacheGen/ KVQuant. You just need to show what I wrote in the above.} in this paper, this comes at the expense of reduced KV compression rates and can compromise the end-to-end performance.}
% CacheGen, KVQuant, and our proposed system achieve around 85\% KV compression, whereas FP8 achieves only 50\%.

% \red{We implemented a strawman method using vLLM FP8 for KV~\cite{vllm-fp8} for disaggregated LLM inference and tested it using Llama-3.1 with the Cocktail dataset to compare with quantization-based methods.}
% Since none of the GPUs in the experiments support FP8 computation, we converted KV data from FP8 to FP16 before attention computation. We then manually halved the time spent in matrix multiplication in attention to have simulated end-to-end performance.
% The simulation results indicate that, when using V100 prefill instances, the average KV transmission time across all requests accounts for up to 37\% of the end-to-end time. This highlights that FP8 still suffers from high KV transmission overhead. Compared to CacheGen and KVQuant, FP8 reduces the end-to-end time by only 7\%-11\%, as it still has high KV transmission overhead and a longer decode time caused by higher memory access latency for KV data during the decode stage. Furthermore, in the end-to-end performance evaluation in~\cref{sec:e2e_perf_test}, our FP8 simulation results show that its end-to-end time can be up to 48\% higher than our proposed system, again due to its high KV transmission overhead and decode time.
% Therefore, FP8 can effectively improve end-to-end performance while maintaining high accuracy only when the hardware supports it, and sufficient bandwidth is available\sh{you need to highlight what I wrote above. Here, the point is to show it cannot effectively reduce data size, so not suitable in our scenario (which has limited bandwidth). Do not say "...sufficient bandwidth is available".}.




\DEL{
\section{Low-Precision Floating Points}\label{sec:fp8_for_kv}\sh{FP8 testing should be combined into the previous subsection. It belongs to the same category of acheGen and
KVQuant.}\ran{I don't actually know why this is in the Motivation section.}

\sh{either you move it to the top and get the same observation, or add another observation here saying: Though low-precision methods coud avoid the dequantization step and reduce computation time, their compression rate is low, so are not applicable for cheap GPUs with limited bandwidth.-make sure your results support this.}

\MM{\red{Changes marked in red below.} This should come before homomorphic.  Again, you need to make clear to the reader which are the comparison points/straw men, and which is our much better solution, in this section.}

LLM developers have explored using FP8~\cite{vllm-fp8} to reduce KV size and accelerate computation. However, adopting FP8 for KV in disaggregated LLM inference involves trade-offs between hardware requirements, accuracy, and end-to-end performance \sh{this should not be the main claim. It has the same drawbacks as CacheGen and
KVQuant, and it has lower compression ratio or not so effective in reducing data size. Additionally, you can briefly use one sentence to say it is not supported by the hardwared.}.
First, NVIDIA GPUs require at least an H100 to support FP8 computation. \ran{The term `at least an H100' is not clear. }Given the high cost of H100 GPUs, they are typically used for decode instances, leaving the prefill stage without FP8 acceleration.
\red{Second, although Table~\ref{tab:accuracy_perf} shows that FP8 achieves approximately 1\% higher accuracy compared to CacheGen, KVQuant, and our proposed system\sh{our proposed system is \sys, if yes, never mention our proposed system in this section. If no, indicate what the system is. There is no point to compare FP8 and CacheGen/ KVQuant. You just need to show what I wrote in the above.} in this paper, this comes at the expense of reduced KV compression rates and can compromise the end-to-end performance.}
CacheGen, KVQuant, and our proposed system achieve around 85\% KV compression, whereas FP8 achieves only 50\%.

\red{We implemented a strawman method using vLLM FP8 for KV~\cite{vllm-fp8} for disaggregated LLM inference and tested it using Llama-3.1 with the Cocktail dataset to compare with quantization-based methods.}
Since none of the GPUs in the experiments support FP8 computation, we converted KV data from FP8 to FP16 before attention computation. We then manually halved the time spent in matrix multiplication in attention to have simulated end-to-end performance.
The simulation results indicate that, when using V100 prefill instances, the average KV transmission time across all requests accounts for up to 37\% of the end-to-end time. This highlights that FP8 still suffers from high KV transmission overhead. Compared to CacheGen and KVQuant, FP8 reduces the end-to-end time by only 7\%-11\%, as it still has high KV transmission overhead and a longer decode time caused by higher memory access latency for KV data during the decode stage. Furthermore, in the end-to-end performance evaluation in~\cref{sec:e2e_perf_test}, our FP8 simulation results show that its end-to-end time can be up to 48\% higher than our proposed system, again due to its high KV transmission overhead and decode time.
Therefore, FP8 can effectively improve end-to-end performance while maintaining high accuracy only when the hardware supports it, and sufficient bandwidth is available\sh{you need to highlight what I wrote above. Here, the point is to show it cannot effectively reduce data size, so not suitable in our scenario (which has limited bandwidth). Do not say "...sufficient bandwidth is available".}.

}


% \subsection{Homomorphic Quantization for KV}\sh{you can remove this subsection--all are described in Intro already}
% \ran{This is our solution and not part of the Motivation.}

% \MM{\red{Rewrote the following paragraph.} Cite previous work on homomorphic quantization here -- we didn't invent this, just a new application/design.
% Maybe it's in the intro, but hype up the idea more!
% }
% \red{[Zeyu: I cite the THC paper, and it's for matrix addition instead of matrix multiplication. I still say we propose homomorphic quantization for KV-related matrix multiplications. Pls see the following.]}
% To leverage the benefits of KV quantization while reducing the overhead caused by dequantization, we should avoid dequantizing KV data before attention computation.
% Existing quantization work~\cite{thc2024li} has introduced homomorphic quantization for gradient aggregation in machine learning training systems, enabling direct aggregation of quantized gradient data and eliminating the overhead of dequantization and quantization before and after aggregation. Inspired by this homomorphic quantization, we can also directly use low-precision quantized KV data for attention computation and apply a small amount of computation to convert the output into an approximation of the true output, thereby reducing dequantization overhead and accelerating attention computation.
% However, the homomorphic quantization in~\cite{thc2024li} is limited to matrix addition and cannot be applied to KV-related matrix multiplications in attention. To address this, we propose a \underline{Ho}momorphic \underline{Quant}ization method specifically designed for KV-related matrix multiplications (\sys) and use it to develop a disaggregated LLM inference system efficient in communication, computation, and memory.

% \sh{\red{I think we can remove this section because in the introduction we've discussed it.} where is the subsection of low precision?}


\section{Background}\label{sec:bkg}



\noindent \textbf{LLM inference basics.}
LLM inference consists of two stages: prefill and decode. During the prefill stage, the model processes a sequence of input tokens, also referred to as the prompt. The LLM processes the input tokens to generate the first token. This first token is then used in the decode stage, where it is fed back into the model to generate the next token. The generated token serves as the input for the next decode iteration, and the process repeats iteratively to generate subsequent tokens.
% \sh{above can be deleted if need space}
LLMs consist of multiple identical layers and use the attention mechanism~\cite{self-attention} in each layer to evaluate the interdependencies between tokens in a sentence across different aspects represented by different attention heads. The input tokens, represented by the embedding matrix $E$, are transformed into a query matrix $Q^h$, a key matrix $K^h$, and a value matrix $V^h$ in each attention head $h$ by:
\vspace{-0in}
\begin{equation}\label{equ:qkv}
    Q^h=EW_Q^h \mbox{, ~} K^h=EW_K^h \mbox{, ~} V^h=EW_V^h, \vspace{0in}
\end{equation} where $W_Q^h$, $W_K^h$, and $W_V^h$ are the parameter matrices.
In prefill, the input tokens' $Q^h$, $K^h$, and $V^h$ are used for self-attention computation, and $K^h$ and $V^h$ are cached in memory for reuse in the decode stage. In decode, the input token's $K^h$ and $V^h$ are respectively combined with all previous tokens' $K^h$ and $V^h$ in the cache to form the new $K^h$ and $V^h$ that will be used in the self-attention computation.
The self-attention computes the output $O^h$ of head $h$ by: %using $Q^h$ from the input token(s), the combined $K^h$, and the combined $V^h$ as follows:
\vspace{-0.06in}
\begin{equation}\label{eq:attention}
    % O^h=P^hV^h \mbox{ for } P^h=\mbox{Softmax}(\frac{Q^h(K^h)^T}{\sqrt{d_h}})\vspace{-0.05in}
    O^h=\mbox{\textit{softmax}}\bigg(\frac{Q^h(K^h)^T}{\sqrt{d_h}}\bigg)V^h=P^hV^h. \vspace{-0.05in}
\end{equation}
The \textit{softmax} function operates row-wise on the input matrix $[x_{i,j}]$ as follows:
\vspace{-0.13in}
\begin{equation}\label{eq:softmax}
    % \begin{split}
        \DEL{[b_{i,j}]=\mbox{Softmax}([a_{i,j}])\mbox{, for }
        b_{i,j}=}
        \frac{exp(x_{i,j})}{\sum_{k=1}^{t_i}exp(x_{i,k})}, \vspace{-0in}
    % \end{split}
\end{equation}where $t_i$ is the index of the token on row $i$.
The outputs from all heads are concatenated together along the head dimension to form the self-attention output $O$. The model further processes $O$ through a linear transformation, a Multi-Layer Perceptron (MLP), and other operations to finally produce logits, which are used to generate an output token. Self-attention accounts for a large proportion of computational overhead in the entire process.




\noindent \textbf{Disaggregated LLM Inference.}
In disaggregated LLM inference, during the prefill stage, the prompt tokens' $Q$, $K$, and $V$ are generated and used for self-attention on the prefill instance. The prefill stage outputs the first token, which is then transferred to the decode instance along with the KV data of prompt tokens.
If no decode instance has enough memory to serve the request, the prefill instance temporarily stores the first token and KV data in its CPU memory
until a decode instance is available~\cite{strati2024dejavukvcachestreamingfast}.
On the decode instance, the first token is fed to the model as the input, and its $Q$, $K$, and $V$ are generated. The input token's $K$ and $V$ are combined with the previous tokens' $K$ and $V$. The input token's $Q$ and the combined $K$ and $V$ are used for self-attention computation.
The decode iteration outputs the next token, which will be fed into the decode model as the next input, followed by the same steps in the decode stage. This process repeats until the maximum output length is reached or the End-of-Sentence (EOS) token is output.


\section{System Design}\label{sec:design}
%In this section, we provide a detailed explanation of the design principles of \sys. Table~\ref{tab:symbols} presents the primary notations used in this paper.

% \begin{table}[h]
% \centering
% \small
% \begin{tabular}{ |c|l||c|l|  }
%  \hline
%  $d$ & Model dimension size & $d_h$ & Attention head dimension size \\
%  \hline
%  $Q$ & Query matrix & $K$ & Key matrix \\
%  \hline
%  $V$ & Value matrix & $S$ & Attention score \\
%  \hline
%  $P$& Attention probability & $L$& Sequence length \\
%  \hline
%  $m$& Minimum value & $s$& Scale value \\
%  %$R_p$ & Compression matrix with $p$ \\
%  \hline
%  \end{tabular}
%  \vspace{-0in}
% \caption{Notations used in the paper.}\label{tab:symbols}
% \vspace{-0.25in}
% \end{table}



%\vspace{-0.15in}
% \sh{remove vspace to reduce space between the sections and subsection unless you really need them}
\subsection{Overview}



% \vspace{-0.1in}
% \begin{table}[h]
% \centering
% \small
% \begin{tabular}{ |c|l||c|l|  }
%  \hline
%  $d_h$ & Attention head dimension size & $Q$ & Query matrix \\
%  \hline
%  $K$ & Key matrix & $V$ & Value matrix \\
%  \hline
%  $S$ & Attention score & $P$& Attention probability \\
%  \hline
%  $L$& Sequence length & $m$& Minimum value \\
%  \hline
%  $s$& Scale value & $q$& Quantized value \\
%  %$R_p$ & Compression matrix with $p$ \\
%  \hline
%  \end{tabular}
%  \vspace{-0in}
% \caption{Notations used in the paper.}\label{tab:symbols}
% \vspace{-0.25in}
% \end{table}

% \MM{A little confusing below.  Are the $K$, $Q$, $V$ the same for prefill and decode?  How are they known/are they inputs.  Since we've been saying we avoid dequantization point out/emphasize it's done once at the end (as opposed to repeatedly through the computations?).}


Observation~\ref{obs:kv_bottleneck} implies that we need to reduce the communication time, computation time, and memory access latency for KV. Therefore, we can use quantized KV values. However, this will generate high dequantization time overhead based on Observation~\ref{obs:dequant_overhead}.
To quantize KV values while eliminating the dequantization time overhead, \sys provides homomorphic quantization on KV-related matrix multiplications. Employing \sys in the disaggregated LLM inference reduces the KV data transmission time, the computation time, and the memory access latency for KV, thereby improving end-to-end response time.
% Note that reducing memory requirements allows more requests to be processed in a batch, thereby decreasing waiting time.
% \zeyu{I think we can remove the previous sentence.}
% \ran{Are people actually doing this batching? Can you give a reference?}
Fig.~\ref{fig:overview} illustrates the workflow of employing \sys in the disaggregated LLM inference.
Since CacheGen has a KV compression rate of 86\%, and KVQuant uses 2-bit quantization to achieve a similar compression rate, we also use 2-bit quantization for KV.
%\red{We add extra steps to quantize $Q$, $K$, and $V$ so that we can perform self-attention on quantized data.}
The prefill instance generates $Q$, $K$, and $V$ from prompt tokens (\circled{1}) and quantizes them to $Q'$, $K'$, and $V'$ (\circled{2}).
$Q$ will be discarded right after computation, which means the quantized $Q'$ does not need to have a minimal size to save space. Thus, $Q$ uses 8-bit quantization rather than 2-bit to increase accuracy.
The first matrix multiplication between $Q'$ and $K'$ as in Eq.~\eqref{eq:attention} is performed using homomorphic quantization (\circled{3}) to output attention score $S$ without the need to dequantize $K'$, accelerated by GPU's INT8 computation capacity.
The attention score $S$ is transformed to attention probability $P$ via \textit{softmax} in Eq.~\eqref{eq:softmax} (\circled{4}), which is then quantized to $P'$ using INT8 quantization for accuracy (\circled{2}). $P'$ and $V'$ are multiplied using homomorphic quantization for acceleration (\circled{3}). The self-attention output $O$ is processed by further operations to finally output the first token (\circled{5}).
If no decode instance has enough GPU memory, the prefill instance will swap the quantized KV data to CPU memory (\circled{6}).
When a decode instance has enough memory, the prefill instance transmits the first token, $K'$, $V'$, and the quantization metadata, the minimum value $m$ and the scale value $s$, to the decode instance (\circled{7}).
$K'$ and $V'$ are stored in KV cache on the decode instance (\circled{8}).
The decode instance generates the $Q$, $K$, and $V$ from the first token (\circled{1}) and quantizes them to $Q'$, $K'$, and $V'$ (\circled{2}) as on the prefill instance. The first token's $K'$ and $V'$ are merged with all prior tokens' $K'$ and $V'$ along the token dimension, respectively (\circled{9}). The homomorphic quantization for the first token's $Q'$ and the updated $K'$ and $V'$ is performed in the same way as prefill.
The next token is generated through further operations (\circled{5}) and fed to the model to proceed to the next decode iteration (\circled{1}).



% and performs a quantized attention computation, a modified self-attention operation on the quantized data to approximate the actual output (\circled{2}). The self-attention output is then dequantized to provide an approximation of the real output (\circled{3}).
% At the end of prefill, if no decode instance has enough GPU memory, the quantized KV is temporarily stored in CPU memory on prefill instances~\cite{strati2024dejavukvcachestreamingfast}; otherwise, it is transmitted to a decode instance with the shortest updated queue length~\cite{splitwise} (\circled{4}).
% \sh{\DONE you need to add step 4}The decode instance quantizes the input token's $Q$, $K$, and $V$ (\circled{5}). The new quantized $K$ and $V$ are combined with previous tokens' quantized $K$ and $V$ for the quantized attention computation (\circled{6}), followed by a dequantization step for the attention output (\circled{7}). The steps of \sys are highlighted in bold, while the remaining steps in Fig.~\ref{fig:overview} follow the original process of disaggregated LLM inference.



% \zeyu{the next paragraph will be removed.}
% During prefill, the $Q$, $K$, and $V$ of the prompt tokens are first quantized before attention computation (\circled{1}). Quantized attention is then performed to accelerate computation (\circled{2}). The output is dequantized through minimal computation to obtain an approximation of the true output (\circled{3}). At the end of prefill, if no decode instance has enough GPU memory\sh{\DONE be specific about this, which condition}, the quantized KV is temporarily stored in CPU memory on prefill instances~\cite{strati2024dejavukvcachestreamingfast}; otherwise, it is transmitted to a decode instance with the shortest updated queue length~\cite{splitwise} \sh{\DONE after you explained a concept, no need to explain it again}(\circled{4}).
% During the decode stage, the $Q$, $K$, and $V$ of the input token are quantized before attention (\circled{5}). The quantized attention \sh{you need to explain this new term} is then performed on the received quantized KV data (\circled{6}). Finally, the attention output is dequantized (\circled{7}).

\vspace{-0in}
\begin{figure*}[h]
    \centering
    \includegraphics[width=1\linewidth]{fig/overview_detail.pdf}
    \vspace{-0.25in}
    \caption{Overview of \sys in disaggregated LLM inference.}
    \label{fig:overview}
    \vspace{-0in}
\end{figure*}

% \sh{in Fig5, in step 7, add word explanation for the notations}

\subsection{Homomorphic Quantization for Matrix Multiplication}\label{sec:hoq_mm}



% \sh{step3 and step6 are the same? if yes, make words the same, and change 6 to 3 here. If not the same, indicate what are different. Step2 also is explained beliw, right?}


KV-related computation in Eq.~\eqref{eq:attention} involves two matrix multiplications: %$Q \times K$, and $P \times V$.
$Q^h(K^h)^T$ and $P^hV^h$.
We aim to perform multiplication on two quantized matrices to leverage the GPU's INT4 or INT8 computation capabilities without the need to dequantize matrix values before computation to get an approximation of the true output.
For this goal, we propose homomorphic quantization for the matrix multiplications. That is, for matrix multiplication $C=AB$, the method first quantizes $A$ and $B$ to obtain $A'$ and $B'$. It then performs the matrix multiplication $C'=A'B'$ to have the quantized output $C'$.
% , where $A'B'$\sh{remove this second part} can be accelerated by INT4 or INT8 computation.
$C'$ is subsequently approximated into $C$ with a minimal overhead.
%Thus, homomorphic quantization for KV essentially quantizes the two matrices being multiplied.
% Since CacheGen has a KV compression rate of 86\%, and KVQuant uses 2-bit quantization to achieve a similar compression rate, we also use 2-bit quantization.


% \ran{The notation here is confusing. What is $G$? Is it a set? I think you don't need the current $G$, and wherever you use $\Pi$ you just use $G$ (say that its an integer parameter that stands for the group size). Later you also have $|GS|$ in multiple places, which is probably the same as $\Pi$?}

We use an asymmetric 2-bit stochastic quantization~\cite{squant} to reduce quantization error, which partitions the elements of each row or column into partitions.
Fig.~\ref{fig:hoq_matmul} shows examples for partitioning. We denote the dimensions of the matrices $A$ and $B$ by $M \times Z$ and $Z \times N$, respectively. Fig.~\ref{fig:hoq_matmul_1} illustrates quantization by partitioning elements in each row of $A$ and each column of $B$. The quantization partition size, denoted by $\Pi$, is the number of elements in a partition.
To increase accuracy, we can perform finer-grained quantization by setting a smaller $\Pi$, i.e., dividing the inner dimensions of the two matrices into more partitions. For example, Fig.~\ref{fig:hoq_matmul_2} illustrates quantization by partitioning elements in half of a row of $A$ and half of a column of $B$. %with each group sharing a common minimum value $m$ and scale $s$.
In each partition $i$, the method identifies the minimum ($min_i$) and maximum ($max_i$) values of the matrix elements and computes the $scale=\frac{max_i-min_i}{2^2-1}$. Each original value $x$ in a partition is quantized to an integer $x'=round(\frac{x-min_i}{scale})$. The stochastic rounding $round(*)$ rounds $*$ to $\lfloor * \rfloor$ with probability $(\lceil * \rceil-*)/\lceil * \rceil-\lfloor * \rfloor)$ and to $\lceil * \rceil$ otherwise. %\sh{\red{yes} is it what Shay indicated in the end of the meeting? make sure you did not misunderstand or miss anything. }

% \ran{I think we have here a suboptimal choice of quantization levels. If we are doing biased (round-to-nearest) quantization and have 2-bits, thus four quantization levels, it may make more sense to use $\min_i+\max_i/8, \min_i+\max_3/8, \min_i+\max_5/8, \min_i+\max_7/8$ as this reduces the worst-case error to $(\min_i-\max_i)/8$. Another thing we may want to try to unbiased quantization.}




\begin{figure}[h]
    \centering
    \subfigure[Each row/column forms a~partition.\label{fig:hoq_matmul_1}]
    {\includegraphics[width=0.45\linewidth,height=2.3cm]{fig/hoq_matmul_1.pdf}}
    \subfigure[A half row/column forms a~partition.\label{fig:hoq_matmul_2}]
    {\includegraphics[width=0.45\linewidth,height=2.3cm]{fig/hoq_matmul_2.pdf}}

    \vspace{-0.2in}
    \caption{Illustration of partitioning for quantization.}
    \label{fig:hoq_matmul}
    \vspace{-0in}
\end{figure}



%\vspace{-0.13in}
% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.95\columnwidth]{fig/hoq_matmul.pdf}
%     \vspace{-0.16in}
%     \caption{Matrix multiplication with homomorphic quantization.}
%     \label{fig:hoq_matmul}
%     \vspace{-0.13in}
% \end{figure}

%The basic process of homomorphic quantization is that for the matrix multiplication $C=AB$, it first quantizes $A$ and $B$ to obtain $A'$ and $B'$, along with their respective \textit{min} and \textit{scale} values. Then, matrix multiplication




%After $C'=A' * B'$ is performed, and $C'$ is subsequently converted back to an approximation of $C$ using the previously stored \textit{min} and \textit{scale} values. \sh{list equ here}\sh{A and B have their own max, min values? what are the \textit{min} and \textit{scale} of C, write down the convertion equation here}

%\sh{Eq4 is not dequantization. If there is a dequantizion as in step 3, pls list the Equ here. otherwise, seems no step3? Eq4 is for step2.}
%\zeyu{eq4 is the homomorphic quantization now. It has a step to approximate $C$ to $C'$ with a minimal overhead.}

%\sh{I am confused about the dequantization step, step 3 in the fig, isn't the result already the approximation of the actual result, like the left part of (4)?}

Next, we need to estimate $C$ given $C'$. For Fig.~\ref{fig:hoq_matmul_1}, let $a_{iz}$ represent the element in the $i$-th row and $z$-th column of $A$, and $b_{zj}$ represent the element in the $z$-th row and $j$-th column of $B$. The matrix multiplication $C=AB$ can then be expressed as $c_{ij}= \sum_{z=1}^Z a_{iz}b_{zj}$ for $i\in[1,M]$ and $j\in[1,N]$.
Let $m_{a_i}$ and $s_{a_i}$ denote the minimum and scale values of $a_{iz}$. Since $a_{iz}'=round(\frac{a_{iz}-m_{a_i}}{s_{a_i}})$ and $b_{zj}'=round(\frac{b_{zj}-m_{b_j}}{s_{b_j}})$, we have $a_{iz}\approx s_{a_i}q_{a_{iz}}+m_{a_i}$ and $b_{zj}\approx s_{b_j}q_{b_{zj}}+m_{b_j}$. %where  $m_{b_j}$ and $s_{b_j}$ denote the minimum and scale values of $b_{zj}$.
Thus, $(AB)_{ij}$ can be extended to:\vspace{-0.1in}



\DEL{\sh{\DONE how to get these equs?}
\sh{\DONE notations here that are not explained need to be explained}

\sh{\DONE you need to write high-level introduction/purpose first before giving details, e.g., for (3), you may say first: how can we conduct computation on quantized data to obtain the result similar to the result of the original data. Before you give the time complexity, also give a question like: what's the time complexity for ... operation? what is the step of this operation in the overview figure?}
}

%\sh{if you change A and B to the actual QKV, will this make it easy to follow?}


\vspace{-0.05in}
\begin{equation}\label{eq:hoq_matmul}
\begin{split}
    \sum_z a_{iz}b_{zj}\approx\mbox{ } & s_{a_i}s_{b_j}\sum_z a_{iz}'b_{zj}' + m_{b_j}s_{a_i}\sum_z a_{iz}' + \\
    & m_{a_i}s_{b_j}\sum_z b_{zj}' + Zm_{a_i}m_{b_j},
\end{split}
\end{equation}\vspace{-0.05in}
\vspace{-0.0in}

\noindent where $\{\sum_z a_{iz}'b_{zj}',  \forall i,j\}$ is the quantized matrix multiplication that can be accelerated by INT8 computation. The other terms in Eq.~\eqref{eq:hoq_matmul} approximate $\sum_z a_{iz}'b_{zj}'$ ($C'$) into $\sum_z a_{iz}b_{zj}$ ($C$).
Eq.~\eqref{eq:hoq_matmul} is the homomorphic quantization for multiplication.

%\red{The next question concerns

We now analyze the computational cost of matrix multiplication and approximation in Eq.~\eqref{eq:hoq_matmul}.
% It calculates an approximation of the real output $AB$ based on the quantized matrices, the minimum value $m$ and the scale $s$.
The computational cost for $\sum_z a_{iz}'b_{zj}'$ is $2MZN$. The remaining cost for approximating $\sum_z a_{iz}'b_{zj}'$ into $\sum_z a_{iz}b_{zj}$ is $2MN$ for multiplying $s_{a_i}$, $s_{b_i}$, and $\sum_z a_{iz}'b_{zj}'$, $MN+MZ$ for $m_{b_j}s_{a_i}\sum_z a_{iz}'$, $MN+NZ$ for $m_{a_i}s_{b_j}\sum_z b_{zj}'$, $2MN$ for $Zm_{a_i}m_{b_j}$, and $3MN$ for adding all terms, totaling $9MN+MZ+NZ$.

%which can be accelerated if the GPU supports INT4 or INT8 matrix computation


For Fig.~\ref{fig:hoq_matmul_2}, $AB$ can be viewed as $[A_1,A_2][B_1,B_2]^T$, which equals to $A_1B_1^T + A_2B_2^T$, where $A_1$, $A_2$, $B_1$, and $B_2$ are blocks, and each of them has multiple partitions. The multiplications $A_1B_1^T$ and $A_2B_2^T$ are performed using Eq.~\eqref{eq:hoq_matmul}, separately, followed by a summation to produce the result of $AB$.


%\noindent\textbf{Increase accuracy.}
%Fig.~\ref{fig:hoq_matmul_2} illustrates dividing $A$ and $B$ along the inner dimension into groups of size $Z_1$ and $Z_2$.
%\red{Elements in a half row of $A$ or in a half column of $B$ form a group.}
%\sh{\DONE give more details, what form a group %now}. %To apply Eq.~\eqref{eq:hoq_matmul} for homomorphic quantization in this case,



\subsection{Homomorphic Quantization in Disaggregated LLM
Inference}\label{sec:hoqkv_for_llm}

% \sh{\red{we do not need it because we have fig overview now.} you forgot to add the pseudocode I asked. Also, the following descrption should be based on the workflow. What the prefill instance does, what the decode instance does}


% Then, the quantized matrix multiplications must be performed separately for the submatrices belonging to the same region (i.e., groups in the same color in Fig.~\ref{fig:hoq_matmul_2}),
% \sh{\red{no, not the same}is region also group, just use one term}
% and the dequantized results from all different regions are summed to obtain the final result\sh{I do not quite understand these 2 steps. how and why "separately", why "sum". you need to give Equs.}. In the decode iteration, since the shape of the result for each region is $1\times d_h$, the overhead introduced by the addition operation is negligible compared to the computational cost of matrix multiplications.


% \noindent\textbf{Reduce dequantization overhead.}
We provide a detailed explanation of how our homomorphic quantization in Eq.~\eqref{eq:hoq_matmul} is applied to the self-attention in disaggregated LLM inference in Fig.~\ref{fig:overview}. Fig.~\ref{fig:hoq_self_attn} shows an example.
% \sys applies Eq.~\eqref{eq:hoq_matmul} to the two matrix multiplications, $Q^h(K^h)^T$ and $P^hV^h$, in the self-attention computation in Eq.~\eqref{eq:attention}.
The inner dimension of $Q$ and $K$ is the head dimension of size $d_h$, and we divide this dimension into two partitions, so that $Q$ and $K$ both have two blocks, determined by the partition size $\Pi$. $\Pi$ must be set as a multiple of 16 to ensure efficient execution of matrix operations on the underlying GPU hardware.
%\red{The number of blocks in Fig.~\ref{fig:hoq_self_attn} is an example, and the actual number of blocks is identified by the partition size we use.}
% \sh{\DONE describe how to determine the number of blocks or the partition size here and below in practice}
$Q$ and $K$ are quantized for homomorphic quantization to output the attention score $S$, which will be processed to attention probability $P$ via $softmax$ in Eq.~\eqref{eq:softmax}.
The inner dimension of $P$ and $V$ is the sequence dimension (denoted by $L_{KV}$) rather than the head dimension. We divide this sequence dimension into three parts so that $P$ and $V$ both have three blocks, which is determined by $\Pi$.
$P$ and $V$ are quantized for homomorphic quantization to output $O$ for token generation.
During the prefill stage, the sequence lengths of $Q$ ($L_{Q}$) and $KV$ ($L_{KV}$) in Fig.~\ref{fig:hoq_self_attn} are identical, equaling to the number of prompt tokens. The prefill instance executes Eq.~\eqref{eq:attention}, during which $Q^h(K^h)^T$ and $P^hV^h$ are calculated using Eq.~\eqref{eq:hoq_matmul}. The prefill instance sends the first output token, the quantized $K'$ and $V'$, the minimum values $m$, and the maximum values $s$ to the decode instance. During the decode stage, $L_Q=1$ for the single input token. The decode instance conduct the same process as the prefill instance. After a token is generated in an iteration, it appends the token's $K'$ and $V'$ to the previous $K'$ and $V'$ along the sequence dimension $L_{KV}$ as shown in Fig.~\ref{fig:hoq_self_attn} and repeats the same process to generate the next token.



%Step \circled{9} in Fig.~\ref{fig:overview} is performed by Thus, $L_{KV}$ equals the number of previous tokens plus one.



\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\columnwidth]{fig/hoq_self_attn.pdf}
    \vspace{-0.1in}
    \caption{Illustration of partitioning in self-attention.}
    \label{fig:hoq_self_attn}
    \vspace{-0in}
\end{figure}


% Every $\Pi$ elements in each row of $Q$ or each column of $K$ form a group, sharing a minimum value $m$ and a scale $s$ for quantization.
% Multiplying $Q$ and $K$ using Eq.~\eqref{eq:hoq_matmul} produces the attention score $S$, which is then transformed by \textit{softmax} in Eq.~\eqref{eq:softmax} to the probability $P$. The second matrix multiplication is between $P$ and $V$, and their inner dimension is the sequence dimension of size $L_{KV}$ rather than the head dimension. We divide this sequence dimension into smaller quantization groups of size $gs$, and every $gs$ elements in each row of $P$ or each column of $V$ form a quantization group.
% $P$ and $V$ are multiplied using Eq.~\eqref{eq:hoq_matmul} to produce the self-attention output.


% \noindent\textbf{The overhead of approximation in homomorphic quantization during decode.}



\noindent\textbf{Summation elimination.}
We first analyze the overhead of approximation in homomorphic quantization during the decode stage and then explain how we reduce this overhead.
As mentioned, in each decode iteration, matrices $Q$ and $K$ have $M=L_Q=1$, $Z=d_h$, and $N=L_{KV}$; matrices $P$ and $V$ have $M=L_Q=1$, $Z=L_{KV}$, and $N=d_h$.
%Approximating the quantized matrix multiplication output in
The approximation in Eq.~\eqref{eq:hoq_matmul} has the computational cost of $9MN+MZ+NZ$, as explained in \cref{sec:hoq_mm}, which means the cost of the approximation is $10(d_h+L_{KV})+2d_hL_{KV}$ in each decode iteration.
If we do not use our homomorphic quantization, and we dequantize KV using the dequantization operation $sx'+m$ in every decode iteration, the cost for dequantizing KV is $2d_hL_{KV}$ for $K$ and $2d_hL_{KV}$ for $V$, totaling $4d_hL_{KV}$. Our approximation overhead is
$2d_hL_{KV}-10(d_h+L_{KV})$
less than the overhead for dequantizing KV.
% \sh{\DONE you need to say: Our approximation overhead is LESS than....  \red{I think we do not need computation because the computational complexity is the same. They are matrix multiplications with the same size.} Is it possible for you to get the overhead for their computation and dequantizing, and compare the overheads for getting appromations -- this is comparable.}
In a decode iteration, since the summation term $\sum_z b_{zj}'$ for approximation in Eq.~\eqref{eq:hoq_matmul} has a cost of $NZ$, it leads to a cost of $d_hL_{KV}$ for summing the elements in quantized $K$ and a cost of $d_hL_{KV}$ for summing the elements in quantized $V$, totaling $2d_hL_{KV}$, in each decode iteration.
To reduce the summation computation, we store the sum $\sum_z b_{zj}'$ for $K$ and $V$ during decode and reuse them every iteration to avoid the recomputation cost.
For a partition size of $\Pi$ with $b$-bit integer quantization, the integer sum $\sum_z b_{zj}'$ requires at most $b+\lceil \log_2{\Pi} \rceil$ bits for storage. For example, for $\Pi=64$ with 2-bit quantization, each partition needs at most eight bits to store a sum value. Therefore, we use $b+\lceil \log_2{\Pi} \rceil$ bits to store the sum value.
This only needs a little extra memory, up to $\sim$2.7\% of the GPU memory capacity~(\cref{sec:ablation}).
%\red{Since we remove the summation cost of $2d_hL_{KV}$ from the approximation cost of $10(d_h+L_{KV})+2d_hL_{KV}$,
Then, the final approximation cost is only $10(d_h+L_{KV})$ in each decode iteration.
The head dimension size $d_h$ is typically 128. Thus, the KV dequantization cost $4d_hL_{KV}>10(d_h+L_{KV})$ when the sequence length $L_{KV}>2.5$, and $4d_hL_{KV}$ exceeds $10(d_h+L_{KV})$ by an order of magnitude when the sequence length $L_{KV}>30$. Therefore, when $L_{KV}>30$, the approximation cost in Eq.~\eqref{eq:hoq_matmul} can be significantly reduced compared to the KV dequantization overhead in each decode iteration. The longer the sequence, the greater the reduction in the approximation cost.


% where $2d_hL$ is derived  from the term $m_{a_i}s_{b_j}\sum_z q_{b_j}$ for the two matrix multiplications in self-attention.

% The summation term $\sum_z b_{zj}'$ is

% During the decode stage, $K$ and $V$ are composed of those from the new input token along with those from the previous tokens. In each decode iteration, the summation operation $\sum_z q_{b_j}$ in Eq.~\eqref{eq:hoq_matmul} is performed on $K$ and $V$. Since $K$ and $V$ remain unchanged across decode iterations, the summation $\sum_z q_{b_j}$ can be stored for reuse\sh{combine with the introduction of this store method in the previous section}, thereby reducing computational overhead.
% For a partition size of $gs$ with $b$-bit integer quantization, the integer sum $\sum_z q_{b_j}$ requires at most $b+\lceil \log_2{gs} \rceil$ bits for storage, and we use this number of bits to store the sum in the decode stage. For example, for $gs=64$ with 2-bit quantization, each partition\sh{why this sum is for each partition?} needs at most eight bits to store a sum value.



% \red{Rewrote the paragraph above based on the following comments. The following paragraph will be deleted.}
% $M=1$, $Z=d_h$, and $N=L_{KV}$ for $Q^h(K^h)^T$; $M=1$, $Z=L$, and $N=d_h$ for $P^hV^h$.
% Therefore, when using Eq.~\eqref{eq:hoq_matmul} for self-attention computation in each decode iteration, the total computational cost for dequantizing the quantized matrix multiplication output is $10(d_h+L)+2d_hL$, where $2d_hL$ is derived \sh{how to derive it?} from the term $m_{a_i}s_{b_j}\sum_z q_{b_j}$ for the two matrix multiplications in self-attention.
% We store $\sum_z q_{b_j}$\sh{you need to give reason that this value is used in each iteration} during decode at the cost of a little extra memory and reuse it every decode iteration to eliminate the computational cost\sh{what is the operation?} of $2d_hL$. The final dequantization overhead with Eq.~\eqref{eq:hoq_matmul} is $10(d_h+L)$ \sh{with this sore or not?}.
% Without Eq.~\eqref{eq:hoq_matmul}\sh{change it as we discussed. You should first give the time complexity of dequantization with and without our quantization, then say to further reduce it, we do the store, then it can be further reduced to ...}, the cost for dequantizing KV using $sq+m$ is $4d_hL$. Since $d_h$ is typically 128, $4d_hL>10(d_h+L)$ when $L>2.5$, and $4d_hL$ exceeds $10(d_h+L)$ by an order of magnitude when \sh{add nmae of L here} $L>30$. Therefore, when $L>30$, Eq.~\eqref{eq:hoq_matmul} can significantly reduce the overhead caused by dequantizing KV in each decode iteration. The longer the sequence, the greater the reduction in dequantization overhead.\sh{The following is only for your reference. I am modifying the main section based on our meeting.} \sh{it is not easy to understand this section if a person does not have the background. My impression is it focuses on analyzing the time complexisity. The key is to design a Homomorphic Quantization, so the result on the quantized data is close to the result on the original data. You need to first give high-level of this approach. Then, explain step by step. Next, analyze the time comprexity. Also, how (3) is used for QK and hV is not clear. You need to explain it. Basically, how the prefill instance do the quantization and dequantization, and when the decode instance does (1) on quantized data, why the result is close to the result of the original data. When you say "Without Eq(3)", what does not mean? It means without using our  homomorphic quantization. I feel this section presents some theoretical foundation, but how it is adoppted to the system, and the system operation algorithm is not clear.}





\noindent\textbf{Requantization elimination for the last block of $V$.} After each decode iteration, %a new token along with its KV values are generated.
the new token's KV values are appended to all prior tokens' KV values, as shown in Fig.~\ref{fig:hoq_self_attn}.
Since all the elements of a partition in $K$ are arranged along the fixed head dimension,
all the elements of the new token's $K$ form one or more partitions by themselves. Hence, the $[min,max]$ of the previous $K$ partitions won't be changed. In contrast, all the elements of a partition in $V$ are arranged along the sequence dimension, which is incremented by 1 after each iteration. It means the elements of the new token's $V$ will be distributed to the previous partitions.
%does not have a complete partition and thus cannot be directly quantized.
If the number of tokens in the last block of $V$ (as shown in Fig.~\ref{fig:hoq_self_attn}) is less than the partition size $\Pi$, each element of the new token's $V$ on column $j$ is added to the existing partition on column $j$. However, the element of the new token's $V$ on column $j$ may fall outside of the previous $[min_j, max_j]$ range, leading to the necessity of updating the range and
scale $s_j$. Then, all other tokens' values on column $j$ of the last block of $V$ must be requantized. Fig.~\ref{fig:v_last_group} illustrates an example of this. When the $V$ of the new token $t+2$ is added to the last block of $V$, its value on the second column, -2.9, falls outside the previous $[\min, \max]$ range [-2.1,1.7] of that column. Consequently, the $min$ needs to be updated to -2.9, and all values on that column (-2.1 and 1.7) must be requantized based on the update $[\min, \max]$.


\begin{wrapfigure}[11]{c}{0.25\textwidth}\vspace{-0.15in}
  \centering
  \includegraphics[width=0.25\textwidth]{fig/v_last_group.pdf}
  \vspace{-0.35in}
  \caption{An example of requantization in the last block of $V$.}
  \label{fig:v_last_group}
\end{wrapfigure}
\setlength{\columnsep}{8pt}
To do this, we could first dequantize old quantized values using the old $min_j$ and $s_j$, quantize them and the new value by the updated $min_j$ and $s_j$, and then perform the quantized matrix multiplication for the last block of $P$ and the last block of $V$.
However, this requantization process not only increases the quantization error but also introduces extra overhead.
To address this, we do not store the quantized values of the last group of $V$ when its number of tokens does not reach the partition size $\Pi$. Instead, we store the original FP16 values for the last group of $V$ in a cache separate from the quantized KV cache, which only occupies up to 0.51\% of the GPU memory capacity~(\cref{sec:ablation}). The matrix multiplication for the last block of $P$ and the last block of $V$ are performed in FP16 format without quantization.
When the number of tokens in the last block of $V$ reaches $\Pi$, it is quantized and moved to the quantized KV cache.
As the matrix multiplication in FP16 is restricted to the last block of $V$, the associated computation time with FP16 does not scale with sequence length.
\setlength{\columnsep}{\defaultcolumnsep}





% \sh{go over the following with me in meeting}
% We compare the overhead of performing matrix multiplication\sh{what is it? why it needs to compare with requantization?} and requantization for the last block of $V$ in a decode iteration. Given a block of $V$ with the shape of $\Pi\times d_h$, the corresponding $P$ to be multiplied with it has the shape of $1\times \Pi$. The computational cost of $PV$ is $2\Pi d_h$. The requantization cost for $V$ under the worst case is $2\Pi d_h$ for dequantization and $2\Pi d_h$ for quantization, totaling $4\Pi d_h$.
% Therefore, the computational overhead of matrix multiplication and requantization is on the same order of magnitude. Instead of introducing an additional $4\Pi d_h$ requantization overhead, it will be efficient to directly perform matrix multiplication using FP16 for the last block of $V$ to avoid increasing quantization error.
% When the number of tokens in the last block of $V$ reaches $|GS|$ as the decode stage proceeds, the last block of $V$ is quantized, and their FP16 data is discarded, followed by the creation of a new block of $V$.





% To reduce this requantization overhead, we need to increase the probability of the new token's $V$ values falling within the $min$ and $max$ range.
% Prior research~\cite{kvquant} has demonstrated that, apart from occasional outliers, the values of $V$ typically remain stable within a fixed range. To reduce the frequency of requantization, an initial quantization minimum value $\delta_{min}$ and maximum value $\delta_{max}$ are set for the last group of $V$. Specifically, $\delta_{min}$ and $\delta_{max}$ are initialized to correspond to the 1/3 and 2/3 \sh{use two notations here, and then say we set them to 1/3 and 2/3 empirically that leds to negligible accuracy change} quantiles of the typical range of $V$ values, excluding outliers, \red{which is approximately the maximum range we found that has negligible accuracy change.}
% If a new token's value exceeds the range of [$\delta_{min}$, $\delta_{max}$], $\delta_{min}$ or $\delta_{max}$ is adjusted accordingly, followed by an update of $m$ and $s$ to requantize values.\sh{is the previous sentence duplicated with the above, if yes, remove it}







\section{Implementation}\label{sec:implementation}

\noindent\textbf{Attention backend.}
We integrated \sys with a widely adopted memory-efficient attention backend, FlashAttention-2~\cite{flashattn2}, by using OpenAI Triton~\cite{triton} and a Triton-based implementation of FlashAttention-2~\cite{triton-flashattn}. We built our system on top of vLLM~\cite{vllm2023kwon} using the modified FlashAttention-2.
Since Triton currently supports a minimum precision of INT8 for computation, we first convert the format of the quantized data from 2-bit into INT8 before performing matrix multiplication on the quantized data. This operation is performed in local GPU memory instead of global GPU memory,
%\red{the highest level of the GPU memory hierarchy},
%\sh{what is this?\DONE }
to reduce the overhead for accessing data.
% We referred to the Triton-based implementation of FlashAttention-2 from and made modifications based on it.

\noindent\textbf{Kernel fusion.} We implemented two kernels using Triton: \textit{attn\_prefill}, used during the prefill stage, and \textit{attn\_decode}, used during the decode stage. To reduce overhead, \textit{attn\_prefill} fuses the generation of QKV, QKV quantization, and self-attention with homomorphic quantization into a single kernel. In addition to these three steps, \textit{attn\_decode} integrates the process of concatenating the new token's quantized KV data with the previous tokens' KV data into the kernel to further reduce overhead. In decode, \textit{attn\_decode} separates the last block of $V$ from the quantized blocks into a buffer to enable the matrix multiplication with the original FP16 data.

\noindent\textbf{Data management.} To store the quantized KV data, along with their $m$, $s$, and sum value $\sum_z b_{zj}'$, we modified the KV cache structure of vLLM. Values $m$ and $s$ are stored in FP16.
As mentioned in~\cref{sec:hoqkv_for_llm}, we need $b+\lceil \log_2{\Pi} \rceil$ bits to store the sum $\sum_z b_{zj}'$ for a quantization partition with $b$-bit quantization. However, this may lead to memory alignment issues for a certain combination of $b$ and $|GS|$. For example, we need 9 bits to store a sum value for 2-bit quantization with a partition size of 128.
The 9-bit value can not be stored in memory at addresses that align with the GPU's natural boundaries. Therefore, we use INT16 to store a sum for this case to address the issue with memory alignment.
The memory required by the INT16 sum values only accounts for approximately 5\% of the quantized KV data.
% For a group size of $\Pi$ with $b$-bit integer quantization, the integer sum $\sum_z b_{zj}'$ requires at most $b+\lceil \log_2{\Pi} \rceil$ bits for storage. For example, for $\Pi=64$ with 2-bit quantization, each group needs at most eight bits to store a sum value\sh{should you move the analysis to the main section, and refer to there?}. Therefore, we use $b+\lceil \log_2{\Pi} \rceil$ bits to store the sum value.
The transmission of KV data between prefill instances and decode instances is implemented using NCCL~\cite{nccl}.
% Since $Q$ and $P$ are quantized on the fly and are discarded after matrix multiplication on INT8 data, we use 8-bit quantization for $Q$ and $P$ to reduce accuracy loss.

\DEL{
\noindent\textbf{Differences from TurboAttention.}
Recent work TurboAttention~\cite{turboattention} on arXiv, also performs attention computation directly on quantized KV data to accelerate attention.
However, it adopts symmetric quantization to eliminate the approximation step in Eq.~\eqref{eq:hoq_matmul}, which in turn introduces higher quantization error~\cite{turboattention}.
In contrast, our method utilizes stochastic quantization based on asymmetric quantization, resulting in lower quantization error. We store the sum value $\sum_z b_{zj}'$ for KV with minimal memory overhead, effectively reducing the approximation overhead.
% While TurboAttention has an approximation step to convert quantized results into approximations of the real results, it does not store the summation term $\sum_z b_{zj}'$ for KV that is computed every decode iteration. As a result, it incurs recomputation overhead in the decode stage.
Moreover, in each decode iteration, TurboAttention quantizes the last block of $V$ using INT8 before performing the quantized matrix multiplication, which introduces requantization overhead and accumulates quantization error as decoding progresses.
In contrast, \sys stores the original FP16 data for the last block of $V$ with minimal memory overhead, thereby reducing quantization error and eliminating requantization overhead.
}

% \sh{I removed the paragraph for TurboAttention. Adding it to the related work is enough. If you have the claim ours is better in accuracy and etc, people would epect to see your experimental results to verify your claim. if you could add cetain experimental results, it will be much better.}

\section{Performance Evaluation}

We evaluated \sys and present results in this section. The experimental setup is shown in~\cref{sec:exp_setup}. The comparison methods are the disaggregated LLM inference baseline, CacheGen and KVQaunt. %in~\cref{sec:motivation}.
% Since the GPUs in the evaluation do not support FP8 computation, we conducted a simulation of FP8 for disaggregated LLM inference on a real testbed using the method described in~\cref{sec:fp8_for_kv}.
\sys %uses 2-bit quantization to achieve a similar KV compression rate as CacheGen and KVQuant and
uses a partition size $\Pi$=64, achieving 0.16\%-0.78\% higher accuracy on average compared to CacheGen and KVQuant across all datasets and models.


%that leads to a 0.16\%-0.78\% higher accuracy than CacheGen and KVQuant on average with all datasets and models.



\subsection{Experiment Settings}\label{sec:exp_setup}
% \minlan{One idea is to put experiment settings to evaluation. and just simply refer to evaluation with a few sentences here.}
\noindent\textbf{Testbed.}
The AWS GPU instances used in this paper are listed in Table~\ref{tab:gpu_instances}. Unless otherwise indicated, we used two p4de.24xlarge for decode~\cite{distserve, splitwise}; ten g5.12xlarge, sixteen p3.8xlarge, sixteen g4dn.12xlarge, ten g6.12xlarge, or two p4de.24xlarge for prefill so that prefill instances and decode instances have roughly similar capacities, avoiding underutilizing decode instances~\cite{splitwise}. Since existing disaggregated LLM inference systems such as DistServe~\cite{distserve} and SplitWise~\cite{splitwise} do not support Ethernet data transmission, we modified their code to enable it and integrated them onto vLLM~\cite{vllm2023kwon}
as a baseline.
The RPS was set to the maximum processing capacity, following a Poisson distribution as in~\cite{distserve}. %without increasing queuing time \red{compared to the end-to-end time}.
%\red{Requests are sent in the specific RPS based on a Poisson distribution as in~\cite{distserve}.}
The prefill and decode requests were assigned to the respective prefill and decode instances with the shortest updated queue length~\cite{splitwise}, defined by the number of queuing tokens.\looseness=-1
% \red{The GPU memory usage recorded in the experiment is the proportion of GPU memory capacity occupied by model parameters, KV data, and activations.}
% \sh{how did you record, what's the unit: tokens, bytes? usage means allocated or occupied?.}

\begin{table}[h]
\centering
\begin{adjustbox}{max width=\columnwidth}
\begin{tabular}{ |c||c|c|c|c|c|  }
 \hline
 Name & GPUs & GPU memory & Bandwidth & vCPUs & Memory\\
 \hline
 g5.12xlarge & 4 A10G & 96 GiB & 40 Gbps & 48 & 192 GiB \\
 \hline
 p3.8xlarge & 4 V100 & 64 GiB & 10 Gbps & 32 & 244 GiB \\
 \hline
 g4dn.12xlarge & 4 T4 & 64 GiB & 50 Gbps & 48 & 192 GiB \\
 \hline
 g6.12xlarge & 4 L4 & 96 GiB & 40 Gbps & 48 & 192 GiB \\
 \hline
 p4de.24xlarge & 8 A100 & 640 GiB & 400 Gbps & 96 & 1152 GiB \\
 \hline
\end{tabular}
\end{adjustbox}
 \vspace{-0in}
\caption{GPU instances.}\label{tab:gpu_instances}
%\vspace{-0.25in}
\end{table}


\begin{table}[h]\vspace{-0.1in}
\centering
\small
\begin{adjustbox}{max width=\columnwidth}
\begin{tabular}{|c||c|c|c|}
\hline
Model & A10G, L4 & V100, T4 & A100 \\
\hline
Mistral-v0.3 7B (M) & TP=4, no PP & TP=4, no PP & no TP, no PP \\
\hline
Phi-3 14B (P) & TP=2, PP=2 & TP=2, PP=2 & no TP, no PP \\
\hline
Yi 34B (Y) & TP=4, PP=2 & TP=4, PP=2 & TP=4, no PP \\
\hline
Llama-3.1 70B (L) & TP=4, PP=2 & TP=4, PP=4 & TP=4, no PP \\
\hline
Falcon 180B (F) & TP=4, PP=5 & TP=4, PP=8 & TP=4, PP=2 \\
\hline
\end{tabular}
\end{adjustbox}
\vspace{-0in}
\caption{TP and PP degrees.}
\vspace{-0in}
\label{tab:tp_pp_size}
\end{table}


\begin{table}[h]\vspace{-0in}
\centering
\small
% \resizebox{\columnwidth}{!}{%
\begin{tabular}{l|lll|lll|}
\hline
\multicolumn{1}{|l|}{Dataset} &
  \multicolumn{3}{l|}{Input length} &
  \multicolumn{3}{l|}{Output length} \\
  \cline{1-7}
  \multicolumn{1}{|l|}{} &
  \multicolumn{1}{l|}{avg} &
  \multicolumn{1}{l|}{min} &
  max &
  \multicolumn{1}{l|}{avg} &
  \multicolumn{1}{l|}{min} &
  max \\ \hline
\multicolumn{1}{|l|}{IMDb classification~\cite{imdb}}     & \multicolumn{1}{l|}{315}  & \multicolumn{1}{l|}{106}  & 821 & \multicolumn{1}{l|}{37}  & \multicolumn{1}{l|}{16} & 87  \\ \hline
\multicolumn{1}{|l|}{arXiv summarization~\cite{arxiv-summarization}}   & \multicolumn{1}{l|}{6.3K}  & \multicolumn{1}{l|}{1.6K} & 14.1K  & \multicolumn{1}{l|}{243} & \multicolumn{1}{l|}{29} & 464   \\ \hline
\multicolumn{1}{|l|}{Cocktail for IR~\cite{cocktailforir}}   & \multicolumn{1}{l|}{16.2K}  & \multicolumn{1}{l|}{9.4K} & 28.8K  & \multicolumn{1}{l|}{159} & \multicolumn{1}{l|}{44} & 246   \\ \hline
\multicolumn{1}{|l|}{HumanEval~\cite{humaneval}}   & \multicolumn{1}{l|}{204}  & \multicolumn{1}{l|}{75} & 697  & \multicolumn{1}{l|}{139} & \multicolumn{1}{l|}{11} & 552   \\ \hline
\end{tabular}%
% }
\vspace{-0in}
\caption{Dataset properties.}
%\vspace{-0.2in}
\label{tab:dataset}
\end{table}
\vspace{-0in}

\noindent\textbf{Models and datasets.}
We used a range of state-of-the-art models in the paper: Mistral AI Mistral-v0.3 7B~\cite{mistral-v0.3}, Microsoft Phi-3 14B~\cite{phi-3}, 01-ai Yi 34B~\cite{yi-model}, Meta Llama-3.1 70B~\cite{llama3.1}, and TII Falcon 180B~\cite{falcon}. We use the initial letters M, P, Y, L, and F to represent each model, respectively, for the rest of the paper. Table~\ref{tab:tp_pp_size} shows the Tensor Parallelism (TP) and Pipeline Parallelism (PP) degree of each model in different GPU instances, following practical configurations to ensure sufficient GPU memory to handle requests~\cite{distserve, splitwise}.
The datasets we use are listed in Table~\ref{tab:dataset}.
% \sh{the following dataset explanation can be removed if need space}
IMDb includes 27 genres of movies, TV shows, etc., collected on the Internet. It is operated by IMDb.com, Inc., a subsidiary of Amazon. The arXiv summarization has a collection of scientific publications and their summaries on arXiv.org~\cite{arxiv}. Information Retrieval (IR) is the process of retrieving relevant content from vast amounts of information based on a user query. Cocktail is a benchmark for IR, including 8 different IR tasks such as question answering, fact checking, etc. HumanEval evaluates the performance of code completion, including 164 programming problems. We use \textit{ROUGE-1}~\cite{rouge-score} and \textit{Edit Similarity (normalized Levenshtein distance)}~\cite{zhang2024hierarchicalcontextpruningoptimizing, string-similarity} as the accuracy metric for arXiv summarization and HumanEval, respectively. Unless otherwise specified, we default to testing with the Llama-3.1 70B model and the Cocktail dataset with long sequences on A10G prefill instances, which have the same architecture as the A100.




% \begin{table}[h]\vspace{-0.05in}
% \centering
% \begin{adjustbox}{max width=\columnwidth}
% \begin{tabular}{|c|c|c|c|c|c|c|}
% \hline
% & & Baseline & CacheGen & KVQuant & \sys $gs$=128 & \sys $gs$=64 \\
% \hline
% \multirow{5}{*}{IMDb} & M & 84.81\% & 83.04\% & 82.97\% & 82.71\% & 83.46\% \\
% \cline{2-7}
% & P & 87.84\% & 86.15\% & 86.05\% & 86.07\% & 86.64\% \\
% \cline{2-7}
% & Y & 93.87\% & 91.79\% & 91.54\% & 91.35\% & 92.31\% \\
% \cline{2-7}
% & L & 95.73\% & 93.85\% & 93.89\% & 93.76\% & 94.17\% \\
% \cline{2-7}
% & F & 85.63\% & 83.94\% & 84.12\% & 84.03\% & 84.72\% \\
% \hline
% \multirow{5}{*}{arXiv} & M & 79.40\% & 77.87\% & 77.83\% & 77.72\% & 78.26\% \\
% \cline{2-7}
% & P & 86.35\% & 84.62\% & 84.65\% & 84.49\% & 85.08\% \\
% \cline{2-7}
% & Y & 87.75\% & 86.02\% & 86.11\% & 86.15\% & 86.39\% \\
% \cline{2-7}
% & L & 83.79\% & 82.14\% & 82.08\% & 81.97\% & 82.31\% \\
% \cline{2-7}
% & F & 79.42\% & 77.85\% & 77.76\% & 77.94\% & 78.28\% \\
% \hline
% \multirow{4}{*}{Cocktail} & M & 75.18\% & 73.74\% & 73.72\% & 73.81\% & 74.42\% \\
% \cline{2-7}
% & P & 83.92\% & 82.24\% & 82.26\% & 82.19\% & 82.66\% \\
% \cline{2-7}
% & Y & 85.25\% & 83.51\% & 83.42\% & 82.86\% & 84.04\% \\
% \cline{2-7}
% & L & 86.39\% & 84.71\% & 84.68\% & 84.65\% & 84.87\% \\
% \hline
% \multirow{5}{*}{HumanEval} & M & 89.37\% & 87.73\% & 87.64\% & 87.60\% & 88.19\% \\
% \cline{2-7}
% & P & 91.62\% & 89.75\% & 89.65\% & 89.54\% & 90.16\% \\
% \cline{2-7}
% & Y & 90.79\% & 88.94\% & 88.93\% & 88.91\% & 89.63\% \\
% \cline{2-7}
% & L & 92.45\% & 90.49\% & 90.43\% & 89.77\% & 91.04\% \\
% \cline{2-7}
% & F & 85.21\% & 83.47\% & 83.29\% & 83.19\% & 83.82\% \\
% \hline
% \end{tabular}
% \end{adjustbox}
% \vspace{-0in}
% \caption{Accuracy performance.}
% \vspace{-0in}
% \label{tab:accuracy_perf_old}
% \end{table}

\subsection{End-to-End Time Performance}\label{sec:e2e_perf_test}

We tested Llama-3.1 70B on A10 prefill instances across different datasets. Fig.~\ref{fig:e2e_diff_datasets} shows the average JCT across all requests.
For IMDb, HumanEval, arXiv, and Cocktail, \sys reduces the average JCT by 19.2\%, 22.5\%, 36.8\%, and 41.5\%, respectively, compared to CacheGen, and by 21.2\%, 25.1\%, 40.8\%, and 45.1\%, respectively, compared to KVQuant. This is because \sys leverages quantized matrix multiplication, accelerating both prefill and decode stages while eliminating KV dequantization overhead. For IMDb, HumanEval, arXiv, and Cocktail, \sys reduces the average JCT by 38.6\%, 40.1\%, 55.3\%, and 61.6\%, respectively, compared to the baseline. This improvement is due to \sys not only accelerating prefill and decode stages but also reducing KV transmission overhead. The improvement in JCT achieved by \sys for arXiv and Cocktail is higher than IMDb and HumanEval because arXiv and Cocktail have longer sequences.






\begin{figure}[h]
    \centering
    \includegraphics[width=0.99\columnwidth]{fig/exp/e2e_diff_datasets.pdf}
    \vspace{-0.2in}
    \caption{Average JCT across requests for Llama-3.1 70B with varying datasets.}
    \label{fig:e2e_diff_datasets}
    \vspace{-0in}
\end{figure}



We decompose the average JCT in Fig.~\ref{fig:e2e_diff_datasets} into prefill, quantization, KV communication, dequantization/approximation, and decode time, as shown in Fig.~\ref{fig:e2e_decompose}. The approximation time in homomorphic quantization is only for \sys, and the dequantization time is for other methods.
For IMDb, HumanEval, arXiv, and Cocktail, \sys achieves lower prefill times than other methods by 14.6\%-23.7\%, 17.5\%-23.4\%, 34.2\%-37.5\%, and 35.9\%-41.9\%, respectively. This is due to the acceleration of prefill computation via quantized matrix multiplication in Eq.~\eqref{eq:hoq_matmul}.
The longer the sequence, the greater the improvement in prefill time.
The quantization overhead for CacheGen, KVQuant, and \sys accounts for only 1.25\%-2.91\% of JCT, as the quantization of KV data for each token occurs only once during the entire end-to-end process. By compressing KV data to approximately 15\% of its original size, \sys, CacheGen, and KVQuant reduce KV transmission time by 80.6\%-85.4\% compared to the baseline, enabling KV transmission to account for only 1.31\%-5.4\% of JCT.
\sys eliminates the KV dequantization overhead, which accounts for 17.2\%-30.4\% of JCT in CacheGen and KVQuant, replacing it with only 1.53\%-3.18\% overhead for approximation in homomorphic quantization.

For IMDb, HumanEval, arXiv, and Cocktail, \sys reduces the decode time by 11.5\%-12.2\%, 13.7\%-14.5\%, 24.3\%-25.6\%, and 32.1\%-33.7\% compared to CacheGen and KVQuant, due to accelerated computation from quantized matrix multiplication. For short-sequence datasets IMDb and HumanEval, the improvement in decode time is only 11.5\%-14.5\% because the number of tokens in the last block of $V$ has a high proportion in the sequence length, which leads to compromised efficiency with FP16 computation. The improvement in decode time for long-sequence datasets arXiv, and Cocktail is around 20\% higher than IMDb and HumanEval because the proportion of the number of tokens of the last block of $V$ in the sequence length is small, and thus, the quantized matrix multiplication benefits the decode time more.
CacheGen and KVQuant reduce decode time by 16.5\%-38.1\% compared to the baseline because the reduced KV size improves the memory access latency. \sys also benefits from the improvement in memory access latency.
\sys is effective in reducing JCT for different datasets.
% The longer the sequence, the greater the improvement in decode time.


Table~\ref{tab:peak_gpu_mem} lists the peak GPU memory usage on decode instances for different datasets. CacheGen, KVQuant, and \sys can reduce the peak GPU memory usage by 13.9\%-19.6\% for short-sequence datasets IMDb and HumanEval and by 25.0\%-33.6\% for long-sequence datasets.
\sys has 0.6\% and 2.9\% higher peak GPU memory usage because it stores the sum value for the approximation step in homomorphic quantization and stores the FP16 data for the last block of $V$.


\begin{table}[h]
\centering
\begin{adjustbox}{max width=\columnwidth}
\begin{tabular}{ |c||c|c|c|c|  }
 \hline
 & IMDb & arXiv & Cocktail & HumanEval \\
 \hline
 Baseline & 65.3\% & 83.1\% & 93.7\% & 68.9\% \\
 \hline
 CacheGen & 49.6\% & 56.2\% & 61.3\% & 50.8\% \\
 \hline
 KVQuant & 48.5\% & 55.9\% & 60.1\% & 49.3\% \\
 \hline
 \sys & 51.4\% & 58.1\% & 63.0\% & 51.4\% \\
 \hline
\end{tabular}
\end{adjustbox}
 \vspace{-0in}
\caption{Peak GPU memory usage on decode instances with varying datasets.}\label{tab:peak_gpu_mem}
% \vspace{-0.1in}
\end{table}




\begin{figure}[h]
    \centering
    \includegraphics[width=0.99\columnwidth]{fig/exp/e2e_decompose.pdf}
    \vspace{-0.15in}
    \caption{Average JCT decomposition for Llama-3.1 70B with varying datasets.}
    \label{fig:e2e_decompose}
    %\vspace{-0.1in}
\end{figure}



% Fig.~\ref{fig:mem_usage_datasets} shows the peak GPU memory usage on the decode instance for different datasets. CacheGen, KVQuant, and \sys reduce the peak GPU memory usage by 14.5\%-17.8\% compared to the baseline for short-sequence datasets IMDb and HumanEval and by 26.1\%-28.3\% for long-sequence datasets arXiv and Cocktail. CacheGen, KVQuant, and \sys can efficiently reduce the size of KV data, and long sequences gain more benefits from KV quantization.
% \sys incurs 2.2\%-2.7\% higher peak GPU memory usage than CacheGen and KVQuant because it stores the sum values $\sum_z b_{zj}$ in Eq.~\eqref{eq:hoq_matmul} for KV during the decode stage to avoid redundant summation operations.
% \sys is effective in reducing the GPU memory usage for different datasets.
% % \sh{confused. first, explain ohw to measure the peak GPU memory usage. Then, explain the purpose of this experiment. do you want to show higher mem uage is better or lower is better?}


% % To understand the memory utilization of different methods during the decode stage, we tested Llama-3.1 with the Cocktail under various RPS using A10G prefill instances and recorded the peak memory usage of the decode GPU. Fig.~\ref{fig:mem_usage} presents the results.
% % Model parameters occupy 40\% of the GPU memory, with the remaining space for KV data and activations\sh{at the beginning, explain how you set the KV cahe}. As RPS increases from 0.06 to 0.18, the peak GPU memory usage for KVQuant, CacheGen, \sys, and the baseline grows from 50.8\%, 52.2\%, 54.3\%, and 69.1\% to 78.5\%, 80.3\%, 83.9\%, and 88.2\%, respectively\sh{you can directly indicate how many \% is increased for each method}.
% % At an RPS of 0.06, the baseline exhibits 14.8\%-18.3\% higher peak GPU memory usage compared to other methods due to the larger memory footprint of its KV data. As the RPS increases, the GPU begins to batch more \sh{how to batch requests needs to be explained in the background, until the KVC is fully allocated?} decode requests, resulting in a rise in peak GPU memory usage across all methods. Since CacheGen, KVQuant, and \sys can serve more decode requests due to the reduced KV size, the gap in peak memory usage between these methods and the baseline narrows to 4.3\%-9.7\% at RPS=0.18.


% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.99\columnwidth]{fig/exp/mem_usage_datasets.pdf}
%     \vspace{-0.16in}
%     \caption{Peak memory usage on a decode instance for Llama-3.1 with varying datasets.}
%     \label{fig:mem_usage_datasets}
%     \vspace{-0.1in}
% \end{figure}

\begin{table*}[h]\vspace{-0in}
\centering
\begin{adjustbox}{max width=\linewidth}
\begin{tabular}{|c||c|c|c|c|c||c|c|c|c|c||c|c|c|c||c|c|c|c|c|}
\hline
& \multicolumn{5}{|c||}{IMDb} & \multicolumn{5}{|c||}{arXiv} & \multicolumn{4}{|c||}{Cocktail} & \multicolumn{5}{|c|}{HumanEval} \\
\hline
& M & P & Y & L & F & M & P & Y & L & F & M & P & Y & L & M & P & Y & L & F \\
\hline
Baseline & 84.81\% & 87.84\% & 93.87\% & 95.73\% & 85.63\% & 79.40\% & 86.35\% & 87.75\% & 83.79\% & 79.42\% & 75.18\% & 83.92\% & 85.25\% & 86.39\% & 89.37\% & 91.62\% & 90.79\% & 92.45\% & 85.21\% \\
\hline
% FP8 & 84.55\% & 87.61\% & 93.49\% & 95.49\% & 85.24\% & 79.31\% & 86.17\% & 87.50\% & 83.47\% & 79.20\% & 74.88\% & 83.59\% & 84.97\% & 85.96\% & 89.15\% & 91.33\% & 90.55\% & 92.30\% & 84.97\% \\
% \hline
\sys ($\Pi$=32) & 83.79\% & 87.08\% & 92.82\% & 94.73\% & 84.96\% & 78.53\% & 85.51\% & 86.69\% & 82.63\% & 78.75\% & 74.63\% & 83.05\% & 84.39\% & 85.54\% & 88.68\% & 90.50\% & 90.02\% & 91.28\% & 84.26\% \\
\hline
\sys ($\Pi$=64) & 83.46\% & 86.64\% & 92.31\% & 94.17\% & 84.72\% & 78.26\% & 85.08\% & 86.39\% & 82.31\% & 78.28\% & 74.42\% & 82.66\% & 84.04\% & 84.87\% & 88.19\% & 90.16\% & 89.63\% & 91.04\% & 83.82\% \\
\hline
CacheGen & 83.04\% & 86.15\% & 91.79\% & 93.85\% & 83.94\% & 77.87\% & 84.62\% & 86.02\% & 82.14\% & 77.85\% & 73.74\% & 82.24\% & 83.51\% & 84.71\% & 87.73\% & 89.75\% & 88.94\% & 90.49\% & 83.47\% \\
\hline
KVQuant & 82.97\% & 86.05\% & 91.54\% & 93.89\% & 84.12\% & 77.83\% & 84.65\% & 86.11\% & 82.08\% & 77.76\% & 73.72\% & 82.26\% & 83.42\% & 84.68\% & 87.64\% & 89.65\% & 88.93\% & 90.43\% & 83.29\% \\
\hline
\sys ($\Pi$=128) & 82.71\% & 86.07\% & 91.35\% & 93.76\% & 84.03\% & 77.72\% & 84.49\% & 86.15\% & 81.97\% & 77.94\% & 73.81\% & 82.19\% & 82.86\% & 84.65\% & 87.60\% & 89.54\% & 88.91\% & 89.77\% & 83.19\% \\
\hline
\end{tabular}
\end{adjustbox}
\vspace{-0in}
\caption{Accuracy performance.
}
\vspace{0.2in}
\label{tab:accuracy_perf}
\end{table*}

We also tested different models with the Cocktail using A10G prefill instances.
% As in~\cref{sec:motivation}, Falcon 180B, due to its 2K context window length limitation, was tested using another long-sequence dataset arXiv, denoted by F-arXiv.
Fig.~\ref{fig:e2e_diff_models} shows the average JCT across all requests.
For M, P, Y, L, and F-arXiv, \sys reduces the average JCT by 42.4\%, 39.1\%, 44.8\%, 41.5\%, and 31.7\%, respectively, compared to CacheGen. \sys reduces the average JCT by 48.3\%, 46.5\%, 50.7\%, 45.1\%, and 37.6\%, respectively, compared to KVQuant. Compared to the baseline, \sys reduces it by 54.6\%, 57.2\%, 58.7\%, 61.6\%, and 53.3\%, respectively. The reasons are the same as those discussed in Fig.~\ref{fig:e2e_diff_datasets}.
% When using the same dataset, the model size \sh{maybe the model size difference is not big enough. model size determines the KVC size and then the effect of the KV data quantization} does not significantly impact the improvement of end-to-end performance.
\sys's improvement on F-arXiv is smaller than that for other models because the sequence length used in F-arXiv is shorter, capped at 2K.
\sys is effective in reducing JCT for different models.


\begin{figure}[h]
    \centering
    \includegraphics[width=0.99\columnwidth]{fig/exp/e2e_diff_models.pdf}
    \vspace{-0.2in}
    \caption{Average JCT across requests for different models with Cocktail or arXiv.}
    \label{fig:e2e_diff_models}
    %\vspace{-0.1in}
\end{figure}


We tested Llama-3.1 70B with the Cocktail on different prefill instances, and Fig.~\ref{fig:e2e_diff_gpus} shows the average JCT across all requests under different prefill
instances. For A10G, V100, T4, L4, and A100, \sys reduces the average JCT compared to CacheGen by 41.5\%, 37.4\%, 43.1\%, 45.3\%, and 48.5\%; compared to KVQuant by 45.1\%, 41.7\%, 46.6\%, 50.5\%, and 52.3\%; and compared to the baseline by 61.6\%, 70.9\%, 62.1\%, 59.3\%, and 60.5\%, respectively.
The improvement of \sys over CacheGen and KVQuant on V100 is the smallest because the V100 tensor core does not support INT8 matrix multiplication, making it unable to accelerate prefill computation. However, \sys achieves the highest improvement over the baseline on V100, as V100 has the lowest bandwidth, and \sys significantly improves the KV transmission speed.
\sys is effective in reducing JCT for various prefill instances with various bandwidths and compute capacity.


\begin{figure}[h]
    \centering
    \includegraphics[width=0.99\columnwidth]{fig/exp/e2e_diff_gpus.pdf}
    \vspace{-0.2in}
    \caption{Average JCT across requests for Llama-3.1 70B with Cocktail using varying prefill instances.}
    \label{fig:e2e_diff_gpus}
    %\vspace{-0.1in}
\end{figure}



% \noindent\textbf{Simulation for FP8.}
% We conducted FP8 simulation experiments for Llama-3.1 70B with Cocktail. The results indicate that for A10G, T4, L4, and A100 prefill instances, FP8 has 4-34\% higher average end-to-end time across requests compared to \sys. On V100 prefill instances with lower bandwidth, FP8 has up to 48\% higher average end-to-end time compared to \sys. This is because FP8 still incurs high KV transmission overhead and longer decode time due to higher memory access latency for KV data during the decode stage.

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.99\columnwidth]{fig/exp/tbt.pdf}
%     \vspace{-0.16in}
%     \caption{Average TBT.}
%     \label{fig:tbt}
%     \vspace{-0.1in}
% \end{figure}

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.99\columnwidth]{fig/exp/e2e.pdf}
%     \vspace{-0.16in}
%     \caption{Average end-to-end time.}
%     \label{fig:e2e}
%     \vspace{-0.1in}
% \end{figure}





\subsection{Accuracy Performance}\label{sec:acc_perf}
We measured the accuracy of each method across different datasets and models, as shown in Table~\ref{tab:accuracy_perf}.
\sys uses three partition sizes $\Pi$=128, 64, and 32, denoted by \sys ($\Pi$=128), \sys ($\Pi$=64), and \sys ($\Pi$=32).
Compared to the baseline, \sys ($\Pi$=$32$), \sys ($\Pi$=$64$), CacheGen, KVQuant, and \sys ($\Pi$=$128$) exhibit accuracy losses of 0.55\%-1.17\%, 0.76\%-1.56\%, 1.44\%-2.08\%, 1.46\%-2.33\%, and 1.37\%-2.68\%, respectively.
\sys ($\Pi$=$32$) and \sys ($\Pi$=$64$) achieve higher accuracy than CacheGen and KVQuant due to the fine-grained partition size.
% On average, CacheGen performs slightly better than KVQuant, with differences not exceeding 0.04\%\sh{no point to compare these two methods}.
Meanwhile, \sys ($\Pi$=$128$) has a 0.21\%-1.27\% lower accuracy than \sys ($\Pi$=$128$) due to the larger partition size and has a slightly lower accuracy than KVQuant on average, by no more than 0.12\%.
Across different models and datasets, all methods exhibit similar accuracy loss trends.
Since the partition size of 64 is enough for \sys to have better accuracy than CacheGen and KVQuant, we use $\Pi$=$64$ as the default partition size for \sys in the \mbox{evaluation.}

% \sh{you used M, P, Y L, F to represent models. better to use the full names.}



% \ran{This table might be problematic, as it only shows the accuracy and not the time taken. A reviewer that doesn't spend enough time to understand the paper might simply say we have no improvement over FP8.}
% \zeyu{Due to time limit, I will put FP8/FP6/FP4 discussion in the section after motivation}


\subsection{Ablation Study}\label{sec:ablation}


We conducted ablation study to understand the impact of two of our optimizations on end-to-end performance and accuracy: the summation elimination for Eq.~\eqref{eq:hoq_matmul} and requantization elimination for the last block of $V$. We tested the following variations. \sys/SE represents \sys without \textit{Summation Elimination}, which means it does not store the term $\sum_z b_{zj}$ in Eq.~\eqref{eq:hoq_matmul} and recomputes it every decode iteration. \sys/RQE represents the system without \textit{ReQuantization Elimination} for the last block of $V$,
and instead checks and requantizes the last block of $V$ every decode iteration.
Fig.~\ref{fig:e2e_ablation} shows the average JCT across all requests for \sys, \sys/SE, and \sys/RQE when varying datasets.
\sys/SE has 13.8\%-15.3\% higher average JCT for the short-sequence datasets (IMDb and HumanEval) and has 22.1\%-25.9\% higher average JCT for the long-sequence datasets (arXiv and Cocktail) compared to \sys. Long sequences increase the overhead in recomputing the sum value $\sum_z b_{zj}$ for KV.
\sys/RQE has 17.8\%-21.7\% higher average JCT for the short-sequence datasets and has 0.09\%-1.2\% higher average JCT for the long-sequence datasets compared to \sys. As the sequence length increases, the proportion of tokens in the last block of $V$ compared to the total number of tokens decreases, making the overhead of requantizing the last block of $V$ comparatively smaller.
Table~\ref{tab:acc_ablation} lists the decrease in accuracy for \sys/RQE compared to \sys under different datasets. Since the quantization error caused by requantization of the last block of $V$ is only accumulated during the decode stage, the accuracy decrease is affected by the output length. IMDb has an average output length of 37, while others have an average output length of 139-243. Thus, IMDb has a 0.08\%-0.15\% less decrease in accuracy than other datasets.
SE is effective in reducing the summation overhead, and RQE can avoid accuracy loss without compromising \mbox{the end-to-end~performance.}




\begin{figure}[h]
    \centering
    \includegraphics[width=0.99\columnwidth]{fig/exp/e2e_ablation.pdf}
    \vspace{-0.2in}
    \caption{Average JCT across requests for individual methods with Llama-3.1 70B.}
    \label{fig:e2e_ablation}
    %\vspace{-0.1in}
\end{figure}

\begin{table}[h]\vspace{-0in}
\centering
\begin{adjustbox}{max width=\columnwidth}
\begin{tabular}{|c|c|c|c|}
\hline
IMDb & arXiv & Cocktail & HumanEval \\
\hline
-0.14\% & -0.29\% & -0.22\% & -0.25\%\\
\hline
\end{tabular}
\end{adjustbox}
\vspace{-0in}
\caption{The decrease in accuracy for \sys/RQE compared to \sys.}
\vspace{-0in}
\label{tab:acc_ablation}
\end{table}


For SE, storing the sum values $\sum_z b_{zj}$ in Eq.~\eqref{eq:hoq_matmul} for KV to avoid redundant computation only requires 2.2\%-2.7\% of GPU memory capacity in the experiment.
For RQE, storing the FP16 values for the last block of $V$ only requires 0.24\%-0.51\% of GPU memory capacity.


% For IMDb, HumanEval, arXiv, and Cocktail, \sys/RQR has 24.7\%, 20.8\%, 1.2\%, and 0.09\% higher average end-to-end time than \sys, respectively. As the sequence length increases, the proportion of tokens in the last group of $V$ compared to the total number of tokens decreases, making the overhead of requantizing the last group of V comparatively smaller.
% In these tests the accuracy difference between \sys and \sys/RQR across all models and datasets ranges from -0.02\% to 0.05\%. This demonstrates that RQR reduces the requantization overhead with essentially negligible change in accuracy.








% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.99\columnwidth]{fig/exp/e2e_indiv.pdf}
%     \vspace{-0.16in}
%     \caption{Average end-to-end time for individual methods.}
%     \label{fig:e2e_indiv}
%     \vspace{-0.1in}
% \end{figure}


\subsection{Sensitivity Testing}




% \zeyu{Replaced the previous fig with a table and made the average time clear}

% \MM{\red{DONE with a table} The numbers below don't make sense/clarify.  Do you mean on average over the 4 datasets the percentage difference is?  How can you have one number for 4 datasets?
% I think we've discussed that these numbers are meaningless you need a chart with both overhead and accuracy together  I'd just do a table.
% }
We investigated the impact of \sys's different quantization partition sizes on end-to-end performance for Llama-3.1 70B on A10G prefill instances with different datasets.
Table.~\ref{tab:acc_e2e_group_sizes} lists the increase in accuracy and the percentage increase in the average JCT across all requests for quantization partition sizes $\Pi$=32 and $\Pi$=64 compared to $\Pi$=128. While $\Pi$=32 achieves the highest accuracy with up to 1.53\% increase, it can increase the average JCT by up to 28\%, which means it is a trade-off between accuracy and end-to-end performance requirements.
% \sh{you need to explain these terms first, also put these results in a table for fig}


% For HumanEval, IMDb, arXiv, and Cocktail, $gs$=64 results in 5.1\%, 5.9\%, 8.3\%, and 9.2\% higher average end-to-end time compared to $gs$=128, respectively.
% Similarly, $gs$=32 results in 10.4\%, 13.9\%, 23.4\%, and 27.1\% higher average end-to-end time compared to $gs$=64.
% As indicated in Table.~\ref{tab:accuracy_perf}, $gs$=64 has 0.22\%-1.27\% higher accuracy than $gs$=128.
% Long-sequence datasets arXiv and Cocktail have higher overhead than HumanEval and arXiv. The choice of partition size depends on the trade-off between accuracy and end-to-end performance requirements.


\begin{table}[h]%\vspace{in}
\centering
\begin{adjustbox}{max width=\columnwidth}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
& \multicolumn{2}{|c|}{IMDb} & \multicolumn{2}{|c|}{arXiv} & \multicolumn{2}{|c|}{Cocktail} & \multicolumn{2}{|c|}{HumanEval} \\
\hline
& Acc. & JCT & Acc. & JCT & Acc. & JCT & Acc. & JCT \\
\hline
$\Pi$=32 & 0.92-1.46\% & 17.1\% & 0.53-1.02\% & 25.2\% & 0.81-1.53\% & 28\% & 0.95-1.51\% & 13.8\% \\
\hline
$\Pi$=64 & 0.4-0.96\% & 5.9\% & 0.23-0.59\% & 8.3\% & 0.22-1.18\% & 9.2\% & 0.59-1.27\% & 5.1\% \\
\hline
\end{tabular}
\end{adjustbox}
\vspace{-0in}
\caption{The increase in accuracy and average JCT across requests compared to $\Pi$=128.}
\vspace{-0.1in}
\label{tab:acc_e2e_group_sizes}
\end{table}


% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.99\columnwidth]{fig/exp/e2e_sensitivity.pdf}
%     \vspace{-0.16in}
%     \caption{Average end-to-end time across requests for Llama-3.1 70B with different group sizes.}
%     \label{fig:e2e_sensitivity}
%     \vspace{-0.1in}
% \end{figure}



\subsection{Scalability Testing}

% \sh{Add a fig for E2E with varying arrival time, and a fig for increasing the number of prefill instances and decode instances if you can}

\DEL{We conducted a scalibility testing with Llama-3.1 70B and the Cocktail to demonstrate the ability of \sys to mitigate network bottlenecks when the ratio $p$ of the number of prefill instances to the number of decode instances increases from 1 to 8.}

%One model replica was used for decode, while one to eight model replicas were used for prefill.

We then test the ability of \sys to mitigate network bottlenecks using Llama-3.1 70B and Cocktail. We use $p$ to denote the ratio of the number of model replicas for prefill to the number of model replicas for decode.
The decode model ran on an A100 instance, and due to TP=4, only half of the A100 instance's GPU and network resources (i.e., 4 GPUs and a 200Gbps network) were allocated to serve it. The model replicas for prefill ran on A10G instances, and with TP=4 and PP=2, each prefill model required two A10G instances to serve it.
The RPS was set to $0.02p$.
Fig.~\ref{fig:e2e_scale} shows the average JCT across all requests when $p$ increases from 1 to 8. When $p$ increases from 1 to 8, the baseline's average JCT increases by 127\%, whereas CacheGen, KVQuant, and \sys only have an increase of 31-43\%. This demonstrates that CacheGen, KVQuant, and \sys effectively reduce the overhead caused by KV transmission at large scales.

% We evaluated the impact of various methods on the end-to-end performance of Llama-3.1 70B with Cocktail under different scales of prefill model replicas. For the decode instance, we used an instance with four A100 GPUs and a 200Gbps network. A10G instances were used as prefill instances. The TP and PP sizes for the model were the same as~\cref{sec:motivation}, with each prefill model replica requiring two A10G instances. For each additional prefill model replica, the RPS was increased by 0.02.




\begin{figure}[h]
    \centering
    \includegraphics[width=0.99\columnwidth]{fig/exp/e2e_scale.pdf}
    \vspace{-0.19in}
    \caption{Average JCT across requests with varying $p$.}
    \label{fig:e2e_scale}
    \vspace{-0.1in}
\end{figure}


\section{Limitations and Future Work}
Since one of our focuses is on mitigating the KV transmission bottleneck in disaggregated LLM inference, we employ 2-bit quantization to minimize KV transmission overhead. However, due to the low precision of 2-bit quantization, we must use small partition sizes to improve accuracy, which can increase JCT.
To address this, we plan to explore new quantization schemes that can reduce JCT and improve accuracy in disaggregated LLM inference in the future.
Additionally, since Triton only supports INT8 computation and incurs runtime overhead, we intend to implement \sys directly in CUDA to support INT4 computation and reduce execution overhead in out future work.




% \MM{This feels like it should come later, at the middel or end, and then you should give a clearer description of how this work differs fro ours. }
% We note that a recent paper on arXiv introduces TurboAttention~\cite{turboattention}, which employs the quantized execution\sh{what's new or novel and what's the approach in this quantized execution? Does it belong to the same category of FP8 above? Applied scenarios is not something for difference. The difference must be in the approaches themselves.} of attention to accelerate attention. Unlike TurboAttention, \sys focuses on disaggregated LLM inference, aiming to reduce KV transmission and dequantization overhead while simultaneously accelerating computation to enhance end-to-end performance. \sys is applicable to any scenario involving KV transmission, such as sequence-parallelism-based LLM training~\cite{deepspeed-ulysses} and inference systems~\cite{loongserve} for long-context models.



\section{Related Work}
\medskip
\noindent \textbf{Homomorphic Compression.}
The concept of homomorphic compression, where arithmetic operations are performed directly on compressed data, has been explored in the context of gradient aggregation~\cite{thc2024li}, enabling a parameter server to aggregate compressed gradients without decompressing them. However, this method is limited to addition operations and is unsuitable for the matrix multiplications required in the attention computation procedure.

\medskip
\noindent \textbf{Disaggregated LLM inference.} Disaggregated LLM inference.~\cite{distserve, splitwise, mooncake, memserve, strati2024dejavukvcachestreamingfast} has gained popularity as it separates the computation-intensive prefill and memory-intensive decode stages into different instances, improving resource utilization.
Two works~\cite{distserve, splitwise, strati2024dejavukvcachestreamingfast} focus on disaggregated LLM inference systems. Other research~\cite{mooncake, memserve} focuses on KV cache sharing, where KV data generated during the prefill stage is stored and shared across requests to reduce computation time. However, transmitting KV values from the prefill instance to the decode instance can become a bottleneck, a challenge that intensifies with KV data sharing. Our work addresses this issue by introducing homomorphic quantization, eliminating significant KV dequantization overhead.

%these methods do not reduce the size of KV data to lower transmission and memory overhead, nor do they leverage low-precision computation to accelerate attention. Our work addresses these gaps by using homomorphic quantization without introducing the significant overhead caused by KV dequantization.

\medskip
\noindent\textbf{KV Quantization.}
Many methods use quantization to compress KV~\cite{zipcache, kang2024gear, kivi, cachegen, kvquant} by reducing the high-bit representation FP16 to lower-bit representations in order to reduce the size of KV data. %It reduces the number of bits required to store KV.
However, during attention computation, the quantized KV in the KV cache must first be dequantized to recover the original data, introducing significant decompression overhead. \sys, on the other hand, uses homomorphic quantization to avoid the KV dequantization overhead and performs attention computation on low-precision data to accelerate speed.

% Recent work TurboAttention~\cite{turboattention} on arXiv, also performs attention computation directly on quantized KV data; however, it focuses solely on accelerating computation and mitigating KV cache bottlenecks. In contrast, our work not only accelerates computation and mitigates KV cache bottlenecks but also focuses on minimizing KV communication overhead in disaggregated LLM inference by leveraging 2-bit quantization.
% Furthermore, TurboAttention adopts symmetric quantization to eliminate the approximation step in Eq.~\eqref{eq:hoq_matmul}, which in turn introduces higher quantization error~\cite{turboattention}.
% In contrast, our method utilizes stochastic quantization based on asymmetric quantization, resulting in lower quantization error. We store the sum value $\sum_z b_{zj}'$ for KV with minimal memory overhead, effectively reducing the approximation overhead.

Recent work TurboAttention~\cite{turboattention} on arXiv, also performs attention computation directly on quantized KV data; however, it focuses solely on accelerating computation and mitigating KV cache bottlenecks. In contrast, our work not only accelerates computation and mitigates KV cache bottlenecks but also focuses on minimizing KV communication overhead in disaggregated LLM inference by leveraging 2-bit quantization.
Furthermore, TurboAttention chooses symmetric quantization to reduce computation overhead, which has a lower accuracy than asymmetric quantization~\cite{turboattention}. \sys uses asymmetric quantization with summation elimination to increase accuracy. In addition, \sys eliminates the requantization overhead and reduce quantization error for the last block of $V$ during the decode stage.


% Furthermore, as indicated in~\cref{sec:implementation}, TurboAttention uses symmetric quantization that decreases accuracy to eliminate approximation\sh{change "approximation" to "dequantization"?}, while \sys uses asymmetric quantization with summation elimination to increase accuracy. In addition, \sys eliminates the requantization overhead and reduce quantization error for the last block of $V$ during the decode stage.

% Recent work TurboAttention~\cite{turboattention} on arXiv, also performs attention computation directly on quantized KV data; however, it focuses solely on accelerating computation and mitigating KV cache bottlenecks. In contrast, our work not only accelerates computation and mitigates KV cache bottlenecks but also focuses on minimizing KV communication overhead in disaggregated LLM inference by leveraging 2-bit quantization.
% Furthermore, as indicated in~\cref{sec:implementation}, TurboAttention uses symmetric quantization that decreases accuracy to eliminate approximation\sh{change "approximation" to "dequantization"?}, while \sys uses asymmetric quantization with summation elimination to increase accuracy. In addition, \sys eliminates the requantization overhead and reduce quantization error for the last block of $V$ during the decode stage.

% Recent work TurboAttention~\cite{turboattention} on arXiv, also performs attention computation directly on quantized KV data to accelerate attention.
% However, it adopts symmetric quantization to eliminate the approximation step in Eq.~\eqref{eq:hoq_matmul}, which in turn introduces higher quantization error~\cite{turboattention}.
% In contrast, our method utilizes stochastic quantization based on asymmetric quantization, resulting in lower quantization error. We store the sum value $\sum_z b_{zj}'$ for KV with minimal memory overhead, effectively reducing the approximation overhead.
% % While TurboAttention has an approximation step to convert quantized results into approximations of the real results, it does not store the summation term $\sum_z b_{zj}'$ for KV that is computed every decode iteration. As a result, it incurs recomputation overhead in the decode stage.
% Moreover, in each decode iteration, TurboAttention quantizes the last block of $V$ using INT8 before performing the quantized matrix multiplication, which introduces requantization overhead and accumulates quantization error as decoding progresses.
% In contrast, \sys stores the original FP16 data for the last block of $V$ with minimal memory overhead, thereby reducing quantization error and eliminating requantization overhead.

\medskip
\noindent\textbf{KV Eviction.}
Another solution for KV compression is KV eviction~\cite{h2o2023zhang, ge2023model, scissorhands2023liu, pyramidinfer, l2norm-kv, keyformer, dynamic-context-pruning, infinigen, zhang2024efficientsparseattentionneeds, jiang2024minference}, also known as KV pruning, which removes unimportant tokens' KV that have minimal impact on inference results, measured by attention scores.
Eviction-based methods and quantization-based methods are complementary: the former reduces the number of elements in the KV matrix, while the latter lowers the precision of those elements. These two approaches can be combined for enhanced effectiveness. While our work focuses on homomorphic quantization, we plan to explore how KV eviction can address the challenges in future work.
%\sh{think about whether KV eviction can be used to reduce data size in transmission. any problem if we use it? Either it is out of the scope or our future work \red{Do not see challenges here. Probably an implementation idea, but need to explore. Other quantization paper also mentions eviction and quantization are complementary in the dicsussion.}}


\DEL{
\noindent\textbf{TurboAttenion.}
Recent work TurboAttention~\cite{turboattention}, uploaded on arXiv, also performs attention computation directly on quantized KV data; however, it focuses solely on accelerating computation and mitigating KV cache bottlenecks. In contrast, our work not only accelerates computation and mitigates KV cache bottlenecks but also focuses on minimizing KV communication overhead in disaggregated LLM inference by leveraging 2-bit quantization.
Furthermore, as indicated in~\cref{sec:implementation}, TurboAttention uses symmetric quantization that decreases accuracy to eliminate approximation, while \sys uses asymmetric quantization with summation elimination to increase accuracy. \sys can also eliminate the requantization overhead and reduce quantization error for the last block of $V$ during the decode stage.
}

% \sh{can you move it to the end of "KV Quantization", and remove }




\section{Conclusion}
\sys is a novel quantization method for disaggregated LLM inference, which reduces the KV transmission overhead, computation time, and memory access latency for KV without introducing the expensive KV dequantization overhead. We integrate \sys into a widely adopted memory-efficient attention kernel, FlashAttention-2, and build our system on vLLM.
We conduct extensive experiments to demonstrate the efficiency of \sys, where \sys can reduce JCT by up to 70.9\% due to the improvement in communication, computation, memory access latency, and {the elimination of KV dequantization. }
%
This work does not raise any ethical issues.


% reduces JCT by 70.9\% and 52.3\% compared to the disaggregated LLM inference baseline and state-of-the-art KV quantization methods, respectively.
% \sh{\DONE do not use similar sentences as Abstract. Refer to THC paper and see how they write Abstract and Conclusion} %due to the improvement in prefill and decode time and the elimination of KV dequantization overhead.




% We propose \sys, a homomorphic quantization system to reduce KV transmission overhead while eliminating the costly KV dequantization step, accelerate computation, lower memory usage in disaggregated LLM inference.
% Compared to the disaggregated baseline, \sys reduces KV communication time by up to 85.4\%. It also achieves up to 52.3\% reduction in JCT, compared to state-of-the-art KV quantization methods.



\bibliographystyle{ACM-Reference-Format}
\bibliography{reference}

\end{document}
