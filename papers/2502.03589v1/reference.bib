%% LaTeX2e file `main.bib'
%% generated by the `filecontents' environment
%% from source `main' on 2023/07/23.
%%
%-------------------------------------------------------------------------------
@article{Bulatov2023ScalingTT,
  title={Scaling Transformer to 1M tokens and beyond with RMT},
  author={Aydar Bulatov and Yuri Kuratov and Mikhail S. Burtsev},
  journal={ArXiv},
  year={2023},
  volume={abs/2304.11062},
  url={https://api.semanticscholar.org/CorpusID:258291566}
}

@misc{deepspeed-fastgen,
author = {{Microsoft}},
title = {{DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference}},
howpublished = {\url{https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-fastgen}}
}


@misc{humaneval,
      title={Evaluating Large Language Models Trained on Code}, 
      author={Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Ponde de Oliveira Pinto and Jared Kaplan and Harri Edwards and Yuri Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and Dave Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William Hebgen Guss and Alex Nichol and Alex Paino and Nikolas Tezak and Jie Tang and Igor Babuschkin and Suchir Balaji and Shantanu Jain and William Saunders and Christopher Hesse and Andrew N. Carr and Jan Leike and Josh Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},
      year={2021},
      eprint={2107.03374},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2107.03374}, 
}


@article{roformer,
author = {Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
title = {RoFormer: Enhanced transformer with Rotary Position Embedding},
year = {2024},
issue_date = {Feb 2024},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {568},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2023.127063},
doi = {10.1016/j.neucom.2023.127063},
journal = {Neurocomput.},
month = mar,
numpages = {12},
keywords = {Pre-trained language models, Position information encoding, Pre-training, Natural language processing}
}

@misc{imdb,
title = {Genre Classification Dataset IMDb},
howpublished = {\url{https://www.kaggle.com/datasets/hijest/genre-classification-dataset-imdb}},
year={2020},
author={IMDb}
}

@misc{cocktailforir,
      title={Cocktail: A Comprehensive Information Retrieval Benchmark with LLM-Generated Documents Integration}, 
      author={Sunhao Dai and Weihao Liu and Yuqi Zhou and Liang Pang and Rongju Ruan and Gang Wang and Zhenhua Dong and Jun Xu and Ji-Rong Wen},
      year={2024},
      eprint={2405.16546},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2405.16546}, 
}

@misc{arxiv,
title = {arXiv},
howpublished = {\url{https://arxiv.org}},
author = {arXiv},
note = {Accessed: 2025-01-30},
year = {2025}
}

@inproceedings {Agrawal2023SARATHIEL,
author = {Amey Agrawal and Nitin Kedia and Ashish Panwar and Jayashree Mohan and Nipun Kwatra and Bhargav Gulavani and Alexey Tumanov and Ramachandran Ramjee},
title = {Taming {Throughput-Latency} Tradeoff in {LLM} Inference with {Sarathi-Serve}},
booktitle = {18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24)},
year = {2024},
isbn = {978-1-939133-40-3},
address = {Santa Clara, CA},
pages = {117--134},
url = {https://www.usenix.org/conference/osdi24/presentation/agrawal},
publisher = {USENIX Association},
month = jul
}



@inproceedings{katharopoulos2018not,
  title={Not all samples are created equal: Deep learning with importance sampling},
  author={Katharopoulos, Angelos and Fleuret, Fran{\c{c}}ois},
  booktitle={International conference on machine learning},
  pages={2525--2534},
  year={2018},
  organization={PMLR}
}

@misc{OpenAIAPI,
  title = {OpenAI API Documentation},
  howpublished = {https://help.openai.com/en/articles/
4936856-what-are-tokens-and-how-to-count-them}
}

@inproceedings{rouge-score,
  title={Rouge: A package for automatic evaluation of summaries},
  author={Lin, Chin-Yew},
  booktitle={Text summarization branches out},
  pages={74--81},
  year={2004}
}

@misc{infer-without-infer,
      title={Inference without Interference: Disaggregate LLM Inference for Mixed Downstream Workloads}, 
      author={Cunchen Hu and Heyang Huang and Liangliang Xu and Xusheng Chen and Jiang Xu and Shuang Chen and Hao Feng and Chenxi Wang and Sa Wang and Yungang Bao and Ninghui Sun and Yizhou Shan},
      year={2024},
      eprint={2401.11181},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2401.11181}, 
}

@misc{a10g-inference,
  title = {{NVIDIA A10 vs A10G for ML} model inference},
  howpublished = {\url{https://www.baseten.co/blog/nvidia-a10-vs-a10g-for-ml-model-inference/}},
author = {Philip Kiely},
year = {2024}
}


@misc{mooncake,
      title={Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving}, 
      author={Ruoyu Qin and Zheming Li and Weiran He and Mingxing Zhang and Yongwei Wu and Weimin Zheng and Xinran Xu},
      year={2024},
      eprint={2407.00079},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2407.00079}, 
}


@misc{string-similarity,
  title = {String Similarity Metrics – Edit Distance},
  howpublished = {\url{https://www.baeldung.com/cs/string-similarity-edit-distance}},
author={Gang Wu},
year={2024}
}


@inproceedings{loongserve,
author = {Wu, Bingyang and Liu, Shengyu and Zhong, Yinmin and Sun, Peng and Liu, Xuanzhe and Jin, Xin},
title = {LoongServe: Efficiently Serving Long-Context Large Language Models with Elastic Sequence Parallelism},
year = {2024},
isbn = {9798400712517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3694715.3695948},
doi = {10.1145/3694715.3695948},
pages = {640–654},
numpages = {15},
keywords = {inference serving, large language models, elastic sequence parallelism},
location = {Austin, TX, USA},
series = {SOSP '24}
}

@misc{turboattention,
      title={TurboAttention: Efficient Attention Approximation For High Throughputs LLMs}, 
      author={Hao Kang and Srikant Bharadwaj and James Hensman and Tushar Krishna and Victor Ruhle and Saravan Rajmohan},
      year={2024},
      eprint={2412.08585},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2412.08585}, 
}

@online{gemini-1.5,
  title = {Gemini 1.5},
  howpublished = {\url{https://gemini.google.com/app}},
author = {Google},
year={2024}
}


@misc{tencentcloud-a100,
  title = {{Tencent Cloud} - {A100} Instances},
  howpublished = {\url{https://www.tencentcloud.com/document/product/560/19701##GT4}},
author = {Tencent Cloud},
year = {2024}
}

@misc{aws-gpu-instances,
  title = {Recommended {AWS GPU} Instances},
  howpublished = {\url{https://docs.aws.amazon.com/dlami/latest/devguide/gpu.html}},
author = {Amazon Web Services, Inc.},
year={2024}
}



@misc{vllm-fp8,
  title = {{vLLM FP8 for KV} Cache},
  howpublished = {\url{https://docs.vllm.ai/en/v0.5.4/quantization/fp8_e4m3_kvcache.html}},
author = {vLLM Team},
year = {2024}
}


@misc{zhang2024hierarchicalcontextpruningoptimizing,
      title={Hierarchical Context Pruning: Optimizing Real-World Code Completion with Repository-Level Pretrained Code LLMs}, 
      author={Lei Zhang and Yunshui Li and Jiaming Li and Xiaobo Xia and Jiaxi Yang and Run Luo and Minzheng Wang and Longze Chen and Junhao Liu and Min Yang},
      year={2024},
      eprint={2406.18294},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.18294}, 
}



@article{jin2023s,
  title={S$^{3}$: Increasing GPU Utilization during Generative Inference for Higher Throughput},
  author={Jin, Yunho and Wu, Chun-Feng and Brooks, David and Wei, Gu-Yeon},
  journal={Proc. of NeurIPS},
  year={2023}
}

@misc{memserve,
      title={MemServe: Context Caching for Disaggregated LLM Serving with Elastic Memory Pool}, 
      author={Cunchen Hu and Heyang Huang and Junhao Hu and Jiang Xu and Xusheng Chen and Tao Xie and Chenxi Wang and Sa Wang and Yungang Bao and Ninghui Sun and Yizhou Shan},
      year={2024},
      eprint={2406.17565},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2406.17565}, 
}

@article{timetoken,
author = {Brysbaert, Marc},
year = {2019},
month = {08},
pages = {},
title = {How many words do we read per minute? A review and meta-analysis of reading rate},
volume = {109},
journal = {Journal of Memory and Language},
doi = {10.1016/j.jml.2019.104047}
}


@inproceedings{transformer,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}


@article{bert,
  author       = {Jacob Devlin and
                  Ming{-}Wei Chang and
                  Kenton Lee and
                  Kristina Toutanova},
  title        = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
                  Understanding},
  journal      = {CoRR},
  volume       = {abs/1810.04805},
  year         = {2018},
  url          = {http://arxiv.org/abs/1810.04805},
  eprinttype    = {arXiv},
  eprint       = {1810.04805},
  timestamp    = {Tue, 30 Oct 2018 20:39:56 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{gpt-3,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}


@article{megatron-lm,
  author       = {Mohammad Shoeybi and
                  Mostofa Patwary and
                  Raul Puri and
                  Patrick LeGresley and
                  Jared Casper and
                  Bryan Catanzaro},
  title        = {Megatron-LM: Training Multi-Billion Parameter Language Models Using
                  Model Parallelism},
  journal      = {CoRR},
  volume       = {abs/1909.08053},
  year         = {2019},
  url          = {http://arxiv.org/abs/1909.08053},
  eprinttype    = {arXiv},
  eprint       = {1909.08053},
  timestamp    = {Tue, 24 Sep 2019 11:33:51 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1909-08053.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{guo2021longt5,
  title={LongT5: Efficient text-to-text transformer for long sequences},
  author={Guo, Mandy and Ainslie, Joshua and Uthus, David and Ontanon, Santiago and Ni, Jianmo and Sung, Yun-Hsuan and Yang, Yinfei},
  journal={arXiv preprint arXiv:2112.07916},
  year={2021}
}


@inbook{gpipe,
author = {Huang, Yanping and Cheng, Youlong and Bapna, Ankur and Firat, Orhan and Chen, Mia Xu and Chen, Dehao and Lee, HyoukJoong and Ngiam, Jiquan and Le, Quoc V. and Wu, Yonghui and Chen, Zhifeng},
title = {GPipe: Efficient Training of Giant Neural Networks Using Pipeline Parallelism},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {10},
numpages = {10}
}



@inproceedings{pipedream,
author = {Narayanan, Deepak and Harlap, Aaron and Phanishayee, Amar and Seshadri, Vivek and Devanur, Nikhil R. and Ganger, Gregory R. and Gibbons, Phillip B. and Zaharia, Matei},
title = {PipeDream: Generalized Pipeline Parallelism for DNN Training},
year = {2019},
isbn = {9781450368735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341301.3359646},
doi = {10.1145/3341301.3359646},
booktitle = {Proceedings of the 27th ACM Symposium on Operating Systems Principles},
pages = {1-15},
numpages = {15},
location = {Huntsville, Ontario, Canada},
series = {SOSP '19}
}


@inproceedings{efficient2021narayanan,
author = {Narayanan, Deepak and Shoeybi, Mohammad and Casper, Jared and LeGresley, Patrick and Patwary, Mostofa and Korthikanti, Vijay and Vainbrand, Dmitri and Kashinkunti, Prethvi and Bernauer, Julie and Catanzaro, Bryan and Phanishayee, Amar and Zaharia, Matei},
title = {Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM},
year = {2021},
isbn = {9781450384421},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458817.3476209},
doi = {10.1145/3458817.3476209},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {58},
numpages = {15},
location = {St. Louis, Missouri},
series = {SC '21}
}


@misc{korthikanti2022reducing,
      title={Reducing Activation Recomputation in Large Transformer Models}, 
      author={Vijay Korthikanti and Jared Casper and Sangkug Lym and Lawrence McAfee and Michael Andersch and Mohammad Shoeybi and Bryan Catanzaro},
      year={2022},
      eprint={2205.05198},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{ding2023longnet,
      title={LongNet: Scaling Transformers to 1,000,000,000 Tokens}, 
      author={Jiayu Ding and Shuming Ma and Li Dong and Xingxing Zhang and Shaohan Huang and Wenhui Wang and Nanning Zheng and Furu Wei},
      year={2023},
      eprint={2307.02486},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@inproceedings{dean2012large,
 author = {Dean, Jeffrey and Corrado, Greg and Monga, Rajat and Chen, Kai and Devin, Matthieu and Mao, Mark and Ranzato, Marc\textquotesingle aurelio and Senior, Andrew and Tucker, Paul and Yang, Ke and Le, Quoc and Ng, Andrew},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Large Scale Distributed Deep Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/6aca97005c68f1206823815f66102863-Paper.pdf},
 volume = {25},
 year = {2012}
}





@misc{nccl,
  howpublished = {\url{https://docs.nvidia.com/deeplearning/nccl/index.html}},
  title = {NVIDIA NCCL Documentation},
author = {NVIDIA Corporation},
year={2025}
}

@misc{deepspeed,
  howpublished = {\url{https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/}},
  title = {{DeepSpeed}: Extreme-scale model training for everyone},
}



@inproceedings {alpa,
author = {Lianmin Zheng and Zhuohan Li and Hao Zhang and Yonghao Zhuang and Zhifeng Chen and Yanping Huang and Yida Wang and Yuanzhong Xu and Danyang Zhuo and Eric P. Xing and Joseph E. Gonzalez and Ion Stoica},
title = {Alpa: Automating Inter- and {Intra-Operator} Parallelism for Distributed Deep Learning},
booktitle = {16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)},
year = {2022},
isbn = {978-1-939133-28-1},
address = {Carlsbad, CA},
pages = {559--578},
url = {https://www.usenix.org/conference/osdi22/presentation/zheng-lianmin},
publisher = {USENIX Association},
month = jul,
}

@inproceedings{li2023how,
title={How Long Can Context Length of Open-Source {LLM}s truly Promise?},
author={Dacheng Li and Rulin Shao and Anze Xie and Ying Sheng and Lianmin Zheng and Joseph Gonzalez and Ion Stoica and Xuezhe Ma and Hao Zhang},
booktitle={NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following},
year={2023},
url={https://openreview.net/forum?id=LywifFNXV5}
}

@article{shaden2022using,
  author       = {Shaden Smith and
                  Mostofa Patwary and
                  Brandon Norick and
                  Patrick LeGresley and
                  Samyam Rajbhandari and
                  Jared Casper and
                  Zhun Liu and
                  Shrimai Prabhumoye and
                  George Zerveas and
                  Vijay Korthikanti and
                  Elton Zheng and
                  Rewon Child and
                  Reza Yazdani Aminabadi and
                  Julie Bernauer and
                  Xia Song and
                  Mohammad Shoeybi and
                  Yuxiong He and
                  Michael Houston and
                  Saurabh Tiwary and
                  Bryan Catanzaro},
  title        = {Using DeepSpeed and Megatron to Train Megatron-Turing {NLG} 530B,
                  {A} Large-Scale Generative Language Model},
  journal      = {CoRR},
  volume       = {abs/2201.11990},
  year         = {2022},
  url          = {https://arxiv.org/abs/2201.11990},
  eprinttype    = {arXiv},
  eprint       = {2201.11990},
  timestamp    = {Wed, 02 Feb 2022 15:00:01 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2201-11990.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}




@article{li2021sequence,
  author       = {Shenggui Li and
                  Fuzhao Xue and
                  Yongbin Li and
                  Yang You},
  title        = {Sequence Parallelism: Long Sequence Training from System Perspective},
  journal      = {CoRR},
  volume       = {abs/2105.13120},
  year         = {2021},
  url          = {https://arxiv.org/abs/2105.13120},
  eprinttype    = {arXiv},
  eprint       = {2105.13120},
  timestamp    = {Sat, 17 Dec 2022 01:15:27 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2105-13120.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{adhikari2019docbert,
  author       = {Ashutosh Adhikari and
                  Achyudh Ram and
                  Raphael Tang and
                  Jimmy Lin},
  title        = {DocBERT: {BERT} for Document Classification},
  journal      = {CoRR},
  volume       = {abs/1904.08398},
  year         = {2019},
  url          = {http://arxiv.org/abs/1904.08398},
  eprinttype    = {arXiv},
  eprint       = {1904.08398},
  timestamp    = {Fri, 26 Apr 2019 13:18:53 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1904-08398.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@misc{dai2022revisiting,
      title={Revisiting Transformer-based Models for Long Document Classification}, 
      author={Xiang Dai and Ilias Chalkidis and Sune Darkner and Desmond Elliott},
      year={2022},
      eprint={2204.06683},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@inproceedings{egonmwan2019transformer,
    title = "Transformer-based Model for Single Documents Neural Summarization",
    author = "Egonmwan, Elozino  and Chali, Yllias",
    booktitle = "Proceedings of the 3rd Workshop on Neural Generation and Translation",
    month = nov,
    year = "2019",
    address = "Hong Kong",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-5607",
    doi = "10.18653/v1/D19-5607",
    pages = "70--79",
}

@INPROCEEDINGS{zi2022source,
  author={Gong, Zi and Gao, Cuiyun and Wang, Yasheng and Gu, Wenchao and Peng, Yun and Xu, Zenglin},
  booktitle={2022 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)}, 
  title={Source Code Summarization with Structural Relative Position Guided Transformer}, 
  year={2022},
  volume={},
  number={},
  pages={13-24},
  doi={10.1109/SANER53432.2022.00013}}


@misc{zhang2022hegel,
      title={{HEGEL}: Hypergraph Transformer for Long Document Summarization}, 
      author={Haopeng Zhang and Xiao Liu and Jiawei Zhang},
      year={2022},
      eprint={2210.04126},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@article{wang2020linformer,
  author       = {Sinong Wang and
                  Belinda Z. Li and
                  Madian Khabsa and
                  Han Fang and
                  Hao Ma},
  title        = {Linformer: Self-Attention with Linear Complexity},
  journal      = {CoRR},
  volume       = {abs/2006.04768},
  year         = {2020},
  url          = {https://arxiv.org/abs/2006.04768},
  eprinttype    = {arXiv},
  eprint       = {2006.04768},
  timestamp    = {Mon, 06 Feb 2023 11:49:42 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2006-04768.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}




@INPROCEEDINGS{winata2020lightweight,
  author={Winata, Genta Indra and Cahyawijaya, Samuel and Lin, Zhaojiang and Liu, Zihan and Fung, Pascale},
  booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Lightweight and Efficient End-To-End Speech Recognition Using Low-Rank Transformer}, 
  year={2020},
  volume={},
  number={},
  pages={6144-6148},
  doi={10.1109/ICASSP40776.2020.9053878}}

@misc{flashattn2,
      title={FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning}, 
      author={Tri Dao},
      year={2023},
      eprint={2307.08691},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2307.08691}, 
}

@article{katharopoulos2020transformers,
  author       = {Angelos Katharopoulos and
                  Apoorv Vyas and
                  Nikolaos Pappas and
                  Fran{\c{c}}ois Fleuret},
  title        = {Transformers are RNNs: Fast Autoregressive Transformers with Linear
                  Attention},
  journal      = {CoRR},
  volume       = {abs/2006.16236},
  year         = {2020},
  url          = {https://arxiv.org/abs/2006.16236},
  eprinttype    = {arXiv},
  eprint       = {2006.16236},
  timestamp    = {Wed, 01 Jul 2020 15:21:23 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2006-16236.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{choromanski2020rethinking,
  author       = {Krzysztof Choromanski and
                  Valerii Likhosherstov and
                  David Dohan and
                  Xingyou Song and
                  Andreea Gane and
                  Tam{\'{a}}s Sarl{\'{o}}s and
                  Peter Hawkins and
                  Jared Davis and
                  Afroz Mohiuddin and
                  Lukasz Kaiser and
                  David Belanger and
                  Lucy J. Colwell and
                  Adrian Weller},
  title        = {Rethinking Attention with Performers},
  journal      = {CoRR},
  volume       = {abs/2009.14794},
  year         = {2020},
  url          = {https://arxiv.org/abs/2009.14794},
  eprinttype    = {arXiv},
  eprint       = {2009.14794},
  timestamp    = {Wed, 23 Jun 2021 10:58:18 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2009-14794.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{qin2022devil,
      title={The Devil in Linear Transformer}, 
      author={Zhen Qin and XiaoDong Han and Weixuan Sun and Dongxu Li and Lingpeng Kong and Nick Barnes and Yiran Zhong},
      year={2022},
      eprint={2210.10340},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@InProceedings{chaudhuri2019set-transformer,
  title = 	 {Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks},
  author =       {Lee, Juho and Lee, Yoonho and Kim, Jungtaek and Kosiorek, Adam and Choi, Seungjin and Teh, Yee Whye},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {3744--3753},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/lee19d/lee19d.pdf},
  url = 	 {https://proceedings.mlr.press/v97/lee19d.html}
}



@InProceedings{jaegle2021perceiver,
  title = 	 {Perceiver: General Perception with Iterative Attention},
  author =       {Jaegle, Andrew and Gimeno, Felix and Brock, Andy and Vinyals, Oriol and Zisserman, Andrew and Carreira, Joao},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {4651--4664},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/jaegle21a/jaegle21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/jaegle21a.html},
}

@inproceedings{ma2021luna,
 author = {Ma, Xuezhe and Kong, Xiang and Wang, Sinong and Zhou, Chunting and May, Jonathan and Ma, Hao and Zettlemoyer, Luke},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {2441--2453},
 publisher = {Curran Associates, Inc.},
 title = {Luna: Linear Unified Nested Attention},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/14319d9cfc6123106878dc20b94fbaf3-Paper.pdf},
 volume = {34},
 year = {2021}
}


@article{dai2019transformer-xl,
  author       = {Zihang Dai and
                  Zhilin Yang and
                  Yiming Yang and
                  Jaime G. Carbonell and
                  Quoc V. Le and
                  Ruslan Salakhutdinov},
  title        = {Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context},
  journal      = {CoRR},
  volume       = {abs/1901.02860},
  year         = {2019},
  url          = {http://arxiv.org/abs/1901.02860},
  eprinttype    = {arXiv},
  eprint       = {1901.02860},
  timestamp    = {Fri, 01 Feb 2019 13:39:59 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1901-02860.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@misc{bulatov2023scaling,
      title={Scaling Transformer to 1M tokens and beyond with RMT}, 
      author={Aydar Bulatov and Yuri Kuratov and Mikhail S. Burtsev},
      year={2023},
      eprint={2304.11062},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{wu2022memorizing,
      title={Memorizing Transformers}, 
      author={Yuhuai Wu and Markus N. Rabe and DeLesley Hutchins and Christian Szegedy},
      year={2022},
      eprint={2203.08913},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@misc{wang2023augmenting,
      title={Augmenting Language Models with Long-Term Memory}, 
      author={Weizhi Wang and Li Dong and Hao Cheng and Xiaodong Liu and Xifeng Yan and Jianfeng Gao and Furu Wei},
      year={2023},
      eprint={2306.07174},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@inproceedings {yu2022orca,
author = {Gyeong-In Yu and Joo Seong Jeong and Geon-Woo Kim and Soojeong Kim and Byung-Gon Chun},
title = {Orca: A Distributed Serving System for {Transformer-Based} Generative Models},
booktitle = {16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)},
year = {2022},
isbn = {978-1-939133-28-1},
address = {Carlsbad, CA},
pages = {521--538},
url = {https://www.usenix.org/conference/osdi22/presentation/yu},
publisher = {USENIX Association},
month = jul,
}


@inproceedings {li2023alpaserve,
author = {Zhuohan Li and Lianmin Zheng and Yinmin Zhong and Vincent Liu and Ying Sheng and Xin Jin and Yanping Huang and Zhifeng Chen and Hao Zhang and Joseph E. Gonzalez and Ion Stoica},
title = {{AlpaServe}: Statistical Multiplexing with Model Parallelism for Deep Learning Serving},
booktitle = {17th USENIX Symposium on Operating Systems Design and Implementation (OSDI 23)},
year = {2023},
isbn = {978-1-939133-34-2},
address = {Boston, MA},
pages = {663--679},
url = {https://www.usenix.org/conference/osdi23/presentation/li-zhouhan},
publisher = {USENIX Association},
month = jul,
}


@inproceedings {zheng2022alpa,
author = {Lianmin Zheng and Zhuohan Li and Hao Zhang and Yonghao Zhuang and Zhifeng Chen and Yanping Huang and Yida Wang and Yuanzhong Xu and Danyang Zhuo and Eric P. Xing and Joseph E. Gonzalez and Ion Stoica},
title = {Alpa: Automating Inter- and {Intra-Operator} Parallelism for Distributed Deep Learning},
booktitle = {16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)},
year = {2022},
isbn = {978-1-939133-28-1},
address = {Carlsbad, CA},
pages = {559--578},
url = {https://www.usenix.org/conference/osdi22/presentation/zheng-lianmin},
publisher = {USENIX Association},
month = jul,
}



@misc{jin2023s3,
      title={S$^{3}$: Increasing GPU Utilization during Generative Inference for Higher Throughput}, 
      author={Yunho Jin and Chun-Feng Wu and David Brooks and Gu-Yeon Wei},
      year={2023},
      eprint={2306.06000},
      archivePrefix={arXiv},
      primaryClass={cs.AR}
}

@misc{wu2023fast,
      title={Fast Distributed Inference Serving for Large Language Models}, 
      author={Bingyang Wu and Yinmin Zhong and Zili Zhang and Gang Huang and Xuanzhe Liu and Xin Jin},
      year={2023},
      eprint={2305.05920},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{sheng2023flexgen,
      title={FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU}, 
      author={Ying Sheng and Lianmin Zheng and Binhang Yuan and Zhuohan Li and Max Ryabinin and Daniel Y. Fu and Zhiqiang Xie and Beidi Chen and Clark Barrett and Joseph E. Gonzalez and Percy Liang and Christopher Re and Ion Stoica and Ce Zhang},
      year={2023},
      eprint={2303.06865},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}



@misc{zheng2023response,
      title={Response Length Perception and Sequence Scheduling: An LLM-Empowered LLM Inference Pipeline}, 
      author={Zangwei Zheng and Xiaozhe Ren and Fuzhao Xue and Yang Luo and Xin Jiang and Yang You},
      year={2023},
      eprint={2305.13144},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@article{gpt,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year={2018},
  publisher={OpenAI}
}


@article{gpt-2,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}


@misc{gpt-4,
      title={Toolformer: Language Models Can Teach Themselves to Use Tools}, 
      author={Timo Schick and Jane Dwivedi-Yu and Roberto Dessi and Roberta Raileanu and Maria Lomeli and Luke Zettlemoyer and Nicola Cancedda and Thomas Scialom},
      year={2023},
      eprint={2302.04761},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{textsynth,
  howpublished = {\url{https://textsynth.com/completion.html}},
  title = {TextSynth: Text Completion},
}


@misc{cutlass,
  howpublished = {\url{https://github.com/NVIDIA/cutlass}},
  title = {{NVIDIA CUTLASS}},
  author = {NVIDIA Corporation}
}

@misc{quart,
  howpublished = {\url{https://quart.palletsprojects.com/en/latest/}},
  title = {Quart},
}

@misc{chatgpt,
  howpublished = {\url{https://chat.openai.com/}, [Accessed in Aug. 2023]},
  title = {OpenAI: {ChatGPT}}
}





@misc{transformer-translation,
  howpublished = {\url{https://blog.huawei.com/2022/02/01/speaking-your-language-transformer-machine-translation/}},
  title = {Speaking Your Language: The Transformer in Machine Translation},
}

@misc{deepspeed-ulysses,
      title={DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models}, 
      author={Sam Ade Jacobs and Masahiro Tanaka and Chengming Zhang and Minjia Zhang and Shuaiwen Leon Song and Samyam Rajbhandari and Yuxiong He},
      year={2023},
      eprint={2309.14509},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@misc{awsp4,
  howpublished = {\url{https://aws.amazon.com/ec2/instance-types/p4/}},
  title = {Amazon {EC2 P4} Instances},
}



@misc{triton-flashattn,
  howpublished = {\url{https://triton-lang.org/main/getting-started/tutorials/06-fused-attention.html}},
  title = {Fused Attention in {Triton}},
 author= {Philippe Tillet},
year = {2020}
}


@misc{triton,
  howpublished = {\url{https://openai.com/index/triton/}},
  title = {{Introducing Triton: Open-source GPU programming for neural networks}},
author = {OpenAI},
year = {2021}
}

@misc{openvswitch,
  howpublished = {\url{https://www.openvswitch.org/}},
  title = {{Open vSwitch}},
}


@inproceedings{bertasius2021space,
  title={Is space-time attention all you need for video understanding?},
  author={Bertasius, Gedas and Wang, Heng and Torresani, Lorenzo},
  booktitle={ICML},
  volume={2},
  number={3},
  pages={4},
  year={2021}
}


@misc{li2023survey,
      title={A Survey on Transformers in Reinforcement Learning}, 
      author={Wenzhe Li and Hao Luo and Zichuan Lin and Chongjie Zhang and Zongqing Lu and Deheng Ye},
      year={2023},
      eprint={2301.03044},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{func-word,
      title={Definition and Examples of Function Words in English},
      year={2023},
      howpublished = {\url{https://www.thoughtco.com/function-word-grammar-1690876}},
}



@misc{gpt-4o,
      title={Hello {GPT-4o}},
      year={2024},
      howpublished = {\url{https://openai.com/index/hello-gpt-4o/}},
}



@inproceedings {infinigen,
author = {Wonbeom Lee and Jungi Lee and Junghwan Seo and Jaewoong Sim},
title = {{InfiniGen}: Efficient Generative Inference of Large Language Models with Dynamic {KV} Cache Management},
booktitle = {18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24)},
year = {2024},
isbn = {978-1-939133-40-3},
address = {Santa Clara, CA},
pages = {155--172},
url = {https://www.usenix.org/conference/osdi24/presentation/lee},
publisher = {USENIX Association},
month = jul
}




@InProceedings{yu2023magvit,
    author    = {Yu, Lijun and Cheng, Yong and Sohn, Kihyuk and Lezama, Jos\'e and Zhang, Han and Chang, Huiwen and Hauptmann, Alexander G. and Yang, Ming-Hsuan and Hao, Yuan and Essa, Irfan and Jiang, Lu},
    title     = {MAGVIT: Masked Generative Video Transformer},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2023},
    pages     = {10459-10469}
}

@article{pile,
  title={The {P}ile: An 800GB Dataset of Diverse Text for Language Modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and Presser, Shawn and Leahy, Connor},
  journal={arXiv preprint arXiv:2101.00027},
  year={2020}
}

@book{zhou2003location,
  title={Location-based node ids: Enabling explicit locality in dhts},
  author={Zhou, Shuheng and Ganger, Gregory R},
  year={2003}
}






@inproceedings{gpt3int82022dettmers,
 author = {Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {30318--30332},
 publisher = {Curran Associates, Inc.},
 title = {GPT3.int8(): 8-bit Matrix Multiplication for Transformers at Scale},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/c3ba4962c05c49636d4c6206a97e9c8a-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}


@InProceedings{multiplying2021blalock,
  title = 	 {Multiplying Matrices Without Multiplying},
  author =       {Blalock, Davis and Guttag, John},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {992--1004},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/blalock21a/blalock21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/blalock21a.html},
}


@inproceedings{biqgemm2020jeon,
author = {Jeon, Yongkweon and Park, Baeseong and Kwon, Se Jung and Kim, Byeongwook and Yun, Jeongin and Lee, Dongsoo},
title = {BiQGEMM: matrix multiplication with lookup table for binary-coding-based quantized DNNs},
year = {2020},
isbn = {9781728199986},
publisher = {IEEE Press},
articleno = {95},
numpages = {16},
keywords = {quantization, model compression, machine learning, deep learning, GEMV, GEMM, AI inference},
location = {Atlanta, Georgia},
series = {SC '20}
}


@inproceedings{encoder2018raganato,
    title = "An Analysis of Encoder Representations in Transformer-Based Machine Translation",
    author = {Raganato, Alessandro  and
      Tiedemann, J{\"o}rg},
    editor = "Linzen, Tal  and
      Chrupa{\l}a, Grzegorz  and
      Alishahi, Afra",
    booktitle = "Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-5431",
    doi = "10.18653/v1/W18-5431",
    pages = "287--297",
}

@article{embedding2022singh,
title = {A novel approach for dimension reduction using word embedding: An enhanced text classification approach},
journal = {International Journal of Information Management Data Insights},
volume = {2},
number = {1},
pages = {100061},
year = {2022},
issn = {2667-0968},
doi = {https://doi.org/10.1016/j.jjimei.2022.100061},
url = {https://www.sciencedirect.com/science/article/pii/S2667096822000052},
author = {Ksh. Nareshkumar Singh and S. Dickeeta Devi and H. Mamata Devi and Anjana Kakoti Mahanta},
keywords = {Dimension reduction, Document representation, GloVe, Term weighting},
}




@misc{svd-solver,
      title={Singular Value Decomposition Solver from SciPy},
      year={2024},
      howpublished = {\url{https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.svd.html}},
}


@misc{open-compute,
      title={Open Compute Project},
      year={2025},
      howpublished = {\url{https://www.opencompute.org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf}},
}


@misc{vanbaalen2023fp8versusint8efficient,
      title={FP8 versus INT8 for efficient deep learning inference}, 
      author={Mart van Baalen and Andrey Kuzmin and Suparna S Nair and Yuwei Ren and Eric Mahurin and Chirag Patel and Sundar Subramanian and Sanghyuk Lee and Markus Nagel and Joseph Soriaga and Tijmen Blankevoort},
      year={2023},
      eprint={2303.17951},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2303.17951}, 
}

@INPROCEEDINGS{intorfp,
  author={Zhang, Yijia and Zhao, Lingran and Cao, Shijie and Zhang, Sicheng and Wang, Wenqiang and Cao, Ting and Yang, Fan and Yang, Mao and Zhang, Shanghang and Xu, Ningyi},
  booktitle={2024 IEEE International Conference on Multimedia and Expo (ICME)}, 
  title={Integer or Floating Point? New Outlooks for Low-Bit Quantization on Large Language Models}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  keywords={Quantization (signal);Costs;Tensors;Large language models;Multimedia systems;Graphics processing units;Hardware;Large Language Model;Low-bit Quantization;Floating Point},
  doi={10.1109/ICME57554.2024.10688089}}


@article{online-softmax2018milakov,
  author       = {Maxim Milakov and
                  Natalia Gimelshein},
  title        = {Online normalizer calculation for softmax},
  journal      = {CoRR},
  volume       = {abs/1805.02867},
  year         = {2018},
  url          = {http://arxiv.org/abs/1805.02867},
  eprinttype    = {arXiv},
  eprint       = {1805.02867},
  timestamp    = {Mon, 13 Aug 2018 16:48:49 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1805-02867.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{h2o2023zhang,
 author = {Zhang, Zhenyu and Sheng, Ying and Zhou, Tianyi and Chen, Tianlong and Zheng, Lianmin and Cai, Ruisi and Song, Zhao and Tian, Yuandong and R\'{e}, Christopher and Barrett, Clark and Wang, Zhangyang "Atlas" and Chen, Beidi},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Neumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {34661--34710},
 publisher = {Curran Associates, Inc.},
 title = {H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/6ceefa7b15572587b78ecfcebb2827f8-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}


@INPROCEEDINGS{splitwise,
  author={Patel, Pratyush and Choukse, Esha and Zhang, Chaojie and Shah, Aashaka and Goiri, Íñigo and Maleki, Saeed and Bianchini, Ricardo},
  booktitle={2024 ACM/IEEE 51st Annual International Symposium on Computer Architecture (ISCA)}, 
  title={Splitwise: Efficient Generative LLM Inference Using Phase Splitting}, 
  year={2024},
  volume={},
  number={},
  pages={118-132},
  keywords={Costs;Processor scheduling;Large language models;Computational modeling;Graphics processing units;Computer architecture;Throughput;Large language models;Cluster deployments;Scheduling;GPUs;Inference efficiency;Machine learning;Resource management},
  doi={10.1109/ISCA59077.2024.00019}}



@inproceedings{
jiang2024minference,
title={{MI}nference 1.0: Accelerating Pre-filling for Long-Context {LLM}s via Dynamic Sparse Attention},
author={Huiqiang Jiang and YUCHENG LI and Chengruidong Zhang and Qianhui Wu and Xufang Luo and Surin Ahn and Zhenhua Han and Amir H. Abdi and Dongsheng Li and Chin-Yew Lin and Yuqing Yang and Lili Qiu},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=fPBACAbqSN}
}

@inproceedings{dynamic-context-pruning,
 author = {Anagnostidis, Sotiris and Pavllo, Dario and Biggio, Luca and Noci, Lorenzo and Lucchi, Aurelien and Hofmann, Thomas},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {65202--65223},
 publisher = {Curran Associates, Inc.},
 title = {Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/cdaac2a02c4fdcae77ba083b110efcc3-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}



@misc{zhang2024efficientsparseattentionneeds,
      title={Efficient Sparse Attention needs Adaptive Token Release}, 
      author={Chaoran Zhang and Lixin Zou and Dan Luo and Min Tang and Xiangyang Luo and Zihao Li and Chenliang Li},
      year={2024},
      eprint={2407.02328},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.02328}, 
}

@inproceedings{l2norm-kv,
    title = "A Simple and Effective $L\_2$ Norm-Based Strategy for {KV} Cache Compression",
    author = "Devoto, Alessio  and
      Zhao, Yu  and
      Scardapane, Simone  and
      Minervini, Pasquale",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.1027",
    doi = "10.18653/v1/2024.emnlp-main.1027",
    pages = "18476--18499",
}

@misc{pyramidinfer,
      title={PyramidInfer: Pyramid KV Cache Compression for High-throughput LLM Inference}, 
      author={Dongjie Yang and XiaoDong Han and Yan Gao and Yao Hu and Shilin Zhang and Hai Zhao},
      year={2024},
      eprint={2405.12532},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.12532}, 
}

@inproceedings{keyformer,
 author = {Adnan, Muhammad and Arunkumar, Akhil and Jain, Gaurav and Nair, Prashant and Soloveychik, Ilya and Kamath, Purushotham},
 booktitle = {Proceedings of Machine Learning and Systems},
 editor = {P. Gibbons and G. Pekhimenko and C. De Sa},
 pages = {114--127},
 title = {Keyformer: KV Cache reduction through key tokens selection for Efficient Generative Inference},
 url = {https://proceedings.mlsys.org/paper_files/paper/2024/file/48fecef47b19fe501d27d338b6d52582-Paper-Conference.pdf},
 volume = {6},
 year = {2024}
}



@InProceedings{kivi,
  title = 	 {{KIVI}: A Tuning-Free Asymmetric 2bit Quantization for {KV} Cache},
  author =       {Liu, Zirui and Yuan, Jiayi and Jin, Hongye and Zhong, Shaochen and Xu, Zhaozhuo and Braverman, Vladimir and Chen, Beidi and Hu, Xia},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {32332--32344},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/liu24bz/liu24bz.pdf},
  url = 	 {https://proceedings.mlr.press/v235/liu24bz.html},
}

@misc{lorc,
      title={LoRC: Low-Rank Compression for LLMs KV Cache with a Progressive Compression Strategy}, 
      author={Rongzhi Zhang and Kuang Wang and Liyuan Liu and Shuohang Wang and Hao Cheng and Chao Zhang and Yelong Shen},
      year={2024},
      eprint={2410.03111},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.03111}, 
}

@misc{palu,
      title={Palu: Compressing KV-Cache with Low-Rank Projection}, 
      author={Chi-Chih Chang and Wei-Cheng Lin and Chien-Yu Lin and Chong-Yan Chen and Yu-Fang Hu and Pei-Shuo Wang and Ning-Chi Huang and Luis Ceze and Mohamed S. Abdelfattah and Kai-Chiang Wu},
      year={2024},
      eprint={2407.21118},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.21118}, 
}

@misc{fp6,
      title={FP6-LLM: Efficiently Serving Large Language Models Through FP6-Centric Algorithm-System Co-Design}, 
      author={Haojun Xia and Zhen Zheng and Xiaoxia Wu and Shiyang Chen and Zhewei Yao and Stephen Youn and Arash Bakhtiari and Michael Wyatt and Donglin Zhuang and Zhongzhu Zhou and Olatunji Ruwase and Yuxiong He and Shuaiwen Leon Song},
      year={2024},
      eprint={2401.14112},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2401.14112}, 
}


@INPROCEEDINGS{fp4,
  author={Wang, Jie and Liu, Huanxi and Feng, Dawei and Ding, Jie and Ding, Bo},
  booktitle={2024 IEEE International Conference on Joint Cloud Computing (JCC)}, 
  title={FP4-Quantization: Lossless 4bit Quantization for Large Language Models}, 
  year={2024},
  volume={},
  number={},
  pages={61-67},
  keywords={Degradation;Cloud computing;Quantization (signal);Accuracy;Costs;Computational modeling;Large language models},
  doi={10.1109/JCC62314.2024.00017}}


@InProceedings{squant,
author="Klauder, John R.",
editor="Mitter, H.
and Lang, C. B.",
title="Stochastic Quantization",
booktitle="Recent Developments in High-Energy Physics",
year="1983",
publisher="Springer Vienna",
address="Vienna",
pages="251--281",
abstract="In this introductory survey to stochastic quantization we outline this new approach for scalar fields, gauge fields, fermion fields, and Condensed matter problems such as electrons in solids and the Statistical mechanics of quantum spins.",
isbn="978-3-7091-7651-1"
}




@InProceedings{strati2024dejavukvcachestreamingfast,
  title = 	 {DéjàVu: {KV}-cache Streaming for Fast, Fault-tolerant Generative {LLM} Serving},
  author =       {Strati, Foteini and Mcallister, Sara and Phanishayee, Amar and Tarnawski, Jakub and Klimovic, Ana},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {46745--46771},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/strati24a/strati24a.pdf},
  url = 	 {https://proceedings.mlr.press/v235/strati24a.html},
  abstract = 	 {Distributed LLM serving is costly and often underutilizes hardware accelerators due to three key challenges: bubbles in pipeline-parallel deployments caused by the bimodal latency of prompt and token processing, GPU memory overprovisioning, and long recovery times in case of failures. DéjàVu addresses all these challenges using a versatile and efficient KV cache streaming library (DéjàVuLib). Using DéjàVuLib, we propose and implement efficient prompt-token disaggregation to reduce pipeline bubbles, microbatch swapping for efficient GPU memory management, and state replication for fault-tolerance. We highlight the efficacy of these solutions on a range of large models across cloud deployments.}
}




@inproceedings{cachegen,
author = {Liu, Yuhan and Li, Hanchen and Cheng, Yihua and Ray, Siddhant and Huang, Yuyang and Zhang, Qizheng and Du, Kuntai and Yao, Jiayi and Lu, Shan and Ananthanarayanan, Ganesh and Maire, Michael and Hoffmann, Henry and Holtzman, Ari and Jiang, Junchen},
title = {CacheGen: KV Cache Compression and Streaming for Fast Large Language Model Serving},
year = {2024},
isbn = {9798400706141},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3651890.3672274},
doi = {10.1145/3651890.3672274},
abstract = {As large language models (LLMs) take on complex tasks, their inputs are supplemented with longer contexts that incorporate domain knowledge. Yet using long contexts is challenging as nothing can be generated until the whole context is processed by the LLM. While the context-processing delay can be reduced by reusing the KV cache of a context across different inputs, fetching the KV cache, which contains large tensors, over the network can cause high extra network delays.CacheGen is a fast context-loading module for LLM systems. First, CacheGen uses a custom tensor encoder, leveraging KV cache's distributional properties to encode a KV cache into more compact bitstream representations with negligible decoding overhead, to save bandwidth usage. Second, CacheGen adapts the compression level of different parts of a KV cache to cope with changes in available bandwidth, in order to maintain low context-loading delay and high generation quality. We test CacheGen on popular LLMs and datasets. Compared to the recent systems that reuse the KV cache, CacheGen reduces the KV cache size by 3.5--4.3x and the total delay in fetching and processing contexts by 3.2--3.7x with negligible impact on the LLM response quality. Our code is at: https://github.com/UChi-JCL/CacheGen.},
booktitle = {Proceedings of the ACM SIGCOMM 2024 Conference},
pages = {38–56},
numpages = {19},
keywords = {large language models, KV cache, compression},
location = {Sydney, NSW, Australia},
series = {ACM SIGCOMM '24}
}

@inproceedings{
kvquant,
title={{KVQ}uant: Towards 10 Million Context Length {LLM} Inference with {KV} Cache Quantization},
author={Coleman Richard Charles Hooper and Sehoon Kim and Hiva Mohammadzadeh and Michael W. Mahoney and Sophia Shao and Kurt Keutzer and Amir Gholami},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=0LXotew9Du}
}


@misc{zipcache,
      title={ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification}, 
      author={Yefei He and Luoming Zhang and Weijia Wu and Jing Liu and Hong Zhou and Bohan Zhuang},
      year={2024},
      eprint={2405.14256},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.14256}, 
}


@inproceedings{scissorhands2023liu,
 author = {Liu, Zichang and Desai, Aditya and Liao, Fangshuo and Wang, Weitao and Xie, Victor and Xu, Zhaozhuo and Kyrillidis, Anastasios and Shrivastava, Anshumali},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Neumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {52342--52364},
 publisher = {Curran Associates, Inc.},
 title = {Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/a452a7c6c463e4ae8fbdc614c6e983e6-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}


@article{kang2024gear,
      title={{GEAR}: An Efficient {KV} Cache Compression Recipe for Near-Lossless Generative Inference of {LLM}}, 
      author={Hao Kang and Qingru Zhang and Souvik Kundu and Geonhwa Jeong and Zaoxing Liu and Tushar Krishna and Tuo Zhao},
      year={2024},
      eprint={2403.05527},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      journal={arXiv preprint arXiv:2403.05527}
}


@misc{yao2023zeroquantv2,
      title={{ZeroQuant-V2}: Exploring Post-training Quantization in {LLMs} from Comprehensive Study to Low Rank Compensation}, 
      author={Zhewei Yao and Xiaoxia Wu and Cheng Li and Stephen Youn and Yuxiong He},
      year={2023},
      eprint={2303.08302},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      journal={arXiv preprint arXiv:2303.08302},
}


@inproceedings{pit2023zheng,
author = {Zheng, Ningxin and Jiang, Huiqiang and Zhang, Quanlu and Han, Zhenhua and Ma, Lingxiao and Yang, Yuqing and Yang, Fan and Zhang, Chengruidong and Qiu, Lili and Yang, Mao and Zhou, Lidong},
title = {{PIT}: Optimization of Dynamic Sparse Deep Learning Models via Permutation Invariant Transformation},
year = {2023},
isbn = {9798400702297},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3600006.3613139},
doi = {10.1145/3600006.3613139},
booktitle = {Proceedings of the 29th Symposium on Operating Systems Principles},
pages = {331–347},
numpages = {17},
keywords = {deep learning compilers, dynamic sparsity, dynamic compilers, transformation},
location = {, Koblenz, Germany, },
series = {SOSP '23}
}


@misc{park2024lutgemm,
      title={{LUT-GEMM}: Quantized Matrix Multiplication based on {LUTs} for Efficient Inference in Large-Scale Generative Language Models}, 
      author={Gunho Park and Baeseong Park and Minsub Kim and Sungjae Lee and Jeonghoon Kim and Beomseok Kwon and Se Jung Kwon and Byeongwook Kim and Youngjoo Lee and Dongsoo Lee},
      year={2024},
      eprint={2206.09557},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      journal={arXiv preprint arXiv:2206.09557},
}

@misc{opt,
      title={Facebook OPT Models},
      year={2024},
      howpublished = {\url{https://huggingface.co/models?sort=trending&search=facebook+opt}},
}


@misc{falcon,
      title={Falcon-180B},
      year={2025},
      howpublished = {\url{https://huggingface.co/tiiuae/falcon-180B}},
}


@misc{yi-model,
      title={01-ai Model {Yi}},
      year={2025},
      howpublished = {\url{https://huggingface.co/01-ai/Yi-34B-200K}},
}


@misc{llama2,
      title={Meta Llama-2 Models},
      year={2025},
      howpublished = {\url{https://huggingface.co/models?sort=trending&search=meta+Llama-2}},
}





@misc{hkvq-code,
      title={The code of {HKVQ}},
      year={2025},
      howpublished = {\url{https://anonymous.4open.science/r/HKVQ}},
}

@misc{llama3.1,
      title={{Meta Llama-3.1}},
      year={2025},
      howpublished = {\url{https://llama.meta.com/}},
}

@misc{mistral-v0.3,
      title={{Mistral-v0.3}},
      year={2025},
      howpublished = {\url{https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3}},
}

@inproceedings {distserve,
author = {Yinmin Zhong and Shengyu Liu and Junda Chen and Jianbo Hu and Yibo Zhu and Xuanzhe Liu and Xin Jin and Hao Zhang},
title = {{DistServe}: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving},
booktitle = {18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24)},
year = {2024},
isbn = {978-1-939133-40-3},
address = {Santa Clara, CA},
pages = {193--210},
url = {https://www.usenix.org/conference/osdi24/presentation/zhong-yinmin},
publisher = {USENIX Association},
month = jul
}

@misc{phi-3,
      title={{Microsoft Phi-3}},
      year={2025},
      howpublished = {\url{https://huggingface.co/microsoft/Phi-3-medium-128k-instruct}},
}

@misc{opt,
      title={{OPT Models}},
      year={2024},
      howpublished = {\url{https://huggingface.co/docs/transformers/en/model_doc/opt}},
}


@misc{gemmateam2024gemma2improvingopen,
      title={Gemma 2: Improving Open Language Models at a Practical Size}, 
      author={Gemma Team},
      year={2024},
      eprint={2408.00118},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2408.00118}, 
}

@article{holmes2024deepspeedfastgen,
      title={DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference}, 
      author={Connor Holmes and Masahiro Tanaka and Michael Wyatt and Ammar Ahmad Awan and Jeff Rasley and Samyam Rajbhandari and Reza Yazdani Aminabadi and Heyang Qin and Arash Bakhtiari and Lev Kurilenko and Yuxiong He},
      year={2024},
      eprint={2401.08671},
      archivePrefix={arXiv},
      primaryClass={cs.PF},
      journal={arXiv preprint arXiv:2401.08671},
}



@inproceedings{vllm2023kwon, author = {Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion}, title = {Efficient Memory Management for Large Language Model Serving with PagedAttention}, year = {2023}, isbn = {9798400702297}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3600006.3613165}, doi = {10.1145/3600006.3613165}, abstract = {High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time. However, existing systems struggle because the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. To address this problem, we propose PagedAttention, an attention algorithm inspired by the classical virtual memory and paging techniques in operating systems. On top of it, we build vLLM, an LLM serving system that achieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV cache within and across requests to further reduce memory usage. Our evaluations show that vLLM improves the throughput of popular LLMs by 2--4\texttimes{} with the same level of latency compared to the state-of-the-art systems, such as FasterTransformer and Orca. The improvement is more pronounced with longer sequences, larger models, and more complex decoding algorithms. vLLM's source code is publicly available at https://github.com/vllm-project/vllm.}, booktitle = {Proceedings of the 29th Symposium on Operating Systems Principles}, pages = {611–626}, numpages = {16}, location = {, Koblenz, Germany, }, series = {SOSP '23} }


@article{ge2023model,
  title={Model tells you what to discard: Adaptive kv cache compression for llms},
  author={Ge, Suyu and Zhang, Yunan and Liu, Liyuan and Zhang, Minjia and Han, Jiawei and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2310.01801},
  year={2023}
}


@misc{alpaca,
      title={Alpaca Dataset},
      year={2024},
      howpublished = {\url{https://huggingface.co/datasets/tatsu-lab/alpaca}},
}

@misc{sharegpt,
      title={{ShareGPT} Team},
      year={2024},
      howpublished = {\url{https://sharegpt.com/}},
}



@misc{self-attention,
      title={{GPT-4} explaining Self-Attention Mechanism},
      year={2025},
      howpublished = {\url{https://www.linkedin.com/pulse/gpt-4-explaining-self-attention-mechanism-fatos-ismali/}},
}

@misc{starcoder2,
      title={{StarCoder2}},
      year={2024},
      howpublished = {\url{https://huggingface.co/bigcode/starcoder2-15b-instruct-v0.1}},
}


@misc{hf-tgi,
      title={{Hugging Face TGI}},
      year={2024},
      howpublished = {\url{https://huggingface.co/text-generation-inference}},
}

@misc{tensorrt-llm,
      title={{TensorRT-LLM}},
      year={2024},
      howpublished = {\url{https://github.com/NVIDIA/TensorRT-LLM}},
}


@misc{huggingface,
      title={{Hugging Face}: The {AI} community building the future},
      year={2024},
      howpublished = {\url{https://huggingface.co/}},
}



@ARTICLE{svd,
  author={Klema, V. and Laub, A.},
  journal={IEEE Transactions on Automatic Control}, 
  title={The singular value decomposition: Its computation and some applications}, 
  year={1980},
  volume={25},
  number={2},
  pages={164-176},
  keywords={Singular value decomposition;Computer applications;Linear systems;Digital arithmetic;Control systems;Mathematical model;Military computing;Finite wordlength effects;Distributed computing;Floating-point arithmetic},
  doi={10.1109/TAC.1980.1102314}}


@article{wang2023zero++,
  title={Zero++: Extremely efficient collective communication for giant model training},
  author={Wang, Guanhua and Qin, Heyang and Jacobs, Sam Ade and Holmes, Connor and Rajbhandari, Samyam and Ruwase, Olatunji and Yan, Feng and Yang, Lei and He, Yuxiong},
  journal={arXiv preprint arXiv:2306.10209},
  year={2023}
}

@inproceedings{thc2024li,
author = {Minghao Li and Ran Ben Basat and Shay Vargaftik and ChonLam Lao and Kevin Xu and Michael Mitzenmacher and Minlan Yu},
title = {{THC}: Accelerating Distributed Deep Learning Using Tensor Homomorphic Compression},
booktitle = {21st USENIX Symposium on Networked Systems Design and Implementation (NSDI 24)},
year = {2024},
isbn = {978-1-939133-39-7},
address = {Santa Clara, CA},
pages = {1191--1211},
url = {https://www.usenix.org/conference/nsdi24/presentation/li-minghao},
publisher = {USENIX Association},
month = apr
}


@inproceedings{zeroquant2022yao,
 author = {Yao, Zhewei and Yazdani Aminabadi, Reza and Zhang, Minjia and Wu, Xiaoxia and Li, Conglong and He, Yuxiong},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {27168--27183},
 publisher = {Curran Associates, Inc.},
 title = {ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/adf7fa39d65e2983d724ff7da57f00ac-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}


@article{frantar2022gptq,
  title={Gptq: Accurate post-training quantization for generative pre-trained transformers},
  author={Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan},
  journal={arXiv preprint arXiv:2210.17323},
  year={2022}
}



@InProceedings{kbit2023dettmers,
  title = 	 {The case for 4-bit precision: k-bit Inference Scaling Laws},
  author =       {Dettmers, Tim and Zettlemoyer, Luke},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {7750--7774},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/dettmers23a/dettmers23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/dettmers23a.html},
}

@article{zeroquantv2-2023yao,
  title={A comprehensive study on post-training quantization for large language models},
  author={Yao, Zhewei and Li, Cheng and Wu, Xiaoxia and Youn, Stephen and He, Yuxiong},
  journal={arXiv preprint arXiv:2303.08302},
  year={2023}
}



@inproceedings{xiao2023smoothquant,
  title={Smoothquant: Accurate and efficient post-training quantization for large language models},
  author={Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Wu, Hao and Demouth, Julien and Han, Song},
  booktitle={International Conference on Machine Learning},
  pages={38087--38099},
  year={2023},
  organization={PMLR}
}



@InProceedings{deepspeed-moe,
  title = 	 {{D}eep{S}peed-{M}o{E}: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation {AI} Scale},
  author =       {Rajbhandari, Samyam and Li, Conglong and Yao, Zhewei and Zhang, Minjia and Aminabadi, Reza Yazdani and Awan, Ammar Ahmad and Rasley, Jeff and He, Yuxiong},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {18332--18346},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/rajbhandari22a/rajbhandari22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/rajbhandari22a.html},
}





@article{geva2020transformer,
  title={Transformer feed-forward layers are key-value memories},
  author={Geva, Mor and Schuster, Roei and Berant, Jonathan and Levy, Omer},
  journal={arXiv preprint arXiv:2012.14913},
  year={2020}
}


@misc{arxiv-summarization,
      title={A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents}, 
      author={Arman Cohan and Franck Dernoncourt and Doo Soon Kim and Trung Bui and Seokhwan Kim and Walter Chang and Nazli Goharian},
      year={2018},
      eprint={1804.05685},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1804.05685}, 
}


@inproceedings{learned-token2022kim,
author = {Kim, Sehoon and Shen, Sheng and Thorsley, David and Gholami, Amir and Kwon, Woosuk and Hassoun, Joseph and Keutzer, Kurt},
title = {Learned Token Pruning for Transformers},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539260},
doi = {10.1145/3534678.3539260},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {784–794},
numpages = {11},
keywords = {deep learning, natural language processing, network pruning},
location = {Washington DC, USA},
series = {KDD '22}
}


@inproceedings {orca2022yu,
author = {Gyeong-In Yu and Joo Seong Jeong and Geon-Woo Kim and Soojeong Kim and Byung-Gon Chun},
title = {Orca: A Distributed Serving System for {Transformer-Based} Generative Models},
booktitle = {16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)},
year = {2022},
isbn = {978-1-939133-28-1},
address = {Carlsbad, CA},
pages = {521--538},
url = {https://www.usenix.org/conference/osdi22/presentation/yu},
publisher = {USENIX Association},
month = jul
}


https://anonymous.4open.science/r/ZeroC


@misc{zeroc-code,
      title={{ZeroC} Code},
      year={2024},
      howpublished = {\url{https://anonymous.4open.science/r/ZeroC}},
}


@misc{github-copilot,
      title={{GitHub Copilot}},
      year={2024},
      howpublished = {\url{https://github.com/features/copilot/}},
}

