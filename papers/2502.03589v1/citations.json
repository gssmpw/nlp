[
  {
    "index": 0,
    "papers": [
      {
        "key": "thc2024li",
        "author": "Minghao Li and Ran Ben Basat and Shay Vargaftik and ChonLam Lao and Kevin Xu and Michael Mitzenmacher and Minlan Yu",
        "title": "{THC}: Accelerating Distributed Deep Learning Using Tensor Homomorphic Compression"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "distserve",
        "author": "Yinmin Zhong and Shengyu Liu and Junda Chen and Jianbo Hu and Yibo Zhu and Xuanzhe Liu and Xin Jin and Hao Zhang",
        "title": "{DistServe}: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving"
      },
      {
        "key": "splitwise",
        "author": "Patel, Pratyush and Choukse, Esha and Zhang, Chaojie and Shah, Aashaka and Goiri, \u00cd\u00f1igo and Maleki, Saeed and Bianchini, Ricardo",
        "title": "Splitwise: Efficient Generative LLM Inference Using Phase Splitting"
      },
      {
        "key": "mooncake",
        "author": "Ruoyu Qin and Zheming Li and Weiran He and Mingxing Zhang and Yongwei Wu and Weimin Zheng and Xinran Xu",
        "title": "Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving"
      },
      {
        "key": "memserve",
        "author": "Cunchen Hu and Heyang Huang and Junhao Hu and Jiang Xu and Xusheng Chen and Tao Xie and Chenxi Wang and Sa Wang and Yungang Bao and Ninghui Sun and Yizhou Shan",
        "title": "MemServe: Context Caching for Disaggregated LLM Serving with Elastic Memory Pool"
      },
      {
        "key": "strati2024dejavukvcachestreamingfast",
        "author": "Strati, Foteini and Mcallister, Sara and Phanishayee, Amar and Tarnawski, Jakub and Klimovic, Ana",
        "title": "D\u00e9j\u00e0Vu: {KV}-cache Streaming for Fast, Fault-tolerant Generative {LLM} Serving"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "distserve",
        "author": "Yinmin Zhong and Shengyu Liu and Junda Chen and Jianbo Hu and Yibo Zhu and Xuanzhe Liu and Xin Jin and Hao Zhang",
        "title": "{DistServe}: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving"
      },
      {
        "key": "splitwise",
        "author": "Patel, Pratyush and Choukse, Esha and Zhang, Chaojie and Shah, Aashaka and Goiri, \u00cd\u00f1igo and Maleki, Saeed and Bianchini, Ricardo",
        "title": "Splitwise: Efficient Generative LLM Inference Using Phase Splitting"
      },
      {
        "key": "strati2024dejavukvcachestreamingfast",
        "author": "Strati, Foteini and Mcallister, Sara and Phanishayee, Amar and Tarnawski, Jakub and Klimovic, Ana",
        "title": "D\u00e9j\u00e0Vu: {KV}-cache Streaming for Fast, Fault-tolerant Generative {LLM} Serving"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "mooncake",
        "author": "Ruoyu Qin and Zheming Li and Weiran He and Mingxing Zhang and Yongwei Wu and Weimin Zheng and Xinran Xu",
        "title": "Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving"
      },
      {
        "key": "memserve",
        "author": "Cunchen Hu and Heyang Huang and Junhao Hu and Jiang Xu and Xusheng Chen and Tao Xie and Chenxi Wang and Sa Wang and Yungang Bao and Ninghui Sun and Yizhou Shan",
        "title": "MemServe: Context Caching for Disaggregated LLM Serving with Elastic Memory Pool"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "zipcache",
        "author": "Yefei He and Luoming Zhang and Weijia Wu and Jing Liu and Hong Zhou and Bohan Zhuang",
        "title": "ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification"
      },
      {
        "key": "kang2024gear",
        "author": "Hao Kang and Qingru Zhang and Souvik Kundu and Geonhwa Jeong and Zaoxing Liu and Tushar Krishna and Tuo Zhao",
        "title": "{GEAR}: An Efficient {KV} Cache Compression Recipe for Near-Lossless Generative Inference of {LLM}"
      },
      {
        "key": "kivi",
        "author": "Liu, Zirui and Yuan, Jiayi and Jin, Hongye and Zhong, Shaochen and Xu, Zhaozhuo and Braverman, Vladimir and Chen, Beidi and Hu, Xia",
        "title": "{KIVI}: A Tuning-Free Asymmetric 2bit Quantization for {KV} Cache"
      },
      {
        "key": "cachegen",
        "author": "Liu, Yuhan and Li, Hanchen and Cheng, Yihua and Ray, Siddhant and Huang, Yuyang and Zhang, Qizheng and Du, Kuntai and Yao, Jiayi and Lu, Shan and Ananthanarayanan, Ganesh and Maire, Michael and Hoffmann, Henry and Holtzman, Ari and Jiang, Junchen",
        "title": "CacheGen: KV Cache Compression and Streaming for Fast Large Language Model Serving"
      },
      {
        "key": "kvquant",
        "author": "Coleman Richard Charles Hooper and Sehoon Kim and Hiva Mohammadzadeh and Michael W. Mahoney and Sophia Shao and Kurt Keutzer and Amir Gholami",
        "title": "{KVQ}uant: Towards 10 Million Context Length {LLM} Inference with {KV} Cache Quantization"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "turboattention",
        "author": "Hao Kang and Srikant Bharadwaj and James Hensman and Tushar Krishna and Victor Ruhle and Saravan Rajmohan",
        "title": "TurboAttention: Efficient Attention Approximation For High Throughputs LLMs"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "turboattention",
        "author": "Hao Kang and Srikant Bharadwaj and James Hensman and Tushar Krishna and Victor Ruhle and Saravan Rajmohan",
        "title": "TurboAttention: Efficient Attention Approximation For High Throughputs LLMs"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "turboattention",
        "author": "Hao Kang and Srikant Bharadwaj and James Hensman and Tushar Krishna and Victor Ruhle and Saravan Rajmohan",
        "title": "TurboAttention: Efficient Attention Approximation For High Throughputs LLMs"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "turboattention",
        "author": "Hao Kang and Srikant Bharadwaj and James Hensman and Tushar Krishna and Victor Ruhle and Saravan Rajmohan",
        "title": "TurboAttention: Efficient Attention Approximation For High Throughputs LLMs"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "turboattention",
        "author": "Hao Kang and Srikant Bharadwaj and James Hensman and Tushar Krishna and Victor Ruhle and Saravan Rajmohan",
        "title": "TurboAttention: Efficient Attention Approximation For High Throughputs LLMs"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "turboattention",
        "author": "Hao Kang and Srikant Bharadwaj and James Hensman and Tushar Krishna and Victor Ruhle and Saravan Rajmohan",
        "title": "TurboAttention: Efficient Attention Approximation For High Throughputs LLMs"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "turboattention",
        "author": "Hao Kang and Srikant Bharadwaj and James Hensman and Tushar Krishna and Victor Ruhle and Saravan Rajmohan",
        "title": "TurboAttention: Efficient Attention Approximation For High Throughputs LLMs"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "h2o2023zhang",
        "author": "Zhang, Zhenyu and Sheng, Ying and Zhou, Tianyi and Chen, Tianlong and Zheng, Lianmin and Cai, Ruisi and Song, Zhao and Tian, Yuandong and R\\'{e}, Christopher and Barrett, Clark and Wang, Zhangyang \"Atlas\" and Chen, Beidi",
        "title": "H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models"
      },
      {
        "key": "ge2023model",
        "author": "Ge, Suyu and Zhang, Yunan and Liu, Liyuan and Zhang, Minjia and Han, Jiawei and Gao, Jianfeng",
        "title": "Model tells you what to discard: Adaptive kv cache compression for llms"
      },
      {
        "key": "scissorhands2023liu",
        "author": "Liu, Zichang and Desai, Aditya and Liao, Fangshuo and Wang, Weitao and Xie, Victor and Xu, Zhaozhuo and Kyrillidis, Anastasios and Shrivastava, Anshumali",
        "title": "Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time"
      },
      {
        "key": "pyramidinfer",
        "author": "Dongjie Yang and XiaoDong Han and Yan Gao and Yao Hu and Shilin Zhang and Hai Zhao",
        "title": "PyramidInfer: Pyramid KV Cache Compression for High-throughput LLM Inference"
      },
      {
        "key": "l2norm-kv",
        "author": "Devoto, Alessio  and\nZhao, Yu  and\nScardapane, Simone  and\nMinervini, Pasquale",
        "title": "A Simple and Effective $L\\_2$ Norm-Based Strategy for {KV} Cache Compression"
      },
      {
        "key": "keyformer",
        "author": "Adnan, Muhammad and Arunkumar, Akhil and Jain, Gaurav and Nair, Prashant and Soloveychik, Ilya and Kamath, Purushotham",
        "title": "Keyformer: KV Cache reduction through key tokens selection for Efficient Generative Inference"
      },
      {
        "key": "dynamic-context-pruning",
        "author": "Anagnostidis, Sotiris and Pavllo, Dario and Biggio, Luca and Noci, Lorenzo and Lucchi, Aurelien and Hofmann, Thomas",
        "title": "Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers"
      },
      {
        "key": "infinigen",
        "author": "Wonbeom Lee and Jungi Lee and Junghwan Seo and Jaewoong Sim",
        "title": "{InfiniGen}: Efficient Generative Inference of Large Language Models with Dynamic {KV} Cache Management"
      },
      {
        "key": "zhang2024efficientsparseattentionneeds",
        "author": "Chaoran Zhang and Lixin Zou and Dan Luo and Min Tang and Xiangyang Luo and Zihao Li and Chenliang Li",
        "title": "Efficient Sparse Attention needs Adaptive Token Release"
      },
      {
        "key": "jiang2024minference",
        "author": "Huiqiang Jiang and YUCHENG LI and Chengruidong Zhang and Qianhui Wu and Xufang Luo and Surin Ahn and Zhenhua Han and Amir H. Abdi and Dongsheng Li and Chin-Yew Lin and Yuqing Yang and Lili Qiu",
        "title": "{MI}nference 1.0: Accelerating Pre-filling for Long-Context {LLM}s via Dynamic Sparse Attention"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "turboattention",
        "author": "Hao Kang and Srikant Bharadwaj and James Hensman and Tushar Krishna and Victor Ruhle and Saravan Rajmohan",
        "title": "TurboAttention: Efficient Attention Approximation For High Throughputs LLMs"
      }
    ]
  }
]