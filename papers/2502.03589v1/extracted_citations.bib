@inproceedings{cachegen,
author = {Liu, Yuhan and Li, Hanchen and Cheng, Yihua and Ray, Siddhant and Huang, Yuyang and Zhang, Qizheng and Du, Kuntai and Yao, Jiayi and Lu, Shan and Ananthanarayanan, Ganesh and Maire, Michael and Hoffmann, Henry and Holtzman, Ari and Jiang, Junchen},
title = {CacheGen: KV Cache Compression and Streaming for Fast Large Language Model Serving},
year = {2024},
isbn = {9798400706141},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3651890.3672274},
doi = {10.1145/3651890.3672274},
abstract = {As large language models (LLMs) take on complex tasks, their inputs are supplemented with longer contexts that incorporate domain knowledge. Yet using long contexts is challenging as nothing can be generated until the whole context is processed by the LLM. While the context-processing delay can be reduced by reusing the KV cache of a context across different inputs, fetching the KV cache, which contains large tensors, over the network can cause high extra network delays.CacheGen is a fast context-loading module for LLM systems. First, CacheGen uses a custom tensor encoder, leveraging KV cache's distributional properties to encode a KV cache into more compact bitstream representations with negligible decoding overhead, to save bandwidth usage. Second, CacheGen adapts the compression level of different parts of a KV cache to cope with changes in available bandwidth, in order to maintain low context-loading delay and high generation quality. We test CacheGen on popular LLMs and datasets. Compared to the recent systems that reuse the KV cache, CacheGen reduces the KV cache size by 3.5--4.3x and the total delay in fetching and processing contexts by 3.2--3.7x with negligible impact on the LLM response quality. Our code is at: https://github.com/UChi-JCL/CacheGen.},
booktitle = {Proceedings of the ACM SIGCOMM 2024 Conference},
pages = {38–56},
numpages = {19},
keywords = {large language models, KV cache, compression},
location = {Sydney, NSW, Australia},
series = {ACM SIGCOMM '24}
}

@inproceedings{dynamic-context-pruning,
 author = {Anagnostidis, Sotiris and Pavllo, Dario and Biggio, Luca and Noci, Lorenzo and Lucchi, Aurelien and Hofmann, Thomas},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {65202--65223},
 publisher = {Curran Associates, Inc.},
 title = {Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/cdaac2a02c4fdcae77ba083b110efcc3-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}

@article{ge2023model,
  title={Model tells you what to discard: Adaptive kv cache compression for llms},
  author={Ge, Suyu and Zhang, Yunan and Liu, Liyuan and Zhang, Minjia and Han, Jiawei and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2310.01801},
  year={2023}
}

@inproceedings{h2o2023zhang,
 author = {Zhang, Zhenyu and Sheng, Ying and Zhou, Tianyi and Chen, Tianlong and Zheng, Lianmin and Cai, Ruisi and Song, Zhao and Tian, Yuandong and R\'{e}, Christopher and Barrett, Clark and Wang, Zhangyang "Atlas" and Chen, Beidi},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Neumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {34661--34710},
 publisher = {Curran Associates, Inc.},
 title = {H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/6ceefa7b15572587b78ecfcebb2827f8-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}

@article{kang2024gear,
      title={{GEAR}: An Efficient {KV} Cache Compression Recipe for Near-Lossless Generative Inference of {LLM}}, 
      author={Hao Kang and Qingru Zhang and Souvik Kundu and Geonhwa Jeong and Zaoxing Liu and Tushar Krishna and Tuo Zhao},
      year={2024},
      eprint={2403.05527},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      journal={arXiv preprint arXiv:2403.05527}
}

@inproceedings{keyformer,
 author = {Adnan, Muhammad and Arunkumar, Akhil and Jain, Gaurav and Nair, Prashant and Soloveychik, Ilya and Kamath, Purushotham},
 booktitle = {Proceedings of Machine Learning and Systems},
 editor = {P. Gibbons and G. Pekhimenko and C. De Sa},
 pages = {114--127},
 title = {Keyformer: KV Cache reduction through key tokens selection for Efficient Generative Inference},
 url = {https://proceedings.mlsys.org/paper_files/paper/2024/file/48fecef47b19fe501d27d338b6d52582-Paper-Conference.pdf},
 volume = {6},
 year = {2024}
}

@InProceedings{kivi,
  title = 	 {{KIVI}: A Tuning-Free Asymmetric 2bit Quantization for {KV} Cache},
  author =       {Liu, Zirui and Yuan, Jiayi and Jin, Hongye and Zhong, Shaochen and Xu, Zhaozhuo and Braverman, Vladimir and Chen, Beidi and Hu, Xia},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {32332--32344},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/liu24bz/liu24bz.pdf},
  url = 	 {https://proceedings.mlr.press/v235/liu24bz.html},
}

@inproceedings{l2norm-kv,
    title = "A Simple and Effective $L\_2$ Norm-Based Strategy for {KV} Cache Compression",
    author = "Devoto, Alessio  and
      Zhao, Yu  and
      Scardapane, Simone  and
      Minervini, Pasquale",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.1027",
    doi = "10.18653/v1/2024.emnlp-main.1027",
    pages = "18476--18499",
}

@misc{memserve,
      title={MemServe: Context Caching for Disaggregated LLM Serving with Elastic Memory Pool}, 
      author={Cunchen Hu and Heyang Huang and Junhao Hu and Jiang Xu and Xusheng Chen and Tao Xie and Chenxi Wang and Sa Wang and Yungang Bao and Ninghui Sun and Yizhou Shan},
      year={2024},
      eprint={2406.17565},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2406.17565}, 
}

@misc{mooncake,
      title={Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving}, 
      author={Ruoyu Qin and Zheming Li and Weiran He and Mingxing Zhang and Yongwei Wu and Weimin Zheng and Xinran Xu},
      year={2024},
      eprint={2407.00079},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2407.00079}, 
}

@misc{pyramidinfer,
      title={PyramidInfer: Pyramid KV Cache Compression for High-throughput LLM Inference}, 
      author={Dongjie Yang and XiaoDong Han and Yan Gao and Yao Hu and Shilin Zhang and Hai Zhao},
      year={2024},
      eprint={2405.12532},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.12532}, 
}

@inproceedings{scissorhands2023liu,
 author = {Liu, Zichang and Desai, Aditya and Liao, Fangshuo and Wang, Weitao and Xie, Victor and Xu, Zhaozhuo and Kyrillidis, Anastasios and Shrivastava, Anshumali},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Neumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {52342--52364},
 publisher = {Curran Associates, Inc.},
 title = {Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/a452a7c6c463e4ae8fbdc614c6e983e6-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}

@INPROCEEDINGS{splitwise,
  author={Patel, Pratyush and Choukse, Esha and Zhang, Chaojie and Shah, Aashaka and Goiri, Íñigo and Maleki, Saeed and Bianchini, Ricardo},
  booktitle={2024 ACM/IEEE 51st Annual International Symposium on Computer Architecture (ISCA)}, 
  title={Splitwise: Efficient Generative LLM Inference Using Phase Splitting}, 
  year={2024},
  volume={},
  number={},
  pages={118-132},
  keywords={Costs;Processor scheduling;Large language models;Computational modeling;Graphics processing units;Computer architecture;Throughput;Large language models;Cluster deployments;Scheduling;GPUs;Inference efficiency;Machine learning;Resource management},
  doi={10.1109/ISCA59077.2024.00019}}

@InProceedings{strati2024dejavukvcachestreamingfast,
  title = 	 {DéjàVu: {KV}-cache Streaming for Fast, Fault-tolerant Generative {LLM} Serving},
  author =       {Strati, Foteini and Mcallister, Sara and Phanishayee, Amar and Tarnawski, Jakub and Klimovic, Ana},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {46745--46771},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/strati24a/strati24a.pdf},
  url = 	 {https://proceedings.mlr.press/v235/strati24a.html},
  abstract = 	 {Distributed LLM serving is costly and often underutilizes hardware accelerators due to three key challenges: bubbles in pipeline-parallel deployments caused by the bimodal latency of prompt and token processing, GPU memory overprovisioning, and long recovery times in case of failures. DéjàVu addresses all these challenges using a versatile and efficient KV cache streaming library (DéjàVuLib). Using DéjàVuLib, we propose and implement efficient prompt-token disaggregation to reduce pipeline bubbles, microbatch swapping for efficient GPU memory management, and state replication for fault-tolerance. We highlight the efficacy of these solutions on a range of large models across cloud deployments.}
}

@inproceedings{thc2024li,
author = {Minghao Li and Ran Ben Basat and Shay Vargaftik and ChonLam Lao and Kevin Xu and Michael Mitzenmacher and Minlan Yu},
title = {{THC}: Accelerating Distributed Deep Learning Using Tensor Homomorphic Compression},
booktitle = {21st USENIX Symposium on Networked Systems Design and Implementation (NSDI 24)},
year = {2024},
isbn = {978-1-939133-39-7},
address = {Santa Clara, CA},
pages = {1191--1211},
url = {https://www.usenix.org/conference/nsdi24/presentation/li-minghao},
publisher = {USENIX Association},
month = apr
}

@misc{turboattention,
      title={TurboAttention: Efficient Attention Approximation For High Throughputs LLMs}, 
      author={Hao Kang and Srikant Bharadwaj and James Hensman and Tushar Krishna and Victor Ruhle and Saravan Rajmohan},
      year={2024},
      eprint={2412.08585},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2412.08585}, 
}

@misc{zhang2024efficientsparseattentionneeds,
      title={Efficient Sparse Attention needs Adaptive Token Release}, 
      author={Chaoran Zhang and Lixin Zou and Dan Luo and Min Tang and Xiangyang Luo and Zihao Li and Chenliang Li},
      year={2024},
      eprint={2407.02328},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.02328}, 
}

@misc{zipcache,
      title={ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification}, 
      author={Yefei He and Luoming Zhang and Weijia Wu and Jing Liu and Hong Zhou and Bohan Zhuang},
      year={2024},
      eprint={2405.14256},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.14256}, 
}

