\section{Related Work}
\label{sec:relatedwork}

\begin{figure}[b]
    \centering
    \includegraphics[width=0.82\linewidth]{Figure/RelatedWork/self-instruct.eps}
    \caption{Overall organization and operations of Self-Instruct}
    \label{fig:self-instruct}
\end{figure}

\begin{figure*}[t!]% 
	\centering
	\includegraphics[width=0.9\linewidth]{Figure/Method/overall-process.eps}
	\caption{Overall organization and operations of \ours{}}
	\label{fig:sedi-instruct}%
\end{figure*}
\subsection{Instruction Tuning}
Instruction tuning aims to enhance the performance of pretrained LLMs by
fine-tuning. Typically, a pretrained LLM such as **Brown et al., "Language Models are Few-Shot Learners"**, **RoBERTa**____,
and **LLaMA-3**____ generates outputs based
on statistical patterns learned from vast amounts of text data, predicting the
next word in a sequence. However, these predictions often differ from user
expectations because the model selects the highest-probability token rather than
generating a response tailored to the specific request.







Instruction tuning addresses this issue by aligning model outputs more closely
with human expectations. Specifically, to ensure that the model works as
expected, 
fine-tuning is conducted using 
an instruction dataset that consists of input queries and desired responses.
Notably, instruction-tuned models like **Petroni et al., "Language Models as Knowledge Bases"**____,
**InstructGPT**____, and **gemma-it**____ have significantly
improved zero-shot performance. This indicates that instruction tuning enables
models to better leverage the existing knowledge embedded within their parameters
from pretraining.



Despite its potential, instruction tuning encounters significant challenges,
particularly related to data scarcity. Effective instruction tuning demands
large, diverse, and high-quality instruction datasets to ensure learning across
various contexts. However, the availability of such datasets is often limited,
impeding the model's ability to generalize and perform well on unseen tasks.


\subsection{Overcoming Challenges of Limited Datasets}
One effective solution for overcoming data scarcity is data augmentation.
% While commonly used in computer vision tasks through methods such as cropping,
% flipping, and rotating images, data augmentation is also effective for LLM.
Back translation____ translates text data into
another language and then back to the original language, generating textual data
with different words while preserving the original context. **Jason Wei and Kai Zou**, proposed a straightforward data augmentation technique called
Easy Data Augmentation (EDA), which consists of four simple operations: synonym
replacement, random insertion, random swap, and random deletion. These
techniques help augment datasets, improving model performance____. However,
replicating the dataset may not sufficiently improve task and context diversity.

One fundamental solution to the data collection challenge is leveraging
LLMs for data synthesis. 
The representative framework is **Brown et al., "Language Models are Few-Shot Learners"**____. 
As shown in \FIG{fig:self-instruct},
Self-Instruct utilizes prompt engineering
and produces generated instructions ($\mathbb{G}$) by feeding
a randomly chosen subset ($\mathbb{S}'$) of seed instructions ($\mathbb{S}$) 
to ChatGPT API.
The generated instructions are then subjected
to a filtering process that compares the ROUGE-L similarity
with kept instructions ($\mathbb{K}$).
If the similarity between a newly generated
instruction and kept instructions exceeds
a specified threshold (indicating
that the new instruction is %too
similar to those already in the dataset),
the instruction will be excluded from the training dataset.
Otherwise, it is included in $\mathbb{K}$ as a new unique instruction.
This approach %not only
alleviates data scarcity %but also 
and improves
the diversity of the synthesized datasets, making it a promising strategy for
training robust AI models.

However, the instruction dataset synthesized by the Self-Instruct,
such as the **ALPACA 52K dataset**____, often contains a significant amount of 
inaccurate or irrelevant responses, leading to low-quality data.
This is due to the lack of a validation or feedback process beyond
simple similarity-based filtering. 
As a result, there is a strong need 
for a post-validation process to improve the quality of instructions.

% Alpagasus 논문에서 LLM filter라는 용어를 활용했기에 ChatGPT 대신 LLM filter라고 했습니다.
Alpagasus____ addresses this issue 
by using LLM filters (e.g., **ChatGPT**)
to further filter out the kept instructions of the ALPACA 52K dataset,
creating a refined dataset of 9K high-quality entries. 
Remarkably, this smaller and high-quality
dataset achieves performance equal to or better than the original 52K dataset.
Additionally, **Feng et al.**, demonstrated that by
filtering out 87.5\% of a synthesized dataset using human verifiers, the resulting
model outperforms one trained on the entire dataset.

\begin{comment}
\sj{(아래 문장의 경우 이전 설명과 많이 겹쳐서 특별히 필요 없을 듯 합니다.
대신에 Fig. 2를 다를 위치에 넣어서 과도한 API invocation 문제는 언급해야할듯합니다.)}
\fixme{
Moreover, the ROUGE-L similarity-based data filtering process of 
the Self-Instruct methodology also involves an excessive number of ChatGPT 
API requests. This filtering process adds a new instruction to the task pool only if its ROUGE-L
similarity with any existing instructions is less than 0.7 \review{(the number is
empirically set in Self-Instruct).} While this approach effectively encourages
diversity, it results in the elimination of over half of the generated instructions.
\FIG{fig:api-inefficiency} illustrates the actual number of generated data
points over time compared to the number of ChatGPT API requests made. Initially,
most of the generated instructions are kept. However, as the number of existing
instructions increases, more than half of the new instructions are filtered out.
This trend becomes increasingly severe as the amount of data grows, highlighting
inefficiencies within the filtering process. Consequently, this inefficiency
leads to high resource and cost consumption.
}
\end{comment}