\section{Introduction}
\label{sec:intro}

% 8.16 10:25 - 남은 작업
% 중요: Appendix 작성 완료하기, 분량 맞추기 (1/4p 넘침), reference 확인하기, 
% 마이너: 그림에 수식 들어가있는 것들 글씨 드래그 안됨
% 완료: XX 되어있는 것들 채우기, 숫자 확인 하기, 그림 최대한 줄이기

Many novel Artificial Intelligence (AI)-based services have been emerging in the
wake of the Large Language Models (LLMs). ChatGPT, a representative LLM-based
service, recorded approximately 1.6 billion visitors worldwide in December 2023
and was reported to be used by more than 100 million people
weekly~\cite{chatgpt, chatgptuser}. Additionally, many companies have already
introduced AI or are considering introducing it. This suggests that AI is going
beyond mere technological innovation and is fundamentally changing various
industries.

To effectively leverage AI technologies in the industry, a substantial amount of
domain-specific data is essentially required, 
as the performance of AI models heavily
depends on the quantity and quality of data.
However, it is challenging to obtain data that reflects diverse
real-world scenarios and that is tailored to specific domains. 
Additionally, security, privacy, and ethics issues further 
complicate data collection efforts. The average
cost of assembling a dataset is prohibitively high.
For example, the price of high-quality language
datasets is up to \$0.15/word~\cite{datasetprice}.

To address the challenges of data collection, various
techniques such as data augmentation~\cite{backtranslation,eda,self-instruct}, data-efficient
training~\cite{data-efficient}, and transfer learning~\cite{torrey2010transfer} have been proposed.
%\cancel{Recently, a novel approach called Self-Instruct has been introduced.}
Among these, Self-Instruct, which automatically generates 
large amounts of datasets, has received significant attention 
for its ability to mitigate data shortages.
%\cancel{Self-Instruct mitigates data shortages by generating additional data through the
%following steps:}
Self-Instruct operates in the following three steps:
%\cancel{First, leveraging the seed data created by humans, ChatGPT
%generates the data.}
The first step is a generation phase. 
By supplying \textit{seed instructions} prepared by humans
to LLM services like ChatGPT,
it automatically synthesizes a set of instructions,
which are called \textit{generated instructions}.
%\cancel{The second step is a filtering phase to eliminate
%duplicates, ensuring the diversity of the generated data.}
The second step is a filtering phase, which
% quality가 나쁘다 = 이전 것과 similar하다 를 나타기 위해 추가
eliminates duplicates and useless ones 
(that negatively impact training efficiency)
from the generated instructions.
Through this step, only meaningful instructions, 
called \textit{kept instructions}, are selectively chosen and 
kept for future use.
%which ensures the diversity of the generated instructions.
%\cancel{After filtering, the instructions that are not eliminated
%and remain are referred to as kept instructions, which make up the dataset.}
%\cancel{Lastly, once the
%dataset is sufficiently expanded and refined, the training of AI models
%commences.}
The final step is a training phase, where AI models are trained
using the kept instructions. %which are sufficiently refined.
Self-Instruct can mitigate the difficulties associated with
domain-specific and large-scale instruction data collection by automating data generation.





Despite its potential, Self-Instruct does have limitations
that need to be addressed.
% One significant limitation of Self-Instruct is the quality of the generated data.
One significant limitation of Self-Instruct is 
\textit{low data quality}.
A substantial portion of the generated data (or instructions) is inaccurate,
compromising the overall effectiveness of instruction tuning. 
%\review{For example, instructions that are similar to previously kept ones are 
%considered low-quality since they do not add new learning opportunities for the model.}
Prior studies have shown that
a model trained using only 10\% of the kept instructions
achieves similar or even better accuracy
than one trained using the entire kept instructions~\cite{alpagasus}.
This indicates that a considerable amount of the kept instructions is 
inaccurate and is not effective in training models,
highlighting the need for improved
data generation and validation processes to enhance the
quality of the dataset.

Another limitation of Self-Instruct is \textit{filtering inefficiency}. 
To improve the quality of kept instructions used for training,
Self-Instruct discards a significant percentage of generated instructions
by performing the filtering process. This inevitably increases the number of 
useless ChatGPT API calls that are actually not needed.
\FIG{fig:api-inefficiency}(a) illustrates the ratio of kept instructions 
to the number of ChatGPT API requests we actually made over time.
\FIG{fig:api-inefficiency}(b) presents the accumulated numbers of kept and 
generated instructions for \FIG{fig:api-inefficiency}(a).
To create 10K kept instructions,
Self-Instruct needs to generate 24K
instructions, meaning that approximately 58\% of the generated data
is abandoned~\cite{self-instruct}.
This trend becomes increasingly severe as the amount of data grows,
showing inefficiencies in the filtering process.
This implies that the excessive number of ChatGPT API invocations
is unnecessary, 
resulting in a waste of computing resources in data centers 
and higher operational expenses.
Consequently, more efficient filtering mechanisms are highly needed.
\begin{figure}[t]% 그림 글씨 크기 키우기
	\centering
	\subfloat[Ratio of kept instructions to API requests]{\includegraphics[width=0.428\linewidth]{Figure/RelatedWork/kept-instruction-ratio.eps}}
	\hspace{3pt}
	\subfloat[Comparison of \# of generated instructions and kept]{\includegraphics[width=0.442\linewidth]{Figure/RelatedWork/compare-num-instruction.eps}}
	\caption{Filtering inefficiency problem}%
	\label{fig:api-inefficiency}%
\end{figure}




In this paper, we propose a novel data generation
framework, \textbf{Se}lf-\textbf{Di}rect
\textbf{Instruct}ion generation, \ours{} in short.
As illustrated in \FIG{fig:objective}, the ultimate goal of \ours{}
is to generate high-quality instructions at low costs, beating other %conventional
data generation approaches~\cite{self-instruct,sitgpt4,alpaca}
that require high data collection costs (e.g., manual collection~\cite{llama3}) or 
produce low-quality data (e.g., data augmentation~\cite{backtranslation, eda}).
\ours{} achieves this by effectively addressing 
the issues of low data quality and filtering inefficiency.

The design of \ours{} is based on two insights derived from prior literature.
First, \ours{} tackles the low data quality problem
by leveraging the inherent properties of few-shot learning, 
which Self-Instruct is based on. 
%\sj{Prior studies have heavily relied on human effort. 
%After data generation, they
%include human feedback-based filtering as the post-validation process 
%to obtain high-quality kept instructions~\cite{modelcollapse1}.}
We focus on the fact that the performance
% (이제 맞는 표현일까요?) -> MetaGAN (NeurIPS'18): [...] can help boost the performance of few-shot learning. 그 외에도 많이 쓰이는 표현
of the few-shot learning is
directly influenced by the quality of the source data,
which corresponds to the seed instructions in Self-Instruct.
\ours{} improves the quality of seed instructions by employing
an \textit{iterative feedback task generation technique}
that integrates the training and generation processes.
During training, \ours{} extracts measures indicating the quality of 
the seed data and uses these to refine and update existing seed
instructions without any human involvement.
In this way, \ours{} continually provides high-quality seed data
for instruction tuning.

\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{Figure/Method/our-objective.eps}
    \caption{Goal of \ours{}}
    \label{fig:objective}
\end{figure}

Second, we address the filtering inefficiency problem
by balancing the distribution
of kept instructions over batches.
Previous studies reported that even if
a training dataset is relatively skewed (that is,
the label or task distribution of the training dataset is imbalanced),
we can minimize the negative impact by making each batch 
have balanced data~\cite{8215530}.
Keeping this property in mind, \ours{} employs a \textit{diversity-based filtering
technique}. \ours{} generates instructions in the same manner
as Self-Instruct but generously accepts (low-quality) instructions 
that are moderately similar to previously kept ones.
%(뭔가 이전것과 비슷한것이 low-quality라는 언급이 있어야 정확할 듯)}
Instead, improving the diversity of instructions 
within a batch can maintain comparable accuracy
without discarding many of the generated instructions. As a result,
it lowers the cost of generating instructions.


To evaluate \ours{}, we train two models using Llama-3-8B:
one utilizing \ours{} and the other leveraging the Self-Instruct.
To benchmark our models against the ideal scenario where Llama-3-8B
operates at its maximum potential,
we compare \ours{} with the Llama-3-8B-Instruct model that is
tuned with 10M human-written instructions.
We also include the Falcon-7B-Instruct and Gemma-7B-Instruct
models, whose parameter sizes are similar to \ours{}.
Our results show that the model trained with \ours{} not only outperforms
the existing models except for Llama-3-8B-Instruct 
but also achieves significant cost efficiency, reducing expenses by up to 36\% compared to Self-Instruct.




\begin{comment}
\fixme{We conduct experiments to evaluate the effectiveness of \ours{}.
We first compare the performance of models trained with \ours{}
against other LLMs of a similar scale across various benchmarks. We then
carry out an in-depth analysis to ensure 
instruction tuning is executed correctly through
competitive evaluation. We finally investigate 
whether the instruction generation of \ours{} is more cost-effective 
than that of Self-Instruct, mainly focusing on
how much the diversity-based filtering reduces costs. 
In Appendix, a more detailed
investigation of the impact of the iterative feedback task generation, along
with other experiment details, are provided. 
All evaluation codes are available on \url{https://github.com/}. }
\end{comment}

