\section{Method}
\label{sec:method}
%\sj{아래 내용이 intro와 좀 겹치는 듯 합니다. intro로 옮기고 여기서 짧게 설명하거나
%혹은 intro를 더 짧게하고 이곳 설명을 유지하거나 둘중 하나를 선택해야할 듯. 일단
%전자가 본 연구의 기여를 먼저 설명하기에 더 좋기는 합니다.}

%The Self-Instruct approach is an alternative to the limited dataset challenge,
%but there are two critical problems: data quality problem and filtering
%inefficiency problem. 
In this section, we present our data generation framework,
\ours{}.
\FIG{fig:sedi-instruct} illustrates 
%\cancel{the overall process and objective of \ours{}.}
the overall organization and operations of \ours{}.
In contrast to Self-Instruct shown in~\FIG{fig:self-instruct},
\ours{} combines the training and generation processes in a pipeline manner
so that the two components interplay to produce better outputs at a lower cost.
Similar to Self-Instruct, \ours{} uses LLMs to generate the 
instructions $\mathbb{G}$ 
using a subset of seed instructions $\mathbb{S}'$ 
that are randomly chosen 
from the entire seed instruction set $\mathbb{S}$
(see \wcircled{1}). 
Diversity-based filtering then composes
a set of kept instructions $\mathbb{K}$
by filtering out useless ones in $\mathbb{G}$ (\wcircled{2}). 
Finally, through the iterative feedback task generation,
\ours{} replaces some seed instructions in $\mathbb{S}$
with new ones in $\mathbb{B}_{n+1}$ which are selected by analyzing 
training logs collected during the training process (\wcircled{3}).
%The up-to-date seeds are later used to create new instructions.
%\review{\cancel{We improve filtering efficiency without negatively impacting 
%model training by employing diversity-based filtering. 
%Simultaneously, we propose an iterative feedback task generation 
%that pipelines the data generation and training processes,
%thereby improving the quality of the seed data based on insights 
%from the training process and enhancing the quality of the generated data.}
%}

In the rest of this section, we describe the diversity-based filtering 
and then present how the iterative feedback task generation operates.

\begin{comment}
\begin{table}[]
\caption{Terminology}
\label{tab:my-table}
\footnotesize
\begin{tabular}{c|l}
Mark & Description \\ \hline
$\mathbb{S}$ & The set of seed instructions \\
$\mathbb{G}$ & The set of instructions generated in a single generation cycle \\
$\mathbb{K}$ & The set of instructions kept in a single generation cycle \\
$\mathbb{D}$ & The set of all instructions kept so far \\
$\mathbb{C}$ & The set of clusters
\end{tabular}
\end{table}
\end{comment}

\begin{comment}
We adopt a strategy that enhances the diversity of batches
by implementing loose ROUGE-L similarity-based filtering. This approach is
designed to improve filtering efficiency without compromising model
performance, achieving low cost.
Our methodology is inspired by research findings
suggesting that increasing the diversity of each batch (local diversity) is
beneficial, especially when the data distribution is skewed, meaning low global
diversity.~\cite{todo, todo}

Simultaneously, to enhance the quality of data generation, we integrate the data
generation and training procedures in a pipeline manner. This integration allows
us to leverage insights gained during the training process to improve the
quality of the generated data. Through this approach, we aim to reduce the
number of API calls required for data generation or human effort for post-validation,
thereby achieving a cost-efficient method while also enhancing the quality of
the data by feedback-based data generation.
\end{comment}



\subsection{Diversity-based Filtering}

Many approaches to enhancing the effectiveness of Self-Instruct have focused on
retaining high-quality instructions~\cite{alpagasus, modelcollapse1}. 
To achieve this, they attempt to eliminate inaccurate and 
redundant instructions through simple heuristics, 
such as ranking the instructions and removing those with low ranks. 
While this is effective, excessive filtering results in many
unnecessary inferences that involve inefficient API usage and resource waste. 
%Our diversity-based filtering aims to reduce the number of eliminated instructions
%while preserving accuracy. 


To mitigate this inefficiency, our diversity-based filtering aims to minimize
discarded instructions by allowing slight redundancy in the kept instructions.
The existing filtering method uses a ROUGE-L similarity threshold of 0.7, 
which is relatively tight,
but we loosen it % this number
to 0.85. With the threshold of 0.7, 
a moderate number of redundant instructions, 
accounting for about half of the generated instructions, are removed.
In our case, with the threshold of 0.85, 
only instructions that exhibit significant redundancy are discarded, 
which accounts for roughly 20\% of the total instructions.


This strategy, however, poses a risk of reducing diversity, potentially leading
to a skewed distribution in the overall dataset. 
This decline in global diversity can degrade 
training efficiency.
We address this problem by enhancing the local diversity within
each batch.
This strategy minimizes the
negative impact of data redundancy on the model, as it helps the loss function
to learn uniformly, thereby improving the stability of the training process.
Consequently, it contributes to better generalization performance, even when
the data distribution is a little biased~\cite{batchdiversity2, batchdiversity3}.

To maximize diversity within each batch, 
we classify the generated instructions into clusters based on their similarity. 
The number of clusters matches the batch size. 
For example, with a batch size of 16, we create 16 clusters, 
each containing similar instructions. When forming a batch, we select one instruction from each cluster, one by one, and include it in the batch.


\begin{algorithm}[t]
\begin{algorithmic}[1]
\caption{Diversity-based filtering}
\label{alg:diversity-filtering}
\footnotesize
\Statex \hspace{-\algorithmicindent} \textbf{Input:} ROUGE-L similarity threshold ($\theta_{ROUGE}$), seed instructions ($\mathbb{S}$), kept instructions ($\mathbb{K}$), Clusters ($\mathbb{C}$)

\Statex \hspace{-\algorithmicindent} \textbf{Output:} Clusters ($\mathbb{C}$)
\vspace{-5pt}
\Statex \hspace{-\algorithmicindent} \rule{230pt}{.2pt}

\State $\mathbb{S}' \gets \text{random.sample}(\mathbb{S},3)$
\State $\mathbb{G} \gets \textsc{GenerateInstruction}(\mathbb{S}')$
\For{\textbf{all} $gen_{i} \in \mathbb{G}$}
    \State $isVariant \gets \text{True}$
    \For{\textbf{all} $kept_{j} \in \mathbb{K}$}
        \State $sim \gets \textsc{ROUGE-L-Similarity}(gen_{i}, kept_{j})$
        \If{$sim > \theta_{ROUGE}$}
            \State $isVariant \gets \text{False}$
            \State \textbf{break}
        \EndIf
    \EndFor
    \If{$isVariant$}
        \State $\mathbb{K}.\text{add}(gen_{i})$
        \State $class \gets \textsc{Classifier}(gen_{i})$
        \State $\mathbb{C}[class].\text{put}(gen_{i})$
    \EndIf
\EndFor
\State \Return $\mathbb{C}$
\end{algorithmic}
\end{algorithm}

\begin{comment}
\ours{} adopts an alleviated ROUGE-score-based filtering technique. Self-Instruct takes
only the instructions when its ROUGE-L similarity with any existing instruction
is less than 0.7. As shown in \FIG{fig:api-inefficiency}, with this restriction, almost 50\% of
generations are eliminated; that is, another 50\% of instructions are unnecessary.
We set the ROUGE-L similarity over 0.85 that is, only 20\% instructions, which are
almost the same instructions as existing instructions, are eliminated. However,
simply lowering ROUGE-L similarity restrictions has a negative impact on the diversity
of the task pool.

To train the diverse context of the generated instruction for the target model,
we make each batch have local diversity. Our filtering mechanism aims to optimize
this process by structuring data batches to minimize the need for extensive
filtering.
\end{comment}

\textbf{Filtering Algorithm}
\begin{comment}
\cancel{Algorithm \ref{alg:diversity-filtering} illustrates our filtering algorithm.
The initial step involves selecting a random set of seed instructions ($I_{seed}$)
and generating a new set of instructions ($I_{gen}$) using a language model
such as ChatGPT. For each generated instruction ($gen$), 
the algorithm computes the ROUGE-L similarity with all previously kept instructions ($I_{kept}$).
If the similarity score exceeds the threshold ($\theta_{ROUGE}$),
the instruction is treated as redundant, and the $isVariant$ flag is set to False,
preventing its inclusion. 
Whereas, if the similarity is below the threshold,
the instruction is added to the $I_{kept}$, 
indicating this acceptance into the final pool of instructions. 
This approach reduces the amount of instruction being discarded,
thus minimizing wasted API calls. However, relaxing the threshold to
remove less instructions can negatively impact diversity.
}
\end{comment}
Algorithm \ref{alg:diversity-filtering} illustrates our filtering algorithm.
The initial step involves extracting $\mathbb{S}'$, a subset of
$\mathbb{S}$, by randomly selecting three elements from $\mathbb{S}$ (Line 1).
Next, it generates a new set of generated
instructions $\mathbb{G}$ based on $\mathbb{S}'$ using a language model
such as ChatGPT (Line 2).
For each generated instruction $gen_i$ in $\mathbb{G}$, 
the algorithm computes its ROUGE-L similarity score $sim$ 
with each instruction $kept_j$ in a kept instruction set $\mathbb{K}$ (Lines 3--6). 
Note that $\mathbb{K}$ contains unique instructions included in the final 
instruction pool.
If there exists at least one $kept_j$ instruction whose
$sim$ exceeds the threshold $\theta_{ROUGE}$,
$gen_i$ is treated as redundant, and $isVariant$ is set to False
to prevent its inclusion in the final pool (Lines 7--11).
On the other hand, if $sim$ is below $\theta_{ROUGE}$ for all $kept_i$ in $\mathbb{K}$,
$gen_i$ is added to $\mathbb{K}$, 
indicating that the instruction $gen_i$ is included 
into $\mathbb{K}$ as a new unique instruction (Lines 12--13).
%\todo{(한번 revise 할 필요 있음. 문장 중간에 line number를 넣어도 될 듯)}

\begin{comment}
\cancel{
To minimize the negative impact of reduced diversity on model training, we
employ clustering-based batch configure method. Once an instruction is accepted,
it is classified into an appropriate category using a predefined classifier
model. This classifier vectorizes the instructions and performs PCA (Principal
Component Analysis) to reduce the dimensionality to $\log({batch\_size})$. The
resulting dimensions from the PCA are then used to perform clustering based on
the quadrants in the reduced space. This approach results in the creation of a
number of clusters equal to the batch size. This categorization allows the
algorithm to group instructions into clusters ($\mathbb{C}$), which are then used to
structure the batches for training.
}
\end{comment}

As we explained before, \ours{} uses the relaxed similarly threshold,
$\theta_{ROUGE} = 0.85$, compared to the existing designs.
To minimize the negative impact of the reduced diversity on model training, 
we employ clustering-based batch configure method. 
Once the instruction $gen_i$ is accepted to be included in $\mathbb{K}$,
it is classified into an appropriate category $class$ using 
a predefined classifier model \textsc{Classifier} (Line 14). 
This classifier vectorizes the instructions and performs  
Principal Component Analysis (PCA) to reduce the dimensionality 
to $\log({batch\_size})$. The
resulting dimensions from PCA are then used to perform clustering based on
the quadrants in the reduced space. 
%This approach results in the creation of a
%number of clusters equal to the batch size. 
This approach produces a number of clusters equal to the batch size.
This categorization allows the
algorithm to group instructions into clusters $\mathbb{C}$, 
which are then used to construct batches for training (Line 15).

During the training, % During the training process
an instruction is popped from each cluster to form a batch
when the \texttt{\_\_getitem\_\_} function that creates a batch 
to feed to the model is called.
Through this, each batch is formed to encompass a diverse range of
instructions, promoting the model's ability to generalize across various
contexts and minimizing the need for strict filtering. Therefore, by employing a
relaxed ROUGE-L similarity threshold and clustering similar instructions 
in a batch, the algorithm reduces the unnecessary elimination of instructions 
and maximizes the retention of helpful information.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{Figure/Method/seed-update.eps}
%    \caption{Seed instruction selection from the candidates} %Candidate selection for adding to seed instructions
    \caption{Identification of attractive batches and instructions}
    \label{fig:seed-update}
\end{figure}




\subsection{Iterative Feedback Task Generation}

%Existing instruction generation methods are separate from the training
%process, often involving manual filtering by human intervention~\cite{alpagasus}.
%We enhance the quality of generated data without human efforts by improving 
%the performance of few-shot learning Self-Instruct is based on. 
As we explained in Introduction,
the quality of \ours{} is highly affected by the quality of seed instructions.
\ours{} aims to make the quality of seed instructions better 
by adding new seed instructions to $\mathbb{S}$ if they can lead to better model training and
evicting ones from $\mathbb{S}$ if they are less effective for the training.
\ours{} first finds good candidate batches and instructions 
by leveraging insights gained from the training phase.
Then, it identifies valueless seed instructions in the set $\mathbb{S}$.
Finally, it adds new seed instructions to $\mathbb{S}$, while
removing valueless ones.
%To find good seed instructions, \ours{} leverages insights gained 
%from the training phase.

\begin{comment}
\cancel{
Few-shot learning is a method in which a few samples are provided to an LLM when
performing a new task. The quality of this method is based on the example's
quality. Therefore, \ours{} attempts to enhance the quality of these examples, 
which is the seed instructions.
More specifically, we aim to make the quality of instructions in
the seed instructions better by including
seed instructions that can lead to better model training and
evicting what is less effective.
To achieve this, we pipeline the instruction generation
and training processes, leveraging insights gained during training to improve the
quality of instructions generated through few-shot learning.
%\fixme{(이건 generated instruction? 뭔가 혼란스럽지
%않게 통일하는 방법이 있으면 좋을듯; 단, 문맥상에서는 이해하기 어렵지 않음.)} 
}
\end{comment}


% 8.16 06:18 jw: 앞뒤랑 중복되는 것 같아서 뺐습니다.
\begin{comment}
\FIG{fig:seed-update} illustrates the details of updating 
seed instructions. 
We identify high-quality batches 
that greatly contribute to training and then,
from these batches, we select 
instructions 
that are not redundant with those already in the 
seed instruction set. These selected instructions are finally included in the seed instruction set.
\end{comment}

\textbf{Finding Candidate Instructions}
\FIG{fig:seed-update} illustrates how \ours{} identifies attractive
batches and chooses high-quality instructions that will
be added to the seed instruction set $\mathbb{S}$.
During training, \ours{} records information about batches $\mathbb{B}_i$
in \textit{Training log}.
The information in the log reflects the training quality and is subsequently used 
to select a high-quality batch. 
There exist various metrics to measure the quality of batches for training,
which may include loss variance, gradient norm, and the sum of both.
Based on our empirical study, we decide to use the gradient norm (GN) because 
it is the most suitable one, reflecting the training quality of batches.



For every ten iterations, we identify the batch with the highest gradient norm. 
For the first ten iterations, we do not choose any batches 
as the gradient norms at these iterations 
do not accurately reflect the quality of training.
Once a batch $\mathbb{B}_{i}$ is selected, we look for candidate seed instructions
in the batch.
In the same way as Lines 3--13 of Algorithm~\ref{alg:diversity-filtering}, 
we choose instructions in $\mathbb{B}_{i}$ that are not similar to those in $\mathbb{S}$. More specifically,
we only include instructions whose ROUGE-L similarity $sim$ exceeds 
$\theta_{ROUGE}=0.7$. $\theta_{ROUGE}$ of 0.7 is tighter %higher
than 0.85 we used
for the diversity-based filtering. 
%We choose this number to
%ensure that the seed instruction set only maintains high
%diversity and high-quality instructions.
This is because every instruction in a batch already went through
the filtering process and thus always has
a lower ROUGE-L similarity than 0.85.
\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{Figure/Method/seed-evict.eps}
%    \caption{\fixme{Seed instruction evict process    }}
    \caption{Selection of victim seed instructions in $\mathbb{S}$}
    \label{fig:seed-evict}
\end{figure}

\begin{comment}
\cancel{
Once instructions to add are decided, we should choose victim seed instructions that will be removed from $\mathbb{S}$. The victim seed instruction is selected based on its \textit{score} that indicates the diversity of the generated instructions produced by corresponding seed instruction. This score is maintained in the \textit{seed instruction score table}. When a seed is added, it is included in the table; conversely, when it is evicted, it is removed from the table.

The score is calculated as illustrated in Figure 6, an example of this process. During the generation phase, a subset $\mathbb{S}'$ in the same manner in line 1 of Algorithm~\ref{alg:diversity-filtering}. For instance, assume that $seed_a$, $seed_b$, and $seed_c$ are selected. Using these three seed instructions, generated instructions $\mathbb{G}$ are produced. In the example, five generated instructions are created.
Subsequently, in the filtering phase, one generated instruction is discarded, and four will be included in the kept instructions $\mathbb{K}$. After this phase, the seed instruction score table is updated. For each seed instruction that participated in this generation (in this case, $seed_a$, $seed_b$, and $seed_c$), the \textit{SeedGen} is incremented by the number of generated instructions, which is five. Simultaneously, the \textit{SeedKept} is incremented by the number of instructions that passed the filtering, which is four. These values accumulate over the generations.
Finally, the score of the participating three seed instructions is updated, which is computed as the ratio of \textit{SeedKept} to \textit{SeedGen}. For example, the score of $seed_b$ was $21/25 = 0.84$, but after this generation, it is updated to $(21+4)/(25+5) = 0.83$.

When the seed add operation is invoked every ten iterations, the seed instruction with the lowest score is evicted from $\mathbb{S}$ and the seed instruction score table. A lower value indicates that the seed instruction is not generating diverse instructions effectively. As a result, the iterative feedback task generation allows for adding seed instructions with potentially high quality to $\mathbb{S}$ while evicting those that have not performed well in generating diverse instruction. This process ensures that the seed instruction set $\mathbb{S}$ is maintained at the highest quality.
}
\end{comment}

\textbf{Finding Victim Seed Instructions}
Once candidate instructions to add are selected, we must choose 
the seed instructions to be removed from the set $\mathbb{S}$.
For the victim selection, we maintain a \textit{seed score}
for each seed instruction in a \textit{seed score table}. 
The seed score represents the diversity (or quality) of 
instructions generated from a particular seed instruction. 
This score is determined when we calculate the ROUGE-L similarity of 
generated instructions to decide whether or not to add them to 
$\mathbb{K}$.
If many generated instructions from a seed instruction are retained, 
it means that the seed instruction is of high quality.

\FIG{fig:seed-evict} illustrates an example of how \ours{} calculates
scores for seed instructions. To compute scores, 
the seed score table maintains two additional fields: 
SeedGen and SeedKept.
During the generation phase, a subset $\mathbb{S}'$ 
is selected from $\mathbb{S}$ in the same manner as we described
in Algorithm~\ref{alg:diversity-filtering} (see Line 1). 
Let us assume that seed instructions $seed_a$, $seed_b$, and $seed_c$ are
selected for $\mathbb{S}'$. 
Using them, \ours{} creates generated instructions $\mathbb{G}$. 
The number of instructions generated is randomly determined by the LLM;
in our example, five instructions are created.
The corresponding SeedGen field 
in the seed score table is then increased by 5.
Subsequently, in the filtering phase, 
one generated instruction is discarded through the filtering, and four survive,
which are then added to the kept instructions $\mathbb{K}$. 
Similarly, the corresponding SeedKept field in the table 
is increased by 4.
Finally, the scores of the three seed instructions are updated 
as the ratio of \textit{SeedKept} to \textit{SeedGen}. For example, the score of $seed_b$ was $21/25 = 0.84$, but it is updated to $(21+4)/(25+5) = 0.83$.

\textbf{Seed Replacement}
Seed instructions with low scores are evicted from $\mathbb{S}$
and are removed from the seed score table. 
Instead, more valuable instructions chosen from batches are added to $\mathbb{S}$.
This replacement of seed instructions is reasonable 
because a low score indicates that the seed instruction is unlikely to
generate diverse instructions.
In this manner, the iterative feedback task generation 
is able to keep qualified seed instructions for training.






\begin{comment}
Instruciton이 add되면 S로 부터 삭제될 victim seed instruction을 정해야함.
Victim은 해당 seed instruciton이 얼마나 diversity한 generated instruction을 생성했는지에
대한 값, score를 가지고 선정됨.
Score는 seed instruction score table에서 유지되며, 해당 table은 seed instruction이 key로써 작동함.
해당 table의 seed instruction은 seed가 add되면 추가되고, evict되면 table에서 삭제됨.
Score는 다음과 같은 방식으로 매겨짐. 그림 6은 매겨지는 예시를 보여줌.
instruction을 생성할 때, Algorithm 1의 line 1과 같은 방식으로 S'가 선택됨.
예를 들어 seeda, seedb, seedc가 선택되었다고 하자.
이 세 seed instruction을 통해 generation instruction G가 produce됨.
예시에선 14개의 generated instruction이 생성됨.
이때 line 3-13에 의해 filtering이 수행됨. 즉 몇개의 generated instructions가 filtering됨
그림에서는 3개의 generated instruction이 discard되고, 11개의 instruction이 kept됨
한번의 generation 사이클이 모두 끝나고 나면, seed instruction score table을 업데이트 함.
table에서 해당 사이클에 참여한 seeda, seedb, seedc에 대한 seedgen에는 해당
사이클에서 생성된 generated instruction의 수, 14를 더해줌
그리고 seedkept에는 해당 사이클에서 필터링을 통과한 kept instruction의 수 11을 더해줌
해당 값들은 매 사이클마다 누적됨. 다음 사이클에서 seedn, seedm, seedl이 선택되어 instruction generation 과정에 참여했다면
해당 seed instruction의 seedgen, seedkept가 업데이트 될 것임.
마지막으로 score를 업데이트 하는데, score는 seedkept/seedgen으로 계산됨
\end{comment}

\begin{table*}[t]
\caption{Summary of model accuracies over various benchmarks}
\label{tab:benchmarks}
\footnotesize
\centering
\begin{tabular}{l|cccc|c}
                            & \textbf{AlpacaEval}      & \textbf{MMLU (5-shot)} & \textbf{Hellaswag (0-shot)} & \textbf{ARC (0-shot)}   & \textbf{Average} \\ \cline{2-6} 
\textbf{Model}              & \textbf{Win \% vs GPT-4} & \textbf{Accuracy (\%)} & \textbf{Accuracy (\%)}      & \textbf{Accuracy (\%)}  & \textbf{}        \\ \thickhline
Llama-3-8B-Instruct         & 9.1                      & 65.7                   & 57.7                        & 72.2                    & 51.2             \\
Llama-3-8B + Self-Instruct  & 4.6                      & 56.5                   & 55.7                        & 67.7                    & 46.1             \\
\rowcolor[HTML]{DFDFDF} 
Llama-3-8B + \ours{} (ours) & 5.4                      & 56.6                   & 56.1                        & 69.3                    & 46.9             \\
Falcon-7B-Instruct          & 1.8                      & 25.1                   & 51.7                        & 62.0                    & 35.2             \\
Gemma-7B-Instruct           & 0.2                      & 50.2                   & 55.9                        & 66.3                    & 43.2             
\end{tabular}
\end{table*}


\begin{comment}
\fixme{
Once instructions to add are decided, we should choose victim instructions
that will be removed from \fixme{S}.
Our seed eviction policy is shown in \FIG{fig:seed-evict}. 
For each seed, we record the number of kept instructions (\# of kept) and the
total number of instructions generated (\# of gen) whenever the seed participates
in a generation task, and we use this as the eviction criterion; 
specifically, seeds with the lowest ratio of ``\# of gen" to ``\# of kept" are evicted.
That is, in \FIG{fig:seed-evict}, when a subset $\mathbb{S}' = \{seed_n, seed_m, seed_l\}$ 
is randomly selected from the seed instruction set $\mathbb{S}$ and used to generate
instruction $\mathbb{G}$, the cardinality of $\mathbb{G}$ ($|\mathbb{G}|$) is added
to the \# of gen of each seed instruction in $\mathbb{S}'$. And then, the number of
$gen_i$ where treat as $isVariant = \text{True}$ is added to the \# of kept for each seed
instruction in $\mathbb{S}'$ (the symbols are from Algorithm \ref{alg:diversity-filtering}).
This criterion indicates how diverse the instructions
generated by a particular seed have been.
% Appendix에 Algorithm 1에다가 해당 내용을 합친 pseudo code를 넣을까?
}

%우리는 각 seed가 instruction generation 작업에
%참여했을 때, 지금까지 생성한 instruction 대비, 지금까지 kept된 instruciton의 수를 evict 기준으로 하여, 해당 값이 가장 낮은 seed를 방출한다.
%즉, \FIG{fig:seed-evict}에서 $\mathbb{S}$에서 랜덤으로 추출된 $\mathbb{S}' = \{seed_n, seed_m, seed_l\}$을 가지고
%instruction $\mathbb{G}$을 생성했을 때, $|\mathbb{G}|$를 $\mathbb{S}'$의 각 seed instrucion
%의 \# of gen에 더해주고 $isVariant = \text{True}$로 설정된 $gen_i$의 수를 각 seed instruciton의 \# of kept
%에 더해준다. (Algorithm \ref{alg:diversity-filtering}에 \# of gen, \# of kept를 다루는 것까지 합쳐진 수도 코드는 Appendix를 참조하라.)
%이러한 기준은 해당 seed instruction이 얼마나 다양한 instruction을 생성해왔는지에 대한 지표로 작용한다.

\fixme{
When the seed inclusion operation is invoked, which occurs every 10 iterations, the seed 
instruction in the set $\mathbb{S}$ with the lowest \# of kept/\# of gen ratio is evicted.
A lower value indicates that the seed instruction is not generating diverse instructions
effectively. As a result, the iterative feedback task generation
allows for the addition of seed instructions with
potentially high quality to $\mathbb{S}$ while evicting those that have not performed well
in generating diverse instruction. This process ensuring that the seed instruction set $\mathbb{S}$
is maintained at the high-quality
}

%Seed including 작업이 호출되면, 즉 매 10 iteration 마다, seed instruction set $\mathbb{S}$ 내에서 \# of kept/\# of gen가
%가장 작은 instruction을 evict한다. 해당 값이 가장 작은 seed instruction은 diversity한 instruction을 잘 생성하지 못한다는 것을 의미한다.
%결과적으로, 이를 통해 퀄리티가 좋을 가능성이 있는 seed instruction들은 $\mathbb{S}$에 추가하고, 좋지 못한 생성 작업을 보인 seed instruction들을 evict함으로써
%seed instruction set $\mathbb{S}$를 최고의 퀄리티로 유지할 수 있도록 한다.





and then select seed candidates from the instructions
in that batch. The candidate with the lowest ROUGE-L similarity compared to the
existing seed instructions is added to the seed instruction set.
\fixme{(위를 보면 batch 내의 instruction 중 sim 값이 가장 작은 것을 선택한다는 것 같네요.
즉, GN은 instruction 선택의 조건은 아니죠?)}
Concurrently,
the seed instruction in the pool with the lowest score is removed.
\fixme{(여기서의 lowst score가 similarity와 좀 혼동되는데, 이건 위에서 similarlity score라는
용어를 사용했기 때문인듯...} 
The score
is determined by the ratio of kept instruction ($I_{kept}/I_{gen}$) in the
instruction generation involving the seed instruction: a high score indicates that
instruction generated from the seed instruction has high diversity 
\fixme{(위의 ratio의 경우 전체 gen set에서 kept set의 비율인데... 즉, 집합단위의
비율 계산인데, 각각의 instruction 이 마치 각각의 ratio 값을 가지고 있는 듯 합니다. 
이게 어떻게 가능하죠?
알고리즘 그림으로 보자면, instruction은 kept던 gen이던 그냥 gen set 혹은 kept set 집합
안에 있는 element인데, 그러면 각 instruction은 동일한 ratio 값을 가질 것 같은데...)}. It is important
to note that we do not update the seed instructions during the initial training
phase (\fixme{여기서 initial training phase가 어떤 뜻이죠? 최초의 training 인가요?}), as the high gradient norm at this stage does not accurately reflect the
training quality for each batch.
\end{comment}

\begin{comment}
Every ten iterations, we identify the batch with the highest gradient norm,
and among the instructions in this batch, we select seed candidates. The candidate
with the lowest ROUGE-L similarity to the existing seed instructions (with a
threshold of 0.7) is added to the seed instructions set, while the one with
the lowest score is evicted. The score evaluates the diversity of generated
instructions: higher scores are assigned when a seed instruction generates
a high proportion of unique data compared to kept instructions. Notably, during
the initial training phase, where the gradient norm is high, we do not update
the seed instructions.

\fixme{아래의 summary 부분은 이미 강조되어 전체를 없애도 될 겁니다.
대신 기법 설명에 더 많은 공간을 할애하는게 좋을 것 같습니다.}
\cancel{
To conclude, our proposed methodology integrates the data generation and
training processes in a pipeline, leveraging the strengths of
few-shot learning to iteratively refine the quality of generated data.
By focusing on the optimization of seed instructions and employing diversity-based
filtering, we ensure the model is exposed to a rich variety of high-quality
examples. This approach not only minimizes the need for human intervention
but also enhances the overall effectiveness and efficiency of the training
process.
}
\end{comment}
