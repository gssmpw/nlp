%\section{Discussion and Limitation}
%\label{sec:disc}

%\sj{(추후에 section 요약 추가)}
\begin{table}[b]
\centering
\caption{Investigation of the effect of model collapse}
\label{tab:collapse}
\resizebox{\linewidth}{!}{%
\begin{tabular}{l|cc}
\textbf{}  & \begin{tabular}[c]{@{}c@{}}Llama-3-8B + \\ \ours{} (ours)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Llama-3-8B + \\ Self-Instruct\end{tabular} \\ \hline
\textbf{AlpacaEval (Win \%)} & 5.2 (5.4)                                                                                   & 3.3 (4.6)                                                                           \\
\textbf{MMLU (Acc \%)}       & 53.2 (56.6)                                                                                 & 50.9 (56.5)                                                                         \\
\textbf{Hellaswag (Acc \%)}  & 55.7 (56.1)                                                                                 & 53.0 (55.7)                                                                          \\
\textbf{ARC (Acc \%)}        & 67.4 (69.3)                                                                                 & 65.8 (67.7)                                                                         
\end{tabular}
}
\end{table}
\subsection{Impact of Model Collapse}
Several studies report that training a model on synthetic data can
significantly degrade its performance. This phenomenon is called 
\textit{model collapse}~\cite{modelcollapse1, modelcollapse2, modelcollapse3}.
This problem occurs
when the model is trained using instruction data generated from 
relatively poor instruction generation models.
In our setting, since the instruction generation model (GPT-3.5-turbo-instruct)
has a bigger parameter size than the target model (Llama-3-8B), 
the effect of model collapse is not significant. 
\begin{comment}
\begin{table}[t]
\scriptsize
\centering
\caption{Investigation of the effect of model collapse}%Impact of instruction generation model}
\label{tab:collapse}
%\resizebox{\linewidth}{!}{%
\begin{tabular}{c|cc}
\multicolumn{1}{l|}{\textbf{}} &
  \begin{tabular}[c]{@{}c@{}}Llama-3-8B +\\ \ours{} (ours)\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Llama-3-8B +\\ Self-Instruct\end{tabular} \\ \thickhline
  \begin{tabular}[c]{@{}c@{}}\textbf{AlpacaEval}\\(Win \% vs GPT-4)\end{tabular} & 5.2 (5.4)   & 3.3 (4.6)   \\ \hline
  \begin{tabular}[c]{@{}c@{}}\textbf{MMLU}\\(Accuracy \%)\end{tabular}           & 53.2 (56.6) & 50.9 (56.5) \\ \hline
  \begin{tabular}[c]{@{}c@{}}\textbf{Hellaswag}\\(Accuracy \%)\end{tabular}      & 55.7 (56.1) & 53.0 (55.7) \\ \hline
  \begin{tabular}[c]{@{}c@{}}\textbf{ARC}\\(Accuracy \%)\end{tabular}            & 67.4 (69.3) & 65.8 (67.7)
\end{tabular}%
%}
\end{table}
\end{comment}
%\begin{comment}

%\end{comment}

\TAB{tab:collapse} shows the results of re-evaluating the benchmarks 
using Llama-3-8B-Instruct as the instruction generation model
(which is smaller in a parameter space than GPT-3.5-turbo-instruct).
The values in parentheses are taken from
the results with GPT-3.5-turbo-instruct.
Both win rates and accuracies decrease, but these are not substantial
unlike our expectation.
Despite its smaller parameter space, 
%the effect of the model collapse is not significant 
%even with Llama-3-8B-Instruct. This is due to the fact that 
Llama-3-8B-Instruct has already learned sufficient information during
pre-training, and 
thus aligning the response has a higher impact on model training.
% 무슨말인지 모르고 작성하신것 치고는 완벽하신데요...
Nevertheless, using a larger instruction generation model is still
more effective because it can generate a wider range of instructions, 
which can be beneficial for instruction tuning~\cite{instruction-tuning}.

\begin{comment}
\sj{(치명적이지 않다면 아래 내용은 꼭 필요할 것 같지는 않네요)}
However, there is still a lack of in-depth insight into how the specific
characteristics of the instruction generation model, such as parameter size,
generation setting (temperature, top-p, and so on), and the dataset used for
pre-training or fine-tuning, impact the quality of the instruction dataset.
An in-depth analysis of what the better instruction generation model is could 
provide a deeper understanding of how to optimize instruction generation.
Exploring these factors will be crucial to better 
leveraging synthetic data while mitigating the risks of model collapse.
\end{comment}

\subsection{Safety}

\begin{figure}
    \centering
    \includegraphics[width=0.85\linewidth]{Figure/Discussion/safety-compare.eps}
    \caption{Case study for AI responses to harmful queries}
    \label{fig:safety-compare}
\end{figure}

\begin{comment}
\cancel{
In this work, we developed and evaluated \ours{} without incorporating safety 
considerations. The limitation of \ours{} is the integration with the training 
phase and instruction generation phase that makes it hard to incorporate the 
safety consideration. We conduct case studies on models' responses
to harmful or violent queries. 
As shown in \FIG{fig:safety-compare}, the model trained with \ours{} generates 
responses without rejecting unsafe content. In contrast, Llama-3-8B-Instruct 
refuses to generate harmful context, as the model has been trained with 
Reinforcement Learning with Human Feedback (RLHF)~\cite{rlhf} and Supervised 
Fine Tuning (SFT)~\cite{sft1, sft2, sft3} to enhance their ability to handle unsafe queries.
}
\end{comment}

We haven't given serious attention to safety issues when 
developing \ours{}.
According to our case studies on 
models' responses to harmful or violent queries which are shown 
in \FIG{fig:safety-compare},
the model trained with \ours{} generates 
responses without rejecting unsafe content.
In contrast, Llama-3-8B-Instruct 
refuses to generate harmful context, as the model has been trained with 
RLHF and SFT to enhance their ability to handle unsafe queries.


One promising approach to addressing safety issues is the use of prompt
engineering~\cite{promptsafety}. We conduct a generation task using Llama-2's
default system prompt~\cite{llama2prompt}. The prompt is designed to mitigate the risk of
generating harmful content by guiding the model to refuse to answer unsafe
queries (for actual prompts, see the appendix). Our results indicate that
prompt engineering can effectively prevent the generation
of dangerous responses, suggesting that it could serve as a viable
strategy for the safety concerns in \ours{}.