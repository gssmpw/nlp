\appendix
\section{Appendix}


\begin{table}[ht]
\centering
\caption{Overview of the training hyterparameter}
\label{tab:hyperparameter}
\begin{tabular}{lllllll|l}
\textbf{Hyperparameter} &&&&&&& \textbf{Value} \\ \hline
Learning rate           &&&&&&& $2\cdot10^{-5}$\\
Weight decay            &&&&&&& 0              \\
Warmup ratio            &&&&&&& 0.03           \\
Batch size              &&&&&&& 16             \\
\# of epoch             &&&&&&& 3              \\
Max length              &&&&&&& 512           
\end{tabular}%
\end{table}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.99\linewidth]{Figure/Appendix/bad_seed.eps}
    \caption{Low-scored seed instructions}
    \label{fig:bad-seed}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.99\linewidth]{Figure/Appendix/good_seed.eps}
    \caption{High-scored seed instructions}
    \label{fig:good-seed}
\end{figure}




\subsection{Hyperparameter Setting}
We performed instruction tuning on LLaMA-3-8B using the 
hyperparameters listed in \TAB{tab:hyperparameter}. All 
four models we tuned: Llama-3-8B + \ours{} with Llama-3-8B-Instruct and ChatGPT API
and Llama-3-8B + Self-Instruct with Llama-3-8B-Instruct and ChatGPT API.
These four models are trained using
the same hyperparameter settings.


\subsection{Seed Instruction Analysis According to Seed Score}
\FIG{fig:bad-seed} and \FIG{fig:good-seed} illustrate the example of seed instructions 
based on their seed scores, which are low and high, respectively.
The low-scoring seed instructions in \FIG{fig:bad-seed} focus on technical and
mechanical tasks, such as text completion, regular expression generation, 
and number sorting. In contrast, the high-scoring seed instructions in 
\FIG{fig:good-seed} involve tasks that require a creative and humanistic approach,
such as questioning stereotypes, discussing natural phenomena, and writing recommendation
letters. The results suggest that seeds with characteristics similar to those in \FIG{fig:good-seed}
may be more effective for generating diverse data where context comprehension and creativity are crucial.

\subsection{Detailed Model Information}

\TAB{tab:model-info} summarizes each model's characteristics, including
the volume and method of instruction data used during training and the
additional techniques employed to enhance the model's performance. 

The Llama-3-8B + Self-Instruct and Llama-3-8B + \ours{} models 
were trained with 30,164 instructions collected through their respective data generation
frameworks without additional training techniques. Falcon-7B-Instruct stands out
with 250 million tokens of synthesized data combined with the manual collection 
but does not employ RLHF or SFT. On the other hand, Gemma-7B-Instruct's training
data volume remains undisclosed, although it similarly uses synthesized data and
manual collection and is further refined using RLHF and SFT.







\begin{table}[t]
\caption{Detailed model information}
\label{tab:model-info}
\resizebox{\linewidth}{!}{%
\begin{tabular}{l|ccc}
 &
  \textbf{\begin{tabular}[c]{@{}c@{}}Dataset\\ scale\end{tabular}} &
  \textbf{\begin{tabular}[c]{@{}c@{}}Instruction\\ collection\\ method\end{tabular}} &
  \textbf{\begin{tabular}[c]{@{}c@{}}Additional\\ training\\ techniques\end{tabular}} \\ \hline
\begin{tabular}[c]{@{}l@{}}Llama-3-8B\\ -Instruct\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Over 10M\\ instructions\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Manual\\ collection\end{tabular} &
  RLHF, SFT \\ \hline
\begin{tabular}[c]{@{}l@{}}Llama-3-8B\\ + Self-Instruct\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}30,164\\ instructions\end{tabular} &
  Self-Instruct &
  None \\ \hline
\begin{tabular}[c]{@{}l@{}}Llama-3-8B\\ + \ours{}\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}30,164\\ instructions\end{tabular} &
  \ours{} &
  None \\ \hline
\begin{tabular}[c]{@{}l@{}}Falcon-7B\\ -Instruct\end{tabular} &
  250M tokens &
  \begin{tabular}[c]{@{}c@{}}Synthesized data\\ + manual collection\end{tabular} &
  None \\ \hline
\begin{tabular}[c]{@{}l@{}}Gemma-7B\\ -Instruct\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Not publicly\\ disclosed\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Synthesized data\\ + manual colleciton\end{tabular} &
  RLHF, SFT
\end{tabular}%
}
\end{table}


\subsection{Detailed benchmark}


\begin{table*}[!t]
\centering
\caption{Detailed benchmark results of MMLU and ARC}
\label{tab:detail-bench}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l|cc
>{\columncolor[HTML]{DFDFDF}}c cc}
\textbf{Groups} & \textbf{Llama-3-8B-Instruct} & \textbf{Llama-3-8B + Self-Instruct} & \textbf{Llama-3-8B + \ours{}} & \textbf{Falcon-7B-Instruct} & \textbf{Gemma-7B-Instruct} \\ \thickhline
\textbf{MMLU}   & \textbf{65.7}  & \textbf{56.5} & \textbf{56.6} & \textbf{25.1} & \textbf{50.2}      \\ \hline
STEM            & 56.5  & 47.5 & 49.3 & 23.1 & 43.0 \\
Humanities      & 60.4  & 51.0 & 49.8 & 24.9 & 44.7 \\
Social sciences & 76.6  & 66.6 & 66.7 & 24.2 & 58.3 \\
Other           & 72.2  & 64.1 & 64.1 & 28.2 & 44.7 \\ \thickhline
\textbf{ARC}    & \textbf{72.2}  & \textbf{67.7} & \textbf{69.3} & \textbf{62.0} & \textbf{66.3} \\ \hline
ARC challenge   & 53.2  & 47.7 & 49.8 & 40.1 & 47.3  \\
ARC easy      &  81.2 &  77.5 &  79.0 &  72.7 &  75.6  
\end{tabular}%
}
\end{table*}

\TAB{tab:detail-bench} presents detailed information on benchmarks that include 
subgroup evaluations, as referenced in the benchmark evaluation results of \TAB{tab:benchmarks}.
Similar to the results in \TAB{tab:detail-bench}, the Llama-3-8B + \ours{}
model does not fall behind in overall performance. This is noteworthy considering 
that the data collection costs for this model were significantly lower compared to
other models.


\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.999\linewidth]{Figure/Appendix/prompts.eps}
    \caption{All of the used prompts}
    \label{fig:detail-prompt}
\end{figure*}

\subsection{Used Prompts}
\FIG{fig:detail-prompt} shows all the prompts we used in the paper. 
An instruction generation prompt is used to generate the instructions. 
Seed instructions are appended to this prompt by the first three tasks 
from a List of 20 tasks and then fed to the instruction-generating model.
Llama-2 default prompt is designed to mitigate the risk of generating 
harmful content by guiding the model to refuse to answer unsafe queries.
For the competitive evaluation, we use the competitive evaluation prompt.
Each answer of the models is filled in the \texttt{\{answer\_1\}} or \texttt{\{answer\_2\}}.