\section{Evaluation}
\label{sec:eval}

We conduct experiments to evaluate the effectiveness of \ours{}.
We first compare the performance of models trained with \ours{}
against other LLMs of a similar scale across various benchmarks. We then
carry out an in-depth analysis to ensure 
instruction tuning is executed correctly through
competitive evaluation. We also investigate 
whether the instruction generation of \ours{} is more cost-effective 
than that of Self-Instruct, mainly focusing on
how much diversity-based filtering reduces costs. 
Finally, we explore the impact
of model collapse and the potential safety issue of \ours{}.
In Appendix, a more detailed
investigation of the impact of the iterative feedback task generation, along
with other experiment details, are provided. 
All codes are available at \url{https://github.com/}. 

\subsection{Training Recipe}
\textbf{Our model.}
We use the Llama-3-8B model~\cite{llama3} as the base model
and train it with 30,164 instructions generated using \ours{}.
The seed instructions used at
the beginning of data generation are the same as those from
Self-Instruct. 
For detailed hyperparameters, please refer to Appendix.

\noindent \textbf{Baseline models.}
We compare \ours{} with four different models:
Llama-3-8B-Instruct, Llama-3-8B + Self-Instruct,
Falcon-7B-Instruct, and Gemma-7B-Instruct.
Llama-3-8B-Instruct represents the ideal instruction-tuned 
model. It is based on Llama-3-8B and is tuned using 10M 
manually collected instructions.
Llama-3-8B-Instruct has also been trained 
using Reinforcement Learning with Human Feedback (RLHF)~\cite{rlhf} 
and supervised fine-tuning (SFT)~\cite{sft1} to further 
enhance its performance. Such optimizations
enable Llama-3-8B-Instruct to outperform the other models.

Llama-3-8B + Self-Instruct is also based on Llama-3-8B,
but unlike Llama-3-8B-Instruct,
it is trained with instructions synthesized using Self-Instruct.

We also include Falcon-7B-Instruct and Gemma-7B-Instruct,
which have similar model sizes (7-8 billion parameters),
to evaluate \ours{} against models other than Llama-3-8B.
%to show the standing of our models. 
For detailed information on the models, please refer to Appendix.


\begin{comment}
\cancel{
For the comparisons, we use several instruction-tuned
models. 
We also utilize the Llama-3-8B model as the base model and
train it with 30,164 instructions, but they are generated 
using Self-Instruct. By comparing this model with our model, 
we can fairly and directly compare \ours{} with Self-Instruct.
We include a Llama-3-8B-Instruct as a baseline, and this model has 
conducted instruction tuning on the same base model as ours, but 
it was trained with 10M manually collected instructions. 
Also, this model has been trained with Reinforcement Learning with 
Human Feedback (RLHF)~\cite{rlhf} and supervised
fine-tuning (SFT)~\cite{sft1} to improve the model's 
performance further. This model can be considered the peak performance 
of an instruction-tuned model based on the Llama-3-8B model. By 
comparing this model with ours, we can assess how close we are to 
the peak performance.
Additionally, we evaluate other 7-8 B-sized models, including 
Falcon-7B-Instruct and Gemma-7B-Instruct,
to show the standing of our models. 
For detailed information on the models, please refer to Appendix
%For detailed information on the model training technique and instruction 
%collection method, as well as the number of instructions used in training, 
%please refer to Appendix.
}
\end{comment}

\noindent \textbf{Hardware setup.} 
We use a machine that has two AMD EPYC 7742 3.3GHz
64-core CPUs and 2TB DDR4 DRAM. The machine is also equipped with eight RTX-3090
GPUs.
We use Ubuntu 22.04 as the OS and the version of
Python packages are torch 2.1.2 and deepspeed 0.14.4.
%In this section, we describe the training recipe used for evaluating our
%methodologies. For all our experiments, we selected LLaMA-3-8B as the
%pre-trained model. To generate instruction data, we used two models:
%LLaMA-3-8B-Instruct and GPT-3.5-Turbo. Ultimately, we fine-tune models using our
%methodology and compare them with with models fine-tuned using the training
%recipe of Alpaca that is based on the Self-Instruct method. We represent our
%model that using LLaMA-3-8B-Instruct and GPT-3.5-Turbo generated dataset as
%$\mathrm{Ours}_{\mathrm{LLaMA}}$ and $\mathrm{Ours}_{\mathrm{GPT}}$, respectively. Similarly, the
%baseline alpaca model represented as $\mathrm{Alpaca}_{\mathrm{LLaMA}}$ and
%$\mathrm{Alpaca}_{\mathrm{GPT}}$. 
\begin{comment}
Additionally, we adjusted the training datasets to evaluate
how well the models performed on specific tasks. This involved modifying the
prompts used for few-shot learning. The details of the modified prompts can be
found in the Appendix.
\end{comment}

\subsection{Benchmark Performance}
We evaluate the models using
various datasets, including AlpacaEval~\cite{alpacaeval}, MMLU~\cite{MMLU}, Hellaswag~\cite{hellaswag},
and ARC~\cite{DROP}. 
We measure a win rate for AlpacaEval by comparing 
outputs from the models against those from GPT-4. %the reference model, GPT-4. 
A higher win rate indicates better alignment with expected responses. 
For the other benchmarks, we measure accuracy, 
representing the probability of correctly answering questions.
We measure the accuracy of MMLU in a 5-shot setting and 
the accuracy of Hellaswag and ARC in a zero-shot setting~\cite{scalingllm}.



\TAB{tab:benchmarks} shows the results,
where a higher value indicates better performance. 
Except for Llama-3-8B-Instruct which presents the ideal performance
with instruction tuning yet requires serious human efforts to create
seed instructions,
\ours{} outperforms all the other models we chose to compare.
% Llama-3-8B-Instruct가 가장 좋지만, 쓰기 힘든 이유를 여기다 적으면 좋을 것 같은데,,,
% Except for Llama-3-8B-Instruct, SEDI-INSTRUCT outperforms all the other models we chose to compare. Llama-3-8B-Instruct shows the most outstanding performance, but achieving such model performance requires an enormous cost in data collection.
Notably, despite using a more cost-effective data generation method,
\ours{} outperforms the Self-Instruct based model, showing 
5.2\% higher
accuracy on average. 
% 원래 XX - YY%라고 되어있었는데 MMLU에서 0.1% 더 높아서... 살짝 우리 모델이 멍청해보일까봐 average로 바꿨습니다.
% 계산 (((5.4/4.6-1)+(56.6/56.5-1)+(56.1/55.7-1)+(69.3/67.7-1))/4) * 100 = 5.2
% for the datasets.
As will be discussed later, this higher accuracy is achieved with 36\%
lower training cost compared to Self-Instruct.
%\ours{} lowers the cost of training the model
%by up to 36\% compared to Self-Instruct.


\begin{comment}
\subsubsection{AlpacaEval}
First, we evaluate the models on AlpacaEval~\cite{alpacaeval}. AlpacaEval is an
automatic evaluator designed for instruction-following language models. It provides
a quick, cost-effective, and high-quality way to assess these models by generating a
leaderboard and annotations for model outputs. AlpacaEval uses advanced
configurations and annotators to ensure a high agreement rate with human annotations,
making it a reliable tool for evaluating the performance of language models across
various tasks.

\subsubsection{MMLU}
First, we evaluate the models on AlpacaEval~\cite{alpacaeval}. AlpacaEval is an
automatic evaluator designed for instruction-following language models. It provides
a quick, cost-effective, and high-quality way to assess these models by generating a
leaderboard and annotations for model outputs. AlpacaEval uses advanced
configurations and annotators to ensure a high agreement rate with human annotations,
making it a reliable tool for evaluating the performance of language models across
various tasks.
\end{comment}


\begin{figure}[t]
    \centering
    \includegraphics[width=0.88\linewidth]{Figure/Evaluation/compettitive.eps}
    \caption{Competitive evaluation results}
    \label{fig:competitive}
\end{figure}

% 학습한 모델의 출력을 response라는 용어로 통일
\subsection{Competitive Evaluation}
\begin{comment}
\cancel{
In this experiment, for a direct comparison of the quality of the model's responses,
referring to previous studies~\cite{alpagasus, alpacafarm}, we employ an 
automated evaluation method using an API-based LLM to compare our model and baseline.
The responses from both models are input into an evaluation model (e.g., GPT-4), 
which is instructed to assign a score between 1 and 10 for each response.
\review{To mitigate positional bias, 
we evaluate both when our model’s response is placed before and when it is placed after the baseline model’s response.
The final outcome is defined as ``Win-Tie-Lose"; ``Win" means our model wins twice for both orders, ``Tie" means wins once and loses once, and ``Lose" means our model loses twice.}
The datasets used for the competition are the Vicuna test set (Vicuna)~\cite{vicuna1}, Anthropic's 
helpful test set (Helpful)~\cite{helpful}, the Koala test set (Koala)~\cite{koala}, the Open Assistant test set (OASST)~\cite{oasst}, 
and the Self-Instruct test set (Self-Instruct)~\cite{self-instruct}.}
\end{comment}
To assess the quality of the model's responses,
we make use of an automated competitive evaluation method 
that utilizes LLMs to compare the quality of responses~\cite{alpagasus, alpacafarm}. 
We compare our model (Llama-3-8B + \ours{}) 
with Llama-3-8B + Self-Instruct.
The responses from the models are input to GPT-4
which assigns a score between 1 and 10 for each response.
To mitigate a positional bias, 
we measure scores in two different orders:
first, when the responses from Llama-3-8B + \ours{} are input into GPT-4 
before those from Llama-3-8B + Self-Instruct, and second, when they are input afterward.
The final outcome is defined as ``Win-Tie-Lose"; ``Win" means our model wins twice for both orders, 
``Tie" means wins and loses once, and ``Lose" means our model loses twice.
The datasets used for the competition are the Vicuna test set (Vicuna)~\cite{vicuna1}, Anthropic's 
helpful test set (Helpful)~\cite{helpful}, the Koala test set (Koala)~\cite{koala}, the Open Assistant test set (OASST)~\cite{oasst}, 
and the Self-Instruct test set (Self-Instruct)~\cite{self-instruct}.


\FIG{fig:competitive} illustrates the results. 
%We conduct a competitive 
%evaluation of our model (Llama-3-8B + \ours{}) against Llama-3-8B + Self-Instruct. 
As the results indicate, our model outperforms the 
Self-Instruct-based model for all of the five test sets.
It demonstrates that \ours{} generates more 
effective instructions for training than Self-Instruct through the iterative
feedback task generation.
%\review{These results also imply that our diversity-based filtering does not negatively 
%impact not only previous benchmark performance but also the model response quality.}



\subsection{Cost Analysis}

\begin{comment}
\cancel{
We evaluate the efficiency of \ours{} in generating instructions compared to
Self-Instruct method. The experiment measured the number of API requests sent
and the API cost to generate 10,000 kept instructions, meaning instructions that 
passed the filtering process. As shown in \FIG{fig:filtering-eval}(a), \ours{} 
demonstrates an improvement in efficiency by requiring fewer API requests to generate 
the 10,000 instructions. This efficiency gain is achieved by reducing the number of 
discarded instructions. Notably, as shown in previous experiments, this less strict 
filtering approach does not negatively impact model training, as it maintains the 
diversity of the batch configuration.
}
\end{comment}

We evaluate the cost effectiveness of \ours{} in generating instructions 
compared to Self-Instruct. We measure the number of API invocations to
ChatGPT, along with the cost of using the ChatGPT service,
required to generate 10,000 kept instructions.
As shown in \FIG{fig:filtering-eval}(a), \ours{} 
requires 36\% fewer API invocations than Self-Instruct to
generate 10,000 instructions.
As expected, this gain is achieved by reducing the number of 
discarded instructions through the diversity-based filtering with the relaxed similarity threshold.
Despite fewer API calls, \ours{} outperforms Self-Instruct 
in terms of model accuracy as we discussed before.
%Again, the cost efficiency of \ours{} is obtained with higher model accuracy
%than Self-Instruct.
%without negatively affecting the accuracy of the model.


Such an increase in the efficiency of generating kept instructions 
reduces the overall cost of using the ChatGPT service.
For our experiment, 
we utilize the GPT-3.5-turbo-instruct API~\cite{gpt3.5-api}, 
which charges \$1.5 per 1M tokens. 
As illustrated in \FIG{fig:filtering-eval}(b), 
\ours{} achieves 1.6$\times$ reduction in the cost 
due to fewer API calls.
%needed to create 10,000 kept instructions.
Also, as shown in \FIG{fig:filtering-eval}(a), 
the efficiency gap between \ours{} and
Self-Instruct gets wider as more instructions are generated. 
As a result, \ours{} has the potential to achieve greater cost savings.


\begin{figure}[t]% 
	\centering
	\subfloat[\# of kept per API calls]{\includegraphics[width=0.55\linewidth]{Figure/Evaluation/cost-1.eps}}
	\subfloat[Cost]{\includegraphics[width=0.36\linewidth]{Figure/Evaluation/cost-2.eps}}
	\caption{Instruction data generating cost analysis}%
	\label{fig:filtering-eval}%
\end{figure}