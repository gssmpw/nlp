\section{Experiment}

\subsection{General Intelligence Evaluation}
A major challenge faced by speech-based dialogue models is that, compared to pure text-based dialogue models, their performance tends to degrade. To evaluate the "intelligence" of speech models, we use text-to-text modeling capabilities as a baseline and assess the performance of pre-trained speech-to-text models. The evaluation dataset consists of two types: story continuation ability and commonsense reasoning ability.

\textbf{Sentence continuation.}
For the continuation ability evaluation, we use the sStoryCloze dataset \cite{hassid2024textually}. Additionally, we introduce the zh-sStoryCloze dataset, which is created by translating the English version of sStoryCloze into its Chinese counterpart via a translation engine and replacing English names with Chinese ones to better suit the Chinese context. Each sample in both evaluation sets consists of five sentences, divided into positive and negative samples. The last sentence differs between the two, with the last sentence of the positive sample being the correct continuation. A prediction is considered correct if the perplexity of the last sentence in the positive sample is lower than that of the negative sample.

\textbf{Commonsense reasoning.}
For the commonsense reasoning ability evaluation, the goal is to assess whether the model possesses domain-specific knowledge. Drawing inspiration from the design of sStoryCloze, we use the GPT-4o API to rewrite and filter the CMMLU dataset \cite{li2023cmmlu}, ultimately creating the sCMMLU dataset with 4,743 commonsense questions. For each multiple-choice question in the original CMMLU, we rewrite it into four statements with the same first half and different second halves according to the answer options. A prediction is considered correct if the perplexity of the correct option's statement is lower than that of the other options.

\input{tables/iq}
\textbf{Results.}
The general intelligence evaluation results in two aspects are shown in~\autoref{tab:iq-performance}. It can be observed that Baichuan-Audio consistently outperforms previous models in the sentence continuation evaluation task. Given the current architecture, the intelligence of the speech multimodal model is inherently dependent on the reasoning capability of the pure text LLM. Consequently, the accuracy of the T$\to$T mode serves as the upper bound for end-to-end speech models, while the accuracy of the S$\to$T mode is inherently lower than that of the T$\to$T mode. Our goal is to bridge this gap by improving the accuracy of the S$\to$T mode to approach that of the T$\to$T mode, thereby enhancing the intelligence of end-to-end speech models. Moreover, we observe that a two-stage training strategy effectively mitigates the degradation in model intelligence compared to single-stage training.



\subsection{Performance in ASR/TTS Tasks}
For ASR evaluation in the general scene, we report results on the Fleurs~\citep{fleurs2022arxiv} Chinese (\textit{zh}) and English (\textit{en}) test sets, as well as the WenetSpeech~\citep{zhang2022wenetspeech10000hoursmultidomain} \textit{test\_net} dataset. To assess performance in more challenging ASR scenarios,  we include results from the WenetSpeech~\citep{zhang2022wenetspeech10000hoursmultidomain} \textit{test\_meeting} dataset and the KeSpeech~\citep{tang2021kespeech} test set, which evaluate the model's ASR capabilities in `Meeting' and `Chinese dialect' contexts. Baichuan-Audio exhibits a strong audio transcription capacity in~\autoref{tab:audio-general-asr}. On the Fleurs dataset (test-zh), it achieves a WER of 3.2\%, significantly lower than Whisper-large-v3 (12.4\%) and Qwen2-Audio-Base (4.3\%). For the WenetSpeech dataset, Baichuan-Audio-Base achieves a WER of 7.2\% on test\_net and 8.5\% on test\_meeting. On the KeSpeech dataset, Baichuan-Audio-Base excels across multiple Chinese dialects. In addition to ASR, Baichuan-Audio excels in both S2TT and TTS tasks. For S2TT task, which aims to translate the audio signal in the source to the target language. We evaluate the model's S2TT performance between Chinese and English using the zh2en and en2zh subsets of the Covost2~\citep{wang2020covost2massivelymultilingual} dataset, with BLEU~\citep{papineni-etal-2002-bleu} scores as the evaluation metric. The evaluation results of S2TT and TTS tasks are summarized in~\autoref{tab:bleu-tts}. 
\input{tables/asr_performance}
\input{tables/bleu-tts}



\subsection{Performance in Audio Understanding Tasks}

\textbf{Baselines.} We compare Baichuan-Audio with the following baselines: proprietary model (GPT-4o-Audio~\cite{HelloGPT4o}), open-source voice model (GLM-4-Voice~\cite{zeng2024glm}), and open-source models for omni-modal (VITA-1.5~\cite{fu2025vita}, MiniCPM-o 2.6~\cite{yao2024minicpm}).

\textbf{Evaluation Benchmarks.} To assess the audio understanding capabilities of Baichuan-Audio, we have built and open-sourced an OpenAudioBench and use GPT-4o~\cite{HelloGPT4o} to evaluate the results, including Reasoning QA(self-constructed), Spoken Llama Questions~\cite{nachmani2023spoken}, Web Questions~\cite{berant2013semantic}, TriviaQA~\cite{joshi2017triviaqa}, and AlpacaEval~\cite{li2023alpacaeval}. For AlpacaEval, we select two subsets \texttt{helpful base} and \texttt{vicuna} from the original AlpacaEval dataset and remove questions related to math and code. This process follows Llama-Omni~\cite{fang2024llama}, with the aim of obtaining questions more suitable for speech scenarios, and the final AlpacaEval benchmark in our report comprises 199 questions in total. Considering the substantial size of the Web Questions and TriviaQA datasets, a full evaluation is impractical. Therefore, we randomly sample 1,000 questions from each original dataset. The instructions for these three benchmarks were synthesized using our TTS model.

For Reasoning QA, we use GPT-4o to evaluate the score of the answers based on the given reference answers, and then calculate the accuracy rate. For Llama Questions, Web Questions, and TriviaQA, we provide reference answers and use GPT-4o to assess the correctness of the model's responses. The final score is the percentage of answers judged as correct.

For all audio benchmarks, we consider two different settings: 1) speech-to-speech generation in a non cascaded manner (denoted as S$\to$S), where the input is audio and the output is interleaved text and audio. The output text is then merged and used for evaluation. 2) speech-to-text generation (denoted as S$\to$T, where the input is audio and the output is text, which is used for evaluation.

\textbf{Results.} As shown in \autoref{tab:audio bench}, our model performs excellently on audio understanding benchmarks, outperforming the latest open-source models. In the S$\to$T setting, Baichuan-Audio significantly outperforms models of the same size in AlpacaEval, achieving score of 77.4. In the S$\to$S setting, Baichuan-Audio surpasses GLM-4-Voice across the board, particularly leading by 11.4 and 20.7 in Reasoning QA and AlpacaEval.


\begin{table}[!ht]
    \caption{\textbf{Results on audio understanding benchmarks.} $\nabla$: The modalities parameter is set to ["text", "audio"], evaluation based on the output text. $\diamondsuit$: Supports only text-audio interleaved output. $\square$: Cascade output method, evaluation based on the output text.}
    \label{tab:audio bench}
    \resizebox{\textwidth}{!}{
    \centering
    \begin{tabular}{@{}ccccccccccc@{}}
        \toprule
        \multicolumn{1}{c|}{}    & \multicolumn{10}{c}{\textbf{Audio Comprehensive Capacity}} \\
        \cmidrule{2-11}
        \multicolumn{1}{c|}{\multirow{2}{*}{\textbf{Model}}} & \multicolumn{2}{c|}{Reasoning QA} & \multicolumn{2}{c|}{Llama Questions} & \multicolumn{2}{c|}{Web Questions} & \multicolumn{2}{c|}{TriviaQA} & \multicolumn{2}{c}{AlpacaEval} \\
        \cmidrule(l){2-11}
         \multicolumn{1}{c|}{} & \textit{s $\to$ t} & \multicolumn{1}{c|}{\textit{s $\to$ s}} & \textit{s $\to$ t} & \multicolumn{1}{c|}{\textit{s $\to$ s}} & \textit{s $\to$ t} & \multicolumn{1}{c|}{\textit{s $\to$ s}} & \textit{s $\to$ t} & \multicolumn{1}{c|}{\textit{s $\to$ s}} & \textit{s $\to$ t} & \textit{s $\to$ s} \\
          \midrule
         \multicolumn{1}{c|}{GPT-4o-Audio$^\nabla$} & \textbf{55.6} & \multicolumn{1}{c|}{-} & \textbf{88.4} & \multicolumn{1}{c|}{-} & \textbf{81.0} & \multicolumn{1}{c|}{-} & \textbf{90.6} & \multicolumn{1}{c|}{-} & \textbf{80.1} & - \\
         \multicolumn{1}{c|}{GLM-4-Voice (9B)$^\diamondsuit$} & - & \multicolumn{1}{c|}{26.5} & - & \multicolumn{1}{c|}{71.0} & - & \multicolumn{1}{c|}{51.5} & - & \multicolumn{1}{c|}{46.6} & - & 48.9 \\
         \multicolumn{1}{c|}{VITA-1.5 (7B)$^\square$} & 41.0 & \multicolumn{1}{c|}{-} & 74.2 & \multicolumn{1}{c|}{-} & 57.3 & \multicolumn{1}{c|}{-} & 46.8 & \multicolumn{1}{c|}{-} & 68.2 & - \\
        \multicolumn{1}{c|}{MiniCPM-o 2.6 (7B)$^\square$} & 38.6 & \multicolumn{1}{c|}{-} & 77.8 & \multicolumn{1}{c|}{-} & 68.6 & \multicolumn{1}{c|}{-} & 61.9 & \multicolumn{1}{c|}{-} & 51.8 & - \\
         \multicolumn{1}{c|}{\textbf{Baichuan-Audio (7B)}} & 41.9 & \multicolumn{1}{c|}{\textbf{37.9}} & 78.4 & \multicolumn{1}{c|}{\textbf{74.5}} & 64.5 & \multicolumn{1}{c|}{\textbf{60.3}} & 61.7 & \multicolumn{1}{c|}{\textbf{54.2}} & 77.4 & \textbf{69.6} \\
        \bottomrule
    \end{tabular}}
\end{table}
