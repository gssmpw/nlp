\section{Introduction}

The development of audio dialogue models has shifted from traditional cascade modeling approaches to end-to-end audio large models \cite{hassid2024textually}. Traditional audio dialogue models operate through a sequential framework, where audio input is processed via Automatic Speech Recognition (ASR) to generate text. This text is then used by a language model to produce responses, which are converted into synthesized speech through Text-to-Speech (TTS) systems. While this approach leverages the capabilities of large language models (LLMs), it overlooks the influence of paralinguistic information and introduces processing delays along with errors accumulated through multiple data conversions. This step-by-step processing chain exacerbates these issues, impacting overall performance. In contrast, contemporary end-to-end audio large models, such as GPT-4o \cite{HelloGPT4o}, adopt a unified framework to directly process audio input, streamlining the task flow and enhancing both understanding and generation of audio content. This evolution facilitates more natural, fluid audio interactions and improves real-time performance.

Recently, open-source end-to-end audio interaction systems have advanced toward achieving real-time performance. These systems can be categorized into three main types. The first, exemplified by Freeze-Omni \cite{wang2024freeze}, aligns audio and text modalities through modality adapters, enabling pre-trained large language models to process audio inputs, with hidden states converted into speech waveforms via a speech decoder. While this approach preserves the original capabilities of LLMs, it lacks a holistic end-to-end understanding of the audio modality. The second type, represented by Moshi \cite{defossez2024moshi}, utilizes discrete audio tokens as input and adopts a multi-stream architecture to output both audio and text concurrently. The third type, such as GLM-4-Voice \cite{zeng2024glm}, differs by generating interleaved audio and text outputs, allowing text to guide audio generation for enhanced output quality. However, a key challenge for the second and third types lies in the integration of audio modalities, which often results in a noticeable reduction in reasoning capabilities compared to textual large language models.

In this work, we introduce Baichuan-Audio, an end-to-end audio large language model designed for real-time speech interaction. Similar to Moshi and GLM-4-Voice, Baichuan-Audio extends pre-trained LLMs to enable end-to-end audio input and output. This is achieved through the integration of the Baichuan-Audio-Tokenizer and a stream-matching decoder, which discretize audio signals into tokens and decode audio tokens back into speech waveforms, respectively. The tokenizer operates at a frame rate of 12.5 Hz and employs multi-codebook discretization to retain both semantic and acoustic information, enabling effective modeling of the speech modality within the LLM. Baichuan-Audio further incorporates an independent audio head to enhance the capability of model to process and capture unique audio features. We conducted large-scale pretraining on audio-text data comprising approximately 100 billion tokens. Based on an extensive audio corpus comprising 887k hours, we implement an interleaved data processing approach, drawing upon the methodology outlined in \cite{kim2024unified,nguyen2025spirit,zeng2024scaling}, to facilitate effective knowledge transfer within the LLM framework. To preserve textual understanding during audio modeling, a two-stage pretraining strategy was introduced, where audio embeddings and the audio head were initially trained independently to maintain language comprehension. Baichuan-Audio demonstrates exceptional performance in real-time speech interactions and exhibits robust question-answering capabilities, highlighting its versatility and efficiency.
The main contributions of \textbf{Baichuan-Audio} can be summarized as follows: 
\begin{itemize}

    \item \textbf{Unified and Outstanding Speech Capabilities}: We design an 8-layer RVQ audio tokenizer (Baichuan-Audio-Tokenizer) achieves an optimal balance between capturing semantic and acoustic information with 12.5 Hz frame rate, which supports high-quality controllable bilingual (Chinese and English) real-time conversations.

    \item \textbf{End-to-end Speech Interaction}: Baichuan-Audio is designed to process text, audio inputs, delivering high-quality text and speech outputs. It is capable of delivering high-quality, seamless speech interaction while maintaining intelligent response. At the same time, we have also open-sourced the training data and foundational model, providing valuable resources and tools to advance research and innovation in the field of voice interaction.
\end{itemize}

