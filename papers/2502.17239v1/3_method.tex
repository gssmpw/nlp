\section{\ours}
%
Baichuan-Audio is an advanced end-to-end audio large language model designed for real-time speech interaction. The overall architecture of the Baichuan-Audio model is presented in \autoref{fig:audio}. The model architecture is structured around three foundational components: the Baichuan-Audio Tokenizer, the audio LLM and an audio decoder. The processing pipeline begins with the audio tokenizer, which converts raw audio input into discrete tokens by capturing both semantic and acoustic information. This is achieved through a combination of high-level feature extraction via the Whisper Encoder and RVQ techniques. The audio LLM then generates aligned text and audio tokens in an alternating manner, facilitated by a specialized token that enables seamless modality switching between text and audio. The audio tokens are subsequently processed by an independent audio head. Finally, a flow-matching based audio decoder reconstructs a high-quality Mel spectrogram from these tokens, which is then converted into an audio waveform via a vocoder. In this section, we will delve into the model architecture of Baichuan-Audio, providing a comprehensive analysis of its training data and the methodologies employed in its development.

\begin{figure*}[t]
  \centering
  \includegraphics[width=16cm]{figures/audio-llm.pdf}
  \caption{\textbf{The overview of Baichuan-Audio.} Our model is an end-to-end large audio  language model. When generating audio, the audio LLM alternately predicts text tokens and audio tokens. The audio tokens are then decoded by the flow-matching based audio decoder to produce the final audio.}
  \label{fig:audio}
\end{figure*}

\subsection{Audio Tokenization}


\par The main challenge for current audio tokenizers lies in achieving an optimal balance between capturing the semantic and acoustic information in the input speech signal \cite{wang2024speech}. We believe that models such as Baichuan-Omni and Qwen-Audio provide a more direct approach to capturing semantic features, compared to self-supervised learning methods like HuBERT \cite{hsu2021hubert}. Meanwhile, audio tokenizer like Encodec \cite{defossez2022high} and SpeechTokenizer \cite{zhang2024speechtokenizer} are particularly effective at fully reconstructing audio features. To combine the advantages of these two approaches and inspired by the work of \cite{wu2024vila}, we propose Baichuan-Audio-Tokenizer, an audio tokenizer based on RVQ \cite{defossez2022high} and multi-objective training, as illustrated in \autoref{fig:tokenizer}. The Baichuan-Audio-Tokenizer retains the audio encoder and the LLM component from Baichuan-Omini \cite{li2024baichuan}, while adding an audio decoder structure after the encoder to reconstruct the input Mel spectrogram. The audio tokenizer is trained using a multi-objective optimization approach to effectively capture both semantic and acoustic information.

The Baichuan-Audio-Tokenizer utilizes a frame rate design of 12.5 tokens per second. The high-level audio features are extracted from Mel spectrograms using the Whisper Large Encoder~\cite{radford2022robustspeechrecognitionlargescale}, followed by a residual convolutional network performing 4$\times$ downsampling to obtain low frame rate audio features. Since the audio features output by the Whisper Encoder are high-dimensional, it is essential to use an 8-layer RVQ to minimize information loss during the quantization process. We design the codebook sizes in a decreasing manner as $\{8K, 4K, 2K, 1K, 1K, 1K, 1K, 1K\}$. The audio decoder employs a fully symmetric structure to the Whisper Encoder, processing its input through a deconvolution module for 4× upsampling. After passing through a series of Transformer layers, the sequence undergoes an additional 2× upsampling, resulting in a coarse Mel-spectrogram representation with 100 tokens per second. Inspired by \cite{li2019neural,meng2024autoregressive}, we design a refined network to improve the accuracy of Mel-spectrogram reconstruction, yielding high-quality refined Mel-spectrogram features. In the design of the audio reconstruction loss, we refer to \cite{meng2024autoregressive} and adopt a combination of L2 and L1 losses as the reconstruction loss. The reconstruction loss is defined as follows:
\begin{equation}
\begin{aligned}
  \text{Loss}_{reconstruct} &= L_1(\text{Mel}_{gt}, \text{Mel}_{coarse}) + L_1(\text{Mel}_{gt}, \text{Mel}_{refined}) \\
  &+ L_2(\text{Mel}_{gt}, \text{Mel}_{coarse}) + L_2(\text{Mel}_{gt}, \text{Mel}_{refined})
\end{aligned}
\end{equation}
\begin{wrapfigure}{r}{0.45\textwidth}
  \centering
  \includegraphics[width=6cm]{figures/vq.pdf}
  \caption{Baichuan-Audio-Tokenizer.}
  \label{fig:tokenizer}
\end{wrapfigure}
To enhance audio reconstruction quality, we introduce a multi-scale Mel loss approach \cite{kumar2024high}, utilizing two distinct hop lengths and window sizes. This effectively mitigates information loss caused by dimensionality reduction and downsampling interpolation during the transformation from the decoder output to the Mel-spectrogram. By optimizing across multiple scales, the method preserves more detailed audio features, enhancing both reconstruction fidelity and training stability. For the pretrained LLM, the objective is to maximize the softmax probability of text outputs in audio understanding tasks. To ensure semantic alignment, the pretrained LLM maximizes the text softmax probability for audio understanding tasks, with its parameters kept fixed during training to preserve the alignment between the audio tokenizer and the textual LLM space. For the selection of LLM size, we observed during the training of audio understanding models that different LLM sizes have minimal impact on ASR performance metrics. Therefore, we chose a pretrained LLM with 1.5 billion parameters for continued pretraining. This LLM size also aligns well with the Audio Decoder, resulting in smaller gradient norm differences between the two components and improved training stability. We utilize an Exponential Moving Average (EMA) strategy to update the codebook and employ a Straight-Through Estimator (STE) for backpropagating gradients to the encoder. Additionally, we utilize a Vector Quantization (VQ) commitment loss to ensure the encoder outputs align closely with the codebook entries. The VQ commitment loss is defined as:
\begin{equation}
\text{Loss}_{commit} = | z - \text{quantize}(z) |_2^2,
\end{equation}
The total loss is a weighted combination of the multi-scale reconstruction loss, text-audio alignment loss (for the LLM), and VQ commitment loss:
\begin{equation}
\text{Loss}_{total} = \lambda_1 \cdot \text{Loss}^{'}_{reconstruct} + \lambda_2 \cdot \text{Loss}_{llm} +  \lambda_3 \cdot \text{Loss}_{commit}
\end{equation}
\textbf{RVQ Training Details.}
During the VQ training process, we introduce layerwise dropout, randomly discarding the VQ outputs of all layers starting from the second layer onward. This approach ensures that key semantic information is concentrated in the top layers of the codebook, achieving an effect similar to distilling the first-layer codebook in SpeechTokenizer \cite{zhang2024speechtokenizer}. To enhance codebook utilization, we implement a restart strategy alongside Gumbel sampling. Specifically, if a cluster within the codebook remains unused for a certain number of steps, it is randomly replaced with an input from the current batch. Meanwhile, we employ Gumbel-Softmax sampling during training to introduce stochasticity, ensuring effective exploration of the codebook space.  Additionally, to constrain the distribution of the codebook within a specific range, the L2 norm of the codebook is maintained during EMA updates. The final update formula is:
\begin{equation}
  c_{j, t} = c_{j, t-1} \times \alpha + \frac{1}{N} \sum_{x_{i} \in c_{j,t}} x_{i}
  \label{eq:formula1}
\end{equation}

\begin{equation}
  c_{j,t} = (1 - \beta) \times c_{j, t}^{\prime}
  \label{eq:formula2}
\end{equation}

where \( c_{j, t} \) denotes the codebook cluster at time step \( t \), \( c_{j, t-1} \) is its previous state, \( \alpha \) is the EMA decay factor, \( N \) is the number of inputs assigned to the cluster, and \( x_i \) represents the input samples in the current batch associated with \( c_{j, t} \). Here, \( c_{j, t}' \) represents the intermediate update of the cluster before normalization, and \( \beta \) is the constraint factor used to regulate the L2 norm of the codebook vector, ensuring its distribution remains within a bounded range. For the L2 norm constraint, \( c_{j, t}' \) is the intermediate updated cluster before normalization, and \( \beta \) is the constraint factor controlling the compression of the distribution of codebook vector. We observed that L2 norm constraints significantly reduce the mutual information of the codebook, concentrating the VQ distribution on a few top token IDs. This helps simplify downstream tasks, such as TTS, by improving the prediction accuracy of audio tokens. However, excessively strong L2 norm constraints can lead to codebook redundancy and reduce overall codebook utilization. Therefore, the $\beta$ parameter in the formula requires careful fine-tuning. To address this, we adopt a multi-stage progressive training strategy, gradually increasing the proportion of Whisper Encoder outputs replaced by VQ results, from 10\% to 100\%, while simultaneously adjusting the weight of the commit loss. We found that replacing the entire sample with VQ is more effective than randomly replacing individual tokens with VQ, i.e., instance-level replacement is better than token-level replacement.


\textbf{Training Data.}
In addition to traditional tasks such as Automatic Speech Recognition (ASR), Audio Query Answering (AQA), and Speech-to-Text Translation (S2TT), we incorporate a proportion of audio-text interleaved data into the training process. This strategy aims to enhance the VQ module's capability to model complex contextual scenarios. Specifically, the training dataset consists of 135k hours of ASR data, 11k hours of AQA data, 9k hours of S2TT translation data, and 52k hours of audio-text interleaved data. Further details about the dataset are provided in Section 3.3.

\textbf{Evaluation of Baichuan-Audio-Tokenizer.}
To evaluate the performance of the tokenizer in terms of pre-training and post-training losses, we trained a non-VQ version of the audio understanding model as a baseline using the same data and base model. For both the VQ and non-VQ models, the parameters of the LLM were kept frozen during training to ensure a fair comparison and to isolate the impact of the VQ mechanism on the overall performance. From \autoref{tab:vq8-metrics}, we can see that the 8-layer vq is closer to the baseline and has the least loss of semantic content. As shown in \autoref{tab:vq_metrics}, the ASR results of the 8-layer VQ model and the baseline across multiple datasets demonstrate that the trained 8-layer VQ model achieves competitive performance.

\input{tables/vq8}
\begin{table}[ht]
    \caption{\textbf{Comparison of ASR Performance between the Baseline and the 8-layer VQ Model.} The evaluation metric is WER(\%).}
    \centering
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{lcc}
        \hline
        \textbf{Dataset} & \textbf{Baseline} & \textbf{VQ Model (8 Layers)} \\
        \hline
        Fleurs-ZH & 3.54 & 4.15 \\
        Fleurs-EN & 5.98 & 8.64 \\
        Med-ZH-inhouse & 4.39 & 6.03 \\
        Wenet testnet & 7.32 & 8.29 \\
        Wenet testmeeting & 8.54 & 9.03 \\
        Covost-2 en2zh BLEU & 33.94 & 30.4 \\
        Covost-2 zh2en BLEU & 21.7 & 19.7 \\
        Clotho AQA & 40.4 & 37.2 \\
        \hline
    \end{tabular}
    \label{tab:vq_metrics}
\end{table}
\subsection{Flow-matching based audio decoder}



To improve the quality and fidelity of synthesized audio, the audio decoder module is enhanced using a flow-matching model \cite{lipman2022flow}, trained on 24 kHz audio to generate target Mel spectrograms. The flow-matching decoder includes a Pre-Net and a conditional decoder, as shown in \autoref{fig:decoder}. The Pre-Net maps intermediate representations to a prior for the vocoder, using an MLP and a 12-layer transformer. The MLP projects 1280-dimensional, 50 Hz features to 512 dimensions, refined by the transformer, and a final linear layer converts them into 80-dimensional Mel spectrograms. 
\begin{wrapfigure}{r}{0.4\textwidth}
  \centering
  \includegraphics[width=3.6cm]{figures/decoder.pdf}
  \caption{Flow-matching based audio decoder.}
  \label{fig:decoder}
\end{wrapfigure}
The flow-matching conditional decoder uses a U-Net structure trained with OT-CFM, inspired by Matcha-TTS \cite{mehta2024matcha} and CosyVoice \cite{du2024cosyvoice}. The U-Net includes one down-sampling block, one up-sampling block, and 12 intermediate blocks, each with a ResNet1D and transformer layer (256 dimensions). A linear layer projects features to 80-dimensional Mel spectrograms. As the model already encodes acoustic information (e.g., speaker timbre) via reconstruction loss, no additional speaker embeddings are used. The generated Mel spectrograms are converted into waveforms using the HiFi-GAN \cite{kong2020hifi,du2024cosyvoice} vocoder\footnote{https://www.modelscope.cn/models/iic/CosyVoice2-0.5B}.

\textbf{Training Details.}
The flow-matching model was trained on about 270k hours of audio, including Mandarin, English, various dialects, and multilingual data. Data quality was refined using ensemble ASR and MOS filtering. During training, the AudioEncoder, VQ layers, and AudioDecoder were fixed, while the flow-matching Pre-Net and decoder were trained with a prior loss added to the Pre-Net.

\textbf{Reconstruction result.}
We evaluated the performance of the trained flow-matching network on the LibriSpeech-dev set. UTMOS \cite{saeki2022utmos} was used as a proxy for subjective audio perception, with the score improving from 3.43 to 4.05, closely approaching the ground truth score of 4.08. Content quality was assessed using Whisper ASR \cite{radford2023robust}, where the Word Error Rate (WER) decreased from 2.84 to 2.78. 

\input{tables/fm_eval}

\subsection{Audio LLM}
\par The Baichuan-Audio extends a pre-trained LLM by incorporating the newly introduced Baichuan-Audio-Tokenizer, which includes audio embedding layers and an independent audio head. Specifically, the audio tokens from Baichuan-Audio-Tokenizer are first transformed into audio embeddings through audio embedding layers. The audio LLM alternately generates aligned text tokens and audio tokens, facilitated by a special token that enables modality switching between text and audio. The generated audio tokens are processed by an independent audio head, consisting of 3 layers of depth transformers and 8 classification heads. Finally, the audio embeddings are passed through an audio encoder, such as a flow-matching-based audio encoder and vocoder, to reconstruct audio waveforms. 
\footnotetext[2]{https://github.com/sarulab-speech/UTMOS22}

\textbf{Audio embedding.}
First, the 8 discrete audio token are summed through a corresponding number of embedding layers to obtain audio embeddings. Each embedding layer takes an input dimension that is one greater than the size of the corresponding codebook, due to the inclusion of an additional special token that signifies the end of audio token generation.

\textbf{Audio head.}
The generated audio tokens are processed using an independent audio head, which consists of 3 layers of depth transformers and 8 classification heads. The depth transformer has a depth of 8, predicting audio embeddings for 8 codebooks. Finally, the classification heads are used to obtain the logits for each codebook corresponding to the audio tokens.
%参数量(4+16*2)*3* 3584**2 +  (8+4+2+1+4)*1024*3584
%heads：3584*1024*(8+4+2+1+1+1+1)



Compared to pure text-based large models, speech language models often struggle to generate semantically coherent outputs. Research in \cite{wang2024speech} indicates that this issue primarily arises from the introduction of duration and paralinguistic information in speech. To address this issue, two types of interleaved data are employed during pre-training: Audio-Text Interleaved (INTLV) and Interleaved Text-to-Speech (ITTS), which contribute to enhancing audio understanding and generation capabilities. During inference, discrete audio tokens are fed into the LLM, and the model alternately generates aligned text tokens and audio tokens. Special tokens facilitate modality switching between text and audio. This forced alignment approach ensures that the model generates coherent and complete textual content before synthesizing the corresponding audio tokens, effectively guiding the generation of audio tokens and mitigating the issue of semantic degradation.


\begin{figure*}[htb]
  \centering
  \includegraphics[width=16cm]{figures/data.pdf}
  \caption{Pipeline of interleaved data collection.}
  \label{fig:data}
\end{figure*}
\subsubsection{Pre-training details}
\textbf{Pre-training data.}
\input{tables/audiodata}
The detailed statistics of the training data for audio pre-training are shown in \autoref{tab:audio-data-summary}. Audio data can be broadly categorized into two primary types: audio understanding data and audio generation data. Audio understanding data includes Automatic Speech Recognition (ASR), Audio Question Answering (AQA), Speech-to-Text Translation, and Audio-Text Interleave data. Audio generation data encompasses Text-to-Speech (TTS), Interleaved Text-to-Speech data, and pure audio data. Interleaved data consists of alternating text and audio modalities, segmented by punctuation marks to facilitate cross-modal knowledge transfer. The interleaved aligned generation data composed of fully aligned text and audio content, designed to enhance the model’s ability to generate audio tokens under text supervision. The audio-text paired data (e.g., ASR and TTS data) improve the performance on fundamental speech tasks. Pure audio data, on the other hand, enhances the capability to independently process audio modalities. The interleaved data collection process is shown in \autoref{fig:data}, which is divided into two types: crawl data and synthetic data. In total, we obtained 142k hours of ITTS data and 393k hours of INTLV data. The interleaved data is segmented using LLM, which performs natural segmentation based on punctuation or natural pauses in the text content. For the segmented text data of synthetic data, we also use a large language model for text normalization \cite{zhang2024chat}. During pre-training, we excluded the loss computation for the audio segments in the Audio-Text Interleaved data, a design choice that differs from GLM-4-Voice. Empirical observations under the current about 50B training audio data scale revealed that computing loss for the audio segments in INTLV data led to performance degradation. This decision is further justified by the inherent modality conflict between audio and text, as well as the absence of a requirement for text-to-audio continuation during inference. Consequently, we omitted the loss calculation for the audio segments in INTLV data. For ITTS data, loss was computed for both audio and text segments, except for the initial text segment, to enhance the model's capability in text-guided audio generation.



\textbf{Two stage training strategy.}
To address the potential disruption of the original textual knowledge in the LLM caused by the distinct characteristics of speech features compared to text features, we propose a two-stage training strategy to mitigate training conflicts between modalities. In the first stage, the parameters of the LLM remain fixed, allowing only the parameters of the audio embedding layer and the audio head to be updated. In the second stage, all parameters, except for those of the text embedding layer and the LM head, are made trainable.

\subsubsection{Supervised fine-tuning details}
The supervised fine-tuning stage aims to enhance the model's ability to follow complex instructions across a range of tasks. The audio SFT data are derived from a large collection of textual instructions. High-quality instructions are selected using a filtering strategy based on instruction type, diversity, and overall quality. Audio instructions are synthesized using a curated dataset of 10,000 distinct voice tones. Corresponding text responses are generated and segmented at natural conversational pauses before being converted into audio using the designated voice tones. These datasets cover multiple tasks and contain approximately 242k audio data pairs. 

To ensure the quality of the synthesized audio, Automatic Speech Recognition (ASR) is applied to the generated audio files. The ASR outputs are compared against the original text to validate quality. This process results in the creation of high-quality end-to-end conversational datasets. Synthesized audio files with errors are added to the Text-to-Speech (TTS) dataset, while cases with ASR errors are incorporated into the ASR training dataset. This iterative approach of incorporating challenging examples enhances both TTS and ASR performance.

Special attention is required to address cases where text-to-audio conversion makes the original textual response unsuitable as an audio reply. This issue arises due to differences in tone, speed, and expression between text and audio. Some textual content may fail to convey the intended meaning or introduce ambiguity when converted into audio. Consequently, careful review and adjustment of such cases are essential during the generation process. This ensures that the synthesized data accurately reflects real-world voice interaction scenarios, enhancing data reliability and improving the model's practical applicability.





