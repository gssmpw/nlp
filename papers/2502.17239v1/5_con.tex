\section{Conclusion}

In this paper, we introduce Baichuan-Audio, an end-to-end large language model designed for audio that integrates both speech comprehension and generation. The model employs a multi-codebook discretization of speech signals at 12.5 Hz via a pre-trained ASR model, which preserves both semantic and acoustic information in speech tokens. Additionally, an independent audio head is specifically designed to process these tokens efficiently. To balance audio modeling and language capability preservation, a two-stage pre-training strategy with interleaved data is adopted. The proposed framework supports speech interaction through text-guided aligned speech generation, thereby further retaining the model's foundational cognitive abilities. With open-sourced training data and models, Baichuan-Audio makes a significant contribution to the advancement of real-time speech interaction systems.