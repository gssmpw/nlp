\section{Related works}
\subsection{Audio Large Language Models}
The development of audio large language models is driven by advancements in speech tokenizers and large language model research. The speech tokenizers serve as a crucial bridge between audio segments and discrete language models by transforming continuous audio signals into discrete tokens. The self-supervised learning models, such as HuBERT\cite{hsu2021hubert} and WavLM\cite{chen2022wavlm}, effectively capture semantic information from speech. The neural acoustic codecs \cite{zeghidour2021soundstream,defossez2022high,kumar2024high} are designed to preserve the full range of audio signal information through discrete encoding. Recent studies \cite{zhang2024speechtokenizer,defossez2024moshi} have also utilized distilling semantic features sush as pre-trained HuBERT to ensure that specific layers of residual vector quantization (RVQ) retain enriched semantic content. In audio understanding, methods like Qwen-Audio\cite{chu2023qwen,chu2024qwen2}, Wavllm\cite{hu2024wavllm}, and Pengi\cite{deshmukh2023pengi} combine Whisper encoder\cite{radford2023robust} with large language models, utilizing multi-task learning strategies across speech and language tasks. These approaches have demonstrated excellent performance in speech-to-text tasks. In text-to-speech (TTS), Wang et al. introduced VALL-E\cite{wang2023neural}, conceptualizing TTS as a conditional language modeling problem. To further enhance the efficiency and fidelity of speech synthesis, decoders such as CosyVoice\cite{du2024cosyvoice} and Matcha-TTS\cite{mehta2024matcha} leverage flow-matching techniques, enabling high-quality speech generation.


\subsection{End-to-end audio LLM}
End-to-end speech interaction models have emerged as a central research focus within the speech processing community, driven by the increasing demand for seamless and efficient multimodal communication systems. Inspired by GPT-4o\cite{HelloGPT4o}, significant advancements have been made in the development of open-source models. For instance, Moshi \cite{defossez2024moshi} introduces an end-to-end full-duplex spoken dialogue foundation model, capable of simultaneously generating audio tokens and text tokens through a multi-stream output mechanism. GLM-4-Voice \cite{zeng2024glm} leverages interleaved data for pre-training to enable text-guided interleaved generation of speech. Freeze-Omni \cite{wang2024freeze} extends the capabilities of large language models (LLMs) to process speech modalities by incorporating modality-specific adapters. It further employs a speech decoder to transform the LLM's output into audio tokens, enabling high-quality speech generation. Following a similar technical approach to Freeze-Omni, models such as VITA-1.5 \cite{fu2025vita} and SALMONN-Omni \cite{yu2024salmonn}. These developments collectively highlight the ongoing progress in creating robust and versatile frameworks for speech-based applications.