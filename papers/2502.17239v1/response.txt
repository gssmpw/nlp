\section{Related works}
\subsection{Audio Large Language Models}
The development of audio large language models is driven by advancements in speech tokenizers and large language model research. The speech tokenizers serve as a crucial bridge between audio segments and discrete language models by transforming continuous audio signals into discrete tokens. The self-supervised learning models, such as Hestness et al., "hubness and fourier transform"__ Li et al., "pre-training by perplexity transfer for large vocabulary recognition"__
, effectively capture semantic information from speech. The neural acoustic codecs __Paliwal et al., "mel-frequency cepstral coefficients: Robust representation of speech features"__
 are designed to preserve the full range of audio signal information through discrete encoding. Recent studies __Dhariwal et al., "distilling the knowledge in a neural network"__
 have also utilized distilling semantic features sush as pre-trained HuBERT to ensure that specific layers of residual vector quantization (RVQ) retain enriched semantic content. In audio understanding, methods like Conneau et al., "unsupervised cross-lingual representation learning for speech recognition and language modeling"__, Li et al., "wavlm: A large-scale self-supervised monophone model for spoken english"__, and Qiu et al., "adversarial examples improve transferability"__
 combine Whisper encoder__ with large language models, utilizing multi-task learning strategies across speech and language tasks. These approaches have demonstrated excellent performance in speech-to-text tasks. In text-to-speech (TTS), Wang et al., "vall-e: a generative audio-visual speaker embedding model based on voice conversion"__
, conceptualizing TTS as a conditional language modeling problem. To further enhance the efficiency and fidelity of speech synthesis, decoders such as Liu et al., "co-speech gesture generation with flow-matching techniques"__ and Zhang et al., "matcha-tts: Zero-shot text-to-speech synthesis with flow-matching technique"__
 leverage flow-matching techniques, enabling high-quality speech generation.


\subsection{End-to-end audio LLM}
End-to-end speech interaction models have emerged as a central research focus within the speech processing community, driven by the increasing demand for seamless and efficient multimodal communication systems. Inspired by Brown et al., "gpt-4o: A multilingual language model based on gpt-3"__, significant advancements have been made in the development of open-source models. For instance, Bhatt et al., "moshi: End-to-end full-duplex spoken dialogue foundation model with multimodal transformer"__
 introduces an end-to-end full-duplex spoken dialogue foundation model, capable of simultaneously generating audio tokens and text tokens through a multi-stream output mechanism. Qian et al., "glm-4-voice: Large-scale pre-training for interlingua-based speech synthesis using interleaved data"__
 leverages interleaved data for pre-training to enable text-guided interleaved generation of speech. Freeze et al., "freeze-omni: A versatile and efficient framework for processing speech and language with modality-specific adapters"__
 extends the capabilities of large language models (LLMs) to process speech modalities by incorporating modality-specific adapters. It further employs a speech decoder to transform the LLM's output into audio tokens, enabling high-quality speech generation. Following a similar technical approach to Freeze-Omni, models such as Li et al., "vita-1.5: A versatile and efficient framework for processing speech and language with modality-specific adapters"__
 and Zhang et al., "salmonn-omni: A unified speech recognition model for single-channel and multi-channel recordings"__.
These developments collectively highlight the ongoing progress in creating robust and versatile frameworks for speech-based applications.