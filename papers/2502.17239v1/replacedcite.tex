\section{Related works}
\subsection{Audio Large Language Models}
The development of audio large language models is driven by advancements in speech tokenizers and large language model research. The speech tokenizers serve as a crucial bridge between audio segments and discrete language models by transforming continuous audio signals into discrete tokens. The self-supervised learning models, such as HuBERT____ and WavLM____, effectively capture semantic information from speech. The neural acoustic codecs ____ are designed to preserve the full range of audio signal information through discrete encoding. Recent studies ____ have also utilized distilling semantic features sush as pre-trained HuBERT to ensure that specific layers of residual vector quantization (RVQ) retain enriched semantic content. In audio understanding, methods like Qwen-Audio____, Wavllm____, and Pengi____ combine Whisper encoder____ with large language models, utilizing multi-task learning strategies across speech and language tasks. These approaches have demonstrated excellent performance in speech-to-text tasks. In text-to-speech (TTS), Wang et al. introduced VALL-E____, conceptualizing TTS as a conditional language modeling problem. To further enhance the efficiency and fidelity of speech synthesis, decoders such as CosyVoice____ and Matcha-TTS____ leverage flow-matching techniques, enabling high-quality speech generation.


\subsection{End-to-end audio LLM}
End-to-end speech interaction models have emerged as a central research focus within the speech processing community, driven by the increasing demand for seamless and efficient multimodal communication systems. Inspired by GPT-4o____, significant advancements have been made in the development of open-source models. For instance, Moshi ____ introduces an end-to-end full-duplex spoken dialogue foundation model, capable of simultaneously generating audio tokens and text tokens through a multi-stream output mechanism. GLM-4-Voice ____ leverages interleaved data for pre-training to enable text-guided interleaved generation of speech. Freeze-Omni ____ extends the capabilities of large language models (LLMs) to process speech modalities by incorporating modality-specific adapters. It further employs a speech decoder to transform the LLM's output into audio tokens, enabling high-quality speech generation. Following a similar technical approach to Freeze-Omni, models such as VITA-1.5 ____ and SALMONN-Omni ____. These developments collectively highlight the ongoing progress in creating robust and versatile frameworks for speech-based applications.