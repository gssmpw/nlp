\section{Related Work}
\textbf{Machine Learning for Gyrokinetics.}
Most of the literature to date has focused on multilayer perceptrons as surrogate models of turbulence models that adopt the quasilinear approximation. 
Faster integrated models ____  of tokamak discharges were obtained for interpretative modelling in existing machines ____ as well as predictive modelling ____ and control studies ____ of future reactor-class devices, clearly highlighting the benefits of surrogate modelling, albeit on reduced order models.

The literature on surrogates for higher-fidelity models is sparse. 
To model the linear spectra of micro-tearing modes, ____ propose leveraging gaussian process regression (GPR).
GPR also enables uncertainty quantification, but does not scale well to larger datasets and dimensionalities.
Therefore, ____ leverage convolutional neural networks based on the spectral images of the absolute values of the distribution function along with the electric potential at a fixed time slice and predict the corresponding heat flux and the time to saturation. 
____ extends this idea to a two-dimensional multimodal input space, including electrostatic potentials.
Our method fundamentally differs in that it directly models the time evolution of the 5D distribution function, thus enabling the computation of turbulent fluxes at any time. 

\textbf{Neural Operators.} 
Importantly, our \ourmethod{} does not fall into the category of neural operators, as it is not resolution invariant but operates on a fixed resolution.
However, in future work we aim to extend our model to a neural operator, hence we elaborate on important related work in this regard.
Neural operators ____ are formulated with the objective of learning a mapping between function spaces, usually defined as Banach spaces U, V of functions defined on compact input and output domains X and Y, respectively. 
Neural operators enable continuous outputs that remain consistent across varying input sampling resolutions. 
A neural operator $\hat{\cG} : \cU \mapsto \cV$ approximates the ground truth operator $\cG : \text{U} \mapsto \text{V}$, and is usually composed of three maps $\hat{\cG} := \text{D} \circ \text{A} \circ \text{E}$ ____, comprising the encoder E, the approximator A, and the decoder D. 
Training a neural operator involves constructing a dataset of input-output function pairs evaluated at discrete spatial locations. 
Training minimizes a mean squared error loss over the discretized space using gradient descent.

Over the recent years, driven by advances in neural operator learning, deep neural network-based surrogates have emerged as a computationally efficient alternative in science and engineering ____, impacting e.g., weather forecasting ____, protein folding ____, material
design ____, and multi-physics modeling____. All these success stories share the common thread of deep learning surrogates unlocking new possibilities to overcome seemingly insurmountable challenges____.

\textbf{Swin Transformers.}
Other works, such as the 4D fMRI Swin Transformer ____ extend Swin Transformers to the specific case of four dimensions (three spatial and time) to process fMRI scans. 
Since our nD layers do not rely on assumptions about the number of dimensions, we effectively automate the process used to derive SwiFT (4D) and Video-Swin ____ (3D) from standard 2D Swin layers.