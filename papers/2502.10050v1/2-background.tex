\section{Background}

\subsection{Traditional Recommendation} 

In conventional recommendation systems, the problem is typically formulated over a user space $\mathcal{U} = {[u_1, u_2, ..., u_m]}$, an item space $\mathcal{I} = {[i_1, i_2, ..., i_n]}$, and their interaction matrix $\mathcal{D} \in \mathbb{R}^{m \times n}$. The fundamental goal is to learn a preference function $p: \mathcal{U} \times \mathcal{I} \rightarrow \mathbb{R}$ that predicts user preferences:
\begin{equation}
\min_{\theta} \sum_{(u,i) \in \mathcal{D}} \mathcal{L}(p_{\theta}(u,i), y_{u,i}) \ ,
\end{equation}
where $p_{\theta}(u,i)$ represents the predicted preference and $y_{u,i}$ denotes the ground truth interaction. 
While various approaches have been proposed, from matrix factorization~\cite{hu2008collaborative} to deep learning~\cite{he2017neural}, these traditional methods face several inherent limitations. 
First, they struggle to understand complex user intents beyond numerical interactions. 
Second, they lack the ability to engage in meaningful interactions to explore user preferences. 
Third, their recommendations often appear as a ``black box'' without clear explanations for users.


\subsection{LLM as Agent}

Large Language Model (LLM) as an agent is an emerging research direction that has garnered significant attention~\cite{park2023generative}. 
By transcending the traditional static prompt-response paradigm, it establishes a dynamic decision-making framework~\cite{patil2023gorilla} capable of systematically decomposing complex tasks into manageable components. 
A typical LLM-powered agent architecture integrates four fundamental modules~\cite{wang2024survey}: (1) the Profile module, which constructs and maintains comprehensive user feature representations; (2) the Memory module, which orchestrates historical interactions and preserves contextual information for systematic experience accumulation; (3) the Planning module, which formulates strategic policies through sophisticated task decomposition and multi-objective optimization; and (4) the Action module, which executes decisions and facilitates environment interaction. 
The emergence of pioneering works such as ReAct~\cite{react}, Toolformer~\cite{toolformer}, and HuggingGPT~\cite{hugginggpt} has significantly advanced this field.



\subsection{LLM Agents for Recommendation}


In LLM-powered agent for recommender systems, we formulate the recommendation process through an agent-centric framework. 
Let $a \in \mathcal{A}$ denote an agent equipped with a set of functional modules $\mathcal{F} = {\mathcal{F}_1, \mathcal{F}_2, ..., \mathcal{F}_K}$, where each module $\mathcal{F}_k$ represents a specific capability. 
The recommendation process for a user $u$ can be formally expressed as:
\begin{equation}
\hat{\mathbf{y}}_u = f({\mathcal{F}_k(X_u)}), k=1 \cdots K \ ,
\end{equation}
where $X_u \in \mathcal{X}$ represents the input space containing user-specific information (e.g., interaction history, contextual features), and $\hat{\mathbf{y}}_u \in \mathbb{R}^N$ denotes the predicted preference distribution over the item space. 
The integration function $f: {\mathcal{F}_k(X_u)} \rightarrow \mathbb{R}^N$ synthesizes module outputs to generate final recommendations.
Building upon the previously introduced four functional module (Profile, Memory, Planning, and Action), this formulation provides a flexible framework that can accommodate various LLM-powered agent recommendation approaches. 
These modules operate in a closed-loop framework, where interaction data continuously enriches user profiles and system memory, informing planning strategies that ultimately manifest as personalized recommendations through action execution and feedback collection.