\section{Datasets and Evaluations}

In this section, we report the datasets and evaluation metrics used by various methods. 
The dataset information comes from the original source or paper.

\subsection{Datasets}

\paragraph{Traditional Recommendation Dataset} 

In Table~\ref{tab:traditional}, we list several traditional recommendation datasets for evaluating model performance.
These datasets provide comprehensive interaction data from various platform, including user-item interactions, timestamps, and review text, enabling the assessment of recommendation models. 
Several state-of-the-art methods have demonstrated their effectiveness using these datasets. 

For instance, the ``Books'' dataset (10.3M users, 4.4M items) from \textbf{Amazon Review data}~\cite{mcauley2015image} has been used to evaluate Agent4Rec~\cite{zhang2024generative} and BiLLP~\cite{shi2024large} performance on large-scale tasks, while the ``Video Games'' dataset (2.8M users, 137.2K items) has validated DRDT~\cite{wang2023drdt} and RAH~\cite{shu2024rah} capabilities. 
The ``Beauty'' dataset (632K users, 112.6K items) has been utilized by IntcRecAgent~\cite{huang2023recommender} and DRDT~\cite{wang2023drdt} to demonstrate their proficiency in recommendation. 
These diverse applications underscore the datasets' crucial role in advancing LLM-powered agent recommender systems and providing a foundation for evaluating various of algorithms.


The \textbf{MovieLens datasets}, introduced by ~\cite{harper2015movielens}, represent another crucial benchmark for evaluating LLM-powered agents recommenders, offering different scales of movie rating data from the MovieLens platform. 
These datasets range from MovieLens-100K (0.9K users, 1.6K items) to MovieLens-20M (138.5K users, 27.3K items), providing researchers with flexibility in testing their methods across different data scales.
Various state-of-the-art approaches have utilized these datasets: FLOW~\cite{cai2024flow} and MACRS~\cite{fang2024multi} have been validated on the smaller MovieLens-100K dataset, while Agent4Rec~\cite{zhang2024generative}, DRDT~\cite{wang2023drdt}, and MACRS~\cite{fang2024multi} have demonstrated their capabilities on MovieLens-1M. 
The larger variants like MovieLens-10M and MovieLens-20M have been employed by InteRecAgent~\cite{huang2023recommender} and RecAgent~\cite{yoon2024evaluating} respectively, showcasing the scalability of their approaches. 
This hierarchical structure of MovieLens datasets makes them particularly valuable for systematically evaluating recommendation algorithms at different scales.

\begin{table*}[t]
\resizebox{0.926\linewidth}{!}{
\begin{tabular}{p{2.5cm}cp{4cm}cccccp{4cm}}
\toprule[1.5pt]
\textbf{Category} & \textbf{Datasets} & \textbf{Reference} & \textbf{Users} & \textbf{Items} & \textbf{Interactions}   & \textbf{Conversations} & \textbf{Turns} & \textbf{Methods} \\
\midrule
\multirow{16}{*}[-5em]{\textbf{\makecell*[c]{Traditional \\ Recommendation \\ Dataset}}}
    & Books & \multirow{8}{*}{\cite{mcauley2015image}} & 10.3M & 
4.4M & 29.5M & - & - & Agent4Rec, BiLLP, RAH, SUBER \\
    & CDs and Vinyl &  & 1.8M &  701.7K  & 4.8M & - & - & AgentCF, KGLA, ToolRec \\
    & Video Games &  & 2.8M & 137.2K & 4.6M  & - & - &  DRDT, RAH, LUSIM \\
    & Beauty &  & 632.0K & 112.6K & 701.5K & - & - & InteRecAgent, DRDT, RecMind \\
    & Clothing &  & 22.6M & 7.2M & 66.0M &- & - & DRDT \\
    & Movies &  &  6.5M & 747.8K & 17.3M &- & - & RAH, LUSIM \\
    & Office Products &  & 7.6M & 710.4K & 12.8M &- & -  & AgentCF \\
    & Music &  & 101.0K & 70.5K & 130.4K &- & - & LUSIM \\
\cmidrule{2-9}

& Movielens-100K & \multirow{4}{*}{\cite{harper2015movielens}} & 0.9K &  
 1.6K & 100K &- & - & FLOW,  MACRS, SUBER \\
    & Movielens-1M &  & 6K &  
 3.7K & 1.0M &- & - & Agent4Rec, RecAgent, DRDT,  MACRS, ToolRec \\
 & Movielens-10M &  & 69.9K &  
 10.6K & 10M &- & - & InteRecAgent \\
  & Movielens-20M &  & 138.5K &  
 27.3K & 20M &- & - &  MACRS, UserSimulator \\
\cmidrule{2-9}

    & Steam & \cite{kang2018self} & 334.7K & 13K & 3.7M &- & - & Agent4Rec, BiLLP, FLOW, InteRecAgent \\
\cmidrule{2-9}
 & Lastfm & \cite{cantador2011second} & 1.2K & 4.6K & 73.5K &- & - & FLOW  \\
\cmidrule{2-9}
 & Yelp & \url{https://www.yelp.com/dataset} & 30.4K & 20.4K & 316.3K &- & - & RecMind, ToolRec, LUSIM \\
\cmidrule{2-9}
 & Anime & \url{https://www.kaggle.com/datasets} & 73.5K & 12.2K & 1.05M &- & - & LUSIM \\
\midrule
\multirow{3}{*}{\textbf{\makecell*[c]{Conversational \\ Recommendation \\ Dataset}}} & ReDial & \cite{li2018towards} & 0.9K & 51.6K & - & 10K & - & UserSimulator, CSHI\\
& Reddit & \cite{he2023large} & 36.2K  & 51.2K & - & 634.4K  & 1.6M & UserSimulator \\
& OpenDialKG & \cite{moon2019opendialkg} & -  & - &  - & 15.6K & 91.2K & CSHI \\
\bottomrule[1.5pt]
\end{tabular}}
\centering
\caption{Summary of Used Experimental Datasets.}
\label{tab:traditional}
\end{table*}

The \textbf{Steam}, \textbf{Lastfm}, \textbf{Anime}, and \textbf{Yelp} datasets provide diverse domain-specific evaluation scenarios for LLM-powered agent recommender systems. 
The Steam dataset, introduced by~\cite{kang2018self}, contains 3.7M interactions between 334.7K users and 13K gaming items, and has been extensively used by methods such as Agent4Rec~\cite{zhang2024generative}, BiLLP~\cite{shi2024large}, FLOW~\cite{cai2024flow}, and InteRecAgent~\cite{huang2023recommender} to validate their effectiveness in game recommendation. 
The Lastfm dataset~\cite{cantador2011second}, focusing on music recommendation, comprises 73.5K interactions from 1.2K users on 4.6K music items, and has been specifically utilized by FLOW~\cite{cai2024flow} to demonstrate its capabilities in the music domain. 
Additionally, the Yelp dataset, containing 316.3K interactions between 30.4K users and 20.4K items, has been employed by RecMind~\cite{wang2024recmind} to evaluate its performance in recommendations.
These domain-specific datasets offer unique evaluation opportunities in specialized recommendation contexts.

% \begin{table*}
% \resizebox{0.7\linewidth}{!}{
% \begin{tabular}{ccccccc}
% \toprule[1.5pt]
%  \textbf{Dataset} & \textbf{Reference} & \textbf{Users} & \textbf{Items} & \textbf{Conversations} & \textbf{Turns}  & \textbf{Methods} \\
% \midrule
% ReDial & \cite{li2018towards} & 0.9K & 51.6K & 10K & - & RecAgent, CSHI\\
% Reddit & \cite{he2023large} & 36.2K  & 51.2K & 634.4K  & 1.6M & RecAgent \\
% OpenDialKG & \cite{moon2019opendialkg} & -  & - & 15.6K & 91.2K & CSHI \\
% \bottomrule[1.5pt]
% \end{tabular}}
% \centering
% \caption{Conversational Recommendation Dataset}
% \label{tab:conversation}
% \end{table*}


\paragraph{Conversational Recommendation Dataset}

In addition to the above traditional recommendation datasets, some works~\cite{cshi}  evaluate the model performance on conversational datasets.
In Table~\ref{tab:traditional}, we list three widely-adopted datasets: \textbf{ReDial}~\cite{li2018towards}, \textbf{Reddit}~\cite{he2023large}, and \textbf{OpenDialKG}~\cite{moon2019opendialkg}. 
The ReDial dataset comprises 11348 multi-turn dialogues involving 6925 movies, where participants engage in seeker-recommender interactions. 
The Reddit dataset is derived from movie recommendation discussions within Reddit communities, where users post recommendation requests and receive responses with movie suggestions, often accompanied by explanatory rationales. 
This extensive dataset encompasses 634392 conversations, 1669720 dialogue turns, 36247 users, and 51203 movies.
CSHI~\cite{cshi} employs ReDial (movie domain, including 10006 dialogues) and OpenDialKG (multiple domains, including 13802 dialogues) for performance evaluation.
UserSimulator~\cite{yoon2024evaluating} evaluates on the Redial and Reddit datasets in a variety of ways, including behavior simulation and memory module believability, etc.
These authentic human-human conversations serve as crucial benchmarks for assessing the model capabilities of LLM-powered agents recommender systems.


It is worth mentioning that considering the agent recommender system based on LLMs, it is necessary to frequently call LLMs or APIs when the model is running. 
In order to save resources and time, some methods sample data from the original dataset for performance evaluation.
For instance, AgentCF~\cite{zhang2024agentcf} randomly samples two subsets (one dense and one sparse), with each subset containing 100 users.
DRDT~\cite{wang2023drdt} randomly samples 200 users from each dataset and uses the target items along with 19 randomly sampled items as the candidate item set.


\subsection{Evaluation}

In Table~\ref{tab:evaluation}, we summary the evaluation metrics used by recent representative methods.

\begin{table*}
\centering
\resizebox{0.9\linewidth}{!}{
\begin{tabular}{p{5cm}p{6cm}p{6cm}}
\toprule[1.5pt]
 \textbf{Category} & \textbf{Metrics} & \textbf{ Methods} \\
\midrule
\multirow{2}{*}[-1em]{Standard Recommendation} & NDCG@K, Recall@K, HR@K, Hit@K, MRR, Acc, F1-Score, MAP & DRDT, RecMind, InteRecAgent, RAH, MACRS, PMS, Agent4Rec, AgentCF, KGLA, FLOW, CSHI, ToolRec, SUBER \\
  & RMSE, MAE, MSE & RecMind \\
\midrule
Language Generation Quality  & BLEU, ROUGE & RecMind, PMS \\
\midrule
Reinforcement Learning  & Rewards & LUSIM, BiLLP, SUBER \\
\midrule
Conversational Efficiency  & Average Turn (AT), Success Rate (SR)  & InteRecAgent, MACRS, CSHI \\
\midrule
\multirow{2}{*}[-1em]{Custom Indicators} & Proactivity, Economy, Explainability, Correctness, Consistency, Efficiency & AutoConcierge \\
& Simulated user behaviors believability, Agent memory believability  & RecAgent \\
\bottomrule[1.5pt]
\end{tabular}}
\centering
\caption{Summary of Used Evaluation Metrics.}
\label{tab:evaluation}
\end{table*}

\paragraph{Standard Recommendation Metrics} Most existing methods employ standard recommendation evaluation metrics to assess model performance. The commonly utilized metrics including Normalized Discounted Cumulative Gain (NDCG@K), Recall@K and Hit Ratio@K (HR@K), etc.
For instance, AgentCF~\cite{zhang2024agentcf} evaluates its performance using NDCG@K and Recall@K on the MovieLens-1M dataset. 
Similarly, DRDT~\cite{wang2023drdt} conducts comprehensive evaluations using Recall@{10,20} and NDCG@{10,20} across multiple datasets including ML-1M, Games, and Luxury datasets.
Hit Ratio@K (HR@K) is another crucial metric for evaluating recommendation performance.
RecMind~\cite{wang2024recmind} employ that for evaluating the recommendation tasks on Amazon Reviews (Beauty) and Yelp datasets.


\paragraph{Language Generation Quality} Some methods~\cite{wang2024recmind} consider the evaluation of language generation quality (e.g., recommendation explanation generation, review summarization), which primarily rely on BLEU and ROUGE metrics.  
BLEU measures the precision of generated text against references, while ROUGE evaluates recall-based similarity, enabling comprehensive assessment of language generation capabilities in recommendation scenarios.
PMS~\cite{pms} utilizes the ROUGE to evaluate the quality of its generated textual recommendations.

\paragraph{Reinforcement Learning Metrics} In evaluating LLM-powered agent recommender systems for long-term engagement, BiLLP~\cite{shi2024large} employs three key metrics adopted from reinforcement learning: trajectory length, average single-round reward, and cumulative trajectory reward. 
Similarly, LUSIM~\cite{lusim} uses the total reward to reflect the overall user engagement during the entire interaction process, and the average reward to represent the average quality of a single recommendation.
These metrics are to evaluate both immediate recommendation quality and long-term engagement effectiveness.

\paragraph{Conversational Efficiency Metrics} Recent research has introduced more comprehensive metrics to evaluate the efficiency of conversational interactions in recommender systems. For instance, MACRS~\cite{fang2024multi} employs key interaction-focused metrics such as Success Rate  (proportion of successful recommendations) and Average Turn (AT) (number of interaction rounds needed to reach a recommendation) per session. 
These metrics assess how effectively the system can understand user preferences and deliver accurate recommendations while minimizing the number of interaction turns.

\paragraph{Custom Indicators} Beyond conventional metrics, some methods~\cite{yoon2024evaluating} propose customized evaluation frameworks.
AutoConcierge~\cite{zeng2024automated} presents six evaluation metrics for task-driven conversational agents: proactivity, economy, explainability, correctness, consistency, and efficiency.
RecAgent~\cite{wang2023user} proposes simulated user behaviors believability and Agent memory believability, to assess the credibility of LLM-simulated user interactions and memory mechanism effectiveness.
These metrics assess system engagement, dialogue efficiency, answer interpretability, response accuracy, requirement fulfillment, and response time, respectively.

In all, these metrics prioritize a holistic understanding of conversational performance, emphasizing balance between efficient recommendation delivery, and maintaining high-quality dialogue throughout the recommendation process.



% agent4rec Movielens-1M [15], Steam[22], Amazon-Book / recommendation res

% agentcf amazon cds office / recommendation res

% autoconierage ete dataset / proactivity,Economy,Explainability,Correctness,Consistency,Efficiency

% BILLP  Steam[22], Amazon-Book /  the trajectory length (Len), the average single-round reward (Reach), and the cumulative reward 

% drdt Movielens-1M dataset and the Games and Luxury Beauty categories from the Amazon Review datase / ranking performance

%FLOW Lastfm [5], Steam [24], and MovieLens [14] /  recommendation performance and user simulation perfor- mance.

% InterRecagent  Steam4, MovieLens5 and Amazon Beaut /  Since both our method and baselines utilize LLMs to generate response, which exhibit state-of-the-art text generation capabilities, our experiments primarily com- pare recommendation performance of different methods. For the user simulator strategy, we employ two metrics: Hit@k and AT@k, representing the success of recommending the target item within k turns and the average turns (AT) re- quired for a successful recommendation, respectively. Un- successful recommendations within k rounds are recorded as k + 1 in calculating AT. In the one-turn strategy, we fo- cus on the Recall@k and NDCG@k metric for retrieval and ranking task, respectively. In Recall@k, the k represents the retrieval of k items, whereas in NDCG@k, the k denotes the number of candidates to be ranked.

% KGLA : amazon review CDs, Clothing, and Beauty, / recommendation performance

% MACRS : MovieLens / Success Rate (↑) Average Turns (↓) Hit Ratio@5 (↑) Hit Ratio@10 (↑)


% PMS

% MACREC

% RAH Amazon: Movies,Books, and Video Games / alignment, burden reduction, and bias mitigation.

% recagent : 

% rEcllm


% recmind : Amazon Reviews (Beauty) and Yelp. / recommendation performance

%  Usersimularot ： ReDial Reddit Movinelens imdb /  ItemsTalk BinPref OpenPref RecRequest

% For example:
% \begin{equation}
% \text{Recall@K} = \frac{|G \cap R_K|}{|G|} \ ,
% \end{equation}
% where G denotes the set of ground truth items for a user, and $R_K$ represents the top-K items in the recommended list.
% \begin{equation}
% \text{NDCG@K} = \frac{\text{DCG@K}}{\text{IDCG@K}}, \quad \text{DCG@K} = \sum_{i=1}^K \frac{2^{r_i} - 1}{\log_2(i + 1)}
% \end{equation}
% where DCG@K is the Discounted Cumulative Gain at position K, IDCG@K is the ideal DCG@K, and $r_i$ represents the relevance score of the item at position i.

% For instance, AgentCF~\cite{zhang2024agentcf} evaluates its performance using NDCG@K and Recall@K on the MovieLens-1M dataset. 
% Similarly, DRDT~\cite{wang2023drdt} conducts comprehensive evaluations using Recall@{10,20} and NDCG@{10,20} across multiple datasets including ML-1M, Games, and Luxury datasets. 
% Hit Ratio@K (HR@K) is another crucial metric for evaluating recommendation performance:
% \begin{equation}
% \text{HR@K} = \frac{1}{|U|} \sum_{u \in U} \mathbb{1}(G_u \cap R_{u,K} \neq \emptyset) \ ,
% \end{equation}
% where U represents the set of all users, $G_u$ denotes the ground truth items for user u, and $R_{u,K}$ represents the top-K recommended items for user u. The indicator function $\mathbb{1}(\cdot)$ returns 1 if a ground truth item appears in the top-K recommendations, and 0 otherwise.
%This metric is employed by RecMind~\cite{wang2024recmind}, which evaluates the recommendation tasks on Amazon Reviews (Beauty) and Yelp datasets. 
%These standardized metrics ensures fair comparisons and validates the generalizability of different approaches.