\section{Related Research Fields}
\paragraph{LLM-powered Recommender Systems} In recent years, recommender systems based on Large Language Models (LLMs) have attracted widespread attention. 
Such systems make full use of the powerful language understanding and generation capabilities of LLMs, bringing a new paradigm to traditional recommender systems.
Most existing methods are primarily designed for rating prediction**Bischof, "Recurrent Recommender Networks"** and sequential recommendation**Hidasi et al., "Session-based Recommendations with Recurrent Neural Networks"**.
CoLLM**Zhang et al., "Collaborative Learning of Deep Models for Recommendation Systems"** captures and maps the collaborative information through external traditional models, forming collaborative embeddings used by LLMs. 
LlamaRec**Yuan et al., "LlamaRec: A Large-Scale Multi-Task Recommender System with Llama-2-7b"** fine-tunes Llama-2-7b for list-wise ranking of the pre-selected items.
However, these methods would face significant limitations: the inability to simulate authentic user behaviors for enhanced personalization, the lack of effective memory mechanisms for long-term context awareness, and the rigid pipeline structure that prevents flexible task decomposition and seamless integration with external tools.

\paragraph{Conversational Recommender Systems}

Conversational recommender systems (CRS) have emerged as a significant research direction in recent years**Liu et al., "Conversational Recommendation Systems: A Survey"**, which are similar to the LLM-powered agent recommender systems. 
However, traditional methods**Zhang et al., "A Comparative Study of Conversational Recommender Systems"** have two main drawbacks: attribute-based approaches are limited by rigid dialogue patterns, while generation-based methods suffer from restricted knowledge and poor generalization capabilities of small language models.