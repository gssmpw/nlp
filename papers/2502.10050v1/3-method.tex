\section{Methods}



In this section, we sort out existing LLM-powered agent recommendation works based on the overall objective of the method and the agent components of different methods.

\subsection{Method Objective}

In Table~\ref{tab:comparison}, we classify method objectives of existing methods into three categories: recommender-oriented approaches, interaction-oriented methods, and simulation-oriented methods.
The illustrations of categories are shown in Figure~\ref{objective}.


\textbf{(1) Recommender-oriented} approaches focus on developing intelligent recommendation equipped with enhanced planning, reasoning, memory, and tool-using capabilities. 
In these approaches, LLMs leverage users' historical behaviors to generate direct recommendation decisions. 
For instance, as shown in Figure~\ref{objective}, when a user demonstrates recent engagement with technology news and AI-related content, the system might strategically recommend: ``Here are 5 articles about latest large language model breakthroughs, 3 introductory articles about machine learning basics, and 2 popular science pieces about AI's impact on society.''
This paradigm demonstrates how agents can effectively combine their core capabilities to deliver direct item recommendations.


Representative works in this direction include RecMind~\cite{wang2024recmind}, which develops a unified LLM agent with comprehensive capabilities to generate recommendations directly through LLM outputs.
MACRec, which introduces an agent-collaboration mechanism that orchestrates different types of agents to provide personalized recommendations~\cite{wang2024macrec}.

\textbf{(2) Interaction-oriented} methods focus on enabling natural language interaction and enhancing recommendation interpretability through conversational engagement. 
These approaches utilize LLMs to conduct human-like dialogues or explanation while making recommendations. 
For example, as shown in Figure~\ref{objective}, an LLM might respond to a user query with: ``I noticed that you like science fiction movies, especially after watching The Descent and Star Trek recently. Considering this preference, I would like to recommend Space Odyssey 2001, a classic film that also explores profound themes about human and alien civilizations. What do you think?''
Such interactive recommendations showcase the agent's ability to not only track user preferences but also articulate recommendations in a conversational manner, explaining the reasoning behind suggestions.

AutoConcierge~\cite{zeng2024automated} uses natural language conversations to understand user needs and collect user preferences, and uses LLM to understand and generate language, ultimately providing explainable personalized restaurant recommendations.
RAH~\cite{shu2024rah} is a human-computer interaction recommendation framework based on LLM agents. 
It realizes personalized recommendations and user intent understanding through the ResSys-Assistant-Human tripartite interaction and the Learn-Act-Critic loop mechanism.


\begin{table*}[t]
\resizebox{0.92\linewidth}{!}{
\begin{tabular}{c|c|c|c|c|c}
\toprule[1.5pt]
\textbf{Category} & \textbf{Methods} & \textbf{Profile Module} & \textbf{Memory Module} & \textbf{Planning Module} & \textbf{Action Module} \\
\midrule
\multirow{7}{*}[-1.5em]{\textbf{\makecell*[c]{Recommender- \\ oriented \\ Method}}} 
 & RAH~\cite{shu2024rah}  & $\times$ & \checkmark & \checkmark & \checkmark \\
\cmidrule{2-6}
 & ToolRec~\cite{toolrec} & $\times$  & \checkmark & $\times$ & \checkmark \\
\cmidrule{2-6}
 & PMS~\cite{pms} & \checkmark  & $\times$ & $\times$ & \checkmark  \\
\cmidrule{2-6}
 & DRDT~\cite{wang2023drdt} & $\times$ & $\times$ & \checkmark & $\times$  \\
\cmidrule{2-6}
 & BiLLP~\cite{shi2024large} & $\times$ & \checkmark & \checkmark & \checkmark  \\
\cmidrule{2-6}
 & RecMind~\cite{wang2024recmind} & $\times$ & \checkmark & \checkmark & \checkmark  \\
\cmidrule{2-6}
 & MACRec~\cite{wang2024macrec} & \checkmark & $\times$ & \checkmark & \checkmark  \\
\midrule
\multirow{7}{*}[-1.5em]{\textbf{\makecell*[c]{Interaction- \\ oriented \\ Method}}} 
& AutoConcierge~\cite{zeng2024automated} & $\times$ & \checkmark & \checkmark & \checkmark  \\
\cmidrule{2-6}
& MACRS~\cite{fang2024multi} & \checkmark & \checkmark & \checkmark & \checkmark \\
\cmidrule{2-6}
 & RecLLM~\cite{friedman2023leveraging} & \checkmark & \checkmark & $\times$ & \checkmark  \\
\cmidrule{2-6}
 & InteRecAgent~\cite{huang2023recommender} & \checkmark & \checkmark & \checkmark & \checkmark  \\
\cmidrule{2-6}
 & MAS~\cite{thakkar2024personalized} & \checkmark & \checkmark & \checkmark & \checkmark \\
\cmidrule{2-6}
 & H-MACRS~\cite{hmacrs} & \checkmark  & \checkmark & $\times$ & \checkmark  \\
\cmidrule{2-6}
 & Rec4Agentverse~\cite{Rec4Agentverse} & \checkmark  & $\times$ & \checkmark & $\times$  \\
\midrule
\multirow{8}{*}[-2em]{\textbf{\makecell*[c]{Simulation- \\ oriented \\ Method}}} & KGLA~\cite{guo2024knowledge} & \checkmark & \checkmark & $\times$ & \checkmark \\
\cmidrule{2-6}
 & CSHI~\cite{cshi} & \checkmark & \checkmark & $\times$ & \checkmark \\
\cmidrule{2-6}
 & SUBER~\cite{suber} & \checkmark  & \checkmark & $\times$ & $\times$ \\
\cmidrule{2-6}
 & LUSIM~\cite{lusim} & \checkmark  & \checkmark & $\times$ & $\times$ \\
\cmidrule{2-6}
 & FLOW~\cite{cai2024flow} & \checkmark & \checkmark & $\times$ & \checkmark \\
\cmidrule{2-6}
 & Agent4Rec~\cite{zhang2024generative} & \checkmark & \checkmark & $\times$ & \checkmark \\
\cmidrule{2-6}
 & AgentCF~\cite{zhang2024agentcf} & \checkmark & \checkmark & $\times$ & \checkmark  \\
\cmidrule{2-6}
 & UserSimulator~\cite{yoon2024evaluating} & \checkmark & $\times$ & $\times$ & \checkmark  \\
 \cmidrule{2-6}
 & RecAgent~\cite{wang2023user} & \checkmark & \checkmark & $\times$ & \checkmark  \\
\bottomrule[1.5pt]
\end{tabular}}
\centering
\caption{Comparison of Different LLM-powered Agent Recommendation Methods.}
\label{tab:comparison}
\end{table*}


\textbf{(3) Simulation-oriented} methods aim to authentically replicate user behaviors and preferences through sophisticated simulation techniques. These approaches leverage LLMs to generate realistic user responses to recommendations. For instance, when simulating user feedback, an LLM might generate: ``As a user who is keen to explore new music, I will click on this new song that combines jazz and electronic elements because it matches my interest in experimental music while maintaining the rhythmic style that I like.''
These methods focus on using agents to simulate user behaviors and item characteristics in RSs.

Agent4Rec~\cite{zhang2024generative} utilizes LLM-empowered generative agents as user simulators to model authentic interactions between users and recommender systems, aiming to replicate and evaluate realistic user behaviors in recommendation environments.
AgentCF~\cite{zhang2024agentcf} models both users and items as LLM-powered agents that autonomously interact and collaboratively learn from each other to simulate authentic user-item interactions in recommender systems.
UserSimulator proposes~\cite{yoon2024evaluating} an evaluation protocol to assess LLMs as generative user simulators in conversational recommendation through five tasks to measure how closely these simulators can emulate authentic user behaviors.


\subsection{Agent Components}

The LLM-based agent recommendation architecture consists of four main modules: Profile Module, Memory Module, Planning Module, and Action Module.
Figure~\ref{component} illustrates the core components of the architecture and corresponding functions.


\textbf{(1) Profile Module} is a fundamental component that constructs and maintains dynamic representations of users and items in recommender systems. 
Through continuous analysis of historical interactions, it captures temporal and contextual patterns in user behavior. 
For example, when the system observes that a user often browses technology news on weekday mornings and likes to watch travel content on weekends, the Profile Module will build a user profile of ``focusing on technology news on weekdays and preferring leisure content on weekends''. 
This adaptive profiling approach integrates behavioral patterns, user preferences, and external knowledge to enable highly personalized recommendations.

The profile module in Agent4Rec~\cite{zhang2024generative} incorporates dual components: quantifiable social traits (activity, conformity, and diversity) and personalized preferences extracted via LLM, enabling a comprehensive simulation of user characteristics.
MACRec~\cite{wang2024macrec} incorporates a user and item analyst, which play a crucial role in understanding user preferences and item characteristics.
AgentCF~\cite{zhang2024agentcf} constructs natural language-based user profiles to capture dynamic user preferences and item profiles to represent item characteristics and potential adopters' preferences, enabling personalized agent-based collaborative filtering.


\textbf{(2) Memory Module} serves as a contextual brain that manages and leverages historical interactions and experiences to enhance recommendation quality. 
It maintains a structured repository of past interactions, emotional responses, and conversational context to enable more informed decisions. 
For example, in a restaurant recommendation scenario, when a user comments ``that Sichuan restaurant was too spicy last time'', the Memory Module retrieves the specific restaurant reference from historical interactions and incorporates this preference signal into future recommendations, helping avoid overly spicy options. 
Through this continuous accumulation and utilization of experiential knowledge, the module enables more personalized and context-aware recommendations that reflect users' past experiences and preferences.

RecAgent~\cite{wang2023user} comprises three hierarchical levels: sensory memory, short-term memory, and long-term memory. The sensory memory processes environmental inputs, while short-term memory serves as an intermediate layer that can be transformed into long-term memory through repetitive reinforcement. Long-term memory stores crucial reusable information and facilitates self-reflection and knowledge generalization.
Agent4Rec~\cite{zhang2024generative} consists of factual memory (recording interactive behaviors) and emotional memory (capturing psychological states), stored in both natural language and vector representations, and managed through three mechanisms: retrieval, writing, and reflection.

\textbf{(3) Planning Module} outputs intelligent recommendation strategies by designing multi-step action plans that balance immediate user satisfaction with long-term engagement goals. 
It dynamically formulates recommendation trajectories through careful strategy generation and task sequencing. 
For example, in video recommendation, the system might construct a strategic plan: ``first recommend a popular video to establish user interest, and then gradually introduce niche but high-quality related content, while maintaining the diversity of genres, and ultimately achieve the goal of both satisfying user interest and expanding horizons''.
Through this planning approach, the module optimizes resource allocation and adapts recommendation sequences to achieve both user engagement and item discovery.

BiLLP~\cite{shi2024large} planning mechanism employs a hierarchical structure with two levels: macro-learning (Planner and Reflector LLMs) generates high-level strategic plans and guidelines from experience, while micro-learning (Actor-Critic) translates these plans into specific recommendations.
MACRS~\cite{fang2024multi} uses a multi-agent planning system where a Planner Agent coordinates three Responder Agents (Ask, Recommend, Chat) through multi-step reasoning. 
The system adjusts its dialogue strategy through a feedback mechanism, enabling reflective planning based on user interactions.




\textbf{(4) Action Module} serves as the execution engine that transforms decisions into concrete recommendations through systematic interaction with various system components.  
For example, in an e-commerce scenario, when receiving the directive ``recommend entry-level camera for new user'' from the Planning Module, the Action Module executes a coordinated sequence: analyzing purchase patterns of similar users, querying the product database with specific price and feature constraints, generating targeted recommendations, and capturing user feedback. 
This execution enables the system to deliver contextually appropriate recommendations while continuously learning from interaction outcomes.

RecAgent~\cite{wang2023user} orchestrates naturalistic agent interactions within recommender systems and social environments through a unified prompting framework, incorporating six action modalities (encompassing search, browse, click, pagination, chat, and broadcast functionalities).
InteRecAgent~\cite{huang2023recommender} action module integrates three core tools (information querying, item retrieval, and item ranking) while leveraging a Candidate Bus for sequential tool communication, enabling an end-to-end interactive process from user queries to final recommendations.


% \begin{table*}[t]
% \resizebox{0.96\linewidth}{!}{
% \begin{tabular}{c|c|c|c|c|c|c|c}
% \toprule[1.5pt]
% \textbf{Method} & \textbf{Profile} & \textbf{Memory} & \textbf{Planning} & \textbf{Action} & \textbf{Recommender} & \textbf{Interaction} & \textbf{Simulation} \\
% \midrule
% MACRS~\cite{fang2024multi} & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & $\times$ & $\times$   \\
% \midrule
% ToolRec~\cite{toolrec} & $\times$  & \checkmark & $\times$ & \checkmark & \checkmark & $\times$ & $\times$ \\
% \midrule
% PMS~\cite{hmacrs} & \checkmark  & $\times$ & $\times$ & \checkmark  &  \checkmark & $\times$ & $\times$ \\
% \midrule
% DRDT~\cite{wang2023drdt} & $\times$ & $\times$ & \checkmark & $\times$ & \checkmark & $\times$ & $\times$  \\
% \midrule
% BiLLP~\cite{shi2024large} & $\times$ & \checkmark & \checkmark & \checkmark & \checkmark & $\times$ & $\times$  \\
% \midrule
% RecMind~\cite{wang2024recmind} & $\times$ & \checkmark & \checkmark & \checkmark & \checkmark & $\times$ & $\times$ \\
% \midrule
% MACRec~\cite{wang2024macrec} & $\times$ & $\times$ & \checkmark & \checkmark & \checkmark & $\times$ & $\times$ \\
% \midrule
% AutoConcierge~\cite{zeng2024automated} & $\times$ & \checkmark & \checkmark & \checkmark & $\times$ & \checkmark & $\times$ \\
% \midrule
% RecLLM~\cite{friedman2023leveraging} & \checkmark & $\times$ & $\times$ & \checkmark & $\times$ & \checkmark & $\times$  \\
% \midrule
% RAH~\cite{shu2024rah}  & $\times$ & \checkmark & \checkmark & \checkmark & $\times$ & \checkmark & $\times$  \\
% \midrule
% InteRecAgent~\cite{huang2023recommender} & \checkmark & \checkmark & \checkmark & \checkmark & $\times$ & \checkmark & $\times$  \\
% \midrule
% MAS~\cite{thakkar2024personalized} & \checkmark & \checkmark & \checkmark & \checkmark & $\times$ & \checkmark & $\times$ \\
% \midrule
% KGLA~\cite{guo2024knowledge} & \checkmark & \checkmark & $\times$ & \checkmark & $\times$ & $\times$ & \checkmark \\
% \midrule
% CSHI~\cite{cshi} & \checkmark & \checkmark & $\times$ & \checkmark & $\times$ & $\times$ & \checkmark \\
% \midrule
% Suber~\cite{suber} & \checkmark  & \checkmark & $\times$ & $\times$ & $\times$ & $\times$ & \checkmark \\
% \midrule
% LUSIM~\cite{lusim} & \checkmark  & \checkmark & $\times$ & $\times$ & $\times$ & $\times$ & \checkmark \\
% \midrule
% FLOW~\cite{cai2024flow} & \checkmark & \checkmark & \checkmark & \checkmark & $\times$ & $\times$ & \checkmark \\
% \midrule
% Agent4Rec~\cite{zhang2024generative} & \checkmark & \checkmark & $\times$ & \checkmark & $\times$ & $\times$ & \checkmark \\
% \midrule
% AgentCF~\cite{zhang2024agentcf} & \checkmark & \checkmark & $\times$ & \checkmark & $\times$ & $\times$ & \checkmark  \\
% \midrule
% RecAgent~\cite{yoon2024evaluating} & \checkmark & $\times$ & $\times$ & \checkmark & $\times$ & $\times$ & \checkmark  \\
% \midrule
% H-MACRS~\cite{hmacrs} & \checkmark  & \checkmark & $\times$ & $\times$ & $\times$ &  \checkmark & $\times$ \\
% \midrule
% Rec4Agentverse~\cite{Rec4Agentverse} & \checkmark  & $\times$ & \checkmark & $\times$  & $\times$ & \checkmark &  $\times$ \\
% \bottomrule[1.5pt]
% \end{tabular}}
% \centering
% \caption{Comparison of Different LLM-powered Agent Recommender Methods.}
% \end{table*}