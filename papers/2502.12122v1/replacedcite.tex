\section{Related Work}
\paragraph{PEFT:} As large language models continue to grow, parameter-efficient fine-tuning (PEFT) has become a popular approach to reducing computational and storage costs. Among various methods____, LoRA____ has emerged as one of the most widely used. 
Building on its success, several approaches have been proposed to enhance different aspects of PEFT____. One such method, LoRA-XS____, further optimizes parameter efficiency by enabling flexible control over the number of trainable parameters per adaptation module. 
\our{} reuses the idea of SVD-based projections to reduce the parameter space dimensionality. 
%, allowing fine-tuning to be tailored to different computational constraints.

\paragraph{Bayesian LoRAs:} Standard LoRA____ does not account for uncertainty, making fine-tuned models susceptible to miscalibration. Then, Bayesian LoRA approaches integrate Bayesian inference techniques into LoRA to improve uncertainty estimation and generalization.

Several Bayesian LoRA methods have been proposed, each employing different Bayesian techniques to address these challenges.  SWAG-LoRA ____ combines Stochastic Weight Averaging-Gaussian (SWAG) with LoRA to enable approximate Bayesian inference, significantly improving model calibration and reducing overconfidence. Laplace-LoRA ____ applies a Laplace approximation to the posterior over LoRA parameters. Bella ____ introduces an approach that reduces the cost of Bayesian deep ensembles by applying multiple low-rank perturbations to a pre-trained model.
%
BLoB (Bayesian Low-Rank Adaptation by Backpropagation) ____ jointly learns both the mean and covariance of model parameters throughout the fine-tuning process using Variational Inference. B-LoRA ____ introduces a Bayesian perspective to both quantization and rank selection by using a prior distribution over these hyperparameters, optimizing model efficiency and reducing bit operations.

 The key challenge lies in balancing uncertainty modeling with parameter efficiency, as Bayesian inference typically increases both the number of trainable parameters and computational cost.
Despite their advantages, Bayesian LoRA methods face challenges related to increased parameter count and computational cost. One major issue is the higher storage and memory requirements, as Bayesian methods often require additional parameters to model uncertainty, particularly those involving covariance estimation, such as SWAG-LoRA. 
Scalability remains a concern for methods that explicitly model uncertainty across a large number of parameters.


% %\vspace{-0.05cm}