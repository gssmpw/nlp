% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@inproceedings{NEURIPS2019_118921ef,
 author = {Maddox, Wesley J and Izmailov, Pavel and Garipov, Timur and Vetrov, Dmitry P and Wilson, Andrew Gordon},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {A Simple Baseline for Bayesian Uncertainty in Deep Learning},
 volume = {32},
 year = {2019}
}


@inproceedings{
    onal2024gaussian,
    title={Gaussian Stochastic Weight Averaging for Bayesian Low-rank Adaptation of Large Language Models},
    author={Emre Onal and Klemens Fl{\"o}ge and Emma Caldwell and Arsen Sheverdin and Vincent Fortuin},
    booktitle={Sixth Symposium on Advances in Approximate Bayesian Inference - Non Archival Track},
    year={2024},
}

@inproceedings{robeyns2024laplaceLora,
    author={Maxime Robeyns},
    title={Bayesian Low-Rank Adaptation for Large Language Models},
  booktitle={ICLR},
  year={2024}
}

@inproceedings{wang2024blob,
    title={{BL}oB: Bayesian Low-Rank Adaptation by Backpropagation for Large Language Models},
    author={Yibin Wang and Haizhou Shi and Ligong Han and Dimitris N. Metaxas and Hao Wang},
    booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
    year={2024},
}


@inproceedings{meo2024bayesianlora,
    title={Bayesian-Lo{RA}: Lo{RA} based Parameter Efficient Fine-Tuning using Optimal Quantization levels and Rank Values trough Differentiable Bayesian Gates},
    author={Cristian Meo and Ksenia Sycheva and Anirudh Goyal and Justin Dauwels},
    booktitle={2nd Workshop on Advancing Neural Network Training: Computational Efficiency, Scalability, and Resource Optimization (WANT@ICML 2024)},
    year={2024},
}

@misc{doan2025bayesianlowranklearningbella,
      title={Bayesian Low-Rank LeArning (Bella): A Practical Approach to Bayesian Neural Networks}, 
      author={Bao Gia Doan and Afshar Shamsi and Xiao-Yu Guo and Arash Mohammadi and Hamid Alinejad-Rokny and Dino Sejdinovic and Damien Teney and Damith C. Ranasinghe and Ehsan Abbasnejad},
      year={2025},
      eprint={2407.20891},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.20891}, 
}

@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@article{balazy2024lora,
  title={LoRA-XS: Low-Rank Adaptation with Extremely Small Number of Parameters},
  author={Ba{\l}azy, Klaudia and Banaei, Mohammadreza and Aberer, Karl and Tabor, Jacek},
  journal={arXiv preprint arXiv:2405.17604},
  year={2024}
}

@inproceedings{houlsby2019parameter,
  title={Parameter-efficient transfer learning for NLP},
  author={Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  booktitle={International conference on machine learning},
  pages={2790--2799},
  year={2019},
  organization={PMLR}
}

@article{lester2021power,
  title={The power of scale for parameter-efficient prompt tuning},
  author={Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  journal={arXiv preprint arXiv:2104.08691},
  year={2021}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{li2021prefix,
  title={Prefix-tuning: Optimizing continuous prompts for generation},
  author={Li, Xiang Lisa and Liang, Percy},
  journal={arXiv preprint arXiv:2101.00190},
  year={2021}
}

@inproceedings{guo-etal-2021-parameter,
    title = "Parameter-Efficient Transfer Learning with Diff Pruning",
    author = "Guo, Demi  and
      Rush, Alexander  and
      Kim, Yoon",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.378/",
    doi = "10.18653/v1/2021.acl-long.378",
    pages = "4884--4896",
}

@article{kopiczko2023vera,
  title={Vera: Vector-based random matrix adaptation},
  author={Kopiczko, Dawid Jan and Blankevoort, Tijmen and Asano, Yuki Markus},
  journal={arXiv preprint arXiv:2310.11454},
  year={2023}
}

@inproceedings{adalora,
  title={Adaptive budget allocation for parameter-efficient fine-tuning},
  author={Zhang, Qingru and Chen, Minshuo and Bukharin, Alexander and He, Pengcheng and Cheng, Yu and Chen, Weizhu and Zhao, Tuo},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2023}
}

@article{dettmers2024qlora,
  title={Qlora: Efficient finetuning of quantized llms},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}

@misc{wang2019,
      title={GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding}, 
      author={Alex Wang and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel R. Bowman},
      year={2019},
      eprint={1804.07461},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1804.07461}, 
}

@misc{wolf2020,
      title={HuggingFace's Transformers: State-of-the-art Natural Language Processing}, 
      author={Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush},
      year={2020},
      eprint={1910.03771},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1910.03771}, 
}

@misc{liu2019,
      title={RoBERTa: A Robustly Optimized BERT Pretraining Approach}, 
      author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
      year={2019},
      eprint={1907.11692},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1907.11692}, 
}

@inproceedings{NIPS2017_3f5ee243,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@misc{loshchilov2019,
      title={Decoupled Weight Decay Regularization}, 
      author={Ilya Loshchilov and Frank Hutter},
      year={2019},
      eprint={1711.05101},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1711.05101}, 
}





@article{opendelta,
	journal = {Nature Machine Intelligence},
    author = {Ding, Ning and Qin, Yujia and Yang, Guang and Wei, Fuchao and Yang, Zonghan and Su, Yusheng and Hu, Shengding and Chen, Yulin and Chan, Chi-Min and Chen, Weize and Yi, Jing and Zhao, Weilin and Wang, Xiaozhi and Liu, Zhiyuan and Zheng, Hai-Tao and Chen, Jianfei and Liu, Yang and Tang, Jie and Li, Juanzi and Sun, Maosong},
	number = {3},
	pages = {220--235},
	title = {Parameter-efficient fine-tuning of large-scale pre-trained language models},
	volume = {5},
	year = {2023},
 }

@inproceedings{he2023preserving,
  title={Preserving Pre-trained Features Helps Calibrate Fine-tuned Language Models},
  author={He, Guande and Chen, Jianfei and Zhu, Jun},
  booktitle={ICLR},
  year={2023}
}

@inproceedings{xiao2022uncertainty,
  title={Uncertainty Quantification with Pre-trained Language Models: A Large-Scale Empirical Analysis},
  author={Xiao, Yuxin and Liang, Paul Pu and Bhatt, Umang and Neiswanger, Willie and Salakhutdinov, Ruslan and Morency, Louis-Philippe},
  booktitle={EMNLP},
  year={2022}
}

@inproceedings{zhang2021bayesian,
  title={Bayesian attention belief networks},
  author={Zhang, Shujian and Fan, Xinjie and Chen, Bo and Zhou, Mingyuan},
  booktitle={ICML},
  year={2021},
}

@misc{openai2023gpt4,
      title={{GPT}-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{ritter2018scalable,
  title={A scalable laplace approximation for neural networks},
  author={Ritter, Hippolyt and Botev, Aleksandar and Barber, David},
  booktitle={ICLR},
  year={2018},
}

@article{mackay1992practical,
  title={A practical Bayesian framework for backpropagation networks},
  author={MacKay, David JC},
  journal={Neural computation},
  year={1992},
}

@inproceedings{foong2019between,
  title={'In-Between'Uncertainty in Bayesian Neural Networks},
  author={Foong, Andrew YK and Li, Yingzhen and Hern{\'a}ndez-Lobato, Jos{\'e} Miguel and Turner, Richard E},
  booktitle={ICML Workshop on Uncertainty and Robustness in Deep Learning},
  year={2019}
}


@article{daxberger2021laplace,
  title={Laplace redux-effortless bayesian deep learning},
  author={Daxberger, Erik and Kristiadi, Agustinus and Immer, Alexander and Eschenhagen, Runa and Bauer, Matthias and Hennig, Philipp},
  journal={NeurIPS},
  year={2021}
}


@article{deng2022accelerated,
  title={Accelerated Linearized Laplace Approximation for Bayesian Deep Learning},
  author={Deng, Zhijie and Zhou, Feng and Zhu, Jun},
  journal={NeurIPS},
  year={2022}
}


@inproceedings{antoran2022adapting,
  title={Adapting the linearised laplace model evidence for modern deep learning},
  author={Antor{\'a}n, Javier and Janz, David and Allingham, James U and Daxberger, Erik and Barbano, Riccardo Rb and Nalisnick, Eric and Hern{\'a}ndez-Lobato, Jos{\'e} Miguel},
  booktitle={ICML},
  year={2022},
}


@article{desai2020calibration,
  title={Calibration of pre-trained transformers},
  author={Desai, Shrey and Durrett, Greg},
  journal={arXiv preprint arXiv:2003.07892},
  year={2020}
}


@inproceedings{wang2018glue,
  title={GLUE: A multi-task benchmark and analysis platform for natural language understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  booktitle={ICLR},
  year={2019}
}


@inproceedings{wang2019superglue,
  title={Superglue: A stickier benchmark for general-purpose language understanding systems},
  author={Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel},
  booktitle={NeurIPS},
  year={2019}
}


@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}


@inproceedings{daxberger2021bayesian,
  title={Bayesian deep learning via subnetwork inference},
  author={Daxberger, Erik and Nalisnick, Eric and Allingham, James U and Antor{\'a}n, Javier and Hern{\'a}ndez-Lobato, Jos{\'e} Miguel},
  booktitle={ICML},
  year={2021},
}


@inproceedings{kristiadi2020being,
  title={Being {B}ayesian, even just a bit, fixes overconfidence in relu networks},
  author={Kristiadi, Agustinus and Hein, Matthias and Hennig, Philipp},
  booktitle={ICML},
  year={2020},
}


@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{liu2022few,
  title={Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning},
  author={Liu, Haokun and Tam, Derek and Muqeeth, Mohammed and Mohta, Jay and Huang, Tenghao and Bansal, Mohit and Raffel, Colin A},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={1950--1965},
  year={2022}
}

@article{ding2022delta,
  title={Delta tuning: A comprehensive study of parameter efficient methods for pre-trained language models},
  author={Ding, Ning and Qin, Yujia and Yang, Guang and Wei, Fuchao and Yang, Zonghan and Su, Yusheng and Hu, Shengding and Chen, Yulin and Chan, Chi-Min and Chen, Weize and others},
  journal={arXiv preprint arXiv:2203.06904},
  year={2022}
}

@article{chung2022scaling,
  title={Scaling instruction-finetuned language models},
  author={Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
  journal={arXiv preprint arXiv:2210.11416},
  year={2022}
}
@article{wang2022self,
  title={Self-instruct: Aligning language model with self generated instructions},
  author={Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2212.10560},
  year={2022}
}
@article{wei2021finetuned,
  title={Finetuned language models are zero-shot learners},
  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent Y and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  journal={arXiv preprint arXiv:2109.01652},
  year={2021}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{jiang2021can,
  title={How can we know when language models know? on the calibration of language models for question answering},
  author={Jiang, Zhengbao and Araki, Jun and Ding, Haibo and Neubig, Graham},
  journal={Transactions of the Association for Computational Linguistics},
  volume={9},
  pages={962--977},
  year={2021},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{tian2023just,
  title={Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback},
  author={Tian, Katherine and Mitchell, Eric and Zhou, Allan and Sharma, Archit and Rafailov, Rafael and Yao, Huaxiu and Finn, Chelsea and Manning, Christopher D},
  journal={arXiv preprint arXiv:2305.14975},
  year={2023}
}

@inproceedings{ober2021global,
  title={Global inducing point variational posteriors for bayesian neural networks and deep gaussian processes},
  author={Ober, Sebastian W and Aitchison, Laurence},
  booktitle={International Conference on Machine Learning},
  pages={8248--8259},
  year={2021},
  organization={PMLR}
}

@article{fortuin2021bayesian,
  title={Bayesian neural network priors revisited},
  author={Fortuin, Vincent and Garriga-Alonso, Adri{\`a} and Ober, Sebastian W and Wenzel, Florian and R{\"a}tsch, Gunnar and Turner, Richard E and van der Wilk, Mark and Aitchison, Laurence},
  journal={arXiv preprint arXiv:2102.06571},
  year={2021}
}

@inproceedings{blundell2015weight,
  title={Weight uncertainty in neural network},
  author={Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
  booktitle={International conference on machine learning},
  pages={1613--1622},
  year={2015},
  organization={PMLR}
}

@inproceedings{izmailov2021bayesian,
  title={What are Bayesian neural network posteriors really like?},
  author={Izmailov, Pavel and Vikram, Sharad and Hoffman, Matthew D and Wilson, Andrew Gordon Gordon},
  booktitle={International conference on machine learning},
  pages={4629--4640},
  year={2021},
  organization={PMLR}
}

@article{zhang2019cyclical,
  title={Cyclical stochastic gradient MCMC for Bayesian deep learning},
  author={Zhang, Ruqi and Li, Chunyuan and Zhang, Jianyi and Chen, Changyou and Wilson, Andrew Gordon},
  journal={arXiv preprint arXiv:1902.03932},
  year={2019}
}

@article{cinquin2021pathologies,
  title={Pathologies in priors and inference for Bayesian transformers},
  author={Cinquin, Tristan and Immer, Alexander and Horn, Max and Fortuin, Vincent},
  journal={arXiv preprint arXiv:2110.04020},
  year={2021}
}

@article{chen2023calibrating,
  title={Calibrating Transformers via Sparse Gaussian Processes},
  author={Chen, Wenlong and Li, Yingzhen},
  journal={arXiv preprint arXiv:2303.02444},
  year={2023}
}

@article{tran2019bayesian,
  title={Bayesian layers: A module for neural network uncertainty},
  author={Tran, Dustin and Dusenberry, Mike and Van Der Wilk, Mark and Hafner, Danijar},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{xue2021bayesian,
  title={Bayesian transformer language models for speech recognition},
  author={Xue, Boyang and Yu, Jianwei and Xu, Junhao and Liu, Shansong and Hu, Shoukang and Ye, Zi and Geng, Mengzhe and Liu, Xunying and Meng, Helen},
  booktitle={ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={7378--7382},
  year={2021},
  organization={IEEE}
}

@article{coker2021wide,
  title={Wide mean-field variational bayesian neural networks ignore the data},
  author={Coker, Beau and Pan, Weiwei and Doshi-Velez, Finale},
  journal={arXiv preprint arXiv:2106.07052},
  year={2021}
}
@article{fan2020bayesian,
  title={Bayesian attention modules},
  author={Fan, Xinjie and Zhang, Shujian and Chen, Bo and Zhou, Mingyuan},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={16362--16376},
  year={2020}
}

@article{dettmers2023qlora,
  title={Qlora: Efficient finetuning of quantized llms},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2305.14314},
  year={2023}
}

@article{singhalLargeLanguageModels2022,
  title = {Large {{Language Models Encode Clinical Knowledge}}},
  author = {Singhal, Karan and Azizi, Shekoofeh and Tu, Tao and Mahdavi, S. Sara and Wei, Jason and Chung, Hyung Won and Scales, Nathan and Tanwani, Ajay and {Cole-Lewis}, Heather and Pfohl, Stephen and Payne, Perry and Seneviratne, Martin and Gamble, Paul and Kelly, Chris and Scharli, Nathaneal and Chowdhery, Aakanksha and Mansfield, Philip and y Arcas, Blaise Aguera and Webster, Dale and Corrado, Greg S. and Matias, Yossi and Chou, Katherine and Gottweis, Juraj and Tomasev, Nenad and Liu, Yun and Rajkomar, Alvin and Barral, Joelle and Semturs, Christopher and Karthikesalingam, Alan and Natarajan, Vivek},
  year = {2022},
  journal = {{arXiv}},
}

@misc{wuBloombergGPTLargeLanguage2023,
  title = {{{BloombergGPT}}: {{A Large Language Model}} for {{Finance}}},
  shorttitle = {{{BloombergGPT}}},
  author = {Wu, Shijie and Irsoy, Ozan and Lu, Steven and Dabravolski, Vadim and Dredze, Mark and Gehrmann, Sebastian and Kambadur, Prabhanjan and Rosenberg, David and Mann, Gideon},
  year = {2023},
  month = may,
  publisher = {{arXiv}},
}

@misc{lampinenPassiveLearningActive2023,
  title = {Passive Learning of Active Causal Strategies in Agents and Language Models},
  author = {Lampinen, Andrew Kyle and Chan, Stephanie C. Y. and Dasgupta, Ishita and Nam, Andrew J. and Wang, Jane X.},
  year = {2023},
  month = may,
  publisher = {{arXiv}},
  urldate = {2023-06-07},
}


@Misc{peft,
  title =        {PEFT: State-of-the-art Parameter-Efficient Fine-Tuning methods},
  author =       {Sourab Mangrulkar and Sylvain Gugger and Lysandre Debut and Younes Belkada and Sayak Paul},
  howpublished = {\url{https://github.com/huggingface/peft}},
  year =         {2022}
}


@inproceedings{immer2021improving,
  title={Improving predictions of Bayesian neural nets via local linearization},
  author={Immer, Alexander and Korzepa, Maciej and Bauer, Matthias},
  booktitle={AISTAT},
  year={2021},
}

@inproceedings{wolf2020transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush",
    booktitle = "EMNLP",
    year = "2020"
}


@article{kunstner2019limitations,
  title={Limitations of the empirical fisher approximation for natural gradient descent},
  author={Kunstner, Frederik and Hennig, Philipp and Balles, Lukas},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}


@article{touvron2023llama2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}


@article{zhao2020role,
  title={On the role of dataset quality and heterogeneity in model confidence},
  author={Zhao, Yuan and Chen, Jiasi and Oymak, Samet},
  journal={arXiv preprint arXiv:2002.09831},
  year={2020}
}


@article{osawa2023asdl,
  title={ASDL: A Unified Interface for Gradient Preconditioning in PyTorch},
  author={Osawa, Kazuki and Ishikawa, Satoki and Yokota, Rio and Li, Shigang and Hoefler, Torsten},
  journal={arXiv preprint arXiv:2305.04684},
  year={2023}
}


@article{gleave2022uncertainty,
  title={Uncertainty estimation for language reward models},
  author={Gleave, Adam and Irving, Geoffrey},
  journal={arXiv preprint arXiv:2203.07472},
  year={2022}
}


@inproceedings{clark2019boolq,
  title =     {BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions},
  author =    {Clark, Christopher and Lee, Kenton and Chang, Ming-Wei and Kwiatkowski, Tom and Collins, Michael and Toutanova, Kristina},
  booktitle = {NAACL},
  year =      {2019}
}


@article{allenai:arc,
      author    = {Peter Clark  and Isaac Cowhey and Oren Etzioni and Tushar Khot and Ashish Sabharwal and Carissa Schoenick and Oyvind Tafjord},
      title     = {Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge},
      journal   = {arXiv:1803.05457v1},
      year      = {2018}
}


@article{sakaguchi2021winogrande,
  title={Winogrande: An adversarial winograd schema challenge at scale},
  author={Sakaguchi, Keisuke and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
  journal={Communications of the ACM},
  year={2021},
}


@inproceedings{OpenBookQA2018,
 title={Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering},
 author={Todor Mihaylov and Peter Clark and Tushar Khot and Ashish Sabharwal},
 booktitle={EMNLP},
 year={2018}
}


@article{Lu2020UncertaintyEW,
  title={Uncertainty Estimation with Infinitesimal Jackknife, Its Distribution and Mean-Field Approximation},
  author={Zhiyun Lu and Eugene Ie and Fei Sha},
  journal={ArXiv},
  year={2020},
  volume={abs/2006.07584},
}



@article{shridhar2019comprehensive,
  title={A comprehensive guide to bayesian convolutional neural network with variational inference},
  author={Shridhar, Kumar and Laumann, Felix and Liwicki, Marcus},
  journal={arXiv preprint arXiv:1901.02731},
  year={2019}
}


@inproceedings{dusenberry2020efficient,
  title={Efficient and scalable bayesian neural nets with rank-1 factors},
  author={Dusenberry, Michael and Jerfel, Ghassen and Wen, Yeming and Ma, Yian and Snoek, Jasper and Heller, Katherine and Lakshminarayanan, Balaji and Tran, Dustin},
  booktitle={International conference on machine learning},
  pages={2782--2792},
  year={2020},
  organization={PMLR}
}


@article{kadavath2022language,
  title={Language models (mostly) know what they know},
  author={Kadavath, Saurav and Conerly, Tom and Askell, Amanda and Henighan, Tom and Drain, Dawn and Perez, Ethan and Schiefer, Nicholas and Hatfield-Dodds, Zac and DasSarma, Nova and Tran-Johnson, Eli and others},
  journal={arXiv preprint arXiv:2207.05221},
  year={2022}
}



@article{li2022pre,
  title={Pre-trained language models for interactive decision-making},
  author={Li, Shuang and Puig, Xavier and Paxton, Chris and Du, Yilun and Wang, Clinton and Fan, Linxi and Chen, Tao and Huang, De-An and Aky{\"u}rek, Ekin and Anandkumar, Anima and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={31199--31212},
  year={2022}
}



@article{lee2019mixout,
  title={Mixout: Effective regularization to finetune large-scale pretrained language models},
  author={Lee, Cheolhyoung and Cho, Kyunghyun and Kang, Wanmo},
  journal={arXiv preprint arXiv:1909.11299},
  year={2019}
}


@article{park2022calibration,
  title={On the calibration of pre-trained language models using mixup guided by area under the margin and saliency},
  author={Park, Seo Yeon and Caragea, Cornelia},
  journal={arXiv preprint arXiv:2203.07559},
  year={2022}
}


@article{zhang2017mixup,
  title={mixup: Beyond empirical risk minimization},
  author={Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N and Lopez-Paz, David},
  journal={arXiv preprint arXiv:1710.09412},
  year={2017}
}


@article{ovadia2019can,
  title={Can you trust your model's uncertainty? evaluating predictive uncertainty under dataset shift},
  author={Ovadia, Yaniv and Fertig, Emily and Ren, Jie and Nado, Zachary and Sculley, David and Nowozin, Sebastian and Dillon, Joshua and Lakshminarayanan, Balaji and Snoek, Jasper},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}


@inproceedings{gal2016dropout,
  title={Dropout as a bayesian approximation: Representing model uncertainty in deep learning},
  author={Gal, Yarin and Ghahramani, Zoubin},
  booktitle={international conference on machine learning},
  pages={1050--1059},
  year={2016},
  organization={PMLR}
}


@inproceedings{guo2017calibration,
  title={On calibration of modern neural networks},
  author={Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q},
  booktitle={International conference on machine learning},
  pages={1321--1330},
  year={2017},
  organization={PMLR}
}


@article{chen2017checkpoint,
  title={Checkpoint ensembles: Ensemble methods from a single training process},
  author={Chen, Hugh and Lundberg, Scott and Lee, Su-In},
  journal={arXiv preprint arXiv:1710.03282},
  year={2017}
}


@article{hendrycks2020measuring,
  title={Measuring massive multitask language understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2009.03300},
  year={2020}
}



@article{mackay1998choice,
	author = {MacKay, David J. C. },
	journal = {Machine Learning},
	number = {1},
	pages = {77--86},
	title = {Choice of Basis for Laplace Approximation},
	volume = {33},
	year = {1998},}


@article{ashukha2020pitfalls,
  title={Pitfalls of in-domain uncertainty estimation and ensembling in deep learning},
  author={Ashukha, Arsenii and Lyzhov, Alexander and Molchanov, Dmitry and Vetrov, Dmitry},
  journal={arXiv preprint arXiv:2002.06470},
  year={2020}
}


@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}


@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}


@article{lakshminarayanan2017simple,
  title={Simple and scalable predictive uncertainty estimation using deep ensembles},
  author={Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}


@article{wang2023lora,
  title={LoRA ensembles for large language model fine-tuning},
  author={Wang, Xi and Aitchison, Laurence and Rudolph, Maja},
  journal={arXiv preprint arXiv:2310.00035},
  year={2023}
}


@article{shi2023dept,
  title={DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning},
  author={Shi, Zhengxiang and Lipani, Aldo},
  journal={arXiv preprint arXiv:2309.05173},
  year={2023}
}


@inproceedings{aitchison2021deep,
  title={Deep kernel processes},
  author={Aitchison, Laurence and Yang, Adam and Ober, Sebastian W},
  booktitle={International Conference on Machine Learning},
  pages={130--140},
  year={2021},
  organization={PMLR}
}



@article{zhai2023uncertainty,
  title={Uncertainty-Penalized Reinforcement Learning from Human Feedback with Diverse Reward LoRA Ensembles},
  author={Zhai, Yuanzhao and Zhang, Han and Lei, Yu and Yu, Yue and Xu, Kele and Feng, Dawei and Ding, Bo and Wang, Huaimin},
  journal={arXiv preprint arXiv:2401.00243},
  year={2023}
}