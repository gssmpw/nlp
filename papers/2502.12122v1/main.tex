% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
% \usepackage[review]{acl}
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% Added
\usepackage{amsfonts}
\def\R{\mathbb{R}}
\newcommand{\Singular}{\textit{S}}
\usepackage{booktabs}
\usepackage{multirow}
\newcommand{\our}{B-LoRA-XS}
\usepackage{mathabx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Minimal Ranks, Maximum Confidence: \\ Parameter-efficient Uncertainty Quantification for LoRA}


\author{Patryk Marszałek$^{\ast}$ \quad Klaudia Bałazy \quad  Jacek Tabor \quad  Tomasz Kuśmierczyk$^{\ast}$ \\ \\
  Jagiellonian University  \\
%   e-mail:  \texttt{t.kusmierczyk@uj.edu.pl} 
}



\begin{document}
\maketitle
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[1]{Primary contributors.}

\begin{abstract}
Low-Rank Adaptation (LoRA) enables parameter-efficient fine-tuning of large language models by decomposing weight updates into low-rank matrices, significantly reducing storage and computational overhead. While effective, standard LoRA lacks mechanisms for uncertainty quantification, leading to overconfident and poorly calibrated models. Bayesian variants of LoRA address this limitation, but at the cost of a significantly increased number of trainable parameters, partially offsetting the original efficiency gains. Additionally, these models are harder to train and may suffer from unstable convergence.  
In this work, we propose a novel parameter-efficient Bayesian LoRA, demonstrating that effective uncertainty quantification can be achieved in very low-dimensional parameter spaces. The proposed method achieves strong performance with improved calibration and generalization while maintaining computational efficiency. Our empirical findings show that, with the appropriate projection of the weight space: (1) uncertainty can be effectively modeled in a low-dimensional space, and (2) weight covariances exhibit low ranks.
\end{abstract}

\section{Introduction}

LoRA (Low-Rank Adaptation)~\cite{hu2021lora} reduces computational overhead by decomposing the update weights of pre-trained models into low-rank matrices, enabling efficient adaptation to downstream tasks.  
Minimizing the number of trainable parameters reduces memory and storage requirements, making large-scale model adaptation feasible. Reducing computational overhead speeds up training time and makes adaptation possible in resource-constrained settings.  

Unlike pre-trained models, which are relatively well-calibrated \citep{openai2023gpt4}, fine-tuned large models (e.g., LLMs) often become overconfident and poorly calibrated~\citep{jiang2021can,tian2023just,xiao2022uncertainty,he2023preserving}, especially when trained on limited data. This hinders their usability for applications where uncertainty-aware decisions are performed. 

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{fig/avg_lorar.pdf}
    \caption{
Performance averaged over multiple GLUE datasets (individual results in Fig.~\ref{fig:lorar}). Our method achieves superior calibration (ECE) and competitive predictive performance (Brier) while maintaining computational efficiency. For example, at $r=8$ ($\blacktriangleup$), we reduce ECE by half with only 1/10th LoRA parameters.  
    }
    \label{fig:teaser}
\end{figure}



\begin{figure*}[t]
    %\vspace{-0.5cm}
    \centering
    \begin{minipage}{0.7\textwidth}
    \includegraphics[width=1.0\linewidth]{fig/\our-vis.pdf}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.29\textwidth}
    \resizebox{1.0\textwidth}{!}{% <------ Don't forget this %
    \begin{tabular}{llrrr}
        \toprule
         & \textbf{Method} & \textbf{$r$} & $k$ & \textbf{\# Parameters} \\
        \cmidrule{1-5}
        \multirow{4}{*}{\rotatebox{90}{Standard}} & LoRA & 2 & - & 0.2M \\
        & LoRA & 8 & - & 0.8M \\
        \cmidrule{2-5}
        & LoRA-XS & 8 & - & 6k \\
        & LoRA-XS & 25 & - & 60k \\
        \midrule
        \multirow{6}{*}{\rotatebox{90}{Bayesian}} & SWAG-LoRA & 2 & 10 & 2.4M \\
        & SWAG-LoRA & 8 & 10 & 9.4M \\
        & SWAG-LoRA & 8 & 5 & 5.5M \\
        \cmidrule{2-5}
        & \emph{\our} & 8 & 10 & 74k \\
        & \emph{\our} & 25 & 10 & 0.7M \\
        & \emph{\our} & 25 & 5 & 0.4M \\
        \bottomrule
    \end{tabular}

        }
    \end{minipage}
    %\vspace{-0.15cm}
    \caption{
    \textbf{(left):}~Weight-adaptation approaches: LoRA vs.\ \our.
    As indicated by the color coding, some parameters remain frozen (\emph{blue}), others are trained (\emph{orange}) or obtained via SVD (\emph{green}).
    \textbf{(right):}~Number of trainable parameters per method. XS variants remain computationally competitive even for ranks as large as $r=25$.    
    }
    \label{fig:method}
    %\vspace{-0.5cm}
\end{figure*}

Bayesian treatment is then frequently proposed to address overconfidence in neural networks~\citep{blundell2015weight,kristiadi2020being,aitchison2021deep,izmailov2021bayesian}. Consequently, recently proposed Bayesian variants of LoRA~\citep{onal2024gaussian,robeyns2024laplaceLora,doan2025bayesianlowranklearningbella} address the aforementioned challenges by introducing uncertainty estimation directly into the fine-tuning process. During training, these models continuously adjust both the mean and covariance of fine-tuned parameters to achieve better generalization and uncertainty quantification.  

Learning the posterior covariance matrix is necessary for modeling epistemic uncertainty. However, its size grows quadratically with the number of parameters, which can easily cancel out the benefits of LoRA, in addition to making learning significantly harder. Using low-rank, Kronecker-factored, or diagonal-only covariances partially alleviates the problem, but as we demonstrate in Sec.~\ref{sec:experiments}, this comes at the cost of results quality loss. Furthermore, even at rank = 2, the number of trainable parameters is quadrupled compared to vanilla LoRA. This creates a need for an alternative approach that retains covariance modeling capacity while reducing the number of required parameters.  

%We address this need by proposing
We propose
a method that learns Bayesian posteriors for weights projected onto a low-dimensional manifold, hence maintaining parameter efficiency. The thoughtfully selected projection allows for the effective representation of the covariances between weights through covariances between representations in the lower-dimensional space. In this design, we follow the work of \citet{balazy2024lora}, who recently proposed a strategy for finding such projections with SVD. We prove that they are effective for learning Bayesian models as well.  

%Our key insight is that 
Operating in such a reduced parameter space significantly improves the feasibility of Bayesian inference. % by lowering computational costs. 
We show that %in this space, 
correlations between weights can be represented very efficiently -- unlike in the original weight space, we can use covariance matrices with ranks as low as $k=2$. Thanks to the low number of parameters, training is also more stable. Finally, the method achieves superior calibration and accuracy at low budgets (e.g., see Fig.~\ref{fig:teaser}). %, comparable to vanilla {LoRA-XS}.

In the Appendix, we supplement the results presented in the paper with a detailed overview of the experimental setup. We make our code publicly available at \href{https://github.com/gmum/b-lora-xs/}{github}.



\begin{figure*}[ht]
    % \vspace{-0.5cm}
    \centering
    \begin{minipage}{1.0\textwidth}
    {\scriptsize
    % \setlength{\baselineskip}{0.0em} % Reduce default line spacing
    \makebox[0.95\textwidth]{CoLA} 
    \includegraphics[width=0.9975\textwidth]{fig/cola_lorar.pdf} 
    %\vspace{-0.0cm}
    
    \makebox[0.95\textwidth]{MRPC}  
    \includegraphics[width=0.935\textwidth]{fig/mrpc_lorar.pdf} 
    %\vspace{-0.0cm}
    
    \makebox[0.95\textwidth]{RTE}  
    \includegraphics[width=0.868\textwidth]{fig/rte_lorar.pdf} 
    %\vspace{-0.05cm}

    \makebox[0.95\textwidth]{SST-2} 
    \includegraphics[width=0.878\textwidth]{fig/sst2_lorar.pdf}  
    %\vspace{-0.15cm}

    }
    \end{minipage}%
    \\
    \caption{
    Median$\pm$std. accuracy (left), ECE (middle), and NLL (right) on 4 GLUE tasks (rows) vs. total parameter count for several methods and varying ranks $r$. \our{} (our) achieves the accuracy and the calibration of SWAG-LoRA while using significantly fewer parameters than LoRA.
    See Fig.~\ref{fig:teaser} for averaged results.
    }
    \label{fig:lorar}
\end{figure*}




\section{Method}
\label{sec:method}


\textbf{LoRA} fine-tunes large pre-trained models by learning low-rank weight updates $\Delta W$ instead of training the weights $W$ directly. For a pre-trained parameter matrix \(W^0 \in \mathbb{R}^{m \times n}\) that is kept fixed, LoRA learns a rank-\(r\) update \(\Delta W = AB\), where \(A \in \mathbb{R}^{m \times r}\) and \(B \in \mathbb{R}^{r \times n}\) have far fewer parameters. The effective weight is then:
$
W = W^0 + \Delta W = W^0 + AB,
$
where only \(A\) and \(B\) are trained. Typically, LoRA is applied jointly at multiple layers, yielding a set of updates $\{\Delta W_{l}\}$.

\textbf{Bayesian treatment} of a neural network involves finding the posterior \(p(\theta \mid \mathcal{D})\) given training data \(\mathcal{D}\). By Bayes' theorem:
$
p(\theta \mid \mathcal{D}) \;=\; \frac{p(\mathcal{D} \mid \theta)\,p(\theta)}{p(\mathcal{D})},
$
where $\theta$ represents the model's parameters (i.e., weights) considered random variables. 
In particular, for the Bayesian LoRA setting, \emph{$\theta$ denotes a set of the learned model updates}, while the remaining frozen weights are hidden inside the model likelihood, given by $p(\mathcal{D} \mid \theta) = \prod_{i \in [\mathcal{D}]} p(y_i | x_i, \theta)$.
The learned posterior allows Bayesian model averaging at inference as:
$
p(y_* \mid x_*, \mathcal{D}) \;=\; \int p(y_* \mid x_*, \theta)\,p(\theta \mid \mathcal{D})\,d\theta \approx 
\frac{1}{S} \sum_{\theta \sim p(\theta|D)} p(y_* \mid x_*, \theta),
$
where we use $S=15$ samples from the posterior.

\textbf{Bayesian LoRAs} obtain the posterior for $\{\Delta W_{l}\}$ through the learned posterior for $\theta = \cup_l \{ A_{l} \cup B_{l} \}$, where $l$ indexes the weight updates (layers). The posterior itself is approximated either using a set of particles or a closed-form distribution. Due to its superior performance, we rely on the latter and assume $p(\theta | \mathcal{D}) \approx \mathcal{N}(\mu, \Sigma)$, where $\mu$ is the vector of means (of size equal to the number of learned parameters) and $\Sigma$ is the covariance matrix, whose size grows quadratically with the total number of parameters. Notably, we aim to \emph{model cross-layer interdependencies}, requiring covariance estimation also across weights in different layers $\{l\}$. However, this results in an impractically large number of parameters.
Consequently, we explore methods to reduce this cost by representing distributions $p(\{\Delta W\} | \mathcal{D})$ differently.




\textbf{In LoRA-XS}~\cite{balazy2024lora}, the adaptation matrices $A$ and $B$ are initialized using the truncated SVD of the corresponding pre-trained weight matrices $W^0$. 
This initialization captures the most informative singular components of the original weights. Under the assumption that the fine-tuned task is similar to the original task, these projections retain the functional properties also for downstream adaptations. 
%
LoRA-XS then \emph{freezes} $A$ and $B$ and inserts a small trainable matrix $R \in \mathcal{R}^{r \times r}$ between them, reducing the number of trainable parameters to $r^2$ ($r^2 \ll (n + m) \cdot r$) per weight matrix.
Then, the fine-tuning update is:
$
h = x W^0 + x \Delta W = x W^0 + x A R B,
$
where $A \in \mathbb{R}^{m \times r}$ and $B \in \mathbb{R}^{r \times n}$ are low-rank matrices obtained from the truncated SVD of $W^0$, specifically $A = U_r \Singular_r$ and $B = V_r^T$. 


\begin{figure*}[!t]
    %\vspace{-0.3cm}
    \begin{minipage}{1.0\textwidth}
    % \centering
    {\scriptsize 
        \makebox[0.95\textwidth]{CoLA} \\
    % \hspace{2cm}
    \includegraphics[width=1.0\linewidth]{fig/CoLA_covrank.pdf} \\
        \makebox[0.95\textwidth]{MRPC} \\
    \includegraphics[width=0.945\linewidth]{fig/mrpc_covrank.pdf}\\
    }    
    \end{minipage}
    %\vspace{-0.1cm}
    \caption{
Impact of the posterior covariance matrix rank ($k=0$ indicates the case with no off-diagonal elements) for CoLA (top) and MRPC (bottom). For brevity, confidence bars ($\pm$ standard deviation) are omitted for MRPC. The colored lines represent non-Bayesian baselines (e.g., standard LoRA or LoRA-XS at a given rank $r$).   
    }
    \label{fig:covrank}
\end{figure*}


\textbf{\our}, proposed in this paper, leverages the frozen projections $A$ and $B$ for effective and efficient Bayesian learning. In Sec.~\ref{sec:experiments}, we empirically demonstrate that $A$ and $B$, obtained from the SVD of the pre-trained weights, are not only effective for point-wise fine-tuning but also enable effective uncertainty quantification for $\{\Delta W_l\}$ through modeling covariances for $\{ R_l \}$.
Although we never compute it explicitly, the covariance matrix for individual $\Delta W$ is expressed as $\Sigma_{\Delta W} = (B^T  \otimes A)\Sigma_R(B^T \otimes A)^T$, where $\Sigma_R$ is the (intra-layer) covariance matrix for $R$ and $\otimes$ denotes the Kronecker product. 

In practice, we simply learn the joint posterior $p(\theta = \cup_l R_l | \mathcal{D}) \approx \mathcal{N}(\mu, \Sigma)$ for the inner matrices $R$. The covariance matrix $\Sigma$ captures both inter-layer and intra-layer dependencies, allowing the model to learn complex relationships.
At inference, similar to LoRA, we use samples of $R$ along with the respective projections $A$ and $B$ to obtain $h$, as realized through samples of $\Delta W$, however without ever computing it explicitly.

The parameters $\mu$ and $\Sigma$ are learned efficiently using \textbf{SWAG}~\cite{NEURIPS2019_118921ef} (though Variational Inference or the Laplace approximation could also be used). After a burn-in phase (a fixed 10 or 25 epochs) of the gradient-based optimization, the algorithm maintains $\hat \mu$ -- a running average of $\theta$ -- along with $k$ vectors of differences $\hat D_{{last}} = \theta_{{last}} - \hat \mu$ for the last $k$ values of $\theta$, and a running average of $\theta^2$. Based on these averages, we estimate the variances $\hat \sigma^2$ for individual parameters and approximate the covariance as
$
\hat \Sigma \approx \frac{1}{2}(\hat D \cdot \hat D^T + {diag}(\hat \sigma^2)),
$
which constitutes a rank-$k$ approximation to the covariance matrix.


We illustrate \our{}  method %in comparison to vanilla LoRA 
in Fig.~\ref{fig:method}.
Our method uses the total of $|\theta| \cdot (k+2)$ parameters, where $|\theta| = \sum_l r^2_l$.





\begin{figure*}[!t]
    %\vspace{-0.2cm}
    \centering
    {\scriptsize 
    CoLA\\
    \includegraphics[width=1.0\linewidth]{fig/CoLA_subsampling.pdf} \\
    MRPC\\
    \hspace{0.075cm}
    \includegraphics[width=0.9875\linewidth]{fig/mrpc_subsampling.pdf}\\
    }
    %\vspace{-0.1cm}
    \caption{Accuracy, ECE and NLL change as the training set is progressively reduced (e.g.\ -90\% means using only 10\% of the data for training). The dashed line marks the model's performance when trained on the full dataset.}
    \label{fig:subsampling}
    %\vspace{-0.15cm}
\end{figure*}

\section{Experiments}
\label{sec:experiments}

\textbf{Setup:}
We performed experimental evaluation on four GLUE tasks~\cite{wang2019} (RTE, MRPC, CoLA, and SST-2) using RoBERTA-large~\cite{liu2019}. We compare our method (\our) against the standard LoRA, LoRA-XS -- a parameter efficient variant, and against SWAG-LoRA~\cite{onal2024gaussian} -- a Bayesian variant.
For LoRA-XS and \our{} we considered ranks $r \in \{2, 8, 16, 25\}$ and for LoRA and SWAG-LoRA due to limited budget we were able to test $r \in \{2, 8\}$. The number of parameters (a \emph{proxy for storage and computation costs}) as a function of ranks $r$ and $k$ is summarized in Fig.~\ref{sec:method}.  
%For RTE and MRPC, we initialized weights for LoRA-XS and \our{} from an MNLI-finetuned checkpoint. 
We report accuracy (higher is better), ECE and NLL (lower is better) of median$\pm$std across 5 runs. 


\textbf{Performance analysis:} 
Fig.~\ref{fig:lorar} compares accuracy and calibration against total parameter count  across 4 datasets. Bayesian variants, including \our{} (ours) and SWAG-LoRA, outperform their non-Bayesian counterparts, particularly in ECE and NLL. However, our model achieves the best results with 5–15 times fewer parameters.  

%\vspace{-0.05cm}
Apart from very tight budgets (e.g., \#parameters $<300k$) where LoRA-XS excels, \our{} is also the best option in terms of accuracy. While SWAG-LoRA sometimes performs well, its results vary significantly between runs. In contrast, \our{} exhibits stable and consistent convergence. Finally, as results for MRPC and CoLA suggest, its performance remains robust across different values of $k$, whereas SWAG-LoRA's ECE deteriorates significantly at $k=2$.

\textbf{Covariance matrix rank analysis:} 
Figure~\ref{fig:covrank} compares the sensitivity of the Bayesian LoRA variants to changes in covariance matrix rank $k$.  
Markers indicate model sizes (e.g., SWAG-LoRA $\gg$ \our). As expected, SWAG-LoRA deteriorates proportionally as rank decreases. On the other hand, \our{} maintains its performance across a wide range of $k$. Significant degradation occurs only when off-diagonal covariance values are entirely ignored (i.e., at $k=0$). Notably, \our{} achieves the best calibration at low ranks, particularly at $k=2$ or $k=5$. This demonstrates that the SVD-based projection effectively disentangles parameters, enabling low-dimensional uncertainty modeling.  


\textbf{Data size reduction analysis:} 
Figure~\ref{fig:subsampling} compares how accuracy, ECE, and NLL degrade when training data is subsampled.  
All methods predictably lose accuracy as data size decreases, with little difference between the various LoRA-based approaches.  
We conclude that Bayesian learning does not improve robustness in this case.  
However, we note variations across datasets in terms of accuracy. For example, in MRPC, the decline is more pronounced, likely due to the dataset smaller size.  


\section{Related Work}

\paragraph{PEFT:} As large language models continue to grow, parameter-efficient fine-tuning (PEFT) has become a popular approach to reducing computational and storage costs. Among various methods~\cite{houlsby2019parameter,guo-etal-2021-parameter,li2021prefix,lester2021power}, LoRA~\cite{hu2021lora} has emerged as one of the most widely used. 
Building on its success, several approaches have been proposed to enhance different aspects of PEFT~\cite{kopiczko2023vera,adalora,dettmers2024qlora}. One such method, LoRA-XS~\cite{balazy2024lora}, further optimizes parameter efficiency by enabling flexible control over the number of trainable parameters per adaptation module. 
\our{} reuses the idea of SVD-based projections to reduce the parameter space dimensionality. 
%, allowing fine-tuning to be tailored to different computational constraints.

\paragraph{Bayesian LoRAs:} Standard LoRA~\cite{hu2021lora} does not account for uncertainty, making fine-tuned models susceptible to miscalibration. Then, Bayesian LoRA approaches integrate Bayesian inference techniques into LoRA to improve uncertainty estimation and generalization.

Several Bayesian LoRA methods have been proposed, each employing different Bayesian techniques to address these challenges.  SWAG-LoRA \cite{onal2024gaussian} combines Stochastic Weight Averaging-Gaussian (SWAG) with LoRA to enable approximate Bayesian inference, significantly improving model calibration and reducing overconfidence. Laplace-LoRA \cite{robeyns2024laplaceLora} applies a Laplace approximation to the posterior over LoRA parameters. Bella \cite{doan2025bayesianlowranklearningbella} introduces an approach that reduces the cost of Bayesian deep ensembles by applying multiple low-rank perturbations to a pre-trained model.
%
BLoB (Bayesian Low-Rank Adaptation by Backpropagation) \cite{wang2024blob} jointly learns both the mean and covariance of model parameters throughout the fine-tuning process using Variational Inference. B-LoRA \cite{meo2024bayesianlora} introduces a Bayesian perspective to both quantization and rank selection by using a prior distribution over these hyperparameters, optimizing model efficiency and reducing bit operations.

 The key challenge lies in balancing uncertainty modeling with parameter efficiency, as Bayesian inference typically increases both the number of trainable parameters and computational cost.
Despite their advantages, Bayesian LoRA methods face challenges related to increased parameter count and computational cost. One major issue is the higher storage and memory requirements, as Bayesian methods often require additional parameters to model uncertainty, particularly those involving covariance estimation, such as SWAG-LoRA. 
Scalability remains a concern for methods that explicitly model uncertainty across a large number of parameters.


% %\vspace{-0.05cm}
\section{Conclusion}
%\vspace{-0.05cm}
\our{} addresses the lack of uncertainty quantification in LoRA fine-tuning while maintaining parameter efficiency. It utilizes truncated SVD to project model updates into a lower-dimensional space and leverages the Bayesian framework to enhance uncertainty estimation.

Our method consistently achieves \emph{lower expected calibration error and negative log-likelihood} compared to the standard and parameter-efficient LoRA. Moreover, it matches or surpasses the accuracy of the Bayesian LoRA baseline while \emph{using significantly fewer parameters, exhibiting greater training stability, and relying on simpler, lower-rank covariance representations}.

We conclude that, with an appropriate weight space transformation, combining low-rank parameter updates with suitably low-rank covariance approximations consistently improves both predictive performance and calibration.

\clearpage
\section*{Acknowledgments}

This research is part of the project No. {2022/45/P/ST6/02969} co-funded by the National
Science Centre and the European Union Framework Programme for Research and
Innovation Horizon 2020 under the Marie Skłodowska-Curie grant agreement No.
945339. For the purpose of Open Access, the author has applied a CC-BY public copyright
licence to any Author Accepted Manuscript (AAM) version arising from this submission. 
\\
\includegraphics[width=1cm]{fig/eu_flag.jpg} \includegraphics[width=1.9cm]{fig/ccby.png}
\\ \\
We gratefully acknowledge Polish high-performance computing infrastructure PLGrid (HPC Center: ACK Cyfronet AGH) for providing computer facilities and support within computational grant no. PLG/2024/017893
\\ \\ 
The work of Klaudia Bałazy was supported by the National Centre of Science (Poland) Grant No. 2020/39/D/ST6/ 01332. Klaudia Bałazy is affiliated with Doctoral School of Exact and Natural Sciences at the Jagiellonian University.

% \section*{Limitations}

% \our{} relies on SWAG for posterior learning, though alternative methods may further improve its performance.
% It incorporates SVD computation as an additional step; however, this is a one-time operation that is computationally efficient and negligible compared to fine-tuning time.
% %
% Our experimental evaluation was conducted on GLUE benchmarks using RoBERTa-Large. We used the number of parameters as a proxy for computational and storage costs, and evaluated performance using accuracy, ECE, and NLL metrics.

\bibliography{custom}

\clearpage

\appendix


\section{Scientific Artifacts Licenses}

Listed below are the licenses for the scientific artifacts used in this research. For complete information, please use the links below and refer to the original sources.

%%% our artifacts, probably everything 
Scientific Artifacts: RoBERTa-large (MIT), MRPC (\href{https://aclanthology.org/I05-5002/}{Unknown}), RTE (\href{https://tac.nist.gov/publications/2009/additional.papers/RTE5_overview.proceedings.pdf}{Unknown}), COLA (\href{https://arxiv.org/pdf/1805.12471}{Unknown}), SST-2 (\href{https://aclanthology.org/D13-1170/}{Unknown}), HuggingFace Transformers Library (Apache-2.0), SWAG-LoRa repository\footnote{\url{https://github.com/fortuinlab/swag-lora}} (\href{https://github.com/fortuinlab/swag-lora}{MIT}), LoRa-XS  repository\footnote{\url{https://github.com/MohammadrezaBanaei/LoRA-XS}} (\href{https://github.com/MohammadrezaBanaei/LoRA-XS}{Unknown}).

\section{Model Size And Budget}

\begin{itemize}
    \item RoBERTA-large: 355M parameters
    \item GPUs: RTX4090 and V100-SXM2-32GB, each run was performed on a single GPU
    \item GPU total time: $\approx$ 63 days
\end{itemize}



\section{Statistics For Data}
We followed the original GLUE train-validation split
\begin{itemize}
    \item MRPC - train: 3'668, val: 408
    \item RTE - train: 2'490, val: 277
    \item CoLa - train: 8'551, val: 1'043
    \item SST2 - train: 67'349, val: 872
\end{itemize}

\section{Experimental Setup Details}


The study was conducted on a subset of the GLUE benchmark~\cite{wang2019}, specifically on RTE, MRPC, CoLA, and SST-2 tasks (with the original train-validation split), using RoBERTa-large~\cite{liu2019} checkpoints from the HuggingFace Transformers library~\cite{wolf2020}. 
For the RTE and MRPC tasks, we followed LoRA-XS and initialized LoRA-XS modules with weights fine-tuned on the MNLI task.
We integrated B-LoRA-XS/LoRA-XS modules into the Query, Value, Attention Output, and Output Fully Connected weight matrices in all transformer layers~\cite{NIPS2017_3f5ee243}, whereas due to budget limits, standard LoRA and SWAG-LoRA modules were added only to the Query and Value matrices. Note, this is sufficient for SWAG-LoRA to achieve its best performance.

For each dataset, for the burn-in stage of training, we adopted hyperparameters from the LoRA-XS paper. These include: learning rate, batch size, AdamW optimizer~\cite{loshchilov2019}, linear scheduler with warm-up, dropout, and the LoRA scaling factor $\alpha$. For standard LoRA we followed the same setup, except for the learning rate, which was determined through grid search. 
%After the base model converges, we proceed to the second phase of the SWAG process, where model checkpoints are collected. 
Similarly, the SWAG starting epoch (e.g. 10 or 25) was selected through grid search. % for each dataset and LoRA method independently. 
Based on the findings from SWAG-LoRA, we used a constant learning rate scheduler (SWALR) with warm-up. The SWAG learning rate was set to the maximum learning rate from the first (burn-in) phase of training. Unless stated otherwise, we used a low-rank covariance matrix approximation with the rank $k=10$. In all our experiments, SWAG estimation was applied exclusively to the LoRA modules, and SWAG predictions were consistently obtained with $S=15$ model samples.


% \section{Acknowledgments}
% We acknowledge the use of ChatGPT for grammar checking and generation of the initial version of the plotting code.

\end{document}


