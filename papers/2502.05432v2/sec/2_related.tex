\section{Related Works}
\label{sec:related_works}


% \textbf{Comprehension of human motion.} Deep learning has advanced pose estimation from handcrafted features to sophisticated convolutional and transformer-based models, improving accuracy and efficiency. One-shot CNN-based methods predict 3D poses from RGB images using voxel heatmaps and frameworks incorporating 2D and 3D labels during training \cite{Zhou2017, Pavlakos2018, sarandi2021metrabs}. Graph Convolutional Networks (GCNs) and Transformers \cite{VaswaniTransformer, dosovitskiy2021an} are increasingly used to handle spatial relationships among body joints, capturing both temporal and spatial dependencies in pose sequences \cite{LinWang2021, PoseFormer, Xu2021, Wehrbein2021, PoseSeq2Seq, 10030830}. The complexity arises from the inherent variability in human poses, occlusions, dynamic backgrounds, and the high dimensionality of visual data \cite{xie2024dynamic, alinezhad2023understanding}. Various approaches have been proposed to address these challenges, particularly in the domain of action recognition, which can be broadly categorized into pixel-based, keypoint-based, and heatmap-based methods.

%Pixel-based methods use raw video to extract spatial and temporal features directly from the pixel data . These methods capture rich visual information using deep convolutional networks including 3D convolutions, they are more susceptible to background noise and irrelevant context, leading to performance degradation \cite{wang2023memory, zaheer2022generative}. Additionally, the high dimensionality of pixel data increases computational complexity and resources required \cite{rasheed2023fine}.

Various approaches have been proposed to address the challenges of understanding human motion, including pixel-based \cite{bertasius2021space, wang2023memory, chen2024internvl}, keypoint/graph-based \cite{yan2018spatial, shi2019two, yang2021unik}, and heatmap-based methods \cite{PoseConv3D, feng2023diffpose}. Beyond these modalities, researchers have explored diverse model architectures to leverage them effectively, such as one-shot CNN-based methods \cite{Zhou2017, Pavlakos2018, sarandi2021metrabs}, Graph Convolutional Networks (GCNs), and Transformers \cite{LinWang2021, PoseFormer, Xu2021, Wehrbein2021, PoseSeq2Seq, 10030830}.

Building on these representations, discrete Variational Autoencoders (dVAEs) \cite{RameshZSdVAE, VqVAE, VqVAE2, jang2017categorical} have been developed to capture low-frequency image structures by constructing a quantized latent space. This latent encoding has enabled the use of dVAE encoders for training transformers to learn joint distributions between text and images \cite{RameshZSdVAE} and to model spatial correlations between image patches \cite{dosovitskiy2021an, bao2022beit}. These self-supervised transformers can then be fine-tuned for tasks such as image classification and object localization.

Building on these advancements, our work leverages the strengths of foundational models and discrete representation learning to address the unique challenges in human motion understanding. By combining the generalization of transformer with the structured representations of discrete variational encoders, our Motion Foundation Model (MoFM) introduces a self-supervised approach that captures both the spatio-temporal granularity of motion heatmaps and the interpretability of discrete motion tokens. This approach enables MoFM to generalize effectively across various human motion tasks, establishing it as a versatile backbone for applications such as action recognition and anomaly detection.

Foundation models have revolutionized natural language processing (NLP) and computer vision through large-scale pretraining and self-supervised learning, achieving remarkable generalization capabilities and enabling easy fine-tuning for domain-specific tasks \cite{Devlin2019BERTPO, brown2020language, myers2024foundation}. In NLP, models such as BERT \cite{Devlin2019BERTPO}, GPT-3 \cite{brown2020language}, and K2 \cite{deng2024k2} have demonstrated the effectiveness of training on unlabeled data to capture context and language nuances. Furthermore, transformer architectures with attention mechanisms \cite{vaswani2017attention, dosovitskiy2021an} have been integral to the success of foundational models across tasks.

In computer vision, foundation models \cite{kirillov2023segment, chen2022internvl} have adapted these principles to address complex visual tasks. They leverage masked image modeling \cite{bao2021beit}, vision transformers \cite{dosovitskiy2021an}, and extensive datasets to learn meaningful visual representations.


\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{\linewidth}
        \includegraphics[width=\linewidth]{fig/dVED_e_3.pdf}
        \caption{dVED Encoder.}
        \label{fig:dvede}
    \end{subfigure}
    \\
    \begin{subfigure}[b]{\linewidth}
        \includegraphics[width=\linewidth]{fig/dVED_d_4.pdf}
        \caption{dVED Decoder.}
        \label{fig:dvedd}
    \end{subfigure}
    \caption{Architecture of the proposed custom dVED encoder and decoder. The variable \textit{r} represents the number of layers in which the 2D/3D residual block can be repeated.}
    \label{fig:dved}
\end{figure}