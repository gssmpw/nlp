% \begin{figure*}[h!]
%     \centering
    
%     % Main figure showing only video frames and corresponding heatmaps
%     \includegraphics[width=\textwidth,trim={0 0 0 0},clip]{fig/combined_video_heatmap_larger.pdf}
% \caption{Visualization of videos with corresponding pose tokens normalized by vocabulary size. Each row shows skeletal motion frames alongside tokens, with values mapped from blue (low) to red (high), illustrating the alignment of skeletal actions with pose vocabulary.}
% \label{fig:motionbook}
% \end{figure*}

\begin{abstract}
% Despite extensive research on Large Language Models (LLMs) and vision-language models, comprehensive video understanding remains underexplored, with most efforts focused on single-image tasks. Applying LLM principles to videos introduces significant computational complexity and challenges in learning and generalization due to environmental noise and the dynamic nature of videos. This paper presents a novel multi-stage training framework for semantic understanding and self-supervised training of LLMs to comprehend complex human behaviors in videos, focusing on human motions in time and space. By abstracting pixel data to heatmaps, this approach protects privacy, reduces pixel-level noise, and enhances learning. Key innovations include a novel tokenization and embedding method with a human movement dictionary, enabling large-scale pretraining with lower computational and model complexity. The method uses Thermal Cubes and Pose Tokens to represent spatio-temporal pose heatmaps, applying discrete variational models for efficient pose discretization and forming a Pose Book dictionary. A new masking strategy for thermal cubes facilitates self-supervised transformer training. Extensive experiments validate the discrete variational model's performance, showing its effectiveness in reconstructing pose data during pretraining and fine-tuning. Our approach achieves competitive results in complex downstream tasks such as action classification.

% Foundation Models (FM) have increasingly drawn the attention of researchers due to their scalability and generalization across diverse tasks. Inspired by the success of FMs and the principles that have driven advancements in Large Language Models (LLMs), we introduce MoFM as a novel Motion Foundation Model. MoFM is designed for the semantic understanding of complex human motions in both time and space. By abstracting pixel-level data into heatmaps and limiting exposure to raw visual data, MoFM aims to move a step forward in reducing background noise and bias. To facilitate large-scale training, PoseBook, a comprehensive human motion dictionary of discretized motions is designed and employed. PoseBook leverages Thermal Cubes and Pose Tokens to capture spatio-temporal pose heatmaps, with discrete variational models' principles applied to encode and organize human movements into discrete units for efficient and scalable representation. MoFM, trained on a large corpus of motion data, is designed to serve as a foundational backbone for diverse downstream tasks, such as action recognition and anomaly detection. \reza{While the focus of this work is not to surpass current state-of-the-art models in these tasks,} it seeks to introduce a large-scale foundation model capable of capturing and understanding the intricacies of human motion. The addition of a simple fully connected head demonstrates the most straightforward baseline, emphasizing MoFM's potential as an effective backbone for various motion-based applications.

Foundation Models (FM) have increasingly drawn the attention of researchers due to their scalability and generalization across diverse tasks. Inspired by the success of FMs and the principles that have driven advancements in Large Language Models (LLMs), we introduce MoFM as a novel Motion Foundation Model. MoFM is designed for the semantic understanding of complex human motions in both time and space. To facilitate large-scale training, MotionBook, a comprehensive human motion dictionary of discretized motions is designed and employed. MotionBook utilizes Thermal Cubes to capture spatio-temporal motion heatmaps, applying principles from discrete variational models to encode human movements into discrete units for a more efficient and scalable representation. MoFM, trained on a large corpus of motion data, provides a foundational backbone adaptable to diverse downstream tasks, supporting paradigms such as one-shot, unsupervised, and supervised tasks. This versatility makes MoFM well-suited for a wide range of motion-based applications.


% it seeks to introduce a large-scale foundation model capable of capturing and understanding the intricacies of human motion. The addition of a simple fully connected head demonstrates the most straightforward baseline, emphasizing MoFM's potential as an effective backbone for various motion-based applications.

\end{abstract}