\section{Introduction}
\label{sec:intro}

Human-centric computer vision tasks such as pose estimation \cite{sun2019deep, fang2022alphapose, cheng2020higherhrnet}, action recognition \cite{PoseConv3D, geng2023focusing, yang2024one}, and anomaly detection \cite{noghre2024exploratory, hirschorn2023normalizing, wang2023memory} have garnered significant research attention. These tasks are pivotal in various applications, including surveillance systems \cite{pazho2023ancilia, neff2019revamp, ardabili2023understanding} and healthcare monitoring \cite{eswaran2024artificial, khang2024application, luo2023computer}.

Despite substantial progress, effectively capturing the complexity of human motion is a challenging endeavor due to the inherent variability in human poses, dynamic backgrounds, and the high dimensionality of visual data \cite{xie2024dynamic, alinezhad2023understanding}. To tackle these challenges, it is imperative to consider both optimization of the data modalities and development of advanced models capable of effectively understanding these data.

Utilizing the pixel data is a major approach in analyzing videos \cite{wang2023memory, zaheer2022generative, rasheed2023fine}, which can be susceptible to background noise and biases introduced by visual contexts. A common strategy to simplify human motion analysis is to employ pose estimation techniques \cite{sun2019deep, fang2022alphapose, cheng2020higherhrnet}. However, reducing raw pixel data to a sparse set of keypoints can lead to excessive abstraction and loss of valuable information such as fine-grained limb movements and contextual spatial relationships \cite{kocabas2019self, PoseConv3D}. Therefore, a balance must be struck between the high-dimensional raw pixel data and the overly abstracted keypoint representations. To address this issue, using heatmaps as the middle ground between pixels and poses is a well-known practice \cite{feng2023diffpose, luo2021rethinking}. Heatmaps, as the probable location of joints, preserve spatial uncertainty and provide a richer representation of human motion compared to discrete keypoints \cite{PoseConv3D}.

The emergence of Foundation Models (FMs) \cite{wang2024sam, wang2023internimage, chen2024internvl, ma2023crepe} has revolutionized various AI domains, particularly in natural language processing with models like BERT \cite{kenton2019bert}, GPT-3 \cite{brown2020language}, and other Large Language Models (LLMs) \cite{deng2024k2, myers2024foundation}. Such models have demonstrated remarkable scalability and generalization capabilities without extensive task-specific training. These models exhibit exceptional scalability and generalizability, without requiring extensive task-specific training. Consequently, FMs present a promising pathway for advancing human motion understanding.

Inspired by these advancements, we propose a novel Motion Foundation Model (MoFM) designed for the semantic understanding of complex human motions. Instead of processing entire videos or relying solely on keypoint-based poses, MoFM focuses on spatio-temporal heatmaps. To enable large-scale self-supervised training for human motion representation, we introduce a customized discrete Variational Encoder-Decoder (dVED), following principles from discrete Variational Autoencoders (dVAEs) \cite{vahdat2018dvae++, biswas2020dvae, vahdat2018dvae}. Mapping motion heatmaps into Thermal Cubes, we then utilize the proposed dVED to encode them into a discrete latent representation (motion tokens as visualized in \cref{fig:motionbook}). This process creates the MotionBook dictionary, organizing human movements into discrete units. The encoder of dVED functions as a tokenizer to enable BERT-style \cite{kenton2019bert} self-supervised MoFM backbone training through the masking of Thermal Cubes.


% The MoFM backbone, trained on a large corpus of data, is designed to serve as a foundational model for a range of downstream tasks, such as action classification and human anomaly detection. Our aim is not to outperform current state-of-the-art (SOTA) models in these specific tasks but to establish a versatile foundation model. MoFM backbone facilitates adaption to diverse downstream tasks with a simple adjustment to the task-specific head.

The MoFM backbone, trained on a large corpus of data, is designed to serve as a foundational model that can seamlessly adapt to diverse downstream tasks such as action classification and human anomaly detection. Rather than aiming to outperform existing state-of-the-art (SOTA) models, the MoFM backbone’s goal is to provide a flexible, task-agnostic foundation. Through minimal adjustments to the task-specific head, MoFM readily supports a range of applications, from one-shot learning scenarios to unsupervised and supervised tasks, underscoring its versatility for human motion analysis without the need for extensive retraining.

To assess the potential of the MoFM backbone, we target four human-centric downstream tasks: action recognition, one-shot action recognition, self-supervised anomaly detection, and supervised anomaly detection. For each task, we attach a simple, task-specific fully connected head to the MoFM backbone to establish a baseline. The results from this approach demonstrate MoFM’s effectiveness as a foundational model for motion-based applications.

Our experiments highlight its capacity to generalize across tasks, underscoring MoFM’s versatility as a comprehensive solution for diverse human motion understanding. The contributions of this paper are:

\begin{itemize}
    \item A discrete Variational Encoder-Decoder (dVED) designed to encode Thermal Cubes into a discrete latent space (MotionBook) for structured human movement representation.
    \item Pose-aware self-supervised BERT-style training to apply masked Thermal Cubes for task-agnostic learning of human motion.
    \item Motion Foundation Model (MoFM) for semantic understanding of human motion, serving as foundational backbone for diverse downstream tasks.
\end{itemize}