\section{MoFM}
\label{sec:approch}


\subsection{Spatio-Temporal Pose Representation}
Given an input sequence of 2D skeleton keypoints across $F$ frames for $J$ joints,  our initial step involves creating a spatio-temporal heatmap. This is achieved by implementing $J$ Gaussian functions across the $F$ frames as follows:
\begin{align}
U_f=e^{-\frac{(i-x_{jf})^2+(i-y_{jf})^2}{2\times\sigma^2}}, \label{eq:guss_factor}
\end{align}
where $\sigma$ is responsible for determining the spread of the energy around each joint, labeled as $j$, located at coordinates $\langle x_{jf}, y_{jf}\rangle$ for the frame $f$.  A max-pooling filter, spanning over \( J \) joints, is applied to the thermal energy matrix \( U | U \in \mathbb{R}^{J \times F \times H \times W} \), where H and W are the height and the width of the heatmap, respectively. This operation condenses \( U \) into a matrix of reduced dimensions \( U_R \), where \( U_R \in \mathbb{R}^{F \times H \times W} \). This technique simplifies the representation of thermal energy across frames by emphasizing the most significant energy values in each region of the heatmap. Both \( U \) and \( U_R \) are subsequently utilized for training the dVED.

\subsubsection{Thermal Cubes}
To configure the data for processing by a transformer (MoFM backbone), the thermal matrix \(U\) must be segmented into multiple cubes, capturing both the spatial and temporal dimensions of the motion. Thus, \(U\) is transformed into \(K = \frac{HW}{D_S \times D_S}\) cubes, where each cube \(C_i \in \mathbb{R}^{J \times F \times D_S^2}\) and $D_s$ is the patch size. In this work, we centered and scaled the skeleton to fit a heatmap size of \(H = W = 72\). The skeleton structure has \(J = 17\) joints, and \(F = 48\). We also set \(D_S\) to 4, resulting in the thermal matrix \(U\) being divided into \(K = 324\) cubes, each with the shape \(C_i \in \mathbb{R}^{17 \times 48 \times 4 \times 4}\).

 
\subsubsection{Motion Tokens}\label{subsec:pose_token}
To enable masked input self-supervised training of a transformer, we first need to convert thermal cubes into discrete tokens. Following the methodology outlined by \cite{RameshZSdVAE, jang2017categorical}, we employ a custom discrete Variational Encoder-Decoder (dVED) to facilitate the mapping function \( \mathcal{G}: U \to \widehat{U}_{R} \):
\begin{align}
    \widehat{U}_{R} = f_{\text{dec}}(f_{\text{GS}}(f_{\text{enc}}(U; \theta), \tau); \phi),
\end{align}
where \( \theta \) and \( \phi \) represent the encoder and decoder learnable parameters, respectively, and \( \tau \) is the Gumbel-Softmax temperature parameter. The encoder, Gumbel-Softmax approximation, and decoder are represented by \( f_{\text{enc}} \), \( f_{\text{GS}} \), and \( f_{\text{dec}} \), respectively. 
Formally, the encoder component of the dVED, which functions as a tokenizer, converts the initial heatmap, \( C = \{C_i\}_{i=0}^{K-1} \), into \(K\) discrete tokens, denoted as \( z = \{z_i\}_{i=0}^{K-1} \). Each \(z_i\) is a categorical variable with class probabilities \( \pi = \{\pi_i\}_{i=0}^{T-1} \). \( T \) indicates the size of the vocabulary. Using the Gumbel-Softmax trick, we have: 
\begin{align}
    z_i = \text{one\_hot}(\argmax_i{[g_i + \log\pi_i]}),
\end{align}
where \( g = \{g_i\}_{i=0}^{T-1} \) are samples drawn from Gumbel(0, 1)\footnote{The random variable \( g \) distributed as \( \text{Gumbel}(0, 1) \) can be generated by \( g = -\log(-\log(u)) \), where \( u \) is drawn from a uniform distribution, \( u \sim U(0, 1) \). Alternatively, it is also common to use exponential distribution sampling, where \( g = -\log(x) \), and \( x \sim \text{Exp}(\lambda) \) is drawn from an exponential distribution with a rate parameter \( \lambda = 1 \).}. 
The vocabulary, \(\mathcal{V}\), is constructed as a MotionBook and is defined by the following matrix:
\begin{align}
\mathcal{V} = 
\begin{pmatrix}
v_{0,0} & v_{1,1} & \cdots & v_{0,D-1} \\
\vdots & \vdots & \ddots & \vdots \\
v_{T-1,0} & v_{T-1,1} & \cdots & v_{T-1,D-1} \\
\end{pmatrix},
\end{align}
where \( D \) denotes the dimensionality of each vocabulary entry. The motion dictionaries are trained and later used by the decoder to reconstruct \(\widehat{U}_R\). 
\begin{figure*}[h]
  \centering
  \includegraphics[width=1.0\textwidth,,trim= 10 10 20 10,clip]{fig/Flow_2.pdf}
  \caption{Overview of Motion Foundation Model (MoFM). Poses are converted into heatmap representations using a Gaussian function (\cref{eq:guss_factor}), producing a series of thermal cubes. Before pre-training, we train the custom dVED model for reconstruction. This involves tokenizing a series of heatmap cubes in both spatial and temporal dimensions according to a learned vocabulary. After cubing, tokens are masked keypoint-wise with a special mask embedding [\texttt{M}]. The resulting \(\{C_i^{m}\}_{i=0}^{K-1}\) masked cubes are then fed into a vision transformer encoder. The backbone predicts the visual tokens of the discretized image based on \(\{z_i\}_{i=0}^{K-1}\) generated by dVED.}
  \label{fig:self_trans}
\end{figure*}
\subsection{dVED for Motion Discretization}
In this paper, we present a custom dVED model with distinct encoder and decoder structures. The encoder utilizes 3D ResBlocks and 3D convolution blocks for dimensionality reduction, while the decoder employs 2D transposed convolutions and 2D ResBlocks to reconstruct inputs from the MotionBook back to \(U_R\) as illustrated in \cref{fig:dved}. 

%Our analysis of the dVAE architecture revealed challenges in training phase to perform the mapping \(\mathcal{G}_{3D}: U \rightarrow \widehat{U}\). A significant challenge arises from using a single-layer Pose Book, \(\mathcal{V}\), implemented with \texttt{nn.Embedding} in PyTorch. For an input cube \(C_i \in  \mathbb{R}^{J \times F \times D_{S}^2}\) processed by 3D convolutions, the output logits are in the format \(\langle T, f, h, w \rangle\). This configuration results in a sampling dimension \(\acute{K} = f \times h \times w\) from \(\mathcal{V}\). The core issue lies in mapping both spatial dimensions, \(h\) and \(w\), and the temporal dimension \(f\) to a single Pose Book, which cannot adequately represent both spatial and temporal features simultaneously. Consequently, we designed the encoder to output logits in the shape of \(\mathbb{R}^{T\times 1\times h\times w}\) and the decoder to produce heatmaps in the shape of \(\mathbb{R}^{F\times H\times W }\), mapping its hidden inputs to \(\widehat{U}_R\). The following sections explain the pose discretization and training process based on the described model structure.


%\subsection{dVED Training}
The encoder, \({q}_{\theta}(z|U)\), is trained to map thermal cubes to a discrete latent space, $z$, while the decoder reconstructs the condensed heatmap, $U_{R}$, by applying \({p}_{\phi}(U_{R}|z)\). Therefore, The objective function can be formalized as follows:
\begin{align}
\label{eq:dv_loss}
\begin{split}
& \mathcal{L} = -\mathbb{E}_{{q}_{\theta}(z|U)}[\log {p}_{\phi}(U_{R}|z)] \\
& + \beta \times D_{\mathrm{KL}}({q}_{\theta}(z|U) \parallel p(z))
\end{split}
\end{align}
where \( \log {p}_{\phi}(U_{R}|z) \) represents the likelihood of the reconstructed data given the latent variables, \( D_{\mathrm{KL}} \) is the KL divergence between the approximate posterior \( {q}_{\theta}(z|U) \) and the prior \( p(z) \), typically chosen to be uniform over the discrete categories, and $\beta$ is the coefficient for the KL divergence term. 

% \input{tab/dvae_arch}
% \input{tab/dve_hyper}

The Gumbel-Softmax trick allows this objective to be optimized via gradient descent by approximating categorical sampling using the softmax function:
\begin{align}
y_i = \frac{\exp((\log(\pi_i) + g_i) / \tau)}{\sum_{j=0}^{T-1} \exp((\log(\pi_j) + g_j) / \tau)},
\end{align}
where \( \tau \) is the temperature that controls the softness of the distribution. We schedule \(\tau\) to be dynamically reduced during the training phase to improve performance. For our setup, we approximate the reconstruction term \(\mathbb{E}_{{q}_{\theta}(z|U)}[\log {p}_{\phi}(U_{R}|z)]\) with the smooth L1 loss function.

For better generalization of dVED, it is essential that the poses encompass a wide variety of actions covering the full range of motion. We chose the CMU Panoptic dataset, a comprehensive collection offering sequences of pose information from multiple perspectives. For each randomly selected video from the dataset, we divided it into \(F\) bins and randomly selected a frame from each bin. We then applied \cref{eq:guss_factor} to generate \(U\), and \(U_R\). The dVED model architecture features an encoder with two 3D Convolution and two 3D ResNet blocks, and a decoder with two 2D Convolution and two 2D ResNet blocks, with a hidden dimension of 256. The pose book also has a 256 hidden dimension and a vocabulary size of 8192.
Training was performed over 20 epochs with a batch size of 24, utilizing the AdamW optimizer and a WarmupCosineLR scheduler. The maximum and minimum learning rates were set to $3.0e-4$ and $1.0e-6$, respectively, with a single warmup epoch. \cref{fig:combined_frames} qualitatively compares the performance of dVED, shown in \cref{fig:dvae}, with the ground truth, shown in \cref{fig:gt}. A notable phenomenon observed in the reconstructed heatmap is the ghosting effect around moving joints, which clearly demonstrates how the thermal energy transitions from one point to another in the direction of movement.



\subsection{MoFM Backbone and Self-supervised Training}
The Spatio-Temporal Embedding Layer of the MoFM consists of $\textit{l}$ layers of 3D residual block ($R$) to map $C$ to \(\mathcal{H} | \mathcal{H} \in \mathbb{R}^{K \times H}\) where $H$ is the hidden size of the vision transfomer encoder. The transformer encoder used in our work is the base transformer model proposed in \cite{dosovitskiy2021an, VaswaniTransformer}. The architecture uses 12 layers and 12 attention heads, each with a size of 64. The hidden dimension, feed-forward network size, and maximum sequence length are set to 768, 3072, and 384, respectively.
 
After training dVED, we used its encoder as a tokenizer. As outlined in \cref{subsec:pose_token}, to enable self-supervised training of the transformer encoder, we split the input \(U\) into \(K\) cubes, \(\{C_i\}_{i=0}^{K-1}\), and utilized the dVED tokenizer to map these cubes to the latent space, \(\{z_i\}_{i=0}^{K-1}\). Consequently, we set the transformer's input sequence length to \(K\). 

We propose a targeted approach for motion-wise masking of spatio-temporal cubes, detailed in Algorithm \ref{algo:blocking}. Initially, we experimented with blockwise masking as suggested in \cite{bao2022beit}; however, while this reduced the loss error, it did not yield qualitatively accurate reconstructions. We attribute this to the random block placements, which often masked regions without any pose skeletons, leading to ineffective learning. To address this, we developed keypoint-wise masking. In this approach, we randomly select a joint \( j \) from the given set \( P \). For each selected joint \( j \), located at coordinates \(\langle x_{jf}, y_{jf} \rangle\) in frame \( f \), we identify the block containing the joint and mark it as a candidate for masking. This process is repeated until the number of masked blocks specified by \( num\_block\_to\_mask \) in Algorithm \ref{algo:blocking} is reached.

The transformer encoder is then trained to classify the received Thermal Cubes based on the output of the dVED encoder. \cref{fig:self_trans} illustrates the closed-loop training process, in which the dVED encoder functions as a tokenizer to support self-supervised training. Masked cubes, represented as [\texttt{M}], are shaded in gray and treated as learnable parameters. Training was conducted over 20 epochs with a batch size of 32, using the AdamW optimizer with a cosine learning rate scheduler and warmup. The learning rate was set between a maximum of \(1.5 \times 10^{-4}\) and a minimum of \(1.0 \times 10^{-8}\), with a single warmup epoch.

\input{alg/masking}

% \input{tab/trans_arch}
\input{tab/trans_pre}

