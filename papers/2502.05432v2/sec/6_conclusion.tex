\section{Conclusion}
\label{sec:conslusion}
% In this work, we presented \memt~, a solution for human-focused video understanding that extends existing Vision Transformer (ViT) models through a multi-stage training framework. This framework pre-trains a generic transformer to contextually understand human body movement. Our methodology creates a dictionary of discretized human motions (the human motion codebook) for tokenization, generating motion embeddings in both time and space. It begins by modeling human motion in thermal cubes and formulates a discrete Variational Encoder-Decoder (\dV) model to map these thermal cubes to a discrete latent space, creating a pose book dictionary. The decoder uses the motion book to reconstruct the heatmap, ensuring the dVED's performance, which is evaluated both quantitatively and qualitatively. The encoder then acts as a tokenizer to enable BERT-style self-supervised transformer training via masking thermal cubes. \memt~achieves minimal model size overhead by abstracting tokenization to the human motion heatmap rather than the original pixel stream. We demonstrated that the generic model could effectively perform downstream tasks, such as action classification, with competitive performance compared to state-of-the-art narrow application-focused approaches. The experimental results showed that with a minimal 2.4\% increase in the ViT base model size, \memt~functions effectively as a motion transformer for general human motion understanding over sequences of videos. Our main contributions include introducing a novel approach to modeling human motion using thermal cubes, proposing a temporal-spatial \dV~to discretize human poses into a discrete latent space, evaluating the \dV~performance through heatmap motion book dictionary reconstruction, and demonstrating the effectiveness of the pre-trained generic transformer in downstream tasks like action classification, comparing its performance with existing methods.

In this work we presented MoFM, a large-scale motion foundation model that understands the semantics of human motions. By utilizing Thermal Cubes and a discrete Variational Encoder-Decoder (dVED) model, we mapped spatio-temporal heatmaps into a discrete latent space, creating the comprehensive MotionBook dictionary. This approach enables task-agnostic self-supervised training, allowing MoFM to learn rich representations of human motion. Using MoFM as a foundational backbone, augmented with a simple fully connected linear layer to create a baseline, we have demonstrated its capabilities in handling downstream tasks including action classification, one-shot action classification, self-supervised human anomaly detection, and supervised human anomaly detection. We believe that MoFM will serve as a valuable foundation for future research endeavors in human motion understanding, enabling the development of more advanced models and applications in the field.