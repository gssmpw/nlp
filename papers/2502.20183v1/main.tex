%\documentclass[12pt, draftclsnofoot, onecolumn]{IEEEtran}
\documentclass[lettersize,journal]{IEEEtran}

\usepackage{graphicx} 
\usepackage{epstopdf}
\usepackage{subfigure}
\usepackage{stfloats}
\usepackage{algorithm, algorithmic}
\usepackage{diagbox}
\usepackage{multirow}
\usepackage{mathtools}
\usepackage{setspace}  
\usepackage{threeparttable}
\usepackage{textcomp,booktabs}
\usepackage[usenames,dvipsnames]{color}
\usepackage{colortbl}
\usepackage{indentfirst}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{color}
\usepackage{enumitem}

\newtheorem{lemma}{\bf Lemma}
\newtheorem{proposition}{\bf Proposition}
\newtheorem{definition}{Definition}
\theoremstyle{definition}
\newtheorem{remark}{\bf Remark}

\newtheorem{theorem}{Theorem}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}

\interdisplaylinepenalty=2500

\definecolor{mygray}{gray}{.9}
\definecolor{mypink}{rgb}{.99,.91,.95}
\definecolor{mycyan}{cmyk}{.3,0,0,0}

\setlength{\parindent}{1em}

\begin{document}
\title{Mixture of Experts-augmented Deep Unfolding for Activity Detection in IRS-aided Systems}
\author{Zeyi Ren, Qingfeng Lin, Jingreng Lei, Yang Li, and Yik-Chung Wu 
 \thanks{Z. Ren, Q. Lin and J. Lei are with the Department of Electrical and Electronic Engineering, The University of Hong Kong, Hong Kong, and also with the Shenzhen Research Institute of Big Data, Shenzhen 518172, China (e-mail: \{renzeyi, qflin, leijr\}@eee.hku.hk).}
\thanks{Y. Li is with Shenzhen Research Institute of Big Data,
Shenzhen~518172, China, also with the School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen 518172, China (e-mail: liyang@sribd.cn).}
\thanks{Y.-C. Wu is with the Department of Electrical and Electronic Engineering, The University of Hong Kong, Hong Kong (e-mail: ycwu@eee.hku.hk).}
}

\maketitle

\begin{abstract}
In the realm of activity detection for massive machine-type communications, intelligent reflecting surfaces (IRS) have shown significant potential in enhancing coverage for devices lacking direct connections to the base station (BS). However, traditional activity detection methods are typically designed for a single type of channel model, which does not reflect the complexities of real-world scenarios, particularly in systems incorporating IRS. To address this challenge, this paper introduces a novel approach that combines model-driven deep unfolding with a mixture of experts (MoE) framework. By automatically selecting one of three expert designs and applying it to the unfolded projected gradient method, our approach eliminates the need for prior knowledge of channel types between devices and the BS. Simulation results demonstrate that the proposed MoE-augmented deep unfolding method surpasses the traditional covariance-based method and black-box neural network design, delivering superior detection performance under mixed channel fading conditions.
\end{abstract}

\begin{IEEEkeywords}
Activity detection,
deep unfolding,
massive machine-type communications,
mixture of experts.

\end{IEEEkeywords}

\section{Introduction} \label{intro}

% As an emerging paradigm for the future communication systems, mMTC aims to support a large number of devices over the same time and frequency resources. Traditional grant-based random access is no longer effective for mMTC due to its low access efficiency. Consequently, grant-free random access becomes a viable alternative, where the device can send its data without
% obtaining the permission from the AP. However, as massive connectivity involves a large number of devices, it is impossible to assign orthogonal signature sequences for these potential devices, and the device activity detection becomes more complicated than that utilizing orthogonal signature sequences.

% There are generally two lines of research for device activity detection. On one hand, by exploiting the sporadic traffic in mMTC, the activity detection can be formulated as a compressed sensing (CS) problem, in which device activities can be estimated jointly with the instantaneous channel state information (CSI) by sparse recovery algorithms. On the other hand, when the statistical CSI is known, the activity detection can be formulated as a maximum likelihood estimation problem, which is often called the covariance approach because the problem depends only on the sample covariance matrix of the received signals. It was shown both theoretically and empirically that the detection performance of the covariance approach is generally much better than that of the CS based approach, especially for massive MIMO. 

% While activity detection has been extensively studied from many perspectives, existing approaches xx.
 
% Numerical results are presented to demonstrate xx.

% The remainder of this paper is organized as follows. System model and problem formulation are presented in Section~xx. 

% \textit{Notation:} We use upper-case bold letters, lower-case bold letters, and lower-case letters to denote matrices, vectors, and scalars, respectively. The real and complex domains are denoted by $\mathbb{R}$ and $\mathbb{C}$, respectively. We denote the transpose and conjugate transpose of a vector/matrix by $(\cdot)^{\mathrm{T}}$ and $(\cdot)^{\mathrm{H}}$, respectively, while the inverse of a matrix is denoted by $(\cdot)^{-1}$.
Massive machine-type communications (mMTC) have been expected to play a vital role to empower the sixth-generation (6G) vision of future ubiquitous connectivity. To meet the low latency requirement in mMTC, grant-free random access is recognized as a promising solution. However, grant-free random access requires the base station (BS) to perform activity detection~\cite{Liuliang_TSP_FUTURE,tsp_hao,liyang_ad}. Due to the large number of potential Internet of Things (IoT) devices and nonorthogonal signature sequences, activity detection is a challenging task.

Mathematically, the optimization algorithms for activity detection task have been broadly categorized into two types: compressed sensing (CS)-based algorithms~\cite{tspll} and covariance-based algorithms~\cite{haghighatshoar2018, C_ICC_Czl_ad, TWC_Lin2022}. Both theoretically and empirically,
it has been demonstrated that the covariance-based algorithms
generally outperform the CS-based algorithms in terms of detection performance~\cite{tit_c}. However, covariance-based algorithms rely on the tractability of the covariance matrix of the received signal, which further requires accurate channel fading statistics from various devices to the BS. This might not be practical, especially in the recent intelligent reflecting surface (IRS)-aided systems~\cite{J_TWC_shaoxiaodan_risad,J_TSP_shiyuanming_risad} where some devices may directly connected to the BS while other through the help of an IRS. 

On the other hand, deep learning-based algorithms have become popular in communication research due to their ability to overcome modeling inaccuracy~\cite{Sun_TSP,lei}. However, black box deep learning designs (e.g., multi-layer perceptrons (MLP) or convolutional neural networks) do not incorporate the domain knowledge of communication systems. This usually results in unsatisfactory performance or the neural network not being able to converge during training. To overcome such drawbacks, model-driven deep unfolding has emerged as a viable alternative. By regarding each iteration of an optimization algorithm as one layer of the neural networks, the model-driven deep unfolding algorithm embeds the domain knowledge into neural network designs while exploiting the data to determine the behavior of the model.

From the above arguments, applying deep unfolding to the activity detection problem in IRS-aided systems seems to be an obvious choice. However, there are still two challenges. Firstly, due to the existence of the IRS composite channel, the rank-one update usually employed in covariance-based method is not applicable anymore.  To this end, we propose a projected gradient descent update to facilitate the unfolding. Secondly, even after unfolding, the algorithm still depends on the knowledge of each device's fading type. To circumvent this issue, we leverage the mixture of experts (MoE) approach~\cite{Du_MoE,wc_du}, which takes in the received signal and determines which type of fading channels dominates so that an appropriate expert can be used. In this way, there is no need to know the fading channel type for each device to execute the algorithm.

Numerical results demonstrate that the proposed MoE-augmented model-driven deep unfolding outperforms conventional covariance-based methods and black-box neural network designs. Furthermore, the performance loss from not knowing the fading channel type of each device is minimal compared to the unfolded network with perfect information. 


\section{System Model and Problem Formulation}   \label{sys}

\subsection{System Model}
Consider a single-cell network with a BS having $M$ antennas, as shown in Fig.~\ref{Fig7}. In this network, there are three sets of devices, with the sets denoted by $\mathcal{K}_1$, $\mathcal{K}_2$, and $\mathcal{K}_3$, respectively. For devices in $\mathcal{K}_1$, their direct links to the BS are obstructed, thus require an IRS with $N$ reflecting elements to enhance communication quality~\cite{J_TWC_shaoxiaodan_risad,J_TSP_shiyuanming_risad}. For $\mathcal{K}_2$ and $\mathcal{K}_3$, the devices are those only with direct links to the BS, with devices in $\mathcal{K}_2$ following Rician fading and those in $\mathcal{K}_3$ following Rayleigh fading.

\begin{figure} [t]
	\centering 
		 \includegraphics[width=0.4\textwidth]{Figs/fig_system_model.png}  
	
  	\caption{IRS-aided massive random access with mixed channel types}  \label{Fig7}
\end{figure}

Let $\mathbf{f}_k \in \mathbb{C}^{M}$ denote the small-scale fading from device~$k$ to the BS, it can be expressed as
%\begin{align}\label{h} 
%\mathbf{h}_k \sim \sqrt{\frac{\kappa_{\text{U}}}{1+\kappa_{\text{U}}}} \mathbf{h}_k^{\text{LoS}}+\sqrt{\frac{1}{1+\kappa_{\text{U}}}} \mathcal{CN} \left(\mathbf{0}, \mathbf{I}_{N}\right), 
%\end{align}
\begin{align}\label{f}
    \mathbf{f}_k \sim \sqrt{\frac{\kappa_{\text{U}}}{1+\kappa_{\text{U}}}} \mathbf{f}_k^{\text{LoS}}+\sqrt{\frac{1}{1+\kappa_{\text{U}}}} \mathcal{CN} \left(\mathbf{0}, \mathbf{I}_{M}\right),
\end{align}
where $\mathbf{f}_k^{\text{LoS}}$ is given by the array response vector at the BS due to device~$k$, and $\kappa_{\text{U}}$ is the Rician factor. In case of Rayleigh fading, $\kappa_{\text{U}}$ = 0. Similarly, the small-scale fading from device~$k$ to the IRS and that from the IRS to the $m$-th antenna of the BS are modeled as:
\begin{align}\label{h} 
\mathbf{h}_k \sim \sqrt{\frac{\kappa_{\text{R}}}{1+\kappa_{\text{R}}}} \mathbf{h}_k^{\text{LoS}}+\sqrt{\frac{1}{1+\kappa_{\text{R}}}} \mathcal{CN} \left(\mathbf{0}, \mathbf{I}_{N}\right), 
\end{align}
\begin{align} \label{g}
\mathbf{g}_m \sim \sqrt{\frac{\kappa_{\text{B}}}{1+\kappa_{\text{B}}}} \mathbf{g}_m^{\text{LoS}}+\sqrt{\frac{1}{1+\kappa_{\text{B}}}} \mathcal{CN} \left(\mathbf{0}, \mathbf{I}_{N}\right), 
\end{align}
where $\kappa_{\text{R}}$ and $\kappa_{\text{B}}$ are the corresponding Rician factors.

Denote the device activity indicator of device~$k$ as $b_k = 1$ if it is active and 0 otherwise.
Moreover, a unique signature sequence ${\mathbf{s}}_{k}=\left[ s_{k, 1}, s_{k, 2}, \ldots, s_{k, L}\right]^{T} \in \mathbb{C}^{L}$ is assigned to each device~$k$. At the start of each block, all the active devices send their signature sequences and the BS conducts activity detection based on the received signals. Assuming that the transmissions from different devices are synchronous, the received signal is derived as
\begin{align}\label{receive}
\begin{aligned}
\mathbf{Y} &= \sum_{k\in\mathcal{K}_1} \underbrace{b_{k}  \sqrt{p_k \beta_{k}}}_{a_k}\mathbf{s}_{k}   {\mathbf{h}}^{H}_{k} \mathbf{\Theta} \mathbf{G} + \sum_{k\in \mathcal{K}_2} b_{k}\sqrt{p_k \beta_{k}}\mathbf{s}_{k}   {\mathbf{f}}^{H}_{k}\\
&+ \sum_{k\in \mathcal{K}_3} b_{k}\sqrt{p_k \beta_{k}}\mathbf{s}_{k}   {\mathbf{f}}^{H}_{k} +\mathbf{W},
\end{aligned}
\end{align}
where $\mathbf{G}=[\mathbf{g}_1,\mathbf{g}_2,\dots,\mathbf{g}_M]$,  $\beta_{k}$ is the cascaded large-scale fading coefficient for device $k$, $p_{k}$ is the transmit power of the $k$-th device, $\mathbf{\Theta}$ is a diagonal matrix containing the phase-shift coefficients of the IRS elements. The elements of $\mathbf{W} \in \mathbb{C}^{L\times M}$ are independent and identically distributed (i.i.d.) Gaussian noise at the BS following $\mathcal{C}\mathcal{N}\left(0, \sigma_{w}^{2}\right)$ with $\sigma_{w}^{2}$ being the noise power.
\subsection{Problem Formulation}
The activity detection problem is mathematically equivalent to detecting whether $a_{k}\triangleq b_{k}  \sqrt{p_k\beta_k}$ is positive or zero. To this end, we treat the device activities $\{a_k\}_{k\in\mathcal{K}_1\cup\mathcal{K}_2\cup\mathcal{K}_3}$ as a set of unknown but deterministic parameters, and model the received signal $\mathbf{Y}$ as a random variable. With $\mathbf{y}_m$ denoting the $m$-th column of $\mathbf{Y}$, the covariance matrix of $\mathbf{y}_m$ is given by the following proposition. %and introduce an approximation of the loglikihood function as follow $-p\left(\mathbf{Y} ;\left\{a_k\right\}_{k=1}^{K_3}\right) \approx \sum_{m=1}^M p\left(\mathbf{y}_m ;\left\{a_k\right\}_{k=1}^{K_3}\right)$. %The reason lies in the fact that directly computing the covariance matrix of $\mathbf{Y}$ can be computationally expensive and retains noise statistics which may amplify the impact of noise, whereas column summation can suppress noise through non-coherent averaging and improve effective SNR.

\begin{proposition}
Given the channel models $\mathbf{f}_k$ in~\eqref{f}$, \mathbf{h}_k$ in~\eqref{h} and $\mathbf{g}_m$ in~\eqref{g}, when N is sufficiently large, the likelihood function of $\left\{a_k\right\}_{k\in\mathcal{K}_1\cup\mathcal{K}_2\cup\mathcal{K}_3}$ is given by
\begin{align} \label{likelihood_z}
&\log p\left(\mathbf{y}_m ; \left\{a_k\right\}_{k\in\mathcal{K}_1\cup\mathcal{K}_2\cup\mathcal{K}_3}\right)=- L \log \pi \nonumber \\& -\left(\log \left|\boldsymbol{\Sigma}_m\right|+\operatorname{tr}\left(\boldsymbol{\Sigma}_m^{-1}\left(\mathbf{y}_m-\bar{\mathbf{y}}_m\right)\left(\mathbf{y}_m-\bar{\mathbf{y}}_m\right)^H\right)\right),
\end{align}
where
\begin{align}
    \begin{aligned}
    \bar{\mathbf{y}}_m &=\sum_{k\in\mathcal{K}_1} {a_k}\mathbf{s}_{k}(\sqrt{\frac{\kappa_{\text{U}}}{1+\kappa_{\text{U}}}} \mathbf{h}_k^{\text{LoS}})^H \mathbf{\Theta} \sqrt{\frac{\kappa_{\text{B}}}{1+\kappa_{\text{B}}}} \mathbf{g}_m^{\text{LoS}}\\
    &+\sum_{k\in\mathcal{K}_2}{a_k}\mathbf{s}_{k}   \sqrt{\frac{\kappa_{\text{U}}}{1+\kappa_{\text{U}}}} \mathbf{f}_{k}^{\text{LoS}}(m)^*
    \end{aligned}
\end{align}
%$\bar{\mathbf{y}}_m=\sum_{k\in\mathcal{K}_1} {a_k}\mathbf{s}_{k}   (\sqrt{\frac{\kappa_{\text{U}}}{1+\kappa_{\text{U}}}} \mathbf{h}_k^{\text{LoS}})^H \mathbf{\Theta} \sqrt{\frac{\kappa_{\text{B}}}{1+\kappa_{\text{B}}}} \mathbf{g}_m^{\text{LoS}}$ +$\sum_{k\in\mathcal{K}_2}{a_k}\mathbf{s}_{k}   (\sqrt{\frac{\kappa_{\text{U}}}{1+\kappa_{\text{U}}}} \mathbf{h}_k^{\text{LoS}})^H$
and 
\begin{align}
\begin{aligned}
\boldsymbol{\Sigma}_m = \sum_{k\in\mathcal{K}_1} {a_k} \Xi_{m,k} \mathbf{s}_{k} \mathbf{s}_{k}^H &+\sum_{k\in\mathcal{K}_2} \frac{a_k\mathbf{s}_{k} \mathbf{s}_{k}^H}{1+\kappa_{\text{U}}}\\
&+\sum_{k\in\mathcal{K}_3} {a_k}  \mathbf{s}_{k} \mathbf{s}_{k}^H +\sigma_w^2 \mathbf{I}_M, 
\label{xxx}
\end{aligned}
\end{align}
with $\mathbf{f}_{k}^{\text{LoS}}(m)$ denoting the $m$-th element of $\mathbf{f}_{k}^{\text{LoS}}$ and $\Xi_{m,k} =\frac{N +\kappa_{\text{B}}\|{\mathbf{g}}_m^{\text{LoS}}\|_2^2+\kappa_{\text{U}}\|{\mathbf{h}}_k^{\text{LoS}}\|_2^2}{(1+\kappa_{\text{B}})(1+\kappa_{\text{U}})}$.
\end{proposition}
\begin{proof}
Please see Appendix~A.
\end{proof}


Due to the presence of the IRS, different columns of $\mathbf{Y}$ are not mutually independent. Therefore, an explicit expression for the joint likelihood function of all columns of $\mathbf{Y}$ cannot be obtained. To this end, the activity detection problem can be formulated by minimizing the following approximated negative log-likelihood function $-\log p\left(\mathbf{Y} ;\left\{a_k\right\}_{k\in\mathcal{K}_1\cup\mathcal{K}_2\cup\mathcal{K}_3}\right)\approx \sum_{m=1}^M-\log p\left(\mathbf{y}_m ; \left\{a_k\right\}_{k\in\mathcal{K}_1\cup\mathcal{K}_2\cup\mathcal{K}_3}\right)$, subject to the constraints on $\{{a}_{k}\}_{k\in\mathcal{K}_1\cup\mathcal{K}_2\cup\mathcal{K}_3}$, i.e., 
\begin{subequations} \label{p}
\begin{align} 
& \min _{\{a_k\}_{k}}\sum_{m=1}^M\log \left|\boldsymbol{\Sigma}_m\right|+\operatorname{tr}\left(\boldsymbol{\Sigma}_m^{-1}\left(\mathbf{y}_m-\bar{\mathbf{y}}_m\right)\left(\mathbf{y}_m-\bar{\mathbf{y}}_m\right)^H\right) \label{obj} \\
& ~~\text{s.t.}~~~~ {a}_{k} \geq 0, ~~~\forall k\in\mathcal{K}_1\cup\mathcal{K}_2\cup\mathcal{K}_3. \label{p1_constraint}
\end{align}
\end{subequations}


\section{The Proposed Mixture of Experts-augmented Deep Unfolding Design}   \label{autoencoder}

In general, rank-one update is a standard way to solve for activity status in the covariance method~\cite{C_ICC_Czl_ad, TWC_Lin2022}. However, since the objective function~\eqref{obj} involves the mean $ \bar{\mathbf{y}}_m$ and the covariance matrix $\boldsymbol{\Sigma}_m$, and they both depend on the activity status $a_k$, rank-one update for estimating $a_k$ is not applicable anymore. Thus, we introduce a projected gradient descent (PGD) approach to solve~\eqref{p}. Furthermore, to rectify the approximation in the likelihood function~\eqref{obj}, the PGD method is unfolded to a neural network so that model inaccuracy could be compensated in a data driven manner. 

\subsection{Projected Gradient Dscent Approach}   \label{pgd_fixed }
Noticing that~\eqref{obj} is differentiable with respect to $\{a_k\}_{k\in\mathcal{K}_1\cup\mathcal{K}_2\cup\mathcal{K}_3}$ and the constraint \eqref{p1_constraint} is simple, the PGD approach can provide efficient optimization. In particular, at the $i$-th iteration, the update procedure is written as
\begin{align} \label{eq4}
\hat{a}_{k}^{(i)}=\max \left\{\hat{a}_{k}^{(i-1)}-\eta^{(i)} d_{k}^{(i-1)}, 0\right\},
\end{align}
where $\hat{a}_{k}^{(i)}$ is the estimate of the $k$-th device's activity at the $i$-th iteration, $\eta^{(i)}$ denotes the stepsize, and $d_{k}^{(i-1)}$ is the gradient of \eqref{obj} at $\hat{a}_{k}^{(i-1)}$, which is given by
\begin{equation}\label{grad}
    d_{k}^{(i-1)}=\sum_{m=1}^M d_{k,m}^{(i-1)}
\end{equation}
where $d_{k,m}^{(i-1)}$ is given on the top of this page. Algorithm~\ref{a1} summarizes the PGD approach.
% \begin{eqnarray} \label{gradient}
% d_{k,m}^{(i-1)} &=& \operatorname{tr}\left(\xi_m \mathbf{s}_{k} \mathbf{s}_{k}^H\left({\boldsymbol{\Sigma}}_m^{(i-1)} \right)^{-1}\right) - \operatorname{tr}\left(\xi_m \mathbf{s}_{k} \mathbf{s}_{k}^H\left({\boldsymbol{\Sigma}_m}^{(i-1)} \right)^{-1}\mathbf{y}_m\mathbf{y}_m^H\left({\boldsymbol{\Sigma}_m}^{(i-1)} \right)^{-1}\right) \nonumber \\
% &&- \operatorname{tr}\left(\xi_m \mathbf{s}_{k} \mathbf{s}_{k}^H\left({\boldsymbol{\Sigma}_m}^{(i-1)} \right)^{-1}\bar{\mathbf{y}}_m^{(i-1)}\left(\bar{\mathbf{y}}_m^{(i-1)}\right)^H\left({\boldsymbol{\Sigma}_m}^{(i-1)} \right)^{-1}\right) \nonumber \\
% &&+ \operatorname{tr}\left(\mathbf{s}_{k}   (\mathbf{h}_k^{\text{LoS}})^H \mathbf{\Theta} \mathbf{g}^{\text{LoS}}_m\left(\bar{\mathbf{y}}_m^{(i-1)}\right)^H\left({\boldsymbol{\Sigma}}^{(i-1)}_m \right)^{-1}\right) \nonumber \\
% &&+ \operatorname{tr}\left(\left(\bar{\mathbf{y}}_m^{(i-1)}\right)\left(\mathbf{s}_{k}   (\mathbf{h}_k^{\text{LoS}})^H \mathbf{\Theta} \mathbf{g}^{\text{LoS}}_m\right)^H\left({\boldsymbol{\Sigma}}^{(i-1)}_m \right)^{-1}\right) \nonumber \\
% &&- \operatorname{tr}\left(\mathbf{s}_{k}   (\mathbf{h}_k^{\text{LoS}})^H \mathbf{\Theta} \mathbf{g}_m^{\text{LoS}}\mathbf{y}_m^H\left({\boldsymbol{\Sigma}}_m^{(i-1)} \right)^{-1}\right) \nonumber \\
% &&-\operatorname{tr}\left(\mathbf{y}_m\left(\mathbf{s}_{k}   (\mathbf{h}_k^{\text{LoS}})^H \mathbf{\Theta} \mathbf{g}_m^{\text{LoS}}\right)^{H}\left({\boldsymbol{\Sigma}}_m^{(i-1)} \right)^{-1}\right) \nonumber \\
% &&+\operatorname{tr}\left(\xi_m \mathbf{s}_{k} \mathbf{s}_{k}^H\left({\boldsymbol{\Sigma}_m}^{(i-1)} \right)^{-1}\mathbf{y}_m\left(\bar{\mathbf{y}}_m^{(i-1)}\right)^H\left({\boldsymbol{\Sigma}_m}^{(i-1)} \right)^{-1}\right) \nonumber \\
% &&+\operatorname{tr}\left(\xi_m \mathbf{s}_{k} \mathbf{s}_{k}^H\left({\boldsymbol{\Sigma}_m}^{(i-1)} \right)^{-1}\bar{\mathbf{y}}_m^{(i-1)}\mathbf{y}_m^H\left({\boldsymbol{\Sigma}_m}^{(i-1)} \right)^{-1}\right) 
% \end{eqnarray}
\begin{figure*}
\begin{align} 
d_{k,m}^{(i-1)} &= \operatorname{tr}\left(\Xi_{m,k} \mathbf{s}_{k} \mathbf{s}_{k}^H\left({\boldsymbol{\Sigma}}_m^{(i-1)} \right)^{-1}\right) - \operatorname{tr}\left(\Xi_{m,k} \mathbf{s}_{k} \mathbf{s}_{k}^H\left({\boldsymbol{\Sigma}}_m^{(i-1)} \right)^{-1}\left(\mathbf{y}-\bar{\mathbf{y}}_m^{(i-1)}\right)\left(\mathbf{y}_m-\bar{\mathbf{y}}_m^{(i-1)}\right)^H\left({\boldsymbol{\Sigma}}_m^{(i-1)} \right)^{-1}\right) \nonumber \\
&- \operatorname{tr}\left(\mathbf{s}_{k}   (\mathbf{h}_k^{\text{LoS}})^H \mathbf{\Theta} \mathbf{g}_m^{\text{LoS}}\left(\mathbf{y}_m-\bar{\mathbf{y}}_m^{(i-1)}\right)^H\left({\boldsymbol{\Sigma}}_m^{(i-1)} \right)^{-1}\right) - \operatorname{tr}\left(\left(\mathbf{y}_m-\bar{\mathbf{y}}_m^{(i-1)}\right)\left(\mathbf{s}_{k}   (\mathbf{h}_k^{\text{LoS}})^H \mathbf{\Theta} \mathbf{g}_m^{\text{LoS}}\right)^H\left({\boldsymbol{\Sigma}}_m^{(i-1)} \right)^{-1}\right) \nonumber
\end{align}
\hrulefill
\end{figure*}


% and $k = 1, \dots, K_1$. For $k = {K_1}+1, \dots, K_3$, $d_{k,m}$ can be easily obtained by utilizing the definition of $\Xi$. Due to the page limitation, the specific mathematical formulations will not be presented here.

\begin{algorithm}[t] 
\begin{algorithmic}[1]
%\small
\caption{PGD} 
\label{a1}
\STATE \textbf{Input} The received signal ${\mathbf{Y}}$, the signature matrix ${\mathbf{S}}$, phase shift $\mathbf{\Theta}$,  and the noise power $\sigma_{{w}}^2$.
\STATE \textbf{Initialize}  $\left\{\hat{a}_{k}^{(0)} = 0\right\}_{k\in\mathcal{K}_1\cup\mathcal{K}_2\cup\mathcal{K}_3}$.
\STATE \textbf{repeat} ($i = 1, 2, ... $)
\STATE ~~~~Calculate $\boldsymbol{\Sigma}_m^{(i-1)}$ by~\eqref{xxx}.
\STATE ~~~~Calculate the gradient $d_{k}^{(i-1)}$ at $\hat{a}_{k}^{(i-1)}$ by performing $d_{k}^{(i-1)}=\sum_{m=1}^M d_{k,m}^{(i-1)}$.
\STATE ~~~~Calculate $\hat{a}_{k}^{(i)}=\max \left\{ \hat{a}_{k}^{(i-1)}-\eta^{(i)} d_{k}^{(i-1)}, 0\right\},~\forall k\in\mathcal{K}_1\cup\mathcal{K}_2\cup\mathcal{K}_3$.
\STATE \textbf{until} Convergence
\STATE \textbf{Output} $\left\{\hat{a}^{(I)}_{k}\right\}_{k\in\mathcal{K}_1\cup\mathcal{K}_2\cup\mathcal{K}_3}$ where $I$ is the last iteration index.
\end{algorithmic}  
\end{algorithm}

\subsection{The Deep Unfolded Network}   \label{phase }

Applying deep unfolding to the PGD approach, each iteration of Algorithm~\ref{a1} is regarded as one layer of the neural network. The key in constructing a deep unfolded network is to introduce trainable parameters in each layer of the neural network. In Algorithm~\ref{a1}, since the choice of $\eta^{(i)}$ influences the convergence rate and detection performance, we regard it as a trainable parameter. Furthermore, to avoid the matrix inverse in~line~5 of Algorithm~1 and increase the learning capability, we design a linear layer $\mathbf{A}^{(i)}\boldsymbol{\Sigma}_m^{(i-1)} + \mathbf{B}^{(i)}$ with trainable parameters $\mathbf{A}^{(i)}$ and $\mathbf{B}^{(i)}$ to approximate $\left(\boldsymbol{\Sigma}_m^{(i-1)}\right)^{-1}$. 

The unrolled network consists of $I$ cascaded layers with trainable parameters $\left\{ \mathbf{A}^{(i)},\mathbf{B}^{(i)}, \eta^{(i)}\right\}_{i=1}^{I}$ \textcolor{black}{as shown in~Fig.~\ref{Fig11}}.
Specifically, for the $i$-th layer, the input is $\left(\mathbf{Y}, \mathbf{S}, \left\{\hat{a}_{k}^{(i-1)}\right\}_{k\in\mathcal{K}_1\cup\mathcal{K}_2\cup\mathcal{K}_3} \right)$ and the output is $\left\{\hat{a}_{k}^{(i)}\right\}_{k\in\mathcal{K}_1\cup\mathcal{K}_2\cup\mathcal{K}_3}$.


\subsection{Mixture of Experts Design}
\label{MoE}

Both the PGD and its deep unfolded version require the computation of $\boldsymbol{\Sigma}_m^{(i-1)}$ in~\eqref{xxx} with the current activity status $\hat{a}_k^{(i-1)}$. However, calculating $\boldsymbol{\Sigma}_m^{(i-1)}$ requires the precise knowledge of which devices belong to which group: $\mathcal{K}_1$, $\mathcal{K}_2$, or $\mathcal{K}_3$. This information is difficult to acquire in practice. One naive way to handle this is to assume all the devices belong to only one of the groups. This corresponds to the covariance matrix being%which users possess direct links and which users are required to go through the IRS. This prerequisite represents a condition that is not feasible in practice. To address this challenge and enhance the flexibility of the network, we propose a MoE design. We introduce three kinds of expert as follow: 
\begin{align}
\begin{split}
  \boldsymbol{\Sigma}_m^{\text{expert}_1} &= \sum_{k\in\mathcal{K}_1\cup\mathcal{K}_2\cup\mathcal{K}_3} {a_k} \Xi_{m,k} \mathbf{s}_{k} \mathbf{s}_{k}^H+\sigma_w^2 \mathbf{I}_M,\\
  \boldsymbol{\Sigma}_m^{\text{expert}_2} &= \sum_{k\in\mathcal{K}_1\cup\mathcal{K}_2\cup\mathcal{K}_3} \frac{a_k\mathbf{s}_{k} \mathbf{s}_{k}^H}{1+\kappa_{\text{U}}}+\sigma_w^2 \mathbf{I}_M,\\
  \boldsymbol{\Sigma}_m^{\text{expert}_3} &= \sum_{k\in\mathcal{K}_1\cup\mathcal{K}_2\cup\mathcal{K}_3} {a_k}  \mathbf{s}_{k} \mathbf{s}_{k}^H+\sigma_w^2 \mathbf{I}_M. \nonumber
\end{split}
\end{align}

However, how do we know which covariance matrix we should use in practice? To address this challenge, we propose a MoE design. In particular, one of the major challenges in constructing a MoE architecture lies in designing the gate network to process input information and assigning the most appropriate expert for subsequent computation. To this end, we propose a MLP as the gate network. Upon feeding the received signal matrix ${\mathbf{Y}}$ into the gate network, it outputs the proportions of three types of devices. By detecting the device type with the highest proportion, we select the corresponding expert to compute the covariance matrix. The specific structure of the MoE system and its integration with the unfolded network are illustrated in Fig.~\ref{Fig11}. By introducing this MoE-module,  it is not necessary to acquire the channel type information between each device and the BS anymore.
\begin{figure} [t]
	\centering 
		 \includegraphics[width=0.48\textwidth]{Figs/unfolding_moe_3.png}  
	
  	\caption{Architecture of the unfolded network, with details on the feed forward process of the i-th layer and the MoE system. }  \label{Fig11}
\end{figure}



\section{Simulation Results} \label{sim}

In this section, we compare the performance of the proposed MoE-augmented deep unfolding approach, traditional coordinate descent (CD) algorithm (assume all channels are zero-mean Gaussian), the derived PGD algorithm and a transformer-based detector~\cite{LY_transformer}. After obtaining the estimate $\hat{a}_{k}$ for each device, the estimate indicator $\hat{b}_{k}$ is recovered from
\begin{equation}
\hat{b}_{k}= \begin{cases}1, & \text {if} \quad \hat{a}_{k} \geq a^{t h}, \\ 0, & \text {else},\end{cases}
\end{equation}
where $a^{th}$ is a threshold that controls the trade-off between the probabilities of false alarm (PF) and missed detection (PM), which are defined as~\cite{C_ICC_Czl_ad}:
\begin{equation}
\mathrm{PM}=1-\frac{\sum_{k=1}^K {b}_{k} \hat{b}_{k}}{\sum_{k=1}^K{b}_{k}}, ~~\mathrm{PF}=\frac{\sum_{k=1}^K \hat{b}_{k}\left(1-{b}_{k}\right)}{\sum_{k=1}^K\left(1-{b}_{k}\right)}.\nonumber
\end{equation}




\subsection{Simulation Setting and Training Procedure}

Under a three dimensional Cartesian coordinate system, we set the locations of the BS and the IRS at $(0,0,10)$ and $(5,50,10)$ in meter, respectively. In addition, devices from $\mathcal{K}_1$ are randomly and uniformly located in a circular area of radius $40$ m around the center $(200,0,0)$. And the devices in $\mathcal{K}_2$ and $\mathcal{K}_3$ are located in the left half circular area with radius $40$m around the center $(0,120,0)$. The large-scale coefficient of each device $\beta_k$ is generated according to $\beta_k = -60 - 22\log_{10}(d_{k}d_{0})$ in dB, where $d_{k}$ denotes the distance between device~$k$ and the IRS, and $d_{0}$ denotes the distance between the IRS and the BS. The transmit power of each device is set as $p_k = 23$ dBm. The BS is equiped with $32$ antennas and the IRS is with $40$ reflecting elements. The number of potential devices is $K = 100$, with the activity probability being 0.2. The length of signature sequence $s_k$ is set at $L = 20$.
%Other simulation parameters are summarized in \textcolor{black}{Table.~\ref{table1}}.

%\begin{table}[] 
%\caption{The summary of simulation setting}
%\centering
%\begin{tabular}{ll} 
%\hline
%\textbf{Environment parameters}       & \textbf{Value} \\ \hline
%BS antennas $M$    & $32$    \\
%IRS elements $N$   &  $40$     \\
%Potential devices $K_3$  & $100$      \\
%Active ratio $\alpha$    &   $0.2$  \\
%The length of signature sequences $L$ &  $20$\\
%Transmit power $p_k$ & $23$~dBm\\
%Noise power $\sigma_{w}^{2}$ & -$95$~dBm \\ 
%Phase shifts $\mathbf{\Theta}$ & $\mathbf{I}_N$ \\\hline
%\end{tabular} \label{table1}
%\end{table}
%\textbf{Deep unfolding parameters} & \textbf{Value} \\ \hline
%Unfolded layer $I$         & $4$     \\
%Training epoch            &  $80$   \\
%Training sample & $1.28\times10^{6}$ \\
%Batch size & $128$ \\
%Learning rate & $3\times10^{-4}$ \\ 
%Optimizer & Adam \\ 
%Loss function & MSE \\ \hline

%For the training procedure of the deep unfolded networks, the training dataset composes of $1.28\times10^{6}$ training samples. 
To train an MLP-based gate network, the received signal matrix $\mathbf{Y} \in \mathbb{C}^{L \times M}$ is first preprocessed by concatenating real and imaginary components into a real-valued vector $\mathbf{y}_{\text{real}} = [\text{Re}(\text{vec}(\mathbf{Y})); \text{Im}(\text{vec}(\mathbf{Y}))] \in \mathbb{R}^{2LM}$. The MLP architecture consists of three fully-connected layers: an input layer ($2LM \to 512$) with ReLU activation, a hidden layer ($512 \to 128$) with tanh activation, and an output layer ($128 \to 3$) with softmax normalization to produce $\rho_1$, $\rho_2$, $\rho_3$ as the estimate of user group proportions. The training dataset composes of $1.28\times10^{6}$ training samples %and the ground-truth proportions are calculated as:
%\begin{equation}
    %\rho_i^{\text{true}} = \frac{|\mathcal{K}_i|}{|\mathcal{K}_1|+|\mathcal{K}_2|+|\mathcal{K}_3|}, \quad i=1,2,3.
%\end{equation}%Training data is generated through Monte Carlo simulations where active users in $\mathcal{K}_1$, $\mathcal{K}_2$, and $\mathcal{K}_3$ are randomly selected with activity indicators $\{a_k\}$, and corresponding $\mathbf{Y}$ matrices are computed using the channel model $\mathbf{Y} = \bar{\mathbf{Y}} + \mathbf{W}$ from Proposition 1. 
and the gate network is trained using KL divergence loss:
\begin{equation}
    \mathcal{L} = \sum_{i=1}^3 \rho_i^{\text{true}} \log\left(\frac{\rho_i^{\text{true}}}{\rho_i}\right),
\end{equation}
where 
\begin{equation}
    \rho_i^{\text{true}} = \frac{|\mathcal{K}_i|}{\sum_{i=1}^3|\mathcal{K}_i|}, \quad i=1,2,3.
\end{equation}
 %where each iteration involves: (1) forward propagation through the MLP layers, (2) loss computation comparing predicted proportions $\mathbf{p}^{\text{pred}}$ to simulated ground truth, (3) backpropagation to update weights, and (4) validation using holdout data to prevent overfitting. 
For the deep unfolding network, we use incremental training~\cite{twc_LIN}, and it is found that the network with more than 4 layers only leads to a small performance improvement. Therefore, we choose $I=4$. %And to determine the number of unfolded layer $I$, we adopt the following incremental training strategy~\cite{twc_LIN}. More precisely, we update the trainable parameters $\left\{ \mathbf{A}^{(l)},\mathbf{B}^{(l)}, \eta^{(l)}\right\}_{l=1}^{i}$ with a standard mini-batch stochastic gradient descent step.
%\begin{align}
   % \min_{\{A^{(l)}, B^{(l)}, \eta^{(l)}\}_{l = 1}^{i}} - \mathbb{E} \left[ \frac{1}{K_3} \sum_{k = 1}^{K_3} (\hat{a}_{k}^{(i)} - a_{k}^{(0)})^2 \right],
%\end{align}
%And in the $i+1$-th round, the trainable parameters $\left\{ \mathbf{A}^{(l)},\mathbf{B}^{(l)}, \eta^{(l)}\right\}_{l=1}^{i+1}$ will be changed to
%\begin{align}
    %\min_{\{A^{(l)}, B^{(l)}, \eta^{(l)}\}_{l = 1}^{i+1}} - \mathbb{E} \left[ \frac{1}{K_3} \sum_{k = 1}^{K_3} (\hat{a}_{k}^{(i+1)} - a_{k}^{(0)})^2 \right].
%\end{align}
%In simple terms, we attach an additional layer for $i+1$-th round of incremental learning, which alters the objective function while the adopted values of trainable parameters from the previous round serve as the initial values for the subsequent roundâ€™s training. 
%The training process is stopped when the change of the mean square error on the validation set is smaller than $10^{-4}$. According to this training procedure, we determine to set the unfolded layers $I$ at 4 and batch size at 128.

\subsection{Simulation Results} 
First, we present the PM-PF curves of various methods under the setting of: $\kappa_\text{U}$ = $\kappa_\text{R}$ = $\kappa_\text{B}$ = 10 dB, $\sigma_w^2$ = -95 dBm. Among 100 devices, 40\% of them are from $\mathcal{K}_1$ and the proportions of devices in $\mathcal{K}_2$ and $\mathcal{K}_3$ are both 30\%. In Fig.~\ref{fig:moe}, it can be observed that the proposed deep unfolding design significantly outperforms both the optimization-based PGD and CD methods as the deep unfolded network could compensate the inaccurate modeling. Furthermore, the deep unfolding method is superior than the transformer based detector, since deep unfolding utilizes the mathematical principles as implicit training guidelines, making it easier to achieve better detection performance. While the original deep unfolding method still needs to obtain the specific channel type information between devices and the BS, the MoE-aided deep unfolding network, which does not need the knowledge of which device belongs to which group, only suffers from a small degradation compared to the deep unfolded network with perfect information. In contrast, if no such information is available for the PGD algorithm, it has a significant degradation compared to the PGD with perfect information.  

\begin{figure} [t]
	\centering
 	{\label{fig:moe} %% label for 2 subfigure
		\includegraphics[width=0.4\textwidth]{Figs/fig1_pmpf_v1.png}}
	  
  	\caption{Performance comparison in terms of PM and PF.}  \label{fig:moe}
\end{figure}

Next, Fig.~\ref{fig:2} illustrates the flexibility and generalization ability of the proposed MoE-augmented deep unfolding approach. We compared the error rates of various methods with the proportion of devices in $\mathcal{K}_1$ varies from 0\% to 80\%. The number of remaining devices is equally divided between $\mathcal{K}_2$ and $\mathcal{K}_3$. We can observe that the deep unfolding network that uses a single fixed expert only performs well if the network setting matches the expert's prior knowledge. However, with the MoE system enabled, the gate network can learn which type of device has the highest proportion and select the best expert accordingly. This property leads to the MoE-based deep unfolding network performing close to the deep unfolding network with perfect devices' grouping information.

\begin{figure} [t]
	\centering
 	{\label{fig:1} %% label for 2 subfigure
		\includegraphics[width=0.4\textwidth]{Figs/fig2_error_v2.png}}
	  
  	\caption{PF = PM versus percentage of devices in $\mathcal{K}_1$.}  \label{fig:2}
\end{figure}

\section{Conclusions}

In this paper, we proposed a model-driven deep learning approach for activity detection in IRS-aided systems with mixed types of channel fading. Specifically, we derived an approximated covariance-based formulation and introduced a deep unfolding network based on the projected gradient method. By further incorporating a MoE network, the proposed method does not require prior knowledge of which device belongs to which channel type. Simulation results showed that the MoE-augmented model-driven deep unfolding method achieves better detection performance than traditional covariance-based methods and black-box neural network designs. Furthermore, the performance loss from not knowing the fading channel type of each device is minimal compared to the unfolded network with perfect information.

% \newpage
\appendices
\section{Proof of Proposition 1}
Let $\mathbf{h}_k^{\text{NLoS}}\sim   \mathcal{CN} \left(\mathbf{0}, \mathbf{I}_{N}\right)$ and $\mathbf{g}_m^{\text{NLoS}}\sim   \mathcal{CN} \left(\mathbf{0}, \mathbf{I}_{N}\right)$ be the non-line-of-site component of $\mathbf{h}_k$ and $\mathbf{g}_m$, respectively. Based on~\cite[Proposition 1]{twc_LIN}, we have~$(\mathbf{h}_k^{\text{NLoS}})^H \mathbf{\Theta} \mathbf{g}_m^{\text{NLoS}} \sim   \mathcal{CN} \left(\mathbf{0}, N\mathbf{I}_{M}\right)$ when $N$ is sufficiently large. Therefore, $\mathbf{Y}$ is a complex Gaussian distributed matrix. The mean of $\mathbf{y}_m$ is given by
\begin{align}\label{r1_m}
    \begin{aligned}
    \mathbb{E}\left[ \mathbf{y}_m\right] &=\sum_{k\in\mathcal{K}_1} {a_k}\mathbf{s}_{k}(\sqrt{\frac{\kappa_{\text{U}}}{1+\kappa_{\text{U}}}} \mathbf{h}_k^{\text{LoS}})^H \mathbf{\Theta} \sqrt{\frac{\kappa_{\text{B}}}{1+\kappa_{\text{B}}}} \mathbf{g}_m^{\text{LoS}}\\
    &+\sum_{k\in\mathcal{K}_2}{a_k}\mathbf{s}_{k}   \sqrt{\frac{\kappa_{\text{U}}}{1+\kappa_{\text{U}}}} \mathbf{f}_{k}^{\text{LoS}}(m)^*\nonumber
    \end{aligned}
\end{align}
and
\begin{eqnarray} \label{r1_y}
&& \mathbb{E}\left[ (\mathbf{y}_m - \mathbb{E}\left[ \mathbf{y}_m\right])(\mathbf{y}_m - \mathbb{E}\left[ \mathbf{y}_m\right])^H\right]  \nonumber \\
% &=& \sum_{k=1}^{K} {a_k} \|\mathbf{g}_m^{\text{LoS}}\|^2\mathbf{s}_{k} \mathbf{s}_{k}^H  +  \sum_{k=1}^{K} {a_k} \|\mathbf{h}_k^{\text{LoS}}\|^2\mathbf{s}_{k} \mathbf{s}_{k}^H   +  \sum_{k=1}^{K} {a_k}\mathbf{s}_{k}   \mathbf{s}_k^H+\sigma_w^2 \mathbf{I}_M \nonumber \\
&=&\sum_{k\in\mathcal{K}_1} {a_k}\frac{\kappa_{\text{U}}\|{\mathbf{h}}_k^{\text{LoS}}\|^2}{(1+\kappa_{\text{B}})(1+\kappa_{\text{U}})}\mathbf{s}_{k} \mathbf{s}_{k}^H + \sum_{k\in\mathcal{K}_1} \frac{{a_k}N\mathbf{s}_{k} \mathbf{s}_{k}^H}{(1+\kappa_{\text{B}})(1+\kappa_{\text{U}})} \nonumber\\
&+& \sum_{k\in\mathcal{K}_1} {a_k}\frac{\kappa_{\text{B}}\|{\mathbf{g}_m}^{\text{LoS}}\|^2}{(1+\kappa_{\text{B}})(1+\kappa_{\text{U}})}\mathbf{s}_{k} \mathbf{s}_{k}^H
+\sum_{k\in\mathcal{K}_2} \frac{a_k\mathbf{s}_{k} \mathbf{s}_{k}^H}{1+\kappa_{\text{U}}} \nonumber\\
&+& \sum_{k\in\mathcal{K}_3} {a_k}  \mathbf{s}_{k} \mathbf{s}_{k}^H +\sigma_w^2 \mathbf{I}_M, \nonumber\\
&=&\!\sum_{k\in\mathcal{K}_1} {a_k} \Xi_{m,k} \mathbf{s}_{k} \mathbf{s}_{k}^H \!+\!\sum_{k\in\mathcal{K}_2} \frac{a_k\mathbf{s}_{k} \mathbf{s}_{k}^H}{1+\kappa_{\text{U}}}+\!\sum_{k\in\mathcal{K}_3} {a_k}  \mathbf{s}_{k} \mathbf{s}_{k}^H \nonumber\\
&+&\sigma_w^2 \mathbf{I}_M, \nonumber
\end{eqnarray}
where $\Xi_{m,k} =\frac{N +\kappa_{\text{B}}\|{\mathbf{g}}_m^{\text{LoS}}\|^2+\kappa_{\text{U}}\|{\mathbf{h}}_k^{\text{LoS}}\|^2}{(1+\kappa_{\text{B}})(1+\kappa_{\text{U}})}$. Once obtaining $\bar{\mathbf{y}}_m \triangleq \mathbb{E}\left[ \mathbf{y}_m \right]$ and $\mathbf{\Sigma}_m \triangleq \mathbb{E}\left[ (\mathbf{y}_m - \mathbb{E}\left[ \mathbf{y}_m\right]) \right.\left.(\mathbf{y}_m - \mathbb{E}\left[ \mathbf{y}_m\right])^H\right]$, we have \eqref{likelihood_z}.
\bibliographystyle{IEEEtran}
\bibliography{ref}
\vfill

\end{document}


