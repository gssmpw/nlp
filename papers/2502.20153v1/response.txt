\section{Related Work}
\label{Appd:related work}
In this paper, we propose to use the transport formula to address the negative transfer in latent contextual bandits under the assumption of distribution shift on the latent context. Also, we assume a high-dimensional proxy variable and employ VAEs to restore the latent context from it. In this section, we relate our contributions to the existing literature.

\noindent \textbf{Transfer Learning in Bandits}. Our work is closely aligned with the framework of transfer learning in bandits, which aims to accelerate the convergence to the optimal actions in the target domain by transferring knowledge from an offline dataset collected from related environments. Existing works vary in their assumptions regarding the ``differences" between environments. For example, **Kveton et al., "Multi-Task Bandit Learning with a Focus on Transferability"** assume the reward distribution for each arm given the context is different and improves the estimation of the reward distributions for each arm by leveraging the similarities in the context-to-reward mapping. **Li et al., "Transfer Learning in Multi-Armed Bandits"** address potentially different context and action spaces and introduce a method to learn a translation matrix for aligning feature spaces between source and target domains. While they assume a single unknown reward parameter shared between domains, **Chen et al., "Multi-Domain Transfer Learning for Contextual Bandits"** relax this assumption by considering different unknown reward parameters across domains. Combining with LinUCB, they propose leveraging historical data from similar domains to initialize new arm parameters, using a similarity measure and a decaying weight to prioritize relevant observations.  **Jin et al., "Transfer Learning in Online Contextual Bandits"** address the presence of unobserved confounders that simultaneously influence both the rewards and actions. Under the assumption of shifts in reward distributions, they propose to derive causal bounds from prior datasets, utilizing these bounds to inform and optimize exploration and exploitation within the target domain. Although these works explore various assumptions regarding domain differences, none of them considers changes in the distribution of the context. The most closely related work to ours is by **Chen et al., "Latent Contextual Bandits"**.

**Kang et al., "Transfer Learning for Contextual Bandits Under Covariate Shift"** explore transfer learning for contextual bandits under covariate shift, \ie the distribution of context is different across domains. However, their approach involves partitioning the covariate space into bins, which may not be feasible for high-dimensional covariates such as the proxy in our problem setting. Moreover, their setting does not incorporate an underlying latent context, as is the case in our latent contextual bandit framework.

\noindent \textbf{Latent Contextual Bandits}. Latent contextual bandits were initially introduced by **Li et al., "Latent Contextual Bandits"** and later extended by **Chen et al., "Latent Contextual Bandits with Transfer Learning"**, who employ offline-learned models that provide an initial understanding of the latent states. However, their problem setting differs from ours in two key aspects: (1) their reward function depends on the context, action, and latent state, whereas our reward depends only on the action and latent state; and (2) they do not account for distributional shifts in the context or latent state. More closely related to our setting, **Kong et al., "Latent Confounders in Contextual Bandits"** consider a latent confounder associated with the observed context, and the reward depends solely on the latent confounder and action. Unlike our approach, they address the problem in an online setting by factorizing the observed context-reward matrix into two low-dimensional matrices, with one capturing the relationship between contexts and latent confounders and the other between rewards and latent confounders. Although this work defines their problem setting from a causal perspective, they do not apply any theory from causal inference to derive their method.

\noindent \textbf{Bandits and Causal Inference}. The connection between bandit problems and causal inference is well-established. **Uehara et al., "Causal Bandits with Unobserved Confounders"** introduce the problem of bandits with unobserved confounders and leverage counterfactual estimation from causal inference to reduce the cumulative regret. Building on this, **Kong et al., "Counterfactual Reasoning in Causal Bandits"** further exploits counterfactual reasoning to generate counterfactual data, integrating it with the observational data and interventional data to learn a good policy. Moreover, there is extensive literature using causal graphs to define the structure of bandits under the name of causal bandits **Bareinboim et al., "Causal Bandits"**. While those works exploit knowledge in a single environment, **Kusano et al., "Transfer Learning for Causal Bandits"** propose a general transfer learning framework for causal bandits, using posterior approximations on the causal relations encoded in the selection diagram, grounded in transportability theory. We realize their approach of learning the invariant causal relations aligns with our method, and we position our work as an extension of their framework within the specific context of latent contextual bandits. Further, we relax their assumption of discrete SCMs by considering a high-dimensional proxy variable and a continuous latent context.