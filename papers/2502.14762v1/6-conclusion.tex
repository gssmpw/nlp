\section{Conclusions}
In this paper, we first introduce a new parameter efficient fine tuning module LuCA that enhances acquired knowledge by combining an adapter with a calibrator, thereby adapting to new information with well-refined features. Second, we propose a novel PTM-based CIL approach TOSCA which employs a single, sparse LuCA module that operates solely on the final \texttt{[CLS]} token before the classifier, enabling efficient and orthogonal task adaptation. Our approach consistently outperforms state-of-the-art methods by effectively navigating the stability-plasticity trade-off, while introducing significantly fewer parameters and overhead costs.

\textbf{Limitations and future works.}
Possible limitations include the utilization of pre-trained models since it highly relies on the generalization strength of the pre-trained models by adapting thorough a single token only.
In future work, we aim to explore further application scenarios, such as few-shot class-incremental learning to further enhance its versatility and impact.

\section*{Broader Impact}
This paper presents a work whose goal is to advance the field of machine learning, especially on the subject of exemplar-free class-incremental learning. Besides the advancements in the field, it eliminates the need to store data by introducing lightweight single trainable module, thereby diminishing privacy, memory, computation, and scalability concerns.

\section*{Acknowledgements}
This work is supported by TAILOR, a project funded by the EU Horizon programme under GA No. 952215; SYNERGIES, a project funded by the EU Horizon programme under GA No. 101146542; and Dutch e-infrastructure with the support of SURF Cooperative using GA no. EINF-10242.
