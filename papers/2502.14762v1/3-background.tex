\section{Background}
In this section, we first formally introduce the preliminaries of class-incremental learning and how pre-trained models are utilized to incrementally learn. We then present the overview of existing approaches using pre-trained models and their limitations.

\subsection{Class-Incremental Learning (CIL)}
CIL is a learning scenario where a model continually learns to classify new classes to build a unified classifier~\cite{icarl}.
Formally, we train models sequentially on a series of datasets $\{D^1, D^2, ... , D^B\}$ where $D^b = \{(x_i, y_i)\}_{i=1}^{n_b}$ is the $b$-th training set with $n_b$ instances.
Within this setting, each training instance $\mathbf{x}_i \in \mathbb{R}^D$ is associated with a class $y_i \in Y_b$. Here, $Y_b$ defines the set of labels for dataset $b$, and it is ensured that $Y_b \cap Y_{b'} = \emptyset$ for any $b \neq b'$, i.e. non-overlapping classes for different datasets. During the $b$-th training stage, the model is updated using data exclusively from $D_b$.

From the model perspective, following typical PTM-based CIL works~\cite{l2p, dualprompt, codaprompt, simplecil_aper, ease, mos}, we assume that a PTM is available for the initialization of the model $f(\mathbf{x})$ which we define with two components: $f(\mathbf{x}) = W^\top \phi(\mathbf{x})$, where $\phi(\cdot) : \mathbb{R}^D \to \mathbb{R}^d$ is the feature extractor and $W \in \mathbb{R}^{d \times |\mathcal{Y}_b|}$ is the classifier.
For a standard ViT~\cite{vit}, the initial encoding layer converts the image into a sequence of output features, denoted as $\mathbf{x}_e \in \mathbb{R}^{L \times d}$, where $L$ is the sequence length. We simplify this by assuming \texttt{[CLS]} token is already prepended in $\mathbf{x}_e$ as the first token. The sequence $\mathbf{x}_e$ is then processed through subsequent layers, including multi-head self-attention and MLP, to produce the final embeddings. Finally, the embedded \texttt{[CLS]} token is considered as $\phi(\mathbf{x})$.

The effectiveness of the model is evaluated across all encountered classes, collectively represented as $\mathcal{Y}_b = Y_1 \cup Y_2 \cup \cdots Y_b$, after each learning stage.
Specifically, we aim to find a model $f(\mathbf{x}) : X \rightarrow \mathcal{Y}_b$ that minimizes empirical risk across all test dataset \textit{without task indices} by balancing between learning new classes and retaining information about old ones in the \textit{exemplar-free setting}~\cite{l2p, dualprompt, codaprompt, simplecil_aper, ease, mos}.

\subsection{Overview of PTM-Based CIL}
In the era of PTMs, the main idea of many works seeks to modify the pre-trained weights slightly, to maintain the generalization strength and we can mainly divide these approaches into three.

\paragraph{Learning Prototypical Classifiers.}
These methods \cite{nmc, simplecil_aper} focus on learning a set of prototypical class representations, typically by computing class centroids or prototypes from the features of incremental classes.
Given an input instance $\mathbf{x}$ with label $y \in \mathcal{Y}_b$, let $\phi(\mathbf{x})$ be its feature vector extracted by a pre-trained backbone. The class prototype $\mathbf{p}_y$ is defined as in \cref{eq:prototype} and instances are classified by measuring their distance to these prototypes in the feature space.
It is an efficient solution for simple class-incremental learning tasks by training only a classifier.

\begin{equation}
\mathbf{p}_y = \frac{1}{n_b} \sum_{i=1}^{n_b} \phi(\mathbf{x}_i)
\label{eq:prototype}
\end{equation}

However, these methods tend to rely too heavily on pre-trained knowledge and often fail to sufficiently adapt to new classes. This limits their effectiveness in more complex learning scenarios requiring feature-space reorganization.

\paragraph{Learning Prompts.}
This body of works \cite{l2p, dualprompt, codaprompt} construct and train a learnable pool of prompts that can be shared across all tasks to influence the self-attention process either from the input layer alone or across all layers. 
This prompt pool with a size of $M$ is denoted as $\mathcal{P} = \{P_1, P_2, \cdots, P_M\}$, where $P_j \in \mathbb{R}^{L_p \times d}$ represents a single prompt with token length $L_p$ and the same embedding size $d$ as image patch embedding~$\mathbf{x}_e$. Each prompt is paired with a trainable key vector $k_i \in \mathbb{R}^{d_k}$ encodes task-specific information while preserving the pre-trained backbone $\phi(\cdot)$, creating a set of key-prompt pairs $\{(k_1, P_1), (k_2, P_2), \cdots, (k_M, P_M)\}$.
The training objective jointly optimizes prompts, keys, and classifier through \cref{eq:prompt-training} where $\ell(\cdot,\cdot)$ is cross-entropy loss measuring the discrepancy between the prediction and ground truth, $\gamma(\cdot,\cdot)$ measures cosine similarity between keys and queries, and $\lambda$ balances task performance against prompt selection efficacy.

\begin{equation}
\min_{\mathcal{P}, \mathcal{K}, \phi} \ell(W^\top \phi(\mathbf{x}; \mathcal{P}), y) + \lambda \sum_{i=1}^N \gamma(\phi(\mathbf{x}), k_{s_i})
\label{eq:prompt-training}
\end{equation}

During inference, the model first extracts key features $\phi(\mathbf{x})$ from the frozen backbone without any prompts to solve the prompt retrieval objective given in \cref{eq:prompt-selection}, selecting the top-$N$ prompts relevant to the input. These prompts then condition the transformer's self-attention layers via concatenation with patch embeddings, yielding final predictions through the modified encoder $\phi(\mathbf{x}; \mathcal{P})$.

\begin{equation}
K_x^* = \arg\min_{\{s_i\}_{i=1}^N \subseteq [M]} \sum_{i=1}^N \gamma(\phi(\mathbf{x}), k_{s_i}),
\label{eq:prompt-selection}
\end{equation}

Although they present relatively efficient adaptations, selecting the correct prompt for a given task becomes challenging especially in long and complex scenarios, as the fixed key embedding space $\phi(\cdot)$ struggles to discriminate between semantically similar but task-distinct prompts, leading to retrieval conflicts when $\gamma(k_i, k_j) \approx 1$ for prompts $P_i$, $P_j$ from incompatible tasks, resulting in forgetting.

\paragraph{Learning Adapters.}
These approaches \cite{simplecil_aper, ease, mos} address catastrophic forgetting by inserting lightweight neural modules called adapters into the PTM's layers, enabling task-specific adaptations while preserving frozen base parameters. Each set of adapters $\mathcal{A}_b = \{A_1, A_2, ... A_N\}$ for task $b$ operates via residual connections on $N$ number of transformer layers, typically projecting features through a low-dimensional bottleneck given an intermediate feature $\mathbf{x}_i$ as in \cref{eq:adapter} where $\mathbf{z}$ is usually the output of MLP block of a transformer layer and ($r \ll d$) form the adapter's projection layers, and $\sigma$ denotes a non-linear activation. This residual formulation allows gradual feature adaptation without destabilizing the PTM's original representations.

\begin{equation}
    A(\mathbf{z}) = \sigma(\mathbf{z}W_{down})W_{up} + \mathbf{z}, \quad W_{down} \in \mathbb{R}^{d \times r}, \ W_{up} \in \mathbb{R}^{r \times d}
    \label{eq:adapter}
\end{equation}

Task-specific adapter sets then can be trained using either a feature concatenation strategy or a module merging strategy. Under the feature concatenation strategy, adapter sets are trained sequentially for each session and their outputs are concatenated with the PTM features at the cost of quadratic scaling or a linear increase in dimensionality. In contrast, the module merging strategy builds on previous adapter sets where each new set $\mathcal{A}_b$ refines the representation produced by the preceding set $\mathcal{A}_{b-1}$ to produce a gradual and unified feature representation. This is more parameter-efficient compared to the feature concatenation strategy but it risks accumulating feature drift over successive tasks, especially when new class distributions diverge significantly from those of earlier sessions.

%\begin{equation}
%\Phi(\mathbf{x}) = [\phi(\mathbf{x};\mathcal{A}_1), \dots, \phi(\mathbf{x};\mathcal{A}_B)] \in \mathbb{R}^{Bd}
%\label{eq:subspace-concat}
%\end{equation}

Although adapter-based methods offer an architectural advantage such as not being limited to input token spaces, they modify the pre-trained modelâ€™s feature representations via residual additions. These modifications introduce subtle yet cumulative deviations from the original pre-trained feature space, particularly more pronounced in deeper layers. the standard practice of inserting adapters into all $N$ transformer layers incurs substantial parameter overhead by requiring $(B \times N \times 2dr)$ additional parameters where $B$ denotes the number of tasks, $r$ is the bottleneck projection dimension and $d$ is the embedding size.
Consequently, while individual adapters remain lightweight, their pervasive placement across layers and tasks challenges overall parameter efficiency during both training and inference.