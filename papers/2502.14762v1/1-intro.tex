\section{Introduction} 
\label{sec:intro}
Learning continuously from a series of concepts or classes using a unified model is a challenging problem due to \textit{catastrophic forgetting} \cite{catastrophic}—a phenomenon where the model’s performance on earlier concepts degrades as new classes are observed. Class-incremental learning (CIL) addresses this issue by enabling models to acquire knowledge from new classes while preserving their ability to correctly classify previously learned categories~\cite{masana_survey}. Until recently, most CIL methods have focused on relatively small networks such as ResNets \cite{resnet} and often trained them from scratch with random initialization \cite{deepcil_survey, defyingforget_survey}.
With the rise of large Pre-Trained Models (PTMs) \cite{swin, convit, clip} such as Vision Transformers (ViTs) \cite{vit}, many CIL methods now capitalize on the robust representations provided by PTMs, marking a significant paradigm shift in the field \cite{sprompt, nmc, l2p, dualprompt, codaprompt, simplecil_aper, ease, mos}. Specifically, several PTM-based CIL approaches \cite{ptm_effect1, ptm_effect2, ptm_effect3} have demonstrated that strong initial representations, derived from large-scale pre-training, enhance incremental learning by enabling new classes to be learned with fewer training steps. However, sequential fine-tuning of the PTMs still changes their original representations and often leads to a substantial level of forgetting \cite{ranpac, film, slca}.

While parameter-efficient fine-tuning (PEFT) methods like prompts and adapters mitigate forgetting by restricting updates to small subsets of parameters, they introduce new trade-offs. Prompts maintain minimal adjustments on PTMs and offer great stability but often face challenges in adapting to specific tasks. In contrast, adapters provide localized feature refinement with high plasticity but this usually comes at the expense of quadratic parameter growth as the model depth increases.
This leads us to an essential question in PTM-based CIL:

\begin{center}
\colorbox{gray!15}{ % Change "gray!20" to any color you prefer
\parbox{0.97\textwidth}{
\centering
\vspace{2mm}
\textit{How can we address the stability-plasticity dilemma \cite{stability_plasticity} by unifying prompt-level stability \\ and adapter-level plasticity, while maintaining efficient parameter scalability?}
\vspace{2mm}
}
}
\end{center}

To address this question, we first propose a new PEFT module ‘Learn and Calibrate’, or LuCA, which comprises two components: (1) a residual adapter that applies task-specific feature transformations, and (2) a calibrator that enhances discriminative features via attention-like gating. 
We then propose a novel module placement strategy that integrates a single LuCA module, operating exclusively on the final \texttt{[CLS]} token representation of ViTs. We term this approach ‘Token-level Sparse Calibration and Adaptation’, or shortly TOSCA.
By localizing adaptations at the final semantic aggregation point, we preserve the PTM’s feature hierarchy: low/mid-level features remain frozen to maintain generalizability while final high-level abstractions adapt to a given task, akin to the harmony of ventral visual stream \cite{ventral, ventral1} and prefrontal cortex \cite{cortex, cortex1, cortex2}.

Specifically, each increment is residually acquired by a dedicated LuCA module sparsified by $\ell_1$-regularization. This promotes parameter orthogonality and enhances module specialization or distinctiveness across modules. Inference protocol leverages entropy minimization over task-specific predictions, as correct modules produce low-entropy class distributions. 
This approach eliminates the need for task identifiers or exemplar replay while achieving state-of-the-art performance without complicated procedures. 

\vspace{1mm}
Our contributions are three-fold:
\begin{enumerate}[label=\Roman*., leftmargin=*, align=left]
\item We introduce a PEFT module LuCA designed to learn task-specific residual transformations while refining features through additional calibration gating.

\item We then propose TOSCA, a neuro-inspired and theoretically grounded PTM-based CIL approach that strategically integrates our LuCA module with a simple design. It effectively balances prompt-level stability and adapter-level plasticity while maintaining a model-agnostic parameter count, unlike many prompt- and adapter-based methods that scale linearly with the number of layers.

\item We validate TOSCA's advantages with extensive experiments on six benchmarks: (i) 7–21\% higher accuracy than prompt-based methods and 4–12\% higher than adapter-based methods on out-of-distribution datasets, (ii) \textasciitilde{2.5}$\times$ faster runtime, and (iii) \textasciitilde{8}$\times$ fewer parameters than layer-wise adapters in ViT-B/16.
\end{enumerate}





