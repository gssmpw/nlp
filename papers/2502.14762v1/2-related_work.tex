\section{Related Work}
%In this section, we review recent advancements in class-incremental learning methods, focusing on both approaches using randomly initialized models and those leveraging pre-trained models to mitigate catastrophic forgetting and improve model performance.

\paragraph{CIL with Randomly Initialized Models.}
Not a long time ago, the primary focus in CIL was training deep neural networks sequentially from scratch, with the goal of efficiently acquiring the knowledge of new classes while minimizing forgetting of previous ones. Common CIL strategies can be categorized into four main approaches: Regularization-based methods \cite{ewc, lwf, mas, si} maintain the model by selectively stabilizing changes in parameters or predictions.
Replay-based methods approximate and reconstruct previously learned data distributions by either storing~\cite{icarl, gdumb, gem, bic, wa, rainbow, rmm, cls-er, esmer} or generating~\cite{decebal2016gen, shin2017continual, he2018exemplar, hu2019overcoming, fetril} samples from past experiences.
Architecture-based methods~\cite{expertgate, der, foster, par, memo} allocate distinct parameters and subspaces to different sets of classes. 
Parameter isolation methods utilize iterative pruning~\cite{packnet, clnp, supsup, cps} or dynamic sparse training~\cite{nispa, wsn, sparcl, softsubnet} to preserve key parameters.
These strategies have laid the foundation for advancing CIL methodologies.
\paragraph{CIL with Pre-trained Models.}
In contrast, recent advancements in CIL research have shifted towards leveraging PTMs. Representations derived from pre-training have proven effective not only in facilitating knowledge transfer but also in mitigating catastrophic forgetting during the downstream continual learning \cite{ptm_effect1, ptm_effect2}. Additionally, pre-training on a large set of base classes enables incremental learning with minimal adaptations \cite{ptm_effect3}. Therefore, methods in this context aim to improve performance with minimal additions while freezing the PTMs.
L2P \cite{l2p} borrows a technique from NLP by introducing a learnable prompt pool and selecting instance-specific prompts via a key-query matching selection mechanism to guide the PTMs response. 
DualPrompt \cite{dualprompt} extends L2P by designing G-Prompt and E-Prompt, which encode task-invariant and task-specific instructions. 


\begin{figure}[t]
\captionsetup{font=small}
  \centering
  \begin{subfigure}{0.463\textwidth}
    \includegraphics[width=\textwidth]{figs/tosca1.pdf}
    \caption{Prompt-Based}
    \label{fig:params}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.23\textwidth}
    \includegraphics[width=\textwidth]{figs/tosca2.pdf}
    \caption{Adapter-Based}
    \label{fig:abla_heat}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.2\textwidth}
    \includegraphics[width=\textwidth]{figs/tosca3.pdf}
    \caption{TOSCA (ours)}
    \label{fig:abla_line}
  \end{subfigure}
\caption{Prompt-based methods influence the self-attention process of a PTM, either from the input layer alone or across all layers. Adapter-based methods enable task-specific adaptations by inserting lightweight neural modules into the PTMâ€™s layers. In contrast, we propose a single trainable module that operates exclusively on the final \texttt{[CLS]} token representation, efficiently adapting and calibrating features just before classification. This design offers a streamlined and effective alternative to both prompt- and adapter-based methods.}
\end{figure}

CODA-Prompt \cite{codaprompt} uses contrastive learning to decorrelate representations of the prompts to reduce interference and combine them by attention-based weighting method.
APER \cite{simplecil_aper} explores various PEFT methods including adapters and shows that prototypical classifiers named SimpleCIL serve as a strong baseline. 
EASE \cite{ease} attaches adapters to each layer of PTMs to create expandable subspaces and during inference, it concatenates all feature representations from different sets of adapters to perform on a single classifier. 
MOS \cite{mos} adds replay generation for classifier alignment and an adapter merging over EASE to reduce mistakenly retrieving irrelevant modules during inference due to parameter drift. 