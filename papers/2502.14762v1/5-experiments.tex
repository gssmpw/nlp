
\section{Experiments}
In this section, we describe the experimental setup and present experiments conducted on six different benchmarks to evaluate the incremental learning capabilities of TOSCA, in comparison with other state-of-the-art algorithms.
Additionally, we share a parameter and run-time analysis as well as an ablation study to provide the robustness of the proposed method more in detail. 
Please see our \cref{Compared Methods and TOSCA} for more details.

\subsection{Experimental Setup}
\paragraph{Datasets.}
Since PTMs often exhibit substantial knowledge of upstream tasks, we adopt the evaluation framework proposed in~\cite{l2p, dualprompt, codaprompt, simplecil_aper, ease, mos} to assess their performance across a diverse set of benchmarks. These include CIFAR-100 \cite{cifar}, CUB-200 \cite{cub}, ImageNet-R \cite{imagenet-r} ImageNet-A \cite{imagenet-a}, OmniBenchmark \cite{omnibenchmark}, and VTAB \cite{vtab}. These datasets encompass both standard CIL benchmarks and out-of-distribution datasets which exhibit significant domain shifts relative to the dataset used for pre-training (e.g. ImageNet~\cite{imagenet}). Specifically, the datasets vary in scale: CIFAR-100 has 100 classes of natural images, each with 500 training images. CUB-200 dataset consists of images from 200 bird classes, with about 30 images per class for training. ImageNet-R includes 24000 for training images from 200 classes with abstract forms. ImageNet-A consists of 200 classes and 7500 training samples that are usually misclassified by ResNet models. Omnibenchmark with 300 classes and VTAB with 100 classes are designed to evaluate the generalization of visual representations.

\paragraph{Dataset Splits.}
Following~\cite{simplecil_aper, ease, mos}, we use the notation ‘B-$m$ Inc-$n$’ to represent the class splits, where $m$ specifies the number of classes in the initial stage and $n$ denotes the number of classes added at each incremental stage. Consistent with~\cite{icarl}, we shuffle the class order randomly using the seed $1993$ before splitting the data. To ensure a fair comparison, the training and testing sets for all methods are maintained the same.

\paragraph{Comparison Methods.}
We select state-of-the-art PTM-based CIL methods for comparison, including SimpleCIL~\cite{simplecil_aper}, L2P~\cite{l2p}, DualPrompt~\cite{dualprompt}, CODA-Prompt~\cite{codaprompt}, APER~\cite{simplecil_aper}, EASE~\cite{ease} and MOS~\cite{mos}. We also include one lower-bound and one upper-bound reference point: 'Finetune' sequentially fine-tunes the PTM; and 'joint' trains the model with all classes at the same time. All methods are implemented using the same PTM for consistency.

\paragraph{Evaluation Metrics.}
We compare the methods based on the accuracy over all stages obtained after last stage and the accuracy across all stages. Formally, building on~\cite{icarl}, we denote the Top-1 accuracy after the $b$-th stage as $\mathcal{A}_b$ and use $\mathcal{A}_B$ to represent the performance after the final stage. The average performance across all incremental stages then measured by $\bar{\mathcal{A}} = \frac{1}{B} \sum_{b=1}^B\mathcal{A}_b$.

\paragraph{Implementation Details.}
We conduct our experiments on an NVIDIA A100, and reproduce the compared methods using PyTorch~\cite{pytorch} and Pilot~\cite{pilot}. Consistent with~\cite{simplecil_aper, ease, mos}, we utilize two representative pre-trained models: ViT-B/16-IN21K and ViT-B/16-IN1K. Both models are pre-trained on ImageNet21K, with the latter further fine-tuned on ImageNet1K.
For TOSCA, we train the model using the SGD optimizer with a batch size of $48$ over $20$ epochs. The learning rate starts at $0.025$ and follows a cosine annealing schedule. The projection dimension $r$ is set to 48 and the $\ell_1$ contribution $\lambda$ is set to $0.0005$.  
We perform multiple runs with five different random seeds and report mean and standard deviation for each method.

\begin{table}[h]
\captionsetup{font=small}
\caption{Average and last accuracy [\%] performance on six datasets with \textbf{ViT-B/16-IN21K} as the backbone. ‘IN-R/A’ stands for ‘ImageNet-R/A,’ and ‘OmniBench’ stands for ‘OmniBenchmark.’ We report all compared methods with their source code and show the best performance in bold.}
\label{tab:in21k}
\fontsize{15}{25}\selectfont
%\setlength{\arrayrulewidth}{0.01pt}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lcccccccccccc}
\hline
\multirow{2}{*}{Method} &
  \multicolumn{2}{c}{CIFAR B0 Inc5} &
  \multicolumn{2}{c}{CUB B0 Inc10} &
  \multicolumn{2}{c}{IN-R B0 Inc20} &
  \multicolumn{2}{c}{IN-A B0 Inc20} &
  \multicolumn{2}{c}{OmniBench B0 Inc30} &
  \multicolumn{2}{c}{VTAB B0 Inc10} \\
 &
  \multicolumn{1}{c}{$\bar{\mathcal{A}}$} &
  \multicolumn{1}{c}{$\mathcal{A}_B$} &
  \multicolumn{1}{c}{$\bar{\mathcal{A}}$} &
  \multicolumn{1}{c}{$\mathcal{A}_B$} &
  \multicolumn{1}{c}{$\bar{\mathcal{A}}$} &
  \multicolumn{1}{c}{$\mathcal{A}_B$} &
  \multicolumn{1}{c}{$\bar{\mathcal{A}}$} &
  \multicolumn{1}{c}{$\mathcal{A}_B$} &
  \multicolumn{1}{c}{$\bar{\mathcal{A}}$} &
  \multicolumn{1}{c}{$\mathcal{A}_B$} &
  \multicolumn{1}{c}{$\bar{\mathcal{A}}$} &
  \multicolumn{1}{c}{$\mathcal{A}_B$} \\ \hline
Joint &
  $-$ &
  96.21 \footnotesize{± 1.0} &
  $-$ &
  92.62 \footnotesize{± 1.1} &
  $-$ &
  81.92 \footnotesize{± 1.4} &
  $-$ &
  67.97 \footnotesize{± 1.9} &
  $-$ &
  85.44 \footnotesize{± 1.2} &
  $-$ &
  94.96 \footnotesize{± 1.2} \\ \hline
Finetune &
  60.65 \footnotesize{± 5.6} &
  48.12 \footnotesize{± 3.3} &
  55.78 \footnotesize{± 2.8} &
  33.13 \footnotesize{± 3.3} &
  59.09 \footnotesize{± 3.7} &
  49.46 \footnotesize{± 3.3} &
  30.98 \footnotesize{± 3.4} &
  19.86 \footnotesize{± 1.8} &
  63.71 \footnotesize{± 1.0} &
  45.45 \footnotesize{± 1.0} &
  31.60 \footnotesize{± 6.0} &
  21.63 \footnotesize{± 8.3} \\
SimpleCIL &
  86.48 \footnotesize{± 0.8} &
  81.28 \footnotesize{± 0.1} &
  91.58 \footnotesize{± 1.3} &
  86.73 \footnotesize{± 0.1} &
  61.31 \footnotesize{± 0.4} &
  54.55 \footnotesize{± 0.1} &
  58.92 \footnotesize{± 1.0} &
  48.77 \footnotesize{± 0.1} &
  79.59 \footnotesize{± 1.5} &
  73.13 \footnotesize{± 0.1} &
  90.65 \footnotesize{± 1.1} &
  84.43 \footnotesize{± 0.1} \\
L2P &
  84.90 \footnotesize{± 1.2} &
  80.06 \footnotesize{± 1.4} &
  73.22 \footnotesize{± 1.8} &
  61.55 \footnotesize{± 1.7} &
  75.92 \footnotesize{± 0.7} &
  70.88 \footnotesize{± 0.7} &
  50.13 \footnotesize{± 1.8} &
  42.80 \footnotesize{± 1.1} &
  73.96 \footnotesize{± 2.0} &
  64.63 \footnotesize{± 0.6} &
  78.61 \footnotesize{± 4.2} &
  64.81 \footnotesize{± 2.9} \\
DualPrompt &
  85.61 \footnotesize{± 1.3} &
  79.92 \footnotesize{± 0.4} &
  81.36 \footnotesize{± 1.8} &
  70.51 \footnotesize{± 1.1} &
  71.48 \footnotesize{± 0.5} &
  66.09 \footnotesize{± 1.3} &
  51.57 \footnotesize{± 0.4} &
  40.56 \footnotesize{± 1.6} &
  75.58 \footnotesize{± 1.4} &
  66.46 \footnotesize{± 0.8} &
  86.86 \footnotesize{± 2.8} &
  75.86 \footnotesize{± 3.7} \\
CODA-Prompt &
  87.64 \footnotesize{± 0.4} &
  81.46 \footnotesize{± 0.3} &
  77.65 \footnotesize{± 1.0} &
  68.44 \footnotesize{± 1.0} &
  76.25 \footnotesize{± 0.3} &
  71.39 \footnotesize{± 0.3} &
  58.82 \footnotesize{± 0.78} &
  47.18 \footnotesize{± 0.9} &
  73.73 \footnotesize{± 0.5} &
  69.46 \footnotesize{± 0.7} &
  87.60 \footnotesize{± 0.5} &
  86.71 \footnotesize{± 0.8} \\
APER-Adapter &
  89.57 \footnotesize{± 0.9} &
  84.91 \footnotesize{± 0.2} &
  91.62 \footnotesize{± 1.2} &
  86.72 \footnotesize{± 0.2} &
  74.81 \footnotesize{± 0.8} &
  66.97 \footnotesize{± 0.8} &
  59.57 \footnotesize{± 1.6} &
  49.46 \footnotesize{± 0.4} &
  80.48 \footnotesize{± 1.2} &
  74.04 \footnotesize{± 0.3} &
  90.59 \footnotesize{± 1.0} &
  84.28 \footnotesize{± 0.2} \\
EASE &
  90.79 \footnotesize{± 0.8} &
  85.97 \footnotesize{± 0.6} &
  92.51 \footnotesize{± 1.3} &
  86.49 \footnotesize{± 1.2} &
  80.35 \footnotesize{± 1.0}&
  75.74 \footnotesize{± 0.8}&
  64.00 \footnotesize{± 1.5}&
  54.99 \footnotesize{± 1.0} &
  81.11 \footnotesize{± 0.8} &
  74.16 \footnotesize{± 2.0} &
  90.26 \footnotesize{± 3.6} &
  82.07 \footnotesize{± 3.0} \\
MOS &
  93.45 \footnotesize{± 0.9} &
  90.04 \footnotesize{± 0.6} &
  93.42 \footnotesize{± 1.2} &
  90.07 \footnotesize{± 0.9} &
  82.26 \footnotesize{± 1.0} &
  77.62 \footnotesize{± 0.9} &
  63.57 \footnotesize{± 2.0} &
  54.60 \footnotesize{± 0.8} &
  84.73 \footnotesize{± 1.1} &
  79.97 \footnotesize{± 0.9} &
  92.75 \footnotesize{± 1.0} &
  92.74 \footnotesize{± 0.9} \\ \hline
\rowcolor{pink!20}
TOSCA (ours) &
  \textbf{96.37} \footnotesize{± 0.5} &
  \textbf{95.64} \footnotesize{± 0.8} &
  \textbf{93.47} \footnotesize{± 1.9} &
  \textbf{91.09} \footnotesize{± 1.8} &
  \textbf{82.27} \footnotesize{± 1.9} &
  \textbf{78.28} \footnotesize{± 1.9} &
  \textbf{66.92} \footnotesize{± 3.0} &
  \textbf{65.37} \footnotesize{± 2.9} &
  \textbf{84.75} \footnotesize{± 2.6} &
  \textbf{82.35} \footnotesize{± 1.0} &
  \textbf{96.59} \footnotesize{± 1.6} &
  \textbf{93.87} \footnotesize{± 2.0} \\ \hline
\end{tabular}%
}
\vskip -0.1cm
\end{table}

\begin{figure}[h]
\centering
\begin{tikzpicture}
    \begin{groupplot}[
        group style={
            group size=3 by 2,
            horizontal sep=1cm,
            vertical sep=1.6cm,
        },
        width=0.36\textwidth,
        height=0.28\textwidth,
        xlabel={Number of Classes},
        ylabel={Accuracy [\%]},
        xmin=0, xmax=100,        % Adjust based on your data range
        ymin=0, ymax=100,       % Adjust based on your data range
        grid=major,
        legend columns=4,
        legend style={font=\small,
        draw=none,
        align=left,
        text width=2.5cm,
        at={(0,-2.2)}, 
        anchor=north west},
        legend image post style={scale=0.8}, 
        cycle list={% Define styles for 9 baselines + our method
            {blue, mark size= 0.5pt},
            {green!60!black, mark size= 0.5pt},
            {red, mark size= 0.5pt},
            {cyan, mark size= 0.5pt},
            {violet, mark size= 0.5pt},
            {brown, mark size= 0.5pt},
            {orange, mark size= 0.5pt},
            {black, line width=1.2pt, mark size= 0.6pt}
        },
        every axis plot/.append style={semithick}
    ]
    
    % First row
    \nextgroupplot[title={CIFAR B0 Inc5}, xmin=5, xmax=100, ymin=75, xlabel={}]
        \addplot+[] coordinates {(5, 94.0) (10, 92.7) (15, 89.2) (20, 87.25) (25, 83.6) (30, 82.1) (35, 80.71) (40, 80.32) (45, 80.18) (50, 79.08) (55, 78.82) (60, 78.1) (65, 77.77) (70, 77.0) (75, 77.11) (80, 76.81) (85, 76.56) (90, 76.34) (95, 76.24) (100, 76.24)};
        \addplot+[] coordinates {(5, 98.4) (10, 96.0) (15, 92.2) (20, 90.15) (25, 89.24) (30, 87.37) (35, 83.54) (40, 84.25) (45, 84.04) (50, 82.0) (55, 82.56) (60, 82.52) (65, 81.54) (70, 81.0) (75, 80.07) (80, 79.35) (85, 77.09) (90, 76.97) (95, 77.6) (100, 77.79)};
        \addplot+[] coordinates {(5, 94.0) (10, 90.4) (15, 89.6) (20, 88.15) (25, 88.68) (30, 86.4) (35, 84.94) (40, 83.52) (45, 83.09) (50, 80.42) (55, 80.38) (60, 81.27) (65, 81.12) (70, 79.61) (75, 78.81) (80, 77.71) (85, 76.68) (90, 76.39) (95, 76.11) (100, 76.33)};
        \addplot+[] coordinates {(5, 98.4) (10, 97.9) (15, 92.73) (20, 91.9) (25, 91.16) (30, 90.17) (35, 89.8) (40, 87.88) (45, 87.93) (50, 85.96) (55, 85.22) (60, 83.77) (65, 82.69) (70, 82.7) (75, 81.41) (80, 81.59) (85, 81.64) (90, 81.83) (95, 82.15) (100, 80.85)};
        \addplot+[] coordinates {(5, 98.4) (10, 96.1) (15, 95.27) (20, 93.55) (25, 93.04) (30, 92.57) (35, 91.4) (40, 90.88) (45, 89.91) (50, 89.14) (55, 87.45) (60, 87.22) (65, 86.85) (70, 87.0) (75, 86.11) (80, 84.66) (85, 84.32) (90, 84.07) (95, 84.25) (100, 83.56)};
        \addplot+[] coordinates {(5, 99.4) (10, 98.1) (15, 96.73) (20, 95.85) (25, 95.24) (30, 94.17) (35, 93.11) (40, 92.62) (45, 91.73) (50, 90.76) (55, 89.27) (60, 89.13) (65, 88.72) (70, 88.64) (75, 87.73) (80, 85.98) (85, 85.42) (90, 85.13) (95, 85.29) (100, 84.63)};
        \addplot+[] coordinates {(5, 99.6) (10, 98.3) (15, 97.8) (20, 96.15) (25, 95.84) (30, 94.63) (35, 94.11) (40, 93.95) (45, 93.4) (50, 92.68) (55, 91.84) (60, 91.5) (65, 91.12) (70, 91.27) (75, 90.71) (80, 89.81) (85, 89.25) (90, 88.92) (95, 88.99) (100, 88.36)};
        \addplot+[] coordinates {(5, 97.4) (10, 97.3) (15, 97.2) (20, 97.2) (25, 94.32) (30, 94.5) (35, 94.8) (40, 95.5) (45, 95.71) (50, 95.82) (55, 95.91) (60, 95.88) (65, 95.63) (70, 95.71) (75, 95.28) (80, 95.31) (85, 95.32) (90, 95.16) (95, 95.16) (100, 95.22)};       
        
        \node at (axis cs:90, 95) [anchor=south] {\scriptsize{\textbf{6.86}$\uparrow$}};
        
        \legend{SimpleCIL, L2P, DualPrompt, CODAPrompt, APER, EASE, MOS, TOSCA (ours)}
    
    \nextgroupplot[title={CUB B0 Inc10}, xmin=10, xmax=200, ymin=55, xlabel={}, ylabel={}]
        \addplot+[] coordinates {(10, 95.33) (20, 91.08) (30, 91.21) (40, 90.02) (50, 90.92) (60, 89.93) (70, 90.37) (80, 90.3) (90, 88.91) (100, 88.9) (110, 86.63) (120, 86.13) (130, 86.26) (140, 86.11) (150, 85.76) (160, 85.55) (170, 85.41) (180, 85.11) (190, 84.82) (200, 85.16)};
        
        \addplot+[] coordinates {(10, 95.33) (20, 83.57) (30, 77.88) (40, 75.05) (50, 73.12) (60, 72.48) (70, 71.72) (80, 71.98) (90, 72.32) (100, 68.69) (110, 64.95) (120, 63.62) (130, 62.85) (140, 60.77) (150, 58.27) (160, 58.89) (170, 58.63) (180, 57.37) (190, 57.79) (200, 57.97)};
        
        \addplot+[] coordinates {(10, 98.94) (20, 96.55) (30, 96.81) (40, 93.01) (50, 89.07) (60, 86.66) (70, 80.42) (80, 78.88) (90, 77.29) (100, 74.61) (110, 75.16) (120, 72.61) (130, 71.59) (140, 71.59) (150, 69.32) (160, 69.47) (170, 68.06) (180, 67.26) (190, 65.18) (200, 63.4)};
        
        \addplot+[] coordinates {(10, 92.79) (20, 91.34) (30, 83.52) (40, 82.52) (50, 73.31) (60, 68.75) (70, 65.82) (80, 67.58) (90, 68.83) (100, 71.05) (110, 70.63) (120, 68.84) (130, 65.58) (140, 65.2) (150, 63.64) (160, 62.35) (170, 62.15) (180, 60.81) (190, 61.25) (200, 61.87)};
        
        \addplot+[] coordinates {(10, 97.67) (20, 97.6) (30, 95.25) (40, 91.67) (50, 91.44) (60, 92.26) (70, 91.71) (80, 90.99) (90, 90.32) (100, 89.19) (110, 87.84) (120, 86.84) (130, 86.35) (140, 85.59) (150, 84.58) (160, 83.59) (170, 82.76) (180, 82.14) (190, 81.02) (200, 80.58)};
        
        \addplot+[] coordinates {(10, 97.67) (20, 97.6) (30, 94.97) (40, 91.88) (50, 91.1) (60, 91.97) (70, 91.34) (80, 90.77) (90, 90.04) (100, 88.94) (110, 87.84) (120, 87.06) (130, 86.61) (140, 85.96) (150, 85.15) (160, 84.12) (170, 83.01) (180, 82.43) (190, 81.24) (200, 80.75)};
        
        \addplot+[] coordinates {(10, 98.39) (20, 97.1) (30, 96.4) (40, 96.33) (50, 95.17) (60, 94.63) (70, 91.56) (80, 92.0) (90, 91.63) (100, 91.33) (110, 91.05) (120, 90.78) (130, 89.64) (140, 89.83) (150, 89.21) (160, 88.17) (170, 88.48) (180, 88.45) (190, 87.99) (200, 88.03)};

        \addplot+[] coordinates {(10, 100.0) (20, 95.07) (30, 96.17) (40, 93.94) (50, 94.54) (60, 93.7) (70, 93.6) (80, 94.16) (90, 90.25) (100, 89.86) (110, 90.62) (120, 90.0) (130, 90.02) (140, 90.06) (150, 90.0) (160, 89.77) (170, 88.74) (180, 88.87) (190, 89.0) (200, 88.82)};       
        
        \node at (axis cs:179, 89) [anchor=south] {\scriptsize{\textbf{0.98}$\uparrow$}};
    
    \nextgroupplot[title={ImageNet-R B0 Inc20}, xmin=20, xmax=200, ymin=60, ytick={65, 80, 95}, ymax=95, xlabel={}, ylabel={}]
        \addplot+[] coordinates {(20, 78.96) (40, 72.14) (60, 70.11) (80, 68.29) (100, 66.0) (120, 64.44) (140, 64.05) (160, 63.08) (180, 62.28) (200, 61.27)};
        
        \addplot+[] coordinates {(20, 85.15) (40, 81.46) (60, 80.49) (80, 77.56) (100, 76.05) (120, 74.9) (140, 73.93) (160, 72.82) (180, 72.9) (200, 72.17)};
        
        \addplot+[] coordinates {(20, 83.48) (40, 77.57) (60, 76.81) (80, 74.68) (100, 73.73) (120, 71.96) (140, 71.32) (160, 70.36) (180, 70.62) (200, 69.88)};
        
        \addplot+[] coordinates {(20, 86.48) (40, 85.66) (60, 82.6) (80, 80.5) (100, 78.58) (120, 77.38) (140, 75.31) (160, 74.29) (180, 74.34) (200, 73.32)};
        
        \addplot+[] coordinates {(20, 91.47) (40, 85.92) (60, 82.72) (80, 79.7) (100, 77.29) (120, 75.94) (140, 75.23) (160, 74.15) (180, 72.92) (200, 71.87)};
        
        \addplot+[] coordinates {(20, 92.82) (40, 90.47) (60, 86.57) (80, 83.51) (100, 81.85) (120, 81.0) (140, 80.16) (160, 78.93) (180, 77.75) (200, 77.42)};
        
        \addplot+[] coordinates {(20, 90.39) (40, 88.63) (60, 85.58) (80, 82.61) (100, 81.95) (120, 81.14) (140, 80.6) (160, 79.66) (180, 78.49) (200, 78.65)};
        
        \addplot+[] coordinates {(20, 88.34) (40, 84.94) (60, 84.38) (80, 84.39) (100, 84.25) (120, 84.17) (140, 83.77) (160, 83.94) (180, 82.81) (200, 82.52)};

        \node at (axis cs:183, 82) [anchor=south] {\scriptsize{\textbf{3.67}$\uparrow$}};
    
    % Second row
    \nextgroupplot[title={ImageNet-A B0 Inc20}, xmin=20, xmax=200, ymin=40, ymax=85, ytick={45, 65, 85}]
        \addplot+[] coordinates {(20, 75.88) (40, 66.03) (60, 59.82) (80, 56.83) (100, 55.62) (120, 56.07) (140, 54.86) (160, 53.76) (180, 51.71) (200, 49.44)};
        
        \addplot+[] coordinates {(20, 72.0) (40, 61.67) (60, 58.4) (80, 54.15) (100, 50.06) (120, 49.95) (140, 48.8) (160, 45.99) (180, 45.02) (200, 44.7)};
        
        \addplot+[] coordinates {(20, 74.29) (40, 70.83) (60, 63.66) (80, 60.6) (100, 55.89) (120, 54.77) (140, 52.19) (160, 50.24) (180, 48.32) (200, 47.2)};
        
        \addplot+[] coordinates {(20, 79.8) (40, 75.4) (60, 70.33) (80, 67.78) (100, 65.93) (120, 63.18) (140, 59.84) (160, 57.28) (180, 54.28) (200, 52.73)};
        
        \addplot+[] coordinates {(20, 78.29) (40, 79.44) (60, 72.06) (80, 70.49) (100, 67.42) (120, 65.64) (140, 63.78) (160, 61.86) (180, 60.14) (200, 58.99)};
        
        \addplot+[] coordinates {(20, 83.29) (40, 79.7) (60, 77.16) (80, 75.69) (100, 72.9) (120, 68.29) (140, 67.17) (160, 65.56) (180, 63.2) (200, 61.75)};
        
        \addplot+[] coordinates {(20, 80.12) (40, 75.9) (60, 76.3) (80, 71.64) (100, 71.04) (120, 67.28) (140, 65.61) (160, 63.41) (180, 61.92) (200, 61.55)};
        
        \addplot+[] coordinates {(20, 80.12) (40, 77.34) (60, 77.28) (80, 74.44) (100, 76.37) (120, 74.2) (140, 74.21) (160, 74.23) (180, 74.18) (200, 73.67)};

        \node at (axis cs:180, 73) [anchor=south] {\scriptsize{\textbf{12.02}$\uparrow$}};
        
    \nextgroupplot[title={Omnibenchmark B0 Inc30}, xmin=30, xmax=300, ymin=60, ymax=95, ylabel={}, ytick={65, 80, 95}]
        \addplot+[] coordinates {(30, 86.5) (60, 86.74) (90, 84.37) (120, 81.13) (150, 78.66) (180, 76.11) (210, 74.79) (240, 73.12) (270, 72.4) (300, 72.2)};
        \addplot+[] coordinates {(30, 89.95) (60, 83.25) (90, 75.64) (120, 69.28) (150, 68.37) (180, 66.44) (210, 64.43) (240, 64.9) (270, 62.77) (300, 62.81)};
        \addplot+[] coordinates {(30, 88.5) (60, 82.07) (90, 79.42) (120, 76.37) (150, 73.68) (180, 70.91) (210, 68.27) (240, 65.12) (270, 63.93) (300, 63.58)};
        \addplot+[] coordinates {(30, 90.8) (60, 84.54) (90, 64.81) (120, 64.48) (150, 68.75) (180, 71.73) (210, 72.31) (240, 68.95) (270, 67.72) (300, 67.39)};
        \addplot+[] coordinates {(30, 88.83) (60, 86.74) (90, 83.15) (120, 79.67) (150, 77.92) (180, 75.08) (210, 73.86) (240, 71.99) (270, 70.93) (300, 70.81)};
        \addplot+[] coordinates {(30, 89.83) (60, 87.91) (90, 84.09) (120, 80.84) (150, 78.59) (180, 76.03) (210, 74.81) (240, 73.06) (270, 72.06) (300, 71.93)};
        \addplot+[] coordinates {(30, 92.63) (60, 88.03) (90, 86.43) (120, 84.51) (150, 83.33) (180, 81.94) (210, 81.11) (240, 80.65) (270, 80.3) (300, 78.47)};
        \addplot+[] coordinates {(30, 91.33) (60, 88.07) (90, 83.76) (120, 83.67) (150, 83.09) (180, 80.73) (210, 79.06) (240, 78.55) (270, 78.93) (300, 78.88)};

        \node at (axis cs:274, 80) [anchor=south] {\scriptsize{\textbf{0.58}$\uparrow$}};

    \nextgroupplot[title={VTAB B0 Inc10}, xmin=10, xmax=50, ymin=65, ylabel={}, xtick={20, 30, 40, 50}, ytick={70, 85, 100}]
        \addplot+[] coordinates {(10, 94.27) (20, 95.67) (30, 93.2) (40, 88.61) (50, 83.61)};
        \addplot+[] coordinates {(10, 97.98) (20, 79.38) (30, 81.89) (40, 71.62) (50, 66.52)};
        \addplot+[] coordinates {(10, 96.95) (20, 85.98) (30, 80.68) (40, 79.5) (50, 68.59)};
        \addplot+[] coordinates {(10, 91.5) (20, 80.14) (30, 84.19) (40, 85.22) (50, 82.45)};
        \addplot+[] coordinates {(10, 97.13) (20, 88.9) (30, 85.36) (40, 85.89) (50, 82.62)};
        \addplot+[] coordinates {(10, 97.86) (20, 91.49) (30, 87.71) (40, 88.16) (50, 83.32)};
        \addplot+[] coordinates {(10, 91.7) (20, 92.66) (30, 92.45) (40, 91.73) (50, 91.60)};
        \addplot+[] coordinates {(10, 97.14) (20, 97.82) (30, 96.93) (40, 94.15) (50, 92.94)};

        \node at (axis cs:46, 92.5) [anchor=south] {\scriptsize{\textbf{1.25}$\uparrow$}};
        
    \end{groupplot}
\end{tikzpicture}
\captionsetup{font=small}
\caption{Performance curve of different methods under different settings. All methods are initialized with \textbf{ViT-B/16-IN1K}. We annotate the relative improvement of TOSCA above the runner-up method with numerical numbers at the last incremental stage.}
\label{fig:in1k}
\vskip -0.2cm
\end{figure}



\subsection{State-of-the-art Comparison}
In this section, we compare TOSCA to other state-of-the-art methods on six benchmark datasets and different pre-trained backbones. \cref{tab:in21k} reports the accuracy after the final learning stage with ViT-B/16-IN21K for different methods. Our approach achieves the best performance among all six benchmarks, substantially outperforming the current state-of-the-art methods, i.e. EASE, and MOS which is \textit{not} completely exemplar-free due to replay generation for classifier alignment.
We also report the incremental performance trend over the training sessions for different methods in \cref{fig:in1k}. We find TOSCA outperforms the runner-up method by $4\% - 12\%$ on CIFAR100, ImageNet-R, and ImageNet-A as highlighted in the annotations at the end of each image. 
Additionally, we also compare TOSCA with a traditional 100-epoch joint training as an upper-bound. Although joint training still leads, our approach remains highly close and demonstrates competitive performance by training only a single lightweight module per task. 
These results indicate that TOSCA is a versatile approach, effective in enhancing performance with various backbones. Please see our \cref{Additional Results} for more experimental results.

\begin{figure}[h]
\captionsetup{font=small}
  \centering
  \begin{subfigure}{0.327\textwidth}
    \includegraphics[width=\textwidth]{figs/run_acc_param.pdf}
    \caption{Memory \& computational cost}
    \label{fig:params}
  \end{subfigure}
  \begin{subfigure}{0.327\textwidth}
    \includegraphics[width=\textwidth]{figs/ablation_heatmap.pdf}
    \caption{Hyperparameter analysis}
    \label{fig:abla_heat}
  \end{subfigure}
  \begin{subfigure}{0.327\textwidth}
    \includegraphics[width=\textwidth]{figs/ablation_line.pdf}
    \caption{Design and component ablation}
    \label{fig:abla_line}
  \end{subfigure}
\caption{Performance evaluation of TOSCA across different perspectives. (a) Memory \& computational cost highlights TOSCA’s efficiency, (b) Hyperparameter analysis illustrates effect of $\ell_1$ strength ($\lambda$) and projection dimension ($r$) on accuracy, (c) Design and component ablation presents the impact of different components and flows on accuracy.}
  \vskip -0.25cm
\end{figure}

\subsection{Parameter and Run-Time Analysis}
We further investigate PTM-based CIL approaches in terms of accuracy, computational cost (run time), and parameter efficiency on CIFAR B0 Inc5 benchmark in \cref{fig:params}. TOSCA achieves the top performance while maintaining a low computational cost and parameter overhead per task. In contrast, methods like CODA-Prompt and EASE require significantly more parameters and longer run times, making them less efficient. Notably, MOS also attains high accuracy, but it comes at a higher computational expense due to additional processes such as adapter merging and replay generation. Overall, TOSCA demonstrates its effectiveness in CIL with minimal parameter overhead and shorter run-time, striking a balance between efficiency and performance.

\subsection{Ablation Study}
We also perform an ablation study on CIFAR B0 Inc10, evaluating the incremental performance across different learning settings. 
First, we analyze the impact of $\ell_1$-regularization strength ($\lambda$) and projection dimension ($r$) on performance, as shown in \cref{fig:abla_heat}.
Our findings indicate that moderate $\ell_1$ regularization enhances accuracy, with performance peaking at $\lambda = 5e^{-4}$. This promotes the orthogonality among different modules, improving module selection during inference. However, excessive sparsity degrades performance by excessively constraining representations, thereby reducing expressiveness and learning capacity. 
Similarly, increasing the projection dimension ($r$) improves accuracy up to $r = 48$, beyond which performance deteriorates due to the larger bottleneck. Based on these observations, we identify the optimal configuration as $\lambda = 5e^{-4}$ and $r = 48$, achieving an accuracy of $95.3\%$.
Additionally, we compare the performance of different components, alternative module designs and configurations against LuCA in \cref{fig:abla_line}. This includes a reversed variant, LuCA\_r, which integrates new information atop the calibrated pre-trained features, formulated as $\phi(\mathbf{x})' = A(C(\phi(\mathbf{x})))$. Our results highlight the crucial role of the calibrator while LuCA surpasses other design variants by effectively harmonizing its two modules working together.
