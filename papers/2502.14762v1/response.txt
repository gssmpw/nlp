\section{Related Work}
%In this section, we review recent advancements in class-incremental learning methods, focusing on both approaches using randomly initialized models and those leveraging pre-trained models to mitigate catastrophic forgetting and improve model performance.

\paragraph{CIL with Randomly Initialized Models.}
Not a long time ago, the primary focus in CIL was training deep neural networks sequentially from scratch, with the goal of efficiently acquiring the knowledge of new classes while minimizing forgetting of previous ones. Common CIL strategies can be categorized into four main approaches: Regularization-based methods **Srivastava et al., "Memory-Efficient Training of Recurrent Neural Networks"** maintain the model by selectively stabilizing changes in parameters or predictions.
Replay-based methods approximate and reconstruct previously learned data distributions by either storing **Van de Ven et al., "Expert Gate: A Semi-supervised Approach to Continual Learning"** or generating **Chen et al., "A Continual Learning Framework for Neural Networks with an Adaptive Forgetting Mechanism"** samples from past experiences.
Architecture-based methods **Riemer et al., "Learning Multiple Visual Domains with Residual Adaptation"** allocate distinct parameters and subspaces to different sets of classes. 
Parameter isolation methods utilize iterative pruning **Kemker et al., "Pruning and Transfer: A Simple Approach for Continual Learning"** or dynamic sparse training **Kim et al., "Dynamically Sparse Training: Efficient Regularization of Neural Networks"** to preserve key parameters.
These strategies have laid the foundation for advancing CIL methodologies.
\paragraph{CIL with Pre-trained Models.}
In contrast, recent advancements in CIL research have shifted towards leveraging PTMs. Representations derived from pre-training have proven effective not only in facilitating knowledge transfer but also in mitigating catastrophic forgetting during the downstream continual learning **Cheng et al., "Efficient Continual Learning via Partial Model Update"**. Additionally, pre-training on a large set of base classes enables incremental learning with minimal adaptations **Rebuffi et al., "ICARL: Incremental Classifier and Representation Learning"**. Therefore, methods in this context aim to improve performance with minimal additions while freezing the PTMs.
L2P **Houlsby et al., "Parameter-Efficient Transfer Learning for NLP"** borrows a technique from NLP by introducing a learnable prompt pool and selecting instance-specific prompts via a key-query matching selection mechanism to guide the PTMs response. 
DualPrompt **Zhou et al., "Dual Prompt Tuning: A Simple yet Effective Approach for Zero-Shot Generalization"** extends L2P by designing G-Prompt and E-Prompt, which encode task-invariant and task-specific instructions. 


\begin{figure}[t]
\captionsetup{font=small}
  \centering
  \begin{subfigure}{0.463\textwidth}
    \includegraphics[width=\textwidth]{figs/tosca1.pdf}
    \caption{Prompt-Based}
    \label{fig:params}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.23\textwidth}
    \includegraphics[width=\textwidth]{figs/tosca2.pdf}
    \caption{Adapter-Based}
    \label{fig:abla_heat}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.2\textwidth}
    \includegraphics[width=\textwidth]{figs/tosca3.pdf}
    \caption{TOSCA (ours)}
    \label{fig:abla_line}
  \end{subfigure}
\caption{Prompt-based methods influence the self-attention process of a PTM, either from the input layer alone or across all layers. Adapter-based methods enable task-specific adaptations by inserting lightweight neural modules into the PTMâ€™s layers. In contrast, we propose a single trainable module that operates exclusively on the final \texttt{[CLS]} token representation, efficiently adapting and calibrating features just before classification. This design offers a streamlined and effective alternative to both prompt- and adapter-based methods.}
\end{figure}

CODA-Prompt **Kim et al., "Contrastive Decorrelation for Adaptation"** uses contrastive learning to decorrelate representations of the prompts to reduce interference and combine them by attention-based weighting method.
APER **Al-Shedivat et al., "PEFT: Data-Efficient Continual Learning with Partial-Elastic-Frontier Transfer"** explores various PEFT methods including adapters and shows that prototypical classifiers named SimpleCIL serve as a strong baseline. 
EASE **Ren et al., "Efficient Adapters for Set-Based Continual Learning"** attaches adapters to each layer of PTMs to create expandable subspaces and during inference, it concatenates all feature representations from different sets of adapters to perform on a single classifier. 
MOS **Chen et al., "Multi-Objective Sparse Adaptation for Continual Learning"** adds replay generation for classifier alignment and an adapter merging over EASE to reduce mistakenly retrieving irrelevant modules during inference due to parameter drift.