@InProceedings{Cognitive_Mapping_and_Planning_for_Visual_Navigation,
author = {Gupta, Saurabh and Davidson, James and Levine, Sergey and Sukthankar, Rahul and Malik, Jitendra},
title = {Cognitive Mapping and Planning for Visual Navigation},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {July},
year = {2017}
}

@inproceedings{Learning_About_Objects_by_Learning_to_Interact,
 author = {Lohmann, Martin and Salvador, Jordi and Kembhavi, Aniruddha and Mottaghi, Roozbeh},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {3930--3941},
 publisher = {Curran Associates, Inc.},
 title = {Learning About Objects by Learning to Interact with Them},
 url = {https://proceedings.neurips.cc/paper/2020/file/291597a100aadd814d197af4f4bab3a7-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{Learning_Affordance_Landscapes_Interaction_Exploration,
 author = {Nagarajan, Tushar and Grauman, Kristen},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {2005--2015},
 publisher = {Curran Associates, Inc.},
 title = {Learning Affordance Landscapes for Interaction Exploration in 3D Environments},
 url = {https://proceedings.neurips.cc/paper/2020/file/15825aee15eb335cc13f9b559f166ee8-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{Learning_Exploration_policies_for_Navigation,
author = "Chen, Tao and Gupta, Saurabh and Gupta, Abhinav",
title = "Learning Exploration Policies for Navigation",
booktitle = "International Conference on Learning Representations",
year = "2019",
url = "https://openreview.net/pdf?id=SyMWn05F7"
}

@article{Learning_compositional_models,
  title={Learning compositional models of robot skills for task and motion planning},
  author={Wang, Zi and Garrett, Caelan Reed and Kaelbling, Leslie Pack and Lozano-P{\'e}rez, Tom{\'a}s},
  journal={The International Journal of Robotics Research},
  volume={40},
  number={6-7},
  pages={866--894},
  year={2021},
  publisher={SAGE Publications Sage UK: London, England}
}

@inproceedings{Theory_of_Affordances_in_Reinforcement_Learning,
  title={What can I do here? A Theory of Affordances in Reinforcement Learning},
  author={Khetarpal, Khimya and Ahmed, Zafarali and Comanici, Gheorghe and Abel, David and Precup, Doina},
  booktitle={International Conference on Machine Learning},
  pages={5243--5253},
  year={2020},
  organization={PMLR}
}

@INPROCEEDINGS{Visual_Semantic_Planning_Using_Deep_Successor_Representations,
  author={Zhu, Yuke and Gordon, Daniel and Kolve, Eric and Fox, Dieter and Fei-Fei, Li and Gupta, Abhinav and Mottaghi, Roozbeh and Farhadi, Ali},
  booktitle={2017 IEEE International Conference on Computer Vision (ICCV)}, 
  title={Visual Semantic Planning Using Deep Successor Representations}, 
  year={2017},
  volume={},
  number={},
  pages={483-492},
  doi={10.1109/ICCV.2017.60}}

@inproceedings{curiosity_driven_exploration,
author = {Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A. and Darrell, Trevor},
title = {Curiosity-Driven Exploration by Self-Supervised Prediction},
year = {2017},
publisher = {JMLR.org},
abstract = {In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent's ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal; 2) exploration with no extrinsic reward, where curiosity pushes the agent to explore more efficiently; and 3) generalization to unseen scenarios (e.g. new levels of the same game) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2778â€“2787},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@article{deep_affordance_foresight,
  author    = {Danfei Xu and
               Ajay Mandlekar and
               Roberto Mart{\'{\i}}n{-}Mart{\'{\i}}n and
               Yuke Zhu and
               Silvio Savarese and
               Fei{-}Fei Li},
  title     = {Deep Affordance Foresight: Planning Through What Can Be Done in the
               Future},
  journal   = {CoRR},
  volume    = {abs/2011.08424},
  year      = {2020},
  url       = {https://arxiv.org/abs/2011.08424},
  archivePrefix = {arXiv},
  eprint    = {2011.08424},
  timestamp = {Wed, 18 Nov 2020 16:48:35 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2011-08424.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

