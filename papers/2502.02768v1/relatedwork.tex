\section{Related Work}
\label{related_work}

\textbf{Learning based approaches to perform tasks}: Existing works on embodied agents with pure learning-based approaches try to train reinforcement learning agents to do tasks through trial and error in the environment. They receive a large reward upon completing the tasks. Some of the popular reinforcement learning-based approaches use curiosity (\cite{curiosity_driven_exploration}) and coverage (\cite{Learning_Exploration_policies_for_Navigation}) to overcome the issue of sparse rewards. They are generally trained for very specific tasks such as exploring the world they are in or simple tasks like navigation to a point in the environment. Hence, they end up learning policies for exploration \cite{Learning_Exploration_policies_for_Navigation} or policies for specific tasks (\cite{Cognitive_Mapping_and_Planning_for_Visual_Navigation}). In our work, the learning is not done for any task but over properties of objects, and the tasks are completed by planning a sequence of actions based on the learned affordances. These affordances are not specific to any of the tasks.

% \textbf{Model-based RL for tasks}: In model-based RL approach for solving tasks in a real-world environment, the approach generally is to learn a dynamics model of the world. This model is then used to predict the next state of the world based on the current state and an action \cite{Theory_of_Affordances_in_Reinforcement_Learning} \cite{Visual_Semantic_Planning_Using_Deep_Successor_Representations}.

\textbf{Affordance learning for tasks}: Some of the recent work in affordance learning in conjunction with reinforcement learning has been used to learn to perform tasks in 3D environments. In these approaches, the affordances models learn to predict possible actions in the given state of the world from the egocentric view the agent has. Now, an RL agent is then trained to perform tasks in the world by using this vision-based affordance model to narrow the set of possible actions in any given situation, which helps in learning to perform tasks faster.
( \cite{Learning_Affordance_Landscapes_Interaction_Exploration} \cite{Learning_About_Objects_by_Learning_to_Interact}). In our work, we learn to be capable of performing any task, even tasks not seen before but which are possible to execute through a composition of existing skills. The other approach to using affordances to perform tasks is to learn affordances not only to predict what actions are possible at a given moment but also what future actions are feasible if a certain action is taken (\cite{deep_affordance_foresight}). The main difference with our work is that they too limit their affordance to what actions are feasible whereas our affordances provide us the information about how to perform an action along with what actions are feasible. 

 %image-based affordance segmentation model, convolutional neural network that maps image regions to the likelihood they
%permit each action, densifying the rewards for exploration.
 
\textbf{Planning-based approaches to perform tasks}: PDDLStream, as described in the previous section, is a framework for solving TAMP problems that converts the discrete and continuous planning problem into a discrete planning problem using streams. Streams are conditional samplers and also have a declarative component that certifies the facts generated by the samplers about the properties the generated values satisfy in the given environment. These generated values are then used as facts in a discrete planning problem. Learning-based approaches which extend the PDDLStream framework (\cite{Learning_compositional_models}) learn the samplers to generate values for certain continuous variables. However, their models depend on the fact that the object orientation does not change. Our approach is an extension of PDDLStream work where we learn the conditional samplers as opposed to human-designed samplers, and the learned models are invariant to the object orientation/pose in the world. 

%\textbf{Planning based approaches with affordance learning} - deep affordance forewishg etc. 

%\item Planning based agents doing tasks in continuous world with learning (technically our work) but also the compositional learning paper and forceful manipulation paper

%\item Our work â€“ planning based agents doing tasks in cont. world with affordance learning