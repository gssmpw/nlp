%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{placeins}
\usepackage{hyperref}

% \usepackage{icml2025}
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\newcommand{\by}{$\times$}

\usepackage{multirow}
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\ding{55}}

\usepackage{tabulary}
\newcolumntype{x}[1]{>{\centering\arraybackslash}p{#1pt}}

\usepackage{xcolor}
\usepackage{colortbl}
\definecolor{citecolor}{RGB}{34,139,34}
\definecolor{demphcolor}{gray}{.5}
\newcommand{\demph}[1]{\textcolor{demphcolor}{#1}}
\newcommand{\gtext}[1]{\textcolor{citecolor}{#1}}
\newcommand{\rtext}[1]{\textcolor{red}{#1}}
\usepackage{url}

\newlength\savewidth\newcommand\shline{\noalign{\global\savewidth\arrayrulewidth
  \global\arrayrulewidth 1pt}\hline\noalign{\global\arrayrulewidth\savewidth}}
\newcommand{\tablestyle}[2]{\setlength{\tabcolsep}{#1}\renewcommand{\arraystretch}{#2}\centering\footnotesize}

\icmltitlerunning{Scaling Laws in Patchification: An Image Is Worth 50,176 Tokens And More}

\input{math_commands}

\begin{document}

\twocolumn[
\icmltitle{Scaling Laws in Patchification: An Image Is Worth 50,176 Tokens And More}

\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Feng Wang}{jhu}
\icmlauthor{Yaodong Yu}{ucb}
\icmlauthor{Guoyizhe Wei}{jhu}
\icmlauthor{Wei Shao}{uf}
\icmlauthor{Yuyin Zhou}{ucsc}
\icmlauthor{Alan Yuille}{jhu}
\icmlauthor{Cihang Xie}{ucsc}
\end{icmlauthorlist}
\icmlaffiliation{jhu}{Johns Hopkins University}
\icmlaffiliation{ucb}{UC Berkeley}
\icmlaffiliation{uf}{University of Florida}
\icmlaffiliation{ucsc}{UC Santa Cruz}

\centering
$^1$ Johns Hopkins University\quad $^2$ UC Berkeley\quad $^3$ University of Florida\quad $^4$ UC Santa Cruz

\vskip 0.3in
]

% \printAffiliationsAndNotice{}

\begin{abstract}

Since the introduction of Vision Transformer (ViT), patchification has long been regarded as a \textit{de facto} image tokenization approach for plain visual architectures. By compressing the spatial size of images, this approach can effectively shorten the token sequence and reduce the computational cost of ViT-like plain architectures. In this work, we aim to thoroughly examine the information loss caused by this patchification-based compressive encoding paradigm and how it affects visual understanding. We conduct extensive patch size scaling experiments and excitedly observe an intriguing scaling law in patchification: the models can consistently benefit from decreased patch sizes and attain improved predictive performance, until it reaches the minimum patch size of 1\by1, \emph{i.e.}, pixel tokenization. This conclusion is broadly applicable across different vision tasks, various input scales, and diverse architectures such as ViT and the recent Mamba models. Moreover, as a by-product, we discover that with smaller patches, task-specific decoder heads become less critical for dense prediction. In the experiments, we successfully scale up the visual sequence to an exceptional length of 50,176 tokens, achieving a competitive test accuracy of 84.6\% with a base-sized model on the ImageNet-1k benchmark. We hope this study can provide insights and theoretical foundations for future works of building non-compressive vision models. Code is available at \url{https://github.com/wangf3014/Patch_Scaling}.
\end{abstract}

\vspace{-0.5cm}
\section{Introduction}

\begin{figure*}[t]
    \centering
    \begin{subfigure}[t]{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/loss_base_64_vit.pdf}
        \vspace{-0.8cm}
        \caption{DeiT-B, 64\by64 Input, CLS}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/loss_base_128.pdf}
        \vspace{-0.8cm}
        \caption{Adventurer-B, 128\by128 Input, CLS}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/loss_base_224.pdf}
        \vspace{-0.8cm}
        \caption{Adventurer-B, 224\by224 Input, CLS}
    \end{subfigure}

    \begin{subfigure}[t]{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/loss_seg.pdf}
        \vspace{-0.8cm}
        \caption{ADE20k Semantic Segmentation}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/loss_obd.pdf}
        \vspace{-0.8cm}
        \caption{COCO Object Detection}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/loss_ins.pdf}
        \vspace{-0.8cm}
        \caption{COCO Instance Segmentation}
    \end{subfigure}

    \begin{subfigure}[t]{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/loss_base_128_vit.pdf}
        \vspace{-0.8cm}
        \caption{DeiT-B, 128\by128 Input, CLS}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/loss_large_128.pdf}
        \vspace{-0.8cm}
        \caption{Adventurer-L, 128\by128, CLS}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/loss_tiny_224.pdf}
        \vspace{-0.8cm}
        \caption{Adventurer-T, 224\by224, CLS}
    \end{subfigure}
    \caption{\textbf{Patchification Scaling Laws.} We observe a smooth and consistent decrease in test loss across different vision tasks, input resolutions, and model architectures when reducing the patch size. The performance gains remain considerably significant even when scaling down the patch size to 1\by1. In all sub-figures, both $x$ and $y$ axes are in log scale. CLS denotes ImageNet-1k classification.}
    \label{fig:scaling}
    \vspace{-10pt}
\end{figure*}

In the past few years, we have witnessed the great success of Vision Transformers (ViTs) in representation learning, with a series of visual foundation models learned with this plain architecture achieving highly competitive performance and establishing effective connections to other modalities such as natural language~\cite{vit,dino,clip,coca,stablediff,sam,llava}. A key insight behind the ViT-like architectures lies in a compressive encoding paradigm: instead of directly processing raw pixels that introduces significant complexity, these architectures leverage a patchification layer to compress images into spatially smaller feature maps, making the representation space of an image roughly equivalent to that of a medium-length text consisting of a few hundred tokens.

However, we argue that this operation often incurs irreversible information loss to visual inputs. For example, intuitively, we believe the information contained in a 224\by224 resolution image is generally much richer than that in a text consisting of 196 words; however they have nearly the same size of representation space under a ViT encoder with patch size 16\by16 (we suppose the vision and language encoders share the same embedding dimension). The difference in information content between visual and textual data can also be directly reflected in their storage requirements: storing an uncompressed 24-bit, 224\by224 resolution image requires approximately 147KB, whereas storing a 196-word text only needs about 1.15KB. Empirically, if we manually reduce the compression rate, for example, by changing the patch size of DeiT-Base from 16\by16 to 8\by8, we can observe a significant accuracy improvement from 81.8\% to 83.5\% on the ImageNet-1k classification benchmark~\cite{imagenet}.

Nonetheless, since the computation of self-attention scales quadratically with sequence length, ViT architectures are sensitive to the patch size. At the time when ViT was first introduced in late 2020, it needed to ensure that its computational cost was comparable to that of the CNN counterparts; and given the computational capacity at that time, the models had to be computationally manageable in terms of memory consumption and training time, especially when trained with the medium-resolution, medium-scale ImageNet~\cite{imagenet} and beyond~\cite{jft}. As a result, the architectural design of ViT had to compromise with a compressive encoding paradigm achieved through patchification. The success of this design, patchification with a typical 16\by16-pixel kernel, has led to its widespread adoption as a default component in various subsequent architectures, even including those non-attention models such as ConvNeXt~\cite{convnext} and Vision Mamba~\cite{vim}, while the impact of information loss posed by this compressive encoding paradigm has not been well studied.

In this work, we aim to thoroughly examine how compressive encoding affects visual representations and whether patch size can be a new scaling dimension for modern visual architectures. While the concept of Scaling Laws~\cite{scalinglaw} has been broadly testified in natural language processing, leading to a great prosperity of Large Language Models over the past few years~\cite{llama,gemini,gpt4}, the scaling-up of vision models faces practical issues in the dimensions of both parameter size and input size (detailed in Section~\ref{sec:abl}). Here, we aim to revisit the scaling potential of vision models from a new perspective of spatial compression, attempting to unlock the compressed information by reducing the patch size. We highlight that through patchification, there is significant room for scaling up the model's computation, and a new scaling law may emerge during this process.

Thanks to the rapid advancements in hardware, efficient attention mechanisms~\cite{flashattn,pagedattn}, as well as linear-complexity structures~\cite{linearattn,rwkv,mamba}, we can now extensively validate the impact of patchification at a standard input size (\emph{e.g.}, 224\by224 for ImageNet) with manageable computing resources. We conduct a series of straightforward scaling experiments on patchification, gradually reducing the model's patch size from the typical 16\by16 down to 1\by1 to lower the compression rate and observe how performance changes. We employ both ViT and Adventurer~\cite{adventurer}, a Mamba-based~\cite{mamba2} linear-complexity architecture, to make our conclusions generalizable and experiments affordable in computation. To our surprise, this simple scaling study delivers three intriguing discoveries:

First, as shown in Figure~\ref{fig:scaling}, we excitedly observe a new scaling law for patchification in vision models. Similar to the scaling laws discovered in early studies~\cite{scalinglaw} on language—where continuously increasing model parameters reliably leads to consistent performance gains—we have identified a new scaling dimension for vision models, which is reflected in the observation that as the compression rate (\emph{i.e.}, patch size) decreases, the model's test loss smoothly declines, reaching its limit at single-pixel patch sizes that essentially form a non-compressive encoding paradigm. This conclusion broadly holds true for various vision tasks, diverse input scales, and different visual architectures.

Second, we confirm that visual encoding can be performed in a very long token sequence, while patchification is not a requisite for building effective vision models, but rather a compromise to memory and computation overhead when resource is limited. The information lost in the compression of the patchification layer is actually crucial for the model's prediction: on the standard 224\by224-resolution ImageNet-1k classification benchmark, we remove the patchification operation and form a super-long visual sequence consisting of 50,176 tokens, by which we boost the model's test accuracy from 82.6\% to a remarkable result of 84.6\%. 

Finally, we observe a compelling phenomenon in semantic segmentation: as we transit from patch-based tokenization to pixel-level modeling, the traditional necessity for a decoder head—long considered a default component since the inception of deep network architectures—can be eliminated without compromising performance. This architectural simplification is potentially profound, suggesting the possibility of developing decoder-free dense prediction models and illuminating the path toward a universal, encoder-only visual architecture capable of learning from every pixel.

\section{Related Work}
\label{sec:rel}

\textbf{Generic visual backbones.} The development of visual backbones has fundamentally shaped the field of computer vision. Initially dominated by Convolutional Neural Networks (CNNs), these architectures have evolved to gain increasing capabilities for visual representation learning. Pioneering works such as LeNet~\cite{lenet} and AlexNet~\cite{alexnet} have proven the significant effectiveness of convolutional architectures in large-scale image classification tasks. Following these foundational models, the architecture has been refined with the innovations in model depth~\cite{vgg}, residual connection~\cite{resnet,densenet}, and efficient neural architecture search~\cite{efficientnet}.

The landscape of visual backbones underwent another round of significant transformation with the introduction of ViTs~\cite{vit} in late 2020, where a novel plain architecture was proposed that treats images akin to language sequences. This model utilizes a simple patchification layer to convert images into sequences of tokens, which are then processed using mechanisms adapted from language models. This approach opened new avenues in handling visual data without the inductive biases inherent in CNNs, demonstrating competitive performance on several benchmarks. The success of ViTs have spurred rapid development and innovations in data-efficient training strategies~\cite{deit,3things,deit3}, self-supervised learning techniques~\cite{dino,mocov3,beit,mae}, vision-language understanding~\cite{clip,align,llava,flamingo,coca}, and hierarchical architecture designs~\cite{swin,crossvit,t2tvit,metaformer}.

Inspired by the patchification design of transformers, there have been many CNN-based~\cite{convnext} and State Space Model~\cite{ssm-control,ssm1,ssm2} based architectures~\cite{vim,mambar,adventurer} following the same paradigm. Notably, the Mamba~\cite{mamba,mamba2} token mixer, due to its advantage of linear complexity, has recently been widely used to explore vision tasks and has achieved competitive results~\cite{vim,mambar,adventurer,arm,vmamba,plainmamba,mambavision,localmamba,videomamba,jamba}. Among them, the Adventurer~\cite{adventurer} architecture, which significantly simplifies the overall model, has demonstrated superior speed compared to the Transformer. In this paper, we employ it as one of the primary experimental models.

\textbf{Visual architecture scaling.} Scaling laws was initially studied in natural language processing~\cite{scalinglaw}. In vision, a similar concept has guided the community to scale up foundational models in both parameter size and data volume. For example, in the age of CNNs, EfficientNets~\cite{efficientnet,efficientnetv2} have proposed to scale-up the models in depth, width, and  resolution. These advancements were then integrated into ResNets, leading to nearly Billion-level parameter CNNs~\cite{xie2020self,bigvit,gpipe,revisitresnet,resnettimm}. Scaling the parameter count of Vision Transformers has also shown a great success in modern visual understanding benchmarks and has exhibited state-of-the-art results~\cite{deepervit,deepvit,scalingvit,vit22b}.

\section{Method}
\label{sec:method}

\subsection{Problem Formulation}
\label{sec:mtd_problem}

This work aims to investigate the impact of spatial compression on the representation capability of modern visual architectures by scaling the downsampling rate of the patchification operation. The primary experiments are conducted on ViT-like plain architectures, with their definition as follows: The image encoder $\mF:\sR^{3\times w\times h}\rightarrow\sR^{L\times D}$ consists of a patchification layer at the beginning, positional embeddings, and a number of cascade token mixers and channel mixers. The patchification layer divides the input image $\vx\in\sR^{3\times w\times h}$ into non-overlapping patches of size $p\times p$, flattening and projects them into a 1D token sequence $\vx'\in\sR^{L\times D}$. The following mixer layers extract deep visual features while keeping the sequence length $L$ and the feature dimensionality $D$ unchanged—which means the patchification layer makes the only spatial compression throughout the whole visual encoder.

To eliminate the influence of different mixer types on the results of patchification scaling, we conduct the main experiments using two visual encoders: the standard ViT~\cite{vit} and Adventurer~\cite{adventurer}. Due to the significant memory and computation challenges posed by the quadratic complexity of self-attention, ViT is only used for context lengths within 4,096 in this work. For longer sequence tasks, we employ Adventurer, a recent Mamba-based~\cite{mamba,mamba2} efficient architecture that excels in modeling long range dependencies with linear complexity. Adventurer shares the same plain framework as ViT, with spatial compression only presents in the initial patchification layer, while the key difference is that Adventurer leverages the recent Mamba~\cite{mamba2} module as its token mixer, which has a linear complexity relative to sequence length and allows us to perform pixel tokenization for even the standard 224\by224 resolution inputs within reasonable computational resources (\emph{e.g.}, 256 A100 GPUs). Remarkably, in our experiments, we form a super-long visual sequence of 50,176 tokens for ImageNet inputs by scaling down the patch size to 1\by1.

\subsection{Technical Details}
\label{sec:mtd_tech}

We conduct patchification scaling experiments on image classification, semantic segmentation, object detection and instance segmentation tasks. Following the standard design of ViTs~\cite{vit} and Adventurer, we extract holistic visual features by a learnable [CLS] token for classification. For object detection and instance segmentation, we load backbones pretrained with classification and employ a Cascade Mask R-CNN~\cite{cascadercnn} as decoder head. Note that we use the same patch size for classification pretraining and downstream finetuning to ensure consistency in the scaling property.

For semantic segmentation, in addition to evaluating the standard encoder-decoder structure, we also explore a decoder-free approach to observe the emerging properties of patchification scaling. Specifically, instead of using a deep UperNet~\cite{upernet} as the default segmentation head, we employ a simple linear layer to project the dense features extracted by the backbone into the category dimension for training the semantic segmentation task.  

This modification is based on the following prior assumption: in dense prediction tasks like semantic segmentation, the decoder head serves two main functions. The first is addressing the issue where the backbone's high downsampling rate results in feature granularity that is insufficient for pixel-level predictions—typically mitigated by designs such as atrous convolution and multi-scale feature fusion~\cite{deeplab,deeplabv3+}. The second function is enhancing the model's learning capacity by introducing additional trainable parameters. Under this assumption, we believe that if the backbone's compression rate is already very low, the decoder's benefits would be limited to the second aspect. Therefore, task-specific decoder head designs become less critical, and training a general high-fidelity backbone alone would be sufficient to handle various vision tasks.

\section{Experiments}
\label{sec:exp}

\begin{table*}[t]
    \tablestyle{5pt}{1.1}
    \begin{tabular}{lccccccccr}
        \multirow{2}{*}{Model}
        & \multirow{2}{*}{Input size} & 
        & \multicolumn{4}{c}{\textit{patch tokenization}} &
        & \multicolumn{2}{c}{\textit{pixel tokenization}}
        \\\cline{4-7} \cline{9-10}
        &&& $p$=16 & $p$=8 & $p$=4 & $p$=2 & & $p$=1 & seq. length \\\shline
        
        DeiT-Base~\cite{deit} & 64\by64 && 68.2 & 76.9 & 80.1 & 80.8 && \bf 81.3 & 4,096 \\
        DeiT-Base~\cite{deit} & 128\by128 && 78.1 & 81.0 & 82.3 & 82.9 && - & - \\
        Adventurer-Base~\cite{adventurer} & 64\by64 && 69.2 & 77.2 & 80.0 & 80.5 && \bf 80.9 & 4,096 \\
        Adventurer-Base~\cite{adventurer} & 128\by128 && 79.0 & 81.5 & 81.8 & 82.2 && \bf 82.4 & 16,384 \\\rowcolor{cyan!10}
        Adventurer-Base~\cite{adventurer} & 224\by224 && 82.6 & 83.9 & 84.3 & 84.5 && \bf 84.6 & \bf 50,176 \\
        
    \end{tabular}

    \caption{\textbf{Detailed ImageNet classification results.} As patch size (denoted as $p$) decreases, the test accuracy (\%) on ImageNet-1k~\cite{imagenet} consistently improves and reaches the best performance with pixel tokenization. We highlight that we successfully scale up the visual token sequence to an unprecedented length of 50,176, with a competitive 84.6 test accuracy obtained by a base-sized model.}
    \label{tab:cls}
    \vspace{-5pt}
\end{table*}

\subsection{Experimental Setup}

The experiments are conducted on the standard ImageNet-1k~\cite{imagenet} classification, ADE20k~\cite{ade20k} semantic segmentation, and COCO~\cite{coco} object detection and instance segmentation benchmarks. For ViTs, we follow the data-efficient strategy of DeiT~\cite{deit} to train the model for 300 epochs by an AdamW~\cite{adamw} optimizer with a 1024 batch size, 0.001 learning rate and 0.05 weight decay. For Adventurer, we basically refer to their optimized multi-stage training recipe to improve efficiency and obtain competitive results. The details of the training strategy can be found in Appendix. In semantic segmentation, we follow the prior practice of DeiT and Adventurer to finetune the classification models with an AdamW optimizer, 5e-5 learning rate, 0.01 weight decay, a total batch size of 16 for 160k iterations. We train object detection and instance segmentation with AdamW optimizer, 1e-4 learning rate and 0.05 weight decay for 12 epochs.

\subsection{Main Results}

As shown in Figure~\ref{fig:scaling}, we first evaluate the model's patchification scaling performance using test loss as a unified metric across different input sizes, tasks, and parameter scales. We observe an interesting phenomenon that the model's predictive performance consistently improves as the patch size decreases. This observation effectively highlights the negative impact of the existing compressive encoding approach in visual models and supports our initial hypothesis: patchification is not a necessary component for visual encoders; its primary role is to improve computational efficiency at the cost of partial information loss. Although this efficiency gain is significant for Transformer models with quadratic complexity, our findings suggest that when the computing resource allows—and indeed, computational power has evolved rapidly over years—we should reconsider the traditional compressive encoding approach and begin embracing the notion of \textit{\textbf{``a pixel is worth a token''}} that stands for a non-compressive representation learning paradigm.

We also observe that reducing the patch size not only improves performance in dense prediction tasks like semantic segmentation and instance segmentation—which naturally favor fine feature granularities and for which smaller patch size is a direct solution—but also benefits holistic tasks like image classification, which inherently do not require fine-grained representations. This result indicates that the primary benefit of reducing the patch size comes from unlocking the visual information that is previously compressed by patchification. This information, often considered insignificant low-level features in the past, is actually considerably critical for visual understanding.

\textbf{ImageNet classification} results are elaborated in Table~\ref{tab:cls}. As shown, in terms of test accuracy, the models also experience a smooth and consistent performance improvement with patch size decreasing. Notably, with the help of Adventurer's linear time complexity and efficient memory consumption, we successfully scale up the visual token sequence to a length of 50,176 in the ImageNet classification task. To our knowledge, this is the \textit{first time }that modern visual architectures have extended the input sequence to such a length and processed it directly without partitioning. It not only achieves a highly competitive 84.6\% test accuracy with a base-sized model (100M parameters), but more importantly, it demonstrates that visual understanding can be effectively performed from very long contexts.

\begin{table}[t]
    \tablestyle{5pt}{1.1}
    \begin{tabular}{lcccr}
        Model & Decoder & Params & Patch size & mIoU \\\shline
        \multirow{5}{*}{Adventurer-T} & \demph{\textit{UperNet}} & \demph{\textit{17M}} & \demph{\textit{16\by16}} & \demph{\textit{41.3}} \\
        & None & 12M & 16\by16 & 40.0 \\
        & None & 12M & 8\by8 & 41.6 \\
        & None & 13M & 4\by4 & 42.1 \\\rowcolor{cyan!10}
        \cellcolor{white} & None & 13M & 2\by2 & \bf 42.5 \\\hline

        \multirow{5}{*}{Adventurer-B} & \demph{\textit{UperNet}} & \demph{\textit{112M}} & \demph{\textit{16\by16}} & \demph{\textit{45.7}} \\
        & None & 99M & 16\by16 & 44.0 \\
        & None & 99M & 8\by8 & 45.5 \\
        & None & 100M & 4\by4 & 46.3 \\\rowcolor{cyan!10}
        \cellcolor{white} & None & 100M & 2\by2 & \bf 46.8 \\ 
    \end{tabular}
    \caption{\textbf{ADE20k semantic segmentation.} We focus on decoder-free structures and observe the mIoU score improves smoothly when patch size shrinks. We highlight the results that reach the limits of hardware capabilities in \textcolor{cyan}{blue} and best results \textbf{bolded}.}
    \label{tab:ade}
\end{table}

\begin{figure}[hbt!]
    \centering
    \includegraphics[width=0.9\columnwidth]{figures/seg_bar.pdf}
    \vspace{-0.3cm}
    \caption{\textbf{Decoder's impact on semantic segmentation.} We train a semantic segmentation model with the same backbone but different decoder heads: an UperNet with 13M parameters and a simple linear layer with 0.2M parameters. We observe that as patch size decreases, the impact of the decoder head diminishes.}
    \label{fig:decoder}
    \vspace{-10pt}
\end{figure}

\textbf{ADE20k semantic segmentation} results are summarized in Table~\ref{tab:ade}. As shown, we observe the same scaling behavior in this dense prediction task, with its test loss smoothly decreasing (see Figure~\ref{fig:scaling}) and mIoU score consistently improving as patch size shrinks. It is worth noting that even though we eliminate the task-specific decoder head in this experiment, the encoder-only models—whether the 13M-parameter tiny-sized model or the 100M-parameter base-sized model—can still produce competitive results when the encoding compression rate becomes sufficiently low.

Figure~\ref{fig:decoder} presents a direct comparison on the impact of decoders in semantic segmentation, where we load the same pretrained backbone (Adventurer-Base) and finetune it separately with a UperNet~\cite{upernet} and a simple linear projection layer. As shown, with a high spatial compression rate such as 16$\times$, the model can easily benefit from a decoder head; however, as the patch size decreases and the encoder itself can produce sufficiently fine-grained features, the functionality of decoders starts to be marginalized.

Interestingly, this experiment validates our hypothesis presented in Section~\ref{sec:mtd_tech}, demonstrating that the core component of developing dense prediction models lies in reducing the spatial compression rate, while the help that decoder heads can provide is very limited. This insight further suggests that with non-compressive encoders, it becomes feasible to build a visual foundation model that could provide pixel-level representations and effectively supports various downstream tasks without requiring significant efforts to adapt to their specific objectives. In this work, we keep focusing on the  exploration of patchification scaling and leave the development of \textit{\textbf{pixel foundation models}} for future research. We hope that our findings here can provide a solid theoretical foundation for such endeavors.

\begin{table}[t]
    \tablestyle{4.5pt}{1.1}
    \begin{tabular}{ccccc|ccc}
        Model & Patch & AP$^\text{b}$ & AP$^\text{b}_{50}$ & AP$^\text{b}_{75}$ & AP$^\text{m}$ & AP$^\text{m}_{50}$ & AP$^\text{m}_{75}$ \\\shline
        \multirow{5}{*}{\rotatebox{90}{Adventurer-T}}
        & 32\by32 & 44.7 & 63.3 & 48.6 & 38.4 & 60.4 & 41.4 \\
        & 16\by16 & 46.5 & 65.2 & 50.4 & 40.3 & 62.2 & 43.5 \\
        & 8\by8 & 48.0   & 66.7 & 51.8 & 41.7 & 63.6 & 45.0 \\
        & 4\by4 & 48.5   & 67.1 & 52.3 & 42.2 & 64.1 & 45.4\\
        & \cellcolor{cyan!10} 2\by2 & \cellcolor{cyan!10} \bf 48.7   & \cellcolor{cyan!10} \bf 67.3 & \cellcolor{cyan!10} \bf 52.4 & \cellcolor{cyan!10} \bf 42.4 & \cellcolor{cyan!10} \bf 64.3 & \cellcolor{cyan!10} \bf 45.7\\\hline
        \multirow{5}{*}{\rotatebox{90}{Adventurer-B}}
        & 64\by64 & 44.1 & 62.8 & 48.0 & 38.3 & 60.1 & 41.8\\
        & 32\by32 & 46.4 & 65.0 & 50.3 & 40.6 & 62.5 & 43.1\\
        & 16\by16 & 48.4 & 67.2 & 52.4 & 42.0 & 64.8 & 45.0 \\
        & 8\by8 & 49.5   & 67.9 & 53.3 & 42.9 & 65.5 & 46.1\\
        & \cellcolor{cyan!10} 4\by4 & \cellcolor{cyan!10} \bf 50.3   & \cellcolor{cyan!10} \bf 68.5 & \cellcolor{cyan!10} \bf 54.0 & \cellcolor{cyan!10} \bf 43.4 & \cellcolor{cyan!10} \bf 66.0 & \cellcolor{cyan!10} \bf 46.6\\
    \end{tabular}
    \caption{\textbf{COCO object detection and instance segmentation.} Similar to classification and semantic segmentation results, these two tasks exhibit consistently enhanced performance as patch size decreases. We highlight the results that reach the limits of hardware capabilities in \textcolor{cyan}{blue} and best results \textbf{bolded}.}
    \label{tab:coco}
    \vspace{-0.7cm}
\end{table}

\textbf{COCO object detection and instance segmentation} tasks also showcase a similar effect of patchification scaling. As summarized in Table~\ref{tab:coco}, both tasks achieve their best performance when the patch size reaches the hardware's computational limits (2\by2). Compared to the high compression baselines, both Adventurer-Tiny and Base models demonstrate significant precision improvements, such as 48.7\% \emph{vs.} 44.7\% for Adventurer-Tiny and 50.3\% \emph{vs.} 44.1\% for Base. Notably, we have conducted patch size scaling experiments across four tasks: object classification, semantic segmentation, object detection, and instance segmentation. These experiments span a variety of input resolutions (from 64\by64 to a short side of 800), different training objectives, and different token mixer types (self-attention and Mamba~\cite{mamba}). Despite these variations, a consistent and generalizable conclusion emerges: \textit{\textbf{Reducing patch size reliably guarantees performance gains}}.

\begin{figure}[t]
    \centering
    \begin{subfigure}{\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{figures/patch_vs_param.pdf}
        \vspace{-0.8cm}
        \caption{Scaling from Adventurer-Base/16, 224\by224 input.}
        \label{fig:pvpa}
    \end{subfigure}

    \vspace{-0.05cm}
        
    \begin{subfigure}{\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{figures/patch_vs_param_vit.pdf}
        \vspace{-0.8cm}
        \caption{Scaling form ViT-Base/16, 128\by128 input.}
        \label{fig:pvpv}
    \end{subfigure}
    \vspace{-0.7cm}
    \caption{\textbf{Patch size scaling \emph{vs.} parameter scaling.} Given an Adventurer-Base with 224\by224-resolution inputs, we scale up the model along two dimensions respectively. The model struggles to achieve further accuracy improvements beyond $\sim$760M parameters, whereas scaling down the patch size continues to show a consistent upward trend in performance.} 
    \vspace{-0.5cm}
    \label{fig:pvp}
\end{figure}

\subsection{Ablation Studies}
\label{sec:abl}

\textbf{Patchification scaling \emph{vs.} parameter scaling.} In Figure~\ref{fig:pvpa}, we compare the impact of scaling down the patch size versus scaling up the parameter count on the performance of Adventurer. As shown, for a fixed patch size and input size, increasing the parameter count within a certain range (\emph{e.g.}, up to 760M parameters) yields significant performance gains. However, further scaling beyond this point does not necessarily lead to additional benefits. In fact, overcoming the parameter scaling bottleneck in vision models is both technically challenging and costly. It often requires investing in higher-quality training data~\cite{scalingvit,clip}, incorporating self-supervised learning approaches~\cite{dino,mae}, and making extensive hyperparameter tuning efforts~\cite{deit3}.

In contrast, patch size scaling not only exhibits a better computation-accuracy tradeoff and achieves higher performance limits than parameter scaling, but it also offers a simpler and more straightforward learning process: when training with different patch sizes, there is no need to modify training strategies or datasets, and all experiments can be done in a single run using the same set of hyperparameters.

The potential of patchification scaling is even more evident in ViT. With the same input scale, reducing the patch size yields greater performance improvements for ViT compared to the linear-complexity Adventurer. Additionally, in terms of FLOPs, ViT has more room for scaling, as its computation grows quadratically with sequence length. As shown in Figure~\ref{fig:pvpv}, due to this quadratic complexity, ViT experiences a larger increase in FLOPs than Adventurer when scaling down the patch size, leading to a similar accuracy growth over FLOPs as that of parameter scaling (\emph{e.g.}, ViT-Base/8 \emph{vs.} ViT-Large/16). However, when investing higher FLOPs, parameter scaling falls significant short and may easily collapse with higher parameter counts.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/input_scale.pdf}
    \vspace{-1cm}
    \caption{\textbf{Input size scaling with fixed sequence length.} We fix the ratio of $image\_size / patch\_size$ and scale up the input size for ImageNet classification. As shown, when the input size is scaled beyond its original resolutions (\emph{e.g.}, typically 460 for ImageNet), further interpolating the input images does not yield additional accuracy gains. Instead, it leads to a rapid increase in patchification parameters, resulting in training instability that ultimately harms performance.}
    \vspace{-0.6cm}
    \label{fig:input_scale}
\end{figure}

\textbf{Limitations of input size scaling.} Compared to scaling down the patch size at a fixed input size, another method—directly scaling up the input size—can achieve a similar effect of reducing the compression rate and extending the token sequence. However, we contend that changing the input size is not a flexible and applicable approach for effective scaling, as its upper bound is easily constrained by the original resolution of the image. For example, in the standard ImageNet classification benchmark, images are resized to 224\by224 for both training and evaluation stages~\cite{deit,convnext,adventurer}. This input size has actually compressed the visual information, as the average ImageNet image size is approximately 490\by430 pixels. Within this range, scaling the input size is generally more effective than scaling the patch size. For example, Adventurer-B/16 with a 448×448 input outperforms Adventurer-B/8 with a 224×224 input by 0.4\%, despite having similar parameter counts and FLOPs.

However, beyond this input size, further increasing the input dimensions provides diminishing returns in performance gains. If we fix the sequence length—scaling up the input size while proportionally increasing the patch size—the model undergoes a rapid growth in parameter count in the patchification layer, which may easily result in reduced model efficiency and stability during scaling. We showcase this issue in Figure~\ref{fig:input_scale}, where it is observed that resizing inputs beyond their original resolutions does not provide additional information gains. Instead, the over-parameterization of the patchification layer leads to a considerable performance degradation. As comparison, the direct scaling of patch size can effectively avoid the over-parameterization issue, making it a flexible and practical scaling dimension for modern visual architectures.

\textbf{Scaling in both dimensions.} In Table 4, we provide more ImageNet classification results with Adventurer models, where we scale them in both model size (parameter count) and patch size. As shown, the two scaling dimensions work synergistically when offering performance gains, with the highest accuracies consistently achieved by either the largest models or the smallest patch sizes. As analyzed earlier, the function of patch size scaling lies in reducing the spatial compression rate and thereby enabling the extraction of richer information from the data itself. Intuitively, this effect does not conflict with scaling up the model size, where performance gains mainly stem from enhanced fitting capabilities provided by increased parameters.

\begin{table}[t]
    \tablestyle{6pt}{1.2}
    \begin{tabular}{cccccc}
        \multirow{2}{*}{Model size} & \multicolumn{5}{c}{Patch size} \\\cline{2-6}
        & 16\by16 & 8\by8 & 4\by4 & 2\by2 & 1\by1 \\\shline
        \multicolumn{6}{l}{\textit{\demph{with 128\by128 resolution inputs:}}} \\
        Tiny & \cellcolor{cyan!1}72.6 & \cellcolor{cyan!1}78.5 & \cellcolor{cyan!1}80.4 & \cellcolor{cyan!0.8}80.6 & \cellcolor{cyan!1.6}80.7 \\
Small & \cellcolor{cyan!1}77.6 & \cellcolor{cyan!0.4}80.5 & \cellcolor{cyan!2.8}80.9 & \cellcolor{cyan!5.6}81.2 & \cellcolor{cyan!7.2}81.4 \\
Base & \cellcolor{cyan!1}79.0 & \cellcolor{cyan!9.2}81.5 & \cellcolor{cyan!12.0}81.8 & \cellcolor{cyan!16.8}82.2 & \cellcolor{cyan!19.2}82.4 \\
Large & \cellcolor{cyan!1}79.8 & \cellcolor{cyan!16.8}82.2 & \cellcolor{cyan!21.6}82.6 & \cellcolor{cyan!25.6}82.9 & \cellcolor{cyan!28.0}83.1 \\\hline

        \multicolumn{6}{l}{\textit{\demph{with 224\by224 resolution input:}}} \\
        Tiny & \cellcolor{cyan!1}78.2 & \cellcolor{cyan!1}80.9 & \cellcolor{cyan!1}81.3 & \cellcolor{cyan!1}81.7 & \cellcolor{cyan!1}81.9 \\
Small & \cellcolor{cyan!1}81.8 & \cellcolor{cyan!4.8}83.0 & \cellcolor{cyan!9.6}83.5 & \cellcolor{cyan!12.0}83.7 & \cellcolor{cyan!13.2}83.8 \\
Base & \cellcolor{cyan!7.2}82.6 & \cellcolor{cyan!16.8}83.9 & \cellcolor{cyan!21.6}84.3 & \cellcolor{cyan!24.0}84.5 & \cellcolor{cyan!25.2}84.6 \\
    \end{tabular}
    \caption{\textbf{Scaling both patch and model sizes.} The gains from patch size scaling and model size scaling are not conflicting; combining both can lead to further performance improvements. The numbers denote ImageNet accuracy (\%) with Adventurer models. We associate the results with different \textcolor{cyan}{shades} for clear observation.}
    \label{tab:biscale}
     \vspace{-0.5cm}
    
\end{table}

The results in this experiment suggest that, given sufficient computing resources, we can easily transfer past advancements in parameter scaling to patchification scaling. In other words, patchification scaling can serve as a complement to model size scaling—with the current data volume, we have already observed the limitations of parameter scaling in vision models~\cite{scalingvit,vit22b}, while it is promising to see more future breakthroughs in visual encoding with the help of this new scaling dimension.

\begin{table}[t]
    \tablestyle{5pt}{1.1}
    \begin{tabular}{crrr}
    Model & Length & By extending & By scaling \\\shline
    \multirow{2}{*}{DeiT-Base,}& 64 &  78.1 & 78.1 \\
    \multirow{2}{*}{128\by128 input} & 256 & 78.2 \gtext{(+0.1)} & 81.0 \gtext{(+2.9)} \\
    & \cellcolor{cyan!10} 1,024 & \cellcolor{cyan!10} 78.2 \gtext{(+0.1)} & \cellcolor{cyan!10} 82.3 \gtext{(+4.2)} \\\hline
    
    & 196 & 82.6 & 82.6 \\
    Adventurer-Base, & 784 & 82.7 \gtext{(+0.1)} & 83.9 \gtext{(+1.3)} \\
    224\by224 input & 3,136 & 82.8 \gtext{(+0.2)} & 84.3 \gtext{(+1.7)} \\
    & \cellcolor{cyan!10} 12,544 & \cellcolor{cyan!10} 82.8 \gtext{(+0.2)} & \cellcolor{cyan!10} 84.5 \gtext{(+1.9)} \\
        
    \end{tabular}
    \caption{\textbf{Ablation of sequence length.} Extending the sequence length alone does not yield significant improvements (column ``by extending''), whereas reducing patch size and lowering information compression rate is the primary source of performance gains (column ``by scaling''). Performance is measured by ImageNet-1k accuracy (\%), with longest sequences highlighted in \textcolor{cyan}{blue}.}
    \label{tab:seq}
    \vspace{-0.5cm}
\end{table}

\textbf{Impact of sequence length.} Intuitively, scaling down the patch size has two direct effects. First, smaller patch sizes allow the model to receive richer, more fine-grained input information, which can greatly benefit its inference abilities. Second, reducing the patch size directly extends the token sequence, and for token mixers like self-attention or Mamba, longer sequences inherently expand the model's representational space, enhancing its capabilities in feature processing. Both factors can potentially have a significant impact. We seek to demonstrate that the performance improvement from reducing the patch size primarily arises from the information gain due to a lower compression rate, rather than from the enhanced representational capacity associated with an extended sequence length.

We conduct a direct ablation study: in contrast to our patch size scaling approach, we set up an additional experiment that extends the input sequence interpolating on existing tokens. Specifically, in this comparison, we retain the original large patch size (16\by16) but perform spatial interpolation on the tokens produced by the patch embedding, by which we extend the input sequence without introducing any new information. As shown in Table~\ref{tab:seq}, this approach does not bring substantial improvements to the model’s performance (see column ``By extending''). In contrast to the significant gains achieved through patchification scaling (\emph{e.g.}, 4.2\% accuracy on ImageNet), this ablation study effectively demonstrates that the benefits of our approach primarily stem from unlocking the visual information compressed by large patch sizes, enabling the model to focus on more detailed visual features, while simply scaling the sequence length itself has only a minimal impact on performance.

\begin{table}[t]
    \tablestyle{5pt}{1.1}
    \begin{tabular}{crrrr}
        \multirow{2}{*}{Patch} & \multirow{2}{*}{Length} & Memory & \multicolumn{2}{c}{GPU hours} \\\cline{4-5}
        & & (per image) & DeiT-Base & Adv-Base \\\shline
        16 & 196 & 62MB & 0.36 & 0.45 \\
        8 & 784 & 252MB & 1.86 & 1.76\\
        4 & 3,136 & 1,024MB & 9.79 & 6.86 \\
        2 & 12,544 & 4,057MB & 80.06 & 27.45 \\\rowcolor{cyan!10}
        1 & 50,176 & 16,118MB & 967.99 & 115.08 \\
    \end{tabular}

    \caption{\textbf{Computational overhead} for training a DeiT-Base and Adventurer-Base at 224\by224 resolution inputs and different patch sizes. Memory usage is calculated based on the per-image consumption in ViT. GPU hours (for each ImageNet epoch) are estimated on a single A100 GPU. The models are trained at Float16 precision with FlashAttention~\cite{flashattn} applied in ViT. The detailed evaluation protocol can be found in Appendix. }
    \label{tab:resource}
    \vspace{-0.3cm}
\end{table}

\subsection{Discussions}

We summarize the computational requirements involved in the patchification scaling experiments in Table~\ref{tab:resource}. As shown, the super-long visual token sequences associated with small patch sizes impose a significant hardware overhead on ViT architectures. This overhead was indeed a major challenge around five years ago, when V100 GPUs with 16/32GB memory remained the mainstream hardware for AI training. However, with rapid advancements in hardware development, efficient parallel computing mechanisms, as well as low-complexity visual architectures, the idea of \textit{\textbf{``learning from pixels''}} has become increasingly feasible.

In the experiments, we have demonstrated many key benefits of patchification scaling, such as direct performance improvements, reduced dependence on decoders, and the ability to overcome many limitations of parameter scaling and input size scaling. These emerging properties suggest that, when computational resources allow, we should gradually reduce or even abandon the spatial compression mechanisms in vision encoders, fully exploiting all the information inherently provided by the data. We hope this paper can provide insights and inspiration for a transition from the current patch-based compressive encoding paradigm to pixel-based non-compressive visual foundation models.

\section{Conclusion}
In this work, we conduct extensive studies in reducing the spatial compression rate in patchification layers and discover a new scaling dimension for visual encoding, which we term \textbf{Patchification Scaling Laws}. The new scaling laws suggest that, with more computational resources invested, leveraging smaller patch sizes consistently leads to improved predictive performance. This conclusion is broadly applicable across various vision tasks, different input resolutions, and diverse model architectures. As a by-product, we also identify an interesting emerging property of patchification scaling: when the encoder patch size becomes sufficiently small, the benefits provided by task-specific decoder heads diminish significantly. We hope the discoveries in this paper can provide a solid theoretical foundation for the future pixel learning paradigm and the development of non-compressive visual foundation models.

\section*{Acknowledgments}
We would like to thank TPU Research Cloud (TRC) program and Google Cloud Research Credits program for partially supporting our computing needs.

\bibliography{main}
\bibliographystyle{icml2025}

\newpage
\appendix
\onecolumn

\section*{Appendix}
\label{sec:apdx}

\subsection*{A. More Technical Details}
\textbf{The detailed configuration} of the models used in this paper are elaborated in Table~\ref{tab:config}. For ViTs, We basically follow the configurations in the DeiT series models~\cite{deit,deit3}, but change the default patch size of DeiT-Huge to 16\by16, the same as the other DeiT models. For Adventurer, we scale up the model to a huge size following the same rule of DeiT; we set its embedding dimension to 1,280, keeping its original MLP ratio and employ 32 blocks in total.

\begin{table}[h!]
    \tablestyle{5pt}{1.1}
    \begin{tabular}{lcccc}
        Model & Embedding dimension & MLP dimension & Blocks & Parameters \\\shline
        DeiT-Tiny~\cite{deit} & 192 & 768 & 12 & 5M \\
        DeiT-Small~\cite{deit} & 384 & 1,536 & 12 & 22M \\
        DeiT-Base~\cite{deit} & 768 & 3,072 & 12 & 86M \\
        DeiT-Large~\cite{deit3} & 1,024 & 4,096 & 24 & 304M \\
        DeiT-Huge~\cite{deit3} & 1,280 & 5,120 & 32 & 631M \\\hline
        Adventurer-Tiny~\cite{adventurer} & 256 & 640 & 12 & 12M \\
        Adventurer-Small~\cite{adventurer} & 512 & 1,280 & 12 & 44M \\
        Adventurer-Base~\cite{adventurer} & 768 & 1,920 & 12 & 99M \\
        Adventurer-Large~\cite{adventurer} & 1,024 & 2,560 & 24 & 346M \\
        Adventurer-Huge~\cite{adventurer} & 1,280 & 3,200 & 32 & 759M \\
    \end{tabular}

    \caption{\textbf{Model configurations.} All models have a 16\by16 patch size by default.}
    \label{tab:config}
\end{table}

\textbf{Protocols of estimating memory and GPU hours.} In Table~\ref{tab:resource}, we present an estimation of the GPU memory and training hours required for DeiT and Adventurer. Here we give more details of how they are evaluated. We calculate the memory consumption by each image. That means, the reported numbers have excluded the memory used for storing the model, optimizer, and other hyper-parameters. The actual memory demand increases linearly with batch size. To evaluate the GPU hours required for training, we set a total batch size of 1,024 and use the minimum number of nodes necessary for training (depends on the total memory demand). Each node is equipped with 8 A100/80GB GPUs. The estimated training hours are then multiplied by the total number of GPUs used to ensure that the reported numbers are normalized.

\begin{table}[h]
    \centering
    \tablestyle{10pt}{1.1}
    \begin{tabular}{lcc}
    Config & Tiny/Small/Base & Large/Huge \\\shline
    optimizer & \multicolumn{2}{c}{AdamW} \\
    base learning rate & 5e-4 & 2e-4 \\
    weight decay & 0.05 & 0.3 \\
    epochs & 300 & 200 \\
    optimizer betas & 0.9, 0.999 & 0.9, 0.95 \\
    batch size & 1024 & 4096 \\
    warmup epochs & 5 & 20 \\
    stochastic depth (drop path) & 0.1 & 0.2 \\
    layer-wise lr decay & \multicolumn{2}{c}{\xmark} \\
    label smoothing & \multicolumn{2}{c}{\xmark} \\
    random erasing  & \multicolumn{2}{c}{\xmark} \\
    Rand Augmentation & \multicolumn{2}{c}{\xmark} \\
    repeated augmentation & \multicolumn{2}{c}{\cmark} \\
    ThreeAugmentation & \multicolumn{2}{c}{\cmark} \\
    \end{tabular}
    \caption{\textbf{Recipe of the pretraining stage,} for 64\by64 or 128\by128 pixel inputs.}
    \label{tab:recipe_pt}
\end{table}

\textbf{Training recipes.} In this work, we train DeiT-Tiny, Small, and Base with the official repository~\cite{deit} and recipe. For DeiT-Large and Huge, there is not training configuration in the original DeiT paper so we follow the supervised training pipeline reported in~\cite{mae}. Note that the Pixel Transformer~\cite{pit} which conduct pixel tokenization experiments with low resolution images (28\by28) employs the same training recipe.

For Adventurer, we mostly follow its original multi-stage strategy~\cite{adventurer} to train our models. Specifically, for 64\by64 resolution inputs, we simply perform the pretraining stage (shown in Table~\ref{tab:recipe_pt}) for 300 epochs for all model sizes. For 128\by128 resolution inputs, we additionally perform a finetuning stage (shown in Table~\ref{tab:recipe_ft}) for enhanced results. For the standard 224\by224 resolution inputs, we follow the practice of Mamba-Reg~\cite{mambar} and Adventurer~\cite{adventurer} to load the pretrained model at 128\by128, performing an intermediate training stage (shown in Table~\ref{tab:recipe_mid}) for 100 epochs and then a finetuning stage for 20 epochs. We highlight that this multi-stage training strategy is highly efficient for our experiments as we can fully exploit the models pretrained at lower resolutions. For example, we only need to train the 224\by224-input models for 120 epochs since we can load the weights pretrained at 128\by128 resolution inputs.

Notably, for both DeiT and Adventurer, \textit{\textbf{there is no need to adjust training recipes for different patch sizes}}, which we consider to be one of the flexible and practical advantages of patchification scaling.

\begin{table}[h]
    \centering
    \tablestyle{10pt}{1.1}
    \begin{tabular}{lcc}
    Config & Small/Base & Large \\\shline
    optimizer & \multicolumn{2}{c}{AdamW} \\
    base learning rate & 1e-5 & 2e-5 \\
    weight decay & 0.1 & 0.1 \\
    epochs & 20 & 50 \\
    optimizer betas & 0.9, 0.999 & 0.9, 0.95 \\
    batch size & 512 & 512 \\
    warmup epochs & 5 & 5 \\
    stochastic depth (drop path) & 0.4 (S), 0.6 (B) & 0.6 \\
    layer-wise lr decay & \xmark & 0.95 \\
    label smoothing & \multicolumn{2}{c}{0.1} \\
    random erasing  & \multicolumn{2}{c}{\xmark} \\
    Rand Augmentation & \multicolumn{2}{c}{rand-m9-mstd0.5-inc1} \\
    repeated augmentation & \multicolumn{2}{c}{\xmark} \\
    ThreeAugmentation & \multicolumn{2}{c}{\xmark} \\
    \end{tabular}
    \caption{\textbf{Recipe of the finetuning stage,} for 128\by128 or 224\by224 pixel inputs.}
    \label{tab:recipe_ft}
\end{table}

\begin{table}[h]
    \centering
    \tablestyle{10pt}{1.1}
    \begin{tabular}{lcc}
    Config & Small/Base & Large \\\shline
    optimizer & \multicolumn{2}{c}{AdamW} \\
    base learning rate & 5e-4 & 8e-4 \\
    weight decay & 0.05 & 0.3 \\
    epochs & 100 & 50 \\
    optimizer betas & 0.9, 0.999 & 0.9, 0.95 \\
    batch size & 1024 & 4096 \\
    warmup epochs & 5 & 20 \\
    stochastic depth (drop path) & 0.2 (S), 0.4 (B) & 0.4 \\
    layer-wise lr decay & \xmark & 0.9 \\
    label smoothing & \multicolumn{2}{c}{\xmark} \\
    random erasing  & \multicolumn{2}{c}{\xmark} \\
    Rand Augmentation & \multicolumn{2}{c}{\xmark} \\
    repeated augmentation & \multicolumn{2}{c}{\cmark} \\
    ThreeAugmentation & \multicolumn{2}{c}{\cmark} \\
    \end{tabular}
    \caption{\textbf{Recipe of the intermediate training stage,} for 224\by224 pixel inputs.}
    \label{tab:recipe_mid}
\end{table}


\end{document}