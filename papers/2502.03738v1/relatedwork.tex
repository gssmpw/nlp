\section{Related Work}
\label{sec:rel}

\textbf{Generic visual backbones.} The development of visual backbones has fundamentally shaped the field of computer vision. Initially dominated by Convolutional Neural Networks (CNNs), these architectures have evolved to gain increasing capabilities for visual representation learning. Pioneering works such as LeNet~\cite{lenet} and AlexNet~\cite{alexnet} have proven the significant effectiveness of convolutional architectures in large-scale image classification tasks. Following these foundational models, the architecture has been refined with the innovations in model depth~\cite{vgg}, residual connection~\cite{resnet,densenet}, and efficient neural architecture search~\cite{efficientnet}.

The landscape of visual backbones underwent another round of significant transformation with the introduction of ViTs~\cite{vit} in late 2020, where a novel plain architecture was proposed that treats images akin to language sequences. This model utilizes a simple patchification layer to convert images into sequences of tokens, which are then processed using mechanisms adapted from language models. This approach opened new avenues in handling visual data without the inductive biases inherent in CNNs, demonstrating competitive performance on several benchmarks. The success of ViTs have spurred rapid development and innovations in data-efficient training strategies~\cite{deit,3things,deit3}, self-supervised learning techniques~\cite{dino,mocov3,beit,mae}, vision-language understanding~\cite{clip,align,llava,flamingo,coca}, and hierarchical architecture designs~\cite{swin,crossvit,t2tvit,metaformer}.

Inspired by the patchification design of transformers, there have been many CNN-based~\cite{convnext} and State Space Model~\cite{ssm-control,ssm1,ssm2} based architectures~\cite{vim,mambar,adventurer} following the same paradigm. Notably, the Mamba~\cite{mamba,mamba2} token mixer, due to its advantage of linear complexity, has recently been widely used to explore vision tasks and has achieved competitive results~\cite{vim,mambar,adventurer,arm,vmamba,plainmamba,mambavision,localmamba,videomamba,jamba}. Among them, the Adventurer~\cite{adventurer} architecture, which significantly simplifies the overall model, has demonstrated superior speed compared to the Transformer. In this paper, we employ it as one of the primary experimental models.

\textbf{Visual architecture scaling.} Scaling laws was initially studied in natural language processing~\cite{scalinglaw}. In vision, a similar concept has guided the community to scale up foundational models in both parameter size and data volume. For example, in the age of CNNs, EfficientNets~\cite{efficientnet,efficientnetv2} have proposed to scale-up the models in depth, width, and  resolution. These advancements were then integrated into ResNets, leading to nearly Billion-level parameter CNNs~\cite{xie2020self,bigvit,gpipe,revisitresnet,resnettimm}. Scaling the parameter count of Vision Transformers has also shown a great success in modern visual understanding benchmarks and has exhibited state-of-the-art results~\cite{deepervit,deepvit,scalingvit,vit22b}.