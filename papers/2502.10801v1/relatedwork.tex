\section{Related Work}
%-------------------------------------------------------------------------------

%-------------------------------------------------------------------------------
\subsection{Face Swapping}
%-------------------------------------------------------------------------------
Face swapping employs advanced AI to manipulate visual content, altering identity features and generating face-swapped images with specific expressions and movements~\cite{mirsky2021creation}. This field has seen extensive research, with numerous methods proposed~\cite{tolosana2020deepfakes}. Early image-level methods transfer target face attributes to the source face using segmentation masks for blending~\cite{nirkin2018face}. For example, FSGAN~\cite{nirkin2019fsgan} uses a reenactment network to transfer expressions and poses, followed by a blending network combining Poisson optimization ~\cite{perez2003poisson} and perceptual loss. However, these methods are sensitive to source images and often fail to preserve target image attributes, leading to artifacts. 
State-of-the-art feature-level methods extract identity features from the source face, and attribute features from the target face ~\cite{bao2018IPGAN,chen2020simswap}, then decode these features into generated images. FaceShifter~\cite{li2019FaceShifter} produces high-fidelity results by adaptively integrating target attributes (e.g., expression, lighting) and handling occlusions in a self-supervised manner. SimSwap~\cite{chen2020simswap} further improves this by introducing a weak feature matching loss, preserving attributes, and avoiding mismatched poses and expressions. Overall, face swapping continues to evolve, focusing on reducing artifacts and improving identity and attribute preservation.

However, the malicious exploitation of face-swapping technology could pose significant threats to security and privacy ~\cite{tan2024rethinking}, particularly by presenting unprecedented challenges to identity verification services that rely on underlying facial recognition technology.
Tariq \textit{et al.} ~\cite{tariq2022real} examine the vulnerability of the celebrity recognition APIs to DeepFake attacks, revealing significant weaknesses in identity verification systems.
Li \textit{et al.} ~\cite{li2022seeing} investigate the security of facial liveness verification systems, including facial recognition technology, highlighting vulnerabilities in widely deployed APIs within the evolving attack-defense landscape.

%-------------------------------------------------------------------------------
\subsection{Defense Method}
%-------------------------------------------------------------------------------
To combat face-swapping threats, researchers have focused on detection~\cite{gu2022delving, wang2024deepfake}, primarily using binary classification models~\cite{masood2023trend1}.  
However, these passive methods detect DeepFakes only after creation, allowing security breaches, identity theft, or reputational damage to have already occurred first.

Recent efforts in proactive defense against facial manipulation involve adding imperceptible adversarial perturbations to user images to disrupt the synthesis of generated images ~\cite{ruiz2020disrupting, huang2021initiative}. For instance, Yeh \textit{et al.} \cite{yeh2020disrupting} propose distortion and nullifying attacks, which maximize the distance between adversarial and original outputs or minimize the distance between adversarial images and inputs, respectively. These methods effectively protect against image-translation-based DeepFake attacks in white-box scenarios. In black-box settings, Ruiz \textit{et al.} ~\cite{ruiz2020protecting} propose a query-based adversarial attack, though its practicality is limited due to high query demands.
Existing defenses mainly target attribute editing or facial reenactment (e.g., StarGAN ~\cite{choi2018stargan}, GANimation ~\cite{pumarola2018ganimation}), neglecting face-swapping models that manipulate hard biometric features, which pose greater threats. Dong \textit{et al.} \cite{dong2023restricted} use adversarial example transferability to resist face swapping in restricted black-box scenarios. However, like prior methods, they disrupt images at the pixel level, reducing visual quality without significantly altering identity features, thus failing to fully mitigate identity theft risks. Additionally, current methods often rely on specific DeepFake models, limiting their effectiveness across different models.

%-------------------------------------------------------------------------------