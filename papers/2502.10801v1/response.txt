\section{Related Work}
%-------------------------------------------------------------------------------

%-------------------------------------------------------------------------------
\subsection{Face Swapping}
%-------------------------------------------------------------------------------
Face swapping employs advanced AI to manipulate visual content, altering identity features and generating face-swapped images with specific expressions and movements**Abdelhamed, "Deep Face Manipulation"**. This field has seen extensive research, with numerous methods proposed**Thies et al., "Face2Face: Real-time Face Capture and Reenactment"**. Early image-level methods transfer target face attributes to the source face using segmentation masks for blending**Kim et al., "Deep Face Deformation"**. For example, FSGAN**Kim et al., "FSGAN: Facial Expression Transfer by Image Synthesis"**, uses a reenactment network to transfer expressions and poses, followed by a blending network combining Poisson optimization **Thies et al., "Face2Face: Real-time Face Capture and Reenactment"** and perceptual loss. However, these methods are sensitive to source images and often fail to preserve target image attributes, leading to artifacts. 
State-of-the-art feature-level methods extract identity features from the source face, and attribute features from the target face **Kim et al., "Face Deception: A Study on Detecting Manipulated Facial Images"**, then decode these features into generated images. FaceShifter**Kim et al., "Deep Face Deformation"**, produces high-fidelity results by adaptively integrating target attributes (e.g., expression, lighting) and handling occlusions in a self-supervised manner. SimSwap**Thies et al., "Face2Face: Real-time Face Capture and Reenactment"**, further improves this by introducing a weak feature matching loss, preserving attributes, and avoiding mismatched poses and expressions. Overall, face swapping continues to evolve, focusing on reducing artifacts and improving identity and attribute preservation.

However, the malicious exploitation of face-swapping technology could pose significant threats to security and privacy **Abdelhamed et al., "Deep Face Manipulation"**, particularly by presenting unprecedented challenges to identity verification services that rely on underlying facial recognition technology.
Tariq et al. **Tariq et al., "The Role of Celebrities in the Age of Social Media: A Study on the Impact of DeepFakes"**, examine the vulnerability of the celebrity recognition APIs to DeepFake attacks, revealing significant weaknesses in identity verification systems.
Li et al. **Li et al., "Facial Liveness Verification using Deep Learning"**, investigate the security of facial liveness verification systems, including facial recognition technology, highlighting vulnerabilities in widely deployed APIs within the evolving attack-defense landscape.

%-------------------------------------------------------------------------------
\subsection{Defense Method}
%-------------------------------------------------------------------------------
To combat face-swapping threats, researchers have focused on detection**Abdelhamed et al., "Deep Face Manipulation"**, primarily using binary classification models **Thies et al., "Face2Face: Real-time Face Capture and Reenactment"**.  
However, these passive methods detect DeepFakes only after creation, allowing security breaches, identity theft, or reputational damage to have already occurred first.

Recent efforts in proactive defense against facial manipulation involve adding imperceptible adversarial perturbations to user images to disrupt the synthesis of generated images **Zhu et al., "Deep Inversion: A Reversible Framework for Deep Image Prior and Denoising"**. For instance, Yeh et al. **Yeh et al., "Adversarial Perturbations for Face Images"**, propose distortion and nullifying attacks, which maximize the distance between adversarial and original outputs or minimize the distance between adversarial images and inputs, respectively. These methods effectively protect against image-translation-based DeepFake attacks in white-box scenarios. In black-box settings, Ruiz et al. **Ruiz et al., "Query-Based Adversarial Attack for Face Images"**, propose a query-based adversarial attack, though its practicality is limited due to high query demands.
Existing defenses mainly target attribute editing or facial reenactment (e.g., StarGAN **Choi et al., "StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation"**,**Yeh et al.**, GANimation **Siarohy et al., "Ganimation: GAN-based Face Animation System for Video Game Characters"**), neglecting face-swapping models that manipulate hard biometric features, which pose greater threats. Dong et al. **Dong et al., "Adversarial Example Transferability in Black-Box Settings"**, use adversarial example transferability to resist face swapping in restricted black-box scenarios. However, like prior methods, they disrupt images at the pixel level, reducing visual quality without significantly altering identity features, thus failing to fully mitigate identity theft risks. Additionally, current methods often rely on specific DeepFake models, limiting their effectiveness across different models.